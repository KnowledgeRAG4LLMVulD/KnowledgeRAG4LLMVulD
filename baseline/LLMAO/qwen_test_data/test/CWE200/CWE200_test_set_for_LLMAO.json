[
    {
        "code": "static ssize_t snd_timer_user_read(struct file *file, char __user *buffer,\n\t\t\t\t   size_t count, loff_t *offset)\n{\n\tstruct snd_timer_user *tu;\n\tlong result = 0, unit;\n\tint qhead;\n\tint err = 0;\n\ttu = file->private_data;\n\tunit = tu->tread ? sizeof(struct snd_timer_tread) : sizeof(struct snd_timer_read);\n\tspin_lock_irq(&tu->qlock);\n\twhile ((long)count - result >= unit) {\n\t\twhile (!tu->qused) {\n\t\t\twait_queue_t wait;\n\t\t\tif ((file->f_flags & O_NONBLOCK) != 0 || result > 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\tinit_waitqueue_entry(&wait, current);\n\t\t\tadd_wait_queue(&tu->qchange_sleep, &wait);\n\t\t\tspin_unlock_irq(&tu->qlock);\n\t\t\tschedule();\n\t\t\tspin_lock_irq(&tu->qlock);\n\t\t\tremove_wait_queue(&tu->qchange_sleep, &wait);\n\t\t\tif (tu->disconnected) {\n\t\t\t\terr = -ENODEV;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = -ERESTARTSYS;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t}\n\t\tqhead = tu->qhead++;\n\t\ttu->qhead %= tu->queue_size;\n\t\ttu->qused--;\n\t\tspin_unlock_irq(&tu->qlock);\n\t\tmutex_lock(&tu->ioctl_lock);\n\t\tif (tu->tread) {\n\t\t\tif (copy_to_user(buffer, &tu->tqueue[qhead],\n\t\t\t\t\t sizeof(struct snd_timer_tread)))\n\t\t\t\terr = -EFAULT;\n\t\t} else {\n\t\t\tif (copy_to_user(buffer, &tu->queue[qhead],\n\t\t\t\t\t sizeof(struct snd_timer_read)))\n\t\t\t\terr = -EFAULT;\n\t\t}\n\t\tmutex_unlock(&tu->ioctl_lock);\n\t\tspin_lock_irq(&tu->qlock);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t\tresult += unit;\n\t\tbuffer += unit;\n\t}\n _error:\n\tspin_unlock_irq(&tu->qlock);\n\treturn result > 0 ? result : err;\n}",
        "bug_line_number": [
            8,
            9,
            20,
            21,
            22,
            37,
            47,
            55,
            56
        ],
        "vul": 1,
        "id": 0
    },
    {
        "code": "static int parse_status(const char *value)\n{\n\tint ret = 0;\n\tchar *c;\n\t/* skip a header line */\n\tc = strchr(value, '\\n');\n\tif (!c)\n\t\treturn -1;\n\tc++;\n\twhile (*c != '\\0') {\n\t\tint port, status, speed, devid;\n\t\tunsigned long socket;\n\t\tchar lbusid[SYSFS_BUS_ID_SIZE];\n\t\tstruct usbip_imported_device *idev;\n\t\tchar hub[3];\n\t\tret = sscanf(c, \"%2s  %d %d %d %x %lx %31s\\n\",\n\t\t\t\thub, &port, &status, &speed,\n\t\t\t\t&devid, &socket, lbusid);\n\t\tif (ret < 5) {\n\t\t\tdbg(\"sscanf failed: %d\", ret);\n\t\t\tBUG();\n\t\t}\n\t\tdbg(\"hub %s port %d status %d speed %d devid %x\",\n\t\t\t\thub, port, status, speed, devid);\n\t\tdbg(\"socket %lx lbusid %s\", socket, lbusid);\n\t\t/* if a device is connected, look at it */\n\t\tidev = &vhci_driver->idev[port];\n\t\tmemset(idev, 0, sizeof(*idev));\n\t\tif (strncmp(\"hs\", hub, 2) == 0)\n\t\t\tidev->hub = HUB_SPEED_HIGH;\n\t\telse /* strncmp(\"ss\", hub, 2) == 0 */\n\t\t\tidev->hub = HUB_SPEED_SUPER;\n\t\tidev->port\t= port;\n\t\tidev->status\t= status;\n\t\tidev->devid\t= devid;\n\t\tidev->busnum\t= (devid >> 16);\n\t\tidev->devnum\t= (devid & 0x0000ffff);\n\t\tif (idev->status != VDEV_ST_NULL\n\t\t    && idev->status != VDEV_ST_NOTASSIGNED) {\n\t\t\tidev = imported_device_init(idev, lbusid);\n\t\t\tif (!idev) {\n\t\t\t\tdbg(\"imported_device_init failed\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t\t/* go to the next line */\n\t\tc = strchr(c, '\\n');\n\t\tif (!c)\n\t\t\tbreak;\n\t\tc++;\n\t}\n\tdbg(\"exit\");\n\treturn 0;\n}",
        "bug_line_number": [
            11,
            12,
            15,
            16,
            17,
            18,
            24,
            25
        ],
        "vul": 1,
        "id": 2
    },
    {
        "code": "static ssize_t nports_show(struct device *dev, struct device_attribute *attr,\n\t\t\t   char *out)\n{\n\tchar *s = out;\n\t/*\n\t * Half the ports are for SPEED_HIGH and half for SPEED_SUPER, thus the * 2.\n\t */\n\tout += sprintf(out, \"%d\\n\", VHCI_PORTS * vhci_num_controllers);\n\treturn out - s;\n}",
        "bug_line_number": [
            5,
            6
        ],
        "vul": 1,
        "id": 4
    },
    {
        "code": "static unsigned int help(struct sk_buff *skb,\n\t\t\t enum ip_conntrack_info ctinfo,\n\t\t\t unsigned int protoff,\n\t\t\t unsigned int matchoff,\n\t\t\t unsigned int matchlen,\n\t\t\t struct nf_conntrack_expect *exp)\n{\n\tchar buffer[sizeof(\"4294967296 65635\")];\n\tu_int16_t port;\n\tunsigned int ret;\n\t/* Reply comes from server. */\n\texp->saved_proto.tcp.port = exp->tuple.dst.u.tcp.port;\n\texp->dir = IP_CT_DIR_REPLY;\n\texp->expectfn = nf_nat_follow_master;\n\t/* Try to get same port: if not, try to change it. */\n\tfor (port = ntohs(exp->saved_proto.tcp.port); port != 0; port++) {\n\t\tint ret;\n\t\texp->tuple.dst.u.tcp.port = htons(port);\n\t\tret = nf_ct_expect_related(exp);\n\t\tif (ret == 0)\n\t\t\tbreak;\n\t\telse if (ret != -EBUSY) {\n\t\t\tport = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (port == 0) {\n\t\tnf_ct_helper_log(skb, exp->master, \"all ports in use\");\n\t\treturn NF_DROP;\n\t}\n\tret = nf_nat_mangle_tcp_packet(skb, exp->master, ctinfo,\n\t\t\t\t       protoff, matchoff, matchlen, buffer,\n\t\t\t\t       strlen(buffer));\n\tif (ret != NF_ACCEPT) {\n\t\tnf_ct_helper_log(skb, exp->master, \"cannot mangle packet\");\n\t\tnf_ct_unexpect_related(exp);\n\t}\n\treturn ret;\n}",
        "bug_line_number": [
            7,
            8,
            10,
            11,
            27,
            28,
            30,
            31,
            32,
            33,
            34,
            35
        ],
        "vul": 1,
        "id": 6
    },
    {
        "code": "int fb_copy_cmap(const struct fb_cmap *from, struct fb_cmap *to)\n{\n\tint tooff = 0, fromoff = 0;\n\tint size;\n\tif (to->start > from->start)\n\t\tfromoff = to->start - from->start;\n\telse\n\t\ttooff = from->start - to->start;\n\tsize = to->len - tooff;\n\tif (size > (int) (from->len - fromoff))\n\t\tsize = from->len - fromoff;\n\tif (size <= 0)\n\t\treturn -EINVAL;\n\tsize *= sizeof(u16);\n\tmemcpy(to->red+tooff, from->red+fromoff, size);\n\tmemcpy(to->green+tooff, from->green+fromoff, size);\n\tmemcpy(to->blue+tooff, from->blue+fromoff, size);\n\tif (from->transp && to->transp)\n\t\tmemcpy(to->transp+tooff, from->transp+fromoff, size);\n\treturn 0;\n}",
        "bug_line_number": [
            2,
            3,
            4,
            8,
            9,
            10,
            11,
            12
        ],
        "vul": 1,
        "id": 8
    },
    {
        "code": "int fb_cmap_to_user(const struct fb_cmap *from, struct fb_cmap_user *to)\n{\n\tint tooff = 0, fromoff = 0;\n\tint size;\n\tif (to->start > from->start)\n\t\tfromoff = to->start - from->start;\n\telse\n\t\ttooff = from->start - to->start;\n\tsize = to->len - tooff;\n\tif (size > (int) (from->len - fromoff))\n\t\tsize = from->len - fromoff;\n\tif (size <= 0)\n\t\treturn -EINVAL;\n\tsize *= sizeof(u16);\n\tif (copy_to_user(to->red+tooff, from->red+fromoff, size))\n\t\treturn -EFAULT;\n\tif (copy_to_user(to->green+tooff, from->green+fromoff, size))\n\t\treturn -EFAULT;\n\tif (copy_to_user(to->blue+tooff, from->blue+fromoff, size))\n\t\treturn -EFAULT;\n\tif (from->transp && to->transp)\n\t\tif (copy_to_user(to->transp+tooff, from->transp+fromoff, size))\n\t\t\treturn -EFAULT;\n\treturn 0;\n}",
        "bug_line_number": [
            2,
            3,
            4,
            8,
            9,
            10,
            11,
            12
        ],
        "vul": 1,
        "id": 10
    },
    {
        "code": "static void snd_timer_user_ccallback(struct snd_timer_instance *timeri,\n\t\t\t\t     int event,\n\t\t\t\t     struct timespec *tstamp,\n\t\t\t\t     unsigned long resolution)\n{\n\tstruct snd_timer_user *tu = timeri->callback_data;\n\tstruct snd_timer_tread r1;\n\tunsigned long flags;\n\tif (event >= SNDRV_TIMER_EVENT_START &&\n\t    event <= SNDRV_TIMER_EVENT_PAUSE)\n\t\ttu->tstamp = *tstamp;\n\tif ((tu->filter & (1 << event)) == 0 || !tu->tread)\n\t\treturn;\n\tr1.event = event;\n\tr1.tstamp = *tstamp;\n\tr1.val = resolution;\n\tspin_lock_irqsave(&tu->qlock, flags);\n\tsnd_timer_user_append_to_tqueue(tu, &r1);\n\tspin_unlock_irqrestore(&tu->qlock, flags);\n\tkill_fasync(&tu->fasync, SIGIO, POLL_IN);\n\twake_up(&tu->qchange_sleep);\n}",
        "bug_line_number": [
            12,
            13
        ],
        "vul": 1,
        "id": 12
    },
    {
        "code": "\t\t} else\n\t\t\tret = -ESRCH;\n\t}\n\tbreak;\n\tcase IP_VS_SO_GET_DESTS:\n\t{\n\t\tstruct ip_vs_get_dests *get;\n\t\tint size;\n\t\tget = (struct ip_vs_get_dests *)arg;\n\t\tsize = sizeof(*get) +\n\t\t\tsizeof(struct ip_vs_dest_entry) * get->num_dests;\n\t\tif (*len != size) {\n\t\t\tpr_err(\"length: %u != %u\\n\", *len, size);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tret = __ip_vs_get_dest_entries(net, get, user);\n\t}\n\tbreak;\n\tcase IP_VS_SO_GET_TIMEOUT:\n\t{\n\t\tstruct ip_vs_timeout_user t;\n\t\t__ip_vs_get_timeouts(net, &t);\n\t\tif (copy_to_user(user, &t, sizeof(t)) != 0)\n\t\t\tret = -EFAULT;\n\t}\n\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\nout:\n\tmutex_unlock(&__ip_vs_mutex);\n\treturn ret;\n}",
        "bug_line_number": [
            21,
            22
        ],
        "vul": 1,
        "id": 14
    },
    {
        "code": "static int blkif_completion(unsigned long *id,\n\t\t\t    struct blkfront_ring_info *rinfo,\n\t\t\t    struct blkif_response *bret)\n{\n\tint i = 0;\n\tstruct scatterlist *sg;\n\tint num_sg, num_grant;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct blk_shadow *s = &rinfo->shadow[*id];\n\tstruct copy_from_grant data = {\n\t\t.grant_idx = 0,\n\t};\n\tnum_grant = s->req.operation == BLKIF_OP_INDIRECT ?\n\t\ts->req.u.indirect.nr_segments : s->req.u.rw.nr_segments;\n\t/* The I/O request may be split in two. */\n\tif (unlikely(s->associated_id != NO_ASSOCIATED_ID)) {\n\t\tstruct blk_shadow *s2 = &rinfo->shadow[s->associated_id];\n\t\t/* Keep the status of the current response in shadow. */\n\t\ts->status = blkif_rsp_to_req_status(bret->status);\n\t\t/* Wait the second response if not yet here. */\n\t\tif (s2->status < REQ_DONE)\n\t\t\treturn 0;\n\t\tbret->status = blkif_get_final_status(s->status,\n\t\t\t\t\t\t      s2->status);\n\t\t/*\n\t\t * All the grants is stored in the first shadow in order\n\t\t * to make the completion code simpler.\n\t\t */\n\t\tnum_grant += s2->req.u.rw.nr_segments;\n\t\t/*\n\t\t * The two responses may not come in order. Only the\n\t\t * first request will store the scatter-gather list.\n\t\t */\n\t\tif (s2->num_sg != 0) {\n\t\t\t/* Update \"id\" with the ID of the first response. */\n\t\t\t*id = s->associated_id;\n\t\t\ts = s2;\n\t\t}\n\t\t/*\n\t\t * We don't need anymore the second request, so recycling\n\t\t * it now.\n\t\t */\n\t\tif (add_id_to_freelist(rinfo, s->associated_id))\n\t\t\tWARN(1, \"%s: can't recycle the second part (id = %ld) of the request\\n\",\n\t\t\t     info->gd->disk_name, s->associated_id);\n\t}\n\tdata.s = s;\n\tnum_sg = s->num_sg;\n\tif (bret->operation == BLKIF_OP_READ && info->feature_persistent) {\n\t\tfor_each_sg(s->sg, sg, num_sg, i) {\n\t\t\tBUG_ON(sg->offset + sg->length > PAGE_SIZE);\n\t\t\tdata.bvec_offset = sg->offset;\n\t\t\tdata.bvec_data = kmap_atomic(sg_page(sg));\n\t\t\tgnttab_foreach_grant_in_range(sg_page(sg),\n\t\t\t\t\t\t      sg->offset,\n\t\t\t\t\t\t      sg->length,\n\t\t\t\t\t\t      blkif_copy_from_grant,\n\t\t\t\t\t\t      &data);\n\t\t\tkunmap_atomic(data.bvec_data);\n\t\t}\n\t}\n\t/* Add the persistent grant into the list of free grants */\n\tfor (i = 0; i < num_grant; i++) {\n\t\tif (!gnttab_try_end_foreign_access(s->grants_used[i]->gref)) {\n\t\t\t/*\n\t\t\t * If the grant is still mapped by the backend (the\n\t\t\t * backend has chosen to make this grant persistent)\n\t\t\t * we add it at the head of the list, so it will be\n\t\t\t * reused first.\n\t\t\t */\n\t\t\tif (!info->feature_persistent) {\n\t\t\t\tpr_alert(\"backed has not unmapped grant: %u\\n\",\n\t\t\t\t\t s->grants_used[i]->gref);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tlist_add(&s->grants_used[i]->node, &rinfo->grants);\n\t\t\trinfo->persistent_gnts_c++;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If the grant is not mapped by the backend we add it\n\t\t\t * to the tail of the list, so it will not be picked\n\t\t\t * again unless we run out of persistent grants.\n\t\t\t */\n\t\t\ts->grants_used[i]->gref = INVALID_GRANT_REF;\n\t\t\tlist_add_tail(&s->grants_used[i]->node, &rinfo->grants);\n\t\t}\n\t}\n\tif (s->req.operation == BLKIF_OP_INDIRECT) {\n\t\tfor (i = 0; i < INDIRECT_GREFS(num_grant); i++) {\n\t\t\tif (!gnttab_try_end_foreign_access(s->indirect_grants[i]->gref)) {\n\t\t\t\tif (!info->feature_persistent) {\n\t\t\t\t\tpr_alert(\"backed has not unmapped grant: %u\\n\",\n\t\t\t\t\t\t s->indirect_grants[i]->gref);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tlist_add(&s->indirect_grants[i]->node, &rinfo->grants);\n\t\t\t\trinfo->persistent_gnts_c++;\n\t\t\t} else {\n\t\t\t\tstruct page *indirect_page;\n\t\t\t\t/*\n\t\t\t\t * Add the used indirect page back to the list of\n\t\t\t\t * available pages for indirect grefs.\n\t\t\t\t */\n\t\t\t\tif (!info->feature_persistent) {\n\t\t\t\t\tindirect_page = s->indirect_grants[i]->page;\n\t\t\t\t\tlist_add(&indirect_page->lru, &rinfo->indirect_pages);",
        "bug_line_number": [
            48,
            49,
            103,
            104
        ],
        "vul": 1,
        "id": 16
    },
    {
        "code": "static struct grant *get_indirect_grant(grant_ref_t *gref_head,\n\t\t\t\t\tstruct blkfront_ring_info *rinfo)\n{\n\tstruct grant *gnt_list_entry = get_free_grant(rinfo);\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tif (gnt_list_entry->gref != INVALID_GRANT_REF)\n\t\treturn gnt_list_entry;\n\t/* Assign a gref to this page */\n\tgnt_list_entry->gref = gnttab_claim_grant_reference(gref_head);\n\tBUG_ON(gnt_list_entry->gref == -ENOSPC);\n\tif (!info->feature_persistent) {\n\t\tstruct page *indirect_page;\n\t\t/* Fetch a pre-allocated page to use for indirect grefs */\n\t\tBUG_ON(list_empty(&rinfo->indirect_pages));\n\t\tindirect_page = list_first_entry(&rinfo->indirect_pages,\n\t\t\t\t\t\t struct page, lru);\n\t\tlist_del(&indirect_page->lru);\n\t\tgnt_list_entry->page = indirect_page;\n\t}\n\tgrant_foreign_access(gnt_list_entry, info);\n\treturn gnt_list_entry;\n}",
        "bug_line_number": [
            10,
            11
        ],
        "vul": 1,
        "id": 18
    },
    {
        "code": "static int talk_to_blkback(struct xenbus_device *dev,\n\t\t\t   struct blkfront_info *info)\n{\n\tconst char *message = NULL;\n\tstruct xenbus_transaction xbt;\n\tint err;\n\tunsigned int i, max_page_order;\n\tunsigned int ring_page_order;\n\tstruct blkfront_ring_info *rinfo;\n\tif (!info)\n\t\treturn -ENODEV;\n\tmax_page_order = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t      \"max-ring-page-order\", 0);\n\tring_page_order = min(xen_blkif_max_ring_order, max_page_order);\n\tinfo->nr_ring_pages = 1 << ring_page_order;\n\terr = negotiate_mq(info);\n\tif (err)\n\t\tgoto destroy_blkring;\n\tfor_each_rinfo(info, rinfo, i) {\n\t\t/* Create shared ring, alloc event channel. */\n\t\terr = setup_blkring(dev, rinfo);\n\t\tif (err)\n\t\t\tgoto destroy_blkring;\n\t}\nagain:\n\terr = xenbus_transaction_start(&xbt);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"starting transaction\");\n\t\tgoto destroy_blkring;\n\t}\n\tif (info->nr_ring_pages > 1) {\n\t\terr = xenbus_printf(xbt, dev->nodename, \"ring-page-order\", \"%u\",\n\t\t\t\t    ring_page_order);\n\t\tif (err) {\n\t\t\tmessage = \"writing ring-page-order\";\n\t\t\tgoto abort_transaction;\n\t\t}\n\t}\n\t/* We already got the number of queues/rings in _probe */\n\tif (info->nr_rings == 1) {\n\t\terr = write_per_ring_nodes(xbt, info->rinfo, dev->nodename);\n\t\tif (err)\n\t\t\tgoto destroy_blkring;\n\t} else {\n\t\tchar *path;\n\t\tsize_t pathsize;\n\t\terr = xenbus_printf(xbt, dev->nodename, \"multi-queue-num-queues\", \"%u\",\n\t\t\t\t    info->nr_rings);\n\t\tif (err) {\n\t\t\tmessage = \"writing multi-queue-num-queues\";\n\t\t\tgoto abort_transaction;\n\t\t}\n\t\tpathsize = strlen(dev->nodename) + QUEUE_NAME_LEN;\n\t\tpath = kmalloc(pathsize, GFP_KERNEL);\n\t\tif (!path) {\n\t\t\terr = -ENOMEM;\n\t\t\tmessage = \"ENOMEM while writing ring references\";\n\t\t\tgoto abort_transaction;\n\t\t}\n\t\tfor_each_rinfo(info, rinfo, i) {\n\t\t\tmemset(path, 0, pathsize);\n\t\t\tsnprintf(path, pathsize, \"%s/queue-%u\", dev->nodename, i);\n\t\t\terr = write_per_ring_nodes(xbt, rinfo, path);\n\t\t\tif (err) {\n\t\t\t\tkfree(path);\n\t\t\t\tgoto destroy_blkring;\n\t\t\t}\n\t\t}\n\t\tkfree(path);\n\t}\n\terr = xenbus_printf(xbt, dev->nodename, \"protocol\", \"%s\",\n\t\t\t    XEN_IO_PROTO_ABI_NATIVE);\n\tif (err) {\n\t\tmessage = \"writing protocol\";\n\t\tgoto abort_transaction;\n\t}\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-persistent\", \"%u\",\n\t\t\tinfo->feature_persistent);\n\tif (err)\n\t\tdev_warn(&dev->dev,\n\t\t\t \"writing persistent grants feature to xenbus\");\n\terr = xenbus_transaction_end(xbt, 0);\n\tif (err) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto again;\n\t\txenbus_dev_fatal(dev, err, \"completing transaction\");\n\t\tgoto destroy_blkring;\n\t}\n\tfor_each_rinfo(info, rinfo, i) {\n\t\tunsigned int j;\n\t\tfor (j = 0; j < BLK_RING_SIZE(info); j++)\n\t\t\trinfo->shadow[j].req.u.rw.id = j + 1;\n\t\trinfo->shadow[BLK_RING_SIZE(info)-1].req.u.rw.id = 0x0fffffff;\n\t}\n\txenbus_switch_state(dev, XenbusStateInitialised);\n\treturn 0;\n abort_transaction:\n\txenbus_transaction_end(xbt, 1);\n\tif (message)\n\t\txenbus_dev_fatal(dev, err, \"%s\", message);\n destroy_blkring:\n\tblkif_free(info, 0);\n\treturn err;\n}",
        "bug_line_number": [
            10,
            11
        ],
        "vul": 1,
        "id": 20
    },
    {
        "code": "void __cpuinit cpu_init(void)\n{\n\tint cpu = smp_processor_id();\n\tstruct task_struct *curr = current;\n\tstruct tss_struct *t = &per_cpu(init_tss, cpu);\n\tstruct thread_struct *thread = &curr->thread;\n\tif (cpumask_test_and_set_cpu(cpu, cpu_initialized_mask)) {\n\t\tprintk(KERN_WARNING \"CPU#%d already initialized!\\n\", cpu);\n\t\tfor (;;)\n\t\t\tlocal_irq_enable();\n\t}\n\tprintk(KERN_INFO \"Initializing CPU#%d\\n\", cpu);\n\tif (cpu_has_vme || cpu_has_tsc || cpu_has_de)\n\t\tclear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);\n\tload_idt(&idt_descr);\n\tswitch_to_new_gdt(cpu);\n\t/*\n\t * Set up and load the per-CPU TSS and LDT\n\t */\n\tatomic_inc(&init_mm.mm_count);\n\tcurr->active_mm = &init_mm;\n\tBUG_ON(curr->mm);\n\tenter_lazy_tlb(&init_mm, curr);\n\tload_sp0(t, thread);\n\tset_tss_desc(cpu, t);\n\tload_TR_desc();\n\tload_LDT(&init_mm.context);\n\tt->x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);\n#ifdef CONFIG_DOUBLEFAULT\n\t/* Set up doublefault TSS pointer in the GDT */\n\t__set_tss_desc(cpu, GDT_ENTRY_DOUBLEFAULT_TSS, &doublefault_tss);\n#endif\n\tclear_all_debug_regs();\n\tdbg_restore_debug_regs();\n\tfpu_init();\n\txsave_init();\n}",
        "bug_line_number": [
            35
        ],
        "vul": 1,
        "id": 22
    },
    {
        "code": "static inline fpu_switch_t switch_fpu_prepare(struct task_struct *old, struct task_struct *new, int cpu)\n{\n\tfpu_switch_t fpu;\n\t/*\n\t * If the task has used the math, pre-load the FPU on xsave processors\n\t * or if the past 5 consecutive context-switches used math.\n\t */\n\tfpu.preload = tsk_used_math(new) && (use_xsave() ||\n\t\t\t\t\t     new->fpu_counter > 5);\n\tif (__thread_has_fpu(old)) {\n\t\tif (!__save_init_fpu(old))\n\t\t\tcpu = ~0;\n\t\told->thread.fpu.last_cpu = cpu;\n\t\told->thread.fpu.has_fpu = 0;\t/* But leave fpu_owner_task! */\n\t\t/* Don't change CR0.TS if we just switch! */\n\t\tif (fpu.preload) {\n\t\t\tnew->fpu_counter++;\n\t\t\t__thread_set_has_fpu(new);\n\t\t\tprefetch(new->thread.fpu.state);\n\t\t} else if (!use_xsave())\n\t\t\tstts();\n\t} else {\n\t\told->fpu_counter = 0;\n\t\told->thread.fpu.last_cpu = ~0;\n\t\tif (fpu.preload) {\n\t\t\tnew->fpu_counter++;\n\t\t\tif (!use_xsave() && fpu_lazy_restore(new, cpu))\n\t\t\t\tfpu.preload = 0;\n\t\t\telse\n\t\t\t\tprefetch(new->thread.fpu.state);\n\t\t\t__thread_fpu_begin(new);\n\t\t}\n\t}\n\treturn fpu;\n}",
        "bug_line_number": [
            7,
            8,
            19,
            20,
            26,
            27
        ],
        "vul": 1,
        "id": 24
    },
    {
        "code": "static int do_video_set_spu_palette(unsigned int fd, unsigned int cmd,\n\t\tstruct compat_video_spu_palette __user *up)\n{\n\tstruct video_spu_palette __user *up_native;\n\tcompat_uptr_t palp;\n\tint length, err;\n\terr  = get_user(palp, &up->palette);\n\terr |= get_user(length, &up->length);\n\tup_native = compat_alloc_user_space(sizeof(struct video_spu_palette));\n\terr  = put_user(compat_ptr(palp), &up_native->palette);\n\terr |= put_user(length, &up_native->length);\n\tif (err)\n\t\treturn -EFAULT;\n\terr = sys_ioctl(fd, cmd, (unsigned long) up_native);\n\treturn err;\n}",
        "bug_line_number": [
            7,
            8
        ],
        "vul": 1,
        "id": 26
    },
    {
        "code": "\t\t\t\tsg_count[i] = usg->sg[i].count;\n\t\t\t\tif (sg_count[i] >\n\t\t\t\t    ((dev->adapter_info.options &\n\t\t\t\t     AAC_OPT_NEW_COMM) ?\n\t\t\t\t      (dev->scsi_host_ptr->max_sectors << 9) :\n\t\t\t\t      65536)) {\n\t\t\t\t\trcode = -EINVAL;\n\t\t\t\t\tgoto cleanup;\n\t\t\t\t}\n\t\t\t\tp = kmalloc(sg_count[i], GFP_KERNEL|GFP_DMA32);\n\t\t\t\tif (!p) {\n\t\t\t\t\tdprintk((KERN_DEBUG\"aacraid: Could not allocate SG buffer - size = %d buffer number %d of %d\\n\",\n\t\t\t\t\t\tsg_count[i], i, usg->count));\n\t\t\t\t\trcode = -ENOMEM;\n\t\t\t\t\tgoto cleanup;\n\t\t\t\t}\n\t\t\t\taddr = (u64)usg->sg[i].addr[0];\n\t\t\t\taddr += ((u64)usg->sg[i].addr[1]) << 32;\n\t\t\t\tsg_user[i] = (void __user *)addr;\n\t\t\t\tsg_list[i] = p; // save so we can clean up later\n\t\t\t\tsg_indx = i;\n\t\t\t\tif (flags & SRB_DataOut) {\n\t\t\t\t\tif (copy_from_user(p, sg_user[i],\n\t\t\t\t\t\tsg_count[i])){\n\t\t\t\t\t\tdprintk((KERN_DEBUG\"aacraid: Could not copy sg data from user\\n\"));\n\t\t\t\t\t\trcode = -EFAULT;\n\t\t\t\t\t\tgoto cleanup;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\taddr = pci_map_single(dev->pdev, p, usg->sg[i].count, data_dir);\n\t\t\t\tpsg->sg[i].addr = cpu_to_le32(addr & 0xffffffff);\n\t\t\t\tbyte_count += usg->sg[i].count;\n\t\t\t\tpsg->sg[i].count = cpu_to_le32(sg_count[i]);\n\t\t\t}\n\t\t} else {\n\t\t\tfor (i = 0; i < upsg->count; i++) {\n\t\t\t\tdma_addr_t addr;\n\t\t\t\tvoid* p;\n\t\t\t\tsg_count[i] = upsg->sg[i].count;\n\t\t\t\tif (sg_count[i] >\n\t\t\t\t    ((dev->adapter_info.options &\n\t\t\t\t     AAC_OPT_NEW_COMM) ?\n\t\t\t\t      (dev->scsi_host_ptr->max_sectors << 9) :\n\t\t\t\t      65536)) {\n\t\t\t\t\trcode = -EINVAL;\n\t\t\t\t\tgoto cleanup;\n\t\t\t\t}\n\t\t\t\tp = kmalloc(sg_count[i], GFP_KERNEL|GFP_DMA32);\n\t\t\t\tif (!p) {\n\t\t\t\t\tdprintk((KERN_DEBUG\"aacraid: Could not allocate SG buffer - size = %d buffer number %d of %d\\n\",\n\t\t\t\t\t  sg_count[i], i, upsg->count));\n\t\t\t\t\trcode = -ENOMEM;\n\t\t\t\t\tgoto cleanup;\n\t\t\t\t}\n\t\t\t\tsg_user[i] = (void __user *)(uintptr_t)upsg->sg[i].addr;\n\t\t\t\tsg_list[i] = p; // save so we can clean up later\n\t\t\t\tsg_indx = i;\n\t\t\t\tif (flags & SRB_DataOut) {\n\t\t\t\t\tif (copy_from_user(p, sg_user[i],\n\t\t\t\t\t\tsg_count[i])) {\n\t\t\t\t\t\tdprintk((KERN_DEBUG\"aacraid: Could not copy sg data from user\\n\"));\n\t\t\t\t\t\trcode = -EFAULT;\n\t\t\t\t\t\tgoto cleanup;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\taddr = pci_map_single(dev->pdev, p,\n\t\t\t\t\tsg_count[i], data_dir);\n\t\t\t\tpsg->sg[i].addr = cpu_to_le32(addr);\n\t\t\t\tbyte_count += sg_count[i];\n\t\t\t\tpsg->sg[i].count = cpu_to_le32(sg_count[i]);\n\t\t\t}\n\t\t}\n\t\tsrbcmd->count = cpu_to_le32(byte_count);\n\t\tif (user_srbcmd->sg.count)\n\t\t\tpsg->count = cpu_to_le32(sg_indx+1);\n\t\telse\n\t\t\tpsg->count = 0;\n\t\tstatus = aac_fib_send(ScsiPortCommand, srbfib, actual_fibsize, FsaNormal, 1, 1, NULL, NULL);\n\t}\n\tif (status == -ERESTARTSYS) {\n\t\trcode = -ERESTARTSYS;\n\t\tgoto cleanup;\n\t}\n\tif (status != 0) {\n\t\tdprintk((KERN_DEBUG\"aacraid: Could not send raw srb fib to hba\\n\"));\n\t\trcode = -ENXIO;\n\t\tgoto cleanup;\n\t}\n\tif (flags & SRB_DataIn) {\n\t\tfor(i = 0 ; i <= sg_indx; i++){\n\t\t\tif (copy_to_user(sg_user[i], sg_list[i], sg_count[i])) {\n\t\t\t\tdprintk((KERN_DEBUG\"aacraid: Could not copy sg data to user\\n\"));\n\t\t\t\trcode = -EFAULT;\n\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t}\n\t}\n\tuser_reply = arg + fibsize;\n\tif (is_native_device) {\n\t\tstruct aac_hba_resp *err =\n\t\t\t&((struct aac_native_hba *)srbfib->hw_fib_va)->resp.err;\n\t\tstruct aac_srb_reply reply;\n\t\treply.status = ST_OK;\n\t\tif (srbfib->flags & FIB_CONTEXT_FLAG_FASTRESP) {\n\t\t\t/* fast response */\n\t\t\treply.srb_status = SRB_STATUS_SUCCESS;\n\t\t\treply.scsi_status = 0;\n\t\t\treply.data_xfer_length = byte_count;",
        "bug_line_number": [
            101,
            102
        ],
        "vul": 1,
        "id": 28
    },
    {
        "code": "int udp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\tsize_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tbool slow;\n\t/*\n\t *\tCheck any passed addresses\n\t */\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ip_recv_error(sk, msg, len);\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb,\n\t\t\t\t\t\t       sizeof(struct udphdr),\n\t\t\t\t\t\t       msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udp_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t   UDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\tif (!peeked)\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_port = udp_hdr(skb)->source;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tUDP_INC_STATS_USER(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\n\t\tUDP_INC_STATS_USER(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t}\n\tunlock_sock_fast(sk, slow);\n\tif (noblock)\n\t\treturn -EAGAIN;\n\t/* starting over for a new packet */\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}",
        "bug_line_number": [
            11,
            12,
            13,
            14,
            15,
            66,
            67
        ],
        "vul": 1,
        "id": 30
    },
    {
        "code": "static int rawv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)msg->msg_name;\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\tif (addr_len)\n\t\t*addr_len=sizeof(*sin6);\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\tif (skb_csum_unnecessary(skb)) {\n\t\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\t} else if (msg->msg_flags&MSG_TRUNC) {\n\t\tif (__skb_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\t} else {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, 0, msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (err)\n\t\tgoto out_free;\n\t/* Copy the address. */\n\tif (sin6) {\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = 0;\n\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\tsin6->sin6_flowinfo = 0;\n\t\tsin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t\t  IP6CB(skb)->iif);\n\t}\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = skb->len;\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\ncsum_copy_err:\n\tskb_kill_datagram(sk, skb, flags);\n\t/* Error for blocking case is chosen to masquerade\n\t   as some normal condition.\n\t */\n\terr = (flags&MSG_DONTWAIT) ? -EAGAIN : -EHOSTUNREACH;\n\tgoto out;\n}",
        "bug_line_number": [
            11,
            12,
            45,
            46
        ],
        "vul": 1,
        "id": 32
    },
    {
        "code": "int udpv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint is_udp4;\n\tbool slow;\n\tif (addr_len)\n\t\t*addr_len = sizeof(struct sockaddr_in6);\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, sizeof(struct udphdr), msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udpv6_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tif (is_udp4)\n\t\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t   UDP_MIB_INERRORS,\n\t\t\t\t\t\t   is_udplite);\n\t\t\telse\n\t\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t    UDP_MIB_INERRORS,\n\t\t\t\t\t\t    is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\tif (!peeked) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t}\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in6 *sin6;\n\t\tsin6 = (struct sockaddr_in6 *) msg->msg_name;\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\t\tif (is_udp4) {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\t\tsin6->sin6_scope_id = 0;\n\t\t} else {\n\t\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tsin6->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t    IP6CB(skb)->iif);\n\t\t}\n\t}\n\tif (is_udp4) {\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\t}\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;",
        "bug_line_number": [
            13,
            14,
            88,
            89
        ],
        "vul": 1,
        "id": 34
    },
    {
        "code": "static int recv_msg(struct kiocb *iocb, struct socket *sock,\n\t\t    struct msghdr *m, size_t buf_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tipc_port *tport = tipc_sk_port(sk);\n\tstruct sk_buff *buf;\n\tstruct tipc_msg *msg;\n\tlong timeout;\n\tunsigned int sz;\n\tu32 err;\n\tint res;\n\t/* Catch invalid receive requests */\n\tif (unlikely(!buf_len))\n\t\treturn -EINVAL;\n\tlock_sock(sk);\n\tif (unlikely(sock->state == SS_UNCONNECTED)) {\n\t\tres = -ENOTCONN;\n\t\tgoto exit;\n\t}\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\nrestart:\n\t/* Look for a message in receive queue; wait if necessary */\n\twhile (skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tif (sock->state == SS_DISCONNECTING) {\n\t\t\tres = -ENOTCONN;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (timeout <= 0L) {\n\t\t\tres = timeout ? timeout : -EWOULDBLOCK;\n\t\t\tgoto exit;\n\t\t}\n\t\trelease_sock(sk);\n\t\ttimeout = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t\t\t   tipc_rx_ready(sock),\n\t\t\t\t\t\t\t   timeout);\n\t\tlock_sock(sk);\n\t}\n\t/* Look at first message in receive queue */\n\tbuf = skb_peek(&sk->sk_receive_queue);\n\tmsg = buf_msg(buf);\n\tsz = msg_data_sz(msg);\n\terr = msg_errcode(msg);\n\t/* Discard an empty non-errored message & try again */\n\tif ((!sz) && (!err)) {\n\t\tadvance_rx_queue(sk);\n\t\tgoto restart;\n\t}\n\t/* Capture sender's address (optional) */\n\tset_orig_addr(m, msg);\n\t/* Capture ancillary data (optional) */\n\tres = anc_data_recv(m, msg, tport);\n\tif (res)\n\t\tgoto exit;\n\t/* Capture message data (if valid) & compute return value (always) */\n\tif (!err) {\n\t\tif (unlikely(buf_len < sz)) {\n\t\t\tsz = buf_len;\n\t\t\tm->msg_flags |= MSG_TRUNC;\n\t\t}\n\t\tres = skb_copy_datagram_iovec(buf, msg_hdr_sz(msg),\n\t\t\t\t\t      m->msg_iov, sz);\n\t\tif (res)\n\t\t\tgoto exit;\n\t\tres = sz;\n\t} else {\n\t\tif ((sock->state == SS_READY) ||\n\t\t    ((err == TIPC_CONN_SHUTDOWN) || m->msg_control))\n\t\t\tres = 0;\n\t\telse\n\t\t\tres = -ECONNRESET;\n\t}\n\t/* Consume received message (optional) */\n\tif (likely(!(flags & MSG_PEEK))) {\n\t\tif ((sock->state != SS_READY) &&\n\t\t    (++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))\n\t\t\ttipc_acknowledge(tport->ref, tport->conn_unacked);\n\t\tadvance_rx_queue(sk);\n\t}\nexit:\n\trelease_sock(sk);\n\treturn res;\n}",
        "bug_line_number": [
            18,
            19
        ],
        "vul": 1,
        "id": 36
    },
    {
        "code": "static int tty_open(struct inode *inode, struct file *filp)\n{\n\tstruct tty_struct *tty = NULL;\n\tint noctty, retval;\n\tstruct tty_driver *driver;\n\tint index;\n\tdev_t device = inode->i_rdev;\n\tunsigned saved_flags = filp->f_flags;\n\tnonseekable_open(inode, filp);\nretry_open:\n\tnoctty = filp->f_flags & O_NOCTTY;\n\tindex  = -1;\n\tretval = 0;\n\tmutex_lock(&tty_mutex);\n\ttty_lock();\n\tif (device == MKDEV(TTYAUX_MAJOR, 0)) {\n\t\ttty = get_current_tty();\n\t\tif (!tty) {\n\t\t\ttty_unlock();\n\t\t\tmutex_unlock(&tty_mutex);\n\t\t\treturn -ENXIO;\n\t\t}\n\t\tdriver = tty_driver_kref_get(tty->driver);\n\t\tindex = tty->index;\n\t\tfilp->f_flags |= O_NONBLOCK; /* Don't let /dev/tty block */\n\t\t/* noctty = 1; */\n\t\t/* FIXME: Should we take a driver reference ? */\n\t\ttty_kref_put(tty);\n\t\tgoto got_driver;\n\t}\n#ifdef CONFIG_VT\n\tif (device == MKDEV(TTY_MAJOR, 0)) {\n\t\textern struct tty_driver *console_driver;\n\t\tdriver = tty_driver_kref_get(console_driver);\n\t\tindex = fg_console;\n\t\tnoctty = 1;\n\t\tgoto got_driver;\n\t}\n#endif\n\tif (device == MKDEV(TTYAUX_MAJOR, 1)) {\n\t\tstruct tty_driver *console_driver = console_device(&index);\n\t\tif (console_driver) {\n\t\t\tdriver = tty_driver_kref_get(console_driver);\n\t\t\tif (driver) {\n\t\t\t\t/* Don't let /dev/console block */\n\t\t\t\tfilp->f_flags |= O_NONBLOCK;\n\t\t\t\tnoctty = 1;\n\t\t\t\tgoto got_driver;\n\t\t\t}\n\t\t}\n\t\ttty_unlock();\n\t\tmutex_unlock(&tty_mutex);\n\t\treturn -ENODEV;\n\t}\n\tdriver = get_tty_driver(device, &index);\n\tif (!driver) {\n\t\ttty_unlock();\n\t\tmutex_unlock(&tty_mutex);\n\t\treturn -ENODEV;\n\t}\ngot_driver:\n\tif (!tty) {\n\t\t/* check whether we're reopening an existing tty */\n\t\ttty = tty_driver_lookup_tty(driver, inode, index);\n\t\tif (IS_ERR(tty)) {\n\t\t\ttty_unlock();\n\t\t\tmutex_unlock(&tty_mutex);\n\t\t\treturn PTR_ERR(tty);\n\t\t}\n\t}\n\tif (tty) {\n\t\tretval = tty_reopen(tty);\n\t\tif (retval)\n\t\t\ttty = ERR_PTR(retval);\n\t} else\n\t\ttty = tty_init_dev(driver, index, 0);\n\tmutex_unlock(&tty_mutex);\n\ttty_driver_kref_put(driver);\n\tif (IS_ERR(tty)) {\n\t\ttty_unlock();\n\t\treturn PTR_ERR(tty);\n\t}\n\tretval = tty_add_file(tty, filp);\n\tif (retval) {\n\t\ttty_unlock();\n\t\ttty_release(inode, filp);\n\t\treturn retval;\n\t}\n\tcheck_tty_count(tty, \"tty_open\");\n\tif (tty->driver->type == TTY_DRIVER_TYPE_PTY &&\n\t    tty->driver->subtype == PTY_TYPE_MASTER)\n\t\tnoctty = 1;\n#ifdef TTY_DEBUG_HANGUP\n\tprintk(KERN_DEBUG \"opening %s...\", tty->name);\n#endif\n\tif (tty->ops->open)\n\t\tretval = tty->ops->open(tty, filp);\n\telse\n\t\tretval = -ENODEV;\n\tfilp->f_flags = saved_flags;\n\tif (!retval && test_bit(TTY_EXCLUSIVE, &tty->flags) &&\n\t\t\t\t\t\t!capable(CAP_SYS_ADMIN))\n\t\tretval = -EBUSY;\n\tif (retval) {\n#ifdef TTY_DEBUG_HANGUP\n\t\tprintk(KERN_DEBUG \"error %d in opening %s...\", retval,\n\t\t       tty->name);\n#endif",
        "bug_line_number": [
            66,
            67
        ],
        "vul": 1,
        "id": 38
    },
    {
        "code": "__visible __notrace_funcgraph struct task_struct *\n__switch_to(struct task_struct *prev_p, struct task_struct *next_p)\n{\n\tstruct thread_struct *prev = &prev_p->thread;\n\tstruct thread_struct *next = &next_p->thread;\n\tint cpu = smp_processor_id();\n\tstruct tss_struct *tss = &per_cpu(init_tss, cpu);\n\tunsigned fsindex, gsindex;\n\tfpu_switch_t fpu;\n\tfpu = switch_fpu_prepare(prev_p, next_p, cpu);\n\t/*\n\t * Reload esp0, LDT and the page table pointer:\n\t */\n\tload_sp0(tss, next);\n\t/*\n\t * Switch DS and ES.\n\t * This won't pick up thread selector changes, but I guess that is ok.\n\t */\n\tsavesegment(es, prev->es);\n\tif (unlikely(next->es | prev->es))\n\t\tloadsegment(es, next->es);\n\tsavesegment(ds, prev->ds);\n\tif (unlikely(next->ds | prev->ds))\n\t\tloadsegment(ds, next->ds);\n\t/* We must save %fs and %gs before load_TLS() because\n\t * %fs and %gs may be cleared by load_TLS().\n\t *\n\t * (e.g. xen_load_tls())\n\t */\n\tsavesegment(fs, fsindex);\n\tsavesegment(gs, gsindex);\n\tload_TLS(next, cpu);\n\t/*\n\t * Leave lazy mode, flushing any hypercalls made here.\n\t * This must be done before restoring TLS segments so\n\t * the GDT and LDT are properly updated, and must be\n\t * done before math_state_restore, so the TS bit is up\n\t * to date.\n\t */\n\tarch_end_context_switch(next_p);\n\t/*\n\t * Switch FS and GS.\n\t *\n\t * Segment register != 0 always requires a reload.  Also\n\t * reload when it has changed.  When prev process used 64bit\n\t * base always reload to avoid an information leak.\n\t */\n\tif (unlikely(fsindex | next->fsindex | prev->fs)) {\n\t\tloadsegment(fs, next->fsindex);\n\t\t/*\n\t\t * Check if the user used a selector != 0; if yes\n\t\t *  clear 64bit base, since overloaded base is always\n\t\t *  mapped to the Null selector\n\t\t */\n\t\tif (fsindex)\n\t\t\tprev->fs = 0;\n\t}\n\t/* when next process has a 64bit base use it */\n\tif (next->fs)\n\t\twrmsrl(MSR_FS_BASE, next->fs);\n\tprev->fsindex = fsindex;\n\tif (unlikely(gsindex | next->gsindex | prev->gs)) {\n\t\tload_gs_index(next->gsindex);\n\t\tif (gsindex)\n\t\t\tprev->gs = 0;\n\t}\n\tif (next->gs)\n\t\twrmsrl(MSR_KERNEL_GS_BASE, next->gs);\n\tprev->gsindex = gsindex;\n\tswitch_fpu_finish(next_p, fpu);\n\t/*\n\t * Switch the PDA and FPU contexts.\n\t */\n\tprev->usersp = this_cpu_read(old_rsp);\n\tthis_cpu_write(old_rsp, next->usersp);\n\tthis_cpu_write(current_task, next_p);\n\t/*\n\t * If it were not for PREEMPT_ACTIVE we could guarantee that the\n\t * preempt_count of all tasks was equal here and this would not be\n\t * needed.\n\t */\n\ttask_thread_info(prev_p)->saved_preempt_count = this_cpu_read(__preempt_count);\n\tthis_cpu_write(__preempt_count, task_thread_info(next_p)->saved_preempt_count);\n\tthis_cpu_write(kernel_stack,\n\t\t  (unsigned long)task_stack_page(next_p) +\n\t\t  THREAD_SIZE - KERNEL_STACK_OFFSET);\n\t/*\n\t * Now maybe reload the debug registers and handle I/O bitmaps\n\t */\n\tif (unlikely(task_thread_info(next_p)->flags & _TIF_WORK_CTXSW_NEXT ||\n\t\t     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))\n\t\t__switch_to_xtra(prev_p, next_p, tss);\n\treturn prev_p;\n}",
        "bug_line_number": [
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            43,
            44,
            45,
            46,
            50,
            51,
            52,
            53,
            57,
            62,
            63
        ],
        "vul": 1,
        "id": 40
    },
    {
        "code": "static void toggle_count_cache_flush(bool enable)\n{\n\tif (!enable || !security_ftr_enabled(SEC_FTR_FLUSH_COUNT_CACHE)) {\n\t\tpatch_instruction_site(&patch__call_flush_count_cache, PPC_INST_NOP);\n\t\tcount_cache_flush_type = COUNT_CACHE_FLUSH_NONE;\n\t\tpr_info(\"count-cache-flush: software flush disabled.\\n\");\n\t\treturn;\n\t}\n\tpatch_branch_site(&patch__call_flush_count_cache,\n\t\t\t  (u64)&flush_count_cache, BRANCH_SET_LINK);\n\tif (!security_ftr_enabled(SEC_FTR_BCCTR_FLUSH_ASSIST)) {\n\t\tcount_cache_flush_type = COUNT_CACHE_FLUSH_SW;\n\t\tpr_info(\"count-cache-flush: full software flush sequence enabled.\\n\");\n\t\treturn;\n\t}\n\tpatch_instruction_site(&patch__flush_count_cache_return, PPC_INST_BLR);\n\tcount_cache_flush_type = COUNT_CACHE_FLUSH_HW;\n\tpr_info(\"count-cache-flush: hardware assisted flush sequence enabled\\n\");\n}",
        "bug_line_number": [
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10
        ],
        "vul": 1,
        "id": 42
    },
    {
        "code": "void __detach_mounts(struct dentry *dentry)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *mnt;\n\tnamespace_lock();\n\tmp = lookup_mountpoint(dentry);\n\tif (IS_ERR_OR_NULL(mp))\n\t\tgoto out_unlock;\n\tlock_mount_hash();\n\twhile (!hlist_empty(&mp->m_list)) {\n\t\tmnt = hlist_entry(mp->m_list.first, struct mount, mnt_mp_list);\n\t\tif (mnt->mnt.mnt_flags & MNT_UMOUNT) {\n\t\t\tstruct mount *p, *tmp;\n\t\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\t\thlist_add_head(&p->mnt_umount.s_list, &unmounted);\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\telse umount_tree(mnt, 0);\n\t}\n\tunlock_mount_hash();\n\tput_mountpoint(mp);\nout_unlock:\n\tnamespace_unlock();\n}",
        "bug_line_number": [
            18,
            19
        ],
        "vul": 1,
        "id": 44
    },
    {
        "code": "static int talk_to_netback(struct xenbus_device *dev,\n\t\t\t   struct netfront_info *info)\n{\n\tconst char *message;\n\tstruct xenbus_transaction xbt;\n\tint err;\n\tunsigned int feature_split_evtchn;\n\tunsigned int i = 0;\n\tunsigned int max_queues = 0;\n\tstruct netfront_queue *queue = NULL;\n\tunsigned int num_queues = 1;\n\tu8 addr[ETH_ALEN];\n\tinfo->netdev->irq = 0;\n\t/* Check if backend supports multiple queues */\n\tmax_queues = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t  \"multi-queue-max-queues\", 1);\n\tnum_queues = min(max_queues, xennet_max_queues);\n\t/* Check feature-split-event-channels */\n\tfeature_split_evtchn = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\"feature-split-event-channels\", 0);\n\t/* Read mac addr. */\n\terr = xen_net_read_mac(dev, addr);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"parsing %s/mac\", dev->nodename);\n\t\tgoto out_unlocked;\n\t}\n\teth_hw_addr_set(info->netdev, addr);\n\tinfo->netback_has_xdp_headroom = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\t\t      \"feature-xdp-headroom\", 0);\n\tif (info->netback_has_xdp_headroom) {\n\t\t/* set the current xen-netfront xdp state */\n\t\terr = talk_to_netback_xdp(info, info->netfront_xdp_enabled ?\n\t\t\t\t\t  NETBACK_XDP_HEADROOM_ENABLE :\n\t\t\t\t\t  NETBACK_XDP_HEADROOM_DISABLE);\n\t\tif (err)\n\t\t\tgoto out_unlocked;\n\t}\n\trtnl_lock();\n\tif (info->queues)\n\t\txennet_destroy_queues(info);\n\t/* For the case of a reconnect reset the \"broken\" indicator. */\n\tinfo->broken = false;\n\terr = xennet_create_queues(info, &num_queues);\n\tif (err < 0) {\n\t\txenbus_dev_fatal(dev, err, \"creating queues\");\n\t\tkfree(info->queues);\n\t\tinfo->queues = NULL;\n\t\tgoto out;\n\t}\n\trtnl_unlock();\n\t/* Create shared ring, alloc event channel -- for each queue */\n\tfor (i = 0; i < num_queues; ++i) {\n\t\tqueue = &info->queues[i];\n\t\terr = setup_netfront(dev, queue, feature_split_evtchn);\n\t\tif (err)\n\t\t\tgoto destroy_ring;\n\t}\nagain:\n\terr = xenbus_transaction_start(&xbt);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"starting transaction\");\n\t\tgoto destroy_ring;\n\t}\n\tif (xenbus_exists(XBT_NIL,\n\t\t\t  info->xbdev->otherend, \"multi-queue-max-queues\")) {\n\t\t/* Write the number of queues */\n\t\terr = xenbus_printf(xbt, dev->nodename,\n\t\t\t\t    \"multi-queue-num-queues\", \"%u\", num_queues);\n\t\tif (err) {\n\t\t\tmessage = \"writing multi-queue-num-queues\";\n\t\t\tgoto abort_transaction_no_dev_fatal;\n\t\t}\n\t}\n\tif (num_queues == 1) {\n\t\terr = write_queue_xenstore_keys(&info->queues[0], &xbt, 0); /* flat */\n\t\tif (err)\n\t\t\tgoto abort_transaction_no_dev_fatal;\n\t} else {\n\t\t/* Write the keys for each queue */\n\t\tfor (i = 0; i < num_queues; ++i) {\n\t\t\tqueue = &info->queues[i];\n\t\t\terr = write_queue_xenstore_keys(queue, &xbt, 1); /* hierarchical */\n\t\t\tif (err)\n\t\t\t\tgoto abort_transaction_no_dev_fatal;\n\t\t}\n\t}\n\t/* The remaining keys are not queue-specific */\n\terr = xenbus_printf(xbt, dev->nodename, \"request-rx-copy\", \"%u\",\n\t\t\t    1);\n\tif (err) {\n\t\tmessage = \"writing request-rx-copy\";\n\t\tgoto abort_transaction;\n\t}\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-rx-notify\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-rx-notify\";\n\t\tgoto abort_transaction;\n\t}\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-sg\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-sg\";\n\t\tgoto abort_transaction;\n\t}\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-gso-tcpv4\", \"%d\", 1);",
        "bug_line_number": [
            12,
            13
        ],
        "vul": 1,
        "id": 46
    },
    {
        "code": "static void binder_transaction_buffer_release(struct binder_proc *proc,\n\t\t\t\t\t      struct binder_buffer *buffer,\n\t\t\t\t\t      binder_size_t *failed_at)\n{\n\tbinder_size_t *offp, *off_start, *off_end;\n\tint debug_id = buffer->debug_id;\n\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t     \"%d buffer release %d, size %zd-%zd, failed at %p\\n\",\n\t\t     proc->pid, buffer->debug_id,\n\t\t     buffer->data_size, buffer->offsets_size, failed_at);\n\tif (buffer->target_node)\n\t\tbinder_dec_node(buffer->target_node, 1, 0);\n\toff_start = (binder_size_t *)(buffer->data +\n\t\t\t\t      ALIGN(buffer->data_size, sizeof(void *)));\n\tif (failed_at)\n\t\toff_end = failed_at;\n\telse\n\t\toff_end = (void *)off_start + buffer->offsets_size;\n\tfor (offp = off_start; offp < off_end; offp++) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size = binder_validate_object(buffer, *offp);\n\t\tif (object_size == 0) {\n\t\t\tpr_err(\"transaction release %d bad object at offset %lld, size %zd\\n\",\n\t\t\t       debug_id, (u64)*offp, buffer->data_size);\n\t\t\tcontinue;\n\t\t}\n\t\thdr = (struct binder_object_header *)(buffer->data + *offp);\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_node *node;\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tnode = binder_get_node(proc, fp->binder);\n\t\t\tif (node == NULL) {\n\t\t\t\tpr_err(\"transaction release %d bad node %016llx\\n\",\n\t\t\t\t       debug_id, (u64)fp->binder);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        node %d u%016llx\\n\",\n\t\t\t\t     node->debug_id, (u64)node->ptr);\n\t\t\tbinder_dec_node(node, hdr->type == BINDER_TYPE_BINDER,\n\t\t\t\t\t0);\n\t\t\tbinder_put_node(node);\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_ref_data rdata;\n\t\t\tint ret;\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_dec_ref_for_handle(proc, fp->handle,\n\t\t\t\thdr->type == BINDER_TYPE_HANDLE, &rdata);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"transaction release %d bad handle %d, ret = %d\\n\",\n\t\t\t\t debug_id, fp->handle, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        ref %d desc %d\\n\",\n\t\t\t\t     rdata.debug_id, rdata.desc);\n\t\t} break;\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        fd %d\\n\", fp->fd);\n\t\t\tif (failed_at)\n\t\t\t\ttask_close_fd(proc, fp->fd);\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR:\n\t\t\t/*\n\t\t\t * Nothing to do here, this will get cleaned up when the\n\t\t\t * transaction buffer gets freed\n\t\t\t */\n\t\t\tbreak;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_fd_array_object *fda;\n\t\t\tstruct binder_buffer_object *parent;\n\t\t\tuintptr_t parent_buffer;\n\t\t\tu32 *fd_array;\n\t\t\tsize_t fd_index;\n\t\t\tbinder_size_t fd_buf_size;\n\t\t\tfda = to_binder_fd_array_object(hdr);\n\t\t\tparent = binder_validate_ptr(buffer, fda->parent,\n\t\t\t\t\t\t     off_start,\n\t\t\t\t\t\t     offp - off_start);\n\t\t\tif (!parent) {\n\t\t\t\tpr_err(\"transaction release %d bad parent offset\\n\",\n\t\t\t\t       debug_id);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Since the parent was already fixed up, convert it\n\t\t\t * back to kernel address space to access it\n\t\t\t */\n\t\t\tparent_buffer = parent->buffer -\n\t\t\t\tbinder_alloc_get_user_buffer_offset(\n\t\t\t\t\t\t&proc->alloc);\n\t\t\tfd_buf_size = sizeof(u32) * fda->num_fds;\n\t\t\tif (fda->num_fds >= SIZE_MAX / sizeof(u32)) {\n\t\t\t\tpr_err(\"transaction release %d invalid number of fds (%lld)\\n\",\n\t\t\t\t       debug_id, (u64)fda->num_fds);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (fd_buf_size > parent->length ||\n\t\t\t    fda->parent_offset > parent->length - fd_buf_size) {\n\t\t\t\t/* No space for all file descriptors here. */\n\t\t\t\tpr_err(\"transaction release %d not enough space for %lld fds in buffer\\n\",",
        "bug_line_number": [
            7,
            8
        ],
        "vul": 1,
        "id": 48
    },
    {
        "code": "static struct sock *pep_sock_accept(struct sock *sk, int flags, int *errp,\n\t\t\t\t    bool kern)\n{\n\tstruct pep_sock *pn = pep_sk(sk), *newpn;\n\tstruct sock *newsk = NULL;\n\tstruct sk_buff *skb;\n\tstruct pnpipehdr *hdr;\n\tstruct sockaddr_pn dst, src;\n\tint err;\n\tu16 peer_type;\n\tu8 pipe_handle, enabled, n_sb;\n\tu8 aligned = 0;\n\tskb = skb_recv_datagram(sk, 0, flags & O_NONBLOCK, errp);\n\tif (!skb)\n\t\treturn NULL;\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_LISTEN) {\n\t\terr = -EINVAL;\n\t\tgoto drop;\n\t}\n\tsk_acceptq_removed(sk);\n\terr = -EPROTO;\n\tif (!pskb_may_pull(skb, sizeof(*hdr) + 4))\n\t\tgoto drop;\n\thdr = pnp_hdr(skb);\n\tpipe_handle = hdr->pipe_handle;\n\tswitch (hdr->state_after_connect) {\n\tcase PN_PIPE_DISABLE:\n\t\tenabled = 0;\n\t\tbreak;\n\tcase PN_PIPE_ENABLE:\n\t\tenabled = 1;\n\t\tbreak;\n\tdefault:\n\t\tpep_reject_conn(sk, skb, PN_PIPE_ERR_INVALID_PARAM,\n\t\t\t\tGFP_KERNEL);\n\t\tgoto drop;\n\t}\n\tpeer_type = hdr->other_pep_type << 8;\n\t/* Parse sub-blocks (options) */\n\tn_sb = hdr->data[3];\n\twhile (n_sb > 0) {\n\t\tu8 type, buf[1], len = sizeof(buf);\n\t\tconst u8 *data = pep_get_sb(skb, &type, &len, buf);\n\t\tif (data == NULL)\n\t\t\tgoto drop;\n\t\tswitch (type) {\n\t\tcase PN_PIPE_SB_CONNECT_REQ_PEP_SUB_TYPE:\n\t\t\tif (len < 1)\n\t\t\t\tgoto drop;\n\t\t\tpeer_type = (peer_type & 0xff00) | data[0];\n\t\t\tbreak;\n\t\tcase PN_PIPE_SB_ALIGNED_DATA:\n\t\t\taligned = data[0] != 0;\n\t\t\tbreak;\n\t\t}\n\t\tn_sb--;\n\t}\n\t/* Check for duplicate pipe handle */\n\tnewsk = pep_find_pipe(&pn->hlist, &dst, pipe_handle);\n\tif (unlikely(newsk)) {\n\t\t__sock_put(newsk);\n\t\tnewsk = NULL;\n\t\tpep_reject_conn(sk, skb, PN_PIPE_ERR_PEP_IN_USE, GFP_KERNEL);\n\t\tgoto drop;\n\t}\n\t/* Create a new to-be-accepted sock */\n\tnewsk = sk_alloc(sock_net(sk), PF_PHONET, GFP_KERNEL, sk->sk_prot,\n\t\t\t kern);\n\tif (!newsk) {\n\t\tpep_reject_conn(sk, skb, PN_PIPE_ERR_OVERLOAD, GFP_KERNEL);\n\t\terr = -ENOBUFS;\n\t\tgoto drop;\n\t}\n\tsock_init_data(NULL, newsk);\n\tnewsk->sk_state = TCP_SYN_RECV;\n\tnewsk->sk_backlog_rcv = pipe_do_rcv;\n\tnewsk->sk_protocol = sk->sk_protocol;\n\tnewsk->sk_destruct = pipe_destruct;\n\tnewpn = pep_sk(newsk);\n\tpn_skb_get_dst_sockaddr(skb, &dst);\n\tpn_skb_get_src_sockaddr(skb, &src);\n\tnewpn->pn_sk.sobject = pn_sockaddr_get_object(&dst);\n\tnewpn->pn_sk.dobject = pn_sockaddr_get_object(&src);\n\tnewpn->pn_sk.resource = pn_sockaddr_get_resource(&dst);\n\tsock_hold(sk);\n\tnewpn->listener = sk;\n\tskb_queue_head_init(&newpn->ctrlreq_queue);\n\tnewpn->pipe_handle = pipe_handle;\n\tatomic_set(&newpn->tx_credits, 0);\n\tnewpn->ifindex = 0;\n\tnewpn->peer_type = peer_type;\n\tnewpn->rx_credits = 0;\n\tnewpn->rx_fc = newpn->tx_fc = PN_LEGACY_FLOW_CONTROL;\n\tnewpn->init_enable = enabled;\n\tnewpn->aligned = aligned;\n\terr = pep_accept_conn(newsk, skb);\n\tif (err) {\n\t\tsock_put(newsk);\n\t\tnewsk = NULL;\n\t\tgoto drop;\n\t}\n\tsk_add_node(newsk, &pn->hlist);\ndrop:\n\trelease_sock(sk);\n\tkfree_skb(skb);\n\t*errp = err;\n\treturn newsk;\n}",
        "bug_line_number": [
            97,
            98
        ],
        "vul": 1,
        "id": 50
    },
    {
        "code": "static int hidp_setup_hid(struct hidp_session *session,\n\t\t\t\tstruct hidp_connadd_req *req)\n{\n\tstruct hid_device *hid;\n\tint err;\n\tsession->rd_data = kzalloc(req->rd_size, GFP_KERNEL);\n\tif (!session->rd_data)\n\t\treturn -ENOMEM;\n\tif (copy_from_user(session->rd_data, req->rd_data, req->rd_size)) {\n\t\terr = -EFAULT;\n\t\tgoto fault;\n\t}\n\tsession->rd_size = req->rd_size;\n\thid = hid_allocate_device();\n\tif (IS_ERR(hid)) {\n\t\terr = PTR_ERR(hid);\n\t\tgoto fault;\n\t}\n\tsession->hid = hid;\n\thid->driver_data = session;\n\thid->bus     = BUS_BLUETOOTH;\n\thid->vendor  = req->vendor;\n\thid->product = req->product;\n\thid->version = req->version;\n\thid->country = req->country;\n\tstrncpy(hid->name, req->name, 128);\n\tsnprintf(hid->phys, sizeof(hid->phys), \"%pMR\",\n\t\t &bt_sk(session->ctrl_sock->sk)->src);\n\tsnprintf(hid->uniq, sizeof(hid->uniq), \"%pMR\",\n\t\t &bt_sk(session->ctrl_sock->sk)->dst);\n\thid->dev.parent = &session->conn->dev;\n\thid->ll_driver = &hidp_hid_driver;\n\thid->hid_get_raw_report = hidp_get_raw_report;\n\thid->hid_output_raw_report = hidp_output_raw_report;\n\t/* True if device is blacklisted in drivers/hid/hid-core.c */\n\tif (hid_ignore(hid)) {\n\t\thid_destroy_device(session->hid);\n\t\tsession->hid = NULL;\n\t\treturn -ENODEV;\n\t}\n\treturn 0;\nfault:\n\tkfree(session->rd_data);\n\tsession->rd_data = NULL;\n\treturn err;\n}",
        "bug_line_number": [
            25,
            26
        ],
        "vul": 1,
        "id": 52
    },
    {
        "code": "\t\t\t    info->full_pathname,\n\t\t\t    acpi_ut_get_type_name(info->node->type)));\n\t\tstatus = AE_TYPE;\n\t\tgoto cleanup;\n\tcase ACPI_TYPE_METHOD:\n\t\t/*\n\t\t * 2) Object is a control method - execute it\n\t\t */\n\t\t/* Verify that there is a method object associated with this node */\n\t\tif (!info->obj_desc) {\n\t\t\tACPI_ERROR((AE_INFO,\n\t\t\t\t    \"%s: Method has no attached sub-object\",\n\t\t\t\t    info->full_pathname));\n\t\t\tstatus = AE_NULL_OBJECT;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tACPI_DEBUG_PRINT((ACPI_DB_EXEC,\n\t\t\t\t  \"**** Execute method [%s] at AML address %p length %X\\n\",\n\t\t\t\t  info->full_pathname,\n\t\t\t\t  info->obj_desc->method.aml_start + 1,\n\t\t\t\t  info->obj_desc->method.aml_length - 1));\n\t\t/*\n\t\t * Any namespace deletion must acquire both the namespace and\n\t\t * interpreter locks to ensure that no thread is using the portion of\n\t\t * the namespace that is being deleted.\n\t\t *\n\t\t * Execute the method via the interpreter. The interpreter is locked\n\t\t * here before calling into the AML parser\n\t\t */\n\t\tacpi_ex_enter_interpreter();\n\t\tstatus = acpi_ps_execute_method(info);\n\t\tacpi_ex_exit_interpreter();\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * 3) All other non-method objects -- get the current object value\n\t\t */\n\t\t/*\n\t\t * Some objects require additional resolution steps (e.g., the Node\n\t\t * may be a field that must be read, etc.) -- we can't just grab\n\t\t * the object out of the node.\n\t\t *\n\t\t * Use resolve_node_to_value() to get the associated value.\n\t\t *\n\t\t * NOTE: we can get away with passing in NULL for a walk state because\n\t\t * the Node is guaranteed to not be a reference to either a method\n\t\t * local or a method argument (because this interface is never called\n\t\t * from a running method.)\n\t\t *\n\t\t * Even though we do not directly invoke the interpreter for object\n\t\t * resolution, we must lock it because we could access an op_region.\n\t\t * The op_region access code assumes that the interpreter is locked.\n\t\t */\n\t\tacpi_ex_enter_interpreter();\n\t\t/* TBD: resolve_node_to_value has a strange interface, fix */\n\t\tinfo->return_object =\n\t\t    ACPI_CAST_PTR(union acpi_operand_object, info->node);\n\t\tstatus =\n\t\t    acpi_ex_resolve_node_to_value(ACPI_CAST_INDIRECT_PTR\n\t\t\t\t\t\t  (struct acpi_namespace_node,\n\t\t\t\t\t\t   &info->return_object), NULL);\n\t\tacpi_ex_exit_interpreter();\n\t\tif (ACPI_FAILURE(status)) {\n\t\t\tinfo->return_object = NULL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tACPI_DEBUG_PRINT((ACPI_DB_NAMES, \"Returned object %p [%s]\\n\",\n\t\t\t\t  info->return_object,\n\t\t\t\t  acpi_ut_get_object_type_name(info->\n\t\t\t\t\t\t\t       return_object)));\n\t\tstatus = AE_CTRL_RETURN_VALUE;\t/* Always has a \"return value\" */\n\t\tbreak;\n\t}\n\t/*\n\t * For predefined names, check the return value against the ACPI\n\t * specification. Some incorrect return value types are repaired.\n\t */\n\t(void)acpi_ns_check_return_value(info->node, info, info->param_count,\n\t\t\t\t\t status, &info->return_object);\n\t/* Check if there is a return value that must be dealt with */\n\tif (status == AE_CTRL_RETURN_VALUE) {\n\t\t/* If caller does not want the return value, delete it */\n\t\tif (info->flags & ACPI_IGNORE_RETURN_VALUE) {\n\t\t\tacpi_ut_remove_reference(info->return_object);\n\t\t\tinfo->return_object = NULL;\n\t\t}\n\t\t/* Map AE_CTRL_RETURN_VALUE to AE_OK, we are done with it */\n\t\tstatus = AE_OK;\n\t}\n\tACPI_DEBUG_PRINT((ACPI_DB_NAMES,\n\t\t\t  \"*** Completed evaluation of object %s ***\\n\",\n\t\t\t  info->relative_pathname));\ncleanup:\n\t/*\n\t * Namespace was unlocked by the handling acpi_ns* function, so we\n\t * just free the pathname and return\n\t */",
        "bug_line_number": [
            87,
            88
        ],
        "vul": 1,
        "id": 54
    },
    {
        "code": "static int get_bitmap_file(struct mddev *mddev, void __user * arg)\n{\n\tmdu_bitmap_file_t *file = NULL; /* too big for stack allocation */\n\tchar *ptr;\n\tint err;\n\tfile = kmalloc(sizeof(*file), GFP_NOIO);\n\tif (!file)\n\t\treturn -ENOMEM;\n\terr = 0;\n\tspin_lock(&mddev->lock);\n\t/* bitmap disabled, zero the first byte and copy out */\n\tif (!mddev->bitmap_info.file)\n\t\tfile->pathname[0] = '\\0';\n\telse if ((ptr = file_path(mddev->bitmap_info.file,\n\t\t\t       file->pathname, sizeof(file->pathname))),\n\t\t IS_ERR(ptr))\n\t\terr = PTR_ERR(ptr);\n\telse\n\t\tmemmove(file->pathname, ptr,\n\t\t\tsizeof(file->pathname)-(ptr-file->pathname));\n\tspin_unlock(&mddev->lock);\n\tif (err == 0 &&\n\t    copy_to_user(arg, file, sizeof(*file)))\n\t\terr = -EFAULT;\n\tkfree(file);\n\treturn err;\n}",
        "bug_line_number": [
            5,
            6
        ],
        "vul": 1,
        "id": 56
    },
    {
        "code": "static inline ssize_t do_tty_write(\n\tssize_t (*write)(struct tty_struct *, struct file *, const unsigned char *, size_t),\n\tstruct tty_struct *tty,\n\tstruct file *file,\n\tconst char __user *buf,\n\tsize_t count)\n{\n\tssize_t ret, written = 0;\n\tunsigned int chunk;\n\tret = tty_write_lock(tty, file->f_flags & O_NDELAY);\n\tif (ret < 0)\n\t\treturn ret;\n\t/*\n\t * We chunk up writes into a temporary buffer. This\n\t * simplifies low-level drivers immensely, since they\n\t * don't have locking issues and user mode accesses.\n\t *\n\t * But if TTY_NO_WRITE_SPLIT is set, we should use a\n\t * big chunk-size..\n\t *\n\t * The default chunk-size is 2kB, because the NTTY\n\t * layer has problems with bigger chunks. It will\n\t * claim to be able to handle more characters than\n\t * it actually does.\n\t *\n\t * FIXME: This can probably go away now except that 64K chunks\n\t * are too likely to fail unless switched to vmalloc...\n\t */\n\tchunk = 2048;\n\tif (test_bit(TTY_NO_WRITE_SPLIT, &tty->flags))\n\t\tchunk = 65536;\n\tif (count < chunk)\n\t\tchunk = count;\n\t/* write_buf/write_cnt is protected by the atomic_write_lock mutex */\n\tif (tty->write_cnt < chunk) {\n\t\tunsigned char *buf_chunk;\n\t\tif (chunk < 1024)\n\t\t\tchunk = 1024;\n\t\tbuf_chunk = kmalloc(chunk, GFP_KERNEL);\n\t\tif (!buf_chunk) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tkfree(tty->write_buf);\n\t\ttty->write_cnt = chunk;\n\t\ttty->write_buf = buf_chunk;\n\t}\n\t/* Do the write .. */\n\tfor (;;) {\n\t\tsize_t size = count;\n\t\tif (size > chunk)\n\t\t\tsize = chunk;\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(tty->write_buf, buf, size))\n\t\t\tbreak;\n\t\tret = write(tty, file, tty->write_buf, size);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\t\twritten += ret;\n\t\tbuf += ret;\n\t\tcount -= ret;\n\t\tif (!count)\n\t\t\tbreak;\n\t\tret = -ERESTARTSYS;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tif (written) {\n\t\tstruct inode *inode = file->f_path.dentry->d_inode;\n\t\tinode->i_mtime = current_fs_time(inode->i_sb);\n\t\tret = written;\n\t}\nout:\n\ttty_write_unlock(tty);\n\treturn ret;\n}",
        "bug_line_number": [
            68,
            69,
            70,
            71,
            72
        ],
        "vul": 1,
        "id": 58
    },
    {
        "code": "static int raw_cmd_copyout(int cmd, void __user *param,\n\t\t\t\t  struct floppy_raw_cmd *ptr)\n{\n\tint ret;\n\twhile (ptr) {\n\t\tret = copy_to_user(param, ptr, sizeof(*ptr));\n\t\tif (ret)\n\t\t\treturn -EFAULT;\n\t\tparam += sizeof(struct floppy_raw_cmd);\n\t\tif ((ptr->flags & FD_RAW_READ) && ptr->buffer_length) {\n\t\t\tif (ptr->length >= 0 &&\n\t\t\t    ptr->length <= ptr->buffer_length) {\n\t\t\t\tlong length = ptr->buffer_length - ptr->length;\n\t\t\t\tret = fd_copyout(ptr->data, ptr->kernel_data,\n\t\t\t\t\t\t length);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tptr = ptr->next;\n\t}\n\treturn 0;\n}",
        "bug_line_number": [
            5,
            6
        ],
        "vul": 1,
        "id": 60
    },
    {
        "code": "unsigned paravirt_patch_jmp(void *insnbuf, const void *target,\n\t\t\t    unsigned long addr, unsigned len)\n{\n\tstruct branch *b = insnbuf;\n\tunsigned long delta = (unsigned long)target - (addr+5);\n\tif (len < 5)\n\t\treturn len;\t/* call too long for patch site */\n\tb->opcode = 0xe9;\t/* jmp */\n\tb->delta = delta;\n\treturn 5;\n}",
        "bug_line_number": [
            5,
            6,
            7
        ],
        "vul": 1,
        "id": 62
    },
    {
        "code": "\tbreak;\n\tcase IOCTL_BCM_GET_DEVICE_DRIVER_INFO: {\n\t\tstruct bcm_driver_info DevInfo;\n\t\tBCM_DEBUG_PRINT(Adapter, DBG_TYPE_OTHERS, OSAL_DBG, DBG_LVL_ALL, \"Called IOCTL_BCM_GET_DEVICE_DRIVER_INFO\\n\");\n\t\tDevInfo.MaxRDMBufferSize = BUFFER_4K;\n\t\tDevInfo.u32DSDStartOffset = EEPROM_CALPARAM_START;\n\t\tDevInfo.u32RxAlignmentCorrection = 0;\n\t\tDevInfo.u32NVMType = Adapter->eNVMType;\n\t\tDevInfo.u32InterfaceType = BCM_USB;\n\t\tif (copy_from_user(&IoBuffer, argp, sizeof(struct bcm_ioctl_buffer)))\n\t\t\treturn -EFAULT;\n\t\tif (IoBuffer.OutputLength < sizeof(DevInfo))\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(IoBuffer.OutputBuffer, &DevInfo, sizeof(DevInfo)))\n\t\t\treturn -EFAULT;\n\t}\n\tbreak;\n\tcase IOCTL_BCM_TIME_SINCE_NET_ENTRY: {\n\t\tstruct bcm_time_elapsed stTimeElapsedSinceNetEntry = {0};\n\t\tBCM_DEBUG_PRINT(Adapter, DBG_TYPE_OTHERS, OSAL_DBG, DBG_LVL_ALL, \"IOCTL_BCM_TIME_SINCE_NET_ENTRY called\");\n\t\tif (copy_from_user(&IoBuffer, argp, sizeof(struct bcm_ioctl_buffer)))\n\t\t\treturn -EFAULT;\n\t\tif (IoBuffer.OutputLength < sizeof(struct bcm_time_elapsed))\n\t\t\treturn -EINVAL;\n\t\tstTimeElapsedSinceNetEntry.ul64TimeElapsedSinceNetEntry = get_seconds() - Adapter->liTimeSinceLastNetEntry;\n\t\tif (copy_to_user(IoBuffer.OutputBuffer, &stTimeElapsedSinceNetEntry, sizeof(struct bcm_time_elapsed)))\n\t\t\treturn -EFAULT;\n\t}\n\tbreak;\n\tcase IOCTL_CLOSE_NOTIFICATION:\n\t\tBCM_DEBUG_PRINT(Adapter, DBG_TYPE_OTHERS, OSAL_DBG, DBG_LVL_ALL, \"IOCTL_CLOSE_NOTIFICATION\");\n\t\tbreak;\n\tdefault:\n\t\tpr_info(DRV_NAME \": unknown ioctl cmd=%#x\\n\", cmd);\n\t\tStatus = STATUS_FAILURE;\n\t\tbreak;\n\t}\n\treturn Status;\n}",
        "bug_line_number": [
            3,
            4
        ],
        "vul": 1,
        "id": 64
    },
    {
        "code": "static int nr_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t      struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_ax25 *sax = (struct sockaddr_ax25 *)msg->msg_name;\n\tsize_t copied;\n\tstruct sk_buff *skb;\n\tint er;\n\t/*\n\t * This works for seqpacket too. The receiver has ordered the queue for\n\t * us! We do one quick check first though\n\t */\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\trelease_sock(sk);\n\t\treturn -ENOTCONN;\n\t}\n\t/* Now we can treat all alike */\n\tif ((skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT, flags & MSG_DONTWAIT, &er)) == NULL) {\n\t\trelease_sock(sk);\n\t\treturn er;\n\t}\n\tskb_reset_transport_header(skb);\n\tcopied     = skb->len;\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\ter = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (er < 0) {\n\t\tskb_free_datagram(sk, skb);\n\t\trelease_sock(sk);\n\t\treturn er;\n\t}\n\tif (sax != NULL) {\n\t\tsax->sax25_family = AF_NETROM;\n\t\tskb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,\n\t\t\t      AX25_ADDR_LEN);\n\t}\n\tmsg->msg_namelen = sizeof(*sax);\n\tskb_free_datagram(sk, skb);\n\trelease_sock(sk);\n\treturn copied;\n}",
        "bug_line_number": [
            34,
            35
        ],
        "vul": 1,
        "id": 66
    },
    {
        "code": "static void __net_random_once_deferred(struct work_struct *w)\n{\n\tstruct __net_random_once_work *work =\n\t\tcontainer_of(w, struct __net_random_once_work, work);\n\tif (!static_key_enabled(work->key))\n\t\tstatic_key_slow_inc(work->key);\n\tkfree(work);\n}",
        "bug_line_number": [
            4,
            5,
            6
        ],
        "vul": 1,
        "id": 68
    },
    {
        "code": "static int tipc_nl_compat_link_dump(struct tipc_nl_compat_msg *msg,\n\t\t\t\t    struct nlattr **attrs)\n{\n\tstruct nlattr *link[TIPC_NLA_LINK_MAX + 1];\n\tstruct tipc_link_info link_info;\n\tint err;\n\tif (!attrs[TIPC_NLA_LINK])\n\t\treturn -EINVAL;\n\terr = nla_parse_nested(link, TIPC_NLA_LINK_MAX, attrs[TIPC_NLA_LINK],\n\t\t\t       NULL);\n\tif (err)\n\t\treturn err;\n\tlink_info.dest = nla_get_flag(link[TIPC_NLA_LINK_DEST]);\n\tlink_info.up = htonl(nla_get_flag(link[TIPC_NLA_LINK_UP]));\n\tstrcpy(link_info.str, nla_data(link[TIPC_NLA_LINK_NAME]));\n\treturn tipc_add_tlv(msg->rep, TIPC_TLV_LINK_INFO,\n\t\t\t    &link_info, sizeof(link_info));\n}",
        "bug_line_number": [
            14,
            15
        ],
        "vul": 1,
        "id": 70
    },
    {
        "code": "static struct binder_ref *binder_get_ref(struct binder_proc *proc,\n\t\t\t\t\t u32 desc, bool need_strong_ref)\n{\n\tstruct rb_node *n = proc->refs_by_desc.rb_node;\n\tstruct binder_ref *ref;\n\twhile (n) {\n\t\tref = rb_entry(n, struct binder_ref, rb_node_desc);\n\t\tif (desc < ref->desc) {\n\t\t\tn = n->rb_left;\n\t\t} else if (desc > ref->desc) {\n\t\t\tn = n->rb_right;\n\t\t} else if (need_strong_ref && !ref->strong) {\n\t\t\tbinder_user_error(\"tried to use weak ref as strong ref\\n\");\n\t\t\treturn NULL;\n\t\t} else {\n\t\t\treturn ref;\n\t\t}\n\t}\n\treturn NULL;\n}",
        "bug_line_number": [
            7,
            8,
            9,
            10,
            11,
            12
        ],
        "vul": 1,
        "id": 72
    },
    {
        "code": "static void print_binder_ref(struct seq_file *m, struct binder_ref *ref)\n{\n\tseq_printf(m, \"  ref %d: desc %d %snode %d s %d w %d d %p\\n\",\n\t\t   ref->debug_id, ref->desc, ref->node->proc ? \"\" : \"dead \",\n\t\t   ref->node->debug_id, ref->strong, ref->weak, ref->death);\n}",
        "bug_line_number": [
            2,
            3,
            4,
            5
        ],
        "vul": 1,
        "id": 74
    },
    {
        "code": "static int pptp_connect(struct socket *sock, struct sockaddr *uservaddr,\n\tint sockaddr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppox *sp = (struct sockaddr_pppox *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct pptp_opt *opt = &po->proto.pptp;\n\tstruct rtable *rt;\n\tstruct flowi4 fl4;\n\tint error = 0;\n\tif (sp->sa_protocol != PX_PROTO_PPTP)\n\t\treturn -EINVAL;\n\tif (lookup_chan_dst(sp->sa_addr.pptp.call_id, sp->sa_addr.pptp.sin_addr.s_addr))\n\t\treturn -EALREADY;\n\tlock_sock(sk);\n\t/* Check for already bound sockets */\n\tif (sk->sk_state & PPPOX_CONNECTED) {\n\t\terror = -EBUSY;\n\t\tgoto end;\n\t}\n\t/* Check for already disconnected sockets, on attempts to disconnect */\n\tif (sk->sk_state & PPPOX_DEAD) {\n\t\terror = -EALREADY;\n\t\tgoto end;\n\t}\n\tif (!opt->src_addr.sin_addr.s_addr || !sp->sa_addr.pptp.sin_addr.s_addr) {\n\t\terror = -EINVAL;\n\t\tgoto end;\n\t}\n\tpo->chan.private = sk;\n\tpo->chan.ops = &pptp_chan_ops;\n\trt = ip_route_output_ports(sock_net(sk), &fl4, sk,\n\t\t\t\t   opt->dst_addr.sin_addr.s_addr,\n\t\t\t\t   opt->src_addr.sin_addr.s_addr,\n\t\t\t\t   0, 0,\n\t\t\t\t   IPPROTO_GRE, RT_CONN_FLAGS(sk), 0);\n\tif (IS_ERR(rt)) {\n\t\terror = -EHOSTUNREACH;\n\t\tgoto end;\n\t}\n\tsk_setup_caps(sk, &rt->dst);\n\tpo->chan.mtu = dst_mtu(&rt->dst);\n\tif (!po->chan.mtu)\n\t\tpo->chan.mtu = PPP_MRU;\n\tip_rt_put(rt);\n\tpo->chan.mtu -= PPTP_HEADER_OVERHEAD;\n\tpo->chan.hdrlen = 2 + sizeof(struct pptp_gre_header);\n\terror = ppp_register_channel(&po->chan);\n\tif (error) {\n\t\tpr_err(\"PPTP: failed to register PPP channel (%d)\\n\", error);\n\t\tgoto end;\n\t}\n\topt->dst_addr = sp->sa_addr.pptp;\n\tsk->sk_state = PPPOX_CONNECTED;\n end:\n\trelease_sock(sk);\n\treturn error;\n}",
        "bug_line_number": [
            9,
            10
        ],
        "vul": 1,
        "id": 76
    },
    {
        "code": "static ssize_t snd_timer_user_read(struct file *file, char __user *buffer,\n\t\t\t\t   size_t count, loff_t *offset)\n{\n\tstruct snd_timer_user *tu;\n\tlong result = 0, unit;\n\tint qhead;\n\tint err = 0;\n\ttu = file->private_data;\n\tunit = tu->tread ? sizeof(struct snd_timer_tread) : sizeof(struct snd_timer_read);\n\tmutex_lock(&tu->ioctl_lock);\n\tspin_lock_irq(&tu->qlock);\n\twhile ((long)count - result >= unit) {\n\t\twhile (!tu->qused) {\n\t\t\twait_queue_t wait;\n\t\t\tif ((file->f_flags & O_NONBLOCK) != 0 || result > 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\tinit_waitqueue_entry(&wait, current);\n\t\t\tadd_wait_queue(&tu->qchange_sleep, &wait);\n\t\t\tspin_unlock_irq(&tu->qlock);\n\t\t\tmutex_unlock(&tu->ioctl_lock);\n\t\t\tschedule();\n\t\t\tmutex_lock(&tu->ioctl_lock);\n\t\t\tspin_lock_irq(&tu->qlock);\n\t\t\tremove_wait_queue(&tu->qchange_sleep, &wait);\n\t\t\tif (tu->disconnected) {\n\t\t\t\terr = -ENODEV;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = -ERESTARTSYS;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t}\n\t\tqhead = tu->qhead++;\n\t\ttu->qhead %= tu->queue_size;\n\t\ttu->qused--;\n\t\tspin_unlock_irq(&tu->qlock);\n\t\tif (tu->tread) {\n\t\t\tif (copy_to_user(buffer, &tu->tqueue[qhead],\n\t\t\t\t\t sizeof(struct snd_timer_tread)))\n\t\t\t\terr = -EFAULT;\n\t\t} else {\n\t\t\tif (copy_to_user(buffer, &tu->queue[qhead],\n\t\t\t\t\t sizeof(struct snd_timer_read)))\n\t\t\t\terr = -EFAULT;\n\t\t}\n\t\tspin_lock_irq(&tu->qlock);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t\tresult += unit;\n\t\tbuffer += unit;\n\t}\n _error:\n\tspin_unlock_irq(&tu->qlock);\n\tmutex_unlock(&tu->ioctl_lock);\n\treturn result > 0 ? result : err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 1
    },
    {
        "code": "static int parse_status(const char *value)\n{\n\tint ret = 0;\n\tchar *c;\n\t/* skip a header line */\n\tc = strchr(value, '\\n');\n\tif (!c)\n\t\treturn -1;\n\tc++;\n\twhile (*c != '\\0') {\n\t\tint port, status, speed, devid;\n\t\tint sockfd;\n\t\tchar lbusid[SYSFS_BUS_ID_SIZE];\n\t\tstruct usbip_imported_device *idev;\n\t\tchar hub[3];\n\t\tret = sscanf(c, \"%2s  %d %d %d %x %u %31s\\n\",\n\t\t\t\thub, &port, &status, &speed,\n\t\t\t\t&devid, &sockfd, lbusid);\n\t\tif (ret < 5) {\n\t\t\tdbg(\"sscanf failed: %d\", ret);\n\t\t\tBUG();\n\t\t}\n\t\tdbg(\"hub %s port %d status %d speed %d devid %x\",\n\t\t\t\thub, port, status, speed, devid);\n\t\tdbg(\"sockfd %u lbusid %s\", sockfd, lbusid);\n\t\t/* if a device is connected, look at it */\n\t\tidev = &vhci_driver->idev[port];\n\t\tmemset(idev, 0, sizeof(*idev));\n\t\tif (strncmp(\"hs\", hub, 2) == 0)\n\t\t\tidev->hub = HUB_SPEED_HIGH;\n\t\telse /* strncmp(\"ss\", hub, 2) == 0 */\n\t\t\tidev->hub = HUB_SPEED_SUPER;\n\t\tidev->port\t= port;\n\t\tidev->status\t= status;\n\t\tidev->devid\t= devid;\n\t\tidev->busnum\t= (devid >> 16);\n\t\tidev->devnum\t= (devid & 0x0000ffff);\n\t\tif (idev->status != VDEV_ST_NULL\n\t\t    && idev->status != VDEV_ST_NOTASSIGNED) {\n\t\t\tidev = imported_device_init(idev, lbusid);\n\t\t\tif (!idev) {\n\t\t\t\tdbg(\"imported_device_init failed\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t\t/* go to the next line */\n\t\tc = strchr(c, '\\n');\n\t\tif (!c)\n\t\t\tbreak;\n\t\tc++;\n\t}\n\tdbg(\"exit\");\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 3
    },
    {
        "code": "static ssize_t nports_show(struct device *dev, struct device_attribute *attr,\n\t\t\t   char *out)\n{\n\tchar *s = out;\n\t/*\n\t * Half the ports are for SPEED_HIGH and half for SPEED_SUPER,\n\t * thus the * 2.\n\t */\n\tout += sprintf(out, \"%d\\n\", VHCI_PORTS * vhci_num_controllers);\n\treturn out - s;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 5
    },
    {
        "code": "static unsigned int help(struct sk_buff *skb,\n\t\t\t enum ip_conntrack_info ctinfo,\n\t\t\t unsigned int protoff,\n\t\t\t unsigned int matchoff,\n\t\t\t unsigned int matchlen,\n\t\t\t struct nf_conntrack_expect *exp)\n{\n\tchar buffer[sizeof(\"4294967296 65635\")];\n\tstruct nf_conn *ct = exp->master;\n\tunion nf_inet_addr newaddr;\n\tu_int16_t port;\n\tunsigned int ret;\n\t/* Reply comes from server. */\n\tnewaddr = ct->tuplehash[IP_CT_DIR_REPLY].tuple.dst.u3;\n\texp->saved_proto.tcp.port = exp->tuple.dst.u.tcp.port;\n\texp->dir = IP_CT_DIR_REPLY;\n\texp->expectfn = nf_nat_follow_master;\n\t/* Try to get same port: if not, try to change it. */\n\tfor (port = ntohs(exp->saved_proto.tcp.port); port != 0; port++) {\n\t\tint ret;\n\t\texp->tuple.dst.u.tcp.port = htons(port);\n\t\tret = nf_ct_expect_related(exp);\n\t\tif (ret == 0)\n\t\t\tbreak;\n\t\telse if (ret != -EBUSY) {\n\t\t\tport = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (port == 0) {\n\t\tnf_ct_helper_log(skb, ct, \"all ports in use\");\n\t\treturn NF_DROP;\n\t}\n\t/* strlen(\"\\1DCC CHAT chat AAAAAAAA P\\1\\n\")=27\n\t * strlen(\"\\1DCC SCHAT chat AAAAAAAA P\\1\\n\")=28\n\t * strlen(\"\\1DCC SEND F AAAAAAAA P S\\1\\n\")=26\n\t * strlen(\"\\1DCC MOVE F AAAAAAAA P S\\1\\n\")=26\n\t * strlen(\"\\1DCC TSEND F AAAAAAAA P S\\1\\n\")=27\n\t *\n\t * AAAAAAAAA: bound addr (1.0.0.0==16777216, min 8 digits,\n\t *                        255.255.255.255==4294967296, 10 digits)\n\t * P:         bound port (min 1 d, max 5d (65635))\n\t * F:         filename   (min 1 d )\n\t * S:         size       (min 1 d )\n\t * 0x01, \\n:  terminators\n\t */\n\t/* AAA = \"us\", ie. where server normally talks to. */\n\tsnprintf(buffer, sizeof(buffer), \"%u %u\", ntohl(newaddr.ip), port);\n\tpr_debug(\"nf_nat_irc: inserting '%s' == %pI4, port %u\\n\",\n\t\t buffer, &newaddr.ip, port);\n\tret = nf_nat_mangle_tcp_packet(skb, ct, ctinfo, protoff, matchoff,\n\t\t\t\t       matchlen, buffer, strlen(buffer));\n\tif (ret != NF_ACCEPT) {\n\t\tnf_ct_helper_log(skb, ct, \"cannot mangle packet\");\n\t\tnf_ct_unexpect_related(exp);\n\t}\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 7
    },
    {
        "code": "int fb_copy_cmap(const struct fb_cmap *from, struct fb_cmap *to)\n{\n\tunsigned int tooff = 0, fromoff = 0;\n\tsize_t size;\n\tif (to->start > from->start)\n\t\tfromoff = to->start - from->start;\n\telse\n\t\ttooff = from->start - to->start;\n\tif (fromoff >= from->len || tooff >= to->len)\n\t\treturn -EINVAL;\n\tsize = min_t(size_t, to->len - tooff, from->len - fromoff);\n\tif (size == 0)\n\t\treturn -EINVAL;\n\tsize *= sizeof(u16);\n\tmemcpy(to->red+tooff, from->red+fromoff, size);\n\tmemcpy(to->green+tooff, from->green+fromoff, size);\n\tmemcpy(to->blue+tooff, from->blue+fromoff, size);\n\tif (from->transp && to->transp)\n\t\tmemcpy(to->transp+tooff, from->transp+fromoff, size);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 9
    },
    {
        "code": "int fb_cmap_to_user(const struct fb_cmap *from, struct fb_cmap_user *to)\n{\n\tunsigned int tooff = 0, fromoff = 0;\n\tsize_t size;\n\tif (to->start > from->start)\n\t\tfromoff = to->start - from->start;\n\telse\n\t\ttooff = from->start - to->start;\n\tif (fromoff >= from->len || tooff >= to->len)\n\t\treturn -EINVAL;\n\tsize = min_t(size_t, to->len - tooff, from->len - fromoff);\n\tif (size == 0)\n\t\treturn -EINVAL;\n\tsize *= sizeof(u16);\n\tif (copy_to_user(to->red+tooff, from->red+fromoff, size))\n\t\treturn -EFAULT;\n\tif (copy_to_user(to->green+tooff, from->green+fromoff, size))\n\t\treturn -EFAULT;\n\tif (copy_to_user(to->blue+tooff, from->blue+fromoff, size))\n\t\treturn -EFAULT;\n\tif (from->transp && to->transp)\n\t\tif (copy_to_user(to->transp+tooff, from->transp+fromoff, size))\n\t\t\treturn -EFAULT;\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 11
    },
    {
        "code": "static void snd_timer_user_ccallback(struct snd_timer_instance *timeri,\n\t\t\t\t     int event,\n\t\t\t\t     struct timespec *tstamp,\n\t\t\t\t     unsigned long resolution)\n{\n\tstruct snd_timer_user *tu = timeri->callback_data;\n\tstruct snd_timer_tread r1;\n\tunsigned long flags;\n\tif (event >= SNDRV_TIMER_EVENT_START &&\n\t    event <= SNDRV_TIMER_EVENT_PAUSE)\n\t\ttu->tstamp = *tstamp;\n\tif ((tu->filter & (1 << event)) == 0 || !tu->tread)\n\t\treturn;\n\tmemset(&r1, 0, sizeof(r1));\n\tr1.event = event;\n\tr1.tstamp = *tstamp;\n\tr1.val = resolution;\n\tspin_lock_irqsave(&tu->qlock, flags);\n\tsnd_timer_user_append_to_tqueue(tu, &r1);\n\tspin_unlock_irqrestore(&tu->qlock, flags);\n\tkill_fasync(&tu->fasync, SIGIO, POLL_IN);\n\twake_up(&tu->qchange_sleep);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 13
    },
    {
        "code": "\t\t} else\n\t\t\tret = -ESRCH;\n\t}\n\tbreak;\n\tcase IP_VS_SO_GET_DESTS:\n\t{\n\t\tstruct ip_vs_get_dests *get;\n\t\tint size;\n\t\tget = (struct ip_vs_get_dests *)arg;\n\t\tsize = sizeof(*get) +\n\t\t\tsizeof(struct ip_vs_dest_entry) * get->num_dests;\n\t\tif (*len != size) {\n\t\t\tpr_err(\"length: %u != %u\\n\", *len, size);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tret = __ip_vs_get_dest_entries(net, get, user);\n\t}\n\tbreak;\n\tcase IP_VS_SO_GET_TIMEOUT:\n\t{\n\t\tstruct ip_vs_timeout_user t;\n\t\tmemset(&t, 0, sizeof(t));\n\t\t__ip_vs_get_timeouts(net, &t);\n\t\tif (copy_to_user(user, &t, sizeof(t)) != 0)\n\t\t\tret = -EFAULT;\n\t}\n\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\nout:\n\tmutex_unlock(&__ip_vs_mutex);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 15
    },
    {
        "code": "static int blkif_completion(unsigned long *id,\n\t\t\t    struct blkfront_ring_info *rinfo,\n\t\t\t    struct blkif_response *bret)\n{\n\tint i = 0;\n\tstruct scatterlist *sg;\n\tint num_sg, num_grant;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct blk_shadow *s = &rinfo->shadow[*id];\n\tstruct copy_from_grant data = {\n\t\t.grant_idx = 0,\n\t};\n\tnum_grant = s->req.operation == BLKIF_OP_INDIRECT ?\n\t\ts->req.u.indirect.nr_segments : s->req.u.rw.nr_segments;\n\t/* The I/O request may be split in two. */\n\tif (unlikely(s->associated_id != NO_ASSOCIATED_ID)) {\n\t\tstruct blk_shadow *s2 = &rinfo->shadow[s->associated_id];\n\t\t/* Keep the status of the current response in shadow. */\n\t\ts->status = blkif_rsp_to_req_status(bret->status);\n\t\t/* Wait the second response if not yet here. */\n\t\tif (s2->status < REQ_DONE)\n\t\t\treturn 0;\n\t\tbret->status = blkif_get_final_status(s->status,\n\t\t\t\t\t\t      s2->status);\n\t\t/*\n\t\t * All the grants is stored in the first shadow in order\n\t\t * to make the completion code simpler.\n\t\t */\n\t\tnum_grant += s2->req.u.rw.nr_segments;\n\t\t/*\n\t\t * The two responses may not come in order. Only the\n\t\t * first request will store the scatter-gather list.\n\t\t */\n\t\tif (s2->num_sg != 0) {\n\t\t\t/* Update \"id\" with the ID of the first response. */\n\t\t\t*id = s->associated_id;\n\t\t\ts = s2;\n\t\t}\n\t\t/*\n\t\t * We don't need anymore the second request, so recycling\n\t\t * it now.\n\t\t */\n\t\tif (add_id_to_freelist(rinfo, s->associated_id))\n\t\t\tWARN(1, \"%s: can't recycle the second part (id = %ld) of the request\\n\",\n\t\t\t     info->gd->disk_name, s->associated_id);\n\t}\n\tdata.s = s;\n\tnum_sg = s->num_sg;\n\tif (bret->operation == BLKIF_OP_READ && info->bounce) {\n\t\tfor_each_sg(s->sg, sg, num_sg, i) {\n\t\t\tBUG_ON(sg->offset + sg->length > PAGE_SIZE);\n\t\t\tdata.bvec_offset = sg->offset;\n\t\t\tdata.bvec_data = kmap_atomic(sg_page(sg));\n\t\t\tgnttab_foreach_grant_in_range(sg_page(sg),\n\t\t\t\t\t\t      sg->offset,\n\t\t\t\t\t\t      sg->length,\n\t\t\t\t\t\t      blkif_copy_from_grant,\n\t\t\t\t\t\t      &data);\n\t\t\tkunmap_atomic(data.bvec_data);\n\t\t}\n\t}\n\t/* Add the persistent grant into the list of free grants */\n\tfor (i = 0; i < num_grant; i++) {\n\t\tif (!gnttab_try_end_foreign_access(s->grants_used[i]->gref)) {\n\t\t\t/*\n\t\t\t * If the grant is still mapped by the backend (the\n\t\t\t * backend has chosen to make this grant persistent)\n\t\t\t * we add it at the head of the list, so it will be\n\t\t\t * reused first.\n\t\t\t */\n\t\t\tif (!info->feature_persistent) {\n\t\t\t\tpr_alert(\"backed has not unmapped grant: %u\\n\",\n\t\t\t\t\t s->grants_used[i]->gref);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tlist_add(&s->grants_used[i]->node, &rinfo->grants);\n\t\t\trinfo->persistent_gnts_c++;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If the grant is not mapped by the backend we add it\n\t\t\t * to the tail of the list, so it will not be picked\n\t\t\t * again unless we run out of persistent grants.\n\t\t\t */\n\t\t\ts->grants_used[i]->gref = INVALID_GRANT_REF;\n\t\t\tlist_add_tail(&s->grants_used[i]->node, &rinfo->grants);\n\t\t}\n\t}\n\tif (s->req.operation == BLKIF_OP_INDIRECT) {\n\t\tfor (i = 0; i < INDIRECT_GREFS(num_grant); i++) {\n\t\t\tif (!gnttab_try_end_foreign_access(s->indirect_grants[i]->gref)) {\n\t\t\t\tif (!info->feature_persistent) {\n\t\t\t\t\tpr_alert(\"backed has not unmapped grant: %u\\n\",\n\t\t\t\t\t\t s->indirect_grants[i]->gref);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tlist_add(&s->indirect_grants[i]->node, &rinfo->grants);\n\t\t\t\trinfo->persistent_gnts_c++;\n\t\t\t} else {\n\t\t\t\tstruct page *indirect_page;\n\t\t\t\t/*\n\t\t\t\t * Add the used indirect page back to the list of\n\t\t\t\t * available pages for indirect grefs.\n\t\t\t\t */\n\t\t\t\tif (!info->bounce) {\n\t\t\t\t\tindirect_page = s->indirect_grants[i]->page;\n\t\t\t\t\tlist_add(&indirect_page->lru, &rinfo->indirect_pages);",
        "bug_line_number": [],
        "vul": 0,
        "id": 17
    },
    {
        "code": "static struct grant *get_indirect_grant(grant_ref_t *gref_head,\n\t\t\t\t\tstruct blkfront_ring_info *rinfo)\n{\n\tstruct grant *gnt_list_entry = get_free_grant(rinfo);\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tif (gnt_list_entry->gref != INVALID_GRANT_REF)\n\t\treturn gnt_list_entry;\n\t/* Assign a gref to this page */\n\tgnt_list_entry->gref = gnttab_claim_grant_reference(gref_head);\n\tBUG_ON(gnt_list_entry->gref == -ENOSPC);\n\tif (!info->bounce) {\n\t\tstruct page *indirect_page;\n\t\t/* Fetch a pre-allocated page to use for indirect grefs */\n\t\tBUG_ON(list_empty(&rinfo->indirect_pages));\n\t\tindirect_page = list_first_entry(&rinfo->indirect_pages,\n\t\t\t\t\t\t struct page, lru);\n\t\tlist_del(&indirect_page->lru);\n\t\tgnt_list_entry->page = indirect_page;\n\t}\n\tgrant_foreign_access(gnt_list_entry, info);\n\treturn gnt_list_entry;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 19
    },
    {
        "code": "static int talk_to_blkback(struct xenbus_device *dev,\n\t\t\t   struct blkfront_info *info)\n{\n\tconst char *message = NULL;\n\tstruct xenbus_transaction xbt;\n\tint err;\n\tunsigned int i, max_page_order;\n\tunsigned int ring_page_order;\n\tstruct blkfront_ring_info *rinfo;\n\tif (!info)\n\t\treturn -ENODEV;\n\t/* Check if backend is trusted. */\n\tinfo->bounce = !xen_blkif_trusted ||\n\t\t       !xenbus_read_unsigned(dev->nodename, \"trusted\", 1);\n\tmax_page_order = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t      \"max-ring-page-order\", 0);\n\tring_page_order = min(xen_blkif_max_ring_order, max_page_order);\n\tinfo->nr_ring_pages = 1 << ring_page_order;\n\terr = negotiate_mq(info);\n\tif (err)\n\t\tgoto destroy_blkring;\n\tfor_each_rinfo(info, rinfo, i) {\n\t\t/* Create shared ring, alloc event channel. */\n\t\terr = setup_blkring(dev, rinfo);\n\t\tif (err)\n\t\t\tgoto destroy_blkring;\n\t}\nagain:\n\terr = xenbus_transaction_start(&xbt);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"starting transaction\");\n\t\tgoto destroy_blkring;\n\t}\n\tif (info->nr_ring_pages > 1) {\n\t\terr = xenbus_printf(xbt, dev->nodename, \"ring-page-order\", \"%u\",\n\t\t\t\t    ring_page_order);\n\t\tif (err) {\n\t\t\tmessage = \"writing ring-page-order\";\n\t\t\tgoto abort_transaction;\n\t\t}\n\t}\n\t/* We already got the number of queues/rings in _probe */\n\tif (info->nr_rings == 1) {\n\t\terr = write_per_ring_nodes(xbt, info->rinfo, dev->nodename);\n\t\tif (err)\n\t\t\tgoto destroy_blkring;\n\t} else {\n\t\tchar *path;\n\t\tsize_t pathsize;\n\t\terr = xenbus_printf(xbt, dev->nodename, \"multi-queue-num-queues\", \"%u\",\n\t\t\t\t    info->nr_rings);\n\t\tif (err) {\n\t\t\tmessage = \"writing multi-queue-num-queues\";\n\t\t\tgoto abort_transaction;\n\t\t}\n\t\tpathsize = strlen(dev->nodename) + QUEUE_NAME_LEN;\n\t\tpath = kmalloc(pathsize, GFP_KERNEL);\n\t\tif (!path) {\n\t\t\terr = -ENOMEM;\n\t\t\tmessage = \"ENOMEM while writing ring references\";\n\t\t\tgoto abort_transaction;\n\t\t}\n\t\tfor_each_rinfo(info, rinfo, i) {\n\t\t\tmemset(path, 0, pathsize);\n\t\t\tsnprintf(path, pathsize, \"%s/queue-%u\", dev->nodename, i);\n\t\t\terr = write_per_ring_nodes(xbt, rinfo, path);\n\t\t\tif (err) {\n\t\t\t\tkfree(path);\n\t\t\t\tgoto destroy_blkring;\n\t\t\t}\n\t\t}\n\t\tkfree(path);\n\t}\n\terr = xenbus_printf(xbt, dev->nodename, \"protocol\", \"%s\",\n\t\t\t    XEN_IO_PROTO_ABI_NATIVE);\n\tif (err) {\n\t\tmessage = \"writing protocol\";\n\t\tgoto abort_transaction;\n\t}\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-persistent\", \"%u\",\n\t\t\tinfo->feature_persistent);\n\tif (err)\n\t\tdev_warn(&dev->dev,\n\t\t\t \"writing persistent grants feature to xenbus\");\n\terr = xenbus_transaction_end(xbt, 0);\n\tif (err) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto again;\n\t\txenbus_dev_fatal(dev, err, \"completing transaction\");\n\t\tgoto destroy_blkring;\n\t}\n\tfor_each_rinfo(info, rinfo, i) {\n\t\tunsigned int j;\n\t\tfor (j = 0; j < BLK_RING_SIZE(info); j++)\n\t\t\trinfo->shadow[j].req.u.rw.id = j + 1;\n\t\trinfo->shadow[BLK_RING_SIZE(info)-1].req.u.rw.id = 0x0fffffff;\n\t}\n\txenbus_switch_state(dev, XenbusStateInitialised);\n\treturn 0;\n abort_transaction:\n\txenbus_transaction_end(xbt, 1);\n\tif (message)\n\t\txenbus_dev_fatal(dev, err, \"%s\", message);\n destroy_blkring:\n\tblkif_free(info, 0);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 21
    },
    {
        "code": "void __cpuinit cpu_init(void)\n{\n\tint cpu = smp_processor_id();\n\tstruct task_struct *curr = current;\n\tstruct tss_struct *t = &per_cpu(init_tss, cpu);\n\tstruct thread_struct *thread = &curr->thread;\n\tif (cpumask_test_and_set_cpu(cpu, cpu_initialized_mask)) {\n\t\tprintk(KERN_WARNING \"CPU#%d already initialized!\\n\", cpu);\n\t\tfor (;;)\n\t\t\tlocal_irq_enable();\n\t}\n\tprintk(KERN_INFO \"Initializing CPU#%d\\n\", cpu);\n\tif (cpu_has_vme || cpu_has_tsc || cpu_has_de)\n\t\tclear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);\n\tload_idt(&idt_descr);\n\tswitch_to_new_gdt(cpu);\n\t/*\n\t * Set up and load the per-CPU TSS and LDT\n\t */\n\tatomic_inc(&init_mm.mm_count);\n\tcurr->active_mm = &init_mm;\n\tBUG_ON(curr->mm);\n\tenter_lazy_tlb(&init_mm, curr);\n\tload_sp0(t, thread);\n\tset_tss_desc(cpu, t);\n\tload_TR_desc();\n\tload_LDT(&init_mm.context);\n\tt->x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);\n#ifdef CONFIG_DOUBLEFAULT\n\t/* Set up doublefault TSS pointer in the GDT */\n\t__set_tss_desc(cpu, GDT_ENTRY_DOUBLEFAULT_TSS, &doublefault_tss);\n#endif\n\tclear_all_debug_regs();\n\tdbg_restore_debug_regs();\n\tfpu_init();\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 23
    },
    {
        "code": "static inline fpu_switch_t switch_fpu_prepare(struct task_struct *old, struct task_struct *new, int cpu)\n{\n\tfpu_switch_t fpu;\n\t/*\n\t * If the task has used the math, pre-load the FPU on xsave processors\n\t * or if the past 5 consecutive context-switches used math.\n\t */\n\tfpu.preload = tsk_used_math(new) && (use_eager_fpu() ||\n\t\t\t\t\t     new->fpu_counter > 5);\n\tif (__thread_has_fpu(old)) {\n\t\tif (!__save_init_fpu(old))\n\t\t\tcpu = ~0;\n\t\told->thread.fpu.last_cpu = cpu;\n\t\told->thread.fpu.has_fpu = 0;\t/* But leave fpu_owner_task! */\n\t\t/* Don't change CR0.TS if we just switch! */\n\t\tif (fpu.preload) {\n\t\t\tnew->fpu_counter++;\n\t\t\t__thread_set_has_fpu(new);\n\t\t\tprefetch(new->thread.fpu.state);\n\t\t} else if (!use_eager_fpu())\n\t\t\tstts();\n\t} else {\n\t\told->fpu_counter = 0;\n\t\told->thread.fpu.last_cpu = ~0;\n\t\tif (fpu.preload) {\n\t\t\tnew->fpu_counter++;\n\t\t\tif (!use_eager_fpu() && fpu_lazy_restore(new, cpu))\n\t\t\t\tfpu.preload = 0;\n\t\t\telse\n\t\t\t\tprefetch(new->thread.fpu.state);\n\t\t\t__thread_fpu_begin(new);\n\t\t}\n\t}\n\treturn fpu;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 25
    },
    {
        "code": "static int do_video_set_spu_palette(unsigned int fd, unsigned int cmd,\n\t\tstruct compat_video_spu_palette __user *up)\n{\n\tstruct video_spu_palette __user *up_native;\n\tcompat_uptr_t palp;\n\tint length, err;\n\terr  = get_user(palp, &up->palette);\n\terr |= get_user(length, &up->length);\n\tif (err)\n\t\treturn -EFAULT;\n\tup_native = compat_alloc_user_space(sizeof(struct video_spu_palette));\n\terr  = put_user(compat_ptr(palp), &up_native->palette);\n\terr |= put_user(length, &up_native->length);\n\tif (err)\n\t\treturn -EFAULT;\n\terr = sys_ioctl(fd, cmd, (unsigned long) up_native);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 27
    },
    {
        "code": "\t\t\t\tsg_count[i] = usg->sg[i].count;\n\t\t\t\tif (sg_count[i] >\n\t\t\t\t    ((dev->adapter_info.options &\n\t\t\t\t     AAC_OPT_NEW_COMM) ?\n\t\t\t\t      (dev->scsi_host_ptr->max_sectors << 9) :\n\t\t\t\t      65536)) {\n\t\t\t\t\trcode = -EINVAL;\n\t\t\t\t\tgoto cleanup;\n\t\t\t\t}\n\t\t\t\tp = kmalloc(sg_count[i], GFP_KERNEL|GFP_DMA32);\n\t\t\t\tif (!p) {\n\t\t\t\t\tdprintk((KERN_DEBUG\"aacraid: Could not allocate SG buffer - size = %d buffer number %d of %d\\n\",\n\t\t\t\t\t\tsg_count[i], i, usg->count));\n\t\t\t\t\trcode = -ENOMEM;\n\t\t\t\t\tgoto cleanup;\n\t\t\t\t}\n\t\t\t\taddr = (u64)usg->sg[i].addr[0];\n\t\t\t\taddr += ((u64)usg->sg[i].addr[1]) << 32;\n\t\t\t\tsg_user[i] = (void __user *)addr;\n\t\t\t\tsg_list[i] = p; // save so we can clean up later\n\t\t\t\tsg_indx = i;\n\t\t\t\tif (flags & SRB_DataOut) {\n\t\t\t\t\tif (copy_from_user(p, sg_user[i],\n\t\t\t\t\t\tsg_count[i])){\n\t\t\t\t\t\tdprintk((KERN_DEBUG\"aacraid: Could not copy sg data from user\\n\"));\n\t\t\t\t\t\trcode = -EFAULT;\n\t\t\t\t\t\tgoto cleanup;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\taddr = pci_map_single(dev->pdev, p, usg->sg[i].count, data_dir);\n\t\t\t\tpsg->sg[i].addr = cpu_to_le32(addr & 0xffffffff);\n\t\t\t\tbyte_count += usg->sg[i].count;\n\t\t\t\tpsg->sg[i].count = cpu_to_le32(sg_count[i]);\n\t\t\t}\n\t\t} else {\n\t\t\tfor (i = 0; i < upsg->count; i++) {\n\t\t\t\tdma_addr_t addr;\n\t\t\t\tvoid* p;\n\t\t\t\tsg_count[i] = upsg->sg[i].count;\n\t\t\t\tif (sg_count[i] >\n\t\t\t\t    ((dev->adapter_info.options &\n\t\t\t\t     AAC_OPT_NEW_COMM) ?\n\t\t\t\t      (dev->scsi_host_ptr->max_sectors << 9) :\n\t\t\t\t      65536)) {\n\t\t\t\t\trcode = -EINVAL;\n\t\t\t\t\tgoto cleanup;\n\t\t\t\t}\n\t\t\t\tp = kmalloc(sg_count[i], GFP_KERNEL|GFP_DMA32);\n\t\t\t\tif (!p) {\n\t\t\t\t\tdprintk((KERN_DEBUG\"aacraid: Could not allocate SG buffer - size = %d buffer number %d of %d\\n\",\n\t\t\t\t\t  sg_count[i], i, upsg->count));\n\t\t\t\t\trcode = -ENOMEM;\n\t\t\t\t\tgoto cleanup;\n\t\t\t\t}\n\t\t\t\tsg_user[i] = (void __user *)(uintptr_t)upsg->sg[i].addr;\n\t\t\t\tsg_list[i] = p; // save so we can clean up later\n\t\t\t\tsg_indx = i;\n\t\t\t\tif (flags & SRB_DataOut) {\n\t\t\t\t\tif (copy_from_user(p, sg_user[i],\n\t\t\t\t\t\tsg_count[i])) {\n\t\t\t\t\t\tdprintk((KERN_DEBUG\"aacraid: Could not copy sg data from user\\n\"));\n\t\t\t\t\t\trcode = -EFAULT;\n\t\t\t\t\t\tgoto cleanup;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\taddr = pci_map_single(dev->pdev, p,\n\t\t\t\t\tsg_count[i], data_dir);\n\t\t\t\tpsg->sg[i].addr = cpu_to_le32(addr);\n\t\t\t\tbyte_count += sg_count[i];\n\t\t\t\tpsg->sg[i].count = cpu_to_le32(sg_count[i]);\n\t\t\t}\n\t\t}\n\t\tsrbcmd->count = cpu_to_le32(byte_count);\n\t\tif (user_srbcmd->sg.count)\n\t\t\tpsg->count = cpu_to_le32(sg_indx+1);\n\t\telse\n\t\t\tpsg->count = 0;\n\t\tstatus = aac_fib_send(ScsiPortCommand, srbfib, actual_fibsize, FsaNormal, 1, 1, NULL, NULL);\n\t}\n\tif (status == -ERESTARTSYS) {\n\t\trcode = -ERESTARTSYS;\n\t\tgoto cleanup;\n\t}\n\tif (status != 0) {\n\t\tdprintk((KERN_DEBUG\"aacraid: Could not send raw srb fib to hba\\n\"));\n\t\trcode = -ENXIO;\n\t\tgoto cleanup;\n\t}\n\tif (flags & SRB_DataIn) {\n\t\tfor(i = 0 ; i <= sg_indx; i++){\n\t\t\tif (copy_to_user(sg_user[i], sg_list[i], sg_count[i])) {\n\t\t\t\tdprintk((KERN_DEBUG\"aacraid: Could not copy sg data to user\\n\"));\n\t\t\t\trcode = -EFAULT;\n\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t}\n\t}\n\tuser_reply = arg + fibsize;\n\tif (is_native_device) {\n\t\tstruct aac_hba_resp *err =\n\t\t\t&((struct aac_native_hba *)srbfib->hw_fib_va)->resp.err;\n\t\tstruct aac_srb_reply reply;\n\t\tmemset(&reply, 0, sizeof(reply));\n\t\treply.status = ST_OK;\n\t\tif (srbfib->flags & FIB_CONTEXT_FLAG_FASTRESP) {\n\t\t\t/* fast response */\n\t\t\treply.srb_status = SRB_STATUS_SUCCESS;\n\t\t\treply.scsi_status = 0;",
        "bug_line_number": [],
        "vul": 0,
        "id": 29
    },
    {
        "code": "int udp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\tsize_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tbool slow;\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ip_recv_error(sk, msg, len);\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb,\n\t\t\t\t\t\t       sizeof(struct udphdr),\n\t\t\t\t\t\t       msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udp_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t   UDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\tif (!peeked)\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_port = udp_hdr(skb)->source;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t*addr_len = sizeof(*sin);\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tUDP_INC_STATS_USER(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\n\t\tUDP_INC_STATS_USER(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t}\n\tunlock_sock_fast(sk, slow);\n\tif (noblock)\n\t\treturn -EAGAIN;\n\t/* starting over for a new packet */\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 31
    },
    {
        "code": "static int rawv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)msg->msg_name;\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\tif (skb_csum_unnecessary(skb)) {\n\t\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\t} else if (msg->msg_flags&MSG_TRUNC) {\n\t\tif (__skb_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\t} else {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, 0, msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (err)\n\t\tgoto out_free;\n\t/* Copy the address. */\n\tif (sin6) {\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = 0;\n\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\tsin6->sin6_flowinfo = 0;\n\t\tsin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t\t  IP6CB(skb)->iif);\n\t\t*addr_len = sizeof(*sin6);\n\t}\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = skb->len;\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\ncsum_copy_err:\n\tskb_kill_datagram(sk, skb, flags);\n\t/* Error for blocking case is chosen to masquerade\n\t   as some normal condition.\n\t */\n\terr = (flags&MSG_DONTWAIT) ? -EAGAIN : -EHOSTUNREACH;\n\tgoto out;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 33
    },
    {
        "code": "int udpv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint is_udp4;\n\tbool slow;\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, sizeof(struct udphdr), msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udpv6_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tif (is_udp4)\n\t\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t   UDP_MIB_INERRORS,\n\t\t\t\t\t\t   is_udplite);\n\t\t\telse\n\t\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t    UDP_MIB_INERRORS,\n\t\t\t\t\t\t    is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\tif (!peeked) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t}\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in6 *sin6;\n\t\tsin6 = (struct sockaddr_in6 *) msg->msg_name;\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\t\tif (is_udp4) {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\t\tsin6->sin6_scope_id = 0;\n\t\t} else {\n\t\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tsin6->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t    IP6CB(skb)->iif);\n\t\t}\n\t\t*addr_len = sizeof(*sin6);\n\t}\n\tif (is_udp4) {\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\t}\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);",
        "bug_line_number": [],
        "vul": 0,
        "id": 35
    },
    {
        "code": "static int recv_msg(struct kiocb *iocb, struct socket *sock,\n\t\t    struct msghdr *m, size_t buf_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tipc_port *tport = tipc_sk_port(sk);\n\tstruct sk_buff *buf;\n\tstruct tipc_msg *msg;\n\tlong timeout;\n\tunsigned int sz;\n\tu32 err;\n\tint res;\n\t/* Catch invalid receive requests */\n\tif (unlikely(!buf_len))\n\t\treturn -EINVAL;\n\tlock_sock(sk);\n\tif (unlikely(sock->state == SS_UNCONNECTED)) {\n\t\tres = -ENOTCONN;\n\t\tgoto exit;\n\t}\n\t/* will be updated in set_orig_addr() if needed */\n\tm->msg_namelen = 0;\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\nrestart:\n\t/* Look for a message in receive queue; wait if necessary */\n\twhile (skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tif (sock->state == SS_DISCONNECTING) {\n\t\t\tres = -ENOTCONN;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (timeout <= 0L) {\n\t\t\tres = timeout ? timeout : -EWOULDBLOCK;\n\t\t\tgoto exit;\n\t\t}\n\t\trelease_sock(sk);\n\t\ttimeout = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t\t\t   tipc_rx_ready(sock),\n\t\t\t\t\t\t\t   timeout);\n\t\tlock_sock(sk);\n\t}\n\t/* Look at first message in receive queue */\n\tbuf = skb_peek(&sk->sk_receive_queue);\n\tmsg = buf_msg(buf);\n\tsz = msg_data_sz(msg);\n\terr = msg_errcode(msg);\n\t/* Discard an empty non-errored message & try again */\n\tif ((!sz) && (!err)) {\n\t\tadvance_rx_queue(sk);\n\t\tgoto restart;\n\t}\n\t/* Capture sender's address (optional) */\n\tset_orig_addr(m, msg);\n\t/* Capture ancillary data (optional) */\n\tres = anc_data_recv(m, msg, tport);\n\tif (res)\n\t\tgoto exit;\n\t/* Capture message data (if valid) & compute return value (always) */\n\tif (!err) {\n\t\tif (unlikely(buf_len < sz)) {\n\t\t\tsz = buf_len;\n\t\t\tm->msg_flags |= MSG_TRUNC;\n\t\t}\n\t\tres = skb_copy_datagram_iovec(buf, msg_hdr_sz(msg),\n\t\t\t\t\t      m->msg_iov, sz);\n\t\tif (res)\n\t\t\tgoto exit;\n\t\tres = sz;\n\t} else {\n\t\tif ((sock->state == SS_READY) ||\n\t\t    ((err == TIPC_CONN_SHUTDOWN) || m->msg_control))\n\t\t\tres = 0;\n\t\telse\n\t\t\tres = -ECONNRESET;\n\t}\n\t/* Consume received message (optional) */\n\tif (likely(!(flags & MSG_PEEK))) {\n\t\tif ((sock->state != SS_READY) &&\n\t\t    (++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))\n\t\t\ttipc_acknowledge(tport->ref, tport->conn_unacked);\n\t\tadvance_rx_queue(sk);\n\t}\nexit:\n\trelease_sock(sk);\n\treturn res;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 37
    },
    {
        "code": "static int tty_open(struct inode *inode, struct file *filp)\n{\n\tstruct tty_struct *tty = NULL;\n\tint noctty, retval;\n\tstruct tty_driver *driver;\n\tint index;\n\tdev_t device = inode->i_rdev;\n\tunsigned saved_flags = filp->f_flags;\n\tnonseekable_open(inode, filp);\nretry_open:\n\tnoctty = filp->f_flags & O_NOCTTY;\n\tindex  = -1;\n\tretval = 0;\n\tmutex_lock(&tty_mutex);\n\ttty_lock();\n\tif (device == MKDEV(TTYAUX_MAJOR, 0)) {\n\t\ttty = get_current_tty();\n\t\tif (!tty) {\n\t\t\ttty_unlock();\n\t\t\tmutex_unlock(&tty_mutex);\n\t\t\treturn -ENXIO;\n\t\t}\n\t\tdriver = tty_driver_kref_get(tty->driver);\n\t\tindex = tty->index;\n\t\tfilp->f_flags |= O_NONBLOCK; /* Don't let /dev/tty block */\n\t\t/* noctty = 1; */\n\t\t/* FIXME: Should we take a driver reference ? */\n\t\ttty_kref_put(tty);\n\t\tgoto got_driver;\n\t}\n#ifdef CONFIG_VT\n\tif (device == MKDEV(TTY_MAJOR, 0)) {\n\t\textern struct tty_driver *console_driver;\n\t\tdriver = tty_driver_kref_get(console_driver);\n\t\tindex = fg_console;\n\t\tnoctty = 1;\n\t\tgoto got_driver;\n\t}\n#endif\n\tif (device == MKDEV(TTYAUX_MAJOR, 1)) {\n\t\tstruct tty_driver *console_driver = console_device(&index);\n\t\tif (console_driver) {\n\t\t\tdriver = tty_driver_kref_get(console_driver);\n\t\t\tif (driver) {\n\t\t\t\t/* Don't let /dev/console block */\n\t\t\t\tfilp->f_flags |= O_NONBLOCK;\n\t\t\t\tnoctty = 1;\n\t\t\t\tgoto got_driver;\n\t\t\t}\n\t\t}\n\t\ttty_unlock();\n\t\tmutex_unlock(&tty_mutex);\n\t\treturn -ENODEV;\n\t}\n\tdriver = get_tty_driver(device, &index);\n\tif (!driver) {\n\t\ttty_unlock();\n\t\tmutex_unlock(&tty_mutex);\n\t\treturn -ENODEV;\n\t}\ngot_driver:\n\tif (!tty) {\n\t\t/* check whether we're reopening an existing tty */\n\t\ttty = tty_driver_lookup_tty(driver, inode, index);\n\t\tif (IS_ERR(tty)) {\n\t\t\ttty_unlock();\n\t\t\tmutex_unlock(&tty_mutex);\n\t\t\ttty_driver_kref_put(driver);\n\t\t\treturn PTR_ERR(tty);\n\t\t}\n\t}\n\tif (tty) {\n\t\tretval = tty_reopen(tty);\n\t\tif (retval)\n\t\t\ttty = ERR_PTR(retval);\n\t} else\n\t\ttty = tty_init_dev(driver, index, 0);\n\tmutex_unlock(&tty_mutex);\n\ttty_driver_kref_put(driver);\n\tif (IS_ERR(tty)) {\n\t\ttty_unlock();\n\t\treturn PTR_ERR(tty);\n\t}\n\tretval = tty_add_file(tty, filp);\n\tif (retval) {\n\t\ttty_unlock();\n\t\ttty_release(inode, filp);\n\t\treturn retval;\n\t}\n\tcheck_tty_count(tty, \"tty_open\");\n\tif (tty->driver->type == TTY_DRIVER_TYPE_PTY &&\n\t    tty->driver->subtype == PTY_TYPE_MASTER)\n\t\tnoctty = 1;\n#ifdef TTY_DEBUG_HANGUP\n\tprintk(KERN_DEBUG \"opening %s...\", tty->name);\n#endif\n\tif (tty->ops->open)\n\t\tretval = tty->ops->open(tty, filp);\n\telse\n\t\tretval = -ENODEV;\n\tfilp->f_flags = saved_flags;\n\tif (!retval && test_bit(TTY_EXCLUSIVE, &tty->flags) &&\n\t\t\t\t\t\t!capable(CAP_SYS_ADMIN))\n\t\tretval = -EBUSY;\n\tif (retval) {\n#ifdef TTY_DEBUG_HANGUP\n\t\tprintk(KERN_DEBUG \"error %d in opening %s...\", retval,\n\t\t       tty->name);",
        "bug_line_number": [],
        "vul": 0,
        "id": 39
    },
    {
        "code": "__visible __notrace_funcgraph struct task_struct *\n__switch_to(struct task_struct *prev_p, struct task_struct *next_p)\n{\n\tstruct thread_struct *prev = &prev_p->thread;\n\tstruct thread_struct *next = &next_p->thread;\n\tint cpu = smp_processor_id();\n\tstruct tss_struct *tss = &per_cpu(init_tss, cpu);\n\tunsigned fsindex, gsindex;\n\tfpu_switch_t fpu;\n\tfpu = switch_fpu_prepare(prev_p, next_p, cpu);\n\t/* Reload esp0 and ss1. */\n\tload_sp0(tss, next);\n\t/* We must save %fs and %gs before load_TLS() because\n\t * %fs and %gs may be cleared by load_TLS().\n\t *\n\t * (e.g. xen_load_tls())\n\t */\n\tsavesegment(fs, fsindex);\n\tsavesegment(gs, gsindex);\n\t/*\n\t * Load TLS before restoring any segments so that segment loads\n\t * reference the correct GDT entries.\n\t */\n\tload_TLS(next, cpu);\n\t/*\n\t * Leave lazy mode, flushing any hypercalls made here.  This\n\t * must be done after loading TLS entries in the GDT but before\n\t * loading segments that might reference them, and and it must\n\t * be done before math_state_restore, so the TS bit is up to\n\t * date.\n\t */\n\tarch_end_context_switch(next_p);\n\t/* Switch DS and ES.\n\t *\n\t * Reading them only returns the selectors, but writing them (if\n\t * nonzero) loads the full descriptor from the GDT or LDT.  The\n\t * LDT for next is loaded in switch_mm, and the GDT is loaded\n\t * above.\n\t *\n\t * We therefore need to write new values to the segment\n\t * registers on every context switch unless both the new and old\n\t * values are zero.\n\t *\n\t * Note that we don't need to do anything for CS and SS, as\n\t * those are saved and restored as part of pt_regs.\n\t */\n\tsavesegment(es, prev->es);\n\tif (unlikely(next->es | prev->es))\n\t\tloadsegment(es, next->es);\n\tsavesegment(ds, prev->ds);\n\tif (unlikely(next->ds | prev->ds))\n\t\tloadsegment(ds, next->ds);\n\t/*\n\t * Switch FS and GS.\n\t *\n\t * These are even more complicated than FS and GS: they have\n\t * 64-bit bases are that controlled by arch_prctl.  Those bases\n\t * only differ from the values in the GDT or LDT if the selector\n\t * is 0.\n\t *\n\t * Loading the segment register resets the hidden base part of\n\t * the register to 0 or the value from the GDT / LDT.  If the\n\t * next base address zero, writing 0 to the segment register is\n\t * much faster than using wrmsr to explicitly zero the base.\n\t *\n\t * The thread_struct.fs and thread_struct.gs values are 0\n\t * if the fs and gs bases respectively are not overridden\n\t * from the values implied by fsindex and gsindex.  They\n\t * are nonzero, and store the nonzero base addresses, if\n\t * the bases are overridden.\n\t *\n\t * (fs != 0 && fsindex != 0) || (gs != 0 && gsindex != 0) should\n\t * be impossible.\n\t *\n\t * Therefore we need to reload the segment registers if either\n\t * the old or new selector is nonzero, and we need to override\n\t * the base address if next thread expects it to be overridden.\n\t *\n\t * This code is unnecessarily slow in the case where the old and\n\t * new indexes are zero and the new base is nonzero -- it will\n\t * unnecessarily write 0 to the selector before writing the new\n\t * base address.\n\t *\n\t * Note: This all depends on arch_prctl being the only way that\n\t * user code can override the segment base.  Once wrfsbase and\n\t * wrgsbase are enabled, most of this code will need to change.\n\t */\n\tif (unlikely(fsindex | next->fsindex | prev->fs)) {\n\t\tloadsegment(fs, next->fsindex);\n\t\t/*\n\t\t * If user code wrote a nonzero value to FS, then it also\n\t\t * cleared the overridden base address.\n\t\t *\n\t\t * XXX: if user code wrote 0 to FS and cleared the base\n\t\t * address itself, we won't notice and we'll incorrectly\n\t\t * restore the prior base address next time we reschdule\n\t\t * the process.\n\t\t */\n\t\tif (fsindex)\n\t\t\tprev->fs = 0;\n\t}\n\tif (next->fs)\n\t\twrmsrl(MSR_FS_BASE, next->fs);\n\tprev->fsindex = fsindex;\n\tif (unlikely(gsindex | next->gsindex | prev->gs)) {\n\t\tload_gs_index(next->gsindex);\n\t\t/* This works (and fails) the same way as fsindex above. */\n\t\tif (gsindex)\n\t\t\tprev->gs = 0;\n\t}\n\tif (next->gs)\n\t\twrmsrl(MSR_KERNEL_GS_BASE, next->gs);\n\tprev->gsindex = gsindex;\n\tswitch_fpu_finish(next_p, fpu);\n\t/*\n\t * Switch the PDA and FPU contexts.\n\t */\n\tprev->usersp = this_cpu_read(old_rsp);\n\tthis_cpu_write(old_rsp, next->usersp);\n\tthis_cpu_write(current_task, next_p);",
        "bug_line_number": [],
        "vul": 0,
        "id": 41
    },
    {
        "code": "static void toggle_count_cache_flush(bool enable)\n{\n\tif (!security_ftr_enabled(SEC_FTR_FLUSH_COUNT_CACHE) &&\n\t    !security_ftr_enabled(SEC_FTR_FLUSH_LINK_STACK))\n\t\tenable = false;\n\tif (!enable) {\n\t\tpatch_instruction_site(&patch__call_flush_count_cache, PPC_INST_NOP);\n\t\tpr_info(\"link-stack-flush: software flush disabled.\\n\");\n\t\tlink_stack_flush_enabled = false;\n\t\tno_count_cache_flush();\n\t\treturn;\n\t}\n\t// This enables the branch from _switch to flush_count_cache\n\tpatch_branch_site(&patch__call_flush_count_cache,\n\t\t\t  (u64)&flush_count_cache, BRANCH_SET_LINK);\n\tpr_info(\"link-stack-flush: software flush enabled.\\n\");\n\tlink_stack_flush_enabled = true;\n\t// If we just need to flush the link stack, patch an early return\n\tif (!security_ftr_enabled(SEC_FTR_FLUSH_COUNT_CACHE)) {\n\t\tpatch_instruction_site(&patch__flush_link_stack_return, PPC_INST_BLR);\n\t\tno_count_cache_flush();\n\t\treturn;\n\t}\n\tif (!security_ftr_enabled(SEC_FTR_BCCTR_FLUSH_ASSIST)) {\n\t\tcount_cache_flush_type = COUNT_CACHE_FLUSH_SW;\n\t\tpr_info(\"count-cache-flush: full software flush sequence enabled.\\n\");\n\t\treturn;\n\t}\n\tpatch_instruction_site(&patch__flush_count_cache_return, PPC_INST_BLR);\n\tcount_cache_flush_type = COUNT_CACHE_FLUSH_HW;\n\tpr_info(\"count-cache-flush: hardware assisted flush sequence enabled\\n\");\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 43
    },
    {
        "code": "void __detach_mounts(struct dentry *dentry)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *mnt;\n\tnamespace_lock();\n\tmp = lookup_mountpoint(dentry);\n\tif (IS_ERR_OR_NULL(mp))\n\t\tgoto out_unlock;\n\tlock_mount_hash();\n\twhile (!hlist_empty(&mp->m_list)) {\n\t\tmnt = hlist_entry(mp->m_list.first, struct mount, mnt_mp_list);\n\t\tif (mnt->mnt.mnt_flags & MNT_UMOUNT) {\n\t\t\tstruct mount *p, *tmp;\n\t\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\t\thlist_add_head(&p->mnt_umount.s_list, &unmounted);\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\telse umount_tree(mnt, UMOUNT_CONNECTED);\n\t}\n\tunlock_mount_hash();\n\tput_mountpoint(mp);\nout_unlock:\n\tnamespace_unlock();\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 45
    },
    {
        "code": "static int talk_to_netback(struct xenbus_device *dev,\n\t\t\t   struct netfront_info *info)\n{\n\tconst char *message;\n\tstruct xenbus_transaction xbt;\n\tint err;\n\tunsigned int feature_split_evtchn;\n\tunsigned int i = 0;\n\tunsigned int max_queues = 0;\n\tstruct netfront_queue *queue = NULL;\n\tunsigned int num_queues = 1;\n\tu8 addr[ETH_ALEN];\n\tinfo->netdev->irq = 0;\n\t/* Check if backend is trusted. */\n\tinfo->bounce = !xennet_trusted ||\n\t\t       !xenbus_read_unsigned(dev->nodename, \"trusted\", 1);\n\t/* Check if backend supports multiple queues */\n\tmax_queues = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t  \"multi-queue-max-queues\", 1);\n\tnum_queues = min(max_queues, xennet_max_queues);\n\t/* Check feature-split-event-channels */\n\tfeature_split_evtchn = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\"feature-split-event-channels\", 0);\n\t/* Read mac addr. */\n\terr = xen_net_read_mac(dev, addr);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"parsing %s/mac\", dev->nodename);\n\t\tgoto out_unlocked;\n\t}\n\teth_hw_addr_set(info->netdev, addr);\n\tinfo->netback_has_xdp_headroom = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\t\t      \"feature-xdp-headroom\", 0);\n\tif (info->netback_has_xdp_headroom) {\n\t\t/* set the current xen-netfront xdp state */\n\t\terr = talk_to_netback_xdp(info, info->netfront_xdp_enabled ?\n\t\t\t\t\t  NETBACK_XDP_HEADROOM_ENABLE :\n\t\t\t\t\t  NETBACK_XDP_HEADROOM_DISABLE);\n\t\tif (err)\n\t\t\tgoto out_unlocked;\n\t}\n\trtnl_lock();\n\tif (info->queues)\n\t\txennet_destroy_queues(info);\n\t/* For the case of a reconnect reset the \"broken\" indicator. */\n\tinfo->broken = false;\n\terr = xennet_create_queues(info, &num_queues);\n\tif (err < 0) {\n\t\txenbus_dev_fatal(dev, err, \"creating queues\");\n\t\tkfree(info->queues);\n\t\tinfo->queues = NULL;\n\t\tgoto out;\n\t}\n\trtnl_unlock();\n\t/* Create shared ring, alloc event channel -- for each queue */\n\tfor (i = 0; i < num_queues; ++i) {\n\t\tqueue = &info->queues[i];\n\t\terr = setup_netfront(dev, queue, feature_split_evtchn);\n\t\tif (err)\n\t\t\tgoto destroy_ring;\n\t}\nagain:\n\terr = xenbus_transaction_start(&xbt);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"starting transaction\");\n\t\tgoto destroy_ring;\n\t}\n\tif (xenbus_exists(XBT_NIL,\n\t\t\t  info->xbdev->otherend, \"multi-queue-max-queues\")) {\n\t\t/* Write the number of queues */\n\t\terr = xenbus_printf(xbt, dev->nodename,\n\t\t\t\t    \"multi-queue-num-queues\", \"%u\", num_queues);\n\t\tif (err) {\n\t\t\tmessage = \"writing multi-queue-num-queues\";\n\t\t\tgoto abort_transaction_no_dev_fatal;\n\t\t}\n\t}\n\tif (num_queues == 1) {\n\t\terr = write_queue_xenstore_keys(&info->queues[0], &xbt, 0); /* flat */\n\t\tif (err)\n\t\t\tgoto abort_transaction_no_dev_fatal;\n\t} else {\n\t\t/* Write the keys for each queue */\n\t\tfor (i = 0; i < num_queues; ++i) {\n\t\t\tqueue = &info->queues[i];\n\t\t\terr = write_queue_xenstore_keys(queue, &xbt, 1); /* hierarchical */\n\t\t\tif (err)\n\t\t\t\tgoto abort_transaction_no_dev_fatal;\n\t\t}\n\t}\n\t/* The remaining keys are not queue-specific */\n\terr = xenbus_printf(xbt, dev->nodename, \"request-rx-copy\", \"%u\",\n\t\t\t    1);\n\tif (err) {\n\t\tmessage = \"writing request-rx-copy\";\n\t\tgoto abort_transaction;\n\t}\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-rx-notify\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-rx-notify\";\n\t\tgoto abort_transaction;\n\t}\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-sg\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-sg\";",
        "bug_line_number": [],
        "vul": 0,
        "id": 47
    },
    {
        "code": "static void binder_transaction_buffer_release(struct binder_proc *proc,\n\t\t\t\t\t      struct binder_buffer *buffer,\n\t\t\t\t\t      binder_size_t *failed_at)\n{\n\tbinder_size_t *offp, *off_start, *off_end;\n\tint debug_id = buffer->debug_id;\n\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t     \"%d buffer release %d, size %zd-%zd, failed at %pK\\n\",\n\t\t     proc->pid, buffer->debug_id,\n\t\t     buffer->data_size, buffer->offsets_size, failed_at);\n\tif (buffer->target_node)\n\t\tbinder_dec_node(buffer->target_node, 1, 0);\n\toff_start = (binder_size_t *)(buffer->data +\n\t\t\t\t      ALIGN(buffer->data_size, sizeof(void *)));\n\tif (failed_at)\n\t\toff_end = failed_at;\n\telse\n\t\toff_end = (void *)off_start + buffer->offsets_size;\n\tfor (offp = off_start; offp < off_end; offp++) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size = binder_validate_object(buffer, *offp);\n\t\tif (object_size == 0) {\n\t\t\tpr_err(\"transaction release %d bad object at offset %lld, size %zd\\n\",\n\t\t\t       debug_id, (u64)*offp, buffer->data_size);\n\t\t\tcontinue;\n\t\t}\n\t\thdr = (struct binder_object_header *)(buffer->data + *offp);\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_node *node;\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tnode = binder_get_node(proc, fp->binder);\n\t\t\tif (node == NULL) {\n\t\t\t\tpr_err(\"transaction release %d bad node %016llx\\n\",\n\t\t\t\t       debug_id, (u64)fp->binder);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        node %d u%016llx\\n\",\n\t\t\t\t     node->debug_id, (u64)node->ptr);\n\t\t\tbinder_dec_node(node, hdr->type == BINDER_TYPE_BINDER,\n\t\t\t\t\t0);\n\t\t\tbinder_put_node(node);\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_ref_data rdata;\n\t\t\tint ret;\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_dec_ref_for_handle(proc, fp->handle,\n\t\t\t\thdr->type == BINDER_TYPE_HANDLE, &rdata);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"transaction release %d bad handle %d, ret = %d\\n\",\n\t\t\t\t debug_id, fp->handle, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        ref %d desc %d\\n\",\n\t\t\t\t     rdata.debug_id, rdata.desc);\n\t\t} break;\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        fd %d\\n\", fp->fd);\n\t\t\tif (failed_at)\n\t\t\t\ttask_close_fd(proc, fp->fd);\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR:\n\t\t\t/*\n\t\t\t * Nothing to do here, this will get cleaned up when the\n\t\t\t * transaction buffer gets freed\n\t\t\t */\n\t\t\tbreak;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_fd_array_object *fda;\n\t\t\tstruct binder_buffer_object *parent;\n\t\t\tuintptr_t parent_buffer;\n\t\t\tu32 *fd_array;\n\t\t\tsize_t fd_index;\n\t\t\tbinder_size_t fd_buf_size;\n\t\t\tfda = to_binder_fd_array_object(hdr);\n\t\t\tparent = binder_validate_ptr(buffer, fda->parent,\n\t\t\t\t\t\t     off_start,\n\t\t\t\t\t\t     offp - off_start);\n\t\t\tif (!parent) {\n\t\t\t\tpr_err(\"transaction release %d bad parent offset\\n\",\n\t\t\t\t       debug_id);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Since the parent was already fixed up, convert it\n\t\t\t * back to kernel address space to access it\n\t\t\t */\n\t\t\tparent_buffer = parent->buffer -\n\t\t\t\tbinder_alloc_get_user_buffer_offset(\n\t\t\t\t\t\t&proc->alloc);\n\t\t\tfd_buf_size = sizeof(u32) * fda->num_fds;\n\t\t\tif (fda->num_fds >= SIZE_MAX / sizeof(u32)) {\n\t\t\t\tpr_err(\"transaction release %d invalid number of fds (%lld)\\n\",\n\t\t\t\t       debug_id, (u64)fda->num_fds);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (fd_buf_size > parent->length ||\n\t\t\t    fda->parent_offset > parent->length - fd_buf_size) {\n\t\t\t\t/* No space for all file descriptors here. */\n\t\t\t\tpr_err(\"transaction release %d not enough space for %lld fds in buffer\\n\",",
        "bug_line_number": [],
        "vul": 0,
        "id": 49
    },
    {
        "code": "static struct sock *pep_sock_accept(struct sock *sk, int flags, int *errp,\n\t\t\t\t    bool kern)\n{\n\tstruct pep_sock *pn = pep_sk(sk), *newpn;\n\tstruct sock *newsk = NULL;\n\tstruct sk_buff *skb;\n\tstruct pnpipehdr *hdr;\n\tstruct sockaddr_pn dst, src;\n\tint err;\n\tu16 peer_type;\n\tu8 pipe_handle, enabled, n_sb;\n\tu8 aligned = 0;\n\tskb = skb_recv_datagram(sk, 0, flags & O_NONBLOCK, errp);\n\tif (!skb)\n\t\treturn NULL;\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_LISTEN) {\n\t\terr = -EINVAL;\n\t\tgoto drop;\n\t}\n\tsk_acceptq_removed(sk);\n\terr = -EPROTO;\n\tif (!pskb_may_pull(skb, sizeof(*hdr) + 4))\n\t\tgoto drop;\n\thdr = pnp_hdr(skb);\n\tpipe_handle = hdr->pipe_handle;\n\tswitch (hdr->state_after_connect) {\n\tcase PN_PIPE_DISABLE:\n\t\tenabled = 0;\n\t\tbreak;\n\tcase PN_PIPE_ENABLE:\n\t\tenabled = 1;\n\t\tbreak;\n\tdefault:\n\t\tpep_reject_conn(sk, skb, PN_PIPE_ERR_INVALID_PARAM,\n\t\t\t\tGFP_KERNEL);\n\t\tgoto drop;\n\t}\n\tpeer_type = hdr->other_pep_type << 8;\n\t/* Parse sub-blocks (options) */\n\tn_sb = hdr->data[3];\n\twhile (n_sb > 0) {\n\t\tu8 type, buf[1], len = sizeof(buf);\n\t\tconst u8 *data = pep_get_sb(skb, &type, &len, buf);\n\t\tif (data == NULL)\n\t\t\tgoto drop;\n\t\tswitch (type) {\n\t\tcase PN_PIPE_SB_CONNECT_REQ_PEP_SUB_TYPE:\n\t\t\tif (len < 1)\n\t\t\t\tgoto drop;\n\t\t\tpeer_type = (peer_type & 0xff00) | data[0];\n\t\t\tbreak;\n\t\tcase PN_PIPE_SB_ALIGNED_DATA:\n\t\t\taligned = data[0] != 0;\n\t\t\tbreak;\n\t\t}\n\t\tn_sb--;\n\t}\n\t/* Check for duplicate pipe handle */\n\tnewsk = pep_find_pipe(&pn->hlist, &dst, pipe_handle);\n\tif (unlikely(newsk)) {\n\t\t__sock_put(newsk);\n\t\tnewsk = NULL;\n\t\tpep_reject_conn(sk, skb, PN_PIPE_ERR_PEP_IN_USE, GFP_KERNEL);\n\t\tgoto drop;\n\t}\n\t/* Create a new to-be-accepted sock */\n\tnewsk = sk_alloc(sock_net(sk), PF_PHONET, GFP_KERNEL, sk->sk_prot,\n\t\t\t kern);\n\tif (!newsk) {\n\t\tpep_reject_conn(sk, skb, PN_PIPE_ERR_OVERLOAD, GFP_KERNEL);\n\t\terr = -ENOBUFS;\n\t\tgoto drop;\n\t}\n\tsock_init_data(NULL, newsk);\n\tnewsk->sk_state = TCP_SYN_RECV;\n\tnewsk->sk_backlog_rcv = pipe_do_rcv;\n\tnewsk->sk_protocol = sk->sk_protocol;\n\tnewsk->sk_destruct = pipe_destruct;\n\tnewpn = pep_sk(newsk);\n\tpn_skb_get_dst_sockaddr(skb, &dst);\n\tpn_skb_get_src_sockaddr(skb, &src);\n\tnewpn->pn_sk.sobject = pn_sockaddr_get_object(&dst);\n\tnewpn->pn_sk.dobject = pn_sockaddr_get_object(&src);\n\tnewpn->pn_sk.resource = pn_sockaddr_get_resource(&dst);\n\tsock_hold(sk);\n\tnewpn->listener = sk;\n\tskb_queue_head_init(&newpn->ctrlreq_queue);\n\tnewpn->pipe_handle = pipe_handle;\n\tatomic_set(&newpn->tx_credits, 0);\n\tnewpn->ifindex = 0;\n\tnewpn->peer_type = peer_type;\n\tnewpn->rx_credits = 0;\n\tnewpn->rx_fc = newpn->tx_fc = PN_LEGACY_FLOW_CONTROL;\n\tnewpn->init_enable = enabled;\n\tnewpn->aligned = aligned;\n\terr = pep_accept_conn(newsk, skb);\n\tif (err) {\n\t\t__sock_put(sk);\n\t\tsock_put(newsk);\n\t\tnewsk = NULL;\n\t\tgoto drop;\n\t}\n\tsk_add_node(newsk, &pn->hlist);\ndrop:\n\trelease_sock(sk);\n\tkfree_skb(skb);\n\t*errp = err;\n\treturn newsk;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 51
    },
    {
        "code": "static int hidp_setup_hid(struct hidp_session *session,\n\t\t\t\tstruct hidp_connadd_req *req)\n{\n\tstruct hid_device *hid;\n\tint err;\n\tsession->rd_data = kzalloc(req->rd_size, GFP_KERNEL);\n\tif (!session->rd_data)\n\t\treturn -ENOMEM;\n\tif (copy_from_user(session->rd_data, req->rd_data, req->rd_size)) {\n\t\terr = -EFAULT;\n\t\tgoto fault;\n\t}\n\tsession->rd_size = req->rd_size;\n\thid = hid_allocate_device();\n\tif (IS_ERR(hid)) {\n\t\terr = PTR_ERR(hid);\n\t\tgoto fault;\n\t}\n\tsession->hid = hid;\n\thid->driver_data = session;\n\thid->bus     = BUS_BLUETOOTH;\n\thid->vendor  = req->vendor;\n\thid->product = req->product;\n\thid->version = req->version;\n\thid->country = req->country;\n\tstrncpy(hid->name, req->name, sizeof(req->name) - 1);\n\tsnprintf(hid->phys, sizeof(hid->phys), \"%pMR\",\n\t\t &bt_sk(session->ctrl_sock->sk)->src);\n\tsnprintf(hid->uniq, sizeof(hid->uniq), \"%pMR\",\n\t\t &bt_sk(session->ctrl_sock->sk)->dst);\n\thid->dev.parent = &session->conn->dev;\n\thid->ll_driver = &hidp_hid_driver;\n\thid->hid_get_raw_report = hidp_get_raw_report;\n\thid->hid_output_raw_report = hidp_output_raw_report;\n\t/* True if device is blacklisted in drivers/hid/hid-core.c */\n\tif (hid_ignore(hid)) {\n\t\thid_destroy_device(session->hid);\n\t\tsession->hid = NULL;\n\t\treturn -ENODEV;\n\t}\n\treturn 0;\nfault:\n\tkfree(session->rd_data);\n\tsession->rd_data = NULL;\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 53
    },
    {
        "code": "\t\t\t    info->full_pathname,\n\t\t\t    acpi_ut_get_type_name(info->node->type)));\n\t\tstatus = AE_TYPE;\n\t\tgoto cleanup;\n\tcase ACPI_TYPE_METHOD:\n\t\t/*\n\t\t * 2) Object is a control method - execute it\n\t\t */\n\t\t/* Verify that there is a method object associated with this node */\n\t\tif (!info->obj_desc) {\n\t\t\tACPI_ERROR((AE_INFO,\n\t\t\t\t    \"%s: Method has no attached sub-object\",\n\t\t\t\t    info->full_pathname));\n\t\t\tstatus = AE_NULL_OBJECT;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tACPI_DEBUG_PRINT((ACPI_DB_EXEC,\n\t\t\t\t  \"**** Execute method [%s] at AML address %p length %X\\n\",\n\t\t\t\t  info->full_pathname,\n\t\t\t\t  info->obj_desc->method.aml_start + 1,\n\t\t\t\t  info->obj_desc->method.aml_length - 1));\n\t\t/*\n\t\t * Any namespace deletion must acquire both the namespace and\n\t\t * interpreter locks to ensure that no thread is using the portion of\n\t\t * the namespace that is being deleted.\n\t\t *\n\t\t * Execute the method via the interpreter. The interpreter is locked\n\t\t * here before calling into the AML parser\n\t\t */\n\t\tacpi_ex_enter_interpreter();\n\t\tstatus = acpi_ps_execute_method(info);\n\t\tacpi_ex_exit_interpreter();\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * 3) All other non-method objects -- get the current object value\n\t\t */\n\t\t/*\n\t\t * Some objects require additional resolution steps (e.g., the Node\n\t\t * may be a field that must be read, etc.) -- we can't just grab\n\t\t * the object out of the node.\n\t\t *\n\t\t * Use resolve_node_to_value() to get the associated value.\n\t\t *\n\t\t * NOTE: we can get away with passing in NULL for a walk state because\n\t\t * the Node is guaranteed to not be a reference to either a method\n\t\t * local or a method argument (because this interface is never called\n\t\t * from a running method.)\n\t\t *\n\t\t * Even though we do not directly invoke the interpreter for object\n\t\t * resolution, we must lock it because we could access an op_region.\n\t\t * The op_region access code assumes that the interpreter is locked.\n\t\t */\n\t\tacpi_ex_enter_interpreter();\n\t\t/* TBD: resolve_node_to_value has a strange interface, fix */\n\t\tinfo->return_object =\n\t\t    ACPI_CAST_PTR(union acpi_operand_object, info->node);\n\t\tstatus =\n\t\t    acpi_ex_resolve_node_to_value(ACPI_CAST_INDIRECT_PTR\n\t\t\t\t\t\t  (struct acpi_namespace_node,\n\t\t\t\t\t\t   &info->return_object), NULL);\n\t\tacpi_ex_exit_interpreter();\n\t\tif (ACPI_FAILURE(status)) {\n\t\t\tinfo->return_object = NULL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tACPI_DEBUG_PRINT((ACPI_DB_NAMES, \"Returned object %p [%s]\\n\",\n\t\t\t\t  info->return_object,\n\t\t\t\t  acpi_ut_get_object_type_name(info->\n\t\t\t\t\t\t\t       return_object)));\n\t\tstatus = AE_CTRL_RETURN_VALUE;\t/* Always has a \"return value\" */\n\t\tbreak;\n\t}\n\t/*\n\t * For predefined names, check the return value against the ACPI\n\t * specification. Some incorrect return value types are repaired.\n\t */\n\t(void)acpi_ns_check_return_value(info->node, info, info->param_count,\n\t\t\t\t\t status, &info->return_object);\n\t/* Check if there is a return value that must be dealt with */\n\tif (status == AE_CTRL_RETURN_VALUE) {\n\t\t/* If caller does not want the return value, delete it */\n\t\tif (info->flags & ACPI_IGNORE_RETURN_VALUE) {\n\t\t\tacpi_ut_remove_reference(info->return_object);\n\t\t\tinfo->return_object = NULL;\n\t\t}\n\t\t/* Map AE_CTRL_RETURN_VALUE to AE_OK, we are done with it */\n\t\tstatus = AE_OK;\n\t} else if (ACPI_FAILURE(status)) {\n\t\t/* If return_object exists, delete it */\n\t\tif (info->return_object) {\n\t\t\tacpi_ut_remove_reference(info->return_object);\n\t\t\tinfo->return_object = NULL;\n\t\t}\n\t}\n\tACPI_DEBUG_PRINT((ACPI_DB_NAMES,",
        "bug_line_number": [],
        "vul": 0,
        "id": 55
    },
    {
        "code": "static int get_bitmap_file(struct mddev *mddev, void __user * arg)\n{\n\tmdu_bitmap_file_t *file = NULL; /* too big for stack allocation */\n\tchar *ptr;\n\tint err;\n\tfile = kzalloc(sizeof(*file), GFP_NOIO);\n\tif (!file)\n\t\treturn -ENOMEM;\n\terr = 0;\n\tspin_lock(&mddev->lock);\n\t/* bitmap disabled, zero the first byte and copy out */\n\tif (!mddev->bitmap_info.file)\n\t\tfile->pathname[0] = '\\0';\n\telse if ((ptr = file_path(mddev->bitmap_info.file,\n\t\t\t       file->pathname, sizeof(file->pathname))),\n\t\t IS_ERR(ptr))\n\t\terr = PTR_ERR(ptr);\n\telse\n\t\tmemmove(file->pathname, ptr,\n\t\t\tsizeof(file->pathname)-(ptr-file->pathname));\n\tspin_unlock(&mddev->lock);\n\tif (err == 0 &&\n\t    copy_to_user(arg, file, sizeof(*file)))\n\t\terr = -EFAULT;\n\tkfree(file);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 57
    },
    {
        "code": "static inline ssize_t do_tty_write(\n\tssize_t (*write)(struct tty_struct *, struct file *, const unsigned char *, size_t),\n\tstruct tty_struct *tty,\n\tstruct file *file,\n\tconst char __user *buf,\n\tsize_t count)\n{\n\tssize_t ret, written = 0;\n\tunsigned int chunk;\n\tret = tty_write_lock(tty, file->f_flags & O_NDELAY);\n\tif (ret < 0)\n\t\treturn ret;\n\t/*\n\t * We chunk up writes into a temporary buffer. This\n\t * simplifies low-level drivers immensely, since they\n\t * don't have locking issues and user mode accesses.\n\t *\n\t * But if TTY_NO_WRITE_SPLIT is set, we should use a\n\t * big chunk-size..\n\t *\n\t * The default chunk-size is 2kB, because the NTTY\n\t * layer has problems with bigger chunks. It will\n\t * claim to be able to handle more characters than\n\t * it actually does.\n\t *\n\t * FIXME: This can probably go away now except that 64K chunks\n\t * are too likely to fail unless switched to vmalloc...\n\t */\n\tchunk = 2048;\n\tif (test_bit(TTY_NO_WRITE_SPLIT, &tty->flags))\n\t\tchunk = 65536;\n\tif (count < chunk)\n\t\tchunk = count;\n\t/* write_buf/write_cnt is protected by the atomic_write_lock mutex */\n\tif (tty->write_cnt < chunk) {\n\t\tunsigned char *buf_chunk;\n\t\tif (chunk < 1024)\n\t\t\tchunk = 1024;\n\t\tbuf_chunk = kmalloc(chunk, GFP_KERNEL);\n\t\tif (!buf_chunk) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tkfree(tty->write_buf);\n\t\ttty->write_cnt = chunk;\n\t\ttty->write_buf = buf_chunk;\n\t}\n\t/* Do the write .. */\n\tfor (;;) {\n\t\tsize_t size = count;\n\t\tif (size > chunk)\n\t\t\tsize = chunk;\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(tty->write_buf, buf, size))\n\t\t\tbreak;\n\t\tret = write(tty, file, tty->write_buf, size);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\t\twritten += ret;\n\t\tbuf += ret;\n\t\tcount -= ret;\n\t\tif (!count)\n\t\t\tbreak;\n\t\tret = -ERESTARTSYS;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tif (written)\n\t\tret = written;\nout:\n\ttty_write_unlock(tty);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 59
    },
    {
        "code": "static int raw_cmd_copyout(int cmd, void __user *param,\n\t\t\t\t  struct floppy_raw_cmd *ptr)\n{\n\tint ret;\n\twhile (ptr) {\n\t\tstruct floppy_raw_cmd cmd = *ptr;\n\t\tcmd.next = NULL;\n\t\tcmd.kernel_data = NULL;\n\t\tret = copy_to_user(param, &cmd, sizeof(cmd));\n\t\tif (ret)\n\t\t\treturn -EFAULT;\n\t\tparam += sizeof(struct floppy_raw_cmd);\n\t\tif ((ptr->flags & FD_RAW_READ) && ptr->buffer_length) {\n\t\t\tif (ptr->length >= 0 &&\n\t\t\t    ptr->length <= ptr->buffer_length) {\n\t\t\t\tlong length = ptr->buffer_length - ptr->length;\n\t\t\t\tret = fd_copyout(ptr->data, ptr->kernel_data,\n\t\t\t\t\t\t length);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tptr = ptr->next;\n\t}\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 61
    },
    {
        "code": "unsigned paravirt_patch_jmp(void *insnbuf, const void *target,\n\t\t\t    unsigned long addr, unsigned len)\n{\n\tstruct branch *b = insnbuf;\n\tunsigned long delta = (unsigned long)target - (addr+5);\n\tif (len < 5) {\n#ifdef CONFIG_RETPOLINE\n\t\tWARN_ONCE(\"Failing to patch indirect JMP in %ps\\n\", (void *)addr);\n#endif\n\t\treturn len;\t/* call too long for patch site */\n\t}\n\tb->opcode = 0xe9;\t/* jmp */\n\tb->delta = delta;\n\treturn 5;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 63
    },
    {
        "code": "\tbreak;\n\tcase IOCTL_BCM_GET_DEVICE_DRIVER_INFO: {\n\t\tstruct bcm_driver_info DevInfo;\n\t\tBCM_DEBUG_PRINT(Adapter, DBG_TYPE_OTHERS, OSAL_DBG, DBG_LVL_ALL, \"Called IOCTL_BCM_GET_DEVICE_DRIVER_INFO\\n\");\n\t\tmemset(&DevInfo, 0, sizeof(DevInfo));\n\t\tDevInfo.MaxRDMBufferSize = BUFFER_4K;\n\t\tDevInfo.u32DSDStartOffset = EEPROM_CALPARAM_START;\n\t\tDevInfo.u32RxAlignmentCorrection = 0;\n\t\tDevInfo.u32NVMType = Adapter->eNVMType;\n\t\tDevInfo.u32InterfaceType = BCM_USB;\n\t\tif (copy_from_user(&IoBuffer, argp, sizeof(struct bcm_ioctl_buffer)))\n\t\t\treturn -EFAULT;\n\t\tif (IoBuffer.OutputLength < sizeof(DevInfo))\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(IoBuffer.OutputBuffer, &DevInfo, sizeof(DevInfo)))\n\t\t\treturn -EFAULT;\n\t}\n\tbreak;\n\tcase IOCTL_BCM_TIME_SINCE_NET_ENTRY: {\n\t\tstruct bcm_time_elapsed stTimeElapsedSinceNetEntry = {0};\n\t\tBCM_DEBUG_PRINT(Adapter, DBG_TYPE_OTHERS, OSAL_DBG, DBG_LVL_ALL, \"IOCTL_BCM_TIME_SINCE_NET_ENTRY called\");\n\t\tif (copy_from_user(&IoBuffer, argp, sizeof(struct bcm_ioctl_buffer)))\n\t\t\treturn -EFAULT;\n\t\tif (IoBuffer.OutputLength < sizeof(struct bcm_time_elapsed))\n\t\t\treturn -EINVAL;\n\t\tstTimeElapsedSinceNetEntry.ul64TimeElapsedSinceNetEntry = get_seconds() - Adapter->liTimeSinceLastNetEntry;\n\t\tif (copy_to_user(IoBuffer.OutputBuffer, &stTimeElapsedSinceNetEntry, sizeof(struct bcm_time_elapsed)))\n\t\t\treturn -EFAULT;\n\t}\n\tbreak;\n\tcase IOCTL_CLOSE_NOTIFICATION:\n\t\tBCM_DEBUG_PRINT(Adapter, DBG_TYPE_OTHERS, OSAL_DBG, DBG_LVL_ALL, \"IOCTL_CLOSE_NOTIFICATION\");\n\t\tbreak;\n\tdefault:\n\t\tpr_info(DRV_NAME \": unknown ioctl cmd=%#x\\n\", cmd);\n\t\tStatus = STATUS_FAILURE;\n\t\tbreak;\n\t}\n\treturn Status;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 65
    },
    {
        "code": "static int nr_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t      struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_ax25 *sax = (struct sockaddr_ax25 *)msg->msg_name;\n\tsize_t copied;\n\tstruct sk_buff *skb;\n\tint er;\n\t/*\n\t * This works for seqpacket too. The receiver has ordered the queue for\n\t * us! We do one quick check first though\n\t */\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\trelease_sock(sk);\n\t\treturn -ENOTCONN;\n\t}\n\t/* Now we can treat all alike */\n\tif ((skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT, flags & MSG_DONTWAIT, &er)) == NULL) {\n\t\trelease_sock(sk);\n\t\treturn er;\n\t}\n\tskb_reset_transport_header(skb);\n\tcopied     = skb->len;\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\ter = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (er < 0) {\n\t\tskb_free_datagram(sk, skb);\n\t\trelease_sock(sk);\n\t\treturn er;\n\t}\n\tif (sax != NULL) {\n\t\tmemset(sax, 0, sizeof(sax));\n\t\tsax->sax25_family = AF_NETROM;\n\t\tskb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,\n\t\t\t      AX25_ADDR_LEN);\n\t}\n\tmsg->msg_namelen = sizeof(*sax);\n\tskb_free_datagram(sk, skb);\n\trelease_sock(sk);\n\treturn copied;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 67
    },
    {
        "code": "static void __net_random_once_deferred(struct work_struct *w)\n{\n\tstruct __net_random_once_work *work =\n\t\tcontainer_of(w, struct __net_random_once_work, work);\n\tBUG_ON(!static_key_enabled(work->key));\n\tstatic_key_slow_dec(work->key);\n\tkfree(work);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 69
    },
    {
        "code": "static int tipc_nl_compat_link_dump(struct tipc_nl_compat_msg *msg,\n\t\t\t\t    struct nlattr **attrs)\n{\n\tstruct nlattr *link[TIPC_NLA_LINK_MAX + 1];\n\tstruct tipc_link_info link_info;\n\tint err;\n\tif (!attrs[TIPC_NLA_LINK])\n\t\treturn -EINVAL;\n\terr = nla_parse_nested(link, TIPC_NLA_LINK_MAX, attrs[TIPC_NLA_LINK],\n\t\t\t       NULL);\n\tif (err)\n\t\treturn err;\n\tlink_info.dest = nla_get_flag(link[TIPC_NLA_LINK_DEST]);\n\tlink_info.up = htonl(nla_get_flag(link[TIPC_NLA_LINK_UP]));\n\tnla_strlcpy(link_info.str, nla_data(link[TIPC_NLA_LINK_NAME]),\n\t\t    TIPC_MAX_LINK_NAME);\n\treturn tipc_add_tlv(msg->rep, TIPC_TLV_LINK_INFO,\n\t\t\t    &link_info, sizeof(link_info));\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 71
    },
    {
        "code": "static struct binder_ref *binder_get_ref(struct binder_proc *proc,\n\t\t\t\t\t u32 desc, bool need_strong_ref)\n{\n\tstruct rb_node *n = proc->refs_by_desc.rb_node;\n\tstruct binder_ref *ref;\n\twhile (n) {\n\t\tref = rb_entry(n, struct binder_ref, rb_node_desc);\n\t\tif (desc < ref->data.desc) {\n\t\t\tn = n->rb_left;\n\t\t} else if (desc > ref->data.desc) {\n\t\t\tn = n->rb_right;\n\t\t} else if (need_strong_ref && !ref->data.strong) {\n\t\t\tbinder_user_error(\"tried to use weak ref as strong ref\\n\");\n\t\t\treturn NULL;\n\t\t} else {\n\t\t\treturn ref;\n\t\t}\n\t}\n\treturn NULL;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 73
    },
    {
        "code": "static void print_binder_ref(struct seq_file *m, struct binder_ref *ref)\n{\n\tseq_printf(m, \"  ref %d: desc %d %snode %d s %d w %d d %pK\\n\",\n\t\t   ref->data.debug_id, ref->data.desc,\n\t\t   ref->node->proc ? \"\" : \"dead \",\n\t\t   ref->node->debug_id, ref->data.strong,\n\t\t   ref->data.weak, ref->death);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 75
    },
    {
        "code": "static int pptp_connect(struct socket *sock, struct sockaddr *uservaddr,\n\tint sockaddr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppox *sp = (struct sockaddr_pppox *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct pptp_opt *opt = &po->proto.pptp;\n\tstruct rtable *rt;\n\tstruct flowi4 fl4;\n\tint error = 0;\n\tif (sockaddr_len < sizeof(struct sockaddr_pppox))\n\t\treturn -EINVAL;\n\tif (sp->sa_protocol != PX_PROTO_PPTP)\n\t\treturn -EINVAL;\n\tif (lookup_chan_dst(sp->sa_addr.pptp.call_id, sp->sa_addr.pptp.sin_addr.s_addr))\n\t\treturn -EALREADY;\n\tlock_sock(sk);\n\t/* Check for already bound sockets */\n\tif (sk->sk_state & PPPOX_CONNECTED) {\n\t\terror = -EBUSY;\n\t\tgoto end;\n\t}\n\t/* Check for already disconnected sockets, on attempts to disconnect */\n\tif (sk->sk_state & PPPOX_DEAD) {\n\t\terror = -EALREADY;\n\t\tgoto end;\n\t}\n\tif (!opt->src_addr.sin_addr.s_addr || !sp->sa_addr.pptp.sin_addr.s_addr) {\n\t\terror = -EINVAL;\n\t\tgoto end;\n\t}\n\tpo->chan.private = sk;\n\tpo->chan.ops = &pptp_chan_ops;\n\trt = ip_route_output_ports(sock_net(sk), &fl4, sk,\n\t\t\t\t   opt->dst_addr.sin_addr.s_addr,\n\t\t\t\t   opt->src_addr.sin_addr.s_addr,\n\t\t\t\t   0, 0,\n\t\t\t\t   IPPROTO_GRE, RT_CONN_FLAGS(sk), 0);\n\tif (IS_ERR(rt)) {\n\t\terror = -EHOSTUNREACH;\n\t\tgoto end;\n\t}\n\tsk_setup_caps(sk, &rt->dst);\n\tpo->chan.mtu = dst_mtu(&rt->dst);\n\tif (!po->chan.mtu)\n\t\tpo->chan.mtu = PPP_MRU;\n\tip_rt_put(rt);\n\tpo->chan.mtu -= PPTP_HEADER_OVERHEAD;\n\tpo->chan.hdrlen = 2 + sizeof(struct pptp_gre_header);\n\terror = ppp_register_channel(&po->chan);\n\tif (error) {\n\t\tpr_err(\"PPTP: failed to register PPP channel (%d)\\n\", error);\n\t\tgoto end;\n\t}\n\topt->dst_addr = sp->sa_addr.pptp;\n\tsk->sk_state = PPPOX_CONNECTED;\n end:\n\trelease_sock(sk);\n\treturn error;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 77
    }
]