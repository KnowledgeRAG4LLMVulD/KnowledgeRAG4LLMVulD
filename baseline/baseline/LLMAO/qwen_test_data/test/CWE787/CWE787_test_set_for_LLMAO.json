[
    {
        "code": "static void ll_free_user_pages(struct page **pages, int npages, int do_dirty)\n{\n\tint i;\n\tfor (i = 0; i < npages; i++) {\n\t\tif (pages[i] == NULL)\n\t\t\tbreak;\n\t\tif (do_dirty)\n\t\t\tset_page_dirty_lock(pages[i]);\n\t\tpage_cache_release(pages[i]);\n\t}\n\tOBD_FREE_LARGE(pages, npages * sizeof(*pages));\n}",
        "bug_line_number": [
            4,
            5,
            10,
            11
        ],
        "vul": 1,
        "id": 0
    },
    {
        "code": "static inline bool kdb_check_flags(kdb_cmdflags_t flags, int permissions,\n\t\t\t\t   bool no_args)\n{\n\t/* permissions comes from userspace so needs massaging slightly */\n\tpermissions &= KDB_ENABLE_MASK;\n\tpermissions |= KDB_ENABLE_ALWAYS_SAFE;\n\t/* some commands change group when launched with no arguments */\n\tif (no_args)\n\t\tpermissions |= permissions << KDB_ENABLE_NO_ARGS_SHIFT;\n\tflags |= KDB_ENABLE_ALL;\n\treturn permissions & flags;\n}",
        "bug_line_number": [
            0,
            1
        ],
        "vul": 1,
        "id": 2
    },
    {
        "code": "\t/* Call the I/O driver's pre_exception routine */\n\tif (dbg_io_ops->pre_exception)\n\t\tdbg_io_ops->pre_exception();\n\t/*\n\t * Get the passive CPU lock which will hold all the non-primary\n\t * CPU in a spin state while the debugger is active\n\t */\n\tif (!kgdb_single_step)\n\t\traw_spin_lock(&dbg_slave_lock);\n#ifdef CONFIG_SMP\n\t/* If send_ready set, slaves are already waiting */\n\tif (ks->send_ready)\n\t\tatomic_set(ks->send_ready, 1);\n\t/* Signal the other CPUs to enter kgdb_wait() */\n\telse if ((!kgdb_single_step) && kgdb_do_roundup)\n\t\tkgdb_roundup_cpus();\n#endif\n\t/*\n\t * Wait for the other CPUs to be notified and be waiting for us:\n\t */\n\ttime_left = MSEC_PER_SEC;\n\twhile (kgdb_do_roundup && --time_left &&\n\t       (atomic_read(&masters_in_kgdb) + atomic_read(&slaves_in_kgdb)) !=\n\t\t   online_cpus)\n\t\tudelay(1000);\n\tif (!time_left)\n\t\tpr_crit(\"Timed out waiting for secondary CPUs.\\n\");\n\t/*\n\t * At this point the primary processor is completely\n\t * in the debugger and all secondary CPUs are quiescent\n\t */\n\tdbg_deactivate_sw_breakpoints();\n\tkgdb_single_step = 0;\n\tkgdb_contthread = current;\n\texception_level = 0;\n\ttrace_on = tracing_is_on();\n\tif (trace_on)\n\t\ttracing_off();\n\twhile (1) {\ncpu_master_loop:\n\t\tif (dbg_kdb_mode) {\n\t\t\tkgdb_connected = 1;\n\t\t\terror = kdb_stub(ks);\n\t\t\tif (error == -1)\n\t\t\t\tcontinue;\n\t\t\tkgdb_connected = 0;\n\t\t} else {\n\t\t\terror = gdb_serial_stub(ks);\n\t\t}\n\t\tif (error == DBG_PASS_EVENT) {\n\t\t\tdbg_kdb_mode = !dbg_kdb_mode;\n\t\t} else if (error == DBG_SWITCH_CPU_EVENT) {\n\t\t\tkgdb_info[dbg_switch_cpu].exception_state |=\n\t\t\t\tDCPU_NEXT_MASTER;\n\t\t\tgoto cpu_loop;\n\t\t} else {\n\t\t\tkgdb_info[cpu].ret_state = error;\n\t\t\tbreak;\n\t\t}\n\t}\n\tdbg_activate_sw_breakpoints();\n\t/* Call the I/O driver's post_exception routine */\n\tif (dbg_io_ops->post_exception)\n\t\tdbg_io_ops->post_exception();\n\tatomic_dec(&ignore_console_lock_warning);\n\tif (!kgdb_single_step) {\n\t\traw_spin_unlock(&dbg_slave_lock);\n\t\t/* Wait till all the CPUs have quit from the debugger. */\n\t\twhile (kgdb_do_roundup && atomic_read(&slaves_in_kgdb))\n\t\t\tcpu_relax();\n\t}\nkgdb_restore:\n\tif (atomic_read(&kgdb_cpu_doing_single_step) != -1) {\n\t\tint sstep_cpu = atomic_read(&kgdb_cpu_doing_single_step);\n\t\tif (kgdb_info[sstep_cpu].task)\n\t\t\tkgdb_sstep_pid = kgdb_info[sstep_cpu].task->pid;\n\t\telse\n\t\t\tkgdb_sstep_pid = 0;\n\t}\n\tif (arch_kgdb_ops.correct_hw_break)\n\t\tarch_kgdb_ops.correct_hw_break();\n\tif (trace_on)\n\t\ttracing_on();\n\tkgdb_info[cpu].debuggerinfo = NULL;\n\tkgdb_info[cpu].task = NULL;\n\tkgdb_info[cpu].exception_state &=\n\t\t~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);\n\tkgdb_info[cpu].enter_kgdb--;\n\tsmp_mb__before_atomic();\n\tatomic_dec(&masters_in_kgdb);\n\t/* Free kgdb_active */\n\tatomic_set(&kgdb_active, -1);\n\traw_spin_unlock(&dbg_master_lock);\n\tdbg_touch_watchdogs();\n\tlocal_irq_restore(flags);\n\trcu_read_unlock();\n\treturn kgdb_info[cpu].ret_state;\n}",
        "bug_line_number": [
            46,
            47
        ],
        "vul": 1,
        "id": 4
    },
    {
        "code": "static inline int l2cap_config_req(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_conf_req *req = (struct l2cap_conf_req *) data;\n\tu16 dcid, flags;\n\tu8 rsp[64];\n\tstruct l2cap_chan *chan;\n\tint len, err = 0;\n\tif (cmd_len < sizeof(*req))\n\t\treturn -EPROTO;\n\tdcid  = __le16_to_cpu(req->dcid);\n\tflags = __le16_to_cpu(req->flags);\n\tBT_DBG(\"dcid 0x%4.4x flags 0x%2.2x\", dcid, flags);\n\tchan = l2cap_get_chan_by_scid(conn, dcid);\n\tif (!chan) {\n\t\tcmd_reject_invalid_cid(conn, cmd->ident, dcid, 0);\n\t\treturn 0;\n\t}\n\tif (chan->state != BT_CONFIG && chan->state != BT_CONNECT2) {\n\t\tcmd_reject_invalid_cid(conn, cmd->ident, chan->scid,\n\t\t\t\t       chan->dcid);\n\t\tgoto unlock;\n\t}\n\t/* Reject if config buffer is too small. */\n\tlen = cmd_len - sizeof(*req);\n\tif (chan->conf_len + len > sizeof(chan->conf_req)) {\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\n\t\t\t       l2cap_build_conf_rsp(chan, rsp,\n\t\t\t       L2CAP_CONF_REJECT, flags), rsp);\n\t\tgoto unlock;\n\t}\n\t/* Store config. */\n\tmemcpy(chan->conf_req + chan->conf_len, req->data, len);\n\tchan->conf_len += len;\n\tif (flags & L2CAP_CONF_FLAG_CONTINUATION) {\n\t\t/* Incomplete config. Send empty response. */\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\n\t\t\t       l2cap_build_conf_rsp(chan, rsp,\n\t\t\t       L2CAP_CONF_SUCCESS, flags), rsp);\n\t\tgoto unlock;\n\t}\n\t/* Complete config. */\n\tlen = l2cap_parse_conf_req(chan, rsp);\n\tif (len < 0) {\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto unlock;\n\t}\n\tchan->ident = cmd->ident;\n\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP, len, rsp);\n\tchan->num_conf_rsp++;\n\t/* Reset config buffer. */\n\tchan->conf_len = 0;\n\tif (!test_bit(CONF_OUTPUT_DONE, &chan->conf_state))\n\t\tgoto unlock;\n\tif (test_bit(CONF_INPUT_DONE, &chan->conf_state)) {\n\t\tset_default_fcs(chan);\n\t\tif (chan->mode == L2CAP_MODE_ERTM ||\n\t\t    chan->mode == L2CAP_MODE_STREAMING)\n\t\t\terr = l2cap_ertm_init(chan);\n\t\tif (err < 0)\n\t\t\tl2cap_send_disconn_req(chan, -err);\n\t\telse\n\t\t\tl2cap_chan_ready(chan);\n\t\tgoto unlock;\n\t}\n\tif (!test_and_set_bit(CONF_REQ_SENT, &chan->conf_state)) {\n\t\tu8 buf[64];\n\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn), L2CAP_CONF_REQ,\n\t\t\t       l2cap_build_conf_req(chan, buf), buf);\n\t\tchan->num_conf_req++;\n\t}\n\t/* Got Conf Rsp PENDING from remote side and assume we sent\n\t   Conf Rsp PENDING in the code above */\n\tif (test_bit(CONF_REM_CONF_PEND, &chan->conf_state) &&\n\t    test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\n\t\t/* check compatibility */\n\t\t/* Send rsp for BR/EDR channel */\n\t\tif (!chan->hs_hcon)\n\t\t\tl2cap_send_efs_conf_rsp(chan, rsp, cmd->ident, flags);\n\t\telse\n\t\t\tchan->ident = cmd->ident;\n\t}\nunlock:\n\tl2cap_chan_unlock(chan);\n\treturn err;\n}",
        "bug_line_number": [
            43,
            44,
            69,
            70
        ],
        "vul": 1,
        "id": 6
    },
    {
        "code": "static void l2cap_security_cfm(struct hci_conn *hcon, u8 status, u8 encrypt)\n{\n\tstruct l2cap_conn *conn = hcon->l2cap_data;\n\tstruct l2cap_chan *chan;\n\tif (!conn)\n\t\treturn;\n\tBT_DBG(\"conn %p status 0x%2.2x encrypt %u\", conn, status, encrypt);\n\tmutex_lock(&conn->chan_lock);\n\tlist_for_each_entry(chan, &conn->chan_l, list) {\n\t\tl2cap_chan_lock(chan);\n\t\tBT_DBG(\"chan %p scid 0x%4.4x state %s\", chan, chan->scid,\n\t\t       state_to_string(chan->state));\n\t\tif (chan->scid == L2CAP_CID_A2MP) {\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!status && encrypt)\n\t\t\tchan->sec_level = hcon->sec_level;\n\t\tif (!__l2cap_no_conn_pending(chan)) {\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!status && (chan->state == BT_CONNECTED ||\n\t\t\t\tchan->state == BT_CONFIG)) {\n\t\t\tchan->ops->resume(chan);\n\t\t\tl2cap_check_encryption(chan, encrypt);\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\t\tif (chan->state == BT_CONNECT) {\n\t\t\tif (!status)\n\t\t\t\tl2cap_start_connection(chan);\n\t\t\telse\n\t\t\t\t__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\n\t\t} else if (chan->state == BT_CONNECT2 &&\n\t\t\t   chan->mode != L2CAP_MODE_LE_FLOWCTL) {\n\t\t\tstruct l2cap_conn_rsp rsp;\n\t\t\t__u16 res, stat;\n\t\t\tif (!status) {\n\t\t\t\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\t\t\t\tres = L2CAP_CR_PEND;\n\t\t\t\t\tstat = L2CAP_CS_AUTHOR_PEND;\n\t\t\t\t\tchan->ops->defer(chan);\n\t\t\t\t} else {\n\t\t\t\t\tl2cap_state_change(chan, BT_CONFIG);\n\t\t\t\t\tres = L2CAP_CR_SUCCESS;\n\t\t\t\t\tstat = L2CAP_CS_NO_INFO;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tl2cap_state_change(chan, BT_DISCONN);\n\t\t\t\t__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\n\t\t\t\tres = L2CAP_CR_SEC_BLOCK;\n\t\t\t\tstat = L2CAP_CS_NO_INFO;\n\t\t\t}\n\t\t\trsp.scid   = cpu_to_le16(chan->dcid);\n\t\t\trsp.dcid   = cpu_to_le16(chan->scid);\n\t\t\trsp.result = cpu_to_le16(res);\n\t\t\trsp.status = cpu_to_le16(stat);\n\t\t\tl2cap_send_cmd(conn, chan->ident, L2CAP_CONN_RSP,\n\t\t\t\t       sizeof(rsp), &rsp);\n\t\t\tif (!test_bit(CONF_REQ_SENT, &chan->conf_state) &&\n\t\t\t    res == L2CAP_CR_SUCCESS) {\n\t\t\t\tchar buf[128];\n\t\t\t\tset_bit(CONF_REQ_SENT, &chan->conf_state);\n\t\t\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn),\n\t\t\t\t\t       L2CAP_CONF_REQ,\n\t\t\t\t\t       l2cap_build_conf_req(chan, buf),\n\t\t\t\t\t       buf);\n\t\t\t\tchan->num_conf_req++;\n\t\t\t}\n\t\t}\n\t\tl2cap_chan_unlock(chan);\n\t}\n\tmutex_unlock(&conn->chan_lock);\n}",
        "bug_line_number": [
            66,
            67
        ],
        "vul": 1,
        "id": 8
    },
    {
        "code": "static int __do_proc_dointvec(void *tbl_data, struct ctl_table *table,\n\t\t  int write, void *buffer,\n\t\t  size_t *lenp, loff_t *ppos,\n\t\t  int (*conv)(bool *negp, unsigned long *lvalp, int *valp,\n\t\t\t      int write, void *data),\n\t\t  void *data)\n{\n\tint *i, vleft, first = 1, err = 0;\n\tsize_t left;\n\tchar *p;\n\tif (!tbl_data || !table->maxlen || !*lenp || (*ppos && !write)) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\ti = (int *) tbl_data;\n\tvleft = table->maxlen / sizeof(*i);\n\tleft = *lenp;\n\tif (!conv)\n\t\tconv = do_proc_dointvec_conv;\n\tif (write) {\n\t\tif (proc_first_pos_non_zero_ignore(ppos, table))\n\t\t\tgoto out;\n\t\tif (left > PAGE_SIZE - 1)\n\t\t\tleft = PAGE_SIZE - 1;\n\t\tp = buffer;\n\t}\n\tfor (; left && vleft--; i++, first=0) {\n\t\tunsigned long lval;\n\t\tbool neg;\n\t\tif (write) {\n\t\t\tleft -= proc_skip_spaces(&p);\n\t\t\tif (!left)\n\t\t\t\tbreak;\n\t\t\terr = proc_get_long(&p, &left, &lval, &neg,\n\t\t\t\t\t     proc_wspace_sep,\n\t\t\t\t\t     sizeof(proc_wspace_sep), NULL);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (conv(&neg, &lval, i, 1, data)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tif (conv(&neg, &lval, i, 0, data)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!first)\n\t\t\t\tproc_put_char(&buffer, &left, '\\t');\n\t\t\tproc_put_long(&buffer, &left, lval, neg);\n\t\t}\n\t}\n\tif (!write && !first && left && !err)\n\t\tproc_put_char(&buffer, &left, '\\n');\n\tif (write && !err && left)\n\t\tleft -= proc_skip_spaces(&p);\n\tif (write && first)\n\t\treturn err ? : -EINVAL;\n\t*lenp -= left;\nout:\n\t*ppos += *lenp;\n\treturn err;\n}",
        "bug_line_number": [
            30,
            31,
            55,
            56
        ],
        "vul": 1,
        "id": 10
    },
    {
        "code": "static void perf_group_attach(struct perf_event *event)\n{\n\tstruct perf_event *group_leader = event->group_leader, *pos;\n\tlockdep_assert_held(&event->ctx->lock);\n\t/*\n\t * We can have double attach due to group movement (move_group) in\n\t * perf_event_open().\n\t */\n\tif (event->attach_state & PERF_ATTACH_GROUP)\n\t\treturn;\n\tevent->attach_state |= PERF_ATTACH_GROUP;\n\tif (group_leader == event)\n\t\treturn;\n\tWARN_ON_ONCE(group_leader->ctx != event->ctx);\n\tgroup_leader->group_caps &= event->event_caps;\n\tlist_add_tail(&event->sibling_list, &group_leader->sibling_list);\n\tgroup_leader->nr_siblings++;\n\tperf_event__header_size(group_leader);\n\tfor_each_sibling_event(pos, group_leader)\n\t\tperf_event__header_size(pos);\n}",
        "bug_line_number": [
            16,
            17
        ],
        "vul": 1,
        "id": 12
    },
    {
        "code": "static void perf_group_detach(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct perf_event *sibling, *tmp;\n\tstruct perf_event_context *ctx = event->ctx;\n\tlockdep_assert_held(&ctx->lock);\n\t/*\n\t * We can have double detach due to exit/hot-unplug + close.\n\t */\n\tif (!(event->attach_state & PERF_ATTACH_GROUP))\n\t\treturn;\n\tevent->attach_state &= ~PERF_ATTACH_GROUP;\n\tperf_put_aux_event(event);\n\t/*\n\t * If this is a sibling, remove it from its group.\n\t */\n\tif (leader != event) {\n\t\tlist_del_init(&event->sibling_list);\n\t\tevent->group_leader->nr_siblings--;\n\t\tgoto out;\n\t}\n\t/*\n\t * If this was a group event with sibling events then\n\t * upgrade the siblings to singleton events by adding them\n\t * to whatever list we are on.\n\t */\n\tlist_for_each_entry_safe(sibling, tmp, &event->sibling_list, sibling_list) {\n\t\tif (sibling->event_caps & PERF_EV_CAP_SIBLING)\n\t\t\tperf_remove_sibling_event(sibling);\n\t\tsibling->group_leader = sibling;\n\t\tlist_del_init(&sibling->sibling_list);\n\t\t/* Inherit group flags from the previous leader */\n\t\tsibling->group_caps = event->group_caps;\n\t\tif (sibling->attach_state & PERF_ATTACH_CONTEXT) {\n\t\t\tadd_event_to_groups(sibling, event->ctx);\n\t\t\tif (sibling->state == PERF_EVENT_STATE_ACTIVE)\n\t\t\t\tlist_add_tail(&sibling->active_list, get_event_list(sibling));\n\t\t}\n\t\tWARN_ON_ONCE(sibling->ctx != event->ctx);\n\t}\nout:\n\tfor_each_sibling_event(tmp, leader)\n\t\tperf_event__header_size(tmp);\n\tperf_event__header_size(leader);\n}",
        "bug_line_number": [
            18,
            19
        ],
        "vul": 1,
        "id": 14
    },
    {
        "code": "static int perf_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *child;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tint ret;\n\tu64 *values;\n\tlockdep_assert_held(&ctx->mutex);\n\tvalues = kzalloc(event->read_size, GFP_KERNEL);\n\tif (!values)\n\t\treturn -ENOMEM;\n\tvalues[0] = 1 + leader->nr_siblings;\n\t/*\n\t * By locking the child_mutex of the leader we effectively\n\t * lock the child list of all siblings.. XXX explain how.\n\t */\n\tmutex_lock(&leader->child_mutex);\n\tret = __perf_read_group_add(leader, read_format, values);\n\tif (ret)\n\t\tgoto unlock;\n\tlist_for_each_entry(child, &leader->child_list, child_list) {\n\t\tret = __perf_read_group_add(child, read_format, values);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\t}\n\tmutex_unlock(&leader->child_mutex);\n\tret = event->read_size;\n\tif (copy_to_user(buf, values, event->read_size))\n\t\tret = -EFAULT;\n\tgoto out;\nunlock:\n\tmutex_unlock(&leader->child_mutex);\nout:\n\tkfree(values);\n\treturn ret;\n}",
        "bug_line_number": [
            12,
            13,
            14,
            15
        ],
        "vul": 1,
        "id": 16
    },
    {
        "code": "int iscsi_session_get_param(struct iscsi_cls_session *cls_session,\n\t\t\t    enum iscsi_param param, char *buf)\n{\n\tstruct iscsi_session *session = cls_session->dd_data;\n\tint len;\n\tswitch(param) {\n\tcase ISCSI_PARAM_FAST_ABORT:\n\t\tlen = sprintf(buf, \"%d\\n\", session->fast_abort);\n\t\tbreak;\n\tcase ISCSI_PARAM_ABORT_TMO:\n\t\tlen = sprintf(buf, \"%d\\n\", session->abort_timeout);\n\t\tbreak;\n\tcase ISCSI_PARAM_LU_RESET_TMO:\n\t\tlen = sprintf(buf, \"%d\\n\", session->lu_reset_timeout);\n\t\tbreak;\n\tcase ISCSI_PARAM_TGT_RESET_TMO:\n\t\tlen = sprintf(buf, \"%d\\n\", session->tgt_reset_timeout);\n\t\tbreak;\n\tcase ISCSI_PARAM_INITIAL_R2T_EN:\n\t\tlen = sprintf(buf, \"%d\\n\", session->initial_r2t_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_MAX_R2T:\n\t\tlen = sprintf(buf, \"%hu\\n\", session->max_r2t);\n\t\tbreak;\n\tcase ISCSI_PARAM_IMM_DATA_EN:\n\t\tlen = sprintf(buf, \"%d\\n\", session->imm_data_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_FIRST_BURST:\n\t\tlen = sprintf(buf, \"%u\\n\", session->first_burst);\n\t\tbreak;\n\tcase ISCSI_PARAM_MAX_BURST:\n\t\tlen = sprintf(buf, \"%u\\n\", session->max_burst);\n\t\tbreak;\n\tcase ISCSI_PARAM_PDU_INORDER_EN:\n\t\tlen = sprintf(buf, \"%d\\n\", session->pdu_inorder_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_DATASEQ_INORDER_EN:\n\t\tlen = sprintf(buf, \"%d\\n\", session->dataseq_inorder_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_DEF_TASKMGMT_TMO:\n\t\tlen = sprintf(buf, \"%d\\n\", session->def_taskmgmt_tmo);\n\t\tbreak;\n\tcase ISCSI_PARAM_ERL:\n\t\tlen = sprintf(buf, \"%d\\n\", session->erl);\n\t\tbreak;\n\tcase ISCSI_PARAM_TARGET_NAME:\n\t\tlen = sprintf(buf, \"%s\\n\", session->targetname);\n\t\tbreak;\n\tcase ISCSI_PARAM_TARGET_ALIAS:\n\t\tlen = sprintf(buf, \"%s\\n\", session->targetalias);\n\t\tbreak;\n\tcase ISCSI_PARAM_TPGT:\n\t\tlen = sprintf(buf, \"%d\\n\", session->tpgt);\n\t\tbreak;\n\tcase ISCSI_PARAM_USERNAME:\n\t\tlen = sprintf(buf, \"%s\\n\", session->username);\n\t\tbreak;\n\tcase ISCSI_PARAM_USERNAME_IN:\n\t\tlen = sprintf(buf, \"%s\\n\", session->username_in);\n\t\tbreak;\n\tcase ISCSI_PARAM_PASSWORD:\n\t\tlen = sprintf(buf, \"%s\\n\", session->password);\n\t\tbreak;\n\tcase ISCSI_PARAM_PASSWORD_IN:\n\t\tlen = sprintf(buf, \"%s\\n\", session->password_in);\n\t\tbreak;\n\tcase ISCSI_PARAM_IFACE_NAME:\n\t\tlen = sprintf(buf, \"%s\\n\", session->ifacename);\n\t\tbreak;\n\tcase ISCSI_PARAM_INITIATOR_NAME:\n\t\tlen = sprintf(buf, \"%s\\n\", session->initiatorname);\n\t\tbreak;\n\tcase ISCSI_PARAM_BOOT_ROOT:\n\t\tlen = sprintf(buf, \"%s\\n\", session->boot_root);\n\t\tbreak;\n\tcase ISCSI_PARAM_BOOT_NIC:\n\t\tlen = sprintf(buf, \"%s\\n\", session->boot_nic);\n\t\tbreak;\n\tcase ISCSI_PARAM_BOOT_TARGET:\n\t\tlen = sprintf(buf, \"%s\\n\", session->boot_target);\n\t\tbreak;\n\tcase ISCSI_PARAM_AUTO_SND_TGT_DISABLE:\n\t\tlen = sprintf(buf, \"%u\\n\", session->auto_snd_tgt_disable);\n\t\tbreak;\n\tcase ISCSI_PARAM_DISCOVERY_SESS:\n\t\tlen = sprintf(buf, \"%u\\n\", session->discovery_sess);\n\t\tbreak;\n\tcase ISCSI_PARAM_PORTAL_TYPE:\n\t\tlen = sprintf(buf, \"%s\\n\", session->portal_type);\n\t\tbreak;\n\tcase ISCSI_PARAM_CHAP_AUTH_EN:\n\t\tlen = sprintf(buf, \"%u\\n\", session->chap_auth_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_DISCOVERY_LOGOUT_EN:\n\t\tlen = sprintf(buf, \"%u\\n\", session->discovery_logout_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_BIDI_CHAP_EN:\n\t\tlen = sprintf(buf, \"%u\\n\", session->bidi_chap_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_DISCOVERY_AUTH_OPTIONAL:\n\t\tlen = sprintf(buf, \"%u\\n\", session->discovery_auth_optional);\n\t\tbreak;\n\tcase ISCSI_PARAM_DEF_TIME2WAIT:\n\t\tlen = sprintf(buf, \"%d\\n\", session->time2wait);\n\t\tbreak;\n\tcase ISCSI_PARAM_DEF_TIME2RETAIN:\n\t\tlen = sprintf(buf, \"%d\\n\", session->time2retain);\n\t\tbreak;\n\tcase ISCSI_PARAM_TSID:\n\t\tlen = sprintf(buf, \"%u\\n\", session->tsid);\n\t\tbreak;\n\tcase ISCSI_PARAM_ISID:\n\t\tlen = sprintf(buf, \"%02x%02x%02x%02x%02x%02x\\n\",\n\t\t\t      session->isid[0], session->isid[1],\n\t\t\t      session->isid[2], session->isid[3],\n\t\t\t      session->isid[4], session->isid[5]);\n\t\tbreak;\n\tcase ISCSI_PARAM_DISCOVERY_PARENT_IDX:\n\t\tlen = sprintf(buf, \"%u\\n\", session->discovery_parent_idx);",
        "bug_line_number": [
            7,
            8,
            10,
            11,
            13,
            14,
            16,
            17,
            19,
            20,
            22,
            23,
            25,
            26,
            28,
            29,
            31,
            32,
            34,
            35,
            37,
            38,
            40,
            41,
            43,
            44,
            46,
            47,
            49,
            50,
            52,
            53,
            55,
            56,
            58,
            59,
            61,
            62,
            64,
            65,
            67,
            68,
            70,
            71,
            73,
            74,
            76,
            77,
            79,
            80,
            82,
            83,
            85,
            86,
            88,
            89,
            91,
            92,
            94,
            95,
            97,
            98,
            100,
            101,
            103,
            104,
            106,
            107,
            109,
            110,
            112,
            113,
            118
        ],
        "vul": 1,
        "id": 18
    },
    {
        "code": "\t\tbreak;\n\tcase ISCSI_PARAM_DISCOVERY_PARENT_TYPE:\n\t\tif (session->discovery_parent_type)\n\t\t\tlen = sprintf(buf, \"%s\\n\",\n\t\t\t\t      session->discovery_parent_type);\n\t\telse\n\t\t\tlen = sprintf(buf, \"\\n\");\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOSYS;\n\t}\n\treturn len;\n}",
        "bug_line_number": [
            0,
            3,
            4,
            6,
            7
        ],
        "vul": 1,
        "id": 18
    },
    {
        "code": "static unsigned long fbcon_getxy(struct vc_data *vc, unsigned long pos,\n\t\t\t\t int *px, int *py)\n{\n\tunsigned long ret;\n\tint x, y;\n\tif (pos >= vc->vc_origin && pos < vc->vc_scr_end) {\n\t\tunsigned long offset = (pos - vc->vc_origin) / 2;\n\t\tx = offset % vc->vc_cols;\n\t\ty = offset / vc->vc_cols;\n\t\tif (vc->vc_num == fg_console)\n\t\t\ty += softback_lines;\n\t\tret = pos + (vc->vc_cols - x) * 2;\n\t} else if (vc->vc_num == fg_console && softback_lines) {\n\t\tunsigned long offset = pos - softback_curr;\n\t\tif (pos < softback_curr)\n\t\t\toffset += softback_end - softback_buf;\n\t\toffset /= 2;\n\t\tx = offset % vc->vc_cols;\n\t\ty = offset / vc->vc_cols;\n\t\tret = pos + (vc->vc_cols - x) * 2;\n\t\tif (ret == softback_end)\n\t\t\tret = softback_buf;\n\t\tif (ret == softback_in)\n\t\t\tret = vc->vc_origin;\n\t} else {\n\t\t/* Should not happen */\n\t\tx = y = 0;\n\t\tret = vc->vc_origin;\n\t}\n\tif (px)\n\t\t*px = x;\n\tif (py)\n\t\t*py = y;\n\treturn ret;\n}",
        "bug_line_number": [
            9,
            10,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23
        ],
        "vul": 1,
        "id": 20
    },
    {
        "code": "static void can_can_gw_rcv(struct sk_buff *skb, void *data)\n{\n\tstruct cgw_job *gwj = (struct cgw_job *)data;\n\tstruct can_frame *cf;\n\tstruct sk_buff *nskb;\n\tint modidx = 0;\n\t/*\n\t * Do not handle CAN frames routed more than 'max_hops' times.\n\t * In general we should never catch this delimiter which is intended\n\t * to cover a misconfiguration protection (e.g. circular CAN routes).\n\t *\n\t * The Controller Area Network controllers only accept CAN frames with\n\t * correct CRCs - which are not visible in the controller registers.\n\t * According to skbuff.h documentation the csum_start element for IP\n\t * checksums is undefined/unused when ip_summed == CHECKSUM_UNNECESSARY.\n\t * Only CAN skbs can be processed here which already have this property.\n\t */\n#define cgw_hops(skb) ((skb)->csum_start)\n\tBUG_ON(skb->ip_summed != CHECKSUM_UNNECESSARY);\n\tif (cgw_hops(skb) >= max_hops) {\n\t\t/* indicate deleted frames due to misconfiguration */\n\t\tgwj->deleted_frames++;\n\t\treturn;\n\t}\n\tif (!(gwj->dst.dev->flags & IFF_UP)) {\n\t\tgwj->dropped_frames++;\n\t\treturn;\n\t}\n\t/* is sending the skb back to the incoming interface not allowed? */\n\tif (!(gwj->flags & CGW_FLAGS_CAN_IIF_TX_OK) &&\n\t    can_skb_prv(skb)->ifindex == gwj->dst.dev->ifindex)\n\t\treturn;\n\t/*\n\t * clone the given skb, which has not been done in can_rcv()\n\t *\n\t * When there is at least one modification function activated,\n\t * we need to copy the skb as we want to modify skb->data.\n\t */\n\tif (gwj->mod.modfunc[0])\n\t\tnskb = skb_copy(skb, GFP_ATOMIC);\n\telse\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\tif (!nskb) {\n\t\tgwj->dropped_frames++;\n\t\treturn;\n\t}\n\t/* put the incremented hop counter in the cloned skb */\n\tcgw_hops(nskb) = cgw_hops(skb) + 1;\n\t/* first processing of this CAN frame -> adjust to private hop limit */\n\tif (gwj->limit_hops && cgw_hops(nskb) == 1)\n\t\tcgw_hops(nskb) = max_hops - gwj->limit_hops + 1;\n\tnskb->dev = gwj->dst.dev;\n\t/* pointer to modifiable CAN frame */\n\tcf = (struct can_frame *)nskb->data;\n\t/* perform preprocessed modification functions if there are any */\n\twhile (modidx < MAX_MODFUNCTIONS && gwj->mod.modfunc[modidx])\n\t\t(*gwj->mod.modfunc[modidx++])(cf, &gwj->mod);\n\t/* check for checksum updates when the CAN frame has been modified */\n\tif (modidx) {\n\t\tif (gwj->mod.csumfunc.crc8)\n\t\t\t(*gwj->mod.csumfunc.crc8)(cf, &gwj->mod.csum.crc8);\n\t\tif (gwj->mod.csumfunc.xor)\n\t\t\t(*gwj->mod.csumfunc.xor)(cf, &gwj->mod.csum.xor);\n\t}\n\t/* clear the skb timestamp if not configured the other way */\n\tif (!(gwj->flags & CGW_FLAGS_CAN_SRC_TSTAMP))\n\t\tnskb->tstamp = 0;\n\t/* send to netdevice */\n\tif (can_send(nskb, gwj->flags & CGW_FLAGS_CAN_ECHO))\n\t\tgwj->dropped_frames++;\n\telse\n\t\tgwj->handled_frames++;\n}",
        "bug_line_number": [
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            71,
            72
        ],
        "vul": 1,
        "id": 22
    },
    {
        "code": "static void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode, bool is_jmp32)\n{\n\ts64 sval;\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\tval = is_jmp32 ? (u32)val : val;\n\tsval = is_jmp32 ? (s64)(s32)val : (s64)val;\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\tcase BPF_JNE:\n\t{\n\t\tstruct bpf_reg_state *reg =\n\t\t\topcode == BPF_JEQ ? true_reg : false_reg;\n\t\t/* For BPF_JEQ, if this is false we know nothing Jon Snow, but\n\t\t * if it is true we know the value for sure. Likewise for\n\t\t * BPF_JNE.\n\t\t */\n\t\tif (is_jmp32) {\n\t\t\tu64 old_v = reg->var_off.value;\n\t\t\tu64 hi_mask = ~0xffffffffULL;\n\t\t\treg->var_off.value = (old_v & hi_mask) | val;\n\t\t\treg->var_off.mask &= hi_mask;\n\t\t} else {\n\t\t\t__mark_reg_known(reg, val);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGE:\n\tcase BPF_JGT:\n\t{\n\t\tu64 false_umax = opcode == BPF_JGT ? val    : val - 1;\n\t\tu64 true_umin = opcode == BPF_JGT ? val + 1 : val;\n\t\tif (is_jmp32) {\n\t\t\tfalse_umax += gen_hi_max(false_reg->var_off);\n\t\t\ttrue_umin += gen_hi_min(true_reg->var_off);\n\t\t}\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, false_umax);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, true_umin);\n\t\tbreak;\n\t}\n\tcase BPF_JSGE:\n\tcase BPF_JSGT:\n\t{\n\t\ts64 false_smax = opcode == BPF_JSGT ? sval    : sval - 1;\n\t\ts64 true_smin = opcode == BPF_JSGT ? sval + 1 : sval;\n\t\t/* If the full s64 was not sign-extended from s32 then don't\n\t\t * deduct further info.\n\t\t */\n\t\tif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\n\t\t\tbreak;\n\t\tfalse_reg->smax_value = min(false_reg->smax_value, false_smax);\n\t\ttrue_reg->smin_value = max(true_reg->smin_value, true_smin);\n\t\tbreak;\n\t}\n\tcase BPF_JLE:\n\tcase BPF_JLT:\n\t{\n\t\tu64 false_umin = opcode == BPF_JLT ? val    : val + 1;\n\t\tu64 true_umax = opcode == BPF_JLT ? val - 1 : val;\n\t\tif (is_jmp32) {\n\t\t\tfalse_umin += gen_hi_min(false_reg->var_off);\n\t\t\ttrue_umax += gen_hi_max(true_reg->var_off);\n\t\t}\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, false_umin);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, true_umax);\n\t\tbreak;\n\t}\n\tcase BPF_JSLE:\n\tcase BPF_JSLT:\n\t{\n\t\ts64 false_smin = opcode == BPF_JSLT ? sval    : sval + 1;\n\t\ts64 true_smax = opcode == BPF_JSLT ? sval - 1 : sval;\n\t\tif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\n\t\t\tbreak;\n\t\tfalse_reg->smin_value = max(false_reg->smin_value, false_smin);\n\t\ttrue_reg->smax_value = min(true_reg->smax_value, true_smax);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\tif (is_jmp32) {\n\t\t__reg_bound_offset32(false_reg);\n\t\t__reg_bound_offset32(true_reg);\n\t}\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);",
        "bug_line_number": [
            101,
            102,
            103,
            104
        ],
        "vul": 1,
        "id": 24
    },
    {
        "code": "static int hash__init_new_context(struct mm_struct *mm)\n{\n\tint index;\n\tindex = hash__alloc_context_id();\n\tif (index < 0)\n\t\treturn index;\n\t/*\n\t * The old code would re-promote on fork, we don't do that when using\n\t * slices as it could cause problem promoting slices that have been\n\t * forced down to 4K.\n\t *\n\t * For book3s we have MMU_NO_CONTEXT set to be ~0. Hence check\n\t * explicitly against context.id == 0. This ensures that we properly\n\t * initialize context slice details for newly allocated mm's (which will\n\t * have id == 0) and don't alter context slice inherited via fork (which\n\t * will have id != 0).\n\t *\n\t * We should not be calling init_new_context() on init_mm. Hence a\n\t * check against 0 is OK.\n\t */\n\tif (mm->context.id == 0)\n\t\tslice_init_new_context_exec(mm);\n\tsubpage_prot_init_new_context(mm);\n\tpkey_mm_init(mm);\n\treturn index;\n}",
        "bug_line_number": [
            3,
            4,
            5,
            21,
            22
        ],
        "vul": 1,
        "id": 26
    },
    {
        "code": "static int size_entry_mwt(struct ebt_entry *entry, const unsigned char *base,\n\t\t\t  unsigned int *total,\n\t\t\t  struct ebt_entries_buf_state *state)\n{\n\tunsigned int i, j, startoff, new_offset = 0;\n\t/* stores match/watchers/targets & offset of next struct ebt_entry: */\n\tunsigned int offsets[4];\n\tunsigned int *offsets_update = NULL;\n\tint ret;\n\tchar *buf_start;\n\tif (*total < sizeof(struct ebt_entries))\n\t\treturn -EINVAL;\n\tif (!entry->bitmask) {\n\t\t*total -= sizeof(struct ebt_entries);\n\t\treturn ebt_buf_add(state, entry, sizeof(struct ebt_entries));\n\t}\n\tif (*total < sizeof(*entry) || entry->next_offset < sizeof(*entry))\n\t\treturn -EINVAL;\n\tstartoff = state->buf_user_offset;\n\t/* pull in most part of ebt_entry, it does not need to be changed. */\n\tret = ebt_buf_add(state, entry,\n\t\t\toffsetof(struct ebt_entry, watchers_offset));\n\tif (ret < 0)\n\t\treturn ret;\n\toffsets[0] = sizeof(struct ebt_entry); /* matches come first */\n\tmemcpy(&offsets[1], &entry->watchers_offset,\n\t\t\tsizeof(offsets) - sizeof(offsets[0]));\n\tif (state->buf_kern_start) {\n\t\tbuf_start = state->buf_kern_start + state->buf_kern_offset;\n\t\toffsets_update = (unsigned int *) buf_start;\n\t}\n\tret = ebt_buf_add(state, &offsets[1],\n\t\t\tsizeof(offsets) - sizeof(offsets[0]));\n\tif (ret < 0)\n\t\treturn ret;\n\tbuf_start = (char *) entry;\n\t/* 0: matches offset, always follows ebt_entry.\n\t * 1: watchers offset, from ebt_entry structure\n\t * 2: target offset, from ebt_entry structure\n\t * 3: next ebt_entry offset, from ebt_entry structure\n\t *\n\t * offsets are relative to beginning of struct ebt_entry (i.e., 0).\n\t */\n\tfor (i = 0, j = 1 ; j < 4 ; j++, i++) {\n\t\tstruct compat_ebt_entry_mwt *match32;\n\t\tunsigned int size;\n\t\tchar *buf = buf_start + offsets[i];\n\t\tif (offsets[i] > offsets[j])\n\t\t\treturn -EINVAL;\n\t\tmatch32 = (struct compat_ebt_entry_mwt *) buf;\n\t\tsize = offsets[j] - offsets[i];\n\t\tret = ebt_size_mwt(match32, size, i, state, base);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tnew_offset += ret;\n\t\tif (offsets_update && new_offset) {\n\t\t\tpr_debug(\"change offset %d to %d\\n\",\n\t\t\t\toffsets_update[i], offsets[j] + new_offset);\n\t\t\toffsets_update[i] = offsets[j] + new_offset;\n\t\t}\n\t}\n\tif (state->buf_kern_start == NULL) {\n\t\tunsigned int offset = buf_start - (char *) base;\n\t\tret = xt_compat_add_offset(NFPROTO_BRIDGE, offset, new_offset);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\tstartoff = state->buf_user_offset - startoff;\n\tif (WARN_ON(*total < startoff))\n\t\treturn -EINVAL;\n\t*total -= startoff;\n\treturn 0;\n}",
        "bug_line_number": [
            42,
            43
        ],
        "vul": 1,
        "id": 28
    },
    {
        "code": "static ssize_t dp_link_settings_write(struct file *f, const char __user *buf,\n\t\t\t\t size_t size, loff_t *pos)\n{\n\tstruct amdgpu_dm_connector *connector = file_inode(f)->i_private;\n\tstruct dc_link *link = connector->dc_link;\n\tstruct dc_link_settings prefer_link_settings;\n\tchar *wr_buf = NULL;\n\tconst uint32_t wr_buf_size = 40;\n\t/* 0: lane_count; 1: link_rate */\n\tint max_param_num = 2;\n\tuint8_t param_nums = 0;\n\tlong param[2];\n\tbool valid_input = true;\n\tif (size == 0)\n\t\treturn -EINVAL;\n\twr_buf = kcalloc(wr_buf_size, sizeof(char), GFP_KERNEL);\n\tif (!wr_buf)\n\t\treturn -ENOSPC;\n\tif (parse_write_buffer_into_params(wr_buf, size,\n\t\t\t\t\t   (long *)param, buf,\n\t\t\t\t\t   max_param_num,\n\t\t\t\t\t   &param_nums)) {\n\t\tkfree(wr_buf);\n\t\treturn -EINVAL;\n\t}\n\tif (param_nums <= 0) {\n\t\tkfree(wr_buf);\n\t\tDRM_DEBUG_DRIVER(\"user data not be read\\n\");\n\t\treturn -EINVAL;\n\t}\n\tswitch (param[0]) {\n\tcase LANE_COUNT_ONE:\n\tcase LANE_COUNT_TWO:\n\tcase LANE_COUNT_FOUR:\n\t\tbreak;\n\tdefault:\n\t\tvalid_input = false;\n\t\tbreak;\n\t}\n\tswitch (param[1]) {\n\tcase LINK_RATE_LOW:\n\tcase LINK_RATE_HIGH:\n\tcase LINK_RATE_RBR2:\n\tcase LINK_RATE_HIGH2:\n\tcase LINK_RATE_HIGH3:\n\t\tbreak;\n\tdefault:\n\t\tvalid_input = false;\n\t\tbreak;\n\t}\n\tif (!valid_input) {\n\t\tkfree(wr_buf);\n\t\tDRM_DEBUG_DRIVER(\"Invalid Input value No HW will be programmed\\n\");\n\t\treturn size;\n\t}\n\t/* save user force lane_count, link_rate to preferred settings\n\t * spread spectrum will not be changed\n\t */\n\tprefer_link_settings.link_spread = link->cur_link_settings.link_spread;\n\tprefer_link_settings.use_link_rate_set = false;\n\tprefer_link_settings.lane_count = param[0];\n\tprefer_link_settings.link_rate = param[1];\n\tdp_retrain_link_dp_test(link, &prefer_link_settings, false);\n\tkfree(wr_buf);\n\treturn size;\n}",
        "bug_line_number": [
            18,
            19
        ],
        "vul": 1,
        "id": 30
    },
    {
        "code": "static int nvme_ns_ioctl(struct nvme_ns *ns, unsigned int cmd,\n\t\tvoid __user *argp)\n{\n\tswitch (cmd) {\n\tcase NVME_IOCTL_ID:\n\t\tforce_successful_syscall_return();\n\t\treturn ns->head->ns_id;\n\tcase NVME_IOCTL_IO_CMD:\n\t\treturn nvme_user_cmd(ns->ctrl, ns, argp);\n\t/*\n\t * struct nvme_user_io can have different padding on some 32-bit ABIs.\n\t * Just accept the compat version as all fields that are used are the\n\t * same size and at the same offset.\n\t */\n#ifdef COMPAT_FOR_U64_ALIGNMENT\n\tcase NVME_IOCTL_SUBMIT_IO32:\n#endif\n\tcase NVME_IOCTL_SUBMIT_IO:\n\t\treturn nvme_submit_io(ns, argp);\n\tcase NVME_IOCTL_IO64_CMD:\n\t\treturn nvme_user_cmd64(ns->ctrl, ns, argp);\n\tdefault:\n\t\tif (!ns->ndev)\n\t\t\treturn -ENOTTY;\n\t\treturn nvme_nvm_ioctl(ns, cmd, argp);\n\t}\n}",
        "bug_line_number": [
            22,
            23,
            24,
            25
        ],
        "vul": 1,
        "id": 32
    },
    {
        "code": "static int put_chars(u32 vtermno, const char *buf, int count)\n{\n\tstruct port *port;\n\tstruct scatterlist sg[1];\n\tif (unlikely(early_put_chars))\n\t\treturn early_put_chars(vtermno, buf, count);\n\tport = find_port_by_vtermno(vtermno);\n\tif (!port)\n\t\treturn -EPIPE;\n\tsg_init_one(sg, buf, count);\n\treturn __send_to_port(port, sg, 1, count, (void *)buf, false);\n}",
        "bug_line_number": [
            3,
            4,
            9,
            10,
            11
        ],
        "vul": 1,
        "id": 34
    },
    {
        "code": "static int ipvlan_process_v4_outbound(struct sk_buff *skb)\n{\n\tconst struct iphdr *ip4h = ip_hdr(skb);\n\tstruct net_device *dev = skb->dev;\n\tstruct net *net = dev_net(dev);\n\tstruct rtable *rt;\n\tint err, ret = NET_XMIT_DROP;\n\tstruct flowi4 fl4 = {\n\t\t.flowi4_oif = dev->ifindex,\n\t\t.flowi4_tos = RT_TOS(ip4h->tos),\n\t\t.flowi4_flags = FLOWI_FLAG_ANYSRC,\n\t\t.flowi4_mark = skb->mark,\n\t\t.daddr = ip4h->daddr,\n\t\t.saddr = ip4h->saddr,\n\t};\n\trt = ip_route_output_flow(net, &fl4, NULL);\n\tif (IS_ERR(rt))\n\t\tgoto err;\n\tif (rt->rt_type != RTN_UNICAST && rt->rt_type != RTN_LOCAL) {\n\t\tip_rt_put(rt);\n\t\tgoto err;\n\t}\n\tskb_dst_set(skb, &rt->dst);\n\terr = ip_local_out(net, skb->sk, skb);\n\tif (unlikely(net_xmit_eval(err)))\n\t\tdev->stats.tx_errors++;\n\telse\n\t\tret = NET_XMIT_SUCCESS;\n\tgoto out;\nerr:\n\tdev->stats.tx_errors++;\n\tkfree_skb(skb);\nout:\n\treturn ret;\n}",
        "bug_line_number": [
            22,
            23
        ],
        "vul": 1,
        "id": 36
    },
    {
        "code": "void l2tp_tunnel_closeall(struct l2tp_tunnel *tunnel)\n{\n\tint hash;\n\tstruct hlist_node *walk;\n\tstruct hlist_node *tmp;\n\tstruct l2tp_session *session;\n\tBUG_ON(tunnel == NULL);\n\tl2tp_info(tunnel, L2TP_MSG_CONTROL, \"%s: closing all sessions...\\n\",\n\t\t  tunnel->name);\n\twrite_lock_bh(&tunnel->hlist_lock);\n\ttunnel->acpt_newsess = false;\n\tfor (hash = 0; hash < L2TP_HASH_SIZE; hash++) {\nagain:\n\t\thlist_for_each_safe(walk, tmp, &tunnel->session_hlist[hash]) {\n\t\t\tsession = hlist_entry(walk, struct l2tp_session, hlist);\n\t\t\tl2tp_info(session, L2TP_MSG_CONTROL,\n\t\t\t\t  \"%s: closing session\\n\", session->name);\n\t\t\thlist_del_init(&session->hlist);\n\t\t\tif (session->ref != NULL)\n\t\t\t\t(*session->ref)(session);\n\t\t\twrite_unlock_bh(&tunnel->hlist_lock);\n\t\t\t__l2tp_session_unhash(session);\n\t\t\tl2tp_session_queue_purge(session);\n\t\t\tif (session->session_close != NULL)\n\t\t\t\t(*session->session_close)(session);\n\t\t\tif (session->deref != NULL)\n\t\t\t\t(*session->deref)(session);\n\t\t\tl2tp_session_dec_refcount(session);\n\t\t\twrite_lock_bh(&tunnel->hlist_lock);\n\t\t\t/* Now restart from the beginning of this hash\n\t\t\t * chain.  We always remove a session from the\n\t\t\t * list so we are guaranteed to make forward\n\t\t\t * progress.\n\t\t\t */\n\t\t\tgoto again;\n\t\t}\n\t}\n\twrite_unlock_bh(&tunnel->hlist_lock);\n}",
        "bug_line_number": [
            17,
            18
        ],
        "vul": 1,
        "id": 38
    },
    {
        "code": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, netoff, hdrlen;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\tbool do_vnet = false;\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\tif (dev->header_ops) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\tsnaplen = skb->len;\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\t/* If we are flooded, just give up */\n\tif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb)\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tdo_vnet = false;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tdo_vnet = false;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\tif (po->tp_version <= TPACKET_V2) {",
        "bug_line_number": [
            11,
            12,
            67,
            68
        ],
        "vul": 1,
        "id": 40
    },
    {
        "code": "static int v4l_enum_fmt(const struct v4l2_ioctl_ops *ops,\n\t\t\t\tstruct file *file, void *fh, void *arg)\n{\n\tstruct v4l2_fmtdesc *p = arg;\n\tstruct video_device *vfd = video_devdata(file);\n\tbool is_vid = vfd->vfl_type == VFL_TYPE_GRABBER;\n\tbool is_sdr = vfd->vfl_type == VFL_TYPE_SDR;\n\tbool is_tch = vfd->vfl_type == VFL_TYPE_TOUCH;\n\tbool is_rx = vfd->vfl_dir != VFL_DIR_TX;\n\tbool is_tx = vfd->vfl_dir != VFL_DIR_RX;\n\tint ret = -EINVAL;\n\tswitch (p->type) {\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE:\n\t\tif (unlikely(!is_rx || (!is_vid && !is_tch) || !ops->vidioc_enum_fmt_vid_cap))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_vid_cap(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_enum_fmt_vid_cap_mplane))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_vid_cap_mplane(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_VIDEO_OVERLAY:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_enum_fmt_vid_overlay))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_vid_overlay(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT:\n\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_enum_fmt_vid_out))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_vid_out(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\n\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_enum_fmt_vid_out_mplane))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_vid_out_mplane(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_SDR_CAPTURE:\n\t\tif (unlikely(!is_rx || !is_sdr || !ops->vidioc_enum_fmt_sdr_cap))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_sdr_cap(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_SDR_OUTPUT:\n\t\tif (unlikely(!is_tx || !is_sdr || !ops->vidioc_enum_fmt_sdr_out))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_sdr_out(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_META_CAPTURE:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_enum_fmt_meta_cap))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_meta_cap(file, fh, arg);\n\t\tbreak;\n\t}\n\tif (ret == 0)\n\t\tv4l_fill_fmtdesc(p);\n\treturn ret;\n}",
        "bug_line_number": [
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            13,
            14,
            18,
            19,
            23,
            24,
            28,
            29,
            33,
            34,
            38,
            39,
            43,
            44,
            48,
            49
        ],
        "vul": 1,
        "id": 42
    },
    {
        "code": "static int con_font_get(struct vc_data *vc, struct console_font_op *op)\n{\n\tstruct console_font font;\n\tint rc = -EINVAL;\n\tint c;\n\tif (op->data) {\n\t\tfont.data = kmalloc(max_font_size, GFP_KERNEL);\n\t\tif (!font.data)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tfont.data = NULL;\n\tconsole_lock();\n\tif (vc->vc_mode != KD_TEXT)\n\t\trc = -EINVAL;\n\telse if (vc->vc_sw->con_font_get)\n\t\trc = vc->vc_sw->con_font_get(vc, &font);\n\telse\n\t\trc = -ENOSYS;\n\tconsole_unlock();\n\tif (rc)\n\t\tgoto out;\n\tc = (font.width+7)/8 * 32 * font.charcount;\n\tif (op->data && font.charcount > op->charcount)\n\t\trc = -ENOSPC;\n\tif (!(op->flags & KD_FONT_FLAG_OLD)) {\n\t\tif (font.width > op->width || font.height > op->height)\n\t\t\trc = -ENOSPC;\n\t} else {\n\t\tif (font.width != 8)\n\t\t\trc = -EIO;\n\t\telse if ((op->height && font.height > op->height) ||\n\t\t\t font.height > 32)\n\t\t\trc = -ENOSPC;\n\t}\n\tif (rc)\n\t\tgoto out;\n\top->height = font.height;\n\top->width = font.width;\n\top->charcount = font.charcount;\n\tif (op->data && copy_to_user(op->data, font.data, c))\n\t\trc = -EFAULT;\nout:\n\tkfree(font.data);\n\treturn rc;\n}",
        "bug_line_number": [
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34
        ],
        "vul": 1,
        "id": 44
    },
    {
        "code": "int smb_inherit_dacl(struct ksmbd_conn *conn,\n\t\t     struct path *path,\n\t\t     unsigned int uid, unsigned int gid)\n{\n\tconst struct smb_sid *psid, *creator = NULL;\n\tstruct smb_ace *parent_aces, *aces;\n\tstruct smb_acl *parent_pdacl;\n\tstruct smb_ntsd *parent_pntsd = NULL;\n\tstruct smb_sid owner_sid, group_sid;\n\tstruct dentry *parent = path->dentry->d_parent;\n\tstruct user_namespace *user_ns = mnt_user_ns(path->mnt);\n\tint inherited_flags = 0, flags = 0, i, ace_cnt = 0, nt_size = 0;\n\tint rc = 0, num_aces, dacloffset, pntsd_type, acl_len;\n\tchar *aces_base;\n\tbool is_dir = S_ISDIR(d_inode(path->dentry)->i_mode);\n\tacl_len = ksmbd_vfs_get_sd_xattr(conn, user_ns,\n\t\t\t\t\t parent, &parent_pntsd);\n\tif (acl_len <= 0)\n\t\treturn -ENOENT;\n\tdacloffset = le32_to_cpu(parent_pntsd->dacloffset);\n\tif (!dacloffset) {\n\t\trc = -EINVAL;\n\t\tgoto free_parent_pntsd;\n\t}\n\tparent_pdacl = (struct smb_acl *)((char *)parent_pntsd + dacloffset);\n\tnum_aces = le32_to_cpu(parent_pdacl->num_aces);\n\tpntsd_type = le16_to_cpu(parent_pntsd->type);\n\taces_base = kmalloc(sizeof(struct smb_ace) * num_aces * 2, GFP_KERNEL);\n\tif (!aces_base) {\n\t\trc = -ENOMEM;\n\t\tgoto free_parent_pntsd;\n\t}\n\taces = (struct smb_ace *)aces_base;\n\tparent_aces = (struct smb_ace *)((char *)parent_pdacl +\n\t\t\tsizeof(struct smb_acl));\n\tif (pntsd_type & DACL_AUTO_INHERITED)\n\t\tinherited_flags = INHERITED_ACE;\n\tfor (i = 0; i < num_aces; i++) {\n\t\tflags = parent_aces->flags;\n\t\tif (!smb_inherit_flags(flags, is_dir))\n\t\t\tgoto pass;\n\t\tif (is_dir) {\n\t\t\tflags &= ~(INHERIT_ONLY_ACE | INHERITED_ACE);\n\t\t\tif (!(flags & CONTAINER_INHERIT_ACE))\n\t\t\t\tflags |= INHERIT_ONLY_ACE;\n\t\t\tif (flags & NO_PROPAGATE_INHERIT_ACE)\n\t\t\t\tflags = 0;\n\t\t} else {\n\t\t\tflags = 0;\n\t\t}\n\t\tif (!compare_sids(&creator_owner, &parent_aces->sid)) {\n\t\t\tcreator = &creator_owner;\n\t\t\tid_to_sid(uid, SIDOWNER, &owner_sid);\n\t\t\tpsid = &owner_sid;\n\t\t} else if (!compare_sids(&creator_group, &parent_aces->sid)) {\n\t\t\tcreator = &creator_group;\n\t\t\tid_to_sid(gid, SIDUNIX_GROUP, &group_sid);\n\t\t\tpsid = &group_sid;\n\t\t} else {\n\t\t\tcreator = NULL;\n\t\t\tpsid = &parent_aces->sid;\n\t\t}\n\t\tif (is_dir && creator && flags & CONTAINER_INHERIT_ACE) {\n\t\t\tsmb_set_ace(aces, psid, parent_aces->type, inherited_flags,\n\t\t\t\t    parent_aces->access_req);\n\t\t\tnt_size += le16_to_cpu(aces->size);\n\t\t\tace_cnt++;\n\t\t\taces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\n\t\t\tflags |= INHERIT_ONLY_ACE;\n\t\t\tpsid = creator;\n\t\t} else if (is_dir && !(parent_aces->flags & NO_PROPAGATE_INHERIT_ACE)) {\n\t\t\tpsid = &parent_aces->sid;\n\t\t}\n\t\tsmb_set_ace(aces, psid, parent_aces->type, flags | inherited_flags,\n\t\t\t    parent_aces->access_req);\n\t\tnt_size += le16_to_cpu(aces->size);\n\t\taces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\n\t\tace_cnt++;\npass:\n\t\tparent_aces =\n\t\t\t(struct smb_ace *)((char *)parent_aces + le16_to_cpu(parent_aces->size));\n\t}\n\tif (nt_size > 0) {\n\t\tstruct smb_ntsd *pntsd;\n\t\tstruct smb_acl *pdacl;\n\t\tstruct smb_sid *powner_sid = NULL, *pgroup_sid = NULL;\n\t\tint powner_sid_size = 0, pgroup_sid_size = 0, pntsd_size;\n\t\tif (parent_pntsd->osidoffset) {\n\t\t\tpowner_sid = (struct smb_sid *)((char *)parent_pntsd +\n\t\t\t\t\tle32_to_cpu(parent_pntsd->osidoffset));\n\t\t\tpowner_sid_size = 1 + 1 + 6 + (powner_sid->num_subauth * 4);\n\t\t}\n\t\tif (parent_pntsd->gsidoffset) {\n\t\t\tpgroup_sid = (struct smb_sid *)((char *)parent_pntsd +\n\t\t\t\t\tle32_to_cpu(parent_pntsd->gsidoffset));\n\t\t\tpgroup_sid_size = 1 + 1 + 6 + (pgroup_sid->num_subauth * 4);\n\t\t}\n\t\tpntsd = kzalloc(sizeof(struct smb_ntsd) + powner_sid_size +\n\t\t\t\tpgroup_sid_size + sizeof(struct smb_acl) +\n\t\t\t\tnt_size, GFP_KERNEL);\n\t\tif (!pntsd) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_aces_base;\n\t\t}\n\t\tpntsd->revision = cpu_to_le16(1);\n\t\tpntsd->type = cpu_to_le16(SELF_RELATIVE | DACL_PRESENT);\n\t\tif (le16_to_cpu(parent_pntsd->type) & DACL_AUTO_INHERITED)",
        "bug_line_number": [
            11,
            12,
            13,
            15,
            16,
            17,
            18,
            20,
            21,
            24,
            25,
            26,
            27,
            34,
            35,
            37,
            38,
            79,
            80,
            81
        ],
        "vul": 1,
        "id": 46
    },
    {
        "code": "static int smb2_calc_max_out_buf_len(struct ksmbd_work *work,\n\t\t\t\t     unsigned short hdr2_len,\n\t\t\t\t     unsigned int out_buf_len)\n{\n\tint free_len;\n\tif (out_buf_len > work->conn->vals->max_trans_size)\n\t\treturn -EINVAL;\n\tfree_len = (int)(work->response_sz -\n\t\t\t (get_rfc1002_len(work->response_buf) + 4)) -\n\t\thdr2_len;\n\tif (free_len < 0)\n\t\treturn -EINVAL;\n\treturn min_t(int, out_buf_len, free_len);\n}",
        "bug_line_number": [
            7,
            8,
            9,
            10
        ],
        "vul": 1,
        "id": 48
    },
    {
        "code": "int v4l2_m2m_dqbuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\n\t\t   struct v4l2_buffer *buf)\n{\n\tstruct vb2_queue *vq;\n\tvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\n\treturn vb2_dqbuf(vq, buf, file->f_flags & O_NONBLOCK);\n}",
        "bug_line_number": [
            3,
            4,
            5,
            6
        ],
        "vul": 1,
        "id": 50
    },
    {
        "code": "static int _nfs4_get_security_label(struct inode *inode, void *buf,\n\t\t\t\t\tsize_t buflen)\n{\n\tstruct nfs_server *server = NFS_SERVER(inode);\n\tstruct nfs_fattr fattr;\n\tstruct nfs4_label label = {0, 0, buflen, buf};\n\tu32 bitmask[3] = { 0, 0, FATTR4_WORD2_SECURITY_LABEL };\n\tstruct nfs4_getattr_arg arg = {\n\t\t.fh\t\t= NFS_FH(inode),\n\t\t.bitmask\t= bitmask,\n\t};\n\tstruct nfs4_getattr_res res = {\n\t\t.fattr\t\t= &fattr,\n\t\t.label\t\t= &label,\n\t\t.server\t\t= server,\n\t};\n\tstruct rpc_message msg = {\n\t\t.rpc_proc\t= &nfs4_procedures[NFSPROC4_CLNT_GETATTR],\n\t\t.rpc_argp\t= &arg,\n\t\t.rpc_resp\t= &res,\n\t};\n\tint ret;\n\tnfs_fattr_init(&fattr);\n\tret = nfs4_call_sync(server->client, server, &msg, &arg.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\treturn ret;\n\tif (!(fattr.valid & NFS_ATTR_FATTR_V4_SECURITY_LABEL))\n\t\treturn -ENOENT;\n\tif (buflen < label.len)\n\t\treturn -ERANGE;\n\treturn 0;\n}",
        "bug_line_number": [
            28,
            29
        ],
        "vul": 1,
        "id": 52
    },
    {
        "code": "static int do_cpuid_func(struct kvm_cpuid_entry2 *entry, u32 func,\n\t\t\t int *nent, int maxnent, unsigned int type)\n{\n\tif (type == KVM_GET_EMULATED_CPUID)\n\t\treturn __do_cpuid_func_emulated(entry, func, nent, maxnent);\n\treturn __do_cpuid_func(entry, func, nent, maxnent);\n}",
        "bug_line_number": [
            2,
            3
        ],
        "vul": 1,
        "id": 54
    },
    {
        "code": "static int mon_bin_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\t/* don't do anything here: \"fault\" will set up page table entries */\n\tvma->vm_ops = &mon_bin_vm_ops;\n\tvma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;\n\tvma->vm_private_data = filp->private_data;\n\tmon_bin_vma_open(vma);\n\treturn 0;\n}",
        "bug_line_number": [
            3,
            4
        ],
        "vul": 1,
        "id": 56
    },
    {
        "code": "static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)\n{\n\tstruct io_buffer *buf;\n\tu64 addr = pbuf->addr;\n\tint i, bid = pbuf->bid;\n\tfor (i = 0; i < pbuf->nbufs; i++) {\n\t\tbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tbreak;\n\t\tbuf->addr = addr;\n\t\tbuf->len = pbuf->len;\n\t\tbuf->bid = bid;\n\t\taddr += pbuf->len;\n\t\tbid++;\n\t\tif (!*head) {\n\t\t\tINIT_LIST_HEAD(&buf->list);\n\t\t\t*head = buf;\n\t\t} else {\n\t\t\tlist_add_tail(&buf->list, &(*head)->list);\n\t\t}\n\t}\n\treturn i ? i : -ENOMEM;\n}",
        "bug_line_number": [
            10,
            11
        ],
        "vul": 1,
        "id": 58
    },
    {
        "code": "static struct sk_buff *tun_napi_alloc_frags(struct tun_file *tfile,\n\t\t\t\t\t    size_t len,\n\t\t\t\t\t    const struct iov_iter *it)\n{\n\tstruct sk_buff *skb;\n\tsize_t linear;\n\tint err;\n\tint i;\n\tif (it->nr_segs > MAX_SKB_FRAGS + 1)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\tlocal_bh_disable();\n\tskb = napi_get_frags(&tfile->napi);\n\tlocal_bh_enable();\n\tif (!skb)\n\t\treturn ERR_PTR(-ENOMEM);\n\tlinear = iov_iter_single_seg_count(it);\n\terr = __skb_grow(skb, linear);\n\tif (err)\n\t\tgoto free;\n\tskb->len = len;\n\tskb->data_len = len - linear;\n\tskb->truesize += skb->data_len;\n\tfor (i = 1; i < it->nr_segs; i++) {\n\t\tsize_t fragsz = it->iov[i].iov_len;\n\t\tstruct page *page;\n\t\tvoid *frag;\n\t\tif (fragsz == 0 || fragsz > PAGE_SIZE) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\t\tfrag = netdev_alloc_frag(fragsz);\n\t\tif (!frag) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t\tpage = virt_to_head_page(frag);\n\t\tskb_fill_page_desc(skb, i - 1, page,\n\t\t\t\t   frag - page_address(page), fragsz);\n\t}\n\treturn skb;\nfree:\n\t/* frees skb and all frags allocated with napi_alloc_frag() */\n\tnapi_free_frags(&tfile->napi);\n\treturn ERR_PTR(err);\n}",
        "bug_line_number": [
            8,
            9
        ],
        "vul": 1,
        "id": 60
    },
    {
        "code": "void xt_compat_target_from_user(struct xt_entry_target *t, void **dstptr,\n\t\t\t\tunsigned int *size)\n{\n\tconst struct xt_target *target = t->u.kernel.target;\n\tstruct compat_xt_entry_target *ct = (struct compat_xt_entry_target *)t;\n\tint pad, off = xt_compat_target_offset(target);\n\tu_int16_t tsize = ct->u.user.target_size;\n\tchar name[sizeof(t->u.user.name)];\n\tt = *dstptr;\n\tmemcpy(t, ct, sizeof(*ct));\n\tif (target->compat_from_user)\n\t\ttarget->compat_from_user(t->data, ct->data);\n\telse\n\t\tmemcpy(t->data, ct->data, tsize - sizeof(*ct));\n\tpad = XT_ALIGN(target->targetsize) - target->targetsize;\n\tif (pad > 0)\n\t\tmemset(t->data + target->targetsize, 0, pad);\n\ttsize += off;\n\tt->u.user.target_size = tsize;\n\tstrlcpy(name, target->name, sizeof(name));\n\tmodule_put(target->me);\n\tstrncpy(t->u.user.name, name, sizeof(t->u.user.name));\n\t*size += off;\n\t*dstptr += tsize;\n}",
        "bug_line_number": [
            5,
            6,
            14,
            15,
            16
        ],
        "vul": 1,
        "id": 62
    },
    {
        "code": "void xt_compat_match_from_user(struct xt_entry_match *m, void **dstptr,\n\t\t\t       unsigned int *size)\n{\n\tconst struct xt_match *match = m->u.kernel.match;\n\tstruct compat_xt_entry_match *cm = (struct compat_xt_entry_match *)m;\n\tint pad, off = xt_compat_match_offset(match);\n\tu_int16_t msize = cm->u.user.match_size;\n\tchar name[sizeof(m->u.user.name)];\n\tm = *dstptr;\n\tmemcpy(m, cm, sizeof(*cm));\n\tif (match->compat_from_user)\n\t\tmatch->compat_from_user(m->data, cm->data);\n\telse\n\t\tmemcpy(m->data, cm->data, msize - sizeof(*cm));\n\tpad = XT_ALIGN(match->matchsize) - match->matchsize;\n\tif (pad > 0)\n\t\tmemset(m->data + match->matchsize, 0, pad);\n\tmsize += off;\n\tm->u.user.match_size = msize;\n\tstrlcpy(name, match->name, sizeof(name));\n\tmodule_put(match->me);\n\tstrncpy(m->u.user.name, name, sizeof(m->u.user.name));\n\t*size += off;\n\t*dstptr += msize;\n}",
        "bug_line_number": [
            5,
            6,
            14,
            15,
            16
        ],
        "vul": 1,
        "id": 64
    },
    {
        "code": "static int\ntranslate_compat_table(struct net *net,\n\t\t       struct xt_table_info **pinfo,\n\t\t       void **pentry0,\n\t\t       const struct compat_ipt_replace *compatr)\n{\n\tunsigned int i, j;\n\tstruct xt_table_info *newinfo, *info;\n\tvoid *pos, *entry0, *entry1;\n\tstruct compat_ipt_entry *iter0;\n\tstruct ipt_replace repl;\n\tunsigned int size;\n\tint ret;\n\tinfo = *pinfo;\n\tentry0 = *pentry0;\n\tsize = compatr->size;\n\tinfo->number = compatr->num_entries;\n\tj = 0;\n\txt_compat_lock(AF_INET);\n\tret = xt_compat_init_offsets(AF_INET, compatr->num_entries);\n\tif (ret)\n\t\tgoto out_unlock;\n\t/* Walk through entries, checking offsets. */\n\txt_entry_foreach(iter0, entry0, compatr->size) {\n\t\tret = check_compat_entry_size_and_hooks(iter0, info, &size,\n\t\t\t\t\t\t\tentry0,\n\t\t\t\t\t\t\tentry0 + compatr->size);\n\t\tif (ret != 0)\n\t\t\tgoto out_unlock;\n\t\t++j;\n\t}\n\tret = -EINVAL;\n\tif (j != compatr->num_entries)\n\t\tgoto out_unlock;\n\tret = -ENOMEM;\n\tnewinfo = xt_alloc_table_info(size);\n\tif (!newinfo)\n\t\tgoto out_unlock;\n\tnewinfo->number = compatr->num_entries;\n\tfor (i = 0; i < NF_INET_NUMHOOKS; i++) {\n\t\tnewinfo->hook_entry[i] = compatr->hook_entry[i];\n\t\tnewinfo->underflow[i] = compatr->underflow[i];\n\t}\n\tentry1 = newinfo->entries;\n\tpos = entry1;\n\tsize = compatr->size;\n\txt_entry_foreach(iter0, entry0, compatr->size)\n\t\tcompat_copy_entry_from_user(iter0, &pos, &size,\n\t\t\t\t\t    newinfo, entry1);\n\t/* all module references in entry0 are now gone.\n\t * entry1/newinfo contains a 64bit ruleset that looks exactly as\n\t * generated by 64bit userspace.\n\t *\n\t * Call standard translate_table() to validate all hook_entrys,\n\t * underflows, check for loops, etc.\n\t */\n\txt_compat_flush_offsets(AF_INET);\n\txt_compat_unlock(AF_INET);\n\tmemcpy(&repl, compatr, sizeof(*compatr));\n\tfor (i = 0; i < NF_INET_NUMHOOKS; i++) {\n\t\trepl.hook_entry[i] = newinfo->hook_entry[i];\n\t\trepl.underflow[i] = newinfo->underflow[i];\n\t}\n\trepl.num_counters = 0;\n\trepl.counters = NULL;\n\trepl.size = newinfo->size;\n\tret = translate_table(net, newinfo, entry1, &repl);\n\tif (ret)\n\t\tgoto free_newinfo;\n\t*pinfo = newinfo;\n\t*pentry0 = entry1;\n\txt_free_table_info(info);\n\treturn 0;\nfree_newinfo:\n\txt_free_table_info(newinfo);\n\treturn ret;\nout_unlock:\n\txt_compat_flush_offsets(AF_INET);\n\txt_compat_unlock(AF_INET);\n\txt_entry_foreach(iter0, entry0, compatr->size) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tcompat_release_entry(iter0);\n\t}\n\treturn ret;\n}",
        "bug_line_number": [
            37,
            38
        ],
        "vul": 1,
        "id": 66
    },
    {
        "code": "static inline int ext4_valid_inum(struct super_block *sb, unsigned long ino)\n{\n\treturn ino == EXT4_ROOT_INO ||\n\t\tino == EXT4_USR_QUOTA_INO ||\n\t\tino == EXT4_GRP_QUOTA_INO ||\n\t\tino == EXT4_BOOT_LOADER_INO ||\n\t\tino == EXT4_JOURNAL_INO ||\n\t\tino == EXT4_RESIZE_INO ||\n\t\t(ino >= EXT4_FIRST_INO(sb) &&\n\t\t ino <= le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count));\n}",
        "bug_line_number": [
            3,
            4,
            5,
            6,
            7
        ],
        "vul": 1,
        "id": 68
    },
    {
        "code": "static void handle_rx(struct vhost_net *net)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tunsigned uninitialized_var(in), log;\n\tstruct vhost_log *vq_log;\n\tstruct msghdr msg = {\n\t\t.msg_name = NULL,\n\t\t.msg_namelen = 0,\n\t\t.msg_control = NULL, /* FIXME: get and handle RX aux data. */\n\t\t.msg_controllen = 0,\n\t\t.msg_iov = vq->iov,\n\t\t.msg_flags = MSG_DONTWAIT,\n\t};\n\tstruct virtio_net_hdr_mrg_rxbuf hdr = {\n\t\t.hdr.flags = 0,\n\t\t.hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE\n\t};\n\tsize_t total_len = 0;\n\tint err, mergeable;\n\ts16 headcount;\n\tsize_t vhost_hlen, sock_hlen;\n\tsize_t vhost_len, sock_len;\n\tstruct socket *sock;\n\tmutex_lock(&vq->mutex);\n\tsock = vq->private_data;\n\tif (!sock)\n\t\tgoto out;\n\tvhost_disable_notify(&net->dev, vq);\n\tvhost_hlen = nvq->vhost_hlen;\n\tsock_hlen = nvq->sock_hlen;\n\tvq_log = unlikely(vhost_has_feature(&net->dev, VHOST_F_LOG_ALL)) ?\n\t\tvq->log : NULL;\n\tmergeable = vhost_has_feature(&net->dev, VIRTIO_NET_F_MRG_RXBUF);\n\twhile ((sock_len = peek_head_len(sock->sk))) {\n\t\tsock_len += sock_hlen;\n\t\tvhost_len = sock_len + vhost_hlen;\n\t\theadcount = get_rx_bufs(vq, vq->heads, vhost_len,\n\t\t\t\t\t&in, vq_log, &log,\n\t\t\t\t\tlikely(mergeable) ? UIO_MAXIOV : 1);\n\t\t/* On error, stop handling until the next kick. */\n\t\tif (unlikely(headcount < 0))\n\t\t\tbreak;\n\t\t/* OK, now we need to know about added descriptors. */\n\t\tif (!headcount) {\n\t\t\tif (unlikely(vhost_enable_notify(&net->dev, vq))) {\n\t\t\t\t/* They have slipped one in as we were\n\t\t\t\t * doing that: check again. */\n\t\t\t\tvhost_disable_notify(&net->dev, vq);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/* Nothing new?  Wait for eventfd to tell us\n\t\t\t * they refilled. */\n\t\t\tbreak;\n\t\t}\n\t\t/* We don't need to be notified again. */\n\t\tif (unlikely((vhost_hlen)))\n\t\t\t/* Skip header. TODO: support TSO. */\n\t\t\tmove_iovec_hdr(vq->iov, nvq->hdr, vhost_hlen, in);\n\t\telse\n\t\t\t/* Copy the header for use in VIRTIO_NET_F_MRG_RXBUF:\n\t\t\t * needed because recvmsg can modify msg_iov. */\n\t\t\tcopy_iovec_hdr(vq->iov, nvq->hdr, sock_hlen, in);\n\t\tmsg.msg_iovlen = in;\n\t\terr = sock->ops->recvmsg(NULL, sock, &msg,\n\t\t\t\t\t sock_len, MSG_DONTWAIT | MSG_TRUNC);\n\t\t/* Userspace might have consumed the packet meanwhile:\n\t\t * it's not supposed to do this usually, but might be hard\n\t\t * to prevent. Discard data we got (if any) and keep going. */\n\t\tif (unlikely(err != sock_len)) {\n\t\t\tpr_debug(\"Discarded rx packet: \"\n\t\t\t\t \" len %d, expected %zd\\n\", err, sock_len);\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(vhost_hlen) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&hdr, 0,\n\t\t\t\t      vhost_hlen)) {\n\t\t\tvq_err(vq, \"Unable to write vnet_hdr at addr %p\\n\",\n\t\t\t       vq->iov->iov_base);\n\t\t\tbreak;\n\t\t}\n\t\t/* TODO: Should check and handle checksum. */\n\t\tif (likely(mergeable) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&headcount,\n\t\t\t\t      offsetof(typeof(hdr), num_buffers),\n\t\t\t\t      sizeof hdr.num_buffers)) {\n\t\t\tvq_err(vq, \"Failed num_buffers write\");\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tbreak;\n\t\t}\n\t\tvhost_add_used_and_signal_n(&net->dev, vq, vq->heads,\n\t\t\t\t\t    headcount);\n\t\tif (unlikely(vq_log))\n\t\t\tvhost_log_write(vq, vq_log, log, vhost_len);\n\t\ttotal_len += vhost_len;\n\t\tif (unlikely(total_len >= VHOST_NET_WEIGHT)) {\n\t\t\tvhost_poll_queue(&vq->poll);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&vq->mutex);\n}",
        "bug_line_number": [
            42,
            43
        ],
        "vul": 1,
        "id": 70
    },
    {
        "code": "int nf_ct_frag6_gather(struct net *net, struct sk_buff *skb, u32 user)\n{\n\tstruct net_device *dev = skb->dev;\n\tint fhoff, nhoff, ret;\n\tstruct frag_hdr *fhdr;\n\tstruct frag_queue *fq;\n\tstruct ipv6hdr *hdr;\n\tu8 prevhdr;\n\t/* Jumbo payload inhibits frag. header */\n\tif (ipv6_hdr(skb)->payload_len == 0) {\n\t\tpr_debug(\"payload len = 0\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (find_prev_fhdr(skb, &prevhdr, &nhoff, &fhoff) < 0)\n\t\treturn -EINVAL;\n\tif (!pskb_may_pull(skb, fhoff + sizeof(*fhdr)))\n\t\treturn -ENOMEM;\n\tskb_set_transport_header(skb, fhoff);\n\thdr = ipv6_hdr(skb);\n\tfhdr = (struct frag_hdr *)skb_transport_header(skb);\n\tfq = fq_find(net, fhdr->identification, user, &hdr->saddr, &hdr->daddr,\n\t\t     skb->dev ? skb->dev->ifindex : 0, ip6_frag_ecn(hdr));\n\tif (fq == NULL) {\n\t\tpr_debug(\"Can't find and can't create new queue\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tspin_lock_bh(&fq->q.lock);\n\tif (nf_ct_frag6_queue(fq, skb, fhdr, nhoff) < 0) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\t/* after queue has assumed skb ownership, only 0 or -EINPROGRESS\n\t * must be returned.\n\t */\n\tret = -EINPROGRESS;\n\tif (fq->q.flags == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\n\t    fq->q.meat == fq->q.len &&\n\t    nf_ct_frag6_reasm(fq, skb, dev))\n\t\tret = 0;\nout_unlock:\n\tspin_unlock_bh(&fq->q.lock);\n\tinet_frag_put(&fq->q, &nf_frags);\n\treturn ret;\n}",
        "bug_line_number": [
            11,
            12,
            14,
            15
        ],
        "vul": 1,
        "id": 72
    },
    {
        "code": "static int usb_parse_configuration(struct usb_device *dev, int cfgidx,\n    struct usb_host_config *config, unsigned char *buffer, int size)\n{\n\tstruct device *ddev = &dev->dev;\n\tunsigned char *buffer0 = buffer;\n\tint cfgno;\n\tint nintf, nintf_orig;\n\tint i, j, n;\n\tstruct usb_interface_cache *intfc;\n\tunsigned char *buffer2;\n\tint size2;\n\tstruct usb_descriptor_header *header;\n\tint len, retval;\n\tu8 inums[USB_MAXINTERFACES], nalts[USB_MAXINTERFACES];\n\tunsigned iad_num = 0;\n\tmemcpy(&config->desc, buffer, USB_DT_CONFIG_SIZE);\n\tif (config->desc.bDescriptorType != USB_DT_CONFIG ||\n\t    config->desc.bLength < USB_DT_CONFIG_SIZE ||\n\t    config->desc.bLength > size) {\n\t\tdev_err(ddev, \"invalid descriptor for config index %d: \"\n\t\t    \"type = 0x%X, length = %d\\n\", cfgidx,\n\t\t    config->desc.bDescriptorType, config->desc.bLength);\n\t\treturn -EINVAL;\n\t}\n\tcfgno = config->desc.bConfigurationValue;\n\tbuffer += config->desc.bLength;\n\tsize -= config->desc.bLength;\n\tnintf = nintf_orig = config->desc.bNumInterfaces;\n\tif (nintf > USB_MAXINTERFACES) {\n\t\tdev_warn(ddev, \"config %d has too many interfaces: %d, \"\n\t\t    \"using maximum allowed: %d\\n\",\n\t\t    cfgno, nintf, USB_MAXINTERFACES);\n\t\tnintf = USB_MAXINTERFACES;\n\t}\n\t/* Go through the descriptors, checking their length and counting the\n\t * number of altsettings for each interface */\n\tn = 0;\n\tfor ((buffer2 = buffer, size2 = size);\n\t      size2 > 0;\n\t     (buffer2 += header->bLength, size2 -= header->bLength)) {\n\t\tif (size2 < sizeof(struct usb_descriptor_header)) {\n\t\t\tdev_warn(ddev, \"config %d descriptor has %d excess \"\n\t\t\t    \"byte%s, ignoring\\n\",\n\t\t\t    cfgno, size2, plural(size2));\n\t\t\tbreak;\n\t\t}\n\t\theader = (struct usb_descriptor_header *) buffer2;\n\t\tif ((header->bLength > size2) || (header->bLength < 2)) {\n\t\t\tdev_warn(ddev, \"config %d has an invalid descriptor \"\n\t\t\t    \"of length %d, skipping remainder of the config\\n\",\n\t\t\t    cfgno, header->bLength);\n\t\t\tbreak;\n\t\t}\n\t\tif (header->bDescriptorType == USB_DT_INTERFACE) {\n\t\t\tstruct usb_interface_descriptor *d;\n\t\t\tint inum;\n\t\t\td = (struct usb_interface_descriptor *) header;\n\t\t\tif (d->bLength < USB_DT_INTERFACE_SIZE) {\n\t\t\t\tdev_warn(ddev, \"config %d has an invalid \"\n\t\t\t\t    \"interface descriptor of length %d, \"\n\t\t\t\t    \"skipping\\n\", cfgno, d->bLength);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tinum = d->bInterfaceNumber;\n\t\t\tif ((dev->quirks & USB_QUIRK_HONOR_BNUMINTERFACES) &&\n\t\t\t    n >= nintf_orig) {\n\t\t\t\tdev_warn(ddev, \"config %d has more interface \"\n\t\t\t\t    \"descriptors, than it declares in \"\n\t\t\t\t    \"bNumInterfaces, ignoring interface \"\n\t\t\t\t    \"number: %d\\n\", cfgno, inum);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (inum >= nintf_orig)\n\t\t\t\tdev_warn(ddev, \"config %d has an invalid \"\n\t\t\t\t    \"interface number: %d but max is %d\\n\",\n\t\t\t\t    cfgno, inum, nintf_orig - 1);\n\t\t\t/* Have we already encountered this interface?\n\t\t\t * Count its altsettings */\n\t\t\tfor (i = 0; i < n; ++i) {\n\t\t\t\tif (inums[i] == inum)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (i < n) {\n\t\t\t\tif (nalts[i] < 255)\n\t\t\t\t\t++nalts[i];\n\t\t\t} else if (n < USB_MAXINTERFACES) {\n\t\t\t\tinums[n] = inum;\n\t\t\t\tnalts[n] = 1;\n\t\t\t\t++n;\n\t\t\t}\n\t\t} else if (header->bDescriptorType ==\n\t\t\t\tUSB_DT_INTERFACE_ASSOCIATION) {\n\t\t\tstruct usb_interface_assoc_descriptor *d;\n\t\t\td = (struct usb_interface_assoc_descriptor *)header;\n\t\t\tif (d->bLength < USB_DT_INTERFACE_ASSOCIATION_SIZE) {\n\t\t\t\tdev_warn(ddev,\n\t\t\t\t\t \"config %d has an invalid interface association descriptor of length %d, skipping\\n\",\n\t\t\t\t\t cfgno, d->bLength);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (iad_num == USB_MAXIADS) {\n\t\t\t\tdev_warn(ddev, \"found more Interface \"\n\t\t\t\t\t       \"Association Descriptors \"\n\t\t\t\t\t       \"than allocated for in \"\n\t\t\t\t\t       \"configuration %d\\n\", cfgno);",
        "bug_line_number": [
            15,
            16,
            27
        ],
        "vul": 1,
        "id": 74
    },
    {
        "code": "int jbd2_journal_dirty_metadata(handle_t *handle, struct buffer_head *bh)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal;\n\tstruct journal_head *jh;\n\tint ret = 0;\n\tif (is_handle_aborted(handle))\n\t\treturn -EROFS;\n\tif (!buffer_jbd(bh)) {\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\t/*\n\t * We don't grab jh reference here since the buffer must be part\n\t * of the running transaction.\n\t */\n\tjh = bh2jh(bh);\n\t/*\n\t * This and the following assertions are unreliable since we may see jh\n\t * in inconsistent state unless we grab bh_state lock. But this is\n\t * crucial to catch bugs so let's do a reliable check until the\n\t * lockless handling is fully proven.\n\t */\n\tif (jh->b_transaction != transaction &&\n\t    jh->b_next_transaction != transaction) {\n\t\tjbd_lock_bh_state(bh);\n\t\tJ_ASSERT_JH(jh, jh->b_transaction == transaction ||\n\t\t\t\tjh->b_next_transaction == transaction);\n\t\tjbd_unlock_bh_state(bh);\n\t}\n\tif (jh->b_modified == 1) {\n\t\t/* If it's in our transaction it must be in BJ_Metadata list. */\n\t\tif (jh->b_transaction == transaction &&\n\t\t    jh->b_jlist != BJ_Metadata) {\n\t\t\tjbd_lock_bh_state(bh);\n\t\t\tJ_ASSERT_JH(jh, jh->b_transaction != transaction ||\n\t\t\t\t\tjh->b_jlist == BJ_Metadata);\n\t\t\tjbd_unlock_bh_state(bh);\n\t\t}\n\t\tgoto out;\n\t}\n\tjournal = transaction->t_journal;\n\tjbd_debug(5, \"journal_head %p\\n\", jh);\n\tJBUFFER_TRACE(jh, \"entry\");\n\tjbd_lock_bh_state(bh);\n\tif (jh->b_modified == 0) {\n\t\t/*\n\t\t * This buffer's got modified and becoming part\n\t\t * of the transaction. This needs to be done\n\t\t * once a transaction -bzzz\n\t\t */\n\t\tjh->b_modified = 1;\n\t\tif (handle->h_buffer_credits <= 0) {\n\t\t\tret = -ENOSPC;\n\t\t\tgoto out_unlock_bh;\n\t\t}\n\t\thandle->h_buffer_credits--;\n\t}\n\t/*\n\t * fastpath, to avoid expensive locking.  If this buffer is already\n\t * on the running transaction's metadata list there is nothing to do.\n\t * Nobody can take it off again because there is a handle open.\n\t * I _think_ we're OK here with SMP barriers - a mistaken decision will\n\t * result in this test being false, so we go in and take the locks.\n\t */\n\tif (jh->b_transaction == transaction && jh->b_jlist == BJ_Metadata) {\n\t\tJBUFFER_TRACE(jh, \"fastpath\");\n\t\tif (unlikely(jh->b_transaction !=\n\t\t\t     journal->j_running_transaction)) {\n\t\t\tprintk(KERN_ERR \"JBD2: %s: \"\n\t\t\t       \"jh->b_transaction (%llu, %p, %u) != \"\n\t\t\t       \"journal->j_running_transaction (%p, %u)\\n\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ? jh->b_transaction->t_tid : 0,\n\t\t\t       journal->j_running_transaction,\n\t\t\t       journal->j_running_transaction ?\n\t\t\t       journal->j_running_transaction->t_tid : 0);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tgoto out_unlock_bh;\n\t}\n\tset_buffer_jbddirty(bh);\n\t/*\n\t * Metadata already on the current transaction list doesn't\n\t * need to be filed.  Metadata on another transaction's list must\n\t * be committing, and will be refiled once the commit completes:\n\t * leave it alone for now.\n\t */\n\tif (jh->b_transaction != transaction) {\n\t\tJBUFFER_TRACE(jh, \"already on other transaction\");\n\t\tif (unlikely(((jh->b_transaction !=\n\t\t\t       journal->j_committing_transaction)) ||\n\t\t\t     (jh->b_next_transaction != transaction))) {\n\t\t\tprintk(KERN_ERR \"jbd2_journal_dirty_metadata: %s: \"\n\t\t\t       \"bad jh for block %llu: \"\n\t\t\t       \"transaction (%p, %u), \"\n\t\t\t       \"jh->b_transaction (%p, %u), \"\n\t\t\t       \"jh->b_next_transaction (%p, %u), jlist %u\\n\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       transaction, transaction->t_tid,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ?\n\t\t\t       jh->b_transaction->t_tid : 0,\n\t\t\t       jh->b_next_transaction,\n\t\t\t       jh->b_next_transaction ?\n\t\t\t       jh->b_next_transaction->t_tid : 0,\n\t\t\t       jh->b_jlist);\n\t\t\tWARN_ON(1);\n\t\t\tret = -EINVAL;\n\t\t}",
        "bug_line_number": [
            34,
            35,
            51,
            55,
            56
        ],
        "vul": 1,
        "id": 76
    },
    {
        "code": "static int joydev_handle_JSIOCSAXMAP(struct joydev *joydev,\n\t\t\t\t     void __user *argp, size_t len)\n{\n\t__u8 *abspam;\n\tint i;\n\tint retval = 0;\n\tlen = min(len, sizeof(joydev->abspam));\n\t/* Validate the map. */\n\tabspam = memdup_user(argp, len);\n\tif (IS_ERR(abspam))\n\t\treturn PTR_ERR(abspam);\n\tfor (i = 0; i < joydev->nabs; i++) {\n\t\tif (abspam[i] > ABS_MAX) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tmemcpy(joydev->abspam, abspam, len);\n\tfor (i = 0; i < joydev->nabs; i++)\n\t\tjoydev->absmap[joydev->abspam[i]] = i;\n out:\n\tkfree(abspam);\n\treturn retval;\n}",
        "bug_line_number": [
            11,
            12
        ],
        "vul": 1,
        "id": 78
    },
    {
        "code": "int kvm_sev_es_string_io(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t unsigned int port, void *data,  unsigned int count,\n\t\t\t int in)\n{\n\tvcpu->arch.sev_pio_data = data;\n\treturn in ? kvm_sev_es_ins(vcpu, size, port, count)\n\t\t  : kvm_sev_es_outs(vcpu, size, port, count);\n}",
        "bug_line_number": [
            5,
            6,
            7
        ],
        "vul": 1,
        "id": 80
    },
    {
        "code": "static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunion vmx_exit_reason exit_reason = vmx->exit_reason;\n\tu32 vectoring_info = vmx->idt_vectoring_info;\n\tu16 exit_handler_index;\n\t/*\n\t * Flush logged GPAs PML buffer, this will make dirty_bitmap more\n\t * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before\n\t * querying dirty_bitmap, we only need to kick all vcpus out of guest\n\t * mode as if vcpus is in root mode, the PML buffer must has been\n\t * flushed already.  Note, PML is never enabled in hardware while\n\t * running L2.\n\t */\n\tif (enable_pml && !is_guest_mode(vcpu))\n\t\tvmx_flush_pml_buffer(vcpu);\n\t/*\n\t * We should never reach this point with a pending nested VM-Enter, and\n\t * more specifically emulation of L2 due to invalid guest state (see\n\t * below) should never happen as that means we incorrectly allowed a\n\t * nested VM-Enter with an invalid vmcs12.\n\t */\n\tWARN_ON_ONCE(vmx->nested.nested_run_pending);\n\t/* If guest state is invalid, start emulating */\n\tif (vmx->emulation_required)\n\t\treturn handle_invalid_guest_state(vcpu);\n\tif (is_guest_mode(vcpu)) {\n\t\t/*\n\t\t * PML is never enabled when running L2, bail immediately if a\n\t\t * PML full exit occurs as something is horribly wrong.\n\t\t */\n\t\tif (exit_reason.basic == EXIT_REASON_PML_FULL)\n\t\t\tgoto unexpected_vmexit;\n\t\t/*\n\t\t * The host physical addresses of some pages of guest memory\n\t\t * are loaded into the vmcs02 (e.g. vmcs12's Virtual APIC\n\t\t * Page). The CPU may write to these pages via their host\n\t\t * physical address while L2 is running, bypassing any\n\t\t * address-translation-based dirty tracking (e.g. EPT write\n\t\t * protection).\n\t\t *\n\t\t * Mark them dirty on every exit from L2 to prevent them from\n\t\t * getting out of sync with dirty tracking.\n\t\t */\n\t\tnested_mark_vmcs12_pages_dirty(vcpu);\n\t\tif (nested_vmx_reflect_vmexit(vcpu))\n\t\t\treturn 1;\n\t}\n\tif (exit_reason.failed_vmentry) {\n\t\tdump_vmcs();\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= exit_reason.full;\n\t\tvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\tif (unlikely(vmx->fail)) {\n\t\tdump_vmcs();\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= vmcs_read32(VM_INSTRUCTION_ERROR);\n\t\tvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\t/*\n\t * Note:\n\t * Do not try to fix EXIT_REASON_EPT_MISCONFIG if it caused by\n\t * delivery event since it indicates guest is accessing MMIO.\n\t * The vm-exit can be triggered again after return to guest that\n\t * will cause infinite loop.\n\t */\n\tif ((vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t    (exit_reason.basic != EXIT_REASON_EXCEPTION_NMI &&\n\t     exit_reason.basic != EXIT_REASON_EPT_VIOLATION &&\n\t     exit_reason.basic != EXIT_REASON_PML_FULL &&\n\t     exit_reason.basic != EXIT_REASON_APIC_ACCESS &&\n\t     exit_reason.basic != EXIT_REASON_TASK_SWITCH)) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;\n\t\tvcpu->run->internal.ndata = 3;\n\t\tvcpu->run->internal.data[0] = vectoring_info;\n\t\tvcpu->run->internal.data[1] = exit_reason.full;\n\t\tvcpu->run->internal.data[2] = vcpu->arch.exit_qualification;\n\t\tif (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG) {\n\t\t\tvcpu->run->internal.ndata++;\n\t\t\tvcpu->run->internal.data[3] =\n\t\t\t\tvmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\t\t}\n\t\tvcpu->run->internal.data[vcpu->run->internal.ndata++] =\n\t\t\tvcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\tif (unlikely(!enable_vnmi &&\n\t\t     vmx->loaded_vmcs->soft_vnmi_blocked)) {\n\t\tif (!vmx_interrupt_blocked(vcpu)) {\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&\n\t\t\t   vcpu->arch.nmi_pending) {\n\t\t\t/*\n\t\t\t * This CPU don't support us in finding the end of an\n\t\t\t * NMI-blocked window if the guest runs with IRQs\n\t\t\t * disabled. So we pull the trigger after 1 s of\n\t\t\t * futile waiting, but inform the user about this.\n\t\t\t */\n\t\t\tprintk(KERN_WARNING \"%s: Breaking out of NMI-blocked \"\n\t\t\t       \"state on VCPU %d after 1 s timeout\\n\",\n\t\t\t       __func__, vcpu->vcpu_id);\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t}\n\t}",
        "bug_line_number": [
            76,
            77,
            79,
            84,
            85,
            86,
            88,
            89,
            90
        ],
        "vul": 1,
        "id": 82
    },
    {
        "code": "int ext4_setup_system_zone(struct super_block *sb)\n{\n\text4_group_t ngroups = ext4_get_groups_count(sb);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp;\n\text4_group_t i;\n\tint flex_size = ext4_flex_bg_size(sbi);\n\tint ret;\n\tif (!test_opt(sb, BLOCK_VALIDITY)) {\n\t\tif (sbi->system_blks.rb_node)\n\t\t\text4_release_system_zone(sb);\n\t\treturn 0;\n\t}\n\tif (sbi->system_blks.rb_node)\n\t\treturn 0;\n\tfor (i=0; i < ngroups; i++) {\n\t\tif (ext4_bg_has_super(sb, i) &&\n\t\t    ((i < 5) || ((i % flex_size) == 0)))\n\t\t\tadd_system_zone(sbi, ext4_group_first_block_no(sb, i),\n\t\t\t\t\text4_bg_num_gdb(sb, i) + 1);\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\t\tret = add_system_zone(sbi, ext4_block_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_table(sb, gdp),\n\t\t\t\tsbi->s_itb_per_group);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tif (test_opt(sb, DEBUG))\n\t\tdebug_print_tree(sbi);\n\treturn 0;\n}",
        "bug_line_number": [
            31,
            32
        ],
        "vul": 1,
        "id": 84
    },
    {
        "code": "int kvmppc_rtas_hcall(struct kvm_vcpu *vcpu)\n{\n\tstruct rtas_token_definition *d;\n\tstruct rtas_args args;\n\trtas_arg_t *orig_rets;\n\tgpa_t args_phys;\n\tint rc;\n\t/*\n\t * r4 contains the guest physical address of the RTAS args\n\t * Mask off the top 4 bits since this is a guest real address\n\t */\n\targs_phys = kvmppc_get_gpr(vcpu, 4) & KVM_PAM;\n\tvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\n\trc = kvm_read_guest(vcpu->kvm, args_phys, &args, sizeof(args));\n\tsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);\n\tif (rc)\n\t\tgoto fail;\n\t/*\n\t * args->rets is a pointer into args->args. Now that we've\n\t * copied args we need to fix it up to point into our copy,\n\t * not the guest args. We also need to save the original\n\t * value so we can restore it on the way out.\n\t */\n\torig_rets = args.rets;\n\targs.rets = &args.args[be32_to_cpu(args.nargs)];\n\tmutex_lock(&vcpu->kvm->arch.rtas_token_lock);\n\trc = -ENOENT;\n\tlist_for_each_entry(d, &vcpu->kvm->arch.rtas_tokens, list) {\n\t\tif (d->token == be32_to_cpu(args.token)) {\n\t\t\td->handler->handler(vcpu, &args);\n\t\t\trc = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&vcpu->kvm->arch.rtas_token_lock);\n\tif (rc == 0) {\n\t\targs.rets = orig_rets;\n\t\trc = kvm_write_guest(vcpu->kvm, args_phys, &args, sizeof(args));\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\treturn rc;\nfail:\n\t/*\n\t * We only get here if the guest has called RTAS with a bogus\n\t * args pointer. That means we can't get to the args, and so we\n\t * can't fail the RTAS call. So fail right out to userspace,\n\t * which should kill the guest.\n\t */\n\treturn rc;\n}",
        "bug_line_number": [
            23,
            24,
            45,
            46,
            47,
            48
        ],
        "vul": 1,
        "id": 86
    },
    {
        "code": "static void\nbinder_free_buf(struct binder_proc *proc,\n\t\tstruct binder_thread *thread,\n\t\tstruct binder_buffer *buffer, bool is_failure)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
        "bug_line_number": [
            30,
            31
        ],
        "vul": 1,
        "id": 88
    },
    {
        "code": "static ssize_t smtcfb_read(struct fb_info *info, char __user *buf,\n\t\t\t   size_t count, loff_t *ppos)\n{\n\tunsigned long p = *ppos;\n\tu32 *buffer, *dst;\n\tu32 __iomem *src;\n\tint c, i, cnt = 0, err = 0;\n\tunsigned long total_size;\n\tif (!info || !info->screen_base)\n\t\treturn -ENODEV;\n\tif (info->state != FBINFO_STATE_RUNNING)\n\t\treturn -EPERM;\n\ttotal_size = info->screen_size;\n\tif (total_size == 0)\n\t\ttotal_size = info->fix.smem_len;\n\tif (p >= total_size)\n\t\treturn 0;\n\tif (count >= total_size)\n\t\tcount = total_size;\n\tif (count + p > total_size)\n\t\tcount = total_size - p;\n\tbuffer = kmalloc((count > PAGE_SIZE) ? PAGE_SIZE : count, GFP_KERNEL);\n\tif (!buffer)\n\t\treturn -ENOMEM;\n\tsrc = (u32 __iomem *)(info->screen_base + p);\n\tif (info->fbops->fb_sync)\n\t\tinfo->fbops->fb_sync(info);\n\twhile (count) {\n\t\tc = (count > PAGE_SIZE) ? PAGE_SIZE : count;\n\t\tdst = buffer;\n\t\tfor (i = c >> 2; i--;) {\n\t\t\t*dst = fb_readl(src++);\n\t\t\t*dst = big_swap(*dst);\n\t\t\tdst++;\n\t\t}\n\t\tif (c & 3) {\n\t\t\tu8 *dst8 = (u8 *)dst;\n\t\t\tu8 __iomem *src8 = (u8 __iomem *)src;\n\t\t\tfor (i = c & 3; i--;) {\n\t\t\t\tif (i & 1) {\n\t\t\t\t\t*dst8++ = fb_readb(++src8);\n\t\t\t\t} else {\n\t\t\t\t\t*dst8++ = fb_readb(--src8);\n\t\t\t\t\tsrc8 += 2;\n\t\t\t\t}\n\t\t\t}\n\t\t\tsrc = (u32 __iomem *)src8;\n\t\t}\n\t\tif (copy_to_user(buf, buffer, c)) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\t*ppos += c;\n\t\tbuf += c;\n\t\tcnt += c;\n\t\tcount -= c;\n\t}\n\tkfree(buffer);\n\treturn (err) ? err : cnt;\n}",
        "bug_line_number": [
            21,
            22,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46
        ],
        "vul": 1,
        "id": 90
    },
    {
        "code": "\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsecurity_task_getsecid(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\textra_buffers_size += ALIGN(secctx_sz, sizeof(u64));\n\t}\n\ttrace_binder_transaction(reply, t, target_node);\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t    t->buffer, buf_offset,\n\t\t\t\t\t    secctx, secctx_sz);\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;",
        "bug_line_number": [
            28,
            29,
            37,
            38
        ],
        "vul": 1,
        "id": 92
    },
    {
        "code": "\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
        "bug_line_number": [
            28,
            29
        ],
        "vul": 1,
        "id": 92
    },
    {
        "code": "static void ll_free_user_pages(struct page **pages, int npages, int do_dirty)\n{\n\tint i;\n\tfor (i = 0; i < npages; i++) {\n\t\tif (do_dirty)\n\t\t\tset_page_dirty_lock(pages[i]);\n\t\tpage_cache_release(pages[i]);\n\t}\n\tkvfree(pages);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 1
    },
    {
        "code": "static bool kdb_check_flags(kdb_cmdflags_t flags, int permissions,\n\t\t\t\t   bool no_args)\n{\n\t/* permissions comes from userspace so needs massaging slightly */\n\tpermissions &= KDB_ENABLE_MASK;\n\tpermissions |= KDB_ENABLE_ALWAYS_SAFE;\n\t/* some commands change group when launched with no arguments */\n\tif (no_args)\n\t\tpermissions |= permissions << KDB_ENABLE_NO_ARGS_SHIFT;\n\tflags |= KDB_ENABLE_ALL;\n\treturn permissions & flags;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 3
    },
    {
        "code": "\t/* Call the I/O driver's pre_exception routine */\n\tif (dbg_io_ops->pre_exception)\n\t\tdbg_io_ops->pre_exception();\n\t/*\n\t * Get the passive CPU lock which will hold all the non-primary\n\t * CPU in a spin state while the debugger is active\n\t */\n\tif (!kgdb_single_step)\n\t\traw_spin_lock(&dbg_slave_lock);\n#ifdef CONFIG_SMP\n\t/* If send_ready set, slaves are already waiting */\n\tif (ks->send_ready)\n\t\tatomic_set(ks->send_ready, 1);\n\t/* Signal the other CPUs to enter kgdb_wait() */\n\telse if ((!kgdb_single_step) && kgdb_do_roundup)\n\t\tkgdb_roundup_cpus();\n#endif\n\t/*\n\t * Wait for the other CPUs to be notified and be waiting for us:\n\t */\n\ttime_left = MSEC_PER_SEC;\n\twhile (kgdb_do_roundup && --time_left &&\n\t       (atomic_read(&masters_in_kgdb) + atomic_read(&slaves_in_kgdb)) !=\n\t\t   online_cpus)\n\t\tudelay(1000);\n\tif (!time_left)\n\t\tpr_crit(\"Timed out waiting for secondary CPUs.\\n\");\n\t/*\n\t * At this point the primary processor is completely\n\t * in the debugger and all secondary CPUs are quiescent\n\t */\n\tdbg_deactivate_sw_breakpoints();\n\tkgdb_single_step = 0;\n\tkgdb_contthread = current;\n\texception_level = 0;\n\ttrace_on = tracing_is_on();\n\tif (trace_on)\n\t\ttracing_off();\n\twhile (1) {\ncpu_master_loop:\n\t\tif (dbg_kdb_mode) {\n\t\t\tkgdb_connected = 1;\n\t\t\terror = kdb_stub(ks);\n\t\t\tif (error == -1)\n\t\t\t\tcontinue;\n\t\t\tkgdb_connected = 0;\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is a brutal way to interfere with the debugger\n\t\t\t * and prevent gdb being used to poke at kernel memory.\n\t\t\t * This could cause trouble if lockdown is applied when\n\t\t\t * there is already an active gdb session. For now the\n\t\t\t * answer is simply \"don't do that\". Typically lockdown\n\t\t\t * *will* be applied before the debug core gets started\n\t\t\t * so only developers using kgdb for fairly advanced\n\t\t\t * early kernel debug can be biten by this. Hopefully\n\t\t\t * they are sophisticated enough to take care of\n\t\t\t * themselves, especially with help from the lockdown\n\t\t\t * message printed on the console!\n\t\t\t */\n\t\t\tif (security_locked_down(LOCKDOWN_DBG_WRITE_KERNEL)) {\n\t\t\t\tif (IS_ENABLED(CONFIG_KGDB_KDB)) {\n\t\t\t\t\t/* Switch back to kdb if possible... */\n\t\t\t\t\tdbg_kdb_mode = 1;\n\t\t\t\t\tcontinue;\n\t\t\t\t} else {\n\t\t\t\t\t/* ... otherwise just bail */\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\terror = gdb_serial_stub(ks);\n\t\t}\n\t\tif (error == DBG_PASS_EVENT) {\n\t\t\tdbg_kdb_mode = !dbg_kdb_mode;\n\t\t} else if (error == DBG_SWITCH_CPU_EVENT) {\n\t\t\tkgdb_info[dbg_switch_cpu].exception_state |=\n\t\t\t\tDCPU_NEXT_MASTER;\n\t\t\tgoto cpu_loop;\n\t\t} else {\n\t\t\tkgdb_info[cpu].ret_state = error;\n\t\t\tbreak;\n\t\t}\n\t}\n\tdbg_activate_sw_breakpoints();\n\t/* Call the I/O driver's post_exception routine */\n\tif (dbg_io_ops->post_exception)\n\t\tdbg_io_ops->post_exception();\n\tatomic_dec(&ignore_console_lock_warning);\n\tif (!kgdb_single_step) {\n\t\traw_spin_unlock(&dbg_slave_lock);\n\t\t/* Wait till all the CPUs have quit from the debugger. */\n\t\twhile (kgdb_do_roundup && atomic_read(&slaves_in_kgdb))\n\t\t\tcpu_relax();\n\t}\nkgdb_restore:\n\tif (atomic_read(&kgdb_cpu_doing_single_step) != -1) {\n\t\tint sstep_cpu = atomic_read(&kgdb_cpu_doing_single_step);\n\t\tif (kgdb_info[sstep_cpu].task)\n\t\t\tkgdb_sstep_pid = kgdb_info[sstep_cpu].task->pid;\n\t\telse\n\t\t\tkgdb_sstep_pid = 0;\n\t}\n\tif (arch_kgdb_ops.correct_hw_break)\n\t\tarch_kgdb_ops.correct_hw_break();\n\tif (trace_on)\n\t\ttracing_on();\n\tkgdb_info[cpu].debuggerinfo = NULL;",
        "bug_line_number": [],
        "vul": 0,
        "id": 5
    },
    {
        "code": "static inline int l2cap_config_req(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_conf_req *req = (struct l2cap_conf_req *) data;\n\tu16 dcid, flags;\n\tu8 rsp[64];\n\tstruct l2cap_chan *chan;\n\tint len, err = 0;\n\tif (cmd_len < sizeof(*req))\n\t\treturn -EPROTO;\n\tdcid  = __le16_to_cpu(req->dcid);\n\tflags = __le16_to_cpu(req->flags);\n\tBT_DBG(\"dcid 0x%4.4x flags 0x%2.2x\", dcid, flags);\n\tchan = l2cap_get_chan_by_scid(conn, dcid);\n\tif (!chan) {\n\t\tcmd_reject_invalid_cid(conn, cmd->ident, dcid, 0);\n\t\treturn 0;\n\t}\n\tif (chan->state != BT_CONFIG && chan->state != BT_CONNECT2) {\n\t\tcmd_reject_invalid_cid(conn, cmd->ident, chan->scid,\n\t\t\t\t       chan->dcid);\n\t\tgoto unlock;\n\t}\n\t/* Reject if config buffer is too small. */\n\tlen = cmd_len - sizeof(*req);\n\tif (chan->conf_len + len > sizeof(chan->conf_req)) {\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\n\t\t\t       l2cap_build_conf_rsp(chan, rsp,\n\t\t\t       L2CAP_CONF_REJECT, flags), rsp);\n\t\tgoto unlock;\n\t}\n\t/* Store config. */\n\tmemcpy(chan->conf_req + chan->conf_len, req->data, len);\n\tchan->conf_len += len;\n\tif (flags & L2CAP_CONF_FLAG_CONTINUATION) {\n\t\t/* Incomplete config. Send empty response. */\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\n\t\t\t       l2cap_build_conf_rsp(chan, rsp,\n\t\t\t       L2CAP_CONF_SUCCESS, flags), rsp);\n\t\tgoto unlock;\n\t}\n\t/* Complete config. */\n\tlen = l2cap_parse_conf_req(chan, rsp, sizeof(rsp));\n\tif (len < 0) {\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto unlock;\n\t}\n\tchan->ident = cmd->ident;\n\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP, len, rsp);\n\tchan->num_conf_rsp++;\n\t/* Reset config buffer. */\n\tchan->conf_len = 0;\n\tif (!test_bit(CONF_OUTPUT_DONE, &chan->conf_state))\n\t\tgoto unlock;\n\tif (test_bit(CONF_INPUT_DONE, &chan->conf_state)) {\n\t\tset_default_fcs(chan);\n\t\tif (chan->mode == L2CAP_MODE_ERTM ||\n\t\t    chan->mode == L2CAP_MODE_STREAMING)\n\t\t\terr = l2cap_ertm_init(chan);\n\t\tif (err < 0)\n\t\t\tl2cap_send_disconn_req(chan, -err);\n\t\telse\n\t\t\tl2cap_chan_ready(chan);\n\t\tgoto unlock;\n\t}\n\tif (!test_and_set_bit(CONF_REQ_SENT, &chan->conf_state)) {\n\t\tu8 buf[64];\n\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn), L2CAP_CONF_REQ,\n\t\t\t       l2cap_build_conf_req(chan, buf, sizeof(buf)), buf);\n\t\tchan->num_conf_req++;\n\t}\n\t/* Got Conf Rsp PENDING from remote side and assume we sent\n\t   Conf Rsp PENDING in the code above */\n\tif (test_bit(CONF_REM_CONF_PEND, &chan->conf_state) &&\n\t    test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\n\t\t/* check compatibility */\n\t\t/* Send rsp for BR/EDR channel */\n\t\tif (!chan->hs_hcon)\n\t\t\tl2cap_send_efs_conf_rsp(chan, rsp, cmd->ident, flags);\n\t\telse\n\t\t\tchan->ident = cmd->ident;\n\t}\nunlock:\n\tl2cap_chan_unlock(chan);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 7
    },
    {
        "code": "static void l2cap_security_cfm(struct hci_conn *hcon, u8 status, u8 encrypt)\n{\n\tstruct l2cap_conn *conn = hcon->l2cap_data;\n\tstruct l2cap_chan *chan;\n\tif (!conn)\n\t\treturn;\n\tBT_DBG(\"conn %p status 0x%2.2x encrypt %u\", conn, status, encrypt);\n\tmutex_lock(&conn->chan_lock);\n\tlist_for_each_entry(chan, &conn->chan_l, list) {\n\t\tl2cap_chan_lock(chan);\n\t\tBT_DBG(\"chan %p scid 0x%4.4x state %s\", chan, chan->scid,\n\t\t       state_to_string(chan->state));\n\t\tif (chan->scid == L2CAP_CID_A2MP) {\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!status && encrypt)\n\t\t\tchan->sec_level = hcon->sec_level;\n\t\tif (!__l2cap_no_conn_pending(chan)) {\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!status && (chan->state == BT_CONNECTED ||\n\t\t\t\tchan->state == BT_CONFIG)) {\n\t\t\tchan->ops->resume(chan);\n\t\t\tl2cap_check_encryption(chan, encrypt);\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\t\tif (chan->state == BT_CONNECT) {\n\t\t\tif (!status)\n\t\t\t\tl2cap_start_connection(chan);\n\t\t\telse\n\t\t\t\t__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\n\t\t} else if (chan->state == BT_CONNECT2 &&\n\t\t\t   chan->mode != L2CAP_MODE_LE_FLOWCTL) {\n\t\t\tstruct l2cap_conn_rsp rsp;\n\t\t\t__u16 res, stat;\n\t\t\tif (!status) {\n\t\t\t\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\t\t\t\tres = L2CAP_CR_PEND;\n\t\t\t\t\tstat = L2CAP_CS_AUTHOR_PEND;\n\t\t\t\t\tchan->ops->defer(chan);\n\t\t\t\t} else {\n\t\t\t\t\tl2cap_state_change(chan, BT_CONFIG);\n\t\t\t\t\tres = L2CAP_CR_SUCCESS;\n\t\t\t\t\tstat = L2CAP_CS_NO_INFO;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tl2cap_state_change(chan, BT_DISCONN);\n\t\t\t\t__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\n\t\t\t\tres = L2CAP_CR_SEC_BLOCK;\n\t\t\t\tstat = L2CAP_CS_NO_INFO;\n\t\t\t}\n\t\t\trsp.scid   = cpu_to_le16(chan->dcid);\n\t\t\trsp.dcid   = cpu_to_le16(chan->scid);\n\t\t\trsp.result = cpu_to_le16(res);\n\t\t\trsp.status = cpu_to_le16(stat);\n\t\t\tl2cap_send_cmd(conn, chan->ident, L2CAP_CONN_RSP,\n\t\t\t\t       sizeof(rsp), &rsp);\n\t\t\tif (!test_bit(CONF_REQ_SENT, &chan->conf_state) &&\n\t\t\t    res == L2CAP_CR_SUCCESS) {\n\t\t\t\tchar buf[128];\n\t\t\t\tset_bit(CONF_REQ_SENT, &chan->conf_state);\n\t\t\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn),\n\t\t\t\t\t       L2CAP_CONF_REQ,\n\t\t\t\t\t       l2cap_build_conf_req(chan, buf, sizeof(buf)),\n\t\t\t\t\t       buf);\n\t\t\t\tchan->num_conf_req++;\n\t\t\t}\n\t\t}\n\t\tl2cap_chan_unlock(chan);\n\t}\n\tmutex_unlock(&conn->chan_lock);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 9
    },
    {
        "code": "static int __do_proc_dointvec(void *tbl_data, struct ctl_table *table,\n\t\t  int write, void *buffer,\n\t\t  size_t *lenp, loff_t *ppos,\n\t\t  int (*conv)(bool *negp, unsigned long *lvalp, int *valp,\n\t\t\t      int write, void *data),\n\t\t  void *data)\n{\n\tint *i, vleft, first = 1, err = 0;\n\tsize_t left;\n\tchar *p;\n\tif (!tbl_data || !table->maxlen || !*lenp || (*ppos && !write)) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\ti = (int *) tbl_data;\n\tvleft = table->maxlen / sizeof(*i);\n\tleft = *lenp;\n\tif (!conv)\n\t\tconv = do_proc_dointvec_conv;\n\tif (write) {\n\t\tif (proc_first_pos_non_zero_ignore(ppos, table))\n\t\t\tgoto out;\n\t\tif (left > PAGE_SIZE - 1)\n\t\t\tleft = PAGE_SIZE - 1;\n\t\tp = buffer;\n\t}\n\tfor (; left && vleft--; i++, first=0) {\n\t\tunsigned long lval;\n\t\tbool neg;\n\t\tif (write) {\n\t\t\tproc_skip_spaces(&p, &left);\n\t\t\tif (!left)\n\t\t\t\tbreak;\n\t\t\terr = proc_get_long(&p, &left, &lval, &neg,\n\t\t\t\t\t     proc_wspace_sep,\n\t\t\t\t\t     sizeof(proc_wspace_sep), NULL);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (conv(&neg, &lval, i, 1, data)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tif (conv(&neg, &lval, i, 0, data)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!first)\n\t\t\t\tproc_put_char(&buffer, &left, '\\t');\n\t\t\tproc_put_long(&buffer, &left, lval, neg);\n\t\t}\n\t}\n\tif (!write && !first && left && !err)\n\t\tproc_put_char(&buffer, &left, '\\n');\n\tif (write && !err && left)\n\t\tproc_skip_spaces(&p, &left);\n\tif (write && first)\n\t\treturn err ? : -EINVAL;\n\t*lenp -= left;\nout:\n\t*ppos += *lenp;\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 11
    },
    {
        "code": "static void perf_group_attach(struct perf_event *event)\n{\n\tstruct perf_event *group_leader = event->group_leader, *pos;\n\tlockdep_assert_held(&event->ctx->lock);\n\t/*\n\t * We can have double attach due to group movement (move_group) in\n\t * perf_event_open().\n\t */\n\tif (event->attach_state & PERF_ATTACH_GROUP)\n\t\treturn;\n\tevent->attach_state |= PERF_ATTACH_GROUP;\n\tif (group_leader == event)\n\t\treturn;\n\tWARN_ON_ONCE(group_leader->ctx != event->ctx);\n\tgroup_leader->group_caps &= event->event_caps;\n\tlist_add_tail(&event->sibling_list, &group_leader->sibling_list);\n\tgroup_leader->nr_siblings++;\n\tgroup_leader->group_generation++;\n\tperf_event__header_size(group_leader);\n\tfor_each_sibling_event(pos, group_leader)\n\t\tperf_event__header_size(pos);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 13
    },
    {
        "code": "static void perf_group_detach(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct perf_event *sibling, *tmp;\n\tstruct perf_event_context *ctx = event->ctx;\n\tlockdep_assert_held(&ctx->lock);\n\t/*\n\t * We can have double detach due to exit/hot-unplug + close.\n\t */\n\tif (!(event->attach_state & PERF_ATTACH_GROUP))\n\t\treturn;\n\tevent->attach_state &= ~PERF_ATTACH_GROUP;\n\tperf_put_aux_event(event);\n\t/*\n\t * If this is a sibling, remove it from its group.\n\t */\n\tif (leader != event) {\n\t\tlist_del_init(&event->sibling_list);\n\t\tevent->group_leader->nr_siblings--;\n\t\tevent->group_leader->group_generation++;\n\t\tgoto out;\n\t}\n\t/*\n\t * If this was a group event with sibling events then\n\t * upgrade the siblings to singleton events by adding them\n\t * to whatever list we are on.\n\t */\n\tlist_for_each_entry_safe(sibling, tmp, &event->sibling_list, sibling_list) {\n\t\tif (sibling->event_caps & PERF_EV_CAP_SIBLING)\n\t\t\tperf_remove_sibling_event(sibling);\n\t\tsibling->group_leader = sibling;\n\t\tlist_del_init(&sibling->sibling_list);\n\t\t/* Inherit group flags from the previous leader */\n\t\tsibling->group_caps = event->group_caps;\n\t\tif (sibling->attach_state & PERF_ATTACH_CONTEXT) {\n\t\t\tadd_event_to_groups(sibling, event->ctx);\n\t\t\tif (sibling->state == PERF_EVENT_STATE_ACTIVE)\n\t\t\t\tlist_add_tail(&sibling->active_list, get_event_list(sibling));\n\t\t}\n\t\tWARN_ON_ONCE(sibling->ctx != event->ctx);\n\t}\nout:\n\tfor_each_sibling_event(tmp, leader)\n\t\tperf_event__header_size(tmp);\n\tperf_event__header_size(leader);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 15
    },
    {
        "code": "static int perf_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *child;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tint ret;\n\tu64 *values;\n\tlockdep_assert_held(&ctx->mutex);\n\tvalues = kzalloc(event->read_size, GFP_KERNEL);\n\tif (!values)\n\t\treturn -ENOMEM;\n\tvalues[0] = 1 + leader->nr_siblings;\n\tmutex_lock(&leader->child_mutex);\n\tret = __perf_read_group_add(leader, read_format, values);\n\tif (ret)\n\t\tgoto unlock;\n\tlist_for_each_entry(child, &leader->child_list, child_list) {\n\t\tret = __perf_read_group_add(child, read_format, values);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\t}\n\tmutex_unlock(&leader->child_mutex);\n\tret = event->read_size;\n\tif (copy_to_user(buf, values, event->read_size))\n\t\tret = -EFAULT;\n\tgoto out;\nunlock:\n\tmutex_unlock(&leader->child_mutex);\nout:\n\tkfree(values);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 17
    },
    {
        "code": "int iscsi_session_get_param(struct iscsi_cls_session *cls_session,\n\t\t\t    enum iscsi_param param, char *buf)\n{\n\tstruct iscsi_session *session = cls_session->dd_data;\n\tint len;\n\tswitch(param) {\n\tcase ISCSI_PARAM_FAST_ABORT:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->fast_abort);\n\t\tbreak;\n\tcase ISCSI_PARAM_ABORT_TMO:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->abort_timeout);\n\t\tbreak;\n\tcase ISCSI_PARAM_LU_RESET_TMO:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->lu_reset_timeout);\n\t\tbreak;\n\tcase ISCSI_PARAM_TGT_RESET_TMO:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->tgt_reset_timeout);\n\t\tbreak;\n\tcase ISCSI_PARAM_INITIAL_R2T_EN:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->initial_r2t_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_MAX_R2T:\n\t\tlen = sysfs_emit(buf, \"%hu\\n\", session->max_r2t);\n\t\tbreak;\n\tcase ISCSI_PARAM_IMM_DATA_EN:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->imm_data_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_FIRST_BURST:\n\t\tlen = sysfs_emit(buf, \"%u\\n\", session->first_burst);\n\t\tbreak;\n\tcase ISCSI_PARAM_MAX_BURST:\n\t\tlen = sysfs_emit(buf, \"%u\\n\", session->max_burst);\n\t\tbreak;\n\tcase ISCSI_PARAM_PDU_INORDER_EN:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->pdu_inorder_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_DATASEQ_INORDER_EN:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->dataseq_inorder_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_DEF_TASKMGMT_TMO:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->def_taskmgmt_tmo);\n\t\tbreak;\n\tcase ISCSI_PARAM_ERL:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->erl);\n\t\tbreak;\n\tcase ISCSI_PARAM_TARGET_NAME:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->targetname);\n\t\tbreak;\n\tcase ISCSI_PARAM_TARGET_ALIAS:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->targetalias);\n\t\tbreak;\n\tcase ISCSI_PARAM_TPGT:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->tpgt);\n\t\tbreak;\n\tcase ISCSI_PARAM_USERNAME:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->username);\n\t\tbreak;\n\tcase ISCSI_PARAM_USERNAME_IN:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->username_in);\n\t\tbreak;\n\tcase ISCSI_PARAM_PASSWORD:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->password);\n\t\tbreak;\n\tcase ISCSI_PARAM_PASSWORD_IN:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->password_in);\n\t\tbreak;\n\tcase ISCSI_PARAM_IFACE_NAME:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->ifacename);\n\t\tbreak;\n\tcase ISCSI_PARAM_INITIATOR_NAME:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->initiatorname);\n\t\tbreak;\n\tcase ISCSI_PARAM_BOOT_ROOT:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->boot_root);\n\t\tbreak;\n\tcase ISCSI_PARAM_BOOT_NIC:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->boot_nic);\n\t\tbreak;\n\tcase ISCSI_PARAM_BOOT_TARGET:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->boot_target);\n\t\tbreak;\n\tcase ISCSI_PARAM_AUTO_SND_TGT_DISABLE:\n\t\tlen = sysfs_emit(buf, \"%u\\n\", session->auto_snd_tgt_disable);\n\t\tbreak;\n\tcase ISCSI_PARAM_DISCOVERY_SESS:\n\t\tlen = sysfs_emit(buf, \"%u\\n\", session->discovery_sess);\n\t\tbreak;\n\tcase ISCSI_PARAM_PORTAL_TYPE:\n\t\tlen = sysfs_emit(buf, \"%s\\n\", session->portal_type);\n\t\tbreak;\n\tcase ISCSI_PARAM_CHAP_AUTH_EN:\n\t\tlen = sysfs_emit(buf, \"%u\\n\", session->chap_auth_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_DISCOVERY_LOGOUT_EN:\n\t\tlen = sysfs_emit(buf, \"%u\\n\", session->discovery_logout_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_BIDI_CHAP_EN:\n\t\tlen = sysfs_emit(buf, \"%u\\n\", session->bidi_chap_en);\n\t\tbreak;\n\tcase ISCSI_PARAM_DISCOVERY_AUTH_OPTIONAL:\n\t\tlen = sysfs_emit(buf, \"%u\\n\", session->discovery_auth_optional);\n\t\tbreak;\n\tcase ISCSI_PARAM_DEF_TIME2WAIT:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->time2wait);\n\t\tbreak;\n\tcase ISCSI_PARAM_DEF_TIME2RETAIN:\n\t\tlen = sysfs_emit(buf, \"%d\\n\", session->time2retain);\n\t\tbreak;\n\tcase ISCSI_PARAM_TSID:\n\t\tlen = sysfs_emit(buf, \"%u\\n\", session->tsid);\n\t\tbreak;\n\tcase ISCSI_PARAM_ISID:\n\t\tlen = sysfs_emit(buf, \"%02x%02x%02x%02x%02x%02x\\n\",\n\t\t\t      session->isid[0], session->isid[1],\n\t\t\t      session->isid[2], session->isid[3],\n\t\t\t      session->isid[4], session->isid[5]);\n\t\tbreak;\n\tcase ISCSI_PARAM_DISCOVERY_PARENT_IDX:\n\t\tlen = sysfs_emit(buf, \"%u\\n\", session->discovery_parent_idx);",
        "bug_line_number": [],
        "vul": 0,
        "id": 19
    },
    {
        "code": "\t\tbreak;\n\tcase ISCSI_PARAM_DISCOVERY_PARENT_TYPE:\n\t\tif (session->discovery_parent_type)\n\t\t\tlen = sysfs_emit(buf, \"%s\\n\",\n\t\t\t\t      session->discovery_parent_type);\n\t\telse\n\t\t\tlen = sysfs_emit(buf, \"\\n\");\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOSYS;\n\t}\n\treturn len;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 19
    },
    {
        "code": "static unsigned long fbcon_getxy(struct vc_data *vc, unsigned long pos,\n\t\t\t\t int *px, int *py)\n{\n\tunsigned long ret;\n\tint x, y;\n\tif (pos >= vc->vc_origin && pos < vc->vc_scr_end) {\n\t\tunsigned long offset = (pos - vc->vc_origin) / 2;\n\t\tx = offset % vc->vc_cols;\n\t\ty = offset / vc->vc_cols;\n\t\tret = pos + (vc->vc_cols - x) * 2;\n\t} else {\n\t\t/* Should not happen */\n\t\tx = y = 0;\n\t\tret = vc->vc_origin;\n\t}\n\tif (px)\n\t\t*px = x;\n\tif (py)\n\t\t*py = y;\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 21
    },
    {
        "code": "static void can_can_gw_rcv(struct sk_buff *skb, void *data)\n{\n\tstruct cgw_job *gwj = (struct cgw_job *)data;\n\tstruct can_frame *cf;\n\tstruct sk_buff *nskb;\n\tint modidx = 0;\n\t/*\n\t * Do not handle CAN frames routed more than 'max_hops' times.\n\t * In general we should never catch this delimiter which is intended\n\t * to cover a misconfiguration protection (e.g. circular CAN routes).\n\t *\n\t * The Controller Area Network controllers only accept CAN frames with\n\t * correct CRCs - which are not visible in the controller registers.\n\t * According to skbuff.h documentation the csum_start element for IP\n\t * checksums is undefined/unused when ip_summed == CHECKSUM_UNNECESSARY.\n\t * Only CAN skbs can be processed here which already have this property.\n\t */\n#define cgw_hops(skb) ((skb)->csum_start)\n\tBUG_ON(skb->ip_summed != CHECKSUM_UNNECESSARY);\n\tif (cgw_hops(skb) >= max_hops) {\n\t\t/* indicate deleted frames due to misconfiguration */\n\t\tgwj->deleted_frames++;\n\t\treturn;\n\t}\n\tif (!(gwj->dst.dev->flags & IFF_UP)) {\n\t\tgwj->dropped_frames++;\n\t\treturn;\n\t}\n\t/* is sending the skb back to the incoming interface not allowed? */\n\tif (!(gwj->flags & CGW_FLAGS_CAN_IIF_TX_OK) &&\n\t    can_skb_prv(skb)->ifindex == gwj->dst.dev->ifindex)\n\t\treturn;\n\t/*\n\t * clone the given skb, which has not been done in can_rcv()\n\t *\n\t * When there is at least one modification function activated,\n\t * we need to copy the skb as we want to modify skb->data.\n\t */\n\tif (gwj->mod.modfunc[0])\n\t\tnskb = skb_copy(skb, GFP_ATOMIC);\n\telse\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\tif (!nskb) {\n\t\tgwj->dropped_frames++;\n\t\treturn;\n\t}\n\t/* put the incremented hop counter in the cloned skb */\n\tcgw_hops(nskb) = cgw_hops(skb) + 1;\n\t/* first processing of this CAN frame -> adjust to private hop limit */\n\tif (gwj->limit_hops && cgw_hops(nskb) == 1)\n\t\tcgw_hops(nskb) = max_hops - gwj->limit_hops + 1;\n\tnskb->dev = gwj->dst.dev;\n\t/* pointer to modifiable CAN frame */\n\tcf = (struct can_frame *)nskb->data;\n\t/* perform preprocessed modification functions if there are any */\n\twhile (modidx < MAX_MODFUNCTIONS && gwj->mod.modfunc[modidx])\n\t\t(*gwj->mod.modfunc[modidx++])(cf, &gwj->mod);\n\t/* Has the CAN frame been modified? */\n\tif (modidx) {\n\t\t/* get available space for the processed CAN frame type */\n\t\tint max_len = nskb->len - offsetof(struct can_frame, data);\n\t\t/* dlc may have changed, make sure it fits to the CAN frame */\n\t\tif (cf->can_dlc > max_len)\n\t\t\tgoto out_delete;\n\t\t/* check for checksum updates in classic CAN length only */\n\t\tif (gwj->mod.csumfunc.crc8) {\n\t\t\tif (cf->can_dlc > 8)\n\t\t\t\tgoto out_delete;\n\t\t\t(*gwj->mod.csumfunc.crc8)(cf, &gwj->mod.csum.crc8);\n\t\t}\n\t\tif (gwj->mod.csumfunc.xor) {\n\t\t\tif (cf->can_dlc > 8)\n\t\t\t\tgoto out_delete;\n\t\t\t(*gwj->mod.csumfunc.xor)(cf, &gwj->mod.csum.xor);\n\t\t}\n\t}\n\t/* clear the skb timestamp if not configured the other way */\n\tif (!(gwj->flags & CGW_FLAGS_CAN_SRC_TSTAMP))\n\t\tnskb->tstamp = 0;\n\t/* send to netdevice */\n\tif (can_send(nskb, gwj->flags & CGW_FLAGS_CAN_ECHO))\n\t\tgwj->dropped_frames++;\n\telse\n\t\tgwj->handled_frames++;\n\treturn;\n out_delete:\n\t/* delete frame due to misconfiguration */\n\tgwj->deleted_frames++;\n\tkfree_skb(nskb);\n\treturn;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 23
    },
    {
        "code": "static void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode, bool is_jmp32)\n{\n\ts64 sval;\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\tval = is_jmp32 ? (u32)val : val;\n\tsval = is_jmp32 ? (s64)(s32)val : (s64)val;\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\tcase BPF_JNE:\n\t{\n\t\tstruct bpf_reg_state *reg =\n\t\t\topcode == BPF_JEQ ? true_reg : false_reg;\n\t\t/* For BPF_JEQ, if this is false we know nothing Jon Snow, but\n\t\t * if it is true we know the value for sure. Likewise for\n\t\t * BPF_JNE.\n\t\t */\n\t\tif (is_jmp32) {\n\t\t\tu64 old_v = reg->var_off.value;\n\t\t\tu64 hi_mask = ~0xffffffffULL;\n\t\t\treg->var_off.value = (old_v & hi_mask) | val;\n\t\t\treg->var_off.mask &= hi_mask;\n\t\t} else {\n\t\t\t__mark_reg_known(reg, val);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGE:\n\tcase BPF_JGT:\n\t{\n\t\tu64 false_umax = opcode == BPF_JGT ? val    : val - 1;\n\t\tu64 true_umin = opcode == BPF_JGT ? val + 1 : val;\n\t\tif (is_jmp32) {\n\t\t\tfalse_umax += gen_hi_max(false_reg->var_off);\n\t\t\ttrue_umin += gen_hi_min(true_reg->var_off);\n\t\t}\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, false_umax);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, true_umin);\n\t\tbreak;\n\t}\n\tcase BPF_JSGE:\n\tcase BPF_JSGT:\n\t{\n\t\ts64 false_smax = opcode == BPF_JSGT ? sval    : sval - 1;\n\t\ts64 true_smin = opcode == BPF_JSGT ? sval + 1 : sval;\n\t\t/* If the full s64 was not sign-extended from s32 then don't\n\t\t * deduct further info.\n\t\t */\n\t\tif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\n\t\t\tbreak;\n\t\tfalse_reg->smax_value = min(false_reg->smax_value, false_smax);\n\t\ttrue_reg->smin_value = max(true_reg->smin_value, true_smin);\n\t\tbreak;\n\t}\n\tcase BPF_JLE:\n\tcase BPF_JLT:\n\t{\n\t\tu64 false_umin = opcode == BPF_JLT ? val    : val + 1;\n\t\tu64 true_umax = opcode == BPF_JLT ? val - 1 : val;\n\t\tif (is_jmp32) {\n\t\t\tfalse_umin += gen_hi_min(false_reg->var_off);\n\t\t\ttrue_umax += gen_hi_max(true_reg->var_off);\n\t\t}\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, false_umin);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, true_umax);\n\t\tbreak;\n\t}\n\tcase BPF_JSLE:\n\tcase BPF_JSLT:\n\t{\n\t\ts64 false_smin = opcode == BPF_JSLT ? sval    : sval + 1;\n\t\ts64 true_smax = opcode == BPF_JSLT ? sval - 1 : sval;\n\t\tif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\n\t\t\tbreak;\n\t\tfalse_reg->smin_value = max(false_reg->smin_value, false_smin);\n\t\ttrue_reg->smax_value = min(true_reg->smax_value, true_smax);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 25
    },
    {
        "code": "static int hash__init_new_context(struct mm_struct *mm)\n{\n\tint index;\n\t/*\n\t * The old code would re-promote on fork, we don't do that when using\n\t * slices as it could cause problem promoting slices that have been\n\t * forced down to 4K.\n\t *\n\t * For book3s we have MMU_NO_CONTEXT set to be ~0. Hence check\n\t * explicitly against context.id == 0. This ensures that we properly\n\t * initialize context slice details for newly allocated mm's (which will\n\t * have id == 0) and don't alter context slice inherited via fork (which\n\t * will have id != 0).\n\t *\n\t * We should not be calling init_new_context() on init_mm. Hence a\n\t * check against 0 is OK.\n\t */\n\tif (mm->context.id == 0)\n\t\tslice_init_new_context_exec(mm);\n\tindex = realloc_context_ids(&mm->context);\n\tif (index < 0)\n\t\treturn index;\n\tsubpage_prot_init_new_context(mm);\n\tpkey_mm_init(mm);\n\treturn index;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 27
    },
    {
        "code": "static int size_entry_mwt(struct ebt_entry *entry, const unsigned char *base,\n\t\t\t  unsigned int *total,\n\t\t\t  struct ebt_entries_buf_state *state)\n{\n\tunsigned int i, j, startoff, new_offset = 0;\n\t/* stores match/watchers/targets & offset of next struct ebt_entry: */\n\tunsigned int offsets[4];\n\tunsigned int *offsets_update = NULL;\n\tint ret;\n\tchar *buf_start;\n\tif (*total < sizeof(struct ebt_entries))\n\t\treturn -EINVAL;\n\tif (!entry->bitmask) {\n\t\t*total -= sizeof(struct ebt_entries);\n\t\treturn ebt_buf_add(state, entry, sizeof(struct ebt_entries));\n\t}\n\tif (*total < sizeof(*entry) || entry->next_offset < sizeof(*entry))\n\t\treturn -EINVAL;\n\tstartoff = state->buf_user_offset;\n\t/* pull in most part of ebt_entry, it does not need to be changed. */\n\tret = ebt_buf_add(state, entry,\n\t\t\toffsetof(struct ebt_entry, watchers_offset));\n\tif (ret < 0)\n\t\treturn ret;\n\toffsets[0] = sizeof(struct ebt_entry); /* matches come first */\n\tmemcpy(&offsets[1], &entry->watchers_offset,\n\t\t\tsizeof(offsets) - sizeof(offsets[0]));\n\tif (state->buf_kern_start) {\n\t\tbuf_start = state->buf_kern_start + state->buf_kern_offset;\n\t\toffsets_update = (unsigned int *) buf_start;\n\t}\n\tret = ebt_buf_add(state, &offsets[1],\n\t\t\tsizeof(offsets) - sizeof(offsets[0]));\n\tif (ret < 0)\n\t\treturn ret;\n\tbuf_start = (char *) entry;\n\t/* 0: matches offset, always follows ebt_entry.\n\t * 1: watchers offset, from ebt_entry structure\n\t * 2: target offset, from ebt_entry structure\n\t * 3: next ebt_entry offset, from ebt_entry structure\n\t *\n\t * offsets are relative to beginning of struct ebt_entry (i.e., 0).\n\t */\n\tfor (i = 0; i < 4 ; ++i) {\n\t\tif (offsets[i] >= *total)\n\t\t\treturn -EINVAL;\n\t\tif (i == 0)\n\t\t\tcontinue;\n\t\tif (offsets[i-1] > offsets[i])\n\t\t\treturn -EINVAL;\n\t}\n\tfor (i = 0, j = 1 ; j < 4 ; j++, i++) {\n\t\tstruct compat_ebt_entry_mwt *match32;\n\t\tunsigned int size;\n\t\tchar *buf = buf_start + offsets[i];\n\t\tif (offsets[i] > offsets[j])\n\t\t\treturn -EINVAL;\n\t\tmatch32 = (struct compat_ebt_entry_mwt *) buf;\n\t\tsize = offsets[j] - offsets[i];\n\t\tret = ebt_size_mwt(match32, size, i, state, base);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tnew_offset += ret;\n\t\tif (offsets_update && new_offset) {\n\t\t\tpr_debug(\"change offset %d to %d\\n\",\n\t\t\t\toffsets_update[i], offsets[j] + new_offset);\n\t\t\toffsets_update[i] = offsets[j] + new_offset;\n\t\t}\n\t}\n\tif (state->buf_kern_start == NULL) {\n\t\tunsigned int offset = buf_start - (char *) base;\n\t\tret = xt_compat_add_offset(NFPROTO_BRIDGE, offset, new_offset);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\tstartoff = state->buf_user_offset - startoff;\n\tif (WARN_ON(*total < startoff))\n\t\treturn -EINVAL;\n\t*total -= startoff;\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 29
    },
    {
        "code": "static ssize_t dp_link_settings_write(struct file *f, const char __user *buf,\n\t\t\t\t size_t size, loff_t *pos)\n{\n\tstruct amdgpu_dm_connector *connector = file_inode(f)->i_private;\n\tstruct dc_link *link = connector->dc_link;\n\tstruct dc_link_settings prefer_link_settings;\n\tchar *wr_buf = NULL;\n\tconst uint32_t wr_buf_size = 40;\n\t/* 0: lane_count; 1: link_rate */\n\tint max_param_num = 2;\n\tuint8_t param_nums = 0;\n\tlong param[2];\n\tbool valid_input = true;\n\tif (size == 0)\n\t\treturn -EINVAL;\n\twr_buf = kcalloc(wr_buf_size, sizeof(char), GFP_KERNEL);\n\tif (!wr_buf)\n\t\treturn -ENOSPC;\n\tif (parse_write_buffer_into_params(wr_buf, wr_buf_size,\n\t\t\t\t\t   (long *)param, buf,\n\t\t\t\t\t   max_param_num,\n\t\t\t\t\t   &param_nums)) {\n\t\tkfree(wr_buf);\n\t\treturn -EINVAL;\n\t}\n\tif (param_nums <= 0) {\n\t\tkfree(wr_buf);\n\t\tDRM_DEBUG_DRIVER(\"user data not be read\\n\");\n\t\treturn -EINVAL;\n\t}\n\tswitch (param[0]) {\n\tcase LANE_COUNT_ONE:\n\tcase LANE_COUNT_TWO:\n\tcase LANE_COUNT_FOUR:\n\t\tbreak;\n\tdefault:\n\t\tvalid_input = false;\n\t\tbreak;\n\t}\n\tswitch (param[1]) {\n\tcase LINK_RATE_LOW:\n\tcase LINK_RATE_HIGH:\n\tcase LINK_RATE_RBR2:\n\tcase LINK_RATE_HIGH2:\n\tcase LINK_RATE_HIGH3:\n\t\tbreak;\n\tdefault:\n\t\tvalid_input = false;\n\t\tbreak;\n\t}\n\tif (!valid_input) {\n\t\tkfree(wr_buf);\n\t\tDRM_DEBUG_DRIVER(\"Invalid Input value No HW will be programmed\\n\");\n\t\treturn size;\n\t}\n\t/* save user force lane_count, link_rate to preferred settings\n\t * spread spectrum will not be changed\n\t */\n\tprefer_link_settings.link_spread = link->cur_link_settings.link_spread;\n\tprefer_link_settings.use_link_rate_set = false;\n\tprefer_link_settings.lane_count = param[0];\n\tprefer_link_settings.link_rate = param[1];\n\tdp_retrain_link_dp_test(link, &prefer_link_settings, false);\n\tkfree(wr_buf);\n\treturn size;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 31
    },
    {
        "code": "static int nvme_ns_ioctl(struct nvme_ns *ns, unsigned int cmd,\n\t\tvoid __user *argp)\n{\n\tswitch (cmd) {\n\tcase NVME_IOCTL_ID:\n\t\tforce_successful_syscall_return();\n\t\treturn ns->head->ns_id;\n\tcase NVME_IOCTL_IO_CMD:\n\t\treturn nvme_user_cmd(ns->ctrl, ns, argp);\n\t/*\n\t * struct nvme_user_io can have different padding on some 32-bit ABIs.\n\t * Just accept the compat version as all fields that are used are the\n\t * same size and at the same offset.\n\t */\n#ifdef COMPAT_FOR_U64_ALIGNMENT\n\tcase NVME_IOCTL_SUBMIT_IO32:\n#endif\n\tcase NVME_IOCTL_SUBMIT_IO:\n\t\treturn nvme_submit_io(ns, argp);\n\tcase NVME_IOCTL_IO64_CMD:\n\t\treturn nvme_user_cmd64(ns->ctrl, ns, argp);\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 33
    },
    {
        "code": "static int put_chars(u32 vtermno, const char *buf, int count)\n{\n\tstruct port *port;\n\tstruct scatterlist sg[1];\n\tvoid *data;\n\tint ret;\n\tif (unlikely(early_put_chars))\n\t\treturn early_put_chars(vtermno, buf, count);\n\tport = find_port_by_vtermno(vtermno);\n\tif (!port)\n\t\treturn -EPIPE;\n\tdata = kmemdup(buf, count, GFP_ATOMIC);\n\tif (!data)\n\t\treturn -ENOMEM;\n\tsg_init_one(sg, data, count);\n\tret = __send_to_port(port, sg, 1, count, data, false);\n\tkfree(data);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 35
    },
    {
        "code": "static int ipvlan_process_v4_outbound(struct sk_buff *skb)\n{\n\tconst struct iphdr *ip4h = ip_hdr(skb);\n\tstruct net_device *dev = skb->dev;\n\tstruct net *net = dev_net(dev);\n\tstruct rtable *rt;\n\tint err, ret = NET_XMIT_DROP;\n\tstruct flowi4 fl4 = {\n\t\t.flowi4_oif = dev->ifindex,\n\t\t.flowi4_tos = RT_TOS(ip4h->tos),\n\t\t.flowi4_flags = FLOWI_FLAG_ANYSRC,\n\t\t.flowi4_mark = skb->mark,\n\t\t.daddr = ip4h->daddr,\n\t\t.saddr = ip4h->saddr,\n\t};\n\trt = ip_route_output_flow(net, &fl4, NULL);\n\tif (IS_ERR(rt))\n\t\tgoto err;\n\tif (rt->rt_type != RTN_UNICAST && rt->rt_type != RTN_LOCAL) {\n\t\tip_rt_put(rt);\n\t\tgoto err;\n\t}\n\tskb_dst_set(skb, &rt->dst);\n\tmemset(IPCB(skb), 0, sizeof(*IPCB(skb)));\n\terr = ip_local_out(net, skb->sk, skb);\n\tif (unlikely(net_xmit_eval(err)))\n\t\tdev->stats.tx_errors++;\n\telse\n\t\tret = NET_XMIT_SUCCESS;\n\tgoto out;\nerr:\n\tdev->stats.tx_errors++;\n\tkfree_skb(skb);\nout:\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 37
    },
    {
        "code": "void l2tp_tunnel_closeall(struct l2tp_tunnel *tunnel)\n{\n\tint hash;\n\tstruct hlist_node *walk;\n\tstruct hlist_node *tmp;\n\tstruct l2tp_session *session;\n\tBUG_ON(tunnel == NULL);\n\tl2tp_info(tunnel, L2TP_MSG_CONTROL, \"%s: closing all sessions...\\n\",\n\t\t  tunnel->name);\n\twrite_lock_bh(&tunnel->hlist_lock);\n\ttunnel->acpt_newsess = false;\n\tfor (hash = 0; hash < L2TP_HASH_SIZE; hash++) {\nagain:\n\t\thlist_for_each_safe(walk, tmp, &tunnel->session_hlist[hash]) {\n\t\t\tsession = hlist_entry(walk, struct l2tp_session, hlist);\n\t\t\tl2tp_info(session, L2TP_MSG_CONTROL,\n\t\t\t\t  \"%s: closing session\\n\", session->name);\n\t\t\thlist_del_init(&session->hlist);\n\t\t\tif (test_and_set_bit(0, &session->dead))\n\t\t\t\tgoto again;\n\t\t\tif (session->ref != NULL)\n\t\t\t\t(*session->ref)(session);\n\t\t\twrite_unlock_bh(&tunnel->hlist_lock);\n\t\t\t__l2tp_session_unhash(session);\n\t\t\tl2tp_session_queue_purge(session);\n\t\t\tif (session->session_close != NULL)\n\t\t\t\t(*session->session_close)(session);\n\t\t\tif (session->deref != NULL)\n\t\t\t\t(*session->deref)(session);\n\t\t\tl2tp_session_dec_refcount(session);\n\t\t\twrite_lock_bh(&tunnel->hlist_lock);\n\t\t\t/* Now restart from the beginning of this hash\n\t\t\t * chain.  We always remove a session from the\n\t\t\t * list so we are guaranteed to make forward\n\t\t\t * progress.\n\t\t\t */\n\t\t\tgoto again;\n\t\t}\n\t}\n\twrite_unlock_bh(&tunnel->hlist_lock);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 39
    },
    {
        "code": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, hdrlen;\n\tunsigned int netoff;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\tbool do_vnet = false;\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\tif (dev->header_ops) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\tsnaplen = skb->len;\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\t/* If we are flooded, just give up */\n\tif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (netoff > USHRT_MAX) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb)\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tdo_vnet = false;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tdo_vnet = false;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,",
        "bug_line_number": [],
        "vul": 0,
        "id": 41
    },
    {
        "code": "static int v4l_enum_fmt(const struct v4l2_ioctl_ops *ops,\n\t\t\t\tstruct file *file, void *fh, void *arg)\n{\n\tstruct v4l2_fmtdesc *p = arg;\n\tint ret = check_fmt(file, p->type);\n\tif (ret)\n\t\treturn ret;\n\tret = -EINVAL;\n\tswitch (p->type) {\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_enum_fmt_vid_cap))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_vid_cap(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\n\t\tif (unlikely(!ops->vidioc_enum_fmt_vid_cap_mplane))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_vid_cap_mplane(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_VIDEO_OVERLAY:\n\t\tif (unlikely(!ops->vidioc_enum_fmt_vid_overlay))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_vid_overlay(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT:\n\t\tif (unlikely(!ops->vidioc_enum_fmt_vid_out))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_vid_out(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\n\t\tif (unlikely(!ops->vidioc_enum_fmt_vid_out_mplane))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_vid_out_mplane(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_SDR_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_enum_fmt_sdr_cap))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_sdr_cap(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_SDR_OUTPUT:\n\t\tif (unlikely(!ops->vidioc_enum_fmt_sdr_out))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_sdr_out(file, fh, arg);\n\t\tbreak;\n\tcase V4L2_BUF_TYPE_META_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_enum_fmt_meta_cap))\n\t\t\tbreak;\n\t\tret = ops->vidioc_enum_fmt_meta_cap(file, fh, arg);\n\t\tbreak;\n\t}\n\tif (ret == 0)\n\t\tv4l_fill_fmtdesc(p);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 43
    },
    {
        "code": "static int con_font_get(struct vc_data *vc, struct console_font_op *op)\n{\n\tstruct console_font font;\n\tint rc = -EINVAL;\n\tint c;\n\tif (op->data) {\n\t\tfont.data = kmalloc(max_font_size, GFP_KERNEL);\n\t\tif (!font.data)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tfont.data = NULL;\n\tconsole_lock();\n\tif (vc->vc_mode != KD_TEXT)\n\t\trc = -EINVAL;\n\telse if (vc->vc_sw->con_font_get)\n\t\trc = vc->vc_sw->con_font_get(vc, &font);\n\telse\n\t\trc = -ENOSYS;\n\tconsole_unlock();\n\tif (rc)\n\t\tgoto out;\n\tc = (font.width+7)/8 * 32 * font.charcount;\n\tif (op->data && font.charcount > op->charcount)\n\t\trc = -ENOSPC;\n\tif (font.width > op->width || font.height > op->height)\n\t\trc = -ENOSPC;\n\tif (rc)\n\t\tgoto out;\n\top->height = font.height;\n\top->width = font.width;\n\top->charcount = font.charcount;\n\tif (op->data && copy_to_user(op->data, font.data, c))\n\t\trc = -EFAULT;\nout:\n\tkfree(font.data);\n\treturn rc;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 45
    },
    {
        "code": "int smb_inherit_dacl(struct ksmbd_conn *conn,\n\t\t     struct path *path,\n\t\t     unsigned int uid, unsigned int gid)\n{\n\tconst struct smb_sid *psid, *creator = NULL;\n\tstruct smb_ace *parent_aces, *aces;\n\tstruct smb_acl *parent_pdacl;\n\tstruct smb_ntsd *parent_pntsd = NULL;\n\tstruct smb_sid owner_sid, group_sid;\n\tstruct dentry *parent = path->dentry->d_parent;\n\tstruct user_namespace *user_ns = mnt_user_ns(path->mnt);\n\tint inherited_flags = 0, flags = 0, i, ace_cnt = 0, nt_size = 0, pdacl_size;\n\tint rc = 0, num_aces, dacloffset, pntsd_type, pntsd_size, acl_len, aces_size;\n\tchar *aces_base;\n\tbool is_dir = S_ISDIR(d_inode(path->dentry)->i_mode);\n\tpntsd_size = ksmbd_vfs_get_sd_xattr(conn, user_ns,\n\t\t\t\t\t    parent, &parent_pntsd);\n\tif (pntsd_size <= 0)\n\t\treturn -ENOENT;\n\tdacloffset = le32_to_cpu(parent_pntsd->dacloffset);\n\tif (!dacloffset || (dacloffset + sizeof(struct smb_acl) > pntsd_size)) {\n\t\trc = -EINVAL;\n\t\tgoto free_parent_pntsd;\n\t}\n\tparent_pdacl = (struct smb_acl *)((char *)parent_pntsd + dacloffset);\n\tacl_len = pntsd_size - dacloffset;\n\tnum_aces = le32_to_cpu(parent_pdacl->num_aces);\n\tpntsd_type = le16_to_cpu(parent_pntsd->type);\n\tpdacl_size = le16_to_cpu(parent_pdacl->size);\n\tif (pdacl_size > acl_len || pdacl_size < sizeof(struct smb_acl)) {\n\t\trc = -EINVAL;\n\t\tgoto free_parent_pntsd;\n\t}\n\taces_base = kmalloc(sizeof(struct smb_ace) * num_aces * 2, GFP_KERNEL);\n\tif (!aces_base) {\n\t\trc = -ENOMEM;\n\t\tgoto free_parent_pntsd;\n\t}\n\taces = (struct smb_ace *)aces_base;\n\tparent_aces = (struct smb_ace *)((char *)parent_pdacl +\n\t\t\tsizeof(struct smb_acl));\n\taces_size = acl_len - sizeof(struct smb_acl);\n\tif (pntsd_type & DACL_AUTO_INHERITED)\n\t\tinherited_flags = INHERITED_ACE;\n\tfor (i = 0; i < num_aces; i++) {\n\t\tint pace_size;\n\t\tif (offsetof(struct smb_ace, access_req) > aces_size)\n\t\t\tbreak;\n\t\tpace_size = le16_to_cpu(parent_aces->size);\n\t\tif (pace_size > aces_size)\n\t\t\tbreak;\n\t\taces_size -= pace_size;\n\t\tflags = parent_aces->flags;\n\t\tif (!smb_inherit_flags(flags, is_dir))\n\t\t\tgoto pass;\n\t\tif (is_dir) {\n\t\t\tflags &= ~(INHERIT_ONLY_ACE | INHERITED_ACE);\n\t\t\tif (!(flags & CONTAINER_INHERIT_ACE))\n\t\t\t\tflags |= INHERIT_ONLY_ACE;\n\t\t\tif (flags & NO_PROPAGATE_INHERIT_ACE)\n\t\t\t\tflags = 0;\n\t\t} else {\n\t\t\tflags = 0;\n\t\t}\n\t\tif (!compare_sids(&creator_owner, &parent_aces->sid)) {\n\t\t\tcreator = &creator_owner;\n\t\t\tid_to_sid(uid, SIDOWNER, &owner_sid);\n\t\t\tpsid = &owner_sid;\n\t\t} else if (!compare_sids(&creator_group, &parent_aces->sid)) {\n\t\t\tcreator = &creator_group;\n\t\t\tid_to_sid(gid, SIDUNIX_GROUP, &group_sid);\n\t\t\tpsid = &group_sid;\n\t\t} else {\n\t\t\tcreator = NULL;\n\t\t\tpsid = &parent_aces->sid;\n\t\t}\n\t\tif (is_dir && creator && flags & CONTAINER_INHERIT_ACE) {\n\t\t\tsmb_set_ace(aces, psid, parent_aces->type, inherited_flags,\n\t\t\t\t    parent_aces->access_req);\n\t\t\tnt_size += le16_to_cpu(aces->size);\n\t\t\tace_cnt++;\n\t\t\taces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\n\t\t\tflags |= INHERIT_ONLY_ACE;\n\t\t\tpsid = creator;\n\t\t} else if (is_dir && !(parent_aces->flags & NO_PROPAGATE_INHERIT_ACE)) {\n\t\t\tpsid = &parent_aces->sid;\n\t\t}\n\t\tsmb_set_ace(aces, psid, parent_aces->type, flags | inherited_flags,\n\t\t\t    parent_aces->access_req);\n\t\tnt_size += le16_to_cpu(aces->size);\n\t\taces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\n\t\tace_cnt++;\npass:\n\t\tparent_aces = (struct smb_ace *)((char *)parent_aces + pace_size);\n\t}\n\tif (nt_size > 0) {\n\t\tstruct smb_ntsd *pntsd;\n\t\tstruct smb_acl *pdacl;\n\t\tstruct smb_sid *powner_sid = NULL, *pgroup_sid = NULL;\n\t\tint powner_sid_size = 0, pgroup_sid_size = 0, pntsd_size;\n\t\tif (parent_pntsd->osidoffset) {\n\t\t\tpowner_sid = (struct smb_sid *)((char *)parent_pntsd +\n\t\t\t\t\tle32_to_cpu(parent_pntsd->osidoffset));\n\t\t\tpowner_sid_size = 1 + 1 + 6 + (powner_sid->num_subauth * 4);",
        "bug_line_number": [],
        "vul": 0,
        "id": 47
    },
    {
        "code": "static int smb2_calc_max_out_buf_len(struct ksmbd_work *work,\n\t\t\t\t     unsigned short hdr2_len,\n\t\t\t\t     unsigned int out_buf_len)\n{\n\tint free_len;\n\tif (out_buf_len > work->conn->vals->max_trans_size)\n\t\treturn -EINVAL;\n\tfree_len = smb2_resp_buf_len(work, hdr2_len);\n\tif (free_len < 0)\n\t\treturn -EINVAL;\n\treturn min_t(int, out_buf_len, free_len);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 49
    },
    {
        "code": "int v4l2_m2m_dqbuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\n\t\t   struct v4l2_buffer *buf)\n{\n\tstruct vb2_queue *vq;\n\tint ret;\n\tvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\n\tret = vb2_dqbuf(vq, buf, file->f_flags & O_NONBLOCK);\n\tif (ret)\n\t\treturn ret;\n\t/* Adjust MMAP memory offsets for the CAPTURE queue */\n\tv4l2_m2m_adjust_mem_offset(vq, buf);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 51
    },
    {
        "code": "static int _nfs4_get_security_label(struct inode *inode, void *buf,\n\t\t\t\t\tsize_t buflen)\n{\n\tstruct nfs_server *server = NFS_SERVER(inode);\n\tstruct nfs_fattr fattr;\n\tstruct nfs4_label label = {0, 0, buflen, buf};\n\tu32 bitmask[3] = { 0, 0, FATTR4_WORD2_SECURITY_LABEL };\n\tstruct nfs4_getattr_arg arg = {\n\t\t.fh\t\t= NFS_FH(inode),\n\t\t.bitmask\t= bitmask,\n\t};\n\tstruct nfs4_getattr_res res = {\n\t\t.fattr\t\t= &fattr,\n\t\t.label\t\t= &label,\n\t\t.server\t\t= server,\n\t};\n\tstruct rpc_message msg = {\n\t\t.rpc_proc\t= &nfs4_procedures[NFSPROC4_CLNT_GETATTR],\n\t\t.rpc_argp\t= &arg,\n\t\t.rpc_resp\t= &res,\n\t};\n\tint ret;\n\tnfs_fattr_init(&fattr);\n\tret = nfs4_call_sync(server->client, server, &msg, &arg.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\treturn ret;\n\tif (!(fattr.valid & NFS_ATTR_FATTR_V4_SECURITY_LABEL))\n\t\treturn -ENOENT;\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 53
    },
    {
        "code": "static int do_cpuid_func(struct kvm_cpuid_entry2 *entry, u32 func,\n\t\t\t int *nent, int maxnent, unsigned int type)\n{\n\tif (*nent >= maxnent)\n\t\treturn -E2BIG;\n\tif (type == KVM_GET_EMULATED_CPUID)\n\t\treturn __do_cpuid_func_emulated(entry, func, nent, maxnent);\n\treturn __do_cpuid_func(entry, func, nent, maxnent);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 55
    },
    {
        "code": "static int mon_bin_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\t/* don't do anything here: \"fault\" will set up page table entries */\n\tvma->vm_ops = &mon_bin_vm_ops;\n\tif (vma->vm_flags & VM_WRITE)\n\t\treturn -EPERM;\n\tvma->vm_flags &= ~VM_MAYWRITE;\n\tvma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;\n\tvma->vm_private_data = filp->private_data;\n\tmon_bin_vma_open(vma);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 57
    },
    {
        "code": "static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)\n{\n\tstruct io_buffer *buf;\n\tu64 addr = pbuf->addr;\n\tint i, bid = pbuf->bid;\n\tfor (i = 0; i < pbuf->nbufs; i++) {\n\t\tbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tbreak;\n\t\tbuf->addr = addr;\n\t\tbuf->len = min_t(__u32, pbuf->len, MAX_RW_COUNT);\n\t\tbuf->bid = bid;\n\t\taddr += pbuf->len;\n\t\tbid++;\n\t\tif (!*head) {\n\t\t\tINIT_LIST_HEAD(&buf->list);\n\t\t\t*head = buf;\n\t\t} else {\n\t\t\tlist_add_tail(&buf->list, &(*head)->list);\n\t\t}\n\t}\n\treturn i ? i : -ENOMEM;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 59
    },
    {
        "code": "static struct sk_buff *tun_napi_alloc_frags(struct tun_file *tfile,\n\t\t\t\t\t    size_t len,\n\t\t\t\t\t    const struct iov_iter *it)\n{\n\tstruct sk_buff *skb;\n\tsize_t linear;\n\tint err;\n\tint i;\n\tif (it->nr_segs > MAX_SKB_FRAGS + 1 ||\n\t    len > (ETH_MAX_MTU - NET_SKB_PAD - NET_IP_ALIGN))\n\t\treturn ERR_PTR(-EMSGSIZE);\n\tlocal_bh_disable();\n\tskb = napi_get_frags(&tfile->napi);\n\tlocal_bh_enable();\n\tif (!skb)\n\t\treturn ERR_PTR(-ENOMEM);\n\tlinear = iov_iter_single_seg_count(it);\n\terr = __skb_grow(skb, linear);\n\tif (err)\n\t\tgoto free;\n\tskb->len = len;\n\tskb->data_len = len - linear;\n\tskb->truesize += skb->data_len;\n\tfor (i = 1; i < it->nr_segs; i++) {\n\t\tsize_t fragsz = it->iov[i].iov_len;\n\t\tstruct page *page;\n\t\tvoid *frag;\n\t\tif (fragsz == 0 || fragsz > PAGE_SIZE) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\t\tfrag = netdev_alloc_frag(fragsz);\n\t\tif (!frag) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t\tpage = virt_to_head_page(frag);\n\t\tskb_fill_page_desc(skb, i - 1, page,\n\t\t\t\t   frag - page_address(page), fragsz);\n\t}\n\treturn skb;\nfree:\n\t/* frees skb and all frags allocated with napi_alloc_frag() */\n\tnapi_free_frags(&tfile->napi);\n\treturn ERR_PTR(err);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 61
    },
    {
        "code": "void xt_compat_target_from_user(struct xt_entry_target *t, void **dstptr,\n\t\t\t\tunsigned int *size)\n{\n\tconst struct xt_target *target = t->u.kernel.target;\n\tstruct compat_xt_entry_target *ct = (struct compat_xt_entry_target *)t;\n\tint off = xt_compat_target_offset(target);\n\tu_int16_t tsize = ct->u.user.target_size;\n\tchar name[sizeof(t->u.user.name)];\n\tt = *dstptr;\n\tmemcpy(t, ct, sizeof(*ct));\n\tif (target->compat_from_user)\n\t\ttarget->compat_from_user(t->data, ct->data);\n\telse\n\t\tmemcpy(t->data, ct->data, tsize - sizeof(*ct));\n\ttsize += off;\n\tt->u.user.target_size = tsize;\n\tstrlcpy(name, target->name, sizeof(name));\n\tmodule_put(target->me);\n\tstrncpy(t->u.user.name, name, sizeof(t->u.user.name));\n\t*size += off;\n\t*dstptr += tsize;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 63
    },
    {
        "code": "void xt_compat_match_from_user(struct xt_entry_match *m, void **dstptr,\n\t\t\t       unsigned int *size)\n{\n\tconst struct xt_match *match = m->u.kernel.match;\n\tstruct compat_xt_entry_match *cm = (struct compat_xt_entry_match *)m;\n\tint off = xt_compat_match_offset(match);\n\tu_int16_t msize = cm->u.user.match_size;\n\tchar name[sizeof(m->u.user.name)];\n\tm = *dstptr;\n\tmemcpy(m, cm, sizeof(*cm));\n\tif (match->compat_from_user)\n\t\tmatch->compat_from_user(m->data, cm->data);\n\telse\n\t\tmemcpy(m->data, cm->data, msize - sizeof(*cm));\n\tmsize += off;\n\tm->u.user.match_size = msize;\n\tstrlcpy(name, match->name, sizeof(name));\n\tmodule_put(match->me);\n\tstrncpy(m->u.user.name, name, sizeof(m->u.user.name));\n\t*size += off;\n\t*dstptr += msize;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 65
    },
    {
        "code": "static int\ntranslate_compat_table(struct net *net,\n\t\t       struct xt_table_info **pinfo,\n\t\t       void **pentry0,\n\t\t       const struct compat_ipt_replace *compatr)\n{\n\tunsigned int i, j;\n\tstruct xt_table_info *newinfo, *info;\n\tvoid *pos, *entry0, *entry1;\n\tstruct compat_ipt_entry *iter0;\n\tstruct ipt_replace repl;\n\tunsigned int size;\n\tint ret;\n\tinfo = *pinfo;\n\tentry0 = *pentry0;\n\tsize = compatr->size;\n\tinfo->number = compatr->num_entries;\n\tj = 0;\n\txt_compat_lock(AF_INET);\n\tret = xt_compat_init_offsets(AF_INET, compatr->num_entries);\n\tif (ret)\n\t\tgoto out_unlock;\n\t/* Walk through entries, checking offsets. */\n\txt_entry_foreach(iter0, entry0, compatr->size) {\n\t\tret = check_compat_entry_size_and_hooks(iter0, info, &size,\n\t\t\t\t\t\t\tentry0,\n\t\t\t\t\t\t\tentry0 + compatr->size);\n\t\tif (ret != 0)\n\t\t\tgoto out_unlock;\n\t\t++j;\n\t}\n\tret = -EINVAL;\n\tif (j != compatr->num_entries)\n\t\tgoto out_unlock;\n\tret = -ENOMEM;\n\tnewinfo = xt_alloc_table_info(size);\n\tif (!newinfo)\n\t\tgoto out_unlock;\n\tmemset(newinfo->entries, 0, size);\n\tnewinfo->number = compatr->num_entries;\n\tfor (i = 0; i < NF_INET_NUMHOOKS; i++) {\n\t\tnewinfo->hook_entry[i] = compatr->hook_entry[i];\n\t\tnewinfo->underflow[i] = compatr->underflow[i];\n\t}\n\tentry1 = newinfo->entries;\n\tpos = entry1;\n\tsize = compatr->size;\n\txt_entry_foreach(iter0, entry0, compatr->size)\n\t\tcompat_copy_entry_from_user(iter0, &pos, &size,\n\t\t\t\t\t    newinfo, entry1);\n\t/* all module references in entry0 are now gone.\n\t * entry1/newinfo contains a 64bit ruleset that looks exactly as\n\t * generated by 64bit userspace.\n\t *\n\t * Call standard translate_table() to validate all hook_entrys,\n\t * underflows, check for loops, etc.\n\t */\n\txt_compat_flush_offsets(AF_INET);\n\txt_compat_unlock(AF_INET);\n\tmemcpy(&repl, compatr, sizeof(*compatr));\n\tfor (i = 0; i < NF_INET_NUMHOOKS; i++) {\n\t\trepl.hook_entry[i] = newinfo->hook_entry[i];\n\t\trepl.underflow[i] = newinfo->underflow[i];\n\t}\n\trepl.num_counters = 0;\n\trepl.counters = NULL;\n\trepl.size = newinfo->size;\n\tret = translate_table(net, newinfo, entry1, &repl);\n\tif (ret)\n\t\tgoto free_newinfo;\n\t*pinfo = newinfo;\n\t*pentry0 = entry1;\n\txt_free_table_info(info);\n\treturn 0;\nfree_newinfo:\n\txt_free_table_info(newinfo);\n\treturn ret;\nout_unlock:\n\txt_compat_flush_offsets(AF_INET);\n\txt_compat_unlock(AF_INET);\n\txt_entry_foreach(iter0, entry0, compatr->size) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tcompat_release_entry(iter0);\n\t}\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 67
    },
    {
        "code": "static inline int ext4_valid_inum(struct super_block *sb, unsigned long ino)\n{\n\treturn ino == EXT4_ROOT_INO ||\n\t\t(ino >= EXT4_FIRST_INO(sb) &&\n\t\t ino <= le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count));\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 69
    },
    {
        "code": "static void handle_rx(struct vhost_net *net)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tunsigned uninitialized_var(in), log;\n\tstruct vhost_log *vq_log;\n\tstruct msghdr msg = {\n\t\t.msg_name = NULL,\n\t\t.msg_namelen = 0,\n\t\t.msg_control = NULL, /* FIXME: get and handle RX aux data. */\n\t\t.msg_controllen = 0,\n\t\t.msg_iov = vq->iov,\n\t\t.msg_flags = MSG_DONTWAIT,\n\t};\n\tstruct virtio_net_hdr_mrg_rxbuf hdr = {\n\t\t.hdr.flags = 0,\n\t\t.hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE\n\t};\n\tsize_t total_len = 0;\n\tint err, mergeable;\n\ts16 headcount;\n\tsize_t vhost_hlen, sock_hlen;\n\tsize_t vhost_len, sock_len;\n\tstruct socket *sock;\n\tmutex_lock(&vq->mutex);\n\tsock = vq->private_data;\n\tif (!sock)\n\t\tgoto out;\n\tvhost_disable_notify(&net->dev, vq);\n\tvhost_hlen = nvq->vhost_hlen;\n\tsock_hlen = nvq->sock_hlen;\n\tvq_log = unlikely(vhost_has_feature(&net->dev, VHOST_F_LOG_ALL)) ?\n\t\tvq->log : NULL;\n\tmergeable = vhost_has_feature(&net->dev, VIRTIO_NET_F_MRG_RXBUF);\n\twhile ((sock_len = peek_head_len(sock->sk))) {\n\t\tsock_len += sock_hlen;\n\t\tvhost_len = sock_len + vhost_hlen;\n\t\theadcount = get_rx_bufs(vq, vq->heads, vhost_len,\n\t\t\t\t\t&in, vq_log, &log,\n\t\t\t\t\tlikely(mergeable) ? UIO_MAXIOV : 1);\n\t\t/* On error, stop handling until the next kick. */\n\t\tif (unlikely(headcount < 0))\n\t\t\tbreak;\n\t\t/* On overrun, truncate and discard */\n\t\tif (unlikely(headcount > UIO_MAXIOV)) {\n\t\t\tmsg.msg_iovlen = 1;\n\t\t\terr = sock->ops->recvmsg(NULL, sock, &msg,\n\t\t\t\t\t\t 1, MSG_DONTWAIT | MSG_TRUNC);\n\t\t\tpr_debug(\"Discarded rx packet: len %zd\\n\", sock_len);\n\t\t\tcontinue;\n\t\t}\n\t\t/* OK, now we need to know about added descriptors. */\n\t\tif (!headcount) {\n\t\t\tif (unlikely(vhost_enable_notify(&net->dev, vq))) {\n\t\t\t\t/* They have slipped one in as we were\n\t\t\t\t * doing that: check again. */\n\t\t\t\tvhost_disable_notify(&net->dev, vq);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/* Nothing new?  Wait for eventfd to tell us\n\t\t\t * they refilled. */\n\t\t\tbreak;\n\t\t}\n\t\t/* We don't need to be notified again. */\n\t\tif (unlikely((vhost_hlen)))\n\t\t\t/* Skip header. TODO: support TSO. */\n\t\t\tmove_iovec_hdr(vq->iov, nvq->hdr, vhost_hlen, in);\n\t\telse\n\t\t\t/* Copy the header for use in VIRTIO_NET_F_MRG_RXBUF:\n\t\t\t * needed because recvmsg can modify msg_iov. */\n\t\t\tcopy_iovec_hdr(vq->iov, nvq->hdr, sock_hlen, in);\n\t\tmsg.msg_iovlen = in;\n\t\terr = sock->ops->recvmsg(NULL, sock, &msg,\n\t\t\t\t\t sock_len, MSG_DONTWAIT | MSG_TRUNC);\n\t\t/* Userspace might have consumed the packet meanwhile:\n\t\t * it's not supposed to do this usually, but might be hard\n\t\t * to prevent. Discard data we got (if any) and keep going. */\n\t\tif (unlikely(err != sock_len)) {\n\t\t\tpr_debug(\"Discarded rx packet: \"\n\t\t\t\t \" len %d, expected %zd\\n\", err, sock_len);\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(vhost_hlen) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&hdr, 0,\n\t\t\t\t      vhost_hlen)) {\n\t\t\tvq_err(vq, \"Unable to write vnet_hdr at addr %p\\n\",\n\t\t\t       vq->iov->iov_base);\n\t\t\tbreak;\n\t\t}\n\t\t/* TODO: Should check and handle checksum. */\n\t\tif (likely(mergeable) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&headcount,\n\t\t\t\t      offsetof(typeof(hdr), num_buffers),\n\t\t\t\t      sizeof hdr.num_buffers)) {\n\t\t\tvq_err(vq, \"Failed num_buffers write\");\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tbreak;\n\t\t}\n\t\tvhost_add_used_and_signal_n(&net->dev, vq, vq->heads,\n\t\t\t\t\t    headcount);\n\t\tif (unlikely(vq_log))\n\t\t\tvhost_log_write(vq, vq_log, log, vhost_len);\n\t\ttotal_len += vhost_len;\n\t\tif (unlikely(total_len >= VHOST_NET_WEIGHT)) {\n\t\t\tvhost_poll_queue(&vq->poll);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&vq->mutex);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 71
    },
    {
        "code": "int nf_ct_frag6_gather(struct net *net, struct sk_buff *skb, u32 user)\n{\n\tstruct net_device *dev = skb->dev;\n\tint fhoff, nhoff, ret;\n\tstruct frag_hdr *fhdr;\n\tstruct frag_queue *fq;\n\tstruct ipv6hdr *hdr;\n\tu8 prevhdr;\n\t/* Jumbo payload inhibits frag. header */\n\tif (ipv6_hdr(skb)->payload_len == 0) {\n\t\tpr_debug(\"payload len = 0\\n\");\n\t\treturn 0;\n\t}\n\tif (find_prev_fhdr(skb, &prevhdr, &nhoff, &fhoff) < 0)\n\t\treturn 0;\n\tif (!pskb_may_pull(skb, fhoff + sizeof(*fhdr)))\n\t\treturn -ENOMEM;\n\tskb_set_transport_header(skb, fhoff);\n\thdr = ipv6_hdr(skb);\n\tfhdr = (struct frag_hdr *)skb_transport_header(skb);\n\tfq = fq_find(net, fhdr->identification, user, &hdr->saddr, &hdr->daddr,\n\t\t     skb->dev ? skb->dev->ifindex : 0, ip6_frag_ecn(hdr));\n\tif (fq == NULL) {\n\t\tpr_debug(\"Can't find and can't create new queue\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tspin_lock_bh(&fq->q.lock);\n\tif (nf_ct_frag6_queue(fq, skb, fhdr, nhoff) < 0) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\t/* after queue has assumed skb ownership, only 0 or -EINPROGRESS\n\t * must be returned.\n\t */\n\tret = -EINPROGRESS;\n\tif (fq->q.flags == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\n\t    fq->q.meat == fq->q.len &&\n\t    nf_ct_frag6_reasm(fq, skb, dev))\n\t\tret = 0;\nout_unlock:\n\tspin_unlock_bh(&fq->q.lock);\n\tinet_frag_put(&fq->q, &nf_frags);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 73
    },
    {
        "code": "static int usb_parse_configuration(struct usb_device *dev, int cfgidx,\n    struct usb_host_config *config, unsigned char *buffer, int size)\n{\n\tstruct device *ddev = &dev->dev;\n\tunsigned char *buffer0 = buffer;\n\tint cfgno;\n\tint nintf, nintf_orig;\n\tint i, j, n;\n\tstruct usb_interface_cache *intfc;\n\tunsigned char *buffer2;\n\tint size2;\n\tstruct usb_descriptor_header *header;\n\tint len, retval;\n\tu8 inums[USB_MAXINTERFACES], nalts[USB_MAXINTERFACES];\n\tunsigned iad_num = 0;\n\tmemcpy(&config->desc, buffer, USB_DT_CONFIG_SIZE);\n\tnintf = nintf_orig = config->desc.bNumInterfaces;\n\tconfig->desc.bNumInterfaces = 0;\t// Adjusted later\n\tif (config->desc.bDescriptorType != USB_DT_CONFIG ||\n\t    config->desc.bLength < USB_DT_CONFIG_SIZE ||\n\t    config->desc.bLength > size) {\n\t\tdev_err(ddev, \"invalid descriptor for config index %d: \"\n\t\t    \"type = 0x%X, length = %d\\n\", cfgidx,\n\t\t    config->desc.bDescriptorType, config->desc.bLength);\n\t\treturn -EINVAL;\n\t}\n\tcfgno = config->desc.bConfigurationValue;\n\tbuffer += config->desc.bLength;\n\tsize -= config->desc.bLength;\n\tif (nintf > USB_MAXINTERFACES) {\n\t\tdev_warn(ddev, \"config %d has too many interfaces: %d, \"\n\t\t    \"using maximum allowed: %d\\n\",\n\t\t    cfgno, nintf, USB_MAXINTERFACES);\n\t\tnintf = USB_MAXINTERFACES;\n\t}\n\t/* Go through the descriptors, checking their length and counting the\n\t * number of altsettings for each interface */\n\tn = 0;\n\tfor ((buffer2 = buffer, size2 = size);\n\t      size2 > 0;\n\t     (buffer2 += header->bLength, size2 -= header->bLength)) {\n\t\tif (size2 < sizeof(struct usb_descriptor_header)) {\n\t\t\tdev_warn(ddev, \"config %d descriptor has %d excess \"\n\t\t\t    \"byte%s, ignoring\\n\",\n\t\t\t    cfgno, size2, plural(size2));\n\t\t\tbreak;\n\t\t}\n\t\theader = (struct usb_descriptor_header *) buffer2;\n\t\tif ((header->bLength > size2) || (header->bLength < 2)) {\n\t\t\tdev_warn(ddev, \"config %d has an invalid descriptor \"\n\t\t\t    \"of length %d, skipping remainder of the config\\n\",\n\t\t\t    cfgno, header->bLength);\n\t\t\tbreak;\n\t\t}\n\t\tif (header->bDescriptorType == USB_DT_INTERFACE) {\n\t\t\tstruct usb_interface_descriptor *d;\n\t\t\tint inum;\n\t\t\td = (struct usb_interface_descriptor *) header;\n\t\t\tif (d->bLength < USB_DT_INTERFACE_SIZE) {\n\t\t\t\tdev_warn(ddev, \"config %d has an invalid \"\n\t\t\t\t    \"interface descriptor of length %d, \"\n\t\t\t\t    \"skipping\\n\", cfgno, d->bLength);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tinum = d->bInterfaceNumber;\n\t\t\tif ((dev->quirks & USB_QUIRK_HONOR_BNUMINTERFACES) &&\n\t\t\t    n >= nintf_orig) {\n\t\t\t\tdev_warn(ddev, \"config %d has more interface \"\n\t\t\t\t    \"descriptors, than it declares in \"\n\t\t\t\t    \"bNumInterfaces, ignoring interface \"\n\t\t\t\t    \"number: %d\\n\", cfgno, inum);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (inum >= nintf_orig)\n\t\t\t\tdev_warn(ddev, \"config %d has an invalid \"\n\t\t\t\t    \"interface number: %d but max is %d\\n\",\n\t\t\t\t    cfgno, inum, nintf_orig - 1);\n\t\t\t/* Have we already encountered this interface?\n\t\t\t * Count its altsettings */\n\t\t\tfor (i = 0; i < n; ++i) {\n\t\t\t\tif (inums[i] == inum)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (i < n) {\n\t\t\t\tif (nalts[i] < 255)\n\t\t\t\t\t++nalts[i];\n\t\t\t} else if (n < USB_MAXINTERFACES) {\n\t\t\t\tinums[n] = inum;\n\t\t\t\tnalts[n] = 1;\n\t\t\t\t++n;\n\t\t\t}\n\t\t} else if (header->bDescriptorType ==\n\t\t\t\tUSB_DT_INTERFACE_ASSOCIATION) {\n\t\t\tstruct usb_interface_assoc_descriptor *d;\n\t\t\td = (struct usb_interface_assoc_descriptor *)header;\n\t\t\tif (d->bLength < USB_DT_INTERFACE_ASSOCIATION_SIZE) {\n\t\t\t\tdev_warn(ddev,\n\t\t\t\t\t \"config %d has an invalid interface association descriptor of length %d, skipping\\n\",\n\t\t\t\t\t cfgno, d->bLength);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (iad_num == USB_MAXIADS) {\n\t\t\t\tdev_warn(ddev, \"found more Interface \"\n\t\t\t\t\t       \"Association Descriptors \"",
        "bug_line_number": [],
        "vul": 0,
        "id": 75
    },
    {
        "code": "int jbd2_journal_dirty_metadata(handle_t *handle, struct buffer_head *bh)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal;\n\tstruct journal_head *jh;\n\tint ret = 0;\n\tif (is_handle_aborted(handle))\n\t\treturn -EROFS;\n\tif (!buffer_jbd(bh)) {\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\t/*\n\t * We don't grab jh reference here since the buffer must be part\n\t * of the running transaction.\n\t */\n\tjh = bh2jh(bh);\n\t/*\n\t * This and the following assertions are unreliable since we may see jh\n\t * in inconsistent state unless we grab bh_state lock. But this is\n\t * crucial to catch bugs so let's do a reliable check until the\n\t * lockless handling is fully proven.\n\t */\n\tif (jh->b_transaction != transaction &&\n\t    jh->b_next_transaction != transaction) {\n\t\tjbd_lock_bh_state(bh);\n\t\tJ_ASSERT_JH(jh, jh->b_transaction == transaction ||\n\t\t\t\tjh->b_next_transaction == transaction);\n\t\tjbd_unlock_bh_state(bh);\n\t}\n\tif (jh->b_modified == 1) {\n\t\t/* If it's in our transaction it must be in BJ_Metadata list. */\n\t\tif (jh->b_transaction == transaction &&\n\t\t    jh->b_jlist != BJ_Metadata) {\n\t\t\tjbd_lock_bh_state(bh);\n\t\t\tif (jh->b_transaction == transaction &&\n\t\t\t    jh->b_jlist != BJ_Metadata)\n\t\t\t\tpr_err(\"JBD2: assertion failure: h_type=%u \"\n\t\t\t\t       \"h_line_no=%u block_no=%llu jlist=%u\\n\",\n\t\t\t\t       handle->h_type, handle->h_line_no,\n\t\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t\t       jh->b_jlist);\n\t\t\tJ_ASSERT_JH(jh, jh->b_transaction != transaction ||\n\t\t\t\t\tjh->b_jlist == BJ_Metadata);\n\t\t\tjbd_unlock_bh_state(bh);\n\t\t}\n\t\tgoto out;\n\t}\n\tjournal = transaction->t_journal;\n\tjbd_debug(5, \"journal_head %p\\n\", jh);\n\tJBUFFER_TRACE(jh, \"entry\");\n\tjbd_lock_bh_state(bh);\n\tif (jh->b_modified == 0) {\n\t\t/*\n\t\t * This buffer's got modified and becoming part\n\t\t * of the transaction. This needs to be done\n\t\t * once a transaction -bzzz\n\t\t */\n\t\tif (handle->h_buffer_credits <= 0) {\n\t\t\tret = -ENOSPC;\n\t\t\tgoto out_unlock_bh;\n\t\t}\n\t\tjh->b_modified = 1;\n\t\thandle->h_buffer_credits--;\n\t}\n\t/*\n\t * fastpath, to avoid expensive locking.  If this buffer is already\n\t * on the running transaction's metadata list there is nothing to do.\n\t * Nobody can take it off again because there is a handle open.\n\t * I _think_ we're OK here with SMP barriers - a mistaken decision will\n\t * result in this test being false, so we go in and take the locks.\n\t */\n\tif (jh->b_transaction == transaction && jh->b_jlist == BJ_Metadata) {\n\t\tJBUFFER_TRACE(jh, \"fastpath\");\n\t\tif (unlikely(jh->b_transaction !=\n\t\t\t     journal->j_running_transaction)) {\n\t\t\tprintk(KERN_ERR \"JBD2: %s: \"\n\t\t\t       \"jh->b_transaction (%llu, %p, %u) != \"\n\t\t\t       \"journal->j_running_transaction (%p, %u)\\n\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ? jh->b_transaction->t_tid : 0,\n\t\t\t       journal->j_running_transaction,\n\t\t\t       journal->j_running_transaction ?\n\t\t\t       journal->j_running_transaction->t_tid : 0);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tgoto out_unlock_bh;\n\t}\n\tset_buffer_jbddirty(bh);\n\t/*\n\t * Metadata already on the current transaction list doesn't\n\t * need to be filed.  Metadata on another transaction's list must\n\t * be committing, and will be refiled once the commit completes:\n\t * leave it alone for now.\n\t */\n\tif (jh->b_transaction != transaction) {\n\t\tJBUFFER_TRACE(jh, \"already on other transaction\");\n\t\tif (unlikely(((jh->b_transaction !=\n\t\t\t       journal->j_committing_transaction)) ||\n\t\t\t     (jh->b_next_transaction != transaction))) {\n\t\t\tprintk(KERN_ERR \"jbd2_journal_dirty_metadata: %s: \"\n\t\t\t       \"bad jh for block %llu: \"\n\t\t\t       \"transaction (%p, %u), \"\n\t\t\t       \"jh->b_transaction (%p, %u), \"\n\t\t\t       \"jh->b_next_transaction (%p, %u), jlist %u\\n\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       transaction, transaction->t_tid,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ?\n\t\t\t       jh->b_transaction->t_tid : 0,",
        "bug_line_number": [],
        "vul": 0,
        "id": 77
    },
    {
        "code": "static int joydev_handle_JSIOCSAXMAP(struct joydev *joydev,\n\t\t\t\t     void __user *argp, size_t len)\n{\n\t__u8 *abspam;\n\tint i;\n\tint retval = 0;\n\tlen = min(len, sizeof(joydev->abspam));\n\t/* Validate the map. */\n\tabspam = memdup_user(argp, len);\n\tif (IS_ERR(abspam))\n\t\treturn PTR_ERR(abspam);\n\tfor (i = 0; i < len && i < joydev->nabs; i++) {\n\t\tif (abspam[i] > ABS_MAX) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tmemcpy(joydev->abspam, abspam, len);\n\tfor (i = 0; i < joydev->nabs; i++)\n\t\tjoydev->absmap[joydev->abspam[i]] = i;\n out:\n\tkfree(abspam);\n\treturn retval;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 79
    },
    {
        "code": "int kvm_sev_es_string_io(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t unsigned int port, void *data,  unsigned int count,\n\t\t\t int in)\n{\n\tvcpu->arch.sev_pio_data = data;\n\tvcpu->arch.sev_pio_count = count;\n\treturn in ? kvm_sev_es_ins(vcpu, size, port)\n\t\t  : kvm_sev_es_outs(vcpu, size, port);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 81
    },
    {
        "code": "static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunion vmx_exit_reason exit_reason = vmx->exit_reason;\n\tu32 vectoring_info = vmx->idt_vectoring_info;\n\tu16 exit_handler_index;\n\t/*\n\t * Flush logged GPAs PML buffer, this will make dirty_bitmap more\n\t * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before\n\t * querying dirty_bitmap, we only need to kick all vcpus out of guest\n\t * mode as if vcpus is in root mode, the PML buffer must has been\n\t * flushed already.  Note, PML is never enabled in hardware while\n\t * running L2.\n\t */\n\tif (enable_pml && !is_guest_mode(vcpu))\n\t\tvmx_flush_pml_buffer(vcpu);\n\t/*\n\t * We should never reach this point with a pending nested VM-Enter, and\n\t * more specifically emulation of L2 due to invalid guest state (see\n\t * below) should never happen as that means we incorrectly allowed a\n\t * nested VM-Enter with an invalid vmcs12.\n\t */\n\tWARN_ON_ONCE(vmx->nested.nested_run_pending);\n\t/* If guest state is invalid, start emulating */\n\tif (vmx->emulation_required)\n\t\treturn handle_invalid_guest_state(vcpu);\n\tif (is_guest_mode(vcpu)) {\n\t\t/*\n\t\t * PML is never enabled when running L2, bail immediately if a\n\t\t * PML full exit occurs as something is horribly wrong.\n\t\t */\n\t\tif (exit_reason.basic == EXIT_REASON_PML_FULL)\n\t\t\tgoto unexpected_vmexit;\n\t\t/*\n\t\t * The host physical addresses of some pages of guest memory\n\t\t * are loaded into the vmcs02 (e.g. vmcs12's Virtual APIC\n\t\t * Page). The CPU may write to these pages via their host\n\t\t * physical address while L2 is running, bypassing any\n\t\t * address-translation-based dirty tracking (e.g. EPT write\n\t\t * protection).\n\t\t *\n\t\t * Mark them dirty on every exit from L2 to prevent them from\n\t\t * getting out of sync with dirty tracking.\n\t\t */\n\t\tnested_mark_vmcs12_pages_dirty(vcpu);\n\t\tif (nested_vmx_reflect_vmexit(vcpu))\n\t\t\treturn 1;\n\t}\n\tif (exit_reason.failed_vmentry) {\n\t\tdump_vmcs();\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= exit_reason.full;\n\t\tvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\tif (unlikely(vmx->fail)) {\n\t\tdump_vmcs();\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= vmcs_read32(VM_INSTRUCTION_ERROR);\n\t\tvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\t/*\n\t * Note:\n\t * Do not try to fix EXIT_REASON_EPT_MISCONFIG if it caused by\n\t * delivery event since it indicates guest is accessing MMIO.\n\t * The vm-exit can be triggered again after return to guest that\n\t * will cause infinite loop.\n\t */\n\tif ((vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t    (exit_reason.basic != EXIT_REASON_EXCEPTION_NMI &&\n\t     exit_reason.basic != EXIT_REASON_EPT_VIOLATION &&\n\t     exit_reason.basic != EXIT_REASON_PML_FULL &&\n\t     exit_reason.basic != EXIT_REASON_APIC_ACCESS &&\n\t     exit_reason.basic != EXIT_REASON_TASK_SWITCH)) {\n\t\tint ndata = 3;\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;\n\t\tvcpu->run->internal.data[0] = vectoring_info;\n\t\tvcpu->run->internal.data[1] = exit_reason.full;\n\t\tvcpu->run->internal.data[2] = vcpu->arch.exit_qualification;\n\t\tif (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG) {\n\t\t\tvcpu->run->internal.data[ndata++] =\n\t\t\t\tvmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\t\t}\n\t\tvcpu->run->internal.data[ndata++] = vcpu->arch.last_vmentry_cpu;\n\t\tvcpu->run->internal.ndata = ndata;\n\t\treturn 0;\n\t}\n\tif (unlikely(!enable_vnmi &&\n\t\t     vmx->loaded_vmcs->soft_vnmi_blocked)) {\n\t\tif (!vmx_interrupt_blocked(vcpu)) {\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&\n\t\t\t   vcpu->arch.nmi_pending) {\n\t\t\t/*\n\t\t\t * This CPU don't support us in finding the end of an\n\t\t\t * NMI-blocked window if the guest runs with IRQs\n\t\t\t * disabled. So we pull the trigger after 1 s of\n\t\t\t * futile waiting, but inform the user about this.\n\t\t\t */\n\t\t\tprintk(KERN_WARNING \"%s: Breaking out of NMI-blocked \"\n\t\t\t       \"state on VCPU %d after 1 s timeout\\n\",\n\t\t\t       __func__, vcpu->vcpu_id);\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t}\n\t}",
        "bug_line_number": [],
        "vul": 0,
        "id": 83
    },
    {
        "code": "int ext4_setup_system_zone(struct super_block *sb)\n{\n\text4_group_t ngroups = ext4_get_groups_count(sb);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp;\n\text4_group_t i;\n\tint flex_size = ext4_flex_bg_size(sbi);\n\tint ret;\n\tif (!test_opt(sb, BLOCK_VALIDITY)) {\n\t\tif (sbi->system_blks.rb_node)\n\t\t\text4_release_system_zone(sb);\n\t\treturn 0;\n\t}\n\tif (sbi->system_blks.rb_node)\n\t\treturn 0;\n\tfor (i=0; i < ngroups; i++) {\n\t\tif (ext4_bg_has_super(sb, i) &&\n\t\t    ((i < 5) || ((i % flex_size) == 0)))\n\t\t\tadd_system_zone(sbi, ext4_group_first_block_no(sb, i),\n\t\t\t\t\text4_bg_num_gdb(sb, i) + 1);\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\t\tret = add_system_zone(sbi, ext4_block_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_table(sb, gdp),\n\t\t\t\tsbi->s_itb_per_group);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tif (ext4_has_feature_journal(sb) && sbi->s_es->s_journal_inum) {\n\t\tret = ext4_protect_reserved_inode(sb,\n\t\t\t\tle32_to_cpu(sbi->s_es->s_journal_inum));\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tif (test_opt(sb, DEBUG))\n\t\tdebug_print_tree(sbi);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 85
    },
    {
        "code": "int kvmppc_rtas_hcall(struct kvm_vcpu *vcpu)\n{\n\tstruct rtas_token_definition *d;\n\tstruct rtas_args args;\n\trtas_arg_t *orig_rets;\n\tgpa_t args_phys;\n\tint rc;\n\t/*\n\t * r4 contains the guest physical address of the RTAS args\n\t * Mask off the top 4 bits since this is a guest real address\n\t */\n\targs_phys = kvmppc_get_gpr(vcpu, 4) & KVM_PAM;\n\tvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\n\trc = kvm_read_guest(vcpu->kvm, args_phys, &args, sizeof(args));\n\tsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);\n\tif (rc)\n\t\tgoto fail;\n\t/*\n\t * args->rets is a pointer into args->args. Now that we've\n\t * copied args we need to fix it up to point into our copy,\n\t * not the guest args. We also need to save the original\n\t * value so we can restore it on the way out.\n\t */\n\torig_rets = args.rets;\n\tif (be32_to_cpu(args.nargs) >= ARRAY_SIZE(args.args)) {\n\t\t/*\n\t\t * Don't overflow our args array: ensure there is room for\n\t\t * at least rets[0] (even if the call specifies 0 nret).\n\t\t *\n\t\t * Each handler must then check for the correct nargs and nret\n\t\t * values, but they may always return failure in rets[0].\n\t\t */\n\t\trc = -EINVAL;\n\t\tgoto fail;\n\t}\n\targs.rets = &args.args[be32_to_cpu(args.nargs)];\n\tmutex_lock(&vcpu->kvm->arch.rtas_token_lock);\n\trc = -ENOENT;\n\tlist_for_each_entry(d, &vcpu->kvm->arch.rtas_tokens, list) {\n\t\tif (d->token == be32_to_cpu(args.token)) {\n\t\t\td->handler->handler(vcpu, &args);\n\t\t\trc = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&vcpu->kvm->arch.rtas_token_lock);\n\tif (rc == 0) {\n\t\targs.rets = orig_rets;\n\t\trc = kvm_write_guest(vcpu->kvm, args_phys, &args, sizeof(args));\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\treturn rc;\nfail:\n\t/*\n\t * We only get here if the guest has called RTAS with a bogus\n\t * args pointer or nargs/nret values that would overflow the\n\t * array. That means we can't get to the args, and so we can't\n\t * fail the RTAS call. So fail right out to userspace, which\n\t * should kill the guest.\n\t *\n\t * SLOF should actually pass the hcall return value from the\n\t * rtas handler call in r3, so enter_rtas could be modified to\n\t * return a failure indication in r3 and we could return such\n\t * errors to the guest rather than failing to host userspace.\n\t * However old guests that don't test for failure could then\n\t * continue silently after errors, so for now we won't do this.\n\t */\n\treturn rc;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 87
    },
    {
        "code": "static void\nbinder_free_buf(struct binder_proc *proc,\n\t\tstruct binder_thread *thread,\n\t\tstruct binder_buffer *buffer, bool is_failure)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_release_entire_buffer(proc, thread, buffer, is_failure);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 89
    },
    {
        "code": "static ssize_t smtcfb_read(struct fb_info *info, char __user *buf,\n\t\t\t   size_t count, loff_t *ppos)\n{\n\tunsigned long p = *ppos;\n\tu32 *buffer, *dst;\n\tu32 __iomem *src;\n\tint c, i, cnt = 0, err = 0;\n\tunsigned long total_size;\n\tif (!info || !info->screen_base)\n\t\treturn -ENODEV;\n\tif (info->state != FBINFO_STATE_RUNNING)\n\t\treturn -EPERM;\n\ttotal_size = info->screen_size;\n\tif (total_size == 0)\n\t\ttotal_size = info->fix.smem_len;\n\tif (p >= total_size)\n\t\treturn 0;\n\tif (count >= total_size)\n\t\tcount = total_size;\n\tif (count + p > total_size)\n\t\tcount = total_size - p;\n\tbuffer = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!buffer)\n\t\treturn -ENOMEM;\n\tsrc = (u32 __iomem *)(info->screen_base + p);\n\tif (info->fbops->fb_sync)\n\t\tinfo->fbops->fb_sync(info);\n\twhile (count) {\n\t\tc = (count > PAGE_SIZE) ? PAGE_SIZE : count;\n\t\tdst = buffer;\n\t\tfor (i = (c + 3) >> 2; i--;) {\n\t\t\tu32 val;\n\t\t\tval = fb_readl(src);\n\t\t\t*dst = big_swap(val);\n\t\t\tsrc++;\n\t\t\tdst++;\n\t\t}\n\t\tif (copy_to_user(buf, buffer, c)) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\t*ppos += c;\n\t\tbuf += c;\n\t\tcnt += c;\n\t\tcount -= c;\n\t}\n\tkfree(buffer);\n\treturn (err) ? err : cnt;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 91
    },
    {
        "code": "\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\t\tsecurity_task_getsecid(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\ttrace_binder_transaction(reply, t, target_node);\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t    t->buffer, buf_offset,\n\t\t\t\t\t    secctx, secctx_sz);\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;",
        "bug_line_number": [],
        "vul": 0,
        "id": 93
    },
    {
        "code": "\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 93
    }
]