[
    {
        "code": "static void\nnvkm_vmm_put_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *prev, *next;\n\tif ((prev = node(vma, prev)) && !prev->used) {\n\t\trb_erase(&prev->tree, &vmm->free);\n\t\tlist_del(&prev->head);\n\t\tvma->addr  = prev->addr;\n\t\tvma->size += prev->size;\n\t\tkfree(prev);\n\t}\n\tif ((next = node(vma, next)) && !next->used) {\n\t\trb_erase(&next->tree, &vmm->free);\n\t\tlist_del(&next->head);\n\t\tvma->size += next->size;\n\t\tkfree(next);\n\t}\n\tnvkm_vmm_free_insert(vmm, vma);\n}",
        "bug_line_number": [
            5,
            6,
            9,
            10,
            12,
            13,
            15,
            16
        ],
        "vul": 1,
        "id": 0
    },
    {
        "code": "static int get_entries(struct net *net, struct arpt_get_entries __user *uptr,\n\t\t       const int *len)\n{\n\tint ret;\n\tstruct arpt_get_entries get;\n\tstruct xt_table *t;\n\tif (*len < sizeof(get))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&get, uptr, sizeof(get)) != 0)\n\t\treturn -EFAULT;\n\tif (*len != sizeof(struct arpt_get_entries) + get.size)\n\t\treturn -EINVAL;\n\tget.name[sizeof(get.name) - 1] = '\\0';\n\tt = xt_find_table_lock(net, NFPROTO_ARP, get.name);\n\tif (!IS_ERR(t)) {\n\t\tconst struct xt_table_info *private = t->private;\n\t\tif (get.size == private->size)\n\t\t\tret = copy_entries_to_user(private->size,\n\t\t\t\t\t\t   t, uptr->entrytable);\n\t\telse\n\t\t\tret = -EAGAIN;\n\t\tmodule_put(t->me);\n\t\txt_table_unlock(t);\n\t} else\n\t\tret = PTR_ERR(t);\n\treturn ret;\n}",
        "bug_line_number": [
            15,
            16
        ],
        "vul": 1,
        "id": 2
    },
    {
        "code": "static struct xt_counters *alloc_counters(const struct xt_table *table)\n{\n\tunsigned int countersize;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\t/* We need atomic snapshot of counters: rest doesn't change\n\t * (other than comefrom, which userspace doesn't care\n\t * about).\n\t */\n\tcountersize = sizeof(struct xt_counters) * private->number;\n\tcounters = vzalloc(countersize);\n\tif (counters == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\tget_counters(private, counters);\n\treturn counters;\n}",
        "bug_line_number": [
            4,
            5
        ],
        "vul": 1,
        "id": 4
    },
    {
        "code": "static struct xt_counters *alloc_counters(const struct xt_table *table)\n{\n\tunsigned int countersize;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\t/* We need atomic snapshot of counters: rest doesn't change\n\t   (other than comefrom, which userspace doesn't care\n\t   about). */\n\tcountersize = sizeof(struct xt_counters) * private->number;\n\tcounters = vzalloc(countersize);\n\tif (counters == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\tget_counters(private, counters);\n\treturn counters;\n}",
        "bug_line_number": [
            4,
            5
        ],
        "vul": 1,
        "id": 6
    },
    {
        "code": "void snd_pcm_detach_substream(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\tif (runtime->private_free != NULL)\n\t\truntime->private_free(runtime);\n\tfree_pages_exact(runtime->status,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\tfree_pages_exact(runtime->control,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control)));\n\tkfree(runtime->hw_constraints.rules);\n\t/* Avoid concurrent access to runtime via PCM timer interface */\n\tif (substream->timer) {\n\t\tspin_lock_irq(&substream->timer->lock);\n\t\tsubstream->runtime = NULL;\n\t\tspin_unlock_irq(&substream->timer->lock);\n\t} else {\n\t\tsubstream->runtime = NULL;\n\t}\n\tkfree(runtime);\n\tput_pid(substream->pid);\n\tsubstream->pid = NULL;\n\tsubstream->pstr->substream_opened--;\n}",
        "bug_line_number": [
            20,
            21
        ],
        "vul": 1,
        "id": 8
    },
    {
        "code": "static int snd_pcm_hw_params(struct snd_pcm_substream *substream,\n\t\t\t     struct snd_pcm_hw_params *params)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tint err, usecs;\n\tunsigned int bits;\n\tsnd_pcm_uframes_t frames;\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn -ENXIO;\n\truntime = substream->runtime;\n\tsnd_pcm_stream_lock_irq(substream);\n\tswitch (runtime->status->state) {\n\tcase SNDRV_PCM_STATE_OPEN:\n\tcase SNDRV_PCM_STATE_SETUP:\n\tcase SNDRV_PCM_STATE_PREPARED:\n\t\tbreak;\n\tdefault:\n\t\tsnd_pcm_stream_unlock_irq(substream);\n\t\treturn -EBADFD;\n\t}\n\tsnd_pcm_stream_unlock_irq(substream);\n#if IS_ENABLED(CONFIG_SND_PCM_OSS)\n\tif (!substream->oss.oss)\n#endif\n\t\tif (atomic_read(&substream->mmap_count))\n\t\t\treturn -EBADFD;\n\tsnd_pcm_sync_stop(substream, true);\n\tparams->rmask = ~0U;\n\terr = snd_pcm_hw_refine(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\terr = snd_pcm_hw_params_choose(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\terr = fixup_unreferenced_params(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\tif (substream->managed_buffer_alloc) {\n\t\terr = snd_pcm_lib_malloc_pages(substream,\n\t\t\t\t\t       params_buffer_bytes(params));\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t\truntime->buffer_changed = err > 0;\n\t}\n\tif (substream->ops->hw_params != NULL) {\n\t\terr = substream->ops->hw_params(substream, params);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t}\n\truntime->access = params_access(params);\n\truntime->format = params_format(params);\n\truntime->subformat = params_subformat(params);\n\truntime->channels = params_channels(params);\n\truntime->rate = params_rate(params);\n\truntime->period_size = params_period_size(params);\n\truntime->periods = params_periods(params);\n\truntime->buffer_size = params_buffer_size(params);\n\truntime->info = params->info;\n\truntime->rate_num = params->rate_num;\n\truntime->rate_den = params->rate_den;\n\truntime->no_period_wakeup =\n\t\t\t(params->info & SNDRV_PCM_INFO_NO_PERIOD_WAKEUP) &&\n\t\t\t(params->flags & SNDRV_PCM_HW_PARAMS_NO_PERIOD_WAKEUP);\n\tbits = snd_pcm_format_physical_width(runtime->format);\n\truntime->sample_bits = bits;\n\tbits *= runtime->channels;\n\truntime->frame_bits = bits;\n\tframes = 1;\n\twhile (bits % 8 != 0) {\n\t\tbits *= 2;\n\t\tframes *= 2;\n\t}\n\truntime->byte_align = bits / 8;\n\truntime->min_align = frames;\n\t/* Default sw params */\n\truntime->tstamp_mode = SNDRV_PCM_TSTAMP_NONE;\n\truntime->period_step = 1;\n\truntime->control->avail_min = runtime->period_size;\n\truntime->start_threshold = 1;\n\truntime->stop_threshold = runtime->buffer_size;\n\truntime->silence_threshold = 0;\n\truntime->silence_size = 0;\n\truntime->boundary = runtime->buffer_size;\n\twhile (runtime->boundary * 2 <= LONG_MAX - runtime->buffer_size)\n\t\truntime->boundary *= 2;\n\t/* clear the buffer for avoiding possible kernel info leaks */\n\tif (runtime->dma_area && !substream->ops->copy_user) {\n\t\tsize_t size = runtime->dma_bytes;\n\t\tif (runtime->info & SNDRV_PCM_INFO_MMAP)\n\t\t\tsize = PAGE_ALIGN(size);\n\t\tmemset(runtime->dma_area, 0, size);\n\t}\n\tsnd_pcm_timer_resolution_change(substream);\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_SETUP);\n\tif (cpu_latency_qos_request_active(&substream->latency_pm_qos_req))\n\t\tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n\tusecs = period_to_usecs(runtime);\n\tif (usecs >= 0)\n\t\tcpu_latency_qos_add_request(&substream->latency_pm_qos_req,\n\t\t\t\t\t    usecs);\n\treturn 0;\n _error:\n\t/* hardware might be unusable from this time,\n\t   so we force application to retry to set\n\t   the correct hardware parameter settings */\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);",
        "bug_line_number": [
            4,
            5,
            9,
            10,
            14,
            15,
            17,
            18,
            19,
            21,
            22,
            23,
            24,
            25,
            26,
            100,
            101,
            102,
            103,
            104,
            105
        ],
        "vul": 1,
        "id": 10
    },
    {
        "code": "\tif (substream->ops->hw_free != NULL)\n\t\tsubstream->ops->hw_free(substream);\n\tif (substream->managed_buffer_alloc)\n\t\tsnd_pcm_lib_free_pages(substream);\n\treturn err;\n}",
        "bug_line_number": [
            0,
            1,
            2,
            3,
            4
        ],
        "vul": 1,
        "id": 10
    },
    {
        "code": "void gru_check_context_placement(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru;\n\t/*\n\t * If the current task is the context owner, verify that the\n\t * context is correctly placed. This test is skipped for non-owner\n\t * references. Pthread apps use non-owner references to the CBRs.\n\t */\n\tgru = gts->ts_gru;\n\tif (!gru || gts->ts_tgid_owner != current->tgid)\n\t\treturn;\n\tif (!gru_check_chiplet_assignment(gru, gts)) {\n\t\tSTAT(check_context_unload);\n\t\tgru_unload_context(gts, 1);\n\t} else if (gru_retarget_intr(gts)) {\n\t\tSTAT(check_context_retarget_intr);\n\t}\n}",
        "bug_line_number": [
            0,
            1,
            2,
            3,
            8,
            9,
            10,
            11,
            13,
            14,
            16,
            17
        ],
        "vul": 1,
        "id": 12
    },
    {
        "code": "static void aio_fsync_work(struct work_struct *work)\n{\n\tstruct fsync_iocb *req = container_of(work, struct fsync_iocb, work);\n\tint ret;\n\tret = vfs_fsync(req->file, req->datasync);\n\tfput(req->file);\n\taio_complete(container_of(req, struct aio_kiocb, fsync), ret, 0);\n}",
        "bug_line_number": [
            5
        ],
        "vul": 1,
        "id": 14
    },
    {
        "code": "static int __io_submit_one(struct kioctx *ctx, const struct iocb *iocb,\n\t\t\t   struct iocb __user *user_iocb, bool compat)\n{\n\tstruct aio_kiocb *req;\n\tssize_t ret;\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(iocb->aio_reserved2)) {\n\t\tpr_debug(\"EINVAL: reserve field set\\n\");\n\t\treturn -EINVAL;\n\t}\n\t/* prevent overflows */\n\tif (unlikely(\n\t    (iocb->aio_buf != (unsigned long)iocb->aio_buf) ||\n\t    (iocb->aio_nbytes != (size_t)iocb->aio_nbytes) ||\n\t    ((ssize_t)iocb->aio_nbytes < 0)\n\t   )) {\n\t\tpr_debug(\"EINVAL: overflow check\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (!get_reqs_available(ctx))\n\t\treturn -EAGAIN;\n\tret = -EAGAIN;\n\treq = aio_get_req(ctx);\n\tif (unlikely(!req))\n\t\tgoto out_put_reqs_available;\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD) {\n\t\t/*\n\t\t * If the IOCB_FLAG_RESFD flag of aio_flags is set, get an\n\t\t * instance of the file* now. The file descriptor must be\n\t\t * an eventfd() fd, and will be signaled for each completed\n\t\t * event using the eventfd_signal() function.\n\t\t */\n\t\treq->ki_eventfd = eventfd_ctx_fdget((int) iocb->aio_resfd);\n\t\tif (IS_ERR(req->ki_eventfd)) {\n\t\t\tret = PTR_ERR(req->ki_eventfd);\n\t\t\treq->ki_eventfd = NULL;\n\t\t\tgoto out_put_req;\n\t\t}\n\t}\n\tret = put_user(KIOCB_KEY, &user_iocb->aio_key);\n\tif (unlikely(ret)) {\n\t\tpr_debug(\"EFAULT: aio_key\\n\");\n\t\tgoto out_put_req;\n\t}\n\treq->ki_user_iocb = user_iocb;\n\treq->ki_user_data = iocb->aio_data;\n\tswitch (iocb->aio_lio_opcode) {\n\tcase IOCB_CMD_PREAD:\n\t\tret = aio_read(&req->rw, iocb, false, compat);\n\t\tbreak;\n\tcase IOCB_CMD_PWRITE:\n\t\tret = aio_write(&req->rw, iocb, false, compat);\n\t\tbreak;\n\tcase IOCB_CMD_PREADV:\n\t\tret = aio_read(&req->rw, iocb, true, compat);\n\t\tbreak;\n\tcase IOCB_CMD_PWRITEV:\n\t\tret = aio_write(&req->rw, iocb, true, compat);\n\t\tbreak;\n\tcase IOCB_CMD_FSYNC:\n\t\tret = aio_fsync(&req->fsync, iocb, false);\n\t\tbreak;\n\tcase IOCB_CMD_FDSYNC:\n\t\tret = aio_fsync(&req->fsync, iocb, true);\n\t\tbreak;\n\tcase IOCB_CMD_POLL:\n\t\tret = aio_poll(req, iocb);\n\t\tbreak;\n\tdefault:\n\t\tpr_debug(\"invalid aio operation %d\\n\", iocb->aio_lio_opcode);\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\t/*\n\t * If ret is 0, we'd either done aio_complete() ourselves or have\n\t * arranged for that to be done asynchronously.  Anything non-zero\n\t * means that we need to destroy req ourselves.\n\t */\n\tif (ret)\n\t\tgoto out_put_req;\n\treturn 0;\nout_put_req:\n\tif (req->ki_eventfd)\n\t\teventfd_ctx_put(req->ki_eventfd);\n\tiocb_put(req);\nout_put_reqs_available:\n\tput_reqs_available(ctx, 1);\n\treturn ret;\n}",
        "bug_line_number": [
            24,
            25
        ],
        "vul": 1,
        "id": 16
    },
    {
        "code": "static void unpin_sdma_pages(struct sdma_mmu_node *node)\n{\n\tif (node->npages) {\n\t\tunpin_vector_pages(node->pq->mm, node->pages, 0, node->npages);\n\t\tatomic_sub(node->npages, &node->pq->n_locked);\n\t}\n}",
        "bug_line_number": [
            3,
            4
        ],
        "vul": 1,
        "id": 18
    },
    {
        "code": "bool hfi1_mmu_rb_remove_unless_exact(struct mmu_rb_handler *handler,\n\t\t\t\t     unsigned long addr, unsigned long len,\n\t\t\t\t     struct mmu_rb_node **rb_node)\n{\n\tstruct mmu_rb_node *node;\n\tunsigned long flags;\n\tbool ret = false;\n\tspin_lock_irqsave(&handler->lock, flags);\n\tnode = __mmu_rb_search(handler, addr, len);\n\tif (node) {\n\t\tif (node->addr == addr && node->len == len)\n\t\t\tgoto unlock;\n\t\t__mmu_int_rb_remove(node, &handler->root);\n\t\tlist_del(&node->list); /* remove from LRU list */\n\t\tret = true;\n\t}\nunlock:\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\t*rb_node = node;\n\treturn ret;\n}",
        "bug_line_number": [
            6,
            7
        ],
        "vul": 1,
        "id": 20
    },
    {
        "code": "int hfi1_user_sdma_alloc_queues(struct hfi1_ctxtdata *uctxt,\n\t\t\t\tstruct hfi1_filedata *fd)\n{\n\tint ret = -ENOMEM;\n\tchar buf[64];\n\tstruct hfi1_devdata *dd;\n\tstruct hfi1_user_sdma_comp_q *cq;\n\tstruct hfi1_user_sdma_pkt_q *pq;\n\tif (!uctxt || !fd)\n\t\treturn -EBADF;\n\tif (!hfi1_sdma_comp_ring_size)\n\t\treturn -EINVAL;\n\tdd = uctxt->dd;\n\tpq = kzalloc(sizeof(*pq), GFP_KERNEL);\n\tif (!pq)\n\t\treturn -ENOMEM;\n\tpq->dd = dd;\n\tpq->ctxt = uctxt->ctxt;\n\tpq->subctxt = fd->subctxt;\n\tpq->n_max_reqs = hfi1_sdma_comp_ring_size;\n\tatomic_set(&pq->n_reqs, 0);\n\tinit_waitqueue_head(&pq->wait);\n\tatomic_set(&pq->n_locked, 0);\n\tpq->mm = fd->mm;\n\tiowait_init(&pq->busy, 0, NULL, NULL, defer_packet_queue,\n\t\t    activate_packet_queue, NULL, NULL);\n\tpq->reqidx = 0;\n\tpq->reqs = kcalloc(hfi1_sdma_comp_ring_size,\n\t\t\t   sizeof(*pq->reqs),\n\t\t\t   GFP_KERNEL);\n\tif (!pq->reqs)\n\t\tgoto pq_reqs_nomem;\n\tpq->req_in_use = kcalloc(BITS_TO_LONGS(hfi1_sdma_comp_ring_size),\n\t\t\t\t sizeof(*pq->req_in_use),\n\t\t\t\t GFP_KERNEL);\n\tif (!pq->req_in_use)\n\t\tgoto pq_reqs_no_in_use;\n\tsnprintf(buf, 64, \"txreq-kmem-cache-%u-%u-%u\", dd->unit, uctxt->ctxt,\n\t\t fd->subctxt);\n\tpq->txreq_cache = kmem_cache_create(buf,\n\t\t\t\t\t    sizeof(struct user_sdma_txreq),\n\t\t\t\t\t    L1_CACHE_BYTES,\n\t\t\t\t\t    SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t    NULL);\n\tif (!pq->txreq_cache) {\n\t\tdd_dev_err(dd, \"[%u] Failed to allocate TxReq cache\\n\",\n\t\t\t   uctxt->ctxt);\n\t\tgoto pq_txreq_nomem;\n\t}\n\tcq = kzalloc(sizeof(*cq), GFP_KERNEL);\n\tif (!cq)\n\t\tgoto cq_nomem;\n\tcq->comps = vmalloc_user(PAGE_ALIGN(sizeof(*cq->comps)\n\t\t\t\t * hfi1_sdma_comp_ring_size));\n\tif (!cq->comps)\n\t\tgoto cq_comps_nomem;\n\tcq->nentries = hfi1_sdma_comp_ring_size;\n\tret = hfi1_mmu_rb_register(pq, pq->mm, &sdma_rb_ops, dd->pport->hfi1_wq,\n\t\t\t\t   &pq->handler);\n\tif (ret) {\n\t\tdd_dev_err(dd, \"Failed to register with MMU %d\", ret);\n\t\tgoto pq_mmu_fail;\n\t}\n\trcu_assign_pointer(fd->pq, pq);\n\tfd->cq = cq;\n\treturn 0;\npq_mmu_fail:\n\tvfree(cq->comps);\ncq_comps_nomem:\n\tkfree(cq);\ncq_nomem:\n\tkmem_cache_destroy(pq->txreq_cache);\npq_txreq_nomem:\n\tkfree(pq->req_in_use);\npq_reqs_no_in_use:\n\tkfree(pq->reqs);\npq_reqs_nomem:\n\tkfree(pq);\n\treturn ret;\n}",
        "bug_line_number": [
            23,
            57,
            58
        ],
        "vul": 1,
        "id": 22
    },
    {
        "code": "static int hfi1_file_open(struct inode *inode, struct file *fp)\n{\n\tstruct hfi1_filedata *fd;\n\tstruct hfi1_devdata *dd = container_of(inode->i_cdev,\n\t\t\t\t\t       struct hfi1_devdata,\n\t\t\t\t\t       user_cdev);\n\tif (!((dd->flags & HFI1_PRESENT) && dd->kregbase1))\n\t\treturn -EINVAL;\n\tif (!atomic_inc_not_zero(&dd->user_refcount))\n\t\treturn -ENXIO;\n\t/* The real work is performed later in assign_ctxt() */\n\tfd = kzalloc(sizeof(*fd), GFP_KERNEL);\n\tif (!fd || init_srcu_struct(&fd->pq_srcu))\n\t\tgoto nomem;\n\tspin_lock_init(&fd->pq_rcu_lock);\n\tspin_lock_init(&fd->tid_lock);\n\tspin_lock_init(&fd->invalid_lock);\n\tfd->rec_cpu_num = -1; /* no cpu affinity by default */\n\tfd->mm = current->mm;\n\tmmgrab(fd->mm);\n\tfd->dd = dd;\n\tfp->private_data = fd;\n\treturn 0;\nnomem:\n\tkfree(fd);\n\tfp->private_data = NULL;\n\tif (atomic_dec_and_test(&dd->user_refcount))\n\t\tcomplete(&dd->user_comp);\n\treturn -ENOMEM;\n}",
        "bug_line_number": [
            18,
            19
        ],
        "vul": 1,
        "id": 24
    },
    {
        "code": "static int pin_rcv_pages(struct hfi1_filedata *fd, struct tid_user_buf *tidbuf)\n{\n\tint pinned;\n\tunsigned int npages;\n\tunsigned long vaddr = tidbuf->vaddr;\n\tstruct page **pages = NULL;\n\tstruct hfi1_devdata *dd = fd->uctxt->dd;\n\t/* Get the number of pages the user buffer spans */\n\tnpages = num_user_pages(vaddr, tidbuf->length);\n\tif (!npages)\n\t\treturn -EINVAL;\n\tif (npages > fd->uctxt->expected_count) {\n\t\tdd_dev_err(dd, \"Expected buffer too big\\n\");\n\t\treturn -EINVAL;\n\t}\n\t/* Allocate the array of struct page pointers needed for pinning */\n\tpages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\t/*\n\t * Pin all the pages of the user buffer. If we can't pin all the\n\t * pages, accept the amount pinned so far and program only that.\n\t * User space knows how to deal with partially programmed buffers.\n\t */\n\tif (!hfi1_can_pin_pages(dd, fd->mm, fd->tid_n_pinned, npages)) {\n\t\tkfree(pages);\n\t\treturn -ENOMEM;\n\t}\n\tpinned = hfi1_acquire_user_pages(fd->mm, vaddr, npages, true, pages);\n\tif (pinned <= 0) {\n\t\tkfree(pages);\n\t\treturn pinned;\n\t}\n\ttidbuf->pages = pages;\n\ttidbuf->npages = npages;\n\tfd->tid_n_pinned += pinned;\n\treturn pinned;\n}",
        "bug_line_number": [
            24,
            25,
            28,
            29
        ],
        "vul": 1,
        "id": 26
    },
    {
        "code": "static void io_rw_resubmit(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint err;\n\terr = io_sq_thread_acquire_mm(ctx, req);\n\tif (io_resubmit_prep(req, err)) {\n\t\trefcount_inc(&req->refs);\n\t\tio_queue_async_work(req);\n\t}\n}",
        "bug_line_number": [
            9,
            10
        ],
        "vul": 1,
        "id": 28
    },
    {
        "code": "static int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t     int sync, void *arg)\n{\n\tstruct wait_page_queue *wpq;\n\tstruct io_kiocb *req = wait->private;\n\tstruct wait_page_key *key = arg;\n\tint ret;\n\twpq = container_of(wait, struct wait_page_queue, wait);\n\tif (!wake_page_match(wpq, key))\n\t\treturn 0;\n\tlist_del_init(&wait->entry);\n\tinit_task_work(&req->task_work, io_req_task_submit);\n\t/* submit ref gets dropped, acquire a new one */\n\trefcount_inc(&req->refs);\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\t\t/* queue just for cancelation */\n\t\tinit_task_work(&req->task_work, io_req_task_cancel);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, 0);\n\t\twake_up_process(tsk);\n\t}\n\treturn 1;\n}",
        "bug_line_number": [
            11,
            12
        ],
        "vul": 1,
        "id": 30
    },
    {
        "code": "static void smsusb_stop_streaming(struct smsusb_device_t *dev)\n{\n\tint i;\n\tfor (i = 0; i < MAX_URBS; i++) {\n\t\tusb_kill_urb(&dev->surbs[i].urb);\n\t\tcancel_work_sync(&dev->surbs[i].wq);\n\t\tif (dev->surbs[i].cb) {\n\t\t\tsmscore_putbuffer(dev->coredev, dev->surbs[i].cb);\n\t\t\tdev->surbs[i].cb = NULL;\n\t\t}\n\t}\n}",
        "bug_line_number": [
            5,
            6
        ],
        "vul": 1,
        "id": 32
    },
    {
        "code": "static void *proc_pid_follow_link(struct dentry *dentry, struct nameidata *nd)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error = -EACCES;\n\t/* We don't need a base pointer in the /proc filesystem */\n\tpath_put(&nd->path);\n\t/* Are we allowed to snoop on the tasks file descriptors? */\n\tif (!proc_fd_access_allowed(inode))\n\t\tgoto out;\n\terror = PROC_I(inode)->op.proc_get_link(inode, &nd->path);\n\tnd->last_type = LAST_BIND;\nout:\n\treturn ERR_PTR(error);\n}",
        "bug_line_number": [
            10
        ],
        "vul": 1,
        "id": 34
    },
    {
        "code": "static int route4_change(struct net *net, struct sk_buff *in_skb,\n\t\t\t struct tcf_proto *tp, unsigned long base, u32 handle,\n\t\t\t struct nlattr **tca, void **arg, bool ovr,\n\t\t\t bool rtnl_held, struct netlink_ext_ack *extack)\n{\n\tstruct route4_head *head = rtnl_dereference(tp->root);\n\tstruct route4_filter __rcu **fp;\n\tstruct route4_filter *fold, *f1, *pfp, *f = NULL;\n\tstruct route4_bucket *b;\n\tstruct nlattr *opt = tca[TCA_OPTIONS];\n\tstruct nlattr *tb[TCA_ROUTE4_MAX + 1];\n\tunsigned int h, th;\n\tint err;\n\tbool new = true;\n\tif (opt == NULL)\n\t\treturn handle ? -EINVAL : 0;\n\terr = nla_parse_nested_deprecated(tb, TCA_ROUTE4_MAX, opt,\n\t\t\t\t\t  route4_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\tfold = *arg;\n\tif (fold && handle && fold->handle != handle)\n\t\t\treturn -EINVAL;\n\terr = -ENOBUFS;\n\tf = kzalloc(sizeof(struct route4_filter), GFP_KERNEL);\n\tif (!f)\n\t\tgoto errout;\n\terr = tcf_exts_init(&f->exts, net, TCA_ROUTE4_ACT, TCA_ROUTE4_POLICE);\n\tif (err < 0)\n\t\tgoto errout;\n\tif (fold) {\n\t\tf->id = fold->id;\n\t\tf->iif = fold->iif;\n\t\tf->res = fold->res;\n\t\tf->handle = fold->handle;\n\t\tf->tp = fold->tp;\n\t\tf->bkt = fold->bkt;\n\t\tnew = false;\n\t}\n\terr = route4_set_parms(net, tp, base, f, handle, head, tb,\n\t\t\t       tca[TCA_RATE], new, ovr, extack);\n\tif (err < 0)\n\t\tgoto errout;\n\th = from_hash(f->handle >> 16);\n\tfp = &f->bkt->ht[h];\n\tfor (pfp = rtnl_dereference(*fp);\n\t     (f1 = rtnl_dereference(*fp)) != NULL;\n\t     fp = &f1->next)\n\t\tif (f->handle < f1->handle)\n\t\t\tbreak;\n\ttcf_block_netif_keep_dst(tp->chain->block);\n\trcu_assign_pointer(f->next, f1);\n\trcu_assign_pointer(*fp, f);\n\tif (fold && fold->handle && f->handle != fold->handle) {\n\t\tth = to_hash(fold->handle);\n\t\th = from_hash(fold->handle >> 16);\n\t\tb = rtnl_dereference(head->table[th]);\n\t\tif (b) {\n\t\t\tfp = &b->ht[h];\n\t\t\tfor (pfp = rtnl_dereference(*fp); pfp;\n\t\t\t     fp = &pfp->next, pfp = rtnl_dereference(*fp)) {\n\t\t\t\tif (pfp == f) {\n\t\t\t\t\t*fp = f->next;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\troute4_reset_fastmap(head);\n\t*arg = f;\n\tif (fold) {\n\t\ttcf_unbind_filter(tp, &fold->res);\n\t\ttcf_exts_get_net(&fold->exts);\n\t\ttcf_queue_work(&fold->rwork, route4_delete_filter_work);\n\t}\n\treturn 0;\nerrout:\n\tif (f)\n\t\ttcf_exts_destroy(&f->exts);\n\tkfree(f);\n\treturn err;\n}",
        "bug_line_number": [
            61,
            62,
            63
        ],
        "vul": 1,
        "id": 36
    },
    {
        "code": "static int io_sqpoll_wait_sq(struct io_ring_ctx *ctx)\n{\n\tint ret = 0;\n\tDEFINE_WAIT(wait);\n\tdo {\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tprepare_to_wait(&ctx->sqo_sq_wait, &wait, TASK_INTERRUPTIBLE);\n\t\tif (unlikely(ctx->sqo_dead)) {\n\t\t\tret = -EOWNERDEAD;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tschedule();\n\t} while (!signal_pending(current));\n\tfinish_wait(&ctx->sqo_sq_wait, &wait);\nout:\n\treturn ret;\n}",
        "bug_line_number": [
            8,
            9,
            10,
            11,
            17
        ],
        "vul": 1,
        "id": 38
    },
    {
        "code": "static void __io_req_task_submit(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\t/* ctx stays valid until unlock, even if we drop all ours ctx->refs */\n\tmutex_lock(&ctx->uring_lock);\n\tif (!ctx->sqo_dead && !(current->flags & PF_EXITING) && !current->in_execve)\n\t\t__io_queue_sqe(req);\n\telse\n\t\t__io_req_task_cancel(req, -EFAULT);\n\tmutex_unlock(&ctx->uring_lock);\n}",
        "bug_line_number": [
            5,
            6
        ],
        "vul": 1,
        "id": 40
    },
    {
        "code": "void __io_uring_task_cancel(void)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tDEFINE_WAIT(wait);\n\ts64 inflight;\n\t/* make sure overflow events are dropped */\n\tatomic_inc(&tctx->in_idle);\n\t/* trigger io_disable_sqo_submit() */\n\tif (tctx->sqpoll) {\n\t\tstruct file *file;\n\t\tunsigned long index;\n\t\txa_for_each(&tctx->xa, index, file)\n\t\t\tio_uring_cancel_sqpoll(file->private_data);\n\t}\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\t__io_uring_files_cancel(NULL);\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\t\t/*\n\t\t * If we've seen completions, retry without waiting. This\n\t\t * avoids a race where a completion comes in before we did\n\t\t * prepare_to_wait().\n\t\t */\n\t\tif (inflight == tctx_inflight(tctx))\n\t\t\tschedule();\n\t\tfinish_wait(&tctx->wait, &wait);\n\t} while (1);\n\tatomic_dec(&tctx->in_idle);\n\tio_uring_clean_tctx(tctx);\n\t/* all current's requests should be gone, we can kill tctx */\n\t__io_uring_free(current);\n}",
        "bug_line_number": [
            7
        ],
        "vul": 1,
        "id": 42
    },
    {
        "code": "int l2tp_session_delete(struct l2tp_session *session)\n{\n\tif (session->ref)\n\t\t(*session->ref)(session);\n\t__l2tp_session_unhash(session);\n\tl2tp_session_queue_purge(session);\n\tif (session->session_close != NULL)\n\t\t(*session->session_close)(session);\n\tif (session->deref)\n\t\t(*session->deref)(session);\n\tl2tp_session_dec_refcount(session);\n\treturn 0;\n}",
        "bug_line_number": [
            1,
            2
        ],
        "vul": 1,
        "id": 44
    },
    {
        "code": "void kvmppc_uvmem_drop_pages(const struct kvm_memory_slot *slot,\n\t\t\t     struct kvm *kvm, bool skip_page_out)\n{\n\tint i;\n\tstruct kvmppc_uvmem_page_pvt *pvt;\n\tstruct page *uvmem_page;\n\tstruct vm_area_struct *vma = NULL;\n\tunsigned long uvmem_pfn, gfn;\n\tunsigned long addr;\n\tmmap_read_lock(kvm->mm);\n\taddr = slot->userspace_addr;\n\tgfn = slot->base_gfn;\n\tfor (i = slot->npages; i; --i, ++gfn, addr += PAGE_SIZE) {\n\t\t/* Fetch the VMA if addr is not in the latest fetched one */\n\t\tif (!vma || addr >= vma->vm_end) {\n\t\t\tvma = vma_lookup(kvm->mm, addr);\n\t\t\tif (!vma) {\n\t\t\t\tpr_err(\"Can't find VMA for gfn:0x%lx\\n\", gfn);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tmutex_lock(&kvm->arch.uvmem_lock);\n\t\tif (kvmppc_gfn_is_uvmem_pfn(gfn, kvm, &uvmem_pfn)) {\n\t\t\tuvmem_page = pfn_to_page(uvmem_pfn);\n\t\t\tpvt = uvmem_page->zone_device_data;\n\t\t\tpvt->skip_page_out = skip_page_out;\n\t\t\tpvt->remove_gfn = true;\n\t\t\tif (__kvmppc_svm_page_out(vma, addr, addr + PAGE_SIZE,\n\t\t\t\t\t\t  PAGE_SHIFT, kvm, pvt->gpa))\n\t\t\t\tpr_err(\"Can't page out gpa:0x%lx addr:0x%lx\\n\",\n\t\t\t\t       pvt->gpa, addr);\n\t\t} else {\n\t\t\t/* Remove the shared flag if any */\n\t\t\tkvmppc_gfn_remove(gfn, kvm);\n\t\t}\n\t\tmutex_unlock(&kvm->arch.uvmem_lock);\n\t}\n\tmmap_read_unlock(kvm->mm);\n}",
        "bug_line_number": [
            28,
            29
        ],
        "vul": 1,
        "id": 46
    },
    {
        "code": "static vm_fault_t dmirror_devmem_fault(struct vm_fault *vmf)\n{\n\tstruct migrate_vma args;\n\tunsigned long src_pfns = 0;\n\tunsigned long dst_pfns = 0;\n\tstruct page *rpage;\n\tstruct dmirror *dmirror;\n\tvm_fault_t ret;\n\t/*\n\t * Normally, a device would use the page->zone_device_data to point to\n\t * the mirror but here we use it to hold the page for the simulated\n\t * device memory and that page holds the pointer to the mirror.\n\t */\n\trpage = vmf->page->zone_device_data;\n\tdmirror = rpage->zone_device_data;\n\t/* FIXME demonstrate how we can adjust migrate range */\n\targs.vma = vmf->vma;\n\targs.start = vmf->address;\n\targs.end = args.start + PAGE_SIZE;\n\targs.src = &src_pfns;\n\targs.dst = &dst_pfns;\n\targs.pgmap_owner = dmirror->mdevice;\n\targs.flags = dmirror_select_device(dmirror);\n\tif (migrate_vma_setup(&args))\n\t\treturn VM_FAULT_SIGBUS;\n\tret = dmirror_devmem_fault_alloc_and_copy(&args, dmirror);\n\tif (ret)\n\t\treturn ret;\n\tmigrate_vma_pages(&args);\n\t/*\n\t * No device finalize step is needed since\n\t * dmirror_devmem_fault_alloc_and_copy() will have already\n\t * invalidated the device page table.\n\t */\n\tmigrate_vma_finalize(&args);\n\treturn 0;\n}",
        "bug_line_number": [
            2,
            3,
            22,
            23
        ],
        "vul": 1,
        "id": 48
    },
    {
        "code": "int __ext4_journal_stop(const char *where, unsigned int line, handle_t *handle)\n{\n\tstruct super_block *sb;\n\tint err;\n\tint rc;\n\tif (!ext4_handle_valid(handle)) {\n\t\text4_put_nojournal(handle);\n\t\treturn 0;\n\t}\n\tif (!handle->h_transaction) {\n\t\terr = jbd2_journal_stop(handle);\n\t\treturn handle->h_err ? handle->h_err : err;\n\t}\n\tsb = handle->h_transaction->t_journal->j_private;\n\terr = handle->h_err;\n\trc = jbd2_journal_stop(handle);\n\tif (!err)\n\t\terr = rc;\n\tif (err)\n\t\t__ext4_std_error(sb, where, line, err);\n\treturn err;\n}",
        "bug_line_number": [
            8,
            9,
            10,
            11,
            12,
            14
        ],
        "vul": 1,
        "id": 50
    },
    {
        "code": "SYSCALL_DEFINE4(set_mempolicy_home_node, unsigned long, start, unsigned long, len,\n\t\tunsigned long, home_node, unsigned long, flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct mempolicy *new, *old;\n\tunsigned long end;\n\tint err = -ENOENT;\n\tVMA_ITERATOR(vmi, mm, start);\n\tstart = untagged_addr(start);\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\t/*\n\t * flags is used for future extension if any.\n\t */\n\tif (flags != 0)\n\t\treturn -EINVAL;\n\t/*\n\t * Check home_node is online to avoid accessing uninitialized\n\t * NODE_DATA.\n\t */\n\tif (home_node >= MAX_NUMNODES || !node_online(home_node))\n\t\treturn -EINVAL;\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\tmmap_write_lock(mm);\n\tprev = vma_prev(&vmi);\n\tfor_each_vma_range(vmi, vma, end) {\n\t\t/*\n\t\t * If any vma in the range got policy other than MPOL_BIND\n\t\t * or MPOL_PREFERRED_MANY we return error. We don't reset\n\t\t * the home node for vmas we already updated before.\n\t\t */\n\t\told = vma_policy(vma);\n\t\tif (!old)\n\t\t\tcontinue;\n\t\tif (old->mode != MPOL_BIND && old->mode != MPOL_PREFERRED_MANY) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tnew = mpol_dup(old);\n\t\tif (IS_ERR(new)) {\n\t\t\terr = PTR_ERR(new);\n\t\t\tbreak;\n\t\t}\n\t\tnew->home_node = home_node;\n\t\terr = mbind_range(&vmi, vma, &prev, start, end, new);\n\t\tmpol_put(new);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tmmap_write_unlock(mm);\n\treturn err;\n}",
        "bug_line_number": [
            48,
            49
        ],
        "vul": 1,
        "id": 52
    },
    {
        "code": "static int vma_replace_policy(struct vm_area_struct *vma,\n\t\t\t\t\t\tstruct mempolicy *pol)\n{\n\tint err;\n\tstruct mempolicy *old;\n\tstruct mempolicy *new;\n\tpr_debug(\"vma %lx-%lx/%lx vm_ops %p vm_file %p set_policy %p\\n\",\n\t\t vma->vm_start, vma->vm_end, vma->vm_pgoff,\n\t\t vma->vm_ops, vma->vm_file,\n\t\t vma->vm_ops ? vma->vm_ops->set_policy : NULL);\n\tnew = mpol_dup(pol);\n\tif (IS_ERR(new))\n\t\treturn PTR_ERR(new);\n\tif (vma->vm_ops && vma->vm_ops->set_policy) {\n\t\terr = vma->vm_ops->set_policy(vma, new);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t}\n\told = vma->vm_policy;\n\tvma->vm_policy = new; /* protected by mmap_lock */\n\tmpol_put(old);\n\treturn 0;\n err_out:\n\tmpol_put(new);\n\treturn err;\n}",
        "bug_line_number": [
            5,
            6
        ],
        "vul": 1,
        "id": 54
    },
    {
        "code": "static __be32\nnfsd4_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tunion nfsd4_op_u *u)\n{\n\tstruct nfsd4_copy *copy = &u->copy;\n\t__be32 status;\n\tstruct nfsd4_copy *async_copy = NULL;\n\tif (nfsd4_ssc_is_inter(copy)) {\n\t\tif (!inter_copy_offload_enable || nfsd4_copy_is_sync(copy)) {\n\t\t\tstatus = nfserr_notsupp;\n\t\t\tgoto out;\n\t\t}\n\t\tstatus = nfsd4_setup_inter_ssc(rqstp, cstate, copy,\n\t\t\t\t&copy->ss_mnt);\n\t\tif (status)\n\t\t\treturn nfserr_offload_denied;\n\t} else {\n\t\tstatus = nfsd4_setup_intra_ssc(rqstp, cstate, copy);\n\t\tif (status)\n\t\t\treturn status;\n\t}\n\tcopy->cp_clp = cstate->clp;\n\tmemcpy(&copy->fh, &cstate->current_fh.fh_handle,\n\t\tsizeof(struct knfsd_fh));\n\tif (nfsd4_copy_is_async(copy)) {\n\t\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\t\tstatus = nfserrno(-ENOMEM);\n\t\tasync_copy = kzalloc(sizeof(struct nfsd4_copy), GFP_KERNEL);\n\t\tif (!async_copy)\n\t\t\tgoto out_err;\n\t\tasync_copy->cp_src = kmalloc(sizeof(*async_copy->cp_src), GFP_KERNEL);\n\t\tif (!async_copy->cp_src)\n\t\t\tgoto out_err;\n\t\tif (!nfs4_init_copy_state(nn, copy))\n\t\t\tgoto out_err;\n\t\trefcount_set(&async_copy->refcount, 1);\n\t\tmemcpy(&copy->cp_res.cb_stateid, &copy->cp_stateid.cs_stid,\n\t\t\tsizeof(copy->cp_res.cb_stateid));\n\t\tdup_copy_fields(copy, async_copy);\n\t\tasync_copy->copy_task = kthread_create(nfsd4_do_async_copy,\n\t\t\t\tasync_copy, \"%s\", \"copy thread\");\n\t\tif (IS_ERR(async_copy->copy_task))\n\t\t\tgoto out_err;\n\t\tspin_lock(&async_copy->cp_clp->async_lock);\n\t\tlist_add(&async_copy->copies,\n\t\t\t\t&async_copy->cp_clp->async_copies);\n\t\tspin_unlock(&async_copy->cp_clp->async_lock);\n\t\twake_up_process(async_copy->copy_task);\n\t\tstatus = nfs_ok;\n\t} else {\n\t\tstatus = nfsd4_do_copy(copy, copy->nf_src->nf_file,\n\t\t\t\t       copy->nf_dst->nf_file, true);\n\t\tnfsd4_cleanup_intra_ssc(copy->nf_src, copy->nf_dst);\n\t}\nout:\n\treturn status;\nout_err:\n\tif (async_copy)\n\t\tcleanup_async_copy(async_copy);\n\tstatus = nfserrno(-ENOMEM);\n\tif (nfsd4_ssc_is_inter(copy))\n\t\tnfsd4_interssc_disconnect(copy->ss_mnt);\n\tgoto out;\n}",
        "bug_line_number": [
            60,
            61,
            62
        ],
        "vul": 1,
        "id": 56
    },
    {
        "code": "int ptp_clock_unregister(struct ptp_clock *ptp)\n{\n\tptp->defunct = 1;\n\twake_up_interruptible(&ptp->tsev_wq);\n\tif (ptp->kworker) {\n\t\tkthread_cancel_delayed_work_sync(&ptp->aux_work);\n\t\tkthread_destroy_worker(ptp->kworker);\n\t}\n\t/* Release the clock's resources. */\n\tif (ptp->pps_source)\n\t\tpps_unregister_source(ptp->pps_source);\n\tdevice_destroy(ptp_class, ptp->devid);\n\tptp_cleanup_pin_groups(ptp);\n\tposix_clock_unregister(&ptp->clock);\n\treturn 0;\n}",
        "bug_line_number": [
            11
        ],
        "vul": 1,
        "id": 58
    },
    {
        "code": "static int llcp_sock_bind(struct socket *sock, struct sockaddr *addr, int alen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct nfc_llcp_local *local;\n\tstruct nfc_dev *dev;\n\tstruct sockaddr_nfc_llcp llcp_addr;\n\tint len, ret = 0;\n\tif (!addr || alen < offsetofend(struct sockaddr, sa_family) ||\n\t    addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\tpr_debug(\"sk %p addr %p family %d\\n\", sk, addr, addr->sa_family);\n\tmemset(&llcp_addr, 0, sizeof(llcp_addr));\n\tlen = min_t(unsigned int, sizeof(llcp_addr), alen);\n\tmemcpy(&llcp_addr, addr, len);\n\t/* This is going to be a listening socket, dsap must be 0 */\n\tif (llcp_addr.dsap != 0)\n\t\treturn -EINVAL;\n\tlock_sock(sk);\n\tif (sk->sk_state != LLCP_CLOSED) {\n\t\tret = -EBADFD;\n\t\tgoto error;\n\t}\n\tdev = nfc_get_device(llcp_addr.dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = nfc_llcp_local_get(local);\n\tllcp_sock->nfc_protocol = llcp_addr.nfc_protocol;\n\tllcp_sock->service_name_len = min_t(unsigned int,\n\t\t\t\t\t    llcp_addr.service_name_len,\n\t\t\t\t\t    NFC_LLCP_MAX_SERVICE_NAME);\n\tllcp_sock->service_name = kmemdup(llcp_addr.service_name,\n\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_put_local;\n\t}\n\tllcp_sock->ssap = nfc_llcp_get_sdp_ssap(local, llcp_sock);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tret = -EADDRINUSE;\n\t\tgoto free_service_name;\n\t}\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;\n\tnfc_llcp_sock_link(&local->sockets, sk);\n\tpr_debug(\"Socket bound to SAP %d\\n\", llcp_sock->ssap);\n\tsk->sk_state = LLCP_BOUND;\n\tnfc_put_device(dev);\n\trelease_sock(sk);\n\treturn 0;\nfree_service_name:\n\tkfree(llcp_sock->service_name);\n\tllcp_sock->service_name = NULL;\nsock_llcp_put_local:\n\tnfc_llcp_local_put(llcp_sock->local);\n\tllcp_sock->local = NULL;\n\tllcp_sock->dev = NULL;\nput_dev:\n\tnfc_put_device(dev);\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "bug_line_number": [
            34,
            35
        ],
        "vul": 1,
        "id": 60
    },
    {
        "code": "int nfc_llcp_data_received(struct nfc_dev *dev, struct sk_buff *skb)\n{\n\tstruct nfc_llcp_local *local;\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tkfree_skb(skb);\n\t\treturn -ENODEV;\n\t}\n\t__nfc_llcp_recv(local, skb);\n\treturn 0;\n}",
        "bug_line_number": [
            8,
            9
        ],
        "vul": 1,
        "id": 62
    },
    {
        "code": "void nfc_llcp_unregister_device(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tpr_debug(\"No such device\\n\");\n\t\treturn;\n\t}\n\tlocal_cleanup(local);\n\tnfc_llcp_local_put(local);\n}",
        "bug_line_number": [
            2,
            3
        ],
        "vul": 1,
        "id": 64
    },
    {
        "code": "u8 *nfc_llcp_general_bytes(struct nfc_dev *dev, size_t *general_bytes_len)\n{\n\tstruct nfc_llcp_local *local;\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\t*general_bytes_len = 0;\n\t\treturn NULL;\n\t}\n\tnfc_llcp_build_gb(local);\n\t*general_bytes_len = local->gb_len;\n\treturn local->gb;\n}",
        "bug_line_number": [
            9,
            10
        ],
        "vul": 1,
        "id": 66
    },
    {
        "code": "int nfc_llcp_set_remote_gb(struct nfc_dev *dev, const u8 *gb, u8 gb_len)\n{\n\tstruct nfc_llcp_local *local;\n\tif (gb_len < 3 || gb_len > NFC_MAX_GT_LEN)\n\t\treturn -EINVAL;\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tpr_err(\"No LLCP device\\n\");\n\t\treturn -ENODEV;\n\t}\n\tmemset(local->remote_gb, 0, NFC_MAX_GT_LEN);\n\tmemcpy(local->remote_gb, gb, gb_len);\n\tlocal->remote_gb_len = gb_len;\n\tif (memcmp(local->remote_gb, llcp_magic, 3)) {\n\t\tpr_err(\"MAC does not support LLCP\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn nfc_llcp_parse_gb_tlv(local,\n\t\t\t\t     &local->remote_gb[3],\n\t\t\t\t     local->remote_gb_len - 3);\n}",
        "bug_line_number": [
            2,
            3,
            15,
            16,
            17,
            18,
            19,
            20
        ],
        "vul": 1,
        "id": 68
    },
    {
        "code": "static noinline int test_btrfs_get_extent(u32 sectorsize, u32 nodesize)\n{\n\tstruct btrfs_fs_info *fs_info = NULL;\n\tstruct inode *inode = NULL;\n\tstruct btrfs_root *root = NULL;\n\tstruct extent_map *em = NULL;\n\tu64 orig_start;\n\tu64 disk_bytenr;\n\tu64 offset;\n\tint ret = -ENOMEM;\n\ttest_msg(\"running btrfs_get_extent tests\");\n\tinode = btrfs_new_test_inode();\n\tif (!inode) {\n\t\ttest_std_err(TEST_ALLOC_INODE);\n\t\treturn ret;\n\t}\n\tBTRFS_I(inode)->location.type = BTRFS_INODE_ITEM_KEY;\n\tBTRFS_I(inode)->location.objectid = BTRFS_FIRST_FREE_OBJECTID;\n\tBTRFS_I(inode)->location.offset = 0;\n\tfs_info = btrfs_alloc_dummy_fs_info(nodesize, sectorsize);\n\tif (!fs_info) {\n\t\ttest_std_err(TEST_ALLOC_FS_INFO);\n\t\tgoto out;\n\t}\n\troot = btrfs_alloc_dummy_root(fs_info);\n\tif (IS_ERR(root)) {\n\t\ttest_std_err(TEST_ALLOC_ROOT);\n\t\tgoto out;\n\t}\n\troot->node = alloc_dummy_extent_buffer(fs_info, nodesize);\n\tif (!root->node) {\n\t\ttest_std_err(TEST_ALLOC_ROOT);\n\t\tgoto out;\n\t}\n\tbtrfs_set_header_nritems(root->node, 0);\n\tbtrfs_set_header_level(root->node, 0);\n\tret = -EINVAL;\n\t/* First with no extents */\n\tBTRFS_I(inode)->root = root;\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\tem = NULL;\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tfree_extent_map(em);\n\tbtrfs_drop_extent_cache(BTRFS_I(inode), 0, (u64)-1, 0);\n\t/*\n\t * All of the magic numbers are based on the mapping setup in\n\t * setup_file_extents, so if you change anything there you need to\n\t * update the comment and update the expected values below.\n\t */\n\tsetup_file_extents(root, sectorsize);\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, (u64)-1, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != 0 || em->len != 5) {\n\t\ttest_err(\n\t\t\"unexpected extent wanted start 0 len 5, got start %llu len %llu\",\n\t\t\tem->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_INLINE) {\n\t\ttest_err(\"expected an inline, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != (sectorsize - 5)) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len 1, got start %llu len %llu\",\n\t\t\toffset, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\t/*\n\t * We don't test anything else for inline since it doesn't get set\n\t * unless we have a page for it to write into.  Maybe we should change\n\t * this?\n\t */\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");",
        "bug_line_number": [
            15,
            16
        ],
        "vul": 1,
        "id": 70
    },
    {
        "code": "struct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *sub_root = root;\n\tstruct btrfs_key location;\n\tint index;\n\tint ret = 0;\n\tif (dentry->d_name.len > BTRFS_NAME_LEN)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\tret = btrfs_inode_by_name(dir, dentry, &location);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\tif (location.type == BTRFS_INODE_ITEM_KEY) {\n\t\tinode = btrfs_iget(dir->i_sb, &location, root, NULL);\n\t\treturn inode;\n\t}\n\tindex = srcu_read_lock(&fs_info->subvol_srcu);\n\tret = fixup_tree_root_location(fs_info, dir, dentry,\n\t\t\t\t       &location, &sub_root);\n\tif (ret < 0) {\n\t\tif (ret != -ENOENT)\n\t\t\tinode = ERR_PTR(ret);\n\t\telse\n\t\t\tinode = new_simple_dir(dir->i_sb, &location, sub_root);\n\t} else {\n\t\tinode = btrfs_iget(dir->i_sb, &location, sub_root, NULL);\n\t}\n\tsrcu_read_unlock(&fs_info->subvol_srcu, index);\n\tif (!IS_ERR(inode) && root != sub_root) {\n\t\tdown_read(&fs_info->cleanup_work_sem);\n\t\tif (!sb_rdonly(inode->i_sb))\n\t\t\tret = btrfs_orphan_cleanup(sub_root);\n\t\tup_read(&fs_info->cleanup_work_sem);\n\t\tif (ret) {\n\t\t\tiput(inode);\n\t\t\tinode = ERR_PTR(ret);\n\t\t}\n\t}\n\treturn inode;\n}",
        "bug_line_number": [
            6,
            7,
            11,
            12,
            15,
            16
        ],
        "vul": 1,
        "id": 72
    },
    {
        "code": "static void nft_setelem_catchall_remove(const struct net *net,\n\t\t\t\t\tconst struct nft_set *set,\n\t\t\t\t\tstruct nft_elem_priv *elem_priv)\n{\n\tstruct nft_set_elem_catchall *catchall, *next;\n\tlist_for_each_entry_safe(catchall, next, &set->catchall_list, list) {\n\t\tif (catchall->elem == elem_priv) {\n\t\t\tlist_del_rcu(&catchall->list);\n\t\t\tkfree_rcu(catchall, rcu);\n\t\t\tbreak;\n\t\t}\n\t}\n}",
        "bug_line_number": [
            7,
            8,
            9
        ],
        "vul": 1,
        "id": 74
    },
    {
        "code": "static void k_fn(struct vc_data *vc, unsigned char value, char up_flag)\n{\n\tif (up_flag)\n\t\treturn;\n\tif ((unsigned)value < ARRAY_SIZE(func_table)) {\n\t\tif (func_table[value])\n\t\t\tputs_queue(vc, func_table[value]);\n\t} else\n\t\tpr_err(\"k_fn called with value=%d\\n\", value);\n}",
        "bug_line_number": [
            4,
            5,
            6,
            7
        ],
        "vul": 1,
        "id": 76
    },
    {
        "code": "static void sixpack_close(struct tty_struct *tty)\n{\n\tstruct sixpack *sp;\n\twrite_lock_irq(&disc_data_lock);\n\tsp = tty->disc_data;\n\ttty->disc_data = NULL;\n\twrite_unlock_irq(&disc_data_lock);\n\tif (!sp)\n\t\treturn;\n\t/*\n\t * We have now ensured that nobody can start using ap from now on, but\n\t * we have to wait for all existing users to finish.\n\t */\n\tif (!refcount_dec_and_test(&sp->refcnt))\n\t\twait_for_completion(&sp->dead);\n\t/* We must stop the queue to avoid potentially scribbling\n\t * on the free buffers. The sp->dead completion is not sufficient\n\t * to protect us from sp->xbuff access.\n\t */\n\tnetif_stop_queue(sp->dev);\n\tdel_timer_sync(&sp->tx_t);\n\tdel_timer_sync(&sp->resync_t);\n\tunregister_netdev(sp->dev);\n\t/* Free all 6pack frame buffers after unreg. */\n\tkfree(sp->rbuff);\n\tkfree(sp->xbuff);\n\tfree_netdev(sp->dev);\n}",
        "bug_line_number": [
            19,
            20,
            22
        ],
        "vul": 1,
        "id": 78
    },
    {
        "code": "static void blk_add_trace_split(void *ignore,\n\t\t\t\tstruct request_queue *q, struct bio *bio,\n\t\t\t\tunsigned int pdu)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\tif (bt) {\n\t\t__be64 rpdu = cpu_to_be64(pdu);\n\t\t__blk_add_trace(bt, bio->bi_iter.bi_sector,\n\t\t\t\tbio->bi_iter.bi_size, bio_op(bio), bio->bi_opf,\n\t\t\t\tBLK_TA_SPLIT, bio->bi_status, sizeof(rpdu),\n\t\t\t\t&rpdu, blk_trace_bio_get_cgid(q, bio));\n\t}\n}",
        "bug_line_number": [
            4,
            5,
            11,
            12
        ],
        "vul": 1,
        "id": 80
    },
    {
        "code": "static ssize_t sysfs_blk_trace_attr_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct hd_struct *p = dev_to_part(dev);\n\tstruct request_queue *q;\n\tstruct block_device *bdev;\n\tssize_t ret = -ENXIO;\n\tbdev = bdget(part_devt(p));\n\tif (bdev == NULL)\n\t\tgoto out;\n\tq = blk_trace_get_queue(bdev);\n\tif (q == NULL)\n\t\tgoto out_bdput;\n\tmutex_lock(&q->blk_trace_mutex);\n\tif (attr == &dev_attr_enable) {\n\t\tret = sprintf(buf, \"%u\\n\", !!q->blk_trace);\n\t\tgoto out_unlock_bdev;\n\t}\n\tif (q->blk_trace == NULL)\n\t\tret = sprintf(buf, \"disabled\\n\");\n\telse if (attr == &dev_attr_act_mask)\n\t\tret = blk_trace_mask2str(buf, q->blk_trace->act_mask);\n\telse if (attr == &dev_attr_pid)\n\t\tret = sprintf(buf, \"%u\\n\", q->blk_trace->pid);\n\telse if (attr == &dev_attr_start_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", q->blk_trace->start_lba);\n\telse if (attr == &dev_attr_end_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", q->blk_trace->end_lba);\nout_unlock_bdev:\n\tmutex_unlock(&q->blk_trace_mutex);\nout_bdput:\n\tbdput(bdev);\nout:\n\treturn ret;\n}",
        "bug_line_number": [
            6,
            7,
            14,
            15,
            16,
            17,
            19,
            20,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29
        ],
        "vul": 1,
        "id": 82
    },
    {
        "code": "static void blk_add_trace_unplug(void *ignore, struct request_queue *q,\n\t\t\t\t    unsigned int depth, bool explicit)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\tif (bt) {\n\t\t__be64 rpdu = cpu_to_be64(depth);\n\t\tu32 what;\n\t\tif (explicit)\n\t\t\twhat = BLK_TA_UNPLUG_IO;\n\t\telse\n\t\t\twhat = BLK_TA_UNPLUG_TIMER;\n\t\t__blk_add_trace(bt, 0, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu, 0);\n\t}\n}",
        "bug_line_number": [
            3,
            4,
            12,
            13
        ],
        "vul": 1,
        "id": 84
    },
    {
        "code": "void blk_add_driver_data(struct request_queue *q,\n\t\t\t struct request *rq,\n\t\t\t void *data, size_t len)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\tif (likely(!bt))\n\t\treturn;\n\t__blk_add_trace(bt, blk_rq_trace_sector(rq), blk_rq_bytes(rq), 0, 0,\n\t\t\t\tBLK_TA_DRV_DATA, 0, len, data,\n\t\t\t\tblk_trace_request_get_cgid(q, rq));\n}",
        "bug_line_number": [
            4,
            5,
            6,
            7,
            9,
            10
        ],
        "vul": 1,
        "id": 86
    },
    {
        "code": "static void blk_add_trace_getrq(void *ignore,\n\t\t\t\tstruct request_queue *q,\n\t\t\t\tstruct bio *bio, int rw)\n{\n\tif (bio)\n\t\tblk_add_trace_bio(q, bio, BLK_TA_GETRQ, 0);\n\telse {\n\t\tstruct blk_trace *bt = q->blk_trace;\n\t\tif (bt)\n\t\t\t__blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_GETRQ, 0, 0,\n\t\t\t\t\tNULL, 0);\n\t}\n}",
        "bug_line_number": [
            7,
            8,
            10,
            11
        ],
        "vul": 1,
        "id": 88
    },
    {
        "code": "void rose_start_idletimer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\tdel_timer(&rose->idletimer);\n\tif (rose->idle > 0) {\n\t\trose->idletimer.function = rose_idletimer_expiry;\n\t\trose->idletimer.expires  = jiffies + rose->idle;\n\t\tadd_timer(&rose->idletimer);\n\t}\n}",
        "bug_line_number": [
            3,
            4,
            7,
            8
        ],
        "vul": 1,
        "id": 90
    },
    {
        "code": "void rose_start_t3timer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\tdel_timer(&rose->timer);\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->t3;\n\tadd_timer(&rose->timer);\n}",
        "bug_line_number": [
            3,
            4,
            6,
            7
        ],
        "vul": 1,
        "id": 92
    },
    {
        "code": "void rose_start_hbtimer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\tdel_timer(&rose->timer);\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->hb;\n\tadd_timer(&rose->timer);\n}",
        "bug_line_number": [
            3,
            4,
            6,
            7
        ],
        "vul": 1,
        "id": 94
    },
    {
        "code": "void rose_start_t2timer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\tdel_timer(&rose->timer);\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->t2;\n\tadd_timer(&rose->timer);\n}",
        "bug_line_number": [
            3,
            4,
            6,
            7
        ],
        "vul": 1,
        "id": 96
    },
    {
        "code": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct *desc;\n\tshort sel;\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn -EINVAL;\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc->type & BIT(3)))\n\t\treturn -EINVAL;\n\tswitch ((desc->l << 1) | desc->d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
        "bug_line_number": [
            2,
            3,
            10,
            11,
            12,
            18,
            19,
            20,
            21
        ],
        "vul": 1,
        "id": 98
    },
    {
        "code": "static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct *desc;\n\tunsigned long limit;\n\tshort sel;\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn 0;\n\tif (user_64bit_mode(regs) || v8086_mode(regs))\n\t\treturn -1L;\n\tif (!sel)\n\t\treturn 0;\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn 0;\n\t/*\n\t * If the granularity bit is set, the limit is given in multiples\n\t * of 4096. This also means that the 12 least significant bits are\n\t * not tested when checking the segment limits. In practice,\n\t * this means that the segment ends in (limit << 12) + 0xfff.\n\t */\n\tlimit = get_desc_limit(desc);\n\tif (desc->g)\n\t\tlimit = (limit << 12) + 0xfff;\n\treturn limit;\n}",
        "bug_line_number": [
            2,
            3,
            12,
            13,
            14,
            21,
            22,
            23
        ],
        "vul": 1,
        "id": 100
    },
    {
        "code": "unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct *desc;\n\tshort sel;\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn -1L;\n\tif (v8086_mode(regs))\n\t\t/*\n\t\t * Base is simply the segment selector shifted 4\n\t\t * bits to the right.\n\t\t */\n\t\treturn (unsigned long)(sel << 4);\n\tif (user_64bit_mode(regs)) {\n\t\t/*\n\t\t * Only FS or GS will have a base address, the rest of\n\t\t * the segments' bases are forced to 0.\n\t\t */\n\t\tunsigned long base;\n\t\tif (seg_reg_idx == INAT_SEG_REG_FS)\n\t\t\trdmsrl(MSR_FS_BASE, base);\n\t\telse if (seg_reg_idx == INAT_SEG_REG_GS)\n\t\t\t/*\n\t\t\t * swapgs was called at the kernel entry point. Thus,\n\t\t\t * MSR_KERNEL_GS_BASE will have the user-space GS base.\n\t\t\t */\n\t\t\trdmsrl(MSR_KERNEL_GS_BASE, base);\n\t\telse\n\t\t\tbase = 0;\n\t\treturn base;\n\t}\n\t/* In protected mode the segment selector cannot be null. */\n\tif (!sel)\n\t\treturn -1L;\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn -1L;\n\treturn get_desc_base(desc);\n}",
        "bug_line_number": [
            2,
            3,
            34,
            35,
            36,
            37,
            38
        ],
        "vul": 1,
        "id": 102
    },
    {
        "code": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tio_req_init_async(req);\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n}",
        "bug_line_number": [
            13,
            14
        ],
        "vul": 1,
        "id": 104
    },
    {
        "code": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->work.identity->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
        "bug_line_number": [
            18,
            19
        ],
        "vul": 1,
        "id": 106
    },
    {
        "code": "static void io_worker_handle_work(struct io_worker *worker)\n\t__releases(wqe->lock)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wq *wq = wqe->wq;\n\tdo {\n\t\tstruct io_wq_work *work;\nget_next:\n\t\t/*\n\t\t * If we got some work, mark us as busy. If we didn't, but\n\t\t * the list isn't empty, it means we stalled on hashed work.\n\t\t * Mark us stalled so we don't keep looking for work when we\n\t\t * can't make progress, any work completion or insertion will\n\t\t * clear the stalled flag.\n\t\t */\n\t\twork = io_get_next_work(wqe);\n\t\tif (work)\n\t\t\t__io_worker_busy(wqe, worker, work);\n\t\telse if (!wq_list_empty(&wqe->work_list))\n\t\t\twqe->flags |= IO_WQE_FLAG_STALLED;\n\t\traw_spin_unlock_irq(&wqe->lock);\n\t\tif (!work)\n\t\t\tbreak;\n\t\tio_assign_current_work(worker, work);\n\t\t/* handle a whole dependent link */\n\t\tdo {\n\t\t\tstruct io_wq_work *next_hashed, *linked;\n\t\t\tunsigned int hash = io_get_work_hash(work);\n\t\t\tnext_hashed = wq_next_work(work);\n\t\t\twq->do_work(work);\n\t\t\tio_assign_current_work(worker, NULL);\n\t\t\tlinked = wq->free_work(work);\n\t\t\twork = next_hashed;\n\t\t\tif (!work && linked && !io_wq_is_hashed(linked)) {\n\t\t\t\twork = linked;\n\t\t\t\tlinked = NULL;\n\t\t\t}\n\t\t\tio_assign_current_work(worker, work);\n\t\t\tif (linked)\n\t\t\t\tio_wqe_enqueue(wqe, linked);\n\t\t\tif (hash != -1U && !next_hashed) {\n\t\t\t\traw_spin_lock_irq(&wqe->lock);\n\t\t\t\twqe->hash_map &= ~BIT_ULL(hash);\n\t\t\t\twqe->flags &= ~IO_WQE_FLAG_STALLED;\n\t\t\t\t/* skip unnecessary unlock-lock wqe->lock */\n\t\t\t\tif (!work)\n\t\t\t\t\tgoto get_next;\n\t\t\t\traw_spin_unlock_irq(&wqe->lock);\n\t\t\t}\n\t\t} while (work);\n\t\traw_spin_lock_irq(&wqe->lock);\n\t} while (1);\n}",
        "bug_line_number": [
            28,
            29
        ],
        "vul": 1,
        "id": 108
    },
    {
        "code": "static int writeback_single_inode(struct inode *inode,\n\t\t\t\t  struct writeback_control *wbc)\n{\n\tstruct bdi_writeback *wb;\n\tint ret = 0;\n\tspin_lock(&inode->i_lock);\n\tif (!atomic_read(&inode->i_count))\n\t\tWARN_ON(!(inode->i_state & (I_WILL_FREE|I_FREEING)));\n\telse\n\t\tWARN_ON(inode->i_state & I_WILL_FREE);\n\tif (inode->i_state & I_SYNC) {\n\t\t/*\n\t\t * Writeback is already running on the inode.  For WB_SYNC_NONE,\n\t\t * that's enough and we can just return.  For WB_SYNC_ALL, we\n\t\t * must wait for the existing writeback to complete, then do\n\t\t * writeback again if there's anything left.\n\t\t */\n\t\tif (wbc->sync_mode != WB_SYNC_ALL)\n\t\t\tgoto out;\n\t\t__inode_wait_for_writeback(inode);\n\t}\n\tWARN_ON(inode->i_state & I_SYNC);\n\t/*\n\t * If the inode is already fully clean, then there's nothing to do.\n\t *\n\t * For data-integrity syncs we also need to check whether any pages are\n\t * still under writeback, e.g. due to prior WB_SYNC_NONE writeback.  If\n\t * there are any such pages, we'll need to wait for them.\n\t */\n\tif (!(inode->i_state & I_DIRTY_ALL) &&\n\t    (wbc->sync_mode != WB_SYNC_ALL ||\n\t     !mapping_tagged(inode->i_mapping, PAGECACHE_TAG_WRITEBACK)))\n\t\tgoto out;\n\tinode->i_state |= I_SYNC;\n\twbc_attach_and_unlock_inode(wbc, inode);\n\tret = __writeback_single_inode(inode, wbc);\n\twbc_detach_inode(wbc);\n\twb = inode_to_wb_and_lock_list(inode);\n\tspin_lock(&inode->i_lock);\n\t/*\n\t * If the inode is now fully clean, then it can be safely removed from\n\t * its writeback list (if any).  Otherwise the flusher threads are\n\t * responsible for the writeback lists.\n\t */\n\tif (!(inode->i_state & I_DIRTY_ALL))\n\t\tinode_cgwb_move_to_attached(inode, wb);\n\telse if (!(inode->i_state & I_SYNC_QUEUED)) {\n\t\tif ((inode->i_state & I_DIRTY))\n\t\t\tredirty_tail_locked(inode, wb);\n\t\telse if (inode->i_state & I_DIRTY_TIME) {\n\t\t\tinode->dirtied_when = jiffies;\n\t\t\tinode_io_list_move_locked(inode, wb, &wb->b_dirty_time);\n\t\t}\n\t}\n\tspin_unlock(&wb->list_lock);\n\tinode_sync_complete(inode);\nout:\n\tspin_unlock(&inode->i_lock);\n\treturn ret;\n}",
        "bug_line_number": [
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52
        ],
        "vul": 1,
        "id": 110
    },
    {
        "code": "int setup_async_work(struct ksmbd_work *work, void (*fn)(void **), void **arg)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct ksmbd_conn *conn = work->conn;\n\tint id;\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\trsp_hdr->Flags |= SMB2_FLAGS_ASYNC_COMMAND;\n\tid = ksmbd_acquire_async_msg_id(&conn->async_ida);\n\tif (id < 0) {\n\t\tpr_err(\"Failed to alloc async message id\\n\");\n\t\treturn id;\n\t}\n\twork->synchronous = false;\n\twork->async_id = id;\n\trsp_hdr->Id.AsyncId = cpu_to_le64(id);\n\tksmbd_debug(SMB,\n\t\t    \"Send interim Response to inform async request id : %d\\n\",\n\t\t    work->async_id);\n\twork->cancel_fn = fn;\n\twork->cancel_argv = arg;\n\tif (list_empty(&work->async_request_entry)) {\n\t\tspin_lock(&conn->request_lock);\n\t\tlist_add_tail(&work->async_request_entry, &conn->async_requests);\n\t\tspin_unlock(&conn->request_lock);\n\t}\n\treturn 0;\n}",
        "bug_line_number": [
            12,
            13
        ],
        "vul": 1,
        "id": 112
    },
    {
        "code": "static int ravb_close(struct net_device *ndev)\n{\n\tstruct device_node *np = ndev->dev.parent->of_node;\n\tstruct ravb_private *priv = netdev_priv(ndev);\n\tconst struct ravb_hw_info *info = priv->info;\n\tstruct ravb_tstamp_skb *ts_skb, *ts_skb2;\n\tnetif_tx_stop_all_queues(ndev);\n\t/* Disable interrupts by clearing the interrupt masks. */\n\travb_write(ndev, 0, RIC0);\n\travb_write(ndev, 0, RIC2);\n\travb_write(ndev, 0, TIC);\n\t/* Stop PTP Clock driver */\n\tif (info->gptp)\n\t\travb_ptp_stop(ndev);\n\t/* Set the config mode to stop the AVB-DMAC's processes */\n\tif (ravb_stop_dma(ndev) < 0)\n\t\tnetdev_err(ndev,\n\t\t\t   \"device will be stopped after h/w processes are done.\\n\");\n\t/* Clear the timestamp list */\n\tif (info->gptp || info->ccc_gac) {\n\t\tlist_for_each_entry_safe(ts_skb, ts_skb2, &priv->ts_skb_list, list) {\n\t\t\tlist_del(&ts_skb->list);\n\t\t\tkfree_skb(ts_skb->skb);\n\t\t\tkfree(ts_skb);\n\t\t}\n\t}\n\t/* PHY disconnect */\n\tif (ndev->phydev) {\n\t\tphy_stop(ndev->phydev);\n\t\tphy_disconnect(ndev->phydev);\n\t\tif (of_phy_is_fixed_link(np))\n\t\t\tof_phy_deregister_fixed_link(np);\n\t}\n\tif (info->multi_irqs) {\n\t\tfree_irq(priv->tx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->tx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->emac_irq, ndev);\n\t\tif (info->err_mgmt_irqs) {\n\t\t\tfree_irq(priv->erra_irq, ndev);\n\t\t\tfree_irq(priv->mgmta_irq, ndev);\n\t\t}\n\t}\n\tfree_irq(ndev->irq, ndev);\n\tif (info->nc_queues)\n\t\tnapi_disable(&priv->napi[RAVB_NC]);\n\tnapi_disable(&priv->napi[RAVB_BE]);\n\t/* Free all the skb's in the RX queue and the DMA buffers. */\n\travb_ring_free(ndev, RAVB_BE);\n\tif (info->nc_queues)\n\t\travb_ring_free(ndev, RAVB_NC);\n\treturn 0;\n}",
        "bug_line_number": [
            32,
            33
        ],
        "vul": 1,
        "id": 114
    },
    {
        "code": "\tif (!(fault & VM_FAULT_RETRY)) {\n\t\tcount_vm_vma_lock_event(VMA_LOCK_SUCCESS);\n\t\tgoto done;\n\t}\n\tcount_vm_vma_lock_event(VMA_LOCK_RETRY);\n\t/* Quick path to respond to signals */\n\tif (fault_signal_pending(fault, regs)) {\n\t\tif (!user_mode(regs))\n\t\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t\t SIGBUS, BUS_ADRERR,\n\t\t\t\t\t\t ARCH_DEFAULT_PKEY);\n\t\treturn;\n\t}\nlock_mmap:\n#endif /* CONFIG_PER_VMA_LOCK */\n\t/*\n\t * Kernel-mode access to the user address space should only occur\n\t * on well-defined single instructions listed in the exception\n\t * tables.  But, an erroneous kernel fault occurring outside one of\n\t * those areas which also holds mmap_lock might deadlock attempting\n\t * to validate the fault against the address space.\n\t *\n\t * Only do the expensive exception table search when we might be at\n\t * risk of a deadlock.  This happens if we\n\t * 1. Failed to acquire mmap_lock, and\n\t * 2. The access did not originate in userspace.\n\t */\n\tif (unlikely(!mmap_read_trylock(mm))) {\n\t\tif (!user_mode(regs) && !search_exception_tables(regs->ip)) {\n\t\t\t/*\n\t\t\t * Fault from code in kernel from\n\t\t\t * which we do not expect faults.\n\t\t\t */\n\t\t\tbad_area_nosemaphore(regs, error_code, address);\n\t\t\treturn;\n\t\t}\nretry:\n\t\tmmap_read_lock(mm);\n\t} else {\n\t\t/*\n\t\t * The above down_read_trylock() might have succeeded in\n\t\t * which case we'll have missed the might_sleep() from\n\t\t * down_read():\n\t\t */\n\t\tmight_sleep();\n\t}\n\tvma = find_vma(mm, address);\n\tif (unlikely(!vma)) {\n\t\tbad_area(regs, error_code, address);\n\t\treturn;\n\t}\n\tif (likely(vma->vm_start <= address))\n\t\tgoto good_area;\n\tif (unlikely(!(vma->vm_flags & VM_GROWSDOWN))) {\n\t\tbad_area(regs, error_code, address);\n\t\treturn;\n\t}\n\tif (unlikely(expand_stack(vma, address))) {\n\t\tbad_area(regs, error_code, address);\n\t\treturn;\n\t}\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\ngood_area:\n\tif (unlikely(access_error(error_code, vma))) {\n\t\tbad_area_access_error(regs, error_code, address, vma);\n\t\treturn;\n\t}\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.  Since we never set FAULT_FLAG_RETRY_NOWAIT, if\n\t * we get VM_FAULT_RETRY back, the mmap_lock has been unlocked.\n\t *\n\t * Note that handle_userfault() may also release and reacquire mmap_lock\n\t * (and not return with VM_FAULT_RETRY), when returning to userland to\n\t * repeat the page fault later with a VM_FAULT_NOPAGE retval\n\t * (potentially after handling any pending signal during the return to\n\t * userland). The return to userland is identified whenever\n\t * FAULT_FLAG_USER|FAULT_FLAG_KILLABLE are both set in flags.\n\t */\n\tfault = handle_mm_fault(vma, address, flags, regs);\n\tif (fault_signal_pending(fault, regs)) {\n\t\t/*\n\t\t * Quick path to respond to signals.  The core mm code\n\t\t * has unlocked the mm for us if we get here.\n\t\t */\n\t\tif (!user_mode(regs))\n\t\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t\t SIGBUS, BUS_ADRERR,\n\t\t\t\t\t\t ARCH_DEFAULT_PKEY);\n\t\treturn;\n\t}\n\t/* The fault is fully completed (including releasing mmap lock) */\n\tif (fault & VM_FAULT_COMPLETED)\n\t\treturn;\n\t/*\n\t * If we need to retry the mmap_lock has already been released,\n\t * and if there is a fatal signal pending there is no guarantee\n\t * that we made any progress. Handle this case first.\n\t */\n\tif (unlikely(fault & VM_FAULT_RETRY)) {\n\t\tflags |= FAULT_FLAG_TRIED;\n\t\tgoto retry;\n\t}\n\tmmap_read_unlock(mm);\n#ifdef CONFIG_PER_VMA_LOCK\ndone:",
        "bug_line_number": [
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            65
        ],
        "vul": 1,
        "id": 116
    },
    {
        "code": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tio_req_init_async(req);\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n}",
        "bug_line_number": [
            13,
            14
        ],
        "vul": 1,
        "id": 118
    },
    {
        "code": "static int fuse_get_user_pages(struct fuse_args_pages *ap, struct iov_iter *ii,\n\t\t\t       size_t *nbytesp, int write,\n\t\t\t       unsigned int max_pages)\n{\n\tsize_t nbytes = 0;  /* # bytes already packed in req */\n\tssize_t ret = 0;\n\t/* Special case for kernel I/O: can copy directly into the buffer */\n\tif (iov_iter_is_kvec(ii)) {\n\t\tunsigned long user_addr = fuse_get_user_addr(ii);\n\t\tsize_t frag_size = fuse_get_frag_size(ii, *nbytesp);\n\t\tif (write)\n\t\t\tap->args.in_args[1].value = (void *) user_addr;\n\t\telse\n\t\t\tap->args.out_args[0].value = (void *) user_addr;\n\t\tiov_iter_advance(ii, frag_size);\n\t\t*nbytesp = frag_size;\n\t\treturn 0;\n\t}\n\twhile (nbytes < *nbytesp && ap->num_pages < max_pages) {\n\t\tunsigned npages;\n\t\tsize_t start;\n\t\tret = iov_iter_get_pages(ii, &ap->pages[ap->num_pages],\n\t\t\t\t\t*nbytesp - nbytes,\n\t\t\t\t\tmax_pages - ap->num_pages,\n\t\t\t\t\t&start);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tiov_iter_advance(ii, ret);\n\t\tnbytes += ret;\n\t\tret += start;\n\t\tnpages = DIV_ROUND_UP(ret, PAGE_SIZE);\n\t\tap->descs[ap->num_pages].offset = start;\n\t\tfuse_page_descs_length_init(ap->descs, ap->num_pages, npages);\n\t\tap->num_pages += npages;\n\t\tap->descs[ap->num_pages - 1].length -=\n\t\t\t(PAGE_SIZE - ret) & (PAGE_SIZE - 1);\n\t}\n\tif (write)\n\t\tap->args.in_pages = true;\n\telse\n\t\tap->args.out_pages = true;\n\t*nbytesp = nbytes;\n\treturn ret < 0 ? ret : 0;\n}",
        "bug_line_number": [
            36,
            37
        ],
        "vul": 1,
        "id": 120
    },
    {
        "code": "static int fuse_copy_page(struct fuse_copy_state *cs, struct page **pagep,\n\t\t\t  unsigned offset, unsigned count, int zeroing)\n{\n\tint err;\n\tstruct page *page = *pagep;\n\tif (page && zeroing && count < PAGE_SIZE)\n\t\tclear_highpage(page);\n\twhile (count) {\n\t\tif (cs->write && cs->pipebufs && page) {\n\t\t\treturn fuse_ref_page(cs, page, offset, count);\n\t\t} else if (!cs->len) {\n\t\t\tif (cs->move_pages && page &&\n\t\t\t    offset == 0 && count == PAGE_SIZE) {\n\t\t\t\terr = fuse_try_move_page(cs, pagep);\n\t\t\t\tif (err <= 0)\n\t\t\t\t\treturn err;\n\t\t\t} else {\n\t\t\t\terr = fuse_copy_fill(cs);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t\tif (page) {\n\t\t\tvoid *mapaddr = kmap_local_page(page);\n\t\t\tvoid *buf = mapaddr + offset;\n\t\t\toffset += fuse_copy_do(cs, &buf, &count);\n\t\t\tkunmap_local(mapaddr);\n\t\t} else\n\t\t\toffset += fuse_copy_do(cs, NULL, &count);\n\t}\n\tif (page && !cs->write)\n\t\tflush_dcache_page(page);\n\treturn 0;\n}",
        "bug_line_number": [
            9,
            10
        ],
        "vul": 1,
        "id": 122
    },
    {
        "code": "int prepare_to_relocate(struct reloc_control *rc)\n{\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\trc->block_rsv = btrfs_alloc_block_rsv(rc->extent_root->fs_info,\n\t\t\t\t\t      BTRFS_BLOCK_RSV_TEMP);\n\tif (!rc->block_rsv)\n\t\treturn -ENOMEM;\n\tmemset(&rc->cluster, 0, sizeof(rc->cluster));\n\trc->search_start = rc->block_group->start;\n\trc->extents_found = 0;\n\trc->nodes_relocated = 0;\n\trc->merging_rsv_size = 0;\n\trc->reserved_bytes = 0;\n\trc->block_rsv->size = rc->extent_root->fs_info->nodesize *\n\t\t\t      RELOCATION_RESERVED_NODES;\n\tret = btrfs_block_rsv_refill(rc->extent_root->fs_info,\n\t\t\t\t     rc->block_rsv, rc->block_rsv->size,\n\t\t\t\t     BTRFS_RESERVE_FLUSH_ALL);\n\tif (ret)\n\t\treturn ret;\n\trc->create_reloc_tree = 1;\n\tset_reloc_control(rc);\n\ttrans = btrfs_join_transaction(rc->extent_root);\n\tif (IS_ERR(trans)) {\n\t\tunset_reloc_control(rc);\n\t\t/*\n\t\t * extent tree is not a ref_cow tree and has no reloc_root to\n\t\t * cleanup.  And callers are responsible to free the above\n\t\t * block rsv.\n\t\t */\n\t\treturn PTR_ERR(trans);\n\t}\n\treturn btrfs_commit_transaction(trans);\n}",
        "bug_line_number": [
            33,
            34
        ],
        "vul": 1,
        "id": 124
    },
    {
        "code": "static struct sk_buff *qfq_dequeue(struct Qdisc *sch)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_aggregate *in_serv_agg = q->in_serv_agg;\n\tstruct qfq_class *cl;\n\tstruct sk_buff *skb = NULL;\n\t/* next-packet len, 0 means no more active classes in in-service agg */\n\tunsigned int len = 0;\n\tif (in_serv_agg == NULL)\n\t\treturn NULL;\n\tif (!list_empty(&in_serv_agg->active))\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\t/*\n\t * If there are no active classes in the in-service aggregate,\n\t * or if the aggregate has not enough budget to serve its next\n\t * class, then choose the next aggregate to serve.\n\t */\n\tif (len == 0 || in_serv_agg->budget < len) {\n\t\tcharge_actual_service(in_serv_agg);\n\t\t/* recharge the budget of the aggregate */\n\t\tin_serv_agg->initial_budget = in_serv_agg->budget =\n\t\t\tin_serv_agg->budgetmax;\n\t\tif (!list_empty(&in_serv_agg->active)) {\n\t\t\t/*\n\t\t\t * Still active: reschedule for\n\t\t\t * service. Possible optimization: if no other\n\t\t\t * aggregate is active, then there is no point\n\t\t\t * in rescheduling this aggregate, and we can\n\t\t\t * just keep it as the in-service one. This\n\t\t\t * should be however a corner case, and to\n\t\t\t * handle it, we would need to maintain an\n\t\t\t * extra num_active_aggs field.\n\t\t\t*/\n\t\t\tqfq_update_agg_ts(q, in_serv_agg, requeue);\n\t\t\tqfq_schedule_agg(q, in_serv_agg);\n\t\t} else if (sch->q.qlen == 0) { /* no aggregate to serve */\n\t\t\tq->in_serv_agg = NULL;\n\t\t\treturn NULL;\n\t\t}\n\t\t/*\n\t\t * If we get here, there are other aggregates queued:\n\t\t * choose the new aggregate to serve.\n\t\t */\n\t\tin_serv_agg = q->in_serv_agg = qfq_choose_next_agg(q);\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\t}\n\tif (!skb)\n\t\treturn NULL;\n\tqdisc_qstats_backlog_dec(sch, skb);\n\tsch->q.qlen--;\n\tqdisc_bstats_update(sch, skb);\n\tagg_dequeue(in_serv_agg, cl, len);\n\t/* If lmax is lowered, through qfq_change_class, for a class\n\t * owning pending packets with larger size than the new value\n\t * of lmax, then the following condition may hold.\n\t */\n\tif (unlikely(in_serv_agg->budget < len))\n\t\tin_serv_agg->budget = 0;\n\telse\n\t\tin_serv_agg->budget -= len;\n\tq->V += (u64)len * q->iwsum;\n\tpr_debug(\"qfq dequeue: len %u F %lld now %lld\\n\",\n\t\t len, (unsigned long long) in_serv_agg->F,\n\t\t (unsigned long long) q->V);\n\treturn skb;\n}",
        "bug_line_number": [
            47,
            48,
            49,
            51
        ],
        "vul": 1,
        "id": 126
    },
    {
        "code": "\tif (drm_is_primary_client(file_priv))\n\t\tuser_srf->master = drm_file_get_master(file_priv);\n\t/**\n\t * From this point, the generic resource management functions\n\t * destroy the object on failure.\n\t */\n\tret = vmw_surface_init(dev_priv, srf, vmw_user_surface_free);\n\tif (unlikely(ret != 0))\n\t\tgoto out_unlock;\n\t/*\n\t * A gb-aware client referencing a shared surface will\n\t * expect a backup buffer to be present.\n\t */\n\tif (dev_priv->has_mob && req->shareable) {\n\t\tuint32_t backup_handle;\n\t\tret = vmw_gem_object_create_with_handle(dev_priv,\n\t\t\t\t\t\t\tfile_priv,\n\t\t\t\t\t\t\tres->guest_memory_size,\n\t\t\t\t\t\t\t&backup_handle,\n\t\t\t\t\t\t\t&res->guest_memory_bo);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tvmw_resource_unreference(&res);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tvmw_bo_reference(res->guest_memory_bo);\n\t\t/*\n\t\t * We don't expose the handle to the userspace and surface\n\t\t * already holds a gem reference\n\t\t */\n\t\tdrm_gem_handle_delete(file_priv, backup_handle);\n\t}\n\ttmp = vmw_resource_reference(&srf->res);\n\tret = ttm_prime_object_init(tfile, res->guest_memory_size, &user_srf->prime,\n\t\t\t\t    req->shareable, VMW_RES_SURFACE,\n\t\t\t\t    &vmw_user_surface_base_release);\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&tmp);\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\trep->sid = user_srf->prime.base.handle;\n\tvmw_resource_unreference(&res);\n\treturn 0;\nout_no_copy:\n\tkfree(srf->offsets);\nout_no_offsets:\n\tkfree(metadata->sizes);\nout_no_sizes:\n\tttm_prime_object_kfree(user_srf, prime);\nout_unlock:\n\treturn ret;\n}",
        "bug_line_number": [
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            24,
            25,
            26,
            27,
            28,
            29
        ],
        "vul": 1,
        "id": 128
    },
    {
        "code": "static int vmw_create_bo_proxy(struct drm_device *dev,\n\t\t\t       const struct drm_mode_fb_cmd2 *mode_cmd,\n\t\t\t       struct vmw_bo *bo_mob,\n\t\t\t       struct vmw_surface **srf_out)\n{\n\tstruct vmw_surface_metadata metadata = {0};\n\tuint32_t format;\n\tstruct vmw_resource *res;\n\tunsigned int bytes_pp;\n\tint ret;\n\tswitch (mode_cmd->pixel_format) {\n\tcase DRM_FORMAT_ARGB8888:\n\tcase DRM_FORMAT_XRGB8888:\n\t\tformat = SVGA3D_X8R8G8B8;\n\t\tbytes_pp = 4;\n\t\tbreak;\n\tcase DRM_FORMAT_RGB565:\n\tcase DRM_FORMAT_XRGB1555:\n\t\tformat = SVGA3D_R5G6B5;\n\t\tbytes_pp = 2;\n\t\tbreak;\n\tcase 8:\n\t\tformat = SVGA3D_P8;\n\t\tbytes_pp = 1;\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Invalid framebuffer format %p4cc\\n\",\n\t\t\t  &mode_cmd->pixel_format);\n\t\treturn -EINVAL;\n\t}\n\tmetadata.format = format;\n\tmetadata.mip_levels[0] = 1;\n\tmetadata.num_sizes = 1;\n\tmetadata.base_size.width = mode_cmd->pitches[0] / bytes_pp;\n\tmetadata.base_size.height =  mode_cmd->height;\n\tmetadata.base_size.depth = 1;\n\tmetadata.scanout = true;\n\tret = vmw_gb_surface_define(vmw_priv(dev), &metadata, srf_out);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed to allocate proxy content buffer\\n\");\n\t\treturn ret;\n\t}\n\tres = &(*srf_out)->res;\n\t/* Reserve and switch the backing mob. */\n\tmutex_lock(&res->dev_priv->cmdbuf_mutex);\n\t(void) vmw_resource_reserve(res, false, true);\n\tvmw_bo_unreference(&res->guest_memory_bo);\n\tres->guest_memory_bo = vmw_bo_reference(bo_mob);\n\tres->guest_memory_offset = 0;\n\tvmw_resource_unreserve(res, false, false, false, NULL, 0);\n\tmutex_unlock(&res->dev_priv->cmdbuf_mutex);\n\treturn 0;\n}",
        "bug_line_number": [
            46,
            47,
            48
        ],
        "vul": 1,
        "id": 130
    },
    {
        "code": "static irqreturn_t sunkbd_interrupt(struct serio *serio,\n\t\tunsigned char data, unsigned int flags)\n{\n\tstruct sunkbd *sunkbd = serio_get_drvdata(serio);\n\tif (sunkbd->reset <= -1) {\n\t\t/*\n\t\t * If cp[i] is 0xff, sunkbd->reset will stay -1.\n\t\t * The keyboard sends 0xff 0xff 0xID on powerup.\n\t\t */\n\t\tsunkbd->reset = data;\n\t\twake_up_interruptible(&sunkbd->wait);\n\t\tgoto out;\n\t}\n\tif (sunkbd->layout == -1) {\n\t\tsunkbd->layout = data;\n\t\twake_up_interruptible(&sunkbd->wait);\n\t\tgoto out;\n\t}\n\tswitch (data) {\n\tcase SUNKBD_RET_RESET:\n\t\tschedule_work(&sunkbd->tq);\n\t\tsunkbd->reset = -1;\n\t\tbreak;\n\tcase SUNKBD_RET_LAYOUT:\n\t\tsunkbd->layout = -1;\n\t\tbreak;\n\tcase SUNKBD_RET_ALLUP: /* All keys released */\n\t\tbreak;\n\tdefault:\n\t\tif (!sunkbd->enabled)\n\t\t\tbreak;\n\t\tif (sunkbd->keycode[data & SUNKBD_KEY]) {\n\t\t\tinput_report_key(sunkbd->dev,\n\t\t\t\t\t sunkbd->keycode[data & SUNKBD_KEY],\n\t\t\t\t\t !(data & SUNKBD_RELEASE));\n\t\t\tinput_sync(sunkbd->dev);\n\t\t} else {\n\t\t\tprintk(KERN_WARNING\n\t\t\t\t\"sunkbd.c: Unknown key (scancode %#x) %s.\\n\",\n\t\t\t\tdata & SUNKBD_KEY,\n\t\t\t\tdata & SUNKBD_RELEASE ? \"released\" : \"pressed\");\n\t\t}\n\t}\nout:\n\treturn IRQ_HANDLED;\n}",
        "bug_line_number": [
            20,
            21
        ],
        "vul": 1,
        "id": 132
    },
    {
        "code": "void usb_sg_cancel(struct usb_sg_request *io)\n{\n\tunsigned long flags;\n\tint i, retval;\n\tspin_lock_irqsave(&io->lock, flags);\n\tif (io->status) {\n\t\tspin_unlock_irqrestore(&io->lock, flags);\n\t\treturn;\n\t}\n\t/* shut everything down */\n\tio->status = -ECONNRESET;\n\tspin_unlock_irqrestore(&io->lock, flags);\n\tfor (i = io->entries - 1; i >= 0; --i) {\n\t\tusb_block_urb(io->urbs[i]);\n\t\tretval = usb_unlink_urb(io->urbs[i]);\n\t\tif (retval != -EINPROGRESS\n\t\t    && retval != -ENODEV\n\t\t    && retval != -EBUSY\n\t\t    && retval != -EIDRM)\n\t\t\tdev_warn(&io->dev->dev, \"%s, unlink --> %d\\n\",\n\t\t\t\t __func__, retval);\n\t}\n}",
        "bug_line_number": [
            5,
            6,
            10,
            11,
            21,
            22
        ],
        "vul": 1,
        "id": 134
    },
    {
        "code": "static int xillyusb_open(struct inode *inode, struct file *filp)\n{\n\tstruct xillyusb_dev *xdev;\n\tstruct xillyusb_channel *chan;\n\tstruct xillyfifo *in_fifo = NULL;\n\tstruct xillyusb_endpoint *out_ep = NULL;\n\tint rc;\n\tint index;\n\trc = xillybus_find_inode(inode, (void **)&xdev, &index);\n\tif (rc)\n\t\treturn rc;\n\tchan = &xdev->channels[index];\n\tfilp->private_data = chan;\n\tmutex_lock(&chan->lock);\n\trc = -ENODEV;\n\tif (xdev->error)\n\t\tgoto unmutex_fail;\n\tif (((filp->f_mode & FMODE_READ) && !chan->readable) ||\n\t    ((filp->f_mode & FMODE_WRITE) && !chan->writable))\n\t\tgoto unmutex_fail;\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_READ) &&\n\t    chan->in_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for read on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_WRITE) &&\n\t    chan->out_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for write on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\trc = -EBUSY;\n\tif (((filp->f_mode & FMODE_READ) && chan->open_for_read) ||\n\t    ((filp->f_mode & FMODE_WRITE) && chan->open_for_write))\n\t\tgoto unmutex_fail;\n\tkref_get(&xdev->kref);\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 1;\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 1;\n\tmutex_unlock(&chan->lock);\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tout_ep = endpoint_alloc(xdev,\n\t\t\t\t\t(chan->chan_idx + 2) | USB_DIR_OUT,\n\t\t\t\t\tbulk_out_work, BUF_SIZE_ORDER, BUFNUM);\n\t\tif (!out_ep) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto unopen;\n\t\t}\n\t\trc = fifo_init(&out_ep->fifo, chan->out_log2_fifo_size);\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\t\tout_ep->fill_mask = -(1 << chan->out_log2_element_size);\n\t\tchan->out_bytes = 0;\n\t\tchan->flushed = 0;\n\t\t/*\n\t\t * Sending a flush request to a previously closed stream\n\t\t * effectively opens it, and also waits until the command is\n\t\t * confirmed by the FPGA. The latter is necessary because the\n\t\t * data is sent through a separate BULK OUT endpoint, and the\n\t\t * xHCI controller is free to reorder transmissions.\n\t\t *\n\t\t * This can't go wrong unless there's a serious hardware error\n\t\t * (or the computer is stuck for 500 ms?)\n\t\t */\n\t\trc = flush_downstream(chan, XILLY_RESPONSE_TIMEOUT, false);\n\t\tif (rc == -ETIMEDOUT) {\n\t\t\trc = -EIO;\n\t\t\treport_io_error(xdev, rc);\n\t\t}\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\t}\n\tif (filp->f_mode & FMODE_READ) {\n\t\tin_fifo = kzalloc(sizeof(*in_fifo), GFP_KERNEL);\n\t\tif (!in_fifo) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto late_unopen;\n\t\t}\n\t\trc = fifo_init(in_fifo, chan->in_log2_fifo_size);\n\t\tif (rc) {\n\t\t\tkfree(in_fifo);\n\t\t\tgoto late_unopen;\n\t\t}\n\t}\n\tmutex_lock(&chan->lock);\n\tif (in_fifo) {\n\t\tchan->in_fifo = in_fifo;\n\t\tchan->read_data_ok = 1;\n\t}\n\tif (out_ep)\n\t\tchan->out_ep = out_ep;",
        "bug_line_number": [
            7,
            8,
            9,
            10,
            11,
            36
        ],
        "vul": 1,
        "id": 136
    },
    {
        "code": "\tmutex_unlock(&chan->lock);\n\tif (in_fifo) {\n\t\tu32 in_checkpoint = 0;\n\t\tif (!chan->in_synchronous)\n\t\t\tin_checkpoint = in_fifo->size >>\n\t\t\t\tchan->in_log2_element_size;\n\t\tchan->in_consumed_bytes = 0;\n\t\tchan->poll_used = 0;\n\t\tchan->in_current_checkpoint = in_checkpoint;\n\t\trc = xillyusb_send_opcode(xdev, (chan->chan_idx << 1) | 1,\n\t\t\t\t\t  OPCODE_SET_CHECKPOINT,\n\t\t\t\t\t  in_checkpoint);\n\t\tif (rc) /* Failure guarantees that opcode wasn't sent */\n\t\t\tgoto unfifo;\n\t\t/*\n\t\t * In non-blocking mode, request the FPGA to send any data it\n\t\t * has right away. Otherwise, the first read() will always\n\t\t * return -EAGAIN, which is OK strictly speaking, but ugly.\n\t\t * Checking and unrolling if this fails isn't worth the\n\t\t * effort -- the error is propagated to the first read()\n\t\t * anyhow.\n\t\t */\n\t\tif (filp->f_flags & O_NONBLOCK)\n\t\t\trequest_read_anything(chan, OPCODE_SET_PUSH);\n\t}\n\treturn 0;\nunfifo:\n\tchan->read_data_ok = 0;\n\tsafely_assign_in_fifo(chan, NULL);\n\tfifo_mem_release(in_fifo);\n\tkfree(in_fifo);\n\tif (out_ep) {\n\t\tmutex_lock(&chan->lock);\n\t\tchan->out_ep = NULL;\n\t\tmutex_unlock(&chan->lock);\n\t}\nlate_unopen:\n\tif (out_ep)\n\t\tendpoint_dealloc(out_ep);\nunopen:\n\tmutex_lock(&chan->lock);\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 0;\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 0;\n\tmutex_unlock(&chan->lock);\n\tkref_put(&xdev->kref, cleanup_dev);\n\treturn rc;\nunmutex_fail:\n\tmutex_unlock(&chan->lock);\n\treturn rc;\n}",
        "bug_line_number": [
            48,
            49
        ],
        "vul": 1,
        "id": 136
    },
    {
        "code": "void ext4_es_insert_delayed_block(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t\t  bool allocated)\n{\n\tstruct extent_status newes;\n\tint err1 = 0;\n\tint err2 = 0;\n\tstruct extent_status *es1 = NULL;\n\tstruct extent_status *es2 = NULL;\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\tes_debug(\"add [%u/1) delayed to extent status tree of inode %lu\\n\",\n\t\t lblk, inode->i_ino);\n\tnewes.es_lblk = lblk;\n\tnewes.es_len = 1;\n\text4_es_store_pblock_status(&newes, ~0, EXTENT_STATUS_DELAYED);\n\ttrace_ext4_es_insert_delayed_block(inode, &newes, allocated);\n\text4_es_insert_extent_check(inode, &newes);\nretry:\n\tif (err1 && !es1)\n\t\tes1 = __es_alloc_extent(true);\n\tif ((err1 || err2) && !es2)\n\t\tes2 = __es_alloc_extent(true);\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr1 = __es_remove_extent(inode, lblk, lblk, NULL, es1);\n\tif (err1 != 0)\n\t\tgoto error;\n\terr2 = __es_insert_extent(inode, &newes, es2);\n\tif (err2 != 0)\n\t\tgoto error;\n\tif (allocated)\n\t\t__insert_pending(inode, lblk);\n\t/* es is pre-allocated but not used, free it. */\n\tif (es1 && !es1->es_len)\n\t\t__es_free_extent(es1);\n\tif (es2 && !es2->es_len)\n\t\t__es_free_extent(es2);\nerror:\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err1 || err2)\n\t\tgoto retry;\n\text4_es_print_tree(inode);\n\text4_print_pending_tree(inode);\n\treturn;\n}",
        "bug_line_number": [
            25,
            26,
            28,
            29,
            31,
            32,
            33,
            34,
            35
        ],
        "vul": 1,
        "id": 138
    },
    {
        "code": "void ext4_es_insert_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len, ext4_fsblk_t pblk,\n\t\t\t   unsigned int status)\n{\n\tstruct extent_status newes;\n\text4_lblk_t end = lblk + len - 1;\n\tint err1 = 0;\n\tint err2 = 0;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct extent_status *es1 = NULL;\n\tstruct extent_status *es2 = NULL;\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\tes_debug(\"add [%u/%u) %llu %x to extent status tree of inode %lu\\n\",\n\t\t lblk, len, pblk, status, inode->i_ino);\n\tif (!len)\n\t\treturn;\n\tBUG_ON(end < lblk);\n\tif ((status & EXTENT_STATUS_DELAYED) &&\n\t    (status & EXTENT_STATUS_WRITTEN)) {\n\t\text4_warning(inode->i_sb, \"Inserting extent [%u/%u] as \"\n\t\t\t\t\" delayed and written which can potentially \"\n\t\t\t\t\" cause data loss.\", lblk, len);\n\t\tWARN_ON(1);\n\t}\n\tnewes.es_lblk = lblk;\n\tnewes.es_len = len;\n\text4_es_store_pblock_status(&newes, pblk, status);\n\ttrace_ext4_es_insert_extent(inode, &newes);\n\text4_es_insert_extent_check(inode, &newes);\nretry:\n\tif (err1 && !es1)\n\t\tes1 = __es_alloc_extent(true);\n\tif ((err1 || err2) && !es2)\n\t\tes2 = __es_alloc_extent(true);\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr1 = __es_remove_extent(inode, lblk, end, NULL, es1);\n\tif (err1 != 0)\n\t\tgoto error;\n\terr2 = __es_insert_extent(inode, &newes, es2);\n\tif (err2 == -ENOMEM && !ext4_es_must_keep(&newes))\n\t\terr2 = 0;\n\tif (err2 != 0)\n\t\tgoto error;\n\tif (sbi->s_cluster_ratio > 1 && test_opt(inode->i_sb, DELALLOC) &&\n\t    (status & EXTENT_STATUS_WRITTEN ||\n\t     status & EXTENT_STATUS_UNWRITTEN))\n\t\t__revise_pending(inode, lblk, len);\n\t/* es is pre-allocated but not used, free it. */\n\tif (es1 && !es1->es_len)\n\t\t__es_free_extent(es1);\n\tif (es2 && !es2->es_len)\n\t\t__es_free_extent(es2);\nerror:\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err1 || err2)\n\t\tgoto retry;\n\text4_es_print_tree(inode);\n\treturn;\n}",
        "bug_line_number": [
            38,
            39,
            43,
            44,
            48,
            49,
            50,
            51,
            52
        ],
        "vul": 1,
        "id": 140
    },
    {
        "code": "static int em_fxrstor(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct fxregs_state fx_state;\n\tint rc;\n\trc = check_fxsr(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = segmented_read(ctxt, ctxt->memop.addr.mem, &fx_state, 512);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\tif (fx_state.mxcsr >> 16)\n\t\treturn emulate_gp(ctxt, 0);\n\tctxt->ops->get_fpu(ctxt);\n\tif (ctxt->mode < X86EMUL_MODE_PROT64)\n\t\trc = fxrstor_fixup(ctxt, &fx_state);\n\tif (rc == X86EMUL_CONTINUE)\n\t\trc = asm_safe(\"fxrstor %[fx]\", : [fx] \"m\"(fx_state));\n\tctxt->ops->put_fpu(ctxt);\n\treturn rc;\n}",
        "bug_line_number": [
            7,
            8
        ],
        "vul": 1,
        "id": 142
    },
    {
        "code": "int ttm_sg_tt_init(struct ttm_dma_tt *ttm_dma, struct ttm_buffer_object *bo,\n\t\t   uint32_t page_flags)\n{\n\tstruct ttm_tt *ttm = &ttm_dma->ttm;\n\tint ret;\n\tttm_tt_init_fields(ttm, bo, page_flags);\n\tINIT_LIST_HEAD(&ttm_dma->pages_list);\n\tif (page_flags & TTM_PAGE_FLAG_SG)\n\t\tret = ttm_sg_tt_alloc_page_directory(ttm_dma);\n\telse\n\t\tret = ttm_dma_tt_alloc_page_directory(ttm_dma);\n\tif (ret) {\n\t\tttm_tt_destroy(ttm);\n\t\tpr_err(\"Failed allocating page table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}",
        "bug_line_number": [
            12
        ],
        "vul": 1,
        "id": 144
    },
    {
        "code": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tevtchn_to_irq[row][col] = -1;\n}",
        "bug_line_number": [
            4,
            5
        ],
        "vul": 1,
        "id": 146
    },
    {
        "code": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\tif (WARN_ON(!info))\n\t\treturn;\n\tlist_del(&info->list);\n\tset_info_for_irq(irq, NULL);\n\tWARN_ON(info->refcnt > 0);\n\tkfree(info);\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\tirq_free_desc(irq);\n}",
        "bug_line_number": [
            2,
            3,
            4,
            5,
            7,
            8
        ],
        "vul": 1,
        "id": 148
    },
    {
        "code": "static void\nbinder_free_buf(struct binder_proc *proc, struct binder_buffer *buffer)\n{\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, buffer, 0, false);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
        "bug_line_number": [
            2,
            3,
            6,
            7
        ],
        "vul": 1,
        "id": 150
    },
    {
        "code": "static ssize_t ucma_migrate_id(struct ucma_file *new_file,\n\t\t\t       const char __user *inbuf,\n\t\t\t       int in_len, int out_len)\n{\n\tstruct rdma_ucm_migrate_id cmd;\n\tstruct rdma_ucm_migrate_resp resp;\n\tstruct ucma_context *ctx;\n\tstruct fd f;\n\tstruct ucma_file *cur_file;\n\tint ret = 0;\n\tif (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n\t\treturn -EFAULT;\n\t/* Get current fd to protect against it being closed */\n\tf = fdget(cmd.fd);\n\tif (!f.file)\n\t\treturn -ENOENT;\n\tif (f.file->f_op != &ucma_fops) {\n\t\tret = -EINVAL;\n\t\tgoto file_put;\n\t}\n\t/* Validate current fd and prevent destruction of id. */\n\tctx = ucma_get_ctx(f.file->private_data, cmd.id);\n\tif (IS_ERR(ctx)) {\n\t\tret = PTR_ERR(ctx);\n\t\tgoto file_put;\n\t}\n\trdma_lock_handler(ctx->cm_id);\n\tcur_file = ctx->file;\n\tif (cur_file == new_file) {\n\t\tmutex_lock(&cur_file->mut);\n\t\tresp.events_reported = ctx->events_reported;\n\t\tmutex_unlock(&cur_file->mut);\n\t\tgoto response;\n\t}\n\t/*\n\t * Migrate events between fd's, maintaining order, and avoiding new\n\t * events being added before existing events.\n\t */\n\tucma_lock_files(cur_file, new_file);\n\txa_lock(&ctx_table);\n\tlist_move_tail(&ctx->list, &new_file->ctx_list);\n\tucma_move_events(ctx, new_file);\n\tctx->file = new_file;\n\tresp.events_reported = ctx->events_reported;\n\txa_unlock(&ctx_table);\n\tucma_unlock_files(cur_file, new_file);\nresponse:\n\tif (copy_to_user(u64_to_user_ptr(cmd.response),\n\t\t\t &resp, sizeof(resp)))\n\t\tret = -EFAULT;\n\trdma_unlock_handler(ctx->cm_id);\n\tucma_put_ctx(ctx);\nfile_put:\n\tfdput(f);\n\treturn ret;\n}",
        "bug_line_number": [
            5,
            6,
            7,
            19,
            20,
            21,
            22,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            49,
            50
        ],
        "vul": 1,
        "id": 152
    },
    {
        "code": "static void ml_ff_destroy(struct ff_device *ff)\n{\n\tstruct ml_device *ml = ff->private;\n\tkfree(ml->private);\n}",
        "bug_line_number": [
            2,
            3
        ],
        "vul": 1,
        "id": 154
    },
    {
        "code": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\tmemset(&req->work, 0, sizeof(req->work));\n\treq->flags |= REQ_F_WORK_INITIALIZED;\n\treq->work.identity = &req->identity;\n}",
        "bug_line_number": [
            5,
            6
        ],
        "vul": 1,
        "id": 156
    },
    {
        "code": "static int tcp_v6_send_synack(const struct sock *sk, struct dst_entry *dst,\n\t\t\t      struct flowi *fl,\n\t\t\t      struct request_sock *req,\n\t\t\t      struct tcp_fastopen_cookie *foc,\n\t\t\t      bool attach_req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 *fl6 = &fl->u.ip6;\n\tstruct sk_buff *skb;\n\tint err = -ENOMEM;\n\t/* First, grab a route. */\n\tif (!dst && (dst = inet6_csk_route_req(sk, fl6, req,\n\t\t\t\t\t       IPPROTO_TCP)) == NULL)\n\t\tgoto done;\n\tskb = tcp_make_synack(sk, dst, req, foc, attach_req);\n\tif (skb) {\n\t\t__tcp_v6_send_check(skb, &ireq->ir_v6_loc_addr,\n\t\t\t\t    &ireq->ir_v6_rmt_addr);\n\t\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\t\tif (np->repflow && ireq->pktopts)\n\t\t\tfl6->flowlabel = ip6_flowlabel(ipv6_hdr(ireq->pktopts));\n\t\terr = ip6_xmit(sk, skb, fl6, np->opt, np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\ndone:\n\treturn err;\n}",
        "bug_line_number": [
            22,
            23
        ],
        "vul": 1,
        "id": 158
    },
    {
        "code": "static bool io_match_task(struct io_kiocb *head, struct task_struct *task,\n\t\t\t  bool cancel_all)\n\t__must_hold(&req->ctx->timeout_lock)\n{\n\tif (task && head->task != task)\n\t\treturn false;\n\treturn cancel_all;\n}",
        "bug_line_number": [
            3,
            4,
            6,
            7
        ],
        "vul": 1,
        "id": 160
    },
    {
        "code": "static s64 tctx_inflight(struct io_uring_task *tctx, bool tracked)\n{\n\tif (tracked)\n\t\treturn 0;\n\treturn percpu_counter_sum(&tctx->inflight);\n}",
        "bug_line_number": [
            3,
            4
        ],
        "vul": 1,
        "id": 162
    },
    {
        "code": "static struct file *io_file_get_normal(struct io_kiocb *req, int fd)\n{\n\tstruct file *file = fget(fd);\n\ttrace_io_uring_file_get(req->ctx, req, req->cqe.user_data, fd);\n\t/* we don't allow fixed io_uring files */\n\tif (file && file->f_op == &io_uring_fops)\n\t\treq->flags |= REQ_F_INFLIGHT;\n\treturn file;\n}",
        "bug_line_number": [
            6,
            7
        ],
        "vul": 1,
        "id": 164
    },
    {
        "code": "static bool io_match_task_safe(struct io_kiocb *head, struct task_struct *task,\n\t\t\t       bool cancel_all)\n{\n\tif (task && head->task != task)\n\t\treturn false;\n\treturn cancel_all;\n}",
        "bug_line_number": [
            2,
            3,
            5,
            6
        ],
        "vul": 1,
        "id": 166
    },
    {
        "code": "static void __io_req_task_work_add(struct io_kiocb *req,\n\t\t\t\t   struct io_uring_task *tctx,\n\t\t\t\t   struct io_wq_work_list *list)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_wq_work_node *node;\n\tunsigned long flags;\n\tbool running;\n\tio_drop_inflight_file(req);\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\twq_list_add_tail(&req->io_task_work.node, list);\n\trunning = tctx->task_running;\n\tif (!running)\n\t\ttctx->task_running = true;\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\t/* task_work already pending, we're done */\n\tif (running)\n\t\treturn;\n\tif (ctx->flags & IORING_SETUP_TASKRUN_FLAG)\n\t\tatomic_or(IORING_SQ_TASKRUN, &ctx->rings->sq_flags);\n\tif (likely(!task_work_add(req->task, &tctx->task_work, ctx->notify_method)))\n\t\treturn;\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\ttctx->task_running = false;\n\tnode = wq_list_merge(&tctx->prio_task_list, &tctx->task_list);\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\twhile (node) {\n\t\treq = container_of(node, struct io_kiocb, io_task_work.node);\n\t\tnode = node->next;\n\t\tif (llist_add(&req->io_task_work.fallback_node,\n\t\t\t      &req->ctx->fallback_llist))\n\t\t\tschedule_delayed_work(&req->ctx->fallback_work, 1);\n\t}\n}",
        "bug_line_number": [
            8
        ],
        "vul": 1,
        "id": 168
    },
    {
        "code": "static int binder_thread_release(struct binder_proc *proc,\n\t\t\t\t struct binder_thread *thread)\n{\n\tstruct binder_transaction *t;\n\tstruct binder_transaction *send_reply = NULL;\n\tint active_transactions = 0;\n\tstruct binder_transaction *last_t = NULL;\n\tbinder_inner_proc_lock(thread->proc);\n\t/*\n\t * take a ref on the proc so it survives\n\t * after we remove this thread from proc->threads.\n\t * The corresponding dec is when we actually\n\t * free the thread in binder_free_thread()\n\t */\n\tproc->tmp_ref++;\n\t/*\n\t * take a ref on this thread to ensure it\n\t * survives while we are releasing it\n\t */\n\tatomic_inc(&thread->tmp_ref);\n\trb_erase(&thread->rb_node, &proc->threads);\n\tt = thread->transaction_stack;\n\tif (t) {\n\t\tspin_lock(&t->lock);\n\t\tif (t->to_thread == thread)\n\t\t\tsend_reply = t;\n\t}\n\tthread->is_dead = true;\n\twhile (t) {\n\t\tlast_t = t;\n\t\tactive_transactions++;\n\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t     \"release %d:%d transaction %d %s, still active\\n\",\n\t\t\t      proc->pid, thread->pid,\n\t\t\t     t->debug_id,\n\t\t\t     (t->to_thread == thread) ? \"in\" : \"out\");\n\t\tif (t->to_thread == thread) {\n\t\t\tt->to_proc = NULL;\n\t\t\tt->to_thread = NULL;\n\t\t\tif (t->buffer) {\n\t\t\t\tt->buffer->transaction = NULL;\n\t\t\t\tt->buffer = NULL;\n\t\t\t}\n\t\t\tt = t->to_parent;\n\t\t} else if (t->from == thread) {\n\t\t\tt->from = NULL;\n\t\t\tt = t->from_parent;\n\t\t} else\n\t\t\tBUG();\n\t\tspin_unlock(&last_t->lock);\n\t\tif (t)\n\t\t\tspin_lock(&t->lock);\n\t}\n\t/*\n\t * If this thread used poll, make sure we remove the waitqueue\n\t * from any epoll data structures holding it with POLLFREE.\n\t * waitqueue_active() is safe to use here because we're holding\n\t * the inner lock.\n\t */\n\tif ((thread->looper & BINDER_LOOPER_STATE_POLL) &&\n\t    waitqueue_active(&thread->wait)) {\n\t\twake_up_poll(&thread->wait, EPOLLHUP | POLLFREE);\n\t}\n\tbinder_inner_proc_unlock(thread->proc);\n\tif (send_reply)\n\t\tbinder_send_failed_reply(send_reply, BR_DEAD_REPLY);\n\tbinder_release_work(proc, &thread->todo);\n\tbinder_thread_dec_tmpref(thread);\n\treturn active_transactions;\n}",
        "bug_line_number": [
            63,
            64
        ],
        "vul": 1,
        "id": 170
    },
    {
        "code": "static void smp_task_done(struct sas_task *task)\n{\n\tif (!del_timer(&task->slow_task->timer))\n\t\treturn;\n\tcomplete(&task->slow_task->completion);\n}",
        "bug_line_number": [
            2,
            3,
            4
        ],
        "vul": 1,
        "id": 172
    },
    {
        "code": "int io_poll_add(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_poll *poll = io_kiocb_to_cmd(req);\n\tstruct io_poll_table ipt;\n\tint ret;\n\tipt.pt._qproc = io_poll_queue_proc;\n\tret = __io_arm_poll_handler(req, poll, &ipt, poll->events);\n\tif (ret) {\n\t\tio_req_set_res(req, ret, 0);\n\t\treturn IOU_OK;\n\t}\n\tif (ipt.error) {\n\t\treq_set_fail(req);\n\t\treturn ipt.error;\n\t}\n\treturn IOU_ISSUE_SKIP_COMPLETE;\n}",
        "bug_line_number": [
            5,
            6
        ],
        "vul": 1,
        "id": 174
    },
    {
        "code": "static __cold void io_ring_ctx_free(struct io_ring_ctx *ctx)\n{\n\tio_sq_thread_finish(ctx);\n\tif (ctx->mm_account) {\n\t\tmmdrop(ctx->mm_account);\n\t\tctx->mm_account = NULL;\n\t}\n\tio_rsrc_refs_drop(ctx);\n\t/* __io_rsrc_put_work() may need uring_lock to progress, wait w/o it */\n\tio_wait_rsrc_data(ctx->buf_data);\n\tio_wait_rsrc_data(ctx->file_data);\n\tmutex_lock(&ctx->uring_lock);\n\tif (ctx->buf_data)\n\t\t__io_sqe_buffers_unregister(ctx);\n\tif (ctx->file_data)\n\t\t__io_sqe_files_unregister(ctx);\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true);\n\tio_eventfd_unregister(ctx);\n\tio_flush_apoll_cache(ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\tio_destroy_buffers(ctx);\n\tif (ctx->sq_creds)\n\t\tput_cred(ctx->sq_creds);\n\tif (ctx->submitter_task)\n\t\tput_task_struct(ctx->submitter_task);\n\t/* there are no registered resources left, nobody uses it */\n\tif (ctx->rsrc_node)\n\t\tio_rsrc_node_destroy(ctx->rsrc_node);\n\tif (ctx->rsrc_backup_node)\n\t\tio_rsrc_node_destroy(ctx->rsrc_backup_node);\n\tflush_delayed_work(&ctx->rsrc_put_work);\n\tflush_delayed_work(&ctx->fallback_work);\n\tWARN_ON_ONCE(!list_empty(&ctx->rsrc_ref_list));\n\tWARN_ON_ONCE(!llist_empty(&ctx->rsrc_put_llist));\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tctx->ring_sock->file = NULL; /* so that iput() is called */\n\t\tsock_release(ctx->ring_sock);\n\t}\n#endif\n\tWARN_ON_ONCE(!list_empty(&ctx->ltimeout_list));\n\tio_mem_free(ctx->rings);\n\tio_mem_free(ctx->sq_sqes);\n\tpercpu_ref_exit(&ctx->refs);\n\tfree_uid(ctx->user);\n\tio_req_caches_free(ctx);\n\tif (ctx->hash_map)\n\t\tio_wq_put_hash(ctx->hash_map);\n\tkfree(ctx->cancel_table.hbs);\n\tkfree(ctx->dummy_ubuf);\n\tkfree(ctx->io_bl);\n\txa_destroy(&ctx->io_bl_xa);\n\tkfree(ctx);\n}",
        "bug_line_number": [
            49,
            50
        ],
        "vul": 1,
        "id": 176
    },
    {
        "code": "static int __io_arm_poll_handler(struct io_kiocb *req,\n\t\t\t\t struct io_poll *poll,\n\t\t\t\t struct io_poll_table *ipt, __poll_t mask)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint v;\n\tINIT_HLIST_NODE(&req->hash_node);\n\treq->work.cancel_seq = atomic_read(&ctx->cancel_seq);\n\tio_init_poll_iocb(poll, mask, io_poll_wake);\n\tpoll->file = req->file;\n\treq->apoll_events = poll->events;\n\tipt->pt._key = mask;\n\tipt->req = req;\n\tipt->error = 0;\n\tipt->nr_entries = 0;\n\t/*\n\t * Take the ownership to delay any tw execution up until we're done\n\t * with poll arming. see io_poll_get_ownership().\n\t */\n\tatomic_set(&req->poll_refs, 1);\n\tmask = vfs_poll(req->file, &ipt->pt) & poll->events;\n\tif (mask &&\n\t   ((poll->events & (EPOLLET|EPOLLONESHOT)) == (EPOLLET|EPOLLONESHOT))) {\n\t\tio_poll_remove_entries(req);\n\t\t/* no one else has access to the req, forget about the ref */\n\t\treturn mask;\n\t}\n\tif (!mask && unlikely(ipt->error || !ipt->nr_entries)) {\n\t\tio_poll_remove_entries(req);\n\t\tif (!ipt->error)\n\t\t\tipt->error = -EINVAL;\n\t\treturn 0;\n\t}\n\tio_poll_req_insert(req);\n\tif (mask && (poll->events & EPOLLET)) {\n\t\t/* can't multishot if failed, just queue the event we've got */\n\t\tif (unlikely(ipt->error || !ipt->nr_entries)) {\n\t\t\tpoll->events |= EPOLLONESHOT;\n\t\t\treq->apoll_events |= EPOLLONESHOT;\n\t\t\tipt->error = 0;\n\t\t}\n\t\t__io_poll_execute(req, mask, poll->events);\n\t\treturn 0;\n\t}\n\t/*\n\t * Release ownership. If someone tried to queue a tw while it was\n\t * locked, kick it off for them.\n\t */\n\tv = atomic_dec_return(&req->poll_refs);\n\tif (unlikely(v & IO_POLL_REF_MASK))\n\t\t__io_poll_execute(req, 0, poll->events);\n\treturn 0;\n}",
        "bug_line_number": [
            33,
            34
        ],
        "vul": 1,
        "id": 178
    },
    {
        "code": "static int log_read_rst(struct ntfs_log *log, u32 l_size, bool first,\n\t\t\tstruct restart_info *info)\n{\n\tu32 skip, vbo;\n\tstruct RESTART_HDR *r_page = kmalloc(DefaultLogPageSize, GFP_NOFS);\n\tif (!r_page)\n\t\treturn -ENOMEM;\n\tmemset(info, 0, sizeof(struct restart_info));\n\t/* Determine which restart area we are looking for. */\n\tif (first) {\n\t\tvbo = 0;\n\t\tskip = 512;\n\t} else {\n\t\tvbo = 512;\n\t\tskip = 0;\n\t}\n\t/* Loop continuously until we succeed. */\n\tfor (; vbo < l_size; vbo = 2 * vbo + skip, skip = 0) {\n\t\tbool usa_error;\n\t\tu32 sys_page_size;\n\t\tbool brst, bchk;\n\t\tstruct RESTART_AREA *ra;\n\t\t/* Read a page header at the current offset. */\n\t\tif (read_log_page(log, vbo, (struct RECORD_PAGE_HDR **)&r_page,\n\t\t\t\t  &usa_error)) {\n\t\t\t/* Ignore any errors. */\n\t\t\tcontinue;\n\t\t}\n\t\t/* Exit if the signature is a log record page. */\n\t\tif (r_page->rhdr.sign == NTFS_RCRD_SIGNATURE) {\n\t\t\tinfo->initialized = true;\n\t\t\tbreak;\n\t\t}\n\t\tbrst = r_page->rhdr.sign == NTFS_RSTR_SIGNATURE;\n\t\tbchk = r_page->rhdr.sign == NTFS_CHKD_SIGNATURE;\n\t\tif (!bchk && !brst) {\n\t\t\tif (r_page->rhdr.sign != NTFS_FFFF_SIGNATURE) {\n\t\t\t\t/*\n\t\t\t\t * Remember if the signature does not\n\t\t\t\t * indicate uninitialized file.\n\t\t\t\t */\n\t\t\t\tinfo->initialized = true;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tra = NULL;\n\t\tinfo->valid_page = false;\n\t\tinfo->initialized = true;\n\t\tinfo->vbo = vbo;\n\t\t/* Let's check the restart area if this is a valid page. */\n\t\tif (!is_rst_page_hdr_valid(vbo, r_page))\n\t\t\tgoto check_result;\n\t\tra = Add2Ptr(r_page, le16_to_cpu(r_page->ra_off));\n\t\tif (!is_rst_area_valid(r_page))\n\t\t\tgoto check_result;\n\t\t/*\n\t\t * We have a valid restart page header and restart area.\n\t\t * If chkdsk was run or we have no clients then we have\n\t\t * no more checking to do.\n\t\t */\n\t\tif (bchk || ra->client_idx[1] == LFS_NO_CLIENT_LE) {\n\t\t\tinfo->valid_page = true;\n\t\t\tgoto check_result;\n\t\t}\n\t\t/* Read the entire restart area. */\n\t\tsys_page_size = le32_to_cpu(r_page->sys_page_size);\n\t\tif (DefaultLogPageSize != sys_page_size) {\n\t\t\tkfree(r_page);\n\t\t\tr_page = kzalloc(sys_page_size, GFP_NOFS);\n\t\t\tif (!r_page)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (read_log_page(log, vbo,\n\t\t\t\t\t  (struct RECORD_PAGE_HDR **)&r_page,\n\t\t\t\t\t  &usa_error)) {\n\t\t\t\t/* Ignore any errors. */\n\t\t\t\tkfree(r_page);\n\t\t\t\tr_page = NULL;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (is_client_area_valid(r_page, usa_error)) {\n\t\t\tinfo->valid_page = true;\n\t\t\tra = Add2Ptr(r_page, le16_to_cpu(r_page->ra_off));\n\t\t}\ncheck_result:\n\t\t/*\n\t\t * If chkdsk was run then update the caller's\n\t\t * values and return.\n\t\t */\n\t\tif (r_page->rhdr.sign == NTFS_CHKD_SIGNATURE) {\n\t\t\tinfo->chkdsk_was_run = true;\n\t\t\tinfo->last_lsn = le64_to_cpu(r_page->rhdr.lsn);\n\t\t\tinfo->restart = true;\n\t\t\tinfo->r_page = r_page;\n\t\t\treturn 0;\n\t\t}\n\t\t/*\n\t\t * If we have a valid page then copy the values\n\t\t * we need from it.\n\t\t */\n\t\tif (info->valid_page) {\n\t\t\tinfo->last_lsn = le64_to_cpu(ra->current_lsn);\n\t\t\tinfo->restart = true;",
        "bug_line_number": [
            7
        ],
        "vul": 1,
        "id": 180
    },
    {
        "code": "static void redo_fd_request(void)\n{\n\tint drive;\n\tint tmp;\n\tlastredo = jiffies;\n\tif (current_drive < N_DRIVE)\n\t\tfloppy_off(current_drive);\ndo_request:\n\tif (!current_req) {\n\t\tint pending;\n\t\tspin_lock_irq(&floppy_lock);\n\t\tpending = set_next_request();\n\t\tspin_unlock_irq(&floppy_lock);\n\t\tif (!pending) {\n\t\t\tdo_floppy = NULL;\n\t\t\tunlock_fdc();\n\t\t\treturn;\n\t\t}\n\t}\n\tdrive = (long)current_req->q->disk->private_data;\n\tset_fdc(drive);\n\treschedule_timeout(current_drive, \"redo fd request\");\n\tset_floppy(drive);\n\traw_cmd = &default_raw_cmd;\n\traw_cmd->flags = 0;\n\tif (start_motor(redo_fd_request))\n\t\treturn;\n\tdisk_change(current_drive);\n\tif (test_bit(current_drive, &fake_change) ||\n\t    test_bit(FD_DISK_CHANGED_BIT, &drive_state[current_drive].flags)) {\n\t\tDPRINT(\"disk absent or changed during operation\\n\");\n\t\trequest_done(0);\n\t\tgoto do_request;\n\t}\n\tif (!_floppy) {\t/* Autodetection */\n\t\tif (!probing) {\n\t\t\tdrive_state[current_drive].probed_format = 0;\n\t\t\tif (next_valid_format(current_drive)) {\n\t\t\t\tDPRINT(\"no autodetectable formats\\n\");\n\t\t\t\t_floppy = NULL;\n\t\t\t\trequest_done(0);\n\t\t\t\tgoto do_request;\n\t\t\t}\n\t\t}\n\t\tprobing = 1;\n\t\t_floppy = floppy_type + drive_params[current_drive].autodetect[drive_state[current_drive].probed_format];\n\t} else\n\t\tprobing = 0;\n\terrors = &(current_req->error_count);\n\ttmp = make_raw_rw_request();\n\tif (tmp < 2) {\n\t\trequest_done(tmp);\n\t\tgoto do_request;\n\t}\n\tif (test_bit(FD_NEED_TWADDLE_BIT, &drive_state[current_drive].flags))\n\t\ttwaddle(current_fdc, current_drive);\n\tschedule_bh(floppy_start);\n\tdebugt(__func__, \"queue fd request\");\n\treturn;\n}",
        "bug_line_number": [
            48
        ],
        "vul": 1,
        "id": 182
    },
    {
        "code": "static loff_t find_tree_dqentry(struct qtree_mem_dqinfo *info,\n\t\t\t\tstruct dquot *dquot, uint blk, int depth)\n{\n\tchar *buf = kmalloc(info->dqi_usable_bs, GFP_NOFS);\n\tloff_t ret = 0;\n\t__le32 *ref = (__le32 *)buf;\n\tif (!buf)\n\t\treturn -ENOMEM;\n\tret = read_blk(info, blk, buf);\n\tif (ret < 0) {\n\t\tquota_error(dquot->dq_sb, \"Can't read quota tree block %u\",\n\t\t\t    blk);\n\t\tgoto out_buf;\n\t}\n\tret = 0;\n\tblk = le32_to_cpu(ref[get_index(info, dquot->dq_id, depth)]);\n\tif (!blk)\t/* No reference? */\n\t\tgoto out_buf;\n\tif (depth < info->dqi_qtree_depth - 1)\n\t\tret = find_tree_dqentry(info, dquot, blk, depth+1);\n\telse\n\t\tret = find_block_dqentry(info, dquot, blk);\nout_buf:\n\tkfree(buf);\n\treturn ret;\n}",
        "bug_line_number": [
            17,
            18
        ],
        "vul": 1,
        "id": 184
    },
    {
        "code": "struct psi_trigger *psi_trigger_create(struct psi_group *group,\n\t\t\tchar *buf, size_t nbytes, enum psi_res res)\n{\n\tstruct psi_trigger *t;\n\tenum psi_states state;\n\tu32 threshold_us;\n\tu32 window_us;\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\tif (sscanf(buf, \"some %u %u\", &threshold_us, &window_us) == 2)\n\t\tstate = PSI_IO_SOME + res * 2;\n\telse if (sscanf(buf, \"full %u %u\", &threshold_us, &window_us) == 2)\n\t\tstate = PSI_IO_FULL + res * 2;\n\telse\n\t\treturn ERR_PTR(-EINVAL);\n\tif (state >= PSI_NONIDLE)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (window_us < WINDOW_MIN_US ||\n\t\twindow_us > WINDOW_MAX_US)\n\t\treturn ERR_PTR(-EINVAL);\n\t/* Check threshold */\n\tif (threshold_us == 0 || threshold_us > window_us)\n\t\treturn ERR_PTR(-EINVAL);\n\tt = kmalloc(sizeof(*t), GFP_KERNEL);\n\tif (!t)\n\t\treturn ERR_PTR(-ENOMEM);\n\tt->group = group;\n\tt->state = state;\n\tt->threshold = threshold_us * NSEC_PER_USEC;\n\tt->win.size = window_us * NSEC_PER_USEC;\n\twindow_reset(&t->win, 0, 0, 0);\n\tt->event = 0;\n\tt->last_event_time = 0;\n\tinit_waitqueue_head(&t->event_wait);\n\tkref_init(&t->refcount);\n\tmutex_lock(&group->trigger_lock);\n\tif (!rcu_access_pointer(group->poll_task)) {\n\t\tstruct task_struct *task;\n\t\ttask = kthread_create(psi_poll_worker, group, \"psimon\");\n\t\tif (IS_ERR(task)) {\n\t\t\tkfree(t);\n\t\t\tmutex_unlock(&group->trigger_lock);\n\t\t\treturn ERR_CAST(task);\n\t\t}\n\t\tatomic_set(&group->poll_wakeup, 0);\n\t\twake_up_process(task);\n\t\trcu_assign_pointer(group->poll_task, task);\n\t}\n\tlist_add(&t->node, &group->triggers);\n\tgroup->poll_min_period = min(group->poll_min_period,\n\t\tdiv_u64(t->win.size, UPDATES_PER_WINDOW));\n\tgroup->nr_triggers[t->state]++;\n\tgroup->poll_states |= (1 << t->state);\n\tmutex_unlock(&group->trigger_lock);\n\treturn t;\n}",
        "bug_line_number": [
            34
        ],
        "vul": 1,
        "id": 186
    },
    {
        "code": "static void timerfd_remove_cancel(struct timerfd_ctx *ctx)\n{\n\tif (ctx->might_cancel) {\n\t\tctx->might_cancel = false;\n\t\tspin_lock(&cancel_lock);\n\t\tlist_del_rcu(&ctx->clist);\n\t\tspin_unlock(&cancel_lock);\n\t}\n}",
        "bug_line_number": [
            2,
            3,
            4,
            5,
            6,
            7,
            8
        ],
        "vul": 1,
        "id": 188
    },
    {
        "code": "static int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\thwc->state = !(flags & PERF_EF_START);\n\thead = find_swevent_head(swhash, event);\n\tif (!head) {\n\t\t/*\n\t\t * We can race with cpu hotplug code. Do not\n\t\t * WARN if the cpu just got unplugged.\n\t\t */\n\t\tWARN_ON_ONCE(swhash->online);\n\t\treturn -EINVAL;\n\t}\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\tperf_event_update_userpage(event);\n\treturn 0;\n}",
        "bug_line_number": [
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18
        ],
        "vul": 1,
        "id": 190
    },
    {
        "code": "static int binder_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tint ret;\n\tstruct binder_proc *proc = filp->private_data;\n\tconst char *failure_string;\n\tif (proc->tsk != current->group_leader)\n\t\treturn -EINVAL;\n\tif ((vma->vm_end - vma->vm_start) > SZ_4M)\n\t\tvma->vm_end = vma->vm_start + SZ_4M;\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d %lx-%lx (%ld K) vma %lx pagep %lx\\n\",\n\t\t     __func__, proc->pid, vma->vm_start, vma->vm_end,\n\t\t     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,\n\t\t     (unsigned long)pgprot_val(vma->vm_page_prot));\n\tif (vma->vm_flags & FORBIDDEN_MMAP_FLAGS) {\n\t\tret = -EPERM;\n\t\tfailure_string = \"bad vm_flags\";\n\t\tgoto err_bad_arg;\n\t}\n\tvma->vm_flags = (vma->vm_flags | VM_DONTCOPY) & ~VM_MAYWRITE;\n\tvma->vm_ops = &binder_vm_ops;\n\tvma->vm_private_data = proc;\n\tret = binder_alloc_mmap_handler(&proc->alloc, vma);\n\tif (ret)\n\t\treturn ret;\n\tproc->files = get_files_struct(current);\n\treturn 0;\nerr_bad_arg:\n\tpr_err(\"binder_mmap: %d %lx-%lx %s failed %d\\n\",\n\t       proc->pid, vma->vm_start, vma->vm_end, failure_string, ret);\n\treturn ret;\n}",
        "bug_line_number": [
            24,
            25,
            26
        ],
        "vul": 1,
        "id": 192
    },
    {
        "code": "static int binder_open(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc;\n\tstruct binder_device *binder_dev;\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE, \"binder_open: %d:%d\\n\",\n\t\t     current->group_leader->pid, current->pid);\n\tproc = kzalloc(sizeof(*proc), GFP_KERNEL);\n\tif (proc == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&proc->inner_lock);\n\tspin_lock_init(&proc->outer_lock);\n\tget_task_struct(current->group_leader);\n\tproc->tsk = current->group_leader;\n\tINIT_LIST_HEAD(&proc->todo);\n\tproc->default_priority = task_nice(current);\n\tbinder_dev = container_of(filp->private_data, struct binder_device,\n\t\t\t\t  miscdev);\n\tproc->context = &binder_dev->context;\n\tbinder_alloc_init(&proc->alloc);\n\tbinder_stats_created(BINDER_STAT_PROC);\n\tproc->pid = current->group_leader->pid;\n\tINIT_LIST_HEAD(&proc->delivered_death);\n\tINIT_LIST_HEAD(&proc->waiting_threads);\n\tfilp->private_data = proc;\n\tmutex_lock(&binder_procs_lock);\n\thlist_add_head(&proc->proc_node, &binder_procs);\n\tmutex_unlock(&binder_procs_lock);\n\tif (binder_debugfs_dir_entry_proc) {\n\t\tchar strbuf[11];\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * proc debug entries are shared between contexts, so\n\t\t * this will fail if the process tries to open the driver\n\t\t * again with a different context. The priting code will\n\t\t * anyway print all contexts that a given PID has, so this\n\t\t * is not a problem.\n\t\t */\n\t\tproc->debugfs_entry = debugfs_create_file(strbuf, S_IRUGO,\n\t\t\tbinder_debugfs_dir_entry_proc,\n\t\t\t(void *)(unsigned long)proc->pid,\n\t\t\t&binder_proc_fops);\n\t}\n\treturn 0;\n}",
        "bug_line_number": [
            12,
            13
        ],
        "vul": 1,
        "id": 194
    },
    {
        "code": "static void task_fd_install(\n\tstruct binder_proc *proc, unsigned int fd, struct file *file)\n{\n\tif (proc->files)\n\t\t__fd_install(proc->files, fd, file);\n}",
        "bug_line_number": [
            2,
            3,
            4,
            5
        ],
        "vul": 1,
        "id": 196
    },
    {
        "code": "static int bigben_probe(struct hid_device *hid,\n\tconst struct hid_device_id *id)\n{\n\tstruct bigben_device *bigben;\n\tstruct hid_input *hidinput;\n\tstruct list_head *report_list;\n\tstruct led_classdev *led;\n\tchar *name;\n\tsize_t name_sz;\n\tint n, error;\n\tbigben = devm_kzalloc(&hid->dev, sizeof(*bigben), GFP_KERNEL);\n\tif (!bigben)\n\t\treturn -ENOMEM;\n\thid_set_drvdata(hid, bigben);\n\tbigben->hid = hid;\n\tbigben->removed = false;\n\terror = hid_parse(hid);\n\tif (error) {\n\t\thid_err(hid, \"parse failed\\n\");\n\t\treturn error;\n\t}\n\terror = hid_hw_start(hid, HID_CONNECT_DEFAULT & ~HID_CONNECT_FF);\n\tif (error) {\n\t\thid_err(hid, \"hw start failed\\n\");\n\t\treturn error;\n\t}\n\treport_list = &hid->report_enum[HID_OUTPUT_REPORT].report_list;\n\tif (list_empty(report_list)) {\n\t\thid_err(hid, \"no output report found\\n\");\n\t\terror = -ENODEV;\n\t\tgoto error_hw_stop;\n\t}\n\tbigben->report = list_entry(report_list->next,\n\t\tstruct hid_report, list);\n\tif (list_empty(&hid->inputs)) {\n\t\thid_err(hid, \"no inputs found\\n\");\n\t\terror = -ENODEV;\n\t\tgoto error_hw_stop;\n\t}\n\thidinput = list_first_entry(&hid->inputs, struct hid_input, list);\n\tset_bit(FF_RUMBLE, hidinput->input->ffbit);\n\tINIT_WORK(&bigben->worker, bigben_worker);\n\tspin_lock_init(&bigben->lock);\n\terror = input_ff_create_memless(hidinput->input, NULL,\n\t\thid_bigben_play_effect);\n\tif (error)\n\t\tgoto error_hw_stop;\n\tname_sz = strlen(dev_name(&hid->dev)) + strlen(\":red:bigben#\") + 1;\n\tfor (n = 0; n < NUM_LEDS; n++) {\n\t\tled = devm_kzalloc(\n\t\t\t&hid->dev,\n\t\t\tsizeof(struct led_classdev) + name_sz,\n\t\t\tGFP_KERNEL\n\t\t);\n\t\tif (!led) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto error_hw_stop;\n\t\t}\n\t\tname = (void *)(&led[1]);\n\t\tsnprintf(name, name_sz,\n\t\t\t\"%s:red:bigben%d\",\n\t\t\tdev_name(&hid->dev), n + 1\n\t\t);\n\t\tled->name = name;\n\t\tled->brightness = (n == 0) ? LED_ON : LED_OFF;\n\t\tled->max_brightness = 1;\n\t\tled->brightness_get = bigben_get_led;\n\t\tled->brightness_set = bigben_set_led;\n\t\tbigben->leds[n] = led;\n\t\terror = devm_led_classdev_register(&hid->dev, led);\n\t\tif (error)\n\t\t\tgoto error_hw_stop;\n\t}\n\t/* initial state: LED1 is on, no rumble effect */\n\tbigben->led_state = BIT(0);\n\tbigben->right_motor_on = 0;\n\tbigben->left_motor_force = 0;\n\tbigben->work_led = true;\n\tbigben->work_ff = true;\n\tschedule_work(&bigben->worker);\n\thid_info(hid, \"LED and force feedback support for BigBen gamepad\\n\");\n\treturn 0;\nerror_hw_stop:\n\thid_hw_stop(hid);\n\treturn error;\n}",
        "bug_line_number": [
            79,
            80
        ],
        "vul": 1,
        "id": 198
    },
    {
        "code": "static int vgem_gem_dumb_create(struct drm_file *file, struct drm_device *dev,\n\t\t\t\tstruct drm_mode_create_dumb *args)\n{\n\tstruct drm_gem_object *gem_object;\n\tu64 pitch, size;\n\tpitch = args->width * DIV_ROUND_UP(args->bpp, 8);\n\tsize = args->height * pitch;\n\tif (size == 0)\n\t\treturn -EINVAL;\n\tgem_object = vgem_gem_create(dev, file, &args->handle, size);\n\tif (IS_ERR(gem_object))\n\t\treturn PTR_ERR(gem_object);\n\targs->size = gem_object->size;\n\targs->pitch = pitch;\n\tDRM_DEBUG(\"Created object of size %lld\\n\", size);\n\treturn 0;\n}",
        "bug_line_number": [
            14,
            15
        ],
        "vul": 1,
        "id": 200
    },
    {
        "code": "static int vmw_cmd_dx_set_streamoutput(struct vmw_private *dev_priv,\n\t\t\t\t       struct vmw_sw_context *sw_context,\n\t\t\t\t       SVGA3dCmdHeader *header)\n{\n\tstruct vmw_ctx_validation_info *ctx_node = sw_context->dx_ctx_node;\n\tstruct vmw_resource *res;\n\tstruct vmw_ctx_bindinfo_so binding;\n\tstruct {\n\t\tSVGA3dCmdHeader header;\n\t\tSVGA3dCmdDXSetStreamOutput body;\n\t} *cmd = container_of(header, typeof(*cmd), header);\n\tint ret;\n\tif (!ctx_node) {\n\t\tDRM_ERROR(\"DX Context not set.\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (cmd->body.soid == SVGA3D_INVALID_ID)\n\t\treturn 0;\n\t/*\n\t * When device does not support SM5 then streamoutput with mob command is\n\t * not available to user-space. Simply return in this case.\n\t */\n\tif (!has_sm5_context(dev_priv))\n\t\treturn 0;\n\t/*\n\t * With SM5 capable device if lookup fails then user-space probably used\n\t * old streamoutput define command. Return without an error.\n\t */\n\tres = vmw_dx_streamoutput_lookup(vmw_context_res_man(ctx_node->ctx),\n\t\t\t\t\t cmd->body.soid);\n\tif (IS_ERR(res)) {\n\t\treturn 0;\n\t}\n\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\tif (ret) {\n\t\tDRM_ERROR(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\tbinding.bi.ctx = ctx_node->ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_so;\n\tbinding.slot = 0; /* Only one SO set to context at a time. */\n\tvmw_binding_add(sw_context->dx_ctx_node->staged, &binding.bi, 0,\n\t\t\tbinding.slot);\n\treturn ret;\n}",
        "bug_line_number": [
            33,
            34,
            35
        ],
        "vul": 1,
        "id": 202
    },
    {
        "code": "static int vmw_execbuf_tie_context(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   uint32_t handle)\n{\n\tstruct vmw_resource *res;\n\tint ret;\n\tunsigned int size;\n\tif (handle == SVGA3D_INVALID_ID)\n\t\treturn 0;\n\tsize = vmw_execbuf_res_size(dev_priv, vmw_res_dx_context);\n\tret = vmw_validation_preload_res(sw_context->ctx, size);\n\tif (ret)\n\t\treturn ret;\n\tres = vmw_user_resource_noref_lookup_handle\n\t\t(dev_priv, sw_context->fp->tfile, handle,\n\t\t user_context_converter);\n\tif (IS_ERR(res)) {\n\t\tVMW_DEBUG_USER(\"Could not find or user DX context 0x%08x.\\n\",\n\t\t\t       (unsigned int) handle);\n\t\treturn PTR_ERR(res);\n\t}\n\tret = vmw_execbuf_res_noref_val_add(sw_context, res, VMW_RES_DIRTY_SET);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\tsw_context->dx_ctx_node = vmw_execbuf_info_from_res(sw_context, res);\n\tsw_context->man = vmw_context_res_man(res);\n\treturn 0;\n}",
        "bug_line_number": [
            13,
            14,
            15,
            16,
            17,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26
        ],
        "vul": 1,
        "id": 204
    },
    {
        "code": "static int vmw_view_res_val_add(struct vmw_sw_context *sw_context,\n\t\t\t\tstruct vmw_resource *view)\n{\n\tint ret;\n\t/*\n\t * First add the resource the view is pointing to, otherwise it may be\n\t * swapped out when the view is validated.\n\t */\n\tret = vmw_execbuf_res_noctx_val_add(sw_context, vmw_view_srf(view),\n\t\t\t\t\t    vmw_view_dirtying(view));\n\tif (ret)\n\t\treturn ret;\n\treturn vmw_execbuf_res_noctx_val_add(sw_context, view,\n\t\t\t\t\t     VMW_RES_DIRTY_NONE);\n}",
        "bug_line_number": [
            8,
            9,
            10,
            12,
            13,
            14
        ],
        "vul": 1,
        "id": 206
    },
    {
        "code": "void put_ucounts(struct ucounts *ucounts)\n{\n\tunsigned long flags;\n\tif (atomic_dec_and_lock_irqsave(&ucounts->count, &ucounts_lock, flags)) {\n\t\thlist_del_init(&ucounts->node);\n\t\tspin_unlock_irqrestore(&ucounts_lock, flags);\n\t\tkfree(ucounts);\n\t}\n}",
        "bug_line_number": [
            5,
            6
        ],
        "vul": 1,
        "id": 208
    },
    {
        "code": "static struct ext4_dir_entry_2 *do_split(handle_t *handle, struct inode *dir,\n\t\t\tstruct buffer_head **bh,struct dx_frame *frame,\n\t\t\tstruct dx_hash_info *hinfo)\n{\n\tunsigned blocksize = dir->i_sb->s_blocksize;\n\tunsigned count, continued;\n\tstruct buffer_head *bh2;\n\text4_lblk_t newblock;\n\tu32 hash2;\n\tstruct dx_map_entry *map;\n\tchar *data1 = (*bh)->b_data, *data2;\n\tunsigned split, move, size;\n\tstruct ext4_dir_entry_2 *de = NULL, *de2;\n\tint\tcsum_size = 0;\n\tint\terr = 0, i;\n\tif (ext4_has_metadata_csum(dir->i_sb))\n\t\tcsum_size = sizeof(struct ext4_dir_entry_tail);\n\tbh2 = ext4_append(handle, dir, &newblock);\n\tif (IS_ERR(bh2)) {\n\t\tbrelse(*bh);\n\t\t*bh = NULL;\n\t\treturn (struct ext4_dir_entry_2 *) bh2;\n\t}\n\tBUFFER_TRACE(*bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, dir->i_sb, *bh,\n\t\t\t\t\t    EXT4_JTR_NONE);\n\tif (err)\n\t\tgoto journal_error;\n\tBUFFER_TRACE(frame->bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, dir->i_sb, frame->bh,\n\t\t\t\t\t    EXT4_JTR_NONE);\n\tif (err)\n\t\tgoto journal_error;\n\tdata2 = bh2->b_data;\n\t/* create map in the end of data2 block */\n\tmap = (struct dx_map_entry *) (data2 + blocksize);\n\tcount = dx_make_map(dir, (struct ext4_dir_entry_2 *) data1,\n\t\t\t     blocksize, hinfo, map);\n\tmap -= count;\n\tdx_sort_map(map, count);\n\t/* Ensure that neither split block is over half full */\n\tsize = 0;\n\tmove = 0;\n\tfor (i = count-1; i >= 0; i--) {\n\t\t/* is more than half of this entry in 2nd half of the block? */\n\t\tif (size + map[i].size/2 > blocksize/2)\n\t\t\tbreak;\n\t\tsize += map[i].size;\n\t\tmove++;\n\t}\n\t/*\n\t * map index at which we will split\n\t *\n\t * If the sum of active entries didn't exceed half the block size, just\n\t * split it in half by count; each resulting block will have at least\n\t * half the space free.\n\t */\n\tif (i > 0)\n\t\tsplit = count - move;\n\telse\n\t\tsplit = count/2;\n\thash2 = map[split].hash;\n\tcontinued = hash2 == map[split - 1].hash;\n\tdxtrace(printk(KERN_INFO \"Split block %lu at %x, %i/%i\\n\",\n\t\t\t(unsigned long)dx_get_block(frame->at),\n\t\t\t\t\thash2, split, count-split));\n\t/* Fancy dance to stay within two buffers */\n\tde2 = dx_move_dirents(dir, data1, data2, map + split, count - split,\n\t\t\t      blocksize);\n\tde = dx_pack_dirents(dir, data1, blocksize);\n\tde->rec_len = ext4_rec_len_to_disk(data1 + (blocksize - csum_size) -\n\t\t\t\t\t   (char *) de,\n\t\t\t\t\t   blocksize);\n\tde2->rec_len = ext4_rec_len_to_disk(data2 + (blocksize - csum_size) -\n\t\t\t\t\t    (char *) de2,\n\t\t\t\t\t    blocksize);\n\tif (csum_size) {\n\t\text4_initialize_dirent_tail(*bh, blocksize);\n\t\text4_initialize_dirent_tail(bh2, blocksize);\n\t}\n\tdxtrace(dx_show_leaf(dir, hinfo, (struct ext4_dir_entry_2 *) data1,\n\t\t\tblocksize, 1));\n\tdxtrace(dx_show_leaf(dir, hinfo, (struct ext4_dir_entry_2 *) data2,\n\t\t\tblocksize, 1));\n\t/* Which block gets the new entry? */\n\tif (hinfo->hash >= hash2) {\n\t\tswap(*bh, bh2);\n\t\tde = de2;\n\t}\n\tdx_insert_block(frame, hash2 + continued, newblock);\n\terr = ext4_handle_dirty_dirblock(handle, dir, bh2);\n\tif (err)\n\t\tgoto journal_error;\n\terr = ext4_handle_dirty_dx_node(handle, dir, frame->bh);\n\tif (err)\n\t\tgoto journal_error;\n\tbrelse(bh2);\n\tdxtrace(dx_show_index(\"frame\", frame->entries));\n\treturn de;\njournal_error:\n\tbrelse(*bh);\n\tbrelse(bh2);\n\t*bh = NULL;\n\text4_std_error(dir->i_sb, err);\n\treturn ERR_PTR(err);\n}",
        "bug_line_number": [
            36,
            37,
            38
        ],
        "vul": 1,
        "id": 210
    },
    {
        "code": "static int p54u_load_firmware(struct ieee80211_hw *dev,\n\t\t\t      struct usb_interface *intf)\n{\n\tstruct usb_device *udev = interface_to_usbdev(intf);\n\tstruct p54u_priv *priv = dev->priv;\n\tstruct device *device = &udev->dev;\n\tint err, i;\n\tBUILD_BUG_ON(ARRAY_SIZE(p54u_fwlist) != __NUM_P54U_HWTYPES);\n\tinit_completion(&priv->fw_wait_load);\n\ti = p54_find_type(priv);\n\tif (i < 0)\n\t\treturn i;\n\tdev_info(&priv->udev->dev, \"Loading firmware file %s\\n\",\n\t       p54u_fwlist[i].fw);\n\tusb_get_dev(udev);\n\terr = request_firmware_nowait(THIS_MODULE, 1, p54u_fwlist[i].fw,\n\t\t\t\t      device, GFP_KERNEL, priv,\n\t\t\t\t      p54u_load_firmware_cb);\n\tif (err) {\n\t\tdev_err(&priv->udev->dev, \"(p54usb) cannot load firmware %s \"\n\t\t\t\t\t  \"(%d)!\\n\", p54u_fwlist[i].fw, err);\n\t\tusb_put_dev(udev);\n\t}\n\treturn err;\n}",
        "bug_line_number": [
            14,
            15,
            21,
            22
        ],
        "vul": 1,
        "id": 212
    },
    {
        "code": "int ext4_mb_add_groupinfo(struct super_block *sb, ext4_group_t group,\n\t\t\t  struct ext4_group_desc *desc)\n{\n\tint i;\n\tint metalen = 0;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_info **meta_group_info;\n\tstruct kmem_cache *cachep = get_groupinfo_cache(sb->s_blocksize_bits);\n\t/*\n\t * First check if this group is the first of a reserved block.\n\t * If it's true, we have to allocate a new table of pointers\n\t * to ext4_group_info structures\n\t */\n\tif (group % EXT4_DESC_PER_BLOCK(sb) == 0) {\n\t\tmetalen = sizeof(*meta_group_info) <<\n\t\t\tEXT4_DESC_PER_BLOCK_BITS(sb);\n\t\tmeta_group_info = kmalloc(metalen, GFP_NOFS);\n\t\tif (meta_group_info == NULL) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't allocate mem \"\n\t\t\t\t \"for a buddy group\");\n\t\t\tgoto exit_meta_group_info;\n\t\t}\n\t\tsbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)] =\n\t\t\tmeta_group_info;\n\t}\n\tmeta_group_info =\n\t\tsbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)];\n\ti = group & (EXT4_DESC_PER_BLOCK(sb) - 1);\n\tmeta_group_info[i] = kmem_cache_zalloc(cachep, GFP_NOFS);\n\tif (meta_group_info[i] == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"can't allocate buddy mem\");\n\t\tgoto exit_group_info;\n\t}\n\tset_bit(EXT4_GROUP_INFO_NEED_INIT_BIT,\n\t\t&(meta_group_info[i]->bb_state));\n\t/*\n\t * initialize bb_free to be able to skip\n\t * empty groups without initialization\n\t */\n\tif (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n\t\tmeta_group_info[i]->bb_free =\n\t\t\text4_free_clusters_after_init(sb, group, desc);\n\t} else {\n\t\tmeta_group_info[i]->bb_free =\n\t\t\text4_free_group_clusters(sb, desc);\n\t}\n\tINIT_LIST_HEAD(&meta_group_info[i]->bb_prealloc_list);\n\tinit_rwsem(&meta_group_info[i]->alloc_sem);\n\tmeta_group_info[i]->bb_free_root = RB_ROOT;\n\tmeta_group_info[i]->bb_largest_free_order = -1;  /* uninit */\n#ifdef DOUBLE_CHECK\n\t{\n\t\tstruct buffer_head *bh;\n\t\tmeta_group_info[i]->bb_bitmap =\n\t\t\tkmalloc(sb->s_blocksize, GFP_NOFS);\n\t\tBUG_ON(meta_group_info[i]->bb_bitmap == NULL);\n\t\tbh = ext4_read_block_bitmap(sb, group);\n\t\tBUG_ON(IS_ERR_OR_NULL(bh));\n\t\tmemcpy(meta_group_info[i]->bb_bitmap, bh->b_data,\n\t\t\tsb->s_blocksize);\n\t\tput_bh(bh);\n\t}\n#endif\n\treturn 0;\nexit_group_info:\n\t/* If a meta_group_info table has been allocated, release it now */\n\tif (group % EXT4_DESC_PER_BLOCK(sb) == 0) {\n\t\tkfree(sbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)]);\n\t\tsbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)] = NULL;\n\t}\nexit_meta_group_info:\n\treturn -ENOMEM;\n} /* ext4_mb_add_groupinfo */",
        "bug_line_number": [
            39,
            40
        ],
        "vul": 1,
        "id": 214
    },
    {
        "code": "int __mdiobus_register(struct mii_bus *bus, struct module *owner)\n{\n\tstruct mdio_device *mdiodev;\n\tint i, err;\n\tstruct gpio_desc *gpiod;\n\tif (NULL == bus || NULL == bus->name ||\n\t    NULL == bus->read || NULL == bus->write)\n\t\treturn -EINVAL;\n\tBUG_ON(bus->state != MDIOBUS_ALLOCATED &&\n\t       bus->state != MDIOBUS_UNREGISTERED);\n\tbus->owner = owner;\n\tbus->dev.parent = bus->parent;\n\tbus->dev.class = &mdio_bus_class;\n\tbus->dev.groups = NULL;\n\tdev_set_name(&bus->dev, \"%s\", bus->id);\n\terr = device_register(&bus->dev);\n\tif (err) {\n\t\tpr_err(\"mii_bus %s failed to register\\n\", bus->id);\n\t\tput_device(&bus->dev);\n\t\treturn -EINVAL;\n\t}\n\tmutex_init(&bus->mdio_lock);\n\t/* de-assert bus level PHY GPIO reset */\n\tgpiod = devm_gpiod_get_optional(&bus->dev, \"reset\", GPIOD_OUT_LOW);\n\tif (IS_ERR(gpiod)) {\n\t\tdev_err(&bus->dev, \"mii_bus %s couldn't get reset GPIO\\n\",\n\t\t\tbus->id);\n\t\tdevice_del(&bus->dev);\n\t\treturn PTR_ERR(gpiod);\n\t} else\tif (gpiod) {\n\t\tbus->reset_gpiod = gpiod;\n\t\tgpiod_set_value_cansleep(gpiod, 1);\n\t\tudelay(bus->reset_delay_us);\n\t\tgpiod_set_value_cansleep(gpiod, 0);\n\t}\n\tif (bus->reset)\n\t\tbus->reset(bus);\n\tfor (i = 0; i < PHY_MAX_ADDR; i++) {\n\t\tif ((bus->phy_mask & (1 << i)) == 0) {\n\t\t\tstruct phy_device *phydev;\n\t\t\tphydev = mdiobus_scan(bus, i);\n\t\t\tif (IS_ERR(phydev) && (PTR_ERR(phydev) != -ENODEV)) {\n\t\t\t\terr = PTR_ERR(phydev);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t}\n\t}\n\tmdiobus_setup_mdiodev_from_board_info(bus, mdiobus_create_device);\n\tbus->state = MDIOBUS_REGISTERED;\n\tpr_info(\"%s: probed\\n\", bus->name);\n\treturn 0;\nerror:\n\twhile (--i >= 0) {\n\t\tmdiodev = bus->mdio_map[i];\n\t\tif (!mdiodev)\n\t\t\tcontinue;\n\t\tmdiodev->device_remove(mdiodev);\n\t\tmdiodev->device_free(mdiodev);\n\t}\n\t/* Put PHYs in RESET to save power */\n\tif (bus->reset_gpiod)\n\t\tgpiod_set_value_cansleep(bus->reset_gpiod, 1);\n\tdevice_del(&bus->dev);\n\treturn err;\n}",
        "bug_line_number": [
            18
        ],
        "vul": 1,
        "id": 216
    },
    {
        "code": "static void __mctp_key_remove(struct mctp_sk_key *key, struct net *net,\n\t\t\t      unsigned long flags, unsigned long reason)\n__releases(&key->lock)\n__must_hold(&net->mctp.keys_lock)\n{\n\tstruct sk_buff *skb;\n\ttrace_mctp_key_release(key, reason);\n\tskb = key->reasm_head;\n\tkey->reasm_head = NULL;\n\tkey->reasm_dead = true;\n\tkey->valid = false;\n\tmctp_dev_release_key(key->dev, key);\n\tspin_unlock_irqrestore(&key->lock, flags);\n\thlist_del(&key->hlist);\n\thlist_del(&key->sklist);\n\t/* unref for the lists */\n\tmctp_key_unref(key);\n\tkfree_skb(skb);\n}",
        "bug_line_number": [
            13,
            14,
            15,
            16,
            17
        ],
        "vul": 1,
        "id": 218
    },
    {
        "code": "static int l2cap_le_connect_req(struct l2cap_conn *conn,\n\t\t\t\tstruct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\tu8 *data)\n{\n\tstruct l2cap_le_conn_req *req = (struct l2cap_le_conn_req *) data;\n\tstruct l2cap_le_conn_rsp rsp;\n\tstruct l2cap_chan *chan, *pchan;\n\tu16 dcid, scid, credits, mtu, mps;\n\t__le16 psm;\n\tu8 result;\n\tif (cmd_len != sizeof(*req))\n\t\treturn -EPROTO;\n\tscid = __le16_to_cpu(req->scid);\n\tmtu  = __le16_to_cpu(req->mtu);\n\tmps  = __le16_to_cpu(req->mps);\n\tpsm  = req->psm;\n\tdcid = 0;\n\tcredits = 0;\n\tif (mtu < 23 || mps < 23)\n\t\treturn -EPROTO;\n\tBT_DBG(\"psm 0x%2.2x scid 0x%4.4x mtu %u mps %u\", __le16_to_cpu(psm),\n\t       scid, mtu, mps);\n\t/* Check if we have socket listening on psm */\n\tpchan = l2cap_global_chan_by_psm(BT_LISTEN, psm, &conn->hcon->src,\n\t\t\t\t\t &conn->hcon->dst, LE_LINK);\n\tif (!pchan) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tchan = NULL;\n\t\tgoto response;\n\t}\n\tmutex_lock(&conn->chan_lock);\n\tl2cap_chan_lock(pchan);\n\tif (!smp_sufficient_security(conn->hcon, pchan->sec_level,\n\t\t\t\t     SMP_ALLOW_STK)) {\n\t\tresult = L2CAP_CR_LE_AUTHENTICATION;\n\t\tchan = NULL;\n\t\tgoto response_unlock;\n\t}\n\t/* Check for valid dynamic CID range */\n\tif (scid < L2CAP_CID_DYN_START || scid > L2CAP_CID_LE_DYN_END) {\n\t\tresult = L2CAP_CR_LE_INVALID_SCID;\n\t\tchan = NULL;\n\t\tgoto response_unlock;\n\t}\n\t/* Check if we already have channel with that dcid */\n\tif (__l2cap_get_chan_by_dcid(conn, scid)) {\n\t\tresult = L2CAP_CR_LE_SCID_IN_USE;\n\t\tchan = NULL;\n\t\tgoto response_unlock;\n\t}\n\tchan = pchan->ops->new_connection(pchan);\n\tif (!chan) {\n\t\tresult = L2CAP_CR_LE_NO_MEM;\n\t\tgoto response_unlock;\n\t}\n\tbacpy(&chan->src, &conn->hcon->src);\n\tbacpy(&chan->dst, &conn->hcon->dst);\n\tchan->src_type = bdaddr_src_type(conn->hcon);\n\tchan->dst_type = bdaddr_dst_type(conn->hcon);\n\tchan->psm  = psm;\n\tchan->dcid = scid;\n\tchan->omtu = mtu;\n\tchan->remote_mps = mps;\n\t__l2cap_chan_add(conn, chan);\n\tl2cap_le_flowctl_init(chan, __le16_to_cpu(req->credits));\n\tdcid = chan->scid;\n\tcredits = chan->rx_credits;\n\t__set_chan_timer(chan, chan->ops->get_sndtimeo(chan));\n\tchan->ident = cmd->ident;\n\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\tl2cap_state_change(chan, BT_CONNECT2);\n\t\t/* The following result value is actually not defined\n\t\t * for LE CoC but we use it to let the function know\n\t\t * that it should bail out after doing its cleanup\n\t\t * instead of sending a response.\n\t\t */\n\t\tresult = L2CAP_CR_PEND;\n\t\tchan->ops->defer(chan);\n\t} else {\n\t\tl2cap_chan_ready(chan);\n\t\tresult = L2CAP_CR_LE_SUCCESS;\n\t}\nresponse_unlock:\n\tl2cap_chan_unlock(pchan);\n\tmutex_unlock(&conn->chan_lock);\n\tl2cap_chan_put(pchan);\n\tif (result == L2CAP_CR_PEND)\n\t\treturn 0;\nresponse:\n\tif (chan) {\n\t\trsp.mtu = cpu_to_le16(chan->imtu);\n\t\trsp.mps = cpu_to_le16(chan->mps);\n\t} else {\n\t\trsp.mtu = 0;\n\t\trsp.mps = 0;\n\t}\n\trsp.dcid    = cpu_to_le16(dcid);\n\trsp.credits = cpu_to_le16(credits);\n\trsp.result  = cpu_to_le16(result);",
        "bug_line_number": [
            21,
            22
        ],
        "vul": 1,
        "id": 220
    },
    {
        "code": "void xfrm_state_fini(struct net *net)\n{\n\tunsigned int sz;\n\tflush_work(&net->xfrm.state_hash_work);\n\tflush_work(&xfrm_state_gc_work);\n\txfrm_state_flush(net, IPSEC_PROTO_ANY, false, true);\n\tWARN_ON(!list_empty(&net->xfrm.state_all));\n\tsz = (net->xfrm.state_hmask + 1) * sizeof(struct hlist_head);\n\tWARN_ON(!hlist_empty(net->xfrm.state_byspi));\n\txfrm_hash_free(net->xfrm.state_byspi, sz);\n\tWARN_ON(!hlist_empty(net->xfrm.state_bysrc));\n\txfrm_hash_free(net->xfrm.state_bysrc, sz);\n\tWARN_ON(!hlist_empty(net->xfrm.state_bydst));\n\txfrm_hash_free(net->xfrm.state_bydst, sz);\n}",
        "bug_line_number": [
            5,
            6
        ],
        "vul": 1,
        "id": 222
    },
    {
        "code": "static inline struct kvm_memory_slot *\nsearch_memslots(struct kvm_memslots *slots, gfn_t gfn)\n{\n\tint start = 0, end = slots->used_slots;\n\tint slot = atomic_read(&slots->lru_slot);\n\tstruct kvm_memory_slot *memslots = slots->memslots;\n\tif (gfn >= memslots[slot].base_gfn &&\n\t    gfn < memslots[slot].base_gfn + memslots[slot].npages)\n\t\treturn &memslots[slot];\n\twhile (start < end) {\n\t\tslot = start + (end - start) / 2;\n\t\tif (gfn >= memslots[slot].base_gfn)\n\t\t\tend = slot;\n\t\telse\n\t\t\tstart = slot + 1;\n\t}\n\tif (gfn >= memslots[start].base_gfn &&\n\t    gfn < memslots[start].base_gfn + memslots[start].npages) {\n\t\tatomic_set(&slots->lru_slot, start);\n\t\treturn &memslots[start];\n\t}\n\treturn NULL;\n}",
        "bug_line_number": [
            5,
            6
        ],
        "vul": 1,
        "id": 224
    },
    {
        "code": "static int dt_remember_or_free_map(struct pinctrl *p, const char *statename,\n\t\t\t\t   struct pinctrl_dev *pctldev,\n\t\t\t\t   struct pinctrl_map *map, unsigned num_maps)\n{\n\tint i;\n\tstruct pinctrl_dt_map *dt_map;\n\t/* Initialize common mapping table entry fields */\n\tfor (i = 0; i < num_maps; i++) {\n\t\tmap[i].dev_name = dev_name(p->dev);\n\t\tmap[i].name = statename;\n\t\tif (pctldev)\n\t\t\tmap[i].ctrl_dev_name = dev_name(pctldev->dev);\n\t}\n\t/* Remember the converted mapping table entries */\n\tdt_map = kzalloc(sizeof(*dt_map), GFP_KERNEL);\n\tif (!dt_map) {\n\t\tdt_free_map(pctldev, map, num_maps);\n\t\treturn -ENOMEM;\n\t}\n\tdt_map->pctldev = pctldev;\n\tdt_map->map = map;\n\tdt_map->num_maps = num_maps;\n\tlist_add_tail(&dt_map->node, &p->dt_maps);\n\treturn pinctrl_register_map(map, num_maps, false);\n}",
        "bug_line_number": [
            8,
            9,
            15,
            16,
            17,
            18,
            19,
            23,
            24
        ],
        "vul": 1,
        "id": 226
    },
    {
        "code": "static void nft_immediate_activate(const struct nft_ctx *ctx,\n\t\t\t\t   const struct nft_expr *expr)\n{\n\tconst struct nft_immediate_expr *priv = nft_expr_priv(expr);\n\treturn nft_data_hold(&priv->data, nft_dreg_to_type(priv->dreg));\n}",
        "bug_line_number": [
            3,
            4
        ],
        "vul": 1,
        "id": 228
    },
    {
        "code": "static void nft_immediate_destroy(const struct nft_ctx *ctx,\n\t\t\t\t  const struct nft_expr *expr)\n{\n\tconst struct nft_immediate_expr *priv = nft_expr_priv(expr);\n\tconst struct nft_data *data = &priv->data;\n\tstruct nft_rule *rule, *n;\n\tstruct nft_ctx chain_ctx;\n\tstruct nft_chain *chain;\n\tif (priv->dreg != NFT_REG_VERDICT)\n\t\treturn;\n\tswitch (data->verdict.code) {\n\tcase NFT_JUMP:\n\tcase NFT_GOTO:\n\t\tchain = data->verdict.chain;\n\t\tif (!nft_chain_is_bound(chain))\n\t\t\tbreak;\n\t\tchain_ctx = *ctx;\n\t\tchain_ctx.chain = chain;\n\t\tlist_for_each_entry_safe(rule, n, &chain->rules, list)\n\t\t\tnf_tables_rule_release(&chain_ctx, rule);\n\t\tnf_tables_chain_destroy(&chain_ctx);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}",
        "bug_line_number": [
            14,
            15,
            16,
            18,
            19,
            20
        ],
        "vul": 1,
        "id": 230
    },
    {
        "code": "static int __nf_tables_abort(struct net *net, enum nfnl_abort_action action)\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(net);\n\tstruct nft_trans *trans, *next;\n\tLIST_HEAD(set_update_list);\n\tstruct nft_trans_elem *te;\n\tif (action == NFNL_ABORT_VALIDATE &&\n\t    nf_tables_validate(net) < 0)\n\t\treturn -EAGAIN;\n\tlist_for_each_entry_safe_reverse(trans, next, &nft_net->commit_list,\n\t\t\t\t\t list) {\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWTABLE:\n\t\t\tif (nft_trans_table_update(trans)) {\n\t\t\t\tif (!(trans->ctx.table->flags & __NFT_TABLE_F_UPDATE)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (trans->ctx.table->flags & __NFT_TABLE_F_WAS_DORMANT) {\n\t\t\t\t\tnf_tables_table_disable(net, trans->ctx.table);\n\t\t\t\t\ttrans->ctx.table->flags |= NFT_TABLE_F_DORMANT;\n\t\t\t\t} else if (trans->ctx.table->flags & __NFT_TABLE_F_WAS_AWAKEN) {\n\t\t\t\t\ttrans->ctx.table->flags &= ~NFT_TABLE_F_DORMANT;\n\t\t\t\t}\n\t\t\t\ttrans->ctx.table->flags &= ~__NFT_TABLE_F_UPDATE;\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t} else {\n\t\t\t\tlist_del_rcu(&trans->ctx.table->list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELTABLE:\n\t\tcase NFT_MSG_DESTROYTABLE:\n\t\t\tnft_clear(trans->ctx.net, trans->ctx.table);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnft_netdev_unregister_hooks(net,\n\t\t\t\t\t\t\t    &nft_trans_chain_hooks(trans),\n\t\t\t\t\t\t\t    true);\n\t\t\t\tfree_percpu(nft_trans_chain_stats(trans));\n\t\t\t\tkfree(nft_trans_chain_name(trans));\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t} else {\n\t\t\t\tif (nft_chain_is_bound(trans->ctx.chain)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\ttrans->ctx.table->use--;\n\t\t\t\tnft_chain_del(trans->ctx.chain);\n\t\t\t\tnf_tables_unregister_hook(trans->ctx.net,\n\t\t\t\t\t\t\t  trans->ctx.table,\n\t\t\t\t\t\t\t  trans->ctx.chain);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELCHAIN:\n\t\tcase NFT_MSG_DESTROYCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tlist_splice(&nft_trans_chain_hooks(trans),\n\t\t\t\t\t    &nft_trans_basechain(trans)->hook_list);\n\t\t\t} else {\n\t\t\t\ttrans->ctx.table->use++;\n\t\t\t\tnft_clear(trans->ctx.net, trans->ctx.chain);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWRULE:\n\t\t\ttrans->ctx.chain->use--;\n\t\t\tlist_del_rcu(&nft_trans_rule(trans)->list);\n\t\t\tnft_rule_expr_deactivate(&trans->ctx,\n\t\t\t\t\t\t nft_trans_rule(trans),\n\t\t\t\t\t\t NFT_TRANS_ABORT);\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELRULE:\n\t\tcase NFT_MSG_DESTROYRULE:\n\t\t\ttrans->ctx.chain->use++;\n\t\t\tnft_clear(trans->ctx.net, nft_trans_rule(trans));\n\t\t\tnft_rule_expr_activate(&trans->ctx, nft_trans_rule(trans));\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (nft_trans_set_update(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttrans->ctx.table->use--;\n\t\t\tif (nft_trans_set_bound(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tlist_del_rcu(&nft_trans_set(trans)->list);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSET:\n\t\tcase NFT_MSG_DESTROYSET:\n\t\t\ttrans->ctx.table->use++;\n\t\t\tnft_clear(trans->ctx.net, nft_trans_set(trans));\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSETELEM:\n\t\t\tif (nft_trans_elem_set_bound(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\t\t\tnft_setelem_remove(net, te->set, &te->elem);\n\t\t\tif (!nft_setelem_is_catchall(te->set, &te->elem))\n\t\t\t\tatomic_dec(&te->set->nelems);\n\t\t\tif (te->set->ops->abort &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}",
        "bug_line_number": [
            44,
            45,
            66,
            67
        ],
        "vul": 1,
        "id": 232
    },
    {
        "code": "static inline void vmacache_invalidate(struct mm_struct *mm)\n{\n\tmm->vmacache_seqnum++;\n\t/* deal with overflows */\n\tif (unlikely(mm->vmacache_seqnum == 0))\n\t\tvmacache_flush_all(mm);\n}",
        "bug_line_number": [
            3,
            4,
            5
        ],
        "vul": 1,
        "id": 234
    },
    {
        "code": "static void slcan_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slcan *sl = tty->disc_data;\n\tschedule_work(&sl->tx_work);\n}",
        "bug_line_number": [
            2,
            3,
            4
        ],
        "vul": 1,
        "id": 236
    },
    {
        "code": "static int vt_disallocate(unsigned int vc_num)\n{\n\tstruct vc_data *vc = NULL;\n\tint ret = 0;\n\tconsole_lock();\n\tif (vt_busy(vc_num))\n\t\tret = -EBUSY;\n\telse if (vc_num)\n\t\tvc = vc_deallocate(vc_num);\n\tconsole_unlock();\n\tif (vc && vc_num >= MIN_NR_CONSOLES) {\n\t\ttty_port_destroy(&vc->port);\n\t\tkfree(vc);\n\t}\n\treturn ret;\n}",
        "bug_line_number": [
            10,
            11,
            12,
            13,
            14
        ],
        "vul": 1,
        "id": 238
    },
    {
        "code": "int vc_allocate(unsigned int currcons)\t/* return 0 on success */\n{\n\tstruct vt_notifier_param param;\n\tstruct vc_data *vc;\n\tWARN_CONSOLE_UNLOCKED();\n\tif (currcons >= MAX_NR_CONSOLES)\n\t\treturn -ENXIO;\n\tif (vc_cons[currcons].d)\n\t\treturn 0;\n\t/* due to the granularity of kmalloc, we waste some memory here */\n\t/* the alloc is done in two steps, to optimize the common situation\n\t   of a 25x80 console (structsize=216, screenbuf_size=4000) */\n\t/* although the numbers above are not valid since long ago, the\n\t   point is still up-to-date and the comment still has its value\n\t   even if only as a historical artifact.  --mj, July 1998 */\n\tparam.vc = vc = kzalloc(sizeof(struct vc_data), GFP_KERNEL);\n\tif (!vc)\n\t\treturn -ENOMEM;\n\tvc_cons[currcons].d = vc;\n\ttty_port_init(&vc->port);\n\tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n\tvisual_init(vc, currcons, 1);\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(vc);\n\tvc->vc_screenbuf = kzalloc(vc->vc_screenbuf_size, GFP_KERNEL);\n\tif (!vc->vc_screenbuf)\n\t\tgoto err_free;\n\t/* If no drivers have overridden us and the user didn't pass a\n\t   boot option, default to displaying the cursor */\n\tif (global_cursor_default == -1)\n\t\tglobal_cursor_default = 1;\n\tvc_init(vc, vc->vc_rows, vc->vc_cols, 1);\n\tvcs_make_sysfs(currcons);\n\tatomic_notifier_call_chain(&vt_notifier_list, VT_ALLOCATE, &param);\n\treturn 0;\nerr_free:\n\tvisual_deinit(vc);\n\tkfree(vc);\n\tvc_cons[currcons].d = NULL;\n\treturn -ENOMEM;\n}",
        "bug_line_number": [
            19,
            20
        ],
        "vul": 1,
        "id": 240
    },
    {
        "code": "static void vt_disallocate_all(void)\n{\n\tstruct vc_data *vc[MAX_NR_CONSOLES];\n\tint i;\n\tconsole_lock();\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++)\n\t\tif (!vt_busy(i))\n\t\t\tvc[i] = vc_deallocate(i);\n\t\telse\n\t\t\tvc[i] = NULL;\n\tconsole_unlock();\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++) {\n\t\tif (vc[i] && i >= MIN_NR_CONSOLES) {\n\t\t\ttty_port_destroy(&vc[i]->port);\n\t\t\tkfree(vc[i]);\n\t\t}\n\t}\n}",
        "bug_line_number": [
            12,
            13,
            14,
            15,
            16
        ],
        "vul": 1,
        "id": 242
    },
    {
        "code": "static void iowarrior_disconnect(struct usb_interface *interface)\n{\n\tstruct iowarrior *dev;\n\tint minor;\n\tdev = usb_get_intfdata(interface);\n\tmutex_lock(&iowarrior_open_disc_lock);\n\tusb_set_intfdata(interface, NULL);\n\t/* prevent device read, write and ioctl */\n\tdev->present = 0;\n\tminor = dev->minor;\n\tmutex_unlock(&iowarrior_open_disc_lock);\n\t/* give back our minor - this will call close() locks need to be dropped at this point*/\n\tusb_deregister_dev(interface, &iowarrior_class);\n\tmutex_lock(&dev->mutex);\n\t/* prevent device read, write and ioctl */\n\tmutex_unlock(&dev->mutex);\n\tif (dev->opened) {\n\t\t/* There is a process that holds a filedescriptor to the device ,\n\t\t   so we only shutdown read-/write-ops going on.\n\t\t   Deleting the device is postponed until close() was called.\n\t\t */\n\t\tusb_kill_urb(dev->int_in_urb);\n\t\twake_up_interruptible(&dev->read_wait);\n\t\twake_up_interruptible(&dev->write_wait);\n\t} else {\n\t\t/* no process is using the device, cleanup now */\n\t\tiowarrior_delete(dev);\n\t}\n\tdev_info(&interface->dev, \"I/O-Warror #%d now disconnected\\n\",\n\t\t minor - IOWARRIOR_MINOR_BASE);\n}",
        "bug_line_number": [
            7,
            8,
            15,
            16,
            23,
            24,
            25,
            26
        ],
        "vul": 1,
        "id": 244
    },
    {
        "code": "static void hfsplus_put_super(struct super_block *sb)\n{\n\tstruct hfsplus_sb_info *sbi = HFSPLUS_SB(sb);\n\thfs_dbg(SUPER, \"hfsplus_put_super\\n\");\n\tcancel_delayed_work_sync(&sbi->sync_work);\n\tif (!sb_rdonly(sb) && sbi->s_vhdr) {\n\t\tstruct hfsplus_vh *vhdr = sbi->s_vhdr;\n\t\tvhdr->modify_date = hfsp_now2mt();\n\t\tvhdr->attributes |= cpu_to_be32(HFSPLUS_VOL_UNMNT);\n\t\tvhdr->attributes &= cpu_to_be32(~HFSPLUS_VOL_INCNSTNT);\n\t\thfsplus_sync_fs(sb, 1);\n\t}\n\thfs_btree_close(sbi->attr_tree);\n\thfs_btree_close(sbi->cat_tree);\n\thfs_btree_close(sbi->ext_tree);\n\tiput(sbi->alloc_file);\n\tiput(sbi->hidden_dir);\n\tkfree(sbi->s_vhdr_buf);\n\tkfree(sbi->s_backup_vhdr_buf);\n\tunload_nls(sbi->nls);\n\tkfree(sb->s_fs_info);\n\tsb->s_fs_info = NULL;\n}",
        "bug_line_number": [
            11,
            12,
            15,
            16
        ],
        "vul": 1,
        "id": 246
    },
    {
        "code": "static long do_get_mempolicy(int *policy, nodemask_t *nmask,\n\t\t\t     unsigned long addr, unsigned long flags)\n{\n\tint err;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mempolicy *pol = current->mempolicy;\n\tif (flags &\n\t\t~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))\n\t\treturn -EINVAL;\n\tif (flags & MPOL_F_MEMS_ALLOWED) {\n\t\tif (flags & (MPOL_F_NODE|MPOL_F_ADDR))\n\t\t\treturn -EINVAL;\n\t\t*policy = 0;\t/* just so it's initialized */\n\t\ttask_lock(current);\n\t\t*nmask  = cpuset_current_mems_allowed;\n\t\ttask_unlock(current);\n\t\treturn 0;\n\t}\n\tif (flags & MPOL_F_ADDR) {\n\t\t/*\n\t\t * Do NOT fall back to task policy if the\n\t\t * vma/shared policy at addr is NULL.  We\n\t\t * want to return MPOL_DEFAULT in this case.\n\t\t */\n\t\tdown_read(&mm->mmap_sem);\n\t\tvma = find_vma_intersection(mm, addr, addr+1);\n\t\tif (!vma) {\n\t\t\tup_read(&mm->mmap_sem);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (vma->vm_ops && vma->vm_ops->get_policy)\n\t\t\tpol = vma->vm_ops->get_policy(vma, addr);\n\t\telse\n\t\t\tpol = vma->vm_policy;\n\t} else if (addr)\n\t\treturn -EINVAL;\n\tif (!pol)\n\t\tpol = &default_policy;\t/* indicates default behavior */\n\tif (flags & MPOL_F_NODE) {\n\t\tif (flags & MPOL_F_ADDR) {\n\t\t\terr = lookup_node(addr);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t\t*policy = err;\n\t\t} else if (pol == current->mempolicy &&\n\t\t\t\tpol->mode == MPOL_INTERLEAVE) {\n\t\t\t*policy = next_node_in(current->il_prev, pol->v.nodes);\n\t\t} else {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\t*policy = pol == &default_policy ? MPOL_DEFAULT :\n\t\t\t\t\t\tpol->mode;\n\t\t/*\n\t\t * Internal mempolicy flags must be masked off before exposing\n\t\t * the policy to userspace.\n\t\t */\n\t\t*policy |= (pol->flags & MPOL_MODE_FLAGS);\n\t}\n\tif (vma) {\n\t\tup_read(&current->mm->mmap_sem);\n\t\tvma = NULL;\n\t}\n\terr = 0;\n\tif (nmask) {\n\t\tif (mpol_store_user_nodemask(pol)) {\n\t\t\t*nmask = pol->w.user_nodemask;\n\t\t} else {\n\t\t\ttask_lock(current);\n\t\t\tget_policy_nodemask(pol, nmask);\n\t\t\ttask_unlock(current);\n\t\t}\n\t}\n out:\n\tmpol_cond_put(pol);\n\tif (vma)\n\t\tup_read(&current->mm->mmap_sem);\n\treturn err;\n}",
        "bug_line_number": [
            61,
            62,
            63,
            64
        ],
        "vul": 1,
        "id": 248
    },
    {
        "code": "unsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len,\n\t\tbool need_rmap_locks)\n{\n\tunsigned long extent, old_end;\n\tstruct mmu_notifier_range range;\n\tpmd_t *old_pmd, *new_pmd;\n\tpud_t *old_pud, *new_pud;\n\told_end = old_addr + len;\n\tflush_cache_range(vma, old_addr, old_end);\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, vma, vma->vm_mm,\n\t\t\t\told_addr, old_end);\n\tmmu_notifier_invalidate_range_start(&range);\n\tfor (; old_addr < old_end; old_addr += extent, new_addr += extent) {\n\t\tcond_resched();\n\t\t/*\n\t\t * If extent is PUD-sized try to speed up the move by moving at the\n\t\t * PUD level if possible.\n\t\t */\n\t\textent = get_extent(NORMAL_PUD, old_addr, old_end, new_addr);\n\t\told_pud = get_old_pud(vma->vm_mm, old_addr);\n\t\tif (!old_pud)\n\t\t\tcontinue;\n\t\tnew_pud = alloc_new_pud(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pud)\n\t\t\tbreak;\n\t\tif (pud_trans_huge(*old_pud) || pud_devmap(*old_pud)) {\n\t\t\tif (extent == HPAGE_PUD_SIZE) {\n\t\t\t\tmove_pgt_entry(HPAGE_PUD, vma, old_addr, new_addr,\n\t\t\t\t\t       old_pud, new_pud, need_rmap_locks);\n\t\t\t\t/* We ignore and continue on error? */\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else if (IS_ENABLED(CONFIG_HAVE_MOVE_PUD) && extent == PUD_SIZE) {\n\t\t\tif (move_pgt_entry(NORMAL_PUD, vma, old_addr, new_addr,\n\t\t\t\t\t   old_pud, new_pud, need_rmap_locks))\n\t\t\t\tcontinue;\n\t\t}\n\t\textent = get_extent(NORMAL_PMD, old_addr, old_end, new_addr);\n\t\told_pmd = get_old_pmd(vma->vm_mm, old_addr);\n\t\tif (!old_pmd)\n\t\t\tcontinue;\n\t\tnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pmd)\n\t\t\tbreak;\n\t\tif (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd) ||\n\t\t    pmd_devmap(*old_pmd)) {\n\t\t\tif (extent == HPAGE_PMD_SIZE &&\n\t\t\t    move_pgt_entry(HPAGE_PMD, vma, old_addr, new_addr,\n\t\t\t\t\t   old_pmd, new_pmd, need_rmap_locks))\n\t\t\t\tcontinue;\n\t\t\tsplit_huge_pmd(vma, old_pmd, old_addr);\n\t\t\tif (pmd_trans_unstable(old_pmd))\n\t\t\t\tcontinue;\n\t\t} else if (IS_ENABLED(CONFIG_HAVE_MOVE_PMD) &&\n\t\t\t   extent == PMD_SIZE) {\n\t\t\t/*\n\t\t\t * If the extent is PMD-sized, try to speed the move by\n\t\t\t * moving at the PMD level if possible.\n\t\t\t */\n\t\t\tif (move_pgt_entry(NORMAL_PMD, vma, old_addr, new_addr,\n\t\t\t\t\t   old_pmd, new_pmd, need_rmap_locks))\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (pte_alloc(new_vma->vm_mm, new_pmd))\n\t\t\tbreak;\n\t\tmove_ptes(vma, old_pmd, old_addr, old_addr + extent, new_vma,\n\t\t\t  new_pmd, new_addr, need_rmap_locks);\n\t}\n\tmmu_notifier_invalidate_range_end(&range);\n\treturn len + old_addr - old_end;\t/* how much done */\n}",
        "bug_line_number": [
            36,
            37,
            62,
            63
        ],
        "vul": 1,
        "id": 250
    },
    {
        "code": "static void adu_disconnect(struct usb_interface *interface)\n{\n\tstruct adu_device *dev;\n\tdev = usb_get_intfdata(interface);\n\tmutex_lock(&dev->mtx);\t/* not interruptible */\n\tdev->udev = NULL;\t/* poison */\n\tusb_deregister_dev(interface, &adu_class);\n\tmutex_unlock(&dev->mtx);\n\tmutex_lock(&adutux_mutex);\n\tusb_set_intfdata(interface, NULL);\n\t/* if the device is not opened, then we clean up right now */\n\tif (!dev->open_count)\n\t\tadu_delete(dev);\n\tmutex_unlock(&adutux_mutex);\n}",
        "bug_line_number": [
            4,
            5,
            7,
            9,
            10
        ],
        "vul": 1,
        "id": 252
    },
    {
        "code": "static int nft_verdict_init(const struct nft_ctx *ctx, struct nft_data *data,\n\t\t\t    struct nft_data_desc *desc, const struct nlattr *nla)\n{\n\tu8 genmask = nft_genmask_next(ctx->net);\n\tstruct nlattr *tb[NFTA_VERDICT_MAX + 1];\n\tstruct nft_chain *chain;\n\tint err;\n\terr = nla_parse_nested_deprecated(tb, NFTA_VERDICT_MAX, nla,\n\t\t\t\t\t  nft_verdict_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\tif (!tb[NFTA_VERDICT_CODE])\n\t\treturn -EINVAL;\n\tdata->verdict.code = ntohl(nla_get_be32(tb[NFTA_VERDICT_CODE]));\n\tswitch (data->verdict.code) {\n\tdefault:\n\t\tswitch (data->verdict.code & NF_VERDICT_MASK) {\n\t\tcase NF_ACCEPT:\n\t\tcase NF_DROP:\n\t\tcase NF_QUEUE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tfallthrough;\n\tcase NFT_CONTINUE:\n\tcase NFT_BREAK:\n\tcase NFT_RETURN:\n\t\tbreak;\n\tcase NFT_JUMP:\n\tcase NFT_GOTO:\n\t\tif (tb[NFTA_VERDICT_CHAIN]) {\n\t\t\tchain = nft_chain_lookup(ctx->net, ctx->table,\n\t\t\t\t\t\t tb[NFTA_VERDICT_CHAIN],\n\t\t\t\t\t\t genmask);\n\t\t} else if (tb[NFTA_VERDICT_CHAIN_ID]) {\n\t\t\tchain = nft_chain_lookup_byid(ctx->net, ctx->table,\n\t\t\t\t\t\t      tb[NFTA_VERDICT_CHAIN_ID]);\n\t\t\tif (IS_ERR(chain))\n\t\t\t\treturn PTR_ERR(chain);\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (IS_ERR(chain))\n\t\t\treturn PTR_ERR(chain);\n\t\tif (nft_is_base_chain(chain))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (nft_chain_is_bound(chain))\n\t\t\treturn -EINVAL;\n\t\tif (desc->flags & NFT_DATA_DESC_SETELEM &&\n\t\t    chain->flags & NFT_CHAIN_BINDING)\n\t\t\treturn -EINVAL;\n\t\tif (!nft_use_inc(&chain->use))\n\t\t\treturn -EMFILE;\n\t\tdata->verdict.chain = chain;\n\t\tbreak;\n\t}\n\tdesc->len = sizeof(data->verdict);\n\treturn 0;\n}",
        "bug_line_number": [
            37,
            38
        ],
        "vul": 1,
        "id": 254
    },
    {
        "code": "static ssize_t\nvcs_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct vc_data *vc;\n\tstruct vcs_poll_data *poll;\n\tunsigned int read;\n\tssize_t ret;\n\tchar *con_buf;\n\tloff_t pos;\n\tbool viewed, attr, uni_mode;\n\tcon_buf = (char *) __get_free_page(GFP_KERNEL);\n\tif (!con_buf)\n\t\treturn -ENOMEM;\n\tpos = *ppos;\n\t/* Select the proper current console and verify\n\t * sanity of the situation under the console lock.\n\t */\n\tconsole_lock();\n\tuni_mode = use_unicode(inode);\n\tattr = use_attributes(inode);\n\tret = -ENXIO;\n\tvc = vcs_vc(inode, &viewed);\n\tif (!vc)\n\t\tgoto unlock_out;\n\tret = -EINVAL;\n\tif (pos < 0)\n\t\tgoto unlock_out;\n\t/* we enforce 32-bit alignment for pos and count in unicode mode */\n\tif (uni_mode && (pos | count) & 3)\n\t\tgoto unlock_out;\n\tpoll = file->private_data;\n\tif (count && poll)\n\t\tpoll->event = 0;\n\tread = 0;\n\tret = 0;\n\twhile (count) {\n\t\tunsigned int this_round, skip = 0;\n\t\tint size;\n\t\t/* Check whether we are above size each round,\n\t\t * as copy_to_user at the end of this loop\n\t\t * could sleep.\n\t\t */\n\t\tsize = vcs_size(vc, attr, uni_mode);\n\t\tif (size < 0) {\n\t\t\tif (read)\n\t\t\t\tbreak;\n\t\t\tret = size;\n\t\t\tgoto unlock_out;\n\t\t}\n\t\tif (pos >= size)\n\t\t\tbreak;\n\t\tif (count > size - pos)\n\t\t\tcount = size - pos;\n\t\tthis_round = count;\n\t\tif (this_round > CON_BUF_SIZE)\n\t\t\tthis_round = CON_BUF_SIZE;\n\t\t/* Perform the whole read into the local con_buf.\n\t\t * Then we can drop the console spinlock and safely\n\t\t * attempt to move it to userspace.\n\t\t */\n\t\tif (uni_mode) {\n\t\t\tret = vcs_read_buf_uni(vc, con_buf, pos, this_round,\n\t\t\t\t\tviewed);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t} else if (!attr) {\n\t\t\tvcs_read_buf_noattr(vc, con_buf, pos, this_round,\n\t\t\t\t\tviewed);\n\t\t} else {\n\t\t\tthis_round = vcs_read_buf(vc, con_buf, pos, this_round,\n\t\t\t\t\tviewed, &skip);\n\t\t}\n\t\t/* Finally, release the console semaphore while we push\n\t\t * all the data to userspace from our temporary buffer.\n\t\t *\n\t\t * AKPM: Even though it's a semaphore, we should drop it because\n\t\t * the pagefault handling code may want to call printk().\n\t\t */\n\t\tconsole_unlock();\n\t\tret = copy_to_user(buf, con_buf + skip, this_round);\n\t\tconsole_lock();\n\t\tif (ret) {\n\t\t\tread += this_round - ret;\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tbuf += this_round;\n\t\tpos += this_round;\n\t\tread += this_round;\n\t\tcount -= this_round;\n\t}\n\t*ppos += read;\n\tif (read)\n\t\tret = read;\nunlock_out:\n\tconsole_unlock();\n\tfree_page((unsigned long) con_buf);\n\treturn ret;\n}",
        "bug_line_number": [
            21,
            22,
            23,
            24,
            38,
            39
        ],
        "vul": 1,
        "id": 256
    },
    {
        "code": "long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\tunion ion_ioctl_arg data;\n\tdir = ion_ioctl_dir(cmd);\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\t/*\n\t * The copy_from_user is unconditional here for both read and write\n\t * to do the validate. If there is no write for the ioctl, the\n\t * buffer is cleared\n\t */\n\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\treturn -EFAULT;\n\tret = validate_ioctl_arg(cmd, &data);\n\tif (ret) {\n\t\tpr_warn_once(\"%s: ioctl validate failed\\n\", __func__);\n\t\treturn ret;\n\t}\n\tif (!(dir & _IOC_WRITE))\n\t\tmemset(&data, 0, sizeof(data));\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.allocation.handle = handle->id;\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tdata.fd.fd = ion_share_dma_buf_fd_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tcase ION_IOC_HEAP_QUERY:\n\t\tret = ion_query_heaps(client, &data.query);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;",
        "bug_line_number": [
            29,
            30,
            31,
            32,
            33,
            101,
            102,
            103,
            105,
            106
        ],
        "vul": 1,
        "id": 258
    },
    {
        "code": "static inline int __bpf_skb_change_head(struct sk_buff *skb, u32 head_room,\n\t\t\t\t\tu64 flags)\n{\n\tu32 max_len = __bpf_skb_max_len(skb);\n\tu32 new_len = skb->len + head_room;\n\tint ret;\n\tif (unlikely(flags || (!skb_is_gso(skb) && new_len > max_len) ||\n\t\t     new_len < skb->len))\n\t\treturn -EINVAL;\n\tret = skb_cow(skb, head_room);\n\tif (likely(!ret)) {\n\t\t/* Idea for this helper is that we currently only\n\t\t * allow to expand on mac header. This means that\n\t\t * skb->protocol network header, etc, stay as is.\n\t\t * Compared to bpf_skb_change_tail(), we're more\n\t\t * flexible due to not needing to linearize or\n\t\t * reset GSO. Intention for this helper is to be\n\t\t * used by an L3 skb that needs to push mac header\n\t\t * for redirection into L2 device.\n\t\t */\n\t\t__skb_push(skb, head_room);\n\t\tmemset(skb->data, 0, head_room);\n\t\tskb_reset_mac_header(skb);\n\t}\n\treturn ret;\n}",
        "bug_line_number": [
            3,
            4
        ],
        "vul": 1,
        "id": 260
    },
    {
        "code": "static void disk_seqf_stop(struct seq_file *seqf, void *v)\n{\n\tstruct class_dev_iter *iter = seqf->private;\n\t/* stop is called even after start failed :-( */\n\tif (iter) {\n\t\tclass_dev_iter_exit(iter);\n\t\tkfree(iter);\n\t}\n}",
        "bug_line_number": [
            6,
            7
        ],
        "vul": 1,
        "id": 262
    },
    {
        "code": "static int snd_seq_device_dev_free(struct snd_device *device)\n{\n\tstruct snd_seq_device *dev = device->device_data;\n\tput_device(&dev->dev);\n\treturn 0;\n}",
        "bug_line_number": [
            2,
            3
        ],
        "vul": 1,
        "id": 264
    },
    {
        "code": "int aa_audit_rule_init(u32 field, u32 op, char *rulestr, void **vrule)\n{\n\tstruct aa_audit_rule *rule;\n\tswitch (field) {\n\tcase AUDIT_SUBJ_ROLE:\n\t\tif (op != Audit_equal && op != Audit_not_equal)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\trule = kzalloc(sizeof(struct aa_audit_rule), GFP_KERNEL);\n\tif (!rule)\n\t\treturn -ENOMEM;\n\t/* Currently rules are treated as coming from the root ns */\n\trule->label = aa_label_parse(&root_ns->unconfined->label, rulestr,\n\t\t\t\t     GFP_KERNEL, true, false);\n\tif (IS_ERR(rule->label)) {\n\t\taa_audit_rule_free(rule);\n\t\treturn PTR_ERR(rule->label);\n\t}\n\t*vrule = rule;\n\treturn 0;\n}",
        "bug_line_number": [
            17,
            18,
            19,
            20
        ],
        "vul": 1,
        "id": 266
    },
    {
        "code": "static int xgene_hwmon_remove(struct platform_device *pdev)\n{\n\tstruct xgene_hwmon_dev *ctx = platform_get_drvdata(pdev);\n\thwmon_device_unregister(ctx->hwmon_dev);\n\tkfifo_free(&ctx->async_msg_fifo);\n\tif (acpi_disabled)\n\t\tmbox_free_channel(ctx->mbox_chan);\n\telse\n\t\tpcc_mbox_free_channel(ctx->pcc_chan);\n\treturn 0;\n}",
        "bug_line_number": [
            2,
            3
        ],
        "vul": 1,
        "id": 268
    },
    {
        "code": "static void nft_dynset_activate(const struct nft_ctx *ctx,\n\t\t\t\tconst struct nft_expr *expr)\n{\n\tstruct nft_dynset *priv = nft_expr_priv(expr);\n\tpriv->set->use++;\n}",
        "bug_line_number": [
            4,
            5
        ],
        "vul": 1,
        "id": 270
    },
    {
        "code": "static void nft_lookup_activate(const struct nft_ctx *ctx,\n\t\t\t\tconst struct nft_expr *expr)\n{\n\tstruct nft_lookup *priv = nft_expr_priv(expr);\n\tpriv->set->use++;\n}",
        "bug_line_number": [
            4,
            5
        ],
        "vul": 1,
        "id": 272
    },
    {
        "code": "static struct l2cap_chan *l2cap_get_chan_by_scid(struct l2cap_conn *conn,\n\t\t\t\t\t\t u16 cid)\n{\n\tstruct l2cap_chan *c;\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_scid(conn, cid);\n\tif (c)\n\t\tl2cap_chan_lock(c);\n\tmutex_unlock(&conn->chan_lock);\n\treturn c;\n}",
        "bug_line_number": [
            6,
            7,
            8
        ],
        "vul": 1,
        "id": 274
    },
    {
        "code": "static void l2cap_move_continue(struct l2cap_conn *conn, u16 icid, u16 result)\n{\n\tstruct l2cap_chan *chan;\n\tstruct hci_chan *hchan = NULL;\n\tchan = l2cap_get_chan_by_scid(conn, icid);\n\tif (!chan) {\n\t\tl2cap_send_move_chan_cfm_icid(conn, icid);\n\t\treturn;\n\t}\n\t__clear_chan_timer(chan);\n\tif (result == L2CAP_MR_PEND)\n\t\t__set_chan_timer(chan, L2CAP_MOVE_ERTX_TIMEOUT);\n\tswitch (chan->move_state) {\n\tcase L2CAP_MOVE_WAIT_LOGICAL_COMP:\n\t\t/* Move confirm will be sent when logical link\n\t\t * is complete.\n\t\t */\n\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_CFM;\n\t\tbreak;\n\tcase L2CAP_MOVE_WAIT_RSP_SUCCESS:\n\t\tif (result == L2CAP_MR_PEND) {\n\t\t\tbreak;\n\t\t} else if (test_bit(CONN_LOCAL_BUSY,\n\t\t\t\t    &chan->conn_state)) {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOCAL_BUSY;\n\t\t} else {\n\t\t\t/* Logical link is up or moving to BR/EDR,\n\t\t\t * proceed with move\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_CONFIRM_RSP;\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_CONFIRMED);\n\t\t}\n\t\tbreak;\n\tcase L2CAP_MOVE_WAIT_RSP:\n\t\t/* Moving to AMP */\n\t\tif (result == L2CAP_MR_SUCCESS) {\n\t\t\t/* Remote is ready, send confirm immediately\n\t\t\t * after logical link is ready\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_CFM;\n\t\t} else {\n\t\t\t/* Both logical link and move success\n\t\t\t * are required to confirm\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_COMP;\n\t\t}\n\t\t/* Placeholder - get hci_chan for logical link */\n\t\tif (!hchan) {\n\t\t\t/* Logical link not available */\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_UNCONFIRMED);\n\t\t\tbreak;\n\t\t}\n\t\t/* If the logical link is not yet connected, do not\n\t\t * send confirmation.\n\t\t */\n\t\tif (hchan->state != BT_CONNECTED)\n\t\t\tbreak;\n\t\t/* Logical link is already ready to go */\n\t\tchan->hs_hcon = hchan->conn;\n\t\tchan->hs_hcon->l2cap_data = chan->conn;\n\t\tif (result == L2CAP_MR_SUCCESS) {\n\t\t\t/* Can confirm now */\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_CONFIRMED);\n\t\t} else {\n\t\t\t/* Now only need move success\n\t\t\t * to confirm\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_RSP_SUCCESS;\n\t\t}\n\t\tl2cap_logical_cfm(chan, hchan, L2CAP_MR_SUCCESS);\n\t\tbreak;\n\tdefault:\n\t\t/* Any other amp move state means the move failed. */\n\t\tchan->move_id = chan->local_amp_id;\n\t\tl2cap_move_done(chan);\n\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_UNCONFIRMED);\n\t}\n\tl2cap_chan_unlock(chan);\n}",
        "bug_line_number": [
            77,
            78
        ],
        "vul": 1,
        "id": 276
    },
    {
        "code": "static struct tc_u_knode *u32_init_knode(struct net *net, struct tcf_proto *tp,\n\t\t\t\t\t struct tc_u_knode *n)\n{\n\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);\n\tstruct tc_u32_sel *s = &n->sel;\n\tstruct tc_u_knode *new;\n\tnew = kzalloc(struct_size(new, sel.keys, s->nkeys), GFP_KERNEL);\n\tif (!new)\n\t\treturn NULL;\n\tRCU_INIT_POINTER(new->next, n->next);\n\tnew->handle = n->handle;\n\tRCU_INIT_POINTER(new->ht_up, n->ht_up);\n\tnew->ifindex = n->ifindex;\n\tnew->fshift = n->fshift;\n\tnew->res = n->res;\n\tnew->flags = n->flags;\n\tRCU_INIT_POINTER(new->ht_down, ht);\n#ifdef CONFIG_CLS_U32_PERF\n\t/* Statistics may be incremented by readers during update\n\t * so we must keep them in tact. When the node is later destroyed\n\t * a special destroy call must be made to not free the pf memory.\n\t */\n\tnew->pf = n->pf;\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tnew->val = n->val;\n\tnew->mask = n->mask;\n\t/* Similarly success statistics must be moved as pointers */\n\tnew->pcpu_success = n->pcpu_success;\n#endif\n\tmemcpy(&new->sel, s, struct_size(s, keys, s->nkeys));\n\tif (tcf_exts_init(&new->exts, net, TCA_U32_ACT, TCA_U32_POLICE)) {\n\t\tkfree(new);\n\t\treturn NULL;\n\t}\n\t/* bump reference count as long as we hold pointer to structure */\n\tif (ht)\n\t\tht->refcnt++;\n\treturn new;\n}",
        "bug_line_number": [
            14
        ],
        "vul": 1,
        "id": 278
    },
    {
        "code": "int saa7134_ts_fini(struct saa7134_dev *dev)\n{\n\tsaa7134_pgtable_free(dev->pci, &dev->ts_q.pt);\n\treturn 0;\n}",
        "bug_line_number": [
            1,
            2
        ],
        "vul": 1,
        "id": 280
    },
    {
        "code": "static void __net_exit nf_tables_exit_net(struct net *net)\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(net);\n\tmutex_lock(&nft_net->commit_mutex);\n\tif (!list_empty(&nft_net->commit_list) ||\n\t    !list_empty(&nft_net->module_list))\n\t\t__nf_tables_abort(net, NFNL_ABORT_NONE);\n\t__nft_release_tables(net);\n\tmutex_unlock(&nft_net->commit_mutex);\n\tWARN_ON_ONCE(!list_empty(&nft_net->tables));\n\tWARN_ON_ONCE(!list_empty(&nft_net->module_list));\n\tWARN_ON_ONCE(!list_empty(&nft_net->notify_list));\n}",
        "bug_line_number": [
            2,
            3,
            4,
            7,
            8
        ],
        "vul": 1,
        "id": 282
    },
    {
        "code": "struct tpm_chip *tpm_chip_alloc(struct device *pdev,\n\t\t\t\tconst struct tpm_class_ops *ops)\n{\n\tstruct tpm_chip *chip;\n\tint rc;\n\tchip = kzalloc(sizeof(*chip), GFP_KERNEL);\n\tif (chip == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\tmutex_init(&chip->tpm_mutex);\n\tinit_rwsem(&chip->ops_sem);\n\tchip->ops = ops;\n\tmutex_lock(&idr_lock);\n\trc = idr_alloc(&dev_nums_idr, NULL, 0, TPM_NUM_DEVICES, GFP_KERNEL);\n\tmutex_unlock(&idr_lock);\n\tif (rc < 0) {\n\t\tdev_err(pdev, \"No available tpm device numbers\\n\");\n\t\tkfree(chip);\n\t\treturn ERR_PTR(rc);\n\t}\n\tchip->dev_num = rc;\n\tdevice_initialize(&chip->dev);\n\tdevice_initialize(&chip->devs);\n\tchip->dev.class = tpm_class;\n\tchip->dev.class->shutdown_pre = tpm_class_shutdown;\n\tchip->dev.release = tpm_dev_release;\n\tchip->dev.parent = pdev;\n\tchip->dev.groups = chip->groups;\n\tchip->devs.parent = pdev;\n\tchip->devs.class = tpmrm_class;\n\tchip->devs.release = tpm_devs_release;\n\t/* get extra reference on main device to hold on\n\t * behalf of devs.  This holds the chip structure\n\t * while cdevs is in use.  The corresponding put\n\t * is in the tpm_devs_release (TPM2 only)\n\t */\n\tif (chip->flags & TPM_CHIP_FLAG_TPM2)\n\t\tget_device(&chip->dev);\n\tif (chip->dev_num == 0)\n\t\tchip->dev.devt = MKDEV(MISC_MAJOR, TPM_MINOR);\n\telse\n\t\tchip->dev.devt = MKDEV(MAJOR(tpm_devt), chip->dev_num);\n\tchip->devs.devt =\n\t\tMKDEV(MAJOR(tpm_devt), chip->dev_num + TPM_NUM_DEVICES);\n\trc = dev_set_name(&chip->dev, \"tpm%d\", chip->dev_num);\n\tif (rc)\n\t\tgoto out;\n\trc = dev_set_name(&chip->devs, \"tpmrm%d\", chip->dev_num);\n\tif (rc)\n\t\tgoto out;\n\tif (!pdev)\n\t\tchip->flags |= TPM_CHIP_FLAG_VIRTUAL;\n\tcdev_init(&chip->cdev, &tpm_fops);\n\tcdev_init(&chip->cdevs, &tpmrm_fops);\n\tchip->cdev.owner = THIS_MODULE;\n\tchip->cdevs.owner = THIS_MODULE;\n\trc = tpm2_init_space(&chip->work_space, TPM2_SPACE_BUFFER_SIZE);\n\tif (rc) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\tchip->locality = -1;\n\treturn chip;\nout:\n\tput_device(&chip->devs);\n\tput_device(&chip->dev);\n\treturn ERR_PTR(rc);\n}",
        "bug_line_number": [
            21,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            41,
            42,
            44,
            45,
            46,
            52,
            54,
            63
        ],
        "vul": 1,
        "id": 284
    },
    {
        "code": "static struct cmd_obj *cmd_hdl_filter(struct _adapter *padapter,\n\t\t\t\t      struct cmd_obj *pcmd)\n{\n\tstruct cmd_obj *pcmd_r;\n\tif (!pcmd)\n\t\treturn pcmd;\n\tpcmd_r = NULL;\n\tswitch (pcmd->cmdcode) {\n\tcase GEN_CMD_CODE(_Read_MACREG):\n\t\tread_macreg_hdl(padapter, (u8 *)pcmd);\n\t\tpcmd_r = pcmd;\n\t\tbreak;\n\tcase GEN_CMD_CODE(_Write_MACREG):\n\t\twrite_macreg_hdl(padapter, (u8 *)pcmd);\n\t\tpcmd_r = pcmd;\n\t\tbreak;\n\tcase GEN_CMD_CODE(_Read_BBREG):\n\t\tread_bbreg_hdl(padapter, (u8 *)pcmd);\n\t\tbreak;\n\tcase GEN_CMD_CODE(_Write_BBREG):\n\t\twrite_bbreg_hdl(padapter, (u8 *)pcmd);\n\t\tbreak;\n\tcase GEN_CMD_CODE(_Read_RFREG):\n\t\tread_rfreg_hdl(padapter, (u8 *)pcmd);\n\t\tbreak;\n\tcase GEN_CMD_CODE(_Write_RFREG):\n\t\twrite_rfreg_hdl(padapter, (u8 *)pcmd);\n\t\tbreak;\n\tcase GEN_CMD_CODE(_SetUsbSuspend):\n\t\tsys_suspend_hdl(padapter, (u8 *)pcmd);\n\t\tbreak;\n\tcase GEN_CMD_CODE(_JoinBss):\n\t\tr8712_joinbss_reset(padapter);\n\t\t/* Before set JoinBss_CMD to FW, driver must ensure FW is in\n\t\t * PS_MODE_ACTIVE. Directly write rpwm to radio on and assign\n\t\t * new pwr_mode to Driver, instead of use workitem to change\n\t\t * state.\n\t\t */\n\t\tif (padapter->pwrctrlpriv.pwr_mode > PS_MODE_ACTIVE) {\n\t\t\tpadapter->pwrctrlpriv.pwr_mode = PS_MODE_ACTIVE;\n\t\t\tmutex_lock(&padapter->pwrctrlpriv.mutex_lock);\n\t\t\tr8712_set_rpwm(padapter, PS_STATE_S4);\n\t\t\tmutex_unlock(&padapter->pwrctrlpriv.mutex_lock);\n\t\t}\n\t\tpcmd_r = pcmd;\n\t\tbreak;\n\tcase _DRV_INT_CMD_:\n\t\tr871x_internal_cmd_hdl(padapter, pcmd->parmbuf);\n\t\tr8712_free_cmd_obj(pcmd);\n\t\tpcmd_r = NULL;\n\t\tbreak;\n\tdefault:\n\t\tpcmd_r = pcmd;\n\t\tbreak;\n\t}\n\treturn pcmd_r; /* if returning pcmd_r == NULL, pcmd must be free. */\n}",
        "bug_line_number": [
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15
        ],
        "vul": 1,
        "id": 286
    },
    {
        "code": "static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n\t\t     struct file *tfile, int fd, int full_check)\n{\n\tint error, pwake = 0;\n\t__poll_t revents;\n\tlong user_watches;\n\tstruct epitem *epi;\n\tstruct ep_pqueue epq;\n\tlockdep_assert_irqs_enabled();\n\tuser_watches = atomic_long_read(&ep->user->epoll_watches);\n\tif (unlikely(user_watches >= max_user_watches))\n\t\treturn -ENOSPC;\n\tif (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\t/* Item initialization follow here ... */\n\tINIT_LIST_HEAD(&epi->rdllink);\n\tINIT_LIST_HEAD(&epi->fllink);\n\tINIT_LIST_HEAD(&epi->pwqlist);\n\tepi->ep = ep;\n\tep_set_ffd(&epi->ffd, tfile, fd);\n\tepi->event = *event;\n\tepi->nwait = 0;\n\tepi->next = EP_UNACTIVE_PTR;\n\tif (epi->event.events & EPOLLWAKEUP) {\n\t\terror = ep_create_wakeup_source(epi);\n\t\tif (error)\n\t\t\tgoto error_create_wakeup_source;\n\t} else {\n\t\tRCU_INIT_POINTER(epi->ws, NULL);\n\t}\n\t/* Initialize the poll table using the queue callback */\n\tepq.epi = epi;\n\tinit_poll_funcptr(&epq.pt, ep_ptable_queue_proc);\n\t/*\n\t * Attach the item to the poll hooks and get current event bits.\n\t * We can safely use the file* here because its usage count has\n\t * been increased by the caller of this function. Note that after\n\t * this operation completes, the poll callback can start hitting\n\t * the new item.\n\t */\n\trevents = ep_item_poll(epi, &epq.pt, 1);\n\t/*\n\t * We have to check if something went wrong during the poll wait queue\n\t * install process. Namely an allocation for a wait queue failed due\n\t * high memory pressure.\n\t */\n\terror = -ENOMEM;\n\tif (epi->nwait < 0)\n\t\tgoto error_unregister;\n\t/* Add the current item to the list of active epoll hook for this file */\n\tspin_lock(&tfile->f_lock);\n\tlist_add_tail_rcu(&epi->fllink, &tfile->f_ep_links);\n\tspin_unlock(&tfile->f_lock);\n\t/*\n\t * Add the current item to the RB tree. All RB tree operations are\n\t * protected by \"mtx\", and ep_insert() is called with \"mtx\" held.\n\t */\n\tep_rbtree_insert(ep, epi);\n\t/* now check if we've created too many backpaths */\n\terror = -EINVAL;\n\tif (full_check && reverse_path_check())\n\t\tgoto error_remove_epi;\n\t/* We have to drop the new item inside our item list to keep track of it */\n\twrite_lock_irq(&ep->lock);\n\t/* record NAPI ID of new item if present */\n\tep_set_busy_poll_napi_id(epi);\n\t/* If the file is already \"ready\" we drop it inside the ready list */\n\tif (revents && !ep_is_linked(epi)) {\n\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n\t\tep_pm_stay_awake(epi);\n\t\t/* Notify waiting tasks that events are available */\n\t\tif (waitqueue_active(&ep->wq))\n\t\t\twake_up(&ep->wq);\n\t\tif (waitqueue_active(&ep->poll_wait))\n\t\t\tpwake++;\n\t}\n\twrite_unlock_irq(&ep->lock);\n\tatomic_long_inc(&ep->user->epoll_watches);\n\t/* We have to call this outside the lock */\n\tif (pwake)\n\t\tep_poll_safewake(ep, NULL);\n\treturn 0;\nerror_remove_epi:\n\tspin_lock(&tfile->f_lock);\n\tlist_del_rcu(&epi->fllink);\n\tspin_unlock(&tfile->f_lock);\n\trb_erase_cached(&epi->rbn, &ep->rbr);\nerror_unregister:\n\tep_unregister_pollwait(ep, epi);\n\t/*\n\t * We need to do this because an event could have been arrived on some\n\t * allocated wait queue. Note that we don't care about the ep->ovflist\n\t * list, since that is used/cleaned only inside a section bound by \"mtx\".\n\t * And ep_insert() is called with \"mtx\" held.\n\t */\n\twrite_lock_irq(&ep->lock);\n\tif (ep_is_linked(epi))\n\t\tlist_del_init(&epi->rdllink);\n\twrite_unlock_irq(&ep->lock);",
        "bug_line_number": [
            29,
            30,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            81,
            82,
            87,
            88
        ],
        "vul": 1,
        "id": 288
    },
    {
        "code": "void snd_usb_mixer_disconnect(struct usb_mixer_interface *mixer)\n{\n\tusb_kill_urb(mixer->urb);\n\tusb_kill_urb(mixer->rc_urb);\n}",
        "bug_line_number": [
            2,
            3,
            4
        ],
        "vul": 1,
        "id": 290
    },
    {
        "code": "int xenvif_connect_data(struct xenvif_queue *queue,\n\t\t\tunsigned long tx_ring_ref,\n\t\t\tunsigned long rx_ring_ref,\n\t\t\tunsigned int tx_evtchn,\n\t\t\tunsigned int rx_evtchn)\n{\n\tstruct xenbus_device *dev = xenvif_to_xenbus_device(queue->vif);\n\tstruct task_struct *task;\n\tint err;\n\tBUG_ON(queue->tx_irq);\n\tBUG_ON(queue->task);\n\tBUG_ON(queue->dealloc_task);\n\terr = xenvif_map_frontend_data_rings(queue, tx_ring_ref,\n\t\t\t\t\t     rx_ring_ref);\n\tif (err < 0)\n\t\tgoto err;\n\tinit_waitqueue_head(&queue->wq);\n\tinit_waitqueue_head(&queue->dealloc_wq);\n\tatomic_set(&queue->inflight_packets, 0);\n\tnetif_napi_add(queue->vif->dev, &queue->napi, xenvif_poll,\n\t\t\tXENVIF_NAPI_WEIGHT);\n\tqueue->stalled = true;\n\ttask = kthread_run(xenvif_kthread_guest_rx, queue,\n\t\t\t   \"%s-guest-rx\", queue->name);\n\tif (IS_ERR(task))\n\t\tgoto kthread_err;\n\tqueue->task = task;\n\ttask = kthread_run(xenvif_dealloc_kthread, queue,\n\t\t\t   \"%s-dealloc\", queue->name);\n\tif (IS_ERR(task))\n\t\tgoto kthread_err;\n\tqueue->dealloc_task = task;\n\tif (tx_evtchn == rx_evtchn) {\n\t\t/* feature-split-event-channels == 0 */\n\t\terr = bind_interdomain_evtchn_to_irqhandler_lateeoi(\n\t\t\tdev, tx_evtchn, xenvif_interrupt, 0,\n\t\t\tqueue->name, queue);\n\t\tif (err < 0)\n\t\t\tgoto err;\n\t\tqueue->tx_irq = queue->rx_irq = err;\n\t\tdisable_irq(queue->tx_irq);\n\t} else {\n\t\t/* feature-split-event-channels == 1 */\n\t\tsnprintf(queue->tx_irq_name, sizeof(queue->tx_irq_name),\n\t\t\t \"%s-tx\", queue->name);\n\t\terr = bind_interdomain_evtchn_to_irqhandler_lateeoi(\n\t\t\tdev, tx_evtchn, xenvif_tx_interrupt, 0,\n\t\t\tqueue->tx_irq_name, queue);\n\t\tif (err < 0)\n\t\t\tgoto err;\n\t\tqueue->tx_irq = err;\n\t\tdisable_irq(queue->tx_irq);\n\t\tsnprintf(queue->rx_irq_name, sizeof(queue->rx_irq_name),\n\t\t\t \"%s-rx\", queue->name);\n\t\terr = bind_interdomain_evtchn_to_irqhandler_lateeoi(\n\t\t\tdev, rx_evtchn, xenvif_rx_interrupt, 0,\n\t\t\tqueue->rx_irq_name, queue);\n\t\tif (err < 0)\n\t\t\tgoto err;\n\t\tqueue->rx_irq = err;\n\t\tdisable_irq(queue->rx_irq);\n\t}\n\treturn 0;\nkthread_err:\n\tpr_warn(\"Could not allocate kthread for %s\\n\", queue->name);\n\terr = PTR_ERR(task);\nerr:\n\txenvif_disconnect_queue(queue);\n\treturn err;\n}",
        "bug_line_number": [
            26,
            27
        ],
        "vul": 1,
        "id": 292
    },
    {
        "code": "static int __init l2tp_eth_init(void)\n{\n\tint err = 0;\n\terr = l2tp_nl_register_ops(L2TP_PWTYPE_ETH, &l2tp_eth_nl_cmd_ops);\n\tif (err)\n\t\tgoto out;\n\terr = register_pernet_device(&l2tp_eth_net_ops);\n\tif (err)\n\t\tgoto out_unreg;\n\tpr_info(\"L2TP ethernet pseudowire support (L2TPv3)\\n\");\n\treturn 0;\nout_unreg:\n\tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);\nout:\n\treturn err;\n}",
        "bug_line_number": [
            5,
            6,
            7,
            8,
            9,
            11,
            12,
            13,
            14
        ],
        "vul": 1,
        "id": 294
    },
    {
        "code": "static int binder_proc_transaction(struct binder_transaction *t,\n\t\t\t\t    struct binder_proc *proc,\n\t\t\t\t    struct binder_thread *thread)\n{\n\tstruct binder_node *node = t->buffer->target_node;\n\tbool oneway = !!(t->flags & TF_ONE_WAY);\n\tbool pending_async = false;\n\tstruct binder_transaction *t_outdated = NULL;\n\tbool frozen = false;\n\tBUG_ON(!node);\n\tbinder_node_lock(node);\n\tif (oneway) {\n\t\tBUG_ON(thread);\n\t\tif (node->has_async_transaction)\n\t\t\tpending_async = true;\n\t\telse\n\t\t\tnode->has_async_transaction = true;\n\t}\n\tbinder_inner_proc_lock(proc);\n\tif (proc->is_frozen) {\n\t\tfrozen = true;\n\t\tproc->sync_recv |= !oneway;\n\t\tproc->async_recv |= oneway;\n\t}\n\tif ((frozen && !oneway) || proc->is_dead ||\n\t\t\t(thread && thread->is_dead)) {\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_node_unlock(node);\n\t\treturn frozen ? BR_FROZEN_REPLY : BR_DEAD_REPLY;\n\t}\n\tif (!thread && !pending_async)\n\t\tthread = binder_select_thread_ilocked(proc);\n\tif (thread) {\n\t\tbinder_enqueue_thread_work_ilocked(thread, &t->work);\n\t} else if (!pending_async) {\n\t\tbinder_enqueue_work_ilocked(&t->work, &proc->todo);\n\t} else {\n\t\tif ((t->flags & TF_UPDATE_TXN) && frozen) {\n\t\t\tt_outdated = binder_find_outdated_transaction_ilocked(t,\n\t\t\t\t\t\t\t\t\t      &node->async_todo);\n\t\t\tif (t_outdated) {\n\t\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t\t     \"txn %d supersedes %d\\n\",\n\t\t\t\t\t     t->debug_id, t_outdated->debug_id);\n\t\t\t\tlist_del_init(&t_outdated->work.entry);\n\t\t\t\tproc->outstanding_txns--;\n\t\t\t}\n\t\t}\n\t\tbinder_enqueue_work_ilocked(&t->work, &node->async_todo);\n\t}\n\tif (!pending_async)\n\t\tbinder_wakeup_thread_ilocked(proc, thread, !oneway /* sync */);\n\tproc->outstanding_txns++;\n\tbinder_inner_proc_unlock(proc);\n\tbinder_node_unlock(node);\n\t/*\n\t * To reduce potential contention, free the outdated transaction and\n\t * buffer after releasing the locks.\n\t */\n\tif (t_outdated) {\n\t\tstruct binder_buffer *buffer = t_outdated->buffer;\n\t\tt_outdated->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t\ttrace_binder_transaction_update_buffer_release(buffer);\n\t\tbinder_transaction_buffer_release(proc, NULL, buffer, 0, 0);\n\t\tbinder_alloc_free_buf(&proc->alloc, buffer);\n\t\tkfree(t_outdated);\n\t\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n\t}\n\tif (oneway && frozen)\n\t\treturn BR_TRANSACTION_PENDING_FROZEN;\n\treturn 0;\n}",
        "bug_line_number": [
            64,
            65
        ],
        "vul": 1,
        "id": 296
    },
    {
        "code": "\t\tif (!is_dir)\n\t\t\tgoto next_attr;\n\t\tni->ni_flags |= NI_FLAG_DIR;\n\t\terr = indx_init(&ni->dir, sbi, attr, INDEX_MUTEX_I30);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tmode = sb->s_root\n\t\t\t       ? (S_IFDIR | (0777 & sbi->options->fs_dmask_inv))\n\t\t\t       : (S_IFDIR | 0777);\n\t\tgoto next_attr;\n\tcase ATTR_ALLOC:\n\t\tif (!is_root || attr->name_len != ARRAY_SIZE(I30_NAME) ||\n\t\t    memcmp(attr_name(attr), I30_NAME, sizeof(I30_NAME)))\n\t\t\tgoto next_attr;\n\t\tinode->i_size = le64_to_cpu(attr->nres.data_size);\n\t\tni->i_valid = le64_to_cpu(attr->nres.valid_size);\n\t\tinode_set_bytes(inode, le64_to_cpu(attr->nres.alloc_size));\n\t\trun = &ni->dir.alloc_run;\n\t\tbreak;\n\tcase ATTR_BITMAP:\n\t\tif (ino == MFT_REC_MFT) {\n\t\t\tif (!attr->non_res)\n\t\t\t\tgoto out;\n#ifndef CONFIG_NTFS3_64BIT_CLUSTER\n\t\t\t/* 0x20000000 = 2^32 / 8 */\n\t\t\tif (le64_to_cpu(attr->nres.alloc_size) >= 0x20000000)\n\t\t\t\tgoto out;\n#endif\n\t\t\trun = &sbi->mft.bitmap.run;\n\t\t\tbreak;\n\t\t} else if (is_dir && attr->name_len == ARRAY_SIZE(I30_NAME) &&\n\t\t\t   !memcmp(attr_name(attr), I30_NAME,\n\t\t\t\t   sizeof(I30_NAME)) &&\n\t\t\t   attr->non_res) {\n\t\t\trun = &ni->dir.bitmap_run;\n\t\t\tbreak;\n\t\t}\n\t\tgoto next_attr;\n\tcase ATTR_REPARSE:\n\t\tif (attr->name_len)\n\t\t\tgoto next_attr;\n\t\trp_fa = ni_parse_reparse(ni, attr, &rp);\n\t\tswitch (rp_fa) {\n\t\tcase REPARSE_LINK:\n\t\t\t/*\n\t\t\t * Normal symlink.\n\t\t\t * Assume one unicode symbol == one utf8.\n\t\t\t */\n\t\t\tinode->i_size = le16_to_cpu(rp.SymbolicLinkReparseBuffer\n\t\t\t\t\t\t\t    .PrintNameLength) /\n\t\t\t\t\tsizeof(u16);\n\t\t\tni->i_valid = inode->i_size;\n\t\t\t/* Clear directory bit. */\n\t\t\tif (ni->ni_flags & NI_FLAG_DIR) {\n\t\t\t\tindx_clear(&ni->dir);\n\t\t\t\tmemset(&ni->dir, 0, sizeof(ni->dir));\n\t\t\t\tni->ni_flags &= ~NI_FLAG_DIR;\n\t\t\t} else {\n\t\t\t\trun_close(&ni->file.run);\n\t\t\t}\n\t\t\tmode = S_IFLNK | 0777;\n\t\t\tis_dir = false;\n\t\t\tif (attr->non_res) {\n\t\t\t\trun = &ni->file.run;\n\t\t\t\tgoto attr_unpack_run; // Double break.\n\t\t\t}\n\t\t\tbreak;\n\t\tcase REPARSE_COMPRESSED:\n\t\t\tbreak;\n\t\tcase REPARSE_DEDUPLICATED:\n\t\t\tbreak;\n\t\t}\n\t\tgoto next_attr;\n\tcase ATTR_EA_INFO:\n\t\tif (!attr->name_len &&\n\t\t    resident_data_ex(attr, sizeof(struct EA_INFO))) {\n\t\t\tni->ni_flags |= NI_FLAG_EA;\n\t\t\t/*\n\t\t\t * ntfs_get_wsl_perm updates inode->i_uid, inode->i_gid, inode->i_mode\n\t\t\t */\n\t\t\tinode->i_mode = mode;\n\t\t\tntfs_get_wsl_perm(inode);\n\t\t\tmode = inode->i_mode;\n\t\t}\n\t\tgoto next_attr;\n\tdefault:\n\t\tgoto next_attr;\n\t}\nattr_unpack_run:\n\troff = le16_to_cpu(attr->nres.run_off);\n\tif (roff > asize) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tt64 = le64_to_cpu(attr->nres.svcn);\n\terr = run_unpack_ex(run, sbi, ino, t64, le64_to_cpu(attr->nres.evcn),\n\t\t\t    t64, Add2Ptr(attr, roff), asize - roff);\n\tif (err < 0)\n\t\tgoto out;\n\terr = 0;\n\tgoto next_attr;",
        "bug_line_number": [
            94,
            95
        ],
        "vul": 1,
        "id": 298
    },
    {
        "code": "int inode_init_always(struct super_block *sb, struct inode *inode)\n{\n\tstatic const struct inode_operations empty_iops;\n\tstatic const struct file_operations no_open_fops = {.open = no_open};\n\tstruct address_space *const mapping = &inode->i_data;\n\tinode->i_sb = sb;\n\tinode->i_blkbits = sb->s_blocksize_bits;\n\tinode->i_flags = 0;\n\tatomic_set(&inode->i_count, 1);\n\tinode->i_op = &empty_iops;\n\tinode->i_fop = &no_open_fops;\n\tinode->__i_nlink = 1;\n\tinode->i_opflags = 0;\n\tif (sb->s_xattr)\n\t\tinode->i_opflags |= IOP_XATTR;\n\ti_uid_write(inode, 0);\n\ti_gid_write(inode, 0);\n\tatomic_set(&inode->i_writecount, 0);\n\tinode->i_size = 0;\n\tinode->i_write_hint = WRITE_LIFE_NOT_SET;\n\tinode->i_blocks = 0;\n\tinode->i_bytes = 0;\n\tinode->i_generation = 0;\n\tinode->i_pipe = NULL;\n\tinode->i_bdev = NULL;\n\tinode->i_cdev = NULL;\n\tinode->i_link = NULL;\n\tinode->i_dir_seq = 0;\n\tinode->i_rdev = 0;\n\tinode->dirtied_when = 0;\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tinode->i_wb_frn_winner = 0;\n\tinode->i_wb_frn_avg_time = 0;\n\tinode->i_wb_frn_history = 0;\n#endif\n\tif (security_inode_alloc(inode))\n\t\tgoto out;\n\tspin_lock_init(&inode->i_lock);\n\tlockdep_set_class(&inode->i_lock, &sb->s_type->i_lock_key);\n\tinit_rwsem(&inode->i_rwsem);\n\tlockdep_set_class(&inode->i_rwsem, &sb->s_type->i_mutex_key);\n\tatomic_set(&inode->i_dio_count, 0);\n\tmapping->a_ops = &empty_aops;\n\tmapping->host = inode;\n\tmapping->flags = 0;\n\tmapping->wb_err = 0;\n\tatomic_set(&mapping->i_mmap_writable, 0);\n#ifdef CONFIG_READ_ONLY_THP_FOR_FS\n\tatomic_set(&mapping->nr_thps, 0);\n#endif\n\tmapping_set_gfp_mask(mapping, GFP_HIGHUSER_MOVABLE);\n\tmapping->private_data = NULL;\n\tmapping->writeback_index = 0;\n\tinode->i_private = NULL;\n\tinode->i_mapping = mapping;\n\tINIT_HLIST_HEAD(&inode->i_dentry);\t/* buggered by rcu freeing */\n#ifdef CONFIG_FS_POSIX_ACL\n\tinode->i_acl = inode->i_default_acl = ACL_NOT_CACHED;\n#endif\n#ifdef CONFIG_FSNOTIFY\n\tinode->i_fsnotify_mask = 0;\n#endif\n\tinode->i_flctx = NULL;\n\tthis_cpu_inc(nr_inodes);\n\treturn 0;\nout:\n\treturn -ENOMEM;\n}",
        "bug_line_number": [
            7,
            8
        ],
        "vul": 1,
        "id": 300
    },
    {
        "code": "static int selinux_msg_queue_msgrcv(struct kern_ipc_perm *msq, struct msg_msg *msg,\n\t\t\t\t    struct task_struct *target,\n\t\t\t\t    long type, int mode)\n{\n\tstruct ipc_security_struct *isec;\n\tstruct msg_security_struct *msec;\n\tstruct common_audit_data ad;\n\tu32 sid = task_sid_subj(target);\n\tint rc;\n\tisec = selinux_ipc(msq);\n\tmsec = selinux_msg_msg(msg);\n\tad.type = LSM_AUDIT_DATA_IPC;\n\tad.u.ipc_id = msq->key;\n\trc = avc_has_perm(&selinux_state,\n\t\t\t  sid, isec->sid,\n\t\t\t  SECCLASS_MSGQ, MSGQ__READ, &ad);\n\tif (!rc)\n\t\trc = avc_has_perm(&selinux_state,\n\t\t\t\t  sid, msec->sid,\n\t\t\t\t  SECCLASS_MSG, MSG__RECEIVE, &ad);\n\treturn rc;\n}",
        "bug_line_number": [
            7,
            8
        ],
        "vul": 1,
        "id": 302
    },
    {
        "code": "static int\ntcpmss_mangle_packet(struct sk_buff *skb,\n\t\t     const struct xt_action_param *par,\n\t\t     unsigned int family,\n\t\t     unsigned int tcphoff,\n\t\t     unsigned int minlen)\n{\n\tconst struct xt_tcpmss_info *info = par->targinfo;\n\tstruct tcphdr *tcph;\n\tint len, tcp_hdrlen;\n\tunsigned int i;\n\t__be16 oldval;\n\tu16 newmss;\n\tu8 *opt;\n\t/* This is a fragment, no TCP header is available */\n\tif (par->fragoff != 0)\n\t\treturn 0;\n\tif (!skb_make_writable(skb, skb->len))\n\t\treturn -1;\n\tlen = skb->len - tcphoff;\n\tif (len < (int)sizeof(struct tcphdr))\n\t\treturn -1;\n\ttcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);\n\ttcp_hdrlen = tcph->doff * 4;\n\tif (len < tcp_hdrlen)\n\t\treturn -1;\n\tif (info->mss == XT_TCPMSS_CLAMP_PMTU) {\n\t\tstruct net *net = xt_net(par);\n\t\tunsigned int in_mtu = tcpmss_reverse_mtu(net, skb, family);\n\t\tunsigned int min_mtu = min(dst_mtu(skb_dst(skb)), in_mtu);\n\t\tif (min_mtu <= minlen) {\n\t\t\tnet_err_ratelimited(\"unknown or invalid path-MTU (%u)\\n\",\n\t\t\t\t\t    min_mtu);\n\t\t\treturn -1;\n\t\t}\n\t\tnewmss = min_mtu - minlen;\n\t} else\n\t\tnewmss = info->mss;\n\topt = (u_int8_t *)tcph;\n\tfor (i = sizeof(struct tcphdr); i <= tcp_hdrlen - TCPOLEN_MSS; i += optlen(opt, i)) {\n\t\tif (opt[i] == TCPOPT_MSS && opt[i+1] == TCPOLEN_MSS) {\n\t\t\tu_int16_t oldmss;\n\t\t\toldmss = (opt[i+2] << 8) | opt[i+3];\n\t\t\t/* Never increase MSS, even when setting it, as\n\t\t\t * doing so results in problems for hosts that rely\n\t\t\t * on MSS being set correctly.\n\t\t\t */\n\t\t\tif (oldmss <= newmss)\n\t\t\t\treturn 0;\n\t\t\topt[i+2] = (newmss & 0xff00) >> 8;\n\t\t\topt[i+3] = newmss & 0x00ff;\n\t\t\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t\t\t htons(oldmss), htons(newmss),\n\t\t\t\t\t\t false);\n\t\t\treturn 0;\n\t\t}\n\t}\n\t/* There is data after the header so the option can't be added\n\t * without moving it, and doing so may make the SYN packet\n\t * itself too large. Accept the packet unmodified instead.\n\t */\n\tif (len > tcp_hdrlen)\n\t\treturn 0;\n\t/*\n\t * MSS Option not found ?! add it..\n\t */\n\tif (skb_tailroom(skb) < TCPOLEN_MSS) {\n\t\tif (pskb_expand_head(skb, 0,\n\t\t\t\t     TCPOLEN_MSS - skb_tailroom(skb),\n\t\t\t\t     GFP_ATOMIC))\n\t\t\treturn -1;\n\t\ttcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);\n\t}\n\tskb_put(skb, TCPOLEN_MSS);\n\t/*\n\t * IPv4: RFC 1122 states \"If an MSS option is not received at\n\t * connection setup, TCP MUST assume a default send MSS of 536\".\n\t * IPv6: RFC 2460 states IPv6 has a minimum MTU of 1280 and a minimum\n\t * length IPv6 header of 60, ergo the default MSS value is 1220\n\t * Since no MSS was provided, we must use the default values\n\t */\n\tif (xt_family(par) == NFPROTO_IPV4)\n\t\tnewmss = min(newmss, (u16)536);\n\telse\n\t\tnewmss = min(newmss, (u16)1220);\n\topt = (u_int8_t *)tcph + sizeof(struct tcphdr);\n\tmemmove(opt + TCPOLEN_MSS, opt, len - sizeof(struct tcphdr));\n\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t htons(len), htons(len + TCPOLEN_MSS), true);\n\topt[0] = TCPOPT_MSS;\n\topt[1] = TCPOLEN_MSS;\n\topt[2] = (newmss & 0xff00) >> 8;\n\topt[3] = newmss & 0x00ff;\n\tinet_proto_csum_replace4(&tcph->check, skb, 0, *((__be32 *)opt), false);\n\toldval = ((__be16 *)tcph)[6];\n\ttcph->doff += TCPOLEN_MSS/4;\n\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t oldval, ((__be16 *)tcph)[6], false);\n\treturn TCPOLEN_MSS;\n}",
        "bug_line_number": [
            24,
            25,
            62,
            63
        ],
        "vul": 1,
        "id": 304
    },
    {
        "code": "static int\nprinter_open(struct inode *inode, struct file *fd)\n{\n\tstruct printer_dev\t*dev;\n\tunsigned long\t\tflags;\n\tint\t\t\tret = -EBUSY;\n\tdev = container_of(inode->i_cdev, struct printer_dev, printer_cdev);\n\tspin_lock_irqsave(&dev->lock, flags);\n\tif (dev->interface < 0) {\n\t\tspin_unlock_irqrestore(&dev->lock, flags);\n\t\treturn -ENODEV;\n\t}\n\tif (!dev->printer_cdev_open) {\n\t\tdev->printer_cdev_open = 1;\n\t\tfd->private_data = dev;\n\t\tret = 0;\n\t\t/* Change the printer status to show that it's on-line. */\n\t\tdev->printer_status |= PRINTER_SELECTED;\n\t}\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\tDBG(dev, \"printer_open returned %x\\n\", ret);\n\treturn ret;\n}",
        "bug_line_number": [
            19,
            20
        ],
        "vul": 1,
        "id": 306
    },
    {
        "code": "static long btrfs_ioctl_qgroup_assign(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_qgroup_assign_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\tif (sa->assign) {\n\t\tret = btrfs_add_qgroup_relation(trans, sa->src, sa->dst);\n\t} else {\n\t\tret = btrfs_del_qgroup_relation(trans, sa->src, sa->dst);\n\t}\n\t/* update qgroup status and info */\n\terr = btrfs_run_qgroups(trans);\n\tif (err < 0)\n\t\tbtrfs_handle_fs_error(fs_info, err,\n\t\t\t\t      \"failed to update qgroup status and info\");\n\terr = btrfs_end_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}",
        "bug_line_number": [
            29,
            30,
            31
        ],
        "vul": 1,
        "id": 308
    },
    {
        "code": "int hns_nic_net_xmit_hw(struct net_device *ndev,\n\t\t\tstruct sk_buff *skb,\n\t\t\tstruct hns_nic_ring_data *ring_data)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct device *dev = ring_to_dev(ring);\n\tstruct netdev_queue *dev_queue;\n\tstruct skb_frag_struct *frag;\n\tint buf_num;\n\tint seg_num;\n\tdma_addr_t dma;\n\tint size, next_to_use;\n\tint i;\n\tswitch (priv->ops.maybe_stop_tx(&skb, &buf_num, ring)) {\n\tcase -EBUSY:\n\t\tring->stats.tx_busy++;\n\t\tgoto out_net_tx_busy;\n\tcase -ENOMEM:\n\t\tring->stats.sw_err_cnt++;\n\t\tnetdev_err(ndev, \"no memory to xmit!\\n\");\n\t\tgoto out_err_tx_ok;\n\tdefault:\n\t\tbreak;\n\t}\n\t/* no. of segments (plus a header) */\n\tseg_num = skb_shinfo(skb)->nr_frags + 1;\n\tnext_to_use = ring->next_to_use;\n\t/* fill the first part */\n\tsize = skb_headlen(skb);\n\tdma = dma_map_single(dev, skb->data, size, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, dma)) {\n\t\tnetdev_err(ndev, \"TX head DMA map failed\\n\");\n\t\tring->stats.sw_err_cnt++;\n\t\tgoto out_err_tx_ok;\n\t}\n\tpriv->ops.fill_desc(ring, skb, size, dma, seg_num == 1 ? 1 : 0,\n\t\t\t    buf_num, DESC_TYPE_SKB, ndev->mtu);\n\t/* fill the fragments */\n\tfor (i = 1; i < seg_num; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i - 1];\n\t\tsize = skb_frag_size(frag);\n\t\tdma = skb_frag_dma_map(dev, frag, 0, size, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, dma)) {\n\t\t\tnetdev_err(ndev, \"TX frag(%d) DMA map failed\\n\", i);\n\t\t\tring->stats.sw_err_cnt++;\n\t\t\tgoto out_map_frag_fail;\n\t\t}\n\t\tpriv->ops.fill_desc(ring, skb_frag_page(frag), size, dma,\n\t\t\t\t    seg_num - 1 == i ? 1 : 0, buf_num,\n\t\t\t\t    DESC_TYPE_PAGE, ndev->mtu);\n\t}\n\t/*complete translate all packets*/\n\tdev_queue = netdev_get_tx_queue(ndev, skb->queue_mapping);\n\tnetdev_tx_sent_queue(dev_queue, skb->len);\n\twmb(); /* commit all data before submit */\n\tassert(skb->queue_mapping < priv->ae_handle->q_num);\n\thnae_queue_xmit(priv->ae_handle->qs[skb->queue_mapping], buf_num);\n\tring->stats.tx_pkts++;\n\tring->stats.tx_bytes += skb->len;\n\treturn NETDEV_TX_OK;\nout_map_frag_fail:\n\twhile (ring->next_to_use != next_to_use) {\n\t\tunfill_desc(ring);\n\t\tif (ring->next_to_use != next_to_use)\n\t\t\tdma_unmap_page(dev,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].dma,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].length,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_single(dev,\n\t\t\t\t\t ring->desc_cb[next_to_use].dma,\n\t\t\t\t\t ring->desc_cb[next_to_use].length,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t}\nout_err_tx_ok:\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\nout_net_tx_busy:\n\tnetif_stop_subqueue(ndev, skb->queue_mapping);\n\t/* Herbert's original patch had:\n\t *  smp_mb__after_netif_stop_queue();\n\t * but since that doesn't exist yet, just open code it.\n\t */\n\tsmp_mb();\n\treturn NETDEV_TX_BUSY;\n}",
        "bug_line_number": [
            0,
            1,
            2,
            3,
            54,
            55
        ],
        "vul": 1,
        "id": 310
    },
    {
        "code": "static int tiocspgrp(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\tstruct pid *pgrp;\n\tpid_t pgrp_nr;\n\tint retval = tty_check_change(real_tty);\n\tif (retval == -EIO)\n\t\treturn -ENOTTY;\n\tif (retval)\n\t\treturn retval;\n\tif (!current->signal->tty ||\n\t    (current->signal->tty != real_tty) ||\n\t    (real_tty->session != task_session(current)))\n\t\treturn -ENOTTY;\n\tif (get_user(pgrp_nr, p))\n\t\treturn -EFAULT;\n\tif (pgrp_nr < 0)\n\t\treturn -EINVAL;\n\trcu_read_lock();\n\tpgrp = find_vpid(pgrp_nr);\n\tretval = -ESRCH;\n\tif (!pgrp)\n\t\tgoto out_unlock;\n\tretval = -EPERM;\n\tif (session_of_pgrp(pgrp) != task_session(current))\n\t\tgoto out_unlock;\n\tretval = 0;\n\tspin_lock_irq(&real_tty->ctrl_lock);\n\tput_pid(real_tty->pgrp);\n\treal_tty->pgrp = get_pid(pgrp);\n\tspin_unlock_irq(&real_tty->ctrl_lock);\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}",
        "bug_line_number": [
            9,
            10,
            11,
            12,
            16,
            17,
            26,
            29,
            31,
            32
        ],
        "vul": 1,
        "id": 312
    },
    {
        "code": "static struct nft_expr *nft_expr_init(const struct nft_ctx *ctx,\n\t\t\t\t      const struct nlattr *nla)\n{\n\tstruct nft_expr_info expr_info;\n\tstruct nft_expr *expr;\n\tstruct module *owner;\n\tint err;\n\terr = nf_tables_expr_parse(ctx, nla, &expr_info);\n\tif (err < 0)\n\t\tgoto err1;\n\terr = -ENOMEM;\n\texpr = kzalloc(expr_info.ops->size, GFP_KERNEL_ACCOUNT);\n\tif (expr == NULL)\n\t\tgoto err2;\n\terr = nf_tables_newexpr(ctx, &expr_info, expr);\n\tif (err < 0)\n\t\tgoto err3;\n\treturn expr;\nerr3:\n\tkfree(expr);\nerr2:\n\towner = expr_info.ops->type->owner;\n\tif (expr_info.ops->type->release_ops)\n\t\texpr_info.ops->type->release_ops(expr_info.ops);\n\tmodule_put(owner);\nerr1:\n\treturn ERR_PTR(err);\n}",
        "bug_line_number": [
            9,
            10,
            13,
            14,
            16,
            17,
            18,
            19,
            20,
            21,
            25,
            26
        ],
        "vul": 1,
        "id": 314
    },
    {
        "code": "void btf_dump__free(struct btf_dump *d)\n{\n\tint i;\n\tif (IS_ERR_OR_NULL(d))\n\t\treturn;\n\tfree(d->type_states);\n\tif (d->cached_names) {\n\t\t/* any set cached name is owned by us and should be freed */\n\t\tfor (i = 0; i <= d->last_id; i++) {\n\t\t\tif (d->cached_names[i])\n\t\t\t\tfree((void *)d->cached_names[i]);\n\t\t}\n\t}\n\tfree(d->cached_names);\n\tfree(d->emit_queue);\n\tfree(d->decl_stack);\n\thashmap__free(d->type_names);\n\thashmap__free(d->ident_names);\n\tfree(d);\n}",
        "bug_line_number": [
            16,
            17,
            18
        ],
        "vul": 1,
        "id": 316
    },
    {
        "code": "static int ipxitf_ioctl(unsigned int cmd, void __user *arg)\n{\n\tint rc = -EINVAL;\n\tstruct ifreq ifr;\n\tint val;\n\tswitch (cmd) {\n\tcase SIOCSIFADDR: {\n\t\tstruct sockaddr_ipx *sipx;\n\t\tstruct ipx_interface_definition f;\n\t\trc = -EFAULT;\n\t\tif (copy_from_user(&ifr, arg, sizeof(ifr)))\n\t\t\tbreak;\n\t\tsipx = (struct sockaddr_ipx *)&ifr.ifr_addr;\n\t\trc = -EINVAL;\n\t\tif (sipx->sipx_family != AF_IPX)\n\t\t\tbreak;\n\t\tf.ipx_network = sipx->sipx_network;\n\t\tmemcpy(f.ipx_device, ifr.ifr_name,\n\t\t\tsizeof(f.ipx_device));\n\t\tmemcpy(f.ipx_node, sipx->sipx_node, IPX_NODE_LEN);\n\t\tf.ipx_dlink_type = sipx->sipx_type;\n\t\tf.ipx_special = sipx->sipx_special;\n\t\tif (sipx->sipx_action == IPX_DLTITF)\n\t\t\trc = ipxitf_delete(&f);\n\t\telse\n\t\t\trc = ipxitf_create(&f);\n\t\tbreak;\n\t}\n\tcase SIOCGIFADDR: {\n\t\tstruct sockaddr_ipx *sipx;\n\t\tstruct ipx_interface *ipxif;\n\t\tstruct net_device *dev;\n\t\trc = -EFAULT;\n\t\tif (copy_from_user(&ifr, arg, sizeof(ifr)))\n\t\t\tbreak;\n\t\tsipx = (struct sockaddr_ipx *)&ifr.ifr_addr;\n\t\tdev  = __dev_get_by_name(&init_net, ifr.ifr_name);\n\t\trc   = -ENODEV;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tipxif = ipxitf_find_using_phys(dev,\n\t\t\t\t\t   ipx_map_frame_type(sipx->sipx_type));\n\t\trc = -EADDRNOTAVAIL;\n\t\tif (!ipxif)\n\t\t\tbreak;\n\t\tsipx->sipx_family\t= AF_IPX;\n\t\tsipx->sipx_network\t= ipxif->if_netnum;\n\t\tmemcpy(sipx->sipx_node, ipxif->if_node,\n\t\t\tsizeof(sipx->sipx_node));\n\t\trc = -EFAULT;\n\t\tif (copy_to_user(arg, &ifr, sizeof(ifr)))\n\t\t\tbreak;\n\t\tipxitf_put(ipxif);\n\t\trc = 0;\n\t\tbreak;\n\t}\n\tcase SIOCAIPXITFCRT:\n\t\trc = -EFAULT;\n\t\tif (get_user(val, (unsigned char __user *) arg))\n\t\t\tbreak;\n\t\trc = 0;\n\t\tipxcfg_auto_create_interfaces = val;\n\t\tbreak;\n\tcase SIOCAIPXPRISLT:\n\t\trc = -EFAULT;\n\t\tif (get_user(val, (unsigned char __user *) arg))\n\t\t\tbreak;\n\t\trc = 0;\n\t\tipxcfg_set_auto_select(val);\n\t\tbreak;\n\t}\n\treturn rc;\n}",
        "bug_line_number": [
            49,
            50,
            51,
            52,
            53
        ],
        "vul": 1,
        "id": 318
    },
    {
        "code": "static int read_rindex_entry(struct gfs2_inode *ip)\n{\n\tstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\n\tconst unsigned bsize = sdp->sd_sb.sb_bsize;\n\tloff_t pos = sdp->sd_rgrps * sizeof(struct gfs2_rindex);\n\tstruct gfs2_rindex buf;\n\tint error;\n\tstruct gfs2_rgrpd *rgd;\n\tif (pos >= i_size_read(&ip->i_inode))\n\t\treturn 1;\n\terror = gfs2_internal_read(ip, (char *)&buf, &pos,\n\t\t\t\t   sizeof(struct gfs2_rindex));\n\tif (error != sizeof(struct gfs2_rindex))\n\t\treturn (error == 0) ? 1 : error;\n\trgd = kmem_cache_zalloc(gfs2_rgrpd_cachep, GFP_NOFS);\n\terror = -ENOMEM;\n\tif (!rgd)\n\t\treturn error;\n\trgd->rd_sbd = sdp;\n\trgd->rd_addr = be64_to_cpu(buf.ri_addr);\n\trgd->rd_length = be32_to_cpu(buf.ri_length);\n\trgd->rd_data0 = be64_to_cpu(buf.ri_data0);\n\trgd->rd_data = be32_to_cpu(buf.ri_data);\n\trgd->rd_bitbytes = be32_to_cpu(buf.ri_bitbytes);\n\tspin_lock_init(&rgd->rd_rsspin);\n\terror = compute_bitstructs(rgd);\n\tif (error)\n\t\tgoto fail;\n\terror = gfs2_glock_get(sdp, rgd->rd_addr,\n\t\t\t       &gfs2_rgrp_glops, CREATE, &rgd->rd_gl);\n\tif (error)\n\t\tgoto fail;\n\trgd->rd_gl->gl_object = rgd;\n\trgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;\n\trgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr + rgd->rd_length) * bsize) - 1;\n\trgd->rd_rgl = (struct gfs2_rgrp_lvb *)rgd->rd_gl->gl_lksb.sb_lvbptr;\n\trgd->rd_flags &= ~(GFS2_RDF_UPTODATE | GFS2_RDF_PREFERRED);\n\tif (rgd->rd_data > sdp->sd_max_rg_data)\n\t\tsdp->sd_max_rg_data = rgd->rd_data;\n\tspin_lock(&sdp->sd_rindex_spin);\n\terror = rgd_insert(rgd);\n\tspin_unlock(&sdp->sd_rindex_spin);\n\tif (!error)\n\t\treturn 0;\n\terror = 0; /* someone else read in the rgrp; free it and ignore it */\n\tgfs2_glock_put(rgd->rd_gl);\nfail:\n\tkfree(rgd->rd_bits);\n\tkmem_cache_free(gfs2_rgrpd_cachep, rgd);\n\treturn error;\n}",
        "bug_line_number": [
            32,
            33,
            34,
            42,
            43,
            44,
            47,
            48
        ],
        "vul": 1,
        "id": 320
    },
    {
        "code": "static struct iscsi_cls_session *\niscsi_sw_tcp_session_create(struct iscsi_endpoint *ep, uint16_t cmds_max,\n\t\t\t    uint16_t qdepth, uint32_t initial_cmdsn)\n{\n\tstruct iscsi_cls_session *cls_session;\n\tstruct iscsi_session *session;\n\tstruct iscsi_sw_tcp_host *tcp_sw_host;\n\tstruct Scsi_Host *shost;\n\tint rc;\n\tif (ep) {\n\t\tprintk(KERN_ERR \"iscsi_tcp: invalid ep %p.\\n\", ep);\n\t\treturn NULL;\n\t}\n\tshost = iscsi_host_alloc(&iscsi_sw_tcp_sht,\n\t\t\t\t sizeof(struct iscsi_sw_tcp_host), 1);\n\tif (!shost)\n\t\treturn NULL;\n\tshost->transportt = iscsi_sw_tcp_scsi_transport;\n\tshost->cmd_per_lun = qdepth;\n\tshost->max_lun = iscsi_max_lun;\n\tshost->max_id = 0;\n\tshost->max_channel = 0;\n\tshost->max_cmd_len = SCSI_MAX_VARLEN_CDB_SIZE;\n\trc = iscsi_host_get_max_scsi_cmds(shost, cmds_max);\n\tif (rc < 0)\n\t\tgoto free_host;\n\tshost->can_queue = rc;\n\tif (iscsi_host_add(shost, NULL))\n\t\tgoto free_host;\n\tcls_session = iscsi_session_setup(&iscsi_sw_tcp_transport, shost,\n\t\t\t\t\t  cmds_max, 0,\n\t\t\t\t\t  sizeof(struct iscsi_tcp_task) +\n\t\t\t\t\t  sizeof(struct iscsi_sw_tcp_hdrbuf),\n\t\t\t\t\t  initial_cmdsn, 0);\n\tif (!cls_session)\n\t\tgoto remove_host;\n\tsession = cls_session->dd_data;\n\ttcp_sw_host = iscsi_host_priv(shost);\n\ttcp_sw_host->session = session;\n\tif (iscsi_tcp_r2tpool_alloc(session))\n\t\tgoto remove_session;\n\treturn cls_session;\nremove_session:\n\tiscsi_session_teardown(cls_session);\nremove_host:\n\tiscsi_host_remove(shost, false);\nfree_host:\n\tiscsi_host_free(shost);\n\treturn NULL;\n}",
        "bug_line_number": [
            37,
            38,
            40,
            41
        ],
        "vul": 1,
        "id": 322
    },
    {
        "code": "int vmw_fence_event_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *file_priv)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct drm_vmw_fence_event_arg *arg =\n\t\t(struct drm_vmw_fence_event_arg *) data;\n\tstruct vmw_fence_obj *fence = NULL;\n\tstruct vmw_fpriv *vmw_fp = vmw_fpriv(file_priv);\n\tstruct ttm_object_file *tfile = vmw_fp->tfile;\n\tstruct drm_vmw_fence_rep __user *user_fence_rep =\n\t\t(struct drm_vmw_fence_rep __user *)(unsigned long)\n\t\targ->fence_rep;\n\tuint32_t handle;\n\tint ret;\n\t/*\n\t * Look up an existing fence object,\n\t * and if user-space wants a new reference,\n\t * add one.\n\t */\n\tif (arg->handle) {\n\t\tstruct ttm_base_object *base =\n\t\t\tvmw_fence_obj_lookup(tfile, arg->handle);\n\t\tif (IS_ERR(base))\n\t\t\treturn PTR_ERR(base);\n\t\tfence = &(container_of(base, struct vmw_user_fence,\n\t\t\t\t       base)->fence);\n\t\t(void) vmw_fence_obj_reference(fence);\n\t\tif (user_fence_rep != NULL) {\n\t\t\tret = ttm_ref_object_add(vmw_fp->tfile, base,\n\t\t\t\t\t\t NULL, false);\n\t\t\tif (unlikely(ret != 0)) {\n\t\t\t\tDRM_ERROR(\"Failed to reference a fence \"\n\t\t\t\t\t  \"object.\\n\");\n\t\t\t\tgoto out_no_ref_obj;\n\t\t\t}\n\t\t\thandle = base->handle;\n\t\t}\n\t\tttm_base_object_unref(&base);\n\t}\n\t/*\n\t * Create a new fence object.\n\t */\n\tif (!fence) {\n\t\tret = vmw_execbuf_fence_commands(file_priv, dev_priv,\n\t\t\t\t\t\t &fence,\n\t\t\t\t\t\t (user_fence_rep) ?\n\t\t\t\t\t\t &handle : NULL);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Fence event failed to create fence.\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\tBUG_ON(fence == NULL);\n\tret = vmw_event_fence_action_create(file_priv, fence,\n\t\t\t\t\t    arg->flags,\n\t\t\t\t\t    arg->user_data,\n\t\t\t\t\t    true);\n\tif (unlikely(ret != 0)) {\n\t\tif (ret != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"Failed to attach event to fence.\\n\");\n\t\tgoto out_no_create;\n\t}\n\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fp, 0, user_fence_rep, fence,\n\t\t\t\t    handle, -1, NULL);\n\tvmw_fence_obj_unreference(&fence);\n\treturn 0;\nout_no_create:\n\tif (user_fence_rep != NULL)\n\t\tttm_ref_object_base_unref(tfile, handle);\nout_no_ref_obj:\n\tvmw_fence_obj_unreference(&fence);\n\treturn ret;\n}",
        "bug_line_number": [
            63,
            64
        ],
        "vul": 1,
        "id": 324
    },
    {
        "code": "static struct kobject *cdev_get(struct cdev *p)\n{\n\tstruct module *owner = p->owner;\n\tstruct kobject *kobj;\n\tif (owner && !try_module_get(owner))\n\t\treturn NULL;\n\tkobj = kobject_get(&p->kobj);\n\tif (!kobj)\n\t\tmodule_put(owner);\n\treturn kobj;\n}",
        "bug_line_number": [
            6,
            7
        ],
        "vul": 1,
        "id": 326
    },
    {
        "code": "static void yurex_delete(struct kref *kref)\n{\n\tstruct usb_yurex *dev = to_yurex_dev(kref);\n\tdev_dbg(&dev->interface->dev, \"%s\\n\", __func__);\n\tusb_put_dev(dev->udev);\n\tif (dev->cntl_urb) {\n\t\tusb_kill_urb(dev->cntl_urb);\n\t\tkfree(dev->cntl_req);\n\t\tif (dev->cntl_buffer)\n\t\t\tusb_free_coherent(dev->udev, YUREX_BUF_SIZE,\n\t\t\t\tdev->cntl_buffer, dev->cntl_urb->transfer_dma);\n\t\tusb_free_urb(dev->cntl_urb);\n\t}\n\tif (dev->urb) {\n\t\tusb_kill_urb(dev->urb);\n\t\tif (dev->int_buffer)\n\t\t\tusb_free_coherent(dev->udev, YUREX_BUF_SIZE,\n\t\t\t\tdev->int_buffer, dev->urb->transfer_dma);\n\t\tusb_free_urb(dev->urb);\n\t}\n\tkfree(dev);\n}",
        "bug_line_number": [
            4,
            19,
            20
        ],
        "vul": 1,
        "id": 328
    },
    {
        "code": "int snd_timer_open(struct snd_timer_instance **ti,\n\t\t   char *owner, struct snd_timer_id *tid,\n\t\t   unsigned int slave_id)\n{\n\tstruct snd_timer *timer;\n\tstruct snd_timer_instance *timeri = NULL;\n\tstruct device *card_dev_to_put = NULL;\n\tint err;\n\tmutex_lock(&register_mutex);\n\tif (tid->dev_class == SNDRV_TIMER_CLASS_SLAVE) {\n\t\t/* open a slave instance */\n\t\tif (tid->dev_sclass <= SNDRV_TIMER_SCLASS_NONE ||\n\t\t    tid->dev_sclass > SNDRV_TIMER_SCLASS_OSS_SEQUENCER) {\n\t\t\tpr_debug(\"ALSA: timer: invalid slave class %i\\n\",\n\t\t\t\t tid->dev_sclass);\n\t\t\terr = -EINVAL;\n\t\t\tgoto unlock;\n\t\t}\n\t\ttimeri = snd_timer_instance_new(owner, NULL);\n\t\tif (!timeri) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto unlock;\n\t\t}\n\t\ttimeri->slave_class = tid->dev_sclass;\n\t\ttimeri->slave_id = tid->device;\n\t\ttimeri->flags |= SNDRV_TIMER_IFLG_SLAVE;\n\t\tlist_add_tail(&timeri->open_list, &snd_timer_slave_list);\n\t\terr = snd_timer_check_slave(timeri);\n\t\tif (err < 0) {\n\t\t\tsnd_timer_close_locked(timeri, &card_dev_to_put);\n\t\t\ttimeri = NULL;\n\t\t}\n\t\tgoto unlock;\n\t}\n\t/* open a master instance */\n\ttimer = snd_timer_find(tid);\n#ifdef CONFIG_MODULES\n\tif (!timer) {\n\t\tmutex_unlock(&register_mutex);\n\t\tsnd_timer_request(tid);\n\t\tmutex_lock(&register_mutex);\n\t\ttimer = snd_timer_find(tid);\n\t}\n#endif\n\tif (!timer) {\n\t\terr = -ENODEV;\n\t\tgoto unlock;\n\t}\n\tif (!list_empty(&timer->open_list_head)) {\n\t\ttimeri = list_entry(timer->open_list_head.next,\n\t\t\t\t    struct snd_timer_instance, open_list);\n\t\tif (timeri->flags & SNDRV_TIMER_IFLG_EXCLUSIVE) {\n\t\t\terr = -EBUSY;\n\t\t\ttimeri = NULL;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\tif (timer->num_instances >= timer->max_instances) {\n\t\terr = -EBUSY;\n\t\tgoto unlock;\n\t}\n\ttimeri = snd_timer_instance_new(owner, timer);\n\tif (!timeri) {\n\t\terr = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\t/* take a card refcount for safe disconnection */\n\tif (timer->card)\n\t\tget_device(&timer->card->card_dev);\n\ttimeri->slave_class = tid->dev_sclass;\n\ttimeri->slave_id = slave_id;\n\tif (list_empty(&timer->open_list_head) && timer->hw.open) {\n\t\terr = timer->hw.open(timer);\n\t\tif (err) {\n\t\t\tkfree(timeri->owner);\n\t\t\tkfree(timeri);\n\t\t\ttimeri = NULL;\n\t\t\tif (timer->card)\n\t\t\t\tcard_dev_to_put = &timer->card->card_dev;\n\t\t\tmodule_put(timer->module);\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\tlist_add_tail(&timeri->open_list, &timer->open_list_head);\n\ttimer->num_instances++;\n\terr = snd_timer_check_master(timeri);\n\tif (err < 0) {\n\t\tsnd_timer_close_locked(timeri, &card_dev_to_put);\n\t\ttimeri = NULL;\n\t}\n unlock:\n\tmutex_unlock(&register_mutex);\n\t/* put_device() is called after unlock for avoiding deadlock */\n\tif (card_dev_to_put)\n\t\tput_device(card_dev_to_put);\n\t*ti = timeri;\n\treturn err;\n}",
        "bug_line_number": [
            49,
            50,
            51,
            52,
            53
        ],
        "vul": 1,
        "id": 330
    },
    {
        "code": "static void\nnvkm_vmm_put_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *prev, *next;\n\tif ((prev = node(vma, prev)) && !prev->used) {\n\t\tvma->addr  = prev->addr;\n\t\tvma->size += prev->size;\n\t\tnvkm_vmm_free_delete(vmm, prev);\n\t}\n\tif ((next = node(vma, next)) && !next->used) {\n\t\tvma->size += next->size;\n\t\tnvkm_vmm_free_delete(vmm, next);\n\t}\n\tnvkm_vmm_free_insert(vmm, vma);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 1
    },
    {
        "code": "static int get_entries(struct net *net, struct arpt_get_entries __user *uptr,\n\t\t       const int *len)\n{\n\tint ret;\n\tstruct arpt_get_entries get;\n\tstruct xt_table *t;\n\tif (*len < sizeof(get))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&get, uptr, sizeof(get)) != 0)\n\t\treturn -EFAULT;\n\tif (*len != sizeof(struct arpt_get_entries) + get.size)\n\t\treturn -EINVAL;\n\tget.name[sizeof(get.name) - 1] = '\\0';\n\tt = xt_find_table_lock(net, NFPROTO_ARP, get.name);\n\tif (!IS_ERR(t)) {\n\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n\t\tif (get.size == private->size)\n\t\t\tret = copy_entries_to_user(private->size,\n\t\t\t\t\t\t   t, uptr->entrytable);\n\t\telse\n\t\t\tret = -EAGAIN;\n\t\tmodule_put(t->me);\n\t\txt_table_unlock(t);\n\t} else\n\t\tret = PTR_ERR(t);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 3
    },
    {
        "code": "static struct xt_counters *alloc_counters(const struct xt_table *table)\n{\n\tunsigned int countersize;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\t/* We need atomic snapshot of counters: rest doesn't change\n\t * (other than comefrom, which userspace doesn't care\n\t * about).\n\t */\n\tcountersize = sizeof(struct xt_counters) * private->number;\n\tcounters = vzalloc(countersize);\n\tif (counters == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\tget_counters(private, counters);\n\treturn counters;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 5
    },
    {
        "code": "static struct xt_counters *alloc_counters(const struct xt_table *table)\n{\n\tunsigned int countersize;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\t/* We need atomic snapshot of counters: rest doesn't change\n\t   (other than comefrom, which userspace doesn't care\n\t   about). */\n\tcountersize = sizeof(struct xt_counters) * private->number;\n\tcounters = vzalloc(countersize);\n\tif (counters == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\tget_counters(private, counters);\n\treturn counters;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 7
    },
    {
        "code": "void snd_pcm_detach_substream(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\tif (runtime->private_free != NULL)\n\t\truntime->private_free(runtime);\n\tfree_pages_exact(runtime->status,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\tfree_pages_exact(runtime->control,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control)));\n\tkfree(runtime->hw_constraints.rules);\n\t/* Avoid concurrent access to runtime via PCM timer interface */\n\tif (substream->timer) {\n\t\tspin_lock_irq(&substream->timer->lock);\n\t\tsubstream->runtime = NULL;\n\t\tspin_unlock_irq(&substream->timer->lock);\n\t} else {\n\t\tsubstream->runtime = NULL;\n\t}\n\tmutex_destroy(&runtime->buffer_mutex);\n\tkfree(runtime);\n\tput_pid(substream->pid);\n\tsubstream->pid = NULL;\n\tsubstream->pstr->substream_opened--;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 9
    },
    {
        "code": "static int snd_pcm_hw_params(struct snd_pcm_substream *substream,\n\t\t\t     struct snd_pcm_hw_params *params)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tint err = 0, usecs;\n\tunsigned int bits;\n\tsnd_pcm_uframes_t frames;\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn -ENXIO;\n\truntime = substream->runtime;\n\tmutex_lock(&runtime->buffer_mutex);\n\tsnd_pcm_stream_lock_irq(substream);\n\tswitch (runtime->status->state) {\n\tcase SNDRV_PCM_STATE_OPEN:\n\tcase SNDRV_PCM_STATE_SETUP:\n\tcase SNDRV_PCM_STATE_PREPARED:\n\t\tif (!is_oss_stream(substream) &&\n\t\t    atomic_read(&substream->mmap_count))\n\t\t\terr = -EBADFD;\n\t\tbreak;\n\tdefault:\n\t\terr = -EBADFD;\n\t\tbreak;\n\t}\n\tsnd_pcm_stream_unlock_irq(substream);\n\tif (err)\n\t\tgoto unlock;\n\tsnd_pcm_sync_stop(substream, true);\n\tparams->rmask = ~0U;\n\terr = snd_pcm_hw_refine(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\terr = snd_pcm_hw_params_choose(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\terr = fixup_unreferenced_params(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\tif (substream->managed_buffer_alloc) {\n\t\terr = snd_pcm_lib_malloc_pages(substream,\n\t\t\t\t\t       params_buffer_bytes(params));\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t\truntime->buffer_changed = err > 0;\n\t}\n\tif (substream->ops->hw_params != NULL) {\n\t\terr = substream->ops->hw_params(substream, params);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t}\n\truntime->access = params_access(params);\n\truntime->format = params_format(params);\n\truntime->subformat = params_subformat(params);\n\truntime->channels = params_channels(params);\n\truntime->rate = params_rate(params);\n\truntime->period_size = params_period_size(params);\n\truntime->periods = params_periods(params);\n\truntime->buffer_size = params_buffer_size(params);\n\truntime->info = params->info;\n\truntime->rate_num = params->rate_num;\n\truntime->rate_den = params->rate_den;\n\truntime->no_period_wakeup =\n\t\t\t(params->info & SNDRV_PCM_INFO_NO_PERIOD_WAKEUP) &&\n\t\t\t(params->flags & SNDRV_PCM_HW_PARAMS_NO_PERIOD_WAKEUP);\n\tbits = snd_pcm_format_physical_width(runtime->format);\n\truntime->sample_bits = bits;\n\tbits *= runtime->channels;\n\truntime->frame_bits = bits;\n\tframes = 1;\n\twhile (bits % 8 != 0) {\n\t\tbits *= 2;\n\t\tframes *= 2;\n\t}\n\truntime->byte_align = bits / 8;\n\truntime->min_align = frames;\n\t/* Default sw params */\n\truntime->tstamp_mode = SNDRV_PCM_TSTAMP_NONE;\n\truntime->period_step = 1;\n\truntime->control->avail_min = runtime->period_size;\n\truntime->start_threshold = 1;\n\truntime->stop_threshold = runtime->buffer_size;\n\truntime->silence_threshold = 0;\n\truntime->silence_size = 0;\n\truntime->boundary = runtime->buffer_size;\n\twhile (runtime->boundary * 2 <= LONG_MAX - runtime->buffer_size)\n\t\truntime->boundary *= 2;\n\t/* clear the buffer for avoiding possible kernel info leaks */\n\tif (runtime->dma_area && !substream->ops->copy_user) {\n\t\tsize_t size = runtime->dma_bytes;\n\t\tif (runtime->info & SNDRV_PCM_INFO_MMAP)\n\t\t\tsize = PAGE_ALIGN(size);\n\t\tmemset(runtime->dma_area, 0, size);\n\t}\n\tsnd_pcm_timer_resolution_change(substream);\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_SETUP);\n\tif (cpu_latency_qos_request_active(&substream->latency_pm_qos_req))\n\t\tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n\tusecs = period_to_usecs(runtime);\n\tif (usecs >= 0)\n\t\tcpu_latency_qos_add_request(&substream->latency_pm_qos_req,\n\t\t\t\t\t    usecs);\n\terr = 0;\n _error:\n\tif (err) {\n\t\t/* hardware might be unusable from this time,\n\t\t * so we force application to retry to set",
        "bug_line_number": [],
        "vul": 0,
        "id": 11
    },
    {
        "code": "\t\t * the correct hardware parameter settings\n\t\t */\n\t\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n\t\tif (substream->ops->hw_free != NULL)\n\t\t\tsubstream->ops->hw_free(substream);\n\t\tif (substream->managed_buffer_alloc)\n\t\t\tsnd_pcm_lib_free_pages(substream);\n\t}\n unlock:\n\tmutex_unlock(&runtime->buffer_mutex);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 11
    },
    {
        "code": "int gru_check_context_placement(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru;\n\tint ret = 0;\n\t/*\n\t * If the current task is the context owner, verify that the\n\t * context is correctly placed. This test is skipped for non-owner\n\t * references. Pthread apps use non-owner references to the CBRs.\n\t */\n\tgru = gts->ts_gru;\n\t/*\n\t * If gru or gts->ts_tgid_owner isn't initialized properly, return\n\t * success to indicate that the caller does not need to unload the\n\t * gru context.The caller is responsible for their inspection and\n\t * reinitialization if needed.\n\t */\n\tif (!gru || gts->ts_tgid_owner != current->tgid)\n\t\treturn ret;\n\tif (!gru_check_chiplet_assignment(gru, gts)) {\n\t\tSTAT(check_context_unload);\n\t\tret = -EINVAL;\n\t} else if (gru_retarget_intr(gts)) {\n\t\tSTAT(check_context_retarget_intr);\n\t}\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 13
    },
    {
        "code": "static void aio_fsync_work(struct work_struct *work)\n{\n\tstruct fsync_iocb *req = container_of(work, struct fsync_iocb, work);\n\tint ret;\n\tret = vfs_fsync(req->file, req->datasync);\n\taio_complete(container_of(req, struct aio_kiocb, fsync), ret, 0);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 15
    },
    {
        "code": "static int __io_submit_one(struct kioctx *ctx, const struct iocb *iocb,\n\t\t\t   struct iocb __user *user_iocb, bool compat)\n{\n\tstruct aio_kiocb *req;\n\tssize_t ret;\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(iocb->aio_reserved2)) {\n\t\tpr_debug(\"EINVAL: reserve field set\\n\");\n\t\treturn -EINVAL;\n\t}\n\t/* prevent overflows */\n\tif (unlikely(\n\t    (iocb->aio_buf != (unsigned long)iocb->aio_buf) ||\n\t    (iocb->aio_nbytes != (size_t)iocb->aio_nbytes) ||\n\t    ((ssize_t)iocb->aio_nbytes < 0)\n\t   )) {\n\t\tpr_debug(\"EINVAL: overflow check\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (!get_reqs_available(ctx))\n\t\treturn -EAGAIN;\n\tret = -EAGAIN;\n\treq = aio_get_req(ctx);\n\tif (unlikely(!req))\n\t\tgoto out_put_reqs_available;\n\treq->ki_filp = fget(iocb->aio_fildes);\n\tret = -EBADF;\n\tif (unlikely(!req->ki_filp))\n\t\tgoto out_put_req;\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD) {\n\t\t/*\n\t\t * If the IOCB_FLAG_RESFD flag of aio_flags is set, get an\n\t\t * instance of the file* now. The file descriptor must be\n\t\t * an eventfd() fd, and will be signaled for each completed\n\t\t * event using the eventfd_signal() function.\n\t\t */\n\t\treq->ki_eventfd = eventfd_ctx_fdget((int) iocb->aio_resfd);\n\t\tif (IS_ERR(req->ki_eventfd)) {\n\t\t\tret = PTR_ERR(req->ki_eventfd);\n\t\t\treq->ki_eventfd = NULL;\n\t\t\tgoto out_put_req;\n\t\t}\n\t}\n\tret = put_user(KIOCB_KEY, &user_iocb->aio_key);\n\tif (unlikely(ret)) {\n\t\tpr_debug(\"EFAULT: aio_key\\n\");\n\t\tgoto out_put_req;\n\t}\n\treq->ki_user_iocb = user_iocb;\n\treq->ki_user_data = iocb->aio_data;\n\tswitch (iocb->aio_lio_opcode) {\n\tcase IOCB_CMD_PREAD:\n\t\tret = aio_read(&req->rw, iocb, false, compat);\n\t\tbreak;\n\tcase IOCB_CMD_PWRITE:\n\t\tret = aio_write(&req->rw, iocb, false, compat);\n\t\tbreak;\n\tcase IOCB_CMD_PREADV:\n\t\tret = aio_read(&req->rw, iocb, true, compat);\n\t\tbreak;\n\tcase IOCB_CMD_PWRITEV:\n\t\tret = aio_write(&req->rw, iocb, true, compat);\n\t\tbreak;\n\tcase IOCB_CMD_FSYNC:\n\t\tret = aio_fsync(&req->fsync, iocb, false);\n\t\tbreak;\n\tcase IOCB_CMD_FDSYNC:\n\t\tret = aio_fsync(&req->fsync, iocb, true);\n\t\tbreak;\n\tcase IOCB_CMD_POLL:\n\t\tret = aio_poll(req, iocb);\n\t\tbreak;\n\tdefault:\n\t\tpr_debug(\"invalid aio operation %d\\n\", iocb->aio_lio_opcode);\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\t/*\n\t * If ret is 0, we'd either done aio_complete() ourselves or have\n\t * arranged for that to be done asynchronously.  Anything non-zero\n\t * means that we need to destroy req ourselves.\n\t */\n\tif (ret)\n\t\tgoto out_put_req;\n\treturn 0;\nout_put_req:\n\tif (req->ki_eventfd)\n\t\teventfd_ctx_put(req->ki_eventfd);\n\tiocb_put(req);\nout_put_reqs_available:\n\tput_reqs_available(ctx, 1);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 17
    },
    {
        "code": "static void unpin_sdma_pages(struct sdma_mmu_node *node)\n{\n\tif (node->npages) {\n\t\tunpin_vector_pages(mm_from_sdma_node(node), node->pages, 0,\n\t\t\t\t   node->npages);\n\t\tatomic_sub(node->npages, &node->pq->n_locked);\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 19
    },
    {
        "code": "bool hfi1_mmu_rb_remove_unless_exact(struct mmu_rb_handler *handler,\n\t\t\t\t     unsigned long addr, unsigned long len,\n\t\t\t\t     struct mmu_rb_node **rb_node)\n{\n\tstruct mmu_rb_node *node;\n\tunsigned long flags;\n\tbool ret = false;\n\tif (current->mm != handler->mn.mm)\n\t\treturn ret;\n\tspin_lock_irqsave(&handler->lock, flags);\n\tnode = __mmu_rb_search(handler, addr, len);\n\tif (node) {\n\t\tif (node->addr == addr && node->len == len)\n\t\t\tgoto unlock;\n\t\t__mmu_int_rb_remove(node, &handler->root);\n\t\tlist_del(&node->list); /* remove from LRU list */\n\t\tret = true;\n\t}\nunlock:\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\t*rb_node = node;\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 21
    },
    {
        "code": "int hfi1_user_sdma_alloc_queues(struct hfi1_ctxtdata *uctxt,\n\t\t\t\tstruct hfi1_filedata *fd)\n{\n\tint ret = -ENOMEM;\n\tchar buf[64];\n\tstruct hfi1_devdata *dd;\n\tstruct hfi1_user_sdma_comp_q *cq;\n\tstruct hfi1_user_sdma_pkt_q *pq;\n\tif (!uctxt || !fd)\n\t\treturn -EBADF;\n\tif (!hfi1_sdma_comp_ring_size)\n\t\treturn -EINVAL;\n\tdd = uctxt->dd;\n\tpq = kzalloc(sizeof(*pq), GFP_KERNEL);\n\tif (!pq)\n\t\treturn -ENOMEM;\n\tpq->dd = dd;\n\tpq->ctxt = uctxt->ctxt;\n\tpq->subctxt = fd->subctxt;\n\tpq->n_max_reqs = hfi1_sdma_comp_ring_size;\n\tatomic_set(&pq->n_reqs, 0);\n\tinit_waitqueue_head(&pq->wait);\n\tatomic_set(&pq->n_locked, 0);\n\tiowait_init(&pq->busy, 0, NULL, NULL, defer_packet_queue,\n\t\t    activate_packet_queue, NULL, NULL);\n\tpq->reqidx = 0;\n\tpq->reqs = kcalloc(hfi1_sdma_comp_ring_size,\n\t\t\t   sizeof(*pq->reqs),\n\t\t\t   GFP_KERNEL);\n\tif (!pq->reqs)\n\t\tgoto pq_reqs_nomem;\n\tpq->req_in_use = kcalloc(BITS_TO_LONGS(hfi1_sdma_comp_ring_size),\n\t\t\t\t sizeof(*pq->req_in_use),\n\t\t\t\t GFP_KERNEL);\n\tif (!pq->req_in_use)\n\t\tgoto pq_reqs_no_in_use;\n\tsnprintf(buf, 64, \"txreq-kmem-cache-%u-%u-%u\", dd->unit, uctxt->ctxt,\n\t\t fd->subctxt);\n\tpq->txreq_cache = kmem_cache_create(buf,\n\t\t\t\t\t    sizeof(struct user_sdma_txreq),\n\t\t\t\t\t    L1_CACHE_BYTES,\n\t\t\t\t\t    SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t    NULL);\n\tif (!pq->txreq_cache) {\n\t\tdd_dev_err(dd, \"[%u] Failed to allocate TxReq cache\\n\",\n\t\t\t   uctxt->ctxt);\n\t\tgoto pq_txreq_nomem;\n\t}\n\tcq = kzalloc(sizeof(*cq), GFP_KERNEL);\n\tif (!cq)\n\t\tgoto cq_nomem;\n\tcq->comps = vmalloc_user(PAGE_ALIGN(sizeof(*cq->comps)\n\t\t\t\t * hfi1_sdma_comp_ring_size));\n\tif (!cq->comps)\n\t\tgoto cq_comps_nomem;\n\tcq->nentries = hfi1_sdma_comp_ring_size;\n\tret = hfi1_mmu_rb_register(pq, &sdma_rb_ops, dd->pport->hfi1_wq,\n\t\t\t\t   &pq->handler);\n\tif (ret) {\n\t\tdd_dev_err(dd, \"Failed to register with MMU %d\", ret);\n\t\tgoto pq_mmu_fail;\n\t}\n\trcu_assign_pointer(fd->pq, pq);\n\tfd->cq = cq;\n\treturn 0;\npq_mmu_fail:\n\tvfree(cq->comps);\ncq_comps_nomem:\n\tkfree(cq);\ncq_nomem:\n\tkmem_cache_destroy(pq->txreq_cache);\npq_txreq_nomem:\n\tkfree(pq->req_in_use);\npq_reqs_no_in_use:\n\tkfree(pq->reqs);\npq_reqs_nomem:\n\tkfree(pq);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 23
    },
    {
        "code": "static int hfi1_file_open(struct inode *inode, struct file *fp)\n{\n\tstruct hfi1_filedata *fd;\n\tstruct hfi1_devdata *dd = container_of(inode->i_cdev,\n\t\t\t\t\t       struct hfi1_devdata,\n\t\t\t\t\t       user_cdev);\n\tif (!((dd->flags & HFI1_PRESENT) && dd->kregbase1))\n\t\treturn -EINVAL;\n\tif (!atomic_inc_not_zero(&dd->user_refcount))\n\t\treturn -ENXIO;\n\t/* The real work is performed later in assign_ctxt() */\n\tfd = kzalloc(sizeof(*fd), GFP_KERNEL);\n\tif (!fd || init_srcu_struct(&fd->pq_srcu))\n\t\tgoto nomem;\n\tspin_lock_init(&fd->pq_rcu_lock);\n\tspin_lock_init(&fd->tid_lock);\n\tspin_lock_init(&fd->invalid_lock);\n\tfd->rec_cpu_num = -1; /* no cpu affinity by default */\n\tfd->dd = dd;\n\tfp->private_data = fd;\n\treturn 0;\nnomem:\n\tkfree(fd);\n\tfp->private_data = NULL;\n\tif (atomic_dec_and_test(&dd->user_refcount))\n\t\tcomplete(&dd->user_comp);\n\treturn -ENOMEM;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 25
    },
    {
        "code": "static int pin_rcv_pages(struct hfi1_filedata *fd, struct tid_user_buf *tidbuf)\n{\n\tint pinned;\n\tunsigned int npages;\n\tunsigned long vaddr = tidbuf->vaddr;\n\tstruct page **pages = NULL;\n\tstruct hfi1_devdata *dd = fd->uctxt->dd;\n\t/* Get the number of pages the user buffer spans */\n\tnpages = num_user_pages(vaddr, tidbuf->length);\n\tif (!npages)\n\t\treturn -EINVAL;\n\tif (npages > fd->uctxt->expected_count) {\n\t\tdd_dev_err(dd, \"Expected buffer too big\\n\");\n\t\treturn -EINVAL;\n\t}\n\t/* Allocate the array of struct page pointers needed for pinning */\n\tpages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\t/*\n\t * Pin all the pages of the user buffer. If we can't pin all the\n\t * pages, accept the amount pinned so far and program only that.\n\t * User space knows how to deal with partially programmed buffers.\n\t */\n\tif (!hfi1_can_pin_pages(dd, current->mm, fd->tid_n_pinned, npages)) {\n\t\tkfree(pages);\n\t\treturn -ENOMEM;\n\t}\n\tpinned = hfi1_acquire_user_pages(current->mm, vaddr, npages, true, pages);\n\tif (pinned <= 0) {\n\t\tkfree(pages);\n\t\treturn pinned;\n\t}\n\ttidbuf->pages = pages;\n\ttidbuf->npages = npages;\n\tfd->tid_n_pinned += pinned;\n\treturn pinned;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 27
    },
    {
        "code": "static void io_rw_resubmit(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint err;\n\terr = io_sq_thread_acquire_mm(ctx, req);\n\tif (io_resubmit_prep(req, err)) {\n\t\trefcount_inc(&req->refs);\n\t\tio_queue_async_work(req);\n\t}\n\tpercpu_ref_put(&ctx->refs);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 29
    },
    {
        "code": "static int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t     int sync, void *arg)\n{\n\tstruct wait_page_queue *wpq;\n\tstruct io_kiocb *req = wait->private;\n\tstruct wait_page_key *key = arg;\n\tint ret;\n\twpq = container_of(wait, struct wait_page_queue, wait);\n\tif (!wake_page_match(wpq, key))\n\t\treturn 0;\n\tlist_del_init(&wait->entry);\n\tinit_task_work(&req->task_work, io_req_task_submit);\n\tpercpu_ref_get(&req->ctx->refs);\n\t/* submit ref gets dropped, acquire a new one */\n\trefcount_inc(&req->refs);\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\t\t/* queue just for cancelation */\n\t\tinit_task_work(&req->task_work, io_req_task_cancel);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, 0);\n\t\twake_up_process(tsk);\n\t}\n\treturn 1;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 31
    },
    {
        "code": "static void smsusb_stop_streaming(struct smsusb_device_t *dev)\n{\n\tint i;\n\tfor (i = 0; i < MAX_URBS; i++) {\n\t\tusb_kill_urb(&dev->surbs[i].urb);\n\t\tif (dev->surbs[i].wq.func)\n\t\t\tcancel_work_sync(&dev->surbs[i].wq);\n\t\tif (dev->surbs[i].cb) {\n\t\t\tsmscore_putbuffer(dev->coredev, dev->surbs[i].cb);\n\t\t\tdev->surbs[i].cb = NULL;\n\t\t}\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 33
    },
    {
        "code": "static void *proc_pid_follow_link(struct dentry *dentry, struct nameidata *nd)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error = -EACCES;\n\t/* We don't need a base pointer in the /proc filesystem */\n\tpath_put(&nd->path);\n\t/* Are we allowed to snoop on the tasks file descriptors? */\n\tif (!proc_fd_access_allowed(inode))\n\t\tgoto out;\n\terror = PROC_I(inode)->op.proc_get_link(inode, &nd->path);\nout:\n\treturn ERR_PTR(error);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 35
    },
    {
        "code": "static int route4_change(struct net *net, struct sk_buff *in_skb,\n\t\t\t struct tcf_proto *tp, unsigned long base, u32 handle,\n\t\t\t struct nlattr **tca, void **arg, bool ovr,\n\t\t\t bool rtnl_held, struct netlink_ext_ack *extack)\n{\n\tstruct route4_head *head = rtnl_dereference(tp->root);\n\tstruct route4_filter __rcu **fp;\n\tstruct route4_filter *fold, *f1, *pfp, *f = NULL;\n\tstruct route4_bucket *b;\n\tstruct nlattr *opt = tca[TCA_OPTIONS];\n\tstruct nlattr *tb[TCA_ROUTE4_MAX + 1];\n\tunsigned int h, th;\n\tint err;\n\tbool new = true;\n\tif (opt == NULL)\n\t\treturn handle ? -EINVAL : 0;\n\terr = nla_parse_nested_deprecated(tb, TCA_ROUTE4_MAX, opt,\n\t\t\t\t\t  route4_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\tfold = *arg;\n\tif (fold && handle && fold->handle != handle)\n\t\t\treturn -EINVAL;\n\terr = -ENOBUFS;\n\tf = kzalloc(sizeof(struct route4_filter), GFP_KERNEL);\n\tif (!f)\n\t\tgoto errout;\n\terr = tcf_exts_init(&f->exts, net, TCA_ROUTE4_ACT, TCA_ROUTE4_POLICE);\n\tif (err < 0)\n\t\tgoto errout;\n\tif (fold) {\n\t\tf->id = fold->id;\n\t\tf->iif = fold->iif;\n\t\tf->res = fold->res;\n\t\tf->handle = fold->handle;\n\t\tf->tp = fold->tp;\n\t\tf->bkt = fold->bkt;\n\t\tnew = false;\n\t}\n\terr = route4_set_parms(net, tp, base, f, handle, head, tb,\n\t\t\t       tca[TCA_RATE], new, ovr, extack);\n\tif (err < 0)\n\t\tgoto errout;\n\th = from_hash(f->handle >> 16);\n\tfp = &f->bkt->ht[h];\n\tfor (pfp = rtnl_dereference(*fp);\n\t     (f1 = rtnl_dereference(*fp)) != NULL;\n\t     fp = &f1->next)\n\t\tif (f->handle < f1->handle)\n\t\t\tbreak;\n\ttcf_block_netif_keep_dst(tp->chain->block);\n\trcu_assign_pointer(f->next, f1);\n\trcu_assign_pointer(*fp, f);\n\tif (fold && fold->handle && f->handle != fold->handle) {\n\t\tth = to_hash(fold->handle);\n\t\th = from_hash(fold->handle >> 16);\n\t\tb = rtnl_dereference(head->table[th]);\n\t\tif (b) {\n\t\t\tfp = &b->ht[h];\n\t\t\tfor (pfp = rtnl_dereference(*fp); pfp;\n\t\t\t     fp = &pfp->next, pfp = rtnl_dereference(*fp)) {\n\t\t\t\tif (pfp == fold) {\n\t\t\t\t\trcu_assign_pointer(*fp, fold->next);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\troute4_reset_fastmap(head);\n\t*arg = f;\n\tif (fold) {\n\t\ttcf_unbind_filter(tp, &fold->res);\n\t\ttcf_exts_get_net(&fold->exts);\n\t\ttcf_queue_work(&fold->rwork, route4_delete_filter_work);\n\t}\n\treturn 0;\nerrout:\n\tif (f)\n\t\ttcf_exts_destroy(&f->exts);\n\tkfree(f);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 37
    },
    {
        "code": "static int io_sqpoll_wait_sq(struct io_ring_ctx *ctx)\n{\n\tint ret = 0;\n\tDEFINE_WAIT(wait);\n\tdo {\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tprepare_to_wait(&ctx->sqo_sq_wait, &wait, TASK_INTERRUPTIBLE);\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tschedule();\n\t} while (!signal_pending(current));\n\tfinish_wait(&ctx->sqo_sq_wait, &wait);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 39
    },
    {
        "code": "static void __io_req_task_submit(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\t/* ctx stays valid until unlock, even if we drop all ours ctx->refs */\n\tmutex_lock(&ctx->uring_lock);\n\tif (!(current->flags & PF_EXITING) && !current->in_execve)\n\t\t__io_queue_sqe(req);\n\telse\n\t\t__io_req_task_cancel(req, -EFAULT);\n\tmutex_unlock(&ctx->uring_lock);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 41
    },
    {
        "code": "void __io_uring_task_cancel(void)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tDEFINE_WAIT(wait);\n\ts64 inflight;\n\t/* make sure overflow events are dropped */\n\tatomic_inc(&tctx->in_idle);\n\tif (tctx->sqpoll) {\n\t\tstruct file *file;\n\t\tunsigned long index;\n\t\txa_for_each(&tctx->xa, index, file)\n\t\t\tio_uring_cancel_sqpoll(file->private_data);\n\t}\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\t__io_uring_files_cancel(NULL);\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\t\t/*\n\t\t * If we've seen completions, retry without waiting. This\n\t\t * avoids a race where a completion comes in before we did\n\t\t * prepare_to_wait().\n\t\t */\n\t\tif (inflight == tctx_inflight(tctx))\n\t\t\tschedule();\n\t\tfinish_wait(&tctx->wait, &wait);\n\t} while (1);\n\tatomic_dec(&tctx->in_idle);\n\tio_uring_clean_tctx(tctx);\n\t/* all current's requests should be gone, we can kill tctx */\n\t__io_uring_free(current);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 43
    },
    {
        "code": "int l2tp_session_delete(struct l2tp_session *session)\n{\n\tif (test_and_set_bit(0, &session->dead))\n\t\treturn 0;\n\tif (session->ref)\n\t\t(*session->ref)(session);\n\t__l2tp_session_unhash(session);\n\tl2tp_session_queue_purge(session);\n\tif (session->session_close != NULL)\n\t\t(*session->session_close)(session);\n\tif (session->deref)\n\t\t(*session->deref)(session);\n\tl2tp_session_dec_refcount(session);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 45
    },
    {
        "code": "void kvmppc_uvmem_drop_pages(const struct kvm_memory_slot *slot,\n\t\t\t     struct kvm *kvm, bool skip_page_out)\n{\n\tint i;\n\tstruct kvmppc_uvmem_page_pvt *pvt;\n\tstruct page *uvmem_page;\n\tstruct vm_area_struct *vma = NULL;\n\tunsigned long uvmem_pfn, gfn;\n\tunsigned long addr;\n\tmmap_read_lock(kvm->mm);\n\taddr = slot->userspace_addr;\n\tgfn = slot->base_gfn;\n\tfor (i = slot->npages; i; --i, ++gfn, addr += PAGE_SIZE) {\n\t\t/* Fetch the VMA if addr is not in the latest fetched one */\n\t\tif (!vma || addr >= vma->vm_end) {\n\t\t\tvma = vma_lookup(kvm->mm, addr);\n\t\t\tif (!vma) {\n\t\t\t\tpr_err(\"Can't find VMA for gfn:0x%lx\\n\", gfn);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tmutex_lock(&kvm->arch.uvmem_lock);\n\t\tif (kvmppc_gfn_is_uvmem_pfn(gfn, kvm, &uvmem_pfn)) {\n\t\t\tuvmem_page = pfn_to_page(uvmem_pfn);\n\t\t\tpvt = uvmem_page->zone_device_data;\n\t\t\tpvt->skip_page_out = skip_page_out;\n\t\t\tpvt->remove_gfn = true;\n\t\t\tif (__kvmppc_svm_page_out(vma, addr, addr + PAGE_SIZE,\n\t\t\t\t\t\t  PAGE_SHIFT, kvm, pvt->gpa, NULL))\n\t\t\t\tpr_err(\"Can't page out gpa:0x%lx addr:0x%lx\\n\",\n\t\t\t\t       pvt->gpa, addr);\n\t\t} else {\n\t\t\t/* Remove the shared flag if any */\n\t\t\tkvmppc_gfn_remove(gfn, kvm);\n\t\t}\n\t\tmutex_unlock(&kvm->arch.uvmem_lock);\n\t}\n\tmmap_read_unlock(kvm->mm);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 47
    },
    {
        "code": "static vm_fault_t dmirror_devmem_fault(struct vm_fault *vmf)\n{\n\tstruct migrate_vma args = { 0 };\n\tunsigned long src_pfns = 0;\n\tunsigned long dst_pfns = 0;\n\tstruct page *rpage;\n\tstruct dmirror *dmirror;\n\tvm_fault_t ret;\n\t/*\n\t * Normally, a device would use the page->zone_device_data to point to\n\t * the mirror but here we use it to hold the page for the simulated\n\t * device memory and that page holds the pointer to the mirror.\n\t */\n\trpage = vmf->page->zone_device_data;\n\tdmirror = rpage->zone_device_data;\n\t/* FIXME demonstrate how we can adjust migrate range */\n\targs.vma = vmf->vma;\n\targs.start = vmf->address;\n\targs.end = args.start + PAGE_SIZE;\n\targs.src = &src_pfns;\n\targs.dst = &dst_pfns;\n\targs.pgmap_owner = dmirror->mdevice;\n\targs.flags = dmirror_select_device(dmirror);\n\targs.fault_page = vmf->page;\n\tif (migrate_vma_setup(&args))\n\t\treturn VM_FAULT_SIGBUS;\n\tret = dmirror_devmem_fault_alloc_and_copy(&args, dmirror);\n\tif (ret)\n\t\treturn ret;\n\tmigrate_vma_pages(&args);\n\t/*\n\t * No device finalize step is needed since\n\t * dmirror_devmem_fault_alloc_and_copy() will have already\n\t * invalidated the device page table.\n\t */\n\tmigrate_vma_finalize(&args);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 49
    },
    {
        "code": "int __ext4_journal_stop(const char *where, unsigned int line, handle_t *handle)\n{\n\tstruct super_block *sb;\n\tint err;\n\tint rc;\n\tif (!ext4_handle_valid(handle)) {\n\t\text4_put_nojournal(handle);\n\t\treturn 0;\n\t}\n\terr = handle->h_err;\n\tif (!handle->h_transaction) {\n\t\trc = jbd2_journal_stop(handle);\n\t\treturn err ? err : rc;\n\t}\n\tsb = handle->h_transaction->t_journal->j_private;\n\trc = jbd2_journal_stop(handle);\n\tif (!err)\n\t\terr = rc;\n\tif (err)\n\t\t__ext4_std_error(sb, where, line, err);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 51
    },
    {
        "code": "SYSCALL_DEFINE4(set_mempolicy_home_node, unsigned long, start, unsigned long, len,\n\t\tunsigned long, home_node, unsigned long, flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct mempolicy *new, *old;\n\tunsigned long end;\n\tint err = -ENOENT;\n\tVMA_ITERATOR(vmi, mm, start);\n\tstart = untagged_addr(start);\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\t/*\n\t * flags is used for future extension if any.\n\t */\n\tif (flags != 0)\n\t\treturn -EINVAL;\n\t/*\n\t * Check home_node is online to avoid accessing uninitialized\n\t * NODE_DATA.\n\t */\n\tif (home_node >= MAX_NUMNODES || !node_online(home_node))\n\t\treturn -EINVAL;\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\tmmap_write_lock(mm);\n\tprev = vma_prev(&vmi);\n\tfor_each_vma_range(vmi, vma, end) {\n\t\t/*\n\t\t * If any vma in the range got policy other than MPOL_BIND\n\t\t * or MPOL_PREFERRED_MANY we return error. We don't reset\n\t\t * the home node for vmas we already updated before.\n\t\t */\n\t\told = vma_policy(vma);\n\t\tif (!old)\n\t\t\tcontinue;\n\t\tif (old->mode != MPOL_BIND && old->mode != MPOL_PREFERRED_MANY) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tnew = mpol_dup(old);\n\t\tif (IS_ERR(new)) {\n\t\t\terr = PTR_ERR(new);\n\t\t\tbreak;\n\t\t}\n\t\tvma_start_write(vma);\n\t\tnew->home_node = home_node;\n\t\terr = mbind_range(&vmi, vma, &prev, start, end, new);\n\t\tmpol_put(new);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tmmap_write_unlock(mm);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 53
    },
    {
        "code": "static int vma_replace_policy(struct vm_area_struct *vma,\n\t\t\t\t\t\tstruct mempolicy *pol)\n{\n\tint err;\n\tstruct mempolicy *old;\n\tstruct mempolicy *new;\n\tvma_assert_write_locked(vma);\n\tpr_debug(\"vma %lx-%lx/%lx vm_ops %p vm_file %p set_policy %p\\n\",\n\t\t vma->vm_start, vma->vm_end, vma->vm_pgoff,\n\t\t vma->vm_ops, vma->vm_file,\n\t\t vma->vm_ops ? vma->vm_ops->set_policy : NULL);\n\tnew = mpol_dup(pol);\n\tif (IS_ERR(new))\n\t\treturn PTR_ERR(new);\n\tif (vma->vm_ops && vma->vm_ops->set_policy) {\n\t\terr = vma->vm_ops->set_policy(vma, new);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t}\n\told = vma->vm_policy;\n\tvma->vm_policy = new; /* protected by mmap_lock */\n\tmpol_put(old);\n\treturn 0;\n err_out:\n\tmpol_put(new);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 55
    },
    {
        "code": "static __be32\nnfsd4_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tunion nfsd4_op_u *u)\n{\n\tstruct nfsd4_copy *copy = &u->copy;\n\t__be32 status;\n\tstruct nfsd4_copy *async_copy = NULL;\n\tif (nfsd4_ssc_is_inter(copy)) {\n\t\tif (!inter_copy_offload_enable || nfsd4_copy_is_sync(copy)) {\n\t\t\tstatus = nfserr_notsupp;\n\t\t\tgoto out;\n\t\t}\n\t\tstatus = nfsd4_setup_inter_ssc(rqstp, cstate, copy,\n\t\t\t\t&copy->ss_mnt);\n\t\tif (status)\n\t\t\treturn nfserr_offload_denied;\n\t} else {\n\t\tstatus = nfsd4_setup_intra_ssc(rqstp, cstate, copy);\n\t\tif (status)\n\t\t\treturn status;\n\t}\n\tcopy->cp_clp = cstate->clp;\n\tmemcpy(&copy->fh, &cstate->current_fh.fh_handle,\n\t\tsizeof(struct knfsd_fh));\n\tif (nfsd4_copy_is_async(copy)) {\n\t\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\t\tstatus = nfserrno(-ENOMEM);\n\t\tasync_copy = kzalloc(sizeof(struct nfsd4_copy), GFP_KERNEL);\n\t\tif (!async_copy)\n\t\t\tgoto out_err;\n\t\tasync_copy->cp_src = kmalloc(sizeof(*async_copy->cp_src), GFP_KERNEL);\n\t\tif (!async_copy->cp_src)\n\t\t\tgoto out_err;\n\t\tif (!nfs4_init_copy_state(nn, copy))\n\t\t\tgoto out_err;\n\t\trefcount_set(&async_copy->refcount, 1);\n\t\tmemcpy(&copy->cp_res.cb_stateid, &copy->cp_stateid.cs_stid,\n\t\t\tsizeof(copy->cp_res.cb_stateid));\n\t\tdup_copy_fields(copy, async_copy);\n\t\tasync_copy->copy_task = kthread_create(nfsd4_do_async_copy,\n\t\t\t\tasync_copy, \"%s\", \"copy thread\");\n\t\tif (IS_ERR(async_copy->copy_task))\n\t\t\tgoto out_err;\n\t\tspin_lock(&async_copy->cp_clp->async_lock);\n\t\tlist_add(&async_copy->copies,\n\t\t\t\t&async_copy->cp_clp->async_copies);\n\t\tspin_unlock(&async_copy->cp_clp->async_lock);\n\t\twake_up_process(async_copy->copy_task);\n\t\tstatus = nfs_ok;\n\t} else {\n\t\tstatus = nfsd4_do_copy(copy, copy->nf_src->nf_file,\n\t\t\t\t       copy->nf_dst->nf_file, true);\n\t\tnfsd4_cleanup_intra_ssc(copy->nf_src, copy->nf_dst);\n\t}\nout:\n\treturn status;\nout_err:\n\tif (async_copy)\n\t\tcleanup_async_copy(async_copy);\n\tstatus = nfserrno(-ENOMEM);\n\t/*\n\t * source's vfsmount of inter-copy will be unmounted\n\t * by the laundromat\n\t */\n\tgoto out;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 57
    },
    {
        "code": "int ptp_clock_unregister(struct ptp_clock *ptp)\n{\n\tptp->defunct = 1;\n\twake_up_interruptible(&ptp->tsev_wq);\n\tif (ptp->kworker) {\n\t\tkthread_cancel_delayed_work_sync(&ptp->aux_work);\n\t\tkthread_destroy_worker(ptp->kworker);\n\t}\n\t/* Release the clock's resources. */\n\tif (ptp->pps_source)\n\t\tpps_unregister_source(ptp->pps_source);\n\tptp_cleanup_pin_groups(ptp);\n\tposix_clock_unregister(&ptp->clock);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 59
    },
    {
        "code": "static int llcp_sock_bind(struct socket *sock, struct sockaddr *addr, int alen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct nfc_llcp_local *local;\n\tstruct nfc_dev *dev;\n\tstruct sockaddr_nfc_llcp llcp_addr;\n\tint len, ret = 0;\n\tif (!addr || alen < offsetofend(struct sockaddr, sa_family) ||\n\t    addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\tpr_debug(\"sk %p addr %p family %d\\n\", sk, addr, addr->sa_family);\n\tmemset(&llcp_addr, 0, sizeof(llcp_addr));\n\tlen = min_t(unsigned int, sizeof(llcp_addr), alen);\n\tmemcpy(&llcp_addr, addr, len);\n\t/* This is going to be a listening socket, dsap must be 0 */\n\tif (llcp_addr.dsap != 0)\n\t\treturn -EINVAL;\n\tlock_sock(sk);\n\tif (sk->sk_state != LLCP_CLOSED) {\n\t\tret = -EBADFD;\n\t\tgoto error;\n\t}\n\tdev = nfc_get_device(llcp_addr.dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = local;\n\tllcp_sock->nfc_protocol = llcp_addr.nfc_protocol;\n\tllcp_sock->service_name_len = min_t(unsigned int,\n\t\t\t\t\t    llcp_addr.service_name_len,\n\t\t\t\t\t    NFC_LLCP_MAX_SERVICE_NAME);\n\tllcp_sock->service_name = kmemdup(llcp_addr.service_name,\n\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_put_local;\n\t}\n\tllcp_sock->ssap = nfc_llcp_get_sdp_ssap(local, llcp_sock);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tret = -EADDRINUSE;\n\t\tgoto free_service_name;\n\t}\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;\n\tnfc_llcp_sock_link(&local->sockets, sk);\n\tpr_debug(\"Socket bound to SAP %d\\n\", llcp_sock->ssap);\n\tsk->sk_state = LLCP_BOUND;\n\tnfc_put_device(dev);\n\trelease_sock(sk);\n\treturn 0;\nfree_service_name:\n\tkfree(llcp_sock->service_name);\n\tllcp_sock->service_name = NULL;\nsock_llcp_put_local:\n\tnfc_llcp_local_put(llcp_sock->local);\n\tllcp_sock->local = NULL;\n\tllcp_sock->dev = NULL;\nput_dev:\n\tnfc_put_device(dev);\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 61
    },
    {
        "code": "int nfc_llcp_data_received(struct nfc_dev *dev, struct sk_buff *skb)\n{\n\tstruct nfc_llcp_local *local;\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tkfree_skb(skb);\n\t\treturn -ENODEV;\n\t}\n\t__nfc_llcp_recv(local, skb);\n\tnfc_llcp_local_put(local);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 63
    },
    {
        "code": "void nfc_llcp_unregister_device(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local = nfc_llcp_remove_local(dev);\n\tif (local == NULL) {\n\t\tpr_debug(\"No such device\\n\");\n\t\treturn;\n\t}\n\tlocal_cleanup(local);\n\tnfc_llcp_local_put(local);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 65
    },
    {
        "code": "u8 *nfc_llcp_general_bytes(struct nfc_dev *dev, size_t *general_bytes_len)\n{\n\tstruct nfc_llcp_local *local;\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\t*general_bytes_len = 0;\n\t\treturn NULL;\n\t}\n\tnfc_llcp_build_gb(local);\n\t*general_bytes_len = local->gb_len;\n\tnfc_llcp_local_put(local);\n\treturn local->gb;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 67
    },
    {
        "code": "int nfc_llcp_set_remote_gb(struct nfc_dev *dev, const u8 *gb, u8 gb_len)\n{\n\tstruct nfc_llcp_local *local;\n\tint err;\n\tif (gb_len < 3 || gb_len > NFC_MAX_GT_LEN)\n\t\treturn -EINVAL;\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tpr_err(\"No LLCP device\\n\");\n\t\treturn -ENODEV;\n\t}\n\tmemset(local->remote_gb, 0, NFC_MAX_GT_LEN);\n\tmemcpy(local->remote_gb, gb, gb_len);\n\tlocal->remote_gb_len = gb_len;\n\tif (memcmp(local->remote_gb, llcp_magic, 3)) {\n\t\tpr_err(\"MAC does not support LLCP\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\terr = nfc_llcp_parse_gb_tlv(local,\n\t\t\t\t     &local->remote_gb[3],\n\t\t\t\t     local->remote_gb_len - 3);\nout:\n\tnfc_llcp_local_put(local);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 69
    },
    {
        "code": "static noinline int test_btrfs_get_extent(u32 sectorsize, u32 nodesize)\n{\n\tstruct btrfs_fs_info *fs_info = NULL;\n\tstruct inode *inode = NULL;\n\tstruct btrfs_root *root = NULL;\n\tstruct extent_map *em = NULL;\n\tu64 orig_start;\n\tu64 disk_bytenr;\n\tu64 offset;\n\tint ret = -ENOMEM;\n\ttest_msg(\"running btrfs_get_extent tests\");\n\tinode = btrfs_new_test_inode();\n\tif (!inode) {\n\t\ttest_std_err(TEST_ALLOC_INODE);\n\t\treturn ret;\n\t}\n\tinode->i_mode = S_IFREG;\n\tBTRFS_I(inode)->location.type = BTRFS_INODE_ITEM_KEY;\n\tBTRFS_I(inode)->location.objectid = BTRFS_FIRST_FREE_OBJECTID;\n\tBTRFS_I(inode)->location.offset = 0;\n\tfs_info = btrfs_alloc_dummy_fs_info(nodesize, sectorsize);\n\tif (!fs_info) {\n\t\ttest_std_err(TEST_ALLOC_FS_INFO);\n\t\tgoto out;\n\t}\n\troot = btrfs_alloc_dummy_root(fs_info);\n\tif (IS_ERR(root)) {\n\t\ttest_std_err(TEST_ALLOC_ROOT);\n\t\tgoto out;\n\t}\n\troot->node = alloc_dummy_extent_buffer(fs_info, nodesize);\n\tif (!root->node) {\n\t\ttest_std_err(TEST_ALLOC_ROOT);\n\t\tgoto out;\n\t}\n\tbtrfs_set_header_nritems(root->node, 0);\n\tbtrfs_set_header_level(root->node, 0);\n\tret = -EINVAL;\n\t/* First with no extents */\n\tBTRFS_I(inode)->root = root;\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\tem = NULL;\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tfree_extent_map(em);\n\tbtrfs_drop_extent_cache(BTRFS_I(inode), 0, (u64)-1, 0);\n\t/*\n\t * All of the magic numbers are based on the mapping setup in\n\t * setup_file_extents, so if you change anything there you need to\n\t * update the comment and update the expected values below.\n\t */\n\tsetup_file_extents(root, sectorsize);\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, (u64)-1, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != 0 || em->len != 5) {\n\t\ttest_err(\n\t\t\"unexpected extent wanted start 0 len 5, got start %llu len %llu\",\n\t\t\tem->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_INLINE) {\n\t\ttest_err(\"expected an inline, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != (sectorsize - 5)) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len 1, got start %llu len %llu\",\n\t\t\toffset, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\t/*\n\t * We don't test anything else for inline since it doesn't get set\n\t * unless we have a page for it to write into.  Maybe we should change\n\t * this?\n\t */\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {",
        "bug_line_number": [],
        "vul": 0,
        "id": 71
    },
    {
        "code": "struct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *sub_root = root;\n\tstruct btrfs_key location;\n\tu8 di_type = 0;\n\tint index;\n\tint ret = 0;\n\tif (dentry->d_name.len > BTRFS_NAME_LEN)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\tret = btrfs_inode_by_name(dir, dentry, &location, &di_type);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\tif (location.type == BTRFS_INODE_ITEM_KEY) {\n\t\tinode = btrfs_iget(dir->i_sb, &location, root, NULL);\n\t\tif (IS_ERR(inode))\n\t\t\treturn inode;\n\t\t/* Do extra check against inode mode with di_type */\n\t\tif (btrfs_inode_type(inode) != di_type) {\n\t\t\tbtrfs_crit(fs_info,\n\"inode mode mismatch with dir: inode mode=0%o btrfs type=%u dir type=%u\",\n\t\t\t\t  inode->i_mode, btrfs_inode_type(inode),\n\t\t\t\t  di_type);\n\t\t\tiput(inode);\n\t\t\treturn ERR_PTR(-EUCLEAN);\n\t\t}\n\t\treturn inode;\n\t}\n\tindex = srcu_read_lock(&fs_info->subvol_srcu);\n\tret = fixup_tree_root_location(fs_info, dir, dentry,\n\t\t\t\t       &location, &sub_root);\n\tif (ret < 0) {\n\t\tif (ret != -ENOENT)\n\t\t\tinode = ERR_PTR(ret);\n\t\telse\n\t\t\tinode = new_simple_dir(dir->i_sb, &location, sub_root);\n\t} else {\n\t\tinode = btrfs_iget(dir->i_sb, &location, sub_root, NULL);\n\t}\n\tsrcu_read_unlock(&fs_info->subvol_srcu, index);\n\tif (!IS_ERR(inode) && root != sub_root) {\n\t\tdown_read(&fs_info->cleanup_work_sem);\n\t\tif (!sb_rdonly(inode->i_sb))\n\t\t\tret = btrfs_orphan_cleanup(sub_root);\n\t\tup_read(&fs_info->cleanup_work_sem);\n\t\tif (ret) {\n\t\t\tiput(inode);\n\t\t\tinode = ERR_PTR(ret);\n\t\t}\n\t}\n\treturn inode;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 73
    },
    {
        "code": "static void nft_setelem_catchall_remove(const struct net *net,\n\t\t\t\t\tconst struct nft_set *set,\n\t\t\t\t\tstruct nft_elem_priv *elem_priv)\n{\n\tstruct nft_set_elem_catchall *catchall, *next;\n\tlist_for_each_entry_safe(catchall, next, &set->catchall_list, list) {\n\t\tif (catchall->elem == elem_priv) {\n\t\t\tnft_setelem_catchall_destroy(catchall);\n\t\t\tbreak;\n\t\t}\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 75
    },
    {
        "code": "static void k_fn(struct vc_data *vc, unsigned char value, char up_flag)\n{\n\tif (up_flag)\n\t\treturn;\n\tif ((unsigned)value < ARRAY_SIZE(func_table)) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&func_buf_lock, flags);\n\t\tif (func_table[value])\n\t\t\tputs_queue(vc, func_table[value]);\n\t\tspin_unlock_irqrestore(&func_buf_lock, flags);\n\t} else\n\t\tpr_err(\"k_fn called with value=%d\\n\", value);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 77
    },
    {
        "code": "static void sixpack_close(struct tty_struct *tty)\n{\n\tstruct sixpack *sp;\n\twrite_lock_irq(&disc_data_lock);\n\tsp = tty->disc_data;\n\ttty->disc_data = NULL;\n\twrite_unlock_irq(&disc_data_lock);\n\tif (!sp)\n\t\treturn;\n\t/*\n\t * We have now ensured that nobody can start using ap from now on, but\n\t * we have to wait for all existing users to finish.\n\t */\n\tif (!refcount_dec_and_test(&sp->refcnt))\n\t\twait_for_completion(&sp->dead);\n\t/* We must stop the queue to avoid potentially scribbling\n\t * on the free buffers. The sp->dead completion is not sufficient\n\t * to protect us from sp->xbuff access.\n\t */\n\tnetif_stop_queue(sp->dev);\n\tunregister_netdev(sp->dev);\n\tdel_timer_sync(&sp->tx_t);\n\tdel_timer_sync(&sp->resync_t);\n\t/* Free all 6pack frame buffers after unreg. */\n\tkfree(sp->rbuff);\n\tkfree(sp->xbuff);\n\tfree_netdev(sp->dev);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 79
    },
    {
        "code": "static void blk_add_trace_split(void *ignore,\n\t\t\t\tstruct request_queue *q, struct bio *bio,\n\t\t\t\tunsigned int pdu)\n{\n\tstruct blk_trace *bt;\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (bt) {\n\t\t__be64 rpdu = cpu_to_be64(pdu);\n\t\t__blk_add_trace(bt, bio->bi_iter.bi_sector,\n\t\t\t\tbio->bi_iter.bi_size, bio_op(bio), bio->bi_opf,\n\t\t\t\tBLK_TA_SPLIT, bio->bi_status, sizeof(rpdu),\n\t\t\t\t&rpdu, blk_trace_bio_get_cgid(q, bio));\n\t}\n\trcu_read_unlock();\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 81
    },
    {
        "code": "static ssize_t sysfs_blk_trace_attr_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct hd_struct *p = dev_to_part(dev);\n\tstruct request_queue *q;\n\tstruct block_device *bdev;\n\tstruct blk_trace *bt;\n\tssize_t ret = -ENXIO;\n\tbdev = bdget(part_devt(p));\n\tif (bdev == NULL)\n\t\tgoto out;\n\tq = blk_trace_get_queue(bdev);\n\tif (q == NULL)\n\t\tgoto out_bdput;\n\tmutex_lock(&q->blk_trace_mutex);\n\tbt = rcu_dereference_protected(q->blk_trace,\n\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));\n\tif (attr == &dev_attr_enable) {\n\t\tret = sprintf(buf, \"%u\\n\", !!bt);\n\t\tgoto out_unlock_bdev;\n\t}\n\tif (bt == NULL)\n\t\tret = sprintf(buf, \"disabled\\n\");\n\telse if (attr == &dev_attr_act_mask)\n\t\tret = blk_trace_mask2str(buf, bt->act_mask);\n\telse if (attr == &dev_attr_pid)\n\t\tret = sprintf(buf, \"%u\\n\", bt->pid);\n\telse if (attr == &dev_attr_start_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", bt->start_lba);\n\telse if (attr == &dev_attr_end_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", bt->end_lba);\nout_unlock_bdev:\n\tmutex_unlock(&q->blk_trace_mutex);\nout_bdput:\n\tbdput(bdev);\nout:\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 83
    },
    {
        "code": "static void blk_add_trace_unplug(void *ignore, struct request_queue *q,\n\t\t\t\t    unsigned int depth, bool explicit)\n{\n\tstruct blk_trace *bt;\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (bt) {\n\t\t__be64 rpdu = cpu_to_be64(depth);\n\t\tu32 what;\n\t\tif (explicit)\n\t\t\twhat = BLK_TA_UNPLUG_IO;\n\t\telse\n\t\t\twhat = BLK_TA_UNPLUG_TIMER;\n\t\t__blk_add_trace(bt, 0, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu, 0);\n\t}\n\trcu_read_unlock();\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 85
    },
    {
        "code": "void blk_add_driver_data(struct request_queue *q,\n\t\t\t struct request *rq,\n\t\t\t void *data, size_t len)\n{\n\tstruct blk_trace *bt;\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\t__blk_add_trace(bt, blk_rq_trace_sector(rq), blk_rq_bytes(rq), 0, 0,\n\t\t\t\tBLK_TA_DRV_DATA, 0, len, data,\n\t\t\t\tblk_trace_request_get_cgid(q, rq));\n\trcu_read_unlock();\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 87
    },
    {
        "code": "static void blk_add_trace_getrq(void *ignore,\n\t\t\t\tstruct request_queue *q,\n\t\t\t\tstruct bio *bio, int rw)\n{\n\tif (bio)\n\t\tblk_add_trace_bio(q, bio, BLK_TA_GETRQ, 0);\n\telse {\n\t\tstruct blk_trace *bt;\n\t\trcu_read_lock();\n\t\tbt = rcu_dereference(q->blk_trace);\n\t\tif (bt)\n\t\t\t__blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_GETRQ, 0, 0,\n\t\t\t\t\tNULL, 0);\n\t\trcu_read_unlock();\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 89
    },
    {
        "code": "void rose_start_idletimer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\tsk_stop_timer(sk, &rose->idletimer);\n\tif (rose->idle > 0) {\n\t\trose->idletimer.function = rose_idletimer_expiry;\n\t\trose->idletimer.expires  = jiffies + rose->idle;\n\t\tsk_reset_timer(sk, &rose->idletimer, rose->idletimer.expires);\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 91
    },
    {
        "code": "void rose_start_t3timer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\tsk_stop_timer(sk, &rose->timer);\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->t3;\n\tsk_reset_timer(sk, &rose->timer, rose->timer.expires);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 93
    },
    {
        "code": "void rose_start_hbtimer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\tsk_stop_timer(sk, &rose->timer);\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->hb;\n\tsk_reset_timer(sk, &rose->timer, rose->timer.expires);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 95
    },
    {
        "code": "void rose_start_t2timer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\tsk_stop_timer(sk, &rose->timer);\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->t2;\n\tsk_reset_timer(sk, &rose->timer, rose->timer.expires);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 97
    },
    {
        "code": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct desc;\n\tshort sel;\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\tif (!get_desc(&desc, sel))\n\t\treturn -EINVAL;\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc.type & BIT(3)))\n\t\treturn -EINVAL;\n\tswitch ((desc.l << 1) | desc.d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 99
    },
    {
        "code": "static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct desc;\n\tunsigned long limit;\n\tshort sel;\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn 0;\n\tif (user_64bit_mode(regs) || v8086_mode(regs))\n\t\treturn -1L;\n\tif (!sel)\n\t\treturn 0;\n\tif (!get_desc(&desc, sel))\n\t\treturn 0;\n\t/*\n\t * If the granularity bit is set, the limit is given in multiples\n\t * of 4096. This also means that the 12 least significant bits are\n\t * not tested when checking the segment limits. In practice,\n\t * this means that the segment ends in (limit << 12) + 0xfff.\n\t */\n\tlimit = get_desc_limit(&desc);\n\tif (desc.g)\n\t\tlimit = (limit << 12) + 0xfff;\n\treturn limit;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 101
    },
    {
        "code": "unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct desc;\n\tshort sel;\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn -1L;\n\tif (v8086_mode(regs))\n\t\t/*\n\t\t * Base is simply the segment selector shifted 4\n\t\t * bits to the right.\n\t\t */\n\t\treturn (unsigned long)(sel << 4);\n\tif (user_64bit_mode(regs)) {\n\t\t/*\n\t\t * Only FS or GS will have a base address, the rest of\n\t\t * the segments' bases are forced to 0.\n\t\t */\n\t\tunsigned long base;\n\t\tif (seg_reg_idx == INAT_SEG_REG_FS)\n\t\t\trdmsrl(MSR_FS_BASE, base);\n\t\telse if (seg_reg_idx == INAT_SEG_REG_GS)\n\t\t\t/*\n\t\t\t * swapgs was called at the kernel entry point. Thus,\n\t\t\t * MSR_KERNEL_GS_BASE will have the user-space GS base.\n\t\t\t */\n\t\t\trdmsrl(MSR_KERNEL_GS_BASE, base);\n\t\telse\n\t\t\tbase = 0;\n\t\treturn base;\n\t}\n\t/* In protected mode the segment selector cannot be null. */\n\tif (!sel)\n\t\treturn -1L;\n\tif (!get_desc(&desc, sel))\n\t\treturn -1L;\n\treturn get_desc_base(&desc);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 103
    },
    {
        "code": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tio_req_init_async(req);\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\tif (!req->work.creds)\n\t\treq->work.creds = get_current_cred();\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 105
    },
    {
        "code": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->task->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 107
    },
    {
        "code": "static void io_worker_handle_work(struct io_worker *worker)\n\t__releases(wqe->lock)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wq *wq = wqe->wq;\n\tdo {\n\t\tstruct io_wq_work *work;\nget_next:\n\t\t/*\n\t\t * If we got some work, mark us as busy. If we didn't, but\n\t\t * the list isn't empty, it means we stalled on hashed work.\n\t\t * Mark us stalled so we don't keep looking for work when we\n\t\t * can't make progress, any work completion or insertion will\n\t\t * clear the stalled flag.\n\t\t */\n\t\twork = io_get_next_work(wqe);\n\t\tif (work)\n\t\t\t__io_worker_busy(wqe, worker, work);\n\t\telse if (!wq_list_empty(&wqe->work_list))\n\t\t\twqe->flags |= IO_WQE_FLAG_STALLED;\n\t\traw_spin_unlock_irq(&wqe->lock);\n\t\tif (!work)\n\t\t\tbreak;\n\t\tio_assign_current_work(worker, work);\n\t\t/* handle a whole dependent link */\n\t\tdo {\n\t\t\tstruct io_wq_work *next_hashed, *linked;\n\t\t\tunsigned int hash = io_get_work_hash(work);\n\t\t\tnext_hashed = wq_next_work(work);\n\t\t\tif (work->creds && worker->cur_creds != work->creds)\n\t\t\t\tio_wq_switch_creds(worker, work);\n\t\t\twq->do_work(work);\n\t\t\tio_assign_current_work(worker, NULL);\n\t\t\tlinked = wq->free_work(work);\n\t\t\twork = next_hashed;\n\t\t\tif (!work && linked && !io_wq_is_hashed(linked)) {\n\t\t\t\twork = linked;\n\t\t\t\tlinked = NULL;\n\t\t\t}\n\t\t\tio_assign_current_work(worker, work);\n\t\t\tif (linked)\n\t\t\t\tio_wqe_enqueue(wqe, linked);\n\t\t\tif (hash != -1U && !next_hashed) {\n\t\t\t\traw_spin_lock_irq(&wqe->lock);\n\t\t\t\twqe->hash_map &= ~BIT_ULL(hash);\n\t\t\t\twqe->flags &= ~IO_WQE_FLAG_STALLED;\n\t\t\t\t/* skip unnecessary unlock-lock wqe->lock */\n\t\t\t\tif (!work)\n\t\t\t\t\tgoto get_next;\n\t\t\t\traw_spin_unlock_irq(&wqe->lock);\n\t\t\t}\n\t\t} while (work);\n\t\traw_spin_lock_irq(&wqe->lock);\n\t} while (1);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 109
    },
    {
        "code": "static int writeback_single_inode(struct inode *inode,\n\t\t\t\t  struct writeback_control *wbc)\n{\n\tstruct bdi_writeback *wb;\n\tint ret = 0;\n\tspin_lock(&inode->i_lock);\n\tif (!atomic_read(&inode->i_count))\n\t\tWARN_ON(!(inode->i_state & (I_WILL_FREE|I_FREEING)));\n\telse\n\t\tWARN_ON(inode->i_state & I_WILL_FREE);\n\tif (inode->i_state & I_SYNC) {\n\t\t/*\n\t\t * Writeback is already running on the inode.  For WB_SYNC_NONE,\n\t\t * that's enough and we can just return.  For WB_SYNC_ALL, we\n\t\t * must wait for the existing writeback to complete, then do\n\t\t * writeback again if there's anything left.\n\t\t */\n\t\tif (wbc->sync_mode != WB_SYNC_ALL)\n\t\t\tgoto out;\n\t\t__inode_wait_for_writeback(inode);\n\t}\n\tWARN_ON(inode->i_state & I_SYNC);\n\t/*\n\t * If the inode is already fully clean, then there's nothing to do.\n\t *\n\t * For data-integrity syncs we also need to check whether any pages are\n\t * still under writeback, e.g. due to prior WB_SYNC_NONE writeback.  If\n\t * there are any such pages, we'll need to wait for them.\n\t */\n\tif (!(inode->i_state & I_DIRTY_ALL) &&\n\t    (wbc->sync_mode != WB_SYNC_ALL ||\n\t     !mapping_tagged(inode->i_mapping, PAGECACHE_TAG_WRITEBACK)))\n\t\tgoto out;\n\tinode->i_state |= I_SYNC;\n\twbc_attach_and_unlock_inode(wbc, inode);\n\tret = __writeback_single_inode(inode, wbc);\n\twbc_detach_inode(wbc);\n\twb = inode_to_wb_and_lock_list(inode);\n\tspin_lock(&inode->i_lock);\n\t/*\n\t * If the inode is freeing, its i_io_list shoudn't be updated\n\t * as it can be finally deleted at this moment.\n\t */\n\tif (!(inode->i_state & I_FREEING)) {\n\t\t/*\n\t\t * If the inode is now fully clean, then it can be safely\n\t\t * removed from its writeback list (if any). Otherwise the\n\t\t * flusher threads are responsible for the writeback lists.\n\t\t */\n\t\tif (!(inode->i_state & I_DIRTY_ALL))\n\t\t\tinode_cgwb_move_to_attached(inode, wb);\n\t\telse if (!(inode->i_state & I_SYNC_QUEUED)) {\n\t\t\tif ((inode->i_state & I_DIRTY))\n\t\t\t\tredirty_tail_locked(inode, wb);\n\t\t\telse if (inode->i_state & I_DIRTY_TIME) {\n\t\t\t\tinode->dirtied_when = jiffies;\n\t\t\t\tinode_io_list_move_locked(inode,\n\t\t\t\t\t\t\t  wb,\n\t\t\t\t\t\t\t  &wb->b_dirty_time);\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock(&wb->list_lock);\n\tinode_sync_complete(inode);\nout:\n\tspin_unlock(&inode->i_lock);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 111
    },
    {
        "code": "int setup_async_work(struct ksmbd_work *work, void (*fn)(void **), void **arg)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct ksmbd_conn *conn = work->conn;\n\tint id;\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\trsp_hdr->Flags |= SMB2_FLAGS_ASYNC_COMMAND;\n\tid = ksmbd_acquire_async_msg_id(&conn->async_ida);\n\tif (id < 0) {\n\t\tpr_err(\"Failed to alloc async message id\\n\");\n\t\treturn id;\n\t}\n\twork->asynchronous = true;\n\twork->async_id = id;\n\trsp_hdr->Id.AsyncId = cpu_to_le64(id);\n\tksmbd_debug(SMB,\n\t\t    \"Send interim Response to inform async request id : %d\\n\",\n\t\t    work->async_id);\n\twork->cancel_fn = fn;\n\twork->cancel_argv = arg;\n\tif (list_empty(&work->async_request_entry)) {\n\t\tspin_lock(&conn->request_lock);\n\t\tlist_add_tail(&work->async_request_entry, &conn->async_requests);\n\t\tspin_unlock(&conn->request_lock);\n\t}\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 113
    },
    {
        "code": "static int ravb_close(struct net_device *ndev)\n{\n\tstruct device_node *np = ndev->dev.parent->of_node;\n\tstruct ravb_private *priv = netdev_priv(ndev);\n\tconst struct ravb_hw_info *info = priv->info;\n\tstruct ravb_tstamp_skb *ts_skb, *ts_skb2;\n\tnetif_tx_stop_all_queues(ndev);\n\t/* Disable interrupts by clearing the interrupt masks. */\n\travb_write(ndev, 0, RIC0);\n\travb_write(ndev, 0, RIC2);\n\travb_write(ndev, 0, TIC);\n\t/* Stop PTP Clock driver */\n\tif (info->gptp)\n\t\travb_ptp_stop(ndev);\n\t/* Set the config mode to stop the AVB-DMAC's processes */\n\tif (ravb_stop_dma(ndev) < 0)\n\t\tnetdev_err(ndev,\n\t\t\t   \"device will be stopped after h/w processes are done.\\n\");\n\t/* Clear the timestamp list */\n\tif (info->gptp || info->ccc_gac) {\n\t\tlist_for_each_entry_safe(ts_skb, ts_skb2, &priv->ts_skb_list, list) {\n\t\t\tlist_del(&ts_skb->list);\n\t\t\tkfree_skb(ts_skb->skb);\n\t\t\tkfree(ts_skb);\n\t\t}\n\t}\n\t/* PHY disconnect */\n\tif (ndev->phydev) {\n\t\tphy_stop(ndev->phydev);\n\t\tphy_disconnect(ndev->phydev);\n\t\tif (of_phy_is_fixed_link(np))\n\t\t\tof_phy_deregister_fixed_link(np);\n\t}\n\tcancel_work_sync(&priv->work);\n\tif (info->multi_irqs) {\n\t\tfree_irq(priv->tx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->tx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->emac_irq, ndev);\n\t\tif (info->err_mgmt_irqs) {\n\t\t\tfree_irq(priv->erra_irq, ndev);\n\t\t\tfree_irq(priv->mgmta_irq, ndev);\n\t\t}\n\t}\n\tfree_irq(ndev->irq, ndev);\n\tif (info->nc_queues)\n\t\tnapi_disable(&priv->napi[RAVB_NC]);\n\tnapi_disable(&priv->napi[RAVB_BE]);\n\t/* Free all the skb's in the RX queue and the DMA buffers. */\n\travb_ring_free(ndev, RAVB_BE);\n\tif (info->nc_queues)\n\t\travb_ring_free(ndev, RAVB_NC);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 115
    },
    {
        "code": "\tif (!(fault & VM_FAULT_RETRY)) {\n\t\tcount_vm_vma_lock_event(VMA_LOCK_SUCCESS);\n\t\tgoto done;\n\t}\n\tcount_vm_vma_lock_event(VMA_LOCK_RETRY);\n\t/* Quick path to respond to signals */\n\tif (fault_signal_pending(fault, regs)) {\n\t\tif (!user_mode(regs))\n\t\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t\t SIGBUS, BUS_ADRERR,\n\t\t\t\t\t\t ARCH_DEFAULT_PKEY);\n\t\treturn;\n\t}\nlock_mmap:\n#endif /* CONFIG_PER_VMA_LOCK */\nretry:\n\tvma = lock_mm_and_find_vma(mm, address, regs);\n\tif (unlikely(!vma)) {\n\t\tbad_area_nosemaphore(regs, error_code, address);\n\t\treturn;\n\t}\n\t/*\n\t * Ok, we have a good vm_area for this memory access, so\n\t * we can handle it..\n\t */\n\tif (unlikely(access_error(error_code, vma))) {\n\t\tbad_area_access_error(regs, error_code, address, vma);\n\t\treturn;\n\t}\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.  Since we never set FAULT_FLAG_RETRY_NOWAIT, if\n\t * we get VM_FAULT_RETRY back, the mmap_lock has been unlocked.\n\t *\n\t * Note that handle_userfault() may also release and reacquire mmap_lock\n\t * (and not return with VM_FAULT_RETRY), when returning to userland to\n\t * repeat the page fault later with a VM_FAULT_NOPAGE retval\n\t * (potentially after handling any pending signal during the return to\n\t * userland). The return to userland is identified whenever\n\t * FAULT_FLAG_USER|FAULT_FLAG_KILLABLE are both set in flags.\n\t */\n\tfault = handle_mm_fault(vma, address, flags, regs);\n\tif (fault_signal_pending(fault, regs)) {\n\t\t/*\n\t\t * Quick path to respond to signals.  The core mm code\n\t\t * has unlocked the mm for us if we get here.\n\t\t */\n\t\tif (!user_mode(regs))\n\t\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t\t SIGBUS, BUS_ADRERR,\n\t\t\t\t\t\t ARCH_DEFAULT_PKEY);\n\t\treturn;\n\t}\n\t/* The fault is fully completed (including releasing mmap lock) */\n\tif (fault & VM_FAULT_COMPLETED)\n\t\treturn;\n\t/*\n\t * If we need to retry the mmap_lock has already been released,\n\t * and if there is a fatal signal pending there is no guarantee\n\t * that we made any progress. Handle this case first.\n\t */\n\tif (unlikely(fault & VM_FAULT_RETRY)) {\n\t\tflags |= FAULT_FLAG_TRIED;\n\t\tgoto retry;\n\t}\n\tmmap_read_unlock(mm);\n#ifdef CONFIG_PER_VMA_LOCK\ndone:\n#endif\n\tif (likely(!(fault & VM_FAULT_ERROR)))\n\t\treturn;\n\tif (fatal_signal_pending(current) && !user_mode(regs)) {\n\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t 0, 0, ARCH_DEFAULT_PKEY);\n\t\treturn;\n\t}\n\tif (fault & VM_FAULT_OOM) {\n\t\t/* Kernel mode? Handle exceptions or die: */\n\t\tif (!user_mode(regs)) {\n\t\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t\t SIGSEGV, SEGV_MAPERR,\n\t\t\t\t\t\t ARCH_DEFAULT_PKEY);\n\t\t\treturn;\n\t\t}\n\t\t/*\n\t\t * We ran out of memory, call the OOM killer, and return the\n\t\t * userspace (which will retry the fault, or kill us if we got\n\t\t * oom-killed):\n\t\t */\n\t\tpagefault_out_of_memory();\n\t} else {\n\t\tif (fault & (VM_FAULT_SIGBUS|VM_FAULT_HWPOISON|\n\t\t\t     VM_FAULT_HWPOISON_LARGE))\n\t\t\tdo_sigbus(regs, error_code, address, fault);\n\t\telse if (fault & VM_FAULT_SIGSEGV)\n\t\t\tbad_area_nosemaphore(regs, error_code, address);\n\t\telse\n\t\t\tBUG();\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 117
    },
    {
        "code": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tio_req_init_async(req);\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\tif (!req->work.creds)\n\t\treq->work.creds = get_current_cred();\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 119
    },
    {
        "code": "static int fuse_get_user_pages(struct fuse_args_pages *ap, struct iov_iter *ii,\n\t\t\t       size_t *nbytesp, int write,\n\t\t\t       unsigned int max_pages)\n{\n\tsize_t nbytes = 0;  /* # bytes already packed in req */\n\tssize_t ret = 0;\n\t/* Special case for kernel I/O: can copy directly into the buffer */\n\tif (iov_iter_is_kvec(ii)) {\n\t\tunsigned long user_addr = fuse_get_user_addr(ii);\n\t\tsize_t frag_size = fuse_get_frag_size(ii, *nbytesp);\n\t\tif (write)\n\t\t\tap->args.in_args[1].value = (void *) user_addr;\n\t\telse\n\t\t\tap->args.out_args[0].value = (void *) user_addr;\n\t\tiov_iter_advance(ii, frag_size);\n\t\t*nbytesp = frag_size;\n\t\treturn 0;\n\t}\n\twhile (nbytes < *nbytesp && ap->num_pages < max_pages) {\n\t\tunsigned npages;\n\t\tsize_t start;\n\t\tret = iov_iter_get_pages(ii, &ap->pages[ap->num_pages],\n\t\t\t\t\t*nbytesp - nbytes,\n\t\t\t\t\tmax_pages - ap->num_pages,\n\t\t\t\t\t&start);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tiov_iter_advance(ii, ret);\n\t\tnbytes += ret;\n\t\tret += start;\n\t\tnpages = DIV_ROUND_UP(ret, PAGE_SIZE);\n\t\tap->descs[ap->num_pages].offset = start;\n\t\tfuse_page_descs_length_init(ap->descs, ap->num_pages, npages);\n\t\tap->num_pages += npages;\n\t\tap->descs[ap->num_pages - 1].length -=\n\t\t\t(PAGE_SIZE - ret) & (PAGE_SIZE - 1);\n\t}\n\tap->args.user_pages = true;\n\tif (write)\n\t\tap->args.in_pages = true;\n\telse\n\t\tap->args.out_pages = true;\n\t*nbytesp = nbytes;\n\treturn ret < 0 ? ret : 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 121
    },
    {
        "code": "static int fuse_copy_page(struct fuse_copy_state *cs, struct page **pagep,\n\t\t\t  unsigned offset, unsigned count, int zeroing)\n{\n\tint err;\n\tstruct page *page = *pagep;\n\tif (page && zeroing && count < PAGE_SIZE)\n\t\tclear_highpage(page);\n\twhile (count) {\n\t\tif (cs->write && cs->pipebufs && page) {\n\t\t\t/*\n\t\t\t * Can't control lifetime of pipe buffers, so always\n\t\t\t * copy user pages.\n\t\t\t */\n\t\t\tif (cs->req->args->user_pages) {\n\t\t\t\terr = fuse_copy_fill(cs);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t} else {\n\t\t\t\treturn fuse_ref_page(cs, page, offset, count);\n\t\t\t}\n\t\t} else if (!cs->len) {\n\t\t\tif (cs->move_pages && page &&\n\t\t\t    offset == 0 && count == PAGE_SIZE) {\n\t\t\t\terr = fuse_try_move_page(cs, pagep);\n\t\t\t\tif (err <= 0)\n\t\t\t\t\treturn err;\n\t\t\t} else {\n\t\t\t\terr = fuse_copy_fill(cs);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t\tif (page) {\n\t\t\tvoid *mapaddr = kmap_local_page(page);\n\t\t\tvoid *buf = mapaddr + offset;\n\t\t\toffset += fuse_copy_do(cs, &buf, &count);\n\t\t\tkunmap_local(mapaddr);\n\t\t} else\n\t\t\toffset += fuse_copy_do(cs, NULL, &count);\n\t}\n\tif (page && !cs->write)\n\t\tflush_dcache_page(page);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 123
    },
    {
        "code": "int prepare_to_relocate(struct reloc_control *rc)\n{\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\trc->block_rsv = btrfs_alloc_block_rsv(rc->extent_root->fs_info,\n\t\t\t\t\t      BTRFS_BLOCK_RSV_TEMP);\n\tif (!rc->block_rsv)\n\t\treturn -ENOMEM;\n\tmemset(&rc->cluster, 0, sizeof(rc->cluster));\n\trc->search_start = rc->block_group->start;\n\trc->extents_found = 0;\n\trc->nodes_relocated = 0;\n\trc->merging_rsv_size = 0;\n\trc->reserved_bytes = 0;\n\trc->block_rsv->size = rc->extent_root->fs_info->nodesize *\n\t\t\t      RELOCATION_RESERVED_NODES;\n\tret = btrfs_block_rsv_refill(rc->extent_root->fs_info,\n\t\t\t\t     rc->block_rsv, rc->block_rsv->size,\n\t\t\t\t     BTRFS_RESERVE_FLUSH_ALL);\n\tif (ret)\n\t\treturn ret;\n\trc->create_reloc_tree = 1;\n\tset_reloc_control(rc);\n\ttrans = btrfs_join_transaction(rc->extent_root);\n\tif (IS_ERR(trans)) {\n\t\tunset_reloc_control(rc);\n\t\t/*\n\t\t * extent tree is not a ref_cow tree and has no reloc_root to\n\t\t * cleanup.  And callers are responsible to free the above\n\t\t * block rsv.\n\t\t */\n\t\treturn PTR_ERR(trans);\n\t}\n\tret = btrfs_commit_transaction(trans);\n\tif (ret)\n\t\tunset_reloc_control(rc);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 125
    },
    {
        "code": "static struct sk_buff *qfq_dequeue(struct Qdisc *sch)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_aggregate *in_serv_agg = q->in_serv_agg;\n\tstruct qfq_class *cl;\n\tstruct sk_buff *skb = NULL;\n\t/* next-packet len, 0 means no more active classes in in-service agg */\n\tunsigned int len = 0;\n\tif (in_serv_agg == NULL)\n\t\treturn NULL;\n\tif (!list_empty(&in_serv_agg->active))\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\t/*\n\t * If there are no active classes in the in-service aggregate,\n\t * or if the aggregate has not enough budget to serve its next\n\t * class, then choose the next aggregate to serve.\n\t */\n\tif (len == 0 || in_serv_agg->budget < len) {\n\t\tcharge_actual_service(in_serv_agg);\n\t\t/* recharge the budget of the aggregate */\n\t\tin_serv_agg->initial_budget = in_serv_agg->budget =\n\t\t\tin_serv_agg->budgetmax;\n\t\tif (!list_empty(&in_serv_agg->active)) {\n\t\t\t/*\n\t\t\t * Still active: reschedule for\n\t\t\t * service. Possible optimization: if no other\n\t\t\t * aggregate is active, then there is no point\n\t\t\t * in rescheduling this aggregate, and we can\n\t\t\t * just keep it as the in-service one. This\n\t\t\t * should be however a corner case, and to\n\t\t\t * handle it, we would need to maintain an\n\t\t\t * extra num_active_aggs field.\n\t\t\t*/\n\t\t\tqfq_update_agg_ts(q, in_serv_agg, requeue);\n\t\t\tqfq_schedule_agg(q, in_serv_agg);\n\t\t} else if (sch->q.qlen == 0) { /* no aggregate to serve */\n\t\t\tq->in_serv_agg = NULL;\n\t\t\treturn NULL;\n\t\t}\n\t\t/*\n\t\t * If we get here, there are other aggregates queued:\n\t\t * choose the new aggregate to serve.\n\t\t */\n\t\tin_serv_agg = q->in_serv_agg = qfq_choose_next_agg(q);\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\t}\n\tif (!skb)\n\t\treturn NULL;\n\tsch->q.qlen--;\n\tskb = agg_dequeue(in_serv_agg, cl, len);\n\tif (!skb) {\n\t\tsch->q.qlen++;\n\t\treturn NULL;\n\t}\n\tqdisc_qstats_backlog_dec(sch, skb);\n\tqdisc_bstats_update(sch, skb);\n\t/* If lmax is lowered, through qfq_change_class, for a class\n\t * owning pending packets with larger size than the new value\n\t * of lmax, then the following condition may hold.\n\t */\n\tif (unlikely(in_serv_agg->budget < len))\n\t\tin_serv_agg->budget = 0;\n\telse\n\t\tin_serv_agg->budget -= len;\n\tq->V += (u64)len * q->iwsum;\n\tpr_debug(\"qfq dequeue: len %u F %lld now %lld\\n\",\n\t\t len, (unsigned long long) in_serv_agg->F,\n\t\t (unsigned long long) q->V);\n\treturn skb;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 127
    },
    {
        "code": "\tif (drm_is_primary_client(file_priv))\n\t\tuser_srf->master = drm_file_get_master(file_priv);\n\t/**\n\t * From this point, the generic resource management functions\n\t * destroy the object on failure.\n\t */\n\tret = vmw_surface_init(dev_priv, srf, vmw_user_surface_free);\n\tif (unlikely(ret != 0))\n\t\tgoto out_unlock;\n\t/*\n\t * A gb-aware client referencing a shared surface will\n\t * expect a backup buffer to be present.\n\t */\n\tif (dev_priv->has_mob && req->shareable) {\n\t\tstruct vmw_bo_params params = {\n\t\t\t.domain = VMW_BO_DOMAIN_SYS,\n\t\t\t.busy_domain = VMW_BO_DOMAIN_SYS,\n\t\t\t.bo_type = ttm_bo_type_device,\n\t\t\t.size = res->guest_memory_size,\n\t\t\t.pin = false\n\t\t};\n\t\tret = vmw_gem_object_create(dev_priv,\n\t\t\t\t\t    &params,\n\t\t\t\t\t    &res->guest_memory_bo);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tvmw_resource_unreference(&res);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\ttmp = vmw_resource_reference(&srf->res);\n\tret = ttm_prime_object_init(tfile, res->guest_memory_size, &user_srf->prime,\n\t\t\t\t    req->shareable, VMW_RES_SURFACE,\n\t\t\t\t    &vmw_user_surface_base_release);\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&tmp);\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\trep->sid = user_srf->prime.base.handle;\n\tvmw_resource_unreference(&res);\n\treturn 0;\nout_no_copy:\n\tkfree(srf->offsets);\nout_no_offsets:\n\tkfree(metadata->sizes);\nout_no_sizes:\n\tttm_prime_object_kfree(user_srf, prime);\nout_unlock:\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 129
    },
    {
        "code": "static int vmw_create_bo_proxy(struct drm_device *dev,\n\t\t\t       const struct drm_mode_fb_cmd2 *mode_cmd,\n\t\t\t       struct vmw_bo *bo_mob,\n\t\t\t       struct vmw_surface **srf_out)\n{\n\tstruct vmw_surface_metadata metadata = {0};\n\tuint32_t format;\n\tstruct vmw_resource *res;\n\tunsigned int bytes_pp;\n\tint ret;\n\tswitch (mode_cmd->pixel_format) {\n\tcase DRM_FORMAT_ARGB8888:\n\tcase DRM_FORMAT_XRGB8888:\n\t\tformat = SVGA3D_X8R8G8B8;\n\t\tbytes_pp = 4;\n\t\tbreak;\n\tcase DRM_FORMAT_RGB565:\n\tcase DRM_FORMAT_XRGB1555:\n\t\tformat = SVGA3D_R5G6B5;\n\t\tbytes_pp = 2;\n\t\tbreak;\n\tcase 8:\n\t\tformat = SVGA3D_P8;\n\t\tbytes_pp = 1;\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Invalid framebuffer format %p4cc\\n\",\n\t\t\t  &mode_cmd->pixel_format);\n\t\treturn -EINVAL;\n\t}\n\tmetadata.format = format;\n\tmetadata.mip_levels[0] = 1;\n\tmetadata.num_sizes = 1;\n\tmetadata.base_size.width = mode_cmd->pitches[0] / bytes_pp;\n\tmetadata.base_size.height =  mode_cmd->height;\n\tmetadata.base_size.depth = 1;\n\tmetadata.scanout = true;\n\tret = vmw_gb_surface_define(vmw_priv(dev), &metadata, srf_out);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed to allocate proxy content buffer\\n\");\n\t\treturn ret;\n\t}\n\tres = &(*srf_out)->res;\n\t/* Reserve and switch the backing mob. */\n\tmutex_lock(&res->dev_priv->cmdbuf_mutex);\n\t(void) vmw_resource_reserve(res, false, true);\n\tvmw_user_bo_unref(&res->guest_memory_bo);\n\tres->guest_memory_bo = vmw_user_bo_ref(bo_mob);\n\tres->guest_memory_offset = 0;\n\tvmw_resource_unreserve(res, false, false, false, NULL, 0);\n\tmutex_unlock(&res->dev_priv->cmdbuf_mutex);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 131
    },
    {
        "code": "static irqreturn_t sunkbd_interrupt(struct serio *serio,\n\t\tunsigned char data, unsigned int flags)\n{\n\tstruct sunkbd *sunkbd = serio_get_drvdata(serio);\n\tif (sunkbd->reset <= -1) {\n\t\t/*\n\t\t * If cp[i] is 0xff, sunkbd->reset will stay -1.\n\t\t * The keyboard sends 0xff 0xff 0xID on powerup.\n\t\t */\n\t\tsunkbd->reset = data;\n\t\twake_up_interruptible(&sunkbd->wait);\n\t\tgoto out;\n\t}\n\tif (sunkbd->layout == -1) {\n\t\tsunkbd->layout = data;\n\t\twake_up_interruptible(&sunkbd->wait);\n\t\tgoto out;\n\t}\n\tswitch (data) {\n\tcase SUNKBD_RET_RESET:\n\t\tif (sunkbd->enabled)\n\t\t\tschedule_work(&sunkbd->tq);\n\t\tsunkbd->reset = -1;\n\t\tbreak;\n\tcase SUNKBD_RET_LAYOUT:\n\t\tsunkbd->layout = -1;\n\t\tbreak;\n\tcase SUNKBD_RET_ALLUP: /* All keys released */\n\t\tbreak;\n\tdefault:\n\t\tif (!sunkbd->enabled)\n\t\t\tbreak;\n\t\tif (sunkbd->keycode[data & SUNKBD_KEY]) {\n\t\t\tinput_report_key(sunkbd->dev,\n\t\t\t\t\t sunkbd->keycode[data & SUNKBD_KEY],\n\t\t\t\t\t !(data & SUNKBD_RELEASE));\n\t\t\tinput_sync(sunkbd->dev);\n\t\t} else {\n\t\t\tprintk(KERN_WARNING\n\t\t\t\t\"sunkbd.c: Unknown key (scancode %#x) %s.\\n\",\n\t\t\t\tdata & SUNKBD_KEY,\n\t\t\t\tdata & SUNKBD_RELEASE ? \"released\" : \"pressed\");\n\t\t}\n\t}\nout:\n\treturn IRQ_HANDLED;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 133
    },
    {
        "code": "void usb_sg_cancel(struct usb_sg_request *io)\n{\n\tunsigned long flags;\n\tint i, retval;\n\tspin_lock_irqsave(&io->lock, flags);\n\tif (io->status || io->count == 0) {\n\t\tspin_unlock_irqrestore(&io->lock, flags);\n\t\treturn;\n\t}\n\t/* shut everything down */\n\tio->status = -ECONNRESET;\n\tio->count++;\t\t/* Keep the request alive until we're done */\n\tspin_unlock_irqrestore(&io->lock, flags);\n\tfor (i = io->entries - 1; i >= 0; --i) {\n\t\tusb_block_urb(io->urbs[i]);\n\t\tretval = usb_unlink_urb(io->urbs[i]);\n\t\tif (retval != -EINPROGRESS\n\t\t    && retval != -ENODEV\n\t\t    && retval != -EBUSY\n\t\t    && retval != -EIDRM)\n\t\t\tdev_warn(&io->dev->dev, \"%s, unlink --> %d\\n\",\n\t\t\t\t __func__, retval);\n\t}\n\tspin_lock_irqsave(&io->lock, flags);\n\tio->count--;\n\tif (!io->count)\n\t\tcomplete(&io->complete);\n\tspin_unlock_irqrestore(&io->lock, flags);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 135
    },
    {
        "code": "static int xillyusb_open(struct inode *inode, struct file *filp)\n{\n\tstruct xillyusb_dev *xdev;\n\tstruct xillyusb_channel *chan;\n\tstruct xillyfifo *in_fifo = NULL;\n\tstruct xillyusb_endpoint *out_ep = NULL;\n\tint rc;\n\tint index;\n\tmutex_lock(&kref_mutex);\n\trc = xillybus_find_inode(inode, (void **)&xdev, &index);\n\tif (rc) {\n\t\tmutex_unlock(&kref_mutex);\n\t\treturn rc;\n\t}\n\tkref_get(&xdev->kref);\n\tmutex_unlock(&kref_mutex);\n\tchan = &xdev->channels[index];\n\tfilp->private_data = chan;\n\tmutex_lock(&chan->lock);\n\trc = -ENODEV;\n\tif (xdev->error)\n\t\tgoto unmutex_fail;\n\tif (((filp->f_mode & FMODE_READ) && !chan->readable) ||\n\t    ((filp->f_mode & FMODE_WRITE) && !chan->writable))\n\t\tgoto unmutex_fail;\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_READ) &&\n\t    chan->in_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for read on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_WRITE) &&\n\t    chan->out_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for write on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\trc = -EBUSY;\n\tif (((filp->f_mode & FMODE_READ) && chan->open_for_read) ||\n\t    ((filp->f_mode & FMODE_WRITE) && chan->open_for_write))\n\t\tgoto unmutex_fail;\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 1;\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 1;\n\tmutex_unlock(&chan->lock);\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tout_ep = endpoint_alloc(xdev,\n\t\t\t\t\t(chan->chan_idx + 2) | USB_DIR_OUT,\n\t\t\t\t\tbulk_out_work, BUF_SIZE_ORDER, BUFNUM);\n\t\tif (!out_ep) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto unopen;\n\t\t}\n\t\trc = fifo_init(&out_ep->fifo, chan->out_log2_fifo_size);\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\t\tout_ep->fill_mask = -(1 << chan->out_log2_element_size);\n\t\tchan->out_bytes = 0;\n\t\tchan->flushed = 0;\n\t\t/*\n\t\t * Sending a flush request to a previously closed stream\n\t\t * effectively opens it, and also waits until the command is\n\t\t * confirmed by the FPGA. The latter is necessary because the\n\t\t * data is sent through a separate BULK OUT endpoint, and the\n\t\t * xHCI controller is free to reorder transmissions.\n\t\t *\n\t\t * This can't go wrong unless there's a serious hardware error\n\t\t * (or the computer is stuck for 500 ms?)\n\t\t */\n\t\trc = flush_downstream(chan, XILLY_RESPONSE_TIMEOUT, false);\n\t\tif (rc == -ETIMEDOUT) {\n\t\t\trc = -EIO;\n\t\t\treport_io_error(xdev, rc);\n\t\t}\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\t}\n\tif (filp->f_mode & FMODE_READ) {\n\t\tin_fifo = kzalloc(sizeof(*in_fifo), GFP_KERNEL);\n\t\tif (!in_fifo) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto late_unopen;\n\t\t}\n\t\trc = fifo_init(in_fifo, chan->in_log2_fifo_size);\n\t\tif (rc) {\n\t\t\tkfree(in_fifo);\n\t\t\tgoto late_unopen;\n\t\t}\n\t}\n\tmutex_lock(&chan->lock);\n\tif (in_fifo) {",
        "bug_line_number": [],
        "vul": 0,
        "id": 137
    },
    {
        "code": "\t\tchan->in_fifo = in_fifo;\n\t\tchan->read_data_ok = 1;\n\t}\n\tif (out_ep)\n\t\tchan->out_ep = out_ep;\n\tmutex_unlock(&chan->lock);\n\tif (in_fifo) {\n\t\tu32 in_checkpoint = 0;\n\t\tif (!chan->in_synchronous)\n\t\t\tin_checkpoint = in_fifo->size >>\n\t\t\t\tchan->in_log2_element_size;\n\t\tchan->in_consumed_bytes = 0;\n\t\tchan->poll_used = 0;\n\t\tchan->in_current_checkpoint = in_checkpoint;\n\t\trc = xillyusb_send_opcode(xdev, (chan->chan_idx << 1) | 1,\n\t\t\t\t\t  OPCODE_SET_CHECKPOINT,\n\t\t\t\t\t  in_checkpoint);\n\t\tif (rc) /* Failure guarantees that opcode wasn't sent */\n\t\t\tgoto unfifo;\n\t\t/*\n\t\t * In non-blocking mode, request the FPGA to send any data it\n\t\t * has right away. Otherwise, the first read() will always\n\t\t * return -EAGAIN, which is OK strictly speaking, but ugly.\n\t\t * Checking and unrolling if this fails isn't worth the\n\t\t * effort -- the error is propagated to the first read()\n\t\t * anyhow.\n\t\t */\n\t\tif (filp->f_flags & O_NONBLOCK)\n\t\t\trequest_read_anything(chan, OPCODE_SET_PUSH);\n\t}\n\treturn 0;\nunfifo:\n\tchan->read_data_ok = 0;\n\tsafely_assign_in_fifo(chan, NULL);\n\tfifo_mem_release(in_fifo);\n\tkfree(in_fifo);\n\tif (out_ep) {\n\t\tmutex_lock(&chan->lock);\n\t\tchan->out_ep = NULL;\n\t\tmutex_unlock(&chan->lock);\n\t}\nlate_unopen:\n\tif (out_ep)\n\t\tendpoint_dealloc(out_ep);\nunopen:\n\tmutex_lock(&chan->lock);\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 0;\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 0;\n\tmutex_unlock(&chan->lock);\n\tkref_put(&xdev->kref, cleanup_dev);\n\treturn rc;\nunmutex_fail:\n\tkref_put(&xdev->kref, cleanup_dev);\n\tmutex_unlock(&chan->lock);\n\treturn rc;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 137
    },
    {
        "code": "void ext4_es_insert_delayed_block(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t\t  bool allocated)\n{\n\tstruct extent_status newes;\n\tint err1 = 0;\n\tint err2 = 0;\n\tstruct extent_status *es1 = NULL;\n\tstruct extent_status *es2 = NULL;\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\tes_debug(\"add [%u/1) delayed to extent status tree of inode %lu\\n\",\n\t\t lblk, inode->i_ino);\n\tnewes.es_lblk = lblk;\n\tnewes.es_len = 1;\n\text4_es_store_pblock_status(&newes, ~0, EXTENT_STATUS_DELAYED);\n\ttrace_ext4_es_insert_delayed_block(inode, &newes, allocated);\n\text4_es_insert_extent_check(inode, &newes);\nretry:\n\tif (err1 && !es1)\n\t\tes1 = __es_alloc_extent(true);\n\tif ((err1 || err2) && !es2)\n\t\tes2 = __es_alloc_extent(true);\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr1 = __es_remove_extent(inode, lblk, lblk, NULL, es1);\n\tif (err1 != 0)\n\t\tgoto error;\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es1) {\n\t\tif (!es1->es_len)\n\t\t\t__es_free_extent(es1);\n\t\tes1 = NULL;\n\t}\n\terr2 = __es_insert_extent(inode, &newes, es2);\n\tif (err2 != 0)\n\t\tgoto error;\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es2) {\n\t\tif (!es2->es_len)\n\t\t\t__es_free_extent(es2);\n\t\tes2 = NULL;\n\t}\n\tif (allocated)\n\t\t__insert_pending(inode, lblk);\nerror:\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err1 || err2)\n\t\tgoto retry;\n\text4_es_print_tree(inode);\n\text4_print_pending_tree(inode);\n\treturn;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 139
    },
    {
        "code": "void ext4_es_insert_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len, ext4_fsblk_t pblk,\n\t\t\t   unsigned int status)\n{\n\tstruct extent_status newes;\n\text4_lblk_t end = lblk + len - 1;\n\tint err1 = 0;\n\tint err2 = 0;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct extent_status *es1 = NULL;\n\tstruct extent_status *es2 = NULL;\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\tes_debug(\"add [%u/%u) %llu %x to extent status tree of inode %lu\\n\",\n\t\t lblk, len, pblk, status, inode->i_ino);\n\tif (!len)\n\t\treturn;\n\tBUG_ON(end < lblk);\n\tif ((status & EXTENT_STATUS_DELAYED) &&\n\t    (status & EXTENT_STATUS_WRITTEN)) {\n\t\text4_warning(inode->i_sb, \"Inserting extent [%u/%u] as \"\n\t\t\t\t\" delayed and written which can potentially \"\n\t\t\t\t\" cause data loss.\", lblk, len);\n\t\tWARN_ON(1);\n\t}\n\tnewes.es_lblk = lblk;\n\tnewes.es_len = len;\n\text4_es_store_pblock_status(&newes, pblk, status);\n\ttrace_ext4_es_insert_extent(inode, &newes);\n\text4_es_insert_extent_check(inode, &newes);\nretry:\n\tif (err1 && !es1)\n\t\tes1 = __es_alloc_extent(true);\n\tif ((err1 || err2) && !es2)\n\t\tes2 = __es_alloc_extent(true);\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr1 = __es_remove_extent(inode, lblk, end, NULL, es1);\n\tif (err1 != 0)\n\t\tgoto error;\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es1) {\n\t\tif (!es1->es_len)\n\t\t\t__es_free_extent(es1);\n\t\tes1 = NULL;\n\t}\n\terr2 = __es_insert_extent(inode, &newes, es2);\n\tif (err2 == -ENOMEM && !ext4_es_must_keep(&newes))\n\t\terr2 = 0;\n\tif (err2 != 0)\n\t\tgoto error;\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es2) {\n\t\tif (!es2->es_len)\n\t\t\t__es_free_extent(es2);\n\t\tes2 = NULL;\n\t}\n\tif (sbi->s_cluster_ratio > 1 && test_opt(inode->i_sb, DELALLOC) &&\n\t    (status & EXTENT_STATUS_WRITTEN ||\n\t     status & EXTENT_STATUS_UNWRITTEN))\n\t\t__revise_pending(inode, lblk, len);\nerror:\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err1 || err2)\n\t\tgoto retry;\n\text4_es_print_tree(inode);\n\treturn;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 141
    },
    {
        "code": "static int em_fxrstor(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct fxregs_state fx_state;\n\tint rc;\n\trc = check_fxsr(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = segmented_read_std(ctxt, ctxt->memop.addr.mem, &fx_state, 512);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\tif (fx_state.mxcsr >> 16)\n\t\treturn emulate_gp(ctxt, 0);\n\tctxt->ops->get_fpu(ctxt);\n\tif (ctxt->mode < X86EMUL_MODE_PROT64)\n\t\trc = fxrstor_fixup(ctxt, &fx_state);\n\tif (rc == X86EMUL_CONTINUE)\n\t\trc = asm_safe(\"fxrstor %[fx]\", : [fx] \"m\"(fx_state));\n\tctxt->ops->put_fpu(ctxt);\n\treturn rc;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 143
    },
    {
        "code": "int ttm_sg_tt_init(struct ttm_dma_tt *ttm_dma, struct ttm_buffer_object *bo,\n\t\t   uint32_t page_flags)\n{\n\tstruct ttm_tt *ttm = &ttm_dma->ttm;\n\tint ret;\n\tttm_tt_init_fields(ttm, bo, page_flags);\n\tINIT_LIST_HEAD(&ttm_dma->pages_list);\n\tif (page_flags & TTM_PAGE_FLAG_SG)\n\t\tret = ttm_sg_tt_alloc_page_directory(ttm_dma);\n\telse\n\t\tret = ttm_dma_tt_alloc_page_directory(ttm_dma);\n\tif (ret) {\n\t\tpr_err(\"Failed allocating page table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 145
    },
    {
        "code": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 147
    },
    {
        "code": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\tunsigned long flags;\n\tif (WARN_ON(!info))\n\t\treturn;\n\twrite_lock_irqsave(&evtchn_rwlock, flags);\n\tlist_del(&info->list);\n\tset_info_for_irq(irq, NULL);\n\tWARN_ON(info->refcnt > 0);\n\twrite_unlock_irqrestore(&evtchn_rwlock, flags);\n\tkfree(info);\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\tirq_free_desc(irq);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 149
    },
    {
        "code": "static void\nbinder_free_buf(struct binder_proc *proc, struct binder_buffer *buffer)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, buffer, 0, false);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 151
    },
    {
        "code": "static ssize_t ucma_migrate_id(struct ucma_file *new_file,\n\t\t\t       const char __user *inbuf,\n\t\t\t       int in_len, int out_len)\n{\n\tstruct rdma_ucm_migrate_id cmd;\n\tstruct rdma_ucm_migrate_resp resp;\n\tstruct ucma_event *uevent, *tmp;\n\tstruct ucma_context *ctx;\n\tLIST_HEAD(event_list);\n\tstruct fd f;\n\tstruct ucma_file *cur_file;\n\tint ret = 0;\n\tif (copy_from_user(&cmd, inbuf, sizeof(cmd)))\n\t\treturn -EFAULT;\n\t/* Get current fd to protect against it being closed */\n\tf = fdget(cmd.fd);\n\tif (!f.file)\n\t\treturn -ENOENT;\n\tif (f.file->f_op != &ucma_fops) {\n\t\tret = -EINVAL;\n\t\tgoto file_put;\n\t}\n\tcur_file = f.file->private_data;\n\t/* Validate current fd and prevent destruction of id. */\n\tctx = ucma_get_ctx(cur_file, cmd.id);\n\tif (IS_ERR(ctx)) {\n\t\tret = PTR_ERR(ctx);\n\t\tgoto file_put;\n\t}\n\trdma_lock_handler(ctx->cm_id);\n\t/*\n\t * ctx->file can only be changed under the handler & xa_lock. xa_load()\n\t * must be checked again to ensure the ctx hasn't begun destruction\n\t * since the ucma_get_ctx().\n\t */\n\txa_lock(&ctx_table);\n\tif (_ucma_find_context(cmd.id, cur_file) != ctx) {\n\t\txa_unlock(&ctx_table);\n\t\tret = -ENOENT;\n\t\tgoto err_unlock;\n\t}\n\tctx->file = new_file;\n\txa_unlock(&ctx_table);\n\tmutex_lock(&cur_file->mut);\n\tlist_del(&ctx->list);\n\t/*\n\t * At this point lock_handler() prevents addition of new uevents for\n\t * this ctx.\n\t */\n\tlist_for_each_entry_safe(uevent, tmp, &cur_file->event_list, list)\n\t\tif (uevent->ctx == ctx)\n\t\t\tlist_move_tail(&uevent->list, &event_list);\n\tresp.events_reported = ctx->events_reported;\n\tmutex_unlock(&cur_file->mut);\n\tmutex_lock(&new_file->mut);\n\tlist_add_tail(&ctx->list, &new_file->ctx_list);\n\tlist_splice_tail(&event_list, &new_file->event_list);\n\tmutex_unlock(&new_file->mut);\n\tif (copy_to_user(u64_to_user_ptr(cmd.response),\n\t\t\t &resp, sizeof(resp)))\n\t\tret = -EFAULT;\nerr_unlock:\n\trdma_unlock_handler(ctx->cm_id);\n\tucma_put_ctx(ctx);\nfile_put:\n\tfdput(f);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 153
    },
    {
        "code": "static void ml_ff_destroy(struct ff_device *ff)\n{\n\tstruct ml_device *ml = ff->private;\n\t/*\n\t * Even though we stop all playing effects when tearing down\n\t * an input device (via input_device_flush() that calls into\n\t * input_ff_flush() that stops and erases all effects), we\n\t * do not actually stop the timer, and therefore we should\n\t * do it here.\n\t */\n\tdel_timer_sync(&ml->timer);\n\tkfree(ml->private);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 155
    },
    {
        "code": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\tmemset(&req->work, 0, sizeof(req->work));\n\treq->flags |= REQ_F_WORK_INITIALIZED;\n\tio_init_identity(&req->identity);\n\treq->work.identity = &req->identity;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 157
    },
    {
        "code": "static int tcp_v6_send_synack(const struct sock *sk, struct dst_entry *dst,\n\t\t\t      struct flowi *fl,\n\t\t\t      struct request_sock *req,\n\t\t\t      struct tcp_fastopen_cookie *foc,\n\t\t\t      bool attach_req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 *fl6 = &fl->u.ip6;\n\tstruct sk_buff *skb;\n\tint err = -ENOMEM;\n\t/* First, grab a route. */\n\tif (!dst && (dst = inet6_csk_route_req(sk, fl6, req,\n\t\t\t\t\t       IPPROTO_TCP)) == NULL)\n\t\tgoto done;\n\tskb = tcp_make_synack(sk, dst, req, foc, attach_req);\n\tif (skb) {\n\t\t__tcp_v6_send_check(skb, &ireq->ir_v6_loc_addr,\n\t\t\t\t    &ireq->ir_v6_rmt_addr);\n\t\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\t\tif (np->repflow && ireq->pktopts)\n\t\t\tfl6->flowlabel = ip6_flowlabel(ipv6_hdr(ireq->pktopts));\n\t\terr = ip6_xmit(sk, skb, fl6, rcu_dereference(np->opt),\n\t\t\t       np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\ndone:\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 159
    },
    {
        "code": "static bool io_match_task(struct io_kiocb *head, struct task_struct *task,\n\t\t\t  bool cancel_all)\n\t__must_hold(&req->ctx->timeout_lock)\n{\n\tstruct io_kiocb *req;\n\tif (task && head->task != task)\n\t\treturn false;\n\tif (cancel_all)\n\t\treturn true;\n\tio_for_each_link(req, head) {\n\t\tif (req->flags & REQ_F_INFLIGHT)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 161
    },
    {
        "code": "static s64 tctx_inflight(struct io_uring_task *tctx, bool tracked)\n{\n\tif (tracked)\n\t\treturn atomic_read(&tctx->inflight_tracked);\n\treturn percpu_counter_sum(&tctx->inflight);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 163
    },
    {
        "code": "static struct file *io_file_get_normal(struct io_kiocb *req, int fd)\n{\n\tstruct file *file = fget(fd);\n\ttrace_io_uring_file_get(req->ctx, req, req->cqe.user_data, fd);\n\t/* we don't allow fixed io_uring files */\n\tif (file && file->f_op == &io_uring_fops)\n\t\tio_req_track_inflight(req);\n\treturn file;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 165
    },
    {
        "code": "static bool io_match_task_safe(struct io_kiocb *head, struct task_struct *task,\n\t\t\t       bool cancel_all)\n{\n\tbool matched;\n\tif (task && head->task != task)\n\t\treturn false;\n\tif (cancel_all)\n\t\treturn true;\n\tif (head->flags & REQ_F_LINK_TIMEOUT) {\n\t\tstruct io_ring_ctx *ctx = head->ctx;\n\t\t/* protect against races with linked timeouts */\n\t\tspin_lock_irq(&ctx->timeout_lock);\n\t\tmatched = io_match_linked(head);\n\t\tspin_unlock_irq(&ctx->timeout_lock);\n\t} else {\n\t\tmatched = io_match_linked(head);\n\t}\n\treturn matched;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 167
    },
    {
        "code": "static void __io_req_task_work_add(struct io_kiocb *req,\n\t\t\t\t   struct io_uring_task *tctx,\n\t\t\t\t   struct io_wq_work_list *list)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_wq_work_node *node;\n\tunsigned long flags;\n\tbool running;\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\twq_list_add_tail(&req->io_task_work.node, list);\n\trunning = tctx->task_running;\n\tif (!running)\n\t\ttctx->task_running = true;\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\t/* task_work already pending, we're done */\n\tif (running)\n\t\treturn;\n\tif (ctx->flags & IORING_SETUP_TASKRUN_FLAG)\n\t\tatomic_or(IORING_SQ_TASKRUN, &ctx->rings->sq_flags);\n\tif (likely(!task_work_add(req->task, &tctx->task_work, ctx->notify_method)))\n\t\treturn;\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\ttctx->task_running = false;\n\tnode = wq_list_merge(&tctx->prio_task_list, &tctx->task_list);\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\twhile (node) {\n\t\treq = container_of(node, struct io_kiocb, io_task_work.node);\n\t\tnode = node->next;\n\t\tif (llist_add(&req->io_task_work.fallback_node,\n\t\t\t      &req->ctx->fallback_llist))\n\t\t\tschedule_delayed_work(&req->ctx->fallback_work, 1);\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 169
    },
    {
        "code": "static int binder_thread_release(struct binder_proc *proc,\n\t\t\t\t struct binder_thread *thread)\n{\n\tstruct binder_transaction *t;\n\tstruct binder_transaction *send_reply = NULL;\n\tint active_transactions = 0;\n\tstruct binder_transaction *last_t = NULL;\n\tbinder_inner_proc_lock(thread->proc);\n\t/*\n\t * take a ref on the proc so it survives\n\t * after we remove this thread from proc->threads.\n\t * The corresponding dec is when we actually\n\t * free the thread in binder_free_thread()\n\t */\n\tproc->tmp_ref++;\n\t/*\n\t * take a ref on this thread to ensure it\n\t * survives while we are releasing it\n\t */\n\tatomic_inc(&thread->tmp_ref);\n\trb_erase(&thread->rb_node, &proc->threads);\n\tt = thread->transaction_stack;\n\tif (t) {\n\t\tspin_lock(&t->lock);\n\t\tif (t->to_thread == thread)\n\t\t\tsend_reply = t;\n\t}\n\tthread->is_dead = true;\n\twhile (t) {\n\t\tlast_t = t;\n\t\tactive_transactions++;\n\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t     \"release %d:%d transaction %d %s, still active\\n\",\n\t\t\t      proc->pid, thread->pid,\n\t\t\t     t->debug_id,\n\t\t\t     (t->to_thread == thread) ? \"in\" : \"out\");\n\t\tif (t->to_thread == thread) {\n\t\t\tt->to_proc = NULL;\n\t\t\tt->to_thread = NULL;\n\t\t\tif (t->buffer) {\n\t\t\t\tt->buffer->transaction = NULL;\n\t\t\t\tt->buffer = NULL;\n\t\t\t}\n\t\t\tt = t->to_parent;\n\t\t} else if (t->from == thread) {\n\t\t\tt->from = NULL;\n\t\t\tt = t->from_parent;\n\t\t} else\n\t\t\tBUG();\n\t\tspin_unlock(&last_t->lock);\n\t\tif (t)\n\t\t\tspin_lock(&t->lock);\n\t}\n\t/*\n\t * If this thread used poll, make sure we remove the waitqueue\n\t * from any epoll data structures holding it with POLLFREE.\n\t * waitqueue_active() is safe to use here because we're holding\n\t * the inner lock.\n\t */\n\tif ((thread->looper & BINDER_LOOPER_STATE_POLL) &&\n\t    waitqueue_active(&thread->wait)) {\n\t\twake_up_poll(&thread->wait, EPOLLHUP | POLLFREE);\n\t}\n\tbinder_inner_proc_unlock(thread->proc);\n\t/*\n\t * This is needed to avoid races between wake_up_poll() above and\n\t * and ep_remove_waitqueue() called for other reasons (eg the epoll file\n\t * descriptor being closed); ep_remove_waitqueue() holds an RCU read\n\t * lock, so we can be sure it's done after calling synchronize_rcu().\n\t */\n\tif (thread->looper & BINDER_LOOPER_STATE_POLL)\n\t\tsynchronize_rcu();\n\tif (send_reply)\n\t\tbinder_send_failed_reply(send_reply, BR_DEAD_REPLY);\n\tbinder_release_work(proc, &thread->todo);\n\tbinder_thread_dec_tmpref(thread);\n\treturn active_transactions;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 171
    },
    {
        "code": "static void smp_task_done(struct sas_task *task)\n{\n\tdel_timer(&task->slow_task->timer);\n\tcomplete(&task->slow_task->completion);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 173
    },
    {
        "code": "int io_poll_add(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_poll *poll = io_kiocb_to_cmd(req);\n\tstruct io_poll_table ipt;\n\tint ret;\n\tipt.pt._qproc = io_poll_queue_proc;\n\t/*\n\t * If sqpoll or single issuer, there is no contention for ->uring_lock\n\t * and we'll end up holding it in tw handlers anyway.\n\t */\n\tif (!(issue_flags & IO_URING_F_UNLOCKED) &&\n\t    (req->ctx->flags & (IORING_SETUP_SQPOLL | IORING_SETUP_SINGLE_ISSUER)))\n\t\treq->flags |= REQ_F_HASH_LOCKED;\n\telse\n\t\treq->flags &= ~REQ_F_HASH_LOCKED;\n\tret = __io_arm_poll_handler(req, poll, &ipt, poll->events);\n\tif (ret) {\n\t\tio_req_set_res(req, ret, 0);\n\t\treturn IOU_OK;\n\t}\n\tif (ipt.error) {\n\t\treq_set_fail(req);\n\t\treturn ipt.error;\n\t}\n\treturn IOU_ISSUE_SKIP_COMPLETE;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 175
    },
    {
        "code": "static __cold void io_ring_ctx_free(struct io_ring_ctx *ctx)\n{\n\tio_sq_thread_finish(ctx);\n\tif (ctx->mm_account) {\n\t\tmmdrop(ctx->mm_account);\n\t\tctx->mm_account = NULL;\n\t}\n\tio_rsrc_refs_drop(ctx);\n\t/* __io_rsrc_put_work() may need uring_lock to progress, wait w/o it */\n\tio_wait_rsrc_data(ctx->buf_data);\n\tio_wait_rsrc_data(ctx->file_data);\n\tmutex_lock(&ctx->uring_lock);\n\tif (ctx->buf_data)\n\t\t__io_sqe_buffers_unregister(ctx);\n\tif (ctx->file_data)\n\t\t__io_sqe_files_unregister(ctx);\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true);\n\tio_eventfd_unregister(ctx);\n\tio_flush_apoll_cache(ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\tio_destroy_buffers(ctx);\n\tif (ctx->sq_creds)\n\t\tput_cred(ctx->sq_creds);\n\tif (ctx->submitter_task)\n\t\tput_task_struct(ctx->submitter_task);\n\t/* there are no registered resources left, nobody uses it */\n\tif (ctx->rsrc_node)\n\t\tio_rsrc_node_destroy(ctx->rsrc_node);\n\tif (ctx->rsrc_backup_node)\n\t\tio_rsrc_node_destroy(ctx->rsrc_backup_node);\n\tflush_delayed_work(&ctx->rsrc_put_work);\n\tflush_delayed_work(&ctx->fallback_work);\n\tWARN_ON_ONCE(!list_empty(&ctx->rsrc_ref_list));\n\tWARN_ON_ONCE(!llist_empty(&ctx->rsrc_put_llist));\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tctx->ring_sock->file = NULL; /* so that iput() is called */\n\t\tsock_release(ctx->ring_sock);\n\t}\n#endif\n\tWARN_ON_ONCE(!list_empty(&ctx->ltimeout_list));\n\tio_mem_free(ctx->rings);\n\tio_mem_free(ctx->sq_sqes);\n\tpercpu_ref_exit(&ctx->refs);\n\tfree_uid(ctx->user);\n\tio_req_caches_free(ctx);\n\tif (ctx->hash_map)\n\t\tio_wq_put_hash(ctx->hash_map);\n\tkfree(ctx->cancel_table.hbs);\n\tkfree(ctx->cancel_table_locked.hbs);\n\tkfree(ctx->dummy_ubuf);\n\tkfree(ctx->io_bl);\n\txa_destroy(&ctx->io_bl_xa);\n\tkfree(ctx);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 177
    },
    {
        "code": "static int __io_arm_poll_handler(struct io_kiocb *req,\n\t\t\t\t struct io_poll *poll,\n\t\t\t\t struct io_poll_table *ipt, __poll_t mask)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint v;\n\tINIT_HLIST_NODE(&req->hash_node);\n\treq->work.cancel_seq = atomic_read(&ctx->cancel_seq);\n\tio_init_poll_iocb(poll, mask, io_poll_wake);\n\tpoll->file = req->file;\n\treq->apoll_events = poll->events;\n\tipt->pt._key = mask;\n\tipt->req = req;\n\tipt->error = 0;\n\tipt->nr_entries = 0;\n\t/*\n\t * Take the ownership to delay any tw execution up until we're done\n\t * with poll arming. see io_poll_get_ownership().\n\t */\n\tatomic_set(&req->poll_refs, 1);\n\tmask = vfs_poll(req->file, &ipt->pt) & poll->events;\n\tif (mask &&\n\t   ((poll->events & (EPOLLET|EPOLLONESHOT)) == (EPOLLET|EPOLLONESHOT))) {\n\t\tio_poll_remove_entries(req);\n\t\t/* no one else has access to the req, forget about the ref */\n\t\treturn mask;\n\t}\n\tif (!mask && unlikely(ipt->error || !ipt->nr_entries)) {\n\t\tio_poll_remove_entries(req);\n\t\tif (!ipt->error)\n\t\t\tipt->error = -EINVAL;\n\t\treturn 0;\n\t}\n\tif (req->flags & REQ_F_HASH_LOCKED)\n\t\tio_poll_req_insert_locked(req);\n\telse\n\t\tio_poll_req_insert(req);\n\tif (mask && (poll->events & EPOLLET)) {\n\t\t/* can't multishot if failed, just queue the event we've got */\n\t\tif (unlikely(ipt->error || !ipt->nr_entries)) {\n\t\t\tpoll->events |= EPOLLONESHOT;\n\t\t\treq->apoll_events |= EPOLLONESHOT;\n\t\t\tipt->error = 0;\n\t\t}\n\t\t__io_poll_execute(req, mask, poll->events);\n\t\treturn 0;\n\t}\n\t/*\n\t * Release ownership. If someone tried to queue a tw while it was\n\t * locked, kick it off for them.\n\t */\n\tv = atomic_dec_return(&req->poll_refs);\n\tif (unlikely(v & IO_POLL_REF_MASK))\n\t\t__io_poll_execute(req, 0, poll->events);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 179
    },
    {
        "code": "static int log_read_rst(struct ntfs_log *log, u32 l_size, bool first,\n\t\t\tstruct restart_info *info)\n{\n\tu32 skip, vbo;\n\tstruct RESTART_HDR *r_page = kmalloc(DefaultLogPageSize, GFP_NOFS);\n\tif (!r_page)\n\t\treturn -ENOMEM;\n\t/* Determine which restart area we are looking for. */\n\tif (first) {\n\t\tvbo = 0;\n\t\tskip = 512;\n\t} else {\n\t\tvbo = 512;\n\t\tskip = 0;\n\t}\n\t/* Loop continuously until we succeed. */\n\tfor (; vbo < l_size; vbo = 2 * vbo + skip, skip = 0) {\n\t\tbool usa_error;\n\t\tu32 sys_page_size;\n\t\tbool brst, bchk;\n\t\tstruct RESTART_AREA *ra;\n\t\t/* Read a page header at the current offset. */\n\t\tif (read_log_page(log, vbo, (struct RECORD_PAGE_HDR **)&r_page,\n\t\t\t\t  &usa_error)) {\n\t\t\t/* Ignore any errors. */\n\t\t\tcontinue;\n\t\t}\n\t\t/* Exit if the signature is a log record page. */\n\t\tif (r_page->rhdr.sign == NTFS_RCRD_SIGNATURE) {\n\t\t\tinfo->initialized = true;\n\t\t\tbreak;\n\t\t}\n\t\tbrst = r_page->rhdr.sign == NTFS_RSTR_SIGNATURE;\n\t\tbchk = r_page->rhdr.sign == NTFS_CHKD_SIGNATURE;\n\t\tif (!bchk && !brst) {\n\t\t\tif (r_page->rhdr.sign != NTFS_FFFF_SIGNATURE) {\n\t\t\t\t/*\n\t\t\t\t * Remember if the signature does not\n\t\t\t\t * indicate uninitialized file.\n\t\t\t\t */\n\t\t\t\tinfo->initialized = true;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tra = NULL;\n\t\tinfo->valid_page = false;\n\t\tinfo->initialized = true;\n\t\tinfo->vbo = vbo;\n\t\t/* Let's check the restart area if this is a valid page. */\n\t\tif (!is_rst_page_hdr_valid(vbo, r_page))\n\t\t\tgoto check_result;\n\t\tra = Add2Ptr(r_page, le16_to_cpu(r_page->ra_off));\n\t\tif (!is_rst_area_valid(r_page))\n\t\t\tgoto check_result;\n\t\t/*\n\t\t * We have a valid restart page header and restart area.\n\t\t * If chkdsk was run or we have no clients then we have\n\t\t * no more checking to do.\n\t\t */\n\t\tif (bchk || ra->client_idx[1] == LFS_NO_CLIENT_LE) {\n\t\t\tinfo->valid_page = true;\n\t\t\tgoto check_result;\n\t\t}\n\t\t/* Read the entire restart area. */\n\t\tsys_page_size = le32_to_cpu(r_page->sys_page_size);\n\t\tif (DefaultLogPageSize != sys_page_size) {\n\t\t\tkfree(r_page);\n\t\t\tr_page = kzalloc(sys_page_size, GFP_NOFS);\n\t\t\tif (!r_page)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (read_log_page(log, vbo,\n\t\t\t\t\t  (struct RECORD_PAGE_HDR **)&r_page,\n\t\t\t\t\t  &usa_error)) {\n\t\t\t\t/* Ignore any errors. */\n\t\t\t\tkfree(r_page);\n\t\t\t\tr_page = NULL;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (is_client_area_valid(r_page, usa_error)) {\n\t\t\tinfo->valid_page = true;\n\t\t\tra = Add2Ptr(r_page, le16_to_cpu(r_page->ra_off));\n\t\t}\ncheck_result:\n\t\t/*\n\t\t * If chkdsk was run then update the caller's\n\t\t * values and return.\n\t\t */\n\t\tif (r_page->rhdr.sign == NTFS_CHKD_SIGNATURE) {\n\t\t\tinfo->chkdsk_was_run = true;\n\t\t\tinfo->last_lsn = le64_to_cpu(r_page->rhdr.lsn);\n\t\t\tinfo->restart = true;\n\t\t\tinfo->r_page = r_page;\n\t\t\treturn 0;\n\t\t}\n\t\t/*\n\t\t * If we have a valid page then copy the values\n\t\t * we need from it.\n\t\t */\n\t\tif (info->valid_page) {\n\t\t\tinfo->last_lsn = le64_to_cpu(ra->current_lsn);\n\t\t\tinfo->restart = true;\n\t\t\tinfo->r_page = r_page;\n\t\t\treturn 0;",
        "bug_line_number": [],
        "vul": 0,
        "id": 181
    },
    {
        "code": "static void redo_fd_request(void)\n{\n\tint drive;\n\tint tmp;\n\tlastredo = jiffies;\n\tif (current_drive < N_DRIVE)\n\t\tfloppy_off(current_drive);\ndo_request:\n\tif (!current_req) {\n\t\tint pending;\n\t\tspin_lock_irq(&floppy_lock);\n\t\tpending = set_next_request();\n\t\tspin_unlock_irq(&floppy_lock);\n\t\tif (!pending) {\n\t\t\tdo_floppy = NULL;\n\t\t\tunlock_fdc();\n\t\t\treturn;\n\t\t}\n\t}\n\tdrive = (long)current_req->q->disk->private_data;\n\tset_fdc(drive);\n\treschedule_timeout(current_drive, \"redo fd request\");\n\tset_floppy(drive);\n\traw_cmd = &default_raw_cmd;\n\traw_cmd->flags = 0;\n\tif (start_motor(redo_fd_request))\n\t\treturn;\n\tdisk_change(current_drive);\n\tif (test_bit(current_drive, &fake_change) ||\n\t    test_bit(FD_DISK_CHANGED_BIT, &drive_state[current_drive].flags)) {\n\t\tDPRINT(\"disk absent or changed during operation\\n\");\n\t\trequest_done(0);\n\t\tgoto do_request;\n\t}\n\tif (!_floppy) {\t/* Autodetection */\n\t\tif (!probing) {\n\t\t\tdrive_state[current_drive].probed_format = 0;\n\t\t\tif (next_valid_format(current_drive)) {\n\t\t\t\tDPRINT(\"no autodetectable formats\\n\");\n\t\t\t\t_floppy = NULL;\n\t\t\t\trequest_done(0);\n\t\t\t\tgoto do_request;\n\t\t\t}\n\t\t}\n\t\tprobing = 1;\n\t\t_floppy = floppy_type + drive_params[current_drive].autodetect[drive_state[current_drive].probed_format];\n\t} else\n\t\tprobing = 0;\n\ttmp = make_raw_rw_request();\n\tif (tmp < 2) {\n\t\trequest_done(tmp);\n\t\tgoto do_request;\n\t}\n\tif (test_bit(FD_NEED_TWADDLE_BIT, &drive_state[current_drive].flags))\n\t\ttwaddle(current_fdc, current_drive);\n\tschedule_bh(floppy_start);\n\tdebugt(__func__, \"queue fd request\");\n\treturn;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 183
    },
    {
        "code": "static loff_t find_tree_dqentry(struct qtree_mem_dqinfo *info,\n\t\t\t\tstruct dquot *dquot, uint blk, int depth)\n{\n\tchar *buf = kmalloc(info->dqi_usable_bs, GFP_NOFS);\n\tloff_t ret = 0;\n\t__le32 *ref = (__le32 *)buf;\n\tif (!buf)\n\t\treturn -ENOMEM;\n\tret = read_blk(info, blk, buf);\n\tif (ret < 0) {\n\t\tquota_error(dquot->dq_sb, \"Can't read quota tree block %u\",\n\t\t\t    blk);\n\t\tgoto out_buf;\n\t}\n\tret = 0;\n\tblk = le32_to_cpu(ref[get_index(info, dquot->dq_id, depth)]);\n\tif (!blk)\t/* No reference? */\n\t\tgoto out_buf;\n\tif (blk < QT_TREEOFF || blk >= info->dqi_blocks) {\n\t\tquota_error(dquot->dq_sb, \"Getting block too big (%u >= %u)\",\n\t\t\t    blk, info->dqi_blocks);\n\t\tret = -EUCLEAN;\n\t\tgoto out_buf;\n\t}\n\tif (depth < info->dqi_qtree_depth - 1)\n\t\tret = find_tree_dqentry(info, dquot, blk, depth+1);\n\telse\n\t\tret = find_block_dqentry(info, dquot, blk);\nout_buf:\n\tkfree(buf);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 185
    },
    {
        "code": "struct psi_trigger *psi_trigger_create(struct psi_group *group,\n\t\t\tchar *buf, size_t nbytes, enum psi_res res)\n{\n\tstruct psi_trigger *t;\n\tenum psi_states state;\n\tu32 threshold_us;\n\tu32 window_us;\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\tif (sscanf(buf, \"some %u %u\", &threshold_us, &window_us) == 2)\n\t\tstate = PSI_IO_SOME + res * 2;\n\telse if (sscanf(buf, \"full %u %u\", &threshold_us, &window_us) == 2)\n\t\tstate = PSI_IO_FULL + res * 2;\n\telse\n\t\treturn ERR_PTR(-EINVAL);\n\tif (state >= PSI_NONIDLE)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (window_us < WINDOW_MIN_US ||\n\t\twindow_us > WINDOW_MAX_US)\n\t\treturn ERR_PTR(-EINVAL);\n\t/* Check threshold */\n\tif (threshold_us == 0 || threshold_us > window_us)\n\t\treturn ERR_PTR(-EINVAL);\n\tt = kmalloc(sizeof(*t), GFP_KERNEL);\n\tif (!t)\n\t\treturn ERR_PTR(-ENOMEM);\n\tt->group = group;\n\tt->state = state;\n\tt->threshold = threshold_us * NSEC_PER_USEC;\n\tt->win.size = window_us * NSEC_PER_USEC;\n\twindow_reset(&t->win, 0, 0, 0);\n\tt->event = 0;\n\tt->last_event_time = 0;\n\tinit_waitqueue_head(&t->event_wait);\n\tmutex_lock(&group->trigger_lock);\n\tif (!rcu_access_pointer(group->poll_task)) {\n\t\tstruct task_struct *task;\n\t\ttask = kthread_create(psi_poll_worker, group, \"psimon\");\n\t\tif (IS_ERR(task)) {\n\t\t\tkfree(t);\n\t\t\tmutex_unlock(&group->trigger_lock);\n\t\t\treturn ERR_CAST(task);\n\t\t}\n\t\tatomic_set(&group->poll_wakeup, 0);\n\t\twake_up_process(task);\n\t\trcu_assign_pointer(group->poll_task, task);\n\t}\n\tlist_add(&t->node, &group->triggers);\n\tgroup->poll_min_period = min(group->poll_min_period,\n\t\tdiv_u64(t->win.size, UPDATES_PER_WINDOW));\n\tgroup->nr_triggers[t->state]++;\n\tgroup->poll_states |= (1 << t->state);\n\tmutex_unlock(&group->trigger_lock);\n\treturn t;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 187
    },
    {
        "code": "static void timerfd_remove_cancel(struct timerfd_ctx *ctx)\n{\n\tspin_lock(&ctx->cancel_lock);\n\t__timerfd_remove_cancel(ctx);\n\tspin_unlock(&ctx->cancel_lock);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 189
    },
    {
        "code": "static int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\thwc->state = !(flags & PERF_EF_START);\n\thead = find_swevent_head(swhash, event);\n\tif (WARN_ON_ONCE(!head))\n\t\treturn -EINVAL;\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\tperf_event_update_userpage(event);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 191
    },
    {
        "code": "static int binder_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tint ret;\n\tstruct binder_proc *proc = filp->private_data;\n\tconst char *failure_string;\n\tif (proc->tsk != current->group_leader)\n\t\treturn -EINVAL;\n\tif ((vma->vm_end - vma->vm_start) > SZ_4M)\n\t\tvma->vm_end = vma->vm_start + SZ_4M;\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d %lx-%lx (%ld K) vma %lx pagep %lx\\n\",\n\t\t     __func__, proc->pid, vma->vm_start, vma->vm_end,\n\t\t     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,\n\t\t     (unsigned long)pgprot_val(vma->vm_page_prot));\n\tif (vma->vm_flags & FORBIDDEN_MMAP_FLAGS) {\n\t\tret = -EPERM;\n\t\tfailure_string = \"bad vm_flags\";\n\t\tgoto err_bad_arg;\n\t}\n\tvma->vm_flags = (vma->vm_flags | VM_DONTCOPY) & ~VM_MAYWRITE;\n\tvma->vm_ops = &binder_vm_ops;\n\tvma->vm_private_data = proc;\n\tret = binder_alloc_mmap_handler(&proc->alloc, vma);\n\tif (ret)\n\t\treturn ret;\n\tmutex_lock(&proc->files_lock);\n\tproc->files = get_files_struct(current);\n\tmutex_unlock(&proc->files_lock);\n\treturn 0;\nerr_bad_arg:\n\tpr_err(\"binder_mmap: %d %lx-%lx %s failed %d\\n\",\n\t       proc->pid, vma->vm_start, vma->vm_end, failure_string, ret);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 193
    },
    {
        "code": "static int binder_open(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc;\n\tstruct binder_device *binder_dev;\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE, \"binder_open: %d:%d\\n\",\n\t\t     current->group_leader->pid, current->pid);\n\tproc = kzalloc(sizeof(*proc), GFP_KERNEL);\n\tif (proc == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&proc->inner_lock);\n\tspin_lock_init(&proc->outer_lock);\n\tget_task_struct(current->group_leader);\n\tproc->tsk = current->group_leader;\n\tmutex_init(&proc->files_lock);\n\tINIT_LIST_HEAD(&proc->todo);\n\tproc->default_priority = task_nice(current);\n\tbinder_dev = container_of(filp->private_data, struct binder_device,\n\t\t\t\t  miscdev);\n\tproc->context = &binder_dev->context;\n\tbinder_alloc_init(&proc->alloc);\n\tbinder_stats_created(BINDER_STAT_PROC);\n\tproc->pid = current->group_leader->pid;\n\tINIT_LIST_HEAD(&proc->delivered_death);\n\tINIT_LIST_HEAD(&proc->waiting_threads);\n\tfilp->private_data = proc;\n\tmutex_lock(&binder_procs_lock);\n\thlist_add_head(&proc->proc_node, &binder_procs);\n\tmutex_unlock(&binder_procs_lock);\n\tif (binder_debugfs_dir_entry_proc) {\n\t\tchar strbuf[11];\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * proc debug entries are shared between contexts, so\n\t\t * this will fail if the process tries to open the driver\n\t\t * again with a different context. The priting code will\n\t\t * anyway print all contexts that a given PID has, so this\n\t\t * is not a problem.\n\t\t */\n\t\tproc->debugfs_entry = debugfs_create_file(strbuf, S_IRUGO,\n\t\t\tbinder_debugfs_dir_entry_proc,\n\t\t\t(void *)(unsigned long)proc->pid,\n\t\t\t&binder_proc_fops);\n\t}\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 195
    },
    {
        "code": "static void task_fd_install(\n\tstruct binder_proc *proc, unsigned int fd, struct file *file)\n{\n\tmutex_lock(&proc->files_lock);\n\tif (proc->files)\n\t\t__fd_install(proc->files, fd, file);\n\tmutex_unlock(&proc->files_lock);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 197
    },
    {
        "code": "static int bigben_probe(struct hid_device *hid,\n\tconst struct hid_device_id *id)\n{\n\tstruct bigben_device *bigben;\n\tstruct hid_input *hidinput;\n\tstruct list_head *report_list;\n\tstruct led_classdev *led;\n\tchar *name;\n\tsize_t name_sz;\n\tint n, error;\n\tbigben = devm_kzalloc(&hid->dev, sizeof(*bigben), GFP_KERNEL);\n\tif (!bigben)\n\t\treturn -ENOMEM;\n\thid_set_drvdata(hid, bigben);\n\tbigben->hid = hid;\n\tbigben->removed = false;\n\terror = hid_parse(hid);\n\tif (error) {\n\t\thid_err(hid, \"parse failed\\n\");\n\t\treturn error;\n\t}\n\terror = hid_hw_start(hid, HID_CONNECT_DEFAULT & ~HID_CONNECT_FF);\n\tif (error) {\n\t\thid_err(hid, \"hw start failed\\n\");\n\t\treturn error;\n\t}\n\treport_list = &hid->report_enum[HID_OUTPUT_REPORT].report_list;\n\tif (list_empty(report_list)) {\n\t\thid_err(hid, \"no output report found\\n\");\n\t\terror = -ENODEV;\n\t\tgoto error_hw_stop;\n\t}\n\tbigben->report = list_entry(report_list->next,\n\t\tstruct hid_report, list);\n\tif (list_empty(&hid->inputs)) {\n\t\thid_err(hid, \"no inputs found\\n\");\n\t\terror = -ENODEV;\n\t\tgoto error_hw_stop;\n\t}\n\thidinput = list_first_entry(&hid->inputs, struct hid_input, list);\n\tset_bit(FF_RUMBLE, hidinput->input->ffbit);\n\tINIT_WORK(&bigben->worker, bigben_worker);\n\tspin_lock_init(&bigben->lock);\n\terror = input_ff_create_memless(hidinput->input, NULL,\n\t\thid_bigben_play_effect);\n\tif (error)\n\t\tgoto error_hw_stop;\n\tname_sz = strlen(dev_name(&hid->dev)) + strlen(\":red:bigben#\") + 1;\n\tfor (n = 0; n < NUM_LEDS; n++) {\n\t\tled = devm_kzalloc(\n\t\t\t&hid->dev,\n\t\t\tsizeof(struct led_classdev) + name_sz,\n\t\t\tGFP_KERNEL\n\t\t);\n\t\tif (!led) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto error_hw_stop;\n\t\t}\n\t\tname = (void *)(&led[1]);\n\t\tsnprintf(name, name_sz,\n\t\t\t\"%s:red:bigben%d\",\n\t\t\tdev_name(&hid->dev), n + 1\n\t\t);\n\t\tled->name = name;\n\t\tled->brightness = (n == 0) ? LED_ON : LED_OFF;\n\t\tled->max_brightness = 1;\n\t\tled->brightness_get = bigben_get_led;\n\t\tled->brightness_set = bigben_set_led;\n\t\tbigben->leds[n] = led;\n\t\terror = devm_led_classdev_register(&hid->dev, led);\n\t\tif (error)\n\t\t\tgoto error_hw_stop;\n\t}\n\t/* initial state: LED1 is on, no rumble effect */\n\tbigben->led_state = BIT(0);\n\tbigben->right_motor_on = 0;\n\tbigben->left_motor_force = 0;\n\tbigben->work_led = true;\n\tbigben->work_ff = true;\n\tbigben_schedule_work(bigben);\n\thid_info(hid, \"LED and force feedback support for BigBen gamepad\\n\");\n\treturn 0;\nerror_hw_stop:\n\thid_hw_stop(hid);\n\treturn error;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 199
    },
    {
        "code": "static int vgem_gem_dumb_create(struct drm_file *file, struct drm_device *dev,\n\t\t\t\tstruct drm_mode_create_dumb *args)\n{\n\tstruct drm_gem_object *gem_object;\n\tu64 pitch, size;\n\tpitch = args->width * DIV_ROUND_UP(args->bpp, 8);\n\tsize = args->height * pitch;\n\tif (size == 0)\n\t\treturn -EINVAL;\n\tgem_object = vgem_gem_create(dev, file, &args->handle, size);\n\tif (IS_ERR(gem_object))\n\t\treturn PTR_ERR(gem_object);\n\targs->size = gem_object->size;\n\targs->pitch = pitch;\n\tdrm_gem_object_put_unlocked(gem_object);\n\tDRM_DEBUG(\"Created object of size %llu\\n\", args->size);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 201
    },
    {
        "code": "static int vmw_cmd_dx_set_streamoutput(struct vmw_private *dev_priv,\n\t\t\t\t       struct vmw_sw_context *sw_context,\n\t\t\t\t       SVGA3dCmdHeader *header)\n{\n\tstruct vmw_ctx_validation_info *ctx_node = sw_context->dx_ctx_node;\n\tstruct vmw_resource *res;\n\tstruct vmw_ctx_bindinfo_so binding;\n\tstruct {\n\t\tSVGA3dCmdHeader header;\n\t\tSVGA3dCmdDXSetStreamOutput body;\n\t} *cmd = container_of(header, typeof(*cmd), header);\n\tint ret;\n\tif (!ctx_node) {\n\t\tDRM_ERROR(\"DX Context not set.\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (cmd->body.soid == SVGA3D_INVALID_ID)\n\t\treturn 0;\n\t/*\n\t * When device does not support SM5 then streamoutput with mob command is\n\t * not available to user-space. Simply return in this case.\n\t */\n\tif (!has_sm5_context(dev_priv))\n\t\treturn 0;\n\t/*\n\t * With SM5 capable device if lookup fails then user-space probably used\n\t * old streamoutput define command. Return without an error.\n\t */\n\tres = vmw_dx_streamoutput_lookup(vmw_context_res_man(ctx_node->ctx),\n\t\t\t\t\t cmd->body.soid);\n\tif (IS_ERR(res)) {\n\t\treturn 0;\n\t}\n\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_NONE,\n\t\t\t\t      vmw_val_add_flag_noctx);\n\tif (ret) {\n\t\tDRM_ERROR(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\tbinding.bi.ctx = ctx_node->ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_so;\n\tbinding.slot = 0; /* Only one SO set to context at a time. */\n\tvmw_binding_add(sw_context->dx_ctx_node->staged, &binding.bi, 0,\n\t\t\tbinding.slot);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 203
    },
    {
        "code": "static int vmw_execbuf_tie_context(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   uint32_t handle)\n{\n\tstruct vmw_resource *res;\n\tint ret;\n\tunsigned int size;\n\tif (handle == SVGA3D_INVALID_ID)\n\t\treturn 0;\n\tsize = vmw_execbuf_res_size(dev_priv, vmw_res_dx_context);\n\tret = vmw_validation_preload_res(sw_context->ctx, size);\n\tif (ret)\n\t\treturn ret;\n\tret = vmw_user_resource_lookup_handle\n\t\t(dev_priv, sw_context->fp->tfile, handle,\n\t\t user_context_converter, &res);\n\tif (ret != 0) {\n\t\tVMW_DEBUG_USER(\"Could not find or user DX context 0x%08x.\\n\",\n\t\t\t       (unsigned int) handle);\n\t\treturn ret;\n\t}\n\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_SET,\n\t\t\t\t      vmw_val_add_flag_none);\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&res);\n\t\treturn ret;\n\t}\n\tsw_context->dx_ctx_node = vmw_execbuf_info_from_res(sw_context, res);\n\tsw_context->man = vmw_context_res_man(res);\n\tvmw_resource_unreference(&res);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 205
    },
    {
        "code": "static int vmw_view_res_val_add(struct vmw_sw_context *sw_context,\n\t\t\t\tstruct vmw_resource *view)\n{\n\tint ret;\n\t/*\n\t * First add the resource the view is pointing to, otherwise it may be\n\t * swapped out when the view is validated.\n\t */\n\tret = vmw_execbuf_res_val_add(sw_context, vmw_view_srf(view),\n\t\t\t\t      vmw_view_dirtying(view), vmw_val_add_flag_noctx);\n\tif (ret)\n\t\treturn ret;\n\treturn vmw_execbuf_res_val_add(sw_context, view, VMW_RES_DIRTY_NONE,\n\t\t\t\t       vmw_val_add_flag_noctx);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 207
    },
    {
        "code": "void put_ucounts(struct ucounts *ucounts)\n{\n\tunsigned long flags;\n\tif (atomic_dec_and_lock_irqsave(&ucounts->count, &ucounts_lock, flags)) {\n\t\thlist_del_init(&ucounts->node);\n\t\tspin_unlock_irqrestore(&ucounts_lock, flags);\n\t\tput_user_ns(ucounts->ns);\n\t\tkfree(ucounts);\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 209
    },
    {
        "code": "static struct ext4_dir_entry_2 *do_split(handle_t *handle, struct inode *dir,\n\t\t\tstruct buffer_head **bh,struct dx_frame *frame,\n\t\t\tstruct dx_hash_info *hinfo)\n{\n\tunsigned blocksize = dir->i_sb->s_blocksize;\n\tunsigned count, continued;\n\tstruct buffer_head *bh2;\n\text4_lblk_t newblock;\n\tu32 hash2;\n\tstruct dx_map_entry *map;\n\tchar *data1 = (*bh)->b_data, *data2;\n\tunsigned split, move, size;\n\tstruct ext4_dir_entry_2 *de = NULL, *de2;\n\tint\tcsum_size = 0;\n\tint\terr = 0, i;\n\tif (ext4_has_metadata_csum(dir->i_sb))\n\t\tcsum_size = sizeof(struct ext4_dir_entry_tail);\n\tbh2 = ext4_append(handle, dir, &newblock);\n\tif (IS_ERR(bh2)) {\n\t\tbrelse(*bh);\n\t\t*bh = NULL;\n\t\treturn (struct ext4_dir_entry_2 *) bh2;\n\t}\n\tBUFFER_TRACE(*bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, dir->i_sb, *bh,\n\t\t\t\t\t    EXT4_JTR_NONE);\n\tif (err)\n\t\tgoto journal_error;\n\tBUFFER_TRACE(frame->bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, dir->i_sb, frame->bh,\n\t\t\t\t\t    EXT4_JTR_NONE);\n\tif (err)\n\t\tgoto journal_error;\n\tdata2 = bh2->b_data;\n\t/* create map in the end of data2 block */\n\tmap = (struct dx_map_entry *) (data2 + blocksize);\n\tcount = dx_make_map(dir, *bh, hinfo, map);\n\tif (count < 0) {\n\t\terr = count;\n\t\tgoto journal_error;\n\t}\n\tmap -= count;\n\tdx_sort_map(map, count);\n\t/* Ensure that neither split block is over half full */\n\tsize = 0;\n\tmove = 0;\n\tfor (i = count-1; i >= 0; i--) {\n\t\t/* is more than half of this entry in 2nd half of the block? */\n\t\tif (size + map[i].size/2 > blocksize/2)\n\t\t\tbreak;\n\t\tsize += map[i].size;\n\t\tmove++;\n\t}\n\t/*\n\t * map index at which we will split\n\t *\n\t * If the sum of active entries didn't exceed half the block size, just\n\t * split it in half by count; each resulting block will have at least\n\t * half the space free.\n\t */\n\tif (i > 0)\n\t\tsplit = count - move;\n\telse\n\t\tsplit = count/2;\n\thash2 = map[split].hash;\n\tcontinued = hash2 == map[split - 1].hash;\n\tdxtrace(printk(KERN_INFO \"Split block %lu at %x, %i/%i\\n\",\n\t\t\t(unsigned long)dx_get_block(frame->at),\n\t\t\t\t\thash2, split, count-split));\n\t/* Fancy dance to stay within two buffers */\n\tde2 = dx_move_dirents(dir, data1, data2, map + split, count - split,\n\t\t\t      blocksize);\n\tde = dx_pack_dirents(dir, data1, blocksize);\n\tde->rec_len = ext4_rec_len_to_disk(data1 + (blocksize - csum_size) -\n\t\t\t\t\t   (char *) de,\n\t\t\t\t\t   blocksize);\n\tde2->rec_len = ext4_rec_len_to_disk(data2 + (blocksize - csum_size) -\n\t\t\t\t\t    (char *) de2,\n\t\t\t\t\t    blocksize);\n\tif (csum_size) {\n\t\text4_initialize_dirent_tail(*bh, blocksize);\n\t\text4_initialize_dirent_tail(bh2, blocksize);\n\t}\n\tdxtrace(dx_show_leaf(dir, hinfo, (struct ext4_dir_entry_2 *) data1,\n\t\t\tblocksize, 1));\n\tdxtrace(dx_show_leaf(dir, hinfo, (struct ext4_dir_entry_2 *) data2,\n\t\t\tblocksize, 1));\n\t/* Which block gets the new entry? */\n\tif (hinfo->hash >= hash2) {\n\t\tswap(*bh, bh2);\n\t\tde = de2;\n\t}\n\tdx_insert_block(frame, hash2 + continued, newblock);\n\terr = ext4_handle_dirty_dirblock(handle, dir, bh2);\n\tif (err)\n\t\tgoto journal_error;\n\terr = ext4_handle_dirty_dx_node(handle, dir, frame->bh);\n\tif (err)\n\t\tgoto journal_error;\n\tbrelse(bh2);\n\tdxtrace(dx_show_index(\"frame\", frame->entries));\n\treturn de;\njournal_error:\n\tbrelse(*bh);\n\tbrelse(bh2);\n\t*bh = NULL;\n\text4_std_error(dir->i_sb, err);\n\treturn ERR_PTR(err);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 211
    },
    {
        "code": "static int p54u_load_firmware(struct ieee80211_hw *dev,\n\t\t\t      struct usb_interface *intf)\n{\n\tstruct usb_device *udev = interface_to_usbdev(intf);\n\tstruct p54u_priv *priv = dev->priv;\n\tstruct device *device = &udev->dev;\n\tint err, i;\n\tBUILD_BUG_ON(ARRAY_SIZE(p54u_fwlist) != __NUM_P54U_HWTYPES);\n\tinit_completion(&priv->fw_wait_load);\n\ti = p54_find_type(priv);\n\tif (i < 0)\n\t\treturn i;\n\tdev_info(&priv->udev->dev, \"Loading firmware file %s\\n\",\n\t       p54u_fwlist[i].fw);\n\tusb_get_intf(intf);\n\terr = request_firmware_nowait(THIS_MODULE, 1, p54u_fwlist[i].fw,\n\t\t\t\t      device, GFP_KERNEL, priv,\n\t\t\t\t      p54u_load_firmware_cb);\n\tif (err) {\n\t\tdev_err(&priv->udev->dev, \"(p54usb) cannot load firmware %s \"\n\t\t\t\t\t  \"(%d)!\\n\", p54u_fwlist[i].fw, err);\n\t\tusb_put_intf(intf);\n\t}\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 213
    },
    {
        "code": "int ext4_mb_add_groupinfo(struct super_block *sb, ext4_group_t group,\n\t\t\t  struct ext4_group_desc *desc)\n{\n\tint i;\n\tint metalen = 0;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_info **meta_group_info;\n\tstruct kmem_cache *cachep = get_groupinfo_cache(sb->s_blocksize_bits);\n\t/*\n\t * First check if this group is the first of a reserved block.\n\t * If it's true, we have to allocate a new table of pointers\n\t * to ext4_group_info structures\n\t */\n\tif (group % EXT4_DESC_PER_BLOCK(sb) == 0) {\n\t\tmetalen = sizeof(*meta_group_info) <<\n\t\t\tEXT4_DESC_PER_BLOCK_BITS(sb);\n\t\tmeta_group_info = kmalloc(metalen, GFP_NOFS);\n\t\tif (meta_group_info == NULL) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't allocate mem \"\n\t\t\t\t \"for a buddy group\");\n\t\t\tgoto exit_meta_group_info;\n\t\t}\n\t\tsbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)] =\n\t\t\tmeta_group_info;\n\t}\n\tmeta_group_info =\n\t\tsbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)];\n\ti = group & (EXT4_DESC_PER_BLOCK(sb) - 1);\n\tmeta_group_info[i] = kmem_cache_zalloc(cachep, GFP_NOFS);\n\tif (meta_group_info[i] == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"can't allocate buddy mem\");\n\t\tgoto exit_group_info;\n\t}\n\tset_bit(EXT4_GROUP_INFO_NEED_INIT_BIT,\n\t\t&(meta_group_info[i]->bb_state));\n\t/*\n\t * initialize bb_free to be able to skip\n\t * empty groups without initialization\n\t */\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {\n\t\tmeta_group_info[i]->bb_free =\n\t\t\text4_free_clusters_after_init(sb, group, desc);\n\t} else {\n\t\tmeta_group_info[i]->bb_free =\n\t\t\text4_free_group_clusters(sb, desc);\n\t}\n\tINIT_LIST_HEAD(&meta_group_info[i]->bb_prealloc_list);\n\tinit_rwsem(&meta_group_info[i]->alloc_sem);\n\tmeta_group_info[i]->bb_free_root = RB_ROOT;\n\tmeta_group_info[i]->bb_largest_free_order = -1;  /* uninit */\n#ifdef DOUBLE_CHECK\n\t{\n\t\tstruct buffer_head *bh;\n\t\tmeta_group_info[i]->bb_bitmap =\n\t\t\tkmalloc(sb->s_blocksize, GFP_NOFS);\n\t\tBUG_ON(meta_group_info[i]->bb_bitmap == NULL);\n\t\tbh = ext4_read_block_bitmap(sb, group);\n\t\tBUG_ON(IS_ERR_OR_NULL(bh));\n\t\tmemcpy(meta_group_info[i]->bb_bitmap, bh->b_data,\n\t\t\tsb->s_blocksize);\n\t\tput_bh(bh);\n\t}\n#endif\n\treturn 0;\nexit_group_info:\n\t/* If a meta_group_info table has been allocated, release it now */\n\tif (group % EXT4_DESC_PER_BLOCK(sb) == 0) {\n\t\tkfree(sbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)]);\n\t\tsbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)] = NULL;\n\t}\nexit_meta_group_info:\n\treturn -ENOMEM;\n} /* ext4_mb_add_groupinfo */",
        "bug_line_number": [],
        "vul": 0,
        "id": 215
    },
    {
        "code": "int __mdiobus_register(struct mii_bus *bus, struct module *owner)\n{\n\tstruct mdio_device *mdiodev;\n\tint i, err;\n\tstruct gpio_desc *gpiod;\n\tif (NULL == bus || NULL == bus->name ||\n\t    NULL == bus->read || NULL == bus->write)\n\t\treturn -EINVAL;\n\tBUG_ON(bus->state != MDIOBUS_ALLOCATED &&\n\t       bus->state != MDIOBUS_UNREGISTERED);\n\tbus->owner = owner;\n\tbus->dev.parent = bus->parent;\n\tbus->dev.class = &mdio_bus_class;\n\tbus->dev.groups = NULL;\n\tdev_set_name(&bus->dev, \"%s\", bus->id);\n\terr = device_register(&bus->dev);\n\tif (err) {\n\t\tpr_err(\"mii_bus %s failed to register\\n\", bus->id);\n\t\treturn -EINVAL;\n\t}\n\tmutex_init(&bus->mdio_lock);\n\t/* de-assert bus level PHY GPIO reset */\n\tgpiod = devm_gpiod_get_optional(&bus->dev, \"reset\", GPIOD_OUT_LOW);\n\tif (IS_ERR(gpiod)) {\n\t\tdev_err(&bus->dev, \"mii_bus %s couldn't get reset GPIO\\n\",\n\t\t\tbus->id);\n\t\tdevice_del(&bus->dev);\n\t\treturn PTR_ERR(gpiod);\n\t} else\tif (gpiod) {\n\t\tbus->reset_gpiod = gpiod;\n\t\tgpiod_set_value_cansleep(gpiod, 1);\n\t\tudelay(bus->reset_delay_us);\n\t\tgpiod_set_value_cansleep(gpiod, 0);\n\t}\n\tif (bus->reset)\n\t\tbus->reset(bus);\n\tfor (i = 0; i < PHY_MAX_ADDR; i++) {\n\t\tif ((bus->phy_mask & (1 << i)) == 0) {\n\t\t\tstruct phy_device *phydev;\n\t\t\tphydev = mdiobus_scan(bus, i);\n\t\t\tif (IS_ERR(phydev) && (PTR_ERR(phydev) != -ENODEV)) {\n\t\t\t\terr = PTR_ERR(phydev);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t}\n\t}\n\tmdiobus_setup_mdiodev_from_board_info(bus, mdiobus_create_device);\n\tbus->state = MDIOBUS_REGISTERED;\n\tpr_info(\"%s: probed\\n\", bus->name);\n\treturn 0;\nerror:\n\twhile (--i >= 0) {\n\t\tmdiodev = bus->mdio_map[i];\n\t\tif (!mdiodev)\n\t\t\tcontinue;\n\t\tmdiodev->device_remove(mdiodev);\n\t\tmdiodev->device_free(mdiodev);\n\t}\n\t/* Put PHYs in RESET to save power */\n\tif (bus->reset_gpiod)\n\t\tgpiod_set_value_cansleep(bus->reset_gpiod, 1);\n\tdevice_del(&bus->dev);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 217
    },
    {
        "code": "static void __mctp_key_remove(struct mctp_sk_key *key, struct net *net,\n\t\t\t      unsigned long flags, unsigned long reason)\n__releases(&key->lock)\n__must_hold(&net->mctp.keys_lock)\n{\n\tstruct sk_buff *skb;\n\ttrace_mctp_key_release(key, reason);\n\tskb = key->reasm_head;\n\tkey->reasm_head = NULL;\n\tkey->reasm_dead = true;\n\tkey->valid = false;\n\tmctp_dev_release_key(key->dev, key);\n\tspin_unlock_irqrestore(&key->lock, flags);\n\tif (!hlist_unhashed(&key->hlist)) {\n\t\thlist_del_init(&key->hlist);\n\t\thlist_del_init(&key->sklist);\n\t\t/* unref for the lists */\n\t\tmctp_key_unref(key);\n\t}\n\tkfree_skb(skb);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 219
    },
    {
        "code": "static int l2cap_le_connect_req(struct l2cap_conn *conn,\n\t\t\t\tstruct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\tu8 *data)\n{\n\tstruct l2cap_le_conn_req *req = (struct l2cap_le_conn_req *) data;\n\tstruct l2cap_le_conn_rsp rsp;\n\tstruct l2cap_chan *chan, *pchan;\n\tu16 dcid, scid, credits, mtu, mps;\n\t__le16 psm;\n\tu8 result;\n\tif (cmd_len != sizeof(*req))\n\t\treturn -EPROTO;\n\tscid = __le16_to_cpu(req->scid);\n\tmtu  = __le16_to_cpu(req->mtu);\n\tmps  = __le16_to_cpu(req->mps);\n\tpsm  = req->psm;\n\tdcid = 0;\n\tcredits = 0;\n\tif (mtu < 23 || mps < 23)\n\t\treturn -EPROTO;\n\tBT_DBG(\"psm 0x%2.2x scid 0x%4.4x mtu %u mps %u\", __le16_to_cpu(psm),\n\t       scid, mtu, mps);\n\t/* BLUETOOTH CORE SPECIFICATION Version 5.3 | Vol 3, Part A\n\t * page 1059:\n\t *\n\t * Valid range: 0x0001-0x00ff\n\t *\n\t * Table 4.15: L2CAP_LE_CREDIT_BASED_CONNECTION_REQ SPSM ranges\n\t */\n\tif (!psm || __le16_to_cpu(psm) > L2CAP_PSM_LE_DYN_END) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tchan = NULL;\n\t\tgoto response;\n\t}\n\t/* Check if we have socket listening on psm */\n\tpchan = l2cap_global_chan_by_psm(BT_LISTEN, psm, &conn->hcon->src,\n\t\t\t\t\t &conn->hcon->dst, LE_LINK);\n\tif (!pchan) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tchan = NULL;\n\t\tgoto response;\n\t}\n\tmutex_lock(&conn->chan_lock);\n\tl2cap_chan_lock(pchan);\n\tif (!smp_sufficient_security(conn->hcon, pchan->sec_level,\n\t\t\t\t     SMP_ALLOW_STK)) {\n\t\tresult = L2CAP_CR_LE_AUTHENTICATION;\n\t\tchan = NULL;\n\t\tgoto response_unlock;\n\t}\n\t/* Check for valid dynamic CID range */\n\tif (scid < L2CAP_CID_DYN_START || scid > L2CAP_CID_LE_DYN_END) {\n\t\tresult = L2CAP_CR_LE_INVALID_SCID;\n\t\tchan = NULL;\n\t\tgoto response_unlock;\n\t}\n\t/* Check if we already have channel with that dcid */\n\tif (__l2cap_get_chan_by_dcid(conn, scid)) {\n\t\tresult = L2CAP_CR_LE_SCID_IN_USE;\n\t\tchan = NULL;\n\t\tgoto response_unlock;\n\t}\n\tchan = pchan->ops->new_connection(pchan);\n\tif (!chan) {\n\t\tresult = L2CAP_CR_LE_NO_MEM;\n\t\tgoto response_unlock;\n\t}\n\tbacpy(&chan->src, &conn->hcon->src);\n\tbacpy(&chan->dst, &conn->hcon->dst);\n\tchan->src_type = bdaddr_src_type(conn->hcon);\n\tchan->dst_type = bdaddr_dst_type(conn->hcon);\n\tchan->psm  = psm;\n\tchan->dcid = scid;\n\tchan->omtu = mtu;\n\tchan->remote_mps = mps;\n\t__l2cap_chan_add(conn, chan);\n\tl2cap_le_flowctl_init(chan, __le16_to_cpu(req->credits));\n\tdcid = chan->scid;\n\tcredits = chan->rx_credits;\n\t__set_chan_timer(chan, chan->ops->get_sndtimeo(chan));\n\tchan->ident = cmd->ident;\n\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\tl2cap_state_change(chan, BT_CONNECT2);\n\t\t/* The following result value is actually not defined\n\t\t * for LE CoC but we use it to let the function know\n\t\t * that it should bail out after doing its cleanup\n\t\t * instead of sending a response.\n\t\t */\n\t\tresult = L2CAP_CR_PEND;\n\t\tchan->ops->defer(chan);\n\t} else {\n\t\tl2cap_chan_ready(chan);\n\t\tresult = L2CAP_CR_LE_SUCCESS;\n\t}\nresponse_unlock:\n\tl2cap_chan_unlock(pchan);\n\tmutex_unlock(&conn->chan_lock);\n\tl2cap_chan_put(pchan);\n\tif (result == L2CAP_CR_PEND)\n\t\treturn 0;",
        "bug_line_number": [],
        "vul": 0,
        "id": 221
    },
    {
        "code": "void xfrm_state_fini(struct net *net)\n{\n\tunsigned int sz;\n\tflush_work(&net->xfrm.state_hash_work);\n\tflush_work(&xfrm_state_gc_work);\n\txfrm_state_flush(net, 0, false, true);\n\tWARN_ON(!list_empty(&net->xfrm.state_all));\n\tsz = (net->xfrm.state_hmask + 1) * sizeof(struct hlist_head);\n\tWARN_ON(!hlist_empty(net->xfrm.state_byspi));\n\txfrm_hash_free(net->xfrm.state_byspi, sz);\n\tWARN_ON(!hlist_empty(net->xfrm.state_bysrc));\n\txfrm_hash_free(net->xfrm.state_bysrc, sz);\n\tWARN_ON(!hlist_empty(net->xfrm.state_bydst));\n\txfrm_hash_free(net->xfrm.state_bydst, sz);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 223
    },
    {
        "code": "static inline struct kvm_memory_slot *\nsearch_memslots(struct kvm_memslots *slots, gfn_t gfn)\n{\n\tint start = 0, end = slots->used_slots;\n\tint slot = atomic_read(&slots->lru_slot);\n\tstruct kvm_memory_slot *memslots = slots->memslots;\n\tif (unlikely(!slots->used_slots))\n\t\treturn NULL;\n\tif (gfn >= memslots[slot].base_gfn &&\n\t    gfn < memslots[slot].base_gfn + memslots[slot].npages)\n\t\treturn &memslots[slot];\n\twhile (start < end) {\n\t\tslot = start + (end - start) / 2;\n\t\tif (gfn >= memslots[slot].base_gfn)\n\t\t\tend = slot;\n\t\telse\n\t\t\tstart = slot + 1;\n\t}\n\tif (gfn >= memslots[start].base_gfn &&\n\t    gfn < memslots[start].base_gfn + memslots[start].npages) {\n\t\tatomic_set(&slots->lru_slot, start);\n\t\treturn &memslots[start];\n\t}\n\treturn NULL;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 225
    },
    {
        "code": "static int dt_remember_or_free_map(struct pinctrl *p, const char *statename,\n\t\t\t\t   struct pinctrl_dev *pctldev,\n\t\t\t\t   struct pinctrl_map *map, unsigned num_maps)\n{\n\tint i;\n\tstruct pinctrl_dt_map *dt_map;\n\t/* Initialize common mapping table entry fields */\n\tfor (i = 0; i < num_maps; i++) {\n\t\tconst char *devname;\n\t\tdevname = kstrdup_const(dev_name(p->dev), GFP_KERNEL);\n\t\tif (!devname)\n\t\t\tgoto err_free_map;\n\t\tmap[i].dev_name = devname;\n\t\tmap[i].name = statename;\n\t\tif (pctldev)\n\t\t\tmap[i].ctrl_dev_name = dev_name(pctldev->dev);\n\t}\n\t/* Remember the converted mapping table entries */\n\tdt_map = kzalloc(sizeof(*dt_map), GFP_KERNEL);\n\tif (!dt_map)\n\t\tgoto err_free_map;\n\tdt_map->pctldev = pctldev;\n\tdt_map->map = map;\n\tdt_map->num_maps = num_maps;\n\tlist_add_tail(&dt_map->node, &p->dt_maps);\n\treturn pinctrl_register_map(map, num_maps, false);\nerr_free_map:\n\tdt_free_map(pctldev, map, num_maps);\n\treturn -ENOMEM;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 227
    },
    {
        "code": "static void nft_immediate_activate(const struct nft_ctx *ctx,\n\t\t\t\t   const struct nft_expr *expr)\n{\n\tconst struct nft_immediate_expr *priv = nft_expr_priv(expr);\n\tconst struct nft_data *data = &priv->data;\n\tstruct nft_ctx chain_ctx;\n\tstruct nft_chain *chain;\n\tstruct nft_rule *rule;\n\tif (priv->dreg == NFT_REG_VERDICT) {\n\t\tswitch (data->verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\tchain = data->verdict.chain;\n\t\t\tif (!nft_chain_binding(chain))\n\t\t\t\tbreak;\n\t\t\tchain_ctx = *ctx;\n\t\t\tchain_ctx.chain = chain;\n\t\t\tlist_for_each_entry(rule, &chain->rules, list)\n\t\t\t\tnft_rule_expr_activate(&chain_ctx, rule);\n\t\t\tnft_clear(ctx->net, chain);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn nft_data_hold(&priv->data, nft_dreg_to_type(priv->dreg));\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 229
    },
    {
        "code": "static void nft_immediate_destroy(const struct nft_ctx *ctx,\n\t\t\t\t  const struct nft_expr *expr)\n{\n\tconst struct nft_immediate_expr *priv = nft_expr_priv(expr);\n\tconst struct nft_data *data = &priv->data;\n\tstruct nft_rule *rule, *n;\n\tstruct nft_ctx chain_ctx;\n\tstruct nft_chain *chain;\n\tif (priv->dreg != NFT_REG_VERDICT)\n\t\treturn;\n\tswitch (data->verdict.code) {\n\tcase NFT_JUMP:\n\tcase NFT_GOTO:\n\t\tchain = data->verdict.chain;\n\t\tif (!nft_chain_binding(chain))\n\t\t\tbreak;\n\t\t/* Rule construction failed, but chain is already bound:\n\t\t * let the transaction records release this chain and its rules.\n\t\t */\n\t\tif (chain->bound) {\n\t\t\tchain->use--;\n\t\t\tbreak;\n\t\t}\n\t\t/* Rule has been deleted, release chain and its rules. */\n\t\tchain_ctx = *ctx;\n\t\tchain_ctx.chain = chain;\n\t\tchain->use--;\n\t\tlist_for_each_entry_safe(rule, n, &chain->rules, list) {\n\t\t\tchain->use--;\n\t\t\tlist_del(&rule->list);\n\t\t\tnf_tables_rule_destroy(&chain_ctx, rule);\n\t\t}\n\t\tnf_tables_chain_destroy(&chain_ctx);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 231
    },
    {
        "code": "static int __nf_tables_abort(struct net *net, enum nfnl_abort_action action)\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(net);\n\tstruct nft_trans *trans, *next;\n\tLIST_HEAD(set_update_list);\n\tstruct nft_trans_elem *te;\n\tif (action == NFNL_ABORT_VALIDATE &&\n\t    nf_tables_validate(net) < 0)\n\t\treturn -EAGAIN;\n\tlist_for_each_entry_safe_reverse(trans, next, &nft_net->commit_list,\n\t\t\t\t\t list) {\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWTABLE:\n\t\t\tif (nft_trans_table_update(trans)) {\n\t\t\t\tif (!(trans->ctx.table->flags & __NFT_TABLE_F_UPDATE)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (trans->ctx.table->flags & __NFT_TABLE_F_WAS_DORMANT) {\n\t\t\t\t\tnf_tables_table_disable(net, trans->ctx.table);\n\t\t\t\t\ttrans->ctx.table->flags |= NFT_TABLE_F_DORMANT;\n\t\t\t\t} else if (trans->ctx.table->flags & __NFT_TABLE_F_WAS_AWAKEN) {\n\t\t\t\t\ttrans->ctx.table->flags &= ~NFT_TABLE_F_DORMANT;\n\t\t\t\t}\n\t\t\t\ttrans->ctx.table->flags &= ~__NFT_TABLE_F_UPDATE;\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t} else {\n\t\t\t\tlist_del_rcu(&trans->ctx.table->list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELTABLE:\n\t\tcase NFT_MSG_DESTROYTABLE:\n\t\t\tnft_clear(trans->ctx.net, trans->ctx.table);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnft_netdev_unregister_hooks(net,\n\t\t\t\t\t\t\t    &nft_trans_chain_hooks(trans),\n\t\t\t\t\t\t\t    true);\n\t\t\t\tfree_percpu(nft_trans_chain_stats(trans));\n\t\t\t\tkfree(nft_trans_chain_name(trans));\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t} else {\n\t\t\t\tif (nft_trans_chain_bound(trans)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\ttrans->ctx.table->use--;\n\t\t\t\tnft_chain_del(trans->ctx.chain);\n\t\t\t\tnf_tables_unregister_hook(trans->ctx.net,\n\t\t\t\t\t\t\t  trans->ctx.table,\n\t\t\t\t\t\t\t  trans->ctx.chain);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELCHAIN:\n\t\tcase NFT_MSG_DESTROYCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tlist_splice(&nft_trans_chain_hooks(trans),\n\t\t\t\t\t    &nft_trans_basechain(trans)->hook_list);\n\t\t\t} else {\n\t\t\t\ttrans->ctx.table->use++;\n\t\t\t\tnft_clear(trans->ctx.net, trans->ctx.chain);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWRULE:\n\t\t\tif (nft_trans_rule_bound(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttrans->ctx.chain->use--;\n\t\t\tlist_del_rcu(&nft_trans_rule(trans)->list);\n\t\t\tnft_rule_expr_deactivate(&trans->ctx,\n\t\t\t\t\t\t nft_trans_rule(trans),\n\t\t\t\t\t\t NFT_TRANS_ABORT);\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELRULE:\n\t\tcase NFT_MSG_DESTROYRULE:\n\t\t\ttrans->ctx.chain->use++;\n\t\t\tnft_clear(trans->ctx.net, nft_trans_rule(trans));\n\t\t\tnft_rule_expr_activate(&trans->ctx, nft_trans_rule(trans));\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (nft_trans_set_update(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttrans->ctx.table->use--;\n\t\t\tif (nft_trans_set_bound(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tlist_del_rcu(&nft_trans_set(trans)->list);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSET:\n\t\tcase NFT_MSG_DESTROYSET:\n\t\t\ttrans->ctx.table->use++;\n\t\t\tnft_clear(trans->ctx.net, nft_trans_set(trans));\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSETELEM:\n\t\t\tif (nft_trans_elem_set_bound(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\t\t\tnft_setelem_remove(net, te->set, &te->elem);\n\t\t\tif (!nft_setelem_is_catchall(te->set, &te->elem))\n\t\t\t\tatomic_dec(&te->set->nelems);\n\t\t\tif (te->set->ops->abort &&",
        "bug_line_number": [],
        "vul": 0,
        "id": 233
    },
    {
        "code": "static inline void vmacache_invalidate(struct mm_struct *mm)\n{\n\tmm->vmacache_seqnum++;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 235
    },
    {
        "code": "static void slcan_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slcan *sl;\n\trcu_read_lock();\n\tsl = rcu_dereference(tty->disc_data);\n\tif (!sl)\n\t\tgoto out;\n\tschedule_work(&sl->tx_work);\nout:\n\trcu_read_unlock();\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 237
    },
    {
        "code": "static int vt_disallocate(unsigned int vc_num)\n{\n\tstruct vc_data *vc = NULL;\n\tint ret = 0;\n\tconsole_lock();\n\tif (vt_busy(vc_num))\n\t\tret = -EBUSY;\n\telse if (vc_num)\n\t\tvc = vc_deallocate(vc_num);\n\tconsole_unlock();\n\tif (vc && vc_num >= MIN_NR_CONSOLES)\n\t\ttty_port_put(&vc->port);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 239
    },
    {
        "code": "int vc_allocate(unsigned int currcons)\t/* return 0 on success */\n{\n\tstruct vt_notifier_param param;\n\tstruct vc_data *vc;\n\tWARN_CONSOLE_UNLOCKED();\n\tif (currcons >= MAX_NR_CONSOLES)\n\t\treturn -ENXIO;\n\tif (vc_cons[currcons].d)\n\t\treturn 0;\n\t/* due to the granularity of kmalloc, we waste some memory here */\n\t/* the alloc is done in two steps, to optimize the common situation\n\t   of a 25x80 console (structsize=216, screenbuf_size=4000) */\n\t/* although the numbers above are not valid since long ago, the\n\t   point is still up-to-date and the comment still has its value\n\t   even if only as a historical artifact.  --mj, July 1998 */\n\tparam.vc = vc = kzalloc(sizeof(struct vc_data), GFP_KERNEL);\n\tif (!vc)\n\t\treturn -ENOMEM;\n\tvc_cons[currcons].d = vc;\n\ttty_port_init(&vc->port);\n\tvc->port.ops = &vc_port_ops;\n\tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n\tvisual_init(vc, currcons, 1);\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(vc);\n\tvc->vc_screenbuf = kzalloc(vc->vc_screenbuf_size, GFP_KERNEL);\n\tif (!vc->vc_screenbuf)\n\t\tgoto err_free;\n\t/* If no drivers have overridden us and the user didn't pass a\n\t   boot option, default to displaying the cursor */\n\tif (global_cursor_default == -1)\n\t\tglobal_cursor_default = 1;\n\tvc_init(vc, vc->vc_rows, vc->vc_cols, 1);\n\tvcs_make_sysfs(currcons);\n\tatomic_notifier_call_chain(&vt_notifier_list, VT_ALLOCATE, &param);\n\treturn 0;\nerr_free:\n\tvisual_deinit(vc);\n\tkfree(vc);\n\tvc_cons[currcons].d = NULL;\n\treturn -ENOMEM;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 241
    },
    {
        "code": "static void vt_disallocate_all(void)\n{\n\tstruct vc_data *vc[MAX_NR_CONSOLES];\n\tint i;\n\tconsole_lock();\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++)\n\t\tif (!vt_busy(i))\n\t\t\tvc[i] = vc_deallocate(i);\n\t\telse\n\t\t\tvc[i] = NULL;\n\tconsole_unlock();\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++) {\n\t\tif (vc[i] && i >= MIN_NR_CONSOLES)\n\t\t\ttty_port_put(&vc[i]->port);\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 243
    },
    {
        "code": "static void iowarrior_disconnect(struct usb_interface *interface)\n{\n\tstruct iowarrior *dev;\n\tint minor;\n\tdev = usb_get_intfdata(interface);\n\tmutex_lock(&iowarrior_open_disc_lock);\n\tusb_set_intfdata(interface, NULL);\n\tminor = dev->minor;\n\tmutex_unlock(&iowarrior_open_disc_lock);\n\t/* give back our minor - this will call close() locks need to be dropped at this point*/\n\tusb_deregister_dev(interface, &iowarrior_class);\n\tmutex_lock(&dev->mutex);\n\t/* prevent device read, write and ioctl */\n\tdev->present = 0;\n\tif (dev->opened) {\n\t\t/* There is a process that holds a filedescriptor to the device ,\n\t\t   so we only shutdown read-/write-ops going on.\n\t\t   Deleting the device is postponed until close() was called.\n\t\t */\n\t\tusb_kill_urb(dev->int_in_urb);\n\t\twake_up_interruptible(&dev->read_wait);\n\t\twake_up_interruptible(&dev->write_wait);\n\t\tmutex_unlock(&dev->mutex);\n\t} else {\n\t\t/* no process is using the device, cleanup now */\n\t\tmutex_unlock(&dev->mutex);\n\t\tiowarrior_delete(dev);\n\t}\n\tdev_info(&interface->dev, \"I/O-Warror #%d now disconnected\\n\",\n\t\t minor - IOWARRIOR_MINOR_BASE);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 245
    },
    {
        "code": "static void hfsplus_put_super(struct super_block *sb)\n{\n\tstruct hfsplus_sb_info *sbi = HFSPLUS_SB(sb);\n\thfs_dbg(SUPER, \"hfsplus_put_super\\n\");\n\tcancel_delayed_work_sync(&sbi->sync_work);\n\tif (!sb_rdonly(sb) && sbi->s_vhdr) {\n\t\tstruct hfsplus_vh *vhdr = sbi->s_vhdr;\n\t\tvhdr->modify_date = hfsp_now2mt();\n\t\tvhdr->attributes |= cpu_to_be32(HFSPLUS_VOL_UNMNT);\n\t\tvhdr->attributes &= cpu_to_be32(~HFSPLUS_VOL_INCNSTNT);\n\t\thfsplus_sync_fs(sb, 1);\n\t}\n\tiput(sbi->alloc_file);\n\tiput(sbi->hidden_dir);\n\thfs_btree_close(sbi->attr_tree);\n\thfs_btree_close(sbi->cat_tree);\n\thfs_btree_close(sbi->ext_tree);\n\tkfree(sbi->s_vhdr_buf);\n\tkfree(sbi->s_backup_vhdr_buf);\n\tunload_nls(sbi->nls);\n\tkfree(sb->s_fs_info);\n\tsb->s_fs_info = NULL;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 247
    },
    {
        "code": "static long do_get_mempolicy(int *policy, nodemask_t *nmask,\n\t\t\t     unsigned long addr, unsigned long flags)\n{\n\tint err;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mempolicy *pol = current->mempolicy;\n\tif (flags &\n\t\t~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))\n\t\treturn -EINVAL;\n\tif (flags & MPOL_F_MEMS_ALLOWED) {\n\t\tif (flags & (MPOL_F_NODE|MPOL_F_ADDR))\n\t\t\treturn -EINVAL;\n\t\t*policy = 0;\t/* just so it's initialized */\n\t\ttask_lock(current);\n\t\t*nmask  = cpuset_current_mems_allowed;\n\t\ttask_unlock(current);\n\t\treturn 0;\n\t}\n\tif (flags & MPOL_F_ADDR) {\n\t\t/*\n\t\t * Do NOT fall back to task policy if the\n\t\t * vma/shared policy at addr is NULL.  We\n\t\t * want to return MPOL_DEFAULT in this case.\n\t\t */\n\t\tdown_read(&mm->mmap_sem);\n\t\tvma = find_vma_intersection(mm, addr, addr+1);\n\t\tif (!vma) {\n\t\t\tup_read(&mm->mmap_sem);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (vma->vm_ops && vma->vm_ops->get_policy)\n\t\t\tpol = vma->vm_ops->get_policy(vma, addr);\n\t\telse\n\t\t\tpol = vma->vm_policy;\n\t} else if (addr)\n\t\treturn -EINVAL;\n\tif (!pol)\n\t\tpol = &default_policy;\t/* indicates default behavior */\n\tif (flags & MPOL_F_NODE) {\n\t\tif (flags & MPOL_F_ADDR) {\n\t\t\terr = lookup_node(addr);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t\t*policy = err;\n\t\t} else if (pol == current->mempolicy &&\n\t\t\t\tpol->mode == MPOL_INTERLEAVE) {\n\t\t\t*policy = next_node_in(current->il_prev, pol->v.nodes);\n\t\t} else {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\t*policy = pol == &default_policy ? MPOL_DEFAULT :\n\t\t\t\t\t\tpol->mode;\n\t\t/*\n\t\t * Internal mempolicy flags must be masked off before exposing\n\t\t * the policy to userspace.\n\t\t */\n\t\t*policy |= (pol->flags & MPOL_MODE_FLAGS);\n\t}\n\terr = 0;\n\tif (nmask) {\n\t\tif (mpol_store_user_nodemask(pol)) {\n\t\t\t*nmask = pol->w.user_nodemask;\n\t\t} else {\n\t\t\ttask_lock(current);\n\t\t\tget_policy_nodemask(pol, nmask);\n\t\t\ttask_unlock(current);\n\t\t}\n\t}\n out:\n\tmpol_cond_put(pol);\n\tif (vma)\n\t\tup_read(&current->mm->mmap_sem);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 249
    },
    {
        "code": "unsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len,\n\t\tbool need_rmap_locks)\n{\n\tunsigned long extent, old_end;\n\tstruct mmu_notifier_range range;\n\tpmd_t *old_pmd, *new_pmd;\n\tpud_t *old_pud, *new_pud;\n\told_end = old_addr + len;\n\tflush_cache_range(vma, old_addr, old_end);\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, vma, vma->vm_mm,\n\t\t\t\told_addr, old_end);\n\tmmu_notifier_invalidate_range_start(&range);\n\tfor (; old_addr < old_end; old_addr += extent, new_addr += extent) {\n\t\tcond_resched();\n\t\t/*\n\t\t * If extent is PUD-sized try to speed up the move by moving at the\n\t\t * PUD level if possible.\n\t\t */\n\t\textent = get_extent(NORMAL_PUD, old_addr, old_end, new_addr);\n\t\told_pud = get_old_pud(vma->vm_mm, old_addr);\n\t\tif (!old_pud)\n\t\t\tcontinue;\n\t\tnew_pud = alloc_new_pud(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pud)\n\t\t\tbreak;\n\t\tif (pud_trans_huge(*old_pud) || pud_devmap(*old_pud)) {\n\t\t\tif (extent == HPAGE_PUD_SIZE) {\n\t\t\t\tmove_pgt_entry(HPAGE_PUD, vma, old_addr, new_addr,\n\t\t\t\t\t       old_pud, new_pud, need_rmap_locks);\n\t\t\t\t/* We ignore and continue on error? */\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else if (IS_ENABLED(CONFIG_HAVE_MOVE_PUD) && extent == PUD_SIZE) {\n\t\t\tif (move_pgt_entry(NORMAL_PUD, vma, old_addr, new_addr,\n\t\t\t\t\t   old_pud, new_pud, true))\n\t\t\t\tcontinue;\n\t\t}\n\t\textent = get_extent(NORMAL_PMD, old_addr, old_end, new_addr);\n\t\told_pmd = get_old_pmd(vma->vm_mm, old_addr);\n\t\tif (!old_pmd)\n\t\t\tcontinue;\n\t\tnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pmd)\n\t\t\tbreak;\n\t\tif (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd) ||\n\t\t    pmd_devmap(*old_pmd)) {\n\t\t\tif (extent == HPAGE_PMD_SIZE &&\n\t\t\t    move_pgt_entry(HPAGE_PMD, vma, old_addr, new_addr,\n\t\t\t\t\t   old_pmd, new_pmd, need_rmap_locks))\n\t\t\t\tcontinue;\n\t\t\tsplit_huge_pmd(vma, old_pmd, old_addr);\n\t\t\tif (pmd_trans_unstable(old_pmd))\n\t\t\t\tcontinue;\n\t\t} else if (IS_ENABLED(CONFIG_HAVE_MOVE_PMD) &&\n\t\t\t   extent == PMD_SIZE) {\n\t\t\t/*\n\t\t\t * If the extent is PMD-sized, try to speed the move by\n\t\t\t * moving at the PMD level if possible.\n\t\t\t */\n\t\t\tif (move_pgt_entry(NORMAL_PMD, vma, old_addr, new_addr,\n\t\t\t\t\t   old_pmd, new_pmd, true))\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (pte_alloc(new_vma->vm_mm, new_pmd))\n\t\t\tbreak;\n\t\tmove_ptes(vma, old_pmd, old_addr, old_addr + extent, new_vma,\n\t\t\t  new_pmd, new_addr, need_rmap_locks);\n\t}\n\tmmu_notifier_invalidate_range_end(&range);\n\treturn len + old_addr - old_end;\t/* how much done */\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 251
    },
    {
        "code": "static void adu_disconnect(struct usb_interface *interface)\n{\n\tstruct adu_device *dev;\n\tdev = usb_get_intfdata(interface);\n\tusb_deregister_dev(interface, &adu_class);\n\tmutex_lock(&adutux_mutex);\n\tusb_set_intfdata(interface, NULL);\n\tmutex_lock(&dev->mtx);\t/* not interruptible */\n\tdev->udev = NULL;\t/* poison */\n\tmutex_unlock(&dev->mtx);\n\t/* if the device is not opened, then we clean up right now */\n\tif (!dev->open_count)\n\t\tadu_delete(dev);\n\tmutex_unlock(&adutux_mutex);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 253
    },
    {
        "code": "static int nft_verdict_init(const struct nft_ctx *ctx, struct nft_data *data,\n\t\t\t    struct nft_data_desc *desc, const struct nlattr *nla)\n{\n\tu8 genmask = nft_genmask_next(ctx->net);\n\tstruct nlattr *tb[NFTA_VERDICT_MAX + 1];\n\tstruct nft_chain *chain;\n\tint err;\n\terr = nla_parse_nested_deprecated(tb, NFTA_VERDICT_MAX, nla,\n\t\t\t\t\t  nft_verdict_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\tif (!tb[NFTA_VERDICT_CODE])\n\t\treturn -EINVAL;\n\tdata->verdict.code = ntohl(nla_get_be32(tb[NFTA_VERDICT_CODE]));\n\tswitch (data->verdict.code) {\n\tdefault:\n\t\tswitch (data->verdict.code & NF_VERDICT_MASK) {\n\t\tcase NF_ACCEPT:\n\t\tcase NF_DROP:\n\t\tcase NF_QUEUE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tfallthrough;\n\tcase NFT_CONTINUE:\n\tcase NFT_BREAK:\n\tcase NFT_RETURN:\n\t\tbreak;\n\tcase NFT_JUMP:\n\tcase NFT_GOTO:\n\t\tif (tb[NFTA_VERDICT_CHAIN]) {\n\t\t\tchain = nft_chain_lookup(ctx->net, ctx->table,\n\t\t\t\t\t\t tb[NFTA_VERDICT_CHAIN],\n\t\t\t\t\t\t genmask);\n\t\t} else if (tb[NFTA_VERDICT_CHAIN_ID]) {\n\t\t\tchain = nft_chain_lookup_byid(ctx->net, ctx->table,\n\t\t\t\t\t\t      tb[NFTA_VERDICT_CHAIN_ID],\n\t\t\t\t\t\t      genmask);\n\t\t\tif (IS_ERR(chain))\n\t\t\t\treturn PTR_ERR(chain);\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (IS_ERR(chain))\n\t\t\treturn PTR_ERR(chain);\n\t\tif (nft_is_base_chain(chain))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (nft_chain_is_bound(chain))\n\t\t\treturn -EINVAL;\n\t\tif (desc->flags & NFT_DATA_DESC_SETELEM &&\n\t\t    chain->flags & NFT_CHAIN_BINDING)\n\t\t\treturn -EINVAL;\n\t\tif (!nft_use_inc(&chain->use))\n\t\t\treturn -EMFILE;\n\t\tdata->verdict.chain = chain;\n\t\tbreak;\n\t}\n\tdesc->len = sizeof(data->verdict);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 255
    },
    {
        "code": "static ssize_t\nvcs_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct vc_data *vc;\n\tstruct vcs_poll_data *poll;\n\tunsigned int read;\n\tssize_t ret;\n\tchar *con_buf;\n\tloff_t pos;\n\tbool viewed, attr, uni_mode;\n\tcon_buf = (char *) __get_free_page(GFP_KERNEL);\n\tif (!con_buf)\n\t\treturn -ENOMEM;\n\tpos = *ppos;\n\t/* Select the proper current console and verify\n\t * sanity of the situation under the console lock.\n\t */\n\tconsole_lock();\n\tuni_mode = use_unicode(inode);\n\tattr = use_attributes(inode);\n\tret = -EINVAL;\n\tif (pos < 0)\n\t\tgoto unlock_out;\n\t/* we enforce 32-bit alignment for pos and count in unicode mode */\n\tif (uni_mode && (pos | count) & 3)\n\t\tgoto unlock_out;\n\tpoll = file->private_data;\n\tif (count && poll)\n\t\tpoll->event = 0;\n\tread = 0;\n\tret = 0;\n\twhile (count) {\n\t\tunsigned int this_round, skip = 0;\n\t\tint size;\n\t\tret = -ENXIO;\n\t\tvc = vcs_vc(inode, &viewed);\n\t\tif (!vc)\n\t\t\tgoto unlock_out;\n\t\t/* Check whether we are above size each round,\n\t\t * as copy_to_user at the end of this loop\n\t\t * could sleep.\n\t\t */\n\t\tsize = vcs_size(vc, attr, uni_mode);\n\t\tif (size < 0) {\n\t\t\tif (read)\n\t\t\t\tbreak;\n\t\t\tret = size;\n\t\t\tgoto unlock_out;\n\t\t}\n\t\tif (pos >= size)\n\t\t\tbreak;\n\t\tif (count > size - pos)\n\t\t\tcount = size - pos;\n\t\tthis_round = count;\n\t\tif (this_round > CON_BUF_SIZE)\n\t\t\tthis_round = CON_BUF_SIZE;\n\t\t/* Perform the whole read into the local con_buf.\n\t\t * Then we can drop the console spinlock and safely\n\t\t * attempt to move it to userspace.\n\t\t */\n\t\tif (uni_mode) {\n\t\t\tret = vcs_read_buf_uni(vc, con_buf, pos, this_round,\n\t\t\t\t\tviewed);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t} else if (!attr) {\n\t\t\tvcs_read_buf_noattr(vc, con_buf, pos, this_round,\n\t\t\t\t\tviewed);\n\t\t} else {\n\t\t\tthis_round = vcs_read_buf(vc, con_buf, pos, this_round,\n\t\t\t\t\tviewed, &skip);\n\t\t}\n\t\t/* Finally, release the console semaphore while we push\n\t\t * all the data to userspace from our temporary buffer.\n\t\t *\n\t\t * AKPM: Even though it's a semaphore, we should drop it because\n\t\t * the pagefault handling code may want to call printk().\n\t\t */\n\t\tconsole_unlock();\n\t\tret = copy_to_user(buf, con_buf + skip, this_round);\n\t\tconsole_lock();\n\t\tif (ret) {\n\t\t\tread += this_round - ret;\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tbuf += this_round;\n\t\tpos += this_round;\n\t\tread += this_round;\n\t\tcount -= this_round;\n\t}\n\t*ppos += read;\n\tif (read)\n\t\tret = read;\nunlock_out:\n\tconsole_unlock();\n\tfree_page((unsigned long) con_buf);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 257
    },
    {
        "code": "long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\tunion ion_ioctl_arg data;\n\tdir = ion_ioctl_dir(cmd);\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\t/*\n\t * The copy_from_user is unconditional here for both read and write\n\t * to do the validate. If there is no write for the ioctl, the\n\t * buffer is cleared\n\t */\n\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\treturn -EFAULT;\n\tret = validate_ioctl_arg(cmd, &data);\n\tif (ret) {\n\t\tpr_warn_once(\"%s: ioctl validate failed\\n\", __func__);\n\t\treturn ret;\n\t}\n\tif (!(dir & _IOC_WRITE))\n\t\tmemset(&data, 0, sizeof(data));\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\t\thandle = __ion_alloc(client, data.allocation.len,\n\t\t\t\t     data.allocation.align,\n\t\t\t\t     data.allocation.heap_id_mask,\n\t\t\t\t     data.allocation.flags, true);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.allocation.handle = handle->id;\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tdata.fd.fd = ion_share_dma_buf_fd_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tcase ION_IOC_HEAP_QUERY:\n\t\tret = ion_query_heaps(client, &data.query);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle) {\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\t\tion_handle_put(cleanup_handle);\n\t\t\t}\n\t\t\treturn -EFAULT;\n\t\t}",
        "bug_line_number": [],
        "vul": 0,
        "id": 259
    },
    {
        "code": "static inline int __bpf_skb_change_head(struct sk_buff *skb, u32 head_room,\n\t\t\t\t\tu64 flags)\n{\n\tu32 max_len = BPF_SKB_MAX_LEN;\n\tu32 new_len = skb->len + head_room;\n\tint ret;\n\tif (unlikely(flags || (!skb_is_gso(skb) && new_len > max_len) ||\n\t\t     new_len < skb->len))\n\t\treturn -EINVAL;\n\tret = skb_cow(skb, head_room);\n\tif (likely(!ret)) {\n\t\t/* Idea for this helper is that we currently only\n\t\t * allow to expand on mac header. This means that\n\t\t * skb->protocol network header, etc, stay as is.\n\t\t * Compared to bpf_skb_change_tail(), we're more\n\t\t * flexible due to not needing to linearize or\n\t\t * reset GSO. Intention for this helper is to be\n\t\t * used by an L3 skb that needs to push mac header\n\t\t * for redirection into L2 device.\n\t\t */\n\t\t__skb_push(skb, head_room);\n\t\tmemset(skb->data, 0, head_room);\n\t\tskb_reset_mac_header(skb);\n\t}\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 261
    },
    {
        "code": "static void disk_seqf_stop(struct seq_file *seqf, void *v)\n{\n\tstruct class_dev_iter *iter = seqf->private;\n\t/* stop is called even after start failed :-( */\n\tif (iter) {\n\t\tclass_dev_iter_exit(iter);\n\t\tkfree(iter);\n\t\tseqf->private = NULL;\n\t}\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 263
    },
    {
        "code": "static int snd_seq_device_dev_free(struct snd_device *device)\n{\n\tstruct snd_seq_device *dev = device->device_data;\n\tcancel_autoload_drivers();\n\tput_device(&dev->dev);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 265
    },
    {
        "code": "int aa_audit_rule_init(u32 field, u32 op, char *rulestr, void **vrule)\n{\n\tstruct aa_audit_rule *rule;\n\tswitch (field) {\n\tcase AUDIT_SUBJ_ROLE:\n\t\tif (op != Audit_equal && op != Audit_not_equal)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\trule = kzalloc(sizeof(struct aa_audit_rule), GFP_KERNEL);\n\tif (!rule)\n\t\treturn -ENOMEM;\n\t/* Currently rules are treated as coming from the root ns */\n\trule->label = aa_label_parse(&root_ns->unconfined->label, rulestr,\n\t\t\t\t     GFP_KERNEL, true, false);\n\tif (IS_ERR(rule->label)) {\n\t\tint err = PTR_ERR(rule->label);\n\t\taa_audit_rule_free(rule);\n\t\treturn err;\n\t}\n\t*vrule = rule;\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 267
    },
    {
        "code": "static int xgene_hwmon_remove(struct platform_device *pdev)\n{\n\tstruct xgene_hwmon_dev *ctx = platform_get_drvdata(pdev);\n\tcancel_work_sync(&ctx->workq);\n\thwmon_device_unregister(ctx->hwmon_dev);\n\tkfifo_free(&ctx->async_msg_fifo);\n\tif (acpi_disabled)\n\t\tmbox_free_channel(ctx->mbox_chan);\n\telse\n\t\tpcc_mbox_free_channel(ctx->pcc_chan);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 269
    },
    {
        "code": "static void nft_dynset_activate(const struct nft_ctx *ctx,\n\t\t\t\tconst struct nft_expr *expr)\n{\n\tstruct nft_dynset *priv = nft_expr_priv(expr);\n\tnf_tables_activate_set(ctx, priv->set);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 271
    },
    {
        "code": "static void nft_lookup_activate(const struct nft_ctx *ctx,\n\t\t\t\tconst struct nft_expr *expr)\n{\n\tstruct nft_lookup *priv = nft_expr_priv(expr);\n\tnf_tables_activate_set(ctx, priv->set);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 273
    },
    {
        "code": "static struct l2cap_chan *l2cap_get_chan_by_scid(struct l2cap_conn *conn,\n\t\t\t\t\t\t u16 cid)\n{\n\tstruct l2cap_chan *c;\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_scid(conn, cid);\n\tif (c) {\n\t\t/* Only lock if chan reference is not 0 */\n\t\tc = l2cap_chan_hold_unless_zero(c);\n\t\tif (c)\n\t\t\tl2cap_chan_lock(c);\n\t}\n\tmutex_unlock(&conn->chan_lock);\n\treturn c;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 275
    },
    {
        "code": "static void l2cap_move_continue(struct l2cap_conn *conn, u16 icid, u16 result)\n{\n\tstruct l2cap_chan *chan;\n\tstruct hci_chan *hchan = NULL;\n\tchan = l2cap_get_chan_by_scid(conn, icid);\n\tif (!chan) {\n\t\tl2cap_send_move_chan_cfm_icid(conn, icid);\n\t\treturn;\n\t}\n\t__clear_chan_timer(chan);\n\tif (result == L2CAP_MR_PEND)\n\t\t__set_chan_timer(chan, L2CAP_MOVE_ERTX_TIMEOUT);\n\tswitch (chan->move_state) {\n\tcase L2CAP_MOVE_WAIT_LOGICAL_COMP:\n\t\t/* Move confirm will be sent when logical link\n\t\t * is complete.\n\t\t */\n\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_CFM;\n\t\tbreak;\n\tcase L2CAP_MOVE_WAIT_RSP_SUCCESS:\n\t\tif (result == L2CAP_MR_PEND) {\n\t\t\tbreak;\n\t\t} else if (test_bit(CONN_LOCAL_BUSY,\n\t\t\t\t    &chan->conn_state)) {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOCAL_BUSY;\n\t\t} else {\n\t\t\t/* Logical link is up or moving to BR/EDR,\n\t\t\t * proceed with move\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_CONFIRM_RSP;\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_CONFIRMED);\n\t\t}\n\t\tbreak;\n\tcase L2CAP_MOVE_WAIT_RSP:\n\t\t/* Moving to AMP */\n\t\tif (result == L2CAP_MR_SUCCESS) {\n\t\t\t/* Remote is ready, send confirm immediately\n\t\t\t * after logical link is ready\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_CFM;\n\t\t} else {\n\t\t\t/* Both logical link and move success\n\t\t\t * are required to confirm\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_COMP;\n\t\t}\n\t\t/* Placeholder - get hci_chan for logical link */\n\t\tif (!hchan) {\n\t\t\t/* Logical link not available */\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_UNCONFIRMED);\n\t\t\tbreak;\n\t\t}\n\t\t/* If the logical link is not yet connected, do not\n\t\t * send confirmation.\n\t\t */\n\t\tif (hchan->state != BT_CONNECTED)\n\t\t\tbreak;\n\t\t/* Logical link is already ready to go */\n\t\tchan->hs_hcon = hchan->conn;\n\t\tchan->hs_hcon->l2cap_data = chan->conn;\n\t\tif (result == L2CAP_MR_SUCCESS) {\n\t\t\t/* Can confirm now */\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_CONFIRMED);\n\t\t} else {\n\t\t\t/* Now only need move success\n\t\t\t * to confirm\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_RSP_SUCCESS;\n\t\t}\n\t\tl2cap_logical_cfm(chan, hchan, L2CAP_MR_SUCCESS);\n\t\tbreak;\n\tdefault:\n\t\t/* Any other amp move state means the move failed. */\n\t\tchan->move_id = chan->local_amp_id;\n\t\tl2cap_move_done(chan);\n\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_UNCONFIRMED);\n\t}\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 277
    },
    {
        "code": "static struct tc_u_knode *u32_init_knode(struct net *net, struct tcf_proto *tp,\n\t\t\t\t\t struct tc_u_knode *n)\n{\n\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);\n\tstruct tc_u32_sel *s = &n->sel;\n\tstruct tc_u_knode *new;\n\tnew = kzalloc(struct_size(new, sel.keys, s->nkeys), GFP_KERNEL);\n\tif (!new)\n\t\treturn NULL;\n\tRCU_INIT_POINTER(new->next, n->next);\n\tnew->handle = n->handle;\n\tRCU_INIT_POINTER(new->ht_up, n->ht_up);\n\tnew->ifindex = n->ifindex;\n\tnew->fshift = n->fshift;\n\tnew->flags = n->flags;\n\tRCU_INIT_POINTER(new->ht_down, ht);\n#ifdef CONFIG_CLS_U32_PERF\n\t/* Statistics may be incremented by readers during update\n\t * so we must keep them in tact. When the node is later destroyed\n\t * a special destroy call must be made to not free the pf memory.\n\t */\n\tnew->pf = n->pf;\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tnew->val = n->val;\n\tnew->mask = n->mask;\n\t/* Similarly success statistics must be moved as pointers */\n\tnew->pcpu_success = n->pcpu_success;\n#endif\n\tmemcpy(&new->sel, s, struct_size(s, keys, s->nkeys));\n\tif (tcf_exts_init(&new->exts, net, TCA_U32_ACT, TCA_U32_POLICE)) {\n\t\tkfree(new);\n\t\treturn NULL;\n\t}\n\t/* bump reference count as long as we hold pointer to structure */\n\tif (ht)\n\t\tht->refcnt++;\n\treturn new;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 279
    },
    {
        "code": "int saa7134_ts_fini(struct saa7134_dev *dev)\n{\n\tdel_timer_sync(&dev->ts_q.timeout);\n\tsaa7134_pgtable_free(dev->pci, &dev->ts_q.pt);\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 281
    },
    {
        "code": "static void __net_exit nf_tables_exit_net(struct net *net)\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(net);\n\tunsigned int gc_seq;\n\tmutex_lock(&nft_net->commit_mutex);\n\tgc_seq = nft_gc_seq_begin(nft_net);\n\tif (!list_empty(&nft_net->commit_list) ||\n\t    !list_empty(&nft_net->module_list))\n\t\t__nf_tables_abort(net, NFNL_ABORT_NONE);\n\t__nft_release_tables(net);\n\tnft_gc_seq_end(nft_net, gc_seq);\n\tmutex_unlock(&nft_net->commit_mutex);\n\tWARN_ON_ONCE(!list_empty(&nft_net->tables));\n\tWARN_ON_ONCE(!list_empty(&nft_net->module_list));\n\tWARN_ON_ONCE(!list_empty(&nft_net->notify_list));\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 283
    },
    {
        "code": "struct tpm_chip *tpm_chip_alloc(struct device *pdev,\n\t\t\t\tconst struct tpm_class_ops *ops)\n{\n\tstruct tpm_chip *chip;\n\tint rc;\n\tchip = kzalloc(sizeof(*chip), GFP_KERNEL);\n\tif (chip == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\tmutex_init(&chip->tpm_mutex);\n\tinit_rwsem(&chip->ops_sem);\n\tchip->ops = ops;\n\tmutex_lock(&idr_lock);\n\trc = idr_alloc(&dev_nums_idr, NULL, 0, TPM_NUM_DEVICES, GFP_KERNEL);\n\tmutex_unlock(&idr_lock);\n\tif (rc < 0) {\n\t\tdev_err(pdev, \"No available tpm device numbers\\n\");\n\t\tkfree(chip);\n\t\treturn ERR_PTR(rc);\n\t}\n\tchip->dev_num = rc;\n\tdevice_initialize(&chip->dev);\n\tchip->dev.class = tpm_class;\n\tchip->dev.class->shutdown_pre = tpm_class_shutdown;\n\tchip->dev.release = tpm_dev_release;\n\tchip->dev.parent = pdev;\n\tchip->dev.groups = chip->groups;\n\tif (chip->dev_num == 0)\n\t\tchip->dev.devt = MKDEV(MISC_MAJOR, TPM_MINOR);\n\telse\n\t\tchip->dev.devt = MKDEV(MAJOR(tpm_devt), chip->dev_num);\n\trc = dev_set_name(&chip->dev, \"tpm%d\", chip->dev_num);\n\tif (rc)\n\t\tgoto out;\n\tif (!pdev)\n\t\tchip->flags |= TPM_CHIP_FLAG_VIRTUAL;\n\tcdev_init(&chip->cdev, &tpm_fops);\n\tchip->cdev.owner = THIS_MODULE;\n\trc = tpm2_init_space(&chip->work_space, TPM2_SPACE_BUFFER_SIZE);\n\tif (rc) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\tchip->locality = -1;\n\treturn chip;\nout:\n\tput_device(&chip->dev);\n\treturn ERR_PTR(rc);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 285
    },
    {
        "code": "static struct cmd_obj *cmd_hdl_filter(struct _adapter *padapter,\n\t\t\t\t      struct cmd_obj *pcmd)\n{\n\tstruct cmd_obj *pcmd_r;\n\tif (!pcmd)\n\t\treturn pcmd;\n\tpcmd_r = NULL;\n\tswitch (pcmd->cmdcode) {\n\tcase GEN_CMD_CODE(_Read_BBREG):\n\t\tread_bbreg_hdl(padapter, (u8 *)pcmd);\n\t\tbreak;\n\tcase GEN_CMD_CODE(_Write_BBREG):\n\t\twrite_bbreg_hdl(padapter, (u8 *)pcmd);\n\t\tbreak;\n\tcase GEN_CMD_CODE(_Read_RFREG):\n\t\tread_rfreg_hdl(padapter, (u8 *)pcmd);\n\t\tbreak;\n\tcase GEN_CMD_CODE(_Write_RFREG):\n\t\twrite_rfreg_hdl(padapter, (u8 *)pcmd);\n\t\tbreak;\n\tcase GEN_CMD_CODE(_SetUsbSuspend):\n\t\tsys_suspend_hdl(padapter, (u8 *)pcmd);\n\t\tbreak;\n\tcase GEN_CMD_CODE(_JoinBss):\n\t\tr8712_joinbss_reset(padapter);\n\t\t/* Before set JoinBss_CMD to FW, driver must ensure FW is in\n\t\t * PS_MODE_ACTIVE. Directly write rpwm to radio on and assign\n\t\t * new pwr_mode to Driver, instead of use workitem to change\n\t\t * state.\n\t\t */\n\t\tif (padapter->pwrctrlpriv.pwr_mode > PS_MODE_ACTIVE) {\n\t\t\tpadapter->pwrctrlpriv.pwr_mode = PS_MODE_ACTIVE;\n\t\t\tmutex_lock(&padapter->pwrctrlpriv.mutex_lock);\n\t\t\tr8712_set_rpwm(padapter, PS_STATE_S4);\n\t\t\tmutex_unlock(&padapter->pwrctrlpriv.mutex_lock);\n\t\t}\n\t\tpcmd_r = pcmd;\n\t\tbreak;\n\tcase _DRV_INT_CMD_:\n\t\tr871x_internal_cmd_hdl(padapter, pcmd->parmbuf);\n\t\tr8712_free_cmd_obj(pcmd);\n\t\tpcmd_r = NULL;\n\t\tbreak;\n\tdefault:\n\t\tpcmd_r = pcmd;\n\t\tbreak;\n\t}\n\treturn pcmd_r; /* if returning pcmd_r == NULL, pcmd must be free. */\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 287
    },
    {
        "code": "static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n\t\t     struct file *tfile, int fd, int full_check)\n{\n\tint error, pwake = 0;\n\t__poll_t revents;\n\tlong user_watches;\n\tstruct epitem *epi;\n\tstruct ep_pqueue epq;\n\tlockdep_assert_irqs_enabled();\n\tuser_watches = atomic_long_read(&ep->user->epoll_watches);\n\tif (unlikely(user_watches >= max_user_watches))\n\t\treturn -ENOSPC;\n\tif (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\t/* Item initialization follow here ... */\n\tINIT_LIST_HEAD(&epi->rdllink);\n\tINIT_LIST_HEAD(&epi->fllink);\n\tINIT_LIST_HEAD(&epi->pwqlist);\n\tepi->ep = ep;\n\tep_set_ffd(&epi->ffd, tfile, fd);\n\tepi->event = *event;\n\tepi->nwait = 0;\n\tepi->next = EP_UNACTIVE_PTR;\n\tif (epi->event.events & EPOLLWAKEUP) {\n\t\terror = ep_create_wakeup_source(epi);\n\t\tif (error)\n\t\t\tgoto error_create_wakeup_source;\n\t} else {\n\t\tRCU_INIT_POINTER(epi->ws, NULL);\n\t}\n\t/* Add the current item to the list of active epoll hook for this file */\n\tspin_lock(&tfile->f_lock);\n\tlist_add_tail_rcu(&epi->fllink, &tfile->f_ep_links);\n\tspin_unlock(&tfile->f_lock);\n\t/*\n\t * Add the current item to the RB tree. All RB tree operations are\n\t * protected by \"mtx\", and ep_insert() is called with \"mtx\" held.\n\t */\n\tep_rbtree_insert(ep, epi);\n\t/* now check if we've created too many backpaths */\n\terror = -EINVAL;\n\tif (full_check && reverse_path_check())\n\t\tgoto error_remove_epi;\n\t/* Initialize the poll table using the queue callback */\n\tepq.epi = epi;\n\tinit_poll_funcptr(&epq.pt, ep_ptable_queue_proc);\n\t/*\n\t * Attach the item to the poll hooks and get current event bits.\n\t * We can safely use the file* here because its usage count has\n\t * been increased by the caller of this function. Note that after\n\t * this operation completes, the poll callback can start hitting\n\t * the new item.\n\t */\n\trevents = ep_item_poll(epi, &epq.pt, 1);\n\t/*\n\t * We have to check if something went wrong during the poll wait queue\n\t * install process. Namely an allocation for a wait queue failed due\n\t * high memory pressure.\n\t */\n\terror = -ENOMEM;\n\tif (epi->nwait < 0)\n\t\tgoto error_unregister;\n\t/* We have to drop the new item inside our item list to keep track of it */\n\twrite_lock_irq(&ep->lock);\n\t/* record NAPI ID of new item if present */\n\tep_set_busy_poll_napi_id(epi);\n\t/* If the file is already \"ready\" we drop it inside the ready list */\n\tif (revents && !ep_is_linked(epi)) {\n\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n\t\tep_pm_stay_awake(epi);\n\t\t/* Notify waiting tasks that events are available */\n\t\tif (waitqueue_active(&ep->wq))\n\t\t\twake_up(&ep->wq);\n\t\tif (waitqueue_active(&ep->poll_wait))\n\t\t\tpwake++;\n\t}\n\twrite_unlock_irq(&ep->lock);\n\tatomic_long_inc(&ep->user->epoll_watches);\n\t/* We have to call this outside the lock */\n\tif (pwake)\n\t\tep_poll_safewake(ep, NULL);\n\treturn 0;\nerror_unregister:\n\tep_unregister_pollwait(ep, epi);\nerror_remove_epi:\n\tspin_lock(&tfile->f_lock);\n\tlist_del_rcu(&epi->fllink);\n\tspin_unlock(&tfile->f_lock);\n\trb_erase_cached(&epi->rbn, &ep->rbr);\n\t/*\n\t * We need to do this because an event could have been arrived on some\n\t * allocated wait queue. Note that we don't care about the ep->ovflist\n\t * list, since that is used/cleaned only inside a section bound by \"mtx\".\n\t * And ep_insert() is called with \"mtx\" held.\n\t */\n\twrite_lock_irq(&ep->lock);\n\tif (ep_is_linked(epi))\n\t\tlist_del_init(&epi->rdllink);\n\twrite_unlock_irq(&ep->lock);",
        "bug_line_number": [],
        "vul": 0,
        "id": 289
    },
    {
        "code": "void snd_usb_mixer_disconnect(struct usb_mixer_interface *mixer)\n{\n\tif (mixer->disconnected)\n\t\treturn;\n\tif (mixer->urb)\n\t\tusb_kill_urb(mixer->urb);\n\tif (mixer->rc_urb)\n\t\tusb_kill_urb(mixer->rc_urb);\n\tmixer->disconnected = true;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 291
    },
    {
        "code": "int xenvif_connect_data(struct xenvif_queue *queue,\n\t\t\tunsigned long tx_ring_ref,\n\t\t\tunsigned long rx_ring_ref,\n\t\t\tunsigned int tx_evtchn,\n\t\t\tunsigned int rx_evtchn)\n{\n\tstruct xenbus_device *dev = xenvif_to_xenbus_device(queue->vif);\n\tstruct task_struct *task;\n\tint err;\n\tBUG_ON(queue->tx_irq);\n\tBUG_ON(queue->task);\n\tBUG_ON(queue->dealloc_task);\n\terr = xenvif_map_frontend_data_rings(queue, tx_ring_ref,\n\t\t\t\t\t     rx_ring_ref);\n\tif (err < 0)\n\t\tgoto err;\n\tinit_waitqueue_head(&queue->wq);\n\tinit_waitqueue_head(&queue->dealloc_wq);\n\tatomic_set(&queue->inflight_packets, 0);\n\tnetif_napi_add(queue->vif->dev, &queue->napi, xenvif_poll,\n\t\t\tXENVIF_NAPI_WEIGHT);\n\tqueue->stalled = true;\n\ttask = kthread_run(xenvif_kthread_guest_rx, queue,\n\t\t\t   \"%s-guest-rx\", queue->name);\n\tif (IS_ERR(task))\n\t\tgoto kthread_err;\n\tqueue->task = task;\n\t/*\n\t * Take a reference to the task in order to prevent it from being freed\n\t * if the thread function returns before kthread_stop is called.\n\t */\n\tget_task_struct(task);\n\ttask = kthread_run(xenvif_dealloc_kthread, queue,\n\t\t\t   \"%s-dealloc\", queue->name);\n\tif (IS_ERR(task))\n\t\tgoto kthread_err;\n\tqueue->dealloc_task = task;\n\tif (tx_evtchn == rx_evtchn) {\n\t\t/* feature-split-event-channels == 0 */\n\t\terr = bind_interdomain_evtchn_to_irqhandler_lateeoi(\n\t\t\tdev, tx_evtchn, xenvif_interrupt, 0,\n\t\t\tqueue->name, queue);\n\t\tif (err < 0)\n\t\t\tgoto err;\n\t\tqueue->tx_irq = queue->rx_irq = err;\n\t\tdisable_irq(queue->tx_irq);\n\t} else {\n\t\t/* feature-split-event-channels == 1 */\n\t\tsnprintf(queue->tx_irq_name, sizeof(queue->tx_irq_name),\n\t\t\t \"%s-tx\", queue->name);\n\t\terr = bind_interdomain_evtchn_to_irqhandler_lateeoi(\n\t\t\tdev, tx_evtchn, xenvif_tx_interrupt, 0,\n\t\t\tqueue->tx_irq_name, queue);\n\t\tif (err < 0)\n\t\t\tgoto err;\n\t\tqueue->tx_irq = err;\n\t\tdisable_irq(queue->tx_irq);\n\t\tsnprintf(queue->rx_irq_name, sizeof(queue->rx_irq_name),\n\t\t\t \"%s-rx\", queue->name);\n\t\terr = bind_interdomain_evtchn_to_irqhandler_lateeoi(\n\t\t\tdev, rx_evtchn, xenvif_rx_interrupt, 0,\n\t\t\tqueue->rx_irq_name, queue);\n\t\tif (err < 0)\n\t\t\tgoto err;\n\t\tqueue->rx_irq = err;\n\t\tdisable_irq(queue->rx_irq);\n\t}\n\treturn 0;\nkthread_err:\n\tpr_warn(\"Could not allocate kthread for %s\\n\", queue->name);\n\terr = PTR_ERR(task);\nerr:\n\txenvif_disconnect_queue(queue);\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 293
    },
    {
        "code": "static int __init l2tp_eth_init(void)\n{\n\tint err = 0;\n\terr = l2tp_nl_register_ops(L2TP_PWTYPE_ETH, &l2tp_eth_nl_cmd_ops);\n\tif (err)\n\t\tgoto err;\n\tpr_info(\"L2TP ethernet pseudowire support (L2TPv3)\\n\");\n\treturn 0;\nerr:\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 295
    },
    {
        "code": "static int binder_proc_transaction(struct binder_transaction *t,\n\t\t\t\t    struct binder_proc *proc,\n\t\t\t\t    struct binder_thread *thread)\n{\n\tstruct binder_node *node = t->buffer->target_node;\n\tbool oneway = !!(t->flags & TF_ONE_WAY);\n\tbool pending_async = false;\n\tstruct binder_transaction *t_outdated = NULL;\n\tbool frozen = false;\n\tBUG_ON(!node);\n\tbinder_node_lock(node);\n\tif (oneway) {\n\t\tBUG_ON(thread);\n\t\tif (node->has_async_transaction)\n\t\t\tpending_async = true;\n\t\telse\n\t\t\tnode->has_async_transaction = true;\n\t}\n\tbinder_inner_proc_lock(proc);\n\tif (proc->is_frozen) {\n\t\tfrozen = true;\n\t\tproc->sync_recv |= !oneway;\n\t\tproc->async_recv |= oneway;\n\t}\n\tif ((frozen && !oneway) || proc->is_dead ||\n\t\t\t(thread && thread->is_dead)) {\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_node_unlock(node);\n\t\treturn frozen ? BR_FROZEN_REPLY : BR_DEAD_REPLY;\n\t}\n\tif (!thread && !pending_async)\n\t\tthread = binder_select_thread_ilocked(proc);\n\tif (thread) {\n\t\tbinder_enqueue_thread_work_ilocked(thread, &t->work);\n\t} else if (!pending_async) {\n\t\tbinder_enqueue_work_ilocked(&t->work, &proc->todo);\n\t} else {\n\t\tif ((t->flags & TF_UPDATE_TXN) && frozen) {\n\t\t\tt_outdated = binder_find_outdated_transaction_ilocked(t,\n\t\t\t\t\t\t\t\t\t      &node->async_todo);\n\t\t\tif (t_outdated) {\n\t\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t\t     \"txn %d supersedes %d\\n\",\n\t\t\t\t\t     t->debug_id, t_outdated->debug_id);\n\t\t\t\tlist_del_init(&t_outdated->work.entry);\n\t\t\t\tproc->outstanding_txns--;\n\t\t\t}\n\t\t}\n\t\tbinder_enqueue_work_ilocked(&t->work, &node->async_todo);\n\t}\n\tif (!pending_async)\n\t\tbinder_wakeup_thread_ilocked(proc, thread, !oneway /* sync */);\n\tproc->outstanding_txns++;\n\tbinder_inner_proc_unlock(proc);\n\tbinder_node_unlock(node);\n\t/*\n\t * To reduce potential contention, free the outdated transaction and\n\t * buffer after releasing the locks.\n\t */\n\tif (t_outdated) {\n\t\tstruct binder_buffer *buffer = t_outdated->buffer;\n\t\tt_outdated->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t\ttrace_binder_transaction_update_buffer_release(buffer);\n\t\tbinder_release_entire_buffer(proc, NULL, buffer, false);\n\t\tbinder_alloc_free_buf(&proc->alloc, buffer);\n\t\tkfree(t_outdated);\n\t\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n\t}\n\tif (oneway && frozen)\n\t\treturn BR_TRANSACTION_PENDING_FROZEN;\n\treturn 0;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 297
    },
    {
        "code": "\t\tif (!is_dir)\n\t\t\tgoto next_attr;\n\t\tni->ni_flags |= NI_FLAG_DIR;\n\t\terr = indx_init(&ni->dir, sbi, attr, INDEX_MUTEX_I30);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tmode = sb->s_root\n\t\t\t       ? (S_IFDIR | (0777 & sbi->options->fs_dmask_inv))\n\t\t\t       : (S_IFDIR | 0777);\n\t\tgoto next_attr;\n\tcase ATTR_ALLOC:\n\t\tif (!is_root || attr->name_len != ARRAY_SIZE(I30_NAME) ||\n\t\t    memcmp(attr_name(attr), I30_NAME, sizeof(I30_NAME)))\n\t\t\tgoto next_attr;\n\t\tinode->i_size = le64_to_cpu(attr->nres.data_size);\n\t\tni->i_valid = le64_to_cpu(attr->nres.valid_size);\n\t\tinode_set_bytes(inode, le64_to_cpu(attr->nres.alloc_size));\n\t\trun = &ni->dir.alloc_run;\n\t\tbreak;\n\tcase ATTR_BITMAP:\n\t\tif (ino == MFT_REC_MFT) {\n\t\t\tif (!attr->non_res)\n\t\t\t\tgoto out;\n#ifndef CONFIG_NTFS3_64BIT_CLUSTER\n\t\t\t/* 0x20000000 = 2^32 / 8 */\n\t\t\tif (le64_to_cpu(attr->nres.alloc_size) >= 0x20000000)\n\t\t\t\tgoto out;\n#endif\n\t\t\trun = &sbi->mft.bitmap.run;\n\t\t\tbreak;\n\t\t} else if (is_dir && attr->name_len == ARRAY_SIZE(I30_NAME) &&\n\t\t\t   !memcmp(attr_name(attr), I30_NAME,\n\t\t\t\t   sizeof(I30_NAME)) &&\n\t\t\t   attr->non_res) {\n\t\t\trun = &ni->dir.bitmap_run;\n\t\t\tbreak;\n\t\t}\n\t\tgoto next_attr;\n\tcase ATTR_REPARSE:\n\t\tif (attr->name_len)\n\t\t\tgoto next_attr;\n\t\trp_fa = ni_parse_reparse(ni, attr, &rp);\n\t\tswitch (rp_fa) {\n\t\tcase REPARSE_LINK:\n\t\t\t/*\n\t\t\t * Normal symlink.\n\t\t\t * Assume one unicode symbol == one utf8.\n\t\t\t */\n\t\t\tinode->i_size = le16_to_cpu(rp.SymbolicLinkReparseBuffer\n\t\t\t\t\t\t\t    .PrintNameLength) /\n\t\t\t\t\tsizeof(u16);\n\t\t\tni->i_valid = inode->i_size;\n\t\t\t/* Clear directory bit. */\n\t\t\tif (ni->ni_flags & NI_FLAG_DIR) {\n\t\t\t\tindx_clear(&ni->dir);\n\t\t\t\tmemset(&ni->dir, 0, sizeof(ni->dir));\n\t\t\t\tni->ni_flags &= ~NI_FLAG_DIR;\n\t\t\t} else {\n\t\t\t\trun_close(&ni->file.run);\n\t\t\t}\n\t\t\tmode = S_IFLNK | 0777;\n\t\t\tis_dir = false;\n\t\t\tif (attr->non_res) {\n\t\t\t\trun = &ni->file.run;\n\t\t\t\tgoto attr_unpack_run; // Double break.\n\t\t\t}\n\t\t\tbreak;\n\t\tcase REPARSE_COMPRESSED:\n\t\t\tbreak;\n\t\tcase REPARSE_DEDUPLICATED:\n\t\t\tbreak;\n\t\t}\n\t\tgoto next_attr;\n\tcase ATTR_EA_INFO:\n\t\tif (!attr->name_len &&\n\t\t    resident_data_ex(attr, sizeof(struct EA_INFO))) {\n\t\t\tni->ni_flags |= NI_FLAG_EA;\n\t\t\t/*\n\t\t\t * ntfs_get_wsl_perm updates inode->i_uid, inode->i_gid, inode->i_mode\n\t\t\t */\n\t\t\tinode->i_mode = mode;\n\t\t\tntfs_get_wsl_perm(inode);\n\t\t\tmode = inode->i_mode;\n\t\t}\n\t\tgoto next_attr;\n\tdefault:\n\t\tgoto next_attr;\n\t}\nattr_unpack_run:\n\troff = le16_to_cpu(attr->nres.run_off);\n\tif (roff > asize) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tt64 = le64_to_cpu(attr->nres.svcn);\n\t/* offset to packed runs is out-of-bounds */\n\tif (roff > asize) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}",
        "bug_line_number": [],
        "vul": 0,
        "id": 299
    },
    {
        "code": "int inode_init_always(struct super_block *sb, struct inode *inode)\n{\n\tstatic const struct inode_operations empty_iops;\n\tstatic const struct file_operations no_open_fops = {.open = no_open};\n\tstruct address_space *const mapping = &inode->i_data;\n\tinode->i_sb = sb;\n\tinode->i_blkbits = sb->s_blocksize_bits;\n\tinode->i_flags = 0;\n\tatomic64_set(&inode->i_sequence, 0);\n\tatomic_set(&inode->i_count, 1);\n\tinode->i_op = &empty_iops;\n\tinode->i_fop = &no_open_fops;\n\tinode->__i_nlink = 1;\n\tinode->i_opflags = 0;\n\tif (sb->s_xattr)\n\t\tinode->i_opflags |= IOP_XATTR;\n\ti_uid_write(inode, 0);\n\ti_gid_write(inode, 0);\n\tatomic_set(&inode->i_writecount, 0);\n\tinode->i_size = 0;\n\tinode->i_write_hint = WRITE_LIFE_NOT_SET;\n\tinode->i_blocks = 0;\n\tinode->i_bytes = 0;\n\tinode->i_generation = 0;\n\tinode->i_pipe = NULL;\n\tinode->i_bdev = NULL;\n\tinode->i_cdev = NULL;\n\tinode->i_link = NULL;\n\tinode->i_dir_seq = 0;\n\tinode->i_rdev = 0;\n\tinode->dirtied_when = 0;\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tinode->i_wb_frn_winner = 0;\n\tinode->i_wb_frn_avg_time = 0;\n\tinode->i_wb_frn_history = 0;\n#endif\n\tif (security_inode_alloc(inode))\n\t\tgoto out;\n\tspin_lock_init(&inode->i_lock);\n\tlockdep_set_class(&inode->i_lock, &sb->s_type->i_lock_key);\n\tinit_rwsem(&inode->i_rwsem);\n\tlockdep_set_class(&inode->i_rwsem, &sb->s_type->i_mutex_key);\n\tatomic_set(&inode->i_dio_count, 0);\n\tmapping->a_ops = &empty_aops;\n\tmapping->host = inode;\n\tmapping->flags = 0;\n\tmapping->wb_err = 0;\n\tatomic_set(&mapping->i_mmap_writable, 0);\n#ifdef CONFIG_READ_ONLY_THP_FOR_FS\n\tatomic_set(&mapping->nr_thps, 0);\n#endif\n\tmapping_set_gfp_mask(mapping, GFP_HIGHUSER_MOVABLE);\n\tmapping->private_data = NULL;\n\tmapping->writeback_index = 0;\n\tinode->i_private = NULL;\n\tinode->i_mapping = mapping;\n\tINIT_HLIST_HEAD(&inode->i_dentry);\t/* buggered by rcu freeing */\n#ifdef CONFIG_FS_POSIX_ACL\n\tinode->i_acl = inode->i_default_acl = ACL_NOT_CACHED;\n#endif\n#ifdef CONFIG_FSNOTIFY\n\tinode->i_fsnotify_mask = 0;\n#endif\n\tinode->i_flctx = NULL;\n\tthis_cpu_inc(nr_inodes);\n\treturn 0;\nout:\n\treturn -ENOMEM;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 301
    },
    {
        "code": "static int selinux_msg_queue_msgrcv(struct kern_ipc_perm *msq, struct msg_msg *msg,\n\t\t\t\t    struct task_struct *target,\n\t\t\t\t    long type, int mode)\n{\n\tstruct ipc_security_struct *isec;\n\tstruct msg_security_struct *msec;\n\tstruct common_audit_data ad;\n\tu32 sid = task_sid_obj(target);\n\tint rc;\n\tisec = selinux_ipc(msq);\n\tmsec = selinux_msg_msg(msg);\n\tad.type = LSM_AUDIT_DATA_IPC;\n\tad.u.ipc_id = msq->key;\n\trc = avc_has_perm(&selinux_state,\n\t\t\t  sid, isec->sid,\n\t\t\t  SECCLASS_MSGQ, MSGQ__READ, &ad);\n\tif (!rc)\n\t\trc = avc_has_perm(&selinux_state,\n\t\t\t\t  sid, msec->sid,\n\t\t\t\t  SECCLASS_MSG, MSG__RECEIVE, &ad);\n\treturn rc;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 303
    },
    {
        "code": "static int\ntcpmss_mangle_packet(struct sk_buff *skb,\n\t\t     const struct xt_action_param *par,\n\t\t     unsigned int family,\n\t\t     unsigned int tcphoff,\n\t\t     unsigned int minlen)\n{\n\tconst struct xt_tcpmss_info *info = par->targinfo;\n\tstruct tcphdr *tcph;\n\tint len, tcp_hdrlen;\n\tunsigned int i;\n\t__be16 oldval;\n\tu16 newmss;\n\tu8 *opt;\n\t/* This is a fragment, no TCP header is available */\n\tif (par->fragoff != 0)\n\t\treturn 0;\n\tif (!skb_make_writable(skb, skb->len))\n\t\treturn -1;\n\tlen = skb->len - tcphoff;\n\tif (len < (int)sizeof(struct tcphdr))\n\t\treturn -1;\n\ttcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);\n\ttcp_hdrlen = tcph->doff * 4;\n\tif (len < tcp_hdrlen || tcp_hdrlen < sizeof(struct tcphdr))\n\t\treturn -1;\n\tif (info->mss == XT_TCPMSS_CLAMP_PMTU) {\n\t\tstruct net *net = xt_net(par);\n\t\tunsigned int in_mtu = tcpmss_reverse_mtu(net, skb, family);\n\t\tunsigned int min_mtu = min(dst_mtu(skb_dst(skb)), in_mtu);\n\t\tif (min_mtu <= minlen) {\n\t\t\tnet_err_ratelimited(\"unknown or invalid path-MTU (%u)\\n\",\n\t\t\t\t\t    min_mtu);\n\t\t\treturn -1;\n\t\t}\n\t\tnewmss = min_mtu - minlen;\n\t} else\n\t\tnewmss = info->mss;\n\topt = (u_int8_t *)tcph;\n\tfor (i = sizeof(struct tcphdr); i <= tcp_hdrlen - TCPOLEN_MSS; i += optlen(opt, i)) {\n\t\tif (opt[i] == TCPOPT_MSS && opt[i+1] == TCPOLEN_MSS) {\n\t\t\tu_int16_t oldmss;\n\t\t\toldmss = (opt[i+2] << 8) | opt[i+3];\n\t\t\t/* Never increase MSS, even when setting it, as\n\t\t\t * doing so results in problems for hosts that rely\n\t\t\t * on MSS being set correctly.\n\t\t\t */\n\t\t\tif (oldmss <= newmss)\n\t\t\t\treturn 0;\n\t\t\topt[i+2] = (newmss & 0xff00) >> 8;\n\t\t\topt[i+3] = newmss & 0x00ff;\n\t\t\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t\t\t htons(oldmss), htons(newmss),\n\t\t\t\t\t\t false);\n\t\t\treturn 0;\n\t\t}\n\t}\n\t/* There is data after the header so the option can't be added\n\t * without moving it, and doing so may make the SYN packet\n\t * itself too large. Accept the packet unmodified instead.\n\t */\n\tif (len > tcp_hdrlen)\n\t\treturn 0;\n\t/* tcph->doff has 4 bits, do not wrap it to 0 */\n\tif (tcp_hdrlen >= 15 * 4)\n\t\treturn 0;\n\t/*\n\t * MSS Option not found ?! add it..\n\t */\n\tif (skb_tailroom(skb) < TCPOLEN_MSS) {\n\t\tif (pskb_expand_head(skb, 0,\n\t\t\t\t     TCPOLEN_MSS - skb_tailroom(skb),\n\t\t\t\t     GFP_ATOMIC))\n\t\t\treturn -1;\n\t\ttcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);\n\t}\n\tskb_put(skb, TCPOLEN_MSS);\n\t/*\n\t * IPv4: RFC 1122 states \"If an MSS option is not received at\n\t * connection setup, TCP MUST assume a default send MSS of 536\".\n\t * IPv6: RFC 2460 states IPv6 has a minimum MTU of 1280 and a minimum\n\t * length IPv6 header of 60, ergo the default MSS value is 1220\n\t * Since no MSS was provided, we must use the default values\n\t */\n\tif (xt_family(par) == NFPROTO_IPV4)\n\t\tnewmss = min(newmss, (u16)536);\n\telse\n\t\tnewmss = min(newmss, (u16)1220);\n\topt = (u_int8_t *)tcph + sizeof(struct tcphdr);\n\tmemmove(opt + TCPOLEN_MSS, opt, len - sizeof(struct tcphdr));\n\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t htons(len), htons(len + TCPOLEN_MSS), true);\n\topt[0] = TCPOPT_MSS;\n\topt[1] = TCPOLEN_MSS;\n\topt[2] = (newmss & 0xff00) >> 8;\n\topt[3] = newmss & 0x00ff;\n\tinet_proto_csum_replace4(&tcph->check, skb, 0, *((__be32 *)opt), false);\n\toldval = ((__be16 *)tcph)[6];\n\ttcph->doff += TCPOLEN_MSS/4;\n\tinet_proto_csum_replace2(&tcph->check, skb,\n\t\t\t\t oldval, ((__be16 *)tcph)[6], false);\n\treturn TCPOLEN_MSS;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 305
    },
    {
        "code": "static int\nprinter_open(struct inode *inode, struct file *fd)\n{\n\tstruct printer_dev\t*dev;\n\tunsigned long\t\tflags;\n\tint\t\t\tret = -EBUSY;\n\tdev = container_of(inode->i_cdev, struct printer_dev, printer_cdev);\n\tspin_lock_irqsave(&dev->lock, flags);\n\tif (dev->interface < 0) {\n\t\tspin_unlock_irqrestore(&dev->lock, flags);\n\t\treturn -ENODEV;\n\t}\n\tif (!dev->printer_cdev_open) {\n\t\tdev->printer_cdev_open = 1;\n\t\tfd->private_data = dev;\n\t\tret = 0;\n\t\t/* Change the printer status to show that it's on-line. */\n\t\tdev->printer_status |= PRINTER_SELECTED;\n\t}\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\tkref_get(&dev->kref);\n\tDBG(dev, \"printer_open returned %x\\n\", ret);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 307
    },
    {
        "code": "static long btrfs_ioctl_qgroup_assign(struct file *file, void __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_qgroup_assign_args *sa;\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\tint err;\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\tsa = memdup_user(arg, sizeof(*sa));\n\tif (IS_ERR(sa)) {\n\t\tret = PTR_ERR(sa);\n\t\tgoto drop_write;\n\t}\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto out;\n\t}\n\tif (sa->assign) {\n\t\tret = btrfs_add_qgroup_relation(trans, sa->src, sa->dst);\n\t} else {\n\t\tret = btrfs_del_qgroup_relation(trans, sa->src, sa->dst);\n\t}\n\t/* update qgroup status and info */\n\tmutex_lock(&fs_info->qgroup_ioctl_lock);\n\terr = btrfs_run_qgroups(trans);\n\tmutex_unlock(&fs_info->qgroup_ioctl_lock);\n\tif (err < 0)\n\t\tbtrfs_handle_fs_error(fs_info, err,\n\t\t\t\t      \"failed to update qgroup status and info\");\n\terr = btrfs_end_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\nout:\n\tkfree(sa);\ndrop_write:\n\tmnt_drop_write_file(file);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 309
    },
    {
        "code": "netdev_tx_t hns_nic_net_xmit_hw(struct net_device *ndev,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct hns_nic_ring_data *ring_data)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct device *dev = ring_to_dev(ring);\n\tstruct netdev_queue *dev_queue;\n\tstruct skb_frag_struct *frag;\n\tint buf_num;\n\tint seg_num;\n\tdma_addr_t dma;\n\tint size, next_to_use;\n\tint i;\n\tswitch (priv->ops.maybe_stop_tx(&skb, &buf_num, ring)) {\n\tcase -EBUSY:\n\t\tring->stats.tx_busy++;\n\t\tgoto out_net_tx_busy;\n\tcase -ENOMEM:\n\t\tring->stats.sw_err_cnt++;\n\t\tnetdev_err(ndev, \"no memory to xmit!\\n\");\n\t\tgoto out_err_tx_ok;\n\tdefault:\n\t\tbreak;\n\t}\n\t/* no. of segments (plus a header) */\n\tseg_num = skb_shinfo(skb)->nr_frags + 1;\n\tnext_to_use = ring->next_to_use;\n\t/* fill the first part */\n\tsize = skb_headlen(skb);\n\tdma = dma_map_single(dev, skb->data, size, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, dma)) {\n\t\tnetdev_err(ndev, \"TX head DMA map failed\\n\");\n\t\tring->stats.sw_err_cnt++;\n\t\tgoto out_err_tx_ok;\n\t}\n\tpriv->ops.fill_desc(ring, skb, size, dma, seg_num == 1 ? 1 : 0,\n\t\t\t    buf_num, DESC_TYPE_SKB, ndev->mtu);\n\t/* fill the fragments */\n\tfor (i = 1; i < seg_num; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i - 1];\n\t\tsize = skb_frag_size(frag);\n\t\tdma = skb_frag_dma_map(dev, frag, 0, size, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, dma)) {\n\t\t\tnetdev_err(ndev, \"TX frag(%d) DMA map failed\\n\", i);\n\t\t\tring->stats.sw_err_cnt++;\n\t\t\tgoto out_map_frag_fail;\n\t\t}\n\t\tpriv->ops.fill_desc(ring, skb_frag_page(frag), size, dma,\n\t\t\t\t    seg_num - 1 == i ? 1 : 0, buf_num,\n\t\t\t\t    DESC_TYPE_PAGE, ndev->mtu);\n\t}\n\t/*complete translate all packets*/\n\tdev_queue = netdev_get_tx_queue(ndev, skb->queue_mapping);\n\tnetdev_tx_sent_queue(dev_queue, skb->len);\n\tnetif_trans_update(ndev);\n\tndev->stats.tx_bytes += skb->len;\n\tndev->stats.tx_packets++;\n\twmb(); /* commit all data before submit */\n\tassert(skb->queue_mapping < priv->ae_handle->q_num);\n\thnae_queue_xmit(priv->ae_handle->qs[skb->queue_mapping], buf_num);\n\tring->stats.tx_pkts++;\n\tring->stats.tx_bytes += skb->len;\n\treturn NETDEV_TX_OK;\nout_map_frag_fail:\n\twhile (ring->next_to_use != next_to_use) {\n\t\tunfill_desc(ring);\n\t\tif (ring->next_to_use != next_to_use)\n\t\t\tdma_unmap_page(dev,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].dma,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].length,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_single(dev,\n\t\t\t\t\t ring->desc_cb[next_to_use].dma,\n\t\t\t\t\t ring->desc_cb[next_to_use].length,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t}\nout_err_tx_ok:\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\nout_net_tx_busy:\n\tnetif_stop_subqueue(ndev, skb->queue_mapping);\n\t/* Herbert's original patch had:\n\t *  smp_mb__after_netif_stop_queue();\n\t * but since that doesn't exist yet, just open code it.\n\t */\n\tsmp_mb();\n\treturn NETDEV_TX_BUSY;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 311
    },
    {
        "code": "static int tiocspgrp(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\tstruct pid *pgrp;\n\tpid_t pgrp_nr;\n\tint retval = tty_check_change(real_tty);\n\tif (retval == -EIO)\n\t\treturn -ENOTTY;\n\tif (retval)\n\t\treturn retval;\n\tif (get_user(pgrp_nr, p))\n\t\treturn -EFAULT;\n\tif (pgrp_nr < 0)\n\t\treturn -EINVAL;\n\tspin_lock_irq(&real_tty->ctrl_lock);\n\tif (!current->signal->tty ||\n\t    (current->signal->tty != real_tty) ||\n\t    (real_tty->session != task_session(current))) {\n\t\tretval = -ENOTTY;\n\t\tgoto out_unlock_ctrl;\n\t}\n\trcu_read_lock();\n\tpgrp = find_vpid(pgrp_nr);\n\tretval = -ESRCH;\n\tif (!pgrp)\n\t\tgoto out_unlock;\n\tretval = -EPERM;\n\tif (session_of_pgrp(pgrp) != task_session(current))\n\t\tgoto out_unlock;\n\tretval = 0;\n\tput_pid(real_tty->pgrp);\n\treal_tty->pgrp = get_pid(pgrp);\nout_unlock:\n\trcu_read_unlock();\nout_unlock_ctrl:\n\tspin_unlock_irq(&real_tty->ctrl_lock);\n\treturn retval;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 313
    },
    {
        "code": "static struct nft_expr *nft_expr_init(const struct nft_ctx *ctx,\n\t\t\t\t      const struct nlattr *nla)\n{\n\tstruct nft_expr_info expr_info;\n\tstruct nft_expr *expr;\n\tstruct module *owner;\n\tint err;\n\terr = nf_tables_expr_parse(ctx, nla, &expr_info);\n\tif (err < 0)\n\t\tgoto err_expr_parse;\n\terr = -EOPNOTSUPP;\n\tif (!(expr_info.ops->type->flags & NFT_EXPR_STATEFUL))\n\t\tgoto err_expr_stateful;\n\terr = -ENOMEM;\n\texpr = kzalloc(expr_info.ops->size, GFP_KERNEL_ACCOUNT);\n\tif (expr == NULL)\n\t\tgoto err_expr_stateful;\n\terr = nf_tables_newexpr(ctx, &expr_info, expr);\n\tif (err < 0)\n\t\tgoto err_expr_new;\n\treturn expr;\nerr_expr_new:\n\tkfree(expr);\nerr_expr_stateful:\n\towner = expr_info.ops->type->owner;\n\tif (expr_info.ops->type->release_ops)\n\t\texpr_info.ops->type->release_ops(expr_info.ops);\n\tmodule_put(owner);\nerr_expr_parse:\n\treturn ERR_PTR(err);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 315
    },
    {
        "code": "void btf_dump__free(struct btf_dump *d)\n{\n\tint i;\n\tif (IS_ERR_OR_NULL(d))\n\t\treturn;\n\tfree(d->type_states);\n\tif (d->cached_names) {\n\t\t/* any set cached name is owned by us and should be freed */\n\t\tfor (i = 0; i <= d->last_id; i++) {\n\t\t\tif (d->cached_names[i])\n\t\t\t\tfree((void *)d->cached_names[i]);\n\t\t}\n\t}\n\tfree(d->cached_names);\n\tfree(d->emit_queue);\n\tfree(d->decl_stack);\n\tbtf_dump_free_names(d->type_names);\n\tbtf_dump_free_names(d->ident_names);\n\tfree(d);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 317
    },
    {
        "code": "static int ipxitf_ioctl(unsigned int cmd, void __user *arg)\n{\n\tint rc = -EINVAL;\n\tstruct ifreq ifr;\n\tint val;\n\tswitch (cmd) {\n\tcase SIOCSIFADDR: {\n\t\tstruct sockaddr_ipx *sipx;\n\t\tstruct ipx_interface_definition f;\n\t\trc = -EFAULT;\n\t\tif (copy_from_user(&ifr, arg, sizeof(ifr)))\n\t\t\tbreak;\n\t\tsipx = (struct sockaddr_ipx *)&ifr.ifr_addr;\n\t\trc = -EINVAL;\n\t\tif (sipx->sipx_family != AF_IPX)\n\t\t\tbreak;\n\t\tf.ipx_network = sipx->sipx_network;\n\t\tmemcpy(f.ipx_device, ifr.ifr_name,\n\t\t\tsizeof(f.ipx_device));\n\t\tmemcpy(f.ipx_node, sipx->sipx_node, IPX_NODE_LEN);\n\t\tf.ipx_dlink_type = sipx->sipx_type;\n\t\tf.ipx_special = sipx->sipx_special;\n\t\tif (sipx->sipx_action == IPX_DLTITF)\n\t\t\trc = ipxitf_delete(&f);\n\t\telse\n\t\t\trc = ipxitf_create(&f);\n\t\tbreak;\n\t}\n\tcase SIOCGIFADDR: {\n\t\tstruct sockaddr_ipx *sipx;\n\t\tstruct ipx_interface *ipxif;\n\t\tstruct net_device *dev;\n\t\trc = -EFAULT;\n\t\tif (copy_from_user(&ifr, arg, sizeof(ifr)))\n\t\t\tbreak;\n\t\tsipx = (struct sockaddr_ipx *)&ifr.ifr_addr;\n\t\tdev  = __dev_get_by_name(&init_net, ifr.ifr_name);\n\t\trc   = -ENODEV;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tipxif = ipxitf_find_using_phys(dev,\n\t\t\t\t\t   ipx_map_frame_type(sipx->sipx_type));\n\t\trc = -EADDRNOTAVAIL;\n\t\tif (!ipxif)\n\t\t\tbreak;\n\t\tsipx->sipx_family\t= AF_IPX;\n\t\tsipx->sipx_network\t= ipxif->if_netnum;\n\t\tmemcpy(sipx->sipx_node, ipxif->if_node,\n\t\t\tsizeof(sipx->sipx_node));\n\t\trc = 0;\n\t\tif (copy_to_user(arg, &ifr, sizeof(ifr)))\n\t\t\trc = -EFAULT;\n\t\tipxitf_put(ipxif);\n\t\tbreak;\n\t}\n\tcase SIOCAIPXITFCRT:\n\t\trc = -EFAULT;\n\t\tif (get_user(val, (unsigned char __user *) arg))\n\t\t\tbreak;\n\t\trc = 0;\n\t\tipxcfg_auto_create_interfaces = val;\n\t\tbreak;\n\tcase SIOCAIPXPRISLT:\n\t\trc = -EFAULT;\n\t\tif (get_user(val, (unsigned char __user *) arg))\n\t\t\tbreak;\n\t\trc = 0;\n\t\tipxcfg_set_auto_select(val);\n\t\tbreak;\n\t}\n\treturn rc;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 319
    },
    {
        "code": "static int read_rindex_entry(struct gfs2_inode *ip)\n{\n\tstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\n\tconst unsigned bsize = sdp->sd_sb.sb_bsize;\n\tloff_t pos = sdp->sd_rgrps * sizeof(struct gfs2_rindex);\n\tstruct gfs2_rindex buf;\n\tint error;\n\tstruct gfs2_rgrpd *rgd;\n\tif (pos >= i_size_read(&ip->i_inode))\n\t\treturn 1;\n\terror = gfs2_internal_read(ip, (char *)&buf, &pos,\n\t\t\t\t   sizeof(struct gfs2_rindex));\n\tif (error != sizeof(struct gfs2_rindex))\n\t\treturn (error == 0) ? 1 : error;\n\trgd = kmem_cache_zalloc(gfs2_rgrpd_cachep, GFP_NOFS);\n\terror = -ENOMEM;\n\tif (!rgd)\n\t\treturn error;\n\trgd->rd_sbd = sdp;\n\trgd->rd_addr = be64_to_cpu(buf.ri_addr);\n\trgd->rd_length = be32_to_cpu(buf.ri_length);\n\trgd->rd_data0 = be64_to_cpu(buf.ri_data0);\n\trgd->rd_data = be32_to_cpu(buf.ri_data);\n\trgd->rd_bitbytes = be32_to_cpu(buf.ri_bitbytes);\n\tspin_lock_init(&rgd->rd_rsspin);\n\terror = compute_bitstructs(rgd);\n\tif (error)\n\t\tgoto fail;\n\terror = gfs2_glock_get(sdp, rgd->rd_addr,\n\t\t\t       &gfs2_rgrp_glops, CREATE, &rgd->rd_gl);\n\tif (error)\n\t\tgoto fail;\n\trgd->rd_rgl = (struct gfs2_rgrp_lvb *)rgd->rd_gl->gl_lksb.sb_lvbptr;\n\trgd->rd_flags &= ~(GFS2_RDF_UPTODATE | GFS2_RDF_PREFERRED);\n\tif (rgd->rd_data > sdp->sd_max_rg_data)\n\t\tsdp->sd_max_rg_data = rgd->rd_data;\n\tspin_lock(&sdp->sd_rindex_spin);\n\terror = rgd_insert(rgd);\n\tspin_unlock(&sdp->sd_rindex_spin);\n\tif (!error) {\n\t\trgd->rd_gl->gl_object = rgd;\n\t\trgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;\n\t\trgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr +\n\t\t\t\t\t\t    rgd->rd_length) * bsize) - 1;\n\t\treturn 0;\n\t}\n\terror = 0; /* someone else read in the rgrp; free it and ignore it */\n\tgfs2_glock_put(rgd->rd_gl);\nfail:\n\tkfree(rgd->rd_bits);\n\trgd->rd_bits = NULL;\n\tkmem_cache_free(gfs2_rgrpd_cachep, rgd);\n\treturn error;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 321
    },
    {
        "code": "static struct iscsi_cls_session *\niscsi_sw_tcp_session_create(struct iscsi_endpoint *ep, uint16_t cmds_max,\n\t\t\t    uint16_t qdepth, uint32_t initial_cmdsn)\n{\n\tstruct iscsi_cls_session *cls_session;\n\tstruct iscsi_session *session;\n\tstruct iscsi_sw_tcp_host *tcp_sw_host;\n\tstruct Scsi_Host *shost;\n\tint rc;\n\tif (ep) {\n\t\tprintk(KERN_ERR \"iscsi_tcp: invalid ep %p.\\n\", ep);\n\t\treturn NULL;\n\t}\n\tshost = iscsi_host_alloc(&iscsi_sw_tcp_sht,\n\t\t\t\t sizeof(struct iscsi_sw_tcp_host), 1);\n\tif (!shost)\n\t\treturn NULL;\n\tshost->transportt = iscsi_sw_tcp_scsi_transport;\n\tshost->cmd_per_lun = qdepth;\n\tshost->max_lun = iscsi_max_lun;\n\tshost->max_id = 0;\n\tshost->max_channel = 0;\n\tshost->max_cmd_len = SCSI_MAX_VARLEN_CDB_SIZE;\n\trc = iscsi_host_get_max_scsi_cmds(shost, cmds_max);\n\tif (rc < 0)\n\t\tgoto free_host;\n\tshost->can_queue = rc;\n\tif (iscsi_host_add(shost, NULL))\n\t\tgoto free_host;\n\tcls_session = iscsi_session_setup(&iscsi_sw_tcp_transport, shost,\n\t\t\t\t\t  cmds_max, 0,\n\t\t\t\t\t  sizeof(struct iscsi_tcp_task) +\n\t\t\t\t\t  sizeof(struct iscsi_sw_tcp_hdrbuf),\n\t\t\t\t\t  initial_cmdsn, 0);\n\tif (!cls_session)\n\t\tgoto remove_host;\n\tsession = cls_session->dd_data;\n\tif (iscsi_tcp_r2tpool_alloc(session))\n\t\tgoto remove_session;\n\t/* We are now fully setup so expose the session to sysfs. */\n\ttcp_sw_host = iscsi_host_priv(shost);\n\ttcp_sw_host->session = session;\n\treturn cls_session;\nremove_session:\n\tiscsi_session_teardown(cls_session);\nremove_host:\n\tiscsi_host_remove(shost, false);\nfree_host:\n\tiscsi_host_free(shost);\n\treturn NULL;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 323
    },
    {
        "code": "int vmw_fence_event_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *file_priv)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct drm_vmw_fence_event_arg *arg =\n\t\t(struct drm_vmw_fence_event_arg *) data;\n\tstruct vmw_fence_obj *fence = NULL;\n\tstruct vmw_fpriv *vmw_fp = vmw_fpriv(file_priv);\n\tstruct ttm_object_file *tfile = vmw_fp->tfile;\n\tstruct drm_vmw_fence_rep __user *user_fence_rep =\n\t\t(struct drm_vmw_fence_rep __user *)(unsigned long)\n\t\targ->fence_rep;\n\tuint32_t handle;\n\tint ret;\n\t/*\n\t * Look up an existing fence object,\n\t * and if user-space wants a new reference,\n\t * add one.\n\t */\n\tif (arg->handle) {\n\t\tstruct ttm_base_object *base =\n\t\t\tvmw_fence_obj_lookup(tfile, arg->handle);\n\t\tif (IS_ERR(base))\n\t\t\treturn PTR_ERR(base);\n\t\tfence = &(container_of(base, struct vmw_user_fence,\n\t\t\t\t       base)->fence);\n\t\t(void) vmw_fence_obj_reference(fence);\n\t\tif (user_fence_rep != NULL) {\n\t\t\tret = ttm_ref_object_add(vmw_fp->tfile, base,\n\t\t\t\t\t\t NULL, false);\n\t\t\tif (unlikely(ret != 0)) {\n\t\t\t\tDRM_ERROR(\"Failed to reference a fence \"\n\t\t\t\t\t  \"object.\\n\");\n\t\t\t\tgoto out_no_ref_obj;\n\t\t\t}\n\t\t\thandle = base->handle;\n\t\t}\n\t\tttm_base_object_unref(&base);\n\t}\n\t/*\n\t * Create a new fence object.\n\t */\n\tif (!fence) {\n\t\tret = vmw_execbuf_fence_commands(file_priv, dev_priv,\n\t\t\t\t\t\t &fence,\n\t\t\t\t\t\t (user_fence_rep) ?\n\t\t\t\t\t\t &handle : NULL);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Fence event failed to create fence.\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\tBUG_ON(fence == NULL);\n\tret = vmw_event_fence_action_create(file_priv, fence,\n\t\t\t\t\t    arg->flags,\n\t\t\t\t\t    arg->user_data,\n\t\t\t\t\t    true);\n\tif (unlikely(ret != 0)) {\n\t\tif (ret != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"Failed to attach event to fence.\\n\");\n\t\tgoto out_no_create;\n\t}\n\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fp, 0, user_fence_rep, fence,\n\t\t\t\t    handle, -1);\n\tvmw_fence_obj_unreference(&fence);\n\treturn 0;\nout_no_create:\n\tif (user_fence_rep != NULL)\n\t\tttm_ref_object_base_unref(tfile, handle);\nout_no_ref_obj:\n\tvmw_fence_obj_unreference(&fence);\n\treturn ret;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 325
    },
    {
        "code": "static struct kobject *cdev_get(struct cdev *p)\n{\n\tstruct module *owner = p->owner;\n\tstruct kobject *kobj;\n\tif (owner && !try_module_get(owner))\n\t\treturn NULL;\n\tkobj = kobject_get_unless_zero(&p->kobj);\n\tif (!kobj)\n\t\tmodule_put(owner);\n\treturn kobj;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 327
    },
    {
        "code": "static void yurex_delete(struct kref *kref)\n{\n\tstruct usb_yurex *dev = to_yurex_dev(kref);\n\tdev_dbg(&dev->interface->dev, \"%s\\n\", __func__);\n\tif (dev->cntl_urb) {\n\t\tusb_kill_urb(dev->cntl_urb);\n\t\tkfree(dev->cntl_req);\n\t\tif (dev->cntl_buffer)\n\t\t\tusb_free_coherent(dev->udev, YUREX_BUF_SIZE,\n\t\t\t\tdev->cntl_buffer, dev->cntl_urb->transfer_dma);\n\t\tusb_free_urb(dev->cntl_urb);\n\t}\n\tif (dev->urb) {\n\t\tusb_kill_urb(dev->urb);\n\t\tif (dev->int_buffer)\n\t\t\tusb_free_coherent(dev->udev, YUREX_BUF_SIZE,\n\t\t\t\tdev->int_buffer, dev->urb->transfer_dma);\n\t\tusb_free_urb(dev->urb);\n\t}\n\tusb_put_dev(dev->udev);\n\tkfree(dev);\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 329
    },
    {
        "code": "int snd_timer_open(struct snd_timer_instance **ti,\n\t\t   char *owner, struct snd_timer_id *tid,\n\t\t   unsigned int slave_id)\n{\n\tstruct snd_timer *timer;\n\tstruct snd_timer_instance *timeri = NULL;\n\tstruct device *card_dev_to_put = NULL;\n\tint err;\n\tmutex_lock(&register_mutex);\n\tif (tid->dev_class == SNDRV_TIMER_CLASS_SLAVE) {\n\t\t/* open a slave instance */\n\t\tif (tid->dev_sclass <= SNDRV_TIMER_SCLASS_NONE ||\n\t\t    tid->dev_sclass > SNDRV_TIMER_SCLASS_OSS_SEQUENCER) {\n\t\t\tpr_debug(\"ALSA: timer: invalid slave class %i\\n\",\n\t\t\t\t tid->dev_sclass);\n\t\t\terr = -EINVAL;\n\t\t\tgoto unlock;\n\t\t}\n\t\ttimeri = snd_timer_instance_new(owner, NULL);\n\t\tif (!timeri) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto unlock;\n\t\t}\n\t\ttimeri->slave_class = tid->dev_sclass;\n\t\ttimeri->slave_id = tid->device;\n\t\ttimeri->flags |= SNDRV_TIMER_IFLG_SLAVE;\n\t\tlist_add_tail(&timeri->open_list, &snd_timer_slave_list);\n\t\terr = snd_timer_check_slave(timeri);\n\t\tif (err < 0) {\n\t\t\tsnd_timer_close_locked(timeri, &card_dev_to_put);\n\t\t\ttimeri = NULL;\n\t\t}\n\t\tgoto unlock;\n\t}\n\t/* open a master instance */\n\ttimer = snd_timer_find(tid);\n#ifdef CONFIG_MODULES\n\tif (!timer) {\n\t\tmutex_unlock(&register_mutex);\n\t\tsnd_timer_request(tid);\n\t\tmutex_lock(&register_mutex);\n\t\ttimer = snd_timer_find(tid);\n\t}\n#endif\n\tif (!timer) {\n\t\terr = -ENODEV;\n\t\tgoto unlock;\n\t}\n\tif (!list_empty(&timer->open_list_head)) {\n\t\tstruct snd_timer_instance *t =\n\t\t\tlist_entry(timer->open_list_head.next,\n\t\t\t\t    struct snd_timer_instance, open_list);\n\t\tif (t->flags & SNDRV_TIMER_IFLG_EXCLUSIVE) {\n\t\t\terr = -EBUSY;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\tif (timer->num_instances >= timer->max_instances) {\n\t\terr = -EBUSY;\n\t\tgoto unlock;\n\t}\n\ttimeri = snd_timer_instance_new(owner, timer);\n\tif (!timeri) {\n\t\terr = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\t/* take a card refcount for safe disconnection */\n\tif (timer->card)\n\t\tget_device(&timer->card->card_dev);\n\ttimeri->slave_class = tid->dev_sclass;\n\ttimeri->slave_id = slave_id;\n\tif (list_empty(&timer->open_list_head) && timer->hw.open) {\n\t\terr = timer->hw.open(timer);\n\t\tif (err) {\n\t\t\tkfree(timeri->owner);\n\t\t\tkfree(timeri);\n\t\t\ttimeri = NULL;\n\t\t\tif (timer->card)\n\t\t\t\tcard_dev_to_put = &timer->card->card_dev;\n\t\t\tmodule_put(timer->module);\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\tlist_add_tail(&timeri->open_list, &timer->open_list_head);\n\ttimer->num_instances++;\n\terr = snd_timer_check_master(timeri);\n\tif (err < 0) {\n\t\tsnd_timer_close_locked(timeri, &card_dev_to_put);\n\t\ttimeri = NULL;\n\t}\n unlock:\n\tmutex_unlock(&register_mutex);\n\t/* put_device() is called after unlock for avoiding deadlock */\n\tif (card_dev_to_put)\n\t\tput_device(card_dev_to_put);\n\t*ti = timeri;\n\treturn err;\n}",
        "bug_line_number": [],
        "vul": 0,
        "id": 331
    }
]