import json
import os
import sys
import argparse
import re

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import utils.bm25_retriever as bm25_retriever
import utils.llm_client as llm_client
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import time
from tqdm import tqdm

LLM_CLIENT = None
SUMMARY_LLM_CLIENT = None
output_lock = threading.Lock()
file_lock = threading.Lock()

# Global retriever and knowledge base
GLOBAL_PURPOSE_RETRIEVER = None
GLOBAL_FUNCTION_RETRIEVER = None
GLOBAL_CODE_RETRIEVER = None
GLOBAL_CVE_KNOWLEDGE_DICT = None

retrieve_weight = {
    "purpose": 1,
    "function": 1,
    "code": 1
}

def extract_result_from_output(output):
    """
    Extract content between <result> and </result> tags and determine the result
    
    Args:
        output (str): LLM output text
        
    Returns:
        int: Returns 1 if YES is found, 0 if NO is found
        
    Raises:
        ValueError: If <result> tags are not found or the result contains neither YES nor NO
    """
    
    pattern = r'<result>(.*?)</result>'
    matches = re.findall(pattern, output, re.IGNORECASE | re.DOTALL)
    
    if not matches:
        raise ValueError("No <result> and </result> tags found")
    
    result_content = matches[-1].strip()
    
    if "YES" in result_content.upper():
        return 1
    elif "NO" in result_content.upper():
        return 0
    else:
        raise ValueError(f"Result contains neither YES nor NO, actual content: '{result_content}'")

def retry_with_backoff(func, *args, retry_times=3, **kwargs):
    last_exception = None
    for attempt in range(retry_times):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            last_exception = e
            if attempt < retry_times - 1:  
                wait_time = 1.0  
                print(f"Attempt {attempt + 1} failed: {str(e)}. Retrying in {wait_time:.2f} seconds...")
                time.sleep(wait_time)
            else:
                print(f"All {retry_times} attempts failed. Last error: {str(e)}")
                raise last_exception

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file_name", type=str, required=True)
    parser.add_argument("--knowledge_file_name", type=str, required=True)
    parser.add_argument("--output_file_name", type=str, required=True)
    parser.add_argument("--model_name", type=str, required=True)
    parser.add_argument("--summary_model_name", type=str, required=True)
    parser.add_argument("--retrieval_top_k", type=int, default=20)
    parser.add_argument(
        '--early_return',
        action = 'store_true',
        help = "Whether to return early when solution output is YES."
    )
    parser.add_argument(
        '--max_knowledge',
        type = int,
        default = 3,
        help = 'Final MAX value of knowledge used for detection'
    )
    parser.add_argument(
        '--model_settings',
        type = str,
        default = None,
        help = (
            'The settings of the model, format is a key-value pair separated by ";". '
            'e.g. "temperature=0.2;max_tokens=1024;stream=true"'
        )
    )
    parser.add_argument(
        '--thread_pool_size',
        type = int,
        default = 5,
        help = "Size of thread pool when detecting"
    )
    parser.add_argument(
        '--resume',
        action = 'store_true',
        help = 'Whether to resume from a checkpoint.'
    )
    parser.add_argument(
        '--retry_times',
        type = int,
        default = 5,
        help = "Number of retry attempts for LLM API calls"
    )
    args = parser.parse_args()
    args.model_settings = llm_client.parse_kv_string_to_dict(args.model_settings)
    return args

def generate_extraction_prompt_for_vulrag(code_snippet):
    prefix_str = f"""This is a code snippet: \n{code_snippet}\n"""
    # extract purpose prompt
    purpose_prompt = prefix_str + (
        "What is the purpose of the function in the above code snippet? "
        "Please summarize the answer in one sentence with the following format: "
        "Function purpose: \"\""
    )

    # extract function prompt
    function_prompt = prefix_str + (
        "Please summarize the functions of the above code snippet "
        "in the list format without other explanation: "
        "\"The functions of the code snippet are: 1. 2. 3.\""
    )

    return purpose_prompt, function_prompt

def generate_detect_vul_prompt_with_response_in_HTML(code_snippet, cve_knowledge) -> str:
    return f"""I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a similar vulnerability in the code snippet.
Code Snippet:
'''
{code_snippet}
'''
Vulnerability Knowledge:
In a similar code scenario, the following vulnerabilities have been found:
'''
{cve_knowledge}
'''
Please check if the above code snippet contains similar vulnerability behaviors mentioned in the vulnerability knowledge. Perform a step-by-step analysis and conclude your response with either <result> YES </result> or <result> NO </result>.
"""

def generate_detect_sol_prompt_with_response_in_HTML(code_snippet, cve_knowledge) -> str:
    return f"""I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there are similar necessary solution behaviors in the code snippet, which can prevent the occurrence of related vulnerabilities in the vulnerability knowledge.
Code Snippet:
'''
{code_snippet}
'''
Vulnerability Knowledge:
In a similar code scenario, the following vulnerabilities have been found:
'''
{cve_knowledge}
'''
Please check if the above code snippet contains similar solution behaviors mentioned in the vulnerability knowledge. Perform a step-by-step analysis and conclude your response with either <result> YES </result> or <result> NO </result>.
"""

def retrieve_knowledge_by_cve(args, code_snippet, purpose, function):
    global GLOBAL_PURPOSE_RETRIEVER, GLOBAL_FUNCTION_RETRIEVER, GLOBAL_CODE_RETRIEVER, GLOBAL_CVE_KNOWLEDGE_DICT

    # Use global retrievers for global knowledge retrieval
    purpose_index = GLOBAL_PURPOSE_RETRIEVER.search(purpose, top_n=args.retrieval_top_k)
    function_index = GLOBAL_FUNCTION_RETRIEVER.search(function, top_n=args.retrieval_top_k)
    code_index = GLOBAL_CODE_RETRIEVER.search(code_snippet, top_n=args.retrieval_top_k)

    with open(f"output/knowledge/{args.knowledge_file_name}", "r") as f:
        knowledge_data = json.load(f)

    cve_knowledge_dict = GLOBAL_CVE_KNOWLEDGE_DICT

    purpose_cve_list = []
    function_cve_list = []
    code_cve_list = []

    for index in purpose_index:
        if knowledge_data[index]["CVE_id"] not in purpose_cve_list:
            purpose_cve_list.append(knowledge_data[index]["CVE_id"])

    for index in function_index:
        if knowledge_data[index]["CVE_id"] not in function_cve_list:
            function_cve_list.append(knowledge_data[index]["CVE_id"])
    
    for index in code_index:
        if knowledge_data[index]["CVE_id"] not in code_cve_list:
            code_cve_list.append(knowledge_data[index]["CVE_id"])
    
    ### start to rank the knowledge

    cve_retrieved_list = list(set(purpose_cve_list + function_cve_list + code_cve_list))

    # Rank the retrieved CVEs
    cve_id_dict = {}
    for cve_id in cve_retrieved_list:
        cve_id_dict[cve_id] = 0
        p_index = purpose_cve_list.index(cve_id) if cve_id in purpose_cve_list else len(purpose_cve_list)
        f_index = function_cve_list.index(cve_id) if cve_id in function_cve_list else len(function_cve_list)
        c_index = code_cve_list.index(cve_id) if cve_id in code_cve_list else len(code_cve_list)
        cve_id_dict[cve_id] += p_index * retrieve_weight["purpose"] + \
                              f_index * retrieve_weight["function"] + \
                              c_index * retrieve_weight["code"]

    # Sort by weight in ascending order
    sorted_cve_list = [item[0] for item in sorted(cve_id_dict.items(), key=lambda x: x[1], reverse=False)]

    final_knowledge_list = []
    
    for cve_id in sorted_cve_list:
        items = cve_knowledge_dict[cve_id]
        # Extract purpose, function, code of all knowledge items under the current CVE
        item_purpose_list = [item["GPT_purpose"] for item in items]
        item_function_list = [item["GPT_function"] for item in items]
        item_code_list = [item["code_before_change"] for item in items]
        
        # Retrieve knowledge items within the current CVE
        item_purpose_retriever = bm25_retriever.BM25Retriever()
        item_purpose_retriever.set_corpus(item_purpose_list)
        item_purpose_index = item_purpose_retriever.search(purpose, top_n=-1)

        item_function_retriever = bm25_retriever.BM25Retriever()
        item_function_retriever.set_corpus(item_function_list)
        item_function_index = item_function_retriever.search(function, top_n=-1)

        item_code_retriever = bm25_retriever.BM25Retriever()
        item_code_retriever.set_corpus(item_code_list)
        item_code_index = item_code_retriever.search(code_snippet, top_n=-1)
        
        # Rank knowledge items under the current CVE
        item_score_dict = {}
        for idx, item in enumerate(items):
            # Use index as identifier
            item_score_dict[idx] = 0
            # Use index position in the original list to get position in BM25 results
            purpose_pos = item_purpose_index.index(idx) if idx in item_purpose_index else len(item_purpose_index)
            function_pos = item_function_index.index(idx) if idx in item_function_index else len(item_function_index)
            code_pos = item_code_index.index(idx) if idx in item_code_index else len(item_code_index)
            item_score_dict[idx] += purpose_pos * retrieve_weight["purpose"] + \
                                   function_pos * retrieve_weight["function"] + \
                                   code_pos * retrieve_weight["code"]
        
        # Sort by weight in ascending order, select the knowledge item with the smallest weight
        sorted_items = sorted(item_score_dict.items(), key=lambda x: x[1], reverse=False)
        if sorted_items:
            best_idx = sorted_items[0][0]
            best_item = items[best_idx]
            final_knowledge_list.append(best_item)
    
    # Return the sorted knowledge list

    ans = final_knowledge_list[:min(args.max_knowledge, len(final_knowledge_list))]

    return_knowledge_list = []

    for knowledge in ans:
        return_knowledge_list.append({
            "cve_id": knowledge["CVE_id"],
            "vulnerability_behavior":{
                "preconditions_for_vulnerability": knowledge["preconditions_for_vulnerability"],
                "trigger_condition":knowledge["trigger_condition"],
                "specific_code_behavior_causing_vulnerability":knowledge["specific_code_behavior_causing_vulnerability"]
            },
            "solution_behavior":knowledge['solution']
        })

    return return_knowledge_list

def detect_code(args,code):
    global LLM_CLIENT
    global SUMMARY_LLM_CLIENT

    purpose_prompt, function_prompt = generate_extraction_prompt_for_vulrag(code)

    purpose_output = None
    for attempt in range(args.retry_times):
        try:
            purpose_prompt_dict = llm_client.generate_simple_prompt(purpose_prompt)
            purpose_output = SUMMARY_LLM_CLIENT.generate_text(purpose_prompt_dict, args.model_settings)
            break
        except Exception as e:
            if attempt == args.retry_times - 1:
                raise e
            print(f"Failed to get purpose, attempt {attempt + 1}: {str(e)}")
            time.sleep(1)

    assert purpose_output is not None, "Purpose output is None"
    
    purpose = llm_client.extract_LLM_response_by_prefix(
                        purpose_output,
                        "Function purpose:"
                    )

    function_output = None
    for attempt in range(args.retry_times):
        try:
            function_prompt_dict = llm_client.generate_simple_prompt(function_prompt)
            function_output = SUMMARY_LLM_CLIENT.generate_text(function_prompt_dict, args.model_settings)
            break
        except Exception as e:
            if attempt == args.retry_times - 1:
                raise e
            print(f"Failed to get function, attempt {attempt + 1}: {str(e)}")
            time.sleep(1)
    
    assert function_output is not None, "Function output is None"

    function = llm_client.extract_LLM_response_by_prefix(
                        function_output,
                        "The functions of the code snippet are:"
                    )
    
    knowledge_list = retrieve_knowledge_by_cve(args, code, purpose, function)

    detect_result = []

    for vul_knowledge in knowledge_list:
        vul_detect_prompt = generate_detect_vul_prompt_with_response_in_HTML(code, vul_knowledge)
        sol_detect_prompt = generate_detect_sol_prompt_with_response_in_HTML(code, vul_knowledge)
        vul_messages = llm_client.generate_simple_prompt(vul_detect_prompt)
        sol_messages = llm_client.generate_simple_prompt(sol_detect_prompt)

        vul_output = None
        for attempt in range(args.retry_times):
            try:
                vul_output = LLM_CLIENT.generate_text(vul_messages, args.model_settings)
                if vul_output is None:
                    raise Exception("Call LLM Failed")
                extract_result_from_output(llm_client.remove_thinking(vul_output))
                break
            except Exception as e:
                if attempt == args.retry_times - 1:
                    raise e
                print(f"Failed to get vul_output, attempt {attempt + 1}: {str(e)}")
                time.sleep(1)

        sol_output = None
        for attempt in range(args.retry_times):
            try:
                sol_output = LLM_CLIENT.generate_text(sol_messages, args.model_settings)
                if sol_output is None:
                    raise Exception("Call LLM Failed")
                extract_result_from_output(llm_client.remove_thinking(sol_output))
                break
            except Exception as e:
                if attempt == args.retry_times - 1:
                    raise e
                print(f"Failed to get sol_output, attempt {attempt + 1}: {str(e)}")
                time.sleep(1)

        result = {
                "vul_knowledge": vul_knowledge,
                "vul_detect_prompt": vul_detect_prompt,
                "vul_output": vul_output,
                "sol_detect_prompt": sol_detect_prompt,
                "sol_output": sol_output
            }

        detect_result.append(result)

        vul_result = extract_result_from_output(llm_client.remove_thinking(vul_output))
        sol_result = extract_result_from_output(llm_client.remove_thinking(sol_output))

        if vul_result == 1 and sol_result == 0:
            return {
                "code_snippet": code,
                "purpose": purpose,
                "function": function,
                "detect_result": detect_result, 
                "final_result": 1
            }
        elif sol_result == 1 and args.early_return:
            return {
                "code_snippet": code,
                "purpose": purpose,
                "function": function,
                "detect_result": detect_result, 
                "final_result": 0
            }
        else:
            continue
        
    return {
        "code_snippet": code,
        "purpose": purpose,
        "function": function,
        "detect_result": detect_result, 
        "final_result": -1
    }

def detect_sample(args, item, output_data):
    try:
        code_before = item["code_before_change"]
        code_after = item["code_after_change"]

        detect_result_before = detect_code(args, code_before)
        detect_result_before.update({
                            "id": item["id"],
                            "cve_id": item["cve_id"],
                            "code_snippet": code_before, 
                            "detection_model": args.model_name,
                            "summary_model": args.summary_model_name,
                            "model_settings": args.model_settings,
        })
        
        detect_result_after = detect_code(args, code_after)
        detect_result_after.update({
                            "id": item["id"],
                            "cve_id": item["cve_id"],
                            "code_snippet": code_after, 
                            "detection_model": args.model_name,
                            "summary_model": args.summary_model_name,
                            "model_settings": args.model_settings,
        })
        
        
        with output_lock:
            output_data.append({
                "id": item["id"],
                "detect_result_before": detect_result_before,
                "detect_result_after": detect_result_after
            })
            with file_lock:
                with open(f"output/detect/{args.output_file_name}", "w") as f:
                    json.dump(output_data, f, indent=4)
    except Exception as e:
        print(f"Error processing item {item['id']}: {str(e)}")

def process_item(args, item, output_data, resume_set):
    if item["id"] in resume_set:
        return
    detect_sample(args, item, output_data)

def detect_pipeline(args):
    global LLM_CLIENT, SUMMARY_LLM_CLIENT
    global GLOBAL_PURPOSE_RETRIEVER, GLOBAL_FUNCTION_RETRIEVER, GLOBAL_CODE_RETRIEVER, GLOBAL_CVE_KNOWLEDGE_DICT

    LLM_CLIENT = llm_client.get_llm_client(args.model_name)
    SUMMARY_LLM_CLIENT = llm_client.get_llm_client(args.summary_model_name)
    print("Model Name:")
    print(LLM_CLIENT.model_name)
    print("Summary Model Name:")
    print(SUMMARY_LLM_CLIENT.model_name)

    print("Start Loading Knowledge")
    start_time = time.time()
    # --- Load and initialize global knowledge base and retrievers once ---
    with open(f"output/knowledge/{args.knowledge_file_name}", "r") as f:
        knowledge_data = json.load(f)

    GLOBAL_CVE_KNOWLEDGE_DICT = {}
    for item in knowledge_data:
        if item["CVE_id"] not in GLOBAL_CVE_KNOWLEDGE_DICT:
            GLOBAL_CVE_KNOWLEDGE_DICT[item["CVE_id"]] = []
        GLOBAL_CVE_KNOWLEDGE_DICT[item["CVE_id"]].append(item)
    
    purpose_list = [item["GPT_purpose"] for item in knowledge_data]
    function_list = [item["GPT_function"] for item in knowledge_data]
    code_list = [item["code_before_change"] for item in knowledge_data]

    GLOBAL_PURPOSE_RETRIEVER = bm25_retriever.BM25Retriever()
    GLOBAL_PURPOSE_RETRIEVER.set_corpus(purpose_list)

    GLOBAL_FUNCTION_RETRIEVER = bm25_retriever.BM25Retriever()
    GLOBAL_FUNCTION_RETRIEVER.set_corpus(function_list)

    GLOBAL_CODE_RETRIEVER = bm25_retriever.BM25Retriever()
    GLOBAL_CODE_RETRIEVER.set_corpus(code_list)
    end_time = time.time()
    print(f"Loading Knowledge Time: {end_time - start_time} sec")
    # ----------------------------------------

    resume_set = set()
    output_data = []

    if args.resume:
        if os.path.exists(f"output/detect/{args.output_file_name}"):
            with open(f"output/detect/{args.output_file_name}", "r") as f:
                output_data = json.load(f)
                resume_set = set([item["id"] for item in output_data])

    with open(f"data/test/{args.input_file_name}", "r") as f:
        data = json.load(f)

    # Filter out already processed data
    data_to_process = [item for item in data if item["id"] not in resume_set]
    total_items = len(data_to_process)
    
    print(f"Total tasks: {total_items}")
    print(f"Processed tasks: {len(resume_set)}")
    print(f"Tasks to process: {total_items}")

    with ThreadPoolExecutor(max_workers=args.thread_pool_size) as executor:
        futures = []
        for item in data_to_process:
            future = executor.submit(process_item, args, item, output_data, resume_set)
            futures.append(future)
        
        with tqdm(total=total_items, desc="Processing progress", unit="task") as pbar:
            for future in as_completed(futures):
                try:
                    future.result()
                    pbar.update(1)
                except Exception as e:
                    print(f"\nTask failed: {str(e)}")
                    pbar.update(1)  

def main():
    args = parse_args()
    detect_pipeline(args)

if __name__ == "__main__":
    main()

