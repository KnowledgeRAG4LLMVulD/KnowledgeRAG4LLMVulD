[
    {
        "cve_id": "CVE-2012-3511",
        "code_before_change": "static long madvise_remove(struct vm_area_struct *vma,\n\t\t\t\tstruct vm_area_struct **prev,\n\t\t\t\tunsigned long start, unsigned long end)\n{\n\tloff_t offset;\n\tint error;\n\n\t*prev = NULL;\t/* tell sys_madvise we drop mmap_sem */\n\n\tif (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))\n\t\treturn -EINVAL;\n\n\tif (!vma->vm_file || !vma->vm_file->f_mapping\n\t\t|| !vma->vm_file->f_mapping->host) {\n\t\t\treturn -EINVAL;\n\t}\n\n\tif ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))\n\t\treturn -EACCES;\n\n\toffset = (loff_t)(start - vma->vm_start)\n\t\t\t+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n\n\t/* filesystem's fallocate may need to take i_mutex */\n\tup_read(&current->mm->mmap_sem);\n\terror = do_fallocate(vma->vm_file,\n\t\t\t\tFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\n\t\t\t\toffset, end - start);\n\tdown_read(&current->mm->mmap_sem);\n\treturn error;\n}",
        "code_after_change": "static long madvise_remove(struct vm_area_struct *vma,\n\t\t\t\tstruct vm_area_struct **prev,\n\t\t\t\tunsigned long start, unsigned long end)\n{\n\tloff_t offset;\n\tint error;\n\tstruct file *f;\n\n\t*prev = NULL;\t/* tell sys_madvise we drop mmap_sem */\n\n\tif (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))\n\t\treturn -EINVAL;\n\n\tf = vma->vm_file;\n\n\tif (!f || !f->f_mapping || !f->f_mapping->host) {\n\t\t\treturn -EINVAL;\n\t}\n\n\tif ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))\n\t\treturn -EACCES;\n\n\toffset = (loff_t)(start - vma->vm_start)\n\t\t\t+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n\n\t/*\n\t * Filesystem's fallocate may need to take i_mutex.  We need to\n\t * explicitly grab a reference because the vma (and hence the\n\t * vma's reference to the file) can go away as soon as we drop\n\t * mmap_sem.\n\t */\n\tget_file(f);\n\tup_read(&current->mm->mmap_sem);\n\terror = do_fallocate(f,\n\t\t\t\tFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\n\t\t\t\toffset, end - start);\n\tfput(f);\n\tdown_read(&current->mm->mmap_sem);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,14 +4,16 @@\n {\n \tloff_t offset;\n \tint error;\n+\tstruct file *f;\n \n \t*prev = NULL;\t/* tell sys_madvise we drop mmap_sem */\n \n \tif (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))\n \t\treturn -EINVAL;\n \n-\tif (!vma->vm_file || !vma->vm_file->f_mapping\n-\t\t|| !vma->vm_file->f_mapping->host) {\n+\tf = vma->vm_file;\n+\n+\tif (!f || !f->f_mapping || !f->f_mapping->host) {\n \t\t\treturn -EINVAL;\n \t}\n \n@@ -21,11 +23,18 @@\n \toffset = (loff_t)(start - vma->vm_start)\n \t\t\t+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n \n-\t/* filesystem's fallocate may need to take i_mutex */\n+\t/*\n+\t * Filesystem's fallocate may need to take i_mutex.  We need to\n+\t * explicitly grab a reference because the vma (and hence the\n+\t * vma's reference to the file) can go away as soon as we drop\n+\t * mmap_sem.\n+\t */\n+\tget_file(f);\n \tup_read(&current->mm->mmap_sem);\n-\terror = do_fallocate(vma->vm_file,\n+\terror = do_fallocate(f,\n \t\t\t\tFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\n \t\t\t\toffset, end - start);\n+\tfput(f);\n \tdown_read(&current->mm->mmap_sem);\n \treturn error;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct file *f;",
                "\tf = vma->vm_file;",
                "",
                "\tif (!f || !f->f_mapping || !f->f_mapping->host) {",
                "\t/*",
                "\t * Filesystem's fallocate may need to take i_mutex.  We need to",
                "\t * explicitly grab a reference because the vma (and hence the",
                "\t * vma's reference to the file) can go away as soon as we drop",
                "\t * mmap_sem.",
                "\t */",
                "\tget_file(f);",
                "\terror = do_fallocate(f,",
                "\tfput(f);"
            ],
            "deleted": [
                "\tif (!vma->vm_file || !vma->vm_file->f_mapping",
                "\t\t|| !vma->vm_file->f_mapping->host) {",
                "\t/* filesystem's fallocate may need to take i_mutex */",
                "\terror = do_fallocate(vma->vm_file,"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the madvise_remove function in mm/madvise.c in the Linux kernel before 3.4.5 allow local users to cause a denial of service (use-after-free and system crash) via vectors involving a (1) munmap or (2) close system call."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "struct sock *dccp_v4_request_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t       struct request_sock *req,\n\t\t\t\t       struct dst_entry *dst)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct inet_sock *newinet;\n\tstruct sock *newsk;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto exit_overflow;\n\n\tif (dst == NULL && (dst = inet_csk_route_req(sk, req)) == NULL)\n\t\tgoto exit;\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto exit_nonewsk;\n\n\tsk_setup_caps(newsk, dst);\n\n\tnewinet\t\t   = inet_sk(newsk);\n\tireq\t\t   = inet_rsk(req);\n\tnewinet->inet_daddr\t= ireq->rmt_addr;\n\tnewinet->inet_rcv_saddr = ireq->loc_addr;\n\tnewinet->inet_saddr\t= ireq->loc_addr;\n\tnewinet->opt\t   = ireq->opt;\n\tireq->opt\t   = NULL;\n\tnewinet->mc_index  = inet_iif(skb);\n\tnewinet->mc_ttl\t   = ip_hdr(skb)->ttl;\n\tnewinet->inet_id   = jiffies;\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto exit;\n\t}\n\t__inet_hash_nolisten(newsk, NULL);\n\n\treturn newsk;\n\nexit_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nexit_nonewsk:\n\tdst_release(dst);\nexit:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "code_after_change": "struct sock *dccp_v4_request_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t       struct request_sock *req,\n\t\t\t\t       struct dst_entry *dst)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct inet_sock *newinet;\n\tstruct sock *newsk;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto exit_overflow;\n\n\tif (dst == NULL && (dst = inet_csk_route_req(sk, req)) == NULL)\n\t\tgoto exit;\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto exit_nonewsk;\n\n\tsk_setup_caps(newsk, dst);\n\n\tnewinet\t\t   = inet_sk(newsk);\n\tireq\t\t   = inet_rsk(req);\n\tnewinet->inet_daddr\t= ireq->rmt_addr;\n\tnewinet->inet_rcv_saddr = ireq->loc_addr;\n\tnewinet->inet_saddr\t= ireq->loc_addr;\n\tnewinet->inet_opt\t= ireq->opt;\n\tireq->opt\t   = NULL;\n\tnewinet->mc_index  = inet_iif(skb);\n\tnewinet->mc_ttl\t   = ip_hdr(skb)->ttl;\n\tnewinet->inet_id   = jiffies;\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto exit;\n\t}\n\t__inet_hash_nolisten(newsk, NULL);\n\n\treturn newsk;\n\nexit_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nexit_nonewsk:\n\tdst_release(dst);\nexit:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,7 +23,7 @@\n \tnewinet->inet_daddr\t= ireq->rmt_addr;\n \tnewinet->inet_rcv_saddr = ireq->loc_addr;\n \tnewinet->inet_saddr\t= ireq->loc_addr;\n-\tnewinet->opt\t   = ireq->opt;\n+\tnewinet->inet_opt\t= ireq->opt;\n \tireq->opt\t   = NULL;\n \tnewinet->mc_index  = inet_iif(skb);\n \tnewinet->mc_ttl\t   = ip_hdr(skb)->ttl;",
        "function_modified_lines": {
            "added": [
                "\tnewinet->inet_opt\t= ireq->opt;"
            ],
            "deleted": [
                "\tnewinet->opt\t   = ireq->opt;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "int dccp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tconst struct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\tif (inet->opt != NULL && inet->opt->srr) {\n\t\tif (daddr == 0)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet->opt->faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_DCCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (inet->opt == NULL || !inet->opt->srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (inet->inet_saddr == 0)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet->opt != NULL)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;\n\t/*\n\t * Socket identity is still unknown (sport may be zero).\n\t * However we set state to DCCP_REQUESTING and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet_hash_connect(&dccp_death_row, sk);\n\tif (err != 0)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk_setup_caps(sk, &rt->dst);\n\n\tdp->dccps_iss = secure_dccp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t    inet->inet_daddr,\n\t\t\t\t\t\t    inet->inet_sport,\n\t\t\t\t\t\t    inet->inet_dport);\n\tinet->inet_id = dp->dccps_iss ^ jiffies;\n\n\terr = dccp_connect(sk);\n\trt = NULL;\n\tif (err != 0)\n\t\tgoto failure;\nout:\n\treturn err;\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port, if necessary.\n\t */\n\tdccp_set_state(sk, DCCP_CLOSED);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\tgoto out;\n}",
        "code_after_change": "int dccp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tconst struct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\tstruct ip_options_rcu *inet_opt;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     sock_owned_by_user(sk));\n\tif (inet_opt != NULL && inet_opt->opt.srr) {\n\t\tif (daddr == 0)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet_opt->opt.faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_DCCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (inet_opt == NULL || !inet_opt->opt.srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (inet->inet_saddr == 0)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet_opt)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n\t/*\n\t * Socket identity is still unknown (sport may be zero).\n\t * However we set state to DCCP_REQUESTING and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet_hash_connect(&dccp_death_row, sk);\n\tif (err != 0)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk_setup_caps(sk, &rt->dst);\n\n\tdp->dccps_iss = secure_dccp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t    inet->inet_daddr,\n\t\t\t\t\t\t    inet->inet_sport,\n\t\t\t\t\t\t    inet->inet_dport);\n\tinet->inet_id = dp->dccps_iss ^ jiffies;\n\n\terr = dccp_connect(sk);\n\trt = NULL;\n\tif (err != 0)\n\t\tgoto failure;\nout:\n\treturn err;\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port, if necessary.\n\t */\n\tdccp_set_state(sk, DCCP_CLOSED);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,7 @@\n \tstruct flowi4 fl4;\n \tstruct rtable *rt;\n \tint err;\n+\tstruct ip_options_rcu *inet_opt;\n \n \tdp->dccps_role = DCCP_ROLE_CLIENT;\n \n@@ -18,10 +19,13 @@\n \t\treturn -EAFNOSUPPORT;\n \n \tnexthop = daddr = usin->sin_addr.s_addr;\n-\tif (inet->opt != NULL && inet->opt->srr) {\n+\n+\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n+\t\t\t\t\t     sock_owned_by_user(sk));\n+\tif (inet_opt != NULL && inet_opt->opt.srr) {\n \t\tif (daddr == 0)\n \t\t\treturn -EINVAL;\n-\t\tnexthop = inet->opt->faddr;\n+\t\tnexthop = inet_opt->opt.faddr;\n \t}\n \n \torig_sport = inet->inet_sport;\n@@ -38,7 +42,7 @@\n \t\treturn -ENETUNREACH;\n \t}\n \n-\tif (inet->opt == NULL || !inet->opt->srr)\n+\tif (inet_opt == NULL || !inet_opt->opt.srr)\n \t\tdaddr = rt->rt_dst;\n \n \tif (inet->inet_saddr == 0)\n@@ -49,8 +53,8 @@\n \tinet->inet_daddr = daddr;\n \n \tinet_csk(sk)->icsk_ext_hdr_len = 0;\n-\tif (inet->opt != NULL)\n-\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;\n+\tif (inet_opt)\n+\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n \t/*\n \t * Socket identity is still unknown (sport may be zero).\n \t * However we set state to DCCP_REQUESTING and not releasing socket",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *inet_opt;",
                "",
                "\tinet_opt = rcu_dereference_protected(inet->inet_opt,",
                "\t\t\t\t\t     sock_owned_by_user(sk));",
                "\tif (inet_opt != NULL && inet_opt->opt.srr) {",
                "\t\tnexthop = inet_opt->opt.faddr;",
                "\tif (inet_opt == NULL || !inet_opt->opt.srr)",
                "\tif (inet_opt)",
                "\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;"
            ],
            "deleted": [
                "\tif (inet->opt != NULL && inet->opt->srr) {",
                "\t\tnexthop = inet->opt->faddr;",
                "\tif (inet->opt == NULL || !inet->opt->srr)",
                "\tif (inet->opt != NULL)",
                "\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static struct sock *dccp_v6_request_recv_sock(struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *ireq6 = inet6_rsk(req);\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (dst == NULL) {\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_DCCP;\n\t\tipv6_addr_copy(&fl6.daddr, &ireq6->rmt_addr);\n\t\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\t\tipv6_addr_copy(&fl6.saddr, &ireq6->loc_addr);\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.fl6_dport = inet_rsk(req)->rmt_port;\n\t\tfl6.fl6_sport = inet_rsk(req)->loc_port;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, false);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &ireq6->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &ireq6->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &ireq6->loc_addr);\n\tnewsk->sk_bound_dev_if = ireq6->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (ireq6->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(ireq6->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(ireq6->pktopts);\n\t\tireq6->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\tif (opt != NULL) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt != NULL)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\tif (opt != NULL && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\treturn NULL;\n}",
        "code_after_change": "static struct sock *dccp_v6_request_recv_sock(struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *ireq6 = inet6_rsk(req);\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (dst == NULL) {\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_DCCP;\n\t\tipv6_addr_copy(&fl6.daddr, &ireq6->rmt_addr);\n\t\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\t\tipv6_addr_copy(&fl6.saddr, &ireq6->loc_addr);\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.fl6_dport = inet_rsk(req)->rmt_port;\n\t\tfl6.fl6_sport = inet_rsk(req)->loc_port;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, false);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &ireq6->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &ireq6->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &ireq6->loc_addr);\n\tnewsk->sk_bound_dev_if = ireq6->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (ireq6->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(ireq6->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(ireq6->pktopts);\n\t\tireq6->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\tif (opt != NULL) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt != NULL)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\tif (opt != NULL && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -106,7 +106,7 @@\n \n \t   First: no IPv4 options.\n \t */\n-\tnewinet->opt = NULL;\n+\tnewinet->inet_opt = NULL;\n \n \t/* Clone RX bits */\n \tnewnp->rxopt.all = np->rxopt.all;",
        "function_modified_lines": {
            "added": [
                "\tnewinet->inet_opt = NULL;"
            ],
            "deleted": [
                "\tnewinet->opt = NULL;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "int inet_sk_rebuild_header(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);\n\t__be32 daddr;\n\tint err;\n\n\t/* Route is OK, nothing to do. */\n\tif (rt)\n\t\treturn 0;\n\n\t/* Reroute. */\n\tdaddr = inet->inet_daddr;\n\tif (inet->opt && inet->opt->srr)\n\t\tdaddr = inet->opt->faddr;\n\trt = ip_route_output_ports(sock_net(sk), sk, daddr, inet->inet_saddr,\n\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t   sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt)) {\n\t\terr = 0;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t} else {\n\t\terr = PTR_ERR(rt);\n\n\t\t/* Routing failed... */\n\t\tsk->sk_route_caps = 0;\n\t\t/*\n\t\t * Other protocols have to map its equivalent state to TCP_SYN_SENT.\n\t\t * DCCP maps its DCCP_REQUESTING state to TCP_SYN_SENT. -acme\n\t\t */\n\t\tif (!sysctl_ip_dynaddr ||\n\t\t    sk->sk_state != TCP_SYN_SENT ||\n\t\t    (sk->sk_userlocks & SOCK_BINDADDR_LOCK) ||\n\t\t    (err = inet_sk_reselect_saddr(sk)) != 0)\n\t\t\tsk->sk_err_soft = -err;\n\t}\n\n\treturn err;\n}",
        "code_after_change": "int inet_sk_rebuild_header(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);\n\t__be32 daddr;\n\tstruct ip_options_rcu *inet_opt;\n\tint err;\n\n\t/* Route is OK, nothing to do. */\n\tif (rt)\n\t\treturn 0;\n\n\t/* Reroute. */\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tdaddr = inet->inet_daddr;\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trcu_read_unlock();\n\trt = ip_route_output_ports(sock_net(sk), sk, daddr, inet->inet_saddr,\n\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t   sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt)) {\n\t\terr = 0;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t} else {\n\t\terr = PTR_ERR(rt);\n\n\t\t/* Routing failed... */\n\t\tsk->sk_route_caps = 0;\n\t\t/*\n\t\t * Other protocols have to map its equivalent state to TCP_SYN_SENT.\n\t\t * DCCP maps its DCCP_REQUESTING state to TCP_SYN_SENT. -acme\n\t\t */\n\t\tif (!sysctl_ip_dynaddr ||\n\t\t    sk->sk_state != TCP_SYN_SENT ||\n\t\t    (sk->sk_userlocks & SOCK_BINDADDR_LOCK) ||\n\t\t    (err = inet_sk_reselect_saddr(sk)) != 0)\n\t\t\tsk->sk_err_soft = -err;\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n \tstruct inet_sock *inet = inet_sk(sk);\n \tstruct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);\n \t__be32 daddr;\n+\tstruct ip_options_rcu *inet_opt;\n \tint err;\n \n \t/* Route is OK, nothing to do. */\n@@ -10,9 +11,12 @@\n \t\treturn 0;\n \n \t/* Reroute. */\n+\trcu_read_lock();\n+\tinet_opt = rcu_dereference(inet->inet_opt);\n \tdaddr = inet->inet_daddr;\n-\tif (inet->opt && inet->opt->srr)\n-\t\tdaddr = inet->opt->faddr;\n+\tif (inet_opt && inet_opt->opt.srr)\n+\t\tdaddr = inet_opt->opt.faddr;\n+\trcu_read_unlock();\n \trt = ip_route_output_ports(sock_net(sk), sk, daddr, inet->inet_saddr,\n \t\t\t\t   inet->inet_dport, inet->inet_sport,\n \t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *inet_opt;",
                "\trcu_read_lock();",
                "\tinet_opt = rcu_dereference(inet->inet_opt);",
                "\tif (inet_opt && inet_opt->opt.srr)",
                "\t\tdaddr = inet_opt->opt.faddr;",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tif (inet->opt && inet->opt->srr)",
                "\t\tdaddr = inet->opt->faddr;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "void inet_sock_destruct(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\t__skb_queue_purge(&sk->sk_error_queue);\n\n\tsk_mem_reclaim(sk);\n\n\tif (sk->sk_type == SOCK_STREAM && sk->sk_state != TCP_CLOSE) {\n\t\tpr_err(\"Attempt to release TCP socket in state %d %p\\n\",\n\t\t       sk->sk_state, sk);\n\t\treturn;\n\t}\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive inet socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(sk->sk_wmem_queued);\n\tWARN_ON(sk->sk_forward_alloc);\n\n\tkfree(inet->opt);\n\tdst_release(rcu_dereference_check(sk->sk_dst_cache, 1));\n\tsk_refcnt_debug_dec(sk);\n}",
        "code_after_change": "void inet_sock_destruct(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\t__skb_queue_purge(&sk->sk_error_queue);\n\n\tsk_mem_reclaim(sk);\n\n\tif (sk->sk_type == SOCK_STREAM && sk->sk_state != TCP_CLOSE) {\n\t\tpr_err(\"Attempt to release TCP socket in state %d %p\\n\",\n\t\t       sk->sk_state, sk);\n\t\treturn;\n\t}\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive inet socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(sk->sk_wmem_queued);\n\tWARN_ON(sk->sk_forward_alloc);\n\n\tkfree(rcu_dereference_protected(inet->inet_opt, 1));\n\tdst_release(rcu_dereference_check(sk->sk_dst_cache, 1));\n\tsk_refcnt_debug_dec(sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,7 +22,7 @@\n \tWARN_ON(sk->sk_wmem_queued);\n \tWARN_ON(sk->sk_forward_alloc);\n \n-\tkfree(inet->opt);\n+\tkfree(rcu_dereference_protected(inet->inet_opt, 1));\n \tdst_release(rcu_dereference_check(sk->sk_dst_cache, 1));\n \tsk_refcnt_debug_dec(sk);\n }",
        "function_modified_lines": {
            "added": [
                "\tkfree(rcu_dereference_protected(inet->inet_opt, 1));"
            ],
            "deleted": [
                "\tkfree(inet->opt);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static int inet_sk_reselect_saddr(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t__be32 old_saddr = inet->inet_saddr;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\t__be32 new_saddr;\n\n\tif (inet->opt && inet->opt->srr)\n\t\tdaddr = inet->opt->faddr;\n\n\t/* Query new route. */\n\trt = ip_route_connect(&fl4, daddr, 0, RT_CONN_FLAGS(sk),\n\t\t\t      sk->sk_bound_dev_if, sk->sk_protocol,\n\t\t\t      inet->inet_sport, inet->inet_dport, sk, false);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tsk_setup_caps(sk, &rt->dst);\n\n\tnew_saddr = rt->rt_src;\n\n\tif (new_saddr == old_saddr)\n\t\treturn 0;\n\n\tif (sysctl_ip_dynaddr > 1) {\n\t\tprintk(KERN_INFO \"%s(): shifting inet->saddr from %pI4 to %pI4\\n\",\n\t\t       __func__, &old_saddr, &new_saddr);\n\t}\n\n\tinet->inet_saddr = inet->inet_rcv_saddr = new_saddr;\n\n\t/*\n\t * XXX The only one ugly spot where we need to\n\t * XXX really change the sockets identity after\n\t * XXX it has entered the hashes. -DaveM\n\t *\n\t * Besides that, it does not check for connection\n\t * uniqueness. Wait for troubles.\n\t */\n\t__sk_prot_rehash(sk);\n\treturn 0;\n}",
        "code_after_change": "static int inet_sk_reselect_saddr(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t__be32 old_saddr = inet->inet_saddr;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\t__be32 new_saddr;\n\tstruct ip_options_rcu *inet_opt;\n\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     sock_owned_by_user(sk));\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\n\t/* Query new route. */\n\trt = ip_route_connect(&fl4, daddr, 0, RT_CONN_FLAGS(sk),\n\t\t\t      sk->sk_bound_dev_if, sk->sk_protocol,\n\t\t\t      inet->inet_sport, inet->inet_dport, sk, false);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tsk_setup_caps(sk, &rt->dst);\n\n\tnew_saddr = rt->rt_src;\n\n\tif (new_saddr == old_saddr)\n\t\treturn 0;\n\n\tif (sysctl_ip_dynaddr > 1) {\n\t\tprintk(KERN_INFO \"%s(): shifting inet->saddr from %pI4 to %pI4\\n\",\n\t\t       __func__, &old_saddr, &new_saddr);\n\t}\n\n\tinet->inet_saddr = inet->inet_rcv_saddr = new_saddr;\n\n\t/*\n\t * XXX The only one ugly spot where we need to\n\t * XXX really change the sockets identity after\n\t * XXX it has entered the hashes. -DaveM\n\t *\n\t * Besides that, it does not check for connection\n\t * uniqueness. Wait for troubles.\n\t */\n\t__sk_prot_rehash(sk);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,9 +6,12 @@\n \tstruct flowi4 fl4;\n \tstruct rtable *rt;\n \t__be32 new_saddr;\n+\tstruct ip_options_rcu *inet_opt;\n \n-\tif (inet->opt && inet->opt->srr)\n-\t\tdaddr = inet->opt->faddr;\n+\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n+\t\t\t\t\t     sock_owned_by_user(sk));\n+\tif (inet_opt && inet_opt->opt.srr)\n+\t\tdaddr = inet_opt->opt.faddr;\n \n \t/* Query new route. */\n \trt = ip_route_connect(&fl4, daddr, 0, RT_CONN_FLAGS(sk),",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *inet_opt;",
                "\tinet_opt = rcu_dereference_protected(inet->inet_opt,",
                "\t\t\t\t\t     sock_owned_by_user(sk));",
                "\tif (inet_opt && inet_opt->opt.srr)",
                "\t\tdaddr = inet_opt->opt.faddr;"
            ],
            "deleted": [
                "\tif (inet->opt && inet->opt->srr)",
                "\t\tdaddr = inet->opt->faddr;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "void cipso_v4_req_delattr(struct request_sock *req)\n{\n\tstruct ip_options *opt;\n\tstruct inet_request_sock *req_inet;\n\n\treq_inet = inet_rsk(req);\n\topt = req_inet->opt;\n\tif (opt == NULL || opt->cipso == 0)\n\t\treturn;\n\n\tcipso_v4_delopt(&req_inet->opt);\n}",
        "code_after_change": "void cipso_v4_req_delattr(struct request_sock *req)\n{\n\tstruct ip_options_rcu *opt;\n\tstruct inet_request_sock *req_inet;\n\n\treq_inet = inet_rsk(req);\n\topt = req_inet->opt;\n\tif (opt == NULL || opt->opt.cipso == 0)\n\t\treturn;\n\n\tcipso_v4_delopt(&req_inet->opt);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,11 @@\n void cipso_v4_req_delattr(struct request_sock *req)\n {\n-\tstruct ip_options *opt;\n+\tstruct ip_options_rcu *opt;\n \tstruct inet_request_sock *req_inet;\n \n \treq_inet = inet_rsk(req);\n \topt = req_inet->opt;\n-\tif (opt == NULL || opt->cipso == 0)\n+\tif (opt == NULL || opt->opt.cipso == 0)\n \t\treturn;\n \n \tcipso_v4_delopt(&req_inet->opt);",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *opt;",
                "\tif (opt == NULL || opt->opt.cipso == 0)"
            ],
            "deleted": [
                "\tstruct ip_options *opt;",
                "\tif (opt == NULL || opt->cipso == 0)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "void cipso_v4_sock_delattr(struct sock *sk)\n{\n\tint hdr_delta;\n\tstruct ip_options *opt;\n\tstruct inet_sock *sk_inet;\n\n\tsk_inet = inet_sk(sk);\n\topt = sk_inet->opt;\n\tif (opt == NULL || opt->cipso == 0)\n\t\treturn;\n\n\thdr_delta = cipso_v4_delopt(&sk_inet->opt);\n\tif (sk_inet->is_icsk && hdr_delta > 0) {\n\t\tstruct inet_connection_sock *sk_conn = inet_csk(sk);\n\t\tsk_conn->icsk_ext_hdr_len -= hdr_delta;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n}",
        "code_after_change": "void cipso_v4_sock_delattr(struct sock *sk)\n{\n\tint hdr_delta;\n\tstruct ip_options_rcu *opt;\n\tstruct inet_sock *sk_inet;\n\n\tsk_inet = inet_sk(sk);\n\topt = rcu_dereference_protected(sk_inet->inet_opt, 1);\n\tif (opt == NULL || opt->opt.cipso == 0)\n\t\treturn;\n\n\thdr_delta = cipso_v4_delopt(&sk_inet->inet_opt);\n\tif (sk_inet->is_icsk && hdr_delta > 0) {\n\t\tstruct inet_connection_sock *sk_conn = inet_csk(sk);\n\t\tsk_conn->icsk_ext_hdr_len -= hdr_delta;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,15 +1,15 @@\n void cipso_v4_sock_delattr(struct sock *sk)\n {\n \tint hdr_delta;\n-\tstruct ip_options *opt;\n+\tstruct ip_options_rcu *opt;\n \tstruct inet_sock *sk_inet;\n \n \tsk_inet = inet_sk(sk);\n-\topt = sk_inet->opt;\n-\tif (opt == NULL || opt->cipso == 0)\n+\topt = rcu_dereference_protected(sk_inet->inet_opt, 1);\n+\tif (opt == NULL || opt->opt.cipso == 0)\n \t\treturn;\n \n-\thdr_delta = cipso_v4_delopt(&sk_inet->opt);\n+\thdr_delta = cipso_v4_delopt(&sk_inet->inet_opt);\n \tif (sk_inet->is_icsk && hdr_delta > 0) {\n \t\tstruct inet_connection_sock *sk_conn = inet_csk(sk);\n \t\tsk_conn->icsk_ext_hdr_len -= hdr_delta;",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *opt;",
                "\topt = rcu_dereference_protected(sk_inet->inet_opt, 1);",
                "\tif (opt == NULL || opt->opt.cipso == 0)",
                "\thdr_delta = cipso_v4_delopt(&sk_inet->inet_opt);"
            ],
            "deleted": [
                "\tstruct ip_options *opt;",
                "\topt = sk_inet->opt;",
                "\tif (opt == NULL || opt->cipso == 0)",
                "\thdr_delta = cipso_v4_delopt(&sk_inet->opt);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "int cipso_v4_sock_getattr(struct sock *sk, struct netlbl_lsm_secattr *secattr)\n{\n\tstruct ip_options *opt;\n\n\topt = inet_sk(sk)->opt;\n\tif (opt == NULL || opt->cipso == 0)\n\t\treturn -ENOMSG;\n\n\treturn cipso_v4_getattr(opt->__data + opt->cipso - sizeof(struct iphdr),\n\t\t\t\tsecattr);\n}",
        "code_after_change": "int cipso_v4_sock_getattr(struct sock *sk, struct netlbl_lsm_secattr *secattr)\n{\n\tstruct ip_options_rcu *opt;\n\tint res = -ENOMSG;\n\n\trcu_read_lock();\n\topt = rcu_dereference(inet_sk(sk)->inet_opt);\n\tif (opt && opt->opt.cipso)\n\t\tres = cipso_v4_getattr(opt->opt.__data +\n\t\t\t\t\t\topt->opt.cipso -\n\t\t\t\t\t\tsizeof(struct iphdr),\n\t\t\t\t       secattr);\n\trcu_read_unlock();\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,15 @@\n int cipso_v4_sock_getattr(struct sock *sk, struct netlbl_lsm_secattr *secattr)\n {\n-\tstruct ip_options *opt;\n+\tstruct ip_options_rcu *opt;\n+\tint res = -ENOMSG;\n \n-\topt = inet_sk(sk)->opt;\n-\tif (opt == NULL || opt->cipso == 0)\n-\t\treturn -ENOMSG;\n-\n-\treturn cipso_v4_getattr(opt->__data + opt->cipso - sizeof(struct iphdr),\n-\t\t\t\tsecattr);\n+\trcu_read_lock();\n+\topt = rcu_dereference(inet_sk(sk)->inet_opt);\n+\tif (opt && opt->opt.cipso)\n+\t\tres = cipso_v4_getattr(opt->opt.__data +\n+\t\t\t\t\t\topt->opt.cipso -\n+\t\t\t\t\t\tsizeof(struct iphdr),\n+\t\t\t\t       secattr);\n+\trcu_read_unlock();\n+\treturn res;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *opt;",
                "\tint res = -ENOMSG;",
                "\trcu_read_lock();",
                "\topt = rcu_dereference(inet_sk(sk)->inet_opt);",
                "\tif (opt && opt->opt.cipso)",
                "\t\tres = cipso_v4_getattr(opt->opt.__data +",
                "\t\t\t\t\t\topt->opt.cipso -",
                "\t\t\t\t\t\tsizeof(struct iphdr),",
                "\t\t\t\t       secattr);",
                "\trcu_read_unlock();",
                "\treturn res;"
            ],
            "deleted": [
                "\tstruct ip_options *opt;",
                "\topt = inet_sk(sk)->opt;",
                "\tif (opt == NULL || opt->cipso == 0)",
                "\t\treturn -ENOMSG;",
                "",
                "\treturn cipso_v4_getattr(opt->__data + opt->cipso - sizeof(struct iphdr),",
                "\t\t\t\tsecattr);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "int cipso_v4_sock_setattr(struct sock *sk,\n\t\t\t  const struct cipso_v4_doi *doi_def,\n\t\t\t  const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_sock *sk_inet;\n\tstruct inet_connection_sock *sk_conn;\n\n\t/* In the case of sock_create_lite(), the sock->sk field is not\n\t * defined yet but it is not a problem as the only users of these\n\t * \"lite\" PF_INET sockets are functions which do an accept() call\n\t * afterwards so we will label the socket as part of the accept(). */\n\tif (sk == NULL)\n\t\treturn 0;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto socket_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\tmemcpy(opt->__data, buf, buf_len);\n\topt->optlen = opt_len;\n\topt->cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\tsk_inet = inet_sk(sk);\n\tif (sk_inet->is_icsk) {\n\t\tsk_conn = inet_csk(sk);\n\t\tif (sk_inet->opt)\n\t\t\tsk_conn->icsk_ext_hdr_len -= sk_inet->opt->optlen;\n\t\tsk_conn->icsk_ext_hdr_len += opt->optlen;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n\topt = xchg(&sk_inet->opt, opt);\n\tkfree(opt);\n\n\treturn 0;\n\nsocket_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}",
        "code_after_change": "int cipso_v4_sock_setattr(struct sock *sk,\n\t\t\t  const struct cipso_v4_doi *doi_def,\n\t\t\t  const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options_rcu *old, *opt = NULL;\n\tstruct inet_sock *sk_inet;\n\tstruct inet_connection_sock *sk_conn;\n\n\t/* In the case of sock_create_lite(), the sock->sk field is not\n\t * defined yet but it is not a problem as the only users of these\n\t * \"lite\" PF_INET sockets are functions which do an accept() call\n\t * afterwards so we will label the socket as part of the accept(). */\n\tif (sk == NULL)\n\t\treturn 0;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto socket_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\tmemcpy(opt->opt.__data, buf, buf_len);\n\topt->opt.optlen = opt_len;\n\topt->opt.cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\tsk_inet = inet_sk(sk);\n\n\told = rcu_dereference_protected(sk_inet->inet_opt, sock_owned_by_user(sk));\n\tif (sk_inet->is_icsk) {\n\t\tsk_conn = inet_csk(sk);\n\t\tif (old)\n\t\t\tsk_conn->icsk_ext_hdr_len -= old->opt.optlen;\n\t\tsk_conn->icsk_ext_hdr_len += opt->opt.optlen;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n\trcu_assign_pointer(sk_inet->inet_opt, opt);\n\tif (old)\n\t\tcall_rcu(&old->rcu, opt_kfree_rcu);\n\n\treturn 0;\n\nsocket_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tunsigned char *buf = NULL;\n \tu32 buf_len;\n \tu32 opt_len;\n-\tstruct ip_options *opt = NULL;\n+\tstruct ip_options_rcu *old, *opt = NULL;\n \tstruct inet_sock *sk_inet;\n \tstruct inet_connection_sock *sk_conn;\n \n@@ -42,22 +42,25 @@\n \t\tret_val = -ENOMEM;\n \t\tgoto socket_setattr_failure;\n \t}\n-\tmemcpy(opt->__data, buf, buf_len);\n-\topt->optlen = opt_len;\n-\topt->cipso = sizeof(struct iphdr);\n+\tmemcpy(opt->opt.__data, buf, buf_len);\n+\topt->opt.optlen = opt_len;\n+\topt->opt.cipso = sizeof(struct iphdr);\n \tkfree(buf);\n \tbuf = NULL;\n \n \tsk_inet = inet_sk(sk);\n+\n+\told = rcu_dereference_protected(sk_inet->inet_opt, sock_owned_by_user(sk));\n \tif (sk_inet->is_icsk) {\n \t\tsk_conn = inet_csk(sk);\n-\t\tif (sk_inet->opt)\n-\t\t\tsk_conn->icsk_ext_hdr_len -= sk_inet->opt->optlen;\n-\t\tsk_conn->icsk_ext_hdr_len += opt->optlen;\n+\t\tif (old)\n+\t\t\tsk_conn->icsk_ext_hdr_len -= old->opt.optlen;\n+\t\tsk_conn->icsk_ext_hdr_len += opt->opt.optlen;\n \t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n \t}\n-\topt = xchg(&sk_inet->opt, opt);\n-\tkfree(opt);\n+\trcu_assign_pointer(sk_inet->inet_opt, opt);\n+\tif (old)\n+\t\tcall_rcu(&old->rcu, opt_kfree_rcu);\n \n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *old, *opt = NULL;",
                "\tmemcpy(opt->opt.__data, buf, buf_len);",
                "\topt->opt.optlen = opt_len;",
                "\topt->opt.cipso = sizeof(struct iphdr);",
                "",
                "\told = rcu_dereference_protected(sk_inet->inet_opt, sock_owned_by_user(sk));",
                "\t\tif (old)",
                "\t\t\tsk_conn->icsk_ext_hdr_len -= old->opt.optlen;",
                "\t\tsk_conn->icsk_ext_hdr_len += opt->opt.optlen;",
                "\trcu_assign_pointer(sk_inet->inet_opt, opt);",
                "\tif (old)",
                "\t\tcall_rcu(&old->rcu, opt_kfree_rcu);"
            ],
            "deleted": [
                "\tstruct ip_options *opt = NULL;",
                "\tmemcpy(opt->__data, buf, buf_len);",
                "\topt->optlen = opt_len;",
                "\topt->cipso = sizeof(struct iphdr);",
                "\t\tif (sk_inet->opt)",
                "\t\t\tsk_conn->icsk_ext_hdr_len -= sk_inet->opt->optlen;",
                "\t\tsk_conn->icsk_ext_hdr_len += opt->optlen;",
                "\topt = xchg(&sk_inet->opt, opt);",
                "\tkfree(opt);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "int cipso_v4_req_setattr(struct request_sock *req,\n\t\t\t const struct cipso_v4_doi *doi_def,\n\t\t\t const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_request_sock *req_inet;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto req_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\tmemcpy(opt->__data, buf, buf_len);\n\topt->optlen = opt_len;\n\topt->cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\treq_inet = inet_rsk(req);\n\topt = xchg(&req_inet->opt, opt);\n\tkfree(opt);\n\n\treturn 0;\n\nreq_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}",
        "code_after_change": "int cipso_v4_req_setattr(struct request_sock *req,\n\t\t\t const struct cipso_v4_doi *doi_def,\n\t\t\t const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options_rcu *opt = NULL;\n\tstruct inet_request_sock *req_inet;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto req_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\tmemcpy(opt->opt.__data, buf, buf_len);\n\topt->opt.optlen = opt_len;\n\topt->opt.cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\treq_inet = inet_rsk(req);\n\topt = xchg(&req_inet->opt, opt);\n\tif (opt)\n\t\tcall_rcu(&opt->rcu, opt_kfree_rcu);\n\n\treturn 0;\n\nreq_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tunsigned char *buf = NULL;\n \tu32 buf_len;\n \tu32 opt_len;\n-\tstruct ip_options *opt = NULL;\n+\tstruct ip_options_rcu *opt = NULL;\n \tstruct inet_request_sock *req_inet;\n \n \t/* We allocate the maximum CIPSO option size here so we are probably\n@@ -34,15 +34,16 @@\n \t\tret_val = -ENOMEM;\n \t\tgoto req_setattr_failure;\n \t}\n-\tmemcpy(opt->__data, buf, buf_len);\n-\topt->optlen = opt_len;\n-\topt->cipso = sizeof(struct iphdr);\n+\tmemcpy(opt->opt.__data, buf, buf_len);\n+\topt->opt.optlen = opt_len;\n+\topt->opt.cipso = sizeof(struct iphdr);\n \tkfree(buf);\n \tbuf = NULL;\n \n \treq_inet = inet_rsk(req);\n \topt = xchg(&req_inet->opt, opt);\n-\tkfree(opt);\n+\tif (opt)\n+\t\tcall_rcu(&opt->rcu, opt_kfree_rcu);\n \n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *opt = NULL;",
                "\tmemcpy(opt->opt.__data, buf, buf_len);",
                "\topt->opt.optlen = opt_len;",
                "\topt->opt.cipso = sizeof(struct iphdr);",
                "\tif (opt)",
                "\t\tcall_rcu(&opt->rcu, opt_kfree_rcu);"
            ],
            "deleted": [
                "\tstruct ip_options *opt = NULL;",
                "\tmemcpy(opt->__data, buf, buf_len);",
                "\topt->optlen = opt_len;",
                "\topt->cipso = sizeof(struct iphdr);",
                "\tkfree(opt);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static void icmp_reply(struct icmp_bxm *icmp_param, struct sk_buff *skb)\n{\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net *net = dev_net(rt->dst.dev);\n\tstruct sock *sk;\n\tstruct inet_sock *inet;\n\t__be32 daddr;\n\n\tif (ip_options_echo(&icmp_param->replyopts, skb))\n\t\treturn;\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\tinet = inet_sk(sk);\n\n\ticmp_param->data.icmph.checksum = 0;\n\n\tinet->tos = ip_hdr(skb)->tos;\n\tdaddr = ipc.addr = rt->rt_src;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tif (icmp_param->replyopts.optlen) {\n\t\tipc.opt = &icmp_param->replyopts;\n\t\tif (ipc.opt->srr)\n\t\t\tdaddr = icmp_param->replyopts.faddr;\n\t}\n\t{\n\t\tstruct flowi4 fl4 = {\n\t\t\t.daddr = daddr,\n\t\t\t.saddr = rt->rt_spec_dst,\n\t\t\t.flowi4_tos = RT_TOS(ip_hdr(skb)->tos),\n\t\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t};\n\t\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto out_unlock;\n\t}\n\tif (icmpv4_xrlim_allow(net, rt, icmp_param->data.icmph.type,\n\t\t\t       icmp_param->data.icmph.code))\n\t\ticmp_push_reply(icmp_param, &ipc, &rt);\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\n}",
        "code_after_change": "static void icmp_reply(struct icmp_bxm *icmp_param, struct sk_buff *skb)\n{\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net *net = dev_net(rt->dst.dev);\n\tstruct sock *sk;\n\tstruct inet_sock *inet;\n\t__be32 daddr;\n\n\tif (ip_options_echo(&icmp_param->replyopts.opt.opt, skb))\n\t\treturn;\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\tinet = inet_sk(sk);\n\n\ticmp_param->data.icmph.checksum = 0;\n\n\tinet->tos = ip_hdr(skb)->tos;\n\tdaddr = ipc.addr = rt->rt_src;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tif (icmp_param->replyopts.opt.opt.optlen) {\n\t\tipc.opt = &icmp_param->replyopts.opt;\n\t\tif (ipc.opt->opt.srr)\n\t\t\tdaddr = icmp_param->replyopts.opt.opt.faddr;\n\t}\n\t{\n\t\tstruct flowi4 fl4 = {\n\t\t\t.daddr = daddr,\n\t\t\t.saddr = rt->rt_spec_dst,\n\t\t\t.flowi4_tos = RT_TOS(ip_hdr(skb)->tos),\n\t\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t};\n\t\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto out_unlock;\n\t}\n\tif (icmpv4_xrlim_allow(net, rt, icmp_param->data.icmph.type,\n\t\t\t       icmp_param->data.icmph.code))\n\t\ticmp_push_reply(icmp_param, &ipc, &rt);\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \tstruct inet_sock *inet;\n \t__be32 daddr;\n \n-\tif (ip_options_echo(&icmp_param->replyopts, skb))\n+\tif (ip_options_echo(&icmp_param->replyopts.opt.opt, skb))\n \t\treturn;\n \n \tsk = icmp_xmit_lock(net);\n@@ -21,10 +21,10 @@\n \tdaddr = ipc.addr = rt->rt_src;\n \tipc.opt = NULL;\n \tipc.tx_flags = 0;\n-\tif (icmp_param->replyopts.optlen) {\n-\t\tipc.opt = &icmp_param->replyopts;\n-\t\tif (ipc.opt->srr)\n-\t\t\tdaddr = icmp_param->replyopts.faddr;\n+\tif (icmp_param->replyopts.opt.opt.optlen) {\n+\t\tipc.opt = &icmp_param->replyopts.opt;\n+\t\tif (ipc.opt->opt.srr)\n+\t\t\tdaddr = icmp_param->replyopts.opt.opt.faddr;\n \t}\n \t{\n \t\tstruct flowi4 fl4 = {",
        "function_modified_lines": {
            "added": [
                "\tif (ip_options_echo(&icmp_param->replyopts.opt.opt, skb))",
                "\tif (icmp_param->replyopts.opt.opt.optlen) {",
                "\t\tipc.opt = &icmp_param->replyopts.opt;",
                "\t\tif (ipc.opt->opt.srr)",
                "\t\t\tdaddr = icmp_param->replyopts.opt.opt.faddr;"
            ],
            "deleted": [
                "\tif (ip_options_echo(&icmp_param->replyopts, skb))",
                "\tif (icmp_param->replyopts.optlen) {",
                "\t\tipc.opt = &icmp_param->replyopts;",
                "\t\tif (ipc.opt->srr)",
                "\t\t\tdaddr = icmp_param->replyopts.faddr;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "void icmp_send(struct sk_buff *skb_in, int type, int code, __be32 info)\n{\n\tstruct iphdr *iph;\n\tint room;\n\tstruct icmp_bxm icmp_param;\n\tstruct rtable *rt = skb_rtable(skb_in);\n\tstruct ipcm_cookie ipc;\n\t__be32 saddr;\n\tu8  tos;\n\tstruct net *net;\n\tstruct sock *sk;\n\n\tif (!rt)\n\t\tgoto out;\n\tnet = dev_net(rt->dst.dev);\n\n\t/*\n\t *\tFind the original header. It is expected to be valid, of course.\n\t *\tCheck this, icmp_send is called from the most obscure devices\n\t *\tsometimes.\n\t */\n\tiph = ip_hdr(skb_in);\n\n\tif ((u8 *)iph < skb_in->head ||\n\t    (skb_in->network_header + sizeof(*iph)) > skb_in->tail)\n\t\tgoto out;\n\n\t/*\n\t *\tNo replies to physical multicast/broadcast\n\t */\n\tif (skb_in->pkt_type != PACKET_HOST)\n\t\tgoto out;\n\n\t/*\n\t *\tNow check at the protocol level\n\t */\n\tif (rt->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))\n\t\tgoto out;\n\n\t/*\n\t *\tOnly reply to fragment 0. We byte re-order the constant\n\t *\tmask for efficiency.\n\t */\n\tif (iph->frag_off & htons(IP_OFFSET))\n\t\tgoto out;\n\n\t/*\n\t *\tIf we send an ICMP error to an ICMP error a mess would result..\n\t */\n\tif (icmp_pointers[type].error) {\n\t\t/*\n\t\t *\tWe are an error, check if we are replying to an\n\t\t *\tICMP error\n\t\t */\n\t\tif (iph->protocol == IPPROTO_ICMP) {\n\t\t\tu8 _inner_type, *itp;\n\n\t\t\titp = skb_header_pointer(skb_in,\n\t\t\t\t\t\t skb_network_header(skb_in) +\n\t\t\t\t\t\t (iph->ihl << 2) +\n\t\t\t\t\t\t offsetof(struct icmphdr,\n\t\t\t\t\t\t\t  type) -\n\t\t\t\t\t\t skb_in->data,\n\t\t\t\t\t\t sizeof(_inner_type),\n\t\t\t\t\t\t &_inner_type);\n\t\t\tif (itp == NULL)\n\t\t\t\tgoto out;\n\n\t\t\t/*\n\t\t\t *\tAssume any unknown ICMP type is an error. This\n\t\t\t *\tisn't specified by the RFC, but think about it..\n\t\t\t */\n\t\t\tif (*itp > NR_ICMP_TYPES ||\n\t\t\t    icmp_pointers[*itp].error)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\n\t/*\n\t *\tConstruct source address and options.\n\t */\n\n\tsaddr = iph->daddr;\n\tif (!(rt->rt_flags & RTCF_LOCAL)) {\n\t\tstruct net_device *dev = NULL;\n\n\t\trcu_read_lock();\n\t\tif (rt_is_input_route(rt) &&\n\t\t    net->ipv4.sysctl_icmp_errors_use_inbound_ifaddr)\n\t\t\tdev = dev_get_by_index_rcu(net, rt->rt_iif);\n\n\t\tif (dev)\n\t\t\tsaddr = inet_select_addr(dev, 0, RT_SCOPE_LINK);\n\t\telse\n\t\t\tsaddr = 0;\n\t\trcu_read_unlock();\n\t}\n\n\ttos = icmp_pointers[type].error ? ((iph->tos & IPTOS_TOS_MASK) |\n\t\t\t\t\t   IPTOS_PREC_INTERNETCONTROL) :\n\t\t\t\t\t  iph->tos;\n\n\tif (ip_options_echo(&icmp_param.replyopts, skb_in))\n\t\tgoto out_unlock;\n\n\n\t/*\n\t *\tPrepare data for ICMP header.\n\t */\n\n\ticmp_param.data.icmph.type\t = type;\n\ticmp_param.data.icmph.code\t = code;\n\ticmp_param.data.icmph.un.gateway = info;\n\ticmp_param.data.icmph.checksum\t = 0;\n\ticmp_param.skb\t  = skb_in;\n\ticmp_param.offset = skb_network_offset(skb_in);\n\tinet_sk(sk)->tos = tos;\n\tipc.addr = iph->saddr;\n\tipc.opt = &icmp_param.replyopts;\n\tipc.tx_flags = 0;\n\n\trt = icmp_route_lookup(net, skb_in, iph, saddr, tos,\n\t\t\t       type, code, &icmp_param);\n\tif (IS_ERR(rt))\n\t\tgoto out_unlock;\n\n\tif (!icmpv4_xrlim_allow(net, rt, type, code))\n\t\tgoto ende;\n\n\t/* RFC says return as much as we can without exceeding 576 bytes. */\n\n\troom = dst_mtu(&rt->dst);\n\tif (room > 576)\n\t\troom = 576;\n\troom -= sizeof(struct iphdr) + icmp_param.replyopts.optlen;\n\troom -= sizeof(struct icmphdr);\n\n\ticmp_param.data_len = skb_in->len - icmp_param.offset;\n\tif (icmp_param.data_len > room)\n\t\ticmp_param.data_len = room;\n\ticmp_param.head_len = sizeof(struct icmphdr);\n\n\ticmp_push_reply(&icmp_param, &ipc, &rt);\nende:\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\nout:;\n}",
        "code_after_change": "void icmp_send(struct sk_buff *skb_in, int type, int code, __be32 info)\n{\n\tstruct iphdr *iph;\n\tint room;\n\tstruct icmp_bxm icmp_param;\n\tstruct rtable *rt = skb_rtable(skb_in);\n\tstruct ipcm_cookie ipc;\n\t__be32 saddr;\n\tu8  tos;\n\tstruct net *net;\n\tstruct sock *sk;\n\n\tif (!rt)\n\t\tgoto out;\n\tnet = dev_net(rt->dst.dev);\n\n\t/*\n\t *\tFind the original header. It is expected to be valid, of course.\n\t *\tCheck this, icmp_send is called from the most obscure devices\n\t *\tsometimes.\n\t */\n\tiph = ip_hdr(skb_in);\n\n\tif ((u8 *)iph < skb_in->head ||\n\t    (skb_in->network_header + sizeof(*iph)) > skb_in->tail)\n\t\tgoto out;\n\n\t/*\n\t *\tNo replies to physical multicast/broadcast\n\t */\n\tif (skb_in->pkt_type != PACKET_HOST)\n\t\tgoto out;\n\n\t/*\n\t *\tNow check at the protocol level\n\t */\n\tif (rt->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))\n\t\tgoto out;\n\n\t/*\n\t *\tOnly reply to fragment 0. We byte re-order the constant\n\t *\tmask for efficiency.\n\t */\n\tif (iph->frag_off & htons(IP_OFFSET))\n\t\tgoto out;\n\n\t/*\n\t *\tIf we send an ICMP error to an ICMP error a mess would result..\n\t */\n\tif (icmp_pointers[type].error) {\n\t\t/*\n\t\t *\tWe are an error, check if we are replying to an\n\t\t *\tICMP error\n\t\t */\n\t\tif (iph->protocol == IPPROTO_ICMP) {\n\t\t\tu8 _inner_type, *itp;\n\n\t\t\titp = skb_header_pointer(skb_in,\n\t\t\t\t\t\t skb_network_header(skb_in) +\n\t\t\t\t\t\t (iph->ihl << 2) +\n\t\t\t\t\t\t offsetof(struct icmphdr,\n\t\t\t\t\t\t\t  type) -\n\t\t\t\t\t\t skb_in->data,\n\t\t\t\t\t\t sizeof(_inner_type),\n\t\t\t\t\t\t &_inner_type);\n\t\t\tif (itp == NULL)\n\t\t\t\tgoto out;\n\n\t\t\t/*\n\t\t\t *\tAssume any unknown ICMP type is an error. This\n\t\t\t *\tisn't specified by the RFC, but think about it..\n\t\t\t */\n\t\t\tif (*itp > NR_ICMP_TYPES ||\n\t\t\t    icmp_pointers[*itp].error)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\n\t/*\n\t *\tConstruct source address and options.\n\t */\n\n\tsaddr = iph->daddr;\n\tif (!(rt->rt_flags & RTCF_LOCAL)) {\n\t\tstruct net_device *dev = NULL;\n\n\t\trcu_read_lock();\n\t\tif (rt_is_input_route(rt) &&\n\t\t    net->ipv4.sysctl_icmp_errors_use_inbound_ifaddr)\n\t\t\tdev = dev_get_by_index_rcu(net, rt->rt_iif);\n\n\t\tif (dev)\n\t\t\tsaddr = inet_select_addr(dev, 0, RT_SCOPE_LINK);\n\t\telse\n\t\t\tsaddr = 0;\n\t\trcu_read_unlock();\n\t}\n\n\ttos = icmp_pointers[type].error ? ((iph->tos & IPTOS_TOS_MASK) |\n\t\t\t\t\t   IPTOS_PREC_INTERNETCONTROL) :\n\t\t\t\t\t  iph->tos;\n\n\tif (ip_options_echo(&icmp_param.replyopts.opt.opt, skb_in))\n\t\tgoto out_unlock;\n\n\n\t/*\n\t *\tPrepare data for ICMP header.\n\t */\n\n\ticmp_param.data.icmph.type\t = type;\n\ticmp_param.data.icmph.code\t = code;\n\ticmp_param.data.icmph.un.gateway = info;\n\ticmp_param.data.icmph.checksum\t = 0;\n\ticmp_param.skb\t  = skb_in;\n\ticmp_param.offset = skb_network_offset(skb_in);\n\tinet_sk(sk)->tos = tos;\n\tipc.addr = iph->saddr;\n\tipc.opt = &icmp_param.replyopts.opt;\n\tipc.tx_flags = 0;\n\n\trt = icmp_route_lookup(net, skb_in, iph, saddr, tos,\n\t\t\t       type, code, &icmp_param);\n\tif (IS_ERR(rt))\n\t\tgoto out_unlock;\n\n\tif (!icmpv4_xrlim_allow(net, rt, type, code))\n\t\tgoto ende;\n\n\t/* RFC says return as much as we can without exceeding 576 bytes. */\n\n\troom = dst_mtu(&rt->dst);\n\tif (room > 576)\n\t\troom = 576;\n\troom -= sizeof(struct iphdr) + icmp_param.replyopts.opt.opt.optlen;\n\troom -= sizeof(struct icmphdr);\n\n\ticmp_param.data_len = skb_in->len - icmp_param.offset;\n\tif (icmp_param.data_len > room)\n\t\ticmp_param.data_len = room;\n\ticmp_param.head_len = sizeof(struct icmphdr);\n\n\ticmp_push_reply(&icmp_param, &ipc, &rt);\nende:\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\nout:;\n}",
        "patch": "--- code before\n+++ code after\n@@ -104,7 +104,7 @@\n \t\t\t\t\t   IPTOS_PREC_INTERNETCONTROL) :\n \t\t\t\t\t  iph->tos;\n \n-\tif (ip_options_echo(&icmp_param.replyopts, skb_in))\n+\tif (ip_options_echo(&icmp_param.replyopts.opt.opt, skb_in))\n \t\tgoto out_unlock;\n \n \n@@ -120,7 +120,7 @@\n \ticmp_param.offset = skb_network_offset(skb_in);\n \tinet_sk(sk)->tos = tos;\n \tipc.addr = iph->saddr;\n-\tipc.opt = &icmp_param.replyopts;\n+\tipc.opt = &icmp_param.replyopts.opt;\n \tipc.tx_flags = 0;\n \n \trt = icmp_route_lookup(net, skb_in, iph, saddr, tos,\n@@ -136,7 +136,7 @@\n \troom = dst_mtu(&rt->dst);\n \tif (room > 576)\n \t\troom = 576;\n-\troom -= sizeof(struct iphdr) + icmp_param.replyopts.optlen;\n+\troom -= sizeof(struct iphdr) + icmp_param.replyopts.opt.opt.optlen;\n \troom -= sizeof(struct icmphdr);\n \n \ticmp_param.data_len = skb_in->len - icmp_param.offset;",
        "function_modified_lines": {
            "added": [
                "\tif (ip_options_echo(&icmp_param.replyopts.opt.opt, skb_in))",
                "\tipc.opt = &icmp_param.replyopts.opt;",
                "\troom -= sizeof(struct iphdr) + icmp_param.replyopts.opt.opt.optlen;"
            ],
            "deleted": [
                "\tif (ip_options_echo(&icmp_param.replyopts, skb_in))",
                "\tipc.opt = &icmp_param.replyopts;",
                "\troom -= sizeof(struct iphdr) + icmp_param.replyopts.optlen;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static struct rtable *icmp_route_lookup(struct net *net, struct sk_buff *skb_in,\n\t\t\t\t\tconst struct iphdr *iph,\n\t\t\t\t\t__be32 saddr, u8 tos,\n\t\t\t\t\tint type, int code,\n\t\t\t\t\tstruct icmp_bxm *param)\n{\n\tstruct flowi4 fl4 = {\n\t\t.daddr = (param->replyopts.srr ?\n\t\t\t  param->replyopts.faddr : iph->saddr),\n\t\t.saddr = saddr,\n\t\t.flowi4_tos = RT_TOS(tos),\n\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t.fl4_icmp_type = type,\n\t\t.fl4_icmp_code = code,\n\t};\n\tstruct rtable *rt, *rt2;\n\tint err;\n\n\tsecurity_skb_classify_flow(skb_in, flowi4_to_flowi(&fl4));\n\trt = __ip_route_output_key(net, &fl4);\n\tif (IS_ERR(rt))\n\t\treturn rt;\n\n\t/* No need to clone since we're just using its address. */\n\trt2 = rt;\n\n\tif (!fl4.saddr)\n\t\tfl4.saddr = rt->rt_src;\n\n\trt = (struct rtable *) xfrm_lookup(net, &rt->dst,\n\t\t\t\t\t   flowi4_to_flowi(&fl4), NULL, 0);\n\tif (!IS_ERR(rt)) {\n\t\tif (rt != rt2)\n\t\t\treturn rt;\n\t} else if (PTR_ERR(rt) == -EPERM) {\n\t\trt = NULL;\n\t} else\n\t\treturn rt;\n\n\terr = xfrm_decode_session_reverse(skb_in, flowi4_to_flowi(&fl4), AF_INET);\n\tif (err)\n\t\tgoto relookup_failed;\n\n\tif (inet_addr_type(net, fl4.saddr) == RTN_LOCAL) {\n\t\trt2 = __ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt2))\n\t\t\terr = PTR_ERR(rt2);\n\t} else {\n\t\tstruct flowi4 fl4_2 = {};\n\t\tunsigned long orefdst;\n\n\t\tfl4_2.daddr = fl4.saddr;\n\t\trt2 = ip_route_output_key(net, &fl4_2);\n\t\tif (IS_ERR(rt2)) {\n\t\t\terr = PTR_ERR(rt2);\n\t\t\tgoto relookup_failed;\n\t\t}\n\t\t/* Ugh! */\n\t\torefdst = skb_in->_skb_refdst; /* save old refdst */\n\t\terr = ip_route_input(skb_in, fl4.daddr, fl4.saddr,\n\t\t\t\t     RT_TOS(tos), rt2->dst.dev);\n\n\t\tdst_release(&rt2->dst);\n\t\trt2 = skb_rtable(skb_in);\n\t\tskb_in->_skb_refdst = orefdst; /* restore old refdst */\n\t}\n\n\tif (err)\n\t\tgoto relookup_failed;\n\n\trt2 = (struct rtable *) xfrm_lookup(net, &rt2->dst,\n\t\t\t\t\t    flowi4_to_flowi(&fl4), NULL,\n\t\t\t\t\t    XFRM_LOOKUP_ICMP);\n\tif (!IS_ERR(rt2)) {\n\t\tdst_release(&rt->dst);\n\t\trt = rt2;\n\t} else if (PTR_ERR(rt2) == -EPERM) {\n\t\tif (rt)\n\t\t\tdst_release(&rt->dst);\n\t\treturn rt2;\n\t} else {\n\t\terr = PTR_ERR(rt2);\n\t\tgoto relookup_failed;\n\t}\n\treturn rt;\n\nrelookup_failed:\n\tif (rt)\n\t\treturn rt;\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "static struct rtable *icmp_route_lookup(struct net *net, struct sk_buff *skb_in,\n\t\t\t\t\tconst struct iphdr *iph,\n\t\t\t\t\t__be32 saddr, u8 tos,\n\t\t\t\t\tint type, int code,\n\t\t\t\t\tstruct icmp_bxm *param)\n{\n\tstruct flowi4 fl4 = {\n\t\t.daddr = (param->replyopts.opt.opt.srr ?\n\t\t\t  param->replyopts.opt.opt.faddr : iph->saddr),\n\t\t.saddr = saddr,\n\t\t.flowi4_tos = RT_TOS(tos),\n\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t.fl4_icmp_type = type,\n\t\t.fl4_icmp_code = code,\n\t};\n\tstruct rtable *rt, *rt2;\n\tint err;\n\n\tsecurity_skb_classify_flow(skb_in, flowi4_to_flowi(&fl4));\n\trt = __ip_route_output_key(net, &fl4);\n\tif (IS_ERR(rt))\n\t\treturn rt;\n\n\t/* No need to clone since we're just using its address. */\n\trt2 = rt;\n\n\tif (!fl4.saddr)\n\t\tfl4.saddr = rt->rt_src;\n\n\trt = (struct rtable *) xfrm_lookup(net, &rt->dst,\n\t\t\t\t\t   flowi4_to_flowi(&fl4), NULL, 0);\n\tif (!IS_ERR(rt)) {\n\t\tif (rt != rt2)\n\t\t\treturn rt;\n\t} else if (PTR_ERR(rt) == -EPERM) {\n\t\trt = NULL;\n\t} else\n\t\treturn rt;\n\n\terr = xfrm_decode_session_reverse(skb_in, flowi4_to_flowi(&fl4), AF_INET);\n\tif (err)\n\t\tgoto relookup_failed;\n\n\tif (inet_addr_type(net, fl4.saddr) == RTN_LOCAL) {\n\t\trt2 = __ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt2))\n\t\t\terr = PTR_ERR(rt2);\n\t} else {\n\t\tstruct flowi4 fl4_2 = {};\n\t\tunsigned long orefdst;\n\n\t\tfl4_2.daddr = fl4.saddr;\n\t\trt2 = ip_route_output_key(net, &fl4_2);\n\t\tif (IS_ERR(rt2)) {\n\t\t\terr = PTR_ERR(rt2);\n\t\t\tgoto relookup_failed;\n\t\t}\n\t\t/* Ugh! */\n\t\torefdst = skb_in->_skb_refdst; /* save old refdst */\n\t\terr = ip_route_input(skb_in, fl4.daddr, fl4.saddr,\n\t\t\t\t     RT_TOS(tos), rt2->dst.dev);\n\n\t\tdst_release(&rt2->dst);\n\t\trt2 = skb_rtable(skb_in);\n\t\tskb_in->_skb_refdst = orefdst; /* restore old refdst */\n\t}\n\n\tif (err)\n\t\tgoto relookup_failed;\n\n\trt2 = (struct rtable *) xfrm_lookup(net, &rt2->dst,\n\t\t\t\t\t    flowi4_to_flowi(&fl4), NULL,\n\t\t\t\t\t    XFRM_LOOKUP_ICMP);\n\tif (!IS_ERR(rt2)) {\n\t\tdst_release(&rt->dst);\n\t\trt = rt2;\n\t} else if (PTR_ERR(rt2) == -EPERM) {\n\t\tif (rt)\n\t\t\tdst_release(&rt->dst);\n\t\treturn rt2;\n\t} else {\n\t\terr = PTR_ERR(rt2);\n\t\tgoto relookup_failed;\n\t}\n\treturn rt;\n\nrelookup_failed:\n\tif (rt)\n\t\treturn rt;\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,8 @@\n \t\t\t\t\tstruct icmp_bxm *param)\n {\n \tstruct flowi4 fl4 = {\n-\t\t.daddr = (param->replyopts.srr ?\n-\t\t\t  param->replyopts.faddr : iph->saddr),\n+\t\t.daddr = (param->replyopts.opt.opt.srr ?\n+\t\t\t  param->replyopts.opt.opt.faddr : iph->saddr),\n \t\t.saddr = saddr,\n \t\t.flowi4_tos = RT_TOS(tos),\n \t\t.flowi4_proto = IPPROTO_ICMP,",
        "function_modified_lines": {
            "added": [
                "\t\t.daddr = (param->replyopts.opt.opt.srr ?",
                "\t\t\t  param->replyopts.opt.opt.faddr : iph->saddr),"
            ],
            "deleted": [
                "\t\t.daddr = (param->replyopts.srr ?",
                "\t\t\t  param->replyopts.faddr : iph->saddr),"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "struct dst_entry *inet_csk_route_req(struct sock *sk,\n\t\t\t\t     const struct request_sock *req)\n{\n\tstruct rtable *rt;\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ip_options *opt = inet_rsk(req)->opt;\n\tstruct net *net = sock_net(sk);\n\tstruct flowi4 fl4;\n\n\tflowi4_init_output(&fl4, sk->sk_bound_dev_if, sk->sk_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,\n\t\t\t   ireq->loc_addr, ireq->rmt_port, inet_sk(sk)->inet_sport);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto route_err;\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}",
        "code_after_change": "struct dst_entry *inet_csk_route_req(struct sock *sk,\n\t\t\t\t     const struct request_sock *req)\n{\n\tstruct rtable *rt;\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ip_options_rcu *opt = inet_rsk(req)->opt;\n\tstruct net *net = sock_net(sk);\n\tstruct flowi4 fl4;\n\n\tflowi4_init_output(&fl4, sk->sk_bound_dev_if, sk->sk_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->rmt_addr,\n\t\t\t   ireq->loc_addr, ireq->rmt_port, inet_sk(sk)->inet_sport);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto route_err;\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,20 +3,20 @@\n {\n \tstruct rtable *rt;\n \tconst struct inet_request_sock *ireq = inet_rsk(req);\n-\tstruct ip_options *opt = inet_rsk(req)->opt;\n+\tstruct ip_options_rcu *opt = inet_rsk(req)->opt;\n \tstruct net *net = sock_net(sk);\n \tstruct flowi4 fl4;\n \n \tflowi4_init_output(&fl4, sk->sk_bound_dev_if, sk->sk_mark,\n \t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n \t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n-\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,\n+\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->rmt_addr,\n \t\t\t   ireq->loc_addr, ireq->rmt_port, inet_sk(sk)->inet_sport);\n \tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n \trt = ip_route_output_flow(net, &fl4, sk);\n \tif (IS_ERR(rt))\n \t\tgoto no_route;\n-\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)\n+\tif (opt && opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)\n \t\tgoto route_err;\n \treturn &rt->dst;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *opt = inet_rsk(req)->opt;",
                "\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->rmt_addr,",
                "\tif (opt && opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)"
            ],
            "deleted": [
                "\tstruct ip_options *opt = inet_rsk(req)->opt;",
                "\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,",
                "\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "void ip_options_build(struct sk_buff * skb, struct ip_options * opt,\n\t\t\t    __be32 daddr, struct rtable *rt, int is_frag)\n{\n\tunsigned char *iph = skb_network_header(skb);\n\n\tmemcpy(&(IPCB(skb)->opt), opt, sizeof(struct ip_options));\n\tmemcpy(iph+sizeof(struct iphdr), opt->__data, opt->optlen);\n\topt = &(IPCB(skb)->opt);\n\n\tif (opt->srr)\n\t\tmemcpy(iph+opt->srr+iph[opt->srr+1]-4, &daddr, 4);\n\n\tif (!is_frag) {\n\t\tif (opt->rr_needaddr)\n\t\t\tip_rt_get_source(iph+opt->rr+iph[opt->rr+2]-5, rt);\n\t\tif (opt->ts_needaddr)\n\t\t\tip_rt_get_source(iph+opt->ts+iph[opt->ts+2]-9, rt);\n\t\tif (opt->ts_needtime) {\n\t\t\tstruct timespec tv;\n\t\t\t__be32 midtime;\n\t\t\tgetnstimeofday(&tv);\n\t\t\tmidtime = htonl((tv.tv_sec % 86400) * MSEC_PER_SEC + tv.tv_nsec / NSEC_PER_MSEC);\n\t\t\tmemcpy(iph+opt->ts+iph[opt->ts+2]-5, &midtime, 4);\n\t\t}\n\t\treturn;\n\t}\n\tif (opt->rr) {\n\t\tmemset(iph+opt->rr, IPOPT_NOP, iph[opt->rr+1]);\n\t\topt->rr = 0;\n\t\topt->rr_needaddr = 0;\n\t}\n\tif (opt->ts) {\n\t\tmemset(iph+opt->ts, IPOPT_NOP, iph[opt->ts+1]);\n\t\topt->ts = 0;\n\t\topt->ts_needaddr = opt->ts_needtime = 0;\n\t}\n}",
        "code_after_change": "void ip_options_build(struct sk_buff *skb, struct ip_options *opt,\n\t\t\t    __be32 daddr, struct rtable *rt, int is_frag)\n{\n\tunsigned char *iph = skb_network_header(skb);\n\n\tmemcpy(&(IPCB(skb)->opt), opt, sizeof(struct ip_options));\n\tmemcpy(iph+sizeof(struct iphdr), opt->__data, opt->optlen);\n\topt = &(IPCB(skb)->opt);\n\n\tif (opt->srr)\n\t\tmemcpy(iph+opt->srr+iph[opt->srr+1]-4, &daddr, 4);\n\n\tif (!is_frag) {\n\t\tif (opt->rr_needaddr)\n\t\t\tip_rt_get_source(iph+opt->rr+iph[opt->rr+2]-5, rt);\n\t\tif (opt->ts_needaddr)\n\t\t\tip_rt_get_source(iph+opt->ts+iph[opt->ts+2]-9, rt);\n\t\tif (opt->ts_needtime) {\n\t\t\tstruct timespec tv;\n\t\t\t__be32 midtime;\n\t\t\tgetnstimeofday(&tv);\n\t\t\tmidtime = htonl((tv.tv_sec % 86400) * MSEC_PER_SEC + tv.tv_nsec / NSEC_PER_MSEC);\n\t\t\tmemcpy(iph+opt->ts+iph[opt->ts+2]-5, &midtime, 4);\n\t\t}\n\t\treturn;\n\t}\n\tif (opt->rr) {\n\t\tmemset(iph+opt->rr, IPOPT_NOP, iph[opt->rr+1]);\n\t\topt->rr = 0;\n\t\topt->rr_needaddr = 0;\n\t}\n\tif (opt->ts) {\n\t\tmemset(iph+opt->ts, IPOPT_NOP, iph[opt->ts+1]);\n\t\topt->ts = 0;\n\t\topt->ts_needaddr = opt->ts_needtime = 0;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-void ip_options_build(struct sk_buff * skb, struct ip_options * opt,\n+void ip_options_build(struct sk_buff *skb, struct ip_options *opt,\n \t\t\t    __be32 daddr, struct rtable *rt, int is_frag)\n {\n \tunsigned char *iph = skb_network_header(skb);",
        "function_modified_lines": {
            "added": [
                "void ip_options_build(struct sk_buff *skb, struct ip_options *opt,"
            ],
            "deleted": [
                "void ip_options_build(struct sk_buff * skb, struct ip_options * opt,"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static struct ip_options *ip_options_get_alloc(const int optlen)\n{\n\treturn kzalloc(sizeof(struct ip_options) + ((optlen + 3) & ~3),\n\t\t       GFP_KERNEL);\n}",
        "code_after_change": "static struct ip_options_rcu *ip_options_get_alloc(const int optlen)\n{\n\treturn kzalloc(sizeof(struct ip_options_rcu) + ((optlen + 3) & ~3),\n\t\t       GFP_KERNEL);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,5 @@\n-static struct ip_options *ip_options_get_alloc(const int optlen)\n+static struct ip_options_rcu *ip_options_get_alloc(const int optlen)\n {\n-\treturn kzalloc(sizeof(struct ip_options) + ((optlen + 3) & ~3),\n+\treturn kzalloc(sizeof(struct ip_options_rcu) + ((optlen + 3) & ~3),\n \t\t       GFP_KERNEL);\n }",
        "function_modified_lines": {
            "added": [
                "static struct ip_options_rcu *ip_options_get_alloc(const int optlen)",
                "\treturn kzalloc(sizeof(struct ip_options_rcu) + ((optlen + 3) & ~3),"
            ],
            "deleted": [
                "static struct ip_options *ip_options_get_alloc(const int optlen)",
                "\treturn kzalloc(sizeof(struct ip_options) + ((optlen + 3) & ~3),"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "int ip_options_echo(struct ip_options * dopt, struct sk_buff * skb)\n{\n\tstruct ip_options *sopt;\n\tunsigned char *sptr, *dptr;\n\tint soffset, doffset;\n\tint\toptlen;\n\t__be32\tdaddr;\n\n\tmemset(dopt, 0, sizeof(struct ip_options));\n\n\tsopt = &(IPCB(skb)->opt);\n\n\tif (sopt->optlen == 0) {\n\t\tdopt->optlen = 0;\n\t\treturn 0;\n\t}\n\n\tsptr = skb_network_header(skb);\n\tdptr = dopt->__data;\n\n\tdaddr = skb_rtable(skb)->rt_spec_dst;\n\n\tif (sopt->rr) {\n\t\toptlen  = sptr[sopt->rr+1];\n\t\tsoffset = sptr[sopt->rr+2];\n\t\tdopt->rr = dopt->optlen + sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->rr, optlen);\n\t\tif (sopt->rr_needaddr && soffset <= optlen) {\n\t\t\tif (soffset + 3 > optlen)\n\t\t\t\treturn -EINVAL;\n\t\t\tdptr[2] = soffset + 4;\n\t\t\tdopt->rr_needaddr = 1;\n\t\t}\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\tif (sopt->ts) {\n\t\toptlen = sptr[sopt->ts+1];\n\t\tsoffset = sptr[sopt->ts+2];\n\t\tdopt->ts = dopt->optlen + sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->ts, optlen);\n\t\tif (soffset <= optlen) {\n\t\t\tif (sopt->ts_needaddr) {\n\t\t\t\tif (soffset + 3 > optlen)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tdopt->ts_needaddr = 1;\n\t\t\t\tsoffset += 4;\n\t\t\t}\n\t\t\tif (sopt->ts_needtime) {\n\t\t\t\tif (soffset + 3 > optlen)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif ((dptr[3]&0xF) != IPOPT_TS_PRESPEC) {\n\t\t\t\t\tdopt->ts_needtime = 1;\n\t\t\t\t\tsoffset += 4;\n\t\t\t\t} else {\n\t\t\t\t\tdopt->ts_needtime = 0;\n\n\t\t\t\t\tif (soffset + 7 <= optlen) {\n\t\t\t\t\t\t__be32 addr;\n\n\t\t\t\t\t\tmemcpy(&addr, dptr+soffset-1, 4);\n\t\t\t\t\t\tif (inet_addr_type(dev_net(skb_dst(skb)->dev), addr) != RTN_UNICAST) {\n\t\t\t\t\t\t\tdopt->ts_needtime = 1;\n\t\t\t\t\t\t\tsoffset += 8;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tdptr[2] = soffset;\n\t\t}\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\tif (sopt->srr) {\n\t\tunsigned char * start = sptr+sopt->srr;\n\t\t__be32 faddr;\n\n\t\toptlen  = start[1];\n\t\tsoffset = start[2];\n\t\tdoffset = 0;\n\t\tif (soffset > optlen)\n\t\t\tsoffset = optlen + 1;\n\t\tsoffset -= 4;\n\t\tif (soffset > 3) {\n\t\t\tmemcpy(&faddr, &start[soffset-1], 4);\n\t\t\tfor (soffset-=4, doffset=4; soffset > 3; soffset-=4, doffset+=4)\n\t\t\t\tmemcpy(&dptr[doffset-1], &start[soffset-1], 4);\n\t\t\t/*\n\t\t\t * RFC1812 requires to fix illegal source routes.\n\t\t\t */\n\t\t\tif (memcmp(&ip_hdr(skb)->saddr,\n\t\t\t\t   &start[soffset + 3], 4) == 0)\n\t\t\t\tdoffset -= 4;\n\t\t}\n\t\tif (doffset > 3) {\n\t\t\tmemcpy(&start[doffset-1], &daddr, 4);\n\t\t\tdopt->faddr = faddr;\n\t\t\tdptr[0] = start[0];\n\t\t\tdptr[1] = doffset+3;\n\t\t\tdptr[2] = 4;\n\t\t\tdptr += doffset+3;\n\t\t\tdopt->srr = dopt->optlen + sizeof(struct iphdr);\n\t\t\tdopt->optlen += doffset+3;\n\t\t\tdopt->is_strictroute = sopt->is_strictroute;\n\t\t}\n\t}\n\tif (sopt->cipso) {\n\t\toptlen  = sptr[sopt->cipso+1];\n\t\tdopt->cipso = dopt->optlen+sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->cipso, optlen);\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\twhile (dopt->optlen & 3) {\n\t\t*dptr++ = IPOPT_END;\n\t\tdopt->optlen++;\n\t}\n\treturn 0;\n}",
        "code_after_change": "int ip_options_echo(struct ip_options *dopt, struct sk_buff *skb)\n{\n\tconst struct ip_options *sopt;\n\tunsigned char *sptr, *dptr;\n\tint soffset, doffset;\n\tint\toptlen;\n\t__be32\tdaddr;\n\n\tmemset(dopt, 0, sizeof(struct ip_options));\n\n\tsopt = &(IPCB(skb)->opt);\n\n\tif (sopt->optlen == 0)\n\t\treturn 0;\n\n\tsptr = skb_network_header(skb);\n\tdptr = dopt->__data;\n\n\tdaddr = skb_rtable(skb)->rt_spec_dst;\n\n\tif (sopt->rr) {\n\t\toptlen  = sptr[sopt->rr+1];\n\t\tsoffset = sptr[sopt->rr+2];\n\t\tdopt->rr = dopt->optlen + sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->rr, optlen);\n\t\tif (sopt->rr_needaddr && soffset <= optlen) {\n\t\t\tif (soffset + 3 > optlen)\n\t\t\t\treturn -EINVAL;\n\t\t\tdptr[2] = soffset + 4;\n\t\t\tdopt->rr_needaddr = 1;\n\t\t}\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\tif (sopt->ts) {\n\t\toptlen = sptr[sopt->ts+1];\n\t\tsoffset = sptr[sopt->ts+2];\n\t\tdopt->ts = dopt->optlen + sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->ts, optlen);\n\t\tif (soffset <= optlen) {\n\t\t\tif (sopt->ts_needaddr) {\n\t\t\t\tif (soffset + 3 > optlen)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tdopt->ts_needaddr = 1;\n\t\t\t\tsoffset += 4;\n\t\t\t}\n\t\t\tif (sopt->ts_needtime) {\n\t\t\t\tif (soffset + 3 > optlen)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif ((dptr[3]&0xF) != IPOPT_TS_PRESPEC) {\n\t\t\t\t\tdopt->ts_needtime = 1;\n\t\t\t\t\tsoffset += 4;\n\t\t\t\t} else {\n\t\t\t\t\tdopt->ts_needtime = 0;\n\n\t\t\t\t\tif (soffset + 7 <= optlen) {\n\t\t\t\t\t\t__be32 addr;\n\n\t\t\t\t\t\tmemcpy(&addr, dptr+soffset-1, 4);\n\t\t\t\t\t\tif (inet_addr_type(dev_net(skb_dst(skb)->dev), addr) != RTN_UNICAST) {\n\t\t\t\t\t\t\tdopt->ts_needtime = 1;\n\t\t\t\t\t\t\tsoffset += 8;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tdptr[2] = soffset;\n\t\t}\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\tif (sopt->srr) {\n\t\tunsigned char *start = sptr+sopt->srr;\n\t\t__be32 faddr;\n\n\t\toptlen  = start[1];\n\t\tsoffset = start[2];\n\t\tdoffset = 0;\n\t\tif (soffset > optlen)\n\t\t\tsoffset = optlen + 1;\n\t\tsoffset -= 4;\n\t\tif (soffset > 3) {\n\t\t\tmemcpy(&faddr, &start[soffset-1], 4);\n\t\t\tfor (soffset-=4, doffset=4; soffset > 3; soffset-=4, doffset+=4)\n\t\t\t\tmemcpy(&dptr[doffset-1], &start[soffset-1], 4);\n\t\t\t/*\n\t\t\t * RFC1812 requires to fix illegal source routes.\n\t\t\t */\n\t\t\tif (memcmp(&ip_hdr(skb)->saddr,\n\t\t\t\t   &start[soffset + 3], 4) == 0)\n\t\t\t\tdoffset -= 4;\n\t\t}\n\t\tif (doffset > 3) {\n\t\t\tmemcpy(&start[doffset-1], &daddr, 4);\n\t\t\tdopt->faddr = faddr;\n\t\t\tdptr[0] = start[0];\n\t\t\tdptr[1] = doffset+3;\n\t\t\tdptr[2] = 4;\n\t\t\tdptr += doffset+3;\n\t\t\tdopt->srr = dopt->optlen + sizeof(struct iphdr);\n\t\t\tdopt->optlen += doffset+3;\n\t\t\tdopt->is_strictroute = sopt->is_strictroute;\n\t\t}\n\t}\n\tif (sopt->cipso) {\n\t\toptlen  = sptr[sopt->cipso+1];\n\t\tdopt->cipso = dopt->optlen+sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->cipso, optlen);\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\twhile (dopt->optlen & 3) {\n\t\t*dptr++ = IPOPT_END;\n\t\tdopt->optlen++;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n-int ip_options_echo(struct ip_options * dopt, struct sk_buff * skb)\n+int ip_options_echo(struct ip_options *dopt, struct sk_buff *skb)\n {\n-\tstruct ip_options *sopt;\n+\tconst struct ip_options *sopt;\n \tunsigned char *sptr, *dptr;\n \tint soffset, doffset;\n \tint\toptlen;\n@@ -10,10 +10,8 @@\n \n \tsopt = &(IPCB(skb)->opt);\n \n-\tif (sopt->optlen == 0) {\n-\t\tdopt->optlen = 0;\n+\tif (sopt->optlen == 0)\n \t\treturn 0;\n-\t}\n \n \tsptr = skb_network_header(skb);\n \tdptr = dopt->__data;\n@@ -72,7 +70,7 @@\n \t\tdopt->optlen += optlen;\n \t}\n \tif (sopt->srr) {\n-\t\tunsigned char * start = sptr+sopt->srr;\n+\t\tunsigned char *start = sptr+sopt->srr;\n \t\t__be32 faddr;\n \n \t\toptlen  = start[1];",
        "function_modified_lines": {
            "added": [
                "int ip_options_echo(struct ip_options *dopt, struct sk_buff *skb)",
                "\tconst struct ip_options *sopt;",
                "\tif (sopt->optlen == 0)",
                "\t\tunsigned char *start = sptr+sopt->srr;"
            ],
            "deleted": [
                "int ip_options_echo(struct ip_options * dopt, struct sk_buff * skb)",
                "\tstruct ip_options *sopt;",
                "\tif (sopt->optlen == 0) {",
                "\t\tdopt->optlen = 0;",
                "\t}",
                "\t\tunsigned char * start = sptr+sopt->srr;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static int ip_setup_cork(struct sock *sk, struct inet_cork *cork,\n\t\t\t struct ipcm_cookie *ipc, struct rtable **rtp)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options *opt;\n\tstruct rtable *rt;\n\n\t/*\n\t * setup for corking.\n\t */\n\topt = ipc->opt;\n\tif (opt) {\n\t\tif (cork->opt == NULL) {\n\t\t\tcork->opt = kmalloc(sizeof(struct ip_options) + 40,\n\t\t\t\t\t    sk->sk_allocation);\n\t\t\tif (unlikely(cork->opt == NULL))\n\t\t\t\treturn -ENOBUFS;\n\t\t}\n\t\tmemcpy(cork->opt, opt, sizeof(struct ip_options) + opt->optlen);\n\t\tcork->flags |= IPCORK_OPT;\n\t\tcork->addr = ipc->addr;\n\t}\n\trt = *rtp;\n\tif (unlikely(!rt))\n\t\treturn -EFAULT;\n\t/*\n\t * We steal reference to this route, caller should not release it\n\t */\n\t*rtp = NULL;\n\tcork->fragsize = inet->pmtudisc == IP_PMTUDISC_PROBE ?\n\t\t\t rt->dst.dev->mtu : dst_mtu(rt->dst.path);\n\tcork->dst = &rt->dst;\n\tcork->length = 0;\n\tcork->tx_flags = ipc->tx_flags;\n\tcork->page = NULL;\n\tcork->off = 0;\n\n\treturn 0;\n}",
        "code_after_change": "static int ip_setup_cork(struct sock *sk, struct inet_cork *cork,\n\t\t\t struct ipcm_cookie *ipc, struct rtable **rtp)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options_rcu *opt;\n\tstruct rtable *rt;\n\n\t/*\n\t * setup for corking.\n\t */\n\topt = ipc->opt;\n\tif (opt) {\n\t\tif (cork->opt == NULL) {\n\t\t\tcork->opt = kmalloc(sizeof(struct ip_options) + 40,\n\t\t\t\t\t    sk->sk_allocation);\n\t\t\tif (unlikely(cork->opt == NULL))\n\t\t\t\treturn -ENOBUFS;\n\t\t}\n\t\tmemcpy(cork->opt, &opt->opt, sizeof(struct ip_options) + opt->opt.optlen);\n\t\tcork->flags |= IPCORK_OPT;\n\t\tcork->addr = ipc->addr;\n\t}\n\trt = *rtp;\n\tif (unlikely(!rt))\n\t\treturn -EFAULT;\n\t/*\n\t * We steal reference to this route, caller should not release it\n\t */\n\t*rtp = NULL;\n\tcork->fragsize = inet->pmtudisc == IP_PMTUDISC_PROBE ?\n\t\t\t rt->dst.dev->mtu : dst_mtu(rt->dst.path);\n\tcork->dst = &rt->dst;\n\tcork->length = 0;\n\tcork->tx_flags = ipc->tx_flags;\n\tcork->page = NULL;\n\tcork->off = 0;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n \t\t\t struct ipcm_cookie *ipc, struct rtable **rtp)\n {\n \tstruct inet_sock *inet = inet_sk(sk);\n-\tstruct ip_options *opt;\n+\tstruct ip_options_rcu *opt;\n \tstruct rtable *rt;\n \n \t/*\n@@ -16,7 +16,7 @@\n \t\t\tif (unlikely(cork->opt == NULL))\n \t\t\t\treturn -ENOBUFS;\n \t\t}\n-\t\tmemcpy(cork->opt, opt, sizeof(struct ip_options) + opt->optlen);\n+\t\tmemcpy(cork->opt, &opt->opt, sizeof(struct ip_options) + opt->opt.optlen);\n \t\tcork->flags |= IPCORK_OPT;\n \t\tcork->addr = ipc->addr;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *opt;",
                "\t\tmemcpy(cork->opt, &opt->opt, sizeof(struct ip_options) + opt->opt.optlen);"
            ],
            "deleted": [
                "\tstruct ip_options *opt;",
                "\t\tmemcpy(cork->opt, opt, sizeof(struct ip_options) + opt->optlen);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "void ip_send_reply(struct sock *sk, struct sk_buff *skb, struct ip_reply_arg *arg,\n\t\t   unsigned int len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct {\n\t\tstruct ip_options\topt;\n\t\tchar\t\t\tdata[40];\n\t} replyopts;\n\tstruct ipcm_cookie ipc;\n\t__be32 daddr;\n\tstruct rtable *rt = skb_rtable(skb);\n\n\tif (ip_options_echo(&replyopts.opt, skb))\n\t\treturn;\n\n\tdaddr = ipc.addr = rt->rt_src;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\n\tif (replyopts.opt.optlen) {\n\t\tipc.opt = &replyopts.opt;\n\n\t\tif (ipc.opt->srr)\n\t\t\tdaddr = replyopts.opt.faddr;\n\t}\n\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, arg->bound_dev_if, 0,\n\t\t\t\t   RT_TOS(ip_hdr(skb)->tos),\n\t\t\t\t   RT_SCOPE_UNIVERSE, sk->sk_protocol,\n\t\t\t\t   ip_reply_arg_flowi_flags(arg),\n\t\t\t\t   daddr, rt->rt_spec_dst,\n\t\t\t\t   tcp_hdr(skb)->source, tcp_hdr(skb)->dest);\n\t\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(sock_net(sk), &fl4);\n\t\tif (IS_ERR(rt))\n\t\t\treturn;\n\t}\n\n\t/* And let IP do all the hard work.\n\n\t   This chunk is not reenterable, hence spinlock.\n\t   Note that it uses the fact, that this function is called\n\t   with locally disabled BH and that sk cannot be already spinlocked.\n\t */\n\tbh_lock_sock(sk);\n\tinet->tos = ip_hdr(skb)->tos;\n\tsk->sk_priority = skb->priority;\n\tsk->sk_protocol = ip_hdr(skb)->protocol;\n\tsk->sk_bound_dev_if = arg->bound_dev_if;\n\tip_append_data(sk, ip_reply_glue_bits, arg->iov->iov_base, len, 0,\n\t\t       &ipc, &rt, MSG_DONTWAIT);\n\tif ((skb = skb_peek(&sk->sk_write_queue)) != NULL) {\n\t\tif (arg->csumoffset >= 0)\n\t\t\t*((__sum16 *)skb_transport_header(skb) +\n\t\t\t  arg->csumoffset) = csum_fold(csum_add(skb->csum,\n\t\t\t\t\t\t\t\targ->csum));\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tip_push_pending_frames(sk);\n\t}\n\n\tbh_unlock_sock(sk);\n\n\tip_rt_put(rt);\n}",
        "code_after_change": "void ip_send_reply(struct sock *sk, struct sk_buff *skb, struct ip_reply_arg *arg,\n\t\t   unsigned int len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options_data replyopts;\n\tstruct ipcm_cookie ipc;\n\t__be32 daddr;\n\tstruct rtable *rt = skb_rtable(skb);\n\n\tif (ip_options_echo(&replyopts.opt.opt, skb))\n\t\treturn;\n\n\tdaddr = ipc.addr = rt->rt_src;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\n\tif (replyopts.opt.opt.optlen) {\n\t\tipc.opt = &replyopts.opt;\n\n\t\tif (replyopts.opt.opt.srr)\n\t\t\tdaddr = replyopts.opt.opt.faddr;\n\t}\n\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, arg->bound_dev_if, 0,\n\t\t\t\t   RT_TOS(ip_hdr(skb)->tos),\n\t\t\t\t   RT_SCOPE_UNIVERSE, sk->sk_protocol,\n\t\t\t\t   ip_reply_arg_flowi_flags(arg),\n\t\t\t\t   daddr, rt->rt_spec_dst,\n\t\t\t\t   tcp_hdr(skb)->source, tcp_hdr(skb)->dest);\n\t\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(sock_net(sk), &fl4);\n\t\tif (IS_ERR(rt))\n\t\t\treturn;\n\t}\n\n\t/* And let IP do all the hard work.\n\n\t   This chunk is not reenterable, hence spinlock.\n\t   Note that it uses the fact, that this function is called\n\t   with locally disabled BH and that sk cannot be already spinlocked.\n\t */\n\tbh_lock_sock(sk);\n\tinet->tos = ip_hdr(skb)->tos;\n\tsk->sk_priority = skb->priority;\n\tsk->sk_protocol = ip_hdr(skb)->protocol;\n\tsk->sk_bound_dev_if = arg->bound_dev_if;\n\tip_append_data(sk, ip_reply_glue_bits, arg->iov->iov_base, len, 0,\n\t\t       &ipc, &rt, MSG_DONTWAIT);\n\tif ((skb = skb_peek(&sk->sk_write_queue)) != NULL) {\n\t\tif (arg->csumoffset >= 0)\n\t\t\t*((__sum16 *)skb_transport_header(skb) +\n\t\t\t  arg->csumoffset) = csum_fold(csum_add(skb->csum,\n\t\t\t\t\t\t\t\targ->csum));\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tip_push_pending_frames(sk);\n\t}\n\n\tbh_unlock_sock(sk);\n\n\tip_rt_put(rt);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,26 +2,23 @@\n \t\t   unsigned int len)\n {\n \tstruct inet_sock *inet = inet_sk(sk);\n-\tstruct {\n-\t\tstruct ip_options\topt;\n-\t\tchar\t\t\tdata[40];\n-\t} replyopts;\n+\tstruct ip_options_data replyopts;\n \tstruct ipcm_cookie ipc;\n \t__be32 daddr;\n \tstruct rtable *rt = skb_rtable(skb);\n \n-\tif (ip_options_echo(&replyopts.opt, skb))\n+\tif (ip_options_echo(&replyopts.opt.opt, skb))\n \t\treturn;\n \n \tdaddr = ipc.addr = rt->rt_src;\n \tipc.opt = NULL;\n \tipc.tx_flags = 0;\n \n-\tif (replyopts.opt.optlen) {\n+\tif (replyopts.opt.opt.optlen) {\n \t\tipc.opt = &replyopts.opt;\n \n-\t\tif (ipc.opt->srr)\n-\t\t\tdaddr = replyopts.opt.faddr;\n+\t\tif (replyopts.opt.opt.srr)\n+\t\t\tdaddr = replyopts.opt.opt.faddr;\n \t}\n \n \t{",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_data replyopts;",
                "\tif (ip_options_echo(&replyopts.opt.opt, skb))",
                "\tif (replyopts.opt.opt.optlen) {",
                "\t\tif (replyopts.opt.opt.srr)",
                "\t\t\tdaddr = replyopts.opt.opt.faddr;"
            ],
            "deleted": [
                "\tstruct {",
                "\t\tstruct ip_options\topt;",
                "\t\tchar\t\t\tdata[40];",
                "\t} replyopts;",
                "\tif (ip_options_echo(&replyopts.opt, skb))",
                "\tif (replyopts.opt.optlen) {",
                "\t\tif (ipc.opt->srr)",
                "\t\t\tdaddr = replyopts.opt.faddr;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "int ip_queue_xmit(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options *opt = inet->opt;\n\tstruct rtable *rt;\n\tstruct iphdr *iph;\n\tint res;\n\n\t/* Skip all of this if the packet is already routed,\n\t * f.e. by something like SCTP.\n\t */\n\trcu_read_lock();\n\trt = skb_rtable(skb);\n\tif (rt != NULL)\n\t\tgoto packet_routed;\n\n\t/* Make sure we can route this packet. */\n\trt = (struct rtable *)__sk_dst_check(sk, 0);\n\tif (rt == NULL) {\n\t\t__be32 daddr;\n\n\t\t/* Use correct destination address if we have options. */\n\t\tdaddr = inet->inet_daddr;\n\t\tif(opt && opt->srr)\n\t\t\tdaddr = opt->faddr;\n\n\t\t/* If this fails, retransmit mechanism of transport layer will\n\t\t * keep trying until route appears or the connection times\n\t\t * itself out.\n\t\t */\n\t\trt = ip_route_output_ports(sock_net(sk), sk,\n\t\t\t\t\t   daddr, inet->inet_saddr,\n\t\t\t\t\t   inet->inet_dport,\n\t\t\t\t\t   inet->inet_sport,\n\t\t\t\t\t   sk->sk_protocol,\n\t\t\t\t\t   RT_CONN_FLAGS(sk),\n\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto no_route;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t}\n\tskb_dst_set_noref(skb, &rt->dst);\n\npacket_routed:\n\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto no_route;\n\n\t/* OK, we know where to send it, allocate and build IP header. */\n\tskb_push(skb, sizeof(struct iphdr) + (opt ? opt->optlen : 0));\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\t*((__be16 *)iph) = htons((4 << 12) | (5 << 8) | (inet->tos & 0xff));\n\tif (ip_dont_fragment(sk, &rt->dst) && !skb->local_df)\n\t\tiph->frag_off = htons(IP_DF);\n\telse\n\t\tiph->frag_off = 0;\n\tiph->ttl      = ip_select_ttl(inet, &rt->dst);\n\tiph->protocol = sk->sk_protocol;\n\tiph->saddr    = rt->rt_src;\n\tiph->daddr    = rt->rt_dst;\n\t/* Transport layer set skb->h.foo itself. */\n\n\tif (opt && opt->optlen) {\n\t\tiph->ihl += opt->optlen >> 2;\n\t\tip_options_build(skb, opt, inet->inet_daddr, rt, 0);\n\t}\n\n\tip_select_ident_more(iph, &rt->dst, sk,\n\t\t\t     (skb_shinfo(skb)->gso_segs ?: 1) - 1);\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tres = ip_local_out(skb);\n\trcu_read_unlock();\n\treturn res;\n\nno_route:\n\trcu_read_unlock();\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EHOSTUNREACH;\n}",
        "code_after_change": "int ip_queue_xmit(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options_rcu *inet_opt;\n\tstruct rtable *rt;\n\tstruct iphdr *iph;\n\tint res;\n\n\t/* Skip all of this if the packet is already routed,\n\t * f.e. by something like SCTP.\n\t */\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\trt = skb_rtable(skb);\n\tif (rt != NULL)\n\t\tgoto packet_routed;\n\n\t/* Make sure we can route this packet. */\n\trt = (struct rtable *)__sk_dst_check(sk, 0);\n\tif (rt == NULL) {\n\t\t__be32 daddr;\n\n\t\t/* Use correct destination address if we have options. */\n\t\tdaddr = inet->inet_daddr;\n\t\tif (inet_opt && inet_opt->opt.srr)\n\t\t\tdaddr = inet_opt->opt.faddr;\n\n\t\t/* If this fails, retransmit mechanism of transport layer will\n\t\t * keep trying until route appears or the connection times\n\t\t * itself out.\n\t\t */\n\t\trt = ip_route_output_ports(sock_net(sk), sk,\n\t\t\t\t\t   daddr, inet->inet_saddr,\n\t\t\t\t\t   inet->inet_dport,\n\t\t\t\t\t   inet->inet_sport,\n\t\t\t\t\t   sk->sk_protocol,\n\t\t\t\t\t   RT_CONN_FLAGS(sk),\n\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto no_route;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t}\n\tskb_dst_set_noref(skb, &rt->dst);\n\npacket_routed:\n\tif (inet_opt && inet_opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto no_route;\n\n\t/* OK, we know where to send it, allocate and build IP header. */\n\tskb_push(skb, sizeof(struct iphdr) + (inet_opt ? inet_opt->opt.optlen : 0));\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\t*((__be16 *)iph) = htons((4 << 12) | (5 << 8) | (inet->tos & 0xff));\n\tif (ip_dont_fragment(sk, &rt->dst) && !skb->local_df)\n\t\tiph->frag_off = htons(IP_DF);\n\telse\n\t\tiph->frag_off = 0;\n\tiph->ttl      = ip_select_ttl(inet, &rt->dst);\n\tiph->protocol = sk->sk_protocol;\n\tiph->saddr    = rt->rt_src;\n\tiph->daddr    = rt->rt_dst;\n\t/* Transport layer set skb->h.foo itself. */\n\n\tif (inet_opt && inet_opt->opt.optlen) {\n\t\tiph->ihl += inet_opt->opt.optlen >> 2;\n\t\tip_options_build(skb, &inet_opt->opt, inet->inet_daddr, rt, 0);\n\t}\n\n\tip_select_ident_more(iph, &rt->dst, sk,\n\t\t\t     (skb_shinfo(skb)->gso_segs ?: 1) - 1);\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tres = ip_local_out(skb);\n\trcu_read_unlock();\n\treturn res;\n\nno_route:\n\trcu_read_unlock();\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EHOSTUNREACH;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tstruct sock *sk = skb->sk;\n \tstruct inet_sock *inet = inet_sk(sk);\n-\tstruct ip_options *opt = inet->opt;\n+\tstruct ip_options_rcu *inet_opt;\n \tstruct rtable *rt;\n \tstruct iphdr *iph;\n \tint res;\n@@ -11,6 +11,7 @@\n \t * f.e. by something like SCTP.\n \t */\n \trcu_read_lock();\n+\tinet_opt = rcu_dereference(inet->inet_opt);\n \trt = skb_rtable(skb);\n \tif (rt != NULL)\n \t\tgoto packet_routed;\n@@ -22,8 +23,8 @@\n \n \t\t/* Use correct destination address if we have options. */\n \t\tdaddr = inet->inet_daddr;\n-\t\tif(opt && opt->srr)\n-\t\t\tdaddr = opt->faddr;\n+\t\tif (inet_opt && inet_opt->opt.srr)\n+\t\t\tdaddr = inet_opt->opt.faddr;\n \n \t\t/* If this fails, retransmit mechanism of transport layer will\n \t\t * keep trying until route appears or the connection times\n@@ -43,11 +44,11 @@\n \tskb_dst_set_noref(skb, &rt->dst);\n \n packet_routed:\n-\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)\n+\tif (inet_opt && inet_opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)\n \t\tgoto no_route;\n \n \t/* OK, we know where to send it, allocate and build IP header. */\n-\tskb_push(skb, sizeof(struct iphdr) + (opt ? opt->optlen : 0));\n+\tskb_push(skb, sizeof(struct iphdr) + (inet_opt ? inet_opt->opt.optlen : 0));\n \tskb_reset_network_header(skb);\n \tiph = ip_hdr(skb);\n \t*((__be16 *)iph) = htons((4 << 12) | (5 << 8) | (inet->tos & 0xff));\n@@ -61,9 +62,9 @@\n \tiph->daddr    = rt->rt_dst;\n \t/* Transport layer set skb->h.foo itself. */\n \n-\tif (opt && opt->optlen) {\n-\t\tiph->ihl += opt->optlen >> 2;\n-\t\tip_options_build(skb, opt, inet->inet_daddr, rt, 0);\n+\tif (inet_opt && inet_opt->opt.optlen) {\n+\t\tiph->ihl += inet_opt->opt.optlen >> 2;\n+\t\tip_options_build(skb, &inet_opt->opt, inet->inet_daddr, rt, 0);\n \t}\n \n \tip_select_ident_more(iph, &rt->dst, sk,",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *inet_opt;",
                "\tinet_opt = rcu_dereference(inet->inet_opt);",
                "\t\tif (inet_opt && inet_opt->opt.srr)",
                "\t\t\tdaddr = inet_opt->opt.faddr;",
                "\tif (inet_opt && inet_opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)",
                "\tskb_push(skb, sizeof(struct iphdr) + (inet_opt ? inet_opt->opt.optlen : 0));",
                "\tif (inet_opt && inet_opt->opt.optlen) {",
                "\t\tiph->ihl += inet_opt->opt.optlen >> 2;",
                "\t\tip_options_build(skb, &inet_opt->opt, inet->inet_daddr, rt, 0);"
            ],
            "deleted": [
                "\tstruct ip_options *opt = inet->opt;",
                "\t\tif(opt && opt->srr)",
                "\t\t\tdaddr = opt->faddr;",
                "\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)",
                "\tskb_push(skb, sizeof(struct iphdr) + (opt ? opt->optlen : 0));",
                "\tif (opt && opt->optlen) {",
                "\t\tiph->ihl += opt->optlen >> 2;",
                "\t\tip_options_build(skb, opt, inet->inet_daddr, rt, 0);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static int do_ip_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint val;\n\tint len;\n\n\tif (level != SOL_IP)\n\t\treturn -EOPNOTSUPP;\n\n\tif (ip_mroute_opt(optname))\n\t\treturn ip_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase IP_OPTIONS:\n\t{\n\t\tunsigned char optbuf[sizeof(struct ip_options)+40];\n\t\tstruct ip_options * opt = (struct ip_options *)optbuf;\n\t\topt->optlen = 0;\n\t\tif (inet->opt)\n\t\t\tmemcpy(optbuf, inet->opt,\n\t\t\t       sizeof(struct ip_options)+\n\t\t\t       inet->opt->optlen);\n\t\trelease_sock(sk);\n\n\t\tif (opt->optlen == 0)\n\t\t\treturn put_user(0, optlen);\n\n\t\tip_options_undo(opt);\n\n\t\tlen = min_t(unsigned int, len, opt->optlen);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, opt->__data, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase IP_PKTINFO:\n\t\tval = (inet->cmsg_flags & IP_CMSG_PKTINFO) != 0;\n\t\tbreak;\n\tcase IP_RECVTTL:\n\t\tval = (inet->cmsg_flags & IP_CMSG_TTL) != 0;\n\t\tbreak;\n\tcase IP_RECVTOS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_TOS) != 0;\n\t\tbreak;\n\tcase IP_RECVOPTS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_RECVOPTS) != 0;\n\t\tbreak;\n\tcase IP_RETOPTS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_RETOPTS) != 0;\n\t\tbreak;\n\tcase IP_PASSSEC:\n\t\tval = (inet->cmsg_flags & IP_CMSG_PASSSEC) != 0;\n\t\tbreak;\n\tcase IP_RECVORIGDSTADDR:\n\t\tval = (inet->cmsg_flags & IP_CMSG_ORIGDSTADDR) != 0;\n\t\tbreak;\n\tcase IP_TOS:\n\t\tval = inet->tos;\n\t\tbreak;\n\tcase IP_TTL:\n\t\tval = (inet->uc_ttl == -1 ?\n\t\t       sysctl_ip_default_ttl :\n\t\t       inet->uc_ttl);\n\t\tbreak;\n\tcase IP_HDRINCL:\n\t\tval = inet->hdrincl;\n\t\tbreak;\n\tcase IP_NODEFRAG:\n\t\tval = inet->nodefrag;\n\t\tbreak;\n\tcase IP_MTU_DISCOVER:\n\t\tval = inet->pmtudisc;\n\t\tbreak;\n\tcase IP_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tval = 0;\n\t\tdst = sk_dst_get(sk);\n\t\tif (dst) {\n\t\t\tval = dst_mtu(dst);\n\t\t\tdst_release(dst);\n\t\t}\n\t\tif (!val) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -ENOTCONN;\n\t\t}\n\t\tbreak;\n\t}\n\tcase IP_RECVERR:\n\t\tval = inet->recverr;\n\t\tbreak;\n\tcase IP_MULTICAST_TTL:\n\t\tval = inet->mc_ttl;\n\t\tbreak;\n\tcase IP_MULTICAST_LOOP:\n\t\tval = inet->mc_loop;\n\t\tbreak;\n\tcase IP_MULTICAST_IF:\n\t{\n\t\tstruct in_addr addr;\n\t\tlen = min_t(unsigned int, len, sizeof(struct in_addr));\n\t\taddr.s_addr = inet->mc_addr;\n\t\trelease_sock(sk);\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &addr, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase IP_MSFILTER:\n\t{\n\t\tstruct ip_msfilter msf;\n\t\tint err;\n\n\t\tif (len < IP_MSFILTER_SIZE(0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (copy_from_user(&msf, optval, IP_MSFILTER_SIZE(0))) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\terr = ip_mc_msfget(sk, &msf,\n\t\t\t\t   (struct ip_msfilter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0))) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\terr = ip_mc_gsfget(sk, &gsf,\n\t\t\t\t   (struct group_filter __user *)optval,\n\t\t\t\t   optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tcase IP_MULTICAST_ALL:\n\t\tval = inet->mc_all;\n\t\tbreak;\n\tcase IP_PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\n\t\trelease_sock(sk);\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = 0;\n\n\t\tif (inet->cmsg_flags & IP_CMSG_PKTINFO) {\n\t\t\tstruct in_pktinfo info;\n\n\t\t\tinfo.ipi_addr.s_addr = inet->inet_rcv_saddr;\n\t\t\tinfo.ipi_spec_dst.s_addr = inet->inet_rcv_saddr;\n\t\t\tinfo.ipi_ifindex = inet->mc_index;\n\t\t\tput_cmsg(&msg, SOL_IP, IP_PKTINFO, sizeof(info), &info);\n\t\t}\n\t\tif (inet->cmsg_flags & IP_CMSG_TTL) {\n\t\t\tint hlim = inet->mc_ttl;\n\t\t\tput_cmsg(&msg, SOL_IP, IP_TTL, sizeof(hlim), &hlim);\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IP_FREEBIND:\n\t\tval = inet->freebind;\n\t\tbreak;\n\tcase IP_TRANSPARENT:\n\t\tval = inet->transparent;\n\t\tbreak;\n\tcase IP_MINTTL:\n\t\tval = inet->min_ttl;\n\t\tbreak;\n\tdefault:\n\t\trelease_sock(sk);\n\t\treturn -ENOPROTOOPT;\n\t}\n\trelease_sock(sk);\n\n\tif (len < sizeof(int) && len > 0 && val >= 0 && val <= 255) {\n\t\tunsigned char ucval = (unsigned char)val;\n\t\tlen = 1;\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &ucval, 1))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\tlen = min_t(unsigned int, sizeof(int), len);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &val, len))\n\t\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int do_ip_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint val;\n\tint len;\n\n\tif (level != SOL_IP)\n\t\treturn -EOPNOTSUPP;\n\n\tif (ip_mroute_opt(optname))\n\t\treturn ip_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase IP_OPTIONS:\n\t{\n\t\tunsigned char optbuf[sizeof(struct ip_options)+40];\n\t\tstruct ip_options *opt = (struct ip_options *)optbuf;\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t\t     sock_owned_by_user(sk));\n\t\topt->optlen = 0;\n\t\tif (inet_opt)\n\t\t\tmemcpy(optbuf, &inet_opt->opt,\n\t\t\t       sizeof(struct ip_options) +\n\t\t\t       inet_opt->opt.optlen);\n\t\trelease_sock(sk);\n\n\t\tif (opt->optlen == 0)\n\t\t\treturn put_user(0, optlen);\n\n\t\tip_options_undo(opt);\n\n\t\tlen = min_t(unsigned int, len, opt->optlen);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, opt->__data, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase IP_PKTINFO:\n\t\tval = (inet->cmsg_flags & IP_CMSG_PKTINFO) != 0;\n\t\tbreak;\n\tcase IP_RECVTTL:\n\t\tval = (inet->cmsg_flags & IP_CMSG_TTL) != 0;\n\t\tbreak;\n\tcase IP_RECVTOS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_TOS) != 0;\n\t\tbreak;\n\tcase IP_RECVOPTS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_RECVOPTS) != 0;\n\t\tbreak;\n\tcase IP_RETOPTS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_RETOPTS) != 0;\n\t\tbreak;\n\tcase IP_PASSSEC:\n\t\tval = (inet->cmsg_flags & IP_CMSG_PASSSEC) != 0;\n\t\tbreak;\n\tcase IP_RECVORIGDSTADDR:\n\t\tval = (inet->cmsg_flags & IP_CMSG_ORIGDSTADDR) != 0;\n\t\tbreak;\n\tcase IP_TOS:\n\t\tval = inet->tos;\n\t\tbreak;\n\tcase IP_TTL:\n\t\tval = (inet->uc_ttl == -1 ?\n\t\t       sysctl_ip_default_ttl :\n\t\t       inet->uc_ttl);\n\t\tbreak;\n\tcase IP_HDRINCL:\n\t\tval = inet->hdrincl;\n\t\tbreak;\n\tcase IP_NODEFRAG:\n\t\tval = inet->nodefrag;\n\t\tbreak;\n\tcase IP_MTU_DISCOVER:\n\t\tval = inet->pmtudisc;\n\t\tbreak;\n\tcase IP_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tval = 0;\n\t\tdst = sk_dst_get(sk);\n\t\tif (dst) {\n\t\t\tval = dst_mtu(dst);\n\t\t\tdst_release(dst);\n\t\t}\n\t\tif (!val) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -ENOTCONN;\n\t\t}\n\t\tbreak;\n\t}\n\tcase IP_RECVERR:\n\t\tval = inet->recverr;\n\t\tbreak;\n\tcase IP_MULTICAST_TTL:\n\t\tval = inet->mc_ttl;\n\t\tbreak;\n\tcase IP_MULTICAST_LOOP:\n\t\tval = inet->mc_loop;\n\t\tbreak;\n\tcase IP_MULTICAST_IF:\n\t{\n\t\tstruct in_addr addr;\n\t\tlen = min_t(unsigned int, len, sizeof(struct in_addr));\n\t\taddr.s_addr = inet->mc_addr;\n\t\trelease_sock(sk);\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &addr, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase IP_MSFILTER:\n\t{\n\t\tstruct ip_msfilter msf;\n\t\tint err;\n\n\t\tif (len < IP_MSFILTER_SIZE(0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (copy_from_user(&msf, optval, IP_MSFILTER_SIZE(0))) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\terr = ip_mc_msfget(sk, &msf,\n\t\t\t\t   (struct ip_msfilter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0))) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\terr = ip_mc_gsfget(sk, &gsf,\n\t\t\t\t   (struct group_filter __user *)optval,\n\t\t\t\t   optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tcase IP_MULTICAST_ALL:\n\t\tval = inet->mc_all;\n\t\tbreak;\n\tcase IP_PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\n\t\trelease_sock(sk);\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = 0;\n\n\t\tif (inet->cmsg_flags & IP_CMSG_PKTINFO) {\n\t\t\tstruct in_pktinfo info;\n\n\t\t\tinfo.ipi_addr.s_addr = inet->inet_rcv_saddr;\n\t\t\tinfo.ipi_spec_dst.s_addr = inet->inet_rcv_saddr;\n\t\t\tinfo.ipi_ifindex = inet->mc_index;\n\t\t\tput_cmsg(&msg, SOL_IP, IP_PKTINFO, sizeof(info), &info);\n\t\t}\n\t\tif (inet->cmsg_flags & IP_CMSG_TTL) {\n\t\t\tint hlim = inet->mc_ttl;\n\t\t\tput_cmsg(&msg, SOL_IP, IP_TTL, sizeof(hlim), &hlim);\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IP_FREEBIND:\n\t\tval = inet->freebind;\n\t\tbreak;\n\tcase IP_TRANSPARENT:\n\t\tval = inet->transparent;\n\t\tbreak;\n\tcase IP_MINTTL:\n\t\tval = inet->min_ttl;\n\t\tbreak;\n\tdefault:\n\t\trelease_sock(sk);\n\t\treturn -ENOPROTOOPT;\n\t}\n\trelease_sock(sk);\n\n\tif (len < sizeof(int) && len > 0 && val >= 0 && val <= 255) {\n\t\tunsigned char ucval = (unsigned char)val;\n\t\tlen = 1;\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &ucval, 1))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\tlen = min_t(unsigned int, sizeof(int), len);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &val, len))\n\t\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,12 +22,16 @@\n \tcase IP_OPTIONS:\n \t{\n \t\tunsigned char optbuf[sizeof(struct ip_options)+40];\n-\t\tstruct ip_options * opt = (struct ip_options *)optbuf;\n+\t\tstruct ip_options *opt = (struct ip_options *)optbuf;\n+\t\tstruct ip_options_rcu *inet_opt;\n+\n+\t\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n+\t\t\t\t\t\t     sock_owned_by_user(sk));\n \t\topt->optlen = 0;\n-\t\tif (inet->opt)\n-\t\t\tmemcpy(optbuf, inet->opt,\n-\t\t\t       sizeof(struct ip_options)+\n-\t\t\t       inet->opt->optlen);\n+\t\tif (inet_opt)\n+\t\t\tmemcpy(optbuf, &inet_opt->opt,\n+\t\t\t       sizeof(struct ip_options) +\n+\t\t\t       inet_opt->opt.optlen);\n \t\trelease_sock(sk);\n \n \t\tif (opt->optlen == 0)",
        "function_modified_lines": {
            "added": [
                "\t\tstruct ip_options *opt = (struct ip_options *)optbuf;",
                "\t\tstruct ip_options_rcu *inet_opt;",
                "",
                "\t\tinet_opt = rcu_dereference_protected(inet->inet_opt,",
                "\t\t\t\t\t\t     sock_owned_by_user(sk));",
                "\t\tif (inet_opt)",
                "\t\t\tmemcpy(optbuf, &inet_opt->opt,",
                "\t\t\t       sizeof(struct ip_options) +",
                "\t\t\t       inet_opt->opt.optlen);"
            ],
            "deleted": [
                "\t\tstruct ip_options * opt = (struct ip_options *)optbuf;",
                "\t\tif (inet->opt)",
                "\t\t\tmemcpy(optbuf, inet->opt,",
                "\t\t\t       sizeof(struct ip_options)+",
                "\t\t\t       inet->opt->optlen);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static int do_ip_setsockopt(struct sock *sk, int level,\n\t\t\t    int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint val = 0, err;\n\n\tif (((1<<optname) & ((1<<IP_PKTINFO) | (1<<IP_RECVTTL) |\n\t\t\t     (1<<IP_RECVOPTS) | (1<<IP_RECVTOS) |\n\t\t\t     (1<<IP_RETOPTS) | (1<<IP_TOS) |\n\t\t\t     (1<<IP_TTL) | (1<<IP_HDRINCL) |\n\t\t\t     (1<<IP_MTU_DISCOVER) | (1<<IP_RECVERR) |\n\t\t\t     (1<<IP_ROUTER_ALERT) | (1<<IP_FREEBIND) |\n\t\t\t     (1<<IP_PASSSEC) | (1<<IP_TRANSPARENT) |\n\t\t\t     (1<<IP_MINTTL) | (1<<IP_NODEFRAG))) ||\n\t    optname == IP_MULTICAST_TTL ||\n\t    optname == IP_MULTICAST_ALL ||\n\t    optname == IP_MULTICAST_LOOP ||\n\t    optname == IP_RECVORIGDSTADDR) {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else if (optlen >= sizeof(char)) {\n\t\t\tunsigned char ucval;\n\n\t\t\tif (get_user(ucval, (unsigned char __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t\tval = (int) ucval;\n\t\t}\n\t}\n\n\t/* If optlen==0, it is equivalent to val == 0 */\n\n\tif (ip_mroute_opt(optname))\n\t\treturn ip_mroute_setsockopt(sk, optname, optval, optlen);\n\n\terr = 0;\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase IP_OPTIONS:\n\t{\n\t\tstruct ip_options *opt = NULL;\n\t\tif (optlen > 40)\n\t\t\tgoto e_inval;\n\t\terr = ip_options_get_from_user(sock_net(sk), &opt,\n\t\t\t\t\t       optval, optlen);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (inet->is_icsk) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\t\tif (sk->sk_family == PF_INET ||\n\t\t\t    (!((1 << sk->sk_state) &\n\t\t\t       (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t\t     inet->inet_daddr != LOOPBACK4_IPV6)) {\n#endif\n\t\t\t\tif (inet->opt)\n\t\t\t\t\ticsk->icsk_ext_hdr_len -= inet->opt->optlen;\n\t\t\t\tif (opt)\n\t\t\t\t\ticsk->icsk_ext_hdr_len += opt->optlen;\n\t\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\t\t}\n#endif\n\t\t}\n\t\topt = xchg(&inet->opt, opt);\n\t\tkfree(opt);\n\t\tbreak;\n\t}\n\tcase IP_PKTINFO:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_PKTINFO;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_PKTINFO;\n\t\tbreak;\n\tcase IP_RECVTTL:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_TTL;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_TTL;\n\t\tbreak;\n\tcase IP_RECVTOS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_TOS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_TOS;\n\t\tbreak;\n\tcase IP_RECVOPTS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_RECVOPTS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_RECVOPTS;\n\t\tbreak;\n\tcase IP_RETOPTS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_RETOPTS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_RETOPTS;\n\t\tbreak;\n\tcase IP_PASSSEC:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_PASSSEC;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_PASSSEC;\n\t\tbreak;\n\tcase IP_RECVORIGDSTADDR:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_ORIGDSTADDR;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_ORIGDSTADDR;\n\t\tbreak;\n\tcase IP_TOS:\t/* This sets both TOS and Precedence */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tval &= ~3;\n\t\t\tval |= inet->tos & 3;\n\t\t}\n\t\tif (inet->tos != val) {\n\t\t\tinet->tos = val;\n\t\t\tsk->sk_priority = rt_tos2priority(val);\n\t\t\tsk_dst_reset(sk);\n\t\t}\n\t\tbreak;\n\tcase IP_TTL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val != -1 && (val < 0 || val > 255))\n\t\t\tgoto e_inval;\n\t\tinet->uc_ttl = val;\n\t\tbreak;\n\tcase IP_HDRINCL:\n\t\tif (sk->sk_type != SOCK_RAW) {\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tinet->hdrincl = val ? 1 : 0;\n\t\tbreak;\n\tcase IP_NODEFRAG:\n\t\tif (sk->sk_type != SOCK_RAW) {\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tinet->nodefrag = val ? 1 : 0;\n\t\tbreak;\n\tcase IP_MTU_DISCOVER:\n\t\tif (val < IP_PMTUDISC_DONT || val > IP_PMTUDISC_PROBE)\n\t\t\tgoto e_inval;\n\t\tinet->pmtudisc = val;\n\t\tbreak;\n\tcase IP_RECVERR:\n\t\tinet->recverr = !!val;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tbreak;\n\tcase IP_MULTICAST_TTL:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tgoto e_inval;\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val == -1)\n\t\t\tval = 1;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tinet->mc_ttl = val;\n\t\tbreak;\n\tcase IP_MULTICAST_LOOP:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->mc_loop = !!val;\n\t\tbreak;\n\tcase IP_MULTICAST_IF:\n\t{\n\t\tstruct ip_mreqn mreq;\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tgoto e_inval;\n\t\t/*\n\t\t *\tCheck the arguments are allowable\n\t\t */\n\n\t\tif (optlen < sizeof(struct in_addr))\n\t\t\tgoto e_inval;\n\n\t\terr = -EFAULT;\n\t\tif (optlen >= sizeof(struct ip_mreqn)) {\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(mreq)))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\t\tif (optlen >= sizeof(struct in_addr) &&\n\t\t\t    copy_from_user(&mreq.imr_address, optval,\n\t\t\t\t\t   sizeof(struct in_addr)))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!mreq.imr_ifindex) {\n\t\t\tif (mreq.imr_address.s_addr == htonl(INADDR_ANY)) {\n\t\t\t\tinet->mc_index = 0;\n\t\t\t\tinet->mc_addr  = 0;\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev = ip_dev_find(sock_net(sk), mreq.imr_address.s_addr);\n\t\t\tif (dev)\n\t\t\t\tmreq.imr_ifindex = dev->ifindex;\n\t\t} else\n\t\t\tdev = dev_get_by_index(sock_net(sk), mreq.imr_ifindex);\n\n\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\terr = -EINVAL;\n\t\tif (sk->sk_bound_dev_if &&\n\t\t    mreq.imr_ifindex != sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tinet->mc_index = mreq.imr_ifindex;\n\t\tinet->mc_addr  = mreq.imr_address.s_addr;\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\tcase IP_ADD_MEMBERSHIP:\n\tcase IP_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ip_mreqn mreq;\n\n\t\terr = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tif (optlen < sizeof(struct ip_mreq))\n\t\t\tgoto e_inval;\n\t\terr = -EFAULT;\n\t\tif (optlen >= sizeof(struct ip_mreqn)) {\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(mreq)))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(struct ip_mreq)))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (optname == IP_ADD_MEMBERSHIP)\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\telse\n\t\t\terr = ip_mc_leave_group(sk, &mreq);\n\t\tbreak;\n\t}\n\tcase IP_MSFILTER:\n\t{\n\t\tstruct ip_msfilter *msf;\n\n\t\tif (optlen < IP_MSFILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tmsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(msf, optval, optlen)) {\n\t\t\tkfree(msf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (1G-4) overflow in 32 bits */\n\t\tif (msf->imsf_numsrc >= 0x3ffffffcU ||\n\t\t    msf->imsf_numsrc > sysctl_igmp_max_msf) {\n\t\t\tkfree(msf);\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (IP_MSFILTER_SIZE(msf->imsf_numsrc) > optlen) {\n\t\t\tkfree(msf);\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\terr = ip_mc_msfilter(sk, msf, 0);\n\t\tkfree(msf);\n\t\tbreak;\n\t}\n\tcase IP_BLOCK_SOURCE:\n\tcase IP_UNBLOCK_SOURCE:\n\tcase IP_ADD_SOURCE_MEMBERSHIP:\n\tcase IP_DROP_SOURCE_MEMBERSHIP:\n\t{\n\t\tstruct ip_mreq_source mreqs;\n\t\tint omode, add;\n\n\t\tif (optlen != sizeof(struct ip_mreq_source))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&mreqs, optval, sizeof(mreqs))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == IP_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == IP_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == IP_ADD_SOURCE_MEMBERSHIP) {\n\t\t\tstruct ip_mreqn mreq;\n\n\t\t\tmreq.imr_multiaddr.s_addr = mreqs.imr_multiaddr;\n\t\t\tmreq.imr_address.s_addr = mreqs.imr_interface;\n\t\t\tmreq.imr_ifindex = 0;\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\t\tif (err && err != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* IP_DROP_SOURCE_MEMBERSHIP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\terr = ip_mc_source(add, omode, sk, &mreqs, 0);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in *psin;\n\t\tstruct ip_mreqn mreq;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(greq)))\n\t\t\tbreak;\n\t\tpsin = (struct sockaddr_in *)&greq.gr_group;\n\t\tif (psin->sin_family != AF_INET)\n\t\t\tgoto e_inval;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tmreq.imr_multiaddr = psin->sin_addr;\n\t\tmreq.imr_ifindex = greq.gr_interface;\n\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\telse\n\t\t\terr = ip_mc_leave_group(sk, &mreq);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tstruct ip_mreq_source mreqs;\n\t\tstruct sockaddr_in *psin;\n\t\tint omode, add;\n\n\t\tif (optlen != sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET ||\n\t\t    greqs.gsr_source.ss_family != AF_INET) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin = (struct sockaddr_in *)&greqs.gsr_group;\n\t\tmreqs.imr_multiaddr = psin->sin_addr.s_addr;\n\t\tpsin = (struct sockaddr_in *)&greqs.gsr_source;\n\t\tmreqs.imr_sourceaddr = psin->sin_addr.s_addr;\n\t\tmreqs.imr_interface = 0; /* use index for mc_source */\n\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct ip_mreqn mreq;\n\n\t\t\tpsin = (struct sockaddr_in *)&greqs.gsr_group;\n\t\t\tmreq.imr_multiaddr = psin->sin_addr;\n\t\t\tmreq.imr_address.s_addr = 0;\n\t\t\tmreq.imr_ifindex = greqs.gsr_interface;\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\t\tif (err && err != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tgreqs.gsr_interface = mreq.imr_ifindex;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\terr = ip_mc_source(add, omode, sk, &mreqs,\n\t\t\t\t   greqs.gsr_interface);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct sockaddr_in *psin;\n\t\tstruct ip_msfilter *msf = NULL;\n\t\tstruct group_filter *gsf = NULL;\n\t\tint msize, i, ifindex;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen))\n\t\t\tgoto mc_msf_out;\n\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffff ||\n\t\t    gsf->gf_numsrc > sysctl_igmp_max_msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tmsize = IP_MSFILTER_SIZE(gsf->gf_numsrc);\n\t\tmsf = kmalloc(msize, GFP_KERNEL);\n\t\tif (!msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tifindex = gsf->gf_interface;\n\t\tpsin = (struct sockaddr_in *)&gsf->gf_group;\n\t\tif (psin->sin_family != AF_INET) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tmsf->imsf_multiaddr = psin->sin_addr.s_addr;\n\t\tmsf->imsf_interface = 0;\n\t\tmsf->imsf_fmode = gsf->gf_fmode;\n\t\tmsf->imsf_numsrc = gsf->gf_numsrc;\n\t\terr = -EADDRNOTAVAIL;\n\t\tfor (i = 0; i < gsf->gf_numsrc; ++i) {\n\t\t\tpsin = (struct sockaddr_in *)&gsf->gf_slist[i];\n\n\t\t\tif (psin->sin_family != AF_INET)\n\t\t\t\tgoto mc_msf_out;\n\t\t\tmsf->imsf_slist[i] = psin->sin_addr.s_addr;\n\t\t}\n\t\tkfree(gsf);\n\t\tgsf = NULL;\n\n\t\terr = ip_mc_msfilter(sk, msf, ifindex);\nmc_msf_out:\n\t\tkfree(msf);\n\t\tkfree(gsf);\n\t\tbreak;\n\t}\n\tcase IP_MULTICAST_ALL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val != 0 && val != 1)\n\t\t\tgoto e_inval;\n\t\tinet->mc_all = val;\n\t\tbreak;\n\tcase IP_ROUTER_ALERT:\n\t\terr = ip_ra_control(sk, val ? 1 : 0, NULL);\n\t\tbreak;\n\n\tcase IP_FREEBIND:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->freebind = !!val;\n\t\tbreak;\n\n\tcase IP_IPSEC_POLICY:\n\tcase IP_XFRM_POLICY:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\terr = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IP_TRANSPARENT:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\terr = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->transparent = !!val;\n\t\tbreak;\n\n\tcase IP_MINTTL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tinet->min_ttl = val;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn err;\n\ne_inval:\n\trelease_sock(sk);\n\treturn -EINVAL;\n}",
        "code_after_change": "static int do_ip_setsockopt(struct sock *sk, int level,\n\t\t\t    int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint val = 0, err;\n\n\tif (((1<<optname) & ((1<<IP_PKTINFO) | (1<<IP_RECVTTL) |\n\t\t\t     (1<<IP_RECVOPTS) | (1<<IP_RECVTOS) |\n\t\t\t     (1<<IP_RETOPTS) | (1<<IP_TOS) |\n\t\t\t     (1<<IP_TTL) | (1<<IP_HDRINCL) |\n\t\t\t     (1<<IP_MTU_DISCOVER) | (1<<IP_RECVERR) |\n\t\t\t     (1<<IP_ROUTER_ALERT) | (1<<IP_FREEBIND) |\n\t\t\t     (1<<IP_PASSSEC) | (1<<IP_TRANSPARENT) |\n\t\t\t     (1<<IP_MINTTL) | (1<<IP_NODEFRAG))) ||\n\t    optname == IP_MULTICAST_TTL ||\n\t    optname == IP_MULTICAST_ALL ||\n\t    optname == IP_MULTICAST_LOOP ||\n\t    optname == IP_RECVORIGDSTADDR) {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else if (optlen >= sizeof(char)) {\n\t\t\tunsigned char ucval;\n\n\t\t\tif (get_user(ucval, (unsigned char __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t\tval = (int) ucval;\n\t\t}\n\t}\n\n\t/* If optlen==0, it is equivalent to val == 0 */\n\n\tif (ip_mroute_opt(optname))\n\t\treturn ip_mroute_setsockopt(sk, optname, optval, optlen);\n\n\terr = 0;\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase IP_OPTIONS:\n\t{\n\t\tstruct ip_options_rcu *old, *opt = NULL;\n\n\t\tif (optlen > 40)\n\t\t\tgoto e_inval;\n\t\terr = ip_options_get_from_user(sock_net(sk), &opt,\n\t\t\t\t\t       optval, optlen);\n\t\tif (err)\n\t\t\tbreak;\n\t\told = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t\tsock_owned_by_user(sk));\n\t\tif (inet->is_icsk) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\t\tif (sk->sk_family == PF_INET ||\n\t\t\t    (!((1 << sk->sk_state) &\n\t\t\t       (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t\t     inet->inet_daddr != LOOPBACK4_IPV6)) {\n#endif\n\t\t\t\tif (old)\n\t\t\t\t\ticsk->icsk_ext_hdr_len -= old->opt.optlen;\n\t\t\t\tif (opt)\n\t\t\t\t\ticsk->icsk_ext_hdr_len += opt->opt.optlen;\n\t\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\t\t}\n#endif\n\t\t}\n\t\trcu_assign_pointer(inet->inet_opt, opt);\n\t\tif (old)\n\t\t\tcall_rcu(&old->rcu, opt_kfree_rcu);\n\t\tbreak;\n\t}\n\tcase IP_PKTINFO:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_PKTINFO;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_PKTINFO;\n\t\tbreak;\n\tcase IP_RECVTTL:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_TTL;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_TTL;\n\t\tbreak;\n\tcase IP_RECVTOS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_TOS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_TOS;\n\t\tbreak;\n\tcase IP_RECVOPTS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_RECVOPTS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_RECVOPTS;\n\t\tbreak;\n\tcase IP_RETOPTS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_RETOPTS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_RETOPTS;\n\t\tbreak;\n\tcase IP_PASSSEC:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_PASSSEC;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_PASSSEC;\n\t\tbreak;\n\tcase IP_RECVORIGDSTADDR:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_ORIGDSTADDR;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_ORIGDSTADDR;\n\t\tbreak;\n\tcase IP_TOS:\t/* This sets both TOS and Precedence */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tval &= ~3;\n\t\t\tval |= inet->tos & 3;\n\t\t}\n\t\tif (inet->tos != val) {\n\t\t\tinet->tos = val;\n\t\t\tsk->sk_priority = rt_tos2priority(val);\n\t\t\tsk_dst_reset(sk);\n\t\t}\n\t\tbreak;\n\tcase IP_TTL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val != -1 && (val < 0 || val > 255))\n\t\t\tgoto e_inval;\n\t\tinet->uc_ttl = val;\n\t\tbreak;\n\tcase IP_HDRINCL:\n\t\tif (sk->sk_type != SOCK_RAW) {\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tinet->hdrincl = val ? 1 : 0;\n\t\tbreak;\n\tcase IP_NODEFRAG:\n\t\tif (sk->sk_type != SOCK_RAW) {\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tinet->nodefrag = val ? 1 : 0;\n\t\tbreak;\n\tcase IP_MTU_DISCOVER:\n\t\tif (val < IP_PMTUDISC_DONT || val > IP_PMTUDISC_PROBE)\n\t\t\tgoto e_inval;\n\t\tinet->pmtudisc = val;\n\t\tbreak;\n\tcase IP_RECVERR:\n\t\tinet->recverr = !!val;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tbreak;\n\tcase IP_MULTICAST_TTL:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tgoto e_inval;\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val == -1)\n\t\t\tval = 1;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tinet->mc_ttl = val;\n\t\tbreak;\n\tcase IP_MULTICAST_LOOP:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->mc_loop = !!val;\n\t\tbreak;\n\tcase IP_MULTICAST_IF:\n\t{\n\t\tstruct ip_mreqn mreq;\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tgoto e_inval;\n\t\t/*\n\t\t *\tCheck the arguments are allowable\n\t\t */\n\n\t\tif (optlen < sizeof(struct in_addr))\n\t\t\tgoto e_inval;\n\n\t\terr = -EFAULT;\n\t\tif (optlen >= sizeof(struct ip_mreqn)) {\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(mreq)))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\t\tif (optlen >= sizeof(struct in_addr) &&\n\t\t\t    copy_from_user(&mreq.imr_address, optval,\n\t\t\t\t\t   sizeof(struct in_addr)))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!mreq.imr_ifindex) {\n\t\t\tif (mreq.imr_address.s_addr == htonl(INADDR_ANY)) {\n\t\t\t\tinet->mc_index = 0;\n\t\t\t\tinet->mc_addr  = 0;\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev = ip_dev_find(sock_net(sk), mreq.imr_address.s_addr);\n\t\t\tif (dev)\n\t\t\t\tmreq.imr_ifindex = dev->ifindex;\n\t\t} else\n\t\t\tdev = dev_get_by_index(sock_net(sk), mreq.imr_ifindex);\n\n\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\terr = -EINVAL;\n\t\tif (sk->sk_bound_dev_if &&\n\t\t    mreq.imr_ifindex != sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tinet->mc_index = mreq.imr_ifindex;\n\t\tinet->mc_addr  = mreq.imr_address.s_addr;\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\tcase IP_ADD_MEMBERSHIP:\n\tcase IP_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ip_mreqn mreq;\n\n\t\terr = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tif (optlen < sizeof(struct ip_mreq))\n\t\t\tgoto e_inval;\n\t\terr = -EFAULT;\n\t\tif (optlen >= sizeof(struct ip_mreqn)) {\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(mreq)))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(struct ip_mreq)))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (optname == IP_ADD_MEMBERSHIP)\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\telse\n\t\t\terr = ip_mc_leave_group(sk, &mreq);\n\t\tbreak;\n\t}\n\tcase IP_MSFILTER:\n\t{\n\t\tstruct ip_msfilter *msf;\n\n\t\tif (optlen < IP_MSFILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tmsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(msf, optval, optlen)) {\n\t\t\tkfree(msf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (1G-4) overflow in 32 bits */\n\t\tif (msf->imsf_numsrc >= 0x3ffffffcU ||\n\t\t    msf->imsf_numsrc > sysctl_igmp_max_msf) {\n\t\t\tkfree(msf);\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (IP_MSFILTER_SIZE(msf->imsf_numsrc) > optlen) {\n\t\t\tkfree(msf);\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\terr = ip_mc_msfilter(sk, msf, 0);\n\t\tkfree(msf);\n\t\tbreak;\n\t}\n\tcase IP_BLOCK_SOURCE:\n\tcase IP_UNBLOCK_SOURCE:\n\tcase IP_ADD_SOURCE_MEMBERSHIP:\n\tcase IP_DROP_SOURCE_MEMBERSHIP:\n\t{\n\t\tstruct ip_mreq_source mreqs;\n\t\tint omode, add;\n\n\t\tif (optlen != sizeof(struct ip_mreq_source))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&mreqs, optval, sizeof(mreqs))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == IP_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == IP_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == IP_ADD_SOURCE_MEMBERSHIP) {\n\t\t\tstruct ip_mreqn mreq;\n\n\t\t\tmreq.imr_multiaddr.s_addr = mreqs.imr_multiaddr;\n\t\t\tmreq.imr_address.s_addr = mreqs.imr_interface;\n\t\t\tmreq.imr_ifindex = 0;\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\t\tif (err && err != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* IP_DROP_SOURCE_MEMBERSHIP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\terr = ip_mc_source(add, omode, sk, &mreqs, 0);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in *psin;\n\t\tstruct ip_mreqn mreq;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(greq)))\n\t\t\tbreak;\n\t\tpsin = (struct sockaddr_in *)&greq.gr_group;\n\t\tif (psin->sin_family != AF_INET)\n\t\t\tgoto e_inval;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tmreq.imr_multiaddr = psin->sin_addr;\n\t\tmreq.imr_ifindex = greq.gr_interface;\n\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\telse\n\t\t\terr = ip_mc_leave_group(sk, &mreq);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tstruct ip_mreq_source mreqs;\n\t\tstruct sockaddr_in *psin;\n\t\tint omode, add;\n\n\t\tif (optlen != sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET ||\n\t\t    greqs.gsr_source.ss_family != AF_INET) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin = (struct sockaddr_in *)&greqs.gsr_group;\n\t\tmreqs.imr_multiaddr = psin->sin_addr.s_addr;\n\t\tpsin = (struct sockaddr_in *)&greqs.gsr_source;\n\t\tmreqs.imr_sourceaddr = psin->sin_addr.s_addr;\n\t\tmreqs.imr_interface = 0; /* use index for mc_source */\n\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct ip_mreqn mreq;\n\n\t\t\tpsin = (struct sockaddr_in *)&greqs.gsr_group;\n\t\t\tmreq.imr_multiaddr = psin->sin_addr;\n\t\t\tmreq.imr_address.s_addr = 0;\n\t\t\tmreq.imr_ifindex = greqs.gsr_interface;\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\t\tif (err && err != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tgreqs.gsr_interface = mreq.imr_ifindex;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\terr = ip_mc_source(add, omode, sk, &mreqs,\n\t\t\t\t   greqs.gsr_interface);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct sockaddr_in *psin;\n\t\tstruct ip_msfilter *msf = NULL;\n\t\tstruct group_filter *gsf = NULL;\n\t\tint msize, i, ifindex;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen))\n\t\t\tgoto mc_msf_out;\n\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffff ||\n\t\t    gsf->gf_numsrc > sysctl_igmp_max_msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tmsize = IP_MSFILTER_SIZE(gsf->gf_numsrc);\n\t\tmsf = kmalloc(msize, GFP_KERNEL);\n\t\tif (!msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tifindex = gsf->gf_interface;\n\t\tpsin = (struct sockaddr_in *)&gsf->gf_group;\n\t\tif (psin->sin_family != AF_INET) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tmsf->imsf_multiaddr = psin->sin_addr.s_addr;\n\t\tmsf->imsf_interface = 0;\n\t\tmsf->imsf_fmode = gsf->gf_fmode;\n\t\tmsf->imsf_numsrc = gsf->gf_numsrc;\n\t\terr = -EADDRNOTAVAIL;\n\t\tfor (i = 0; i < gsf->gf_numsrc; ++i) {\n\t\t\tpsin = (struct sockaddr_in *)&gsf->gf_slist[i];\n\n\t\t\tif (psin->sin_family != AF_INET)\n\t\t\t\tgoto mc_msf_out;\n\t\t\tmsf->imsf_slist[i] = psin->sin_addr.s_addr;\n\t\t}\n\t\tkfree(gsf);\n\t\tgsf = NULL;\n\n\t\terr = ip_mc_msfilter(sk, msf, ifindex);\nmc_msf_out:\n\t\tkfree(msf);\n\t\tkfree(gsf);\n\t\tbreak;\n\t}\n\tcase IP_MULTICAST_ALL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val != 0 && val != 1)\n\t\t\tgoto e_inval;\n\t\tinet->mc_all = val;\n\t\tbreak;\n\tcase IP_ROUTER_ALERT:\n\t\terr = ip_ra_control(sk, val ? 1 : 0, NULL);\n\t\tbreak;\n\n\tcase IP_FREEBIND:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->freebind = !!val;\n\t\tbreak;\n\n\tcase IP_IPSEC_POLICY:\n\tcase IP_XFRM_POLICY:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\terr = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IP_TRANSPARENT:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\terr = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->transparent = !!val;\n\t\tbreak;\n\n\tcase IP_MINTTL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tinet->min_ttl = val;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn err;\n\ne_inval:\n\trelease_sock(sk);\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,13 +39,16 @@\n \tswitch (optname) {\n \tcase IP_OPTIONS:\n \t{\n-\t\tstruct ip_options *opt = NULL;\n+\t\tstruct ip_options_rcu *old, *opt = NULL;\n+\n \t\tif (optlen > 40)\n \t\t\tgoto e_inval;\n \t\terr = ip_options_get_from_user(sock_net(sk), &opt,\n \t\t\t\t\t       optval, optlen);\n \t\tif (err)\n \t\t\tbreak;\n+\t\told = rcu_dereference_protected(inet->inet_opt,\n+\t\t\t\t\t\tsock_owned_by_user(sk));\n \t\tif (inet->is_icsk) {\n \t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n #if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n@@ -54,17 +57,18 @@\n \t\t\t       (TCPF_LISTEN | TCPF_CLOSE)) &&\n \t\t\t     inet->inet_daddr != LOOPBACK4_IPV6)) {\n #endif\n-\t\t\t\tif (inet->opt)\n-\t\t\t\t\ticsk->icsk_ext_hdr_len -= inet->opt->optlen;\n+\t\t\t\tif (old)\n+\t\t\t\t\ticsk->icsk_ext_hdr_len -= old->opt.optlen;\n \t\t\t\tif (opt)\n-\t\t\t\t\ticsk->icsk_ext_hdr_len += opt->optlen;\n+\t\t\t\t\ticsk->icsk_ext_hdr_len += opt->opt.optlen;\n \t\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n #if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n \t\t\t}\n #endif\n \t\t}\n-\t\topt = xchg(&inet->opt, opt);\n-\t\tkfree(opt);\n+\t\trcu_assign_pointer(inet->inet_opt, opt);\n+\t\tif (old)\n+\t\t\tcall_rcu(&old->rcu, opt_kfree_rcu);\n \t\tbreak;\n \t}\n \tcase IP_PKTINFO:",
        "function_modified_lines": {
            "added": [
                "\t\tstruct ip_options_rcu *old, *opt = NULL;",
                "",
                "\t\told = rcu_dereference_protected(inet->inet_opt,",
                "\t\t\t\t\t\tsock_owned_by_user(sk));",
                "\t\t\t\tif (old)",
                "\t\t\t\t\ticsk->icsk_ext_hdr_len -= old->opt.optlen;",
                "\t\t\t\t\ticsk->icsk_ext_hdr_len += opt->opt.optlen;",
                "\t\trcu_assign_pointer(inet->inet_opt, opt);",
                "\t\tif (old)",
                "\t\t\tcall_rcu(&old->rcu, opt_kfree_rcu);"
            ],
            "deleted": [
                "\t\tstruct ip_options *opt = NULL;",
                "\t\t\t\tif (inet->opt)",
                "\t\t\t\t\ticsk->icsk_ext_hdr_len -= inet->opt->optlen;",
                "\t\t\t\t\ticsk->icsk_ext_hdr_len += opt->optlen;",
                "\t\topt = xchg(&inet->opt, opt);",
                "\t\tkfree(opt);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static int raw_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tstruct sockaddr_in *usin = (struct sockaddr_in *)msg->msg_name;\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tstatic int complained;\n\t\t\tif (!complained++)\n\t\t\t\tprintk(KERN_INFO \"%s forgot to set AF_INET in \"\n\t\t\t\t\t\t \"raw sendmsg. Fix it!\\n\",\n\t\t\t\t\t\t current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt)\n\t\tipc.opt = inet->opt;\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (inet->hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->faddr;\n\t\t}\n\t}\n\ttos = RT_CONN_FLAGS(sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t}\n\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t\t   FLOWI_FLAG_CAN_SLEEP, daddr, saddr, 0, 0);\n\n\t\tif (!inet->hdrincl) {\n\t\t\terr = raw_probe_proto_opt(&fl4, msg);\n\t\t\tif (err)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_flow(sock_net(sk), &fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (inet->hdrincl)\n\t\terr = raw_send_hdrinc(sk, msg->msg_iov, len,\n\t\t\t\t\t&rt, msg->msg_flags);\n\n\t else {\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = rt->rt_dst;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, ip_generic_getfrag, msg->msg_iov, len, 0,\n\t\t\t\t\t&ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "code_after_change": "static int raw_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\tstruct ip_options_data opt_copy;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tstruct sockaddr_in *usin = (struct sockaddr_in *)msg->msg_name;\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tstatic int complained;\n\t\t\tif (!complained++)\n\t\t\t\tprintk(KERN_INFO \"%s forgot to set AF_INET in \"\n\t\t\t\t\t\t \"raw sendmsg. Fix it!\\n\",\n\t\t\t\t\t\t current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (inet->hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->opt.srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->opt.faddr;\n\t\t}\n\t}\n\ttos = RT_CONN_FLAGS(sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t}\n\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t\t   FLOWI_FLAG_CAN_SLEEP, daddr, saddr, 0, 0);\n\n\t\tif (!inet->hdrincl) {\n\t\t\terr = raw_probe_proto_opt(&fl4, msg);\n\t\t\tif (err)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_flow(sock_net(sk), &fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (inet->hdrincl)\n\t\terr = raw_send_hdrinc(sk, msg->msg_iov, len,\n\t\t\t\t\t&rt, msg->msg_flags);\n\n\t else {\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = rt->rt_dst;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, ip_generic_getfrag, msg->msg_iov, len, 0,\n\t\t\t\t\t&ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,7 @@\n \t__be32 saddr;\n \tu8  tos;\n \tint err;\n+\tstruct ip_options_data opt_copy;\n \n \terr = -EMSGSIZE;\n \tif (len > 0xFFFF)\n@@ -69,8 +70,18 @@\n \tsaddr = ipc.addr;\n \tipc.addr = daddr;\n \n-\tif (!ipc.opt)\n-\t\tipc.opt = inet->opt;\n+\tif (!ipc.opt) {\n+\t\tstruct ip_options_rcu *inet_opt;\n+\n+\t\trcu_read_lock();\n+\t\tinet_opt = rcu_dereference(inet->inet_opt);\n+\t\tif (inet_opt) {\n+\t\t\tmemcpy(&opt_copy, inet_opt,\n+\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n+\t\t\tipc.opt = &opt_copy.opt;\n+\t\t}\n+\t\trcu_read_unlock();\n+\t}\n \n \tif (ipc.opt) {\n \t\terr = -EINVAL;\n@@ -79,10 +90,10 @@\n \t\t */\n \t\tif (inet->hdrincl)\n \t\t\tgoto done;\n-\t\tif (ipc.opt->srr) {\n+\t\tif (ipc.opt->opt.srr) {\n \t\t\tif (!daddr)\n \t\t\t\tgoto done;\n-\t\t\tdaddr = ipc.opt->faddr;\n+\t\t\tdaddr = ipc.opt->opt.faddr;\n \t\t}\n \t}\n \ttos = RT_CONN_FLAGS(sk);",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_data opt_copy;",
                "\tif (!ipc.opt) {",
                "\t\tstruct ip_options_rcu *inet_opt;",
                "",
                "\t\trcu_read_lock();",
                "\t\tinet_opt = rcu_dereference(inet->inet_opt);",
                "\t\tif (inet_opt) {",
                "\t\t\tmemcpy(&opt_copy, inet_opt,",
                "\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);",
                "\t\t\tipc.opt = &opt_copy.opt;",
                "\t\t}",
                "\t\trcu_read_unlock();",
                "\t}",
                "\t\tif (ipc.opt->opt.srr) {",
                "\t\t\tdaddr = ipc.opt->opt.faddr;"
            ],
            "deleted": [
                "\tif (!ipc.opt)",
                "\t\tipc.opt = inet->opt;",
                "\t\tif (ipc.opt->srr) {",
                "\t\t\tdaddr = ipc.opt->faddr;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,\n\t\t\t     struct ip_options *opt)\n{\n\tstruct tcp_options_received tcp_opt;\n\tu8 *hash_location;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct rtable *rt;\n\t__u8 rcv_wscale;\n\tbool ecn_ok;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk) ||\n\t    (mss = cookie_check(skb, cookie)) == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, &hash_location, 0);\n\n\tif (!cookie_check_timestamp(&tcp_opt, &ecn_ok))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp_request_sock_ops); /* for safety */\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->rcv_isn\t\t= ntohl(th->seq) - 1;\n\ttreq->snt_isn\t\t= cookie;\n\treq->mss\t\t= mss;\n\tireq->loc_port\t\t= th->dest;\n\tireq->rmt_port\t\t= th->source;\n\tireq->loc_addr\t\t= ip_hdr(skb)->daddr;\n\tireq->rmt_addr\t\t= ip_hdr(skb)->saddr;\n\tireq->ecn_ok\t\t= ecn_ok;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\n\t/* We throwed the options of the initial SYN away, so we hope\n\t * the ACK carries the same options again (see RFC1122 4.2.3.8)\n\t */\n\tif (opt && opt->optlen) {\n\t\tint opt_size = sizeof(struct ip_options) + opt->optlen;\n\n\t\tireq->opt = kmalloc(opt_size, GFP_ATOMIC);\n\t\tif (ireq->opt != NULL && ip_options_echo(ireq->opt, skb)) {\n\t\t\tkfree(ireq->opt);\n\t\t\tireq->opt = NULL;\n\t\t}\n\t}\n\n\tif (security_inet_conn_request(sk, skb, req)) {\n\t\treqsk_free(req);\n\t\tgoto out;\n\t}\n\n\treq->expires\t= 0UL;\n\treq->retrans\t= 0;\n\n\t/*\n\t * We need to lookup the route here to get at the correct\n\t * window size. We should better make sure that the window size\n\t * hasn't changed since we received the original syn, but I see\n\t * no easy way to do this.\n\t */\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, 0, sk->sk_mark, RT_CONN_FLAGS(sk),\n\t\t\t\t   RT_SCOPE_UNIVERSE, IPPROTO_TCP,\n\t\t\t\t   inet_sk_flowi_flags(sk),\n\t\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,\n\t\t\t\t   ireq->loc_addr, th->source, th->dest);\n\t\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(sock_net(sk), &fl4);\n\t\tif (IS_ERR(rt)) {\n\t\t\treqsk_free(req);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* Try to redo what tcp_v4_send_synack did. */\n\treq->window_clamp = tp->window_clamp ? :dst_metric(&rt->dst, RTAX_WINDOW);\n\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rcv_wnd, &req->window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(&rt->dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale  = rcv_wscale;\n\n\tret = get_cookie_sock(sk, skb, req, &rt->dst);\nout:\treturn ret;\n}",
        "code_after_change": "struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,\n\t\t\t     struct ip_options *opt)\n{\n\tstruct tcp_options_received tcp_opt;\n\tu8 *hash_location;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct rtable *rt;\n\t__u8 rcv_wscale;\n\tbool ecn_ok;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk) ||\n\t    (mss = cookie_check(skb, cookie)) == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, &hash_location, 0);\n\n\tif (!cookie_check_timestamp(&tcp_opt, &ecn_ok))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp_request_sock_ops); /* for safety */\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->rcv_isn\t\t= ntohl(th->seq) - 1;\n\ttreq->snt_isn\t\t= cookie;\n\treq->mss\t\t= mss;\n\tireq->loc_port\t\t= th->dest;\n\tireq->rmt_port\t\t= th->source;\n\tireq->loc_addr\t\t= ip_hdr(skb)->daddr;\n\tireq->rmt_addr\t\t= ip_hdr(skb)->saddr;\n\tireq->ecn_ok\t\t= ecn_ok;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\n\t/* We throwed the options of the initial SYN away, so we hope\n\t * the ACK carries the same options again (see RFC1122 4.2.3.8)\n\t */\n\tif (opt && opt->optlen) {\n\t\tint opt_size = sizeof(struct ip_options_rcu) + opt->optlen;\n\n\t\tireq->opt = kmalloc(opt_size, GFP_ATOMIC);\n\t\tif (ireq->opt != NULL && ip_options_echo(&ireq->opt->opt, skb)) {\n\t\t\tkfree(ireq->opt);\n\t\t\tireq->opt = NULL;\n\t\t}\n\t}\n\n\tif (security_inet_conn_request(sk, skb, req)) {\n\t\treqsk_free(req);\n\t\tgoto out;\n\t}\n\n\treq->expires\t= 0UL;\n\treq->retrans\t= 0;\n\n\t/*\n\t * We need to lookup the route here to get at the correct\n\t * window size. We should better make sure that the window size\n\t * hasn't changed since we received the original syn, but I see\n\t * no easy way to do this.\n\t */\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, 0, sk->sk_mark, RT_CONN_FLAGS(sk),\n\t\t\t\t   RT_SCOPE_UNIVERSE, IPPROTO_TCP,\n\t\t\t\t   inet_sk_flowi_flags(sk),\n\t\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,\n\t\t\t\t   ireq->loc_addr, th->source, th->dest);\n\t\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(sock_net(sk), &fl4);\n\t\tif (IS_ERR(rt)) {\n\t\t\treqsk_free(req);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* Try to redo what tcp_v4_send_synack did. */\n\treq->window_clamp = tp->window_clamp ? :dst_metric(&rt->dst, RTAX_WINDOW);\n\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rcv_wnd, &req->window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(&rt->dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale  = rcv_wscale;\n\n\tret = get_cookie_sock(sk, skb, req, &rt->dst);\nout:\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -58,10 +58,10 @@\n \t * the ACK carries the same options again (see RFC1122 4.2.3.8)\n \t */\n \tif (opt && opt->optlen) {\n-\t\tint opt_size = sizeof(struct ip_options) + opt->optlen;\n+\t\tint opt_size = sizeof(struct ip_options_rcu) + opt->optlen;\n \n \t\tireq->opt = kmalloc(opt_size, GFP_ATOMIC);\n-\t\tif (ireq->opt != NULL && ip_options_echo(ireq->opt, skb)) {\n+\t\tif (ireq->opt != NULL && ip_options_echo(&ireq->opt->opt, skb)) {\n \t\t\tkfree(ireq->opt);\n \t\t\tireq->opt = NULL;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\tint opt_size = sizeof(struct ip_options_rcu) + opt->optlen;",
                "\t\tif (ireq->opt != NULL && ip_options_echo(&ireq->opt->opt, skb)) {"
            ],
            "deleted": [
                "\t\tint opt_size = sizeof(struct ip_options) + opt->optlen;",
                "\t\tif (ireq->opt != NULL && ip_options_echo(ireq->opt, skb)) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static struct ip_options *tcp_v4_save_options(struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb)\n{\n\tstruct ip_options *opt = &(IPCB(skb)->opt);\n\tstruct ip_options *dopt = NULL;\n\n\tif (opt && opt->optlen) {\n\t\tint opt_size = optlength(opt);\n\t\tdopt = kmalloc(opt_size, GFP_ATOMIC);\n\t\tif (dopt) {\n\t\t\tif (ip_options_echo(dopt, skb)) {\n\t\t\t\tkfree(dopt);\n\t\t\t\tdopt = NULL;\n\t\t\t}\n\t\t}\n\t}\n\treturn dopt;\n}",
        "code_after_change": "static struct ip_options_rcu *tcp_v4_save_options(struct sock *sk,\n\t\t\t\t\t\t  struct sk_buff *skb)\n{\n\tconst struct ip_options *opt = &(IPCB(skb)->opt);\n\tstruct ip_options_rcu *dopt = NULL;\n\n\tif (opt && opt->optlen) {\n\t\tint opt_size = sizeof(*dopt) + opt->optlen;\n\n\t\tdopt = kmalloc(opt_size, GFP_ATOMIC);\n\t\tif (dopt) {\n\t\t\tif (ip_options_echo(&dopt->opt, skb)) {\n\t\t\t\tkfree(dopt);\n\t\t\t\tdopt = NULL;\n\t\t\t}\n\t\t}\n\t}\n\treturn dopt;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,14 +1,15 @@\n-static struct ip_options *tcp_v4_save_options(struct sock *sk,\n-\t\t\t\t\t      struct sk_buff *skb)\n+static struct ip_options_rcu *tcp_v4_save_options(struct sock *sk,\n+\t\t\t\t\t\t  struct sk_buff *skb)\n {\n-\tstruct ip_options *opt = &(IPCB(skb)->opt);\n-\tstruct ip_options *dopt = NULL;\n+\tconst struct ip_options *opt = &(IPCB(skb)->opt);\n+\tstruct ip_options_rcu *dopt = NULL;\n \n \tif (opt && opt->optlen) {\n-\t\tint opt_size = optlength(opt);\n+\t\tint opt_size = sizeof(*dopt) + opt->optlen;\n+\n \t\tdopt = kmalloc(opt_size, GFP_ATOMIC);\n \t\tif (dopt) {\n-\t\t\tif (ip_options_echo(dopt, skb)) {\n+\t\t\tif (ip_options_echo(&dopt->opt, skb)) {\n \t\t\t\tkfree(dopt);\n \t\t\t\tdopt = NULL;\n \t\t\t}",
        "function_modified_lines": {
            "added": [
                "static struct ip_options_rcu *tcp_v4_save_options(struct sock *sk,",
                "\t\t\t\t\t\t  struct sk_buff *skb)",
                "\tconst struct ip_options *opt = &(IPCB(skb)->opt);",
                "\tstruct ip_options_rcu *dopt = NULL;",
                "\t\tint opt_size = sizeof(*dopt) + opt->optlen;",
                "",
                "\t\t\tif (ip_options_echo(&dopt->opt, skb)) {"
            ],
            "deleted": [
                "static struct ip_options *tcp_v4_save_options(struct sock *sk,",
                "\t\t\t\t\t      struct sk_buff *skb)",
                "\tstruct ip_options *opt = &(IPCB(skb)->opt);",
                "\tstruct ip_options *dopt = NULL;",
                "\t\tint opt_size = optlength(opt);",
                "\t\t\tif (ip_options_echo(dopt, skb)) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\tif (inet->opt && inet->opt->srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet->opt->faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_TCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\tif (err == -ENETUNREACH)\n\t\t\tIP_INC_STATS_BH(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\t\treturn err;\n\t}\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (!inet->opt || !inet->opt->srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (!inet->inet_saddr)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tif (tp->rx_opt.ts_recent_stamp && inet->inet_daddr != daddr) {\n\t\t/* Reset inherited state */\n\t\ttp->rx_opt.ts_recent\t   = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq\t\t   = 0;\n\t}\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp && rt->rt_dst == daddr) {\n\t\tstruct inet_peer *peer = rt_get_peer(rt);\n\t\t/*\n\t\t * VJ's idea. We save last timestamp seen from\n\t\t * the destination in peer table, when entering state\n\t\t * TIME-WAIT * and initialize rx_opt.ts_recent from it,\n\t\t * when trying new connection.\n\t\t */\n\t\tif (peer) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp <= TCP_PAWS_MSL) {\n\t\t\t\ttp->rx_opt.ts_recent_stamp = peer->tcp_ts_stamp;\n\t\t\t\ttp->rx_opt.ts_recent = peer->tcp_ts;\n\t\t\t}\n\t\t}\n\t}\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet->opt)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;\n\n\ttp->rx_opt.mss_clamp = TCP_MSS_DEFAULT;\n\n\t/* Socket identity is still unknown (sport may be zero).\n\t * However we set state to SYN-SENT and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk->sk_gso_type = SKB_GSO_TCPV4;\n\tsk_setup_caps(sk, &rt->dst);\n\n\tif (!tp->write_seq)\n\t\ttp->write_seq = secure_tcp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t\t   inet->inet_daddr,\n\t\t\t\t\t\t\t   inet->inet_sport,\n\t\t\t\t\t\t\t   usin->sin_port);\n\n\tinet->inet_id = tp->write_seq ^ jiffies;\n\n\terr = tcp_connect(sk);\n\trt = NULL;\n\tif (err)\n\t\tgoto failure;\n\n\treturn 0;\n\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port,\n\t * if necessary.\n\t */\n\ttcp_set_state(sk, TCP_CLOSE);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\treturn err;\n}",
        "code_after_change": "int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\tstruct ip_options_rcu *inet_opt;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     sock_owned_by_user(sk));\n\tif (inet_opt && inet_opt->opt.srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet_opt->opt.faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_TCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\tif (err == -ENETUNREACH)\n\t\t\tIP_INC_STATS_BH(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\t\treturn err;\n\t}\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (!inet_opt || !inet_opt->opt.srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (!inet->inet_saddr)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tif (tp->rx_opt.ts_recent_stamp && inet->inet_daddr != daddr) {\n\t\t/* Reset inherited state */\n\t\ttp->rx_opt.ts_recent\t   = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq\t\t   = 0;\n\t}\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp && rt->rt_dst == daddr) {\n\t\tstruct inet_peer *peer = rt_get_peer(rt);\n\t\t/*\n\t\t * VJ's idea. We save last timestamp seen from\n\t\t * the destination in peer table, when entering state\n\t\t * TIME-WAIT * and initialize rx_opt.ts_recent from it,\n\t\t * when trying new connection.\n\t\t */\n\t\tif (peer) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp <= TCP_PAWS_MSL) {\n\t\t\t\ttp->rx_opt.ts_recent_stamp = peer->tcp_ts_stamp;\n\t\t\t\ttp->rx_opt.ts_recent = peer->tcp_ts;\n\t\t\t}\n\t\t}\n\t}\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet_opt)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n\n\ttp->rx_opt.mss_clamp = TCP_MSS_DEFAULT;\n\n\t/* Socket identity is still unknown (sport may be zero).\n\t * However we set state to SYN-SENT and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk->sk_gso_type = SKB_GSO_TCPV4;\n\tsk_setup_caps(sk, &rt->dst);\n\n\tif (!tp->write_seq)\n\t\ttp->write_seq = secure_tcp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t\t   inet->inet_daddr,\n\t\t\t\t\t\t\t   inet->inet_sport,\n\t\t\t\t\t\t\t   usin->sin_port);\n\n\tinet->inet_id = tp->write_seq ^ jiffies;\n\n\terr = tcp_connect(sk);\n\trt = NULL;\n\tif (err)\n\t\tgoto failure;\n\n\treturn 0;\n\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port,\n\t * if necessary.\n\t */\n\ttcp_set_state(sk, TCP_CLOSE);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,7 @@\n \tstruct flowi4 fl4;\n \tstruct rtable *rt;\n \tint err;\n+\tstruct ip_options_rcu *inet_opt;\n \n \tif (addr_len < sizeof(struct sockaddr_in))\n \t\treturn -EINVAL;\n@@ -16,10 +17,12 @@\n \t\treturn -EAFNOSUPPORT;\n \n \tnexthop = daddr = usin->sin_addr.s_addr;\n-\tif (inet->opt && inet->opt->srr) {\n+\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n+\t\t\t\t\t     sock_owned_by_user(sk));\n+\tif (inet_opt && inet_opt->opt.srr) {\n \t\tif (!daddr)\n \t\t\treturn -EINVAL;\n-\t\tnexthop = inet->opt->faddr;\n+\t\tnexthop = inet_opt->opt.faddr;\n \t}\n \n \torig_sport = inet->inet_sport;\n@@ -40,7 +43,7 @@\n \t\treturn -ENETUNREACH;\n \t}\n \n-\tif (!inet->opt || !inet->opt->srr)\n+\tif (!inet_opt || !inet_opt->opt.srr)\n \t\tdaddr = rt->rt_dst;\n \n \tif (!inet->inet_saddr)\n@@ -76,8 +79,8 @@\n \tinet->inet_daddr = daddr;\n \n \tinet_csk(sk)->icsk_ext_hdr_len = 0;\n-\tif (inet->opt)\n-\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;\n+\tif (inet_opt)\n+\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n \n \ttp->rx_opt.mss_clamp = TCP_MSS_DEFAULT;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *inet_opt;",
                "\tinet_opt = rcu_dereference_protected(inet->inet_opt,",
                "\t\t\t\t\t     sock_owned_by_user(sk));",
                "\tif (inet_opt && inet_opt->opt.srr) {",
                "\t\tnexthop = inet_opt->opt.faddr;",
                "\tif (!inet_opt || !inet_opt->opt.srr)",
                "\tif (inet_opt)",
                "\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;"
            ],
            "deleted": [
                "\tif (inet->opt && inet->opt->srr) {",
                "\t\tnexthop = inet->opt->faddr;",
                "\tif (!inet->opt || !inet->opt->srr)",
                "\tif (inet->opt)",
                "\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "struct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req,\n\t\t\t\t  struct dst_entry *dst)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto exit_overflow;\n\n\tif (!dst && (dst = inet_csk_route_req(sk, req)) == NULL)\n\t\tgoto exit;\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (!newsk)\n\t\tgoto exit_nonewsk;\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV4;\n\tsk_setup_caps(newsk, dst);\n\n\tnewtp\t\t      = tcp_sk(newsk);\n\tnewinet\t\t      = inet_sk(newsk);\n\tireq\t\t      = inet_rsk(req);\n\tnewinet->inet_daddr   = ireq->rmt_addr;\n\tnewinet->inet_rcv_saddr = ireq->loc_addr;\n\tnewinet->inet_saddr\t      = ireq->loc_addr;\n\tnewinet->opt\t      = ireq->opt;\n\tireq->opt\t      = NULL;\n\tnewinet->mc_index     = inet_iif(skb);\n\tnewinet->mc_ttl\t      = ip_hdr(skb)->ttl;\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newinet->opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = newinet->opt->optlen;\n\tnewinet->inet_id = newtp->write_seq ^ jiffies;\n\n\ttcp_mtup_init(newsk);\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\tif (tcp_sk(sk)->rx_opt.user_mss &&\n\t    tcp_sk(sk)->rx_opt.user_mss < newtp->advmss)\n\t\tnewtp->advmss = tcp_sk(sk)->rx_opt.user_mss;\n\n\ttcp_initialize_rcv_mss(newsk);\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tkey = tcp_v4_md5_do_lookup(sk, newinet->inet_daddr);\n\tif (key != NULL) {\n\t\t/*\n\t\t * We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\tchar *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);\n\t\tif (newkey != NULL)\n\t\t\ttcp_v4_md5_do_add(newsk, newinet->inet_daddr,\n\t\t\t\t\t  newkey, key->keylen);\n\t\tsk_nocaps_add(newsk, NETIF_F_GSO_MASK);\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto exit;\n\t}\n\t__inet_hash_nolisten(newsk, NULL);\n\n\treturn newsk;\n\nexit_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nexit_nonewsk:\n\tdst_release(dst);\nexit:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "code_after_change": "struct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req,\n\t\t\t\t  struct dst_entry *dst)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\tstruct ip_options_rcu *inet_opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto exit_overflow;\n\n\tif (!dst && (dst = inet_csk_route_req(sk, req)) == NULL)\n\t\tgoto exit;\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (!newsk)\n\t\tgoto exit_nonewsk;\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV4;\n\tsk_setup_caps(newsk, dst);\n\n\tnewtp\t\t      = tcp_sk(newsk);\n\tnewinet\t\t      = inet_sk(newsk);\n\tireq\t\t      = inet_rsk(req);\n\tnewinet->inet_daddr   = ireq->rmt_addr;\n\tnewinet->inet_rcv_saddr = ireq->loc_addr;\n\tnewinet->inet_saddr\t      = ireq->loc_addr;\n\tinet_opt\t      = ireq->opt;\n\trcu_assign_pointer(newinet->inet_opt, inet_opt);\n\tireq->opt\t      = NULL;\n\tnewinet->mc_index     = inet_iif(skb);\n\tnewinet->mc_ttl\t      = ip_hdr(skb)->ttl;\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (inet_opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n\tnewinet->inet_id = newtp->write_seq ^ jiffies;\n\n\ttcp_mtup_init(newsk);\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\tif (tcp_sk(sk)->rx_opt.user_mss &&\n\t    tcp_sk(sk)->rx_opt.user_mss < newtp->advmss)\n\t\tnewtp->advmss = tcp_sk(sk)->rx_opt.user_mss;\n\n\ttcp_initialize_rcv_mss(newsk);\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tkey = tcp_v4_md5_do_lookup(sk, newinet->inet_daddr);\n\tif (key != NULL) {\n\t\t/*\n\t\t * We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\tchar *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);\n\t\tif (newkey != NULL)\n\t\t\ttcp_v4_md5_do_add(newsk, newinet->inet_daddr,\n\t\t\t\t\t  newkey, key->keylen);\n\t\tsk_nocaps_add(newsk, NETIF_F_GSO_MASK);\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto exit;\n\t}\n\t__inet_hash_nolisten(newsk, NULL);\n\n\treturn newsk;\n\nexit_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nexit_nonewsk:\n\tdst_release(dst);\nexit:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,7 @@\n #ifdef CONFIG_TCP_MD5SIG\n \tstruct tcp_md5sig_key *key;\n #endif\n+\tstruct ip_options_rcu *inet_opt;\n \n \tif (sk_acceptq_is_full(sk))\n \t\tgoto exit_overflow;\n@@ -29,13 +30,14 @@\n \tnewinet->inet_daddr   = ireq->rmt_addr;\n \tnewinet->inet_rcv_saddr = ireq->loc_addr;\n \tnewinet->inet_saddr\t      = ireq->loc_addr;\n-\tnewinet->opt\t      = ireq->opt;\n+\tinet_opt\t      = ireq->opt;\n+\trcu_assign_pointer(newinet->inet_opt, inet_opt);\n \tireq->opt\t      = NULL;\n \tnewinet->mc_index     = inet_iif(skb);\n \tnewinet->mc_ttl\t      = ip_hdr(skb)->ttl;\n \tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n-\tif (newinet->opt)\n-\t\tinet_csk(newsk)->icsk_ext_hdr_len = newinet->opt->optlen;\n+\tif (inet_opt)\n+\t\tinet_csk(newsk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n \tnewinet->inet_id = newtp->write_seq ^ jiffies;\n \n \ttcp_mtup_init(newsk);",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *inet_opt;",
                "\tinet_opt\t      = ireq->opt;",
                "\trcu_assign_pointer(newinet->inet_opt, inet_opt);",
                "\tif (inet_opt)",
                "\t\tinet_csk(newsk)->icsk_ext_hdr_len = inet_opt->opt.optlen;"
            ],
            "deleted": [
                "\tnewinet->opt\t      = ireq->opt;",
                "\tif (newinet->opt)",
                "\t\tinet_csk(newsk)->icsk_ext_hdr_len = newinet->opt->optlen;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\tsize_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct flowi4 *fl4;\n\tint ulen = len;\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\tint connected = 0;\n\t__be32 daddr, faddr, saddr;\n\t__be16 dport;\n\tu8  tos;\n\tint err, is_udplite = IS_UDPLITE(sk);\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\tstruct sk_buff *skb;\n\n\tif (len > 0xFFFF)\n\t\treturn -EMSGSIZE;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\tif (msg->msg_flags & MSG_OOB) /* Mirror BSD error message compatibility */\n\t\treturn -EOPNOTSUPP;\n\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\n\tgetfrag = is_udplite ? udplite_getfrag : ip_generic_getfrag;\n\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in * usin = (struct sockaddr_in *)msg->msg_name;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\treturn -EINVAL;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tif (usin->sin_family != AF_UNSPEC)\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t}\n\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t\tif (dport == 0)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = inet->inet_daddr;\n\t\tdport = inet->inet_dport;\n\t\t/* Open fast path for connected socket.\n\t\t   Route will not be used, if at least one option is set.\n\t\t */\n\t\tconnected = 1;\n\t}\n\tipc.addr = inet->inet_saddr;\n\n\tipc.oif = sk->sk_bound_dev_if;\n\terr = sock_tx_timestamp(sk, &ipc.tx_flags);\n\tif (err)\n\t\treturn err;\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t\tconnected = 0;\n\t}\n\tif (!ipc.opt)\n\t\tipc.opt = inet->opt;\n\n\tsaddr = ipc.addr;\n\tipc.addr = faddr = daddr;\n\n\tif (ipc.opt && ipc.opt->srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tfaddr = ipc.opt->faddr;\n\t\tconnected = 0;\n\t}\n\ttos = RT_TOS(inet->tos);\n\tif (sock_flag(sk, SOCK_LOCALROUTE) ||\n\t    (msg->msg_flags & MSG_DONTROUTE) ||\n\t    (ipc.opt && ipc.opt->is_strictroute)) {\n\t\ttos |= RTO_ONLINK;\n\t\tconnected = 0;\n\t}\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t\tconnected = 0;\n\t}\n\n\tif (connected)\n\t\trt = (struct rtable *)sk_dst_check(sk, 0);\n\n\tif (rt == NULL) {\n\t\tstruct flowi4 fl4;\n\t\tstruct net *net = sock_net(sk);\n\n\t\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE, sk->sk_protocol,\n\t\t\t\t   inet_sk_flowi_flags(sk)|FLOWI_FLAG_CAN_SLEEP,\n\t\t\t\t   faddr, saddr, dport, inet->inet_sport);\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_flow(net, &fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tif (err == -ENETUNREACH)\n\t\t\t\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = -EACCES;\n\t\tif ((rt->rt_flags & RTCF_BROADCAST) &&\n\t\t    !sock_flag(sk, SOCK_BROADCAST))\n\t\t\tgoto out;\n\t\tif (connected)\n\t\t\tsk_dst_set(sk, dst_clone(&rt->dst));\n\t}\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tsaddr = rt->rt_src;\n\tif (!ipc.addr)\n\t\tdaddr = ipc.addr = rt->rt_dst;\n\n\t/* Lockless fast path for the non-corking case. */\n\tif (!corkreq) {\n\t\tskb = ip_make_skb(sk, getfrag, msg->msg_iov, ulen,\n\t\t\t\t  sizeof(struct udphdr), &ipc, &rt,\n\t\t\t\t  msg->msg_flags);\n\t\terr = PTR_ERR(skb);\n\t\tif (skb && !IS_ERR(skb))\n\t\t\terr = udp_send_skb(skb, daddr, dport);\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tLIMIT_NETDEBUG(KERN_DEBUG \"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\t/*\n\t *\tNow cork the socket to pend data.\n\t */\n\tfl4 = &inet->cork.fl.u.ip4;\n\tfl4->daddr = daddr;\n\tfl4->saddr = saddr;\n\tfl4->fl4_dport = dport;\n\tfl4->fl4_sport = inet->inet_sport;\n\tup->pending = AF_INET;\n\ndo_append_data:\n\tup->len += ulen;\n\terr = ip_append_data(sk, getfrag, msg->msg_iov, ulen,\n\t\t\tsizeof(struct udphdr), &ipc, &rt,\n\t\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags);\n\tif (err)\n\t\tudp_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\trelease_sock(sk);\n\nout:\n\tip_rt_put(rt);\n\tif (free)\n\t\tkfree(ipc.opt);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}",
        "code_after_change": "int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\tsize_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct flowi4 *fl4;\n\tint ulen = len;\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\tint connected = 0;\n\t__be32 daddr, faddr, saddr;\n\t__be16 dport;\n\tu8  tos;\n\tint err, is_udplite = IS_UDPLITE(sk);\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\tstruct sk_buff *skb;\n\tstruct ip_options_data opt_copy;\n\n\tif (len > 0xFFFF)\n\t\treturn -EMSGSIZE;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\tif (msg->msg_flags & MSG_OOB) /* Mirror BSD error message compatibility */\n\t\treturn -EOPNOTSUPP;\n\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\n\tgetfrag = is_udplite ? udplite_getfrag : ip_generic_getfrag;\n\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in * usin = (struct sockaddr_in *)msg->msg_name;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\treturn -EINVAL;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tif (usin->sin_family != AF_UNSPEC)\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t}\n\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t\tif (dport == 0)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = inet->inet_daddr;\n\t\tdport = inet->inet_dport;\n\t\t/* Open fast path for connected socket.\n\t\t   Route will not be used, if at least one option is set.\n\t\t */\n\t\tconnected = 1;\n\t}\n\tipc.addr = inet->inet_saddr;\n\n\tipc.oif = sk->sk_bound_dev_if;\n\terr = sock_tx_timestamp(sk, &ipc.tx_flags);\n\tif (err)\n\t\treturn err;\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t\tconnected = 0;\n\t}\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = faddr = daddr;\n\n\tif (ipc.opt && ipc.opt->opt.srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tfaddr = ipc.opt->opt.faddr;\n\t\tconnected = 0;\n\t}\n\ttos = RT_TOS(inet->tos);\n\tif (sock_flag(sk, SOCK_LOCALROUTE) ||\n\t    (msg->msg_flags & MSG_DONTROUTE) ||\n\t    (ipc.opt && ipc.opt->opt.is_strictroute)) {\n\t\ttos |= RTO_ONLINK;\n\t\tconnected = 0;\n\t}\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t\tconnected = 0;\n\t}\n\n\tif (connected)\n\t\trt = (struct rtable *)sk_dst_check(sk, 0);\n\n\tif (rt == NULL) {\n\t\tstruct flowi4 fl4;\n\t\tstruct net *net = sock_net(sk);\n\n\t\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE, sk->sk_protocol,\n\t\t\t\t   inet_sk_flowi_flags(sk)|FLOWI_FLAG_CAN_SLEEP,\n\t\t\t\t   faddr, saddr, dport, inet->inet_sport);\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_flow(net, &fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tif (err == -ENETUNREACH)\n\t\t\t\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = -EACCES;\n\t\tif ((rt->rt_flags & RTCF_BROADCAST) &&\n\t\t    !sock_flag(sk, SOCK_BROADCAST))\n\t\t\tgoto out;\n\t\tif (connected)\n\t\t\tsk_dst_set(sk, dst_clone(&rt->dst));\n\t}\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tsaddr = rt->rt_src;\n\tif (!ipc.addr)\n\t\tdaddr = ipc.addr = rt->rt_dst;\n\n\t/* Lockless fast path for the non-corking case. */\n\tif (!corkreq) {\n\t\tskb = ip_make_skb(sk, getfrag, msg->msg_iov, ulen,\n\t\t\t\t  sizeof(struct udphdr), &ipc, &rt,\n\t\t\t\t  msg->msg_flags);\n\t\terr = PTR_ERR(skb);\n\t\tif (skb && !IS_ERR(skb))\n\t\t\terr = udp_send_skb(skb, daddr, dport);\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tLIMIT_NETDEBUG(KERN_DEBUG \"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\t/*\n\t *\tNow cork the socket to pend data.\n\t */\n\tfl4 = &inet->cork.fl.u.ip4;\n\tfl4->daddr = daddr;\n\tfl4->saddr = saddr;\n\tfl4->fl4_dport = dport;\n\tfl4->fl4_sport = inet->inet_sport;\n\tup->pending = AF_INET;\n\ndo_append_data:\n\tup->len += ulen;\n\terr = ip_append_data(sk, getfrag, msg->msg_iov, ulen,\n\t\t\tsizeof(struct udphdr), &ipc, &rt,\n\t\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags);\n\tif (err)\n\t\tudp_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\trelease_sock(sk);\n\nout:\n\tip_rt_put(rt);\n\tif (free)\n\t\tkfree(ipc.opt);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,7 @@\n \tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n \tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n \tstruct sk_buff *skb;\n+\tstruct ip_options_data opt_copy;\n \n \tif (len > 0xFFFF)\n \t\treturn -EMSGSIZE;\n@@ -89,22 +90,32 @@\n \t\t\tfree = 1;\n \t\tconnected = 0;\n \t}\n-\tif (!ipc.opt)\n-\t\tipc.opt = inet->opt;\n+\tif (!ipc.opt) {\n+\t\tstruct ip_options_rcu *inet_opt;\n+\n+\t\trcu_read_lock();\n+\t\tinet_opt = rcu_dereference(inet->inet_opt);\n+\t\tif (inet_opt) {\n+\t\t\tmemcpy(&opt_copy, inet_opt,\n+\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n+\t\t\tipc.opt = &opt_copy.opt;\n+\t\t}\n+\t\trcu_read_unlock();\n+\t}\n \n \tsaddr = ipc.addr;\n \tipc.addr = faddr = daddr;\n \n-\tif (ipc.opt && ipc.opt->srr) {\n+\tif (ipc.opt && ipc.opt->opt.srr) {\n \t\tif (!daddr)\n \t\t\treturn -EINVAL;\n-\t\tfaddr = ipc.opt->faddr;\n+\t\tfaddr = ipc.opt->opt.faddr;\n \t\tconnected = 0;\n \t}\n \ttos = RT_TOS(inet->tos);\n \tif (sock_flag(sk, SOCK_LOCALROUTE) ||\n \t    (msg->msg_flags & MSG_DONTROUTE) ||\n-\t    (ipc.opt && ipc.opt->is_strictroute)) {\n+\t    (ipc.opt && ipc.opt->opt.is_strictroute)) {\n \t\ttos |= RTO_ONLINK;\n \t\tconnected = 0;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_data opt_copy;",
                "\tif (!ipc.opt) {",
                "\t\tstruct ip_options_rcu *inet_opt;",
                "",
                "\t\trcu_read_lock();",
                "\t\tinet_opt = rcu_dereference(inet->inet_opt);",
                "\t\tif (inet_opt) {",
                "\t\t\tmemcpy(&opt_copy, inet_opt,",
                "\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);",
                "\t\t\tipc.opt = &opt_copy.opt;",
                "\t\t}",
                "\t\trcu_read_unlock();",
                "\t}",
                "\tif (ipc.opt && ipc.opt->opt.srr) {",
                "\t\tfaddr = ipc.opt->opt.faddr;",
                "\t    (ipc.opt && ipc.opt->opt.is_strictroute)) {"
            ],
            "deleted": [
                "\tif (!ipc.opt)",
                "\t\tipc.opt = inet->opt;",
                "\tif (ipc.opt && ipc.opt->srr) {",
                "\t\tfaddr = ipc.opt->faddr;",
                "\t    (ipc.opt && ipc.opt->is_strictroute)) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static struct sock * tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t  struct request_sock *req,\n\t\t\t\t\t  struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *treq;\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct tcp6_sock *newtcp6sk;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst);\n\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\t\tnewinet = inet_sk(newsk);\n\t\tnewnp = inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\ttreq = inet6_rsk(req);\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, req);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\n\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &treq->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &treq->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &treq->loc_addr);\n\tnewsk->sk_bound_dev_if = treq->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->opt = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (treq->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(treq->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(treq->pktopts);\n\t\ttreq->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\tif (opt) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\ttcp_mtup_init(newsk);\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\ttcp_initialize_rcv_mss(newsk);\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tif ((key = tcp_v6_md5_do_lookup(sk, &newnp->daddr)) != NULL) {\n\t\t/* We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\tchar *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);\n\t\tif (newkey != NULL)\n\t\t\ttcp_v6_md5_do_add(newsk, &newnp->daddr,\n\t\t\t\t\t  newkey, key->keylen);\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tif (opt && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "code_after_change": "static struct sock * tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t  struct request_sock *req,\n\t\t\t\t\t  struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *treq;\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct tcp6_sock *newtcp6sk;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst);\n\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\t\tnewinet = inet_sk(newsk);\n\t\tnewnp = inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\ttreq = inet6_rsk(req);\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, req);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\n\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &treq->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &treq->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &treq->loc_addr);\n\tnewsk->sk_bound_dev_if = treq->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (treq->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(treq->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(treq->pktopts);\n\t\ttreq->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\tif (opt) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\ttcp_mtup_init(newsk);\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\ttcp_initialize_rcv_mss(newsk);\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tif ((key = tcp_v6_md5_do_lookup(sk, &newnp->daddr)) != NULL) {\n\t\t/* We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\tchar *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);\n\t\tif (newkey != NULL)\n\t\t\ttcp_v6_md5_do_add(newsk, &newnp->daddr,\n\t\t\t\t\t  newkey, key->keylen);\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tif (opt && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -107,7 +107,7 @@\n \n \t   First: no IPv4 options.\n \t */\n-\tnewinet->opt = NULL;\n+\tnewinet->inet_opt = NULL;\n \tnewnp->ipv6_fl_list = NULL;\n \n \t/* Clone RX bits */",
        "function_modified_lines": {
            "added": [
                "\tnewinet->inet_opt = NULL;"
            ],
            "deleted": [
                "\tnewinet->opt = NULL;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static int l2tp_ip_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct sk_buff *skb;\n\tint rc;\n\tstruct l2tp_ip_sock *lsa = l2tp_ip_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options *opt = inet->opt;\n\tstruct rtable *rt = NULL;\n\tint connected = 0;\n\t__be32 daddr;\n\n\tif (sock_flag(sk, SOCK_DEAD))\n\t\treturn -ENOTCONN;\n\n\t/* Get and verify the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_l2tpip *lip = (struct sockaddr_l2tpip *) msg->msg_name;\n\t\tif (msg->msg_namelen < sizeof(*lip))\n\t\t\treturn -EINVAL;\n\n\t\tif (lip->l2tp_family != AF_INET) {\n\t\t\tif (lip->l2tp_family != AF_UNSPEC)\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t}\n\n\t\tdaddr = lip->l2tp_addr.s_addr;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = inet->inet_daddr;\n\t\tconnected = 1;\n\t}\n\n\t/* Allocate a socket buffer */\n\trc = -ENOMEM;\n\tskb = sock_wmalloc(sk, 2 + NET_SKB_PAD + sizeof(struct iphdr) +\n\t\t\t   4 + len, 0, GFP_KERNEL);\n\tif (!skb)\n\t\tgoto error;\n\n\t/* Reserve space for headers, putting IP header on 4-byte boundary. */\n\tskb_reserve(skb, 2 + NET_SKB_PAD);\n\tskb_reset_network_header(skb);\n\tskb_reserve(skb, sizeof(struct iphdr));\n\tskb_reset_transport_header(skb);\n\n\t/* Insert 0 session_id */\n\t*((__be32 *) skb_put(skb, 4)) = 0;\n\n\t/* Copy user data into skb */\n\trc = memcpy_fromiovec(skb_put(skb, len), msg->msg_iov, len);\n\tif (rc < 0) {\n\t\tkfree_skb(skb);\n\t\tgoto error;\n\t}\n\n\tif (connected)\n\t\trt = (struct rtable *) __sk_dst_check(sk, 0);\n\n\tif (rt == NULL) {\n\t\t/* Use correct destination address if we have options. */\n\t\tif (opt && opt->srr)\n\t\t\tdaddr = opt->faddr;\n\n\t\t/* If this fails, retransmit mechanism of transport layer will\n\t\t * keep trying until route appears or the connection times\n\t\t * itself out.\n\t\t */\n\t\trt = ip_route_output_ports(sock_net(sk), sk,\n\t\t\t\t\t   daddr, inet->inet_saddr,\n\t\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto no_route;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t}\n\tskb_dst_set(skb, dst_clone(&rt->dst));\n\n\t/* Queue the packet to IP for output */\n\trc = ip_queue_xmit(skb);\n\nerror:\n\t/* Update stats */\n\tif (rc >= 0) {\n\t\tlsa->tx_packets++;\n\t\tlsa->tx_bytes += len;\n\t\trc = len;\n\t} else {\n\t\tlsa->tx_errors++;\n\t}\n\n\treturn rc;\n\nno_route:\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EHOSTUNREACH;\n}",
        "code_after_change": "static int l2tp_ip_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct sk_buff *skb;\n\tint rc;\n\tstruct l2tp_ip_sock *lsa = l2tp_ip_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = NULL;\n\tint connected = 0;\n\t__be32 daddr;\n\n\tif (sock_flag(sk, SOCK_DEAD))\n\t\treturn -ENOTCONN;\n\n\t/* Get and verify the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_l2tpip *lip = (struct sockaddr_l2tpip *) msg->msg_name;\n\t\tif (msg->msg_namelen < sizeof(*lip))\n\t\t\treturn -EINVAL;\n\n\t\tif (lip->l2tp_family != AF_INET) {\n\t\t\tif (lip->l2tp_family != AF_UNSPEC)\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t}\n\n\t\tdaddr = lip->l2tp_addr.s_addr;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = inet->inet_daddr;\n\t\tconnected = 1;\n\t}\n\n\t/* Allocate a socket buffer */\n\trc = -ENOMEM;\n\tskb = sock_wmalloc(sk, 2 + NET_SKB_PAD + sizeof(struct iphdr) +\n\t\t\t   4 + len, 0, GFP_KERNEL);\n\tif (!skb)\n\t\tgoto error;\n\n\t/* Reserve space for headers, putting IP header on 4-byte boundary. */\n\tskb_reserve(skb, 2 + NET_SKB_PAD);\n\tskb_reset_network_header(skb);\n\tskb_reserve(skb, sizeof(struct iphdr));\n\tskb_reset_transport_header(skb);\n\n\t/* Insert 0 session_id */\n\t*((__be32 *) skb_put(skb, 4)) = 0;\n\n\t/* Copy user data into skb */\n\trc = memcpy_fromiovec(skb_put(skb, len), msg->msg_iov, len);\n\tif (rc < 0) {\n\t\tkfree_skb(skb);\n\t\tgoto error;\n\t}\n\n\tif (connected)\n\t\trt = (struct rtable *) __sk_dst_check(sk, 0);\n\n\tif (rt == NULL) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t\t     sock_owned_by_user(sk));\n\n\t\t/* Use correct destination address if we have options. */\n\t\tif (inet_opt && inet_opt->opt.srr)\n\t\t\tdaddr = inet_opt->opt.faddr;\n\n\t\t/* If this fails, retransmit mechanism of transport layer will\n\t\t * keep trying until route appears or the connection times\n\t\t * itself out.\n\t\t */\n\t\trt = ip_route_output_ports(sock_net(sk), sk,\n\t\t\t\t\t   daddr, inet->inet_saddr,\n\t\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto no_route;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t}\n\tskb_dst_set(skb, dst_clone(&rt->dst));\n\n\t/* Queue the packet to IP for output */\n\trc = ip_queue_xmit(skb);\n\nerror:\n\t/* Update stats */\n\tif (rc >= 0) {\n\t\tlsa->tx_packets++;\n\t\tlsa->tx_bytes += len;\n\t\trc = len;\n\t} else {\n\t\tlsa->tx_errors++;\n\t}\n\n\treturn rc;\n\nno_route:\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EHOSTUNREACH;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \tint rc;\n \tstruct l2tp_ip_sock *lsa = l2tp_ip_sk(sk);\n \tstruct inet_sock *inet = inet_sk(sk);\n-\tstruct ip_options *opt = inet->opt;\n \tstruct rtable *rt = NULL;\n \tint connected = 0;\n \t__be32 daddr;\n@@ -59,9 +58,14 @@\n \t\trt = (struct rtable *) __sk_dst_check(sk, 0);\n \n \tif (rt == NULL) {\n+\t\tstruct ip_options_rcu *inet_opt;\n+\n+\t\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n+\t\t\t\t\t\t     sock_owned_by_user(sk));\n+\n \t\t/* Use correct destination address if we have options. */\n-\t\tif (opt && opt->srr)\n-\t\t\tdaddr = opt->faddr;\n+\t\tif (inet_opt && inet_opt->opt.srr)\n+\t\t\tdaddr = inet_opt->opt.faddr;\n \n \t\t/* If this fails, retransmit mechanism of transport layer will\n \t\t * keep trying until route appears or the connection times",
        "function_modified_lines": {
            "added": [
                "\t\tstruct ip_options_rcu *inet_opt;",
                "",
                "\t\tinet_opt = rcu_dereference_protected(inet->inet_opt,",
                "\t\t\t\t\t\t     sock_owned_by_user(sk));",
                "",
                "\t\tif (inet_opt && inet_opt->opt.srr)",
                "\t\t\tdaddr = inet_opt->opt.faddr;"
            ],
            "deleted": [
                "\tstruct ip_options *opt = inet->opt;",
                "\t\tif (opt && opt->srr)",
                "\t\t\tdaddr = opt->faddr;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."
    },
    {
        "cve_id": "CVE-2012-4508",
        "code_before_change": "static int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint uninitialized;\n\tint split_flag1, flags1;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tuninitialized = ext4_ext_is_uninitialized(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_drop_refs(path);\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1;\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : map->m_len;\n}",
        "code_after_change": "static int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint uninitialized;\n\tint split_flag1, flags1;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tuninitialized = ext4_ext_is_uninitialized(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNINIT2;\n\t\tif (split_flag & EXT4_EXT_DATA_VALID2)\n\t\t\tsplit_flag1 |= EXT4_EXT_DATA_VALID1;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_drop_refs(path);\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & (EXT4_EXT_MAY_ZEROOUT |\n\t\t\t\t\t    EXT4_EXT_DATA_VALID2);\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1;\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : map->m_len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,12 +19,13 @@\n \tuninitialized = ext4_ext_is_uninitialized(ex);\n \n \tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n-\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n-\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n+\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;\n \t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n \t\tif (uninitialized)\n \t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1 |\n \t\t\t\t       EXT4_EXT_MARK_UNINIT2;\n+\t\tif (split_flag & EXT4_EXT_DATA_VALID2)\n+\t\t\tsplit_flag1 |= EXT4_EXT_DATA_VALID1;\n \t\terr = ext4_split_extent_at(handle, inode, path,\n \t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n \t\tif (err)\n@@ -37,8 +38,8 @@\n \t\treturn PTR_ERR(path);\n \n \tif (map->m_lblk >= ee_block) {\n-\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n-\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n+\t\tsplit_flag1 = split_flag & (EXT4_EXT_MAY_ZEROOUT |\n+\t\t\t\t\t    EXT4_EXT_DATA_VALID2);\n \t\tif (uninitialized)\n \t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1;\n \t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)",
        "function_modified_lines": {
            "added": [
                "\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;",
                "\t\tif (split_flag & EXT4_EXT_DATA_VALID2)",
                "\t\t\tsplit_flag1 |= EXT4_EXT_DATA_VALID1;",
                "\t\tsplit_flag1 = split_flag & (EXT4_EXT_MAY_ZEROOUT |",
                "\t\t\t\t\t    EXT4_EXT_DATA_VALID2);"
            ],
            "deleted": [
                "\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?",
                "\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;",
                "\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?",
                "\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized."
    },
    {
        "cve_id": "CVE-2012-4508",
        "code_before_change": "static int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT1)\n\t\text4_ext_mark_uninitialized(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\text4_ext_mark_uninitialized(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}",
        "code_after_change": "static int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==\n\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT1)\n\t\text4_ext_mark_uninitialized(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\text4_ext_mark_uninitialized(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {\n\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1)\n\t\t\t\terr = ext4_ext_zeroout(inode, ex2);\n\t\t\telse\n\t\t\t\terr = ext4_ext_zeroout(inode, ex);\n\t\t} else\n\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,9 @@\n \tstruct ext4_extent *ex2 = NULL;\n \tunsigned int ee_len, depth;\n \tint err = 0;\n+\n+\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==\n+\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));\n \n \text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n \t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n@@ -70,7 +73,14 @@\n \n \terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n \tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n-\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n+\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {\n+\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1)\n+\t\t\t\terr = ext4_ext_zeroout(inode, ex2);\n+\t\t\telse\n+\t\t\t\terr = ext4_ext_zeroout(inode, ex);\n+\t\t} else\n+\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n+\n \t\tif (err)\n \t\t\tgoto fix_extent_len;\n \t\t/* update the extent length and mark as initialized */",
        "function_modified_lines": {
            "added": [
                "",
                "\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==",
                "\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));",
                "\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {",
                "\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1)",
                "\t\t\t\terr = ext4_ext_zeroout(inode, ex2);",
                "\t\t\telse",
                "\t\t\t\terr = ext4_ext_zeroout(inode, ex);",
                "\t\t} else",
                "\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);",
                ""
            ],
            "deleted": [
                "\t\terr = ext4_ext_zeroout(inode, &orig_ex);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized."
    },
    {
        "cve_id": "CVE-2012-4508",
        "code_before_change": "static int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\ttrace_ext4_ext_handle_uninitialized_extents(inode, map, allocated,\n\t\t\t\t\t\t    newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\tret = ext4_split_unwritten_extents(handle, inode, map,\n\t\t\t\t\t\t   path, flags);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, path);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\tmap->m_lblk, map->m_len);\n\t\tif (reserved_clusters)\n\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\t     reserved_clusters,\n\t\t\t\t\t\t     0);\n\t}\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}",
        "code_after_change": "static int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\ttrace_ext4_ext_handle_uninitialized_extents(inode, map, allocated,\n\t\t\t\t\t\t    newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\tret = ext4_split_unwritten_extents(handle, inode, map,\n\t\t\t\t\t\t   path, flags);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, path);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\tmap->m_lblk, map->m_len);\n\t\tif (reserved_clusters)\n\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\t     reserved_clusters,\n\t\t\t\t\t\t     0);\n\t}\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,7 +38,7 @@\n \t}\n \t/* IO end_io complete, convert the filled extent to written */\n \tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n-\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,\n+\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,\n \t\t\t\t\t\t\tpath);\n \t\tif (ret >= 0) {\n \t\t\text4_update_inode_fsync_trans(handle, inode, 1);",
        "function_modified_lines": {
            "added": [
                "\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,"
            ],
            "deleted": [
                "\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized."
    },
    {
        "cve_id": "CVE-2012-4508",
        "code_before_change": "static int ext4_split_unwritten_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tint flags)\n{\n\text4_lblk_t eof_block;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len;\n\tint split_flag = 0, depth;\n\n\text_debug(\"ext4_split_unwritten_extents: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\tsplit_flag |= EXT4_EXT_MARK_UNINIT2;\n\n\tflags |= EXT4_GET_BLOCKS_PRE_IO;\n\treturn ext4_split_extent(handle, inode, path, map, split_flag, flags);\n}",
        "code_after_change": "static int ext4_split_unwritten_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tint flags)\n{\n\text4_lblk_t eof_block;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len;\n\tint split_flag = 0, depth;\n\n\text_debug(\"ext4_split_unwritten_extents: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\tsplit_flag |= EXT4_EXT_MARK_UNINIT2;\n\tif (flags & EXT4_GET_BLOCKS_CONVERT)\n\t\tsplit_flag |= EXT4_EXT_DATA_VALID2;\n\tflags |= EXT4_GET_BLOCKS_PRE_IO;\n\treturn ext4_split_extent(handle, inode, path, map, split_flag, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,7 +29,8 @@\n \n \tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n \tsplit_flag |= EXT4_EXT_MARK_UNINIT2;\n-\n+\tif (flags & EXT4_GET_BLOCKS_CONVERT)\n+\t\tsplit_flag |= EXT4_EXT_DATA_VALID2;\n \tflags |= EXT4_GET_BLOCKS_PRE_IO;\n \treturn ext4_split_extent(handle, inode, path, map, split_flag, flags);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (flags & EXT4_GET_BLOCKS_CONVERT)",
                "\t\tsplit_flag |= EXT4_EXT_DATA_VALID2;"
            ],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized."
    },
    {
        "cve_id": "CVE-2013-0871",
        "code_before_change": "int ptrace_request(struct task_struct *child, long request,\n\t\t   unsigned long addr, unsigned long data)\n{\n\tbool seized = child->ptrace & PT_SEIZED;\n\tint ret = -EIO;\n\tsiginfo_t siginfo, *si;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\tunsigned long flags;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\treturn generic_ptrace_peekdata(child, addr, data);\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\treturn generic_ptrace_pokedata(child, addr, data);\n\n#ifdef PTRACE_OLDSETOPTIONS\n\tcase PTRACE_OLDSETOPTIONS:\n#endif\n\tcase PTRACE_SETOPTIONS:\n\t\tret = ptrace_setoptions(child, data);\n\t\tbreak;\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user(child->ptrace_message, datalp);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user(datavp, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tif (copy_from_user(&siginfo, datavp, sizeof siginfo))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_INTERRUPT:\n\t\t/*\n\t\t * Stop tracee without any side-effect on signal or job\n\t\t * control.  At least one trap is guaranteed to happen\n\t\t * after this request.  If @child is already trapped, the\n\t\t * current trap is not disturbed and another trap will\n\t\t * happen after the current trap is ended with PTRACE_CONT.\n\t\t *\n\t\t * The actual trap might not be PTRACE_EVENT_STOP trap but\n\t\t * the pending condition is cleared regardless.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * INTERRUPT doesn't disturb existing trap sans one\n\t\t * exception.  If ptracer issued LISTEN for the current\n\t\t * STOP, this INTERRUPT should clear LISTEN and re-trap\n\t\t * tracee into STOP.\n\t\t */\n\t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n\t\t\tsignal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n\n\t\tunlock_task_sighand(child, &flags);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase PTRACE_LISTEN:\n\t\t/*\n\t\t * Listen for events.  Tracee must be in STOP.  It's not\n\t\t * resumed per-se but is not considered to be in TRACED by\n\t\t * wait(2) or ptrace(2).  If an async event (e.g. group\n\t\t * stop state change) happens, tracee will enter STOP trap\n\t\t * again.  Alternatively, ptracer can issue INTERRUPT to\n\t\t * finish listening and re-trap tracee into STOP.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\tsi = child->last_siginfo;\n\t\tif (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {\n\t\t\tchild->jobctl |= JOBCTL_LISTENING;\n\t\t\t/*\n\t\t\t * If NOTIFY is set, it means event happened between\n\t\t\t * start of this trap and now.  Trigger re-trap.\n\t\t\t */\n\t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n\t\t\t\tsignal_wake_up(child, true);\n\t\t\tret = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t\tbreak;\n\n\tcase PTRACE_DETACH:\t /* detach a process that was attached. */\n\t\tret = ptrace_detach(child, data);\n\t\tbreak;\n\n#ifdef CONFIG_BINFMT_ELF_FDPIC\n\tcase PTRACE_GETFDPIC: {\n\t\tstruct mm_struct *mm = get_task_mm(child);\n\t\tunsigned long tmp = 0;\n\n\t\tret = -ESRCH;\n\t\tif (!mm)\n\t\t\tbreak;\n\n\t\tswitch (addr) {\n\t\tcase PTRACE_GETFDPIC_EXEC:\n\t\t\ttmp = mm->context.exec_fdpic_loadmap;\n\t\t\tbreak;\n\t\tcase PTRACE_GETFDPIC_INTERP:\n\t\t\ttmp = mm->context.interp_fdpic_loadmap;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tmmput(mm);\n\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n\tcase PTRACE_SINGLESTEP:\n#endif\n#ifdef PTRACE_SINGLEBLOCK\n\tcase PTRACE_SINGLEBLOCK:\n#endif\n#ifdef PTRACE_SYSEMU\n\tcase PTRACE_SYSEMU:\n\tcase PTRACE_SYSEMU_SINGLESTEP:\n#endif\n\tcase PTRACE_SYSCALL:\n\tcase PTRACE_CONT:\n\t\treturn ptrace_resume(child, request, data);\n\n\tcase PTRACE_KILL:\n\t\tif (child->exit_state)\t/* already dead */\n\t\t\treturn 0;\n\t\treturn ptrace_resume(child, request, SIGKILL);\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct iovec __user *uiov = datavp;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(kiov.iov_base, &uiov->iov_base) ||\n\t\t    __get_user(kiov.iov_len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "int ptrace_request(struct task_struct *child, long request,\n\t\t   unsigned long addr, unsigned long data)\n{\n\tbool seized = child->ptrace & PT_SEIZED;\n\tint ret = -EIO;\n\tsiginfo_t siginfo, *si;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\tunsigned long flags;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\treturn generic_ptrace_peekdata(child, addr, data);\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\treturn generic_ptrace_pokedata(child, addr, data);\n\n#ifdef PTRACE_OLDSETOPTIONS\n\tcase PTRACE_OLDSETOPTIONS:\n#endif\n\tcase PTRACE_SETOPTIONS:\n\t\tret = ptrace_setoptions(child, data);\n\t\tbreak;\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user(child->ptrace_message, datalp);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user(datavp, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tif (copy_from_user(&siginfo, datavp, sizeof siginfo))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_INTERRUPT:\n\t\t/*\n\t\t * Stop tracee without any side-effect on signal or job\n\t\t * control.  At least one trap is guaranteed to happen\n\t\t * after this request.  If @child is already trapped, the\n\t\t * current trap is not disturbed and another trap will\n\t\t * happen after the current trap is ended with PTRACE_CONT.\n\t\t *\n\t\t * The actual trap might not be PTRACE_EVENT_STOP trap but\n\t\t * the pending condition is cleared regardless.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * INTERRUPT doesn't disturb existing trap sans one\n\t\t * exception.  If ptracer issued LISTEN for the current\n\t\t * STOP, this INTERRUPT should clear LISTEN and re-trap\n\t\t * tracee into STOP.\n\t\t */\n\t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n\n\t\tunlock_task_sighand(child, &flags);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase PTRACE_LISTEN:\n\t\t/*\n\t\t * Listen for events.  Tracee must be in STOP.  It's not\n\t\t * resumed per-se but is not considered to be in TRACED by\n\t\t * wait(2) or ptrace(2).  If an async event (e.g. group\n\t\t * stop state change) happens, tracee will enter STOP trap\n\t\t * again.  Alternatively, ptracer can issue INTERRUPT to\n\t\t * finish listening and re-trap tracee into STOP.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\tsi = child->last_siginfo;\n\t\tif (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {\n\t\t\tchild->jobctl |= JOBCTL_LISTENING;\n\t\t\t/*\n\t\t\t * If NOTIFY is set, it means event happened between\n\t\t\t * start of this trap and now.  Trigger re-trap.\n\t\t\t */\n\t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n\t\t\t\tptrace_signal_wake_up(child, true);\n\t\t\tret = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t\tbreak;\n\n\tcase PTRACE_DETACH:\t /* detach a process that was attached. */\n\t\tret = ptrace_detach(child, data);\n\t\tbreak;\n\n#ifdef CONFIG_BINFMT_ELF_FDPIC\n\tcase PTRACE_GETFDPIC: {\n\t\tstruct mm_struct *mm = get_task_mm(child);\n\t\tunsigned long tmp = 0;\n\n\t\tret = -ESRCH;\n\t\tif (!mm)\n\t\t\tbreak;\n\n\t\tswitch (addr) {\n\t\tcase PTRACE_GETFDPIC_EXEC:\n\t\t\ttmp = mm->context.exec_fdpic_loadmap;\n\t\t\tbreak;\n\t\tcase PTRACE_GETFDPIC_INTERP:\n\t\t\ttmp = mm->context.interp_fdpic_loadmap;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tmmput(mm);\n\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n\tcase PTRACE_SINGLESTEP:\n#endif\n#ifdef PTRACE_SINGLEBLOCK\n\tcase PTRACE_SINGLEBLOCK:\n#endif\n#ifdef PTRACE_SYSEMU\n\tcase PTRACE_SYSEMU:\n\tcase PTRACE_SYSEMU_SINGLESTEP:\n#endif\n\tcase PTRACE_SYSCALL:\n\tcase PTRACE_CONT:\n\t\treturn ptrace_resume(child, request, data);\n\n\tcase PTRACE_KILL:\n\t\tif (child->exit_state)\t/* already dead */\n\t\t\treturn 0;\n\t\treturn ptrace_resume(child, request, SIGKILL);\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct iovec __user *uiov = datavp;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(kiov.iov_base, &uiov->iov_base) ||\n\t\t    __get_user(kiov.iov_len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -60,7 +60,7 @@\n \t\t * tracee into STOP.\n \t\t */\n \t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n-\t\t\tsignal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n+\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n \n \t\tunlock_task_sighand(child, &flags);\n \t\tret = 0;\n@@ -86,7 +86,7 @@\n \t\t\t * start of this trap and now.  Trigger re-trap.\n \t\t\t */\n \t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n-\t\t\t\tsignal_wake_up(child, true);\n+\t\t\t\tptrace_signal_wake_up(child, true);\n \t\t\tret = 0;\n \t\t}\n \t\tunlock_task_sighand(child, &flags);",
        "function_modified_lines": {
            "added": [
                "\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);",
                "\t\t\t\tptrace_signal_wake_up(child, true);"
            ],
            "deleted": [
                "\t\t\tsignal_wake_up(child, child->jobctl & JOBCTL_LISTENING);",
                "\t\t\t\tsignal_wake_up(child, true);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death."
    },
    {
        "cve_id": "CVE-2013-0871",
        "code_before_change": "void __ptrace_unlink(struct task_struct *child)\n{\n\tBUG_ON(!child->ptrace);\n\n\tchild->ptrace = 0;\n\tchild->parent = child->real_parent;\n\tlist_del_init(&child->ptrace_entry);\n\n\tspin_lock(&child->sighand->siglock);\n\n\t/*\n\t * Clear all pending traps and TRAPPING.  TRAPPING should be\n\t * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.\n\t */\n\ttask_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);\n\ttask_clear_jobctl_trapping(child);\n\n\t/*\n\t * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and\n\t * @child isn't dead.\n\t */\n\tif (!(child->flags & PF_EXITING) &&\n\t    (child->signal->flags & SIGNAL_STOP_STOPPED ||\n\t     child->signal->group_stop_count)) {\n\t\tchild->jobctl |= JOBCTL_STOP_PENDING;\n\n\t\t/*\n\t\t * This is only possible if this thread was cloned by the\n\t\t * traced task running in the stopped group, set the signal\n\t\t * for the future reports.\n\t\t * FIXME: we should change ptrace_init_task() to handle this\n\t\t * case.\n\t\t */\n\t\tif (!(child->jobctl & JOBCTL_STOP_SIGMASK))\n\t\t\tchild->jobctl |= SIGSTOP;\n\t}\n\n\t/*\n\t * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick\n\t * @child in the butt.  Note that @resume should be used iff @child\n\t * is in TASK_TRACED; otherwise, we might unduly disrupt\n\t * TASK_KILLABLE sleeps.\n\t */\n\tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\n\t\tsignal_wake_up(child, task_is_traced(child));\n\n\tspin_unlock(&child->sighand->siglock);\n}",
        "code_after_change": "void __ptrace_unlink(struct task_struct *child)\n{\n\tBUG_ON(!child->ptrace);\n\n\tchild->ptrace = 0;\n\tchild->parent = child->real_parent;\n\tlist_del_init(&child->ptrace_entry);\n\n\tspin_lock(&child->sighand->siglock);\n\n\t/*\n\t * Clear all pending traps and TRAPPING.  TRAPPING should be\n\t * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.\n\t */\n\ttask_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);\n\ttask_clear_jobctl_trapping(child);\n\n\t/*\n\t * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and\n\t * @child isn't dead.\n\t */\n\tif (!(child->flags & PF_EXITING) &&\n\t    (child->signal->flags & SIGNAL_STOP_STOPPED ||\n\t     child->signal->group_stop_count)) {\n\t\tchild->jobctl |= JOBCTL_STOP_PENDING;\n\n\t\t/*\n\t\t * This is only possible if this thread was cloned by the\n\t\t * traced task running in the stopped group, set the signal\n\t\t * for the future reports.\n\t\t * FIXME: we should change ptrace_init_task() to handle this\n\t\t * case.\n\t\t */\n\t\tif (!(child->jobctl & JOBCTL_STOP_SIGMASK))\n\t\t\tchild->jobctl |= SIGSTOP;\n\t}\n\n\t/*\n\t * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick\n\t * @child in the butt.  Note that @resume should be used iff @child\n\t * is in TASK_TRACED; otherwise, we might unduly disrupt\n\t * TASK_KILLABLE sleeps.\n\t */\n\tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\n\t\tptrace_signal_wake_up(child, true);\n\n\tspin_unlock(&child->sighand->siglock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -42,7 +42,7 @@\n \t * TASK_KILLABLE sleeps.\n \t */\n \tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\n-\t\tsignal_wake_up(child, task_is_traced(child));\n+\t\tptrace_signal_wake_up(child, true);\n \n \tspin_unlock(&child->sighand->siglock);\n }",
        "function_modified_lines": {
            "added": [
                "\t\tptrace_signal_wake_up(child, true);"
            ],
            "deleted": [
                "\t\tsignal_wake_up(child, task_is_traced(child));"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death."
    },
    {
        "cve_id": "CVE-2013-0871",
        "code_before_change": "static int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up(task, 1);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT,\n\t\t\t    ptrace_trapping_sleep_fn, TASK_UNINTERRUPTIBLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}",
        "code_after_change": "static int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT,\n\t\t\t    ptrace_trapping_sleep_fn, TASK_UNINTERRUPTIBLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -81,7 +81,7 @@\n \t */\n \tif (task_is_stopped(task) &&\n \t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n-\t\tsignal_wake_up(task, 1);\n+\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n \n \tspin_unlock(&task->sighand->siglock);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tsignal_wake_up_state(task, __TASK_STOPPED);"
            ],
            "deleted": [
                "\t\tsignal_wake_up(task, 1);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death."
    },
    {
        "cve_id": "CVE-2013-0871",
        "code_before_change": "static void ptrace_trap_notify(struct task_struct *t)\n{\n\tWARN_ON_ONCE(!(t->ptrace & PT_SEIZED));\n\tassert_spin_locked(&t->sighand->siglock);\n\n\ttask_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);\n\tsignal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\n}",
        "code_after_change": "static void ptrace_trap_notify(struct task_struct *t)\n{\n\tWARN_ON_ONCE(!(t->ptrace & PT_SEIZED));\n\tassert_spin_locked(&t->sighand->siglock);\n\n\ttask_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);\n\tptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,5 +4,5 @@\n \tassert_spin_locked(&t->sighand->siglock);\n \n \ttask_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);\n-\tsignal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\n+\tptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);\n }",
        "function_modified_lines": {
            "added": [
                "\tptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);"
            ],
            "deleted": [
                "\tsignal_wake_up(t, t->jobctl & JOBCTL_LISTENING);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death."
    },
    {
        "cve_id": "CVE-2013-1792",
        "code_before_change": "int install_user_keyrings(void)\n{\n\tstruct user_struct *user;\n\tconst struct cred *cred;\n\tstruct key *uid_keyring, *session_keyring;\n\tkey_perm_t user_keyring_perm;\n\tchar buf[20];\n\tint ret;\n\tuid_t uid;\n\n\tuser_keyring_perm = (KEY_POS_ALL & ~KEY_POS_SETATTR) | KEY_USR_ALL;\n\tcred = current_cred();\n\tuser = cred->user;\n\tuid = from_kuid(cred->user_ns, user->uid);\n\n\tkenter(\"%p{%u}\", user, uid);\n\n\tif (user->uid_keyring) {\n\t\tkleave(\" = 0 [exist]\");\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&key_user_keyring_mutex);\n\tret = 0;\n\n\tif (!user->uid_keyring) {\n\t\t/* get the UID-specific keyring\n\t\t * - there may be one in existence already as it may have been\n\t\t *   pinned by a session, but the user_struct pointing to it\n\t\t *   may have been destroyed by setuid */\n\t\tsprintf(buf, \"_uid.%u\", uid);\n\n\t\tuid_keyring = find_keyring_by_name(buf, true);\n\t\tif (IS_ERR(uid_keyring)) {\n\t\t\tuid_keyring = keyring_alloc(buf, user->uid, INVALID_GID,\n\t\t\t\t\t\t    cred, user_keyring_perm,\n\t\t\t\t\t\t    KEY_ALLOC_IN_QUOTA, NULL);\n\t\t\tif (IS_ERR(uid_keyring)) {\n\t\t\t\tret = PTR_ERR(uid_keyring);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t}\n\n\t\t/* get a default session keyring (which might also exist\n\t\t * already) */\n\t\tsprintf(buf, \"_uid_ses.%u\", uid);\n\n\t\tsession_keyring = find_keyring_by_name(buf, true);\n\t\tif (IS_ERR(session_keyring)) {\n\t\t\tsession_keyring =\n\t\t\t\tkeyring_alloc(buf, user->uid, INVALID_GID,\n\t\t\t\t\t      cred, user_keyring_perm,\n\t\t\t\t\t      KEY_ALLOC_IN_QUOTA, NULL);\n\t\t\tif (IS_ERR(session_keyring)) {\n\t\t\t\tret = PTR_ERR(session_keyring);\n\t\t\t\tgoto error_release;\n\t\t\t}\n\n\t\t\t/* we install a link from the user session keyring to\n\t\t\t * the user keyring */\n\t\t\tret = key_link(session_keyring, uid_keyring);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error_release_both;\n\t\t}\n\n\t\t/* install the keyrings */\n\t\tuser->uid_keyring = uid_keyring;\n\t\tuser->session_keyring = session_keyring;\n\t}\n\n\tmutex_unlock(&key_user_keyring_mutex);\n\tkleave(\" = 0\");\n\treturn 0;\n\nerror_release_both:\n\tkey_put(session_keyring);\nerror_release:\n\tkey_put(uid_keyring);\nerror:\n\tmutex_unlock(&key_user_keyring_mutex);\n\tkleave(\" = %d\", ret);\n\treturn ret;\n}",
        "code_after_change": "int install_user_keyrings(void)\n{\n\tstruct user_struct *user;\n\tconst struct cred *cred;\n\tstruct key *uid_keyring, *session_keyring;\n\tkey_perm_t user_keyring_perm;\n\tchar buf[20];\n\tint ret;\n\tuid_t uid;\n\n\tuser_keyring_perm = (KEY_POS_ALL & ~KEY_POS_SETATTR) | KEY_USR_ALL;\n\tcred = current_cred();\n\tuser = cred->user;\n\tuid = from_kuid(cred->user_ns, user->uid);\n\n\tkenter(\"%p{%u}\", user, uid);\n\n\tif (user->uid_keyring && user->session_keyring) {\n\t\tkleave(\" = 0 [exist]\");\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&key_user_keyring_mutex);\n\tret = 0;\n\n\tif (!user->uid_keyring) {\n\t\t/* get the UID-specific keyring\n\t\t * - there may be one in existence already as it may have been\n\t\t *   pinned by a session, but the user_struct pointing to it\n\t\t *   may have been destroyed by setuid */\n\t\tsprintf(buf, \"_uid.%u\", uid);\n\n\t\tuid_keyring = find_keyring_by_name(buf, true);\n\t\tif (IS_ERR(uid_keyring)) {\n\t\t\tuid_keyring = keyring_alloc(buf, user->uid, INVALID_GID,\n\t\t\t\t\t\t    cred, user_keyring_perm,\n\t\t\t\t\t\t    KEY_ALLOC_IN_QUOTA, NULL);\n\t\t\tif (IS_ERR(uid_keyring)) {\n\t\t\t\tret = PTR_ERR(uid_keyring);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t}\n\n\t\t/* get a default session keyring (which might also exist\n\t\t * already) */\n\t\tsprintf(buf, \"_uid_ses.%u\", uid);\n\n\t\tsession_keyring = find_keyring_by_name(buf, true);\n\t\tif (IS_ERR(session_keyring)) {\n\t\t\tsession_keyring =\n\t\t\t\tkeyring_alloc(buf, user->uid, INVALID_GID,\n\t\t\t\t\t      cred, user_keyring_perm,\n\t\t\t\t\t      KEY_ALLOC_IN_QUOTA, NULL);\n\t\t\tif (IS_ERR(session_keyring)) {\n\t\t\t\tret = PTR_ERR(session_keyring);\n\t\t\t\tgoto error_release;\n\t\t\t}\n\n\t\t\t/* we install a link from the user session keyring to\n\t\t\t * the user keyring */\n\t\t\tret = key_link(session_keyring, uid_keyring);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error_release_both;\n\t\t}\n\n\t\t/* install the keyrings */\n\t\tuser->uid_keyring = uid_keyring;\n\t\tuser->session_keyring = session_keyring;\n\t}\n\n\tmutex_unlock(&key_user_keyring_mutex);\n\tkleave(\" = 0\");\n\treturn 0;\n\nerror_release_both:\n\tkey_put(session_keyring);\nerror_release:\n\tkey_put(uid_keyring);\nerror:\n\tmutex_unlock(&key_user_keyring_mutex);\n\tkleave(\" = %d\", ret);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,7 +15,7 @@\n \n \tkenter(\"%p{%u}\", user, uid);\n \n-\tif (user->uid_keyring) {\n+\tif (user->uid_keyring && user->session_keyring) {\n \t\tkleave(\" = 0 [exist]\");\n \t\treturn 0;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif (user->uid_keyring && user->session_keyring) {"
            ],
            "deleted": [
                "\tif (user->uid_keyring) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the install_user_keyrings function in security/keys/process_keys.c in the Linux kernel before 3.8.3 allows local users to cause a denial of service (NULL pointer dereference and system crash) via crafted keyctl system calls that trigger keyring operations in simultaneous threads."
    },
    {
        "cve_id": "CVE-2013-1935",
        "code_before_change": "static int vcpu_enter_guest(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\tbool req_int_win = !irqchip_in_kernel(vcpu->kvm) &&\n\t\tvcpu->run->request_interrupt_window;\n\tbool req_event;\n\n\tif (vcpu->requests) {\n\t\tif (kvm_check_request(KVM_REQ_MMU_RELOAD, vcpu))\n\t\t\tkvm_mmu_unload(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_MIGRATE_TIMER, vcpu))\n\t\t\t__kvm_migrate_timers(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_KVMCLOCK_UPDATE, vcpu)) {\n\t\t\tr = kvm_write_guest_time(vcpu);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_MMU_SYNC, vcpu))\n\t\t\tkvm_mmu_sync_roots(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu))\n\t\t\tkvm_x86_ops->tlb_flush(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_REPORT_TPR_ACCESS, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_TPR_ACCESS;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_TRIPLE_FAULT, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_DEACTIVATE_FPU, vcpu)) {\n\t\t\tvcpu->fpu_active = 0;\n\t\t\tkvm_x86_ops->fpu_deactivate(vcpu);\n\t\t}\n\t}\n\n\tr = kvm_mmu_reload(vcpu);\n\tif (unlikely(r))\n\t\tgoto out;\n\n\tpreempt_disable();\n\n\tkvm_x86_ops->prepare_guest_switch(vcpu);\n\tif (vcpu->fpu_active)\n\t\tkvm_load_guest_fpu(vcpu);\n\tkvm_load_guest_xcr0(vcpu);\n\n\tatomic_set(&vcpu->guest_mode, 1);\n\tsmp_wmb();\n\n\tlocal_irq_disable();\n\n\treq_event = kvm_check_request(KVM_REQ_EVENT, vcpu);\n\n\tif (!atomic_read(&vcpu->guest_mode) || vcpu->requests\n\t    || need_resched() || signal_pending(current)) {\n\t\tif (req_event)\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tatomic_set(&vcpu->guest_mode, 0);\n\t\tsmp_wmb();\n\t\tlocal_irq_enable();\n\t\tpreempt_enable();\n\t\tr = 1;\n\t\tgoto out;\n\t}\n\n\tif (req_event || req_int_win) {\n\t\tinject_pending_event(vcpu);\n\n\t\t/* enable NMI/IRQ window open exits if needed */\n\t\tif (vcpu->arch.nmi_pending)\n\t\t\tkvm_x86_ops->enable_nmi_window(vcpu);\n\t\telse if (kvm_cpu_has_interrupt(vcpu) || req_int_win)\n\t\t\tkvm_x86_ops->enable_irq_window(vcpu);\n\n\t\tif (kvm_lapic_enabled(vcpu)) {\n\t\t\tupdate_cr8_intercept(vcpu);\n\t\t\tkvm_lapic_sync_to_vapic(vcpu);\n\t\t}\n\t}\n\n\tsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);\n\n\tkvm_guest_enter();\n\n\tif (unlikely(vcpu->arch.switch_db_regs)) {\n\t\tset_debugreg(0, 7);\n\t\tset_debugreg(vcpu->arch.eff_db[0], 0);\n\t\tset_debugreg(vcpu->arch.eff_db[1], 1);\n\t\tset_debugreg(vcpu->arch.eff_db[2], 2);\n\t\tset_debugreg(vcpu->arch.eff_db[3], 3);\n\t}\n\n\ttrace_kvm_entry(vcpu->vcpu_id);\n\tkvm_x86_ops->run(vcpu);\n\n\t/*\n\t * If the guest has used debug registers, at least dr7\n\t * will be disabled while returning to the host.\n\t * If we don't have active breakpoints in the host, we don't\n\t * care about the messed up debug address registers. But if\n\t * we have some of them active, restore the old state.\n\t */\n\tif (hw_breakpoint_active())\n\t\thw_breakpoint_restore();\n\n\tkvm_get_msr(vcpu, MSR_IA32_TSC, &vcpu->arch.last_guest_tsc);\n\n\tatomic_set(&vcpu->guest_mode, 0);\n\tsmp_wmb();\n\tlocal_irq_enable();\n\n\t++vcpu->stat.exits;\n\n\t/*\n\t * We must have an instruction between local_irq_enable() and\n\t * kvm_guest_exit(), so the timer interrupt isn't delayed by\n\t * the interrupt shadow.  The stat.exits increment will do nicely.\n\t * But we need to prevent reordering, hence this barrier():\n\t */\n\tbarrier();\n\n\tkvm_guest_exit();\n\n\tpreempt_enable();\n\n\tvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\n\n\t/*\n\t * Profile KVM exit RIPs:\n\t */\n\tif (unlikely(prof_on == KVM_PROFILING)) {\n\t\tunsigned long rip = kvm_rip_read(vcpu);\n\t\tprofile_hit(KVM_PROFILING, (void *)rip);\n\t}\n\n\n\tkvm_lapic_sync_from_vapic(vcpu);\n\n\tr = kvm_x86_ops->handle_exit(vcpu);\nout:\n\treturn r;\n}",
        "code_after_change": "static int vcpu_enter_guest(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\tbool req_int_win = !irqchip_in_kernel(vcpu->kvm) &&\n\t\tvcpu->run->request_interrupt_window;\n\n\tif (vcpu->requests) {\n\t\tif (kvm_check_request(KVM_REQ_MMU_RELOAD, vcpu))\n\t\t\tkvm_mmu_unload(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_MIGRATE_TIMER, vcpu))\n\t\t\t__kvm_migrate_timers(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_KVMCLOCK_UPDATE, vcpu)) {\n\t\t\tr = kvm_write_guest_time(vcpu);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_MMU_SYNC, vcpu))\n\t\t\tkvm_mmu_sync_roots(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu))\n\t\t\tkvm_x86_ops->tlb_flush(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_REPORT_TPR_ACCESS, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_TPR_ACCESS;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_TRIPLE_FAULT, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_DEACTIVATE_FPU, vcpu)) {\n\t\t\tvcpu->fpu_active = 0;\n\t\t\tkvm_x86_ops->fpu_deactivate(vcpu);\n\t\t}\n\t}\n\n\tr = kvm_mmu_reload(vcpu);\n\tif (unlikely(r))\n\t\tgoto out;\n\n\tif (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win) {\n\t\tinject_pending_event(vcpu);\n\n\t\t/* enable NMI/IRQ window open exits if needed */\n\t\tif (vcpu->arch.nmi_pending)\n\t\t\tkvm_x86_ops->enable_nmi_window(vcpu);\n\t\telse if (kvm_cpu_has_interrupt(vcpu) || req_int_win)\n\t\t\tkvm_x86_ops->enable_irq_window(vcpu);\n\n\t\tif (kvm_lapic_enabled(vcpu)) {\n\t\t\tupdate_cr8_intercept(vcpu);\n\t\t\tkvm_lapic_sync_to_vapic(vcpu);\n\t\t}\n\t}\n\n\tpreempt_disable();\n\n\tkvm_x86_ops->prepare_guest_switch(vcpu);\n\tif (vcpu->fpu_active)\n\t\tkvm_load_guest_fpu(vcpu);\n\tkvm_load_guest_xcr0(vcpu);\n\n\tatomic_set(&vcpu->guest_mode, 1);\n\tsmp_wmb();\n\n\tlocal_irq_disable();\n\n\tif (!atomic_read(&vcpu->guest_mode) || vcpu->requests\n\t    || need_resched() || signal_pending(current)) {\n\t\tatomic_set(&vcpu->guest_mode, 0);\n\t\tsmp_wmb();\n\t\tlocal_irq_enable();\n\t\tpreempt_enable();\n\t\tkvm_x86_ops->cancel_injection(vcpu);\n\t\tr = 1;\n\t\tgoto out;\n\t}\n\n\tsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);\n\n\tkvm_guest_enter();\n\n\tif (unlikely(vcpu->arch.switch_db_regs)) {\n\t\tset_debugreg(0, 7);\n\t\tset_debugreg(vcpu->arch.eff_db[0], 0);\n\t\tset_debugreg(vcpu->arch.eff_db[1], 1);\n\t\tset_debugreg(vcpu->arch.eff_db[2], 2);\n\t\tset_debugreg(vcpu->arch.eff_db[3], 3);\n\t}\n\n\ttrace_kvm_entry(vcpu->vcpu_id);\n\tkvm_x86_ops->run(vcpu);\n\n\t/*\n\t * If the guest has used debug registers, at least dr7\n\t * will be disabled while returning to the host.\n\t * If we don't have active breakpoints in the host, we don't\n\t * care about the messed up debug address registers. But if\n\t * we have some of them active, restore the old state.\n\t */\n\tif (hw_breakpoint_active())\n\t\thw_breakpoint_restore();\n\n\tkvm_get_msr(vcpu, MSR_IA32_TSC, &vcpu->arch.last_guest_tsc);\n\n\tatomic_set(&vcpu->guest_mode, 0);\n\tsmp_wmb();\n\tlocal_irq_enable();\n\n\t++vcpu->stat.exits;\n\n\t/*\n\t * We must have an instruction between local_irq_enable() and\n\t * kvm_guest_exit(), so the timer interrupt isn't delayed by\n\t * the interrupt shadow.  The stat.exits increment will do nicely.\n\t * But we need to prevent reordering, hence this barrier():\n\t */\n\tbarrier();\n\n\tkvm_guest_exit();\n\n\tpreempt_enable();\n\n\tvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\n\n\t/*\n\t * Profile KVM exit RIPs:\n\t */\n\tif (unlikely(prof_on == KVM_PROFILING)) {\n\t\tunsigned long rip = kvm_rip_read(vcpu);\n\t\tprofile_hit(KVM_PROFILING, (void *)rip);\n\t}\n\n\n\tkvm_lapic_sync_from_vapic(vcpu);\n\n\tr = kvm_x86_ops->handle_exit(vcpu);\nout:\n\treturn r;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,6 @@\n \tint r;\n \tbool req_int_win = !irqchip_in_kernel(vcpu->kvm) &&\n \t\tvcpu->run->request_interrupt_window;\n-\tbool req_event;\n \n \tif (vcpu->requests) {\n \t\tif (kvm_check_request(KVM_REQ_MMU_RELOAD, vcpu))\n@@ -39,6 +38,21 @@\n \tif (unlikely(r))\n \t\tgoto out;\n \n+\tif (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win) {\n+\t\tinject_pending_event(vcpu);\n+\n+\t\t/* enable NMI/IRQ window open exits if needed */\n+\t\tif (vcpu->arch.nmi_pending)\n+\t\t\tkvm_x86_ops->enable_nmi_window(vcpu);\n+\t\telse if (kvm_cpu_has_interrupt(vcpu) || req_int_win)\n+\t\t\tkvm_x86_ops->enable_irq_window(vcpu);\n+\n+\t\tif (kvm_lapic_enabled(vcpu)) {\n+\t\t\tupdate_cr8_intercept(vcpu);\n+\t\t\tkvm_lapic_sync_to_vapic(vcpu);\n+\t\t}\n+\t}\n+\n \tpreempt_disable();\n \n \tkvm_x86_ops->prepare_guest_switch(vcpu);\n@@ -51,33 +65,15 @@\n \n \tlocal_irq_disable();\n \n-\treq_event = kvm_check_request(KVM_REQ_EVENT, vcpu);\n-\n \tif (!atomic_read(&vcpu->guest_mode) || vcpu->requests\n \t    || need_resched() || signal_pending(current)) {\n-\t\tif (req_event)\n-\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n \t\tatomic_set(&vcpu->guest_mode, 0);\n \t\tsmp_wmb();\n \t\tlocal_irq_enable();\n \t\tpreempt_enable();\n+\t\tkvm_x86_ops->cancel_injection(vcpu);\n \t\tr = 1;\n \t\tgoto out;\n-\t}\n-\n-\tif (req_event || req_int_win) {\n-\t\tinject_pending_event(vcpu);\n-\n-\t\t/* enable NMI/IRQ window open exits if needed */\n-\t\tif (vcpu->arch.nmi_pending)\n-\t\t\tkvm_x86_ops->enable_nmi_window(vcpu);\n-\t\telse if (kvm_cpu_has_interrupt(vcpu) || req_int_win)\n-\t\t\tkvm_x86_ops->enable_irq_window(vcpu);\n-\n-\t\tif (kvm_lapic_enabled(vcpu)) {\n-\t\t\tupdate_cr8_intercept(vcpu);\n-\t\t\tkvm_lapic_sync_to_vapic(vcpu);\n-\t\t}\n \t}\n \n \tsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);",
        "function_modified_lines": {
            "added": [
                "\tif (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win) {",
                "\t\tinject_pending_event(vcpu);",
                "",
                "\t\t/* enable NMI/IRQ window open exits if needed */",
                "\t\tif (vcpu->arch.nmi_pending)",
                "\t\t\tkvm_x86_ops->enable_nmi_window(vcpu);",
                "\t\telse if (kvm_cpu_has_interrupt(vcpu) || req_int_win)",
                "\t\t\tkvm_x86_ops->enable_irq_window(vcpu);",
                "",
                "\t\tif (kvm_lapic_enabled(vcpu)) {",
                "\t\t\tupdate_cr8_intercept(vcpu);",
                "\t\t\tkvm_lapic_sync_to_vapic(vcpu);",
                "\t\t}",
                "\t}",
                "",
                "\t\tkvm_x86_ops->cancel_injection(vcpu);"
            ],
            "deleted": [
                "\tbool req_event;",
                "\treq_event = kvm_check_request(KVM_REQ_EVENT, vcpu);",
                "",
                "\t\tif (req_event)",
                "\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);",
                "\t}",
                "",
                "\tif (req_event || req_int_win) {",
                "\t\tinject_pending_event(vcpu);",
                "",
                "\t\t/* enable NMI/IRQ window open exits if needed */",
                "\t\tif (vcpu->arch.nmi_pending)",
                "\t\t\tkvm_x86_ops->enable_nmi_window(vcpu);",
                "\t\telse if (kvm_cpu_has_interrupt(vcpu) || req_int_win)",
                "\t\t\tkvm_x86_ops->enable_irq_window(vcpu);",
                "",
                "\t\tif (kvm_lapic_enabled(vcpu)) {",
                "\t\t\tupdate_cr8_intercept(vcpu);",
                "\t\t\tkvm_lapic_sync_to_vapic(vcpu);",
                "\t\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A certain Red Hat patch to the KVM subsystem in the kernel package before 2.6.32-358.11.1.el6 on Red Hat Enterprise Linux (RHEL) 6 does not properly implement the PV EOI feature, which allows guest OS users to cause a denial of service (host OS crash) by leveraging a time window during which interrupts are disabled but copy_to_user function calls are possible."
    },
    {
        "cve_id": "CVE-2013-3302",
        "code_before_change": "static int\nsmb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)\n{\n\tint rc;\n\tstruct kvec *iov = rqst->rq_iov;\n\tint n_vec = rqst->rq_nvec;\n\tunsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);\n\tunsigned int i;\n\tsize_t total_len = 0, sent;\n\tstruct socket *ssocket = server->ssocket;\n\tint val = 1;\n\n\tcFYI(1, \"Sending smb: smb_len=%u\", smb_buf_length);\n\tdump_smb(iov[0].iov_base, iov[0].iov_len);\n\n\t/* cork the socket */\n\tkernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,\n\t\t\t\t(char *)&val, sizeof(val));\n\n\trc = smb_send_kvec(server, iov, n_vec, &sent);\n\tif (rc < 0)\n\t\tgoto uncork;\n\n\ttotal_len += sent;\n\n\t/* now walk the page array and send each page in it */\n\tfor (i = 0; i < rqst->rq_npages; i++) {\n\t\tstruct kvec p_iov;\n\n\t\tcifs_rqst_page_to_kvec(rqst, i, &p_iov);\n\t\trc = smb_send_kvec(server, &p_iov, 1, &sent);\n\t\tkunmap(rqst->rq_pages[i]);\n\t\tif (rc < 0)\n\t\t\tbreak;\n\n\t\ttotal_len += sent;\n\t}\n\nuncork:\n\t/* uncork it */\n\tval = 0;\n\tkernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,\n\t\t\t\t(char *)&val, sizeof(val));\n\n\tif ((total_len > 0) && (total_len != smb_buf_length + 4)) {\n\t\tcFYI(1, \"partial send (wanted=%u sent=%zu): terminating \"\n\t\t\t\"session\", smb_buf_length + 4, total_len);\n\t\t/*\n\t\t * If we have only sent part of an SMB then the next SMB could\n\t\t * be taken as the remainder of this one. We need to kill the\n\t\t * socket so the server throws away the partial SMB\n\t\t */\n\t\tserver->tcpStatus = CifsNeedReconnect;\n\t}\n\n\tif (rc < 0 && rc != -EINTR)\n\t\tcERROR(1, \"Error %d sending data on socket to server\", rc);\n\telse\n\t\trc = 0;\n\n\treturn rc;\n}",
        "code_after_change": "static int\nsmb_send_rqst(struct TCP_Server_Info *server, struct smb_rqst *rqst)\n{\n\tint rc;\n\tstruct kvec *iov = rqst->rq_iov;\n\tint n_vec = rqst->rq_nvec;\n\tunsigned int smb_buf_length = get_rfc1002_length(iov[0].iov_base);\n\tunsigned int i;\n\tsize_t total_len = 0, sent;\n\tstruct socket *ssocket = server->ssocket;\n\tint val = 1;\n\n\tif (ssocket == NULL)\n\t\treturn -ENOTSOCK;\n\n\tcFYI(1, \"Sending smb: smb_len=%u\", smb_buf_length);\n\tdump_smb(iov[0].iov_base, iov[0].iov_len);\n\n\t/* cork the socket */\n\tkernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,\n\t\t\t\t(char *)&val, sizeof(val));\n\n\trc = smb_send_kvec(server, iov, n_vec, &sent);\n\tif (rc < 0)\n\t\tgoto uncork;\n\n\ttotal_len += sent;\n\n\t/* now walk the page array and send each page in it */\n\tfor (i = 0; i < rqst->rq_npages; i++) {\n\t\tstruct kvec p_iov;\n\n\t\tcifs_rqst_page_to_kvec(rqst, i, &p_iov);\n\t\trc = smb_send_kvec(server, &p_iov, 1, &sent);\n\t\tkunmap(rqst->rq_pages[i]);\n\t\tif (rc < 0)\n\t\t\tbreak;\n\n\t\ttotal_len += sent;\n\t}\n\nuncork:\n\t/* uncork it */\n\tval = 0;\n\tkernel_setsockopt(ssocket, SOL_TCP, TCP_CORK,\n\t\t\t\t(char *)&val, sizeof(val));\n\n\tif ((total_len > 0) && (total_len != smb_buf_length + 4)) {\n\t\tcFYI(1, \"partial send (wanted=%u sent=%zu): terminating \"\n\t\t\t\"session\", smb_buf_length + 4, total_len);\n\t\t/*\n\t\t * If we have only sent part of an SMB then the next SMB could\n\t\t * be taken as the remainder of this one. We need to kill the\n\t\t * socket so the server throws away the partial SMB\n\t\t */\n\t\tserver->tcpStatus = CifsNeedReconnect;\n\t}\n\n\tif (rc < 0 && rc != -EINTR)\n\t\tcERROR(1, \"Error %d sending data on socket to server\", rc);\n\telse\n\t\trc = 0;\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,9 @@\n \tsize_t total_len = 0, sent;\n \tstruct socket *ssocket = server->ssocket;\n \tint val = 1;\n+\n+\tif (ssocket == NULL)\n+\t\treturn -ENOTSOCK;\n \n \tcFYI(1, \"Sending smb: smb_len=%u\", smb_buf_length);\n \tdump_smb(iov[0].iov_base, iov[0].iov_len);",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (ssocket == NULL)",
                "\t\treturn -ENOTSOCK;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the smb_send_rqst function in fs/cifs/transport.c in the Linux kernel before 3.7.2 allows local users to cause a denial of service (NULL pointer dereference and OOPS) or possibly have unspecified other impact via vectors involving a reconnection event."
    },
    {
        "cve_id": "CVE-2013-3302",
        "code_before_change": "static int\nsmb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,\n\t\tsize_t *sent)\n{\n\tint rc = 0;\n\tint i = 0;\n\tstruct msghdr smb_msg;\n\tunsigned int remaining;\n\tsize_t first_vec = 0;\n\tstruct socket *ssocket = server->ssocket;\n\n\t*sent = 0;\n\n\tif (ssocket == NULL)\n\t\treturn -ENOTSOCK; /* BB eventually add reconnect code here */\n\n\tsmb_msg.msg_name = (struct sockaddr *) &server->dstaddr;\n\tsmb_msg.msg_namelen = sizeof(struct sockaddr);\n\tsmb_msg.msg_control = NULL;\n\tsmb_msg.msg_controllen = 0;\n\tif (server->noblocksnd)\n\t\tsmb_msg.msg_flags = MSG_DONTWAIT + MSG_NOSIGNAL;\n\telse\n\t\tsmb_msg.msg_flags = MSG_NOSIGNAL;\n\n\tremaining = 0;\n\tfor (i = 0; i < n_vec; i++)\n\t\tremaining += iov[i].iov_len;\n\n\ti = 0;\n\twhile (remaining) {\n\t\t/*\n\t\t * If blocking send, we try 3 times, since each can block\n\t\t * for 5 seconds. For nonblocking  we have to try more\n\t\t * but wait increasing amounts of time allowing time for\n\t\t * socket to clear.  The overall time we wait in either\n\t\t * case to send on the socket is about 15 seconds.\n\t\t * Similarly we wait for 15 seconds for a response from\n\t\t * the server in SendReceive[2] for the server to send\n\t\t * a response back for most types of requests (except\n\t\t * SMB Write past end of file which can be slow, and\n\t\t * blocking lock operations). NFS waits slightly longer\n\t\t * than CIFS, but this can make it take longer for\n\t\t * nonresponsive servers to be detected and 15 seconds\n\t\t * is more than enough time for modern networks to\n\t\t * send a packet.  In most cases if we fail to send\n\t\t * after the retries we will kill the socket and\n\t\t * reconnect which may clear the network problem.\n\t\t */\n\t\trc = kernel_sendmsg(ssocket, &smb_msg, &iov[first_vec],\n\t\t\t\t    n_vec - first_vec, remaining);\n\t\tif (rc == -ENOSPC || rc == -EAGAIN) {\n\t\t\t/*\n\t\t\t * Catch if a low level driver returns -ENOSPC. This\n\t\t\t * WARN_ON will be removed by 3.10 if no one reports\n\t\t\t * seeing this.\n\t\t\t */\n\t\t\tWARN_ON_ONCE(rc == -ENOSPC);\n\t\t\ti++;\n\t\t\tif (i >= 14 || (!server->noblocksnd && (i > 2))) {\n\t\t\t\tcERROR(1, \"sends on sock %p stuck for 15 \"\n\t\t\t\t\t  \"seconds\", ssocket);\n\t\t\t\trc = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmsleep(1 << i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (rc < 0)\n\t\t\tbreak;\n\n\t\t/* send was at least partially successful */\n\t\t*sent += rc;\n\n\t\tif (rc == remaining) {\n\t\t\tremaining = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc > remaining) {\n\t\t\tcERROR(1, \"sent %d requested %d\", rc, remaining);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc == 0) {\n\t\t\t/* should never happen, letting socket clear before\n\t\t\t   retrying is our only obvious option here */\n\t\t\tcERROR(1, \"tcp sent no data\");\n\t\t\tmsleep(500);\n\t\t\tcontinue;\n\t\t}\n\n\t\tremaining -= rc;\n\n\t\t/* the line below resets i */\n\t\tfor (i = first_vec; i < n_vec; i++) {\n\t\t\tif (iov[i].iov_len) {\n\t\t\t\tif (rc > iov[i].iov_len) {\n\t\t\t\t\trc -= iov[i].iov_len;\n\t\t\t\t\tiov[i].iov_len = 0;\n\t\t\t\t} else {\n\t\t\t\t\tiov[i].iov_base += rc;\n\t\t\t\t\tiov[i].iov_len -= rc;\n\t\t\t\t\tfirst_vec = i;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\ti = 0; /* in case we get ENOSPC on the next send */\n\t\trc = 0;\n\t}\n\treturn rc;\n}",
        "code_after_change": "static int\nsmb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,\n\t\tsize_t *sent)\n{\n\tint rc = 0;\n\tint i = 0;\n\tstruct msghdr smb_msg;\n\tunsigned int remaining;\n\tsize_t first_vec = 0;\n\tstruct socket *ssocket = server->ssocket;\n\n\t*sent = 0;\n\n\tsmb_msg.msg_name = (struct sockaddr *) &server->dstaddr;\n\tsmb_msg.msg_namelen = sizeof(struct sockaddr);\n\tsmb_msg.msg_control = NULL;\n\tsmb_msg.msg_controllen = 0;\n\tif (server->noblocksnd)\n\t\tsmb_msg.msg_flags = MSG_DONTWAIT + MSG_NOSIGNAL;\n\telse\n\t\tsmb_msg.msg_flags = MSG_NOSIGNAL;\n\n\tremaining = 0;\n\tfor (i = 0; i < n_vec; i++)\n\t\tremaining += iov[i].iov_len;\n\n\ti = 0;\n\twhile (remaining) {\n\t\t/*\n\t\t * If blocking send, we try 3 times, since each can block\n\t\t * for 5 seconds. For nonblocking  we have to try more\n\t\t * but wait increasing amounts of time allowing time for\n\t\t * socket to clear.  The overall time we wait in either\n\t\t * case to send on the socket is about 15 seconds.\n\t\t * Similarly we wait for 15 seconds for a response from\n\t\t * the server in SendReceive[2] for the server to send\n\t\t * a response back for most types of requests (except\n\t\t * SMB Write past end of file which can be slow, and\n\t\t * blocking lock operations). NFS waits slightly longer\n\t\t * than CIFS, but this can make it take longer for\n\t\t * nonresponsive servers to be detected and 15 seconds\n\t\t * is more than enough time for modern networks to\n\t\t * send a packet.  In most cases if we fail to send\n\t\t * after the retries we will kill the socket and\n\t\t * reconnect which may clear the network problem.\n\t\t */\n\t\trc = kernel_sendmsg(ssocket, &smb_msg, &iov[first_vec],\n\t\t\t\t    n_vec - first_vec, remaining);\n\t\tif (rc == -ENOSPC || rc == -EAGAIN) {\n\t\t\t/*\n\t\t\t * Catch if a low level driver returns -ENOSPC. This\n\t\t\t * WARN_ON will be removed by 3.10 if no one reports\n\t\t\t * seeing this.\n\t\t\t */\n\t\t\tWARN_ON_ONCE(rc == -ENOSPC);\n\t\t\ti++;\n\t\t\tif (i >= 14 || (!server->noblocksnd && (i > 2))) {\n\t\t\t\tcERROR(1, \"sends on sock %p stuck for 15 \"\n\t\t\t\t\t  \"seconds\", ssocket);\n\t\t\t\trc = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmsleep(1 << i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (rc < 0)\n\t\t\tbreak;\n\n\t\t/* send was at least partially successful */\n\t\t*sent += rc;\n\n\t\tif (rc == remaining) {\n\t\t\tremaining = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc > remaining) {\n\t\t\tcERROR(1, \"sent %d requested %d\", rc, remaining);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc == 0) {\n\t\t\t/* should never happen, letting socket clear before\n\t\t\t   retrying is our only obvious option here */\n\t\t\tcERROR(1, \"tcp sent no data\");\n\t\t\tmsleep(500);\n\t\t\tcontinue;\n\t\t}\n\n\t\tremaining -= rc;\n\n\t\t/* the line below resets i */\n\t\tfor (i = first_vec; i < n_vec; i++) {\n\t\t\tif (iov[i].iov_len) {\n\t\t\t\tif (rc > iov[i].iov_len) {\n\t\t\t\t\trc -= iov[i].iov_len;\n\t\t\t\t\tiov[i].iov_len = 0;\n\t\t\t\t} else {\n\t\t\t\t\tiov[i].iov_base += rc;\n\t\t\t\t\tiov[i].iov_len -= rc;\n\t\t\t\t\tfirst_vec = i;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\ti = 0; /* in case we get ENOSPC on the next send */\n\t\trc = 0;\n\t}\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,9 +10,6 @@\n \tstruct socket *ssocket = server->ssocket;\n \n \t*sent = 0;\n-\n-\tif (ssocket == NULL)\n-\t\treturn -ENOTSOCK; /* BB eventually add reconnect code here */\n \n \tsmb_msg.msg_name = (struct sockaddr *) &server->dstaddr;\n \tsmb_msg.msg_namelen = sizeof(struct sockaddr);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tif (ssocket == NULL)",
                "\t\treturn -ENOTSOCK; /* BB eventually add reconnect code here */"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the smb_send_rqst function in fs/cifs/transport.c in the Linux kernel before 3.7.2 allows local users to cause a denial of service (NULL pointer dereference and OOPS) or possibly have unspecified other impact via vectors involving a reconnection event."
    },
    {
        "cve_id": "CVE-2013-7026",
        "code_before_change": "static void shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)\n{\n\tns->shm_tot -= (shp->shm_segsz + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tshm_rmid(ns, shp);\n\tshm_unlock(shp);\n\tif (!is_file_hugepages(shp->shm_file))\n\t\tshmem_lock(shp->shm_file, 0, shp->mlock_user);\n\telse if (shp->mlock_user)\n\t\tuser_shm_unlock(file_inode(shp->shm_file)->i_size,\n\t\t\t\t\t\tshp->mlock_user);\n\tfput (shp->shm_file);\n\tipc_rcu_putref(shp, shm_rcu_free);\n}",
        "code_after_change": "static void shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)\n{\n\tstruct file *shm_file;\n\n\tshm_file = shp->shm_file;\n\tshp->shm_file = NULL;\n\tns->shm_tot -= (shp->shm_segsz + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tshm_rmid(ns, shp);\n\tshm_unlock(shp);\n\tif (!is_file_hugepages(shm_file))\n\t\tshmem_lock(shm_file, 0, shp->mlock_user);\n\telse if (shp->mlock_user)\n\t\tuser_shm_unlock(file_inode(shm_file)->i_size, shp->mlock_user);\n\tfput(shm_file);\n\tipc_rcu_putref(shp, shm_rcu_free);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,13 +1,16 @@\n static void shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)\n {\n+\tstruct file *shm_file;\n+\n+\tshm_file = shp->shm_file;\n+\tshp->shm_file = NULL;\n \tns->shm_tot -= (shp->shm_segsz + PAGE_SIZE - 1) >> PAGE_SHIFT;\n \tshm_rmid(ns, shp);\n \tshm_unlock(shp);\n-\tif (!is_file_hugepages(shp->shm_file))\n-\t\tshmem_lock(shp->shm_file, 0, shp->mlock_user);\n+\tif (!is_file_hugepages(shm_file))\n+\t\tshmem_lock(shm_file, 0, shp->mlock_user);\n \telse if (shp->mlock_user)\n-\t\tuser_shm_unlock(file_inode(shp->shm_file)->i_size,\n-\t\t\t\t\t\tshp->mlock_user);\n-\tfput (shp->shm_file);\n+\t\tuser_shm_unlock(file_inode(shm_file)->i_size, shp->mlock_user);\n+\tfput(shm_file);\n \tipc_rcu_putref(shp, shm_rcu_free);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct file *shm_file;",
                "",
                "\tshm_file = shp->shm_file;",
                "\tshp->shm_file = NULL;",
                "\tif (!is_file_hugepages(shm_file))",
                "\t\tshmem_lock(shm_file, 0, shp->mlock_user);",
                "\t\tuser_shm_unlock(file_inode(shm_file)->i_size, shp->mlock_user);",
                "\tfput(shm_file);"
            ],
            "deleted": [
                "\tif (!is_file_hugepages(shp->shm_file))",
                "\t\tshmem_lock(shp->shm_file, 0, shp->mlock_user);",
                "\t\tuser_shm_unlock(file_inode(shp->shm_file)->i_size,",
                "\t\t\t\t\t\tshp->mlock_user);",
                "\tfput (shp->shm_file);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in ipc/shm.c in the Linux kernel before 3.12.2 allow local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via a crafted application that uses shmctl IPC_RMID operations in conjunction with other shm system calls."
    },
    {
        "cve_id": "CVE-2013-7026",
        "code_before_change": "\nSYSCALL_DEFINE3(shmctl, int, shmid, int, cmd, struct shmid_ds __user *, buf)\n{\n\tstruct shmid_kernel *shp;\n\tint err, version;\n\tstruct ipc_namespace *ns;\n\n\tif (cmd < 0 || shmid < 0)\n\t\treturn -EINVAL;\n\n\tversion = ipc_parse_version(&cmd);\n\tns = current->nsproxy->ipc_ns;\n\n\tswitch (cmd) {\n\tcase IPC_INFO:\n\tcase SHM_INFO:\n\tcase SHM_STAT:\n\tcase IPC_STAT:\n\t\treturn shmctl_nolock(ns, shmid, cmd, version, buf);\n\tcase IPC_RMID:\n\tcase IPC_SET:\n\t\treturn shmctl_down(ns, shmid, cmd, buf, version);\n\tcase SHM_LOCK:\n\tcase SHM_UNLOCK:\n\t{\n\t\tstruct file *shm_file;\n\n\t\trcu_read_lock();\n\t\tshp = shm_obtain_object_check(ns, shmid);\n\t\tif (IS_ERR(shp)) {\n\t\t\terr = PTR_ERR(shp);\n\t\t\tgoto out_unlock1;\n\t\t}\n\n\t\taudit_ipc_obj(&(shp->shm_perm));\n\t\terr = security_shm_shmctl(shp, cmd);\n\t\tif (err)\n\t\t\tgoto out_unlock1;\n\n\t\tipc_lock_object(&shp->shm_perm);\n\t\tif (!ns_capable(ns->user_ns, CAP_IPC_LOCK)) {\n\t\t\tkuid_t euid = current_euid();\n\t\t\terr = -EPERM;\n\t\t\tif (!uid_eq(euid, shp->shm_perm.uid) &&\n\t\t\t    !uid_eq(euid, shp->shm_perm.cuid))\n\t\t\t\tgoto out_unlock0;\n\t\t\tif (cmd == SHM_LOCK && !rlimit(RLIMIT_MEMLOCK))\n\t\t\t\tgoto out_unlock0;\n\t\t}\n\n\t\tshm_file = shp->shm_file;\n\t\tif (is_file_hugepages(shm_file))\n\t\t\tgoto out_unlock0;\n\n\t\tif (cmd == SHM_LOCK) {\n\t\t\tstruct user_struct *user = current_user();\n\t\t\terr = shmem_lock(shm_file, 1, user);\n\t\t\tif (!err && !(shp->shm_perm.mode & SHM_LOCKED)) {\n\t\t\t\tshp->shm_perm.mode |= SHM_LOCKED;\n\t\t\t\tshp->mlock_user = user;\n\t\t\t}\n\t\t\tgoto out_unlock0;\n\t\t}\n\n\t\t/* SHM_UNLOCK */\n\t\tif (!(shp->shm_perm.mode & SHM_LOCKED))\n\t\t\tgoto out_unlock0;\n\t\tshmem_lock(shm_file, 0, shp->mlock_user);\n\t\tshp->shm_perm.mode &= ~SHM_LOCKED;\n\t\tshp->mlock_user = NULL;\n\t\tget_file(shm_file);\n\t\tipc_unlock_object(&shp->shm_perm);\n\t\trcu_read_unlock();\n\t\tshmem_unlock_mapping(shm_file->f_mapping);\n\n\t\tfput(shm_file);\n\t\treturn err;\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\nout_unlock0:\n\tipc_unlock_object(&shp->shm_perm);\nout_unlock1:\n\trcu_read_unlock();\n\treturn err;\n}",
        "code_after_change": "\nSYSCALL_DEFINE3(shmctl, int, shmid, int, cmd, struct shmid_ds __user *, buf)\n{\n\tstruct shmid_kernel *shp;\n\tint err, version;\n\tstruct ipc_namespace *ns;\n\n\tif (cmd < 0 || shmid < 0)\n\t\treturn -EINVAL;\n\n\tversion = ipc_parse_version(&cmd);\n\tns = current->nsproxy->ipc_ns;\n\n\tswitch (cmd) {\n\tcase IPC_INFO:\n\tcase SHM_INFO:\n\tcase SHM_STAT:\n\tcase IPC_STAT:\n\t\treturn shmctl_nolock(ns, shmid, cmd, version, buf);\n\tcase IPC_RMID:\n\tcase IPC_SET:\n\t\treturn shmctl_down(ns, shmid, cmd, buf, version);\n\tcase SHM_LOCK:\n\tcase SHM_UNLOCK:\n\t{\n\t\tstruct file *shm_file;\n\n\t\trcu_read_lock();\n\t\tshp = shm_obtain_object_check(ns, shmid);\n\t\tif (IS_ERR(shp)) {\n\t\t\terr = PTR_ERR(shp);\n\t\t\tgoto out_unlock1;\n\t\t}\n\n\t\taudit_ipc_obj(&(shp->shm_perm));\n\t\terr = security_shm_shmctl(shp, cmd);\n\t\tif (err)\n\t\t\tgoto out_unlock1;\n\n\t\tipc_lock_object(&shp->shm_perm);\n\t\tif (!ns_capable(ns->user_ns, CAP_IPC_LOCK)) {\n\t\t\tkuid_t euid = current_euid();\n\t\t\terr = -EPERM;\n\t\t\tif (!uid_eq(euid, shp->shm_perm.uid) &&\n\t\t\t    !uid_eq(euid, shp->shm_perm.cuid))\n\t\t\t\tgoto out_unlock0;\n\t\t\tif (cmd == SHM_LOCK && !rlimit(RLIMIT_MEMLOCK))\n\t\t\t\tgoto out_unlock0;\n\t\t}\n\n\t\tshm_file = shp->shm_file;\n\n\t\t/* check if shm_destroy() is tearing down shp */\n\t\tif (shm_file == NULL) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock0;\n\t\t}\n\n\t\tif (is_file_hugepages(shm_file))\n\t\t\tgoto out_unlock0;\n\n\t\tif (cmd == SHM_LOCK) {\n\t\t\tstruct user_struct *user = current_user();\n\t\t\terr = shmem_lock(shm_file, 1, user);\n\t\t\tif (!err && !(shp->shm_perm.mode & SHM_LOCKED)) {\n\t\t\t\tshp->shm_perm.mode |= SHM_LOCKED;\n\t\t\t\tshp->mlock_user = user;\n\t\t\t}\n\t\t\tgoto out_unlock0;\n\t\t}\n\n\t\t/* SHM_UNLOCK */\n\t\tif (!(shp->shm_perm.mode & SHM_LOCKED))\n\t\t\tgoto out_unlock0;\n\t\tshmem_lock(shm_file, 0, shp->mlock_user);\n\t\tshp->shm_perm.mode &= ~SHM_LOCKED;\n\t\tshp->mlock_user = NULL;\n\t\tget_file(shm_file);\n\t\tipc_unlock_object(&shp->shm_perm);\n\t\trcu_read_unlock();\n\t\tshmem_unlock_mapping(shm_file->f_mapping);\n\n\t\tfput(shm_file);\n\t\treturn err;\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\nout_unlock0:\n\tipc_unlock_object(&shp->shm_perm);\nout_unlock1:\n\trcu_read_unlock();\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -49,6 +49,13 @@\n \t\t}\n \n \t\tshm_file = shp->shm_file;\n+\n+\t\t/* check if shm_destroy() is tearing down shp */\n+\t\tif (shm_file == NULL) {\n+\t\t\terr = -EIDRM;\n+\t\t\tgoto out_unlock0;\n+\t\t}\n+\n \t\tif (is_file_hugepages(shm_file))\n \t\t\tgoto out_unlock0;\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\t\t/* check if shm_destroy() is tearing down shp */",
                "\t\tif (shm_file == NULL) {",
                "\t\t\terr = -EIDRM;",
                "\t\t\tgoto out_unlock0;",
                "\t\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in ipc/shm.c in the Linux kernel before 3.12.2 allow local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via a crafted application that uses shmctl IPC_RMID operations in conjunction with other shm system calls."
    },
    {
        "cve_id": "CVE-2013-7026",
        "code_before_change": "long do_shmat(int shmid, char __user *shmaddr, int shmflg, ulong *raddr,\n\t      unsigned long shmlba)\n{\n\tstruct shmid_kernel *shp;\n\tunsigned long addr;\n\tunsigned long size;\n\tstruct file * file;\n\tint    err;\n\tunsigned long flags;\n\tunsigned long prot;\n\tint acc_mode;\n\tstruct ipc_namespace *ns;\n\tstruct shm_file_data *sfd;\n\tstruct path path;\n\tfmode_t f_mode;\n\tunsigned long populate = 0;\n\n\terr = -EINVAL;\n\tif (shmid < 0)\n\t\tgoto out;\n\telse if ((addr = (ulong)shmaddr)) {\n\t\tif (addr & (shmlba - 1)) {\n\t\t\tif (shmflg & SHM_RND)\n\t\t\t\taddr &= ~(shmlba - 1);\t   /* round down */\n\t\t\telse\n#ifndef __ARCH_FORCE_SHMLBA\n\t\t\t\tif (addr & ~PAGE_MASK)\n#endif\n\t\t\t\t\tgoto out;\n\t\t}\n\t\tflags = MAP_SHARED | MAP_FIXED;\n\t} else {\n\t\tif ((shmflg & SHM_REMAP))\n\t\t\tgoto out;\n\n\t\tflags = MAP_SHARED;\n\t}\n\n\tif (shmflg & SHM_RDONLY) {\n\t\tprot = PROT_READ;\n\t\tacc_mode = S_IRUGO;\n\t\tf_mode = FMODE_READ;\n\t} else {\n\t\tprot = PROT_READ | PROT_WRITE;\n\t\tacc_mode = S_IRUGO | S_IWUGO;\n\t\tf_mode = FMODE_READ | FMODE_WRITE;\n\t}\n\tif (shmflg & SHM_EXEC) {\n\t\tprot |= PROT_EXEC;\n\t\tacc_mode |= S_IXUGO;\n\t}\n\n\t/*\n\t * We cannot rely on the fs check since SYSV IPC does have an\n\t * additional creator id...\n\t */\n\tns = current->nsproxy->ipc_ns;\n\trcu_read_lock();\n\tshp = shm_obtain_object_check(ns, shmid);\n\tif (IS_ERR(shp)) {\n\t\terr = PTR_ERR(shp);\n\t\tgoto out_unlock;\n\t}\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &shp->shm_perm, acc_mode))\n\t\tgoto out_unlock;\n\n\terr = security_shm_shmat(shp, shmaddr, shmflg);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tipc_lock_object(&shp->shm_perm);\n\tpath = shp->shm_file->f_path;\n\tpath_get(&path);\n\tshp->shm_nattch++;\n\tsize = i_size_read(path.dentry->d_inode);\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\n\terr = -ENOMEM;\n\tsfd = kzalloc(sizeof(*sfd), GFP_KERNEL);\n\tif (!sfd) {\n\t\tpath_put(&path);\n\t\tgoto out_nattch;\n\t}\n\n\tfile = alloc_file(&path, f_mode,\n\t\t\t  is_file_hugepages(shp->shm_file) ?\n\t\t\t\t&shm_file_operations_huge :\n\t\t\t\t&shm_file_operations);\n\terr = PTR_ERR(file);\n\tif (IS_ERR(file)) {\n\t\tkfree(sfd);\n\t\tpath_put(&path);\n\t\tgoto out_nattch;\n\t}\n\n\tfile->private_data = sfd;\n\tfile->f_mapping = shp->shm_file->f_mapping;\n\tsfd->id = shp->shm_perm.id;\n\tsfd->ns = get_ipc_ns(ns);\n\tsfd->file = shp->shm_file;\n\tsfd->vm_ops = NULL;\n\n\terr = security_mmap_file(file, prot, flags);\n\tif (err)\n\t\tgoto out_fput;\n\n\tdown_write(&current->mm->mmap_sem);\n\tif (addr && !(shmflg & SHM_REMAP)) {\n\t\terr = -EINVAL;\n\t\tif (find_vma_intersection(current->mm, addr, addr + size))\n\t\t\tgoto invalid;\n\t\t/*\n\t\t * If shm segment goes below stack, make sure there is some\n\t\t * space left for the stack to grow (at least 4 pages).\n\t\t */\n\t\tif (addr < current->mm->start_stack &&\n\t\t    addr > current->mm->start_stack - size - PAGE_SIZE * 5)\n\t\t\tgoto invalid;\n\t}\n\n\taddr = do_mmap_pgoff(file, addr, size, prot, flags, 0, &populate);\n\t*raddr = addr;\n\terr = 0;\n\tif (IS_ERR_VALUE(addr))\n\t\terr = (long)addr;\ninvalid:\n\tup_write(&current->mm->mmap_sem);\n\tif (populate)\n\t\tmm_populate(addr, populate);\n\nout_fput:\n\tfput(file);\n\nout_nattch:\n\tdown_write(&shm_ids(ns).rwsem);\n\tshp = shm_lock(ns, shmid);\n\tBUG_ON(IS_ERR(shp));\n\tshp->shm_nattch--;\n\tif (shm_may_destroy(ns, shp))\n\t\tshm_destroy(ns, shp);\n\telse\n\t\tshm_unlock(shp);\n\tup_write(&shm_ids(ns).rwsem);\n\treturn err;\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\treturn err;\n}",
        "code_after_change": "long do_shmat(int shmid, char __user *shmaddr, int shmflg, ulong *raddr,\n\t      unsigned long shmlba)\n{\n\tstruct shmid_kernel *shp;\n\tunsigned long addr;\n\tunsigned long size;\n\tstruct file * file;\n\tint    err;\n\tunsigned long flags;\n\tunsigned long prot;\n\tint acc_mode;\n\tstruct ipc_namespace *ns;\n\tstruct shm_file_data *sfd;\n\tstruct path path;\n\tfmode_t f_mode;\n\tunsigned long populate = 0;\n\n\terr = -EINVAL;\n\tif (shmid < 0)\n\t\tgoto out;\n\telse if ((addr = (ulong)shmaddr)) {\n\t\tif (addr & (shmlba - 1)) {\n\t\t\tif (shmflg & SHM_RND)\n\t\t\t\taddr &= ~(shmlba - 1);\t   /* round down */\n\t\t\telse\n#ifndef __ARCH_FORCE_SHMLBA\n\t\t\t\tif (addr & ~PAGE_MASK)\n#endif\n\t\t\t\t\tgoto out;\n\t\t}\n\t\tflags = MAP_SHARED | MAP_FIXED;\n\t} else {\n\t\tif ((shmflg & SHM_REMAP))\n\t\t\tgoto out;\n\n\t\tflags = MAP_SHARED;\n\t}\n\n\tif (shmflg & SHM_RDONLY) {\n\t\tprot = PROT_READ;\n\t\tacc_mode = S_IRUGO;\n\t\tf_mode = FMODE_READ;\n\t} else {\n\t\tprot = PROT_READ | PROT_WRITE;\n\t\tacc_mode = S_IRUGO | S_IWUGO;\n\t\tf_mode = FMODE_READ | FMODE_WRITE;\n\t}\n\tif (shmflg & SHM_EXEC) {\n\t\tprot |= PROT_EXEC;\n\t\tacc_mode |= S_IXUGO;\n\t}\n\n\t/*\n\t * We cannot rely on the fs check since SYSV IPC does have an\n\t * additional creator id...\n\t */\n\tns = current->nsproxy->ipc_ns;\n\trcu_read_lock();\n\tshp = shm_obtain_object_check(ns, shmid);\n\tif (IS_ERR(shp)) {\n\t\terr = PTR_ERR(shp);\n\t\tgoto out_unlock;\n\t}\n\n\terr = -EACCES;\n\tif (ipcperms(ns, &shp->shm_perm, acc_mode))\n\t\tgoto out_unlock;\n\n\terr = security_shm_shmat(shp, shmaddr, shmflg);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tipc_lock_object(&shp->shm_perm);\n\n\t/* check if shm_destroy() is tearing down shp */\n\tif (shp->shm_file == NULL) {\n\t\tipc_unlock_object(&shp->shm_perm);\n\t\terr = -EIDRM;\n\t\tgoto out_unlock;\n\t}\n\n\tpath = shp->shm_file->f_path;\n\tpath_get(&path);\n\tshp->shm_nattch++;\n\tsize = i_size_read(path.dentry->d_inode);\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\n\terr = -ENOMEM;\n\tsfd = kzalloc(sizeof(*sfd), GFP_KERNEL);\n\tif (!sfd) {\n\t\tpath_put(&path);\n\t\tgoto out_nattch;\n\t}\n\n\tfile = alloc_file(&path, f_mode,\n\t\t\t  is_file_hugepages(shp->shm_file) ?\n\t\t\t\t&shm_file_operations_huge :\n\t\t\t\t&shm_file_operations);\n\terr = PTR_ERR(file);\n\tif (IS_ERR(file)) {\n\t\tkfree(sfd);\n\t\tpath_put(&path);\n\t\tgoto out_nattch;\n\t}\n\n\tfile->private_data = sfd;\n\tfile->f_mapping = shp->shm_file->f_mapping;\n\tsfd->id = shp->shm_perm.id;\n\tsfd->ns = get_ipc_ns(ns);\n\tsfd->file = shp->shm_file;\n\tsfd->vm_ops = NULL;\n\n\terr = security_mmap_file(file, prot, flags);\n\tif (err)\n\t\tgoto out_fput;\n\n\tdown_write(&current->mm->mmap_sem);\n\tif (addr && !(shmflg & SHM_REMAP)) {\n\t\terr = -EINVAL;\n\t\tif (find_vma_intersection(current->mm, addr, addr + size))\n\t\t\tgoto invalid;\n\t\t/*\n\t\t * If shm segment goes below stack, make sure there is some\n\t\t * space left for the stack to grow (at least 4 pages).\n\t\t */\n\t\tif (addr < current->mm->start_stack &&\n\t\t    addr > current->mm->start_stack - size - PAGE_SIZE * 5)\n\t\t\tgoto invalid;\n\t}\n\n\taddr = do_mmap_pgoff(file, addr, size, prot, flags, 0, &populate);\n\t*raddr = addr;\n\terr = 0;\n\tif (IS_ERR_VALUE(addr))\n\t\terr = (long)addr;\ninvalid:\n\tup_write(&current->mm->mmap_sem);\n\tif (populate)\n\t\tmm_populate(addr, populate);\n\nout_fput:\n\tfput(file);\n\nout_nattch:\n\tdown_write(&shm_ids(ns).rwsem);\n\tshp = shm_lock(ns, shmid);\n\tBUG_ON(IS_ERR(shp));\n\tshp->shm_nattch--;\n\tif (shm_may_destroy(ns, shp))\n\t\tshm_destroy(ns, shp);\n\telse\n\t\tshm_unlock(shp);\n\tup_write(&shm_ids(ns).rwsem);\n\treturn err;\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -71,6 +71,14 @@\n \t\tgoto out_unlock;\n \n \tipc_lock_object(&shp->shm_perm);\n+\n+\t/* check if shm_destroy() is tearing down shp */\n+\tif (shp->shm_file == NULL) {\n+\t\tipc_unlock_object(&shp->shm_perm);\n+\t\terr = -EIDRM;\n+\t\tgoto out_unlock;\n+\t}\n+\n \tpath = shp->shm_file->f_path;\n \tpath_get(&path);\n \tshp->shm_nattch++;",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* check if shm_destroy() is tearing down shp */",
                "\tif (shp->shm_file == NULL) {",
                "\t\tipc_unlock_object(&shp->shm_perm);",
                "\t\terr = -EIDRM;",
                "\t\tgoto out_unlock;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in ipc/shm.c in the Linux kernel before 3.12.2 allow local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via a crafted application that uses shmctl IPC_RMID operations in conjunction with other shm system calls."
    },
    {
        "cve_id": "CVE-2014-0100",
        "code_before_change": "static struct inet_frag_queue *inet_frag_intern(struct netns_frags *nf,\n\t\tstruct inet_frag_queue *qp_in, struct inet_frags *f,\n\t\tvoid *arg)\n{\n\tstruct inet_frag_bucket *hb;\n\tstruct inet_frag_queue *qp;\n\tunsigned int hash;\n\n\tread_lock(&f->lock); /* Protects against hash rebuild */\n\t/*\n\t * While we stayed w/o the lock other CPU could update\n\t * the rnd seed, so we need to re-calculate the hash\n\t * chain. Fortunatelly the qp_in can be used to get one.\n\t */\n\thash = f->hashfn(qp_in);\n\thb = &f->hash[hash];\n\tspin_lock(&hb->chain_lock);\n\n#ifdef CONFIG_SMP\n\t/* With SMP race we have to recheck hash table, because\n\t * such entry could be created on other cpu, while we\n\t * released the hash bucket lock.\n\t */\n\thlist_for_each_entry(qp, &hb->chain, list) {\n\t\tif (qp->net == nf && f->match(qp, arg)) {\n\t\t\tatomic_inc(&qp->refcnt);\n\t\t\tspin_unlock(&hb->chain_lock);\n\t\t\tread_unlock(&f->lock);\n\t\t\tqp_in->last_in |= INET_FRAG_COMPLETE;\n\t\t\tinet_frag_put(qp_in, f);\n\t\t\treturn qp;\n\t\t}\n\t}\n#endif\n\tqp = qp_in;\n\tif (!mod_timer(&qp->timer, jiffies + nf->timeout))\n\t\tatomic_inc(&qp->refcnt);\n\n\tatomic_inc(&qp->refcnt);\n\thlist_add_head(&qp->list, &hb->chain);\n\tspin_unlock(&hb->chain_lock);\n\tread_unlock(&f->lock);\n\tinet_frag_lru_add(nf, qp);\n\treturn qp;\n}",
        "code_after_change": "static struct inet_frag_queue *inet_frag_intern(struct netns_frags *nf,\n\t\tstruct inet_frag_queue *qp_in, struct inet_frags *f,\n\t\tvoid *arg)\n{\n\tstruct inet_frag_bucket *hb;\n\tstruct inet_frag_queue *qp;\n\tunsigned int hash;\n\n\tread_lock(&f->lock); /* Protects against hash rebuild */\n\t/*\n\t * While we stayed w/o the lock other CPU could update\n\t * the rnd seed, so we need to re-calculate the hash\n\t * chain. Fortunatelly the qp_in can be used to get one.\n\t */\n\thash = f->hashfn(qp_in);\n\thb = &f->hash[hash];\n\tspin_lock(&hb->chain_lock);\n\n#ifdef CONFIG_SMP\n\t/* With SMP race we have to recheck hash table, because\n\t * such entry could be created on other cpu, while we\n\t * released the hash bucket lock.\n\t */\n\thlist_for_each_entry(qp, &hb->chain, list) {\n\t\tif (qp->net == nf && f->match(qp, arg)) {\n\t\t\tatomic_inc(&qp->refcnt);\n\t\t\tspin_unlock(&hb->chain_lock);\n\t\t\tread_unlock(&f->lock);\n\t\t\tqp_in->last_in |= INET_FRAG_COMPLETE;\n\t\t\tinet_frag_put(qp_in, f);\n\t\t\treturn qp;\n\t\t}\n\t}\n#endif\n\tqp = qp_in;\n\tif (!mod_timer(&qp->timer, jiffies + nf->timeout))\n\t\tatomic_inc(&qp->refcnt);\n\n\tatomic_inc(&qp->refcnt);\n\thlist_add_head(&qp->list, &hb->chain);\n\tinet_frag_lru_add(nf, qp);\n\tspin_unlock(&hb->chain_lock);\n\tread_unlock(&f->lock);\n\n\treturn qp;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,8 +38,9 @@\n \n \tatomic_inc(&qp->refcnt);\n \thlist_add_head(&qp->list, &hb->chain);\n+\tinet_frag_lru_add(nf, qp);\n \tspin_unlock(&hb->chain_lock);\n \tread_unlock(&f->lock);\n-\tinet_frag_lru_add(nf, qp);\n+\n \treturn qp;\n }",
        "function_modified_lines": {
            "added": [
                "\tinet_frag_lru_add(nf, qp);",
                ""
            ],
            "deleted": [
                "\tinet_frag_lru_add(nf, qp);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the inet_frag_intern function in net/ipv4/inet_fragment.c in the Linux kernel through 3.13.6 allows remote attackers to cause a denial of service (use-after-free error) or possibly have unspecified other impact via a large series of fragmented ICMP Echo Request packets to a system with a heavy CPU load."
    },
    {
        "cve_id": "CVE-2014-0196",
        "code_before_change": "static ssize_t n_tty_write(struct tty_struct *tty, struct file *file,\n\t\t\t   const unsigned char *buf, size_t nr)\n{\n\tconst unsigned char *b = buf;\n\tDECLARE_WAITQUEUE(wait, current);\n\tint c;\n\tssize_t retval = 0;\n\n\t/* Job control check -- must be done at start (POSIX.1 7.1.1.4). */\n\tif (L_TOSTOP(tty) && file->f_op->write != redirected_tty_write) {\n\t\tretval = tty_check_change(tty);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\tdown_read(&tty->termios_rwsem);\n\n\t/* Write out any echoed characters that are still pending */\n\tprocess_echoes(tty);\n\n\tadd_wait_queue(&tty->write_wait, &wait);\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current)) {\n\t\t\tretval = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (tty_hung_up_p(file) || (tty->link && !tty->link->count)) {\n\t\t\tretval = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (O_OPOST(tty)) {\n\t\t\twhile (nr > 0) {\n\t\t\t\tssize_t num = process_output_block(tty, b, nr);\n\t\t\t\tif (num < 0) {\n\t\t\t\t\tif (num == -EAGAIN)\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tretval = num;\n\t\t\t\t\tgoto break_out;\n\t\t\t\t}\n\t\t\t\tb += num;\n\t\t\t\tnr -= num;\n\t\t\t\tif (nr == 0)\n\t\t\t\t\tbreak;\n\t\t\t\tc = *b;\n\t\t\t\tif (process_output(c, tty) < 0)\n\t\t\t\t\tbreak;\n\t\t\t\tb++; nr--;\n\t\t\t}\n\t\t\tif (tty->ops->flush_chars)\n\t\t\t\ttty->ops->flush_chars(tty);\n\t\t} else {\n\t\t\twhile (nr > 0) {\n\t\t\t\tc = tty->ops->write(tty, b, nr);\n\t\t\t\tif (c < 0) {\n\t\t\t\t\tretval = c;\n\t\t\t\t\tgoto break_out;\n\t\t\t\t}\n\t\t\t\tif (!c)\n\t\t\t\t\tbreak;\n\t\t\t\tb += c;\n\t\t\t\tnr -= c;\n\t\t\t}\n\t\t}\n\t\tif (!nr)\n\t\t\tbreak;\n\t\tif (file->f_flags & O_NONBLOCK) {\n\t\t\tretval = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tup_read(&tty->termios_rwsem);\n\n\t\tschedule();\n\n\t\tdown_read(&tty->termios_rwsem);\n\t}\nbreak_out:\n\t__set_current_state(TASK_RUNNING);\n\tremove_wait_queue(&tty->write_wait, &wait);\n\tif (b - buf != nr && tty->fasync)\n\t\tset_bit(TTY_DO_WRITE_WAKEUP, &tty->flags);\n\tup_read(&tty->termios_rwsem);\n\treturn (b - buf) ? b - buf : retval;\n}",
        "code_after_change": "static ssize_t n_tty_write(struct tty_struct *tty, struct file *file,\n\t\t\t   const unsigned char *buf, size_t nr)\n{\n\tconst unsigned char *b = buf;\n\tDECLARE_WAITQUEUE(wait, current);\n\tint c;\n\tssize_t retval = 0;\n\n\t/* Job control check -- must be done at start (POSIX.1 7.1.1.4). */\n\tif (L_TOSTOP(tty) && file->f_op->write != redirected_tty_write) {\n\t\tretval = tty_check_change(tty);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\tdown_read(&tty->termios_rwsem);\n\n\t/* Write out any echoed characters that are still pending */\n\tprocess_echoes(tty);\n\n\tadd_wait_queue(&tty->write_wait, &wait);\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current)) {\n\t\t\tretval = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (tty_hung_up_p(file) || (tty->link && !tty->link->count)) {\n\t\t\tretval = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (O_OPOST(tty)) {\n\t\t\twhile (nr > 0) {\n\t\t\t\tssize_t num = process_output_block(tty, b, nr);\n\t\t\t\tif (num < 0) {\n\t\t\t\t\tif (num == -EAGAIN)\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tretval = num;\n\t\t\t\t\tgoto break_out;\n\t\t\t\t}\n\t\t\t\tb += num;\n\t\t\t\tnr -= num;\n\t\t\t\tif (nr == 0)\n\t\t\t\t\tbreak;\n\t\t\t\tc = *b;\n\t\t\t\tif (process_output(c, tty) < 0)\n\t\t\t\t\tbreak;\n\t\t\t\tb++; nr--;\n\t\t\t}\n\t\t\tif (tty->ops->flush_chars)\n\t\t\t\ttty->ops->flush_chars(tty);\n\t\t} else {\n\t\t\tstruct n_tty_data *ldata = tty->disc_data;\n\n\t\t\twhile (nr > 0) {\n\t\t\t\tmutex_lock(&ldata->output_lock);\n\t\t\t\tc = tty->ops->write(tty, b, nr);\n\t\t\t\tmutex_unlock(&ldata->output_lock);\n\t\t\t\tif (c < 0) {\n\t\t\t\t\tretval = c;\n\t\t\t\t\tgoto break_out;\n\t\t\t\t}\n\t\t\t\tif (!c)\n\t\t\t\t\tbreak;\n\t\t\t\tb += c;\n\t\t\t\tnr -= c;\n\t\t\t}\n\t\t}\n\t\tif (!nr)\n\t\t\tbreak;\n\t\tif (file->f_flags & O_NONBLOCK) {\n\t\t\tretval = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tup_read(&tty->termios_rwsem);\n\n\t\tschedule();\n\n\t\tdown_read(&tty->termios_rwsem);\n\t}\nbreak_out:\n\t__set_current_state(TASK_RUNNING);\n\tremove_wait_queue(&tty->write_wait, &wait);\n\tif (b - buf != nr && tty->fasync)\n\t\tset_bit(TTY_DO_WRITE_WAKEUP, &tty->flags);\n\tup_read(&tty->termios_rwsem);\n\treturn (b - buf) ? b - buf : retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -50,8 +50,12 @@\n \t\t\tif (tty->ops->flush_chars)\n \t\t\t\ttty->ops->flush_chars(tty);\n \t\t} else {\n+\t\t\tstruct n_tty_data *ldata = tty->disc_data;\n+\n \t\t\twhile (nr > 0) {\n+\t\t\t\tmutex_lock(&ldata->output_lock);\n \t\t\t\tc = tty->ops->write(tty, b, nr);\n+\t\t\t\tmutex_unlock(&ldata->output_lock);\n \t\t\t\tif (c < 0) {\n \t\t\t\t\tretval = c;\n \t\t\t\t\tgoto break_out;",
        "function_modified_lines": {
            "added": [
                "\t\t\tstruct n_tty_data *ldata = tty->disc_data;",
                "",
                "\t\t\t\tmutex_lock(&ldata->output_lock);",
                "\t\t\t\tmutex_unlock(&ldata->output_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The n_tty_write function in drivers/tty/n_tty.c in the Linux kernel through 3.14.3 does not properly manage tty driver access in the \"LECHO & !OPOST\" case, which allows local users to cause a denial of service (memory corruption and system crash) or gain privileges by triggering a race condition involving read and write operations with long strings."
    },
    {
        "cve_id": "CVE-2014-2672",
        "code_before_change": "void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,\n\t\t       struct ath_node *an)\n{\n\tstruct ath_atx_tid *tid;\n\tstruct ath_atx_ac *ac;\n\tstruct ath_txq *txq;\n\tbool buffered;\n\tint tidno;\n\n\tfor (tidno = 0, tid = &an->tid[tidno];\n\t     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {\n\n\t\tif (!tid->sched)\n\t\t\tcontinue;\n\n\t\tac = tid->ac;\n\t\ttxq = ac->txq;\n\n\t\tath_txq_lock(sc, txq);\n\n\t\tbuffered = ath_tid_has_buffered(tid);\n\n\t\ttid->sched = false;\n\t\tlist_del(&tid->list);\n\n\t\tif (ac->sched) {\n\t\t\tac->sched = false;\n\t\t\tlist_del(&ac->list);\n\t\t}\n\n\t\tath_txq_unlock(sc, txq);\n\n\t\tieee80211_sta_set_buffered(sta, tidno, buffered);\n\t}\n}",
        "code_after_change": "void ath_tx_aggr_sleep(struct ieee80211_sta *sta, struct ath_softc *sc,\n\t\t       struct ath_node *an)\n{\n\tstruct ath_atx_tid *tid;\n\tstruct ath_atx_ac *ac;\n\tstruct ath_txq *txq;\n\tbool buffered;\n\tint tidno;\n\n\tfor (tidno = 0, tid = &an->tid[tidno];\n\t     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {\n\n\t\tac = tid->ac;\n\t\ttxq = ac->txq;\n\n\t\tath_txq_lock(sc, txq);\n\n\t\tif (!tid->sched) {\n\t\t\tath_txq_unlock(sc, txq);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbuffered = ath_tid_has_buffered(tid);\n\n\t\ttid->sched = false;\n\t\tlist_del(&tid->list);\n\n\t\tif (ac->sched) {\n\t\t\tac->sched = false;\n\t\t\tlist_del(&ac->list);\n\t\t}\n\n\t\tath_txq_unlock(sc, txq);\n\n\t\tieee80211_sta_set_buffered(sta, tidno, buffered);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,13 +10,15 @@\n \tfor (tidno = 0, tid = &an->tid[tidno];\n \t     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {\n \n-\t\tif (!tid->sched)\n-\t\t\tcontinue;\n-\n \t\tac = tid->ac;\n \t\ttxq = ac->txq;\n \n \t\tath_txq_lock(sc, txq);\n+\n+\t\tif (!tid->sched) {\n+\t\t\tath_txq_unlock(sc, txq);\n+\t\t\tcontinue;\n+\t\t}\n \n \t\tbuffered = ath_tid_has_buffered(tid);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\t\tif (!tid->sched) {",
                "\t\t\tath_txq_unlock(sc, txq);",
                "\t\t\tcontinue;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tif (!tid->sched)",
                "\t\t\tcontinue;",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ath_tx_aggr_sleep function in drivers/net/wireless/ath/ath9k/xmit.c in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via a large amount of network traffic that triggers certain list deletions."
    },
    {
        "cve_id": "CVE-2014-2706",
        "code_before_change": "struct sta_info *sta_info_alloc(struct ieee80211_sub_if_data *sdata,\n\t\t\t\tconst u8 *addr, gfp_t gfp)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta;\n\tstruct timespec uptime;\n\tstruct ieee80211_tx_latency_bin_ranges *tx_latency;\n\tint i;\n\n\tsta = kzalloc(sizeof(*sta) + local->hw.sta_data_size, gfp);\n\tif (!sta)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttx_latency = rcu_dereference(local->tx_latency);\n\t/* init stations Tx latency statistics && TID bins */\n\tif (tx_latency) {\n\t\tsta->tx_lat = kzalloc(IEEE80211_NUM_TIDS *\n\t\t\t\t      sizeof(struct ieee80211_tx_latency_stat),\n\t\t\t\t      GFP_ATOMIC);\n\t\tif (!sta->tx_lat) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto free;\n\t\t}\n\n\t\tif (tx_latency->n_ranges) {\n\t\t\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++) {\n\t\t\t\t/* size of bins is size of the ranges +1 */\n\t\t\t\tsta->tx_lat[i].bin_count =\n\t\t\t\t\ttx_latency->n_ranges + 1;\n\t\t\t\tsta->tx_lat[i].bins =\n\t\t\t\t\tkcalloc(sta->tx_lat[i].bin_count,\n\t\t\t\t\t\tsizeof(u32), GFP_ATOMIC);\n\t\t\t\tif (!sta->tx_lat[i].bins) {\n\t\t\t\t\trcu_read_unlock();\n\t\t\t\t\tgoto free;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tspin_lock_init(&sta->lock);\n\tINIT_WORK(&sta->drv_unblock_wk, sta_unblock);\n\tINIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);\n\tmutex_init(&sta->ampdu_mlme.mtx);\n#ifdef CONFIG_MAC80211_MESH\n\tif (ieee80211_vif_is_mesh(&sdata->vif) &&\n\t    !sdata->u.mesh.user_mpm)\n\t\tinit_timer(&sta->plink_timer);\n\tsta->nonpeer_pm = NL80211_MESH_POWER_ACTIVE;\n#endif\n\n\tmemcpy(sta->sta.addr, addr, ETH_ALEN);\n\tsta->local = local;\n\tsta->sdata = sdata;\n\tsta->last_rx = jiffies;\n\n\tsta->sta_state = IEEE80211_STA_NONE;\n\n\tdo_posix_clock_monotonic_gettime(&uptime);\n\tsta->last_connected = uptime.tv_sec;\n\tewma_init(&sta->avg_signal, 1024, 8);\n\tfor (i = 0; i < ARRAY_SIZE(sta->chain_signal_avg); i++)\n\t\tewma_init(&sta->chain_signal_avg[i], 1024, 8);\n\n\tif (sta_prepare_rate_control(local, sta, gfp))\n\t\tgoto free;\n\n\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++) {\n\t\t/*\n\t\t * timer_to_tid must be initialized with identity mapping\n\t\t * to enable session_timer's data differentiation. See\n\t\t * sta_rx_agg_session_timer_expired for usage.\n\t\t */\n\t\tsta->timer_to_tid[i] = i;\n\t}\n\tfor (i = 0; i < IEEE80211_NUM_ACS; i++) {\n\t\tskb_queue_head_init(&sta->ps_tx_buf[i]);\n\t\tskb_queue_head_init(&sta->tx_filtered[i]);\n\t}\n\n\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++)\n\t\tsta->last_seq_ctrl[i] = cpu_to_le16(USHRT_MAX);\n\n\tsta->sta.smps_mode = IEEE80211_SMPS_OFF;\n\tif (sdata->vif.type == NL80211_IFTYPE_AP ||\n\t    sdata->vif.type == NL80211_IFTYPE_AP_VLAN) {\n\t\tstruct ieee80211_supported_band *sband =\n\t\t\tlocal->hw.wiphy->bands[ieee80211_get_sdata_band(sdata)];\n\t\tu8 smps = (sband->ht_cap.cap & IEEE80211_HT_CAP_SM_PS) >>\n\t\t\t\tIEEE80211_HT_CAP_SM_PS_SHIFT;\n\t\t/*\n\t\t * Assume that hostapd advertises our caps in the beacon and\n\t\t * this is the known_smps_mode for a station that just assciated\n\t\t */\n\t\tswitch (smps) {\n\t\tcase WLAN_HT_SMPS_CONTROL_DISABLED:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_OFF;\n\t\t\tbreak;\n\t\tcase WLAN_HT_SMPS_CONTROL_STATIC:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_STATIC;\n\t\t\tbreak;\n\t\tcase WLAN_HT_SMPS_CONTROL_DYNAMIC:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_DYNAMIC;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON(1);\n\t\t}\n\t}\n\n\tsta_dbg(sdata, \"Allocated STA %pM\\n\", sta->sta.addr);\n\treturn sta;\n\nfree:\n\tif (sta->tx_lat) {\n\t\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++)\n\t\t\tkfree(sta->tx_lat[i].bins);\n\t\tkfree(sta->tx_lat);\n\t}\n\tkfree(sta);\n\treturn NULL;\n}",
        "code_after_change": "struct sta_info *sta_info_alloc(struct ieee80211_sub_if_data *sdata,\n\t\t\t\tconst u8 *addr, gfp_t gfp)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta;\n\tstruct timespec uptime;\n\tstruct ieee80211_tx_latency_bin_ranges *tx_latency;\n\tint i;\n\n\tsta = kzalloc(sizeof(*sta) + local->hw.sta_data_size, gfp);\n\tif (!sta)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttx_latency = rcu_dereference(local->tx_latency);\n\t/* init stations Tx latency statistics && TID bins */\n\tif (tx_latency) {\n\t\tsta->tx_lat = kzalloc(IEEE80211_NUM_TIDS *\n\t\t\t\t      sizeof(struct ieee80211_tx_latency_stat),\n\t\t\t\t      GFP_ATOMIC);\n\t\tif (!sta->tx_lat) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto free;\n\t\t}\n\n\t\tif (tx_latency->n_ranges) {\n\t\t\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++) {\n\t\t\t\t/* size of bins is size of the ranges +1 */\n\t\t\t\tsta->tx_lat[i].bin_count =\n\t\t\t\t\ttx_latency->n_ranges + 1;\n\t\t\t\tsta->tx_lat[i].bins =\n\t\t\t\t\tkcalloc(sta->tx_lat[i].bin_count,\n\t\t\t\t\t\tsizeof(u32), GFP_ATOMIC);\n\t\t\t\tif (!sta->tx_lat[i].bins) {\n\t\t\t\t\trcu_read_unlock();\n\t\t\t\t\tgoto free;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tspin_lock_init(&sta->lock);\n\tspin_lock_init(&sta->ps_lock);\n\tINIT_WORK(&sta->drv_unblock_wk, sta_unblock);\n\tINIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);\n\tmutex_init(&sta->ampdu_mlme.mtx);\n#ifdef CONFIG_MAC80211_MESH\n\tif (ieee80211_vif_is_mesh(&sdata->vif) &&\n\t    !sdata->u.mesh.user_mpm)\n\t\tinit_timer(&sta->plink_timer);\n\tsta->nonpeer_pm = NL80211_MESH_POWER_ACTIVE;\n#endif\n\n\tmemcpy(sta->sta.addr, addr, ETH_ALEN);\n\tsta->local = local;\n\tsta->sdata = sdata;\n\tsta->last_rx = jiffies;\n\n\tsta->sta_state = IEEE80211_STA_NONE;\n\n\tdo_posix_clock_monotonic_gettime(&uptime);\n\tsta->last_connected = uptime.tv_sec;\n\tewma_init(&sta->avg_signal, 1024, 8);\n\tfor (i = 0; i < ARRAY_SIZE(sta->chain_signal_avg); i++)\n\t\tewma_init(&sta->chain_signal_avg[i], 1024, 8);\n\n\tif (sta_prepare_rate_control(local, sta, gfp))\n\t\tgoto free;\n\n\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++) {\n\t\t/*\n\t\t * timer_to_tid must be initialized with identity mapping\n\t\t * to enable session_timer's data differentiation. See\n\t\t * sta_rx_agg_session_timer_expired for usage.\n\t\t */\n\t\tsta->timer_to_tid[i] = i;\n\t}\n\tfor (i = 0; i < IEEE80211_NUM_ACS; i++) {\n\t\tskb_queue_head_init(&sta->ps_tx_buf[i]);\n\t\tskb_queue_head_init(&sta->tx_filtered[i]);\n\t}\n\n\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++)\n\t\tsta->last_seq_ctrl[i] = cpu_to_le16(USHRT_MAX);\n\n\tsta->sta.smps_mode = IEEE80211_SMPS_OFF;\n\tif (sdata->vif.type == NL80211_IFTYPE_AP ||\n\t    sdata->vif.type == NL80211_IFTYPE_AP_VLAN) {\n\t\tstruct ieee80211_supported_band *sband =\n\t\t\tlocal->hw.wiphy->bands[ieee80211_get_sdata_band(sdata)];\n\t\tu8 smps = (sband->ht_cap.cap & IEEE80211_HT_CAP_SM_PS) >>\n\t\t\t\tIEEE80211_HT_CAP_SM_PS_SHIFT;\n\t\t/*\n\t\t * Assume that hostapd advertises our caps in the beacon and\n\t\t * this is the known_smps_mode for a station that just assciated\n\t\t */\n\t\tswitch (smps) {\n\t\tcase WLAN_HT_SMPS_CONTROL_DISABLED:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_OFF;\n\t\t\tbreak;\n\t\tcase WLAN_HT_SMPS_CONTROL_STATIC:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_STATIC;\n\t\t\tbreak;\n\t\tcase WLAN_HT_SMPS_CONTROL_DYNAMIC:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_DYNAMIC;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON(1);\n\t\t}\n\t}\n\n\tsta_dbg(sdata, \"Allocated STA %pM\\n\", sta->sta.addr);\n\treturn sta;\n\nfree:\n\tif (sta->tx_lat) {\n\t\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++)\n\t\t\tkfree(sta->tx_lat[i].bins);\n\t\tkfree(sta->tx_lat);\n\t}\n\tkfree(sta);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -41,6 +41,7 @@\n \trcu_read_unlock();\n \n \tspin_lock_init(&sta->lock);\n+\tspin_lock_init(&sta->ps_lock);\n \tINIT_WORK(&sta->drv_unblock_wk, sta_unblock);\n \tINIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);\n \tmutex_init(&sta->ampdu_mlme.mtx);",
        "function_modified_lines": {
            "added": [
                "\tspin_lock_init(&sta->ps_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the mac80211 subsystem in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via network traffic that improperly interacts with the WLAN_STA_PS_STA state (aka power-save mode), related to sta_info.c and tx.c."
    },
    {
        "cve_id": "CVE-2014-2706",
        "code_before_change": "void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sk_buff_head pending;\n\tint filtered = 0, buffered = 0, ac;\n\tunsigned long flags;\n\n\tclear_sta_flag(sta, WLAN_STA_SP);\n\n\tBUILD_BUG_ON(BITS_TO_LONGS(IEEE80211_NUM_TIDS) > 1);\n\tsta->driver_buffered_tids = 0;\n\n\tif (!(local->hw.flags & IEEE80211_HW_AP_LINK_PS))\n\t\tdrv_sta_notify(local, sdata, STA_NOTIFY_AWAKE, &sta->sta);\n\n\tskb_queue_head_init(&pending);\n\n\t/* Send all buffered frames to the station */\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n\t\tint count = skb_queue_len(&pending), tmp;\n\n\t\tspin_lock_irqsave(&sta->tx_filtered[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->tx_filtered[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->tx_filtered[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tfiltered += tmp - count;\n\t\tcount = tmp;\n\n\t\tspin_lock_irqsave(&sta->ps_tx_buf[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->ps_tx_buf[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->ps_tx_buf[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tbuffered += tmp - count;\n\t}\n\n\tieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);\n\n\t/* This station just woke up and isn't aware of our SMPS state */\n\tif (!ieee80211_smps_is_restrictive(sta->known_smps_mode,\n\t\t\t\t\t   sdata->smps_mode) &&\n\t    sta->known_smps_mode != sdata->bss->req_smps &&\n\t    sta_info_tx_streams(sta) != 1) {\n\t\tht_dbg(sdata,\n\t\t       \"%pM just woke up and MIMO capable - update SMPS\\n\",\n\t\t       sta->sta.addr);\n\t\tieee80211_send_smps_action(sdata, sdata->bss->req_smps,\n\t\t\t\t\t   sta->sta.addr,\n\t\t\t\t\t   sdata->vif.bss_conf.bssid);\n\t}\n\n\tlocal->total_ps_buffered -= buffered;\n\n\tsta_info_recalc_tim(sta);\n\n\tps_dbg(sdata,\n\t       \"STA %pM aid %d sending %d filtered/%d PS frames since STA not sleeping anymore\\n\",\n\t       sta->sta.addr, sta->sta.aid, filtered, buffered);\n}",
        "code_after_change": "void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sk_buff_head pending;\n\tint filtered = 0, buffered = 0, ac;\n\tunsigned long flags;\n\n\tclear_sta_flag(sta, WLAN_STA_SP);\n\n\tBUILD_BUG_ON(BITS_TO_LONGS(IEEE80211_NUM_TIDS) > 1);\n\tsta->driver_buffered_tids = 0;\n\n\tif (!(local->hw.flags & IEEE80211_HW_AP_LINK_PS))\n\t\tdrv_sta_notify(local, sdata, STA_NOTIFY_AWAKE, &sta->sta);\n\n\tskb_queue_head_init(&pending);\n\n\t/* sync with ieee80211_tx_h_unicast_ps_buf */\n\tspin_lock(&sta->ps_lock);\n\t/* Send all buffered frames to the station */\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n\t\tint count = skb_queue_len(&pending), tmp;\n\n\t\tspin_lock_irqsave(&sta->tx_filtered[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->tx_filtered[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->tx_filtered[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tfiltered += tmp - count;\n\t\tcount = tmp;\n\n\t\tspin_lock_irqsave(&sta->ps_tx_buf[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->ps_tx_buf[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->ps_tx_buf[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tbuffered += tmp - count;\n\t}\n\n\tieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);\n\tspin_unlock(&sta->ps_lock);\n\n\t/* This station just woke up and isn't aware of our SMPS state */\n\tif (!ieee80211_smps_is_restrictive(sta->known_smps_mode,\n\t\t\t\t\t   sdata->smps_mode) &&\n\t    sta->known_smps_mode != sdata->bss->req_smps &&\n\t    sta_info_tx_streams(sta) != 1) {\n\t\tht_dbg(sdata,\n\t\t       \"%pM just woke up and MIMO capable - update SMPS\\n\",\n\t\t       sta->sta.addr);\n\t\tieee80211_send_smps_action(sdata, sdata->bss->req_smps,\n\t\t\t\t\t   sta->sta.addr,\n\t\t\t\t\t   sdata->vif.bss_conf.bssid);\n\t}\n\n\tlocal->total_ps_buffered -= buffered;\n\n\tsta_info_recalc_tim(sta);\n\n\tps_dbg(sdata,\n\t       \"STA %pM aid %d sending %d filtered/%d PS frames since STA not sleeping anymore\\n\",\n\t       sta->sta.addr, sta->sta.aid, filtered, buffered);\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,8 @@\n \n \tskb_queue_head_init(&pending);\n \n+\t/* sync with ieee80211_tx_h_unicast_ps_buf */\n+\tspin_lock(&sta->ps_lock);\n \t/* Send all buffered frames to the station */\n \tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n \t\tint count = skb_queue_len(&pending), tmp;\n@@ -35,6 +37,7 @@\n \t}\n \n \tieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);\n+\tspin_unlock(&sta->ps_lock);\n \n \t/* This station just woke up and isn't aware of our SMPS state */\n \tif (!ieee80211_smps_is_restrictive(sta->known_smps_mode,",
        "function_modified_lines": {
            "added": [
                "\t/* sync with ieee80211_tx_h_unicast_ps_buf */",
                "\tspin_lock(&sta->ps_lock);",
                "\tspin_unlock(&sta->ps_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the mac80211 subsystem in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via network traffic that improperly interacts with the WLAN_STA_PS_STA state (aka power-save mode), related to sta_info.c and tx.c."
    },
    {
        "cve_id": "CVE-2014-2706",
        "code_before_change": "static ieee80211_tx_result\nieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tstruct sta_info *sta = tx->sta;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_local *local = tx->local;\n\n\tif (unlikely(!sta))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||\n\t\t      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&\n\t\t     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {\n\t\tint ac = skb_get_queue_mapping(tx->skb);\n\n\t\tps_dbg(sta->sdata, \"STA %pM aid %d: PS buffer for AC %d\\n\",\n\t\t       sta->sta.addr, sta->sta.aid, ac);\n\t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n\t\t\tpurge_old_ps_buffers(tx->local);\n\t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n\t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n\t\t\tps_dbg(tx->sdata,\n\t\t\t       \"STA %pM TX buffer for AC %d full - dropping oldest frame\\n\",\n\t\t\t       sta->sta.addr, ac);\n\t\t\tieee80211_free_txskb(&local->hw, old);\n\t\t} else\n\t\t\ttx->local->total_ps_buffered++;\n\n\t\tinfo->control.jiffies = jiffies;\n\t\tinfo->control.vif = &tx->sdata->vif;\n\t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n\t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n\t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n\n\t\tif (!timer_pending(&local->sta_cleanup))\n\t\t\tmod_timer(&local->sta_cleanup,\n\t\t\t\t  round_jiffies(jiffies +\n\t\t\t\t\t\tSTA_INFO_CLEANUP_INTERVAL));\n\n\t\t/*\n\t\t * We queued up some frames, so the TIM bit might\n\t\t * need to be set, recalculate it.\n\t\t */\n\t\tsta_info_recalc_tim(sta);\n\n\t\treturn TX_QUEUED;\n\t} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {\n\t\tps_dbg(tx->sdata,\n\t\t       \"STA %pM in PS mode, but polling/in SP -> send frame\\n\",\n\t\t       sta->sta.addr);\n\t}\n\n\treturn TX_CONTINUE;\n}",
        "code_after_change": "static ieee80211_tx_result\nieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tstruct sta_info *sta = tx->sta;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_local *local = tx->local;\n\n\tif (unlikely(!sta))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||\n\t\t      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&\n\t\t     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {\n\t\tint ac = skb_get_queue_mapping(tx->skb);\n\n\t\tps_dbg(sta->sdata, \"STA %pM aid %d: PS buffer for AC %d\\n\",\n\t\t       sta->sta.addr, sta->sta.aid, ac);\n\t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n\t\t\tpurge_old_ps_buffers(tx->local);\n\n\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */\n\t\tspin_lock(&sta->ps_lock);\n\t\t/*\n\t\t * STA woke up the meantime and all the frames on ps_tx_buf have\n\t\t * been queued to pending queue. No reordering can happen, go\n\t\t * ahead and Tx the packet.\n\t\t */\n\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&\n\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {\n\t\t\tspin_unlock(&sta->ps_lock);\n\t\t\treturn TX_CONTINUE;\n\t\t}\n\n\t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n\t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n\t\t\tps_dbg(tx->sdata,\n\t\t\t       \"STA %pM TX buffer for AC %d full - dropping oldest frame\\n\",\n\t\t\t       sta->sta.addr, ac);\n\t\t\tieee80211_free_txskb(&local->hw, old);\n\t\t} else\n\t\t\ttx->local->total_ps_buffered++;\n\n\t\tinfo->control.jiffies = jiffies;\n\t\tinfo->control.vif = &tx->sdata->vif;\n\t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n\t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n\t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n\t\tspin_unlock(&sta->ps_lock);\n\n\t\tif (!timer_pending(&local->sta_cleanup))\n\t\t\tmod_timer(&local->sta_cleanup,\n\t\t\t\t  round_jiffies(jiffies +\n\t\t\t\t\t\tSTA_INFO_CLEANUP_INTERVAL));\n\n\t\t/*\n\t\t * We queued up some frames, so the TIM bit might\n\t\t * need to be set, recalculate it.\n\t\t */\n\t\tsta_info_recalc_tim(sta);\n\n\t\treturn TX_QUEUED;\n\t} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {\n\t\tps_dbg(tx->sdata,\n\t\t       \"STA %pM in PS mode, but polling/in SP -> send frame\\n\",\n\t\t       sta->sta.addr);\n\t}\n\n\treturn TX_CONTINUE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,20 @@\n \t\t       sta->sta.addr, sta->sta.aid, ac);\n \t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n \t\t\tpurge_old_ps_buffers(tx->local);\n+\n+\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */\n+\t\tspin_lock(&sta->ps_lock);\n+\t\t/*\n+\t\t * STA woke up the meantime and all the frames on ps_tx_buf have\n+\t\t * been queued to pending queue. No reordering can happen, go\n+\t\t * ahead and Tx the packet.\n+\t\t */\n+\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&\n+\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {\n+\t\t\tspin_unlock(&sta->ps_lock);\n+\t\t\treturn TX_CONTINUE;\n+\t\t}\n+\n \t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n \t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n \t\t\tps_dbg(tx->sdata,\n@@ -31,6 +45,7 @@\n \t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n \t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n \t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n+\t\tspin_unlock(&sta->ps_lock);\n \n \t\tif (!timer_pending(&local->sta_cleanup))\n \t\t\tmod_timer(&local->sta_cleanup,",
        "function_modified_lines": {
            "added": [
                "",
                "\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */",
                "\t\tspin_lock(&sta->ps_lock);",
                "\t\t/*",
                "\t\t * STA woke up the meantime and all the frames on ps_tx_buf have",
                "\t\t * been queued to pending queue. No reordering can happen, go",
                "\t\t * ahead and Tx the packet.",
                "\t\t */",
                "\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&",
                "\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {",
                "\t\t\tspin_unlock(&sta->ps_lock);",
                "\t\t\treturn TX_CONTINUE;",
                "\t\t}",
                "",
                "\t\tspin_unlock(&sta->ps_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the mac80211 subsystem in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via network traffic that improperly interacts with the WLAN_STA_PS_STA state (aka power-save mode), related to sta_info.c and tx.c."
    },
    {
        "cve_id": "CVE-2014-3611",
        "code_before_change": "void __kvm_migrate_pit_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_pit *pit = vcpu->kvm->arch.vpit;\n\tstruct hrtimer *timer;\n\n\tif (!kvm_vcpu_is_bsp(vcpu) || !pit)\n\t\treturn;\n\n\ttimer = &pit->pit_state.timer;\n\tif (hrtimer_cancel(timer))\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS);\n}",
        "code_after_change": "void __kvm_migrate_pit_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_pit *pit = vcpu->kvm->arch.vpit;\n\tstruct hrtimer *timer;\n\n\tif (!kvm_vcpu_is_bsp(vcpu) || !pit)\n\t\treturn;\n\n\ttimer = &pit->pit_state.timer;\n\tmutex_lock(&pit->pit_state.lock);\n\tif (hrtimer_cancel(timer))\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS);\n\tmutex_unlock(&pit->pit_state.lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,8 @@\n \t\treturn;\n \n \ttimer = &pit->pit_state.timer;\n+\tmutex_lock(&pit->pit_state.lock);\n \tif (hrtimer_cancel(timer))\n \t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS);\n+\tmutex_unlock(&pit->pit_state.lock);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&pit->pit_state.lock);",
                "\tmutex_unlock(&pit->pit_state.lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the __kvm_migrate_pit_timer function in arch/x86/kvm/i8254.c in the KVM subsystem in the Linux kernel through 3.17.2 allows guest OS users to cause a denial of service (host OS crash) by leveraging incorrect PIT emulation."
    },
    {
        "cve_id": "CVE-2014-3940",
        "code_before_change": "static void queue_pages_hugetlb_pmd_range(struct vm_area_struct *vma,\n\t\tpmd_t *pmd, const nodemask_t *nodes, unsigned long flags,\n\t\t\t\t    void *private)\n{\n#ifdef CONFIG_HUGETLB_PAGE\n\tint nid;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\n\tptl = huge_pte_lock(hstate_vma(vma), vma->vm_mm, (pte_t *)pmd);\n\tpage = pte_page(huge_ptep_get((pte_t *)pmd));\n\tnid = page_to_nid(page);\n\tif (node_isset(nid, *nodes) == !!(flags & MPOL_MF_INVERT))\n\t\tgoto unlock;\n\t/* With MPOL_MF_MOVE, we migrate only unshared hugepage. */\n\tif (flags & (MPOL_MF_MOVE_ALL) ||\n\t    (flags & MPOL_MF_MOVE && page_mapcount(page) == 1))\n\t\tisolate_huge_page(page, private);\nunlock:\n\tspin_unlock(ptl);\n#else\n\tBUG();\n#endif\n}",
        "code_after_change": "static void queue_pages_hugetlb_pmd_range(struct vm_area_struct *vma,\n\t\tpmd_t *pmd, const nodemask_t *nodes, unsigned long flags,\n\t\t\t\t    void *private)\n{\n#ifdef CONFIG_HUGETLB_PAGE\n\tint nid;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t entry;\n\n\tptl = huge_pte_lock(hstate_vma(vma), vma->vm_mm, (pte_t *)pmd);\n\tentry = huge_ptep_get((pte_t *)pmd);\n\tif (!pte_present(entry))\n\t\tgoto unlock;\n\tpage = pte_page(entry);\n\tnid = page_to_nid(page);\n\tif (node_isset(nid, *nodes) == !!(flags & MPOL_MF_INVERT))\n\t\tgoto unlock;\n\t/* With MPOL_MF_MOVE, we migrate only unshared hugepage. */\n\tif (flags & (MPOL_MF_MOVE_ALL) ||\n\t    (flags & MPOL_MF_MOVE && page_mapcount(page) == 1))\n\t\tisolate_huge_page(page, private);\nunlock:\n\tspin_unlock(ptl);\n#else\n\tBUG();\n#endif\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,9 +6,13 @@\n \tint nid;\n \tstruct page *page;\n \tspinlock_t *ptl;\n+\tpte_t entry;\n \n \tptl = huge_pte_lock(hstate_vma(vma), vma->vm_mm, (pte_t *)pmd);\n-\tpage = pte_page(huge_ptep_get((pte_t *)pmd));\n+\tentry = huge_ptep_get((pte_t *)pmd);\n+\tif (!pte_present(entry))\n+\t\tgoto unlock;\n+\tpage = pte_page(entry);\n \tnid = page_to_nid(page);\n \tif (node_isset(nid, *nodes) == !!(flags & MPOL_MF_INVERT))\n \t\tgoto unlock;",
        "function_modified_lines": {
            "added": [
                "\tpte_t entry;",
                "\tentry = huge_ptep_get((pte_t *)pmd);",
                "\tif (!pte_present(entry))",
                "\t\tgoto unlock;",
                "\tpage = pte_page(entry);"
            ],
            "deleted": [
                "\tpage = pte_page(huge_ptep_get((pte_t *)pmd));"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The Linux kernel through 3.14.5 does not properly consider the presence of hugetlb entries, which allows local users to cause a denial of service (memory corruption or system crash) by accessing certain memory locations, as demonstrated by triggering a race condition via numa_maps read operations during hugepage migration, related to fs/proc/task_mmu.c and mm/mempolicy.c."
    },
    {
        "cve_id": "CVE-2014-4652",
        "code_before_change": "static int snd_ctl_elem_user_get(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tstruct user_element *ue = kcontrol->private_data;\n\n\tmemcpy(&ucontrol->value, ue->elem_data, ue->elem_data_size);\n\treturn 0;\n}",
        "code_after_change": "static int snd_ctl_elem_user_get(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tstruct user_element *ue = kcontrol->private_data;\n\n\tmutex_lock(&ue->card->user_ctl_lock);\n\tmemcpy(&ucontrol->value, ue->elem_data, ue->elem_data_size);\n\tmutex_unlock(&ue->card->user_ctl_lock);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,8 @@\n {\n \tstruct user_element *ue = kcontrol->private_data;\n \n+\tmutex_lock(&ue->card->user_ctl_lock);\n \tmemcpy(&ucontrol->value, ue->elem_data, ue->elem_data_size);\n+\tmutex_unlock(&ue->card->user_ctl_lock);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&ue->card->user_ctl_lock);",
                "\tmutex_unlock(&ue->card->user_ctl_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the tlv handler functionality in the snd_ctl_elem_user_tlv function in sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 allows local users to obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access."
    },
    {
        "cve_id": "CVE-2014-4652",
        "code_before_change": "static int snd_ctl_elem_user_put(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tint change;\n\tstruct user_element *ue = kcontrol->private_data;\n\n\tchange = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;\n\tif (change)\n\t\tmemcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);\n\treturn change;\n}",
        "code_after_change": "static int snd_ctl_elem_user_put(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tint change;\n\tstruct user_element *ue = kcontrol->private_data;\n\n\tmutex_lock(&ue->card->user_ctl_lock);\n\tchange = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;\n\tif (change)\n\t\tmemcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);\n\tmutex_unlock(&ue->card->user_ctl_lock);\n\treturn change;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,8 +4,10 @@\n \tint change;\n \tstruct user_element *ue = kcontrol->private_data;\n \n+\tmutex_lock(&ue->card->user_ctl_lock);\n \tchange = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;\n \tif (change)\n \t\tmemcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);\n+\tmutex_unlock(&ue->card->user_ctl_lock);\n \treturn change;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&ue->card->user_ctl_lock);",
                "\tmutex_unlock(&ue->card->user_ctl_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the tlv handler functionality in the snd_ctl_elem_user_tlv function in sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 allows local users to obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access."
    },
    {
        "cve_id": "CVE-2014-4652",
        "code_before_change": "static int snd_ctl_elem_user_tlv(struct snd_kcontrol *kcontrol,\n\t\t\t\t int op_flag,\n\t\t\t\t unsigned int size,\n\t\t\t\t unsigned int __user *tlv)\n{\n\tstruct user_element *ue = kcontrol->private_data;\n\tint change = 0;\n\tvoid *new_data;\n\n\tif (op_flag > 0) {\n\t\tif (size > 1024 * 128)\t/* sane value */\n\t\t\treturn -EINVAL;\n\n\t\tnew_data = memdup_user(tlv, size);\n\t\tif (IS_ERR(new_data))\n\t\t\treturn PTR_ERR(new_data);\n\t\tchange = ue->tlv_data_size != size;\n\t\tif (!change)\n\t\t\tchange = memcmp(ue->tlv_data, new_data, size);\n\t\tkfree(ue->tlv_data);\n\t\tue->tlv_data = new_data;\n\t\tue->tlv_data_size = size;\n\t} else {\n\t\tif (! ue->tlv_data_size || ! ue->tlv_data)\n\t\t\treturn -ENXIO;\n\t\tif (size < ue->tlv_data_size)\n\t\t\treturn -ENOSPC;\n\t\tif (copy_to_user(tlv, ue->tlv_data, ue->tlv_data_size))\n\t\t\treturn -EFAULT;\n\t}\n\treturn change;\n}",
        "code_after_change": "static int snd_ctl_elem_user_tlv(struct snd_kcontrol *kcontrol,\n\t\t\t\t int op_flag,\n\t\t\t\t unsigned int size,\n\t\t\t\t unsigned int __user *tlv)\n{\n\tstruct user_element *ue = kcontrol->private_data;\n\tint change = 0;\n\tvoid *new_data;\n\n\tif (op_flag > 0) {\n\t\tif (size > 1024 * 128)\t/* sane value */\n\t\t\treturn -EINVAL;\n\n\t\tnew_data = memdup_user(tlv, size);\n\t\tif (IS_ERR(new_data))\n\t\t\treturn PTR_ERR(new_data);\n\t\tmutex_lock(&ue->card->user_ctl_lock);\n\t\tchange = ue->tlv_data_size != size;\n\t\tif (!change)\n\t\t\tchange = memcmp(ue->tlv_data, new_data, size);\n\t\tkfree(ue->tlv_data);\n\t\tue->tlv_data = new_data;\n\t\tue->tlv_data_size = size;\n\t\tmutex_unlock(&ue->card->user_ctl_lock);\n\t} else {\n\t\tint ret = 0;\n\n\t\tmutex_lock(&ue->card->user_ctl_lock);\n\t\tif (!ue->tlv_data_size || !ue->tlv_data) {\n\t\t\tret = -ENXIO;\n\t\t\tgoto err_unlock;\n\t\t}\n\t\tif (size < ue->tlv_data_size) {\n\t\t\tret = -ENOSPC;\n\t\t\tgoto err_unlock;\n\t\t}\n\t\tif (copy_to_user(tlv, ue->tlv_data, ue->tlv_data_size))\n\t\t\tret = -EFAULT;\nerr_unlock:\n\t\tmutex_unlock(&ue->card->user_ctl_lock);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn change;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,19 +14,32 @@\n \t\tnew_data = memdup_user(tlv, size);\n \t\tif (IS_ERR(new_data))\n \t\t\treturn PTR_ERR(new_data);\n+\t\tmutex_lock(&ue->card->user_ctl_lock);\n \t\tchange = ue->tlv_data_size != size;\n \t\tif (!change)\n \t\t\tchange = memcmp(ue->tlv_data, new_data, size);\n \t\tkfree(ue->tlv_data);\n \t\tue->tlv_data = new_data;\n \t\tue->tlv_data_size = size;\n+\t\tmutex_unlock(&ue->card->user_ctl_lock);\n \t} else {\n-\t\tif (! ue->tlv_data_size || ! ue->tlv_data)\n-\t\t\treturn -ENXIO;\n-\t\tif (size < ue->tlv_data_size)\n-\t\t\treturn -ENOSPC;\n+\t\tint ret = 0;\n+\n+\t\tmutex_lock(&ue->card->user_ctl_lock);\n+\t\tif (!ue->tlv_data_size || !ue->tlv_data) {\n+\t\t\tret = -ENXIO;\n+\t\t\tgoto err_unlock;\n+\t\t}\n+\t\tif (size < ue->tlv_data_size) {\n+\t\t\tret = -ENOSPC;\n+\t\t\tgoto err_unlock;\n+\t\t}\n \t\tif (copy_to_user(tlv, ue->tlv_data, ue->tlv_data_size))\n-\t\t\treturn -EFAULT;\n+\t\t\tret = -EFAULT;\n+err_unlock:\n+\t\tmutex_unlock(&ue->card->user_ctl_lock);\n+\t\tif (ret)\n+\t\t\treturn ret;\n \t}\n \treturn change;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tmutex_lock(&ue->card->user_ctl_lock);",
                "\t\tmutex_unlock(&ue->card->user_ctl_lock);",
                "\t\tint ret = 0;",
                "",
                "\t\tmutex_lock(&ue->card->user_ctl_lock);",
                "\t\tif (!ue->tlv_data_size || !ue->tlv_data) {",
                "\t\t\tret = -ENXIO;",
                "\t\t\tgoto err_unlock;",
                "\t\t}",
                "\t\tif (size < ue->tlv_data_size) {",
                "\t\t\tret = -ENOSPC;",
                "\t\t\tgoto err_unlock;",
                "\t\t}",
                "\t\t\tret = -EFAULT;",
                "err_unlock:",
                "\t\tmutex_unlock(&ue->card->user_ctl_lock);",
                "\t\tif (ret)",
                "\t\t\treturn ret;"
            ],
            "deleted": [
                "\t\tif (! ue->tlv_data_size || ! ue->tlv_data)",
                "\t\t\treturn -ENXIO;",
                "\t\tif (size < ue->tlv_data_size)",
                "\t\t\treturn -ENOSPC;",
                "\t\t\treturn -EFAULT;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the tlv handler functionality in the snd_ctl_elem_user_tlv function in sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 allows local users to obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access."
    },
    {
        "cve_id": "CVE-2014-4652",
        "code_before_change": "int snd_card_new(struct device *parent, int idx, const char *xid,\n\t\t    struct module *module, int extra_size,\n\t\t    struct snd_card **card_ret)\n{\n\tstruct snd_card *card;\n\tint err;\n\n\tif (snd_BUG_ON(!card_ret))\n\t\treturn -EINVAL;\n\t*card_ret = NULL;\n\n\tif (extra_size < 0)\n\t\textra_size = 0;\n\tcard = kzalloc(sizeof(*card) + extra_size, GFP_KERNEL);\n\tif (!card)\n\t\treturn -ENOMEM;\n\tif (extra_size > 0)\n\t\tcard->private_data = (char *)card + sizeof(struct snd_card);\n\tif (xid)\n\t\tstrlcpy(card->id, xid, sizeof(card->id));\n\terr = 0;\n\tmutex_lock(&snd_card_mutex);\n\tif (idx < 0) /* first check the matching module-name slot */\n\t\tidx = get_slot_from_bitmask(idx, module_slot_match, module);\n\tif (idx < 0) /* if not matched, assign an empty slot */\n\t\tidx = get_slot_from_bitmask(idx, check_empty_slot, module);\n\tif (idx < 0)\n\t\terr = -ENODEV;\n\telse if (idx < snd_ecards_limit) {\n\t\tif (test_bit(idx, snd_cards_lock))\n\t\t\terr = -EBUSY;\t/* invalid */\n\t} else if (idx >= SNDRV_CARDS)\n\t\terr = -ENODEV;\n\tif (err < 0) {\n\t\tmutex_unlock(&snd_card_mutex);\n\t\tdev_err(parent, \"cannot find the slot for index %d (range 0-%i), error: %d\\n\",\n\t\t\t idx, snd_ecards_limit - 1, err);\n\t\tkfree(card);\n\t\treturn err;\n\t}\n\tset_bit(idx, snd_cards_lock);\t\t/* lock it */\n\tif (idx >= snd_ecards_limit)\n\t\tsnd_ecards_limit = idx + 1; /* increase the limit */\n\tmutex_unlock(&snd_card_mutex);\n\tcard->dev = parent;\n\tcard->number = idx;\n\tcard->module = module;\n\tINIT_LIST_HEAD(&card->devices);\n\tinit_rwsem(&card->controls_rwsem);\n\trwlock_init(&card->ctl_files_rwlock);\n\tINIT_LIST_HEAD(&card->controls);\n\tINIT_LIST_HEAD(&card->ctl_files);\n\tspin_lock_init(&card->files_lock);\n\tINIT_LIST_HEAD(&card->files_list);\n#ifdef CONFIG_PM\n\tmutex_init(&card->power_lock);\n\tinit_waitqueue_head(&card->power_sleep);\n#endif\n\n\tdevice_initialize(&card->card_dev);\n\tcard->card_dev.parent = parent;\n\tcard->card_dev.class = sound_class;\n\tcard->card_dev.release = release_card_device;\n\tcard->card_dev.groups = card_dev_attr_groups;\n\terr = kobject_set_name(&card->card_dev.kobj, \"card%d\", idx);\n\tif (err < 0)\n\t\tgoto __error;\n\n\t/* the control interface cannot be accessed from the user space until */\n\t/* snd_cards_bitmask and snd_cards are set with snd_card_register */\n\terr = snd_ctl_create(card);\n\tif (err < 0) {\n\t\tdev_err(parent, \"unable to register control minors\\n\");\n\t\tgoto __error;\n\t}\n\terr = snd_info_card_create(card);\n\tif (err < 0) {\n\t\tdev_err(parent, \"unable to create card info\\n\");\n\t\tgoto __error_ctl;\n\t}\n\t*card_ret = card;\n\treturn 0;\n\n      __error_ctl:\n\tsnd_device_free_all(card);\n      __error:\n\tput_device(&card->card_dev);\n  \treturn err;\n}",
        "code_after_change": "int snd_card_new(struct device *parent, int idx, const char *xid,\n\t\t    struct module *module, int extra_size,\n\t\t    struct snd_card **card_ret)\n{\n\tstruct snd_card *card;\n\tint err;\n\n\tif (snd_BUG_ON(!card_ret))\n\t\treturn -EINVAL;\n\t*card_ret = NULL;\n\n\tif (extra_size < 0)\n\t\textra_size = 0;\n\tcard = kzalloc(sizeof(*card) + extra_size, GFP_KERNEL);\n\tif (!card)\n\t\treturn -ENOMEM;\n\tif (extra_size > 0)\n\t\tcard->private_data = (char *)card + sizeof(struct snd_card);\n\tif (xid)\n\t\tstrlcpy(card->id, xid, sizeof(card->id));\n\terr = 0;\n\tmutex_lock(&snd_card_mutex);\n\tif (idx < 0) /* first check the matching module-name slot */\n\t\tidx = get_slot_from_bitmask(idx, module_slot_match, module);\n\tif (idx < 0) /* if not matched, assign an empty slot */\n\t\tidx = get_slot_from_bitmask(idx, check_empty_slot, module);\n\tif (idx < 0)\n\t\terr = -ENODEV;\n\telse if (idx < snd_ecards_limit) {\n\t\tif (test_bit(idx, snd_cards_lock))\n\t\t\terr = -EBUSY;\t/* invalid */\n\t} else if (idx >= SNDRV_CARDS)\n\t\terr = -ENODEV;\n\tif (err < 0) {\n\t\tmutex_unlock(&snd_card_mutex);\n\t\tdev_err(parent, \"cannot find the slot for index %d (range 0-%i), error: %d\\n\",\n\t\t\t idx, snd_ecards_limit - 1, err);\n\t\tkfree(card);\n\t\treturn err;\n\t}\n\tset_bit(idx, snd_cards_lock);\t\t/* lock it */\n\tif (idx >= snd_ecards_limit)\n\t\tsnd_ecards_limit = idx + 1; /* increase the limit */\n\tmutex_unlock(&snd_card_mutex);\n\tcard->dev = parent;\n\tcard->number = idx;\n\tcard->module = module;\n\tINIT_LIST_HEAD(&card->devices);\n\tinit_rwsem(&card->controls_rwsem);\n\trwlock_init(&card->ctl_files_rwlock);\n\tmutex_init(&card->user_ctl_lock);\n\tINIT_LIST_HEAD(&card->controls);\n\tINIT_LIST_HEAD(&card->ctl_files);\n\tspin_lock_init(&card->files_lock);\n\tINIT_LIST_HEAD(&card->files_list);\n#ifdef CONFIG_PM\n\tmutex_init(&card->power_lock);\n\tinit_waitqueue_head(&card->power_sleep);\n#endif\n\n\tdevice_initialize(&card->card_dev);\n\tcard->card_dev.parent = parent;\n\tcard->card_dev.class = sound_class;\n\tcard->card_dev.release = release_card_device;\n\tcard->card_dev.groups = card_dev_attr_groups;\n\terr = kobject_set_name(&card->card_dev.kobj, \"card%d\", idx);\n\tif (err < 0)\n\t\tgoto __error;\n\n\t/* the control interface cannot be accessed from the user space until */\n\t/* snd_cards_bitmask and snd_cards are set with snd_card_register */\n\terr = snd_ctl_create(card);\n\tif (err < 0) {\n\t\tdev_err(parent, \"unable to register control minors\\n\");\n\t\tgoto __error;\n\t}\n\terr = snd_info_card_create(card);\n\tif (err < 0) {\n\t\tdev_err(parent, \"unable to create card info\\n\");\n\t\tgoto __error_ctl;\n\t}\n\t*card_ret = card;\n\treturn 0;\n\n      __error_ctl:\n\tsnd_device_free_all(card);\n      __error:\n\tput_device(&card->card_dev);\n  \treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -48,6 +48,7 @@\n \tINIT_LIST_HEAD(&card->devices);\n \tinit_rwsem(&card->controls_rwsem);\n \trwlock_init(&card->ctl_files_rwlock);\n+\tmutex_init(&card->user_ctl_lock);\n \tINIT_LIST_HEAD(&card->controls);\n \tINIT_LIST_HEAD(&card->ctl_files);\n \tspin_lock_init(&card->files_lock);",
        "function_modified_lines": {
            "added": [
                "\tmutex_init(&card->user_ctl_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the tlv handler functionality in the snd_ctl_elem_user_tlv function in sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 allows local users to obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access."
    },
    {
        "cve_id": "CVE-2014-7842",
        "code_before_change": "static int handle_emulation_failure(struct kvm_vcpu *vcpu)\n{\n\tint r = EMULATE_DONE;\n\n\t++vcpu->stat.insn_emulation_fail;\n\ttrace_kvm_emulate_insn_failed(vcpu);\n\tif (!is_guest_mode(vcpu)) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\tr = EMULATE_FAIL;\n\t}\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\n\treturn r;\n}",
        "code_after_change": "static int handle_emulation_failure(struct kvm_vcpu *vcpu)\n{\n\tint r = EMULATE_DONE;\n\n\t++vcpu->stat.insn_emulation_fail;\n\ttrace_kvm_emulate_insn_failed(vcpu);\n\tif (!is_guest_mode(vcpu) && kvm_x86_ops->get_cpl(vcpu) == 0) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\tr = EMULATE_FAIL;\n\t}\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\n\treturn r;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \n \t++vcpu->stat.insn_emulation_fail;\n \ttrace_kvm_emulate_insn_failed(vcpu);\n-\tif (!is_guest_mode(vcpu)) {\n+\tif (!is_guest_mode(vcpu) && kvm_x86_ops->get_cpl(vcpu) == 0) {\n \t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n \t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n \t\tvcpu->run->internal.ndata = 0;",
        "function_modified_lines": {
            "added": [
                "\tif (!is_guest_mode(vcpu) && kvm_x86_ops->get_cpl(vcpu) == 0) {"
            ],
            "deleted": [
                "\tif (!is_guest_mode(vcpu)) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in arch/x86/kvm/x86.c in the Linux kernel before 3.17.4 allows guest OS users to cause a denial of service (guest OS crash) via a crafted application that performs an MMIO transaction or a PIO transaction to trigger a guest userspace emulation error report, a similar issue to CVE-2010-5313."
    },
    {
        "cve_id": "CVE-2014-8086",
        "code_before_change": "static ssize_t\next4_file_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file_inode(iocb->ki_filp);\n\tstruct mutex *aio_mutex = NULL;\n\tstruct blk_plug plug;\n\tint o_direct = file->f_flags & O_DIRECT;\n\tint overwrite = 0;\n\tsize_t length = iov_iter_count(from);\n\tssize_t ret;\n\tloff_t pos = iocb->ki_pos;\n\n\t/*\n\t * Unaligned direct AIO must be serialized; see comment above\n\t * In the case of O_APPEND, assume that we must always serialize\n\t */\n\tif (o_direct &&\n\t    ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS) &&\n\t    !is_sync_kiocb(iocb) &&\n\t    (file->f_flags & O_APPEND ||\n\t     ext4_unaligned_aio(inode, from, pos))) {\n\t\taio_mutex = ext4_aio_mutex(inode);\n\t\tmutex_lock(aio_mutex);\n\t\text4_unwritten_wait(inode);\n\t}\n\n\tmutex_lock(&inode->i_mutex);\n\tif (file->f_flags & O_APPEND)\n\t\tiocb->ki_pos = pos = i_size_read(inode);\n\n\t/*\n\t * If we have encountered a bitmap-format file, the size limit\n\t * is smaller than s_maxbytes, which is for extent-mapped files.\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t\tif ((pos > sbi->s_bitmap_maxbytes) ||\n\t\t    (pos == sbi->s_bitmap_maxbytes && length > 0)) {\n\t\t\tmutex_unlock(&inode->i_mutex);\n\t\t\tret = -EFBIG;\n\t\t\tgoto errout;\n\t\t}\n\n\t\tif (pos + length > sbi->s_bitmap_maxbytes)\n\t\t\tiov_iter_truncate(from, sbi->s_bitmap_maxbytes - pos);\n\t}\n\n\tif (o_direct) {\n\t\tblk_start_plug(&plug);\n\n\t\tiocb->private = &overwrite;\n\n\t\t/* check whether we do a DIO overwrite or not */\n\t\tif (ext4_should_dioread_nolock(inode) && !aio_mutex &&\n\t\t    !file->f_mapping->nrpages && pos + length <= i_size_read(inode)) {\n\t\t\tstruct ext4_map_blocks map;\n\t\t\tunsigned int blkbits = inode->i_blkbits;\n\t\t\tint err, len;\n\n\t\t\tmap.m_lblk = pos >> blkbits;\n\t\t\tmap.m_len = (EXT4_BLOCK_ALIGN(pos + length, blkbits) >> blkbits)\n\t\t\t\t- map.m_lblk;\n\t\t\tlen = map.m_len;\n\n\t\t\terr = ext4_map_blocks(NULL, inode, &map, 0);\n\t\t\t/*\n\t\t\t * 'err==len' means that all of blocks has\n\t\t\t * been preallocated no matter they are\n\t\t\t * initialized or not.  For excluding\n\t\t\t * unwritten extents, we need to check\n\t\t\t * m_flags.  There are two conditions that\n\t\t\t * indicate for initialized extents.  1) If we\n\t\t\t * hit extent cache, EXT4_MAP_MAPPED flag is\n\t\t\t * returned; 2) If we do a real lookup,\n\t\t\t * non-flags are returned.  So we should check\n\t\t\t * these two conditions.\n\t\t\t */\n\t\t\tif (err == len && (map.m_flags & EXT4_MAP_MAPPED))\n\t\t\t\toverwrite = 1;\n\t\t}\n\t}\n\n\tret = __generic_file_write_iter(iocb, from);\n\tmutex_unlock(&inode->i_mutex);\n\n\tif (ret > 0) {\n\t\tssize_t err;\n\n\t\terr = generic_write_sync(file, iocb->ki_pos - ret, ret);\n\t\tif (err < 0)\n\t\t\tret = err;\n\t}\n\tif (o_direct)\n\t\tblk_finish_plug(&plug);\n\nerrout:\n\tif (aio_mutex)\n\t\tmutex_unlock(aio_mutex);\n\treturn ret;\n}",
        "code_after_change": "static ssize_t\next4_file_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file_inode(iocb->ki_filp);\n\tstruct mutex *aio_mutex = NULL;\n\tstruct blk_plug plug;\n\tint o_direct = file->f_flags & O_DIRECT;\n\tint overwrite = 0;\n\tsize_t length = iov_iter_count(from);\n\tssize_t ret;\n\tloff_t pos = iocb->ki_pos;\n\n\t/*\n\t * Unaligned direct AIO must be serialized; see comment above\n\t * In the case of O_APPEND, assume that we must always serialize\n\t */\n\tif (o_direct &&\n\t    ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS) &&\n\t    !is_sync_kiocb(iocb) &&\n\t    (file->f_flags & O_APPEND ||\n\t     ext4_unaligned_aio(inode, from, pos))) {\n\t\taio_mutex = ext4_aio_mutex(inode);\n\t\tmutex_lock(aio_mutex);\n\t\text4_unwritten_wait(inode);\n\t}\n\n\tmutex_lock(&inode->i_mutex);\n\tif (file->f_flags & O_APPEND)\n\t\tiocb->ki_pos = pos = i_size_read(inode);\n\n\t/*\n\t * If we have encountered a bitmap-format file, the size limit\n\t * is smaller than s_maxbytes, which is for extent-mapped files.\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t\tif ((pos > sbi->s_bitmap_maxbytes) ||\n\t\t    (pos == sbi->s_bitmap_maxbytes && length > 0)) {\n\t\t\tmutex_unlock(&inode->i_mutex);\n\t\t\tret = -EFBIG;\n\t\t\tgoto errout;\n\t\t}\n\n\t\tif (pos + length > sbi->s_bitmap_maxbytes)\n\t\t\tiov_iter_truncate(from, sbi->s_bitmap_maxbytes - pos);\n\t}\n\n\tiocb->private = &overwrite;\n\tif (o_direct) {\n\t\tblk_start_plug(&plug);\n\n\n\t\t/* check whether we do a DIO overwrite or not */\n\t\tif (ext4_should_dioread_nolock(inode) && !aio_mutex &&\n\t\t    !file->f_mapping->nrpages && pos + length <= i_size_read(inode)) {\n\t\t\tstruct ext4_map_blocks map;\n\t\t\tunsigned int blkbits = inode->i_blkbits;\n\t\t\tint err, len;\n\n\t\t\tmap.m_lblk = pos >> blkbits;\n\t\t\tmap.m_len = (EXT4_BLOCK_ALIGN(pos + length, blkbits) >> blkbits)\n\t\t\t\t- map.m_lblk;\n\t\t\tlen = map.m_len;\n\n\t\t\terr = ext4_map_blocks(NULL, inode, &map, 0);\n\t\t\t/*\n\t\t\t * 'err==len' means that all of blocks has\n\t\t\t * been preallocated no matter they are\n\t\t\t * initialized or not.  For excluding\n\t\t\t * unwritten extents, we need to check\n\t\t\t * m_flags.  There are two conditions that\n\t\t\t * indicate for initialized extents.  1) If we\n\t\t\t * hit extent cache, EXT4_MAP_MAPPED flag is\n\t\t\t * returned; 2) If we do a real lookup,\n\t\t\t * non-flags are returned.  So we should check\n\t\t\t * these two conditions.\n\t\t\t */\n\t\t\tif (err == len && (map.m_flags & EXT4_MAP_MAPPED))\n\t\t\t\toverwrite = 1;\n\t\t}\n\t}\n\n\tret = __generic_file_write_iter(iocb, from);\n\tmutex_unlock(&inode->i_mutex);\n\n\tif (ret > 0) {\n\t\tssize_t err;\n\n\t\terr = generic_write_sync(file, iocb->ki_pos - ret, ret);\n\t\tif (err < 0)\n\t\t\tret = err;\n\t}\n\tif (o_direct)\n\t\tblk_finish_plug(&plug);\n\nerrout:\n\tif (aio_mutex)\n\t\tmutex_unlock(aio_mutex);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,10 +47,10 @@\n \t\t\tiov_iter_truncate(from, sbi->s_bitmap_maxbytes - pos);\n \t}\n \n+\tiocb->private = &overwrite;\n \tif (o_direct) {\n \t\tblk_start_plug(&plug);\n \n-\t\tiocb->private = &overwrite;\n \n \t\t/* check whether we do a DIO overwrite or not */\n \t\tif (ext4_should_dioread_nolock(inode) && !aio_mutex &&",
        "function_modified_lines": {
            "added": [
                "\tiocb->private = &overwrite;"
            ],
            "deleted": [
                "\t\tiocb->private = &overwrite;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ext4_file_write_iter function in fs/ext4/file.c in the Linux kernel through 3.17 allows local users to cause a denial of service (file unavailability) via a combination of a write action and an F_SETFL fcntl operation for the O_DIRECT flag."
    },
    {
        "cve_id": "CVE-2014-9529",
        "code_before_change": "static noinline void key_gc_unused_keys(struct list_head *keys)\n{\n\twhile (!list_empty(keys)) {\n\t\tstruct key *key =\n\t\t\tlist_entry(keys->next, struct key, graveyard_link);\n\t\tlist_del(&key->graveyard_link);\n\n\t\tkdebug(\"- %u\", key->serial);\n\t\tkey_check(key);\n\n\t\tsecurity_key_free(key);\n\n\t\t/* deal with the user's key tracking and quota */\n\t\tif (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {\n\t\t\tspin_lock(&key->user->lock);\n\t\t\tkey->user->qnkeys--;\n\t\t\tkey->user->qnbytes -= key->quotalen;\n\t\t\tspin_unlock(&key->user->lock);\n\t\t}\n\n\t\tatomic_dec(&key->user->nkeys);\n\t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))\n\t\t\tatomic_dec(&key->user->nikeys);\n\n\t\tkey_user_put(key->user);\n\n\t\t/* now throw away the key memory */\n\t\tif (key->type->destroy)\n\t\t\tkey->type->destroy(key);\n\n\t\tkfree(key->description);\n\n#ifdef KEY_DEBUGGING\n\t\tkey->magic = KEY_DEBUG_MAGIC_X;\n#endif\n\t\tkmem_cache_free(key_jar, key);\n\t}\n}",
        "code_after_change": "static noinline void key_gc_unused_keys(struct list_head *keys)\n{\n\twhile (!list_empty(keys)) {\n\t\tstruct key *key =\n\t\t\tlist_entry(keys->next, struct key, graveyard_link);\n\t\tlist_del(&key->graveyard_link);\n\n\t\tkdebug(\"- %u\", key->serial);\n\t\tkey_check(key);\n\n\t\tsecurity_key_free(key);\n\n\t\t/* deal with the user's key tracking and quota */\n\t\tif (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {\n\t\t\tspin_lock(&key->user->lock);\n\t\t\tkey->user->qnkeys--;\n\t\t\tkey->user->qnbytes -= key->quotalen;\n\t\t\tspin_unlock(&key->user->lock);\n\t\t}\n\n\t\tatomic_dec(&key->user->nkeys);\n\t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))\n\t\t\tatomic_dec(&key->user->nikeys);\n\n\t\t/* now throw away the key memory */\n\t\tif (key->type->destroy)\n\t\t\tkey->type->destroy(key);\n\n\t\tkey_user_put(key->user);\n\n\t\tkfree(key->description);\n\n#ifdef KEY_DEBUGGING\n\t\tkey->magic = KEY_DEBUG_MAGIC_X;\n#endif\n\t\tkmem_cache_free(key_jar, key);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,11 +22,11 @@\n \t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))\n \t\t\tatomic_dec(&key->user->nikeys);\n \n-\t\tkey_user_put(key->user);\n-\n \t\t/* now throw away the key memory */\n \t\tif (key->type->destroy)\n \t\t\tkey->type->destroy(key);\n+\n+\t\tkey_user_put(key->user);\n \n \t\tkfree(key->description);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\t\tkey_user_put(key->user);"
            ],
            "deleted": [
                "\t\tkey_user_put(key->user);",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the key_gc_unused_keys function in security/keys/gc.c in the Linux kernel through 3.18.2 allows local users to cause a denial of service (memory corruption or panic) or possibly have unspecified other impact via keyctl commands that trigger access to a key structure member during garbage collection of a key."
    },
    {
        "cve_id": "CVE-2014-9710",
        "code_before_change": "int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t      *root, struct btrfs_key *key, struct btrfs_path *p, int\n\t\t      ins_len, int cow)\n{\n\tstruct extent_buffer *b;\n\tint slot;\n\tint ret;\n\tint err;\n\tint level;\n\tint lowest_unlock = 1;\n\tint root_lock;\n\t/* everything at write_lock_level or lower must be write locked */\n\tint write_lock_level = 0;\n\tu8 lowest_level = 0;\n\tint min_write_lock_level;\n\tint prev_cmp;\n\n\tlowest_level = p->lowest_level;\n\tWARN_ON(lowest_level && ins_len > 0);\n\tWARN_ON(p->nodes[0] != NULL);\n\tBUG_ON(!cow && ins_len);\n\n\tif (ins_len < 0) {\n\t\tlowest_unlock = 2;\n\n\t\t/* when we are removing items, we might have to go up to level\n\t\t * two as we update tree pointers  Make sure we keep write\n\t\t * for those levels as well\n\t\t */\n\t\twrite_lock_level = 2;\n\t} else if (ins_len > 0) {\n\t\t/*\n\t\t * for inserting items, make sure we have a write lock on\n\t\t * level 1 so we can update keys\n\t\t */\n\t\twrite_lock_level = 1;\n\t}\n\n\tif (!cow)\n\t\twrite_lock_level = -1;\n\n\tif (cow && (p->keep_locks || p->lowest_level))\n\t\twrite_lock_level = BTRFS_MAX_LEVEL;\n\n\tmin_write_lock_level = write_lock_level;\n\nagain:\n\tprev_cmp = -1;\n\t/*\n\t * we try very hard to do read locks on the root\n\t */\n\troot_lock = BTRFS_READ_LOCK;\n\tlevel = 0;\n\tif (p->search_commit_root) {\n\t\t/*\n\t\t * the commit roots are read only\n\t\t * so we always do read locks\n\t\t */\n\t\tif (p->need_commit_sem)\n\t\t\tdown_read(&root->fs_info->commit_root_sem);\n\t\tb = root->commit_root;\n\t\textent_buffer_get(b);\n\t\tlevel = btrfs_header_level(b);\n\t\tif (p->need_commit_sem)\n\t\t\tup_read(&root->fs_info->commit_root_sem);\n\t\tif (!p->skip_locking)\n\t\t\tbtrfs_tree_read_lock(b);\n\t} else {\n\t\tif (p->skip_locking) {\n\t\t\tb = btrfs_root_node(root);\n\t\t\tlevel = btrfs_header_level(b);\n\t\t} else {\n\t\t\t/* we don't know the level of the root node\n\t\t\t * until we actually have it read locked\n\t\t\t */\n\t\t\tb = btrfs_read_lock_root_node(root);\n\t\t\tlevel = btrfs_header_level(b);\n\t\t\tif (level <= write_lock_level) {\n\t\t\t\t/* whoops, must trade for write lock */\n\t\t\t\tbtrfs_tree_read_unlock(b);\n\t\t\t\tfree_extent_buffer(b);\n\t\t\t\tb = btrfs_lock_root_node(root);\n\t\t\t\troot_lock = BTRFS_WRITE_LOCK;\n\n\t\t\t\t/* the level might have changed, check again */\n\t\t\t\tlevel = btrfs_header_level(b);\n\t\t\t}\n\t\t}\n\t}\n\tp->nodes[level] = b;\n\tif (!p->skip_locking)\n\t\tp->locks[level] = root_lock;\n\n\twhile (b) {\n\t\tlevel = btrfs_header_level(b);\n\n\t\t/*\n\t\t * setup the path here so we can release it under lock\n\t\t * contention with the cow code\n\t\t */\n\t\tif (cow) {\n\t\t\t/*\n\t\t\t * if we don't really need to cow this block\n\t\t\t * then we don't want to set the path blocking,\n\t\t\t * so we test it here\n\t\t\t */\n\t\t\tif (!should_cow_block(trans, root, b))\n\t\t\t\tgoto cow_done;\n\n\t\t\t/*\n\t\t\t * must have write locks on this node and the\n\t\t\t * parent\n\t\t\t */\n\t\t\tif (level > write_lock_level ||\n\t\t\t    (level + 1 > write_lock_level &&\n\t\t\t    level + 1 < BTRFS_MAX_LEVEL &&\n\t\t\t    p->nodes[level + 1])) {\n\t\t\t\twrite_lock_level = level + 1;\n\t\t\t\tbtrfs_release_path(p);\n\t\t\t\tgoto again;\n\t\t\t}\n\n\t\t\tbtrfs_set_path_blocking(p);\n\t\t\terr = btrfs_cow_block(trans, root, b,\n\t\t\t\t\t      p->nodes[level + 1],\n\t\t\t\t\t      p->slots[level + 1], &b);\n\t\t\tif (err) {\n\t\t\t\tret = err;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\ncow_done:\n\t\tp->nodes[level] = b;\n\t\tbtrfs_clear_path_blocking(p, NULL, 0);\n\n\t\t/*\n\t\t * we have a lock on b and as long as we aren't changing\n\t\t * the tree, there is no way to for the items in b to change.\n\t\t * It is safe to drop the lock on our parent before we\n\t\t * go through the expensive btree search on b.\n\t\t *\n\t\t * If we're inserting or deleting (ins_len != 0), then we might\n\t\t * be changing slot zero, which may require changing the parent.\n\t\t * So, we can't drop the lock until after we know which slot\n\t\t * we're operating on.\n\t\t */\n\t\tif (!ins_len && !p->keep_locks) {\n\t\t\tint u = level + 1;\n\n\t\t\tif (u < BTRFS_MAX_LEVEL && p->locks[u]) {\n\t\t\t\tbtrfs_tree_unlock_rw(p->nodes[u], p->locks[u]);\n\t\t\t\tp->locks[u] = 0;\n\t\t\t}\n\t\t}\n\n\t\tret = key_search(b, key, level, &prev_cmp, &slot);\n\n\t\tif (level != 0) {\n\t\t\tint dec = 0;\n\t\t\tif (ret && slot > 0) {\n\t\t\t\tdec = 1;\n\t\t\t\tslot -= 1;\n\t\t\t}\n\t\t\tp->slots[level] = slot;\n\t\t\terr = setup_nodes_for_search(trans, root, p, b, level,\n\t\t\t\t\t     ins_len, &write_lock_level);\n\t\t\tif (err == -EAGAIN)\n\t\t\t\tgoto again;\n\t\t\tif (err) {\n\t\t\t\tret = err;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t\tb = p->nodes[level];\n\t\t\tslot = p->slots[level];\n\n\t\t\t/*\n\t\t\t * slot 0 is special, if we change the key\n\t\t\t * we have to update the parent pointer\n\t\t\t * which means we must have a write lock\n\t\t\t * on the parent\n\t\t\t */\n\t\t\tif (slot == 0 && ins_len &&\n\t\t\t    write_lock_level < level + 1) {\n\t\t\t\twrite_lock_level = level + 1;\n\t\t\t\tbtrfs_release_path(p);\n\t\t\t\tgoto again;\n\t\t\t}\n\n\t\t\tunlock_up(p, level, lowest_unlock,\n\t\t\t\t  min_write_lock_level, &write_lock_level);\n\n\t\t\tif (level == lowest_level) {\n\t\t\t\tif (dec)\n\t\t\t\t\tp->slots[level]++;\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\terr = read_block_for_search(trans, root, p,\n\t\t\t\t\t\t    &b, level, slot, key, 0);\n\t\t\tif (err == -EAGAIN)\n\t\t\t\tgoto again;\n\t\t\tif (err) {\n\t\t\t\tret = err;\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tif (!p->skip_locking) {\n\t\t\t\tlevel = btrfs_header_level(b);\n\t\t\t\tif (level <= write_lock_level) {\n\t\t\t\t\terr = btrfs_try_tree_write_lock(b);\n\t\t\t\t\tif (!err) {\n\t\t\t\t\t\tbtrfs_set_path_blocking(p);\n\t\t\t\t\t\tbtrfs_tree_lock(b);\n\t\t\t\t\t\tbtrfs_clear_path_blocking(p, b,\n\t\t\t\t\t\t\t\t  BTRFS_WRITE_LOCK);\n\t\t\t\t\t}\n\t\t\t\t\tp->locks[level] = BTRFS_WRITE_LOCK;\n\t\t\t\t} else {\n\t\t\t\t\terr = btrfs_try_tree_read_lock(b);\n\t\t\t\t\tif (!err) {\n\t\t\t\t\t\tbtrfs_set_path_blocking(p);\n\t\t\t\t\t\tbtrfs_tree_read_lock(b);\n\t\t\t\t\t\tbtrfs_clear_path_blocking(p, b,\n\t\t\t\t\t\t\t\t  BTRFS_READ_LOCK);\n\t\t\t\t\t}\n\t\t\t\t\tp->locks[level] = BTRFS_READ_LOCK;\n\t\t\t\t}\n\t\t\t\tp->nodes[level] = b;\n\t\t\t}\n\t\t} else {\n\t\t\tp->slots[level] = slot;\n\t\t\tif (ins_len > 0 &&\n\t\t\t    btrfs_leaf_free_space(root, b) < ins_len) {\n\t\t\t\tif (write_lock_level < 1) {\n\t\t\t\t\twrite_lock_level = 1;\n\t\t\t\t\tbtrfs_release_path(p);\n\t\t\t\t\tgoto again;\n\t\t\t\t}\n\n\t\t\t\tbtrfs_set_path_blocking(p);\n\t\t\t\terr = split_leaf(trans, root, key,\n\t\t\t\t\t\t p, ins_len, ret == 0);\n\t\t\t\tbtrfs_clear_path_blocking(p, NULL, 0);\n\n\t\t\t\tBUG_ON(err > 0);\n\t\t\t\tif (err) {\n\t\t\t\t\tret = err;\n\t\t\t\t\tgoto done;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!p->search_for_split)\n\t\t\t\tunlock_up(p, level, lowest_unlock,\n\t\t\t\t\t  min_write_lock_level, &write_lock_level);\n\t\t\tgoto done;\n\t\t}\n\t}\n\tret = 1;\ndone:\n\t/*\n\t * we don't really know what they plan on doing with the path\n\t * from here on, so for now just mark it as blocking\n\t */\n\tif (!p->leave_spinning)\n\t\tbtrfs_set_path_blocking(p);\n\tif (ret < 0)\n\t\tbtrfs_release_path(p);\n\treturn ret;\n}",
        "code_after_change": "int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root\n\t\t      *root, struct btrfs_key *key, struct btrfs_path *p, int\n\t\t      ins_len, int cow)\n{\n\tstruct extent_buffer *b;\n\tint slot;\n\tint ret;\n\tint err;\n\tint level;\n\tint lowest_unlock = 1;\n\tint root_lock;\n\t/* everything at write_lock_level or lower must be write locked */\n\tint write_lock_level = 0;\n\tu8 lowest_level = 0;\n\tint min_write_lock_level;\n\tint prev_cmp;\n\n\tlowest_level = p->lowest_level;\n\tWARN_ON(lowest_level && ins_len > 0);\n\tWARN_ON(p->nodes[0] != NULL);\n\tBUG_ON(!cow && ins_len);\n\n\tif (ins_len < 0) {\n\t\tlowest_unlock = 2;\n\n\t\t/* when we are removing items, we might have to go up to level\n\t\t * two as we update tree pointers  Make sure we keep write\n\t\t * for those levels as well\n\t\t */\n\t\twrite_lock_level = 2;\n\t} else if (ins_len > 0) {\n\t\t/*\n\t\t * for inserting items, make sure we have a write lock on\n\t\t * level 1 so we can update keys\n\t\t */\n\t\twrite_lock_level = 1;\n\t}\n\n\tif (!cow)\n\t\twrite_lock_level = -1;\n\n\tif (cow && (p->keep_locks || p->lowest_level))\n\t\twrite_lock_level = BTRFS_MAX_LEVEL;\n\n\tmin_write_lock_level = write_lock_level;\n\nagain:\n\tprev_cmp = -1;\n\t/*\n\t * we try very hard to do read locks on the root\n\t */\n\troot_lock = BTRFS_READ_LOCK;\n\tlevel = 0;\n\tif (p->search_commit_root) {\n\t\t/*\n\t\t * the commit roots are read only\n\t\t * so we always do read locks\n\t\t */\n\t\tif (p->need_commit_sem)\n\t\t\tdown_read(&root->fs_info->commit_root_sem);\n\t\tb = root->commit_root;\n\t\textent_buffer_get(b);\n\t\tlevel = btrfs_header_level(b);\n\t\tif (p->need_commit_sem)\n\t\t\tup_read(&root->fs_info->commit_root_sem);\n\t\tif (!p->skip_locking)\n\t\t\tbtrfs_tree_read_lock(b);\n\t} else {\n\t\tif (p->skip_locking) {\n\t\t\tb = btrfs_root_node(root);\n\t\t\tlevel = btrfs_header_level(b);\n\t\t} else {\n\t\t\t/* we don't know the level of the root node\n\t\t\t * until we actually have it read locked\n\t\t\t */\n\t\t\tb = btrfs_read_lock_root_node(root);\n\t\t\tlevel = btrfs_header_level(b);\n\t\t\tif (level <= write_lock_level) {\n\t\t\t\t/* whoops, must trade for write lock */\n\t\t\t\tbtrfs_tree_read_unlock(b);\n\t\t\t\tfree_extent_buffer(b);\n\t\t\t\tb = btrfs_lock_root_node(root);\n\t\t\t\troot_lock = BTRFS_WRITE_LOCK;\n\n\t\t\t\t/* the level might have changed, check again */\n\t\t\t\tlevel = btrfs_header_level(b);\n\t\t\t}\n\t\t}\n\t}\n\tp->nodes[level] = b;\n\tif (!p->skip_locking)\n\t\tp->locks[level] = root_lock;\n\n\twhile (b) {\n\t\tlevel = btrfs_header_level(b);\n\n\t\t/*\n\t\t * setup the path here so we can release it under lock\n\t\t * contention with the cow code\n\t\t */\n\t\tif (cow) {\n\t\t\t/*\n\t\t\t * if we don't really need to cow this block\n\t\t\t * then we don't want to set the path blocking,\n\t\t\t * so we test it here\n\t\t\t */\n\t\t\tif (!should_cow_block(trans, root, b))\n\t\t\t\tgoto cow_done;\n\n\t\t\t/*\n\t\t\t * must have write locks on this node and the\n\t\t\t * parent\n\t\t\t */\n\t\t\tif (level > write_lock_level ||\n\t\t\t    (level + 1 > write_lock_level &&\n\t\t\t    level + 1 < BTRFS_MAX_LEVEL &&\n\t\t\t    p->nodes[level + 1])) {\n\t\t\t\twrite_lock_level = level + 1;\n\t\t\t\tbtrfs_release_path(p);\n\t\t\t\tgoto again;\n\t\t\t}\n\n\t\t\tbtrfs_set_path_blocking(p);\n\t\t\terr = btrfs_cow_block(trans, root, b,\n\t\t\t\t\t      p->nodes[level + 1],\n\t\t\t\t\t      p->slots[level + 1], &b);\n\t\t\tif (err) {\n\t\t\t\tret = err;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\ncow_done:\n\t\tp->nodes[level] = b;\n\t\tbtrfs_clear_path_blocking(p, NULL, 0);\n\n\t\t/*\n\t\t * we have a lock on b and as long as we aren't changing\n\t\t * the tree, there is no way to for the items in b to change.\n\t\t * It is safe to drop the lock on our parent before we\n\t\t * go through the expensive btree search on b.\n\t\t *\n\t\t * If we're inserting or deleting (ins_len != 0), then we might\n\t\t * be changing slot zero, which may require changing the parent.\n\t\t * So, we can't drop the lock until after we know which slot\n\t\t * we're operating on.\n\t\t */\n\t\tif (!ins_len && !p->keep_locks) {\n\t\t\tint u = level + 1;\n\n\t\t\tif (u < BTRFS_MAX_LEVEL && p->locks[u]) {\n\t\t\t\tbtrfs_tree_unlock_rw(p->nodes[u], p->locks[u]);\n\t\t\t\tp->locks[u] = 0;\n\t\t\t}\n\t\t}\n\n\t\tret = key_search(b, key, level, &prev_cmp, &slot);\n\n\t\tif (level != 0) {\n\t\t\tint dec = 0;\n\t\t\tif (ret && slot > 0) {\n\t\t\t\tdec = 1;\n\t\t\t\tslot -= 1;\n\t\t\t}\n\t\t\tp->slots[level] = slot;\n\t\t\terr = setup_nodes_for_search(trans, root, p, b, level,\n\t\t\t\t\t     ins_len, &write_lock_level);\n\t\t\tif (err == -EAGAIN)\n\t\t\t\tgoto again;\n\t\t\tif (err) {\n\t\t\t\tret = err;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t\tb = p->nodes[level];\n\t\t\tslot = p->slots[level];\n\n\t\t\t/*\n\t\t\t * slot 0 is special, if we change the key\n\t\t\t * we have to update the parent pointer\n\t\t\t * which means we must have a write lock\n\t\t\t * on the parent\n\t\t\t */\n\t\t\tif (slot == 0 && ins_len &&\n\t\t\t    write_lock_level < level + 1) {\n\t\t\t\twrite_lock_level = level + 1;\n\t\t\t\tbtrfs_release_path(p);\n\t\t\t\tgoto again;\n\t\t\t}\n\n\t\t\tunlock_up(p, level, lowest_unlock,\n\t\t\t\t  min_write_lock_level, &write_lock_level);\n\n\t\t\tif (level == lowest_level) {\n\t\t\t\tif (dec)\n\t\t\t\t\tp->slots[level]++;\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\terr = read_block_for_search(trans, root, p,\n\t\t\t\t\t\t    &b, level, slot, key, 0);\n\t\t\tif (err == -EAGAIN)\n\t\t\t\tgoto again;\n\t\t\tif (err) {\n\t\t\t\tret = err;\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tif (!p->skip_locking) {\n\t\t\t\tlevel = btrfs_header_level(b);\n\t\t\t\tif (level <= write_lock_level) {\n\t\t\t\t\terr = btrfs_try_tree_write_lock(b);\n\t\t\t\t\tif (!err) {\n\t\t\t\t\t\tbtrfs_set_path_blocking(p);\n\t\t\t\t\t\tbtrfs_tree_lock(b);\n\t\t\t\t\t\tbtrfs_clear_path_blocking(p, b,\n\t\t\t\t\t\t\t\t  BTRFS_WRITE_LOCK);\n\t\t\t\t\t}\n\t\t\t\t\tp->locks[level] = BTRFS_WRITE_LOCK;\n\t\t\t\t} else {\n\t\t\t\t\terr = btrfs_try_tree_read_lock(b);\n\t\t\t\t\tif (!err) {\n\t\t\t\t\t\tbtrfs_set_path_blocking(p);\n\t\t\t\t\t\tbtrfs_tree_read_lock(b);\n\t\t\t\t\t\tbtrfs_clear_path_blocking(p, b,\n\t\t\t\t\t\t\t\t  BTRFS_READ_LOCK);\n\t\t\t\t\t}\n\t\t\t\t\tp->locks[level] = BTRFS_READ_LOCK;\n\t\t\t\t}\n\t\t\t\tp->nodes[level] = b;\n\t\t\t}\n\t\t} else {\n\t\t\tp->slots[level] = slot;\n\t\t\tif (ins_len > 0 &&\n\t\t\t    btrfs_leaf_free_space(root, b) < ins_len) {\n\t\t\t\tif (write_lock_level < 1) {\n\t\t\t\t\twrite_lock_level = 1;\n\t\t\t\t\tbtrfs_release_path(p);\n\t\t\t\t\tgoto again;\n\t\t\t\t}\n\n\t\t\t\tbtrfs_set_path_blocking(p);\n\t\t\t\terr = split_leaf(trans, root, key,\n\t\t\t\t\t\t p, ins_len, ret == 0);\n\t\t\t\tbtrfs_clear_path_blocking(p, NULL, 0);\n\n\t\t\t\tBUG_ON(err > 0);\n\t\t\t\tif (err) {\n\t\t\t\t\tret = err;\n\t\t\t\t\tgoto done;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!p->search_for_split)\n\t\t\t\tunlock_up(p, level, lowest_unlock,\n\t\t\t\t\t  min_write_lock_level, &write_lock_level);\n\t\t\tgoto done;\n\t\t}\n\t}\n\tret = 1;\ndone:\n\t/*\n\t * we don't really know what they plan on doing with the path\n\t * from here on, so for now just mark it as blocking\n\t */\n\tif (!p->leave_spinning)\n\t\tbtrfs_set_path_blocking(p);\n\tif (ret < 0 && !p->skip_release_on_error)\n\t\tbtrfs_release_path(p);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -262,7 +262,7 @@\n \t */\n \tif (!p->leave_spinning)\n \t\tbtrfs_set_path_blocking(p);\n-\tif (ret < 0)\n+\tif (ret < 0 && !p->skip_release_on_error)\n \t\tbtrfs_release_path(p);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (ret < 0 && !p->skip_release_on_error)"
            ],
            "deleted": [
                "\tif (ret < 0)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The Btrfs implementation in the Linux kernel before 3.19 does not ensure that the visible xattr state is consistent with a requested replacement, which allows local users to bypass intended ACL settings and gain privileges via standard filesystem operations (1) during an xattr-replacement time window, related to a race condition, or (2) after an xattr-replacement attempt that fails because the data does not fit."
    },
    {
        "cve_id": "CVE-2014-9710",
        "code_before_change": "static struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      const char *name, int name_len)\n{\n\tstruct btrfs_dir_item *dir_item;\n\tunsigned long name_ptr;\n\tu32 total_len;\n\tu32 cur = 0;\n\tu32 this_len;\n\tstruct extent_buffer *leaf;\n\n\tleaf = path->nodes[0];\n\tdir_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dir_item);\n\tif (verify_dir_item(root, leaf, dir_item))\n\t\treturn NULL;\n\n\ttotal_len = btrfs_item_size_nr(leaf, path->slots[0]);\n\twhile (cur < total_len) {\n\t\tthis_len = sizeof(*dir_item) +\n\t\t\tbtrfs_dir_name_len(leaf, dir_item) +\n\t\t\tbtrfs_dir_data_len(leaf, dir_item);\n\t\tname_ptr = (unsigned long)(dir_item + 1);\n\n\t\tif (btrfs_dir_name_len(leaf, dir_item) == name_len &&\n\t\t    memcmp_extent_buffer(leaf, name, name_ptr, name_len) == 0)\n\t\t\treturn dir_item;\n\n\t\tcur += this_len;\n\t\tdir_item = (struct btrfs_dir_item *)((char *)dir_item +\n\t\t\t\t\t\t     this_len);\n\t}\n\treturn NULL;\n}",
        "code_after_change": "struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n\t\t\t\t\t\t struct btrfs_path *path,\n\t\t\t\t\t\t const char *name, int name_len)\n{\n\tstruct btrfs_dir_item *dir_item;\n\tunsigned long name_ptr;\n\tu32 total_len;\n\tu32 cur = 0;\n\tu32 this_len;\n\tstruct extent_buffer *leaf;\n\n\tleaf = path->nodes[0];\n\tdir_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dir_item);\n\tif (verify_dir_item(root, leaf, dir_item))\n\t\treturn NULL;\n\n\ttotal_len = btrfs_item_size_nr(leaf, path->slots[0]);\n\twhile (cur < total_len) {\n\t\tthis_len = sizeof(*dir_item) +\n\t\t\tbtrfs_dir_name_len(leaf, dir_item) +\n\t\t\tbtrfs_dir_data_len(leaf, dir_item);\n\t\tname_ptr = (unsigned long)(dir_item + 1);\n\n\t\tif (btrfs_dir_name_len(leaf, dir_item) == name_len &&\n\t\t    memcmp_extent_buffer(leaf, name, name_ptr, name_len) == 0)\n\t\t\treturn dir_item;\n\n\t\tcur += this_len;\n\t\tdir_item = (struct btrfs_dir_item *)((char *)dir_item +\n\t\t\t\t\t\t     this_len);\n\t}\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n-static struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n-\t\t\t      struct btrfs_path *path,\n-\t\t\t      const char *name, int name_len)\n+struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n+\t\t\t\t\t\t struct btrfs_path *path,\n+\t\t\t\t\t\t const char *name, int name_len)\n {\n \tstruct btrfs_dir_item *dir_item;\n \tunsigned long name_ptr;",
        "function_modified_lines": {
            "added": [
                "struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,",
                "\t\t\t\t\t\t struct btrfs_path *path,",
                "\t\t\t\t\t\t const char *name, int name_len)"
            ],
            "deleted": [
                "static struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,",
                "\t\t\t      struct btrfs_path *path,",
                "\t\t\t      const char *name, int name_len)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The Btrfs implementation in the Linux kernel before 3.19 does not ensure that the visible xattr state is consistent with a requested replacement, which allows local users to bypass intended ACL settings and gain privileges via standard filesystem operations (1) during an xattr-replacement time window, related to a race condition, or (2) after an xattr-replacement attempt that fails because the data does not fit."
    },
    {
        "cve_id": "CVE-2014-9710",
        "code_before_change": "static int do_setxattr(struct btrfs_trans_handle *trans,\n\t\t       struct inode *inode, const char *name,\n\t\t       const void *value, size_t size, int flags)\n{\n\tstruct btrfs_dir_item *di;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_path *path;\n\tsize_t name_len = strlen(name);\n\tint ret = 0;\n\n\tif (name_len + size > BTRFS_MAX_XATTR_SIZE(root))\n\t\treturn -ENOSPC;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tif (flags & XATTR_REPLACE) {\n\t\tdi = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode), name,\n\t\t\t\t\tname_len, -1);\n\t\tif (IS_ERR(di)) {\n\t\t\tret = PTR_ERR(di);\n\t\t\tgoto out;\n\t\t} else if (!di) {\n\t\t\tret = -ENODATA;\n\t\t\tgoto out;\n\t\t}\n\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tbtrfs_release_path(path);\n\n\t\t/*\n\t\t * remove the attribute\n\t\t */\n\t\tif (!value)\n\t\t\tgoto out;\n\t} else {\n\t\tdi = btrfs_lookup_xattr(NULL, root, path, btrfs_ino(inode),\n\t\t\t\t\tname, name_len, 0);\n\t\tif (IS_ERR(di)) {\n\t\t\tret = PTR_ERR(di);\n\t\t\tgoto out;\n\t\t}\n\t\tif (!di && !value)\n\t\t\tgoto out;\n\t\tbtrfs_release_path(path);\n\t}\n\nagain:\n\tret = btrfs_insert_xattr_item(trans, root, path, btrfs_ino(inode),\n\t\t\t\t      name, name_len, value, size);\n\t/*\n\t * If we're setting an xattr to a new value but the new value is say\n\t * exactly BTRFS_MAX_XATTR_SIZE, we could end up with EOVERFLOW getting\n\t * back from split_leaf.  This is because it thinks we'll be extending\n\t * the existing item size, but we're asking for enough space to add the\n\t * item itself.  So if we get EOVERFLOW just set ret to EEXIST and let\n\t * the rest of the function figure it out.\n\t */\n\tif (ret == -EOVERFLOW)\n\t\tret = -EEXIST;\n\n\tif (ret == -EEXIST) {\n\t\tif (flags & XATTR_CREATE)\n\t\t\tgoto out;\n\t\t/*\n\t\t * We can't use the path we already have since we won't have the\n\t\t * proper locking for a delete, so release the path and\n\t\t * re-lookup to delete the thing.\n\t\t */\n\t\tbtrfs_release_path(path);\n\t\tdi = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode),\n\t\t\t\t\tname, name_len, -1);\n\t\tif (IS_ERR(di)) {\n\t\t\tret = PTR_ERR(di);\n\t\t\tgoto out;\n\t\t} else if (!di) {\n\t\t\t/* Shouldn't happen but just in case... */\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto again;\n\t\t}\n\n\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * We have a value to set, so go back and try to insert it now.\n\t\t */\n\t\tif (value) {\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto again;\n\t\t}\n\t}\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}",
        "code_after_change": "static int do_setxattr(struct btrfs_trans_handle *trans,\n\t\t       struct inode *inode, const char *name,\n\t\t       const void *value, size_t size, int flags)\n{\n\tstruct btrfs_dir_item *di = NULL;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_path *path;\n\tsize_t name_len = strlen(name);\n\tint ret = 0;\n\n\tif (name_len + size > BTRFS_MAX_XATTR_SIZE(root))\n\t\treturn -ENOSPC;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tpath->skip_release_on_error = 1;\n\n\tif (!value) {\n\t\tdi = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode),\n\t\t\t\t\tname, name_len, -1);\n\t\tif (!di && (flags & XATTR_REPLACE))\n\t\t\tret = -ENODATA;\n\t\telse if (di)\n\t\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * For a replace we can't just do the insert blindly.\n\t * Do a lookup first (read-only btrfs_search_slot), and return if xattr\n\t * doesn't exist. If it exists, fall down below to the insert/replace\n\t * path - we can't race with a concurrent xattr delete, because the VFS\n\t * locks the inode's i_mutex before calling setxattr or removexattr.\n\t */\n\tif (flags & XATTR_REPLACE) {\n\t\tASSERT(mutex_is_locked(&inode->i_mutex));\n\t\tdi = btrfs_lookup_xattr(NULL, root, path, btrfs_ino(inode),\n\t\t\t\t\tname, name_len, 0);\n\t\tif (!di) {\n\t\t\tret = -ENODATA;\n\t\t\tgoto out;\n\t\t}\n\t\tbtrfs_release_path(path);\n\t\tdi = NULL;\n\t}\n\n\tret = btrfs_insert_xattr_item(trans, root, path, btrfs_ino(inode),\n\t\t\t\t      name, name_len, value, size);\n\tif (ret == -EOVERFLOW) {\n\t\t/*\n\t\t * We have an existing item in a leaf, split_leaf couldn't\n\t\t * expand it. That item might have or not a dir_item that\n\t\t * matches our target xattr, so lets check.\n\t\t */\n\t\tret = 0;\n\t\tbtrfs_assert_tree_locked(path->nodes[0]);\n\t\tdi = btrfs_match_dir_item_name(root, path, name, name_len);\n\t\tif (!di && !(flags & XATTR_REPLACE)) {\n\t\t\tret = -ENOSPC;\n\t\t\tgoto out;\n\t\t}\n\t} else if (ret == -EEXIST) {\n\t\tret = 0;\n\t\tdi = btrfs_match_dir_item_name(root, path, name, name_len);\n\t\tASSERT(di); /* logic error */\n\t} else if (ret) {\n\t\tgoto out;\n\t}\n\n\tif (di && (flags & XATTR_CREATE)) {\n\t\tret = -EEXIST;\n\t\tgoto out;\n\t}\n\n\tif (di) {\n\t\t/*\n\t\t * We're doing a replace, and it must be atomic, that is, at\n\t\t * any point in time we have either the old or the new xattr\n\t\t * value in the tree. We don't want readers (getxattr and\n\t\t * listxattrs) to miss a value, this is specially important\n\t\t * for ACLs.\n\t\t */\n\t\tconst int slot = path->slots[0];\n\t\tstruct extent_buffer *leaf = path->nodes[0];\n\t\tconst u16 old_data_len = btrfs_dir_data_len(leaf, di);\n\t\tconst u32 item_size = btrfs_item_size_nr(leaf, slot);\n\t\tconst u32 data_size = sizeof(*di) + name_len + size;\n\t\tstruct btrfs_item *item;\n\t\tunsigned long data_ptr;\n\t\tchar *ptr;\n\n\t\tif (size > old_data_len) {\n\t\t\tif (btrfs_leaf_free_space(root, leaf) <\n\t\t\t    (size - old_data_len)) {\n\t\t\t\tret = -ENOSPC;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tif (old_data_len + name_len + sizeof(*di) == item_size) {\n\t\t\t/* No other xattrs packed in the same leaf item. */\n\t\t\tif (size > old_data_len)\n\t\t\t\tbtrfs_extend_item(root, path,\n\t\t\t\t\t\t  size - old_data_len);\n\t\t\telse if (size < old_data_len)\n\t\t\t\tbtrfs_truncate_item(root, path, data_size, 1);\n\t\t} else {\n\t\t\t/* There are other xattrs packed in the same item. */\n\t\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tbtrfs_extend_item(root, path, data_size);\n\t\t}\n\n\t\titem = btrfs_item_nr(slot);\n\t\tptr = btrfs_item_ptr(leaf, slot, char);\n\t\tptr += btrfs_item_size(leaf, item) - data_size;\n\t\tdi = (struct btrfs_dir_item *)ptr;\n\t\tbtrfs_set_dir_data_len(leaf, di, size);\n\t\tdata_ptr = ((unsigned long)(di + 1)) + name_len;\n\t\twrite_extent_buffer(leaf, value, data_ptr, size);\n\t\tbtrfs_mark_buffer_dirty(leaf);\n\t} else {\n\t\t/*\n\t\t * Insert, and we had space for the xattr, so path->slots[0] is\n\t\t * where our xattr dir_item is and btrfs_insert_xattr_item()\n\t\t * filled it.\n\t\t */\n\t}\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n \t\t       struct inode *inode, const char *name,\n \t\t       const void *value, size_t size, int flags)\n {\n-\tstruct btrfs_dir_item *di;\n+\tstruct btrfs_dir_item *di = NULL;\n \tstruct btrfs_root *root = BTRFS_I(inode)->root;\n \tstruct btrfs_path *path;\n \tsize_t name_len = strlen(name);\n@@ -14,84 +14,119 @@\n \tpath = btrfs_alloc_path();\n \tif (!path)\n \t\treturn -ENOMEM;\n+\tpath->skip_release_on_error = 1;\n \n+\tif (!value) {\n+\t\tdi = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode),\n+\t\t\t\t\tname, name_len, -1);\n+\t\tif (!di && (flags & XATTR_REPLACE))\n+\t\t\tret = -ENODATA;\n+\t\telse if (di)\n+\t\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n+\t\tgoto out;\n+\t}\n+\n+\t/*\n+\t * For a replace we can't just do the insert blindly.\n+\t * Do a lookup first (read-only btrfs_search_slot), and return if xattr\n+\t * doesn't exist. If it exists, fall down below to the insert/replace\n+\t * path - we can't race with a concurrent xattr delete, because the VFS\n+\t * locks the inode's i_mutex before calling setxattr or removexattr.\n+\t */\n \tif (flags & XATTR_REPLACE) {\n-\t\tdi = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode), name,\n-\t\t\t\t\tname_len, -1);\n-\t\tif (IS_ERR(di)) {\n-\t\t\tret = PTR_ERR(di);\n-\t\t\tgoto out;\n-\t\t} else if (!di) {\n+\t\tASSERT(mutex_is_locked(&inode->i_mutex));\n+\t\tdi = btrfs_lookup_xattr(NULL, root, path, btrfs_ino(inode),\n+\t\t\t\t\tname, name_len, 0);\n+\t\tif (!di) {\n \t\t\tret = -ENODATA;\n \t\t\tgoto out;\n \t\t}\n-\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n-\t\tif (ret)\n-\t\t\tgoto out;\n \t\tbtrfs_release_path(path);\n+\t\tdi = NULL;\n+\t}\n \n+\tret = btrfs_insert_xattr_item(trans, root, path, btrfs_ino(inode),\n+\t\t\t\t      name, name_len, value, size);\n+\tif (ret == -EOVERFLOW) {\n \t\t/*\n-\t\t * remove the attribute\n+\t\t * We have an existing item in a leaf, split_leaf couldn't\n+\t\t * expand it. That item might have or not a dir_item that\n+\t\t * matches our target xattr, so lets check.\n \t\t */\n-\t\tif (!value)\n-\t\t\tgoto out;\n-\t} else {\n-\t\tdi = btrfs_lookup_xattr(NULL, root, path, btrfs_ino(inode),\n-\t\t\t\t\tname, name_len, 0);\n-\t\tif (IS_ERR(di)) {\n-\t\t\tret = PTR_ERR(di);\n+\t\tret = 0;\n+\t\tbtrfs_assert_tree_locked(path->nodes[0]);\n+\t\tdi = btrfs_match_dir_item_name(root, path, name, name_len);\n+\t\tif (!di && !(flags & XATTR_REPLACE)) {\n+\t\t\tret = -ENOSPC;\n \t\t\tgoto out;\n \t\t}\n-\t\tif (!di && !value)\n-\t\t\tgoto out;\n-\t\tbtrfs_release_path(path);\n+\t} else if (ret == -EEXIST) {\n+\t\tret = 0;\n+\t\tdi = btrfs_match_dir_item_name(root, path, name, name_len);\n+\t\tASSERT(di); /* logic error */\n+\t} else if (ret) {\n+\t\tgoto out;\n \t}\n \n-again:\n-\tret = btrfs_insert_xattr_item(trans, root, path, btrfs_ino(inode),\n-\t\t\t\t      name, name_len, value, size);\n-\t/*\n-\t * If we're setting an xattr to a new value but the new value is say\n-\t * exactly BTRFS_MAX_XATTR_SIZE, we could end up with EOVERFLOW getting\n-\t * back from split_leaf.  This is because it thinks we'll be extending\n-\t * the existing item size, but we're asking for enough space to add the\n-\t * item itself.  So if we get EOVERFLOW just set ret to EEXIST and let\n-\t * the rest of the function figure it out.\n-\t */\n-\tif (ret == -EOVERFLOW)\n+\tif (di && (flags & XATTR_CREATE)) {\n \t\tret = -EEXIST;\n+\t\tgoto out;\n+\t}\n \n-\tif (ret == -EEXIST) {\n-\t\tif (flags & XATTR_CREATE)\n-\t\t\tgoto out;\n+\tif (di) {\n \t\t/*\n-\t\t * We can't use the path we already have since we won't have the\n-\t\t * proper locking for a delete, so release the path and\n-\t\t * re-lookup to delete the thing.\n+\t\t * We're doing a replace, and it must be atomic, that is, at\n+\t\t * any point in time we have either the old or the new xattr\n+\t\t * value in the tree. We don't want readers (getxattr and\n+\t\t * listxattrs) to miss a value, this is specially important\n+\t\t * for ACLs.\n \t\t */\n-\t\tbtrfs_release_path(path);\n-\t\tdi = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode),\n-\t\t\t\t\tname, name_len, -1);\n-\t\tif (IS_ERR(di)) {\n-\t\t\tret = PTR_ERR(di);\n-\t\t\tgoto out;\n-\t\t} else if (!di) {\n-\t\t\t/* Shouldn't happen but just in case... */\n-\t\t\tbtrfs_release_path(path);\n-\t\t\tgoto again;\n+\t\tconst int slot = path->slots[0];\n+\t\tstruct extent_buffer *leaf = path->nodes[0];\n+\t\tconst u16 old_data_len = btrfs_dir_data_len(leaf, di);\n+\t\tconst u32 item_size = btrfs_item_size_nr(leaf, slot);\n+\t\tconst u32 data_size = sizeof(*di) + name_len + size;\n+\t\tstruct btrfs_item *item;\n+\t\tunsigned long data_ptr;\n+\t\tchar *ptr;\n+\n+\t\tif (size > old_data_len) {\n+\t\t\tif (btrfs_leaf_free_space(root, leaf) <\n+\t\t\t    (size - old_data_len)) {\n+\t\t\t\tret = -ENOSPC;\n+\t\t\t\tgoto out;\n+\t\t\t}\n \t\t}\n \n-\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n-\t\tif (ret)\n-\t\t\tgoto out;\n+\t\tif (old_data_len + name_len + sizeof(*di) == item_size) {\n+\t\t\t/* No other xattrs packed in the same leaf item. */\n+\t\t\tif (size > old_data_len)\n+\t\t\t\tbtrfs_extend_item(root, path,\n+\t\t\t\t\t\t  size - old_data_len);\n+\t\t\telse if (size < old_data_len)\n+\t\t\t\tbtrfs_truncate_item(root, path, data_size, 1);\n+\t\t} else {\n+\t\t\t/* There are other xattrs packed in the same item. */\n+\t\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);\n+\t\t\tif (ret)\n+\t\t\t\tgoto out;\n+\t\t\tbtrfs_extend_item(root, path, data_size);\n+\t\t}\n \n+\t\titem = btrfs_item_nr(slot);\n+\t\tptr = btrfs_item_ptr(leaf, slot, char);\n+\t\tptr += btrfs_item_size(leaf, item) - data_size;\n+\t\tdi = (struct btrfs_dir_item *)ptr;\n+\t\tbtrfs_set_dir_data_len(leaf, di, size);\n+\t\tdata_ptr = ((unsigned long)(di + 1)) + name_len;\n+\t\twrite_extent_buffer(leaf, value, data_ptr, size);\n+\t\tbtrfs_mark_buffer_dirty(leaf);\n+\t} else {\n \t\t/*\n-\t\t * We have a value to set, so go back and try to insert it now.\n+\t\t * Insert, and we had space for the xattr, so path->slots[0] is\n+\t\t * where our xattr dir_item is and btrfs_insert_xattr_item()\n+\t\t * filled it.\n \t\t */\n-\t\tif (value) {\n-\t\t\tbtrfs_release_path(path);\n-\t\t\tgoto again;\n-\t\t}\n \t}\n out:\n \tbtrfs_free_path(path);",
        "function_modified_lines": {
            "added": [
                "\tstruct btrfs_dir_item *di = NULL;",
                "\tpath->skip_release_on_error = 1;",
                "\tif (!value) {",
                "\t\tdi = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode),",
                "\t\t\t\t\tname, name_len, -1);",
                "\t\tif (!di && (flags & XATTR_REPLACE))",
                "\t\t\tret = -ENODATA;",
                "\t\telse if (di)",
                "\t\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);",
                "\t\tgoto out;",
                "\t}",
                "",
                "\t/*",
                "\t * For a replace we can't just do the insert blindly.",
                "\t * Do a lookup first (read-only btrfs_search_slot), and return if xattr",
                "\t * doesn't exist. If it exists, fall down below to the insert/replace",
                "\t * path - we can't race with a concurrent xattr delete, because the VFS",
                "\t * locks the inode's i_mutex before calling setxattr or removexattr.",
                "\t */",
                "\t\tASSERT(mutex_is_locked(&inode->i_mutex));",
                "\t\tdi = btrfs_lookup_xattr(NULL, root, path, btrfs_ino(inode),",
                "\t\t\t\t\tname, name_len, 0);",
                "\t\tif (!di) {",
                "\t\tdi = NULL;",
                "\t}",
                "\tret = btrfs_insert_xattr_item(trans, root, path, btrfs_ino(inode),",
                "\t\t\t\t      name, name_len, value, size);",
                "\tif (ret == -EOVERFLOW) {",
                "\t\t * We have an existing item in a leaf, split_leaf couldn't",
                "\t\t * expand it. That item might have or not a dir_item that",
                "\t\t * matches our target xattr, so lets check.",
                "\t\tret = 0;",
                "\t\tbtrfs_assert_tree_locked(path->nodes[0]);",
                "\t\tdi = btrfs_match_dir_item_name(root, path, name, name_len);",
                "\t\tif (!di && !(flags & XATTR_REPLACE)) {",
                "\t\t\tret = -ENOSPC;",
                "\t} else if (ret == -EEXIST) {",
                "\t\tret = 0;",
                "\t\tdi = btrfs_match_dir_item_name(root, path, name, name_len);",
                "\t\tASSERT(di); /* logic error */",
                "\t} else if (ret) {",
                "\t\tgoto out;",
                "\tif (di && (flags & XATTR_CREATE)) {",
                "\t\tgoto out;",
                "\t}",
                "\tif (di) {",
                "\t\t * We're doing a replace, and it must be atomic, that is, at",
                "\t\t * any point in time we have either the old or the new xattr",
                "\t\t * value in the tree. We don't want readers (getxattr and",
                "\t\t * listxattrs) to miss a value, this is specially important",
                "\t\t * for ACLs.",
                "\t\tconst int slot = path->slots[0];",
                "\t\tstruct extent_buffer *leaf = path->nodes[0];",
                "\t\tconst u16 old_data_len = btrfs_dir_data_len(leaf, di);",
                "\t\tconst u32 item_size = btrfs_item_size_nr(leaf, slot);",
                "\t\tconst u32 data_size = sizeof(*di) + name_len + size;",
                "\t\tstruct btrfs_item *item;",
                "\t\tunsigned long data_ptr;",
                "\t\tchar *ptr;",
                "",
                "\t\tif (size > old_data_len) {",
                "\t\t\tif (btrfs_leaf_free_space(root, leaf) <",
                "\t\t\t    (size - old_data_len)) {",
                "\t\t\t\tret = -ENOSPC;",
                "\t\t\t\tgoto out;",
                "\t\t\t}",
                "\t\tif (old_data_len + name_len + sizeof(*di) == item_size) {",
                "\t\t\t/* No other xattrs packed in the same leaf item. */",
                "\t\t\tif (size > old_data_len)",
                "\t\t\t\tbtrfs_extend_item(root, path,",
                "\t\t\t\t\t\t  size - old_data_len);",
                "\t\t\telse if (size < old_data_len)",
                "\t\t\t\tbtrfs_truncate_item(root, path, data_size, 1);",
                "\t\t} else {",
                "\t\t\t/* There are other xattrs packed in the same item. */",
                "\t\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);",
                "\t\t\tif (ret)",
                "\t\t\t\tgoto out;",
                "\t\t\tbtrfs_extend_item(root, path, data_size);",
                "\t\t}",
                "\t\titem = btrfs_item_nr(slot);",
                "\t\tptr = btrfs_item_ptr(leaf, slot, char);",
                "\t\tptr += btrfs_item_size(leaf, item) - data_size;",
                "\t\tdi = (struct btrfs_dir_item *)ptr;",
                "\t\tbtrfs_set_dir_data_len(leaf, di, size);",
                "\t\tdata_ptr = ((unsigned long)(di + 1)) + name_len;",
                "\t\twrite_extent_buffer(leaf, value, data_ptr, size);",
                "\t\tbtrfs_mark_buffer_dirty(leaf);",
                "\t} else {",
                "\t\t * Insert, and we had space for the xattr, so path->slots[0] is",
                "\t\t * where our xattr dir_item is and btrfs_insert_xattr_item()",
                "\t\t * filled it."
            ],
            "deleted": [
                "\tstruct btrfs_dir_item *di;",
                "\t\tdi = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode), name,",
                "\t\t\t\t\tname_len, -1);",
                "\t\tif (IS_ERR(di)) {",
                "\t\t\tret = PTR_ERR(di);",
                "\t\t\tgoto out;",
                "\t\t} else if (!di) {",
                "\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);",
                "\t\tif (ret)",
                "\t\t\tgoto out;",
                "\t\t * remove the attribute",
                "\t\tif (!value)",
                "\t\t\tgoto out;",
                "\t} else {",
                "\t\tdi = btrfs_lookup_xattr(NULL, root, path, btrfs_ino(inode),",
                "\t\t\t\t\tname, name_len, 0);",
                "\t\tif (IS_ERR(di)) {",
                "\t\t\tret = PTR_ERR(di);",
                "\t\tif (!di && !value)",
                "\t\t\tgoto out;",
                "\t\tbtrfs_release_path(path);",
                "again:",
                "\tret = btrfs_insert_xattr_item(trans, root, path, btrfs_ino(inode),",
                "\t\t\t\t      name, name_len, value, size);",
                "\t/*",
                "\t * If we're setting an xattr to a new value but the new value is say",
                "\t * exactly BTRFS_MAX_XATTR_SIZE, we could end up with EOVERFLOW getting",
                "\t * back from split_leaf.  This is because it thinks we'll be extending",
                "\t * the existing item size, but we're asking for enough space to add the",
                "\t * item itself.  So if we get EOVERFLOW just set ret to EEXIST and let",
                "\t * the rest of the function figure it out.",
                "\t */",
                "\tif (ret == -EOVERFLOW)",
                "\tif (ret == -EEXIST) {",
                "\t\tif (flags & XATTR_CREATE)",
                "\t\t\tgoto out;",
                "\t\t * We can't use the path we already have since we won't have the",
                "\t\t * proper locking for a delete, so release the path and",
                "\t\t * re-lookup to delete the thing.",
                "\t\tbtrfs_release_path(path);",
                "\t\tdi = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode),",
                "\t\t\t\t\tname, name_len, -1);",
                "\t\tif (IS_ERR(di)) {",
                "\t\t\tret = PTR_ERR(di);",
                "\t\t\tgoto out;",
                "\t\t} else if (!di) {",
                "\t\t\t/* Shouldn't happen but just in case... */",
                "\t\t\tbtrfs_release_path(path);",
                "\t\t\tgoto again;",
                "\t\tret = btrfs_delete_one_dir_name(trans, root, path, di);",
                "\t\tif (ret)",
                "\t\t\tgoto out;",
                "\t\t * We have a value to set, so go back and try to insert it now.",
                "\t\tif (value) {",
                "\t\t\tbtrfs_release_path(path);",
                "\t\t\tgoto again;",
                "\t\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The Btrfs implementation in the Linux kernel before 3.19 does not ensure that the visible xattr state is consistent with a requested replacement, which allows local users to bypass intended ACL settings and gain privileges via standard filesystem operations (1) during an xattr-replacement time window, related to a race condition, or (2) after an xattr-replacement attempt that fails because the data does not fit."
    },
    {
        "cve_id": "CVE-2014-9914",
        "code_before_change": "void ip4_datagram_release_cb(struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\tif (! __sk_dst_get(sk) || __sk_dst_check(sk, 0))\n\t\treturn;\n\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trt = ip_route_output_ports(sock_net(sk), &fl4, sk, daddr,\n\t\t\t\t   inet->inet_saddr, inet->inet_dport,\n\t\t\t\t   inet->inet_sport, sk->sk_protocol,\n\t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt))\n\t\t__sk_dst_set(sk, &rt->dst);\n\trcu_read_unlock();\n}",
        "code_after_change": "void ip4_datagram_release_cb(struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct dst_entry *dst;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\trcu_read_lock();\n\n\tdst = __sk_dst_get(sk);\n\tif (!dst || !dst->obsolete || dst->ops->check(dst, 0)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trt = ip_route_output_ports(sock_net(sk), &fl4, sk, daddr,\n\t\t\t\t   inet->inet_saddr, inet->inet_dport,\n\t\t\t\t   inet->inet_sport, sk->sk_protocol,\n\t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n\n\tdst = !IS_ERR(rt) ? &rt->dst : NULL;\n\tsk_dst_set(sk, dst);\n\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,13 +3,17 @@\n \tconst struct inet_sock *inet = inet_sk(sk);\n \tconst struct ip_options_rcu *inet_opt;\n \t__be32 daddr = inet->inet_daddr;\n+\tstruct dst_entry *dst;\n \tstruct flowi4 fl4;\n \tstruct rtable *rt;\n \n-\tif (! __sk_dst_get(sk) || __sk_dst_check(sk, 0))\n+\trcu_read_lock();\n+\n+\tdst = __sk_dst_get(sk);\n+\tif (!dst || !dst->obsolete || dst->ops->check(dst, 0)) {\n+\t\trcu_read_unlock();\n \t\treturn;\n-\n-\trcu_read_lock();\n+\t}\n \tinet_opt = rcu_dereference(inet->inet_opt);\n \tif (inet_opt && inet_opt->opt.srr)\n \t\tdaddr = inet_opt->opt.faddr;\n@@ -17,7 +21,9 @@\n \t\t\t\t   inet->inet_saddr, inet->inet_dport,\n \t\t\t\t   inet->inet_sport, sk->sk_protocol,\n \t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n-\tif (!IS_ERR(rt))\n-\t\t__sk_dst_set(sk, &rt->dst);\n+\n+\tdst = !IS_ERR(rt) ? &rt->dst : NULL;\n+\tsk_dst_set(sk, dst);\n+\n \trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct dst_entry *dst;",
                "\trcu_read_lock();",
                "",
                "\tdst = __sk_dst_get(sk);",
                "\tif (!dst || !dst->obsolete || dst->ops->check(dst, 0)) {",
                "\t\trcu_read_unlock();",
                "\t}",
                "",
                "\tdst = !IS_ERR(rt) ? &rt->dst : NULL;",
                "\tsk_dst_set(sk, dst);",
                ""
            ],
            "deleted": [
                "\tif (! __sk_dst_get(sk) || __sk_dst_check(sk, 0))",
                "",
                "\trcu_read_lock();",
                "\tif (!IS_ERR(rt))",
                "\t\t__sk_dst_set(sk, &rt->dst);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ip4_datagram_release_cb function in net/ipv4/datagram.c in the Linux kernel before 3.15.2 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect expectations about locking during multithreaded access to internal data structures for IPv4 UDP sockets."
    },
    {
        "cve_id": "CVE-2015-1420",
        "code_before_change": "static int handle_to_path(int mountdirfd, struct file_handle __user *ufh,\n\t\t   struct path *path)\n{\n\tint retval = 0;\n\tstruct file_handle f_handle;\n\tstruct file_handle *handle = NULL;\n\n\t/*\n\t * With handle we don't look at the execute bit on the\n\t * the directory. Ideally we would like CAP_DAC_SEARCH.\n\t * But we don't have that\n\t */\n\tif (!capable(CAP_DAC_READ_SEARCH)) {\n\t\tretval = -EPERM;\n\t\tgoto out_err;\n\t}\n\tif (copy_from_user(&f_handle, ufh, sizeof(struct file_handle))) {\n\t\tretval = -EFAULT;\n\t\tgoto out_err;\n\t}\n\tif ((f_handle.handle_bytes > MAX_HANDLE_SZ) ||\n\t    (f_handle.handle_bytes == 0)) {\n\t\tretval = -EINVAL;\n\t\tgoto out_err;\n\t}\n\thandle = kmalloc(sizeof(struct file_handle) + f_handle.handle_bytes,\n\t\t\t GFP_KERNEL);\n\tif (!handle) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\t/* copy the full handle */\n\tif (copy_from_user(handle, ufh,\n\t\t\t   sizeof(struct file_handle) +\n\t\t\t   f_handle.handle_bytes)) {\n\t\tretval = -EFAULT;\n\t\tgoto out_handle;\n\t}\n\n\tretval = do_handle_to_path(mountdirfd, handle, path);\n\nout_handle:\n\tkfree(handle);\nout_err:\n\treturn retval;\n}",
        "code_after_change": "static int handle_to_path(int mountdirfd, struct file_handle __user *ufh,\n\t\t   struct path *path)\n{\n\tint retval = 0;\n\tstruct file_handle f_handle;\n\tstruct file_handle *handle = NULL;\n\n\t/*\n\t * With handle we don't look at the execute bit on the\n\t * the directory. Ideally we would like CAP_DAC_SEARCH.\n\t * But we don't have that\n\t */\n\tif (!capable(CAP_DAC_READ_SEARCH)) {\n\t\tretval = -EPERM;\n\t\tgoto out_err;\n\t}\n\tif (copy_from_user(&f_handle, ufh, sizeof(struct file_handle))) {\n\t\tretval = -EFAULT;\n\t\tgoto out_err;\n\t}\n\tif ((f_handle.handle_bytes > MAX_HANDLE_SZ) ||\n\t    (f_handle.handle_bytes == 0)) {\n\t\tretval = -EINVAL;\n\t\tgoto out_err;\n\t}\n\thandle = kmalloc(sizeof(struct file_handle) + f_handle.handle_bytes,\n\t\t\t GFP_KERNEL);\n\tif (!handle) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\t/* copy the full handle */\n\t*handle = f_handle;\n\tif (copy_from_user(&handle->f_handle,\n\t\t\t   &ufh->f_handle,\n\t\t\t   f_handle.handle_bytes)) {\n\t\tretval = -EFAULT;\n\t\tgoto out_handle;\n\t}\n\n\tretval = do_handle_to_path(mountdirfd, handle, path);\n\nout_handle:\n\tkfree(handle);\nout_err:\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,8 +30,9 @@\n \t\tgoto out_err;\n \t}\n \t/* copy the full handle */\n-\tif (copy_from_user(handle, ufh,\n-\t\t\t   sizeof(struct file_handle) +\n+\t*handle = f_handle;\n+\tif (copy_from_user(&handle->f_handle,\n+\t\t\t   &ufh->f_handle,\n \t\t\t   f_handle.handle_bytes)) {\n \t\tretval = -EFAULT;\n \t\tgoto out_handle;",
        "function_modified_lines": {
            "added": [
                "\t*handle = f_handle;",
                "\tif (copy_from_user(&handle->f_handle,",
                "\t\t\t   &ufh->f_handle,"
            ],
            "deleted": [
                "\tif (copy_from_user(handle, ufh,",
                "\t\t\t   sizeof(struct file_handle) +"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the handle_to_path function in fs/fhandle.c in the Linux kernel through 3.19.1 allows local users to bypass intended size restrictions and trigger read operations on additional memory locations by changing the handle_bytes value of a file handle during the execution of this function."
    },
    {
        "cve_id": "CVE-2015-3212",
        "code_before_change": "static void sctp_sock_migrate(struct sock *oldsk, struct sock *newsk,\n\t\t\t      struct sctp_association *assoc,\n\t\t\t      sctp_socket_type_t type)\n{\n\tstruct sctp_sock *oldsp = sctp_sk(oldsk);\n\tstruct sctp_sock *newsp = sctp_sk(newsk);\n\tstruct sctp_bind_bucket *pp; /* hash list port iterator */\n\tstruct sctp_endpoint *newep = newsp->ep;\n\tstruct sk_buff *skb, *tmp;\n\tstruct sctp_ulpevent *event;\n\tstruct sctp_bind_hashbucket *head;\n\tstruct list_head tmplist;\n\n\t/* Migrate socket buffer sizes and all the socket level options to the\n\t * new socket.\n\t */\n\tnewsk->sk_sndbuf = oldsk->sk_sndbuf;\n\tnewsk->sk_rcvbuf = oldsk->sk_rcvbuf;\n\t/* Brute force copy old sctp opt. */\n\tif (oldsp->do_auto_asconf) {\n\t\tmemcpy(&tmplist, &newsp->auto_asconf_list, sizeof(tmplist));\n\t\tinet_sk_copy_descendant(newsk, oldsk);\n\t\tmemcpy(&newsp->auto_asconf_list, &tmplist, sizeof(tmplist));\n\t} else\n\t\tinet_sk_copy_descendant(newsk, oldsk);\n\n\t/* Restore the ep value that was overwritten with the above structure\n\t * copy.\n\t */\n\tnewsp->ep = newep;\n\tnewsp->hmac = NULL;\n\n\t/* Hook this new socket in to the bind_hash list. */\n\thead = &sctp_port_hashtable[sctp_phashfn(sock_net(oldsk),\n\t\t\t\t\t\t inet_sk(oldsk)->inet_num)];\n\tlocal_bh_disable();\n\tspin_lock(&head->lock);\n\tpp = sctp_sk(oldsk)->bind_hash;\n\tsk_add_bind_node(newsk, &pp->owner);\n\tsctp_sk(newsk)->bind_hash = pp;\n\tinet_sk(newsk)->inet_num = inet_sk(oldsk)->inet_num;\n\tspin_unlock(&head->lock);\n\tlocal_bh_enable();\n\n\t/* Copy the bind_addr list from the original endpoint to the new\n\t * endpoint so that we can handle restarts properly\n\t */\n\tsctp_bind_addr_dup(&newsp->ep->base.bind_addr,\n\t\t\t\t&oldsp->ep->base.bind_addr, GFP_KERNEL);\n\n\t/* Move any messages in the old socket's receive queue that are for the\n\t * peeled off association to the new socket's receive queue.\n\t */\n\tsctp_skb_for_each(skb, &oldsk->sk_receive_queue, tmp) {\n\t\tevent = sctp_skb2event(skb);\n\t\tif (event->asoc == assoc) {\n\t\t\t__skb_unlink(skb, &oldsk->sk_receive_queue);\n\t\t\t__skb_queue_tail(&newsk->sk_receive_queue, skb);\n\t\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\t\t}\n\t}\n\n\t/* Clean up any messages pending delivery due to partial\n\t * delivery.   Three cases:\n\t * 1) No partial deliver;  no work.\n\t * 2) Peeling off partial delivery; keep pd_lobby in new pd_lobby.\n\t * 3) Peeling off non-partial delivery; move pd_lobby to receive_queue.\n\t */\n\tskb_queue_head_init(&newsp->pd_lobby);\n\tatomic_set(&sctp_sk(newsk)->pd_mode, assoc->ulpq.pd_mode);\n\n\tif (atomic_read(&sctp_sk(oldsk)->pd_mode)) {\n\t\tstruct sk_buff_head *queue;\n\n\t\t/* Decide which queue to move pd_lobby skbs to. */\n\t\tif (assoc->ulpq.pd_mode) {\n\t\t\tqueue = &newsp->pd_lobby;\n\t\t} else\n\t\t\tqueue = &newsk->sk_receive_queue;\n\n\t\t/* Walk through the pd_lobby, looking for skbs that\n\t\t * need moved to the new socket.\n\t\t */\n\t\tsctp_skb_for_each(skb, &oldsp->pd_lobby, tmp) {\n\t\t\tevent = sctp_skb2event(skb);\n\t\t\tif (event->asoc == assoc) {\n\t\t\t\t__skb_unlink(skb, &oldsp->pd_lobby);\n\t\t\t\t__skb_queue_tail(queue, skb);\n\t\t\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\t\t\t}\n\t\t}\n\n\t\t/* Clear up any skbs waiting for the partial\n\t\t * delivery to finish.\n\t\t */\n\t\tif (assoc->ulpq.pd_mode)\n\t\t\tsctp_clear_pd(oldsk, NULL);\n\n\t}\n\n\tsctp_skb_for_each(skb, &assoc->ulpq.reasm, tmp)\n\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\n\tsctp_skb_for_each(skb, &assoc->ulpq.lobby, tmp)\n\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\n\t/* Set the type of socket to indicate that it is peeled off from the\n\t * original UDP-style socket or created with the accept() call on a\n\t * TCP-style socket..\n\t */\n\tnewsp->type = type;\n\n\t/* Mark the new socket \"in-use\" by the user so that any packets\n\t * that may arrive on the association after we've moved it are\n\t * queued to the backlog.  This prevents a potential race between\n\t * backlog processing on the old socket and new-packet processing\n\t * on the new socket.\n\t *\n\t * The caller has just allocated newsk so we can guarantee that other\n\t * paths won't try to lock it and then oldsk.\n\t */\n\tlock_sock_nested(newsk, SINGLE_DEPTH_NESTING);\n\tsctp_assoc_migrate(assoc, newsk);\n\n\t/* If the association on the newsk is already closed before accept()\n\t * is called, set RCV_SHUTDOWN flag.\n\t */\n\tif (sctp_state(assoc, CLOSED) && sctp_style(newsk, TCP))\n\t\tnewsk->sk_shutdown |= RCV_SHUTDOWN;\n\n\tnewsk->sk_state = SCTP_SS_ESTABLISHED;\n\trelease_sock(newsk);\n}",
        "code_after_change": "static void sctp_sock_migrate(struct sock *oldsk, struct sock *newsk,\n\t\t\t      struct sctp_association *assoc,\n\t\t\t      sctp_socket_type_t type)\n{\n\tstruct sctp_sock *oldsp = sctp_sk(oldsk);\n\tstruct sctp_sock *newsp = sctp_sk(newsk);\n\tstruct sctp_bind_bucket *pp; /* hash list port iterator */\n\tstruct sctp_endpoint *newep = newsp->ep;\n\tstruct sk_buff *skb, *tmp;\n\tstruct sctp_ulpevent *event;\n\tstruct sctp_bind_hashbucket *head;\n\n\t/* Migrate socket buffer sizes and all the socket level options to the\n\t * new socket.\n\t */\n\tnewsk->sk_sndbuf = oldsk->sk_sndbuf;\n\tnewsk->sk_rcvbuf = oldsk->sk_rcvbuf;\n\t/* Brute force copy old sctp opt. */\n\tsctp_copy_descendant(newsk, oldsk);\n\n\t/* Restore the ep value that was overwritten with the above structure\n\t * copy.\n\t */\n\tnewsp->ep = newep;\n\tnewsp->hmac = NULL;\n\n\t/* Hook this new socket in to the bind_hash list. */\n\thead = &sctp_port_hashtable[sctp_phashfn(sock_net(oldsk),\n\t\t\t\t\t\t inet_sk(oldsk)->inet_num)];\n\tlocal_bh_disable();\n\tspin_lock(&head->lock);\n\tpp = sctp_sk(oldsk)->bind_hash;\n\tsk_add_bind_node(newsk, &pp->owner);\n\tsctp_sk(newsk)->bind_hash = pp;\n\tinet_sk(newsk)->inet_num = inet_sk(oldsk)->inet_num;\n\tspin_unlock(&head->lock);\n\tlocal_bh_enable();\n\n\t/* Copy the bind_addr list from the original endpoint to the new\n\t * endpoint so that we can handle restarts properly\n\t */\n\tsctp_bind_addr_dup(&newsp->ep->base.bind_addr,\n\t\t\t\t&oldsp->ep->base.bind_addr, GFP_KERNEL);\n\n\t/* Move any messages in the old socket's receive queue that are for the\n\t * peeled off association to the new socket's receive queue.\n\t */\n\tsctp_skb_for_each(skb, &oldsk->sk_receive_queue, tmp) {\n\t\tevent = sctp_skb2event(skb);\n\t\tif (event->asoc == assoc) {\n\t\t\t__skb_unlink(skb, &oldsk->sk_receive_queue);\n\t\t\t__skb_queue_tail(&newsk->sk_receive_queue, skb);\n\t\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\t\t}\n\t}\n\n\t/* Clean up any messages pending delivery due to partial\n\t * delivery.   Three cases:\n\t * 1) No partial deliver;  no work.\n\t * 2) Peeling off partial delivery; keep pd_lobby in new pd_lobby.\n\t * 3) Peeling off non-partial delivery; move pd_lobby to receive_queue.\n\t */\n\tskb_queue_head_init(&newsp->pd_lobby);\n\tatomic_set(&sctp_sk(newsk)->pd_mode, assoc->ulpq.pd_mode);\n\n\tif (atomic_read(&sctp_sk(oldsk)->pd_mode)) {\n\t\tstruct sk_buff_head *queue;\n\n\t\t/* Decide which queue to move pd_lobby skbs to. */\n\t\tif (assoc->ulpq.pd_mode) {\n\t\t\tqueue = &newsp->pd_lobby;\n\t\t} else\n\t\t\tqueue = &newsk->sk_receive_queue;\n\n\t\t/* Walk through the pd_lobby, looking for skbs that\n\t\t * need moved to the new socket.\n\t\t */\n\t\tsctp_skb_for_each(skb, &oldsp->pd_lobby, tmp) {\n\t\t\tevent = sctp_skb2event(skb);\n\t\t\tif (event->asoc == assoc) {\n\t\t\t\t__skb_unlink(skb, &oldsp->pd_lobby);\n\t\t\t\t__skb_queue_tail(queue, skb);\n\t\t\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\t\t\t}\n\t\t}\n\n\t\t/* Clear up any skbs waiting for the partial\n\t\t * delivery to finish.\n\t\t */\n\t\tif (assoc->ulpq.pd_mode)\n\t\t\tsctp_clear_pd(oldsk, NULL);\n\n\t}\n\n\tsctp_skb_for_each(skb, &assoc->ulpq.reasm, tmp)\n\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\n\tsctp_skb_for_each(skb, &assoc->ulpq.lobby, tmp)\n\t\tsctp_skb_set_owner_r_frag(skb, newsk);\n\n\t/* Set the type of socket to indicate that it is peeled off from the\n\t * original UDP-style socket or created with the accept() call on a\n\t * TCP-style socket..\n\t */\n\tnewsp->type = type;\n\n\t/* Mark the new socket \"in-use\" by the user so that any packets\n\t * that may arrive on the association after we've moved it are\n\t * queued to the backlog.  This prevents a potential race between\n\t * backlog processing on the old socket and new-packet processing\n\t * on the new socket.\n\t *\n\t * The caller has just allocated newsk so we can guarantee that other\n\t * paths won't try to lock it and then oldsk.\n\t */\n\tlock_sock_nested(newsk, SINGLE_DEPTH_NESTING);\n\tsctp_assoc_migrate(assoc, newsk);\n\n\t/* If the association on the newsk is already closed before accept()\n\t * is called, set RCV_SHUTDOWN flag.\n\t */\n\tif (sctp_state(assoc, CLOSED) && sctp_style(newsk, TCP))\n\t\tnewsk->sk_shutdown |= RCV_SHUTDOWN;\n\n\tnewsk->sk_state = SCTP_SS_ESTABLISHED;\n\trelease_sock(newsk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,6 @@\n \tstruct sk_buff *skb, *tmp;\n \tstruct sctp_ulpevent *event;\n \tstruct sctp_bind_hashbucket *head;\n-\tstruct list_head tmplist;\n \n \t/* Migrate socket buffer sizes and all the socket level options to the\n \t * new socket.\n@@ -17,12 +16,7 @@\n \tnewsk->sk_sndbuf = oldsk->sk_sndbuf;\n \tnewsk->sk_rcvbuf = oldsk->sk_rcvbuf;\n \t/* Brute force copy old sctp opt. */\n-\tif (oldsp->do_auto_asconf) {\n-\t\tmemcpy(&tmplist, &newsp->auto_asconf_list, sizeof(tmplist));\n-\t\tinet_sk_copy_descendant(newsk, oldsk);\n-\t\tmemcpy(&newsp->auto_asconf_list, &tmplist, sizeof(tmplist));\n-\t} else\n-\t\tinet_sk_copy_descendant(newsk, oldsk);\n+\tsctp_copy_descendant(newsk, oldsk);\n \n \t/* Restore the ep value that was overwritten with the above structure\n \t * copy.",
        "function_modified_lines": {
            "added": [
                "\tsctp_copy_descendant(newsk, oldsk);"
            ],
            "deleted": [
                "\tstruct list_head tmplist;",
                "\tif (oldsp->do_auto_asconf) {",
                "\t\tmemcpy(&tmplist, &newsp->auto_asconf_list, sizeof(tmplist));",
                "\t\tinet_sk_copy_descendant(newsk, oldsk);",
                "\t\tmemcpy(&newsp->auto_asconf_list, &tmplist, sizeof(tmplist));",
                "\t} else",
                "\t\tinet_sk_copy_descendant(newsk, oldsk);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in net/sctp/socket.c in the Linux kernel before 4.1.2 allows local users to cause a denial of service (list corruption and panic) via a rapid series of system calls related to sockets, as demonstrated by setsockopt calls."
    },
    {
        "cve_id": "CVE-2015-3212",
        "code_before_change": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tmemset(&sp->subscribe, 0, sizeof(struct sctp_event_subscribe));\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tpercpu_counter_inc(&sctp_sockets_allocated);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\tif (net->sctp.default_auto_asconf) {\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t} else\n\t\tsp->do_auto_asconf = 0;\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
        "code_after_change": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tmemset(&sp->subscribe, 0, sizeof(struct sctp_event_subscribe));\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tpercpu_counter_inc(&sctp_sockets_allocated);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\n\t/* Nothing can fail after this block, otherwise\n\t * sctp_destroy_sock() will be called without addr_wq_lock held\n\t */\n\tif (net->sctp.default_auto_asconf) {\n\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);\n\t} else {\n\t\tsp->do_auto_asconf = 0;\n\t}\n\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -125,12 +125,20 @@\n \tlocal_bh_disable();\n \tpercpu_counter_inc(&sctp_sockets_allocated);\n \tsock_prot_inuse_add(net, sk->sk_prot, 1);\n+\n+\t/* Nothing can fail after this block, otherwise\n+\t * sctp_destroy_sock() will be called without addr_wq_lock held\n+\t */\n \tif (net->sctp.default_auto_asconf) {\n+\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n \t\tlist_add_tail(&sp->auto_asconf_list,\n \t\t    &net->sctp.auto_asconf_splist);\n \t\tsp->do_auto_asconf = 1;\n-\t} else\n+\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);\n+\t} else {\n \t\tsp->do_auto_asconf = 0;\n+\t}\n+\n \tlocal_bh_enable();\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* Nothing can fail after this block, otherwise",
                "\t * sctp_destroy_sock() will be called without addr_wq_lock held",
                "\t */",
                "\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);",
                "\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);",
                "\t} else {",
                "\t}",
                ""
            ],
            "deleted": [
                "\t} else"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in net/sctp/socket.c in the Linux kernel before 4.1.2 allows local users to cause a denial of service (list corruption and panic) via a rapid series of system calls related to sockets, as demonstrated by setsockopt calls."
    },
    {
        "cve_id": "CVE-2015-3212",
        "code_before_change": "static int sctp_setsockopt_auto_asconf(struct sock *sk, char __user *optval,\n\t\t\t\t\tunsigned int optlen)\n{\n\tint val;\n\tstruct sctp_sock *sp = sctp_sk(sk);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\tif (!sctp_is_ep_boundall(sk) && val)\n\t\treturn -EINVAL;\n\tif ((val && sp->do_auto_asconf) || (!val && !sp->do_auto_asconf))\n\t\treturn 0;\n\n\tif (val == 0 && sp->do_auto_asconf) {\n\t\tlist_del(&sp->auto_asconf_list);\n\t\tsp->do_auto_asconf = 0;\n\t} else if (val && !sp->do_auto_asconf) {\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &sock_net(sk)->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int sctp_setsockopt_auto_asconf(struct sock *sk, char __user *optval,\n\t\t\t\t\tunsigned int optlen)\n{\n\tint val;\n\tstruct sctp_sock *sp = sctp_sk(sk);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\tif (!sctp_is_ep_boundall(sk) && val)\n\t\treturn -EINVAL;\n\tif ((val && sp->do_auto_asconf) || (!val && !sp->do_auto_asconf))\n\t\treturn 0;\n\n\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n\tif (val == 0 && sp->do_auto_asconf) {\n\t\tlist_del(&sp->auto_asconf_list);\n\t\tsp->do_auto_asconf = 0;\n\t} else if (val && !sp->do_auto_asconf) {\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &sock_net(sk)->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t}\n\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,7 @@\n \tif ((val && sp->do_auto_asconf) || (!val && !sp->do_auto_asconf))\n \t\treturn 0;\n \n+\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n \tif (val == 0 && sp->do_auto_asconf) {\n \t\tlist_del(&sp->auto_asconf_list);\n \t\tsp->do_auto_asconf = 0;\n@@ -21,5 +22,6 @@\n \t\t    &sock_net(sk)->sctp.auto_asconf_splist);\n \t\tsp->do_auto_asconf = 1;\n \t}\n+\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);",
                "\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in net/sctp/socket.c in the Linux kernel before 4.1.2 allows local users to cause a denial of service (list corruption and panic) via a rapid series of system calls related to sockets, as demonstrated by setsockopt calls."
    },
    {
        "cve_id": "CVE-2015-3212",
        "code_before_change": "static void sctp_close(struct sock *sk, long timeout)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_association *asoc;\n\tstruct list_head *pos, *temp;\n\tunsigned int data_was_unread;\n\n\tpr_debug(\"%s: sk:%p, timeout:%ld\\n\", __func__, sk, timeout);\n\n\tlock_sock(sk);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\tsk->sk_state = SCTP_SS_CLOSING;\n\n\tep = sctp_sk(sk)->ep;\n\n\t/* Clean up any skbs sitting on the receive queue.  */\n\tdata_was_unread = sctp_queue_purge_ulpevents(&sk->sk_receive_queue);\n\tdata_was_unread += sctp_queue_purge_ulpevents(&sctp_sk(sk)->pd_lobby);\n\n\t/* Walk all associations on an endpoint.  */\n\tlist_for_each_safe(pos, temp, &ep->asocs) {\n\t\tasoc = list_entry(pos, struct sctp_association, asocs);\n\n\t\tif (sctp_style(sk, TCP)) {\n\t\t\t/* A closed association can still be in the list if\n\t\t\t * it belongs to a TCP-style listening socket that is\n\t\t\t * not yet accepted. If so, free it. If not, send an\n\t\t\t * ABORT or SHUTDOWN based on the linger options.\n\t\t\t */\n\t\t\tif (sctp_state(asoc, CLOSED)) {\n\t\t\t\tsctp_unhash_established(asoc);\n\t\t\t\tsctp_association_free(asoc);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (data_was_unread || !skb_queue_empty(&asoc->ulpq.lobby) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm) ||\n\t\t    (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime)) {\n\t\t\tstruct sctp_chunk *chunk;\n\n\t\t\tchunk = sctp_make_abort_user(asoc, NULL, 0);\n\t\t\tif (chunk)\n\t\t\t\tsctp_primitive_ABORT(net, asoc, chunk);\n\t\t} else\n\t\t\tsctp_primitive_SHUTDOWN(net, asoc, NULL);\n\t}\n\n\t/* On a TCP-style socket, block for at most linger_time if set. */\n\tif (sctp_style(sk, TCP) && timeout)\n\t\tsctp_wait_for_close(sk, timeout);\n\n\t/* This will run the backlog queue.  */\n\trelease_sock(sk);\n\n\t/* Supposedly, no process has access to the socket, but\n\t * the net layers still may.\n\t */\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\n\t/* Hold the sock, since sk_common_release() will put sock_put()\n\t * and we have just a little more cleanup.\n\t */\n\tsock_hold(sk);\n\tsk_common_release(sk);\n\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\n\tsock_put(sk);\n\n\tSCTP_DBG_OBJCNT_DEC(sock);\n}",
        "code_after_change": "static void sctp_close(struct sock *sk, long timeout)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_association *asoc;\n\tstruct list_head *pos, *temp;\n\tunsigned int data_was_unread;\n\n\tpr_debug(\"%s: sk:%p, timeout:%ld\\n\", __func__, sk, timeout);\n\n\tlock_sock(sk);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\tsk->sk_state = SCTP_SS_CLOSING;\n\n\tep = sctp_sk(sk)->ep;\n\n\t/* Clean up any skbs sitting on the receive queue.  */\n\tdata_was_unread = sctp_queue_purge_ulpevents(&sk->sk_receive_queue);\n\tdata_was_unread += sctp_queue_purge_ulpevents(&sctp_sk(sk)->pd_lobby);\n\n\t/* Walk all associations on an endpoint.  */\n\tlist_for_each_safe(pos, temp, &ep->asocs) {\n\t\tasoc = list_entry(pos, struct sctp_association, asocs);\n\n\t\tif (sctp_style(sk, TCP)) {\n\t\t\t/* A closed association can still be in the list if\n\t\t\t * it belongs to a TCP-style listening socket that is\n\t\t\t * not yet accepted. If so, free it. If not, send an\n\t\t\t * ABORT or SHUTDOWN based on the linger options.\n\t\t\t */\n\t\t\tif (sctp_state(asoc, CLOSED)) {\n\t\t\t\tsctp_unhash_established(asoc);\n\t\t\t\tsctp_association_free(asoc);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (data_was_unread || !skb_queue_empty(&asoc->ulpq.lobby) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm) ||\n\t\t    (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime)) {\n\t\t\tstruct sctp_chunk *chunk;\n\n\t\t\tchunk = sctp_make_abort_user(asoc, NULL, 0);\n\t\t\tif (chunk)\n\t\t\t\tsctp_primitive_ABORT(net, asoc, chunk);\n\t\t} else\n\t\t\tsctp_primitive_SHUTDOWN(net, asoc, NULL);\n\t}\n\n\t/* On a TCP-style socket, block for at most linger_time if set. */\n\tif (sctp_style(sk, TCP) && timeout)\n\t\tsctp_wait_for_close(sk, timeout);\n\n\t/* This will run the backlog queue.  */\n\trelease_sock(sk);\n\n\t/* Supposedly, no process has access to the socket, but\n\t * the net layers still may.\n\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock\n\t * held and that should be grabbed before socket lock.\n\t */\n\tspin_lock_bh(&net->sctp.addr_wq_lock);\n\tbh_lock_sock(sk);\n\n\t/* Hold the sock, since sk_common_release() will put sock_put()\n\t * and we have just a little more cleanup.\n\t */\n\tsock_hold(sk);\n\tsk_common_release(sk);\n\n\tbh_unlock_sock(sk);\n\tspin_unlock_bh(&net->sctp.addr_wq_lock);\n\n\tsock_put(sk);\n\n\tSCTP_DBG_OBJCNT_DEC(sock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -56,8 +56,10 @@\n \n \t/* Supposedly, no process has access to the socket, but\n \t * the net layers still may.\n+\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock\n+\t * held and that should be grabbed before socket lock.\n \t */\n-\tlocal_bh_disable();\n+\tspin_lock_bh(&net->sctp.addr_wq_lock);\n \tbh_lock_sock(sk);\n \n \t/* Hold the sock, since sk_common_release() will put sock_put()\n@@ -67,7 +69,7 @@\n \tsk_common_release(sk);\n \n \tbh_unlock_sock(sk);\n-\tlocal_bh_enable();\n+\tspin_unlock_bh(&net->sctp.addr_wq_lock);\n \n \tsock_put(sk);\n ",
        "function_modified_lines": {
            "added": [
                "\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock",
                "\t * held and that should be grabbed before socket lock.",
                "\tspin_lock_bh(&net->sctp.addr_wq_lock);",
                "\tspin_unlock_bh(&net->sctp.addr_wq_lock);"
            ],
            "deleted": [
                "\tlocal_bh_disable();",
                "\tlocal_bh_enable();"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in net/sctp/socket.c in the Linux kernel before 4.1.2 allows local users to cause a denial of service (list corruption and panic) via a rapid series of system calls related to sockets, as demonstrated by setsockopt calls."
    },
    {
        "cve_id": "CVE-2015-3339",
        "code_before_change": "int prepare_binprm(struct linux_binprm *bprm)\n{\n\tstruct inode *inode = file_inode(bprm->file);\n\tumode_t mode = inode->i_mode;\n\tint retval;\n\n\n\t/* clear any previous set[ug]id data from a previous binary */\n\tbprm->cred->euid = current_euid();\n\tbprm->cred->egid = current_egid();\n\n\tif (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&\n\t    !task_no_new_privs(current) &&\n\t    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&\n\t    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {\n\t\t/* Set-uid? */\n\t\tif (mode & S_ISUID) {\n\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\t\tbprm->cred->euid = inode->i_uid;\n\t\t}\n\n\t\t/* Set-gid? */\n\t\t/*\n\t\t * If setgid is set but no group execute bit then this\n\t\t * is a candidate for mandatory locking, not a setgid\n\t\t * executable.\n\t\t */\n\t\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n\t\t\tbprm->cred->egid = inode->i_gid;\n\t\t}\n\t}\n\n\t/* fill in binprm security blob */\n\tretval = security_bprm_set_creds(bprm);\n\tif (retval)\n\t\treturn retval;\n\tbprm->cred_prepared = 1;\n\n\tmemset(bprm->buf, 0, BINPRM_BUF_SIZE);\n\treturn kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);\n}",
        "code_after_change": "int prepare_binprm(struct linux_binprm *bprm)\n{\n\tint retval;\n\n\tbprm_fill_uid(bprm);\n\n\t/* fill in binprm security blob */\n\tretval = security_bprm_set_creds(bprm);\n\tif (retval)\n\t\treturn retval;\n\tbprm->cred_prepared = 1;\n\n\tmemset(bprm->buf, 0, BINPRM_BUF_SIZE);\n\treturn kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,35 +1,8 @@\n int prepare_binprm(struct linux_binprm *bprm)\n {\n-\tstruct inode *inode = file_inode(bprm->file);\n-\tumode_t mode = inode->i_mode;\n \tint retval;\n \n-\n-\t/* clear any previous set[ug]id data from a previous binary */\n-\tbprm->cred->euid = current_euid();\n-\tbprm->cred->egid = current_egid();\n-\n-\tif (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&\n-\t    !task_no_new_privs(current) &&\n-\t    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&\n-\t    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {\n-\t\t/* Set-uid? */\n-\t\tif (mode & S_ISUID) {\n-\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n-\t\t\tbprm->cred->euid = inode->i_uid;\n-\t\t}\n-\n-\t\t/* Set-gid? */\n-\t\t/*\n-\t\t * If setgid is set but no group execute bit then this\n-\t\t * is a candidate for mandatory locking, not a setgid\n-\t\t * executable.\n-\t\t */\n-\t\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n-\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;\n-\t\t\tbprm->cred->egid = inode->i_gid;\n-\t\t}\n-\t}\n+\tbprm_fill_uid(bprm);\n \n \t/* fill in binprm security blob */\n \tretval = security_bprm_set_creds(bprm);",
        "function_modified_lines": {
            "added": [
                "\tbprm_fill_uid(bprm);"
            ],
            "deleted": [
                "\tstruct inode *inode = file_inode(bprm->file);",
                "\tumode_t mode = inode->i_mode;",
                "",
                "\t/* clear any previous set[ug]id data from a previous binary */",
                "\tbprm->cred->euid = current_euid();",
                "\tbprm->cred->egid = current_egid();",
                "",
                "\tif (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&",
                "\t    !task_no_new_privs(current) &&",
                "\t    kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&",
                "\t    kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {",
                "\t\t/* Set-uid? */",
                "\t\tif (mode & S_ISUID) {",
                "\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;",
                "\t\t\tbprm->cred->euid = inode->i_uid;",
                "\t\t}",
                "",
                "\t\t/* Set-gid? */",
                "\t\t/*",
                "\t\t * If setgid is set but no group execute bit then this",
                "\t\t * is a candidate for mandatory locking, not a setgid",
                "\t\t * executable.",
                "\t\t */",
                "\t\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {",
                "\t\t\tbprm->per_clear |= PER_CLEAR_ON_SETID;",
                "\t\t\tbprm->cred->egid = inode->i_gid;",
                "\t\t}",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the prepare_binprm function in fs/exec.c in the Linux kernel before 3.19.6 allows local users to gain privileges by executing a setuid program at a time instant when a chown to root is in progress, and the ownership is changed but the setuid bit is not yet stripped."
    },
    {
        "cve_id": "CVE-2015-4170",
        "code_before_change": "static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n{\n\tlong tmp = *old;\n\t*old = atomic_long_cmpxchg(&sem->count, *old, new);\n\treturn *old == tmp;\n}",
        "code_after_change": "static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n{\n\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);\n\tif (tmp == *old) {\n\t\t*old = new;\n\t\treturn 1;\n\t} else {\n\t\t*old = tmp;\n\t\treturn 0;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,11 @@\n static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n {\n-\tlong tmp = *old;\n-\t*old = atomic_long_cmpxchg(&sem->count, *old, new);\n-\treturn *old == tmp;\n+\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);\n+\tif (tmp == *old) {\n+\t\t*old = new;\n+\t\treturn 1;\n+\t} else {\n+\t\t*old = tmp;\n+\t\treturn 0;\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);",
                "\tif (tmp == *old) {",
                "\t\t*old = new;",
                "\t\treturn 1;",
                "\t} else {",
                "\t\t*old = tmp;",
                "\t\treturn 0;",
                "\t}"
            ],
            "deleted": [
                "\tlong tmp = *old;",
                "\t*old = atomic_long_cmpxchg(&sem->count, *old, new);",
                "\treturn *old == tmp;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ldsem_cmpxchg function in drivers/tty/tty_ldsem.c in the Linux kernel before 3.13-rc4-next-20131218 allows local users to cause a denial of service (ldsem_down_read and ldsem_down_write deadlock) by establishing a new tty thread during shutdown of a previous tty thread."
    },
    {
        "cve_id": "CVE-2015-7550",
        "code_before_change": "long keyctl_read_key(key_serial_t keyid, char __user *buffer, size_t buflen)\n{\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tlong ret;\n\n\t/* find the key first */\n\tkey_ref = lookup_user_key(keyid, 0, 0);\n\tif (IS_ERR(key_ref)) {\n\t\tret = -ENOKEY;\n\t\tgoto error;\n\t}\n\n\tkey = key_ref_to_ptr(key_ref);\n\n\t/* see if we can read it directly */\n\tret = key_permission(key_ref, KEY_NEED_READ);\n\tif (ret == 0)\n\t\tgoto can_read_key;\n\tif (ret != -EACCES)\n\t\tgoto error;\n\n\t/* we can't; see if it's searchable from this process's keyrings\n\t * - we automatically take account of the fact that it may be\n\t *   dangling off an instantiation key\n\t */\n\tif (!is_key_possessed(key_ref)) {\n\t\tret = -EACCES;\n\t\tgoto error2;\n\t}\n\n\t/* the key is probably readable - now try to read it */\ncan_read_key:\n\tret = key_validate(key);\n\tif (ret == 0) {\n\t\tret = -EOPNOTSUPP;\n\t\tif (key->type->read) {\n\t\t\t/* read the data with the semaphore held (since we\n\t\t\t * might sleep) */\n\t\t\tdown_read(&key->sem);\n\t\t\tret = key->type->read(key, buffer, buflen);\n\t\t\tup_read(&key->sem);\n\t\t}\n\t}\n\nerror2:\n\tkey_put(key);\nerror:\n\treturn ret;\n}",
        "code_after_change": "long keyctl_read_key(key_serial_t keyid, char __user *buffer, size_t buflen)\n{\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tlong ret;\n\n\t/* find the key first */\n\tkey_ref = lookup_user_key(keyid, 0, 0);\n\tif (IS_ERR(key_ref)) {\n\t\tret = -ENOKEY;\n\t\tgoto error;\n\t}\n\n\tkey = key_ref_to_ptr(key_ref);\n\n\t/* see if we can read it directly */\n\tret = key_permission(key_ref, KEY_NEED_READ);\n\tif (ret == 0)\n\t\tgoto can_read_key;\n\tif (ret != -EACCES)\n\t\tgoto error;\n\n\t/* we can't; see if it's searchable from this process's keyrings\n\t * - we automatically take account of the fact that it may be\n\t *   dangling off an instantiation key\n\t */\n\tif (!is_key_possessed(key_ref)) {\n\t\tret = -EACCES;\n\t\tgoto error2;\n\t}\n\n\t/* the key is probably readable - now try to read it */\ncan_read_key:\n\tret = -EOPNOTSUPP;\n\tif (key->type->read) {\n\t\t/* Read the data with the semaphore held (since we might sleep)\n\t\t * to protect against the key being updated or revoked.\n\t\t */\n\t\tdown_read(&key->sem);\n\t\tret = key_validate(key);\n\t\tif (ret == 0)\n\t\t\tret = key->type->read(key, buffer, buflen);\n\t\tup_read(&key->sem);\n\t}\n\nerror2:\n\tkey_put(key);\nerror:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,16 +31,16 @@\n \n \t/* the key is probably readable - now try to read it */\n can_read_key:\n-\tret = key_validate(key);\n-\tif (ret == 0) {\n-\t\tret = -EOPNOTSUPP;\n-\t\tif (key->type->read) {\n-\t\t\t/* read the data with the semaphore held (since we\n-\t\t\t * might sleep) */\n-\t\t\tdown_read(&key->sem);\n+\tret = -EOPNOTSUPP;\n+\tif (key->type->read) {\n+\t\t/* Read the data with the semaphore held (since we might sleep)\n+\t\t * to protect against the key being updated or revoked.\n+\t\t */\n+\t\tdown_read(&key->sem);\n+\t\tret = key_validate(key);\n+\t\tif (ret == 0)\n \t\t\tret = key->type->read(key, buffer, buflen);\n-\t\t\tup_read(&key->sem);\n-\t\t}\n+\t\tup_read(&key->sem);\n \t}\n \n error2:",
        "function_modified_lines": {
            "added": [
                "\tret = -EOPNOTSUPP;",
                "\tif (key->type->read) {",
                "\t\t/* Read the data with the semaphore held (since we might sleep)",
                "\t\t * to protect against the key being updated or revoked.",
                "\t\t */",
                "\t\tdown_read(&key->sem);",
                "\t\tret = key_validate(key);",
                "\t\tif (ret == 0)",
                "\t\tup_read(&key->sem);"
            ],
            "deleted": [
                "\tret = key_validate(key);",
                "\tif (ret == 0) {",
                "\t\tret = -EOPNOTSUPP;",
                "\t\tif (key->type->read) {",
                "\t\t\t/* read the data with the semaphore held (since we",
                "\t\t\t * might sleep) */",
                "\t\t\tdown_read(&key->sem);",
                "\t\t\tup_read(&key->sem);",
                "\t\t}"
            ]
        },
        "cwe": [
            "CWE-362",
            "NVD-CWE-Other"
        ],
        "cve_description": "The keyctl_read_key function in security/keys/keyctl.c in the Linux kernel before 4.3.4 does not properly use a semaphore, which allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact via a crafted application that leverages a race condition between keyctl_revoke and keyctl_read calls."
    },
    {
        "cve_id": "CVE-2015-7613",
        "code_before_change": "static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tstruct msg_queue *msq;\n\tint id, retval;\n\tkey_t key = params->key;\n\tint msgflg = params->flg;\n\n\tmsq = ipc_rcu_alloc(sizeof(*msq));\n\tif (!msq)\n\t\treturn -ENOMEM;\n\n\tmsq->q_perm.mode = msgflg & S_IRWXUGO;\n\tmsq->q_perm.key = key;\n\n\tmsq->q_perm.security = NULL;\n\tretval = security_msg_queue_alloc(msq);\n\tif (retval) {\n\t\tipc_rcu_putref(msq, ipc_rcu_free);\n\t\treturn retval;\n\t}\n\n\t/* ipc_addid() locks msq upon success. */\n\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n\tif (id < 0) {\n\t\tipc_rcu_putref(msq, msg_rcu_free);\n\t\treturn id;\n\t}\n\n\tmsq->q_stime = msq->q_rtime = 0;\n\tmsq->q_ctime = get_seconds();\n\tmsq->q_cbytes = msq->q_qnum = 0;\n\tmsq->q_qbytes = ns->msg_ctlmnb;\n\tmsq->q_lspid = msq->q_lrpid = 0;\n\tINIT_LIST_HEAD(&msq->q_messages);\n\tINIT_LIST_HEAD(&msq->q_receivers);\n\tINIT_LIST_HEAD(&msq->q_senders);\n\n\tipc_unlock_object(&msq->q_perm);\n\trcu_read_unlock();\n\n\treturn msq->q_perm.id;\n}",
        "code_after_change": "static int newque(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tstruct msg_queue *msq;\n\tint id, retval;\n\tkey_t key = params->key;\n\tint msgflg = params->flg;\n\n\tmsq = ipc_rcu_alloc(sizeof(*msq));\n\tif (!msq)\n\t\treturn -ENOMEM;\n\n\tmsq->q_perm.mode = msgflg & S_IRWXUGO;\n\tmsq->q_perm.key = key;\n\n\tmsq->q_perm.security = NULL;\n\tretval = security_msg_queue_alloc(msq);\n\tif (retval) {\n\t\tipc_rcu_putref(msq, ipc_rcu_free);\n\t\treturn retval;\n\t}\n\n\tmsq->q_stime = msq->q_rtime = 0;\n\tmsq->q_ctime = get_seconds();\n\tmsq->q_cbytes = msq->q_qnum = 0;\n\tmsq->q_qbytes = ns->msg_ctlmnb;\n\tmsq->q_lspid = msq->q_lrpid = 0;\n\tINIT_LIST_HEAD(&msq->q_messages);\n\tINIT_LIST_HEAD(&msq->q_receivers);\n\tINIT_LIST_HEAD(&msq->q_senders);\n\n\t/* ipc_addid() locks msq upon success. */\n\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n\tif (id < 0) {\n\t\tipc_rcu_putref(msq, msg_rcu_free);\n\t\treturn id;\n\t}\n\n\tipc_unlock_object(&msq->q_perm);\n\trcu_read_unlock();\n\n\treturn msq->q_perm.id;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,13 +19,6 @@\n \t\treturn retval;\n \t}\n \n-\t/* ipc_addid() locks msq upon success. */\n-\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n-\tif (id < 0) {\n-\t\tipc_rcu_putref(msq, msg_rcu_free);\n-\t\treturn id;\n-\t}\n-\n \tmsq->q_stime = msq->q_rtime = 0;\n \tmsq->q_ctime = get_seconds();\n \tmsq->q_cbytes = msq->q_qnum = 0;\n@@ -35,6 +28,13 @@\n \tINIT_LIST_HEAD(&msq->q_receivers);\n \tINIT_LIST_HEAD(&msq->q_senders);\n \n+\t/* ipc_addid() locks msq upon success. */\n+\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);\n+\tif (id < 0) {\n+\t\tipc_rcu_putref(msq, msg_rcu_free);\n+\t\treturn id;\n+\t}\n+\n \tipc_unlock_object(&msq->q_perm);\n \trcu_read_unlock();\n ",
        "function_modified_lines": {
            "added": [
                "\t/* ipc_addid() locks msq upon success. */",
                "\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);",
                "\tif (id < 0) {",
                "\t\tipc_rcu_putref(msq, msg_rcu_free);",
                "\t\treturn id;",
                "\t}",
                ""
            ],
            "deleted": [
                "\t/* ipc_addid() locks msq upon success. */",
                "\tid = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);",
                "\tif (id < 0) {",
                "\t\tipc_rcu_putref(msq, msg_rcu_free);",
                "\t\treturn id;",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c."
    },
    {
        "cve_id": "CVE-2015-7613",
        "code_before_change": "static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tkey_t key = params->key;\n\tint shmflg = params->flg;\n\tsize_t size = params->u.size;\n\tint error;\n\tstruct shmid_kernel *shp;\n\tsize_t numpages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstruct file *file;\n\tchar name[13];\n\tint id;\n\tvm_flags_t acctflag = 0;\n\n\tif (size < SHMMIN || size > ns->shm_ctlmax)\n\t\treturn -EINVAL;\n\n\tif (numpages << PAGE_SHIFT < size)\n\t\treturn -ENOSPC;\n\n\tif (ns->shm_tot + numpages < ns->shm_tot ||\n\t\t\tns->shm_tot + numpages > ns->shm_ctlall)\n\t\treturn -ENOSPC;\n\n\tshp = ipc_rcu_alloc(sizeof(*shp));\n\tif (!shp)\n\t\treturn -ENOMEM;\n\n\tshp->shm_perm.key = key;\n\tshp->shm_perm.mode = (shmflg & S_IRWXUGO);\n\tshp->mlock_user = NULL;\n\n\tshp->shm_perm.security = NULL;\n\terror = security_shm_alloc(shp);\n\tif (error) {\n\t\tipc_rcu_putref(shp, ipc_rcu_free);\n\t\treturn error;\n\t}\n\n\tsprintf(name, \"SYSV%08x\", key);\n\tif (shmflg & SHM_HUGETLB) {\n\t\tstruct hstate *hs;\n\t\tsize_t hugesize;\n\n\t\ths = hstate_sizelog((shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t\tif (!hs) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto no_file;\n\t\t}\n\t\thugesize = ALIGN(size, huge_page_size(hs));\n\n\t\t/* hugetlb_file_setup applies strict accounting */\n\t\tif (shmflg & SHM_NORESERVE)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = hugetlb_file_setup(name, hugesize, acctflag,\n\t\t\t\t  &shp->mlock_user, HUGETLB_SHMFS_INODE,\n\t\t\t\t(shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t} else {\n\t\t/*\n\t\t * Do not allow no accounting for OVERCOMMIT_NEVER, even\n\t\t * if it's asked for.\n\t\t */\n\t\tif  ((shmflg & SHM_NORESERVE) &&\n\t\t\t\tsysctl_overcommit_memory != OVERCOMMIT_NEVER)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = shmem_kernel_file_setup(name, size, acctflag);\n\t}\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto no_file;\n\n\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n\tif (id < 0) {\n\t\terror = id;\n\t\tgoto no_id;\n\t}\n\n\tshp->shm_cprid = task_tgid_vnr(current);\n\tshp->shm_lprid = 0;\n\tshp->shm_atim = shp->shm_dtim = 0;\n\tshp->shm_ctim = get_seconds();\n\tshp->shm_segsz = size;\n\tshp->shm_nattch = 0;\n\tshp->shm_file = file;\n\tshp->shm_creator = current;\n\tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n\n\t/*\n\t * shmid gets reported as \"inode#\" in /proc/pid/maps.\n\t * proc-ps tools use this. Changing this will break them.\n\t */\n\tfile_inode(file)->i_ino = shp->shm_perm.id;\n\n\tns->shm_tot += numpages;\n\terror = shp->shm_perm.id;\n\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\treturn error;\n\nno_id:\n\tif (is_file_hugepages(file) && shp->mlock_user)\n\t\tuser_shm_unlock(size, shp->mlock_user);\n\tfput(file);\nno_file:\n\tipc_rcu_putref(shp, shm_rcu_free);\n\treturn error;\n}",
        "code_after_change": "static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tkey_t key = params->key;\n\tint shmflg = params->flg;\n\tsize_t size = params->u.size;\n\tint error;\n\tstruct shmid_kernel *shp;\n\tsize_t numpages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstruct file *file;\n\tchar name[13];\n\tint id;\n\tvm_flags_t acctflag = 0;\n\n\tif (size < SHMMIN || size > ns->shm_ctlmax)\n\t\treturn -EINVAL;\n\n\tif (numpages << PAGE_SHIFT < size)\n\t\treturn -ENOSPC;\n\n\tif (ns->shm_tot + numpages < ns->shm_tot ||\n\t\t\tns->shm_tot + numpages > ns->shm_ctlall)\n\t\treturn -ENOSPC;\n\n\tshp = ipc_rcu_alloc(sizeof(*shp));\n\tif (!shp)\n\t\treturn -ENOMEM;\n\n\tshp->shm_perm.key = key;\n\tshp->shm_perm.mode = (shmflg & S_IRWXUGO);\n\tshp->mlock_user = NULL;\n\n\tshp->shm_perm.security = NULL;\n\terror = security_shm_alloc(shp);\n\tif (error) {\n\t\tipc_rcu_putref(shp, ipc_rcu_free);\n\t\treturn error;\n\t}\n\n\tsprintf(name, \"SYSV%08x\", key);\n\tif (shmflg & SHM_HUGETLB) {\n\t\tstruct hstate *hs;\n\t\tsize_t hugesize;\n\n\t\ths = hstate_sizelog((shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t\tif (!hs) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto no_file;\n\t\t}\n\t\thugesize = ALIGN(size, huge_page_size(hs));\n\n\t\t/* hugetlb_file_setup applies strict accounting */\n\t\tif (shmflg & SHM_NORESERVE)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = hugetlb_file_setup(name, hugesize, acctflag,\n\t\t\t\t  &shp->mlock_user, HUGETLB_SHMFS_INODE,\n\t\t\t\t(shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t} else {\n\t\t/*\n\t\t * Do not allow no accounting for OVERCOMMIT_NEVER, even\n\t\t * if it's asked for.\n\t\t */\n\t\tif  ((shmflg & SHM_NORESERVE) &&\n\t\t\t\tsysctl_overcommit_memory != OVERCOMMIT_NEVER)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = shmem_kernel_file_setup(name, size, acctflag);\n\t}\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto no_file;\n\n\tshp->shm_cprid = task_tgid_vnr(current);\n\tshp->shm_lprid = 0;\n\tshp->shm_atim = shp->shm_dtim = 0;\n\tshp->shm_ctim = get_seconds();\n\tshp->shm_segsz = size;\n\tshp->shm_nattch = 0;\n\tshp->shm_file = file;\n\tshp->shm_creator = current;\n\n\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n\tif (id < 0) {\n\t\terror = id;\n\t\tgoto no_id;\n\t}\n\n\tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n\n\t/*\n\t * shmid gets reported as \"inode#\" in /proc/pid/maps.\n\t * proc-ps tools use this. Changing this will break them.\n\t */\n\tfile_inode(file)->i_ino = shp->shm_perm.id;\n\n\tns->shm_tot += numpages;\n\terror = shp->shm_perm.id;\n\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\treturn error;\n\nno_id:\n\tif (is_file_hugepages(file) && shp->mlock_user)\n\t\tuser_shm_unlock(size, shp->mlock_user);\n\tfput(file);\nno_file:\n\tipc_rcu_putref(shp, shm_rcu_free);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -68,12 +68,6 @@\n \tif (IS_ERR(file))\n \t\tgoto no_file;\n \n-\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n-\tif (id < 0) {\n-\t\terror = id;\n-\t\tgoto no_id;\n-\t}\n-\n \tshp->shm_cprid = task_tgid_vnr(current);\n \tshp->shm_lprid = 0;\n \tshp->shm_atim = shp->shm_dtim = 0;\n@@ -82,6 +76,13 @@\n \tshp->shm_nattch = 0;\n \tshp->shm_file = file;\n \tshp->shm_creator = current;\n+\n+\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n+\tif (id < 0) {\n+\t\terror = id;\n+\t\tgoto no_id;\n+\t}\n+\n \tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "",
                "\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);",
                "\tif (id < 0) {",
                "\t\terror = id;",
                "\t\tgoto no_id;",
                "\t}",
                ""
            ],
            "deleted": [
                "\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);",
                "\tif (id < 0) {",
                "\t\terror = id;",
                "\t\tgoto no_id;",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c."
    },
    {
        "cve_id": "CVE-2015-7613",
        "code_before_change": "int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n{\n\tkuid_t euid;\n\tkgid_t egid;\n\tint id;\n\tint next_id = ids->next_id;\n\n\tif (size > IPCMNI)\n\t\tsize = IPCMNI;\n\n\tif (ids->in_use >= size)\n\t\treturn -ENOSPC;\n\n\tidr_preload(GFP_KERNEL);\n\n\tspin_lock_init(&new->lock);\n\tnew->deleted = false;\n\trcu_read_lock();\n\tspin_lock(&new->lock);\n\n\tid = idr_alloc(&ids->ipcs_idr, new,\n\t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n\t\t       GFP_NOWAIT);\n\tidr_preload_end();\n\tif (id < 0) {\n\t\tspin_unlock(&new->lock);\n\t\trcu_read_unlock();\n\t\treturn id;\n\t}\n\n\tids->in_use++;\n\n\tcurrent_euid_egid(&euid, &egid);\n\tnew->cuid = new->uid = euid;\n\tnew->gid = new->cgid = egid;\n\n\tif (next_id < 0) {\n\t\tnew->seq = ids->seq++;\n\t\tif (ids->seq > IPCID_SEQ_MAX)\n\t\t\tids->seq = 0;\n\t} else {\n\t\tnew->seq = ipcid_to_seqx(next_id);\n\t\tids->next_id = -1;\n\t}\n\n\tnew->id = ipc_buildid(id, new->seq);\n\treturn id;\n}",
        "code_after_change": "int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int size)\n{\n\tkuid_t euid;\n\tkgid_t egid;\n\tint id;\n\tint next_id = ids->next_id;\n\n\tif (size > IPCMNI)\n\t\tsize = IPCMNI;\n\n\tif (ids->in_use >= size)\n\t\treturn -ENOSPC;\n\n\tidr_preload(GFP_KERNEL);\n\n\tspin_lock_init(&new->lock);\n\tnew->deleted = false;\n\trcu_read_lock();\n\tspin_lock(&new->lock);\n\n\tcurrent_euid_egid(&euid, &egid);\n\tnew->cuid = new->uid = euid;\n\tnew->gid = new->cgid = egid;\n\n\tid = idr_alloc(&ids->ipcs_idr, new,\n\t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n\t\t       GFP_NOWAIT);\n\tidr_preload_end();\n\tif (id < 0) {\n\t\tspin_unlock(&new->lock);\n\t\trcu_read_unlock();\n\t\treturn id;\n\t}\n\n\tids->in_use++;\n\n\tif (next_id < 0) {\n\t\tnew->seq = ids->seq++;\n\t\tif (ids->seq > IPCID_SEQ_MAX)\n\t\t\tids->seq = 0;\n\t} else {\n\t\tnew->seq = ipcid_to_seqx(next_id);\n\t\tids->next_id = -1;\n\t}\n\n\tnew->id = ipc_buildid(id, new->seq);\n\treturn id;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,6 +18,10 @@\n \trcu_read_lock();\n \tspin_lock(&new->lock);\n \n+\tcurrent_euid_egid(&euid, &egid);\n+\tnew->cuid = new->uid = euid;\n+\tnew->gid = new->cgid = egid;\n+\n \tid = idr_alloc(&ids->ipcs_idr, new,\n \t\t       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,\n \t\t       GFP_NOWAIT);\n@@ -29,10 +33,6 @@\n \t}\n \n \tids->in_use++;\n-\n-\tcurrent_euid_egid(&euid, &egid);\n-\tnew->cuid = new->uid = euid;\n-\tnew->gid = new->cgid = egid;\n \n \tif (next_id < 0) {\n \t\tnew->seq = ids->seq++;",
        "function_modified_lines": {
            "added": [
                "\tcurrent_euid_egid(&euid, &egid);",
                "\tnew->cuid = new->uid = euid;",
                "\tnew->gid = new->cgid = egid;",
                ""
            ],
            "deleted": [
                "",
                "\tcurrent_euid_egid(&euid, &egid);",
                "\tnew->cuid = new->uid = euid;",
                "\tnew->gid = new->cgid = egid;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c."
    },
    {
        "cve_id": "CVE-2015-7990",
        "code_before_change": "static struct rds_connection *__rds_conn_create(struct net *net,\n\t\t\t\t\t\t__be32 laddr, __be32 faddr,\n\t\t\t\t       struct rds_transport *trans, gfp_t gfp,\n\t\t\t\t       int is_outgoing)\n{\n\tstruct rds_connection *conn, *parent = NULL;\n\tstruct hlist_head *head = rds_conn_bucket(laddr, faddr);\n\tstruct rds_transport *loop_trans;\n\tunsigned long flags;\n\tint ret;\n\n\trcu_read_lock();\n\tconn = rds_conn_lookup(net, head, laddr, faddr, trans);\n\tif (conn && conn->c_loopback && conn->c_trans != &rds_loop_transport &&\n\t    laddr == faddr && !is_outgoing) {\n\t\t/* This is a looped back IB connection, and we're\n\t\t * called by the code handling the incoming connect.\n\t\t * We need a second connection object into which we\n\t\t * can stick the other QP. */\n\t\tparent = conn;\n\t\tconn = parent->c_passive;\n\t}\n\trcu_read_unlock();\n\tif (conn)\n\t\tgoto out;\n\n\tconn = kmem_cache_zalloc(rds_conn_slab, gfp);\n\tif (!conn) {\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tINIT_HLIST_NODE(&conn->c_hash_node);\n\tconn->c_laddr = laddr;\n\tconn->c_faddr = faddr;\n\tspin_lock_init(&conn->c_lock);\n\tconn->c_next_tx_seq = 1;\n\trds_conn_net_set(conn, net);\n\n\tinit_waitqueue_head(&conn->c_waitq);\n\tINIT_LIST_HEAD(&conn->c_send_queue);\n\tINIT_LIST_HEAD(&conn->c_retrans);\n\n\tret = rds_cong_get_maps(conn);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This is where a connection becomes loopback.  If *any* RDS sockets\n\t * can bind to the destination address then we'd rather the messages\n\t * flow through loopback rather than either transport.\n\t */\n\tloop_trans = rds_trans_get_preferred(net, faddr);\n\tif (loop_trans) {\n\t\trds_trans_put(loop_trans);\n\t\tconn->c_loopback = 1;\n\t\tif (is_outgoing && trans->t_prefer_loopback) {\n\t\t\t/* \"outgoing\" connection - and the transport\n\t\t\t * says it wants the connection handled by the\n\t\t\t * loopback transport. This is what TCP does.\n\t\t\t */\n\t\t\ttrans = &rds_loop_transport;\n\t\t}\n\t}\n\n\tif (trans == NULL) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(-ENODEV);\n\t\tgoto out;\n\t}\n\n\tconn->c_trans = trans;\n\n\tret = trans->conn_alloc(conn, gfp);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tatomic_set(&conn->c_state, RDS_CONN_DOWN);\n\tconn->c_send_gen = 0;\n\tconn->c_outgoing = (is_outgoing ? 1 : 0);\n\tconn->c_reconnect_jiffies = 0;\n\tINIT_DELAYED_WORK(&conn->c_send_w, rds_send_worker);\n\tINIT_DELAYED_WORK(&conn->c_recv_w, rds_recv_worker);\n\tINIT_DELAYED_WORK(&conn->c_conn_w, rds_connect_worker);\n\tINIT_WORK(&conn->c_down_w, rds_shutdown_worker);\n\tmutex_init(&conn->c_cm_lock);\n\tconn->c_flags = 0;\n\n\trdsdebug(\"allocated conn %p for %pI4 -> %pI4 over %s %s\\n\",\n\t  conn, &laddr, &faddr,\n\t  trans->t_name ? trans->t_name : \"[unknown]\",\n\t  is_outgoing ? \"(outgoing)\" : \"\");\n\n\t/*\n\t * Since we ran without holding the conn lock, someone could\n\t * have created the same conn (either normal or passive) in the\n\t * interim. We check while holding the lock. If we won, we complete\n\t * init and return our conn. If we lost, we rollback and return the\n\t * other one.\n\t */\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\tif (parent) {\n\t\t/* Creating passive conn */\n\t\tif (parent->c_passive) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = parent->c_passive;\n\t\t} else {\n\t\t\tparent->c_passive = conn;\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t} else {\n\t\t/* Creating normal conn */\n\t\tstruct rds_connection *found;\n\n\t\tfound = rds_conn_lookup(net, head, laddr, faddr, trans);\n\t\tif (found) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = found;\n\t\t} else {\n\t\t\thlist_add_head_rcu(&conn->c_hash_node, head);\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n\nout:\n\treturn conn;\n}",
        "code_after_change": "static struct rds_connection *__rds_conn_create(struct net *net,\n\t\t\t\t\t\t__be32 laddr, __be32 faddr,\n\t\t\t\t       struct rds_transport *trans, gfp_t gfp,\n\t\t\t\t       int is_outgoing)\n{\n\tstruct rds_connection *conn, *parent = NULL;\n\tstruct hlist_head *head = rds_conn_bucket(laddr, faddr);\n\tstruct rds_transport *loop_trans;\n\tunsigned long flags;\n\tint ret;\n\n\trcu_read_lock();\n\tconn = rds_conn_lookup(net, head, laddr, faddr, trans);\n\tif (conn && conn->c_loopback && conn->c_trans != &rds_loop_transport &&\n\t    laddr == faddr && !is_outgoing) {\n\t\t/* This is a looped back IB connection, and we're\n\t\t * called by the code handling the incoming connect.\n\t\t * We need a second connection object into which we\n\t\t * can stick the other QP. */\n\t\tparent = conn;\n\t\tconn = parent->c_passive;\n\t}\n\trcu_read_unlock();\n\tif (conn)\n\t\tgoto out;\n\n\tconn = kmem_cache_zalloc(rds_conn_slab, gfp);\n\tif (!conn) {\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tINIT_HLIST_NODE(&conn->c_hash_node);\n\tconn->c_laddr = laddr;\n\tconn->c_faddr = faddr;\n\tspin_lock_init(&conn->c_lock);\n\tconn->c_next_tx_seq = 1;\n\trds_conn_net_set(conn, net);\n\n\tinit_waitqueue_head(&conn->c_waitq);\n\tINIT_LIST_HEAD(&conn->c_send_queue);\n\tINIT_LIST_HEAD(&conn->c_retrans);\n\n\tret = rds_cong_get_maps(conn);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This is where a connection becomes loopback.  If *any* RDS sockets\n\t * can bind to the destination address then we'd rather the messages\n\t * flow through loopback rather than either transport.\n\t */\n\tloop_trans = rds_trans_get_preferred(net, faddr);\n\tif (loop_trans) {\n\t\trds_trans_put(loop_trans);\n\t\tconn->c_loopback = 1;\n\t\tif (is_outgoing && trans->t_prefer_loopback) {\n\t\t\t/* \"outgoing\" connection - and the transport\n\t\t\t * says it wants the connection handled by the\n\t\t\t * loopback transport. This is what TCP does.\n\t\t\t */\n\t\t\ttrans = &rds_loop_transport;\n\t\t}\n\t}\n\n\tconn->c_trans = trans;\n\n\tret = trans->conn_alloc(conn, gfp);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tatomic_set(&conn->c_state, RDS_CONN_DOWN);\n\tconn->c_send_gen = 0;\n\tconn->c_outgoing = (is_outgoing ? 1 : 0);\n\tconn->c_reconnect_jiffies = 0;\n\tINIT_DELAYED_WORK(&conn->c_send_w, rds_send_worker);\n\tINIT_DELAYED_WORK(&conn->c_recv_w, rds_recv_worker);\n\tINIT_DELAYED_WORK(&conn->c_conn_w, rds_connect_worker);\n\tINIT_WORK(&conn->c_down_w, rds_shutdown_worker);\n\tmutex_init(&conn->c_cm_lock);\n\tconn->c_flags = 0;\n\n\trdsdebug(\"allocated conn %p for %pI4 -> %pI4 over %s %s\\n\",\n\t  conn, &laddr, &faddr,\n\t  trans->t_name ? trans->t_name : \"[unknown]\",\n\t  is_outgoing ? \"(outgoing)\" : \"\");\n\n\t/*\n\t * Since we ran without holding the conn lock, someone could\n\t * have created the same conn (either normal or passive) in the\n\t * interim. We check while holding the lock. If we won, we complete\n\t * init and return our conn. If we lost, we rollback and return the\n\t * other one.\n\t */\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\tif (parent) {\n\t\t/* Creating passive conn */\n\t\tif (parent->c_passive) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = parent->c_passive;\n\t\t} else {\n\t\t\tparent->c_passive = conn;\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t} else {\n\t\t/* Creating normal conn */\n\t\tstruct rds_connection *found;\n\n\t\tfound = rds_conn_lookup(net, head, laddr, faddr, trans);\n\t\tif (found) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = found;\n\t\t} else {\n\t\t\thlist_add_head_rcu(&conn->c_hash_node, head);\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n\nout:\n\treturn conn;\n}",
        "patch": "--- code before\n+++ code after\n@@ -66,12 +66,6 @@\n \t\t}\n \t}\n \n-\tif (trans == NULL) {\n-\t\tkmem_cache_free(rds_conn_slab, conn);\n-\t\tconn = ERR_PTR(-ENODEV);\n-\t\tgoto out;\n-\t}\n-\n \tconn->c_trans = trans;\n \n \tret = trans->conn_alloc(conn, gfp);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tif (trans == NULL) {",
                "\t\tkmem_cache_free(rds_conn_slab, conn);",
                "\t\tconn = ERR_PTR(-ENODEV);",
                "\t\tgoto out;",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the rds_sendmsg function in net/rds/sendmsg.c in the Linux kernel before 4.3.3 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by using a socket that was not properly bound.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2015-6937."
    },
    {
        "cve_id": "CVE-2015-7990",
        "code_before_change": "int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rds_sock *rs = rds_sk_to_rs(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t__be32 daddr;\n\t__be16 dport;\n\tstruct rds_message *rm = NULL;\n\tstruct rds_connection *conn;\n\tint ret = 0;\n\tint queued = 0, allocated_mr = 0;\n\tint nonblock = msg->msg_flags & MSG_DONTWAIT;\n\tlong timeo = sock_sndtimeo(sk, nonblock);\n\n\t/* Mirror Linux UDP mirror of BSD error message compatibility */\n\t/* XXX: Perhaps MSG_MORE someday */\n\tif (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (msg->msg_namelen) {\n\t\t/* XXX fail non-unicast destination IPs? */\n\t\tif (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t} else {\n\t\t/* We only care about consistency with ->connect() */\n\t\tlock_sock(sk);\n\t\tdaddr = rs->rs_conn_addr;\n\t\tdport = rs->rs_conn_port;\n\t\trelease_sock(sk);\n\t}\n\n\t/* racing with another thread binding seems ok here */\n\tif (daddr == 0 || rs->rs_bound_addr == 0) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (payload_len > rds_sk_sndbuf(rs)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\t/* size of rm including all sgs */\n\tret = rds_rm_size(msg, payload_len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\trm = rds_message_alloc(ret, GFP_KERNEL);\n\tif (!rm) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Attach data to the rm */\n\tif (payload_len) {\n\t\trm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));\n\t\tif (!rm->data.op_sg) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = rds_message_copy_from_user(rm, &msg->msg_iter);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\trm->data.op_active = 1;\n\n\trm->m_daddr = daddr;\n\n\t/* rds_conn_create has a spinlock that runs with IRQ off.\n\t * Caching the conn in the socket helps a lot. */\n\tif (rs->rs_conn && rs->rs_conn->c_faddr == daddr)\n\t\tconn = rs->rs_conn;\n\telse {\n\t\tconn = rds_conn_create_outgoing(sock_net(sock->sk),\n\t\t\t\t\t\trs->rs_bound_addr, daddr,\n\t\t\t\t\trs->rs_transport,\n\t\t\t\t\tsock->sk->sk_allocation);\n\t\tif (IS_ERR(conn)) {\n\t\t\tret = PTR_ERR(conn);\n\t\t\tgoto out;\n\t\t}\n\t\trs->rs_conn = conn;\n\t}\n\n\t/* Parse any control messages the user may have included. */\n\tret = rds_cmsg_send(rs, rm, msg, &allocated_mr);\n\tif (ret)\n\t\tgoto out;\n\n\tif (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {\n\t\tprintk_ratelimited(KERN_NOTICE \"rdma_op %p conn xmit_rdma %p\\n\",\n\t\t\t       &rm->rdma, conn->c_trans->xmit_rdma);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {\n\t\tprintk_ratelimited(KERN_NOTICE \"atomic_op %p conn xmit_atomic %p\\n\",\n\t\t\t       &rm->atomic, conn->c_trans->xmit_atomic);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\trds_conn_connect_if_down(conn);\n\n\tret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);\n\tif (ret) {\n\t\trs->rs_seen_congestion = 1;\n\t\tgoto out;\n\t}\n\n\twhile (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,\n\t\t\t\t  dport, &queued)) {\n\t\trds_stats_inc(s_send_queue_full);\n\n\t\tif (nonblock) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\n\t\ttimeo = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\trds_send_queue_rm(rs, conn, rm,\n\t\t\t\t\t\t\t  rs->rs_bound_port,\n\t\t\t\t\t\t\t  dport,\n\t\t\t\t\t\t\t  &queued),\n\t\t\t\t\ttimeo);\n\t\trdsdebug(\"sendmsg woke queued %d timeo %ld\\n\", queued, timeo);\n\t\tif (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)\n\t\t\tcontinue;\n\n\t\tret = timeo;\n\t\tif (ret == 0)\n\t\t\tret = -ETIMEDOUT;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * By now we've committed to the send.  We reuse rds_send_worker()\n\t * to retry sends in the rds thread if the transport asks us to.\n\t */\n\trds_stats_inc(s_send_queued);\n\n\tret = rds_send_xmit(conn);\n\tif (ret == -ENOMEM || ret == -EAGAIN)\n\t\tqueue_delayed_work(rds_wq, &conn->c_send_w, 1);\n\n\trds_message_put(rm);\n\treturn payload_len;\n\nout:\n\t/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.\n\t * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN\n\t * or in any other way, we need to destroy the MR again */\n\tif (allocated_mr)\n\t\trds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);\n\n\tif (rm)\n\t\trds_message_put(rm);\n\treturn ret;\n}",
        "code_after_change": "int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rds_sock *rs = rds_sk_to_rs(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t__be32 daddr;\n\t__be16 dport;\n\tstruct rds_message *rm = NULL;\n\tstruct rds_connection *conn;\n\tint ret = 0;\n\tint queued = 0, allocated_mr = 0;\n\tint nonblock = msg->msg_flags & MSG_DONTWAIT;\n\tlong timeo = sock_sndtimeo(sk, nonblock);\n\n\t/* Mirror Linux UDP mirror of BSD error message compatibility */\n\t/* XXX: Perhaps MSG_MORE someday */\n\tif (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (msg->msg_namelen) {\n\t\t/* XXX fail non-unicast destination IPs? */\n\t\tif (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t} else {\n\t\t/* We only care about consistency with ->connect() */\n\t\tlock_sock(sk);\n\t\tdaddr = rs->rs_conn_addr;\n\t\tdport = rs->rs_conn_port;\n\t\trelease_sock(sk);\n\t}\n\n\tlock_sock(sk);\n\tif (daddr == 0 || rs->rs_bound_addr == 0) {\n\t\trelease_sock(sk);\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\trelease_sock(sk);\n\n\tif (payload_len > rds_sk_sndbuf(rs)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\t/* size of rm including all sgs */\n\tret = rds_rm_size(msg, payload_len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\trm = rds_message_alloc(ret, GFP_KERNEL);\n\tif (!rm) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Attach data to the rm */\n\tif (payload_len) {\n\t\trm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));\n\t\tif (!rm->data.op_sg) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = rds_message_copy_from_user(rm, &msg->msg_iter);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\trm->data.op_active = 1;\n\n\trm->m_daddr = daddr;\n\n\t/* rds_conn_create has a spinlock that runs with IRQ off.\n\t * Caching the conn in the socket helps a lot. */\n\tif (rs->rs_conn && rs->rs_conn->c_faddr == daddr)\n\t\tconn = rs->rs_conn;\n\telse {\n\t\tconn = rds_conn_create_outgoing(sock_net(sock->sk),\n\t\t\t\t\t\trs->rs_bound_addr, daddr,\n\t\t\t\t\trs->rs_transport,\n\t\t\t\t\tsock->sk->sk_allocation);\n\t\tif (IS_ERR(conn)) {\n\t\t\tret = PTR_ERR(conn);\n\t\t\tgoto out;\n\t\t}\n\t\trs->rs_conn = conn;\n\t}\n\n\t/* Parse any control messages the user may have included. */\n\tret = rds_cmsg_send(rs, rm, msg, &allocated_mr);\n\tif (ret)\n\t\tgoto out;\n\n\tif (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {\n\t\tprintk_ratelimited(KERN_NOTICE \"rdma_op %p conn xmit_rdma %p\\n\",\n\t\t\t       &rm->rdma, conn->c_trans->xmit_rdma);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {\n\t\tprintk_ratelimited(KERN_NOTICE \"atomic_op %p conn xmit_atomic %p\\n\",\n\t\t\t       &rm->atomic, conn->c_trans->xmit_atomic);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\trds_conn_connect_if_down(conn);\n\n\tret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);\n\tif (ret) {\n\t\trs->rs_seen_congestion = 1;\n\t\tgoto out;\n\t}\n\n\twhile (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,\n\t\t\t\t  dport, &queued)) {\n\t\trds_stats_inc(s_send_queue_full);\n\n\t\tif (nonblock) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\n\t\ttimeo = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\trds_send_queue_rm(rs, conn, rm,\n\t\t\t\t\t\t\t  rs->rs_bound_port,\n\t\t\t\t\t\t\t  dport,\n\t\t\t\t\t\t\t  &queued),\n\t\t\t\t\ttimeo);\n\t\trdsdebug(\"sendmsg woke queued %d timeo %ld\\n\", queued, timeo);\n\t\tif (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)\n\t\t\tcontinue;\n\n\t\tret = timeo;\n\t\tif (ret == 0)\n\t\t\tret = -ETIMEDOUT;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * By now we've committed to the send.  We reuse rds_send_worker()\n\t * to retry sends in the rds thread if the transport asks us to.\n\t */\n\trds_stats_inc(s_send_queued);\n\n\tret = rds_send_xmit(conn);\n\tif (ret == -ENOMEM || ret == -EAGAIN)\n\t\tqueue_delayed_work(rds_wq, &conn->c_send_w, 1);\n\n\trds_message_put(rm);\n\treturn payload_len;\n\nout:\n\t/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.\n\t * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN\n\t * or in any other way, we need to destroy the MR again */\n\tif (allocated_mr)\n\t\trds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);\n\n\tif (rm)\n\t\trds_message_put(rm);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -35,11 +35,13 @@\n \t\trelease_sock(sk);\n \t}\n \n-\t/* racing with another thread binding seems ok here */\n+\tlock_sock(sk);\n \tif (daddr == 0 || rs->rs_bound_addr == 0) {\n+\t\trelease_sock(sk);\n \t\tret = -ENOTCONN; /* XXX not a great errno */\n \t\tgoto out;\n \t}\n+\trelease_sock(sk);\n \n \tif (payload_len > rds_sk_sndbuf(rs)) {\n \t\tret = -EMSGSIZE;",
        "function_modified_lines": {
            "added": [
                "\tlock_sock(sk);",
                "\t\trelease_sock(sk);",
                "\trelease_sock(sk);"
            ],
            "deleted": [
                "\t/* racing with another thread binding seems ok here */"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the rds_sendmsg function in net/rds/sendmsg.c in the Linux kernel before 4.3.3 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by using a socket that was not properly bound.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2015-6937."
    },
    {
        "cve_id": "CVE-2015-8767",
        "code_before_change": "void sctp_generate_t3_rtx_event(unsigned long peer)\n{\n\tint error;\n\tstruct sctp_transport *transport = (struct sctp_transport *) peer;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct net *net = sock_net(asoc->base.sk);\n\n\t/* Check whether a task is in the sock.  */\n\n\tbh_lock_sock(asoc->base.sk);\n\tif (sock_owned_by_user(asoc->base.sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->T3_rtx_timer, jiffies + (HZ/20)))\n\t\t\tsctp_transport_hold(transport);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this transport really dead and just waiting around for\n\t * the timer to let go of the reference?\n\t */\n\tif (transport->dead)\n\t\tgoto out_unlock;\n\n\t/* Run through the state machine.  */\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(SCTP_EVENT_TIMEOUT_T3_RTX),\n\t\t\t   asoc->state,\n\t\t\t   asoc->ep, asoc,\n\t\t\t   transport, GFP_ATOMIC);\n\n\tif (error)\n\t\tasoc->base.sk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(asoc->base.sk);\n\tsctp_transport_put(transport);\n}",
        "code_after_change": "void sctp_generate_t3_rtx_event(unsigned long peer)\n{\n\tint error;\n\tstruct sctp_transport *transport = (struct sctp_transport *) peer;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct sock *sk = asoc->base.sk;\n\tstruct net *net = sock_net(sk);\n\n\t/* Check whether a task is in the sock.  */\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->T3_rtx_timer, jiffies + (HZ/20)))\n\t\t\tsctp_transport_hold(transport);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this transport really dead and just waiting around for\n\t * the timer to let go of the reference?\n\t */\n\tif (transport->dead)\n\t\tgoto out_unlock;\n\n\t/* Run through the state machine.  */\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(SCTP_EVENT_TIMEOUT_T3_RTX),\n\t\t\t   asoc->state,\n\t\t\t   asoc->ep, asoc,\n\t\t\t   transport, GFP_ATOMIC);\n\n\tif (error)\n\t\tsk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(sk);\n\tsctp_transport_put(transport);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,12 +3,13 @@\n \tint error;\n \tstruct sctp_transport *transport = (struct sctp_transport *) peer;\n \tstruct sctp_association *asoc = transport->asoc;\n-\tstruct net *net = sock_net(asoc->base.sk);\n+\tstruct sock *sk = asoc->base.sk;\n+\tstruct net *net = sock_net(sk);\n \n \t/* Check whether a task is in the sock.  */\n \n-\tbh_lock_sock(asoc->base.sk);\n-\tif (sock_owned_by_user(asoc->base.sk)) {\n+\tbh_lock_sock(sk);\n+\tif (sock_owned_by_user(sk)) {\n \t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n \n \t\t/* Try again later.  */\n@@ -31,9 +32,9 @@\n \t\t\t   transport, GFP_ATOMIC);\n \n \tif (error)\n-\t\tasoc->base.sk->sk_err = -error;\n+\t\tsk->sk_err = -error;\n \n out_unlock:\n-\tbh_unlock_sock(asoc->base.sk);\n+\tbh_unlock_sock(sk);\n \tsctp_transport_put(transport);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct sock *sk = asoc->base.sk;",
                "\tstruct net *net = sock_net(sk);",
                "\tbh_lock_sock(sk);",
                "\tif (sock_owned_by_user(sk)) {",
                "\t\tsk->sk_err = -error;",
                "\tbh_unlock_sock(sk);"
            ],
            "deleted": [
                "\tstruct net *net = sock_net(asoc->base.sk);",
                "\tbh_lock_sock(asoc->base.sk);",
                "\tif (sock_owned_by_user(asoc->base.sk)) {",
                "\t\tasoc->base.sk->sk_err = -error;",
                "\tbh_unlock_sock(asoc->base.sk);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "net/sctp/sm_sideeffect.c in the Linux kernel before 4.3 does not properly manage the relationship between a lock and a socket, which allows local users to cause a denial of service (deadlock) via a crafted sctp_accept call."
    },
    {
        "cve_id": "CVE-2015-8767",
        "code_before_change": "void sctp_generate_proto_unreach_event(unsigned long data)\n{\n\tstruct sctp_transport *transport = (struct sctp_transport *) data;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct net *net = sock_net(asoc->base.sk);\n\n\tbh_lock_sock(asoc->base.sk);\n\tif (sock_owned_by_user(asoc->base.sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->proto_unreach_timer,\n\t\t\t\tjiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this structure just waiting around for us to actually\n\t * get destroyed?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\tsctp_do_sm(net, SCTP_EVENT_T_OTHER,\n\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),\n\t\t   asoc->state, asoc->ep, asoc, transport, GFP_ATOMIC);\n\nout_unlock:\n\tbh_unlock_sock(asoc->base.sk);\n\tsctp_association_put(asoc);\n}",
        "code_after_change": "void sctp_generate_proto_unreach_event(unsigned long data)\n{\n\tstruct sctp_transport *transport = (struct sctp_transport *) data;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct sock *sk = asoc->base.sk;\n\tstruct net *net = sock_net(sk);\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->proto_unreach_timer,\n\t\t\t\tjiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this structure just waiting around for us to actually\n\t * get destroyed?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\tsctp_do_sm(net, SCTP_EVENT_T_OTHER,\n\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),\n\t\t   asoc->state, asoc->ep, asoc, transport, GFP_ATOMIC);\n\nout_unlock:\n\tbh_unlock_sock(sk);\n\tsctp_association_put(asoc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,10 +2,11 @@\n {\n \tstruct sctp_transport *transport = (struct sctp_transport *) data;\n \tstruct sctp_association *asoc = transport->asoc;\n-\tstruct net *net = sock_net(asoc->base.sk);\n+\tstruct sock *sk = asoc->base.sk;\n+\tstruct net *net = sock_net(sk);\n \n-\tbh_lock_sock(asoc->base.sk);\n-\tif (sock_owned_by_user(asoc->base.sk)) {\n+\tbh_lock_sock(sk);\n+\tif (sock_owned_by_user(sk)) {\n \t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n \n \t\t/* Try again later.  */\n@@ -26,6 +27,6 @@\n \t\t   asoc->state, asoc->ep, asoc, transport, GFP_ATOMIC);\n \n out_unlock:\n-\tbh_unlock_sock(asoc->base.sk);\n+\tbh_unlock_sock(sk);\n \tsctp_association_put(asoc);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct sock *sk = asoc->base.sk;",
                "\tstruct net *net = sock_net(sk);",
                "\tbh_lock_sock(sk);",
                "\tif (sock_owned_by_user(sk)) {",
                "\tbh_unlock_sock(sk);"
            ],
            "deleted": [
                "\tstruct net *net = sock_net(asoc->base.sk);",
                "\tbh_lock_sock(asoc->base.sk);",
                "\tif (sock_owned_by_user(asoc->base.sk)) {",
                "\tbh_unlock_sock(asoc->base.sk);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "net/sctp/sm_sideeffect.c in the Linux kernel before 4.3 does not properly manage the relationship between a lock and a socket, which allows local users to cause a denial of service (deadlock) via a crafted sctp_accept call."
    },
    {
        "cve_id": "CVE-2015-8767",
        "code_before_change": "static void sctp_generate_timeout_event(struct sctp_association *asoc,\n\t\t\t\t\tsctp_event_timeout_t timeout_type)\n{\n\tstruct net *net = sock_net(asoc->base.sk);\n\tint error = 0;\n\n\tbh_lock_sock(asoc->base.sk);\n\tif (sock_owned_by_user(asoc->base.sk)) {\n\t\tpr_debug(\"%s: sock is busy: timer %d\\n\", __func__,\n\t\t\t timeout_type);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&asoc->timers[timeout_type], jiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this association really dead and just waiting around for\n\t * the timer to let go of the reference?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\t/* Run through the state machine.  */\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(timeout_type),\n\t\t\t   asoc->state, asoc->ep, asoc,\n\t\t\t   (void *)timeout_type, GFP_ATOMIC);\n\n\tif (error)\n\t\tasoc->base.sk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(asoc->base.sk);\n\tsctp_association_put(asoc);\n}",
        "code_after_change": "static void sctp_generate_timeout_event(struct sctp_association *asoc,\n\t\t\t\t\tsctp_event_timeout_t timeout_type)\n{\n\tstruct sock *sk = asoc->base.sk;\n\tstruct net *net = sock_net(sk);\n\tint error = 0;\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk)) {\n\t\tpr_debug(\"%s: sock is busy: timer %d\\n\", __func__,\n\t\t\t timeout_type);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&asoc->timers[timeout_type], jiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this association really dead and just waiting around for\n\t * the timer to let go of the reference?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\t/* Run through the state machine.  */\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(timeout_type),\n\t\t\t   asoc->state, asoc->ep, asoc,\n\t\t\t   (void *)timeout_type, GFP_ATOMIC);\n\n\tif (error)\n\t\tsk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(sk);\n\tsctp_association_put(asoc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,12 @@\n static void sctp_generate_timeout_event(struct sctp_association *asoc,\n \t\t\t\t\tsctp_event_timeout_t timeout_type)\n {\n-\tstruct net *net = sock_net(asoc->base.sk);\n+\tstruct sock *sk = asoc->base.sk;\n+\tstruct net *net = sock_net(sk);\n \tint error = 0;\n \n-\tbh_lock_sock(asoc->base.sk);\n-\tif (sock_owned_by_user(asoc->base.sk)) {\n+\tbh_lock_sock(sk);\n+\tif (sock_owned_by_user(sk)) {\n \t\tpr_debug(\"%s: sock is busy: timer %d\\n\", __func__,\n \t\t\t timeout_type);\n \n@@ -28,9 +29,9 @@\n \t\t\t   (void *)timeout_type, GFP_ATOMIC);\n \n \tif (error)\n-\t\tasoc->base.sk->sk_err = -error;\n+\t\tsk->sk_err = -error;\n \n out_unlock:\n-\tbh_unlock_sock(asoc->base.sk);\n+\tbh_unlock_sock(sk);\n \tsctp_association_put(asoc);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct sock *sk = asoc->base.sk;",
                "\tstruct net *net = sock_net(sk);",
                "\tbh_lock_sock(sk);",
                "\tif (sock_owned_by_user(sk)) {",
                "\t\tsk->sk_err = -error;",
                "\tbh_unlock_sock(sk);"
            ],
            "deleted": [
                "\tstruct net *net = sock_net(asoc->base.sk);",
                "\tbh_lock_sock(asoc->base.sk);",
                "\tif (sock_owned_by_user(asoc->base.sk)) {",
                "\t\tasoc->base.sk->sk_err = -error;",
                "\tbh_unlock_sock(asoc->base.sk);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "net/sctp/sm_sideeffect.c in the Linux kernel before 4.3 does not properly manage the relationship between a lock and a socket, which allows local users to cause a denial of service (deadlock) via a crafted sctp_accept call."
    },
    {
        "cve_id": "CVE-2015-8767",
        "code_before_change": "void sctp_generate_heartbeat_event(unsigned long data)\n{\n\tint error = 0;\n\tstruct sctp_transport *transport = (struct sctp_transport *) data;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct net *net = sock_net(asoc->base.sk);\n\n\tbh_lock_sock(asoc->base.sk);\n\tif (sock_owned_by_user(asoc->base.sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->hb_timer, jiffies + (HZ/20)))\n\t\t\tsctp_transport_hold(transport);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this structure just waiting around for us to actually\n\t * get destroyed?\n\t */\n\tif (transport->dead)\n\t\tgoto out_unlock;\n\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(SCTP_EVENT_TIMEOUT_HEARTBEAT),\n\t\t\t   asoc->state, asoc->ep, asoc,\n\t\t\t   transport, GFP_ATOMIC);\n\n\tif (error)\n\t\tasoc->base.sk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(asoc->base.sk);\n\tsctp_transport_put(transport);\n}",
        "code_after_change": "void sctp_generate_heartbeat_event(unsigned long data)\n{\n\tint error = 0;\n\tstruct sctp_transport *transport = (struct sctp_transport *) data;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct sock *sk = asoc->base.sk;\n\tstruct net *net = sock_net(sk);\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->hb_timer, jiffies + (HZ/20)))\n\t\t\tsctp_transport_hold(transport);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this structure just waiting around for us to actually\n\t * get destroyed?\n\t */\n\tif (transport->dead)\n\t\tgoto out_unlock;\n\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(SCTP_EVENT_TIMEOUT_HEARTBEAT),\n\t\t\t   asoc->state, asoc->ep, asoc,\n\t\t\t   transport, GFP_ATOMIC);\n\n\tif (error)\n\t\tsk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(sk);\n\tsctp_transport_put(transport);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,10 +3,11 @@\n \tint error = 0;\n \tstruct sctp_transport *transport = (struct sctp_transport *) data;\n \tstruct sctp_association *asoc = transport->asoc;\n-\tstruct net *net = sock_net(asoc->base.sk);\n+\tstruct sock *sk = asoc->base.sk;\n+\tstruct net *net = sock_net(sk);\n \n-\tbh_lock_sock(asoc->base.sk);\n-\tif (sock_owned_by_user(asoc->base.sk)) {\n+\tbh_lock_sock(sk);\n+\tif (sock_owned_by_user(sk)) {\n \t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n \n \t\t/* Try again later.  */\n@@ -27,9 +28,9 @@\n \t\t\t   transport, GFP_ATOMIC);\n \n \tif (error)\n-\t\tasoc->base.sk->sk_err = -error;\n+\t\tsk->sk_err = -error;\n \n out_unlock:\n-\tbh_unlock_sock(asoc->base.sk);\n+\tbh_unlock_sock(sk);\n \tsctp_transport_put(transport);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct sock *sk = asoc->base.sk;",
                "\tstruct net *net = sock_net(sk);",
                "\tbh_lock_sock(sk);",
                "\tif (sock_owned_by_user(sk)) {",
                "\t\tsk->sk_err = -error;",
                "\tbh_unlock_sock(sk);"
            ],
            "deleted": [
                "\tstruct net *net = sock_net(asoc->base.sk);",
                "\tbh_lock_sock(asoc->base.sk);",
                "\tif (sock_owned_by_user(asoc->base.sk)) {",
                "\t\tasoc->base.sk->sk_err = -error;",
                "\tbh_unlock_sock(asoc->base.sk);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "net/sctp/sm_sideeffect.c in the Linux kernel before 4.3 does not properly manage the relationship between a lock and a socket, which allows local users to cause a denial of service (deadlock) via a crafted sctp_accept call."
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "static long ext4_zero_range(struct file *file, loff_t offset,\n\t\t\t    loff_t len, int mode)\n{\n\tstruct inode *inode = file_inode(file);\n\thandle_t *handle = NULL;\n\tunsigned int max_blocks;\n\tloff_t new_size = 0;\n\tint ret = 0;\n\tint flags;\n\tint credits;\n\tint partial_begin, partial_end;\n\tloff_t start, end;\n\text4_lblk_t lblk;\n\tstruct address_space *mapping = inode->i_mapping;\n\tunsigned int blkbits = inode->i_blkbits;\n\n\ttrace_ext4_zero_range(inode, offset, len, mode);\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Write out all dirty pages to avoid race conditions\n\t * Then release them.\n\t */\n\tif (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {\n\t\tret = filemap_write_and_wait_range(mapping, offset,\n\t\t\t\t\t\t   offset + len - 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Round up offset. This is not fallocate, we neet to zero out\n\t * blocks, so convert interior block aligned part of the range to\n\t * unwritten and possibly manually zero out unaligned parts of the\n\t * range.\n\t */\n\tstart = round_up(offset, 1 << blkbits);\n\tend = round_down((offset + len), 1 << blkbits);\n\n\tif (start < offset || end > offset + len)\n\t\treturn -EINVAL;\n\tpartial_begin = offset & ((1 << blkbits) - 1);\n\tpartial_end = (offset + len) & ((1 << blkbits) - 1);\n\n\tlblk = start >> blkbits;\n\tmax_blocks = (end >> blkbits);\n\tif (max_blocks < lblk)\n\t\tmax_blocks = 0;\n\telse\n\t\tmax_blocks -= lblk;\n\n\tmutex_lock(&inode->i_mutex);\n\n\t/*\n\t * Indirect files do not support unwritten extnets\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t     offset + len > i_size_read(inode)) {\n\t\tnew_size = offset + len;\n\t\tret = inode_newsize_ok(inode, new_size);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\t}\n\n\tflags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\n\t/* Preallocate the range including the unaligned edges */\n\tif (partial_begin || partial_end) {\n\t\tret = ext4_alloc_file_blocks(file,\n\t\t\t\tround_down(offset, 1 << blkbits) >> blkbits,\n\t\t\t\t(round_up((offset + len), 1 << blkbits) -\n\t\t\t\t round_down(offset, 1 << blkbits)) >> blkbits,\n\t\t\t\tnew_size, flags, mode);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\n\t}\n\n\t/* Zero range excluding the unaligned edges */\n\tif (max_blocks > 0) {\n\t\tflags |= (EXT4_GET_BLOCKS_CONVERT_UNWRITTEN |\n\t\t\t  EXT4_EX_NOCACHE);\n\n\t\t/* Now release the pages and zero block aligned part of pages*/\n\t\ttruncate_pagecache_range(inode, start, end - 1);\n\t\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\n\t\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\t\text4_inode_block_unlocked_dio(inode);\n\t\tinode_dio_wait(inode);\n\n\t\tret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size,\n\t\t\t\t\t     flags, mode);\n\t\tif (ret)\n\t\t\tgoto out_dio;\n\t}\n\tif (!partial_begin && !partial_end)\n\t\tgoto out_dio;\n\n\t/*\n\t * In worst case we have to writeout two nonadjacent unwritten\n\t * blocks and update the inode\n\t */\n\tcredits = (2 * ext4_ext_index_trans_blocks(inode, 2)) + 1;\n\tif (ext4_should_journal_data(inode))\n\t\tcredits += 2;\n\thandle = ext4_journal_start(inode, EXT4_HT_MISC, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\text4_std_error(inode->i_sb, ret);\n\t\tgoto out_dio;\n\t}\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\tif (new_size) {\n\t\text4_update_inode_size(inode, new_size);\n\t} else {\n\t\t/*\n\t\t* Mark that we allocate beyond EOF so the subsequent truncate\n\t\t* can proceed even if the new size is the same as i_size.\n\t\t*/\n\t\tif ((offset + len) > i_size_read(inode))\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\t}\n\text4_mark_inode_dirty(handle, inode);\n\n\t/* Zero out partial block at the edges of the range */\n\tret = ext4_zero_partial_blocks(handle, inode, offset, len);\n\n\tif (file->f_flags & O_SYNC)\n\t\text4_handle_sync(handle);\n\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
        "code_after_change": "static long ext4_zero_range(struct file *file, loff_t offset,\n\t\t\t    loff_t len, int mode)\n{\n\tstruct inode *inode = file_inode(file);\n\thandle_t *handle = NULL;\n\tunsigned int max_blocks;\n\tloff_t new_size = 0;\n\tint ret = 0;\n\tint flags;\n\tint credits;\n\tint partial_begin, partial_end;\n\tloff_t start, end;\n\text4_lblk_t lblk;\n\tunsigned int blkbits = inode->i_blkbits;\n\n\ttrace_ext4_zero_range(inode, offset, len, mode);\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Round up offset. This is not fallocate, we neet to zero out\n\t * blocks, so convert interior block aligned part of the range to\n\t * unwritten and possibly manually zero out unaligned parts of the\n\t * range.\n\t */\n\tstart = round_up(offset, 1 << blkbits);\n\tend = round_down((offset + len), 1 << blkbits);\n\n\tif (start < offset || end > offset + len)\n\t\treturn -EINVAL;\n\tpartial_begin = offset & ((1 << blkbits) - 1);\n\tpartial_end = (offset + len) & ((1 << blkbits) - 1);\n\n\tlblk = start >> blkbits;\n\tmax_blocks = (end >> blkbits);\n\tif (max_blocks < lblk)\n\t\tmax_blocks = 0;\n\telse\n\t\tmax_blocks -= lblk;\n\n\tmutex_lock(&inode->i_mutex);\n\n\t/*\n\t * Indirect files do not support unwritten extnets\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t     offset + len > i_size_read(inode)) {\n\t\tnew_size = offset + len;\n\t\tret = inode_newsize_ok(inode, new_size);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\t}\n\n\tflags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\n\t/* Preallocate the range including the unaligned edges */\n\tif (partial_begin || partial_end) {\n\t\tret = ext4_alloc_file_blocks(file,\n\t\t\t\tround_down(offset, 1 << blkbits) >> blkbits,\n\t\t\t\t(round_up((offset + len), 1 << blkbits) -\n\t\t\t\t round_down(offset, 1 << blkbits)) >> blkbits,\n\t\t\t\tnew_size, flags, mode);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\n\t}\n\n\t/* Zero range excluding the unaligned edges */\n\tif (max_blocks > 0) {\n\t\tflags |= (EXT4_GET_BLOCKS_CONVERT_UNWRITTEN |\n\t\t\t  EXT4_EX_NOCACHE);\n\n\t\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\t\text4_inode_block_unlocked_dio(inode);\n\t\tinode_dio_wait(inode);\n\n\t\t/*\n\t\t * Prevent page faults from reinstantiating pages we have\n\t\t * released from page cache.\n\t\t */\n\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\t\t/* Now release the pages and zero block aligned part of pages */\n\t\ttruncate_pagecache_range(inode, start, end - 1);\n\t\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\n\t\tret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size,\n\t\t\t\t\t     flags, mode);\n\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t\tif (ret)\n\t\t\tgoto out_dio;\n\t}\n\tif (!partial_begin && !partial_end)\n\t\tgoto out_dio;\n\n\t/*\n\t * In worst case we have to writeout two nonadjacent unwritten\n\t * blocks and update the inode\n\t */\n\tcredits = (2 * ext4_ext_index_trans_blocks(inode, 2)) + 1;\n\tif (ext4_should_journal_data(inode))\n\t\tcredits += 2;\n\thandle = ext4_journal_start(inode, EXT4_HT_MISC, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\text4_std_error(inode->i_sb, ret);\n\t\tgoto out_dio;\n\t}\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\tif (new_size) {\n\t\text4_update_inode_size(inode, new_size);\n\t} else {\n\t\t/*\n\t\t* Mark that we allocate beyond EOF so the subsequent truncate\n\t\t* can proceed even if the new size is the same as i_size.\n\t\t*/\n\t\tif ((offset + len) > i_size_read(inode))\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\t}\n\text4_mark_inode_dirty(handle, inode);\n\n\t/* Zero out partial block at the edges of the range */\n\tret = ext4_zero_partial_blocks(handle, inode, offset, len);\n\n\tif (file->f_flags & O_SYNC)\n\t\text4_handle_sync(handle);\n\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,6 @@\n \tint partial_begin, partial_end;\n \tloff_t start, end;\n \text4_lblk_t lblk;\n-\tstruct address_space *mapping = inode->i_mapping;\n \tunsigned int blkbits = inode->i_blkbits;\n \n \ttrace_ext4_zero_range(inode, offset, len, mode);\n@@ -22,17 +21,6 @@\n \t/* Call ext4_force_commit to flush all data in case of data=journal. */\n \tif (ext4_should_journal_data(inode)) {\n \t\tret = ext4_force_commit(inode->i_sb);\n-\t\tif (ret)\n-\t\t\treturn ret;\n-\t}\n-\n-\t/*\n-\t * Write out all dirty pages to avoid race conditions\n-\t * Then release them.\n-\t */\n-\tif (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {\n-\t\tret = filemap_write_and_wait_range(mapping, offset,\n-\t\t\t\t\t\t   offset + len - 1);\n \t\tif (ret)\n \t\t\treturn ret;\n \t}\n@@ -97,16 +85,22 @@\n \t\tflags |= (EXT4_GET_BLOCKS_CONVERT_UNWRITTEN |\n \t\t\t  EXT4_EX_NOCACHE);\n \n-\t\t/* Now release the pages and zero block aligned part of pages*/\n-\t\ttruncate_pagecache_range(inode, start, end - 1);\n-\t\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n-\n \t\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n \t\text4_inode_block_unlocked_dio(inode);\n \t\tinode_dio_wait(inode);\n \n+\t\t/*\n+\t\t * Prevent page faults from reinstantiating pages we have\n+\t\t * released from page cache.\n+\t\t */\n+\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n+\t\t/* Now release the pages and zero block aligned part of pages */\n+\t\ttruncate_pagecache_range(inode, start, end - 1);\n+\t\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n+\n \t\tret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size,\n \t\t\t\t\t     flags, mode);\n+\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n \t\tif (ret)\n \t\t\tgoto out_dio;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t/*",
                "\t\t * Prevent page faults from reinstantiating pages we have",
                "\t\t * released from page cache.",
                "\t\t */",
                "\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);",
                "\t\t/* Now release the pages and zero block aligned part of pages */",
                "\t\ttruncate_pagecache_range(inode, start, end - 1);",
                "\t\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);",
                "",
                "\t\tup_write(&EXT4_I(inode)->i_mmap_sem);"
            ],
            "deleted": [
                "\tstruct address_space *mapping = inode->i_mapping;",
                "\t\tif (ret)",
                "\t\t\treturn ret;",
                "\t}",
                "",
                "\t/*",
                "\t * Write out all dirty pages to avoid race conditions",
                "\t * Then release them.",
                "\t */",
                "\tif (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {",
                "\t\tret = filemap_write_and_wait_range(mapping, offset,",
                "\t\t\t\t\t\t   offset + len - 1);",
                "\t\t/* Now release the pages and zero block aligned part of pages*/",
                "\t\ttruncate_pagecache_range(inode, start, end - 1);",
                "\t\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling."
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "int ext4_collapse_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t punch_start, punch_stop;\n\thandle_t *handle;\n\tunsigned int credits;\n\tloff_t new_size, ioffset;\n\tint ret;\n\n\t/*\n\t * We need to test this early because xfstests assumes that a\n\t * collapse range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support collapse range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Collapse range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t    len & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\ttrace_ext4_collapse_range(inode, offset, len);\n\n\tpunch_start = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tpunch_stop = (offset + len) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Need to round down offset to be aligned with page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\t\t\t   LLONG_MAX);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Take mutex lock */\n\tmutex_lock(&inode->i_mutex);\n\n\t/*\n\t * There is no need to overlap collapse range with EOF, in which case\n\t * it is effectively a truncate operation\n\t */\n\tif (offset + len >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\ttruncate_pagecache(inode, ioffset);\n\n\t/* Wait for existing dio to complete */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_dio;\n\t}\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tret = ext4_es_remove_extent(inode, punch_start,\n\t\t\t\t    EXT_MAX_BLOCKS - punch_start);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tret = ext4_ext_remove_space(inode, punch_start, punch_stop - 1);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\text4_discard_preallocations(inode);\n\n\tret = ext4_ext_shift_extents(inode, handle, punch_stop,\n\t\t\t\t     punch_stop - punch_start, SHIFT_LEFT);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tnew_size = i_size_read(inode) - len;\n\ti_size_write(inode, new_size);\n\tEXT4_I(inode)->i_disksize = new_size;\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\nout_stop:\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
        "code_after_change": "int ext4_collapse_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t punch_start, punch_stop;\n\thandle_t *handle;\n\tunsigned int credits;\n\tloff_t new_size, ioffset;\n\tint ret;\n\n\t/*\n\t * We need to test this early because xfstests assumes that a\n\t * collapse range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support collapse range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Collapse range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t    len & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\ttrace_ext4_collapse_range(inode, offset, len);\n\n\tpunch_start = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tpunch_stop = (offset + len) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Need to round down offset to be aligned with page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\t\t\t   LLONG_MAX);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Take mutex lock */\n\tmutex_lock(&inode->i_mutex);\n\n\t/*\n\t * There is no need to overlap collapse range with EOF, in which case\n\t * it is effectively a truncate operation\n\t */\n\tif (offset + len >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Wait for existing dio to complete */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Prevent page faults from reinstantiating pages we have released from\n\t * page cache.\n\t */\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\ttruncate_pagecache(inode, ioffset);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_mmap;\n\t}\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tret = ext4_es_remove_extent(inode, punch_start,\n\t\t\t\t    EXT_MAX_BLOCKS - punch_start);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tret = ext4_ext_remove_space(inode, punch_start, punch_stop - 1);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\text4_discard_preallocations(inode);\n\n\tret = ext4_ext_shift_extents(inode, handle, punch_stop,\n\t\t\t\t     punch_stop - punch_start, SHIFT_LEFT);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tnew_size = i_size_read(inode) - len;\n\ti_size_write(inode, new_size);\n\tEXT4_I(inode)->i_disksize = new_size;\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\nout_stop:\n\text4_journal_stop(handle);\nout_mmap:\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -65,17 +65,22 @@\n \t\tgoto out_mutex;\n \t}\n \n-\ttruncate_pagecache(inode, ioffset);\n-\n \t/* Wait for existing dio to complete */\n \text4_inode_block_unlocked_dio(inode);\n \tinode_dio_wait(inode);\n+\n+\t/*\n+\t * Prevent page faults from reinstantiating pages we have released from\n+\t * page cache.\n+\t */\n+\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n+\ttruncate_pagecache(inode, ioffset);\n \n \tcredits = ext4_writepage_trans_blocks(inode);\n \thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n \tif (IS_ERR(handle)) {\n \t\tret = PTR_ERR(handle);\n-\t\tgoto out_dio;\n+\t\tgoto out_mmap;\n \t}\n \n \tdown_write(&EXT4_I(inode)->i_data_sem);\n@@ -114,7 +119,8 @@\n \n out_stop:\n \text4_journal_stop(handle);\n-out_dio:\n+out_mmap:\n+\tup_write(&EXT4_I(inode)->i_mmap_sem);\n \text4_inode_resume_unlocked_dio(inode);\n out_mutex:\n \tmutex_unlock(&inode->i_mutex);",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * Prevent page faults from reinstantiating pages we have released from",
                "\t * page cache.",
                "\t */",
                "\tdown_write(&EXT4_I(inode)->i_mmap_sem);",
                "\ttruncate_pagecache(inode, ioffset);",
                "\t\tgoto out_mmap;",
                "out_mmap:",
                "\tup_write(&EXT4_I(inode)->i_mmap_sem);"
            ],
            "deleted": [
                "\ttruncate_pagecache(inode, ioffset);",
                "",
                "\t\tgoto out_dio;",
                "out_dio:"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling."
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "int ext4_insert_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\thandle_t *handle;\n\tstruct ext4_ext_path *path;\n\tstruct ext4_extent *extent;\n\text4_lblk_t offset_lblk, len_lblk, ee_start_lblk = 0;\n\tunsigned int credits, ee_len;\n\tint ret = 0, depth, split_flag = 0;\n\tloff_t ioffset;\n\n\t/*\n\t * We need to test this early because xfstests assumes that an\n\t * insert range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support insert range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Insert range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t\t\tlen & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_insert_range(inode, offset, len);\n\n\toffset_lblk = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tlen_lblk = len >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Need to round down to align start offset to page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\tLLONG_MAX);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Take mutex lock */\n\tmutex_lock(&inode->i_mutex);\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Check for wrap through zero */\n\tif (inode->i_size + len > inode->i_sb->s_maxbytes) {\n\t\tret = -EFBIG;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Offset should be less than i_size */\n\tif (offset >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\ttruncate_pagecache(inode, ioffset);\n\n\t/* Wait for existing dio to complete */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_dio;\n\t}\n\n\t/* Expand file to avoid data loss if there is error while shifting */\n\tinode->i_size += len;\n\tEXT4_I(inode)->i_disksize += len;\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\tret = ext4_mark_inode_dirty(handle, inode);\n\tif (ret)\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tpath = ext4_find_extent(inode, offset_lblk, NULL, 0);\n\tif (IS_ERR(path)) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tdepth = ext_depth(inode);\n\textent = path[depth].p_ext;\n\tif (extent) {\n\t\tee_start_lblk = le32_to_cpu(extent->ee_block);\n\t\tee_len = ext4_ext_get_actual_len(extent);\n\n\t\t/*\n\t\t * If offset_lblk is not the starting block of extent, split\n\t\t * the extent @offset_lblk\n\t\t */\n\t\tif ((offset_lblk > ee_start_lblk) &&\n\t\t\t\t(offset_lblk < (ee_start_lblk + ee_len))) {\n\t\t\tif (ext4_ext_is_unwritten(extent))\n\t\t\t\tsplit_flag = EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t\t\tEXT4_EXT_MARK_UNWRIT2;\n\t\t\tret = ext4_split_extent_at(handle, inode, &path,\n\t\t\t\t\toffset_lblk, split_flag,\n\t\t\t\t\tEXT4_EX_NOCACHE |\n\t\t\t\t\tEXT4_GET_BLOCKS_PRE_IO |\n\t\t\t\t\tEXT4_GET_BLOCKS_METADATA_NOFAIL);\n\t\t}\n\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t\tif (ret < 0) {\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tgoto out_stop;\n\t\t}\n\t}\n\n\tret = ext4_es_remove_extent(inode, offset_lblk,\n\t\t\tEXT_MAX_BLOCKS - offset_lblk);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\t/*\n\t * if offset_lblk lies in a hole which is at start of file, use\n\t * ee_start_lblk to shift extents\n\t */\n\tret = ext4_ext_shift_extents(inode, handle,\n\t\tee_start_lblk > offset_lblk ? ee_start_lblk : offset_lblk,\n\t\tlen_lblk, SHIFT_RIGHT);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\nout_stop:\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
        "code_after_change": "int ext4_insert_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\thandle_t *handle;\n\tstruct ext4_ext_path *path;\n\tstruct ext4_extent *extent;\n\text4_lblk_t offset_lblk, len_lblk, ee_start_lblk = 0;\n\tunsigned int credits, ee_len;\n\tint ret = 0, depth, split_flag = 0;\n\tloff_t ioffset;\n\n\t/*\n\t * We need to test this early because xfstests assumes that an\n\t * insert range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support insert range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Insert range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t\t\tlen & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_insert_range(inode, offset, len);\n\n\toffset_lblk = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tlen_lblk = len >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Need to round down to align start offset to page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\tLLONG_MAX);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Take mutex lock */\n\tmutex_lock(&inode->i_mutex);\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Check for wrap through zero */\n\tif (inode->i_size + len > inode->i_sb->s_maxbytes) {\n\t\tret = -EFBIG;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Offset should be less than i_size */\n\tif (offset >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Wait for existing dio to complete */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Prevent page faults from reinstantiating pages we have released from\n\t * page cache.\n\t */\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\ttruncate_pagecache(inode, ioffset);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_mmap;\n\t}\n\n\t/* Expand file to avoid data loss if there is error while shifting */\n\tinode->i_size += len;\n\tEXT4_I(inode)->i_disksize += len;\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\tret = ext4_mark_inode_dirty(handle, inode);\n\tif (ret)\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tpath = ext4_find_extent(inode, offset_lblk, NULL, 0);\n\tif (IS_ERR(path)) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tdepth = ext_depth(inode);\n\textent = path[depth].p_ext;\n\tif (extent) {\n\t\tee_start_lblk = le32_to_cpu(extent->ee_block);\n\t\tee_len = ext4_ext_get_actual_len(extent);\n\n\t\t/*\n\t\t * If offset_lblk is not the starting block of extent, split\n\t\t * the extent @offset_lblk\n\t\t */\n\t\tif ((offset_lblk > ee_start_lblk) &&\n\t\t\t\t(offset_lblk < (ee_start_lblk + ee_len))) {\n\t\t\tif (ext4_ext_is_unwritten(extent))\n\t\t\t\tsplit_flag = EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t\t\tEXT4_EXT_MARK_UNWRIT2;\n\t\t\tret = ext4_split_extent_at(handle, inode, &path,\n\t\t\t\t\toffset_lblk, split_flag,\n\t\t\t\t\tEXT4_EX_NOCACHE |\n\t\t\t\t\tEXT4_GET_BLOCKS_PRE_IO |\n\t\t\t\t\tEXT4_GET_BLOCKS_METADATA_NOFAIL);\n\t\t}\n\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t\tif (ret < 0) {\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tgoto out_stop;\n\t\t}\n\t}\n\n\tret = ext4_es_remove_extent(inode, offset_lblk,\n\t\t\tEXT_MAX_BLOCKS - offset_lblk);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\t/*\n\t * if offset_lblk lies in a hole which is at start of file, use\n\t * ee_start_lblk to shift extents\n\t */\n\tret = ext4_ext_shift_extents(inode, handle,\n\t\tee_start_lblk > offset_lblk ? ee_start_lblk : offset_lblk,\n\t\tlen_lblk, SHIFT_RIGHT);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\nout_stop:\n\text4_journal_stop(handle);\nout_mmap:\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -70,17 +70,22 @@\n \t\tgoto out_mutex;\n \t}\n \n-\ttruncate_pagecache(inode, ioffset);\n-\n \t/* Wait for existing dio to complete */\n \text4_inode_block_unlocked_dio(inode);\n \tinode_dio_wait(inode);\n+\n+\t/*\n+\t * Prevent page faults from reinstantiating pages we have released from\n+\t * page cache.\n+\t */\n+\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n+\ttruncate_pagecache(inode, ioffset);\n \n \tcredits = ext4_writepage_trans_blocks(inode);\n \thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n \tif (IS_ERR(handle)) {\n \t\tret = PTR_ERR(handle);\n-\t\tgoto out_dio;\n+\t\tgoto out_mmap;\n \t}\n \n \t/* Expand file to avoid data loss if there is error while shifting */\n@@ -151,7 +156,8 @@\n \n out_stop:\n \text4_journal_stop(handle);\n-out_dio:\n+out_mmap:\n+\tup_write(&EXT4_I(inode)->i_mmap_sem);\n \text4_inode_resume_unlocked_dio(inode);\n out_mutex:\n \tmutex_unlock(&inode->i_mutex);",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * Prevent page faults from reinstantiating pages we have released from",
                "\t * page cache.",
                "\t */",
                "\tdown_write(&EXT4_I(inode)->i_mmap_sem);",
                "\ttruncate_pagecache(inode, ioffset);",
                "\t\tgoto out_mmap;",
                "out_mmap:",
                "\tup_write(&EXT4_I(inode)->i_mmap_sem);"
            ],
            "deleted": [
                "\ttruncate_pagecache(inode, ioffset);",
                "",
                "\t\tgoto out_dio;",
                "out_dio:"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling."
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "static int ext4_dax_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tint result;\n\thandle_t *handle = NULL;\n\tstruct super_block *sb = file_inode(vma->vm_file)->i_sb;\n\tbool write = vmf->flags & FAULT_FLAG_WRITE;\n\n\tif (write) {\n\t\tsb_start_pagefault(sb);\n\t\tfile_update_time(vma->vm_file);\n\t\thandle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,\n\t\t\t\t\t\tEXT4_DATA_TRANS_BLOCKS(sb));\n\t}\n\n\tif (IS_ERR(handle))\n\t\tresult = VM_FAULT_SIGBUS;\n\telse\n\t\tresult = __dax_fault(vma, vmf, ext4_get_block_dax,\n\t\t\t\t\t\text4_end_io_unwritten);\n\n\tif (write) {\n\t\tif (!IS_ERR(handle))\n\t\t\text4_journal_stop(handle);\n\t\tsb_end_pagefault(sb);\n\t}\n\n\treturn result;\n}",
        "code_after_change": "static int ext4_dax_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tint result;\n\thandle_t *handle = NULL;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tstruct super_block *sb = inode->i_sb;\n\tbool write = vmf->flags & FAULT_FLAG_WRITE;\n\n\tif (write) {\n\t\tsb_start_pagefault(sb);\n\t\tfile_update_time(vma->vm_file);\n\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\t\thandle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,\n\t\t\t\t\t\tEXT4_DATA_TRANS_BLOCKS(sb));\n\t} else\n\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\n\tif (IS_ERR(handle))\n\t\tresult = VM_FAULT_SIGBUS;\n\telse\n\t\tresult = __dax_fault(vma, vmf, ext4_get_block_dax,\n\t\t\t\t\t\text4_end_io_unwritten);\n\n\tif (write) {\n\t\tif (!IS_ERR(handle))\n\t\t\text4_journal_stop(handle);\n\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\t\tsb_end_pagefault(sb);\n\t} else\n\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\n\treturn result;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,15 +2,18 @@\n {\n \tint result;\n \thandle_t *handle = NULL;\n-\tstruct super_block *sb = file_inode(vma->vm_file)->i_sb;\n+\tstruct inode *inode = file_inode(vma->vm_file);\n+\tstruct super_block *sb = inode->i_sb;\n \tbool write = vmf->flags & FAULT_FLAG_WRITE;\n \n \tif (write) {\n \t\tsb_start_pagefault(sb);\n \t\tfile_update_time(vma->vm_file);\n+\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n \t\thandle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,\n \t\t\t\t\t\tEXT4_DATA_TRANS_BLOCKS(sb));\n-\t}\n+\t} else\n+\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n \n \tif (IS_ERR(handle))\n \t\tresult = VM_FAULT_SIGBUS;\n@@ -21,8 +24,10 @@\n \tif (write) {\n \t\tif (!IS_ERR(handle))\n \t\t\text4_journal_stop(handle);\n+\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n \t\tsb_end_pagefault(sb);\n-\t}\n+\t} else\n+\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n \n \treturn result;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct inode *inode = file_inode(vma->vm_file);",
                "\tstruct super_block *sb = inode->i_sb;",
                "\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);",
                "\t} else",
                "\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);",
                "\t\tup_read(&EXT4_I(inode)->i_mmap_sem);",
                "\t} else",
                "\t\tup_read(&EXT4_I(inode)->i_mmap_sem);"
            ],
            "deleted": [
                "\tstruct super_block *sb = file_inode(vma->vm_file)->i_sb;",
                "\t}",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling."
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "static int ext4_dax_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\treturn dax_mkwrite(vma, vmf, ext4_get_block_dax,\n\t\t\t\text4_end_io_unwritten);\n}",
        "code_after_change": "static int ext4_dax_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tint err;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\n\tsb_start_pagefault(inode->i_sb);\n\tfile_update_time(vma->vm_file);\n\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\terr = __dax_mkwrite(vma, vmf, ext4_get_block_dax,\n\t\t\t    ext4_end_io_unwritten);\n\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\tsb_end_pagefault(inode->i_sb);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,15 @@\n static int ext4_dax_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\n {\n-\treturn dax_mkwrite(vma, vmf, ext4_get_block_dax,\n-\t\t\t\text4_end_io_unwritten);\n+\tint err;\n+\tstruct inode *inode = file_inode(vma->vm_file);\n+\n+\tsb_start_pagefault(inode->i_sb);\n+\tfile_update_time(vma->vm_file);\n+\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n+\terr = __dax_mkwrite(vma, vmf, ext4_get_block_dax,\n+\t\t\t    ext4_end_io_unwritten);\n+\tup_read(&EXT4_I(inode)->i_mmap_sem);\n+\tsb_end_pagefault(inode->i_sb);\n+\n+\treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tint err;",
                "\tstruct inode *inode = file_inode(vma->vm_file);",
                "",
                "\tsb_start_pagefault(inode->i_sb);",
                "\tfile_update_time(vma->vm_file);",
                "\tdown_read(&EXT4_I(inode)->i_mmap_sem);",
                "\terr = __dax_mkwrite(vma, vmf, ext4_get_block_dax,",
                "\t\t\t    ext4_end_io_unwritten);",
                "\tup_read(&EXT4_I(inode)->i_mmap_sem);",
                "\tsb_end_pagefault(inode->i_sb);",
                "",
                "\treturn err;"
            ],
            "deleted": [
                "\treturn dax_mkwrite(vma, vmf, ext4_get_block_dax,",
                "\t\t\t\text4_end_io_unwritten);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling."
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "static int ext4_dax_pmd_fault(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\t\tpmd_t *pmd, unsigned int flags)\n{\n\tint result;\n\thandle_t *handle = NULL;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tstruct super_block *sb = inode->i_sb;\n\tbool write = flags & FAULT_FLAG_WRITE;\n\n\tif (write) {\n\t\tsb_start_pagefault(sb);\n\t\tfile_update_time(vma->vm_file);\n\t\thandle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,\n\t\t\t\text4_chunk_trans_blocks(inode,\n\t\t\t\t\t\t\tPMD_SIZE / PAGE_SIZE));\n\t}\n\n\tif (IS_ERR(handle))\n\t\tresult = VM_FAULT_SIGBUS;\n\telse\n\t\tresult = __dax_pmd_fault(vma, addr, pmd, flags,\n\t\t\t\text4_get_block_dax, ext4_end_io_unwritten);\n\n\tif (write) {\n\t\tif (!IS_ERR(handle))\n\t\t\text4_journal_stop(handle);\n\t\tsb_end_pagefault(sb);\n\t}\n\n\treturn result;\n}",
        "code_after_change": "static int ext4_dax_pmd_fault(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\t\tpmd_t *pmd, unsigned int flags)\n{\n\tint result;\n\thandle_t *handle = NULL;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tstruct super_block *sb = inode->i_sb;\n\tbool write = flags & FAULT_FLAG_WRITE;\n\n\tif (write) {\n\t\tsb_start_pagefault(sb);\n\t\tfile_update_time(vma->vm_file);\n\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\t\thandle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,\n\t\t\t\text4_chunk_trans_blocks(inode,\n\t\t\t\t\t\t\tPMD_SIZE / PAGE_SIZE));\n\t} else\n\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\n\tif (IS_ERR(handle))\n\t\tresult = VM_FAULT_SIGBUS;\n\telse\n\t\tresult = __dax_pmd_fault(vma, addr, pmd, flags,\n\t\t\t\text4_get_block_dax, ext4_end_io_unwritten);\n\n\tif (write) {\n\t\tif (!IS_ERR(handle))\n\t\t\text4_journal_stop(handle);\n\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\t\tsb_end_pagefault(sb);\n\t} else\n\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\n\treturn result;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,10 +10,12 @@\n \tif (write) {\n \t\tsb_start_pagefault(sb);\n \t\tfile_update_time(vma->vm_file);\n+\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n \t\thandle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,\n \t\t\t\text4_chunk_trans_blocks(inode,\n \t\t\t\t\t\t\tPMD_SIZE / PAGE_SIZE));\n-\t}\n+\t} else\n+\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n \n \tif (IS_ERR(handle))\n \t\tresult = VM_FAULT_SIGBUS;\n@@ -24,8 +26,10 @@\n \tif (write) {\n \t\tif (!IS_ERR(handle))\n \t\t\text4_journal_stop(handle);\n+\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n \t\tsb_end_pagefault(sb);\n-\t}\n+\t} else\n+\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n \n \treturn result;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);",
                "\t} else",
                "\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);",
                "\t\tup_read(&EXT4_I(inode)->i_mmap_sem);",
                "\t} else",
                "\t\tup_read(&EXT4_I(inode)->i_mmap_sem);"
            ],
            "deleted": [
                "\t}",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling."
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "int ext4_punch_hole(struct inode *inode, loff_t offset, loff_t length)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t first_block, stop_block;\n\tstruct address_space *mapping = inode->i_mapping;\n\tloff_t first_block_offset, last_block_offset;\n\thandle_t *handle;\n\tunsigned int credits;\n\tint ret = 0;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_punch_hole(inode, offset, length, 0);\n\n\t/*\n\t * Write out all dirty pages to avoid race conditions\n\t * Then release them.\n\t */\n\tif (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {\n\t\tret = filemap_write_and_wait_range(mapping, offset,\n\t\t\t\t\t\t   offset + length - 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tmutex_lock(&inode->i_mutex);\n\n\t/* No need to punch hole beyond i_size */\n\tif (offset >= inode->i_size)\n\t\tgoto out_mutex;\n\n\t/*\n\t * If the hole extends beyond i_size, set the hole\n\t * to end after the page that contains i_size\n\t */\n\tif (offset + length > inode->i_size) {\n\t\tlength = inode->i_size +\n\t\t   PAGE_CACHE_SIZE - (inode->i_size & (PAGE_CACHE_SIZE - 1)) -\n\t\t   offset;\n\t}\n\n\tif (offset & (sb->s_blocksize - 1) ||\n\t    (offset + length) & (sb->s_blocksize - 1)) {\n\t\t/*\n\t\t * Attach jinode to inode for jbd2 if we do any zeroing of\n\t\t * partial block\n\t\t */\n\t\tret = ext4_inode_attach_jinode(inode);\n\t\tif (ret < 0)\n\t\t\tgoto out_mutex;\n\n\t}\n\n\tfirst_block_offset = round_up(offset, sb->s_blocksize);\n\tlast_block_offset = round_down((offset + length), sb->s_blocksize) - 1;\n\n\t/* Now release the pages and zero block aligned part of pages*/\n\tif (last_block_offset > first_block_offset)\n\t\ttruncate_pagecache_range(inode, first_block_offset,\n\t\t\t\t\t last_block_offset);\n\n\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tcredits = ext4_writepage_trans_blocks(inode);\n\telse\n\t\tcredits = ext4_blocks_for_truncate(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\text4_std_error(sb, ret);\n\t\tgoto out_dio;\n\t}\n\n\tret = ext4_zero_partial_blocks(handle, inode, offset,\n\t\t\t\t       length);\n\tif (ret)\n\t\tgoto out_stop;\n\n\tfirst_block = (offset + sb->s_blocksize - 1) >>\n\t\tEXT4_BLOCK_SIZE_BITS(sb);\n\tstop_block = (offset + length) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* If there are no blocks to remove, return now */\n\tif (first_block >= stop_block)\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tret = ext4_es_remove_extent(inode, first_block,\n\t\t\t\t    stop_block - first_block);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tret = ext4_ext_remove_space(inode, first_block,\n\t\t\t\t\t    stop_block - 1);\n\telse\n\t\tret = ext4_ind_remove_space(handle, inode, first_block,\n\t\t\t\t\t    stop_block);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\n\t/* Now release the pages again to reduce race window */\n\tif (last_block_offset > first_block_offset)\n\t\ttruncate_pagecache_range(inode, first_block_offset,\n\t\t\t\t\t last_block_offset);\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\nout_stop:\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
        "code_after_change": "int ext4_punch_hole(struct inode *inode, loff_t offset, loff_t length)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t first_block, stop_block;\n\tstruct address_space *mapping = inode->i_mapping;\n\tloff_t first_block_offset, last_block_offset;\n\thandle_t *handle;\n\tunsigned int credits;\n\tint ret = 0;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_punch_hole(inode, offset, length, 0);\n\n\t/*\n\t * Write out all dirty pages to avoid race conditions\n\t * Then release them.\n\t */\n\tif (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {\n\t\tret = filemap_write_and_wait_range(mapping, offset,\n\t\t\t\t\t\t   offset + length - 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tmutex_lock(&inode->i_mutex);\n\n\t/* No need to punch hole beyond i_size */\n\tif (offset >= inode->i_size)\n\t\tgoto out_mutex;\n\n\t/*\n\t * If the hole extends beyond i_size, set the hole\n\t * to end after the page that contains i_size\n\t */\n\tif (offset + length > inode->i_size) {\n\t\tlength = inode->i_size +\n\t\t   PAGE_CACHE_SIZE - (inode->i_size & (PAGE_CACHE_SIZE - 1)) -\n\t\t   offset;\n\t}\n\n\tif (offset & (sb->s_blocksize - 1) ||\n\t    (offset + length) & (sb->s_blocksize - 1)) {\n\t\t/*\n\t\t * Attach jinode to inode for jbd2 if we do any zeroing of\n\t\t * partial block\n\t\t */\n\t\tret = ext4_inode_attach_jinode(inode);\n\t\tif (ret < 0)\n\t\t\tgoto out_mutex;\n\n\t}\n\n\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Prevent page faults from reinstantiating pages we have released from\n\t * page cache.\n\t */\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\tfirst_block_offset = round_up(offset, sb->s_blocksize);\n\tlast_block_offset = round_down((offset + length), sb->s_blocksize) - 1;\n\n\t/* Now release the pages and zero block aligned part of pages*/\n\tif (last_block_offset > first_block_offset)\n\t\ttruncate_pagecache_range(inode, first_block_offset,\n\t\t\t\t\t last_block_offset);\n\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tcredits = ext4_writepage_trans_blocks(inode);\n\telse\n\t\tcredits = ext4_blocks_for_truncate(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\text4_std_error(sb, ret);\n\t\tgoto out_dio;\n\t}\n\n\tret = ext4_zero_partial_blocks(handle, inode, offset,\n\t\t\t\t       length);\n\tif (ret)\n\t\tgoto out_stop;\n\n\tfirst_block = (offset + sb->s_blocksize - 1) >>\n\t\tEXT4_BLOCK_SIZE_BITS(sb);\n\tstop_block = (offset + length) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* If there are no blocks to remove, return now */\n\tif (first_block >= stop_block)\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tret = ext4_es_remove_extent(inode, first_block,\n\t\t\t\t    stop_block - first_block);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\tret = ext4_ext_remove_space(inode, first_block,\n\t\t\t\t\t    stop_block - 1);\n\telse\n\t\tret = ext4_ind_remove_space(handle, inode, first_block,\n\t\t\t\t\t    stop_block);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\nout_stop:\n\text4_journal_stop(handle);\nout_dio:\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -52,6 +52,15 @@\n \n \t}\n \n+\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n+\text4_inode_block_unlocked_dio(inode);\n+\tinode_dio_wait(inode);\n+\n+\t/*\n+\t * Prevent page faults from reinstantiating pages we have released from\n+\t * page cache.\n+\t */\n+\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n \tfirst_block_offset = round_up(offset, sb->s_blocksize);\n \tlast_block_offset = round_down((offset + length), sb->s_blocksize) - 1;\n \n@@ -59,10 +68,6 @@\n \tif (last_block_offset > first_block_offset)\n \t\ttruncate_pagecache_range(inode, first_block_offset,\n \t\t\t\t\t last_block_offset);\n-\n-\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n-\text4_inode_block_unlocked_dio(inode);\n-\tinode_dio_wait(inode);\n \n \tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n \t\tcredits = ext4_writepage_trans_blocks(inode);\n@@ -109,16 +114,12 @@\n \tif (IS_SYNC(inode))\n \t\text4_handle_sync(handle);\n \n-\t/* Now release the pages again to reduce race window */\n-\tif (last_block_offset > first_block_offset)\n-\t\ttruncate_pagecache_range(inode, first_block_offset,\n-\t\t\t\t\t last_block_offset);\n-\n \tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n \text4_mark_inode_dirty(handle, inode);\n out_stop:\n \text4_journal_stop(handle);\n out_dio:\n+\tup_write(&EXT4_I(inode)->i_mmap_sem);\n \text4_inode_resume_unlocked_dio(inode);\n out_mutex:\n \tmutex_unlock(&inode->i_mutex);",
        "function_modified_lines": {
            "added": [
                "\t/* Wait all existing dio workers, newcomers will block on i_mutex */",
                "\text4_inode_block_unlocked_dio(inode);",
                "\tinode_dio_wait(inode);",
                "",
                "\t/*",
                "\t * Prevent page faults from reinstantiating pages we have released from",
                "\t * page cache.",
                "\t */",
                "\tdown_write(&EXT4_I(inode)->i_mmap_sem);",
                "\tup_write(&EXT4_I(inode)->i_mmap_sem);"
            ],
            "deleted": [
                "",
                "\t/* Wait all existing dio workers, newcomers will block on i_mutex */",
                "\text4_inode_block_unlocked_dio(inode);",
                "\tinode_dio_wait(inode);",
                "\t/* Now release the pages again to reduce race window */",
                "\tif (last_block_offset > first_block_offset)",
                "\t\ttruncate_pagecache_range(inode, first_block_offset,",
                "\t\t\t\t\t last_block_offset);",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling."
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "int ext4_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tint error, rc = 0;\n\tint orphan = 0;\n\tconst unsigned int ia_valid = attr->ia_valid;\n\n\terror = inode_change_ok(inode, attr);\n\tif (error)\n\t\treturn error;\n\n\tif (is_quota_modification(inode, attr)) {\n\t\terror = dquot_initialize(inode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tif ((ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid)) ||\n\t    (ia_valid & ATTR_GID && !gid_eq(attr->ia_gid, inode->i_gid))) {\n\t\thandle_t *handle;\n\n\t\t/* (user+group)*(old+new) structure, inode write (sb,\n\t\t * inode block, ? - but truncate inode update has it) */\n\t\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA,\n\t\t\t(EXT4_MAXQUOTAS_INIT_BLOCKS(inode->i_sb) +\n\t\t\t EXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb)) + 3);\n\t\tif (IS_ERR(handle)) {\n\t\t\terror = PTR_ERR(handle);\n\t\t\tgoto err_out;\n\t\t}\n\t\terror = dquot_transfer(inode, attr);\n\t\tif (error) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn error;\n\t\t}\n\t\t/* Update corresponding info in inode so that everything is in\n\t\t * one transaction */\n\t\tif (attr->ia_valid & ATTR_UID)\n\t\t\tinode->i_uid = attr->ia_uid;\n\t\tif (attr->ia_valid & ATTR_GID)\n\t\t\tinode->i_gid = attr->ia_gid;\n\t\terror = ext4_mark_inode_dirty(handle, inode);\n\t\text4_journal_stop(handle);\n\t}\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\thandle_t *handle;\n\t\tloff_t oldsize = inode->i_size;\n\t\tint shrink = (attr->ia_size <= inode->i_size);\n\n\t\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t\t\tif (attr->ia_size > sbi->s_bitmap_maxbytes)\n\t\t\t\treturn -EFBIG;\n\t\t}\n\t\tif (!S_ISREG(inode->i_mode))\n\t\t\treturn -EINVAL;\n\n\t\tif (IS_I_VERSION(inode) && attr->ia_size != inode->i_size)\n\t\t\tinode_inc_iversion(inode);\n\n\t\tif (ext4_should_order_data(inode) &&\n\t\t    (attr->ia_size < inode->i_size)) {\n\t\t\terror = ext4_begin_ordered_truncate(inode,\n\t\t\t\t\t\t\t    attr->ia_size);\n\t\t\tif (error)\n\t\t\t\tgoto err_out;\n\t\t}\n\t\tif (attr->ia_size != inode->i_size) {\n\t\t\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 3);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\terror = PTR_ERR(handle);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t\tif (ext4_handle_valid(handle) && shrink) {\n\t\t\t\terror = ext4_orphan_add(handle, inode);\n\t\t\t\torphan = 1;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Update c/mtime on truncate up, ext4_truncate() will\n\t\t\t * update c/mtime in shrink case below\n\t\t\t */\n\t\t\tif (!shrink) {\n\t\t\t\tinode->i_mtime = ext4_current_time(inode);\n\t\t\t\tinode->i_ctime = inode->i_mtime;\n\t\t\t}\n\t\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tEXT4_I(inode)->i_disksize = attr->ia_size;\n\t\t\trc = ext4_mark_inode_dirty(handle, inode);\n\t\t\tif (!error)\n\t\t\t\terror = rc;\n\t\t\t/*\n\t\t\t * We have to update i_size under i_data_sem together\n\t\t\t * with i_disksize to avoid races with writeback code\n\t\t\t * running ext4_wb_update_i_disksize().\n\t\t\t */\n\t\t\tif (!error)\n\t\t\t\ti_size_write(inode, attr->ia_size);\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\text4_journal_stop(handle);\n\t\t\tif (error) {\n\t\t\t\tif (orphan)\n\t\t\t\t\text4_orphan_del(NULL, inode);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t}\n\t\tif (!shrink)\n\t\t\tpagecache_isize_extended(inode, oldsize, inode->i_size);\n\n\t\t/*\n\t\t * Blocks are going to be removed from the inode. Wait\n\t\t * for dio in flight.  Temporarily disable\n\t\t * dioread_nolock to prevent livelock.\n\t\t */\n\t\tif (orphan) {\n\t\t\tif (!ext4_should_journal_data(inode)) {\n\t\t\t\text4_inode_block_unlocked_dio(inode);\n\t\t\t\tinode_dio_wait(inode);\n\t\t\t\text4_inode_resume_unlocked_dio(inode);\n\t\t\t} else\n\t\t\t\text4_wait_for_tail_page_commit(inode);\n\t\t}\n\t\t/*\n\t\t * Truncate pagecache after we've waited for commit\n\t\t * in data=journal mode to make pages freeable.\n\t\t */\n\t\ttruncate_pagecache(inode, inode->i_size);\n\t\tif (shrink)\n\t\t\text4_truncate(inode);\n\t}\n\n\tif (!rc) {\n\t\tsetattr_copy(inode, attr);\n\t\tmark_inode_dirty(inode);\n\t}\n\n\t/*\n\t * If the call to ext4_truncate failed to get a transaction handle at\n\t * all, we need to clean up the in-core orphan list manually.\n\t */\n\tif (orphan && inode->i_nlink)\n\t\text4_orphan_del(NULL, inode);\n\n\tif (!rc && (ia_valid & ATTR_MODE))\n\t\trc = posix_acl_chmod(inode, inode->i_mode);\n\nerr_out:\n\text4_std_error(inode->i_sb, error);\n\tif (!error)\n\t\terror = rc;\n\treturn error;\n}",
        "code_after_change": "int ext4_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tint error, rc = 0;\n\tint orphan = 0;\n\tconst unsigned int ia_valid = attr->ia_valid;\n\n\terror = inode_change_ok(inode, attr);\n\tif (error)\n\t\treturn error;\n\n\tif (is_quota_modification(inode, attr)) {\n\t\terror = dquot_initialize(inode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tif ((ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid)) ||\n\t    (ia_valid & ATTR_GID && !gid_eq(attr->ia_gid, inode->i_gid))) {\n\t\thandle_t *handle;\n\n\t\t/* (user+group)*(old+new) structure, inode write (sb,\n\t\t * inode block, ? - but truncate inode update has it) */\n\t\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA,\n\t\t\t(EXT4_MAXQUOTAS_INIT_BLOCKS(inode->i_sb) +\n\t\t\t EXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb)) + 3);\n\t\tif (IS_ERR(handle)) {\n\t\t\terror = PTR_ERR(handle);\n\t\t\tgoto err_out;\n\t\t}\n\t\terror = dquot_transfer(inode, attr);\n\t\tif (error) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn error;\n\t\t}\n\t\t/* Update corresponding info in inode so that everything is in\n\t\t * one transaction */\n\t\tif (attr->ia_valid & ATTR_UID)\n\t\t\tinode->i_uid = attr->ia_uid;\n\t\tif (attr->ia_valid & ATTR_GID)\n\t\t\tinode->i_gid = attr->ia_gid;\n\t\terror = ext4_mark_inode_dirty(handle, inode);\n\t\text4_journal_stop(handle);\n\t}\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\thandle_t *handle;\n\t\tloff_t oldsize = inode->i_size;\n\t\tint shrink = (attr->ia_size <= inode->i_size);\n\n\t\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t\t\tif (attr->ia_size > sbi->s_bitmap_maxbytes)\n\t\t\t\treturn -EFBIG;\n\t\t}\n\t\tif (!S_ISREG(inode->i_mode))\n\t\t\treturn -EINVAL;\n\n\t\tif (IS_I_VERSION(inode) && attr->ia_size != inode->i_size)\n\t\t\tinode_inc_iversion(inode);\n\n\t\tif (ext4_should_order_data(inode) &&\n\t\t    (attr->ia_size < inode->i_size)) {\n\t\t\terror = ext4_begin_ordered_truncate(inode,\n\t\t\t\t\t\t\t    attr->ia_size);\n\t\t\tif (error)\n\t\t\t\tgoto err_out;\n\t\t}\n\t\tif (attr->ia_size != inode->i_size) {\n\t\t\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 3);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\terror = PTR_ERR(handle);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t\tif (ext4_handle_valid(handle) && shrink) {\n\t\t\t\terror = ext4_orphan_add(handle, inode);\n\t\t\t\torphan = 1;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Update c/mtime on truncate up, ext4_truncate() will\n\t\t\t * update c/mtime in shrink case below\n\t\t\t */\n\t\t\tif (!shrink) {\n\t\t\t\tinode->i_mtime = ext4_current_time(inode);\n\t\t\t\tinode->i_ctime = inode->i_mtime;\n\t\t\t}\n\t\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tEXT4_I(inode)->i_disksize = attr->ia_size;\n\t\t\trc = ext4_mark_inode_dirty(handle, inode);\n\t\t\tif (!error)\n\t\t\t\terror = rc;\n\t\t\t/*\n\t\t\t * We have to update i_size under i_data_sem together\n\t\t\t * with i_disksize to avoid races with writeback code\n\t\t\t * running ext4_wb_update_i_disksize().\n\t\t\t */\n\t\t\tif (!error)\n\t\t\t\ti_size_write(inode, attr->ia_size);\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\text4_journal_stop(handle);\n\t\t\tif (error) {\n\t\t\t\tif (orphan)\n\t\t\t\t\text4_orphan_del(NULL, inode);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t}\n\t\tif (!shrink)\n\t\t\tpagecache_isize_extended(inode, oldsize, inode->i_size);\n\n\t\t/*\n\t\t * Blocks are going to be removed from the inode. Wait\n\t\t * for dio in flight.  Temporarily disable\n\t\t * dioread_nolock to prevent livelock.\n\t\t */\n\t\tif (orphan) {\n\t\t\tif (!ext4_should_journal_data(inode)) {\n\t\t\t\text4_inode_block_unlocked_dio(inode);\n\t\t\t\tinode_dio_wait(inode);\n\t\t\t\text4_inode_resume_unlocked_dio(inode);\n\t\t\t} else\n\t\t\t\text4_wait_for_tail_page_commit(inode);\n\t\t}\n\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\t\t/*\n\t\t * Truncate pagecache after we've waited for commit\n\t\t * in data=journal mode to make pages freeable.\n\t\t */\n\t\ttruncate_pagecache(inode, inode->i_size);\n\t\tif (shrink)\n\t\t\text4_truncate(inode);\n\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t}\n\n\tif (!rc) {\n\t\tsetattr_copy(inode, attr);\n\t\tmark_inode_dirty(inode);\n\t}\n\n\t/*\n\t * If the call to ext4_truncate failed to get a transaction handle at\n\t * all, we need to clean up the in-core orphan list manually.\n\t */\n\tif (orphan && inode->i_nlink)\n\t\text4_orphan_del(NULL, inode);\n\n\tif (!rc && (ia_valid & ATTR_MODE))\n\t\trc = posix_acl_chmod(inode, inode->i_mode);\n\nerr_out:\n\text4_std_error(inode->i_sb, error);\n\tif (!error)\n\t\terror = rc;\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -120,6 +120,7 @@\n \t\t\t} else\n \t\t\t\text4_wait_for_tail_page_commit(inode);\n \t\t}\n+\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n \t\t/*\n \t\t * Truncate pagecache after we've waited for commit\n \t\t * in data=journal mode to make pages freeable.\n@@ -127,6 +128,7 @@\n \t\ttruncate_pagecache(inode, inode->i_size);\n \t\tif (shrink)\n \t\t\text4_truncate(inode);\n+\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n \t}\n \n \tif (!rc) {",
        "function_modified_lines": {
            "added": [
                "\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);",
                "\t\tup_write(&EXT4_I(inode)->i_mmap_sem);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling."
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "int ext4_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct page *page = vmf->page;\n\tloff_t size;\n\tunsigned long len;\n\tint ret;\n\tstruct file *file = vma->vm_file;\n\tstruct inode *inode = file_inode(file);\n\tstruct address_space *mapping = inode->i_mapping;\n\thandle_t *handle;\n\tget_block_t *get_block;\n\tint retries = 0;\n\n\tsb_start_pagefault(inode->i_sb);\n\tfile_update_time(vma->vm_file);\n\t/* Delalloc case is easy... */\n\tif (test_opt(inode->i_sb, DELALLOC) &&\n\t    !ext4_should_journal_data(inode) &&\n\t    !ext4_nonda_switch(inode->i_sb)) {\n\t\tdo {\n\t\t\tret = block_page_mkwrite(vma, vmf,\n\t\t\t\t\t\t   ext4_da_get_block_prep);\n\t\t} while (ret == -ENOSPC &&\n\t\t       ext4_should_retry_alloc(inode->i_sb, &retries));\n\t\tgoto out_ret;\n\t}\n\n\tlock_page(page);\n\tsize = i_size_read(inode);\n\t/* Page got truncated from under us? */\n\tif (page->mapping != mapping || page_offset(page) > size) {\n\t\tunlock_page(page);\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\tif (page->index == size >> PAGE_CACHE_SHIFT)\n\t\tlen = size & ~PAGE_CACHE_MASK;\n\telse\n\t\tlen = PAGE_CACHE_SIZE;\n\t/*\n\t * Return if we have all the buffers mapped. This avoids the need to do\n\t * journal_start/journal_stop which can block and take a long time\n\t */\n\tif (page_has_buffers(page)) {\n\t\tif (!ext4_walk_page_buffers(NULL, page_buffers(page),\n\t\t\t\t\t    0, len, NULL,\n\t\t\t\t\t    ext4_bh_unmapped)) {\n\t\t\t/* Wait so that we don't change page under IO */\n\t\t\twait_for_stable_page(page);\n\t\t\tret = VM_FAULT_LOCKED;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tunlock_page(page);\n\t/* OK, we need to fill the hole... */\n\tif (ext4_should_dioread_nolock(inode))\n\t\tget_block = ext4_get_block_write;\n\telse\n\t\tget_block = ext4_get_block;\nretry_alloc:\n\thandle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE,\n\t\t\t\t    ext4_writepage_trans_blocks(inode));\n\tif (IS_ERR(handle)) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out;\n\t}\n\tret = block_page_mkwrite(vma, vmf, get_block);\n\tif (!ret && ext4_should_journal_data(inode)) {\n\t\tif (ext4_walk_page_buffers(handle, page_buffers(page), 0,\n\t\t\t  PAGE_CACHE_SIZE, NULL, do_journal_get_write_access)) {\n\t\t\tunlock_page(page);\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t\text4_journal_stop(handle);\n\t\t\tgoto out;\n\t\t}\n\t\text4_set_inode_state(inode, EXT4_STATE_JDATA);\n\t}\n\text4_journal_stop(handle);\n\tif (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry_alloc;\nout_ret:\n\tret = block_page_mkwrite_return(ret);\nout:\n\tsb_end_pagefault(inode->i_sb);\n\treturn ret;\n}",
        "code_after_change": "int ext4_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct page *page = vmf->page;\n\tloff_t size;\n\tunsigned long len;\n\tint ret;\n\tstruct file *file = vma->vm_file;\n\tstruct inode *inode = file_inode(file);\n\tstruct address_space *mapping = inode->i_mapping;\n\thandle_t *handle;\n\tget_block_t *get_block;\n\tint retries = 0;\n\n\tsb_start_pagefault(inode->i_sb);\n\tfile_update_time(vma->vm_file);\n\n\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\t/* Delalloc case is easy... */\n\tif (test_opt(inode->i_sb, DELALLOC) &&\n\t    !ext4_should_journal_data(inode) &&\n\t    !ext4_nonda_switch(inode->i_sb)) {\n\t\tdo {\n\t\t\tret = block_page_mkwrite(vma, vmf,\n\t\t\t\t\t\t   ext4_da_get_block_prep);\n\t\t} while (ret == -ENOSPC &&\n\t\t       ext4_should_retry_alloc(inode->i_sb, &retries));\n\t\tgoto out_ret;\n\t}\n\n\tlock_page(page);\n\tsize = i_size_read(inode);\n\t/* Page got truncated from under us? */\n\tif (page->mapping != mapping || page_offset(page) > size) {\n\t\tunlock_page(page);\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\tif (page->index == size >> PAGE_CACHE_SHIFT)\n\t\tlen = size & ~PAGE_CACHE_MASK;\n\telse\n\t\tlen = PAGE_CACHE_SIZE;\n\t/*\n\t * Return if we have all the buffers mapped. This avoids the need to do\n\t * journal_start/journal_stop which can block and take a long time\n\t */\n\tif (page_has_buffers(page)) {\n\t\tif (!ext4_walk_page_buffers(NULL, page_buffers(page),\n\t\t\t\t\t    0, len, NULL,\n\t\t\t\t\t    ext4_bh_unmapped)) {\n\t\t\t/* Wait so that we don't change page under IO */\n\t\t\twait_for_stable_page(page);\n\t\t\tret = VM_FAULT_LOCKED;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tunlock_page(page);\n\t/* OK, we need to fill the hole... */\n\tif (ext4_should_dioread_nolock(inode))\n\t\tget_block = ext4_get_block_write;\n\telse\n\t\tget_block = ext4_get_block;\nretry_alloc:\n\thandle = ext4_journal_start(inode, EXT4_HT_WRITE_PAGE,\n\t\t\t\t    ext4_writepage_trans_blocks(inode));\n\tif (IS_ERR(handle)) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out;\n\t}\n\tret = block_page_mkwrite(vma, vmf, get_block);\n\tif (!ret && ext4_should_journal_data(inode)) {\n\t\tif (ext4_walk_page_buffers(handle, page_buffers(page), 0,\n\t\t\t  PAGE_CACHE_SIZE, NULL, do_journal_get_write_access)) {\n\t\t\tunlock_page(page);\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t\text4_journal_stop(handle);\n\t\t\tgoto out;\n\t\t}\n\t\text4_set_inode_state(inode, EXT4_STATE_JDATA);\n\t}\n\text4_journal_stop(handle);\n\tif (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry_alloc;\nout_ret:\n\tret = block_page_mkwrite_return(ret);\nout:\n\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\tsb_end_pagefault(inode->i_sb);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,8 @@\n \n \tsb_start_pagefault(inode->i_sb);\n \tfile_update_time(vma->vm_file);\n+\n+\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n \t/* Delalloc case is easy... */\n \tif (test_opt(inode->i_sb, DELALLOC) &&\n \t    !ext4_should_journal_data(inode) &&\n@@ -82,6 +84,7 @@\n out_ret:\n \tret = block_page_mkwrite_return(ret);\n out:\n+\tup_read(&EXT4_I(inode)->i_mmap_sem);\n \tsb_end_pagefault(inode->i_sb);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tdown_read(&EXT4_I(inode)->i_mmap_sem);",
                "\tup_read(&EXT4_I(inode)->i_mmap_sem);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling."
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "static void init_once(void *foo)\n{\n\tstruct ext4_inode_info *ei = (struct ext4_inode_info *) foo;\n\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\tinit_rwsem(&ei->xattr_sem);\n\tinit_rwsem(&ei->i_data_sem);\n\tinode_init_once(&ei->vfs_inode);\n}",
        "code_after_change": "static void init_once(void *foo)\n{\n\tstruct ext4_inode_info *ei = (struct ext4_inode_info *) foo;\n\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\tinit_rwsem(&ei->xattr_sem);\n\tinit_rwsem(&ei->i_data_sem);\n\tinit_rwsem(&ei->i_mmap_sem);\n\tinode_init_once(&ei->vfs_inode);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,5 +5,6 @@\n \tINIT_LIST_HEAD(&ei->i_orphan);\n \tinit_rwsem(&ei->xattr_sem);\n \tinit_rwsem(&ei->i_data_sem);\n+\tinit_rwsem(&ei->i_mmap_sem);\n \tinode_init_once(&ei->vfs_inode);\n }",
        "function_modified_lines": {
            "added": [
                "\tinit_rwsem(&ei->i_mmap_sem);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling."
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "static inline void ext4_truncate_failed_write(struct inode *inode)\n{\n\ttruncate_inode_pages(inode->i_mapping, inode->i_size);\n\text4_truncate(inode);\n}",
        "code_after_change": "static inline void ext4_truncate_failed_write(struct inode *inode)\n{\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\ttruncate_inode_pages(inode->i_mapping, inode->i_size);\n\text4_truncate(inode);\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,7 @@\n static inline void ext4_truncate_failed_write(struct inode *inode)\n {\n+\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n \ttruncate_inode_pages(inode->i_mapping, inode->i_size);\n \text4_truncate(inode);\n+\tup_write(&EXT4_I(inode)->i_mmap_sem);\n }",
        "function_modified_lines": {
            "added": [
                "\tdown_write(&EXT4_I(inode)->i_mmap_sem);",
                "\tup_write(&EXT4_I(inode)->i_mmap_sem);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling."
    },
    {
        "cve_id": "CVE-2015-8963",
        "code_before_change": "static int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\n\thwc->state = !(flags & PERF_EF_START);\n\n\thead = find_swevent_head(swhash, event);\n\tif (!head) {\n\t\t/*\n\t\t * We can race with cpu hotplug code. Do not\n\t\t * WARN if the cpu just got unplugged.\n\t\t */\n\t\tWARN_ON_ONCE(swhash->online);\n\t\treturn -EINVAL;\n\t}\n\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}",
        "code_after_change": "static int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\n\thwc->state = !(flags & PERF_EF_START);\n\n\thead = find_swevent_head(swhash, event);\n\tif (WARN_ON_ONCE(!head))\n\t\treturn -EINVAL;\n\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,14 +12,8 @@\n \thwc->state = !(flags & PERF_EF_START);\n \n \thead = find_swevent_head(swhash, event);\n-\tif (!head) {\n-\t\t/*\n-\t\t * We can race with cpu hotplug code. Do not\n-\t\t * WARN if the cpu just got unplugged.\n-\t\t */\n-\t\tWARN_ON_ONCE(swhash->online);\n+\tif (WARN_ON_ONCE(!head))\n \t\treturn -EINVAL;\n-\t}\n \n \thlist_add_head_rcu(&event->hlist_entry, head);\n \tperf_event_update_userpage(event);",
        "function_modified_lines": {
            "added": [
                "\tif (WARN_ON_ONCE(!head))"
            ],
            "deleted": [
                "\tif (!head) {",
                "\t\t/*",
                "\t\t * We can race with cpu hotplug code. Do not",
                "\t\t * WARN if the cpu just got unplugged.",
                "\t\t */",
                "\t\tWARN_ON_ONCE(swhash->online);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation."
    },
    {
        "cve_id": "CVE-2015-8963",
        "code_before_change": "static void perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tswhash->online = true;\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}",
        "code_after_change": "static void perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,6 @@\n \tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n \n \tmutex_lock(&swhash->hlist_mutex);\n-\tswhash->online = true;\n \tif (swhash->hlist_refcount > 0) {\n \t\tstruct swevent_hlist *hlist;\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tswhash->online = true;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation."
    },
    {
        "cve_id": "CVE-2015-8963",
        "code_before_change": "static int swevent_hlist_get_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\tint err = 0;\n\n\tmutex_lock(&swhash->hlist_mutex);\n\n\tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc(sizeof(*hlist), GFP_KERNEL);\n\t\tif (!hlist) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tswhash->hlist_refcount++;\nexit:\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\treturn err;\n}",
        "code_after_change": "static int swevent_hlist_get_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\tint err = 0;\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc(sizeof(*hlist), GFP_KERNEL);\n\t\tif (!hlist) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tswhash->hlist_refcount++;\nexit:\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \tint err = 0;\n \n \tmutex_lock(&swhash->hlist_mutex);\n-\n \tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n \t\tstruct swevent_hlist *hlist;\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation."
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "static void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}",
        "code_after_change": "static void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\t/* release the tag's ownership to the req cloned from */\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,12 @@\n \tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n \n \tif (q->mq_ops) {\n+\t\tstruct blk_mq_hw_ctx *hctx;\n+\n+\t\t/* release the tag's ownership to the req cloned from */\n \t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n+\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\n+\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\n \t\tflush_rq->tag = -1;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tstruct blk_mq_hw_ctx *hctx;",
                "",
                "\t\t/* release the tag's ownership to the req cloned from */",
                "\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);",
                "\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046."
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "static bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time.\n\t */\n\tif (q->mq_ops) {\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}",
        "code_after_change": "static bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time. And acquire the tag's\n\t * ownership for flush req.\n\t */\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t\tfq->orig_rq = first_rq;\n\n\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,11 +25,18 @@\n \n \t/*\n \t * Borrow tag from the first request since they can't\n-\t * be in flight at the same time.\n+\t * be in flight at the same time. And acquire the tag's\n+\t * ownership for flush req.\n \t */\n \tif (q->mq_ops) {\n+\t\tstruct blk_mq_hw_ctx *hctx;\n+\n \t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n \t\tflush_rq->tag = first_rq->tag;\n+\t\tfq->orig_rq = first_rq;\n+\n+\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);\n+\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);\n \t}\n \n \tflush_rq->cmd_type = REQ_TYPE_FS;",
        "function_modified_lines": {
            "added": [
                "\t * be in flight at the same time. And acquire the tag's",
                "\t * ownership for flush req.",
                "\t\tstruct blk_mq_hw_ctx *hctx;",
                "",
                "\t\tfq->orig_rq = first_rq;",
                "",
                "\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);",
                "\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);"
            ],
            "deleted": [
                "\t * be in flight at the same time."
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046."
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
        "code_after_change": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = hctx->tags->rqs[off + bit];\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n \t\t     bit < bm->depth;\n \t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n-\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n+\t\t\trq = hctx->tags->rqs[off + bit];\n \t\t\tif (rq->q == hctx->queue)\n \t\t\t\tfn(hctx, rq, data, reserved);\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\trq = hctx->tags->rqs[off + bit];"
            ],
            "deleted": [
                "\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046."
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "static void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = blk_mq_tag_to_rq(tags, off + bit);\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
        "code_after_change": "static void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = tags->rqs[off + bit];\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,7 +13,7 @@\n \t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n \t\t     bit < bm->depth;\n \t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n-\t\t\trq = blk_mq_tag_to_rq(tags, off + bit);\n+\t\t\trq = tags->rqs[off + bit];\n \t\t\tfn(rq, data, reserved);\n \t\t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\trq = tags->rqs[off + bit];"
            ],
            "deleted": [
                "\t\t\trq = blk_mq_tag_to_rq(tags, off + bit);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046."
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\tstruct request *rq = tags->rqs[tag];\n\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n\n\tif (!is_flush_request(rq, fq, tag))\n\t\treturn rq;\n\n\treturn fq->flush_rq;\n}",
        "code_after_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\treturn tags->rqs[tag];\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,4 @@\n struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n {\n-\tstruct request *rq = tags->rqs[tag];\n-\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n-\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n-\n-\tif (!is_flush_request(rq, fq, tag))\n-\t\treturn rq;\n-\n-\treturn fq->flush_rq;\n+\treturn tags->rqs[tag];\n }",
        "function_modified_lines": {
            "added": [
                "\treturn tags->rqs[tag];"
            ],
            "deleted": [
                "\tstruct request *rq = tags->rqs[tag];",
                "\t/* mq_ctx of flush rq is always cloned from the corresponding req */",
                "\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);",
                "",
                "\tif (!is_flush_request(rq, fq, tag))",
                "\t\treturn rq;",
                "",
                "\treturn fq->flush_rq;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046."
    },
    {
        "cve_id": "CVE-2016-0723",
        "code_before_change": "long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct tty_struct *tty = file_tty(file);\n\tstruct tty_struct *real_tty;\n\tvoid __user *p = (void __user *)arg;\n\tint retval;\n\tstruct tty_ldisc *ld;\n\n\tif (tty_paranoia_check(tty, file_inode(file), \"tty_ioctl\"))\n\t\treturn -EINVAL;\n\n\treal_tty = tty_pair_get_tty(tty);\n\n\t/*\n\t * Factor out some common prep work\n\t */\n\tswitch (cmd) {\n\tcase TIOCSETD:\n\tcase TIOCSBRK:\n\tcase TIOCCBRK:\n\tcase TCSBRK:\n\tcase TCSBRKP:\n\t\tretval = tty_check_change(tty);\n\t\tif (retval)\n\t\t\treturn retval;\n\t\tif (cmd != TIOCCBRK) {\n\t\t\ttty_wait_until_sent(tty, 0);\n\t\t\tif (signal_pending(current))\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t *\tNow do the stuff.\n\t */\n\tswitch (cmd) {\n\tcase TIOCSTI:\n\t\treturn tiocsti(tty, p);\n\tcase TIOCGWINSZ:\n\t\treturn tiocgwinsz(real_tty, p);\n\tcase TIOCSWINSZ:\n\t\treturn tiocswinsz(real_tty, p);\n\tcase TIOCCONS:\n\t\treturn real_tty != tty ? -EINVAL : tioccons(file);\n\tcase FIONBIO:\n\t\treturn fionbio(file, p);\n\tcase TIOCEXCL:\n\t\tset_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCNXCL:\n\t\tclear_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCGEXCL:\n\t{\n\t\tint excl = test_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn put_user(excl, (int __user *)p);\n\t}\n\tcase TIOCNOTTY:\n\t\tif (current->signal->tty != tty)\n\t\t\treturn -ENOTTY;\n\t\tno_tty();\n\t\treturn 0;\n\tcase TIOCSCTTY:\n\t\treturn tiocsctty(real_tty, file, arg);\n\tcase TIOCGPGRP:\n\t\treturn tiocgpgrp(tty, real_tty, p);\n\tcase TIOCSPGRP:\n\t\treturn tiocspgrp(tty, real_tty, p);\n\tcase TIOCGSID:\n\t\treturn tiocgsid(tty, real_tty, p);\n\tcase TIOCGETD:\n\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);\n\tcase TIOCSETD:\n\t\treturn tiocsetd(tty, p);\n\tcase TIOCVHANGUP:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\ttty_vhangup(tty);\n\t\treturn 0;\n\tcase TIOCGDEV:\n\t{\n\t\tunsigned int ret = new_encode_dev(tty_devnum(real_tty));\n\t\treturn put_user(ret, (unsigned int __user *)p);\n\t}\n\t/*\n\t * Break handling\n\t */\n\tcase TIOCSBRK:\t/* Turn break on, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, -1);\n\t\treturn 0;\n\tcase TIOCCBRK:\t/* Turn break off, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, 0);\n\t\treturn 0;\n\tcase TCSBRK:   /* SVID version: non-zero arg --> no break */\n\t\t/* non-zero arg means wait for all output data\n\t\t * to be sent (performed above) but don't send break.\n\t\t * This is used by the tcdrain() termios function.\n\t\t */\n\t\tif (!arg)\n\t\t\treturn send_break(tty, 250);\n\t\treturn 0;\n\tcase TCSBRKP:\t/* support for POSIX tcsendbreak() */\n\t\treturn send_break(tty, arg ? arg*100 : 250);\n\n\tcase TIOCMGET:\n\t\treturn tty_tiocmget(tty, p);\n\tcase TIOCMSET:\n\tcase TIOCMBIC:\n\tcase TIOCMBIS:\n\t\treturn tty_tiocmset(tty, cmd, p);\n\tcase TIOCGICOUNT:\n\t\tretval = tty_tiocgicount(tty, p);\n\t\t/* For the moment allow fall through to the old method */\n        \tif (retval != -EINVAL)\n\t\t\treturn retval;\n\t\tbreak;\n\tcase TCFLSH:\n\t\tswitch (arg) {\n\t\tcase TCIFLUSH:\n\t\tcase TCIOFLUSH:\n\t\t/* flush tty buffer and allow ldisc to process ioctl */\n\t\t\ttty_buffer_flush(tty, NULL);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase TIOCSSERIAL:\n\t\ttty_warn_deprecated_flags(p);\n\t\tbreak;\n\t}\n\tif (tty->ops->ioctl) {\n\t\tretval = tty->ops->ioctl(tty, cmd, arg);\n\t\tif (retval != -ENOIOCTLCMD)\n\t\t\treturn retval;\n\t}\n\tld = tty_ldisc_ref_wait(tty);\n\tretval = -EINVAL;\n\tif (ld->ops->ioctl) {\n\t\tretval = ld->ops->ioctl(tty, file, cmd, arg);\n\t\tif (retval == -ENOIOCTLCMD)\n\t\t\tretval = -ENOTTY;\n\t}\n\ttty_ldisc_deref(ld);\n\treturn retval;\n}",
        "code_after_change": "long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct tty_struct *tty = file_tty(file);\n\tstruct tty_struct *real_tty;\n\tvoid __user *p = (void __user *)arg;\n\tint retval;\n\tstruct tty_ldisc *ld;\n\n\tif (tty_paranoia_check(tty, file_inode(file), \"tty_ioctl\"))\n\t\treturn -EINVAL;\n\n\treal_tty = tty_pair_get_tty(tty);\n\n\t/*\n\t * Factor out some common prep work\n\t */\n\tswitch (cmd) {\n\tcase TIOCSETD:\n\tcase TIOCSBRK:\n\tcase TIOCCBRK:\n\tcase TCSBRK:\n\tcase TCSBRKP:\n\t\tretval = tty_check_change(tty);\n\t\tif (retval)\n\t\t\treturn retval;\n\t\tif (cmd != TIOCCBRK) {\n\t\t\ttty_wait_until_sent(tty, 0);\n\t\t\tif (signal_pending(current))\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t *\tNow do the stuff.\n\t */\n\tswitch (cmd) {\n\tcase TIOCSTI:\n\t\treturn tiocsti(tty, p);\n\tcase TIOCGWINSZ:\n\t\treturn tiocgwinsz(real_tty, p);\n\tcase TIOCSWINSZ:\n\t\treturn tiocswinsz(real_tty, p);\n\tcase TIOCCONS:\n\t\treturn real_tty != tty ? -EINVAL : tioccons(file);\n\tcase FIONBIO:\n\t\treturn fionbio(file, p);\n\tcase TIOCEXCL:\n\t\tset_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCNXCL:\n\t\tclear_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCGEXCL:\n\t{\n\t\tint excl = test_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn put_user(excl, (int __user *)p);\n\t}\n\tcase TIOCNOTTY:\n\t\tif (current->signal->tty != tty)\n\t\t\treturn -ENOTTY;\n\t\tno_tty();\n\t\treturn 0;\n\tcase TIOCSCTTY:\n\t\treturn tiocsctty(real_tty, file, arg);\n\tcase TIOCGPGRP:\n\t\treturn tiocgpgrp(tty, real_tty, p);\n\tcase TIOCSPGRP:\n\t\treturn tiocspgrp(tty, real_tty, p);\n\tcase TIOCGSID:\n\t\treturn tiocgsid(tty, real_tty, p);\n\tcase TIOCGETD:\n\t\treturn tiocgetd(tty, p);\n\tcase TIOCSETD:\n\t\treturn tiocsetd(tty, p);\n\tcase TIOCVHANGUP:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\ttty_vhangup(tty);\n\t\treturn 0;\n\tcase TIOCGDEV:\n\t{\n\t\tunsigned int ret = new_encode_dev(tty_devnum(real_tty));\n\t\treturn put_user(ret, (unsigned int __user *)p);\n\t}\n\t/*\n\t * Break handling\n\t */\n\tcase TIOCSBRK:\t/* Turn break on, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, -1);\n\t\treturn 0;\n\tcase TIOCCBRK:\t/* Turn break off, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, 0);\n\t\treturn 0;\n\tcase TCSBRK:   /* SVID version: non-zero arg --> no break */\n\t\t/* non-zero arg means wait for all output data\n\t\t * to be sent (performed above) but don't send break.\n\t\t * This is used by the tcdrain() termios function.\n\t\t */\n\t\tif (!arg)\n\t\t\treturn send_break(tty, 250);\n\t\treturn 0;\n\tcase TCSBRKP:\t/* support for POSIX tcsendbreak() */\n\t\treturn send_break(tty, arg ? arg*100 : 250);\n\n\tcase TIOCMGET:\n\t\treturn tty_tiocmget(tty, p);\n\tcase TIOCMSET:\n\tcase TIOCMBIC:\n\tcase TIOCMBIS:\n\t\treturn tty_tiocmset(tty, cmd, p);\n\tcase TIOCGICOUNT:\n\t\tretval = tty_tiocgicount(tty, p);\n\t\t/* For the moment allow fall through to the old method */\n        \tif (retval != -EINVAL)\n\t\t\treturn retval;\n\t\tbreak;\n\tcase TCFLSH:\n\t\tswitch (arg) {\n\t\tcase TCIFLUSH:\n\t\tcase TCIOFLUSH:\n\t\t/* flush tty buffer and allow ldisc to process ioctl */\n\t\t\ttty_buffer_flush(tty, NULL);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase TIOCSSERIAL:\n\t\ttty_warn_deprecated_flags(p);\n\t\tbreak;\n\t}\n\tif (tty->ops->ioctl) {\n\t\tretval = tty->ops->ioctl(tty, cmd, arg);\n\t\tif (retval != -ENOIOCTLCMD)\n\t\t\treturn retval;\n\t}\n\tld = tty_ldisc_ref_wait(tty);\n\tretval = -EINVAL;\n\tif (ld->ops->ioctl) {\n\t\tretval = ld->ops->ioctl(tty, file, cmd, arg);\n\t\tif (retval == -ENOIOCTLCMD)\n\t\t\tretval = -ENOTTY;\n\t}\n\ttty_ldisc_deref(ld);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -70,7 +70,7 @@\n \tcase TIOCGSID:\n \t\treturn tiocgsid(tty, real_tty, p);\n \tcase TIOCGETD:\n-\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);\n+\t\treturn tiocgetd(tty, p);\n \tcase TIOCSETD:\n \t\treturn tiocsetd(tty, p);\n \tcase TIOCVHANGUP:",
        "function_modified_lines": {
            "added": [
                "\t\treturn tiocgetd(tty, p);"
            ],
            "deleted": [
                "\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);"
            ]
        },
        "cwe": [
            "CWE-200",
            "CWE-362",
            "NVD-CWE-Other"
        ],
        "cve_description": "Race condition in the tty_ioctl function in drivers/tty/tty_io.c in the Linux kernel through 4.4.1 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free and system crash) by making a TIOCGETD ioctl call during processing of a TIOCSETD ioctl call."
    },
    {
        "cve_id": "CVE-2016-10200",
        "code_before_change": "static int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tstruct net *net = sock_net(sk);\n\tint ret;\n\tint chk_addr_ret;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(struct sockaddr_l2tpip))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(net, addr->l2tp_addr.s_addr,\n\t\t\t\t  sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tif (addr->l2tp_addr.s_addr)\n\t\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}",
        "code_after_change": "static int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tstruct net *net = sock_net(sk);\n\tint ret;\n\tint chk_addr_ret;\n\n\tif (addr_len < sizeof(struct sockaddr_l2tpip))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(net, addr->l2tp_addr.s_addr,\n\t\t\t\t  sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out;\n\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tif (addr->l2tp_addr.s_addr)\n\t\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,6 @@\n \tint ret;\n \tint chk_addr_ret;\n \n-\tif (!sock_flag(sk, SOCK_ZAPPED))\n-\t\treturn -EINVAL;\n \tif (addr_len < sizeof(struct sockaddr_l2tpip))\n \t\treturn -EINVAL;\n \tif (addr->l2tp_family != AF_INET)\n@@ -22,6 +20,9 @@\n \tread_unlock_bh(&l2tp_ip_lock);\n \n \tlock_sock(sk);\n+\tif (!sock_flag(sk, SOCK_ZAPPED))\n+\t\tgoto out;\n+\n \tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n \t\tgoto out;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\tgoto out;",
                ""
            ],
            "deleted": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the L2TPv3 IP Encapsulation feature in the Linux kernel before 4.8.14 allows local users to gain privileges or cause a denial of service (use-after-free) by making multiple bind system calls without properly ascertaining whether a socket has the SOCK_ZAPPED status, related to net/l2tp/l2tp_ip.c and net/l2tp/l2tp_ip6.c."
    },
    {
        "cve_id": "CVE-2016-10200",
        "code_before_change": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "code_after_change": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out_unlock;\n\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,6 @@\n \tint addr_type;\n \tint err;\n \n-\tif (!sock_flag(sk, SOCK_ZAPPED))\n-\t\treturn -EINVAL;\n \tif (addr->l2tp_family != AF_INET6)\n \t\treturn -EINVAL;\n \tif (addr_len < sizeof(*addr))\n@@ -35,6 +33,9 @@\n \tlock_sock(sk);\n \n \terr = -EINVAL;\n+\tif (!sock_flag(sk, SOCK_ZAPPED))\n+\t\tgoto out_unlock;\n+\n \tif (sk->sk_state != TCP_CLOSE)\n \t\tgoto out_unlock;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\tgoto out_unlock;",
                ""
            ],
            "deleted": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the L2TPv3 IP Encapsulation feature in the Linux kernel before 4.8.14 allows local users to gain privileges or cause a denial of service (use-after-free) by making multiple bind system calls without properly ascertaining whether a socket has the SOCK_ZAPPED status, related to net/l2tp/l2tp_ip.c and net/l2tp/l2tp_ip6.c."
    },
    {
        "cve_id": "CVE-2016-10741",
        "code_before_change": "STATIC int\n__xfs_get_blocks(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate,\n\tbool\t\t\tdirect,\n\tbool\t\t\tdax_fault)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\txfs_fileoff_t\t\toffset_fsb, end_fsb;\n\tint\t\t\terror = 0;\n\tint\t\t\tlockmode = 0;\n\tstruct xfs_bmbt_irec\timap;\n\tint\t\t\tnimaps = 1;\n\txfs_off_t\t\toffset;\n\tssize_t\t\t\tsize;\n\tint\t\t\tnew = 0;\n\tbool\t\t\tis_cow = false;\n\tbool\t\t\tneed_alloc = false;\n\n\tBUG_ON(create && !direct);\n\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn -EIO;\n\n\toffset = (xfs_off_t)iblock << inode->i_blkbits;\n\tASSERT(bh_result->b_size >= (1 << inode->i_blkbits));\n\tsize = bh_result->b_size;\n\n\tif (!create && offset >= i_size_read(inode))\n\t\treturn 0;\n\n\t/*\n\t * Direct I/O is usually done on preallocated files, so try getting\n\t * a block mapping without an exclusive lock first.\n\t */\n\tlockmode = xfs_ilock_data_map_shared(ip);\n\n\tASSERT(offset <= mp->m_super->s_maxbytes);\n\tif (offset + size > mp->m_super->s_maxbytes)\n\t\tsize = mp->m_super->s_maxbytes - offset;\n\tend_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);\n\toffset_fsb = XFS_B_TO_FSBT(mp, offset);\n\n\tif (create && direct && xfs_is_reflink_inode(ip))\n\t\tis_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,\n\t\t\t\t\t&need_alloc);\n\tif (!is_cow) {\n\t\terror = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,\n\t\t\t\t\t&imap, &nimaps, XFS_BMAPI_ENTIRE);\n\t\t/*\n\t\t * Truncate an overwrite extent if there's a pending CoW\n\t\t * reservation before the end of this extent.  This\n\t\t * forces us to come back to get_blocks to take care of\n\t\t * the CoW.\n\t\t */\n\t\tif (create && direct && nimaps &&\n\t\t    imap.br_startblock != HOLESTARTBLOCK &&\n\t\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t\t    !ISUNWRITTEN(&imap))\n\t\t\txfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t}\n\tASSERT(!need_alloc);\n\tif (error)\n\t\tgoto out_unlock;\n\n\t/* for DAX, we convert unwritten extents directly */\n\tif (create &&\n\t    (!nimaps ||\n\t     (imap.br_startblock == HOLESTARTBLOCK ||\n\t      imap.br_startblock == DELAYSTARTBLOCK) ||\n\t     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {\n\t\t/*\n\t\t * xfs_iomap_write_direct() expects the shared lock. It\n\t\t * is unlocked on return.\n\t\t */\n\t\tif (lockmode == XFS_ILOCK_EXCL)\n\t\t\txfs_ilock_demote(ip, lockmode);\n\n\t\terror = xfs_iomap_write_direct(ip, offset, size,\n\t\t\t\t\t       &imap, nimaps);\n\t\tif (error)\n\t\t\treturn error;\n\t\tnew = 1;\n\n\t\ttrace_xfs_get_blocks_alloc(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_DELALLOC, &imap);\n\t} else if (nimaps) {\n\t\ttrace_xfs_get_blocks_found(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_OVERWRITE, &imap);\n\t\txfs_iunlock(ip, lockmode);\n\t} else {\n\t\ttrace_xfs_get_blocks_notfound(ip, offset, size);\n\t\tgoto out_unlock;\n\t}\n\n\tif (IS_DAX(inode) && create) {\n\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t/* zeroing is not needed at a higher layer */\n\t\tnew = 0;\n\t}\n\n\t/* trim mapping down to size requested */\n\txfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);\n\n\t/*\n\t * For unwritten extents do not report a disk address in the buffered\n\t * read case (treat as if we're reading into a hole).\n\t */\n\tif (imap.br_startblock != HOLESTARTBLOCK &&\n\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t    (create || !ISUNWRITTEN(&imap))) {\n\t\tif (create && direct && !is_cow) {\n\t\t\terror = xfs_bounce_unaligned_dio_write(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\n\t\txfs_map_buffer(inode, bh_result, &imap, offset);\n\t\tif (ISUNWRITTEN(&imap))\n\t\t\tset_buffer_unwritten(bh_result);\n\t\t/* direct IO needs special help */\n\t\tif (create) {\n\t\t\tif (dax_fault)\n\t\t\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t\telse\n\t\t\t\txfs_map_direct(inode, bh_result, &imap, offset,\n\t\t\t\t\t\tis_cow);\n\t\t}\n\t}\n\n\t/*\n\t * If this is a realtime file, data may be on a different device.\n\t * to that pointed to from the buffer_head b_bdev currently.\n\t */\n\tbh_result->b_bdev = xfs_find_bdev_for_inode(inode);\n\n\t/*\n\t * If we previously allocated a block out beyond eof and we are now\n\t * coming back to use it then we will need to flag it as new even if it\n\t * has a disk address.\n\t *\n\t * With sub-block writes into unwritten extents we also need to mark\n\t * the buffer as new so that the unwritten parts of the buffer gets\n\t * correctly zeroed.\n\t */\n\tif (create &&\n\t    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||\n\t     (offset >= i_size_read(inode)) ||\n\t     (new || ISUNWRITTEN(&imap))))\n\t\tset_buffer_new(bh_result);\n\n\tBUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);\n\n\treturn 0;\n\nout_unlock:\n\txfs_iunlock(ip, lockmode);\n\treturn error;\n}",
        "code_after_change": "STATIC int\n__xfs_get_blocks(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate,\n\tbool\t\t\tdirect,\n\tbool\t\t\tdax_fault)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\txfs_fileoff_t\t\toffset_fsb, end_fsb;\n\tint\t\t\terror = 0;\n\tint\t\t\tlockmode = 0;\n\tstruct xfs_bmbt_irec\timap;\n\tint\t\t\tnimaps = 1;\n\txfs_off_t\t\toffset;\n\tssize_t\t\t\tsize;\n\tint\t\t\tnew = 0;\n\tbool\t\t\tis_cow = false;\n\tbool\t\t\tneed_alloc = false;\n\n\tBUG_ON(create && !direct);\n\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn -EIO;\n\n\toffset = (xfs_off_t)iblock << inode->i_blkbits;\n\tASSERT(bh_result->b_size >= (1 << inode->i_blkbits));\n\tsize = bh_result->b_size;\n\n\tif (!create && offset >= i_size_read(inode))\n\t\treturn 0;\n\n\t/*\n\t * Direct I/O is usually done on preallocated files, so try getting\n\t * a block mapping without an exclusive lock first.\n\t */\n\tlockmode = xfs_ilock_data_map_shared(ip);\n\n\tASSERT(offset <= mp->m_super->s_maxbytes);\n\tif (offset + size > mp->m_super->s_maxbytes)\n\t\tsize = mp->m_super->s_maxbytes - offset;\n\tend_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);\n\toffset_fsb = XFS_B_TO_FSBT(mp, offset);\n\n\tif (create && direct && xfs_is_reflink_inode(ip))\n\t\tis_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,\n\t\t\t\t\t&need_alloc);\n\tif (!is_cow) {\n\t\terror = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,\n\t\t\t\t\t&imap, &nimaps, XFS_BMAPI_ENTIRE);\n\t\t/*\n\t\t * Truncate an overwrite extent if there's a pending CoW\n\t\t * reservation before the end of this extent.  This\n\t\t * forces us to come back to get_blocks to take care of\n\t\t * the CoW.\n\t\t */\n\t\tif (create && direct && nimaps &&\n\t\t    imap.br_startblock != HOLESTARTBLOCK &&\n\t\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t\t    !ISUNWRITTEN(&imap))\n\t\t\txfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t}\n\tASSERT(!need_alloc);\n\tif (error)\n\t\tgoto out_unlock;\n\n\t/*\n\t * The only time we can ever safely find delalloc blocks on direct I/O\n\t * is a dio write to post-eof speculative preallocation. All other\n\t * scenarios are indicative of a problem or misuse (such as mixing\n\t * direct and mapped I/O).\n\t *\n\t * The file may be unmapped by the time we get here so we cannot\n\t * reliably fail the I/O based on mapping. Instead, fail the I/O if this\n\t * is a read or a write within eof. Otherwise, carry on but warn as a\n\t * precuation if the file happens to be mapped.\n\t */\n\tif (direct && imap.br_startblock == DELAYSTARTBLOCK) {\n\t\tif (!create || offset < i_size_read(VFS_I(ip))) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\terror = -EIO;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tWARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));\n\t}\n\n\t/* for DAX, we convert unwritten extents directly */\n\tif (create &&\n\t    (!nimaps ||\n\t     (imap.br_startblock == HOLESTARTBLOCK ||\n\t      imap.br_startblock == DELAYSTARTBLOCK) ||\n\t     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {\n\t\t/*\n\t\t * xfs_iomap_write_direct() expects the shared lock. It\n\t\t * is unlocked on return.\n\t\t */\n\t\tif (lockmode == XFS_ILOCK_EXCL)\n\t\t\txfs_ilock_demote(ip, lockmode);\n\n\t\terror = xfs_iomap_write_direct(ip, offset, size,\n\t\t\t\t\t       &imap, nimaps);\n\t\tif (error)\n\t\t\treturn error;\n\t\tnew = 1;\n\n\t\ttrace_xfs_get_blocks_alloc(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_DELALLOC, &imap);\n\t} else if (nimaps) {\n\t\ttrace_xfs_get_blocks_found(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_OVERWRITE, &imap);\n\t\txfs_iunlock(ip, lockmode);\n\t} else {\n\t\ttrace_xfs_get_blocks_notfound(ip, offset, size);\n\t\tgoto out_unlock;\n\t}\n\n\tif (IS_DAX(inode) && create) {\n\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t/* zeroing is not needed at a higher layer */\n\t\tnew = 0;\n\t}\n\n\t/* trim mapping down to size requested */\n\txfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);\n\n\t/*\n\t * For unwritten extents do not report a disk address in the buffered\n\t * read case (treat as if we're reading into a hole).\n\t */\n\tif (imap.br_startblock != HOLESTARTBLOCK &&\n\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t    (create || !ISUNWRITTEN(&imap))) {\n\t\tif (create && direct && !is_cow) {\n\t\t\terror = xfs_bounce_unaligned_dio_write(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\n\t\txfs_map_buffer(inode, bh_result, &imap, offset);\n\t\tif (ISUNWRITTEN(&imap))\n\t\t\tset_buffer_unwritten(bh_result);\n\t\t/* direct IO needs special help */\n\t\tif (create) {\n\t\t\tif (dax_fault)\n\t\t\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t\telse\n\t\t\t\txfs_map_direct(inode, bh_result, &imap, offset,\n\t\t\t\t\t\tis_cow);\n\t\t}\n\t}\n\n\t/*\n\t * If this is a realtime file, data may be on a different device.\n\t * to that pointed to from the buffer_head b_bdev currently.\n\t */\n\tbh_result->b_bdev = xfs_find_bdev_for_inode(inode);\n\n\t/*\n\t * If we previously allocated a block out beyond eof and we are now\n\t * coming back to use it then we will need to flag it as new even if it\n\t * has a disk address.\n\t *\n\t * With sub-block writes into unwritten extents we also need to mark\n\t * the buffer as new so that the unwritten parts of the buffer gets\n\t * correctly zeroed.\n\t */\n\tif (create &&\n\t    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||\n\t     (offset >= i_size_read(inode)) ||\n\t     (new || ISUNWRITTEN(&imap))))\n\t\tset_buffer_new(bh_result);\n\n\treturn 0;\n\nout_unlock:\n\txfs_iunlock(ip, lockmode);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -66,6 +66,26 @@\n \tASSERT(!need_alloc);\n \tif (error)\n \t\tgoto out_unlock;\n+\n+\t/*\n+\t * The only time we can ever safely find delalloc blocks on direct I/O\n+\t * is a dio write to post-eof speculative preallocation. All other\n+\t * scenarios are indicative of a problem or misuse (such as mixing\n+\t * direct and mapped I/O).\n+\t *\n+\t * The file may be unmapped by the time we get here so we cannot\n+\t * reliably fail the I/O based on mapping. Instead, fail the I/O if this\n+\t * is a read or a write within eof. Otherwise, carry on but warn as a\n+\t * precuation if the file happens to be mapped.\n+\t */\n+\tif (direct && imap.br_startblock == DELAYSTARTBLOCK) {\n+\t\tif (!create || offset < i_size_read(VFS_I(ip))) {\n+\t\t\tWARN_ON_ONCE(1);\n+\t\t\terror = -EIO;\n+\t\t\tgoto out_unlock;\n+\t\t}\n+\t\tWARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));\n+\t}\n \n \t/* for DAX, we convert unwritten extents directly */\n \tif (create &&\n@@ -156,8 +176,6 @@\n \t     (new || ISUNWRITTEN(&imap))))\n \t\tset_buffer_new(bh_result);\n \n-\tBUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);\n-\n \treturn 0;\n \n out_unlock:",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * The only time we can ever safely find delalloc blocks on direct I/O",
                "\t * is a dio write to post-eof speculative preallocation. All other",
                "\t * scenarios are indicative of a problem or misuse (such as mixing",
                "\t * direct and mapped I/O).",
                "\t *",
                "\t * The file may be unmapped by the time we get here so we cannot",
                "\t * reliably fail the I/O based on mapping. Instead, fail the I/O if this",
                "\t * is a read or a write within eof. Otherwise, carry on but warn as a",
                "\t * precuation if the file happens to be mapped.",
                "\t */",
                "\tif (direct && imap.br_startblock == DELAYSTARTBLOCK) {",
                "\t\tif (!create || offset < i_size_read(VFS_I(ip))) {",
                "\t\t\tWARN_ON_ONCE(1);",
                "\t\t\terror = -EIO;",
                "\t\t\tgoto out_unlock;",
                "\t\t}",
                "\t\tWARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));",
                "\t}"
            ],
            "deleted": [
                "\tBUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux kernel before 4.9.3, fs/xfs/xfs_aops.c allows local users to cause a denial of service (system crash) because there is a race condition between direct and memory-mapped I/O (associated with a hole) that is handled with BUG_ON instead of an I/O failure."
    },
    {
        "cve_id": "CVE-2016-10906",
        "code_before_change": "static int arc_emac_tx(struct sk_buff *skb, struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tunsigned int len, *txbd_curr = &priv->txbd_curr;\n\tstruct net_device_stats *stats = &ndev->stats;\n\t__le32 *info = &priv->txbd[*txbd_curr].info;\n\tdma_addr_t addr;\n\n\tif (skb_padto(skb, ETH_ZLEN))\n\t\treturn NETDEV_TX_OK;\n\n\tlen = max_t(unsigned int, ETH_ZLEN, skb->len);\n\n\tif (unlikely(!arc_emac_tx_avail(priv))) {\n\t\tnetif_stop_queue(ndev);\n\t\tnetdev_err(ndev, \"BUG! Tx Ring full when queue awake!\\n\");\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\taddr = dma_map_single(&ndev->dev, (void *)skb->data, len,\n\t\t\t      DMA_TO_DEVICE);\n\n\tif (unlikely(dma_mapping_error(&ndev->dev, addr))) {\n\t\tstats->tx_dropped++;\n\t\tstats->tx_errors++;\n\t\tdev_kfree_skb(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\tdma_unmap_addr_set(&priv->tx_buff[*txbd_curr], addr, addr);\n\tdma_unmap_len_set(&priv->tx_buff[*txbd_curr], len, len);\n\n\tpriv->tx_buff[*txbd_curr].skb = skb;\n\tpriv->txbd[*txbd_curr].data = cpu_to_le32(addr);\n\n\t/* Make sure pointer to data buffer is set */\n\twmb();\n\n\tskb_tx_timestamp(skb);\n\n\t*info = cpu_to_le32(FOR_EMAC | FIRST_OR_LAST_MASK | len);\n\n\t/* Increment index to point to the next BD */\n\t*txbd_curr = (*txbd_curr + 1) % TX_BD_NUM;\n\n\t/* Ensure that tx_clean() sees the new txbd_curr before\n\t * checking the queue status. This prevents an unneeded wake\n\t * of the queue in tx_clean().\n\t */\n\tsmp_mb();\n\n\tif (!arc_emac_tx_avail(priv)) {\n\t\tnetif_stop_queue(ndev);\n\t\t/* Refresh tx_dirty */\n\t\tsmp_mb();\n\t\tif (arc_emac_tx_avail(priv))\n\t\t\tnetif_start_queue(ndev);\n\t}\n\n\tarc_reg_set(priv, R_STATUS, TXPL_MASK);\n\n\treturn NETDEV_TX_OK;\n}",
        "code_after_change": "static int arc_emac_tx(struct sk_buff *skb, struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tunsigned int len, *txbd_curr = &priv->txbd_curr;\n\tstruct net_device_stats *stats = &ndev->stats;\n\t__le32 *info = &priv->txbd[*txbd_curr].info;\n\tdma_addr_t addr;\n\n\tif (skb_padto(skb, ETH_ZLEN))\n\t\treturn NETDEV_TX_OK;\n\n\tlen = max_t(unsigned int, ETH_ZLEN, skb->len);\n\n\tif (unlikely(!arc_emac_tx_avail(priv))) {\n\t\tnetif_stop_queue(ndev);\n\t\tnetdev_err(ndev, \"BUG! Tx Ring full when queue awake!\\n\");\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\taddr = dma_map_single(&ndev->dev, (void *)skb->data, len,\n\t\t\t      DMA_TO_DEVICE);\n\n\tif (unlikely(dma_mapping_error(&ndev->dev, addr))) {\n\t\tstats->tx_dropped++;\n\t\tstats->tx_errors++;\n\t\tdev_kfree_skb(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\tdma_unmap_addr_set(&priv->tx_buff[*txbd_curr], addr, addr);\n\tdma_unmap_len_set(&priv->tx_buff[*txbd_curr], len, len);\n\n\tpriv->txbd[*txbd_curr].data = cpu_to_le32(addr);\n\n\t/* Make sure pointer to data buffer is set */\n\twmb();\n\n\tskb_tx_timestamp(skb);\n\n\t*info = cpu_to_le32(FOR_EMAC | FIRST_OR_LAST_MASK | len);\n\n\t/* Make sure info word is set */\n\twmb();\n\n\tpriv->tx_buff[*txbd_curr].skb = skb;\n\n\t/* Increment index to point to the next BD */\n\t*txbd_curr = (*txbd_curr + 1) % TX_BD_NUM;\n\n\t/* Ensure that tx_clean() sees the new txbd_curr before\n\t * checking the queue status. This prevents an unneeded wake\n\t * of the queue in tx_clean().\n\t */\n\tsmp_mb();\n\n\tif (!arc_emac_tx_avail(priv)) {\n\t\tnetif_stop_queue(ndev);\n\t\t/* Refresh tx_dirty */\n\t\tsmp_mb();\n\t\tif (arc_emac_tx_avail(priv))\n\t\t\tnetif_start_queue(ndev);\n\t}\n\n\tarc_reg_set(priv, R_STATUS, TXPL_MASK);\n\n\treturn NETDEV_TX_OK;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,7 +29,6 @@\n \tdma_unmap_addr_set(&priv->tx_buff[*txbd_curr], addr, addr);\n \tdma_unmap_len_set(&priv->tx_buff[*txbd_curr], len, len);\n \n-\tpriv->tx_buff[*txbd_curr].skb = skb;\n \tpriv->txbd[*txbd_curr].data = cpu_to_le32(addr);\n \n \t/* Make sure pointer to data buffer is set */\n@@ -38,6 +37,11 @@\n \tskb_tx_timestamp(skb);\n \n \t*info = cpu_to_le32(FOR_EMAC | FIRST_OR_LAST_MASK | len);\n+\n+\t/* Make sure info word is set */\n+\twmb();\n+\n+\tpriv->tx_buff[*txbd_curr].skb = skb;\n \n \t/* Increment index to point to the next BD */\n \t*txbd_curr = (*txbd_curr + 1) % TX_BD_NUM;",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* Make sure info word is set */",
                "\twmb();",
                "",
                "\tpriv->tx_buff[*txbd_curr].skb = skb;"
            ],
            "deleted": [
                "\tpriv->tx_buff[*txbd_curr].skb = skb;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/net/ethernet/arc/emac_main.c in the Linux kernel before 4.5. A use-after-free is caused by a race condition between the functions arc_emac_tx and arc_emac_tx_clean."
    },
    {
        "cve_id": "CVE-2016-10906",
        "code_before_change": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
        "code_after_change": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\t\ttx_buff->skb = NULL;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \t\tstruct sk_buff *skb = tx_buff->skb;\n \t\tunsigned int info = le32_to_cpu(txbd->info);\n \n-\t\tif ((info & FOR_EMAC) || !txbd->data)\n+\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)\n \t\t\tbreak;\n \n \t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n@@ -39,6 +39,7 @@\n \n \t\ttxbd->data = 0;\n \t\ttxbd->info = 0;\n+\t\ttx_buff->skb = NULL;\n \n \t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)",
                "\t\ttx_buff->skb = NULL;"
            ],
            "deleted": [
                "\t\tif ((info & FOR_EMAC) || !txbd->data)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/net/ethernet/arc/emac_main.c in the Linux kernel before 4.5. A use-after-free is caused by a race condition between the functions arc_emac_tx and arc_emac_tx_clean."
    },
    {
        "cve_id": "CVE-2016-2069",
        "code_before_change": "static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,\n\t\t\t     struct task_struct *tsk)\n{\n\tunsigned cpu = smp_processor_id();\n\n\tif (likely(prev != next)) {\n#ifdef CONFIG_SMP\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tthis_cpu_write(cpu_tlbstate.active_mm, next);\n#endif\n\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\n\t\t/* Re-load page tables */\n\t\tload_cr3(next->pgd);\n\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\n\t\t/* Stop flush ipis for the previous mm */\n\t\tcpumask_clear_cpu(cpu, mm_cpumask(prev));\n\n\t\t/* Load per-mm CR4 state */\n\t\tload_mm_cr4(next);\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\t\t/*\n\t\t * Load the LDT, if the LDT is different.\n\t\t *\n\t\t * It's possible that prev->context.ldt doesn't match\n\t\t * the LDT register.  This can happen if leave_mm(prev)\n\t\t * was called and then modify_ldt changed\n\t\t * prev->context.ldt but suppressed an IPI to this CPU.\n\t\t * In this case, prev->context.ldt != NULL, because we\n\t\t * never set context.ldt to NULL while the mm still\n\t\t * exists.  That means that next->context.ldt !=\n\t\t * prev->context.ldt, because mms never share an LDT.\n\t\t */\n\t\tif (unlikely(prev->context.ldt != next->context.ldt))\n\t\t\tload_mm_ldt(next);\n#endif\n\t}\n#ifdef CONFIG_SMP\n\t  else {\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tBUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);\n\n\t\tif (!cpumask_test_cpu(cpu, mm_cpumask(next))) {\n\t\t\t/*\n\t\t\t * On established mms, the mm_cpumask is only changed\n\t\t\t * from irq context, from ptep_clear_flush() while in\n\t\t\t * lazy tlb mode, and here. Irqs are blocked during\n\t\t\t * schedule, protecting us from simultaneous changes.\n\t\t\t */\n\t\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\t\t\t/*\n\t\t\t * We were in lazy tlb mode and leave_mm disabled\n\t\t\t * tlb flush IPI delivery. We must reload CR3\n\t\t\t * to make sure to use no freed page tables.\n\t\t\t */\n\t\t\tload_cr3(next->pgd);\n\t\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\t\t\tload_mm_cr4(next);\n\t\t\tload_mm_ldt(next);\n\t\t}\n\t}\n#endif\n}",
        "code_after_change": "static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,\n\t\t\t     struct task_struct *tsk)\n{\n\tunsigned cpu = smp_processor_id();\n\n\tif (likely(prev != next)) {\n#ifdef CONFIG_SMP\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tthis_cpu_write(cpu_tlbstate.active_mm, next);\n#endif\n\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\n\t\t/*\n\t\t * Re-load page tables.\n\t\t *\n\t\t * This logic has an ordering constraint:\n\t\t *\n\t\t *  CPU 0: Write to a PTE for 'next'\n\t\t *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.\n\t\t *  CPU 1: set bit 1 in next's mm_cpumask\n\t\t *  CPU 1: load from the PTE that CPU 0 writes (implicit)\n\t\t *\n\t\t * We need to prevent an outcome in which CPU 1 observes\n\t\t * the new PTE value and CPU 0 observes bit 1 clear in\n\t\t * mm_cpumask.  (If that occurs, then the IPI will never\n\t\t * be sent, and CPU 0's TLB will contain a stale entry.)\n\t\t *\n\t\t * The bad outcome can occur if either CPU's load is\n\t\t * reordered before that CPU's store, so both CPUs much\n\t\t * execute full barriers to prevent this from happening.\n\t\t *\n\t\t * Thus, switch_mm needs a full barrier between the\n\t\t * store to mm_cpumask and any operation that could load\n\t\t * from next->pgd.  This barrier synchronizes with\n\t\t * remote TLB flushers.  Fortunately, load_cr3 is\n\t\t * serializing and thus acts as a full barrier.\n\t\t *\n\t\t */\n\t\tload_cr3(next->pgd);\n\n\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\n\t\t/* Stop flush ipis for the previous mm */\n\t\tcpumask_clear_cpu(cpu, mm_cpumask(prev));\n\n\t\t/* Load per-mm CR4 state */\n\t\tload_mm_cr4(next);\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\t\t/*\n\t\t * Load the LDT, if the LDT is different.\n\t\t *\n\t\t * It's possible that prev->context.ldt doesn't match\n\t\t * the LDT register.  This can happen if leave_mm(prev)\n\t\t * was called and then modify_ldt changed\n\t\t * prev->context.ldt but suppressed an IPI to this CPU.\n\t\t * In this case, prev->context.ldt != NULL, because we\n\t\t * never set context.ldt to NULL while the mm still\n\t\t * exists.  That means that next->context.ldt !=\n\t\t * prev->context.ldt, because mms never share an LDT.\n\t\t */\n\t\tif (unlikely(prev->context.ldt != next->context.ldt))\n\t\t\tload_mm_ldt(next);\n#endif\n\t}\n#ifdef CONFIG_SMP\n\t  else {\n\t\tthis_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);\n\t\tBUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);\n\n\t\tif (!cpumask_test_cpu(cpu, mm_cpumask(next))) {\n\t\t\t/*\n\t\t\t * On established mms, the mm_cpumask is only changed\n\t\t\t * from irq context, from ptep_clear_flush() while in\n\t\t\t * lazy tlb mode, and here. Irqs are blocked during\n\t\t\t * schedule, protecting us from simultaneous changes.\n\t\t\t */\n\t\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\n\t\t\t/*\n\t\t\t * We were in lazy tlb mode and leave_mm disabled\n\t\t\t * tlb flush IPI delivery. We must reload CR3\n\t\t\t * to make sure to use no freed page tables.\n\t\t\t *\n\t\t\t * As above, this is a barrier that forces\n\t\t\t * TLB repopulation to be ordered after the\n\t\t\t * store to mm_cpumask.\n\t\t\t */\n\t\t\tload_cr3(next->pgd);\n\t\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\t\t\tload_mm_cr4(next);\n\t\t\tload_mm_ldt(next);\n\t\t}\n\t}\n#endif\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,8 +10,34 @@\n #endif\n \t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n \n-\t\t/* Re-load page tables */\n+\t\t/*\n+\t\t * Re-load page tables.\n+\t\t *\n+\t\t * This logic has an ordering constraint:\n+\t\t *\n+\t\t *  CPU 0: Write to a PTE for 'next'\n+\t\t *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.\n+\t\t *  CPU 1: set bit 1 in next's mm_cpumask\n+\t\t *  CPU 1: load from the PTE that CPU 0 writes (implicit)\n+\t\t *\n+\t\t * We need to prevent an outcome in which CPU 1 observes\n+\t\t * the new PTE value and CPU 0 observes bit 1 clear in\n+\t\t * mm_cpumask.  (If that occurs, then the IPI will never\n+\t\t * be sent, and CPU 0's TLB will contain a stale entry.)\n+\t\t *\n+\t\t * The bad outcome can occur if either CPU's load is\n+\t\t * reordered before that CPU's store, so both CPUs much\n+\t\t * execute full barriers to prevent this from happening.\n+\t\t *\n+\t\t * Thus, switch_mm needs a full barrier between the\n+\t\t * store to mm_cpumask and any operation that could load\n+\t\t * from next->pgd.  This barrier synchronizes with\n+\t\t * remote TLB flushers.  Fortunately, load_cr3 is\n+\t\t * serializing and thus acts as a full barrier.\n+\t\t *\n+\t\t */\n \t\tload_cr3(next->pgd);\n+\n \t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n \n \t\t/* Stop flush ipis for the previous mm */\n@@ -50,10 +76,15 @@\n \t\t\t * schedule, protecting us from simultaneous changes.\n \t\t\t */\n \t\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n+\n \t\t\t/*\n \t\t\t * We were in lazy tlb mode and leave_mm disabled\n \t\t\t * tlb flush IPI delivery. We must reload CR3\n \t\t\t * to make sure to use no freed page tables.\n+\t\t\t *\n+\t\t\t * As above, this is a barrier that forces\n+\t\t\t * TLB repopulation to be ordered after the\n+\t\t\t * store to mm_cpumask.\n \t\t\t */\n \t\t\tload_cr3(next->pgd);\n \t\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);",
        "function_modified_lines": {
            "added": [
                "\t\t/*",
                "\t\t * Re-load page tables.",
                "\t\t *",
                "\t\t * This logic has an ordering constraint:",
                "\t\t *",
                "\t\t *  CPU 0: Write to a PTE for 'next'",
                "\t\t *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.",
                "\t\t *  CPU 1: set bit 1 in next's mm_cpumask",
                "\t\t *  CPU 1: load from the PTE that CPU 0 writes (implicit)",
                "\t\t *",
                "\t\t * We need to prevent an outcome in which CPU 1 observes",
                "\t\t * the new PTE value and CPU 0 observes bit 1 clear in",
                "\t\t * mm_cpumask.  (If that occurs, then the IPI will never",
                "\t\t * be sent, and CPU 0's TLB will contain a stale entry.)",
                "\t\t *",
                "\t\t * The bad outcome can occur if either CPU's load is",
                "\t\t * reordered before that CPU's store, so both CPUs much",
                "\t\t * execute full barriers to prevent this from happening.",
                "\t\t *",
                "\t\t * Thus, switch_mm needs a full barrier between the",
                "\t\t * store to mm_cpumask and any operation that could load",
                "\t\t * from next->pgd.  This barrier synchronizes with",
                "\t\t * remote TLB flushers.  Fortunately, load_cr3 is",
                "\t\t * serializing and thus acts as a full barrier.",
                "\t\t *",
                "\t\t */",
                "",
                "",
                "\t\t\t *",
                "\t\t\t * As above, this is a barrier that forces",
                "\t\t\t * TLB repopulation to be ordered after the",
                "\t\t\t * store to mm_cpumask."
            ],
            "deleted": [
                "\t\t/* Re-load page tables */"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in arch/x86/mm/tlb.c in the Linux kernel before 4.4.1 allows local users to gain privileges by triggering access to a paging structure by a different CPU."
    },
    {
        "cve_id": "CVE-2016-2069",
        "code_before_change": "void flush_tlb_current_task(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tpreempt_disable();\n\n\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\tlocal_flush_tlb();\n\ttrace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);\n\tpreempt_enable();\n}",
        "code_after_change": "void flush_tlb_current_task(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tpreempt_disable();\n\n\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\n\t/* This is an implicit full barrier that synchronizes with switch_mm. */\n\tlocal_flush_tlb();\n\n\ttrace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);\n\tpreempt_enable();\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,10 @@\n \tpreempt_disable();\n \n \tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n+\n+\t/* This is an implicit full barrier that synchronizes with switch_mm. */\n \tlocal_flush_tlb();\n+\n \ttrace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);\n \tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n \t\tflush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* This is an implicit full barrier that synchronizes with switch_mm. */",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in arch/x86/mm/tlb.c in the Linux kernel before 4.4.1 allows local users to gain privileges by triggering access to a paging structure by a different CPU."
    },
    {
        "cve_id": "CVE-2016-2069",
        "code_before_change": "void flush_tlb_page(struct vm_area_struct *vma, unsigned long start)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpreempt_disable();\n\n\tif (current->active_mm == mm) {\n\t\tif (current->mm)\n\t\t\t__flush_tlb_one(start);\n\t\telse\n\t\t\tleave_mm(smp_processor_id());\n\t}\n\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, 0UL);\n\n\tpreempt_enable();\n}",
        "code_after_change": "void flush_tlb_page(struct vm_area_struct *vma, unsigned long start)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpreempt_disable();\n\n\tif (current->active_mm == mm) {\n\t\tif (current->mm) {\n\t\t\t/*\n\t\t\t * Implicit full barrier (INVLPG) that synchronizes\n\t\t\t * with switch_mm.\n\t\t\t */\n\t\t\t__flush_tlb_one(start);\n\t\t} else {\n\t\t\tleave_mm(smp_processor_id());\n\n\t\t\t/* Synchronize with switch_mm. */\n\t\t\tsmp_mb();\n\t\t}\n\t}\n\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, 0UL);\n\n\tpreempt_enable();\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,10 +5,18 @@\n \tpreempt_disable();\n \n \tif (current->active_mm == mm) {\n-\t\tif (current->mm)\n+\t\tif (current->mm) {\n+\t\t\t/*\n+\t\t\t * Implicit full barrier (INVLPG) that synchronizes\n+\t\t\t * with switch_mm.\n+\t\t\t */\n \t\t\t__flush_tlb_one(start);\n-\t\telse\n+\t\t} else {\n \t\t\tleave_mm(smp_processor_id());\n+\n+\t\t\t/* Synchronize with switch_mm. */\n+\t\t\tsmp_mb();\n+\t\t}\n \t}\n \n \tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)",
        "function_modified_lines": {
            "added": [
                "\t\tif (current->mm) {",
                "\t\t\t/*",
                "\t\t\t * Implicit full barrier (INVLPG) that synchronizes",
                "\t\t\t * with switch_mm.",
                "\t\t\t */",
                "\t\t} else {",
                "",
                "\t\t\t/* Synchronize with switch_mm. */",
                "\t\t\tsmp_mb();",
                "\t\t}"
            ],
            "deleted": [
                "\t\tif (current->mm)",
                "\t\telse"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in arch/x86/mm/tlb.c in the Linux kernel before 4.4.1 allows local users to gain privileges by triggering access to a paging structure by a different CPU."
    },
    {
        "cve_id": "CVE-2016-2069",
        "code_before_change": "void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, unsigned long vmflag)\n{\n\tunsigned long addr;\n\t/* do a global flush by default */\n\tunsigned long base_pages_to_flush = TLB_FLUSH_ALL;\n\n\tpreempt_disable();\n\tif (current->active_mm != mm)\n\t\tgoto out;\n\n\tif (!current->mm) {\n\t\tleave_mm(smp_processor_id());\n\t\tgoto out;\n\t}\n\n\tif ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))\n\t\tbase_pages_to_flush = (end - start) >> PAGE_SHIFT;\n\n\tif (base_pages_to_flush > tlb_single_page_flush_ceiling) {\n\t\tbase_pages_to_flush = TLB_FLUSH_ALL;\n\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\t\tlocal_flush_tlb();\n\t} else {\n\t\t/* flush range by one by one 'invlpg' */\n\t\tfor (addr = start; addr < end;\taddr += PAGE_SIZE) {\n\t\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);\n\t\t\t__flush_tlb_single(addr);\n\t\t}\n\t}\n\ttrace_tlb_flush(TLB_LOCAL_MM_SHOOTDOWN, base_pages_to_flush);\nout:\n\tif (base_pages_to_flush == TLB_FLUSH_ALL) {\n\t\tstart = 0UL;\n\t\tend = TLB_FLUSH_ALL;\n\t}\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, end);\n\tpreempt_enable();\n}",
        "code_after_change": "void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, unsigned long vmflag)\n{\n\tunsigned long addr;\n\t/* do a global flush by default */\n\tunsigned long base_pages_to_flush = TLB_FLUSH_ALL;\n\n\tpreempt_disable();\n\tif (current->active_mm != mm) {\n\t\t/* Synchronize with switch_mm. */\n\t\tsmp_mb();\n\n\t\tgoto out;\n\t}\n\n\tif (!current->mm) {\n\t\tleave_mm(smp_processor_id());\n\n\t\t/* Synchronize with switch_mm. */\n\t\tsmp_mb();\n\n\t\tgoto out;\n\t}\n\n\tif ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))\n\t\tbase_pages_to_flush = (end - start) >> PAGE_SHIFT;\n\n\t/*\n\t * Both branches below are implicit full barriers (MOV to CR or\n\t * INVLPG) that synchronize with switch_mm.\n\t */\n\tif (base_pages_to_flush > tlb_single_page_flush_ceiling) {\n\t\tbase_pages_to_flush = TLB_FLUSH_ALL;\n\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\t\tlocal_flush_tlb();\n\t} else {\n\t\t/* flush range by one by one 'invlpg' */\n\t\tfor (addr = start; addr < end;\taddr += PAGE_SIZE) {\n\t\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);\n\t\t\t__flush_tlb_single(addr);\n\t\t}\n\t}\n\ttrace_tlb_flush(TLB_LOCAL_MM_SHOOTDOWN, base_pages_to_flush);\nout:\n\tif (base_pages_to_flush == TLB_FLUSH_ALL) {\n\t\tstart = 0UL;\n\t\tend = TLB_FLUSH_ALL;\n\t}\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, end);\n\tpreempt_enable();\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,17 +6,29 @@\n \tunsigned long base_pages_to_flush = TLB_FLUSH_ALL;\n \n \tpreempt_disable();\n-\tif (current->active_mm != mm)\n+\tif (current->active_mm != mm) {\n+\t\t/* Synchronize with switch_mm. */\n+\t\tsmp_mb();\n+\n \t\tgoto out;\n+\t}\n \n \tif (!current->mm) {\n \t\tleave_mm(smp_processor_id());\n+\n+\t\t/* Synchronize with switch_mm. */\n+\t\tsmp_mb();\n+\n \t\tgoto out;\n \t}\n \n \tif ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))\n \t\tbase_pages_to_flush = (end - start) >> PAGE_SHIFT;\n \n+\t/*\n+\t * Both branches below are implicit full barriers (MOV to CR or\n+\t * INVLPG) that synchronize with switch_mm.\n+\t */\n \tif (base_pages_to_flush > tlb_single_page_flush_ceiling) {\n \t\tbase_pages_to_flush = TLB_FLUSH_ALL;\n \t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);",
        "function_modified_lines": {
            "added": [
                "\tif (current->active_mm != mm) {",
                "\t\t/* Synchronize with switch_mm. */",
                "\t\tsmp_mb();",
                "",
                "\t}",
                "",
                "\t\t/* Synchronize with switch_mm. */",
                "\t\tsmp_mb();",
                "",
                "\t/*",
                "\t * Both branches below are implicit full barriers (MOV to CR or",
                "\t * INVLPG) that synchronize with switch_mm.",
                "\t */"
            ],
            "deleted": [
                "\tif (current->active_mm != mm)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in arch/x86/mm/tlb.c in the Linux kernel before 4.4.1 allows local users to gain privileges by triggering access to a paging structure by a different CPU."
    },
    {
        "cve_id": "CVE-2016-2544",
        "code_before_change": "static void queue_delete(struct snd_seq_queue *q)\n{\n\t/* stop and release the timer */\n\tsnd_seq_timer_stop(q->timer);\n\tsnd_seq_timer_close(q);\n\t/* wait until access free */\n\tsnd_use_lock_sync(&q->use_lock);\n\t/* release resources... */\n\tsnd_seq_prioq_delete(&q->tickq);\n\tsnd_seq_prioq_delete(&q->timeq);\n\tsnd_seq_timer_delete(&q->timer);\n\n\tkfree(q);\n}",
        "code_after_change": "static void queue_delete(struct snd_seq_queue *q)\n{\n\t/* stop and release the timer */\n\tmutex_lock(&q->timer_mutex);\n\tsnd_seq_timer_stop(q->timer);\n\tsnd_seq_timer_close(q);\n\tmutex_unlock(&q->timer_mutex);\n\t/* wait until access free */\n\tsnd_use_lock_sync(&q->use_lock);\n\t/* release resources... */\n\tsnd_seq_prioq_delete(&q->tickq);\n\tsnd_seq_prioq_delete(&q->timeq);\n\tsnd_seq_timer_delete(&q->timer);\n\n\tkfree(q);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,10 @@\n static void queue_delete(struct snd_seq_queue *q)\n {\n \t/* stop and release the timer */\n+\tmutex_lock(&q->timer_mutex);\n \tsnd_seq_timer_stop(q->timer);\n \tsnd_seq_timer_close(q);\n+\tmutex_unlock(&q->timer_mutex);\n \t/* wait until access free */\n \tsnd_use_lock_sync(&q->use_lock);\n \t/* release resources... */",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&q->timer_mutex);",
                "\tmutex_unlock(&q->timer_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the queue_delete function in sound/core/seq/seq_queue.c in the Linux kernel before 4.4.1 allows local users to cause a denial of service (use-after-free and system crash) by making an ioctl call at a certain time."
    },
    {
        "cve_id": "CVE-2016-2545",
        "code_before_change": "void snd_timer_interrupt(struct snd_timer * timer, unsigned long ticks_left)\n{\n\tstruct snd_timer_instance *ti, *ts, *tmp;\n\tunsigned long resolution, ticks;\n\tstruct list_head *p, *ack_list_head;\n\tunsigned long flags;\n\tint use_tasklet = 0;\n\n\tif (timer == NULL)\n\t\treturn;\n\n\tspin_lock_irqsave(&timer->lock, flags);\n\n\t/* remember the current resolution */\n\tif (timer->hw.c_resolution)\n\t\tresolution = timer->hw.c_resolution(timer);\n\telse\n\t\tresolution = timer->hw.resolution;\n\n\t/* loop for all active instances\n\t * Here we cannot use list_for_each_entry because the active_list of a\n\t * processed instance is relinked to done_list_head before the callback\n\t * is called.\n\t */\n\tlist_for_each_entry_safe(ti, tmp, &timer->active_list_head,\n\t\t\t\t active_list) {\n\t\tif (!(ti->flags & SNDRV_TIMER_IFLG_RUNNING))\n\t\t\tcontinue;\n\t\tti->pticks += ticks_left;\n\t\tti->resolution = resolution;\n\t\tif (ti->cticks < ticks_left)\n\t\t\tti->cticks = 0;\n\t\telse\n\t\t\tti->cticks -= ticks_left;\n\t\tif (ti->cticks) /* not expired */\n\t\t\tcontinue;\n\t\tif (ti->flags & SNDRV_TIMER_IFLG_AUTO) {\n\t\t\tti->cticks = ti->ticks;\n\t\t} else {\n\t\t\tti->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\tif (--timer->running)\n\t\t\t\tlist_del(&ti->active_list);\n\t\t}\n\t\tif ((timer->hw.flags & SNDRV_TIMER_HW_TASKLET) ||\n\t\t    (ti->flags & SNDRV_TIMER_IFLG_FAST))\n\t\t\tack_list_head = &timer->ack_list_head;\n\t\telse\n\t\t\tack_list_head = &timer->sack_list_head;\n\t\tif (list_empty(&ti->ack_list))\n\t\t\tlist_add_tail(&ti->ack_list, ack_list_head);\n\t\tlist_for_each_entry(ts, &ti->slave_active_head, active_list) {\n\t\t\tts->pticks = ti->pticks;\n\t\t\tts->resolution = resolution;\n\t\t\tif (list_empty(&ts->ack_list))\n\t\t\t\tlist_add_tail(&ts->ack_list, ack_list_head);\n\t\t}\n\t}\n\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED)\n\t\tsnd_timer_reschedule(timer, timer->sticks);\n\tif (timer->running) {\n\t\tif (timer->hw.flags & SNDRV_TIMER_HW_STOP) {\n\t\t\ttimer->hw.stop(timer);\n\t\t\ttimer->flags |= SNDRV_TIMER_FLG_CHANGE;\n\t\t}\n\t\tif (!(timer->hw.flags & SNDRV_TIMER_HW_AUTO) ||\n\t\t    (timer->flags & SNDRV_TIMER_FLG_CHANGE)) {\n\t\t\t/* restart timer */\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\ttimer->hw.start(timer);\n\t\t}\n\t} else {\n\t\ttimer->hw.stop(timer);\n\t}\n\n\t/* now process all fast callbacks */\n\twhile (!list_empty(&timer->ack_list_head)) {\n\t\tp = timer->ack_list_head.next;\t\t/* get first item */\n\t\tti = list_entry(p, struct snd_timer_instance, ack_list);\n\n\t\t/* remove from ack_list and make empty */\n\t\tlist_del_init(p);\n\n\t\tticks = ti->pticks;\n\t\tti->pticks = 0;\n\n\t\tti->flags |= SNDRV_TIMER_IFLG_CALLBACK;\n\t\tspin_unlock(&timer->lock);\n\t\tif (ti->callback)\n\t\t\tti->callback(ti, resolution, ticks);\n\t\tspin_lock(&timer->lock);\n\t\tti->flags &= ~SNDRV_TIMER_IFLG_CALLBACK;\n\t}\n\n\t/* do we have any slow callbacks? */\n\tuse_tasklet = !list_empty(&timer->sack_list_head);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n\n\tif (use_tasklet)\n\t\ttasklet_schedule(&timer->task_queue);\n}",
        "code_after_change": "void snd_timer_interrupt(struct snd_timer * timer, unsigned long ticks_left)\n{\n\tstruct snd_timer_instance *ti, *ts, *tmp;\n\tunsigned long resolution, ticks;\n\tstruct list_head *p, *ack_list_head;\n\tunsigned long flags;\n\tint use_tasklet = 0;\n\n\tif (timer == NULL)\n\t\treturn;\n\n\tspin_lock_irqsave(&timer->lock, flags);\n\n\t/* remember the current resolution */\n\tif (timer->hw.c_resolution)\n\t\tresolution = timer->hw.c_resolution(timer);\n\telse\n\t\tresolution = timer->hw.resolution;\n\n\t/* loop for all active instances\n\t * Here we cannot use list_for_each_entry because the active_list of a\n\t * processed instance is relinked to done_list_head before the callback\n\t * is called.\n\t */\n\tlist_for_each_entry_safe(ti, tmp, &timer->active_list_head,\n\t\t\t\t active_list) {\n\t\tif (!(ti->flags & SNDRV_TIMER_IFLG_RUNNING))\n\t\t\tcontinue;\n\t\tti->pticks += ticks_left;\n\t\tti->resolution = resolution;\n\t\tif (ti->cticks < ticks_left)\n\t\t\tti->cticks = 0;\n\t\telse\n\t\t\tti->cticks -= ticks_left;\n\t\tif (ti->cticks) /* not expired */\n\t\t\tcontinue;\n\t\tif (ti->flags & SNDRV_TIMER_IFLG_AUTO) {\n\t\t\tti->cticks = ti->ticks;\n\t\t} else {\n\t\t\tti->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\tif (--timer->running)\n\t\t\t\tlist_del_init(&ti->active_list);\n\t\t}\n\t\tif ((timer->hw.flags & SNDRV_TIMER_HW_TASKLET) ||\n\t\t    (ti->flags & SNDRV_TIMER_IFLG_FAST))\n\t\t\tack_list_head = &timer->ack_list_head;\n\t\telse\n\t\t\tack_list_head = &timer->sack_list_head;\n\t\tif (list_empty(&ti->ack_list))\n\t\t\tlist_add_tail(&ti->ack_list, ack_list_head);\n\t\tlist_for_each_entry(ts, &ti->slave_active_head, active_list) {\n\t\t\tts->pticks = ti->pticks;\n\t\t\tts->resolution = resolution;\n\t\t\tif (list_empty(&ts->ack_list))\n\t\t\t\tlist_add_tail(&ts->ack_list, ack_list_head);\n\t\t}\n\t}\n\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED)\n\t\tsnd_timer_reschedule(timer, timer->sticks);\n\tif (timer->running) {\n\t\tif (timer->hw.flags & SNDRV_TIMER_HW_STOP) {\n\t\t\ttimer->hw.stop(timer);\n\t\t\ttimer->flags |= SNDRV_TIMER_FLG_CHANGE;\n\t\t}\n\t\tif (!(timer->hw.flags & SNDRV_TIMER_HW_AUTO) ||\n\t\t    (timer->flags & SNDRV_TIMER_FLG_CHANGE)) {\n\t\t\t/* restart timer */\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\ttimer->hw.start(timer);\n\t\t}\n\t} else {\n\t\ttimer->hw.stop(timer);\n\t}\n\n\t/* now process all fast callbacks */\n\twhile (!list_empty(&timer->ack_list_head)) {\n\t\tp = timer->ack_list_head.next;\t\t/* get first item */\n\t\tti = list_entry(p, struct snd_timer_instance, ack_list);\n\n\t\t/* remove from ack_list and make empty */\n\t\tlist_del_init(p);\n\n\t\tticks = ti->pticks;\n\t\tti->pticks = 0;\n\n\t\tti->flags |= SNDRV_TIMER_IFLG_CALLBACK;\n\t\tspin_unlock(&timer->lock);\n\t\tif (ti->callback)\n\t\t\tti->callback(ti, resolution, ticks);\n\t\tspin_lock(&timer->lock);\n\t\tti->flags &= ~SNDRV_TIMER_IFLG_CALLBACK;\n\t}\n\n\t/* do we have any slow callbacks? */\n\tuse_tasklet = !list_empty(&timer->sack_list_head);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n\n\tif (use_tasklet)\n\t\ttasklet_schedule(&timer->task_queue);\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,7 +39,7 @@\n \t\t} else {\n \t\t\tti->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n \t\t\tif (--timer->running)\n-\t\t\t\tlist_del(&ti->active_list);\n+\t\t\t\tlist_del_init(&ti->active_list);\n \t\t}\n \t\tif ((timer->hw.flags & SNDRV_TIMER_HW_TASKLET) ||\n \t\t    (ti->flags & SNDRV_TIMER_IFLG_FAST))",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tlist_del_init(&ti->active_list);"
            ],
            "deleted": [
                "\t\t\t\tlist_del(&ti->active_list);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The snd_timer_interrupt function in sound/core/timer.c in the Linux kernel before 4.4.1 does not properly maintain a certain linked list, which allows local users to cause a denial of service (race condition and system crash) via a crafted ioctl call."
    },
    {
        "cve_id": "CVE-2016-2546",
        "code_before_change": "static int snd_timer_user_tselect(struct file *file,\n\t\t\t\t  struct snd_timer_select __user *_tselect)\n{\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_select tselect;\n\tchar str[32];\n\tint err = 0;\n\n\ttu = file->private_data;\n\tmutex_lock(&tu->tread_sem);\n\tif (tu->timeri) {\n\t\tsnd_timer_close(tu->timeri);\n\t\ttu->timeri = NULL;\n\t}\n\tif (copy_from_user(&tselect, _tselect, sizeof(tselect))) {\n\t\terr = -EFAULT;\n\t\tgoto __err;\n\t}\n\tsprintf(str, \"application %i\", current->pid);\n\tif (tselect.id.dev_class != SNDRV_TIMER_CLASS_SLAVE)\n\t\ttselect.id.dev_sclass = SNDRV_TIMER_SCLASS_APPLICATION;\n\terr = snd_timer_open(&tu->timeri, str, &tselect.id, current->pid);\n\tif (err < 0)\n\t\tgoto __err;\n\n\tkfree(tu->queue);\n\ttu->queue = NULL;\n\tkfree(tu->tqueue);\n\ttu->tqueue = NULL;\n\tif (tu->tread) {\n\t\ttu->tqueue = kmalloc(tu->queue_size * sizeof(struct snd_timer_tread),\n\t\t\t\t     GFP_KERNEL);\n\t\tif (tu->tqueue == NULL)\n\t\t\terr = -ENOMEM;\n\t} else {\n\t\ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (tu->queue == NULL)\n\t\t\terr = -ENOMEM;\n\t}\n\n      \tif (err < 0) {\n\t\tsnd_timer_close(tu->timeri);\n      \t\ttu->timeri = NULL;\n      \t} else {\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_FAST;\n\t\ttu->timeri->callback = tu->tread\n\t\t\t? snd_timer_user_tinterrupt : snd_timer_user_interrupt;\n\t\ttu->timeri->ccallback = snd_timer_user_ccallback;\n\t\ttu->timeri->callback_data = (void *)tu;\n\t}\n\n      __err:\n      \tmutex_unlock(&tu->tread_sem);\n\treturn err;\n}",
        "code_after_change": "static int snd_timer_user_tselect(struct file *file,\n\t\t\t\t  struct snd_timer_select __user *_tselect)\n{\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_select tselect;\n\tchar str[32];\n\tint err = 0;\n\n\ttu = file->private_data;\n\tif (tu->timeri) {\n\t\tsnd_timer_close(tu->timeri);\n\t\ttu->timeri = NULL;\n\t}\n\tif (copy_from_user(&tselect, _tselect, sizeof(tselect))) {\n\t\terr = -EFAULT;\n\t\tgoto __err;\n\t}\n\tsprintf(str, \"application %i\", current->pid);\n\tif (tselect.id.dev_class != SNDRV_TIMER_CLASS_SLAVE)\n\t\ttselect.id.dev_sclass = SNDRV_TIMER_SCLASS_APPLICATION;\n\terr = snd_timer_open(&tu->timeri, str, &tselect.id, current->pid);\n\tif (err < 0)\n\t\tgoto __err;\n\n\tkfree(tu->queue);\n\ttu->queue = NULL;\n\tkfree(tu->tqueue);\n\ttu->tqueue = NULL;\n\tif (tu->tread) {\n\t\ttu->tqueue = kmalloc(tu->queue_size * sizeof(struct snd_timer_tread),\n\t\t\t\t     GFP_KERNEL);\n\t\tif (tu->tqueue == NULL)\n\t\t\terr = -ENOMEM;\n\t} else {\n\t\ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (tu->queue == NULL)\n\t\t\terr = -ENOMEM;\n\t}\n\n      \tif (err < 0) {\n\t\tsnd_timer_close(tu->timeri);\n      \t\ttu->timeri = NULL;\n      \t} else {\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_FAST;\n\t\ttu->timeri->callback = tu->tread\n\t\t\t? snd_timer_user_tinterrupt : snd_timer_user_interrupt;\n\t\ttu->timeri->ccallback = snd_timer_user_ccallback;\n\t\ttu->timeri->callback_data = (void *)tu;\n\t}\n\n      __err:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,6 @@\n \tint err = 0;\n \n \ttu = file->private_data;\n-\tmutex_lock(&tu->tread_sem);\n \tif (tu->timeri) {\n \t\tsnd_timer_close(tu->timeri);\n \t\ttu->timeri = NULL;\n@@ -51,6 +50,5 @@\n \t}\n \n       __err:\n-      \tmutex_unlock(&tu->tread_sem);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmutex_lock(&tu->tread_sem);",
                "      \tmutex_unlock(&tu->tread_sem);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 uses an incorrect type of mutex, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call."
    },
    {
        "cve_id": "CVE-2016-2546",
        "code_before_change": "static long snd_timer_user_ioctl(struct file *file, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct snd_timer_user *tu;\n\tvoid __user *argp = (void __user *)arg;\n\tint __user *p = argp;\n\n\ttu = file->private_data;\n\tswitch (cmd) {\n\tcase SNDRV_TIMER_IOCTL_PVERSION:\n\t\treturn put_user(SNDRV_TIMER_VERSION, p) ? -EFAULT : 0;\n\tcase SNDRV_TIMER_IOCTL_NEXT_DEVICE:\n\t\treturn snd_timer_user_next_device(argp);\n\tcase SNDRV_TIMER_IOCTL_TREAD:\n\t{\n\t\tint xarg;\n\n\t\tmutex_lock(&tu->tread_sem);\n\t\tif (tu->timeri)\t{\t/* too late */\n\t\t\tmutex_unlock(&tu->tread_sem);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tif (get_user(xarg, p)) {\n\t\t\tmutex_unlock(&tu->tread_sem);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\ttu->tread = xarg ? 1 : 0;\n\t\tmutex_unlock(&tu->tread_sem);\n\t\treturn 0;\n\t}\n\tcase SNDRV_TIMER_IOCTL_GINFO:\n\t\treturn snd_timer_user_ginfo(file, argp);\n\tcase SNDRV_TIMER_IOCTL_GPARAMS:\n\t\treturn snd_timer_user_gparams(file, argp);\n\tcase SNDRV_TIMER_IOCTL_GSTATUS:\n\t\treturn snd_timer_user_gstatus(file, argp);\n\tcase SNDRV_TIMER_IOCTL_SELECT:\n\t\treturn snd_timer_user_tselect(file, argp);\n\tcase SNDRV_TIMER_IOCTL_INFO:\n\t\treturn snd_timer_user_info(file, argp);\n\tcase SNDRV_TIMER_IOCTL_PARAMS:\n\t\treturn snd_timer_user_params(file, argp);\n\tcase SNDRV_TIMER_IOCTL_STATUS:\n\t\treturn snd_timer_user_status(file, argp);\n\tcase SNDRV_TIMER_IOCTL_START:\n\tcase SNDRV_TIMER_IOCTL_START_OLD:\n\t\treturn snd_timer_user_start(file);\n\tcase SNDRV_TIMER_IOCTL_STOP:\n\tcase SNDRV_TIMER_IOCTL_STOP_OLD:\n\t\treturn snd_timer_user_stop(file);\n\tcase SNDRV_TIMER_IOCTL_CONTINUE:\n\tcase SNDRV_TIMER_IOCTL_CONTINUE_OLD:\n\t\treturn snd_timer_user_continue(file);\n\tcase SNDRV_TIMER_IOCTL_PAUSE:\n\tcase SNDRV_TIMER_IOCTL_PAUSE_OLD:\n\t\treturn snd_timer_user_pause(file);\n\t}\n\treturn -ENOTTY;\n}",
        "code_after_change": "static long snd_timer_user_ioctl(struct file *file, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct snd_timer_user *tu = file->private_data;\n\tlong ret;\n\n\tmutex_lock(&tu->ioctl_lock);\n\tret = __snd_timer_user_ioctl(file, cmd, arg);\n\tmutex_unlock(&tu->ioctl_lock);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,59 +1,11 @@\n static long snd_timer_user_ioctl(struct file *file, unsigned int cmd,\n \t\t\t\t unsigned long arg)\n {\n-\tstruct snd_timer_user *tu;\n-\tvoid __user *argp = (void __user *)arg;\n-\tint __user *p = argp;\n+\tstruct snd_timer_user *tu = file->private_data;\n+\tlong ret;\n \n-\ttu = file->private_data;\n-\tswitch (cmd) {\n-\tcase SNDRV_TIMER_IOCTL_PVERSION:\n-\t\treturn put_user(SNDRV_TIMER_VERSION, p) ? -EFAULT : 0;\n-\tcase SNDRV_TIMER_IOCTL_NEXT_DEVICE:\n-\t\treturn snd_timer_user_next_device(argp);\n-\tcase SNDRV_TIMER_IOCTL_TREAD:\n-\t{\n-\t\tint xarg;\n-\n-\t\tmutex_lock(&tu->tread_sem);\n-\t\tif (tu->timeri)\t{\t/* too late */\n-\t\t\tmutex_unlock(&tu->tread_sem);\n-\t\t\treturn -EBUSY;\n-\t\t}\n-\t\tif (get_user(xarg, p)) {\n-\t\t\tmutex_unlock(&tu->tread_sem);\n-\t\t\treturn -EFAULT;\n-\t\t}\n-\t\ttu->tread = xarg ? 1 : 0;\n-\t\tmutex_unlock(&tu->tread_sem);\n-\t\treturn 0;\n-\t}\n-\tcase SNDRV_TIMER_IOCTL_GINFO:\n-\t\treturn snd_timer_user_ginfo(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_GPARAMS:\n-\t\treturn snd_timer_user_gparams(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_GSTATUS:\n-\t\treturn snd_timer_user_gstatus(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_SELECT:\n-\t\treturn snd_timer_user_tselect(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_INFO:\n-\t\treturn snd_timer_user_info(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_PARAMS:\n-\t\treturn snd_timer_user_params(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_STATUS:\n-\t\treturn snd_timer_user_status(file, argp);\n-\tcase SNDRV_TIMER_IOCTL_START:\n-\tcase SNDRV_TIMER_IOCTL_START_OLD:\n-\t\treturn snd_timer_user_start(file);\n-\tcase SNDRV_TIMER_IOCTL_STOP:\n-\tcase SNDRV_TIMER_IOCTL_STOP_OLD:\n-\t\treturn snd_timer_user_stop(file);\n-\tcase SNDRV_TIMER_IOCTL_CONTINUE:\n-\tcase SNDRV_TIMER_IOCTL_CONTINUE_OLD:\n-\t\treturn snd_timer_user_continue(file);\n-\tcase SNDRV_TIMER_IOCTL_PAUSE:\n-\tcase SNDRV_TIMER_IOCTL_PAUSE_OLD:\n-\t\treturn snd_timer_user_pause(file);\n-\t}\n-\treturn -ENOTTY;\n+\tmutex_lock(&tu->ioctl_lock);\n+\tret = __snd_timer_user_ioctl(file, cmd, arg);\n+\tmutex_unlock(&tu->ioctl_lock);\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct snd_timer_user *tu = file->private_data;",
                "\tlong ret;",
                "\tmutex_lock(&tu->ioctl_lock);",
                "\tret = __snd_timer_user_ioctl(file, cmd, arg);",
                "\tmutex_unlock(&tu->ioctl_lock);",
                "\treturn ret;"
            ],
            "deleted": [
                "\tstruct snd_timer_user *tu;",
                "\tvoid __user *argp = (void __user *)arg;",
                "\tint __user *p = argp;",
                "\ttu = file->private_data;",
                "\tswitch (cmd) {",
                "\tcase SNDRV_TIMER_IOCTL_PVERSION:",
                "\t\treturn put_user(SNDRV_TIMER_VERSION, p) ? -EFAULT : 0;",
                "\tcase SNDRV_TIMER_IOCTL_NEXT_DEVICE:",
                "\t\treturn snd_timer_user_next_device(argp);",
                "\tcase SNDRV_TIMER_IOCTL_TREAD:",
                "\t{",
                "\t\tint xarg;",
                "",
                "\t\tmutex_lock(&tu->tread_sem);",
                "\t\tif (tu->timeri)\t{\t/* too late */",
                "\t\t\tmutex_unlock(&tu->tread_sem);",
                "\t\t\treturn -EBUSY;",
                "\t\t}",
                "\t\tif (get_user(xarg, p)) {",
                "\t\t\tmutex_unlock(&tu->tread_sem);",
                "\t\t\treturn -EFAULT;",
                "\t\t}",
                "\t\ttu->tread = xarg ? 1 : 0;",
                "\t\tmutex_unlock(&tu->tread_sem);",
                "\t\treturn 0;",
                "\t}",
                "\tcase SNDRV_TIMER_IOCTL_GINFO:",
                "\t\treturn snd_timer_user_ginfo(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_GPARAMS:",
                "\t\treturn snd_timer_user_gparams(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_GSTATUS:",
                "\t\treturn snd_timer_user_gstatus(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_SELECT:",
                "\t\treturn snd_timer_user_tselect(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_INFO:",
                "\t\treturn snd_timer_user_info(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_PARAMS:",
                "\t\treturn snd_timer_user_params(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_STATUS:",
                "\t\treturn snd_timer_user_status(file, argp);",
                "\tcase SNDRV_TIMER_IOCTL_START:",
                "\tcase SNDRV_TIMER_IOCTL_START_OLD:",
                "\t\treturn snd_timer_user_start(file);",
                "\tcase SNDRV_TIMER_IOCTL_STOP:",
                "\tcase SNDRV_TIMER_IOCTL_STOP_OLD:",
                "\t\treturn snd_timer_user_stop(file);",
                "\tcase SNDRV_TIMER_IOCTL_CONTINUE:",
                "\tcase SNDRV_TIMER_IOCTL_CONTINUE_OLD:",
                "\t\treturn snd_timer_user_continue(file);",
                "\tcase SNDRV_TIMER_IOCTL_PAUSE:",
                "\tcase SNDRV_TIMER_IOCTL_PAUSE_OLD:",
                "\t\treturn snd_timer_user_pause(file);",
                "\t}",
                "\treturn -ENOTTY;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 uses an incorrect type of mutex, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call."
    },
    {
        "cve_id": "CVE-2016-2546",
        "code_before_change": "static int snd_timer_user_open(struct inode *inode, struct file *file)\n{\n\tstruct snd_timer_user *tu;\n\tint err;\n\n\terr = nonseekable_open(inode, file);\n\tif (err < 0)\n\t\treturn err;\n\n\ttu = kzalloc(sizeof(*tu), GFP_KERNEL);\n\tif (tu == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&tu->qlock);\n\tinit_waitqueue_head(&tu->qchange_sleep);\n\tmutex_init(&tu->tread_sem);\n\ttu->ticks = 1;\n\ttu->queue_size = 128;\n\ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),\n\t\t\t    GFP_KERNEL);\n\tif (tu->queue == NULL) {\n\t\tkfree(tu);\n\t\treturn -ENOMEM;\n\t}\n\tfile->private_data = tu;\n\treturn 0;\n}",
        "code_after_change": "static int snd_timer_user_open(struct inode *inode, struct file *file)\n{\n\tstruct snd_timer_user *tu;\n\tint err;\n\n\terr = nonseekable_open(inode, file);\n\tif (err < 0)\n\t\treturn err;\n\n\ttu = kzalloc(sizeof(*tu), GFP_KERNEL);\n\tif (tu == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&tu->qlock);\n\tinit_waitqueue_head(&tu->qchange_sleep);\n\tmutex_init(&tu->ioctl_lock);\n\ttu->ticks = 1;\n\ttu->queue_size = 128;\n\ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),\n\t\t\t    GFP_KERNEL);\n\tif (tu->queue == NULL) {\n\t\tkfree(tu);\n\t\treturn -ENOMEM;\n\t}\n\tfile->private_data = tu;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,7 @@\n \t\treturn -ENOMEM;\n \tspin_lock_init(&tu->qlock);\n \tinit_waitqueue_head(&tu->qchange_sleep);\n-\tmutex_init(&tu->tread_sem);\n+\tmutex_init(&tu->ioctl_lock);\n \ttu->ticks = 1;\n \ttu->queue_size = 128;\n \ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),",
        "function_modified_lines": {
            "added": [
                "\tmutex_init(&tu->ioctl_lock);"
            ],
            "deleted": [
                "\tmutex_init(&tu->tread_sem);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 uses an incorrect type of mutex, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call."
    },
    {
        "cve_id": "CVE-2016-2546",
        "code_before_change": "static int snd_timer_user_release(struct inode *inode, struct file *file)\n{\n\tstruct snd_timer_user *tu;\n\n\tif (file->private_data) {\n\t\ttu = file->private_data;\n\t\tfile->private_data = NULL;\n\t\tif (tu->timeri)\n\t\t\tsnd_timer_close(tu->timeri);\n\t\tkfree(tu->queue);\n\t\tkfree(tu->tqueue);\n\t\tkfree(tu);\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int snd_timer_user_release(struct inode *inode, struct file *file)\n{\n\tstruct snd_timer_user *tu;\n\n\tif (file->private_data) {\n\t\ttu = file->private_data;\n\t\tfile->private_data = NULL;\n\t\tmutex_lock(&tu->ioctl_lock);\n\t\tif (tu->timeri)\n\t\t\tsnd_timer_close(tu->timeri);\n\t\tmutex_unlock(&tu->ioctl_lock);\n\t\tkfree(tu->queue);\n\t\tkfree(tu->tqueue);\n\t\tkfree(tu);\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,10 @@\n \tif (file->private_data) {\n \t\ttu = file->private_data;\n \t\tfile->private_data = NULL;\n+\t\tmutex_lock(&tu->ioctl_lock);\n \t\tif (tu->timeri)\n \t\t\tsnd_timer_close(tu->timeri);\n+\t\tmutex_unlock(&tu->ioctl_lock);\n \t\tkfree(tu->queue);\n \t\tkfree(tu->tqueue);\n \t\tkfree(tu);",
        "function_modified_lines": {
            "added": [
                "\t\tmutex_lock(&tu->ioctl_lock);",
                "\t\tmutex_unlock(&tu->ioctl_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 uses an incorrect type of mutex, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call."
    },
    {
        "cve_id": "CVE-2016-2547",
        "code_before_change": "static void snd_timer_check_master(struct snd_timer_instance *master)\n{\n\tstruct snd_timer_instance *slave, *tmp;\n\n\t/* check all pending slaves */\n\tlist_for_each_entry_safe(slave, tmp, &snd_timer_slave_list, open_list) {\n\t\tif (slave->slave_class == master->slave_class &&\n\t\t    slave->slave_id == master->slave_id) {\n\t\t\tlist_move_tail(&slave->open_list, &master->slave_list_head);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\tslave->master = master;\n\t\t\tslave->timer = master->timer;\n\t\t\tif (slave->flags & SNDRV_TIMER_IFLG_RUNNING)\n\t\t\t\tlist_add_tail(&slave->active_list,\n\t\t\t\t\t      &master->slave_active_head);\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t}\n\t}\n}",
        "code_after_change": "static void snd_timer_check_master(struct snd_timer_instance *master)\n{\n\tstruct snd_timer_instance *slave, *tmp;\n\n\t/* check all pending slaves */\n\tlist_for_each_entry_safe(slave, tmp, &snd_timer_slave_list, open_list) {\n\t\tif (slave->slave_class == master->slave_class &&\n\t\t    slave->slave_id == master->slave_id) {\n\t\t\tlist_move_tail(&slave->open_list, &master->slave_list_head);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\tspin_lock(&master->timer->lock);\n\t\t\tslave->master = master;\n\t\t\tslave->timer = master->timer;\n\t\t\tif (slave->flags & SNDRV_TIMER_IFLG_RUNNING)\n\t\t\t\tlist_add_tail(&slave->active_list,\n\t\t\t\t\t      &master->slave_active_head);\n\t\t\tspin_unlock(&master->timer->lock);\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t}\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,11 +8,13 @@\n \t\t    slave->slave_id == master->slave_id) {\n \t\t\tlist_move_tail(&slave->open_list, &master->slave_list_head);\n \t\t\tspin_lock_irq(&slave_active_lock);\n+\t\t\tspin_lock(&master->timer->lock);\n \t\t\tslave->master = master;\n \t\t\tslave->timer = master->timer;\n \t\t\tif (slave->flags & SNDRV_TIMER_IFLG_RUNNING)\n \t\t\t\tlist_add_tail(&slave->active_list,\n \t\t\t\t\t      &master->slave_active_head);\n+\t\t\tspin_unlock(&master->timer->lock);\n \t\t\tspin_unlock_irq(&slave_active_lock);\n \t\t}\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tspin_lock(&master->timer->lock);",
                "\t\t\tspin_unlock(&master->timer->lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 employs a locking approach that does not consider slave timer instances, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call."
    },
    {
        "cve_id": "CVE-2016-2547",
        "code_before_change": "static int snd_timer_start_slave(struct snd_timer_instance *timeri)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&slave_active_lock, flags);\n\ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n\tif (timeri->master)\n\t\tlist_add_tail(&timeri->active_list,\n\t\t\t      &timeri->master->slave_active_head);\n\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\treturn 1; /* delayed start */\n}",
        "code_after_change": "static int snd_timer_start_slave(struct snd_timer_instance *timeri)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&slave_active_lock, flags);\n\ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n\tif (timeri->master && timeri->timer) {\n\t\tspin_lock(&timeri->timer->lock);\n\t\tlist_add_tail(&timeri->active_list,\n\t\t\t      &timeri->master->slave_active_head);\n\t\tspin_unlock(&timeri->timer->lock);\n\t}\n\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\treturn 1; /* delayed start */\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,9 +4,12 @@\n \n \tspin_lock_irqsave(&slave_active_lock, flags);\n \ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n-\tif (timeri->master)\n+\tif (timeri->master && timeri->timer) {\n+\t\tspin_lock(&timeri->timer->lock);\n \t\tlist_add_tail(&timeri->active_list,\n \t\t\t      &timeri->master->slave_active_head);\n+\t\tspin_unlock(&timeri->timer->lock);\n+\t}\n \tspin_unlock_irqrestore(&slave_active_lock, flags);\n \treturn 1; /* delayed start */\n }",
        "function_modified_lines": {
            "added": [
                "\tif (timeri->master && timeri->timer) {",
                "\t\tspin_lock(&timeri->timer->lock);",
                "\t\tspin_unlock(&timeri->timer->lock);",
                "\t}"
            ],
            "deleted": [
                "\tif (timeri->master)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 employs a locking approach that does not consider slave timer instances, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call."
    },
    {
        "cve_id": "CVE-2016-2547",
        "code_before_change": "static int _snd_timer_stop(struct snd_timer_instance * timeri,\n\t\t\t   int keep_flag, int event)\n{\n\tstruct snd_timer *timer;\n\tunsigned long flags;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\tif (!keep_flag) {\n\t\t\tspin_lock_irqsave(&slave_active_lock, flags);\n\t\t\ttimeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\t\t}\n\t\tgoto __end;\n\t}\n\ttimer = timeri->timer;\n\tif (!timer)\n\t\treturn -EINVAL;\n\tspin_lock_irqsave(&timer->lock, flags);\n\tlist_del_init(&timeri->ack_list);\n\tlist_del_init(&timeri->active_list);\n\tif ((timeri->flags & SNDRV_TIMER_IFLG_RUNNING) &&\n\t    !(--timer->running)) {\n\t\ttimer->hw.stop(timer);\n\t\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED) {\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_RESCHED;\n\t\t\tsnd_timer_reschedule(timer, 0);\n\t\t\tif (timer->flags & SNDRV_TIMER_FLG_CHANGE) {\n\t\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\t\ttimer->hw.start(timer);\n\t\t\t}\n\t\t}\n\t}\n\tif (!keep_flag)\n\t\ttimeri->flags &=\n\t\t\t~(SNDRV_TIMER_IFLG_RUNNING | SNDRV_TIMER_IFLG_START);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n      __end:\n\tif (event != SNDRV_TIMER_EVENT_RESOLUTION)\n\t\tsnd_timer_notify1(timeri, event);\n\treturn 0;\n}",
        "code_after_change": "static int _snd_timer_stop(struct snd_timer_instance * timeri,\n\t\t\t   int keep_flag, int event)\n{\n\tstruct snd_timer *timer;\n\tunsigned long flags;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\tif (!keep_flag) {\n\t\t\tspin_lock_irqsave(&slave_active_lock, flags);\n\t\t\ttimeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\tlist_del_init(&timeri->ack_list);\n\t\t\tlist_del_init(&timeri->active_list);\n\t\t\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\t\t}\n\t\tgoto __end;\n\t}\n\ttimer = timeri->timer;\n\tif (!timer)\n\t\treturn -EINVAL;\n\tspin_lock_irqsave(&timer->lock, flags);\n\tlist_del_init(&timeri->ack_list);\n\tlist_del_init(&timeri->active_list);\n\tif ((timeri->flags & SNDRV_TIMER_IFLG_RUNNING) &&\n\t    !(--timer->running)) {\n\t\ttimer->hw.stop(timer);\n\t\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED) {\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_RESCHED;\n\t\t\tsnd_timer_reschedule(timer, 0);\n\t\t\tif (timer->flags & SNDRV_TIMER_FLG_CHANGE) {\n\t\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\t\ttimer->hw.start(timer);\n\t\t\t}\n\t\t}\n\t}\n\tif (!keep_flag)\n\t\ttimeri->flags &=\n\t\t\t~(SNDRV_TIMER_IFLG_RUNNING | SNDRV_TIMER_IFLG_START);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n      __end:\n\tif (event != SNDRV_TIMER_EVENT_RESOLUTION)\n\t\tsnd_timer_notify1(timeri, event);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,8 @@\n \t\tif (!keep_flag) {\n \t\t\tspin_lock_irqsave(&slave_active_lock, flags);\n \t\t\ttimeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n+\t\t\tlist_del_init(&timeri->ack_list);\n+\t\t\tlist_del_init(&timeri->active_list);\n \t\t\tspin_unlock_irqrestore(&slave_active_lock, flags);\n \t\t}\n \t\tgoto __end;",
        "function_modified_lines": {
            "added": [
                "\t\t\tlist_del_init(&timeri->ack_list);",
                "\t\t\tlist_del_init(&timeri->active_list);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 employs a locking approach that does not consider slave timer instances, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call."
    },
    {
        "cve_id": "CVE-2016-2547",
        "code_before_change": "int snd_timer_close(struct snd_timer_instance *timeri)\n{\n\tstruct snd_timer *timer = NULL;\n\tstruct snd_timer_instance *slave, *tmp;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\t/* force to stop the timer */\n\tsnd_timer_stop(timeri);\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t}\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tmutex_unlock(&register_mutex);\n\t} else {\n\t\ttimer = timeri->timer;\n\t\tif (snd_BUG_ON(!timer))\n\t\t\tgoto out;\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&timer->lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&timer->lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&timer->lock);\n\t\t}\n\t\tspin_unlock_irq(&timer->lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tif (timer && list_empty(&timer->open_list_head) &&\n\t\t    timer->hw.close)\n\t\t\ttimer->hw.close(timer);\n\t\t/* remove slave links */\n\t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n\t\t\t\t\t open_list) {\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);\n\t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n\t\t\tslave->master = NULL;\n\t\t\tslave->timer = NULL;\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t}\n\t\tmutex_unlock(&register_mutex);\n\t}\n out:\n\tif (timeri->private_free)\n\t\ttimeri->private_free(timeri);\n\tkfree(timeri->owner);\n\tkfree(timeri);\n\tif (timer)\n\t\tmodule_put(timer->module);\n\treturn 0;\n}",
        "code_after_change": "int snd_timer_close(struct snd_timer_instance *timeri)\n{\n\tstruct snd_timer *timer = NULL;\n\tstruct snd_timer_instance *slave, *tmp;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\t/* force to stop the timer */\n\tsnd_timer_stop(timeri);\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t}\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tmutex_unlock(&register_mutex);\n\t} else {\n\t\ttimer = timeri->timer;\n\t\tif (snd_BUG_ON(!timer))\n\t\t\tgoto out;\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&timer->lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&timer->lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&timer->lock);\n\t\t}\n\t\tspin_unlock_irq(&timer->lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tif (timer && list_empty(&timer->open_list_head) &&\n\t\t    timer->hw.close)\n\t\t\ttimer->hw.close(timer);\n\t\t/* remove slave links */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\tspin_lock(&timer->lock);\n\t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n\t\t\t\t\t open_list) {\n\t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n\t\t\tslave->master = NULL;\n\t\t\tslave->timer = NULL;\n\t\t\tlist_del_init(&slave->ack_list);\n\t\t\tlist_del_init(&slave->active_list);\n\t\t}\n\t\tspin_unlock(&timer->lock);\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_unlock(&register_mutex);\n\t}\n out:\n\tif (timeri->private_free)\n\t\ttimeri->private_free(timeri);\n\tkfree(timeri->owner);\n\tkfree(timeri);\n\tif (timer)\n\t\tmodule_put(timer->module);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,15 +39,18 @@\n \t\t    timer->hw.close)\n \t\t\ttimer->hw.close(timer);\n \t\t/* remove slave links */\n+\t\tspin_lock_irq(&slave_active_lock);\n+\t\tspin_lock(&timer->lock);\n \t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n \t\t\t\t\t open_list) {\n-\t\t\tspin_lock_irq(&slave_active_lock);\n-\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);\n \t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n \t\t\tslave->master = NULL;\n \t\t\tslave->timer = NULL;\n-\t\t\tspin_unlock_irq(&slave_active_lock);\n+\t\t\tlist_del_init(&slave->ack_list);\n+\t\t\tlist_del_init(&slave->active_list);\n \t\t}\n+\t\tspin_unlock(&timer->lock);\n+\t\tspin_unlock_irq(&slave_active_lock);\n \t\tmutex_unlock(&register_mutex);\n \t}\n  out:",
        "function_modified_lines": {
            "added": [
                "\t\tspin_lock_irq(&slave_active_lock);",
                "\t\tspin_lock(&timer->lock);",
                "\t\t\tlist_del_init(&slave->ack_list);",
                "\t\t\tlist_del_init(&slave->active_list);",
                "\t\tspin_unlock(&timer->lock);",
                "\t\tspin_unlock_irq(&slave_active_lock);"
            ],
            "deleted": [
                "\t\t\tspin_lock_irq(&slave_active_lock);",
                "\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);",
                "\t\t\tspin_unlock_irq(&slave_active_lock);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 employs a locking approach that does not consider slave timer instances, which allows local users to cause a denial of service (race condition, use-after-free, and system crash) via a crafted ioctl call."
    },
    {
        "cve_id": "CVE-2016-5195",
        "code_before_change": "static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, int *nonblocking)\n{\n\tunsigned int fault_flags = 0;\n\tint ret;\n\n\t/* mlock all present pages, but do not fault in new pages */\n\tif ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)\n\t\treturn -ENOENT;\n\t/* For mm_populate(), just skip the stack guard page. */\n\tif ((*flags & FOLL_POPULATE) &&\n\t\t\t(stack_guard_page_start(vma, address) ||\n\t\t\t stack_guard_page_end(vma, address + PAGE_SIZE)))\n\t\treturn -ENOENT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (nonblocking)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\tVM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tif (ret & VM_FAULT_ERROR) {\n\t\tif (ret & VM_FAULT_OOM)\n\t\t\treturn -ENOMEM;\n\t\tif (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n\t\t\treturn *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;\n\t\tif (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))\n\t\t\treturn -EFAULT;\n\t\tBUG();\n\t}\n\n\tif (tsk) {\n\t\tif (ret & VM_FAULT_MAJOR)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (nonblocking)\n\t\t\t*nonblocking = 0;\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n\t * can thus safely do subsequent page lookups as if they were reads.\n\t * But only do so when looping for pte_write is futile: in some cases\n\t * userspace may also be wanting to write to the gotten user page,\n\t * which a read fault here might prevent (a readonly page might get\n\t * reCOWed by userspace write).\n\t */\n\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n\t\t*flags &= ~FOLL_WRITE;\n\treturn 0;\n}",
        "code_after_change": "static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, int *nonblocking)\n{\n\tunsigned int fault_flags = 0;\n\tint ret;\n\n\t/* mlock all present pages, but do not fault in new pages */\n\tif ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)\n\t\treturn -ENOENT;\n\t/* For mm_populate(), just skip the stack guard page. */\n\tif ((*flags & FOLL_POPULATE) &&\n\t\t\t(stack_guard_page_start(vma, address) ||\n\t\t\t stack_guard_page_end(vma, address + PAGE_SIZE)))\n\t\treturn -ENOENT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (nonblocking)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\tVM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tif (ret & VM_FAULT_ERROR) {\n\t\tif (ret & VM_FAULT_OOM)\n\t\t\treturn -ENOMEM;\n\t\tif (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n\t\t\treturn *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;\n\t\tif (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))\n\t\t\treturn -EFAULT;\n\t\tBUG();\n\t}\n\n\tif (tsk) {\n\t\tif (ret & VM_FAULT_MAJOR)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (nonblocking)\n\t\t\t*nonblocking = 0;\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n\t * can thus safely do subsequent page lookups as if they were reads.\n\t * But only do so when looping for pte_write is futile: in some cases\n\t * userspace may also be wanting to write to the gotten user page,\n\t * which a read fault here might prevent (a readonly page might get\n\t * reCOWed by userspace write).\n\t */\n\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n\t        *flags |= FOLL_COW;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -59,6 +59,6 @@\n \t * reCOWed by userspace write).\n \t */\n \tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n-\t\t*flags &= ~FOLL_WRITE;\n+\t        *flags |= FOLL_COW;\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t        *flags |= FOLL_COW;"
            ],
            "deleted": [
                "\t\t*flags &= ~FOLL_WRITE;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in mm/gup.c in the Linux kernel 2.x through 4.x before 4.8.3 allows local users to gain privileges by leveraging incorrect handling of a copy-on-write (COW) feature to write to a read-only memory mapping, as exploited in the wild in October 2016, aka \"Dirty COW.\""
    },
    {
        "cve_id": "CVE-2016-5195",
        "code_before_change": "static struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap = NULL;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\tif ((flags & FOLL_WRITE) && !pte_write(pte)) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\treturn NULL;\n\t}\n\n\tpage = vm_normal_page(vma, address, pte);\n\tif (!page && pte_devmap(pte) && (flags & FOLL_GET)) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET case since\n\t\t * they are only valid while holding the pgmap reference.\n\t\t */\n\t\tpgmap = get_dev_pagemap(pte_pfn(pte), NULL);\n\t\tif (pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tint ret;\n\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (flags & FOLL_SPLIT && PageTransCompound(page)) {\n\t\tint ret;\n\t\tget_page(page);\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tlock_page(page);\n\t\tret = split_huge_page(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\tgoto retry;\n\t}\n\n\tif (flags & FOLL_GET) {\n\t\tget_page(page);\n\n\t\t/* drop the pgmap reference now that we hold the page */\n\t\tif (pgmap) {\n\t\t\tput_dev_pagemap(pgmap);\n\t\t\tpgmap = NULL;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\n\tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n\t\t/* Do not mlock pte-mapped THP */\n\t\tif (PageTransCompound(page))\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * The preliminary mapping check is mainly to avoid the\n\t\t * pointless overhead of lock_page on the ZERO_PAGE\n\t\t * which might bounce very badly if there is contention.\n\t\t *\n\t\t * If the page is already locked, we don't need to\n\t\t * handle it now - vmscan will handle it later if and\n\t\t * when it attempts to reclaim the page.\n\t\t */\n\t\tif (page->mapping && trylock_page(page)) {\n\t\t\tlru_add_drain();  /* push cached pages to LRU */\n\t\t\t/*\n\t\t\t * Because we lock page here, and migration is\n\t\t\t * blocked by the pte's page reference, and we\n\t\t\t * know the page is still mapped, we don't even\n\t\t\t * need to check for file-cache page truncation.\n\t\t\t */\n\t\t\tmlock_vma_page(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}",
        "code_after_change": "static struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap = NULL;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\tif ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\treturn NULL;\n\t}\n\n\tpage = vm_normal_page(vma, address, pte);\n\tif (!page && pte_devmap(pte) && (flags & FOLL_GET)) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET case since\n\t\t * they are only valid while holding the pgmap reference.\n\t\t */\n\t\tpgmap = get_dev_pagemap(pte_pfn(pte), NULL);\n\t\tif (pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tint ret;\n\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (flags & FOLL_SPLIT && PageTransCompound(page)) {\n\t\tint ret;\n\t\tget_page(page);\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tlock_page(page);\n\t\tret = split_huge_page(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\tgoto retry;\n\t}\n\n\tif (flags & FOLL_GET) {\n\t\tget_page(page);\n\n\t\t/* drop the pgmap reference now that we hold the page */\n\t\tif (pgmap) {\n\t\t\tput_dev_pagemap(pgmap);\n\t\t\tpgmap = NULL;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\n\tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n\t\t/* Do not mlock pte-mapped THP */\n\t\tif (PageTransCompound(page))\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * The preliminary mapping check is mainly to avoid the\n\t\t * pointless overhead of lock_page on the ZERO_PAGE\n\t\t * which might bounce very badly if there is contention.\n\t\t *\n\t\t * If the page is already locked, we don't need to\n\t\t * handle it now - vmscan will handle it later if and\n\t\t * when it attempts to reclaim the page.\n\t\t */\n\t\tif (page->mapping && trylock_page(page)) {\n\t\t\tlru_add_drain();  /* push cached pages to LRU */\n\t\t\t/*\n\t\t\t * Because we lock page here, and migration is\n\t\t\t * blocked by the pte's page reference, and we\n\t\t\t * know the page is still mapped, we don't even\n\t\t\t * need to check for file-cache page truncation.\n\t\t\t */\n\t\t\tmlock_vma_page(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -33,7 +33,7 @@\n \t}\n \tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n \t\tgoto no_page;\n-\tif ((flags & FOLL_WRITE) && !pte_write(pte)) {\n+\tif ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {\n \t\tpte_unmap_unlock(ptep, ptl);\n \t\treturn NULL;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {"
            ],
            "deleted": [
                "\tif ((flags & FOLL_WRITE) && !pte_write(pte)) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in mm/gup.c in the Linux kernel 2.x through 4.x before 4.8.3 allows local users to gain privileges by leveraging incorrect handling of a copy-on-write (COW) feature to write to a read-only memory mapping, as exploited in the wild in October 2016, aka \"Dirty COW.\""
    },
    {
        "cve_id": "CVE-2016-6130",
        "code_before_change": "static int sclp_ctl_ioctl_sccb(void __user *user_area)\n{\n\tstruct sclp_ctl_sccb ctl_sccb;\n\tstruct sccb_header *sccb;\n\tint rc;\n\n\tif (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))\n\t\treturn -EFAULT;\n\tif (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))\n\t\treturn -EOPNOTSUPP;\n\tsccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);\n\tif (!sccb)\n\t\treturn -ENOMEM;\n\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {\n\t\trc = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (sccb->length > PAGE_SIZE || sccb->length < 8)\n\t\treturn -EINVAL;\n\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {\n\t\trc = -EFAULT;\n\t\tgoto out_free;\n\t}\n\trc = sclp_sync_request(ctl_sccb.cmdw, sccb);\n\tif (rc)\n\t\tgoto out_free;\n\tif (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))\n\t\trc = -EFAULT;\nout_free:\n\tfree_page((unsigned long) sccb);\n\treturn rc;\n}",
        "code_after_change": "static int sclp_ctl_ioctl_sccb(void __user *user_area)\n{\n\tstruct sclp_ctl_sccb ctl_sccb;\n\tstruct sccb_header *sccb;\n\tunsigned long copied;\n\tint rc;\n\n\tif (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))\n\t\treturn -EFAULT;\n\tif (!sclp_ctl_cmdw_supported(ctl_sccb.cmdw))\n\t\treturn -EOPNOTSUPP;\n\tsccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);\n\tif (!sccb)\n\t\treturn -ENOMEM;\n\tcopied = PAGE_SIZE -\n\t\tcopy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);\n\tif (offsetof(struct sccb_header, length) +\n\t    sizeof(sccb->length) > copied || sccb->length > copied) {\n\t\trc = -EFAULT;\n\t\tgoto out_free;\n\t}\n\tif (sccb->length < 8) {\n\t\trc = -EINVAL;\n\t\tgoto out_free;\n\t}\n\trc = sclp_sync_request(ctl_sccb.cmdw, sccb);\n\tif (rc)\n\t\tgoto out_free;\n\tif (copy_to_user(u64_to_uptr(ctl_sccb.sccb), sccb, sccb->length))\n\t\trc = -EFAULT;\nout_free:\n\tfree_page((unsigned long) sccb);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n {\n \tstruct sclp_ctl_sccb ctl_sccb;\n \tstruct sccb_header *sccb;\n+\tunsigned long copied;\n \tint rc;\n \n \tif (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))\n@@ -11,14 +12,15 @@\n \tsccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);\n \tif (!sccb)\n \t\treturn -ENOMEM;\n-\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {\n+\tcopied = PAGE_SIZE -\n+\t\tcopy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);\n+\tif (offsetof(struct sccb_header, length) +\n+\t    sizeof(sccb->length) > copied || sccb->length > copied) {\n \t\trc = -EFAULT;\n \t\tgoto out_free;\n \t}\n-\tif (sccb->length > PAGE_SIZE || sccb->length < 8)\n-\t\treturn -EINVAL;\n-\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {\n-\t\trc = -EFAULT;\n+\tif (sccb->length < 8) {\n+\t\trc = -EINVAL;\n \t\tgoto out_free;\n \t}\n \trc = sclp_sync_request(ctl_sccb.cmdw, sccb);",
        "function_modified_lines": {
            "added": [
                "\tunsigned long copied;",
                "\tcopied = PAGE_SIZE -",
                "\t\tcopy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);",
                "\tif (offsetof(struct sccb_header, length) +",
                "\t    sizeof(sccb->length) > copied || sccb->length > copied) {",
                "\tif (sccb->length < 8) {",
                "\t\trc = -EINVAL;"
            ],
            "deleted": [
                "\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {",
                "\tif (sccb->length > PAGE_SIZE || sccb->length < 8)",
                "\t\treturn -EINVAL;",
                "\tif (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {",
                "\t\trc = -EFAULT;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the sclp_ctl_ioctl_sccb function in drivers/s390/char/sclp_ctl.c in the Linux kernel before 4.6 allows local users to obtain sensitive information from kernel memory by changing a certain length value, aka a \"double fetch\" vulnerability."
    },
    {
        "cve_id": "CVE-2016-6136",
        "code_before_change": "static void audit_log_execve_info(struct audit_context *context,\n\t\t\t\t  struct audit_buffer **ab)\n{\n\tint i, len;\n\tsize_t len_sent = 0;\n\tconst char __user *p;\n\tchar *buf;\n\n\tp = (const char __user *)current->mm->arg_start;\n\n\taudit_log_format(*ab, \"argc=%d\", context->execve.argc);\n\n\t/*\n\t * we need some kernel buffer to hold the userspace args.  Just\n\t * allocate one big one rather than allocating one of the right size\n\t * for every single argument inside audit_log_single_execve_arg()\n\t * should be <8k allocation so should be pretty safe.\n\t */\n\tbuf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n\tif (!buf) {\n\t\taudit_panic(\"out of memory for argv string\");\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < context->execve.argc; i++) {\n\t\tlen = audit_log_single_execve_arg(context, ab, i,\n\t\t\t\t\t\t  &len_sent, p, buf);\n\t\tif (len <= 0)\n\t\t\tbreak;\n\t\tp += len;\n\t}\n\tkfree(buf);\n}",
        "code_after_change": "static void audit_log_execve_info(struct audit_context *context,\n\t\t\t\t  struct audit_buffer **ab)\n{\n\tlong len_max;\n\tlong len_rem;\n\tlong len_full;\n\tlong len_buf;\n\tlong len_abuf;\n\tlong len_tmp;\n\tbool require_data;\n\tbool encode;\n\tunsigned int iter;\n\tunsigned int arg;\n\tchar *buf_head;\n\tchar *buf;\n\tconst char __user *p = (const char __user *)current->mm->arg_start;\n\n\t/* NOTE: this buffer needs to be large enough to hold all the non-arg\n\t *       data we put in the audit record for this argument (see the\n\t *       code below) ... at this point in time 96 is plenty */\n\tchar abuf[96];\n\n\t/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the\n\t *       current value of 7500 is not as important as the fact that it\n\t *       is less than 8k, a setting of 7500 gives us plenty of wiggle\n\t *       room if we go over a little bit in the logging below */\n\tWARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);\n\tlen_max = MAX_EXECVE_AUDIT_LEN;\n\n\t/* scratch buffer to hold the userspace args */\n\tbuf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n\tif (!buf_head) {\n\t\taudit_panic(\"out of memory for argv string\");\n\t\treturn;\n\t}\n\tbuf = buf_head;\n\n\taudit_log_format(*ab, \"argc=%d\", context->execve.argc);\n\n\tlen_rem = len_max;\n\tlen_buf = 0;\n\tlen_full = 0;\n\trequire_data = true;\n\tencode = false;\n\titer = 0;\n\targ = 0;\n\tdo {\n\t\t/* NOTE: we don't ever want to trust this value for anything\n\t\t *       serious, but the audit record format insists we\n\t\t *       provide an argument length for really long arguments,\n\t\t *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but\n\t\t *       to use strncpy_from_user() to obtain this value for\n\t\t *       recording in the log, although we don't use it\n\t\t *       anywhere here to avoid a double-fetch problem */\n\t\tif (len_full == 0)\n\t\t\tlen_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n\n\t\t/* read more data from userspace */\n\t\tif (require_data) {\n\t\t\t/* can we make more room in the buffer? */\n\t\t\tif (buf != buf_head) {\n\t\t\t\tmemmove(buf_head, buf, len_buf);\n\t\t\t\tbuf = buf_head;\n\t\t\t}\n\n\t\t\t/* fetch as much as we can of the argument */\n\t\t\tlen_tmp = strncpy_from_user(&buf_head[len_buf], p,\n\t\t\t\t\t\t    len_max - len_buf);\n\t\t\tif (len_tmp == -EFAULT) {\n\t\t\t\t/* unable to copy from userspace */\n\t\t\t\tsend_sig(SIGKILL, current, 0);\n\t\t\t\tgoto out;\n\t\t\t} else if (len_tmp == (len_max - len_buf)) {\n\t\t\t\t/* buffer is not large enough */\n\t\t\t\trequire_data = true;\n\t\t\t\t/* NOTE: if we are going to span multiple\n\t\t\t\t *       buffers force the encoding so we stand\n\t\t\t\t *       a chance at a sane len_full value and\n\t\t\t\t *       consistent record encoding */\n\t\t\t\tencode = true;\n\t\t\t\tlen_full = len_full * 2;\n\t\t\t\tp += len_tmp;\n\t\t\t} else {\n\t\t\t\trequire_data = false;\n\t\t\t\tif (!encode)\n\t\t\t\t\tencode = audit_string_contains_control(\n\t\t\t\t\t\t\t\tbuf, len_tmp);\n\t\t\t\t/* try to use a trusted value for len_full */\n\t\t\t\tif (len_full < len_max)\n\t\t\t\t\tlen_full = (encode ?\n\t\t\t\t\t\t    len_tmp * 2 : len_tmp);\n\t\t\t\tp += len_tmp + 1;\n\t\t\t}\n\t\t\tlen_buf += len_tmp;\n\t\t\tbuf_head[len_buf] = '\\0';\n\n\t\t\t/* length of the buffer in the audit record? */\n\t\t\tlen_abuf = (encode ? len_buf * 2 : len_buf + 2);\n\t\t}\n\n\t\t/* write as much as we can to the audit log */\n\t\tif (len_buf > 0) {\n\t\t\t/* NOTE: some magic numbers here - basically if we\n\t\t\t *       can't fit a reasonable amount of data into the\n\t\t\t *       existing audit buffer, flush it and start with\n\t\t\t *       a new buffer */\n\t\t\tif ((sizeof(abuf) + 8) > len_rem) {\n\t\t\t\tlen_rem = len_max;\n\t\t\t\taudit_log_end(*ab);\n\t\t\t\t*ab = audit_log_start(context,\n\t\t\t\t\t\t      GFP_KERNEL, AUDIT_EXECVE);\n\t\t\t\tif (!*ab)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t/* create the non-arg portion of the arg record */\n\t\t\tlen_tmp = 0;\n\t\t\tif (require_data || (iter > 0) ||\n\t\t\t    ((len_abuf + sizeof(abuf)) > len_rem)) {\n\t\t\t\tif (iter == 0) {\n\t\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n\t\t\t\t\t\t\tsizeof(abuf) - len_tmp,\n\t\t\t\t\t\t\t\" a%d_len=%lu\",\n\t\t\t\t\t\t\targ, len_full);\n\t\t\t\t}\n\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n\t\t\t\t\t\t    \" a%d[%d]=\", arg, iter++);\n\t\t\t} else\n\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n\t\t\t\t\t\t    \" a%d=\", arg);\n\t\t\tWARN_ON(len_tmp >= sizeof(abuf));\n\t\t\tabuf[sizeof(abuf) - 1] = '\\0';\n\n\t\t\t/* log the arg in the audit record */\n\t\t\taudit_log_format(*ab, \"%s\", abuf);\n\t\t\tlen_rem -= len_tmp;\n\t\t\tlen_tmp = len_buf;\n\t\t\tif (encode) {\n\t\t\t\tif (len_abuf > len_rem)\n\t\t\t\t\tlen_tmp = len_rem / 2; /* encoding */\n\t\t\t\taudit_log_n_hex(*ab, buf, len_tmp);\n\t\t\t\tlen_rem -= len_tmp * 2;\n\t\t\t\tlen_abuf -= len_tmp * 2;\n\t\t\t} else {\n\t\t\t\tif (len_abuf > len_rem)\n\t\t\t\t\tlen_tmp = len_rem - 2; /* quotes */\n\t\t\t\taudit_log_n_string(*ab, buf, len_tmp);\n\t\t\t\tlen_rem -= len_tmp + 2;\n\t\t\t\t/* don't subtract the \"2\" because we still need\n\t\t\t\t * to add quotes to the remaining string */\n\t\t\t\tlen_abuf -= len_tmp;\n\t\t\t}\n\t\t\tlen_buf -= len_tmp;\n\t\t\tbuf += len_tmp;\n\t\t}\n\n\t\t/* ready to move to the next argument? */\n\t\tif ((len_buf == 0) && !require_data) {\n\t\t\targ++;\n\t\t\titer = 0;\n\t\t\tlen_full = 0;\n\t\t\trequire_data = true;\n\t\t\tencode = false;\n\t\t}\n\t} while (arg < context->execve.argc);\n\n\t/* NOTE: the caller handles the final audit_log_end() call */\n\nout:\n\tkfree(buf_head);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,33 +1,173 @@\n static void audit_log_execve_info(struct audit_context *context,\n \t\t\t\t  struct audit_buffer **ab)\n {\n-\tint i, len;\n-\tsize_t len_sent = 0;\n-\tconst char __user *p;\n+\tlong len_max;\n+\tlong len_rem;\n+\tlong len_full;\n+\tlong len_buf;\n+\tlong len_abuf;\n+\tlong len_tmp;\n+\tbool require_data;\n+\tbool encode;\n+\tunsigned int iter;\n+\tunsigned int arg;\n+\tchar *buf_head;\n \tchar *buf;\n+\tconst char __user *p = (const char __user *)current->mm->arg_start;\n \n-\tp = (const char __user *)current->mm->arg_start;\n+\t/* NOTE: this buffer needs to be large enough to hold all the non-arg\n+\t *       data we put in the audit record for this argument (see the\n+\t *       code below) ... at this point in time 96 is plenty */\n+\tchar abuf[96];\n+\n+\t/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the\n+\t *       current value of 7500 is not as important as the fact that it\n+\t *       is less than 8k, a setting of 7500 gives us plenty of wiggle\n+\t *       room if we go over a little bit in the logging below */\n+\tWARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);\n+\tlen_max = MAX_EXECVE_AUDIT_LEN;\n+\n+\t/* scratch buffer to hold the userspace args */\n+\tbuf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n+\tif (!buf_head) {\n+\t\taudit_panic(\"out of memory for argv string\");\n+\t\treturn;\n+\t}\n+\tbuf = buf_head;\n \n \taudit_log_format(*ab, \"argc=%d\", context->execve.argc);\n \n-\t/*\n-\t * we need some kernel buffer to hold the userspace args.  Just\n-\t * allocate one big one rather than allocating one of the right size\n-\t * for every single argument inside audit_log_single_execve_arg()\n-\t * should be <8k allocation so should be pretty safe.\n-\t */\n-\tbuf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n-\tif (!buf) {\n-\t\taudit_panic(\"out of memory for argv string\");\n-\t\treturn;\n-\t}\n+\tlen_rem = len_max;\n+\tlen_buf = 0;\n+\tlen_full = 0;\n+\trequire_data = true;\n+\tencode = false;\n+\titer = 0;\n+\targ = 0;\n+\tdo {\n+\t\t/* NOTE: we don't ever want to trust this value for anything\n+\t\t *       serious, but the audit record format insists we\n+\t\t *       provide an argument length for really long arguments,\n+\t\t *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but\n+\t\t *       to use strncpy_from_user() to obtain this value for\n+\t\t *       recording in the log, although we don't use it\n+\t\t *       anywhere here to avoid a double-fetch problem */\n+\t\tif (len_full == 0)\n+\t\t\tlen_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n \n-\tfor (i = 0; i < context->execve.argc; i++) {\n-\t\tlen = audit_log_single_execve_arg(context, ab, i,\n-\t\t\t\t\t\t  &len_sent, p, buf);\n-\t\tif (len <= 0)\n-\t\t\tbreak;\n-\t\tp += len;\n-\t}\n-\tkfree(buf);\n+\t\t/* read more data from userspace */\n+\t\tif (require_data) {\n+\t\t\t/* can we make more room in the buffer? */\n+\t\t\tif (buf != buf_head) {\n+\t\t\t\tmemmove(buf_head, buf, len_buf);\n+\t\t\t\tbuf = buf_head;\n+\t\t\t}\n+\n+\t\t\t/* fetch as much as we can of the argument */\n+\t\t\tlen_tmp = strncpy_from_user(&buf_head[len_buf], p,\n+\t\t\t\t\t\t    len_max - len_buf);\n+\t\t\tif (len_tmp == -EFAULT) {\n+\t\t\t\t/* unable to copy from userspace */\n+\t\t\t\tsend_sig(SIGKILL, current, 0);\n+\t\t\t\tgoto out;\n+\t\t\t} else if (len_tmp == (len_max - len_buf)) {\n+\t\t\t\t/* buffer is not large enough */\n+\t\t\t\trequire_data = true;\n+\t\t\t\t/* NOTE: if we are going to span multiple\n+\t\t\t\t *       buffers force the encoding so we stand\n+\t\t\t\t *       a chance at a sane len_full value and\n+\t\t\t\t *       consistent record encoding */\n+\t\t\t\tencode = true;\n+\t\t\t\tlen_full = len_full * 2;\n+\t\t\t\tp += len_tmp;\n+\t\t\t} else {\n+\t\t\t\trequire_data = false;\n+\t\t\t\tif (!encode)\n+\t\t\t\t\tencode = audit_string_contains_control(\n+\t\t\t\t\t\t\t\tbuf, len_tmp);\n+\t\t\t\t/* try to use a trusted value for len_full */\n+\t\t\t\tif (len_full < len_max)\n+\t\t\t\t\tlen_full = (encode ?\n+\t\t\t\t\t\t    len_tmp * 2 : len_tmp);\n+\t\t\t\tp += len_tmp + 1;\n+\t\t\t}\n+\t\t\tlen_buf += len_tmp;\n+\t\t\tbuf_head[len_buf] = '\\0';\n+\n+\t\t\t/* length of the buffer in the audit record? */\n+\t\t\tlen_abuf = (encode ? len_buf * 2 : len_buf + 2);\n+\t\t}\n+\n+\t\t/* write as much as we can to the audit log */\n+\t\tif (len_buf > 0) {\n+\t\t\t/* NOTE: some magic numbers here - basically if we\n+\t\t\t *       can't fit a reasonable amount of data into the\n+\t\t\t *       existing audit buffer, flush it and start with\n+\t\t\t *       a new buffer */\n+\t\t\tif ((sizeof(abuf) + 8) > len_rem) {\n+\t\t\t\tlen_rem = len_max;\n+\t\t\t\taudit_log_end(*ab);\n+\t\t\t\t*ab = audit_log_start(context,\n+\t\t\t\t\t\t      GFP_KERNEL, AUDIT_EXECVE);\n+\t\t\t\tif (!*ab)\n+\t\t\t\t\tgoto out;\n+\t\t\t}\n+\n+\t\t\t/* create the non-arg portion of the arg record */\n+\t\t\tlen_tmp = 0;\n+\t\t\tif (require_data || (iter > 0) ||\n+\t\t\t    ((len_abuf + sizeof(abuf)) > len_rem)) {\n+\t\t\t\tif (iter == 0) {\n+\t\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t\tsizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t\t\" a%d_len=%lu\",\n+\t\t\t\t\t\t\targ, len_full);\n+\t\t\t\t}\n+\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t    \" a%d[%d]=\", arg, iter++);\n+\t\t\t} else\n+\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t    \" a%d=\", arg);\n+\t\t\tWARN_ON(len_tmp >= sizeof(abuf));\n+\t\t\tabuf[sizeof(abuf) - 1] = '\\0';\n+\n+\t\t\t/* log the arg in the audit record */\n+\t\t\taudit_log_format(*ab, \"%s\", abuf);\n+\t\t\tlen_rem -= len_tmp;\n+\t\t\tlen_tmp = len_buf;\n+\t\t\tif (encode) {\n+\t\t\t\tif (len_abuf > len_rem)\n+\t\t\t\t\tlen_tmp = len_rem / 2; /* encoding */\n+\t\t\t\taudit_log_n_hex(*ab, buf, len_tmp);\n+\t\t\t\tlen_rem -= len_tmp * 2;\n+\t\t\t\tlen_abuf -= len_tmp * 2;\n+\t\t\t} else {\n+\t\t\t\tif (len_abuf > len_rem)\n+\t\t\t\t\tlen_tmp = len_rem - 2; /* quotes */\n+\t\t\t\taudit_log_n_string(*ab, buf, len_tmp);\n+\t\t\t\tlen_rem -= len_tmp + 2;\n+\t\t\t\t/* don't subtract the \"2\" because we still need\n+\t\t\t\t * to add quotes to the remaining string */\n+\t\t\t\tlen_abuf -= len_tmp;\n+\t\t\t}\n+\t\t\tlen_buf -= len_tmp;\n+\t\t\tbuf += len_tmp;\n+\t\t}\n+\n+\t\t/* ready to move to the next argument? */\n+\t\tif ((len_buf == 0) && !require_data) {\n+\t\t\targ++;\n+\t\t\titer = 0;\n+\t\t\tlen_full = 0;\n+\t\t\trequire_data = true;\n+\t\t\tencode = false;\n+\t\t}\n+\t} while (arg < context->execve.argc);\n+\n+\t/* NOTE: the caller handles the final audit_log_end() call */\n+\n+out:\n+\tkfree(buf_head);\n }",
        "function_modified_lines": {
            "added": [
                "\tlong len_max;",
                "\tlong len_rem;",
                "\tlong len_full;",
                "\tlong len_buf;",
                "\tlong len_abuf;",
                "\tlong len_tmp;",
                "\tbool require_data;",
                "\tbool encode;",
                "\tunsigned int iter;",
                "\tunsigned int arg;",
                "\tchar *buf_head;",
                "\tconst char __user *p = (const char __user *)current->mm->arg_start;",
                "\t/* NOTE: this buffer needs to be large enough to hold all the non-arg",
                "\t *       data we put in the audit record for this argument (see the",
                "\t *       code below) ... at this point in time 96 is plenty */",
                "\tchar abuf[96];",
                "",
                "\t/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the",
                "\t *       current value of 7500 is not as important as the fact that it",
                "\t *       is less than 8k, a setting of 7500 gives us plenty of wiggle",
                "\t *       room if we go over a little bit in the logging below */",
                "\tWARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);",
                "\tlen_max = MAX_EXECVE_AUDIT_LEN;",
                "",
                "\t/* scratch buffer to hold the userspace args */",
                "\tbuf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);",
                "\tif (!buf_head) {",
                "\t\taudit_panic(\"out of memory for argv string\");",
                "\t\treturn;",
                "\t}",
                "\tbuf = buf_head;",
                "\tlen_rem = len_max;",
                "\tlen_buf = 0;",
                "\tlen_full = 0;",
                "\trequire_data = true;",
                "\tencode = false;",
                "\titer = 0;",
                "\targ = 0;",
                "\tdo {",
                "\t\t/* NOTE: we don't ever want to trust this value for anything",
                "\t\t *       serious, but the audit record format insists we",
                "\t\t *       provide an argument length for really long arguments,",
                "\t\t *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but",
                "\t\t *       to use strncpy_from_user() to obtain this value for",
                "\t\t *       recording in the log, although we don't use it",
                "\t\t *       anywhere here to avoid a double-fetch problem */",
                "\t\tif (len_full == 0)",
                "\t\t\tlen_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;",
                "\t\t/* read more data from userspace */",
                "\t\tif (require_data) {",
                "\t\t\t/* can we make more room in the buffer? */",
                "\t\t\tif (buf != buf_head) {",
                "\t\t\t\tmemmove(buf_head, buf, len_buf);",
                "\t\t\t\tbuf = buf_head;",
                "\t\t\t}",
                "",
                "\t\t\t/* fetch as much as we can of the argument */",
                "\t\t\tlen_tmp = strncpy_from_user(&buf_head[len_buf], p,",
                "\t\t\t\t\t\t    len_max - len_buf);",
                "\t\t\tif (len_tmp == -EFAULT) {",
                "\t\t\t\t/* unable to copy from userspace */",
                "\t\t\t\tsend_sig(SIGKILL, current, 0);",
                "\t\t\t\tgoto out;",
                "\t\t\t} else if (len_tmp == (len_max - len_buf)) {",
                "\t\t\t\t/* buffer is not large enough */",
                "\t\t\t\trequire_data = true;",
                "\t\t\t\t/* NOTE: if we are going to span multiple",
                "\t\t\t\t *       buffers force the encoding so we stand",
                "\t\t\t\t *       a chance at a sane len_full value and",
                "\t\t\t\t *       consistent record encoding */",
                "\t\t\t\tencode = true;",
                "\t\t\t\tlen_full = len_full * 2;",
                "\t\t\t\tp += len_tmp;",
                "\t\t\t} else {",
                "\t\t\t\trequire_data = false;",
                "\t\t\t\tif (!encode)",
                "\t\t\t\t\tencode = audit_string_contains_control(",
                "\t\t\t\t\t\t\t\tbuf, len_tmp);",
                "\t\t\t\t/* try to use a trusted value for len_full */",
                "\t\t\t\tif (len_full < len_max)",
                "\t\t\t\t\tlen_full = (encode ?",
                "\t\t\t\t\t\t    len_tmp * 2 : len_tmp);",
                "\t\t\t\tp += len_tmp + 1;",
                "\t\t\t}",
                "\t\t\tlen_buf += len_tmp;",
                "\t\t\tbuf_head[len_buf] = '\\0';",
                "",
                "\t\t\t/* length of the buffer in the audit record? */",
                "\t\t\tlen_abuf = (encode ? len_buf * 2 : len_buf + 2);",
                "\t\t}",
                "",
                "\t\t/* write as much as we can to the audit log */",
                "\t\tif (len_buf > 0) {",
                "\t\t\t/* NOTE: some magic numbers here - basically if we",
                "\t\t\t *       can't fit a reasonable amount of data into the",
                "\t\t\t *       existing audit buffer, flush it and start with",
                "\t\t\t *       a new buffer */",
                "\t\t\tif ((sizeof(abuf) + 8) > len_rem) {",
                "\t\t\t\tlen_rem = len_max;",
                "\t\t\t\taudit_log_end(*ab);",
                "\t\t\t\t*ab = audit_log_start(context,",
                "\t\t\t\t\t\t      GFP_KERNEL, AUDIT_EXECVE);",
                "\t\t\t\tif (!*ab)",
                "\t\t\t\t\tgoto out;",
                "\t\t\t}",
                "",
                "\t\t\t/* create the non-arg portion of the arg record */",
                "\t\t\tlen_tmp = 0;",
                "\t\t\tif (require_data || (iter > 0) ||",
                "\t\t\t    ((len_abuf + sizeof(abuf)) > len_rem)) {",
                "\t\t\t\tif (iter == 0) {",
                "\t\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],",
                "\t\t\t\t\t\t\tsizeof(abuf) - len_tmp,",
                "\t\t\t\t\t\t\t\" a%d_len=%lu\",",
                "\t\t\t\t\t\t\targ, len_full);",
                "\t\t\t\t}",
                "\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],",
                "\t\t\t\t\t\t    sizeof(abuf) - len_tmp,",
                "\t\t\t\t\t\t    \" a%d[%d]=\", arg, iter++);",
                "\t\t\t} else",
                "\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],",
                "\t\t\t\t\t\t    sizeof(abuf) - len_tmp,",
                "\t\t\t\t\t\t    \" a%d=\", arg);",
                "\t\t\tWARN_ON(len_tmp >= sizeof(abuf));",
                "\t\t\tabuf[sizeof(abuf) - 1] = '\\0';",
                "",
                "\t\t\t/* log the arg in the audit record */",
                "\t\t\taudit_log_format(*ab, \"%s\", abuf);",
                "\t\t\tlen_rem -= len_tmp;",
                "\t\t\tlen_tmp = len_buf;",
                "\t\t\tif (encode) {",
                "\t\t\t\tif (len_abuf > len_rem)",
                "\t\t\t\t\tlen_tmp = len_rem / 2; /* encoding */",
                "\t\t\t\taudit_log_n_hex(*ab, buf, len_tmp);",
                "\t\t\t\tlen_rem -= len_tmp * 2;",
                "\t\t\t\tlen_abuf -= len_tmp * 2;",
                "\t\t\t} else {",
                "\t\t\t\tif (len_abuf > len_rem)",
                "\t\t\t\t\tlen_tmp = len_rem - 2; /* quotes */",
                "\t\t\t\taudit_log_n_string(*ab, buf, len_tmp);",
                "\t\t\t\tlen_rem -= len_tmp + 2;",
                "\t\t\t\t/* don't subtract the \"2\" because we still need",
                "\t\t\t\t * to add quotes to the remaining string */",
                "\t\t\t\tlen_abuf -= len_tmp;",
                "\t\t\t}",
                "\t\t\tlen_buf -= len_tmp;",
                "\t\t\tbuf += len_tmp;",
                "\t\t}",
                "",
                "\t\t/* ready to move to the next argument? */",
                "\t\tif ((len_buf == 0) && !require_data) {",
                "\t\t\targ++;",
                "\t\t\titer = 0;",
                "\t\t\tlen_full = 0;",
                "\t\t\trequire_data = true;",
                "\t\t\tencode = false;",
                "\t\t}",
                "\t} while (arg < context->execve.argc);",
                "",
                "\t/* NOTE: the caller handles the final audit_log_end() call */",
                "",
                "out:",
                "\tkfree(buf_head);"
            ],
            "deleted": [
                "\tint i, len;",
                "\tsize_t len_sent = 0;",
                "\tconst char __user *p;",
                "\tp = (const char __user *)current->mm->arg_start;",
                "\t/*",
                "\t * we need some kernel buffer to hold the userspace args.  Just",
                "\t * allocate one big one rather than allocating one of the right size",
                "\t * for every single argument inside audit_log_single_execve_arg()",
                "\t * should be <8k allocation so should be pretty safe.",
                "\t */",
                "\tbuf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);",
                "\tif (!buf) {",
                "\t\taudit_panic(\"out of memory for argv string\");",
                "\t\treturn;",
                "\t}",
                "\tfor (i = 0; i < context->execve.argc; i++) {",
                "\t\tlen = audit_log_single_execve_arg(context, ab, i,",
                "\t\t\t\t\t\t  &len_sent, p, buf);",
                "\t\tif (len <= 0)",
                "\t\t\tbreak;",
                "\t\tp += len;",
                "\t}",
                "\tkfree(buf);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a \"double fetch\" vulnerability."
    },
    {
        "cve_id": "CVE-2016-6156",
        "code_before_change": "static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)\n{\n\tlong ret;\n\tstruct cros_ec_command u_cmd;\n\tstruct cros_ec_command *s_cmd;\n\n\tif (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))\n\t\treturn -EFAULT;\n\n\tif ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||\n\t    (u_cmd.insize > EC_MAX_MSG_BYTES))\n\t\treturn -EINVAL;\n\n\ts_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),\n\t\t\tGFP_KERNEL);\n\tif (!s_cmd)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\ts_cmd->command += ec->cmd_offset;\n\tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n\t/* Only copy data to userland if data was received. */\n\tif (ret < 0)\n\t\tgoto exit;\n\n\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))\n\t\tret = -EFAULT;\nexit:\n\tkfree(s_cmd);\n\treturn ret;\n}",
        "code_after_change": "static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)\n{\n\tlong ret;\n\tstruct cros_ec_command u_cmd;\n\tstruct cros_ec_command *s_cmd;\n\n\tif (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))\n\t\treturn -EFAULT;\n\n\tif ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||\n\t    (u_cmd.insize > EC_MAX_MSG_BYTES))\n\t\treturn -EINVAL;\n\n\ts_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),\n\t\t\tGFP_KERNEL);\n\tif (!s_cmd)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\tif (u_cmd.outsize != s_cmd->outsize ||\n\t    u_cmd.insize != s_cmd->insize) {\n\t\tret = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\ts_cmd->command += ec->cmd_offset;\n\tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n\t/* Only copy data to userland if data was received. */\n\tif (ret < 0)\n\t\tgoto exit;\n\n\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))\n\t\tret = -EFAULT;\nexit:\n\tkfree(s_cmd);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,13 +21,19 @@\n \t\tgoto exit;\n \t}\n \n+\tif (u_cmd.outsize != s_cmd->outsize ||\n+\t    u_cmd.insize != s_cmd->insize) {\n+\t\tret = -EINVAL;\n+\t\tgoto exit;\n+\t}\n+\n \ts_cmd->command += ec->cmd_offset;\n \tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n \t/* Only copy data to userland if data was received. */\n \tif (ret < 0)\n \t\tgoto exit;\n \n-\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))\n+\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))\n \t\tret = -EFAULT;\n exit:\n \tkfree(s_cmd);",
        "function_modified_lines": {
            "added": [
                "\tif (u_cmd.outsize != s_cmd->outsize ||",
                "\t    u_cmd.insize != s_cmd->insize) {",
                "\t\tret = -EINVAL;",
                "\t\tgoto exit;",
                "\t}",
                "",
                "\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))"
            ],
            "deleted": [
                "\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ec_device_ioctl_xcmd function in drivers/platform/chrome/cros_ec_dev.c in the Linux kernel before 4.7 allows local users to cause a denial of service (out-of-bounds array access) by changing a certain size value, aka a \"double fetch\" vulnerability."
    },
    {
        "cve_id": "CVE-2016-6480",
        "code_before_change": "static int ioctl_send_fib(struct aac_dev * dev, void __user *arg)\n{\n\tstruct hw_fib * kfib;\n\tstruct fib *fibptr;\n\tstruct hw_fib * hw_fib = (struct hw_fib *)0;\n\tdma_addr_t hw_fib_pa = (dma_addr_t)0LL;\n\tunsigned size;\n\tint retval;\n\n\tif (dev->in_reset) {\n\t\treturn -EBUSY;\n\t}\n\tfibptr = aac_fib_alloc(dev);\n\tif(fibptr == NULL) {\n\t\treturn -ENOMEM;\n\t}\n\n\tkfib = fibptr->hw_fib_va;\n\t/*\n\t *\tFirst copy in the header so that we can check the size field.\n\t */\n\tif (copy_from_user((void *)kfib, arg, sizeof(struct aac_fibhdr))) {\n\t\taac_fib_free(fibptr);\n\t\treturn -EFAULT;\n\t}\n\t/*\n\t *\tSince we copy based on the fib header size, make sure that we\n\t *\twill not overrun the buffer when we copy the memory. Return\n\t *\tan error if we would.\n\t */\n\tsize = le16_to_cpu(kfib->header.Size) + sizeof(struct aac_fibhdr);\n\tif (size < le16_to_cpu(kfib->header.SenderSize))\n\t\tsize = le16_to_cpu(kfib->header.SenderSize);\n\tif (size > dev->max_fib_size) {\n\t\tdma_addr_t daddr;\n\n\t\tif (size > 2048) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tkfib = pci_alloc_consistent(dev->pdev, size, &daddr);\n\t\tif (!kfib) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\t/* Highjack the hw_fib */\n\t\thw_fib = fibptr->hw_fib_va;\n\t\thw_fib_pa = fibptr->hw_fib_pa;\n\t\tfibptr->hw_fib_va = kfib;\n\t\tfibptr->hw_fib_pa = daddr;\n\t\tmemset(((char *)kfib) + dev->max_fib_size, 0, size - dev->max_fib_size);\n\t\tmemcpy(kfib, hw_fib, dev->max_fib_size);\n\t}\n\n\tif (copy_from_user(kfib, arg, size)) {\n\t\tretval = -EFAULT;\n\t\tgoto cleanup;\n\t}\n\n\tif (kfib->header.Command == cpu_to_le16(TakeABreakPt)) {\n\t\taac_adapter_interrupt(dev);\n\t\t/*\n\t\t * Since we didn't really send a fib, zero out the state to allow\n\t\t * cleanup code not to assert.\n\t\t */\n\t\tkfib->header.XferState = 0;\n\t} else {\n\t\tretval = aac_fib_send(le16_to_cpu(kfib->header.Command), fibptr,\n\t\t\t\tle16_to_cpu(kfib->header.Size) , FsaNormal,\n\t\t\t\t1, 1, NULL, NULL);\n\t\tif (retval) {\n\t\t\tgoto cleanup;\n\t\t}\n\t\tif (aac_fib_complete(fibptr) != 0) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\t/*\n\t *\tMake sure that the size returned by the adapter (which includes\n\t *\tthe header) is less than or equal to the size of a fib, so we\n\t *\tdon't corrupt application data. Then copy that size to the user\n\t *\tbuffer. (Don't try to add the header information again, since it\n\t *\twas already included by the adapter.)\n\t */\n\n\tretval = 0;\n\tif (copy_to_user(arg, (void *)kfib, size))\n\t\tretval = -EFAULT;\ncleanup:\n\tif (hw_fib) {\n\t\tpci_free_consistent(dev->pdev, size, kfib, fibptr->hw_fib_pa);\n\t\tfibptr->hw_fib_pa = hw_fib_pa;\n\t\tfibptr->hw_fib_va = hw_fib;\n\t}\n\tif (retval != -ERESTARTSYS)\n\t\taac_fib_free(fibptr);\n\treturn retval;\n}",
        "code_after_change": "static int ioctl_send_fib(struct aac_dev * dev, void __user *arg)\n{\n\tstruct hw_fib * kfib;\n\tstruct fib *fibptr;\n\tstruct hw_fib * hw_fib = (struct hw_fib *)0;\n\tdma_addr_t hw_fib_pa = (dma_addr_t)0LL;\n\tunsigned int size, osize;\n\tint retval;\n\n\tif (dev->in_reset) {\n\t\treturn -EBUSY;\n\t}\n\tfibptr = aac_fib_alloc(dev);\n\tif(fibptr == NULL) {\n\t\treturn -ENOMEM;\n\t}\n\n\tkfib = fibptr->hw_fib_va;\n\t/*\n\t *\tFirst copy in the header so that we can check the size field.\n\t */\n\tif (copy_from_user((void *)kfib, arg, sizeof(struct aac_fibhdr))) {\n\t\taac_fib_free(fibptr);\n\t\treturn -EFAULT;\n\t}\n\t/*\n\t *\tSince we copy based on the fib header size, make sure that we\n\t *\twill not overrun the buffer when we copy the memory. Return\n\t *\tan error if we would.\n\t */\n\tosize = size = le16_to_cpu(kfib->header.Size) +\n\t\tsizeof(struct aac_fibhdr);\n\tif (size < le16_to_cpu(kfib->header.SenderSize))\n\t\tsize = le16_to_cpu(kfib->header.SenderSize);\n\tif (size > dev->max_fib_size) {\n\t\tdma_addr_t daddr;\n\n\t\tif (size > 2048) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tkfib = pci_alloc_consistent(dev->pdev, size, &daddr);\n\t\tif (!kfib) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\t/* Highjack the hw_fib */\n\t\thw_fib = fibptr->hw_fib_va;\n\t\thw_fib_pa = fibptr->hw_fib_pa;\n\t\tfibptr->hw_fib_va = kfib;\n\t\tfibptr->hw_fib_pa = daddr;\n\t\tmemset(((char *)kfib) + dev->max_fib_size, 0, size - dev->max_fib_size);\n\t\tmemcpy(kfib, hw_fib, dev->max_fib_size);\n\t}\n\n\tif (copy_from_user(kfib, arg, size)) {\n\t\tretval = -EFAULT;\n\t\tgoto cleanup;\n\t}\n\n\t/* Sanity check the second copy */\n\tif ((osize != le16_to_cpu(kfib->header.Size) +\n\t\tsizeof(struct aac_fibhdr))\n\t\t|| (size < le16_to_cpu(kfib->header.SenderSize))) {\n\t\tretval = -EINVAL;\n\t\tgoto cleanup;\n\t}\n\n\tif (kfib->header.Command == cpu_to_le16(TakeABreakPt)) {\n\t\taac_adapter_interrupt(dev);\n\t\t/*\n\t\t * Since we didn't really send a fib, zero out the state to allow\n\t\t * cleanup code not to assert.\n\t\t */\n\t\tkfib->header.XferState = 0;\n\t} else {\n\t\tretval = aac_fib_send(le16_to_cpu(kfib->header.Command), fibptr,\n\t\t\t\tle16_to_cpu(kfib->header.Size) , FsaNormal,\n\t\t\t\t1, 1, NULL, NULL);\n\t\tif (retval) {\n\t\t\tgoto cleanup;\n\t\t}\n\t\tif (aac_fib_complete(fibptr) != 0) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\t/*\n\t *\tMake sure that the size returned by the adapter (which includes\n\t *\tthe header) is less than or equal to the size of a fib, so we\n\t *\tdon't corrupt application data. Then copy that size to the user\n\t *\tbuffer. (Don't try to add the header information again, since it\n\t *\twas already included by the adapter.)\n\t */\n\n\tretval = 0;\n\tif (copy_to_user(arg, (void *)kfib, size))\n\t\tretval = -EFAULT;\ncleanup:\n\tif (hw_fib) {\n\t\tpci_free_consistent(dev->pdev, size, kfib, fibptr->hw_fib_pa);\n\t\tfibptr->hw_fib_pa = hw_fib_pa;\n\t\tfibptr->hw_fib_va = hw_fib;\n\t}\n\tif (retval != -ERESTARTSYS)\n\t\taac_fib_free(fibptr);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \tstruct fib *fibptr;\n \tstruct hw_fib * hw_fib = (struct hw_fib *)0;\n \tdma_addr_t hw_fib_pa = (dma_addr_t)0LL;\n-\tunsigned size;\n+\tunsigned int size, osize;\n \tint retval;\n \n \tif (dev->in_reset) {\n@@ -28,7 +28,8 @@\n \t *\twill not overrun the buffer when we copy the memory. Return\n \t *\tan error if we would.\n \t */\n-\tsize = le16_to_cpu(kfib->header.Size) + sizeof(struct aac_fibhdr);\n+\tosize = size = le16_to_cpu(kfib->header.Size) +\n+\t\tsizeof(struct aac_fibhdr);\n \tif (size < le16_to_cpu(kfib->header.SenderSize))\n \t\tsize = le16_to_cpu(kfib->header.SenderSize);\n \tif (size > dev->max_fib_size) {\n@@ -56,6 +57,14 @@\n \n \tif (copy_from_user(kfib, arg, size)) {\n \t\tretval = -EFAULT;\n+\t\tgoto cleanup;\n+\t}\n+\n+\t/* Sanity check the second copy */\n+\tif ((osize != le16_to_cpu(kfib->header.Size) +\n+\t\tsizeof(struct aac_fibhdr))\n+\t\t|| (size < le16_to_cpu(kfib->header.SenderSize))) {\n+\t\tretval = -EINVAL;\n \t\tgoto cleanup;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\tunsigned int size, osize;",
                "\tosize = size = le16_to_cpu(kfib->header.Size) +",
                "\t\tsizeof(struct aac_fibhdr);",
                "\t\tgoto cleanup;",
                "\t}",
                "",
                "\t/* Sanity check the second copy */",
                "\tif ((osize != le16_to_cpu(kfib->header.Size) +",
                "\t\tsizeof(struct aac_fibhdr))",
                "\t\t|| (size < le16_to_cpu(kfib->header.SenderSize))) {",
                "\t\tretval = -EINVAL;"
            ],
            "deleted": [
                "\tunsigned size;",
                "\tsize = le16_to_cpu(kfib->header.Size) + sizeof(struct aac_fibhdr);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ioctl_send_fib function in drivers/scsi/aacraid/commctrl.c in the Linux kernel through 4.7 allows local users to cause a denial of service (out-of-bounds access or system crash) by changing a certain size value, aka a \"double fetch\" vulnerability."
    },
    {
        "cve_id": "CVE-2016-6516",
        "code_before_change": "static long ioctl_file_dedupe_range(struct file *file, void __user *arg)\n{\n\tstruct file_dedupe_range __user *argp = arg;\n\tstruct file_dedupe_range *same = NULL;\n\tint ret;\n\tunsigned long size;\n\tu16 count;\n\n\tif (get_user(count, &argp->dest_count)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tsize = offsetof(struct file_dedupe_range __user, info[count]);\n\n\tsame = memdup_user(argp, size);\n\tif (IS_ERR(same)) {\n\t\tret = PTR_ERR(same);\n\t\tsame = NULL;\n\t\tgoto out;\n\t}\n\n\tret = vfs_dedupe_file_range(file, same);\n\tif (ret)\n\t\tgoto out;\n\n\tret = copy_to_user(argp, same, size);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(same);\n\treturn ret;\n}",
        "code_after_change": "static long ioctl_file_dedupe_range(struct file *file, void __user *arg)\n{\n\tstruct file_dedupe_range __user *argp = arg;\n\tstruct file_dedupe_range *same = NULL;\n\tint ret;\n\tunsigned long size;\n\tu16 count;\n\n\tif (get_user(count, &argp->dest_count)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tsize = offsetof(struct file_dedupe_range __user, info[count]);\n\n\tsame = memdup_user(argp, size);\n\tif (IS_ERR(same)) {\n\t\tret = PTR_ERR(same);\n\t\tsame = NULL;\n\t\tgoto out;\n\t}\n\n\tsame->dest_count = count;\n\tret = vfs_dedupe_file_range(file, same);\n\tif (ret)\n\t\tgoto out;\n\n\tret = copy_to_user(argp, same, size);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(same);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,6 +20,7 @@\n \t\tgoto out;\n \t}\n \n+\tsame->dest_count = count;\n \tret = vfs_dedupe_file_range(file, same);\n \tif (ret)\n \t\tgoto out;",
        "function_modified_lines": {
            "added": [
                "\tsame->dest_count = count;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-119",
            "CWE-362"
        ],
        "cve_description": "Race condition in the ioctl_file_dedupe_range function in fs/ioctl.c in the Linux kernel through 4.7 allows local users to cause a denial of service (heap-based buffer overflow) or possibly gain privileges by changing a certain count value, aka a \"double fetch\" vulnerability."
    },
    {
        "cve_id": "CVE-2016-7911",
        "code_before_change": "static int get_task_ioprio(struct task_struct *p)\n{\n\tint ret;\n\n\tret = security_task_getioprio(p);\n\tif (ret)\n\t\tgoto out;\n\tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n\tif (p->io_context)\n\t\tret = p->io_context->ioprio;\nout:\n\treturn ret;\n}",
        "code_after_change": "static int get_task_ioprio(struct task_struct *p)\n{\n\tint ret;\n\n\tret = security_task_getioprio(p);\n\tif (ret)\n\t\tgoto out;\n\tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n\ttask_lock(p);\n\tif (p->io_context)\n\t\tret = p->io_context->ioprio;\n\ttask_unlock(p);\nout:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,10 @@\n \tif (ret)\n \t\tgoto out;\n \tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n+\ttask_lock(p);\n \tif (p->io_context)\n \t\tret = p->io_context->ioprio;\n+\ttask_unlock(p);\n out:\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\ttask_lock(p);",
                "\ttask_unlock(p);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the get_task_ioprio function in block/ioprio.c in the Linux kernel before 4.6.6 allows local users to gain privileges or cause a denial of service (use-after-free) via a crafted ioprio_get system call."
    },
    {
        "cve_id": "CVE-2016-7916",
        "code_before_change": "static ssize_t environ_read(struct file *file, char __user *buf,\n\t\t\tsize_t count, loff_t *ppos)\n{\n\tchar *page;\n\tunsigned long src = *ppos;\n\tint ret = 0;\n\tstruct mm_struct *mm = file->private_data;\n\tunsigned long env_start, env_end;\n\n\tif (!mm)\n\t\treturn 0;\n\n\tpage = (char *)__get_free_page(GFP_TEMPORARY);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tret = 0;\n\tif (!atomic_inc_not_zero(&mm->mm_users))\n\t\tgoto free;\n\n\tdown_read(&mm->mmap_sem);\n\tenv_start = mm->env_start;\n\tenv_end = mm->env_end;\n\tup_read(&mm->mmap_sem);\n\n\twhile (count > 0) {\n\t\tsize_t this_len, max_len;\n\t\tint retval;\n\n\t\tif (src >= (env_end - env_start))\n\t\t\tbreak;\n\n\t\tthis_len = env_end - (env_start + src);\n\n\t\tmax_len = min_t(size_t, PAGE_SIZE, count);\n\t\tthis_len = min(max_len, this_len);\n\n\t\tretval = access_remote_vm(mm, (env_start + src),\n\t\t\tpage, this_len, 0);\n\n\t\tif (retval <= 0) {\n\t\t\tret = retval;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_to_user(buf, page, retval)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret += retval;\n\t\tsrc += retval;\n\t\tbuf += retval;\n\t\tcount -= retval;\n\t}\n\t*ppos = src;\n\tmmput(mm);\n\nfree:\n\tfree_page((unsigned long) page);\n\treturn ret;\n}",
        "code_after_change": "static ssize_t environ_read(struct file *file, char __user *buf,\n\t\t\tsize_t count, loff_t *ppos)\n{\n\tchar *page;\n\tunsigned long src = *ppos;\n\tint ret = 0;\n\tstruct mm_struct *mm = file->private_data;\n\tunsigned long env_start, env_end;\n\n\t/* Ensure the process spawned far enough to have an environment. */\n\tif (!mm || !mm->env_end)\n\t\treturn 0;\n\n\tpage = (char *)__get_free_page(GFP_TEMPORARY);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tret = 0;\n\tif (!atomic_inc_not_zero(&mm->mm_users))\n\t\tgoto free;\n\n\tdown_read(&mm->mmap_sem);\n\tenv_start = mm->env_start;\n\tenv_end = mm->env_end;\n\tup_read(&mm->mmap_sem);\n\n\twhile (count > 0) {\n\t\tsize_t this_len, max_len;\n\t\tint retval;\n\n\t\tif (src >= (env_end - env_start))\n\t\t\tbreak;\n\n\t\tthis_len = env_end - (env_start + src);\n\n\t\tmax_len = min_t(size_t, PAGE_SIZE, count);\n\t\tthis_len = min(max_len, this_len);\n\n\t\tretval = access_remote_vm(mm, (env_start + src),\n\t\t\tpage, this_len, 0);\n\n\t\tif (retval <= 0) {\n\t\t\tret = retval;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_to_user(buf, page, retval)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret += retval;\n\t\tsrc += retval;\n\t\tbuf += retval;\n\t\tcount -= retval;\n\t}\n\t*ppos = src;\n\tmmput(mm);\n\nfree:\n\tfree_page((unsigned long) page);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,8 @@\n \tstruct mm_struct *mm = file->private_data;\n \tunsigned long env_start, env_end;\n \n-\tif (!mm)\n+\t/* Ensure the process spawned far enough to have an environment. */\n+\tif (!mm || !mm->env_end)\n \t\treturn 0;\n \n \tpage = (char *)__get_free_page(GFP_TEMPORARY);",
        "function_modified_lines": {
            "added": [
                "\t/* Ensure the process spawned far enough to have an environment. */",
                "\tif (!mm || !mm->env_end)"
            ],
            "deleted": [
                "\tif (!mm)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the environ_read function in fs/proc/base.c in the Linux kernel before 4.5.4 allows local users to obtain sensitive information from kernel memory by reading a /proc/*/environ file during a process-setup time interval in which environment-variable copying is incomplete."
    },
    {
        "cve_id": "CVE-2016-8655",
        "code_before_change": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n\tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n\t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n\t\tgoto out;\n\t}\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    (int)(req->tp_block_size -\n\t\t\t  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t/* Transmit path is not supported. We checked\n\t\t * it above but just being paranoid\n\t\t */\n\t\t\tif (!tx_ring)\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (closing && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\trelease_sock(sk);\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\treturn err;\n}",
        "code_after_change": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\tlock_sock(sk);\n\t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n\tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n\t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n\t\tgoto out;\n\t}\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    (int)(req->tp_block_size -\n\t\t\t  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t/* Transmit path is not supported. We checked\n\t\t * it above but just being paranoid\n\t\t */\n\t\t\tif (!tx_ring)\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (closing && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,7 @@\n \t/* Added to avoid minimal code churn */\n \tstruct tpacket_req *req = &req_u->req;\n \n+\tlock_sock(sk);\n \t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n \tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n \t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n@@ -92,7 +93,6 @@\n \t\t\tgoto out;\n \t}\n \n-\tlock_sock(sk);\n \n \t/* Detach socket from network */\n \tspin_lock(&po->bind_lock);\n@@ -141,10 +141,10 @@\n \t\tif (!tx_ring)\n \t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n \t}\n-\trelease_sock(sk);\n \n \tif (pg_vec)\n \t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\n out:\n+\trelease_sock(sk);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tlock_sock(sk);",
                "\trelease_sock(sk);"
            ],
            "deleted": [
                "\tlock_sock(sk);",
                "\trelease_sock(sk);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in net/packet/af_packet.c in the Linux kernel through 4.8.12 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging the CAP_NET_RAW capability to change a socket version, related to the packet_set_ring and packet_setsockopt functions."
    },
    {
        "cve_id": "CVE-2016-8655",
        "code_before_change": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_version = val;\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
        "code_after_change": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_version = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -71,19 +71,25 @@\n \n \t\tif (optlen != sizeof(val))\n \t\t\treturn -EINVAL;\n-\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n-\t\t\treturn -EBUSY;\n \t\tif (copy_from_user(&val, optval, sizeof(val)))\n \t\t\treturn -EFAULT;\n \t\tswitch (val) {\n \t\tcase TPACKET_V1:\n \t\tcase TPACKET_V2:\n \t\tcase TPACKET_V3:\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\treturn -EINVAL;\n+\t\t}\n+\t\tlock_sock(sk);\n+\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n+\t\t\tret = -EBUSY;\n+\t\t} else {\n \t\t\tpo->tp_version = val;\n-\t\t\treturn 0;\n-\t\tdefault:\n-\t\t\treturn -EINVAL;\n+\t\t\tret = 0;\n \t\t}\n+\t\trelease_sock(sk);\n+\t\treturn ret;\n \t}\n \tcase PACKET_RESERVE:\n \t{",
        "function_modified_lines": {
            "added": [
                "\t\t\tbreak;",
                "\t\tdefault:",
                "\t\t\treturn -EINVAL;",
                "\t\t}",
                "\t\tlock_sock(sk);",
                "\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {",
                "\t\t\tret = -EBUSY;",
                "\t\t} else {",
                "\t\t\tret = 0;",
                "\t\trelease_sock(sk);",
                "\t\treturn ret;"
            ],
            "deleted": [
                "\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)",
                "\t\t\treturn -EBUSY;",
                "\t\t\treturn 0;",
                "\t\tdefault:",
                "\t\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in net/packet/af_packet.c in the Linux kernel through 4.8.12 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging the CAP_NET_RAW capability to change a socket version, related to the packet_set_ring and packet_setsockopt functions."
    },
    {
        "cve_id": "CVE-2016-9794",
        "code_before_change": "void snd_pcm_period_elapsed(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tunsigned long flags;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\n\tsnd_pcm_stream_lock_irqsave(substream, flags);\n\tif (!snd_pcm_running(substream) ||\n\t    snd_pcm_update_hw_ptr0(substream, 1) < 0)\n\t\tgoto _end;\n\n#ifdef CONFIG_SND_PCM_TIMER\n\tif (substream->timer_running)\n\t\tsnd_timer_interrupt(substream->timer, 1);\n#endif\n _end:\n\tsnd_pcm_stream_unlock_irqrestore(substream, flags);\n\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n}",
        "code_after_change": "void snd_pcm_period_elapsed(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tunsigned long flags;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\n\tsnd_pcm_stream_lock_irqsave(substream, flags);\n\tif (!snd_pcm_running(substream) ||\n\t    snd_pcm_update_hw_ptr0(substream, 1) < 0)\n\t\tgoto _end;\n\n#ifdef CONFIG_SND_PCM_TIMER\n\tif (substream->timer_running)\n\t\tsnd_timer_interrupt(substream->timer, 1);\n#endif\n _end:\n\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n\tsnd_pcm_stream_unlock_irqrestore(substream, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,6 @@\n \t\tsnd_timer_interrupt(substream->timer, 1);\n #endif\n  _end:\n+\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n \tsnd_pcm_stream_unlock_irqrestore(substream, flags);\n-\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n }",
        "function_modified_lines": {
            "added": [
                "\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);"
            ],
            "deleted": [
                "\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the snd_pcm_period_elapsed function in sound/core/pcm_lib.c in the ALSA subsystem in the Linux kernel before 4.7 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via a crafted SNDRV_PCM_TRIGGER_START command."
    },
    {
        "cve_id": "CVE-2016-9806",
        "code_before_change": "static int netlink_dump(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct netlink_callback *cb;\n\tstruct sk_buff *skb = NULL;\n\tstruct nlmsghdr *nlh;\n\tint len, err = -ENOBUFS;\n\tint alloc_min_size;\n\tint alloc_size;\n\n\tmutex_lock(nlk->cb_mutex);\n\tif (!nlk->cb_running) {\n\t\terr = -EINVAL;\n\t\tgoto errout_skb;\n\t}\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto errout_skb;\n\n\t/* NLMSG_GOODSIZE is small to avoid high order allocations being\n\t * required, but it makes sense to _attempt_ a 16K bytes allocation\n\t * to reduce number of system calls on dump operations, if user\n\t * ever provided a big enough buffer.\n\t */\n\tcb = &nlk->cb;\n\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n\n\tif (alloc_min_size < nlk->max_recvmsg_len) {\n\t\talloc_size = nlk->max_recvmsg_len;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL |\n\t\t\t\t\t    __GFP_NOWARN | __GFP_NORETRY);\n\t}\n\tif (!skb) {\n\t\talloc_size = alloc_min_size;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL);\n\t}\n\tif (!skb)\n\t\tgoto errout_skb;\n\n\t/* Trim skb to allocated size. User is expected to provide buffer as\n\t * large as max(min_dump_alloc, 16KiB (mac_recvmsg_len capped at\n\t * netlink_recvmsg())). dump will pack as many smaller messages as\n\t * could fit within the allocated skb. skb is typically allocated\n\t * with larger space than required (could be as much as near 2x the\n\t * requested size with align to next power of 2 approach). Allowing\n\t * dump to use the excess space makes it difficult for a user to have a\n\t * reasonable static buffer based on the expected largest dump of a\n\t * single netdev. The outcome is MSG_TRUNC error.\n\t */\n\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);\n\tnetlink_skb_set_owner_r(skb, sk);\n\n\tlen = cb->dump(skb, cb);\n\n\tif (len > 0) {\n\t\tmutex_unlock(nlk->cb_mutex);\n\n\t\tif (sk_filter(sk, skb))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\t__netlink_sendskb(sk, skb);\n\t\treturn 0;\n\t}\n\n\tnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\n\tif (!nlh)\n\t\tgoto errout_skb;\n\n\tnl_dump_check_consistent(cb, nlh);\n\n\tmemcpy(nlmsg_data(nlh), &len, sizeof(len));\n\n\tif (sk_filter(sk, skb))\n\t\tkfree_skb(skb);\n\telse\n\t\t__netlink_sendskb(sk, skb);\n\n\tif (cb->done)\n\t\tcb->done(cb);\n\n\tnlk->cb_running = false;\n\tmutex_unlock(nlk->cb_mutex);\n\tmodule_put(cb->module);\n\tconsume_skb(cb->skb);\n\treturn 0;\n\nerrout_skb:\n\tmutex_unlock(nlk->cb_mutex);\n\tkfree_skb(skb);\n\treturn err;\n}",
        "code_after_change": "static int netlink_dump(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct netlink_callback *cb;\n\tstruct sk_buff *skb = NULL;\n\tstruct nlmsghdr *nlh;\n\tstruct module *module;\n\tint len, err = -ENOBUFS;\n\tint alloc_min_size;\n\tint alloc_size;\n\n\tmutex_lock(nlk->cb_mutex);\n\tif (!nlk->cb_running) {\n\t\terr = -EINVAL;\n\t\tgoto errout_skb;\n\t}\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto errout_skb;\n\n\t/* NLMSG_GOODSIZE is small to avoid high order allocations being\n\t * required, but it makes sense to _attempt_ a 16K bytes allocation\n\t * to reduce number of system calls on dump operations, if user\n\t * ever provided a big enough buffer.\n\t */\n\tcb = &nlk->cb;\n\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n\n\tif (alloc_min_size < nlk->max_recvmsg_len) {\n\t\talloc_size = nlk->max_recvmsg_len;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL |\n\t\t\t\t\t    __GFP_NOWARN | __GFP_NORETRY);\n\t}\n\tif (!skb) {\n\t\talloc_size = alloc_min_size;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL);\n\t}\n\tif (!skb)\n\t\tgoto errout_skb;\n\n\t/* Trim skb to allocated size. User is expected to provide buffer as\n\t * large as max(min_dump_alloc, 16KiB (mac_recvmsg_len capped at\n\t * netlink_recvmsg())). dump will pack as many smaller messages as\n\t * could fit within the allocated skb. skb is typically allocated\n\t * with larger space than required (could be as much as near 2x the\n\t * requested size with align to next power of 2 approach). Allowing\n\t * dump to use the excess space makes it difficult for a user to have a\n\t * reasonable static buffer based on the expected largest dump of a\n\t * single netdev. The outcome is MSG_TRUNC error.\n\t */\n\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);\n\tnetlink_skb_set_owner_r(skb, sk);\n\n\tlen = cb->dump(skb, cb);\n\n\tif (len > 0) {\n\t\tmutex_unlock(nlk->cb_mutex);\n\n\t\tif (sk_filter(sk, skb))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\t__netlink_sendskb(sk, skb);\n\t\treturn 0;\n\t}\n\n\tnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\n\tif (!nlh)\n\t\tgoto errout_skb;\n\n\tnl_dump_check_consistent(cb, nlh);\n\n\tmemcpy(nlmsg_data(nlh), &len, sizeof(len));\n\n\tif (sk_filter(sk, skb))\n\t\tkfree_skb(skb);\n\telse\n\t\t__netlink_sendskb(sk, skb);\n\n\tif (cb->done)\n\t\tcb->done(cb);\n\n\tnlk->cb_running = false;\n\tmodule = cb->module;\n\tskb = cb->skb;\n\tmutex_unlock(nlk->cb_mutex);\n\tmodule_put(module);\n\tconsume_skb(skb);\n\treturn 0;\n\nerrout_skb:\n\tmutex_unlock(nlk->cb_mutex);\n\tkfree_skb(skb);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \tstruct netlink_callback *cb;\n \tstruct sk_buff *skb = NULL;\n \tstruct nlmsghdr *nlh;\n+\tstruct module *module;\n \tint len, err = -ENOBUFS;\n \tint alloc_min_size;\n \tint alloc_size;\n@@ -79,9 +80,11 @@\n \t\tcb->done(cb);\n \n \tnlk->cb_running = false;\n+\tmodule = cb->module;\n+\tskb = cb->skb;\n \tmutex_unlock(nlk->cb_mutex);\n-\tmodule_put(cb->module);\n-\tconsume_skb(cb->skb);\n+\tmodule_put(module);\n+\tconsume_skb(skb);\n \treturn 0;\n \n errout_skb:",
        "function_modified_lines": {
            "added": [
                "\tstruct module *module;",
                "\tmodule = cb->module;",
                "\tskb = cb->skb;",
                "\tmodule_put(module);",
                "\tconsume_skb(skb);"
            ],
            "deleted": [
                "\tmodule_put(cb->module);",
                "\tconsume_skb(cb->skb);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-415"
        ],
        "cve_description": "Race condition in the netlink_dump function in net/netlink/af_netlink.c in the Linux kernel before 4.6.3 allows local users to cause a denial of service (double free) or possibly have unspecified other impact via a crafted application that makes sendmsg system calls, leading to a free operation associated with a new dump that started earlier than anticipated."
    },
    {
        "cve_id": "CVE-2017-1000112",
        "code_before_change": "static int __ip_append_data(struct sock *sk,\n\t\t\t    struct flowi4 *fl4,\n\t\t\t    struct sk_buff_head *queue,\n\t\t\t    struct inet_cork *cork,\n\t\t\t    struct page_frag *pfrag,\n\t\t\t    int getfrag(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t    unsigned int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\n\tstruct ip_options *opt = cork->opt;\n\tint hh_len;\n\tint exthdrlen;\n\tint mtu;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\tunsigned int maxfraglen, fragheaderlen, maxnonfragsize;\n\tint csummode = CHECKSUM_NONE;\n\tstruct rtable *rt = (struct rtable *)cork->dst;\n\tu32 tskey = 0;\n\n\tskb = skb_peek_tail(queue);\n\n\texthdrlen = !skb ? rt->dst.header_len : 0;\n\tmtu = cork->fragsize;\n\tif (cork->tx_flags & SKBTX_ANY_SW_TSTAMP &&\n\t    sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)\n\t\ttskey = sk->sk_tskey++;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\tmaxnonfragsize = ip_sk_ignore_df(sk) ? 0xFFFF : mtu;\n\n\tif (cork->length + length > maxnonfragsize - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, fl4->daddr, inet->inet_dport,\n\t\t\t       mtu - (opt ? opt->optlen : 0));\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/*\n\t * transhdrlen > 0 means that this is the first fragment and we wish\n\t * it won't be fragmented in the future.\n\t */\n\tif (transhdrlen &&\n\t    length + fragheaderlen <= mtu &&\n\t    rt->dst.dev->features & (NETIF_F_HW_CSUM | NETIF_F_IP_CSUM) &&\n\t    !(flags & MSG_MORE) &&\n\t    !exthdrlen)\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tcork->length += length;\n\tif ((((length + (skb ? skb->len : fragheaderlen)) > mtu) ||\n\t     (skb && skb_is_gso(skb))) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO) && !dst_xfrm(&rt->dst) &&\n\t    (sk->sk_type == SOCK_DGRAM) && !sk->sk_no_check_tx) {\n\t\terr = ip_ufo_append_data(sk, queue, getfrag, from, length,\n\t\t\t\t\t hh_len, fragheaderlen, transhdrlen,\n\t\t\t\t\t maxfraglen, flags);\n\t\tif (err)\n\t\t\tgoto error;\n\t\treturn 0;\n\t}\n\n\t/* So, what's going on in the loop below?\n\t *\n\t * We use calculated fragment length to generate chained skb,\n\t * each of segments is IP fragment ready for sending to network after\n\t * adding appropriate IP header.\n\t */\n\n\tif (!skb)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = mtu - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\n\t\t\tstruct sk_buff *skb_prev;\nalloc_new_skb:\n\t\t\tskb_prev = skb;\n\t\t\tif (skb_prev)\n\t\t\t\tfraggap = skb_prev->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\t\t\tif (datalen > mtu - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen;\n\t\t\tfraglen = datalen + fragheaderlen;\n\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse\n\t\t\t\talloclen = fraglen;\n\n\t\t\talloclen += exthdrlen;\n\n\t\t\t/* The last fragment gets additional space at tail.\n\t\t\t * Note, with MSG_MORE we overallocate on fragments,\n\t\t\t * because we have no idea what fragment will be\n\t\t\t * the last.\n\t\t\t */\n\t\t\tif (datalen == length + fraggap)\n\t\t\t\talloclen += rt->dst.trailer_len;\n\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len + 15,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (refcount_read(&sk->sk_wmem_alloc) <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = sock_wmalloc(sk,\n\t\t\t\t\t\t\t   alloclen + hh_len + 15, 1,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\t\tif (unlikely(!skb))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t}\n\t\t\tif (!skb)\n\t\t\t\tgoto error;\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\n\t\t\t/* only the initial fragment is time stamped */\n\t\t\tskb_shinfo(skb)->tx_flags = cork->tx_flags;\n\t\t\tcork->tx_flags = 0;\n\t\t\tskb_shinfo(skb)->tskey = tskey;\n\t\t\ttskey = 0;\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen + exthdrlen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tdata += fragheaderlen + exthdrlen;\n\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\tcopy = datalen - transhdrlen - fraggap;\n\t\t\tif (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= datalen - fraggap;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tcsummode = CHECKSUM_NONE;\n\n\t\t\tif ((flags & MSG_CONFIRM) && !skb_prev)\n\t\t\t\tskb_set_dst_pending_confirm(skb, 1);\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\t__skb_queue_tail(queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG)) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\n\t\t\terr = -ENOMEM;\n\t\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\t\tgoto error;\n\n\t\t\tif (!skb_can_coalesce(skb, i, pfrag->page,\n\t\t\t\t\t      pfrag->offset)) {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tif (i == MAX_SKB_FRAGS)\n\t\t\t\t\tgoto error;\n\n\t\t\t\t__skb_fill_page_desc(skb, i, pfrag->page,\n\t\t\t\t\t\t     pfrag->offset, 0);\n\t\t\t\tskb_shinfo(skb)->nr_frags = ++i;\n\t\t\t\tget_page(pfrag->page);\n\t\t\t}\n\t\t\tcopy = min_t(int, copy, pfrag->size - pfrag->offset);\n\t\t\tif (getfrag(from,\n\t\t\t\t    page_address(pfrag->page) + pfrag->offset,\n\t\t\t\t    offset, copy, skb->len, skb) < 0)\n\t\t\t\tgoto error_efault;\n\n\t\t\tpfrag->offset += copy;\n\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\trefcount_add(copy, &sk->sk_wmem_alloc);\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\n\treturn 0;\n\nerror_efault:\n\terr = -EFAULT;\nerror:\n\tcork->length -= length;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}",
        "code_after_change": "static int __ip_append_data(struct sock *sk,\n\t\t\t    struct flowi4 *fl4,\n\t\t\t    struct sk_buff_head *queue,\n\t\t\t    struct inet_cork *cork,\n\t\t\t    struct page_frag *pfrag,\n\t\t\t    int getfrag(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t    unsigned int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\n\tstruct ip_options *opt = cork->opt;\n\tint hh_len;\n\tint exthdrlen;\n\tint mtu;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\tunsigned int maxfraglen, fragheaderlen, maxnonfragsize;\n\tint csummode = CHECKSUM_NONE;\n\tstruct rtable *rt = (struct rtable *)cork->dst;\n\tu32 tskey = 0;\n\n\tskb = skb_peek_tail(queue);\n\n\texthdrlen = !skb ? rt->dst.header_len : 0;\n\tmtu = cork->fragsize;\n\tif (cork->tx_flags & SKBTX_ANY_SW_TSTAMP &&\n\t    sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)\n\t\ttskey = sk->sk_tskey++;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\tmaxnonfragsize = ip_sk_ignore_df(sk) ? 0xFFFF : mtu;\n\n\tif (cork->length + length > maxnonfragsize - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, fl4->daddr, inet->inet_dport,\n\t\t\t       mtu - (opt ? opt->optlen : 0));\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/*\n\t * transhdrlen > 0 means that this is the first fragment and we wish\n\t * it won't be fragmented in the future.\n\t */\n\tif (transhdrlen &&\n\t    length + fragheaderlen <= mtu &&\n\t    rt->dst.dev->features & (NETIF_F_HW_CSUM | NETIF_F_IP_CSUM) &&\n\t    !(flags & MSG_MORE) &&\n\t    !exthdrlen)\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tcork->length += length;\n\tif ((skb && skb_is_gso(skb)) ||\n\t    (((length + (skb ? skb->len : fragheaderlen)) > mtu) &&\n\t    (skb_queue_len(queue) <= 1) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO) && !dst_xfrm(&rt->dst) &&\n\t    (sk->sk_type == SOCK_DGRAM) && !sk->sk_no_check_tx)) {\n\t\terr = ip_ufo_append_data(sk, queue, getfrag, from, length,\n\t\t\t\t\t hh_len, fragheaderlen, transhdrlen,\n\t\t\t\t\t maxfraglen, flags);\n\t\tif (err)\n\t\t\tgoto error;\n\t\treturn 0;\n\t}\n\n\t/* So, what's going on in the loop below?\n\t *\n\t * We use calculated fragment length to generate chained skb,\n\t * each of segments is IP fragment ready for sending to network after\n\t * adding appropriate IP header.\n\t */\n\n\tif (!skb)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = mtu - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\n\t\t\tstruct sk_buff *skb_prev;\nalloc_new_skb:\n\t\t\tskb_prev = skb;\n\t\t\tif (skb_prev)\n\t\t\t\tfraggap = skb_prev->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\t\t\tif (datalen > mtu - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen;\n\t\t\tfraglen = datalen + fragheaderlen;\n\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse\n\t\t\t\talloclen = fraglen;\n\n\t\t\talloclen += exthdrlen;\n\n\t\t\t/* The last fragment gets additional space at tail.\n\t\t\t * Note, with MSG_MORE we overallocate on fragments,\n\t\t\t * because we have no idea what fragment will be\n\t\t\t * the last.\n\t\t\t */\n\t\t\tif (datalen == length + fraggap)\n\t\t\t\talloclen += rt->dst.trailer_len;\n\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len + 15,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (refcount_read(&sk->sk_wmem_alloc) <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = sock_wmalloc(sk,\n\t\t\t\t\t\t\t   alloclen + hh_len + 15, 1,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\t\tif (unlikely(!skb))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t}\n\t\t\tif (!skb)\n\t\t\t\tgoto error;\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\n\t\t\t/* only the initial fragment is time stamped */\n\t\t\tskb_shinfo(skb)->tx_flags = cork->tx_flags;\n\t\t\tcork->tx_flags = 0;\n\t\t\tskb_shinfo(skb)->tskey = tskey;\n\t\t\ttskey = 0;\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen + exthdrlen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tdata += fragheaderlen + exthdrlen;\n\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\tcopy = datalen - transhdrlen - fraggap;\n\t\t\tif (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= datalen - fraggap;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tcsummode = CHECKSUM_NONE;\n\n\t\t\tif ((flags & MSG_CONFIRM) && !skb_prev)\n\t\t\t\tskb_set_dst_pending_confirm(skb, 1);\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\t__skb_queue_tail(queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG)) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\n\t\t\terr = -ENOMEM;\n\t\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\t\tgoto error;\n\n\t\t\tif (!skb_can_coalesce(skb, i, pfrag->page,\n\t\t\t\t\t      pfrag->offset)) {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tif (i == MAX_SKB_FRAGS)\n\t\t\t\t\tgoto error;\n\n\t\t\t\t__skb_fill_page_desc(skb, i, pfrag->page,\n\t\t\t\t\t\t     pfrag->offset, 0);\n\t\t\t\tskb_shinfo(skb)->nr_frags = ++i;\n\t\t\t\tget_page(pfrag->page);\n\t\t\t}\n\t\t\tcopy = min_t(int, copy, pfrag->size - pfrag->offset);\n\t\t\tif (getfrag(from,\n\t\t\t\t    page_address(pfrag->page) + pfrag->offset,\n\t\t\t\t    offset, copy, skb->len, skb) < 0)\n\t\t\t\tgoto error_efault;\n\n\t\t\tpfrag->offset += copy;\n\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\trefcount_add(copy, &sk->sk_wmem_alloc);\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\n\treturn 0;\n\nerror_efault:\n\terr = -EFAULT;\nerror:\n\tcork->length -= length;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -55,11 +55,12 @@\n \t\tcsummode = CHECKSUM_PARTIAL;\n \n \tcork->length += length;\n-\tif ((((length + (skb ? skb->len : fragheaderlen)) > mtu) ||\n-\t     (skb && skb_is_gso(skb))) &&\n+\tif ((skb && skb_is_gso(skb)) ||\n+\t    (((length + (skb ? skb->len : fragheaderlen)) > mtu) &&\n+\t    (skb_queue_len(queue) <= 1) &&\n \t    (sk->sk_protocol == IPPROTO_UDP) &&\n \t    (rt->dst.dev->features & NETIF_F_UFO) && !dst_xfrm(&rt->dst) &&\n-\t    (sk->sk_type == SOCK_DGRAM) && !sk->sk_no_check_tx) {\n+\t    (sk->sk_type == SOCK_DGRAM) && !sk->sk_no_check_tx)) {\n \t\terr = ip_ufo_append_data(sk, queue, getfrag, from, length,\n \t\t\t\t\t hh_len, fragheaderlen, transhdrlen,\n \t\t\t\t\t maxfraglen, flags);",
        "function_modified_lines": {
            "added": [
                "\tif ((skb && skb_is_gso(skb)) ||",
                "\t    (((length + (skb ? skb->len : fragheaderlen)) > mtu) &&",
                "\t    (skb_queue_len(queue) <= 1) &&",
                "\t    (sk->sk_type == SOCK_DGRAM) && !sk->sk_no_check_tx)) {"
            ],
            "deleted": [
                "\tif ((((length + (skb ? skb->len : fragheaderlen)) > mtu) ||",
                "\t     (skb && skb_is_gso(skb))) &&",
                "\t    (sk->sk_type == SOCK_DGRAM) && !sk->sk_no_check_tx) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux kernel: Exploitable memory corruption due to UFO to non-UFO path switch. When building a UFO packet with MSG_MORE __ip_append_data() calls ip_ufo_append_data() to append. However in between two send() calls, the append path can be switched from UFO to non-UFO one, which leads to a memory corruption. In case UFO packet lengths exceeds MTU, copy = maxfraglen - skb->len becomes negative on the non-UFO path and the branch to allocate new skb is taken. This triggers fragmentation and computation of fraggap = skb_prev->len - maxfraglen. Fraggap can exceed MTU, causing copy = datalen - transhdrlen - fraggap to become negative. Subsequently skb_copy_and_csum_bits() writes out-of-bounds. A similar issue is present in IPv6 code. The bug was introduced in e89e9cf539a2 (\"[IPv4/IPv6]: UFO Scatter-gather approach\") on Oct 18 2005."
    },
    {
        "cve_id": "CVE-2017-1000112",
        "code_before_change": "ssize_t\tip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,\n\t\t       int offset, size_t size, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct rtable *rt;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_cork *cork;\n\tint hh_len;\n\tint mtu;\n\tint len;\n\tint err;\n\tunsigned int maxfraglen, fragheaderlen, fraggap, maxnonfragsize;\n\n\tif (inet->hdrincl)\n\t\treturn -EPERM;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\n\tif (skb_queue_empty(&sk->sk_write_queue))\n\t\treturn -EINVAL;\n\n\tcork = &inet->cork.base;\n\trt = (struct rtable *)cork->dst;\n\tif (cork->flags & IPCORK_OPT)\n\t\topt = cork->opt;\n\n\tif (!(rt->dst.dev->features&NETIF_F_SG))\n\t\treturn -EOPNOTSUPP;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\tmtu = cork->fragsize;\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\tmaxnonfragsize = ip_sk_ignore_df(sk) ? 0xFFFF : mtu;\n\n\tif (cork->length + size > maxnonfragsize - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, fl4->daddr, inet->inet_dport,\n\t\t\t       mtu - (opt ? opt->optlen : 0));\n\t\treturn -EMSGSIZE;\n\t}\n\n\tskb = skb_peek_tail(&sk->sk_write_queue);\n\tif (!skb)\n\t\treturn -EINVAL;\n\n\tif ((size + skb->len > mtu) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO)) {\n\t\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tskb_shinfo(skb)->gso_size = mtu - fragheaderlen;\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\t}\n\tcork->length += size;\n\n\twhile (size > 0) {\n\t\tif (skb_is_gso(skb)) {\n\t\t\tlen = size;\n\t\t} else {\n\n\t\t\t/* Check if the remaining data fits into current packet. */\n\t\t\tlen = mtu - skb->len;\n\t\t\tif (len < size)\n\t\t\t\tlen = maxfraglen - skb->len;\n\t\t}\n\t\tif (len <= 0) {\n\t\t\tstruct sk_buff *skb_prev;\n\t\t\tint alloclen;\n\n\t\t\tskb_prev = skb;\n\t\t\tfraggap = skb_prev->len - maxfraglen;\n\n\t\t\talloclen = fragheaderlen + hh_len + fraggap + 15;\n\t\t\tskb = sock_wmalloc(sk, alloclen, 1, sk->sk_allocation);\n\t\t\tif (unlikely(!skb)) {\n\t\t\t\terr = -ENOBUFS;\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tskb_put(skb, fragheaderlen + fraggap);\n\t\t\tskb_reset_network_header(skb);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(skb_prev,\n\t\t\t\t\t\t\t\t   maxfraglen,\n\t\t\t\t\t\t    skb_transport_header(skb),\n\t\t\t\t\t\t\t\t   fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (len > size)\n\t\t\tlen = size;\n\n\t\tif (skb_append_pagefrags(skb, page, offset, len)) {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t\t__wsum csum;\n\t\t\tcsum = csum_page(page, offset, len);\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, skb->len);\n\t\t}\n\n\t\tskb->len += len;\n\t\tskb->data_len += len;\n\t\tskb->truesize += len;\n\t\trefcount_add(len, &sk->sk_wmem_alloc);\n\t\toffset += len;\n\t\tsize -= len;\n\t}\n\treturn 0;\n\nerror:\n\tcork->length -= size;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}",
        "code_after_change": "ssize_t\tip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,\n\t\t       int offset, size_t size, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct rtable *rt;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_cork *cork;\n\tint hh_len;\n\tint mtu;\n\tint len;\n\tint err;\n\tunsigned int maxfraglen, fragheaderlen, fraggap, maxnonfragsize;\n\n\tif (inet->hdrincl)\n\t\treturn -EPERM;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\n\tif (skb_queue_empty(&sk->sk_write_queue))\n\t\treturn -EINVAL;\n\n\tcork = &inet->cork.base;\n\trt = (struct rtable *)cork->dst;\n\tif (cork->flags & IPCORK_OPT)\n\t\topt = cork->opt;\n\n\tif (!(rt->dst.dev->features&NETIF_F_SG))\n\t\treturn -EOPNOTSUPP;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\tmtu = cork->fragsize;\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\tmaxnonfragsize = ip_sk_ignore_df(sk) ? 0xFFFF : mtu;\n\n\tif (cork->length + size > maxnonfragsize - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, fl4->daddr, inet->inet_dport,\n\t\t\t       mtu - (opt ? opt->optlen : 0));\n\t\treturn -EMSGSIZE;\n\t}\n\n\tskb = skb_peek_tail(&sk->sk_write_queue);\n\tif (!skb)\n\t\treturn -EINVAL;\n\n\tif ((size + skb->len > mtu) &&\n\t    (skb_queue_len(&sk->sk_write_queue) == 1) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO)) {\n\t\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tskb_shinfo(skb)->gso_size = mtu - fragheaderlen;\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\t}\n\tcork->length += size;\n\n\twhile (size > 0) {\n\t\tif (skb_is_gso(skb)) {\n\t\t\tlen = size;\n\t\t} else {\n\n\t\t\t/* Check if the remaining data fits into current packet. */\n\t\t\tlen = mtu - skb->len;\n\t\t\tif (len < size)\n\t\t\t\tlen = maxfraglen - skb->len;\n\t\t}\n\t\tif (len <= 0) {\n\t\t\tstruct sk_buff *skb_prev;\n\t\t\tint alloclen;\n\n\t\t\tskb_prev = skb;\n\t\t\tfraggap = skb_prev->len - maxfraglen;\n\n\t\t\talloclen = fragheaderlen + hh_len + fraggap + 15;\n\t\t\tskb = sock_wmalloc(sk, alloclen, 1, sk->sk_allocation);\n\t\t\tif (unlikely(!skb)) {\n\t\t\t\terr = -ENOBUFS;\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tskb_put(skb, fragheaderlen + fraggap);\n\t\t\tskb_reset_network_header(skb);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(skb_prev,\n\t\t\t\t\t\t\t\t   maxfraglen,\n\t\t\t\t\t\t    skb_transport_header(skb),\n\t\t\t\t\t\t\t\t   fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (len > size)\n\t\t\tlen = size;\n\n\t\tif (skb_append_pagefrags(skb, page, offset, len)) {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t\t__wsum csum;\n\t\t\tcsum = csum_page(page, offset, len);\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, skb->len);\n\t\t}\n\n\t\tskb->len += len;\n\t\tskb->data_len += len;\n\t\tskb->truesize += len;\n\t\trefcount_add(len, &sk->sk_wmem_alloc);\n\t\toffset += len;\n\t\tsize -= len;\n\t}\n\treturn 0;\n\nerror:\n\tcork->length -= size;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,6 +47,7 @@\n \t\treturn -EINVAL;\n \n \tif ((size + skb->len > mtu) &&\n+\t    (skb_queue_len(&sk->sk_write_queue) == 1) &&\n \t    (sk->sk_protocol == IPPROTO_UDP) &&\n \t    (rt->dst.dev->features & NETIF_F_UFO)) {\n \t\tif (skb->ip_summed != CHECKSUM_PARTIAL)",
        "function_modified_lines": {
            "added": [
                "\t    (skb_queue_len(&sk->sk_write_queue) == 1) &&"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux kernel: Exploitable memory corruption due to UFO to non-UFO path switch. When building a UFO packet with MSG_MORE __ip_append_data() calls ip_ufo_append_data() to append. However in between two send() calls, the append path can be switched from UFO to non-UFO one, which leads to a memory corruption. In case UFO packet lengths exceeds MTU, copy = maxfraglen - skb->len becomes negative on the non-UFO path and the branch to allocate new skb is taken. This triggers fragmentation and computation of fraggap = skb_prev->len - maxfraglen. Fraggap can exceed MTU, causing copy = datalen - transhdrlen - fraggap to become negative. Subsequently skb_copy_and_csum_bits() writes out-of-bounds. A similar issue is present in IPv6 code. The bug was introduced in e89e9cf539a2 (\"[IPv4/IPv6]: UFO Scatter-gather approach\") on Oct 18 2005."
    },
    {
        "cve_id": "CVE-2017-1000112",
        "code_before_change": "static int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udphdr *uh;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\t__wsum csum = 0;\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = inet->inet_sport;\n\tuh->dest = fl4->fl4_dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\tif (is_udplite)  \t\t\t\t /*     UDP-Lite      */\n\t\tcsum = udplite_csum(skb);\n\n\telse if (sk->sk_no_check_tx) {   /* UDP csum disabled */\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tgoto send;\n\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\n\n\t\tudp4_hwcsum(skb, fl4->saddr, fl4->daddr);\n\t\tgoto send;\n\n\t} else\n\t\tcsum = udp_csum(skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_tcpudp_magic(fl4->saddr, fl4->daddr, len,\n\t\t\t\t      sk->sk_protocol, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip_send_skb(sock_net(sk), skb);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet->recverr) {\n\t\t\tUDP_INC_STATS(sock_net(sk),\n\t\t\t\t      UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP_INC_STATS(sock_net(sk),\n\t\t\t      UDP_MIB_OUTDATAGRAMS, is_udplite);\n\treturn err;\n}",
        "code_after_change": "static int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udphdr *uh;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\t__wsum csum = 0;\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = inet->inet_sport;\n\tuh->dest = fl4->fl4_dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\tif (is_udplite)  \t\t\t\t /*     UDP-Lite      */\n\t\tcsum = udplite_csum(skb);\n\n\telse if (sk->sk_no_check_tx && !skb_is_gso(skb)) {   /* UDP csum off */\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tgoto send;\n\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\n\n\t\tudp4_hwcsum(skb, fl4->saddr, fl4->daddr);\n\t\tgoto send;\n\n\t} else\n\t\tcsum = udp_csum(skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_tcpudp_magic(fl4->saddr, fl4->daddr, len,\n\t\t\t\t      sk->sk_protocol, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip_send_skb(sock_net(sk), skb);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet->recverr) {\n\t\t\tUDP_INC_STATS(sock_net(sk),\n\t\t\t\t      UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP_INC_STATS(sock_net(sk),\n\t\t\t      UDP_MIB_OUTDATAGRAMS, is_udplite);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,7 +21,7 @@\n \tif (is_udplite)  \t\t\t\t /*     UDP-Lite      */\n \t\tcsum = udplite_csum(skb);\n \n-\telse if (sk->sk_no_check_tx) {   /* UDP csum disabled */\n+\telse if (sk->sk_no_check_tx && !skb_is_gso(skb)) {   /* UDP csum off */\n \n \t\tskb->ip_summed = CHECKSUM_NONE;\n \t\tgoto send;",
        "function_modified_lines": {
            "added": [
                "\telse if (sk->sk_no_check_tx && !skb_is_gso(skb)) {   /* UDP csum off */"
            ],
            "deleted": [
                "\telse if (sk->sk_no_check_tx) {   /* UDP csum disabled */"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux kernel: Exploitable memory corruption due to UFO to non-UFO path switch. When building a UFO packet with MSG_MORE __ip_append_data() calls ip_ufo_append_data() to append. However in between two send() calls, the append path can be switched from UFO to non-UFO one, which leads to a memory corruption. In case UFO packet lengths exceeds MTU, copy = maxfraglen - skb->len becomes negative on the non-UFO path and the branch to allocate new skb is taken. This triggers fragmentation and computation of fraggap = skb_prev->len - maxfraglen. Fraggap can exceed MTU, causing copy = datalen - transhdrlen - fraggap to become negative. Subsequently skb_copy_and_csum_bits() writes out-of-bounds. A similar issue is present in IPv6 code. The bug was introduced in e89e9cf539a2 (\"[IPv4/IPv6]: UFO Scatter-gather approach\") on Oct 18 2005."
    },
    {
        "cve_id": "CVE-2017-1000112",
        "code_before_change": "static int __ip6_append_data(struct sock *sk,\n\t\t\t     struct flowi6 *fl6,\n\t\t\t     struct sk_buff_head *queue,\n\t\t\t     struct inet_cork *cork,\n\t\t\t     struct inet6_cork *v6_cork,\n\t\t\t     struct page_frag *pfrag,\n\t\t\t     int getfrag(void *from, char *to, int offset,\n\t\t\t\t\t int len, int odd, struct sk_buff *skb),\n\t\t\t     void *from, int length, int transhdrlen,\n\t\t\t     unsigned int flags, struct ipcm6_cookie *ipc6,\n\t\t\t     const struct sockcm_cookie *sockc)\n{\n\tstruct sk_buff *skb, *skb_prev = NULL;\n\tunsigned int maxfraglen, fragheaderlen, mtu, orig_mtu;\n\tint exthdrlen = 0;\n\tint dst_exthdrlen = 0;\n\tint hh_len;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\t__u8 tx_flags = 0;\n\tu32 tskey = 0;\n\tstruct rt6_info *rt = (struct rt6_info *)cork->dst;\n\tstruct ipv6_txoptions *opt = v6_cork->opt;\n\tint csummode = CHECKSUM_NONE;\n\tunsigned int maxnonfragsize, headersize;\n\n\tskb = skb_peek_tail(queue);\n\tif (!skb) {\n\t\texthdrlen = opt ? opt->opt_flen : 0;\n\t\tdst_exthdrlen = rt->dst.header_len - rt->rt6i_nfheader_len;\n\t}\n\n\tmtu = cork->fragsize;\n\torig_mtu = mtu;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct ipv6hdr) + rt->rt6i_nfheader_len +\n\t\t\t(opt ? opt->opt_nflen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen -\n\t\t     sizeof(struct frag_hdr);\n\n\theadersize = sizeof(struct ipv6hdr) +\n\t\t     (opt ? opt->opt_flen + opt->opt_nflen : 0) +\n\t\t     (dst_allfrag(&rt->dst) ?\n\t\t      sizeof(struct frag_hdr) : 0) +\n\t\t     rt->rt6i_nfheader_len;\n\n\tif (cork->length + length > mtu - headersize && ipc6->dontfrag &&\n\t    (sk->sk_protocol == IPPROTO_UDP ||\n\t     sk->sk_protocol == IPPROTO_RAW)) {\n\t\tipv6_local_rxpmtu(sk, fl6, mtu - headersize +\n\t\t\t\tsizeof(struct ipv6hdr));\n\t\tgoto emsgsize;\n\t}\n\n\tif (ip6_sk_ignore_df(sk))\n\t\tmaxnonfragsize = sizeof(struct ipv6hdr) + IPV6_MAXPLEN;\n\telse\n\t\tmaxnonfragsize = mtu;\n\n\tif (cork->length + length > maxnonfragsize - headersize) {\nemsgsize:\n\t\tipv6_local_error(sk, EMSGSIZE, fl6,\n\t\t\t\t mtu - headersize +\n\t\t\t\t sizeof(struct ipv6hdr));\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/* CHECKSUM_PARTIAL only with no extension headers and when\n\t * we are not going to fragment\n\t */\n\tif (transhdrlen && sk->sk_protocol == IPPROTO_UDP &&\n\t    headersize == sizeof(struct ipv6hdr) &&\n\t    length <= mtu - headersize &&\n\t    !(flags & MSG_MORE) &&\n\t    rt->dst.dev->features & (NETIF_F_IPV6_CSUM | NETIF_F_HW_CSUM))\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tif (sk->sk_type == SOCK_DGRAM || sk->sk_type == SOCK_RAW) {\n\t\tsock_tx_timestamp(sk, sockc->tsflags, &tx_flags);\n\t\tif (tx_flags & SKBTX_ANY_SW_TSTAMP &&\n\t\t    sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)\n\t\t\ttskey = sk->sk_tskey++;\n\t}\n\n\t/*\n\t * Let's try using as much space as possible.\n\t * Use MTU if total length of the message fits into the MTU.\n\t * Otherwise, we need to reserve fragment header and\n\t * fragment alignment (= 8-15 octects, in total).\n\t *\n\t * Note that we may need to \"move\" the data from the tail of\n\t * of the buffer to the new fragment when we split\n\t * the message.\n\t *\n\t * FIXME: It may be fragmented into multiple chunks\n\t *        at once if non-fragmentable extension headers\n\t *        are too large.\n\t * --yoshfuji\n\t */\n\n\tcork->length += length;\n\tif ((((length + (skb ? skb->len : headersize)) > mtu) ||\n\t     (skb && skb_is_gso(skb))) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO) && !dst_xfrm(&rt->dst) &&\n\t    (sk->sk_type == SOCK_DGRAM) && !udp_get_no_check6_tx(sk)) {\n\t\terr = ip6_ufo_append_data(sk, queue, getfrag, from, length,\n\t\t\t\t\t  hh_len, fragheaderlen, exthdrlen,\n\t\t\t\t\t  transhdrlen, mtu, flags, fl6);\n\t\tif (err)\n\t\t\tgoto error;\n\t\treturn 0;\n\t}\n\n\tif (!skb)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\nalloc_new_skb:\n\t\t\t/* There's no room in the current skb */\n\t\t\tif (skb)\n\t\t\t\tfraggap = skb->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\t\t\t/* update mtu and maxfraglen if necessary */\n\t\t\tif (!skb || !skb_prev)\n\t\t\t\tip6_append_data_mtu(&mtu, &maxfraglen,\n\t\t\t\t\t\t    fragheaderlen, skb, rt,\n\t\t\t\t\t\t    orig_mtu);\n\n\t\t\tskb_prev = skb;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\n\t\t\tif (datalen > (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen - rt->dst.trailer_len;\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse\n\t\t\t\talloclen = datalen + fragheaderlen;\n\n\t\t\talloclen += dst_exthdrlen;\n\n\t\t\tif (datalen != length + fraggap) {\n\t\t\t\t/*\n\t\t\t\t * this is not the last fragment, the trailer\n\t\t\t\t * space is regarded as data space.\n\t\t\t\t */\n\t\t\t\tdatalen += rt->dst.trailer_len;\n\t\t\t}\n\n\t\t\talloclen += rt->dst.trailer_len;\n\t\t\tfraglen = datalen + fragheaderlen;\n\n\t\t\t/*\n\t\t\t * We just reserve space for fragment header.\n\t\t\t * Note: this may be overallocation if the message\n\t\t\t * (without MSG_MORE) fits into the MTU.\n\t\t\t */\n\t\t\talloclen += sizeof(struct frag_hdr);\n\n\t\t\tcopy = datalen - transhdrlen - fraggap;\n\t\t\tif (copy < 0) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (refcount_read(&sk->sk_wmem_alloc) <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = sock_wmalloc(sk,\n\t\t\t\t\t\t\t   alloclen + hh_len, 1,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\t\tif (unlikely(!skb))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t}\n\t\t\tif (!skb)\n\t\t\t\tgoto error;\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->protocol = htons(ETH_P_IPV6);\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\t/* reserve for fragmentation and ipsec header */\n\t\t\tskb_reserve(skb, hh_len + sizeof(struct frag_hdr) +\n\t\t\t\t    dst_exthdrlen);\n\n\t\t\t/* Only the initial fragment is time stamped */\n\t\t\tskb_shinfo(skb)->tx_flags = tx_flags;\n\t\t\ttx_flags = 0;\n\t\t\tskb_shinfo(skb)->tskey = tskey;\n\t\t\ttskey = 0;\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tdata += fragheaderlen;\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\t\t\tif (copy > 0 &&\n\t\t\t    getfrag(from, data + transhdrlen, offset,\n\t\t\t\t    copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= datalen - fraggap;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tdst_exthdrlen = 0;\n\n\t\t\tif ((flags & MSG_CONFIRM) && !skb_prev)\n\t\t\t\tskb_set_dst_pending_confirm(skb, 1);\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue\n\t\t\t */\n\t\t\t__skb_queue_tail(queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG)) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\n\t\t\terr = -ENOMEM;\n\t\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\t\tgoto error;\n\n\t\t\tif (!skb_can_coalesce(skb, i, pfrag->page,\n\t\t\t\t\t      pfrag->offset)) {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tif (i == MAX_SKB_FRAGS)\n\t\t\t\t\tgoto error;\n\n\t\t\t\t__skb_fill_page_desc(skb, i, pfrag->page,\n\t\t\t\t\t\t     pfrag->offset, 0);\n\t\t\t\tskb_shinfo(skb)->nr_frags = ++i;\n\t\t\t\tget_page(pfrag->page);\n\t\t\t}\n\t\t\tcopy = min_t(int, copy, pfrag->size - pfrag->offset);\n\t\t\tif (getfrag(from,\n\t\t\t\t    page_address(pfrag->page) + pfrag->offset,\n\t\t\t\t    offset, copy, skb->len, skb) < 0)\n\t\t\t\tgoto error_efault;\n\n\t\t\tpfrag->offset += copy;\n\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\trefcount_add(copy, &sk->sk_wmem_alloc);\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\n\treturn 0;\n\nerror_efault:\n\terr = -EFAULT;\nerror:\n\tcork->length -= length;\n\tIP6_INC_STATS(sock_net(sk), rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}",
        "code_after_change": "static int __ip6_append_data(struct sock *sk,\n\t\t\t     struct flowi6 *fl6,\n\t\t\t     struct sk_buff_head *queue,\n\t\t\t     struct inet_cork *cork,\n\t\t\t     struct inet6_cork *v6_cork,\n\t\t\t     struct page_frag *pfrag,\n\t\t\t     int getfrag(void *from, char *to, int offset,\n\t\t\t\t\t int len, int odd, struct sk_buff *skb),\n\t\t\t     void *from, int length, int transhdrlen,\n\t\t\t     unsigned int flags, struct ipcm6_cookie *ipc6,\n\t\t\t     const struct sockcm_cookie *sockc)\n{\n\tstruct sk_buff *skb, *skb_prev = NULL;\n\tunsigned int maxfraglen, fragheaderlen, mtu, orig_mtu;\n\tint exthdrlen = 0;\n\tint dst_exthdrlen = 0;\n\tint hh_len;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\t__u8 tx_flags = 0;\n\tu32 tskey = 0;\n\tstruct rt6_info *rt = (struct rt6_info *)cork->dst;\n\tstruct ipv6_txoptions *opt = v6_cork->opt;\n\tint csummode = CHECKSUM_NONE;\n\tunsigned int maxnonfragsize, headersize;\n\n\tskb = skb_peek_tail(queue);\n\tif (!skb) {\n\t\texthdrlen = opt ? opt->opt_flen : 0;\n\t\tdst_exthdrlen = rt->dst.header_len - rt->rt6i_nfheader_len;\n\t}\n\n\tmtu = cork->fragsize;\n\torig_mtu = mtu;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct ipv6hdr) + rt->rt6i_nfheader_len +\n\t\t\t(opt ? opt->opt_nflen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen -\n\t\t     sizeof(struct frag_hdr);\n\n\theadersize = sizeof(struct ipv6hdr) +\n\t\t     (opt ? opt->opt_flen + opt->opt_nflen : 0) +\n\t\t     (dst_allfrag(&rt->dst) ?\n\t\t      sizeof(struct frag_hdr) : 0) +\n\t\t     rt->rt6i_nfheader_len;\n\n\tif (cork->length + length > mtu - headersize && ipc6->dontfrag &&\n\t    (sk->sk_protocol == IPPROTO_UDP ||\n\t     sk->sk_protocol == IPPROTO_RAW)) {\n\t\tipv6_local_rxpmtu(sk, fl6, mtu - headersize +\n\t\t\t\tsizeof(struct ipv6hdr));\n\t\tgoto emsgsize;\n\t}\n\n\tif (ip6_sk_ignore_df(sk))\n\t\tmaxnonfragsize = sizeof(struct ipv6hdr) + IPV6_MAXPLEN;\n\telse\n\t\tmaxnonfragsize = mtu;\n\n\tif (cork->length + length > maxnonfragsize - headersize) {\nemsgsize:\n\t\tipv6_local_error(sk, EMSGSIZE, fl6,\n\t\t\t\t mtu - headersize +\n\t\t\t\t sizeof(struct ipv6hdr));\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/* CHECKSUM_PARTIAL only with no extension headers and when\n\t * we are not going to fragment\n\t */\n\tif (transhdrlen && sk->sk_protocol == IPPROTO_UDP &&\n\t    headersize == sizeof(struct ipv6hdr) &&\n\t    length <= mtu - headersize &&\n\t    !(flags & MSG_MORE) &&\n\t    rt->dst.dev->features & (NETIF_F_IPV6_CSUM | NETIF_F_HW_CSUM))\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tif (sk->sk_type == SOCK_DGRAM || sk->sk_type == SOCK_RAW) {\n\t\tsock_tx_timestamp(sk, sockc->tsflags, &tx_flags);\n\t\tif (tx_flags & SKBTX_ANY_SW_TSTAMP &&\n\t\t    sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)\n\t\t\ttskey = sk->sk_tskey++;\n\t}\n\n\t/*\n\t * Let's try using as much space as possible.\n\t * Use MTU if total length of the message fits into the MTU.\n\t * Otherwise, we need to reserve fragment header and\n\t * fragment alignment (= 8-15 octects, in total).\n\t *\n\t * Note that we may need to \"move\" the data from the tail of\n\t * of the buffer to the new fragment when we split\n\t * the message.\n\t *\n\t * FIXME: It may be fragmented into multiple chunks\n\t *        at once if non-fragmentable extension headers\n\t *        are too large.\n\t * --yoshfuji\n\t */\n\n\tcork->length += length;\n\tif ((skb && skb_is_gso(skb)) ||\n\t    (((length + (skb ? skb->len : headersize)) > mtu) &&\n\t    (skb_queue_len(queue) <= 1) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO) && !dst_xfrm(&rt->dst) &&\n\t    (sk->sk_type == SOCK_DGRAM) && !udp_get_no_check6_tx(sk))) {\n\t\terr = ip6_ufo_append_data(sk, queue, getfrag, from, length,\n\t\t\t\t\t  hh_len, fragheaderlen, exthdrlen,\n\t\t\t\t\t  transhdrlen, mtu, flags, fl6);\n\t\tif (err)\n\t\t\tgoto error;\n\t\treturn 0;\n\t}\n\n\tif (!skb)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\nalloc_new_skb:\n\t\t\t/* There's no room in the current skb */\n\t\t\tif (skb)\n\t\t\t\tfraggap = skb->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\t\t\t/* update mtu and maxfraglen if necessary */\n\t\t\tif (!skb || !skb_prev)\n\t\t\t\tip6_append_data_mtu(&mtu, &maxfraglen,\n\t\t\t\t\t\t    fragheaderlen, skb, rt,\n\t\t\t\t\t\t    orig_mtu);\n\n\t\t\tskb_prev = skb;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\n\t\t\tif (datalen > (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen - rt->dst.trailer_len;\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse\n\t\t\t\talloclen = datalen + fragheaderlen;\n\n\t\t\talloclen += dst_exthdrlen;\n\n\t\t\tif (datalen != length + fraggap) {\n\t\t\t\t/*\n\t\t\t\t * this is not the last fragment, the trailer\n\t\t\t\t * space is regarded as data space.\n\t\t\t\t */\n\t\t\t\tdatalen += rt->dst.trailer_len;\n\t\t\t}\n\n\t\t\talloclen += rt->dst.trailer_len;\n\t\t\tfraglen = datalen + fragheaderlen;\n\n\t\t\t/*\n\t\t\t * We just reserve space for fragment header.\n\t\t\t * Note: this may be overallocation if the message\n\t\t\t * (without MSG_MORE) fits into the MTU.\n\t\t\t */\n\t\t\talloclen += sizeof(struct frag_hdr);\n\n\t\t\tcopy = datalen - transhdrlen - fraggap;\n\t\t\tif (copy < 0) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (refcount_read(&sk->sk_wmem_alloc) <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = sock_wmalloc(sk,\n\t\t\t\t\t\t\t   alloclen + hh_len, 1,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\t\tif (unlikely(!skb))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t}\n\t\t\tif (!skb)\n\t\t\t\tgoto error;\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->protocol = htons(ETH_P_IPV6);\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\t/* reserve for fragmentation and ipsec header */\n\t\t\tskb_reserve(skb, hh_len + sizeof(struct frag_hdr) +\n\t\t\t\t    dst_exthdrlen);\n\n\t\t\t/* Only the initial fragment is time stamped */\n\t\t\tskb_shinfo(skb)->tx_flags = tx_flags;\n\t\t\ttx_flags = 0;\n\t\t\tskb_shinfo(skb)->tskey = tskey;\n\t\t\ttskey = 0;\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tdata += fragheaderlen;\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\t\t\tif (copy > 0 &&\n\t\t\t    getfrag(from, data + transhdrlen, offset,\n\t\t\t\t    copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= datalen - fraggap;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tdst_exthdrlen = 0;\n\n\t\t\tif ((flags & MSG_CONFIRM) && !skb_prev)\n\t\t\t\tskb_set_dst_pending_confirm(skb, 1);\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue\n\t\t\t */\n\t\t\t__skb_queue_tail(queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG)) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\n\t\t\terr = -ENOMEM;\n\t\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\t\tgoto error;\n\n\t\t\tif (!skb_can_coalesce(skb, i, pfrag->page,\n\t\t\t\t\t      pfrag->offset)) {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tif (i == MAX_SKB_FRAGS)\n\t\t\t\t\tgoto error;\n\n\t\t\t\t__skb_fill_page_desc(skb, i, pfrag->page,\n\t\t\t\t\t\t     pfrag->offset, 0);\n\t\t\t\tskb_shinfo(skb)->nr_frags = ++i;\n\t\t\t\tget_page(pfrag->page);\n\t\t\t}\n\t\t\tcopy = min_t(int, copy, pfrag->size - pfrag->offset);\n\t\t\tif (getfrag(from,\n\t\t\t\t    page_address(pfrag->page) + pfrag->offset,\n\t\t\t\t    offset, copy, skb->len, skb) < 0)\n\t\t\t\tgoto error_efault;\n\n\t\t\tpfrag->offset += copy;\n\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\trefcount_add(copy, &sk->sk_wmem_alloc);\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\n\treturn 0;\n\nerror_efault:\n\terr = -EFAULT;\nerror:\n\tcork->length -= length;\n\tIP6_INC_STATS(sock_net(sk), rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -102,11 +102,12 @@\n \t */\n \n \tcork->length += length;\n-\tif ((((length + (skb ? skb->len : headersize)) > mtu) ||\n-\t     (skb && skb_is_gso(skb))) &&\n+\tif ((skb && skb_is_gso(skb)) ||\n+\t    (((length + (skb ? skb->len : headersize)) > mtu) &&\n+\t    (skb_queue_len(queue) <= 1) &&\n \t    (sk->sk_protocol == IPPROTO_UDP) &&\n \t    (rt->dst.dev->features & NETIF_F_UFO) && !dst_xfrm(&rt->dst) &&\n-\t    (sk->sk_type == SOCK_DGRAM) && !udp_get_no_check6_tx(sk)) {\n+\t    (sk->sk_type == SOCK_DGRAM) && !udp_get_no_check6_tx(sk))) {\n \t\terr = ip6_ufo_append_data(sk, queue, getfrag, from, length,\n \t\t\t\t\t  hh_len, fragheaderlen, exthdrlen,\n \t\t\t\t\t  transhdrlen, mtu, flags, fl6);",
        "function_modified_lines": {
            "added": [
                "\tif ((skb && skb_is_gso(skb)) ||",
                "\t    (((length + (skb ? skb->len : headersize)) > mtu) &&",
                "\t    (skb_queue_len(queue) <= 1) &&",
                "\t    (sk->sk_type == SOCK_DGRAM) && !udp_get_no_check6_tx(sk))) {"
            ],
            "deleted": [
                "\tif ((((length + (skb ? skb->len : headersize)) > mtu) ||",
                "\t     (skb && skb_is_gso(skb))) &&",
                "\t    (sk->sk_type == SOCK_DGRAM) && !udp_get_no_check6_tx(sk)) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux kernel: Exploitable memory corruption due to UFO to non-UFO path switch. When building a UFO packet with MSG_MORE __ip_append_data() calls ip_ufo_append_data() to append. However in between two send() calls, the append path can be switched from UFO to non-UFO one, which leads to a memory corruption. In case UFO packet lengths exceeds MTU, copy = maxfraglen - skb->len becomes negative on the non-UFO path and the branch to allocate new skb is taken. This triggers fragmentation and computation of fraggap = skb_prev->len - maxfraglen. Fraggap can exceed MTU, causing copy = datalen - transhdrlen - fraggap to become negative. Subsequently skb_copy_and_csum_bits() writes out-of-bounds. A similar issue is present in IPv6 code. The bug was introduced in e89e9cf539a2 (\"[IPv4/IPv6]: UFO Scatter-gather approach\") on Oct 18 2005."
    },
    {
        "cve_id": "CVE-2017-1000405",
        "code_before_change": "struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,\n\t\tpud_t *pud, int flags)\n{\n\tunsigned long pfn = pud_pfn(*pud);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap;\n\tstruct page *page;\n\n\tassert_spin_locked(pud_lockptr(mm, pud));\n\n\tif (flags & FOLL_WRITE && !pud_write(*pud))\n\t\treturn NULL;\n\n\tif (pud_present(*pud) && pud_devmap(*pud))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pud(vma, addr, pud);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & FOLL_GET))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PUD_MASK) >> PAGE_SHIFT;\n\tpgmap = get_dev_pagemap(pfn, NULL);\n\tif (!pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tget_page(page);\n\tput_dev_pagemap(pgmap);\n\n\treturn page;\n}",
        "code_after_change": "struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,\n\t\tpud_t *pud, int flags)\n{\n\tunsigned long pfn = pud_pfn(*pud);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap;\n\tstruct page *page;\n\n\tassert_spin_locked(pud_lockptr(mm, pud));\n\n\tif (flags & FOLL_WRITE && !pud_write(*pud))\n\t\treturn NULL;\n\n\tif (pud_present(*pud) && pud_devmap(*pud))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pud(vma, addr, pud, flags);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & FOLL_GET))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PUD_MASK) >> PAGE_SHIFT;\n\tpgmap = get_dev_pagemap(pfn, NULL);\n\tif (!pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tget_page(page);\n\tput_dev_pagemap(pgmap);\n\n\treturn page;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,7 +17,7 @@\n \t\treturn NULL;\n \n \tif (flags & FOLL_TOUCH)\n-\t\ttouch_pud(vma, addr, pud);\n+\t\ttouch_pud(vma, addr, pud, flags);\n \n \t/*\n \t * device mapped pages can only be returned if the",
        "function_modified_lines": {
            "added": [
                "\t\ttouch_pud(vma, addr, pud, flags);"
            ],
            "deleted": [
                "\t\ttouch_pud(vma, addr, pud);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The Linux Kernel versions 2.6.38 through 4.14 have a problematic use of pmd_mkdirty() in the touch_pmd() function inside the THP implementation. touch_pmd() can be reached by get_user_pages(). In such case, the pmd will become dirty. This scenario breaks the new can_follow_write_pmd()'s logic - pmd can become dirty without going through a COW cycle. This bug is not as severe as the original \"Dirty cow\" because an ext4 file (or any other regular file) cannot be mapped using THP. Nevertheless, it does allow us to overwrite read-only huge pages. For example, the zero huge page and sealed shmem files can be overwritten (since their mapping can be populated using THP). Note that after the first write page-fault to the zero page, it will be replaced with a new fresh (and zeroed) thp."
    },
    {
        "cve_id": "CVE-2017-1000405",
        "code_before_change": "struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\tpmd_t *pmd, int flags)\n{\n\tunsigned long pfn = pmd_pfn(*pmd);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap;\n\tstruct page *page;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\t/*\n\t * When we COW a devmap PMD entry, we split it into PTEs, so we should\n\t * not be in this function with `flags & FOLL_COW` set.\n\t */\n\tWARN_ONCE(flags & FOLL_COW, \"mm: In follow_devmap_pmd with FOLL_COW set\");\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\treturn NULL;\n\n\tif (pmd_present(*pmd) && pmd_devmap(*pmd))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & FOLL_GET))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PMD_MASK) >> PAGE_SHIFT;\n\tpgmap = get_dev_pagemap(pfn, NULL);\n\tif (!pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tget_page(page);\n\tput_dev_pagemap(pgmap);\n\n\treturn page;\n}",
        "code_after_change": "struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\tpmd_t *pmd, int flags)\n{\n\tunsigned long pfn = pmd_pfn(*pmd);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap;\n\tstruct page *page;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\t/*\n\t * When we COW a devmap PMD entry, we split it into PTEs, so we should\n\t * not be in this function with `flags & FOLL_COW` set.\n\t */\n\tWARN_ONCE(flags & FOLL_COW, \"mm: In follow_devmap_pmd with FOLL_COW set\");\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\treturn NULL;\n\n\tif (pmd_present(*pmd) && pmd_devmap(*pmd))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd, flags);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & FOLL_GET))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PMD_MASK) >> PAGE_SHIFT;\n\tpgmap = get_dev_pagemap(pfn, NULL);\n\tif (!pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tget_page(page);\n\tput_dev_pagemap(pgmap);\n\n\treturn page;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,7 +23,7 @@\n \t\treturn NULL;\n \n \tif (flags & FOLL_TOUCH)\n-\t\ttouch_pmd(vma, addr, pmd);\n+\t\ttouch_pmd(vma, addr, pmd, flags);\n \n \t/*\n \t * device mapped pages can only be returned if the",
        "function_modified_lines": {
            "added": [
                "\t\ttouch_pmd(vma, addr, pmd, flags);"
            ],
            "deleted": [
                "\t\ttouch_pmd(vma, addr, pmd);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The Linux Kernel versions 2.6.38 through 4.14 have a problematic use of pmd_mkdirty() in the touch_pmd() function inside the THP implementation. touch_pmd() can be reached by get_user_pages(). In such case, the pmd will become dirty. This scenario breaks the new can_follow_write_pmd()'s logic - pmd can become dirty without going through a COW cycle. This bug is not as severe as the original \"Dirty cow\" because an ext4 file (or any other regular file) cannot be mapped using THP. Nevertheless, it does allow us to overwrite read-only huge pages. For example, the zero huge page and sealed shmem files can be overwritten (since their mapping can be populated using THP). Note that after the first write page-fault to the zero page, it will be replaced with a new fresh (and zeroed) thp."
    },
    {
        "cve_id": "CVE-2017-1000405",
        "code_before_change": "struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long addr,\n\t\t\t\t   pmd_t *pmd,\n\t\t\t\t   unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page = NULL;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\tif (flags & FOLL_WRITE && !can_follow_write_pmd(*pmd, flags))\n\t\tgoto out;\n\n\t/* Avoid dumping huge zero page */\n\tif ((flags & FOLL_DUMP) && is_huge_zero_pmd(*pmd))\n\t\treturn ERR_PTR(-EFAULT);\n\n\t/* Full NUMA hinting faults to serialise migration in fault paths */\n\tif ((flags & FOLL_NUMA) && pmd_protnone(*pmd))\n\t\tgoto out;\n\n\tpage = pmd_page(*pmd);\n\tVM_BUG_ON_PAGE(!PageHead(page) && !is_zone_device_page(page), page);\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd);\n\tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n\t\t/*\n\t\t * We don't mlock() pte-mapped THPs. This way we can avoid\n\t\t * leaking mlocked pages into non-VM_LOCKED VMAs.\n\t\t *\n\t\t * For anon THP:\n\t\t *\n\t\t * In most cases the pmd is the only mapping of the page as we\n\t\t * break COW for the mlock() -- see gup_flags |= FOLL_WRITE for\n\t\t * writable private mappings in populate_vma_page_range().\n\t\t *\n\t\t * The only scenario when we have the page shared here is if we\n\t\t * mlocking read-only mapping shared over fork(). We skip\n\t\t * mlocking such pages.\n\t\t *\n\t\t * For file THP:\n\t\t *\n\t\t * We can expect PageDoubleMap() to be stable under page lock:\n\t\t * for file pages we set it in page_add_file_rmap(), which\n\t\t * requires page to be locked.\n\t\t */\n\n\t\tif (PageAnon(page) && compound_mapcount(page) != 1)\n\t\t\tgoto skip_mlock;\n\t\tif (PageDoubleMap(page) || !page->mapping)\n\t\t\tgoto skip_mlock;\n\t\tif (!trylock_page(page))\n\t\t\tgoto skip_mlock;\n\t\tlru_add_drain();\n\t\tif (page->mapping && !PageDoubleMap(page))\n\t\t\tmlock_vma_page(page);\n\t\tunlock_page(page);\n\t}\nskip_mlock:\n\tpage += (addr & ~HPAGE_PMD_MASK) >> PAGE_SHIFT;\n\tVM_BUG_ON_PAGE(!PageCompound(page) && !is_zone_device_page(page), page);\n\tif (flags & FOLL_GET)\n\t\tget_page(page);\n\nout:\n\treturn page;\n}",
        "code_after_change": "struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long addr,\n\t\t\t\t   pmd_t *pmd,\n\t\t\t\t   unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page = NULL;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\tif (flags & FOLL_WRITE && !can_follow_write_pmd(*pmd, flags))\n\t\tgoto out;\n\n\t/* Avoid dumping huge zero page */\n\tif ((flags & FOLL_DUMP) && is_huge_zero_pmd(*pmd))\n\t\treturn ERR_PTR(-EFAULT);\n\n\t/* Full NUMA hinting faults to serialise migration in fault paths */\n\tif ((flags & FOLL_NUMA) && pmd_protnone(*pmd))\n\t\tgoto out;\n\n\tpage = pmd_page(*pmd);\n\tVM_BUG_ON_PAGE(!PageHead(page) && !is_zone_device_page(page), page);\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd, flags);\n\tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n\t\t/*\n\t\t * We don't mlock() pte-mapped THPs. This way we can avoid\n\t\t * leaking mlocked pages into non-VM_LOCKED VMAs.\n\t\t *\n\t\t * For anon THP:\n\t\t *\n\t\t * In most cases the pmd is the only mapping of the page as we\n\t\t * break COW for the mlock() -- see gup_flags |= FOLL_WRITE for\n\t\t * writable private mappings in populate_vma_page_range().\n\t\t *\n\t\t * The only scenario when we have the page shared here is if we\n\t\t * mlocking read-only mapping shared over fork(). We skip\n\t\t * mlocking such pages.\n\t\t *\n\t\t * For file THP:\n\t\t *\n\t\t * We can expect PageDoubleMap() to be stable under page lock:\n\t\t * for file pages we set it in page_add_file_rmap(), which\n\t\t * requires page to be locked.\n\t\t */\n\n\t\tif (PageAnon(page) && compound_mapcount(page) != 1)\n\t\t\tgoto skip_mlock;\n\t\tif (PageDoubleMap(page) || !page->mapping)\n\t\t\tgoto skip_mlock;\n\t\tif (!trylock_page(page))\n\t\t\tgoto skip_mlock;\n\t\tlru_add_drain();\n\t\tif (page->mapping && !PageDoubleMap(page))\n\t\t\tmlock_vma_page(page);\n\t\tunlock_page(page);\n\t}\nskip_mlock:\n\tpage += (addr & ~HPAGE_PMD_MASK) >> PAGE_SHIFT;\n\tVM_BUG_ON_PAGE(!PageCompound(page) && !is_zone_device_page(page), page);\n\tif (flags & FOLL_GET)\n\t\tget_page(page);\n\nout:\n\treturn page;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,7 +22,7 @@\n \tpage = pmd_page(*pmd);\n \tVM_BUG_ON_PAGE(!PageHead(page) && !is_zone_device_page(page), page);\n \tif (flags & FOLL_TOUCH)\n-\t\ttouch_pmd(vma, addr, pmd);\n+\t\ttouch_pmd(vma, addr, pmd, flags);\n \tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n \t\t/*\n \t\t * We don't mlock() pte-mapped THPs. This way we can avoid",
        "function_modified_lines": {
            "added": [
                "\t\ttouch_pmd(vma, addr, pmd, flags);"
            ],
            "deleted": [
                "\t\ttouch_pmd(vma, addr, pmd);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The Linux Kernel versions 2.6.38 through 4.14 have a problematic use of pmd_mkdirty() in the touch_pmd() function inside the THP implementation. touch_pmd() can be reached by get_user_pages(). In such case, the pmd will become dirty. This scenario breaks the new can_follow_write_pmd()'s logic - pmd can become dirty without going through a COW cycle. This bug is not as severe as the original \"Dirty cow\" because an ext4 file (or any other regular file) cannot be mapped using THP. Nevertheless, it does allow us to overwrite read-only huge pages. For example, the zero huge page and sealed shmem files can be overwritten (since their mapping can be populated using THP). Note that after the first write page-fault to the zero page, it will be replaced with a new fresh (and zeroed) thp."
    },
    {
        "cve_id": "CVE-2017-12146",
        "code_before_change": "static ssize_t driver_override_store(struct device *dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tchar *driver_override, *old = pdev->driver_override, *cp;\n\n\tif (count > PATH_MAX)\n\t\treturn -EINVAL;\n\n\tdriver_override = kstrndup(buf, count, GFP_KERNEL);\n\tif (!driver_override)\n\t\treturn -ENOMEM;\n\n\tcp = strchr(driver_override, '\\n');\n\tif (cp)\n\t\t*cp = '\\0';\n\n\tif (strlen(driver_override)) {\n\t\tpdev->driver_override = driver_override;\n\t} else {\n\t\tkfree(driver_override);\n\t\tpdev->driver_override = NULL;\n\t}\n\n\tkfree(old);\n\n\treturn count;\n}",
        "code_after_change": "static ssize_t driver_override_store(struct device *dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tchar *driver_override, *old, *cp;\n\n\tif (count > PATH_MAX)\n\t\treturn -EINVAL;\n\n\tdriver_override = kstrndup(buf, count, GFP_KERNEL);\n\tif (!driver_override)\n\t\treturn -ENOMEM;\n\n\tcp = strchr(driver_override, '\\n');\n\tif (cp)\n\t\t*cp = '\\0';\n\n\tdevice_lock(dev);\n\told = pdev->driver_override;\n\tif (strlen(driver_override)) {\n\t\tpdev->driver_override = driver_override;\n\t} else {\n\t\tkfree(driver_override);\n\t\tpdev->driver_override = NULL;\n\t}\n\tdevice_unlock(dev);\n\n\tkfree(old);\n\n\treturn count;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t\t\t\t     const char *buf, size_t count)\n {\n \tstruct platform_device *pdev = to_platform_device(dev);\n-\tchar *driver_override, *old = pdev->driver_override, *cp;\n+\tchar *driver_override, *old, *cp;\n \n \tif (count > PATH_MAX)\n \t\treturn -EINVAL;\n@@ -16,12 +16,15 @@\n \tif (cp)\n \t\t*cp = '\\0';\n \n+\tdevice_lock(dev);\n+\told = pdev->driver_override;\n \tif (strlen(driver_override)) {\n \t\tpdev->driver_override = driver_override;\n \t} else {\n \t\tkfree(driver_override);\n \t\tpdev->driver_override = NULL;\n \t}\n+\tdevice_unlock(dev);\n \n \tkfree(old);\n ",
        "function_modified_lines": {
            "added": [
                "\tchar *driver_override, *old, *cp;",
                "\tdevice_lock(dev);",
                "\told = pdev->driver_override;",
                "\tdevice_unlock(dev);"
            ],
            "deleted": [
                "\tchar *driver_override, *old = pdev->driver_override, *cp;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The driver_override implementation in drivers/base/platform.c in the Linux kernel before 4.12.1 allows local users to gain privileges by leveraging a race condition between a read operation and a store operation that involve different overrides."
    },
    {
        "cve_id": "CVE-2017-12146",
        "code_before_change": "static ssize_t driver_override_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\n\treturn sprintf(buf, \"%s\\n\", pdev->driver_override);\n}",
        "code_after_change": "static ssize_t driver_override_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tssize_t len;\n\n\tdevice_lock(dev);\n\tlen = sprintf(buf, \"%s\\n\", pdev->driver_override);\n\tdevice_unlock(dev);\n\treturn len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,10 @@\n \t\t\t\t    struct device_attribute *attr, char *buf)\n {\n \tstruct platform_device *pdev = to_platform_device(dev);\n+\tssize_t len;\n \n-\treturn sprintf(buf, \"%s\\n\", pdev->driver_override);\n+\tdevice_lock(dev);\n+\tlen = sprintf(buf, \"%s\\n\", pdev->driver_override);\n+\tdevice_unlock(dev);\n+\treturn len;\n }",
        "function_modified_lines": {
            "added": [
                "\tssize_t len;",
                "\tdevice_lock(dev);",
                "\tlen = sprintf(buf, \"%s\\n\", pdev->driver_override);",
                "\tdevice_unlock(dev);",
                "\treturn len;"
            ],
            "deleted": [
                "\treturn sprintf(buf, \"%s\\n\", pdev->driver_override);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The driver_override implementation in drivers/base/platform.c in the Linux kernel before 4.12.1 allows local users to gain privileges by leveraging a race condition between a read operation and a store operation that involve different overrides."
    },
    {
        "cve_id": "CVE-2017-15129",
        "code_before_change": "struct net *get_net_ns_by_id(struct net *net, int id)\n{\n\tstruct net *peer;\n\n\tif (id < 0)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tspin_lock_bh(&net->nsid_lock);\n\tpeer = idr_find(&net->netns_ids, id);\n\tif (peer)\n\t\tget_net(peer);\n\tspin_unlock_bh(&net->nsid_lock);\n\trcu_read_unlock();\n\n\treturn peer;\n}",
        "code_after_change": "struct net *get_net_ns_by_id(struct net *net, int id)\n{\n\tstruct net *peer;\n\n\tif (id < 0)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tspin_lock_bh(&net->nsid_lock);\n\tpeer = idr_find(&net->netns_ids, id);\n\tif (peer)\n\t\tpeer = maybe_get_net(peer);\n\tspin_unlock_bh(&net->nsid_lock);\n\trcu_read_unlock();\n\n\treturn peer;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \tspin_lock_bh(&net->nsid_lock);\n \tpeer = idr_find(&net->netns_ids, id);\n \tif (peer)\n-\t\tget_net(peer);\n+\t\tpeer = maybe_get_net(peer);\n \tspin_unlock_bh(&net->nsid_lock);\n \trcu_read_unlock();\n ",
        "function_modified_lines": {
            "added": [
                "\t\tpeer = maybe_get_net(peer);"
            ],
            "deleted": [
                "\t\tget_net(peer);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free vulnerability was found in network namespaces code affecting the Linux kernel before 4.14.11. The function get_net_ns_by_id() in net/core/net_namespace.c does not check for the net::count value after it has found a peer network in netns_ids idr, which could lead to double free and memory corruption. This vulnerability could allow an unprivileged local user to induce kernel memory corruption on the system, leading to a crash. Due to the nature of the flaw, privilege escalation cannot be fully ruled out, although it is thought to be unlikely."
    },
    {
        "cve_id": "CVE-2017-15265",
        "code_before_change": "static int snd_seq_ioctl_create_port(struct snd_seq_client *client, void *arg)\n{\n\tstruct snd_seq_port_info *info = arg;\n\tstruct snd_seq_client_port *port;\n\tstruct snd_seq_port_callback *callback;\n\n\t/* it is not allowed to create the port for an another client */\n\tif (info->addr.client != client->number)\n\t\treturn -EPERM;\n\n\tport = snd_seq_create_port(client, (info->flags & SNDRV_SEQ_PORT_FLG_GIVEN_PORT) ? info->addr.port : -1);\n\tif (port == NULL)\n\t\treturn -ENOMEM;\n\n\tif (client->type == USER_CLIENT && info->kernel) {\n\t\tsnd_seq_delete_port(client, port->addr.port);\n\t\treturn -EINVAL;\n\t}\n\tif (client->type == KERNEL_CLIENT) {\n\t\tif ((callback = info->kernel) != NULL) {\n\t\t\tif (callback->owner)\n\t\t\t\tport->owner = callback->owner;\n\t\t\tport->private_data = callback->private_data;\n\t\t\tport->private_free = callback->private_free;\n\t\t\tport->event_input = callback->event_input;\n\t\t\tport->c_src.open = callback->subscribe;\n\t\t\tport->c_src.close = callback->unsubscribe;\n\t\t\tport->c_dest.open = callback->use;\n\t\t\tport->c_dest.close = callback->unuse;\n\t\t}\n\t}\n\n\tinfo->addr = port->addr;\n\n\tsnd_seq_set_port_info(port, info);\n\tsnd_seq_system_client_ev_port_start(port->addr.client, port->addr.port);\n\n\treturn 0;\n}",
        "code_after_change": "static int snd_seq_ioctl_create_port(struct snd_seq_client *client, void *arg)\n{\n\tstruct snd_seq_port_info *info = arg;\n\tstruct snd_seq_client_port *port;\n\tstruct snd_seq_port_callback *callback;\n\tint port_idx;\n\n\t/* it is not allowed to create the port for an another client */\n\tif (info->addr.client != client->number)\n\t\treturn -EPERM;\n\n\tport = snd_seq_create_port(client, (info->flags & SNDRV_SEQ_PORT_FLG_GIVEN_PORT) ? info->addr.port : -1);\n\tif (port == NULL)\n\t\treturn -ENOMEM;\n\n\tif (client->type == USER_CLIENT && info->kernel) {\n\t\tport_idx = port->addr.port;\n\t\tsnd_seq_port_unlock(port);\n\t\tsnd_seq_delete_port(client, port_idx);\n\t\treturn -EINVAL;\n\t}\n\tif (client->type == KERNEL_CLIENT) {\n\t\tif ((callback = info->kernel) != NULL) {\n\t\t\tif (callback->owner)\n\t\t\t\tport->owner = callback->owner;\n\t\t\tport->private_data = callback->private_data;\n\t\t\tport->private_free = callback->private_free;\n\t\t\tport->event_input = callback->event_input;\n\t\t\tport->c_src.open = callback->subscribe;\n\t\t\tport->c_src.close = callback->unsubscribe;\n\t\t\tport->c_dest.open = callback->use;\n\t\t\tport->c_dest.close = callback->unuse;\n\t\t}\n\t}\n\n\tinfo->addr = port->addr;\n\n\tsnd_seq_set_port_info(port, info);\n\tsnd_seq_system_client_ev_port_start(port->addr.client, port->addr.port);\n\tsnd_seq_port_unlock(port);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n \tstruct snd_seq_port_info *info = arg;\n \tstruct snd_seq_client_port *port;\n \tstruct snd_seq_port_callback *callback;\n+\tint port_idx;\n \n \t/* it is not allowed to create the port for an another client */\n \tif (info->addr.client != client->number)\n@@ -13,7 +14,9 @@\n \t\treturn -ENOMEM;\n \n \tif (client->type == USER_CLIENT && info->kernel) {\n-\t\tsnd_seq_delete_port(client, port->addr.port);\n+\t\tport_idx = port->addr.port;\n+\t\tsnd_seq_port_unlock(port);\n+\t\tsnd_seq_delete_port(client, port_idx);\n \t\treturn -EINVAL;\n \t}\n \tif (client->type == KERNEL_CLIENT) {\n@@ -34,6 +37,7 @@\n \n \tsnd_seq_set_port_info(port, info);\n \tsnd_seq_system_client_ev_port_start(port->addr.client, port->addr.port);\n+\tsnd_seq_port_unlock(port);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tint port_idx;",
                "\t\tport_idx = port->addr.port;",
                "\t\tsnd_seq_port_unlock(port);",
                "\t\tsnd_seq_delete_port(client, port_idx);",
                "\tsnd_seq_port_unlock(port);"
            ],
            "deleted": [
                "\t\tsnd_seq_delete_port(client, port->addr.port);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ALSA subsystem in the Linux kernel before 4.13.8 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via crafted /dev/snd/seq ioctl calls, related to sound/core/seq/seq_clientmgr.c and sound/core/seq/seq_ports.c."
    },
    {
        "cve_id": "CVE-2017-15265",
        "code_before_change": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\tsprintf(new_port->name, \"port-%d\", num);\n\n\treturn new_port;\n}",
        "code_after_change": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\tsnd_use_lock_use(&new_port->use_lock);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\tsprintf(new_port->name, \"port-%d\", num);\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\n\treturn new_port;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,6 +26,7 @@\n \tsnd_use_lock_init(&new_port->use_lock);\n \tport_subs_info_init(&new_port->c_src);\n \tport_subs_info_init(&new_port->c_dest);\n+\tsnd_use_lock_use(&new_port->use_lock);\n \n \tnum = port >= 0 ? port : 0;\n \tmutex_lock(&client->ports_mutex);\n@@ -40,9 +41,9 @@\n \tlist_add_tail(&new_port->list, &p->list);\n \tclient->num_ports++;\n \tnew_port->addr.port = num;\t/* store the port number in the port */\n+\tsprintf(new_port->name, \"port-%d\", num);\n \twrite_unlock_irqrestore(&client->ports_lock, flags);\n \tmutex_unlock(&client->ports_mutex);\n-\tsprintf(new_port->name, \"port-%d\", num);\n \n \treturn new_port;\n }",
        "function_modified_lines": {
            "added": [
                "\tsnd_use_lock_use(&new_port->use_lock);",
                "\tsprintf(new_port->name, \"port-%d\", num);"
            ],
            "deleted": [
                "\tsprintf(new_port->name, \"port-%d\", num);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ALSA subsystem in the Linux kernel before 4.13.8 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via crafted /dev/snd/seq ioctl calls, related to sound/core/seq/seq_clientmgr.c and sound/core/seq/seq_ports.c."
    },
    {
        "cve_id": "CVE-2017-17712",
        "code_before_change": "static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\tstruct ip_options_data opt_copy;\n\tstruct raw_frag_vec rfv;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tpr_info_once(\"%s: %s forgot to set AF_INET. Fix it!\\n\",\n\t\t\t\t     __func__, current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.sockc.tsflags = sk->sk_tsflags;\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.ttl = 0;\n\tipc.tos = -1;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sk, msg, &ipc, false);\n\t\tif (unlikely(err)) {\n\t\t\tkfree(ipc.opt);\n\t\t\tgoto out;\n\t\t}\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (inet->hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->opt.srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->opt.faddr;\n\t\t}\n\t}\n\ttos = get_rtconn_flags(&ipc, sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t} else if (!ipc.oif)\n\t\tipc.oif = inet->uc_index;\n\n\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t   inet_sk_flowi_flags(sk) |\n\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n\t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n\n\tif (!inet->hdrincl) {\n\t\trfv.msg = msg;\n\t\trfv.hlen = 0;\n\n\t\terr = raw_probe_proto_opt(&rfv, &fl4);\n\t\tif (err)\n\t\t\tgoto done;\n\t}\n\n\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto done;\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (inet->hdrincl)\n\t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n\t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n\n\t else {\n\t\tsock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);\n\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = fl4.daddr;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, &fl4, raw_getfrag,\n\t\t\t\t     &rfv, len, 0,\n\t\t\t\t     &ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk, &fl4);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tif (msg->msg_flags & MSG_PROBE)\n\t\tdst_confirm_neigh(&rt->dst, &fl4.daddr);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "code_after_change": "static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\tstruct ip_options_data opt_copy;\n\tstruct raw_frag_vec rfv;\n\tint hdrincl;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/* hdrincl should be READ_ONCE(inet->hdrincl)\n\t * but READ_ONCE() doesn't work with bit fields\n\t */\n\thdrincl = inet->hdrincl;\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tpr_info_once(\"%s: %s forgot to set AF_INET. Fix it!\\n\",\n\t\t\t\t     __func__, current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.sockc.tsflags = sk->sk_tsflags;\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.ttl = 0;\n\tipc.tos = -1;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sk, msg, &ipc, false);\n\t\tif (unlikely(err)) {\n\t\t\tkfree(ipc.opt);\n\t\t\tgoto out;\n\t\t}\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->opt.srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->opt.faddr;\n\t\t}\n\t}\n\ttos = get_rtconn_flags(&ipc, sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t} else if (!ipc.oif)\n\t\tipc.oif = inet->uc_index;\n\n\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t   inet_sk_flowi_flags(sk) |\n\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n\t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n\n\tif (!hdrincl) {\n\t\trfv.msg = msg;\n\t\trfv.hlen = 0;\n\n\t\terr = raw_probe_proto_opt(&rfv, &fl4);\n\t\tif (err)\n\t\t\tgoto done;\n\t}\n\n\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto done;\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (hdrincl)\n\t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n\t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n\n\t else {\n\t\tsock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);\n\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = fl4.daddr;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, &fl4, raw_getfrag,\n\t\t\t\t     &rfv, len, 0,\n\t\t\t\t     &ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk, &fl4);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tif (msg->msg_flags & MSG_PROBE)\n\t\tdst_confirm_neigh(&rt->dst, &fl4.daddr);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,11 +12,16 @@\n \tint err;\n \tstruct ip_options_data opt_copy;\n \tstruct raw_frag_vec rfv;\n+\tint hdrincl;\n \n \terr = -EMSGSIZE;\n \tif (len > 0xFFFF)\n \t\tgoto out;\n \n+\t/* hdrincl should be READ_ONCE(inet->hdrincl)\n+\t * but READ_ONCE() doesn't work with bit fields\n+\t */\n+\thdrincl = inet->hdrincl;\n \t/*\n \t *\tCheck the flags.\n \t */\n@@ -92,7 +97,7 @@\n \t\t/* Linux does not mangle headers on raw sockets,\n \t\t * so that IP options + IP_HDRINCL is non-sense.\n \t\t */\n-\t\tif (inet->hdrincl)\n+\t\tif (hdrincl)\n \t\t\tgoto done;\n \t\tif (ipc.opt->opt.srr) {\n \t\t\tif (!daddr)\n@@ -114,12 +119,12 @@\n \n \tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n \t\t\t   RT_SCOPE_UNIVERSE,\n-\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n+\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n \t\t\t   inet_sk_flowi_flags(sk) |\n-\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n+\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n \t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n \n-\tif (!inet->hdrincl) {\n+\tif (!hdrincl) {\n \t\trfv.msg = msg;\n \t\trfv.hlen = 0;\n \n@@ -144,7 +149,7 @@\n \t\tgoto do_confirm;\n back_from_confirm:\n \n-\tif (inet->hdrincl)\n+\tif (hdrincl)\n \t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n \t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n ",
        "function_modified_lines": {
            "added": [
                "\tint hdrincl;",
                "\t/* hdrincl should be READ_ONCE(inet->hdrincl)",
                "\t * but READ_ONCE() doesn't work with bit fields",
                "\t */",
                "\thdrincl = inet->hdrincl;",
                "\t\tif (hdrincl)",
                "\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,",
                "\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),",
                "\tif (!hdrincl) {",
                "\tif (hdrincl)"
            ],
            "deleted": [
                "\t\tif (inet->hdrincl)",
                "\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,",
                "\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),",
                "\tif (!inet->hdrincl) {",
                "\tif (inet->hdrincl)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The raw_sendmsg() function in net/ipv4/raw.c in the Linux kernel through 4.14.6 has a race condition in inet->hdrincl that leads to uninitialized stack pointer usage; this allows a local user to execute code and gain privileges."
    },
    {
        "cve_id": "CVE-2017-18224",
        "code_before_change": "static ssize_t ocfs2_direct_IO(struct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct ocfs2_super *osb = OCFS2_SB(inode->i_sb);\n\tget_block_t *get_block;\n\n\t/*\n\t * Fallback to buffered I/O if we see an inode without\n\t * extents.\n\t */\n\tif (OCFS2_I(inode)->ip_dyn_features & OCFS2_INLINE_DATA_FL)\n\t\treturn 0;\n\n\t/* Fallback to buffered I/O if we do not support append dio. */\n\tif (iocb->ki_pos + iter->count > i_size_read(inode) &&\n\t    !ocfs2_supports_append_dio(osb))\n\t\treturn 0;\n\n\tif (iov_iter_rw(iter) == READ)\n\t\tget_block = ocfs2_get_block;\n\telse\n\t\tget_block = ocfs2_dio_get_block;\n\n\treturn __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,\n\t\t\t\t    iter, get_block,\n\t\t\t\t    ocfs2_dio_end_io, NULL, 0);\n}",
        "code_after_change": "static ssize_t ocfs2_direct_IO(struct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct ocfs2_super *osb = OCFS2_SB(inode->i_sb);\n\tget_block_t *get_block;\n\n\t/*\n\t * Fallback to buffered I/O if we see an inode without\n\t * extents.\n\t */\n\tif (OCFS2_I(inode)->ip_dyn_features & OCFS2_INLINE_DATA_FL)\n\t\treturn 0;\n\n\t/* Fallback to buffered I/O if we do not support append dio. */\n\tif (iocb->ki_pos + iter->count > i_size_read(inode) &&\n\t    !ocfs2_supports_append_dio(osb))\n\t\treturn 0;\n\n\tif (iov_iter_rw(iter) == READ)\n\t\tget_block = ocfs2_lock_get_block;\n\telse\n\t\tget_block = ocfs2_dio_wr_get_block;\n\n\treturn __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,\n\t\t\t\t    iter, get_block,\n\t\t\t\t    ocfs2_dio_end_io, NULL, 0);\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,9 +18,9 @@\n \t\treturn 0;\n \n \tif (iov_iter_rw(iter) == READ)\n-\t\tget_block = ocfs2_get_block;\n+\t\tget_block = ocfs2_lock_get_block;\n \telse\n-\t\tget_block = ocfs2_dio_get_block;\n+\t\tget_block = ocfs2_dio_wr_get_block;\n \n \treturn __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,\n \t\t\t\t    iter, get_block,",
        "function_modified_lines": {
            "added": [
                "\t\tget_block = ocfs2_lock_get_block;",
                "\t\tget_block = ocfs2_dio_wr_get_block;"
            ],
            "deleted": [
                "\t\tget_block = ocfs2_get_block;",
                "\t\tget_block = ocfs2_dio_get_block;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux kernel before 4.15, fs/ocfs2/aops.c omits use of a semaphore and consequently has a race condition for access to the extent tree during read operations in DIRECT mode, which allows local users to cause a denial of service (BUG) by modifying a certain e_cpos field."
    },
    {
        "cve_id": "CVE-2017-18249",
        "code_before_change": "static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)\n{\n\tstruct f2fs_nm_info *nm_i = NM_I(sbi);\n\tstruct free_nid *i;\n\tstruct nat_entry *ne;\n\tint err;\n\n\t/* 0 nid should not be used */\n\tif (unlikely(nid == 0))\n\t\treturn false;\n\n\tif (build) {\n\t\t/* do not add allocated nids */\n\t\tne = __lookup_nat_cache(nm_i, nid);\n\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n\t\t\treturn false;\n\t}\n\n\ti = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);\n\ti->nid = nid;\n\ti->state = NID_NEW;\n\n\tif (radix_tree_preload(GFP_NOFS)) {\n\t\tkmem_cache_free(free_nid_slab, i);\n\t\treturn true;\n\t}\n\n\tspin_lock(&nm_i->nid_list_lock);\n\terr = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);\n\tspin_unlock(&nm_i->nid_list_lock);\n\tradix_tree_preload_end();\n\tif (err) {\n\t\tkmem_cache_free(free_nid_slab, i);\n\t\treturn true;\n\t}\n\treturn true;\n}",
        "code_after_change": "static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)\n{\n\tstruct f2fs_nm_info *nm_i = NM_I(sbi);\n\tstruct free_nid *i, *e;\n\tstruct nat_entry *ne;\n\tint err = -EINVAL;\n\tbool ret = false;\n\n\t/* 0 nid should not be used */\n\tif (unlikely(nid == 0))\n\t\treturn false;\n\n\ti = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);\n\ti->nid = nid;\n\ti->state = NID_NEW;\n\n\tif (radix_tree_preload(GFP_NOFS))\n\t\tgoto err;\n\n\tspin_lock(&nm_i->nid_list_lock);\n\n\tif (build) {\n\t\t/*\n\t\t *   Thread A             Thread B\n\t\t *  - f2fs_create\n\t\t *   - f2fs_new_inode\n\t\t *    - alloc_nid\n\t\t *     - __insert_nid_to_list(ALLOC_NID_LIST)\n\t\t *                     - f2fs_balance_fs_bg\n\t\t *                      - build_free_nids\n\t\t *                       - __build_free_nids\n\t\t *                        - scan_nat_page\n\t\t *                         - add_free_nid\n\t\t *                          - __lookup_nat_cache\n\t\t *  - f2fs_add_link\n\t\t *   - init_inode_metadata\n\t\t *    - new_inode_page\n\t\t *     - new_node_page\n\t\t *      - set_node_addr\n\t\t *  - alloc_nid_done\n\t\t *   - __remove_nid_from_list(ALLOC_NID_LIST)\n\t\t *                         - __insert_nid_to_list(FREE_NID_LIST)\n\t\t */\n\t\tne = __lookup_nat_cache(nm_i, nid);\n\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n\t\t\tgoto err_out;\n\n\t\te = __lookup_free_nid_list(nm_i, nid);\n\t\tif (e) {\n\t\t\tif (e->state == NID_NEW)\n\t\t\t\tret = true;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\tret = true;\n\terr = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);\nerr_out:\n\tspin_unlock(&nm_i->nid_list_lock);\n\tradix_tree_preload_end();\nerr:\n\tif (err)\n\t\tkmem_cache_free(free_nid_slab, i);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,38 +1,65 @@\n static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)\n {\n \tstruct f2fs_nm_info *nm_i = NM_I(sbi);\n-\tstruct free_nid *i;\n+\tstruct free_nid *i, *e;\n \tstruct nat_entry *ne;\n-\tint err;\n+\tint err = -EINVAL;\n+\tbool ret = false;\n \n \t/* 0 nid should not be used */\n \tif (unlikely(nid == 0))\n \t\treturn false;\n \n-\tif (build) {\n-\t\t/* do not add allocated nids */\n-\t\tne = __lookup_nat_cache(nm_i, nid);\n-\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n-\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n-\t\t\treturn false;\n-\t}\n-\n \ti = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);\n \ti->nid = nid;\n \ti->state = NID_NEW;\n \n-\tif (radix_tree_preload(GFP_NOFS)) {\n-\t\tkmem_cache_free(free_nid_slab, i);\n-\t\treturn true;\n-\t}\n+\tif (radix_tree_preload(GFP_NOFS))\n+\t\tgoto err;\n \n \tspin_lock(&nm_i->nid_list_lock);\n+\n+\tif (build) {\n+\t\t/*\n+\t\t *   Thread A             Thread B\n+\t\t *  - f2fs_create\n+\t\t *   - f2fs_new_inode\n+\t\t *    - alloc_nid\n+\t\t *     - __insert_nid_to_list(ALLOC_NID_LIST)\n+\t\t *                     - f2fs_balance_fs_bg\n+\t\t *                      - build_free_nids\n+\t\t *                       - __build_free_nids\n+\t\t *                        - scan_nat_page\n+\t\t *                         - add_free_nid\n+\t\t *                          - __lookup_nat_cache\n+\t\t *  - f2fs_add_link\n+\t\t *   - init_inode_metadata\n+\t\t *    - new_inode_page\n+\t\t *     - new_node_page\n+\t\t *      - set_node_addr\n+\t\t *  - alloc_nid_done\n+\t\t *   - __remove_nid_from_list(ALLOC_NID_LIST)\n+\t\t *                         - __insert_nid_to_list(FREE_NID_LIST)\n+\t\t */\n+\t\tne = __lookup_nat_cache(nm_i, nid);\n+\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||\n+\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))\n+\t\t\tgoto err_out;\n+\n+\t\te = __lookup_free_nid_list(nm_i, nid);\n+\t\tif (e) {\n+\t\t\tif (e->state == NID_NEW)\n+\t\t\t\tret = true;\n+\t\t\tgoto err_out;\n+\t\t}\n+\t}\n+\tret = true;\n \terr = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);\n+err_out:\n \tspin_unlock(&nm_i->nid_list_lock);\n \tradix_tree_preload_end();\n-\tif (err) {\n+err:\n+\tif (err)\n \t\tkmem_cache_free(free_nid_slab, i);\n-\t\treturn true;\n-\t}\n-\treturn true;\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct free_nid *i, *e;",
                "\tint err = -EINVAL;",
                "\tbool ret = false;",
                "\tif (radix_tree_preload(GFP_NOFS))",
                "\t\tgoto err;",
                "",
                "\tif (build) {",
                "\t\t/*",
                "\t\t *   Thread A             Thread B",
                "\t\t *  - f2fs_create",
                "\t\t *   - f2fs_new_inode",
                "\t\t *    - alloc_nid",
                "\t\t *     - __insert_nid_to_list(ALLOC_NID_LIST)",
                "\t\t *                     - f2fs_balance_fs_bg",
                "\t\t *                      - build_free_nids",
                "\t\t *                       - __build_free_nids",
                "\t\t *                        - scan_nat_page",
                "\t\t *                         - add_free_nid",
                "\t\t *                          - __lookup_nat_cache",
                "\t\t *  - f2fs_add_link",
                "\t\t *   - init_inode_metadata",
                "\t\t *    - new_inode_page",
                "\t\t *     - new_node_page",
                "\t\t *      - set_node_addr",
                "\t\t *  - alloc_nid_done",
                "\t\t *   - __remove_nid_from_list(ALLOC_NID_LIST)",
                "\t\t *                         - __insert_nid_to_list(FREE_NID_LIST)",
                "\t\t */",
                "\t\tne = __lookup_nat_cache(nm_i, nid);",
                "\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||",
                "\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))",
                "\t\t\tgoto err_out;",
                "",
                "\t\te = __lookup_free_nid_list(nm_i, nid);",
                "\t\tif (e) {",
                "\t\t\tif (e->state == NID_NEW)",
                "\t\t\t\tret = true;",
                "\t\t\tgoto err_out;",
                "\t\t}",
                "\t}",
                "\tret = true;",
                "err_out:",
                "err:",
                "\tif (err)",
                "\treturn ret;"
            ],
            "deleted": [
                "\tstruct free_nid *i;",
                "\tint err;",
                "\tif (build) {",
                "\t\t/* do not add allocated nids */",
                "\t\tne = __lookup_nat_cache(nm_i, nid);",
                "\t\tif (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||",
                "\t\t\t\tnat_get_blkaddr(ne) != NULL_ADDR))",
                "\t\t\treturn false;",
                "\t}",
                "",
                "\tif (radix_tree_preload(GFP_NOFS)) {",
                "\t\tkmem_cache_free(free_nid_slab, i);",
                "\t\treturn true;",
                "\t}",
                "\tif (err) {",
                "\t\treturn true;",
                "\t}",
                "\treturn true;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The add_free_nid function in fs/f2fs/node.c in the Linux kernel before 4.12 does not properly track an allocated nid, which allows local users to cause a denial of service (race condition) or possibly have unspecified other impact via concurrent threads."
    },
    {
        "cve_id": "CVE-2017-6001",
        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\tint cgroup_fd = -1;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!attr.sample_max_stack)\n\t\tattr.sample_max_stack = sysctl_perf_event_max_stack;\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tif (task) {\n\t\terr = mutex_lock_interruptible(&task->signal->cred_guard_mutex);\n\t\tif (err)\n\t\t\tgoto err_cpus;\n\n\t\t/*\n\t\t * Reuse ptrace permission checks for now.\n\t\t *\n\t\t * We must hold cred_guard_mutex across this and any potential\n\t\t * perf_install_in_context() call for this new event to\n\t\t * serialize against exec() altering our credentials (and the\n\t\t * perf_event_exit_task() that could imply).\n\t\t */\n\t\terr = -EACCES;\n\t\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))\n\t\t\tgoto err_cred;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP)\n\t\tcgroup_fd = pid;\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL, cgroup_fd);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cred;\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (attr.use_clockid) {\n\t\terr = perf_event_set_clock(event, attr.clockid);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t}\n\n\tif (pmu->task_ctx_nr == perf_sw_context)\n\t\tevent->event_caps |= PERF_EV_CAP_SOFTWARE;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {\n\t\terr = -EBUSY;\n\t\tgoto err_context;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\n\t\t/* All events in a group should have the same clock */\n\t\tif (group_leader->clock != event->clock)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tevent_file = NULL;\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\t\tif (gctx->task == TASK_TOMBSTONE) {\n\t\t\terr = -ESRCH;\n\t\t\tgoto err_locked;\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tif (ctx->task == TASK_TOMBSTONE) {\n\t\terr = -ESRCH;\n\t\tgoto err_locked;\n\t}\n\n\tif (!perf_event_validate_size(event)) {\n\t\terr = -E2BIG;\n\t\tgoto err_locked;\n\t}\n\n\t/*\n\t * Must be under the same ctx::mutex as perf_install_in_context(),\n\t * because we need to serialize with concurrent event creation.\n\t */\n\tif (!exclusive_event_installable(event, ctx)) {\n\t\t/* exclusive and group stuff are assumed mutually exclusive */\n\t\tWARN_ON_ONCE(move_group);\n\n\t\terr = -EBUSY;\n\t\tgoto err_locked;\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\t/*\n\t * This is the point on no return; we cannot fail hereafter. This is\n\t * where we start modifying current state.\n\t */\n\n\tif (move_group) {\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tperf_remove_from_context(group_leader, 0);\n\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, 0);\n\t\t\tput_ctx(gctx);\n\t\t}\n\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\t/*\n\t\t * Install the group siblings before the group leader.\n\t\t *\n\t\t * Because a group leader will try and install the entire group\n\t\t * (through the sibling list, which is still in-tact), we can\n\t\t * end up with siblings installed in the wrong context.\n\t\t *\n\t\t * By installing siblings first we NO-OP because they're not\n\t\t * reachable through the group lists.\n\t\t */\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_event__state_init(sibling);\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\n\t\t/*\n\t\t * Now that all events are installed in @ctx, nothing\n\t\t * references @gctx anymore, so drop the last reference we have\n\t\t * on it.\n\t\t */\n\t\tput_ctx(gctx);\n\t}\n\n\t/*\n\t * Precalculate sample_data sizes; do while holding ctx::mutex such\n\t * that we're serialized against further additions and before\n\t * perf_install_in_context() which is the point the event is active and\n\t * can use these values.\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\tevent->owner = current;\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group)\n\t\tmutex_unlock(&gctx->mutex);\n\tmutex_unlock(&ctx->mutex);\n\n\tif (task) {\n\t\tmutex_unlock(&task->signal->cred_guard_mutex);\n\t\tput_task_struct(task);\n\t}\n\n\tput_online_cpus();\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_locked:\n\tif (move_group)\n\t\tmutex_unlock(&gctx->mutex);\n\tmutex_unlock(&ctx->mutex);\n/* err_file: */\n\tfput(event_file);\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\t/*\n\t * If event_file is set, the fput() above will have called ->release()\n\t * and that will take care of freeing the event.\n\t */\n\tif (!event_file)\n\t\tfree_event(event);\nerr_cred:\n\tif (task)\n\t\tmutex_unlock(&task->signal->cred_guard_mutex);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\tint cgroup_fd = -1;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!attr.sample_max_stack)\n\t\tattr.sample_max_stack = sysctl_perf_event_max_stack;\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tif (task) {\n\t\terr = mutex_lock_interruptible(&task->signal->cred_guard_mutex);\n\t\tif (err)\n\t\t\tgoto err_cpus;\n\n\t\t/*\n\t\t * Reuse ptrace permission checks for now.\n\t\t *\n\t\t * We must hold cred_guard_mutex across this and any potential\n\t\t * perf_install_in_context() call for this new event to\n\t\t * serialize against exec() altering our credentials (and the\n\t\t * perf_event_exit_task() that could imply).\n\t\t */\n\t\terr = -EACCES;\n\t\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))\n\t\t\tgoto err_cred;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP)\n\t\tcgroup_fd = pid;\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL, cgroup_fd);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cred;\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (attr.use_clockid) {\n\t\terr = perf_event_set_clock(event, attr.clockid);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t}\n\n\tif (pmu->task_ctx_nr == perf_sw_context)\n\t\tevent->event_caps |= PERF_EV_CAP_SOFTWARE;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {\n\t\terr = -EBUSY;\n\t\tgoto err_context;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\n\t\t/* All events in a group should have the same clock */\n\t\tif (group_leader->clock != event->clock)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tevent_file = NULL;\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = __perf_event_ctx_lock_double(group_leader, ctx);\n\n\t\tif (gctx->task == TASK_TOMBSTONE) {\n\t\t\terr = -ESRCH;\n\t\t\tgoto err_locked;\n\t\t}\n\n\t\t/*\n\t\t * Check if we raced against another sys_perf_event_open() call\n\t\t * moving the software group underneath us.\n\t\t */\n\t\tif (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * If someone moved the group out from under us, check\n\t\t\t * if this new event wound up on the same ctx, if so\n\t\t\t * its the regular !move_group case, otherwise fail.\n\t\t\t */\n\t\t\tif (gctx != ctx) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_locked;\n\t\t\t} else {\n\t\t\t\tperf_event_ctx_unlock(group_leader, gctx);\n\t\t\t\tmove_group = 0;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tif (ctx->task == TASK_TOMBSTONE) {\n\t\terr = -ESRCH;\n\t\tgoto err_locked;\n\t}\n\n\tif (!perf_event_validate_size(event)) {\n\t\terr = -E2BIG;\n\t\tgoto err_locked;\n\t}\n\n\t/*\n\t * Must be under the same ctx::mutex as perf_install_in_context(),\n\t * because we need to serialize with concurrent event creation.\n\t */\n\tif (!exclusive_event_installable(event, ctx)) {\n\t\t/* exclusive and group stuff are assumed mutually exclusive */\n\t\tWARN_ON_ONCE(move_group);\n\n\t\terr = -EBUSY;\n\t\tgoto err_locked;\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\t/*\n\t * This is the point on no return; we cannot fail hereafter. This is\n\t * where we start modifying current state.\n\t */\n\n\tif (move_group) {\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tperf_remove_from_context(group_leader, 0);\n\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, 0);\n\t\t\tput_ctx(gctx);\n\t\t}\n\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\t/*\n\t\t * Install the group siblings before the group leader.\n\t\t *\n\t\t * Because a group leader will try and install the entire group\n\t\t * (through the sibling list, which is still in-tact), we can\n\t\t * end up with siblings installed in the wrong context.\n\t\t *\n\t\t * By installing siblings first we NO-OP because they're not\n\t\t * reachable through the group lists.\n\t\t */\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_event__state_init(sibling);\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\n\t\t/*\n\t\t * Now that all events are installed in @ctx, nothing\n\t\t * references @gctx anymore, so drop the last reference we have\n\t\t * on it.\n\t\t */\n\t\tput_ctx(gctx);\n\t}\n\n\t/*\n\t * Precalculate sample_data sizes; do while holding ctx::mutex such\n\t * that we're serialized against further additions and before\n\t * perf_install_in_context() which is the point the event is active and\n\t * can use these values.\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\tevent->owner = current;\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group)\n\t\tperf_event_ctx_unlock(group_leader, gctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tif (task) {\n\t\tmutex_unlock(&task->signal->cred_guard_mutex);\n\t\tput_task_struct(task);\n\t}\n\n\tput_online_cpus();\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_locked:\n\tif (move_group)\n\t\tperf_event_ctx_unlock(group_leader, gctx);\n\tmutex_unlock(&ctx->mutex);\n/* err_file: */\n\tfput(event_file);\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\t/*\n\t * If event_file is set, the fput() above will have called ->release()\n\t * and that will take care of freeing the event.\n\t */\n\tif (!event_file)\n\t\tfree_event(event);\nerr_cred:\n\tif (task)\n\t\tmutex_unlock(&task->signal->cred_guard_mutex);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -234,11 +234,30 @@\n \t}\n \n \tif (move_group) {\n-\t\tgctx = group_leader->ctx;\n-\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n+\t\tgctx = __perf_event_ctx_lock_double(group_leader, ctx);\n+\n \t\tif (gctx->task == TASK_TOMBSTONE) {\n \t\t\terr = -ESRCH;\n \t\t\tgoto err_locked;\n+\t\t}\n+\n+\t\t/*\n+\t\t * Check if we raced against another sys_perf_event_open() call\n+\t\t * moving the software group underneath us.\n+\t\t */\n+\t\tif (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n+\t\t\t/*\n+\t\t\t * If someone moved the group out from under us, check\n+\t\t\t * if this new event wound up on the same ctx, if so\n+\t\t\t * its the regular !move_group case, otherwise fail.\n+\t\t\t */\n+\t\t\tif (gctx != ctx) {\n+\t\t\t\terr = -EINVAL;\n+\t\t\t\tgoto err_locked;\n+\t\t\t} else {\n+\t\t\t\tperf_event_ctx_unlock(group_leader, gctx);\n+\t\t\t\tmove_group = 0;\n+\t\t\t}\n \t\t}\n \t} else {\n \t\tmutex_lock(&ctx->mutex);\n@@ -341,7 +360,7 @@\n \tperf_unpin_context(ctx);\n \n \tif (move_group)\n-\t\tmutex_unlock(&gctx->mutex);\n+\t\tperf_event_ctx_unlock(group_leader, gctx);\n \tmutex_unlock(&ctx->mutex);\n \n \tif (task) {\n@@ -367,7 +386,7 @@\n \n err_locked:\n \tif (move_group)\n-\t\tmutex_unlock(&gctx->mutex);\n+\t\tperf_event_ctx_unlock(group_leader, gctx);\n \tmutex_unlock(&ctx->mutex);\n /* err_file: */\n \tfput(event_file);",
        "function_modified_lines": {
            "added": [
                "\t\tgctx = __perf_event_ctx_lock_double(group_leader, ctx);",
                "",
                "\t\t}",
                "",
                "\t\t/*",
                "\t\t * Check if we raced against another sys_perf_event_open() call",
                "\t\t * moving the software group underneath us.",
                "\t\t */",
                "\t\tif (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {",
                "\t\t\t/*",
                "\t\t\t * If someone moved the group out from under us, check",
                "\t\t\t * if this new event wound up on the same ctx, if so",
                "\t\t\t * its the regular !move_group case, otherwise fail.",
                "\t\t\t */",
                "\t\t\tif (gctx != ctx) {",
                "\t\t\t\terr = -EINVAL;",
                "\t\t\t\tgoto err_locked;",
                "\t\t\t} else {",
                "\t\t\t\tperf_event_ctx_unlock(group_leader, gctx);",
                "\t\t\t\tmove_group = 0;",
                "\t\t\t}",
                "\t\tperf_event_ctx_unlock(group_leader, gctx);",
                "\t\tperf_event_ctx_unlock(group_leader, gctx);"
            ],
            "deleted": [
                "\t\tgctx = group_leader->ctx;",
                "\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);",
                "\t\tmutex_unlock(&gctx->mutex);",
                "\t\tmutex_unlock(&gctx->mutex);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in kernel/events/core.c in the Linux kernel before 4.9.7 allows local users to gain privileges via a crafted application that makes concurrent perf_event_open system calls for moving a software group into a hardware context.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2016-6786."
    },
    {
        "cve_id": "CVE-2017-6346",
        "code_before_change": "static int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f, *match;\n\tu8 type = type_flags & 0xff;\n\tu8 flags = type_flags >> 8;\n\tint err;\n\n\tswitch (type) {\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\n\t\t\treturn -EINVAL;\n\tcase PACKET_FANOUT_HASH:\n\tcase PACKET_FANOUT_LB:\n\tcase PACKET_FANOUT_CPU:\n\tcase PACKET_FANOUT_RND:\n\tcase PACKET_FANOUT_QM:\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (!po->running)\n\t\treturn -EINVAL;\n\n\tif (po->fanout)\n\t\treturn -EALREADY;\n\n\tif (type == PACKET_FANOUT_ROLLOVER ||\n\t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n\t\tpo->rollover = kzalloc(sizeof(*po->rollover), GFP_KERNEL);\n\t\tif (!po->rollover)\n\t\t\treturn -ENOMEM;\n\t\tatomic_long_set(&po->rollover->num, 0);\n\t\tatomic_long_set(&po->rollover->num_huge, 0);\n\t\tatomic_long_set(&po->rollover->num_failed, 0);\n\t}\n\n\tmutex_lock(&fanout_mutex);\n\tmatch = NULL;\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\tmatch = f;\n\t\t\tbreak;\n\t\t}\n\t}\n\terr = -EINVAL;\n\tif (match && match->flags != flags)\n\t\tgoto out;\n\tif (!match) {\n\t\terr = -ENOMEM;\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match)\n\t\t\tgoto out;\n\t\twrite_pnet(&match->net, sock_net(sk));\n\t\tmatch->id = id;\n\t\tmatch->type = type;\n\t\tmatch->flags = flags;\n\t\tINIT_LIST_HEAD(&match->list);\n\t\tspin_lock_init(&match->lock);\n\t\tatomic_set(&match->sk_ref, 0);\n\t\tfanout_init_data(match);\n\t\tmatch->prot_hook.type = po->prot_hook.type;\n\t\tmatch->prot_hook.dev = po->prot_hook.dev;\n\t\tmatch->prot_hook.func = packet_rcv_fanout;\n\t\tmatch->prot_hook.af_packet_priv = match;\n\t\tmatch->prot_hook.id_match = match_fanout_group;\n\t\tdev_add_pack(&match->prot_hook);\n\t\tlist_add(&match->list, &fanout_list);\n\t}\n\terr = -EINVAL;\n\tif (match->type == type &&\n\t    match->prot_hook.type == po->prot_hook.type &&\n\t    match->prot_hook.dev == po->prot_hook.dev) {\n\t\terr = -ENOSPC;\n\t\tif (atomic_read(&match->sk_ref) < PACKET_FANOUT_MAX) {\n\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\tpo->fanout = match;\n\t\t\tatomic_inc(&match->sk_ref);\n\t\t\t__fanout_link(sk, po);\n\t\t\terr = 0;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&fanout_mutex);\n\tif (err) {\n\t\tkfree(po->rollover);\n\t\tpo->rollover = NULL;\n\t}\n\treturn err;\n}",
        "code_after_change": "static int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n{\n\tstruct packet_rollover *rollover = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f, *match;\n\tu8 type = type_flags & 0xff;\n\tu8 flags = type_flags >> 8;\n\tint err;\n\n\tswitch (type) {\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\n\t\t\treturn -EINVAL;\n\tcase PACKET_FANOUT_HASH:\n\tcase PACKET_FANOUT_LB:\n\tcase PACKET_FANOUT_CPU:\n\tcase PACKET_FANOUT_RND:\n\tcase PACKET_FANOUT_QM:\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&fanout_mutex);\n\n\terr = -EINVAL;\n\tif (!po->running)\n\t\tgoto out;\n\n\terr = -EALREADY;\n\tif (po->fanout)\n\t\tgoto out;\n\n\tif (type == PACKET_FANOUT_ROLLOVER ||\n\t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n\t\terr = -ENOMEM;\n\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);\n\t\tif (!rollover)\n\t\t\tgoto out;\n\t\tatomic_long_set(&rollover->num, 0);\n\t\tatomic_long_set(&rollover->num_huge, 0);\n\t\tatomic_long_set(&rollover->num_failed, 0);\n\t\tpo->rollover = rollover;\n\t}\n\n\tmatch = NULL;\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\tmatch = f;\n\t\t\tbreak;\n\t\t}\n\t}\n\terr = -EINVAL;\n\tif (match && match->flags != flags)\n\t\tgoto out;\n\tif (!match) {\n\t\terr = -ENOMEM;\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match)\n\t\t\tgoto out;\n\t\twrite_pnet(&match->net, sock_net(sk));\n\t\tmatch->id = id;\n\t\tmatch->type = type;\n\t\tmatch->flags = flags;\n\t\tINIT_LIST_HEAD(&match->list);\n\t\tspin_lock_init(&match->lock);\n\t\tatomic_set(&match->sk_ref, 0);\n\t\tfanout_init_data(match);\n\t\tmatch->prot_hook.type = po->prot_hook.type;\n\t\tmatch->prot_hook.dev = po->prot_hook.dev;\n\t\tmatch->prot_hook.func = packet_rcv_fanout;\n\t\tmatch->prot_hook.af_packet_priv = match;\n\t\tmatch->prot_hook.id_match = match_fanout_group;\n\t\tdev_add_pack(&match->prot_hook);\n\t\tlist_add(&match->list, &fanout_list);\n\t}\n\terr = -EINVAL;\n\tif (match->type == type &&\n\t    match->prot_hook.type == po->prot_hook.type &&\n\t    match->prot_hook.dev == po->prot_hook.dev) {\n\t\terr = -ENOSPC;\n\t\tif (atomic_read(&match->sk_ref) < PACKET_FANOUT_MAX) {\n\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\tpo->fanout = match;\n\t\t\tatomic_inc(&match->sk_ref);\n\t\t\t__fanout_link(sk, po);\n\t\t\terr = 0;\n\t\t}\n\t}\nout:\n\tif (err && rollover) {\n\t\tkfree(rollover);\n\t\tpo->rollover = NULL;\n\t}\n\tmutex_unlock(&fanout_mutex);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n static int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n {\n+\tstruct packet_rollover *rollover = NULL;\n \tstruct packet_sock *po = pkt_sk(sk);\n \tstruct packet_fanout *f, *match;\n \tu8 type = type_flags & 0xff;\n@@ -22,23 +23,28 @@\n \t\treturn -EINVAL;\n \t}\n \n+\tmutex_lock(&fanout_mutex);\n+\n+\terr = -EINVAL;\n \tif (!po->running)\n-\t\treturn -EINVAL;\n+\t\tgoto out;\n \n+\terr = -EALREADY;\n \tif (po->fanout)\n-\t\treturn -EALREADY;\n+\t\tgoto out;\n \n \tif (type == PACKET_FANOUT_ROLLOVER ||\n \t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n-\t\tpo->rollover = kzalloc(sizeof(*po->rollover), GFP_KERNEL);\n-\t\tif (!po->rollover)\n-\t\t\treturn -ENOMEM;\n-\t\tatomic_long_set(&po->rollover->num, 0);\n-\t\tatomic_long_set(&po->rollover->num_huge, 0);\n-\t\tatomic_long_set(&po->rollover->num_failed, 0);\n+\t\terr = -ENOMEM;\n+\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);\n+\t\tif (!rollover)\n+\t\t\tgoto out;\n+\t\tatomic_long_set(&rollover->num, 0);\n+\t\tatomic_long_set(&rollover->num_huge, 0);\n+\t\tatomic_long_set(&rollover->num_failed, 0);\n+\t\tpo->rollover = rollover;\n \t}\n \n-\tmutex_lock(&fanout_mutex);\n \tmatch = NULL;\n \tlist_for_each_entry(f, &fanout_list, list) {\n \t\tif (f->id == id &&\n@@ -85,10 +91,10 @@\n \t\t}\n \t}\n out:\n-\tmutex_unlock(&fanout_mutex);\n-\tif (err) {\n-\t\tkfree(po->rollover);\n+\tif (err && rollover) {\n+\t\tkfree(rollover);\n \t\tpo->rollover = NULL;\n \t}\n+\tmutex_unlock(&fanout_mutex);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct packet_rollover *rollover = NULL;",
                "\tmutex_lock(&fanout_mutex);",
                "",
                "\terr = -EINVAL;",
                "\t\tgoto out;",
                "\terr = -EALREADY;",
                "\t\tgoto out;",
                "\t\terr = -ENOMEM;",
                "\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);",
                "\t\tif (!rollover)",
                "\t\t\tgoto out;",
                "\t\tatomic_long_set(&rollover->num, 0);",
                "\t\tatomic_long_set(&rollover->num_huge, 0);",
                "\t\tatomic_long_set(&rollover->num_failed, 0);",
                "\t\tpo->rollover = rollover;",
                "\tif (err && rollover) {",
                "\t\tkfree(rollover);",
                "\tmutex_unlock(&fanout_mutex);"
            ],
            "deleted": [
                "\t\treturn -EINVAL;",
                "\t\treturn -EALREADY;",
                "\t\tpo->rollover = kzalloc(sizeof(*po->rollover), GFP_KERNEL);",
                "\t\tif (!po->rollover)",
                "\t\t\treturn -ENOMEM;",
                "\t\tatomic_long_set(&po->rollover->num, 0);",
                "\t\tatomic_long_set(&po->rollover->num_huge, 0);",
                "\t\tatomic_long_set(&po->rollover->num_failed, 0);",
                "\tmutex_lock(&fanout_mutex);",
                "\tmutex_unlock(&fanout_mutex);",
                "\tif (err) {",
                "\t\tkfree(po->rollover);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in net/packet/af_packet.c in the Linux kernel before 4.9.13 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via a multithreaded application that makes PACKET_FANOUT setsockopt system calls."
    },
    {
        "cve_id": "CVE-2017-6346",
        "code_before_change": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tf = po->fanout;\n\tif (!f)\n\t\treturn;\n\n\tmutex_lock(&fanout_mutex);\n\tpo->fanout = NULL;\n\n\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\tlist_del(&f->list);\n\t\tdev_remove_pack(&f->prot_hook);\n\t\tfanout_release_data(f);\n\t\tkfree(f);\n\t}\n\tmutex_unlock(&fanout_mutex);\n\n\tif (po->rollover)\n\t\tkfree_rcu(po->rollover, rcu);\n}",
        "code_after_change": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tmutex_lock(&fanout_mutex);\n\tf = po->fanout;\n\tif (f) {\n\t\tpo->fanout = NULL;\n\n\t\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\t\tlist_del(&f->list);\n\t\t\tdev_remove_pack(&f->prot_hook);\n\t\t\tfanout_release_data(f);\n\t\t\tkfree(f);\n\t\t}\n\n\t\tif (po->rollover)\n\t\t\tkfree_rcu(po->rollover, rcu);\n\t}\n\tmutex_unlock(&fanout_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,21 +3,20 @@\n \tstruct packet_sock *po = pkt_sk(sk);\n \tstruct packet_fanout *f;\n \n+\tmutex_lock(&fanout_mutex);\n \tf = po->fanout;\n-\tif (!f)\n-\t\treturn;\n+\tif (f) {\n+\t\tpo->fanout = NULL;\n \n-\tmutex_lock(&fanout_mutex);\n-\tpo->fanout = NULL;\n+\t\tif (atomic_dec_and_test(&f->sk_ref)) {\n+\t\t\tlist_del(&f->list);\n+\t\t\tdev_remove_pack(&f->prot_hook);\n+\t\t\tfanout_release_data(f);\n+\t\t\tkfree(f);\n+\t\t}\n \n-\tif (atomic_dec_and_test(&f->sk_ref)) {\n-\t\tlist_del(&f->list);\n-\t\tdev_remove_pack(&f->prot_hook);\n-\t\tfanout_release_data(f);\n-\t\tkfree(f);\n+\t\tif (po->rollover)\n+\t\t\tkfree_rcu(po->rollover, rcu);\n \t}\n \tmutex_unlock(&fanout_mutex);\n-\n-\tif (po->rollover)\n-\t\tkfree_rcu(po->rollover, rcu);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&fanout_mutex);",
                "\tif (f) {",
                "\t\tpo->fanout = NULL;",
                "\t\tif (atomic_dec_and_test(&f->sk_ref)) {",
                "\t\t\tlist_del(&f->list);",
                "\t\t\tdev_remove_pack(&f->prot_hook);",
                "\t\t\tfanout_release_data(f);",
                "\t\t\tkfree(f);",
                "\t\t}",
                "\t\tif (po->rollover)",
                "\t\t\tkfree_rcu(po->rollover, rcu);"
            ],
            "deleted": [
                "\tif (!f)",
                "\t\treturn;",
                "\tmutex_lock(&fanout_mutex);",
                "\tpo->fanout = NULL;",
                "\tif (atomic_dec_and_test(&f->sk_ref)) {",
                "\t\tlist_del(&f->list);",
                "\t\tdev_remove_pack(&f->prot_hook);",
                "\t\tfanout_release_data(f);",
                "\t\tkfree(f);",
                "",
                "\tif (po->rollover)",
                "\t\tkfree_rcu(po->rollover, rcu);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in net/packet/af_packet.c in the Linux kernel before 4.9.13 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via a multithreaded application that makes PACKET_FANOUT setsockopt system calls."
    },
    {
        "cve_id": "CVE-2017-6874",
        "code_before_change": "static void put_ucounts(struct ucounts *ucounts)\n{\n\tunsigned long flags;\n\n\tif (atomic_dec_and_test(&ucounts->count)) {\n\t\tspin_lock_irqsave(&ucounts_lock, flags);\n\t\thlist_del_init(&ucounts->node);\n\t\tspin_unlock_irqrestore(&ucounts_lock, flags);\n\n\t\tkfree(ucounts);\n\t}\n}",
        "code_after_change": "static void put_ucounts(struct ucounts *ucounts)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ucounts_lock, flags);\n\tucounts->count -= 1;\n\tif (!ucounts->count)\n\t\thlist_del_init(&ucounts->node);\n\telse\n\t\tucounts = NULL;\n\tspin_unlock_irqrestore(&ucounts_lock, flags);\n\n\tkfree(ucounts);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,11 +2,13 @@\n {\n \tunsigned long flags;\n \n-\tif (atomic_dec_and_test(&ucounts->count)) {\n-\t\tspin_lock_irqsave(&ucounts_lock, flags);\n+\tspin_lock_irqsave(&ucounts_lock, flags);\n+\tucounts->count -= 1;\n+\tif (!ucounts->count)\n \t\thlist_del_init(&ucounts->node);\n-\t\tspin_unlock_irqrestore(&ucounts_lock, flags);\n+\telse\n+\t\tucounts = NULL;\n+\tspin_unlock_irqrestore(&ucounts_lock, flags);\n \n-\t\tkfree(ucounts);\n-\t}\n+\tkfree(ucounts);\n }",
        "function_modified_lines": {
            "added": [
                "\tspin_lock_irqsave(&ucounts_lock, flags);",
                "\tucounts->count -= 1;",
                "\tif (!ucounts->count)",
                "\telse",
                "\t\tucounts = NULL;",
                "\tspin_unlock_irqrestore(&ucounts_lock, flags);",
                "\tkfree(ucounts);"
            ],
            "deleted": [
                "\tif (atomic_dec_and_test(&ucounts->count)) {",
                "\t\tspin_lock_irqsave(&ucounts_lock, flags);",
                "\t\tspin_unlock_irqrestore(&ucounts_lock, flags);",
                "\t\tkfree(ucounts);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/ucount.c in the Linux kernel through 4.10.2 allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via crafted system calls that leverage certain decrement behavior that causes incorrect interaction between put_ucounts and get_ucounts."
    },
    {
        "cve_id": "CVE-2017-6874",
        "code_before_change": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tatomic_set(&new->count, 0);\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n\t\tucounts = NULL;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
        "code_after_change": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tnew->count = 0;\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (ucounts->count == INT_MAX)\n\t\tucounts = NULL;\n\telse\n\t\tucounts->count += 1;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,7 +14,7 @@\n \n \t\tnew->ns = ns;\n \t\tnew->uid = uid;\n-\t\tatomic_set(&new->count, 0);\n+\t\tnew->count = 0;\n \n \t\tspin_lock_irq(&ucounts_lock);\n \t\tucounts = find_ucounts(ns, uid, hashent);\n@@ -25,8 +25,10 @@\n \t\t\tucounts = new;\n \t\t}\n \t}\n-\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n+\tif (ucounts->count == INT_MAX)\n \t\tucounts = NULL;\n+\telse\n+\t\tucounts->count += 1;\n \tspin_unlock_irq(&ucounts_lock);\n \treturn ucounts;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tnew->count = 0;",
                "\tif (ucounts->count == INT_MAX)",
                "\telse",
                "\t\tucounts->count += 1;"
            ],
            "deleted": [
                "\t\tatomic_set(&new->count, 0);",
                "\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/ucount.c in the Linux kernel through 4.10.2 allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via crafted system calls that leverage certain decrement behavior that causes incorrect interaction between put_ucounts and get_ucounts."
    },
    {
        "cve_id": "CVE-2017-7533",
        "code_before_change": "struct dentry *debugfs_rename(struct dentry *old_dir, struct dentry *old_dentry,\n\t\tstruct dentry *new_dir, const char *new_name)\n{\n\tint error;\n\tstruct dentry *dentry = NULL, *trap;\n\tconst char *old_name;\n\n\ttrap = lock_rename(new_dir, old_dir);\n\t/* Source or destination directories don't exist? */\n\tif (d_really_is_negative(old_dir) || d_really_is_negative(new_dir))\n\t\tgoto exit;\n\t/* Source does not exist, cyclic rename, or mountpoint? */\n\tif (d_really_is_negative(old_dentry) || old_dentry == trap ||\n\t    d_mountpoint(old_dentry))\n\t\tgoto exit;\n\tdentry = lookup_one_len(new_name, new_dir, strlen(new_name));\n\t/* Lookup failed, cyclic rename or target exists? */\n\tif (IS_ERR(dentry) || dentry == trap || d_really_is_positive(dentry))\n\t\tgoto exit;\n\n\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n\n\terror = simple_rename(d_inode(old_dir), old_dentry, d_inode(new_dir),\n\t\t\t      dentry, 0);\n\tif (error) {\n\t\tfsnotify_oldname_free(old_name);\n\t\tgoto exit;\n\t}\n\td_move(old_dentry, dentry);\n\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,\n\t\td_is_dir(old_dentry),\n\t\tNULL, old_dentry);\n\tfsnotify_oldname_free(old_name);\n\tunlock_rename(new_dir, old_dir);\n\tdput(dentry);\n\treturn old_dentry;\nexit:\n\tif (dentry && !IS_ERR(dentry))\n\t\tdput(dentry);\n\tunlock_rename(new_dir, old_dir);\n\treturn NULL;\n}",
        "code_after_change": "struct dentry *debugfs_rename(struct dentry *old_dir, struct dentry *old_dentry,\n\t\tstruct dentry *new_dir, const char *new_name)\n{\n\tint error;\n\tstruct dentry *dentry = NULL, *trap;\n\tstruct name_snapshot old_name;\n\n\ttrap = lock_rename(new_dir, old_dir);\n\t/* Source or destination directories don't exist? */\n\tif (d_really_is_negative(old_dir) || d_really_is_negative(new_dir))\n\t\tgoto exit;\n\t/* Source does not exist, cyclic rename, or mountpoint? */\n\tif (d_really_is_negative(old_dentry) || old_dentry == trap ||\n\t    d_mountpoint(old_dentry))\n\t\tgoto exit;\n\tdentry = lookup_one_len(new_name, new_dir, strlen(new_name));\n\t/* Lookup failed, cyclic rename or target exists? */\n\tif (IS_ERR(dentry) || dentry == trap || d_really_is_positive(dentry))\n\t\tgoto exit;\n\n\ttake_dentry_name_snapshot(&old_name, old_dentry);\n\n\terror = simple_rename(d_inode(old_dir), old_dentry, d_inode(new_dir),\n\t\t\t      dentry, 0);\n\tif (error) {\n\t\trelease_dentry_name_snapshot(&old_name);\n\t\tgoto exit;\n\t}\n\td_move(old_dentry, dentry);\n\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name.name,\n\t\td_is_dir(old_dentry),\n\t\tNULL, old_dentry);\n\trelease_dentry_name_snapshot(&old_name);\n\tunlock_rename(new_dir, old_dir);\n\tdput(dentry);\n\treturn old_dentry;\nexit:\n\tif (dentry && !IS_ERR(dentry))\n\t\tdput(dentry);\n\tunlock_rename(new_dir, old_dir);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n {\n \tint error;\n \tstruct dentry *dentry = NULL, *trap;\n-\tconst char *old_name;\n+\tstruct name_snapshot old_name;\n \n \ttrap = lock_rename(new_dir, old_dir);\n \t/* Source or destination directories don't exist? */\n@@ -18,19 +18,19 @@\n \tif (IS_ERR(dentry) || dentry == trap || d_really_is_positive(dentry))\n \t\tgoto exit;\n \n-\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n+\ttake_dentry_name_snapshot(&old_name, old_dentry);\n \n \terror = simple_rename(d_inode(old_dir), old_dentry, d_inode(new_dir),\n \t\t\t      dentry, 0);\n \tif (error) {\n-\t\tfsnotify_oldname_free(old_name);\n+\t\trelease_dentry_name_snapshot(&old_name);\n \t\tgoto exit;\n \t}\n \td_move(old_dentry, dentry);\n-\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,\n+\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name.name,\n \t\td_is_dir(old_dentry),\n \t\tNULL, old_dentry);\n-\tfsnotify_oldname_free(old_name);\n+\trelease_dentry_name_snapshot(&old_name);\n \tunlock_rename(new_dir, old_dir);\n \tdput(dentry);\n \treturn old_dentry;",
        "function_modified_lines": {
            "added": [
                "\tstruct name_snapshot old_name;",
                "\ttake_dentry_name_snapshot(&old_name, old_dentry);",
                "\t\trelease_dentry_name_snapshot(&old_name);",
                "\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name.name,",
                "\trelease_dentry_name_snapshot(&old_name);"
            ],
            "deleted": [
                "\tconst char *old_name;",
                "\told_name = fsnotify_oldname_init(old_dentry->d_name.name);",
                "\t\tfsnotify_oldname_free(old_name);",
                "\tfsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,",
                "\tfsnotify_oldname_free(old_name);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the fsnotify implementation in the Linux kernel through 4.12.4 allows local users to gain privileges or cause a denial of service (memory corruption) via a crafted application that leverages simultaneous execution of the inotify_handle_event and vfs_rename functions."
    },
    {
        "cve_id": "CVE-2017-7533",
        "code_before_change": "int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n\t       struct inode *new_dir, struct dentry *new_dentry,\n\t       struct inode **delegated_inode, unsigned int flags)\n{\n\tint error;\n\tbool is_dir = d_is_dir(old_dentry);\n\tconst unsigned char *old_name;\n\tstruct inode *source = old_dentry->d_inode;\n\tstruct inode *target = new_dentry->d_inode;\n\tbool new_is_dir = false;\n\tunsigned max_links = new_dir->i_sb->s_max_links;\n\n\tif (source == target)\n\t\treturn 0;\n\n\terror = may_delete(old_dir, old_dentry, is_dir);\n\tif (error)\n\t\treturn error;\n\n\tif (!target) {\n\t\terror = may_create(new_dir, new_dentry);\n\t} else {\n\t\tnew_is_dir = d_is_dir(new_dentry);\n\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\terror = may_delete(new_dir, new_dentry, is_dir);\n\t\telse\n\t\t\terror = may_delete(new_dir, new_dentry, new_is_dir);\n\t}\n\tif (error)\n\t\treturn error;\n\n\tif (!old_dir->i_op->rename)\n\t\treturn -EPERM;\n\n\t/*\n\t * If we are going to change the parent - check write permissions,\n\t * we'll need to flip '..'.\n\t */\n\tif (new_dir != old_dir) {\n\t\tif (is_dir) {\n\t\t\terror = inode_permission(source, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t\tif ((flags & RENAME_EXCHANGE) && new_is_dir) {\n\t\t\terror = inode_permission(target, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t}\n\n\terror = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,\n\t\t\t\t      flags);\n\tif (error)\n\t\treturn error;\n\n\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n\tdget(new_dentry);\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_lock(target);\n\n\terror = -EBUSY;\n\tif (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))\n\t\tgoto out;\n\n\tif (max_links && new_dir != old_dir) {\n\t\terror = -EMLINK;\n\t\tif (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t\tif ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&\n\t\t    old_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t}\n\tif (is_dir && !(flags & RENAME_EXCHANGE) && target)\n\t\tshrink_dcache_parent(new_dentry);\n\tif (!is_dir) {\n\t\terror = try_break_deleg(source, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\tif (target && !new_is_dir) {\n\t\terror = try_break_deleg(target, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\terror = old_dir->i_op->rename(old_dir, old_dentry,\n\t\t\t\t       new_dir, new_dentry, flags);\n\tif (error)\n\t\tgoto out;\n\n\tif (!(flags & RENAME_EXCHANGE) && target) {\n\t\tif (is_dir)\n\t\t\ttarget->i_flags |= S_DEAD;\n\t\tdont_mount(new_dentry);\n\t\tdetach_mounts(new_dentry);\n\t}\n\tif (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\td_move(old_dentry, new_dentry);\n\t\telse\n\t\t\td_exchange(old_dentry, new_dentry);\n\t}\nout:\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tunlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_unlock(target);\n\tdput(new_dentry);\n\tif (!error) {\n\t\tfsnotify_move(old_dir, new_dir, old_name, is_dir,\n\t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n\t\tif (flags & RENAME_EXCHANGE) {\n\t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n\t\t\t\t      new_is_dir, NULL, new_dentry);\n\t\t}\n\t}\n\tfsnotify_oldname_free(old_name);\n\n\treturn error;\n}",
        "code_after_change": "int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n\t       struct inode *new_dir, struct dentry *new_dentry,\n\t       struct inode **delegated_inode, unsigned int flags)\n{\n\tint error;\n\tbool is_dir = d_is_dir(old_dentry);\n\tstruct inode *source = old_dentry->d_inode;\n\tstruct inode *target = new_dentry->d_inode;\n\tbool new_is_dir = false;\n\tunsigned max_links = new_dir->i_sb->s_max_links;\n\tstruct name_snapshot old_name;\n\n\tif (source == target)\n\t\treturn 0;\n\n\terror = may_delete(old_dir, old_dentry, is_dir);\n\tif (error)\n\t\treturn error;\n\n\tif (!target) {\n\t\terror = may_create(new_dir, new_dentry);\n\t} else {\n\t\tnew_is_dir = d_is_dir(new_dentry);\n\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\terror = may_delete(new_dir, new_dentry, is_dir);\n\t\telse\n\t\t\terror = may_delete(new_dir, new_dentry, new_is_dir);\n\t}\n\tif (error)\n\t\treturn error;\n\n\tif (!old_dir->i_op->rename)\n\t\treturn -EPERM;\n\n\t/*\n\t * If we are going to change the parent - check write permissions,\n\t * we'll need to flip '..'.\n\t */\n\tif (new_dir != old_dir) {\n\t\tif (is_dir) {\n\t\t\terror = inode_permission(source, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t\tif ((flags & RENAME_EXCHANGE) && new_is_dir) {\n\t\t\terror = inode_permission(target, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t}\n\n\terror = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,\n\t\t\t\t      flags);\n\tif (error)\n\t\treturn error;\n\n\ttake_dentry_name_snapshot(&old_name, old_dentry);\n\tdget(new_dentry);\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_lock(target);\n\n\terror = -EBUSY;\n\tif (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))\n\t\tgoto out;\n\n\tif (max_links && new_dir != old_dir) {\n\t\terror = -EMLINK;\n\t\tif (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t\tif ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&\n\t\t    old_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t}\n\tif (is_dir && !(flags & RENAME_EXCHANGE) && target)\n\t\tshrink_dcache_parent(new_dentry);\n\tif (!is_dir) {\n\t\terror = try_break_deleg(source, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\tif (target && !new_is_dir) {\n\t\terror = try_break_deleg(target, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\terror = old_dir->i_op->rename(old_dir, old_dentry,\n\t\t\t\t       new_dir, new_dentry, flags);\n\tif (error)\n\t\tgoto out;\n\n\tif (!(flags & RENAME_EXCHANGE) && target) {\n\t\tif (is_dir)\n\t\t\ttarget->i_flags |= S_DEAD;\n\t\tdont_mount(new_dentry);\n\t\tdetach_mounts(new_dentry);\n\t}\n\tif (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\td_move(old_dentry, new_dentry);\n\t\telse\n\t\t\td_exchange(old_dentry, new_dentry);\n\t}\nout:\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tunlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_unlock(target);\n\tdput(new_dentry);\n\tif (!error) {\n\t\tfsnotify_move(old_dir, new_dir, old_name.name, is_dir,\n\t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n\t\tif (flags & RENAME_EXCHANGE) {\n\t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n\t\t\t\t      new_is_dir, NULL, new_dentry);\n\t\t}\n\t}\n\trelease_dentry_name_snapshot(&old_name);\n\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,11 +4,11 @@\n {\n \tint error;\n \tbool is_dir = d_is_dir(old_dentry);\n-\tconst unsigned char *old_name;\n \tstruct inode *source = old_dentry->d_inode;\n \tstruct inode *target = new_dentry->d_inode;\n \tbool new_is_dir = false;\n \tunsigned max_links = new_dir->i_sb->s_max_links;\n+\tstruct name_snapshot old_name;\n \n \tif (source == target)\n \t\treturn 0;\n@@ -55,7 +55,7 @@\n \tif (error)\n \t\treturn error;\n \n-\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n+\ttake_dentry_name_snapshot(&old_name, old_dentry);\n \tdget(new_dentry);\n \tif (!is_dir || (flags & RENAME_EXCHANGE))\n \t\tlock_two_nondirectories(source, target);\n@@ -110,14 +110,14 @@\n \t\tinode_unlock(target);\n \tdput(new_dentry);\n \tif (!error) {\n-\t\tfsnotify_move(old_dir, new_dir, old_name, is_dir,\n+\t\tfsnotify_move(old_dir, new_dir, old_name.name, is_dir,\n \t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n \t\tif (flags & RENAME_EXCHANGE) {\n \t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n \t\t\t\t      new_is_dir, NULL, new_dentry);\n \t\t}\n \t}\n-\tfsnotify_oldname_free(old_name);\n+\trelease_dentry_name_snapshot(&old_name);\n \n \treturn error;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct name_snapshot old_name;",
                "\ttake_dentry_name_snapshot(&old_name, old_dentry);",
                "\t\tfsnotify_move(old_dir, new_dir, old_name.name, is_dir,",
                "\trelease_dentry_name_snapshot(&old_name);"
            ],
            "deleted": [
                "\tconst unsigned char *old_name;",
                "\told_name = fsnotify_oldname_init(old_dentry->d_name.name);",
                "\t\tfsnotify_move(old_dir, new_dir, old_name, is_dir,",
                "\tfsnotify_oldname_free(old_name);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the fsnotify implementation in the Linux kernel through 4.12.4 allows local users to gain privileges or cause a denial of service (memory corruption) via a crafted application that leverages simultaneous execution of the inotify_handle_event and vfs_rename functions."
    },
    {
        "cve_id": "CVE-2017-7533",
        "code_before_change": "int __fsnotify_parent(const struct path *path, struct dentry *dentry, __u32 mask)\n{\n\tstruct dentry *parent;\n\tstruct inode *p_inode;\n\tint ret = 0;\n\n\tif (!dentry)\n\t\tdentry = path->dentry;\n\n\tif (!(dentry->d_flags & DCACHE_FSNOTIFY_PARENT_WATCHED))\n\t\treturn 0;\n\n\tparent = dget_parent(dentry);\n\tp_inode = parent->d_inode;\n\n\tif (unlikely(!fsnotify_inode_watches_children(p_inode)))\n\t\t__fsnotify_update_child_dentry_flags(p_inode);\n\telse if (p_inode->i_fsnotify_mask & mask) {\n\t\t/* we are notifying a parent so come up with the new mask which\n\t\t * specifies these are events which came from a child. */\n\t\tmask |= FS_EVENT_ON_CHILD;\n\n\t\tif (path)\n\t\t\tret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,\n\t\t\t\t       dentry->d_name.name, 0);\n\t\telse\n\t\t\tret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,\n\t\t\t\t       dentry->d_name.name, 0);\n\t}\n\n\tdput(parent);\n\n\treturn ret;\n}",
        "code_after_change": "int __fsnotify_parent(const struct path *path, struct dentry *dentry, __u32 mask)\n{\n\tstruct dentry *parent;\n\tstruct inode *p_inode;\n\tint ret = 0;\n\n\tif (!dentry)\n\t\tdentry = path->dentry;\n\n\tif (!(dentry->d_flags & DCACHE_FSNOTIFY_PARENT_WATCHED))\n\t\treturn 0;\n\n\tparent = dget_parent(dentry);\n\tp_inode = parent->d_inode;\n\n\tif (unlikely(!fsnotify_inode_watches_children(p_inode)))\n\t\t__fsnotify_update_child_dentry_flags(p_inode);\n\telse if (p_inode->i_fsnotify_mask & mask) {\n\t\tstruct name_snapshot name;\n\n\t\t/* we are notifying a parent so come up with the new mask which\n\t\t * specifies these are events which came from a child. */\n\t\tmask |= FS_EVENT_ON_CHILD;\n\n\t\ttake_dentry_name_snapshot(&name, dentry);\n\t\tif (path)\n\t\t\tret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,\n\t\t\t\t       name.name, 0);\n\t\telse\n\t\t\tret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,\n\t\t\t\t       name.name, 0);\n\t\trelease_dentry_name_snapshot(&name);\n\t}\n\n\tdput(parent);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,16 +16,20 @@\n \tif (unlikely(!fsnotify_inode_watches_children(p_inode)))\n \t\t__fsnotify_update_child_dentry_flags(p_inode);\n \telse if (p_inode->i_fsnotify_mask & mask) {\n+\t\tstruct name_snapshot name;\n+\n \t\t/* we are notifying a parent so come up with the new mask which\n \t\t * specifies these are events which came from a child. */\n \t\tmask |= FS_EVENT_ON_CHILD;\n \n+\t\ttake_dentry_name_snapshot(&name, dentry);\n \t\tif (path)\n \t\t\tret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,\n-\t\t\t\t       dentry->d_name.name, 0);\n+\t\t\t\t       name.name, 0);\n \t\telse\n \t\t\tret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,\n-\t\t\t\t       dentry->d_name.name, 0);\n+\t\t\t\t       name.name, 0);\n+\t\trelease_dentry_name_snapshot(&name);\n \t}\n \n \tdput(parent);",
        "function_modified_lines": {
            "added": [
                "\t\tstruct name_snapshot name;",
                "",
                "\t\ttake_dentry_name_snapshot(&name, dentry);",
                "\t\t\t\t       name.name, 0);",
                "\t\t\t\t       name.name, 0);",
                "\t\trelease_dentry_name_snapshot(&name);"
            ],
            "deleted": [
                "\t\t\t\t       dentry->d_name.name, 0);",
                "\t\t\t\t       dentry->d_name.name, 0);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the fsnotify implementation in the Linux kernel through 4.12.4 allows local users to gain privileges or cause a denial of service (memory corruption) via a crafted application that leverages simultaneous execution of the inotify_handle_event and vfs_rename functions."
    },
    {
        "cve_id": "CVE-2018-1000004",
        "code_before_change": "static long snd_seq_ioctl(struct file *file, unsigned int cmd,\n\t\t\t  unsigned long arg)\n{\n\tstruct snd_seq_client *client = file->private_data;\n\t/* To use kernel stack for ioctl data. */\n\tunion {\n\t\tint pversion;\n\t\tint client_id;\n\t\tstruct snd_seq_system_info\tsystem_info;\n\t\tstruct snd_seq_running_info\trunning_info;\n\t\tstruct snd_seq_client_info\tclient_info;\n\t\tstruct snd_seq_port_info\tport_info;\n\t\tstruct snd_seq_port_subscribe\tport_subscribe;\n\t\tstruct snd_seq_queue_info\tqueue_info;\n\t\tstruct snd_seq_queue_status\tqueue_status;\n\t\tstruct snd_seq_queue_tempo\ttempo;\n\t\tstruct snd_seq_queue_timer\tqueue_timer;\n\t\tstruct snd_seq_queue_client\tqueue_client;\n\t\tstruct snd_seq_client_pool\tclient_pool;\n\t\tstruct snd_seq_remove_events\tremove_events;\n\t\tstruct snd_seq_query_subs\tquery_subs;\n\t} buf;\n\tconst struct ioctl_handler *handler;\n\tunsigned long size;\n\tint err;\n\n\tif (snd_BUG_ON(!client))\n\t\treturn -ENXIO;\n\n\tfor (handler = ioctl_handlers; handler->cmd > 0; ++handler) {\n\t\tif (handler->cmd == cmd)\n\t\t\tbreak;\n\t}\n\tif (handler->cmd == 0)\n\t\treturn -ENOTTY;\n\n\tmemset(&buf, 0, sizeof(buf));\n\n\t/*\n\t * All of ioctl commands for ALSA sequencer get an argument of size\n\t * within 13 bits. We can safely pick up the size from the command.\n\t */\n\tsize = _IOC_SIZE(handler->cmd);\n\tif (handler->cmd & IOC_IN) {\n\t\tif (copy_from_user(&buf, (const void __user *)arg, size))\n\t\t\treturn -EFAULT;\n\t}\n\n\terr = handler->func(client, &buf);\n\tif (err >= 0) {\n\t\t/* Some commands includes a bug in 'dir' field. */\n\t\tif (handler->cmd == SNDRV_SEQ_IOCTL_SET_QUEUE_CLIENT ||\n\t\t    handler->cmd == SNDRV_SEQ_IOCTL_SET_CLIENT_POOL ||\n\t\t    (handler->cmd & IOC_OUT))\n\t\t\tif (copy_to_user((void __user *)arg, &buf, size))\n\t\t\t\treturn -EFAULT;\n\t}\n\n\treturn err;\n}",
        "code_after_change": "static long snd_seq_ioctl(struct file *file, unsigned int cmd,\n\t\t\t  unsigned long arg)\n{\n\tstruct snd_seq_client *client = file->private_data;\n\t/* To use kernel stack for ioctl data. */\n\tunion {\n\t\tint pversion;\n\t\tint client_id;\n\t\tstruct snd_seq_system_info\tsystem_info;\n\t\tstruct snd_seq_running_info\trunning_info;\n\t\tstruct snd_seq_client_info\tclient_info;\n\t\tstruct snd_seq_port_info\tport_info;\n\t\tstruct snd_seq_port_subscribe\tport_subscribe;\n\t\tstruct snd_seq_queue_info\tqueue_info;\n\t\tstruct snd_seq_queue_status\tqueue_status;\n\t\tstruct snd_seq_queue_tempo\ttempo;\n\t\tstruct snd_seq_queue_timer\tqueue_timer;\n\t\tstruct snd_seq_queue_client\tqueue_client;\n\t\tstruct snd_seq_client_pool\tclient_pool;\n\t\tstruct snd_seq_remove_events\tremove_events;\n\t\tstruct snd_seq_query_subs\tquery_subs;\n\t} buf;\n\tconst struct ioctl_handler *handler;\n\tunsigned long size;\n\tint err;\n\n\tif (snd_BUG_ON(!client))\n\t\treturn -ENXIO;\n\n\tfor (handler = ioctl_handlers; handler->cmd > 0; ++handler) {\n\t\tif (handler->cmd == cmd)\n\t\t\tbreak;\n\t}\n\tif (handler->cmd == 0)\n\t\treturn -ENOTTY;\n\n\tmemset(&buf, 0, sizeof(buf));\n\n\t/*\n\t * All of ioctl commands for ALSA sequencer get an argument of size\n\t * within 13 bits. We can safely pick up the size from the command.\n\t */\n\tsize = _IOC_SIZE(handler->cmd);\n\tif (handler->cmd & IOC_IN) {\n\t\tif (copy_from_user(&buf, (const void __user *)arg, size))\n\t\t\treturn -EFAULT;\n\t}\n\n\tmutex_lock(&client->ioctl_mutex);\n\terr = handler->func(client, &buf);\n\tmutex_unlock(&client->ioctl_mutex);\n\tif (err >= 0) {\n\t\t/* Some commands includes a bug in 'dir' field. */\n\t\tif (handler->cmd == SNDRV_SEQ_IOCTL_SET_QUEUE_CLIENT ||\n\t\t    handler->cmd == SNDRV_SEQ_IOCTL_SET_CLIENT_POOL ||\n\t\t    (handler->cmd & IOC_OUT))\n\t\t\tif (copy_to_user((void __user *)arg, &buf, size))\n\t\t\t\treturn -EFAULT;\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -46,7 +46,9 @@\n \t\t\treturn -EFAULT;\n \t}\n \n+\tmutex_lock(&client->ioctl_mutex);\n \terr = handler->func(client, &buf);\n+\tmutex_unlock(&client->ioctl_mutex);\n \tif (err >= 0) {\n \t\t/* Some commands includes a bug in 'dir' field. */\n \t\tif (handler->cmd == SNDRV_SEQ_IOCTL_SET_QUEUE_CLIENT ||",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&client->ioctl_mutex);",
                "\tmutex_unlock(&client->ioctl_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux kernel 4.12, 3.10, 2.6 and possibly earlier versions a race condition vulnerability exists in the sound system, this can lead to a deadlock and denial of service condition."
    },
    {
        "cve_id": "CVE-2018-1000004",
        "code_before_change": "static struct snd_seq_client *seq_create_client1(int client_index, int poolsize)\n{\n\tunsigned long flags;\n\tint c;\n\tstruct snd_seq_client *client;\n\n\t/* init client data */\n\tclient = kzalloc(sizeof(*client), GFP_KERNEL);\n\tif (client == NULL)\n\t\treturn NULL;\n\tclient->pool = snd_seq_pool_new(poolsize);\n\tif (client->pool == NULL) {\n\t\tkfree(client);\n\t\treturn NULL;\n\t}\n\tclient->type = NO_CLIENT;\n\tsnd_use_lock_init(&client->use_lock);\n\trwlock_init(&client->ports_lock);\n\tmutex_init(&client->ports_mutex);\n\tINIT_LIST_HEAD(&client->ports_list_head);\n\n\t/* find free slot in the client table */\n\tspin_lock_irqsave(&clients_lock, flags);\n\tif (client_index < 0) {\n\t\tfor (c = SNDRV_SEQ_DYNAMIC_CLIENTS_BEGIN;\n\t\t     c < SNDRV_SEQ_MAX_CLIENTS;\n\t\t     c++) {\n\t\t\tif (clienttab[c] || clienttablock[c])\n\t\t\t\tcontinue;\n\t\t\tclienttab[client->number = c] = client;\n\t\t\tspin_unlock_irqrestore(&clients_lock, flags);\n\t\t\treturn client;\n\t\t}\n\t} else {\n\t\tif (clienttab[client_index] == NULL && !clienttablock[client_index]) {\n\t\t\tclienttab[client->number = client_index] = client;\n\t\t\tspin_unlock_irqrestore(&clients_lock, flags);\n\t\t\treturn client;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&clients_lock, flags);\n\tsnd_seq_pool_delete(&client->pool);\n\tkfree(client);\n\treturn NULL;\t/* no free slot found or busy, return failure code */\n}",
        "code_after_change": "static struct snd_seq_client *seq_create_client1(int client_index, int poolsize)\n{\n\tunsigned long flags;\n\tint c;\n\tstruct snd_seq_client *client;\n\n\t/* init client data */\n\tclient = kzalloc(sizeof(*client), GFP_KERNEL);\n\tif (client == NULL)\n\t\treturn NULL;\n\tclient->pool = snd_seq_pool_new(poolsize);\n\tif (client->pool == NULL) {\n\t\tkfree(client);\n\t\treturn NULL;\n\t}\n\tclient->type = NO_CLIENT;\n\tsnd_use_lock_init(&client->use_lock);\n\trwlock_init(&client->ports_lock);\n\tmutex_init(&client->ports_mutex);\n\tINIT_LIST_HEAD(&client->ports_list_head);\n\tmutex_init(&client->ioctl_mutex);\n\n\t/* find free slot in the client table */\n\tspin_lock_irqsave(&clients_lock, flags);\n\tif (client_index < 0) {\n\t\tfor (c = SNDRV_SEQ_DYNAMIC_CLIENTS_BEGIN;\n\t\t     c < SNDRV_SEQ_MAX_CLIENTS;\n\t\t     c++) {\n\t\t\tif (clienttab[c] || clienttablock[c])\n\t\t\t\tcontinue;\n\t\t\tclienttab[client->number = c] = client;\n\t\t\tspin_unlock_irqrestore(&clients_lock, flags);\n\t\t\treturn client;\n\t\t}\n\t} else {\n\t\tif (clienttab[client_index] == NULL && !clienttablock[client_index]) {\n\t\t\tclienttab[client->number = client_index] = client;\n\t\t\tspin_unlock_irqrestore(&clients_lock, flags);\n\t\t\treturn client;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&clients_lock, flags);\n\tsnd_seq_pool_delete(&client->pool);\n\tkfree(client);\n\treturn NULL;\t/* no free slot found or busy, return failure code */\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,6 +18,7 @@\n \trwlock_init(&client->ports_lock);\n \tmutex_init(&client->ports_mutex);\n \tINIT_LIST_HEAD(&client->ports_list_head);\n+\tmutex_init(&client->ioctl_mutex);\n \n \t/* find free slot in the client table */\n \tspin_lock_irqsave(&clients_lock, flags);",
        "function_modified_lines": {
            "added": [
                "\tmutex_init(&client->ioctl_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux kernel 4.12, 3.10, 2.6 and possibly earlier versions a race condition vulnerability exists in the sound system, this can lead to a deadlock and denial of service condition."
    },
    {
        "cve_id": "CVE-2018-12232",
        "code_before_change": "static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n{\n\tint err = simple_setattr(dentry, iattr);\n\n\tif (!err && (iattr->ia_valid & ATTR_UID)) {\n\t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n\n\t\tsock->sk->sk_uid = iattr->ia_uid;\n\t}\n\n\treturn err;\n}",
        "code_after_change": "static int sockfs_setattr(struct dentry *dentry, struct iattr *iattr)\n{\n\tint err = simple_setattr(dentry, iattr);\n\n\tif (!err && (iattr->ia_valid & ATTR_UID)) {\n\t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n\n\t\tif (sock->sk)\n\t\t\tsock->sk->sk_uid = iattr->ia_uid;\n\t\telse\n\t\t\terr = -ENOENT;\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,10 @@\n \tif (!err && (iattr->ia_valid & ATTR_UID)) {\n \t\tstruct socket *sock = SOCKET_I(d_inode(dentry));\n \n-\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\tif (sock->sk)\n+\t\t\tsock->sk->sk_uid = iattr->ia_uid;\n+\t\telse\n+\t\t\terr = -ENOENT;\n \t}\n \n \treturn err;",
        "function_modified_lines": {
            "added": [
                "\t\tif (sock->sk)",
                "\t\t\tsock->sk->sk_uid = iattr->ia_uid;",
                "\t\telse",
                "\t\t\terr = -ENOENT;"
            ],
            "deleted": [
                "\t\tsock->sk->sk_uid = iattr->ia_uid;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash."
    },
    {
        "cve_id": "CVE-2018-12232",
        "code_before_change": "static int sock_close(struct inode *inode, struct file *filp)\n{\n\tsock_release(SOCKET_I(inode));\n\treturn 0;\n}",
        "code_after_change": "static int sock_close(struct inode *inode, struct file *filp)\n{\n\t__sock_release(SOCKET_I(inode), inode);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,5 @@\n static int sock_close(struct inode *inode, struct file *filp)\n {\n-\tsock_release(SOCKET_I(inode));\n+\t__sock_release(SOCKET_I(inode), inode);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t__sock_release(SOCKET_I(inode), inode);"
            ],
            "deleted": [
                "\tsock_release(SOCKET_I(inode));"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash."
    },
    {
        "cve_id": "CVE-2018-12232",
        "code_before_change": "void sock_release(struct socket *sock)\n{\n\tif (sock->ops) {\n\t\tstruct module *owner = sock->ops->owner;\n\n\t\tsock->ops->release(sock);\n\t\tsock->ops = NULL;\n\t\tmodule_put(owner);\n\t}\n\n\tif (rcu_dereference_protected(sock->wq, 1)->fasync_list)\n\t\tpr_err(\"%s: fasync list not empty!\\n\", __func__);\n\n\tif (!sock->file) {\n\t\tiput(SOCK_INODE(sock));\n\t\treturn;\n\t}\n\tsock->file = NULL;\n}",
        "code_after_change": "void sock_release(struct socket *sock)\n{\n\t__sock_release(sock, NULL);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,19 +1,4 @@\n void sock_release(struct socket *sock)\n {\n-\tif (sock->ops) {\n-\t\tstruct module *owner = sock->ops->owner;\n-\n-\t\tsock->ops->release(sock);\n-\t\tsock->ops = NULL;\n-\t\tmodule_put(owner);\n-\t}\n-\n-\tif (rcu_dereference_protected(sock->wq, 1)->fasync_list)\n-\t\tpr_err(\"%s: fasync list not empty!\\n\", __func__);\n-\n-\tif (!sock->file) {\n-\t\tiput(SOCK_INODE(sock));\n-\t\treturn;\n-\t}\n-\tsock->file = NULL;\n+\t__sock_release(sock, NULL);\n }",
        "function_modified_lines": {
            "added": [
                "\t__sock_release(sock, NULL);"
            ],
            "deleted": [
                "\tif (sock->ops) {",
                "\t\tstruct module *owner = sock->ops->owner;",
                "",
                "\t\tsock->ops->release(sock);",
                "\t\tsock->ops = NULL;",
                "\t\tmodule_put(owner);",
                "\t}",
                "",
                "\tif (rcu_dereference_protected(sock->wq, 1)->fasync_list)",
                "\t\tpr_err(\"%s: fasync list not empty!\\n\", __func__);",
                "",
                "\tif (!sock->file) {",
                "\t\tiput(SOCK_INODE(sock));",
                "\t\treturn;",
                "\t}",
                "\tsock->file = NULL;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash."
    },
    {
        "cve_id": "CVE-2018-12633",
        "code_before_change": "static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct vbg_session *session = filp->private_data;\n\tsize_t returned_size, size;\n\tstruct vbg_ioctl_hdr hdr;\n\tbool is_vmmdev_req;\n\tint ret = 0;\n\tvoid *buf;\n\n\tif (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))\n\t\treturn -EFAULT;\n\n\tif (hdr.version != VBG_IOCTL_HDR_VERSION)\n\t\treturn -EINVAL;\n\n\tif (hdr.size_in < sizeof(hdr) ||\n\t    (hdr.size_out && hdr.size_out < sizeof(hdr)))\n\t\treturn -EINVAL;\n\n\tsize = max(hdr.size_in, hdr.size_out);\n\tif (_IOC_SIZE(req) && _IOC_SIZE(req) != size)\n\t\treturn -EINVAL;\n\tif (size > SZ_16M)\n\t\treturn -E2BIG;\n\n\t/*\n\t * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid\n\t * the need for a bounce-buffer and another copy later on.\n\t */\n\tis_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||\n\t\t\t req == VBG_IOCTL_VMMDEV_REQUEST_BIG;\n\n\tif (is_vmmdev_req)\n\t\tbuf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);\n\telse\n\t\tbuf = kmalloc(size, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(buf, (void *)arg, hdr.size_in)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\tif (hdr.size_in < size)\n\t\tmemset(buf + hdr.size_in, 0, size -  hdr.size_in);\n\n\tret = vbg_core_ioctl(session, req, buf);\n\tif (ret)\n\t\tgoto out;\n\n\treturned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;\n\tif (returned_size > size) {\n\t\tvbg_debug(\"%s: too much output data %zu > %zu\\n\",\n\t\t\t  __func__, returned_size, size);\n\t\treturned_size = size;\n\t}\n\tif (copy_to_user((void *)arg, buf, returned_size) != 0)\n\t\tret = -EFAULT;\n\nout:\n\tif (is_vmmdev_req)\n\t\tvbg_req_free(buf, size);\n\telse\n\t\tkfree(buf);\n\n\treturn ret;\n}",
        "code_after_change": "static long vbg_misc_device_ioctl(struct file *filp, unsigned int req,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct vbg_session *session = filp->private_data;\n\tsize_t returned_size, size;\n\tstruct vbg_ioctl_hdr hdr;\n\tbool is_vmmdev_req;\n\tint ret = 0;\n\tvoid *buf;\n\n\tif (copy_from_user(&hdr, (void *)arg, sizeof(hdr)))\n\t\treturn -EFAULT;\n\n\tif (hdr.version != VBG_IOCTL_HDR_VERSION)\n\t\treturn -EINVAL;\n\n\tif (hdr.size_in < sizeof(hdr) ||\n\t    (hdr.size_out && hdr.size_out < sizeof(hdr)))\n\t\treturn -EINVAL;\n\n\tsize = max(hdr.size_in, hdr.size_out);\n\tif (_IOC_SIZE(req) && _IOC_SIZE(req) != size)\n\t\treturn -EINVAL;\n\tif (size > SZ_16M)\n\t\treturn -E2BIG;\n\n\t/*\n\t * IOCTL_VMMDEV_REQUEST needs the buffer to be below 4G to avoid\n\t * the need for a bounce-buffer and another copy later on.\n\t */\n\tis_vmmdev_req = (req & ~IOCSIZE_MASK) == VBG_IOCTL_VMMDEV_REQUEST(0) ||\n\t\t\t req == VBG_IOCTL_VMMDEV_REQUEST_BIG;\n\n\tif (is_vmmdev_req)\n\t\tbuf = vbg_req_alloc(size, VBG_IOCTL_HDR_TYPE_DEFAULT);\n\telse\n\t\tbuf = kmalloc(size, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\t*((struct vbg_ioctl_hdr *)buf) = hdr;\n\tif (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),\n\t\t\t   hdr.size_in - sizeof(hdr))) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\tif (hdr.size_in < size)\n\t\tmemset(buf + hdr.size_in, 0, size -  hdr.size_in);\n\n\tret = vbg_core_ioctl(session, req, buf);\n\tif (ret)\n\t\tgoto out;\n\n\treturned_size = ((struct vbg_ioctl_hdr *)buf)->size_out;\n\tif (returned_size > size) {\n\t\tvbg_debug(\"%s: too much output data %zu > %zu\\n\",\n\t\t\t  __func__, returned_size, size);\n\t\treturned_size = size;\n\t}\n\tif (copy_to_user((void *)arg, buf, returned_size) != 0)\n\t\tret = -EFAULT;\n\nout:\n\tif (is_vmmdev_req)\n\t\tvbg_req_free(buf, size);\n\telse\n\t\tkfree(buf);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,7 +38,9 @@\n \tif (!buf)\n \t\treturn -ENOMEM;\n \n-\tif (copy_from_user(buf, (void *)arg, hdr.size_in)) {\n+\t*((struct vbg_ioctl_hdr *)buf) = hdr;\n+\tif (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),\n+\t\t\t   hdr.size_in - sizeof(hdr))) {\n \t\tret = -EFAULT;\n \t\tgoto out;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t*((struct vbg_ioctl_hdr *)buf) = hdr;",
                "\tif (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),",
                "\t\t\t   hdr.size_in - sizeof(hdr))) {"
            ],
            "deleted": [
                "\tif (copy_from_user(buf, (void *)arg, hdr.size_in)) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 4.17.2. vbg_misc_device_ioctl() in drivers/virt/vboxguest/vboxguest_linux.c reads the same user data twice with copy_from_user. The header part of the user data is double-fetched, and a malicious user thread can tamper with the critical variables (hdr.size_in and hdr.size_out) in the header between the two fetches because of a race condition, leading to severe kernel errors, such as buffer over-accesses. This bug can cause a local denial of service and information leakage."
    },
    {
        "cve_id": "CVE-2018-17972",
        "code_before_change": "static int proc_pid_stack(struct seq_file *m, struct pid_namespace *ns,\n\t\t\t  struct pid *pid, struct task_struct *task)\n{\n\tstruct stack_trace trace;\n\tunsigned long *entries;\n\tint err;\n\n\tentries = kmalloc_array(MAX_STACK_TRACE_DEPTH, sizeof(*entries),\n\t\t\t\tGFP_KERNEL);\n\tif (!entries)\n\t\treturn -ENOMEM;\n\n\ttrace.nr_entries\t= 0;\n\ttrace.max_entries\t= MAX_STACK_TRACE_DEPTH;\n\ttrace.entries\t\t= entries;\n\ttrace.skip\t\t= 0;\n\n\terr = lock_trace(task);\n\tif (!err) {\n\t\tunsigned int i;\n\n\t\tsave_stack_trace_tsk(task, &trace);\n\n\t\tfor (i = 0; i < trace.nr_entries; i++) {\n\t\t\tseq_printf(m, \"[<0>] %pB\\n\", (void *)entries[i]);\n\t\t}\n\t\tunlock_trace(task);\n\t}\n\tkfree(entries);\n\n\treturn err;\n}",
        "code_after_change": "static int proc_pid_stack(struct seq_file *m, struct pid_namespace *ns,\n\t\t\t  struct pid *pid, struct task_struct *task)\n{\n\tstruct stack_trace trace;\n\tunsigned long *entries;\n\tint err;\n\n\t/*\n\t * The ability to racily run the kernel stack unwinder on a running task\n\t * and then observe the unwinder output is scary; while it is useful for\n\t * debugging kernel issues, it can also allow an attacker to leak kernel\n\t * stack contents.\n\t * Doing this in a manner that is at least safe from races would require\n\t * some work to ensure that the remote task can not be scheduled; and\n\t * even then, this would still expose the unwinder as local attack\n\t * surface.\n\t * Therefore, this interface is restricted to root.\n\t */\n\tif (!file_ns_capable(m->file, &init_user_ns, CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tentries = kmalloc_array(MAX_STACK_TRACE_DEPTH, sizeof(*entries),\n\t\t\t\tGFP_KERNEL);\n\tif (!entries)\n\t\treturn -ENOMEM;\n\n\ttrace.nr_entries\t= 0;\n\ttrace.max_entries\t= MAX_STACK_TRACE_DEPTH;\n\ttrace.entries\t\t= entries;\n\ttrace.skip\t\t= 0;\n\n\terr = lock_trace(task);\n\tif (!err) {\n\t\tunsigned int i;\n\n\t\tsave_stack_trace_tsk(task, &trace);\n\n\t\tfor (i = 0; i < trace.nr_entries; i++) {\n\t\t\tseq_printf(m, \"[<0>] %pB\\n\", (void *)entries[i]);\n\t\t}\n\t\tunlock_trace(task);\n\t}\n\tkfree(entries);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,20 @@\n \tstruct stack_trace trace;\n \tunsigned long *entries;\n \tint err;\n+\n+\t/*\n+\t * The ability to racily run the kernel stack unwinder on a running task\n+\t * and then observe the unwinder output is scary; while it is useful for\n+\t * debugging kernel issues, it can also allow an attacker to leak kernel\n+\t * stack contents.\n+\t * Doing this in a manner that is at least safe from races would require\n+\t * some work to ensure that the remote task can not be scheduled; and\n+\t * even then, this would still expose the unwinder as local attack\n+\t * surface.\n+\t * Therefore, this interface is restricted to root.\n+\t */\n+\tif (!file_ns_capable(m->file, &init_user_ns, CAP_SYS_ADMIN))\n+\t\treturn -EACCES;\n \n \tentries = kmalloc_array(MAX_STACK_TRACE_DEPTH, sizeof(*entries),\n \t\t\t\tGFP_KERNEL);",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * The ability to racily run the kernel stack unwinder on a running task",
                "\t * and then observe the unwinder output is scary; while it is useful for",
                "\t * debugging kernel issues, it can also allow an attacker to leak kernel",
                "\t * stack contents.",
                "\t * Doing this in a manner that is at least safe from races would require",
                "\t * some work to ensure that the remote task can not be scheduled; and",
                "\t * even then, this would still expose the unwinder as local attack",
                "\t * surface.",
                "\t * Therefore, this interface is restricted to root.",
                "\t */",
                "\tif (!file_ns_capable(m->file, &init_user_ns, CAP_SYS_ADMIN))",
                "\t\treturn -EACCES;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in the proc_pid_stack function in fs/proc/base.c in the Linux kernel through 4.18.11. It does not ensure that only root may inspect the kernel stack of an arbitrary task, allowing a local attacker to exploit racy stack unwinding and leak kernel task stack contents."
    },
    {
        "cve_id": "CVE-2018-18559",
        "code_before_change": "static int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n\t\t\t  __be16 proto)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct net_device *dev_curr;\n\t__be16 proto_curr;\n\tbool need_rehook;\n\tstruct net_device *dev = NULL;\n\tint ret = 0;\n\tbool unlisted = false;\n\n\tlock_sock(sk);\n\tspin_lock(&po->bind_lock);\n\trcu_read_lock();\n\n\tif (po->fanout) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (name) {\n\t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (ifindex) {\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), ifindex);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (dev)\n\t\tdev_hold(dev);\n\n\tproto_curr = po->prot_hook.type;\n\tdev_curr = po->prot_hook.dev;\n\n\tneed_rehook = proto_curr != proto || dev_curr != dev;\n\n\tif (need_rehook) {\n\t\tif (po->running) {\n\t\t\trcu_read_unlock();\n\t\t\t__unregister_prot_hook(sk, true);\n\t\t\trcu_read_lock();\n\t\t\tdev_curr = po->prot_hook.dev;\n\t\t\tif (dev)\n\t\t\t\tunlisted = !dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t\t\t dev->ifindex);\n\t\t}\n\n\t\tpo->num = proto;\n\t\tpo->prot_hook.type = proto;\n\n\t\tif (unlikely(unlisted)) {\n\t\t\tdev_put(dev);\n\t\t\tpo->prot_hook.dev = NULL;\n\t\t\tpo->ifindex = -1;\n\t\t\tpacket_cached_dev_reset(po);\n\t\t} else {\n\t\t\tpo->prot_hook.dev = dev;\n\t\t\tpo->ifindex = dev ? dev->ifindex : 0;\n\t\t\tpacket_cached_dev_assign(po, dev);\n\t\t}\n\t}\n\tif (dev_curr)\n\t\tdev_put(dev_curr);\n\n\tif (proto == 0 || !need_rehook)\n\t\tgoto out_unlock;\n\n\tif (!unlisted && (!dev || (dev->flags & IFF_UP))) {\n\t\tregister_prot_hook(sk);\n\t} else {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\tspin_unlock(&po->bind_lock);\n\trelease_sock(sk);\n\treturn ret;\n}",
        "code_after_change": "static int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n\t\t\t  __be16 proto)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct net_device *dev_curr;\n\t__be16 proto_curr;\n\tbool need_rehook;\n\tstruct net_device *dev = NULL;\n\tint ret = 0;\n\tbool unlisted = false;\n\n\tlock_sock(sk);\n\tspin_lock(&po->bind_lock);\n\trcu_read_lock();\n\n\tif (po->fanout) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (name) {\n\t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (ifindex) {\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), ifindex);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (dev)\n\t\tdev_hold(dev);\n\n\tproto_curr = po->prot_hook.type;\n\tdev_curr = po->prot_hook.dev;\n\n\tneed_rehook = proto_curr != proto || dev_curr != dev;\n\n\tif (need_rehook) {\n\t\tif (po->running) {\n\t\t\trcu_read_unlock();\n\t\t\t/* prevents packet_notifier() from calling\n\t\t\t * register_prot_hook()\n\t\t\t */\n\t\t\tpo->num = 0;\n\t\t\t__unregister_prot_hook(sk, true);\n\t\t\trcu_read_lock();\n\t\t\tdev_curr = po->prot_hook.dev;\n\t\t\tif (dev)\n\t\t\t\tunlisted = !dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t\t\t dev->ifindex);\n\t\t}\n\n\t\tBUG_ON(po->running);\n\t\tpo->num = proto;\n\t\tpo->prot_hook.type = proto;\n\n\t\tif (unlikely(unlisted)) {\n\t\t\tdev_put(dev);\n\t\t\tpo->prot_hook.dev = NULL;\n\t\t\tpo->ifindex = -1;\n\t\t\tpacket_cached_dev_reset(po);\n\t\t} else {\n\t\t\tpo->prot_hook.dev = dev;\n\t\t\tpo->ifindex = dev ? dev->ifindex : 0;\n\t\t\tpacket_cached_dev_assign(po, dev);\n\t\t}\n\t}\n\tif (dev_curr)\n\t\tdev_put(dev_curr);\n\n\tif (proto == 0 || !need_rehook)\n\t\tgoto out_unlock;\n\n\tif (!unlisted && (!dev || (dev->flags & IFF_UP))) {\n\t\tregister_prot_hook(sk);\n\t} else {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\tspin_unlock(&po->bind_lock);\n\trelease_sock(sk);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,6 +43,10 @@\n \tif (need_rehook) {\n \t\tif (po->running) {\n \t\t\trcu_read_unlock();\n+\t\t\t/* prevents packet_notifier() from calling\n+\t\t\t * register_prot_hook()\n+\t\t\t */\n+\t\t\tpo->num = 0;\n \t\t\t__unregister_prot_hook(sk, true);\n \t\t\trcu_read_lock();\n \t\t\tdev_curr = po->prot_hook.dev;\n@@ -51,6 +55,7 @@\n \t\t\t\t\t\t\t\t dev->ifindex);\n \t\t}\n \n+\t\tBUG_ON(po->running);\n \t\tpo->num = proto;\n \t\tpo->prot_hook.type = proto;\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t/* prevents packet_notifier() from calling",
                "\t\t\t * register_prot_hook()",
                "\t\t\t */",
                "\t\t\tpo->num = 0;",
                "\t\tBUG_ON(po->running);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel through 4.19, a use-after-free can occur due to a race condition between fanout_add from setsockopt and bind on an AF_PACKET socket. This issue exists because of the 15fe076edea787807a7cdc168df832544b58eba6 incomplete fix for a race condition. The code mishandles a certain multithreaded case involving a packet_do_bind unregister action followed by a packet_notifier register action. Later, packet_release operates on only one of the two applicable linked lists. The attacker can achieve Program Counter control."
    },
    {
        "cve_id": "CVE-2018-20836",
        "code_before_change": "static void smp_task_timedout(struct timer_list *t)\n{\n\tstruct sas_task_slow *slow = from_timer(slow, t, timer);\n\tstruct sas_task *task = slow->task;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&task->task_state_lock, flags);\n\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))\n\t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n\tspin_unlock_irqrestore(&task->task_state_lock, flags);\n\n\tcomplete(&task->slow_task->completion);\n}",
        "code_after_change": "static void smp_task_timedout(struct timer_list *t)\n{\n\tstruct sas_task_slow *slow = from_timer(slow, t, timer);\n\tstruct sas_task *task = slow->task;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&task->task_state_lock, flags);\n\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {\n\t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n\t\tcomplete(&task->slow_task->completion);\n\t}\n\tspin_unlock_irqrestore(&task->task_state_lock, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,9 +5,9 @@\n \tunsigned long flags;\n \n \tspin_lock_irqsave(&task->task_state_lock, flags);\n-\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))\n+\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {\n \t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n+\t\tcomplete(&task->slow_task->completion);\n+\t}\n \tspin_unlock_irqrestore(&task->task_state_lock, flags);\n-\n-\tcomplete(&task->slow_task->completion);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {",
                "\t\tcomplete(&task->slow_task->completion);",
                "\t}"
            ],
            "deleted": [
                "\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))",
                "",
                "\tcomplete(&task->slow_task->completion);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.20. There is a race condition in smp_task_timedout() and smp_task_done() in drivers/scsi/libsas/sas_expander.c, leading to a use-after-free."
    },
    {
        "cve_id": "CVE-2018-20836",
        "code_before_change": "static void smp_task_done(struct sas_task *task)\n{\n\tif (!del_timer(&task->slow_task->timer))\n\t\treturn;\n\tcomplete(&task->slow_task->completion);\n}",
        "code_after_change": "static void smp_task_done(struct sas_task *task)\n{\n\tdel_timer(&task->slow_task->timer);\n\tcomplete(&task->slow_task->completion);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,5 @@\n static void smp_task_done(struct sas_task *task)\n {\n-\tif (!del_timer(&task->slow_task->timer))\n-\t\treturn;\n+\tdel_timer(&task->slow_task->timer);\n \tcomplete(&task->slow_task->completion);\n }",
        "function_modified_lines": {
            "added": [
                "\tdel_timer(&task->slow_task->timer);"
            ],
            "deleted": [
                "\tif (!del_timer(&task->slow_task->timer))",
                "\t\treturn;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.20. There is a race condition in smp_task_timedout() and smp_task_done() in drivers/scsi/libsas/sas_expander.c, leading to a use-after-free."
    },
    {
        "cve_id": "CVE-2018-5344",
        "code_before_change": "static void lo_release(struct gendisk *disk, fmode_t mode)\n{\n\tstruct loop_device *lo = disk->private_data;\n\tint err;\n\n\tif (atomic_dec_return(&lo->lo_refcnt))\n\t\treturn;\n\n\tmutex_lock(&lo->lo_ctl_mutex);\n\tif (lo->lo_flags & LO_FLAGS_AUTOCLEAR) {\n\t\t/*\n\t\t * In autoclear mode, stop the loop thread\n\t\t * and remove configuration after last close.\n\t\t */\n\t\terr = loop_clr_fd(lo);\n\t\tif (!err)\n\t\t\treturn;\n\t} else if (lo->lo_state == Lo_bound) {\n\t\t/*\n\t\t * Otherwise keep thread (if running) and config,\n\t\t * but flush possible ongoing bios in thread.\n\t\t */\n\t\tblk_mq_freeze_queue(lo->lo_queue);\n\t\tblk_mq_unfreeze_queue(lo->lo_queue);\n\t}\n\n\tmutex_unlock(&lo->lo_ctl_mutex);\n}",
        "code_after_change": "static void lo_release(struct gendisk *disk, fmode_t mode)\n{\n\tmutex_lock(&loop_index_mutex);\n\t__lo_release(disk->private_data);\n\tmutex_unlock(&loop_index_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,28 +1,6 @@\n static void lo_release(struct gendisk *disk, fmode_t mode)\n {\n-\tstruct loop_device *lo = disk->private_data;\n-\tint err;\n-\n-\tif (atomic_dec_return(&lo->lo_refcnt))\n-\t\treturn;\n-\n-\tmutex_lock(&lo->lo_ctl_mutex);\n-\tif (lo->lo_flags & LO_FLAGS_AUTOCLEAR) {\n-\t\t/*\n-\t\t * In autoclear mode, stop the loop thread\n-\t\t * and remove configuration after last close.\n-\t\t */\n-\t\terr = loop_clr_fd(lo);\n-\t\tif (!err)\n-\t\t\treturn;\n-\t} else if (lo->lo_state == Lo_bound) {\n-\t\t/*\n-\t\t * Otherwise keep thread (if running) and config,\n-\t\t * but flush possible ongoing bios in thread.\n-\t\t */\n-\t\tblk_mq_freeze_queue(lo->lo_queue);\n-\t\tblk_mq_unfreeze_queue(lo->lo_queue);\n-\t}\n-\n-\tmutex_unlock(&lo->lo_ctl_mutex);\n+\tmutex_lock(&loop_index_mutex);\n+\t__lo_release(disk->private_data);\n+\tmutex_unlock(&loop_index_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&loop_index_mutex);",
                "\t__lo_release(disk->private_data);",
                "\tmutex_unlock(&loop_index_mutex);"
            ],
            "deleted": [
                "\tstruct loop_device *lo = disk->private_data;",
                "\tint err;",
                "",
                "\tif (atomic_dec_return(&lo->lo_refcnt))",
                "\t\treturn;",
                "",
                "\tmutex_lock(&lo->lo_ctl_mutex);",
                "\tif (lo->lo_flags & LO_FLAGS_AUTOCLEAR) {",
                "\t\t/*",
                "\t\t * In autoclear mode, stop the loop thread",
                "\t\t * and remove configuration after last close.",
                "\t\t */",
                "\t\terr = loop_clr_fd(lo);",
                "\t\tif (!err)",
                "\t\t\treturn;",
                "\t} else if (lo->lo_state == Lo_bound) {",
                "\t\t/*",
                "\t\t * Otherwise keep thread (if running) and config,",
                "\t\t * but flush possible ongoing bios in thread.",
                "\t\t */",
                "\t\tblk_mq_freeze_queue(lo->lo_queue);",
                "\t\tblk_mq_unfreeze_queue(lo->lo_queue);",
                "\t}",
                "",
                "\tmutex_unlock(&lo->lo_ctl_mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel through 4.14.13, drivers/block/loop.c mishandles lo_release serialization, which allows attackers to cause a denial of service (__lock_acquire use-after-free) or possibly have unspecified other impact."
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "static int stub_probe(struct usb_device *udev)\n{\n\tstruct stub_device *sdev = NULL;\n\tconst char *udev_busid = dev_name(&udev->dev);\n\tstruct bus_id_priv *busid_priv;\n\tint rc;\n\n\tdev_dbg(&udev->dev, \"Enter probe\\n\");\n\n\t/* check we should claim or not by busid_table */\n\tbusid_priv = get_busid_priv(udev_busid);\n\tif (!busid_priv || (busid_priv->status == STUB_BUSID_REMOV) ||\n\t    (busid_priv->status == STUB_BUSID_OTHER)) {\n\t\tdev_info(&udev->dev,\n\t\t\t\"%s is not in match_busid table... skip!\\n\",\n\t\t\tudev_busid);\n\n\t\t/*\n\t\t * Return value should be ENODEV or ENOXIO to continue trying\n\t\t * other matched drivers by the driver core.\n\t\t * See driver_probe_device() in driver/base/dd.c\n\t\t */\n\t\treturn -ENODEV;\n\t}\n\n\tif (udev->descriptor.bDeviceClass == USB_CLASS_HUB) {\n\t\tdev_dbg(&udev->dev, \"%s is a usb hub device... skip!\\n\",\n\t\t\t udev_busid);\n\t\treturn -ENODEV;\n\t}\n\n\tif (!strcmp(udev->bus->bus_name, \"vhci_hcd\")) {\n\t\tdev_dbg(&udev->dev,\n\t\t\t\"%s is attached on vhci_hcd... skip!\\n\",\n\t\t\tudev_busid);\n\n\t\treturn -ENODEV;\n\t}\n\n\t/* ok, this is my device */\n\tsdev = stub_device_alloc(udev);\n\tif (!sdev)\n\t\treturn -ENOMEM;\n\n\tdev_info(&udev->dev,\n\t\t\"usbip-host: register new device (bus %u dev %u)\\n\",\n\t\tudev->bus->busnum, udev->devnum);\n\n\tbusid_priv->shutdown_busid = 0;\n\n\t/* set private data to usb_device */\n\tdev_set_drvdata(&udev->dev, sdev);\n\tbusid_priv->sdev = sdev;\n\tbusid_priv->udev = udev;\n\n\t/*\n\t * Claim this hub port.\n\t * It doesn't matter what value we pass as owner\n\t * (struct dev_state) as long as it is unique.\n\t */\n\trc = usb_hub_claim_port(udev->parent, udev->portnum,\n\t\t\t(struct usb_dev_state *) udev);\n\tif (rc) {\n\t\tdev_dbg(&udev->dev, \"unable to claim port\\n\");\n\t\tgoto err_port;\n\t}\n\n\trc = stub_add_files(&udev->dev);\n\tif (rc) {\n\t\tdev_err(&udev->dev, \"stub_add_files for %s\\n\", udev_busid);\n\t\tgoto err_files;\n\t}\n\tbusid_priv->status = STUB_BUSID_ALLOC;\n\n\treturn 0;\nerr_files:\n\tusb_hub_release_port(udev->parent, udev->portnum,\n\t\t\t     (struct usb_dev_state *) udev);\nerr_port:\n\tdev_set_drvdata(&udev->dev, NULL);\n\tusb_put_dev(udev);\n\n\tbusid_priv->sdev = NULL;\n\tstub_device_free(sdev);\n\treturn rc;\n}",
        "code_after_change": "static int stub_probe(struct usb_device *udev)\n{\n\tstruct stub_device *sdev = NULL;\n\tconst char *udev_busid = dev_name(&udev->dev);\n\tstruct bus_id_priv *busid_priv;\n\tint rc = 0;\n\n\tdev_dbg(&udev->dev, \"Enter probe\\n\");\n\n\t/* check we should claim or not by busid_table */\n\tbusid_priv = get_busid_priv(udev_busid);\n\tif (!busid_priv || (busid_priv->status == STUB_BUSID_REMOV) ||\n\t    (busid_priv->status == STUB_BUSID_OTHER)) {\n\t\tdev_info(&udev->dev,\n\t\t\t\"%s is not in match_busid table... skip!\\n\",\n\t\t\tudev_busid);\n\n\t\t/*\n\t\t * Return value should be ENODEV or ENOXIO to continue trying\n\t\t * other matched drivers by the driver core.\n\t\t * See driver_probe_device() in driver/base/dd.c\n\t\t */\n\t\trc = -ENODEV;\n\t\tgoto call_put_busid_priv;\n\t}\n\n\tif (udev->descriptor.bDeviceClass == USB_CLASS_HUB) {\n\t\tdev_dbg(&udev->dev, \"%s is a usb hub device... skip!\\n\",\n\t\t\t udev_busid);\n\t\trc = -ENODEV;\n\t\tgoto call_put_busid_priv;\n\t}\n\n\tif (!strcmp(udev->bus->bus_name, \"vhci_hcd\")) {\n\t\tdev_dbg(&udev->dev,\n\t\t\t\"%s is attached on vhci_hcd... skip!\\n\",\n\t\t\tudev_busid);\n\n\t\trc = -ENODEV;\n\t\tgoto call_put_busid_priv;\n\t}\n\n\t/* ok, this is my device */\n\tsdev = stub_device_alloc(udev);\n\tif (!sdev) {\n\t\trc = -ENOMEM;\n\t\tgoto call_put_busid_priv;\n\t}\n\n\tdev_info(&udev->dev,\n\t\t\"usbip-host: register new device (bus %u dev %u)\\n\",\n\t\tudev->bus->busnum, udev->devnum);\n\n\tbusid_priv->shutdown_busid = 0;\n\n\t/* set private data to usb_device */\n\tdev_set_drvdata(&udev->dev, sdev);\n\tbusid_priv->sdev = sdev;\n\tbusid_priv->udev = udev;\n\n\t/*\n\t * Claim this hub port.\n\t * It doesn't matter what value we pass as owner\n\t * (struct dev_state) as long as it is unique.\n\t */\n\trc = usb_hub_claim_port(udev->parent, udev->portnum,\n\t\t\t(struct usb_dev_state *) udev);\n\tif (rc) {\n\t\tdev_dbg(&udev->dev, \"unable to claim port\\n\");\n\t\tgoto err_port;\n\t}\n\n\trc = stub_add_files(&udev->dev);\n\tif (rc) {\n\t\tdev_err(&udev->dev, \"stub_add_files for %s\\n\", udev_busid);\n\t\tgoto err_files;\n\t}\n\tbusid_priv->status = STUB_BUSID_ALLOC;\n\n\trc = 0;\n\tgoto call_put_busid_priv;\n\nerr_files:\n\tusb_hub_release_port(udev->parent, udev->portnum,\n\t\t\t     (struct usb_dev_state *) udev);\nerr_port:\n\tdev_set_drvdata(&udev->dev, NULL);\n\tusb_put_dev(udev);\n\n\tbusid_priv->sdev = NULL;\n\tstub_device_free(sdev);\n\ncall_put_busid_priv:\n\tput_busid_priv(busid_priv);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \tstruct stub_device *sdev = NULL;\n \tconst char *udev_busid = dev_name(&udev->dev);\n \tstruct bus_id_priv *busid_priv;\n-\tint rc;\n+\tint rc = 0;\n \n \tdev_dbg(&udev->dev, \"Enter probe\\n\");\n \n@@ -20,13 +20,15 @@\n \t\t * other matched drivers by the driver core.\n \t\t * See driver_probe_device() in driver/base/dd.c\n \t\t */\n-\t\treturn -ENODEV;\n+\t\trc = -ENODEV;\n+\t\tgoto call_put_busid_priv;\n \t}\n \n \tif (udev->descriptor.bDeviceClass == USB_CLASS_HUB) {\n \t\tdev_dbg(&udev->dev, \"%s is a usb hub device... skip!\\n\",\n \t\t\t udev_busid);\n-\t\treturn -ENODEV;\n+\t\trc = -ENODEV;\n+\t\tgoto call_put_busid_priv;\n \t}\n \n \tif (!strcmp(udev->bus->bus_name, \"vhci_hcd\")) {\n@@ -34,13 +36,16 @@\n \t\t\t\"%s is attached on vhci_hcd... skip!\\n\",\n \t\t\tudev_busid);\n \n-\t\treturn -ENODEV;\n+\t\trc = -ENODEV;\n+\t\tgoto call_put_busid_priv;\n \t}\n \n \t/* ok, this is my device */\n \tsdev = stub_device_alloc(udev);\n-\tif (!sdev)\n-\t\treturn -ENOMEM;\n+\tif (!sdev) {\n+\t\trc = -ENOMEM;\n+\t\tgoto call_put_busid_priv;\n+\t}\n \n \tdev_info(&udev->dev,\n \t\t\"usbip-host: register new device (bus %u dev %u)\\n\",\n@@ -72,7 +77,9 @@\n \t}\n \tbusid_priv->status = STUB_BUSID_ALLOC;\n \n-\treturn 0;\n+\trc = 0;\n+\tgoto call_put_busid_priv;\n+\n err_files:\n \tusb_hub_release_port(udev->parent, udev->portnum,\n \t\t\t     (struct usb_dev_state *) udev);\n@@ -82,5 +89,8 @@\n \n \tbusid_priv->sdev = NULL;\n \tstub_device_free(sdev);\n+\n+call_put_busid_priv:\n+\tput_busid_priv(busid_priv);\n \treturn rc;\n }",
        "function_modified_lines": {
            "added": [
                "\tint rc = 0;",
                "\t\trc = -ENODEV;",
                "\t\tgoto call_put_busid_priv;",
                "\t\trc = -ENODEV;",
                "\t\tgoto call_put_busid_priv;",
                "\t\trc = -ENODEV;",
                "\t\tgoto call_put_busid_priv;",
                "\tif (!sdev) {",
                "\t\trc = -ENOMEM;",
                "\t\tgoto call_put_busid_priv;",
                "\t}",
                "\trc = 0;",
                "\tgoto call_put_busid_priv;",
                "",
                "",
                "call_put_busid_priv:",
                "\tput_busid_priv(busid_priv);"
            ],
            "deleted": [
                "\tint rc;",
                "\t\treturn -ENODEV;",
                "\t\treturn -ENODEV;",
                "\t\treturn -ENODEV;",
                "\tif (!sdev)",
                "\t\treturn -ENOMEM;",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets."
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "static void stub_disconnect(struct usb_device *udev)\n{\n\tstruct stub_device *sdev;\n\tconst char *udev_busid = dev_name(&udev->dev);\n\tstruct bus_id_priv *busid_priv;\n\tint rc;\n\n\tdev_dbg(&udev->dev, \"Enter disconnect\\n\");\n\n\tbusid_priv = get_busid_priv(udev_busid);\n\tif (!busid_priv) {\n\t\tBUG();\n\t\treturn;\n\t}\n\n\tsdev = dev_get_drvdata(&udev->dev);\n\n\t/* get stub_device */\n\tif (!sdev) {\n\t\tdev_err(&udev->dev, \"could not get device\");\n\t\treturn;\n\t}\n\n\tdev_set_drvdata(&udev->dev, NULL);\n\n\t/*\n\t * NOTE: rx/tx threads are invoked for each usb_device.\n\t */\n\tstub_remove_files(&udev->dev);\n\n\t/* release port */\n\trc = usb_hub_release_port(udev->parent, udev->portnum,\n\t\t\t\t  (struct usb_dev_state *) udev);\n\tif (rc) {\n\t\tdev_dbg(&udev->dev, \"unable to release port\\n\");\n\t\treturn;\n\t}\n\n\t/* If usb reset is called from event handler */\n\tif (usbip_in_eh(current))\n\t\treturn;\n\n\t/* shutdown the current connection */\n\tshutdown_busid(busid_priv);\n\n\tusb_put_dev(sdev->udev);\n\n\t/* free sdev */\n\tbusid_priv->sdev = NULL;\n\tstub_device_free(sdev);\n\n\tif (busid_priv->status == STUB_BUSID_ALLOC)\n\t\tbusid_priv->status = STUB_BUSID_ADDED;\n}",
        "code_after_change": "static void stub_disconnect(struct usb_device *udev)\n{\n\tstruct stub_device *sdev;\n\tconst char *udev_busid = dev_name(&udev->dev);\n\tstruct bus_id_priv *busid_priv;\n\tint rc;\n\n\tdev_dbg(&udev->dev, \"Enter disconnect\\n\");\n\n\tbusid_priv = get_busid_priv(udev_busid);\n\tif (!busid_priv) {\n\t\tBUG();\n\t\treturn;\n\t}\n\n\tsdev = dev_get_drvdata(&udev->dev);\n\n\t/* get stub_device */\n\tif (!sdev) {\n\t\tdev_err(&udev->dev, \"could not get device\");\n\t\tgoto call_put_busid_priv;\n\t}\n\n\tdev_set_drvdata(&udev->dev, NULL);\n\n\t/*\n\t * NOTE: rx/tx threads are invoked for each usb_device.\n\t */\n\tstub_remove_files(&udev->dev);\n\n\t/* release port */\n\trc = usb_hub_release_port(udev->parent, udev->portnum,\n\t\t\t\t  (struct usb_dev_state *) udev);\n\tif (rc) {\n\t\tdev_dbg(&udev->dev, \"unable to release port\\n\");\n\t\tgoto call_put_busid_priv;\n\t}\n\n\t/* If usb reset is called from event handler */\n\tif (usbip_in_eh(current))\n\t\tgoto call_put_busid_priv;\n\n\t/* shutdown the current connection */\n\tshutdown_busid(busid_priv);\n\n\tusb_put_dev(sdev->udev);\n\n\t/* free sdev */\n\tbusid_priv->sdev = NULL;\n\tstub_device_free(sdev);\n\n\tif (busid_priv->status == STUB_BUSID_ALLOC)\n\t\tbusid_priv->status = STUB_BUSID_ADDED;\n\ncall_put_busid_priv:\n\tput_busid_priv(busid_priv);\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,7 @@\n \t/* get stub_device */\n \tif (!sdev) {\n \t\tdev_err(&udev->dev, \"could not get device\");\n-\t\treturn;\n+\t\tgoto call_put_busid_priv;\n \t}\n \n \tdev_set_drvdata(&udev->dev, NULL);\n@@ -33,12 +33,12 @@\n \t\t\t\t  (struct usb_dev_state *) udev);\n \tif (rc) {\n \t\tdev_dbg(&udev->dev, \"unable to release port\\n\");\n-\t\treturn;\n+\t\tgoto call_put_busid_priv;\n \t}\n \n \t/* If usb reset is called from event handler */\n \tif (usbip_in_eh(current))\n-\t\treturn;\n+\t\tgoto call_put_busid_priv;\n \n \t/* shutdown the current connection */\n \tshutdown_busid(busid_priv);\n@@ -51,4 +51,7 @@\n \n \tif (busid_priv->status == STUB_BUSID_ALLOC)\n \t\tbusid_priv->status = STUB_BUSID_ADDED;\n+\n+call_put_busid_priv:\n+\tput_busid_priv(busid_priv);\n }",
        "function_modified_lines": {
            "added": [
                "\t\tgoto call_put_busid_priv;",
                "\t\tgoto call_put_busid_priv;",
                "\t\tgoto call_put_busid_priv;",
                "",
                "call_put_busid_priv:",
                "\tput_busid_priv(busid_priv);"
            ],
            "deleted": [
                "\t\treturn;",
                "\t\treturn;",
                "\t\treturn;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets."
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "int del_match_busid(char *busid)\n{\n\tint idx;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx < 0)\n\t\tgoto out;\n\n\t/* found */\n\tret = 0;\n\n\tif (busid_table[idx].status == STUB_BUSID_OTHER)\n\t\tmemset(busid_table[idx].name, 0, BUSID_SIZE);\n\n\tif ((busid_table[idx].status != STUB_BUSID_OTHER) &&\n\t    (busid_table[idx].status != STUB_BUSID_ADDED))\n\t\tbusid_table[idx].status = STUB_BUSID_REMOV;\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
        "code_after_change": "int del_match_busid(char *busid)\n{\n\tint idx;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx < 0)\n\t\tgoto out;\n\n\t/* found */\n\tret = 0;\n\n\tspin_lock(&busid_table[idx].busid_lock);\n\n\tif (busid_table[idx].status == STUB_BUSID_OTHER)\n\t\tmemset(busid_table[idx].name, 0, BUSID_SIZE);\n\n\tif ((busid_table[idx].status != STUB_BUSID_OTHER) &&\n\t    (busid_table[idx].status != STUB_BUSID_ADDED))\n\t\tbusid_table[idx].status = STUB_BUSID_REMOV;\n\n\tspin_unlock(&busid_table[idx].busid_lock);\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,8 @@\n \t/* found */\n \tret = 0;\n \n+\tspin_lock(&busid_table[idx].busid_lock);\n+\n \tif (busid_table[idx].status == STUB_BUSID_OTHER)\n \t\tmemset(busid_table[idx].name, 0, BUSID_SIZE);\n \n@@ -18,6 +20,7 @@\n \t    (busid_table[idx].status != STUB_BUSID_ADDED))\n \t\tbusid_table[idx].status = STUB_BUSID_REMOV;\n \n+\tspin_unlock(&busid_table[idx].busid_lock);\n out:\n \tspin_unlock(&busid_table_lock);\n ",
        "function_modified_lines": {
            "added": [
                "\tspin_lock(&busid_table[idx].busid_lock);",
                "",
                "\tspin_unlock(&busid_table[idx].busid_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets."
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "static ssize_t match_busid_show(struct device_driver *drv, char *buf)\n{\n\tint i;\n\tchar *out = buf;\n\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tif (busid_table[i].name[0])\n\t\t\tout += sprintf(out, \"%s \", busid_table[i].name);\n\tspin_unlock(&busid_table_lock);\n\tout += sprintf(out, \"\\n\");\n\n\treturn out - buf;\n}",
        "code_after_change": "static ssize_t match_busid_show(struct device_driver *drv, char *buf)\n{\n\tint i;\n\tchar *out = buf;\n\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tspin_lock(&busid_table[i].busid_lock);\n\t\tif (busid_table[i].name[0])\n\t\t\tout += sprintf(out, \"%s \", busid_table[i].name);\n\t\tspin_unlock(&busid_table[i].busid_lock);\n\t}\n\tspin_unlock(&busid_table_lock);\n\tout += sprintf(out, \"\\n\");\n\n\treturn out - buf;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,9 +4,12 @@\n \tchar *out = buf;\n \n \tspin_lock(&busid_table_lock);\n-\tfor (i = 0; i < MAX_BUSID; i++)\n+\tfor (i = 0; i < MAX_BUSID; i++) {\n+\t\tspin_lock(&busid_table[i].busid_lock);\n \t\tif (busid_table[i].name[0])\n \t\t\tout += sprintf(out, \"%s \", busid_table[i].name);\n+\t\tspin_unlock(&busid_table[i].busid_lock);\n+\t}\n \tspin_unlock(&busid_table_lock);\n \tout += sprintf(out, \"\\n\");\n ",
        "function_modified_lines": {
            "added": [
                "\tfor (i = 0; i < MAX_BUSID; i++) {",
                "\t\tspin_lock(&busid_table[i].busid_lock);",
                "\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t}"
            ],
            "deleted": [
                "\tfor (i = 0; i < MAX_BUSID; i++)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets."
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "struct bus_id_priv *get_busid_priv(const char *busid)\n{\n\tint idx;\n\tstruct bus_id_priv *bid = NULL;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx >= 0)\n\t\tbid = &(busid_table[idx]);\n\tspin_unlock(&busid_table_lock);\n\n\treturn bid;\n}",
        "code_after_change": "struct bus_id_priv *get_busid_priv(const char *busid)\n{\n\tint idx;\n\tstruct bus_id_priv *bid = NULL;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx >= 0) {\n\t\tbid = &(busid_table[idx]);\n\t\t/* get busid_lock before returning */\n\t\tspin_lock(&bid->busid_lock);\n\t}\n\tspin_unlock(&busid_table_lock);\n\n\treturn bid;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,11 @@\n \n \tspin_lock(&busid_table_lock);\n \tidx = get_busid_idx(busid);\n-\tif (idx >= 0)\n+\tif (idx >= 0) {\n \t\tbid = &(busid_table[idx]);\n+\t\t/* get busid_lock before returning */\n+\t\tspin_lock(&bid->busid_lock);\n+\t}\n \tspin_unlock(&busid_table_lock);\n \n \treturn bid;",
        "function_modified_lines": {
            "added": [
                "\tif (idx >= 0) {",
                "\t\t/* get busid_lock before returning */",
                "\t\tspin_lock(&bid->busid_lock);",
                "\t}"
            ],
            "deleted": [
                "\tif (idx >= 0)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets."
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "static void stub_device_rebind(void)\n{\n#if IS_MODULE(CONFIG_USBIP_HOST)\n\tstruct bus_id_priv *busid_priv;\n\tint i;\n\n\t/* update status to STUB_BUSID_OTHER so probe ignores the device */\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tbusid_priv->status = STUB_BUSID_OTHER;\n\t\t}\n\t}\n\tspin_unlock(&busid_table_lock);\n\n\t/* now run rebind */\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tdo_rebind(busid_table[i].name, busid_priv);\n\t\t}\n\t}\n#endif\n}",
        "code_after_change": "static void stub_device_rebind(void)\n{\n#if IS_MODULE(CONFIG_USBIP_HOST)\n\tstruct bus_id_priv *busid_priv;\n\tint i;\n\n\t/* update status to STUB_BUSID_OTHER so probe ignores the device */\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tbusid_priv->status = STUB_BUSID_OTHER;\n\t\t}\n\t}\n\tspin_unlock(&busid_table_lock);\n\n\t/* now run rebind - no need to hold locks. driver files are removed */\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tdo_rebind(busid_table[i].name, busid_priv);\n\t\t}\n\t}\n#endif\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,7 +15,7 @@\n \t}\n \tspin_unlock(&busid_table_lock);\n \n-\t/* now run rebind */\n+\t/* now run rebind - no need to hold locks. driver files are removed */\n \tfor (i = 0; i < MAX_BUSID; i++) {\n \t\tif (busid_table[i].name[0] &&\n \t\t    busid_table[i].shutdown_busid) {",
        "function_modified_lines": {
            "added": [
                "\t/* now run rebind - no need to hold locks. driver files are removed */"
            ],
            "deleted": [
                "\t/* now run rebind */"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets."
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "static void init_busid_table(void)\n{\n\t/*\n\t * This also sets the bus_table[i].status to\n\t * STUB_BUSID_OTHER, which is 0.\n\t */\n\tmemset(busid_table, 0, sizeof(busid_table));\n\n\tspin_lock_init(&busid_table_lock);\n}",
        "code_after_change": "static void init_busid_table(void)\n{\n\tint i;\n\n\t/*\n\t * This also sets the bus_table[i].status to\n\t * STUB_BUSID_OTHER, which is 0.\n\t */\n\tmemset(busid_table, 0, sizeof(busid_table));\n\n\tspin_lock_init(&busid_table_lock);\n\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tspin_lock_init(&busid_table[i].busid_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,7 @@\n static void init_busid_table(void)\n {\n+\tint i;\n+\n \t/*\n \t * This also sets the bus_table[i].status to\n \t * STUB_BUSID_OTHER, which is 0.\n@@ -7,4 +9,7 @@\n \tmemset(busid_table, 0, sizeof(busid_table));\n \n \tspin_lock_init(&busid_table_lock);\n+\n+\tfor (i = 0; i < MAX_BUSID; i++)\n+\t\tspin_lock_init(&busid_table[i].busid_lock);\n }",
        "function_modified_lines": {
            "added": [
                "\tint i;",
                "",
                "",
                "\tfor (i = 0; i < MAX_BUSID; i++)",
                "\t\tspin_lock_init(&busid_table[i].busid_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets."
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "static ssize_t rebind_store(struct device_driver *dev, const char *buf,\n\t\t\t\t size_t count)\n{\n\tint ret;\n\tint len;\n\tstruct bus_id_priv *bid;\n\n\t/* buf length should be less that BUSID_SIZE */\n\tlen = strnlen(buf, BUSID_SIZE);\n\n\tif (!(len < BUSID_SIZE))\n\t\treturn -EINVAL;\n\n\tbid = get_busid_priv(buf);\n\tif (!bid)\n\t\treturn -ENODEV;\n\n\t/* mark the device for deletion so probe ignores it during rescan */\n\tbid->status = STUB_BUSID_OTHER;\n\n\tret = do_rebind((char *) buf, bid);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* delete device from busid_table */\n\tdel_match_busid((char *) buf);\n\n\treturn count;\n}",
        "code_after_change": "static ssize_t rebind_store(struct device_driver *dev, const char *buf,\n\t\t\t\t size_t count)\n{\n\tint ret;\n\tint len;\n\tstruct bus_id_priv *bid;\n\n\t/* buf length should be less that BUSID_SIZE */\n\tlen = strnlen(buf, BUSID_SIZE);\n\n\tif (!(len < BUSID_SIZE))\n\t\treturn -EINVAL;\n\n\tbid = get_busid_priv(buf);\n\tif (!bid)\n\t\treturn -ENODEV;\n\n\t/* mark the device for deletion so probe ignores it during rescan */\n\tbid->status = STUB_BUSID_OTHER;\n\t/* release the busid lock */\n\tput_busid_priv(bid);\n\n\tret = do_rebind((char *) buf, bid);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* delete device from busid_table */\n\tdel_match_busid((char *) buf);\n\n\treturn count;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,8 @@\n \n \t/* mark the device for deletion so probe ignores it during rescan */\n \tbid->status = STUB_BUSID_OTHER;\n+\t/* release the busid lock */\n+\tput_busid_priv(bid);\n \n \tret = do_rebind((char *) buf, bid);\n \tif (ret < 0)",
        "function_modified_lines": {
            "added": [
                "\t/* release the busid lock */",
                "\tput_busid_priv(bid);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets."
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "static int add_match_busid(char *busid)\n{\n\tint i;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\t/* already registered? */\n\tif (get_busid_idx(busid) >= 0) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tif (!busid_table[i].name[0]) {\n\t\t\tstrlcpy(busid_table[i].name, busid, BUSID_SIZE);\n\t\t\tif ((busid_table[i].status != STUB_BUSID_ALLOC) &&\n\t\t\t    (busid_table[i].status != STUB_BUSID_REMOV))\n\t\t\t\tbusid_table[i].status = STUB_BUSID_ADDED;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
        "code_after_change": "static int add_match_busid(char *busid)\n{\n\tint i;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\t/* already registered? */\n\tif (get_busid_idx(busid) >= 0) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tspin_lock(&busid_table[i].busid_lock);\n\t\tif (!busid_table[i].name[0]) {\n\t\t\tstrlcpy(busid_table[i].name, busid, BUSID_SIZE);\n\t\t\tif ((busid_table[i].status != STUB_BUSID_ALLOC) &&\n\t\t\t    (busid_table[i].status != STUB_BUSID_REMOV))\n\t\t\t\tbusid_table[i].status = STUB_BUSID_ADDED;\n\t\t\tret = 0;\n\t\t\tspin_unlock(&busid_table[i].busid_lock);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&busid_table[i].busid_lock);\n\t}\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,15 +10,19 @@\n \t\tgoto out;\n \t}\n \n-\tfor (i = 0; i < MAX_BUSID; i++)\n+\tfor (i = 0; i < MAX_BUSID; i++) {\n+\t\tspin_lock(&busid_table[i].busid_lock);\n \t\tif (!busid_table[i].name[0]) {\n \t\t\tstrlcpy(busid_table[i].name, busid, BUSID_SIZE);\n \t\t\tif ((busid_table[i].status != STUB_BUSID_ALLOC) &&\n \t\t\t    (busid_table[i].status != STUB_BUSID_REMOV))\n \t\t\t\tbusid_table[i].status = STUB_BUSID_ADDED;\n \t\t\tret = 0;\n+\t\t\tspin_unlock(&busid_table[i].busid_lock);\n \t\t\tbreak;\n \t\t}\n+\t\tspin_unlock(&busid_table[i].busid_lock);\n+\t}\n \n out:\n \tspin_unlock(&busid_table_lock);",
        "function_modified_lines": {
            "added": [
                "\tfor (i = 0; i < MAX_BUSID; i++) {",
                "\t\tspin_lock(&busid_table[i].busid_lock);",
                "\t\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t}"
            ],
            "deleted": [
                "\tfor (i = 0; i < MAX_BUSID; i++)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets."
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "static int get_busid_idx(const char *busid)\n{\n\tint i;\n\tint idx = -1;\n\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tif (busid_table[i].name[0])\n\t\t\tif (!strncmp(busid_table[i].name, busid, BUSID_SIZE)) {\n\t\t\t\tidx = i;\n\t\t\t\tbreak;\n\t\t\t}\n\treturn idx;\n}",
        "code_after_change": "static int get_busid_idx(const char *busid)\n{\n\tint i;\n\tint idx = -1;\n\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tspin_lock(&busid_table[i].busid_lock);\n\t\tif (busid_table[i].name[0])\n\t\t\tif (!strncmp(busid_table[i].name, busid, BUSID_SIZE)) {\n\t\t\t\tidx = i;\n\t\t\t\tspin_unlock(&busid_table[i].busid_lock);\n\t\t\t\tbreak;\n\t\t\t}\n\t\tspin_unlock(&busid_table[i].busid_lock);\n\t}\n\treturn idx;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,11 +3,15 @@\n \tint i;\n \tint idx = -1;\n \n-\tfor (i = 0; i < MAX_BUSID; i++)\n+\tfor (i = 0; i < MAX_BUSID; i++) {\n+\t\tspin_lock(&busid_table[i].busid_lock);\n \t\tif (busid_table[i].name[0])\n \t\t\tif (!strncmp(busid_table[i].name, busid, BUSID_SIZE)) {\n \t\t\t\tidx = i;\n+\t\t\t\tspin_unlock(&busid_table[i].busid_lock);\n \t\t\t\tbreak;\n \t\t\t}\n+\t\tspin_unlock(&busid_table[i].busid_lock);\n+\t}\n \treturn idx;\n }",
        "function_modified_lines": {
            "added": [
                "\tfor (i = 0; i < MAX_BUSID; i++) {",
                "\t\tspin_lock(&busid_table[i].busid_lock);",
                "\t\t\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t}"
            ],
            "deleted": [
                "\tfor (i = 0; i < MAX_BUSID; i++)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets."
    },
    {
        "cve_id": "CVE-2018-5873",
        "code_before_change": "static void *__ns_get_path(struct path *path, struct ns_common *ns)\n{\n\tstruct vfsmount *mnt = nsfs_mnt;\n\tstruct qstr qname = { .name = \"\", };\n\tstruct dentry *dentry;\n\tstruct inode *inode;\n\tunsigned long d;\n\n\trcu_read_lock();\n\td = atomic_long_read(&ns->stashed);\n\tif (!d)\n\t\tgoto slow;\n\tdentry = (struct dentry *)d;\n\tif (!lockref_get_not_dead(&dentry->d_lockref))\n\t\tgoto slow;\n\trcu_read_unlock();\n\tns->ops->put(ns);\ngot_it:\n\tpath->mnt = mntget(mnt);\n\tpath->dentry = dentry;\n\treturn NULL;\nslow:\n\trcu_read_unlock();\n\tinode = new_inode_pseudo(mnt->mnt_sb);\n\tif (!inode) {\n\t\tns->ops->put(ns);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tinode->i_ino = ns->inum;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = current_time(inode);\n\tinode->i_flags |= S_IMMUTABLE;\n\tinode->i_mode = S_IFREG | S_IRUGO;\n\tinode->i_fop = &ns_file_operations;\n\tinode->i_private = ns;\n\n\tdentry = d_alloc_pseudo(mnt->mnt_sb, &qname);\n\tif (!dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\td_instantiate(dentry, inode);\n\tdentry->d_fsdata = (void *)ns->ops;\n\td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n\tif (d) {\n\t\td_delete(dentry);\t/* make sure ->d_prune() does nothing */\n\t\tdput(dentry);\n\t\tcpu_relax();\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\tgoto got_it;\n}",
        "code_after_change": "static void *__ns_get_path(struct path *path, struct ns_common *ns)\n{\n\tstruct vfsmount *mnt = nsfs_mnt;\n\tstruct qstr qname = { .name = \"\", };\n\tstruct dentry *dentry;\n\tstruct inode *inode;\n\tunsigned long d;\n\n\trcu_read_lock();\n\td = atomic_long_read(&ns->stashed);\n\tif (!d)\n\t\tgoto slow;\n\tdentry = (struct dentry *)d;\n\tif (!lockref_get_not_dead(&dentry->d_lockref))\n\t\tgoto slow;\n\trcu_read_unlock();\n\tns->ops->put(ns);\ngot_it:\n\tpath->mnt = mntget(mnt);\n\tpath->dentry = dentry;\n\treturn NULL;\nslow:\n\trcu_read_unlock();\n\tinode = new_inode_pseudo(mnt->mnt_sb);\n\tif (!inode) {\n\t\tns->ops->put(ns);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tinode->i_ino = ns->inum;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = current_time(inode);\n\tinode->i_flags |= S_IMMUTABLE;\n\tinode->i_mode = S_IFREG | S_IRUGO;\n\tinode->i_fop = &ns_file_operations;\n\tinode->i_private = ns;\n\n\tdentry = d_alloc_pseudo(mnt->mnt_sb, &qname);\n\tif (!dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\td_instantiate(dentry, inode);\n\tdentry->d_flags |= DCACHE_RCUACCESS;\n\tdentry->d_fsdata = (void *)ns->ops;\n\td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n\tif (d) {\n\t\td_delete(dentry);\t/* make sure ->d_prune() does nothing */\n\t\tdput(dentry);\n\t\tcpu_relax();\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\tgoto got_it;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,6 +39,7 @@\n \t\treturn ERR_PTR(-ENOMEM);\n \t}\n \td_instantiate(dentry, inode);\n+\tdentry->d_flags |= DCACHE_RCUACCESS;\n \tdentry->d_fsdata = (void *)ns->ops;\n \td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n \tif (d) {",
        "function_modified_lines": {
            "added": [
                "\tdentry->d_flags |= DCACHE_RCUACCESS;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the __ns_get_path function in fs/nsfs.c in the Linux kernel before 4.11. Due to a race condition when accessing files, a Use After Free condition can occur. This also affects all Android releases from CAF using the Linux kernel (Android for MSM, Firefox OS for MSM, QRD Android) before security patch level 2018-07-05."
    },
    {
        "cve_id": "CVE-2018-7566",
        "code_before_change": "static ssize_t snd_seq_write(struct file *file, const char __user *buf,\n\t\t\t     size_t count, loff_t *offset)\n{\n\tstruct snd_seq_client *client = file->private_data;\n\tint written = 0, len;\n\tint err = -EINVAL;\n\tstruct snd_seq_event event;\n\n\tif (!(snd_seq_file_flags(file) & SNDRV_SEQ_LFLG_OUTPUT))\n\t\treturn -ENXIO;\n\n\t/* check client structures are in place */\n\tif (snd_BUG_ON(!client))\n\t\treturn -ENXIO;\n\t\t\n\tif (!client->accept_output || client->pool == NULL)\n\t\treturn -ENXIO;\n\n\t/* allocate the pool now if the pool is not allocated yet */ \n\tif (client->pool->size > 0 && !snd_seq_write_pool_allocated(client)) {\n\t\tif (snd_seq_pool_init(client->pool) < 0)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* only process whole events */\n\twhile (count >= sizeof(struct snd_seq_event)) {\n\t\t/* Read in the event header from the user */\n\t\tlen = sizeof(event);\n\t\tif (copy_from_user(&event, buf, len)) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tevent.source.client = client->number;\t/* fill in client number */\n\t\t/* Check for extension data length */\n\t\tif (check_event_type_and_length(&event)) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* check for special events */\n\t\tif (event.type == SNDRV_SEQ_EVENT_NONE)\n\t\t\tgoto __skip_event;\n\t\telse if (snd_seq_ev_is_reserved(&event)) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (snd_seq_ev_is_variable(&event)) {\n\t\t\tint extlen = event.data.ext.len & ~SNDRV_SEQ_EXT_MASK;\n\t\t\tif ((size_t)(extlen + len) > count) {\n\t\t\t\t/* back out, will get an error this time or next */\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* set user space pointer */\n\t\t\tevent.data.ext.len = extlen | SNDRV_SEQ_EXT_USRPTR;\n\t\t\tevent.data.ext.ptr = (char __force *)buf\n\t\t\t\t\t\t+ sizeof(struct snd_seq_event);\n\t\t\tlen += extlen; /* increment data length */\n\t\t} else {\n#ifdef CONFIG_COMPAT\n\t\t\tif (client->convert32 && snd_seq_ev_is_varusr(&event)) {\n\t\t\t\tvoid *ptr = (void __force *)compat_ptr(event.data.raw32.d[1]);\n\t\t\t\tevent.data.ext.ptr = ptr;\n\t\t\t}\n#endif\n\t\t}\n\n\t\t/* ok, enqueue it */\n\t\terr = snd_seq_client_enqueue_event(client, &event, file,\n\t\t\t\t\t\t   !(file->f_flags & O_NONBLOCK),\n\t\t\t\t\t\t   0, 0);\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t__skip_event:\n\t\t/* Update pointers and counts */\n\t\tcount -= len;\n\t\tbuf += len;\n\t\twritten += len;\n\t}\n\n\treturn written ? written : err;\n}",
        "code_after_change": "static ssize_t snd_seq_write(struct file *file, const char __user *buf,\n\t\t\t     size_t count, loff_t *offset)\n{\n\tstruct snd_seq_client *client = file->private_data;\n\tint written = 0, len;\n\tint err;\n\tstruct snd_seq_event event;\n\n\tif (!(snd_seq_file_flags(file) & SNDRV_SEQ_LFLG_OUTPUT))\n\t\treturn -ENXIO;\n\n\t/* check client structures are in place */\n\tif (snd_BUG_ON(!client))\n\t\treturn -ENXIO;\n\t\t\n\tif (!client->accept_output || client->pool == NULL)\n\t\treturn -ENXIO;\n\n\t/* allocate the pool now if the pool is not allocated yet */ \n\tif (client->pool->size > 0 && !snd_seq_write_pool_allocated(client)) {\n\t\tmutex_lock(&client->ioctl_mutex);\n\t\terr = snd_seq_pool_init(client->pool);\n\t\tmutex_unlock(&client->ioctl_mutex);\n\t\tif (err < 0)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* only process whole events */\n\terr = -EINVAL;\n\twhile (count >= sizeof(struct snd_seq_event)) {\n\t\t/* Read in the event header from the user */\n\t\tlen = sizeof(event);\n\t\tif (copy_from_user(&event, buf, len)) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tevent.source.client = client->number;\t/* fill in client number */\n\t\t/* Check for extension data length */\n\t\tif (check_event_type_and_length(&event)) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* check for special events */\n\t\tif (event.type == SNDRV_SEQ_EVENT_NONE)\n\t\t\tgoto __skip_event;\n\t\telse if (snd_seq_ev_is_reserved(&event)) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (snd_seq_ev_is_variable(&event)) {\n\t\t\tint extlen = event.data.ext.len & ~SNDRV_SEQ_EXT_MASK;\n\t\t\tif ((size_t)(extlen + len) > count) {\n\t\t\t\t/* back out, will get an error this time or next */\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* set user space pointer */\n\t\t\tevent.data.ext.len = extlen | SNDRV_SEQ_EXT_USRPTR;\n\t\t\tevent.data.ext.ptr = (char __force *)buf\n\t\t\t\t\t\t+ sizeof(struct snd_seq_event);\n\t\t\tlen += extlen; /* increment data length */\n\t\t} else {\n#ifdef CONFIG_COMPAT\n\t\t\tif (client->convert32 && snd_seq_ev_is_varusr(&event)) {\n\t\t\t\tvoid *ptr = (void __force *)compat_ptr(event.data.raw32.d[1]);\n\t\t\t\tevent.data.ext.ptr = ptr;\n\t\t\t}\n#endif\n\t\t}\n\n\t\t/* ok, enqueue it */\n\t\terr = snd_seq_client_enqueue_event(client, &event, file,\n\t\t\t\t\t\t   !(file->f_flags & O_NONBLOCK),\n\t\t\t\t\t\t   0, 0);\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t__skip_event:\n\t\t/* Update pointers and counts */\n\t\tcount -= len;\n\t\tbuf += len;\n\t\twritten += len;\n\t}\n\n\treturn written ? written : err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n {\n \tstruct snd_seq_client *client = file->private_data;\n \tint written = 0, len;\n-\tint err = -EINVAL;\n+\tint err;\n \tstruct snd_seq_event event;\n \n \tif (!(snd_seq_file_flags(file) & SNDRV_SEQ_LFLG_OUTPUT))\n@@ -18,11 +18,15 @@\n \n \t/* allocate the pool now if the pool is not allocated yet */ \n \tif (client->pool->size > 0 && !snd_seq_write_pool_allocated(client)) {\n-\t\tif (snd_seq_pool_init(client->pool) < 0)\n+\t\tmutex_lock(&client->ioctl_mutex);\n+\t\terr = snd_seq_pool_init(client->pool);\n+\t\tmutex_unlock(&client->ioctl_mutex);\n+\t\tif (err < 0)\n \t\t\treturn -ENOMEM;\n \t}\n \n \t/* only process whole events */\n+\terr = -EINVAL;\n \twhile (count >= sizeof(struct snd_seq_event)) {\n \t\t/* Read in the event header from the user */\n \t\tlen = sizeof(event);",
        "function_modified_lines": {
            "added": [
                "\tint err;",
                "\t\tmutex_lock(&client->ioctl_mutex);",
                "\t\terr = snd_seq_pool_init(client->pool);",
                "\t\tmutex_unlock(&client->ioctl_mutex);",
                "\t\tif (err < 0)",
                "\terr = -EINVAL;"
            ],
            "deleted": [
                "\tint err = -EINVAL;",
                "\t\tif (snd_seq_pool_init(client->pool) < 0)"
            ]
        },
        "cwe": [
            "CWE-119",
            "CWE-362"
        ],
        "cve_description": "The Linux kernel 4.15 has a Buffer Overflow via an SNDRV_SEQ_IOCTL_SET_CLIENT_POOL ioctl write operation to /dev/snd/seq by a local user."
    },
    {
        "cve_id": "CVE-2018-7995",
        "code_before_change": "static ssize_t store_int_with_restart(struct device *s,\n\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t      const char *buf, size_t size)\n{\n\tssize_t ret = device_store_int(s, attr, buf, size);\n\tmce_restart();\n\treturn ret;\n}",
        "code_after_change": "static ssize_t store_int_with_restart(struct device *s,\n\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t      const char *buf, size_t size)\n{\n\tunsigned long old_check_interval = check_interval;\n\tssize_t ret = device_store_ulong(s, attr, buf, size);\n\n\tif (check_interval == old_check_interval)\n\t\treturn ret;\n\n\tif (check_interval < 1)\n\t\tcheck_interval = 1;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tmce_restart();\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,18 @@\n \t\t\t\t      struct device_attribute *attr,\n \t\t\t\t      const char *buf, size_t size)\n {\n-\tssize_t ret = device_store_int(s, attr, buf, size);\n+\tunsigned long old_check_interval = check_interval;\n+\tssize_t ret = device_store_ulong(s, attr, buf, size);\n+\n+\tif (check_interval == old_check_interval)\n+\t\treturn ret;\n+\n+\tif (check_interval < 1)\n+\t\tcheck_interval = 1;\n+\n+\tmutex_lock(&mce_sysfs_mutex);\n \tmce_restart();\n+\tmutex_unlock(&mce_sysfs_mutex);\n+\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tunsigned long old_check_interval = check_interval;",
                "\tssize_t ret = device_store_ulong(s, attr, buf, size);",
                "",
                "\tif (check_interval == old_check_interval)",
                "\t\treturn ret;",
                "",
                "\tif (check_interval < 1)",
                "\t\tcheck_interval = 1;",
                "",
                "\tmutex_lock(&mce_sysfs_mutex);",
                "\tmutex_unlock(&mce_sysfs_mutex);",
                ""
            ],
            "deleted": [
                "\tssize_t ret = device_store_int(s, attr, buf, size);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the store_int_with_restart() function in arch/x86/kernel/cpu/mcheck/mce.c in the Linux kernel through 4.15.7 allows local users to cause a denial of service (panic) by leveraging root access to write to the check_interval file in a /sys/devices/system/machinecheck/machinecheck<cpu number> directory. NOTE: a third party has indicated that this report is not security relevant"
    },
    {
        "cve_id": "CVE-2018-7995",
        "code_before_change": "static ssize_t set_cmci_disabled(struct device *s,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tif (mca_cfg.cmci_disabled ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable cmci */\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.cmci_disabled = true;\n\t\t} else {\n\t\t\t/* enable cmci */\n\t\t\tmca_cfg.cmci_disabled = false;\n\t\t\ton_each_cpu(mce_enable_ce, NULL, 1);\n\t\t}\n\t}\n\treturn size;\n}",
        "code_after_change": "static ssize_t set_cmci_disabled(struct device *s,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tif (mca_cfg.cmci_disabled ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable cmci */\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.cmci_disabled = true;\n\t\t} else {\n\t\t\t/* enable cmci */\n\t\t\tmca_cfg.cmci_disabled = false;\n\t\t\ton_each_cpu(mce_enable_ce, NULL, 1);\n\t\t}\n\t}\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn size;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tif (kstrtou64(buf, 0, &new) < 0)\n \t\treturn -EINVAL;\n \n+\tmutex_lock(&mce_sysfs_mutex);\n \tif (mca_cfg.cmci_disabled ^ !!new) {\n \t\tif (new) {\n \t\t\t/* disable cmci */\n@@ -18,5 +19,7 @@\n \t\t\ton_each_cpu(mce_enable_ce, NULL, 1);\n \t\t}\n \t}\n+\tmutex_unlock(&mce_sysfs_mutex);\n+\n \treturn size;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&mce_sysfs_mutex);",
                "\tmutex_unlock(&mce_sysfs_mutex);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the store_int_with_restart() function in arch/x86/kernel/cpu/mcheck/mce.c in the Linux kernel through 4.15.7 allows local users to cause a denial of service (panic) by leveraging root access to write to the check_interval file in a /sys/devices/system/machinecheck/machinecheck<cpu number> directory. NOTE: a third party has indicated that this report is not security relevant"
    },
    {
        "cve_id": "CVE-2018-7995",
        "code_before_change": "static ssize_t set_ignore_ce(struct device *s,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tif (mca_cfg.ignore_ce ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable ce features */\n\t\t\tmce_timer_delete_all();\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.ignore_ce = true;\n\t\t} else {\n\t\t\t/* enable ce features */\n\t\t\tmca_cfg.ignore_ce = false;\n\t\t\ton_each_cpu(mce_enable_ce, (void *)1, 1);\n\t\t}\n\t}\n\treturn size;\n}",
        "code_after_change": "static ssize_t set_ignore_ce(struct device *s,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tif (mca_cfg.ignore_ce ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable ce features */\n\t\t\tmce_timer_delete_all();\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.ignore_ce = true;\n\t\t} else {\n\t\t\t/* enable ce features */\n\t\t\tmca_cfg.ignore_ce = false;\n\t\t\ton_each_cpu(mce_enable_ce, (void *)1, 1);\n\t\t}\n\t}\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn size;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tif (kstrtou64(buf, 0, &new) < 0)\n \t\treturn -EINVAL;\n \n+\tmutex_lock(&mce_sysfs_mutex);\n \tif (mca_cfg.ignore_ce ^ !!new) {\n \t\tif (new) {\n \t\t\t/* disable ce features */\n@@ -19,5 +20,7 @@\n \t\t\ton_each_cpu(mce_enable_ce, (void *)1, 1);\n \t\t}\n \t}\n+\tmutex_unlock(&mce_sysfs_mutex);\n+\n \treturn size;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&mce_sysfs_mutex);",
                "\tmutex_unlock(&mce_sysfs_mutex);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the store_int_with_restart() function in arch/x86/kernel/cpu/mcheck/mce.c in the Linux kernel through 4.15.7 allows local users to cause a denial of service (panic) by leveraging root access to write to the check_interval file in a /sys/devices/system/machinecheck/machinecheck<cpu number> directory. NOTE: a third party has indicated that this report is not security relevant"
    },
    {
        "cve_id": "CVE-2018-8897",
        "code_before_change": "dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n{\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t/*\n\t * ftrace must be first, everything else may cause a recursive crash.\n\t * See note by declaration of modifying_ftrace_code in ftrace.c\n\t */\n\tif (unlikely(atomic_read(&modifying_ftrace_code)) &&\n\t    ftrace_int3_handler(regs))\n\t\treturn;\n#endif\n\tif (poke_int3_handler(regs))\n\t\treturn;\n\n\tist_enter(regs);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n\tif (kgdb_ll_trap(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */\n\n#ifdef CONFIG_KPROBES\n\tif (kprobe_int3_handler(regs))\n\t\tgoto exit;\n#endif\n\n\tif (notify_die(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n\n\t/*\n\t * Let others (NMI) know that the debug stack is in use\n\t * as we may switch to the interrupt stack.\n\t */\n\tdebug_stack_usage_inc();\n\tcond_local_irq_enable(regs);\n\tdo_trap(X86_TRAP_BP, SIGTRAP, \"int3\", regs, error_code, NULL);\n\tcond_local_irq_disable(regs);\n\tdebug_stack_usage_dec();\nexit:\n\tist_exit(regs);\n}",
        "code_after_change": "dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)\n{\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t/*\n\t * ftrace must be first, everything else may cause a recursive crash.\n\t * See note by declaration of modifying_ftrace_code in ftrace.c\n\t */\n\tif (unlikely(atomic_read(&modifying_ftrace_code)) &&\n\t    ftrace_int3_handler(regs))\n\t\treturn;\n#endif\n\tif (poke_int3_handler(regs))\n\t\treturn;\n\n\t/*\n\t * Use ist_enter despite the fact that we don't use an IST stack.\n\t * We can be called from a kprobe in non-CONTEXT_KERNEL kernel\n\t * mode or even during context tracking state changes.\n\t *\n\t * This means that we can't schedule.  That's okay.\n\t */\n\tist_enter(regs);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n\tif (kgdb_ll_trap(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */\n\n#ifdef CONFIG_KPROBES\n\tif (kprobe_int3_handler(regs))\n\t\tgoto exit;\n#endif\n\n\tif (notify_die(DIE_INT3, \"int3\", regs, error_code, X86_TRAP_BP,\n\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\tgoto exit;\n\n\tcond_local_irq_enable(regs);\n\tdo_trap(X86_TRAP_BP, SIGTRAP, \"int3\", regs, error_code, NULL);\n\tcond_local_irq_disable(regs);\n\nexit:\n\tist_exit(regs);\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,13 @@\n \tif (poke_int3_handler(regs))\n \t\treturn;\n \n+\t/*\n+\t * Use ist_enter despite the fact that we don't use an IST stack.\n+\t * We can be called from a kprobe in non-CONTEXT_KERNEL kernel\n+\t * mode or even during context tracking state changes.\n+\t *\n+\t * This means that we can't schedule.  That's okay.\n+\t */\n \tist_enter(regs);\n \tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n@@ -29,15 +36,10 @@\n \t\t\tSIGTRAP) == NOTIFY_STOP)\n \t\tgoto exit;\n \n-\t/*\n-\t * Let others (NMI) know that the debug stack is in use\n-\t * as we may switch to the interrupt stack.\n-\t */\n-\tdebug_stack_usage_inc();\n \tcond_local_irq_enable(regs);\n \tdo_trap(X86_TRAP_BP, SIGTRAP, \"int3\", regs, error_code, NULL);\n \tcond_local_irq_disable(regs);\n-\tdebug_stack_usage_dec();\n+\n exit:\n \tist_exit(regs);\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * Use ist_enter despite the fact that we don't use an IST stack.",
                "\t * We can be called from a kprobe in non-CONTEXT_KERNEL kernel",
                "\t * mode or even during context tracking state changes.",
                "\t *",
                "\t * This means that we can't schedule.  That's okay.",
                "\t */",
                ""
            ],
            "deleted": [
                "\t/*",
                "\t * Let others (NMI) know that the debug stack is in use",
                "\t * as we may switch to the interrupt stack.",
                "\t */",
                "\tdebug_stack_usage_inc();",
                "\tdebug_stack_usage_dec();"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for #DB exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs."
    },
    {
        "cve_id": "CVE-2019-11815",
        "code_before_change": "static void rds_tcp_kill_sock(struct net *net)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\tLIST_HEAD(tmp_list);\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\tstruct socket *lsock = rtn->rds_tcp_listen_sock;\n\n\trtn->rds_tcp_listen_sock = NULL;\n\trds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n\t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n\n\t\tif (net != c_net || !tc->t_sock)\n\t\t\tcontinue;\n\t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n\t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);\n\t\t} else {\n\t\t\tlist_del(&tc->t_tcp_node);\n\t\t\ttc->t_tcp_node_detached = true;\n\t\t}\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)\n\t\trds_conn_destroy(tc->t_cpath->cp_conn);\n}",
        "code_after_change": "static void rds_tcp_kill_sock(struct net *net)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\tLIST_HEAD(tmp_list);\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\tstruct socket *lsock = rtn->rds_tcp_listen_sock;\n\n\trtn->rds_tcp_listen_sock = NULL;\n\trds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n\t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n\n\t\tif (net != c_net)\n\t\t\tcontinue;\n\t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n\t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);\n\t\t} else {\n\t\t\tlist_del(&tc->t_tcp_node);\n\t\t\ttc->t_tcp_node_detached = true;\n\t\t}\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)\n\t\trds_conn_destroy(tc->t_cpath->cp_conn);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n \t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n \n-\t\tif (net != c_net || !tc->t_sock)\n+\t\tif (net != c_net)\n \t\t\tcontinue;\n \t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n \t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);",
        "function_modified_lines": {
            "added": [
                "\t\tif (net != c_net)"
            ],
            "deleted": [
                "\t\tif (net != c_net || !tc->t_sock)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in rds_tcp_kill_sock in net/rds/tcp.c in the Linux kernel before 5.0.8. There is a race condition leading to a use-after-free, related to net namespace cleanup."
    },
    {
        "cve_id": "CVE-2019-13233",
        "code_before_change": "static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct *desc;\n\tunsigned long limit;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn 0;\n\n\tif (user_64bit_mode(regs) || v8086_mode(regs))\n\t\treturn -1L;\n\n\tif (!sel)\n\t\treturn 0;\n\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn 0;\n\n\t/*\n\t * If the granularity bit is set, the limit is given in multiples\n\t * of 4096. This also means that the 12 least significant bits are\n\t * not tested when checking the segment limits. In practice,\n\t * this means that the segment ends in (limit << 12) + 0xfff.\n\t */\n\tlimit = get_desc_limit(desc);\n\tif (desc->g)\n\t\tlimit = (limit << 12) + 0xfff;\n\n\treturn limit;\n}",
        "code_after_change": "static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct desc;\n\tunsigned long limit;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn 0;\n\n\tif (user_64bit_mode(regs) || v8086_mode(regs))\n\t\treturn -1L;\n\n\tif (!sel)\n\t\treturn 0;\n\n\tif (!get_desc(&desc, sel))\n\t\treturn 0;\n\n\t/*\n\t * If the granularity bit is set, the limit is given in multiples\n\t * of 4096. This also means that the 12 least significant bits are\n\t * not tested when checking the segment limits. In practice,\n\t * this means that the segment ends in (limit << 12) + 0xfff.\n\t */\n\tlimit = get_desc_limit(&desc);\n\tif (desc.g)\n\t\tlimit = (limit << 12) + 0xfff;\n\n\treturn limit;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)\n {\n-\tstruct desc_struct *desc;\n+\tstruct desc_struct desc;\n \tunsigned long limit;\n \tshort sel;\n \n@@ -14,8 +14,7 @@\n \tif (!sel)\n \t\treturn 0;\n \n-\tdesc = get_desc(sel);\n-\tif (!desc)\n+\tif (!get_desc(&desc, sel))\n \t\treturn 0;\n \n \t/*\n@@ -24,8 +23,8 @@\n \t * not tested when checking the segment limits. In practice,\n \t * this means that the segment ends in (limit << 12) + 0xfff.\n \t */\n-\tlimit = get_desc_limit(desc);\n-\tif (desc->g)\n+\tlimit = get_desc_limit(&desc);\n+\tif (desc.g)\n \t\tlimit = (limit << 12) + 0xfff;\n \n \treturn limit;",
        "function_modified_lines": {
            "added": [
                "\tstruct desc_struct desc;",
                "\tif (!get_desc(&desc, sel))",
                "\tlimit = get_desc_limit(&desc);",
                "\tif (desc.g)"
            ],
            "deleted": [
                "\tstruct desc_struct *desc;",
                "\tdesc = get_desc(sel);",
                "\tif (!desc)",
                "\tlimit = get_desc_limit(desc);",
                "\tif (desc->g)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In arch/x86/lib/insn-eval.c in the Linux kernel before 5.1.9, there is a use-after-free for access to an LDT entry because of a race condition between modify_ldt() and a #BR exception for an MPX bounds violation."
    },
    {
        "cve_id": "CVE-2019-13233",
        "code_before_change": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct *desc;\n\tshort sel;\n\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc->type & BIT(3)))\n\t\treturn -EINVAL;\n\n\tswitch ((desc->l << 1) | desc->d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
        "code_after_change": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct desc;\n\tshort sel;\n\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\n\tif (!get_desc(&desc, sel))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc.type & BIT(3)))\n\t\treturn -EINVAL;\n\n\tswitch ((desc.l << 1) | desc.d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n int insn_get_code_seg_params(struct pt_regs *regs)\n {\n-\tstruct desc_struct *desc;\n+\tstruct desc_struct desc;\n \tshort sel;\n \n \tif (v8086_mode(regs))\n@@ -11,8 +11,7 @@\n \tif (sel < 0)\n \t\treturn sel;\n \n-\tdesc = get_desc(sel);\n-\tif (!desc)\n+\tif (!get_desc(&desc, sel))\n \t\treturn -EINVAL;\n \n \t/*\n@@ -20,10 +19,10 @@\n \t * determines whether a segment contains data or code. If this is a data\n \t * segment, return error.\n \t */\n-\tif (!(desc->type & BIT(3)))\n+\tif (!(desc.type & BIT(3)))\n \t\treturn -EINVAL;\n \n-\tswitch ((desc->l << 1) | desc->d) {\n+\tswitch ((desc.l << 1) | desc.d) {\n \tcase 0: /*\n \t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n \t\t * both 16-bit.",
        "function_modified_lines": {
            "added": [
                "\tstruct desc_struct desc;",
                "\tif (!get_desc(&desc, sel))",
                "\tif (!(desc.type & BIT(3)))",
                "\tswitch ((desc.l << 1) | desc.d) {"
            ],
            "deleted": [
                "\tstruct desc_struct *desc;",
                "\tdesc = get_desc(sel);",
                "\tif (!desc)",
                "\tif (!(desc->type & BIT(3)))",
                "\tswitch ((desc->l << 1) | desc->d) {"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In arch/x86/lib/insn-eval.c in the Linux kernel before 5.1.9, there is a use-after-free for access to an LDT entry because of a race condition between modify_ldt() and a #BR exception for an MPX bounds violation."
    },
    {
        "cve_id": "CVE-2019-13233",
        "code_before_change": "unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct *desc;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn -1L;\n\n\tif (v8086_mode(regs))\n\t\t/*\n\t\t * Base is simply the segment selector shifted 4\n\t\t * bits to the right.\n\t\t */\n\t\treturn (unsigned long)(sel << 4);\n\n\tif (user_64bit_mode(regs)) {\n\t\t/*\n\t\t * Only FS or GS will have a base address, the rest of\n\t\t * the segments' bases are forced to 0.\n\t\t */\n\t\tunsigned long base;\n\n\t\tif (seg_reg_idx == INAT_SEG_REG_FS)\n\t\t\trdmsrl(MSR_FS_BASE, base);\n\t\telse if (seg_reg_idx == INAT_SEG_REG_GS)\n\t\t\t/*\n\t\t\t * swapgs was called at the kernel entry point. Thus,\n\t\t\t * MSR_KERNEL_GS_BASE will have the user-space GS base.\n\t\t\t */\n\t\t\trdmsrl(MSR_KERNEL_GS_BASE, base);\n\t\telse\n\t\t\tbase = 0;\n\t\treturn base;\n\t}\n\n\t/* In protected mode the segment selector cannot be null. */\n\tif (!sel)\n\t\treturn -1L;\n\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn -1L;\n\n\treturn get_desc_base(desc);\n}",
        "code_after_change": "unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct desc;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn -1L;\n\n\tif (v8086_mode(regs))\n\t\t/*\n\t\t * Base is simply the segment selector shifted 4\n\t\t * bits to the right.\n\t\t */\n\t\treturn (unsigned long)(sel << 4);\n\n\tif (user_64bit_mode(regs)) {\n\t\t/*\n\t\t * Only FS or GS will have a base address, the rest of\n\t\t * the segments' bases are forced to 0.\n\t\t */\n\t\tunsigned long base;\n\n\t\tif (seg_reg_idx == INAT_SEG_REG_FS)\n\t\t\trdmsrl(MSR_FS_BASE, base);\n\t\telse if (seg_reg_idx == INAT_SEG_REG_GS)\n\t\t\t/*\n\t\t\t * swapgs was called at the kernel entry point. Thus,\n\t\t\t * MSR_KERNEL_GS_BASE will have the user-space GS base.\n\t\t\t */\n\t\t\trdmsrl(MSR_KERNEL_GS_BASE, base);\n\t\telse\n\t\t\tbase = 0;\n\t\treturn base;\n\t}\n\n\t/* In protected mode the segment selector cannot be null. */\n\tif (!sel)\n\t\treturn -1L;\n\n\tif (!get_desc(&desc, sel))\n\t\treturn -1L;\n\n\treturn get_desc_base(&desc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)\n {\n-\tstruct desc_struct *desc;\n+\tstruct desc_struct desc;\n \tshort sel;\n \n \tsel = get_segment_selector(regs, seg_reg_idx);\n@@ -38,9 +38,8 @@\n \tif (!sel)\n \t\treturn -1L;\n \n-\tdesc = get_desc(sel);\n-\tif (!desc)\n+\tif (!get_desc(&desc, sel))\n \t\treturn -1L;\n \n-\treturn get_desc_base(desc);\n+\treturn get_desc_base(&desc);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct desc_struct desc;",
                "\tif (!get_desc(&desc, sel))",
                "\treturn get_desc_base(&desc);"
            ],
            "deleted": [
                "\tstruct desc_struct *desc;",
                "\tdesc = get_desc(sel);",
                "\tif (!desc)",
                "\treturn get_desc_base(desc);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In arch/x86/lib/insn-eval.c in the Linux kernel before 5.1.9, there is a use-after-free for access to an LDT entry because of a race condition between modify_ldt() and a #BR exception for an MPX bounds violation."
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "void vivid_stop_generating_vid_cap(struct vivid_dev *dev, bool *pstreaming)\n{\n\tdprintk(dev, 1, \"%s\\n\", __func__);\n\n\tif (dev->kthread_vid_cap == NULL)\n\t\treturn;\n\n\t*pstreaming = false;\n\tif (pstreaming == &dev->vid_cap_streaming) {\n\t\t/* Release all active buffers */\n\t\twhile (!list_empty(&dev->vid_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vid_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vid_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vid_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->vbi_cap_streaming) {\n\t\twhile (!list_empty(&dev->vbi_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vbi_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vbi_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vbi_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->meta_cap_streaming) {\n\t\twhile (!list_empty(&dev->meta_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->meta_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_meta_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"meta_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (dev->vid_cap_streaming || dev->vbi_cap_streaming ||\n\t    dev->meta_cap_streaming)\n\t\treturn;\n\n\t/* shutdown control thread */\n\tvivid_grab_controls(dev, false);\n\tmutex_unlock(&dev->mutex);\n\tkthread_stop(dev->kthread_vid_cap);\n\tdev->kthread_vid_cap = NULL;\n\tmutex_lock(&dev->mutex);\n}",
        "code_after_change": "void vivid_stop_generating_vid_cap(struct vivid_dev *dev, bool *pstreaming)\n{\n\tdprintk(dev, 1, \"%s\\n\", __func__);\n\n\tif (dev->kthread_vid_cap == NULL)\n\t\treturn;\n\n\t*pstreaming = false;\n\tif (pstreaming == &dev->vid_cap_streaming) {\n\t\t/* Release all active buffers */\n\t\twhile (!list_empty(&dev->vid_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vid_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vid_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vid_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->vbi_cap_streaming) {\n\t\twhile (!list_empty(&dev->vbi_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vbi_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vbi_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vbi_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->meta_cap_streaming) {\n\t\twhile (!list_empty(&dev->meta_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->meta_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_meta_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"meta_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (dev->vid_cap_streaming || dev->vbi_cap_streaming ||\n\t    dev->meta_cap_streaming)\n\t\treturn;\n\n\t/* shutdown control thread */\n\tvivid_grab_controls(dev, false);\n\tkthread_stop(dev->kthread_vid_cap);\n\tdev->kthread_vid_cap = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -58,8 +58,6 @@\n \n \t/* shutdown control thread */\n \tvivid_grab_controls(dev, false);\n-\tmutex_unlock(&dev->mutex);\n \tkthread_stop(dev->kthread_vid_cap);\n \tdev->kthread_vid_cap = NULL;\n-\tmutex_lock(&dev->mutex);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmutex_unlock(&dev->mutex);",
                "\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free."
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "static int vivid_thread_vid_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\tint dropped_bufs;\n\n\tdprintk(dev, 1, \"Video Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->cap_seq_offset = 0;\n\tdev->cap_seq_count = 0;\n\tdev->cap_seq_resync = false;\n\tdev->jiffies_vid_cap = jiffies;\n\tdev->cap_stream_start = ktime_get_ns();\n\tvivid_cap_update_frame_period(dev);\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->mutex);\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->cap_seq_resync) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = dev->cap_seq_count + 1;\n\t\t\tdev->cap_seq_count = 0;\n\t\t\tdev->cap_stream_start += dev->cap_frame_period *\n\t\t\t\t\t\t dev->cap_seq_offset;\n\t\t\tvivid_cap_update_frame_period(dev);\n\t\t\tdev->cap_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_cap.numerator;\n\t\tdenominator = dev->timeperframe_vid_cap.denominator;\n\n\t\tif (dev->field_cap == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdropped_bufs = buffers_since_start + dev->cap_seq_offset - dev->cap_seq_count;\n\t\tdev->cap_seq_count = buffers_since_start + dev->cap_seq_offset;\n\t\tdev->vid_cap_seq_count = dev->cap_seq_count - dev->vid_cap_seq_start;\n\t\tdev->vbi_cap_seq_count = dev->cap_seq_count - dev->vbi_cap_seq_start;\n\t\tdev->meta_cap_seq_count = dev->cap_seq_count - dev->meta_cap_seq_start;\n\n\t\tvivid_thread_vid_cap_tick(dev, dropped_bufs);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * including the current buffer.\n\t\t */\n\t\tnumerators_since_start = ++buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_cap;\n\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Capture Thread End\\n\");\n\treturn 0;\n}",
        "code_after_change": "static int vivid_thread_vid_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\tint dropped_bufs;\n\n\tdprintk(dev, 1, \"Video Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->cap_seq_offset = 0;\n\tdev->cap_seq_count = 0;\n\tdev->cap_seq_resync = false;\n\tdev->jiffies_vid_cap = jiffies;\n\tdev->cap_stream_start = ktime_get_ns();\n\tvivid_cap_update_frame_period(dev);\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (!mutex_trylock(&dev->mutex)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->cap_seq_resync) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = dev->cap_seq_count + 1;\n\t\t\tdev->cap_seq_count = 0;\n\t\t\tdev->cap_stream_start += dev->cap_frame_period *\n\t\t\t\t\t\t dev->cap_seq_offset;\n\t\t\tvivid_cap_update_frame_period(dev);\n\t\t\tdev->cap_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_cap.numerator;\n\t\tdenominator = dev->timeperframe_vid_cap.denominator;\n\n\t\tif (dev->field_cap == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdropped_bufs = buffers_since_start + dev->cap_seq_offset - dev->cap_seq_count;\n\t\tdev->cap_seq_count = buffers_since_start + dev->cap_seq_offset;\n\t\tdev->vid_cap_seq_count = dev->cap_seq_count - dev->vid_cap_seq_start;\n\t\tdev->vbi_cap_seq_count = dev->cap_seq_count - dev->vbi_cap_seq_start;\n\t\tdev->meta_cap_seq_count = dev->cap_seq_count - dev->meta_cap_seq_start;\n\n\t\tvivid_thread_vid_cap_tick(dev, dropped_bufs);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * including the current buffer.\n\t\t */\n\t\tnumerators_since_start = ++buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_cap;\n\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Capture Thread End\\n\");\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,11 @@\n \t\tif (kthread_should_stop())\n \t\t\tbreak;\n \n-\t\tmutex_lock(&dev->mutex);\n+\t\tif (!mutex_trylock(&dev->mutex)) {\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tcontinue;\n+\t\t}\n+\n \t\tcur_jiffies = jiffies;\n \t\tif (dev->cap_seq_resync) {\n \t\t\tdev->jiffies_vid_cap = cur_jiffies;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!mutex_trylock(&dev->mutex)) {",
                "\t\t\tschedule_timeout_uninterruptible(1);",
                "\t\t\tcontinue;",
                "\t\t}",
                ""
            ],
            "deleted": [
                "\t\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free."
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "static int vivid_thread_vid_out(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\n\tdprintk(dev, 1, \"Video Output Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->out_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->out_seq_count = 0xffffff80U;\n\tdev->jiffies_vid_out = jiffies;\n\tdev->vid_out_seq_start = dev->vbi_out_seq_start = 0;\n\tdev->meta_out_seq_start = 0;\n\tdev->out_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->mutex);\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->out_seq_resync) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = dev->out_seq_count + 1;\n\t\t\tdev->out_seq_count = 0;\n\t\t\tdev->out_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_out.numerator;\n\t\tdenominator = dev->timeperframe_vid_out.denominator;\n\n\t\tif (dev->field_out == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_out;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->out_seq_count = buffers_since_start + dev->out_seq_offset;\n\t\tdev->vid_out_seq_count = dev->out_seq_count - dev->vid_out_seq_start;\n\t\tdev->vbi_out_seq_count = dev->out_seq_count - dev->vbi_out_seq_start;\n\t\tdev->meta_out_seq_count = dev->out_seq_count - dev->meta_out_seq_start;\n\n\t\tvivid_thread_vid_out_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tnumerators_since_start = buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_out;\n\n\t\t/* Increase by the 'numerator' of one buffer */\n\t\tnumerators_since_start += numerator;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Output Thread End\\n\");\n\treturn 0;\n}",
        "code_after_change": "static int vivid_thread_vid_out(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\n\tdprintk(dev, 1, \"Video Output Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->out_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->out_seq_count = 0xffffff80U;\n\tdev->jiffies_vid_out = jiffies;\n\tdev->vid_out_seq_start = dev->vbi_out_seq_start = 0;\n\tdev->meta_out_seq_start = 0;\n\tdev->out_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (!mutex_trylock(&dev->mutex)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->out_seq_resync) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = dev->out_seq_count + 1;\n\t\t\tdev->out_seq_count = 0;\n\t\t\tdev->out_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_out.numerator;\n\t\tdenominator = dev->timeperframe_vid_out.denominator;\n\n\t\tif (dev->field_out == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_out;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->out_seq_count = buffers_since_start + dev->out_seq_offset;\n\t\tdev->vid_out_seq_count = dev->out_seq_count - dev->vid_out_seq_start;\n\t\tdev->vbi_out_seq_count = dev->out_seq_count - dev->vbi_out_seq_start;\n\t\tdev->meta_out_seq_count = dev->out_seq_count - dev->meta_out_seq_start;\n\n\t\tvivid_thread_vid_out_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tnumerators_since_start = buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_out;\n\n\t\t/* Increase by the 'numerator' of one buffer */\n\t\tnumerators_since_start += numerator;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Output Thread End\\n\");\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,11 @@\n \t\tif (kthread_should_stop())\n \t\t\tbreak;\n \n-\t\tmutex_lock(&dev->mutex);\n+\t\tif (!mutex_trylock(&dev->mutex)) {\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tcontinue;\n+\t\t}\n+\n \t\tcur_jiffies = jiffies;\n \t\tif (dev->out_seq_resync) {\n \t\t\tdev->jiffies_vid_out = cur_jiffies;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!mutex_trylock(&dev->mutex)) {",
                "\t\t\tschedule_timeout_uninterruptible(1);",
                "\t\t\tcontinue;",
                "\t\t}",
                ""
            ],
            "deleted": [
                "\t\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free."
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "void vivid_stop_generating_vid_out(struct vivid_dev *dev, bool *pstreaming)\n{\n\tdprintk(dev, 1, \"%s\\n\", __func__);\n\n\tif (dev->kthread_vid_out == NULL)\n\t\treturn;\n\n\t*pstreaming = false;\n\tif (pstreaming == &dev->vid_out_streaming) {\n\t\t/* Release all active buffers */\n\t\twhile (!list_empty(&dev->vid_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vid_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vid_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vid_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->vbi_out_streaming) {\n\t\twhile (!list_empty(&dev->vbi_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vbi_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vbi_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vbi_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->meta_out_streaming) {\n\t\twhile (!list_empty(&dev->meta_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->meta_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_meta_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"meta_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (dev->vid_out_streaming || dev->vbi_out_streaming ||\n\t    dev->meta_out_streaming)\n\t\treturn;\n\n\t/* shutdown control thread */\n\tvivid_grab_controls(dev, false);\n\tmutex_unlock(&dev->mutex);\n\tkthread_stop(dev->kthread_vid_out);\n\tdev->kthread_vid_out = NULL;\n\tmutex_lock(&dev->mutex);\n}",
        "code_after_change": "void vivid_stop_generating_vid_out(struct vivid_dev *dev, bool *pstreaming)\n{\n\tdprintk(dev, 1, \"%s\\n\", __func__);\n\n\tif (dev->kthread_vid_out == NULL)\n\t\treturn;\n\n\t*pstreaming = false;\n\tif (pstreaming == &dev->vid_out_streaming) {\n\t\t/* Release all active buffers */\n\t\twhile (!list_empty(&dev->vid_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vid_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vid_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vid_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->vbi_out_streaming) {\n\t\twhile (!list_empty(&dev->vbi_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vbi_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vbi_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vbi_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->meta_out_streaming) {\n\t\twhile (!list_empty(&dev->meta_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->meta_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_meta_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"meta_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (dev->vid_out_streaming || dev->vbi_out_streaming ||\n\t    dev->meta_out_streaming)\n\t\treturn;\n\n\t/* shutdown control thread */\n\tvivid_grab_controls(dev, false);\n\tkthread_stop(dev->kthread_vid_out);\n\tdev->kthread_vid_out = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -58,8 +58,6 @@\n \n \t/* shutdown control thread */\n \tvivid_grab_controls(dev, false);\n-\tmutex_unlock(&dev->mutex);\n \tkthread_stop(dev->kthread_vid_out);\n \tdev->kthread_vid_out = NULL;\n-\tmutex_lock(&dev->mutex);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmutex_unlock(&dev->mutex);",
                "\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free."
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "static void sdr_cap_stop_streaming(struct vb2_queue *vq)\n{\n\tstruct vivid_dev *dev = vb2_get_drv_priv(vq);\n\n\tif (dev->kthread_sdr_cap == NULL)\n\t\treturn;\n\n\twhile (!list_empty(&dev->sdr_cap_active)) {\n\t\tstruct vivid_buffer *buf;\n\n\t\tbuf = list_entry(dev->sdr_cap_active.next,\n\t\t\t\tstruct vivid_buffer, list);\n\t\tlist_del(&buf->list);\n\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t   &dev->ctrl_hdl_sdr_cap);\n\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t}\n\n\t/* shutdown control thread */\n\tmutex_unlock(&dev->mutex);\n\tkthread_stop(dev->kthread_sdr_cap);\n\tdev->kthread_sdr_cap = NULL;\n\tmutex_lock(&dev->mutex);\n}",
        "code_after_change": "static void sdr_cap_stop_streaming(struct vb2_queue *vq)\n{\n\tstruct vivid_dev *dev = vb2_get_drv_priv(vq);\n\n\tif (dev->kthread_sdr_cap == NULL)\n\t\treturn;\n\n\twhile (!list_empty(&dev->sdr_cap_active)) {\n\t\tstruct vivid_buffer *buf;\n\n\t\tbuf = list_entry(dev->sdr_cap_active.next,\n\t\t\t\tstruct vivid_buffer, list);\n\t\tlist_del(&buf->list);\n\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t   &dev->ctrl_hdl_sdr_cap);\n\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t}\n\n\t/* shutdown control thread */\n\tkthread_stop(dev->kthread_sdr_cap);\n\tdev->kthread_sdr_cap = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,8 +17,6 @@\n \t}\n \n \t/* shutdown control thread */\n-\tmutex_unlock(&dev->mutex);\n \tkthread_stop(dev->kthread_sdr_cap);\n \tdev->kthread_sdr_cap = NULL;\n-\tmutex_lock(&dev->mutex);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmutex_unlock(&dev->mutex);",
                "\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free."
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "static int vivid_thread_sdr_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 samples_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\n\tdprintk(dev, 1, \"SDR Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->sdr_cap_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->sdr_cap_seq_offset = 0xffffff80U;\n\tdev->jiffies_sdr_cap = jiffies;\n\tdev->sdr_cap_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->mutex);\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->sdr_cap_seq_resync) {\n\t\t\tdev->jiffies_sdr_cap = cur_jiffies;\n\t\t\tdev->sdr_cap_seq_offset = dev->sdr_cap_seq_count + 1;\n\t\t\tdev->sdr_cap_seq_count = 0;\n\t\t\tdev->sdr_cap_seq_resync = false;\n\t\t}\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_sdr_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start =\n\t\t\t(u64)jiffies_since_start * dev->sdr_adc_freq +\n\t\t\t\t      (HZ * SDR_CAP_SAMPLES_PER_BUF) / 2;\n\t\tdo_div(buffers_since_start, HZ * SDR_CAP_SAMPLES_PER_BUF);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_sdr_cap = cur_jiffies;\n\t\t\tdev->sdr_cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->sdr_cap_seq_count =\n\t\t\tbuffers_since_start + dev->sdr_cap_seq_offset;\n\n\t\tvivid_thread_sdr_cap_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of samples streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tsamples_since_start = buffers_since_start * SDR_CAP_SAMPLES_PER_BUF;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_sdr_cap;\n\n\t\t/* Increase by the number of samples in one buffer */\n\t\tsamples_since_start += SDR_CAP_SAMPLES_PER_BUF;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = samples_since_start * HZ +\n\t\t\t\t\t   dev->sdr_adc_freq / 2;\n\t\tdo_div(next_jiffies_since_start, dev->sdr_adc_freq);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"SDR Capture Thread End\\n\");\n\treturn 0;\n}",
        "code_after_change": "static int vivid_thread_sdr_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 samples_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\n\tdprintk(dev, 1, \"SDR Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->sdr_cap_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->sdr_cap_seq_offset = 0xffffff80U;\n\tdev->jiffies_sdr_cap = jiffies;\n\tdev->sdr_cap_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (!mutex_trylock(&dev->mutex)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->sdr_cap_seq_resync) {\n\t\t\tdev->jiffies_sdr_cap = cur_jiffies;\n\t\t\tdev->sdr_cap_seq_offset = dev->sdr_cap_seq_count + 1;\n\t\t\tdev->sdr_cap_seq_count = 0;\n\t\t\tdev->sdr_cap_seq_resync = false;\n\t\t}\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_sdr_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start =\n\t\t\t(u64)jiffies_since_start * dev->sdr_adc_freq +\n\t\t\t\t      (HZ * SDR_CAP_SAMPLES_PER_BUF) / 2;\n\t\tdo_div(buffers_since_start, HZ * SDR_CAP_SAMPLES_PER_BUF);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_sdr_cap = cur_jiffies;\n\t\t\tdev->sdr_cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->sdr_cap_seq_count =\n\t\t\tbuffers_since_start + dev->sdr_cap_seq_offset;\n\n\t\tvivid_thread_sdr_cap_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of samples streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tsamples_since_start = buffers_since_start * SDR_CAP_SAMPLES_PER_BUF;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_sdr_cap;\n\n\t\t/* Increase by the number of samples in one buffer */\n\t\tsamples_since_start += SDR_CAP_SAMPLES_PER_BUF;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = samples_since_start * HZ +\n\t\t\t\t\t   dev->sdr_adc_freq / 2;\n\t\tdo_div(next_jiffies_since_start, dev->sdr_adc_freq);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"SDR Capture Thread End\\n\");\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -24,7 +24,11 @@\n \t\tif (kthread_should_stop())\n \t\t\tbreak;\n \n-\t\tmutex_lock(&dev->mutex);\n+\t\tif (!mutex_trylock(&dev->mutex)) {\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tcontinue;\n+\t\t}\n+\n \t\tcur_jiffies = jiffies;\n \t\tif (dev->sdr_cap_seq_resync) {\n \t\t\tdev->jiffies_sdr_cap = cur_jiffies;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!mutex_trylock(&dev->mutex)) {",
                "\t\t\tschedule_timeout_uninterruptible(1);",
                "\t\t\tcontinue;",
                "\t\t}",
                ""
            ],
            "deleted": [
                "\t\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free."
    },
    {
        "cve_id": "CVE-2019-19537",
        "code_before_change": "void usb_deregister_dev(struct usb_interface *intf,\n\t\t\tstruct usb_class_driver *class_driver)\n{\n\tif (intf->minor == -1)\n\t\treturn;\n\n\tdev_dbg(&intf->dev, \"removing %d minor\\n\", intf->minor);\n\n\tdown_write(&minor_rwsem);\n\tusb_minors[intf->minor] = NULL;\n\tup_write(&minor_rwsem);\n\n\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));\n\tintf->usb_dev = NULL;\n\tintf->minor = -1;\n\tdestroy_usb_class();\n}",
        "code_after_change": "void usb_deregister_dev(struct usb_interface *intf,\n\t\t\tstruct usb_class_driver *class_driver)\n{\n\tif (intf->minor == -1)\n\t\treturn;\n\n\tdev_dbg(&intf->dev, \"removing %d minor\\n\", intf->minor);\n\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));\n\n\tdown_write(&minor_rwsem);\n\tusb_minors[intf->minor] = NULL;\n\tup_write(&minor_rwsem);\n\n\tintf->usb_dev = NULL;\n\tintf->minor = -1;\n\tdestroy_usb_class();\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,12 +5,12 @@\n \t\treturn;\n \n \tdev_dbg(&intf->dev, \"removing %d minor\\n\", intf->minor);\n+\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));\n \n \tdown_write(&minor_rwsem);\n \tusb_minors[intf->minor] = NULL;\n \tup_write(&minor_rwsem);\n \n-\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));\n \tintf->usb_dev = NULL;\n \tintf->minor = -1;\n \tdestroy_usb_class();",
        "function_modified_lines": {
            "added": [
                "\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));"
            ],
            "deleted": [
                "\tdevice_destroy(usb_class->class, MKDEV(USB_MAJOR, intf->minor));"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux kernel before 5.2.10, there is a race condition bug that can be caused by a malicious USB device in the USB character device driver layer, aka CID-303911cfc5b9. This affects drivers/usb/core/file.c."
    },
    {
        "cve_id": "CVE-2019-19537",
        "code_before_change": "int usb_register_dev(struct usb_interface *intf,\n\t\t     struct usb_class_driver *class_driver)\n{\n\tint retval;\n\tint minor_base = class_driver->minor_base;\n\tint minor;\n\tchar name[20];\n\n#ifdef CONFIG_USB_DYNAMIC_MINORS\n\t/*\n\t * We don't care what the device tries to start at, we want to start\n\t * at zero to pack the devices into the smallest available space with\n\t * no holes in the minor range.\n\t */\n\tminor_base = 0;\n#endif\n\n\tif (class_driver->fops == NULL)\n\t\treturn -EINVAL;\n\tif (intf->minor >= 0)\n\t\treturn -EADDRINUSE;\n\n\tmutex_lock(&init_usb_class_mutex);\n\tretval = init_usb_class();\n\tmutex_unlock(&init_usb_class_mutex);\n\n\tif (retval)\n\t\treturn retval;\n\n\tdev_dbg(&intf->dev, \"looking for a minor, starting at %d\\n\", minor_base);\n\n\tdown_write(&minor_rwsem);\n\tfor (minor = minor_base; minor < MAX_USB_MINORS; ++minor) {\n\t\tif (usb_minors[minor])\n\t\t\tcontinue;\n\n\t\tusb_minors[minor] = class_driver->fops;\n\t\tintf->minor = minor;\n\t\tbreak;\n\t}\n\tup_write(&minor_rwsem);\n\tif (intf->minor < 0)\n\t\treturn -EXFULL;\n\n\t/* create a usb class device for this usb interface */\n\tsnprintf(name, sizeof(name), class_driver->name, minor - minor_base);\n\tintf->usb_dev = device_create(usb_class->class, &intf->dev,\n\t\t\t\t      MKDEV(USB_MAJOR, minor), class_driver,\n\t\t\t\t      \"%s\", kbasename(name));\n\tif (IS_ERR(intf->usb_dev)) {\n\t\tdown_write(&minor_rwsem);\n\t\tusb_minors[minor] = NULL;\n\t\tintf->minor = -1;\n\t\tup_write(&minor_rwsem);\n\t\tretval = PTR_ERR(intf->usb_dev);\n\t}\n\treturn retval;\n}",
        "code_after_change": "int usb_register_dev(struct usb_interface *intf,\n\t\t     struct usb_class_driver *class_driver)\n{\n\tint retval;\n\tint minor_base = class_driver->minor_base;\n\tint minor;\n\tchar name[20];\n\n#ifdef CONFIG_USB_DYNAMIC_MINORS\n\t/*\n\t * We don't care what the device tries to start at, we want to start\n\t * at zero to pack the devices into the smallest available space with\n\t * no holes in the minor range.\n\t */\n\tminor_base = 0;\n#endif\n\n\tif (class_driver->fops == NULL)\n\t\treturn -EINVAL;\n\tif (intf->minor >= 0)\n\t\treturn -EADDRINUSE;\n\n\tmutex_lock(&init_usb_class_mutex);\n\tretval = init_usb_class();\n\tmutex_unlock(&init_usb_class_mutex);\n\n\tif (retval)\n\t\treturn retval;\n\n\tdev_dbg(&intf->dev, \"looking for a minor, starting at %d\\n\", minor_base);\n\n\tdown_write(&minor_rwsem);\n\tfor (minor = minor_base; minor < MAX_USB_MINORS; ++minor) {\n\t\tif (usb_minors[minor])\n\t\t\tcontinue;\n\n\t\tusb_minors[minor] = class_driver->fops;\n\t\tintf->minor = minor;\n\t\tbreak;\n\t}\n\tif (intf->minor < 0) {\n\t\tup_write(&minor_rwsem);\n\t\treturn -EXFULL;\n\t}\n\n\t/* create a usb class device for this usb interface */\n\tsnprintf(name, sizeof(name), class_driver->name, minor - minor_base);\n\tintf->usb_dev = device_create(usb_class->class, &intf->dev,\n\t\t\t\t      MKDEV(USB_MAJOR, minor), class_driver,\n\t\t\t\t      \"%s\", kbasename(name));\n\tif (IS_ERR(intf->usb_dev)) {\n\t\tusb_minors[minor] = NULL;\n\t\tintf->minor = -1;\n\t\tretval = PTR_ERR(intf->usb_dev);\n\t}\n\tup_write(&minor_rwsem);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,9 +38,10 @@\n \t\tintf->minor = minor;\n \t\tbreak;\n \t}\n-\tup_write(&minor_rwsem);\n-\tif (intf->minor < 0)\n+\tif (intf->minor < 0) {\n+\t\tup_write(&minor_rwsem);\n \t\treturn -EXFULL;\n+\t}\n \n \t/* create a usb class device for this usb interface */\n \tsnprintf(name, sizeof(name), class_driver->name, minor - minor_base);\n@@ -48,11 +49,10 @@\n \t\t\t\t      MKDEV(USB_MAJOR, minor), class_driver,\n \t\t\t\t      \"%s\", kbasename(name));\n \tif (IS_ERR(intf->usb_dev)) {\n-\t\tdown_write(&minor_rwsem);\n \t\tusb_minors[minor] = NULL;\n \t\tintf->minor = -1;\n-\t\tup_write(&minor_rwsem);\n \t\tretval = PTR_ERR(intf->usb_dev);\n \t}\n+\tup_write(&minor_rwsem);\n \treturn retval;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (intf->minor < 0) {",
                "\t\tup_write(&minor_rwsem);",
                "\t}",
                "\tup_write(&minor_rwsem);"
            ],
            "deleted": [
                "\tup_write(&minor_rwsem);",
                "\tif (intf->minor < 0)",
                "\t\tdown_write(&minor_rwsem);",
                "\t\tup_write(&minor_rwsem);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux kernel before 5.2.10, there is a race condition bug that can be caused by a malicious USB device in the USB character device driver layer, aka CID-303911cfc5b9. This affects drivers/usb/core/file.c."
    },
    {
        "cve_id": "CVE-2019-2213",
        "code_before_change": "static void binder_free_transaction(struct binder_transaction *t)\n{\n\tif (t->buffer)\n\t\tt->buffer->transaction = NULL;\n\tbinder_free_txn_fixups(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n}",
        "code_after_change": "static void binder_free_transaction(struct binder_transaction *t)\n{\n\tstruct binder_proc *target_proc = t->to_proc;\n\n\tif (target_proc) {\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (t->buffer)\n\t\t\tt->buffer->transaction = NULL;\n\t\tbinder_inner_proc_unlock(target_proc);\n\t}\n\t/*\n\t * If the transaction has no target_proc, then\n\t * t->buffer->transaction has already been cleared.\n\t */\n\tbinder_free_txn_fixups(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,17 @@\n static void binder_free_transaction(struct binder_transaction *t)\n {\n-\tif (t->buffer)\n-\t\tt->buffer->transaction = NULL;\n+\tstruct binder_proc *target_proc = t->to_proc;\n+\n+\tif (target_proc) {\n+\t\tbinder_inner_proc_lock(target_proc);\n+\t\tif (t->buffer)\n+\t\t\tt->buffer->transaction = NULL;\n+\t\tbinder_inner_proc_unlock(target_proc);\n+\t}\n+\t/*\n+\t * If the transaction has no target_proc, then\n+\t * t->buffer->transaction has already been cleared.\n+\t */\n \tbinder_free_txn_fixups(t);\n \tkfree(t);\n \tbinder_stats_deleted(BINDER_STAT_TRANSACTION);",
        "function_modified_lines": {
            "added": [
                "\tstruct binder_proc *target_proc = t->to_proc;",
                "",
                "\tif (target_proc) {",
                "\t\tbinder_inner_proc_lock(target_proc);",
                "\t\tif (t->buffer)",
                "\t\t\tt->buffer->transaction = NULL;",
                "\t\tbinder_inner_proc_unlock(target_proc);",
                "\t}",
                "\t/*",
                "\t * If the transaction has no target_proc, then",
                "\t * t->buffer->transaction has already been cleared.",
                "\t */"
            ],
            "deleted": [
                "\tif (t->buffer)",
                "\t\tt->buffer->transaction = NULL;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In binder_free_transaction of binder.c, there is a possible use-after-free due to a race condition. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-133758011References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2019-2213",
        "code_before_change": "static void\nbinder_free_buf(struct binder_proc *proc, struct binder_buffer *buffer)\n{\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, buffer, 0, false);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
        "code_after_change": "static void\nbinder_free_buf(struct binder_proc *proc, struct binder_buffer *buffer)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, buffer, 0, false);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,12 @@\n static void\n binder_free_buf(struct binder_proc *proc, struct binder_buffer *buffer)\n {\n+\tbinder_inner_proc_lock(proc);\n \tif (buffer->transaction) {\n \t\tbuffer->transaction->buffer = NULL;\n \t\tbuffer->transaction = NULL;\n \t}\n+\tbinder_inner_proc_unlock(proc);\n \tif (buffer->async_transaction && buffer->target_node) {\n \t\tstruct binder_node *buf_node;\n \t\tstruct binder_work *w;",
        "function_modified_lines": {
            "added": [
                "\tbinder_inner_proc_lock(proc);",
                "\tbinder_inner_proc_unlock(proc);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In binder_free_transaction of binder.c, there is a possible use-after-free due to a race condition. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-133758011References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2019-3016",
        "code_before_change": "static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)\n{\n\tif (!(vcpu->arch.st.msr_val & KVM_MSR_ENABLED))\n\t\treturn;\n\n\tvcpu->arch.st.steal.preempted = KVM_VCPU_PREEMPTED;\n\n\tkvm_write_guest_offset_cached(vcpu->kvm, &vcpu->arch.st.stime,\n\t\t\t&vcpu->arch.st.steal.preempted,\n\t\t\toffsetof(struct kvm_steal_time, preempted),\n\t\t\tsizeof(vcpu->arch.st.steal.preempted));\n}",
        "code_after_change": "static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)\n{\n\tif (!(vcpu->arch.st.msr_val & KVM_MSR_ENABLED))\n\t\treturn;\n\n\tif (vcpu->arch.st.steal.preempted)\n\t\treturn;\n\n\tvcpu->arch.st.steal.preempted = KVM_VCPU_PREEMPTED;\n\n\tkvm_write_guest_offset_cached(vcpu->kvm, &vcpu->arch.st.stime,\n\t\t\t&vcpu->arch.st.steal.preempted,\n\t\t\toffsetof(struct kvm_steal_time, preempted),\n\t\t\tsizeof(vcpu->arch.st.steal.preempted));\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,9 @@\n static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)\n {\n \tif (!(vcpu->arch.st.msr_val & KVM_MSR_ENABLED))\n+\t\treturn;\n+\n+\tif (vcpu->arch.st.steal.preempted)\n \t\treturn;\n \n \tvcpu->arch.st.steal.preempted = KVM_VCPU_PREEMPTED;",
        "function_modified_lines": {
            "added": [
                "\t\treturn;",
                "",
                "\tif (vcpu->arch.st.steal.preempted)"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In a Linux KVM guest that has PV TLB enabled, a process in the guest kernel may be able to read memory locations from another process in the same guest. This problem is limit to the host running linux kernel 4.10 with a guest running linux kernel 4.16 or later. The problem mainly affects AMD processors but Intel CPUs cannot be ruled out."
    },
    {
        "cve_id": "CVE-2019-6133",
        "code_before_change": "static __latent_entropy struct task_struct *copy_process(\n\t\t\t\t\tunsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace,\n\t\t\t\t\tunsigned long tls,\n\t\t\t\t\tint node)\n{\n\tint retval;\n\tstruct task_struct *p;\n\tstruct multiprocess_signals delayed;\n\n\t/*\n\t * Don't allow sharing the root directory with processes in a different\n\t * namespace\n\t */\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid or user namespace\n\t * do not allow it to share a thread group with the forking task.\n\t */\n\tif (clone_flags & CLONE_THREAD) {\n\t\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\n\t\t    (task_active_pid_ns(current) !=\n\t\t\t\tcurrent->nsproxy->pid_ns_for_children))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * Force any signals received before this point to be delivered\n\t * before the fork happens.  Collect up signals sent to multiple\n\t * processes that happen during the fork and delay them so that\n\t * they appear to happen after the fork.\n\t */\n\tsigemptyset(&delayed.signal);\n\tINIT_HLIST_NODE(&delayed.node);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (!(clone_flags & CLONE_THREAD))\n\t\thlist_add_head(&delayed.node, &current->signal->multiprocess);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n\tretval = -ERESTARTNOINTR;\n\tif (signal_pending(current))\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current, node);\n\tif (!p)\n\t\tgoto fork_out;\n\n\t/*\n\t * This _must_ happen before we call free_task(), i.e. before we jump\n\t * to any of the bad_fork_* labels. This is to avoid freeing\n\t * p->set_child_tid which is (ab)used as a kthread's data pointer for\n\t * kernel threads (PF_KTHREAD).\n\t */\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (p->real_cred->user != INIT_USER &&\n\t\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);\n\tp->flags |= PF_FORKNOEXEC;\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tp->utimescaled = p->stimescaled = 0;\n#endif\n\tprev_cputime_init(&p->prev_cputime);\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqcount_init(&p->vtime.seqcount);\n\tp->vtime.starttime = 0;\n\tp->vtime.state = VTIME_INACTIVE;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n#ifdef CONFIG_PSI\n\tp->psi_flags = 0;\n#endif\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tp->start_time = ktime_get_ns();\n\tp->real_start_time = ktime_get_boot_ns();\n\tp->io_context = NULL;\n\taudit_set_context(p, NULL);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_threadgroup_lock;\n\t}\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n\n\tp->pagefault_disabled = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n\tlockdep_init_task(p);\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_BCACHE\n\tp->sequential_io\t= 0;\n\tp->sequential_io_avg\t= 0;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tretval = sched_fork(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_perf;\n\t/* copy all the process information */\n\tshm_init_task(p);\n\tretval = security_task_alloc(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_security;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tstackleak_task_init(p);\n\n\tif (pid != &init_struct_pid) {\n\t\tpid = alloc_pid(p->nsproxy->pid_ns_for_children);\n\t\tif (IS_ERR(pid)) {\n\t\t\tretval = PTR_ERR(pid);\n\t\t\tgoto bad_fork_cleanup_thread;\n\t\t}\n\t}\n\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tsas_ss_reset(p);\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->exit_signal = -1;\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tif (clone_flags & CLONE_PARENT)\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t\telse\n\t\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\tp->pdeath_signal = 0;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\tcgroup_threadgroup_change_begin(current);\n\t/*\n\t * Ensure that the cgroup subsystem policies allow the new process to be\n\t * forked. It should be noted the the new process's css_set can be changed\n\t * between here and cgroup_post_fork() if an organisation operation is in\n\t * progress.\n\t */\n\tretval = cgroup_can_fork(p);\n\tif (retval)\n\t\tgoto bad_fork_free_pid;\n\n\t/*\n\t * Make it visible to the rest of the system, but dont wake it up yet.\n\t * Need tasklist lock for parent etc handling!\n\t */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Copy seccomp details explicitly here, in case they were changed\n\t * before holding sighand lock.\n\t */\n\tcopy_seccomp(p);\n\n\trseq_fork(p, clone_flags);\n\n\t/* Don't start children in a dying pid namespace */\n\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\n\t\tretval = -ENOMEM;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* Let kill terminate clone/fork in the middle */\n\tif (fatal_signal_pending(current)) {\n\t\tretval = -EINTR;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\n\tinit_task_pid_links(p);\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tinit_task_pid(p, PIDTYPE_PID, pid);\n\t\tif (thread_group_leader(p)) {\n\t\t\tinit_task_pid(p, PIDTYPE_TGID, pid);\n\t\t\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\n\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\t\t\tp->signal->shared_pending.signal = delayed.signal;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\t/*\n\t\t\t * Inherit has_child_subreaper flag under the same\n\t\t\t * tasklist_lock with adding child to the process tree\n\t\t\t * for propagate_has_child_subreaper optimization.\n\t\t\t */\n\t\t\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\n\t\t\t\t\t\t\t p->real_parent->signal->is_child_subreaper;\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\tattach_pid(p, PIDTYPE_TGID);\n\t\t\tattach_pid(p, PIDTYPE_PGID);\n\t\t\tattach_pid(p, PIDTYPE_SID);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t} else {\n\t\t\tcurrent->signal->nr_threads++;\n\t\t\tatomic_inc(&current->signal->live);\n\t\t\tatomic_inc(&current->signal->sigcnt);\n\t\t\ttask_join_group_stop(p);\n\t\t\tlist_add_tail_rcu(&p->thread_group,\n\t\t\t\t\t  &p->group_leader->thread_group);\n\t\t\tlist_add_tail_rcu(&p->thread_node,\n\t\t\t\t\t  &p->signal->thread_head);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID);\n\t\tnr_threads++;\n\t}\n\ttotal_forks++;\n\thlist_del_init(&delayed.node);\n\tspin_unlock(&current->sighand->siglock);\n\tsyscall_tracepoint_update(p);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tcgroup_threadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\tuprobe_copy_process(p, clone_flags);\n\n\treturn p;\n\nbad_fork_cancel_cgroup:\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tcgroup_cancel_fork(p);\nbad_fork_free_pid:\n\tcgroup_threadgroup_change_end(current);\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_thread:\n\texit_thread(p);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_security:\n\tsecurity_task_free(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_perf:\n\tperf_event_free_task(p);\nbad_fork_cleanup_policy:\n\tlockdep_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_threadgroup_lock:\n#endif\n\tdelayacct_tsk_free(p);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tp->state = TASK_DEAD;\n\tput_task_stack(p);\n\tfree_task(p);\nfork_out:\n\tspin_lock_irq(&current->sighand->siglock);\n\thlist_del_init(&delayed.node);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ERR_PTR(retval);\n}",
        "code_after_change": "static __latent_entropy struct task_struct *copy_process(\n\t\t\t\t\tunsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace,\n\t\t\t\t\tunsigned long tls,\n\t\t\t\t\tint node)\n{\n\tint retval;\n\tstruct task_struct *p;\n\tstruct multiprocess_signals delayed;\n\n\t/*\n\t * Don't allow sharing the root directory with processes in a different\n\t * namespace\n\t */\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid or user namespace\n\t * do not allow it to share a thread group with the forking task.\n\t */\n\tif (clone_flags & CLONE_THREAD) {\n\t\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\n\t\t    (task_active_pid_ns(current) !=\n\t\t\t\tcurrent->nsproxy->pid_ns_for_children))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * Force any signals received before this point to be delivered\n\t * before the fork happens.  Collect up signals sent to multiple\n\t * processes that happen during the fork and delay them so that\n\t * they appear to happen after the fork.\n\t */\n\tsigemptyset(&delayed.signal);\n\tINIT_HLIST_NODE(&delayed.node);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (!(clone_flags & CLONE_THREAD))\n\t\thlist_add_head(&delayed.node, &current->signal->multiprocess);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n\tretval = -ERESTARTNOINTR;\n\tif (signal_pending(current))\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current, node);\n\tif (!p)\n\t\tgoto fork_out;\n\n\t/*\n\t * This _must_ happen before we call free_task(), i.e. before we jump\n\t * to any of the bad_fork_* labels. This is to avoid freeing\n\t * p->set_child_tid which is (ab)used as a kthread's data pointer for\n\t * kernel threads (PF_KTHREAD).\n\t */\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (p->real_cred->user != INIT_USER &&\n\t\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);\n\tp->flags |= PF_FORKNOEXEC;\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tp->utimescaled = p->stimescaled = 0;\n#endif\n\tprev_cputime_init(&p->prev_cputime);\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqcount_init(&p->vtime.seqcount);\n\tp->vtime.starttime = 0;\n\tp->vtime.state = VTIME_INACTIVE;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n#ifdef CONFIG_PSI\n\tp->psi_flags = 0;\n#endif\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tp->io_context = NULL;\n\taudit_set_context(p, NULL);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_threadgroup_lock;\n\t}\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n\n\tp->pagefault_disabled = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n\tlockdep_init_task(p);\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_BCACHE\n\tp->sequential_io\t= 0;\n\tp->sequential_io_avg\t= 0;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tretval = sched_fork(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_perf;\n\t/* copy all the process information */\n\tshm_init_task(p);\n\tretval = security_task_alloc(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_security;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tstackleak_task_init(p);\n\n\tif (pid != &init_struct_pid) {\n\t\tpid = alloc_pid(p->nsproxy->pid_ns_for_children);\n\t\tif (IS_ERR(pid)) {\n\t\t\tretval = PTR_ERR(pid);\n\t\t\tgoto bad_fork_cleanup_thread;\n\t\t}\n\t}\n\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tsas_ss_reset(p);\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->exit_signal = -1;\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tif (clone_flags & CLONE_PARENT)\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t\telse\n\t\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\tp->pdeath_signal = 0;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\tcgroup_threadgroup_change_begin(current);\n\t/*\n\t * Ensure that the cgroup subsystem policies allow the new process to be\n\t * forked. It should be noted the the new process's css_set can be changed\n\t * between here and cgroup_post_fork() if an organisation operation is in\n\t * progress.\n\t */\n\tretval = cgroup_can_fork(p);\n\tif (retval)\n\t\tgoto bad_fork_free_pid;\n\n\t/*\n\t * From this point on we must avoid any synchronous user-space\n\t * communication until we take the tasklist-lock. In particular, we do\n\t * not want user-space to be able to predict the process start-time by\n\t * stalling fork(2) after we recorded the start_time but before it is\n\t * visible to the system.\n\t */\n\n\tp->start_time = ktime_get_ns();\n\tp->real_start_time = ktime_get_boot_ns();\n\n\t/*\n\t * Make it visible to the rest of the system, but dont wake it up yet.\n\t * Need tasklist lock for parent etc handling!\n\t */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Copy seccomp details explicitly here, in case they were changed\n\t * before holding sighand lock.\n\t */\n\tcopy_seccomp(p);\n\n\trseq_fork(p, clone_flags);\n\n\t/* Don't start children in a dying pid namespace */\n\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\n\t\tretval = -ENOMEM;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* Let kill terminate clone/fork in the middle */\n\tif (fatal_signal_pending(current)) {\n\t\tretval = -EINTR;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\n\tinit_task_pid_links(p);\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tinit_task_pid(p, PIDTYPE_PID, pid);\n\t\tif (thread_group_leader(p)) {\n\t\t\tinit_task_pid(p, PIDTYPE_TGID, pid);\n\t\t\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\n\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\t\t\tp->signal->shared_pending.signal = delayed.signal;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\t/*\n\t\t\t * Inherit has_child_subreaper flag under the same\n\t\t\t * tasklist_lock with adding child to the process tree\n\t\t\t * for propagate_has_child_subreaper optimization.\n\t\t\t */\n\t\t\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\n\t\t\t\t\t\t\t p->real_parent->signal->is_child_subreaper;\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\tattach_pid(p, PIDTYPE_TGID);\n\t\t\tattach_pid(p, PIDTYPE_PGID);\n\t\t\tattach_pid(p, PIDTYPE_SID);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t} else {\n\t\t\tcurrent->signal->nr_threads++;\n\t\t\tatomic_inc(&current->signal->live);\n\t\t\tatomic_inc(&current->signal->sigcnt);\n\t\t\ttask_join_group_stop(p);\n\t\t\tlist_add_tail_rcu(&p->thread_group,\n\t\t\t\t\t  &p->group_leader->thread_group);\n\t\t\tlist_add_tail_rcu(&p->thread_node,\n\t\t\t\t\t  &p->signal->thread_head);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID);\n\t\tnr_threads++;\n\t}\n\ttotal_forks++;\n\thlist_del_init(&delayed.node);\n\tspin_unlock(&current->sighand->siglock);\n\tsyscall_tracepoint_update(p);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tcgroup_threadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\tuprobe_copy_process(p, clone_flags);\n\n\treturn p;\n\nbad_fork_cancel_cgroup:\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tcgroup_cancel_fork(p);\nbad_fork_free_pid:\n\tcgroup_threadgroup_change_end(current);\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_thread:\n\texit_thread(p);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_security:\n\tsecurity_task_free(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_perf:\n\tperf_event_free_task(p);\nbad_fork_cleanup_policy:\n\tlockdep_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_threadgroup_lock:\n#endif\n\tdelayacct_tsk_free(p);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tp->state = TASK_DEAD;\n\tput_task_stack(p);\n\tfree_task(p);\nfork_out:\n\tspin_lock_irq(&current->sighand->siglock);\n\thlist_del_init(&delayed.node);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ERR_PTR(retval);\n}",
        "patch": "--- code before\n+++ code after\n@@ -161,8 +161,6 @@\n \n \tposix_cpu_timers_init(p);\n \n-\tp->start_time = ktime_get_ns();\n-\tp->real_start_time = ktime_get_boot_ns();\n \tp->io_context = NULL;\n \taudit_set_context(p, NULL);\n \tcgroup_fork(p);\n@@ -327,6 +325,17 @@\n \tretval = cgroup_can_fork(p);\n \tif (retval)\n \t\tgoto bad_fork_free_pid;\n+\n+\t/*\n+\t * From this point on we must avoid any synchronous user-space\n+\t * communication until we take the tasklist-lock. In particular, we do\n+\t * not want user-space to be able to predict the process start-time by\n+\t * stalling fork(2) after we recorded the start_time but before it is\n+\t * visible to the system.\n+\t */\n+\n+\tp->start_time = ktime_get_ns();\n+\tp->real_start_time = ktime_get_boot_ns();\n \n \t/*\n \t * Make it visible to the rest of the system, but dont wake it up yet.",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * From this point on we must avoid any synchronous user-space",
                "\t * communication until we take the tasklist-lock. In particular, we do",
                "\t * not want user-space to be able to predict the process start-time by",
                "\t * stalling fork(2) after we recorded the start_time but before it is",
                "\t * visible to the system.",
                "\t */",
                "",
                "\tp->start_time = ktime_get_ns();",
                "\tp->real_start_time = ktime_get_boot_ns();"
            ],
            "deleted": [
                "\tp->start_time = ktime_get_ns();",
                "\tp->real_start_time = ktime_get_boot_ns();"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In PolicyKit (aka polkit) 0.115, the \"start time\" protection mechanism can be bypassed because fork() is not atomic, and therefore authorization decisions are improperly cached. This is related to lack of uid checking in polkitbackend/polkitbackendinteractiveauthority.c."
    },
    {
        "cve_id": "CVE-2019-6974",
        "code_before_change": "static int kvm_ioctl_create_device(struct kvm *kvm,\n\t\t\t\t   struct kvm_create_device *cd)\n{\n\tstruct kvm_device_ops *ops = NULL;\n\tstruct kvm_device *dev;\n\tbool test = cd->flags & KVM_CREATE_DEVICE_TEST;\n\tint ret;\n\n\tif (cd->type >= ARRAY_SIZE(kvm_device_ops_table))\n\t\treturn -ENODEV;\n\n\tops = kvm_device_ops_table[cd->type];\n\tif (ops == NULL)\n\t\treturn -ENODEV;\n\n\tif (test)\n\t\treturn 0;\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdev->ops = ops;\n\tdev->kvm = kvm;\n\n\tmutex_lock(&kvm->lock);\n\tret = ops->create(dev, cd->type);\n\tif (ret < 0) {\n\t\tmutex_unlock(&kvm->lock);\n\t\tkfree(dev);\n\t\treturn ret;\n\t}\n\tlist_add(&dev->vm_node, &kvm->devices);\n\tmutex_unlock(&kvm->lock);\n\n\tif (ops->init)\n\t\tops->init(dev);\n\n\tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n\tif (ret < 0) {\n\t\tmutex_lock(&kvm->lock);\n\t\tlist_del(&dev->vm_node);\n\t\tmutex_unlock(&kvm->lock);\n\t\tops->destroy(dev);\n\t\treturn ret;\n\t}\n\n\tkvm_get_kvm(kvm);\n\tcd->fd = ret;\n\treturn 0;\n}",
        "code_after_change": "static int kvm_ioctl_create_device(struct kvm *kvm,\n\t\t\t\t   struct kvm_create_device *cd)\n{\n\tstruct kvm_device_ops *ops = NULL;\n\tstruct kvm_device *dev;\n\tbool test = cd->flags & KVM_CREATE_DEVICE_TEST;\n\tint ret;\n\n\tif (cd->type >= ARRAY_SIZE(kvm_device_ops_table))\n\t\treturn -ENODEV;\n\n\tops = kvm_device_ops_table[cd->type];\n\tif (ops == NULL)\n\t\treturn -ENODEV;\n\n\tif (test)\n\t\treturn 0;\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdev->ops = ops;\n\tdev->kvm = kvm;\n\n\tmutex_lock(&kvm->lock);\n\tret = ops->create(dev, cd->type);\n\tif (ret < 0) {\n\t\tmutex_unlock(&kvm->lock);\n\t\tkfree(dev);\n\t\treturn ret;\n\t}\n\tlist_add(&dev->vm_node, &kvm->devices);\n\tmutex_unlock(&kvm->lock);\n\n\tif (ops->init)\n\t\tops->init(dev);\n\n\tkvm_get_kvm(kvm);\n\tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n\tif (ret < 0) {\n\t\tkvm_put_kvm(kvm);\n\t\tmutex_lock(&kvm->lock);\n\t\tlist_del(&dev->vm_node);\n\t\tmutex_unlock(&kvm->lock);\n\t\tops->destroy(dev);\n\t\treturn ret;\n\t}\n\n\tcd->fd = ret;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -36,8 +36,10 @@\n \tif (ops->init)\n \t\tops->init(dev);\n \n+\tkvm_get_kvm(kvm);\n \tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n \tif (ret < 0) {\n+\t\tkvm_put_kvm(kvm);\n \t\tmutex_lock(&kvm->lock);\n \t\tlist_del(&dev->vm_node);\n \t\tmutex_unlock(&kvm->lock);\n@@ -45,7 +47,6 @@\n \t\treturn ret;\n \t}\n \n-\tkvm_get_kvm(kvm);\n \tcd->fd = ret;\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tkvm_get_kvm(kvm);",
                "\t\tkvm_put_kvm(kvm);"
            ],
            "deleted": [
                "\tkvm_get_kvm(kvm);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 4.20.8, kvm_ioctl_create_device in virt/kvm/kvm_main.c mishandles reference counting because of a race condition, leading to a use-after-free."
    },
    {
        "cve_id": "CVE-2020-0030",
        "code_before_change": "static int binder_thread_release(struct binder_proc *proc,\n\t\t\t\t struct binder_thread *thread)\n{\n\tstruct binder_transaction *t;\n\tstruct binder_transaction *send_reply = NULL;\n\tint active_transactions = 0;\n\tstruct binder_transaction *last_t = NULL;\n\n\tbinder_inner_proc_lock(thread->proc);\n\t/*\n\t * take a ref on the proc so it survives\n\t * after we remove this thread from proc->threads.\n\t * The corresponding dec is when we actually\n\t * free the thread in binder_free_thread()\n\t */\n\tproc->tmp_ref++;\n\t/*\n\t * take a ref on this thread to ensure it\n\t * survives while we are releasing it\n\t */\n\tatomic_inc(&thread->tmp_ref);\n\trb_erase(&thread->rb_node, &proc->threads);\n\tt = thread->transaction_stack;\n\tif (t) {\n\t\tspin_lock(&t->lock);\n\t\tif (t->to_thread == thread)\n\t\t\tsend_reply = t;\n\t}\n\tthread->is_dead = true;\n\n\twhile (t) {\n\t\tlast_t = t;\n\t\tactive_transactions++;\n\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t     \"release %d:%d transaction %d %s, still active\\n\",\n\t\t\t      proc->pid, thread->pid,\n\t\t\t     t->debug_id,\n\t\t\t     (t->to_thread == thread) ? \"in\" : \"out\");\n\n\t\tif (t->to_thread == thread) {\n\t\t\tt->to_proc = NULL;\n\t\t\tt->to_thread = NULL;\n\t\t\tif (t->buffer) {\n\t\t\t\tt->buffer->transaction = NULL;\n\t\t\t\tt->buffer = NULL;\n\t\t\t}\n\t\t\tt = t->to_parent;\n\t\t} else if (t->from == thread) {\n\t\t\tt->from = NULL;\n\t\t\tt = t->from_parent;\n\t\t} else\n\t\t\tBUG();\n\t\tspin_unlock(&last_t->lock);\n\t\tif (t)\n\t\t\tspin_lock(&t->lock);\n\t}\n\n\t/*\n\t * If this thread used poll, make sure we remove the waitqueue\n\t * from any epoll data structures holding it with POLLFREE.\n\t * waitqueue_active() is safe to use here because we're holding\n\t * the inner lock.\n\t */\n\tif ((thread->looper & BINDER_LOOPER_STATE_POLL) &&\n\t    waitqueue_active(&thread->wait)) {\n\t\twake_up_poll(&thread->wait, EPOLLHUP | POLLFREE);\n\t}\n\n\tbinder_inner_proc_unlock(thread->proc);\n\n\tif (send_reply)\n\t\tbinder_send_failed_reply(send_reply, BR_DEAD_REPLY);\n\tbinder_release_work(proc, &thread->todo);\n\tbinder_thread_dec_tmpref(thread);\n\treturn active_transactions;\n}",
        "code_after_change": "static int binder_thread_release(struct binder_proc *proc,\n\t\t\t\t struct binder_thread *thread)\n{\n\tstruct binder_transaction *t;\n\tstruct binder_transaction *send_reply = NULL;\n\tint active_transactions = 0;\n\tstruct binder_transaction *last_t = NULL;\n\n\tbinder_inner_proc_lock(thread->proc);\n\t/*\n\t * take a ref on the proc so it survives\n\t * after we remove this thread from proc->threads.\n\t * The corresponding dec is when we actually\n\t * free the thread in binder_free_thread()\n\t */\n\tproc->tmp_ref++;\n\t/*\n\t * take a ref on this thread to ensure it\n\t * survives while we are releasing it\n\t */\n\tatomic_inc(&thread->tmp_ref);\n\trb_erase(&thread->rb_node, &proc->threads);\n\tt = thread->transaction_stack;\n\tif (t) {\n\t\tspin_lock(&t->lock);\n\t\tif (t->to_thread == thread)\n\t\t\tsend_reply = t;\n\t}\n\tthread->is_dead = true;\n\n\twhile (t) {\n\t\tlast_t = t;\n\t\tactive_transactions++;\n\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t     \"release %d:%d transaction %d %s, still active\\n\",\n\t\t\t      proc->pid, thread->pid,\n\t\t\t     t->debug_id,\n\t\t\t     (t->to_thread == thread) ? \"in\" : \"out\");\n\n\t\tif (t->to_thread == thread) {\n\t\t\tt->to_proc = NULL;\n\t\t\tt->to_thread = NULL;\n\t\t\tif (t->buffer) {\n\t\t\t\tt->buffer->transaction = NULL;\n\t\t\t\tt->buffer = NULL;\n\t\t\t}\n\t\t\tt = t->to_parent;\n\t\t} else if (t->from == thread) {\n\t\t\tt->from = NULL;\n\t\t\tt = t->from_parent;\n\t\t} else\n\t\t\tBUG();\n\t\tspin_unlock(&last_t->lock);\n\t\tif (t)\n\t\t\tspin_lock(&t->lock);\n\t}\n\n\t/*\n\t * If this thread used poll, make sure we remove the waitqueue\n\t * from any epoll data structures holding it with POLLFREE.\n\t * waitqueue_active() is safe to use here because we're holding\n\t * the inner lock.\n\t */\n\tif ((thread->looper & BINDER_LOOPER_STATE_POLL) &&\n\t    waitqueue_active(&thread->wait)) {\n\t\twake_up_poll(&thread->wait, EPOLLHUP | POLLFREE);\n\t}\n\n\tbinder_inner_proc_unlock(thread->proc);\n\n\t/*\n\t * This is needed to avoid races between wake_up_poll() above and\n\t * and ep_remove_waitqueue() called for other reasons (eg the epoll file\n\t * descriptor being closed); ep_remove_waitqueue() holds an RCU read\n\t * lock, so we can be sure it's done after calling synchronize_rcu().\n\t */\n\tif (thread->looper & BINDER_LOOPER_STATE_POLL)\n\t\tsynchronize_rcu();\n\n\tif (send_reply)\n\t\tbinder_send_failed_reply(send_reply, BR_DEAD_REPLY);\n\tbinder_release_work(proc, &thread->todo);\n\tbinder_thread_dec_tmpref(thread);\n\treturn active_transactions;\n}",
        "patch": "--- code before\n+++ code after\n@@ -68,6 +68,15 @@\n \n \tbinder_inner_proc_unlock(thread->proc);\n \n+\t/*\n+\t * This is needed to avoid races between wake_up_poll() above and\n+\t * and ep_remove_waitqueue() called for other reasons (eg the epoll file\n+\t * descriptor being closed); ep_remove_waitqueue() holds an RCU read\n+\t * lock, so we can be sure it's done after calling synchronize_rcu().\n+\t */\n+\tif (thread->looper & BINDER_LOOPER_STATE_POLL)\n+\t\tsynchronize_rcu();\n+\n \tif (send_reply)\n \t\tbinder_send_failed_reply(send_reply, BR_DEAD_REPLY);\n \tbinder_release_work(proc, &thread->todo);",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * This is needed to avoid races between wake_up_poll() above and",
                "\t * and ep_remove_waitqueue() called for other reasons (eg the epoll file",
                "\t * descriptor being closed); ep_remove_waitqueue() holds an RCU read",
                "\t * lock, so we can be sure it's done after calling synchronize_rcu().",
                "\t */",
                "\tif (thread->looper & BINDER_LOOPER_STATE_POLL)",
                "\t\tsynchronize_rcu();",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In binder_thread_release of binder.c, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-145286050References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2020-0066",
        "code_before_change": "static int netlink_dump(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct netlink_callback *cb;\n\tstruct sk_buff *skb = NULL;\n\tstruct nlmsghdr *nlh;\n\tint len, err = -ENOBUFS;\n\tint alloc_size;\n\n\tmutex_lock(nlk->cb_mutex);\n\tif (!nlk->cb_running) {\n\t\terr = -EINVAL;\n\t\tgoto errout_skb;\n\t}\n\n\tcb = &nlk->cb;\n\talloc_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n\n\tif (!netlink_rx_is_mmaped(sk) &&\n\t    atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto errout_skb;\n\n\t/* NLMSG_GOODSIZE is small to avoid high order allocations being\n\t * required, but it makes sense to _attempt_ a 16K bytes allocation\n\t * to reduce number of system calls on dump operations, if user\n\t * ever provided a big enough buffer.\n\t */\n\tif (alloc_size < nlk->max_recvmsg_len) {\n\t\tskb = netlink_alloc_skb(sk,\n\t\t\t\t\tnlk->max_recvmsg_len,\n\t\t\t\t\tnlk->portid,\n\t\t\t\t\tGFP_KERNEL |\n\t\t\t\t\t__GFP_NOWARN |\n\t\t\t\t\t__GFP_NORETRY);\n\t\t/* available room should be exact amount to avoid MSG_TRUNC */\n\t\tif (skb)\n\t\t\tskb_reserve(skb, skb_tailroom(skb) -\n\t\t\t\t\t nlk->max_recvmsg_len);\n\t}\n\tif (!skb)\n\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\n\t\t\t\t\tGFP_KERNEL);\n\tif (!skb)\n\t\tgoto errout_skb;\n\tnetlink_skb_set_owner_r(skb, sk);\n\n\tlen = cb->dump(skb, cb);\n\n\tif (len > 0) {\n\t\tmutex_unlock(nlk->cb_mutex);\n\n\t\tif (sk_filter(sk, skb))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\t__netlink_sendskb(sk, skb);\n\t\treturn 0;\n\t}\n\n\tnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\n\tif (!nlh)\n\t\tgoto errout_skb;\n\n\tnl_dump_check_consistent(cb, nlh);\n\n\tmemcpy(nlmsg_data(nlh), &len, sizeof(len));\n\n\tif (sk_filter(sk, skb))\n\t\tkfree_skb(skb);\n\telse\n\t\t__netlink_sendskb(sk, skb);\n\n\tif (cb->done)\n\t\tcb->done(cb);\n\n\tnlk->cb_running = false;\n\tmutex_unlock(nlk->cb_mutex);\n\tmodule_put(cb->module);\n\tconsume_skb(cb->skb);\n\treturn 0;\n\nerrout_skb:\n\tmutex_unlock(nlk->cb_mutex);\n\tkfree_skb(skb);\n\treturn err;\n}",
        "code_after_change": "static int netlink_dump(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct netlink_callback *cb;\n\tstruct sk_buff *skb = NULL;\n\tstruct nlmsghdr *nlh;\n\tint len, err = -ENOBUFS;\n\tint alloc_min_size;\n\tint alloc_size;\n\n\tmutex_lock(nlk->cb_mutex);\n\tif (!nlk->cb_running) {\n\t\terr = -EINVAL;\n\t\tgoto errout_skb;\n\t}\n\n\tif (!netlink_rx_is_mmaped(sk) &&\n\t    atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto errout_skb;\n\n\t/* NLMSG_GOODSIZE is small to avoid high order allocations being\n\t * required, but it makes sense to _attempt_ a 16K bytes allocation\n\t * to reduce number of system calls on dump operations, if user\n\t * ever provided a big enough buffer.\n\t */\n\tcb = &nlk->cb;\n\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n\n\tif (alloc_min_size < nlk->max_recvmsg_len) {\n\t\talloc_size = nlk->max_recvmsg_len;\n\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\n\t\t\t\t\tGFP_KERNEL |\n\t\t\t\t\t__GFP_NOWARN |\n\t\t\t\t\t__GFP_NORETRY);\n\t}\n\tif (!skb) {\n\t\talloc_size = alloc_min_size;\n\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\n\t\t\t\t\tGFP_KERNEL);\n\t}\n\tif (!skb)\n\t\tgoto errout_skb;\n\n\t/* Trim skb to allocated size. User is expected to provide buffer as\n\t * large as max(min_dump_alloc, 16KiB (mac_recvmsg_len capped at\n\t * netlink_recvmsg())). dump will pack as many smaller messages as\n\t * could fit within the allocated skb. skb is typically allocated\n\t * with larger space than required (could be as much as near 2x the\n\t * requested size with align to next power of 2 approach). Allowing\n\t * dump to use the excess space makes it difficult for a user to have a\n\t * reasonable static buffer based on the expected largest dump of a\n\t * single netdev. The outcome is MSG_TRUNC error.\n\t */\n\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);\n\tnetlink_skb_set_owner_r(skb, sk);\n\n\tlen = cb->dump(skb, cb);\n\n\tif (len > 0) {\n\t\tmutex_unlock(nlk->cb_mutex);\n\n\t\tif (sk_filter(sk, skb))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\t__netlink_sendskb(sk, skb);\n\t\treturn 0;\n\t}\n\n\tnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\n\tif (!nlh)\n\t\tgoto errout_skb;\n\n\tnl_dump_check_consistent(cb, nlh);\n\n\tmemcpy(nlmsg_data(nlh), &len, sizeof(len));\n\n\tif (sk_filter(sk, skb))\n\t\tkfree_skb(skb);\n\telse\n\t\t__netlink_sendskb(sk, skb);\n\n\tif (cb->done)\n\t\tcb->done(cb);\n\n\tnlk->cb_running = false;\n\tmutex_unlock(nlk->cb_mutex);\n\tmodule_put(cb->module);\n\tconsume_skb(cb->skb);\n\treturn 0;\n\nerrout_skb:\n\tmutex_unlock(nlk->cb_mutex);\n\tkfree_skb(skb);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tstruct sk_buff *skb = NULL;\n \tstruct nlmsghdr *nlh;\n \tint len, err = -ENOBUFS;\n+\tint alloc_min_size;\n \tint alloc_size;\n \n \tmutex_lock(nlk->cb_mutex);\n@@ -12,9 +13,6 @@\n \t\terr = -EINVAL;\n \t\tgoto errout_skb;\n \t}\n-\n-\tcb = &nlk->cb;\n-\talloc_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n \n \tif (!netlink_rx_is_mmaped(sk) &&\n \t    atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n@@ -25,23 +23,35 @@\n \t * to reduce number of system calls on dump operations, if user\n \t * ever provided a big enough buffer.\n \t */\n-\tif (alloc_size < nlk->max_recvmsg_len) {\n-\t\tskb = netlink_alloc_skb(sk,\n-\t\t\t\t\tnlk->max_recvmsg_len,\n-\t\t\t\t\tnlk->portid,\n+\tcb = &nlk->cb;\n+\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n+\n+\tif (alloc_min_size < nlk->max_recvmsg_len) {\n+\t\talloc_size = nlk->max_recvmsg_len;\n+\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\n \t\t\t\t\tGFP_KERNEL |\n \t\t\t\t\t__GFP_NOWARN |\n \t\t\t\t\t__GFP_NORETRY);\n-\t\t/* available room should be exact amount to avoid MSG_TRUNC */\n-\t\tif (skb)\n-\t\t\tskb_reserve(skb, skb_tailroom(skb) -\n-\t\t\t\t\t nlk->max_recvmsg_len);\n+\t}\n+\tif (!skb) {\n+\t\talloc_size = alloc_min_size;\n+\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\n+\t\t\t\t\tGFP_KERNEL);\n \t}\n \tif (!skb)\n-\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\n-\t\t\t\t\tGFP_KERNEL);\n-\tif (!skb)\n \t\tgoto errout_skb;\n+\n+\t/* Trim skb to allocated size. User is expected to provide buffer as\n+\t * large as max(min_dump_alloc, 16KiB (mac_recvmsg_len capped at\n+\t * netlink_recvmsg())). dump will pack as many smaller messages as\n+\t * could fit within the allocated skb. skb is typically allocated\n+\t * with larger space than required (could be as much as near 2x the\n+\t * requested size with align to next power of 2 approach). Allowing\n+\t * dump to use the excess space makes it difficult for a user to have a\n+\t * reasonable static buffer based on the expected largest dump of a\n+\t * single netdev. The outcome is MSG_TRUNC error.\n+\t */\n+\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);\n \tnetlink_skb_set_owner_r(skb, sk);\n \n \tlen = cb->dump(skb, cb);",
        "function_modified_lines": {
            "added": [
                "\tint alloc_min_size;",
                "\tcb = &nlk->cb;",
                "\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);",
                "",
                "\tif (alloc_min_size < nlk->max_recvmsg_len) {",
                "\t\talloc_size = nlk->max_recvmsg_len;",
                "\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,",
                "\t}",
                "\tif (!skb) {",
                "\t\talloc_size = alloc_min_size;",
                "\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,",
                "\t\t\t\t\tGFP_KERNEL);",
                "",
                "\t/* Trim skb to allocated size. User is expected to provide buffer as",
                "\t * large as max(min_dump_alloc, 16KiB (mac_recvmsg_len capped at",
                "\t * netlink_recvmsg())). dump will pack as many smaller messages as",
                "\t * could fit within the allocated skb. skb is typically allocated",
                "\t * with larger space than required (could be as much as near 2x the",
                "\t * requested size with align to next power of 2 approach). Allowing",
                "\t * dump to use the excess space makes it difficult for a user to have a",
                "\t * reasonable static buffer based on the expected largest dump of a",
                "\t * single netdev. The outcome is MSG_TRUNC error.",
                "\t */",
                "\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);"
            ],
            "deleted": [
                "",
                "\tcb = &nlk->cb;",
                "\talloc_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);",
                "\tif (alloc_size < nlk->max_recvmsg_len) {",
                "\t\tskb = netlink_alloc_skb(sk,",
                "\t\t\t\t\tnlk->max_recvmsg_len,",
                "\t\t\t\t\tnlk->portid,",
                "\t\t/* available room should be exact amount to avoid MSG_TRUNC */",
                "\t\tif (skb)",
                "\t\t\tskb_reserve(skb, skb_tailroom(skb) -",
                "\t\t\t\t\t nlk->max_recvmsg_len);",
                "\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,",
                "\t\t\t\t\tGFP_KERNEL);",
                "\tif (!skb)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-787"
        ],
        "cve_description": "In the netlink driver, there is a possible out of bounds write due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-65025077"
    },
    {
        "cve_id": "CVE-2020-0305",
        "code_before_change": "static struct kobject *cdev_get(struct cdev *p)\n{\n\tstruct module *owner = p->owner;\n\tstruct kobject *kobj;\n\n\tif (owner && !try_module_get(owner))\n\t\treturn NULL;\n\tkobj = kobject_get(&p->kobj);\n\tif (!kobj)\n\t\tmodule_put(owner);\n\treturn kobj;\n}",
        "code_after_change": "static struct kobject *cdev_get(struct cdev *p)\n{\n\tstruct module *owner = p->owner;\n\tstruct kobject *kobj;\n\n\tif (owner && !try_module_get(owner))\n\t\treturn NULL;\n\tkobj = kobject_get_unless_zero(&p->kobj);\n\tif (!kobj)\n\t\tmodule_put(owner);\n\treturn kobj;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \n \tif (owner && !try_module_get(owner))\n \t\treturn NULL;\n-\tkobj = kobject_get(&p->kobj);\n+\tkobj = kobject_get_unless_zero(&p->kobj);\n \tif (!kobj)\n \t\tmodule_put(owner);\n \treturn kobj;",
        "function_modified_lines": {
            "added": [
                "\tkobj = kobject_get_unless_zero(&p->kobj);"
            ],
            "deleted": [
                "\tkobj = kobject_get(&p->kobj);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In cdev_get of char_dev.c, there is a possible use-after-free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android-10Android ID: A-153467744"
    },
    {
        "cve_id": "CVE-2020-11884",
        "code_before_change": "mm_segment_t enable_sacf_uaccess(void)\n{\n\tmm_segment_t old_fs;\n\tunsigned long asce, cr;\n\n\told_fs = current->thread.mm_segment;\n\tif (old_fs & 1)\n\t\treturn old_fs;\n\tcurrent->thread.mm_segment |= 1;\n\tasce = S390_lowcore.kernel_asce;\n\tif (likely(old_fs == USER_DS)) {\n\t\t__ctl_store(cr, 1, 1);\n\t\tif (cr != S390_lowcore.kernel_asce) {\n\t\t\t__ctl_load(S390_lowcore.kernel_asce, 1, 1);\n\t\t\tset_cpu_flag(CIF_ASCE_PRIMARY);\n\t\t}\n\t\tasce = S390_lowcore.user_asce;\n\t}\n\t__ctl_store(cr, 7, 7);\n\tif (cr != asce) {\n\t\t__ctl_load(asce, 7, 7);\n\t\tset_cpu_flag(CIF_ASCE_SECONDARY);\n\t}\n\treturn old_fs;\n}",
        "code_after_change": "mm_segment_t enable_sacf_uaccess(void)\n{\n\tmm_segment_t old_fs;\n\tunsigned long asce, cr;\n\tunsigned long flags;\n\n\told_fs = current->thread.mm_segment;\n\tif (old_fs & 1)\n\t\treturn old_fs;\n\t/* protect against a concurrent page table upgrade */\n\tlocal_irq_save(flags);\n\tcurrent->thread.mm_segment |= 1;\n\tasce = S390_lowcore.kernel_asce;\n\tif (likely(old_fs == USER_DS)) {\n\t\t__ctl_store(cr, 1, 1);\n\t\tif (cr != S390_lowcore.kernel_asce) {\n\t\t\t__ctl_load(S390_lowcore.kernel_asce, 1, 1);\n\t\t\tset_cpu_flag(CIF_ASCE_PRIMARY);\n\t\t}\n\t\tasce = S390_lowcore.user_asce;\n\t}\n\t__ctl_store(cr, 7, 7);\n\tif (cr != asce) {\n\t\t__ctl_load(asce, 7, 7);\n\t\tset_cpu_flag(CIF_ASCE_SECONDARY);\n\t}\n\tlocal_irq_restore(flags);\n\treturn old_fs;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,10 +2,13 @@\n {\n \tmm_segment_t old_fs;\n \tunsigned long asce, cr;\n+\tunsigned long flags;\n \n \told_fs = current->thread.mm_segment;\n \tif (old_fs & 1)\n \t\treturn old_fs;\n+\t/* protect against a concurrent page table upgrade */\n+\tlocal_irq_save(flags);\n \tcurrent->thread.mm_segment |= 1;\n \tasce = S390_lowcore.kernel_asce;\n \tif (likely(old_fs == USER_DS)) {\n@@ -21,5 +24,6 @@\n \t\t__ctl_load(asce, 7, 7);\n \t\tset_cpu_flag(CIF_ASCE_SECONDARY);\n \t}\n+\tlocal_irq_restore(flags);\n \treturn old_fs;\n }",
        "function_modified_lines": {
            "added": [
                "\tunsigned long flags;",
                "\t/* protect against a concurrent page table upgrade */",
                "\tlocal_irq_save(flags);",
                "\tlocal_irq_restore(flags);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux kernel 4.19 through 5.6.7 on the s390 platform, code execution may occur because of a race condition, as demonstrated by code in enable_sacf_uaccess in arch/s390/lib/uaccess.c that fails to protect against a concurrent page table upgrade, aka CID-3f777e19d171. A crash could also occur."
    },
    {
        "cve_id": "CVE-2020-11884",
        "code_before_change": "static void __crst_table_upgrade(void *arg)\n{\n\tstruct mm_struct *mm = arg;\n\n\tif (current->active_mm == mm)\n\t\tset_user_asce(mm);\n\t__tlb_flush_local();\n}",
        "code_after_change": "static void __crst_table_upgrade(void *arg)\n{\n\tstruct mm_struct *mm = arg;\n\n\t/* we must change all active ASCEs to avoid the creation of new TLBs */\n\tif (current->active_mm == mm) {\n\t\tS390_lowcore.user_asce = mm->context.asce;\n\t\tif (current->thread.mm_segment == USER_DS) {\n\t\t\t__ctl_load(S390_lowcore.user_asce, 1, 1);\n\t\t\t/* Mark user-ASCE present in CR1 */\n\t\t\tclear_cpu_flag(CIF_ASCE_PRIMARY);\n\t\t}\n\t\tif (current->thread.mm_segment == USER_DS_SACF) {\n\t\t\t__ctl_load(S390_lowcore.user_asce, 7, 7);\n\t\t\t/* enable_sacf_uaccess does all or nothing */\n\t\t\tWARN_ON(!test_cpu_flag(CIF_ASCE_SECONDARY));\n\t\t}\n\t}\n\t__tlb_flush_local();\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,19 @@\n {\n \tstruct mm_struct *mm = arg;\n \n-\tif (current->active_mm == mm)\n-\t\tset_user_asce(mm);\n+\t/* we must change all active ASCEs to avoid the creation of new TLBs */\n+\tif (current->active_mm == mm) {\n+\t\tS390_lowcore.user_asce = mm->context.asce;\n+\t\tif (current->thread.mm_segment == USER_DS) {\n+\t\t\t__ctl_load(S390_lowcore.user_asce, 1, 1);\n+\t\t\t/* Mark user-ASCE present in CR1 */\n+\t\t\tclear_cpu_flag(CIF_ASCE_PRIMARY);\n+\t\t}\n+\t\tif (current->thread.mm_segment == USER_DS_SACF) {\n+\t\t\t__ctl_load(S390_lowcore.user_asce, 7, 7);\n+\t\t\t/* enable_sacf_uaccess does all or nothing */\n+\t\t\tWARN_ON(!test_cpu_flag(CIF_ASCE_SECONDARY));\n+\t\t}\n+\t}\n \t__tlb_flush_local();\n }",
        "function_modified_lines": {
            "added": [
                "\t/* we must change all active ASCEs to avoid the creation of new TLBs */",
                "\tif (current->active_mm == mm) {",
                "\t\tS390_lowcore.user_asce = mm->context.asce;",
                "\t\tif (current->thread.mm_segment == USER_DS) {",
                "\t\t\t__ctl_load(S390_lowcore.user_asce, 1, 1);",
                "\t\t\t/* Mark user-ASCE present in CR1 */",
                "\t\t\tclear_cpu_flag(CIF_ASCE_PRIMARY);",
                "\t\t}",
                "\t\tif (current->thread.mm_segment == USER_DS_SACF) {",
                "\t\t\t__ctl_load(S390_lowcore.user_asce, 7, 7);",
                "\t\t\t/* enable_sacf_uaccess does all or nothing */",
                "\t\t\tWARN_ON(!test_cpu_flag(CIF_ASCE_SECONDARY));",
                "\t\t}",
                "\t}"
            ],
            "deleted": [
                "\tif (current->active_mm == mm)",
                "\t\tset_user_asce(mm);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux kernel 4.19 through 5.6.7 on the s390 platform, code execution may occur because of a race condition, as demonstrated by code in enable_sacf_uaccess in arch/s390/lib/uaccess.c that fails to protect against a concurrent page table upgrade, aka CID-3f777e19d171. A crash could also occur."
    },
    {
        "cve_id": "CVE-2020-12114",
        "code_before_change": "static void detach_mnt(struct mount *mnt, struct path *old_path)\n{\n\told_path->dentry = mnt->mnt_mountpoint;\n\told_path->mnt = &mnt->mnt_parent->mnt;\n\tput_mountpoint(unhash_mnt(mnt));\n}",
        "code_after_change": "static void detach_mnt(struct mount *mnt, struct path *old_path)\n{\n\told_path->dentry = dget(mnt->mnt_mountpoint);\n\told_path->mnt = &mnt->mnt_parent->mnt;\n\tput_mountpoint(unhash_mnt(mnt));\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static void detach_mnt(struct mount *mnt, struct path *old_path)\n {\n-\told_path->dentry = mnt->mnt_mountpoint;\n+\told_path->dentry = dget(mnt->mnt_mountpoint);\n \told_path->mnt = &mnt->mnt_parent->mnt;\n \tput_mountpoint(unhash_mnt(mnt));\n }",
        "function_modified_lines": {
            "added": [
                "\told_path->dentry = dget(mnt->mnt_mountpoint);"
            ],
            "deleted": [
                "\told_path->dentry = mnt->mnt_mountpoint;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A pivot_root race condition in fs/namespace.c in the Linux kernel 4.4.x before 4.4.221, 4.9.x before 4.9.221, 4.14.x before 4.14.178, 4.19.x before 4.19.119, and 5.x before 5.3 allows local users to cause a denial of service (panic) by corrupting a mountpoint reference counter."
    },
    {
        "cve_id": "CVE-2020-12114",
        "code_before_change": "static void drop_mountpoint(struct fs_pin *p)\n{\n\tstruct mount *m = container_of(p, struct mount, mnt_umount);\n\tdput(m->mnt_ex_mountpoint);\n\tpin_remove(p);\n\tmntput(&m->mnt);\n}",
        "code_after_change": "static void drop_mountpoint(struct fs_pin *p)\n{\n\tstruct mount *m = container_of(p, struct mount, mnt_umount);\n\tpin_remove(p);\n\tmntput(&m->mnt);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,6 @@\n static void drop_mountpoint(struct fs_pin *p)\n {\n \tstruct mount *m = container_of(p, struct mount, mnt_umount);\n-\tdput(m->mnt_ex_mountpoint);\n \tpin_remove(p);\n \tmntput(&m->mnt);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tdput(m->mnt_ex_mountpoint);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A pivot_root race condition in fs/namespace.c in the Linux kernel 4.4.x before 4.4.221, 4.9.x before 4.9.221, 4.14.x before 4.14.178, 4.19.x before 4.19.119, and 5.x before 5.3 allows local users to cause a denial of service (panic) by corrupting a mountpoint reference counter."
    },
    {
        "cve_id": "CVE-2020-12114",
        "code_before_change": "static struct mountpoint *get_mountpoint(struct dentry *dentry)\n{\n\tstruct mountpoint *mp, *new = NULL;\n\tint ret;\n\n\tif (d_mountpoint(dentry)) {\n\t\t/* might be worth a WARN_ON() */\n\t\tif (d_unlinked(dentry))\n\t\t\treturn ERR_PTR(-ENOENT);\nmountpoint:\n\t\tread_seqlock_excl(&mount_lock);\n\t\tmp = lookup_mountpoint(dentry);\n\t\tread_sequnlock_excl(&mount_lock);\n\t\tif (mp)\n\t\t\tgoto done;\n\t}\n\n\tif (!new)\n\t\tnew = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);\n\tif (!new)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\n\t/* Exactly one processes may set d_mounted */\n\tret = d_set_mounted(dentry);\n\n\t/* Someone else set d_mounted? */\n\tif (ret == -EBUSY)\n\t\tgoto mountpoint;\n\n\t/* The dentry is not available as a mountpoint? */\n\tmp = ERR_PTR(ret);\n\tif (ret)\n\t\tgoto done;\n\n\t/* Add the new mountpoint to the hash table */\n\tread_seqlock_excl(&mount_lock);\n\tnew->m_dentry = dentry;\n\tnew->m_count = 1;\n\thlist_add_head(&new->m_hash, mp_hash(dentry));\n\tINIT_HLIST_HEAD(&new->m_list);\n\tread_sequnlock_excl(&mount_lock);\n\n\tmp = new;\n\tnew = NULL;\ndone:\n\tkfree(new);\n\treturn mp;\n}",
        "code_after_change": "static struct mountpoint *get_mountpoint(struct dentry *dentry)\n{\n\tstruct mountpoint *mp, *new = NULL;\n\tint ret;\n\n\tif (d_mountpoint(dentry)) {\n\t\t/* might be worth a WARN_ON() */\n\t\tif (d_unlinked(dentry))\n\t\t\treturn ERR_PTR(-ENOENT);\nmountpoint:\n\t\tread_seqlock_excl(&mount_lock);\n\t\tmp = lookup_mountpoint(dentry);\n\t\tread_sequnlock_excl(&mount_lock);\n\t\tif (mp)\n\t\t\tgoto done;\n\t}\n\n\tif (!new)\n\t\tnew = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);\n\tif (!new)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\n\t/* Exactly one processes may set d_mounted */\n\tret = d_set_mounted(dentry);\n\n\t/* Someone else set d_mounted? */\n\tif (ret == -EBUSY)\n\t\tgoto mountpoint;\n\n\t/* The dentry is not available as a mountpoint? */\n\tmp = ERR_PTR(ret);\n\tif (ret)\n\t\tgoto done;\n\n\t/* Add the new mountpoint to the hash table */\n\tread_seqlock_excl(&mount_lock);\n\tnew->m_dentry = dget(dentry);\n\tnew->m_count = 1;\n\thlist_add_head(&new->m_hash, mp_hash(dentry));\n\tINIT_HLIST_HEAD(&new->m_list);\n\tread_sequnlock_excl(&mount_lock);\n\n\tmp = new;\n\tnew = NULL;\ndone:\n\tkfree(new);\n\treturn mp;\n}",
        "patch": "--- code before\n+++ code after\n@@ -35,7 +35,7 @@\n \n \t/* Add the new mountpoint to the hash table */\n \tread_seqlock_excl(&mount_lock);\n-\tnew->m_dentry = dentry;\n+\tnew->m_dentry = dget(dentry);\n \tnew->m_count = 1;\n \thlist_add_head(&new->m_hash, mp_hash(dentry));\n \tINIT_HLIST_HEAD(&new->m_list);",
        "function_modified_lines": {
            "added": [
                "\tnew->m_dentry = dget(dentry);"
            ],
            "deleted": [
                "\tnew->m_dentry = dentry;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A pivot_root race condition in fs/namespace.c in the Linux kernel 4.4.x before 4.4.221, 4.9.x before 4.9.221, 4.14.x before 4.14.178, 4.19.x before 4.19.119, and 5.x before 5.3 allows local users to cause a denial of service (panic) by corrupting a mountpoint reference counter."
    },
    {
        "cve_id": "CVE-2020-12114",
        "code_before_change": "void mnt_set_mountpoint(struct mount *mnt,\n\t\t\tstruct mountpoint *mp,\n\t\t\tstruct mount *child_mnt)\n{\n\tmp->m_count++;\n\tmnt_add_count(mnt, 1);\t/* essentially, that's mntget */\n\tchild_mnt->mnt_mountpoint = dget(mp->m_dentry);\n\tchild_mnt->mnt_parent = mnt;\n\tchild_mnt->mnt_mp = mp;\n\thlist_add_head(&child_mnt->mnt_mp_list, &mp->m_list);\n}",
        "code_after_change": "void mnt_set_mountpoint(struct mount *mnt,\n\t\t\tstruct mountpoint *mp,\n\t\t\tstruct mount *child_mnt)\n{\n\tmp->m_count++;\n\tmnt_add_count(mnt, 1);\t/* essentially, that's mntget */\n\tchild_mnt->mnt_mountpoint = mp->m_dentry;\n\tchild_mnt->mnt_parent = mnt;\n\tchild_mnt->mnt_mp = mp;\n\thlist_add_head(&child_mnt->mnt_mp_list, &mp->m_list);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n {\n \tmp->m_count++;\n \tmnt_add_count(mnt, 1);\t/* essentially, that's mntget */\n-\tchild_mnt->mnt_mountpoint = dget(mp->m_dentry);\n+\tchild_mnt->mnt_mountpoint = mp->m_dentry;\n \tchild_mnt->mnt_parent = mnt;\n \tchild_mnt->mnt_mp = mp;\n \thlist_add_head(&child_mnt->mnt_mp_list, &mp->m_list);",
        "function_modified_lines": {
            "added": [
                "\tchild_mnt->mnt_mountpoint = mp->m_dentry;"
            ],
            "deleted": [
                "\tchild_mnt->mnt_mountpoint = dget(mp->m_dentry);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A pivot_root race condition in fs/namespace.c in the Linux kernel 4.4.x before 4.4.221, 4.9.x before 4.9.221, 4.14.x before 4.14.178, 4.19.x before 4.19.119, and 5.x before 5.3 allows local users to cause a denial of service (panic) by corrupting a mountpoint reference counter."
    },
    {
        "cve_id": "CVE-2020-12114",
        "code_before_change": "static void mntput_no_expire(struct mount *mnt)\n{\n\trcu_read_lock();\n\tif (likely(READ_ONCE(mnt->mnt_ns))) {\n\t\t/*\n\t\t * Since we don't do lock_mount_hash() here,\n\t\t * ->mnt_ns can change under us.  However, if it's\n\t\t * non-NULL, then there's a reference that won't\n\t\t * be dropped until after an RCU delay done after\n\t\t * turning ->mnt_ns NULL.  So if we observe it\n\t\t * non-NULL under rcu_read_lock(), the reference\n\t\t * we are dropping is not the final one.\n\t\t */\n\t\tmnt_add_count(mnt, -1);\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\t/*\n\t * make sure that if __legitimize_mnt() has not seen us grab\n\t * mount_lock, we'll see their refcount increment here.\n\t */\n\tsmp_mb();\n\tmnt_add_count(mnt, -1);\n\tif (mnt_get_count(mnt)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\n\tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n\t\tstruct mount *p, *tmp;\n\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\tumount_mnt(p);\n\t\t}\n\t}\n\tunlock_mount_hash();\n\n\tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n\t\tstruct task_struct *task = current;\n\t\tif (likely(!(task->flags & PF_KTHREAD))) {\n\t\t\tinit_task_work(&mnt->mnt_rcu, __cleanup_mnt);\n\t\t\tif (!task_work_add(task, &mnt->mnt_rcu, true))\n\t\t\t\treturn;\n\t\t}\n\t\tif (llist_add(&mnt->mnt_llist, &delayed_mntput_list))\n\t\t\tschedule_delayed_work(&delayed_mntput_work, 1);\n\t\treturn;\n\t}\n\tcleanup_mnt(mnt);\n}",
        "code_after_change": "static void mntput_no_expire(struct mount *mnt)\n{\n\tLIST_HEAD(list);\n\n\trcu_read_lock();\n\tif (likely(READ_ONCE(mnt->mnt_ns))) {\n\t\t/*\n\t\t * Since we don't do lock_mount_hash() here,\n\t\t * ->mnt_ns can change under us.  However, if it's\n\t\t * non-NULL, then there's a reference that won't\n\t\t * be dropped until after an RCU delay done after\n\t\t * turning ->mnt_ns NULL.  So if we observe it\n\t\t * non-NULL under rcu_read_lock(), the reference\n\t\t * we are dropping is not the final one.\n\t\t */\n\t\tmnt_add_count(mnt, -1);\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\t/*\n\t * make sure that if __legitimize_mnt() has not seen us grab\n\t * mount_lock, we'll see their refcount increment here.\n\t */\n\tsmp_mb();\n\tmnt_add_count(mnt, -1);\n\tif (mnt_get_count(mnt)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\n\tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n\t\tstruct mount *p, *tmp;\n\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\t__put_mountpoint(unhash_mnt(p), &list);\n\t\t}\n\t}\n\tunlock_mount_hash();\n\tshrink_dentry_list(&list);\n\n\tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n\t\tstruct task_struct *task = current;\n\t\tif (likely(!(task->flags & PF_KTHREAD))) {\n\t\t\tinit_task_work(&mnt->mnt_rcu, __cleanup_mnt);\n\t\t\tif (!task_work_add(task, &mnt->mnt_rcu, true))\n\t\t\t\treturn;\n\t\t}\n\t\tif (llist_add(&mnt->mnt_llist, &delayed_mntput_list))\n\t\t\tschedule_delayed_work(&delayed_mntput_work, 1);\n\t\treturn;\n\t}\n\tcleanup_mnt(mnt);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,7 @@\n static void mntput_no_expire(struct mount *mnt)\n {\n+\tLIST_HEAD(list);\n+\n \trcu_read_lock();\n \tif (likely(READ_ONCE(mnt->mnt_ns))) {\n \t\t/*\n@@ -40,10 +42,11 @@\n \tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n \t\tstruct mount *p, *tmp;\n \t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n-\t\t\tumount_mnt(p);\n+\t\t\t__put_mountpoint(unhash_mnt(p), &list);\n \t\t}\n \t}\n \tunlock_mount_hash();\n+\tshrink_dentry_list(&list);\n \n \tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n \t\tstruct task_struct *task = current;",
        "function_modified_lines": {
            "added": [
                "\tLIST_HEAD(list);",
                "",
                "\t\t\t__put_mountpoint(unhash_mnt(p), &list);",
                "\tshrink_dentry_list(&list);"
            ],
            "deleted": [
                "\t\t\tumount_mnt(p);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A pivot_root race condition in fs/namespace.c in the Linux kernel 4.4.x before 4.4.221, 4.9.x before 4.9.221, 4.14.x before 4.14.178, 4.19.x before 4.19.119, and 5.x before 5.3 allows local users to cause a denial of service (panic) by corrupting a mountpoint reference counter."
    },
    {
        "cve_id": "CVE-2020-12114",
        "code_before_change": "static void umount_mnt(struct mount *mnt)\n{\n\t/* old mountpoint will be dropped when we can do that */\n\tmnt->mnt_ex_mountpoint = mnt->mnt_mountpoint;\n\tput_mountpoint(unhash_mnt(mnt));\n}",
        "code_after_change": "static void umount_mnt(struct mount *mnt)\n{\n\tput_mountpoint(unhash_mnt(mnt));\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,4 @@\n static void umount_mnt(struct mount *mnt)\n {\n-\t/* old mountpoint will be dropped when we can do that */\n-\tmnt->mnt_ex_mountpoint = mnt->mnt_mountpoint;\n \tput_mountpoint(unhash_mnt(mnt));\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t/* old mountpoint will be dropped when we can do that */",
                "\tmnt->mnt_ex_mountpoint = mnt->mnt_mountpoint;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A pivot_root race condition in fs/namespace.c in the Linux kernel 4.4.x before 4.4.221, 4.9.x before 4.9.221, 4.14.x before 4.14.178, 4.19.x before 4.19.119, and 5.x before 5.3 allows local users to cause a denial of service (panic) by corrupting a mountpoint reference counter."
    },
    {
        "cve_id": "CVE-2020-12114",
        "code_before_change": "static void put_mountpoint(struct mountpoint *mp)\n{\n\tif (!--mp->m_count) {\n\t\tstruct dentry *dentry = mp->m_dentry;\n\t\tBUG_ON(!hlist_empty(&mp->m_list));\n\t\tspin_lock(&dentry->d_lock);\n\t\tdentry->d_flags &= ~DCACHE_MOUNTED;\n\t\tspin_unlock(&dentry->d_lock);\n\t\thlist_del(&mp->m_hash);\n\t\tkfree(mp);\n\t}\n}",
        "code_after_change": "static void put_mountpoint(struct mountpoint *mp)\n{\n\t__put_mountpoint(mp, &ex_mountpoints);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,4 @@\n static void put_mountpoint(struct mountpoint *mp)\n {\n-\tif (!--mp->m_count) {\n-\t\tstruct dentry *dentry = mp->m_dentry;\n-\t\tBUG_ON(!hlist_empty(&mp->m_list));\n-\t\tspin_lock(&dentry->d_lock);\n-\t\tdentry->d_flags &= ~DCACHE_MOUNTED;\n-\t\tspin_unlock(&dentry->d_lock);\n-\t\thlist_del(&mp->m_hash);\n-\t\tkfree(mp);\n-\t}\n+\t__put_mountpoint(mp, &ex_mountpoints);\n }",
        "function_modified_lines": {
            "added": [
                "\t__put_mountpoint(mp, &ex_mountpoints);"
            ],
            "deleted": [
                "\tif (!--mp->m_count) {",
                "\t\tstruct dentry *dentry = mp->m_dentry;",
                "\t\tBUG_ON(!hlist_empty(&mp->m_list));",
                "\t\tspin_lock(&dentry->d_lock);",
                "\t\tdentry->d_flags &= ~DCACHE_MOUNTED;",
                "\t\tspin_unlock(&dentry->d_lock);",
                "\t\thlist_del(&mp->m_hash);",
                "\t\tkfree(mp);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A pivot_root race condition in fs/namespace.c in the Linux kernel 4.4.x before 4.4.221, 4.9.x before 4.9.221, 4.14.x before 4.14.178, 4.19.x before 4.19.119, and 5.x before 5.3 allows local users to cause a denial of service (panic) by corrupting a mountpoint reference counter."
    },
    {
        "cve_id": "CVE-2020-12114",
        "code_before_change": "static void namespace_unlock(void)\n{\n\tstruct hlist_head head;\n\n\thlist_move_list(&unmounted, &head);\n\n\tup_write(&namespace_sem);\n\n\tif (likely(hlist_empty(&head)))\n\t\treturn;\n\n\tsynchronize_rcu_expedited();\n\n\tgroup_pin_kill(&head);\n}",
        "code_after_change": "static void namespace_unlock(void)\n{\n\tstruct hlist_head head;\n\tLIST_HEAD(list);\n\n\thlist_move_list(&unmounted, &head);\n\tlist_splice_init(&ex_mountpoints, &list);\n\n\tup_write(&namespace_sem);\n\n\tshrink_dentry_list(&list);\n\n\tif (likely(hlist_empty(&head)))\n\t\treturn;\n\n\tsynchronize_rcu_expedited();\n\n\tgroup_pin_kill(&head);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,14 @@\n static void namespace_unlock(void)\n {\n \tstruct hlist_head head;\n+\tLIST_HEAD(list);\n \n \thlist_move_list(&unmounted, &head);\n+\tlist_splice_init(&ex_mountpoints, &list);\n \n \tup_write(&namespace_sem);\n+\n+\tshrink_dentry_list(&list);\n \n \tif (likely(hlist_empty(&head)))\n \t\treturn;",
        "function_modified_lines": {
            "added": [
                "\tLIST_HEAD(list);",
                "\tlist_splice_init(&ex_mountpoints, &list);",
                "",
                "\tshrink_dentry_list(&list);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A pivot_root race condition in fs/namespace.c in the Linux kernel 4.4.x before 4.4.221, 4.9.x before 4.9.221, 4.14.x before 4.14.178, 4.19.x before 4.19.119, and 5.x before 5.3 allows local users to cause a denial of service (panic) by corrupting a mountpoint reference counter."
    },
    {
        "cve_id": "CVE-2020-12114",
        "code_before_change": "void mnt_change_mountpoint(struct mount *parent, struct mountpoint *mp, struct mount *mnt)\n{\n\tstruct mountpoint *old_mp = mnt->mnt_mp;\n\tstruct dentry *old_mountpoint = mnt->mnt_mountpoint;\n\tstruct mount *old_parent = mnt->mnt_parent;\n\n\tlist_del_init(&mnt->mnt_child);\n\thlist_del_init(&mnt->mnt_mp_list);\n\thlist_del_init_rcu(&mnt->mnt_hash);\n\n\tattach_mnt(mnt, parent, mp);\n\n\tput_mountpoint(old_mp);\n\n\t/*\n\t * Safely avoid even the suggestion this code might sleep or\n\t * lock the mount hash by taking advantage of the knowledge that\n\t * mnt_change_mountpoint will not release the final reference\n\t * to a mountpoint.\n\t *\n\t * During mounting, the mount passed in as the parent mount will\n\t * continue to use the old mountpoint and during unmounting, the\n\t * old mountpoint will continue to exist until namespace_unlock,\n\t * which happens well after mnt_change_mountpoint.\n\t */\n\tspin_lock(&old_mountpoint->d_lock);\n\told_mountpoint->d_lockref.count--;\n\tspin_unlock(&old_mountpoint->d_lock);\n\n\tmnt_add_count(old_parent, -1);\n}",
        "code_after_change": "void mnt_change_mountpoint(struct mount *parent, struct mountpoint *mp, struct mount *mnt)\n{\n\tstruct mountpoint *old_mp = mnt->mnt_mp;\n\tstruct mount *old_parent = mnt->mnt_parent;\n\n\tlist_del_init(&mnt->mnt_child);\n\thlist_del_init(&mnt->mnt_mp_list);\n\thlist_del_init_rcu(&mnt->mnt_hash);\n\n\tattach_mnt(mnt, parent, mp);\n\n\tput_mountpoint(old_mp);\n\tmnt_add_count(old_parent, -1);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,6 @@\n void mnt_change_mountpoint(struct mount *parent, struct mountpoint *mp, struct mount *mnt)\n {\n \tstruct mountpoint *old_mp = mnt->mnt_mp;\n-\tstruct dentry *old_mountpoint = mnt->mnt_mountpoint;\n \tstruct mount *old_parent = mnt->mnt_parent;\n \n \tlist_del_init(&mnt->mnt_child);\n@@ -11,21 +10,5 @@\n \tattach_mnt(mnt, parent, mp);\n \n \tput_mountpoint(old_mp);\n-\n-\t/*\n-\t * Safely avoid even the suggestion this code might sleep or\n-\t * lock the mount hash by taking advantage of the knowledge that\n-\t * mnt_change_mountpoint will not release the final reference\n-\t * to a mountpoint.\n-\t *\n-\t * During mounting, the mount passed in as the parent mount will\n-\t * continue to use the old mountpoint and during unmounting, the\n-\t * old mountpoint will continue to exist until namespace_unlock,\n-\t * which happens well after mnt_change_mountpoint.\n-\t */\n-\tspin_lock(&old_mountpoint->d_lock);\n-\told_mountpoint->d_lockref.count--;\n-\tspin_unlock(&old_mountpoint->d_lock);\n-\n \tmnt_add_count(old_parent, -1);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct dentry *old_mountpoint = mnt->mnt_mountpoint;",
                "",
                "\t/*",
                "\t * Safely avoid even the suggestion this code might sleep or",
                "\t * lock the mount hash by taking advantage of the knowledge that",
                "\t * mnt_change_mountpoint will not release the final reference",
                "\t * to a mountpoint.",
                "\t *",
                "\t * During mounting, the mount passed in as the parent mount will",
                "\t * continue to use the old mountpoint and during unmounting, the",
                "\t * old mountpoint will continue to exist until namespace_unlock,",
                "\t * which happens well after mnt_change_mountpoint.",
                "\t */",
                "\tspin_lock(&old_mountpoint->d_lock);",
                "\told_mountpoint->d_lockref.count--;",
                "\tspin_unlock(&old_mountpoint->d_lock);",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A pivot_root race condition in fs/namespace.c in the Linux kernel 4.4.x before 4.4.221, 4.9.x before 4.9.221, 4.14.x before 4.14.178, 4.19.x before 4.19.119, and 5.x before 5.3 allows local users to cause a denial of service (panic) by corrupting a mountpoint reference counter."
    },
    {
        "cve_id": "CVE-2020-12652",
        "code_before_change": "static int\ncompat_mpt_command(struct file *filp, unsigned int cmd,\n\t\t\tunsigned long arg)\n{\n\tstruct mpt_ioctl_command32 karg32;\n\tstruct mpt_ioctl_command32 __user *uarg = (struct mpt_ioctl_command32 __user *) arg;\n\tstruct mpt_ioctl_command karg;\n\tMPT_ADAPTER *iocp = NULL;\n\tint iocnum, iocnumX;\n\tint nonblock = (filp->f_flags & O_NONBLOCK);\n\tint ret;\n\n\tif (copy_from_user(&karg32, (char __user *)arg, sizeof(karg32)))\n\t\treturn -EFAULT;\n\n\t/* Verify intended MPT adapter */\n\tiocnumX = karg32.hdr.iocnum & 0xFF;\n\tif (((iocnum = mpt_verify_adapter(iocnumX, &iocp)) < 0) ||\n\t    (iocp == NULL)) {\n\t\tprintk(KERN_DEBUG MYNAM \"::compat_mpt_command @%d - ioc%d not found!\\n\",\n\t\t\t__LINE__, iocnumX);\n\t\treturn -ENODEV;\n\t}\n\n\tif ((ret = mptctl_syscall_down(iocp, nonblock)) != 0)\n\t\treturn ret;\n\n\tdctlprintk(iocp, printk(MYIOC_s_DEBUG_FMT \"compat_mpt_command() called\\n\",\n\t    iocp->name));\n\t/* Copy data to karg */\n\tkarg.hdr.iocnum = karg32.hdr.iocnum;\n\tkarg.hdr.port = karg32.hdr.port;\n\tkarg.timeout = karg32.timeout;\n\tkarg.maxReplyBytes = karg32.maxReplyBytes;\n\n\tkarg.dataInSize = karg32.dataInSize;\n\tkarg.dataOutSize = karg32.dataOutSize;\n\tkarg.maxSenseBytes = karg32.maxSenseBytes;\n\tkarg.dataSgeOffset = karg32.dataSgeOffset;\n\n\tkarg.replyFrameBufPtr = (char __user *)(unsigned long)karg32.replyFrameBufPtr;\n\tkarg.dataInBufPtr = (char __user *)(unsigned long)karg32.dataInBufPtr;\n\tkarg.dataOutBufPtr = (char __user *)(unsigned long)karg32.dataOutBufPtr;\n\tkarg.senseDataPtr = (char __user *)(unsigned long)karg32.senseDataPtr;\n\n\t/* Pass new structure to do_mpt_command\n\t */\n\tret = mptctl_do_mpt_command (karg, &uarg->MF);\n\n\tmutex_unlock(&iocp->ioctl_cmds.mutex);\n\n\treturn ret;\n}",
        "code_after_change": "static int\ncompat_mpt_command(struct file *filp, unsigned int cmd,\n\t\t\tunsigned long arg)\n{\n\tstruct mpt_ioctl_command32 karg32;\n\tstruct mpt_ioctl_command32 __user *uarg = (struct mpt_ioctl_command32 __user *) arg;\n\tstruct mpt_ioctl_command karg;\n\tMPT_ADAPTER *iocp = NULL;\n\tint iocnum, iocnumX;\n\tint nonblock = (filp->f_flags & O_NONBLOCK);\n\tint ret;\n\n\tif (copy_from_user(&karg32, (char __user *)arg, sizeof(karg32)))\n\t\treturn -EFAULT;\n\n\t/* Verify intended MPT adapter */\n\tiocnumX = karg32.hdr.iocnum & 0xFF;\n\tif (((iocnum = mpt_verify_adapter(iocnumX, &iocp)) < 0) ||\n\t    (iocp == NULL)) {\n\t\tprintk(KERN_DEBUG MYNAM \"::compat_mpt_command @%d - ioc%d not found!\\n\",\n\t\t\t__LINE__, iocnumX);\n\t\treturn -ENODEV;\n\t}\n\n\tif ((ret = mptctl_syscall_down(iocp, nonblock)) != 0)\n\t\treturn ret;\n\n\tdctlprintk(iocp, printk(MYIOC_s_DEBUG_FMT \"compat_mpt_command() called\\n\",\n\t    iocp->name));\n\t/* Copy data to karg */\n\tkarg.hdr.iocnum = karg32.hdr.iocnum;\n\tkarg.hdr.port = karg32.hdr.port;\n\tkarg.timeout = karg32.timeout;\n\tkarg.maxReplyBytes = karg32.maxReplyBytes;\n\n\tkarg.dataInSize = karg32.dataInSize;\n\tkarg.dataOutSize = karg32.dataOutSize;\n\tkarg.maxSenseBytes = karg32.maxSenseBytes;\n\tkarg.dataSgeOffset = karg32.dataSgeOffset;\n\n\tkarg.replyFrameBufPtr = (char __user *)(unsigned long)karg32.replyFrameBufPtr;\n\tkarg.dataInBufPtr = (char __user *)(unsigned long)karg32.dataInBufPtr;\n\tkarg.dataOutBufPtr = (char __user *)(unsigned long)karg32.dataOutBufPtr;\n\tkarg.senseDataPtr = (char __user *)(unsigned long)karg32.senseDataPtr;\n\n\t/* Pass new structure to do_mpt_command\n\t */\n\tret = mptctl_do_mpt_command (iocp, karg, &uarg->MF);\n\n\tmutex_unlock(&iocp->ioctl_cmds.mutex);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -45,7 +45,7 @@\n \n \t/* Pass new structure to do_mpt_command\n \t */\n-\tret = mptctl_do_mpt_command (karg, &uarg->MF);\n+\tret = mptctl_do_mpt_command (iocp, karg, &uarg->MF);\n \n \tmutex_unlock(&iocp->ioctl_cmds.mutex);\n ",
        "function_modified_lines": {
            "added": [
                "\tret = mptctl_do_mpt_command (iocp, karg, &uarg->MF);"
            ],
            "deleted": [
                "\tret = mptctl_do_mpt_command (karg, &uarg->MF);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The __mptctl_ioctl function in drivers/message/fusion/mptctl.c in the Linux kernel before 5.4.14 allows local users to hold an incorrect lock during the ioctl operation and trigger a race condition, i.e., a \"double fetch\" vulnerability, aka CID-28d76df18f0a. NOTE: the vendor states \"The security impact of this bug is not as bad as it could have been because these operations are all privileged and root already has enormous destructive power.\""
    },
    {
        "cve_id": "CVE-2020-12652",
        "code_before_change": "static long\n__mptctl_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tmpt_ioctl_header __user *uhdr = (void __user *) arg;\n\tmpt_ioctl_header\t khdr;\n\tint iocnum;\n\tunsigned iocnumX;\n\tint nonblock = (file->f_flags & O_NONBLOCK);\n\tint ret;\n\tMPT_ADAPTER *iocp = NULL;\n\n\tif (copy_from_user(&khdr, uhdr, sizeof(khdr))) {\n\t\tprintk(KERN_ERR MYNAM \"%s::mptctl_ioctl() @%d - \"\n\t\t\t\t\"Unable to copy mpt_ioctl_header data @ %p\\n\",\n\t\t\t\t__FILE__, __LINE__, uhdr);\n\t\treturn -EFAULT;\n\t}\n\tret = -ENXIO;\t\t\t\t/* (-6) No such device or address */\n\n\t/* Verify intended MPT adapter - set iocnum and the adapter\n\t * pointer (iocp)\n\t */\n\tiocnumX = khdr.iocnum & 0xFF;\n\tif (((iocnum = mpt_verify_adapter(iocnumX, &iocp)) < 0) ||\n\t    (iocp == NULL))\n\t\treturn -ENODEV;\n\n\tif (!iocp->active) {\n\t\tprintk(KERN_DEBUG MYNAM \"%s::mptctl_ioctl() @%d - Controller disabled.\\n\",\n\t\t\t\t__FILE__, __LINE__);\n\t\treturn -EFAULT;\n\t}\n\n\t/* Handle those commands that are just returning\n\t * information stored in the driver.\n\t * These commands should never time out and are unaffected\n\t * by TM and FW reloads.\n\t */\n\tif ((cmd & ~IOCSIZE_MASK) == (MPTIOCINFO & ~IOCSIZE_MASK)) {\n\t\treturn mptctl_getiocinfo(arg, _IOC_SIZE(cmd));\n\t} else if (cmd == MPTTARGETINFO) {\n\t\treturn mptctl_gettargetinfo(arg);\n\t} else if (cmd == MPTTEST) {\n\t\treturn mptctl_readtest(arg);\n\t} else if (cmd == MPTEVENTQUERY) {\n\t\treturn mptctl_eventquery(arg);\n\t} else if (cmd == MPTEVENTENABLE) {\n\t\treturn mptctl_eventenable(arg);\n\t} else if (cmd == MPTEVENTREPORT) {\n\t\treturn mptctl_eventreport(arg);\n\t} else if (cmd == MPTFWREPLACE) {\n\t\treturn mptctl_replace_fw(arg);\n\t}\n\n\t/* All of these commands require an interrupt or\n\t * are unknown/illegal.\n\t */\n\tif ((ret = mptctl_syscall_down(iocp, nonblock)) != 0)\n\t\treturn ret;\n\n\tif (cmd == MPTFWDOWNLOAD)\n\t\tret = mptctl_fw_download(arg);\n\telse if (cmd == MPTCOMMAND)\n\t\tret = mptctl_mpt_command(arg);\n\telse if (cmd == MPTHARDRESET)\n\t\tret = mptctl_do_reset(arg);\n\telse if ((cmd & ~IOCSIZE_MASK) == (HP_GETHOSTINFO & ~IOCSIZE_MASK))\n\t\tret = mptctl_hp_hostinfo(arg, _IOC_SIZE(cmd));\n\telse if (cmd == HP_GETTARGETINFO)\n\t\tret = mptctl_hp_targetinfo(arg);\n\telse\n\t\tret = -EINVAL;\n\n\tmutex_unlock(&iocp->ioctl_cmds.mutex);\n\n\treturn ret;\n}",
        "code_after_change": "static long\n__mptctl_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tmpt_ioctl_header __user *uhdr = (void __user *) arg;\n\tmpt_ioctl_header\t khdr;\n\tint iocnum;\n\tunsigned iocnumX;\n\tint nonblock = (file->f_flags & O_NONBLOCK);\n\tint ret;\n\tMPT_ADAPTER *iocp = NULL;\n\n\tif (copy_from_user(&khdr, uhdr, sizeof(khdr))) {\n\t\tprintk(KERN_ERR MYNAM \"%s::mptctl_ioctl() @%d - \"\n\t\t\t\t\"Unable to copy mpt_ioctl_header data @ %p\\n\",\n\t\t\t\t__FILE__, __LINE__, uhdr);\n\t\treturn -EFAULT;\n\t}\n\tret = -ENXIO;\t\t\t\t/* (-6) No such device or address */\n\n\t/* Verify intended MPT adapter - set iocnum and the adapter\n\t * pointer (iocp)\n\t */\n\tiocnumX = khdr.iocnum & 0xFF;\n\tif (((iocnum = mpt_verify_adapter(iocnumX, &iocp)) < 0) ||\n\t    (iocp == NULL))\n\t\treturn -ENODEV;\n\n\tif (!iocp->active) {\n\t\tprintk(KERN_DEBUG MYNAM \"%s::mptctl_ioctl() @%d - Controller disabled.\\n\",\n\t\t\t\t__FILE__, __LINE__);\n\t\treturn -EFAULT;\n\t}\n\n\t/* Handle those commands that are just returning\n\t * information stored in the driver.\n\t * These commands should never time out and are unaffected\n\t * by TM and FW reloads.\n\t */\n\tif ((cmd & ~IOCSIZE_MASK) == (MPTIOCINFO & ~IOCSIZE_MASK)) {\n\t\treturn mptctl_getiocinfo(iocp, arg, _IOC_SIZE(cmd));\n\t} else if (cmd == MPTTARGETINFO) {\n\t\treturn mptctl_gettargetinfo(iocp, arg);\n\t} else if (cmd == MPTTEST) {\n\t\treturn mptctl_readtest(iocp, arg);\n\t} else if (cmd == MPTEVENTQUERY) {\n\t\treturn mptctl_eventquery(iocp, arg);\n\t} else if (cmd == MPTEVENTENABLE) {\n\t\treturn mptctl_eventenable(iocp, arg);\n\t} else if (cmd == MPTEVENTREPORT) {\n\t\treturn mptctl_eventreport(iocp, arg);\n\t} else if (cmd == MPTFWREPLACE) {\n\t\treturn mptctl_replace_fw(iocp, arg);\n\t}\n\n\t/* All of these commands require an interrupt or\n\t * are unknown/illegal.\n\t */\n\tif ((ret = mptctl_syscall_down(iocp, nonblock)) != 0)\n\t\treturn ret;\n\n\tif (cmd == MPTFWDOWNLOAD)\n\t\tret = mptctl_fw_download(iocp, arg);\n\telse if (cmd == MPTCOMMAND)\n\t\tret = mptctl_mpt_command(iocp, arg);\n\telse if (cmd == MPTHARDRESET)\n\t\tret = mptctl_do_reset(iocp, arg);\n\telse if ((cmd & ~IOCSIZE_MASK) == (HP_GETHOSTINFO & ~IOCSIZE_MASK))\n\t\tret = mptctl_hp_hostinfo(iocp, arg, _IOC_SIZE(cmd));\n\telse if (cmd == HP_GETTARGETINFO)\n\t\tret = mptctl_hp_targetinfo(iocp, arg);\n\telse\n\t\tret = -EINVAL;\n\n\tmutex_unlock(&iocp->ioctl_cmds.mutex);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -37,19 +37,19 @@\n \t * by TM and FW reloads.\n \t */\n \tif ((cmd & ~IOCSIZE_MASK) == (MPTIOCINFO & ~IOCSIZE_MASK)) {\n-\t\treturn mptctl_getiocinfo(arg, _IOC_SIZE(cmd));\n+\t\treturn mptctl_getiocinfo(iocp, arg, _IOC_SIZE(cmd));\n \t} else if (cmd == MPTTARGETINFO) {\n-\t\treturn mptctl_gettargetinfo(arg);\n+\t\treturn mptctl_gettargetinfo(iocp, arg);\n \t} else if (cmd == MPTTEST) {\n-\t\treturn mptctl_readtest(arg);\n+\t\treturn mptctl_readtest(iocp, arg);\n \t} else if (cmd == MPTEVENTQUERY) {\n-\t\treturn mptctl_eventquery(arg);\n+\t\treturn mptctl_eventquery(iocp, arg);\n \t} else if (cmd == MPTEVENTENABLE) {\n-\t\treturn mptctl_eventenable(arg);\n+\t\treturn mptctl_eventenable(iocp, arg);\n \t} else if (cmd == MPTEVENTREPORT) {\n-\t\treturn mptctl_eventreport(arg);\n+\t\treturn mptctl_eventreport(iocp, arg);\n \t} else if (cmd == MPTFWREPLACE) {\n-\t\treturn mptctl_replace_fw(arg);\n+\t\treturn mptctl_replace_fw(iocp, arg);\n \t}\n \n \t/* All of these commands require an interrupt or\n@@ -59,15 +59,15 @@\n \t\treturn ret;\n \n \tif (cmd == MPTFWDOWNLOAD)\n-\t\tret = mptctl_fw_download(arg);\n+\t\tret = mptctl_fw_download(iocp, arg);\n \telse if (cmd == MPTCOMMAND)\n-\t\tret = mptctl_mpt_command(arg);\n+\t\tret = mptctl_mpt_command(iocp, arg);\n \telse if (cmd == MPTHARDRESET)\n-\t\tret = mptctl_do_reset(arg);\n+\t\tret = mptctl_do_reset(iocp, arg);\n \telse if ((cmd & ~IOCSIZE_MASK) == (HP_GETHOSTINFO & ~IOCSIZE_MASK))\n-\t\tret = mptctl_hp_hostinfo(arg, _IOC_SIZE(cmd));\n+\t\tret = mptctl_hp_hostinfo(iocp, arg, _IOC_SIZE(cmd));\n \telse if (cmd == HP_GETTARGETINFO)\n-\t\tret = mptctl_hp_targetinfo(arg);\n+\t\tret = mptctl_hp_targetinfo(iocp, arg);\n \telse\n \t\tret = -EINVAL;\n ",
        "function_modified_lines": {
            "added": [
                "\t\treturn mptctl_getiocinfo(iocp, arg, _IOC_SIZE(cmd));",
                "\t\treturn mptctl_gettargetinfo(iocp, arg);",
                "\t\treturn mptctl_readtest(iocp, arg);",
                "\t\treturn mptctl_eventquery(iocp, arg);",
                "\t\treturn mptctl_eventenable(iocp, arg);",
                "\t\treturn mptctl_eventreport(iocp, arg);",
                "\t\treturn mptctl_replace_fw(iocp, arg);",
                "\t\tret = mptctl_fw_download(iocp, arg);",
                "\t\tret = mptctl_mpt_command(iocp, arg);",
                "\t\tret = mptctl_do_reset(iocp, arg);",
                "\t\tret = mptctl_hp_hostinfo(iocp, arg, _IOC_SIZE(cmd));",
                "\t\tret = mptctl_hp_targetinfo(iocp, arg);"
            ],
            "deleted": [
                "\t\treturn mptctl_getiocinfo(arg, _IOC_SIZE(cmd));",
                "\t\treturn mptctl_gettargetinfo(arg);",
                "\t\treturn mptctl_readtest(arg);",
                "\t\treturn mptctl_eventquery(arg);",
                "\t\treturn mptctl_eventenable(arg);",
                "\t\treturn mptctl_eventreport(arg);",
                "\t\treturn mptctl_replace_fw(arg);",
                "\t\tret = mptctl_fw_download(arg);",
                "\t\tret = mptctl_mpt_command(arg);",
                "\t\tret = mptctl_do_reset(arg);",
                "\t\tret = mptctl_hp_hostinfo(arg, _IOC_SIZE(cmd));",
                "\t\tret = mptctl_hp_targetinfo(arg);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The __mptctl_ioctl function in drivers/message/fusion/mptctl.c in the Linux kernel before 5.4.14 allows local users to hold an incorrect lock during the ioctl operation and trigger a race condition, i.e., a \"double fetch\" vulnerability, aka CID-28d76df18f0a. NOTE: the vendor states \"The security impact of this bug is not as bad as it could have been because these operations are all privileged and root already has enormous destructive power.\""
    },
    {
        "cve_id": "CVE-2020-12652",
        "code_before_change": "static int\ncompat_mptfwxfer_ioctl(struct file *filp, unsigned int cmd,\n\t\t\tunsigned long arg)\n{\n\tstruct mpt_fw_xfer32 kfw32;\n\tstruct mpt_fw_xfer kfw;\n\tMPT_ADAPTER *iocp = NULL;\n\tint iocnum, iocnumX;\n\tint nonblock = (filp->f_flags & O_NONBLOCK);\n\tint ret;\n\n\n\tif (copy_from_user(&kfw32, (char __user *)arg, sizeof(kfw32)))\n\t\treturn -EFAULT;\n\n\t/* Verify intended MPT adapter */\n\tiocnumX = kfw32.iocnum & 0xFF;\n\tif (((iocnum = mpt_verify_adapter(iocnumX, &iocp)) < 0) ||\n\t    (iocp == NULL)) {\n\t\tprintk(KERN_DEBUG MYNAM \"::compat_mptfwxfer_ioctl @%d - ioc%d not found!\\n\",\n\t\t\t__LINE__, iocnumX);\n\t\treturn -ENODEV;\n\t}\n\n\tif ((ret = mptctl_syscall_down(iocp, nonblock)) != 0)\n\t\treturn ret;\n\n\tdctlprintk(iocp, printk(MYIOC_s_DEBUG_FMT \"compat_mptfwxfer_ioctl() called\\n\",\n\t    iocp->name));\n\tkfw.iocnum = iocnum;\n\tkfw.fwlen = kfw32.fwlen;\n\tkfw.bufp = compat_ptr(kfw32.bufp);\n\n\tret = mptctl_do_fw_download(kfw.iocnum, kfw.bufp, kfw.fwlen);\n\n\tmutex_unlock(&iocp->ioctl_cmds.mutex);\n\n\treturn ret;\n}",
        "code_after_change": "static int\ncompat_mptfwxfer_ioctl(struct file *filp, unsigned int cmd,\n\t\t\tunsigned long arg)\n{\n\tstruct mpt_fw_xfer32 kfw32;\n\tstruct mpt_fw_xfer kfw;\n\tMPT_ADAPTER *iocp = NULL;\n\tint iocnum, iocnumX;\n\tint nonblock = (filp->f_flags & O_NONBLOCK);\n\tint ret;\n\n\n\tif (copy_from_user(&kfw32, (char __user *)arg, sizeof(kfw32)))\n\t\treturn -EFAULT;\n\n\t/* Verify intended MPT adapter */\n\tiocnumX = kfw32.iocnum & 0xFF;\n\tif (((iocnum = mpt_verify_adapter(iocnumX, &iocp)) < 0) ||\n\t    (iocp == NULL)) {\n\t\tprintk(KERN_DEBUG MYNAM \"::compat_mptfwxfer_ioctl @%d - ioc%d not found!\\n\",\n\t\t\t__LINE__, iocnumX);\n\t\treturn -ENODEV;\n\t}\n\n\tif ((ret = mptctl_syscall_down(iocp, nonblock)) != 0)\n\t\treturn ret;\n\n\tdctlprintk(iocp, printk(MYIOC_s_DEBUG_FMT \"compat_mptfwxfer_ioctl() called\\n\",\n\t    iocp->name));\n\tkfw.iocnum = iocnum;\n\tkfw.fwlen = kfw32.fwlen;\n\tkfw.bufp = compat_ptr(kfw32.bufp);\n\n\tret = mptctl_do_fw_download(iocp, kfw.bufp, kfw.fwlen);\n\n\tmutex_unlock(&iocp->ioctl_cmds.mutex);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,7 +31,7 @@\n \tkfw.fwlen = kfw32.fwlen;\n \tkfw.bufp = compat_ptr(kfw32.bufp);\n \n-\tret = mptctl_do_fw_download(kfw.iocnum, kfw.bufp, kfw.fwlen);\n+\tret = mptctl_do_fw_download(iocp, kfw.bufp, kfw.fwlen);\n \n \tmutex_unlock(&iocp->ioctl_cmds.mutex);\n ",
        "function_modified_lines": {
            "added": [
                "\tret = mptctl_do_fw_download(iocp, kfw.bufp, kfw.fwlen);"
            ],
            "deleted": [
                "\tret = mptctl_do_fw_download(kfw.iocnum, kfw.bufp, kfw.fwlen);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The __mptctl_ioctl function in drivers/message/fusion/mptctl.c in the Linux kernel before 5.4.14 allows local users to hold an incorrect lock during the ioctl operation and trigger a race condition, i.e., a \"double fetch\" vulnerability, aka CID-28d76df18f0a. NOTE: the vendor states \"The security impact of this bug is not as bad as it could have been because these operations are all privileged and root already has enormous destructive power.\""
    },
    {
        "cve_id": "CVE-2020-14416",
        "code_before_change": "static void slcan_close(struct tty_struct *tty)\n{\n\tstruct slcan *sl = (struct slcan *) tty->disc_data;\n\n\t/* First make sure we're connected. */\n\tif (!sl || sl->magic != SLCAN_MAGIC || sl->tty != tty)\n\t\treturn;\n\n\tspin_lock_bh(&sl->lock);\n\ttty->disc_data = NULL;\n\tsl->tty = NULL;\n\tspin_unlock_bh(&sl->lock);\n\n\tflush_work(&sl->tx_work);\n\n\t/* Flush network side */\n\tunregister_netdev(sl->dev);\n\t/* This will complete via sl_free_netdev */\n}",
        "code_after_change": "static void slcan_close(struct tty_struct *tty)\n{\n\tstruct slcan *sl = (struct slcan *) tty->disc_data;\n\n\t/* First make sure we're connected. */\n\tif (!sl || sl->magic != SLCAN_MAGIC || sl->tty != tty)\n\t\treturn;\n\n\tspin_lock_bh(&sl->lock);\n\trcu_assign_pointer(tty->disc_data, NULL);\n\tsl->tty = NULL;\n\tspin_unlock_bh(&sl->lock);\n\n\tsynchronize_rcu();\n\tflush_work(&sl->tx_work);\n\n\t/* Flush network side */\n\tunregister_netdev(sl->dev);\n\t/* This will complete via sl_free_netdev */\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,10 +7,11 @@\n \t\treturn;\n \n \tspin_lock_bh(&sl->lock);\n-\ttty->disc_data = NULL;\n+\trcu_assign_pointer(tty->disc_data, NULL);\n \tsl->tty = NULL;\n \tspin_unlock_bh(&sl->lock);\n \n+\tsynchronize_rcu();\n \tflush_work(&sl->tx_work);\n \n \t/* Flush network side */",
        "function_modified_lines": {
            "added": [
                "\trcu_assign_pointer(tty->disc_data, NULL);",
                "\tsynchronize_rcu();"
            ],
            "deleted": [
                "\ttty->disc_data = NULL;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.4.16, a race condition in tty->disc_data handling in the slip and slcan line discipline could lead to a use-after-free, aka CID-0ace17d56824. This affects drivers/net/slip/slip.c and drivers/net/can/slcan.c."
    },
    {
        "cve_id": "CVE-2020-14416",
        "code_before_change": "static void slcan_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slcan *sl = tty->disc_data;\n\n\tschedule_work(&sl->tx_work);\n}",
        "code_after_change": "static void slcan_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slcan *sl;\n\n\trcu_read_lock();\n\tsl = rcu_dereference(tty->disc_data);\n\tif (!sl)\n\t\tgoto out;\n\n\tschedule_work(&sl->tx_work);\nout:\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,13 @@\n static void slcan_write_wakeup(struct tty_struct *tty)\n {\n-\tstruct slcan *sl = tty->disc_data;\n+\tstruct slcan *sl;\n+\n+\trcu_read_lock();\n+\tsl = rcu_dereference(tty->disc_data);\n+\tif (!sl)\n+\t\tgoto out;\n \n \tschedule_work(&sl->tx_work);\n+out:\n+\trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct slcan *sl;",
                "",
                "\trcu_read_lock();",
                "\tsl = rcu_dereference(tty->disc_data);",
                "\tif (!sl)",
                "\t\tgoto out;",
                "out:",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tstruct slcan *sl = tty->disc_data;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.4.16, a race condition in tty->disc_data handling in the slip and slcan line discipline could lead to a use-after-free, aka CID-0ace17d56824. This affects drivers/net/slip/slip.c and drivers/net/can/slcan.c."
    },
    {
        "cve_id": "CVE-2020-25285",
        "code_before_change": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}",
        "code_after_change": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,9 +13,8 @@\n \tif (write && hstate_is_gigantic(h))\n \t\treturn -EINVAL;\n \n-\ttable->data = &tmp;\n-\ttable->maxlen = sizeof(unsigned long);\n-\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n+\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n+\t\t\t\t\t     &tmp);\n \tif (ret)\n \t\tgoto out;\n ",
        "function_modified_lines": {
            "added": [
                "\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,",
                "\t\t\t\t\t     &tmp);"
            ],
            "deleted": [
                "\ttable->data = &tmp;",
                "\ttable->maxlen = sizeof(unsigned long);",
                "\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-787",
            "CWE-476"
        ],
        "cve_description": "A race condition between hugetlb sysctl handlers in mm/hugetlb.c in the Linux kernel before 5.8.8 could be used by local attackers to corrupt memory, cause a NULL pointer dereference, or possibly have unspecified other impact, aka CID-17743798d812."
    },
    {
        "cve_id": "CVE-2020-25285",
        "code_before_change": "static int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}",
        "code_after_change": "static int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,9 +9,8 @@\n \tif (!hugepages_supported())\n \t\treturn -EOPNOTSUPP;\n \n-\ttable->data = &tmp;\n-\ttable->maxlen = sizeof(unsigned long);\n-\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n+\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n+\t\t\t\t\t     &tmp);\n \tif (ret)\n \t\tgoto out;\n ",
        "function_modified_lines": {
            "added": [
                "\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,",
                "\t\t\t\t\t     &tmp);"
            ],
            "deleted": [
                "\ttable->data = &tmp;",
                "\ttable->maxlen = sizeof(unsigned long);",
                "\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-787",
            "CWE-476"
        ],
        "cve_description": "A race condition between hugetlb sysctl handlers in mm/hugetlb.c in the Linux kernel before 5.8.8 could be used by local attackers to corrupt memory, cause a NULL pointer dereference, or possibly have unspecified other impact, aka CID-17743798d812."
    },
    {
        "cve_id": "CVE-2020-27067",
        "code_before_change": "static void l2tp_eth_dev_uninit(struct net_device *dev)\n{\n\tstruct l2tp_eth *priv = netdev_priv(dev);\n\tstruct l2tp_eth_net *pn = l2tp_eth_pernet(dev_net(dev));\n\n\tspin_lock(&pn->l2tp_eth_lock);\n\tlist_del_init(&priv->list);\n\tspin_unlock(&pn->l2tp_eth_lock);\n\tdev_put(dev);\n}",
        "code_after_change": "static void l2tp_eth_dev_uninit(struct net_device *dev)\n{\n\tdev_put(dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,4 @@\n static void l2tp_eth_dev_uninit(struct net_device *dev)\n {\n-\tstruct l2tp_eth *priv = netdev_priv(dev);\n-\tstruct l2tp_eth_net *pn = l2tp_eth_pernet(dev_net(dev));\n-\n-\tspin_lock(&pn->l2tp_eth_lock);\n-\tlist_del_init(&priv->list);\n-\tspin_unlock(&pn->l2tp_eth_lock);\n \tdev_put(dev);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct l2tp_eth *priv = netdev_priv(dev);",
                "\tstruct l2tp_eth_net *pn = l2tp_eth_pernet(dev_net(dev));",
                "",
                "\tspin_lock(&pn->l2tp_eth_lock);",
                "\tlist_del_init(&priv->list);",
                "\tspin_unlock(&pn->l2tp_eth_lock);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the l2tp subsystem, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152409173"
    },
    {
        "cve_id": "CVE-2020-27067",
        "code_before_change": "static int l2tp_eth_create(struct net *net, struct l2tp_tunnel *tunnel,\n\t\t\t   u32 session_id, u32 peer_session_id,\n\t\t\t   struct l2tp_session_cfg *cfg)\n{\n\tunsigned char name_assign_type;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct l2tp_session *session;\n\tstruct l2tp_eth *priv;\n\tstruct l2tp_eth_sess *spriv;\n\tint rc;\n\tstruct l2tp_eth_net *pn;\n\n\tif (cfg->ifname) {\n\t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n\t\tname_assign_type = NET_NAME_USER;\n\t} else {\n\t\tstrcpy(name, L2TP_ETH_DEV_NAME);\n\t\tname_assign_type = NET_NAME_ENUM;\n\t}\n\n\tsession = l2tp_session_create(sizeof(*spriv), tunnel, session_id,\n\t\t\t\t      peer_session_id, cfg);\n\tif (IS_ERR(session)) {\n\t\trc = PTR_ERR(session);\n\t\tgoto out;\n\t}\n\n\tdev = alloc_netdev(sizeof(*priv), name, name_assign_type,\n\t\t\t   l2tp_eth_dev_setup);\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto out_del_session;\n\t}\n\n\tdev_net_set(dev, net);\n\tdev->min_mtu = 0;\n\tdev->max_mtu = ETH_MAX_MTU;\n\tl2tp_eth_adjust_mtu(tunnel, session, dev);\n\n\tpriv = netdev_priv(dev);\n\tpriv->dev = dev;\n\tpriv->session = session;\n\tINIT_LIST_HEAD(&priv->list);\n\n\tpriv->tunnel_sock = tunnel->sock;\n\tsession->recv_skb = l2tp_eth_dev_recv;\n\tsession->session_close = l2tp_eth_delete;\n#if IS_ENABLED(CONFIG_L2TP_DEBUGFS)\n\tsession->show = l2tp_eth_show;\n#endif\n\n\tspriv = l2tp_session_priv(session);\n\tspriv->dev = dev;\n\n\trc = register_netdev(dev);\n\tif (rc < 0)\n\t\tgoto out_del_dev;\n\n\t__module_get(THIS_MODULE);\n\t/* Must be done after register_netdev() */\n\tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n\n\tdev_hold(dev);\n\tpn = l2tp_eth_pernet(dev_net(dev));\n\tspin_lock(&pn->l2tp_eth_lock);\n\tlist_add(&priv->list, &pn->l2tp_eth_dev_list);\n\tspin_unlock(&pn->l2tp_eth_lock);\n\n\treturn 0;\n\nout_del_dev:\n\tfree_netdev(dev);\n\tspriv->dev = NULL;\nout_del_session:\n\tl2tp_session_delete(session);\nout:\n\treturn rc;\n}",
        "code_after_change": "static int l2tp_eth_create(struct net *net, struct l2tp_tunnel *tunnel,\n\t\t\t   u32 session_id, u32 peer_session_id,\n\t\t\t   struct l2tp_session_cfg *cfg)\n{\n\tunsigned char name_assign_type;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct l2tp_session *session;\n\tstruct l2tp_eth *priv;\n\tstruct l2tp_eth_sess *spriv;\n\tint rc;\n\n\tif (cfg->ifname) {\n\t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n\t\tname_assign_type = NET_NAME_USER;\n\t} else {\n\t\tstrcpy(name, L2TP_ETH_DEV_NAME);\n\t\tname_assign_type = NET_NAME_ENUM;\n\t}\n\n\tsession = l2tp_session_create(sizeof(*spriv), tunnel, session_id,\n\t\t\t\t      peer_session_id, cfg);\n\tif (IS_ERR(session)) {\n\t\trc = PTR_ERR(session);\n\t\tgoto out;\n\t}\n\n\tdev = alloc_netdev(sizeof(*priv), name, name_assign_type,\n\t\t\t   l2tp_eth_dev_setup);\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto out_del_session;\n\t}\n\n\tdev_net_set(dev, net);\n\tdev->min_mtu = 0;\n\tdev->max_mtu = ETH_MAX_MTU;\n\tl2tp_eth_adjust_mtu(tunnel, session, dev);\n\n\tpriv = netdev_priv(dev);\n\tpriv->dev = dev;\n\tpriv->session = session;\n\n\tpriv->tunnel_sock = tunnel->sock;\n\tsession->recv_skb = l2tp_eth_dev_recv;\n\tsession->session_close = l2tp_eth_delete;\n#if IS_ENABLED(CONFIG_L2TP_DEBUGFS)\n\tsession->show = l2tp_eth_show;\n#endif\n\n\tspriv = l2tp_session_priv(session);\n\tspriv->dev = dev;\n\n\trc = register_netdev(dev);\n\tif (rc < 0)\n\t\tgoto out_del_dev;\n\n\t__module_get(THIS_MODULE);\n\t/* Must be done after register_netdev() */\n\tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n\n\tdev_hold(dev);\n\n\treturn 0;\n\nout_del_dev:\n\tfree_netdev(dev);\n\tspriv->dev = NULL;\nout_del_session:\n\tl2tp_session_delete(session);\nout:\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,6 @@\n \tstruct l2tp_eth *priv;\n \tstruct l2tp_eth_sess *spriv;\n \tint rc;\n-\tstruct l2tp_eth_net *pn;\n \n \tif (cfg->ifname) {\n \t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n@@ -41,7 +40,6 @@\n \tpriv = netdev_priv(dev);\n \tpriv->dev = dev;\n \tpriv->session = session;\n-\tINIT_LIST_HEAD(&priv->list);\n \n \tpriv->tunnel_sock = tunnel->sock;\n \tsession->recv_skb = l2tp_eth_dev_recv;\n@@ -62,10 +60,6 @@\n \tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n \n \tdev_hold(dev);\n-\tpn = l2tp_eth_pernet(dev_net(dev));\n-\tspin_lock(&pn->l2tp_eth_lock);\n-\tlist_add(&priv->list, &pn->l2tp_eth_dev_list);\n-\tspin_unlock(&pn->l2tp_eth_lock);\n \n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct l2tp_eth_net *pn;",
                "\tINIT_LIST_HEAD(&priv->list);",
                "\tpn = l2tp_eth_pernet(dev_net(dev));",
                "\tspin_lock(&pn->l2tp_eth_lock);",
                "\tlist_add(&priv->list, &pn->l2tp_eth_dev_list);",
                "\tspin_unlock(&pn->l2tp_eth_lock);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the l2tp subsystem, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152409173"
    },
    {
        "cve_id": "CVE-2020-27067",
        "code_before_change": "static int __init l2tp_eth_init(void)\n{\n\tint err = 0;\n\n\terr = l2tp_nl_register_ops(L2TP_PWTYPE_ETH, &l2tp_eth_nl_cmd_ops);\n\tif (err)\n\t\tgoto out;\n\n\terr = register_pernet_device(&l2tp_eth_net_ops);\n\tif (err)\n\t\tgoto out_unreg;\n\n\tpr_info(\"L2TP ethernet pseudowire support (L2TPv3)\\n\");\n\n\treturn 0;\n\nout_unreg:\n\tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);\nout:\n\treturn err;\n}",
        "code_after_change": "static int __init l2tp_eth_init(void)\n{\n\tint err = 0;\n\n\terr = l2tp_nl_register_ops(L2TP_PWTYPE_ETH, &l2tp_eth_nl_cmd_ops);\n\tif (err)\n\t\tgoto err;\n\n\tpr_info(\"L2TP ethernet pseudowire support (L2TPv3)\\n\");\n\n\treturn 0;\n\nerr:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,18 +4,12 @@\n \n \terr = l2tp_nl_register_ops(L2TP_PWTYPE_ETH, &l2tp_eth_nl_cmd_ops);\n \tif (err)\n-\t\tgoto out;\n-\n-\terr = register_pernet_device(&l2tp_eth_net_ops);\n-\tif (err)\n-\t\tgoto out_unreg;\n+\t\tgoto err;\n \n \tpr_info(\"L2TP ethernet pseudowire support (L2TPv3)\\n\");\n \n \treturn 0;\n \n-out_unreg:\n-\tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);\n-out:\n+err:\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tgoto err;",
                "err:"
            ],
            "deleted": [
                "\t\tgoto out;",
                "",
                "\terr = register_pernet_device(&l2tp_eth_net_ops);",
                "\tif (err)",
                "\t\tgoto out_unreg;",
                "out_unreg:",
                "\tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);",
                "out:"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the l2tp subsystem, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152409173"
    },
    {
        "cve_id": "CVE-2020-27067",
        "code_before_change": "static void __exit l2tp_eth_exit(void)\n{\n\tunregister_pernet_device(&l2tp_eth_net_ops);\n\tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);\n}",
        "code_after_change": "static void __exit l2tp_eth_exit(void)\n{\n\tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,4 @@\n static void __exit l2tp_eth_exit(void)\n {\n-\tunregister_pernet_device(&l2tp_eth_net_ops);\n \tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tunregister_pernet_device(&l2tp_eth_net_ops);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the l2tp subsystem, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152409173"
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "int get_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -1;\n\tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n\t\treturn -1;\n\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];\n}",
        "code_after_change": "int get_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -1;\n\tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n\t\treturn -1;\n\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,5 +4,5 @@\n \t\treturn -1;\n \tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n \t\treturn -1;\n-\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];\n+\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);"
            ],
            "deleted": [
                "\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5."
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tevtchn_to_irq[row][col] = -1;\n}",
        "code_after_change": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,5 +3,5 @@\n \tunsigned col;\n \n \tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n-\t\tevtchn_to_irq[row][col] = -1;\n+\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);\n }",
        "function_modified_lines": {
            "added": [
                "\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);"
            ],
            "deleted": [
                "\t\tevtchn_to_irq[row][col] = -1;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5."
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\tlist_del(&info->list);\n\n\tset_info_for_irq(irq, NULL);\n\n\tWARN_ON(info->refcnt > 0);\n\n\tkfree(info);\n\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\n\tirq_free_desc(irq);\n}",
        "code_after_change": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\tunsigned long flags;\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\twrite_lock_irqsave(&evtchn_rwlock, flags);\n\n\tlist_del(&info->list);\n\n\tset_info_for_irq(irq, NULL);\n\n\tWARN_ON(info->refcnt > 0);\n\n\twrite_unlock_irqrestore(&evtchn_rwlock, flags);\n\n\tkfree(info);\n\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\n\tirq_free_desc(irq);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,15 +1,20 @@\n static void xen_free_irq(unsigned irq)\n {\n \tstruct irq_info *info = info_for_irq(irq);\n+\tunsigned long flags;\n \n \tif (WARN_ON(!info))\n \t\treturn;\n+\n+\twrite_lock_irqsave(&evtchn_rwlock, flags);\n \n \tlist_del(&info->list);\n \n \tset_info_for_irq(irq, NULL);\n \n \tWARN_ON(info->refcnt > 0);\n+\n+\twrite_unlock_irqrestore(&evtchn_rwlock, flags);\n \n \tkfree(info);\n ",
        "function_modified_lines": {
            "added": [
                "\tunsigned long flags;",
                "",
                "\twrite_lock_irqsave(&evtchn_rwlock, flags);",
                "",
                "\twrite_unlock_irqrestore(&evtchn_rwlock, flags);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5."
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "static int set_evtchn_to_irq(evtchn_port_t evtchn, unsigned int irq)\n{\n\tunsigned row;\n\tunsigned col;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\trow = EVTCHN_ROW(evtchn);\n\tcol = EVTCHN_COL(evtchn);\n\n\tif (evtchn_to_irq[row] == NULL) {\n\t\t/* Unallocated irq entries return -1 anyway */\n\t\tif (irq == -1)\n\t\t\treturn 0;\n\n\t\tevtchn_to_irq[row] = (int *)get_zeroed_page(GFP_KERNEL);\n\t\tif (evtchn_to_irq[row] == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tclear_evtchn_to_irq_row(row);\n\t}\n\n\tevtchn_to_irq[row][col] = irq;\n\treturn 0;\n}",
        "code_after_change": "static int set_evtchn_to_irq(evtchn_port_t evtchn, unsigned int irq)\n{\n\tunsigned row;\n\tunsigned col;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\trow = EVTCHN_ROW(evtchn);\n\tcol = EVTCHN_COL(evtchn);\n\n\tif (evtchn_to_irq[row] == NULL) {\n\t\t/* Unallocated irq entries return -1 anyway */\n\t\tif (irq == -1)\n\t\t\treturn 0;\n\n\t\tevtchn_to_irq[row] = (int *)get_zeroed_page(GFP_KERNEL);\n\t\tif (evtchn_to_irq[row] == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tclear_evtchn_to_irq_row(row);\n\t}\n\n\tWRITE_ONCE(evtchn_to_irq[row][col], irq);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,6 @@\n \t\tclear_evtchn_to_irq_row(row);\n \t}\n \n-\tevtchn_to_irq[row][col] = irq;\n+\tWRITE_ONCE(evtchn_to_irq[row][col], irq);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tWRITE_ONCE(evtchn_to_irq[row][col], irq);"
            ],
            "deleted": [
                "\tevtchn_to_irq[row][col] = irq;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5."
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "static void __xen_evtchn_do_upcall(void)\n{\n\tstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\n\tint cpu = smp_processor_id();\n\n\tdo {\n\t\tvcpu_info->evtchn_upcall_pending = 0;\n\n\t\txen_evtchn_handle_events(cpu);\n\n\t\tBUG_ON(!irqs_disabled());\n\n\t\tvirt_rmb(); /* Hypervisor can set upcall pending. */\n\n\t} while (vcpu_info->evtchn_upcall_pending);\n}",
        "code_after_change": "static void __xen_evtchn_do_upcall(void)\n{\n\tstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\n\tint cpu = smp_processor_id();\n\n\tread_lock(&evtchn_rwlock);\n\n\tdo {\n\t\tvcpu_info->evtchn_upcall_pending = 0;\n\n\t\txen_evtchn_handle_events(cpu);\n\n\t\tBUG_ON(!irqs_disabled());\n\n\t\tvirt_rmb(); /* Hypervisor can set upcall pending. */\n\n\t} while (vcpu_info->evtchn_upcall_pending);\n\n\tread_unlock(&evtchn_rwlock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,8 @@\n {\n \tstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\n \tint cpu = smp_processor_id();\n+\n+\tread_lock(&evtchn_rwlock);\n \n \tdo {\n \t\tvcpu_info->evtchn_upcall_pending = 0;\n@@ -13,4 +15,6 @@\n \t\tvirt_rmb(); /* Hypervisor can set upcall pending. */\n \n \t} while (vcpu_info->evtchn_upcall_pending);\n+\n+\tread_unlock(&evtchn_rwlock);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tread_lock(&evtchn_rwlock);",
                "",
                "\tread_unlock(&evtchn_rwlock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5."
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "evtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))\n\t\treturn 0;\n\n\treturn info_for_irq(irq)->evtchn;\n}",
        "code_after_change": "evtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tconst struct irq_info *info = NULL;\n\n\tif (likely(irq < nr_irqs))\n\t\tinfo = info_for_irq(irq);\n\tif (!info)\n\t\treturn 0;\n\n\treturn info->evtchn;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,11 @@\n evtchn_port_t evtchn_from_irq(unsigned irq)\n {\n-\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))\n+\tconst struct irq_info *info = NULL;\n+\n+\tif (likely(irq < nr_irqs))\n+\t\tinfo = info_for_irq(irq);\n+\tif (!info)\n \t\treturn 0;\n \n-\treturn info_for_irq(irq)->evtchn;\n+\treturn info->evtchn;\n }",
        "function_modified_lines": {
            "added": [
                "\tconst struct irq_info *info = NULL;",
                "",
                "\tif (likely(irq < nr_irqs))",
                "\t\tinfo = info_for_irq(irq);",
                "\tif (!info)",
                "\treturn info->evtchn;"
            ],
            "deleted": [
                "\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))",
                "\treturn info_for_irq(irq)->evtchn;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5."
    },
    {
        "cve_id": "CVE-2020-27825",
        "code_before_change": "void ring_buffer_reset_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\tatomic_inc(&cpu_buffer->resize_disabled);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\treset_disabled_cpu_buffer(cpu_buffer);\n\n\tatomic_dec(&cpu_buffer->record_disabled);\n\tatomic_dec(&cpu_buffer->resize_disabled);\n}",
        "code_after_change": "void ring_buffer_reset_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tatomic_inc(&cpu_buffer->resize_disabled);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\treset_disabled_cpu_buffer(cpu_buffer);\n\n\tatomic_dec(&cpu_buffer->record_disabled);\n\tatomic_dec(&cpu_buffer->resize_disabled);\n\n\tmutex_unlock(&buffer->mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,9 @@\n \n \tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n \t\treturn;\n+\n+\t/* prevent another thread from changing buffer sizes */\n+\tmutex_lock(&buffer->mutex);\n \n \tatomic_inc(&cpu_buffer->resize_disabled);\n \tatomic_inc(&cpu_buffer->record_disabled);\n@@ -15,4 +18,6 @@\n \n \tatomic_dec(&cpu_buffer->record_disabled);\n \tatomic_dec(&cpu_buffer->resize_disabled);\n+\n+\tmutex_unlock(&buffer->mutex);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* prevent another thread from changing buffer sizes */",
                "\tmutex_lock(&buffer->mutex);",
                "",
                "\tmutex_unlock(&buffer->mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free flaw was found in kernel/trace/ring_buffer.c in Linux kernel (before 5.10-rc1). There was a race problem in trace_open and resize of cpu buffer running parallely on different cpus, may cause a denial of service problem (DOS). This flaw could even allow a local attacker with special user privilege to a kernel information leak threat."
    },
    {
        "cve_id": "CVE-2020-27825",
        "code_before_change": "void ring_buffer_reset_online_cpus(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\tatomic_inc(&cpu_buffer->resize_disabled);\n\t\tatomic_inc(&cpu_buffer->record_disabled);\n\t}\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\treset_disabled_cpu_buffer(cpu_buffer);\n\n\t\tatomic_dec(&cpu_buffer->record_disabled);\n\t\tatomic_dec(&cpu_buffer->resize_disabled);\n\t}\n}",
        "code_after_change": "void ring_buffer_reset_online_cpus(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\tatomic_inc(&cpu_buffer->resize_disabled);\n\t\tatomic_inc(&cpu_buffer->record_disabled);\n\t}\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\treset_disabled_cpu_buffer(cpu_buffer);\n\n\t\tatomic_dec(&cpu_buffer->record_disabled);\n\t\tatomic_dec(&cpu_buffer->resize_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,9 @@\n {\n \tstruct ring_buffer_per_cpu *cpu_buffer;\n \tint cpu;\n+\n+\t/* prevent another thread from changing buffer sizes */\n+\tmutex_lock(&buffer->mutex);\n \n \tfor_each_online_buffer_cpu(buffer, cpu) {\n \t\tcpu_buffer = buffer->buffers[cpu];\n@@ -21,4 +24,6 @@\n \t\tatomic_dec(&cpu_buffer->record_disabled);\n \t\tatomic_dec(&cpu_buffer->resize_disabled);\n \t}\n+\n+\tmutex_unlock(&buffer->mutex);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* prevent another thread from changing buffer sizes */",
                "\tmutex_lock(&buffer->mutex);",
                "",
                "\tmutex_unlock(&buffer->mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free flaw was found in kernel/trace/ring_buffer.c in Linux kernel (before 5.10-rc1). There was a race problem in trace_open and resize of cpu buffer running parallely on different cpus, may cause a denial of service problem (DOS). This flaw could even allow a local attacker with special user privilege to a kernel information leak threat."
    },
    {
        "cve_id": "CVE-2020-29368",
        "code_before_change": "void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long address, bool freeze, struct page *page)\n{\n\tspinlock_t *ptl;\n\tstruct mmu_notifier_range range;\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,\n\t\t\t\taddress & HPAGE_PMD_MASK,\n\t\t\t\t(address & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\tptl = pmd_lock(vma->vm_mm, pmd);\n\n\t/*\n\t * If caller asks to setup a migration entries, we need a page to check\n\t * pmd against. Otherwise we can end up replacing wrong page.\n\t */\n\tVM_BUG_ON(freeze && !page);\n\tif (page && page != pmd_page(*pmd))\n\t        goto out;\n\n\tif (pmd_trans_huge(*pmd)) {\n\t\tpage = pmd_page(*pmd);\n\t\tif (PageMlocked(page))\n\t\t\tclear_page_mlock(page);\n\t} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))\n\t\tgoto out;\n\t__split_huge_pmd_locked(vma, pmd, range.start, freeze);\nout:\n\tspin_unlock(ptl);\n\t/*\n\t * No need to double call mmu_notifier->invalidate_range() callback.\n\t * They are 3 cases to consider inside __split_huge_pmd_locked():\n\t *  1) pmdp_huge_clear_flush_notify() call invalidate_range() obvious\n\t *  2) __split_huge_zero_page_pmd() read only zero page and any write\n\t *    fault will trigger a flush_notify before pointing to a new page\n\t *    (it is fine if the secondary mmu keeps pointing to the old zero\n\t *    page in the meantime)\n\t *  3) Split a huge pmd into pte pointing to the same page. No need\n\t *     to invalidate secondary tlb entry they are all still valid.\n\t *     any further changes to individual pte will notify. So no need\n\t *     to call mmu_notifier->invalidate_range()\n\t */\n\tmmu_notifier_invalidate_range_only_end(&range);\n}",
        "code_after_change": "void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long address, bool freeze, struct page *page)\n{\n\tspinlock_t *ptl;\n\tstruct mmu_notifier_range range;\n\tbool was_locked = false;\n\tpmd_t _pmd;\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,\n\t\t\t\taddress & HPAGE_PMD_MASK,\n\t\t\t\t(address & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\tptl = pmd_lock(vma->vm_mm, pmd);\n\n\t/*\n\t * If caller asks to setup a migration entries, we need a page to check\n\t * pmd against. Otherwise we can end up replacing wrong page.\n\t */\n\tVM_BUG_ON(freeze && !page);\n\tif (page) {\n\t\tVM_WARN_ON_ONCE(!PageLocked(page));\n\t\twas_locked = true;\n\t\tif (page != pmd_page(*pmd))\n\t\t\tgoto out;\n\t}\n\nrepeat:\n\tif (pmd_trans_huge(*pmd)) {\n\t\tif (!page) {\n\t\t\tpage = pmd_page(*pmd);\n\t\t\tif (unlikely(!trylock_page(page))) {\n\t\t\t\tget_page(page);\n\t\t\t\t_pmd = *pmd;\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tlock_page(page);\n\t\t\t\tspin_lock(ptl);\n\t\t\t\tif (unlikely(!pmd_same(*pmd, _pmd))) {\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tput_page(page);\n\t\t\t\t\tpage = NULL;\n\t\t\t\t\tgoto repeat;\n\t\t\t\t}\n\t\t\t\tput_page(page);\n\t\t\t}\n\t\t}\n\t\tif (PageMlocked(page))\n\t\t\tclear_page_mlock(page);\n\t} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))\n\t\tgoto out;\n\t__split_huge_pmd_locked(vma, pmd, range.start, freeze);\nout:\n\tspin_unlock(ptl);\n\tif (!was_locked && page)\n\t\tunlock_page(page);\n\t/*\n\t * No need to double call mmu_notifier->invalidate_range() callback.\n\t * They are 3 cases to consider inside __split_huge_pmd_locked():\n\t *  1) pmdp_huge_clear_flush_notify() call invalidate_range() obvious\n\t *  2) __split_huge_zero_page_pmd() read only zero page and any write\n\t *    fault will trigger a flush_notify before pointing to a new page\n\t *    (it is fine if the secondary mmu keeps pointing to the old zero\n\t *    page in the meantime)\n\t *  3) Split a huge pmd into pte pointing to the same page. No need\n\t *     to invalidate secondary tlb entry they are all still valid.\n\t *     any further changes to individual pte will notify. So no need\n\t *     to call mmu_notifier->invalidate_range()\n\t */\n\tmmu_notifier_invalidate_range_only_end(&range);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,8 @@\n {\n \tspinlock_t *ptl;\n \tstruct mmu_notifier_range range;\n+\tbool was_locked = false;\n+\tpmd_t _pmd;\n \n \tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,\n \t\t\t\taddress & HPAGE_PMD_MASK,\n@@ -15,11 +17,32 @@\n \t * pmd against. Otherwise we can end up replacing wrong page.\n \t */\n \tVM_BUG_ON(freeze && !page);\n-\tif (page && page != pmd_page(*pmd))\n-\t        goto out;\n+\tif (page) {\n+\t\tVM_WARN_ON_ONCE(!PageLocked(page));\n+\t\twas_locked = true;\n+\t\tif (page != pmd_page(*pmd))\n+\t\t\tgoto out;\n+\t}\n \n+repeat:\n \tif (pmd_trans_huge(*pmd)) {\n-\t\tpage = pmd_page(*pmd);\n+\t\tif (!page) {\n+\t\t\tpage = pmd_page(*pmd);\n+\t\t\tif (unlikely(!trylock_page(page))) {\n+\t\t\t\tget_page(page);\n+\t\t\t\t_pmd = *pmd;\n+\t\t\t\tspin_unlock(ptl);\n+\t\t\t\tlock_page(page);\n+\t\t\t\tspin_lock(ptl);\n+\t\t\t\tif (unlikely(!pmd_same(*pmd, _pmd))) {\n+\t\t\t\t\tunlock_page(page);\n+\t\t\t\t\tput_page(page);\n+\t\t\t\t\tpage = NULL;\n+\t\t\t\t\tgoto repeat;\n+\t\t\t\t}\n+\t\t\t\tput_page(page);\n+\t\t\t}\n+\t\t}\n \t\tif (PageMlocked(page))\n \t\t\tclear_page_mlock(page);\n \t} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))\n@@ -27,6 +50,8 @@\n \t__split_huge_pmd_locked(vma, pmd, range.start, freeze);\n out:\n \tspin_unlock(ptl);\n+\tif (!was_locked && page)\n+\t\tunlock_page(page);\n \t/*\n \t * No need to double call mmu_notifier->invalidate_range() callback.\n \t * They are 3 cases to consider inside __split_huge_pmd_locked():",
        "function_modified_lines": {
            "added": [
                "\tbool was_locked = false;",
                "\tpmd_t _pmd;",
                "\tif (page) {",
                "\t\tVM_WARN_ON_ONCE(!PageLocked(page));",
                "\t\twas_locked = true;",
                "\t\tif (page != pmd_page(*pmd))",
                "\t\t\tgoto out;",
                "\t}",
                "repeat:",
                "\t\tif (!page) {",
                "\t\t\tpage = pmd_page(*pmd);",
                "\t\t\tif (unlikely(!trylock_page(page))) {",
                "\t\t\t\tget_page(page);",
                "\t\t\t\t_pmd = *pmd;",
                "\t\t\t\tspin_unlock(ptl);",
                "\t\t\t\tlock_page(page);",
                "\t\t\t\tspin_lock(ptl);",
                "\t\t\t\tif (unlikely(!pmd_same(*pmd, _pmd))) {",
                "\t\t\t\t\tunlock_page(page);",
                "\t\t\t\t\tput_page(page);",
                "\t\t\t\t\tpage = NULL;",
                "\t\t\t\t\tgoto repeat;",
                "\t\t\t\t}",
                "\t\t\t\tput_page(page);",
                "\t\t\t}",
                "\t\t}",
                "\tif (!was_locked && page)",
                "\t\tunlock_page(page);"
            ],
            "deleted": [
                "\tif (page && page != pmd_page(*pmd))",
                "\t        goto out;",
                "\t\tpage = pmd_page(*pmd);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in __split_huge_pmd in mm/huge_memory.c in the Linux kernel before 5.7.5. The copy-on-write implementation can grant unintended write access because of a race condition in a THP mapcount check, aka CID-c444eb564fb1."
    },
    {
        "cve_id": "CVE-2020-29369",
        "code_before_change": "static void\ndetach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,\n\tstruct vm_area_struct *prev, unsigned long end)\n{\n\tstruct vm_area_struct **insertion_point;\n\tstruct vm_area_struct *tail_vma = NULL;\n\n\tinsertion_point = (prev ? &prev->vm_next : &mm->mmap);\n\tvma->vm_prev = NULL;\n\tdo {\n\t\tvma_rb_erase(vma, &mm->mm_rb);\n\t\tmm->map_count--;\n\t\ttail_vma = vma;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\n\t*insertion_point = vma;\n\tif (vma) {\n\t\tvma->vm_prev = prev;\n\t\tvma_gap_update(vma);\n\t} else\n\t\tmm->highest_vm_end = prev ? vm_end_gap(prev) : 0;\n\ttail_vma->vm_next = NULL;\n\n\t/* Kill the cache */\n\tvmacache_invalidate(mm);\n}",
        "code_after_change": "static bool\ndetach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,\n\tstruct vm_area_struct *prev, unsigned long end)\n{\n\tstruct vm_area_struct **insertion_point;\n\tstruct vm_area_struct *tail_vma = NULL;\n\n\tinsertion_point = (prev ? &prev->vm_next : &mm->mmap);\n\tvma->vm_prev = NULL;\n\tdo {\n\t\tvma_rb_erase(vma, &mm->mm_rb);\n\t\tmm->map_count--;\n\t\ttail_vma = vma;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\n\t*insertion_point = vma;\n\tif (vma) {\n\t\tvma->vm_prev = prev;\n\t\tvma_gap_update(vma);\n\t} else\n\t\tmm->highest_vm_end = prev ? vm_end_gap(prev) : 0;\n\ttail_vma->vm_next = NULL;\n\n\t/* Kill the cache */\n\tvmacache_invalidate(mm);\n\n\t/*\n\t * Do not downgrade mmap_lock if we are next to VM_GROWSDOWN or\n\t * VM_GROWSUP VMA. Such VMAs can change their size under\n\t * down_read(mmap_lock) and collide with the VMA we are about to unmap.\n\t */\n\tif (vma && (vma->vm_flags & VM_GROWSDOWN))\n\t\treturn false;\n\tif (prev && (prev->vm_flags & VM_GROWSUP))\n\t\treturn false;\n\treturn true;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-static void\n+static bool\n detach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,\n \tstruct vm_area_struct *prev, unsigned long end)\n {\n@@ -23,4 +23,15 @@\n \n \t/* Kill the cache */\n \tvmacache_invalidate(mm);\n+\n+\t/*\n+\t * Do not downgrade mmap_lock if we are next to VM_GROWSDOWN or\n+\t * VM_GROWSUP VMA. Such VMAs can change their size under\n+\t * down_read(mmap_lock) and collide with the VMA we are about to unmap.\n+\t */\n+\tif (vma && (vma->vm_flags & VM_GROWSDOWN))\n+\t\treturn false;\n+\tif (prev && (prev->vm_flags & VM_GROWSUP))\n+\t\treturn false;\n+\treturn true;\n }",
        "function_modified_lines": {
            "added": [
                "static bool",
                "",
                "\t/*",
                "\t * Do not downgrade mmap_lock if we are next to VM_GROWSDOWN or",
                "\t * VM_GROWSUP VMA. Such VMAs can change their size under",
                "\t * down_read(mmap_lock) and collide with the VMA we are about to unmap.",
                "\t */",
                "\tif (vma && (vma->vm_flags & VM_GROWSDOWN))",
                "\t\treturn false;",
                "\tif (prev && (prev->vm_flags & VM_GROWSUP))",
                "\t\treturn false;",
                "\treturn true;"
            ],
            "deleted": [
                "static void"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in mm/mmap.c in the Linux kernel before 5.7.11. There is a race condition between certain expand functions (expand_downwards and expand_upwards) and page-table free operations from an munmap call, aka CID-246c320a8cfe."
    },
    {
        "cve_id": "CVE-2020-29369",
        "code_before_change": "int __do_munmap(struct mm_struct *mm, unsigned long start, size_t len,\n\t\tstruct list_head *uf, bool downgrade)\n{\n\tunsigned long end;\n\tstruct vm_area_struct *vma, *prev, *last;\n\n\tif ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * arch_unmap() might do unmaps itself.  It must be called\n\t * and finish any rbtree manipulation before this code\n\t * runs and also starts to manipulate the rbtree.\n\t */\n\tarch_unmap(mm, start, end);\n\n\t/* Find the first overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma)\n\t\treturn 0;\n\tprev = vma->vm_prev;\n\t/* we have  start < vma->vm_end  */\n\n\t/* if it doesn't overlap, we have nothing.. */\n\tif (vma->vm_start >= end)\n\t\treturn 0;\n\n\t/*\n\t * If we need to split any vma, do it now to save pain later.\n\t *\n\t * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially\n\t * unmapped vm_area_struct will remain in use: so lower split_vma\n\t * places tmp vma above, and higher split_vma places tmp vma below.\n\t */\n\tif (start > vma->vm_start) {\n\t\tint error;\n\n\t\t/*\n\t\t * Make sure that map_count on return from munmap() will\n\t\t * not exceed its limit; but let map_count go just above\n\t\t * its limit temporarily, to help free resources as expected.\n\t\t */\n\t\tif (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)\n\t\t\treturn -ENOMEM;\n\n\t\terror = __split_vma(mm, vma, start, 0);\n\t\tif (error)\n\t\t\treturn error;\n\t\tprev = vma;\n\t}\n\n\t/* Does it split the last one? */\n\tlast = find_vma(mm, end);\n\tif (last && end > last->vm_start) {\n\t\tint error = __split_vma(mm, last, end, 1);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tvma = prev ? prev->vm_next : mm->mmap;\n\n\tif (unlikely(uf)) {\n\t\t/*\n\t\t * If userfaultfd_unmap_prep returns an error the vmas\n\t\t * will remain splitted, but userland will get a\n\t\t * highly unexpected error anyway. This is no\n\t\t * different than the case where the first of the two\n\t\t * __split_vma fails, but we don't undo the first\n\t\t * split, despite we could. This is unlikely enough\n\t\t * failure that it's not worth optimizing it for.\n\t\t */\n\t\tint error = userfaultfd_unmap_prep(vma, start, end, uf);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/*\n\t * unlock any mlock()ed ranges before detaching vmas\n\t */\n\tif (mm->locked_vm) {\n\t\tstruct vm_area_struct *tmp = vma;\n\t\twhile (tmp && tmp->vm_start < end) {\n\t\t\tif (tmp->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm -= vma_pages(tmp);\n\t\t\t\tmunlock_vma_pages_all(tmp);\n\t\t\t}\n\n\t\t\ttmp = tmp->vm_next;\n\t\t}\n\t}\n\n\t/* Detach vmas from rbtree */\n\tdetach_vmas_to_be_unmapped(mm, vma, prev, end);\n\n\tif (downgrade)\n\t\tmmap_write_downgrade(mm);\n\n\tunmap_region(mm, vma, prev, start, end);\n\n\t/* Fix up all other VM information */\n\tremove_vma_list(mm, vma);\n\n\treturn downgrade ? 1 : 0;\n}",
        "code_after_change": "int __do_munmap(struct mm_struct *mm, unsigned long start, size_t len,\n\t\tstruct list_head *uf, bool downgrade)\n{\n\tunsigned long end;\n\tstruct vm_area_struct *vma, *prev, *last;\n\n\tif ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * arch_unmap() might do unmaps itself.  It must be called\n\t * and finish any rbtree manipulation before this code\n\t * runs and also starts to manipulate the rbtree.\n\t */\n\tarch_unmap(mm, start, end);\n\n\t/* Find the first overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma)\n\t\treturn 0;\n\tprev = vma->vm_prev;\n\t/* we have  start < vma->vm_end  */\n\n\t/* if it doesn't overlap, we have nothing.. */\n\tif (vma->vm_start >= end)\n\t\treturn 0;\n\n\t/*\n\t * If we need to split any vma, do it now to save pain later.\n\t *\n\t * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially\n\t * unmapped vm_area_struct will remain in use: so lower split_vma\n\t * places tmp vma above, and higher split_vma places tmp vma below.\n\t */\n\tif (start > vma->vm_start) {\n\t\tint error;\n\n\t\t/*\n\t\t * Make sure that map_count on return from munmap() will\n\t\t * not exceed its limit; but let map_count go just above\n\t\t * its limit temporarily, to help free resources as expected.\n\t\t */\n\t\tif (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)\n\t\t\treturn -ENOMEM;\n\n\t\terror = __split_vma(mm, vma, start, 0);\n\t\tif (error)\n\t\t\treturn error;\n\t\tprev = vma;\n\t}\n\n\t/* Does it split the last one? */\n\tlast = find_vma(mm, end);\n\tif (last && end > last->vm_start) {\n\t\tint error = __split_vma(mm, last, end, 1);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tvma = prev ? prev->vm_next : mm->mmap;\n\n\tif (unlikely(uf)) {\n\t\t/*\n\t\t * If userfaultfd_unmap_prep returns an error the vmas\n\t\t * will remain splitted, but userland will get a\n\t\t * highly unexpected error anyway. This is no\n\t\t * different than the case where the first of the two\n\t\t * __split_vma fails, but we don't undo the first\n\t\t * split, despite we could. This is unlikely enough\n\t\t * failure that it's not worth optimizing it for.\n\t\t */\n\t\tint error = userfaultfd_unmap_prep(vma, start, end, uf);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/*\n\t * unlock any mlock()ed ranges before detaching vmas\n\t */\n\tif (mm->locked_vm) {\n\t\tstruct vm_area_struct *tmp = vma;\n\t\twhile (tmp && tmp->vm_start < end) {\n\t\t\tif (tmp->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm -= vma_pages(tmp);\n\t\t\t\tmunlock_vma_pages_all(tmp);\n\t\t\t}\n\n\t\t\ttmp = tmp->vm_next;\n\t\t}\n\t}\n\n\t/* Detach vmas from rbtree */\n\tif (!detach_vmas_to_be_unmapped(mm, vma, prev, end))\n\t\tdowngrade = false;\n\n\tif (downgrade)\n\t\tmmap_write_downgrade(mm);\n\n\tunmap_region(mm, vma, prev, start, end);\n\n\t/* Fix up all other VM information */\n\tremove_vma_list(mm, vma);\n\n\treturn downgrade ? 1 : 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -94,7 +94,8 @@\n \t}\n \n \t/* Detach vmas from rbtree */\n-\tdetach_vmas_to_be_unmapped(mm, vma, prev, end);\n+\tif (!detach_vmas_to_be_unmapped(mm, vma, prev, end))\n+\t\tdowngrade = false;\n \n \tif (downgrade)\n \t\tmmap_write_downgrade(mm);",
        "function_modified_lines": {
            "added": [
                "\tif (!detach_vmas_to_be_unmapped(mm, vma, prev, end))",
                "\t\tdowngrade = false;"
            ],
            "deleted": [
                "\tdetach_vmas_to_be_unmapped(mm, vma, prev, end);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in mm/mmap.c in the Linux kernel before 5.7.11. There is a race condition between certain expand functions (expand_downwards and expand_upwards) and page-table free operations from an munmap call, aka CID-246c320a8cfe."
    },
    {
        "cve_id": "CVE-2020-29370",
        "code_before_change": "int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,\n\t\t\t  void **p)\n{\n\tstruct kmem_cache_cpu *c;\n\tint i;\n\n\t/* memcg and kmem_cache debug support */\n\ts = slab_pre_alloc_hook(s, flags);\n\tif (unlikely(!s))\n\t\treturn false;\n\t/*\n\t * Drain objects in the per cpu slab, while disabling local\n\t * IRQs, which protects against PREEMPT and interrupts\n\t * handlers invoking normal fastpath.\n\t */\n\tlocal_irq_disable();\n\tc = this_cpu_ptr(s->cpu_slab);\n\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *object = c->freelist;\n\n\t\tif (unlikely(!object)) {\n\t\t\t/*\n\t\t\t * Invoking slow path likely have side-effect\n\t\t\t * of re-populating per CPU c->freelist\n\t\t\t */\n\t\t\tp[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,\n\t\t\t\t\t    _RET_IP_, c);\n\t\t\tif (unlikely(!p[i]))\n\t\t\t\tgoto error;\n\n\t\t\tc = this_cpu_ptr(s->cpu_slab);\n\t\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\n\t\t\tcontinue; /* goto for-loop */\n\t\t}\n\t\tc->freelist = get_freepointer(s, object);\n\t\tp[i] = object;\n\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\t}\n\tc->tid = next_tid(c->tid);\n\tlocal_irq_enable();\n\n\t/* Clear memory outside IRQ disabled fastpath loop */\n\tif (unlikely(slab_want_init_on_alloc(flags, s))) {\n\t\tint j;\n\n\t\tfor (j = 0; j < i; j++)\n\t\t\tmemset(p[j], 0, s->object_size);\n\t}\n\n\t/* memcg and kmem_cache debug support */\n\tslab_post_alloc_hook(s, flags, size, p);\n\treturn i;\nerror:\n\tlocal_irq_enable();\n\tslab_post_alloc_hook(s, flags, i, p);\n\t__kmem_cache_free_bulk(s, i, p);\n\treturn 0;\n}",
        "code_after_change": "int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,\n\t\t\t  void **p)\n{\n\tstruct kmem_cache_cpu *c;\n\tint i;\n\n\t/* memcg and kmem_cache debug support */\n\ts = slab_pre_alloc_hook(s, flags);\n\tif (unlikely(!s))\n\t\treturn false;\n\t/*\n\t * Drain objects in the per cpu slab, while disabling local\n\t * IRQs, which protects against PREEMPT and interrupts\n\t * handlers invoking normal fastpath.\n\t */\n\tlocal_irq_disable();\n\tc = this_cpu_ptr(s->cpu_slab);\n\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *object = c->freelist;\n\n\t\tif (unlikely(!object)) {\n\t\t\t/*\n\t\t\t * We may have removed an object from c->freelist using\n\t\t\t * the fastpath in the previous iteration; in that case,\n\t\t\t * c->tid has not been bumped yet.\n\t\t\t * Since ___slab_alloc() may reenable interrupts while\n\t\t\t * allocating memory, we should bump c->tid now.\n\t\t\t */\n\t\t\tc->tid = next_tid(c->tid);\n\n\t\t\t/*\n\t\t\t * Invoking slow path likely have side-effect\n\t\t\t * of re-populating per CPU c->freelist\n\t\t\t */\n\t\t\tp[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,\n\t\t\t\t\t    _RET_IP_, c);\n\t\t\tif (unlikely(!p[i]))\n\t\t\t\tgoto error;\n\n\t\t\tc = this_cpu_ptr(s->cpu_slab);\n\t\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\n\t\t\tcontinue; /* goto for-loop */\n\t\t}\n\t\tc->freelist = get_freepointer(s, object);\n\t\tp[i] = object;\n\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\t}\n\tc->tid = next_tid(c->tid);\n\tlocal_irq_enable();\n\n\t/* Clear memory outside IRQ disabled fastpath loop */\n\tif (unlikely(slab_want_init_on_alloc(flags, s))) {\n\t\tint j;\n\n\t\tfor (j = 0; j < i; j++)\n\t\t\tmemset(p[j], 0, s->object_size);\n\t}\n\n\t/* memcg and kmem_cache debug support */\n\tslab_post_alloc_hook(s, flags, size, p);\n\treturn i;\nerror:\n\tlocal_irq_enable();\n\tslab_post_alloc_hook(s, flags, i, p);\n\t__kmem_cache_free_bulk(s, i, p);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,6 +20,15 @@\n \t\tvoid *object = c->freelist;\n \n \t\tif (unlikely(!object)) {\n+\t\t\t/*\n+\t\t\t * We may have removed an object from c->freelist using\n+\t\t\t * the fastpath in the previous iteration; in that case,\n+\t\t\t * c->tid has not been bumped yet.\n+\t\t\t * Since ___slab_alloc() may reenable interrupts while\n+\t\t\t * allocating memory, we should bump c->tid now.\n+\t\t\t */\n+\t\t\tc->tid = next_tid(c->tid);\n+\n \t\t\t/*\n \t\t\t * Invoking slow path likely have side-effect\n \t\t\t * of re-populating per CPU c->freelist",
        "function_modified_lines": {
            "added": [
                "\t\t\t/*",
                "\t\t\t * We may have removed an object from c->freelist using",
                "\t\t\t * the fastpath in the previous iteration; in that case,",
                "\t\t\t * c->tid has not been bumped yet.",
                "\t\t\t * Since ___slab_alloc() may reenable interrupts while",
                "\t\t\t * allocating memory, we should bump c->tid now.",
                "\t\t\t */",
                "\t\t\tc->tid = next_tid(c->tid);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in kmem_cache_alloc_bulk in mm/slub.c in the Linux kernel before 5.5.11. The slowpath lacks the required TID increment, aka CID-fd4d9c7d0c71."
    },
    {
        "cve_id": "CVE-2020-29372",
        "code_before_change": "int do_madvise(unsigned long start, size_t len_in, int behavior)\n{\n\tunsigned long end, tmp;\n\tstruct vm_area_struct *vma, *prev;\n\tint unmapped_error = 0;\n\tint error = -EINVAL;\n\tint write;\n\tsize_t len;\n\tstruct blk_plug plug;\n\n\tstart = untagged_addr(start);\n\n\tif (!madvise_behavior_valid(behavior))\n\t\treturn error;\n\n\tif (!PAGE_ALIGNED(start))\n\t\treturn error;\n\tlen = PAGE_ALIGN(len_in);\n\n\t/* Check to see whether len was rounded up from small -ve to zero */\n\tif (len_in && !len)\n\t\treturn error;\n\n\tend = start + len;\n\tif (end < start)\n\t\treturn error;\n\n\terror = 0;\n\tif (end == start)\n\t\treturn error;\n\n#ifdef CONFIG_MEMORY_FAILURE\n\tif (behavior == MADV_HWPOISON || behavior == MADV_SOFT_OFFLINE)\n\t\treturn madvise_inject_error(behavior, start, start + len_in);\n#endif\n\n\twrite = madvise_need_mmap_write(behavior);\n\tif (write) {\n\t\tif (down_write_killable(&current->mm->mmap_sem))\n\t\t\treturn -EINTR;\n\t} else {\n\t\tdown_read(&current->mm->mmap_sem);\n\t}\n\n\t/*\n\t * If the interval [start,end) covers some unmapped address\n\t * ranges, just ignore them, but return -ENOMEM at the end.\n\t * - different from the way of handling in mlock etc.\n\t */\n\tvma = find_vma_prev(current->mm, start, &prev);\n\tif (vma && start > vma->vm_start)\n\t\tprev = vma;\n\n\tblk_start_plug(&plug);\n\tfor (;;) {\n\t\t/* Still start < end. */\n\t\terror = -ENOMEM;\n\t\tif (!vma)\n\t\t\tgoto out;\n\n\t\t/* Here start < (end|vma->vm_end). */\n\t\tif (start < vma->vm_start) {\n\t\t\tunmapped_error = -ENOMEM;\n\t\t\tstart = vma->vm_start;\n\t\t\tif (start >= end)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t/* Here vma->vm_start <= start < (end|vma->vm_end) */\n\t\ttmp = vma->vm_end;\n\t\tif (end < tmp)\n\t\t\ttmp = end;\n\n\t\t/* Here vma->vm_start <= start < tmp <= (end|vma->vm_end). */\n\t\terror = madvise_vma(vma, &prev, start, tmp, behavior);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tstart = tmp;\n\t\tif (prev && start < prev->vm_end)\n\t\t\tstart = prev->vm_end;\n\t\terror = unmapped_error;\n\t\tif (start >= end)\n\t\t\tgoto out;\n\t\tif (prev)\n\t\t\tvma = prev->vm_next;\n\t\telse\t/* madvise_remove dropped mmap_sem */\n\t\t\tvma = find_vma(current->mm, start);\n\t}\nout:\n\tblk_finish_plug(&plug);\n\tif (write)\n\t\tup_write(&current->mm->mmap_sem);\n\telse\n\t\tup_read(&current->mm->mmap_sem);\n\n\treturn error;\n}",
        "code_after_change": "int do_madvise(unsigned long start, size_t len_in, int behavior)\n{\n\tunsigned long end, tmp;\n\tstruct vm_area_struct *vma, *prev;\n\tint unmapped_error = 0;\n\tint error = -EINVAL;\n\tint write;\n\tsize_t len;\n\tstruct blk_plug plug;\n\n\tstart = untagged_addr(start);\n\n\tif (!madvise_behavior_valid(behavior))\n\t\treturn error;\n\n\tif (!PAGE_ALIGNED(start))\n\t\treturn error;\n\tlen = PAGE_ALIGN(len_in);\n\n\t/* Check to see whether len was rounded up from small -ve to zero */\n\tif (len_in && !len)\n\t\treturn error;\n\n\tend = start + len;\n\tif (end < start)\n\t\treturn error;\n\n\terror = 0;\n\tif (end == start)\n\t\treturn error;\n\n#ifdef CONFIG_MEMORY_FAILURE\n\tif (behavior == MADV_HWPOISON || behavior == MADV_SOFT_OFFLINE)\n\t\treturn madvise_inject_error(behavior, start, start + len_in);\n#endif\n\n\twrite = madvise_need_mmap_write(behavior);\n\tif (write) {\n\t\tif (down_write_killable(&current->mm->mmap_sem))\n\t\t\treturn -EINTR;\n\n\t\t/*\n\t\t * We may have stolen the mm from another process\n\t\t * that is undergoing core dumping.\n\t\t *\n\t\t * Right now that's io_ring, in the future it may\n\t\t * be remote process management and not \"current\"\n\t\t * at all.\n\t\t *\n\t\t * We need to fix core dumping to not do this,\n\t\t * but for now we have the mmget_still_valid()\n\t\t * model.\n\t\t */\n\t\tif (!mmget_still_valid(current->mm)) {\n\t\t\tup_write(&current->mm->mmap_sem);\n\t\t\treturn -EINTR;\n\t\t}\n\t} else {\n\t\tdown_read(&current->mm->mmap_sem);\n\t}\n\n\t/*\n\t * If the interval [start,end) covers some unmapped address\n\t * ranges, just ignore them, but return -ENOMEM at the end.\n\t * - different from the way of handling in mlock etc.\n\t */\n\tvma = find_vma_prev(current->mm, start, &prev);\n\tif (vma && start > vma->vm_start)\n\t\tprev = vma;\n\n\tblk_start_plug(&plug);\n\tfor (;;) {\n\t\t/* Still start < end. */\n\t\terror = -ENOMEM;\n\t\tif (!vma)\n\t\t\tgoto out;\n\n\t\t/* Here start < (end|vma->vm_end). */\n\t\tif (start < vma->vm_start) {\n\t\t\tunmapped_error = -ENOMEM;\n\t\t\tstart = vma->vm_start;\n\t\t\tif (start >= end)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t/* Here vma->vm_start <= start < (end|vma->vm_end) */\n\t\ttmp = vma->vm_end;\n\t\tif (end < tmp)\n\t\t\ttmp = end;\n\n\t\t/* Here vma->vm_start <= start < tmp <= (end|vma->vm_end). */\n\t\terror = madvise_vma(vma, &prev, start, tmp, behavior);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tstart = tmp;\n\t\tif (prev && start < prev->vm_end)\n\t\t\tstart = prev->vm_end;\n\t\terror = unmapped_error;\n\t\tif (start >= end)\n\t\t\tgoto out;\n\t\tif (prev)\n\t\t\tvma = prev->vm_next;\n\t\telse\t/* madvise_remove dropped mmap_sem */\n\t\t\tvma = find_vma(current->mm, start);\n\t}\nout:\n\tblk_finish_plug(&plug);\n\tif (write)\n\t\tup_write(&current->mm->mmap_sem);\n\telse\n\t\tup_read(&current->mm->mmap_sem);\n\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,6 +38,23 @@\n \tif (write) {\n \t\tif (down_write_killable(&current->mm->mmap_sem))\n \t\t\treturn -EINTR;\n+\n+\t\t/*\n+\t\t * We may have stolen the mm from another process\n+\t\t * that is undergoing core dumping.\n+\t\t *\n+\t\t * Right now that's io_ring, in the future it may\n+\t\t * be remote process management and not \"current\"\n+\t\t * at all.\n+\t\t *\n+\t\t * We need to fix core dumping to not do this,\n+\t\t * but for now we have the mmget_still_valid()\n+\t\t * model.\n+\t\t */\n+\t\tif (!mmget_still_valid(current->mm)) {\n+\t\t\tup_write(&current->mm->mmap_sem);\n+\t\t\treturn -EINTR;\n+\t\t}\n \t} else {\n \t\tdown_read(&current->mm->mmap_sem);\n \t}",
        "function_modified_lines": {
            "added": [
                "",
                "\t\t/*",
                "\t\t * We may have stolen the mm from another process",
                "\t\t * that is undergoing core dumping.",
                "\t\t *",
                "\t\t * Right now that's io_ring, in the future it may",
                "\t\t * be remote process management and not \"current\"",
                "\t\t * at all.",
                "\t\t *",
                "\t\t * We need to fix core dumping to not do this,",
                "\t\t * but for now we have the mmget_still_valid()",
                "\t\t * model.",
                "\t\t */",
                "\t\tif (!mmget_still_valid(current->mm)) {",
                "\t\t\tup_write(&current->mm->mmap_sem);",
                "\t\t\treturn -EINTR;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in do_madvise in mm/madvise.c in the Linux kernel before 5.6.8. There is a race condition between coredump operations and the IORING_OP_MADVISE implementation, aka CID-bc0c4d1e176e."
    },
    {
        "cve_id": "CVE-2020-29374",
        "code_before_change": "static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)\n{\n\tconst unsigned long num_pages = obj->base.size >> PAGE_SHIFT;\n\tstruct mm_struct *mm = obj->userptr.mm->mm;\n\tstruct page **pvec;\n\tstruct sg_table *pages;\n\tbool active;\n\tint pinned;\n\n\t/* If userspace should engineer that these pages are replaced in\n\t * the vma between us binding this page into the GTT and completion\n\t * of rendering... Their loss. If they change the mapping of their\n\t * pages they need to create a new bo to point to the new vma.\n\t *\n\t * However, that still leaves open the possibility of the vma\n\t * being copied upon fork. Which falls under the same userspace\n\t * synchronisation issue as a regular bo, except that this time\n\t * the process may not be expecting that a particular piece of\n\t * memory is tied to the GPU.\n\t *\n\t * Fortunately, we can hook into the mmu_notifier in order to\n\t * discard the page references prior to anything nasty happening\n\t * to the vma (discard or cloning) which should prevent the more\n\t * egregious cases from causing harm.\n\t */\n\n\tif (obj->userptr.work) {\n\t\t/* active flag should still be held for the pending work */\n\t\tif (IS_ERR(obj->userptr.work))\n\t\t\treturn PTR_ERR(obj->userptr.work);\n\t\telse\n\t\t\treturn -EAGAIN;\n\t}\n\n\tpvec = NULL;\n\tpinned = 0;\n\n\tif (mm == current->mm) {\n\t\tpvec = kvmalloc_array(num_pages, sizeof(struct page *),\n\t\t\t\t      GFP_KERNEL |\n\t\t\t\t      __GFP_NORETRY |\n\t\t\t\t      __GFP_NOWARN);\n\t\tif (pvec) /* defer to worker if malloc fails */\n\t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n\t\t\t\t\t\t       num_pages,\n\t\t\t\t\t\t       !i915_gem_object_is_readonly(obj),\n\t\t\t\t\t\t       pvec);\n\t}\n\n\tactive = false;\n\tif (pinned < 0) {\n\t\tpages = ERR_PTR(pinned);\n\t\tpinned = 0;\n\t} else if (pinned < num_pages) {\n\t\tpages = __i915_gem_userptr_get_pages_schedule(obj);\n\t\tactive = pages == ERR_PTR(-EAGAIN);\n\t} else {\n\t\tpages = __i915_gem_userptr_alloc_pages(obj, pvec, num_pages);\n\t\tactive = !IS_ERR(pages);\n\t}\n\tif (active)\n\t\t__i915_gem_userptr_set_active(obj, true);\n\n\tif (IS_ERR(pages))\n\t\trelease_pages(pvec, pinned);\n\tkvfree(pvec);\n\n\treturn PTR_ERR_OR_ZERO(pages);\n}",
        "code_after_change": "static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)\n{\n\tconst unsigned long num_pages = obj->base.size >> PAGE_SHIFT;\n\tstruct mm_struct *mm = obj->userptr.mm->mm;\n\tstruct page **pvec;\n\tstruct sg_table *pages;\n\tbool active;\n\tint pinned;\n\n\t/* If userspace should engineer that these pages are replaced in\n\t * the vma between us binding this page into the GTT and completion\n\t * of rendering... Their loss. If they change the mapping of their\n\t * pages they need to create a new bo to point to the new vma.\n\t *\n\t * However, that still leaves open the possibility of the vma\n\t * being copied upon fork. Which falls under the same userspace\n\t * synchronisation issue as a regular bo, except that this time\n\t * the process may not be expecting that a particular piece of\n\t * memory is tied to the GPU.\n\t *\n\t * Fortunately, we can hook into the mmu_notifier in order to\n\t * discard the page references prior to anything nasty happening\n\t * to the vma (discard or cloning) which should prevent the more\n\t * egregious cases from causing harm.\n\t */\n\n\tif (obj->userptr.work) {\n\t\t/* active flag should still be held for the pending work */\n\t\tif (IS_ERR(obj->userptr.work))\n\t\t\treturn PTR_ERR(obj->userptr.work);\n\t\telse\n\t\t\treturn -EAGAIN;\n\t}\n\n\tpvec = NULL;\n\tpinned = 0;\n\n\tif (mm == current->mm) {\n\t\tpvec = kvmalloc_array(num_pages, sizeof(struct page *),\n\t\t\t\t      GFP_KERNEL |\n\t\t\t\t      __GFP_NORETRY |\n\t\t\t\t      __GFP_NOWARN);\n\t\t/*\n\t\t * Using __get_user_pages_fast() with a read-only\n\t\t * access is questionable. A read-only page may be\n\t\t * COW-broken, and then this might end up giving\n\t\t * the wrong side of the COW..\n\t\t *\n\t\t * We may or may not care.\n\t\t */\n\t\tif (pvec) /* defer to worker if malloc fails */\n\t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n\t\t\t\t\t\t       num_pages,\n\t\t\t\t\t\t       !i915_gem_object_is_readonly(obj),\n\t\t\t\t\t\t       pvec);\n\t}\n\n\tactive = false;\n\tif (pinned < 0) {\n\t\tpages = ERR_PTR(pinned);\n\t\tpinned = 0;\n\t} else if (pinned < num_pages) {\n\t\tpages = __i915_gem_userptr_get_pages_schedule(obj);\n\t\tactive = pages == ERR_PTR(-EAGAIN);\n\t} else {\n\t\tpages = __i915_gem_userptr_alloc_pages(obj, pvec, num_pages);\n\t\tactive = !IS_ERR(pages);\n\t}\n\tif (active)\n\t\t__i915_gem_userptr_set_active(obj, true);\n\n\tif (IS_ERR(pages))\n\t\trelease_pages(pvec, pinned);\n\tkvfree(pvec);\n\n\treturn PTR_ERR_OR_ZERO(pages);\n}",
        "patch": "--- code before\n+++ code after\n@@ -40,6 +40,14 @@\n \t\t\t\t      GFP_KERNEL |\n \t\t\t\t      __GFP_NORETRY |\n \t\t\t\t      __GFP_NOWARN);\n+\t\t/*\n+\t\t * Using __get_user_pages_fast() with a read-only\n+\t\t * access is questionable. A read-only page may be\n+\t\t * COW-broken, and then this might end up giving\n+\t\t * the wrong side of the COW..\n+\t\t *\n+\t\t * We may or may not care.\n+\t\t */\n \t\tif (pvec) /* defer to worker if malloc fails */\n \t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n \t\t\t\t\t\t       num_pages,",
        "function_modified_lines": {
            "added": [
                "\t\t/*",
                "\t\t * Using __get_user_pages_fast() with a read-only",
                "\t\t * access is questionable. A read-only page may be",
                "\t\t * COW-broken, and then this might end up giving",
                "\t\t * the wrong side of the COW..",
                "\t\t *",
                "\t\t * We may or may not care.",
                "\t\t */"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-863"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58."
    },
    {
        "cve_id": "CVE-2020-29374",
        "code_before_change": "static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *locked)\n{\n\tlong ret = 0, i = 0;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct follow_page_context ctx = { NULL };\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\tstart = untagged_addr(start);\n\n\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));\n\n\t/*\n\t * If FOLL_FORCE is set then do not force a full fault as the hinting\n\t * fault information is unrelated to the reference behaviour of a task\n\t * using the address space\n\t */\n\tif (!(gup_flags & FOLL_FORCE))\n\t\tgup_flags |= FOLL_NUMA;\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned int foll_flags = gup_flags;\n\t\tunsigned int page_increm;\n\n\t\t/* first iteration or cross vma bound */\n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = find_extend_vma(mm, start);\n\t\t\tif (!vma && in_gate_area(mm, start)) {\n\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,\n\t\t\t\t\t\tgup_flags, &vma,\n\t\t\t\t\t\tpages ? &pages[i] : NULL);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tctx.page_mask = 0;\n\t\t\t\tgoto next_page;\n\t\t\t}\n\n\t\t\tif (!vma || check_vma_flags(vma, gup_flags)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &nr_pages, i,\n\t\t\t\t\t\tgup_flags, locked);\n\t\t\t\tif (locked && *locked == 0) {\n\t\t\t\t\t/*\n\t\t\t\t\t * We've got a VM_FAULT_RETRY\n\t\t\t\t\t * and we've lost mmap_sem.\n\t\t\t\t\t * We must stop here.\n\t\t\t\t\t */\n\t\t\t\t\tBUG_ON(gup_flags & FOLL_NOWAIT);\n\t\t\t\t\tBUG_ON(ret != 0);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\nretry:\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\tcond_resched();\n\n\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);\n\t\tif (!page) {\n\t\t\tret = faultin_page(tsk, vma, start, &foll_flags,\n\t\t\t\t\t   locked);\n\t\t\tswitch (ret) {\n\t\t\tcase 0:\n\t\t\t\tgoto retry;\n\t\t\tcase -EBUSY:\n\t\t\t\tret = 0;\n\t\t\t\tfallthrough;\n\t\t\tcase -EFAULT:\n\t\t\tcase -ENOMEM:\n\t\t\tcase -EHWPOISON:\n\t\t\t\tgoto out;\n\t\t\tcase -ENOENT:\n\t\t\t\tgoto next_page;\n\t\t\t}\n\t\t\tBUG();\n\t\t} else if (PTR_ERR(page) == -EEXIST) {\n\t\t\t/*\n\t\t\t * Proper page table entry exists, but no corresponding\n\t\t\t * struct page.\n\t\t\t */\n\t\t\tgoto next_page;\n\t\t} else if (IS_ERR(page)) {\n\t\t\tret = PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\n\t\tif (pages) {\n\t\t\tpages[i] = page;\n\t\t\tflush_anon_page(vma, page, start);\n\t\t\tflush_dcache_page(page);\n\t\t\tctx.page_mask = 0;\n\t\t}\nnext_page:\n\t\tif (vmas) {\n\t\t\tvmas[i] = vma;\n\t\t\tctx.page_mask = 0;\n\t\t}\n\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);\n\t\tif (page_increm > nr_pages)\n\t\t\tpage_increm = nr_pages;\n\t\ti += page_increm;\n\t\tstart += page_increm * PAGE_SIZE;\n\t\tnr_pages -= page_increm;\n\t} while (nr_pages);\nout:\n\tif (ctx.pgmap)\n\t\tput_dev_pagemap(ctx.pgmap);\n\treturn i ? i : ret;\n}",
        "code_after_change": "static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *locked)\n{\n\tlong ret = 0, i = 0;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct follow_page_context ctx = { NULL };\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\tstart = untagged_addr(start);\n\n\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));\n\n\t/*\n\t * If FOLL_FORCE is set then do not force a full fault as the hinting\n\t * fault information is unrelated to the reference behaviour of a task\n\t * using the address space\n\t */\n\tif (!(gup_flags & FOLL_FORCE))\n\t\tgup_flags |= FOLL_NUMA;\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned int foll_flags = gup_flags;\n\t\tunsigned int page_increm;\n\n\t\t/* first iteration or cross vma bound */\n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = find_extend_vma(mm, start);\n\t\t\tif (!vma && in_gate_area(mm, start)) {\n\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,\n\t\t\t\t\t\tgup_flags, &vma,\n\t\t\t\t\t\tpages ? &pages[i] : NULL);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tctx.page_mask = 0;\n\t\t\t\tgoto next_page;\n\t\t\t}\n\n\t\t\tif (!vma || check_vma_flags(vma, gup_flags)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\t\tif (should_force_cow_break(vma, foll_flags))\n\t\t\t\t\tfoll_flags |= FOLL_WRITE;\n\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &nr_pages, i,\n\t\t\t\t\t\tfoll_flags, locked);\n\t\t\t\tif (locked && *locked == 0) {\n\t\t\t\t\t/*\n\t\t\t\t\t * We've got a VM_FAULT_RETRY\n\t\t\t\t\t * and we've lost mmap_sem.\n\t\t\t\t\t * We must stop here.\n\t\t\t\t\t */\n\t\t\t\t\tBUG_ON(gup_flags & FOLL_NOWAIT);\n\t\t\t\t\tBUG_ON(ret != 0);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (should_force_cow_break(vma, foll_flags))\n\t\t\tfoll_flags |= FOLL_WRITE;\n\nretry:\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\tcond_resched();\n\n\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);\n\t\tif (!page) {\n\t\t\tret = faultin_page(tsk, vma, start, &foll_flags,\n\t\t\t\t\t   locked);\n\t\t\tswitch (ret) {\n\t\t\tcase 0:\n\t\t\t\tgoto retry;\n\t\t\tcase -EBUSY:\n\t\t\t\tret = 0;\n\t\t\t\tfallthrough;\n\t\t\tcase -EFAULT:\n\t\t\tcase -ENOMEM:\n\t\t\tcase -EHWPOISON:\n\t\t\t\tgoto out;\n\t\t\tcase -ENOENT:\n\t\t\t\tgoto next_page;\n\t\t\t}\n\t\t\tBUG();\n\t\t} else if (PTR_ERR(page) == -EEXIST) {\n\t\t\t/*\n\t\t\t * Proper page table entry exists, but no corresponding\n\t\t\t * struct page.\n\t\t\t */\n\t\t\tgoto next_page;\n\t\t} else if (IS_ERR(page)) {\n\t\t\tret = PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\n\t\tif (pages) {\n\t\t\tpages[i] = page;\n\t\t\tflush_anon_page(vma, page, start);\n\t\t\tflush_dcache_page(page);\n\t\t\tctx.page_mask = 0;\n\t\t}\nnext_page:\n\t\tif (vmas) {\n\t\t\tvmas[i] = vma;\n\t\t\tctx.page_mask = 0;\n\t\t}\n\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);\n\t\tif (page_increm > nr_pages)\n\t\t\tpage_increm = nr_pages;\n\t\ti += page_increm;\n\t\tstart += page_increm * PAGE_SIZE;\n\t\tnr_pages -= page_increm;\n\t} while (nr_pages);\nout:\n\tif (ctx.pgmap)\n\t\tput_dev_pagemap(ctx.pgmap);\n\treturn i ? i : ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -45,9 +45,11 @@\n \t\t\t\tgoto out;\n \t\t\t}\n \t\t\tif (is_vm_hugetlb_page(vma)) {\n+\t\t\t\tif (should_force_cow_break(vma, foll_flags))\n+\t\t\t\t\tfoll_flags |= FOLL_WRITE;\n \t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n \t\t\t\t\t\t&start, &nr_pages, i,\n-\t\t\t\t\t\tgup_flags, locked);\n+\t\t\t\t\t\tfoll_flags, locked);\n \t\t\t\tif (locked && *locked == 0) {\n \t\t\t\t\t/*\n \t\t\t\t\t * We've got a VM_FAULT_RETRY\n@@ -61,6 +63,10 @@\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t}\n+\n+\t\tif (should_force_cow_break(vma, foll_flags))\n+\t\t\tfoll_flags |= FOLL_WRITE;\n+\n retry:\n \t\t/*\n \t\t * If we have a pending SIGKILL, don't keep faulting pages and",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tif (should_force_cow_break(vma, foll_flags))",
                "\t\t\t\t\tfoll_flags |= FOLL_WRITE;",
                "\t\t\t\t\t\tfoll_flags, locked);",
                "",
                "\t\tif (should_force_cow_break(vma, foll_flags))",
                "\t\t\tfoll_flags |= FOLL_WRITE;",
                ""
            ],
            "deleted": [
                "\t\t\t\t\t\tgup_flags, locked);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-863"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58."
    },
    {
        "cve_id": "CVE-2020-29374",
        "code_before_change": "static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n{\n\treturn pte_write(pte) ||\n\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));\n}",
        "code_after_change": "static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n{\n\treturn pte_write(pte) || ((flags & FOLL_COW) && pte_dirty(pte));\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,4 @@\n static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n {\n-\treturn pte_write(pte) ||\n-\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));\n+\treturn pte_write(pte) || ((flags & FOLL_COW) && pte_dirty(pte));\n }",
        "function_modified_lines": {
            "added": [
                "\treturn pte_write(pte) || ((flags & FOLL_COW) && pte_dirty(pte));"
            ],
            "deleted": [
                "\treturn pte_write(pte) ||",
                "\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-863"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58."
    },
    {
        "cve_id": "CVE-2020-29374",
        "code_before_change": "int __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages)\n{\n\tunsigned long len, end;\n\tunsigned long flags;\n\tint nr_pinned = 0;\n\t/*\n\t * Internally (within mm/gup.c), gup fast variants must set FOLL_GET,\n\t * because gup fast is always a \"pin with a +1 page refcount\" request.\n\t */\n\tunsigned int gup_flags = FOLL_GET;\n\n\tif (write)\n\t\tgup_flags |= FOLL_WRITE;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn 0;\n\n\t/*\n\t * Disable interrupts.  We use the nested form as we can already have\n\t * interrupts disabled by get_futex_key.\n\t *\n\t * With interrupts disabled, we block page table pages from being\n\t * freed from under us. See struct mmu_table_batch comments in\n\t * include/asm-generic/tlb.h for more details.\n\t *\n\t * We do not adopt an rcu_read_lock(.) here as we also want to\n\t * block IPIs that come from THPs splitting.\n\t */\n\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_save(flags);\n\t\tgup_pgd_range(start, end, gup_flags, pages, &nr_pinned);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\treturn nr_pinned;\n}",
        "code_after_change": "int __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages)\n{\n\tunsigned long len, end;\n\tunsigned long flags;\n\tint nr_pinned = 0;\n\t/*\n\t * Internally (within mm/gup.c), gup fast variants must set FOLL_GET,\n\t * because gup fast is always a \"pin with a +1 page refcount\" request.\n\t */\n\tunsigned int gup_flags = FOLL_GET;\n\n\tif (write)\n\t\tgup_flags |= FOLL_WRITE;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn 0;\n\n\t/*\n\t * Disable interrupts.  We use the nested form as we can already have\n\t * interrupts disabled by get_futex_key.\n\t *\n\t * With interrupts disabled, we block page table pages from being\n\t * freed from under us. See struct mmu_table_batch comments in\n\t * include/asm-generic/tlb.h for more details.\n\t *\n\t * We do not adopt an rcu_read_lock(.) here as we also want to\n\t * block IPIs that come from THPs splitting.\n\t *\n\t * NOTE! We allow read-only gup_fast() here, but you'd better be\n\t * careful about possible COW pages. You'll get _a_ COW page, but\n\t * not necessarily the one you intended to get depending on what\n\t * COW event happens after this. COW may break the page copy in a\n\t * random direction.\n\t */\n\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_save(flags);\n\t\tgup_pgd_range(start, end, gup_flags, pages, &nr_pinned);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\treturn nr_pinned;\n}",
        "patch": "--- code before\n+++ code after\n@@ -32,6 +32,12 @@\n \t *\n \t * We do not adopt an rcu_read_lock(.) here as we also want to\n \t * block IPIs that come from THPs splitting.\n+\t *\n+\t * NOTE! We allow read-only gup_fast() here, but you'd better be\n+\t * careful about possible COW pages. You'll get _a_ COW page, but\n+\t * not necessarily the one you intended to get depending on what\n+\t * COW event happens after this. COW may break the page copy in a\n+\t * random direction.\n \t */\n \n \tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&",
        "function_modified_lines": {
            "added": [
                "\t *",
                "\t * NOTE! We allow read-only gup_fast() here, but you'd better be",
                "\t * careful about possible COW pages. You'll get _a_ COW page, but",
                "\t * not necessarily the one you intended to get depending on what",
                "\t * COW event happens after this. COW may break the page copy in a",
                "\t * random direction."
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-863"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58."
    },
    {
        "cve_id": "CVE-2020-29374",
        "code_before_change": "static int internal_get_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\t\t\tunsigned int gup_flags,\n\t\t\t\t\tstruct page **pages)\n{\n\tunsigned long addr, len, end;\n\tint nr_pinned = 0, ret = 0;\n\n\tif (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM |\n\t\t\t\t       FOLL_FORCE | FOLL_PIN | FOLL_GET)))\n\t\treturn -EINVAL;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\taddr = start;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn -EFAULT;\n\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_disable();\n\t\tgup_pgd_range(addr, end, gup_flags, pages, &nr_pinned);\n\t\tlocal_irq_enable();\n\t\tret = nr_pinned;\n\t}\n\n\tif (nr_pinned < nr_pages) {\n\t\t/* Try to get the remaining pages with get_user_pages */\n\t\tstart += nr_pinned << PAGE_SHIFT;\n\t\tpages += nr_pinned;\n\n\t\tret = __gup_longterm_unlocked(start, nr_pages - nr_pinned,\n\t\t\t\t\t      gup_flags, pages);\n\n\t\t/* Have to be a bit careful with return values */\n\t\tif (nr_pinned > 0) {\n\t\t\tif (ret < 0)\n\t\t\t\tret = nr_pinned;\n\t\t\telse\n\t\t\t\tret += nr_pinned;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int internal_get_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\t\t\tunsigned int gup_flags,\n\t\t\t\t\tstruct page **pages)\n{\n\tunsigned long addr, len, end;\n\tint nr_pinned = 0, ret = 0;\n\n\tif (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM |\n\t\t\t\t       FOLL_FORCE | FOLL_PIN | FOLL_GET)))\n\t\treturn -EINVAL;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\taddr = start;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * The FAST_GUP case requires FOLL_WRITE even for pure reads,\n\t * because get_user_pages() may need to cause an early COW in\n\t * order to avoid confusing the normal COW routines. So only\n\t * targets that are already writable are safe to do by just\n\t * looking at the page tables.\n\t */\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_disable();\n\t\tgup_pgd_range(addr, end, gup_flags | FOLL_WRITE, pages, &nr_pinned);\n\t\tlocal_irq_enable();\n\t\tret = nr_pinned;\n\t}\n\n\tif (nr_pinned < nr_pages) {\n\t\t/* Try to get the remaining pages with get_user_pages */\n\t\tstart += nr_pinned << PAGE_SHIFT;\n\t\tpages += nr_pinned;\n\n\t\tret = __gup_longterm_unlocked(start, nr_pages - nr_pinned,\n\t\t\t\t\t      gup_flags, pages);\n\n\t\t/* Have to be a bit careful with return values */\n\t\tif (nr_pinned > 0) {\n\t\t\tif (ret < 0)\n\t\t\t\tret = nr_pinned;\n\t\t\telse\n\t\t\t\tret += nr_pinned;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,10 +19,17 @@\n \tif (unlikely(!access_ok((void __user *)start, len)))\n \t\treturn -EFAULT;\n \n+\t/*\n+\t * The FAST_GUP case requires FOLL_WRITE even for pure reads,\n+\t * because get_user_pages() may need to cause an early COW in\n+\t * order to avoid confusing the normal COW routines. So only\n+\t * targets that are already writable are safe to do by just\n+\t * looking at the page tables.\n+\t */\n \tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n \t    gup_fast_permitted(start, end)) {\n \t\tlocal_irq_disable();\n-\t\tgup_pgd_range(addr, end, gup_flags, pages, &nr_pinned);\n+\t\tgup_pgd_range(addr, end, gup_flags | FOLL_WRITE, pages, &nr_pinned);\n \t\tlocal_irq_enable();\n \t\tret = nr_pinned;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * The FAST_GUP case requires FOLL_WRITE even for pure reads,",
                "\t * because get_user_pages() may need to cause an early COW in",
                "\t * order to avoid confusing the normal COW routines. So only",
                "\t * targets that are already writable are safe to do by just",
                "\t * looking at the page tables.",
                "\t */",
                "\t\tgup_pgd_range(addr, end, gup_flags | FOLL_WRITE, pages, &nr_pinned);"
            ],
            "deleted": [
                "\t\tgup_pgd_range(addr, end, gup_flags, pages, &nr_pinned);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-863"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58."
    },
    {
        "cve_id": "CVE-2020-29374",
        "code_before_change": "static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)\n{\n\treturn pmd_write(pmd) ||\n\t       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));\n}",
        "code_after_change": "static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)\n{\n\treturn pmd_write(pmd) || ((flags & FOLL_COW) && pmd_dirty(pmd));\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,4 @@\n static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)\n {\n-\treturn pmd_write(pmd) ||\n-\t       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));\n+\treturn pmd_write(pmd) || ((flags & FOLL_COW) && pmd_dirty(pmd));\n }",
        "function_modified_lines": {
            "added": [
                "\treturn pmd_write(pmd) || ((flags & FOLL_COW) && pmd_dirty(pmd));"
            ],
            "deleted": [
                "\treturn pmd_write(pmd) ||",
                "\t       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-863"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58."
    },
    {
        "cve_id": "CVE-2020-36557",
        "code_before_change": "static int con_install(struct tty_driver *driver, struct tty_struct *tty)\n{\n\tunsigned int currcons = tty->index;\n\tstruct vc_data *vc;\n\tint ret;\n\n\tconsole_lock();\n\tret = vc_allocate(currcons);\n\tif (ret)\n\t\tgoto unlock;\n\n\tvc = vc_cons[currcons].d;\n\n\t/* Still being freed */\n\tif (vc->port.tty) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto unlock;\n\t}\n\n\tret = tty_port_install(&vc->port, driver, tty);\n\tif (ret)\n\t\tgoto unlock;\n\n\ttty->driver_data = vc;\n\tvc->port.tty = tty;\n\n\tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n\t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;\n\t\ttty->winsize.ws_col = vc_cons[currcons].d->vc_cols;\n\t}\n\tif (vc->vc_utf)\n\t\ttty->termios.c_iflag |= IUTF8;\n\telse\n\t\ttty->termios.c_iflag &= ~IUTF8;\nunlock:\n\tconsole_unlock();\n\treturn ret;\n}",
        "code_after_change": "static int con_install(struct tty_driver *driver, struct tty_struct *tty)\n{\n\tunsigned int currcons = tty->index;\n\tstruct vc_data *vc;\n\tint ret;\n\n\tconsole_lock();\n\tret = vc_allocate(currcons);\n\tif (ret)\n\t\tgoto unlock;\n\n\tvc = vc_cons[currcons].d;\n\n\t/* Still being freed */\n\tif (vc->port.tty) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto unlock;\n\t}\n\n\tret = tty_port_install(&vc->port, driver, tty);\n\tif (ret)\n\t\tgoto unlock;\n\n\ttty->driver_data = vc;\n\tvc->port.tty = tty;\n\ttty_port_get(&vc->port);\n\n\tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n\t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;\n\t\ttty->winsize.ws_col = vc_cons[currcons].d->vc_cols;\n\t}\n\tif (vc->vc_utf)\n\t\ttty->termios.c_iflag |= IUTF8;\n\telse\n\t\ttty->termios.c_iflag &= ~IUTF8;\nunlock:\n\tconsole_unlock();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,6 +23,7 @@\n \n \ttty->driver_data = vc;\n \tvc->port.tty = tty;\n+\ttty_port_get(&vc->port);\n \n \tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n \t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;",
        "function_modified_lines": {
            "added": [
                "\ttty_port_get(&vc->port);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free."
    },
    {
        "cve_id": "CVE-2020-36557",
        "code_before_change": "int vc_allocate(unsigned int currcons)\t/* return 0 on success */\n{\n\tstruct vt_notifier_param param;\n\tstruct vc_data *vc;\n\n\tWARN_CONSOLE_UNLOCKED();\n\n\tif (currcons >= MAX_NR_CONSOLES)\n\t\treturn -ENXIO;\n\n\tif (vc_cons[currcons].d)\n\t\treturn 0;\n\n\t/* due to the granularity of kmalloc, we waste some memory here */\n\t/* the alloc is done in two steps, to optimize the common situation\n\t   of a 25x80 console (structsize=216, screenbuf_size=4000) */\n\t/* although the numbers above are not valid since long ago, the\n\t   point is still up-to-date and the comment still has its value\n\t   even if only as a historical artifact.  --mj, July 1998 */\n\tparam.vc = vc = kzalloc(sizeof(struct vc_data), GFP_KERNEL);\n\tif (!vc)\n\t\treturn -ENOMEM;\n\n\tvc_cons[currcons].d = vc;\n\ttty_port_init(&vc->port);\n\tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n\n\tvisual_init(vc, currcons, 1);\n\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(vc);\n\n\tvc->vc_screenbuf = kzalloc(vc->vc_screenbuf_size, GFP_KERNEL);\n\tif (!vc->vc_screenbuf)\n\t\tgoto err_free;\n\n\t/* If no drivers have overridden us and the user didn't pass a\n\t   boot option, default to displaying the cursor */\n\tif (global_cursor_default == -1)\n\t\tglobal_cursor_default = 1;\n\n\tvc_init(vc, vc->vc_rows, vc->vc_cols, 1);\n\tvcs_make_sysfs(currcons);\n\tatomic_notifier_call_chain(&vt_notifier_list, VT_ALLOCATE, &param);\n\n\treturn 0;\nerr_free:\n\tvisual_deinit(vc);\n\tkfree(vc);\n\tvc_cons[currcons].d = NULL;\n\treturn -ENOMEM;\n}",
        "code_after_change": "int vc_allocate(unsigned int currcons)\t/* return 0 on success */\n{\n\tstruct vt_notifier_param param;\n\tstruct vc_data *vc;\n\n\tWARN_CONSOLE_UNLOCKED();\n\n\tif (currcons >= MAX_NR_CONSOLES)\n\t\treturn -ENXIO;\n\n\tif (vc_cons[currcons].d)\n\t\treturn 0;\n\n\t/* due to the granularity of kmalloc, we waste some memory here */\n\t/* the alloc is done in two steps, to optimize the common situation\n\t   of a 25x80 console (structsize=216, screenbuf_size=4000) */\n\t/* although the numbers above are not valid since long ago, the\n\t   point is still up-to-date and the comment still has its value\n\t   even if only as a historical artifact.  --mj, July 1998 */\n\tparam.vc = vc = kzalloc(sizeof(struct vc_data), GFP_KERNEL);\n\tif (!vc)\n\t\treturn -ENOMEM;\n\n\tvc_cons[currcons].d = vc;\n\ttty_port_init(&vc->port);\n\tvc->port.ops = &vc_port_ops;\n\tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n\n\tvisual_init(vc, currcons, 1);\n\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(vc);\n\n\tvc->vc_screenbuf = kzalloc(vc->vc_screenbuf_size, GFP_KERNEL);\n\tif (!vc->vc_screenbuf)\n\t\tgoto err_free;\n\n\t/* If no drivers have overridden us and the user didn't pass a\n\t   boot option, default to displaying the cursor */\n\tif (global_cursor_default == -1)\n\t\tglobal_cursor_default = 1;\n\n\tvc_init(vc, vc->vc_rows, vc->vc_cols, 1);\n\tvcs_make_sysfs(currcons);\n\tatomic_notifier_call_chain(&vt_notifier_list, VT_ALLOCATE, &param);\n\n\treturn 0;\nerr_free:\n\tvisual_deinit(vc);\n\tkfree(vc);\n\tvc_cons[currcons].d = NULL;\n\treturn -ENOMEM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,6 +23,7 @@\n \n \tvc_cons[currcons].d = vc;\n \ttty_port_init(&vc->port);\n+\tvc->port.ops = &vc_port_ops;\n \tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n \n \tvisual_init(vc, currcons, 1);",
        "function_modified_lines": {
            "added": [
                "\tvc->port.ops = &vc_port_ops;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free."
    },
    {
        "cve_id": "CVE-2020-36557",
        "code_before_change": "static int vt_disallocate(unsigned int vc_num)\n{\n\tstruct vc_data *vc = NULL;\n\tint ret = 0;\n\n\tconsole_lock();\n\tif (vt_busy(vc_num))\n\t\tret = -EBUSY;\n\telse if (vc_num)\n\t\tvc = vc_deallocate(vc_num);\n\tconsole_unlock();\n\n\tif (vc && vc_num >= MIN_NR_CONSOLES) {\n\t\ttty_port_destroy(&vc->port);\n\t\tkfree(vc);\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int vt_disallocate(unsigned int vc_num)\n{\n\tstruct vc_data *vc = NULL;\n\tint ret = 0;\n\n\tconsole_lock();\n\tif (vt_busy(vc_num))\n\t\tret = -EBUSY;\n\telse if (vc_num)\n\t\tvc = vc_deallocate(vc_num);\n\tconsole_unlock();\n\n\tif (vc && vc_num >= MIN_NR_CONSOLES)\n\t\ttty_port_put(&vc->port);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,10 +10,8 @@\n \t\tvc = vc_deallocate(vc_num);\n \tconsole_unlock();\n \n-\tif (vc && vc_num >= MIN_NR_CONSOLES) {\n-\t\ttty_port_destroy(&vc->port);\n-\t\tkfree(vc);\n-\t}\n+\tif (vc && vc_num >= MIN_NR_CONSOLES)\n+\t\ttty_port_put(&vc->port);\n \n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (vc && vc_num >= MIN_NR_CONSOLES)",
                "\t\ttty_port_put(&vc->port);"
            ],
            "deleted": [
                "\tif (vc && vc_num >= MIN_NR_CONSOLES) {",
                "\t\ttty_port_destroy(&vc->port);",
                "\t\tkfree(vc);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free."
    },
    {
        "cve_id": "CVE-2020-36557",
        "code_before_change": "static void vt_disallocate_all(void)\n{\n\tstruct vc_data *vc[MAX_NR_CONSOLES];\n\tint i;\n\n\tconsole_lock();\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++)\n\t\tif (!vt_busy(i))\n\t\t\tvc[i] = vc_deallocate(i);\n\t\telse\n\t\t\tvc[i] = NULL;\n\tconsole_unlock();\n\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++) {\n\t\tif (vc[i] && i >= MIN_NR_CONSOLES) {\n\t\t\ttty_port_destroy(&vc[i]->port);\n\t\t\tkfree(vc[i]);\n\t\t}\n\t}\n}",
        "code_after_change": "static void vt_disallocate_all(void)\n{\n\tstruct vc_data *vc[MAX_NR_CONSOLES];\n\tint i;\n\n\tconsole_lock();\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++)\n\t\tif (!vt_busy(i))\n\t\t\tvc[i] = vc_deallocate(i);\n\t\telse\n\t\t\tvc[i] = NULL;\n\tconsole_unlock();\n\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++) {\n\t\tif (vc[i] && i >= MIN_NR_CONSOLES)\n\t\t\ttty_port_put(&vc[i]->port);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,9 +12,7 @@\n \tconsole_unlock();\n \n \tfor (i = 1; i < MAX_NR_CONSOLES; i++) {\n-\t\tif (vc[i] && i >= MIN_NR_CONSOLES) {\n-\t\t\ttty_port_destroy(&vc[i]->port);\n-\t\t\tkfree(vc[i]);\n-\t\t}\n+\t\tif (vc[i] && i >= MIN_NR_CONSOLES)\n+\t\t\ttty_port_put(&vc[i]->port);\n \t}\n }",
        "function_modified_lines": {
            "added": [
                "\t\tif (vc[i] && i >= MIN_NR_CONSOLES)",
                "\t\t\ttty_port_put(&vc[i]->port);"
            ],
            "deleted": [
                "\t\tif (vc[i] && i >= MIN_NR_CONSOLES) {",
                "\t\t\ttty_port_destroy(&vc[i]->port);",
                "\t\t\tkfree(vc[i]);",
                "\t\t}"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free."
    },
    {
        "cve_id": "CVE-2020-36558",
        "code_before_change": "int vt_ioctl(struct tty_struct *tty,\n\t     unsigned int cmd, unsigned long arg)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tstruct console_font_op op;\t/* used in multiple places here */\n\tunsigned int console;\n\tunsigned char ucval;\n\tunsigned int uival;\n\tvoid __user *up = (void __user *)arg;\n\tint i, perm;\n\tint ret = 0;\n\n\tconsole = vc->vc_num;\n\n\n\tif (!vc_cons_allocated(console)) { \t/* impossible? */\n\t\tret = -ENOIOCTLCMD;\n\t\tgoto out;\n\t}\n\n\n\t/*\n\t * To have permissions to do most of the vt ioctls, we either have\n\t * to be the owner of the tty, or have CAP_SYS_TTY_CONFIG.\n\t */\n\tperm = 0;\n\tif (current->signal->tty == tty || capable(CAP_SYS_TTY_CONFIG))\n\t\tperm = 1;\n \n\tswitch (cmd) {\n\tcase TIOCLINUX:\n\t\tret = tioclinux(tty, arg);\n\t\tbreak;\n\tcase KIOCSOUND:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\t/*\n\t\t * The use of PIT_TICK_RATE is historic, it used to be\n\t\t * the platform-dependent CLOCK_TICK_RATE between 2.6.12\n\t\t * and 2.6.36, which was a minor but unfortunate ABI\n\t\t * change. kd_mksound is locked by the input layer.\n\t\t */\n\t\tif (arg)\n\t\t\targ = PIT_TICK_RATE / arg;\n\t\tkd_mksound(arg, 0);\n\t\tbreak;\n\n\tcase KDMKTONE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t{\n\t\tunsigned int ticks, count;\n\t\t\n\t\t/*\n\t\t * Generate the tone for the appropriate number of ticks.\n\t\t * If the time is zero, turn off sound ourselves.\n\t\t */\n\t\tticks = msecs_to_jiffies((arg >> 16) & 0xffff);\n\t\tcount = ticks ? (arg & 0xffff) : 0;\n\t\tif (count)\n\t\t\tcount = PIT_TICK_RATE / count;\n\t\tkd_mksound(count, ticks);\n\t\tbreak;\n\t}\n\n\tcase KDGKBTYPE:\n\t\t/*\n\t\t * this is na\u00efve.\n\t\t */\n\t\tucval = KB_101;\n\t\tret = put_user(ucval, (char __user *)arg);\n\t\tbreak;\n\n\t\t/*\n\t\t * These cannot be implemented on any machine that implements\n\t\t * ioperm() in user level (such as Alpha PCs) or not at all.\n\t\t *\n\t\t * XXX: you should never use these, just call ioperm directly..\n\t\t */\n#ifdef CONFIG_X86\n\tcase KDADDIO:\n\tcase KDDELIO:\n\t\t/*\n\t\t * KDADDIO and KDDELIO may be able to add ports beyond what\n\t\t * we reject here, but to be safe...\n\t\t *\n\t\t * These are locked internally via sys_ioperm\n\t\t */\n\t\tif (arg < GPFIRST || arg > GPLAST) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tret = ksys_ioperm(arg, 1, (cmd == KDADDIO)) ? -ENXIO : 0;\n\t\tbreak;\n\n\tcase KDENABIO:\n\tcase KDDISABIO:\n\t\tret = ksys_ioperm(GPFIRST, GPNUM,\n\t\t\t\t  (cmd == KDENABIO)) ? -ENXIO : 0;\n\t\tbreak;\n#endif\n\n\t/* Linux m68k/i386 interface for setting the keyboard delay/repeat rate */\n\t\t\n\tcase KDKBDREP:\n\t{\n\t\tstruct kbd_repeat kbrep;\n\t\t\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&kbrep, up, sizeof(struct kbd_repeat))) {\n\t\t\tret =  -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tret = kbd_rate(&kbrep);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &kbrep, sizeof(struct kbd_repeat)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase KDSETMODE:\n\t\t/*\n\t\t * currently, setting the mode from KD_TEXT to KD_GRAPHICS\n\t\t * doesn't do a whole lot. i'm not sure if it should do any\n\t\t * restoration of modes or what...\n\t\t *\n\t\t * XXX It should at least call into the driver, fbdev's definitely\n\t\t * need to restore their engine state. --BenH\n\t\t */\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tswitch (arg) {\n\t\tcase KD_GRAPHICS:\n\t\t\tbreak;\n\t\tcase KD_TEXT0:\n\t\tcase KD_TEXT1:\n\t\t\targ = KD_TEXT;\n\t\tcase KD_TEXT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\t/* FIXME: this needs the console lock extending */\n\t\tif (vc->vc_mode == (unsigned char) arg)\n\t\t\tbreak;\n\t\tvc->vc_mode = (unsigned char) arg;\n\t\tif (console != fg_console)\n\t\t\tbreak;\n\t\t/*\n\t\t * explicitly blank/unblank the screen if switching modes\n\t\t */\n\t\tconsole_lock();\n\t\tif (arg == KD_TEXT)\n\t\t\tdo_unblank_screen(1);\n\t\telse\n\t\t\tdo_blank_screen(1);\n\t\tconsole_unlock();\n\t\tbreak;\n\n\tcase KDGETMODE:\n\t\tuival = vc->vc_mode;\n\t\tgoto setint;\n\n\tcase KDMAPDISP:\n\tcase KDUNMAPDISP:\n\t\t/*\n\t\t * these work like a combination of mmap and KDENABIO.\n\t\t * this could be easily finished.\n\t\t */\n\t\tret = -EINVAL;\n\t\tbreak;\n\n\tcase KDSKBMODE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tret = vt_do_kdskbmode(console, arg);\n\t\tif (ret == 0)\n\t\t\ttty_ldisc_flush(tty);\n\t\tbreak;\n\n\tcase KDGKBMODE:\n\t\tuival = vt_do_kdgkbmode(console);\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\t/* this could be folded into KDSKBMODE, but for compatibility\n\t   reasons it is not so easy to fold KDGKBMETA into KDGKBMODE */\n\tcase KDSKBMETA:\n\t\tret = vt_do_kdskbmeta(console, arg);\n\t\tbreak;\n\n\tcase KDGKBMETA:\n\t\t/* FIXME: should review whether this is worth locking */\n\t\tuival = vt_do_kdgkbmeta(console);\n\tsetint:\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\tcase KDGETKEYCODE:\n\tcase KDSETKEYCODE:\n\t\tif(!capable(CAP_SYS_TTY_CONFIG))\n\t\t\tperm = 0;\n\t\tret = vt_do_kbkeycode_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\tcase KDGKBENT:\n\tcase KDSKBENT:\n\t\tret = vt_do_kdsk_ioctl(cmd, up, perm, console);\n\t\tbreak;\n\n\tcase KDGKBSENT:\n\tcase KDSKBSENT:\n\t\tret = vt_do_kdgkb_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\t/* Diacritical processing. Handled in keyboard.c as it has\n\t   to operate on the keyboard locks and structures */\n\tcase KDGKBDIACR:\n\tcase KDGKBDIACRUC:\n\tcase KDSKBDIACR:\n\tcase KDSKBDIACRUC:\n\t\tret = vt_do_diacrit(cmd, up, perm);\n\t\tbreak;\n\n\t/* the ioctls below read/set the flags usually shown in the leds */\n\t/* don't use them - they will go away without warning */\n\tcase KDGKBLED:\n\tcase KDSKBLED:\n\tcase KDGETLED:\n\tcase KDSETLED:\n\t\tret = vt_do_kdskled(console, cmd, arg, perm);\n\t\tbreak;\n\n\t/*\n\t * A process can indicate its willingness to accept signals\n\t * generated by pressing an appropriate key combination.\n\t * Thus, one can have a daemon that e.g. spawns a new console\n\t * upon a keypress and then changes to it.\n\t * See also the kbrequest field of inittab(5).\n\t */\n\tcase KDSIGACCEPT:\n\t{\n\t\tif (!perm || !capable(CAP_KILL))\n\t\t\treturn -EPERM;\n\t\tif (!valid_signal(arg) || arg < 1 || arg == SIGKILL)\n\t\t\tret = -EINVAL;\n\t\telse {\n\t\t\tspin_lock_irq(&vt_spawn_con.lock);\n\t\t\tput_pid(vt_spawn_con.pid);\n\t\t\tvt_spawn_con.pid = get_pid(task_pid(current));\n\t\t\tvt_spawn_con.sig = arg;\n\t\t\tspin_unlock_irq(&vt_spawn_con.lock);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_SETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&tmp, up, sizeof(struct vt_mode))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (tmp.mode != VT_AUTO && tmp.mode != VT_PROCESS) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tconsole_lock();\n\t\tvc->vt_mode = tmp;\n\t\t/* the frsig is ignored, so we set it to 0 */\n\t\tvc->vt_mode.frsig = 0;\n\t\tput_pid(vc->vt_pid);\n\t\tvc->vt_pid = get_pid(task_pid(current));\n\t\t/* no switch is required -- saw@shade.msu.ru */\n\t\tvc->vt_newvt = -1;\n\t\tconsole_unlock();\n\t\tbreak;\n\t}\n\n\tcase VT_GETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\t\tint rc;\n\n\t\tconsole_lock();\n\t\tmemcpy(&tmp, &vc->vt_mode, sizeof(struct vt_mode));\n\t\tconsole_unlock();\n\n\t\trc = copy_to_user(up, &tmp, sizeof(struct vt_mode));\n\t\tif (rc)\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns global vt state. Note that VT 0 is always open, since\n\t * it's an alias for the current VT, and people can't use it here.\n\t * We cannot return state for more than 16 VTs, since v_state is short.\n\t */\n\tcase VT_GETSTATE:\n\t{\n\t\tstruct vt_stat __user *vtstat = up;\n\t\tunsigned short state, mask;\n\n\t\t/* Review: FIXME: Console lock ? */\n\t\tif (put_user(fg_console + 1, &vtstat->v_active))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tstate = 1;\t/* /dev/tty0 is always open */\n\t\t\tfor (i = 0, mask = 2; i < MAX_NR_CONSOLES && mask;\n\t\t\t\t\t\t\t++i, mask <<= 1)\n\t\t\t\tif (VT_IS_IN_USE(i))\n\t\t\t\t\tstate |= mask;\n\t\t\tret = put_user(state, &vtstat->v_state);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns the first available (non-opened) console.\n\t */\n\tcase VT_OPENQRY:\n\t\t/* FIXME: locking ? - but then this is a stupid API */\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; ++i)\n\t\t\tif (! VT_IS_IN_USE(i))\n\t\t\t\tbreak;\n\t\tuival = i < MAX_NR_CONSOLES ? (i+1) : -1;\n\t\tgoto setint;\t\t \n\n\t/*\n\t * ioctl(fd, VT_ACTIVATE, num) will cause us to switch to vt # num,\n\t * with num >= 1 (switches to vt 0, our console, are not allowed, just\n\t * to preserve sanity).\n\t */\n\tcase VT_ACTIVATE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret =  -ENXIO;\n\t\telse {\n\t\t\targ--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(arg);\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tset_console(arg);\n\t\t}\n\t\tbreak;\n\n\tcase VT_SETACTIVATE:\n\t{\n\t\tstruct vt_setactivate vsa;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&vsa, (struct vt_setactivate __user *)arg,\n\t\t\t\t\tsizeof(struct vt_setactivate))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (vsa.console == 0 || vsa.console > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse {\n\t\t\tvsa.console = array_index_nospec(vsa.console,\n\t\t\t\t\t\t\t MAX_NR_CONSOLES + 1);\n\t\t\tvsa.console--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(vsa.console);\n\t\t\tif (ret == 0) {\n\t\t\t\tstruct vc_data *nvc;\n\t\t\t\t/* This is safe providing we don't drop the\n\t\t\t\t   console sem between vc_allocate and\n\t\t\t\t   finishing referencing nvc */\n\t\t\t\tnvc = vc_cons[vsa.console].d;\n\t\t\t\tnvc->vt_mode = vsa.mode;\n\t\t\t\tnvc->vt_mode.frsig = 0;\n\t\t\t\tput_pid(nvc->vt_pid);\n\t\t\t\tnvc->vt_pid = get_pid(task_pid(current));\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\t/* Commence switch and lock */\n\t\t\t/* Review set_console locks */\n\t\t\tset_console(vsa.console);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * wait until the specified VT has been activated\n\t */\n\tcase VT_WAITACTIVE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse\n\t\t\tret = vt_waitactive(arg);\n\t\tbreak;\n\n\t/*\n\t * If a vt is under process control, the kernel will not switch to it\n\t * immediately, but postpone the operation until the process calls this\n\t * ioctl, allowing the switch to complete.\n\t *\n\t * According to the X sources this is the behavior:\n\t *\t0:\tpending switch-from not OK\n\t *\t1:\tpending switch-from OK\n\t *\t2:\tcompleted switch-to OK\n\t */\n\tcase VT_RELDISP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tconsole_lock();\n\t\tif (vc->vt_mode.mode != VT_PROCESS) {\n\t\t\tconsole_unlock();\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\t/*\n\t\t * Switching-from response\n\t\t */\n\t\tif (vc->vt_newvt >= 0) {\n\t\t\tif (arg == 0)\n\t\t\t\t/*\n\t\t\t\t * Switch disallowed, so forget we were trying\n\t\t\t\t * to do it.\n\t\t\t\t */\n\t\t\t\tvc->vt_newvt = -1;\n\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * The current vt has been released, so\n\t\t\t\t * complete the switch.\n\t\t\t\t */\n\t\t\t\tint newvt;\n\t\t\t\tnewvt = vc->vt_newvt;\n\t\t\t\tvc->vt_newvt = -1;\n\t\t\t\tret = vc_allocate(newvt);\n\t\t\t\tif (ret) {\n\t\t\t\t\tconsole_unlock();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/*\n\t\t\t\t * When we actually do the console switch,\n\t\t\t\t * make sure we are atomic with respect to\n\t\t\t\t * other console switches..\n\t\t\t\t */\n\t\t\t\tcomplete_change_console(vc_cons[newvt].d);\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * Switched-to response\n\t\t\t */\n\t\t\t/*\n\t\t\t * If it's just an ACK, ignore it\n\t\t\t */\n\t\t\tif (arg != VT_ACKACQ)\n\t\t\t\tret = -EINVAL;\n\t\t}\n\t\tconsole_unlock();\n\t\tbreak;\n\n\t /*\n\t  * Disallocate memory associated to VT (but leave VT1)\n\t  */\n\t case VT_DISALLOCATE:\n\t\tif (arg > MAX_NR_CONSOLES) {\n\t\t\tret = -ENXIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (arg == 0)\n\t\t\tvt_disallocate_all();\n\t\telse\n\t\t\tret = vt_disallocate(--arg);\n\t\tbreak;\n\n\tcase VT_RESIZE:\n\t{\n\t\tstruct vt_sizes __user *vtsizes = up;\n\t\tstruct vc_data *vc;\n\n\t\tushort ll,cc;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (get_user(ll, &vtsizes->v_rows) ||\n\t\t    get_user(cc, &vtsizes->v_cols))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tconsole_lock();\n\t\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\t\tvc = vc_cons[i].d;\n\n\t\t\t\tif (vc) {\n\t\t\t\t\tvc->vc_resize_user = 1;\n\t\t\t\t\t/* FIXME: review v tty lock */\n\t\t\t\t\tvc_resize(vc_cons[i].d, cc, ll);\n\t\t\t\t}\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_RESIZEX:\n\t{\n\t\tstruct vt_consize v;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&v, up, sizeof(struct vt_consize)))\n\t\t\treturn -EFAULT;\n\t\t/* FIXME: Should check the copies properly */\n\t\tif (!v.v_vlin)\n\t\t\tv.v_vlin = vc->vc_scan_lines;\n\t\tif (v.v_clin) {\n\t\t\tint rows = v.v_vlin/v.v_clin;\n\t\t\tif (v.v_rows != rows) {\n\t\t\t\tif (v.v_rows) /* Parameters don't add up */\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_rows = rows;\n\t\t\t}\n\t\t}\n\t\tif (v.v_vcol && v.v_ccol) {\n\t\t\tint cols = v.v_vcol/v.v_ccol;\n\t\t\tif (v.v_cols != cols) {\n\t\t\t\tif (v.v_cols)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_cols = cols;\n\t\t\t}\n\t\t}\n\n\t\tif (v.v_clin > 32)\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\tif (!vc_cons[i].d)\n\t\t\t\tcontinue;\n\t\t\tconsole_lock();\n\t\t\tif (v.v_vlin)\n\t\t\t\tvc_cons[i].d->vc_scan_lines = v.v_vlin;\n\t\t\tif (v.v_clin)\n\t\t\t\tvc_cons[i].d->vc_font.height = v.v_clin;\n\t\t\tvc_cons[i].d->vc_resize_user = 1;\n\t\t\tvc_resize(vc_cons[i].d, v.v_cols, v.v_rows);\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase PIO_FONT: {\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\top.op = KD_FONT_OP_SET;\n\t\top.flags = KD_FONT_FLAG_OLD | KD_FONT_FLAG_DONT_RECALC;\t/* Compatibility */\n\t\top.width = 8;\n\t\top.height = 0;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase GIO_FONT: {\n\t\top.op = KD_FONT_OP_GET;\n\t\top.flags = KD_FONT_FLAG_OLD;\n\t\top.width = 8;\n\t\top.height = 32;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase PIO_CMAP:\n                if (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t                ret = con_set_cmap(up);\n\t\tbreak;\n\n\tcase GIO_CMAP:\n                ret = con_get_cmap(up);\n\t\tbreak;\n\n\tcase PIO_FONTX:\n\tcase GIO_FONTX:\n\t\tret = do_fontx_ioctl(cmd, up, perm, &op);\n\t\tbreak;\n\n\tcase PIO_FONTRESET:\n\t{\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n#ifdef BROKEN_GRAPHICS_PROGRAMS\n\t\t/* With BROKEN_GRAPHICS_PROGRAMS defined, the default\n\t\t   font is not saved. */\n\t\tret = -ENOSYS;\n\t\tbreak;\n#else\n\t\t{\n\t\top.op = KD_FONT_OP_SET_DEFAULT;\n\t\top.data = NULL;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tconsole_lock();\n\t\tcon_set_default_unimap(vc_cons[fg_console].d);\n\t\tconsole_unlock();\n\t\tbreak;\n\t\t}\n#endif\n\t}\n\n\tcase KDFONTOP: {\n\t\tif (copy_from_user(&op, up, sizeof(op))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!perm && op.op != KD_FONT_OP_GET)\n\t\t\treturn -EPERM;\n\t\tret = con_font_op(vc, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &op, sizeof(op)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase PIO_SCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_old(up);\n\t\tbreak;\n\n\tcase GIO_SCRNMAP:\n\t\tret = con_get_trans_old(up);\n\t\tbreak;\n\n\tcase PIO_UNISCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_new(up);\n\t\tbreak;\n\n\tcase GIO_UNISCRNMAP:\n\t\tret = con_get_trans_new(up);\n\t\tbreak;\n\n\tcase PIO_UNIMAPCLR:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tcon_clear_unimap(vc);\n\t\tbreak;\n\n\tcase PIO_UNIMAP:\n\tcase GIO_UNIMAP:\n\t\tret = do_unimap_ioctl(cmd, up, perm, vc);\n\t\tbreak;\n\n\tcase VT_LOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 1;\n\t\tbreak;\n\tcase VT_UNLOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 0;\n\t\tbreak;\n\tcase VT_GETHIFONTMASK:\n\t\tret = put_user(vc->vc_hi_font_mask,\n\t\t\t\t\t(unsigned short __user *)arg);\n\t\tbreak;\n\tcase VT_WAITEVENT:\n\t\tret = vt_event_wait_ioctl((struct vt_event __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENOIOCTLCMD;\n\t}\nout:\n\treturn ret;\n}",
        "code_after_change": "int vt_ioctl(struct tty_struct *tty,\n\t     unsigned int cmd, unsigned long arg)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tstruct console_font_op op;\t/* used in multiple places here */\n\tunsigned int console;\n\tunsigned char ucval;\n\tunsigned int uival;\n\tvoid __user *up = (void __user *)arg;\n\tint i, perm;\n\tint ret = 0;\n\n\tconsole = vc->vc_num;\n\n\n\tif (!vc_cons_allocated(console)) { \t/* impossible? */\n\t\tret = -ENOIOCTLCMD;\n\t\tgoto out;\n\t}\n\n\n\t/*\n\t * To have permissions to do most of the vt ioctls, we either have\n\t * to be the owner of the tty, or have CAP_SYS_TTY_CONFIG.\n\t */\n\tperm = 0;\n\tif (current->signal->tty == tty || capable(CAP_SYS_TTY_CONFIG))\n\t\tperm = 1;\n \n\tswitch (cmd) {\n\tcase TIOCLINUX:\n\t\tret = tioclinux(tty, arg);\n\t\tbreak;\n\tcase KIOCSOUND:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\t/*\n\t\t * The use of PIT_TICK_RATE is historic, it used to be\n\t\t * the platform-dependent CLOCK_TICK_RATE between 2.6.12\n\t\t * and 2.6.36, which was a minor but unfortunate ABI\n\t\t * change. kd_mksound is locked by the input layer.\n\t\t */\n\t\tif (arg)\n\t\t\targ = PIT_TICK_RATE / arg;\n\t\tkd_mksound(arg, 0);\n\t\tbreak;\n\n\tcase KDMKTONE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t{\n\t\tunsigned int ticks, count;\n\t\t\n\t\t/*\n\t\t * Generate the tone for the appropriate number of ticks.\n\t\t * If the time is zero, turn off sound ourselves.\n\t\t */\n\t\tticks = msecs_to_jiffies((arg >> 16) & 0xffff);\n\t\tcount = ticks ? (arg & 0xffff) : 0;\n\t\tif (count)\n\t\t\tcount = PIT_TICK_RATE / count;\n\t\tkd_mksound(count, ticks);\n\t\tbreak;\n\t}\n\n\tcase KDGKBTYPE:\n\t\t/*\n\t\t * this is na\u00efve.\n\t\t */\n\t\tucval = KB_101;\n\t\tret = put_user(ucval, (char __user *)arg);\n\t\tbreak;\n\n\t\t/*\n\t\t * These cannot be implemented on any machine that implements\n\t\t * ioperm() in user level (such as Alpha PCs) or not at all.\n\t\t *\n\t\t * XXX: you should never use these, just call ioperm directly..\n\t\t */\n#ifdef CONFIG_X86\n\tcase KDADDIO:\n\tcase KDDELIO:\n\t\t/*\n\t\t * KDADDIO and KDDELIO may be able to add ports beyond what\n\t\t * we reject here, but to be safe...\n\t\t *\n\t\t * These are locked internally via sys_ioperm\n\t\t */\n\t\tif (arg < GPFIRST || arg > GPLAST) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tret = ksys_ioperm(arg, 1, (cmd == KDADDIO)) ? -ENXIO : 0;\n\t\tbreak;\n\n\tcase KDENABIO:\n\tcase KDDISABIO:\n\t\tret = ksys_ioperm(GPFIRST, GPNUM,\n\t\t\t\t  (cmd == KDENABIO)) ? -ENXIO : 0;\n\t\tbreak;\n#endif\n\n\t/* Linux m68k/i386 interface for setting the keyboard delay/repeat rate */\n\t\t\n\tcase KDKBDREP:\n\t{\n\t\tstruct kbd_repeat kbrep;\n\t\t\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&kbrep, up, sizeof(struct kbd_repeat))) {\n\t\t\tret =  -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tret = kbd_rate(&kbrep);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &kbrep, sizeof(struct kbd_repeat)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase KDSETMODE:\n\t\t/*\n\t\t * currently, setting the mode from KD_TEXT to KD_GRAPHICS\n\t\t * doesn't do a whole lot. i'm not sure if it should do any\n\t\t * restoration of modes or what...\n\t\t *\n\t\t * XXX It should at least call into the driver, fbdev's definitely\n\t\t * need to restore their engine state. --BenH\n\t\t */\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tswitch (arg) {\n\t\tcase KD_GRAPHICS:\n\t\t\tbreak;\n\t\tcase KD_TEXT0:\n\t\tcase KD_TEXT1:\n\t\t\targ = KD_TEXT;\n\t\tcase KD_TEXT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\t/* FIXME: this needs the console lock extending */\n\t\tif (vc->vc_mode == (unsigned char) arg)\n\t\t\tbreak;\n\t\tvc->vc_mode = (unsigned char) arg;\n\t\tif (console != fg_console)\n\t\t\tbreak;\n\t\t/*\n\t\t * explicitly blank/unblank the screen if switching modes\n\t\t */\n\t\tconsole_lock();\n\t\tif (arg == KD_TEXT)\n\t\t\tdo_unblank_screen(1);\n\t\telse\n\t\t\tdo_blank_screen(1);\n\t\tconsole_unlock();\n\t\tbreak;\n\n\tcase KDGETMODE:\n\t\tuival = vc->vc_mode;\n\t\tgoto setint;\n\n\tcase KDMAPDISP:\n\tcase KDUNMAPDISP:\n\t\t/*\n\t\t * these work like a combination of mmap and KDENABIO.\n\t\t * this could be easily finished.\n\t\t */\n\t\tret = -EINVAL;\n\t\tbreak;\n\n\tcase KDSKBMODE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tret = vt_do_kdskbmode(console, arg);\n\t\tif (ret == 0)\n\t\t\ttty_ldisc_flush(tty);\n\t\tbreak;\n\n\tcase KDGKBMODE:\n\t\tuival = vt_do_kdgkbmode(console);\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\t/* this could be folded into KDSKBMODE, but for compatibility\n\t   reasons it is not so easy to fold KDGKBMETA into KDGKBMODE */\n\tcase KDSKBMETA:\n\t\tret = vt_do_kdskbmeta(console, arg);\n\t\tbreak;\n\n\tcase KDGKBMETA:\n\t\t/* FIXME: should review whether this is worth locking */\n\t\tuival = vt_do_kdgkbmeta(console);\n\tsetint:\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\tcase KDGETKEYCODE:\n\tcase KDSETKEYCODE:\n\t\tif(!capable(CAP_SYS_TTY_CONFIG))\n\t\t\tperm = 0;\n\t\tret = vt_do_kbkeycode_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\tcase KDGKBENT:\n\tcase KDSKBENT:\n\t\tret = vt_do_kdsk_ioctl(cmd, up, perm, console);\n\t\tbreak;\n\n\tcase KDGKBSENT:\n\tcase KDSKBSENT:\n\t\tret = vt_do_kdgkb_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\t/* Diacritical processing. Handled in keyboard.c as it has\n\t   to operate on the keyboard locks and structures */\n\tcase KDGKBDIACR:\n\tcase KDGKBDIACRUC:\n\tcase KDSKBDIACR:\n\tcase KDSKBDIACRUC:\n\t\tret = vt_do_diacrit(cmd, up, perm);\n\t\tbreak;\n\n\t/* the ioctls below read/set the flags usually shown in the leds */\n\t/* don't use them - they will go away without warning */\n\tcase KDGKBLED:\n\tcase KDSKBLED:\n\tcase KDGETLED:\n\tcase KDSETLED:\n\t\tret = vt_do_kdskled(console, cmd, arg, perm);\n\t\tbreak;\n\n\t/*\n\t * A process can indicate its willingness to accept signals\n\t * generated by pressing an appropriate key combination.\n\t * Thus, one can have a daemon that e.g. spawns a new console\n\t * upon a keypress and then changes to it.\n\t * See also the kbrequest field of inittab(5).\n\t */\n\tcase KDSIGACCEPT:\n\t{\n\t\tif (!perm || !capable(CAP_KILL))\n\t\t\treturn -EPERM;\n\t\tif (!valid_signal(arg) || arg < 1 || arg == SIGKILL)\n\t\t\tret = -EINVAL;\n\t\telse {\n\t\t\tspin_lock_irq(&vt_spawn_con.lock);\n\t\t\tput_pid(vt_spawn_con.pid);\n\t\t\tvt_spawn_con.pid = get_pid(task_pid(current));\n\t\t\tvt_spawn_con.sig = arg;\n\t\t\tspin_unlock_irq(&vt_spawn_con.lock);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_SETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&tmp, up, sizeof(struct vt_mode))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (tmp.mode != VT_AUTO && tmp.mode != VT_PROCESS) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tconsole_lock();\n\t\tvc->vt_mode = tmp;\n\t\t/* the frsig is ignored, so we set it to 0 */\n\t\tvc->vt_mode.frsig = 0;\n\t\tput_pid(vc->vt_pid);\n\t\tvc->vt_pid = get_pid(task_pid(current));\n\t\t/* no switch is required -- saw@shade.msu.ru */\n\t\tvc->vt_newvt = -1;\n\t\tconsole_unlock();\n\t\tbreak;\n\t}\n\n\tcase VT_GETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\t\tint rc;\n\n\t\tconsole_lock();\n\t\tmemcpy(&tmp, &vc->vt_mode, sizeof(struct vt_mode));\n\t\tconsole_unlock();\n\n\t\trc = copy_to_user(up, &tmp, sizeof(struct vt_mode));\n\t\tif (rc)\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns global vt state. Note that VT 0 is always open, since\n\t * it's an alias for the current VT, and people can't use it here.\n\t * We cannot return state for more than 16 VTs, since v_state is short.\n\t */\n\tcase VT_GETSTATE:\n\t{\n\t\tstruct vt_stat __user *vtstat = up;\n\t\tunsigned short state, mask;\n\n\t\t/* Review: FIXME: Console lock ? */\n\t\tif (put_user(fg_console + 1, &vtstat->v_active))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tstate = 1;\t/* /dev/tty0 is always open */\n\t\t\tfor (i = 0, mask = 2; i < MAX_NR_CONSOLES && mask;\n\t\t\t\t\t\t\t++i, mask <<= 1)\n\t\t\t\tif (VT_IS_IN_USE(i))\n\t\t\t\t\tstate |= mask;\n\t\t\tret = put_user(state, &vtstat->v_state);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns the first available (non-opened) console.\n\t */\n\tcase VT_OPENQRY:\n\t\t/* FIXME: locking ? - but then this is a stupid API */\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; ++i)\n\t\t\tif (! VT_IS_IN_USE(i))\n\t\t\t\tbreak;\n\t\tuival = i < MAX_NR_CONSOLES ? (i+1) : -1;\n\t\tgoto setint;\t\t \n\n\t/*\n\t * ioctl(fd, VT_ACTIVATE, num) will cause us to switch to vt # num,\n\t * with num >= 1 (switches to vt 0, our console, are not allowed, just\n\t * to preserve sanity).\n\t */\n\tcase VT_ACTIVATE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret =  -ENXIO;\n\t\telse {\n\t\t\targ--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(arg);\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tset_console(arg);\n\t\t}\n\t\tbreak;\n\n\tcase VT_SETACTIVATE:\n\t{\n\t\tstruct vt_setactivate vsa;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&vsa, (struct vt_setactivate __user *)arg,\n\t\t\t\t\tsizeof(struct vt_setactivate))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (vsa.console == 0 || vsa.console > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse {\n\t\t\tvsa.console = array_index_nospec(vsa.console,\n\t\t\t\t\t\t\t MAX_NR_CONSOLES + 1);\n\t\t\tvsa.console--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(vsa.console);\n\t\t\tif (ret == 0) {\n\t\t\t\tstruct vc_data *nvc;\n\t\t\t\t/* This is safe providing we don't drop the\n\t\t\t\t   console sem between vc_allocate and\n\t\t\t\t   finishing referencing nvc */\n\t\t\t\tnvc = vc_cons[vsa.console].d;\n\t\t\t\tnvc->vt_mode = vsa.mode;\n\t\t\t\tnvc->vt_mode.frsig = 0;\n\t\t\t\tput_pid(nvc->vt_pid);\n\t\t\t\tnvc->vt_pid = get_pid(task_pid(current));\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\t/* Commence switch and lock */\n\t\t\t/* Review set_console locks */\n\t\t\tset_console(vsa.console);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * wait until the specified VT has been activated\n\t */\n\tcase VT_WAITACTIVE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse\n\t\t\tret = vt_waitactive(arg);\n\t\tbreak;\n\n\t/*\n\t * If a vt is under process control, the kernel will not switch to it\n\t * immediately, but postpone the operation until the process calls this\n\t * ioctl, allowing the switch to complete.\n\t *\n\t * According to the X sources this is the behavior:\n\t *\t0:\tpending switch-from not OK\n\t *\t1:\tpending switch-from OK\n\t *\t2:\tcompleted switch-to OK\n\t */\n\tcase VT_RELDISP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tconsole_lock();\n\t\tif (vc->vt_mode.mode != VT_PROCESS) {\n\t\t\tconsole_unlock();\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\t/*\n\t\t * Switching-from response\n\t\t */\n\t\tif (vc->vt_newvt >= 0) {\n\t\t\tif (arg == 0)\n\t\t\t\t/*\n\t\t\t\t * Switch disallowed, so forget we were trying\n\t\t\t\t * to do it.\n\t\t\t\t */\n\t\t\t\tvc->vt_newvt = -1;\n\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * The current vt has been released, so\n\t\t\t\t * complete the switch.\n\t\t\t\t */\n\t\t\t\tint newvt;\n\t\t\t\tnewvt = vc->vt_newvt;\n\t\t\t\tvc->vt_newvt = -1;\n\t\t\t\tret = vc_allocate(newvt);\n\t\t\t\tif (ret) {\n\t\t\t\t\tconsole_unlock();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/*\n\t\t\t\t * When we actually do the console switch,\n\t\t\t\t * make sure we are atomic with respect to\n\t\t\t\t * other console switches..\n\t\t\t\t */\n\t\t\t\tcomplete_change_console(vc_cons[newvt].d);\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * Switched-to response\n\t\t\t */\n\t\t\t/*\n\t\t\t * If it's just an ACK, ignore it\n\t\t\t */\n\t\t\tif (arg != VT_ACKACQ)\n\t\t\t\tret = -EINVAL;\n\t\t}\n\t\tconsole_unlock();\n\t\tbreak;\n\n\t /*\n\t  * Disallocate memory associated to VT (but leave VT1)\n\t  */\n\t case VT_DISALLOCATE:\n\t\tif (arg > MAX_NR_CONSOLES) {\n\t\t\tret = -ENXIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (arg == 0)\n\t\t\tvt_disallocate_all();\n\t\telse\n\t\t\tret = vt_disallocate(--arg);\n\t\tbreak;\n\n\tcase VT_RESIZE:\n\t{\n\t\tstruct vt_sizes __user *vtsizes = up;\n\t\tstruct vc_data *vc;\n\n\t\tushort ll,cc;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (get_user(ll, &vtsizes->v_rows) ||\n\t\t    get_user(cc, &vtsizes->v_cols))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tconsole_lock();\n\t\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\t\tvc = vc_cons[i].d;\n\n\t\t\t\tif (vc) {\n\t\t\t\t\tvc->vc_resize_user = 1;\n\t\t\t\t\t/* FIXME: review v tty lock */\n\t\t\t\t\tvc_resize(vc_cons[i].d, cc, ll);\n\t\t\t\t}\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_RESIZEX:\n\t{\n\t\tstruct vt_consize v;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&v, up, sizeof(struct vt_consize)))\n\t\t\treturn -EFAULT;\n\t\t/* FIXME: Should check the copies properly */\n\t\tif (!v.v_vlin)\n\t\t\tv.v_vlin = vc->vc_scan_lines;\n\t\tif (v.v_clin) {\n\t\t\tint rows = v.v_vlin/v.v_clin;\n\t\t\tif (v.v_rows != rows) {\n\t\t\t\tif (v.v_rows) /* Parameters don't add up */\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_rows = rows;\n\t\t\t}\n\t\t}\n\t\tif (v.v_vcol && v.v_ccol) {\n\t\t\tint cols = v.v_vcol/v.v_ccol;\n\t\t\tif (v.v_cols != cols) {\n\t\t\t\tif (v.v_cols)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_cols = cols;\n\t\t\t}\n\t\t}\n\n\t\tif (v.v_clin > 32)\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\tstruct vc_data *vcp;\n\n\t\t\tif (!vc_cons[i].d)\n\t\t\t\tcontinue;\n\t\t\tconsole_lock();\n\t\t\tvcp = vc_cons[i].d;\n\t\t\tif (vcp) {\n\t\t\t\tif (v.v_vlin)\n\t\t\t\t\tvcp->vc_scan_lines = v.v_vlin;\n\t\t\t\tif (v.v_clin)\n\t\t\t\t\tvcp->vc_font.height = v.v_clin;\n\t\t\t\tvcp->vc_resize_user = 1;\n\t\t\t\tvc_resize(vcp, v.v_cols, v.v_rows);\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase PIO_FONT: {\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\top.op = KD_FONT_OP_SET;\n\t\top.flags = KD_FONT_FLAG_OLD | KD_FONT_FLAG_DONT_RECALC;\t/* Compatibility */\n\t\top.width = 8;\n\t\top.height = 0;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase GIO_FONT: {\n\t\top.op = KD_FONT_OP_GET;\n\t\top.flags = KD_FONT_FLAG_OLD;\n\t\top.width = 8;\n\t\top.height = 32;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase PIO_CMAP:\n                if (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t                ret = con_set_cmap(up);\n\t\tbreak;\n\n\tcase GIO_CMAP:\n                ret = con_get_cmap(up);\n\t\tbreak;\n\n\tcase PIO_FONTX:\n\tcase GIO_FONTX:\n\t\tret = do_fontx_ioctl(cmd, up, perm, &op);\n\t\tbreak;\n\n\tcase PIO_FONTRESET:\n\t{\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n#ifdef BROKEN_GRAPHICS_PROGRAMS\n\t\t/* With BROKEN_GRAPHICS_PROGRAMS defined, the default\n\t\t   font is not saved. */\n\t\tret = -ENOSYS;\n\t\tbreak;\n#else\n\t\t{\n\t\top.op = KD_FONT_OP_SET_DEFAULT;\n\t\top.data = NULL;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tconsole_lock();\n\t\tcon_set_default_unimap(vc_cons[fg_console].d);\n\t\tconsole_unlock();\n\t\tbreak;\n\t\t}\n#endif\n\t}\n\n\tcase KDFONTOP: {\n\t\tif (copy_from_user(&op, up, sizeof(op))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!perm && op.op != KD_FONT_OP_GET)\n\t\t\treturn -EPERM;\n\t\tret = con_font_op(vc, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &op, sizeof(op)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase PIO_SCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_old(up);\n\t\tbreak;\n\n\tcase GIO_SCRNMAP:\n\t\tret = con_get_trans_old(up);\n\t\tbreak;\n\n\tcase PIO_UNISCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_new(up);\n\t\tbreak;\n\n\tcase GIO_UNISCRNMAP:\n\t\tret = con_get_trans_new(up);\n\t\tbreak;\n\n\tcase PIO_UNIMAPCLR:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tcon_clear_unimap(vc);\n\t\tbreak;\n\n\tcase PIO_UNIMAP:\n\tcase GIO_UNIMAP:\n\t\tret = do_unimap_ioctl(cmd, up, perm, vc);\n\t\tbreak;\n\n\tcase VT_LOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 1;\n\t\tbreak;\n\tcase VT_UNLOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 0;\n\t\tbreak;\n\tcase VT_GETHIFONTMASK:\n\t\tret = put_user(vc->vc_hi_font_mask,\n\t\t\t\t\t(unsigned short __user *)arg);\n\t\tbreak;\n\tcase VT_WAITEVENT:\n\t\tret = vt_event_wait_ioctl((struct vt_event __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENOIOCTLCMD;\n\t}\nout:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -544,15 +544,20 @@\n \t\t\treturn -EINVAL;\n \n \t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n+\t\t\tstruct vc_data *vcp;\n+\n \t\t\tif (!vc_cons[i].d)\n \t\t\t\tcontinue;\n \t\t\tconsole_lock();\n-\t\t\tif (v.v_vlin)\n-\t\t\t\tvc_cons[i].d->vc_scan_lines = v.v_vlin;\n-\t\t\tif (v.v_clin)\n-\t\t\t\tvc_cons[i].d->vc_font.height = v.v_clin;\n-\t\t\tvc_cons[i].d->vc_resize_user = 1;\n-\t\t\tvc_resize(vc_cons[i].d, v.v_cols, v.v_rows);\n+\t\t\tvcp = vc_cons[i].d;\n+\t\t\tif (vcp) {\n+\t\t\t\tif (v.v_vlin)\n+\t\t\t\t\tvcp->vc_scan_lines = v.v_vlin;\n+\t\t\t\tif (v.v_clin)\n+\t\t\t\t\tvcp->vc_font.height = v.v_clin;\n+\t\t\t\tvcp->vc_resize_user = 1;\n+\t\t\t\tvc_resize(vcp, v.v_cols, v.v_rows);\n+\t\t\t}\n \t\t\tconsole_unlock();\n \t\t}\n \t\tbreak;",
        "function_modified_lines": {
            "added": [
                "\t\t\tstruct vc_data *vcp;",
                "",
                "\t\t\tvcp = vc_cons[i].d;",
                "\t\t\tif (vcp) {",
                "\t\t\t\tif (v.v_vlin)",
                "\t\t\t\t\tvcp->vc_scan_lines = v.v_vlin;",
                "\t\t\t\tif (v.v_clin)",
                "\t\t\t\t\tvcp->vc_font.height = v.v_clin;",
                "\t\t\t\tvcp->vc_resize_user = 1;",
                "\t\t\t\tvc_resize(vcp, v.v_cols, v.v_rows);",
                "\t\t\t}"
            ],
            "deleted": [
                "\t\t\tif (v.v_vlin)",
                "\t\t\t\tvc_cons[i].d->vc_scan_lines = v.v_vlin;",
                "\t\t\tif (v.v_clin)",
                "\t\t\t\tvc_cons[i].d->vc_font.height = v.v_clin;",
                "\t\t\tvc_cons[i].d->vc_resize_user = 1;",
                "\t\t\tvc_resize(vc_cons[i].d, v.v_cols, v.v_rows);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476"
        ],
        "cve_description": "A race condition in the Linux kernel before 5.5.7 involving VT_RESIZEX could lead to a NULL pointer dereference and general protection fault."
    },
    {
        "cve_id": "CVE-2021-0920",
        "code_before_change": "static int unix_stream_read_generic(struct unix_stream_read_state *state,\n\t\t\t\t    bool freezable)\n{\n\tstruct scm_cookie scm;\n\tstruct socket *sock = state->socket;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tint copied = 0;\n\tint flags = state->flags;\n\tint noblock = flags & MSG_DONTWAIT;\n\tbool check_creds = false;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\tsize_t size = state->size;\n\tunsigned int last_len;\n\n\tif (unlikely(sk->sk_state != TCP_ESTABLISHED)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (unlikely(flags & MSG_OOB)) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, noblock);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\tmutex_lock(&u->iolock);\n\n\tskip = max(sk_peek_offset(sk, flags), 0);\n\n\tdo {\n\t\tint chunk;\n\t\tbool drop_skb;\n\t\tstruct sk_buff *skb, *last;\n\nredo:\n\t\tunix_state_lock(sk);\n\t\tif (sock_flag(sk, SOCK_DEAD)) {\n\t\t\terr = -ECONNRESET;\n\t\t\tgoto unlock;\n\t\t}\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\n\t\tlast_len = last ? last->len : 0;\nagain:\n\t\tif (skb == NULL) {\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\tif (!timeo) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmutex_unlock(&u->iolock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last,\n\t\t\t\t\t\t      last_len, freezable);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tscm_destroy(&scm);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmutex_lock(&u->iolock);\n\t\t\tgoto redo;\nunlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif (!unix_skb_scm_eq(skb, &scm))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tunix_set_secdata(&scm, skb);\n\t\t\tcheck_creds = true;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (state->msg && state->msg->msg_name) {\n\t\t\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr,\n\t\t\t\t\t state->msg->msg_name);\n\t\t\tunix_copy_addr(state->msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tskb_get(skb);\n\t\tchunk = state->recv_actor(skb, skip, chunk, state);\n\t\tdrop_skb = !unix_skb_len(skb);\n\t\t/* skb is only safe to use if !drop_skb */\n\t\tconsume_skb(skb);\n\t\tif (chunk < 0) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\tif (drop_skb) {\n\t\t\t/* the skb was touched by a concurrent reader;\n\t\t\t * we should not expect anything from this skb\n\t\t\t * anymore and assume it invalid - we can be\n\t\t\t * sure it was dropped from the socket queue\n\t\t\t *\n\t\t\t * let's report a short read\n\t\t\t */\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp) {\n\t\t\t\tscm_stat_del(sk, skb);\n\t\t\t\tunix_detach_fds(&scm, skb);\n\t\t\t}\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (scm.fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tbreak;\n\n\t\t\tskip = 0;\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tunix_state_lock(sk);\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (skb)\n\t\t\t\tgoto again;\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->iolock);\n\tif (state->msg)\n\t\tscm_recv(sock, state->msg, &scm, flags);\n\telse\n\t\tscm_destroy(&scm);\nout:\n\treturn copied ? : err;\n}",
        "code_after_change": "static int unix_stream_read_generic(struct unix_stream_read_state *state,\n\t\t\t\t    bool freezable)\n{\n\tstruct scm_cookie scm;\n\tstruct socket *sock = state->socket;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tint copied = 0;\n\tint flags = state->flags;\n\tint noblock = flags & MSG_DONTWAIT;\n\tbool check_creds = false;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\tsize_t size = state->size;\n\tunsigned int last_len;\n\n\tif (unlikely(sk->sk_state != TCP_ESTABLISHED)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (unlikely(flags & MSG_OOB)) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, noblock);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\tmutex_lock(&u->iolock);\n\n\tskip = max(sk_peek_offset(sk, flags), 0);\n\n\tdo {\n\t\tint chunk;\n\t\tbool drop_skb;\n\t\tstruct sk_buff *skb, *last;\n\nredo:\n\t\tunix_state_lock(sk);\n\t\tif (sock_flag(sk, SOCK_DEAD)) {\n\t\t\terr = -ECONNRESET;\n\t\t\tgoto unlock;\n\t\t}\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\n\t\tlast_len = last ? last->len : 0;\nagain:\n\t\tif (skb == NULL) {\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\tif (!timeo) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmutex_unlock(&u->iolock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last,\n\t\t\t\t\t\t      last_len, freezable);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tscm_destroy(&scm);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmutex_lock(&u->iolock);\n\t\t\tgoto redo;\nunlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif (!unix_skb_scm_eq(skb, &scm))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tunix_set_secdata(&scm, skb);\n\t\t\tcheck_creds = true;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (state->msg && state->msg->msg_name) {\n\t\t\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr,\n\t\t\t\t\t state->msg->msg_name);\n\t\t\tunix_copy_addr(state->msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tskb_get(skb);\n\t\tchunk = state->recv_actor(skb, skip, chunk, state);\n\t\tdrop_skb = !unix_skb_len(skb);\n\t\t/* skb is only safe to use if !drop_skb */\n\t\tconsume_skb(skb);\n\t\tif (chunk < 0) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\tif (drop_skb) {\n\t\t\t/* the skb was touched by a concurrent reader;\n\t\t\t * we should not expect anything from this skb\n\t\t\t * anymore and assume it invalid - we can be\n\t\t\t * sure it was dropped from the socket queue\n\t\t\t *\n\t\t\t * let's report a short read\n\t\t\t */\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp) {\n\t\t\t\tscm_stat_del(sk, skb);\n\t\t\t\tunix_detach_fds(&scm, skb);\n\t\t\t}\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (scm.fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tunix_peek_fds(&scm, skb);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tbreak;\n\n\t\t\tskip = 0;\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tunix_state_lock(sk);\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (skb)\n\t\t\t\tgoto again;\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->iolock);\n\tif (state->msg)\n\t\tscm_recv(sock, state->msg, &scm, flags);\n\telse\n\t\tscm_destroy(&scm);\nout:\n\treturn copied ? : err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -169,7 +169,7 @@\n \t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n \t\t\t */\n \t\t\tif (UNIXCB(skb).fp)\n-\t\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n+\t\t\t\tunix_peek_fds(&scm, skb);\n \n \t\t\tsk_peek_offset_fwd(sk, chunk);\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tunix_peek_fds(&scm, skb);"
            ],
            "deleted": [
                "\t\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In unix_scm_to_skb of af_unix.c, there is a possible use after free bug due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-196926917References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2021-0920",
        "code_before_change": "static int unix_dgram_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t      size_t size, int flags)\n{\n\tstruct scm_cookie scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sk_buff *skb, *last;\n\tlong timeo;\n\tint skip;\n\tint err;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\tdo {\n\t\tmutex_lock(&u->iolock);\n\n\t\tskip = sk_peek_offset(sk, flags);\n\t\tskb = __skb_try_recv_datagram(sk, &sk->sk_receive_queue, flags,\n\t\t\t\t\t      &skip, &err, &last);\n\t\tif (skb) {\n\t\t\tif (!(flags & MSG_PEEK))\n\t\t\t\tscm_stat_del(sk, skb);\n\t\t\tbreak;\n\t\t}\n\n\t\tmutex_unlock(&u->iolock);\n\n\t\tif (err != -EAGAIN)\n\t\t\tbreak;\n\t} while (timeo &&\n\t\t !__skb_wait_for_more_packets(sk, &sk->sk_receive_queue,\n\t\t\t\t\t      &err, &timeo, last));\n\n\tif (!skb) { /* implies iolock unlocked */\n\t\tunix_state_lock(sk);\n\t\t/* Signal EOF on disconnected non-blocking SEQPACKET socket. */\n\t\tif (sk->sk_type == SOCK_SEQPACKET && err == -EAGAIN &&\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\t\terr = 0;\n\t\tunix_state_unlock(sk);\n\t\tgoto out;\n\t}\n\n\tif (wq_has_sleeper(&u->peer_wait))\n\t\twake_up_interruptible_sync_poll(&u->peer_wait,\n\t\t\t\t\t\tEPOLLOUT | EPOLLWRNORM |\n\t\t\t\t\t\tEPOLLWRBAND);\n\n\tif (msg->msg_name)\n\t\tunix_copy_addr(msg, skb->sk);\n\n\tif (size > skb->len - skip)\n\t\tsize = skb->len - skip;\n\telse if (size < skb->len - skip)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\terr = skb_copy_datagram_msg(skb, skip, msg, size);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock_flag(sk, SOCK_RCVTSTAMP))\n\t\t__sock_recv_timestamp(msg, sk, skb);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\tunix_set_secdata(&scm, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tif (UNIXCB(skb).fp)\n\t\t\tunix_detach_fds(&scm, skb);\n\n\t\tsk_peek_offset_bwd(sk, skb->len);\n\t} else {\n\t\t/* It is questionable: on PEEK we could:\n\t\t   - do not return fds - good, but too simple 8)\n\t\t   - return fds, and do not return them on read (old strategy,\n\t\t     apparently wrong)\n\t\t   - clone fds (I chose it for now, it is the most universal\n\t\t     solution)\n\n\t\t   POSIX 1003.1g does not actually define this clearly\n\t\t   at all. POSIX 1003.1g doesn't define a lot of things\n\t\t   clearly however!\n\n\t\t*/\n\n\t\tsk_peek_offset_fwd(sk, size);\n\n\t\tif (UNIXCB(skb).fp)\n\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n\t}\n\terr = (flags & MSG_TRUNC) ? skb->len - skip : size;\n\n\tscm_recv(sock, msg, &scm, flags);\n\nout_free:\n\tskb_free_datagram(sk, skb);\n\tmutex_unlock(&u->iolock);\nout:\n\treturn err;\n}",
        "code_after_change": "static int unix_dgram_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t      size_t size, int flags)\n{\n\tstruct scm_cookie scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sk_buff *skb, *last;\n\tlong timeo;\n\tint skip;\n\tint err;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\tdo {\n\t\tmutex_lock(&u->iolock);\n\n\t\tskip = sk_peek_offset(sk, flags);\n\t\tskb = __skb_try_recv_datagram(sk, &sk->sk_receive_queue, flags,\n\t\t\t\t\t      &skip, &err, &last);\n\t\tif (skb) {\n\t\t\tif (!(flags & MSG_PEEK))\n\t\t\t\tscm_stat_del(sk, skb);\n\t\t\tbreak;\n\t\t}\n\n\t\tmutex_unlock(&u->iolock);\n\n\t\tif (err != -EAGAIN)\n\t\t\tbreak;\n\t} while (timeo &&\n\t\t !__skb_wait_for_more_packets(sk, &sk->sk_receive_queue,\n\t\t\t\t\t      &err, &timeo, last));\n\n\tif (!skb) { /* implies iolock unlocked */\n\t\tunix_state_lock(sk);\n\t\t/* Signal EOF on disconnected non-blocking SEQPACKET socket. */\n\t\tif (sk->sk_type == SOCK_SEQPACKET && err == -EAGAIN &&\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\t\terr = 0;\n\t\tunix_state_unlock(sk);\n\t\tgoto out;\n\t}\n\n\tif (wq_has_sleeper(&u->peer_wait))\n\t\twake_up_interruptible_sync_poll(&u->peer_wait,\n\t\t\t\t\t\tEPOLLOUT | EPOLLWRNORM |\n\t\t\t\t\t\tEPOLLWRBAND);\n\n\tif (msg->msg_name)\n\t\tunix_copy_addr(msg, skb->sk);\n\n\tif (size > skb->len - skip)\n\t\tsize = skb->len - skip;\n\telse if (size < skb->len - skip)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\terr = skb_copy_datagram_msg(skb, skip, msg, size);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock_flag(sk, SOCK_RCVTSTAMP))\n\t\t__sock_recv_timestamp(msg, sk, skb);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\tunix_set_secdata(&scm, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tif (UNIXCB(skb).fp)\n\t\t\tunix_detach_fds(&scm, skb);\n\n\t\tsk_peek_offset_bwd(sk, skb->len);\n\t} else {\n\t\t/* It is questionable: on PEEK we could:\n\t\t   - do not return fds - good, but too simple 8)\n\t\t   - return fds, and do not return them on read (old strategy,\n\t\t     apparently wrong)\n\t\t   - clone fds (I chose it for now, it is the most universal\n\t\t     solution)\n\n\t\t   POSIX 1003.1g does not actually define this clearly\n\t\t   at all. POSIX 1003.1g doesn't define a lot of things\n\t\t   clearly however!\n\n\t\t*/\n\n\t\tsk_peek_offset_fwd(sk, size);\n\n\t\tif (UNIXCB(skb).fp)\n\t\t\tunix_peek_fds(&scm, skb);\n\t}\n\terr = (flags & MSG_TRUNC) ? skb->len - skip : size;\n\n\tscm_recv(sock, msg, &scm, flags);\n\nout_free:\n\tskb_free_datagram(sk, skb);\n\tmutex_unlock(&u->iolock);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -92,7 +92,7 @@\n \t\tsk_peek_offset_fwd(sk, size);\n \n \t\tif (UNIXCB(skb).fp)\n-\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n+\t\t\tunix_peek_fds(&scm, skb);\n \t}\n \terr = (flags & MSG_TRUNC) ? skb->len - skip : size;\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tunix_peek_fds(&scm, skb);"
            ],
            "deleted": [
                "\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In unix_scm_to_skb of af_unix.c, there is a possible use after free bug due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-196926917References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2021-20261",
        "code_before_change": "static int do_format(int drive, struct format_descr *tmp_format_req)\n{\n\tint ret;\n\n\tif (lock_fdc(drive, true))\n\t\treturn -EINTR;\n\n\tset_floppy(drive);\n\tif (!_floppy ||\n\t    _floppy->track > DP->tracks ||\n\t    tmp_format_req->track >= _floppy->track ||\n\t    tmp_format_req->head >= _floppy->head ||\n\t    (_floppy->sect << 2) % (1 << FD_SIZECODE(_floppy)) ||\n\t    !_floppy->fmt_gap) {\n\t\tprocess_fd_request();\n\t\treturn -EINVAL;\n\t}\n\tformat_req = *tmp_format_req;\n\tformat_errors = 0;\n\tcont = &format_cont;\n\terrors = &format_errors;\n\tret = wait_til_done(redo_format, true);\n\tif (ret == -EINTR)\n\t\treturn -EINTR;\n\tprocess_fd_request();\n\treturn ret;\n}",
        "code_after_change": "static int do_format(int drive, struct format_descr *tmp_format_req)\n{\n\tint ret;\n\n\tif (lock_fdc(drive))\n\t\treturn -EINTR;\n\n\tset_floppy(drive);\n\tif (!_floppy ||\n\t    _floppy->track > DP->tracks ||\n\t    tmp_format_req->track >= _floppy->track ||\n\t    tmp_format_req->head >= _floppy->head ||\n\t    (_floppy->sect << 2) % (1 << FD_SIZECODE(_floppy)) ||\n\t    !_floppy->fmt_gap) {\n\t\tprocess_fd_request();\n\t\treturn -EINVAL;\n\t}\n\tformat_req = *tmp_format_req;\n\tformat_errors = 0;\n\tcont = &format_cont;\n\terrors = &format_errors;\n\tret = wait_til_done(redo_format, true);\n\tif (ret == -EINTR)\n\t\treturn -EINTR;\n\tprocess_fd_request();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tint ret;\n \n-\tif (lock_fdc(drive, true))\n+\tif (lock_fdc(drive))\n \t\treturn -EINTR;\n \n \tset_floppy(drive);",
        "function_modified_lines": {
            "added": [
                "\tif (lock_fdc(drive))"
            ],
            "deleted": [
                "\tif (lock_fdc(drive, true))"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw."
    },
    {
        "cve_id": "CVE-2021-20261",
        "code_before_change": "static unsigned int floppy_check_events(struct gendisk *disk,\n\t\t\t\t\tunsigned int clearing)\n{\n\tint drive = (long)disk->private_data;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\n\tif (time_after(jiffies, UDRS->last_checked + UDP->checkfreq)) {\n\t\tlock_fdc(drive, false);\n\t\tpoll_drive(false, 0);\n\t\tprocess_fd_request();\n\t}\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\treturn 0;\n}",
        "code_after_change": "static unsigned int floppy_check_events(struct gendisk *disk,\n\t\t\t\t\tunsigned int clearing)\n{\n\tint drive = (long)disk->private_data;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\n\tif (time_after(jiffies, UDRS->last_checked + UDP->checkfreq)) {\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tpoll_drive(false, 0);\n\t\tprocess_fd_request();\n\t}\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,8 @@\n \t\treturn DISK_EVENT_MEDIA_CHANGE;\n \n \tif (time_after(jiffies, UDRS->last_checked + UDP->checkfreq)) {\n-\t\tlock_fdc(drive, false);\n+\t\tif (lock_fdc(drive))\n+\t\t\treturn -EINTR;\n \t\tpoll_drive(false, 0);\n \t\tprocess_fd_request();\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (lock_fdc(drive))",
                "\t\t\treturn -EINTR;"
            ],
            "deleted": [
                "\t\tlock_fdc(drive, false);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw."
    },
    {
        "cve_id": "CVE-2021-20261",
        "code_before_change": "static int user_reset_fdc(int drive, int arg, bool interruptible)\n{\n\tint ret;\n\n\tif (lock_fdc(drive, interruptible))\n\t\treturn -EINTR;\n\n\tif (arg == FD_RESET_ALWAYS)\n\t\tFDCS->reset = 1;\n\tif (FDCS->reset) {\n\t\tcont = &reset_cont;\n\t\tret = wait_til_done(reset_fdc, interruptible);\n\t\tif (ret == -EINTR)\n\t\t\treturn -EINTR;\n\t}\n\tprocess_fd_request();\n\treturn 0;\n}",
        "code_after_change": "static int user_reset_fdc(int drive, int arg, bool interruptible)\n{\n\tint ret;\n\n\tif (lock_fdc(drive))\n\t\treturn -EINTR;\n\n\tif (arg == FD_RESET_ALWAYS)\n\t\tFDCS->reset = 1;\n\tif (FDCS->reset) {\n\t\tcont = &reset_cont;\n\t\tret = wait_til_done(reset_fdc, interruptible);\n\t\tif (ret == -EINTR)\n\t\t\treturn -EINTR;\n\t}\n\tprocess_fd_request();\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tint ret;\n \n-\tif (lock_fdc(drive, interruptible))\n+\tif (lock_fdc(drive))\n \t\treturn -EINTR;\n \n \tif (arg == FD_RESET_ALWAYS)",
        "function_modified_lines": {
            "added": [
                "\tif (lock_fdc(drive))"
            ],
            "deleted": [
                "\tif (lock_fdc(drive, interruptible))"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw."
    },
    {
        "cve_id": "CVE-2021-20261",
        "code_before_change": "static int get_floppy_geometry(int drive, int type, struct floppy_struct **g)\n{\n\tif (type)\n\t\t*g = &floppy_type[type];\n\telse {\n\t\tif (lock_fdc(drive, false))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(false, 0) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\t*g = current_type[drive];\n\t}\n\tif (!*g)\n\t\treturn -ENODEV;\n\treturn 0;\n}",
        "code_after_change": "static int get_floppy_geometry(int drive, int type, struct floppy_struct **g)\n{\n\tif (type)\n\t\t*g = &floppy_type[type];\n\telse {\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(false, 0) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\t*g = current_type[drive];\n\t}\n\tif (!*g)\n\t\treturn -ENODEV;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \tif (type)\n \t\t*g = &floppy_type[type];\n \telse {\n-\t\tif (lock_fdc(drive, false))\n+\t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;\n \t\tif (poll_drive(false, 0) == -EINTR)\n \t\t\treturn -EINTR;",
        "function_modified_lines": {
            "added": [
                "\t\tif (lock_fdc(drive))"
            ],
            "deleted": [
                "\t\tif (lock_fdc(drive, false))"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw."
    },
    {
        "cve_id": "CVE-2021-20261",
        "code_before_change": "static int floppy_revalidate(struct gendisk *disk)\n{\n\tint drive = (long)disk->private_data;\n\tint cf;\n\tint res = 0;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive)) {\n\t\tif (WARN(atomic_read(&usage_count) == 0,\n\t\t\t \"VFS: revalidate called on non-open device.\\n\"))\n\t\t\treturn -EFAULT;\n\n\t\tlock_fdc(drive, false);\n\t\tcf = (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t\t      test_bit(FD_VERIFY_BIT, &UDRS->flags));\n\t\tif (!(cf || test_bit(drive, &fake_change) || drive_no_geom(drive))) {\n\t\t\tprocess_fd_request();\t/*already done by another thread */\n\t\t\treturn 0;\n\t\t}\n\t\tUDRS->maxblock = 0;\n\t\tUDRS->maxtrack = 0;\n\t\tif (buffer_drive == drive)\n\t\t\tbuffer_track = -1;\n\t\tclear_bit(drive, &fake_change);\n\t\tclear_bit(FD_DISK_CHANGED_BIT, &UDRS->flags);\n\t\tif (cf)\n\t\t\tUDRS->generation++;\n\t\tif (drive_no_geom(drive)) {\n\t\t\t/* auto-sensing */\n\t\t\tres = __floppy_read_block_0(opened_bdev[drive], drive);\n\t\t} else {\n\t\t\tif (cf)\n\t\t\t\tpoll_drive(false, FD_RAW_NEED_DISK);\n\t\t\tprocess_fd_request();\n\t\t}\n\t}\n\tset_capacity(disk, floppy_sizes[UDRS->fd_device]);\n\treturn res;\n}",
        "code_after_change": "static int floppy_revalidate(struct gendisk *disk)\n{\n\tint drive = (long)disk->private_data;\n\tint cf;\n\tint res = 0;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive)) {\n\t\tif (WARN(atomic_read(&usage_count) == 0,\n\t\t\t \"VFS: revalidate called on non-open device.\\n\"))\n\t\t\treturn -EFAULT;\n\n\t\tres = lock_fdc(drive);\n\t\tif (res)\n\t\t\treturn res;\n\t\tcf = (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t\t      test_bit(FD_VERIFY_BIT, &UDRS->flags));\n\t\tif (!(cf || test_bit(drive, &fake_change) || drive_no_geom(drive))) {\n\t\t\tprocess_fd_request();\t/*already done by another thread */\n\t\t\treturn 0;\n\t\t}\n\t\tUDRS->maxblock = 0;\n\t\tUDRS->maxtrack = 0;\n\t\tif (buffer_drive == drive)\n\t\t\tbuffer_track = -1;\n\t\tclear_bit(drive, &fake_change);\n\t\tclear_bit(FD_DISK_CHANGED_BIT, &UDRS->flags);\n\t\tif (cf)\n\t\t\tUDRS->generation++;\n\t\tif (drive_no_geom(drive)) {\n\t\t\t/* auto-sensing */\n\t\t\tres = __floppy_read_block_0(opened_bdev[drive], drive);\n\t\t} else {\n\t\t\tif (cf)\n\t\t\t\tpoll_drive(false, FD_RAW_NEED_DISK);\n\t\t\tprocess_fd_request();\n\t\t}\n\t}\n\tset_capacity(disk, floppy_sizes[UDRS->fd_device]);\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,9 @@\n \t\t\t \"VFS: revalidate called on non-open device.\\n\"))\n \t\t\treturn -EFAULT;\n \n-\t\tlock_fdc(drive, false);\n+\t\tres = lock_fdc(drive);\n+\t\tif (res)\n+\t\t\treturn res;\n \t\tcf = (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n \t\t      test_bit(FD_VERIFY_BIT, &UDRS->flags));\n \t\tif (!(cf || test_bit(drive, &fake_change) || drive_no_geom(drive))) {",
        "function_modified_lines": {
            "added": [
                "\t\tres = lock_fdc(drive);",
                "\t\tif (res)",
                "\t\t\treturn res;"
            ],
            "deleted": [
                "\t\tlock_fdc(drive, false);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw."
    },
    {
        "cve_id": "CVE-2021-20321",
        "code_before_change": "static int ovl_rename(struct user_namespace *mnt_userns, struct inode *olddir,\n\t\t      struct dentry *old, struct inode *newdir,\n\t\t      struct dentry *new, unsigned int flags)\n{\n\tint err;\n\tstruct dentry *old_upperdir;\n\tstruct dentry *new_upperdir;\n\tstruct dentry *olddentry;\n\tstruct dentry *newdentry;\n\tstruct dentry *trap;\n\tbool old_opaque;\n\tbool new_opaque;\n\tbool cleanup_whiteout = false;\n\tbool update_nlink = false;\n\tbool overwrite = !(flags & RENAME_EXCHANGE);\n\tbool is_dir = d_is_dir(old);\n\tbool new_is_dir = d_is_dir(new);\n\tbool samedir = olddir == newdir;\n\tstruct dentry *opaquedir = NULL;\n\tconst struct cred *old_cred = NULL;\n\tLIST_HEAD(list);\n\n\terr = -EINVAL;\n\tif (flags & ~(RENAME_EXCHANGE | RENAME_NOREPLACE))\n\t\tgoto out;\n\n\tflags &= ~RENAME_NOREPLACE;\n\n\t/* Don't copy up directory trees */\n\terr = -EXDEV;\n\tif (!ovl_can_move(old))\n\t\tgoto out;\n\tif (!overwrite && !ovl_can_move(new))\n\t\tgoto out;\n\n\tif (overwrite && new_is_dir && !ovl_pure_upper(new)) {\n\t\terr = ovl_check_empty_dir(new, &list);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (overwrite) {\n\t\tif (ovl_lower_positive(old)) {\n\t\t\tif (!ovl_dentry_is_whiteout(new)) {\n\t\t\t\t/* Whiteout source */\n\t\t\t\tflags |= RENAME_WHITEOUT;\n\t\t\t} else {\n\t\t\t\t/* Switch whiteouts */\n\t\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\t}\n\t\t} else if (is_dir && ovl_dentry_is_whiteout(new)) {\n\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\tcleanup_whiteout = true;\n\t\t}\n\t}\n\n\terr = ovl_want_write(old);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_copy_up(old);\n\tif (err)\n\t\tgoto out_drop_write;\n\n\terr = ovl_copy_up(new->d_parent);\n\tif (err)\n\t\tgoto out_drop_write;\n\tif (!overwrite) {\n\t\terr = ovl_copy_up(new);\n\t\tif (err)\n\t\t\tgoto out_drop_write;\n\t} else if (d_inode(new)) {\n\t\terr = ovl_nlink_start(new);\n\t\tif (err)\n\t\t\tgoto out_drop_write;\n\n\t\tupdate_nlink = true;\n\t}\n\n\told_cred = ovl_override_creds(old->d_sb);\n\n\tif (!list_empty(&list)) {\n\t\topaquedir = ovl_clear_empty(new, &list);\n\t\terr = PTR_ERR(opaquedir);\n\t\tif (IS_ERR(opaquedir)) {\n\t\t\topaquedir = NULL;\n\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\told_upperdir = ovl_dentry_upper(old->d_parent);\n\tnew_upperdir = ovl_dentry_upper(new->d_parent);\n\n\tif (!samedir) {\n\t\t/*\n\t\t * When moving a merge dir or non-dir with copy up origin into\n\t\t * a new parent, we are marking the new parent dir \"impure\".\n\t\t * When ovl_iterate() iterates an \"impure\" upper dir, it will\n\t\t * lookup the origin inodes of the entries to fill d_ino.\n\t\t */\n\t\tif (ovl_type_origin(old)) {\n\t\t\terr = ovl_set_impure(new->d_parent, new_upperdir);\n\t\t\tif (err)\n\t\t\t\tgoto out_revert_creds;\n\t\t}\n\t\tif (!overwrite && ovl_type_origin(new)) {\n\t\t\terr = ovl_set_impure(old->d_parent, old_upperdir);\n\t\t\tif (err)\n\t\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\ttrap = lock_rename(new_upperdir, old_upperdir);\n\n\tolddentry = lookup_one_len(old->d_name.name, old_upperdir,\n\t\t\t\t   old->d_name.len);\n\terr = PTR_ERR(olddentry);\n\tif (IS_ERR(olddentry))\n\t\tgoto out_unlock;\n\n\terr = -ESTALE;\n\tif (!ovl_matches_upper(old, olddentry))\n\t\tgoto out_dput_old;\n\n\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n\t\t\t\t   new->d_name.len);\n\terr = PTR_ERR(newdentry);\n\tif (IS_ERR(newdentry))\n\t\tgoto out_dput_old;\n\n\told_opaque = ovl_dentry_is_opaque(old);\n\tnew_opaque = ovl_dentry_is_opaque(new);\n\n\terr = -ESTALE;\n\tif (d_inode(new) && ovl_dentry_upper(new)) {\n\t\tif (opaquedir) {\n\t\t\tif (newdentry != opaquedir)\n\t\t\t\tgoto out_dput;\n\t\t} else {\n\t\t\tif (!ovl_matches_upper(new, newdentry))\n\t\t\t\tgoto out_dput;\n\t\t}\n\t} else {\n\t\tif (!d_is_negative(newdentry) &&\n\t\t    (!new_opaque || !ovl_is_whiteout(newdentry)))\n\t\t\tgoto out_dput;\n\t}\n\n\tif (olddentry == trap)\n\t\tgoto out_dput;\n\tif (newdentry == trap)\n\t\tgoto out_dput;\n\n\tif (olddentry->d_inode == newdentry->d_inode)\n\t\tgoto out_dput;\n\n\terr = 0;\n\tif (ovl_type_merge_or_lower(old))\n\t\terr = ovl_set_redirect(old, samedir);\n\telse if (is_dir && !old_opaque && ovl_type_merge(new->d_parent))\n\t\terr = ovl_set_opaque_xerr(old, olddentry, -EXDEV);\n\tif (err)\n\t\tgoto out_dput;\n\n\tif (!overwrite && ovl_type_merge_or_lower(new))\n\t\terr = ovl_set_redirect(new, samedir);\n\telse if (!overwrite && new_is_dir && !new_opaque &&\n\t\t ovl_type_merge(old->d_parent))\n\t\terr = ovl_set_opaque_xerr(new, newdentry, -EXDEV);\n\tif (err)\n\t\tgoto out_dput;\n\n\terr = ovl_do_rename(old_upperdir->d_inode, olddentry,\n\t\t\t    new_upperdir->d_inode, newdentry, flags);\n\tif (err)\n\t\tgoto out_dput;\n\n\tif (cleanup_whiteout)\n\t\tovl_cleanup(old_upperdir->d_inode, newdentry);\n\n\tif (overwrite && d_inode(new)) {\n\t\tif (new_is_dir)\n\t\t\tclear_nlink(d_inode(new));\n\t\telse\n\t\t\tovl_drop_nlink(new);\n\t}\n\n\tovl_dir_modified(old->d_parent, ovl_type_origin(old) ||\n\t\t\t (!overwrite && ovl_type_origin(new)));\n\tovl_dir_modified(new->d_parent, ovl_type_origin(old) ||\n\t\t\t (d_inode(new) && ovl_type_origin(new)));\n\n\t/* copy ctime: */\n\tovl_copyattr(d_inode(olddentry), d_inode(old));\n\tif (d_inode(new) && ovl_dentry_upper(new))\n\t\tovl_copyattr(d_inode(newdentry), d_inode(new));\n\nout_dput:\n\tdput(newdentry);\nout_dput_old:\n\tdput(olddentry);\nout_unlock:\n\tunlock_rename(new_upperdir, old_upperdir);\nout_revert_creds:\n\trevert_creds(old_cred);\n\tif (update_nlink)\n\t\tovl_nlink_end(new);\nout_drop_write:\n\tovl_drop_write(old);\nout:\n\tdput(opaquedir);\n\tovl_cache_free(&list);\n\treturn err;\n}",
        "code_after_change": "static int ovl_rename(struct user_namespace *mnt_userns, struct inode *olddir,\n\t\t      struct dentry *old, struct inode *newdir,\n\t\t      struct dentry *new, unsigned int flags)\n{\n\tint err;\n\tstruct dentry *old_upperdir;\n\tstruct dentry *new_upperdir;\n\tstruct dentry *olddentry;\n\tstruct dentry *newdentry;\n\tstruct dentry *trap;\n\tbool old_opaque;\n\tbool new_opaque;\n\tbool cleanup_whiteout = false;\n\tbool update_nlink = false;\n\tbool overwrite = !(flags & RENAME_EXCHANGE);\n\tbool is_dir = d_is_dir(old);\n\tbool new_is_dir = d_is_dir(new);\n\tbool samedir = olddir == newdir;\n\tstruct dentry *opaquedir = NULL;\n\tconst struct cred *old_cred = NULL;\n\tLIST_HEAD(list);\n\n\terr = -EINVAL;\n\tif (flags & ~(RENAME_EXCHANGE | RENAME_NOREPLACE))\n\t\tgoto out;\n\n\tflags &= ~RENAME_NOREPLACE;\n\n\t/* Don't copy up directory trees */\n\terr = -EXDEV;\n\tif (!ovl_can_move(old))\n\t\tgoto out;\n\tif (!overwrite && !ovl_can_move(new))\n\t\tgoto out;\n\n\tif (overwrite && new_is_dir && !ovl_pure_upper(new)) {\n\t\terr = ovl_check_empty_dir(new, &list);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (overwrite) {\n\t\tif (ovl_lower_positive(old)) {\n\t\t\tif (!ovl_dentry_is_whiteout(new)) {\n\t\t\t\t/* Whiteout source */\n\t\t\t\tflags |= RENAME_WHITEOUT;\n\t\t\t} else {\n\t\t\t\t/* Switch whiteouts */\n\t\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\t}\n\t\t} else if (is_dir && ovl_dentry_is_whiteout(new)) {\n\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\tcleanup_whiteout = true;\n\t\t}\n\t}\n\n\terr = ovl_want_write(old);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_copy_up(old);\n\tif (err)\n\t\tgoto out_drop_write;\n\n\terr = ovl_copy_up(new->d_parent);\n\tif (err)\n\t\tgoto out_drop_write;\n\tif (!overwrite) {\n\t\terr = ovl_copy_up(new);\n\t\tif (err)\n\t\t\tgoto out_drop_write;\n\t} else if (d_inode(new)) {\n\t\terr = ovl_nlink_start(new);\n\t\tif (err)\n\t\t\tgoto out_drop_write;\n\n\t\tupdate_nlink = true;\n\t}\n\n\told_cred = ovl_override_creds(old->d_sb);\n\n\tif (!list_empty(&list)) {\n\t\topaquedir = ovl_clear_empty(new, &list);\n\t\terr = PTR_ERR(opaquedir);\n\t\tif (IS_ERR(opaquedir)) {\n\t\t\topaquedir = NULL;\n\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\told_upperdir = ovl_dentry_upper(old->d_parent);\n\tnew_upperdir = ovl_dentry_upper(new->d_parent);\n\n\tif (!samedir) {\n\t\t/*\n\t\t * When moving a merge dir or non-dir with copy up origin into\n\t\t * a new parent, we are marking the new parent dir \"impure\".\n\t\t * When ovl_iterate() iterates an \"impure\" upper dir, it will\n\t\t * lookup the origin inodes of the entries to fill d_ino.\n\t\t */\n\t\tif (ovl_type_origin(old)) {\n\t\t\terr = ovl_set_impure(new->d_parent, new_upperdir);\n\t\t\tif (err)\n\t\t\t\tgoto out_revert_creds;\n\t\t}\n\t\tif (!overwrite && ovl_type_origin(new)) {\n\t\t\terr = ovl_set_impure(old->d_parent, old_upperdir);\n\t\t\tif (err)\n\t\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\ttrap = lock_rename(new_upperdir, old_upperdir);\n\n\tolddentry = lookup_one_len(old->d_name.name, old_upperdir,\n\t\t\t\t   old->d_name.len);\n\terr = PTR_ERR(olddentry);\n\tif (IS_ERR(olddentry))\n\t\tgoto out_unlock;\n\n\terr = -ESTALE;\n\tif (!ovl_matches_upper(old, olddentry))\n\t\tgoto out_dput_old;\n\n\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n\t\t\t\t   new->d_name.len);\n\terr = PTR_ERR(newdentry);\n\tif (IS_ERR(newdentry))\n\t\tgoto out_dput_old;\n\n\told_opaque = ovl_dentry_is_opaque(old);\n\tnew_opaque = ovl_dentry_is_opaque(new);\n\n\terr = -ESTALE;\n\tif (d_inode(new) && ovl_dentry_upper(new)) {\n\t\tif (opaquedir) {\n\t\t\tif (newdentry != opaquedir)\n\t\t\t\tgoto out_dput;\n\t\t} else {\n\t\t\tif (!ovl_matches_upper(new, newdentry))\n\t\t\t\tgoto out_dput;\n\t\t}\n\t} else {\n\t\tif (!d_is_negative(newdentry)) {\n\t\t\tif (!new_opaque || !ovl_is_whiteout(newdentry))\n\t\t\t\tgoto out_dput;\n\t\t} else {\n\t\t\tif (flags & RENAME_EXCHANGE)\n\t\t\t\tgoto out_dput;\n\t\t}\n\t}\n\n\tif (olddentry == trap)\n\t\tgoto out_dput;\n\tif (newdentry == trap)\n\t\tgoto out_dput;\n\n\tif (olddentry->d_inode == newdentry->d_inode)\n\t\tgoto out_dput;\n\n\terr = 0;\n\tif (ovl_type_merge_or_lower(old))\n\t\terr = ovl_set_redirect(old, samedir);\n\telse if (is_dir && !old_opaque && ovl_type_merge(new->d_parent))\n\t\terr = ovl_set_opaque_xerr(old, olddentry, -EXDEV);\n\tif (err)\n\t\tgoto out_dput;\n\n\tif (!overwrite && ovl_type_merge_or_lower(new))\n\t\terr = ovl_set_redirect(new, samedir);\n\telse if (!overwrite && new_is_dir && !new_opaque &&\n\t\t ovl_type_merge(old->d_parent))\n\t\terr = ovl_set_opaque_xerr(new, newdentry, -EXDEV);\n\tif (err)\n\t\tgoto out_dput;\n\n\terr = ovl_do_rename(old_upperdir->d_inode, olddentry,\n\t\t\t    new_upperdir->d_inode, newdentry, flags);\n\tif (err)\n\t\tgoto out_dput;\n\n\tif (cleanup_whiteout)\n\t\tovl_cleanup(old_upperdir->d_inode, newdentry);\n\n\tif (overwrite && d_inode(new)) {\n\t\tif (new_is_dir)\n\t\t\tclear_nlink(d_inode(new));\n\t\telse\n\t\t\tovl_drop_nlink(new);\n\t}\n\n\tovl_dir_modified(old->d_parent, ovl_type_origin(old) ||\n\t\t\t (!overwrite && ovl_type_origin(new)));\n\tovl_dir_modified(new->d_parent, ovl_type_origin(old) ||\n\t\t\t (d_inode(new) && ovl_type_origin(new)));\n\n\t/* copy ctime: */\n\tovl_copyattr(d_inode(olddentry), d_inode(old));\n\tif (d_inode(new) && ovl_dentry_upper(new))\n\t\tovl_copyattr(d_inode(newdentry), d_inode(new));\n\nout_dput:\n\tdput(newdentry);\nout_dput_old:\n\tdput(olddentry);\nout_unlock:\n\tunlock_rename(new_upperdir, old_upperdir);\nout_revert_creds:\n\trevert_creds(old_cred);\n\tif (update_nlink)\n\t\tovl_nlink_end(new);\nout_drop_write:\n\tovl_drop_write(old);\nout:\n\tdput(opaquedir);\n\tovl_cache_free(&list);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -141,9 +141,13 @@\n \t\t\t\tgoto out_dput;\n \t\t}\n \t} else {\n-\t\tif (!d_is_negative(newdentry) &&\n-\t\t    (!new_opaque || !ovl_is_whiteout(newdentry)))\n-\t\t\tgoto out_dput;\n+\t\tif (!d_is_negative(newdentry)) {\n+\t\t\tif (!new_opaque || !ovl_is_whiteout(newdentry))\n+\t\t\t\tgoto out_dput;\n+\t\t} else {\n+\t\t\tif (flags & RENAME_EXCHANGE)\n+\t\t\t\tgoto out_dput;\n+\t\t}\n \t}\n \n \tif (olddentry == trap)",
        "function_modified_lines": {
            "added": [
                "\t\tif (!d_is_negative(newdentry)) {",
                "\t\t\tif (!new_opaque || !ovl_is_whiteout(newdentry))",
                "\t\t\t\tgoto out_dput;",
                "\t\t} else {",
                "\t\t\tif (flags & RENAME_EXCHANGE)",
                "\t\t\t\tgoto out_dput;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tif (!d_is_negative(newdentry) &&",
                "\t\t    (!new_opaque || !ovl_is_whiteout(newdentry)))",
                "\t\t\tgoto out_dput;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition accessing file object in the Linux kernel OverlayFS subsystem was found in the way users do rename in specific way with OverlayFS. A local user could use this flaw to crash the system."
    },
    {
        "cve_id": "CVE-2021-23133",
        "code_before_change": "static void sctp_destroy_sock(struct sock *sk)\n{\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\t/* Release our hold on the endpoint. */\n\tsp = sctp_sk(sk);\n\t/* This could happen during socket init, thus we bail out\n\t * early, since the rest of the below is not setup either.\n\t */\n\tif (sp->ep == NULL)\n\t\treturn;\n\n\tif (sp->do_auto_asconf) {\n\t\tsp->do_auto_asconf = 0;\n\t\tlist_del(&sp->auto_asconf_list);\n\t}\n\tsctp_endpoint_free(sp->ep);\n\tlocal_bh_disable();\n\tsk_sockets_allocated_dec(sk);\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\tlocal_bh_enable();\n}",
        "code_after_change": "static void sctp_destroy_sock(struct sock *sk)\n{\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\t/* Release our hold on the endpoint. */\n\tsp = sctp_sk(sk);\n\t/* This could happen during socket init, thus we bail out\n\t * early, since the rest of the below is not setup either.\n\t */\n\tif (sp->ep == NULL)\n\t\treturn;\n\n\tif (sp->do_auto_asconf) {\n\t\tsp->do_auto_asconf = 0;\n\t\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_del(&sp->auto_asconf_list);\n\t\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n\t}\n\tsctp_endpoint_free(sp->ep);\n\tlocal_bh_disable();\n\tsk_sockets_allocated_dec(sk);\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\tlocal_bh_enable();\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,7 +14,9 @@\n \n \tif (sp->do_auto_asconf) {\n \t\tsp->do_auto_asconf = 0;\n+\t\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n \t\tlist_del(&sp->auto_asconf_list);\n+\t\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n \t}\n \tsctp_endpoint_free(sp->ep);\n \tlocal_bh_disable();",
        "function_modified_lines": {
            "added": [
                "\t\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);",
                "\t\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition in Linux kernel SCTP sockets (net/sctp/socket.c) before 5.12-rc8 can lead to kernel privilege escalation from the context of a network service or an unprivileged process. If sctp_destroy_sock is called without sock_net(sk)->sctp.addr_wq_lock then an element is removed from the auto_asconf_splist list without any proper locking. This can be exploited by an attacker with network service privileges to escalate to root or from the context of an unprivileged user directly if a BPF_CGROUP_INET_SOCK_CREATE is attached which denies creation of some SCTP socket."
    },
    {
        "cve_id": "CVE-2021-23133",
        "code_before_change": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\tsk->sk_gso_type = SKB_GSO_SCTP;\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tsp->subscribe = 0;\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->udp_port    = htons(net->sctp.udp_port);\n\tsp->encap_port  = htons(net->sctp.encap_port);\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pf_retrans  = net->sctp.pf_retrans;\n\tsp->ps_retrans  = net->sctp.ps_retrans;\n\tsp->pf_expose   = net->sctp.pf_expose;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\tsp->default_ss = SCTP_SS_DEFAULT;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tsk_sockets_allocated_inc(sk);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\n\t/* Nothing can fail after this block, otherwise\n\t * sctp_destroy_sock() will be called without addr_wq_lock held\n\t */\n\tif (net->sctp.default_auto_asconf) {\n\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);\n\t} else {\n\t\tsp->do_auto_asconf = 0;\n\t}\n\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
        "code_after_change": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\tsk->sk_gso_type = SKB_GSO_SCTP;\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tsp->subscribe = 0;\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->udp_port    = htons(net->sctp.udp_port);\n\tsp->encap_port  = htons(net->sctp.encap_port);\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pf_retrans  = net->sctp.pf_retrans;\n\tsp->ps_retrans  = net->sctp.ps_retrans;\n\tsp->pf_expose   = net->sctp.pf_expose;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\tsp->default_ss = SCTP_SS_DEFAULT;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tsk_sockets_allocated_inc(sk);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\n\tif (net->sctp.default_auto_asconf) {\n\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);\n\t} else {\n\t\tsp->do_auto_asconf = 0;\n\t}\n\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -134,9 +134,6 @@\n \tsk_sockets_allocated_inc(sk);\n \tsock_prot_inuse_add(net, sk->sk_prot, 1);\n \n-\t/* Nothing can fail after this block, otherwise\n-\t * sctp_destroy_sock() will be called without addr_wq_lock held\n-\t */\n \tif (net->sctp.default_auto_asconf) {\n \t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n \t\tlist_add_tail(&sp->auto_asconf_list,",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t/* Nothing can fail after this block, otherwise",
                "\t * sctp_destroy_sock() will be called without addr_wq_lock held",
                "\t */"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition in Linux kernel SCTP sockets (net/sctp/socket.c) before 5.12-rc8 can lead to kernel privilege escalation from the context of a network service or an unprivileged process. If sctp_destroy_sock is called without sock_net(sk)->sctp.addr_wq_lock then an element is removed from the auto_asconf_splist list without any proper locking. This can be exploited by an attacker with network service privileges to escalate to root or from the context of an unprivileged user directly if a BPF_CGROUP_INET_SOCK_CREATE is attached which denies creation of some SCTP socket."
    },
    {
        "cve_id": "CVE-2021-23133",
        "code_before_change": "static void sctp_close(struct sock *sk, long timeout)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_association *asoc;\n\tstruct list_head *pos, *temp;\n\tunsigned int data_was_unread;\n\n\tpr_debug(\"%s: sk:%p, timeout:%ld\\n\", __func__, sk, timeout);\n\n\tlock_sock_nested(sk, SINGLE_DEPTH_NESTING);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\tinet_sk_set_state(sk, SCTP_SS_CLOSING);\n\n\tep = sctp_sk(sk)->ep;\n\n\t/* Clean up any skbs sitting on the receive queue.  */\n\tdata_was_unread = sctp_queue_purge_ulpevents(&sk->sk_receive_queue);\n\tdata_was_unread += sctp_queue_purge_ulpevents(&sctp_sk(sk)->pd_lobby);\n\n\t/* Walk all associations on an endpoint.  */\n\tlist_for_each_safe(pos, temp, &ep->asocs) {\n\t\tasoc = list_entry(pos, struct sctp_association, asocs);\n\n\t\tif (sctp_style(sk, TCP)) {\n\t\t\t/* A closed association can still be in the list if\n\t\t\t * it belongs to a TCP-style listening socket that is\n\t\t\t * not yet accepted. If so, free it. If not, send an\n\t\t\t * ABORT or SHUTDOWN based on the linger options.\n\t\t\t */\n\t\t\tif (sctp_state(asoc, CLOSED)) {\n\t\t\t\tsctp_association_free(asoc);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (data_was_unread || !skb_queue_empty(&asoc->ulpq.lobby) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm_uo) ||\n\t\t    (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime)) {\n\t\t\tstruct sctp_chunk *chunk;\n\n\t\t\tchunk = sctp_make_abort_user(asoc, NULL, 0);\n\t\t\tsctp_primitive_ABORT(net, asoc, chunk);\n\t\t} else\n\t\t\tsctp_primitive_SHUTDOWN(net, asoc, NULL);\n\t}\n\n\t/* On a TCP-style socket, block for at most linger_time if set. */\n\tif (sctp_style(sk, TCP) && timeout)\n\t\tsctp_wait_for_close(sk, timeout);\n\n\t/* This will run the backlog queue.  */\n\trelease_sock(sk);\n\n\t/* Supposedly, no process has access to the socket, but\n\t * the net layers still may.\n\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock\n\t * held and that should be grabbed before socket lock.\n\t */\n\tspin_lock_bh(&net->sctp.addr_wq_lock);\n\tbh_lock_sock_nested(sk);\n\n\t/* Hold the sock, since sk_common_release() will put sock_put()\n\t * and we have just a little more cleanup.\n\t */\n\tsock_hold(sk);\n\tsk_common_release(sk);\n\n\tbh_unlock_sock(sk);\n\tspin_unlock_bh(&net->sctp.addr_wq_lock);\n\n\tsock_put(sk);\n\n\tSCTP_DBG_OBJCNT_DEC(sock);\n}",
        "code_after_change": "static void sctp_close(struct sock *sk, long timeout)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_association *asoc;\n\tstruct list_head *pos, *temp;\n\tunsigned int data_was_unread;\n\n\tpr_debug(\"%s: sk:%p, timeout:%ld\\n\", __func__, sk, timeout);\n\n\tlock_sock_nested(sk, SINGLE_DEPTH_NESTING);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\tinet_sk_set_state(sk, SCTP_SS_CLOSING);\n\n\tep = sctp_sk(sk)->ep;\n\n\t/* Clean up any skbs sitting on the receive queue.  */\n\tdata_was_unread = sctp_queue_purge_ulpevents(&sk->sk_receive_queue);\n\tdata_was_unread += sctp_queue_purge_ulpevents(&sctp_sk(sk)->pd_lobby);\n\n\t/* Walk all associations on an endpoint.  */\n\tlist_for_each_safe(pos, temp, &ep->asocs) {\n\t\tasoc = list_entry(pos, struct sctp_association, asocs);\n\n\t\tif (sctp_style(sk, TCP)) {\n\t\t\t/* A closed association can still be in the list if\n\t\t\t * it belongs to a TCP-style listening socket that is\n\t\t\t * not yet accepted. If so, free it. If not, send an\n\t\t\t * ABORT or SHUTDOWN based on the linger options.\n\t\t\t */\n\t\t\tif (sctp_state(asoc, CLOSED)) {\n\t\t\t\tsctp_association_free(asoc);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (data_was_unread || !skb_queue_empty(&asoc->ulpq.lobby) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm) ||\n\t\t    !skb_queue_empty(&asoc->ulpq.reasm_uo) ||\n\t\t    (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime)) {\n\t\t\tstruct sctp_chunk *chunk;\n\n\t\t\tchunk = sctp_make_abort_user(asoc, NULL, 0);\n\t\t\tsctp_primitive_ABORT(net, asoc, chunk);\n\t\t} else\n\t\t\tsctp_primitive_SHUTDOWN(net, asoc, NULL);\n\t}\n\n\t/* On a TCP-style socket, block for at most linger_time if set. */\n\tif (sctp_style(sk, TCP) && timeout)\n\t\tsctp_wait_for_close(sk, timeout);\n\n\t/* This will run the backlog queue.  */\n\trelease_sock(sk);\n\n\t/* Supposedly, no process has access to the socket, but\n\t * the net layers still may.\n\t */\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\n\t/* Hold the sock, since sk_common_release() will put sock_put()\n\t * and we have just a little more cleanup.\n\t */\n\tsock_hold(sk);\n\tsk_common_release(sk);\n\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\n\tsock_put(sk);\n\n\tSCTP_DBG_OBJCNT_DEC(sock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -55,11 +55,9 @@\n \n \t/* Supposedly, no process has access to the socket, but\n \t * the net layers still may.\n-\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock\n-\t * held and that should be grabbed before socket lock.\n \t */\n-\tspin_lock_bh(&net->sctp.addr_wq_lock);\n-\tbh_lock_sock_nested(sk);\n+\tlocal_bh_disable();\n+\tbh_lock_sock(sk);\n \n \t/* Hold the sock, since sk_common_release() will put sock_put()\n \t * and we have just a little more cleanup.\n@@ -68,7 +66,7 @@\n \tsk_common_release(sk);\n \n \tbh_unlock_sock(sk);\n-\tspin_unlock_bh(&net->sctp.addr_wq_lock);\n+\tlocal_bh_enable();\n \n \tsock_put(sk);\n ",
        "function_modified_lines": {
            "added": [
                "\tlocal_bh_disable();",
                "\tbh_lock_sock(sk);",
                "\tlocal_bh_enable();"
            ],
            "deleted": [
                "\t * Also, sctp_destroy_sock() needs to be called with addr_wq_lock",
                "\t * held and that should be grabbed before socket lock.",
                "\tspin_lock_bh(&net->sctp.addr_wq_lock);",
                "\tbh_lock_sock_nested(sk);",
                "\tspin_unlock_bh(&net->sctp.addr_wq_lock);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition in Linux kernel SCTP sockets (net/sctp/socket.c) before 5.12-rc8 can lead to kernel privilege escalation from the context of a network service or an unprivileged process. If sctp_destroy_sock is called without sock_net(sk)->sctp.addr_wq_lock then an element is removed from the auto_asconf_splist list without any proper locking. This can be exploited by an attacker with network service privileges to escalate to root or from the context of an unprivileged user directly if a BPF_CGROUP_INET_SOCK_CREATE is attached which denies creation of some SCTP socket."
    },
    {
        "cve_id": "CVE-2021-28964",
        "code_before_change": "static inline struct extent_buffer *\nget_old_root(struct btrfs_root *root, u64 time_seq)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct tree_mod_elem *tm;\n\tstruct extent_buffer *eb = NULL;\n\tstruct extent_buffer *eb_root;\n\tu64 eb_root_owner = 0;\n\tstruct extent_buffer *old;\n\tstruct tree_mod_root *old_root = NULL;\n\tu64 old_generation = 0;\n\tu64 logical;\n\tint level;\n\n\teb_root = btrfs_read_lock_root_node(root);\n\ttm = __tree_mod_log_oldest_root(eb_root, time_seq);\n\tif (!tm)\n\t\treturn eb_root;\n\n\tif (tm->op == MOD_LOG_ROOT_REPLACE) {\n\t\told_root = &tm->old_root;\n\t\told_generation = tm->generation;\n\t\tlogical = old_root->logical;\n\t\tlevel = old_root->level;\n\t} else {\n\t\tlogical = eb_root->start;\n\t\tlevel = btrfs_header_level(eb_root);\n\t}\n\n\ttm = tree_mod_log_search(fs_info, logical, time_seq);\n\tif (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t\told = read_tree_block(fs_info, logical, root->root_key.objectid,\n\t\t\t\t      0, level, NULL);\n\t\tif (WARN_ON(IS_ERR(old) || !extent_buffer_uptodate(old))) {\n\t\t\tif (!IS_ERR(old))\n\t\t\t\tfree_extent_buffer(old);\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"failed to read tree block %llu from get_old_root\",\n\t\t\t\t   logical);\n\t\t} else {\n\t\t\teb = btrfs_clone_extent_buffer(old);\n\t\t\tfree_extent_buffer(old);\n\t\t}\n\t} else if (old_root) {\n\t\teb_root_owner = btrfs_header_owner(eb_root);\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t\teb = alloc_dummy_extent_buffer(fs_info, logical);\n\t} else {\n\t\teb = btrfs_clone_extent_buffer(eb_root);\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t}\n\n\tif (!eb)\n\t\treturn NULL;\n\tif (old_root) {\n\t\tbtrfs_set_header_bytenr(eb, eb->start);\n\t\tbtrfs_set_header_backref_rev(eb, BTRFS_MIXED_BACKREF_REV);\n\t\tbtrfs_set_header_owner(eb, eb_root_owner);\n\t\tbtrfs_set_header_level(eb, old_root->level);\n\t\tbtrfs_set_header_generation(eb, old_generation);\n\t}\n\tbtrfs_set_buffer_lockdep_class(btrfs_header_owner(eb), eb,\n\t\t\t\t       btrfs_header_level(eb));\n\tbtrfs_tree_read_lock(eb);\n\tif (tm)\n\t\t__tree_mod_log_rewind(fs_info, eb, time_seq, tm);\n\telse\n\t\tWARN_ON(btrfs_header_level(eb) != 0);\n\tWARN_ON(btrfs_header_nritems(eb) > BTRFS_NODEPTRS_PER_BLOCK(fs_info));\n\n\treturn eb;\n}",
        "code_after_change": "static inline struct extent_buffer *\nget_old_root(struct btrfs_root *root, u64 time_seq)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct tree_mod_elem *tm;\n\tstruct extent_buffer *eb = NULL;\n\tstruct extent_buffer *eb_root;\n\tu64 eb_root_owner = 0;\n\tstruct extent_buffer *old;\n\tstruct tree_mod_root *old_root = NULL;\n\tu64 old_generation = 0;\n\tu64 logical;\n\tint level;\n\n\teb_root = btrfs_read_lock_root_node(root);\n\ttm = __tree_mod_log_oldest_root(eb_root, time_seq);\n\tif (!tm)\n\t\treturn eb_root;\n\n\tif (tm->op == MOD_LOG_ROOT_REPLACE) {\n\t\told_root = &tm->old_root;\n\t\told_generation = tm->generation;\n\t\tlogical = old_root->logical;\n\t\tlevel = old_root->level;\n\t} else {\n\t\tlogical = eb_root->start;\n\t\tlevel = btrfs_header_level(eb_root);\n\t}\n\n\ttm = tree_mod_log_search(fs_info, logical, time_seq);\n\tif (old_root && tm && tm->op != MOD_LOG_KEY_REMOVE_WHILE_FREEING) {\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t\told = read_tree_block(fs_info, logical, root->root_key.objectid,\n\t\t\t\t      0, level, NULL);\n\t\tif (WARN_ON(IS_ERR(old) || !extent_buffer_uptodate(old))) {\n\t\t\tif (!IS_ERR(old))\n\t\t\t\tfree_extent_buffer(old);\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"failed to read tree block %llu from get_old_root\",\n\t\t\t\t   logical);\n\t\t} else {\n\t\t\tbtrfs_tree_read_lock(old);\n\t\t\teb = btrfs_clone_extent_buffer(old);\n\t\t\tbtrfs_tree_read_unlock(old);\n\t\t\tfree_extent_buffer(old);\n\t\t}\n\t} else if (old_root) {\n\t\teb_root_owner = btrfs_header_owner(eb_root);\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t\teb = alloc_dummy_extent_buffer(fs_info, logical);\n\t} else {\n\t\teb = btrfs_clone_extent_buffer(eb_root);\n\t\tbtrfs_tree_read_unlock(eb_root);\n\t\tfree_extent_buffer(eb_root);\n\t}\n\n\tif (!eb)\n\t\treturn NULL;\n\tif (old_root) {\n\t\tbtrfs_set_header_bytenr(eb, eb->start);\n\t\tbtrfs_set_header_backref_rev(eb, BTRFS_MIXED_BACKREF_REV);\n\t\tbtrfs_set_header_owner(eb, eb_root_owner);\n\t\tbtrfs_set_header_level(eb, old_root->level);\n\t\tbtrfs_set_header_generation(eb, old_generation);\n\t}\n\tbtrfs_set_buffer_lockdep_class(btrfs_header_owner(eb), eb,\n\t\t\t\t       btrfs_header_level(eb));\n\tbtrfs_tree_read_lock(eb);\n\tif (tm)\n\t\t__tree_mod_log_rewind(fs_info, eb, time_seq, tm);\n\telse\n\t\tWARN_ON(btrfs_header_level(eb) != 0);\n\tWARN_ON(btrfs_header_nritems(eb) > BTRFS_NODEPTRS_PER_BLOCK(fs_info));\n\n\treturn eb;\n}",
        "patch": "--- code before\n+++ code after\n@@ -40,7 +40,9 @@\n \t\t\t\t   \"failed to read tree block %llu from get_old_root\",\n \t\t\t\t   logical);\n \t\t} else {\n+\t\t\tbtrfs_tree_read_lock(old);\n \t\t\teb = btrfs_clone_extent_buffer(old);\n+\t\t\tbtrfs_tree_read_unlock(old);\n \t\t\tfree_extent_buffer(old);\n \t\t}\n \t} else if (old_root) {",
        "function_modified_lines": {
            "added": [
                "\t\t\tbtrfs_tree_read_lock(old);",
                "\t\t\tbtrfs_tree_read_unlock(old);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was discovered in get_old_root in fs/btrfs/ctree.c in the Linux kernel through 5.11.8. It allows attackers to cause a denial of service (BUG) because of a lack of locking on an extent buffer before a cloning operation, aka CID-dbcc7d57bffc."
    },
    {
        "cve_id": "CVE-2021-29265",
        "code_before_change": "static ssize_t usbip_sockfd_store(struct device *dev, struct device_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\tstruct stub_device *sdev = dev_get_drvdata(dev);\n\tint sockfd = 0;\n\tstruct socket *socket;\n\tint rv;\n\n\tif (!sdev) {\n\t\tdev_err(dev, \"sdev is null\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\trv = sscanf(buf, \"%d\", &sockfd);\n\tif (rv != 1)\n\t\treturn -EINVAL;\n\n\tif (sockfd != -1) {\n\t\tint err;\n\n\t\tdev_info(dev, \"stub up\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\n\t\tif (sdev->ud.status != SDEV_ST_AVAILABLE) {\n\t\t\tdev_err(dev, \"not ready\\n\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tsocket = sockfd_lookup(sockfd, &err);\n\t\tif (!socket) {\n\t\t\tdev_err(dev, \"failed to lookup sock\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (socket->type != SOCK_STREAM) {\n\t\t\tdev_err(dev, \"Expecting SOCK_STREAM - found %d\",\n\t\t\t\tsocket->type);\n\t\t\tgoto sock_err;\n\t\t}\n\n\t\tsdev->ud.tcp_socket = socket;\n\t\tsdev->ud.sockfd = sockfd;\n\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\tsdev->ud.tcp_rx = kthread_get_run(stub_rx_loop, &sdev->ud,\n\t\t\t\t\t\t  \"stub_rx\");\n\t\tsdev->ud.tcp_tx = kthread_get_run(stub_tx_loop, &sdev->ud,\n\t\t\t\t\t\t  \"stub_tx\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tsdev->ud.status = SDEV_ST_USED;\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t} else {\n\t\tdev_info(dev, \"stub down\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tif (sdev->ud.status != SDEV_ST_USED)\n\t\t\tgoto err;\n\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\tusbip_event_add(&sdev->ud, SDEV_EVENT_DOWN);\n\t}\n\n\treturn count;\n\nsock_err:\n\tsockfd_put(socket);\nerr:\n\tspin_unlock_irq(&sdev->ud.lock);\n\treturn -EINVAL;\n}",
        "code_after_change": "static ssize_t usbip_sockfd_store(struct device *dev, struct device_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\tstruct stub_device *sdev = dev_get_drvdata(dev);\n\tint sockfd = 0;\n\tstruct socket *socket;\n\tint rv;\n\tstruct task_struct *tcp_rx = NULL;\n\tstruct task_struct *tcp_tx = NULL;\n\n\tif (!sdev) {\n\t\tdev_err(dev, \"sdev is null\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\trv = sscanf(buf, \"%d\", &sockfd);\n\tif (rv != 1)\n\t\treturn -EINVAL;\n\n\tif (sockfd != -1) {\n\t\tint err;\n\n\t\tdev_info(dev, \"stub up\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\n\t\tif (sdev->ud.status != SDEV_ST_AVAILABLE) {\n\t\t\tdev_err(dev, \"not ready\\n\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tsocket = sockfd_lookup(sockfd, &err);\n\t\tif (!socket) {\n\t\t\tdev_err(dev, \"failed to lookup sock\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (socket->type != SOCK_STREAM) {\n\t\t\tdev_err(dev, \"Expecting SOCK_STREAM - found %d\",\n\t\t\t\tsocket->type);\n\t\t\tgoto sock_err;\n\t\t}\n\n\t\t/* unlock and create threads and get tasks */\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\t\ttcp_rx = kthread_create(stub_rx_loop, &sdev->ud, \"stub_rx\");\n\t\tif (IS_ERR(tcp_rx)) {\n\t\t\tsockfd_put(socket);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttcp_tx = kthread_create(stub_tx_loop, &sdev->ud, \"stub_tx\");\n\t\tif (IS_ERR(tcp_tx)) {\n\t\t\tkthread_stop(tcp_rx);\n\t\t\tsockfd_put(socket);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* get task structs now */\n\t\tget_task_struct(tcp_rx);\n\t\tget_task_struct(tcp_tx);\n\n\t\t/* lock and update sdev->ud state */\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tsdev->ud.tcp_socket = socket;\n\t\tsdev->ud.sockfd = sockfd;\n\t\tsdev->ud.tcp_rx = tcp_rx;\n\t\tsdev->ud.tcp_tx = tcp_tx;\n\t\tsdev->ud.status = SDEV_ST_USED;\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\twake_up_process(sdev->ud.tcp_rx);\n\t\twake_up_process(sdev->ud.tcp_tx);\n\n\t} else {\n\t\tdev_info(dev, \"stub down\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tif (sdev->ud.status != SDEV_ST_USED)\n\t\t\tgoto err;\n\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\tusbip_event_add(&sdev->ud, SDEV_EVENT_DOWN);\n\t}\n\n\treturn count;\n\nsock_err:\n\tsockfd_put(socket);\nerr:\n\tspin_unlock_irq(&sdev->ud.lock);\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,8 @@\n \tint sockfd = 0;\n \tstruct socket *socket;\n \tint rv;\n+\tstruct task_struct *tcp_rx = NULL;\n+\tstruct task_struct *tcp_tx = NULL;\n \n \tif (!sdev) {\n \t\tdev_err(dev, \"sdev is null\\n\");\n@@ -39,19 +41,35 @@\n \t\t\tgoto sock_err;\n \t\t}\n \n+\t\t/* unlock and create threads and get tasks */\n+\t\tspin_unlock_irq(&sdev->ud.lock);\n+\t\ttcp_rx = kthread_create(stub_rx_loop, &sdev->ud, \"stub_rx\");\n+\t\tif (IS_ERR(tcp_rx)) {\n+\t\t\tsockfd_put(socket);\n+\t\t\treturn -EINVAL;\n+\t\t}\n+\t\ttcp_tx = kthread_create(stub_tx_loop, &sdev->ud, \"stub_tx\");\n+\t\tif (IS_ERR(tcp_tx)) {\n+\t\t\tkthread_stop(tcp_rx);\n+\t\t\tsockfd_put(socket);\n+\t\t\treturn -EINVAL;\n+\t\t}\n+\n+\t\t/* get task structs now */\n+\t\tget_task_struct(tcp_rx);\n+\t\tget_task_struct(tcp_tx);\n+\n+\t\t/* lock and update sdev->ud state */\n+\t\tspin_lock_irq(&sdev->ud.lock);\n \t\tsdev->ud.tcp_socket = socket;\n \t\tsdev->ud.sockfd = sockfd;\n-\n+\t\tsdev->ud.tcp_rx = tcp_rx;\n+\t\tsdev->ud.tcp_tx = tcp_tx;\n+\t\tsdev->ud.status = SDEV_ST_USED;\n \t\tspin_unlock_irq(&sdev->ud.lock);\n \n-\t\tsdev->ud.tcp_rx = kthread_get_run(stub_rx_loop, &sdev->ud,\n-\t\t\t\t\t\t  \"stub_rx\");\n-\t\tsdev->ud.tcp_tx = kthread_get_run(stub_tx_loop, &sdev->ud,\n-\t\t\t\t\t\t  \"stub_tx\");\n-\n-\t\tspin_lock_irq(&sdev->ud.lock);\n-\t\tsdev->ud.status = SDEV_ST_USED;\n-\t\tspin_unlock_irq(&sdev->ud.lock);\n+\t\twake_up_process(sdev->ud.tcp_rx);\n+\t\twake_up_process(sdev->ud.tcp_tx);\n \n \t} else {\n \t\tdev_info(dev, \"stub down\\n\");",
        "function_modified_lines": {
            "added": [
                "\tstruct task_struct *tcp_rx = NULL;",
                "\tstruct task_struct *tcp_tx = NULL;",
                "\t\t/* unlock and create threads and get tasks */",
                "\t\tspin_unlock_irq(&sdev->ud.lock);",
                "\t\ttcp_rx = kthread_create(stub_rx_loop, &sdev->ud, \"stub_rx\");",
                "\t\tif (IS_ERR(tcp_rx)) {",
                "\t\t\tsockfd_put(socket);",
                "\t\t\treturn -EINVAL;",
                "\t\t}",
                "\t\ttcp_tx = kthread_create(stub_tx_loop, &sdev->ud, \"stub_tx\");",
                "\t\tif (IS_ERR(tcp_tx)) {",
                "\t\t\tkthread_stop(tcp_rx);",
                "\t\t\tsockfd_put(socket);",
                "\t\t\treturn -EINVAL;",
                "\t\t}",
                "",
                "\t\t/* get task structs now */",
                "\t\tget_task_struct(tcp_rx);",
                "\t\tget_task_struct(tcp_tx);",
                "",
                "\t\t/* lock and update sdev->ud state */",
                "\t\tspin_lock_irq(&sdev->ud.lock);",
                "\t\tsdev->ud.tcp_rx = tcp_rx;",
                "\t\tsdev->ud.tcp_tx = tcp_tx;",
                "\t\tsdev->ud.status = SDEV_ST_USED;",
                "\t\twake_up_process(sdev->ud.tcp_rx);",
                "\t\twake_up_process(sdev->ud.tcp_tx);"
            ],
            "deleted": [
                "",
                "\t\tsdev->ud.tcp_rx = kthread_get_run(stub_rx_loop, &sdev->ud,",
                "\t\t\t\t\t\t  \"stub_rx\");",
                "\t\tsdev->ud.tcp_tx = kthread_get_run(stub_tx_loop, &sdev->ud,",
                "\t\t\t\t\t\t  \"stub_tx\");",
                "",
                "\t\tspin_lock_irq(&sdev->ud.lock);",
                "\t\tsdev->ud.status = SDEV_ST_USED;",
                "\t\tspin_unlock_irq(&sdev->ud.lock);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.11.7. usbip_sockfd_store in drivers/usb/usbip/stub_dev.c allows attackers to cause a denial of service (GPF) because the stub-up sequence has race conditions during an update of the local and shared status, aka CID-9380afd6df70."
    },
    {
        "cve_id": "CVE-2021-32399",
        "code_before_change": "int hci_req_sync(struct hci_dev *hdev, int (*req)(struct hci_request *req,\n\t\t\t\t\t\t  unsigned long opt),\n\t\t unsigned long opt, u32 timeout, u8 *hci_status)\n{\n\tint ret;\n\n\tif (!test_bit(HCI_UP, &hdev->flags))\n\t\treturn -ENETDOWN;\n\n\t/* Serialize all requests */\n\thci_req_sync_lock(hdev);\n\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n\thci_req_sync_unlock(hdev);\n\n\treturn ret;\n}",
        "code_after_change": "int hci_req_sync(struct hci_dev *hdev, int (*req)(struct hci_request *req,\n\t\t\t\t\t\t  unsigned long opt),\n\t\t unsigned long opt, u32 timeout, u8 *hci_status)\n{\n\tint ret;\n\n\t/* Serialize all requests */\n\thci_req_sync_lock(hdev);\n\t/* check the state after obtaing the lock to protect the HCI_UP\n\t * against any races from hci_dev_do_close when the controller\n\t * gets removed.\n\t */\n\tif (test_bit(HCI_UP, &hdev->flags))\n\t\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n\telse\n\t\tret = -ENETDOWN;\n\thci_req_sync_unlock(hdev);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,12 +4,16 @@\n {\n \tint ret;\n \n-\tif (!test_bit(HCI_UP, &hdev->flags))\n-\t\treturn -ENETDOWN;\n-\n \t/* Serialize all requests */\n \thci_req_sync_lock(hdev);\n-\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n+\t/* check the state after obtaing the lock to protect the HCI_UP\n+\t * against any races from hci_dev_do_close when the controller\n+\t * gets removed.\n+\t */\n+\tif (test_bit(HCI_UP, &hdev->flags))\n+\t\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n+\telse\n+\t\tret = -ENETDOWN;\n \thci_req_sync_unlock(hdev);\n \n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "\t/* check the state after obtaing the lock to protect the HCI_UP",
                "\t * against any races from hci_dev_do_close when the controller",
                "\t * gets removed.",
                "\t */",
                "\tif (test_bit(HCI_UP, &hdev->flags))",
                "\t\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);",
                "\telse",
                "\t\tret = -ENETDOWN;"
            ],
            "deleted": [
                "\tif (!test_bit(HCI_UP, &hdev->flags))",
                "\t\treturn -ENETDOWN;",
                "",
                "\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "net/bluetooth/hci_request.c in the Linux kernel through 5.12.2 has a race condition for removal of the HCI controller."
    },
    {
        "cve_id": "CVE-2021-3348",
        "code_before_change": "static int nbd_add_socket(struct nbd_device *nbd, unsigned long arg,\n\t\t\t  bool netlink)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct socket *sock;\n\tstruct nbd_sock **socks;\n\tstruct nbd_sock *nsock;\n\tint err;\n\n\tsock = nbd_get_socket(nbd, arg, &err);\n\tif (!sock)\n\t\treturn err;\n\n\tif (!netlink && !nbd->task_setup &&\n\t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n\t\tnbd->task_setup = current;\n\n\tif (!netlink &&\n\t    (nbd->task_setup != current ||\n\t     test_bit(NBD_RT_BOUND, &config->runtime_flags))) {\n\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\"Device being setup by another task\");\n\t\terr = -EBUSY;\n\t\tgoto put_socket;\n\t}\n\n\tnsock = kzalloc(sizeof(*nsock), GFP_KERNEL);\n\tif (!nsock) {\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tsocks = krealloc(config->socks, (config->num_connections + 1) *\n\t\t\t sizeof(struct nbd_sock *), GFP_KERNEL);\n\tif (!socks) {\n\t\tkfree(nsock);\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tconfig->socks = socks;\n\n\tnsock->fallback_index = -1;\n\tnsock->dead = false;\n\tmutex_init(&nsock->tx_lock);\n\tnsock->sock = sock;\n\tnsock->pending = NULL;\n\tnsock->sent = 0;\n\tnsock->cookie = 0;\n\tsocks[config->num_connections++] = nsock;\n\tatomic_inc(&config->live_connections);\n\n\treturn 0;\n\nput_socket:\n\tsockfd_put(sock);\n\treturn err;\n}",
        "code_after_change": "static int nbd_add_socket(struct nbd_device *nbd, unsigned long arg,\n\t\t\t  bool netlink)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct socket *sock;\n\tstruct nbd_sock **socks;\n\tstruct nbd_sock *nsock;\n\tint err;\n\n\tsock = nbd_get_socket(nbd, arg, &err);\n\tif (!sock)\n\t\treturn err;\n\n\t/*\n\t * We need to make sure we don't get any errant requests while we're\n\t * reallocating the ->socks array.\n\t */\n\tblk_mq_freeze_queue(nbd->disk->queue);\n\n\tif (!netlink && !nbd->task_setup &&\n\t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n\t\tnbd->task_setup = current;\n\n\tif (!netlink &&\n\t    (nbd->task_setup != current ||\n\t     test_bit(NBD_RT_BOUND, &config->runtime_flags))) {\n\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\"Device being setup by another task\");\n\t\terr = -EBUSY;\n\t\tgoto put_socket;\n\t}\n\n\tnsock = kzalloc(sizeof(*nsock), GFP_KERNEL);\n\tif (!nsock) {\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tsocks = krealloc(config->socks, (config->num_connections + 1) *\n\t\t\t sizeof(struct nbd_sock *), GFP_KERNEL);\n\tif (!socks) {\n\t\tkfree(nsock);\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tconfig->socks = socks;\n\n\tnsock->fallback_index = -1;\n\tnsock->dead = false;\n\tmutex_init(&nsock->tx_lock);\n\tnsock->sock = sock;\n\tnsock->pending = NULL;\n\tnsock->sent = 0;\n\tnsock->cookie = 0;\n\tsocks[config->num_connections++] = nsock;\n\tatomic_inc(&config->live_connections);\n\tblk_mq_unfreeze_queue(nbd->disk->queue);\n\n\treturn 0;\n\nput_socket:\n\tblk_mq_unfreeze_queue(nbd->disk->queue);\n\tsockfd_put(sock);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,12 @@\n \tsock = nbd_get_socket(nbd, arg, &err);\n \tif (!sock)\n \t\treturn err;\n+\n+\t/*\n+\t * We need to make sure we don't get any errant requests while we're\n+\t * reallocating the ->socks array.\n+\t */\n+\tblk_mq_freeze_queue(nbd->disk->queue);\n \n \tif (!netlink && !nbd->task_setup &&\n \t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n@@ -49,10 +55,12 @@\n \tnsock->cookie = 0;\n \tsocks[config->num_connections++] = nsock;\n \tatomic_inc(&config->live_connections);\n+\tblk_mq_unfreeze_queue(nbd->disk->queue);\n \n \treturn 0;\n \n put_socket:\n+\tblk_mq_unfreeze_queue(nbd->disk->queue);\n \tsockfd_put(sock);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * We need to make sure we don't get any errant requests while we're",
                "\t * reallocating the ->socks array.",
                "\t */",
                "\tblk_mq_freeze_queue(nbd->disk->queue);",
                "\tblk_mq_unfreeze_queue(nbd->disk->queue);",
                "\tblk_mq_unfreeze_queue(nbd->disk->queue);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "nbd_add_socket in drivers/block/nbd.c in the Linux kernel through 5.10.12 has an ndb_queue_rq use-after-free that could be triggered by local attackers (with access to the nbd device) via an I/O request at a certain point during device setup, aka CID-b98e762e3d71."
    },
    {
        "cve_id": "CVE-2021-3573",
        "code_before_change": "void hci_sock_dev_event(struct hci_dev *hdev, int event)\n{\n\tBT_DBG(\"hdev %s event %d\", hdev->name, event);\n\n\tif (atomic_read(&monitor_promisc)) {\n\t\tstruct sk_buff *skb;\n\n\t\t/* Send event to monitor */\n\t\tskb = create_monitor_event(hdev, event);\n\t\tif (skb) {\n\t\t\thci_send_to_channel(HCI_CHANNEL_MONITOR, skb,\n\t\t\t\t\t    HCI_SOCK_TRUSTED, NULL);\n\t\t\tkfree_skb(skb);\n\t\t}\n\t}\n\n\tif (event <= HCI_DEV_DOWN) {\n\t\tstruct hci_ev_si_device ev;\n\n\t\t/* Send event to sockets */\n\t\tev.event  = event;\n\t\tev.dev_id = hdev->id;\n\t\thci_si_event(NULL, HCI_EV_SI_DEVICE, sizeof(ev), &ev);\n\t}\n\n\tif (event == HCI_DEV_UNREG) {\n\t\tstruct sock *sk;\n\n\t\t/* Detach sockets from device */\n\t\tread_lock(&hci_sk_list.lock);\n\t\tsk_for_each(sk, &hci_sk_list.head) {\n\t\t\tbh_lock_sock_nested(sk);\n\t\t\tif (hci_pi(sk)->hdev == hdev) {\n\t\t\t\thci_pi(sk)->hdev = NULL;\n\t\t\t\tsk->sk_err = EPIPE;\n\t\t\t\tsk->sk_state = BT_OPEN;\n\t\t\t\tsk->sk_state_change(sk);\n\n\t\t\t\thci_dev_put(hdev);\n\t\t\t}\n\t\t\tbh_unlock_sock(sk);\n\t\t}\n\t\tread_unlock(&hci_sk_list.lock);\n\t}\n}",
        "code_after_change": "void hci_sock_dev_event(struct hci_dev *hdev, int event)\n{\n\tBT_DBG(\"hdev %s event %d\", hdev->name, event);\n\n\tif (atomic_read(&monitor_promisc)) {\n\t\tstruct sk_buff *skb;\n\n\t\t/* Send event to monitor */\n\t\tskb = create_monitor_event(hdev, event);\n\t\tif (skb) {\n\t\t\thci_send_to_channel(HCI_CHANNEL_MONITOR, skb,\n\t\t\t\t\t    HCI_SOCK_TRUSTED, NULL);\n\t\t\tkfree_skb(skb);\n\t\t}\n\t}\n\n\tif (event <= HCI_DEV_DOWN) {\n\t\tstruct hci_ev_si_device ev;\n\n\t\t/* Send event to sockets */\n\t\tev.event  = event;\n\t\tev.dev_id = hdev->id;\n\t\thci_si_event(NULL, HCI_EV_SI_DEVICE, sizeof(ev), &ev);\n\t}\n\n\tif (event == HCI_DEV_UNREG) {\n\t\tstruct sock *sk;\n\n\t\t/* Detach sockets from device */\n\t\tread_lock(&hci_sk_list.lock);\n\t\tsk_for_each(sk, &hci_sk_list.head) {\n\t\t\tlock_sock(sk);\n\t\t\tif (hci_pi(sk)->hdev == hdev) {\n\t\t\t\thci_pi(sk)->hdev = NULL;\n\t\t\t\tsk->sk_err = EPIPE;\n\t\t\t\tsk->sk_state = BT_OPEN;\n\t\t\t\tsk->sk_state_change(sk);\n\n\t\t\t\thci_dev_put(hdev);\n\t\t\t}\n\t\t\trelease_sock(sk);\n\t\t}\n\t\tread_unlock(&hci_sk_list.lock);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,7 +29,7 @@\n \t\t/* Detach sockets from device */\n \t\tread_lock(&hci_sk_list.lock);\n \t\tsk_for_each(sk, &hci_sk_list.head) {\n-\t\t\tbh_lock_sock_nested(sk);\n+\t\t\tlock_sock(sk);\n \t\t\tif (hci_pi(sk)->hdev == hdev) {\n \t\t\t\thci_pi(sk)->hdev = NULL;\n \t\t\t\tsk->sk_err = EPIPE;\n@@ -38,7 +38,7 @@\n \n \t\t\t\thci_dev_put(hdev);\n \t\t\t}\n-\t\t\tbh_unlock_sock(sk);\n+\t\t\trelease_sock(sk);\n \t\t}\n \t\tread_unlock(&hci_sk_list.lock);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tlock_sock(sk);",
                "\t\t\trelease_sock(sk);"
            ],
            "deleted": [
                "\t\t\tbh_lock_sock_nested(sk);",
                "\t\t\tbh_unlock_sock(sk);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free in function hci_sock_bound_ioctl() of the Linux kernel HCI subsystem was found in the way user calls ioct HCIUNBLOCKADDR or other way triggers race condition of the call hci_unregister_dev() together with one of the calls hci_sock_blacklist_add(), hci_sock_blacklist_del(), hci_get_conn_info(), hci_get_auth_info(). A privileged local user could use this flaw to crash the system or escalate their privileges on the system. This flaw affects the Linux kernel versions prior to 5.13-rc5."
    },
    {
        "cve_id": "CVE-2021-3609",
        "code_before_change": "static int bcm_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net;\n\tstruct bcm_sock *bo;\n\tstruct bcm_op *op, *next;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\tbo = bcm_sk(sk);\n\n\t/* remove bcm_ops, timer, rx_unregister(), etc. */\n\n\tspin_lock(&bcm_notifier_lock);\n\twhile (bcm_busy_notifier == bo) {\n\t\tspin_unlock(&bcm_notifier_lock);\n\t\tschedule_timeout_uninterruptible(1);\n\t\tspin_lock(&bcm_notifier_lock);\n\t}\n\tlist_del(&bo->notifier);\n\tspin_unlock(&bcm_notifier_lock);\n\n\tlock_sock(sk);\n\n\tlist_for_each_entry_safe(op, next, &bo->tx_ops, list)\n\t\tbcm_remove_op(op);\n\n\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list) {\n\t\t/*\n\t\t * Don't care if we're bound or not (due to netdev problems)\n\t\t * can_rx_unregister() is always a save thing to do here.\n\t\t */\n\t\tif (op->ifindex) {\n\t\t\t/*\n\t\t\t * Only remove subscriptions that had not\n\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t * in bcm_notifier()\n\t\t\t */\n\t\t\tif (op->rx_reg_dev) {\n\t\t\t\tstruct net_device *dev;\n\n\t\t\t\tdev = dev_get_by_index(net, op->ifindex);\n\t\t\t\tif (dev) {\n\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\tdev_put(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tcan_rx_unregister(net, NULL, op->can_id,\n\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t  bcm_rx_handler, op);\n\n\t\tbcm_remove_op(op);\n\t}\n\n#if IS_ENABLED(CONFIG_PROC_FS)\n\t/* remove procfs entry */\n\tif (net->can.bcmproc_dir && bo->bcm_proc_read)\n\t\tremove_proc_entry(bo->procname, net->can.bcmproc_dir);\n#endif /* CONFIG_PROC_FS */\n\n\t/* remove device reference */\n\tif (bo->bound) {\n\t\tbo->bound   = 0;\n\t\tbo->ifindex = 0;\n\t}\n\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\trelease_sock(sk);\n\tsock_put(sk);\n\n\treturn 0;\n}",
        "code_after_change": "static int bcm_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net;\n\tstruct bcm_sock *bo;\n\tstruct bcm_op *op, *next;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\tbo = bcm_sk(sk);\n\n\t/* remove bcm_ops, timer, rx_unregister(), etc. */\n\n\tspin_lock(&bcm_notifier_lock);\n\twhile (bcm_busy_notifier == bo) {\n\t\tspin_unlock(&bcm_notifier_lock);\n\t\tschedule_timeout_uninterruptible(1);\n\t\tspin_lock(&bcm_notifier_lock);\n\t}\n\tlist_del(&bo->notifier);\n\tspin_unlock(&bcm_notifier_lock);\n\n\tlock_sock(sk);\n\n\tlist_for_each_entry_safe(op, next, &bo->tx_ops, list)\n\t\tbcm_remove_op(op);\n\n\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list) {\n\t\t/*\n\t\t * Don't care if we're bound or not (due to netdev problems)\n\t\t * can_rx_unregister() is always a save thing to do here.\n\t\t */\n\t\tif (op->ifindex) {\n\t\t\t/*\n\t\t\t * Only remove subscriptions that had not\n\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t * in bcm_notifier()\n\t\t\t */\n\t\t\tif (op->rx_reg_dev) {\n\t\t\t\tstruct net_device *dev;\n\n\t\t\t\tdev = dev_get_by_index(net, op->ifindex);\n\t\t\t\tif (dev) {\n\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\tdev_put(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tcan_rx_unregister(net, NULL, op->can_id,\n\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t  bcm_rx_handler, op);\n\n\t}\n\n\tsynchronize_rcu();\n\n\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list)\n\t\tbcm_remove_op(op);\n\n#if IS_ENABLED(CONFIG_PROC_FS)\n\t/* remove procfs entry */\n\tif (net->can.bcmproc_dir && bo->bcm_proc_read)\n\t\tremove_proc_entry(bo->procname, net->can.bcmproc_dir);\n#endif /* CONFIG_PROC_FS */\n\n\t/* remove device reference */\n\tif (bo->bound) {\n\t\tbo->bound   = 0;\n\t\tbo->ifindex = 0;\n\t}\n\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\trelease_sock(sk);\n\tsock_put(sk);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -52,8 +52,12 @@\n \t\t\t\t\t  REGMASK(op->can_id),\n \t\t\t\t\t  bcm_rx_handler, op);\n \n+\t}\n+\n+\tsynchronize_rcu();\n+\n+\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list)\n \t\tbcm_remove_op(op);\n-\t}\n \n #if IS_ENABLED(CONFIG_PROC_FS)\n \t/* remove procfs entry */",
        "function_modified_lines": {
            "added": [
                "\t}",
                "",
                "\tsynchronize_rcu();",
                "",
                "\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list)"
            ],
            "deleted": [
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": ".A flaw was found in the CAN BCM networking protocol in the Linux kernel, where a local attacker can abuse a flaw in the CAN subsystem to corrupt memory, crash the system or escalate privileges. This race condition in net/can/bcm.c in the Linux kernel allows for local privilege escalation to root."
    },
    {
        "cve_id": "CVE-2021-3609",
        "code_before_change": "static int bcm_delete_rx_op(struct list_head *ops, struct bcm_msg_head *mh,\n\t\t\t    int ifindex)\n{\n\tstruct bcm_op *op, *n;\n\n\tlist_for_each_entry_safe(op, n, ops, list) {\n\t\tif ((op->can_id == mh->can_id) && (op->ifindex == ifindex) &&\n\t\t    (op->flags & CAN_FD_FRAME) == (mh->flags & CAN_FD_FRAME)) {\n\n\t\t\t/*\n\t\t\t * Don't care if we're bound or not (due to netdev\n\t\t\t * problems) can_rx_unregister() is always a save\n\t\t\t * thing to do here.\n\t\t\t */\n\t\t\tif (op->ifindex) {\n\t\t\t\t/*\n\t\t\t\t * Only remove subscriptions that had not\n\t\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t\t * in bcm_notifier()\n\t\t\t\t */\n\t\t\t\tif (op->rx_reg_dev) {\n\t\t\t\t\tstruct net_device *dev;\n\n\t\t\t\t\tdev = dev_get_by_index(sock_net(op->sk),\n\t\t\t\t\t\t\t       op->ifindex);\n\t\t\t\t\tif (dev) {\n\t\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\t\tdev_put(dev);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else\n\t\t\t\tcan_rx_unregister(sock_net(op->sk), NULL,\n\t\t\t\t\t\t  op->can_id,\n\t\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t\t  bcm_rx_handler, op);\n\n\t\t\tlist_del(&op->list);\n\t\t\tbcm_remove_op(op);\n\t\t\treturn 1; /* done */\n\t\t}\n\t}\n\n\treturn 0; /* not found */\n}",
        "code_after_change": "static int bcm_delete_rx_op(struct list_head *ops, struct bcm_msg_head *mh,\n\t\t\t    int ifindex)\n{\n\tstruct bcm_op *op, *n;\n\n\tlist_for_each_entry_safe(op, n, ops, list) {\n\t\tif ((op->can_id == mh->can_id) && (op->ifindex == ifindex) &&\n\t\t    (op->flags & CAN_FD_FRAME) == (mh->flags & CAN_FD_FRAME)) {\n\n\t\t\t/*\n\t\t\t * Don't care if we're bound or not (due to netdev\n\t\t\t * problems) can_rx_unregister() is always a save\n\t\t\t * thing to do here.\n\t\t\t */\n\t\t\tif (op->ifindex) {\n\t\t\t\t/*\n\t\t\t\t * Only remove subscriptions that had not\n\t\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t\t * in bcm_notifier()\n\t\t\t\t */\n\t\t\t\tif (op->rx_reg_dev) {\n\t\t\t\t\tstruct net_device *dev;\n\n\t\t\t\t\tdev = dev_get_by_index(sock_net(op->sk),\n\t\t\t\t\t\t\t       op->ifindex);\n\t\t\t\t\tif (dev) {\n\t\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\t\tdev_put(dev);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else\n\t\t\t\tcan_rx_unregister(sock_net(op->sk), NULL,\n\t\t\t\t\t\t  op->can_id,\n\t\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t\t  bcm_rx_handler, op);\n\n\t\t\tlist_del(&op->list);\n\t\t\tsynchronize_rcu();\n\t\t\tbcm_remove_op(op);\n\t\t\treturn 1; /* done */\n\t\t}\n\t}\n\n\treturn 0; /* not found */\n}",
        "patch": "--- code before\n+++ code after\n@@ -35,6 +35,7 @@\n \t\t\t\t\t\t  bcm_rx_handler, op);\n \n \t\t\tlist_del(&op->list);\n+\t\t\tsynchronize_rcu();\n \t\t\tbcm_remove_op(op);\n \t\t\treturn 1; /* done */\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tsynchronize_rcu();"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": ".A flaw was found in the CAN BCM networking protocol in the Linux kernel, where a local attacker can abuse a flaw in the CAN subsystem to corrupt memory, crash the system or escalate privileges. This race condition in net/can/bcm.c in the Linux kernel allows for local privilege escalation to root."
    },
    {
        "cve_id": "CVE-2021-3640",
        "code_before_change": "static int sco_sock_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t    size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\terr = sock_error(sk);\n\tif (err)\n\t\treturn err;\n\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == BT_CONNECTED)\n\t\terr = sco_send_frame(sk, msg, len);\n\telse\n\t\terr = -ENOTCONN;\n\n\trelease_sock(sk);\n\treturn err;\n}",
        "code_after_change": "static int sco_sock_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t    size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tvoid *buf;\n\tint err;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\terr = sock_error(sk);\n\tif (err)\n\t\treturn err;\n\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tbuf = kmalloc(len, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tif (memcpy_from_msg(buf, msg, len)) {\n\t\tkfree(buf);\n\t\treturn -EFAULT;\n\t}\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == BT_CONNECTED)\n\t\terr = sco_send_frame(sk, buf, len, msg->msg_flags);\n\telse\n\t\terr = -ENOTCONN;\n\n\trelease_sock(sk);\n\tkfree(buf);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n \t\t\t    size_t len)\n {\n \tstruct sock *sk = sock->sk;\n+\tvoid *buf;\n \tint err;\n \n \tBT_DBG(\"sock %p, sk %p\", sock, sk);\n@@ -13,13 +14,23 @@\n \tif (msg->msg_flags & MSG_OOB)\n \t\treturn -EOPNOTSUPP;\n \n+\tbuf = kmalloc(len, GFP_KERNEL);\n+\tif (!buf)\n+\t\treturn -ENOMEM;\n+\n+\tif (memcpy_from_msg(buf, msg, len)) {\n+\t\tkfree(buf);\n+\t\treturn -EFAULT;\n+\t}\n+\n \tlock_sock(sk);\n \n \tif (sk->sk_state == BT_CONNECTED)\n-\t\terr = sco_send_frame(sk, msg, len);\n+\t\terr = sco_send_frame(sk, buf, len, msg->msg_flags);\n \telse\n \t\terr = -ENOTCONN;\n \n \trelease_sock(sk);\n+\tkfree(buf);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tvoid *buf;",
                "\tbuf = kmalloc(len, GFP_KERNEL);",
                "\tif (!buf)",
                "\t\treturn -ENOMEM;",
                "",
                "\tif (memcpy_from_msg(buf, msg, len)) {",
                "\t\tkfree(buf);",
                "\t\treturn -EFAULT;",
                "\t}",
                "",
                "\t\terr = sco_send_frame(sk, buf, len, msg->msg_flags);",
                "\tkfree(buf);"
            ],
            "deleted": [
                "\t\terr = sco_send_frame(sk, msg, len);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw use-after-free in function sco_sock_sendmsg() of the Linux kernel HCI subsystem was found in the way user calls ioct UFFDIO_REGISTER or other way triggers race condition of the call sco_conn_del() together with the call sco_sock_sendmsg() with the expected controllable faulting memory page. A privileged local user could use this flaw to crash the system or escalate their privileges on the system."
    },
    {
        "cve_id": "CVE-2021-3752",
        "code_before_change": "static void l2cap_sock_close_cb(struct l2cap_chan *chan)\n{\n\tstruct sock *sk = chan->data;\n\n\tl2cap_sock_kill(sk);\n}",
        "code_after_change": "static void l2cap_sock_close_cb(struct l2cap_chan *chan)\n{\n\tstruct sock *sk = chan->data;\n\n\tif (!sk)\n\t\treturn;\n\n\tl2cap_sock_kill(sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,5 +2,8 @@\n {\n \tstruct sock *sk = chan->data;\n \n+\tif (!sk)\n+\t\treturn;\n+\n \tl2cap_sock_kill(sk);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!sk)",
                "\t\treturn;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s Bluetooth subsystem in the way user calls connect to the socket and disconnect simultaneously due to a race condition. This flaw allows a user to crash the system or escalate their privileges. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability."
    },
    {
        "cve_id": "CVE-2021-3752",
        "code_before_change": "static void l2cap_sock_destruct(struct sock *sk)\n{\n\tBT_DBG(\"sk %p\", sk);\n\n\tif (l2cap_pi(sk)->chan)\n\t\tl2cap_chan_put(l2cap_pi(sk)->chan);\n\n\tif (l2cap_pi(sk)->rx_busy_skb) {\n\t\tkfree_skb(l2cap_pi(sk)->rx_busy_skb);\n\t\tl2cap_pi(sk)->rx_busy_skb = NULL;\n\t}\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tskb_queue_purge(&sk->sk_write_queue);\n}",
        "code_after_change": "static void l2cap_sock_destruct(struct sock *sk)\n{\n\tBT_DBG(\"sk %p\", sk);\n\n\tif (l2cap_pi(sk)->chan) {\n\t\tl2cap_pi(sk)->chan->data = NULL;\n\t\tl2cap_chan_put(l2cap_pi(sk)->chan);\n\t}\n\n\tif (l2cap_pi(sk)->rx_busy_skb) {\n\t\tkfree_skb(l2cap_pi(sk)->rx_busy_skb);\n\t\tl2cap_pi(sk)->rx_busy_skb = NULL;\n\t}\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tskb_queue_purge(&sk->sk_write_queue);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,8 +2,10 @@\n {\n \tBT_DBG(\"sk %p\", sk);\n \n-\tif (l2cap_pi(sk)->chan)\n+\tif (l2cap_pi(sk)->chan) {\n+\t\tl2cap_pi(sk)->chan->data = NULL;\n \t\tl2cap_chan_put(l2cap_pi(sk)->chan);\n+\t}\n \n \tif (l2cap_pi(sk)->rx_busy_skb) {\n \t\tkfree_skb(l2cap_pi(sk)->rx_busy_skb);",
        "function_modified_lines": {
            "added": [
                "\tif (l2cap_pi(sk)->chan) {",
                "\t\tl2cap_pi(sk)->chan->data = NULL;",
                "\t}"
            ],
            "deleted": [
                "\tif (l2cap_pi(sk)->chan)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s Bluetooth subsystem in the way user calls connect to the socket and disconnect simultaneously due to a race condition. This flaw allows a user to crash the system or escalate their privileges. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability."
    },
    {
        "cve_id": "CVE-2021-3752",
        "code_before_change": "static void l2cap_sock_teardown_cb(struct l2cap_chan *chan, int err)\n{\n\tstruct sock *sk = chan->data;\n\tstruct sock *parent;\n\n\tBT_DBG(\"chan %p state %s\", chan, state_to_string(chan->state));\n\n\t/* This callback can be called both for server (BT_LISTEN)\n\t * sockets as well as \"normal\" ones. To avoid lockdep warnings\n\t * with child socket locking (through l2cap_sock_cleanup_listen)\n\t * we need separation into separate nesting levels. The simplest\n\t * way to accomplish this is to inherit the nesting level used\n\t * for the channel.\n\t */\n\tlock_sock_nested(sk, atomic_read(&chan->nesting));\n\n\tparent = bt_sk(sk)->parent;\n\n\tswitch (chan->state) {\n\tcase BT_OPEN:\n\tcase BT_BOUND:\n\tcase BT_CLOSED:\n\t\tbreak;\n\tcase BT_LISTEN:\n\t\tl2cap_sock_cleanup_listen(sk);\n\t\tsk->sk_state = BT_CLOSED;\n\t\tchan->state = BT_CLOSED;\n\n\t\tbreak;\n\tdefault:\n\t\tsk->sk_state = BT_CLOSED;\n\t\tchan->state = BT_CLOSED;\n\n\t\tsk->sk_err = err;\n\n\t\tif (parent) {\n\t\t\tbt_accept_unlink(sk);\n\t\t\tparent->sk_data_ready(parent);\n\t\t} else {\n\t\t\tsk->sk_state_change(sk);\n\t\t}\n\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\n\t/* Only zap after cleanup to avoid use after free race */\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n}",
        "code_after_change": "static void l2cap_sock_teardown_cb(struct l2cap_chan *chan, int err)\n{\n\tstruct sock *sk = chan->data;\n\tstruct sock *parent;\n\n\tif (!sk)\n\t\treturn;\n\n\tBT_DBG(\"chan %p state %s\", chan, state_to_string(chan->state));\n\n\t/* This callback can be called both for server (BT_LISTEN)\n\t * sockets as well as \"normal\" ones. To avoid lockdep warnings\n\t * with child socket locking (through l2cap_sock_cleanup_listen)\n\t * we need separation into separate nesting levels. The simplest\n\t * way to accomplish this is to inherit the nesting level used\n\t * for the channel.\n\t */\n\tlock_sock_nested(sk, atomic_read(&chan->nesting));\n\n\tparent = bt_sk(sk)->parent;\n\n\tswitch (chan->state) {\n\tcase BT_OPEN:\n\tcase BT_BOUND:\n\tcase BT_CLOSED:\n\t\tbreak;\n\tcase BT_LISTEN:\n\t\tl2cap_sock_cleanup_listen(sk);\n\t\tsk->sk_state = BT_CLOSED;\n\t\tchan->state = BT_CLOSED;\n\n\t\tbreak;\n\tdefault:\n\t\tsk->sk_state = BT_CLOSED;\n\t\tchan->state = BT_CLOSED;\n\n\t\tsk->sk_err = err;\n\n\t\tif (parent) {\n\t\t\tbt_accept_unlink(sk);\n\t\t\tparent->sk_data_ready(parent);\n\t\t} else {\n\t\t\tsk->sk_state_change(sk);\n\t\t}\n\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\n\t/* Only zap after cleanup to avoid use after free race */\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,9 @@\n {\n \tstruct sock *sk = chan->data;\n \tstruct sock *parent;\n+\n+\tif (!sk)\n+\t\treturn;\n \n \tBT_DBG(\"chan %p state %s\", chan, state_to_string(chan->state));\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (!sk)",
                "\t\treturn;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s Bluetooth subsystem in the way user calls connect to the socket and disconnect simultaneously due to a race condition. This flaw allows a user to crash the system or escalate their privileges. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability."
    },
    {
        "cve_id": "CVE-2021-39648",
        "code_before_change": "static ssize_t gadget_dev_desc_UDC_show(struct config_item *item, char *page)\n{\n\tchar *udc_name = to_gadget_info(item)->composite.gadget_driver.udc_name;\n\n\treturn sprintf(page, \"%s\\n\", udc_name ?: \"\");\n}",
        "code_after_change": "static ssize_t gadget_dev_desc_UDC_show(struct config_item *item, char *page)\n{\n\tstruct gadget_info *gi = to_gadget_info(item);\n\tchar *udc_name;\n\tint ret;\n\n\tmutex_lock(&gi->lock);\n\tudc_name = gi->composite.gadget_driver.udc_name;\n\tret = sprintf(page, \"%s\\n\", udc_name ?: \"\");\n\tmutex_unlock(&gi->lock);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,13 @@\n static ssize_t gadget_dev_desc_UDC_show(struct config_item *item, char *page)\n {\n-\tchar *udc_name = to_gadget_info(item)->composite.gadget_driver.udc_name;\n+\tstruct gadget_info *gi = to_gadget_info(item);\n+\tchar *udc_name;\n+\tint ret;\n \n-\treturn sprintf(page, \"%s\\n\", udc_name ?: \"\");\n+\tmutex_lock(&gi->lock);\n+\tudc_name = gi->composite.gadget_driver.udc_name;\n+\tret = sprintf(page, \"%s\\n\", udc_name ?: \"\");\n+\tmutex_unlock(&gi->lock);\n+\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct gadget_info *gi = to_gadget_info(item);",
                "\tchar *udc_name;",
                "\tint ret;",
                "\tmutex_lock(&gi->lock);",
                "\tudc_name = gi->composite.gadget_driver.udc_name;",
                "\tret = sprintf(page, \"%s\\n\", udc_name ?: \"\");",
                "\tmutex_unlock(&gi->lock);",
                "",
                "\treturn ret;"
            ],
            "deleted": [
                "\tchar *udc_name = to_gadget_info(item)->composite.gadget_driver.udc_name;",
                "\treturn sprintf(page, \"%s\\n\", udc_name ?: \"\");"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In gadget_dev_desc_UDC_show of configfs.c, there is a possible disclosure of kernel heap memory due to a race condition. This could lead to local information disclosure with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-160822094References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2021-39686",
        "code_before_change": "static void binder_free_proc(struct binder_proc *proc)\n{\n\tstruct binder_device *device;\n\n\tBUG_ON(!list_empty(&proc->todo));\n\tBUG_ON(!list_empty(&proc->delivered_death));\n\tif (proc->outstanding_txns)\n\t\tpr_warn(\"%s: Unexpected outstanding_txns %d\\n\",\n\t\t\t__func__, proc->outstanding_txns);\n\tdevice = container_of(proc->context, struct binder_device, context);\n\tif (refcount_dec_and_test(&device->ref)) {\n\t\tkfree(proc->context->name);\n\t\tkfree(device);\n\t}\n\tbinder_alloc_deferred_release(&proc->alloc);\n\tput_task_struct(proc->tsk);\n\tbinder_stats_deleted(BINDER_STAT_PROC);\n\tkfree(proc);\n}",
        "code_after_change": "static void binder_free_proc(struct binder_proc *proc)\n{\n\tstruct binder_device *device;\n\n\tBUG_ON(!list_empty(&proc->todo));\n\tBUG_ON(!list_empty(&proc->delivered_death));\n\tif (proc->outstanding_txns)\n\t\tpr_warn(\"%s: Unexpected outstanding_txns %d\\n\",\n\t\t\t__func__, proc->outstanding_txns);\n\tdevice = container_of(proc->context, struct binder_device, context);\n\tif (refcount_dec_and_test(&device->ref)) {\n\t\tkfree(proc->context->name);\n\t\tkfree(device);\n\t}\n\tbinder_alloc_deferred_release(&proc->alloc);\n\tput_task_struct(proc->tsk);\n\tput_cred(proc->cred);\n\tbinder_stats_deleted(BINDER_STAT_PROC);\n\tkfree(proc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,6 +14,7 @@\n \t}\n \tbinder_alloc_deferred_release(&proc->alloc);\n \tput_task_struct(proc->tsk);\n+\tput_cred(proc->cred);\n \tbinder_stats_deleted(BINDER_STAT_PROC);\n \tkfree(proc);\n }",
        "function_modified_lines": {
            "added": [
                "\tput_cred(proc->cred);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In several functions of binder.c, there is a possible way to represent the wrong domain to SELinux due to a race condition. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-200688826References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2021-39686",
        "code_before_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\tstrscpy(e->context_name, proc->context->name, BINDERFS_MAX_NAME);\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle, %u\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid, tr->target.handle);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc->pid == proc->pid) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (WARN_ON(proc == target_proc)) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\n\t\t/*\n\t\t * Arguably this should be the task's subjective LSM secid but\n\t\t * we can't reliably access the subjective creds of a task\n\t\t * other than our own so we must use the objective creds, which\n\t\t * are safe to access.  The downside is that if a task is\n\t\t * temporarily overriding it's creds it will not be reflected\n\t\t * here; however, it isn't clear that binder would handle that\n\t\t * case well anyway.\n\t\t */\n\t\tsecurity_task_getsecid_obj(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY), current->tgid);\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tint err;\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\terr = binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  t->buffer, buf_offset,\n\t\t\t\t\t\t  secctx, secctx_sz);\n\t\tif (err) {\n\t\t\tt->security_ctx = 0;\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\tt->buffer->clear_on_free = !!(t->flags & TF_CLEAR_BUF);\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size -\n\t\tALIGN(secctx_sz, sizeof(u64));\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  &object_offset,\n\t\t\t\t\t\t  t->buffer,\n\t\t\t\t\t\t  buffer_offset,\n\t\t\t\t\t\t  sizeof(object_offset))) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tfp->pad_binder = 0;\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tbp, sizeof(*bp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\tif (t->buffer->oneway_spam_suspect)\n\t\ttcomplete->type = BINDER_WORK_TRANSACTION_ONEWAY_SPAM_SUSPECT;\n\telse\n\t\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead || target_proc->is_frozen) {\n\t\t\treturn_error = target_thread->is_dead ?\n\t\t\t\tBR_DEAD_REPLY : BR_FROZEN_REPLY;\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\ttarget_proc->outstanding_txns++;\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\treturn_error = binder_proc_transaction(t,\n\t\t\t\ttarget_proc, target_thread);\n\t\tif (return_error) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\treturn_error = binder_proc_transaction(t, target_proc, NULL);\n\t\tif (return_error)\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tif (trace_binder_txn_latency_free_enabled())\n\t\tbinder_txn_latency_free(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
        "code_after_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\tstrscpy(e->context_name, proc->context->name, BINDERFS_MAX_NAME);\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle, %u\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid, tr->target.handle);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc->pid == proc->pid) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (WARN_ON(proc == target_proc)) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = proc->cred->euid;\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\n\t\t/*\n\t\t * Arguably this should be the task's subjective LSM secid but\n\t\t * we can't reliably access the subjective creds of a task\n\t\t * other than our own so we must use the objective creds, which\n\t\t * are safe to access.  The downside is that if a task is\n\t\t * temporarily overriding it's creds it will not be reflected\n\t\t * here; however, it isn't clear that binder would handle that\n\t\t * case well anyway.\n\t\t */\n\t\tsecurity_task_getsecid_obj(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY), current->tgid);\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tint err;\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\terr = binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  t->buffer, buf_offset,\n\t\t\t\t\t\t  secctx, secctx_sz);\n\t\tif (err) {\n\t\t\tt->security_ctx = 0;\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\tt->buffer->clear_on_free = !!(t->flags & TF_CLEAR_BUF);\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size -\n\t\tALIGN(secctx_sz, sizeof(u64));\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  &object_offset,\n\t\t\t\t\t\t  t->buffer,\n\t\t\t\t\t\t  buffer_offset,\n\t\t\t\t\t\t  sizeof(object_offset))) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tfp->pad_binder = 0;\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tbp, sizeof(*bp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\tif (t->buffer->oneway_spam_suspect)\n\t\ttcomplete->type = BINDER_WORK_TRANSACTION_ONEWAY_SPAM_SUSPECT;\n\telse\n\t\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead || target_proc->is_frozen) {\n\t\t\treturn_error = target_thread->is_dead ?\n\t\t\t\tBR_DEAD_REPLY : BR_FROZEN_REPLY;\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\ttarget_proc->outstanding_txns++;\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\treturn_error = binder_proc_transaction(t,\n\t\t\t\ttarget_proc, target_thread);\n\t\tif (return_error) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\treturn_error = binder_proc_transaction(t, target_proc, NULL);\n\t\tif (return_error)\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tif (trace_binder_txn_latency_free_enabled())\n\t\tbinder_txn_latency_free(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -266,7 +266,7 @@\n \t\tt->from = thread;\n \telse\n \t\tt->from = NULL;\n-\tt->sender_euid = task_euid(proc->tsk);\n+\tt->sender_euid = proc->cred->euid;\n \tt->to_proc = target_proc;\n \tt->to_thread = target_thread;\n \tt->code = tr->code;",
        "function_modified_lines": {
            "added": [
                "\tt->sender_euid = proc->cred->euid;"
            ],
            "deleted": [
                "\tt->sender_euid = task_euid(proc->tsk);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In several functions of binder.c, there is a possible way to represent the wrong domain to SELinux due to a race condition. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-200688826References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2021-39686",
        "code_before_change": "static int binder_open(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc, *itr;\n\tstruct binder_device *binder_dev;\n\tstruct binderfs_info *info;\n\tstruct dentry *binder_binderfs_dir_entry_proc = NULL;\n\tbool existing_pid = false;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE, \"%s: %d:%d\\n\", __func__,\n\t\t     current->group_leader->pid, current->pid);\n\n\tproc = kzalloc(sizeof(*proc), GFP_KERNEL);\n\tif (proc == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&proc->inner_lock);\n\tspin_lock_init(&proc->outer_lock);\n\tget_task_struct(current->group_leader);\n\tproc->tsk = current->group_leader;\n\tINIT_LIST_HEAD(&proc->todo);\n\tinit_waitqueue_head(&proc->freeze_wait);\n\tproc->default_priority = task_nice(current);\n\t/* binderfs stashes devices in i_private */\n\tif (is_binderfs_device(nodp)) {\n\t\tbinder_dev = nodp->i_private;\n\t\tinfo = nodp->i_sb->s_fs_info;\n\t\tbinder_binderfs_dir_entry_proc = info->proc_log_dir;\n\t} else {\n\t\tbinder_dev = container_of(filp->private_data,\n\t\t\t\t\t  struct binder_device, miscdev);\n\t}\n\trefcount_inc(&binder_dev->ref);\n\tproc->context = &binder_dev->context;\n\tbinder_alloc_init(&proc->alloc);\n\n\tbinder_stats_created(BINDER_STAT_PROC);\n\tproc->pid = current->group_leader->pid;\n\tINIT_LIST_HEAD(&proc->delivered_death);\n\tINIT_LIST_HEAD(&proc->waiting_threads);\n\tfilp->private_data = proc;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(itr, &binder_procs, proc_node) {\n\t\tif (itr->pid == proc->pid) {\n\t\t\texisting_pid = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\thlist_add_head(&proc->proc_node, &binder_procs);\n\tmutex_unlock(&binder_procs_lock);\n\n\tif (binder_debugfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * proc debug entries are shared between contexts.\n\t\t * Only create for the first PID to avoid debugfs log spamming\n\t\t * The printing code will anyway print all contexts for a given\n\t\t * PID so this is not a problem.\n\t\t */\n\t\tproc->debugfs_entry = debugfs_create_file(strbuf, 0444,\n\t\t\tbinder_debugfs_dir_entry_proc,\n\t\t\t(void *)(unsigned long)proc->pid,\n\t\t\t&proc_fops);\n\t}\n\n\tif (binder_binderfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\t\tstruct dentry *binderfs_entry;\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * Similar to debugfs, the process specific log file is shared\n\t\t * between contexts. Only create for the first PID.\n\t\t * This is ok since same as debugfs, the log file will contain\n\t\t * information on all contexts of a given PID.\n\t\t */\n\t\tbinderfs_entry = binderfs_create_file(binder_binderfs_dir_entry_proc,\n\t\t\tstrbuf, &proc_fops, (void *)(unsigned long)proc->pid);\n\t\tif (!IS_ERR(binderfs_entry)) {\n\t\t\tproc->binderfs_entry = binderfs_entry;\n\t\t} else {\n\t\t\tint error;\n\n\t\t\terror = PTR_ERR(binderfs_entry);\n\t\t\tpr_warn(\"Unable to create file %s in binderfs (error %d)\\n\",\n\t\t\t\tstrbuf, error);\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int binder_open(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc, *itr;\n\tstruct binder_device *binder_dev;\n\tstruct binderfs_info *info;\n\tstruct dentry *binder_binderfs_dir_entry_proc = NULL;\n\tbool existing_pid = false;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE, \"%s: %d:%d\\n\", __func__,\n\t\t     current->group_leader->pid, current->pid);\n\n\tproc = kzalloc(sizeof(*proc), GFP_KERNEL);\n\tif (proc == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&proc->inner_lock);\n\tspin_lock_init(&proc->outer_lock);\n\tget_task_struct(current->group_leader);\n\tproc->tsk = current->group_leader;\n\tproc->cred = get_cred(filp->f_cred);\n\tINIT_LIST_HEAD(&proc->todo);\n\tinit_waitqueue_head(&proc->freeze_wait);\n\tproc->default_priority = task_nice(current);\n\t/* binderfs stashes devices in i_private */\n\tif (is_binderfs_device(nodp)) {\n\t\tbinder_dev = nodp->i_private;\n\t\tinfo = nodp->i_sb->s_fs_info;\n\t\tbinder_binderfs_dir_entry_proc = info->proc_log_dir;\n\t} else {\n\t\tbinder_dev = container_of(filp->private_data,\n\t\t\t\t\t  struct binder_device, miscdev);\n\t}\n\trefcount_inc(&binder_dev->ref);\n\tproc->context = &binder_dev->context;\n\tbinder_alloc_init(&proc->alloc);\n\n\tbinder_stats_created(BINDER_STAT_PROC);\n\tproc->pid = current->group_leader->pid;\n\tINIT_LIST_HEAD(&proc->delivered_death);\n\tINIT_LIST_HEAD(&proc->waiting_threads);\n\tfilp->private_data = proc;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(itr, &binder_procs, proc_node) {\n\t\tif (itr->pid == proc->pid) {\n\t\t\texisting_pid = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\thlist_add_head(&proc->proc_node, &binder_procs);\n\tmutex_unlock(&binder_procs_lock);\n\n\tif (binder_debugfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * proc debug entries are shared between contexts.\n\t\t * Only create for the first PID to avoid debugfs log spamming\n\t\t * The printing code will anyway print all contexts for a given\n\t\t * PID so this is not a problem.\n\t\t */\n\t\tproc->debugfs_entry = debugfs_create_file(strbuf, 0444,\n\t\t\tbinder_debugfs_dir_entry_proc,\n\t\t\t(void *)(unsigned long)proc->pid,\n\t\t\t&proc_fops);\n\t}\n\n\tif (binder_binderfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\t\tstruct dentry *binderfs_entry;\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * Similar to debugfs, the process specific log file is shared\n\t\t * between contexts. Only create for the first PID.\n\t\t * This is ok since same as debugfs, the log file will contain\n\t\t * information on all contexts of a given PID.\n\t\t */\n\t\tbinderfs_entry = binderfs_create_file(binder_binderfs_dir_entry_proc,\n\t\t\tstrbuf, &proc_fops, (void *)(unsigned long)proc->pid);\n\t\tif (!IS_ERR(binderfs_entry)) {\n\t\t\tproc->binderfs_entry = binderfs_entry;\n\t\t} else {\n\t\t\tint error;\n\n\t\t\terror = PTR_ERR(binderfs_entry);\n\t\t\tpr_warn(\"Unable to create file %s in binderfs (error %d)\\n\",\n\t\t\t\tstrbuf, error);\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,7 @@\n \tspin_lock_init(&proc->outer_lock);\n \tget_task_struct(current->group_leader);\n \tproc->tsk = current->group_leader;\n+\tproc->cred = get_cred(filp->f_cred);\n \tINIT_LIST_HEAD(&proc->todo);\n \tinit_waitqueue_head(&proc->freeze_wait);\n \tproc->default_priority = task_nice(current);",
        "function_modified_lines": {
            "added": [
                "\tproc->cred = get_cred(filp->f_cred);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In several functions of binder.c, there is a possible way to represent the wrong domain to SELinux due to a race condition. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-200688826References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2021-39713",
        "code_before_change": "static int tc_get_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp = NULL;\n\tunsigned long cl = 0;\n\tvoid *fh = NULL;\n\tint err;\n\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tparent = t->tcm_parent;\n\n\tif (prio == 0) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid filter command with priority of zero\");\n\t\treturn -ENOENT;\n\t}\n\n\t/* Find head of filter chain. */\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, false);\n\tif (!chain) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot find specified filter chain\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, false);\n\tif (!tp || IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = tp ? PTR_ERR(tp) : -ENOENT;\n\t\tgoto errout;\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter handle not found\");\n\t\terr = -ENOENT;\n\t} else {\n\t\terr = tfilter_notify(net, skb, n, tp, block, q, parent,\n\t\t\t\t     fh, RTM_NEWTFILTER, true);\n\t\tif (err < 0)\n\t\t\tNL_SET_ERR_MSG(extack, \"Failed to send filter notify message\");\n\t}\n\nerrout:\n\tif (chain)\n\t\ttcf_chain_put(chain);\n\treturn err;\n}",
        "code_after_change": "static int tc_get_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp = NULL;\n\tunsigned long cl = 0;\n\tvoid *fh = NULL;\n\tint err;\n\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tparent = t->tcm_parent;\n\n\tif (prio == 0) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid filter command with priority of zero\");\n\t\treturn -ENOENT;\n\t}\n\n\t/* Find head of filter chain. */\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, false);\n\tif (!chain) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot find specified filter chain\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, false);\n\tif (!tp || IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = tp ? PTR_ERR(tp) : -ENOENT;\n\t\tgoto errout;\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter handle not found\");\n\t\terr = -ENOENT;\n\t} else {\n\t\terr = tfilter_notify(net, skb, n, tp, block, q, parent,\n\t\t\t\t     fh, RTM_NEWTFILTER, true);\n\t\tif (err < 0)\n\t\t\tNL_SET_ERR_MSG(extack, \"Failed to send filter notify message\");\n\t}\n\nerrout:\n\tif (chain)\n\t\ttcf_chain_put(chain);\n\ttcf_block_release(q, block);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -80,5 +80,6 @@\n errout:\n \tif (chain)\n \t\ttcf_chain_put(chain);\n+\ttcf_block_release(q, block);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\ttcf_block_release(q, block);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Product: AndroidVersions: Android kernelAndroid ID: A-173788806References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2021-39713",
        "code_before_change": "static struct tcf_block *tcf_block_find(struct net *net, struct Qdisc **q,\n\t\t\t\t\tu32 *parent, unsigned long *cl,\n\t\t\t\t\tint ifindex, u32 block_index,\n\t\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct tcf_block *block;\n\n\tif (ifindex == TCM_IFINDEX_MAGIC_BLOCK) {\n\t\tblock = tcf_block_lookup(net, block_index);\n\t\tif (!block) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Block of given index was not found\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t} else {\n\t\tconst struct Qdisc_class_ops *cops;\n\t\tstruct net_device *dev;\n\n\t\t/* Find link */\n\t\tdev = __dev_get_by_index(net, ifindex);\n\t\tif (!dev)\n\t\t\treturn ERR_PTR(-ENODEV);\n\n\t\t/* Find qdisc */\n\t\tif (!*parent) {\n\t\t\t*q = dev->qdisc;\n\t\t\t*parent = (*q)->handle;\n\t\t} else {\n\t\t\t*q = qdisc_lookup(dev, TC_H_MAJ(*parent));\n\t\t\tif (!*q) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Parent Qdisc doesn't exists\");\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t}\n\t\t}\n\n\t\t/* Is it classful? */\n\t\tcops = (*q)->ops->cl_ops;\n\t\tif (!cops) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Qdisc not classful\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\n\t\tif (!cops->tcf_block) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Class doesn't support blocks\");\n\t\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t\t}\n\n\t\t/* Do we search for filter, attached to class? */\n\t\tif (TC_H_MIN(*parent)) {\n\t\t\t*cl = cops->find(*q, *parent);\n\t\t\tif (*cl == 0) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Specified class doesn't exist\");\n\t\t\t\treturn ERR_PTR(-ENOENT);\n\t\t\t}\n\t\t}\n\n\t\t/* And the last stroke */\n\t\tblock = cops->tcf_block(*q, *cl, extack);\n\t\tif (!block)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\tif (tcf_block_shared(block)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"This filter block is shared. Please use the block index to manipulate the filters\");\n\t\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t\t}\n\t}\n\n\treturn block;\n}",
        "code_after_change": "static struct tcf_block *tcf_block_find(struct net *net, struct Qdisc **q,\n\t\t\t\t\tu32 *parent, unsigned long *cl,\n\t\t\t\t\tint ifindex, u32 block_index,\n\t\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct tcf_block *block;\n\tint err = 0;\n\n\tif (ifindex == TCM_IFINDEX_MAGIC_BLOCK) {\n\t\tblock = tcf_block_lookup(net, block_index);\n\t\tif (!block) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Block of given index was not found\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t} else {\n\t\tconst struct Qdisc_class_ops *cops;\n\t\tstruct net_device *dev;\n\n\t\trcu_read_lock();\n\n\t\t/* Find link */\n\t\tdev = dev_get_by_index_rcu(net, ifindex);\n\t\tif (!dev) {\n\t\t\trcu_read_unlock();\n\t\t\treturn ERR_PTR(-ENODEV);\n\t\t}\n\n\t\t/* Find qdisc */\n\t\tif (!*parent) {\n\t\t\t*q = dev->qdisc;\n\t\t\t*parent = (*q)->handle;\n\t\t} else {\n\t\t\t*q = qdisc_lookup_rcu(dev, TC_H_MAJ(*parent));\n\t\t\tif (!*q) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Parent Qdisc doesn't exists\");\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto errout_rcu;\n\t\t\t}\n\t\t}\n\n\t\t*q = qdisc_refcount_inc_nz(*q);\n\t\tif (!*q) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Parent Qdisc doesn't exists\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_rcu;\n\t\t}\n\n\t\t/* Is it classful? */\n\t\tcops = (*q)->ops->cl_ops;\n\t\tif (!cops) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Qdisc not classful\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_rcu;\n\t\t}\n\n\t\tif (!cops->tcf_block) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Class doesn't support blocks\");\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto errout_rcu;\n\t\t}\n\n\t\t/* At this point we know that qdisc is not noop_qdisc,\n\t\t * which means that qdisc holds a reference to net_device\n\t\t * and we hold a reference to qdisc, so it is safe to release\n\t\t * rcu read lock.\n\t\t */\n\t\trcu_read_unlock();\n\n\t\t/* Do we search for filter, attached to class? */\n\t\tif (TC_H_MIN(*parent)) {\n\t\t\t*cl = cops->find(*q, *parent);\n\t\t\tif (*cl == 0) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Specified class doesn't exist\");\n\t\t\t\terr = -ENOENT;\n\t\t\t\tgoto errout_qdisc;\n\t\t\t}\n\t\t}\n\n\t\t/* And the last stroke */\n\t\tblock = cops->tcf_block(*q, *cl, extack);\n\t\tif (!block) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_qdisc;\n\t\t}\n\t\tif (tcf_block_shared(block)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"This filter block is shared. Please use the block index to manipulate the filters\");\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto errout_qdisc;\n\t\t}\n\t}\n\n\treturn block;\n\nerrout_rcu:\n\trcu_read_unlock();\nerrout_qdisc:\n\tif (*q)\n\t\tqdisc_put(*q);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \t\t\t\t\tstruct netlink_ext_ack *extack)\n {\n \tstruct tcf_block *block;\n+\tint err = 0;\n \n \tif (ifindex == TCM_IFINDEX_MAGIC_BLOCK) {\n \t\tblock = tcf_block_lookup(net, block_index);\n@@ -15,53 +16,85 @@\n \t\tconst struct Qdisc_class_ops *cops;\n \t\tstruct net_device *dev;\n \n+\t\trcu_read_lock();\n+\n \t\t/* Find link */\n-\t\tdev = __dev_get_by_index(net, ifindex);\n-\t\tif (!dev)\n+\t\tdev = dev_get_by_index_rcu(net, ifindex);\n+\t\tif (!dev) {\n+\t\t\trcu_read_unlock();\n \t\t\treturn ERR_PTR(-ENODEV);\n+\t\t}\n \n \t\t/* Find qdisc */\n \t\tif (!*parent) {\n \t\t\t*q = dev->qdisc;\n \t\t\t*parent = (*q)->handle;\n \t\t} else {\n-\t\t\t*q = qdisc_lookup(dev, TC_H_MAJ(*parent));\n+\t\t\t*q = qdisc_lookup_rcu(dev, TC_H_MAJ(*parent));\n \t\t\tif (!*q) {\n \t\t\t\tNL_SET_ERR_MSG(extack, \"Parent Qdisc doesn't exists\");\n-\t\t\t\treturn ERR_PTR(-EINVAL);\n+\t\t\t\terr = -EINVAL;\n+\t\t\t\tgoto errout_rcu;\n \t\t\t}\n+\t\t}\n+\n+\t\t*q = qdisc_refcount_inc_nz(*q);\n+\t\tif (!*q) {\n+\t\t\tNL_SET_ERR_MSG(extack, \"Parent Qdisc doesn't exists\");\n+\t\t\terr = -EINVAL;\n+\t\t\tgoto errout_rcu;\n \t\t}\n \n \t\t/* Is it classful? */\n \t\tcops = (*q)->ops->cl_ops;\n \t\tif (!cops) {\n \t\t\tNL_SET_ERR_MSG(extack, \"Qdisc not classful\");\n-\t\t\treturn ERR_PTR(-EINVAL);\n+\t\t\terr = -EINVAL;\n+\t\t\tgoto errout_rcu;\n \t\t}\n \n \t\tif (!cops->tcf_block) {\n \t\t\tNL_SET_ERR_MSG(extack, \"Class doesn't support blocks\");\n-\t\t\treturn ERR_PTR(-EOPNOTSUPP);\n+\t\t\terr = -EOPNOTSUPP;\n+\t\t\tgoto errout_rcu;\n \t\t}\n+\n+\t\t/* At this point we know that qdisc is not noop_qdisc,\n+\t\t * which means that qdisc holds a reference to net_device\n+\t\t * and we hold a reference to qdisc, so it is safe to release\n+\t\t * rcu read lock.\n+\t\t */\n+\t\trcu_read_unlock();\n \n \t\t/* Do we search for filter, attached to class? */\n \t\tif (TC_H_MIN(*parent)) {\n \t\t\t*cl = cops->find(*q, *parent);\n \t\t\tif (*cl == 0) {\n \t\t\t\tNL_SET_ERR_MSG(extack, \"Specified class doesn't exist\");\n-\t\t\t\treturn ERR_PTR(-ENOENT);\n+\t\t\t\terr = -ENOENT;\n+\t\t\t\tgoto errout_qdisc;\n \t\t\t}\n \t\t}\n \n \t\t/* And the last stroke */\n \t\tblock = cops->tcf_block(*q, *cl, extack);\n-\t\tif (!block)\n-\t\t\treturn ERR_PTR(-EINVAL);\n+\t\tif (!block) {\n+\t\t\terr = -EINVAL;\n+\t\t\tgoto errout_qdisc;\n+\t\t}\n \t\tif (tcf_block_shared(block)) {\n \t\t\tNL_SET_ERR_MSG(extack, \"This filter block is shared. Please use the block index to manipulate the filters\");\n-\t\t\treturn ERR_PTR(-EOPNOTSUPP);\n+\t\t\terr = -EOPNOTSUPP;\n+\t\t\tgoto errout_qdisc;\n \t\t}\n \t}\n \n \treturn block;\n+\n+errout_rcu:\n+\trcu_read_unlock();\n+errout_qdisc:\n+\tif (*q)\n+\t\tqdisc_put(*q);\n+\treturn ERR_PTR(err);\n }",
        "function_modified_lines": {
            "added": [
                "\tint err = 0;",
                "\t\trcu_read_lock();",
                "",
                "\t\tdev = dev_get_by_index_rcu(net, ifindex);",
                "\t\tif (!dev) {",
                "\t\t\trcu_read_unlock();",
                "\t\t}",
                "\t\t\t*q = qdisc_lookup_rcu(dev, TC_H_MAJ(*parent));",
                "\t\t\t\terr = -EINVAL;",
                "\t\t\t\tgoto errout_rcu;",
                "\t\t}",
                "",
                "\t\t*q = qdisc_refcount_inc_nz(*q);",
                "\t\tif (!*q) {",
                "\t\t\tNL_SET_ERR_MSG(extack, \"Parent Qdisc doesn't exists\");",
                "\t\t\terr = -EINVAL;",
                "\t\t\tgoto errout_rcu;",
                "\t\t\terr = -EINVAL;",
                "\t\t\tgoto errout_rcu;",
                "\t\t\terr = -EOPNOTSUPP;",
                "\t\t\tgoto errout_rcu;",
                "",
                "\t\t/* At this point we know that qdisc is not noop_qdisc,",
                "\t\t * which means that qdisc holds a reference to net_device",
                "\t\t * and we hold a reference to qdisc, so it is safe to release",
                "\t\t * rcu read lock.",
                "\t\t */",
                "\t\trcu_read_unlock();",
                "\t\t\t\terr = -ENOENT;",
                "\t\t\t\tgoto errout_qdisc;",
                "\t\tif (!block) {",
                "\t\t\terr = -EINVAL;",
                "\t\t\tgoto errout_qdisc;",
                "\t\t}",
                "\t\t\terr = -EOPNOTSUPP;",
                "\t\t\tgoto errout_qdisc;",
                "",
                "errout_rcu:",
                "\trcu_read_unlock();",
                "errout_qdisc:",
                "\tif (*q)",
                "\t\tqdisc_put(*q);",
                "\treturn ERR_PTR(err);"
            ],
            "deleted": [
                "\t\tdev = __dev_get_by_index(net, ifindex);",
                "\t\tif (!dev)",
                "\t\t\t*q = qdisc_lookup(dev, TC_H_MAJ(*parent));",
                "\t\t\t\treturn ERR_PTR(-EINVAL);",
                "\t\t\treturn ERR_PTR(-EINVAL);",
                "\t\t\treturn ERR_PTR(-EOPNOTSUPP);",
                "\t\t\t\treturn ERR_PTR(-ENOENT);",
                "\t\tif (!block)",
                "\t\t\treturn ERR_PTR(-EINVAL);",
                "\t\t\treturn ERR_PTR(-EOPNOTSUPP);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Product: AndroidVersions: Android kernelAndroid ID: A-173788806References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2021-39713",
        "code_before_change": "static int tc_del_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp = NULL;\n\tunsigned long cl = 0;\n\tvoid *fh = NULL;\n\tint err;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tparent = t->tcm_parent;\n\n\tif (prio == 0 && (protocol || t->tcm_handle || tca[TCA_KIND])) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot flush filters with protocol, handle or kind set\");\n\t\treturn -ENOENT;\n\t}\n\n\t/* Find head of filter chain. */\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, false);\n\tif (!chain) {\n\t\t/* User requested flush on non-existent chain. Nothing to do,\n\t\t * so just return success.\n\t\t */\n\t\tif (prio == 0) {\n\t\t\terr = 0;\n\t\t\tgoto errout;\n\t\t}\n\t\tNL_SET_ERR_MSG(extack, \"Cannot find specified filter chain\");\n\t\terr = -ENOENT;\n\t\tgoto errout;\n\t}\n\n\tif (prio == 0) {\n\t\ttfilter_notify_chain(net, skb, block, q, parent, n,\n\t\t\t\t     chain, RTM_DELTFILTER);\n\t\ttcf_chain_flush(chain);\n\t\terr = 0;\n\t\tgoto errout;\n\t}\n\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, false);\n\tif (!tp || IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = tp ? PTR_ERR(tp) : -ENOENT;\n\t\tgoto errout;\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (t->tcm_handle == 0) {\n\t\t\ttcf_chain_tp_remove(chain, &chain_info, tp);\n\t\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t\t       RTM_DELTFILTER, false);\n\t\t\ttcf_proto_destroy(tp, extack);\n\t\t\terr = 0;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Specified filter handle not found\");\n\t\t\terr = -ENOENT;\n\t\t}\n\t} else {\n\t\tbool last;\n\n\t\terr = tfilter_del_notify(net, skb, n, tp, block,\n\t\t\t\t\t q, parent, fh, false, &last,\n\t\t\t\t\t extack);\n\t\tif (err)\n\t\t\tgoto errout;\n\t\tif (last) {\n\t\t\ttcf_chain_tp_remove(chain, &chain_info, tp);\n\t\t\ttcf_proto_destroy(tp, extack);\n\t\t}\n\t}\n\nerrout:\n\tif (chain)\n\t\ttcf_chain_put(chain);\n\treturn err;\n}",
        "code_after_change": "static int tc_del_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp = NULL;\n\tunsigned long cl = 0;\n\tvoid *fh = NULL;\n\tint err;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tparent = t->tcm_parent;\n\n\tif (prio == 0 && (protocol || t->tcm_handle || tca[TCA_KIND])) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot flush filters with protocol, handle or kind set\");\n\t\treturn -ENOENT;\n\t}\n\n\t/* Find head of filter chain. */\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, false);\n\tif (!chain) {\n\t\t/* User requested flush on non-existent chain. Nothing to do,\n\t\t * so just return success.\n\t\t */\n\t\tif (prio == 0) {\n\t\t\terr = 0;\n\t\t\tgoto errout;\n\t\t}\n\t\tNL_SET_ERR_MSG(extack, \"Cannot find specified filter chain\");\n\t\terr = -ENOENT;\n\t\tgoto errout;\n\t}\n\n\tif (prio == 0) {\n\t\ttfilter_notify_chain(net, skb, block, q, parent, n,\n\t\t\t\t     chain, RTM_DELTFILTER);\n\t\ttcf_chain_flush(chain);\n\t\terr = 0;\n\t\tgoto errout;\n\t}\n\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, false);\n\tif (!tp || IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = tp ? PTR_ERR(tp) : -ENOENT;\n\t\tgoto errout;\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (t->tcm_handle == 0) {\n\t\t\ttcf_chain_tp_remove(chain, &chain_info, tp);\n\t\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t\t       RTM_DELTFILTER, false);\n\t\t\ttcf_proto_destroy(tp, extack);\n\t\t\terr = 0;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Specified filter handle not found\");\n\t\t\terr = -ENOENT;\n\t\t}\n\t} else {\n\t\tbool last;\n\n\t\terr = tfilter_del_notify(net, skb, n, tp, block,\n\t\t\t\t\t q, parent, fh, false, &last,\n\t\t\t\t\t extack);\n\t\tif (err)\n\t\t\tgoto errout;\n\t\tif (last) {\n\t\t\ttcf_chain_tp_remove(chain, &chain_info, tp);\n\t\t\ttcf_proto_destroy(tp, extack);\n\t\t}\n\t}\n\nerrout:\n\tif (chain)\n\t\ttcf_chain_put(chain);\n\ttcf_block_release(q, block);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -113,5 +113,6 @@\n errout:\n \tif (chain)\n \t\ttcf_chain_put(chain);\n+\ttcf_block_release(q, block);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\ttcf_block_release(q, block);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Product: AndroidVersions: Android kernelAndroid ID: A-173788806References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2021-39713",
        "code_before_change": "static int tc_new_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tbool prio_allocate;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp;\n\tunsigned long cl;\n\tvoid *fh;\n\tint err;\n\tint tp_created;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\ttp_created = 0;\n\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tprio_allocate = false;\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is provided by the user,\n\t\t * we allocate one.\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_CREATE) {\n\t\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t\t\tprio_allocate = true;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid filter command with priority of zero\");\n\t\t\treturn -ENOENT;\n\t\t}\n\t}\n\n\t/* Find head of filter chain. */\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, true);\n\tif (!chain) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot create specified filter chain\");\n\t\terr = -ENOMEM;\n\t\tgoto errout;\n\t}\n\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, prio_allocate);\n\tif (IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = PTR_ERR(tp);\n\t\tgoto errout;\n\t}\n\n\tif (tp == NULL) {\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Filter kind and protocol must be specified\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout;\n\t\t}\n\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout;\n\t\t}\n\n\t\tif (prio_allocate)\n\t\t\tprio = tcf_auto_prio(tcf_chain_tp_prev(&chain_info));\n\n\t\ttp = tcf_proto_create(nla_data(tca[TCA_KIND]),\n\t\t\t\t      protocol, prio, chain, extack);\n\t\tif (IS_ERR(tp)) {\n\t\t\terr = PTR_ERR(tp);\n\t\t\tgoto errout;\n\t\t}\n\t\ttp_created = 1;\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout;\n\t\t}\n\t} else if (n->nlmsg_flags & NLM_F_EXCL) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter already exists\");\n\t\terr = -EEXIST;\n\t\tgoto errout;\n\t}\n\n\tif (chain->tmplt_ops && chain->tmplt_ops != tp->ops) {\n\t\tNL_SET_ERR_MSG(extack, \"Chain template is set to a different filter kind\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh,\n\t\t\t      n->nlmsg_flags & NLM_F_CREATE ? TCA_ACT_NOREPLACE : TCA_ACT_REPLACE,\n\t\t\t      extack);\n\tif (err == 0) {\n\t\tif (tp_created)\n\t\t\ttcf_chain_tp_insert(chain, &chain_info, tp);\n\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t       RTM_NEWTFILTER, false);\n\t} else {\n\t\tif (tp_created)\n\t\t\ttcf_proto_destroy(tp, NULL);\n\t}\n\nerrout:\n\tif (chain)\n\t\ttcf_chain_put(chain);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n}",
        "code_after_change": "static int tc_new_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tbool prio_allocate;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp;\n\tunsigned long cl;\n\tvoid *fh;\n\tint err;\n\tint tp_created;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\ttp_created = 0;\n\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tprio_allocate = false;\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is provided by the user,\n\t\t * we allocate one.\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_CREATE) {\n\t\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t\t\tprio_allocate = true;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid filter command with priority of zero\");\n\t\t\treturn -ENOENT;\n\t\t}\n\t}\n\n\t/* Find head of filter chain. */\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, true);\n\tif (!chain) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot create specified filter chain\");\n\t\terr = -ENOMEM;\n\t\tgoto errout;\n\t}\n\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, prio_allocate);\n\tif (IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = PTR_ERR(tp);\n\t\tgoto errout;\n\t}\n\n\tif (tp == NULL) {\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Filter kind and protocol must be specified\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout;\n\t\t}\n\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout;\n\t\t}\n\n\t\tif (prio_allocate)\n\t\t\tprio = tcf_auto_prio(tcf_chain_tp_prev(&chain_info));\n\n\t\ttp = tcf_proto_create(nla_data(tca[TCA_KIND]),\n\t\t\t\t      protocol, prio, chain, extack);\n\t\tif (IS_ERR(tp)) {\n\t\t\terr = PTR_ERR(tp);\n\t\t\tgoto errout;\n\t\t}\n\t\ttp_created = 1;\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout;\n\t\t}\n\t} else if (n->nlmsg_flags & NLM_F_EXCL) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter already exists\");\n\t\terr = -EEXIST;\n\t\tgoto errout;\n\t}\n\n\tif (chain->tmplt_ops && chain->tmplt_ops != tp->ops) {\n\t\tNL_SET_ERR_MSG(extack, \"Chain template is set to a different filter kind\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh,\n\t\t\t      n->nlmsg_flags & NLM_F_CREATE ? TCA_ACT_NOREPLACE : TCA_ACT_REPLACE,\n\t\t\t      extack);\n\tif (err == 0) {\n\t\tif (tp_created)\n\t\t\ttcf_chain_tp_insert(chain, &chain_info, tp);\n\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t       RTM_NEWTFILTER, false);\n\t} else {\n\t\tif (tp_created)\n\t\t\ttcf_proto_destroy(tp, NULL);\n\t}\n\nerrout:\n\tif (chain)\n\t\ttcf_chain_put(chain);\n\ttcf_block_release(q, block);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -146,6 +146,7 @@\n errout:\n \tif (chain)\n \t\ttcf_chain_put(chain);\n+\ttcf_block_release(q, block);\n \tif (err == -EAGAIN)\n \t\t/* Replay the request. */\n \t\tgoto replay;",
        "function_modified_lines": {
            "added": [
                "\ttcf_block_release(q, block);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Product: AndroidVersions: Android kernelAndroid ID: A-173788806References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2021-39713",
        "code_before_change": "static int tc_ctl_chain(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tunsigned long cl;\n\tint err;\n\n\tif (n->nlmsg_type != RTM_GETCHAIN &&\n\t    !netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block))\n\t\treturn PTR_ERR(block);\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\treturn -EINVAL;\n\t}\n\tchain = tcf_chain_lookup(block, chain_index);\n\tif (n->nlmsg_type == RTM_NEWCHAIN) {\n\t\tif (chain) {\n\t\t\tif (tcf_chain_held_by_acts_only(chain)) {\n\t\t\t\t/* The chain exists only because there is\n\t\t\t\t * some action referencing it.\n\t\t\t\t */\n\t\t\t\ttcf_chain_hold(chain);\n\t\t\t} else {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Filter chain already exists\");\n\t\t\t\treturn -EEXIST;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWCHAIN and NLM_F_CREATE to create a new chain\");\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t\tchain = tcf_chain_create(block, chain_index);\n\t\t\tif (!chain) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Failed to create filter chain\");\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!chain || tcf_chain_held_by_acts_only(chain)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Cannot find specified filter chain\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttcf_chain_hold(chain);\n\t}\n\n\tswitch (n->nlmsg_type) {\n\tcase RTM_NEWCHAIN:\n\t\terr = tc_chain_tmplt_add(chain, net, tca, extack);\n\t\tif (err)\n\t\t\tgoto errout;\n\t\t/* In case the chain was successfully added, take a reference\n\t\t * to the chain. This ensures that an empty chain\n\t\t * does not disappear at the end of this function.\n\t\t */\n\t\ttcf_chain_hold(chain);\n\t\tchain->explicitly_created = true;\n\t\ttc_chain_notify(chain, NULL, 0, NLM_F_CREATE | NLM_F_EXCL,\n\t\t\t\tRTM_NEWCHAIN, false);\n\t\tbreak;\n\tcase RTM_DELCHAIN:\n\t\ttfilter_notify_chain(net, skb, block, q, parent, n,\n\t\t\t\t     chain, RTM_DELTFILTER);\n\t\t/* Flush the chain first as the user requested chain removal. */\n\t\ttcf_chain_flush(chain);\n\t\t/* In case the chain was successfully deleted, put a reference\n\t\t * to the chain previously taken during addition.\n\t\t */\n\t\ttcf_chain_put_explicitly_created(chain);\n\t\tchain->explicitly_created = false;\n\t\tbreak;\n\tcase RTM_GETCHAIN:\n\t\terr = tc_chain_notify(chain, skb, n->nlmsg_seq,\n\t\t\t\t      n->nlmsg_seq, n->nlmsg_type, true);\n\t\tif (err < 0)\n\t\t\tNL_SET_ERR_MSG(extack, \"Failed to send chain notify message\");\n\t\tbreak;\n\tdefault:\n\t\terr = -EOPNOTSUPP;\n\t\tNL_SET_ERR_MSG(extack, \"Unsupported message type\");\n\t\tgoto errout;\n\t}\n\nerrout:\n\ttcf_chain_put(chain);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n}",
        "code_after_change": "static int tc_ctl_chain(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tunsigned long cl;\n\tint err;\n\n\tif (n->nlmsg_type != RTM_GETCHAIN &&\n\t    !netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block))\n\t\treturn PTR_ERR(block);\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout_block;\n\t}\n\tchain = tcf_chain_lookup(block, chain_index);\n\tif (n->nlmsg_type == RTM_NEWCHAIN) {\n\t\tif (chain) {\n\t\t\tif (tcf_chain_held_by_acts_only(chain)) {\n\t\t\t\t/* The chain exists only because there is\n\t\t\t\t * some action referencing it.\n\t\t\t\t */\n\t\t\t\ttcf_chain_hold(chain);\n\t\t\t} else {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Filter chain already exists\");\n\t\t\t\terr = -EEXIST;\n\t\t\t\tgoto errout_block;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWCHAIN and NLM_F_CREATE to create a new chain\");\n\t\t\t\terr = -ENOENT;\n\t\t\t\tgoto errout_block;\n\t\t\t}\n\t\t\tchain = tcf_chain_create(block, chain_index);\n\t\t\tif (!chain) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Failed to create filter chain\");\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto errout_block;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!chain || tcf_chain_held_by_acts_only(chain)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Cannot find specified filter chain\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_block;\n\t\t}\n\t\ttcf_chain_hold(chain);\n\t}\n\n\tswitch (n->nlmsg_type) {\n\tcase RTM_NEWCHAIN:\n\t\terr = tc_chain_tmplt_add(chain, net, tca, extack);\n\t\tif (err)\n\t\t\tgoto errout;\n\t\t/* In case the chain was successfully added, take a reference\n\t\t * to the chain. This ensures that an empty chain\n\t\t * does not disappear at the end of this function.\n\t\t */\n\t\ttcf_chain_hold(chain);\n\t\tchain->explicitly_created = true;\n\t\ttc_chain_notify(chain, NULL, 0, NLM_F_CREATE | NLM_F_EXCL,\n\t\t\t\tRTM_NEWCHAIN, false);\n\t\tbreak;\n\tcase RTM_DELCHAIN:\n\t\ttfilter_notify_chain(net, skb, block, q, parent, n,\n\t\t\t\t     chain, RTM_DELTFILTER);\n\t\t/* Flush the chain first as the user requested chain removal. */\n\t\ttcf_chain_flush(chain);\n\t\t/* In case the chain was successfully deleted, put a reference\n\t\t * to the chain previously taken during addition.\n\t\t */\n\t\ttcf_chain_put_explicitly_created(chain);\n\t\tchain->explicitly_created = false;\n\t\tbreak;\n\tcase RTM_GETCHAIN:\n\t\terr = tc_chain_notify(chain, skb, n->nlmsg_seq,\n\t\t\t\t      n->nlmsg_seq, n->nlmsg_type, true);\n\t\tif (err < 0)\n\t\t\tNL_SET_ERR_MSG(extack, \"Failed to send chain notify message\");\n\t\tbreak;\n\tdefault:\n\t\terr = -EOPNOTSUPP;\n\t\tNL_SET_ERR_MSG(extack, \"Unsupported message type\");\n\t\tgoto errout;\n\t}\n\nerrout:\n\ttcf_chain_put(chain);\nerrout_block:\n\ttcf_block_release(q, block);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -33,7 +33,8 @@\n \tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n \tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n \t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n-\t\treturn -EINVAL;\n+\t\terr = -EINVAL;\n+\t\tgoto errout_block;\n \t}\n \tchain = tcf_chain_lookup(block, chain_index);\n \tif (n->nlmsg_type == RTM_NEWCHAIN) {\n@@ -45,23 +46,27 @@\n \t\t\t\ttcf_chain_hold(chain);\n \t\t\t} else {\n \t\t\t\tNL_SET_ERR_MSG(extack, \"Filter chain already exists\");\n-\t\t\t\treturn -EEXIST;\n+\t\t\t\terr = -EEXIST;\n+\t\t\t\tgoto errout_block;\n \t\t\t}\n \t\t} else {\n \t\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n \t\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWCHAIN and NLM_F_CREATE to create a new chain\");\n-\t\t\t\treturn -ENOENT;\n+\t\t\t\terr = -ENOENT;\n+\t\t\t\tgoto errout_block;\n \t\t\t}\n \t\t\tchain = tcf_chain_create(block, chain_index);\n \t\t\tif (!chain) {\n \t\t\t\tNL_SET_ERR_MSG(extack, \"Failed to create filter chain\");\n-\t\t\t\treturn -ENOMEM;\n+\t\t\t\terr = -ENOMEM;\n+\t\t\t\tgoto errout_block;\n \t\t\t}\n \t\t}\n \t} else {\n \t\tif (!chain || tcf_chain_held_by_acts_only(chain)) {\n \t\t\tNL_SET_ERR_MSG(extack, \"Cannot find specified filter chain\");\n-\t\t\treturn -EINVAL;\n+\t\t\terr = -EINVAL;\n+\t\t\tgoto errout_block;\n \t\t}\n \t\ttcf_chain_hold(chain);\n \t}\n@@ -105,6 +110,8 @@\n \n errout:\n \ttcf_chain_put(chain);\n+errout_block:\n+\ttcf_block_release(q, block);\n \tif (err == -EAGAIN)\n \t\t/* Replay the request. */\n \t\tgoto replay;",
        "function_modified_lines": {
            "added": [
                "\t\terr = -EINVAL;",
                "\t\tgoto errout_block;",
                "\t\t\t\terr = -EEXIST;",
                "\t\t\t\tgoto errout_block;",
                "\t\t\t\terr = -ENOENT;",
                "\t\t\t\tgoto errout_block;",
                "\t\t\t\terr = -ENOMEM;",
                "\t\t\t\tgoto errout_block;",
                "\t\t\terr = -EINVAL;",
                "\t\t\tgoto errout_block;",
                "errout_block:",
                "\ttcf_block_release(q, block);"
            ],
            "deleted": [
                "\t\treturn -EINVAL;",
                "\t\t\t\treturn -EEXIST;",
                "\t\t\t\treturn -ENOENT;",
                "\t\t\t\treturn -ENOMEM;",
                "\t\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Product: AndroidVersions: Android kernelAndroid ID: A-173788806References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2021-40490",
        "code_before_change": "int ext4_write_inline_data_end(struct inode *inode, loff_t pos, unsigned len,\n\t\t\t       unsigned copied, struct page *page)\n{\n\tint ret, no_expand;\n\tvoid *kaddr;\n\tstruct ext4_iloc iloc;\n\n\tif (unlikely(copied < len)) {\n\t\tif (!PageUptodate(page)) {\n\t\t\tcopied = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = ext4_get_inode_loc(inode, &iloc);\n\tif (ret) {\n\t\text4_std_error(inode->i_sb, ret);\n\t\tcopied = 0;\n\t\tgoto out;\n\t}\n\n\text4_write_lock_xattr(inode, &no_expand);\n\tBUG_ON(!ext4_has_inline_data(inode));\n\n\tkaddr = kmap_atomic(page);\n\text4_write_inline_data(inode, &iloc, kaddr, pos, len);\n\tkunmap_atomic(kaddr);\n\tSetPageUptodate(page);\n\t/* clear page dirty so that writepages wouldn't work for us. */\n\tClearPageDirty(page);\n\n\text4_write_unlock_xattr(inode, &no_expand);\n\tbrelse(iloc.bh);\n\tmark_inode_dirty(inode);\nout:\n\treturn copied;\n}",
        "code_after_change": "int ext4_write_inline_data_end(struct inode *inode, loff_t pos, unsigned len,\n\t\t\t       unsigned copied, struct page *page)\n{\n\tint ret, no_expand;\n\tvoid *kaddr;\n\tstruct ext4_iloc iloc;\n\n\tif (unlikely(copied < len)) {\n\t\tif (!PageUptodate(page)) {\n\t\t\tcopied = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = ext4_get_inode_loc(inode, &iloc);\n\tif (ret) {\n\t\text4_std_error(inode->i_sb, ret);\n\t\tcopied = 0;\n\t\tgoto out;\n\t}\n\n\text4_write_lock_xattr(inode, &no_expand);\n\tBUG_ON(!ext4_has_inline_data(inode));\n\n\t/*\n\t * ei->i_inline_off may have changed since ext4_write_begin()\n\t * called ext4_try_to_write_inline_data()\n\t */\n\t(void) ext4_find_inline_data_nolock(inode);\n\n\tkaddr = kmap_atomic(page);\n\text4_write_inline_data(inode, &iloc, kaddr, pos, len);\n\tkunmap_atomic(kaddr);\n\tSetPageUptodate(page);\n\t/* clear page dirty so that writepages wouldn't work for us. */\n\tClearPageDirty(page);\n\n\text4_write_unlock_xattr(inode, &no_expand);\n\tbrelse(iloc.bh);\n\tmark_inode_dirty(inode);\nout:\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,6 +22,12 @@\n \text4_write_lock_xattr(inode, &no_expand);\n \tBUG_ON(!ext4_has_inline_data(inode));\n \n+\t/*\n+\t * ei->i_inline_off may have changed since ext4_write_begin()\n+\t * called ext4_try_to_write_inline_data()\n+\t */\n+\t(void) ext4_find_inline_data_nolock(inode);\n+\n \tkaddr = kmap_atomic(page);\n \text4_write_inline_data(inode, &iloc, kaddr, pos, len);\n \tkunmap_atomic(kaddr);",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * ei->i_inline_off may have changed since ext4_write_begin()",
                "\t * called ext4_try_to_write_inline_data()",
                "\t */",
                "\t(void) ext4_find_inline_data_nolock(inode);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was discovered in ext4_write_inline_data_end in fs/ext4/inline.c in the ext4 subsystem in the Linux kernel through 5.13.13."
    },
    {
        "cve_id": "CVE-2021-4083",
        "code_before_change": "static struct file *__fget_files(struct files_struct *files, unsigned int fd,\n\t\t\t\t fmode_t mask, unsigned int refs)\n{\n\tstruct file *file;\n\n\trcu_read_lock();\nloop:\n\tfile = files_lookup_fd_rcu(files, fd);\n\tif (file) {\n\t\t/* File object ref couldn't be taken.\n\t\t * dup2() atomicity guarantee is the reason\n\t\t * we loop to catch the new file (or NULL pointer)\n\t\t */\n\t\tif (file->f_mode & mask)\n\t\t\tfile = NULL;\n\t\telse if (!get_file_rcu_many(file, refs))\n\t\t\tgoto loop;\n\t}\n\trcu_read_unlock();\n\n\treturn file;\n}",
        "code_after_change": "static struct file *__fget_files(struct files_struct *files, unsigned int fd,\n\t\t\t\t fmode_t mask, unsigned int refs)\n{\n\tstruct file *file;\n\n\trcu_read_lock();\nloop:\n\tfile = files_lookup_fd_rcu(files, fd);\n\tif (file) {\n\t\t/* File object ref couldn't be taken.\n\t\t * dup2() atomicity guarantee is the reason\n\t\t * we loop to catch the new file (or NULL pointer)\n\t\t */\n\t\tif (file->f_mode & mask)\n\t\t\tfile = NULL;\n\t\telse if (!get_file_rcu_many(file, refs))\n\t\t\tgoto loop;\n\t\telse if (files_lookup_fd_raw(files, fd) != file) {\n\t\t\tfput_many(file, refs);\n\t\t\tgoto loop;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn file;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,6 +15,10 @@\n \t\t\tfile = NULL;\n \t\telse if (!get_file_rcu_many(file, refs))\n \t\t\tgoto loop;\n+\t\telse if (files_lookup_fd_raw(files, fd) != file) {\n+\t\t\tfput_many(file, refs);\n+\t\t\tgoto loop;\n+\t\t}\n \t}\n \trcu_read_unlock();\n ",
        "function_modified_lines": {
            "added": [
                "\t\telse if (files_lookup_fd_raw(files, fd) != file) {",
                "\t\t\tfput_many(file, refs);",
                "\t\t\tgoto loop;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A read-after-free memory flaw was found in the Linux kernel's garbage collection for Unix domain socket file handlers in the way users call close() and fget() simultaneously and can potentially trigger a race condition. This flaw allows a local user to crash the system or escalate their privileges on the system. This flaw affects Linux kernel versions prior to 5.16-rc4."
    },
    {
        "cve_id": "CVE-2021-4202",
        "code_before_change": "inline int nci_request(struct nci_dev *ndev,\n\t\t       void (*req)(struct nci_dev *ndev,\n\t\t\t\t   const void *opt),\n\t\t       const void *opt, __u32 timeout)\n{\n\tint rc;\n\n\tif (!test_bit(NCI_UP, &ndev->flags))\n\t\treturn -ENETDOWN;\n\n\t/* Serialize all requests */\n\tmutex_lock(&ndev->req_lock);\n\trc = __nci_request(ndev, req, opt, timeout);\n\tmutex_unlock(&ndev->req_lock);\n\n\treturn rc;\n}",
        "code_after_change": "inline int nci_request(struct nci_dev *ndev,\n\t\t       void (*req)(struct nci_dev *ndev,\n\t\t\t\t   const void *opt),\n\t\t       const void *opt, __u32 timeout)\n{\n\tint rc;\n\n\t/* Serialize all requests */\n\tmutex_lock(&ndev->req_lock);\n\t/* check the state after obtaing the lock against any races\n\t * from nci_close_device when the device gets removed.\n\t */\n\tif (test_bit(NCI_UP, &ndev->flags))\n\t\trc = __nci_request(ndev, req, opt, timeout);\n\telse\n\t\trc = -ENETDOWN;\n\tmutex_unlock(&ndev->req_lock);\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,12 +5,15 @@\n {\n \tint rc;\n \n-\tif (!test_bit(NCI_UP, &ndev->flags))\n-\t\treturn -ENETDOWN;\n-\n \t/* Serialize all requests */\n \tmutex_lock(&ndev->req_lock);\n-\trc = __nci_request(ndev, req, opt, timeout);\n+\t/* check the state after obtaing the lock against any races\n+\t * from nci_close_device when the device gets removed.\n+\t */\n+\tif (test_bit(NCI_UP, &ndev->flags))\n+\t\trc = __nci_request(ndev, req, opt, timeout);\n+\telse\n+\t\trc = -ENETDOWN;\n \tmutex_unlock(&ndev->req_lock);\n \n \treturn rc;",
        "function_modified_lines": {
            "added": [
                "\t/* check the state after obtaing the lock against any races",
                "\t * from nci_close_device when the device gets removed.",
                "\t */",
                "\tif (test_bit(NCI_UP, &ndev->flags))",
                "\t\trc = __nci_request(ndev, req, opt, timeout);",
                "\telse",
                "\t\trc = -ENETDOWN;"
            ],
            "deleted": [
                "\tif (!test_bit(NCI_UP, &ndev->flags))",
                "\t\treturn -ENETDOWN;",
                "",
                "\trc = __nci_request(ndev, req, opt, timeout);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nci_request in net/nfc/nci/core.c in NFC Controller Interface (NCI) in the Linux kernel. This flaw could allow a local attacker with user privileges to cause a data race problem while the device is getting removed, leading to a privilege escalation problem."
    },
    {
        "cve_id": "CVE-2021-4203",
        "code_before_change": "int sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tunion {\n\t\tint val;\n\t\tu64 val64;\n\t\tunsigned long ulval;\n\t\tstruct linger ling;\n\t\tstruct old_timeval32 tm32;\n\t\tstruct __kernel_old_timeval tm;\n\t\tstruct  __kernel_sock_timeval stm;\n\t\tstruct sock_txtime txtime;\n\t\tstruct so_timestamping timestamping;\n\t} v;\n\n\tint lv = sizeof(int);\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tmemset(&v, 0, sizeof(v));\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tv.val = sock_flag(sk, SOCK_DBG);\n\t\tbreak;\n\n\tcase SO_DONTROUTE:\n\t\tv.val = sock_flag(sk, SOCK_LOCALROUTE);\n\t\tbreak;\n\n\tcase SO_BROADCAST:\n\t\tv.val = sock_flag(sk, SOCK_BROADCAST);\n\t\tbreak;\n\n\tcase SO_SNDBUF:\n\t\tv.val = sk->sk_sndbuf;\n\t\tbreak;\n\n\tcase SO_RCVBUF:\n\t\tv.val = sk->sk_rcvbuf;\n\t\tbreak;\n\n\tcase SO_REUSEADDR:\n\t\tv.val = sk->sk_reuse;\n\t\tbreak;\n\n\tcase SO_REUSEPORT:\n\t\tv.val = sk->sk_reuseport;\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tv.val = sock_flag(sk, SOCK_KEEPOPEN);\n\t\tbreak;\n\n\tcase SO_TYPE:\n\t\tv.val = sk->sk_type;\n\t\tbreak;\n\n\tcase SO_PROTOCOL:\n\t\tv.val = sk->sk_protocol;\n\t\tbreak;\n\n\tcase SO_DOMAIN:\n\t\tv.val = sk->sk_family;\n\t\tbreak;\n\n\tcase SO_ERROR:\n\t\tv.val = -sock_error(sk);\n\t\tif (v.val == 0)\n\t\t\tv.val = xchg(&sk->sk_err_soft, 0);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tv.val = sock_flag(sk, SOCK_URGINLINE);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tv.val = sk->sk_no_check_tx;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tv.val = sk->sk_priority;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tlv\t\t= sizeof(v.ling);\n\t\tv.ling.l_onoff\t= sock_flag(sk, SOCK_LINGER);\n\t\tv.ling.l_linger\t= sk->sk_lingertime / HZ;\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) &&\n\t\t\t\t!sock_flag(sk, SOCK_TSTAMP_NEW) &&\n\t\t\t\t!sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && !sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING_OLD:\n\t\tlv = sizeof(v.timestamping);\n\t\tv.timestamping.flags = sk->sk_tsflags;\n\t\tv.timestamping.bind_phc = sk->sk_bind_phc;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO_OLD:\n\tcase SO_RCVTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_rcvtimeo, &v, SO_RCVTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO_OLD:\n\tcase SO_SNDTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_sndtimeo, &v, SO_SNDTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tv.val = sk->sk_rcvlowat;\n\t\tbreak;\n\n\tcase SO_SNDLOWAT:\n\t\tv.val = 1;\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tv.val = !!test_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERCRED:\n\t{\n\t\tstruct ucred peercred;\n\t\tif (len > sizeof(peercred))\n\t\t\tlen = sizeof(peercred);\n\t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n\t\tif (copy_to_user(optval, &peercred, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERGROUPS:\n\t{\n\t\tint ret, n;\n\n\t\tif (!sk->sk_peer_cred)\n\t\t\treturn -ENODATA;\n\n\t\tn = sk->sk_peer_cred->group_info->ngroups;\n\t\tif (len < n * sizeof(gid_t)) {\n\t\t\tlen = n * sizeof(gid_t);\n\t\t\treturn put_user(len, optlen) ? -EFAULT : -ERANGE;\n\t\t}\n\t\tlen = n * sizeof(gid_t);\n\n\t\tret = groups_to_user((gid_t __user *)optval,\n\t\t\t\t     sk->sk_peer_cred->group_info);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERNAME:\n\t{\n\t\tchar address[128];\n\n\t\tlv = sock->ops->getname(sock, (struct sockaddr *)address, 2);\n\t\tif (lv < 0)\n\t\t\treturn -ENOTCONN;\n\t\tif (lv < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(optval, address, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\t/* Dubious BSD thing... Probably nobody even uses it, but\n\t * the UNIX standard wants it for whatever reason... -DaveM\n\t */\n\tcase SO_ACCEPTCONN:\n\t\tv.val = sk->sk_state == TCP_LISTEN;\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tv.val = !!test_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERSEC:\n\t\treturn security_socket_getpeersec_stream(sock, optval, optlen, len);\n\n\tcase SO_MARK:\n\t\tv.val = sk->sk_mark;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tv.val = sock_flag(sk, SOCK_RXQ_OVFL);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tv.val = sock_flag(sk, SOCK_WIFI_STATUS);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (!sock->ops->set_peek_off)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tv.val = sk->sk_peek_off;\n\t\tbreak;\n\tcase SO_NOFCS:\n\t\tv.val = sock_flag(sk, SOCK_NOFCS);\n\t\tbreak;\n\n\tcase SO_BINDTODEVICE:\n\t\treturn sock_getbindtodevice(sk, optval, optlen, len);\n\n\tcase SO_GET_FILTER:\n\t\tlen = sk_get_filter(sk, (struct sock_filter __user *)optval, len);\n\t\tif (len < 0)\n\t\t\treturn len;\n\n\t\tgoto lenout;\n\n\tcase SO_LOCK_FILTER:\n\t\tv.val = sock_flag(sk, SOCK_FILTER_LOCKED);\n\t\tbreak;\n\n\tcase SO_BPF_EXTENSIONS:\n\t\tv.val = bpf_tell_extensions();\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tv.val = sock_flag(sk, SOCK_SELECT_ERR_QUEUE);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\tv.val = sk->sk_ll_usec;\n\t\tbreak;\n\tcase SO_PREFER_BUSY_POLL:\n\t\tv.val = READ_ONCE(sk->sk_prefer_busy_poll);\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\tif (sizeof(v.ulval) != sizeof(v.val) && len >= sizeof(v.ulval)) {\n\t\t\tlv = sizeof(v.ulval);\n\t\t\tv.ulval = sk->sk_max_pacing_rate;\n\t\t} else {\n\t\t\t/* 32bit version */\n\t\t\tv.val = min_t(unsigned long, sk->sk_max_pacing_rate, ~0U);\n\t\t}\n\t\tbreak;\n\n\tcase SO_INCOMING_CPU:\n\t\tv.val = READ_ONCE(sk->sk_incoming_cpu);\n\t\tbreak;\n\n\tcase SO_MEMINFO:\n\t{\n\t\tu32 meminfo[SK_MEMINFO_VARS];\n\n\t\tsk_get_meminfo(sk, meminfo);\n\n\t\tlen = min_t(unsigned int, len, sizeof(meminfo));\n\t\tif (copy_to_user(optval, &meminfo, len))\n\t\t\treturn -EFAULT;\n\n\t\tgoto lenout;\n\t}\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_INCOMING_NAPI_ID:\n\t\tv.val = READ_ONCE(sk->sk_napi_id);\n\n\t\t/* aggregate non-NAPI IDs down to 0 */\n\t\tif (v.val < MIN_NAPI_ID)\n\t\t\tv.val = 0;\n\n\t\tbreak;\n#endif\n\n\tcase SO_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len < lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_gen_cookie(sk);\n\t\tbreak;\n\n\tcase SO_ZEROCOPY:\n\t\tv.val = sock_flag(sk, SOCK_ZEROCOPY);\n\t\tbreak;\n\n\tcase SO_TXTIME:\n\t\tlv = sizeof(v.txtime);\n\t\tv.txtime.clockid = sk->sk_clockid;\n\t\tv.txtime.flags |= sk->sk_txtime_deadline_mode ?\n\t\t\t\t  SOF_TXTIME_DEADLINE_MODE : 0;\n\t\tv.txtime.flags |= sk->sk_txtime_report_errors ?\n\t\t\t\t  SOF_TXTIME_REPORT_ERRORS : 0;\n\t\tbreak;\n\n\tcase SO_BINDTOIFINDEX:\n\t\tv.val = sk->sk_bound_dev_if;\n\t\tbreak;\n\n\tcase SO_NETNS_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len != lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_net(sk)->net_cookie;\n\t\tbreak;\n\n\tcase SO_BUF_LOCK:\n\t\tv.val = sk->sk_userlocks & SOCK_BUF_LOCK_MASK;\n\t\tbreak;\n\n\tdefault:\n\t\t/* We implement the SO_SNDLOWAT etc to not be settable\n\t\t * (1003.1g 7).\n\t\t */\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (copy_to_user(optval, &v, len))\n\t\treturn -EFAULT;\nlenout:\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "code_after_change": "int sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tunion {\n\t\tint val;\n\t\tu64 val64;\n\t\tunsigned long ulval;\n\t\tstruct linger ling;\n\t\tstruct old_timeval32 tm32;\n\t\tstruct __kernel_old_timeval tm;\n\t\tstruct  __kernel_sock_timeval stm;\n\t\tstruct sock_txtime txtime;\n\t\tstruct so_timestamping timestamping;\n\t} v;\n\n\tint lv = sizeof(int);\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tmemset(&v, 0, sizeof(v));\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tv.val = sock_flag(sk, SOCK_DBG);\n\t\tbreak;\n\n\tcase SO_DONTROUTE:\n\t\tv.val = sock_flag(sk, SOCK_LOCALROUTE);\n\t\tbreak;\n\n\tcase SO_BROADCAST:\n\t\tv.val = sock_flag(sk, SOCK_BROADCAST);\n\t\tbreak;\n\n\tcase SO_SNDBUF:\n\t\tv.val = sk->sk_sndbuf;\n\t\tbreak;\n\n\tcase SO_RCVBUF:\n\t\tv.val = sk->sk_rcvbuf;\n\t\tbreak;\n\n\tcase SO_REUSEADDR:\n\t\tv.val = sk->sk_reuse;\n\t\tbreak;\n\n\tcase SO_REUSEPORT:\n\t\tv.val = sk->sk_reuseport;\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tv.val = sock_flag(sk, SOCK_KEEPOPEN);\n\t\tbreak;\n\n\tcase SO_TYPE:\n\t\tv.val = sk->sk_type;\n\t\tbreak;\n\n\tcase SO_PROTOCOL:\n\t\tv.val = sk->sk_protocol;\n\t\tbreak;\n\n\tcase SO_DOMAIN:\n\t\tv.val = sk->sk_family;\n\t\tbreak;\n\n\tcase SO_ERROR:\n\t\tv.val = -sock_error(sk);\n\t\tif (v.val == 0)\n\t\t\tv.val = xchg(&sk->sk_err_soft, 0);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tv.val = sock_flag(sk, SOCK_URGINLINE);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tv.val = sk->sk_no_check_tx;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tv.val = sk->sk_priority;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tlv\t\t= sizeof(v.ling);\n\t\tv.ling.l_onoff\t= sock_flag(sk, SOCK_LINGER);\n\t\tv.ling.l_linger\t= sk->sk_lingertime / HZ;\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) &&\n\t\t\t\t!sock_flag(sk, SOCK_TSTAMP_NEW) &&\n\t\t\t\t!sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && !sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING_OLD:\n\t\tlv = sizeof(v.timestamping);\n\t\tv.timestamping.flags = sk->sk_tsflags;\n\t\tv.timestamping.bind_phc = sk->sk_bind_phc;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO_OLD:\n\tcase SO_RCVTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_rcvtimeo, &v, SO_RCVTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO_OLD:\n\tcase SO_SNDTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_sndtimeo, &v, SO_SNDTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tv.val = sk->sk_rcvlowat;\n\t\tbreak;\n\n\tcase SO_SNDLOWAT:\n\t\tv.val = 1;\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tv.val = !!test_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERCRED:\n\t{\n\t\tstruct ucred peercred;\n\t\tif (len > sizeof(peercred))\n\t\t\tlen = sizeof(peercred);\n\n\t\tspin_lock(&sk->sk_peer_lock);\n\t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n\t\tspin_unlock(&sk->sk_peer_lock);\n\n\t\tif (copy_to_user(optval, &peercred, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERGROUPS:\n\t{\n\t\tconst struct cred *cred;\n\t\tint ret, n;\n\n\t\tcred = sk_get_peer_cred(sk);\n\t\tif (!cred)\n\t\t\treturn -ENODATA;\n\n\t\tn = cred->group_info->ngroups;\n\t\tif (len < n * sizeof(gid_t)) {\n\t\t\tlen = n * sizeof(gid_t);\n\t\t\tput_cred(cred);\n\t\t\treturn put_user(len, optlen) ? -EFAULT : -ERANGE;\n\t\t}\n\t\tlen = n * sizeof(gid_t);\n\n\t\tret = groups_to_user((gid_t __user *)optval, cred->group_info);\n\t\tput_cred(cred);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERNAME:\n\t{\n\t\tchar address[128];\n\n\t\tlv = sock->ops->getname(sock, (struct sockaddr *)address, 2);\n\t\tif (lv < 0)\n\t\t\treturn -ENOTCONN;\n\t\tif (lv < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(optval, address, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\t/* Dubious BSD thing... Probably nobody even uses it, but\n\t * the UNIX standard wants it for whatever reason... -DaveM\n\t */\n\tcase SO_ACCEPTCONN:\n\t\tv.val = sk->sk_state == TCP_LISTEN;\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tv.val = !!test_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERSEC:\n\t\treturn security_socket_getpeersec_stream(sock, optval, optlen, len);\n\n\tcase SO_MARK:\n\t\tv.val = sk->sk_mark;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tv.val = sock_flag(sk, SOCK_RXQ_OVFL);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tv.val = sock_flag(sk, SOCK_WIFI_STATUS);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (!sock->ops->set_peek_off)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tv.val = sk->sk_peek_off;\n\t\tbreak;\n\tcase SO_NOFCS:\n\t\tv.val = sock_flag(sk, SOCK_NOFCS);\n\t\tbreak;\n\n\tcase SO_BINDTODEVICE:\n\t\treturn sock_getbindtodevice(sk, optval, optlen, len);\n\n\tcase SO_GET_FILTER:\n\t\tlen = sk_get_filter(sk, (struct sock_filter __user *)optval, len);\n\t\tif (len < 0)\n\t\t\treturn len;\n\n\t\tgoto lenout;\n\n\tcase SO_LOCK_FILTER:\n\t\tv.val = sock_flag(sk, SOCK_FILTER_LOCKED);\n\t\tbreak;\n\n\tcase SO_BPF_EXTENSIONS:\n\t\tv.val = bpf_tell_extensions();\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tv.val = sock_flag(sk, SOCK_SELECT_ERR_QUEUE);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\tv.val = sk->sk_ll_usec;\n\t\tbreak;\n\tcase SO_PREFER_BUSY_POLL:\n\t\tv.val = READ_ONCE(sk->sk_prefer_busy_poll);\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\tif (sizeof(v.ulval) != sizeof(v.val) && len >= sizeof(v.ulval)) {\n\t\t\tlv = sizeof(v.ulval);\n\t\t\tv.ulval = sk->sk_max_pacing_rate;\n\t\t} else {\n\t\t\t/* 32bit version */\n\t\t\tv.val = min_t(unsigned long, sk->sk_max_pacing_rate, ~0U);\n\t\t}\n\t\tbreak;\n\n\tcase SO_INCOMING_CPU:\n\t\tv.val = READ_ONCE(sk->sk_incoming_cpu);\n\t\tbreak;\n\n\tcase SO_MEMINFO:\n\t{\n\t\tu32 meminfo[SK_MEMINFO_VARS];\n\n\t\tsk_get_meminfo(sk, meminfo);\n\n\t\tlen = min_t(unsigned int, len, sizeof(meminfo));\n\t\tif (copy_to_user(optval, &meminfo, len))\n\t\t\treturn -EFAULT;\n\n\t\tgoto lenout;\n\t}\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_INCOMING_NAPI_ID:\n\t\tv.val = READ_ONCE(sk->sk_napi_id);\n\n\t\t/* aggregate non-NAPI IDs down to 0 */\n\t\tif (v.val < MIN_NAPI_ID)\n\t\t\tv.val = 0;\n\n\t\tbreak;\n#endif\n\n\tcase SO_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len < lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_gen_cookie(sk);\n\t\tbreak;\n\n\tcase SO_ZEROCOPY:\n\t\tv.val = sock_flag(sk, SOCK_ZEROCOPY);\n\t\tbreak;\n\n\tcase SO_TXTIME:\n\t\tlv = sizeof(v.txtime);\n\t\tv.txtime.clockid = sk->sk_clockid;\n\t\tv.txtime.flags |= sk->sk_txtime_deadline_mode ?\n\t\t\t\t  SOF_TXTIME_DEADLINE_MODE : 0;\n\t\tv.txtime.flags |= sk->sk_txtime_report_errors ?\n\t\t\t\t  SOF_TXTIME_REPORT_ERRORS : 0;\n\t\tbreak;\n\n\tcase SO_BINDTOIFINDEX:\n\t\tv.val = sk->sk_bound_dev_if;\n\t\tbreak;\n\n\tcase SO_NETNS_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len != lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_net(sk)->net_cookie;\n\t\tbreak;\n\n\tcase SO_BUF_LOCK:\n\t\tv.val = sk->sk_userlocks & SOCK_BUF_LOCK_MASK;\n\t\tbreak;\n\n\tdefault:\n\t\t/* We implement the SO_SNDLOWAT etc to not be settable\n\t\t * (1003.1g 7).\n\t\t */\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (copy_to_user(optval, &v, len))\n\t\treturn -EFAULT;\nlenout:\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -148,7 +148,11 @@\n \t\tstruct ucred peercred;\n \t\tif (len > sizeof(peercred))\n \t\t\tlen = sizeof(peercred);\n+\n+\t\tspin_lock(&sk->sk_peer_lock);\n \t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n+\t\tspin_unlock(&sk->sk_peer_lock);\n+\n \t\tif (copy_to_user(optval, &peercred, len))\n \t\t\treturn -EFAULT;\n \t\tgoto lenout;\n@@ -156,20 +160,23 @@\n \n \tcase SO_PEERGROUPS:\n \t{\n+\t\tconst struct cred *cred;\n \t\tint ret, n;\n \n-\t\tif (!sk->sk_peer_cred)\n+\t\tcred = sk_get_peer_cred(sk);\n+\t\tif (!cred)\n \t\t\treturn -ENODATA;\n \n-\t\tn = sk->sk_peer_cred->group_info->ngroups;\n+\t\tn = cred->group_info->ngroups;\n \t\tif (len < n * sizeof(gid_t)) {\n \t\t\tlen = n * sizeof(gid_t);\n+\t\t\tput_cred(cred);\n \t\t\treturn put_user(len, optlen) ? -EFAULT : -ERANGE;\n \t\t}\n \t\tlen = n * sizeof(gid_t);\n \n-\t\tret = groups_to_user((gid_t __user *)optval,\n-\t\t\t\t     sk->sk_peer_cred->group_info);\n+\t\tret = groups_to_user((gid_t __user *)optval, cred->group_info);\n+\t\tput_cred(cred);\n \t\tif (ret)\n \t\t\treturn ret;\n \t\tgoto lenout;",
        "function_modified_lines": {
            "added": [
                "",
                "\t\tspin_lock(&sk->sk_peer_lock);",
                "\t\tspin_unlock(&sk->sk_peer_lock);",
                "",
                "\t\tconst struct cred *cred;",
                "\t\tcred = sk_get_peer_cred(sk);",
                "\t\tif (!cred)",
                "\t\tn = cred->group_info->ngroups;",
                "\t\t\tput_cred(cred);",
                "\t\tret = groups_to_user((gid_t __user *)optval, cred->group_info);",
                "\t\tput_cred(cred);"
            ],
            "deleted": [
                "\t\tif (!sk->sk_peer_cred)",
                "\t\tn = sk->sk_peer_cred->group_info->ngroups;",
                "\t\tret = groups_to_user((gid_t __user *)optval,",
                "\t\t\t\t     sk->sk_peer_cred->group_info);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information."
    },
    {
        "cve_id": "CVE-2021-4203",
        "code_before_change": "static void __sk_destruct(struct rcu_head *head)\n{\n\tstruct sock *sk = container_of(head, struct sock, sk_rcu);\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       refcount_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n#ifdef CONFIG_BPF_SYSCALL\n\tbpf_sk_storage_free(sk);\n#endif\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tpr_debug(\"%s: optmem leakage (%d bytes) detected\\n\",\n\t\t\t __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t}\n\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\tif (likely(sk->sk_net_refcnt))\n\t\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}",
        "code_after_change": "static void __sk_destruct(struct rcu_head *head)\n{\n\tstruct sock *sk = container_of(head, struct sock, sk_rcu);\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       refcount_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n#ifdef CONFIG_BPF_SYSCALL\n\tbpf_sk_storage_free(sk);\n#endif\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tpr_debug(\"%s: optmem leakage (%d bytes) detected\\n\",\n\t\t\t __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t}\n\n\t/* We do not need to acquire sk->sk_peer_lock, we are the last user. */\n\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\n\tif (likely(sk->sk_net_refcnt))\n\t\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,9 +28,10 @@\n \t\tsk->sk_frag.page = NULL;\n \t}\n \n-\tif (sk->sk_peer_cred)\n-\t\tput_cred(sk->sk_peer_cred);\n+\t/* We do not need to acquire sk->sk_peer_lock, we are the last user. */\n+\tput_cred(sk->sk_peer_cred);\n \tput_pid(sk->sk_peer_pid);\n+\n \tif (likely(sk->sk_net_refcnt))\n \t\tput_net(sock_net(sk));\n \tsk_prot_free(sk->sk_prot_creator, sk);",
        "function_modified_lines": {
            "added": [
                "\t/* We do not need to acquire sk->sk_peer_lock, we are the last user. */",
                "\tput_cred(sk->sk_peer_cred);",
                ""
            ],
            "deleted": [
                "\tif (sk->sk_peer_cred)",
                "\t\tput_cred(sk->sk_peer_cred);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information."
    },
    {
        "cve_id": "CVE-2021-4203",
        "code_before_change": "void sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tsk_init_common(sk);\n\tsk->sk_send_head\t=\tNULL;\n\n\ttimer_setup(&sk->sk_timer, NULL, 0);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tRCU_INIT_POINTER(sk->sk_wq, &sock->wq);\n\t\tsock->sk\t=\tsk;\n\t\tsk->sk_uid\t=\tSOCK_INODE(sock)->i_uid;\n\t} else {\n\t\tRCU_INIT_POINTER(sk->sk_wq, NULL);\n\t\tsk->sk_uid\t=\tmake_kuid(sock_net(sk)->user_ns, 0);\n\t}\n\n\trwlock_init(&sk->sk_callback_lock);\n\tif (sk->sk_kern_sock)\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_kern_callback_keys + sk->sk_family,\n\t\t\taf_family_kern_clock_key_strings[sk->sk_family]);\n\telse\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_frag.page\t=\tNULL;\n\tsk->sk_frag.offset\t=\t0;\n\tsk->sk_peek_off\t\t=\t-1;\n\n\tsk->sk_peer_pid \t=\tNULL;\n\tsk->sk_peer_cred\t=\tNULL;\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = SK_DEFAULT_STAMP;\n#if BITS_PER_LONG==32\n\tseqlock_init(&sk->sk_stamp_seq);\n#endif\n\tatomic_set(&sk->sk_zckey, 0);\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tsk->sk_napi_id\t\t=\t0;\n\tsk->sk_ll_usec\t\t=\tsysctl_net_busy_read;\n#endif\n\n\tsk->sk_max_pacing_rate = ~0UL;\n\tsk->sk_pacing_rate = ~0UL;\n\tWRITE_ONCE(sk->sk_pacing_shift, 10);\n\tsk->sk_incoming_cpu = -1;\n\n\tsk_rx_queue_clear(sk);\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.rst for details)\n\t */\n\tsmp_wmb();\n\trefcount_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}",
        "code_after_change": "void sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tsk_init_common(sk);\n\tsk->sk_send_head\t=\tNULL;\n\n\ttimer_setup(&sk->sk_timer, NULL, 0);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tRCU_INIT_POINTER(sk->sk_wq, &sock->wq);\n\t\tsock->sk\t=\tsk;\n\t\tsk->sk_uid\t=\tSOCK_INODE(sock)->i_uid;\n\t} else {\n\t\tRCU_INIT_POINTER(sk->sk_wq, NULL);\n\t\tsk->sk_uid\t=\tmake_kuid(sock_net(sk)->user_ns, 0);\n\t}\n\n\trwlock_init(&sk->sk_callback_lock);\n\tif (sk->sk_kern_sock)\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_kern_callback_keys + sk->sk_family,\n\t\t\taf_family_kern_clock_key_strings[sk->sk_family]);\n\telse\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_frag.page\t=\tNULL;\n\tsk->sk_frag.offset\t=\t0;\n\tsk->sk_peek_off\t\t=\t-1;\n\n\tsk->sk_peer_pid \t=\tNULL;\n\tsk->sk_peer_cred\t=\tNULL;\n\tspin_lock_init(&sk->sk_peer_lock);\n\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = SK_DEFAULT_STAMP;\n#if BITS_PER_LONG==32\n\tseqlock_init(&sk->sk_stamp_seq);\n#endif\n\tatomic_set(&sk->sk_zckey, 0);\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tsk->sk_napi_id\t\t=\t0;\n\tsk->sk_ll_usec\t\t=\tsysctl_net_busy_read;\n#endif\n\n\tsk->sk_max_pacing_rate = ~0UL;\n\tsk->sk_pacing_rate = ~0UL;\n\tWRITE_ONCE(sk->sk_pacing_shift, 10);\n\tsk->sk_incoming_cpu = -1;\n\n\tsk_rx_queue_clear(sk);\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.rst for details)\n\t */\n\tsmp_wmb();\n\trefcount_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,6 +47,8 @@\n \n \tsk->sk_peer_pid \t=\tNULL;\n \tsk->sk_peer_cred\t=\tNULL;\n+\tspin_lock_init(&sk->sk_peer_lock);\n+\n \tsk->sk_write_pending\t=\t0;\n \tsk->sk_rcvlowat\t\t=\t1;\n \tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;",
        "function_modified_lines": {
            "added": [
                "\tspin_lock_init(&sk->sk_peer_lock);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information."
    },
    {
        "cve_id": "CVE-2021-4203",
        "code_before_change": "static void copy_peercred(struct sock *sk, struct sock *peersk)\n{\n\tput_pid(sk->sk_peer_pid);\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tsk->sk_peer_pid  = get_pid(peersk->sk_peer_pid);\n\tsk->sk_peer_cred = get_cred(peersk->sk_peer_cred);\n}",
        "code_after_change": "static void copy_peercred(struct sock *sk, struct sock *peersk)\n{\n\tconst struct cred *old_cred;\n\tstruct pid *old_pid;\n\n\tif (sk < peersk) {\n\t\tspin_lock(&sk->sk_peer_lock);\n\t\tspin_lock_nested(&peersk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n\t} else {\n\t\tspin_lock(&peersk->sk_peer_lock);\n\t\tspin_lock_nested(&sk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n\t}\n\told_pid = sk->sk_peer_pid;\n\told_cred = sk->sk_peer_cred;\n\tsk->sk_peer_pid  = get_pid(peersk->sk_peer_pid);\n\tsk->sk_peer_cred = get_cred(peersk->sk_peer_cred);\n\n\tspin_unlock(&sk->sk_peer_lock);\n\tspin_unlock(&peersk->sk_peer_lock);\n\n\tput_pid(old_pid);\n\tput_cred(old_cred);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,23 @@\n static void copy_peercred(struct sock *sk, struct sock *peersk)\n {\n-\tput_pid(sk->sk_peer_pid);\n-\tif (sk->sk_peer_cred)\n-\t\tput_cred(sk->sk_peer_cred);\n+\tconst struct cred *old_cred;\n+\tstruct pid *old_pid;\n+\n+\tif (sk < peersk) {\n+\t\tspin_lock(&sk->sk_peer_lock);\n+\t\tspin_lock_nested(&peersk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n+\t} else {\n+\t\tspin_lock(&peersk->sk_peer_lock);\n+\t\tspin_lock_nested(&sk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n+\t}\n+\told_pid = sk->sk_peer_pid;\n+\told_cred = sk->sk_peer_cred;\n \tsk->sk_peer_pid  = get_pid(peersk->sk_peer_pid);\n \tsk->sk_peer_cred = get_cred(peersk->sk_peer_cred);\n+\n+\tspin_unlock(&sk->sk_peer_lock);\n+\tspin_unlock(&peersk->sk_peer_lock);\n+\n+\tput_pid(old_pid);\n+\tput_cred(old_cred);\n }",
        "function_modified_lines": {
            "added": [
                "\tconst struct cred *old_cred;",
                "\tstruct pid *old_pid;",
                "",
                "\tif (sk < peersk) {",
                "\t\tspin_lock(&sk->sk_peer_lock);",
                "\t\tspin_lock_nested(&peersk->sk_peer_lock, SINGLE_DEPTH_NESTING);",
                "\t} else {",
                "\t\tspin_lock(&peersk->sk_peer_lock);",
                "\t\tspin_lock_nested(&sk->sk_peer_lock, SINGLE_DEPTH_NESTING);",
                "\t}",
                "\told_pid = sk->sk_peer_pid;",
                "\told_cred = sk->sk_peer_cred;",
                "",
                "\tspin_unlock(&sk->sk_peer_lock);",
                "\tspin_unlock(&peersk->sk_peer_lock);",
                "",
                "\tput_pid(old_pid);",
                "\tput_cred(old_cred);"
            ],
            "deleted": [
                "\tput_pid(sk->sk_peer_pid);",
                "\tif (sk->sk_peer_cred)",
                "\t\tput_cred(sk->sk_peer_cred);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information."
    },
    {
        "cve_id": "CVE-2021-4203",
        "code_before_change": "static void init_peercred(struct sock *sk)\n{\n\tput_pid(sk->sk_peer_pid);\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tsk->sk_peer_pid  = get_pid(task_tgid(current));\n\tsk->sk_peer_cred = get_current_cred();\n}",
        "code_after_change": "static void init_peercred(struct sock *sk)\n{\n\tconst struct cred *old_cred;\n\tstruct pid *old_pid;\n\n\tspin_lock(&sk->sk_peer_lock);\n\told_pid = sk->sk_peer_pid;\n\told_cred = sk->sk_peer_cred;\n\tsk->sk_peer_pid  = get_pid(task_tgid(current));\n\tsk->sk_peer_cred = get_current_cred();\n\tspin_unlock(&sk->sk_peer_lock);\n\n\tput_pid(old_pid);\n\tput_cred(old_cred);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,15 @@\n static void init_peercred(struct sock *sk)\n {\n-\tput_pid(sk->sk_peer_pid);\n-\tif (sk->sk_peer_cred)\n-\t\tput_cred(sk->sk_peer_cred);\n+\tconst struct cred *old_cred;\n+\tstruct pid *old_pid;\n+\n+\tspin_lock(&sk->sk_peer_lock);\n+\told_pid = sk->sk_peer_pid;\n+\told_cred = sk->sk_peer_cred;\n \tsk->sk_peer_pid  = get_pid(task_tgid(current));\n \tsk->sk_peer_cred = get_current_cred();\n+\tspin_unlock(&sk->sk_peer_lock);\n+\n+\tput_pid(old_pid);\n+\tput_cred(old_cred);\n }",
        "function_modified_lines": {
            "added": [
                "\tconst struct cred *old_cred;",
                "\tstruct pid *old_pid;",
                "",
                "\tspin_lock(&sk->sk_peer_lock);",
                "\told_pid = sk->sk_peer_pid;",
                "\told_cred = sk->sk_peer_cred;",
                "\tspin_unlock(&sk->sk_peer_lock);",
                "",
                "\tput_pid(old_pid);",
                "\tput_cred(old_cred);"
            ],
            "deleted": [
                "\tput_pid(sk->sk_peer_pid);",
                "\tif (sk->sk_peer_cred)",
                "\t\tput_cred(sk->sk_peer_cred);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information."
    },
    {
        "cve_id": "CVE-2021-44733",
        "code_before_change": "void tee_shm_free(struct tee_shm *shm)\n{\n\t/*\n\t * dma_buf_put() decreases the dmabuf reference counter and will\n\t * call tee_shm_release() when the last reference is gone.\n\t *\n\t * In the case of driver private memory we call tee_shm_release\n\t * directly instead as it doesn't have a reference counter.\n\t */\n\tif (shm->flags & TEE_SHM_DMA_BUF)\n\t\tdma_buf_put(shm->dmabuf);\n\telse\n\t\ttee_shm_release(shm);\n}",
        "code_after_change": "void tee_shm_free(struct tee_shm *shm)\n{\n\ttee_shm_put(shm);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,14 +1,4 @@\n void tee_shm_free(struct tee_shm *shm)\n {\n-\t/*\n-\t * dma_buf_put() decreases the dmabuf reference counter and will\n-\t * call tee_shm_release() when the last reference is gone.\n-\t *\n-\t * In the case of driver private memory we call tee_shm_release\n-\t * directly instead as it doesn't have a reference counter.\n-\t */\n-\tif (shm->flags & TEE_SHM_DMA_BUF)\n-\t\tdma_buf_put(shm->dmabuf);\n-\telse\n-\t\ttee_shm_release(shm);\n+\ttee_shm_put(shm);\n }",
        "function_modified_lines": {
            "added": [
                "\ttee_shm_put(shm);"
            ],
            "deleted": [
                "\t/*",
                "\t * dma_buf_put() decreases the dmabuf reference counter and will",
                "\t * call tee_shm_release() when the last reference is gone.",
                "\t *",
                "\t * In the case of driver private memory we call tee_shm_release",
                "\t * directly instead as it doesn't have a reference counter.",
                "\t */",
                "\tif (shm->flags & TEE_SHM_DMA_BUF)",
                "\t\tdma_buf_put(shm->dmabuf);",
                "\telse",
                "\t\ttee_shm_release(shm);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object."
    },
    {
        "cve_id": "CVE-2021-44733",
        "code_before_change": "struct tee_shm *tee_shm_get_from_id(struct tee_context *ctx, int id)\n{\n\tstruct tee_device *teedev;\n\tstruct tee_shm *shm;\n\n\tif (!ctx)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tteedev = ctx->teedev;\n\tmutex_lock(&teedev->mutex);\n\tshm = idr_find(&teedev->idr, id);\n\tif (!shm || shm->ctx != ctx)\n\t\tshm = ERR_PTR(-EINVAL);\n\telse if (shm->flags & TEE_SHM_DMA_BUF)\n\t\tget_dma_buf(shm->dmabuf);\n\tmutex_unlock(&teedev->mutex);\n\treturn shm;\n}",
        "code_after_change": "struct tee_shm *tee_shm_get_from_id(struct tee_context *ctx, int id)\n{\n\tstruct tee_device *teedev;\n\tstruct tee_shm *shm;\n\n\tif (!ctx)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tteedev = ctx->teedev;\n\tmutex_lock(&teedev->mutex);\n\tshm = idr_find(&teedev->idr, id);\n\t/*\n\t * If the tee_shm was found in the IDR it must have a refcount\n\t * larger than 0 due to the guarantee in tee_shm_put() below. So\n\t * it's safe to use refcount_inc().\n\t */\n\tif (!shm || shm->ctx != ctx)\n\t\tshm = ERR_PTR(-EINVAL);\n\telse\n\t\trefcount_inc(&shm->refcount);\n\tmutex_unlock(&teedev->mutex);\n\treturn shm;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,10 +9,15 @@\n \tteedev = ctx->teedev;\n \tmutex_lock(&teedev->mutex);\n \tshm = idr_find(&teedev->idr, id);\n+\t/*\n+\t * If the tee_shm was found in the IDR it must have a refcount\n+\t * larger than 0 due to the guarantee in tee_shm_put() below. So\n+\t * it's safe to use refcount_inc().\n+\t */\n \tif (!shm || shm->ctx != ctx)\n \t\tshm = ERR_PTR(-EINVAL);\n-\telse if (shm->flags & TEE_SHM_DMA_BUF)\n-\t\tget_dma_buf(shm->dmabuf);\n+\telse\n+\t\trefcount_inc(&shm->refcount);\n \tmutex_unlock(&teedev->mutex);\n \treturn shm;\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * If the tee_shm was found in the IDR it must have a refcount",
                "\t * larger than 0 due to the guarantee in tee_shm_put() below. So",
                "\t * it's safe to use refcount_inc().",
                "\t */",
                "\telse",
                "\t\trefcount_inc(&shm->refcount);"
            ],
            "deleted": [
                "\telse if (shm->flags & TEE_SHM_DMA_BUF)",
                "\t\tget_dma_buf(shm->dmabuf);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object."
    },
    {
        "cve_id": "CVE-2021-44733",
        "code_before_change": "struct tee_shm *tee_shm_register(struct tee_context *ctx, unsigned long addr,\n\t\t\t\t size_t length, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tconst u32 req_user_flags = TEE_SHM_DMA_BUF | TEE_SHM_USER_MAPPED;\n\tconst u32 req_kernel_flags = TEE_SHM_DMA_BUF | TEE_SHM_KERNEL_MAPPED;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\tint num_pages;\n\tunsigned long start;\n\n\tif (flags != req_user_flags && flags != req_kernel_flags)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->desc->ops->shm_register ||\n\t    !teedev->desc->ops->shm_unregister) {\n\t\ttee_device_put(teedev);\n\t\treturn ERR_PTR(-ENOTSUPP);\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\tshm->flags = flags | TEE_SHM_REGISTER;\n\tshm->ctx = ctx;\n\tshm->id = -1;\n\taddr = untagged_addr(addr);\n\tstart = rounddown(addr, PAGE_SIZE);\n\tshm->offset = addr - start;\n\tshm->size = length;\n\tnum_pages = (roundup(addr + length, PAGE_SIZE) - start) / PAGE_SIZE;\n\tshm->pages = kcalloc(num_pages, sizeof(*shm->pages), GFP_KERNEL);\n\tif (!shm->pages) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\tif (flags & TEE_SHM_USER_MAPPED) {\n\t\trc = pin_user_pages_fast(start, num_pages, FOLL_WRITE,\n\t\t\t\t\t shm->pages);\n\t} else {\n\t\tstruct kvec *kiov;\n\t\tint i;\n\n\t\tkiov = kcalloc(num_pages, sizeof(*kiov), GFP_KERNEL);\n\t\tif (!kiov) {\n\t\t\tret = ERR_PTR(-ENOMEM);\n\t\t\tgoto err;\n\t\t}\n\n\t\tfor (i = 0; i < num_pages; i++) {\n\t\t\tkiov[i].iov_base = (void *)(start + i * PAGE_SIZE);\n\t\t\tkiov[i].iov_len = PAGE_SIZE;\n\t\t}\n\n\t\trc = get_kernel_pages(kiov, num_pages, 0, shm->pages);\n\t\tkfree(kiov);\n\t}\n\tif (rc > 0)\n\t\tshm->num_pages = rc;\n\tif (rc != num_pages) {\n\t\tif (rc >= 0)\n\t\t\trc = -ENOMEM;\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\tmutex_lock(&teedev->mutex);\n\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\tmutex_unlock(&teedev->mutex);\n\n\tif (shm->id < 0) {\n\t\tret = ERR_PTR(shm->id);\n\t\tgoto err;\n\t}\n\n\trc = teedev->desc->ops->shm_register(ctx, shm, shm->pages,\n\t\t\t\t\t     shm->num_pages, start);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\n\t\texp_info.ops = &tee_shm_dma_buf_ops;\n\t\texp_info.size = shm->size;\n\t\texp_info.flags = O_RDWR;\n\t\texp_info.priv = shm;\n\n\t\tshm->dmabuf = dma_buf_export(&exp_info);\n\t\tif (IS_ERR(shm->dmabuf)) {\n\t\t\tret = ERR_CAST(shm->dmabuf);\n\t\t\tteedev->desc->ops->shm_unregister(ctx, shm);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\treturn shm;\nerr:\n\tif (shm) {\n\t\tif (shm->id >= 0) {\n\t\t\tmutex_lock(&teedev->mutex);\n\t\t\tidr_remove(&teedev->idr, shm->id);\n\t\t\tmutex_unlock(&teedev->mutex);\n\t\t}\n\t\trelease_registered_pages(shm);\n\t}\n\tkfree(shm);\n\tteedev_ctx_put(ctx);\n\ttee_device_put(teedev);\n\treturn ret;\n}",
        "code_after_change": "struct tee_shm *tee_shm_register(struct tee_context *ctx, unsigned long addr,\n\t\t\t\t size_t length, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tconst u32 req_user_flags = TEE_SHM_DMA_BUF | TEE_SHM_USER_MAPPED;\n\tconst u32 req_kernel_flags = TEE_SHM_DMA_BUF | TEE_SHM_KERNEL_MAPPED;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\tint num_pages;\n\tunsigned long start;\n\n\tif (flags != req_user_flags && flags != req_kernel_flags)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->desc->ops->shm_register ||\n\t    !teedev->desc->ops->shm_unregister) {\n\t\ttee_device_put(teedev);\n\t\treturn ERR_PTR(-ENOTSUPP);\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\trefcount_set(&shm->refcount, 1);\n\tshm->flags = flags | TEE_SHM_REGISTER;\n\tshm->ctx = ctx;\n\tshm->id = -1;\n\taddr = untagged_addr(addr);\n\tstart = rounddown(addr, PAGE_SIZE);\n\tshm->offset = addr - start;\n\tshm->size = length;\n\tnum_pages = (roundup(addr + length, PAGE_SIZE) - start) / PAGE_SIZE;\n\tshm->pages = kcalloc(num_pages, sizeof(*shm->pages), GFP_KERNEL);\n\tif (!shm->pages) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\tif (flags & TEE_SHM_USER_MAPPED) {\n\t\trc = pin_user_pages_fast(start, num_pages, FOLL_WRITE,\n\t\t\t\t\t shm->pages);\n\t} else {\n\t\tstruct kvec *kiov;\n\t\tint i;\n\n\t\tkiov = kcalloc(num_pages, sizeof(*kiov), GFP_KERNEL);\n\t\tif (!kiov) {\n\t\t\tret = ERR_PTR(-ENOMEM);\n\t\t\tgoto err;\n\t\t}\n\n\t\tfor (i = 0; i < num_pages; i++) {\n\t\t\tkiov[i].iov_base = (void *)(start + i * PAGE_SIZE);\n\t\t\tkiov[i].iov_len = PAGE_SIZE;\n\t\t}\n\n\t\trc = get_kernel_pages(kiov, num_pages, 0, shm->pages);\n\t\tkfree(kiov);\n\t}\n\tif (rc > 0)\n\t\tshm->num_pages = rc;\n\tif (rc != num_pages) {\n\t\tif (rc >= 0)\n\t\t\trc = -ENOMEM;\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\tmutex_lock(&teedev->mutex);\n\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\tmutex_unlock(&teedev->mutex);\n\n\tif (shm->id < 0) {\n\t\tret = ERR_PTR(shm->id);\n\t\tgoto err;\n\t}\n\n\trc = teedev->desc->ops->shm_register(ctx, shm, shm->pages,\n\t\t\t\t\t     shm->num_pages, start);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\treturn shm;\nerr:\n\tif (shm) {\n\t\tif (shm->id >= 0) {\n\t\t\tmutex_lock(&teedev->mutex);\n\t\t\tidr_remove(&teedev->idr, shm->id);\n\t\t\tmutex_unlock(&teedev->mutex);\n\t\t}\n\t\trelease_registered_pages(shm);\n\t}\n\tkfree(shm);\n\tteedev_ctx_put(ctx);\n\ttee_device_put(teedev);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,6 +30,7 @@\n \t\tgoto err;\n \t}\n \n+\trefcount_set(&shm->refcount, 1);\n \tshm->flags = flags | TEE_SHM_REGISTER;\n \tshm->ctx = ctx;\n \tshm->id = -1;\n@@ -90,22 +91,6 @@\n \t\tgoto err;\n \t}\n \n-\tif (flags & TEE_SHM_DMA_BUF) {\n-\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n-\n-\t\texp_info.ops = &tee_shm_dma_buf_ops;\n-\t\texp_info.size = shm->size;\n-\t\texp_info.flags = O_RDWR;\n-\t\texp_info.priv = shm;\n-\n-\t\tshm->dmabuf = dma_buf_export(&exp_info);\n-\t\tif (IS_ERR(shm->dmabuf)) {\n-\t\t\tret = ERR_CAST(shm->dmabuf);\n-\t\t\tteedev->desc->ops->shm_unregister(ctx, shm);\n-\t\t\tgoto err;\n-\t\t}\n-\t}\n-\n \treturn shm;\n err:\n \tif (shm) {",
        "function_modified_lines": {
            "added": [
                "\trefcount_set(&shm->refcount, 1);"
            ],
            "deleted": [
                "\tif (flags & TEE_SHM_DMA_BUF) {",
                "\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);",
                "",
                "\t\texp_info.ops = &tee_shm_dma_buf_ops;",
                "\t\texp_info.size = shm->size;",
                "\t\texp_info.flags = O_RDWR;",
                "\t\texp_info.priv = shm;",
                "",
                "\t\tshm->dmabuf = dma_buf_export(&exp_info);",
                "\t\tif (IS_ERR(shm->dmabuf)) {",
                "\t\t\tret = ERR_CAST(shm->dmabuf);",
                "\t\t\tteedev->desc->ops->shm_unregister(ctx, shm);",
                "\t\t\tgoto err;",
                "\t\t}",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object."
    },
    {
        "cve_id": "CVE-2021-44733",
        "code_before_change": "void tee_shm_put(struct tee_shm *shm)\n{\n\tif (shm->flags & TEE_SHM_DMA_BUF)\n\t\tdma_buf_put(shm->dmabuf);\n}",
        "code_after_change": "void tee_shm_put(struct tee_shm *shm)\n{\n\tstruct tee_device *teedev = shm->ctx->teedev;\n\tbool do_release = false;\n\n\tmutex_lock(&teedev->mutex);\n\tif (refcount_dec_and_test(&shm->refcount)) {\n\t\t/*\n\t\t * refcount has reached 0, we must now remove it from the\n\t\t * IDR before releasing the mutex. This will guarantee that\n\t\t * the refcount_inc() in tee_shm_get_from_id() never starts\n\t\t * from 0.\n\t\t */\n\t\tif (shm->flags & TEE_SHM_DMA_BUF)\n\t\t\tidr_remove(&teedev->idr, shm->id);\n\t\tdo_release = true;\n\t}\n\tmutex_unlock(&teedev->mutex);\n\n\tif (do_release)\n\t\ttee_shm_release(teedev, shm);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,22 @@\n void tee_shm_put(struct tee_shm *shm)\n {\n-\tif (shm->flags & TEE_SHM_DMA_BUF)\n-\t\tdma_buf_put(shm->dmabuf);\n+\tstruct tee_device *teedev = shm->ctx->teedev;\n+\tbool do_release = false;\n+\n+\tmutex_lock(&teedev->mutex);\n+\tif (refcount_dec_and_test(&shm->refcount)) {\n+\t\t/*\n+\t\t * refcount has reached 0, we must now remove it from the\n+\t\t * IDR before releasing the mutex. This will guarantee that\n+\t\t * the refcount_inc() in tee_shm_get_from_id() never starts\n+\t\t * from 0.\n+\t\t */\n+\t\tif (shm->flags & TEE_SHM_DMA_BUF)\n+\t\t\tidr_remove(&teedev->idr, shm->id);\n+\t\tdo_release = true;\n+\t}\n+\tmutex_unlock(&teedev->mutex);\n+\n+\tif (do_release)\n+\t\ttee_shm_release(teedev, shm);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct tee_device *teedev = shm->ctx->teedev;",
                "\tbool do_release = false;",
                "",
                "\tmutex_lock(&teedev->mutex);",
                "\tif (refcount_dec_and_test(&shm->refcount)) {",
                "\t\t/*",
                "\t\t * refcount has reached 0, we must now remove it from the",
                "\t\t * IDR before releasing the mutex. This will guarantee that",
                "\t\t * the refcount_inc() in tee_shm_get_from_id() never starts",
                "\t\t * from 0.",
                "\t\t */",
                "\t\tif (shm->flags & TEE_SHM_DMA_BUF)",
                "\t\t\tidr_remove(&teedev->idr, shm->id);",
                "\t\tdo_release = true;",
                "\t}",
                "\tmutex_unlock(&teedev->mutex);",
                "",
                "\tif (do_release)",
                "\t\ttee_shm_release(teedev, shm);"
            ],
            "deleted": [
                "\tif (shm->flags & TEE_SHM_DMA_BUF)",
                "\t\tdma_buf_put(shm->dmabuf);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object."
    },
    {
        "cve_id": "CVE-2021-44733",
        "code_before_change": "struct tee_shm *tee_shm_alloc(struct tee_context *ctx, size_t size, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tstruct tee_shm_pool_mgr *poolm = NULL;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\n\tif (!(flags & TEE_SHM_MAPPED)) {\n\t\tdev_err(teedev->dev.parent,\n\t\t\t\"only mapped allocations supported\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif ((flags & ~(TEE_SHM_MAPPED | TEE_SHM_DMA_BUF | TEE_SHM_PRIV))) {\n\t\tdev_err(teedev->dev.parent, \"invalid shm flags 0x%x\", flags);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->pool) {\n\t\t/* teedev has been detached from driver */\n\t\tret = ERR_PTR(-EINVAL);\n\t\tgoto err_dev_put;\n\t}\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err_dev_put;\n\t}\n\n\tshm->flags = flags | TEE_SHM_POOL;\n\tshm->ctx = ctx;\n\tif (flags & TEE_SHM_DMA_BUF)\n\t\tpoolm = teedev->pool->dma_buf_mgr;\n\telse\n\t\tpoolm = teedev->pool->private_mgr;\n\n\trc = poolm->ops->alloc(poolm, shm, size);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err_kfree;\n\t}\n\n\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\n\t\tmutex_lock(&teedev->mutex);\n\t\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\t\tmutex_unlock(&teedev->mutex);\n\t\tif (shm->id < 0) {\n\t\t\tret = ERR_PTR(shm->id);\n\t\t\tgoto err_pool_free;\n\t\t}\n\n\t\texp_info.ops = &tee_shm_dma_buf_ops;\n\t\texp_info.size = shm->size;\n\t\texp_info.flags = O_RDWR;\n\t\texp_info.priv = shm;\n\n\t\tshm->dmabuf = dma_buf_export(&exp_info);\n\t\tif (IS_ERR(shm->dmabuf)) {\n\t\t\tret = ERR_CAST(shm->dmabuf);\n\t\t\tgoto err_rem;\n\t\t}\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\treturn shm;\nerr_rem:\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tmutex_lock(&teedev->mutex);\n\t\tidr_remove(&teedev->idr, shm->id);\n\t\tmutex_unlock(&teedev->mutex);\n\t}\nerr_pool_free:\n\tpoolm->ops->free(poolm, shm);\nerr_kfree:\n\tkfree(shm);\nerr_dev_put:\n\ttee_device_put(teedev);\n\treturn ret;\n}",
        "code_after_change": "struct tee_shm *tee_shm_alloc(struct tee_context *ctx, size_t size, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tstruct tee_shm_pool_mgr *poolm = NULL;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\n\tif (!(flags & TEE_SHM_MAPPED)) {\n\t\tdev_err(teedev->dev.parent,\n\t\t\t\"only mapped allocations supported\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif ((flags & ~(TEE_SHM_MAPPED | TEE_SHM_DMA_BUF | TEE_SHM_PRIV))) {\n\t\tdev_err(teedev->dev.parent, \"invalid shm flags 0x%x\", flags);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->pool) {\n\t\t/* teedev has been detached from driver */\n\t\tret = ERR_PTR(-EINVAL);\n\t\tgoto err_dev_put;\n\t}\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err_dev_put;\n\t}\n\n\trefcount_set(&shm->refcount, 1);\n\tshm->flags = flags | TEE_SHM_POOL;\n\tshm->ctx = ctx;\n\tif (flags & TEE_SHM_DMA_BUF)\n\t\tpoolm = teedev->pool->dma_buf_mgr;\n\telse\n\t\tpoolm = teedev->pool->private_mgr;\n\n\trc = poolm->ops->alloc(poolm, shm, size);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err_kfree;\n\t}\n\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tmutex_lock(&teedev->mutex);\n\t\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\t\tmutex_unlock(&teedev->mutex);\n\t\tif (shm->id < 0) {\n\t\t\tret = ERR_PTR(shm->id);\n\t\t\tgoto err_pool_free;\n\t\t}\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\treturn shm;\nerr_pool_free:\n\tpoolm->ops->free(poolm, shm);\nerr_kfree:\n\tkfree(shm);\nerr_dev_put:\n\ttee_device_put(teedev);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -32,6 +32,7 @@\n \t\tgoto err_dev_put;\n \t}\n \n+\trefcount_set(&shm->refcount, 1);\n \tshm->flags = flags | TEE_SHM_POOL;\n \tshm->ctx = ctx;\n \tif (flags & TEE_SHM_DMA_BUF)\n@@ -45,10 +46,7 @@\n \t\tgoto err_kfree;\n \t}\n \n-\n \tif (flags & TEE_SHM_DMA_BUF) {\n-\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n-\n \t\tmutex_lock(&teedev->mutex);\n \t\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n \t\tmutex_unlock(&teedev->mutex);\n@@ -56,28 +54,11 @@\n \t\t\tret = ERR_PTR(shm->id);\n \t\t\tgoto err_pool_free;\n \t\t}\n-\n-\t\texp_info.ops = &tee_shm_dma_buf_ops;\n-\t\texp_info.size = shm->size;\n-\t\texp_info.flags = O_RDWR;\n-\t\texp_info.priv = shm;\n-\n-\t\tshm->dmabuf = dma_buf_export(&exp_info);\n-\t\tif (IS_ERR(shm->dmabuf)) {\n-\t\t\tret = ERR_CAST(shm->dmabuf);\n-\t\t\tgoto err_rem;\n-\t\t}\n \t}\n \n \tteedev_ctx_get(ctx);\n \n \treturn shm;\n-err_rem:\n-\tif (flags & TEE_SHM_DMA_BUF) {\n-\t\tmutex_lock(&teedev->mutex);\n-\t\tidr_remove(&teedev->idr, shm->id);\n-\t\tmutex_unlock(&teedev->mutex);\n-\t}\n err_pool_free:\n \tpoolm->ops->free(poolm, shm);\n err_kfree:",
        "function_modified_lines": {
            "added": [
                "\trefcount_set(&shm->refcount, 1);"
            ],
            "deleted": [
                "",
                "\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);",
                "",
                "",
                "\t\texp_info.ops = &tee_shm_dma_buf_ops;",
                "\t\texp_info.size = shm->size;",
                "\t\texp_info.flags = O_RDWR;",
                "\t\texp_info.priv = shm;",
                "",
                "\t\tshm->dmabuf = dma_buf_export(&exp_info);",
                "\t\tif (IS_ERR(shm->dmabuf)) {",
                "\t\t\tret = ERR_CAST(shm->dmabuf);",
                "\t\t\tgoto err_rem;",
                "\t\t}",
                "err_rem:",
                "\tif (flags & TEE_SHM_DMA_BUF) {",
                "\t\tmutex_lock(&teedev->mutex);",
                "\t\tidr_remove(&teedev->idr, shm->id);",
                "\t\tmutex_unlock(&teedev->mutex);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object."
    },
    {
        "cve_id": "CVE-2021-44733",
        "code_before_change": "int tee_shm_get_fd(struct tee_shm *shm)\n{\n\tint fd;\n\n\tif (!(shm->flags & TEE_SHM_DMA_BUF))\n\t\treturn -EINVAL;\n\n\tget_dma_buf(shm->dmabuf);\n\tfd = dma_buf_fd(shm->dmabuf, O_CLOEXEC);\n\tif (fd < 0)\n\t\tdma_buf_put(shm->dmabuf);\n\treturn fd;\n}",
        "code_after_change": "int tee_shm_get_fd(struct tee_shm *shm)\n{\n\tint fd;\n\n\tif (!(shm->flags & TEE_SHM_DMA_BUF))\n\t\treturn -EINVAL;\n\n\t/* matched by tee_shm_put() in tee_shm_op_release() */\n\trefcount_inc(&shm->refcount);\n\tfd = anon_inode_getfd(\"tee_shm\", &tee_shm_fops, shm, O_RDWR);\n\tif (fd < 0)\n\t\ttee_shm_put(shm);\n\treturn fd;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,9 +5,10 @@\n \tif (!(shm->flags & TEE_SHM_DMA_BUF))\n \t\treturn -EINVAL;\n \n-\tget_dma_buf(shm->dmabuf);\n-\tfd = dma_buf_fd(shm->dmabuf, O_CLOEXEC);\n+\t/* matched by tee_shm_put() in tee_shm_op_release() */\n+\trefcount_inc(&shm->refcount);\n+\tfd = anon_inode_getfd(\"tee_shm\", &tee_shm_fops, shm, O_RDWR);\n \tif (fd < 0)\n-\t\tdma_buf_put(shm->dmabuf);\n+\t\ttee_shm_put(shm);\n \treturn fd;\n }",
        "function_modified_lines": {
            "added": [
                "\t/* matched by tee_shm_put() in tee_shm_op_release() */",
                "\trefcount_inc(&shm->refcount);",
                "\tfd = anon_inode_getfd(\"tee_shm\", &tee_shm_fops, shm, O_RDWR);",
                "\t\ttee_shm_put(shm);"
            ],
            "deleted": [
                "\tget_dma_buf(shm->dmabuf);",
                "\tfd = dma_buf_fd(shm->dmabuf, O_CLOEXEC);",
                "\t\tdma_buf_put(shm->dmabuf);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object."
    },
    {
        "cve_id": "CVE-2022-1048",
        "code_before_change": "int snd_pcm_attach_substream(struct snd_pcm *pcm, int stream,\n\t\t\t     struct file *file,\n\t\t\t     struct snd_pcm_substream **rsubstream)\n{\n\tstruct snd_pcm_str * pstr;\n\tstruct snd_pcm_substream *substream;\n\tstruct snd_pcm_runtime *runtime;\n\tstruct snd_card *card;\n\tint prefer_subdevice;\n\tsize_t size;\n\n\tif (snd_BUG_ON(!pcm || !rsubstream))\n\t\treturn -ENXIO;\n\tif (snd_BUG_ON(stream != SNDRV_PCM_STREAM_PLAYBACK &&\n\t\t       stream != SNDRV_PCM_STREAM_CAPTURE))\n\t\treturn -EINVAL;\n\t*rsubstream = NULL;\n\tpstr = &pcm->streams[stream];\n\tif (pstr->substream == NULL || pstr->substream_count == 0)\n\t\treturn -ENODEV;\n\n\tcard = pcm->card;\n\tprefer_subdevice = snd_ctl_get_preferred_subdevice(card, SND_CTL_SUBDEV_PCM);\n\n\tif (pcm->info_flags & SNDRV_PCM_INFO_HALF_DUPLEX) {\n\t\tint opposite = !stream;\n\n\t\tfor (substream = pcm->streams[opposite].substream; substream;\n\t\t     substream = substream->next) {\n\t\t\tif (SUBSTREAM_BUSY(substream))\n\t\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tif (file->f_flags & O_APPEND) {\n\t\tif (prefer_subdevice < 0) {\n\t\t\tif (pstr->substream_count > 1)\n\t\t\t\treturn -EINVAL; /* must be unique */\n\t\t\tsubstream = pstr->substream;\n\t\t} else {\n\t\t\tfor (substream = pstr->substream; substream;\n\t\t\t     substream = substream->next)\n\t\t\t\tif (substream->number == prefer_subdevice)\n\t\t\t\t\tbreak;\n\t\t}\n\t\tif (! substream)\n\t\t\treturn -ENODEV;\n\t\tif (! SUBSTREAM_BUSY(substream))\n\t\t\treturn -EBADFD;\n\t\tsubstream->ref_count++;\n\t\t*rsubstream = substream;\n\t\treturn 0;\n\t}\n\n\tfor (substream = pstr->substream; substream; substream = substream->next) {\n\t\tif (!SUBSTREAM_BUSY(substream) &&\n\t\t    (prefer_subdevice == -1 ||\n\t\t     substream->number == prefer_subdevice))\n\t\t\tbreak;\n\t}\n\tif (substream == NULL)\n\t\treturn -EAGAIN;\n\n\truntime = kzalloc(sizeof(*runtime), GFP_KERNEL);\n\tif (runtime == NULL)\n\t\treturn -ENOMEM;\n\n\tsize = PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status));\n\truntime->status = alloc_pages_exact(size, GFP_KERNEL);\n\tif (runtime->status == NULL) {\n\t\tkfree(runtime);\n\t\treturn -ENOMEM;\n\t}\n\tmemset(runtime->status, 0, size);\n\n\tsize = PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control));\n\truntime->control = alloc_pages_exact(size, GFP_KERNEL);\n\tif (runtime->control == NULL) {\n\t\tfree_pages_exact(runtime->status,\n\t\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\t\tkfree(runtime);\n\t\treturn -ENOMEM;\n\t}\n\tmemset(runtime->control, 0, size);\n\n\tinit_waitqueue_head(&runtime->sleep);\n\tinit_waitqueue_head(&runtime->tsleep);\n\n\truntime->status->state = SNDRV_PCM_STATE_OPEN;\n\n\tsubstream->runtime = runtime;\n\tsubstream->private_data = pcm->private_data;\n\tsubstream->ref_count = 1;\n\tsubstream->f_flags = file->f_flags;\n\tsubstream->pid = get_pid(task_pid(current));\n\tpstr->substream_opened++;\n\t*rsubstream = substream;\n\treturn 0;\n}",
        "code_after_change": "int snd_pcm_attach_substream(struct snd_pcm *pcm, int stream,\n\t\t\t     struct file *file,\n\t\t\t     struct snd_pcm_substream **rsubstream)\n{\n\tstruct snd_pcm_str * pstr;\n\tstruct snd_pcm_substream *substream;\n\tstruct snd_pcm_runtime *runtime;\n\tstruct snd_card *card;\n\tint prefer_subdevice;\n\tsize_t size;\n\n\tif (snd_BUG_ON(!pcm || !rsubstream))\n\t\treturn -ENXIO;\n\tif (snd_BUG_ON(stream != SNDRV_PCM_STREAM_PLAYBACK &&\n\t\t       stream != SNDRV_PCM_STREAM_CAPTURE))\n\t\treturn -EINVAL;\n\t*rsubstream = NULL;\n\tpstr = &pcm->streams[stream];\n\tif (pstr->substream == NULL || pstr->substream_count == 0)\n\t\treturn -ENODEV;\n\n\tcard = pcm->card;\n\tprefer_subdevice = snd_ctl_get_preferred_subdevice(card, SND_CTL_SUBDEV_PCM);\n\n\tif (pcm->info_flags & SNDRV_PCM_INFO_HALF_DUPLEX) {\n\t\tint opposite = !stream;\n\n\t\tfor (substream = pcm->streams[opposite].substream; substream;\n\t\t     substream = substream->next) {\n\t\t\tif (SUBSTREAM_BUSY(substream))\n\t\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tif (file->f_flags & O_APPEND) {\n\t\tif (prefer_subdevice < 0) {\n\t\t\tif (pstr->substream_count > 1)\n\t\t\t\treturn -EINVAL; /* must be unique */\n\t\t\tsubstream = pstr->substream;\n\t\t} else {\n\t\t\tfor (substream = pstr->substream; substream;\n\t\t\t     substream = substream->next)\n\t\t\t\tif (substream->number == prefer_subdevice)\n\t\t\t\t\tbreak;\n\t\t}\n\t\tif (! substream)\n\t\t\treturn -ENODEV;\n\t\tif (! SUBSTREAM_BUSY(substream))\n\t\t\treturn -EBADFD;\n\t\tsubstream->ref_count++;\n\t\t*rsubstream = substream;\n\t\treturn 0;\n\t}\n\n\tfor (substream = pstr->substream; substream; substream = substream->next) {\n\t\tif (!SUBSTREAM_BUSY(substream) &&\n\t\t    (prefer_subdevice == -1 ||\n\t\t     substream->number == prefer_subdevice))\n\t\t\tbreak;\n\t}\n\tif (substream == NULL)\n\t\treturn -EAGAIN;\n\n\truntime = kzalloc(sizeof(*runtime), GFP_KERNEL);\n\tif (runtime == NULL)\n\t\treturn -ENOMEM;\n\n\tsize = PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status));\n\truntime->status = alloc_pages_exact(size, GFP_KERNEL);\n\tif (runtime->status == NULL) {\n\t\tkfree(runtime);\n\t\treturn -ENOMEM;\n\t}\n\tmemset(runtime->status, 0, size);\n\n\tsize = PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control));\n\truntime->control = alloc_pages_exact(size, GFP_KERNEL);\n\tif (runtime->control == NULL) {\n\t\tfree_pages_exact(runtime->status,\n\t\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\t\tkfree(runtime);\n\t\treturn -ENOMEM;\n\t}\n\tmemset(runtime->control, 0, size);\n\n\tinit_waitqueue_head(&runtime->sleep);\n\tinit_waitqueue_head(&runtime->tsleep);\n\n\truntime->status->state = SNDRV_PCM_STATE_OPEN;\n\tmutex_init(&runtime->buffer_mutex);\n\n\tsubstream->runtime = runtime;\n\tsubstream->private_data = pcm->private_data;\n\tsubstream->ref_count = 1;\n\tsubstream->f_flags = file->f_flags;\n\tsubstream->pid = get_pid(task_pid(current));\n\tpstr->substream_opened++;\n\t*rsubstream = substream;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -87,6 +87,7 @@\n \tinit_waitqueue_head(&runtime->tsleep);\n \n \truntime->status->state = SNDRV_PCM_STATE_OPEN;\n+\tmutex_init(&runtime->buffer_mutex);\n \n \tsubstream->runtime = runtime;\n \tsubstream->private_data = pcm->private_data;",
        "function_modified_lines": {
            "added": [
                "\tmutex_init(&runtime->buffer_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s sound subsystem in the way a user triggers concurrent calls of PCM hw_params. The hw_free ioctls or similar race condition happens inside ALSA PCM for other ioctls. This flaw allows a local user to crash or potentially escalate their privileges on the system."
    },
    {
        "cve_id": "CVE-2022-1048",
        "code_before_change": "void snd_pcm_detach_substream(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\tif (runtime->private_free != NULL)\n\t\truntime->private_free(runtime);\n\tfree_pages_exact(runtime->status,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\tfree_pages_exact(runtime->control,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control)));\n\tkfree(runtime->hw_constraints.rules);\n\t/* Avoid concurrent access to runtime via PCM timer interface */\n\tif (substream->timer) {\n\t\tspin_lock_irq(&substream->timer->lock);\n\t\tsubstream->runtime = NULL;\n\t\tspin_unlock_irq(&substream->timer->lock);\n\t} else {\n\t\tsubstream->runtime = NULL;\n\t}\n\tkfree(runtime);\n\tput_pid(substream->pid);\n\tsubstream->pid = NULL;\n\tsubstream->pstr->substream_opened--;\n}",
        "code_after_change": "void snd_pcm_detach_substream(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\tif (runtime->private_free != NULL)\n\t\truntime->private_free(runtime);\n\tfree_pages_exact(runtime->status,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\tfree_pages_exact(runtime->control,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control)));\n\tkfree(runtime->hw_constraints.rules);\n\t/* Avoid concurrent access to runtime via PCM timer interface */\n\tif (substream->timer) {\n\t\tspin_lock_irq(&substream->timer->lock);\n\t\tsubstream->runtime = NULL;\n\t\tspin_unlock_irq(&substream->timer->lock);\n\t} else {\n\t\tsubstream->runtime = NULL;\n\t}\n\tmutex_destroy(&runtime->buffer_mutex);\n\tkfree(runtime);\n\tput_pid(substream->pid);\n\tsubstream->pid = NULL;\n\tsubstream->pstr->substream_opened--;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,6 +20,7 @@\n \t} else {\n \t\tsubstream->runtime = NULL;\n \t}\n+\tmutex_destroy(&runtime->buffer_mutex);\n \tkfree(runtime);\n \tput_pid(substream->pid);\n \tsubstream->pid = NULL;",
        "function_modified_lines": {
            "added": [
                "\tmutex_destroy(&runtime->buffer_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s sound subsystem in the way a user triggers concurrent calls of PCM hw_params. The hw_free ioctls or similar race condition happens inside ALSA PCM for other ioctls. This flaw allows a local user to crash or potentially escalate their privileges on the system."
    },
    {
        "cve_id": "CVE-2022-1048",
        "code_before_change": "static int snd_pcm_hw_free(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tint result;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn -ENXIO;\n\truntime = substream->runtime;\n\tsnd_pcm_stream_lock_irq(substream);\n\tswitch (runtime->status->state) {\n\tcase SNDRV_PCM_STATE_SETUP:\n\tcase SNDRV_PCM_STATE_PREPARED:\n\t\tbreak;\n\tdefault:\n\t\tsnd_pcm_stream_unlock_irq(substream);\n\t\treturn -EBADFD;\n\t}\n\tsnd_pcm_stream_unlock_irq(substream);\n\tif (atomic_read(&substream->mmap_count))\n\t\treturn -EBADFD;\n\tresult = do_hw_free(substream);\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n\tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n\treturn result;\n}",
        "code_after_change": "static int snd_pcm_hw_free(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tint result = 0;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn -ENXIO;\n\truntime = substream->runtime;\n\tmutex_lock(&runtime->buffer_mutex);\n\tsnd_pcm_stream_lock_irq(substream);\n\tswitch (runtime->status->state) {\n\tcase SNDRV_PCM_STATE_SETUP:\n\tcase SNDRV_PCM_STATE_PREPARED:\n\t\tif (atomic_read(&substream->mmap_count))\n\t\t\tresult = -EBADFD;\n\t\tbreak;\n\tdefault:\n\t\tresult = -EBADFD;\n\t\tbreak;\n\t}\n\tsnd_pcm_stream_unlock_irq(substream);\n\tif (result)\n\t\tgoto unlock;\n\tresult = do_hw_free(substream);\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n\tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n unlock:\n\tmutex_unlock(&runtime->buffer_mutex);\n\treturn result;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,25 +1,30 @@\n static int snd_pcm_hw_free(struct snd_pcm_substream *substream)\n {\n \tstruct snd_pcm_runtime *runtime;\n-\tint result;\n+\tint result = 0;\n \n \tif (PCM_RUNTIME_CHECK(substream))\n \t\treturn -ENXIO;\n \truntime = substream->runtime;\n+\tmutex_lock(&runtime->buffer_mutex);\n \tsnd_pcm_stream_lock_irq(substream);\n \tswitch (runtime->status->state) {\n \tcase SNDRV_PCM_STATE_SETUP:\n \tcase SNDRV_PCM_STATE_PREPARED:\n+\t\tif (atomic_read(&substream->mmap_count))\n+\t\t\tresult = -EBADFD;\n \t\tbreak;\n \tdefault:\n-\t\tsnd_pcm_stream_unlock_irq(substream);\n-\t\treturn -EBADFD;\n+\t\tresult = -EBADFD;\n+\t\tbreak;\n \t}\n \tsnd_pcm_stream_unlock_irq(substream);\n-\tif (atomic_read(&substream->mmap_count))\n-\t\treturn -EBADFD;\n+\tif (result)\n+\t\tgoto unlock;\n \tresult = do_hw_free(substream);\n \tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n \tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n+ unlock:\n+\tmutex_unlock(&runtime->buffer_mutex);\n \treturn result;\n }",
        "function_modified_lines": {
            "added": [
                "\tint result = 0;",
                "\tmutex_lock(&runtime->buffer_mutex);",
                "\t\tif (atomic_read(&substream->mmap_count))",
                "\t\t\tresult = -EBADFD;",
                "\t\tresult = -EBADFD;",
                "\t\tbreak;",
                "\tif (result)",
                "\t\tgoto unlock;",
                " unlock:",
                "\tmutex_unlock(&runtime->buffer_mutex);"
            ],
            "deleted": [
                "\tint result;",
                "\t\tsnd_pcm_stream_unlock_irq(substream);",
                "\t\treturn -EBADFD;",
                "\tif (atomic_read(&substream->mmap_count))",
                "\t\treturn -EBADFD;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s sound subsystem in the way a user triggers concurrent calls of PCM hw_params. The hw_free ioctls or similar race condition happens inside ALSA PCM for other ioctls. This flaw allows a local user to crash or potentially escalate their privileges on the system."
    },
    {
        "cve_id": "CVE-2022-1048",
        "code_before_change": "static int snd_pcm_hw_params(struct snd_pcm_substream *substream,\n\t\t\t     struct snd_pcm_hw_params *params)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tint err, usecs;\n\tunsigned int bits;\n\tsnd_pcm_uframes_t frames;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn -ENXIO;\n\truntime = substream->runtime;\n\tsnd_pcm_stream_lock_irq(substream);\n\tswitch (runtime->status->state) {\n\tcase SNDRV_PCM_STATE_OPEN:\n\tcase SNDRV_PCM_STATE_SETUP:\n\tcase SNDRV_PCM_STATE_PREPARED:\n\t\tbreak;\n\tdefault:\n\t\tsnd_pcm_stream_unlock_irq(substream);\n\t\treturn -EBADFD;\n\t}\n\tsnd_pcm_stream_unlock_irq(substream);\n#if IS_ENABLED(CONFIG_SND_PCM_OSS)\n\tif (!substream->oss.oss)\n#endif\n\t\tif (atomic_read(&substream->mmap_count))\n\t\t\treturn -EBADFD;\n\n\tsnd_pcm_sync_stop(substream, true);\n\n\tparams->rmask = ~0U;\n\terr = snd_pcm_hw_refine(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = snd_pcm_hw_params_choose(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = fixup_unreferenced_params(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\tif (substream->managed_buffer_alloc) {\n\t\terr = snd_pcm_lib_malloc_pages(substream,\n\t\t\t\t\t       params_buffer_bytes(params));\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t\truntime->buffer_changed = err > 0;\n\t}\n\n\tif (substream->ops->hw_params != NULL) {\n\t\terr = substream->ops->hw_params(substream, params);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t}\n\n\truntime->access = params_access(params);\n\truntime->format = params_format(params);\n\truntime->subformat = params_subformat(params);\n\truntime->channels = params_channels(params);\n\truntime->rate = params_rate(params);\n\truntime->period_size = params_period_size(params);\n\truntime->periods = params_periods(params);\n\truntime->buffer_size = params_buffer_size(params);\n\truntime->info = params->info;\n\truntime->rate_num = params->rate_num;\n\truntime->rate_den = params->rate_den;\n\truntime->no_period_wakeup =\n\t\t\t(params->info & SNDRV_PCM_INFO_NO_PERIOD_WAKEUP) &&\n\t\t\t(params->flags & SNDRV_PCM_HW_PARAMS_NO_PERIOD_WAKEUP);\n\n\tbits = snd_pcm_format_physical_width(runtime->format);\n\truntime->sample_bits = bits;\n\tbits *= runtime->channels;\n\truntime->frame_bits = bits;\n\tframes = 1;\n\twhile (bits % 8 != 0) {\n\t\tbits *= 2;\n\t\tframes *= 2;\n\t}\n\truntime->byte_align = bits / 8;\n\truntime->min_align = frames;\n\n\t/* Default sw params */\n\truntime->tstamp_mode = SNDRV_PCM_TSTAMP_NONE;\n\truntime->period_step = 1;\n\truntime->control->avail_min = runtime->period_size;\n\truntime->start_threshold = 1;\n\truntime->stop_threshold = runtime->buffer_size;\n\truntime->silence_threshold = 0;\n\truntime->silence_size = 0;\n\truntime->boundary = runtime->buffer_size;\n\twhile (runtime->boundary * 2 <= LONG_MAX - runtime->buffer_size)\n\t\truntime->boundary *= 2;\n\n\t/* clear the buffer for avoiding possible kernel info leaks */\n\tif (runtime->dma_area && !substream->ops->copy_user) {\n\t\tsize_t size = runtime->dma_bytes;\n\n\t\tif (runtime->info & SNDRV_PCM_INFO_MMAP)\n\t\t\tsize = PAGE_ALIGN(size);\n\t\tmemset(runtime->dma_area, 0, size);\n\t}\n\n\tsnd_pcm_timer_resolution_change(substream);\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_SETUP);\n\n\tif (cpu_latency_qos_request_active(&substream->latency_pm_qos_req))\n\t\tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n\tusecs = period_to_usecs(runtime);\n\tif (usecs >= 0)\n\t\tcpu_latency_qos_add_request(&substream->latency_pm_qos_req,\n\t\t\t\t\t    usecs);\n\treturn 0;\n _error:\n\t/* hardware might be unusable from this time,\n\t   so we force application to retry to set\n\t   the correct hardware parameter settings */\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n\tif (substream->ops->hw_free != NULL)\n\t\tsubstream->ops->hw_free(substream);\n\tif (substream->managed_buffer_alloc)\n\t\tsnd_pcm_lib_free_pages(substream);\n\treturn err;\n}",
        "code_after_change": "static int snd_pcm_hw_params(struct snd_pcm_substream *substream,\n\t\t\t     struct snd_pcm_hw_params *params)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tint err = 0, usecs;\n\tunsigned int bits;\n\tsnd_pcm_uframes_t frames;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn -ENXIO;\n\truntime = substream->runtime;\n\tmutex_lock(&runtime->buffer_mutex);\n\tsnd_pcm_stream_lock_irq(substream);\n\tswitch (runtime->status->state) {\n\tcase SNDRV_PCM_STATE_OPEN:\n\tcase SNDRV_PCM_STATE_SETUP:\n\tcase SNDRV_PCM_STATE_PREPARED:\n\t\tif (!is_oss_stream(substream) &&\n\t\t    atomic_read(&substream->mmap_count))\n\t\t\terr = -EBADFD;\n\t\tbreak;\n\tdefault:\n\t\terr = -EBADFD;\n\t\tbreak;\n\t}\n\tsnd_pcm_stream_unlock_irq(substream);\n\tif (err)\n\t\tgoto unlock;\n\n\tsnd_pcm_sync_stop(substream, true);\n\n\tparams->rmask = ~0U;\n\terr = snd_pcm_hw_refine(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = snd_pcm_hw_params_choose(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = fixup_unreferenced_params(substream, params);\n\tif (err < 0)\n\t\tgoto _error;\n\n\tif (substream->managed_buffer_alloc) {\n\t\terr = snd_pcm_lib_malloc_pages(substream,\n\t\t\t\t\t       params_buffer_bytes(params));\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t\truntime->buffer_changed = err > 0;\n\t}\n\n\tif (substream->ops->hw_params != NULL) {\n\t\terr = substream->ops->hw_params(substream, params);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t}\n\n\truntime->access = params_access(params);\n\truntime->format = params_format(params);\n\truntime->subformat = params_subformat(params);\n\truntime->channels = params_channels(params);\n\truntime->rate = params_rate(params);\n\truntime->period_size = params_period_size(params);\n\truntime->periods = params_periods(params);\n\truntime->buffer_size = params_buffer_size(params);\n\truntime->info = params->info;\n\truntime->rate_num = params->rate_num;\n\truntime->rate_den = params->rate_den;\n\truntime->no_period_wakeup =\n\t\t\t(params->info & SNDRV_PCM_INFO_NO_PERIOD_WAKEUP) &&\n\t\t\t(params->flags & SNDRV_PCM_HW_PARAMS_NO_PERIOD_WAKEUP);\n\n\tbits = snd_pcm_format_physical_width(runtime->format);\n\truntime->sample_bits = bits;\n\tbits *= runtime->channels;\n\truntime->frame_bits = bits;\n\tframes = 1;\n\twhile (bits % 8 != 0) {\n\t\tbits *= 2;\n\t\tframes *= 2;\n\t}\n\truntime->byte_align = bits / 8;\n\truntime->min_align = frames;\n\n\t/* Default sw params */\n\truntime->tstamp_mode = SNDRV_PCM_TSTAMP_NONE;\n\truntime->period_step = 1;\n\truntime->control->avail_min = runtime->period_size;\n\truntime->start_threshold = 1;\n\truntime->stop_threshold = runtime->buffer_size;\n\truntime->silence_threshold = 0;\n\truntime->silence_size = 0;\n\truntime->boundary = runtime->buffer_size;\n\twhile (runtime->boundary * 2 <= LONG_MAX - runtime->buffer_size)\n\t\truntime->boundary *= 2;\n\n\t/* clear the buffer for avoiding possible kernel info leaks */\n\tif (runtime->dma_area && !substream->ops->copy_user) {\n\t\tsize_t size = runtime->dma_bytes;\n\n\t\tif (runtime->info & SNDRV_PCM_INFO_MMAP)\n\t\t\tsize = PAGE_ALIGN(size);\n\t\tmemset(runtime->dma_area, 0, size);\n\t}\n\n\tsnd_pcm_timer_resolution_change(substream);\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_SETUP);\n\n\tif (cpu_latency_qos_request_active(&substream->latency_pm_qos_req))\n\t\tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n\tusecs = period_to_usecs(runtime);\n\tif (usecs >= 0)\n\t\tcpu_latency_qos_add_request(&substream->latency_pm_qos_req,\n\t\t\t\t\t    usecs);\n\terr = 0;\n _error:\n\tif (err) {\n\t\t/* hardware might be unusable from this time,\n\t\t * so we force application to retry to set\n\t\t * the correct hardware parameter settings\n\t\t */\n\t\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n\t\tif (substream->ops->hw_free != NULL)\n\t\t\tsubstream->ops->hw_free(substream);\n\t\tif (substream->managed_buffer_alloc)\n\t\t\tsnd_pcm_lib_free_pages(substream);\n\t}\n unlock:\n\tmutex_unlock(&runtime->buffer_mutex);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,29 +2,30 @@\n \t\t\t     struct snd_pcm_hw_params *params)\n {\n \tstruct snd_pcm_runtime *runtime;\n-\tint err, usecs;\n+\tint err = 0, usecs;\n \tunsigned int bits;\n \tsnd_pcm_uframes_t frames;\n \n \tif (PCM_RUNTIME_CHECK(substream))\n \t\treturn -ENXIO;\n \truntime = substream->runtime;\n+\tmutex_lock(&runtime->buffer_mutex);\n \tsnd_pcm_stream_lock_irq(substream);\n \tswitch (runtime->status->state) {\n \tcase SNDRV_PCM_STATE_OPEN:\n \tcase SNDRV_PCM_STATE_SETUP:\n \tcase SNDRV_PCM_STATE_PREPARED:\n+\t\tif (!is_oss_stream(substream) &&\n+\t\t    atomic_read(&substream->mmap_count))\n+\t\t\terr = -EBADFD;\n \t\tbreak;\n \tdefault:\n-\t\tsnd_pcm_stream_unlock_irq(substream);\n-\t\treturn -EBADFD;\n+\t\terr = -EBADFD;\n+\t\tbreak;\n \t}\n \tsnd_pcm_stream_unlock_irq(substream);\n-#if IS_ENABLED(CONFIG_SND_PCM_OSS)\n-\tif (!substream->oss.oss)\n-#endif\n-\t\tif (atomic_read(&substream->mmap_count))\n-\t\t\treturn -EBADFD;\n+\tif (err)\n+\t\tgoto unlock;\n \n \tsnd_pcm_sync_stop(substream, true);\n \n@@ -112,15 +113,20 @@\n \tif (usecs >= 0)\n \t\tcpu_latency_qos_add_request(&substream->latency_pm_qos_req,\n \t\t\t\t\t    usecs);\n-\treturn 0;\n+\terr = 0;\n  _error:\n-\t/* hardware might be unusable from this time,\n-\t   so we force application to retry to set\n-\t   the correct hardware parameter settings */\n-\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n-\tif (substream->ops->hw_free != NULL)\n-\t\tsubstream->ops->hw_free(substream);\n-\tif (substream->managed_buffer_alloc)\n-\t\tsnd_pcm_lib_free_pages(substream);\n+\tif (err) {\n+\t\t/* hardware might be unusable from this time,\n+\t\t * so we force application to retry to set\n+\t\t * the correct hardware parameter settings\n+\t\t */\n+\t\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n+\t\tif (substream->ops->hw_free != NULL)\n+\t\t\tsubstream->ops->hw_free(substream);\n+\t\tif (substream->managed_buffer_alloc)\n+\t\t\tsnd_pcm_lib_free_pages(substream);\n+\t}\n+ unlock:\n+\tmutex_unlock(&runtime->buffer_mutex);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tint err = 0, usecs;",
                "\tmutex_lock(&runtime->buffer_mutex);",
                "\t\tif (!is_oss_stream(substream) &&",
                "\t\t    atomic_read(&substream->mmap_count))",
                "\t\t\terr = -EBADFD;",
                "\t\terr = -EBADFD;",
                "\t\tbreak;",
                "\tif (err)",
                "\t\tgoto unlock;",
                "\terr = 0;",
                "\tif (err) {",
                "\t\t/* hardware might be unusable from this time,",
                "\t\t * so we force application to retry to set",
                "\t\t * the correct hardware parameter settings",
                "\t\t */",
                "\t\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);",
                "\t\tif (substream->ops->hw_free != NULL)",
                "\t\t\tsubstream->ops->hw_free(substream);",
                "\t\tif (substream->managed_buffer_alloc)",
                "\t\t\tsnd_pcm_lib_free_pages(substream);",
                "\t}",
                " unlock:",
                "\tmutex_unlock(&runtime->buffer_mutex);"
            ],
            "deleted": [
                "\tint err, usecs;",
                "\t\tsnd_pcm_stream_unlock_irq(substream);",
                "\t\treturn -EBADFD;",
                "#if IS_ENABLED(CONFIG_SND_PCM_OSS)",
                "\tif (!substream->oss.oss)",
                "#endif",
                "\t\tif (atomic_read(&substream->mmap_count))",
                "\t\t\treturn -EBADFD;",
                "\treturn 0;",
                "\t/* hardware might be unusable from this time,",
                "\t   so we force application to retry to set",
                "\t   the correct hardware parameter settings */",
                "\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);",
                "\tif (substream->ops->hw_free != NULL)",
                "\t\tsubstream->ops->hw_free(substream);",
                "\tif (substream->managed_buffer_alloc)",
                "\t\tsnd_pcm_lib_free_pages(substream);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel\u2019s sound subsystem in the way a user triggers concurrent calls of PCM hw_params. The hw_free ioctls or similar race condition happens inside ALSA PCM for other ioctls. This flaw allows a local user to crash or potentially escalate their privileges on the system."
    },
    {
        "cve_id": "CVE-2022-1462",
        "code_before_change": "static int pty_write(struct tty_struct *tty, const unsigned char *buf, int c)\n{\n\tstruct tty_struct *to = tty->link;\n\tunsigned long flags;\n\n\tif (tty->flow.stopped)\n\t\treturn 0;\n\n\tif (c > 0) {\n\t\tspin_lock_irqsave(&to->port->lock, flags);\n\t\t/* Stuff the data into the input queue of the other end */\n\t\tc = tty_insert_flip_string(to->port, buf, c);\n\t\tspin_unlock_irqrestore(&to->port->lock, flags);\n\t\t/* And shovel */\n\t\tif (c)\n\t\t\ttty_flip_buffer_push(to->port);\n\t}\n\treturn c;\n}",
        "code_after_change": "static int pty_write(struct tty_struct *tty, const unsigned char *buf, int c)\n{\n\tstruct tty_struct *to = tty->link;\n\n\tif (tty->flow.stopped || !c)\n\t\treturn 0;\n\n\treturn tty_insert_flip_string_and_push_buffer(to->port, buf, c);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,19 +1,9 @@\n static int pty_write(struct tty_struct *tty, const unsigned char *buf, int c)\n {\n \tstruct tty_struct *to = tty->link;\n-\tunsigned long flags;\n \n-\tif (tty->flow.stopped)\n+\tif (tty->flow.stopped || !c)\n \t\treturn 0;\n \n-\tif (c > 0) {\n-\t\tspin_lock_irqsave(&to->port->lock, flags);\n-\t\t/* Stuff the data into the input queue of the other end */\n-\t\tc = tty_insert_flip_string(to->port, buf, c);\n-\t\tspin_unlock_irqrestore(&to->port->lock, flags);\n-\t\t/* And shovel */\n-\t\tif (c)\n-\t\t\ttty_flip_buffer_push(to->port);\n-\t}\n-\treturn c;\n+\treturn tty_insert_flip_string_and_push_buffer(to->port, buf, c);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (tty->flow.stopped || !c)",
                "\treturn tty_insert_flip_string_and_push_buffer(to->port, buf, c);"
            ],
            "deleted": [
                "\tunsigned long flags;",
                "\tif (tty->flow.stopped)",
                "\tif (c > 0) {",
                "\t\tspin_lock_irqsave(&to->port->lock, flags);",
                "\t\t/* Stuff the data into the input queue of the other end */",
                "\t\tc = tty_insert_flip_string(to->port, buf, c);",
                "\t\tspin_unlock_irqrestore(&to->port->lock, flags);",
                "\t\t/* And shovel */",
                "\t\tif (c)",
                "\t\t\ttty_flip_buffer_push(to->port);",
                "\t}",
                "\treturn c;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An out-of-bounds read flaw was found in the Linux kernel\u2019s TeleTYpe subsystem. The issue occurs in how a user triggers a race condition using ioctls TIOCSPTLCK and TIOCGPTPEER and TIOCSTI and TCXONC with leakage of memory in the flush_to_ldisc function. This flaw allows a local user to crash the system or read unauthorized random data from memory."
    },
    {
        "cve_id": "CVE-2022-1729",
        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *gctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\tint cgroup_fd = -1;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\t/* Do we allow access to perf_event_open(2) ? */\n\terr = security_perf_event_open(&attr, PERF_SECURITY_OPEN);\n\tif (err)\n\t\treturn err;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\terr = perf_allow_kernel(&attr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (attr.namespaces) {\n\t\tif (!perfmon_capable())\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Only privileged users can get physical addresses */\n\tif ((attr.sample_type & PERF_SAMPLE_PHYS_ADDR)) {\n\t\terr = perf_allow_kernel(&attr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* REGS_INTR can leak data, lockdown must prevent this */\n\tif (attr.sample_type & PERF_SAMPLE_REGS_INTR) {\n\t\terr = security_locked_down(LOCKDOWN_PERF);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP)\n\t\tcgroup_fd = pid;\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL, cgroup_fd);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_task;\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (attr.use_clockid) {\n\t\terr = perf_event_set_clock(event, attr.clockid);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t}\n\n\tif (pmu->task_ctx_nr == perf_sw_context)\n\t\tevent->event_caps |= PERF_EV_CAP_SOFTWARE;\n\n\tif (group_leader) {\n\t\tif (is_software_event(event) &&\n\t\t    !in_software_context(group_leader)) {\n\t\t\t/*\n\t\t\t * If the event is a sw event, but the group_leader\n\t\t\t * is on hw context.\n\t\t\t *\n\t\t\t * Allow the addition of software events to hw\n\t\t\t * groups, this is safe because software events\n\t\t\t * never fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->ctx->pmu;\n\t\t} else if (!is_software_event(event) &&\n\t\t\t   is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\n\t\t/* All events in a group should have the same clock */\n\t\tif (group_leader->clock != event->clock)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Make sure we're both events for the same CPU;\n\t\t * grouping events for different CPUs is broken; since\n\t\t * you can never concurrently schedule them anyhow.\n\t\t */\n\t\tif (group_leader->cpu != event->cpu)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Make sure we're both on the same task, or both\n\t\t * per-CPU events.\n\t\t */\n\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Do not allow to attach to a group in a different task\n\t\t * or CPU context. If we're moving SW events, we'll fix\n\t\t * this up later, so allow that.\n\t\t */\n\t\tif (!move_group && group_leader->ctx != ctx)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tevent_file = NULL;\n\t\tgoto err_context;\n\t}\n\n\tif (task) {\n\t\terr = down_read_interruptible(&task->signal->exec_update_lock);\n\t\tif (err)\n\t\t\tgoto err_file;\n\n\t\t/*\n\t\t * We must hold exec_update_lock across this and any potential\n\t\t * perf_install_in_context() call for this new event to\n\t\t * serialize against exec() altering our credentials (and the\n\t\t * perf_event_exit_task() that could imply).\n\t\t */\n\t\terr = -EACCES;\n\t\tif (!perf_check_permission(&attr, task))\n\t\t\tgoto err_cred;\n\t}\n\n\tif (move_group) {\n\t\tgctx = __perf_event_ctx_lock_double(group_leader, ctx);\n\n\t\tif (gctx->task == TASK_TOMBSTONE) {\n\t\t\terr = -ESRCH;\n\t\t\tgoto err_locked;\n\t\t}\n\n\t\t/*\n\t\t * Check if we raced against another sys_perf_event_open() call\n\t\t * moving the software group underneath us.\n\t\t */\n\t\tif (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * If someone moved the group out from under us, check\n\t\t\t * if this new event wound up on the same ctx, if so\n\t\t\t * its the regular !move_group case, otherwise fail.\n\t\t\t */\n\t\t\tif (gctx != ctx) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_locked;\n\t\t\t} else {\n\t\t\t\tperf_event_ctx_unlock(group_leader, gctx);\n\t\t\t\tmove_group = 0;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Failure to create exclusive events returns -EBUSY.\n\t\t */\n\t\terr = -EBUSY;\n\t\tif (!exclusive_event_installable(group_leader, ctx))\n\t\t\tgoto err_locked;\n\n\t\tfor_each_sibling_event(sibling, group_leader) {\n\t\t\tif (!exclusive_event_installable(sibling, ctx))\n\t\t\t\tgoto err_locked;\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tif (ctx->task == TASK_TOMBSTONE) {\n\t\terr = -ESRCH;\n\t\tgoto err_locked;\n\t}\n\n\tif (!perf_event_validate_size(event)) {\n\t\terr = -E2BIG;\n\t\tgoto err_locked;\n\t}\n\n\tif (!task) {\n\t\t/*\n\t\t * Check if the @cpu we're creating an event for is online.\n\t\t *\n\t\t * We use the perf_cpu_context::ctx::mutex to serialize against\n\t\t * the hotplug notifiers. See perf_event_{init,exit}_cpu().\n\t\t */\n\t\tstruct perf_cpu_context *cpuctx =\n\t\t\tcontainer_of(ctx, struct perf_cpu_context, ctx);\n\n\t\tif (!cpuctx->online) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err_locked;\n\t\t}\n\t}\n\n\tif (perf_need_aux_event(event) && !perf_get_aux_event(event, group_leader)) {\n\t\terr = -EINVAL;\n\t\tgoto err_locked;\n\t}\n\n\t/*\n\t * Must be under the same ctx::mutex as perf_install_in_context(),\n\t * because we need to serialize with concurrent event creation.\n\t */\n\tif (!exclusive_event_installable(event, ctx)) {\n\t\terr = -EBUSY;\n\t\tgoto err_locked;\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\t/*\n\t * This is the point on no return; we cannot fail hereafter. This is\n\t * where we start modifying current state.\n\t */\n\n\tif (move_group) {\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tperf_remove_from_context(group_leader, 0);\n\t\tput_ctx(gctx);\n\n\t\tfor_each_sibling_event(sibling, group_leader) {\n\t\t\tperf_remove_from_context(sibling, 0);\n\t\t\tput_ctx(gctx);\n\t\t}\n\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\t/*\n\t\t * Install the group siblings before the group leader.\n\t\t *\n\t\t * Because a group leader will try and install the entire group\n\t\t * (through the sibling list, which is still in-tact), we can\n\t\t * end up with siblings installed in the wrong context.\n\t\t *\n\t\t * By installing siblings first we NO-OP because they're not\n\t\t * reachable through the group lists.\n\t\t */\n\t\tfor_each_sibling_event(sibling, group_leader) {\n\t\t\tperf_event__state_init(sibling);\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t}\n\n\t/*\n\t * Precalculate sample_data sizes; do while holding ctx::mutex such\n\t * that we're serialized against further additions and before\n\t * perf_install_in_context() which is the point the event is active and\n\t * can use these values.\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\tevent->owner = current;\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group)\n\t\tperf_event_ctx_unlock(group_leader, gctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tif (task) {\n\t\tup_read(&task->signal->exec_update_lock);\n\t\tput_task_struct(task);\n\t}\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_locked:\n\tif (move_group)\n\t\tperf_event_ctx_unlock(group_leader, gctx);\n\tmutex_unlock(&ctx->mutex);\nerr_cred:\n\tif (task)\n\t\tup_read(&task->signal->exec_update_lock);\nerr_file:\n\tfput(event_file);\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\t/*\n\t * If event_file is set, the fput() above will have called ->release()\n\t * and that will take care of freeing the event.\n\t */\n\tif (!event_file)\n\t\tfree_event(event);\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *gctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\tint cgroup_fd = -1;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\t/* Do we allow access to perf_event_open(2) ? */\n\terr = security_perf_event_open(&attr, PERF_SECURITY_OPEN);\n\tif (err)\n\t\treturn err;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\terr = perf_allow_kernel(&attr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (attr.namespaces) {\n\t\tif (!perfmon_capable())\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Only privileged users can get physical addresses */\n\tif ((attr.sample_type & PERF_SAMPLE_PHYS_ADDR)) {\n\t\terr = perf_allow_kernel(&attr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* REGS_INTR can leak data, lockdown must prevent this */\n\tif (attr.sample_type & PERF_SAMPLE_REGS_INTR) {\n\t\terr = security_locked_down(LOCKDOWN_PERF);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP)\n\t\tcgroup_fd = pid;\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL, cgroup_fd);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_task;\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (attr.use_clockid) {\n\t\terr = perf_event_set_clock(event, attr.clockid);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t}\n\n\tif (pmu->task_ctx_nr == perf_sw_context)\n\t\tevent->event_caps |= PERF_EV_CAP_SOFTWARE;\n\n\tif (group_leader) {\n\t\tif (is_software_event(event) &&\n\t\t    !in_software_context(group_leader)) {\n\t\t\t/*\n\t\t\t * If the event is a sw event, but the group_leader\n\t\t\t * is on hw context.\n\t\t\t *\n\t\t\t * Allow the addition of software events to hw\n\t\t\t * groups, this is safe because software events\n\t\t\t * never fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->ctx->pmu;\n\t\t} else if (!is_software_event(event) &&\n\t\t\t   is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\n\t\t/* All events in a group should have the same clock */\n\t\tif (group_leader->clock != event->clock)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Make sure we're both events for the same CPU;\n\t\t * grouping events for different CPUs is broken; since\n\t\t * you can never concurrently schedule them anyhow.\n\t\t */\n\t\tif (group_leader->cpu != event->cpu)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Make sure we're both on the same task, or both\n\t\t * per-CPU events.\n\t\t */\n\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Do not allow to attach to a group in a different task\n\t\t * or CPU context. If we're moving SW events, we'll fix\n\t\t * this up later, so allow that.\n\t\t *\n\t\t * Racy, not holding group_leader->ctx->mutex, see comment with\n\t\t * perf_event_ctx_lock().\n\t\t */\n\t\tif (!move_group && group_leader->ctx != ctx)\n\t\t\tgoto err_context;\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tevent_file = NULL;\n\t\tgoto err_context;\n\t}\n\n\tif (task) {\n\t\terr = down_read_interruptible(&task->signal->exec_update_lock);\n\t\tif (err)\n\t\t\tgoto err_file;\n\n\t\t/*\n\t\t * We must hold exec_update_lock across this and any potential\n\t\t * perf_install_in_context() call for this new event to\n\t\t * serialize against exec() altering our credentials (and the\n\t\t * perf_event_exit_task() that could imply).\n\t\t */\n\t\terr = -EACCES;\n\t\tif (!perf_check_permission(&attr, task))\n\t\t\tgoto err_cred;\n\t}\n\n\tif (move_group) {\n\t\tgctx = __perf_event_ctx_lock_double(group_leader, ctx);\n\n\t\tif (gctx->task == TASK_TOMBSTONE) {\n\t\t\terr = -ESRCH;\n\t\t\tgoto err_locked;\n\t\t}\n\n\t\t/*\n\t\t * Check if we raced against another sys_perf_event_open() call\n\t\t * moving the software group underneath us.\n\t\t */\n\t\tif (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * If someone moved the group out from under us, check\n\t\t\t * if this new event wound up on the same ctx, if so\n\t\t\t * its the regular !move_group case, otherwise fail.\n\t\t\t */\n\t\t\tif (gctx != ctx) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_locked;\n\t\t\t} else {\n\t\t\t\tperf_event_ctx_unlock(group_leader, gctx);\n\t\t\t\tmove_group = 0;\n\t\t\t\tgoto not_move_group;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Failure to create exclusive events returns -EBUSY.\n\t\t */\n\t\terr = -EBUSY;\n\t\tif (!exclusive_event_installable(group_leader, ctx))\n\t\t\tgoto err_locked;\n\n\t\tfor_each_sibling_event(sibling, group_leader) {\n\t\t\tif (!exclusive_event_installable(sibling, ctx))\n\t\t\t\tgoto err_locked;\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\n\t\t/*\n\t\t * Now that we hold ctx->lock, (re)validate group_leader->ctx == ctx,\n\t\t * see the group_leader && !move_group test earlier.\n\t\t */\n\t\tif (group_leader && group_leader->ctx != ctx) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_locked;\n\t\t}\n\t}\nnot_move_group:\n\n\tif (ctx->task == TASK_TOMBSTONE) {\n\t\terr = -ESRCH;\n\t\tgoto err_locked;\n\t}\n\n\tif (!perf_event_validate_size(event)) {\n\t\terr = -E2BIG;\n\t\tgoto err_locked;\n\t}\n\n\tif (!task) {\n\t\t/*\n\t\t * Check if the @cpu we're creating an event for is online.\n\t\t *\n\t\t * We use the perf_cpu_context::ctx::mutex to serialize against\n\t\t * the hotplug notifiers. See perf_event_{init,exit}_cpu().\n\t\t */\n\t\tstruct perf_cpu_context *cpuctx =\n\t\t\tcontainer_of(ctx, struct perf_cpu_context, ctx);\n\n\t\tif (!cpuctx->online) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err_locked;\n\t\t}\n\t}\n\n\tif (perf_need_aux_event(event) && !perf_get_aux_event(event, group_leader)) {\n\t\terr = -EINVAL;\n\t\tgoto err_locked;\n\t}\n\n\t/*\n\t * Must be under the same ctx::mutex as perf_install_in_context(),\n\t * because we need to serialize with concurrent event creation.\n\t */\n\tif (!exclusive_event_installable(event, ctx)) {\n\t\terr = -EBUSY;\n\t\tgoto err_locked;\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\t/*\n\t * This is the point on no return; we cannot fail hereafter. This is\n\t * where we start modifying current state.\n\t */\n\n\tif (move_group) {\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tperf_remove_from_context(group_leader, 0);\n\t\tput_ctx(gctx);\n\n\t\tfor_each_sibling_event(sibling, group_leader) {\n\t\t\tperf_remove_from_context(sibling, 0);\n\t\t\tput_ctx(gctx);\n\t\t}\n\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\t/*\n\t\t * Install the group siblings before the group leader.\n\t\t *\n\t\t * Because a group leader will try and install the entire group\n\t\t * (through the sibling list, which is still in-tact), we can\n\t\t * end up with siblings installed in the wrong context.\n\t\t *\n\t\t * By installing siblings first we NO-OP because they're not\n\t\t * reachable through the group lists.\n\t\t */\n\t\tfor_each_sibling_event(sibling, group_leader) {\n\t\t\tperf_event__state_init(sibling);\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t}\n\n\t/*\n\t * Precalculate sample_data sizes; do while holding ctx::mutex such\n\t * that we're serialized against further additions and before\n\t * perf_install_in_context() which is the point the event is active and\n\t * can use these values.\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\tevent->owner = current;\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group)\n\t\tperf_event_ctx_unlock(group_leader, gctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tif (task) {\n\t\tup_read(&task->signal->exec_update_lock);\n\t\tput_task_struct(task);\n\t}\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_locked:\n\tif (move_group)\n\t\tperf_event_ctx_unlock(group_leader, gctx);\n\tmutex_unlock(&ctx->mutex);\nerr_cred:\n\tif (task)\n\t\tup_read(&task->signal->exec_update_lock);\nerr_file:\n\tfput(event_file);\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\t/*\n\t * If event_file is set, the fput() above will have called ->release()\n\t * and that will take care of freeing the event.\n\t */\n\tif (!event_file)\n\t\tfree_event(event);\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -205,6 +205,9 @@\n \t\t * Do not allow to attach to a group in a different task\n \t\t * or CPU context. If we're moving SW events, we'll fix\n \t\t * this up later, so allow that.\n+\t\t *\n+\t\t * Racy, not holding group_leader->ctx->mutex, see comment with\n+\t\t * perf_event_ctx_lock().\n \t\t */\n \t\tif (!move_group && group_leader->ctx != ctx)\n \t\t\tgoto err_context;\n@@ -270,6 +273,7 @@\n \t\t\t} else {\n \t\t\t\tperf_event_ctx_unlock(group_leader, gctx);\n \t\t\t\tmove_group = 0;\n+\t\t\t\tgoto not_move_group;\n \t\t\t}\n \t\t}\n \n@@ -286,7 +290,17 @@\n \t\t}\n \t} else {\n \t\tmutex_lock(&ctx->mutex);\n-\t}\n+\n+\t\t/*\n+\t\t * Now that we hold ctx->lock, (re)validate group_leader->ctx == ctx,\n+\t\t * see the group_leader && !move_group test earlier.\n+\t\t */\n+\t\tif (group_leader && group_leader->ctx != ctx) {\n+\t\t\terr = -EINVAL;\n+\t\t\tgoto err_locked;\n+\t\t}\n+\t}\n+not_move_group:\n \n \tif (ctx->task == TASK_TOMBSTONE) {\n \t\terr = -ESRCH;",
        "function_modified_lines": {
            "added": [
                "\t\t *",
                "\t\t * Racy, not holding group_leader->ctx->mutex, see comment with",
                "\t\t * perf_event_ctx_lock().",
                "\t\t\t\tgoto not_move_group;",
                "",
                "\t\t/*",
                "\t\t * Now that we hold ctx->lock, (re)validate group_leader->ctx == ctx,",
                "\t\t * see the group_leader && !move_group test earlier.",
                "\t\t */",
                "\t\tif (group_leader && group_leader->ctx != ctx) {",
                "\t\t\terr = -EINVAL;",
                "\t\t\tgoto err_locked;",
                "\t\t}",
                "\t}",
                "not_move_group:"
            ],
            "deleted": [
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found the Linux kernel in perf_event_open() which can be exploited by an unprivileged user to gain root privileges. The bug allows to build several exploit primitives such as kernel address information leak, arbitrary execution, etc."
    },
    {
        "cve_id": "CVE-2022-20141",
        "code_before_change": "int ip_check_mc_rcu(struct in_device *in_dev, __be32 mc_addr, __be32 src_addr, u8 proto)\n{\n\tstruct ip_mc_list *im;\n\tstruct ip_mc_list __rcu **mc_hash;\n\tstruct ip_sf_list *psf;\n\tint rv = 0;\n\n\tmc_hash = rcu_dereference(in_dev->mc_hash);\n\tif (mc_hash) {\n\t\tu32 hash = hash_32((__force u32)mc_addr, MC_HASH_SZ_LOG);\n\n\t\tfor (im = rcu_dereference(mc_hash[hash]);\n\t\t     im != NULL;\n\t\t     im = rcu_dereference(im->next_hash)) {\n\t\t\tif (im->multiaddr == mc_addr)\n\t\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tfor_each_pmc_rcu(in_dev, im) {\n\t\t\tif (im->multiaddr == mc_addr)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (im && proto == IPPROTO_IGMP) {\n\t\trv = 1;\n\t} else if (im) {\n\t\tif (src_addr) {\n\t\t\tfor (psf = im->sources; psf; psf = psf->sf_next) {\n\t\t\t\tif (psf->sf_inaddr == src_addr)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (psf)\n\t\t\t\trv = psf->sf_count[MCAST_INCLUDE] ||\n\t\t\t\t\tpsf->sf_count[MCAST_EXCLUDE] !=\n\t\t\t\t\tim->sfcount[MCAST_EXCLUDE];\n\t\t\telse\n\t\t\t\trv = im->sfcount[MCAST_EXCLUDE] != 0;\n\t\t} else\n\t\t\trv = 1; /* unspecified source; tentatively allow */\n\t}\n\treturn rv;\n}",
        "code_after_change": "int ip_check_mc_rcu(struct in_device *in_dev, __be32 mc_addr, __be32 src_addr, u8 proto)\n{\n\tstruct ip_mc_list *im;\n\tstruct ip_mc_list __rcu **mc_hash;\n\tstruct ip_sf_list *psf;\n\tint rv = 0;\n\n\tmc_hash = rcu_dereference(in_dev->mc_hash);\n\tif (mc_hash) {\n\t\tu32 hash = hash_32((__force u32)mc_addr, MC_HASH_SZ_LOG);\n\n\t\tfor (im = rcu_dereference(mc_hash[hash]);\n\t\t     im != NULL;\n\t\t     im = rcu_dereference(im->next_hash)) {\n\t\t\tif (im->multiaddr == mc_addr)\n\t\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tfor_each_pmc_rcu(in_dev, im) {\n\t\t\tif (im->multiaddr == mc_addr)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (im && proto == IPPROTO_IGMP) {\n\t\trv = 1;\n\t} else if (im) {\n\t\tif (src_addr) {\n\t\t\tspin_lock_bh(&im->lock);\n\t\t\tfor (psf = im->sources; psf; psf = psf->sf_next) {\n\t\t\t\tif (psf->sf_inaddr == src_addr)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (psf)\n\t\t\t\trv = psf->sf_count[MCAST_INCLUDE] ||\n\t\t\t\t\tpsf->sf_count[MCAST_EXCLUDE] !=\n\t\t\t\t\tim->sfcount[MCAST_EXCLUDE];\n\t\t\telse\n\t\t\t\trv = im->sfcount[MCAST_EXCLUDE] != 0;\n\t\t\tspin_unlock_bh(&im->lock);\n\t\t} else\n\t\t\trv = 1; /* unspecified source; tentatively allow */\n\t}\n\treturn rv;\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,6 +25,7 @@\n \t\trv = 1;\n \t} else if (im) {\n \t\tif (src_addr) {\n+\t\t\tspin_lock_bh(&im->lock);\n \t\t\tfor (psf = im->sources; psf; psf = psf->sf_next) {\n \t\t\t\tif (psf->sf_inaddr == src_addr)\n \t\t\t\t\tbreak;\n@@ -35,6 +36,7 @@\n \t\t\t\t\tim->sfcount[MCAST_EXCLUDE];\n \t\t\telse\n \t\t\t\trv = im->sfcount[MCAST_EXCLUDE] != 0;\n+\t\t\tspin_unlock_bh(&im->lock);\n \t\t} else\n \t\t\trv = 1; /* unspecified source; tentatively allow */\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tspin_lock_bh(&im->lock);",
                "\t\t\tspin_unlock_bh(&im->lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "In ip_check_mc_rcu of igmp.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege when opening and closing inet sockets with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-112551163References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2022-20148",
        "code_before_change": "static int f2fs_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct f2fs_sb_info *sbi;\n\tstruct f2fs_super_block *raw_super;\n\tstruct inode *root;\n\tint err;\n\tbool skip_recovery = false, need_fsck = false;\n\tchar *options = NULL;\n\tint recovery, i, valid_super_block;\n\tstruct curseg_info *seg_i;\n\tint retry_cnt = 1;\n\ntry_onemore:\n\terr = -EINVAL;\n\traw_super = NULL;\n\tvalid_super_block = -1;\n\trecovery = 0;\n\n\t/* allocate memory for f2fs-specific super block info */\n\tsbi = kzalloc(sizeof(struct f2fs_sb_info), GFP_KERNEL);\n\tif (!sbi)\n\t\treturn -ENOMEM;\n\n\tsbi->sb = sb;\n\n\t/* Load the checksum driver */\n\tsbi->s_chksum_driver = crypto_alloc_shash(\"crc32\", 0, 0);\n\tif (IS_ERR(sbi->s_chksum_driver)) {\n\t\tf2fs_err(sbi, \"Cannot load crc32 driver.\");\n\t\terr = PTR_ERR(sbi->s_chksum_driver);\n\t\tsbi->s_chksum_driver = NULL;\n\t\tgoto free_sbi;\n\t}\n\n\t/* set a block size */\n\tif (unlikely(!sb_set_blocksize(sb, F2FS_BLKSIZE))) {\n\t\tf2fs_err(sbi, \"unable to set blocksize\");\n\t\tgoto free_sbi;\n\t}\n\n\terr = read_raw_super_block(sbi, &raw_super, &valid_super_block,\n\t\t\t\t\t\t\t\t&recovery);\n\tif (err)\n\t\tgoto free_sbi;\n\n\tsb->s_fs_info = sbi;\n\tsbi->raw_super = raw_super;\n\n\t/* precompute checksum seed for metadata */\n\tif (f2fs_sb_has_inode_chksum(sbi))\n\t\tsbi->s_chksum_seed = f2fs_chksum(sbi, ~0, raw_super->uuid,\n\t\t\t\t\t\tsizeof(raw_super->uuid));\n\n\tdefault_options(sbi);\n\t/* parse mount options */\n\toptions = kstrdup((const char *)data, GFP_KERNEL);\n\tif (data && !options) {\n\t\terr = -ENOMEM;\n\t\tgoto free_sb_buf;\n\t}\n\n\terr = parse_options(sb, options, false);\n\tif (err)\n\t\tgoto free_options;\n\n\tsb->s_maxbytes = max_file_blocks(NULL) <<\n\t\t\t\tle32_to_cpu(raw_super->log_blocksize);\n\tsb->s_max_links = F2FS_LINK_MAX;\n\n\terr = f2fs_setup_casefold(sbi);\n\tif (err)\n\t\tgoto free_options;\n\n#ifdef CONFIG_QUOTA\n\tsb->dq_op = &f2fs_quota_operations;\n\tsb->s_qcop = &f2fs_quotactl_ops;\n\tsb->s_quota_types = QTYPE_MASK_USR | QTYPE_MASK_GRP | QTYPE_MASK_PRJ;\n\n\tif (f2fs_sb_has_quota_ino(sbi)) {\n\t\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\t\tif (f2fs_qf_ino(sbi->sb, i))\n\t\t\t\tsbi->nquota_files++;\n\t\t}\n\t}\n#endif\n\n\tsb->s_op = &f2fs_sops;\n#ifdef CONFIG_FS_ENCRYPTION\n\tsb->s_cop = &f2fs_cryptops;\n#endif\n#ifdef CONFIG_FS_VERITY\n\tsb->s_vop = &f2fs_verityops;\n#endif\n\tsb->s_xattr = f2fs_xattr_handlers;\n\tsb->s_export_op = &f2fs_export_ops;\n\tsb->s_magic = F2FS_SUPER_MAGIC;\n\tsb->s_time_gran = 1;\n\tsb->s_flags = (sb->s_flags & ~SB_POSIXACL) |\n\t\t(test_opt(sbi, POSIX_ACL) ? SB_POSIXACL : 0);\n\tmemcpy(&sb->s_uuid, raw_super->uuid, sizeof(raw_super->uuid));\n\tsb->s_iflags |= SB_I_CGROUPWB;\n\n\t/* init f2fs-specific super block info */\n\tsbi->valid_super_block = valid_super_block;\n\tinit_rwsem(&sbi->gc_lock);\n\tmutex_init(&sbi->writepages);\n\tinit_rwsem(&sbi->cp_global_sem);\n\tinit_rwsem(&sbi->node_write);\n\tinit_rwsem(&sbi->node_change);\n\n\t/* disallow all the data/node/meta page writes */\n\tset_sbi_flag(sbi, SBI_POR_DOING);\n\tspin_lock_init(&sbi->stat_lock);\n\n\tfor (i = 0; i < NR_PAGE_TYPE; i++) {\n\t\tint n = (i == META) ? 1 : NR_TEMP_TYPE;\n\t\tint j;\n\n\t\tsbi->write_io[i] =\n\t\t\tf2fs_kmalloc(sbi,\n\t\t\t\t     array_size(n,\n\t\t\t\t\t\tsizeof(struct f2fs_bio_info)),\n\t\t\t\t     GFP_KERNEL);\n\t\tif (!sbi->write_io[i]) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free_bio_info;\n\t\t}\n\n\t\tfor (j = HOT; j < n; j++) {\n\t\t\tinit_rwsem(&sbi->write_io[i][j].io_rwsem);\n\t\t\tsbi->write_io[i][j].sbi = sbi;\n\t\t\tsbi->write_io[i][j].bio = NULL;\n\t\t\tspin_lock_init(&sbi->write_io[i][j].io_lock);\n\t\t\tINIT_LIST_HEAD(&sbi->write_io[i][j].io_list);\n\t\t\tINIT_LIST_HEAD(&sbi->write_io[i][j].bio_list);\n\t\t\tinit_rwsem(&sbi->write_io[i][j].bio_list_lock);\n\t\t}\n\t}\n\n\tinit_rwsem(&sbi->cp_rwsem);\n\tinit_rwsem(&sbi->quota_sem);\n\tinit_waitqueue_head(&sbi->cp_wait);\n\tinit_sb_info(sbi);\n\n\terr = f2fs_init_iostat(sbi);\n\tif (err)\n\t\tgoto free_bio_info;\n\n\terr = init_percpu_info(sbi);\n\tif (err)\n\t\tgoto free_iostat;\n\n\tif (F2FS_IO_ALIGNED(sbi)) {\n\t\tsbi->write_io_dummy =\n\t\t\tmempool_create_page_pool(2 * (F2FS_IO_SIZE(sbi) - 1), 0);\n\t\tif (!sbi->write_io_dummy) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free_percpu;\n\t\t}\n\t}\n\n\t/* init per sbi slab cache */\n\terr = f2fs_init_xattr_caches(sbi);\n\tif (err)\n\t\tgoto free_io_dummy;\n\terr = f2fs_init_page_array_cache(sbi);\n\tif (err)\n\t\tgoto free_xattr_cache;\n\n\t/* get an inode for meta space */\n\tsbi->meta_inode = f2fs_iget(sb, F2FS_META_INO(sbi));\n\tif (IS_ERR(sbi->meta_inode)) {\n\t\tf2fs_err(sbi, \"Failed to read F2FS meta data inode\");\n\t\terr = PTR_ERR(sbi->meta_inode);\n\t\tgoto free_page_array_cache;\n\t}\n\n\terr = f2fs_get_valid_checkpoint(sbi);\n\tif (err) {\n\t\tf2fs_err(sbi, \"Failed to get valid F2FS checkpoint\");\n\t\tgoto free_meta_inode;\n\t}\n\n\tif (__is_set_ckpt_flags(F2FS_CKPT(sbi), CP_QUOTA_NEED_FSCK_FLAG))\n\t\tset_sbi_flag(sbi, SBI_QUOTA_NEED_REPAIR);\n\tif (__is_set_ckpt_flags(F2FS_CKPT(sbi), CP_DISABLED_QUICK_FLAG)) {\n\t\tset_sbi_flag(sbi, SBI_CP_DISABLED_QUICK);\n\t\tsbi->interval_time[DISABLE_TIME] = DEF_DISABLE_QUICK_INTERVAL;\n\t}\n\n\tif (__is_set_ckpt_flags(F2FS_CKPT(sbi), CP_FSCK_FLAG))\n\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\t/* Initialize device list */\n\terr = f2fs_scan_devices(sbi);\n\tif (err) {\n\t\tf2fs_err(sbi, \"Failed to find devices\");\n\t\tgoto free_devices;\n\t}\n\n\terr = f2fs_init_post_read_wq(sbi);\n\tif (err) {\n\t\tf2fs_err(sbi, \"Failed to initialize post read workqueue\");\n\t\tgoto free_devices;\n\t}\n\n\tsbi->total_valid_node_count =\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_node_count);\n\tpercpu_counter_set(&sbi->total_valid_inode_count,\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_inode_count));\n\tsbi->user_block_count = le64_to_cpu(sbi->ckpt->user_block_count);\n\tsbi->total_valid_block_count =\n\t\t\t\tle64_to_cpu(sbi->ckpt->valid_block_count);\n\tsbi->last_valid_block_count = sbi->total_valid_block_count;\n\tsbi->reserved_blocks = 0;\n\tsbi->current_reserved_blocks = 0;\n\tlimit_reserve_root(sbi);\n\tadjust_unusable_cap_perc(sbi);\n\n\tfor (i = 0; i < NR_INODE_TYPE; i++) {\n\t\tINIT_LIST_HEAD(&sbi->inode_list[i]);\n\t\tspin_lock_init(&sbi->inode_lock[i]);\n\t}\n\tmutex_init(&sbi->flush_lock);\n\n\tf2fs_init_extent_cache_info(sbi);\n\n\tf2fs_init_ino_entry_info(sbi);\n\n\tf2fs_init_fsync_node_info(sbi);\n\n\t/* setup checkpoint request control and start checkpoint issue thread */\n\tf2fs_init_ckpt_req_control(sbi);\n\tif (!f2fs_readonly(sb) && !test_opt(sbi, DISABLE_CHECKPOINT) &&\n\t\t\ttest_opt(sbi, MERGE_CHECKPOINT)) {\n\t\terr = f2fs_start_ckpt_thread(sbi);\n\t\tif (err) {\n\t\t\tf2fs_err(sbi,\n\t\t\t    \"Failed to start F2FS issue_checkpoint_thread (%d)\",\n\t\t\t    err);\n\t\t\tgoto stop_ckpt_thread;\n\t\t}\n\t}\n\n\t/* setup f2fs internal modules */\n\terr = f2fs_build_segment_manager(sbi);\n\tif (err) {\n\t\tf2fs_err(sbi, \"Failed to initialize F2FS segment manager (%d)\",\n\t\t\t err);\n\t\tgoto free_sm;\n\t}\n\terr = f2fs_build_node_manager(sbi);\n\tif (err) {\n\t\tf2fs_err(sbi, \"Failed to initialize F2FS node manager (%d)\",\n\t\t\t err);\n\t\tgoto free_nm;\n\t}\n\n\t/* For write statistics */\n\tsbi->sectors_written_start = f2fs_get_sectors_written(sbi);\n\n\t/* Read accumulated write IO statistics if exists */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_NODE);\n\tif (__exist_node_summaries(sbi))\n\t\tsbi->kbytes_written =\n\t\t\tle64_to_cpu(seg_i->journal->info.kbytes_written);\n\n\tf2fs_build_gc_manager(sbi);\n\n\terr = f2fs_build_stats(sbi);\n\tif (err)\n\t\tgoto free_nm;\n\n\t/* get an inode for node space */\n\tsbi->node_inode = f2fs_iget(sb, F2FS_NODE_INO(sbi));\n\tif (IS_ERR(sbi->node_inode)) {\n\t\tf2fs_err(sbi, \"Failed to read node inode\");\n\t\terr = PTR_ERR(sbi->node_inode);\n\t\tgoto free_stats;\n\t}\n\n\t/* read root inode and dentry */\n\troot = f2fs_iget(sb, F2FS_ROOT_INO(sbi));\n\tif (IS_ERR(root)) {\n\t\tf2fs_err(sbi, \"Failed to read root inode\");\n\t\terr = PTR_ERR(root);\n\t\tgoto free_node_inode;\n\t}\n\tif (!S_ISDIR(root->i_mode) || !root->i_blocks ||\n\t\t\t!root->i_size || !root->i_nlink) {\n\t\tiput(root);\n\t\terr = -EINVAL;\n\t\tgoto free_node_inode;\n\t}\n\n\tsb->s_root = d_make_root(root); /* allocate root dentry */\n\tif (!sb->s_root) {\n\t\terr = -ENOMEM;\n\t\tgoto free_node_inode;\n\t}\n\n\terr = f2fs_init_compress_inode(sbi);\n\tif (err)\n\t\tgoto free_root_inode;\n\n\terr = f2fs_register_sysfs(sbi);\n\tif (err)\n\t\tgoto free_compress_inode;\n\n#ifdef CONFIG_QUOTA\n\t/* Enable quota usage during mount */\n\tif (f2fs_sb_has_quota_ino(sbi) && !f2fs_readonly(sb)) {\n\t\terr = f2fs_enable_quotas(sb);\n\t\tif (err)\n\t\t\tf2fs_err(sbi, \"Cannot turn on quotas: error %d\", err);\n\t}\n#endif\n\t/* if there are any orphan inodes, free them */\n\terr = f2fs_recover_orphan_inodes(sbi);\n\tif (err)\n\t\tgoto free_meta;\n\n\tif (unlikely(is_set_ckpt_flags(sbi, CP_DISABLED_FLAG)))\n\t\tgoto reset_checkpoint;\n\n\t/* recover fsynced data */\n\tif (!test_opt(sbi, DISABLE_ROLL_FORWARD) &&\n\t\t\t!test_opt(sbi, NORECOVERY)) {\n\t\t/*\n\t\t * mount should be failed, when device has readonly mode, and\n\t\t * previous checkpoint was not done by clean system shutdown.\n\t\t */\n\t\tif (f2fs_hw_is_readonly(sbi)) {\n\t\t\tif (!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\t\t\terr = f2fs_recover_fsync_data(sbi, true);\n\t\t\t\tif (err > 0) {\n\t\t\t\t\terr = -EROFS;\n\t\t\t\t\tf2fs_err(sbi, \"Need to recover fsync data, but \"\n\t\t\t\t\t\t\"write access unavailable, please try \"\n\t\t\t\t\t\t\"mount w/ disable_roll_forward or norecovery\");\n\t\t\t\t}\n\t\t\t\tif (err < 0)\n\t\t\t\t\tgoto free_meta;\n\t\t\t}\n\t\t\tf2fs_info(sbi, \"write access unavailable, skipping recovery\");\n\t\t\tgoto reset_checkpoint;\n\t\t}\n\n\t\tif (need_fsck)\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\t\tif (skip_recovery)\n\t\t\tgoto reset_checkpoint;\n\n\t\terr = f2fs_recover_fsync_data(sbi, false);\n\t\tif (err < 0) {\n\t\t\tif (err != -ENOMEM)\n\t\t\t\tskip_recovery = true;\n\t\t\tneed_fsck = true;\n\t\t\tf2fs_err(sbi, \"Cannot recover all fsync data errno=%d\",\n\t\t\t\t err);\n\t\t\tgoto free_meta;\n\t\t}\n\t} else {\n\t\terr = f2fs_recover_fsync_data(sbi, true);\n\n\t\tif (!f2fs_readonly(sb) && err > 0) {\n\t\t\terr = -EINVAL;\n\t\t\tf2fs_err(sbi, \"Need to recover fsync data\");\n\t\t\tgoto free_meta;\n\t\t}\n\t}\n\n\t/*\n\t * If the f2fs is not readonly and fsync data recovery succeeds,\n\t * check zoned block devices' write pointer consistency.\n\t */\n\tif (!err && !f2fs_readonly(sb) && f2fs_sb_has_blkzoned(sbi)) {\n\t\terr = f2fs_check_write_pointer(sbi);\n\t\tif (err)\n\t\t\tgoto free_meta;\n\t}\n\nreset_checkpoint:\n\tf2fs_init_inmem_curseg(sbi);\n\n\t/* f2fs_recover_fsync_data() cleared this already */\n\tclear_sbi_flag(sbi, SBI_POR_DOING);\n\n\tif (test_opt(sbi, DISABLE_CHECKPOINT)) {\n\t\terr = f2fs_disable_checkpoint(sbi);\n\t\tif (err)\n\t\t\tgoto sync_free_meta;\n\t} else if (is_set_ckpt_flags(sbi, CP_DISABLED_FLAG)) {\n\t\tf2fs_enable_checkpoint(sbi);\n\t}\n\n\t/*\n\t * If filesystem is not mounted as read-only then\n\t * do start the gc_thread.\n\t */\n\tif ((F2FS_OPTION(sbi).bggc_mode != BGGC_MODE_OFF ||\n\t\ttest_opt(sbi, GC_MERGE)) && !f2fs_readonly(sb)) {\n\t\t/* After POR, we can run background GC thread.*/\n\t\terr = f2fs_start_gc_thread(sbi);\n\t\tif (err)\n\t\t\tgoto sync_free_meta;\n\t}\n\tkvfree(options);\n\n\t/* recover broken superblock */\n\tif (recovery) {\n\t\terr = f2fs_commit_super(sbi, true);\n\t\tf2fs_info(sbi, \"Try to recover %dth superblock, ret: %d\",\n\t\t\t  sbi->valid_super_block ? 1 : 2, err);\n\t}\n\n\tf2fs_join_shrinker(sbi);\n\n\tf2fs_tuning_parameters(sbi);\n\n\tf2fs_notice(sbi, \"Mounted with checkpoint version = %llx\",\n\t\t    cur_cp_version(F2FS_CKPT(sbi)));\n\tf2fs_update_time(sbi, CP_TIME);\n\tf2fs_update_time(sbi, REQ_TIME);\n\tclear_sbi_flag(sbi, SBI_CP_DISABLED_QUICK);\n\treturn 0;\n\nsync_free_meta:\n\t/* safe to flush all the data */\n\tsync_filesystem(sbi->sb);\n\tretry_cnt = 0;\n\nfree_meta:\n#ifdef CONFIG_QUOTA\n\tf2fs_truncate_quota_inode_pages(sb);\n\tif (f2fs_sb_has_quota_ino(sbi) && !f2fs_readonly(sb))\n\t\tf2fs_quota_off_umount(sbi->sb);\n#endif\n\t/*\n\t * Some dirty meta pages can be produced by f2fs_recover_orphan_inodes()\n\t * failed by EIO. Then, iput(node_inode) can trigger balance_fs_bg()\n\t * followed by f2fs_write_checkpoint() through f2fs_write_node_pages(), which\n\t * falls into an infinite loop in f2fs_sync_meta_pages().\n\t */\n\ttruncate_inode_pages_final(META_MAPPING(sbi));\n\t/* evict some inodes being cached by GC */\n\tevict_inodes(sb);\n\tf2fs_unregister_sysfs(sbi);\nfree_compress_inode:\n\tf2fs_destroy_compress_inode(sbi);\nfree_root_inode:\n\tdput(sb->s_root);\n\tsb->s_root = NULL;\nfree_node_inode:\n\tf2fs_release_ino_entry(sbi, true);\n\ttruncate_inode_pages_final(NODE_MAPPING(sbi));\n\tiput(sbi->node_inode);\n\tsbi->node_inode = NULL;\nfree_stats:\n\tf2fs_destroy_stats(sbi);\nfree_nm:\n\tf2fs_destroy_node_manager(sbi);\nfree_sm:\n\tf2fs_destroy_segment_manager(sbi);\n\tf2fs_destroy_post_read_wq(sbi);\nstop_ckpt_thread:\n\tf2fs_stop_ckpt_thread(sbi);\nfree_devices:\n\tdestroy_device_list(sbi);\n\tkvfree(sbi->ckpt);\nfree_meta_inode:\n\tmake_bad_inode(sbi->meta_inode);\n\tiput(sbi->meta_inode);\n\tsbi->meta_inode = NULL;\nfree_page_array_cache:\n\tf2fs_destroy_page_array_cache(sbi);\nfree_xattr_cache:\n\tf2fs_destroy_xattr_caches(sbi);\nfree_io_dummy:\n\tmempool_destroy(sbi->write_io_dummy);\nfree_percpu:\n\tdestroy_percpu_info(sbi);\nfree_iostat:\n\tf2fs_destroy_iostat(sbi);\nfree_bio_info:\n\tfor (i = 0; i < NR_PAGE_TYPE; i++)\n\t\tkvfree(sbi->write_io[i]);\n\n#ifdef CONFIG_UNICODE\n\tutf8_unload(sb->s_encoding);\n\tsb->s_encoding = NULL;\n#endif\nfree_options:\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(F2FS_OPTION(sbi).s_qf_names[i]);\n#endif\n\tfscrypt_free_dummy_policy(&F2FS_OPTION(sbi).dummy_enc_policy);\n\tkvfree(options);\nfree_sb_buf:\n\tkfree(raw_super);\nfree_sbi:\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi);\n\n\t/* give only one another chance */\n\tif (retry_cnt > 0 && skip_recovery) {\n\t\tretry_cnt--;\n\t\tshrink_dcache_sb(sb);\n\t\tgoto try_onemore;\n\t}\n\treturn err;\n}",
        "code_after_change": "static int f2fs_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct f2fs_sb_info *sbi;\n\tstruct f2fs_super_block *raw_super;\n\tstruct inode *root;\n\tint err;\n\tbool skip_recovery = false, need_fsck = false;\n\tchar *options = NULL;\n\tint recovery, i, valid_super_block;\n\tstruct curseg_info *seg_i;\n\tint retry_cnt = 1;\n\ntry_onemore:\n\terr = -EINVAL;\n\traw_super = NULL;\n\tvalid_super_block = -1;\n\trecovery = 0;\n\n\t/* allocate memory for f2fs-specific super block info */\n\tsbi = kzalloc(sizeof(struct f2fs_sb_info), GFP_KERNEL);\n\tif (!sbi)\n\t\treturn -ENOMEM;\n\n\tsbi->sb = sb;\n\n\t/* Load the checksum driver */\n\tsbi->s_chksum_driver = crypto_alloc_shash(\"crc32\", 0, 0);\n\tif (IS_ERR(sbi->s_chksum_driver)) {\n\t\tf2fs_err(sbi, \"Cannot load crc32 driver.\");\n\t\terr = PTR_ERR(sbi->s_chksum_driver);\n\t\tsbi->s_chksum_driver = NULL;\n\t\tgoto free_sbi;\n\t}\n\n\t/* set a block size */\n\tif (unlikely(!sb_set_blocksize(sb, F2FS_BLKSIZE))) {\n\t\tf2fs_err(sbi, \"unable to set blocksize\");\n\t\tgoto free_sbi;\n\t}\n\n\terr = read_raw_super_block(sbi, &raw_super, &valid_super_block,\n\t\t\t\t\t\t\t\t&recovery);\n\tif (err)\n\t\tgoto free_sbi;\n\n\tsb->s_fs_info = sbi;\n\tsbi->raw_super = raw_super;\n\n\t/* precompute checksum seed for metadata */\n\tif (f2fs_sb_has_inode_chksum(sbi))\n\t\tsbi->s_chksum_seed = f2fs_chksum(sbi, ~0, raw_super->uuid,\n\t\t\t\t\t\tsizeof(raw_super->uuid));\n\n\tdefault_options(sbi);\n\t/* parse mount options */\n\toptions = kstrdup((const char *)data, GFP_KERNEL);\n\tif (data && !options) {\n\t\terr = -ENOMEM;\n\t\tgoto free_sb_buf;\n\t}\n\n\terr = parse_options(sb, options, false);\n\tif (err)\n\t\tgoto free_options;\n\n\tsb->s_maxbytes = max_file_blocks(NULL) <<\n\t\t\t\tle32_to_cpu(raw_super->log_blocksize);\n\tsb->s_max_links = F2FS_LINK_MAX;\n\n\terr = f2fs_setup_casefold(sbi);\n\tif (err)\n\t\tgoto free_options;\n\n#ifdef CONFIG_QUOTA\n\tsb->dq_op = &f2fs_quota_operations;\n\tsb->s_qcop = &f2fs_quotactl_ops;\n\tsb->s_quota_types = QTYPE_MASK_USR | QTYPE_MASK_GRP | QTYPE_MASK_PRJ;\n\n\tif (f2fs_sb_has_quota_ino(sbi)) {\n\t\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\t\tif (f2fs_qf_ino(sbi->sb, i))\n\t\t\t\tsbi->nquota_files++;\n\t\t}\n\t}\n#endif\n\n\tsb->s_op = &f2fs_sops;\n#ifdef CONFIG_FS_ENCRYPTION\n\tsb->s_cop = &f2fs_cryptops;\n#endif\n#ifdef CONFIG_FS_VERITY\n\tsb->s_vop = &f2fs_verityops;\n#endif\n\tsb->s_xattr = f2fs_xattr_handlers;\n\tsb->s_export_op = &f2fs_export_ops;\n\tsb->s_magic = F2FS_SUPER_MAGIC;\n\tsb->s_time_gran = 1;\n\tsb->s_flags = (sb->s_flags & ~SB_POSIXACL) |\n\t\t(test_opt(sbi, POSIX_ACL) ? SB_POSIXACL : 0);\n\tmemcpy(&sb->s_uuid, raw_super->uuid, sizeof(raw_super->uuid));\n\tsb->s_iflags |= SB_I_CGROUPWB;\n\n\t/* init f2fs-specific super block info */\n\tsbi->valid_super_block = valid_super_block;\n\tinit_rwsem(&sbi->gc_lock);\n\tmutex_init(&sbi->writepages);\n\tinit_rwsem(&sbi->cp_global_sem);\n\tinit_rwsem(&sbi->node_write);\n\tinit_rwsem(&sbi->node_change);\n\n\t/* disallow all the data/node/meta page writes */\n\tset_sbi_flag(sbi, SBI_POR_DOING);\n\tspin_lock_init(&sbi->stat_lock);\n\n\tfor (i = 0; i < NR_PAGE_TYPE; i++) {\n\t\tint n = (i == META) ? 1 : NR_TEMP_TYPE;\n\t\tint j;\n\n\t\tsbi->write_io[i] =\n\t\t\tf2fs_kmalloc(sbi,\n\t\t\t\t     array_size(n,\n\t\t\t\t\t\tsizeof(struct f2fs_bio_info)),\n\t\t\t\t     GFP_KERNEL);\n\t\tif (!sbi->write_io[i]) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free_bio_info;\n\t\t}\n\n\t\tfor (j = HOT; j < n; j++) {\n\t\t\tinit_rwsem(&sbi->write_io[i][j].io_rwsem);\n\t\t\tsbi->write_io[i][j].sbi = sbi;\n\t\t\tsbi->write_io[i][j].bio = NULL;\n\t\t\tspin_lock_init(&sbi->write_io[i][j].io_lock);\n\t\t\tINIT_LIST_HEAD(&sbi->write_io[i][j].io_list);\n\t\t\tINIT_LIST_HEAD(&sbi->write_io[i][j].bio_list);\n\t\t\tinit_rwsem(&sbi->write_io[i][j].bio_list_lock);\n\t\t}\n\t}\n\n\tinit_rwsem(&sbi->cp_rwsem);\n\tinit_rwsem(&sbi->quota_sem);\n\tinit_waitqueue_head(&sbi->cp_wait);\n\tinit_sb_info(sbi);\n\n\terr = f2fs_init_iostat(sbi);\n\tif (err)\n\t\tgoto free_bio_info;\n\n\terr = init_percpu_info(sbi);\n\tif (err)\n\t\tgoto free_iostat;\n\n\tif (F2FS_IO_ALIGNED(sbi)) {\n\t\tsbi->write_io_dummy =\n\t\t\tmempool_create_page_pool(2 * (F2FS_IO_SIZE(sbi) - 1), 0);\n\t\tif (!sbi->write_io_dummy) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free_percpu;\n\t\t}\n\t}\n\n\t/* init per sbi slab cache */\n\terr = f2fs_init_xattr_caches(sbi);\n\tif (err)\n\t\tgoto free_io_dummy;\n\terr = f2fs_init_page_array_cache(sbi);\n\tif (err)\n\t\tgoto free_xattr_cache;\n\n\t/* get an inode for meta space */\n\tsbi->meta_inode = f2fs_iget(sb, F2FS_META_INO(sbi));\n\tif (IS_ERR(sbi->meta_inode)) {\n\t\tf2fs_err(sbi, \"Failed to read F2FS meta data inode\");\n\t\terr = PTR_ERR(sbi->meta_inode);\n\t\tgoto free_page_array_cache;\n\t}\n\n\terr = f2fs_get_valid_checkpoint(sbi);\n\tif (err) {\n\t\tf2fs_err(sbi, \"Failed to get valid F2FS checkpoint\");\n\t\tgoto free_meta_inode;\n\t}\n\n\tif (__is_set_ckpt_flags(F2FS_CKPT(sbi), CP_QUOTA_NEED_FSCK_FLAG))\n\t\tset_sbi_flag(sbi, SBI_QUOTA_NEED_REPAIR);\n\tif (__is_set_ckpt_flags(F2FS_CKPT(sbi), CP_DISABLED_QUICK_FLAG)) {\n\t\tset_sbi_flag(sbi, SBI_CP_DISABLED_QUICK);\n\t\tsbi->interval_time[DISABLE_TIME] = DEF_DISABLE_QUICK_INTERVAL;\n\t}\n\n\tif (__is_set_ckpt_flags(F2FS_CKPT(sbi), CP_FSCK_FLAG))\n\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\t/* Initialize device list */\n\terr = f2fs_scan_devices(sbi);\n\tif (err) {\n\t\tf2fs_err(sbi, \"Failed to find devices\");\n\t\tgoto free_devices;\n\t}\n\n\terr = f2fs_init_post_read_wq(sbi);\n\tif (err) {\n\t\tf2fs_err(sbi, \"Failed to initialize post read workqueue\");\n\t\tgoto free_devices;\n\t}\n\n\tsbi->total_valid_node_count =\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_node_count);\n\tpercpu_counter_set(&sbi->total_valid_inode_count,\n\t\t\t\tle32_to_cpu(sbi->ckpt->valid_inode_count));\n\tsbi->user_block_count = le64_to_cpu(sbi->ckpt->user_block_count);\n\tsbi->total_valid_block_count =\n\t\t\t\tle64_to_cpu(sbi->ckpt->valid_block_count);\n\tsbi->last_valid_block_count = sbi->total_valid_block_count;\n\tsbi->reserved_blocks = 0;\n\tsbi->current_reserved_blocks = 0;\n\tlimit_reserve_root(sbi);\n\tadjust_unusable_cap_perc(sbi);\n\n\tfor (i = 0; i < NR_INODE_TYPE; i++) {\n\t\tINIT_LIST_HEAD(&sbi->inode_list[i]);\n\t\tspin_lock_init(&sbi->inode_lock[i]);\n\t}\n\tmutex_init(&sbi->flush_lock);\n\n\tf2fs_init_extent_cache_info(sbi);\n\n\tf2fs_init_ino_entry_info(sbi);\n\n\tf2fs_init_fsync_node_info(sbi);\n\n\t/* setup checkpoint request control and start checkpoint issue thread */\n\tf2fs_init_ckpt_req_control(sbi);\n\tif (!f2fs_readonly(sb) && !test_opt(sbi, DISABLE_CHECKPOINT) &&\n\t\t\ttest_opt(sbi, MERGE_CHECKPOINT)) {\n\t\terr = f2fs_start_ckpt_thread(sbi);\n\t\tif (err) {\n\t\t\tf2fs_err(sbi,\n\t\t\t    \"Failed to start F2FS issue_checkpoint_thread (%d)\",\n\t\t\t    err);\n\t\t\tgoto stop_ckpt_thread;\n\t\t}\n\t}\n\n\t/* setup f2fs internal modules */\n\terr = f2fs_build_segment_manager(sbi);\n\tif (err) {\n\t\tf2fs_err(sbi, \"Failed to initialize F2FS segment manager (%d)\",\n\t\t\t err);\n\t\tgoto free_sm;\n\t}\n\terr = f2fs_build_node_manager(sbi);\n\tif (err) {\n\t\tf2fs_err(sbi, \"Failed to initialize F2FS node manager (%d)\",\n\t\t\t err);\n\t\tgoto free_nm;\n\t}\n\n\t/* For write statistics */\n\tsbi->sectors_written_start = f2fs_get_sectors_written(sbi);\n\n\t/* Read accumulated write IO statistics if exists */\n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_NODE);\n\tif (__exist_node_summaries(sbi))\n\t\tsbi->kbytes_written =\n\t\t\tle64_to_cpu(seg_i->journal->info.kbytes_written);\n\n\tf2fs_build_gc_manager(sbi);\n\n\terr = f2fs_build_stats(sbi);\n\tif (err)\n\t\tgoto free_nm;\n\n\t/* get an inode for node space */\n\tsbi->node_inode = f2fs_iget(sb, F2FS_NODE_INO(sbi));\n\tif (IS_ERR(sbi->node_inode)) {\n\t\tf2fs_err(sbi, \"Failed to read node inode\");\n\t\terr = PTR_ERR(sbi->node_inode);\n\t\tgoto free_stats;\n\t}\n\n\t/* read root inode and dentry */\n\troot = f2fs_iget(sb, F2FS_ROOT_INO(sbi));\n\tif (IS_ERR(root)) {\n\t\tf2fs_err(sbi, \"Failed to read root inode\");\n\t\terr = PTR_ERR(root);\n\t\tgoto free_node_inode;\n\t}\n\tif (!S_ISDIR(root->i_mode) || !root->i_blocks ||\n\t\t\t!root->i_size || !root->i_nlink) {\n\t\tiput(root);\n\t\terr = -EINVAL;\n\t\tgoto free_node_inode;\n\t}\n\n\tsb->s_root = d_make_root(root); /* allocate root dentry */\n\tif (!sb->s_root) {\n\t\terr = -ENOMEM;\n\t\tgoto free_node_inode;\n\t}\n\n\terr = f2fs_init_compress_inode(sbi);\n\tif (err)\n\t\tgoto free_root_inode;\n\n\terr = f2fs_register_sysfs(sbi);\n\tif (err)\n\t\tgoto free_compress_inode;\n\n#ifdef CONFIG_QUOTA\n\t/* Enable quota usage during mount */\n\tif (f2fs_sb_has_quota_ino(sbi) && !f2fs_readonly(sb)) {\n\t\terr = f2fs_enable_quotas(sb);\n\t\tif (err)\n\t\t\tf2fs_err(sbi, \"Cannot turn on quotas: error %d\", err);\n\t}\n#endif\n\t/* if there are any orphan inodes, free them */\n\terr = f2fs_recover_orphan_inodes(sbi);\n\tif (err)\n\t\tgoto free_meta;\n\n\tif (unlikely(is_set_ckpt_flags(sbi, CP_DISABLED_FLAG)))\n\t\tgoto reset_checkpoint;\n\n\t/* recover fsynced data */\n\tif (!test_opt(sbi, DISABLE_ROLL_FORWARD) &&\n\t\t\t!test_opt(sbi, NORECOVERY)) {\n\t\t/*\n\t\t * mount should be failed, when device has readonly mode, and\n\t\t * previous checkpoint was not done by clean system shutdown.\n\t\t */\n\t\tif (f2fs_hw_is_readonly(sbi)) {\n\t\t\tif (!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\t\t\terr = f2fs_recover_fsync_data(sbi, true);\n\t\t\t\tif (err > 0) {\n\t\t\t\t\terr = -EROFS;\n\t\t\t\t\tf2fs_err(sbi, \"Need to recover fsync data, but \"\n\t\t\t\t\t\t\"write access unavailable, please try \"\n\t\t\t\t\t\t\"mount w/ disable_roll_forward or norecovery\");\n\t\t\t\t}\n\t\t\t\tif (err < 0)\n\t\t\t\t\tgoto free_meta;\n\t\t\t}\n\t\t\tf2fs_info(sbi, \"write access unavailable, skipping recovery\");\n\t\t\tgoto reset_checkpoint;\n\t\t}\n\n\t\tif (need_fsck)\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\n\t\tif (skip_recovery)\n\t\t\tgoto reset_checkpoint;\n\n\t\terr = f2fs_recover_fsync_data(sbi, false);\n\t\tif (err < 0) {\n\t\t\tif (err != -ENOMEM)\n\t\t\t\tskip_recovery = true;\n\t\t\tneed_fsck = true;\n\t\t\tf2fs_err(sbi, \"Cannot recover all fsync data errno=%d\",\n\t\t\t\t err);\n\t\t\tgoto free_meta;\n\t\t}\n\t} else {\n\t\terr = f2fs_recover_fsync_data(sbi, true);\n\n\t\tif (!f2fs_readonly(sb) && err > 0) {\n\t\t\terr = -EINVAL;\n\t\t\tf2fs_err(sbi, \"Need to recover fsync data\");\n\t\t\tgoto free_meta;\n\t\t}\n\t}\n\n\t/*\n\t * If the f2fs is not readonly and fsync data recovery succeeds,\n\t * check zoned block devices' write pointer consistency.\n\t */\n\tif (!err && !f2fs_readonly(sb) && f2fs_sb_has_blkzoned(sbi)) {\n\t\terr = f2fs_check_write_pointer(sbi);\n\t\tif (err)\n\t\t\tgoto free_meta;\n\t}\n\nreset_checkpoint:\n\tf2fs_init_inmem_curseg(sbi);\n\n\t/* f2fs_recover_fsync_data() cleared this already */\n\tclear_sbi_flag(sbi, SBI_POR_DOING);\n\n\tif (test_opt(sbi, DISABLE_CHECKPOINT)) {\n\t\terr = f2fs_disable_checkpoint(sbi);\n\t\tif (err)\n\t\t\tgoto sync_free_meta;\n\t} else if (is_set_ckpt_flags(sbi, CP_DISABLED_FLAG)) {\n\t\tf2fs_enable_checkpoint(sbi);\n\t}\n\n\t/*\n\t * If filesystem is not mounted as read-only then\n\t * do start the gc_thread.\n\t */\n\tif ((F2FS_OPTION(sbi).bggc_mode != BGGC_MODE_OFF ||\n\t\ttest_opt(sbi, GC_MERGE)) && !f2fs_readonly(sb)) {\n\t\t/* After POR, we can run background GC thread.*/\n\t\terr = f2fs_start_gc_thread(sbi);\n\t\tif (err)\n\t\t\tgoto sync_free_meta;\n\t}\n\tkvfree(options);\n\n\t/* recover broken superblock */\n\tif (recovery) {\n\t\terr = f2fs_commit_super(sbi, true);\n\t\tf2fs_info(sbi, \"Try to recover %dth superblock, ret: %d\",\n\t\t\t  sbi->valid_super_block ? 1 : 2, err);\n\t}\n\n\tf2fs_join_shrinker(sbi);\n\n\tf2fs_tuning_parameters(sbi);\n\n\tf2fs_notice(sbi, \"Mounted with checkpoint version = %llx\",\n\t\t    cur_cp_version(F2FS_CKPT(sbi)));\n\tf2fs_update_time(sbi, CP_TIME);\n\tf2fs_update_time(sbi, REQ_TIME);\n\tclear_sbi_flag(sbi, SBI_CP_DISABLED_QUICK);\n\treturn 0;\n\nsync_free_meta:\n\t/* safe to flush all the data */\n\tsync_filesystem(sbi->sb);\n\tretry_cnt = 0;\n\nfree_meta:\n#ifdef CONFIG_QUOTA\n\tf2fs_truncate_quota_inode_pages(sb);\n\tif (f2fs_sb_has_quota_ino(sbi) && !f2fs_readonly(sb))\n\t\tf2fs_quota_off_umount(sbi->sb);\n#endif\n\t/*\n\t * Some dirty meta pages can be produced by f2fs_recover_orphan_inodes()\n\t * failed by EIO. Then, iput(node_inode) can trigger balance_fs_bg()\n\t * followed by f2fs_write_checkpoint() through f2fs_write_node_pages(), which\n\t * falls into an infinite loop in f2fs_sync_meta_pages().\n\t */\n\ttruncate_inode_pages_final(META_MAPPING(sbi));\n\t/* evict some inodes being cached by GC */\n\tevict_inodes(sb);\n\tf2fs_unregister_sysfs(sbi);\nfree_compress_inode:\n\tf2fs_destroy_compress_inode(sbi);\nfree_root_inode:\n\tdput(sb->s_root);\n\tsb->s_root = NULL;\nfree_node_inode:\n\tf2fs_release_ino_entry(sbi, true);\n\ttruncate_inode_pages_final(NODE_MAPPING(sbi));\n\tiput(sbi->node_inode);\n\tsbi->node_inode = NULL;\nfree_stats:\n\tf2fs_destroy_stats(sbi);\nfree_nm:\n\t/* stop discard thread before destroying node manager */\n\tf2fs_stop_discard_thread(sbi);\n\tf2fs_destroy_node_manager(sbi);\nfree_sm:\n\tf2fs_destroy_segment_manager(sbi);\n\tf2fs_destroy_post_read_wq(sbi);\nstop_ckpt_thread:\n\tf2fs_stop_ckpt_thread(sbi);\nfree_devices:\n\tdestroy_device_list(sbi);\n\tkvfree(sbi->ckpt);\nfree_meta_inode:\n\tmake_bad_inode(sbi->meta_inode);\n\tiput(sbi->meta_inode);\n\tsbi->meta_inode = NULL;\nfree_page_array_cache:\n\tf2fs_destroy_page_array_cache(sbi);\nfree_xattr_cache:\n\tf2fs_destroy_xattr_caches(sbi);\nfree_io_dummy:\n\tmempool_destroy(sbi->write_io_dummy);\nfree_percpu:\n\tdestroy_percpu_info(sbi);\nfree_iostat:\n\tf2fs_destroy_iostat(sbi);\nfree_bio_info:\n\tfor (i = 0; i < NR_PAGE_TYPE; i++)\n\t\tkvfree(sbi->write_io[i]);\n\n#ifdef CONFIG_UNICODE\n\tutf8_unload(sb->s_encoding);\n\tsb->s_encoding = NULL;\n#endif\nfree_options:\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(F2FS_OPTION(sbi).s_qf_names[i]);\n#endif\n\tfscrypt_free_dummy_policy(&F2FS_OPTION(sbi).dummy_enc_policy);\n\tkvfree(options);\nfree_sb_buf:\n\tkfree(raw_super);\nfree_sbi:\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi);\n\n\t/* give only one another chance */\n\tif (retry_cnt > 0 && skip_recovery) {\n\t\tretry_cnt--;\n\t\tshrink_dcache_sb(sb);\n\t\tgoto try_onemore;\n\t}\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -460,6 +460,8 @@\n free_stats:\n \tf2fs_destroy_stats(sbi);\n free_nm:\n+\t/* stop discard thread before destroying node manager */\n+\tf2fs_stop_discard_thread(sbi);\n \tf2fs_destroy_node_manager(sbi);\n free_sm:\n \tf2fs_destroy_segment_manager(sbi);",
        "function_modified_lines": {
            "added": [
                "\t/* stop discard thread before destroying node manager */",
                "\tf2fs_stop_discard_thread(sbi);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In TBD of TBD, there is a possible use-after-free due to a race condition. This could lead to local escalation of privilege in the kernel with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-219513976References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2022-20154",
        "code_before_change": "static void sctp_diag_dump(struct sk_buff *skb, struct netlink_callback *cb,\n\t\t\t   const struct inet_diag_req_v2 *r)\n{\n\tu32 idiag_states = r->idiag_states;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct sctp_comm_param commp = {\n\t\t.skb = skb,\n\t\t.cb = cb,\n\t\t.r = r,\n\t\t.net_admin = netlink_net_capable(cb->skb, CAP_NET_ADMIN),\n\t};\n\tint pos = cb->args[2];\n\n\t/* eps hashtable dumps\n\t * args:\n\t * 0 : if it will traversal listen sock\n\t * 1 : to record the sock pos of this time's traversal\n\t * 4 : to work as a temporary variable to traversal list\n\t */\n\tif (cb->args[0] == 0) {\n\t\tif (!(idiag_states & TCPF_LISTEN))\n\t\t\tgoto skip;\n\t\tif (sctp_for_each_endpoint(sctp_ep_dump, &commp))\n\t\t\tgoto done;\nskip:\n\t\tcb->args[0] = 1;\n\t\tcb->args[1] = 0;\n\t\tcb->args[4] = 0;\n\t}\n\n\t/* asocs by transport hashtable dump\n\t * args:\n\t * 1 : to record the assoc pos of this time's traversal\n\t * 2 : to record the transport pos of this time's traversal\n\t * 3 : to mark if we have dumped the ep info of the current asoc\n\t * 4 : to work as a temporary variable to traversal list\n\t * 5 : to save the sk we get from travelsing the tsp list.\n\t */\n\tif (!(idiag_states & ~(TCPF_LISTEN | TCPF_CLOSE)))\n\t\tgoto done;\n\n\tsctp_for_each_transport(sctp_sock_filter, sctp_sock_dump,\n\t\t\t\tnet, &pos, &commp);\n\tcb->args[2] = pos;\n\ndone:\n\tcb->args[1] = cb->args[4];\n\tcb->args[4] = 0;\n}",
        "code_after_change": "static void sctp_diag_dump(struct sk_buff *skb, struct netlink_callback *cb,\n\t\t\t   const struct inet_diag_req_v2 *r)\n{\n\tu32 idiag_states = r->idiag_states;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct sctp_comm_param commp = {\n\t\t.skb = skb,\n\t\t.cb = cb,\n\t\t.r = r,\n\t\t.net_admin = netlink_net_capable(cb->skb, CAP_NET_ADMIN),\n\t};\n\tint pos = cb->args[2];\n\n\t/* eps hashtable dumps\n\t * args:\n\t * 0 : if it will traversal listen sock\n\t * 1 : to record the sock pos of this time's traversal\n\t * 4 : to work as a temporary variable to traversal list\n\t */\n\tif (cb->args[0] == 0) {\n\t\tif (!(idiag_states & TCPF_LISTEN))\n\t\t\tgoto skip;\n\t\tif (sctp_for_each_endpoint(sctp_ep_dump, &commp))\n\t\t\tgoto done;\nskip:\n\t\tcb->args[0] = 1;\n\t\tcb->args[1] = 0;\n\t\tcb->args[4] = 0;\n\t}\n\n\t/* asocs by transport hashtable dump\n\t * args:\n\t * 1 : to record the assoc pos of this time's traversal\n\t * 2 : to record the transport pos of this time's traversal\n\t * 3 : to mark if we have dumped the ep info of the current asoc\n\t * 4 : to work as a temporary variable to traversal list\n\t * 5 : to save the sk we get from travelsing the tsp list.\n\t */\n\tif (!(idiag_states & ~(TCPF_LISTEN | TCPF_CLOSE)))\n\t\tgoto done;\n\n\tsctp_transport_traverse_process(sctp_sock_filter, sctp_sock_dump,\n\t\t\t\t\tnet, &pos, &commp);\n\tcb->args[2] = pos;\n\ndone:\n\tcb->args[1] = cb->args[4];\n\tcb->args[4] = 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,8 +39,8 @@\n \tif (!(idiag_states & ~(TCPF_LISTEN | TCPF_CLOSE)))\n \t\tgoto done;\n \n-\tsctp_for_each_transport(sctp_sock_filter, sctp_sock_dump,\n-\t\t\t\tnet, &pos, &commp);\n+\tsctp_transport_traverse_process(sctp_sock_filter, sctp_sock_dump,\n+\t\t\t\t\tnet, &pos, &commp);\n \tcb->args[2] = pos;\n \n done:",
        "function_modified_lines": {
            "added": [
                "\tsctp_transport_traverse_process(sctp_sock_filter, sctp_sock_dump,",
                "\t\t\t\t\tnet, &pos, &commp);"
            ],
            "deleted": [
                "\tsctp_for_each_transport(sctp_sock_filter, sctp_sock_dump,",
                "\t\t\t\tnet, &pos, &commp);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In lock_sock_nested of sock.c, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-174846563References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2022-20154",
        "code_before_change": "void sctp_endpoint_hold(struct sctp_endpoint *ep)\n{\n\trefcount_inc(&ep->base.refcnt);\n}",
        "code_after_change": "int sctp_endpoint_hold(struct sctp_endpoint *ep)\n{\n\treturn refcount_inc_not_zero(&ep->base.refcnt);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-void sctp_endpoint_hold(struct sctp_endpoint *ep)\n+int sctp_endpoint_hold(struct sctp_endpoint *ep)\n {\n-\trefcount_inc(&ep->base.refcnt);\n+\treturn refcount_inc_not_zero(&ep->base.refcnt);\n }",
        "function_modified_lines": {
            "added": [
                "int sctp_endpoint_hold(struct sctp_endpoint *ep)",
                "\treturn refcount_inc_not_zero(&ep->base.refcnt);"
            ],
            "deleted": [
                "void sctp_endpoint_hold(struct sctp_endpoint *ep)",
                "\trefcount_inc(&ep->base.refcnt);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In lock_sock_nested of sock.c, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-174846563References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2022-20154",
        "code_before_change": "static void sctp_endpoint_destroy(struct sctp_endpoint *ep)\n{\n\tstruct sock *sk;\n\n\tif (unlikely(!ep->base.dead)) {\n\t\tWARN(1, \"Attempt to destroy undead endpoint %p!\\n\", ep);\n\t\treturn;\n\t}\n\n\t/* Free the digest buffer */\n\tkfree(ep->digest);\n\n\t/* SCTP-AUTH: Free up AUTH releated data such as shared keys\n\t * chunks and hmacs arrays that were allocated\n\t */\n\tsctp_auth_destroy_keys(&ep->endpoint_shared_keys);\n\tsctp_auth_free(ep);\n\n\t/* Cleanup. */\n\tsctp_inq_free(&ep->base.inqueue);\n\tsctp_bind_addr_free(&ep->base.bind_addr);\n\n\tmemset(ep->secret_key, 0, sizeof(ep->secret_key));\n\n\tsk = ep->base.sk;\n\t/* Remove and free the port */\n\tif (sctp_sk(sk)->bind_hash)\n\t\tsctp_put_port(sk);\n\n\tsctp_sk(sk)->ep = NULL;\n\t/* Give up our hold on the sock */\n\tsock_put(sk);\n\n\tkfree(ep);\n\tSCTP_DBG_OBJCNT_DEC(ep);\n}",
        "code_after_change": "static void sctp_endpoint_destroy(struct sctp_endpoint *ep)\n{\n\tstruct sock *sk;\n\n\tif (unlikely(!ep->base.dead)) {\n\t\tWARN(1, \"Attempt to destroy undead endpoint %p!\\n\", ep);\n\t\treturn;\n\t}\n\n\t/* Free the digest buffer */\n\tkfree(ep->digest);\n\n\t/* SCTP-AUTH: Free up AUTH releated data such as shared keys\n\t * chunks and hmacs arrays that were allocated\n\t */\n\tsctp_auth_destroy_keys(&ep->endpoint_shared_keys);\n\tsctp_auth_free(ep);\n\n\t/* Cleanup. */\n\tsctp_inq_free(&ep->base.inqueue);\n\tsctp_bind_addr_free(&ep->base.bind_addr);\n\n\tmemset(ep->secret_key, 0, sizeof(ep->secret_key));\n\n\tsk = ep->base.sk;\n\t/* Remove and free the port */\n\tif (sctp_sk(sk)->bind_hash)\n\t\tsctp_put_port(sk);\n\n\tcall_rcu(&ep->rcu, sctp_endpoint_destroy_rcu);\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,10 +27,5 @@\n \tif (sctp_sk(sk)->bind_hash)\n \t\tsctp_put_port(sk);\n \n-\tsctp_sk(sk)->ep = NULL;\n-\t/* Give up our hold on the sock */\n-\tsock_put(sk);\n-\n-\tkfree(ep);\n-\tSCTP_DBG_OBJCNT_DEC(ep);\n+\tcall_rcu(&ep->rcu, sctp_endpoint_destroy_rcu);\n }",
        "function_modified_lines": {
            "added": [
                "\tcall_rcu(&ep->rcu, sctp_endpoint_destroy_rcu);"
            ],
            "deleted": [
                "\tsctp_sk(sk)->ep = NULL;",
                "\t/* Give up our hold on the sock */",
                "\tsock_put(sk);",
                "",
                "\tkfree(ep);",
                "\tSCTP_DBG_OBJCNT_DEC(ep);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In lock_sock_nested of sock.c, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-174846563References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2022-20567",
        "code_before_change": "static void pppol2tp_session_close(struct l2tp_session *session)\n{\n}",
        "code_after_change": "static void pppol2tp_session_close(struct l2tp_session *session)\n{\n\tstruct pppol2tp_session *ps;\n\n\tps = l2tp_session_priv(session);\n\tmutex_lock(&ps->sk_lock);\n\tps->__sk = rcu_dereference_protected(ps->sk,\n\t\t\t\t\t     lockdep_is_held(&ps->sk_lock));\n\tRCU_INIT_POINTER(ps->sk, NULL);\n\tif (ps->__sk)\n\t\tcall_rcu(&ps->rcu, pppol2tp_put_sk);\n\tmutex_unlock(&ps->sk_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,3 +1,13 @@\n static void pppol2tp_session_close(struct l2tp_session *session)\n {\n+\tstruct pppol2tp_session *ps;\n+\n+\tps = l2tp_session_priv(session);\n+\tmutex_lock(&ps->sk_lock);\n+\tps->__sk = rcu_dereference_protected(ps->sk,\n+\t\t\t\t\t     lockdep_is_held(&ps->sk_lock));\n+\tRCU_INIT_POINTER(ps->sk, NULL);\n+\tif (ps->__sk)\n+\t\tcall_rcu(&ps->rcu, pppol2tp_put_sk);\n+\tmutex_unlock(&ps->sk_lock);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct pppol2tp_session *ps;",
                "",
                "\tps = l2tp_session_priv(session);",
                "\tmutex_lock(&ps->sk_lock);",
                "\tps->__sk = rcu_dereference_protected(ps->sk,",
                "\t\t\t\t\t     lockdep_is_held(&ps->sk_lock));",
                "\tRCU_INIT_POINTER(ps->sk, NULL);",
                "\tif (ps->__sk)",
                "\t\tcall_rcu(&ps->rcu, pppol2tp_put_sk);",
                "\tmutex_unlock(&ps->sk_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In pppol2tp_create of l2tp_ppp.c, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-186777253References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2022-20567",
        "code_before_change": "static int pppol2tp_connect(struct socket *sock, struct sockaddr *uservaddr,\n\t\t\t    int sockaddr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppol2tp *sp = (struct sockaddr_pppol2tp *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct l2tp_session *session = NULL;\n\tstruct l2tp_tunnel *tunnel;\n\tstruct pppol2tp_session *ps;\n\tstruct l2tp_session_cfg cfg = { 0, };\n\tint error = 0;\n\tu32 tunnel_id, peer_tunnel_id;\n\tu32 session_id, peer_session_id;\n\tbool drop_refcnt = false;\n\tbool drop_tunnel = false;\n\tint ver = 2;\n\tint fd;\n\n\tlock_sock(sk);\n\n\terror = -EINVAL;\n\tif (sp->sa_protocol != PX_PROTO_OL2TP)\n\t\tgoto end;\n\n\t/* Check for already bound sockets */\n\terror = -EBUSY;\n\tif (sk->sk_state & PPPOX_CONNECTED)\n\t\tgoto end;\n\n\t/* We don't supporting rebinding anyway */\n\terror = -EALREADY;\n\tif (sk->sk_user_data)\n\t\tgoto end; /* socket is already attached */\n\n\t/* Get params from socket address. Handle L2TPv2 and L2TPv3.\n\t * This is nasty because there are different sockaddr_pppol2tp\n\t * structs for L2TPv2, L2TPv3, over IPv4 and IPv6. We use\n\t * the sockaddr size to determine which structure the caller\n\t * is using.\n\t */\n\tpeer_tunnel_id = 0;\n\tif (sockaddr_len == sizeof(struct sockaddr_pppol2tp)) {\n\t\tfd = sp->pppol2tp.fd;\n\t\ttunnel_id = sp->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp->pppol2tp.d_tunnel;\n\t\tsession_id = sp->pppol2tp.s_session;\n\t\tpeer_session_id = sp->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3)) {\n\t\tstruct sockaddr_pppol2tpv3 *sp3 =\n\t\t\t(struct sockaddr_pppol2tpv3 *) sp;\n\t\tver = 3;\n\t\tfd = sp3->pppol2tp.fd;\n\t\ttunnel_id = sp3->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp3->pppol2tp.d_tunnel;\n\t\tsession_id = sp3->pppol2tp.s_session;\n\t\tpeer_session_id = sp3->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpin6)) {\n\t\tstruct sockaddr_pppol2tpin6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpin6 *) sp;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3in6)) {\n\t\tstruct sockaddr_pppol2tpv3in6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpv3in6 *) sp;\n\t\tver = 3;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else {\n\t\terror = -EINVAL;\n\t\tgoto end; /* bad socket address */\n\t}\n\n\t/* Don't bind if tunnel_id is 0 */\n\terror = -EINVAL;\n\tif (tunnel_id == 0)\n\t\tgoto end;\n\n\ttunnel = l2tp_tunnel_get(sock_net(sk), tunnel_id);\n\tif (tunnel)\n\t\tdrop_tunnel = true;\n\n\t/* Special case: create tunnel context if session_id and\n\t * peer_session_id is 0. Otherwise look up tunnel using supplied\n\t * tunnel id.\n\t */\n\tif ((session_id == 0) && (peer_session_id == 0)) {\n\t\tif (tunnel == NULL) {\n\t\t\tstruct l2tp_tunnel_cfg tcfg = {\n\t\t\t\t.encap = L2TP_ENCAPTYPE_UDP,\n\t\t\t\t.debug = 0,\n\t\t\t};\n\t\t\terror = l2tp_tunnel_create(sock_net(sk), fd, ver, tunnel_id, peer_tunnel_id, &tcfg, &tunnel);\n\t\t\tif (error < 0)\n\t\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Error if we can't find the tunnel */\n\t\terror = -ENOENT;\n\t\tif (tunnel == NULL)\n\t\t\tgoto end;\n\n\t\t/* Error if socket is not prepped */\n\t\tif (tunnel->sock == NULL)\n\t\t\tgoto end;\n\t}\n\n\tif (tunnel->recv_payload_hook == NULL)\n\t\ttunnel->recv_payload_hook = pppol2tp_recv_payload_hook;\n\n\tif (tunnel->peer_tunnel_id == 0)\n\t\ttunnel->peer_tunnel_id = peer_tunnel_id;\n\n\tsession = l2tp_session_get(sock_net(sk), tunnel, session_id);\n\tif (session) {\n\t\tdrop_refcnt = true;\n\t\tps = l2tp_session_priv(session);\n\n\t\t/* Using a pre-existing session is fine as long as it hasn't\n\t\t * been connected yet.\n\t\t */\n\t\tmutex_lock(&ps->sk_lock);\n\t\tif (rcu_dereference_protected(ps->sk,\n\t\t\t\t\t      lockdep_is_held(&ps->sk_lock))) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\terror = -EEXIST;\n\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Default MTU must allow space for UDP/L2TP/PPP headers */\n\t\tcfg.mtu = 1500 - PPPOL2TP_HEADER_OVERHEAD;\n\t\tcfg.mru = cfg.mtu;\n\n\t\tsession = l2tp_session_create(sizeof(struct pppol2tp_session),\n\t\t\t\t\t      tunnel, session_id,\n\t\t\t\t\t      peer_session_id, &cfg);\n\t\tif (IS_ERR(session)) {\n\t\t\terror = PTR_ERR(session);\n\t\t\tgoto end;\n\t\t}\n\n\t\tpppol2tp_session_init(session);\n\t\tps = l2tp_session_priv(session);\n\t\tl2tp_session_inc_refcount(session);\n\n\t\tmutex_lock(&ps->sk_lock);\n\t\terror = l2tp_session_register(session, tunnel);\n\t\tif (error < 0) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\tkfree(session);\n\t\t\tgoto end;\n\t\t}\n\t\tdrop_refcnt = true;\n\t}\n\n\t/* Special case: if source & dest session_id == 0x0000, this\n\t * socket is being created to manage the tunnel. Just set up\n\t * the internal context for use by ioctl() and sockopt()\n\t * handlers.\n\t */\n\tif ((session->session_id == 0) &&\n\t    (session->peer_session_id == 0)) {\n\t\terror = 0;\n\t\tgoto out_no_ppp;\n\t}\n\n\t/* The only header we need to worry about is the L2TP\n\t * header. This size is different depending on whether\n\t * sequence numbers are enabled for the data channel.\n\t */\n\tpo->chan.hdrlen = PPPOL2TP_L2TP_HDR_SIZE_NOSEQ;\n\n\tpo->chan.private = sk;\n\tpo->chan.ops\t = &pppol2tp_chan_ops;\n\tpo->chan.mtu\t = session->mtu;\n\n\terror = ppp_register_net_channel(sock_net(sk), &po->chan);\n\tif (error) {\n\t\tmutex_unlock(&ps->sk_lock);\n\t\tgoto end;\n\t}\n\nout_no_ppp:\n\t/* This is how we get the session context from the socket. */\n\tsk->sk_user_data = session;\n\trcu_assign_pointer(ps->sk, sk);\n\tmutex_unlock(&ps->sk_lock);\n\n\t/* Keep the reference we've grabbed on the session: sk doesn't expect\n\t * the session to disappear. pppol2tp_session_destruct() is responsible\n\t * for dropping it.\n\t */\n\tdrop_refcnt = false;\n\n\tsk->sk_state = PPPOX_CONNECTED;\n\tl2tp_info(session, L2TP_MSG_CONTROL, \"%s: created\\n\",\n\t\t  session->name);\n\nend:\n\tif (drop_refcnt)\n\t\tl2tp_session_dec_refcount(session);\n\tif (drop_tunnel)\n\t\tl2tp_tunnel_dec_refcount(tunnel);\n\trelease_sock(sk);\n\n\treturn error;\n}",
        "code_after_change": "static int pppol2tp_connect(struct socket *sock, struct sockaddr *uservaddr,\n\t\t\t    int sockaddr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppol2tp *sp = (struct sockaddr_pppol2tp *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct l2tp_session *session = NULL;\n\tstruct l2tp_tunnel *tunnel;\n\tstruct pppol2tp_session *ps;\n\tstruct l2tp_session_cfg cfg = { 0, };\n\tint error = 0;\n\tu32 tunnel_id, peer_tunnel_id;\n\tu32 session_id, peer_session_id;\n\tbool drop_refcnt = false;\n\tbool drop_tunnel = false;\n\tint ver = 2;\n\tint fd;\n\n\tlock_sock(sk);\n\n\terror = -EINVAL;\n\tif (sp->sa_protocol != PX_PROTO_OL2TP)\n\t\tgoto end;\n\n\t/* Check for already bound sockets */\n\terror = -EBUSY;\n\tif (sk->sk_state & PPPOX_CONNECTED)\n\t\tgoto end;\n\n\t/* We don't supporting rebinding anyway */\n\terror = -EALREADY;\n\tif (sk->sk_user_data)\n\t\tgoto end; /* socket is already attached */\n\n\t/* Get params from socket address. Handle L2TPv2 and L2TPv3.\n\t * This is nasty because there are different sockaddr_pppol2tp\n\t * structs for L2TPv2, L2TPv3, over IPv4 and IPv6. We use\n\t * the sockaddr size to determine which structure the caller\n\t * is using.\n\t */\n\tpeer_tunnel_id = 0;\n\tif (sockaddr_len == sizeof(struct sockaddr_pppol2tp)) {\n\t\tfd = sp->pppol2tp.fd;\n\t\ttunnel_id = sp->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp->pppol2tp.d_tunnel;\n\t\tsession_id = sp->pppol2tp.s_session;\n\t\tpeer_session_id = sp->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3)) {\n\t\tstruct sockaddr_pppol2tpv3 *sp3 =\n\t\t\t(struct sockaddr_pppol2tpv3 *) sp;\n\t\tver = 3;\n\t\tfd = sp3->pppol2tp.fd;\n\t\ttunnel_id = sp3->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp3->pppol2tp.d_tunnel;\n\t\tsession_id = sp3->pppol2tp.s_session;\n\t\tpeer_session_id = sp3->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpin6)) {\n\t\tstruct sockaddr_pppol2tpin6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpin6 *) sp;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3in6)) {\n\t\tstruct sockaddr_pppol2tpv3in6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpv3in6 *) sp;\n\t\tver = 3;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else {\n\t\terror = -EINVAL;\n\t\tgoto end; /* bad socket address */\n\t}\n\n\t/* Don't bind if tunnel_id is 0 */\n\terror = -EINVAL;\n\tif (tunnel_id == 0)\n\t\tgoto end;\n\n\ttunnel = l2tp_tunnel_get(sock_net(sk), tunnel_id);\n\tif (tunnel)\n\t\tdrop_tunnel = true;\n\n\t/* Special case: create tunnel context if session_id and\n\t * peer_session_id is 0. Otherwise look up tunnel using supplied\n\t * tunnel id.\n\t */\n\tif ((session_id == 0) && (peer_session_id == 0)) {\n\t\tif (tunnel == NULL) {\n\t\t\tstruct l2tp_tunnel_cfg tcfg = {\n\t\t\t\t.encap = L2TP_ENCAPTYPE_UDP,\n\t\t\t\t.debug = 0,\n\t\t\t};\n\t\t\terror = l2tp_tunnel_create(sock_net(sk), fd, ver, tunnel_id, peer_tunnel_id, &tcfg, &tunnel);\n\t\t\tif (error < 0)\n\t\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Error if we can't find the tunnel */\n\t\terror = -ENOENT;\n\t\tif (tunnel == NULL)\n\t\t\tgoto end;\n\n\t\t/* Error if socket is not prepped */\n\t\tif (tunnel->sock == NULL)\n\t\t\tgoto end;\n\t}\n\n\tif (tunnel->recv_payload_hook == NULL)\n\t\ttunnel->recv_payload_hook = pppol2tp_recv_payload_hook;\n\n\tif (tunnel->peer_tunnel_id == 0)\n\t\ttunnel->peer_tunnel_id = peer_tunnel_id;\n\n\tsession = l2tp_session_get(sock_net(sk), tunnel, session_id);\n\tif (session) {\n\t\tdrop_refcnt = true;\n\t\tps = l2tp_session_priv(session);\n\n\t\t/* Using a pre-existing session is fine as long as it hasn't\n\t\t * been connected yet.\n\t\t */\n\t\tmutex_lock(&ps->sk_lock);\n\t\tif (rcu_dereference_protected(ps->sk,\n\t\t\t\t\t      lockdep_is_held(&ps->sk_lock))) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\terror = -EEXIST;\n\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Default MTU must allow space for UDP/L2TP/PPP headers */\n\t\tcfg.mtu = 1500 - PPPOL2TP_HEADER_OVERHEAD;\n\t\tcfg.mru = cfg.mtu;\n\n\t\tsession = l2tp_session_create(sizeof(struct pppol2tp_session),\n\t\t\t\t\t      tunnel, session_id,\n\t\t\t\t\t      peer_session_id, &cfg);\n\t\tif (IS_ERR(session)) {\n\t\t\terror = PTR_ERR(session);\n\t\t\tgoto end;\n\t\t}\n\n\t\tpppol2tp_session_init(session);\n\t\tps = l2tp_session_priv(session);\n\t\tl2tp_session_inc_refcount(session);\n\n\t\tmutex_lock(&ps->sk_lock);\n\t\terror = l2tp_session_register(session, tunnel);\n\t\tif (error < 0) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\tkfree(session);\n\t\t\tgoto end;\n\t\t}\n\t\tdrop_refcnt = true;\n\t}\n\n\t/* Special case: if source & dest session_id == 0x0000, this\n\t * socket is being created to manage the tunnel. Just set up\n\t * the internal context for use by ioctl() and sockopt()\n\t * handlers.\n\t */\n\tif ((session->session_id == 0) &&\n\t    (session->peer_session_id == 0)) {\n\t\terror = 0;\n\t\tgoto out_no_ppp;\n\t}\n\n\t/* The only header we need to worry about is the L2TP\n\t * header. This size is different depending on whether\n\t * sequence numbers are enabled for the data channel.\n\t */\n\tpo->chan.hdrlen = PPPOL2TP_L2TP_HDR_SIZE_NOSEQ;\n\n\tpo->chan.private = sk;\n\tpo->chan.ops\t = &pppol2tp_chan_ops;\n\tpo->chan.mtu\t = session->mtu;\n\n\terror = ppp_register_net_channel(sock_net(sk), &po->chan);\n\tif (error) {\n\t\tmutex_unlock(&ps->sk_lock);\n\t\tgoto end;\n\t}\n\nout_no_ppp:\n\t/* This is how we get the session context from the socket. */\n\tsock_hold(sk);\n\tsk->sk_user_data = session;\n\trcu_assign_pointer(ps->sk, sk);\n\tmutex_unlock(&ps->sk_lock);\n\n\t/* Keep the reference we've grabbed on the session: sk doesn't expect\n\t * the session to disappear. pppol2tp_session_destruct() is responsible\n\t * for dropping it.\n\t */\n\tdrop_refcnt = false;\n\n\tsk->sk_state = PPPOX_CONNECTED;\n\tl2tp_info(session, L2TP_MSG_CONTROL, \"%s: created\\n\",\n\t\t  session->name);\n\nend:\n\tif (drop_refcnt)\n\t\tl2tp_session_dec_refcount(session);\n\tif (drop_tunnel)\n\t\tl2tp_tunnel_dec_refcount(tunnel);\n\trelease_sock(sk);\n\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -187,6 +187,7 @@\n \n out_no_ppp:\n \t/* This is how we get the session context from the socket. */\n+\tsock_hold(sk);\n \tsk->sk_user_data = session;\n \trcu_assign_pointer(ps->sk, sk);\n \tmutex_unlock(&ps->sk_lock);",
        "function_modified_lines": {
            "added": [
                "\tsock_hold(sk);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In pppol2tp_create of l2tp_ppp.c, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-186777253References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2022-20567",
        "code_before_change": "static int pppol2tp_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct l2tp_session *session;\n\tint error;\n\n\tif (!sk)\n\t\treturn 0;\n\n\terror = -EBADF;\n\tlock_sock(sk);\n\tif (sock_flag(sk, SOCK_DEAD) != 0)\n\t\tgoto error;\n\n\tpppox_unbind_sock(sk);\n\n\t/* Signal the death of the socket. */\n\tsk->sk_state = PPPOX_DEAD;\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\tsession = pppol2tp_sock_to_session(sk);\n\n\tif (session != NULL) {\n\t\tstruct pppol2tp_session *ps;\n\n\t\tl2tp_session_delete(session);\n\n\t\tps = l2tp_session_priv(session);\n\t\tmutex_lock(&ps->sk_lock);\n\t\tps->__sk = rcu_dereference_protected(ps->sk,\n\t\t\t\t\t\t     lockdep_is_held(&ps->sk_lock));\n\t\tRCU_INIT_POINTER(ps->sk, NULL);\n\t\tmutex_unlock(&ps->sk_lock);\n\t\tcall_rcu(&ps->rcu, pppol2tp_put_sk);\n\n\t\t/* Rely on the sock_put() call at the end of the function for\n\t\t * dropping the reference held by pppol2tp_sock_to_session().\n\t\t * The last reference will be dropped by pppol2tp_put_sk().\n\t\t */\n\t}\n\trelease_sock(sk);\n\n\t/* This will delete the session context via\n\t * pppol2tp_session_destruct() if the socket's refcnt drops to\n\t * zero.\n\t */\n\tsock_put(sk);\n\n\treturn 0;\n\nerror:\n\trelease_sock(sk);\n\treturn error;\n}",
        "code_after_change": "static int pppol2tp_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct l2tp_session *session;\n\tint error;\n\n\tif (!sk)\n\t\treturn 0;\n\n\terror = -EBADF;\n\tlock_sock(sk);\n\tif (sock_flag(sk, SOCK_DEAD) != 0)\n\t\tgoto error;\n\n\tpppox_unbind_sock(sk);\n\n\t/* Signal the death of the socket. */\n\tsk->sk_state = PPPOX_DEAD;\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\t/* If the socket is associated with a session,\n\t * l2tp_session_delete will call pppol2tp_session_close which\n\t * will drop the session's ref on the socket.\n\t */\n\tsession = pppol2tp_sock_to_session(sk);\n\tif (session) {\n\t\tl2tp_session_delete(session);\n\t\t/* drop the ref obtained by pppol2tp_sock_to_session */\n\t\tsock_put(sk);\n\t}\n\n\trelease_sock(sk);\n\n\t/* This will delete the session context via\n\t * pppol2tp_session_destruct() if the socket's refcnt drops to\n\t * zero.\n\t */\n\tsock_put(sk);\n\n\treturn 0;\n\nerror:\n\trelease_sock(sk);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,26 +19,17 @@\n \tsock_orphan(sk);\n \tsock->sk = NULL;\n \n+\t/* If the socket is associated with a session,\n+\t * l2tp_session_delete will call pppol2tp_session_close which\n+\t * will drop the session's ref on the socket.\n+\t */\n \tsession = pppol2tp_sock_to_session(sk);\n+\tif (session) {\n+\t\tl2tp_session_delete(session);\n+\t\t/* drop the ref obtained by pppol2tp_sock_to_session */\n+\t\tsock_put(sk);\n+\t}\n \n-\tif (session != NULL) {\n-\t\tstruct pppol2tp_session *ps;\n-\n-\t\tl2tp_session_delete(session);\n-\n-\t\tps = l2tp_session_priv(session);\n-\t\tmutex_lock(&ps->sk_lock);\n-\t\tps->__sk = rcu_dereference_protected(ps->sk,\n-\t\t\t\t\t\t     lockdep_is_held(&ps->sk_lock));\n-\t\tRCU_INIT_POINTER(ps->sk, NULL);\n-\t\tmutex_unlock(&ps->sk_lock);\n-\t\tcall_rcu(&ps->rcu, pppol2tp_put_sk);\n-\n-\t\t/* Rely on the sock_put() call at the end of the function for\n-\t\t * dropping the reference held by pppol2tp_sock_to_session().\n-\t\t * The last reference will be dropped by pppol2tp_put_sk().\n-\t\t */\n-\t}\n \trelease_sock(sk);\n \n \t/* This will delete the session context via",
        "function_modified_lines": {
            "added": [
                "\t/* If the socket is associated with a session,",
                "\t * l2tp_session_delete will call pppol2tp_session_close which",
                "\t * will drop the session's ref on the socket.",
                "\t */",
                "\tif (session) {",
                "\t\tl2tp_session_delete(session);",
                "\t\t/* drop the ref obtained by pppol2tp_sock_to_session */",
                "\t\tsock_put(sk);",
                "\t}"
            ],
            "deleted": [
                "\tif (session != NULL) {",
                "\t\tstruct pppol2tp_session *ps;",
                "",
                "\t\tl2tp_session_delete(session);",
                "",
                "\t\tps = l2tp_session_priv(session);",
                "\t\tmutex_lock(&ps->sk_lock);",
                "\t\tps->__sk = rcu_dereference_protected(ps->sk,",
                "\t\t\t\t\t\t     lockdep_is_held(&ps->sk_lock));",
                "\t\tRCU_INIT_POINTER(ps->sk, NULL);",
                "\t\tmutex_unlock(&ps->sk_lock);",
                "\t\tcall_rcu(&ps->rcu, pppol2tp_put_sk);",
                "",
                "\t\t/* Rely on the sock_put() call at the end of the function for",
                "\t\t * dropping the reference held by pppol2tp_sock_to_session().",
                "\t\t * The last reference will be dropped by pppol2tp_put_sk().",
                "\t\t */",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In pppol2tp_create of l2tp_ppp.c, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-186777253References: Upstream kernel"
    },
    {
        "cve_id": "CVE-2022-23036",
        "code_before_change": "void gnttab_end_foreign_access(grant_ref_t ref, int readonly,\n\t\t\t       unsigned long page)\n{\n\tif (gnttab_end_foreign_access_ref(ref, readonly)) {\n\t\tput_free_entry(ref);\n\t\tif (page != 0)\n\t\t\tput_page(virt_to_page(page));\n\t} else\n\t\tgnttab_add_deferred(ref, readonly,\n\t\t\t\t    page ? virt_to_page(page) : NULL);\n}",
        "code_after_change": "void gnttab_end_foreign_access(grant_ref_t ref, int readonly,\n\t\t\t       unsigned long page)\n{\n\tif (gnttab_try_end_foreign_access(ref)) {\n\t\tif (page != 0)\n\t\t\tput_page(virt_to_page(page));\n\t} else\n\t\tgnttab_add_deferred(ref, readonly,\n\t\t\t\t    page ? virt_to_page(page) : NULL);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,7 @@\n void gnttab_end_foreign_access(grant_ref_t ref, int readonly,\n \t\t\t       unsigned long page)\n {\n-\tif (gnttab_end_foreign_access_ref(ref, readonly)) {\n-\t\tput_free_entry(ref);\n+\tif (gnttab_try_end_foreign_access(ref)) {\n \t\tif (page != 0)\n \t\t\tput_page(virt_to_page(page));\n \t} else",
        "function_modified_lines": {
            "added": [
                "\tif (gnttab_try_end_foreign_access(ref)) {"
            ],
            "deleted": [
                "\tif (gnttab_end_foreign_access_ref(ref, readonly)) {",
                "\t\tput_free_entry(ref);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux PV device frontends vulnerable to attacks by backends T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Several Linux PV device frontends are using the grant table interfaces for removing access rights of the backends in ways being subject to race conditions, resulting in potential data leaks, data corruption by malicious backends, and denial of service triggered by malicious backends: blkfront, netfront, scsifront and the gntalloc driver are testing whether a grant reference is still in use. If this is not the case, they assume that a following removal of the granted access will always succeed, which is not true in case the backend has mapped the granted page between those two operations. As a result the backend can keep access to the memory page of the guest no matter how the page will be used after the frontend I/O has finished. The xenbus driver has a similar problem, as it doesn't check the success of removing the granted access of a shared ring buffer. blkfront: CVE-2022-23036 netfront: CVE-2022-23037 scsifront: CVE-2022-23038 gntalloc: CVE-2022-23039 xenbus: CVE-2022-23040 blkfront, netfront, scsifront, usbfront, dmabuf, xenbus, 9p, kbdfront, and pvcalls are using a functionality to delay freeing a grant reference until it is no longer in use, but the freeing of the related data page is not synchronized with dropping the granted access. As a result the backend can keep access to the memory page even after it has been freed and then re-used for a different purpose. CVE-2022-23041 netfront will fail a BUG_ON() assertion if it fails to revoke access in the rx path. This will result in a Denial of Service (DoS) situation of the guest which can be triggered by the backend. CVE-2022-23042"
    },
    {
        "cve_id": "CVE-2022-23037",
        "code_before_change": "static bool xennet_tx_buf_gc(struct netfront_queue *queue)\n{\n\tRING_IDX cons, prod;\n\tunsigned short id;\n\tstruct sk_buff *skb;\n\tbool more_to_do;\n\tbool work_done = false;\n\tconst struct device *dev = &queue->info->netdev->dev;\n\n\tBUG_ON(!netif_carrier_ok(queue->info->netdev));\n\n\tdo {\n\t\tprod = queue->tx.sring->rsp_prod;\n\t\tif (RING_RESPONSE_PROD_OVERFLOW(&queue->tx, prod)) {\n\t\t\tdev_alert(dev, \"Illegal number of responses %u\\n\",\n\t\t\t\t  prod - queue->tx.rsp_cons);\n\t\t\tgoto err;\n\t\t}\n\t\trmb(); /* Ensure we see responses up to 'rp'. */\n\n\t\tfor (cons = queue->tx.rsp_cons; cons != prod; cons++) {\n\t\t\tstruct xen_netif_tx_response txrsp;\n\n\t\t\twork_done = true;\n\n\t\t\tRING_COPY_RESPONSE(&queue->tx, cons, &txrsp);\n\t\t\tif (txrsp.status == XEN_NETIF_RSP_NULL)\n\t\t\t\tcontinue;\n\n\t\t\tid = txrsp.id;\n\t\t\tif (id >= RING_SIZE(&queue->tx)) {\n\t\t\t\tdev_alert(dev,\n\t\t\t\t\t  \"Response has incorrect id (%u)\\n\",\n\t\t\t\t\t  id);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif (queue->tx_link[id] != TX_PENDING) {\n\t\t\t\tdev_alert(dev,\n\t\t\t\t\t  \"Response for inactive request\\n\");\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tqueue->tx_link[id] = TX_LINK_NONE;\n\t\t\tskb = queue->tx_skbs[id];\n\t\t\tqueue->tx_skbs[id] = NULL;\n\t\t\tif (unlikely(gnttab_query_foreign_access(\n\t\t\t\tqueue->grant_tx_ref[id]) != 0)) {\n\t\t\t\tdev_alert(dev,\n\t\t\t\t\t  \"Grant still in use by backend domain\\n\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tgnttab_end_foreign_access_ref(\n\t\t\t\tqueue->grant_tx_ref[id], GNTMAP_readonly);\n\t\t\tgnttab_release_grant_reference(\n\t\t\t\t&queue->gref_tx_head, queue->grant_tx_ref[id]);\n\t\t\tqueue->grant_tx_ref[id] = GRANT_INVALID_REF;\n\t\t\tqueue->grant_tx_page[id] = NULL;\n\t\t\tadd_id_to_list(&queue->tx_skb_freelist, queue->tx_link, id);\n\t\t\tdev_kfree_skb_irq(skb);\n\t\t}\n\n\t\tqueue->tx.rsp_cons = prod;\n\n\t\tRING_FINAL_CHECK_FOR_RESPONSES(&queue->tx, more_to_do);\n\t} while (more_to_do);\n\n\txennet_maybe_wake_tx(queue);\n\n\treturn work_done;\n\n err:\n\tqueue->info->broken = true;\n\tdev_alert(dev, \"Disabled for further use\\n\");\n\n\treturn work_done;\n}",
        "code_after_change": "static bool xennet_tx_buf_gc(struct netfront_queue *queue)\n{\n\tRING_IDX cons, prod;\n\tunsigned short id;\n\tstruct sk_buff *skb;\n\tbool more_to_do;\n\tbool work_done = false;\n\tconst struct device *dev = &queue->info->netdev->dev;\n\n\tBUG_ON(!netif_carrier_ok(queue->info->netdev));\n\n\tdo {\n\t\tprod = queue->tx.sring->rsp_prod;\n\t\tif (RING_RESPONSE_PROD_OVERFLOW(&queue->tx, prod)) {\n\t\t\tdev_alert(dev, \"Illegal number of responses %u\\n\",\n\t\t\t\t  prod - queue->tx.rsp_cons);\n\t\t\tgoto err;\n\t\t}\n\t\trmb(); /* Ensure we see responses up to 'rp'. */\n\n\t\tfor (cons = queue->tx.rsp_cons; cons != prod; cons++) {\n\t\t\tstruct xen_netif_tx_response txrsp;\n\n\t\t\twork_done = true;\n\n\t\t\tRING_COPY_RESPONSE(&queue->tx, cons, &txrsp);\n\t\t\tif (txrsp.status == XEN_NETIF_RSP_NULL)\n\t\t\t\tcontinue;\n\n\t\t\tid = txrsp.id;\n\t\t\tif (id >= RING_SIZE(&queue->tx)) {\n\t\t\t\tdev_alert(dev,\n\t\t\t\t\t  \"Response has incorrect id (%u)\\n\",\n\t\t\t\t\t  id);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif (queue->tx_link[id] != TX_PENDING) {\n\t\t\t\tdev_alert(dev,\n\t\t\t\t\t  \"Response for inactive request\\n\");\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tqueue->tx_link[id] = TX_LINK_NONE;\n\t\t\tskb = queue->tx_skbs[id];\n\t\t\tqueue->tx_skbs[id] = NULL;\n\t\t\tif (unlikely(!gnttab_end_foreign_access_ref(\n\t\t\t\tqueue->grant_tx_ref[id], GNTMAP_readonly))) {\n\t\t\t\tdev_alert(dev,\n\t\t\t\t\t  \"Grant still in use by backend domain\\n\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tgnttab_release_grant_reference(\n\t\t\t\t&queue->gref_tx_head, queue->grant_tx_ref[id]);\n\t\t\tqueue->grant_tx_ref[id] = GRANT_INVALID_REF;\n\t\t\tqueue->grant_tx_page[id] = NULL;\n\t\t\tadd_id_to_list(&queue->tx_skb_freelist, queue->tx_link, id);\n\t\t\tdev_kfree_skb_irq(skb);\n\t\t}\n\n\t\tqueue->tx.rsp_cons = prod;\n\n\t\tRING_FINAL_CHECK_FOR_RESPONSES(&queue->tx, more_to_do);\n\t} while (more_to_do);\n\n\txennet_maybe_wake_tx(queue);\n\n\treturn work_done;\n\n err:\n\tqueue->info->broken = true;\n\tdev_alert(dev, \"Disabled for further use\\n\");\n\n\treturn work_done;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,14 +43,12 @@\n \t\t\tqueue->tx_link[id] = TX_LINK_NONE;\n \t\t\tskb = queue->tx_skbs[id];\n \t\t\tqueue->tx_skbs[id] = NULL;\n-\t\t\tif (unlikely(gnttab_query_foreign_access(\n-\t\t\t\tqueue->grant_tx_ref[id]) != 0)) {\n+\t\t\tif (unlikely(!gnttab_end_foreign_access_ref(\n+\t\t\t\tqueue->grant_tx_ref[id], GNTMAP_readonly))) {\n \t\t\t\tdev_alert(dev,\n \t\t\t\t\t  \"Grant still in use by backend domain\\n\");\n \t\t\t\tgoto err;\n \t\t\t}\n-\t\t\tgnttab_end_foreign_access_ref(\n-\t\t\t\tqueue->grant_tx_ref[id], GNTMAP_readonly);\n \t\t\tgnttab_release_grant_reference(\n \t\t\t\t&queue->gref_tx_head, queue->grant_tx_ref[id]);\n \t\t\tqueue->grant_tx_ref[id] = GRANT_INVALID_REF;",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (unlikely(!gnttab_end_foreign_access_ref(",
                "\t\t\t\tqueue->grant_tx_ref[id], GNTMAP_readonly))) {"
            ],
            "deleted": [
                "\t\t\tif (unlikely(gnttab_query_foreign_access(",
                "\t\t\t\tqueue->grant_tx_ref[id]) != 0)) {",
                "\t\t\tgnttab_end_foreign_access_ref(",
                "\t\t\t\tqueue->grant_tx_ref[id], GNTMAP_readonly);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux PV device frontends vulnerable to attacks by backends T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Several Linux PV device frontends are using the grant table interfaces for removing access rights of the backends in ways being subject to race conditions, resulting in potential data leaks, data corruption by malicious backends, and denial of service triggered by malicious backends: blkfront, netfront, scsifront and the gntalloc driver are testing whether a grant reference is still in use. If this is not the case, they assume that a following removal of the granted access will always succeed, which is not true in case the backend has mapped the granted page between those two operations. As a result the backend can keep access to the memory page of the guest no matter how the page will be used after the frontend I/O has finished. The xenbus driver has a similar problem, as it doesn't check the success of removing the granted access of a shared ring buffer. blkfront: CVE-2022-23036 netfront: CVE-2022-23037 scsifront: CVE-2022-23038 gntalloc: CVE-2022-23039 xenbus: CVE-2022-23040 blkfront, netfront, scsifront, usbfront, dmabuf, xenbus, 9p, kbdfront, and pvcalls are using a functionality to delay freeing a grant reference until it is no longer in use, but the freeing of the related data page is not synchronized with dropping the granted access. As a result the backend can keep access to the memory page even after it has been freed and then re-used for a different purpose. CVE-2022-23041 netfront will fail a BUG_ON() assertion if it fails to revoke access in the rx path. This will result in a Denial of Service (DoS) situation of the guest which can be triggered by the backend. CVE-2022-23042"
    },
    {
        "cve_id": "CVE-2022-23038",
        "code_before_change": "void gnttab_end_foreign_access(grant_ref_t ref, int readonly,\n\t\t\t       unsigned long page)\n{\n\tif (gnttab_end_foreign_access_ref(ref, readonly)) {\n\t\tput_free_entry(ref);\n\t\tif (page != 0)\n\t\t\tput_page(virt_to_page(page));\n\t} else\n\t\tgnttab_add_deferred(ref, readonly,\n\t\t\t\t    page ? virt_to_page(page) : NULL);\n}",
        "code_after_change": "void gnttab_end_foreign_access(grant_ref_t ref, int readonly,\n\t\t\t       unsigned long page)\n{\n\tif (gnttab_try_end_foreign_access(ref)) {\n\t\tif (page != 0)\n\t\t\tput_page(virt_to_page(page));\n\t} else\n\t\tgnttab_add_deferred(ref, readonly,\n\t\t\t\t    page ? virt_to_page(page) : NULL);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,7 @@\n void gnttab_end_foreign_access(grant_ref_t ref, int readonly,\n \t\t\t       unsigned long page)\n {\n-\tif (gnttab_end_foreign_access_ref(ref, readonly)) {\n-\t\tput_free_entry(ref);\n+\tif (gnttab_try_end_foreign_access(ref)) {\n \t\tif (page != 0)\n \t\t\tput_page(virt_to_page(page));\n \t} else",
        "function_modified_lines": {
            "added": [
                "\tif (gnttab_try_end_foreign_access(ref)) {"
            ],
            "deleted": [
                "\tif (gnttab_end_foreign_access_ref(ref, readonly)) {",
                "\t\tput_free_entry(ref);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux PV device frontends vulnerable to attacks by backends T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Several Linux PV device frontends are using the grant table interfaces for removing access rights of the backends in ways being subject to race conditions, resulting in potential data leaks, data corruption by malicious backends, and denial of service triggered by malicious backends: blkfront, netfront, scsifront and the gntalloc driver are testing whether a grant reference is still in use. If this is not the case, they assume that a following removal of the granted access will always succeed, which is not true in case the backend has mapped the granted page between those two operations. As a result the backend can keep access to the memory page of the guest no matter how the page will be used after the frontend I/O has finished. The xenbus driver has a similar problem, as it doesn't check the success of removing the granted access of a shared ring buffer. blkfront: CVE-2022-23036 netfront: CVE-2022-23037 scsifront: CVE-2022-23038 gntalloc: CVE-2022-23039 xenbus: CVE-2022-23040 blkfront, netfront, scsifront, usbfront, dmabuf, xenbus, 9p, kbdfront, and pvcalls are using a functionality to delay freeing a grant reference until it is no longer in use, but the freeing of the related data page is not synchronized with dropping the granted access. As a result the backend can keep access to the memory page even after it has been freed and then re-used for a different purpose. CVE-2022-23041 netfront will fail a BUG_ON() assertion if it fails to revoke access in the rx path. This will result in a Denial of Service (DoS) situation of the guest which can be triggered by the backend. CVE-2022-23042"
    },
    {
        "cve_id": "CVE-2022-23039",
        "code_before_change": "static int add_grefs(struct ioctl_gntalloc_alloc_gref *op,\n\tuint32_t *gref_ids, struct gntalloc_file_private_data *priv)\n{\n\tint i, rc, readonly;\n\tLIST_HEAD(queue_gref);\n\tLIST_HEAD(queue_file);\n\tstruct gntalloc_gref *gref, *next;\n\n\treadonly = !(op->flags & GNTALLOC_FLAG_WRITABLE);\n\tfor (i = 0; i < op->count; i++) {\n\t\tgref = kzalloc(sizeof(*gref), GFP_KERNEL);\n\t\tif (!gref) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto undo;\n\t\t}\n\t\tlist_add_tail(&gref->next_gref, &queue_gref);\n\t\tlist_add_tail(&gref->next_file, &queue_file);\n\t\tgref->users = 1;\n\t\tgref->file_index = op->index + i * PAGE_SIZE;\n\t\tgref->page = alloc_page(GFP_KERNEL|__GFP_ZERO);\n\t\tif (!gref->page) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto undo;\n\t\t}\n\n\t\t/* Grant foreign access to the page. */\n\t\trc = gnttab_grant_foreign_access(op->domid,\n\t\t\t\t\t\t xen_page_to_gfn(gref->page),\n\t\t\t\t\t\t readonly);\n\t\tif (rc < 0)\n\t\t\tgoto undo;\n\t\tgref_ids[i] = gref->gref_id = rc;\n\t}\n\n\t/* Add to gref lists. */\n\tmutex_lock(&gref_mutex);\n\tlist_splice_tail(&queue_gref, &gref_list);\n\tlist_splice_tail(&queue_file, &priv->list);\n\tmutex_unlock(&gref_mutex);\n\n\treturn 0;\n\nundo:\n\tmutex_lock(&gref_mutex);\n\tgref_size -= (op->count - i);\n\n\tlist_for_each_entry_safe(gref, next, &queue_file, next_file) {\n\t\tlist_del(&gref->next_file);\n\t\t__del_gref(gref);\n\t}\n\n\t/* It's possible for the target domain to map the just-allocated grant\n\t * references by blindly guessing their IDs; if this is done, then\n\t * __del_gref will leave them in the queue_gref list. They need to be\n\t * added to the global list so that we can free them when they are no\n\t * longer referenced.\n\t */\n\tif (unlikely(!list_empty(&queue_gref)))\n\t\tlist_splice_tail(&queue_gref, &gref_list);\n\tmutex_unlock(&gref_mutex);\n\treturn rc;\n}",
        "code_after_change": "static int add_grefs(struct ioctl_gntalloc_alloc_gref *op,\n\tuint32_t *gref_ids, struct gntalloc_file_private_data *priv)\n{\n\tint i, rc, readonly;\n\tLIST_HEAD(queue_gref);\n\tLIST_HEAD(queue_file);\n\tstruct gntalloc_gref *gref, *next;\n\n\treadonly = !(op->flags & GNTALLOC_FLAG_WRITABLE);\n\tfor (i = 0; i < op->count; i++) {\n\t\tgref = kzalloc(sizeof(*gref), GFP_KERNEL);\n\t\tif (!gref) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto undo;\n\t\t}\n\t\tlist_add_tail(&gref->next_gref, &queue_gref);\n\t\tlist_add_tail(&gref->next_file, &queue_file);\n\t\tgref->users = 1;\n\t\tgref->file_index = op->index + i * PAGE_SIZE;\n\t\tgref->page = alloc_page(GFP_KERNEL|__GFP_ZERO);\n\t\tif (!gref->page) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto undo;\n\t\t}\n\n\t\t/* Grant foreign access to the page. */\n\t\trc = gnttab_grant_foreign_access(op->domid,\n\t\t\t\t\t\t xen_page_to_gfn(gref->page),\n\t\t\t\t\t\t readonly);\n\t\tif (rc < 0)\n\t\t\tgoto undo;\n\t\tgref_ids[i] = gref->gref_id = rc;\n\t}\n\n\t/* Add to gref lists. */\n\tmutex_lock(&gref_mutex);\n\tlist_splice_tail(&queue_gref, &gref_list);\n\tlist_splice_tail(&queue_file, &priv->list);\n\tmutex_unlock(&gref_mutex);\n\n\treturn 0;\n\nundo:\n\tmutex_lock(&gref_mutex);\n\tgref_size -= (op->count - i);\n\n\tlist_for_each_entry_safe(gref, next, &queue_file, next_file) {\n\t\tlist_del(&gref->next_file);\n\t\t__del_gref(gref);\n\t}\n\n\tmutex_unlock(&gref_mutex);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -49,14 +49,6 @@\n \t\t__del_gref(gref);\n \t}\n \n-\t/* It's possible for the target domain to map the just-allocated grant\n-\t * references by blindly guessing their IDs; if this is done, then\n-\t * __del_gref will leave them in the queue_gref list. They need to be\n-\t * added to the global list so that we can free them when they are no\n-\t * longer referenced.\n-\t */\n-\tif (unlikely(!list_empty(&queue_gref)))\n-\t\tlist_splice_tail(&queue_gref, &gref_list);\n \tmutex_unlock(&gref_mutex);\n \treturn rc;\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t/* It's possible for the target domain to map the just-allocated grant",
                "\t * references by blindly guessing their IDs; if this is done, then",
                "\t * __del_gref will leave them in the queue_gref list. They need to be",
                "\t * added to the global list so that we can free them when they are no",
                "\t * longer referenced.",
                "\t */",
                "\tif (unlikely(!list_empty(&queue_gref)))",
                "\t\tlist_splice_tail(&queue_gref, &gref_list);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux PV device frontends vulnerable to attacks by backends T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Several Linux PV device frontends are using the grant table interfaces for removing access rights of the backends in ways being subject to race conditions, resulting in potential data leaks, data corruption by malicious backends, and denial of service triggered by malicious backends: blkfront, netfront, scsifront and the gntalloc driver are testing whether a grant reference is still in use. If this is not the case, they assume that a following removal of the granted access will always succeed, which is not true in case the backend has mapped the granted page between those two operations. As a result the backend can keep access to the memory page of the guest no matter how the page will be used after the frontend I/O has finished. The xenbus driver has a similar problem, as it doesn't check the success of removing the granted access of a shared ring buffer. blkfront: CVE-2022-23036 netfront: CVE-2022-23037 scsifront: CVE-2022-23038 gntalloc: CVE-2022-23039 xenbus: CVE-2022-23040 blkfront, netfront, scsifront, usbfront, dmabuf, xenbus, 9p, kbdfront, and pvcalls are using a functionality to delay freeing a grant reference until it is no longer in use, but the freeing of the related data page is not synchronized with dropping the granted access. As a result the backend can keep access to the memory page even after it has been freed and then re-used for a different purpose. CVE-2022-23041 netfront will fail a BUG_ON() assertion if it fails to revoke access in the rx path. This will result in a Denial of Service (DoS) situation of the guest which can be triggered by the backend. CVE-2022-23042"
    },
    {
        "cve_id": "CVE-2022-23039",
        "code_before_change": "static void __del_gref(struct gntalloc_gref *gref)\n{\n\tif (gref->notify.flags & UNMAP_NOTIFY_CLEAR_BYTE) {\n\t\tuint8_t *tmp = kmap(gref->page);\n\t\ttmp[gref->notify.pgoff] = 0;\n\t\tkunmap(gref->page);\n\t}\n\tif (gref->notify.flags & UNMAP_NOTIFY_SEND_EVENT) {\n\t\tnotify_remote_via_evtchn(gref->notify.event);\n\t\tevtchn_put(gref->notify.event);\n\t}\n\n\tgref->notify.flags = 0;\n\n\tif (gref->gref_id) {\n\t\tif (gnttab_query_foreign_access(gref->gref_id))\n\t\t\treturn;\n\n\t\tif (!gnttab_end_foreign_access_ref(gref->gref_id, 0))\n\t\t\treturn;\n\n\t\tgnttab_free_grant_reference(gref->gref_id);\n\t}\n\n\tgref_size--;\n\tlist_del(&gref->next_gref);\n\n\tif (gref->page)\n\t\t__free_page(gref->page);\n\n\tkfree(gref);\n}",
        "code_after_change": "static void __del_gref(struct gntalloc_gref *gref)\n{\n\tunsigned long addr;\n\n\tif (gref->notify.flags & UNMAP_NOTIFY_CLEAR_BYTE) {\n\t\tuint8_t *tmp = kmap(gref->page);\n\t\ttmp[gref->notify.pgoff] = 0;\n\t\tkunmap(gref->page);\n\t}\n\tif (gref->notify.flags & UNMAP_NOTIFY_SEND_EVENT) {\n\t\tnotify_remote_via_evtchn(gref->notify.event);\n\t\tevtchn_put(gref->notify.event);\n\t}\n\n\tgref->notify.flags = 0;\n\n\tif (gref->gref_id) {\n\t\tif (gref->page) {\n\t\t\taddr = (unsigned long)page_to_virt(gref->page);\n\t\t\tgnttab_end_foreign_access(gref->gref_id, 0, addr);\n\t\t} else\n\t\t\tgnttab_free_grant_reference(gref->gref_id);\n\t}\n\n\tgref_size--;\n\tlist_del(&gref->next_gref);\n\n\tkfree(gref);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,7 @@\n static void __del_gref(struct gntalloc_gref *gref)\n {\n+\tunsigned long addr;\n+\n \tif (gref->notify.flags & UNMAP_NOTIFY_CLEAR_BYTE) {\n \t\tuint8_t *tmp = kmap(gref->page);\n \t\ttmp[gref->notify.pgoff] = 0;\n@@ -13,20 +15,15 @@\n \tgref->notify.flags = 0;\n \n \tif (gref->gref_id) {\n-\t\tif (gnttab_query_foreign_access(gref->gref_id))\n-\t\t\treturn;\n-\n-\t\tif (!gnttab_end_foreign_access_ref(gref->gref_id, 0))\n-\t\t\treturn;\n-\n-\t\tgnttab_free_grant_reference(gref->gref_id);\n+\t\tif (gref->page) {\n+\t\t\taddr = (unsigned long)page_to_virt(gref->page);\n+\t\t\tgnttab_end_foreign_access(gref->gref_id, 0, addr);\n+\t\t} else\n+\t\t\tgnttab_free_grant_reference(gref->gref_id);\n \t}\n \n \tgref_size--;\n \tlist_del(&gref->next_gref);\n \n-\tif (gref->page)\n-\t\t__free_page(gref->page);\n-\n \tkfree(gref);\n }",
        "function_modified_lines": {
            "added": [
                "\tunsigned long addr;",
                "",
                "\t\tif (gref->page) {",
                "\t\t\taddr = (unsigned long)page_to_virt(gref->page);",
                "\t\t\tgnttab_end_foreign_access(gref->gref_id, 0, addr);",
                "\t\t} else",
                "\t\t\tgnttab_free_grant_reference(gref->gref_id);"
            ],
            "deleted": [
                "\t\tif (gnttab_query_foreign_access(gref->gref_id))",
                "\t\t\treturn;",
                "",
                "\t\tif (!gnttab_end_foreign_access_ref(gref->gref_id, 0))",
                "\t\t\treturn;",
                "",
                "\t\tgnttab_free_grant_reference(gref->gref_id);",
                "\tif (gref->page)",
                "\t\t__free_page(gref->page);",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux PV device frontends vulnerable to attacks by backends T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Several Linux PV device frontends are using the grant table interfaces for removing access rights of the backends in ways being subject to race conditions, resulting in potential data leaks, data corruption by malicious backends, and denial of service triggered by malicious backends: blkfront, netfront, scsifront and the gntalloc driver are testing whether a grant reference is still in use. If this is not the case, they assume that a following removal of the granted access will always succeed, which is not true in case the backend has mapped the granted page between those two operations. As a result the backend can keep access to the memory page of the guest no matter how the page will be used after the frontend I/O has finished. The xenbus driver has a similar problem, as it doesn't check the success of removing the granted access of a shared ring buffer. blkfront: CVE-2022-23036 netfront: CVE-2022-23037 scsifront: CVE-2022-23038 gntalloc: CVE-2022-23039 xenbus: CVE-2022-23040 blkfront, netfront, scsifront, usbfront, dmabuf, xenbus, 9p, kbdfront, and pvcalls are using a functionality to delay freeing a grant reference until it is no longer in use, but the freeing of the related data page is not synchronized with dropping the granted access. As a result the backend can keep access to the memory page even after it has been freed and then re-used for a different purpose. CVE-2022-23041 netfront will fail a BUG_ON() assertion if it fails to revoke access in the rx path. This will result in a Denial of Service (DoS) situation of the guest which can be triggered by the backend. CVE-2022-23042"
    },
    {
        "cve_id": "CVE-2022-23040",
        "code_before_change": "int xenbus_grant_ring(struct xenbus_device *dev, void *vaddr,\n\t\t      unsigned int nr_pages, grant_ref_t *grefs)\n{\n\tint err;\n\tint i, j;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned long gfn;\n\n\t\tif (is_vmalloc_addr(vaddr))\n\t\t\tgfn = pfn_to_gfn(vmalloc_to_pfn(vaddr));\n\t\telse\n\t\t\tgfn = virt_to_gfn(vaddr);\n\n\t\terr = gnttab_grant_foreign_access(dev->otherend_id, gfn, 0);\n\t\tif (err < 0) {\n\t\t\txenbus_dev_fatal(dev, err,\n\t\t\t\t\t \"granting access to ring page\");\n\t\t\tgoto fail;\n\t\t}\n\t\tgrefs[i] = err;\n\n\t\tvaddr = vaddr + XEN_PAGE_SIZE;\n\t}\n\n\treturn 0;\n\nfail:\n\tfor (j = 0; j < i; j++)\n\t\tgnttab_end_foreign_access_ref(grefs[j], 0);\n\treturn err;\n}",
        "code_after_change": "int xenbus_grant_ring(struct xenbus_device *dev, void *vaddr,\n\t\t      unsigned int nr_pages, grant_ref_t *grefs)\n{\n\tint err;\n\tunsigned int i;\n\tgrant_ref_t gref_head;\n\n\terr = gnttab_alloc_grant_references(nr_pages, &gref_head);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"granting access to ring page\");\n\t\treturn err;\n\t}\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned long gfn;\n\n\t\tif (is_vmalloc_addr(vaddr))\n\t\t\tgfn = pfn_to_gfn(vmalloc_to_pfn(vaddr));\n\t\telse\n\t\t\tgfn = virt_to_gfn(vaddr);\n\n\t\tgrefs[i] = gnttab_claim_grant_reference(&gref_head);\n\t\tgnttab_grant_foreign_access_ref(grefs[i], dev->otherend_id,\n\t\t\t\t\t\tgfn, 0);\n\n\t\tvaddr = vaddr + XEN_PAGE_SIZE;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,14 @@\n \t\t      unsigned int nr_pages, grant_ref_t *grefs)\n {\n \tint err;\n-\tint i, j;\n+\tunsigned int i;\n+\tgrant_ref_t gref_head;\n+\n+\terr = gnttab_alloc_grant_references(nr_pages, &gref_head);\n+\tif (err) {\n+\t\txenbus_dev_fatal(dev, err, \"granting access to ring page\");\n+\t\treturn err;\n+\t}\n \n \tfor (i = 0; i < nr_pages; i++) {\n \t\tunsigned long gfn;\n@@ -12,21 +19,12 @@\n \t\telse\n \t\t\tgfn = virt_to_gfn(vaddr);\n \n-\t\terr = gnttab_grant_foreign_access(dev->otherend_id, gfn, 0);\n-\t\tif (err < 0) {\n-\t\t\txenbus_dev_fatal(dev, err,\n-\t\t\t\t\t \"granting access to ring page\");\n-\t\t\tgoto fail;\n-\t\t}\n-\t\tgrefs[i] = err;\n+\t\tgrefs[i] = gnttab_claim_grant_reference(&gref_head);\n+\t\tgnttab_grant_foreign_access_ref(grefs[i], dev->otherend_id,\n+\t\t\t\t\t\tgfn, 0);\n \n \t\tvaddr = vaddr + XEN_PAGE_SIZE;\n \t}\n \n \treturn 0;\n-\n-fail:\n-\tfor (j = 0; j < i; j++)\n-\t\tgnttab_end_foreign_access_ref(grefs[j], 0);\n-\treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tunsigned int i;",
                "\tgrant_ref_t gref_head;",
                "",
                "\terr = gnttab_alloc_grant_references(nr_pages, &gref_head);",
                "\tif (err) {",
                "\t\txenbus_dev_fatal(dev, err, \"granting access to ring page\");",
                "\t\treturn err;",
                "\t}",
                "\t\tgrefs[i] = gnttab_claim_grant_reference(&gref_head);",
                "\t\tgnttab_grant_foreign_access_ref(grefs[i], dev->otherend_id,",
                "\t\t\t\t\t\tgfn, 0);"
            ],
            "deleted": [
                "\tint i, j;",
                "\t\terr = gnttab_grant_foreign_access(dev->otherend_id, gfn, 0);",
                "\t\tif (err < 0) {",
                "\t\t\txenbus_dev_fatal(dev, err,",
                "\t\t\t\t\t \"granting access to ring page\");",
                "\t\t\tgoto fail;",
                "\t\t}",
                "\t\tgrefs[i] = err;",
                "",
                "fail:",
                "\tfor (j = 0; j < i; j++)",
                "\t\tgnttab_end_foreign_access_ref(grefs[j], 0);",
                "\treturn err;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux PV device frontends vulnerable to attacks by backends T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Several Linux PV device frontends are using the grant table interfaces for removing access rights of the backends in ways being subject to race conditions, resulting in potential data leaks, data corruption by malicious backends, and denial of service triggered by malicious backends: blkfront, netfront, scsifront and the gntalloc driver are testing whether a grant reference is still in use. If this is not the case, they assume that a following removal of the granted access will always succeed, which is not true in case the backend has mapped the granted page between those two operations. As a result the backend can keep access to the memory page of the guest no matter how the page will be used after the frontend I/O has finished. The xenbus driver has a similar problem, as it doesn't check the success of removing the granted access of a shared ring buffer. blkfront: CVE-2022-23036 netfront: CVE-2022-23037 scsifront: CVE-2022-23038 gntalloc: CVE-2022-23039 xenbus: CVE-2022-23040 blkfront, netfront, scsifront, usbfront, dmabuf, xenbus, 9p, kbdfront, and pvcalls are using a functionality to delay freeing a grant reference until it is no longer in use, but the freeing of the related data page is not synchronized with dropping the granted access. As a result the backend can keep access to the memory page even after it has been freed and then re-used for a different purpose. CVE-2022-23041 netfront will fail a BUG_ON() assertion if it fails to revoke access in the rx path. This will result in a Denial of Service (DoS) situation of the guest which can be triggered by the backend. CVE-2022-23042"
    },
    {
        "cve_id": "CVE-2022-23042",
        "code_before_change": "static int xennet_poll(struct napi_struct *napi, int budget)\n{\n\tstruct netfront_queue *queue = container_of(napi, struct netfront_queue, napi);\n\tstruct net_device *dev = queue->info->netdev;\n\tstruct sk_buff *skb;\n\tstruct netfront_rx_info rinfo;\n\tstruct xen_netif_rx_response *rx = &rinfo.rx;\n\tstruct xen_netif_extra_info *extras = rinfo.extras;\n\tRING_IDX i, rp;\n\tint work_done;\n\tstruct sk_buff_head rxq;\n\tstruct sk_buff_head errq;\n\tstruct sk_buff_head tmpq;\n\tint err;\n\tbool need_xdp_flush = false;\n\n\tspin_lock(&queue->rx_lock);\n\n\tskb_queue_head_init(&rxq);\n\tskb_queue_head_init(&errq);\n\tskb_queue_head_init(&tmpq);\n\n\trp = queue->rx.sring->rsp_prod;\n\tif (RING_RESPONSE_PROD_OVERFLOW(&queue->rx, rp)) {\n\t\tdev_alert(&dev->dev, \"Illegal number of responses %u\\n\",\n\t\t\t  rp - queue->rx.rsp_cons);\n\t\tqueue->info->broken = true;\n\t\tspin_unlock(&queue->rx_lock);\n\t\treturn 0;\n\t}\n\trmb(); /* Ensure we see queued responses up to 'rp'. */\n\n\ti = queue->rx.rsp_cons;\n\twork_done = 0;\n\twhile ((i != rp) && (work_done < budget)) {\n\t\tRING_COPY_RESPONSE(&queue->rx, i, rx);\n\t\tmemset(extras, 0, sizeof(rinfo.extras));\n\n\t\terr = xennet_get_responses(queue, &rinfo, rp, &tmpq,\n\t\t\t\t\t   &need_xdp_flush);\n\n\t\tif (unlikely(err)) {\nerr:\n\t\t\twhile ((skb = __skb_dequeue(&tmpq)))\n\t\t\t\t__skb_queue_tail(&errq, skb);\n\t\t\tdev->stats.rx_errors++;\n\t\t\ti = queue->rx.rsp_cons;\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb = __skb_dequeue(&tmpq);\n\n\t\tif (extras[XEN_NETIF_EXTRA_TYPE_GSO - 1].type) {\n\t\t\tstruct xen_netif_extra_info *gso;\n\t\t\tgso = &extras[XEN_NETIF_EXTRA_TYPE_GSO - 1];\n\n\t\t\tif (unlikely(xennet_set_skb_gso(skb, gso))) {\n\t\t\t\t__skb_queue_head(&tmpq, skb);\n\t\t\t\txennet_set_rx_rsp_cons(queue,\n\t\t\t\t\t\t       queue->rx.rsp_cons +\n\t\t\t\t\t\t       skb_queue_len(&tmpq));\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\n\t\tNETFRONT_SKB_CB(skb)->pull_to = rx->status;\n\t\tif (NETFRONT_SKB_CB(skb)->pull_to > RX_COPY_THRESHOLD)\n\t\t\tNETFRONT_SKB_CB(skb)->pull_to = RX_COPY_THRESHOLD;\n\n\t\tskb_frag_off_set(&skb_shinfo(skb)->frags[0], rx->offset);\n\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[0], rx->status);\n\t\tskb->data_len = rx->status;\n\t\tskb->len += rx->status;\n\n\t\tif (unlikely(xennet_fill_frags(queue, skb, &tmpq)))\n\t\t\tgoto err;\n\n\t\tif (rx->flags & XEN_NETRXF_csum_blank)\n\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\telse if (rx->flags & XEN_NETRXF_data_validated)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\t__skb_queue_tail(&rxq, skb);\n\n\t\ti = queue->rx.rsp_cons + 1;\n\t\txennet_set_rx_rsp_cons(queue, i);\n\t\twork_done++;\n\t}\n\tif (need_xdp_flush)\n\t\txdp_do_flush();\n\n\t__skb_queue_purge(&errq);\n\n\twork_done -= handle_incoming_queue(queue, &rxq);\n\n\txennet_alloc_rx_buffers(queue);\n\n\tif (work_done < budget) {\n\t\tint more_to_do = 0;\n\n\t\tnapi_complete_done(napi, work_done);\n\n\t\tRING_FINAL_CHECK_FOR_RESPONSES(&queue->rx, more_to_do);\n\t\tif (more_to_do)\n\t\t\tnapi_schedule(napi);\n\t}\n\n\tspin_unlock(&queue->rx_lock);\n\n\treturn work_done;\n}",
        "code_after_change": "static int xennet_poll(struct napi_struct *napi, int budget)\n{\n\tstruct netfront_queue *queue = container_of(napi, struct netfront_queue, napi);\n\tstruct net_device *dev = queue->info->netdev;\n\tstruct sk_buff *skb;\n\tstruct netfront_rx_info rinfo;\n\tstruct xen_netif_rx_response *rx = &rinfo.rx;\n\tstruct xen_netif_extra_info *extras = rinfo.extras;\n\tRING_IDX i, rp;\n\tint work_done;\n\tstruct sk_buff_head rxq;\n\tstruct sk_buff_head errq;\n\tstruct sk_buff_head tmpq;\n\tint err;\n\tbool need_xdp_flush = false;\n\n\tspin_lock(&queue->rx_lock);\n\n\tskb_queue_head_init(&rxq);\n\tskb_queue_head_init(&errq);\n\tskb_queue_head_init(&tmpq);\n\n\trp = queue->rx.sring->rsp_prod;\n\tif (RING_RESPONSE_PROD_OVERFLOW(&queue->rx, rp)) {\n\t\tdev_alert(&dev->dev, \"Illegal number of responses %u\\n\",\n\t\t\t  rp - queue->rx.rsp_cons);\n\t\tqueue->info->broken = true;\n\t\tspin_unlock(&queue->rx_lock);\n\t\treturn 0;\n\t}\n\trmb(); /* Ensure we see queued responses up to 'rp'. */\n\n\ti = queue->rx.rsp_cons;\n\twork_done = 0;\n\twhile ((i != rp) && (work_done < budget)) {\n\t\tRING_COPY_RESPONSE(&queue->rx, i, rx);\n\t\tmemset(extras, 0, sizeof(rinfo.extras));\n\n\t\terr = xennet_get_responses(queue, &rinfo, rp, &tmpq,\n\t\t\t\t\t   &need_xdp_flush);\n\n\t\tif (unlikely(err)) {\n\t\t\tif (queue->info->broken) {\n\t\t\t\tspin_unlock(&queue->rx_lock);\n\t\t\t\treturn 0;\n\t\t\t}\nerr:\n\t\t\twhile ((skb = __skb_dequeue(&tmpq)))\n\t\t\t\t__skb_queue_tail(&errq, skb);\n\t\t\tdev->stats.rx_errors++;\n\t\t\ti = queue->rx.rsp_cons;\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb = __skb_dequeue(&tmpq);\n\n\t\tif (extras[XEN_NETIF_EXTRA_TYPE_GSO - 1].type) {\n\t\t\tstruct xen_netif_extra_info *gso;\n\t\t\tgso = &extras[XEN_NETIF_EXTRA_TYPE_GSO - 1];\n\n\t\t\tif (unlikely(xennet_set_skb_gso(skb, gso))) {\n\t\t\t\t__skb_queue_head(&tmpq, skb);\n\t\t\t\txennet_set_rx_rsp_cons(queue,\n\t\t\t\t\t\t       queue->rx.rsp_cons +\n\t\t\t\t\t\t       skb_queue_len(&tmpq));\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\n\t\tNETFRONT_SKB_CB(skb)->pull_to = rx->status;\n\t\tif (NETFRONT_SKB_CB(skb)->pull_to > RX_COPY_THRESHOLD)\n\t\t\tNETFRONT_SKB_CB(skb)->pull_to = RX_COPY_THRESHOLD;\n\n\t\tskb_frag_off_set(&skb_shinfo(skb)->frags[0], rx->offset);\n\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[0], rx->status);\n\t\tskb->data_len = rx->status;\n\t\tskb->len += rx->status;\n\n\t\tif (unlikely(xennet_fill_frags(queue, skb, &tmpq)))\n\t\t\tgoto err;\n\n\t\tif (rx->flags & XEN_NETRXF_csum_blank)\n\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\telse if (rx->flags & XEN_NETRXF_data_validated)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\t__skb_queue_tail(&rxq, skb);\n\n\t\ti = queue->rx.rsp_cons + 1;\n\t\txennet_set_rx_rsp_cons(queue, i);\n\t\twork_done++;\n\t}\n\tif (need_xdp_flush)\n\t\txdp_do_flush();\n\n\t__skb_queue_purge(&errq);\n\n\twork_done -= handle_incoming_queue(queue, &rxq);\n\n\txennet_alloc_rx_buffers(queue);\n\n\tif (work_done < budget) {\n\t\tint more_to_do = 0;\n\n\t\tnapi_complete_done(napi, work_done);\n\n\t\tRING_FINAL_CHECK_FOR_RESPONSES(&queue->rx, more_to_do);\n\t\tif (more_to_do)\n\t\t\tnapi_schedule(napi);\n\t}\n\n\tspin_unlock(&queue->rx_lock);\n\n\treturn work_done;\n}",
        "patch": "--- code before\n+++ code after\n@@ -40,6 +40,10 @@\n \t\t\t\t\t   &need_xdp_flush);\n \n \t\tif (unlikely(err)) {\n+\t\t\tif (queue->info->broken) {\n+\t\t\t\tspin_unlock(&queue->rx_lock);\n+\t\t\t\treturn 0;\n+\t\t\t}\n err:\n \t\t\twhile ((skb = __skb_dequeue(&tmpq)))\n \t\t\t\t__skb_queue_tail(&errq, skb);",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (queue->info->broken) {",
                "\t\t\t\tspin_unlock(&queue->rx_lock);",
                "\t\t\t\treturn 0;",
                "\t\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux PV device frontends vulnerable to attacks by backends T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Several Linux PV device frontends are using the grant table interfaces for removing access rights of the backends in ways being subject to race conditions, resulting in potential data leaks, data corruption by malicious backends, and denial of service triggered by malicious backends: blkfront, netfront, scsifront and the gntalloc driver are testing whether a grant reference is still in use. If this is not the case, they assume that a following removal of the granted access will always succeed, which is not true in case the backend has mapped the granted page between those two operations. As a result the backend can keep access to the memory page of the guest no matter how the page will be used after the frontend I/O has finished. The xenbus driver has a similar problem, as it doesn't check the success of removing the granted access of a shared ring buffer. blkfront: CVE-2022-23036 netfront: CVE-2022-23037 scsifront: CVE-2022-23038 gntalloc: CVE-2022-23039 xenbus: CVE-2022-23040 blkfront, netfront, scsifront, usbfront, dmabuf, xenbus, 9p, kbdfront, and pvcalls are using a functionality to delay freeing a grant reference until it is no longer in use, but the freeing of the related data page is not synchronized with dropping the granted access. As a result the backend can keep access to the memory page even after it has been freed and then re-used for a different purpose. CVE-2022-23041 netfront will fail a BUG_ON() assertion if it fails to revoke access in the rx path. This will result in a Denial of Service (DoS) situation of the guest which can be triggered by the backend. CVE-2022-23042"
    },
    {
        "cve_id": "CVE-2022-23042",
        "code_before_change": "static int xennet_get_responses(struct netfront_queue *queue,\n\t\t\t\tstruct netfront_rx_info *rinfo, RING_IDX rp,\n\t\t\t\tstruct sk_buff_head *list,\n\t\t\t\tbool *need_xdp_flush)\n{\n\tstruct xen_netif_rx_response *rx = &rinfo->rx, rx_local;\n\tint max = XEN_NETIF_NR_SLOTS_MIN + (rx->status <= RX_COPY_THRESHOLD);\n\tRING_IDX cons = queue->rx.rsp_cons;\n\tstruct sk_buff *skb = xennet_get_rx_skb(queue, cons);\n\tstruct xen_netif_extra_info *extras = rinfo->extras;\n\tgrant_ref_t ref = xennet_get_rx_ref(queue, cons);\n\tstruct device *dev = &queue->info->netdev->dev;\n\tstruct bpf_prog *xdp_prog;\n\tstruct xdp_buff xdp;\n\tunsigned long ret;\n\tint slots = 1;\n\tint err = 0;\n\tu32 verdict;\n\n\tif (rx->flags & XEN_NETRXF_extra_info) {\n\t\terr = xennet_get_extras(queue, extras, rp);\n\t\tif (!err) {\n\t\t\tif (extras[XEN_NETIF_EXTRA_TYPE_XDP - 1].type) {\n\t\t\t\tstruct xen_netif_extra_info *xdp;\n\n\t\t\t\txdp = &extras[XEN_NETIF_EXTRA_TYPE_XDP - 1];\n\t\t\t\trx->offset = xdp->u.xdp.headroom;\n\t\t\t}\n\t\t}\n\t\tcons = queue->rx.rsp_cons;\n\t}\n\n\tfor (;;) {\n\t\tif (unlikely(rx->status < 0 ||\n\t\t\t     rx->offset + rx->status > XEN_PAGE_SIZE)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"rx->offset: %u, size: %d\\n\",\n\t\t\t\t\t rx->offset, rx->status);\n\t\t\txennet_move_rx_slot(queue, skb, ref);\n\t\t\terr = -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\n\t\t/*\n\t\t * This definitely indicates a bug, either in this driver or in\n\t\t * the backend driver. In future this should flag the bad\n\t\t * situation to the system controller to reboot the backend.\n\t\t */\n\t\tif (ref == GRANT_INVALID_REF) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Bad rx response id %d.\\n\",\n\t\t\t\t\t rx->id);\n\t\t\terr = -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\n\t\tret = gnttab_end_foreign_access_ref(ref, 0);\n\t\tBUG_ON(!ret);\n\n\t\tgnttab_release_grant_reference(&queue->gref_rx_head, ref);\n\n\t\trcu_read_lock();\n\t\txdp_prog = rcu_dereference(queue->xdp_prog);\n\t\tif (xdp_prog) {\n\t\t\tif (!(rx->flags & XEN_NETRXF_more_data)) {\n\t\t\t\t/* currently only a single page contains data */\n\t\t\t\tverdict = xennet_run_xdp(queue,\n\t\t\t\t\t\t\t skb_frag_page(&skb_shinfo(skb)->frags[0]),\n\t\t\t\t\t\t\t rx, xdp_prog, &xdp, need_xdp_flush);\n\t\t\t\tif (verdict != XDP_PASS)\n\t\t\t\t\terr = -EINVAL;\n\t\t\t} else {\n\t\t\t\t/* drop the frame */\n\t\t\t\terr = -EINVAL;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\nnext:\n\t\t__skb_queue_tail(list, skb);\n\t\tif (!(rx->flags & XEN_NETRXF_more_data))\n\t\t\tbreak;\n\n\t\tif (cons + slots == rp) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Need more slots\\n\");\n\t\t\terr = -ENOENT;\n\t\t\tbreak;\n\t\t}\n\n\t\tRING_COPY_RESPONSE(&queue->rx, cons + slots, &rx_local);\n\t\trx = &rx_local;\n\t\tskb = xennet_get_rx_skb(queue, cons + slots);\n\t\tref = xennet_get_rx_ref(queue, cons + slots);\n\t\tslots++;\n\t}\n\n\tif (unlikely(slots > max)) {\n\t\tif (net_ratelimit())\n\t\t\tdev_warn(dev, \"Too many slots\\n\");\n\t\terr = -E2BIG;\n\t}\n\n\tif (unlikely(err))\n\t\txennet_set_rx_rsp_cons(queue, cons + slots);\n\n\treturn err;\n}",
        "code_after_change": "static int xennet_get_responses(struct netfront_queue *queue,\n\t\t\t\tstruct netfront_rx_info *rinfo, RING_IDX rp,\n\t\t\t\tstruct sk_buff_head *list,\n\t\t\t\tbool *need_xdp_flush)\n{\n\tstruct xen_netif_rx_response *rx = &rinfo->rx, rx_local;\n\tint max = XEN_NETIF_NR_SLOTS_MIN + (rx->status <= RX_COPY_THRESHOLD);\n\tRING_IDX cons = queue->rx.rsp_cons;\n\tstruct sk_buff *skb = xennet_get_rx_skb(queue, cons);\n\tstruct xen_netif_extra_info *extras = rinfo->extras;\n\tgrant_ref_t ref = xennet_get_rx_ref(queue, cons);\n\tstruct device *dev = &queue->info->netdev->dev;\n\tstruct bpf_prog *xdp_prog;\n\tstruct xdp_buff xdp;\n\tint slots = 1;\n\tint err = 0;\n\tu32 verdict;\n\n\tif (rx->flags & XEN_NETRXF_extra_info) {\n\t\terr = xennet_get_extras(queue, extras, rp);\n\t\tif (!err) {\n\t\t\tif (extras[XEN_NETIF_EXTRA_TYPE_XDP - 1].type) {\n\t\t\t\tstruct xen_netif_extra_info *xdp;\n\n\t\t\t\txdp = &extras[XEN_NETIF_EXTRA_TYPE_XDP - 1];\n\t\t\t\trx->offset = xdp->u.xdp.headroom;\n\t\t\t}\n\t\t}\n\t\tcons = queue->rx.rsp_cons;\n\t}\n\n\tfor (;;) {\n\t\tif (unlikely(rx->status < 0 ||\n\t\t\t     rx->offset + rx->status > XEN_PAGE_SIZE)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"rx->offset: %u, size: %d\\n\",\n\t\t\t\t\t rx->offset, rx->status);\n\t\t\txennet_move_rx_slot(queue, skb, ref);\n\t\t\terr = -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\n\t\t/*\n\t\t * This definitely indicates a bug, either in this driver or in\n\t\t * the backend driver. In future this should flag the bad\n\t\t * situation to the system controller to reboot the backend.\n\t\t */\n\t\tif (ref == GRANT_INVALID_REF) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Bad rx response id %d.\\n\",\n\t\t\t\t\t rx->id);\n\t\t\terr = -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\n\t\tif (!gnttab_end_foreign_access_ref(ref, 0)) {\n\t\t\tdev_alert(dev,\n\t\t\t\t  \"Grant still in use by backend domain\\n\");\n\t\t\tqueue->info->broken = true;\n\t\t\tdev_alert(dev, \"Disabled for further use\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tgnttab_release_grant_reference(&queue->gref_rx_head, ref);\n\n\t\trcu_read_lock();\n\t\txdp_prog = rcu_dereference(queue->xdp_prog);\n\t\tif (xdp_prog) {\n\t\t\tif (!(rx->flags & XEN_NETRXF_more_data)) {\n\t\t\t\t/* currently only a single page contains data */\n\t\t\t\tverdict = xennet_run_xdp(queue,\n\t\t\t\t\t\t\t skb_frag_page(&skb_shinfo(skb)->frags[0]),\n\t\t\t\t\t\t\t rx, xdp_prog, &xdp, need_xdp_flush);\n\t\t\t\tif (verdict != XDP_PASS)\n\t\t\t\t\terr = -EINVAL;\n\t\t\t} else {\n\t\t\t\t/* drop the frame */\n\t\t\t\terr = -EINVAL;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\nnext:\n\t\t__skb_queue_tail(list, skb);\n\t\tif (!(rx->flags & XEN_NETRXF_more_data))\n\t\t\tbreak;\n\n\t\tif (cons + slots == rp) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Need more slots\\n\");\n\t\t\terr = -ENOENT;\n\t\t\tbreak;\n\t\t}\n\n\t\tRING_COPY_RESPONSE(&queue->rx, cons + slots, &rx_local);\n\t\trx = &rx_local;\n\t\tskb = xennet_get_rx_skb(queue, cons + slots);\n\t\tref = xennet_get_rx_ref(queue, cons + slots);\n\t\tslots++;\n\t}\n\n\tif (unlikely(slots > max)) {\n\t\tif (net_ratelimit())\n\t\t\tdev_warn(dev, \"Too many slots\\n\");\n\t\terr = -E2BIG;\n\t}\n\n\tif (unlikely(err))\n\t\txennet_set_rx_rsp_cons(queue, cons + slots);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,6 @@\n \tstruct device *dev = &queue->info->netdev->dev;\n \tstruct bpf_prog *xdp_prog;\n \tstruct xdp_buff xdp;\n-\tunsigned long ret;\n \tint slots = 1;\n \tint err = 0;\n \tu32 verdict;\n@@ -54,8 +53,13 @@\n \t\t\tgoto next;\n \t\t}\n \n-\t\tret = gnttab_end_foreign_access_ref(ref, 0);\n-\t\tBUG_ON(!ret);\n+\t\tif (!gnttab_end_foreign_access_ref(ref, 0)) {\n+\t\t\tdev_alert(dev,\n+\t\t\t\t  \"Grant still in use by backend domain\\n\");\n+\t\t\tqueue->info->broken = true;\n+\t\t\tdev_alert(dev, \"Disabled for further use\\n\");\n+\t\t\treturn -EINVAL;\n+\t\t}\n \n \t\tgnttab_release_grant_reference(&queue->gref_rx_head, ref);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tif (!gnttab_end_foreign_access_ref(ref, 0)) {",
                "\t\t\tdev_alert(dev,",
                "\t\t\t\t  \"Grant still in use by backend domain\\n\");",
                "\t\t\tqueue->info->broken = true;",
                "\t\t\tdev_alert(dev, \"Disabled for further use\\n\");",
                "\t\t\treturn -EINVAL;",
                "\t\t}"
            ],
            "deleted": [
                "\tunsigned long ret;",
                "\t\tret = gnttab_end_foreign_access_ref(ref, 0);",
                "\t\tBUG_ON(!ret);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux PV device frontends vulnerable to attacks by backends T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Several Linux PV device frontends are using the grant table interfaces for removing access rights of the backends in ways being subject to race conditions, resulting in potential data leaks, data corruption by malicious backends, and denial of service triggered by malicious backends: blkfront, netfront, scsifront and the gntalloc driver are testing whether a grant reference is still in use. If this is not the case, they assume that a following removal of the granted access will always succeed, which is not true in case the backend has mapped the granted page between those two operations. As a result the backend can keep access to the memory page of the guest no matter how the page will be used after the frontend I/O has finished. The xenbus driver has a similar problem, as it doesn't check the success of removing the granted access of a shared ring buffer. blkfront: CVE-2022-23036 netfront: CVE-2022-23037 scsifront: CVE-2022-23038 gntalloc: CVE-2022-23039 xenbus: CVE-2022-23040 blkfront, netfront, scsifront, usbfront, dmabuf, xenbus, 9p, kbdfront, and pvcalls are using a functionality to delay freeing a grant reference until it is no longer in use, but the freeing of the related data page is not synchronized with dropping the granted access. As a result the backend can keep access to the memory page even after it has been freed and then re-used for a different purpose. CVE-2022-23041 netfront will fail a BUG_ON() assertion if it fails to revoke access in the rx path. This will result in a Denial of Service (DoS) situation of the guest which can be triggered by the backend. CVE-2022-23042"
    },
    {
        "cve_id": "CVE-2022-23042",
        "code_before_change": "static int setup_netfront(struct xenbus_device *dev,\n\t\t\tstruct netfront_queue *queue, unsigned int feature_split_evtchn)\n{\n\tstruct xen_netif_tx_sring *txs;\n\tstruct xen_netif_rx_sring *rxs;\n\tgrant_ref_t gref;\n\tint err;\n\n\tqueue->tx_ring_ref = GRANT_INVALID_REF;\n\tqueue->rx_ring_ref = GRANT_INVALID_REF;\n\tqueue->rx.sring = NULL;\n\tqueue->tx.sring = NULL;\n\n\ttxs = (struct xen_netif_tx_sring *)get_zeroed_page(GFP_NOIO | __GFP_HIGH);\n\tif (!txs) {\n\t\terr = -ENOMEM;\n\t\txenbus_dev_fatal(dev, err, \"allocating tx ring page\");\n\t\tgoto fail;\n\t}\n\tSHARED_RING_INIT(txs);\n\tFRONT_RING_INIT(&queue->tx, txs, XEN_PAGE_SIZE);\n\n\terr = xenbus_grant_ring(dev, txs, 1, &gref);\n\tif (err < 0)\n\t\tgoto grant_tx_ring_fail;\n\tqueue->tx_ring_ref = gref;\n\n\trxs = (struct xen_netif_rx_sring *)get_zeroed_page(GFP_NOIO | __GFP_HIGH);\n\tif (!rxs) {\n\t\terr = -ENOMEM;\n\t\txenbus_dev_fatal(dev, err, \"allocating rx ring page\");\n\t\tgoto alloc_rx_ring_fail;\n\t}\n\tSHARED_RING_INIT(rxs);\n\tFRONT_RING_INIT(&queue->rx, rxs, XEN_PAGE_SIZE);\n\n\terr = xenbus_grant_ring(dev, rxs, 1, &gref);\n\tif (err < 0)\n\t\tgoto grant_rx_ring_fail;\n\tqueue->rx_ring_ref = gref;\n\n\tif (feature_split_evtchn)\n\t\terr = setup_netfront_split(queue);\n\t/* setup single event channel if\n\t *  a) feature-split-event-channels == 0\n\t *  b) feature-split-event-channels == 1 but failed to setup\n\t */\n\tif (!feature_split_evtchn || err)\n\t\terr = setup_netfront_single(queue);\n\n\tif (err)\n\t\tgoto alloc_evtchn_fail;\n\n\treturn 0;\n\n\t/* If we fail to setup netfront, it is safe to just revoke access to\n\t * granted pages because backend is not accessing it at this point.\n\t */\nalloc_evtchn_fail:\n\tgnttab_end_foreign_access_ref(queue->rx_ring_ref, 0);\ngrant_rx_ring_fail:\n\tfree_page((unsigned long)rxs);\nalloc_rx_ring_fail:\n\tgnttab_end_foreign_access_ref(queue->tx_ring_ref, 0);\ngrant_tx_ring_fail:\n\tfree_page((unsigned long)txs);\nfail:\n\treturn err;\n}",
        "code_after_change": "static int setup_netfront(struct xenbus_device *dev,\n\t\t\tstruct netfront_queue *queue, unsigned int feature_split_evtchn)\n{\n\tstruct xen_netif_tx_sring *txs;\n\tstruct xen_netif_rx_sring *rxs = NULL;\n\tgrant_ref_t gref;\n\tint err;\n\n\tqueue->tx_ring_ref = GRANT_INVALID_REF;\n\tqueue->rx_ring_ref = GRANT_INVALID_REF;\n\tqueue->rx.sring = NULL;\n\tqueue->tx.sring = NULL;\n\n\ttxs = (struct xen_netif_tx_sring *)get_zeroed_page(GFP_NOIO | __GFP_HIGH);\n\tif (!txs) {\n\t\terr = -ENOMEM;\n\t\txenbus_dev_fatal(dev, err, \"allocating tx ring page\");\n\t\tgoto fail;\n\t}\n\tSHARED_RING_INIT(txs);\n\tFRONT_RING_INIT(&queue->tx, txs, XEN_PAGE_SIZE);\n\n\terr = xenbus_grant_ring(dev, txs, 1, &gref);\n\tif (err < 0)\n\t\tgoto fail;\n\tqueue->tx_ring_ref = gref;\n\n\trxs = (struct xen_netif_rx_sring *)get_zeroed_page(GFP_NOIO | __GFP_HIGH);\n\tif (!rxs) {\n\t\terr = -ENOMEM;\n\t\txenbus_dev_fatal(dev, err, \"allocating rx ring page\");\n\t\tgoto fail;\n\t}\n\tSHARED_RING_INIT(rxs);\n\tFRONT_RING_INIT(&queue->rx, rxs, XEN_PAGE_SIZE);\n\n\terr = xenbus_grant_ring(dev, rxs, 1, &gref);\n\tif (err < 0)\n\t\tgoto fail;\n\tqueue->rx_ring_ref = gref;\n\n\tif (feature_split_evtchn)\n\t\terr = setup_netfront_split(queue);\n\t/* setup single event channel if\n\t *  a) feature-split-event-channels == 0\n\t *  b) feature-split-event-channels == 1 but failed to setup\n\t */\n\tif (!feature_split_evtchn || err)\n\t\terr = setup_netfront_single(queue);\n\n\tif (err)\n\t\tgoto fail;\n\n\treturn 0;\n\n\t/* If we fail to setup netfront, it is safe to just revoke access to\n\t * granted pages because backend is not accessing it at this point.\n\t */\n fail:\n\tif (queue->rx_ring_ref != GRANT_INVALID_REF) {\n\t\tgnttab_end_foreign_access(queue->rx_ring_ref, 0,\n\t\t\t\t\t  (unsigned long)rxs);\n\t\tqueue->rx_ring_ref = GRANT_INVALID_REF;\n\t} else {\n\t\tfree_page((unsigned long)rxs);\n\t}\n\tif (queue->tx_ring_ref != GRANT_INVALID_REF) {\n\t\tgnttab_end_foreign_access(queue->tx_ring_ref, 0,\n\t\t\t\t\t  (unsigned long)txs);\n\t\tqueue->tx_ring_ref = GRANT_INVALID_REF;\n\t} else {\n\t\tfree_page((unsigned long)txs);\n\t}\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n \t\t\tstruct netfront_queue *queue, unsigned int feature_split_evtchn)\n {\n \tstruct xen_netif_tx_sring *txs;\n-\tstruct xen_netif_rx_sring *rxs;\n+\tstruct xen_netif_rx_sring *rxs = NULL;\n \tgrant_ref_t gref;\n \tint err;\n \n@@ -22,21 +22,21 @@\n \n \terr = xenbus_grant_ring(dev, txs, 1, &gref);\n \tif (err < 0)\n-\t\tgoto grant_tx_ring_fail;\n+\t\tgoto fail;\n \tqueue->tx_ring_ref = gref;\n \n \trxs = (struct xen_netif_rx_sring *)get_zeroed_page(GFP_NOIO | __GFP_HIGH);\n \tif (!rxs) {\n \t\terr = -ENOMEM;\n \t\txenbus_dev_fatal(dev, err, \"allocating rx ring page\");\n-\t\tgoto alloc_rx_ring_fail;\n+\t\tgoto fail;\n \t}\n \tSHARED_RING_INIT(rxs);\n \tFRONT_RING_INIT(&queue->rx, rxs, XEN_PAGE_SIZE);\n \n \terr = xenbus_grant_ring(dev, rxs, 1, &gref);\n \tif (err < 0)\n-\t\tgoto grant_rx_ring_fail;\n+\t\tgoto fail;\n \tqueue->rx_ring_ref = gref;\n \n \tif (feature_split_evtchn)\n@@ -49,21 +49,27 @@\n \t\terr = setup_netfront_single(queue);\n \n \tif (err)\n-\t\tgoto alloc_evtchn_fail;\n+\t\tgoto fail;\n \n \treturn 0;\n \n \t/* If we fail to setup netfront, it is safe to just revoke access to\n \t * granted pages because backend is not accessing it at this point.\n \t */\n-alloc_evtchn_fail:\n-\tgnttab_end_foreign_access_ref(queue->rx_ring_ref, 0);\n-grant_rx_ring_fail:\n-\tfree_page((unsigned long)rxs);\n-alloc_rx_ring_fail:\n-\tgnttab_end_foreign_access_ref(queue->tx_ring_ref, 0);\n-grant_tx_ring_fail:\n-\tfree_page((unsigned long)txs);\n-fail:\n+ fail:\n+\tif (queue->rx_ring_ref != GRANT_INVALID_REF) {\n+\t\tgnttab_end_foreign_access(queue->rx_ring_ref, 0,\n+\t\t\t\t\t  (unsigned long)rxs);\n+\t\tqueue->rx_ring_ref = GRANT_INVALID_REF;\n+\t} else {\n+\t\tfree_page((unsigned long)rxs);\n+\t}\n+\tif (queue->tx_ring_ref != GRANT_INVALID_REF) {\n+\t\tgnttab_end_foreign_access(queue->tx_ring_ref, 0,\n+\t\t\t\t\t  (unsigned long)txs);\n+\t\tqueue->tx_ring_ref = GRANT_INVALID_REF;\n+\t} else {\n+\t\tfree_page((unsigned long)txs);\n+\t}\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct xen_netif_rx_sring *rxs = NULL;",
                "\t\tgoto fail;",
                "\t\tgoto fail;",
                "\t\tgoto fail;",
                "\t\tgoto fail;",
                " fail:",
                "\tif (queue->rx_ring_ref != GRANT_INVALID_REF) {",
                "\t\tgnttab_end_foreign_access(queue->rx_ring_ref, 0,",
                "\t\t\t\t\t  (unsigned long)rxs);",
                "\t\tqueue->rx_ring_ref = GRANT_INVALID_REF;",
                "\t} else {",
                "\t\tfree_page((unsigned long)rxs);",
                "\t}",
                "\tif (queue->tx_ring_ref != GRANT_INVALID_REF) {",
                "\t\tgnttab_end_foreign_access(queue->tx_ring_ref, 0,",
                "\t\t\t\t\t  (unsigned long)txs);",
                "\t\tqueue->tx_ring_ref = GRANT_INVALID_REF;",
                "\t} else {",
                "\t\tfree_page((unsigned long)txs);",
                "\t}"
            ],
            "deleted": [
                "\tstruct xen_netif_rx_sring *rxs;",
                "\t\tgoto grant_tx_ring_fail;",
                "\t\tgoto alloc_rx_ring_fail;",
                "\t\tgoto grant_rx_ring_fail;",
                "\t\tgoto alloc_evtchn_fail;",
                "alloc_evtchn_fail:",
                "\tgnttab_end_foreign_access_ref(queue->rx_ring_ref, 0);",
                "grant_rx_ring_fail:",
                "\tfree_page((unsigned long)rxs);",
                "alloc_rx_ring_fail:",
                "\tgnttab_end_foreign_access_ref(queue->tx_ring_ref, 0);",
                "grant_tx_ring_fail:",
                "\tfree_page((unsigned long)txs);",
                "fail:"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Linux PV device frontends vulnerable to attacks by backends T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Several Linux PV device frontends are using the grant table interfaces for removing access rights of the backends in ways being subject to race conditions, resulting in potential data leaks, data corruption by malicious backends, and denial of service triggered by malicious backends: blkfront, netfront, scsifront and the gntalloc driver are testing whether a grant reference is still in use. If this is not the case, they assume that a following removal of the granted access will always succeed, which is not true in case the backend has mapped the granted page between those two operations. As a result the backend can keep access to the memory page of the guest no matter how the page will be used after the frontend I/O has finished. The xenbus driver has a similar problem, as it doesn't check the success of removing the granted access of a shared ring buffer. blkfront: CVE-2022-23036 netfront: CVE-2022-23037 scsifront: CVE-2022-23038 gntalloc: CVE-2022-23039 xenbus: CVE-2022-23040 blkfront, netfront, scsifront, usbfront, dmabuf, xenbus, 9p, kbdfront, and pvcalls are using a functionality to delay freeing a grant reference until it is no longer in use, but the freeing of the related data page is not synchronized with dropping the granted access. As a result the backend can keep access to the memory page even after it has been freed and then re-used for a different purpose. CVE-2022-23041 netfront will fail a BUG_ON() assertion if it fails to revoke access in the rx path. This will result in a Denial of Service (DoS) situation of the guest which can be triggered by the backend. CVE-2022-23042"
    },
    {
        "cve_id": "CVE-2022-2590",
        "code_before_change": "static int faultin_page(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, bool unshare,\n\t\tint *locked)\n{\n\tunsigned int fault_flags = 0;\n\tvm_fault_t ret;\n\n\tif (*flags & FOLL_NOFAULT)\n\t\treturn -EFAULT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (locked)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\t/*\n\t\t * Note: FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_TRIED\n\t\t * can co-exist\n\t\t */\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\tif (unshare) {\n\t\tfault_flags |= FAULT_FLAG_UNSHARE;\n\t\t/* FAULT_FLAG_WRITE and FAULT_FLAG_UNSHARE are incompatible */\n\t\tVM_BUG_ON(fault_flags & FAULT_FLAG_WRITE);\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags, NULL);\n\n\tif (ret & VM_FAULT_COMPLETED) {\n\t\t/*\n\t\t * With FAULT_FLAG_RETRY_NOWAIT we'll never release the\n\t\t * mmap lock in the page fault handler. Sanity check this.\n\t\t */\n\t\tWARN_ON_ONCE(fault_flags & FAULT_FLAG_RETRY_NOWAIT);\n\t\tif (locked)\n\t\t\t*locked = 0;\n\t\t/*\n\t\t * We should do the same as VM_FAULT_RETRY, but let's not\n\t\t * return -EBUSY since that's not reflecting the reality of\n\t\t * what has happened - we've just fully completed a page\n\t\t * fault, with the mmap lock released.  Use -EAGAIN to show\n\t\t * that we want to take the mmap lock _again_.\n\t\t */\n\t\treturn -EAGAIN;\n\t}\n\n\tif (ret & VM_FAULT_ERROR) {\n\t\tint err = vm_fault_to_errno(ret, *flags);\n\n\t\tif (err)\n\t\t\treturn err;\n\t\tBUG();\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (locked && !(fault_flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\t\t*locked = 0;\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n\t * can thus safely do subsequent page lookups as if they were reads.\n\t * But only do so when looping for pte_write is futile: in some cases\n\t * userspace may also be wanting to write to the gotten user page,\n\t * which a read fault here might prevent (a readonly page might get\n\t * reCOWed by userspace write).\n\t */\n\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n\t\t*flags |= FOLL_COW;\n\treturn 0;\n}",
        "code_after_change": "static int faultin_page(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, bool unshare,\n\t\tint *locked)\n{\n\tunsigned int fault_flags = 0;\n\tvm_fault_t ret;\n\n\tif (*flags & FOLL_NOFAULT)\n\t\treturn -EFAULT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (locked)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\t/*\n\t\t * Note: FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_TRIED\n\t\t * can co-exist\n\t\t */\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\tif (unshare) {\n\t\tfault_flags |= FAULT_FLAG_UNSHARE;\n\t\t/* FAULT_FLAG_WRITE and FAULT_FLAG_UNSHARE are incompatible */\n\t\tVM_BUG_ON(fault_flags & FAULT_FLAG_WRITE);\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags, NULL);\n\n\tif (ret & VM_FAULT_COMPLETED) {\n\t\t/*\n\t\t * With FAULT_FLAG_RETRY_NOWAIT we'll never release the\n\t\t * mmap lock in the page fault handler. Sanity check this.\n\t\t */\n\t\tWARN_ON_ONCE(fault_flags & FAULT_FLAG_RETRY_NOWAIT);\n\t\tif (locked)\n\t\t\t*locked = 0;\n\t\t/*\n\t\t * We should do the same as VM_FAULT_RETRY, but let's not\n\t\t * return -EBUSY since that's not reflecting the reality of\n\t\t * what has happened - we've just fully completed a page\n\t\t * fault, with the mmap lock released.  Use -EAGAIN to show\n\t\t * that we want to take the mmap lock _again_.\n\t\t */\n\t\treturn -EAGAIN;\n\t}\n\n\tif (ret & VM_FAULT_ERROR) {\n\t\tint err = vm_fault_to_errno(ret, *flags);\n\n\t\tif (err)\n\t\t\treturn err;\n\t\tBUG();\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (locked && !(fault_flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\t\t*locked = 0;\n\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -62,16 +62,5 @@\n \t\treturn -EBUSY;\n \t}\n \n-\t/*\n-\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n-\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n-\t * can thus safely do subsequent page lookups as if they were reads.\n-\t * But only do so when looping for pte_write is futile: in some cases\n-\t * userspace may also be wanting to write to the gotten user page,\n-\t * which a read fault here might prevent (a readonly page might get\n-\t * reCOWed by userspace write).\n-\t */\n-\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n-\t\t*flags |= FOLL_COW;\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t/*",
                "\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when",
                "\t * necessary, even if maybe_mkwrite decided not to set pte_write. We",
                "\t * can thus safely do subsequent page lookups as if they were reads.",
                "\t * But only do so when looping for pte_write is futile: in some cases",
                "\t * userspace may also be wanting to write to the gotten user page,",
                "\t * which a read fault here might prevent (a readonly page might get",
                "\t * reCOWed by userspace write).",
                "\t */",
                "\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))",
                "\t\t*flags |= FOLL_COW;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the way the Linux kernel's memory subsystem handled the copy-on-write (COW) breakage of private read-only shared memory mappings. This flaw allows an unprivileged, local user to gain write access to read-only memory mappings, increasing their privileges on the system."
    },
    {
        "cve_id": "CVE-2022-2590",
        "code_before_change": "struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\tpmd_t *pmd, int flags, struct dev_pagemap **pgmap)\n{\n\tunsigned long pfn = pmd_pfn(*pmd);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\t/*\n\t * When we COW a devmap PMD entry, we split it into PTEs, so we should\n\t * not be in this function with `flags & FOLL_COW` set.\n\t */\n\tWARN_ONCE(flags & FOLL_COW, \"mm: In follow_devmap_pmd with FOLL_COW set\");\n\n\t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn NULL;\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\treturn NULL;\n\n\tif (pmd_present(*pmd) && pmd_devmap(*pmd))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd, flags & FOLL_WRITE);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & (FOLL_GET | FOLL_PIN)))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PMD_MASK) >> PAGE_SHIFT;\n\t*pgmap = get_dev_pagemap(pfn, *pgmap);\n\tif (!*pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tif (!try_grab_page(page, flags))\n\t\tpage = ERR_PTR(-ENOMEM);\n\n\treturn page;\n}",
        "code_after_change": "struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\tpmd_t *pmd, int flags, struct dev_pagemap **pgmap)\n{\n\tunsigned long pfn = pmd_pfn(*pmd);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn NULL;\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\treturn NULL;\n\n\tif (pmd_present(*pmd) && pmd_devmap(*pmd))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd, flags & FOLL_WRITE);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & (FOLL_GET | FOLL_PIN)))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PMD_MASK) >> PAGE_SHIFT;\n\t*pgmap = get_dev_pagemap(pfn, *pgmap);\n\tif (!*pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tif (!try_grab_page(page, flags))\n\t\tpage = ERR_PTR(-ENOMEM);\n\n\treturn page;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,12 +6,6 @@\n \tstruct page *page;\n \n \tassert_spin_locked(pmd_lockptr(mm, pmd));\n-\n-\t/*\n-\t * When we COW a devmap PMD entry, we split it into PTEs, so we should\n-\t * not be in this function with `flags & FOLL_COW` set.\n-\t */\n-\tWARN_ONCE(flags & FOLL_COW, \"mm: In follow_devmap_pmd with FOLL_COW set\");\n \n \t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n \tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\t/*",
                "\t * When we COW a devmap PMD entry, we split it into PTEs, so we should",
                "\t * not be in this function with `flags & FOLL_COW` set.",
                "\t */",
                "\tWARN_ONCE(flags & FOLL_COW, \"mm: In follow_devmap_pmd with FOLL_COW set\");"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the way the Linux kernel's memory subsystem handled the copy-on-write (COW) breakage of private read-only shared memory mappings. This flaw allows an unprivileged, local user to gain write access to read-only memory mappings, increasing their privileges on the system."
    },
    {
        "cve_id": "CVE-2022-2590",
        "code_before_change": "struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long addr,\n\t\t\t\t   pmd_t *pmd,\n\t\t\t\t   unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page = NULL;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\tif (flags & FOLL_WRITE && !can_follow_write_pmd(*pmd, flags))\n\t\tgoto out;\n\n\t/* Avoid dumping huge zero page */\n\tif ((flags & FOLL_DUMP) && is_huge_zero_pmd(*pmd))\n\t\treturn ERR_PTR(-EFAULT);\n\n\t/* Full NUMA hinting faults to serialise migration in fault paths */\n\tif ((flags & FOLL_NUMA) && pmd_protnone(*pmd))\n\t\tgoto out;\n\n\tpage = pmd_page(*pmd);\n\tVM_BUG_ON_PAGE(!PageHead(page) && !is_zone_device_page(page), page);\n\n\tif (!pmd_write(*pmd) && gup_must_unshare(flags, page))\n\t\treturn ERR_PTR(-EMLINK);\n\n\tVM_BUG_ON_PAGE((flags & FOLL_PIN) && PageAnon(page) &&\n\t\t\t!PageAnonExclusive(page), page);\n\n\tif (!try_grab_page(page, flags))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd, flags & FOLL_WRITE);\n\n\tpage += (addr & ~HPAGE_PMD_MASK) >> PAGE_SHIFT;\n\tVM_BUG_ON_PAGE(!PageCompound(page) && !is_zone_device_page(page), page);\n\nout:\n\treturn page;\n}",
        "code_after_change": "struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long addr,\n\t\t\t\t   pmd_t *pmd,\n\t\t\t\t   unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\tpage = pmd_page(*pmd);\n\tVM_BUG_ON_PAGE(!PageHead(page) && !is_zone_device_page(page), page);\n\n\tif ((flags & FOLL_WRITE) &&\n\t    !can_follow_write_pmd(*pmd, page, vma, flags))\n\t\treturn NULL;\n\n\t/* Avoid dumping huge zero page */\n\tif ((flags & FOLL_DUMP) && is_huge_zero_pmd(*pmd))\n\t\treturn ERR_PTR(-EFAULT);\n\n\t/* Full NUMA hinting faults to serialise migration in fault paths */\n\tif ((flags & FOLL_NUMA) && pmd_protnone(*pmd))\n\t\treturn NULL;\n\n\tif (!pmd_write(*pmd) && gup_must_unshare(flags, page))\n\t\treturn ERR_PTR(-EMLINK);\n\n\tVM_BUG_ON_PAGE((flags & FOLL_PIN) && PageAnon(page) &&\n\t\t\t!PageAnonExclusive(page), page);\n\n\tif (!try_grab_page(page, flags))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd, flags & FOLL_WRITE);\n\n\tpage += (addr & ~HPAGE_PMD_MASK) >> PAGE_SHIFT;\n\tVM_BUG_ON_PAGE(!PageCompound(page) && !is_zone_device_page(page), page);\n\n\treturn page;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,12 +4,16 @@\n \t\t\t\t   unsigned int flags)\n {\n \tstruct mm_struct *mm = vma->vm_mm;\n-\tstruct page *page = NULL;\n+\tstruct page *page;\n \n \tassert_spin_locked(pmd_lockptr(mm, pmd));\n \n-\tif (flags & FOLL_WRITE && !can_follow_write_pmd(*pmd, flags))\n-\t\tgoto out;\n+\tpage = pmd_page(*pmd);\n+\tVM_BUG_ON_PAGE(!PageHead(page) && !is_zone_device_page(page), page);\n+\n+\tif ((flags & FOLL_WRITE) &&\n+\t    !can_follow_write_pmd(*pmd, page, vma, flags))\n+\t\treturn NULL;\n \n \t/* Avoid dumping huge zero page */\n \tif ((flags & FOLL_DUMP) && is_huge_zero_pmd(*pmd))\n@@ -17,10 +21,7 @@\n \n \t/* Full NUMA hinting faults to serialise migration in fault paths */\n \tif ((flags & FOLL_NUMA) && pmd_protnone(*pmd))\n-\t\tgoto out;\n-\n-\tpage = pmd_page(*pmd);\n-\tVM_BUG_ON_PAGE(!PageHead(page) && !is_zone_device_page(page), page);\n+\t\treturn NULL;\n \n \tif (!pmd_write(*pmd) && gup_must_unshare(flags, page))\n \t\treturn ERR_PTR(-EMLINK);\n@@ -37,6 +38,5 @@\n \tpage += (addr & ~HPAGE_PMD_MASK) >> PAGE_SHIFT;\n \tVM_BUG_ON_PAGE(!PageCompound(page) && !is_zone_device_page(page), page);\n \n-out:\n \treturn page;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct page *page;",
                "\tpage = pmd_page(*pmd);",
                "\tVM_BUG_ON_PAGE(!PageHead(page) && !is_zone_device_page(page), page);",
                "",
                "\tif ((flags & FOLL_WRITE) &&",
                "\t    !can_follow_write_pmd(*pmd, page, vma, flags))",
                "\t\treturn NULL;",
                "\t\treturn NULL;"
            ],
            "deleted": [
                "\tstruct page *page = NULL;",
                "\tif (flags & FOLL_WRITE && !can_follow_write_pmd(*pmd, flags))",
                "\t\tgoto out;",
                "\t\tgoto out;",
                "",
                "\tpage = pmd_page(*pmd);",
                "\tVM_BUG_ON_PAGE(!PageHead(page) && !is_zone_device_page(page), page);",
                "out:"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the way the Linux kernel's memory subsystem handled the copy-on-write (COW) breakage of private read-only shared memory mappings. This flaw allows an unprivileged, local user to gain write access to read-only memory mappings, increasing their privileges on the system."
    },
    {
        "cve_id": "CVE-2022-28796",
        "code_before_change": "void jbd2_journal_lock_updates(journal_t *journal)\n{\n\tDEFINE_WAIT(wait);\n\n\tjbd2_might_wait_for_commit(journal);\n\n\twrite_lock(&journal->j_state_lock);\n\t++journal->j_barrier_count;\n\n\t/* Wait until there are no reserved handles */\n\tif (atomic_read(&journal->j_reserved_credits)) {\n\t\twrite_unlock(&journal->j_state_lock);\n\t\twait_event(journal->j_wait_reserved,\n\t\t\t   atomic_read(&journal->j_reserved_credits) == 0);\n\t\twrite_lock(&journal->j_state_lock);\n\t}\n\n\t/* Wait until there are no running t_updates */\n\tjbd2_journal_wait_updates(journal);\n\n\twrite_unlock(&journal->j_state_lock);\n\n\t/*\n\t * We have now established a barrier against other normal updates, but\n\t * we also need to barrier against other jbd2_journal_lock_updates() calls\n\t * to make sure that we serialise special journal-locked operations\n\t * too.\n\t */\n\tmutex_lock(&journal->j_barrier);\n}",
        "code_after_change": "void jbd2_journal_lock_updates(journal_t *journal)\n{\n\tjbd2_might_wait_for_commit(journal);\n\n\twrite_lock(&journal->j_state_lock);\n\t++journal->j_barrier_count;\n\n\t/* Wait until there are no reserved handles */\n\tif (atomic_read(&journal->j_reserved_credits)) {\n\t\twrite_unlock(&journal->j_state_lock);\n\t\twait_event(journal->j_wait_reserved,\n\t\t\t   atomic_read(&journal->j_reserved_credits) == 0);\n\t\twrite_lock(&journal->j_state_lock);\n\t}\n\n\t/* Wait until there are no running t_updates */\n\tjbd2_journal_wait_updates(journal);\n\n\twrite_unlock(&journal->j_state_lock);\n\n\t/*\n\t * We have now established a barrier against other normal updates, but\n\t * we also need to barrier against other jbd2_journal_lock_updates() calls\n\t * to make sure that we serialise special journal-locked operations\n\t * too.\n\t */\n\tmutex_lock(&journal->j_barrier);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,5 @@\n void jbd2_journal_lock_updates(journal_t *journal)\n {\n-\tDEFINE_WAIT(wait);\n-\n \tjbd2_might_wait_for_commit(journal);\n \n \twrite_lock(&journal->j_state_lock);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tDEFINE_WAIT(wait);",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "jbd2_journal_wait_updates in fs/jbd2/transaction.c in the Linux kernel before 5.17.1 has a use-after-free caused by a transaction_t race condition."
    },
    {
        "cve_id": "CVE-2022-28796",
        "code_before_change": "void jbd2_journal_wait_updates(journal_t *journal)\n{\n\ttransaction_t *commit_transaction = journal->j_running_transaction;\n\n\tif (!commit_transaction)\n\t\treturn;\n\n\tspin_lock(&commit_transaction->t_handle_lock);\n\twhile (atomic_read(&commit_transaction->t_updates)) {\n\t\tDEFINE_WAIT(wait);\n\n\t\tprepare_to_wait(&journal->j_wait_updates, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tif (atomic_read(&commit_transaction->t_updates)) {\n\t\t\tspin_unlock(&commit_transaction->t_handle_lock);\n\t\t\twrite_unlock(&journal->j_state_lock);\n\t\t\tschedule();\n\t\t\twrite_lock(&journal->j_state_lock);\n\t\t\tspin_lock(&commit_transaction->t_handle_lock);\n\t\t}\n\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t}\n\tspin_unlock(&commit_transaction->t_handle_lock);\n}",
        "code_after_change": "void jbd2_journal_wait_updates(journal_t *journal)\n{\n\tDEFINE_WAIT(wait);\n\n\twhile (1) {\n\t\t/*\n\t\t * Note that the running transaction can get freed under us if\n\t\t * this transaction is getting committed in\n\t\t * jbd2_journal_commit_transaction() ->\n\t\t * jbd2_journal_free_transaction(). This can only happen when we\n\t\t * release j_state_lock -> schedule() -> acquire j_state_lock.\n\t\t * Hence we should everytime retrieve new j_running_transaction\n\t\t * value (after j_state_lock release acquire cycle), else it may\n\t\t * lead to use-after-free of old freed transaction.\n\t\t */\n\t\ttransaction_t *transaction = journal->j_running_transaction;\n\n\t\tif (!transaction)\n\t\t\tbreak;\n\n\t\tspin_lock(&transaction->t_handle_lock);\n\t\tprepare_to_wait(&journal->j_wait_updates, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tif (!atomic_read(&transaction->t_updates)) {\n\t\t\tspin_unlock(&transaction->t_handle_lock);\n\t\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&transaction->t_handle_lock);\n\t\twrite_unlock(&journal->j_state_lock);\n\t\tschedule();\n\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t\twrite_lock(&journal->j_state_lock);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,24 +1,35 @@\n void jbd2_journal_wait_updates(journal_t *journal)\n {\n-\ttransaction_t *commit_transaction = journal->j_running_transaction;\n+\tDEFINE_WAIT(wait);\n \n-\tif (!commit_transaction)\n-\t\treturn;\n+\twhile (1) {\n+\t\t/*\n+\t\t * Note that the running transaction can get freed under us if\n+\t\t * this transaction is getting committed in\n+\t\t * jbd2_journal_commit_transaction() ->\n+\t\t * jbd2_journal_free_transaction(). This can only happen when we\n+\t\t * release j_state_lock -> schedule() -> acquire j_state_lock.\n+\t\t * Hence we should everytime retrieve new j_running_transaction\n+\t\t * value (after j_state_lock release acquire cycle), else it may\n+\t\t * lead to use-after-free of old freed transaction.\n+\t\t */\n+\t\ttransaction_t *transaction = journal->j_running_transaction;\n \n-\tspin_lock(&commit_transaction->t_handle_lock);\n-\twhile (atomic_read(&commit_transaction->t_updates)) {\n-\t\tDEFINE_WAIT(wait);\n+\t\tif (!transaction)\n+\t\t\tbreak;\n \n+\t\tspin_lock(&transaction->t_handle_lock);\n \t\tprepare_to_wait(&journal->j_wait_updates, &wait,\n-\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n-\t\tif (atomic_read(&commit_transaction->t_updates)) {\n-\t\t\tspin_unlock(&commit_transaction->t_handle_lock);\n-\t\t\twrite_unlock(&journal->j_state_lock);\n-\t\t\tschedule();\n-\t\t\twrite_lock(&journal->j_state_lock);\n-\t\t\tspin_lock(&commit_transaction->t_handle_lock);\n+\t\t\t\tTASK_UNINTERRUPTIBLE);\n+\t\tif (!atomic_read(&transaction->t_updates)) {\n+\t\t\tspin_unlock(&transaction->t_handle_lock);\n+\t\t\tfinish_wait(&journal->j_wait_updates, &wait);\n+\t\t\tbreak;\n \t\t}\n+\t\tspin_unlock(&transaction->t_handle_lock);\n+\t\twrite_unlock(&journal->j_state_lock);\n+\t\tschedule();\n \t\tfinish_wait(&journal->j_wait_updates, &wait);\n+\t\twrite_lock(&journal->j_state_lock);\n \t}\n-\tspin_unlock(&commit_transaction->t_handle_lock);\n }",
        "function_modified_lines": {
            "added": [
                "\tDEFINE_WAIT(wait);",
                "\twhile (1) {",
                "\t\t/*",
                "\t\t * Note that the running transaction can get freed under us if",
                "\t\t * this transaction is getting committed in",
                "\t\t * jbd2_journal_commit_transaction() ->",
                "\t\t * jbd2_journal_free_transaction(). This can only happen when we",
                "\t\t * release j_state_lock -> schedule() -> acquire j_state_lock.",
                "\t\t * Hence we should everytime retrieve new j_running_transaction",
                "\t\t * value (after j_state_lock release acquire cycle), else it may",
                "\t\t * lead to use-after-free of old freed transaction.",
                "\t\t */",
                "\t\ttransaction_t *transaction = journal->j_running_transaction;",
                "\t\tif (!transaction)",
                "\t\t\tbreak;",
                "\t\tspin_lock(&transaction->t_handle_lock);",
                "\t\t\t\tTASK_UNINTERRUPTIBLE);",
                "\t\tif (!atomic_read(&transaction->t_updates)) {",
                "\t\t\tspin_unlock(&transaction->t_handle_lock);",
                "\t\t\tfinish_wait(&journal->j_wait_updates, &wait);",
                "\t\t\tbreak;",
                "\t\tspin_unlock(&transaction->t_handle_lock);",
                "\t\twrite_unlock(&journal->j_state_lock);",
                "\t\tschedule();",
                "\t\twrite_lock(&journal->j_state_lock);"
            ],
            "deleted": [
                "\ttransaction_t *commit_transaction = journal->j_running_transaction;",
                "\tif (!commit_transaction)",
                "\t\treturn;",
                "\tspin_lock(&commit_transaction->t_handle_lock);",
                "\twhile (atomic_read(&commit_transaction->t_updates)) {",
                "\t\tDEFINE_WAIT(wait);",
                "\t\t\t\t\tTASK_UNINTERRUPTIBLE);",
                "\t\tif (atomic_read(&commit_transaction->t_updates)) {",
                "\t\t\tspin_unlock(&commit_transaction->t_handle_lock);",
                "\t\t\twrite_unlock(&journal->j_state_lock);",
                "\t\t\tschedule();",
                "\t\t\twrite_lock(&journal->j_state_lock);",
                "\t\t\tspin_lock(&commit_transaction->t_handle_lock);",
                "\tspin_unlock(&commit_transaction->t_handle_lock);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "jbd2_journal_wait_updates in fs/jbd2/transaction.c in the Linux kernel before 5.17.1 has a use-after-free caused by a transaction_t race condition."
    },
    {
        "cve_id": "CVE-2022-29582",
        "code_before_change": "static __cold void io_flush_timeouts(struct io_ring_ctx *ctx)\n\t__must_hold(&ctx->completion_lock)\n{\n\tu32 seq = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n\n\tspin_lock_irq(&ctx->timeout_lock);\n\twhile (!list_empty(&ctx->timeout_list)) {\n\t\tu32 events_needed, events_got;\n\t\tstruct io_kiocb *req = list_first_entry(&ctx->timeout_list,\n\t\t\t\t\t\tstruct io_kiocb, timeout.list);\n\n\t\tif (io_is_timeout_noseq(req))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Since seq can easily wrap around over time, subtract\n\t\t * the last seq at which timeouts were flushed before comparing.\n\t\t * Assuming not more than 2^31-1 events have happened since,\n\t\t * these subtractions won't have wrapped, so we can check if\n\t\t * target is in [last_seq, current_seq] by comparing the two.\n\t\t */\n\t\tevents_needed = req->timeout.target_seq - ctx->cq_last_tm_flush;\n\t\tevents_got = seq - ctx->cq_last_tm_flush;\n\t\tif (events_got < events_needed)\n\t\t\tbreak;\n\n\t\tlist_del_init(&req->timeout.list);\n\t\tio_kill_timeout(req, 0);\n\t}\n\tctx->cq_last_tm_flush = seq;\n\tspin_unlock_irq(&ctx->timeout_lock);\n}",
        "code_after_change": "static __cold void io_flush_timeouts(struct io_ring_ctx *ctx)\n\t__must_hold(&ctx->completion_lock)\n{\n\tu32 seq = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n\tstruct io_kiocb *req, *tmp;\n\n\tspin_lock_irq(&ctx->timeout_lock);\n\tlist_for_each_entry_safe(req, tmp, &ctx->timeout_list, timeout.list) {\n\t\tu32 events_needed, events_got;\n\n\t\tif (io_is_timeout_noseq(req))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Since seq can easily wrap around over time, subtract\n\t\t * the last seq at which timeouts were flushed before comparing.\n\t\t * Assuming not more than 2^31-1 events have happened since,\n\t\t * these subtractions won't have wrapped, so we can check if\n\t\t * target is in [last_seq, current_seq] by comparing the two.\n\t\t */\n\t\tevents_needed = req->timeout.target_seq - ctx->cq_last_tm_flush;\n\t\tevents_got = seq - ctx->cq_last_tm_flush;\n\t\tif (events_got < events_needed)\n\t\t\tbreak;\n\n\t\tio_kill_timeout(req, 0);\n\t}\n\tctx->cq_last_tm_flush = seq;\n\tspin_unlock_irq(&ctx->timeout_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,12 +2,11 @@\n \t__must_hold(&ctx->completion_lock)\n {\n \tu32 seq = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n+\tstruct io_kiocb *req, *tmp;\n \n \tspin_lock_irq(&ctx->timeout_lock);\n-\twhile (!list_empty(&ctx->timeout_list)) {\n+\tlist_for_each_entry_safe(req, tmp, &ctx->timeout_list, timeout.list) {\n \t\tu32 events_needed, events_got;\n-\t\tstruct io_kiocb *req = list_first_entry(&ctx->timeout_list,\n-\t\t\t\t\t\tstruct io_kiocb, timeout.list);\n \n \t\tif (io_is_timeout_noseq(req))\n \t\t\tbreak;\n@@ -24,7 +23,6 @@\n \t\tif (events_got < events_needed)\n \t\t\tbreak;\n \n-\t\tlist_del_init(&req->timeout.list);\n \t\tio_kill_timeout(req, 0);\n \t}\n \tctx->cq_last_tm_flush = seq;",
        "function_modified_lines": {
            "added": [
                "\tstruct io_kiocb *req, *tmp;",
                "\tlist_for_each_entry_safe(req, tmp, &ctx->timeout_list, timeout.list) {"
            ],
            "deleted": [
                "\twhile (!list_empty(&ctx->timeout_list)) {",
                "\t\tstruct io_kiocb *req = list_first_entry(&ctx->timeout_list,",
                "\t\t\t\t\t\tstruct io_kiocb, timeout.list);",
                "\t\tlist_del_init(&req->timeout.list);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux kernel before 5.17.3, fs/io_uring.c has a use-after-free due to a race condition in io_uring timeouts. This can be triggered by a local user who has no access to any user namespace; however, the race condition perhaps can only be exploited infrequently."
    },
    {
        "cve_id": "CVE-2022-29582",
        "code_before_change": "static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t   bool is_timeout_link)\n{\n\tstruct io_timeout_data *data;\n\tunsigned flags;\n\tu32 off = READ_ONCE(sqe->off);\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len != 1 ||\n\t    sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\tif (off && is_timeout_link)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->timeout_flags);\n\tif (flags & ~(IORING_TIMEOUT_ABS | IORING_TIMEOUT_CLOCK_MASK |\n\t\t      IORING_TIMEOUT_ETIME_SUCCESS))\n\t\treturn -EINVAL;\n\t/* more than one clock specified is invalid, obviously */\n\tif (hweight32(flags & IORING_TIMEOUT_CLOCK_MASK) > 1)\n\t\treturn -EINVAL;\n\n\tINIT_LIST_HEAD(&req->timeout.list);\n\treq->timeout.off = off;\n\tif (unlikely(off && !req->ctx->off_timeout_used))\n\t\treq->ctx->off_timeout_used = true;\n\n\tif (WARN_ON_ONCE(req_has_async_data(req)))\n\t\treturn -EFAULT;\n\tif (io_alloc_async_data(req))\n\t\treturn -ENOMEM;\n\n\tdata = req->async_data;\n\tdata->req = req;\n\tdata->flags = flags;\n\n\tif (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))\n\t\treturn -EFAULT;\n\n\tif (data->ts.tv_sec < 0 || data->ts.tv_nsec < 0)\n\t\treturn -EINVAL;\n\n\tdata->mode = io_translate_timeout_mode(flags);\n\thrtimer_init(&data->timer, io_timeout_get_clock(data), data->mode);\n\n\tif (is_timeout_link) {\n\t\tstruct io_submit_link *link = &req->ctx->submit_state.link;\n\n\t\tif (!link->head)\n\t\t\treturn -EINVAL;\n\t\tif (link->last->opcode == IORING_OP_LINK_TIMEOUT)\n\t\t\treturn -EINVAL;\n\t\treq->timeout.head = link->last;\n\t\tlink->last->flags |= REQ_F_ARM_LTIMEOUT;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t   bool is_timeout_link)\n{\n\tstruct io_timeout_data *data;\n\tunsigned flags;\n\tu32 off = READ_ONCE(sqe->off);\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len != 1 ||\n\t    sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\tif (off && is_timeout_link)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->timeout_flags);\n\tif (flags & ~(IORING_TIMEOUT_ABS | IORING_TIMEOUT_CLOCK_MASK |\n\t\t      IORING_TIMEOUT_ETIME_SUCCESS))\n\t\treturn -EINVAL;\n\t/* more than one clock specified is invalid, obviously */\n\tif (hweight32(flags & IORING_TIMEOUT_CLOCK_MASK) > 1)\n\t\treturn -EINVAL;\n\n\tINIT_LIST_HEAD(&req->timeout.list);\n\treq->timeout.off = off;\n\tif (unlikely(off && !req->ctx->off_timeout_used))\n\t\treq->ctx->off_timeout_used = true;\n\n\tif (WARN_ON_ONCE(req_has_async_data(req)))\n\t\treturn -EFAULT;\n\tif (io_alloc_async_data(req))\n\t\treturn -ENOMEM;\n\n\tdata = req->async_data;\n\tdata->req = req;\n\tdata->flags = flags;\n\n\tif (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))\n\t\treturn -EFAULT;\n\n\tif (data->ts.tv_sec < 0 || data->ts.tv_nsec < 0)\n\t\treturn -EINVAL;\n\n\tINIT_LIST_HEAD(&req->timeout.list);\n\tdata->mode = io_translate_timeout_mode(flags);\n\thrtimer_init(&data->timer, io_timeout_get_clock(data), data->mode);\n\n\tif (is_timeout_link) {\n\t\tstruct io_submit_link *link = &req->ctx->submit_state.link;\n\n\t\tif (!link->head)\n\t\t\treturn -EINVAL;\n\t\tif (link->last->opcode == IORING_OP_LINK_TIMEOUT)\n\t\t\treturn -EINVAL;\n\t\treq->timeout.head = link->last;\n\t\tlink->last->flags |= REQ_F_ARM_LTIMEOUT;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -40,6 +40,7 @@\n \tif (data->ts.tv_sec < 0 || data->ts.tv_nsec < 0)\n \t\treturn -EINVAL;\n \n+\tINIT_LIST_HEAD(&req->timeout.list);\n \tdata->mode = io_translate_timeout_mode(flags);\n \thrtimer_init(&data->timer, io_timeout_get_clock(data), data->mode);\n ",
        "function_modified_lines": {
            "added": [
                "\tINIT_LIST_HEAD(&req->timeout.list);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux kernel before 5.17.3, fs/io_uring.c has a use-after-free due to a race condition in io_uring timeouts. This can be triggered by a local user who has no access to any user namespace; however, the race condition perhaps can only be exploited infrequently."
    },
    {
        "cve_id": "CVE-2022-2959",
        "code_before_change": "int pipe_resize_ring(struct pipe_inode_info *pipe, unsigned int nr_slots)\n{\n\tstruct pipe_buffer *bufs;\n\tunsigned int head, tail, mask, n;\n\n\t/*\n\t * We can shrink the pipe, if arg is greater than the ring occupancy.\n\t * Since we don't expect a lot of shrink+grow operations, just free and\n\t * allocate again like we would do for growing.  If the pipe currently\n\t * contains more buffers than arg, then return busy.\n\t */\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\tn = pipe_occupancy(pipe->head, pipe->tail);\n\tif (nr_slots < n)\n\t\treturn -EBUSY;\n\n\tbufs = kcalloc(nr_slots, sizeof(*bufs),\n\t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n\tif (unlikely(!bufs))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * The pipe array wraps around, so just start the new one at zero\n\t * and adjust the indices.\n\t */\n\tif (n > 0) {\n\t\tunsigned int h = head & mask;\n\t\tunsigned int t = tail & mask;\n\t\tif (h > t) {\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       n * sizeof(struct pipe_buffer));\n\t\t} else {\n\t\t\tunsigned int tsize = pipe->ring_size - t;\n\t\t\tif (h > 0)\n\t\t\t\tmemcpy(bufs + tsize, pipe->bufs,\n\t\t\t\t       h * sizeof(struct pipe_buffer));\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       tsize * sizeof(struct pipe_buffer));\n\t\t}\n\t}\n\n\thead = n;\n\ttail = 0;\n\n\tkfree(pipe->bufs);\n\tpipe->bufs = bufs;\n\tpipe->ring_size = nr_slots;\n\tif (pipe->max_usage > nr_slots)\n\t\tpipe->max_usage = nr_slots;\n\tpipe->tail = tail;\n\tpipe->head = head;\n\n\t/* This might have made more room for writers */\n\twake_up_interruptible(&pipe->wr_wait);\n\treturn 0;\n}",
        "code_after_change": "int pipe_resize_ring(struct pipe_inode_info *pipe, unsigned int nr_slots)\n{\n\tstruct pipe_buffer *bufs;\n\tunsigned int head, tail, mask, n;\n\n\tbufs = kcalloc(nr_slots, sizeof(*bufs),\n\t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n\tif (unlikely(!bufs))\n\t\treturn -ENOMEM;\n\n\tspin_lock_irq(&pipe->rd_wait.lock);\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\n\tn = pipe_occupancy(head, tail);\n\tif (nr_slots < n) {\n\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\tkfree(bufs);\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The pipe array wraps around, so just start the new one at zero\n\t * and adjust the indices.\n\t */\n\tif (n > 0) {\n\t\tunsigned int h = head & mask;\n\t\tunsigned int t = tail & mask;\n\t\tif (h > t) {\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       n * sizeof(struct pipe_buffer));\n\t\t} else {\n\t\t\tunsigned int tsize = pipe->ring_size - t;\n\t\t\tif (h > 0)\n\t\t\t\tmemcpy(bufs + tsize, pipe->bufs,\n\t\t\t\t       h * sizeof(struct pipe_buffer));\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       tsize * sizeof(struct pipe_buffer));\n\t\t}\n\t}\n\n\thead = n;\n\ttail = 0;\n\n\tkfree(pipe->bufs);\n\tpipe->bufs = bufs;\n\tpipe->ring_size = nr_slots;\n\tif (pipe->max_usage > nr_slots)\n\t\tpipe->max_usage = nr_slots;\n\tpipe->tail = tail;\n\tpipe->head = head;\n\n\tspin_unlock_irq(&pipe->rd_wait.lock);\n\n\t/* This might have made more room for writers */\n\twake_up_interruptible(&pipe->wr_wait);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,23 +3,22 @@\n \tstruct pipe_buffer *bufs;\n \tunsigned int head, tail, mask, n;\n \n-\t/*\n-\t * We can shrink the pipe, if arg is greater than the ring occupancy.\n-\t * Since we don't expect a lot of shrink+grow operations, just free and\n-\t * allocate again like we would do for growing.  If the pipe currently\n-\t * contains more buffers than arg, then return busy.\n-\t */\n-\tmask = pipe->ring_size - 1;\n-\thead = pipe->head;\n-\ttail = pipe->tail;\n-\tn = pipe_occupancy(pipe->head, pipe->tail);\n-\tif (nr_slots < n)\n-\t\treturn -EBUSY;\n-\n \tbufs = kcalloc(nr_slots, sizeof(*bufs),\n \t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n \tif (unlikely(!bufs))\n \t\treturn -ENOMEM;\n+\n+\tspin_lock_irq(&pipe->rd_wait.lock);\n+\tmask = pipe->ring_size - 1;\n+\thead = pipe->head;\n+\ttail = pipe->tail;\n+\n+\tn = pipe_occupancy(head, tail);\n+\tif (nr_slots < n) {\n+\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n+\t\tkfree(bufs);\n+\t\treturn -EBUSY;\n+\t}\n \n \t/*\n \t * The pipe array wraps around, so just start the new one at zero\n@@ -52,6 +51,8 @@\n \tpipe->tail = tail;\n \tpipe->head = head;\n \n+\tspin_unlock_irq(&pipe->rd_wait.lock);\n+\n \t/* This might have made more room for writers */\n \twake_up_interruptible(&pipe->wr_wait);\n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "",
                "\tspin_lock_irq(&pipe->rd_wait.lock);",
                "\tmask = pipe->ring_size - 1;",
                "\thead = pipe->head;",
                "\ttail = pipe->tail;",
                "",
                "\tn = pipe_occupancy(head, tail);",
                "\tif (nr_slots < n) {",
                "\t\tspin_unlock_irq(&pipe->rd_wait.lock);",
                "\t\tkfree(bufs);",
                "\t\treturn -EBUSY;",
                "\t}",
                "\tspin_unlock_irq(&pipe->rd_wait.lock);",
                ""
            ],
            "deleted": [
                "\t/*",
                "\t * We can shrink the pipe, if arg is greater than the ring occupancy.",
                "\t * Since we don't expect a lot of shrink+grow operations, just free and",
                "\t * allocate again like we would do for growing.  If the pipe currently",
                "\t * contains more buffers than arg, then return busy.",
                "\t */",
                "\tmask = pipe->ring_size - 1;",
                "\thead = pipe->head;",
                "\ttail = pipe->tail;",
                "\tn = pipe_occupancy(pipe->head, pipe->tail);",
                "\tif (nr_slots < n)",
                "\t\treturn -EBUSY;",
                ""
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition was found in the Linux kernel's watch queue due to a missing lock in pipe_resize_ring(). The specific flaw exists within the handling of pipe buffers. The issue results from the lack of proper locking when performing operations on an object. This flaw allows a local user to crash the system or escalate their privileges on the system."
    },
    {
        "cve_id": "CVE-2022-3028",
        "code_before_change": "static int pfkey_register(struct sock *sk, struct sk_buff *skb, const struct sadb_msg *hdr, void * const *ext_hdrs)\n{\n\tstruct pfkey_sock *pfk = pfkey_sk(sk);\n\tstruct sk_buff *supp_skb;\n\n\tif (hdr->sadb_msg_satype > SADB_SATYPE_MAX)\n\t\treturn -EINVAL;\n\n\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC) {\n\t\tif (pfk->registered&(1<<hdr->sadb_msg_satype))\n\t\t\treturn -EEXIST;\n\t\tpfk->registered |= (1<<hdr->sadb_msg_satype);\n\t}\n\n\txfrm_probe_algs();\n\n\tsupp_skb = compose_sadb_supported(hdr, GFP_KERNEL | __GFP_ZERO);\n\tif (!supp_skb) {\n\t\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC)\n\t\t\tpfk->registered &= ~(1<<hdr->sadb_msg_satype);\n\n\t\treturn -ENOBUFS;\n\t}\n\n\tpfkey_broadcast(supp_skb, GFP_KERNEL, BROADCAST_REGISTERED, sk,\n\t\t\tsock_net(sk));\n\treturn 0;\n}",
        "code_after_change": "static int pfkey_register(struct sock *sk, struct sk_buff *skb, const struct sadb_msg *hdr, void * const *ext_hdrs)\n{\n\tstruct pfkey_sock *pfk = pfkey_sk(sk);\n\tstruct sk_buff *supp_skb;\n\n\tif (hdr->sadb_msg_satype > SADB_SATYPE_MAX)\n\t\treturn -EINVAL;\n\n\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC) {\n\t\tif (pfk->registered&(1<<hdr->sadb_msg_satype))\n\t\t\treturn -EEXIST;\n\t\tpfk->registered |= (1<<hdr->sadb_msg_satype);\n\t}\n\n\tmutex_lock(&pfkey_mutex);\n\txfrm_probe_algs();\n\n\tsupp_skb = compose_sadb_supported(hdr, GFP_KERNEL | __GFP_ZERO);\n\tmutex_unlock(&pfkey_mutex);\n\n\tif (!supp_skb) {\n\t\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC)\n\t\t\tpfk->registered &= ~(1<<hdr->sadb_msg_satype);\n\n\t\treturn -ENOBUFS;\n\t}\n\n\tpfkey_broadcast(supp_skb, GFP_KERNEL, BROADCAST_REGISTERED, sk,\n\t\t\tsock_net(sk));\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,9 +12,12 @@\n \t\tpfk->registered |= (1<<hdr->sadb_msg_satype);\n \t}\n \n+\tmutex_lock(&pfkey_mutex);\n \txfrm_probe_algs();\n \n \tsupp_skb = compose_sadb_supported(hdr, GFP_KERNEL | __GFP_ZERO);\n+\tmutex_unlock(&pfkey_mutex);\n+\n \tif (!supp_skb) {\n \t\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC)\n \t\t\tpfk->registered &= ~(1<<hdr->sadb_msg_satype);",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&pfkey_mutex);",
                "\tmutex_unlock(&pfkey_mutex);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-787"
        ],
        "cve_description": "A race condition was found in the Linux kernel's IP framework for transforming packets (XFRM subsystem) when multiple calls to xfrm_probe_algs occurred simultaneously. This flaw could allow a local attacker to potentially trigger an out-of-bounds write or leak kernel heap memory by performing an out-of-bounds read and copying it into a socket."
    },
    {
        "cve_id": "CVE-2022-3521",
        "code_before_change": "static int kcm_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct kcm_sock *kcm;\n\tstruct kcm_mux *mux;\n\tstruct kcm_psock *psock;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tkcm = kcm_sk(sk);\n\tmux = kcm->mux;\n\n\tsock_orphan(sk);\n\tkfree_skb(kcm->seq_skb);\n\n\tlock_sock(sk);\n\t/* Purge queue under lock to avoid race condition with tx_work trying\n\t * to act when queue is nonempty. If tx_work runs after this point\n\t * it will just return.\n\t */\n\t__skb_queue_purge(&sk->sk_write_queue);\n\n\t/* Set tx_stopped. This is checked when psock is bound to a kcm and we\n\t * get a writespace callback. This prevents further work being queued\n\t * from the callback (unbinding the psock occurs after canceling work.\n\t */\n\tkcm->tx_stopped = 1;\n\n\trelease_sock(sk);\n\n\tspin_lock_bh(&mux->lock);\n\tif (kcm->tx_wait) {\n\t\t/* Take of tx_wait list, after this point there should be no way\n\t\t * that a psock will be assigned to this kcm.\n\t\t */\n\t\tlist_del(&kcm->wait_psock_list);\n\t\tkcm->tx_wait = false;\n\t}\n\tspin_unlock_bh(&mux->lock);\n\n\t/* Cancel work. After this point there should be no outside references\n\t * to the kcm socket.\n\t */\n\tcancel_work_sync(&kcm->tx_work);\n\n\tlock_sock(sk);\n\tpsock = kcm->tx_psock;\n\tif (psock) {\n\t\t/* A psock was reserved, so we need to kill it since it\n\t\t * may already have some bytes queued from a message. We\n\t\t * need to do this after removing kcm from tx_wait list.\n\t\t */\n\t\tkcm_abort_tx_psock(psock, EPIPE, false);\n\t\tunreserve_psock(kcm);\n\t}\n\trelease_sock(sk);\n\n\tWARN_ON(kcm->tx_wait);\n\tWARN_ON(kcm->tx_psock);\n\n\tsock->sk = NULL;\n\n\tkcm_done(kcm);\n\n\treturn 0;\n}",
        "code_after_change": "static int kcm_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct kcm_sock *kcm;\n\tstruct kcm_mux *mux;\n\tstruct kcm_psock *psock;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tkcm = kcm_sk(sk);\n\tmux = kcm->mux;\n\n\tlock_sock(sk);\n\tsock_orphan(sk);\n\tkfree_skb(kcm->seq_skb);\n\n\t/* Purge queue under lock to avoid race condition with tx_work trying\n\t * to act when queue is nonempty. If tx_work runs after this point\n\t * it will just return.\n\t */\n\t__skb_queue_purge(&sk->sk_write_queue);\n\n\t/* Set tx_stopped. This is checked when psock is bound to a kcm and we\n\t * get a writespace callback. This prevents further work being queued\n\t * from the callback (unbinding the psock occurs after canceling work.\n\t */\n\tkcm->tx_stopped = 1;\n\n\trelease_sock(sk);\n\n\tspin_lock_bh(&mux->lock);\n\tif (kcm->tx_wait) {\n\t\t/* Take of tx_wait list, after this point there should be no way\n\t\t * that a psock will be assigned to this kcm.\n\t\t */\n\t\tlist_del(&kcm->wait_psock_list);\n\t\tkcm->tx_wait = false;\n\t}\n\tspin_unlock_bh(&mux->lock);\n\n\t/* Cancel work. After this point there should be no outside references\n\t * to the kcm socket.\n\t */\n\tcancel_work_sync(&kcm->tx_work);\n\n\tlock_sock(sk);\n\tpsock = kcm->tx_psock;\n\tif (psock) {\n\t\t/* A psock was reserved, so we need to kill it since it\n\t\t * may already have some bytes queued from a message. We\n\t\t * need to do this after removing kcm from tx_wait list.\n\t\t */\n\t\tkcm_abort_tx_psock(psock, EPIPE, false);\n\t\tunreserve_psock(kcm);\n\t}\n\trelease_sock(sk);\n\n\tWARN_ON(kcm->tx_wait);\n\tWARN_ON(kcm->tx_psock);\n\n\tsock->sk = NULL;\n\n\tkcm_done(kcm);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,10 +11,10 @@\n \tkcm = kcm_sk(sk);\n \tmux = kcm->mux;\n \n+\tlock_sock(sk);\n \tsock_orphan(sk);\n \tkfree_skb(kcm->seq_skb);\n \n-\tlock_sock(sk);\n \t/* Purge queue under lock to avoid race condition with tx_work trying\n \t * to act when queue is nonempty. If tx_work runs after this point\n \t * it will just return.",
        "function_modified_lines": {
            "added": [
                "\tlock_sock(sk);"
            ],
            "deleted": [
                "\tlock_sock(sk);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function kcm_tx_work of the file net/kcm/kcmsock.c of the component kcm. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211018 is the identifier assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3564",
        "code_before_change": "static int l2cap_rx_state_recv(struct l2cap_chan *chan,\n\t\t\t       struct l2cap_ctrl *control,\n\t\t\t       struct sk_buff *skb, u8 event)\n{\n\tint err = 0;\n\tbool skb_in_use = false;\n\n\tBT_DBG(\"chan %p, control %p, skb %p, event %d\", chan, control, skb,\n\t       event);\n\n\tswitch (event) {\n\tcase L2CAP_EV_RECV_IFRAME:\n\t\tswitch (l2cap_classify_txseq(chan, control->txseq)) {\n\t\tcase L2CAP_TXSEQ_EXPECTED:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\n\t\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\t\tBT_DBG(\"Busy, discarding expected seq %d\",\n\t\t\t\t       control->txseq);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tchan->expected_tx_seq = __next_seq(chan,\n\t\t\t\t\t\t\t   control->txseq);\n\n\t\t\tchan->buffer_seq = chan->expected_tx_seq;\n\t\t\tskb_in_use = true;\n\n\t\t\terr = l2cap_reassemble_sdu(chan, skb, control);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\n\t\t\tif (control->final) {\n\t\t\t\tif (!test_and_clear_bit(CONN_REJ_ACT,\n\t\t\t\t\t\t\t&chan->conn_state)) {\n\t\t\t\t\tcontrol->final = 0;\n\t\t\t\t\tl2cap_retransmit_all(chan, control);\n\t\t\t\t\tl2cap_ertm_send(chan);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!test_bit(CONN_LOCAL_BUSY, &chan->conn_state))\n\t\t\t\tl2cap_send_ack(chan);\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_UNEXPECTED:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\n\t\t\t/* Can't issue SREJ frames in the local busy state.\n\t\t\t * Drop this frame, it will be seen as missing\n\t\t\t * when local busy is exited.\n\t\t\t */\n\t\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\t\tBT_DBG(\"Busy, discarding unexpected seq %d\",\n\t\t\t\t       control->txseq);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* There was a gap in the sequence, so an SREJ\n\t\t\t * must be sent for each missing frame.  The\n\t\t\t * current frame is stored for later use.\n\t\t\t */\n\t\t\tskb_queue_tail(&chan->srej_q, skb);\n\t\t\tskb_in_use = true;\n\t\t\tBT_DBG(\"Queued %p (queue len %d)\", skb,\n\t\t\t       skb_queue_len(&chan->srej_q));\n\n\t\t\tclear_bit(CONN_SREJ_ACT, &chan->conn_state);\n\t\t\tl2cap_seq_list_clear(&chan->srej_list);\n\t\t\tl2cap_send_srej(chan, control->txseq);\n\n\t\t\tchan->rx_state = L2CAP_RX_STATE_SREJ_SENT;\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_DUPLICATE:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_INVALID_IGNORE:\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_INVALID:\n\t\tdefault:\n\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase L2CAP_EV_RECV_RR:\n\t\tl2cap_pass_to_tx(chan, control);\n\t\tif (control->final) {\n\t\t\tclear_bit(CONN_REMOTE_BUSY, &chan->conn_state);\n\n\t\t\tif (!test_and_clear_bit(CONN_REJ_ACT, &chan->conn_state) &&\n\t\t\t    !__chan_is_moving(chan)) {\n\t\t\t\tcontrol->final = 0;\n\t\t\t\tl2cap_retransmit_all(chan, control);\n\t\t\t}\n\n\t\t\tl2cap_ertm_send(chan);\n\t\t} else if (control->poll) {\n\t\t\tl2cap_send_i_or_rr_or_rnr(chan);\n\t\t} else {\n\t\t\tif (test_and_clear_bit(CONN_REMOTE_BUSY,\n\t\t\t\t\t       &chan->conn_state) &&\n\t\t\t    chan->unacked_frames)\n\t\t\t\t__set_retrans_timer(chan);\n\n\t\t\tl2cap_ertm_send(chan);\n\t\t}\n\t\tbreak;\n\tcase L2CAP_EV_RECV_RNR:\n\t\tset_bit(CONN_REMOTE_BUSY, &chan->conn_state);\n\t\tl2cap_pass_to_tx(chan, control);\n\t\tif (control && control->poll) {\n\t\t\tset_bit(CONN_SEND_FBIT, &chan->conn_state);\n\t\t\tl2cap_send_rr_or_rnr(chan, 0);\n\t\t}\n\t\t__clear_retrans_timer(chan);\n\t\tl2cap_seq_list_clear(&chan->retrans_list);\n\t\tbreak;\n\tcase L2CAP_EV_RECV_REJ:\n\t\tl2cap_handle_rej(chan, control);\n\t\tbreak;\n\tcase L2CAP_EV_RECV_SREJ:\n\t\tl2cap_handle_srej(chan, control);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (skb && !skb_in_use) {\n\t\tBT_DBG(\"Freeing %p\", skb);\n\t\tkfree_skb(skb);\n\t}\n\n\treturn err;\n}",
        "code_after_change": "static int l2cap_rx_state_recv(struct l2cap_chan *chan,\n\t\t\t       struct l2cap_ctrl *control,\n\t\t\t       struct sk_buff *skb, u8 event)\n{\n\tstruct l2cap_ctrl local_control;\n\tint err = 0;\n\tbool skb_in_use = false;\n\n\tBT_DBG(\"chan %p, control %p, skb %p, event %d\", chan, control, skb,\n\t       event);\n\n\tswitch (event) {\n\tcase L2CAP_EV_RECV_IFRAME:\n\t\tswitch (l2cap_classify_txseq(chan, control->txseq)) {\n\t\tcase L2CAP_TXSEQ_EXPECTED:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\n\t\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\t\tBT_DBG(\"Busy, discarding expected seq %d\",\n\t\t\t\t       control->txseq);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tchan->expected_tx_seq = __next_seq(chan,\n\t\t\t\t\t\t\t   control->txseq);\n\n\t\t\tchan->buffer_seq = chan->expected_tx_seq;\n\t\t\tskb_in_use = true;\n\n\t\t\t/* l2cap_reassemble_sdu may free skb, hence invalidate\n\t\t\t * control, so make a copy in advance to use it after\n\t\t\t * l2cap_reassemble_sdu returns and to avoid the race\n\t\t\t * condition, for example:\n\t\t\t *\n\t\t\t * The current thread calls:\n\t\t\t *   l2cap_reassemble_sdu\n\t\t\t *     chan->ops->recv == l2cap_sock_recv_cb\n\t\t\t *       __sock_queue_rcv_skb\n\t\t\t * Another thread calls:\n\t\t\t *   bt_sock_recvmsg\n\t\t\t *     skb_recv_datagram\n\t\t\t *     skb_free_datagram\n\t\t\t * Then the current thread tries to access control, but\n\t\t\t * it was freed by skb_free_datagram.\n\t\t\t */\n\t\t\tlocal_control = *control;\n\t\t\terr = l2cap_reassemble_sdu(chan, skb, control);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\n\t\t\tif (local_control.final) {\n\t\t\t\tif (!test_and_clear_bit(CONN_REJ_ACT,\n\t\t\t\t\t\t\t&chan->conn_state)) {\n\t\t\t\t\tlocal_control.final = 0;\n\t\t\t\t\tl2cap_retransmit_all(chan, &local_control);\n\t\t\t\t\tl2cap_ertm_send(chan);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!test_bit(CONN_LOCAL_BUSY, &chan->conn_state))\n\t\t\t\tl2cap_send_ack(chan);\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_UNEXPECTED:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\n\t\t\t/* Can't issue SREJ frames in the local busy state.\n\t\t\t * Drop this frame, it will be seen as missing\n\t\t\t * when local busy is exited.\n\t\t\t */\n\t\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\t\tBT_DBG(\"Busy, discarding unexpected seq %d\",\n\t\t\t\t       control->txseq);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* There was a gap in the sequence, so an SREJ\n\t\t\t * must be sent for each missing frame.  The\n\t\t\t * current frame is stored for later use.\n\t\t\t */\n\t\t\tskb_queue_tail(&chan->srej_q, skb);\n\t\t\tskb_in_use = true;\n\t\t\tBT_DBG(\"Queued %p (queue len %d)\", skb,\n\t\t\t       skb_queue_len(&chan->srej_q));\n\n\t\t\tclear_bit(CONN_SREJ_ACT, &chan->conn_state);\n\t\t\tl2cap_seq_list_clear(&chan->srej_list);\n\t\t\tl2cap_send_srej(chan, control->txseq);\n\n\t\t\tchan->rx_state = L2CAP_RX_STATE_SREJ_SENT;\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_DUPLICATE:\n\t\t\tl2cap_pass_to_tx(chan, control);\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_INVALID_IGNORE:\n\t\t\tbreak;\n\t\tcase L2CAP_TXSEQ_INVALID:\n\t\tdefault:\n\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase L2CAP_EV_RECV_RR:\n\t\tl2cap_pass_to_tx(chan, control);\n\t\tif (control->final) {\n\t\t\tclear_bit(CONN_REMOTE_BUSY, &chan->conn_state);\n\n\t\t\tif (!test_and_clear_bit(CONN_REJ_ACT, &chan->conn_state) &&\n\t\t\t    !__chan_is_moving(chan)) {\n\t\t\t\tcontrol->final = 0;\n\t\t\t\tl2cap_retransmit_all(chan, control);\n\t\t\t}\n\n\t\t\tl2cap_ertm_send(chan);\n\t\t} else if (control->poll) {\n\t\t\tl2cap_send_i_or_rr_or_rnr(chan);\n\t\t} else {\n\t\t\tif (test_and_clear_bit(CONN_REMOTE_BUSY,\n\t\t\t\t\t       &chan->conn_state) &&\n\t\t\t    chan->unacked_frames)\n\t\t\t\t__set_retrans_timer(chan);\n\n\t\t\tl2cap_ertm_send(chan);\n\t\t}\n\t\tbreak;\n\tcase L2CAP_EV_RECV_RNR:\n\t\tset_bit(CONN_REMOTE_BUSY, &chan->conn_state);\n\t\tl2cap_pass_to_tx(chan, control);\n\t\tif (control && control->poll) {\n\t\t\tset_bit(CONN_SEND_FBIT, &chan->conn_state);\n\t\t\tl2cap_send_rr_or_rnr(chan, 0);\n\t\t}\n\t\t__clear_retrans_timer(chan);\n\t\tl2cap_seq_list_clear(&chan->retrans_list);\n\t\tbreak;\n\tcase L2CAP_EV_RECV_REJ:\n\t\tl2cap_handle_rej(chan, control);\n\t\tbreak;\n\tcase L2CAP_EV_RECV_SREJ:\n\t\tl2cap_handle_srej(chan, control);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (skb && !skb_in_use) {\n\t\tBT_DBG(\"Freeing %p\", skb);\n\t\tkfree_skb(skb);\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n \t\t\t       struct l2cap_ctrl *control,\n \t\t\t       struct sk_buff *skb, u8 event)\n {\n+\tstruct l2cap_ctrl local_control;\n \tint err = 0;\n \tbool skb_in_use = false;\n \n@@ -26,15 +27,32 @@\n \t\t\tchan->buffer_seq = chan->expected_tx_seq;\n \t\t\tskb_in_use = true;\n \n+\t\t\t/* l2cap_reassemble_sdu may free skb, hence invalidate\n+\t\t\t * control, so make a copy in advance to use it after\n+\t\t\t * l2cap_reassemble_sdu returns and to avoid the race\n+\t\t\t * condition, for example:\n+\t\t\t *\n+\t\t\t * The current thread calls:\n+\t\t\t *   l2cap_reassemble_sdu\n+\t\t\t *     chan->ops->recv == l2cap_sock_recv_cb\n+\t\t\t *       __sock_queue_rcv_skb\n+\t\t\t * Another thread calls:\n+\t\t\t *   bt_sock_recvmsg\n+\t\t\t *     skb_recv_datagram\n+\t\t\t *     skb_free_datagram\n+\t\t\t * Then the current thread tries to access control, but\n+\t\t\t * it was freed by skb_free_datagram.\n+\t\t\t */\n+\t\t\tlocal_control = *control;\n \t\t\terr = l2cap_reassemble_sdu(chan, skb, control);\n \t\t\tif (err)\n \t\t\t\tbreak;\n \n-\t\t\tif (control->final) {\n+\t\t\tif (local_control.final) {\n \t\t\t\tif (!test_and_clear_bit(CONN_REJ_ACT,\n \t\t\t\t\t\t\t&chan->conn_state)) {\n-\t\t\t\t\tcontrol->final = 0;\n-\t\t\t\t\tl2cap_retransmit_all(chan, control);\n+\t\t\t\t\tlocal_control.final = 0;\n+\t\t\t\t\tl2cap_retransmit_all(chan, &local_control);\n \t\t\t\t\tl2cap_ertm_send(chan);\n \t\t\t\t}\n \t\t\t}",
        "function_modified_lines": {
            "added": [
                "\tstruct l2cap_ctrl local_control;",
                "\t\t\t/* l2cap_reassemble_sdu may free skb, hence invalidate",
                "\t\t\t * control, so make a copy in advance to use it after",
                "\t\t\t * l2cap_reassemble_sdu returns and to avoid the race",
                "\t\t\t * condition, for example:",
                "\t\t\t *",
                "\t\t\t * The current thread calls:",
                "\t\t\t *   l2cap_reassemble_sdu",
                "\t\t\t *     chan->ops->recv == l2cap_sock_recv_cb",
                "\t\t\t *       __sock_queue_rcv_skb",
                "\t\t\t * Another thread calls:",
                "\t\t\t *   bt_sock_recvmsg",
                "\t\t\t *     skb_recv_datagram",
                "\t\t\t *     skb_free_datagram",
                "\t\t\t * Then the current thread tries to access control, but",
                "\t\t\t * it was freed by skb_free_datagram.",
                "\t\t\t */",
                "\t\t\tlocal_control = *control;",
                "\t\t\tif (local_control.final) {",
                "\t\t\t\t\tlocal_control.final = 0;",
                "\t\t\t\t\tl2cap_retransmit_all(chan, &local_control);"
            ],
            "deleted": [
                "\t\t\tif (control->final) {",
                "\t\t\t\t\tcontrol->final = 0;",
                "\t\t\t\t\tl2cap_retransmit_all(chan, control);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability classified as critical was found in Linux Kernel. Affected by this vulnerability is the function l2cap_reassemble_sdu of the file net/bluetooth/l2cap_core.c of the component Bluetooth. The manipulation leads to use after free. It is recommended to apply a patch to fix this issue. The associated identifier of this vulnerability is VDB-211087."
    },
    {
        "cve_id": "CVE-2022-3564",
        "code_before_change": "static int l2cap_stream_rx(struct l2cap_chan *chan, struct l2cap_ctrl *control,\n\t\t\t   struct sk_buff *skb)\n{\n\tBT_DBG(\"chan %p, control %p, skb %p, state %d\", chan, control, skb,\n\t       chan->rx_state);\n\n\tif (l2cap_classify_txseq(chan, control->txseq) ==\n\t    L2CAP_TXSEQ_EXPECTED) {\n\t\tl2cap_pass_to_tx(chan, control);\n\n\t\tBT_DBG(\"buffer_seq %u->%u\", chan->buffer_seq,\n\t\t       __next_seq(chan, chan->buffer_seq));\n\n\t\tchan->buffer_seq = __next_seq(chan, chan->buffer_seq);\n\n\t\tl2cap_reassemble_sdu(chan, skb, control);\n\t} else {\n\t\tif (chan->sdu) {\n\t\t\tkfree_skb(chan->sdu);\n\t\t\tchan->sdu = NULL;\n\t\t}\n\t\tchan->sdu_last_frag = NULL;\n\t\tchan->sdu_len = 0;\n\n\t\tif (skb) {\n\t\t\tBT_DBG(\"Freeing %p\", skb);\n\t\t\tkfree_skb(skb);\n\t\t}\n\t}\n\n\tchan->last_acked_seq = control->txseq;\n\tchan->expected_tx_seq = __next_seq(chan, control->txseq);\n\n\treturn 0;\n}",
        "code_after_change": "static int l2cap_stream_rx(struct l2cap_chan *chan, struct l2cap_ctrl *control,\n\t\t\t   struct sk_buff *skb)\n{\n\t/* l2cap_reassemble_sdu may free skb, hence invalidate control, so store\n\t * the txseq field in advance to use it after l2cap_reassemble_sdu\n\t * returns and to avoid the race condition, for example:\n\t *\n\t * The current thread calls:\n\t *   l2cap_reassemble_sdu\n\t *     chan->ops->recv == l2cap_sock_recv_cb\n\t *       __sock_queue_rcv_skb\n\t * Another thread calls:\n\t *   bt_sock_recvmsg\n\t *     skb_recv_datagram\n\t *     skb_free_datagram\n\t * Then the current thread tries to access control, but it was freed by\n\t * skb_free_datagram.\n\t */\n\tu16 txseq = control->txseq;\n\n\tBT_DBG(\"chan %p, control %p, skb %p, state %d\", chan, control, skb,\n\t       chan->rx_state);\n\n\tif (l2cap_classify_txseq(chan, txseq) == L2CAP_TXSEQ_EXPECTED) {\n\t\tl2cap_pass_to_tx(chan, control);\n\n\t\tBT_DBG(\"buffer_seq %u->%u\", chan->buffer_seq,\n\t\t       __next_seq(chan, chan->buffer_seq));\n\n\t\tchan->buffer_seq = __next_seq(chan, chan->buffer_seq);\n\n\t\tl2cap_reassemble_sdu(chan, skb, control);\n\t} else {\n\t\tif (chan->sdu) {\n\t\t\tkfree_skb(chan->sdu);\n\t\t\tchan->sdu = NULL;\n\t\t}\n\t\tchan->sdu_last_frag = NULL;\n\t\tchan->sdu_len = 0;\n\n\t\tif (skb) {\n\t\t\tBT_DBG(\"Freeing %p\", skb);\n\t\t\tkfree_skb(skb);\n\t\t}\n\t}\n\n\tchan->last_acked_seq = txseq;\n\tchan->expected_tx_seq = __next_seq(chan, txseq);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,27 @@\n static int l2cap_stream_rx(struct l2cap_chan *chan, struct l2cap_ctrl *control,\n \t\t\t   struct sk_buff *skb)\n {\n+\t/* l2cap_reassemble_sdu may free skb, hence invalidate control, so store\n+\t * the txseq field in advance to use it after l2cap_reassemble_sdu\n+\t * returns and to avoid the race condition, for example:\n+\t *\n+\t * The current thread calls:\n+\t *   l2cap_reassemble_sdu\n+\t *     chan->ops->recv == l2cap_sock_recv_cb\n+\t *       __sock_queue_rcv_skb\n+\t * Another thread calls:\n+\t *   bt_sock_recvmsg\n+\t *     skb_recv_datagram\n+\t *     skb_free_datagram\n+\t * Then the current thread tries to access control, but it was freed by\n+\t * skb_free_datagram.\n+\t */\n+\tu16 txseq = control->txseq;\n+\n \tBT_DBG(\"chan %p, control %p, skb %p, state %d\", chan, control, skb,\n \t       chan->rx_state);\n \n-\tif (l2cap_classify_txseq(chan, control->txseq) ==\n-\t    L2CAP_TXSEQ_EXPECTED) {\n+\tif (l2cap_classify_txseq(chan, txseq) == L2CAP_TXSEQ_EXPECTED) {\n \t\tl2cap_pass_to_tx(chan, control);\n \n \t\tBT_DBG(\"buffer_seq %u->%u\", chan->buffer_seq,\n@@ -28,8 +44,8 @@\n \t\t}\n \t}\n \n-\tchan->last_acked_seq = control->txseq;\n-\tchan->expected_tx_seq = __next_seq(chan, control->txseq);\n+\tchan->last_acked_seq = txseq;\n+\tchan->expected_tx_seq = __next_seq(chan, txseq);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t/* l2cap_reassemble_sdu may free skb, hence invalidate control, so store",
                "\t * the txseq field in advance to use it after l2cap_reassemble_sdu",
                "\t * returns and to avoid the race condition, for example:",
                "\t *",
                "\t * The current thread calls:",
                "\t *   l2cap_reassemble_sdu",
                "\t *     chan->ops->recv == l2cap_sock_recv_cb",
                "\t *       __sock_queue_rcv_skb",
                "\t * Another thread calls:",
                "\t *   bt_sock_recvmsg",
                "\t *     skb_recv_datagram",
                "\t *     skb_free_datagram",
                "\t * Then the current thread tries to access control, but it was freed by",
                "\t * skb_free_datagram.",
                "\t */",
                "\tu16 txseq = control->txseq;",
                "",
                "\tif (l2cap_classify_txseq(chan, txseq) == L2CAP_TXSEQ_EXPECTED) {",
                "\tchan->last_acked_seq = txseq;",
                "\tchan->expected_tx_seq = __next_seq(chan, txseq);"
            ],
            "deleted": [
                "\tif (l2cap_classify_txseq(chan, control->txseq) ==",
                "\t    L2CAP_TXSEQ_EXPECTED) {",
                "\tchan->last_acked_seq = control->txseq;",
                "\tchan->expected_tx_seq = __next_seq(chan, control->txseq);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability classified as critical was found in Linux Kernel. Affected by this vulnerability is the function l2cap_reassemble_sdu of the file net/bluetooth/l2cap_core.c of the component Bluetooth. The manipulation leads to use after free. It is recommended to apply a patch to fix this issue. The associated identifier of this vulnerability is VDB-211087."
    },
    {
        "cve_id": "CVE-2022-3566",
        "code_before_change": "int tcp_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, USER_SOCKPTR(optval),\n\t\t\t\t USER_SOCKPTR(optlen));\n}",
        "code_after_change": "int tcp_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */\n\t\treturn READ_ONCE(icsk->icsk_af_ops)->getsockopt(sk, level, optname,\n\t\t\t\t\t\t\t\toptval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, USER_SOCKPTR(optval),\n\t\t\t\t USER_SOCKPTR(optlen));\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,8 +4,9 @@\n \tstruct inet_connection_sock *icsk = inet_csk(sk);\n \n \tif (level != SOL_TCP)\n-\t\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n-\t\t\t\t\t\t     optval, optlen);\n+\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */\n+\t\treturn READ_ONCE(icsk->icsk_af_ops)->getsockopt(sk, level, optname,\n+\t\t\t\t\t\t\t\toptval, optlen);\n \treturn do_tcp_getsockopt(sk, level, optname, USER_SOCKPTR(optval),\n \t\t\t\t USER_SOCKPTR(optlen));\n }",
        "function_modified_lines": {
            "added": [
                "\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */",
                "\t\treturn READ_ONCE(icsk->icsk_af_ops)->getsockopt(sk, level, optname,",
                "\t\t\t\t\t\t\t\toptval, optlen);"
            ],
            "deleted": [
                "\t\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,",
                "\t\t\t\t\t\t     optval, optlen);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability, which was classified as problematic, was found in Linux Kernel. This affects the function tcp_getsockopt/tcp_setsockopt of the component TCP Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. The identifier VDB-211089 was assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3566",
        "code_before_change": "int tcp_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t   unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}",
        "code_after_change": "int tcp_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t   unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */\n\t\treturn READ_ONCE(icsk->icsk_af_ops)->setsockopt(sk, level, optname,\n\t\t\t\t\t\t\t\toptval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,8 @@\n \tconst struct inet_connection_sock *icsk = inet_csk(sk);\n \n \tif (level != SOL_TCP)\n-\t\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n-\t\t\t\t\t\t     optval, optlen);\n+\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */\n+\t\treturn READ_ONCE(icsk->icsk_af_ops)->setsockopt(sk, level, optname,\n+\t\t\t\t\t\t\t\toptval, optlen);\n \treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n }",
        "function_modified_lines": {
            "added": [
                "\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */",
                "\t\treturn READ_ONCE(icsk->icsk_af_ops)->setsockopt(sk, level, optname,",
                "\t\t\t\t\t\t\t\toptval, optlen);"
            ],
            "deleted": [
                "\t\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,",
                "\t\t\t\t\t\t     optval, optlen);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability, which was classified as problematic, was found in Linux Kernel. This affects the function tcp_getsockopt/tcp_setsockopt of the component TCP Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. The identifier VDB-211089 was assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3566",
        "code_before_change": "int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t       sockptr_t optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (sockptr_is_null(optval))\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tsockopt_lock_sock(sk);\n\n\t/* Another thread has converted the socket into IPv4 with\n\t * IPV6_ADDRFORM concurrently.\n\t */\n\tif (unlikely(sk->sk_family != AF_INET6))\n\t\tgoto unlock;\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tif (sk->sk_prot != &tcpv6_prot) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t__ipv6_sock_mc_close(sk);\n\t\t\t__ipv6_sock_ac_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_stream_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, &tcp_prot);\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_dgram_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, prot);\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\n\t\t\t/* Disable all options not to allocate memory anymore,\n\t\t\t * but there is still a race.  See the lockless path\n\t\t\t * in udpv6_sendmsg() and ipv6_local_rxpmtu().\n\t\t\t */\n\t\t\tnp->rxopt.all = 0;\n\n\t\t\tinet6_cleanup_sock(sk);\n\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tval &= ~INET_ECN_MASK;\n\t\t\tval |= np->tclass & INET_ECN_MASK;\n\t\t}\n\t\tif (np->tclass != val) {\n\t\t\tnp->tclass = val;\n\t\t\tsk_dst_reset(sk);\n\t\t}\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !sockopt_ns_capable(net->user_ns, CAP_NET_RAW) &&\n\t\t    !sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FREEBIND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we also don't have a separate freebind bit for IPV6 */\n\t\tinet_sk(sk)->freebind = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t\tretv = ipv6_set_opt_hdr(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) ||\n\t\t\t sockptr_is_null(optval))\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_sockptr(&pkt, optval, sizeof(pkt))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!sk_dev_equal_l3scope(sk, pkt.ipi6_ifindex))\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tstruct ipcm6_cookie ipc6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\trefcount_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(opt + 1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\t\tipc6.opt = opt;\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, &ipc6);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\t\t\tint midx;\n\n\t\t\trcu_read_lock();\n\n\t\t\tdev = dev_get_by_index_rcu(net, val);\n\t\t\tif (!dev) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmidx = l3mdev_master_ifindex_rcu(dev);\n\n\t\t\trcu_read_unlock();\n\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != val &&\n\t\t\t    (!midx || midx != sk->sk_bound_dev_if))\n\t\t\t\tgoto e_inval;\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_MULTICAST_ALL:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->mc_all = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t\t    optlen);\n\t\telse\n\t\t\tretv = ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t     optlen);\n\t\tbreak;\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t\tretv = do_ipv6_mcast_group_source(sk, optname, optval, optlen);\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_set_mcast_msfilter(sk, optval,\n\t\t\t\t\t\t\t      optlen);\n\t\telse\n\t\t\tretv = ipv6_set_mcast_msfilter(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT_ISOLATE:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rtalert_isolate = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = __ip6_sock_set_addr_preferences(sk, val);\n\t\tbreak;\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\n\t\tif (val)\n\t\t\tstatic_branch_enable(&ip6_min_hopcount);\n\n\t\t/* tcp_v6_err() and tcp_v6_rcv() might read min_hopcount\n\t\t * while we are changing it.\n\t\t */\n\t\tWRITE_ONCE(np->min_hopcount, val);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tnp->autoflowlabel_set = 1;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVFRAGSIZE:\n\t\tnp->rxopt.bits.recvfragsize = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR_RFC4884:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 1)\n\t\t\tgoto e_inval;\n\t\tnp->recverr_rfc4884 = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\nunlock:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "code_after_change": "int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t       sockptr_t optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (sockptr_is_null(optval))\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tsockopt_lock_sock(sk);\n\n\t/* Another thread has converted the socket into IPv4 with\n\t * IPV6_ADDRFORM concurrently.\n\t */\n\tif (unlikely(sk->sk_family != AF_INET6))\n\t\tgoto unlock;\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tif (sk->sk_prot != &tcpv6_prot) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t__ipv6_sock_mc_close(sk);\n\t\t\t__ipv6_sock_ac_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_stream_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, &tcp_prot);\n\t\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n\t\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv4_specific);\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_dgram_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, prot);\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\n\t\t\t/* Disable all options not to allocate memory anymore,\n\t\t\t * but there is still a race.  See the lockless path\n\t\t\t * in udpv6_sendmsg() and ipv6_local_rxpmtu().\n\t\t\t */\n\t\t\tnp->rxopt.all = 0;\n\n\t\t\tinet6_cleanup_sock(sk);\n\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tval &= ~INET_ECN_MASK;\n\t\t\tval |= np->tclass & INET_ECN_MASK;\n\t\t}\n\t\tif (np->tclass != val) {\n\t\t\tnp->tclass = val;\n\t\t\tsk_dst_reset(sk);\n\t\t}\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !sockopt_ns_capable(net->user_ns, CAP_NET_RAW) &&\n\t\t    !sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FREEBIND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we also don't have a separate freebind bit for IPV6 */\n\t\tinet_sk(sk)->freebind = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t\tretv = ipv6_set_opt_hdr(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) ||\n\t\t\t sockptr_is_null(optval))\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_sockptr(&pkt, optval, sizeof(pkt))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!sk_dev_equal_l3scope(sk, pkt.ipi6_ifindex))\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tstruct ipcm6_cookie ipc6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\trefcount_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(opt + 1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\t\tipc6.opt = opt;\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, &ipc6);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\t\t\tint midx;\n\n\t\t\trcu_read_lock();\n\n\t\t\tdev = dev_get_by_index_rcu(net, val);\n\t\t\tif (!dev) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmidx = l3mdev_master_ifindex_rcu(dev);\n\n\t\t\trcu_read_unlock();\n\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != val &&\n\t\t\t    (!midx || midx != sk->sk_bound_dev_if))\n\t\t\t\tgoto e_inval;\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_MULTICAST_ALL:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->mc_all = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t\t    optlen);\n\t\telse\n\t\t\tretv = ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t     optlen);\n\t\tbreak;\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t\tretv = do_ipv6_mcast_group_source(sk, optname, optval, optlen);\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_set_mcast_msfilter(sk, optval,\n\t\t\t\t\t\t\t      optlen);\n\t\telse\n\t\t\tretv = ipv6_set_mcast_msfilter(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT_ISOLATE:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rtalert_isolate = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = __ip6_sock_set_addr_preferences(sk, val);\n\t\tbreak;\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\n\t\tif (val)\n\t\t\tstatic_branch_enable(&ip6_min_hopcount);\n\n\t\t/* tcp_v6_err() and tcp_v6_rcv() might read min_hopcount\n\t\t * while we are changing it.\n\t\t */\n\t\tWRITE_ONCE(np->min_hopcount, val);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tnp->autoflowlabel_set = 1;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVFRAGSIZE:\n\t\tnp->rxopt.bits.recvfragsize = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR_RFC4884:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 1)\n\t\t\tgoto e_inval;\n\t\tnp->recverr_rfc4884 = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\nunlock:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\tsockopt_release_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -86,7 +86,8 @@\n \n \t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_stream_ops */\n \t\t\t\tWRITE_ONCE(sk->sk_prot, &tcp_prot);\n-\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n+\t\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n+\t\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv4_specific);\n \t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n \t\t\t\tsk->sk_family = PF_INET;\n \t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */",
                "\t\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv4_specific);"
            ],
            "deleted": [
                "\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability, which was classified as problematic, was found in Linux Kernel. This affects the function tcp_getsockopt/tcp_setsockopt of the component TCP Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. The identifier VDB-211089 was assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3566",
        "code_before_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct inet_timewait_death_row *tcp_death_row;\n\tstruct ipv6_pinfo *np = tcp_inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct dst_entry *dst;\n\tstruct flowi6 fl6;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr)) {\n\t\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\n\t\t\tipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n\t\t\t\t\t       &usin->sin6_addr);\n\t\telse\n\t\t\tusin->sin6_addr = in6addr_loopback;\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\tWRITE_ONCE(tp->write_seq, 0);\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tif (ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tif (sk_is_mptcp(sk))\n\t\t\tmptcpv6_handle_mapped(sk, true);\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tif (sk_is_mptcp(sk))\n\t\t\t\tmptcpv6_handle_mapped(sk, false);\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tfl6.flowi6_uid = sk->sk_uid;\n\n\topt = rcu_dereference_protected(np->opt, lockdep_sock_is_held(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(&fl6));\n\n\tdst = ip6_dst_lookup_flow(net, sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\ttcp_death_row = &sock_net(sk)->ipv4.tcp_death_row;\n\n\tif (!saddr) {\n\t\tstruct inet_bind_hashbucket *prev_addr_hashbucket = NULL;\n\t\tstruct in6_addr prev_v6_rcv_saddr;\n\n\t\tif (icsk->icsk_bind2_hash) {\n\t\t\tprev_addr_hashbucket = inet_bhashfn_portaddr(tcp_death_row->hashinfo,\n\t\t\t\t\t\t\t\t     sk, net, inet->inet_num);\n\t\t\tprev_v6_rcv_saddr = sk->sk_v6_rcv_saddr;\n\t\t}\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\n\t\tif (prev_addr_hashbucket) {\n\t\t\terr = inet_bhash2_update_saddr(prev_addr_hashbucket, sk);\n\t\t\tif (err) {\n\t\t\t\tsk->sk_v6_rcv_saddr = prev_v6_rcv_saddr;\n\t\t\t\tgoto failure;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\tip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (likely(!tp->repair)) {\n\t\tif (!tp->write_seq)\n\t\t\tWRITE_ONCE(tp->write_seq,\n\t\t\t\t   secure_tcpv6_seq(np->saddr.s6_addr32,\n\t\t\t\t\t\t    sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t    inet->inet_sport,\n\t\t\t\t\t\t    inet->inet_dport));\n\t\ttp->tsoffset = secure_tcpv6_ts_off(net, np->saddr.s6_addr32,\n\t\t\t\t\t\t   sk->sk_v6_daddr.s6_addr32);\n\t}\n\n\tif (tcp_fastopen_defer_connect(sk, &err))\n\t\treturn err;\n\tif (err)\n\t\tgoto late_failure;\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "code_after_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct inet_timewait_death_row *tcp_death_row;\n\tstruct ipv6_pinfo *np = tcp_inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct dst_entry *dst;\n\tstruct flowi6 fl6;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr)) {\n\t\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\n\t\t\tipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n\t\t\t\t\t       &usin->sin6_addr);\n\t\telse\n\t\t\tusin->sin6_addr = in6addr_loopback;\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\tWRITE_ONCE(tp->write_seq, 0);\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tif (ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_mapped);\n\t\tif (sk_is_mptcp(sk))\n\t\t\tmptcpv6_handle_mapped(sk, true);\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_specific);\n\t\t\tif (sk_is_mptcp(sk))\n\t\t\t\tmptcpv6_handle_mapped(sk, false);\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tfl6.flowi6_uid = sk->sk_uid;\n\n\topt = rcu_dereference_protected(np->opt, lockdep_sock_is_held(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(&fl6));\n\n\tdst = ip6_dst_lookup_flow(net, sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\ttcp_death_row = &sock_net(sk)->ipv4.tcp_death_row;\n\n\tif (!saddr) {\n\t\tstruct inet_bind_hashbucket *prev_addr_hashbucket = NULL;\n\t\tstruct in6_addr prev_v6_rcv_saddr;\n\n\t\tif (icsk->icsk_bind2_hash) {\n\t\t\tprev_addr_hashbucket = inet_bhashfn_portaddr(tcp_death_row->hashinfo,\n\t\t\t\t\t\t\t\t     sk, net, inet->inet_num);\n\t\t\tprev_v6_rcv_saddr = sk->sk_v6_rcv_saddr;\n\t\t}\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\n\t\tif (prev_addr_hashbucket) {\n\t\t\terr = inet_bhash2_update_saddr(prev_addr_hashbucket, sk);\n\t\t\tif (err) {\n\t\t\t\tsk->sk_v6_rcv_saddr = prev_v6_rcv_saddr;\n\t\t\t\tgoto failure;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\tip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (likely(!tp->repair)) {\n\t\tif (!tp->write_seq)\n\t\t\tWRITE_ONCE(tp->write_seq,\n\t\t\t\t   secure_tcpv6_seq(np->saddr.s6_addr32,\n\t\t\t\t\t\t    sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t    inet->inet_sport,\n\t\t\t\t\t\t    inet->inet_dport));\n\t\ttp->tsoffset = secure_tcpv6_ts_off(net, np->saddr.s6_addr32,\n\t\t\t\t\t\t   sk->sk_v6_daddr.s6_addr32);\n\t}\n\n\tif (tcp_fastopen_defer_connect(sk, &err))\n\t\treturn err;\n\tif (err)\n\t\tgoto late_failure;\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -94,7 +94,8 @@\n \t\tsin.sin_port = usin->sin6_port;\n \t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n \n-\t\ticsk->icsk_af_ops = &ipv6_mapped;\n+\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n+\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_mapped);\n \t\tif (sk_is_mptcp(sk))\n \t\t\tmptcpv6_handle_mapped(sk, true);\n \t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n@@ -106,7 +107,8 @@\n \n \t\tif (err) {\n \t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n-\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n+\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n+\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_specific);\n \t\t\tif (sk_is_mptcp(sk))\n \t\t\t\tmptcpv6_handle_mapped(sk, false);\n \t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;",
        "function_modified_lines": {
            "added": [
                "\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */",
                "\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_mapped);",
                "\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */",
                "\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_specific);"
            ],
            "deleted": [
                "\t\ticsk->icsk_af_ops = &ipv6_mapped;",
                "\t\t\ticsk->icsk_af_ops = &ipv6_specific;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability, which was classified as problematic, was found in Linux Kernel. This affects the function tcp_getsockopt/tcp_setsockopt of the component TCP Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. The identifier VDB-211089 was assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3567",
        "code_before_change": "int sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t   sockptr_t optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n}",
        "code_after_change": "int sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t   sockptr_t optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\treturn READ_ONCE(sk->sk_prot)->setsockopt(sk, level, optname, optval, optlen);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,5 +3,6 @@\n {\n \tstruct sock *sk = sock->sk;\n \n-\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n+\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n+\treturn READ_ONCE(sk->sk_prot)->setsockopt(sk, level, optname, optval, optlen);\n }",
        "function_modified_lines": {
            "added": [
                "\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */",
                "\treturn READ_ONCE(sk->sk_prot)->setsockopt(sk, level, optname, optval, optlen);"
            ],
            "deleted": [
                "\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function inet6_stream_ops/inet6_dgram_ops of the component IPv6 Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211090 is the identifier assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3567",
        "code_before_change": "int sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n}",
        "code_after_change": "int sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\treturn READ_ONCE(sk->sk_prot)->getsockopt(sk, level, optname, optval, optlen);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,5 +3,6 @@\n {\n \tstruct sock *sk = sock->sk;\n \n-\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n+\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n+\treturn READ_ONCE(sk->sk_prot)->getsockopt(sk, level, optname, optval, optlen);\n }",
        "function_modified_lines": {
            "added": [
                "\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */",
                "\treturn READ_ONCE(sk->sk_prot)->getsockopt(sk, level, optname, optval, optlen);"
            ],
            "deleted": [
                "\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function inet6_stream_ops/inet6_dgram_ops of the component IPv6 Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211090 is the identifier assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3567",
        "code_before_change": "int inet_accept(struct socket *sock, struct socket *newsock, int flags,\n\t\tbool kern)\n{\n\tstruct sock *sk1 = sock->sk;\n\tint err = -EINVAL;\n\tstruct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err, kern);\n\n\tif (!sk2)\n\t\tgoto do_err;\n\n\tlock_sock(sk2);\n\n\tsock_rps_record_flow(sk2);\n\tWARN_ON(!((1 << sk2->sk_state) &\n\t\t  (TCPF_ESTABLISHED | TCPF_SYN_RECV |\n\t\t  TCPF_CLOSE_WAIT | TCPF_CLOSE)));\n\n\tsock_graft(sk2, newsock);\n\n\tnewsock->state = SS_CONNECTED;\n\terr = 0;\n\trelease_sock(sk2);\ndo_err:\n\treturn err;\n}",
        "code_after_change": "int inet_accept(struct socket *sock, struct socket *newsock, int flags,\n\t\tbool kern)\n{\n\tstruct sock *sk1 = sock->sk, *sk2;\n\tint err = -EINVAL;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\tsk2 = READ_ONCE(sk1->sk_prot)->accept(sk1, flags, &err, kern);\n\tif (!sk2)\n\t\tgoto do_err;\n\n\tlock_sock(sk2);\n\n\tsock_rps_record_flow(sk2);\n\tWARN_ON(!((1 << sk2->sk_state) &\n\t\t  (TCPF_ESTABLISHED | TCPF_SYN_RECV |\n\t\t  TCPF_CLOSE_WAIT | TCPF_CLOSE)));\n\n\tsock_graft(sk2, newsock);\n\n\tnewsock->state = SS_CONNECTED;\n\terr = 0;\n\trelease_sock(sk2);\ndo_err:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,11 @@\n int inet_accept(struct socket *sock, struct socket *newsock, int flags,\n \t\tbool kern)\n {\n-\tstruct sock *sk1 = sock->sk;\n+\tstruct sock *sk1 = sock->sk, *sk2;\n \tint err = -EINVAL;\n-\tstruct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err, kern);\n \n+\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n+\tsk2 = READ_ONCE(sk1->sk_prot)->accept(sk1, flags, &err, kern);\n \tif (!sk2)\n \t\tgoto do_err;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct sock *sk1 = sock->sk, *sk2;",
                "\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */",
                "\tsk2 = READ_ONCE(sk1->sk_prot)->accept(sk1, flags, &err, kern);"
            ],
            "deleted": [
                "\tstruct sock *sk1 = sock->sk;",
                "\tstruct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err, kern);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function inet6_stream_ops/inet6_dgram_ops of the component IPv6 Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211090 is the identifier assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3567",
        "code_before_change": "ssize_t inet_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t      size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (unlikely(inet_send_prepare(sk)))\n\t\treturn -EAGAIN;\n\n\tif (sk->sk_prot->sendpage)\n\t\treturn sk->sk_prot->sendpage(sk, page, offset, size, flags);\n\treturn sock_no_sendpage(sock, page, offset, size, flags);\n}",
        "code_after_change": "ssize_t inet_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t      size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tconst struct proto *prot;\n\n\tif (unlikely(inet_send_prepare(sk)))\n\t\treturn -EAGAIN;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\tprot = READ_ONCE(sk->sk_prot);\n\tif (prot->sendpage)\n\t\treturn prot->sendpage(sk, page, offset, size, flags);\n\treturn sock_no_sendpage(sock, page, offset, size, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,11 +2,14 @@\n \t\t      size_t size, int flags)\n {\n \tstruct sock *sk = sock->sk;\n+\tconst struct proto *prot;\n \n \tif (unlikely(inet_send_prepare(sk)))\n \t\treturn -EAGAIN;\n \n-\tif (sk->sk_prot->sendpage)\n-\t\treturn sk->sk_prot->sendpage(sk, page, offset, size, flags);\n+\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n+\tprot = READ_ONCE(sk->sk_prot);\n+\tif (prot->sendpage)\n+\t\treturn prot->sendpage(sk, page, offset, size, flags);\n \treturn sock_no_sendpage(sock, page, offset, size, flags);\n }",
        "function_modified_lines": {
            "added": [
                "\tconst struct proto *prot;",
                "\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */",
                "\tprot = READ_ONCE(sk->sk_prot);",
                "\tif (prot->sendpage)",
                "\t\treturn prot->sendpage(sk, page, offset, size, flags);"
            ],
            "deleted": [
                "\tif (sk->sk_prot->sendpage)",
                "\t\treturn sk->sk_prot->sendpage(sk, page, offset, size, flags);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function inet6_stream_ops/inet6_dgram_ops of the component IPv6 Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211090 is the identifier assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3567",
        "code_before_change": "int inet_dgram_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t       int addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\tif (uaddr->sa_family == AF_UNSPEC)\n\t\treturn sk->sk_prot->disconnect(sk, flags);\n\n\tif (BPF_CGROUP_PRE_CONNECT_ENABLED(sk)) {\n\t\terr = sk->sk_prot->pre_connect(sk, uaddr, addr_len);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (data_race(!inet_sk(sk)->inet_num) && inet_autobind(sk))\n\t\treturn -EAGAIN;\n\treturn sk->sk_prot->connect(sk, uaddr, addr_len);\n}",
        "code_after_change": "int inet_dgram_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t       int addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tconst struct proto *prot;\n\tint err;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\tprot = READ_ONCE(sk->sk_prot);\n\n\tif (uaddr->sa_family == AF_UNSPEC)\n\t\treturn prot->disconnect(sk, flags);\n\n\tif (BPF_CGROUP_PRE_CONNECT_ENABLED(sk)) {\n\t\terr = prot->pre_connect(sk, uaddr, addr_len);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (data_race(!inet_sk(sk)->inet_num) && inet_autobind(sk))\n\t\treturn -EAGAIN;\n\treturn prot->connect(sk, uaddr, addr_len);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,20 +2,25 @@\n \t\t       int addr_len, int flags)\n {\n \tstruct sock *sk = sock->sk;\n+\tconst struct proto *prot;\n \tint err;\n \n \tif (addr_len < sizeof(uaddr->sa_family))\n \t\treturn -EINVAL;\n+\n+\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n+\tprot = READ_ONCE(sk->sk_prot);\n+\n \tif (uaddr->sa_family == AF_UNSPEC)\n-\t\treturn sk->sk_prot->disconnect(sk, flags);\n+\t\treturn prot->disconnect(sk, flags);\n \n \tif (BPF_CGROUP_PRE_CONNECT_ENABLED(sk)) {\n-\t\terr = sk->sk_prot->pre_connect(sk, uaddr, addr_len);\n+\t\terr = prot->pre_connect(sk, uaddr, addr_len);\n \t\tif (err)\n \t\t\treturn err;\n \t}\n \n \tif (data_race(!inet_sk(sk)->inet_num) && inet_autobind(sk))\n \t\treturn -EAGAIN;\n-\treturn sk->sk_prot->connect(sk, uaddr, addr_len);\n+\treturn prot->connect(sk, uaddr, addr_len);\n }",
        "function_modified_lines": {
            "added": [
                "\tconst struct proto *prot;",
                "",
                "\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */",
                "\tprot = READ_ONCE(sk->sk_prot);",
                "",
                "\t\treturn prot->disconnect(sk, flags);",
                "\t\terr = prot->pre_connect(sk, uaddr, addr_len);",
                "\treturn prot->connect(sk, uaddr, addr_len);"
            ],
            "deleted": [
                "\t\treturn sk->sk_prot->disconnect(sk, flags);",
                "\t\terr = sk->sk_prot->pre_connect(sk, uaddr, addr_len);",
                "\treturn sk->sk_prot->connect(sk, uaddr, addr_len);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function inet6_stream_ops/inet6_dgram_ops of the component IPv6 Handler. The manipulation leads to race condition. It is recommended to apply a patch to fix this issue. VDB-211090 is the identifier assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3623",
        "code_before_change": "static struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags,\n\t\tstruct dev_pagemap **pgmap)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\tint ret;\n\n\t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn ERR_PTR(-EINVAL);\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\n\tpage = vm_normal_page(vma, address, pte);\n\n\t/*\n\t * We only care about anon pages in can_follow_write_pte() and don't\n\t * have to worry about pte_devmap() because they are never anon.\n\t */\n\tif ((flags & FOLL_WRITE) &&\n\t    !can_follow_write_pte(pte, page, vma, flags)) {\n\t\tpage = NULL;\n\t\tgoto out;\n\t}\n\n\tif (!page && pte_devmap(pte) && (flags & (FOLL_GET | FOLL_PIN))) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET or FOLL_PIN\n\t\t * case since they are only valid while holding the pgmap\n\t\t * reference.\n\t\t */\n\t\t*pgmap = get_dev_pagemap(pte_pfn(pte), *pgmap);\n\t\tif (*pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!pte_write(pte) && gup_must_unshare(flags, page)) {\n\t\tpage = ERR_PTR(-EMLINK);\n\t\tgoto out;\n\t}\n\n\tVM_BUG_ON_PAGE((flags & FOLL_PIN) && PageAnon(page) &&\n\t\t       !PageAnonExclusive(page), page);\n\n\t/* try_grab_page() does nothing unless FOLL_GET or FOLL_PIN is set. */\n\tif (unlikely(!try_grab_page(page, flags))) {\n\t\tpage = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\t/*\n\t * We need to make the page accessible if and only if we are going\n\t * to access its content (the FOLL_PIN case).  Please see\n\t * Documentation/core-api/pin_user_pages.rst for details.\n\t */\n\tif (flags & FOLL_PIN) {\n\t\tret = arch_make_page_accessible(page);\n\t\tif (ret) {\n\t\t\tunpin_user_page(page);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}",
        "code_after_change": "static struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags,\n\t\tstruct dev_pagemap **pgmap)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\tint ret;\n\n\t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Considering PTE level hugetlb, like continuous-PTE hugetlb on\n\t * ARM64 architecture.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tpage = follow_huge_pmd_pte(vma, address, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\n\tpage = vm_normal_page(vma, address, pte);\n\n\t/*\n\t * We only care about anon pages in can_follow_write_pte() and don't\n\t * have to worry about pte_devmap() because they are never anon.\n\t */\n\tif ((flags & FOLL_WRITE) &&\n\t    !can_follow_write_pte(pte, page, vma, flags)) {\n\t\tpage = NULL;\n\t\tgoto out;\n\t}\n\n\tif (!page && pte_devmap(pte) && (flags & (FOLL_GET | FOLL_PIN))) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET or FOLL_PIN\n\t\t * case since they are only valid while holding the pgmap\n\t\t * reference.\n\t\t */\n\t\t*pgmap = get_dev_pagemap(pte_pfn(pte), *pgmap);\n\t\tif (*pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!pte_write(pte) && gup_must_unshare(flags, page)) {\n\t\tpage = ERR_PTR(-EMLINK);\n\t\tgoto out;\n\t}\n\n\tVM_BUG_ON_PAGE((flags & FOLL_PIN) && PageAnon(page) &&\n\t\t       !PageAnonExclusive(page), page);\n\n\t/* try_grab_page() does nothing unless FOLL_GET or FOLL_PIN is set. */\n\tif (unlikely(!try_grab_page(page, flags))) {\n\t\tpage = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\t/*\n\t * We need to make the page accessible if and only if we are going\n\t * to access its content (the FOLL_PIN case).  Please see\n\t * Documentation/core-api/pin_user_pages.rst for details.\n\t */\n\tif (flags & FOLL_PIN) {\n\t\tret = arch_make_page_accessible(page);\n\t\tif (ret) {\n\t\t\tunpin_user_page(page);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,18 @@\n \tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n \t\t\t (FOLL_PIN | FOLL_GET)))\n \t\treturn ERR_PTR(-EINVAL);\n+\n+\t/*\n+\t * Considering PTE level hugetlb, like continuous-PTE hugetlb on\n+\t * ARM64 architecture.\n+\t */\n+\tif (is_vm_hugetlb_page(vma)) {\n+\t\tpage = follow_huge_pmd_pte(vma, address, flags);\n+\t\tif (page)\n+\t\t\treturn page;\n+\t\treturn no_page_table(vma, flags);\n+\t}\n+\n retry:\n \tif (unlikely(pmd_bad(*pmd)))\n \t\treturn no_page_table(vma, flags);",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * Considering PTE level hugetlb, like continuous-PTE hugetlb on",
                "\t * ARM64 architecture.",
                "\t */",
                "\tif (is_vm_hugetlb_page(vma)) {",
                "\t\tpage = follow_huge_pmd_pte(vma, address, flags);",
                "\t\tif (page)",
                "\t\t\treturn page;",
                "\t\treturn no_page_table(vma, flags);",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been declared as problematic. Affected by this vulnerability is the function follow_page_pte of the file mm/gup.c of the component BPF. The manipulation leads to race condition. The attack can be launched remotely. It is recommended to apply a patch to fix this issue. The identifier VDB-211921 was assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3623",
        "code_before_change": "static struct page *follow_pmd_mask(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long address, pud_t *pudp,\n\t\t\t\t    unsigned int flags,\n\t\t\t\t    struct follow_page_context *ctx)\n{\n\tpmd_t *pmd, pmdval;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpmd = pmd_offset(pudp, address);\n\t/*\n\t * The READ_ONCE() will stabilize the pmdval in a register or\n\t * on the stack so that it will stop changing under the code.\n\t */\n\tpmdval = READ_ONCE(*pmd);\n\tif (pmd_none(pmdval))\n\t\treturn no_page_table(vma, flags);\n\tif (pmd_huge(pmdval) && is_vm_hugetlb_page(vma)) {\n\t\tpage = follow_huge_pmd(mm, address, pmd, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (is_hugepd(__hugepd(pmd_val(pmdval)))) {\n\t\tpage = follow_huge_pd(vma, address,\n\t\t\t\t      __hugepd(pmd_val(pmdval)), flags,\n\t\t\t\t      PMD_SHIFT);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\nretry:\n\tif (!pmd_present(pmdval)) {\n\t\t/*\n\t\t * Should never reach here, if thp migration is not supported;\n\t\t * Otherwise, it must be a thp migration entry.\n\t\t */\n\t\tVM_BUG_ON(!thp_migration_supported() ||\n\t\t\t\t  !is_pmd_migration_entry(pmdval));\n\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tpmdval = READ_ONCE(*pmd);\n\t\t/*\n\t\t * MADV_DONTNEED may convert the pmd to null because\n\t\t * mmap_lock is held in read mode\n\t\t */\n\t\tif (pmd_none(pmdval))\n\t\t\treturn no_page_table(vma, flags);\n\t\tgoto retry;\n\t}\n\tif (pmd_devmap(pmdval)) {\n\t\tptl = pmd_lock(mm, pmd);\n\t\tpage = follow_devmap_pmd(vma, address, pmd, flags, &ctx->pgmap);\n\t\tspin_unlock(ptl);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (likely(!pmd_trans_huge(pmdval)))\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\n\tif ((flags & FOLL_NUMA) && pmd_protnone(pmdval))\n\t\treturn no_page_table(vma, flags);\n\nretry_locked:\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(pmd_none(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (unlikely(!pmd_present(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tgoto retry_locked;\n\t}\n\tif (unlikely(!pmd_trans_huge(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tif (flags & FOLL_SPLIT_PMD) {\n\t\tint ret;\n\t\tpage = pmd_page(*pmd);\n\t\tif (is_huge_zero_page(page)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tret = 0;\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tif (pmd_trans_unstable(pmd))\n\t\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tspin_unlock(ptl);\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tret = pte_alloc(mm, pmd) ? -ENOMEM : 0;\n\t\t}\n\n\t\treturn ret ? ERR_PTR(ret) :\n\t\t\tfollow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tpage = follow_trans_huge_pmd(vma, address, pmd, flags);\n\tspin_unlock(ptl);\n\tctx->page_mask = HPAGE_PMD_NR - 1;\n\treturn page;\n}",
        "code_after_change": "static struct page *follow_pmd_mask(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long address, pud_t *pudp,\n\t\t\t\t    unsigned int flags,\n\t\t\t\t    struct follow_page_context *ctx)\n{\n\tpmd_t *pmd, pmdval;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpmd = pmd_offset(pudp, address);\n\t/*\n\t * The READ_ONCE() will stabilize the pmdval in a register or\n\t * on the stack so that it will stop changing under the code.\n\t */\n\tpmdval = READ_ONCE(*pmd);\n\tif (pmd_none(pmdval))\n\t\treturn no_page_table(vma, flags);\n\tif (pmd_huge(pmdval) && is_vm_hugetlb_page(vma)) {\n\t\tpage = follow_huge_pmd_pte(vma, address, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (is_hugepd(__hugepd(pmd_val(pmdval)))) {\n\t\tpage = follow_huge_pd(vma, address,\n\t\t\t\t      __hugepd(pmd_val(pmdval)), flags,\n\t\t\t\t      PMD_SHIFT);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\nretry:\n\tif (!pmd_present(pmdval)) {\n\t\t/*\n\t\t * Should never reach here, if thp migration is not supported;\n\t\t * Otherwise, it must be a thp migration entry.\n\t\t */\n\t\tVM_BUG_ON(!thp_migration_supported() ||\n\t\t\t\t  !is_pmd_migration_entry(pmdval));\n\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tpmdval = READ_ONCE(*pmd);\n\t\t/*\n\t\t * MADV_DONTNEED may convert the pmd to null because\n\t\t * mmap_lock is held in read mode\n\t\t */\n\t\tif (pmd_none(pmdval))\n\t\t\treturn no_page_table(vma, flags);\n\t\tgoto retry;\n\t}\n\tif (pmd_devmap(pmdval)) {\n\t\tptl = pmd_lock(mm, pmd);\n\t\tpage = follow_devmap_pmd(vma, address, pmd, flags, &ctx->pgmap);\n\t\tspin_unlock(ptl);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (likely(!pmd_trans_huge(pmdval)))\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\n\tif ((flags & FOLL_NUMA) && pmd_protnone(pmdval))\n\t\treturn no_page_table(vma, flags);\n\nretry_locked:\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(pmd_none(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (unlikely(!pmd_present(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tgoto retry_locked;\n\t}\n\tif (unlikely(!pmd_trans_huge(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tif (flags & FOLL_SPLIT_PMD) {\n\t\tint ret;\n\t\tpage = pmd_page(*pmd);\n\t\tif (is_huge_zero_page(page)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tret = 0;\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tif (pmd_trans_unstable(pmd))\n\t\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tspin_unlock(ptl);\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tret = pte_alloc(mm, pmd) ? -ENOMEM : 0;\n\t\t}\n\n\t\treturn ret ? ERR_PTR(ret) :\n\t\t\tfollow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tpage = follow_trans_huge_pmd(vma, address, pmd, flags);\n\tspin_unlock(ptl);\n\tctx->page_mask = HPAGE_PMD_NR - 1;\n\treturn page;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,7 +17,7 @@\n \tif (pmd_none(pmdval))\n \t\treturn no_page_table(vma, flags);\n \tif (pmd_huge(pmdval) && is_vm_hugetlb_page(vma)) {\n-\t\tpage = follow_huge_pmd(mm, address, pmd, flags);\n+\t\tpage = follow_huge_pmd_pte(vma, address, flags);\n \t\tif (page)\n \t\t\treturn page;\n \t\treturn no_page_table(vma, flags);",
        "function_modified_lines": {
            "added": [
                "\t\tpage = follow_huge_pmd_pte(vma, address, flags);"
            ],
            "deleted": [
                "\t\tpage = follow_huge_pmd(mm, address, pmd, flags);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been declared as problematic. Affected by this vulnerability is the function follow_page_pte of the file mm/gup.c of the component BPF. The manipulation leads to race condition. The attack can be launched remotely. It is recommended to apply a patch to fix this issue. The identifier VDB-211921 was assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-3635",
        "code_before_change": "static void __exit idt77252_exit(void)\n{\n\tstruct idt77252_dev *card;\n\tstruct atm_dev *dev;\n\n\tpci_unregister_driver(&idt77252_driver);\n\n\twhile (idt77252_chain) {\n\t\tcard = idt77252_chain;\n\t\tdev = card->atmdev;\n\t\tidt77252_chain = card->next;\n\n\t\tif (dev->phy->stop)\n\t\t\tdev->phy->stop(dev);\n\t\tdeinit_card(card);\n\t\tpci_disable_device(card->pcidev);\n\t\tkfree(card);\n\t}\n\n\tDIPRINTK(\"idt77252: finished cleanup-module().\\n\");\n}",
        "code_after_change": "static void __exit idt77252_exit(void)\n{\n\tstruct idt77252_dev *card;\n\tstruct atm_dev *dev;\n\n\tpci_unregister_driver(&idt77252_driver);\n\n\twhile (idt77252_chain) {\n\t\tcard = idt77252_chain;\n\t\tdev = card->atmdev;\n\t\tidt77252_chain = card->next;\n\t\tdel_timer_sync(&card->tst_timer);\n\n\t\tif (dev->phy->stop)\n\t\t\tdev->phy->stop(dev);\n\t\tdeinit_card(card);\n\t\tpci_disable_device(card->pcidev);\n\t\tkfree(card);\n\t}\n\n\tDIPRINTK(\"idt77252: finished cleanup-module().\\n\");\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,7 @@\n \t\tcard = idt77252_chain;\n \t\tdev = card->atmdev;\n \t\tidt77252_chain = card->next;\n+\t\tdel_timer_sync(&card->tst_timer);\n \n \t\tif (dev->phy->stop)\n \t\t\tdev->phy->stop(dev);",
        "function_modified_lines": {
            "added": [
                "\t\tdel_timer_sync(&card->tst_timer);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A vulnerability, which was classified as critical, has been found in Linux Kernel. Affected by this issue is the function tst_timer of the file drivers/atm/idt77252.c of the component IPsec. The manipulation leads to use after free. It is recommended to apply a patch to fix this issue. VDB-211934 is the identifier assigned to this vulnerability."
    },
    {
        "cve_id": "CVE-2022-39188",
        "code_before_change": "static inline void tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\tif (tlb->fullmm || IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS))\n\t\treturn;\n\n\t/*\n\t * Do a TLB flush and reset the range at VMA boundaries; this avoids\n\t * the ranges growing with the unused space between consecutive VMAs,\n\t * but also the mmu_gather::vma_* flags from tlb_start_vma() rely on\n\t * this.\n\t */\n\ttlb_flush_mmu_tlbonly(tlb);\n}",
        "code_after_change": "static inline void tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\tif (tlb->fullmm)\n\t\treturn;\n\n\t/*\n\t * VM_PFNMAP is more fragile because the core mm will not track the\n\t * page mapcount -- there might not be page-frames for these PFNs after\n\t * all. Force flush TLBs for such ranges to avoid munmap() vs\n\t * unmap_mapping_range() races.\n\t */\n\tif (tlb->vma_pfn || !IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS)) {\n\t\t/*\n\t\t * Do a TLB flush and reset the range at VMA boundaries; this avoids\n\t\t * the ranges growing with the unused space between consecutive VMAs.\n\t\t */\n\t\ttlb_flush_mmu_tlbonly(tlb);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,13 +1,19 @@\n static inline void tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)\n {\n-\tif (tlb->fullmm || IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS))\n+\tif (tlb->fullmm)\n \t\treturn;\n \n \t/*\n-\t * Do a TLB flush and reset the range at VMA boundaries; this avoids\n-\t * the ranges growing with the unused space between consecutive VMAs,\n-\t * but also the mmu_gather::vma_* flags from tlb_start_vma() rely on\n-\t * this.\n+\t * VM_PFNMAP is more fragile because the core mm will not track the\n+\t * page mapcount -- there might not be page-frames for these PFNs after\n+\t * all. Force flush TLBs for such ranges to avoid munmap() vs\n+\t * unmap_mapping_range() races.\n \t */\n-\ttlb_flush_mmu_tlbonly(tlb);\n+\tif (tlb->vma_pfn || !IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS)) {\n+\t\t/*\n+\t\t * Do a TLB flush and reset the range at VMA boundaries; this avoids\n+\t\t * the ranges growing with the unused space between consecutive VMAs.\n+\t\t */\n+\t\ttlb_flush_mmu_tlbonly(tlb);\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tif (tlb->fullmm)",
                "\t * VM_PFNMAP is more fragile because the core mm will not track the",
                "\t * page mapcount -- there might not be page-frames for these PFNs after",
                "\t * all. Force flush TLBs for such ranges to avoid munmap() vs",
                "\t * unmap_mapping_range() races.",
                "\tif (tlb->vma_pfn || !IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS)) {",
                "\t\t/*",
                "\t\t * Do a TLB flush and reset the range at VMA boundaries; this avoids",
                "\t\t * the ranges growing with the unused space between consecutive VMAs.",
                "\t\t */",
                "\t\ttlb_flush_mmu_tlbonly(tlb);",
                "\t}"
            ],
            "deleted": [
                "\tif (tlb->fullmm || IS_ENABLED(CONFIG_MMU_GATHER_MERGE_VMAS))",
                "\t * Do a TLB flush and reset the range at VMA boundaries; this avoids",
                "\t * the ranges growing with the unused space between consecutive VMAs,",
                "\t * but also the mmu_gather::vma_* flags from tlb_start_vma() rely on",
                "\t * this.",
                "\ttlb_flush_mmu_tlbonly(tlb);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in include/asm-generic/tlb.h in the Linux kernel before 5.19. Because of a race condition (unmap_mapping_range versus munmap), a device driver can free a page while it still has stale TLB entries. This only occurs in situations with VM_PFNMAP VMAs."
    },
    {
        "cve_id": "CVE-2022-39188",
        "code_before_change": "static inline void\ntlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma) { }",
        "code_after_change": "static inline void\ntlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\t/*\n\t * flush_tlb_range() implementations that look at VM_HUGETLB (tile,\n\t * mips-4k) flush only large pages.\n\t *\n\t * flush_tlb_range() implementations that flush I-TLB also flush D-TLB\n\t * (tile, xtensa, arm), so it's ok to just add VM_EXEC to an existing\n\t * range.\n\t *\n\t * We rely on tlb_end_vma() to issue a flush, such that when we reset\n\t * these values the batch is empty.\n\t */\n\ttlb->vma_huge = is_vm_hugetlb_page(vma);\n\ttlb->vma_exec = !!(vma->vm_flags & VM_EXEC);\n\ttlb->vma_pfn  = !!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP));\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,2 +1,18 @@\n static inline void\n-tlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma) { }\n+tlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma)\n+{\n+\t/*\n+\t * flush_tlb_range() implementations that look at VM_HUGETLB (tile,\n+\t * mips-4k) flush only large pages.\n+\t *\n+\t * flush_tlb_range() implementations that flush I-TLB also flush D-TLB\n+\t * (tile, xtensa, arm), so it's ok to just add VM_EXEC to an existing\n+\t * range.\n+\t *\n+\t * We rely on tlb_end_vma() to issue a flush, such that when we reset\n+\t * these values the batch is empty.\n+\t */\n+\ttlb->vma_huge = is_vm_hugetlb_page(vma);\n+\ttlb->vma_exec = !!(vma->vm_flags & VM_EXEC);\n+\ttlb->vma_pfn  = !!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP));\n+}",
        "function_modified_lines": {
            "added": [
                "tlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma)",
                "{",
                "\t/*",
                "\t * flush_tlb_range() implementations that look at VM_HUGETLB (tile,",
                "\t * mips-4k) flush only large pages.",
                "\t *",
                "\t * flush_tlb_range() implementations that flush I-TLB also flush D-TLB",
                "\t * (tile, xtensa, arm), so it's ok to just add VM_EXEC to an existing",
                "\t * range.",
                "\t *",
                "\t * We rely on tlb_end_vma() to issue a flush, such that when we reset",
                "\t * these values the batch is empty.",
                "\t */",
                "\ttlb->vma_huge = is_vm_hugetlb_page(vma);",
                "\ttlb->vma_exec = !!(vma->vm_flags & VM_EXEC);",
                "\ttlb->vma_pfn  = !!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP));",
                "}"
            ],
            "deleted": [
                "tlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma) { }"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in include/asm-generic/tlb.h in the Linux kernel before 5.19. Because of a race condition (unmap_mapping_range versus munmap), a device driver can free a page while it still has stale TLB entries. This only occurs in situations with VM_PFNMAP VMAs."
    },
    {
        "cve_id": "CVE-2022-40307",
        "code_before_change": "static int efi_capsule_release(struct inode *inode, struct file *file)\n{\n\tstruct capsule_info *cap_info = file->private_data;\n\n\tkfree(cap_info->pages);\n\tkfree(cap_info->phys);\n\tkfree(file->private_data);\n\tfile->private_data = NULL;\n\treturn 0;\n}",
        "code_after_change": "static int efi_capsule_release(struct inode *inode, struct file *file)\n{\n\tstruct capsule_info *cap_info = file->private_data;\n\n\tif (cap_info->index > 0 &&\n\t    (cap_info->header.headersize == 0 ||\n\t     cap_info->count < cap_info->total_size)) {\n\t\tpr_err(\"capsule upload not complete\\n\");\n\t\tefi_free_all_buff_pages(cap_info);\n\t}\n\n\tkfree(cap_info->pages);\n\tkfree(cap_info->phys);\n\tkfree(file->private_data);\n\tfile->private_data = NULL;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,13 @@\n static int efi_capsule_release(struct inode *inode, struct file *file)\n {\n \tstruct capsule_info *cap_info = file->private_data;\n+\n+\tif (cap_info->index > 0 &&\n+\t    (cap_info->header.headersize == 0 ||\n+\t     cap_info->count < cap_info->total_size)) {\n+\t\tpr_err(\"capsule upload not complete\\n\");\n+\t\tefi_free_all_buff_pages(cap_info);\n+\t}\n \n \tkfree(cap_info->pages);\n \tkfree(cap_info->phys);",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (cap_info->index > 0 &&",
                "\t    (cap_info->header.headersize == 0 ||",
                "\t     cap_info->count < cap_info->total_size)) {",
                "\t\tpr_err(\"capsule upload not complete\\n\");",
                "\t\tefi_free_all_buff_pages(cap_info);",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.19.8. drivers/firmware/efi/capsule-loader.c has a race condition with a resultant use-after-free."
    },
    {
        "cve_id": "CVE-2022-41849",
        "code_before_change": "static void ufx_usb_disconnect(struct usb_interface *interface)\n{\n\tstruct ufx_data *dev;\n\n\tdev = usb_get_intfdata(interface);\n\n\tpr_debug(\"USB disconnect starting\\n\");\n\n\t/* we virtualize until all fb clients release. Then we free */\n\tdev->virtualized = true;\n\n\t/* When non-active we'll update virtual framebuffer, but no new urbs */\n\tatomic_set(&dev->usb_active, 0);\n\n\tusb_set_intfdata(interface, NULL);\n\n\t/* if clients still have us open, will be freed on last close */\n\tif (dev->fb_count == 0)\n\t\tschedule_delayed_work(&dev->free_framebuffer_work, 0);\n\n\t/* release reference taken by kref_init in probe() */\n\tkref_put(&dev->kref, ufx_free);\n\n\t/* consider ufx_data freed */\n}",
        "code_after_change": "static void ufx_usb_disconnect(struct usb_interface *interface)\n{\n\tstruct ufx_data *dev;\n\n\tmutex_lock(&disconnect_mutex);\n\n\tdev = usb_get_intfdata(interface);\n\n\tpr_debug(\"USB disconnect starting\\n\");\n\n\t/* we virtualize until all fb clients release. Then we free */\n\tdev->virtualized = true;\n\n\t/* When non-active we'll update virtual framebuffer, but no new urbs */\n\tatomic_set(&dev->usb_active, 0);\n\n\tusb_set_intfdata(interface, NULL);\n\n\t/* if clients still have us open, will be freed on last close */\n\tif (dev->fb_count == 0)\n\t\tschedule_delayed_work(&dev->free_framebuffer_work, 0);\n\n\t/* release reference taken by kref_init in probe() */\n\tkref_put(&dev->kref, ufx_free);\n\n\t/* consider ufx_data freed */\n\n\tmutex_unlock(&disconnect_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,8 @@\n static void ufx_usb_disconnect(struct usb_interface *interface)\n {\n \tstruct ufx_data *dev;\n+\n+\tmutex_lock(&disconnect_mutex);\n \n \tdev = usb_get_intfdata(interface);\n \n@@ -22,4 +24,6 @@\n \tkref_put(&dev->kref, ufx_free);\n \n \t/* consider ufx_data freed */\n+\n+\tmutex_unlock(&disconnect_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tmutex_lock(&disconnect_mutex);",
                "",
                "\tmutex_unlock(&disconnect_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "drivers/video/fbdev/smscufx.c in the Linux kernel through 5.19.12 has a race condition and resultant use-after-free if a physically proximate attacker removes a USB device while calling open(), aka a race condition between ufx_ops_open and ufx_usb_disconnect."
    },
    {
        "cve_id": "CVE-2022-41849",
        "code_before_change": "static int ufx_ops_open(struct fb_info *info, int user)\n{\n\tstruct ufx_data *dev = info->par;\n\n\t/* fbcon aggressively connects to first framebuffer it finds,\n\t * preventing other clients (X) from working properly. Usually\n\t * not what the user wants. Fail by default with option to enable. */\n\tif (user == 0 && !console)\n\t\treturn -EBUSY;\n\n\t/* If the USB device is gone, we don't accept new opens */\n\tif (dev->virtualized)\n\t\treturn -ENODEV;\n\n\tdev->fb_count++;\n\n\tkref_get(&dev->kref);\n\n\tif (fb_defio && (info->fbdefio == NULL)) {\n\t\t/* enable defio at last moment if not disabled by client */\n\n\t\tstruct fb_deferred_io *fbdefio;\n\n\t\tfbdefio = kzalloc(sizeof(*fbdefio), GFP_KERNEL);\n\t\tif (fbdefio) {\n\t\t\tfbdefio->delay = UFX_DEFIO_WRITE_DELAY;\n\t\t\tfbdefio->deferred_io = ufx_dpy_deferred_io;\n\t\t}\n\n\t\tinfo->fbdefio = fbdefio;\n\t\tfb_deferred_io_init(info);\n\t}\n\n\tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n\t\tinfo->node, user, info, dev->fb_count);\n\n\treturn 0;\n}",
        "code_after_change": "static int ufx_ops_open(struct fb_info *info, int user)\n{\n\tstruct ufx_data *dev = info->par;\n\n\t/* fbcon aggressively connects to first framebuffer it finds,\n\t * preventing other clients (X) from working properly. Usually\n\t * not what the user wants. Fail by default with option to enable. */\n\tif (user == 0 && !console)\n\t\treturn -EBUSY;\n\n\tmutex_lock(&disconnect_mutex);\n\n\t/* If the USB device is gone, we don't accept new opens */\n\tif (dev->virtualized) {\n\t\tmutex_unlock(&disconnect_mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tdev->fb_count++;\n\n\tkref_get(&dev->kref);\n\n\tif (fb_defio && (info->fbdefio == NULL)) {\n\t\t/* enable defio at last moment if not disabled by client */\n\n\t\tstruct fb_deferred_io *fbdefio;\n\n\t\tfbdefio = kzalloc(sizeof(*fbdefio), GFP_KERNEL);\n\t\tif (fbdefio) {\n\t\t\tfbdefio->delay = UFX_DEFIO_WRITE_DELAY;\n\t\t\tfbdefio->deferred_io = ufx_dpy_deferred_io;\n\t\t}\n\n\t\tinfo->fbdefio = fbdefio;\n\t\tfb_deferred_io_init(info);\n\t}\n\n\tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n\t\tinfo->node, user, info, dev->fb_count);\n\n\tmutex_unlock(&disconnect_mutex);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,9 +8,13 @@\n \tif (user == 0 && !console)\n \t\treturn -EBUSY;\n \n+\tmutex_lock(&disconnect_mutex);\n+\n \t/* If the USB device is gone, we don't accept new opens */\n-\tif (dev->virtualized)\n+\tif (dev->virtualized) {\n+\t\tmutex_unlock(&disconnect_mutex);\n \t\treturn -ENODEV;\n+\t}\n \n \tdev->fb_count++;\n \n@@ -34,5 +38,7 @@\n \tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n \t\tinfo->node, user, info, dev->fb_count);\n \n+\tmutex_unlock(&disconnect_mutex);\n+\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&disconnect_mutex);",
                "",
                "\tif (dev->virtualized) {",
                "\t\tmutex_unlock(&disconnect_mutex);",
                "\t}",
                "\tmutex_unlock(&disconnect_mutex);",
                ""
            ],
            "deleted": [
                "\tif (dev->virtualized)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "drivers/video/fbdev/smscufx.c in the Linux kernel through 5.19.12 has a race condition and resultant use-after-free if a physically proximate attacker removes a USB device while calling open(), aka a race condition between ufx_ops_open and ufx_usb_disconnect."
    },
    {
        "cve_id": "CVE-2022-41850",
        "code_before_change": "int roccat_report_event(int minor, u8 const *data)\n{\n\tstruct roccat_device *device;\n\tstruct roccat_reader *reader;\n\tstruct roccat_report *report;\n\tuint8_t *new_value;\n\n\tdevice = devices[minor];\n\n\tnew_value = kmemdup(data, device->report_size, GFP_ATOMIC);\n\tif (!new_value)\n\t\treturn -ENOMEM;\n\n\treport = &device->cbuf[device->cbuf_end];\n\n\t/* passing NULL is safe */\n\tkfree(report->value);\n\n\treport->value = new_value;\n\tdevice->cbuf_end = (device->cbuf_end + 1) % ROCCAT_CBUF_SIZE;\n\n\tlist_for_each_entry(reader, &device->readers, node) {\n\t\t/*\n\t\t * As we already inserted one element, the buffer can't be\n\t\t * empty. If start and end are equal, buffer is full and we\n\t\t * increase start, so that slow reader misses one event, but\n\t\t * gets the newer ones in the right order.\n\t\t */\n\t\tif (reader->cbuf_start == device->cbuf_end)\n\t\t\treader->cbuf_start = (reader->cbuf_start + 1) % ROCCAT_CBUF_SIZE;\n\t}\n\n\twake_up_interruptible(&device->wait);\n\treturn 0;\n}",
        "code_after_change": "int roccat_report_event(int minor, u8 const *data)\n{\n\tstruct roccat_device *device;\n\tstruct roccat_reader *reader;\n\tstruct roccat_report *report;\n\tuint8_t *new_value;\n\n\tdevice = devices[minor];\n\n\tnew_value = kmemdup(data, device->report_size, GFP_ATOMIC);\n\tif (!new_value)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&device->cbuf_lock);\n\n\treport = &device->cbuf[device->cbuf_end];\n\n\t/* passing NULL is safe */\n\tkfree(report->value);\n\n\treport->value = new_value;\n\tdevice->cbuf_end = (device->cbuf_end + 1) % ROCCAT_CBUF_SIZE;\n\n\tlist_for_each_entry(reader, &device->readers, node) {\n\t\t/*\n\t\t * As we already inserted one element, the buffer can't be\n\t\t * empty. If start and end are equal, buffer is full and we\n\t\t * increase start, so that slow reader misses one event, but\n\t\t * gets the newer ones in the right order.\n\t\t */\n\t\tif (reader->cbuf_start == device->cbuf_end)\n\t\t\treader->cbuf_start = (reader->cbuf_start + 1) % ROCCAT_CBUF_SIZE;\n\t}\n\n\tmutex_unlock(&device->cbuf_lock);\n\n\twake_up_interruptible(&device->wait);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,8 @@\n \tnew_value = kmemdup(data, device->report_size, GFP_ATOMIC);\n \tif (!new_value)\n \t\treturn -ENOMEM;\n+\n+\tmutex_lock(&device->cbuf_lock);\n \n \treport = &device->cbuf[device->cbuf_end];\n \n@@ -30,6 +32,8 @@\n \t\t\treader->cbuf_start = (reader->cbuf_start + 1) % ROCCAT_CBUF_SIZE;\n \t}\n \n+\tmutex_unlock(&device->cbuf_lock);\n+\n \twake_up_interruptible(&device->wait);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tmutex_lock(&device->cbuf_lock);",
                "\tmutex_unlock(&device->cbuf_lock);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "roccat_report_event in drivers/hid/hid-roccat.c in the Linux kernel through 5.19.12 has a race condition and resultant use-after-free in certain situations where a report is received while copying a report->value is in progress."
    },
    {
        "cve_id": "CVE-2022-45869",
        "code_before_change": "static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,\n\t\t\t\t       struct kvm_mmu_page *sp,\n\t\t\t\t       struct list_head *invalid_list,\n\t\t\t\t       int *nr_zapped)\n{\n\tbool list_unstable, zapped_root = false;\n\n\ttrace_kvm_mmu_prepare_zap_page(sp);\n\t++kvm->stat.mmu_shadow_zapped;\n\t*nr_zapped = mmu_zap_unsync_children(kvm, sp, invalid_list);\n\t*nr_zapped += kvm_mmu_page_unlink_children(kvm, sp, invalid_list);\n\tkvm_mmu_unlink_parents(sp);\n\n\t/* Zapping children means active_mmu_pages has become unstable. */\n\tlist_unstable = *nr_zapped;\n\n\tif (!sp->role.invalid && sp_has_gptes(sp))\n\t\tunaccount_shadowed(kvm, sp);\n\n\tif (sp->unsync)\n\t\tkvm_unlink_unsync_page(kvm, sp);\n\tif (!sp->root_count) {\n\t\t/* Count self */\n\t\t(*nr_zapped)++;\n\n\t\t/*\n\t\t * Already invalid pages (previously active roots) are not on\n\t\t * the active page list.  See list_del() in the \"else\" case of\n\t\t * !sp->root_count.\n\t\t */\n\t\tif (sp->role.invalid)\n\t\t\tlist_add(&sp->link, invalid_list);\n\t\telse\n\t\t\tlist_move(&sp->link, invalid_list);\n\t\tkvm_unaccount_mmu_page(kvm, sp);\n\t} else {\n\t\t/*\n\t\t * Remove the active root from the active page list, the root\n\t\t * will be explicitly freed when the root_count hits zero.\n\t\t */\n\t\tlist_del(&sp->link);\n\n\t\t/*\n\t\t * Obsolete pages cannot be used on any vCPUs, see the comment\n\t\t * in kvm_mmu_zap_all_fast().  Note, is_obsolete_sp() also\n\t\t * treats invalid shadow pages as being obsolete.\n\t\t */\n\t\tzapped_root = !is_obsolete_sp(kvm, sp);\n\t}\n\n\tif (sp->lpage_disallowed)\n\t\tunaccount_huge_nx_page(kvm, sp);\n\n\tsp->role.invalid = 1;\n\n\t/*\n\t * Make the request to free obsolete roots after marking the root\n\t * invalid, otherwise other vCPUs may not see it as invalid.\n\t */\n\tif (zapped_root)\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_MMU_FREE_OBSOLETE_ROOTS);\n\treturn list_unstable;\n}",
        "code_after_change": "static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,\n\t\t\t\t       struct kvm_mmu_page *sp,\n\t\t\t\t       struct list_head *invalid_list,\n\t\t\t\t       int *nr_zapped)\n{\n\tbool list_unstable, zapped_root = false;\n\n\tlockdep_assert_held_write(&kvm->mmu_lock);\n\ttrace_kvm_mmu_prepare_zap_page(sp);\n\t++kvm->stat.mmu_shadow_zapped;\n\t*nr_zapped = mmu_zap_unsync_children(kvm, sp, invalid_list);\n\t*nr_zapped += kvm_mmu_page_unlink_children(kvm, sp, invalid_list);\n\tkvm_mmu_unlink_parents(sp);\n\n\t/* Zapping children means active_mmu_pages has become unstable. */\n\tlist_unstable = *nr_zapped;\n\n\tif (!sp->role.invalid && sp_has_gptes(sp))\n\t\tunaccount_shadowed(kvm, sp);\n\n\tif (sp->unsync)\n\t\tkvm_unlink_unsync_page(kvm, sp);\n\tif (!sp->root_count) {\n\t\t/* Count self */\n\t\t(*nr_zapped)++;\n\n\t\t/*\n\t\t * Already invalid pages (previously active roots) are not on\n\t\t * the active page list.  See list_del() in the \"else\" case of\n\t\t * !sp->root_count.\n\t\t */\n\t\tif (sp->role.invalid)\n\t\t\tlist_add(&sp->link, invalid_list);\n\t\telse\n\t\t\tlist_move(&sp->link, invalid_list);\n\t\tkvm_unaccount_mmu_page(kvm, sp);\n\t} else {\n\t\t/*\n\t\t * Remove the active root from the active page list, the root\n\t\t * will be explicitly freed when the root_count hits zero.\n\t\t */\n\t\tlist_del(&sp->link);\n\n\t\t/*\n\t\t * Obsolete pages cannot be used on any vCPUs, see the comment\n\t\t * in kvm_mmu_zap_all_fast().  Note, is_obsolete_sp() also\n\t\t * treats invalid shadow pages as being obsolete.\n\t\t */\n\t\tzapped_root = !is_obsolete_sp(kvm, sp);\n\t}\n\n\tif (sp->lpage_disallowed)\n\t\tunaccount_huge_nx_page(kvm, sp);\n\n\tsp->role.invalid = 1;\n\n\t/*\n\t * Make the request to free obsolete roots after marking the root\n\t * invalid, otherwise other vCPUs may not see it as invalid.\n\t */\n\tif (zapped_root)\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_MMU_FREE_OBSOLETE_ROOTS);\n\treturn list_unstable;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n {\n \tbool list_unstable, zapped_root = false;\n \n+\tlockdep_assert_held_write(&kvm->mmu_lock);\n \ttrace_kvm_mmu_prepare_zap_page(sp);\n \t++kvm->stat.mmu_shadow_zapped;\n \t*nr_zapped = mmu_zap_unsync_children(kvm, sp, invalid_list);",
        "function_modified_lines": {
            "added": [
                "\tlockdep_assert_held_write(&kvm->mmu_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition in the x86 KVM subsystem in the Linux kernel through 6.1-rc6 allows guest OS users to cause a denial of service (host OS crash or host OS memory corruption) when nested virtualisation and the TDP MMU are enabled."
    },
    {
        "cve_id": "CVE-2022-45869",
        "code_before_change": "static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tbool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);\n\n\tunsigned long mmu_seq;\n\tint r;\n\n\tfault->gfn = fault->addr >> PAGE_SHIFT;\n\tfault->slot = kvm_vcpu_gfn_to_memslot(vcpu, fault->gfn);\n\n\tif (page_fault_handle_page_track(vcpu, fault))\n\t\treturn RET_PF_EMULATE;\n\n\tr = fast_page_fault(vcpu, fault);\n\tif (r != RET_PF_INVALID)\n\t\treturn r;\n\n\tr = mmu_topup_memory_caches(vcpu, false);\n\tif (r)\n\t\treturn r;\n\n\tmmu_seq = vcpu->kvm->mmu_invalidate_seq;\n\tsmp_rmb();\n\n\tr = kvm_faultin_pfn(vcpu, fault);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = handle_abnormal_pfn(vcpu, fault, ACC_ALL);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = RET_PF_RETRY;\n\n\tif (is_tdp_mmu_fault)\n\t\tread_lock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_lock(&vcpu->kvm->mmu_lock);\n\n\tif (is_page_fault_stale(vcpu, fault, mmu_seq))\n\t\tgoto out_unlock;\n\n\tr = make_mmu_pages_available(vcpu);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tif (is_tdp_mmu_fault)\n\t\tr = kvm_tdp_mmu_map(vcpu, fault);\n\telse\n\t\tr = __direct_map(vcpu, fault);\n\nout_unlock:\n\tif (is_tdp_mmu_fault)\n\t\tread_unlock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_unlock(&vcpu->kvm->mmu_lock);\n\tkvm_release_pfn_clean(fault->pfn);\n\treturn r;\n}",
        "code_after_change": "static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tbool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);\n\n\tunsigned long mmu_seq;\n\tint r;\n\n\tfault->gfn = fault->addr >> PAGE_SHIFT;\n\tfault->slot = kvm_vcpu_gfn_to_memslot(vcpu, fault->gfn);\n\n\tif (page_fault_handle_page_track(vcpu, fault))\n\t\treturn RET_PF_EMULATE;\n\n\tr = fast_page_fault(vcpu, fault);\n\tif (r != RET_PF_INVALID)\n\t\treturn r;\n\n\tr = mmu_topup_memory_caches(vcpu, false);\n\tif (r)\n\t\treturn r;\n\n\tmmu_seq = vcpu->kvm->mmu_invalidate_seq;\n\tsmp_rmb();\n\n\tr = kvm_faultin_pfn(vcpu, fault);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = handle_abnormal_pfn(vcpu, fault, ACC_ALL);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = RET_PF_RETRY;\n\n\tif (is_tdp_mmu_fault)\n\t\tread_lock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_lock(&vcpu->kvm->mmu_lock);\n\n\tif (is_page_fault_stale(vcpu, fault, mmu_seq))\n\t\tgoto out_unlock;\n\n\tif (is_tdp_mmu_fault) {\n\t\tr = kvm_tdp_mmu_map(vcpu, fault);\n\t} else {\n\t\tr = make_mmu_pages_available(vcpu);\n\t\tif (r)\n\t\t\tgoto out_unlock;\n\t\tr = __direct_map(vcpu, fault);\n\t}\n\nout_unlock:\n\tif (is_tdp_mmu_fault)\n\t\tread_unlock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_unlock(&vcpu->kvm->mmu_lock);\n\tkvm_release_pfn_clean(fault->pfn);\n\treturn r;\n}",
        "patch": "--- code before\n+++ code after\n@@ -40,14 +40,14 @@\n \tif (is_page_fault_stale(vcpu, fault, mmu_seq))\n \t\tgoto out_unlock;\n \n-\tr = make_mmu_pages_available(vcpu);\n-\tif (r)\n-\t\tgoto out_unlock;\n-\n-\tif (is_tdp_mmu_fault)\n+\tif (is_tdp_mmu_fault) {\n \t\tr = kvm_tdp_mmu_map(vcpu, fault);\n-\telse\n+\t} else {\n+\t\tr = make_mmu_pages_available(vcpu);\n+\t\tif (r)\n+\t\t\tgoto out_unlock;\n \t\tr = __direct_map(vcpu, fault);\n+\t}\n \n out_unlock:\n \tif (is_tdp_mmu_fault)",
        "function_modified_lines": {
            "added": [
                "\tif (is_tdp_mmu_fault) {",
                "\t} else {",
                "\t\tr = make_mmu_pages_available(vcpu);",
                "\t\tif (r)",
                "\t\t\tgoto out_unlock;",
                "\t}"
            ],
            "deleted": [
                "\tr = make_mmu_pages_available(vcpu);",
                "\tif (r)",
                "\t\tgoto out_unlock;",
                "",
                "\tif (is_tdp_mmu_fault)",
                "\telse"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition in the x86 KVM subsystem in the Linux kernel through 6.1-rc6 allows guest OS users to cause a denial of service (host OS crash or host OS memory corruption) when nested virtualisation and the TDP MMU are enabled."
    },
    {
        "cve_id": "CVE-2022-45887",
        "code_before_change": "static void ttusb_dec_exit_dvb(struct ttusb_dec *dec)\n{\n\tdprintk(\"%s\\n\", __func__);\n\n\tdvb_net_release(&dec->dvb_net);\n\tdec->demux.dmx.close(&dec->demux.dmx);\n\tdec->demux.dmx.remove_frontend(&dec->demux.dmx, &dec->frontend);\n\tdvb_dmxdev_release(&dec->dmxdev);\n\tdvb_dmx_release(&dec->demux);\n\tif (dec->fe) {\n\t\tdvb_unregister_frontend(dec->fe);\n\t\tif (dec->fe->ops.release)\n\t\t\tdec->fe->ops.release(dec->fe);\n\t}\n\tdvb_unregister_adapter(&dec->adapter);\n}",
        "code_after_change": "static void ttusb_dec_exit_dvb(struct ttusb_dec *dec)\n{\n\tdprintk(\"%s\\n\", __func__);\n\n\tdvb_net_release(&dec->dvb_net);\n\tdec->demux.dmx.close(&dec->demux.dmx);\n\tdec->demux.dmx.remove_frontend(&dec->demux.dmx, &dec->frontend);\n\tdvb_dmxdev_release(&dec->dmxdev);\n\tdvb_dmx_release(&dec->demux);\n\tif (dec->fe) {\n\t\tdvb_unregister_frontend(dec->fe);\n\t\tdvb_frontend_detach(dec->fe);\n\t}\n\tdvb_unregister_adapter(&dec->adapter);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,8 +9,7 @@\n \tdvb_dmx_release(&dec->demux);\n \tif (dec->fe) {\n \t\tdvb_unregister_frontend(dec->fe);\n-\t\tif (dec->fe->ops.release)\n-\t\t\tdec->fe->ops.release(dec->fe);\n+\t\tdvb_frontend_detach(dec->fe);\n \t}\n \tdvb_unregister_adapter(&dec->adapter);\n }",
        "function_modified_lines": {
            "added": [
                "\t\tdvb_frontend_detach(dec->fe);"
            ],
            "deleted": [
                "\t\tif (dec->fe->ops.release)",
                "\t\t\tdec->fe->ops.release(dec->fe);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-772"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/media/usb/ttusb-dec/ttusb_dec.c has a memory leak because of the lack of a dvb_frontend_detach call."
    },
    {
        "cve_id": "CVE-2022-45888",
        "code_before_change": "static int xillyusb_open(struct inode *inode, struct file *filp)\n{\n\tstruct xillyusb_dev *xdev;\n\tstruct xillyusb_channel *chan;\n\tstruct xillyfifo *in_fifo = NULL;\n\tstruct xillyusb_endpoint *out_ep = NULL;\n\tint rc;\n\tint index;\n\n\trc = xillybus_find_inode(inode, (void **)&xdev, &index);\n\tif (rc)\n\t\treturn rc;\n\n\tchan = &xdev->channels[index];\n\tfilp->private_data = chan;\n\n\tmutex_lock(&chan->lock);\n\n\trc = -ENODEV;\n\n\tif (xdev->error)\n\t\tgoto unmutex_fail;\n\n\tif (((filp->f_mode & FMODE_READ) && !chan->readable) ||\n\t    ((filp->f_mode & FMODE_WRITE) && !chan->writable))\n\t\tgoto unmutex_fail;\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_READ) &&\n\t    chan->in_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for read on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_WRITE) &&\n\t    chan->out_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for write on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\trc = -EBUSY;\n\n\tif (((filp->f_mode & FMODE_READ) && chan->open_for_read) ||\n\t    ((filp->f_mode & FMODE_WRITE) && chan->open_for_write))\n\t\tgoto unmutex_fail;\n\n\tkref_get(&xdev->kref);\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 1;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 1;\n\n\tmutex_unlock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tout_ep = endpoint_alloc(xdev,\n\t\t\t\t\t(chan->chan_idx + 2) | USB_DIR_OUT,\n\t\t\t\t\tbulk_out_work, BUF_SIZE_ORDER, BUFNUM);\n\n\t\tif (!out_ep) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto unopen;\n\t\t}\n\n\t\trc = fifo_init(&out_ep->fifo, chan->out_log2_fifo_size);\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\n\t\tout_ep->fill_mask = -(1 << chan->out_log2_element_size);\n\t\tchan->out_bytes = 0;\n\t\tchan->flushed = 0;\n\n\t\t/*\n\t\t * Sending a flush request to a previously closed stream\n\t\t * effectively opens it, and also waits until the command is\n\t\t * confirmed by the FPGA. The latter is necessary because the\n\t\t * data is sent through a separate BULK OUT endpoint, and the\n\t\t * xHCI controller is free to reorder transmissions.\n\t\t *\n\t\t * This can't go wrong unless there's a serious hardware error\n\t\t * (or the computer is stuck for 500 ms?)\n\t\t */\n\t\trc = flush_downstream(chan, XILLY_RESPONSE_TIMEOUT, false);\n\n\t\tif (rc == -ETIMEDOUT) {\n\t\t\trc = -EIO;\n\t\t\treport_io_error(xdev, rc);\n\t\t}\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\t}\n\n\tif (filp->f_mode & FMODE_READ) {\n\t\tin_fifo = kzalloc(sizeof(*in_fifo), GFP_KERNEL);\n\n\t\tif (!in_fifo) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto late_unopen;\n\t\t}\n\n\t\trc = fifo_init(in_fifo, chan->in_log2_fifo_size);\n\n\t\tif (rc) {\n\t\t\tkfree(in_fifo);\n\t\t\tgoto late_unopen;\n\t\t}\n\t}\n\n\tmutex_lock(&chan->lock);\n\tif (in_fifo) {\n\t\tchan->in_fifo = in_fifo;\n\t\tchan->read_data_ok = 1;\n\t}\n\tif (out_ep)\n\t\tchan->out_ep = out_ep;\n\tmutex_unlock(&chan->lock);\n\n\tif (in_fifo) {\n\t\tu32 in_checkpoint = 0;\n\n\t\tif (!chan->in_synchronous)\n\t\t\tin_checkpoint = in_fifo->size >>\n\t\t\t\tchan->in_log2_element_size;\n\n\t\tchan->in_consumed_bytes = 0;\n\t\tchan->poll_used = 0;\n\t\tchan->in_current_checkpoint = in_checkpoint;\n\t\trc = xillyusb_send_opcode(xdev, (chan->chan_idx << 1) | 1,\n\t\t\t\t\t  OPCODE_SET_CHECKPOINT,\n\t\t\t\t\t  in_checkpoint);\n\n\t\tif (rc) /* Failure guarantees that opcode wasn't sent */\n\t\t\tgoto unfifo;\n\n\t\t/*\n\t\t * In non-blocking mode, request the FPGA to send any data it\n\t\t * has right away. Otherwise, the first read() will always\n\t\t * return -EAGAIN, which is OK strictly speaking, but ugly.\n\t\t * Checking and unrolling if this fails isn't worth the\n\t\t * effort -- the error is propagated to the first read()\n\t\t * anyhow.\n\t\t */\n\t\tif (filp->f_flags & O_NONBLOCK)\n\t\t\trequest_read_anything(chan, OPCODE_SET_PUSH);\n\t}\n\n\treturn 0;\n\nunfifo:\n\tchan->read_data_ok = 0;\n\tsafely_assign_in_fifo(chan, NULL);\n\tfifo_mem_release(in_fifo);\n\tkfree(in_fifo);\n\n\tif (out_ep) {\n\t\tmutex_lock(&chan->lock);\n\t\tchan->out_ep = NULL;\n\t\tmutex_unlock(&chan->lock);\n\t}\n\nlate_unopen:\n\tif (out_ep)\n\t\tendpoint_dealloc(out_ep);\n\nunopen:\n\tmutex_lock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 0;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 0;\n\n\tmutex_unlock(&chan->lock);\n\n\tkref_put(&xdev->kref, cleanup_dev);\n\n\treturn rc;\n\nunmutex_fail:\n\tmutex_unlock(&chan->lock);\n\treturn rc;\n}",
        "code_after_change": "static int xillyusb_open(struct inode *inode, struct file *filp)\n{\n\tstruct xillyusb_dev *xdev;\n\tstruct xillyusb_channel *chan;\n\tstruct xillyfifo *in_fifo = NULL;\n\tstruct xillyusb_endpoint *out_ep = NULL;\n\tint rc;\n\tint index;\n\n\tmutex_lock(&kref_mutex);\n\n\trc = xillybus_find_inode(inode, (void **)&xdev, &index);\n\tif (rc) {\n\t\tmutex_unlock(&kref_mutex);\n\t\treturn rc;\n\t}\n\n\tkref_get(&xdev->kref);\n\tmutex_unlock(&kref_mutex);\n\n\tchan = &xdev->channels[index];\n\tfilp->private_data = chan;\n\n\tmutex_lock(&chan->lock);\n\n\trc = -ENODEV;\n\n\tif (xdev->error)\n\t\tgoto unmutex_fail;\n\n\tif (((filp->f_mode & FMODE_READ) && !chan->readable) ||\n\t    ((filp->f_mode & FMODE_WRITE) && !chan->writable))\n\t\tgoto unmutex_fail;\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_READ) &&\n\t    chan->in_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for read on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\tif ((filp->f_flags & O_NONBLOCK) && (filp->f_mode & FMODE_WRITE) &&\n\t    chan->out_synchronous) {\n\t\tdev_err(xdev->dev,\n\t\t\t\"open() failed: O_NONBLOCK not allowed for write on this device\\n\");\n\t\tgoto unmutex_fail;\n\t}\n\n\trc = -EBUSY;\n\n\tif (((filp->f_mode & FMODE_READ) && chan->open_for_read) ||\n\t    ((filp->f_mode & FMODE_WRITE) && chan->open_for_write))\n\t\tgoto unmutex_fail;\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 1;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 1;\n\n\tmutex_unlock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tout_ep = endpoint_alloc(xdev,\n\t\t\t\t\t(chan->chan_idx + 2) | USB_DIR_OUT,\n\t\t\t\t\tbulk_out_work, BUF_SIZE_ORDER, BUFNUM);\n\n\t\tif (!out_ep) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto unopen;\n\t\t}\n\n\t\trc = fifo_init(&out_ep->fifo, chan->out_log2_fifo_size);\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\n\t\tout_ep->fill_mask = -(1 << chan->out_log2_element_size);\n\t\tchan->out_bytes = 0;\n\t\tchan->flushed = 0;\n\n\t\t/*\n\t\t * Sending a flush request to a previously closed stream\n\t\t * effectively opens it, and also waits until the command is\n\t\t * confirmed by the FPGA. The latter is necessary because the\n\t\t * data is sent through a separate BULK OUT endpoint, and the\n\t\t * xHCI controller is free to reorder transmissions.\n\t\t *\n\t\t * This can't go wrong unless there's a serious hardware error\n\t\t * (or the computer is stuck for 500 ms?)\n\t\t */\n\t\trc = flush_downstream(chan, XILLY_RESPONSE_TIMEOUT, false);\n\n\t\tif (rc == -ETIMEDOUT) {\n\t\t\trc = -EIO;\n\t\t\treport_io_error(xdev, rc);\n\t\t}\n\n\t\tif (rc)\n\t\t\tgoto late_unopen;\n\t}\n\n\tif (filp->f_mode & FMODE_READ) {\n\t\tin_fifo = kzalloc(sizeof(*in_fifo), GFP_KERNEL);\n\n\t\tif (!in_fifo) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto late_unopen;\n\t\t}\n\n\t\trc = fifo_init(in_fifo, chan->in_log2_fifo_size);\n\n\t\tif (rc) {\n\t\t\tkfree(in_fifo);\n\t\t\tgoto late_unopen;\n\t\t}\n\t}\n\n\tmutex_lock(&chan->lock);\n\tif (in_fifo) {\n\t\tchan->in_fifo = in_fifo;\n\t\tchan->read_data_ok = 1;\n\t}\n\tif (out_ep)\n\t\tchan->out_ep = out_ep;\n\tmutex_unlock(&chan->lock);\n\n\tif (in_fifo) {\n\t\tu32 in_checkpoint = 0;\n\n\t\tif (!chan->in_synchronous)\n\t\t\tin_checkpoint = in_fifo->size >>\n\t\t\t\tchan->in_log2_element_size;\n\n\t\tchan->in_consumed_bytes = 0;\n\t\tchan->poll_used = 0;\n\t\tchan->in_current_checkpoint = in_checkpoint;\n\t\trc = xillyusb_send_opcode(xdev, (chan->chan_idx << 1) | 1,\n\t\t\t\t\t  OPCODE_SET_CHECKPOINT,\n\t\t\t\t\t  in_checkpoint);\n\n\t\tif (rc) /* Failure guarantees that opcode wasn't sent */\n\t\t\tgoto unfifo;\n\n\t\t/*\n\t\t * In non-blocking mode, request the FPGA to send any data it\n\t\t * has right away. Otherwise, the first read() will always\n\t\t * return -EAGAIN, which is OK strictly speaking, but ugly.\n\t\t * Checking and unrolling if this fails isn't worth the\n\t\t * effort -- the error is propagated to the first read()\n\t\t * anyhow.\n\t\t */\n\t\tif (filp->f_flags & O_NONBLOCK)\n\t\t\trequest_read_anything(chan, OPCODE_SET_PUSH);\n\t}\n\n\treturn 0;\n\nunfifo:\n\tchan->read_data_ok = 0;\n\tsafely_assign_in_fifo(chan, NULL);\n\tfifo_mem_release(in_fifo);\n\tkfree(in_fifo);\n\n\tif (out_ep) {\n\t\tmutex_lock(&chan->lock);\n\t\tchan->out_ep = NULL;\n\t\tmutex_unlock(&chan->lock);\n\t}\n\nlate_unopen:\n\tif (out_ep)\n\t\tendpoint_dealloc(out_ep);\n\nunopen:\n\tmutex_lock(&chan->lock);\n\n\tif (filp->f_mode & FMODE_READ)\n\t\tchan->open_for_read = 0;\n\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tchan->open_for_write = 0;\n\n\tmutex_unlock(&chan->lock);\n\n\tkref_put(&xdev->kref, cleanup_dev);\n\n\treturn rc;\n\nunmutex_fail:\n\tkref_put(&xdev->kref, cleanup_dev);\n\tmutex_unlock(&chan->lock);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,9 +7,16 @@\n \tint rc;\n \tint index;\n \n+\tmutex_lock(&kref_mutex);\n+\n \trc = xillybus_find_inode(inode, (void **)&xdev, &index);\n-\tif (rc)\n+\tif (rc) {\n+\t\tmutex_unlock(&kref_mutex);\n \t\treturn rc;\n+\t}\n+\n+\tkref_get(&xdev->kref);\n+\tmutex_unlock(&kref_mutex);\n \n \tchan = &xdev->channels[index];\n \tfilp->private_data = chan;\n@@ -44,8 +51,6 @@\n \tif (((filp->f_mode & FMODE_READ) && chan->open_for_read) ||\n \t    ((filp->f_mode & FMODE_WRITE) && chan->open_for_write))\n \t\tgoto unmutex_fail;\n-\n-\tkref_get(&xdev->kref);\n \n \tif (filp->f_mode & FMODE_READ)\n \t\tchan->open_for_read = 1;\n@@ -183,6 +188,7 @@\n \treturn rc;\n \n unmutex_fail:\n+\tkref_put(&xdev->kref, cleanup_dev);\n \tmutex_unlock(&chan->lock);\n \treturn rc;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&kref_mutex);",
                "",
                "\tif (rc) {",
                "\t\tmutex_unlock(&kref_mutex);",
                "\t}",
                "",
                "\tkref_get(&xdev->kref);",
                "\tmutex_unlock(&kref_mutex);",
                "\tkref_put(&xdev->kref, cleanup_dev);"
            ],
            "deleted": [
                "\tif (rc)",
                "",
                "\tkref_get(&xdev->kref);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/char/xillybus/xillyusb.c has a race condition and use-after-free during physical removal of a USB device."
    },
    {
        "cve_id": "CVE-2022-45888",
        "code_before_change": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tkref_put(&xdev->kref, cleanup_dev);\n}",
        "code_after_change": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tmutex_lock(&kref_mutex);\n\tkref_put(&xdev->kref, cleanup_dev);\n\tmutex_unlock(&kref_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -61,5 +61,7 @@\n \n \txdev->dev = NULL;\n \n+\tmutex_lock(&kref_mutex);\n \tkref_put(&xdev->kref, cleanup_dev);\n+\tmutex_unlock(&kref_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&kref_mutex);",
                "\tmutex_unlock(&kref_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/char/xillybus/xillyusb.c has a race condition and use-after-free during physical removal of a USB device."
    },
    {
        "cve_id": "CVE-2023-1582",
        "code_before_change": "static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,\n\t\t\t     struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct pagemapread *pm = walk->private;\n\tspinlock_t *ptl;\n\tpte_t *pte, *orig_pte;\n\tint err = 0;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tptl = pmd_trans_huge_lock(pmdp, vma);\n\tif (ptl) {\n\t\tu64 flags = 0, frame = 0;\n\t\tpmd_t pmd = *pmdp;\n\t\tstruct page *page = NULL;\n\n\t\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\t\tflags |= PM_SOFT_DIRTY;\n\n\t\tif (pmd_present(pmd)) {\n\t\t\tpage = pmd_page(pmd);\n\n\t\t\tflags |= PM_PRESENT;\n\t\t\tif (pmd_soft_dirty(pmd))\n\t\t\t\tflags |= PM_SOFT_DIRTY;\n\t\t\tif (pmd_uffd_wp(pmd))\n\t\t\t\tflags |= PM_UFFD_WP;\n\t\t\tif (pm->show_pfn)\n\t\t\t\tframe = pmd_pfn(pmd) +\n\t\t\t\t\t((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\t\t}\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\t\telse if (is_swap_pmd(pmd)) {\n\t\t\tswp_entry_t entry = pmd_to_swp_entry(pmd);\n\t\t\tunsigned long offset;\n\n\t\t\tif (pm->show_pfn) {\n\t\t\t\toffset = swp_offset(entry) +\n\t\t\t\t\t((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\t\t\t\tframe = swp_type(entry) |\n\t\t\t\t\t(offset << MAX_SWAPFILES_SHIFT);\n\t\t\t}\n\t\t\tflags |= PM_SWAP;\n\t\t\tif (pmd_swp_soft_dirty(pmd))\n\t\t\t\tflags |= PM_SOFT_DIRTY;\n\t\t\tif (pmd_swp_uffd_wp(pmd))\n\t\t\t\tflags |= PM_UFFD_WP;\n\t\t\tVM_BUG_ON(!is_pmd_migration_entry(pmd));\n\t\t\tpage = pfn_swap_entry_to_page(entry);\n\t\t}\n#endif\n\n\t\tif (page && page_mapcount(page) == 1)\n\t\t\tflags |= PM_MMAP_EXCLUSIVE;\n\n\t\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\t\tpagemap_entry_t pme = make_pme(frame, flags);\n\n\t\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (pm->show_pfn) {\n\t\t\t\tif (flags & PM_PRESENT)\n\t\t\t\t\tframe++;\n\t\t\t\telse if (flags & PM_SWAP)\n\t\t\t\t\tframe += (1 << MAX_SWAPFILES_SHIFT);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(ptl);\n\t\treturn err;\n\t}\n\n\tif (pmd_trans_unstable(pmdp))\n\t\treturn 0;\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\n\t/*\n\t * We can assume that @vma always points to a valid one and @end never\n\t * goes beyond vma->vm_end.\n\t */\n\torig_pte = pte = pte_offset_map_lock(walk->mm, pmdp, addr, &ptl);\n\tfor (; addr < end; pte++, addr += PAGE_SIZE) {\n\t\tpagemap_entry_t pme;\n\n\t\tpme = pte_to_pagemap_entry(pm, vma, addr, *pte);\n\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tpte_unmap_unlock(orig_pte, ptl);\n\n\tcond_resched();\n\n\treturn err;\n}",
        "code_after_change": "static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,\n\t\t\t     struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct pagemapread *pm = walk->private;\n\tspinlock_t *ptl;\n\tpte_t *pte, *orig_pte;\n\tint err = 0;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tbool migration = false;\n\n\tptl = pmd_trans_huge_lock(pmdp, vma);\n\tif (ptl) {\n\t\tu64 flags = 0, frame = 0;\n\t\tpmd_t pmd = *pmdp;\n\t\tstruct page *page = NULL;\n\n\t\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\t\tflags |= PM_SOFT_DIRTY;\n\n\t\tif (pmd_present(pmd)) {\n\t\t\tpage = pmd_page(pmd);\n\n\t\t\tflags |= PM_PRESENT;\n\t\t\tif (pmd_soft_dirty(pmd))\n\t\t\t\tflags |= PM_SOFT_DIRTY;\n\t\t\tif (pmd_uffd_wp(pmd))\n\t\t\t\tflags |= PM_UFFD_WP;\n\t\t\tif (pm->show_pfn)\n\t\t\t\tframe = pmd_pfn(pmd) +\n\t\t\t\t\t((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\t\t}\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\t\telse if (is_swap_pmd(pmd)) {\n\t\t\tswp_entry_t entry = pmd_to_swp_entry(pmd);\n\t\t\tunsigned long offset;\n\n\t\t\tif (pm->show_pfn) {\n\t\t\t\toffset = swp_offset(entry) +\n\t\t\t\t\t((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\t\t\t\tframe = swp_type(entry) |\n\t\t\t\t\t(offset << MAX_SWAPFILES_SHIFT);\n\t\t\t}\n\t\t\tflags |= PM_SWAP;\n\t\t\tif (pmd_swp_soft_dirty(pmd))\n\t\t\t\tflags |= PM_SOFT_DIRTY;\n\t\t\tif (pmd_swp_uffd_wp(pmd))\n\t\t\t\tflags |= PM_UFFD_WP;\n\t\t\tVM_BUG_ON(!is_pmd_migration_entry(pmd));\n\t\t\tmigration = is_migration_entry(entry);\n\t\t\tpage = pfn_swap_entry_to_page(entry);\n\t\t}\n#endif\n\n\t\tif (page && !migration && page_mapcount(page) == 1)\n\t\t\tflags |= PM_MMAP_EXCLUSIVE;\n\n\t\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\t\tpagemap_entry_t pme = make_pme(frame, flags);\n\n\t\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (pm->show_pfn) {\n\t\t\t\tif (flags & PM_PRESENT)\n\t\t\t\t\tframe++;\n\t\t\t\telse if (flags & PM_SWAP)\n\t\t\t\t\tframe += (1 << MAX_SWAPFILES_SHIFT);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(ptl);\n\t\treturn err;\n\t}\n\n\tif (pmd_trans_unstable(pmdp))\n\t\treturn 0;\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\n\t/*\n\t * We can assume that @vma always points to a valid one and @end never\n\t * goes beyond vma->vm_end.\n\t */\n\torig_pte = pte = pte_offset_map_lock(walk->mm, pmdp, addr, &ptl);\n\tfor (; addr < end; pte++, addr += PAGE_SIZE) {\n\t\tpagemap_entry_t pme;\n\n\t\tpme = pte_to_pagemap_entry(pm, vma, addr, *pte);\n\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tpte_unmap_unlock(orig_pte, ptl);\n\n\tcond_resched();\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,9 @@\n \tspinlock_t *ptl;\n \tpte_t *pte, *orig_pte;\n \tint err = 0;\n+#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n+\tbool migration = false;\n \n-#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n \tptl = pmd_trans_huge_lock(pmdp, vma);\n \tif (ptl) {\n \t\tu64 flags = 0, frame = 0;\n@@ -46,11 +47,12 @@\n \t\t\tif (pmd_swp_uffd_wp(pmd))\n \t\t\t\tflags |= PM_UFFD_WP;\n \t\t\tVM_BUG_ON(!is_pmd_migration_entry(pmd));\n+\t\t\tmigration = is_migration_entry(entry);\n \t\t\tpage = pfn_swap_entry_to_page(entry);\n \t\t}\n #endif\n \n-\t\tif (page && page_mapcount(page) == 1)\n+\t\tif (page && !migration && page_mapcount(page) == 1)\n \t\t\tflags |= PM_MMAP_EXCLUSIVE;\n \n \t\tfor (; addr != end; addr += PAGE_SIZE) {",
        "function_modified_lines": {
            "added": [
                "#ifdef CONFIG_TRANSPARENT_HUGEPAGE",
                "\tbool migration = false;",
                "\t\t\tmigration = is_migration_entry(entry);",
                "\t\tif (page && !migration && page_mapcount(page) == 1)"
            ],
            "deleted": [
                "#ifdef CONFIG_TRANSPARENT_HUGEPAGE",
                "\t\tif (page && page_mapcount(page) == 1)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race problem was found in fs/proc/task_mmu.c in the memory management sub-component in the Linux kernel. This issue may allow a local attacker with user privilege to cause a denial of service."
    },
    {
        "cve_id": "CVE-2023-1582",
        "code_before_change": "static void smaps_pte_entry(pte_t *pte, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page = NULL;\n\n\tif (pte_present(*pte)) {\n\t\tpage = vm_normal_page(vma, addr, *pte);\n\t} else if (is_swap_pte(*pte)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(*pte);\n\n\t\tif (!non_swap_entry(swpent)) {\n\t\t\tint mapcount;\n\n\t\t\tmss->swap += PAGE_SIZE;\n\t\t\tmapcount = swp_swapcount(swpent);\n\t\t\tif (mapcount >= 2) {\n\t\t\t\tu64 pss_delta = (u64)PAGE_SIZE << PSS_SHIFT;\n\n\t\t\t\tdo_div(pss_delta, mapcount);\n\t\t\t\tmss->swap_pss += pss_delta;\n\t\t\t} else {\n\t\t\t\tmss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;\n\t\t\t}\n\t\t} else if (is_pfn_swap_entry(swpent))\n\t\t\tpage = pfn_swap_entry_to_page(swpent);\n\t} else {\n\t\tsmaps_pte_hole_lookup(addr, walk);\n\t\treturn;\n\t}\n\n\tif (!page)\n\t\treturn;\n\n\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte), locked);\n}",
        "code_after_change": "static void smaps_pte_entry(pte_t *pte, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page = NULL;\n\tbool migration = false;\n\n\tif (pte_present(*pte)) {\n\t\tpage = vm_normal_page(vma, addr, *pte);\n\t} else if (is_swap_pte(*pte)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(*pte);\n\n\t\tif (!non_swap_entry(swpent)) {\n\t\t\tint mapcount;\n\n\t\t\tmss->swap += PAGE_SIZE;\n\t\t\tmapcount = swp_swapcount(swpent);\n\t\t\tif (mapcount >= 2) {\n\t\t\t\tu64 pss_delta = (u64)PAGE_SIZE << PSS_SHIFT;\n\n\t\t\t\tdo_div(pss_delta, mapcount);\n\t\t\t\tmss->swap_pss += pss_delta;\n\t\t\t} else {\n\t\t\t\tmss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;\n\t\t\t}\n\t\t} else if (is_pfn_swap_entry(swpent)) {\n\t\t\tif (is_migration_entry(swpent))\n\t\t\t\tmigration = true;\n\t\t\tpage = pfn_swap_entry_to_page(swpent);\n\t\t}\n\t} else {\n\t\tsmaps_pte_hole_lookup(addr, walk);\n\t\treturn;\n\t}\n\n\tif (!page)\n\t\treturn;\n\n\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte),\n\t\t      locked, migration);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tstruct vm_area_struct *vma = walk->vma;\n \tbool locked = !!(vma->vm_flags & VM_LOCKED);\n \tstruct page *page = NULL;\n+\tbool migration = false;\n \n \tif (pte_present(*pte)) {\n \t\tpage = vm_normal_page(vma, addr, *pte);\n@@ -24,8 +25,11 @@\n \t\t\t} else {\n \t\t\t\tmss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;\n \t\t\t}\n-\t\t} else if (is_pfn_swap_entry(swpent))\n+\t\t} else if (is_pfn_swap_entry(swpent)) {\n+\t\t\tif (is_migration_entry(swpent))\n+\t\t\t\tmigration = true;\n \t\t\tpage = pfn_swap_entry_to_page(swpent);\n+\t\t}\n \t} else {\n \t\tsmaps_pte_hole_lookup(addr, walk);\n \t\treturn;\n@@ -34,5 +38,6 @@\n \tif (!page)\n \t\treturn;\n \n-\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte), locked);\n+\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte),\n+\t\t      locked, migration);\n }",
        "function_modified_lines": {
            "added": [
                "\tbool migration = false;",
                "\t\t} else if (is_pfn_swap_entry(swpent)) {",
                "\t\t\tif (is_migration_entry(swpent))",
                "\t\t\t\tmigration = true;",
                "\t\t}",
                "\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte),",
                "\t\t      locked, migration);"
            ],
            "deleted": [
                "\t\t} else if (is_pfn_swap_entry(swpent))",
                "\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte), locked);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race problem was found in fs/proc/task_mmu.c in the memory management sub-component in the Linux kernel. This issue may allow a local attacker with user privilege to cause a denial of service."
    },
    {
        "cve_id": "CVE-2023-1582",
        "code_before_change": "static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,\n\t\tstruct vm_area_struct *vma, unsigned long addr, pte_t pte)\n{\n\tu64 frame = 0, flags = 0;\n\tstruct page *page = NULL;\n\n\tif (pte_present(pte)) {\n\t\tif (pm->show_pfn)\n\t\t\tframe = pte_pfn(pte);\n\t\tflags |= PM_PRESENT;\n\t\tpage = vm_normal_page(vma, addr, pte);\n\t\tif (pte_soft_dirty(pte))\n\t\t\tflags |= PM_SOFT_DIRTY;\n\t\tif (pte_uffd_wp(pte))\n\t\t\tflags |= PM_UFFD_WP;\n\t} else if (is_swap_pte(pte)) {\n\t\tswp_entry_t entry;\n\t\tif (pte_swp_soft_dirty(pte))\n\t\t\tflags |= PM_SOFT_DIRTY;\n\t\tif (pte_swp_uffd_wp(pte))\n\t\t\tflags |= PM_UFFD_WP;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (pm->show_pfn)\n\t\t\tframe = swp_type(entry) |\n\t\t\t\t(swp_offset(entry) << MAX_SWAPFILES_SHIFT);\n\t\tflags |= PM_SWAP;\n\t\tif (is_pfn_swap_entry(entry))\n\t\t\tpage = pfn_swap_entry_to_page(entry);\n\t}\n\n\tif (page && !PageAnon(page))\n\t\tflags |= PM_FILE;\n\tif (page && page_mapcount(page) == 1)\n\t\tflags |= PM_MMAP_EXCLUSIVE;\n\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\tflags |= PM_SOFT_DIRTY;\n\n\treturn make_pme(frame, flags);\n}",
        "code_after_change": "static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,\n\t\tstruct vm_area_struct *vma, unsigned long addr, pte_t pte)\n{\n\tu64 frame = 0, flags = 0;\n\tstruct page *page = NULL;\n\tbool migration = false;\n\n\tif (pte_present(pte)) {\n\t\tif (pm->show_pfn)\n\t\t\tframe = pte_pfn(pte);\n\t\tflags |= PM_PRESENT;\n\t\tpage = vm_normal_page(vma, addr, pte);\n\t\tif (pte_soft_dirty(pte))\n\t\t\tflags |= PM_SOFT_DIRTY;\n\t\tif (pte_uffd_wp(pte))\n\t\t\tflags |= PM_UFFD_WP;\n\t} else if (is_swap_pte(pte)) {\n\t\tswp_entry_t entry;\n\t\tif (pte_swp_soft_dirty(pte))\n\t\t\tflags |= PM_SOFT_DIRTY;\n\t\tif (pte_swp_uffd_wp(pte))\n\t\t\tflags |= PM_UFFD_WP;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (pm->show_pfn)\n\t\t\tframe = swp_type(entry) |\n\t\t\t\t(swp_offset(entry) << MAX_SWAPFILES_SHIFT);\n\t\tflags |= PM_SWAP;\n\t\tmigration = is_migration_entry(entry);\n\t\tif (is_pfn_swap_entry(entry))\n\t\t\tpage = pfn_swap_entry_to_page(entry);\n\t}\n\n\tif (page && !PageAnon(page))\n\t\tflags |= PM_FILE;\n\tif (page && !migration && page_mapcount(page) == 1)\n\t\tflags |= PM_MMAP_EXCLUSIVE;\n\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\tflags |= PM_SOFT_DIRTY;\n\n\treturn make_pme(frame, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n {\n \tu64 frame = 0, flags = 0;\n \tstruct page *page = NULL;\n+\tbool migration = false;\n \n \tif (pte_present(pte)) {\n \t\tif (pm->show_pfn)\n@@ -24,13 +25,14 @@\n \t\t\tframe = swp_type(entry) |\n \t\t\t\t(swp_offset(entry) << MAX_SWAPFILES_SHIFT);\n \t\tflags |= PM_SWAP;\n+\t\tmigration = is_migration_entry(entry);\n \t\tif (is_pfn_swap_entry(entry))\n \t\t\tpage = pfn_swap_entry_to_page(entry);\n \t}\n \n \tif (page && !PageAnon(page))\n \t\tflags |= PM_FILE;\n-\tif (page && page_mapcount(page) == 1)\n+\tif (page && !migration && page_mapcount(page) == 1)\n \t\tflags |= PM_MMAP_EXCLUSIVE;\n \tif (vma->vm_flags & VM_SOFTDIRTY)\n \t\tflags |= PM_SOFT_DIRTY;",
        "function_modified_lines": {
            "added": [
                "\tbool migration = false;",
                "\t\tmigration = is_migration_entry(entry);",
                "\tif (page && !migration && page_mapcount(page) == 1)"
            ],
            "deleted": [
                "\tif (page && page_mapcount(page) == 1)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race problem was found in fs/proc/task_mmu.c in the memory management sub-component in the Linux kernel. This issue may allow a local attacker with user privilege to cause a denial of service."
    },
    {
        "cve_id": "CVE-2023-2006",
        "code_before_change": "int rxrpc_connect_call(struct rxrpc_sock *rx,\n\t\t       struct rxrpc_call *call,\n\t\t       struct rxrpc_conn_parameters *cp,\n\t\t       struct sockaddr_rxrpc *srx,\n\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\tstruct rxrpc_net *rxnet = cp->local->rxnet;\n\tint ret = 0;\n\n\t_enter(\"{%d,%lx},\", call->debug_id, call->user_call_ID);\n\n\trxrpc_discard_expired_client_conns(&rxnet->client_conn_reaper);\n\n\tbundle = rxrpc_prep_call(rx, call, cp, srx, gfp);\n\tif (IS_ERR(bundle)) {\n\t\tret = PTR_ERR(bundle);\n\t\tgoto out;\n\t}\n\n\tif (call->state == RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = rxrpc_wait_for_channel(bundle, call, gfp);\n\t\tif (ret < 0)\n\t\t\tgoto wait_failed;\n\t}\n\ngranted_channel:\n\t/* Paired with the write barrier in rxrpc_activate_one_channel(). */\n\tsmp_rmb();\n\nout_put_bundle:\n\trxrpc_put_bundle(bundle);\nout:\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\nwait_failed:\n\tspin_lock(&bundle->channel_lock);\n\tlist_del_init(&call->chan_wait_link);\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (call->state != RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = 0;\n\t\tgoto granted_channel;\n\t}\n\n\ttrace_rxrpc_client(call->conn, ret, rxrpc_client_chan_wait_failed);\n\trxrpc_set_call_completion(call, RXRPC_CALL_LOCAL_ERROR, 0, ret);\n\trxrpc_disconnect_client_call(bundle, call);\n\tgoto out_put_bundle;\n}",
        "code_after_change": "int rxrpc_connect_call(struct rxrpc_sock *rx,\n\t\t       struct rxrpc_call *call,\n\t\t       struct rxrpc_conn_parameters *cp,\n\t\t       struct sockaddr_rxrpc *srx,\n\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\tstruct rxrpc_net *rxnet = cp->local->rxnet;\n\tint ret = 0;\n\n\t_enter(\"{%d,%lx},\", call->debug_id, call->user_call_ID);\n\n\trxrpc_discard_expired_client_conns(&rxnet->client_conn_reaper);\n\n\tbundle = rxrpc_prep_call(rx, call, cp, srx, gfp);\n\tif (IS_ERR(bundle)) {\n\t\tret = PTR_ERR(bundle);\n\t\tgoto out;\n\t}\n\n\tif (call->state == RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = rxrpc_wait_for_channel(bundle, call, gfp);\n\t\tif (ret < 0)\n\t\t\tgoto wait_failed;\n\t}\n\ngranted_channel:\n\t/* Paired with the write barrier in rxrpc_activate_one_channel(). */\n\tsmp_rmb();\n\nout_put_bundle:\n\trxrpc_deactivate_bundle(bundle);\n\trxrpc_put_bundle(bundle);\nout:\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\nwait_failed:\n\tspin_lock(&bundle->channel_lock);\n\tlist_del_init(&call->chan_wait_link);\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (call->state != RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = 0;\n\t\tgoto granted_channel;\n\t}\n\n\ttrace_rxrpc_client(call->conn, ret, rxrpc_client_chan_wait_failed);\n\trxrpc_set_call_completion(call, RXRPC_CALL_LOCAL_ERROR, 0, ret);\n\trxrpc_disconnect_client_call(bundle, call);\n\tgoto out_put_bundle;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,6 +29,7 @@\n \tsmp_rmb();\n \n out_put_bundle:\n+\trxrpc_deactivate_bundle(bundle);\n \trxrpc_put_bundle(bundle);\n out:\n \t_leave(\" = %d\", ret);",
        "function_modified_lines": {
            "added": [
                "\trxrpc_deactivate_bundle(bundle);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-2006",
        "code_before_change": "static struct rxrpc_bundle *rxrpc_alloc_bundle(struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\n\tbundle = kzalloc(sizeof(*bundle), gfp);\n\tif (bundle) {\n\t\tbundle->params = *cp;\n\t\trxrpc_get_peer(bundle->params.peer);\n\t\trefcount_set(&bundle->ref, 1);\n\t\tspin_lock_init(&bundle->channel_lock);\n\t\tINIT_LIST_HEAD(&bundle->waiting_calls);\n\t}\n\treturn bundle;\n}",
        "code_after_change": "static struct rxrpc_bundle *rxrpc_alloc_bundle(struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\n\tbundle = kzalloc(sizeof(*bundle), gfp);\n\tif (bundle) {\n\t\tbundle->params = *cp;\n\t\trxrpc_get_peer(bundle->params.peer);\n\t\trefcount_set(&bundle->ref, 1);\n\t\tatomic_set(&bundle->active, 1);\n\t\tspin_lock_init(&bundle->channel_lock);\n\t\tINIT_LIST_HEAD(&bundle->waiting_calls);\n\t}\n\treturn bundle;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,7 @@\n \t\tbundle->params = *cp;\n \t\trxrpc_get_peer(bundle->params.peer);\n \t\trefcount_set(&bundle->ref, 1);\n+\t\tatomic_set(&bundle->active, 1);\n \t\tspin_lock_init(&bundle->channel_lock);\n \t\tINIT_LIST_HEAD(&bundle->waiting_calls);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tatomic_set(&bundle->active, 1);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-2006",
        "code_before_change": "static void rxrpc_unbundle_conn(struct rxrpc_connection *conn)\n{\n\tstruct rxrpc_bundle *bundle = conn->bundle;\n\tstruct rxrpc_local *local = bundle->params.local;\n\tunsigned int bindex;\n\tbool need_drop = false, need_put = false;\n\tint i;\n\n\t_enter(\"C=%x\", conn->debug_id);\n\n\tif (conn->flags & RXRPC_CONN_FINAL_ACK_MASK)\n\t\trxrpc_process_delayed_final_acks(conn, true);\n\n\tspin_lock(&bundle->channel_lock);\n\tbindex = conn->bundle_shift / RXRPC_MAXCALLS;\n\tif (bundle->conns[bindex] == conn) {\n\t\t_debug(\"clear slot %u\", bindex);\n\t\tbundle->conns[bindex] = NULL;\n\t\tfor (i = 0; i < RXRPC_MAXCALLS; i++)\n\t\t\tclear_bit(conn->bundle_shift + i, &bundle->avail_chans);\n\t\tneed_drop = true;\n\t}\n\tspin_unlock(&bundle->channel_lock);\n\n\t/* If there are no more connections, remove the bundle */\n\tif (!bundle->avail_chans) {\n\t\t_debug(\"maybe unbundle\");\n\t\tspin_lock(&local->client_bundles_lock);\n\n\t\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++)\n\t\t\tif (bundle->conns[i])\n\t\t\t\tbreak;\n\t\tif (i == ARRAY_SIZE(bundle->conns) && !bundle->params.exclusive) {\n\t\t\t_debug(\"erase bundle\");\n\t\t\trb_erase(&bundle->local_node, &local->client_bundles);\n\t\t\tneed_put = true;\n\t\t}\n\n\t\tspin_unlock(&local->client_bundles_lock);\n\t\tif (need_put)\n\t\t\trxrpc_put_bundle(bundle);\n\t}\n\n\tif (need_drop)\n\t\trxrpc_put_connection(conn);\n\t_leave(\"\");\n}",
        "code_after_change": "static void rxrpc_unbundle_conn(struct rxrpc_connection *conn)\n{\n\tstruct rxrpc_bundle *bundle = conn->bundle;\n\tunsigned int bindex;\n\tbool need_drop = false;\n\tint i;\n\n\t_enter(\"C=%x\", conn->debug_id);\n\n\tif (conn->flags & RXRPC_CONN_FINAL_ACK_MASK)\n\t\trxrpc_process_delayed_final_acks(conn, true);\n\n\tspin_lock(&bundle->channel_lock);\n\tbindex = conn->bundle_shift / RXRPC_MAXCALLS;\n\tif (bundle->conns[bindex] == conn) {\n\t\t_debug(\"clear slot %u\", bindex);\n\t\tbundle->conns[bindex] = NULL;\n\t\tfor (i = 0; i < RXRPC_MAXCALLS; i++)\n\t\t\tclear_bit(conn->bundle_shift + i, &bundle->avail_chans);\n\t\tneed_drop = true;\n\t}\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (need_drop) {\n\t\trxrpc_deactivate_bundle(bundle);\n\t\trxrpc_put_connection(conn);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,9 +1,8 @@\n static void rxrpc_unbundle_conn(struct rxrpc_connection *conn)\n {\n \tstruct rxrpc_bundle *bundle = conn->bundle;\n-\tstruct rxrpc_local *local = bundle->params.local;\n \tunsigned int bindex;\n-\tbool need_drop = false, need_put = false;\n+\tbool need_drop = false;\n \tint i;\n \n \t_enter(\"C=%x\", conn->debug_id);\n@@ -22,26 +21,8 @@\n \t}\n \tspin_unlock(&bundle->channel_lock);\n \n-\t/* If there are no more connections, remove the bundle */\n-\tif (!bundle->avail_chans) {\n-\t\t_debug(\"maybe unbundle\");\n-\t\tspin_lock(&local->client_bundles_lock);\n-\n-\t\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++)\n-\t\t\tif (bundle->conns[i])\n-\t\t\t\tbreak;\n-\t\tif (i == ARRAY_SIZE(bundle->conns) && !bundle->params.exclusive) {\n-\t\t\t_debug(\"erase bundle\");\n-\t\t\trb_erase(&bundle->local_node, &local->client_bundles);\n-\t\t\tneed_put = true;\n-\t\t}\n-\n-\t\tspin_unlock(&local->client_bundles_lock);\n-\t\tif (need_put)\n-\t\t\trxrpc_put_bundle(bundle);\n+\tif (need_drop) {\n+\t\trxrpc_deactivate_bundle(bundle);\n+\t\trxrpc_put_connection(conn);\n \t}\n-\n-\tif (need_drop)\n-\t\trxrpc_put_connection(conn);\n-\t_leave(\"\");\n }",
        "function_modified_lines": {
            "added": [
                "\tbool need_drop = false;",
                "\tif (need_drop) {",
                "\t\trxrpc_deactivate_bundle(bundle);",
                "\t\trxrpc_put_connection(conn);"
            ],
            "deleted": [
                "\tstruct rxrpc_local *local = bundle->params.local;",
                "\tbool need_drop = false, need_put = false;",
                "\t/* If there are no more connections, remove the bundle */",
                "\tif (!bundle->avail_chans) {",
                "\t\t_debug(\"maybe unbundle\");",
                "\t\tspin_lock(&local->client_bundles_lock);",
                "",
                "\t\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++)",
                "\t\t\tif (bundle->conns[i])",
                "\t\t\t\tbreak;",
                "\t\tif (i == ARRAY_SIZE(bundle->conns) && !bundle->params.exclusive) {",
                "\t\t\t_debug(\"erase bundle\");",
                "\t\t\trb_erase(&bundle->local_node, &local->client_bundles);",
                "\t\t\tneed_put = true;",
                "\t\t}",
                "",
                "\t\tspin_unlock(&local->client_bundles_lock);",
                "\t\tif (need_put)",
                "\t\t\trxrpc_put_bundle(bundle);",
                "",
                "\tif (need_drop)",
                "\t\trxrpc_put_connection(conn);",
                "\t_leave(\"\");"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-2006",
        "code_before_change": "static struct rxrpc_bundle *rxrpc_look_up_bundle(struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t\t gfp_t gfp)\n{\n\tstatic atomic_t rxrpc_bundle_id;\n\tstruct rxrpc_bundle *bundle, *candidate;\n\tstruct rxrpc_local *local = cp->local;\n\tstruct rb_node *p, **pp, *parent;\n\tlong diff;\n\n\t_enter(\"{%px,%x,%u,%u}\",\n\t       cp->peer, key_serial(cp->key), cp->security_level, cp->upgrade);\n\n\tif (cp->exclusive)\n\t\treturn rxrpc_alloc_bundle(cp, gfp);\n\n\t/* First, see if the bundle is already there. */\n\t_debug(\"search 1\");\n\tspin_lock(&local->client_bundles_lock);\n\tp = local->client_bundles.rb_node;\n\twhile (p) {\n\t\tbundle = rb_entry(p, struct rxrpc_bundle, local_node);\n\n#define cmp(X) ((long)bundle->params.X - (long)cp->X)\n\t\tdiff = (cmp(peer) ?:\n\t\t\tcmp(key) ?:\n\t\t\tcmp(security_level) ?:\n\t\t\tcmp(upgrade));\n#undef cmp\n\t\tif (diff < 0)\n\t\t\tp = p->rb_left;\n\t\telse if (diff > 0)\n\t\t\tp = p->rb_right;\n\t\telse\n\t\t\tgoto found_bundle;\n\t}\n\tspin_unlock(&local->client_bundles_lock);\n\t_debug(\"not found\");\n\n\t/* It wasn't.  We need to add one. */\n\tcandidate = rxrpc_alloc_bundle(cp, gfp);\n\tif (!candidate)\n\t\treturn NULL;\n\n\t_debug(\"search 2\");\n\tspin_lock(&local->client_bundles_lock);\n\tpp = &local->client_bundles.rb_node;\n\tparent = NULL;\n\twhile (*pp) {\n\t\tparent = *pp;\n\t\tbundle = rb_entry(parent, struct rxrpc_bundle, local_node);\n\n#define cmp(X) ((long)bundle->params.X - (long)cp->X)\n\t\tdiff = (cmp(peer) ?:\n\t\t\tcmp(key) ?:\n\t\t\tcmp(security_level) ?:\n\t\t\tcmp(upgrade));\n#undef cmp\n\t\tif (diff < 0)\n\t\t\tpp = &(*pp)->rb_left;\n\t\telse if (diff > 0)\n\t\t\tpp = &(*pp)->rb_right;\n\t\telse\n\t\t\tgoto found_bundle_free;\n\t}\n\n\t_debug(\"new bundle\");\n\tcandidate->debug_id = atomic_inc_return(&rxrpc_bundle_id);\n\trb_link_node(&candidate->local_node, parent, pp);\n\trb_insert_color(&candidate->local_node, &local->client_bundles);\n\trxrpc_get_bundle(candidate);\n\tspin_unlock(&local->client_bundles_lock);\n\t_leave(\" = %u [new]\", candidate->debug_id);\n\treturn candidate;\n\nfound_bundle_free:\n\trxrpc_free_bundle(candidate);\nfound_bundle:\n\trxrpc_get_bundle(bundle);\n\tspin_unlock(&local->client_bundles_lock);\n\t_leave(\" = %u [found]\", bundle->debug_id);\n\treturn bundle;\n}",
        "code_after_change": "static struct rxrpc_bundle *rxrpc_look_up_bundle(struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t\t gfp_t gfp)\n{\n\tstatic atomic_t rxrpc_bundle_id;\n\tstruct rxrpc_bundle *bundle, *candidate;\n\tstruct rxrpc_local *local = cp->local;\n\tstruct rb_node *p, **pp, *parent;\n\tlong diff;\n\n\t_enter(\"{%px,%x,%u,%u}\",\n\t       cp->peer, key_serial(cp->key), cp->security_level, cp->upgrade);\n\n\tif (cp->exclusive)\n\t\treturn rxrpc_alloc_bundle(cp, gfp);\n\n\t/* First, see if the bundle is already there. */\n\t_debug(\"search 1\");\n\tspin_lock(&local->client_bundles_lock);\n\tp = local->client_bundles.rb_node;\n\twhile (p) {\n\t\tbundle = rb_entry(p, struct rxrpc_bundle, local_node);\n\n#define cmp(X) ((long)bundle->params.X - (long)cp->X)\n\t\tdiff = (cmp(peer) ?:\n\t\t\tcmp(key) ?:\n\t\t\tcmp(security_level) ?:\n\t\t\tcmp(upgrade));\n#undef cmp\n\t\tif (diff < 0)\n\t\t\tp = p->rb_left;\n\t\telse if (diff > 0)\n\t\t\tp = p->rb_right;\n\t\telse\n\t\t\tgoto found_bundle;\n\t}\n\tspin_unlock(&local->client_bundles_lock);\n\t_debug(\"not found\");\n\n\t/* It wasn't.  We need to add one. */\n\tcandidate = rxrpc_alloc_bundle(cp, gfp);\n\tif (!candidate)\n\t\treturn NULL;\n\n\t_debug(\"search 2\");\n\tspin_lock(&local->client_bundles_lock);\n\tpp = &local->client_bundles.rb_node;\n\tparent = NULL;\n\twhile (*pp) {\n\t\tparent = *pp;\n\t\tbundle = rb_entry(parent, struct rxrpc_bundle, local_node);\n\n#define cmp(X) ((long)bundle->params.X - (long)cp->X)\n\t\tdiff = (cmp(peer) ?:\n\t\t\tcmp(key) ?:\n\t\t\tcmp(security_level) ?:\n\t\t\tcmp(upgrade));\n#undef cmp\n\t\tif (diff < 0)\n\t\t\tpp = &(*pp)->rb_left;\n\t\telse if (diff > 0)\n\t\t\tpp = &(*pp)->rb_right;\n\t\telse\n\t\t\tgoto found_bundle_free;\n\t}\n\n\t_debug(\"new bundle\");\n\tcandidate->debug_id = atomic_inc_return(&rxrpc_bundle_id);\n\trb_link_node(&candidate->local_node, parent, pp);\n\trb_insert_color(&candidate->local_node, &local->client_bundles);\n\trxrpc_get_bundle(candidate);\n\tspin_unlock(&local->client_bundles_lock);\n\t_leave(\" = %u [new]\", candidate->debug_id);\n\treturn candidate;\n\nfound_bundle_free:\n\trxrpc_free_bundle(candidate);\nfound_bundle:\n\trxrpc_get_bundle(bundle);\n\tatomic_inc(&bundle->active);\n\tspin_unlock(&local->client_bundles_lock);\n\t_leave(\" = %u [found]\", bundle->debug_id);\n\treturn bundle;\n}",
        "patch": "--- code before\n+++ code after\n@@ -76,6 +76,7 @@\n \trxrpc_free_bundle(candidate);\n found_bundle:\n \trxrpc_get_bundle(bundle);\n+\tatomic_inc(&bundle->active);\n \tspin_unlock(&local->client_bundles_lock);\n \t_leave(\" = %u [found]\", bundle->debug_id);\n \treturn bundle;",
        "function_modified_lines": {
            "added": [
                "\tatomic_inc(&bundle->active);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-2006",
        "code_before_change": "static void rxrpc_add_conn_to_bundle(struct rxrpc_bundle *bundle, gfp_t gfp)\n\t__releases(bundle->channel_lock)\n{\n\tstruct rxrpc_connection *candidate = NULL, *old = NULL;\n\tbool conflict;\n\tint i;\n\n\t_enter(\"\");\n\n\tconflict = bundle->alloc_conn;\n\tif (!conflict)\n\t\tbundle->alloc_conn = true;\n\tspin_unlock(&bundle->channel_lock);\n\tif (conflict) {\n\t\t_leave(\" [conf]\");\n\t\treturn;\n\t}\n\n\tcandidate = rxrpc_alloc_client_connection(bundle, gfp);\n\n\tspin_lock(&bundle->channel_lock);\n\tbundle->alloc_conn = false;\n\n\tif (IS_ERR(candidate)) {\n\t\tbundle->alloc_error = PTR_ERR(candidate);\n\t\tspin_unlock(&bundle->channel_lock);\n\t\t_leave(\" [err %ld]\", PTR_ERR(candidate));\n\t\treturn;\n\t}\n\n\tbundle->alloc_error = 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++) {\n\t\tunsigned int shift = i * RXRPC_MAXCALLS;\n\t\tint j;\n\n\t\told = bundle->conns[i];\n\t\tif (!rxrpc_may_reuse_conn(old)) {\n\t\t\tif (old)\n\t\t\t\ttrace_rxrpc_client(old, -1, rxrpc_client_replace);\n\t\t\tcandidate->bundle_shift = shift;\n\t\t\tbundle->conns[i] = candidate;\n\t\t\tfor (j = 0; j < RXRPC_MAXCALLS; j++)\n\t\t\t\tset_bit(shift + j, &bundle->avail_chans);\n\t\t\tcandidate = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\told = NULL;\n\t}\n\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (candidate) {\n\t\t_debug(\"discard C=%x\", candidate->debug_id);\n\t\ttrace_rxrpc_client(candidate, -1, rxrpc_client_duplicate);\n\t\trxrpc_put_connection(candidate);\n\t}\n\n\trxrpc_put_connection(old);\n\t_leave(\"\");\n}",
        "code_after_change": "static void rxrpc_add_conn_to_bundle(struct rxrpc_bundle *bundle, gfp_t gfp)\n\t__releases(bundle->channel_lock)\n{\n\tstruct rxrpc_connection *candidate = NULL, *old = NULL;\n\tbool conflict;\n\tint i;\n\n\t_enter(\"\");\n\n\tconflict = bundle->alloc_conn;\n\tif (!conflict)\n\t\tbundle->alloc_conn = true;\n\tspin_unlock(&bundle->channel_lock);\n\tif (conflict) {\n\t\t_leave(\" [conf]\");\n\t\treturn;\n\t}\n\n\tcandidate = rxrpc_alloc_client_connection(bundle, gfp);\n\n\tspin_lock(&bundle->channel_lock);\n\tbundle->alloc_conn = false;\n\n\tif (IS_ERR(candidate)) {\n\t\tbundle->alloc_error = PTR_ERR(candidate);\n\t\tspin_unlock(&bundle->channel_lock);\n\t\t_leave(\" [err %ld]\", PTR_ERR(candidate));\n\t\treturn;\n\t}\n\n\tbundle->alloc_error = 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++) {\n\t\tunsigned int shift = i * RXRPC_MAXCALLS;\n\t\tint j;\n\n\t\told = bundle->conns[i];\n\t\tif (!rxrpc_may_reuse_conn(old)) {\n\t\t\tif (old)\n\t\t\t\ttrace_rxrpc_client(old, -1, rxrpc_client_replace);\n\t\t\tcandidate->bundle_shift = shift;\n\t\t\tatomic_inc(&bundle->active);\n\t\t\tbundle->conns[i] = candidate;\n\t\t\tfor (j = 0; j < RXRPC_MAXCALLS; j++)\n\t\t\t\tset_bit(shift + j, &bundle->avail_chans);\n\t\t\tcandidate = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\told = NULL;\n\t}\n\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (candidate) {\n\t\t_debug(\"discard C=%x\", candidate->debug_id);\n\t\ttrace_rxrpc_client(candidate, -1, rxrpc_client_duplicate);\n\t\trxrpc_put_connection(candidate);\n\t}\n\n\trxrpc_put_connection(old);\n\t_leave(\"\");\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,6 +39,7 @@\n \t\t\tif (old)\n \t\t\t\ttrace_rxrpc_client(old, -1, rxrpc_client_replace);\n \t\t\tcandidate->bundle_shift = shift;\n+\t\t\tatomic_inc(&bundle->active);\n \t\t\tbundle->conns[i] = candidate;\n \t\t\tfor (j = 0; j < RXRPC_MAXCALLS; j++)\n \t\t\t\tset_bit(shift + j, &bundle->avail_chans);",
        "function_modified_lines": {
            "added": [
                "\t\t\tatomic_inc(&bundle->active);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-2006",
        "code_before_change": "void rxrpc_put_bundle(struct rxrpc_bundle *bundle)\n{\n\tunsigned int d = bundle->debug_id;\n\tbool dead;\n\tint r;\n\n\tdead = __refcount_dec_and_test(&bundle->ref, &r);\n\n\t_debug(\"PUT B=%x %d\", d, r);\n\tif (dead)\n\t\trxrpc_free_bundle(bundle);\n}",
        "code_after_change": "void rxrpc_put_bundle(struct rxrpc_bundle *bundle)\n{\n\tunsigned int d = bundle->debug_id;\n\tbool dead;\n\tint r;\n\n\tdead = __refcount_dec_and_test(&bundle->ref, &r);\n\n\t_debug(\"PUT B=%x %d\", d, r - 1);\n\tif (dead)\n\t\trxrpc_free_bundle(bundle);\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \n \tdead = __refcount_dec_and_test(&bundle->ref, &r);\n \n-\t_debug(\"PUT B=%x %d\", d, r);\n+\t_debug(\"PUT B=%x %d\", d, r - 1);\n \tif (dead)\n \t\trxrpc_free_bundle(bundle);\n }",
        "function_modified_lines": {
            "added": [
                "\t_debug(\"PUT B=%x %d\", d, r - 1);"
            ],
            "deleted": [
                "\t_debug(\"PUT B=%x %d\", d, r);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-2898",
        "code_before_change": "static int f2fs_ioc_resize_fs(struct file *filp, unsigned long arg)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(file_inode(filp));\n\t__u64 block_count;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (f2fs_readonly(sbi->sb))\n\t\treturn -EROFS;\n\n\tif (copy_from_user(&block_count, (void __user *)arg,\n\t\t\t   sizeof(block_count)))\n\t\treturn -EFAULT;\n\n\treturn f2fs_resize_fs(sbi, block_count);\n}",
        "code_after_change": "static int f2fs_ioc_resize_fs(struct file *filp, unsigned long arg)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(file_inode(filp));\n\t__u64 block_count;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (f2fs_readonly(sbi->sb))\n\t\treturn -EROFS;\n\n\tif (copy_from_user(&block_count, (void __user *)arg,\n\t\t\t   sizeof(block_count)))\n\t\treturn -EFAULT;\n\n\treturn f2fs_resize_fs(filp, block_count);\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,5 +13,5 @@\n \t\t\t   sizeof(block_count)))\n \t\treturn -EFAULT;\n \n-\treturn f2fs_resize_fs(sbi, block_count);\n+\treturn f2fs_resize_fs(filp, block_count);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn f2fs_resize_fs(filp, block_count);"
            ],
            "deleted": [
                "\treturn f2fs_resize_fs(sbi, block_count);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476"
        ],
        "cve_description": "There is a null-pointer-dereference flaw found in f2fs_write_end_io in fs/f2fs/data.c in the Linux kernel. This flaw allows a local privileged user to cause a denial of service problem."
    },
    {
        "cve_id": "CVE-2023-3108",
        "code_before_change": "int af_alg_make_sg(struct af_alg_sgl *sgl, struct iov_iter *iter, int len)\n{\n\tsize_t off;\n\tssize_t n;\n\tint npages, i;\n\n\tn = iov_iter_get_pages(iter, sgl->pages, len, ALG_MAX_PAGES, &off);\n\tif (n < 0)\n\t\treturn n;\n\n\tnpages = PAGE_ALIGN(off + n);\n\tif (WARN_ON(npages == 0))\n\t\treturn -EINVAL;\n\n\tsg_init_table(sgl->sg, npages);\n\n\tfor (i = 0, len = n; i < npages; i++) {\n\t\tint plen = min_t(int, len, PAGE_SIZE - off);\n\n\t\tsg_set_page(sgl->sg + i, sgl->pages[i], plen, off);\n\n\t\toff = 0;\n\t\tlen -= plen;\n\t}\n\treturn n;\n}",
        "code_after_change": "int af_alg_make_sg(struct af_alg_sgl *sgl, struct iov_iter *iter, int len)\n{\n\tsize_t off;\n\tssize_t n;\n\tint npages, i;\n\n\tn = iov_iter_get_pages(iter, sgl->pages, len, ALG_MAX_PAGES, &off);\n\tif (n < 0)\n\t\treturn n;\n\n\tnpages = (off + n + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tif (WARN_ON(npages == 0))\n\t\treturn -EINVAL;\n\n\tsg_init_table(sgl->sg, npages);\n\n\tfor (i = 0, len = n; i < npages; i++) {\n\t\tint plen = min_t(int, len, PAGE_SIZE - off);\n\n\t\tsg_set_page(sgl->sg + i, sgl->pages[i], plen, off);\n\n\t\toff = 0;\n\t\tlen -= plen;\n\t}\n\treturn n;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \tif (n < 0)\n \t\treturn n;\n \n-\tnpages = PAGE_ALIGN(off + n);\n+\tnpages = (off + n + PAGE_SIZE - 1) >> PAGE_SHIFT;\n \tif (WARN_ON(npages == 0))\n \t\treturn -EINVAL;\n ",
        "function_modified_lines": {
            "added": [
                "\tnpages = (off + n + PAGE_SIZE - 1) >> PAGE_SHIFT;"
            ],
            "deleted": [
                "\tnpages = PAGE_ALIGN(off + n);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the subsequent get_user_pages_fast in the Linux kernel\u2019s interface for symmetric key cipher algorithms in the skcipher_recvmsg of crypto/algif_skcipher.c function. This flaw allows a local user to crash the system."
    },
    {
        "cve_id": "CVE-2023-3108",
        "code_before_change": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\twhile (iov_iter_count(&msg->msg_iter)) {\n\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t       struct skcipher_sg_list, list);\n\t\tsg = sgl->sg;\n\n\t\twhile (!sg->length)\n\t\t\tsg++;\n\n\t\tused = ctx->used;\n\t\tif (!used) {\n\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t}\n\n\t\tused = min_t(unsigned long, used, iov_iter_count(&msg->msg_iter));\n\n\t\tused = af_alg_make_sg(&ctx->rsgl, &msg->msg_iter, used);\n\t\terr = used;\n\t\tif (err < 0)\n\t\t\tgoto unlock;\n\n\t\tif (ctx->more || used < ctx->used)\n\t\t\tused -= used % bs;\n\n\t\terr = -EINVAL;\n\t\tif (!used)\n\t\t\tgoto free;\n\n\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t     ctx->iv);\n\n\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\tif (err)\n\t\t\tgoto unlock;\n\n\t\tcopied += used;\n\t\tskcipher_pull_sgl(sk, used);\n\t\tiov_iter_advance(&msg->msg_iter, used);\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
        "code_after_change": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\twhile (iov_iter_count(&msg->msg_iter)) {\n\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t       struct skcipher_sg_list, list);\n\t\tsg = sgl->sg;\n\n\t\twhile (!sg->length)\n\t\t\tsg++;\n\n\t\tif (!ctx->used) {\n\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t}\n\n\t\tused = min_t(unsigned long, ctx->used, iov_iter_count(&msg->msg_iter));\n\n\t\tused = af_alg_make_sg(&ctx->rsgl, &msg->msg_iter, used);\n\t\terr = used;\n\t\tif (err < 0)\n\t\t\tgoto unlock;\n\n\t\tif (ctx->more || used < ctx->used)\n\t\t\tused -= used % bs;\n\n\t\terr = -EINVAL;\n\t\tif (!used)\n\t\t\tgoto free;\n\n\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t     ctx->iv);\n\n\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\tif (err)\n\t\t\tgoto unlock;\n\n\t\tcopied += used;\n\t\tskcipher_pull_sgl(sk, used);\n\t\tiov_iter_advance(&msg->msg_iter, used);\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,14 +21,13 @@\n \t\twhile (!sg->length)\n \t\t\tsg++;\n \n-\t\tused = ctx->used;\n-\t\tif (!used) {\n+\t\tif (!ctx->used) {\n \t\t\terr = skcipher_wait_for_data(sk, flags);\n \t\t\tif (err)\n \t\t\t\tgoto unlock;\n \t\t}\n \n-\t\tused = min_t(unsigned long, used, iov_iter_count(&msg->msg_iter));\n+\t\tused = min_t(unsigned long, ctx->used, iov_iter_count(&msg->msg_iter));\n \n \t\tused = af_alg_make_sg(&ctx->rsgl, &msg->msg_iter, used);\n \t\terr = used;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!ctx->used) {",
                "\t\tused = min_t(unsigned long, ctx->used, iov_iter_count(&msg->msg_iter));"
            ],
            "deleted": [
                "\t\tused = ctx->used;",
                "\t\tif (!used) {",
                "\t\tused = min_t(unsigned long, used, iov_iter_count(&msg->msg_iter));"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the subsequent get_user_pages_fast in the Linux kernel\u2019s interface for symmetric key cipher algorithms in the skcipher_recvmsg of crypto/algif_skcipher.c function. This flaw allows a local user to crash the system."
    },
    {
        "cve_id": "CVE-2023-31083",
        "code_before_change": "static int hci_uart_tty_ioctl(struct tty_struct *tty, unsigned int cmd,\n\t\t\t      unsigned long arg)\n{\n\tstruct hci_uart *hu = tty->disc_data;\n\tint err = 0;\n\n\tBT_DBG(\"\");\n\n\t/* Verify the status of the device */\n\tif (!hu)\n\t\treturn -EBADF;\n\n\tswitch (cmd) {\n\tcase HCIUARTSETPROTO:\n\t\tif (!test_and_set_bit(HCI_UART_PROTO_SET, &hu->flags)) {\n\t\t\terr = hci_uart_set_proto(hu, arg);\n\t\t\tif (err)\n\t\t\t\tclear_bit(HCI_UART_PROTO_SET, &hu->flags);\n\t\t} else\n\t\t\terr = -EBUSY;\n\t\tbreak;\n\n\tcase HCIUARTGETPROTO:\n\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags))\n\t\t\terr = hu->proto->id;\n\t\telse\n\t\t\terr = -EUNATCH;\n\t\tbreak;\n\n\tcase HCIUARTGETDEVICE:\n\t\tif (test_bit(HCI_UART_REGISTERED, &hu->flags))\n\t\t\terr = hu->hdev->id;\n\t\telse\n\t\t\terr = -EUNATCH;\n\t\tbreak;\n\n\tcase HCIUARTSETFLAGS:\n\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\terr = hci_uart_set_flags(hu, arg);\n\t\tbreak;\n\n\tcase HCIUARTGETFLAGS:\n\t\terr = hu->hdev_flags;\n\t\tbreak;\n\n\tdefault:\n\t\terr = n_tty_ioctl_helper(tty, cmd, arg);\n\t\tbreak;\n\t}\n\n\treturn err;\n}",
        "code_after_change": "static int hci_uart_tty_ioctl(struct tty_struct *tty, unsigned int cmd,\n\t\t\t      unsigned long arg)\n{\n\tstruct hci_uart *hu = tty->disc_data;\n\tint err = 0;\n\n\tBT_DBG(\"\");\n\n\t/* Verify the status of the device */\n\tif (!hu)\n\t\treturn -EBADF;\n\n\tswitch (cmd) {\n\tcase HCIUARTSETPROTO:\n\t\tif (!test_and_set_bit(HCI_UART_PROTO_SET, &hu->flags)) {\n\t\t\terr = hci_uart_set_proto(hu, arg);\n\t\t\tif (err)\n\t\t\t\tclear_bit(HCI_UART_PROTO_SET, &hu->flags);\n\t\t} else\n\t\t\terr = -EBUSY;\n\t\tbreak;\n\n\tcase HCIUARTGETPROTO:\n\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags) &&\n\t\t    test_bit(HCI_UART_PROTO_READY, &hu->flags))\n\t\t\terr = hu->proto->id;\n\t\telse\n\t\t\terr = -EUNATCH;\n\t\tbreak;\n\n\tcase HCIUARTGETDEVICE:\n\t\tif (test_bit(HCI_UART_REGISTERED, &hu->flags))\n\t\t\terr = hu->hdev->id;\n\t\telse\n\t\t\terr = -EUNATCH;\n\t\tbreak;\n\n\tcase HCIUARTSETFLAGS:\n\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\terr = hci_uart_set_flags(hu, arg);\n\t\tbreak;\n\n\tcase HCIUARTGETFLAGS:\n\t\terr = hu->hdev_flags;\n\t\tbreak;\n\n\tdefault:\n\t\terr = n_tty_ioctl_helper(tty, cmd, arg);\n\t\tbreak;\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,7 +21,8 @@\n \t\tbreak;\n \n \tcase HCIUARTGETPROTO:\n-\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags))\n+\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags) &&\n+\t\t    test_bit(HCI_UART_PROTO_READY, &hu->flags))\n \t\t\terr = hu->proto->id;\n \t\telse\n \t\t\terr = -EUNATCH;",
        "function_modified_lines": {
            "added": [
                "\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags) &&",
                "\t\t    test_bit(HCI_UART_PROTO_READY, &hu->flags))"
            ],
            "deleted": [
                "\t\tif (test_bit(HCI_UART_PROTO_SET, &hu->flags))"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in drivers/bluetooth/hci_ldisc.c in the Linux kernel 6.2. In hci_uart_tty_ioctl, there is a race condition between HCIUARTSETPROTO and HCIUARTGETPROTO. HCI_UART_PROTO_SET is set before hu->proto is set. A NULL pointer dereference may occur."
    },
    {
        "cve_id": "CVE-2023-32250",
        "code_before_change": "static void ksmbd_conn_unlock(struct ksmbd_conn *conn)\n{\n\tmutex_unlock(&conn->srv_mutex);\n}",
        "code_after_change": "void ksmbd_conn_unlock(struct ksmbd_conn *conn)\n{\n\tmutex_unlock(&conn->srv_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-static void ksmbd_conn_unlock(struct ksmbd_conn *conn)\n+void ksmbd_conn_unlock(struct ksmbd_conn *conn)\n {\n \tmutex_unlock(&conn->srv_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "void ksmbd_conn_unlock(struct ksmbd_conn *conn)"
            ],
            "deleted": [
                "static void ksmbd_conn_unlock(struct ksmbd_conn *conn)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the processing of SMB2_SESSION_SETUP commands. The issue results from the lack of proper locking when performing operations on an object. An attacker can leverage this vulnerability to execute code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-32250",
        "code_before_change": "static void ksmbd_conn_lock(struct ksmbd_conn *conn)\n{\n\tmutex_lock(&conn->srv_mutex);\n}",
        "code_after_change": "void ksmbd_conn_lock(struct ksmbd_conn *conn)\n{\n\tmutex_lock(&conn->srv_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-static void ksmbd_conn_lock(struct ksmbd_conn *conn)\n+void ksmbd_conn_lock(struct ksmbd_conn *conn)\n {\n \tmutex_lock(&conn->srv_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "void ksmbd_conn_lock(struct ksmbd_conn *conn)"
            ],
            "deleted": [
                "static void ksmbd_conn_lock(struct ksmbd_conn *conn)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the processing of SMB2_SESSION_SETUP commands. The issue results from the lack of proper locking when performing operations on an object. An attacker can leverage this vulnerability to execute code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-32250",
        "code_before_change": "bool ksmbd_conn_alive(struct ksmbd_conn *conn)\n{\n\tif (!ksmbd_server_running())\n\t\treturn false;\n\n\tif (conn->status == KSMBD_SESS_EXITING)\n\t\treturn false;\n\n\tif (kthread_should_stop())\n\t\treturn false;\n\n\tif (atomic_read(&conn->stats.open_files_count) > 0)\n\t\treturn true;\n\n\t/*\n\t * Stop current session if the time that get last request from client\n\t * is bigger than deadtime user configured and opening file count is\n\t * zero.\n\t */\n\tif (server_conf.deadtime > 0 &&\n\t    time_after(jiffies, conn->last_active + server_conf.deadtime)) {\n\t\tksmbd_debug(CONN, \"No response from client in %lu minutes\\n\",\n\t\t\t    server_conf.deadtime / SMB_ECHO_INTERVAL);\n\t\treturn false;\n\t}\n\treturn true;\n}",
        "code_after_change": "bool ksmbd_conn_alive(struct ksmbd_conn *conn)\n{\n\tif (!ksmbd_server_running())\n\t\treturn false;\n\n\tif (ksmbd_conn_exiting(conn))\n\t\treturn false;\n\n\tif (kthread_should_stop())\n\t\treturn false;\n\n\tif (atomic_read(&conn->stats.open_files_count) > 0)\n\t\treturn true;\n\n\t/*\n\t * Stop current session if the time that get last request from client\n\t * is bigger than deadtime user configured and opening file count is\n\t * zero.\n\t */\n\tif (server_conf.deadtime > 0 &&\n\t    time_after(jiffies, conn->last_active + server_conf.deadtime)) {\n\t\tksmbd_debug(CONN, \"No response from client in %lu minutes\\n\",\n\t\t\t    server_conf.deadtime / SMB_ECHO_INTERVAL);\n\t\treturn false;\n\t}\n\treturn true;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \tif (!ksmbd_server_running())\n \t\treturn false;\n \n-\tif (conn->status == KSMBD_SESS_EXITING)\n+\tif (ksmbd_conn_exiting(conn))\n \t\treturn false;\n \n \tif (kthread_should_stop())",
        "function_modified_lines": {
            "added": [
                "\tif (ksmbd_conn_exiting(conn))"
            ],
            "deleted": [
                "\tif (conn->status == KSMBD_SESS_EXITING)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the processing of SMB2_SESSION_SETUP commands. The issue results from the lack of proper locking when performing operations on an object. An attacker can leverage this vulnerability to execute code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-32250",
        "code_before_change": "static inline int check_conn_state(struct ksmbd_work *work)\n{\n\tstruct smb_hdr *rsp_hdr;\n\n\tif (ksmbd_conn_exiting(work) || ksmbd_conn_need_reconnect(work)) {\n\t\trsp_hdr = work->response_buf;\n\t\trsp_hdr->Status.CifsError = STATUS_CONNECTION_DISCONNECTED;\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static inline int check_conn_state(struct ksmbd_work *work)\n{\n\tstruct smb_hdr *rsp_hdr;\n\n\tif (ksmbd_conn_exiting(work->conn) ||\n\t    ksmbd_conn_need_reconnect(work->conn)) {\n\t\trsp_hdr = work->response_buf;\n\t\trsp_hdr->Status.CifsError = STATUS_CONNECTION_DISCONNECTED;\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,8 @@\n {\n \tstruct smb_hdr *rsp_hdr;\n \n-\tif (ksmbd_conn_exiting(work) || ksmbd_conn_need_reconnect(work)) {\n+\tif (ksmbd_conn_exiting(work->conn) ||\n+\t    ksmbd_conn_need_reconnect(work->conn)) {\n \t\trsp_hdr = work->response_buf;\n \t\trsp_hdr->Status.CifsError = STATUS_CONNECTION_DISCONNECTED;\n \t\treturn 1;",
        "function_modified_lines": {
            "added": [
                "\tif (ksmbd_conn_exiting(work->conn) ||",
                "\t    ksmbd_conn_need_reconnect(work->conn)) {"
            ],
            "deleted": [
                "\tif (ksmbd_conn_exiting(work) || ksmbd_conn_need_reconnect(work)) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the processing of SMB2_SESSION_SETUP commands. The issue results from the lack of proper locking when performing operations on an object. An attacker can leverage this vulnerability to execute code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-32250",
        "code_before_change": "int smb2_handle_negotiate(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct smb2_negotiate_req *req = smb2_get_msg(work->request_buf);\n\tstruct smb2_negotiate_rsp *rsp = smb2_get_msg(work->response_buf);\n\tint rc = 0;\n\tunsigned int smb2_buf_len, smb2_neg_size;\n\t__le32 status;\n\n\tksmbd_debug(SMB, \"Received negotiate request\\n\");\n\tconn->need_neg = false;\n\tif (ksmbd_conn_good(work)) {\n\t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n\t\twork->send_no_response = 1;\n\t\treturn rc;\n\t}\n\n\tif (req->DialectCount == 0) {\n\t\tpr_err(\"malformed packet\\n\");\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tsmb2_buf_len = get_rfc1002_len(work->request_buf);\n\tsmb2_neg_size = offsetof(struct smb2_negotiate_req, Dialects);\n\tif (smb2_neg_size > smb2_buf_len) {\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (conn->dialect == SMB311_PROT_ID) {\n\t\tunsigned int nego_ctxt_off = le32_to_cpu(req->NegotiateContextOffset);\n\n\t\tif (smb2_buf_len < nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size > nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t} else {\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    smb2_buf_len) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tconn->cli_cap = le32_to_cpu(req->Capabilities);\n\tswitch (conn->dialect) {\n\tcase SMB311_PROT_ID:\n\t\tconn->preauth_info =\n\t\t\tkzalloc(sizeof(struct preauth_integrity_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conn->preauth_info) {\n\t\t\trc = -ENOMEM;\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tstatus = deassemble_neg_contexts(conn, req,\n\t\t\t\t\t\t get_rfc1002_len(work->request_buf));\n\t\tif (status != STATUS_SUCCESS) {\n\t\t\tpr_err(\"deassemble_neg_contexts error(0x%x)\\n\",\n\t\t\t       status);\n\t\t\trsp->hdr.Status = status;\n\t\t\trc = -EINVAL;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\trc = init_smb3_11_server(conn);\n\t\tif (rc < 0) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tksmbd_gen_preauth_integrity_hash(conn,\n\t\t\t\t\t\t work->request_buf,\n\t\t\t\t\t\t conn->preauth_info->Preauth_HashValue);\n\t\trsp->NegotiateContextOffset =\n\t\t\t\tcpu_to_le32(OFFSET_OF_NEG_CONTEXT);\n\t\tassemble_neg_contexts(conn, rsp, work->response_buf);\n\t\tbreak;\n\tcase SMB302_PROT_ID:\n\t\tinit_smb3_02_server(conn);\n\t\tbreak;\n\tcase SMB30_PROT_ID:\n\t\tinit_smb3_0_server(conn);\n\t\tbreak;\n\tcase SMB21_PROT_ID:\n\t\tinit_smb2_1_server(conn);\n\t\tbreak;\n\tcase SMB2X_PROT_ID:\n\tcase BAD_PROT_ID:\n\tdefault:\n\t\tksmbd_debug(SMB, \"Server dialect :0x%x not supported\\n\",\n\t\t\t    conn->dialect);\n\t\trsp->hdr.Status = STATUS_NOT_SUPPORTED;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\n\t/* For stats */\n\tconn->connection_type = conn->dialect;\n\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\tmemcpy(conn->ClientGUID, req->ClientGUID,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\tconn->cli_sec_mode = le16_to_cpu(req->SecurityMode);\n\n\trsp->StructureSize = cpu_to_le16(65);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying server\n\t */\n\tmemset(rsp->ServerGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\tksmbd_debug(SMB, \"negotiate context offset %d, count %d\\n\",\n\t\t    le32_to_cpu(rsp->NegotiateContextOffset),\n\t\t    le16_to_cpu(rsp->NegotiateContextCount));\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\t\t\t  le16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf, sizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tconn->use_spnego = true;\n\n\tif ((server_conf.signing == KSMBD_CONFIG_OPT_AUTO ||\n\t     server_conf.signing == KSMBD_CONFIG_OPT_DISABLED) &&\n\t    req->SecurityMode & SMB2_NEGOTIATE_SIGNING_REQUIRED_LE)\n\t\tconn->sign = true;\n\telse if (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY) {\n\t\tserver_conf.enforced_signing = true;\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\t\tconn->sign = true;\n\t}\n\n\tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tksmbd_conn_set_need_negotiate(work);\n\nerr_out:\n\tif (rc < 0)\n\t\tsmb2_set_err_rsp(work);\n\n\treturn rc;\n}",
        "code_after_change": "int smb2_handle_negotiate(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct smb2_negotiate_req *req = smb2_get_msg(work->request_buf);\n\tstruct smb2_negotiate_rsp *rsp = smb2_get_msg(work->response_buf);\n\tint rc = 0;\n\tunsigned int smb2_buf_len, smb2_neg_size;\n\t__le32 status;\n\n\tksmbd_debug(SMB, \"Received negotiate request\\n\");\n\tconn->need_neg = false;\n\tif (ksmbd_conn_good(conn)) {\n\t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n\t\twork->send_no_response = 1;\n\t\treturn rc;\n\t}\n\n\tif (req->DialectCount == 0) {\n\t\tpr_err(\"malformed packet\\n\");\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tsmb2_buf_len = get_rfc1002_len(work->request_buf);\n\tsmb2_neg_size = offsetof(struct smb2_negotiate_req, Dialects);\n\tif (smb2_neg_size > smb2_buf_len) {\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (conn->dialect == SMB311_PROT_ID) {\n\t\tunsigned int nego_ctxt_off = le32_to_cpu(req->NegotiateContextOffset);\n\n\t\tif (smb2_buf_len < nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size > nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t} else {\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    smb2_buf_len) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tconn->cli_cap = le32_to_cpu(req->Capabilities);\n\tswitch (conn->dialect) {\n\tcase SMB311_PROT_ID:\n\t\tconn->preauth_info =\n\t\t\tkzalloc(sizeof(struct preauth_integrity_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conn->preauth_info) {\n\t\t\trc = -ENOMEM;\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tstatus = deassemble_neg_contexts(conn, req,\n\t\t\t\t\t\t get_rfc1002_len(work->request_buf));\n\t\tif (status != STATUS_SUCCESS) {\n\t\t\tpr_err(\"deassemble_neg_contexts error(0x%x)\\n\",\n\t\t\t       status);\n\t\t\trsp->hdr.Status = status;\n\t\t\trc = -EINVAL;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\trc = init_smb3_11_server(conn);\n\t\tif (rc < 0) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tksmbd_gen_preauth_integrity_hash(conn,\n\t\t\t\t\t\t work->request_buf,\n\t\t\t\t\t\t conn->preauth_info->Preauth_HashValue);\n\t\trsp->NegotiateContextOffset =\n\t\t\t\tcpu_to_le32(OFFSET_OF_NEG_CONTEXT);\n\t\tassemble_neg_contexts(conn, rsp, work->response_buf);\n\t\tbreak;\n\tcase SMB302_PROT_ID:\n\t\tinit_smb3_02_server(conn);\n\t\tbreak;\n\tcase SMB30_PROT_ID:\n\t\tinit_smb3_0_server(conn);\n\t\tbreak;\n\tcase SMB21_PROT_ID:\n\t\tinit_smb2_1_server(conn);\n\t\tbreak;\n\tcase SMB2X_PROT_ID:\n\tcase BAD_PROT_ID:\n\tdefault:\n\t\tksmbd_debug(SMB, \"Server dialect :0x%x not supported\\n\",\n\t\t\t    conn->dialect);\n\t\trsp->hdr.Status = STATUS_NOT_SUPPORTED;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\n\t/* For stats */\n\tconn->connection_type = conn->dialect;\n\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\tmemcpy(conn->ClientGUID, req->ClientGUID,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\tconn->cli_sec_mode = le16_to_cpu(req->SecurityMode);\n\n\trsp->StructureSize = cpu_to_le16(65);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying server\n\t */\n\tmemset(rsp->ServerGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\tksmbd_debug(SMB, \"negotiate context offset %d, count %d\\n\",\n\t\t    le32_to_cpu(rsp->NegotiateContextOffset),\n\t\t    le16_to_cpu(rsp->NegotiateContextCount));\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\t\t\t  le16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf, sizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tconn->use_spnego = true;\n\n\tif ((server_conf.signing == KSMBD_CONFIG_OPT_AUTO ||\n\t     server_conf.signing == KSMBD_CONFIG_OPT_DISABLED) &&\n\t    req->SecurityMode & SMB2_NEGOTIATE_SIGNING_REQUIRED_LE)\n\t\tconn->sign = true;\n\telse if (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY) {\n\t\tserver_conf.enforced_signing = true;\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\t\tconn->sign = true;\n\t}\n\n\tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tksmbd_conn_set_need_negotiate(conn);\n\nerr_out:\n\tif (rc < 0)\n\t\tsmb2_set_err_rsp(work);\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \n \tksmbd_debug(SMB, \"Received negotiate request\\n\");\n \tconn->need_neg = false;\n-\tif (ksmbd_conn_good(work)) {\n+\tif (ksmbd_conn_good(conn)) {\n \t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n \t\twork->send_no_response = 1;\n \t\treturn rc;\n@@ -163,7 +163,7 @@\n \t}\n \n \tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n-\tksmbd_conn_set_need_negotiate(work);\n+\tksmbd_conn_set_need_negotiate(conn);\n \n err_out:\n \tif (rc < 0)",
        "function_modified_lines": {
            "added": [
                "\tif (ksmbd_conn_good(conn)) {",
                "\tksmbd_conn_set_need_negotiate(conn);"
            ],
            "deleted": [
                "\tif (ksmbd_conn_good(work)) {",
                "\tksmbd_conn_set_need_negotiate(work);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the processing of SMB2_SESSION_SETUP commands. The issue results from the lack of proper locking when performing operations on an object. An attacker can leverage this vulnerability to execute code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-32250",
        "code_before_change": "static void destroy_previous_session(struct ksmbd_conn *conn,\n\t\t\t\t     struct ksmbd_user *user, u64 id)\n{\n\tstruct ksmbd_session *prev_sess = ksmbd_session_lookup_slowpath(id);\n\tstruct ksmbd_user *prev_user;\n\tstruct channel *chann;\n\tlong index;\n\n\tif (!prev_sess)\n\t\treturn;\n\n\tprev_user = prev_sess->user;\n\n\tif (!prev_user ||\n\t    strcmp(user->name, prev_user->name) ||\n\t    user->passkey_sz != prev_user->passkey_sz ||\n\t    memcmp(user->passkey, prev_user->passkey, user->passkey_sz))\n\t\treturn;\n\n\tprev_sess->state = SMB2_SESSION_EXPIRED;\n\txa_for_each(&prev_sess->ksmbd_chann_list, index, chann)\n\t\tchann->conn->status = KSMBD_SESS_EXITING;\n}",
        "code_after_change": "static void destroy_previous_session(struct ksmbd_conn *conn,\n\t\t\t\t     struct ksmbd_user *user, u64 id)\n{\n\tstruct ksmbd_session *prev_sess = ksmbd_session_lookup_slowpath(id);\n\tstruct ksmbd_user *prev_user;\n\tstruct channel *chann;\n\tlong index;\n\n\tif (!prev_sess)\n\t\treturn;\n\n\tprev_user = prev_sess->user;\n\n\tif (!prev_user ||\n\t    strcmp(user->name, prev_user->name) ||\n\t    user->passkey_sz != prev_user->passkey_sz ||\n\t    memcmp(user->passkey, prev_user->passkey, user->passkey_sz))\n\t\treturn;\n\n\tprev_sess->state = SMB2_SESSION_EXPIRED;\n\txa_for_each(&prev_sess->ksmbd_chann_list, index, chann)\n\t\tksmbd_conn_set_exiting(chann->conn);\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,5 +19,5 @@\n \n \tprev_sess->state = SMB2_SESSION_EXPIRED;\n \txa_for_each(&prev_sess->ksmbd_chann_list, index, chann)\n-\t\tchann->conn->status = KSMBD_SESS_EXITING;\n+\t\tksmbd_conn_set_exiting(chann->conn);\n }",
        "function_modified_lines": {
            "added": [
                "\t\tksmbd_conn_set_exiting(chann->conn);"
            ],
            "deleted": [
                "\t\tchann->conn->status = KSMBD_SESS_EXITING;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the processing of SMB2_SESSION_SETUP commands. The issue results from the lack of proper locking when performing operations on an object. An attacker can leverage this vulnerability to execute code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-32250",
        "code_before_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(work));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(work);\n\treturn 0;\n}",
        "code_after_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(conn));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(conn);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,7 +23,7 @@\n \n \trsp = smb2_get_msg(work->response_buf);\n \n-\tWARN_ON(ksmbd_conn_good(work));\n+\tWARN_ON(ksmbd_conn_good(conn));\n \n \trsp->StructureSize = cpu_to_le16(65);\n \tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n@@ -52,6 +52,6 @@\n \t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n \tconn->use_spnego = true;\n \n-\tksmbd_conn_set_need_negotiate(work);\n+\tksmbd_conn_set_need_negotiate(conn);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tWARN_ON(ksmbd_conn_good(conn));",
                "\tksmbd_conn_set_need_negotiate(conn);"
            ],
            "deleted": [
                "\tWARN_ON(ksmbd_conn_good(work));",
                "\tksmbd_conn_set_need_negotiate(work);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the processing of SMB2_SESSION_SETUP commands. The issue results from the lack of proper locking when performing operations on an object. An attacker can leverage this vulnerability to execute code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-32250",
        "code_before_change": "static int ksmbd_tcp_readv(struct tcp_transport *t, struct kvec *iov_orig,\n\t\t\t   unsigned int nr_segs, unsigned int to_read,\n\t\t\t   int max_retries)\n{\n\tint length = 0;\n\tint total_read;\n\tunsigned int segs;\n\tstruct msghdr ksmbd_msg;\n\tstruct kvec *iov;\n\tstruct ksmbd_conn *conn = KSMBD_TRANS(t)->conn;\n\n\tiov = get_conn_iovec(t, nr_segs);\n\tif (!iov)\n\t\treturn -ENOMEM;\n\n\tksmbd_msg.msg_control = NULL;\n\tksmbd_msg.msg_controllen = 0;\n\n\tfor (total_read = 0; to_read; total_read += length, to_read -= length) {\n\t\ttry_to_freeze();\n\n\t\tif (!ksmbd_conn_alive(conn)) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t}\n\t\tsegs = kvec_array_init(iov, iov_orig, nr_segs, total_read);\n\n\t\tlength = kernel_recvmsg(t->sock, &ksmbd_msg,\n\t\t\t\t\tiov, segs, to_read, 0);\n\n\t\tif (length == -EINTR) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t} else if (conn->status == KSMBD_SESS_NEED_RECONNECT) {\n\t\t\ttotal_read = -EAGAIN;\n\t\t\tbreak;\n\t\t} else if (length == -ERESTARTSYS || length == -EAGAIN) {\n\t\t\t/*\n\t\t\t * If max_retries is negative, Allow unlimited\n\t\t\t * retries to keep connection with inactive sessions.\n\t\t\t */\n\t\t\tif (max_retries == 0) {\n\t\t\t\ttotal_read = length;\n\t\t\t\tbreak;\n\t\t\t} else if (max_retries > 0) {\n\t\t\t\tmax_retries--;\n\t\t\t}\n\n\t\t\tusleep_range(1000, 2000);\n\t\t\tlength = 0;\n\t\t\tcontinue;\n\t\t} else if (length <= 0) {\n\t\t\ttotal_read = length;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn total_read;\n}",
        "code_after_change": "static int ksmbd_tcp_readv(struct tcp_transport *t, struct kvec *iov_orig,\n\t\t\t   unsigned int nr_segs, unsigned int to_read,\n\t\t\t   int max_retries)\n{\n\tint length = 0;\n\tint total_read;\n\tunsigned int segs;\n\tstruct msghdr ksmbd_msg;\n\tstruct kvec *iov;\n\tstruct ksmbd_conn *conn = KSMBD_TRANS(t)->conn;\n\n\tiov = get_conn_iovec(t, nr_segs);\n\tif (!iov)\n\t\treturn -ENOMEM;\n\n\tksmbd_msg.msg_control = NULL;\n\tksmbd_msg.msg_controllen = 0;\n\n\tfor (total_read = 0; to_read; total_read += length, to_read -= length) {\n\t\ttry_to_freeze();\n\n\t\tif (!ksmbd_conn_alive(conn)) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t}\n\t\tsegs = kvec_array_init(iov, iov_orig, nr_segs, total_read);\n\n\t\tlength = kernel_recvmsg(t->sock, &ksmbd_msg,\n\t\t\t\t\tiov, segs, to_read, 0);\n\n\t\tif (length == -EINTR) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t} else if (ksmbd_conn_need_reconnect(conn)) {\n\t\t\ttotal_read = -EAGAIN;\n\t\t\tbreak;\n\t\t} else if (length == -ERESTARTSYS || length == -EAGAIN) {\n\t\t\t/*\n\t\t\t * If max_retries is negative, Allow unlimited\n\t\t\t * retries to keep connection with inactive sessions.\n\t\t\t */\n\t\t\tif (max_retries == 0) {\n\t\t\t\ttotal_read = length;\n\t\t\t\tbreak;\n\t\t\t} else if (max_retries > 0) {\n\t\t\t\tmax_retries--;\n\t\t\t}\n\n\t\t\tusleep_range(1000, 2000);\n\t\t\tlength = 0;\n\t\t\tcontinue;\n\t\t} else if (length <= 0) {\n\t\t\ttotal_read = length;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn total_read;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,7 +31,7 @@\n \t\tif (length == -EINTR) {\n \t\t\ttotal_read = -ESHUTDOWN;\n \t\t\tbreak;\n-\t\t} else if (conn->status == KSMBD_SESS_NEED_RECONNECT) {\n+\t\t} else if (ksmbd_conn_need_reconnect(conn)) {\n \t\t\ttotal_read = -EAGAIN;\n \t\t\tbreak;\n \t\t} else if (length == -ERESTARTSYS || length == -EAGAIN) {",
        "function_modified_lines": {
            "added": [
                "\t\t} else if (ksmbd_conn_need_reconnect(conn)) {"
            ],
            "deleted": [
                "\t\t} else if (conn->status == KSMBD_SESS_NEED_RECONNECT) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the processing of SMB2_SESSION_SETUP commands. The issue results from the lack of proper locking when performing operations on an object. An attacker can leverage this vulnerability to execute code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-32254",
        "code_before_change": "struct ksmbd_tree_connect *ksmbd_tree_conn_lookup(struct ksmbd_session *sess,\n\t\t\t\t\t\t  unsigned int id)\n{\n\treturn xa_load(&sess->tree_conns, id);\n}",
        "code_after_change": "struct ksmbd_tree_connect *ksmbd_tree_conn_lookup(struct ksmbd_session *sess,\n\t\t\t\t\t\t  unsigned int id)\n{\n\tstruct ksmbd_tree_connect *tcon;\n\n\ttcon = xa_load(&sess->tree_conns, id);\n\tif (tcon) {\n\t\tif (test_bit(TREE_CONN_EXPIRE, &tcon->status))\n\t\t\ttcon = NULL;\n\t}\n\n\treturn tcon;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,13 @@\n struct ksmbd_tree_connect *ksmbd_tree_conn_lookup(struct ksmbd_session *sess,\n \t\t\t\t\t\t  unsigned int id)\n {\n-\treturn xa_load(&sess->tree_conns, id);\n+\tstruct ksmbd_tree_connect *tcon;\n+\n+\ttcon = xa_load(&sess->tree_conns, id);\n+\tif (tcon) {\n+\t\tif (test_bit(TREE_CONN_EXPIRE, &tcon->status))\n+\t\t\ttcon = NULL;\n+\t}\n+\n+\treturn tcon;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct ksmbd_tree_connect *tcon;",
                "",
                "\ttcon = xa_load(&sess->tree_conns, id);",
                "\tif (tcon) {",
                "\t\tif (test_bit(TREE_CONN_EXPIRE, &tcon->status))",
                "\t\t\ttcon = NULL;",
                "\t}",
                "",
                "\treturn tcon;"
            ],
            "deleted": [
                "\treturn xa_load(&sess->tree_conns, id);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the processing of SMB2_TREE_DISCONNECT commands. The issue results from the lack of proper locking when performing operations on an object. An attacker can leverage this vulnerability to execute code in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-33203",
        "code_before_change": "static int emac_remove(struct platform_device *pdev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(&pdev->dev);\n\tstruct emac_adapter *adpt = netdev_priv(netdev);\n\n\tunregister_netdev(netdev);\n\tnetif_napi_del(&adpt->rx_q.napi);\n\n\temac_clks_teardown(adpt);\n\n\tput_device(&adpt->phydev->mdio.dev);\n\tmdiobus_unregister(adpt->mii_bus);\n\n\tif (adpt->phy.digital)\n\t\tiounmap(adpt->phy.digital);\n\tiounmap(adpt->phy.base);\n\n\tfree_netdev(netdev);\n\n\treturn 0;\n}",
        "code_after_change": "static int emac_remove(struct platform_device *pdev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(&pdev->dev);\n\tstruct emac_adapter *adpt = netdev_priv(netdev);\n\n\tnetif_carrier_off(netdev);\n\tnetif_tx_disable(netdev);\n\n\tunregister_netdev(netdev);\n\tnetif_napi_del(&adpt->rx_q.napi);\n\n\tfree_irq(adpt->irq.irq, &adpt->irq);\n\tcancel_work_sync(&adpt->work_thread);\n\n\temac_clks_teardown(adpt);\n\n\tput_device(&adpt->phydev->mdio.dev);\n\tmdiobus_unregister(adpt->mii_bus);\n\n\tif (adpt->phy.digital)\n\t\tiounmap(adpt->phy.digital);\n\tiounmap(adpt->phy.base);\n\n\tfree_netdev(netdev);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,8 +3,14 @@\n \tstruct net_device *netdev = dev_get_drvdata(&pdev->dev);\n \tstruct emac_adapter *adpt = netdev_priv(netdev);\n \n+\tnetif_carrier_off(netdev);\n+\tnetif_tx_disable(netdev);\n+\n \tunregister_netdev(netdev);\n \tnetif_napi_del(&adpt->rx_q.napi);\n+\n+\tfree_irq(adpt->irq.irq, &adpt->irq);\n+\tcancel_work_sync(&adpt->work_thread);\n \n \temac_clks_teardown(adpt);\n ",
        "function_modified_lines": {
            "added": [
                "\tnetif_carrier_off(netdev);",
                "\tnetif_tx_disable(netdev);",
                "",
                "",
                "\tfree_irq(adpt->irq.irq, &adpt->irq);",
                "\tcancel_work_sync(&adpt->work_thread);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The Linux kernel before 6.2.9 has a race condition and resultant use-after-free in drivers/net/ethernet/qualcomm/emac/emac.c if a physically proximate attacker unplugs an emac based device."
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "int vmw_dumb_create(struct drm_file *file_priv,\n\t\t    struct drm_device *dev,\n\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_bo *vbo;\n\tint cpp = DIV_ROUND_UP(args->bpp, 8);\n\tint ret;\n\n\tswitch (cpp) {\n\tcase 1: /* DRM_FORMAT_C8 */\n\tcase 2: /* DRM_FORMAT_RGB565 */\n\tcase 4: /* DRM_FORMAT_XRGB8888 */\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * Dumb buffers don't allow anything else.\n\t\t * This is tested via IGT's dumb_buffers\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\targs->pitch = args->width * cpp;\n\targs->size = ALIGN(args->pitch * args->height, PAGE_SIZE);\n\n\tret = vmw_gem_object_create_with_handle(dev_priv, file_priv,\n\t\t\t\t\t\targs->size, &args->handle,\n\t\t\t\t\t\t&vbo);\n\n\treturn ret;\n}",
        "code_after_change": "int vmw_dumb_create(struct drm_file *file_priv,\n\t\t    struct drm_device *dev,\n\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_bo *vbo;\n\tint cpp = DIV_ROUND_UP(args->bpp, 8);\n\tint ret;\n\n\tswitch (cpp) {\n\tcase 1: /* DRM_FORMAT_C8 */\n\tcase 2: /* DRM_FORMAT_RGB565 */\n\tcase 4: /* DRM_FORMAT_XRGB8888 */\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * Dumb buffers don't allow anything else.\n\t\t * This is tested via IGT's dumb_buffers\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\targs->pitch = args->width * cpp;\n\targs->size = ALIGN(args->pitch * args->height, PAGE_SIZE);\n\n\tret = vmw_gem_object_create_with_handle(dev_priv, file_priv,\n\t\t\t\t\t\targs->size, &args->handle,\n\t\t\t\t\t\t&vbo);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(&vbo->tbo.base);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,6 +26,7 @@\n \tret = vmw_gem_object_create_with_handle(dev_priv, file_priv,\n \t\t\t\t\t\targs->size, &args->handle,\n \t\t\t\t\t\t&vbo);\n-\n+\t/* drop reference from allocate - handle holds it now */\n+\tdrm_gem_object_put(&vbo->tbo.base);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t/* drop reference from allocate - handle holds it now */",
                "\tdrm_gem_object_put(&vbo->tbo.base);"
            ],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "int vmw_user_bo_synccpu_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *file_priv)\n{\n\tstruct drm_vmw_synccpu_arg *arg =\n\t\t(struct drm_vmw_synccpu_arg *) data;\n\tstruct vmw_bo *vbo;\n\tint ret;\n\n\tif ((arg->flags & (drm_vmw_synccpu_read | drm_vmw_synccpu_write)) == 0\n\t    || (arg->flags & ~(drm_vmw_synccpu_read | drm_vmw_synccpu_write |\n\t\t\t       drm_vmw_synccpu_dontblock |\n\t\t\t       drm_vmw_synccpu_allow_cs)) != 0) {\n\t\tDRM_ERROR(\"Illegal synccpu flags.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (arg->op) {\n\tcase drm_vmw_synccpu_grab:\n\t\tret = vmw_user_bo_lookup(file_priv, arg->handle, &vbo);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\n\t\tret = vmw_user_bo_synccpu_grab(vbo, arg->flags);\n\t\tvmw_bo_unreference(&vbo);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tif (ret == -ERESTARTSYS || ret == -EBUSY)\n\t\t\t\treturn -EBUSY;\n\t\t\tDRM_ERROR(\"Failed synccpu grab on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tcase drm_vmw_synccpu_release:\n\t\tret = vmw_user_bo_synccpu_release(file_priv,\n\t\t\t\t\t\t  arg->handle,\n\t\t\t\t\t\t  arg->flags);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed synccpu release on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Invalid synccpu operation.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int vmw_user_bo_synccpu_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *file_priv)\n{\n\tstruct drm_vmw_synccpu_arg *arg =\n\t\t(struct drm_vmw_synccpu_arg *) data;\n\tstruct vmw_bo *vbo;\n\tint ret;\n\n\tif ((arg->flags & (drm_vmw_synccpu_read | drm_vmw_synccpu_write)) == 0\n\t    || (arg->flags & ~(drm_vmw_synccpu_read | drm_vmw_synccpu_write |\n\t\t\t       drm_vmw_synccpu_dontblock |\n\t\t\t       drm_vmw_synccpu_allow_cs)) != 0) {\n\t\tDRM_ERROR(\"Illegal synccpu flags.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (arg->op) {\n\tcase drm_vmw_synccpu_grab:\n\t\tret = vmw_user_bo_lookup(file_priv, arg->handle, &vbo);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\n\t\tret = vmw_user_bo_synccpu_grab(vbo, arg->flags);\n\t\tvmw_bo_unreference(&vbo);\n\t\tdrm_gem_object_put(&vbo->tbo.base);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tif (ret == -ERESTARTSYS || ret == -EBUSY)\n\t\t\t\treturn -EBUSY;\n\t\t\tDRM_ERROR(\"Failed synccpu grab on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tcase drm_vmw_synccpu_release:\n\t\tret = vmw_user_bo_synccpu_release(file_priv,\n\t\t\t\t\t\t  arg->handle,\n\t\t\t\t\t\t  arg->flags);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed synccpu release on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Invalid synccpu operation.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,6 +22,7 @@\n \n \t\tret = vmw_user_bo_synccpu_grab(vbo, arg->flags);\n \t\tvmw_bo_unreference(&vbo);\n+\t\tdrm_gem_object_put(&vbo->tbo.base);\n \t\tif (unlikely(ret != 0)) {\n \t\t\tif (ret == -ERESTARTSYS || ret == -EBUSY)\n \t\t\t\treturn -EBUSY;",
        "function_modified_lines": {
            "added": [
                "\t\tdrm_gem_object_put(&vbo->tbo.base);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "static int vmw_user_bo_synccpu_release(struct drm_file *filp,\n\t\t\t\t       uint32_t handle,\n\t\t\t\t       uint32_t flags)\n{\n\tstruct vmw_bo *vmw_bo;\n\tint ret = vmw_user_bo_lookup(filp, handle, &vmw_bo);\n\n\tif (!ret) {\n\t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n\t\t\tatomic_dec(&vmw_bo->cpu_writers);\n\t\t}\n\t\tttm_bo_put(&vmw_bo->tbo);\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int vmw_user_bo_synccpu_release(struct drm_file *filp,\n\t\t\t\t       uint32_t handle,\n\t\t\t\t       uint32_t flags)\n{\n\tstruct vmw_bo *vmw_bo;\n\tint ret = vmw_user_bo_lookup(filp, handle, &vmw_bo);\n\n\tif (!ret) {\n\t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n\t\t\tatomic_dec(&vmw_bo->cpu_writers);\n\t\t}\n\t\tttm_bo_put(&vmw_bo->tbo);\n\t}\n\n\tdrm_gem_object_put(&vmw_bo->tbo.base);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,5 +12,6 @@\n \t\tttm_bo_put(&vmw_bo->tbo);\n \t}\n \n+\tdrm_gem_object_put(&vmw_bo->tbo.base);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tdrm_gem_object_put(&vmw_bo->tbo.base);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n\t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tttm_bo_put(&vmw_bo->tbo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n\t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tttm_bo_put(&vmw_bo->tbo);\n\tdrm_gem_object_put(&vmw_bo->tbo.base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,6 +18,7 @@\n \t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n \tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n \tttm_bo_put(&vmw_bo->tbo);\n+\tdrm_gem_object_put(&vmw_bo->tbo.base);\n \tif (unlikely(ret != 0))\n \t\treturn ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tdrm_gem_object_put(&vmw_bo->tbo.base);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tttm_bo_put(&vmw_bo->tbo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tttm_bo_put(&vmw_bo->tbo);\n\tdrm_gem_object_put(&vmw_bo->tbo.base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n \tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n \tttm_bo_put(&vmw_bo->tbo);\n+\tdrm_gem_object_put(&vmw_bo->tbo.base);\n \tif (unlikely(ret != 0))\n \t\treturn ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tdrm_gem_object_put(&vmw_bo->tbo.base);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "int vmw_gem_object_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tunion drm_vmw_alloc_dmabuf_arg *arg =\n\t    (union drm_vmw_alloc_dmabuf_arg *)data;\n\tstruct drm_vmw_alloc_dmabuf_req *req = &arg->req;\n\tstruct drm_vmw_dmabuf_rep *rep = &arg->rep;\n\tstruct vmw_bo *vbo;\n\tuint32_t handle;\n\tint ret;\n\n\tret = vmw_gem_object_create_with_handle(dev_priv, filp,\n\t\t\t\t\t\treq->size, &handle, &vbo);\n\tif (ret)\n\t\tgoto out_no_bo;\n\n\trep->handle = handle;\n\trep->map_handle = drm_vma_node_offset_addr(&vbo->tbo.base.vma_node);\n\trep->cur_gmr_id = handle;\n\trep->cur_gmr_offset = 0;\nout_no_bo:\n\treturn ret;\n}",
        "code_after_change": "int vmw_gem_object_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tunion drm_vmw_alloc_dmabuf_arg *arg =\n\t    (union drm_vmw_alloc_dmabuf_arg *)data;\n\tstruct drm_vmw_alloc_dmabuf_req *req = &arg->req;\n\tstruct drm_vmw_dmabuf_rep *rep = &arg->rep;\n\tstruct vmw_bo *vbo;\n\tuint32_t handle;\n\tint ret;\n\n\tret = vmw_gem_object_create_with_handle(dev_priv, filp,\n\t\t\t\t\t\treq->size, &handle, &vbo);\n\tif (ret)\n\t\tgoto out_no_bo;\n\n\trep->handle = handle;\n\trep->map_handle = drm_vma_node_offset_addr(&vbo->tbo.base.vma_node);\n\trep->cur_gmr_id = handle;\n\trep->cur_gmr_offset = 0;\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(&vbo->tbo.base);\nout_no_bo:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,6 +19,8 @@\n \trep->map_handle = drm_vma_node_offset_addr(&vbo->tbo.base.vma_node);\n \trep->cur_gmr_id = handle;\n \trep->cur_gmr_offset = 0;\n+\t/* drop reference from allocate - handle holds it now */\n+\tdrm_gem_object_put(&vbo->tbo.base);\n out_no_bo:\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t/* drop reference from allocate - handle holds it now */",
                "\tdrm_gem_object_put(&vbo->tbo.base);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "static struct drm_framebuffer *vmw_kms_fb_create(struct drm_device *dev,\n\t\t\t\t\t\t struct drm_file *file_priv,\n\t\t\t\t\t\t const struct drm_mode_fb_cmd2 *mode_cmd)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_framebuffer *vfb = NULL;\n\tstruct vmw_surface *surface = NULL;\n\tstruct vmw_bo *bo = NULL;\n\tint ret;\n\n\t/* returns either a bo or surface */\n\tret = vmw_user_lookup_handle(dev_priv, file_priv,\n\t\t\t\t     mode_cmd->handles[0],\n\t\t\t\t     &surface, &bo);\n\tif (ret) {\n\t\tDRM_ERROR(\"Invalid buffer object handle %u (0x%x).\\n\",\n\t\t\t  mode_cmd->handles[0], mode_cmd->handles[0]);\n\t\tgoto err_out;\n\t}\n\n\n\tif (!bo &&\n\t    !vmw_kms_srf_ok(dev_priv, mode_cmd->width, mode_cmd->height)) {\n\t\tDRM_ERROR(\"Surface size cannot exceed %dx%d\\n\",\n\t\t\tdev_priv->texture_max_width,\n\t\t\tdev_priv->texture_max_height);\n\t\tgoto err_out;\n\t}\n\n\n\tvfb = vmw_kms_new_framebuffer(dev_priv, bo, surface,\n\t\t\t\t      !(dev_priv->capabilities & SVGA_CAP_3D),\n\t\t\t\t      mode_cmd);\n\tif (IS_ERR(vfb)) {\n\t\tret = PTR_ERR(vfb);\n\t\tgoto err_out;\n\t}\n\nerr_out:\n\t/* vmw_user_lookup_handle takes one ref so does new_fb */\n\tif (bo)\n\t\tvmw_bo_unreference(&bo);\n\tif (surface)\n\t\tvmw_surface_unreference(&surface);\n\n\tif (ret) {\n\t\tDRM_ERROR(\"failed to create vmw_framebuffer: %i\\n\", ret);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn &vfb->base;\n}",
        "code_after_change": "static struct drm_framebuffer *vmw_kms_fb_create(struct drm_device *dev,\n\t\t\t\t\t\t struct drm_file *file_priv,\n\t\t\t\t\t\t const struct drm_mode_fb_cmd2 *mode_cmd)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_framebuffer *vfb = NULL;\n\tstruct vmw_surface *surface = NULL;\n\tstruct vmw_bo *bo = NULL;\n\tint ret;\n\n\t/* returns either a bo or surface */\n\tret = vmw_user_lookup_handle(dev_priv, file_priv,\n\t\t\t\t     mode_cmd->handles[0],\n\t\t\t\t     &surface, &bo);\n\tif (ret) {\n\t\tDRM_ERROR(\"Invalid buffer object handle %u (0x%x).\\n\",\n\t\t\t  mode_cmd->handles[0], mode_cmd->handles[0]);\n\t\tgoto err_out;\n\t}\n\n\n\tif (!bo &&\n\t    !vmw_kms_srf_ok(dev_priv, mode_cmd->width, mode_cmd->height)) {\n\t\tDRM_ERROR(\"Surface size cannot exceed %dx%d\\n\",\n\t\t\tdev_priv->texture_max_width,\n\t\t\tdev_priv->texture_max_height);\n\t\tgoto err_out;\n\t}\n\n\n\tvfb = vmw_kms_new_framebuffer(dev_priv, bo, surface,\n\t\t\t\t      !(dev_priv->capabilities & SVGA_CAP_3D),\n\t\t\t\t      mode_cmd);\n\tif (IS_ERR(vfb)) {\n\t\tret = PTR_ERR(vfb);\n\t\tgoto err_out;\n\t}\n\nerr_out:\n\t/* vmw_user_lookup_handle takes one ref so does new_fb */\n\tif (bo) {\n\t\tvmw_bo_unreference(&bo);\n\t\tdrm_gem_object_put(&bo->tbo.base);\n\t}\n\tif (surface)\n\t\tvmw_surface_unreference(&surface);\n\n\tif (ret) {\n\t\tDRM_ERROR(\"failed to create vmw_framebuffer: %i\\n\", ret);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn &vfb->base;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,8 +38,10 @@\n \n err_out:\n \t/* vmw_user_lookup_handle takes one ref so does new_fb */\n-\tif (bo)\n+\tif (bo) {\n \t\tvmw_bo_unreference(&bo);\n+\t\tdrm_gem_object_put(&bo->tbo.base);\n+\t}\n \tif (surface)\n \t\tvmw_surface_unreference(&surface);\n ",
        "function_modified_lines": {
            "added": [
                "\tif (bo) {",
                "\t\tdrm_gem_object_put(&bo->tbo.base);",
                "\t}"
            ],
            "deleted": [
                "\tif (bo)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "int vmw_overlay_ioctl(struct drm_device *dev, void *data,\n\t\t      struct drm_file *file_priv)\n{\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_overlay *overlay = dev_priv->overlay_priv;\n\tstruct drm_vmw_control_stream_arg *arg =\n\t    (struct drm_vmw_control_stream_arg *)data;\n\tstruct vmw_bo *buf;\n\tstruct vmw_resource *res;\n\tint ret;\n\n\tif (!vmw_overlay_available(dev_priv))\n\t\treturn -ENOSYS;\n\n\tret = vmw_user_stream_lookup(dev_priv, tfile, &arg->stream_id, &res);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&overlay->mutex);\n\n\tif (!arg->enabled) {\n\t\tret = vmw_overlay_stop(dev_priv, arg->stream_id, false, true);\n\t\tgoto out_unlock;\n\t}\n\n\tret = vmw_user_bo_lookup(file_priv, arg->handle, &buf);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = vmw_overlay_update_stream(dev_priv, buf, arg, true);\n\n\tvmw_bo_unreference(&buf);\n\nout_unlock:\n\tmutex_unlock(&overlay->mutex);\n\tvmw_resource_unreference(&res);\n\n\treturn ret;\n}",
        "code_after_change": "int vmw_overlay_ioctl(struct drm_device *dev, void *data,\n\t\t      struct drm_file *file_priv)\n{\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_overlay *overlay = dev_priv->overlay_priv;\n\tstruct drm_vmw_control_stream_arg *arg =\n\t    (struct drm_vmw_control_stream_arg *)data;\n\tstruct vmw_bo *buf;\n\tstruct vmw_resource *res;\n\tint ret;\n\n\tif (!vmw_overlay_available(dev_priv))\n\t\treturn -ENOSYS;\n\n\tret = vmw_user_stream_lookup(dev_priv, tfile, &arg->stream_id, &res);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&overlay->mutex);\n\n\tif (!arg->enabled) {\n\t\tret = vmw_overlay_stop(dev_priv, arg->stream_id, false, true);\n\t\tgoto out_unlock;\n\t}\n\n\tret = vmw_user_bo_lookup(file_priv, arg->handle, &buf);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = vmw_overlay_update_stream(dev_priv, buf, arg, true);\n\n\tvmw_bo_unreference(&buf);\n\tdrm_gem_object_put(&buf->tbo.base);\n\nout_unlock:\n\tmutex_unlock(&overlay->mutex);\n\tvmw_resource_unreference(&res);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,6 +31,7 @@\n \tret = vmw_overlay_update_stream(dev_priv, buf, arg, true);\n \n \tvmw_bo_unreference(&buf);\n+\tdrm_gem_object_put(&buf->tbo.base);\n \n out_unlock:\n \tmutex_unlock(&overlay->mutex);",
        "function_modified_lines": {
            "added": [
                "\tdrm_gem_object_put(&buf->tbo.base);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "static int vmw_shader_define(struct drm_device *dev, struct drm_file *file_priv,\n\t\t\t     enum drm_vmw_shader_type shader_type_drm,\n\t\t\t     u32 buffer_handle, size_t size, size_t offset,\n\t\t\t     uint8_t num_input_sig, uint8_t num_output_sig,\n\t\t\t     uint32_t *shader_handle)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_bo *buffer = NULL;\n\tSVGA3dShaderType shader_type;\n\tint ret;\n\n\tif (buffer_handle != SVGA3D_INVALID_ID) {\n\t\tret = vmw_user_bo_lookup(file_priv, buffer_handle, &buffer);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tVMW_DEBUG_USER(\"Couldn't find buffer for shader creation.\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tif ((u64)buffer->tbo.base.size < (u64)size + (u64)offset) {\n\t\t\tVMW_DEBUG_USER(\"Illegal buffer- or shader size.\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_bad_arg;\n\t\t}\n\t}\n\n\tswitch (shader_type_drm) {\n\tcase drm_vmw_shader_type_vs:\n\t\tshader_type = SVGA3D_SHADERTYPE_VS;\n\t\tbreak;\n\tcase drm_vmw_shader_type_ps:\n\t\tshader_type = SVGA3D_SHADERTYPE_PS;\n\t\tbreak;\n\tdefault:\n\t\tVMW_DEBUG_USER(\"Illegal shader type.\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out_bad_arg;\n\t}\n\n\tret = vmw_user_shader_alloc(dev_priv, buffer, size, offset,\n\t\t\t\t    shader_type, num_input_sig,\n\t\t\t\t    num_output_sig, tfile, shader_handle);\nout_bad_arg:\n\tvmw_bo_unreference(&buffer);\n\treturn ret;\n}",
        "code_after_change": "static int vmw_shader_define(struct drm_device *dev, struct drm_file *file_priv,\n\t\t\t     enum drm_vmw_shader_type shader_type_drm,\n\t\t\t     u32 buffer_handle, size_t size, size_t offset,\n\t\t\t     uint8_t num_input_sig, uint8_t num_output_sig,\n\t\t\t     uint32_t *shader_handle)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_bo *buffer = NULL;\n\tSVGA3dShaderType shader_type;\n\tint ret;\n\n\tif (buffer_handle != SVGA3D_INVALID_ID) {\n\t\tret = vmw_user_bo_lookup(file_priv, buffer_handle, &buffer);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tVMW_DEBUG_USER(\"Couldn't find buffer for shader creation.\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tif ((u64)buffer->tbo.base.size < (u64)size + (u64)offset) {\n\t\t\tVMW_DEBUG_USER(\"Illegal buffer- or shader size.\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_bad_arg;\n\t\t}\n\t}\n\n\tswitch (shader_type_drm) {\n\tcase drm_vmw_shader_type_vs:\n\t\tshader_type = SVGA3D_SHADERTYPE_VS;\n\t\tbreak;\n\tcase drm_vmw_shader_type_ps:\n\t\tshader_type = SVGA3D_SHADERTYPE_PS;\n\t\tbreak;\n\tdefault:\n\t\tVMW_DEBUG_USER(\"Illegal shader type.\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out_bad_arg;\n\t}\n\n\tret = vmw_user_shader_alloc(dev_priv, buffer, size, offset,\n\t\t\t\t    shader_type, num_input_sig,\n\t\t\t\t    num_output_sig, tfile, shader_handle);\nout_bad_arg:\n\tvmw_bo_unreference(&buffer);\n\tdrm_gem_object_put(&buffer->tbo.base);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -42,5 +42,6 @@\n \t\t\t\t    num_output_sig, tfile, shader_handle);\n out_bad_arg:\n \tvmw_bo_unreference(&buffer);\n+\tdrm_gem_object_put(&buffer->tbo.base);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tdrm_gem_object_put(&buffer->tbo.base);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel."
    },
    {
        "cve_id": "CVE-2023-35823",
        "code_before_change": "int saa7134_ts_fini(struct saa7134_dev *dev)\n{\n\tsaa7134_pgtable_free(dev->pci, &dev->ts_q.pt);\n\treturn 0;\n}",
        "code_after_change": "int saa7134_ts_fini(struct saa7134_dev *dev)\n{\n\tdel_timer_sync(&dev->ts_q.timeout);\n\tsaa7134_pgtable_free(dev->pci, &dev->ts_q.pt);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n int saa7134_ts_fini(struct saa7134_dev *dev)\n {\n+\tdel_timer_sync(&dev->ts_q.timeout);\n \tsaa7134_pgtable_free(dev->pci, &dev->ts_q.pt);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tdel_timer_sync(&dev->ts_q.timeout);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in saa7134_finidev in drivers/media/pci/saa7134/saa7134-core.c."
    },
    {
        "cve_id": "CVE-2023-35823",
        "code_before_change": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\treturn 0;\n}",
        "code_after_change": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\tdel_timer_sync(&dev->vbi_q.timeout);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n int saa7134_vbi_fini(struct saa7134_dev *dev)\n {\n \t/* nothing */\n+\tdel_timer_sync(&dev->vbi_q.timeout);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tdel_timer_sync(&dev->vbi_q.timeout);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in saa7134_finidev in drivers/media/pci/saa7134/saa7134-core.c."
    },
    {
        "cve_id": "CVE-2023-35823",
        "code_before_change": "void saa7134_video_fini(struct saa7134_dev *dev)\n{\n\t/* free stuff */\n\tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n\tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);\n\tv4l2_ctrl_handler_free(&dev->ctrl_handler);\n\tif (card_has_radio(dev))\n\t\tv4l2_ctrl_handler_free(&dev->radio_ctrl_handler);\n}",
        "code_after_change": "void saa7134_video_fini(struct saa7134_dev *dev)\n{\n\tdel_timer_sync(&dev->video_q.timeout);\n\t/* free stuff */\n\tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n\tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);\n\tv4l2_ctrl_handler_free(&dev->ctrl_handler);\n\tif (card_has_radio(dev))\n\t\tv4l2_ctrl_handler_free(&dev->radio_ctrl_handler);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n void saa7134_video_fini(struct saa7134_dev *dev)\n {\n+\tdel_timer_sync(&dev->video_q.timeout);\n \t/* free stuff */\n \tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n \tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);",
        "function_modified_lines": {
            "added": [
                "\tdel_timer_sync(&dev->video_q.timeout);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in saa7134_finidev in drivers/media/pci/saa7134/saa7134-core.c."
    },
    {
        "cve_id": "CVE-2023-35824",
        "code_before_change": "static void dm1105_remove(struct pci_dev *pdev)\n{\n\tstruct dm1105_dev *dev = pci_get_drvdata(pdev);\n\tstruct dvb_adapter *dvb_adapter = &dev->dvb_adapter;\n\tstruct dvb_demux *dvbdemux = &dev->demux;\n\tstruct dmx_demux *dmx = &dvbdemux->dmx;\n\n\tdm1105_ir_exit(dev);\n\tdmx->close(dmx);\n\tdvb_net_release(&dev->dvbnet);\n\tif (dev->fe)\n\t\tdvb_unregister_frontend(dev->fe);\n\n\tdmx->disconnect_frontend(dmx);\n\tdmx->remove_frontend(dmx, &dev->mem_frontend);\n\tdmx->remove_frontend(dmx, &dev->hw_frontend);\n\tdvb_dmxdev_release(&dev->dmxdev);\n\tdvb_dmx_release(dvbdemux);\n\tdvb_unregister_adapter(dvb_adapter);\n\ti2c_del_adapter(&dev->i2c_adap);\n\n\tdm1105_hw_exit(dev);\n\tfree_irq(pdev->irq, dev);\n\tpci_iounmap(pdev, dev->io_mem);\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tdm1105_devcount--;\n\tkfree(dev);\n}",
        "code_after_change": "static void dm1105_remove(struct pci_dev *pdev)\n{\n\tstruct dm1105_dev *dev = pci_get_drvdata(pdev);\n\tstruct dvb_adapter *dvb_adapter = &dev->dvb_adapter;\n\tstruct dvb_demux *dvbdemux = &dev->demux;\n\tstruct dmx_demux *dmx = &dvbdemux->dmx;\n\n\tcancel_work_sync(&dev->ir.work);\n\tdm1105_ir_exit(dev);\n\tdmx->close(dmx);\n\tdvb_net_release(&dev->dvbnet);\n\tif (dev->fe)\n\t\tdvb_unregister_frontend(dev->fe);\n\n\tdmx->disconnect_frontend(dmx);\n\tdmx->remove_frontend(dmx, &dev->mem_frontend);\n\tdmx->remove_frontend(dmx, &dev->hw_frontend);\n\tdvb_dmxdev_release(&dev->dmxdev);\n\tdvb_dmx_release(dvbdemux);\n\tdvb_unregister_adapter(dvb_adapter);\n\ti2c_del_adapter(&dev->i2c_adap);\n\n\tdm1105_hw_exit(dev);\n\tfree_irq(pdev->irq, dev);\n\tpci_iounmap(pdev, dev->io_mem);\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tdm1105_devcount--;\n\tkfree(dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tstruct dvb_demux *dvbdemux = &dev->demux;\n \tstruct dmx_demux *dmx = &dvbdemux->dmx;\n \n+\tcancel_work_sync(&dev->ir.work);\n \tdm1105_ir_exit(dev);\n \tdmx->close(dmx);\n \tdvb_net_release(&dev->dvbnet);",
        "function_modified_lines": {
            "added": [
                "\tcancel_work_sync(&dev->ir.work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in dm1105_remove in drivers/media/pci/dm1105/dm1105.c."
    },
    {
        "cve_id": "CVE-2023-35826",
        "code_before_change": "static int cedrus_remove(struct platform_device *pdev)\n{\n\tstruct cedrus_dev *dev = platform_get_drvdata(pdev);\n\n\tif (media_devnode_is_registered(dev->mdev.devnode)) {\n\t\tmedia_device_unregister(&dev->mdev);\n\t\tv4l2_m2m_unregister_media_controller(dev->m2m_dev);\n\t\tmedia_device_cleanup(&dev->mdev);\n\t}\n\n\tv4l2_m2m_release(dev->m2m_dev);\n\tvideo_unregister_device(&dev->vfd);\n\tv4l2_device_unregister(&dev->v4l2_dev);\n\n\tcedrus_hw_remove(dev);\n\n\treturn 0;\n}",
        "code_after_change": "static int cedrus_remove(struct platform_device *pdev)\n{\n\tstruct cedrus_dev *dev = platform_get_drvdata(pdev);\n\n\tcancel_delayed_work_sync(&dev->watchdog_work);\n\tif (media_devnode_is_registered(dev->mdev.devnode)) {\n\t\tmedia_device_unregister(&dev->mdev);\n\t\tv4l2_m2m_unregister_media_controller(dev->m2m_dev);\n\t\tmedia_device_cleanup(&dev->mdev);\n\t}\n\n\tv4l2_m2m_release(dev->m2m_dev);\n\tvideo_unregister_device(&dev->vfd);\n\tv4l2_device_unregister(&dev->v4l2_dev);\n\n\tcedrus_hw_remove(dev);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n {\n \tstruct cedrus_dev *dev = platform_get_drvdata(pdev);\n \n+\tcancel_delayed_work_sync(&dev->watchdog_work);\n \tif (media_devnode_is_registered(dev->mdev.devnode)) {\n \t\tmedia_device_unregister(&dev->mdev);\n \t\tv4l2_m2m_unregister_media_controller(dev->m2m_dev);",
        "function_modified_lines": {
            "added": [
                "\tcancel_delayed_work_sync(&dev->watchdog_work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in cedrus_remove in drivers/staging/media/sunxi/cedrus/cedrus.c."
    },
    {
        "cve_id": "CVE-2023-35827",
        "code_before_change": "static int ravb_close(struct net_device *ndev)\n{\n\tstruct device_node *np = ndev->dev.parent->of_node;\n\tstruct ravb_private *priv = netdev_priv(ndev);\n\tconst struct ravb_hw_info *info = priv->info;\n\tstruct ravb_tstamp_skb *ts_skb, *ts_skb2;\n\n\tnetif_tx_stop_all_queues(ndev);\n\n\t/* Disable interrupts by clearing the interrupt masks. */\n\travb_write(ndev, 0, RIC0);\n\travb_write(ndev, 0, RIC2);\n\travb_write(ndev, 0, TIC);\n\n\t/* Stop PTP Clock driver */\n\tif (info->gptp)\n\t\travb_ptp_stop(ndev);\n\n\t/* Set the config mode to stop the AVB-DMAC's processes */\n\tif (ravb_stop_dma(ndev) < 0)\n\t\tnetdev_err(ndev,\n\t\t\t   \"device will be stopped after h/w processes are done.\\n\");\n\n\t/* Clear the timestamp list */\n\tif (info->gptp || info->ccc_gac) {\n\t\tlist_for_each_entry_safe(ts_skb, ts_skb2, &priv->ts_skb_list, list) {\n\t\t\tlist_del(&ts_skb->list);\n\t\t\tkfree_skb(ts_skb->skb);\n\t\t\tkfree(ts_skb);\n\t\t}\n\t}\n\n\t/* PHY disconnect */\n\tif (ndev->phydev) {\n\t\tphy_stop(ndev->phydev);\n\t\tphy_disconnect(ndev->phydev);\n\t\tif (of_phy_is_fixed_link(np))\n\t\t\tof_phy_deregister_fixed_link(np);\n\t}\n\n\tif (info->multi_irqs) {\n\t\tfree_irq(priv->tx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->tx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->emac_irq, ndev);\n\t\tif (info->err_mgmt_irqs) {\n\t\t\tfree_irq(priv->erra_irq, ndev);\n\t\t\tfree_irq(priv->mgmta_irq, ndev);\n\t\t}\n\t}\n\tfree_irq(ndev->irq, ndev);\n\n\tif (info->nc_queues)\n\t\tnapi_disable(&priv->napi[RAVB_NC]);\n\tnapi_disable(&priv->napi[RAVB_BE]);\n\n\t/* Free all the skb's in the RX queue and the DMA buffers. */\n\travb_ring_free(ndev, RAVB_BE);\n\tif (info->nc_queues)\n\t\travb_ring_free(ndev, RAVB_NC);\n\n\treturn 0;\n}",
        "code_after_change": "static int ravb_close(struct net_device *ndev)\n{\n\tstruct device_node *np = ndev->dev.parent->of_node;\n\tstruct ravb_private *priv = netdev_priv(ndev);\n\tconst struct ravb_hw_info *info = priv->info;\n\tstruct ravb_tstamp_skb *ts_skb, *ts_skb2;\n\n\tnetif_tx_stop_all_queues(ndev);\n\n\t/* Disable interrupts by clearing the interrupt masks. */\n\travb_write(ndev, 0, RIC0);\n\travb_write(ndev, 0, RIC2);\n\travb_write(ndev, 0, TIC);\n\n\t/* Stop PTP Clock driver */\n\tif (info->gptp)\n\t\travb_ptp_stop(ndev);\n\n\t/* Set the config mode to stop the AVB-DMAC's processes */\n\tif (ravb_stop_dma(ndev) < 0)\n\t\tnetdev_err(ndev,\n\t\t\t   \"device will be stopped after h/w processes are done.\\n\");\n\n\t/* Clear the timestamp list */\n\tif (info->gptp || info->ccc_gac) {\n\t\tlist_for_each_entry_safe(ts_skb, ts_skb2, &priv->ts_skb_list, list) {\n\t\t\tlist_del(&ts_skb->list);\n\t\t\tkfree_skb(ts_skb->skb);\n\t\t\tkfree(ts_skb);\n\t\t}\n\t}\n\n\t/* PHY disconnect */\n\tif (ndev->phydev) {\n\t\tphy_stop(ndev->phydev);\n\t\tphy_disconnect(ndev->phydev);\n\t\tif (of_phy_is_fixed_link(np))\n\t\t\tof_phy_deregister_fixed_link(np);\n\t}\n\n\tcancel_work_sync(&priv->work);\n\n\tif (info->multi_irqs) {\n\t\tfree_irq(priv->tx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->tx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->emac_irq, ndev);\n\t\tif (info->err_mgmt_irqs) {\n\t\t\tfree_irq(priv->erra_irq, ndev);\n\t\t\tfree_irq(priv->mgmta_irq, ndev);\n\t\t}\n\t}\n\tfree_irq(ndev->irq, ndev);\n\n\tif (info->nc_queues)\n\t\tnapi_disable(&priv->napi[RAVB_NC]);\n\tnapi_disable(&priv->napi[RAVB_BE]);\n\n\t/* Free all the skb's in the RX queue and the DMA buffers. */\n\travb_ring_free(ndev, RAVB_BE);\n\tif (info->nc_queues)\n\t\travb_ring_free(ndev, RAVB_NC);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,6 +38,8 @@\n \t\t\tof_phy_deregister_fixed_link(np);\n \t}\n \n+\tcancel_work_sync(&priv->work);\n+\n \tif (info->multi_irqs) {\n \t\tfree_irq(priv->tx_irqs[RAVB_NC], ndev);\n \t\tfree_irq(priv->rx_irqs[RAVB_NC], ndev);",
        "function_modified_lines": {
            "added": [
                "\tcancel_work_sync(&priv->work);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.3.8. A use-after-free was found in ravb_remove in drivers/net/ethernet/renesas/ravb_main.c."
    },
    {
        "cve_id": "CVE-2023-35828",
        "code_before_change": "static int renesas_usb3_remove(struct platform_device *pdev)\n{\n\tstruct renesas_usb3 *usb3 = platform_get_drvdata(pdev);\n\n\tdebugfs_remove_recursive(usb3->dentry);\n\tdevice_remove_file(&pdev->dev, &dev_attr_role);\n\n\tusb_role_switch_unregister(usb3->role_sw);\n\n\tusb_del_gadget_udc(&usb3->gadget);\n\treset_control_assert(usb3->usbp_rstc);\n\trenesas_usb3_dma_free_prd(usb3, &pdev->dev);\n\n\t__renesas_usb3_ep_free_request(usb3->ep0_req);\n\tpm_runtime_disable(&pdev->dev);\n\n\treturn 0;\n}",
        "code_after_change": "static int renesas_usb3_remove(struct platform_device *pdev)\n{\n\tstruct renesas_usb3 *usb3 = platform_get_drvdata(pdev);\n\n\tdebugfs_remove_recursive(usb3->dentry);\n\tdevice_remove_file(&pdev->dev, &dev_attr_role);\n\n\tcancel_work_sync(&usb3->role_work);\n\tusb_role_switch_unregister(usb3->role_sw);\n\n\tusb_del_gadget_udc(&usb3->gadget);\n\treset_control_assert(usb3->usbp_rstc);\n\trenesas_usb3_dma_free_prd(usb3, &pdev->dev);\n\n\t__renesas_usb3_ep_free_request(usb3->ep0_req);\n\tpm_runtime_disable(&pdev->dev);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tdebugfs_remove_recursive(usb3->dentry);\n \tdevice_remove_file(&pdev->dev, &dev_attr_role);\n \n+\tcancel_work_sync(&usb3->role_work);\n \tusb_role_switch_unregister(usb3->role_sw);\n \n \tusb_del_gadget_udc(&usb3->gadget);",
        "function_modified_lines": {
            "added": [
                "\tcancel_work_sync(&usb3->role_work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in renesas_usb3_remove in drivers/usb/gadget/udc/renesas_usb3.c."
    },
    {
        "cve_id": "CVE-2023-35829",
        "code_before_change": "static int rkvdec_remove(struct platform_device *pdev)\n{\n\tstruct rkvdec_dev *rkvdec = platform_get_drvdata(pdev);\n\n\trkvdec_v4l2_cleanup(rkvdec);\n\tpm_runtime_disable(&pdev->dev);\n\tpm_runtime_dont_use_autosuspend(&pdev->dev);\n\treturn 0;\n}",
        "code_after_change": "static int rkvdec_remove(struct platform_device *pdev)\n{\n\tstruct rkvdec_dev *rkvdec = platform_get_drvdata(pdev);\n\n\tcancel_delayed_work_sync(&rkvdec->watchdog_work);\n\n\trkvdec_v4l2_cleanup(rkvdec);\n\tpm_runtime_disable(&pdev->dev);\n\tpm_runtime_dont_use_autosuspend(&pdev->dev);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,8 @@\n static int rkvdec_remove(struct platform_device *pdev)\n {\n \tstruct rkvdec_dev *rkvdec = platform_get_drvdata(pdev);\n+\n+\tcancel_delayed_work_sync(&rkvdec->watchdog_work);\n \n \trkvdec_v4l2_cleanup(rkvdec);\n \tpm_runtime_disable(&pdev->dev);",
        "function_modified_lines": {
            "added": [
                "",
                "\tcancel_delayed_work_sync(&rkvdec->watchdog_work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in rkvdec_remove in drivers/staging/media/rkvdec/rkvdec.c."
    },
    {
        "cve_id": "CVE-2023-42756",
        "code_before_change": "static int\ncall_ad(struct net *net, struct sock *ctnl, struct sk_buff *skb,\n\tstruct ip_set *set, struct nlattr *tb[], enum ipset_adt adt,\n\tu32 flags, bool use_lineno)\n{\n\tint ret;\n\tu32 lineno = 0;\n\tbool eexist = flags & IPSET_FLAG_EXIST, retried = false;\n\n\tdo {\n\t\tif (retried) {\n\t\t\t__ip_set_get(set);\n\t\t\tnfnl_unlock(NFNL_SUBSYS_IPSET);\n\t\t\tcond_resched();\n\t\t\tnfnl_lock(NFNL_SUBSYS_IPSET);\n\t\t\t__ip_set_put(set);\n\t\t}\n\n\t\tip_set_lock(set);\n\t\tret = set->variant->uadt(set, tb, adt, &lineno, flags, retried);\n\t\tip_set_unlock(set);\n\t\tretried = true;\n\t} while (ret == -ERANGE ||\n\t\t (ret == -EAGAIN &&\n\t\t  set->variant->resize &&\n\t\t  (ret = set->variant->resize(set, retried)) == 0));\n\n\tif (!ret || (ret == -IPSET_ERR_EXIST && eexist))\n\t\treturn 0;\n\tif (lineno && use_lineno) {\n\t\t/* Error in restore/batch mode: send back lineno */\n\t\tstruct nlmsghdr *rep, *nlh = nlmsg_hdr(skb);\n\t\tstruct sk_buff *skb2;\n\t\tstruct nlmsgerr *errmsg;\n\t\tsize_t payload = min(SIZE_MAX,\n\t\t\t\t     sizeof(*errmsg) + nlmsg_len(nlh));\n\t\tint min_len = nlmsg_total_size(sizeof(struct nfgenmsg));\n\t\tstruct nlattr *cda[IPSET_ATTR_CMD_MAX + 1];\n\t\tstruct nlattr *cmdattr;\n\t\tu32 *errline;\n\n\t\tskb2 = nlmsg_new(payload, GFP_KERNEL);\n\t\tif (!skb2)\n\t\t\treturn -ENOMEM;\n\t\trep = nlmsg_put(skb2, NETLINK_CB(skb).portid,\n\t\t\t\tnlh->nlmsg_seq, NLMSG_ERROR, payload, 0);\n\t\terrmsg = nlmsg_data(rep);\n\t\terrmsg->error = ret;\n\t\tunsafe_memcpy(&errmsg->msg, nlh, nlh->nlmsg_len,\n\t\t\t      /* Bounds checked by the skb layer. */);\n\n\t\tcmdattr = (void *)&errmsg->msg + min_len;\n\n\t\tret = nla_parse(cda, IPSET_ATTR_CMD_MAX, cmdattr,\n\t\t\t\tnlh->nlmsg_len - min_len, ip_set_adt_policy,\n\t\t\t\tNULL);\n\n\t\tif (ret) {\n\t\t\tnlmsg_free(skb2);\n\t\t\treturn ret;\n\t\t}\n\t\terrline = nla_data(cda[IPSET_ATTR_LINENO]);\n\n\t\t*errline = lineno;\n\n\t\tnfnetlink_unicast(skb2, net, NETLINK_CB(skb).portid);\n\t\t/* Signal netlink not to send its ACK/errmsg.  */\n\t\treturn -EINTR;\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int\ncall_ad(struct net *net, struct sock *ctnl, struct sk_buff *skb,\n\tstruct ip_set *set, struct nlattr *tb[], enum ipset_adt adt,\n\tu32 flags, bool use_lineno)\n{\n\tint ret;\n\tu32 lineno = 0;\n\tbool eexist = flags & IPSET_FLAG_EXIST, retried = false;\n\n\tdo {\n\t\tif (retried) {\n\t\t\t__ip_set_get_netlink(set);\n\t\t\tnfnl_unlock(NFNL_SUBSYS_IPSET);\n\t\t\tcond_resched();\n\t\t\tnfnl_lock(NFNL_SUBSYS_IPSET);\n\t\t\t__ip_set_put_netlink(set);\n\t\t}\n\n\t\tip_set_lock(set);\n\t\tret = set->variant->uadt(set, tb, adt, &lineno, flags, retried);\n\t\tip_set_unlock(set);\n\t\tretried = true;\n\t} while (ret == -ERANGE ||\n\t\t (ret == -EAGAIN &&\n\t\t  set->variant->resize &&\n\t\t  (ret = set->variant->resize(set, retried)) == 0));\n\n\tif (!ret || (ret == -IPSET_ERR_EXIST && eexist))\n\t\treturn 0;\n\tif (lineno && use_lineno) {\n\t\t/* Error in restore/batch mode: send back lineno */\n\t\tstruct nlmsghdr *rep, *nlh = nlmsg_hdr(skb);\n\t\tstruct sk_buff *skb2;\n\t\tstruct nlmsgerr *errmsg;\n\t\tsize_t payload = min(SIZE_MAX,\n\t\t\t\t     sizeof(*errmsg) + nlmsg_len(nlh));\n\t\tint min_len = nlmsg_total_size(sizeof(struct nfgenmsg));\n\t\tstruct nlattr *cda[IPSET_ATTR_CMD_MAX + 1];\n\t\tstruct nlattr *cmdattr;\n\t\tu32 *errline;\n\n\t\tskb2 = nlmsg_new(payload, GFP_KERNEL);\n\t\tif (!skb2)\n\t\t\treturn -ENOMEM;\n\t\trep = nlmsg_put(skb2, NETLINK_CB(skb).portid,\n\t\t\t\tnlh->nlmsg_seq, NLMSG_ERROR, payload, 0);\n\t\terrmsg = nlmsg_data(rep);\n\t\terrmsg->error = ret;\n\t\tunsafe_memcpy(&errmsg->msg, nlh, nlh->nlmsg_len,\n\t\t\t      /* Bounds checked by the skb layer. */);\n\n\t\tcmdattr = (void *)&errmsg->msg + min_len;\n\n\t\tret = nla_parse(cda, IPSET_ATTR_CMD_MAX, cmdattr,\n\t\t\t\tnlh->nlmsg_len - min_len, ip_set_adt_policy,\n\t\t\t\tNULL);\n\n\t\tif (ret) {\n\t\t\tnlmsg_free(skb2);\n\t\t\treturn ret;\n\t\t}\n\t\terrline = nla_data(cda[IPSET_ATTR_LINENO]);\n\n\t\t*errline = lineno;\n\n\t\tnfnetlink_unicast(skb2, net, NETLINK_CB(skb).portid);\n\t\t/* Signal netlink not to send its ACK/errmsg.  */\n\t\treturn -EINTR;\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,11 +9,11 @@\n \n \tdo {\n \t\tif (retried) {\n-\t\t\t__ip_set_get(set);\n+\t\t\t__ip_set_get_netlink(set);\n \t\t\tnfnl_unlock(NFNL_SUBSYS_IPSET);\n \t\t\tcond_resched();\n \t\t\tnfnl_lock(NFNL_SUBSYS_IPSET);\n-\t\t\t__ip_set_put(set);\n+\t\t\t__ip_set_put_netlink(set);\n \t\t}\n \n \t\tip_set_lock(set);",
        "function_modified_lines": {
            "added": [
                "\t\t\t__ip_set_get_netlink(set);",
                "\t\t\t__ip_set_put_netlink(set);"
            ],
            "deleted": [
                "\t\t\t__ip_set_get(set);",
                "\t\t\t__ip_set_put(set);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Netfilter subsystem of the Linux kernel. A race condition between IPSET_CMD_ADD and IPSET_CMD_SWAP can lead to a kernel panic due to the invocation of `__ip_set_put` on a wrong `set`. This issue may allow a local user to crash the system."
    },
    {
        "cve_id": "CVE-2023-4732",
        "code_before_change": "int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long addr, pgprot_t newprot, unsigned long cp_flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tspinlock_t *ptl;\n\tpmd_t entry;\n\tbool preserve_write;\n\tint ret;\n\tbool prot_numa = cp_flags & MM_CP_PROT_NUMA;\n\tbool uffd_wp = cp_flags & MM_CP_UFFD_WP;\n\tbool uffd_wp_resolve = cp_flags & MM_CP_UFFD_WP_RESOLVE;\n\n\tptl = __pmd_trans_huge_lock(pmd, vma);\n\tif (!ptl)\n\t\treturn 0;\n\n\tpreserve_write = prot_numa && pmd_write(*pmd);\n\tret = 1;\n\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\tif (is_swap_pmd(*pmd)) {\n\t\tswp_entry_t entry = pmd_to_swp_entry(*pmd);\n\n\t\tVM_BUG_ON(!is_pmd_migration_entry(*pmd));\n\t\tif (is_write_migration_entry(entry)) {\n\t\t\tpmd_t newpmd;\n\t\t\t/*\n\t\t\t * A protection check is difficult so\n\t\t\t * just be safe and disable write\n\t\t\t */\n\t\t\tmake_migration_entry_read(&entry);\n\t\t\tnewpmd = swp_entry_to_pmd(entry);\n\t\t\tif (pmd_swp_soft_dirty(*pmd))\n\t\t\t\tnewpmd = pmd_swp_mksoft_dirty(newpmd);\n\t\t\tset_pmd_at(mm, addr, pmd, newpmd);\n\t\t}\n\t\tgoto unlock;\n\t}\n#endif\n\n\t/*\n\t * Avoid trapping faults against the zero page. The read-only\n\t * data is likely to be read-cached on the local CPU and\n\t * local/remote hits to the zero page are not interesting.\n\t */\n\tif (prot_numa && is_huge_zero_pmd(*pmd))\n\t\tgoto unlock;\n\n\tif (prot_numa && pmd_protnone(*pmd))\n\t\tgoto unlock;\n\n\t/*\n\t * In case prot_numa, we are under mmap_read_lock(mm). It's critical\n\t * to not clear pmd intermittently to avoid race with MADV_DONTNEED\n\t * which is also under mmap_read_lock(mm):\n\t *\n\t *\tCPU0:\t\t\t\tCPU1:\n\t *\t\t\t\tchange_huge_pmd(prot_numa=1)\n\t *\t\t\t\t pmdp_huge_get_and_clear_notify()\n\t * madvise_dontneed()\n\t *  zap_pmd_range()\n\t *   pmd_trans_huge(*pmd) == 0 (without ptl)\n\t *   // skip the pmd\n\t *\t\t\t\t set_pmd_at();\n\t *\t\t\t\t // pmd is re-established\n\t *\n\t * The race makes MADV_DONTNEED miss the huge pmd and don't clear it\n\t * which may break userspace.\n\t *\n\t * pmdp_invalidate() is required to make sure we don't miss\n\t * dirty/young flags set by hardware.\n\t */\n\tentry = pmdp_invalidate(vma, addr, pmd);\n\n\tentry = pmd_modify(entry, newprot);\n\tif (preserve_write)\n\t\tentry = pmd_mk_savedwrite(entry);\n\tif (uffd_wp) {\n\t\tentry = pmd_wrprotect(entry);\n\t\tentry = pmd_mkuffd_wp(entry);\n\t} else if (uffd_wp_resolve) {\n\t\t/*\n\t\t * Leave the write bit to be handled by PF interrupt\n\t\t * handler, then things like COW could be properly\n\t\t * handled.\n\t\t */\n\t\tentry = pmd_clear_uffd_wp(entry);\n\t}\n\tret = HPAGE_PMD_NR;\n\tset_pmd_at(mm, addr, pmd, entry);\n\tBUG_ON(vma_is_anonymous(vma) && !preserve_write && pmd_write(entry));\nunlock:\n\tspin_unlock(ptl);\n\treturn ret;\n}",
        "code_after_change": "int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long addr, pgprot_t newprot, unsigned long cp_flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tspinlock_t *ptl;\n\tpmd_t entry;\n\tbool preserve_write;\n\tint ret;\n\tbool prot_numa = cp_flags & MM_CP_PROT_NUMA;\n\tbool uffd_wp = cp_flags & MM_CP_UFFD_WP;\n\tbool uffd_wp_resolve = cp_flags & MM_CP_UFFD_WP_RESOLVE;\n\n\tptl = __pmd_trans_huge_lock(pmd, vma);\n\tif (!ptl)\n\t\treturn 0;\n\n\tpreserve_write = prot_numa && pmd_write(*pmd);\n\tret = 1;\n\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\tif (is_swap_pmd(*pmd)) {\n\t\tswp_entry_t entry = pmd_to_swp_entry(*pmd);\n\n\t\tVM_BUG_ON(!is_pmd_migration_entry(*pmd));\n\t\tif (is_write_migration_entry(entry)) {\n\t\t\tpmd_t newpmd;\n\t\t\t/*\n\t\t\t * A protection check is difficult so\n\t\t\t * just be safe and disable write\n\t\t\t */\n\t\t\tmake_migration_entry_read(&entry);\n\t\t\tnewpmd = swp_entry_to_pmd(entry);\n\t\t\tif (pmd_swp_soft_dirty(*pmd))\n\t\t\t\tnewpmd = pmd_swp_mksoft_dirty(newpmd);\n\t\t\tif (pmd_swp_uffd_wp(*pmd))\n\t\t\t\tnewpmd = pmd_swp_mkuffd_wp(newpmd);\n\t\t\tset_pmd_at(mm, addr, pmd, newpmd);\n\t\t}\n\t\tgoto unlock;\n\t}\n#endif\n\n\t/*\n\t * Avoid trapping faults against the zero page. The read-only\n\t * data is likely to be read-cached on the local CPU and\n\t * local/remote hits to the zero page are not interesting.\n\t */\n\tif (prot_numa && is_huge_zero_pmd(*pmd))\n\t\tgoto unlock;\n\n\tif (prot_numa && pmd_protnone(*pmd))\n\t\tgoto unlock;\n\n\t/*\n\t * In case prot_numa, we are under mmap_read_lock(mm). It's critical\n\t * to not clear pmd intermittently to avoid race with MADV_DONTNEED\n\t * which is also under mmap_read_lock(mm):\n\t *\n\t *\tCPU0:\t\t\t\tCPU1:\n\t *\t\t\t\tchange_huge_pmd(prot_numa=1)\n\t *\t\t\t\t pmdp_huge_get_and_clear_notify()\n\t * madvise_dontneed()\n\t *  zap_pmd_range()\n\t *   pmd_trans_huge(*pmd) == 0 (without ptl)\n\t *   // skip the pmd\n\t *\t\t\t\t set_pmd_at();\n\t *\t\t\t\t // pmd is re-established\n\t *\n\t * The race makes MADV_DONTNEED miss the huge pmd and don't clear it\n\t * which may break userspace.\n\t *\n\t * pmdp_invalidate() is required to make sure we don't miss\n\t * dirty/young flags set by hardware.\n\t */\n\tentry = pmdp_invalidate(vma, addr, pmd);\n\n\tentry = pmd_modify(entry, newprot);\n\tif (preserve_write)\n\t\tentry = pmd_mk_savedwrite(entry);\n\tif (uffd_wp) {\n\t\tentry = pmd_wrprotect(entry);\n\t\tentry = pmd_mkuffd_wp(entry);\n\t} else if (uffd_wp_resolve) {\n\t\t/*\n\t\t * Leave the write bit to be handled by PF interrupt\n\t\t * handler, then things like COW could be properly\n\t\t * handled.\n\t\t */\n\t\tentry = pmd_clear_uffd_wp(entry);\n\t}\n\tret = HPAGE_PMD_NR;\n\tset_pmd_at(mm, addr, pmd, entry);\n\tBUG_ON(vma_is_anonymous(vma) && !preserve_write && pmd_write(entry));\nunlock:\n\tspin_unlock(ptl);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -32,6 +32,8 @@\n \t\t\tnewpmd = swp_entry_to_pmd(entry);\n \t\t\tif (pmd_swp_soft_dirty(*pmd))\n \t\t\t\tnewpmd = pmd_swp_mksoft_dirty(newpmd);\n+\t\t\tif (pmd_swp_uffd_wp(*pmd))\n+\t\t\t\tnewpmd = pmd_swp_mkuffd_wp(newpmd);\n \t\t\tset_pmd_at(mm, addr, pmd, newpmd);\n \t\t}\n \t\tgoto unlock;",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (pmd_swp_uffd_wp(*pmd))",
                "\t\t\t\tnewpmd = pmd_swp_mkuffd_wp(newpmd);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in pfn_swap_entry_to_page in memory management subsystem in the Linux Kernel. In this flaw, an attacker with a local user privilege may cause a denial of service problem due to a BUG statement referencing pmd_t x."
    },
    {
        "cve_id": "CVE-2023-4732",
        "code_before_change": "void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n{\n\tstruct vm_area_struct *vma = pvmw->vma;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address = pvmw->address;\n\tunsigned long mmun_start = address & HPAGE_PMD_MASK;\n\tpmd_t pmde;\n\tswp_entry_t entry;\n\n\tif (!(pvmw->pmd && !pvmw->pte))\n\t\treturn;\n\n\tentry = pmd_to_swp_entry(*pvmw->pmd);\n\tget_page(new);\n\tpmde = pmd_mkold(mk_huge_pmd(new, vma->vm_page_prot));\n\tif (pmd_swp_soft_dirty(*pvmw->pmd))\n\t\tpmde = pmd_mksoft_dirty(pmde);\n\tif (is_write_migration_entry(entry))\n\t\tpmde = maybe_pmd_mkwrite(pmde, vma);\n\n\tflush_cache_range(vma, mmun_start, mmun_start + HPAGE_PMD_SIZE);\n\tif (PageAnon(new))\n\t\tpage_add_anon_rmap(new, vma, mmun_start, true);\n\telse\n\t\tpage_add_file_rmap(new, true);\n\tset_pmd_at(mm, mmun_start, pvmw->pmd, pmde);\n\tif ((vma->vm_flags & VM_LOCKED) && !PageDoubleMap(new))\n\t\tmlock_vma_page(new);\n\tupdate_mmu_cache_pmd(vma, address, pvmw->pmd);\n}",
        "code_after_change": "void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n{\n\tstruct vm_area_struct *vma = pvmw->vma;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address = pvmw->address;\n\tunsigned long mmun_start = address & HPAGE_PMD_MASK;\n\tpmd_t pmde;\n\tswp_entry_t entry;\n\n\tif (!(pvmw->pmd && !pvmw->pte))\n\t\treturn;\n\n\tentry = pmd_to_swp_entry(*pvmw->pmd);\n\tget_page(new);\n\tpmde = pmd_mkold(mk_huge_pmd(new, vma->vm_page_prot));\n\tif (pmd_swp_soft_dirty(*pvmw->pmd))\n\t\tpmde = pmd_mksoft_dirty(pmde);\n\tif (is_write_migration_entry(entry))\n\t\tpmde = maybe_pmd_mkwrite(pmde, vma);\n\tif (pmd_swp_uffd_wp(*pvmw->pmd))\n\t\tpmde = pmd_wrprotect(pmd_mkuffd_wp(pmde));\n\n\tflush_cache_range(vma, mmun_start, mmun_start + HPAGE_PMD_SIZE);\n\tif (PageAnon(new))\n\t\tpage_add_anon_rmap(new, vma, mmun_start, true);\n\telse\n\t\tpage_add_file_rmap(new, true);\n\tset_pmd_at(mm, mmun_start, pvmw->pmd, pmde);\n\tif ((vma->vm_flags & VM_LOCKED) && !PageDoubleMap(new))\n\t\tmlock_vma_page(new);\n\tupdate_mmu_cache_pmd(vma, address, pvmw->pmd);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,8 @@\n \t\tpmde = pmd_mksoft_dirty(pmde);\n \tif (is_write_migration_entry(entry))\n \t\tpmde = maybe_pmd_mkwrite(pmde, vma);\n+\tif (pmd_swp_uffd_wp(*pvmw->pmd))\n+\t\tpmde = pmd_wrprotect(pmd_mkuffd_wp(pmde));\n \n \tflush_cache_range(vma, mmun_start, mmun_start + HPAGE_PMD_SIZE);\n \tif (PageAnon(new))",
        "function_modified_lines": {
            "added": [
                "\tif (pmd_swp_uffd_wp(*pvmw->pmd))",
                "\t\tpmde = pmd_wrprotect(pmd_mkuffd_wp(pmde));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in pfn_swap_entry_to_page in memory management subsystem in the Linux Kernel. In this flaw, an attacker with a local user privilege may cause a denial of service problem due to a BUG statement referencing pmd_t x."
    },
    {
        "cve_id": "CVE-2023-4732",
        "code_before_change": "static inline int\ncopy_present_pte(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t\t pte_t *dst_pte, pte_t *src_pte, unsigned long addr, int *rss,\n\t\t struct page **prealloc)\n{\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tunsigned long vm_flags = src_vma->vm_flags;\n\tpte_t pte = *src_pte;\n\tstruct page *page;\n\n\tpage = vm_normal_page(src_vma, addr, pte);\n\tif (page) {\n\t\tint retval;\n\n\t\tretval = copy_present_page(dst_vma, src_vma, dst_pte, src_pte,\n\t\t\t\t\t   addr, rss, prealloc, pte, page);\n\t\tif (retval <= 0)\n\t\t\treturn retval;\n\n\t\tget_page(page);\n\t\tpage_dup_rmap(page, false);\n\t\trss[mm_counter(page)]++;\n\t}\n\n\t/*\n\t * If it's a COW mapping, write protect it both\n\t * in the parent and the child\n\t */\n\tif (is_cow_mapping(vm_flags) && pte_write(pte)) {\n\t\tptep_set_wrprotect(src_mm, addr, src_pte);\n\t\tpte = pte_wrprotect(pte);\n\t}\n\n\t/*\n\t * If it's a shared mapping, mark it clean in\n\t * the child\n\t */\n\tif (vm_flags & VM_SHARED)\n\t\tpte = pte_mkclean(pte);\n\tpte = pte_mkold(pte);\n\n\t/*\n\t * Make sure the _PAGE_UFFD_WP bit is cleared if the new VMA\n\t * does not have the VM_UFFD_WP, which means that the uffd\n\t * fork event is not enabled.\n\t */\n\tif (!(vm_flags & VM_UFFD_WP))\n\t\tpte = pte_clear_uffd_wp(pte);\n\n\tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);\n\treturn 0;\n}",
        "code_after_change": "static inline int\ncopy_present_pte(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t\t pte_t *dst_pte, pte_t *src_pte, unsigned long addr, int *rss,\n\t\t struct page **prealloc)\n{\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tunsigned long vm_flags = src_vma->vm_flags;\n\tpte_t pte = *src_pte;\n\tstruct page *page;\n\n\tpage = vm_normal_page(src_vma, addr, pte);\n\tif (page) {\n\t\tint retval;\n\n\t\tretval = copy_present_page(dst_vma, src_vma, dst_pte, src_pte,\n\t\t\t\t\t   addr, rss, prealloc, pte, page);\n\t\tif (retval <= 0)\n\t\t\treturn retval;\n\n\t\tget_page(page);\n\t\tpage_dup_rmap(page, false);\n\t\trss[mm_counter(page)]++;\n\t}\n\n\t/*\n\t * If it's a COW mapping, write protect it both\n\t * in the parent and the child\n\t */\n\tif (is_cow_mapping(vm_flags) && pte_write(pte)) {\n\t\tptep_set_wrprotect(src_mm, addr, src_pte);\n\t\tpte = pte_wrprotect(pte);\n\t}\n\n\t/*\n\t * If it's a shared mapping, mark it clean in\n\t * the child\n\t */\n\tif (vm_flags & VM_SHARED)\n\t\tpte = pte_mkclean(pte);\n\tpte = pte_mkold(pte);\n\n\tif (!userfaultfd_wp(dst_vma))\n\t\tpte = pte_clear_uffd_wp(pte);\n\n\tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,12 +39,7 @@\n \t\tpte = pte_mkclean(pte);\n \tpte = pte_mkold(pte);\n \n-\t/*\n-\t * Make sure the _PAGE_UFFD_WP bit is cleared if the new VMA\n-\t * does not have the VM_UFFD_WP, which means that the uffd\n-\t * fork event is not enabled.\n-\t */\n-\tif (!(vm_flags & VM_UFFD_WP))\n+\tif (!userfaultfd_wp(dst_vma))\n \t\tpte = pte_clear_uffd_wp(pte);\n \n \tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);",
        "function_modified_lines": {
            "added": [
                "\tif (!userfaultfd_wp(dst_vma))"
            ],
            "deleted": [
                "\t/*",
                "\t * Make sure the _PAGE_UFFD_WP bit is cleared if the new VMA",
                "\t * does not have the VM_UFFD_WP, which means that the uffd",
                "\t * fork event is not enabled.",
                "\t */",
                "\tif (!(vm_flags & VM_UFFD_WP))"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in pfn_swap_entry_to_page in memory management subsystem in the Linux Kernel. In this flaw, an attacker with a local user privilege may cause a denial of service problem due to a BUG statement referencing pmd_t x."
    },
    {
        "cve_id": "CVE-2023-4732",
        "code_before_change": "static inline int\ncopy_pmd_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       pud_t *dst_pud, pud_t *src_pud, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tpmd_t *src_pmd, *dst_pmd;\n\tunsigned long next;\n\n\tdst_pmd = pmd_alloc(dst_mm, dst_pud, addr);\n\tif (!dst_pmd)\n\t\treturn -ENOMEM;\n\tsrc_pmd = pmd_offset(src_pud, addr);\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd)\n\t\t\t|| pmd_devmap(*src_pmd)) {\n\t\t\tint err;\n\t\t\tVM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, src_vma);\n\t\t\terr = copy_huge_pmd(dst_mm, src_mm,\n\t\t\t\t\t    dst_pmd, src_pmd, addr, src_vma);\n\t\t\tif (err == -ENOMEM)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (!err)\n\t\t\t\tcontinue;\n\t\t\t/* fall through */\n\t\t}\n\t\tif (pmd_none_or_clear_bad(src_pmd))\n\t\t\tcontinue;\n\t\tif (copy_pte_range(dst_vma, src_vma, dst_pmd, src_pmd,\n\t\t\t\t   addr, next))\n\t\t\treturn -ENOMEM;\n\t} while (dst_pmd++, src_pmd++, addr = next, addr != end);\n\treturn 0;\n}",
        "code_after_change": "static inline int\ncopy_pmd_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       pud_t *dst_pud, pud_t *src_pud, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tpmd_t *src_pmd, *dst_pmd;\n\tunsigned long next;\n\n\tdst_pmd = pmd_alloc(dst_mm, dst_pud, addr);\n\tif (!dst_pmd)\n\t\treturn -ENOMEM;\n\tsrc_pmd = pmd_offset(src_pud, addr);\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd)\n\t\t\t|| pmd_devmap(*src_pmd)) {\n\t\t\tint err;\n\t\t\tVM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, src_vma);\n\t\t\terr = copy_huge_pmd(dst_mm, src_mm, dst_pmd, src_pmd,\n\t\t\t\t\t    addr, dst_vma, src_vma);\n\t\t\tif (err == -ENOMEM)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (!err)\n\t\t\t\tcontinue;\n\t\t\t/* fall through */\n\t\t}\n\t\tif (pmd_none_or_clear_bad(src_pmd))\n\t\t\tcontinue;\n\t\tif (copy_pte_range(dst_vma, src_vma, dst_pmd, src_pmd,\n\t\t\t\t   addr, next))\n\t\t\treturn -ENOMEM;\n\t} while (dst_pmd++, src_pmd++, addr = next, addr != end);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,8 +18,8 @@\n \t\t\t|| pmd_devmap(*src_pmd)) {\n \t\t\tint err;\n \t\t\tVM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, src_vma);\n-\t\t\terr = copy_huge_pmd(dst_mm, src_mm,\n-\t\t\t\t\t    dst_pmd, src_pmd, addr, src_vma);\n+\t\t\terr = copy_huge_pmd(dst_mm, src_mm, dst_pmd, src_pmd,\n+\t\t\t\t\t    addr, dst_vma, src_vma);\n \t\t\tif (err == -ENOMEM)\n \t\t\t\treturn -ENOMEM;\n \t\t\tif (!err)",
        "function_modified_lines": {
            "added": [
                "\t\t\terr = copy_huge_pmd(dst_mm, src_mm, dst_pmd, src_pmd,",
                "\t\t\t\t\t    addr, dst_vma, src_vma);"
            ],
            "deleted": [
                "\t\t\terr = copy_huge_pmd(dst_mm, src_mm,",
                "\t\t\t\t\t    dst_pmd, src_pmd, addr, src_vma);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in pfn_swap_entry_to_page in memory management subsystem in the Linux Kernel. In this flaw, an attacker with a local user privilege may cause a denial of service problem due to a BUG statement referencing pmd_t x."
    },
    {
        "cve_id": "CVE-2023-4732",
        "code_before_change": "static int\ncopy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tpte_t *orig_src_pte, *orig_dst_pte;\n\tpte_t *src_pte, *dst_pte;\n\tspinlock_t *src_ptl, *dst_ptl;\n\tint progress, ret = 0;\n\tint rss[NR_MM_COUNTERS];\n\tswp_entry_t entry = (swp_entry_t){0};\n\tstruct page *prealloc = NULL;\n\nagain:\n\tprogress = 0;\n\tinit_rss_vec(rss);\n\n\tdst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);\n\tif (!dst_pte) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tsrc_pte = pte_offset_map(src_pmd, addr);\n\tsrc_ptl = pte_lockptr(src_mm, src_pmd);\n\tspin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);\n\torig_src_pte = src_pte;\n\torig_dst_pte = dst_pte;\n\tarch_enter_lazy_mmu_mode();\n\n\tdo {\n\t\t/*\n\t\t * We are holding two locks at this point - either of them\n\t\t * could generate latencies in another task on another CPU.\n\t\t */\n\t\tif (progress >= 32) {\n\t\t\tprogress = 0;\n\t\t\tif (need_resched() ||\n\t\t\t    spin_needbreak(src_ptl) || spin_needbreak(dst_ptl))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (pte_none(*src_pte)) {\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(!pte_present(*src_pte))) {\n\t\t\tentry.val = copy_nonpresent_pte(dst_mm, src_mm,\n\t\t\t\t\t\t\tdst_pte, src_pte,\n\t\t\t\t\t\t\tsrc_vma, addr, rss);\n\t\t\tif (entry.val)\n\t\t\t\tbreak;\n\t\t\tprogress += 8;\n\t\t\tcontinue;\n\t\t}\n\t\t/* copy_present_pte() will clear `*prealloc' if consumed */\n\t\tret = copy_present_pte(dst_vma, src_vma, dst_pte, src_pte,\n\t\t\t\t       addr, rss, &prealloc);\n\t\t/*\n\t\t * If we need a pre-allocated page for this pte, drop the\n\t\t * locks, allocate, and try again.\n\t\t */\n\t\tif (unlikely(ret == -EAGAIN))\n\t\t\tbreak;\n\t\tif (unlikely(prealloc)) {\n\t\t\t/*\n\t\t\t * pre-alloc page cannot be reused by next time so as\n\t\t\t * to strictly follow mempolicy (e.g., alloc_page_vma()\n\t\t\t * will allocate page according to address).  This\n\t\t\t * could only happen if one pinned pte changed.\n\t\t\t */\n\t\t\tput_page(prealloc);\n\t\t\tprealloc = NULL;\n\t\t}\n\t\tprogress += 8;\n\t} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);\n\n\tarch_leave_lazy_mmu_mode();\n\tspin_unlock(src_ptl);\n\tpte_unmap(orig_src_pte);\n\tadd_mm_rss_vec(dst_mm, rss);\n\tpte_unmap_unlock(orig_dst_pte, dst_ptl);\n\tcond_resched();\n\n\tif (entry.val) {\n\t\tif (add_swap_count_continuation(entry, GFP_KERNEL) < 0) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tentry.val = 0;\n\t} else if (ret) {\n\t\tWARN_ON_ONCE(ret != -EAGAIN);\n\t\tprealloc = page_copy_prealloc(src_mm, src_vma, addr);\n\t\tif (!prealloc)\n\t\t\treturn -ENOMEM;\n\t\t/* We've captured and resolved the error. Reset, try again. */\n\t\tret = 0;\n\t}\n\tif (addr != end)\n\t\tgoto again;\nout:\n\tif (unlikely(prealloc))\n\t\tput_page(prealloc);\n\treturn ret;\n}",
        "code_after_change": "static int\ncopy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tpte_t *orig_src_pte, *orig_dst_pte;\n\tpte_t *src_pte, *dst_pte;\n\tspinlock_t *src_ptl, *dst_ptl;\n\tint progress, ret = 0;\n\tint rss[NR_MM_COUNTERS];\n\tswp_entry_t entry = (swp_entry_t){0};\n\tstruct page *prealloc = NULL;\n\nagain:\n\tprogress = 0;\n\tinit_rss_vec(rss);\n\n\tdst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);\n\tif (!dst_pte) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tsrc_pte = pte_offset_map(src_pmd, addr);\n\tsrc_ptl = pte_lockptr(src_mm, src_pmd);\n\tspin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);\n\torig_src_pte = src_pte;\n\torig_dst_pte = dst_pte;\n\tarch_enter_lazy_mmu_mode();\n\n\tdo {\n\t\t/*\n\t\t * We are holding two locks at this point - either of them\n\t\t * could generate latencies in another task on another CPU.\n\t\t */\n\t\tif (progress >= 32) {\n\t\t\tprogress = 0;\n\t\t\tif (need_resched() ||\n\t\t\t    spin_needbreak(src_ptl) || spin_needbreak(dst_ptl))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (pte_none(*src_pte)) {\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(!pte_present(*src_pte))) {\n\t\t\tentry.val = copy_nonpresent_pte(dst_mm, src_mm,\n\t\t\t\t\t\t\tdst_pte, src_pte,\n\t\t\t\t\t\t\tdst_vma, src_vma,\n\t\t\t\t\t\t\taddr, rss);\n\t\t\tif (entry.val)\n\t\t\t\tbreak;\n\t\t\tprogress += 8;\n\t\t\tcontinue;\n\t\t}\n\t\t/* copy_present_pte() will clear `*prealloc' if consumed */\n\t\tret = copy_present_pte(dst_vma, src_vma, dst_pte, src_pte,\n\t\t\t\t       addr, rss, &prealloc);\n\t\t/*\n\t\t * If we need a pre-allocated page for this pte, drop the\n\t\t * locks, allocate, and try again.\n\t\t */\n\t\tif (unlikely(ret == -EAGAIN))\n\t\t\tbreak;\n\t\tif (unlikely(prealloc)) {\n\t\t\t/*\n\t\t\t * pre-alloc page cannot be reused by next time so as\n\t\t\t * to strictly follow mempolicy (e.g., alloc_page_vma()\n\t\t\t * will allocate page according to address).  This\n\t\t\t * could only happen if one pinned pte changed.\n\t\t\t */\n\t\t\tput_page(prealloc);\n\t\t\tprealloc = NULL;\n\t\t}\n\t\tprogress += 8;\n\t} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);\n\n\tarch_leave_lazy_mmu_mode();\n\tspin_unlock(src_ptl);\n\tpte_unmap(orig_src_pte);\n\tadd_mm_rss_vec(dst_mm, rss);\n\tpte_unmap_unlock(orig_dst_pte, dst_ptl);\n\tcond_resched();\n\n\tif (entry.val) {\n\t\tif (add_swap_count_continuation(entry, GFP_KERNEL) < 0) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tentry.val = 0;\n\t} else if (ret) {\n\t\tWARN_ON_ONCE(ret != -EAGAIN);\n\t\tprealloc = page_copy_prealloc(src_mm, src_vma, addr);\n\t\tif (!prealloc)\n\t\t\treturn -ENOMEM;\n\t\t/* We've captured and resolved the error. Reset, try again. */\n\t\tret = 0;\n\t}\n\tif (addr != end)\n\t\tgoto again;\nout:\n\tif (unlikely(prealloc))\n\t\tput_page(prealloc);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,7 +47,8 @@\n \t\tif (unlikely(!pte_present(*src_pte))) {\n \t\t\tentry.val = copy_nonpresent_pte(dst_mm, src_mm,\n \t\t\t\t\t\t\tdst_pte, src_pte,\n-\t\t\t\t\t\t\tsrc_vma, addr, rss);\n+\t\t\t\t\t\t\tdst_vma, src_vma,\n+\t\t\t\t\t\t\taddr, rss);\n \t\t\tif (entry.val)\n \t\t\t\tbreak;\n \t\t\tprogress += 8;",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\t\t\tdst_vma, src_vma,",
                "\t\t\t\t\t\t\taddr, rss);"
            ],
            "deleted": [
                "\t\t\t\t\t\t\tsrc_vma, addr, rss);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in pfn_swap_entry_to_page in memory management subsystem in the Linux Kernel. In this flaw, an attacker with a local user privilege may cause a denial of service problem due to a BUG statement referencing pmd_t x."
    },
    {
        "cve_id": "CVE-2023-4732",
        "code_before_change": "static inline int\ncopy_present_page(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t\t  pte_t *dst_pte, pte_t *src_pte, unsigned long addr, int *rss,\n\t\t  struct page **prealloc, pte_t pte, struct page *page)\n{\n\tstruct page *new_page;\n\n\t/*\n\t * What we want to do is to check whether this page may\n\t * have been pinned by the parent process.  If so,\n\t * instead of wrprotect the pte on both sides, we copy\n\t * the page immediately so that we'll always guarantee\n\t * the pinned page won't be randomly replaced in the\n\t * future.\n\t *\n\t * The page pinning checks are just \"has this mm ever\n\t * seen pinning\", along with the (inexact) check of\n\t * the page count. That might give false positives for\n\t * for pinning, but it will work correctly.\n\t */\n\tif (likely(!page_needs_cow_for_dma(src_vma, page)))\n\t\treturn 1;\n\n\tnew_page = *prealloc;\n\tif (!new_page)\n\t\treturn -EAGAIN;\n\n\t/*\n\t * We have a prealloc page, all good!  Take it\n\t * over and copy the page & arm it.\n\t */\n\t*prealloc = NULL;\n\tcopy_user_highpage(new_page, page, addr, src_vma);\n\t__SetPageUptodate(new_page);\n\tpage_add_new_anon_rmap(new_page, dst_vma, addr, false);\n\tlru_cache_add_inactive_or_unevictable(new_page, dst_vma);\n\trss[mm_counter(new_page)]++;\n\n\t/* All done, just insert the new page copy in the child */\n\tpte = mk_pte(new_page, dst_vma->vm_page_prot);\n\tpte = maybe_mkwrite(pte_mkdirty(pte), dst_vma);\n\tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);\n\treturn 0;\n}",
        "code_after_change": "static inline int\ncopy_present_page(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t\t  pte_t *dst_pte, pte_t *src_pte, unsigned long addr, int *rss,\n\t\t  struct page **prealloc, pte_t pte, struct page *page)\n{\n\tstruct page *new_page;\n\n\t/*\n\t * What we want to do is to check whether this page may\n\t * have been pinned by the parent process.  If so,\n\t * instead of wrprotect the pte on both sides, we copy\n\t * the page immediately so that we'll always guarantee\n\t * the pinned page won't be randomly replaced in the\n\t * future.\n\t *\n\t * The page pinning checks are just \"has this mm ever\n\t * seen pinning\", along with the (inexact) check of\n\t * the page count. That might give false positives for\n\t * for pinning, but it will work correctly.\n\t */\n\tif (likely(!page_needs_cow_for_dma(src_vma, page)))\n\t\treturn 1;\n\n\tnew_page = *prealloc;\n\tif (!new_page)\n\t\treturn -EAGAIN;\n\n\t/*\n\t * We have a prealloc page, all good!  Take it\n\t * over and copy the page & arm it.\n\t */\n\t*prealloc = NULL;\n\tcopy_user_highpage(new_page, page, addr, src_vma);\n\t__SetPageUptodate(new_page);\n\tpage_add_new_anon_rmap(new_page, dst_vma, addr, false);\n\tlru_cache_add_inactive_or_unevictable(new_page, dst_vma);\n\trss[mm_counter(new_page)]++;\n\n\t/* All done, just insert the new page copy in the child */\n\tpte = mk_pte(new_page, dst_vma->vm_page_prot);\n\tpte = maybe_mkwrite(pte_mkdirty(pte), dst_vma);\n\tif (userfaultfd_pte_wp(dst_vma, *src_pte))\n\t\t/* Uffd-wp needs to be delivered to dest pte as well */\n\t\tpte = pte_wrprotect(pte_mkuffd_wp(pte));\n\tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,6 +39,9 @@\n \t/* All done, just insert the new page copy in the child */\n \tpte = mk_pte(new_page, dst_vma->vm_page_prot);\n \tpte = maybe_mkwrite(pte_mkdirty(pte), dst_vma);\n+\tif (userfaultfd_pte_wp(dst_vma, *src_pte))\n+\t\t/* Uffd-wp needs to be delivered to dest pte as well */\n+\t\tpte = pte_wrprotect(pte_mkuffd_wp(pte));\n \tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (userfaultfd_pte_wp(dst_vma, *src_pte))",
                "\t\t/* Uffd-wp needs to be delivered to dest pte as well */",
                "\t\tpte = pte_wrprotect(pte_mkuffd_wp(pte));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in pfn_swap_entry_to_page in memory management subsystem in the Linux Kernel. In this flaw, an attacker with a local user privilege may cause a denial of service problem due to a BUG statement referencing pmd_t x."
    },
    {
        "cve_id": "CVE-2023-6546",
        "code_before_change": "static void gsm_cleanup_mux(struct gsm_mux *gsm, bool disc)\n{\n\tint i;\n\tstruct gsm_dlci *dlci = gsm->dlci[0];\n\tstruct gsm_msg *txq, *ntxq;\n\n\tgsm->dead = true;\n\tmutex_lock(&gsm->mutex);\n\n\tif (dlci) {\n\t\tif (disc && dlci->state != DLCI_CLOSED) {\n\t\t\tgsm_dlci_begin_close(dlci);\n\t\t\twait_event(gsm->event, dlci->state == DLCI_CLOSED);\n\t\t}\n\t\tdlci->dead = true;\n\t}\n\n\t/* Finish outstanding timers, making sure they are done */\n\tdel_timer_sync(&gsm->kick_timer);\n\tdel_timer_sync(&gsm->t2_timer);\n\tdel_timer_sync(&gsm->ka_timer);\n\n\t/* Finish writing to ldisc */\n\tflush_work(&gsm->tx_work);\n\n\t/* Free up any link layer users and finally the control channel */\n\tif (gsm->has_devices) {\n\t\tgsm_unregister_devices(gsm_tty_driver, gsm->num);\n\t\tgsm->has_devices = false;\n\t}\n\tfor (i = NUM_DLCI - 1; i >= 0; i--)\n\t\tif (gsm->dlci[i]) {\n\t\t\tgsm_dlci_release(gsm->dlci[i]);\n\t\t\tgsm->dlci[i] = NULL;\n\t\t}\n\tmutex_unlock(&gsm->mutex);\n\t/* Now wipe the queues */\n\ttty_ldisc_flush(gsm->tty);\n\tlist_for_each_entry_safe(txq, ntxq, &gsm->tx_ctrl_list, list)\n\t\tkfree(txq);\n\tINIT_LIST_HEAD(&gsm->tx_ctrl_list);\n\tlist_for_each_entry_safe(txq, ntxq, &gsm->tx_data_list, list)\n\t\tkfree(txq);\n\tINIT_LIST_HEAD(&gsm->tx_data_list);\n}",
        "code_after_change": "static void gsm_cleanup_mux(struct gsm_mux *gsm, bool disc)\n{\n\tint i;\n\tstruct gsm_dlci *dlci;\n\tstruct gsm_msg *txq, *ntxq;\n\n\tgsm->dead = true;\n\tmutex_lock(&gsm->mutex);\n\n\tdlci = gsm->dlci[0];\n\tif (dlci) {\n\t\tif (disc && dlci->state != DLCI_CLOSED) {\n\t\t\tgsm_dlci_begin_close(dlci);\n\t\t\twait_event(gsm->event, dlci->state == DLCI_CLOSED);\n\t\t}\n\t\tdlci->dead = true;\n\t}\n\n\t/* Finish outstanding timers, making sure they are done */\n\tdel_timer_sync(&gsm->kick_timer);\n\tdel_timer_sync(&gsm->t2_timer);\n\tdel_timer_sync(&gsm->ka_timer);\n\n\t/* Finish writing to ldisc */\n\tflush_work(&gsm->tx_work);\n\n\t/* Free up any link layer users and finally the control channel */\n\tif (gsm->has_devices) {\n\t\tgsm_unregister_devices(gsm_tty_driver, gsm->num);\n\t\tgsm->has_devices = false;\n\t}\n\tfor (i = NUM_DLCI - 1; i >= 0; i--)\n\t\tif (gsm->dlci[i]) {\n\t\t\tgsm_dlci_release(gsm->dlci[i]);\n\t\t\tgsm->dlci[i] = NULL;\n\t\t}\n\tmutex_unlock(&gsm->mutex);\n\t/* Now wipe the queues */\n\ttty_ldisc_flush(gsm->tty);\n\tlist_for_each_entry_safe(txq, ntxq, &gsm->tx_ctrl_list, list)\n\t\tkfree(txq);\n\tINIT_LIST_HEAD(&gsm->tx_ctrl_list);\n\tlist_for_each_entry_safe(txq, ntxq, &gsm->tx_data_list, list)\n\t\tkfree(txq);\n\tINIT_LIST_HEAD(&gsm->tx_data_list);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,13 @@\n static void gsm_cleanup_mux(struct gsm_mux *gsm, bool disc)\n {\n \tint i;\n-\tstruct gsm_dlci *dlci = gsm->dlci[0];\n+\tstruct gsm_dlci *dlci;\n \tstruct gsm_msg *txq, *ntxq;\n \n \tgsm->dead = true;\n \tmutex_lock(&gsm->mutex);\n \n+\tdlci = gsm->dlci[0];\n \tif (dlci) {\n \t\tif (disc && dlci->state != DLCI_CLOSED) {\n \t\t\tgsm_dlci_begin_close(dlci);",
        "function_modified_lines": {
            "added": [
                "\tstruct gsm_dlci *dlci;",
                "\tdlci = gsm->dlci[0];"
            ],
            "deleted": [
                "\tstruct gsm_dlci *dlci = gsm->dlci[0];"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the GSM 0710 tty multiplexor in the Linux kernel. This issue occurs when two threads execute the GSMIOC_SETCONF ioctl on the same tty file descriptor with the gsm line discipline enabled, and can lead to a use-after-free problem on a struct gsm_dlci while restarting the gsm mux. This could allow a local unprivileged user to escalate their privileges on the system."
    }
]