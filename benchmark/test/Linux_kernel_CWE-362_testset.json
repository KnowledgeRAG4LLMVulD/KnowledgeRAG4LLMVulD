[
    {
        "cve_id": "CVE-2017-17712",
        "code_before_change": "static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\tstruct ip_options_data opt_copy;\n\tstruct raw_frag_vec rfv;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tpr_info_once(\"%s: %s forgot to set AF_INET. Fix it!\\n\",\n\t\t\t\t     __func__, current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.sockc.tsflags = sk->sk_tsflags;\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.ttl = 0;\n\tipc.tos = -1;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sk, msg, &ipc, false);\n\t\tif (unlikely(err)) {\n\t\t\tkfree(ipc.opt);\n\t\t\tgoto out;\n\t\t}\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (inet->hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->opt.srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->opt.faddr;\n\t\t}\n\t}\n\ttos = get_rtconn_flags(&ipc, sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t} else if (!ipc.oif)\n\t\tipc.oif = inet->uc_index;\n\n\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t   inet_sk_flowi_flags(sk) |\n\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n\t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n\n\tif (!inet->hdrincl) {\n\t\trfv.msg = msg;\n\t\trfv.hlen = 0;\n\n\t\terr = raw_probe_proto_opt(&rfv, &fl4);\n\t\tif (err)\n\t\t\tgoto done;\n\t}\n\n\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto done;\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (inet->hdrincl)\n\t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n\t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n\n\t else {\n\t\tsock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);\n\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = fl4.daddr;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, &fl4, raw_getfrag,\n\t\t\t\t     &rfv, len, 0,\n\t\t\t\t     &ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk, &fl4);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tif (msg->msg_flags & MSG_PROBE)\n\t\tdst_confirm_neigh(&rt->dst, &fl4.daddr);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "code_after_change": "static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\tstruct ip_options_data opt_copy;\n\tstruct raw_frag_vec rfv;\n\tint hdrincl;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/* hdrincl should be READ_ONCE(inet->hdrincl)\n\t * but READ_ONCE() doesn't work with bit fields\n\t */\n\thdrincl = inet->hdrincl;\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tpr_info_once(\"%s: %s forgot to set AF_INET. Fix it!\\n\",\n\t\t\t\t     __func__, current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.sockc.tsflags = sk->sk_tsflags;\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.ttl = 0;\n\tipc.tos = -1;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sk, msg, &ipc, false);\n\t\tif (unlikely(err)) {\n\t\t\tkfree(ipc.opt);\n\t\t\tgoto out;\n\t\t}\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->opt.srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->opt.faddr;\n\t\t}\n\t}\n\ttos = get_rtconn_flags(&ipc, sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t} else if (!ipc.oif)\n\t\tipc.oif = inet->uc_index;\n\n\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t   inet_sk_flowi_flags(sk) |\n\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n\t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n\n\tif (!hdrincl) {\n\t\trfv.msg = msg;\n\t\trfv.hlen = 0;\n\n\t\terr = raw_probe_proto_opt(&rfv, &fl4);\n\t\tif (err)\n\t\t\tgoto done;\n\t}\n\n\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto done;\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (hdrincl)\n\t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n\t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n\n\t else {\n\t\tsock_tx_timestamp(sk, ipc.sockc.tsflags, &ipc.tx_flags);\n\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = fl4.daddr;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, &fl4, raw_getfrag,\n\t\t\t\t     &rfv, len, 0,\n\t\t\t\t     &ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk, &fl4);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tif (msg->msg_flags & MSG_PROBE)\n\t\tdst_confirm_neigh(&rt->dst, &fl4.daddr);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,11 +12,16 @@\n \tint err;\n \tstruct ip_options_data opt_copy;\n \tstruct raw_frag_vec rfv;\n+\tint hdrincl;\n \n \terr = -EMSGSIZE;\n \tif (len > 0xFFFF)\n \t\tgoto out;\n \n+\t/* hdrincl should be READ_ONCE(inet->hdrincl)\n+\t * but READ_ONCE() doesn't work with bit fields\n+\t */\n+\thdrincl = inet->hdrincl;\n \t/*\n \t *\tCheck the flags.\n \t */\n@@ -92,7 +97,7 @@\n \t\t/* Linux does not mangle headers on raw sockets,\n \t\t * so that IP options + IP_HDRINCL is non-sense.\n \t\t */\n-\t\tif (inet->hdrincl)\n+\t\tif (hdrincl)\n \t\t\tgoto done;\n \t\tif (ipc.opt->opt.srr) {\n \t\t\tif (!daddr)\n@@ -114,12 +119,12 @@\n \n \tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n \t\t\t   RT_SCOPE_UNIVERSE,\n-\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n+\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n \t\t\t   inet_sk_flowi_flags(sk) |\n-\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n+\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),\n \t\t\t   daddr, saddr, 0, 0, sk->sk_uid);\n \n-\tif (!inet->hdrincl) {\n+\tif (!hdrincl) {\n \t\trfv.msg = msg;\n \t\trfv.hlen = 0;\n \n@@ -144,7 +149,7 @@\n \t\tgoto do_confirm;\n back_from_confirm:\n \n-\tif (inet->hdrincl)\n+\tif (hdrincl)\n \t\terr = raw_send_hdrinc(sk, &fl4, msg, len,\n \t\t\t\t      &rt, msg->msg_flags, &ipc.sockc);\n ",
        "function_modified_lines": {
            "added": [
                "\tint hdrincl;",
                "\t/* hdrincl should be READ_ONCE(inet->hdrincl)",
                "\t * but READ_ONCE() doesn't work with bit fields",
                "\t */",
                "\thdrincl = inet->hdrincl;",
                "\t\tif (hdrincl)",
                "\t\t\t   hdrincl ? IPPROTO_RAW : sk->sk_protocol,",
                "\t\t\t    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),",
                "\tif (!hdrincl) {",
                "\tif (hdrincl)"
            ],
            "deleted": [
                "\t\tif (inet->hdrincl)",
                "\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,",
                "\t\t\t    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),",
                "\tif (!inet->hdrincl) {",
                "\tif (inet->hdrincl)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The raw_sendmsg() function in net/ipv4/raw.c in the Linux kernel through 4.14.6 has a race condition in inet->hdrincl that leads to uninitialized stack pointer usage; this allows a local user to execute code and gain privileges.",
        "id": 1366
    },
    {
        "cve_id": "CVE-2015-8963",
        "code_before_change": "static int swevent_hlist_get_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\tint err = 0;\n\n\tmutex_lock(&swhash->hlist_mutex);\n\n\tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc(sizeof(*hlist), GFP_KERNEL);\n\t\tif (!hlist) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tswhash->hlist_refcount++;\nexit:\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\treturn err;\n}",
        "code_after_change": "static int swevent_hlist_get_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\tint err = 0;\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc(sizeof(*hlist), GFP_KERNEL);\n\t\tif (!hlist) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tswhash->hlist_refcount++;\nexit:\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \tint err = 0;\n \n \tmutex_lock(&swhash->hlist_mutex);\n-\n \tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n \t\tstruct swevent_hlist *hlist;\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation.",
        "id": 873
    },
    {
        "cve_id": "CVE-2015-8963",
        "code_before_change": "static int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\n\thwc->state = !(flags & PERF_EF_START);\n\n\thead = find_swevent_head(swhash, event);\n\tif (!head) {\n\t\t/*\n\t\t * We can race with cpu hotplug code. Do not\n\t\t * WARN if the cpu just got unplugged.\n\t\t */\n\t\tWARN_ON_ONCE(swhash->online);\n\t\treturn -EINVAL;\n\t}\n\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}",
        "code_after_change": "static int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\n\thwc->state = !(flags & PERF_EF_START);\n\n\thead = find_swevent_head(swhash, event);\n\tif (WARN_ON_ONCE(!head))\n\t\treturn -EINVAL;\n\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,14 +12,8 @@\n \thwc->state = !(flags & PERF_EF_START);\n \n \thead = find_swevent_head(swhash, event);\n-\tif (!head) {\n-\t\t/*\n-\t\t * We can race with cpu hotplug code. Do not\n-\t\t * WARN if the cpu just got unplugged.\n-\t\t */\n-\t\tWARN_ON_ONCE(swhash->online);\n+\tif (WARN_ON_ONCE(!head))\n \t\treturn -EINVAL;\n-\t}\n \n \thlist_add_head_rcu(&event->hlist_entry, head);\n \tperf_event_update_userpage(event);",
        "function_modified_lines": {
            "added": [
                "\tif (WARN_ON_ONCE(!head))"
            ],
            "deleted": [
                "\tif (!head) {",
                "\t\t/*",
                "\t\t * We can race with cpu hotplug code. Do not",
                "\t\t * WARN if the cpu just got unplugged.",
                "\t\t */",
                "\t\tWARN_ON_ONCE(swhash->online);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation.",
        "id": 871
    },
    {
        "cve_id": "CVE-2012-4508",
        "code_before_change": "static int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\ttrace_ext4_ext_handle_uninitialized_extents(inode, map, allocated,\n\t\t\t\t\t\t    newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\tret = ext4_split_unwritten_extents(handle, inode, map,\n\t\t\t\t\t\t   path, flags);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, path);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\tmap->m_lblk, map->m_len);\n\t\tif (reserved_clusters)\n\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\t     reserved_clusters,\n\t\t\t\t\t\t     0);\n\t}\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}",
        "code_after_change": "static int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\ttrace_ext4_ext_handle_uninitialized_extents(inode, map, allocated,\n\t\t\t\t\t\t    newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\tret = ext4_split_unwritten_extents(handle, inode, map,\n\t\t\t\t\t\t   path, flags);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, path);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\tmap->m_lblk, map->m_len);\n\t\tif (reserved_clusters)\n\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\t     reserved_clusters,\n\t\t\t\t\t\t     0);\n\t}\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,7 +38,7 @@\n \t}\n \t/* IO end_io complete, convert the filled extent to written */\n \tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n-\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,\n+\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,\n \t\t\t\t\t\t\tpath);\n \t\tif (ret >= 0) {\n \t\t\text4_update_inode_fsync_trans(handle, inode, 1);",
        "function_modified_lines": {
            "added": [
                "\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,"
            ],
            "deleted": [
                "\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized.",
        "id": 108
    },
    {
        "cve_id": "CVE-2020-12114",
        "code_before_change": "static void mntput_no_expire(struct mount *mnt)\n{\n\trcu_read_lock();\n\tif (likely(READ_ONCE(mnt->mnt_ns))) {\n\t\t/*\n\t\t * Since we don't do lock_mount_hash() here,\n\t\t * ->mnt_ns can change under us.  However, if it's\n\t\t * non-NULL, then there's a reference that won't\n\t\t * be dropped until after an RCU delay done after\n\t\t * turning ->mnt_ns NULL.  So if we observe it\n\t\t * non-NULL under rcu_read_lock(), the reference\n\t\t * we are dropping is not the final one.\n\t\t */\n\t\tmnt_add_count(mnt, -1);\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\t/*\n\t * make sure that if __legitimize_mnt() has not seen us grab\n\t * mount_lock, we'll see their refcount increment here.\n\t */\n\tsmp_mb();\n\tmnt_add_count(mnt, -1);\n\tif (mnt_get_count(mnt)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\n\tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n\t\tstruct mount *p, *tmp;\n\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\tumount_mnt(p);\n\t\t}\n\t}\n\tunlock_mount_hash();\n\n\tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n\t\tstruct task_struct *task = current;\n\t\tif (likely(!(task->flags & PF_KTHREAD))) {\n\t\t\tinit_task_work(&mnt->mnt_rcu, __cleanup_mnt);\n\t\t\tif (!task_work_add(task, &mnt->mnt_rcu, true))\n\t\t\t\treturn;\n\t\t}\n\t\tif (llist_add(&mnt->mnt_llist, &delayed_mntput_list))\n\t\t\tschedule_delayed_work(&delayed_mntput_work, 1);\n\t\treturn;\n\t}\n\tcleanup_mnt(mnt);\n}",
        "code_after_change": "static void mntput_no_expire(struct mount *mnt)\n{\n\tLIST_HEAD(list);\n\n\trcu_read_lock();\n\tif (likely(READ_ONCE(mnt->mnt_ns))) {\n\t\t/*\n\t\t * Since we don't do lock_mount_hash() here,\n\t\t * ->mnt_ns can change under us.  However, if it's\n\t\t * non-NULL, then there's a reference that won't\n\t\t * be dropped until after an RCU delay done after\n\t\t * turning ->mnt_ns NULL.  So if we observe it\n\t\t * non-NULL under rcu_read_lock(), the reference\n\t\t * we are dropping is not the final one.\n\t\t */\n\t\tmnt_add_count(mnt, -1);\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\t/*\n\t * make sure that if __legitimize_mnt() has not seen us grab\n\t * mount_lock, we'll see their refcount increment here.\n\t */\n\tsmp_mb();\n\tmnt_add_count(mnt, -1);\n\tif (mnt_get_count(mnt)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\n\tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n\t\tstruct mount *p, *tmp;\n\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\t__put_mountpoint(unhash_mnt(p), &list);\n\t\t}\n\t}\n\tunlock_mount_hash();\n\tshrink_dentry_list(&list);\n\n\tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n\t\tstruct task_struct *task = current;\n\t\tif (likely(!(task->flags & PF_KTHREAD))) {\n\t\t\tinit_task_work(&mnt->mnt_rcu, __cleanup_mnt);\n\t\t\tif (!task_work_add(task, &mnt->mnt_rcu, true))\n\t\t\t\treturn;\n\t\t}\n\t\tif (llist_add(&mnt->mnt_llist, &delayed_mntput_list))\n\t\t\tschedule_delayed_work(&delayed_mntput_work, 1);\n\t\treturn;\n\t}\n\tcleanup_mnt(mnt);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,7 @@\n static void mntput_no_expire(struct mount *mnt)\n {\n+\tLIST_HEAD(list);\n+\n \trcu_read_lock();\n \tif (likely(READ_ONCE(mnt->mnt_ns))) {\n \t\t/*\n@@ -40,10 +42,11 @@\n \tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n \t\tstruct mount *p, *tmp;\n \t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n-\t\t\tumount_mnt(p);\n+\t\t\t__put_mountpoint(unhash_mnt(p), &list);\n \t\t}\n \t}\n \tunlock_mount_hash();\n+\tshrink_dentry_list(&list);\n \n \tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n \t\tstruct task_struct *task = current;",
        "function_modified_lines": {
            "added": [
                "\tLIST_HEAD(list);",
                "",
                "\t\t\t__put_mountpoint(unhash_mnt(p), &list);",
                "\tshrink_dentry_list(&list);"
            ],
            "deleted": [
                "\t\t\tumount_mnt(p);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A pivot_root race condition in fs/namespace.c in the Linux kernel 4.4.x before 4.4.221, 4.9.x before 4.9.221, 4.14.x before 4.14.178, 4.19.x before 4.19.119, and 5.x before 5.3 allows local users to cause a denial of service (panic) by corrupting a mountpoint reference counter.",
        "id": 2442
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static struct rtable *icmp_route_lookup(struct net *net, struct sk_buff *skb_in,\n\t\t\t\t\tconst struct iphdr *iph,\n\t\t\t\t\t__be32 saddr, u8 tos,\n\t\t\t\t\tint type, int code,\n\t\t\t\t\tstruct icmp_bxm *param)\n{\n\tstruct flowi4 fl4 = {\n\t\t.daddr = (param->replyopts.srr ?\n\t\t\t  param->replyopts.faddr : iph->saddr),\n\t\t.saddr = saddr,\n\t\t.flowi4_tos = RT_TOS(tos),\n\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t.fl4_icmp_type = type,\n\t\t.fl4_icmp_code = code,\n\t};\n\tstruct rtable *rt, *rt2;\n\tint err;\n\n\tsecurity_skb_classify_flow(skb_in, flowi4_to_flowi(&fl4));\n\trt = __ip_route_output_key(net, &fl4);\n\tif (IS_ERR(rt))\n\t\treturn rt;\n\n\t/* No need to clone since we're just using its address. */\n\trt2 = rt;\n\n\tif (!fl4.saddr)\n\t\tfl4.saddr = rt->rt_src;\n\n\trt = (struct rtable *) xfrm_lookup(net, &rt->dst,\n\t\t\t\t\t   flowi4_to_flowi(&fl4), NULL, 0);\n\tif (!IS_ERR(rt)) {\n\t\tif (rt != rt2)\n\t\t\treturn rt;\n\t} else if (PTR_ERR(rt) == -EPERM) {\n\t\trt = NULL;\n\t} else\n\t\treturn rt;\n\n\terr = xfrm_decode_session_reverse(skb_in, flowi4_to_flowi(&fl4), AF_INET);\n\tif (err)\n\t\tgoto relookup_failed;\n\n\tif (inet_addr_type(net, fl4.saddr) == RTN_LOCAL) {\n\t\trt2 = __ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt2))\n\t\t\terr = PTR_ERR(rt2);\n\t} else {\n\t\tstruct flowi4 fl4_2 = {};\n\t\tunsigned long orefdst;\n\n\t\tfl4_2.daddr = fl4.saddr;\n\t\trt2 = ip_route_output_key(net, &fl4_2);\n\t\tif (IS_ERR(rt2)) {\n\t\t\terr = PTR_ERR(rt2);\n\t\t\tgoto relookup_failed;\n\t\t}\n\t\t/* Ugh! */\n\t\torefdst = skb_in->_skb_refdst; /* save old refdst */\n\t\terr = ip_route_input(skb_in, fl4.daddr, fl4.saddr,\n\t\t\t\t     RT_TOS(tos), rt2->dst.dev);\n\n\t\tdst_release(&rt2->dst);\n\t\trt2 = skb_rtable(skb_in);\n\t\tskb_in->_skb_refdst = orefdst; /* restore old refdst */\n\t}\n\n\tif (err)\n\t\tgoto relookup_failed;\n\n\trt2 = (struct rtable *) xfrm_lookup(net, &rt2->dst,\n\t\t\t\t\t    flowi4_to_flowi(&fl4), NULL,\n\t\t\t\t\t    XFRM_LOOKUP_ICMP);\n\tif (!IS_ERR(rt2)) {\n\t\tdst_release(&rt->dst);\n\t\trt = rt2;\n\t} else if (PTR_ERR(rt2) == -EPERM) {\n\t\tif (rt)\n\t\t\tdst_release(&rt->dst);\n\t\treturn rt2;\n\t} else {\n\t\terr = PTR_ERR(rt2);\n\t\tgoto relookup_failed;\n\t}\n\treturn rt;\n\nrelookup_failed:\n\tif (rt)\n\t\treturn rt;\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "static struct rtable *icmp_route_lookup(struct net *net, struct sk_buff *skb_in,\n\t\t\t\t\tconst struct iphdr *iph,\n\t\t\t\t\t__be32 saddr, u8 tos,\n\t\t\t\t\tint type, int code,\n\t\t\t\t\tstruct icmp_bxm *param)\n{\n\tstruct flowi4 fl4 = {\n\t\t.daddr = (param->replyopts.opt.opt.srr ?\n\t\t\t  param->replyopts.opt.opt.faddr : iph->saddr),\n\t\t.saddr = saddr,\n\t\t.flowi4_tos = RT_TOS(tos),\n\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t.fl4_icmp_type = type,\n\t\t.fl4_icmp_code = code,\n\t};\n\tstruct rtable *rt, *rt2;\n\tint err;\n\n\tsecurity_skb_classify_flow(skb_in, flowi4_to_flowi(&fl4));\n\trt = __ip_route_output_key(net, &fl4);\n\tif (IS_ERR(rt))\n\t\treturn rt;\n\n\t/* No need to clone since we're just using its address. */\n\trt2 = rt;\n\n\tif (!fl4.saddr)\n\t\tfl4.saddr = rt->rt_src;\n\n\trt = (struct rtable *) xfrm_lookup(net, &rt->dst,\n\t\t\t\t\t   flowi4_to_flowi(&fl4), NULL, 0);\n\tif (!IS_ERR(rt)) {\n\t\tif (rt != rt2)\n\t\t\treturn rt;\n\t} else if (PTR_ERR(rt) == -EPERM) {\n\t\trt = NULL;\n\t} else\n\t\treturn rt;\n\n\terr = xfrm_decode_session_reverse(skb_in, flowi4_to_flowi(&fl4), AF_INET);\n\tif (err)\n\t\tgoto relookup_failed;\n\n\tif (inet_addr_type(net, fl4.saddr) == RTN_LOCAL) {\n\t\trt2 = __ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt2))\n\t\t\terr = PTR_ERR(rt2);\n\t} else {\n\t\tstruct flowi4 fl4_2 = {};\n\t\tunsigned long orefdst;\n\n\t\tfl4_2.daddr = fl4.saddr;\n\t\trt2 = ip_route_output_key(net, &fl4_2);\n\t\tif (IS_ERR(rt2)) {\n\t\t\terr = PTR_ERR(rt2);\n\t\t\tgoto relookup_failed;\n\t\t}\n\t\t/* Ugh! */\n\t\torefdst = skb_in->_skb_refdst; /* save old refdst */\n\t\terr = ip_route_input(skb_in, fl4.daddr, fl4.saddr,\n\t\t\t\t     RT_TOS(tos), rt2->dst.dev);\n\n\t\tdst_release(&rt2->dst);\n\t\trt2 = skb_rtable(skb_in);\n\t\tskb_in->_skb_refdst = orefdst; /* restore old refdst */\n\t}\n\n\tif (err)\n\t\tgoto relookup_failed;\n\n\trt2 = (struct rtable *) xfrm_lookup(net, &rt2->dst,\n\t\t\t\t\t    flowi4_to_flowi(&fl4), NULL,\n\t\t\t\t\t    XFRM_LOOKUP_ICMP);\n\tif (!IS_ERR(rt2)) {\n\t\tdst_release(&rt->dst);\n\t\trt = rt2;\n\t} else if (PTR_ERR(rt2) == -EPERM) {\n\t\tif (rt)\n\t\t\tdst_release(&rt->dst);\n\t\treturn rt2;\n\t} else {\n\t\terr = PTR_ERR(rt2);\n\t\tgoto relookup_failed;\n\t}\n\treturn rt;\n\nrelookup_failed:\n\tif (rt)\n\t\treturn rt;\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,8 @@\n \t\t\t\t\tstruct icmp_bxm *param)\n {\n \tstruct flowi4 fl4 = {\n-\t\t.daddr = (param->replyopts.srr ?\n-\t\t\t  param->replyopts.faddr : iph->saddr),\n+\t\t.daddr = (param->replyopts.opt.opt.srr ?\n+\t\t\t  param->replyopts.opt.opt.faddr : iph->saddr),\n \t\t.saddr = saddr,\n \t\t.flowi4_tos = RT_TOS(tos),\n \t\t.flowi4_proto = IPPROTO_ICMP,",
        "function_modified_lines": {
            "added": [
                "\t\t.daddr = (param->replyopts.opt.opt.srr ?",
                "\t\t\t  param->replyopts.opt.opt.faddr : iph->saddr),"
            ],
            "deleted": [
                "\t\t.daddr = (param->replyopts.srr ?",
                "\t\t\t  param->replyopts.faddr : iph->saddr),"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic.",
        "id": 77
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static int raw_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tstruct sockaddr_in *usin = (struct sockaddr_in *)msg->msg_name;\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tstatic int complained;\n\t\t\tif (!complained++)\n\t\t\t\tprintk(KERN_INFO \"%s forgot to set AF_INET in \"\n\t\t\t\t\t\t \"raw sendmsg. Fix it!\\n\",\n\t\t\t\t\t\t current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt)\n\t\tipc.opt = inet->opt;\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (inet->hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->faddr;\n\t\t}\n\t}\n\ttos = RT_CONN_FLAGS(sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t}\n\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t\t   FLOWI_FLAG_CAN_SLEEP, daddr, saddr, 0, 0);\n\n\t\tif (!inet->hdrincl) {\n\t\t\terr = raw_probe_proto_opt(&fl4, msg);\n\t\t\tif (err)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_flow(sock_net(sk), &fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (inet->hdrincl)\n\t\terr = raw_send_hdrinc(sk, msg->msg_iov, len,\n\t\t\t\t\t&rt, msg->msg_flags);\n\n\t else {\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = rt->rt_dst;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, ip_generic_getfrag, msg->msg_iov, len, 0,\n\t\t\t\t\t&ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "code_after_change": "static int raw_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\tstruct ip_options_data opt_copy;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tstruct sockaddr_in *usin = (struct sockaddr_in *)msg->msg_name;\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tstatic int complained;\n\t\t\tif (!complained++)\n\t\t\t\tprintk(KERN_INFO \"%s forgot to set AF_INET in \"\n\t\t\t\t\t\t \"raw sendmsg. Fix it!\\n\",\n\t\t\t\t\t\t current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (inet->hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->opt.srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->opt.faddr;\n\t\t}\n\t}\n\ttos = RT_CONN_FLAGS(sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t}\n\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t\t   FLOWI_FLAG_CAN_SLEEP, daddr, saddr, 0, 0);\n\n\t\tif (!inet->hdrincl) {\n\t\t\terr = raw_probe_proto_opt(&fl4, msg);\n\t\t\tif (err)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_flow(sock_net(sk), &fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (inet->hdrincl)\n\t\terr = raw_send_hdrinc(sk, msg->msg_iov, len,\n\t\t\t\t\t&rt, msg->msg_flags);\n\n\t else {\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = rt->rt_dst;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, ip_generic_getfrag, msg->msg_iov, len, 0,\n\t\t\t\t\t&ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,7 @@\n \t__be32 saddr;\n \tu8  tos;\n \tint err;\n+\tstruct ip_options_data opt_copy;\n \n \terr = -EMSGSIZE;\n \tif (len > 0xFFFF)\n@@ -69,8 +70,18 @@\n \tsaddr = ipc.addr;\n \tipc.addr = daddr;\n \n-\tif (!ipc.opt)\n-\t\tipc.opt = inet->opt;\n+\tif (!ipc.opt) {\n+\t\tstruct ip_options_rcu *inet_opt;\n+\n+\t\trcu_read_lock();\n+\t\tinet_opt = rcu_dereference(inet->inet_opt);\n+\t\tif (inet_opt) {\n+\t\t\tmemcpy(&opt_copy, inet_opt,\n+\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n+\t\t\tipc.opt = &opt_copy.opt;\n+\t\t}\n+\t\trcu_read_unlock();\n+\t}\n \n \tif (ipc.opt) {\n \t\terr = -EINVAL;\n@@ -79,10 +90,10 @@\n \t\t */\n \t\tif (inet->hdrincl)\n \t\t\tgoto done;\n-\t\tif (ipc.opt->srr) {\n+\t\tif (ipc.opt->opt.srr) {\n \t\t\tif (!daddr)\n \t\t\t\tgoto done;\n-\t\t\tdaddr = ipc.opt->faddr;\n+\t\t\tdaddr = ipc.opt->opt.faddr;\n \t\t}\n \t}\n \ttos = RT_CONN_FLAGS(sk);",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_data opt_copy;",
                "\tif (!ipc.opt) {",
                "\t\tstruct ip_options_rcu *inet_opt;",
                "",
                "\t\trcu_read_lock();",
                "\t\tinet_opt = rcu_dereference(inet->inet_opt);",
                "\t\tif (inet_opt) {",
                "\t\t\tmemcpy(&opt_copy, inet_opt,",
                "\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);",
                "\t\t\tipc.opt = &opt_copy.opt;",
                "\t\t}",
                "\t\trcu_read_unlock();",
                "\t}",
                "\t\tif (ipc.opt->opt.srr) {",
                "\t\t\tdaddr = ipc.opt->opt.faddr;"
            ],
            "deleted": [
                "\tif (!ipc.opt)",
                "\t\tipc.opt = inet->opt;",
                "\t\tif (ipc.opt->srr) {",
                "\t\t\tdaddr = ipc.opt->faddr;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic.",
        "id": 87
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\tif (inet->opt && inet->opt->srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet->opt->faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_TCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\tif (err == -ENETUNREACH)\n\t\t\tIP_INC_STATS_BH(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\t\treturn err;\n\t}\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (!inet->opt || !inet->opt->srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (!inet->inet_saddr)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tif (tp->rx_opt.ts_recent_stamp && inet->inet_daddr != daddr) {\n\t\t/* Reset inherited state */\n\t\ttp->rx_opt.ts_recent\t   = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq\t\t   = 0;\n\t}\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp && rt->rt_dst == daddr) {\n\t\tstruct inet_peer *peer = rt_get_peer(rt);\n\t\t/*\n\t\t * VJ's idea. We save last timestamp seen from\n\t\t * the destination in peer table, when entering state\n\t\t * TIME-WAIT * and initialize rx_opt.ts_recent from it,\n\t\t * when trying new connection.\n\t\t */\n\t\tif (peer) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp <= TCP_PAWS_MSL) {\n\t\t\t\ttp->rx_opt.ts_recent_stamp = peer->tcp_ts_stamp;\n\t\t\t\ttp->rx_opt.ts_recent = peer->tcp_ts;\n\t\t\t}\n\t\t}\n\t}\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet->opt)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;\n\n\ttp->rx_opt.mss_clamp = TCP_MSS_DEFAULT;\n\n\t/* Socket identity is still unknown (sport may be zero).\n\t * However we set state to SYN-SENT and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk->sk_gso_type = SKB_GSO_TCPV4;\n\tsk_setup_caps(sk, &rt->dst);\n\n\tif (!tp->write_seq)\n\t\ttp->write_seq = secure_tcp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t\t   inet->inet_daddr,\n\t\t\t\t\t\t\t   inet->inet_sport,\n\t\t\t\t\t\t\t   usin->sin_port);\n\n\tinet->inet_id = tp->write_seq ^ jiffies;\n\n\terr = tcp_connect(sk);\n\trt = NULL;\n\tif (err)\n\t\tgoto failure;\n\n\treturn 0;\n\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port,\n\t * if necessary.\n\t */\n\ttcp_set_state(sk, TCP_CLOSE);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\treturn err;\n}",
        "code_after_change": "int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\tstruct ip_options_rcu *inet_opt;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     sock_owned_by_user(sk));\n\tif (inet_opt && inet_opt->opt.srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet_opt->opt.faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_TCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\tif (err == -ENETUNREACH)\n\t\t\tIP_INC_STATS_BH(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\t\treturn err;\n\t}\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (!inet_opt || !inet_opt->opt.srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (!inet->inet_saddr)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tif (tp->rx_opt.ts_recent_stamp && inet->inet_daddr != daddr) {\n\t\t/* Reset inherited state */\n\t\ttp->rx_opt.ts_recent\t   = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq\t\t   = 0;\n\t}\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp && rt->rt_dst == daddr) {\n\t\tstruct inet_peer *peer = rt_get_peer(rt);\n\t\t/*\n\t\t * VJ's idea. We save last timestamp seen from\n\t\t * the destination in peer table, when entering state\n\t\t * TIME-WAIT * and initialize rx_opt.ts_recent from it,\n\t\t * when trying new connection.\n\t\t */\n\t\tif (peer) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp <= TCP_PAWS_MSL) {\n\t\t\t\ttp->rx_opt.ts_recent_stamp = peer->tcp_ts_stamp;\n\t\t\t\ttp->rx_opt.ts_recent = peer->tcp_ts;\n\t\t\t}\n\t\t}\n\t}\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet_opt)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n\n\ttp->rx_opt.mss_clamp = TCP_MSS_DEFAULT;\n\n\t/* Socket identity is still unknown (sport may be zero).\n\t * However we set state to SYN-SENT and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk->sk_gso_type = SKB_GSO_TCPV4;\n\tsk_setup_caps(sk, &rt->dst);\n\n\tif (!tp->write_seq)\n\t\ttp->write_seq = secure_tcp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t\t   inet->inet_daddr,\n\t\t\t\t\t\t\t   inet->inet_sport,\n\t\t\t\t\t\t\t   usin->sin_port);\n\n\tinet->inet_id = tp->write_seq ^ jiffies;\n\n\terr = tcp_connect(sk);\n\trt = NULL;\n\tif (err)\n\t\tgoto failure;\n\n\treturn 0;\n\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port,\n\t * if necessary.\n\t */\n\ttcp_set_state(sk, TCP_CLOSE);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,7 @@\n \tstruct flowi4 fl4;\n \tstruct rtable *rt;\n \tint err;\n+\tstruct ip_options_rcu *inet_opt;\n \n \tif (addr_len < sizeof(struct sockaddr_in))\n \t\treturn -EINVAL;\n@@ -16,10 +17,12 @@\n \t\treturn -EAFNOSUPPORT;\n \n \tnexthop = daddr = usin->sin_addr.s_addr;\n-\tif (inet->opt && inet->opt->srr) {\n+\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n+\t\t\t\t\t     sock_owned_by_user(sk));\n+\tif (inet_opt && inet_opt->opt.srr) {\n \t\tif (!daddr)\n \t\t\treturn -EINVAL;\n-\t\tnexthop = inet->opt->faddr;\n+\t\tnexthop = inet_opt->opt.faddr;\n \t}\n \n \torig_sport = inet->inet_sport;\n@@ -40,7 +43,7 @@\n \t\treturn -ENETUNREACH;\n \t}\n \n-\tif (!inet->opt || !inet->opt->srr)\n+\tif (!inet_opt || !inet_opt->opt.srr)\n \t\tdaddr = rt->rt_dst;\n \n \tif (!inet->inet_saddr)\n@@ -76,8 +79,8 @@\n \tinet->inet_daddr = daddr;\n \n \tinet_csk(sk)->icsk_ext_hdr_len = 0;\n-\tif (inet->opt)\n-\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;\n+\tif (inet_opt)\n+\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n \n \ttp->rx_opt.mss_clamp = TCP_MSS_DEFAULT;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *inet_opt;",
                "\tinet_opt = rcu_dereference_protected(inet->inet_opt,",
                "\t\t\t\t\t     sock_owned_by_user(sk));",
                "\tif (inet_opt && inet_opt->opt.srr) {",
                "\t\tnexthop = inet_opt->opt.faddr;",
                "\tif (!inet_opt || !inet_opt->opt.srr)",
                "\tif (inet_opt)",
                "\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;"
            ],
            "deleted": [
                "\tif (inet->opt && inet->opt->srr) {",
                "\t\tnexthop = inet->opt->faddr;",
                "\tif (!inet->opt || !inet->opt->srr)",
                "\tif (inet->opt)",
                "\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic.",
        "id": 90
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "int cipso_v4_req_setattr(struct request_sock *req,\n\t\t\t const struct cipso_v4_doi *doi_def,\n\t\t\t const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_request_sock *req_inet;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto req_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\tmemcpy(opt->__data, buf, buf_len);\n\topt->optlen = opt_len;\n\topt->cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\treq_inet = inet_rsk(req);\n\topt = xchg(&req_inet->opt, opt);\n\tkfree(opt);\n\n\treturn 0;\n\nreq_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}",
        "code_after_change": "int cipso_v4_req_setattr(struct request_sock *req,\n\t\t\t const struct cipso_v4_doi *doi_def,\n\t\t\t const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options_rcu *opt = NULL;\n\tstruct inet_request_sock *req_inet;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto req_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\tmemcpy(opt->opt.__data, buf, buf_len);\n\topt->opt.optlen = opt_len;\n\topt->opt.cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\treq_inet = inet_rsk(req);\n\topt = xchg(&req_inet->opt, opt);\n\tif (opt)\n\t\tcall_rcu(&opt->rcu, opt_kfree_rcu);\n\n\treturn 0;\n\nreq_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tunsigned char *buf = NULL;\n \tu32 buf_len;\n \tu32 opt_len;\n-\tstruct ip_options *opt = NULL;\n+\tstruct ip_options_rcu *opt = NULL;\n \tstruct inet_request_sock *req_inet;\n \n \t/* We allocate the maximum CIPSO option size here so we are probably\n@@ -34,15 +34,16 @@\n \t\tret_val = -ENOMEM;\n \t\tgoto req_setattr_failure;\n \t}\n-\tmemcpy(opt->__data, buf, buf_len);\n-\topt->optlen = opt_len;\n-\topt->cipso = sizeof(struct iphdr);\n+\tmemcpy(opt->opt.__data, buf, buf_len);\n+\topt->opt.optlen = opt_len;\n+\topt->opt.cipso = sizeof(struct iphdr);\n \tkfree(buf);\n \tbuf = NULL;\n \n \treq_inet = inet_rsk(req);\n \topt = xchg(&req_inet->opt, opt);\n-\tkfree(opt);\n+\tif (opt)\n+\t\tcall_rcu(&opt->rcu, opt_kfree_rcu);\n \n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *opt = NULL;",
                "\tmemcpy(opt->opt.__data, buf, buf_len);",
                "\topt->opt.optlen = opt_len;",
                "\topt->opt.cipso = sizeof(struct iphdr);",
                "\tif (opt)",
                "\t\tcall_rcu(&opt->rcu, opt_kfree_rcu);"
            ],
            "deleted": [
                "\tstruct ip_options *opt = NULL;",
                "\tmemcpy(opt->__data, buf, buf_len);",
                "\topt->optlen = opt_len;",
                "\topt->cipso = sizeof(struct iphdr);",
                "\tkfree(opt);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic.",
        "id": 74
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static struct ip_options *ip_options_get_alloc(const int optlen)\n{\n\treturn kzalloc(sizeof(struct ip_options) + ((optlen + 3) & ~3),\n\t\t       GFP_KERNEL);\n}",
        "code_after_change": "static struct ip_options_rcu *ip_options_get_alloc(const int optlen)\n{\n\treturn kzalloc(sizeof(struct ip_options_rcu) + ((optlen + 3) & ~3),\n\t\t       GFP_KERNEL);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,5 @@\n-static struct ip_options *ip_options_get_alloc(const int optlen)\n+static struct ip_options_rcu *ip_options_get_alloc(const int optlen)\n {\n-\treturn kzalloc(sizeof(struct ip_options) + ((optlen + 3) & ~3),\n+\treturn kzalloc(sizeof(struct ip_options_rcu) + ((optlen + 3) & ~3),\n \t\t       GFP_KERNEL);\n }",
        "function_modified_lines": {
            "added": [
                "static struct ip_options_rcu *ip_options_get_alloc(const int optlen)",
                "\treturn kzalloc(sizeof(struct ip_options_rcu) + ((optlen + 3) & ~3),"
            ],
            "deleted": [
                "static struct ip_options *ip_options_get_alloc(const int optlen)",
                "\treturn kzalloc(sizeof(struct ip_options) + ((optlen + 3) & ~3),"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic.",
        "id": 80
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "void cipso_v4_req_delattr(struct request_sock *req)\n{\n\tstruct ip_options *opt;\n\tstruct inet_request_sock *req_inet;\n\n\treq_inet = inet_rsk(req);\n\topt = req_inet->opt;\n\tif (opt == NULL || opt->cipso == 0)\n\t\treturn;\n\n\tcipso_v4_delopt(&req_inet->opt);\n}",
        "code_after_change": "void cipso_v4_req_delattr(struct request_sock *req)\n{\n\tstruct ip_options_rcu *opt;\n\tstruct inet_request_sock *req_inet;\n\n\treq_inet = inet_rsk(req);\n\topt = req_inet->opt;\n\tif (opt == NULL || opt->opt.cipso == 0)\n\t\treturn;\n\n\tcipso_v4_delopt(&req_inet->opt);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,11 @@\n void cipso_v4_req_delattr(struct request_sock *req)\n {\n-\tstruct ip_options *opt;\n+\tstruct ip_options_rcu *opt;\n \tstruct inet_request_sock *req_inet;\n \n \treq_inet = inet_rsk(req);\n \topt = req_inet->opt;\n-\tif (opt == NULL || opt->cipso == 0)\n+\tif (opt == NULL || opt->opt.cipso == 0)\n \t\treturn;\n \n \tcipso_v4_delopt(&req_inet->opt);",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *opt;",
                "\tif (opt == NULL || opt->opt.cipso == 0)"
            ],
            "deleted": [
                "\tstruct ip_options *opt;",
                "\tif (opt == NULL || opt->cipso == 0)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic.",
        "id": 70
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "struct dst_entry *inet_csk_route_req(struct sock *sk,\n\t\t\t\t     const struct request_sock *req)\n{\n\tstruct rtable *rt;\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ip_options *opt = inet_rsk(req)->opt;\n\tstruct net *net = sock_net(sk);\n\tstruct flowi4 fl4;\n\n\tflowi4_init_output(&fl4, sk->sk_bound_dev_if, sk->sk_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,\n\t\t\t   ireq->loc_addr, ireq->rmt_port, inet_sk(sk)->inet_sport);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto route_err;\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}",
        "code_after_change": "struct dst_entry *inet_csk_route_req(struct sock *sk,\n\t\t\t\t     const struct request_sock *req)\n{\n\tstruct rtable *rt;\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ip_options_rcu *opt = inet_rsk(req)->opt;\n\tstruct net *net = sock_net(sk);\n\tstruct flowi4 fl4;\n\n\tflowi4_init_output(&fl4, sk->sk_bound_dev_if, sk->sk_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->rmt_addr,\n\t\t\t   ireq->loc_addr, ireq->rmt_port, inet_sk(sk)->inet_sport);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto route_err;\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,20 +3,20 @@\n {\n \tstruct rtable *rt;\n \tconst struct inet_request_sock *ireq = inet_rsk(req);\n-\tstruct ip_options *opt = inet_rsk(req)->opt;\n+\tstruct ip_options_rcu *opt = inet_rsk(req)->opt;\n \tstruct net *net = sock_net(sk);\n \tstruct flowi4 fl4;\n \n \tflowi4_init_output(&fl4, sk->sk_bound_dev_if, sk->sk_mark,\n \t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n \t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n-\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,\n+\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->rmt_addr,\n \t\t\t   ireq->loc_addr, ireq->rmt_port, inet_sk(sk)->inet_sport);\n \tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n \trt = ip_route_output_flow(net, &fl4, sk);\n \tif (IS_ERR(rt))\n \t\tgoto no_route;\n-\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)\n+\tif (opt && opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)\n \t\tgoto route_err;\n \treturn &rt->dst;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ip_options_rcu *opt = inet_rsk(req)->opt;",
                "\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->rmt_addr,",
                "\tif (opt && opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)"
            ],
            "deleted": [
                "\tstruct ip_options *opt = inet_rsk(req)->opt;",
                "\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,",
                "\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic.",
        "id": 78
    },
    {
        "cve_id": "CVE-2012-3552",
        "code_before_change": "static struct sock * tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t  struct request_sock *req,\n\t\t\t\t\t  struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *treq;\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct tcp6_sock *newtcp6sk;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst);\n\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\t\tnewinet = inet_sk(newsk);\n\t\tnewnp = inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\ttreq = inet6_rsk(req);\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, req);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\n\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &treq->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &treq->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &treq->loc_addr);\n\tnewsk->sk_bound_dev_if = treq->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->opt = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (treq->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(treq->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(treq->pktopts);\n\t\ttreq->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\tif (opt) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\ttcp_mtup_init(newsk);\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\ttcp_initialize_rcv_mss(newsk);\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tif ((key = tcp_v6_md5_do_lookup(sk, &newnp->daddr)) != NULL) {\n\t\t/* We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\tchar *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);\n\t\tif (newkey != NULL)\n\t\t\ttcp_v6_md5_do_add(newsk, &newnp->daddr,\n\t\t\t\t\t  newkey, key->keylen);\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tif (opt && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "code_after_change": "static struct sock * tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t  struct request_sock *req,\n\t\t\t\t\t  struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *treq;\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct tcp6_sock *newtcp6sk;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst);\n\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\t\tnewinet = inet_sk(newsk);\n\t\tnewnp = inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\ttreq = inet6_rsk(req);\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, req);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\n\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &treq->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &treq->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &treq->loc_addr);\n\tnewsk->sk_bound_dev_if = treq->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (treq->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(treq->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(treq->pktopts);\n\t\ttreq->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\tif (opt) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\ttcp_mtup_init(newsk);\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\ttcp_initialize_rcv_mss(newsk);\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tif ((key = tcp_v6_md5_do_lookup(sk, &newnp->daddr)) != NULL) {\n\t\t/* We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\tchar *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);\n\t\tif (newkey != NULL)\n\t\t\ttcp_v6_md5_do_add(newsk, &newnp->daddr,\n\t\t\t\t\t  newkey, key->keylen);\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tif (opt && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -107,7 +107,7 @@\n \n \t   First: no IPv4 options.\n \t */\n-\tnewinet->opt = NULL;\n+\tnewinet->inet_opt = NULL;\n \tnewnp->ipv6_fl_list = NULL;\n \n \t/* Clone RX bits */",
        "function_modified_lines": {
            "added": [
                "\tnewinet->inet_opt = NULL;"
            ],
            "deleted": [
                "\tnewinet->opt = NULL;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic.",
        "id": 93
    },
    {
        "cve_id": "CVE-2015-4170",
        "code_before_change": "static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n{\n\tlong tmp = *old;\n\t*old = atomic_long_cmpxchg(&sem->count, *old, new);\n\treturn *old == tmp;\n}",
        "code_after_change": "static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n{\n\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);\n\tif (tmp == *old) {\n\t\t*old = new;\n\t\treturn 1;\n\t} else {\n\t\t*old = tmp;\n\t\treturn 0;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,11 @@\n static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\n {\n-\tlong tmp = *old;\n-\t*old = atomic_long_cmpxchg(&sem->count, *old, new);\n-\treturn *old == tmp;\n+\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);\n+\tif (tmp == *old) {\n+\t\t*old = new;\n+\t\treturn 1;\n+\t} else {\n+\t\t*old = tmp;\n+\t\treturn 0;\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tlong tmp = atomic_long_cmpxchg(&sem->count, *old, new);",
                "\tif (tmp == *old) {",
                "\t\t*old = new;",
                "\t\treturn 1;",
                "\t} else {",
                "\t\t*old = tmp;",
                "\t\treturn 0;",
                "\t}"
            ],
            "deleted": [
                "\tlong tmp = *old;",
                "\t*old = atomic_long_cmpxchg(&sem->count, *old, new);",
                "\treturn *old == tmp;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ldsem_cmpxchg function in drivers/tty/tty_ldsem.c in the Linux kernel before 3.13-rc4-next-20131218 allows local users to cause a denial of service (ldsem_down_read and ldsem_down_write deadlock) by establishing a new tty thread during shutdown of a previous tty thread.",
        "id": 764
    },
    {
        "cve_id": "CVE-2021-0920",
        "code_before_change": "static int unix_stream_read_generic(struct unix_stream_read_state *state,\n\t\t\t\t    bool freezable)\n{\n\tstruct scm_cookie scm;\n\tstruct socket *sock = state->socket;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tint copied = 0;\n\tint flags = state->flags;\n\tint noblock = flags & MSG_DONTWAIT;\n\tbool check_creds = false;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\tsize_t size = state->size;\n\tunsigned int last_len;\n\n\tif (unlikely(sk->sk_state != TCP_ESTABLISHED)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (unlikely(flags & MSG_OOB)) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, noblock);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\tmutex_lock(&u->iolock);\n\n\tskip = max(sk_peek_offset(sk, flags), 0);\n\n\tdo {\n\t\tint chunk;\n\t\tbool drop_skb;\n\t\tstruct sk_buff *skb, *last;\n\nredo:\n\t\tunix_state_lock(sk);\n\t\tif (sock_flag(sk, SOCK_DEAD)) {\n\t\t\terr = -ECONNRESET;\n\t\t\tgoto unlock;\n\t\t}\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\n\t\tlast_len = last ? last->len : 0;\nagain:\n\t\tif (skb == NULL) {\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\tif (!timeo) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmutex_unlock(&u->iolock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last,\n\t\t\t\t\t\t      last_len, freezable);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tscm_destroy(&scm);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmutex_lock(&u->iolock);\n\t\t\tgoto redo;\nunlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif (!unix_skb_scm_eq(skb, &scm))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tunix_set_secdata(&scm, skb);\n\t\t\tcheck_creds = true;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (state->msg && state->msg->msg_name) {\n\t\t\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr,\n\t\t\t\t\t state->msg->msg_name);\n\t\t\tunix_copy_addr(state->msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tskb_get(skb);\n\t\tchunk = state->recv_actor(skb, skip, chunk, state);\n\t\tdrop_skb = !unix_skb_len(skb);\n\t\t/* skb is only safe to use if !drop_skb */\n\t\tconsume_skb(skb);\n\t\tif (chunk < 0) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\tif (drop_skb) {\n\t\t\t/* the skb was touched by a concurrent reader;\n\t\t\t * we should not expect anything from this skb\n\t\t\t * anymore and assume it invalid - we can be\n\t\t\t * sure it was dropped from the socket queue\n\t\t\t *\n\t\t\t * let's report a short read\n\t\t\t */\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp) {\n\t\t\t\tscm_stat_del(sk, skb);\n\t\t\t\tunix_detach_fds(&scm, skb);\n\t\t\t}\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (scm.fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tbreak;\n\n\t\t\tskip = 0;\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tunix_state_lock(sk);\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (skb)\n\t\t\t\tgoto again;\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->iolock);\n\tif (state->msg)\n\t\tscm_recv(sock, state->msg, &scm, flags);\n\telse\n\t\tscm_destroy(&scm);\nout:\n\treturn copied ? : err;\n}",
        "code_after_change": "static int unix_stream_read_generic(struct unix_stream_read_state *state,\n\t\t\t\t    bool freezable)\n{\n\tstruct scm_cookie scm;\n\tstruct socket *sock = state->socket;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tint copied = 0;\n\tint flags = state->flags;\n\tint noblock = flags & MSG_DONTWAIT;\n\tbool check_creds = false;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\tsize_t size = state->size;\n\tunsigned int last_len;\n\n\tif (unlikely(sk->sk_state != TCP_ESTABLISHED)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (unlikely(flags & MSG_OOB)) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, noblock);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\tmutex_lock(&u->iolock);\n\n\tskip = max(sk_peek_offset(sk, flags), 0);\n\n\tdo {\n\t\tint chunk;\n\t\tbool drop_skb;\n\t\tstruct sk_buff *skb, *last;\n\nredo:\n\t\tunix_state_lock(sk);\n\t\tif (sock_flag(sk, SOCK_DEAD)) {\n\t\t\terr = -ECONNRESET;\n\t\t\tgoto unlock;\n\t\t}\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\n\t\tlast_len = last ? last->len : 0;\nagain:\n\t\tif (skb == NULL) {\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\tif (!timeo) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmutex_unlock(&u->iolock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last,\n\t\t\t\t\t\t      last_len, freezable);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tscm_destroy(&scm);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmutex_lock(&u->iolock);\n\t\t\tgoto redo;\nunlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif (!unix_skb_scm_eq(skb, &scm))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tunix_set_secdata(&scm, skb);\n\t\t\tcheck_creds = true;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (state->msg && state->msg->msg_name) {\n\t\t\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr,\n\t\t\t\t\t state->msg->msg_name);\n\t\t\tunix_copy_addr(state->msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tskb_get(skb);\n\t\tchunk = state->recv_actor(skb, skip, chunk, state);\n\t\tdrop_skb = !unix_skb_len(skb);\n\t\t/* skb is only safe to use if !drop_skb */\n\t\tconsume_skb(skb);\n\t\tif (chunk < 0) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\tif (drop_skb) {\n\t\t\t/* the skb was touched by a concurrent reader;\n\t\t\t * we should not expect anything from this skb\n\t\t\t * anymore and assume it invalid - we can be\n\t\t\t * sure it was dropped from the socket queue\n\t\t\t *\n\t\t\t * let's report a short read\n\t\t\t */\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp) {\n\t\t\t\tscm_stat_del(sk, skb);\n\t\t\t\tunix_detach_fds(&scm, skb);\n\t\t\t}\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (scm.fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tunix_peek_fds(&scm, skb);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tbreak;\n\n\t\t\tskip = 0;\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tunix_state_lock(sk);\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (skb)\n\t\t\t\tgoto again;\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->iolock);\n\tif (state->msg)\n\t\tscm_recv(sock, state->msg, &scm, flags);\n\telse\n\t\tscm_destroy(&scm);\nout:\n\treturn copied ? : err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -169,7 +169,7 @@\n \t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n \t\t\t */\n \t\t\tif (UNIXCB(skb).fp)\n-\t\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n+\t\t\t\tunix_peek_fds(&scm, skb);\n \n \t\t\tsk_peek_offset_fwd(sk, chunk);\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tunix_peek_fds(&scm, skb);"
            ],
            "deleted": [
                "\t\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In unix_scm_to_skb of af_unix.c, there is a possible use after free bug due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-196926917References: Upstream kernel",
        "id": 2830
    },
    {
        "cve_id": "CVE-2023-2006",
        "code_before_change": "int rxrpc_connect_call(struct rxrpc_sock *rx,\n\t\t       struct rxrpc_call *call,\n\t\t       struct rxrpc_conn_parameters *cp,\n\t\t       struct sockaddr_rxrpc *srx,\n\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\tstruct rxrpc_net *rxnet = cp->local->rxnet;\n\tint ret = 0;\n\n\t_enter(\"{%d,%lx},\", call->debug_id, call->user_call_ID);\n\n\trxrpc_discard_expired_client_conns(&rxnet->client_conn_reaper);\n\n\tbundle = rxrpc_prep_call(rx, call, cp, srx, gfp);\n\tif (IS_ERR(bundle)) {\n\t\tret = PTR_ERR(bundle);\n\t\tgoto out;\n\t}\n\n\tif (call->state == RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = rxrpc_wait_for_channel(bundle, call, gfp);\n\t\tif (ret < 0)\n\t\t\tgoto wait_failed;\n\t}\n\ngranted_channel:\n\t/* Paired with the write barrier in rxrpc_activate_one_channel(). */\n\tsmp_rmb();\n\nout_put_bundle:\n\trxrpc_put_bundle(bundle);\nout:\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\nwait_failed:\n\tspin_lock(&bundle->channel_lock);\n\tlist_del_init(&call->chan_wait_link);\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (call->state != RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = 0;\n\t\tgoto granted_channel;\n\t}\n\n\ttrace_rxrpc_client(call->conn, ret, rxrpc_client_chan_wait_failed);\n\trxrpc_set_call_completion(call, RXRPC_CALL_LOCAL_ERROR, 0, ret);\n\trxrpc_disconnect_client_call(bundle, call);\n\tgoto out_put_bundle;\n}",
        "code_after_change": "int rxrpc_connect_call(struct rxrpc_sock *rx,\n\t\t       struct rxrpc_call *call,\n\t\t       struct rxrpc_conn_parameters *cp,\n\t\t       struct sockaddr_rxrpc *srx,\n\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\tstruct rxrpc_net *rxnet = cp->local->rxnet;\n\tint ret = 0;\n\n\t_enter(\"{%d,%lx},\", call->debug_id, call->user_call_ID);\n\n\trxrpc_discard_expired_client_conns(&rxnet->client_conn_reaper);\n\n\tbundle = rxrpc_prep_call(rx, call, cp, srx, gfp);\n\tif (IS_ERR(bundle)) {\n\t\tret = PTR_ERR(bundle);\n\t\tgoto out;\n\t}\n\n\tif (call->state == RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = rxrpc_wait_for_channel(bundle, call, gfp);\n\t\tif (ret < 0)\n\t\t\tgoto wait_failed;\n\t}\n\ngranted_channel:\n\t/* Paired with the write barrier in rxrpc_activate_one_channel(). */\n\tsmp_rmb();\n\nout_put_bundle:\n\trxrpc_deactivate_bundle(bundle);\n\trxrpc_put_bundle(bundle);\nout:\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\nwait_failed:\n\tspin_lock(&bundle->channel_lock);\n\tlist_del_init(&call->chan_wait_link);\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (call->state != RXRPC_CALL_CLIENT_AWAIT_CONN) {\n\t\tret = 0;\n\t\tgoto granted_channel;\n\t}\n\n\ttrace_rxrpc_client(call->conn, ret, rxrpc_client_chan_wait_failed);\n\trxrpc_set_call_completion(call, RXRPC_CALL_LOCAL_ERROR, 0, ret);\n\trxrpc_disconnect_client_call(bundle, call);\n\tgoto out_put_bundle;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,6 +29,7 @@\n \tsmp_rmb();\n \n out_put_bundle:\n+\trxrpc_deactivate_bundle(bundle);\n \trxrpc_put_bundle(bundle);\n out:\n \t_leave(\" = %d\", ret);",
        "function_modified_lines": {
            "added": [
                "\trxrpc_deactivate_bundle(bundle);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernel's RxRPC network protocol, within the processing of RxRPC bundles. This issue results from the lack of proper locking when performing operations on an object. This may allow an attacker to escalate privileges and execute arbitrary code in the context of the kernel.",
        "id": 3894
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "int vmw_gem_object_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tunion drm_vmw_alloc_dmabuf_arg *arg =\n\t    (union drm_vmw_alloc_dmabuf_arg *)data;\n\tstruct drm_vmw_alloc_dmabuf_req *req = &arg->req;\n\tstruct drm_vmw_dmabuf_rep *rep = &arg->rep;\n\tstruct vmw_bo *vbo;\n\tuint32_t handle;\n\tint ret;\n\n\tret = vmw_gem_object_create_with_handle(dev_priv, filp,\n\t\t\t\t\t\treq->size, &handle, &vbo);\n\tif (ret)\n\t\tgoto out_no_bo;\n\n\trep->handle = handle;\n\trep->map_handle = drm_vma_node_offset_addr(&vbo->tbo.base.vma_node);\n\trep->cur_gmr_id = handle;\n\trep->cur_gmr_offset = 0;\nout_no_bo:\n\treturn ret;\n}",
        "code_after_change": "int vmw_gem_object_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tunion drm_vmw_alloc_dmabuf_arg *arg =\n\t    (union drm_vmw_alloc_dmabuf_arg *)data;\n\tstruct drm_vmw_alloc_dmabuf_req *req = &arg->req;\n\tstruct drm_vmw_dmabuf_rep *rep = &arg->rep;\n\tstruct vmw_bo *vbo;\n\tuint32_t handle;\n\tint ret;\n\n\tret = vmw_gem_object_create_with_handle(dev_priv, filp,\n\t\t\t\t\t\treq->size, &handle, &vbo);\n\tif (ret)\n\t\tgoto out_no_bo;\n\n\trep->handle = handle;\n\trep->map_handle = drm_vma_node_offset_addr(&vbo->tbo.base.vma_node);\n\trep->cur_gmr_id = handle;\n\trep->cur_gmr_offset = 0;\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(&vbo->tbo.base);\nout_no_bo:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,6 +19,8 @@\n \trep->map_handle = drm_vma_node_offset_addr(&vbo->tbo.base.vma_node);\n \trep->cur_gmr_id = handle;\n \trep->cur_gmr_offset = 0;\n+\t/* drop reference from allocate - handle holds it now */\n+\tdrm_gem_object_put(&vbo->tbo.base);\n out_no_bo:\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t/* drop reference from allocate - handle holds it now */",
                "\tdrm_gem_object_put(&vbo->tbo.base);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel.",
        "id": 4083
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "int vmw_dumb_create(struct drm_file *file_priv,\n\t\t    struct drm_device *dev,\n\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_bo *vbo;\n\tint cpp = DIV_ROUND_UP(args->bpp, 8);\n\tint ret;\n\n\tswitch (cpp) {\n\tcase 1: /* DRM_FORMAT_C8 */\n\tcase 2: /* DRM_FORMAT_RGB565 */\n\tcase 4: /* DRM_FORMAT_XRGB8888 */\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * Dumb buffers don't allow anything else.\n\t\t * This is tested via IGT's dumb_buffers\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\targs->pitch = args->width * cpp;\n\targs->size = ALIGN(args->pitch * args->height, PAGE_SIZE);\n\n\tret = vmw_gem_object_create_with_handle(dev_priv, file_priv,\n\t\t\t\t\t\targs->size, &args->handle,\n\t\t\t\t\t\t&vbo);\n\n\treturn ret;\n}",
        "code_after_change": "int vmw_dumb_create(struct drm_file *file_priv,\n\t\t    struct drm_device *dev,\n\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_bo *vbo;\n\tint cpp = DIV_ROUND_UP(args->bpp, 8);\n\tint ret;\n\n\tswitch (cpp) {\n\tcase 1: /* DRM_FORMAT_C8 */\n\tcase 2: /* DRM_FORMAT_RGB565 */\n\tcase 4: /* DRM_FORMAT_XRGB8888 */\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * Dumb buffers don't allow anything else.\n\t\t * This is tested via IGT's dumb_buffers\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\targs->pitch = args->width * cpp;\n\targs->size = ALIGN(args->pitch * args->height, PAGE_SIZE);\n\n\tret = vmw_gem_object_create_with_handle(dev_priv, file_priv,\n\t\t\t\t\t\targs->size, &args->handle,\n\t\t\t\t\t\t&vbo);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(&vbo->tbo.base);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,6 +26,7 @@\n \tret = vmw_gem_object_create_with_handle(dev_priv, file_priv,\n \t\t\t\t\t\targs->size, &args->handle,\n \t\t\t\t\t\t&vbo);\n-\n+\t/* drop reference from allocate - handle holds it now */\n+\tdrm_gem_object_put(&vbo->tbo.base);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t/* drop reference from allocate - handle holds it now */",
                "\tdrm_gem_object_put(&vbo->tbo.base);"
            ],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel.",
        "id": 4078
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tttm_bo_put(&vmw_bo->tbo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tttm_bo_put(&vmw_bo->tbo);\n\tdrm_gem_object_put(&vmw_bo->tbo.base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n \tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n \tttm_bo_put(&vmw_bo->tbo);\n+\tdrm_gem_object_put(&vmw_bo->tbo.base);\n \tif (unlikely(ret != 0))\n \t\treturn ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tdrm_gem_object_put(&vmw_bo->tbo.base);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel.",
        "id": 4082
    },
    {
        "cve_id": "CVE-2023-33951",
        "code_before_change": "static struct drm_framebuffer *vmw_kms_fb_create(struct drm_device *dev,\n\t\t\t\t\t\t struct drm_file *file_priv,\n\t\t\t\t\t\t const struct drm_mode_fb_cmd2 *mode_cmd)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_framebuffer *vfb = NULL;\n\tstruct vmw_surface *surface = NULL;\n\tstruct vmw_bo *bo = NULL;\n\tint ret;\n\n\t/* returns either a bo or surface */\n\tret = vmw_user_lookup_handle(dev_priv, file_priv,\n\t\t\t\t     mode_cmd->handles[0],\n\t\t\t\t     &surface, &bo);\n\tif (ret) {\n\t\tDRM_ERROR(\"Invalid buffer object handle %u (0x%x).\\n\",\n\t\t\t  mode_cmd->handles[0], mode_cmd->handles[0]);\n\t\tgoto err_out;\n\t}\n\n\n\tif (!bo &&\n\t    !vmw_kms_srf_ok(dev_priv, mode_cmd->width, mode_cmd->height)) {\n\t\tDRM_ERROR(\"Surface size cannot exceed %dx%d\\n\",\n\t\t\tdev_priv->texture_max_width,\n\t\t\tdev_priv->texture_max_height);\n\t\tgoto err_out;\n\t}\n\n\n\tvfb = vmw_kms_new_framebuffer(dev_priv, bo, surface,\n\t\t\t\t      !(dev_priv->capabilities & SVGA_CAP_3D),\n\t\t\t\t      mode_cmd);\n\tif (IS_ERR(vfb)) {\n\t\tret = PTR_ERR(vfb);\n\t\tgoto err_out;\n\t}\n\nerr_out:\n\t/* vmw_user_lookup_handle takes one ref so does new_fb */\n\tif (bo)\n\t\tvmw_bo_unreference(&bo);\n\tif (surface)\n\t\tvmw_surface_unreference(&surface);\n\n\tif (ret) {\n\t\tDRM_ERROR(\"failed to create vmw_framebuffer: %i\\n\", ret);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn &vfb->base;\n}",
        "code_after_change": "static struct drm_framebuffer *vmw_kms_fb_create(struct drm_device *dev,\n\t\t\t\t\t\t struct drm_file *file_priv,\n\t\t\t\t\t\t const struct drm_mode_fb_cmd2 *mode_cmd)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_framebuffer *vfb = NULL;\n\tstruct vmw_surface *surface = NULL;\n\tstruct vmw_bo *bo = NULL;\n\tint ret;\n\n\t/* returns either a bo or surface */\n\tret = vmw_user_lookup_handle(dev_priv, file_priv,\n\t\t\t\t     mode_cmd->handles[0],\n\t\t\t\t     &surface, &bo);\n\tif (ret) {\n\t\tDRM_ERROR(\"Invalid buffer object handle %u (0x%x).\\n\",\n\t\t\t  mode_cmd->handles[0], mode_cmd->handles[0]);\n\t\tgoto err_out;\n\t}\n\n\n\tif (!bo &&\n\t    !vmw_kms_srf_ok(dev_priv, mode_cmd->width, mode_cmd->height)) {\n\t\tDRM_ERROR(\"Surface size cannot exceed %dx%d\\n\",\n\t\t\tdev_priv->texture_max_width,\n\t\t\tdev_priv->texture_max_height);\n\t\tgoto err_out;\n\t}\n\n\n\tvfb = vmw_kms_new_framebuffer(dev_priv, bo, surface,\n\t\t\t\t      !(dev_priv->capabilities & SVGA_CAP_3D),\n\t\t\t\t      mode_cmd);\n\tif (IS_ERR(vfb)) {\n\t\tret = PTR_ERR(vfb);\n\t\tgoto err_out;\n\t}\n\nerr_out:\n\t/* vmw_user_lookup_handle takes one ref so does new_fb */\n\tif (bo) {\n\t\tvmw_bo_unreference(&bo);\n\t\tdrm_gem_object_put(&bo->tbo.base);\n\t}\n\tif (surface)\n\t\tvmw_surface_unreference(&surface);\n\n\tif (ret) {\n\t\tDRM_ERROR(\"failed to create vmw_framebuffer: %i\\n\", ret);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn &vfb->base;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,8 +38,10 @@\n \n err_out:\n \t/* vmw_user_lookup_handle takes one ref so does new_fb */\n-\tif (bo)\n+\tif (bo) {\n \t\tvmw_bo_unreference(&bo);\n+\t\tdrm_gem_object_put(&bo->tbo.base);\n+\t}\n \tif (surface)\n \t\tvmw_surface_unreference(&surface);\n ",
        "function_modified_lines": {
            "added": [
                "\tif (bo) {",
                "\t\tdrm_gem_object_put(&bo->tbo.base);",
                "\t}"
            ],
            "deleted": [
                "\tif (bo)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-667"
        ],
        "cve_description": "A race condition vulnerability was found in the vmwgfx driver in the Linux kernel. The flaw exists within the handling of GEM objects. The issue results from improper locking when performing operations on an object. This flaw allows a local privileged user to disclose information in the context of the kernel.",
        "id": 4084
    },
    {
        "cve_id": "CVE-2020-27825",
        "code_before_change": "void ring_buffer_reset_online_cpus(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\tatomic_inc(&cpu_buffer->resize_disabled);\n\t\tatomic_inc(&cpu_buffer->record_disabled);\n\t}\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\treset_disabled_cpu_buffer(cpu_buffer);\n\n\t\tatomic_dec(&cpu_buffer->record_disabled);\n\t\tatomic_dec(&cpu_buffer->resize_disabled);\n\t}\n}",
        "code_after_change": "void ring_buffer_reset_online_cpus(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\tatomic_inc(&cpu_buffer->resize_disabled);\n\t\tatomic_inc(&cpu_buffer->record_disabled);\n\t}\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\treset_disabled_cpu_buffer(cpu_buffer);\n\n\t\tatomic_dec(&cpu_buffer->record_disabled);\n\t\tatomic_dec(&cpu_buffer->resize_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,9 @@\n {\n \tstruct ring_buffer_per_cpu *cpu_buffer;\n \tint cpu;\n+\n+\t/* prevent another thread from changing buffer sizes */\n+\tmutex_lock(&buffer->mutex);\n \n \tfor_each_online_buffer_cpu(buffer, cpu) {\n \t\tcpu_buffer = buffer->buffers[cpu];\n@@ -21,4 +24,6 @@\n \t\tatomic_dec(&cpu_buffer->record_disabled);\n \t\tatomic_dec(&cpu_buffer->resize_disabled);\n \t}\n+\n+\tmutex_unlock(&buffer->mutex);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* prevent another thread from changing buffer sizes */",
                "\tmutex_lock(&buffer->mutex);",
                "",
                "\tmutex_unlock(&buffer->mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free flaw was found in kernel/trace/ring_buffer.c in Linux kernel (before 5.10-rc1). There was a race problem in trace_open and resize of cpu buffer running parallely on different cpus, may cause a denial of service problem (DOS). This flaw could even allow a local attacker with special user privilege to a kernel information leak threat.",
        "id": 2638
    },
    {
        "cve_id": "CVE-2020-27825",
        "code_before_change": "void ring_buffer_reset_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\tatomic_inc(&cpu_buffer->resize_disabled);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\treset_disabled_cpu_buffer(cpu_buffer);\n\n\tatomic_dec(&cpu_buffer->record_disabled);\n\tatomic_dec(&cpu_buffer->resize_disabled);\n}",
        "code_after_change": "void ring_buffer_reset_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tatomic_inc(&cpu_buffer->resize_disabled);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\treset_disabled_cpu_buffer(cpu_buffer);\n\n\tatomic_dec(&cpu_buffer->record_disabled);\n\tatomic_dec(&cpu_buffer->resize_disabled);\n\n\tmutex_unlock(&buffer->mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,9 @@\n \n \tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n \t\treturn;\n+\n+\t/* prevent another thread from changing buffer sizes */\n+\tmutex_lock(&buffer->mutex);\n \n \tatomic_inc(&cpu_buffer->resize_disabled);\n \tatomic_inc(&cpu_buffer->record_disabled);\n@@ -15,4 +18,6 @@\n \n \tatomic_dec(&cpu_buffer->record_disabled);\n \tatomic_dec(&cpu_buffer->resize_disabled);\n+\n+\tmutex_unlock(&buffer->mutex);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* prevent another thread from changing buffer sizes */",
                "\tmutex_lock(&buffer->mutex);",
                "",
                "\tmutex_unlock(&buffer->mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free flaw was found in kernel/trace/ring_buffer.c in Linux kernel (before 5.10-rc1). There was a race problem in trace_open and resize of cpu buffer running parallely on different cpus, may cause a denial of service problem (DOS). This flaw could even allow a local attacker with special user privilege to a kernel information leak threat.",
        "id": 2637
    },
    {
        "cve_id": "CVE-2015-8767",
        "code_before_change": "static void sctp_generate_timeout_event(struct sctp_association *asoc,\n\t\t\t\t\tsctp_event_timeout_t timeout_type)\n{\n\tstruct net *net = sock_net(asoc->base.sk);\n\tint error = 0;\n\n\tbh_lock_sock(asoc->base.sk);\n\tif (sock_owned_by_user(asoc->base.sk)) {\n\t\tpr_debug(\"%s: sock is busy: timer %d\\n\", __func__,\n\t\t\t timeout_type);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&asoc->timers[timeout_type], jiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this association really dead and just waiting around for\n\t * the timer to let go of the reference?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\t/* Run through the state machine.  */\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(timeout_type),\n\t\t\t   asoc->state, asoc->ep, asoc,\n\t\t\t   (void *)timeout_type, GFP_ATOMIC);\n\n\tif (error)\n\t\tasoc->base.sk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(asoc->base.sk);\n\tsctp_association_put(asoc);\n}",
        "code_after_change": "static void sctp_generate_timeout_event(struct sctp_association *asoc,\n\t\t\t\t\tsctp_event_timeout_t timeout_type)\n{\n\tstruct sock *sk = asoc->base.sk;\n\tstruct net *net = sock_net(sk);\n\tint error = 0;\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk)) {\n\t\tpr_debug(\"%s: sock is busy: timer %d\\n\", __func__,\n\t\t\t timeout_type);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&asoc->timers[timeout_type], jiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this association really dead and just waiting around for\n\t * the timer to let go of the reference?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\t/* Run through the state machine.  */\n\terror = sctp_do_sm(net, SCTP_EVENT_T_TIMEOUT,\n\t\t\t   SCTP_ST_TIMEOUT(timeout_type),\n\t\t\t   asoc->state, asoc->ep, asoc,\n\t\t\t   (void *)timeout_type, GFP_ATOMIC);\n\n\tif (error)\n\t\tsk->sk_err = -error;\n\nout_unlock:\n\tbh_unlock_sock(sk);\n\tsctp_association_put(asoc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,12 @@\n static void sctp_generate_timeout_event(struct sctp_association *asoc,\n \t\t\t\t\tsctp_event_timeout_t timeout_type)\n {\n-\tstruct net *net = sock_net(asoc->base.sk);\n+\tstruct sock *sk = asoc->base.sk;\n+\tstruct net *net = sock_net(sk);\n \tint error = 0;\n \n-\tbh_lock_sock(asoc->base.sk);\n-\tif (sock_owned_by_user(asoc->base.sk)) {\n+\tbh_lock_sock(sk);\n+\tif (sock_owned_by_user(sk)) {\n \t\tpr_debug(\"%s: sock is busy: timer %d\\n\", __func__,\n \t\t\t timeout_type);\n \n@@ -28,9 +29,9 @@\n \t\t\t   (void *)timeout_type, GFP_ATOMIC);\n \n \tif (error)\n-\t\tasoc->base.sk->sk_err = -error;\n+\t\tsk->sk_err = -error;\n \n out_unlock:\n-\tbh_unlock_sock(asoc->base.sk);\n+\tbh_unlock_sock(sk);\n \tsctp_association_put(asoc);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct sock *sk = asoc->base.sk;",
                "\tstruct net *net = sock_net(sk);",
                "\tbh_lock_sock(sk);",
                "\tif (sock_owned_by_user(sk)) {",
                "\t\tsk->sk_err = -error;",
                "\tbh_unlock_sock(sk);"
            ],
            "deleted": [
                "\tstruct net *net = sock_net(asoc->base.sk);",
                "\tbh_lock_sock(asoc->base.sk);",
                "\tif (sock_owned_by_user(asoc->base.sk)) {",
                "\t\tasoc->base.sk->sk_err = -error;",
                "\tbh_unlock_sock(asoc->base.sk);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "net/sctp/sm_sideeffect.c in the Linux kernel before 4.3 does not properly manage the relationship between a lock and a socket, which allows local users to cause a denial of service (deadlock) via a crafted sctp_accept call.",
        "id": 842
    },
    {
        "cve_id": "CVE-2014-2706",
        "code_before_change": "static ieee80211_tx_result\nieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tstruct sta_info *sta = tx->sta;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_local *local = tx->local;\n\n\tif (unlikely(!sta))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||\n\t\t      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&\n\t\t     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {\n\t\tint ac = skb_get_queue_mapping(tx->skb);\n\n\t\tps_dbg(sta->sdata, \"STA %pM aid %d: PS buffer for AC %d\\n\",\n\t\t       sta->sta.addr, sta->sta.aid, ac);\n\t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n\t\t\tpurge_old_ps_buffers(tx->local);\n\t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n\t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n\t\t\tps_dbg(tx->sdata,\n\t\t\t       \"STA %pM TX buffer for AC %d full - dropping oldest frame\\n\",\n\t\t\t       sta->sta.addr, ac);\n\t\t\tieee80211_free_txskb(&local->hw, old);\n\t\t} else\n\t\t\ttx->local->total_ps_buffered++;\n\n\t\tinfo->control.jiffies = jiffies;\n\t\tinfo->control.vif = &tx->sdata->vif;\n\t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n\t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n\t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n\n\t\tif (!timer_pending(&local->sta_cleanup))\n\t\t\tmod_timer(&local->sta_cleanup,\n\t\t\t\t  round_jiffies(jiffies +\n\t\t\t\t\t\tSTA_INFO_CLEANUP_INTERVAL));\n\n\t\t/*\n\t\t * We queued up some frames, so the TIM bit might\n\t\t * need to be set, recalculate it.\n\t\t */\n\t\tsta_info_recalc_tim(sta);\n\n\t\treturn TX_QUEUED;\n\t} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {\n\t\tps_dbg(tx->sdata,\n\t\t       \"STA %pM in PS mode, but polling/in SP -> send frame\\n\",\n\t\t       sta->sta.addr);\n\t}\n\n\treturn TX_CONTINUE;\n}",
        "code_after_change": "static ieee80211_tx_result\nieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tstruct sta_info *sta = tx->sta;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_local *local = tx->local;\n\n\tif (unlikely(!sta))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||\n\t\t      test_sta_flag(sta, WLAN_STA_PS_DRIVER)) &&\n\t\t     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {\n\t\tint ac = skb_get_queue_mapping(tx->skb);\n\n\t\tps_dbg(sta->sdata, \"STA %pM aid %d: PS buffer for AC %d\\n\",\n\t\t       sta->sta.addr, sta->sta.aid, ac);\n\t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n\t\t\tpurge_old_ps_buffers(tx->local);\n\n\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */\n\t\tspin_lock(&sta->ps_lock);\n\t\t/*\n\t\t * STA woke up the meantime and all the frames on ps_tx_buf have\n\t\t * been queued to pending queue. No reordering can happen, go\n\t\t * ahead and Tx the packet.\n\t\t */\n\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&\n\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {\n\t\t\tspin_unlock(&sta->ps_lock);\n\t\t\treturn TX_CONTINUE;\n\t\t}\n\n\t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n\t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n\t\t\tps_dbg(tx->sdata,\n\t\t\t       \"STA %pM TX buffer for AC %d full - dropping oldest frame\\n\",\n\t\t\t       sta->sta.addr, ac);\n\t\t\tieee80211_free_txskb(&local->hw, old);\n\t\t} else\n\t\t\ttx->local->total_ps_buffered++;\n\n\t\tinfo->control.jiffies = jiffies;\n\t\tinfo->control.vif = &tx->sdata->vif;\n\t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n\t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n\t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n\t\tspin_unlock(&sta->ps_lock);\n\n\t\tif (!timer_pending(&local->sta_cleanup))\n\t\t\tmod_timer(&local->sta_cleanup,\n\t\t\t\t  round_jiffies(jiffies +\n\t\t\t\t\t\tSTA_INFO_CLEANUP_INTERVAL));\n\n\t\t/*\n\t\t * We queued up some frames, so the TIM bit might\n\t\t * need to be set, recalculate it.\n\t\t */\n\t\tsta_info_recalc_tim(sta);\n\n\t\treturn TX_QUEUED;\n\t} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {\n\t\tps_dbg(tx->sdata,\n\t\t       \"STA %pM in PS mode, but polling/in SP -> send frame\\n\",\n\t\t       sta->sta.addr);\n\t}\n\n\treturn TX_CONTINUE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,20 @@\n \t\t       sta->sta.addr, sta->sta.aid, ac);\n \t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n \t\t\tpurge_old_ps_buffers(tx->local);\n+\n+\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */\n+\t\tspin_lock(&sta->ps_lock);\n+\t\t/*\n+\t\t * STA woke up the meantime and all the frames on ps_tx_buf have\n+\t\t * been queued to pending queue. No reordering can happen, go\n+\t\t * ahead and Tx the packet.\n+\t\t */\n+\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&\n+\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {\n+\t\t\tspin_unlock(&sta->ps_lock);\n+\t\t\treturn TX_CONTINUE;\n+\t\t}\n+\n \t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n \t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n \t\t\tps_dbg(tx->sdata,\n@@ -31,6 +45,7 @@\n \t\tinfo->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;\n \t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n \t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n+\t\tspin_unlock(&sta->ps_lock);\n \n \t\tif (!timer_pending(&local->sta_cleanup))\n \t\t\tmod_timer(&local->sta_cleanup,",
        "function_modified_lines": {
            "added": [
                "",
                "\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */",
                "\t\tspin_lock(&sta->ps_lock);",
                "\t\t/*",
                "\t\t * STA woke up the meantime and all the frames on ps_tx_buf have",
                "\t\t * been queued to pending queue. No reordering can happen, go",
                "\t\t * ahead and Tx the packet.",
                "\t\t */",
                "\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&",
                "\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {",
                "\t\t\tspin_unlock(&sta->ps_lock);",
                "\t\t\treturn TX_CONTINUE;",
                "\t\t}",
                "",
                "\t\tspin_unlock(&sta->ps_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the mac80211 subsystem in the Linux kernel before 3.13.7 allows remote attackers to cause a denial of service (system crash) via network traffic that improperly interacts with the WLAN_STA_PS_STA state (aka power-save mode), related to sta_info.c and tx.c.",
        "id": 496
    },
    {
        "cve_id": "CVE-2014-9710",
        "code_before_change": "static struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      const char *name, int name_len)\n{\n\tstruct btrfs_dir_item *dir_item;\n\tunsigned long name_ptr;\n\tu32 total_len;\n\tu32 cur = 0;\n\tu32 this_len;\n\tstruct extent_buffer *leaf;\n\n\tleaf = path->nodes[0];\n\tdir_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dir_item);\n\tif (verify_dir_item(root, leaf, dir_item))\n\t\treturn NULL;\n\n\ttotal_len = btrfs_item_size_nr(leaf, path->slots[0]);\n\twhile (cur < total_len) {\n\t\tthis_len = sizeof(*dir_item) +\n\t\t\tbtrfs_dir_name_len(leaf, dir_item) +\n\t\t\tbtrfs_dir_data_len(leaf, dir_item);\n\t\tname_ptr = (unsigned long)(dir_item + 1);\n\n\t\tif (btrfs_dir_name_len(leaf, dir_item) == name_len &&\n\t\t    memcmp_extent_buffer(leaf, name, name_ptr, name_len) == 0)\n\t\t\treturn dir_item;\n\n\t\tcur += this_len;\n\t\tdir_item = (struct btrfs_dir_item *)((char *)dir_item +\n\t\t\t\t\t\t     this_len);\n\t}\n\treturn NULL;\n}",
        "code_after_change": "struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n\t\t\t\t\t\t struct btrfs_path *path,\n\t\t\t\t\t\t const char *name, int name_len)\n{\n\tstruct btrfs_dir_item *dir_item;\n\tunsigned long name_ptr;\n\tu32 total_len;\n\tu32 cur = 0;\n\tu32 this_len;\n\tstruct extent_buffer *leaf;\n\n\tleaf = path->nodes[0];\n\tdir_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dir_item);\n\tif (verify_dir_item(root, leaf, dir_item))\n\t\treturn NULL;\n\n\ttotal_len = btrfs_item_size_nr(leaf, path->slots[0]);\n\twhile (cur < total_len) {\n\t\tthis_len = sizeof(*dir_item) +\n\t\t\tbtrfs_dir_name_len(leaf, dir_item) +\n\t\t\tbtrfs_dir_data_len(leaf, dir_item);\n\t\tname_ptr = (unsigned long)(dir_item + 1);\n\n\t\tif (btrfs_dir_name_len(leaf, dir_item) == name_len &&\n\t\t    memcmp_extent_buffer(leaf, name, name_ptr, name_len) == 0)\n\t\t\treturn dir_item;\n\n\t\tcur += this_len;\n\t\tdir_item = (struct btrfs_dir_item *)((char *)dir_item +\n\t\t\t\t\t\t     this_len);\n\t}\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n-static struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n-\t\t\t      struct btrfs_path *path,\n-\t\t\t      const char *name, int name_len)\n+struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n+\t\t\t\t\t\t struct btrfs_path *path,\n+\t\t\t\t\t\t const char *name, int name_len)\n {\n \tstruct btrfs_dir_item *dir_item;\n \tunsigned long name_ptr;",
        "function_modified_lines": {
            "added": [
                "struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,",
                "\t\t\t\t\t\t struct btrfs_path *path,",
                "\t\t\t\t\t\t const char *name, int name_len)"
            ],
            "deleted": [
                "static struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,",
                "\t\t\t      struct btrfs_path *path,",
                "\t\t\t      const char *name, int name_len)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The Btrfs implementation in the Linux kernel before 3.19 does not ensure that the visible xattr state is consistent with a requested replacement, which allows local users to bypass intended ACL settings and gain privileges via standard filesystem operations (1) during an xattr-replacement time window, related to a race condition, or (2) after an xattr-replacement attempt that fails because the data does not fit.",
        "id": 689
    },
    {
        "cve_id": "CVE-2020-29374",
        "code_before_change": "static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)\n{\n\tconst unsigned long num_pages = obj->base.size >> PAGE_SHIFT;\n\tstruct mm_struct *mm = obj->userptr.mm->mm;\n\tstruct page **pvec;\n\tstruct sg_table *pages;\n\tbool active;\n\tint pinned;\n\n\t/* If userspace should engineer that these pages are replaced in\n\t * the vma between us binding this page into the GTT and completion\n\t * of rendering... Their loss. If they change the mapping of their\n\t * pages they need to create a new bo to point to the new vma.\n\t *\n\t * However, that still leaves open the possibility of the vma\n\t * being copied upon fork. Which falls under the same userspace\n\t * synchronisation issue as a regular bo, except that this time\n\t * the process may not be expecting that a particular piece of\n\t * memory is tied to the GPU.\n\t *\n\t * Fortunately, we can hook into the mmu_notifier in order to\n\t * discard the page references prior to anything nasty happening\n\t * to the vma (discard or cloning) which should prevent the more\n\t * egregious cases from causing harm.\n\t */\n\n\tif (obj->userptr.work) {\n\t\t/* active flag should still be held for the pending work */\n\t\tif (IS_ERR(obj->userptr.work))\n\t\t\treturn PTR_ERR(obj->userptr.work);\n\t\telse\n\t\t\treturn -EAGAIN;\n\t}\n\n\tpvec = NULL;\n\tpinned = 0;\n\n\tif (mm == current->mm) {\n\t\tpvec = kvmalloc_array(num_pages, sizeof(struct page *),\n\t\t\t\t      GFP_KERNEL |\n\t\t\t\t      __GFP_NORETRY |\n\t\t\t\t      __GFP_NOWARN);\n\t\tif (pvec) /* defer to worker if malloc fails */\n\t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n\t\t\t\t\t\t       num_pages,\n\t\t\t\t\t\t       !i915_gem_object_is_readonly(obj),\n\t\t\t\t\t\t       pvec);\n\t}\n\n\tactive = false;\n\tif (pinned < 0) {\n\t\tpages = ERR_PTR(pinned);\n\t\tpinned = 0;\n\t} else if (pinned < num_pages) {\n\t\tpages = __i915_gem_userptr_get_pages_schedule(obj);\n\t\tactive = pages == ERR_PTR(-EAGAIN);\n\t} else {\n\t\tpages = __i915_gem_userptr_alloc_pages(obj, pvec, num_pages);\n\t\tactive = !IS_ERR(pages);\n\t}\n\tif (active)\n\t\t__i915_gem_userptr_set_active(obj, true);\n\n\tif (IS_ERR(pages))\n\t\trelease_pages(pvec, pinned);\n\tkvfree(pvec);\n\n\treturn PTR_ERR_OR_ZERO(pages);\n}",
        "code_after_change": "static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)\n{\n\tconst unsigned long num_pages = obj->base.size >> PAGE_SHIFT;\n\tstruct mm_struct *mm = obj->userptr.mm->mm;\n\tstruct page **pvec;\n\tstruct sg_table *pages;\n\tbool active;\n\tint pinned;\n\n\t/* If userspace should engineer that these pages are replaced in\n\t * the vma between us binding this page into the GTT and completion\n\t * of rendering... Their loss. If they change the mapping of their\n\t * pages they need to create a new bo to point to the new vma.\n\t *\n\t * However, that still leaves open the possibility of the vma\n\t * being copied upon fork. Which falls under the same userspace\n\t * synchronisation issue as a regular bo, except that this time\n\t * the process may not be expecting that a particular piece of\n\t * memory is tied to the GPU.\n\t *\n\t * Fortunately, we can hook into the mmu_notifier in order to\n\t * discard the page references prior to anything nasty happening\n\t * to the vma (discard or cloning) which should prevent the more\n\t * egregious cases from causing harm.\n\t */\n\n\tif (obj->userptr.work) {\n\t\t/* active flag should still be held for the pending work */\n\t\tif (IS_ERR(obj->userptr.work))\n\t\t\treturn PTR_ERR(obj->userptr.work);\n\t\telse\n\t\t\treturn -EAGAIN;\n\t}\n\n\tpvec = NULL;\n\tpinned = 0;\n\n\tif (mm == current->mm) {\n\t\tpvec = kvmalloc_array(num_pages, sizeof(struct page *),\n\t\t\t\t      GFP_KERNEL |\n\t\t\t\t      __GFP_NORETRY |\n\t\t\t\t      __GFP_NOWARN);\n\t\t/*\n\t\t * Using __get_user_pages_fast() with a read-only\n\t\t * access is questionable. A read-only page may be\n\t\t * COW-broken, and then this might end up giving\n\t\t * the wrong side of the COW..\n\t\t *\n\t\t * We may or may not care.\n\t\t */\n\t\tif (pvec) /* defer to worker if malloc fails */\n\t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n\t\t\t\t\t\t       num_pages,\n\t\t\t\t\t\t       !i915_gem_object_is_readonly(obj),\n\t\t\t\t\t\t       pvec);\n\t}\n\n\tactive = false;\n\tif (pinned < 0) {\n\t\tpages = ERR_PTR(pinned);\n\t\tpinned = 0;\n\t} else if (pinned < num_pages) {\n\t\tpages = __i915_gem_userptr_get_pages_schedule(obj);\n\t\tactive = pages == ERR_PTR(-EAGAIN);\n\t} else {\n\t\tpages = __i915_gem_userptr_alloc_pages(obj, pvec, num_pages);\n\t\tactive = !IS_ERR(pages);\n\t}\n\tif (active)\n\t\t__i915_gem_userptr_set_active(obj, true);\n\n\tif (IS_ERR(pages))\n\t\trelease_pages(pvec, pinned);\n\tkvfree(pvec);\n\n\treturn PTR_ERR_OR_ZERO(pages);\n}",
        "patch": "--- code before\n+++ code after\n@@ -40,6 +40,14 @@\n \t\t\t\t      GFP_KERNEL |\n \t\t\t\t      __GFP_NORETRY |\n \t\t\t\t      __GFP_NOWARN);\n+\t\t/*\n+\t\t * Using __get_user_pages_fast() with a read-only\n+\t\t * access is questionable. A read-only page may be\n+\t\t * COW-broken, and then this might end up giving\n+\t\t * the wrong side of the COW..\n+\t\t *\n+\t\t * We may or may not care.\n+\t\t */\n \t\tif (pvec) /* defer to worker if malloc fails */\n \t\t\tpinned = __get_user_pages_fast(obj->userptr.ptr,\n \t\t\t\t\t\t       num_pages,",
        "function_modified_lines": {
            "added": [
                "\t\t/*",
                "\t\t * Using __get_user_pages_fast() with a read-only",
                "\t\t * access is questionable. A read-only page may be",
                "\t\t * COW-broken, and then this might end up giving",
                "\t\t * the wrong side of the COW..",
                "\t\t *",
                "\t\t * We may or may not care.",
                "\t\t */"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-863"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.7.3, related to mm/gup.c and mm/huge_memory.c. The get_user_pages (aka gup) implementation, when used for a copy-on-write page, does not properly consider the semantics of read operations and therefore can grant unintended write access, aka CID-17839856fd58.",
        "id": 2670
    },
    {
        "cve_id": "CVE-2023-32250",
        "code_before_change": "static int ksmbd_tcp_readv(struct tcp_transport *t, struct kvec *iov_orig,\n\t\t\t   unsigned int nr_segs, unsigned int to_read,\n\t\t\t   int max_retries)\n{\n\tint length = 0;\n\tint total_read;\n\tunsigned int segs;\n\tstruct msghdr ksmbd_msg;\n\tstruct kvec *iov;\n\tstruct ksmbd_conn *conn = KSMBD_TRANS(t)->conn;\n\n\tiov = get_conn_iovec(t, nr_segs);\n\tif (!iov)\n\t\treturn -ENOMEM;\n\n\tksmbd_msg.msg_control = NULL;\n\tksmbd_msg.msg_controllen = 0;\n\n\tfor (total_read = 0; to_read; total_read += length, to_read -= length) {\n\t\ttry_to_freeze();\n\n\t\tif (!ksmbd_conn_alive(conn)) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t}\n\t\tsegs = kvec_array_init(iov, iov_orig, nr_segs, total_read);\n\n\t\tlength = kernel_recvmsg(t->sock, &ksmbd_msg,\n\t\t\t\t\tiov, segs, to_read, 0);\n\n\t\tif (length == -EINTR) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t} else if (conn->status == KSMBD_SESS_NEED_RECONNECT) {\n\t\t\ttotal_read = -EAGAIN;\n\t\t\tbreak;\n\t\t} else if (length == -ERESTARTSYS || length == -EAGAIN) {\n\t\t\t/*\n\t\t\t * If max_retries is negative, Allow unlimited\n\t\t\t * retries to keep connection with inactive sessions.\n\t\t\t */\n\t\t\tif (max_retries == 0) {\n\t\t\t\ttotal_read = length;\n\t\t\t\tbreak;\n\t\t\t} else if (max_retries > 0) {\n\t\t\t\tmax_retries--;\n\t\t\t}\n\n\t\t\tusleep_range(1000, 2000);\n\t\t\tlength = 0;\n\t\t\tcontinue;\n\t\t} else if (length <= 0) {\n\t\t\ttotal_read = length;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn total_read;\n}",
        "code_after_change": "static int ksmbd_tcp_readv(struct tcp_transport *t, struct kvec *iov_orig,\n\t\t\t   unsigned int nr_segs, unsigned int to_read,\n\t\t\t   int max_retries)\n{\n\tint length = 0;\n\tint total_read;\n\tunsigned int segs;\n\tstruct msghdr ksmbd_msg;\n\tstruct kvec *iov;\n\tstruct ksmbd_conn *conn = KSMBD_TRANS(t)->conn;\n\n\tiov = get_conn_iovec(t, nr_segs);\n\tif (!iov)\n\t\treturn -ENOMEM;\n\n\tksmbd_msg.msg_control = NULL;\n\tksmbd_msg.msg_controllen = 0;\n\n\tfor (total_read = 0; to_read; total_read += length, to_read -= length) {\n\t\ttry_to_freeze();\n\n\t\tif (!ksmbd_conn_alive(conn)) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t}\n\t\tsegs = kvec_array_init(iov, iov_orig, nr_segs, total_read);\n\n\t\tlength = kernel_recvmsg(t->sock, &ksmbd_msg,\n\t\t\t\t\tiov, segs, to_read, 0);\n\n\t\tif (length == -EINTR) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t} else if (ksmbd_conn_need_reconnect(conn)) {\n\t\t\ttotal_read = -EAGAIN;\n\t\t\tbreak;\n\t\t} else if (length == -ERESTARTSYS || length == -EAGAIN) {\n\t\t\t/*\n\t\t\t * If max_retries is negative, Allow unlimited\n\t\t\t * retries to keep connection with inactive sessions.\n\t\t\t */\n\t\t\tif (max_retries == 0) {\n\t\t\t\ttotal_read = length;\n\t\t\t\tbreak;\n\t\t\t} else if (max_retries > 0) {\n\t\t\t\tmax_retries--;\n\t\t\t}\n\n\t\t\tusleep_range(1000, 2000);\n\t\t\tlength = 0;\n\t\t\tcontinue;\n\t\t} else if (length <= 0) {\n\t\t\ttotal_read = length;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn total_read;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,7 +31,7 @@\n \t\tif (length == -EINTR) {\n \t\t\ttotal_read = -ESHUTDOWN;\n \t\t\tbreak;\n-\t\t} else if (conn->status == KSMBD_SESS_NEED_RECONNECT) {\n+\t\t} else if (ksmbd_conn_need_reconnect(conn)) {\n \t\t\ttotal_read = -EAGAIN;\n \t\t\tbreak;\n \t\t} else if (length == -ERESTARTSYS || length == -EAGAIN) {",
        "function_modified_lines": {
            "added": [
                "\t\t} else if (ksmbd_conn_need_reconnect(conn)) {"
            ],
            "deleted": [
                "\t\t} else if (conn->status == KSMBD_SESS_NEED_RECONNECT) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the processing of SMB2_SESSION_SETUP commands. The issue results from the lack of proper locking when performing operations on an object. An attacker can leverage this vulnerability to execute code in the context of the kernel.",
        "id": 4024
    },
    {
        "cve_id": "CVE-2023-32250",
        "code_before_change": "static inline int check_conn_state(struct ksmbd_work *work)\n{\n\tstruct smb_hdr *rsp_hdr;\n\n\tif (ksmbd_conn_exiting(work) || ksmbd_conn_need_reconnect(work)) {\n\t\trsp_hdr = work->response_buf;\n\t\trsp_hdr->Status.CifsError = STATUS_CONNECTION_DISCONNECTED;\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static inline int check_conn_state(struct ksmbd_work *work)\n{\n\tstruct smb_hdr *rsp_hdr;\n\n\tif (ksmbd_conn_exiting(work->conn) ||\n\t    ksmbd_conn_need_reconnect(work->conn)) {\n\t\trsp_hdr = work->response_buf;\n\t\trsp_hdr->Status.CifsError = STATUS_CONNECTION_DISCONNECTED;\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,8 @@\n {\n \tstruct smb_hdr *rsp_hdr;\n \n-\tif (ksmbd_conn_exiting(work) || ksmbd_conn_need_reconnect(work)) {\n+\tif (ksmbd_conn_exiting(work->conn) ||\n+\t    ksmbd_conn_need_reconnect(work->conn)) {\n \t\trsp_hdr = work->response_buf;\n \t\trsp_hdr->Status.CifsError = STATUS_CONNECTION_DISCONNECTED;\n \t\treturn 1;",
        "function_modified_lines": {
            "added": [
                "\tif (ksmbd_conn_exiting(work->conn) ||",
                "\t    ksmbd_conn_need_reconnect(work->conn)) {"
            ],
            "deleted": [
                "\tif (ksmbd_conn_exiting(work) || ksmbd_conn_need_reconnect(work)) {"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the processing of SMB2_SESSION_SETUP commands. The issue results from the lack of proper locking when performing operations on an object. An attacker can leverage this vulnerability to execute code in the context of the kernel.",
        "id": 4020
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "static void init_busid_table(void)\n{\n\t/*\n\t * This also sets the bus_table[i].status to\n\t * STUB_BUSID_OTHER, which is 0.\n\t */\n\tmemset(busid_table, 0, sizeof(busid_table));\n\n\tspin_lock_init(&busid_table_lock);\n}",
        "code_after_change": "static void init_busid_table(void)\n{\n\tint i;\n\n\t/*\n\t * This also sets the bus_table[i].status to\n\t * STUB_BUSID_OTHER, which is 0.\n\t */\n\tmemset(busid_table, 0, sizeof(busid_table));\n\n\tspin_lock_init(&busid_table_lock);\n\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tspin_lock_init(&busid_table[i].busid_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,7 @@\n static void init_busid_table(void)\n {\n+\tint i;\n+\n \t/*\n \t * This also sets the bus_table[i].status to\n \t * STUB_BUSID_OTHER, which is 0.\n@@ -7,4 +9,7 @@\n \tmemset(busid_table, 0, sizeof(busid_table));\n \n \tspin_lock_init(&busid_table_lock);\n+\n+\tfor (i = 0; i < MAX_BUSID; i++)\n+\t\tspin_lock_init(&busid_table[i].busid_lock);\n }",
        "function_modified_lines": {
            "added": [
                "\tint i;",
                "",
                "",
                "\tfor (i = 0; i < MAX_BUSID; i++)",
                "\t\tspin_lock_init(&busid_table[i].busid_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "id": 1835
    },
    {
        "cve_id": "CVE-2018-5814",
        "code_before_change": "static int add_match_busid(char *busid)\n{\n\tint i;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\t/* already registered? */\n\tif (get_busid_idx(busid) >= 0) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tif (!busid_table[i].name[0]) {\n\t\t\tstrlcpy(busid_table[i].name, busid, BUSID_SIZE);\n\t\t\tif ((busid_table[i].status != STUB_BUSID_ALLOC) &&\n\t\t\t    (busid_table[i].status != STUB_BUSID_REMOV))\n\t\t\t\tbusid_table[i].status = STUB_BUSID_ADDED;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
        "code_after_change": "static int add_match_busid(char *busid)\n{\n\tint i;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\t/* already registered? */\n\tif (get_busid_idx(busid) >= 0) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tspin_lock(&busid_table[i].busid_lock);\n\t\tif (!busid_table[i].name[0]) {\n\t\t\tstrlcpy(busid_table[i].name, busid, BUSID_SIZE);\n\t\t\tif ((busid_table[i].status != STUB_BUSID_ALLOC) &&\n\t\t\t    (busid_table[i].status != STUB_BUSID_REMOV))\n\t\t\t\tbusid_table[i].status = STUB_BUSID_ADDED;\n\t\t\tret = 0;\n\t\t\tspin_unlock(&busid_table[i].busid_lock);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&busid_table[i].busid_lock);\n\t}\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,15 +10,19 @@\n \t\tgoto out;\n \t}\n \n-\tfor (i = 0; i < MAX_BUSID; i++)\n+\tfor (i = 0; i < MAX_BUSID; i++) {\n+\t\tspin_lock(&busid_table[i].busid_lock);\n \t\tif (!busid_table[i].name[0]) {\n \t\t\tstrlcpy(busid_table[i].name, busid, BUSID_SIZE);\n \t\t\tif ((busid_table[i].status != STUB_BUSID_ALLOC) &&\n \t\t\t    (busid_table[i].status != STUB_BUSID_REMOV))\n \t\t\t\tbusid_table[i].status = STUB_BUSID_ADDED;\n \t\t\tret = 0;\n+\t\t\tspin_unlock(&busid_table[i].busid_lock);\n \t\t\tbreak;\n \t\t}\n+\t\tspin_unlock(&busid_table[i].busid_lock);\n+\t}\n \n out:\n \tspin_unlock(&busid_table_lock);",
        "function_modified_lines": {
            "added": [
                "\tfor (i = 0; i < MAX_BUSID; i++) {",
                "\t\tspin_lock(&busid_table[i].busid_lock);",
                "\t\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t\tspin_unlock(&busid_table[i].busid_lock);",
                "\t}"
            ],
            "deleted": [
                "\tfor (i = 0; i < MAX_BUSID; i++)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux Kernel before version 4.16.11, 4.14.43, 4.9.102, and 4.4.133, multiple race condition errors when handling probe, disconnect, and rebind operations can be exploited to trigger a use-after-free condition or a NULL pointer dereference by sending multiple USB over IP packets.",
        "id": 1837
    },
    {
        "cve_id": "CVE-2022-20567",
        "code_before_change": "static int pppol2tp_connect(struct socket *sock, struct sockaddr *uservaddr,\n\t\t\t    int sockaddr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppol2tp *sp = (struct sockaddr_pppol2tp *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct l2tp_session *session = NULL;\n\tstruct l2tp_tunnel *tunnel;\n\tstruct pppol2tp_session *ps;\n\tstruct l2tp_session_cfg cfg = { 0, };\n\tint error = 0;\n\tu32 tunnel_id, peer_tunnel_id;\n\tu32 session_id, peer_session_id;\n\tbool drop_refcnt = false;\n\tbool drop_tunnel = false;\n\tint ver = 2;\n\tint fd;\n\n\tlock_sock(sk);\n\n\terror = -EINVAL;\n\tif (sp->sa_protocol != PX_PROTO_OL2TP)\n\t\tgoto end;\n\n\t/* Check for already bound sockets */\n\terror = -EBUSY;\n\tif (sk->sk_state & PPPOX_CONNECTED)\n\t\tgoto end;\n\n\t/* We don't supporting rebinding anyway */\n\terror = -EALREADY;\n\tif (sk->sk_user_data)\n\t\tgoto end; /* socket is already attached */\n\n\t/* Get params from socket address. Handle L2TPv2 and L2TPv3.\n\t * This is nasty because there are different sockaddr_pppol2tp\n\t * structs for L2TPv2, L2TPv3, over IPv4 and IPv6. We use\n\t * the sockaddr size to determine which structure the caller\n\t * is using.\n\t */\n\tpeer_tunnel_id = 0;\n\tif (sockaddr_len == sizeof(struct sockaddr_pppol2tp)) {\n\t\tfd = sp->pppol2tp.fd;\n\t\ttunnel_id = sp->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp->pppol2tp.d_tunnel;\n\t\tsession_id = sp->pppol2tp.s_session;\n\t\tpeer_session_id = sp->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3)) {\n\t\tstruct sockaddr_pppol2tpv3 *sp3 =\n\t\t\t(struct sockaddr_pppol2tpv3 *) sp;\n\t\tver = 3;\n\t\tfd = sp3->pppol2tp.fd;\n\t\ttunnel_id = sp3->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp3->pppol2tp.d_tunnel;\n\t\tsession_id = sp3->pppol2tp.s_session;\n\t\tpeer_session_id = sp3->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpin6)) {\n\t\tstruct sockaddr_pppol2tpin6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpin6 *) sp;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3in6)) {\n\t\tstruct sockaddr_pppol2tpv3in6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpv3in6 *) sp;\n\t\tver = 3;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else {\n\t\terror = -EINVAL;\n\t\tgoto end; /* bad socket address */\n\t}\n\n\t/* Don't bind if tunnel_id is 0 */\n\terror = -EINVAL;\n\tif (tunnel_id == 0)\n\t\tgoto end;\n\n\ttunnel = l2tp_tunnel_get(sock_net(sk), tunnel_id);\n\tif (tunnel)\n\t\tdrop_tunnel = true;\n\n\t/* Special case: create tunnel context if session_id and\n\t * peer_session_id is 0. Otherwise look up tunnel using supplied\n\t * tunnel id.\n\t */\n\tif ((session_id == 0) && (peer_session_id == 0)) {\n\t\tif (tunnel == NULL) {\n\t\t\tstruct l2tp_tunnel_cfg tcfg = {\n\t\t\t\t.encap = L2TP_ENCAPTYPE_UDP,\n\t\t\t\t.debug = 0,\n\t\t\t};\n\t\t\terror = l2tp_tunnel_create(sock_net(sk), fd, ver, tunnel_id, peer_tunnel_id, &tcfg, &tunnel);\n\t\t\tif (error < 0)\n\t\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Error if we can't find the tunnel */\n\t\terror = -ENOENT;\n\t\tif (tunnel == NULL)\n\t\t\tgoto end;\n\n\t\t/* Error if socket is not prepped */\n\t\tif (tunnel->sock == NULL)\n\t\t\tgoto end;\n\t}\n\n\tif (tunnel->recv_payload_hook == NULL)\n\t\ttunnel->recv_payload_hook = pppol2tp_recv_payload_hook;\n\n\tif (tunnel->peer_tunnel_id == 0)\n\t\ttunnel->peer_tunnel_id = peer_tunnel_id;\n\n\tsession = l2tp_session_get(sock_net(sk), tunnel, session_id);\n\tif (session) {\n\t\tdrop_refcnt = true;\n\t\tps = l2tp_session_priv(session);\n\n\t\t/* Using a pre-existing session is fine as long as it hasn't\n\t\t * been connected yet.\n\t\t */\n\t\tmutex_lock(&ps->sk_lock);\n\t\tif (rcu_dereference_protected(ps->sk,\n\t\t\t\t\t      lockdep_is_held(&ps->sk_lock))) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\terror = -EEXIST;\n\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Default MTU must allow space for UDP/L2TP/PPP headers */\n\t\tcfg.mtu = 1500 - PPPOL2TP_HEADER_OVERHEAD;\n\t\tcfg.mru = cfg.mtu;\n\n\t\tsession = l2tp_session_create(sizeof(struct pppol2tp_session),\n\t\t\t\t\t      tunnel, session_id,\n\t\t\t\t\t      peer_session_id, &cfg);\n\t\tif (IS_ERR(session)) {\n\t\t\terror = PTR_ERR(session);\n\t\t\tgoto end;\n\t\t}\n\n\t\tpppol2tp_session_init(session);\n\t\tps = l2tp_session_priv(session);\n\t\tl2tp_session_inc_refcount(session);\n\n\t\tmutex_lock(&ps->sk_lock);\n\t\terror = l2tp_session_register(session, tunnel);\n\t\tif (error < 0) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\tkfree(session);\n\t\t\tgoto end;\n\t\t}\n\t\tdrop_refcnt = true;\n\t}\n\n\t/* Special case: if source & dest session_id == 0x0000, this\n\t * socket is being created to manage the tunnel. Just set up\n\t * the internal context for use by ioctl() and sockopt()\n\t * handlers.\n\t */\n\tif ((session->session_id == 0) &&\n\t    (session->peer_session_id == 0)) {\n\t\terror = 0;\n\t\tgoto out_no_ppp;\n\t}\n\n\t/* The only header we need to worry about is the L2TP\n\t * header. This size is different depending on whether\n\t * sequence numbers are enabled for the data channel.\n\t */\n\tpo->chan.hdrlen = PPPOL2TP_L2TP_HDR_SIZE_NOSEQ;\n\n\tpo->chan.private = sk;\n\tpo->chan.ops\t = &pppol2tp_chan_ops;\n\tpo->chan.mtu\t = session->mtu;\n\n\terror = ppp_register_net_channel(sock_net(sk), &po->chan);\n\tif (error) {\n\t\tmutex_unlock(&ps->sk_lock);\n\t\tgoto end;\n\t}\n\nout_no_ppp:\n\t/* This is how we get the session context from the socket. */\n\tsk->sk_user_data = session;\n\trcu_assign_pointer(ps->sk, sk);\n\tmutex_unlock(&ps->sk_lock);\n\n\t/* Keep the reference we've grabbed on the session: sk doesn't expect\n\t * the session to disappear. pppol2tp_session_destruct() is responsible\n\t * for dropping it.\n\t */\n\tdrop_refcnt = false;\n\n\tsk->sk_state = PPPOX_CONNECTED;\n\tl2tp_info(session, L2TP_MSG_CONTROL, \"%s: created\\n\",\n\t\t  session->name);\n\nend:\n\tif (drop_refcnt)\n\t\tl2tp_session_dec_refcount(session);\n\tif (drop_tunnel)\n\t\tl2tp_tunnel_dec_refcount(tunnel);\n\trelease_sock(sk);\n\n\treturn error;\n}",
        "code_after_change": "static int pppol2tp_connect(struct socket *sock, struct sockaddr *uservaddr,\n\t\t\t    int sockaddr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppol2tp *sp = (struct sockaddr_pppol2tp *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct l2tp_session *session = NULL;\n\tstruct l2tp_tunnel *tunnel;\n\tstruct pppol2tp_session *ps;\n\tstruct l2tp_session_cfg cfg = { 0, };\n\tint error = 0;\n\tu32 tunnel_id, peer_tunnel_id;\n\tu32 session_id, peer_session_id;\n\tbool drop_refcnt = false;\n\tbool drop_tunnel = false;\n\tint ver = 2;\n\tint fd;\n\n\tlock_sock(sk);\n\n\terror = -EINVAL;\n\tif (sp->sa_protocol != PX_PROTO_OL2TP)\n\t\tgoto end;\n\n\t/* Check for already bound sockets */\n\terror = -EBUSY;\n\tif (sk->sk_state & PPPOX_CONNECTED)\n\t\tgoto end;\n\n\t/* We don't supporting rebinding anyway */\n\terror = -EALREADY;\n\tif (sk->sk_user_data)\n\t\tgoto end; /* socket is already attached */\n\n\t/* Get params from socket address. Handle L2TPv2 and L2TPv3.\n\t * This is nasty because there are different sockaddr_pppol2tp\n\t * structs for L2TPv2, L2TPv3, over IPv4 and IPv6. We use\n\t * the sockaddr size to determine which structure the caller\n\t * is using.\n\t */\n\tpeer_tunnel_id = 0;\n\tif (sockaddr_len == sizeof(struct sockaddr_pppol2tp)) {\n\t\tfd = sp->pppol2tp.fd;\n\t\ttunnel_id = sp->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp->pppol2tp.d_tunnel;\n\t\tsession_id = sp->pppol2tp.s_session;\n\t\tpeer_session_id = sp->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3)) {\n\t\tstruct sockaddr_pppol2tpv3 *sp3 =\n\t\t\t(struct sockaddr_pppol2tpv3 *) sp;\n\t\tver = 3;\n\t\tfd = sp3->pppol2tp.fd;\n\t\ttunnel_id = sp3->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp3->pppol2tp.d_tunnel;\n\t\tsession_id = sp3->pppol2tp.s_session;\n\t\tpeer_session_id = sp3->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpin6)) {\n\t\tstruct sockaddr_pppol2tpin6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpin6 *) sp;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3in6)) {\n\t\tstruct sockaddr_pppol2tpv3in6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpv3in6 *) sp;\n\t\tver = 3;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else {\n\t\terror = -EINVAL;\n\t\tgoto end; /* bad socket address */\n\t}\n\n\t/* Don't bind if tunnel_id is 0 */\n\terror = -EINVAL;\n\tif (tunnel_id == 0)\n\t\tgoto end;\n\n\ttunnel = l2tp_tunnel_get(sock_net(sk), tunnel_id);\n\tif (tunnel)\n\t\tdrop_tunnel = true;\n\n\t/* Special case: create tunnel context if session_id and\n\t * peer_session_id is 0. Otherwise look up tunnel using supplied\n\t * tunnel id.\n\t */\n\tif ((session_id == 0) && (peer_session_id == 0)) {\n\t\tif (tunnel == NULL) {\n\t\t\tstruct l2tp_tunnel_cfg tcfg = {\n\t\t\t\t.encap = L2TP_ENCAPTYPE_UDP,\n\t\t\t\t.debug = 0,\n\t\t\t};\n\t\t\terror = l2tp_tunnel_create(sock_net(sk), fd, ver, tunnel_id, peer_tunnel_id, &tcfg, &tunnel);\n\t\t\tif (error < 0)\n\t\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Error if we can't find the tunnel */\n\t\terror = -ENOENT;\n\t\tif (tunnel == NULL)\n\t\t\tgoto end;\n\n\t\t/* Error if socket is not prepped */\n\t\tif (tunnel->sock == NULL)\n\t\t\tgoto end;\n\t}\n\n\tif (tunnel->recv_payload_hook == NULL)\n\t\ttunnel->recv_payload_hook = pppol2tp_recv_payload_hook;\n\n\tif (tunnel->peer_tunnel_id == 0)\n\t\ttunnel->peer_tunnel_id = peer_tunnel_id;\n\n\tsession = l2tp_session_get(sock_net(sk), tunnel, session_id);\n\tif (session) {\n\t\tdrop_refcnt = true;\n\t\tps = l2tp_session_priv(session);\n\n\t\t/* Using a pre-existing session is fine as long as it hasn't\n\t\t * been connected yet.\n\t\t */\n\t\tmutex_lock(&ps->sk_lock);\n\t\tif (rcu_dereference_protected(ps->sk,\n\t\t\t\t\t      lockdep_is_held(&ps->sk_lock))) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\terror = -EEXIST;\n\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Default MTU must allow space for UDP/L2TP/PPP headers */\n\t\tcfg.mtu = 1500 - PPPOL2TP_HEADER_OVERHEAD;\n\t\tcfg.mru = cfg.mtu;\n\n\t\tsession = l2tp_session_create(sizeof(struct pppol2tp_session),\n\t\t\t\t\t      tunnel, session_id,\n\t\t\t\t\t      peer_session_id, &cfg);\n\t\tif (IS_ERR(session)) {\n\t\t\terror = PTR_ERR(session);\n\t\t\tgoto end;\n\t\t}\n\n\t\tpppol2tp_session_init(session);\n\t\tps = l2tp_session_priv(session);\n\t\tl2tp_session_inc_refcount(session);\n\n\t\tmutex_lock(&ps->sk_lock);\n\t\terror = l2tp_session_register(session, tunnel);\n\t\tif (error < 0) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\tkfree(session);\n\t\t\tgoto end;\n\t\t}\n\t\tdrop_refcnt = true;\n\t}\n\n\t/* Special case: if source & dest session_id == 0x0000, this\n\t * socket is being created to manage the tunnel. Just set up\n\t * the internal context for use by ioctl() and sockopt()\n\t * handlers.\n\t */\n\tif ((session->session_id == 0) &&\n\t    (session->peer_session_id == 0)) {\n\t\terror = 0;\n\t\tgoto out_no_ppp;\n\t}\n\n\t/* The only header we need to worry about is the L2TP\n\t * header. This size is different depending on whether\n\t * sequence numbers are enabled for the data channel.\n\t */\n\tpo->chan.hdrlen = PPPOL2TP_L2TP_HDR_SIZE_NOSEQ;\n\n\tpo->chan.private = sk;\n\tpo->chan.ops\t = &pppol2tp_chan_ops;\n\tpo->chan.mtu\t = session->mtu;\n\n\terror = ppp_register_net_channel(sock_net(sk), &po->chan);\n\tif (error) {\n\t\tmutex_unlock(&ps->sk_lock);\n\t\tgoto end;\n\t}\n\nout_no_ppp:\n\t/* This is how we get the session context from the socket. */\n\tsock_hold(sk);\n\tsk->sk_user_data = session;\n\trcu_assign_pointer(ps->sk, sk);\n\tmutex_unlock(&ps->sk_lock);\n\n\t/* Keep the reference we've grabbed on the session: sk doesn't expect\n\t * the session to disappear. pppol2tp_session_destruct() is responsible\n\t * for dropping it.\n\t */\n\tdrop_refcnt = false;\n\n\tsk->sk_state = PPPOX_CONNECTED;\n\tl2tp_info(session, L2TP_MSG_CONTROL, \"%s: created\\n\",\n\t\t  session->name);\n\nend:\n\tif (drop_refcnt)\n\t\tl2tp_session_dec_refcount(session);\n\tif (drop_tunnel)\n\t\tl2tp_tunnel_dec_refcount(tunnel);\n\trelease_sock(sk);\n\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -187,6 +187,7 @@\n \n out_no_ppp:\n \t/* This is how we get the session context from the socket. */\n+\tsock_hold(sk);\n \tsk->sk_user_data = session;\n \trcu_assign_pointer(ps->sk, sk);\n \tmutex_unlock(&ps->sk_lock);",
        "function_modified_lines": {
            "added": [
                "\tsock_hold(sk);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In pppol2tp_create of l2tp_ppp.c, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-186777253References: Upstream kernel",
        "id": 3398
    },
    {
        "cve_id": "CVE-2017-6874",
        "code_before_change": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tatomic_set(&new->count, 0);\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n\t\tucounts = NULL;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
        "code_after_change": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tnew->count = 0;\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (ucounts->count == INT_MAX)\n\t\tucounts = NULL;\n\telse\n\t\tucounts->count += 1;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,7 +14,7 @@\n \n \t\tnew->ns = ns;\n \t\tnew->uid = uid;\n-\t\tatomic_set(&new->count, 0);\n+\t\tnew->count = 0;\n \n \t\tspin_lock_irq(&ucounts_lock);\n \t\tucounts = find_ucounts(ns, uid, hashent);\n@@ -25,8 +25,10 @@\n \t\t\tucounts = new;\n \t\t}\n \t}\n-\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n+\tif (ucounts->count == INT_MAX)\n \t\tucounts = NULL;\n+\telse\n+\t\tucounts->count += 1;\n \tspin_unlock_irq(&ucounts_lock);\n \treturn ucounts;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tnew->count = 0;",
                "\tif (ucounts->count == INT_MAX)",
                "\telse",
                "\t\tucounts->count += 1;"
            ],
            "deleted": [
                "\t\tatomic_set(&new->count, 0);",
                "\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/ucount.c in the Linux kernel through 4.10.2 allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via crafted system calls that leverage certain decrement behavior that causes incorrect interaction between put_ucounts and get_ucounts.",
        "id": 1488
    },
    {
        "cve_id": "CVE-2020-36558",
        "code_before_change": "int vt_ioctl(struct tty_struct *tty,\n\t     unsigned int cmd, unsigned long arg)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tstruct console_font_op op;\t/* used in multiple places here */\n\tunsigned int console;\n\tunsigned char ucval;\n\tunsigned int uival;\n\tvoid __user *up = (void __user *)arg;\n\tint i, perm;\n\tint ret = 0;\n\n\tconsole = vc->vc_num;\n\n\n\tif (!vc_cons_allocated(console)) { \t/* impossible? */\n\t\tret = -ENOIOCTLCMD;\n\t\tgoto out;\n\t}\n\n\n\t/*\n\t * To have permissions to do most of the vt ioctls, we either have\n\t * to be the owner of the tty, or have CAP_SYS_TTY_CONFIG.\n\t */\n\tperm = 0;\n\tif (current->signal->tty == tty || capable(CAP_SYS_TTY_CONFIG))\n\t\tperm = 1;\n \n\tswitch (cmd) {\n\tcase TIOCLINUX:\n\t\tret = tioclinux(tty, arg);\n\t\tbreak;\n\tcase KIOCSOUND:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\t/*\n\t\t * The use of PIT_TICK_RATE is historic, it used to be\n\t\t * the platform-dependent CLOCK_TICK_RATE between 2.6.12\n\t\t * and 2.6.36, which was a minor but unfortunate ABI\n\t\t * change. kd_mksound is locked by the input layer.\n\t\t */\n\t\tif (arg)\n\t\t\targ = PIT_TICK_RATE / arg;\n\t\tkd_mksound(arg, 0);\n\t\tbreak;\n\n\tcase KDMKTONE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t{\n\t\tunsigned int ticks, count;\n\t\t\n\t\t/*\n\t\t * Generate the tone for the appropriate number of ticks.\n\t\t * If the time is zero, turn off sound ourselves.\n\t\t */\n\t\tticks = msecs_to_jiffies((arg >> 16) & 0xffff);\n\t\tcount = ticks ? (arg & 0xffff) : 0;\n\t\tif (count)\n\t\t\tcount = PIT_TICK_RATE / count;\n\t\tkd_mksound(count, ticks);\n\t\tbreak;\n\t}\n\n\tcase KDGKBTYPE:\n\t\t/*\n\t\t * this is naïve.\n\t\t */\n\t\tucval = KB_101;\n\t\tret = put_user(ucval, (char __user *)arg);\n\t\tbreak;\n\n\t\t/*\n\t\t * These cannot be implemented on any machine that implements\n\t\t * ioperm() in user level (such as Alpha PCs) or not at all.\n\t\t *\n\t\t * XXX: you should never use these, just call ioperm directly..\n\t\t */\n#ifdef CONFIG_X86\n\tcase KDADDIO:\n\tcase KDDELIO:\n\t\t/*\n\t\t * KDADDIO and KDDELIO may be able to add ports beyond what\n\t\t * we reject here, but to be safe...\n\t\t *\n\t\t * These are locked internally via sys_ioperm\n\t\t */\n\t\tif (arg < GPFIRST || arg > GPLAST) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tret = ksys_ioperm(arg, 1, (cmd == KDADDIO)) ? -ENXIO : 0;\n\t\tbreak;\n\n\tcase KDENABIO:\n\tcase KDDISABIO:\n\t\tret = ksys_ioperm(GPFIRST, GPNUM,\n\t\t\t\t  (cmd == KDENABIO)) ? -ENXIO : 0;\n\t\tbreak;\n#endif\n\n\t/* Linux m68k/i386 interface for setting the keyboard delay/repeat rate */\n\t\t\n\tcase KDKBDREP:\n\t{\n\t\tstruct kbd_repeat kbrep;\n\t\t\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&kbrep, up, sizeof(struct kbd_repeat))) {\n\t\t\tret =  -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tret = kbd_rate(&kbrep);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &kbrep, sizeof(struct kbd_repeat)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase KDSETMODE:\n\t\t/*\n\t\t * currently, setting the mode from KD_TEXT to KD_GRAPHICS\n\t\t * doesn't do a whole lot. i'm not sure if it should do any\n\t\t * restoration of modes or what...\n\t\t *\n\t\t * XXX It should at least call into the driver, fbdev's definitely\n\t\t * need to restore their engine state. --BenH\n\t\t */\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tswitch (arg) {\n\t\tcase KD_GRAPHICS:\n\t\t\tbreak;\n\t\tcase KD_TEXT0:\n\t\tcase KD_TEXT1:\n\t\t\targ = KD_TEXT;\n\t\tcase KD_TEXT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\t/* FIXME: this needs the console lock extending */\n\t\tif (vc->vc_mode == (unsigned char) arg)\n\t\t\tbreak;\n\t\tvc->vc_mode = (unsigned char) arg;\n\t\tif (console != fg_console)\n\t\t\tbreak;\n\t\t/*\n\t\t * explicitly blank/unblank the screen if switching modes\n\t\t */\n\t\tconsole_lock();\n\t\tif (arg == KD_TEXT)\n\t\t\tdo_unblank_screen(1);\n\t\telse\n\t\t\tdo_blank_screen(1);\n\t\tconsole_unlock();\n\t\tbreak;\n\n\tcase KDGETMODE:\n\t\tuival = vc->vc_mode;\n\t\tgoto setint;\n\n\tcase KDMAPDISP:\n\tcase KDUNMAPDISP:\n\t\t/*\n\t\t * these work like a combination of mmap and KDENABIO.\n\t\t * this could be easily finished.\n\t\t */\n\t\tret = -EINVAL;\n\t\tbreak;\n\n\tcase KDSKBMODE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tret = vt_do_kdskbmode(console, arg);\n\t\tif (ret == 0)\n\t\t\ttty_ldisc_flush(tty);\n\t\tbreak;\n\n\tcase KDGKBMODE:\n\t\tuival = vt_do_kdgkbmode(console);\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\t/* this could be folded into KDSKBMODE, but for compatibility\n\t   reasons it is not so easy to fold KDGKBMETA into KDGKBMODE */\n\tcase KDSKBMETA:\n\t\tret = vt_do_kdskbmeta(console, arg);\n\t\tbreak;\n\n\tcase KDGKBMETA:\n\t\t/* FIXME: should review whether this is worth locking */\n\t\tuival = vt_do_kdgkbmeta(console);\n\tsetint:\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\tcase KDGETKEYCODE:\n\tcase KDSETKEYCODE:\n\t\tif(!capable(CAP_SYS_TTY_CONFIG))\n\t\t\tperm = 0;\n\t\tret = vt_do_kbkeycode_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\tcase KDGKBENT:\n\tcase KDSKBENT:\n\t\tret = vt_do_kdsk_ioctl(cmd, up, perm, console);\n\t\tbreak;\n\n\tcase KDGKBSENT:\n\tcase KDSKBSENT:\n\t\tret = vt_do_kdgkb_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\t/* Diacritical processing. Handled in keyboard.c as it has\n\t   to operate on the keyboard locks and structures */\n\tcase KDGKBDIACR:\n\tcase KDGKBDIACRUC:\n\tcase KDSKBDIACR:\n\tcase KDSKBDIACRUC:\n\t\tret = vt_do_diacrit(cmd, up, perm);\n\t\tbreak;\n\n\t/* the ioctls below read/set the flags usually shown in the leds */\n\t/* don't use them - they will go away without warning */\n\tcase KDGKBLED:\n\tcase KDSKBLED:\n\tcase KDGETLED:\n\tcase KDSETLED:\n\t\tret = vt_do_kdskled(console, cmd, arg, perm);\n\t\tbreak;\n\n\t/*\n\t * A process can indicate its willingness to accept signals\n\t * generated by pressing an appropriate key combination.\n\t * Thus, one can have a daemon that e.g. spawns a new console\n\t * upon a keypress and then changes to it.\n\t * See also the kbrequest field of inittab(5).\n\t */\n\tcase KDSIGACCEPT:\n\t{\n\t\tif (!perm || !capable(CAP_KILL))\n\t\t\treturn -EPERM;\n\t\tif (!valid_signal(arg) || arg < 1 || arg == SIGKILL)\n\t\t\tret = -EINVAL;\n\t\telse {\n\t\t\tspin_lock_irq(&vt_spawn_con.lock);\n\t\t\tput_pid(vt_spawn_con.pid);\n\t\t\tvt_spawn_con.pid = get_pid(task_pid(current));\n\t\t\tvt_spawn_con.sig = arg;\n\t\t\tspin_unlock_irq(&vt_spawn_con.lock);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_SETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&tmp, up, sizeof(struct vt_mode))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (tmp.mode != VT_AUTO && tmp.mode != VT_PROCESS) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tconsole_lock();\n\t\tvc->vt_mode = tmp;\n\t\t/* the frsig is ignored, so we set it to 0 */\n\t\tvc->vt_mode.frsig = 0;\n\t\tput_pid(vc->vt_pid);\n\t\tvc->vt_pid = get_pid(task_pid(current));\n\t\t/* no switch is required -- saw@shade.msu.ru */\n\t\tvc->vt_newvt = -1;\n\t\tconsole_unlock();\n\t\tbreak;\n\t}\n\n\tcase VT_GETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\t\tint rc;\n\n\t\tconsole_lock();\n\t\tmemcpy(&tmp, &vc->vt_mode, sizeof(struct vt_mode));\n\t\tconsole_unlock();\n\n\t\trc = copy_to_user(up, &tmp, sizeof(struct vt_mode));\n\t\tif (rc)\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns global vt state. Note that VT 0 is always open, since\n\t * it's an alias for the current VT, and people can't use it here.\n\t * We cannot return state for more than 16 VTs, since v_state is short.\n\t */\n\tcase VT_GETSTATE:\n\t{\n\t\tstruct vt_stat __user *vtstat = up;\n\t\tunsigned short state, mask;\n\n\t\t/* Review: FIXME: Console lock ? */\n\t\tif (put_user(fg_console + 1, &vtstat->v_active))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tstate = 1;\t/* /dev/tty0 is always open */\n\t\t\tfor (i = 0, mask = 2; i < MAX_NR_CONSOLES && mask;\n\t\t\t\t\t\t\t++i, mask <<= 1)\n\t\t\t\tif (VT_IS_IN_USE(i))\n\t\t\t\t\tstate |= mask;\n\t\t\tret = put_user(state, &vtstat->v_state);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns the first available (non-opened) console.\n\t */\n\tcase VT_OPENQRY:\n\t\t/* FIXME: locking ? - but then this is a stupid API */\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; ++i)\n\t\t\tif (! VT_IS_IN_USE(i))\n\t\t\t\tbreak;\n\t\tuival = i < MAX_NR_CONSOLES ? (i+1) : -1;\n\t\tgoto setint;\t\t \n\n\t/*\n\t * ioctl(fd, VT_ACTIVATE, num) will cause us to switch to vt # num,\n\t * with num >= 1 (switches to vt 0, our console, are not allowed, just\n\t * to preserve sanity).\n\t */\n\tcase VT_ACTIVATE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret =  -ENXIO;\n\t\telse {\n\t\t\targ--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(arg);\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tset_console(arg);\n\t\t}\n\t\tbreak;\n\n\tcase VT_SETACTIVATE:\n\t{\n\t\tstruct vt_setactivate vsa;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&vsa, (struct vt_setactivate __user *)arg,\n\t\t\t\t\tsizeof(struct vt_setactivate))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (vsa.console == 0 || vsa.console > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse {\n\t\t\tvsa.console = array_index_nospec(vsa.console,\n\t\t\t\t\t\t\t MAX_NR_CONSOLES + 1);\n\t\t\tvsa.console--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(vsa.console);\n\t\t\tif (ret == 0) {\n\t\t\t\tstruct vc_data *nvc;\n\t\t\t\t/* This is safe providing we don't drop the\n\t\t\t\t   console sem between vc_allocate and\n\t\t\t\t   finishing referencing nvc */\n\t\t\t\tnvc = vc_cons[vsa.console].d;\n\t\t\t\tnvc->vt_mode = vsa.mode;\n\t\t\t\tnvc->vt_mode.frsig = 0;\n\t\t\t\tput_pid(nvc->vt_pid);\n\t\t\t\tnvc->vt_pid = get_pid(task_pid(current));\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\t/* Commence switch and lock */\n\t\t\t/* Review set_console locks */\n\t\t\tset_console(vsa.console);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * wait until the specified VT has been activated\n\t */\n\tcase VT_WAITACTIVE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse\n\t\t\tret = vt_waitactive(arg);\n\t\tbreak;\n\n\t/*\n\t * If a vt is under process control, the kernel will not switch to it\n\t * immediately, but postpone the operation until the process calls this\n\t * ioctl, allowing the switch to complete.\n\t *\n\t * According to the X sources this is the behavior:\n\t *\t0:\tpending switch-from not OK\n\t *\t1:\tpending switch-from OK\n\t *\t2:\tcompleted switch-to OK\n\t */\n\tcase VT_RELDISP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tconsole_lock();\n\t\tif (vc->vt_mode.mode != VT_PROCESS) {\n\t\t\tconsole_unlock();\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\t/*\n\t\t * Switching-from response\n\t\t */\n\t\tif (vc->vt_newvt >= 0) {\n\t\t\tif (arg == 0)\n\t\t\t\t/*\n\t\t\t\t * Switch disallowed, so forget we were trying\n\t\t\t\t * to do it.\n\t\t\t\t */\n\t\t\t\tvc->vt_newvt = -1;\n\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * The current vt has been released, so\n\t\t\t\t * complete the switch.\n\t\t\t\t */\n\t\t\t\tint newvt;\n\t\t\t\tnewvt = vc->vt_newvt;\n\t\t\t\tvc->vt_newvt = -1;\n\t\t\t\tret = vc_allocate(newvt);\n\t\t\t\tif (ret) {\n\t\t\t\t\tconsole_unlock();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/*\n\t\t\t\t * When we actually do the console switch,\n\t\t\t\t * make sure we are atomic with respect to\n\t\t\t\t * other console switches..\n\t\t\t\t */\n\t\t\t\tcomplete_change_console(vc_cons[newvt].d);\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * Switched-to response\n\t\t\t */\n\t\t\t/*\n\t\t\t * If it's just an ACK, ignore it\n\t\t\t */\n\t\t\tif (arg != VT_ACKACQ)\n\t\t\t\tret = -EINVAL;\n\t\t}\n\t\tconsole_unlock();\n\t\tbreak;\n\n\t /*\n\t  * Disallocate memory associated to VT (but leave VT1)\n\t  */\n\t case VT_DISALLOCATE:\n\t\tif (arg > MAX_NR_CONSOLES) {\n\t\t\tret = -ENXIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (arg == 0)\n\t\t\tvt_disallocate_all();\n\t\telse\n\t\t\tret = vt_disallocate(--arg);\n\t\tbreak;\n\n\tcase VT_RESIZE:\n\t{\n\t\tstruct vt_sizes __user *vtsizes = up;\n\t\tstruct vc_data *vc;\n\n\t\tushort ll,cc;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (get_user(ll, &vtsizes->v_rows) ||\n\t\t    get_user(cc, &vtsizes->v_cols))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tconsole_lock();\n\t\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\t\tvc = vc_cons[i].d;\n\n\t\t\t\tif (vc) {\n\t\t\t\t\tvc->vc_resize_user = 1;\n\t\t\t\t\t/* FIXME: review v tty lock */\n\t\t\t\t\tvc_resize(vc_cons[i].d, cc, ll);\n\t\t\t\t}\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_RESIZEX:\n\t{\n\t\tstruct vt_consize v;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&v, up, sizeof(struct vt_consize)))\n\t\t\treturn -EFAULT;\n\t\t/* FIXME: Should check the copies properly */\n\t\tif (!v.v_vlin)\n\t\t\tv.v_vlin = vc->vc_scan_lines;\n\t\tif (v.v_clin) {\n\t\t\tint rows = v.v_vlin/v.v_clin;\n\t\t\tif (v.v_rows != rows) {\n\t\t\t\tif (v.v_rows) /* Parameters don't add up */\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_rows = rows;\n\t\t\t}\n\t\t}\n\t\tif (v.v_vcol && v.v_ccol) {\n\t\t\tint cols = v.v_vcol/v.v_ccol;\n\t\t\tif (v.v_cols != cols) {\n\t\t\t\tif (v.v_cols)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_cols = cols;\n\t\t\t}\n\t\t}\n\n\t\tif (v.v_clin > 32)\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\tif (!vc_cons[i].d)\n\t\t\t\tcontinue;\n\t\t\tconsole_lock();\n\t\t\tif (v.v_vlin)\n\t\t\t\tvc_cons[i].d->vc_scan_lines = v.v_vlin;\n\t\t\tif (v.v_clin)\n\t\t\t\tvc_cons[i].d->vc_font.height = v.v_clin;\n\t\t\tvc_cons[i].d->vc_resize_user = 1;\n\t\t\tvc_resize(vc_cons[i].d, v.v_cols, v.v_rows);\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase PIO_FONT: {\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\top.op = KD_FONT_OP_SET;\n\t\top.flags = KD_FONT_FLAG_OLD | KD_FONT_FLAG_DONT_RECALC;\t/* Compatibility */\n\t\top.width = 8;\n\t\top.height = 0;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase GIO_FONT: {\n\t\top.op = KD_FONT_OP_GET;\n\t\top.flags = KD_FONT_FLAG_OLD;\n\t\top.width = 8;\n\t\top.height = 32;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase PIO_CMAP:\n                if (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t                ret = con_set_cmap(up);\n\t\tbreak;\n\n\tcase GIO_CMAP:\n                ret = con_get_cmap(up);\n\t\tbreak;\n\n\tcase PIO_FONTX:\n\tcase GIO_FONTX:\n\t\tret = do_fontx_ioctl(cmd, up, perm, &op);\n\t\tbreak;\n\n\tcase PIO_FONTRESET:\n\t{\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n#ifdef BROKEN_GRAPHICS_PROGRAMS\n\t\t/* With BROKEN_GRAPHICS_PROGRAMS defined, the default\n\t\t   font is not saved. */\n\t\tret = -ENOSYS;\n\t\tbreak;\n#else\n\t\t{\n\t\top.op = KD_FONT_OP_SET_DEFAULT;\n\t\top.data = NULL;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tconsole_lock();\n\t\tcon_set_default_unimap(vc_cons[fg_console].d);\n\t\tconsole_unlock();\n\t\tbreak;\n\t\t}\n#endif\n\t}\n\n\tcase KDFONTOP: {\n\t\tif (copy_from_user(&op, up, sizeof(op))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!perm && op.op != KD_FONT_OP_GET)\n\t\t\treturn -EPERM;\n\t\tret = con_font_op(vc, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &op, sizeof(op)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase PIO_SCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_old(up);\n\t\tbreak;\n\n\tcase GIO_SCRNMAP:\n\t\tret = con_get_trans_old(up);\n\t\tbreak;\n\n\tcase PIO_UNISCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_new(up);\n\t\tbreak;\n\n\tcase GIO_UNISCRNMAP:\n\t\tret = con_get_trans_new(up);\n\t\tbreak;\n\n\tcase PIO_UNIMAPCLR:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tcon_clear_unimap(vc);\n\t\tbreak;\n\n\tcase PIO_UNIMAP:\n\tcase GIO_UNIMAP:\n\t\tret = do_unimap_ioctl(cmd, up, perm, vc);\n\t\tbreak;\n\n\tcase VT_LOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 1;\n\t\tbreak;\n\tcase VT_UNLOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 0;\n\t\tbreak;\n\tcase VT_GETHIFONTMASK:\n\t\tret = put_user(vc->vc_hi_font_mask,\n\t\t\t\t\t(unsigned short __user *)arg);\n\t\tbreak;\n\tcase VT_WAITEVENT:\n\t\tret = vt_event_wait_ioctl((struct vt_event __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENOIOCTLCMD;\n\t}\nout:\n\treturn ret;\n}",
        "code_after_change": "int vt_ioctl(struct tty_struct *tty,\n\t     unsigned int cmd, unsigned long arg)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tstruct console_font_op op;\t/* used in multiple places here */\n\tunsigned int console;\n\tunsigned char ucval;\n\tunsigned int uival;\n\tvoid __user *up = (void __user *)arg;\n\tint i, perm;\n\tint ret = 0;\n\n\tconsole = vc->vc_num;\n\n\n\tif (!vc_cons_allocated(console)) { \t/* impossible? */\n\t\tret = -ENOIOCTLCMD;\n\t\tgoto out;\n\t}\n\n\n\t/*\n\t * To have permissions to do most of the vt ioctls, we either have\n\t * to be the owner of the tty, or have CAP_SYS_TTY_CONFIG.\n\t */\n\tperm = 0;\n\tif (current->signal->tty == tty || capable(CAP_SYS_TTY_CONFIG))\n\t\tperm = 1;\n \n\tswitch (cmd) {\n\tcase TIOCLINUX:\n\t\tret = tioclinux(tty, arg);\n\t\tbreak;\n\tcase KIOCSOUND:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\t/*\n\t\t * The use of PIT_TICK_RATE is historic, it used to be\n\t\t * the platform-dependent CLOCK_TICK_RATE between 2.6.12\n\t\t * and 2.6.36, which was a minor but unfortunate ABI\n\t\t * change. kd_mksound is locked by the input layer.\n\t\t */\n\t\tif (arg)\n\t\t\targ = PIT_TICK_RATE / arg;\n\t\tkd_mksound(arg, 0);\n\t\tbreak;\n\n\tcase KDMKTONE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t{\n\t\tunsigned int ticks, count;\n\t\t\n\t\t/*\n\t\t * Generate the tone for the appropriate number of ticks.\n\t\t * If the time is zero, turn off sound ourselves.\n\t\t */\n\t\tticks = msecs_to_jiffies((arg >> 16) & 0xffff);\n\t\tcount = ticks ? (arg & 0xffff) : 0;\n\t\tif (count)\n\t\t\tcount = PIT_TICK_RATE / count;\n\t\tkd_mksound(count, ticks);\n\t\tbreak;\n\t}\n\n\tcase KDGKBTYPE:\n\t\t/*\n\t\t * this is naïve.\n\t\t */\n\t\tucval = KB_101;\n\t\tret = put_user(ucval, (char __user *)arg);\n\t\tbreak;\n\n\t\t/*\n\t\t * These cannot be implemented on any machine that implements\n\t\t * ioperm() in user level (such as Alpha PCs) or not at all.\n\t\t *\n\t\t * XXX: you should never use these, just call ioperm directly..\n\t\t */\n#ifdef CONFIG_X86\n\tcase KDADDIO:\n\tcase KDDELIO:\n\t\t/*\n\t\t * KDADDIO and KDDELIO may be able to add ports beyond what\n\t\t * we reject here, but to be safe...\n\t\t *\n\t\t * These are locked internally via sys_ioperm\n\t\t */\n\t\tif (arg < GPFIRST || arg > GPLAST) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tret = ksys_ioperm(arg, 1, (cmd == KDADDIO)) ? -ENXIO : 0;\n\t\tbreak;\n\n\tcase KDENABIO:\n\tcase KDDISABIO:\n\t\tret = ksys_ioperm(GPFIRST, GPNUM,\n\t\t\t\t  (cmd == KDENABIO)) ? -ENXIO : 0;\n\t\tbreak;\n#endif\n\n\t/* Linux m68k/i386 interface for setting the keyboard delay/repeat rate */\n\t\t\n\tcase KDKBDREP:\n\t{\n\t\tstruct kbd_repeat kbrep;\n\t\t\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&kbrep, up, sizeof(struct kbd_repeat))) {\n\t\t\tret =  -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tret = kbd_rate(&kbrep);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &kbrep, sizeof(struct kbd_repeat)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase KDSETMODE:\n\t\t/*\n\t\t * currently, setting the mode from KD_TEXT to KD_GRAPHICS\n\t\t * doesn't do a whole lot. i'm not sure if it should do any\n\t\t * restoration of modes or what...\n\t\t *\n\t\t * XXX It should at least call into the driver, fbdev's definitely\n\t\t * need to restore their engine state. --BenH\n\t\t */\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tswitch (arg) {\n\t\tcase KD_GRAPHICS:\n\t\t\tbreak;\n\t\tcase KD_TEXT0:\n\t\tcase KD_TEXT1:\n\t\t\targ = KD_TEXT;\n\t\tcase KD_TEXT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\t/* FIXME: this needs the console lock extending */\n\t\tif (vc->vc_mode == (unsigned char) arg)\n\t\t\tbreak;\n\t\tvc->vc_mode = (unsigned char) arg;\n\t\tif (console != fg_console)\n\t\t\tbreak;\n\t\t/*\n\t\t * explicitly blank/unblank the screen if switching modes\n\t\t */\n\t\tconsole_lock();\n\t\tif (arg == KD_TEXT)\n\t\t\tdo_unblank_screen(1);\n\t\telse\n\t\t\tdo_blank_screen(1);\n\t\tconsole_unlock();\n\t\tbreak;\n\n\tcase KDGETMODE:\n\t\tuival = vc->vc_mode;\n\t\tgoto setint;\n\n\tcase KDMAPDISP:\n\tcase KDUNMAPDISP:\n\t\t/*\n\t\t * these work like a combination of mmap and KDENABIO.\n\t\t * this could be easily finished.\n\t\t */\n\t\tret = -EINVAL;\n\t\tbreak;\n\n\tcase KDSKBMODE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tret = vt_do_kdskbmode(console, arg);\n\t\tif (ret == 0)\n\t\t\ttty_ldisc_flush(tty);\n\t\tbreak;\n\n\tcase KDGKBMODE:\n\t\tuival = vt_do_kdgkbmode(console);\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\t/* this could be folded into KDSKBMODE, but for compatibility\n\t   reasons it is not so easy to fold KDGKBMETA into KDGKBMODE */\n\tcase KDSKBMETA:\n\t\tret = vt_do_kdskbmeta(console, arg);\n\t\tbreak;\n\n\tcase KDGKBMETA:\n\t\t/* FIXME: should review whether this is worth locking */\n\t\tuival = vt_do_kdgkbmeta(console);\n\tsetint:\n\t\tret = put_user(uival, (int __user *)arg);\n\t\tbreak;\n\n\tcase KDGETKEYCODE:\n\tcase KDSETKEYCODE:\n\t\tif(!capable(CAP_SYS_TTY_CONFIG))\n\t\t\tperm = 0;\n\t\tret = vt_do_kbkeycode_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\tcase KDGKBENT:\n\tcase KDSKBENT:\n\t\tret = vt_do_kdsk_ioctl(cmd, up, perm, console);\n\t\tbreak;\n\n\tcase KDGKBSENT:\n\tcase KDSKBSENT:\n\t\tret = vt_do_kdgkb_ioctl(cmd, up, perm);\n\t\tbreak;\n\n\t/* Diacritical processing. Handled in keyboard.c as it has\n\t   to operate on the keyboard locks and structures */\n\tcase KDGKBDIACR:\n\tcase KDGKBDIACRUC:\n\tcase KDSKBDIACR:\n\tcase KDSKBDIACRUC:\n\t\tret = vt_do_diacrit(cmd, up, perm);\n\t\tbreak;\n\n\t/* the ioctls below read/set the flags usually shown in the leds */\n\t/* don't use them - they will go away without warning */\n\tcase KDGKBLED:\n\tcase KDSKBLED:\n\tcase KDGETLED:\n\tcase KDSETLED:\n\t\tret = vt_do_kdskled(console, cmd, arg, perm);\n\t\tbreak;\n\n\t/*\n\t * A process can indicate its willingness to accept signals\n\t * generated by pressing an appropriate key combination.\n\t * Thus, one can have a daemon that e.g. spawns a new console\n\t * upon a keypress and then changes to it.\n\t * See also the kbrequest field of inittab(5).\n\t */\n\tcase KDSIGACCEPT:\n\t{\n\t\tif (!perm || !capable(CAP_KILL))\n\t\t\treturn -EPERM;\n\t\tif (!valid_signal(arg) || arg < 1 || arg == SIGKILL)\n\t\t\tret = -EINVAL;\n\t\telse {\n\t\t\tspin_lock_irq(&vt_spawn_con.lock);\n\t\t\tput_pid(vt_spawn_con.pid);\n\t\t\tvt_spawn_con.pid = get_pid(task_pid(current));\n\t\t\tvt_spawn_con.sig = arg;\n\t\t\tspin_unlock_irq(&vt_spawn_con.lock);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_SETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&tmp, up, sizeof(struct vt_mode))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (tmp.mode != VT_AUTO && tmp.mode != VT_PROCESS) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tconsole_lock();\n\t\tvc->vt_mode = tmp;\n\t\t/* the frsig is ignored, so we set it to 0 */\n\t\tvc->vt_mode.frsig = 0;\n\t\tput_pid(vc->vt_pid);\n\t\tvc->vt_pid = get_pid(task_pid(current));\n\t\t/* no switch is required -- saw@shade.msu.ru */\n\t\tvc->vt_newvt = -1;\n\t\tconsole_unlock();\n\t\tbreak;\n\t}\n\n\tcase VT_GETMODE:\n\t{\n\t\tstruct vt_mode tmp;\n\t\tint rc;\n\n\t\tconsole_lock();\n\t\tmemcpy(&tmp, &vc->vt_mode, sizeof(struct vt_mode));\n\t\tconsole_unlock();\n\n\t\trc = copy_to_user(up, &tmp, sizeof(struct vt_mode));\n\t\tif (rc)\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns global vt state. Note that VT 0 is always open, since\n\t * it's an alias for the current VT, and people can't use it here.\n\t * We cannot return state for more than 16 VTs, since v_state is short.\n\t */\n\tcase VT_GETSTATE:\n\t{\n\t\tstruct vt_stat __user *vtstat = up;\n\t\tunsigned short state, mask;\n\n\t\t/* Review: FIXME: Console lock ? */\n\t\tif (put_user(fg_console + 1, &vtstat->v_active))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tstate = 1;\t/* /dev/tty0 is always open */\n\t\t\tfor (i = 0, mask = 2; i < MAX_NR_CONSOLES && mask;\n\t\t\t\t\t\t\t++i, mask <<= 1)\n\t\t\t\tif (VT_IS_IN_USE(i))\n\t\t\t\t\tstate |= mask;\n\t\t\tret = put_user(state, &vtstat->v_state);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * Returns the first available (non-opened) console.\n\t */\n\tcase VT_OPENQRY:\n\t\t/* FIXME: locking ? - but then this is a stupid API */\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; ++i)\n\t\t\tif (! VT_IS_IN_USE(i))\n\t\t\t\tbreak;\n\t\tuival = i < MAX_NR_CONSOLES ? (i+1) : -1;\n\t\tgoto setint;\t\t \n\n\t/*\n\t * ioctl(fd, VT_ACTIVATE, num) will cause us to switch to vt # num,\n\t * with num >= 1 (switches to vt 0, our console, are not allowed, just\n\t * to preserve sanity).\n\t */\n\tcase VT_ACTIVATE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret =  -ENXIO;\n\t\telse {\n\t\t\targ--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(arg);\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tset_console(arg);\n\t\t}\n\t\tbreak;\n\n\tcase VT_SETACTIVATE:\n\t{\n\t\tstruct vt_setactivate vsa;\n\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tif (copy_from_user(&vsa, (struct vt_setactivate __user *)arg,\n\t\t\t\t\tsizeof(struct vt_setactivate))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (vsa.console == 0 || vsa.console > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse {\n\t\t\tvsa.console = array_index_nospec(vsa.console,\n\t\t\t\t\t\t\t MAX_NR_CONSOLES + 1);\n\t\t\tvsa.console--;\n\t\t\tconsole_lock();\n\t\t\tret = vc_allocate(vsa.console);\n\t\t\tif (ret == 0) {\n\t\t\t\tstruct vc_data *nvc;\n\t\t\t\t/* This is safe providing we don't drop the\n\t\t\t\t   console sem between vc_allocate and\n\t\t\t\t   finishing referencing nvc */\n\t\t\t\tnvc = vc_cons[vsa.console].d;\n\t\t\t\tnvc->vt_mode = vsa.mode;\n\t\t\t\tnvc->vt_mode.frsig = 0;\n\t\t\t\tput_pid(nvc->vt_pid);\n\t\t\t\tnvc->vt_pid = get_pid(task_pid(current));\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\t/* Commence switch and lock */\n\t\t\t/* Review set_console locks */\n\t\t\tset_console(vsa.console);\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t * wait until the specified VT has been activated\n\t */\n\tcase VT_WAITACTIVE:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (arg == 0 || arg > MAX_NR_CONSOLES)\n\t\t\tret = -ENXIO;\n\t\telse\n\t\t\tret = vt_waitactive(arg);\n\t\tbreak;\n\n\t/*\n\t * If a vt is under process control, the kernel will not switch to it\n\t * immediately, but postpone the operation until the process calls this\n\t * ioctl, allowing the switch to complete.\n\t *\n\t * According to the X sources this is the behavior:\n\t *\t0:\tpending switch-from not OK\n\t *\t1:\tpending switch-from OK\n\t *\t2:\tcompleted switch-to OK\n\t */\n\tcase VT_RELDISP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tconsole_lock();\n\t\tif (vc->vt_mode.mode != VT_PROCESS) {\n\t\t\tconsole_unlock();\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\t/*\n\t\t * Switching-from response\n\t\t */\n\t\tif (vc->vt_newvt >= 0) {\n\t\t\tif (arg == 0)\n\t\t\t\t/*\n\t\t\t\t * Switch disallowed, so forget we were trying\n\t\t\t\t * to do it.\n\t\t\t\t */\n\t\t\t\tvc->vt_newvt = -1;\n\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * The current vt has been released, so\n\t\t\t\t * complete the switch.\n\t\t\t\t */\n\t\t\t\tint newvt;\n\t\t\t\tnewvt = vc->vt_newvt;\n\t\t\t\tvc->vt_newvt = -1;\n\t\t\t\tret = vc_allocate(newvt);\n\t\t\t\tif (ret) {\n\t\t\t\t\tconsole_unlock();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/*\n\t\t\t\t * When we actually do the console switch,\n\t\t\t\t * make sure we are atomic with respect to\n\t\t\t\t * other console switches..\n\t\t\t\t */\n\t\t\t\tcomplete_change_console(vc_cons[newvt].d);\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * Switched-to response\n\t\t\t */\n\t\t\t/*\n\t\t\t * If it's just an ACK, ignore it\n\t\t\t */\n\t\t\tif (arg != VT_ACKACQ)\n\t\t\t\tret = -EINVAL;\n\t\t}\n\t\tconsole_unlock();\n\t\tbreak;\n\n\t /*\n\t  * Disallocate memory associated to VT (but leave VT1)\n\t  */\n\t case VT_DISALLOCATE:\n\t\tif (arg > MAX_NR_CONSOLES) {\n\t\t\tret = -ENXIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (arg == 0)\n\t\t\tvt_disallocate_all();\n\t\telse\n\t\t\tret = vt_disallocate(--arg);\n\t\tbreak;\n\n\tcase VT_RESIZE:\n\t{\n\t\tstruct vt_sizes __user *vtsizes = up;\n\t\tstruct vc_data *vc;\n\n\t\tushort ll,cc;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (get_user(ll, &vtsizes->v_rows) ||\n\t\t    get_user(cc, &vtsizes->v_cols))\n\t\t\tret = -EFAULT;\n\t\telse {\n\t\t\tconsole_lock();\n\t\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\t\tvc = vc_cons[i].d;\n\n\t\t\t\tif (vc) {\n\t\t\t\t\tvc->vc_resize_user = 1;\n\t\t\t\t\t/* FIXME: review v tty lock */\n\t\t\t\t\tvc_resize(vc_cons[i].d, cc, ll);\n\t\t\t\t}\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase VT_RESIZEX:\n\t{\n\t\tstruct vt_consize v;\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&v, up, sizeof(struct vt_consize)))\n\t\t\treturn -EFAULT;\n\t\t/* FIXME: Should check the copies properly */\n\t\tif (!v.v_vlin)\n\t\t\tv.v_vlin = vc->vc_scan_lines;\n\t\tif (v.v_clin) {\n\t\t\tint rows = v.v_vlin/v.v_clin;\n\t\t\tif (v.v_rows != rows) {\n\t\t\t\tif (v.v_rows) /* Parameters don't add up */\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_rows = rows;\n\t\t\t}\n\t\t}\n\t\tif (v.v_vcol && v.v_ccol) {\n\t\t\tint cols = v.v_vcol/v.v_ccol;\n\t\t\tif (v.v_cols != cols) {\n\t\t\t\tif (v.v_cols)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tv.v_cols = cols;\n\t\t\t}\n\t\t}\n\n\t\tif (v.v_clin > 32)\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\tstruct vc_data *vcp;\n\n\t\t\tif (!vc_cons[i].d)\n\t\t\t\tcontinue;\n\t\t\tconsole_lock();\n\t\t\tvcp = vc_cons[i].d;\n\t\t\tif (vcp) {\n\t\t\t\tif (v.v_vlin)\n\t\t\t\t\tvcp->vc_scan_lines = v.v_vlin;\n\t\t\t\tif (v.v_clin)\n\t\t\t\t\tvcp->vc_font.height = v.v_clin;\n\t\t\t\tvcp->vc_resize_user = 1;\n\t\t\t\tvc_resize(vcp, v.v_cols, v.v_rows);\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase PIO_FONT: {\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\top.op = KD_FONT_OP_SET;\n\t\top.flags = KD_FONT_FLAG_OLD | KD_FONT_FLAG_DONT_RECALC;\t/* Compatibility */\n\t\top.width = 8;\n\t\top.height = 0;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase GIO_FONT: {\n\t\top.op = KD_FONT_OP_GET;\n\t\top.flags = KD_FONT_FLAG_OLD;\n\t\top.width = 8;\n\t\top.height = 32;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tbreak;\n\t}\n\n\tcase PIO_CMAP:\n                if (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t                ret = con_set_cmap(up);\n\t\tbreak;\n\n\tcase GIO_CMAP:\n                ret = con_get_cmap(up);\n\t\tbreak;\n\n\tcase PIO_FONTX:\n\tcase GIO_FONTX:\n\t\tret = do_fontx_ioctl(cmd, up, perm, &op);\n\t\tbreak;\n\n\tcase PIO_FONTRESET:\n\t{\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n#ifdef BROKEN_GRAPHICS_PROGRAMS\n\t\t/* With BROKEN_GRAPHICS_PROGRAMS defined, the default\n\t\t   font is not saved. */\n\t\tret = -ENOSYS;\n\t\tbreak;\n#else\n\t\t{\n\t\top.op = KD_FONT_OP_SET_DEFAULT;\n\t\top.data = NULL;\n\t\tret = con_font_op(vc_cons[fg_console].d, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tconsole_lock();\n\t\tcon_set_default_unimap(vc_cons[fg_console].d);\n\t\tconsole_unlock();\n\t\tbreak;\n\t\t}\n#endif\n\t}\n\n\tcase KDFONTOP: {\n\t\tif (copy_from_user(&op, up, sizeof(op))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!perm && op.op != KD_FONT_OP_GET)\n\t\t\treturn -EPERM;\n\t\tret = con_font_op(vc, &op);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (copy_to_user(up, &op, sizeof(op)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\t}\n\n\tcase PIO_SCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_old(up);\n\t\tbreak;\n\n\tcase GIO_SCRNMAP:\n\t\tret = con_get_trans_old(up);\n\t\tbreak;\n\n\tcase PIO_UNISCRNMAP:\n\t\tif (!perm)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tret = con_set_trans_new(up);\n\t\tbreak;\n\n\tcase GIO_UNISCRNMAP:\n\t\tret = con_get_trans_new(up);\n\t\tbreak;\n\n\tcase PIO_UNIMAPCLR:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tcon_clear_unimap(vc);\n\t\tbreak;\n\n\tcase PIO_UNIMAP:\n\tcase GIO_UNIMAP:\n\t\tret = do_unimap_ioctl(cmd, up, perm, vc);\n\t\tbreak;\n\n\tcase VT_LOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 1;\n\t\tbreak;\n\tcase VT_UNLOCKSWITCH:\n\t\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\t\treturn -EPERM;\n\t\tvt_dont_switch = 0;\n\t\tbreak;\n\tcase VT_GETHIFONTMASK:\n\t\tret = put_user(vc->vc_hi_font_mask,\n\t\t\t\t\t(unsigned short __user *)arg);\n\t\tbreak;\n\tcase VT_WAITEVENT:\n\t\tret = vt_event_wait_ioctl((struct vt_event __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENOIOCTLCMD;\n\t}\nout:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -544,15 +544,20 @@\n \t\t\treturn -EINVAL;\n \n \t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n+\t\t\tstruct vc_data *vcp;\n+\n \t\t\tif (!vc_cons[i].d)\n \t\t\t\tcontinue;\n \t\t\tconsole_lock();\n-\t\t\tif (v.v_vlin)\n-\t\t\t\tvc_cons[i].d->vc_scan_lines = v.v_vlin;\n-\t\t\tif (v.v_clin)\n-\t\t\t\tvc_cons[i].d->vc_font.height = v.v_clin;\n-\t\t\tvc_cons[i].d->vc_resize_user = 1;\n-\t\t\tvc_resize(vc_cons[i].d, v.v_cols, v.v_rows);\n+\t\t\tvcp = vc_cons[i].d;\n+\t\t\tif (vcp) {\n+\t\t\t\tif (v.v_vlin)\n+\t\t\t\t\tvcp->vc_scan_lines = v.v_vlin;\n+\t\t\t\tif (v.v_clin)\n+\t\t\t\t\tvcp->vc_font.height = v.v_clin;\n+\t\t\t\tvcp->vc_resize_user = 1;\n+\t\t\t\tvc_resize(vcp, v.v_cols, v.v_rows);\n+\t\t\t}\n \t\t\tconsole_unlock();\n \t\t}\n \t\tbreak;",
        "function_modified_lines": {
            "added": [
                "\t\t\tstruct vc_data *vcp;",
                "",
                "\t\t\tvcp = vc_cons[i].d;",
                "\t\t\tif (vcp) {",
                "\t\t\t\tif (v.v_vlin)",
                "\t\t\t\t\tvcp->vc_scan_lines = v.v_vlin;",
                "\t\t\t\tif (v.v_clin)",
                "\t\t\t\t\tvcp->vc_font.height = v.v_clin;",
                "\t\t\t\tvcp->vc_resize_user = 1;",
                "\t\t\t\tvc_resize(vcp, v.v_cols, v.v_rows);",
                "\t\t\t}"
            ],
            "deleted": [
                "\t\t\tif (v.v_vlin)",
                "\t\t\t\tvc_cons[i].d->vc_scan_lines = v.v_vlin;",
                "\t\t\tif (v.v_clin)",
                "\t\t\t\tvc_cons[i].d->vc_font.height = v.v_clin;",
                "\t\t\tvc_cons[i].d->vc_resize_user = 1;",
                "\t\t\tvc_resize(vc_cons[i].d, v.v_cols, v.v_rows);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476"
        ],
        "cve_description": "A race condition in the Linux kernel before 5.5.7 involving VT_RESIZEX could lead to a NULL pointer dereference and general protection fault.",
        "id": 2767
    },
    {
        "cve_id": "CVE-2021-29265",
        "code_before_change": "static ssize_t usbip_sockfd_store(struct device *dev, struct device_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\tstruct stub_device *sdev = dev_get_drvdata(dev);\n\tint sockfd = 0;\n\tstruct socket *socket;\n\tint rv;\n\n\tif (!sdev) {\n\t\tdev_err(dev, \"sdev is null\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\trv = sscanf(buf, \"%d\", &sockfd);\n\tif (rv != 1)\n\t\treturn -EINVAL;\n\n\tif (sockfd != -1) {\n\t\tint err;\n\n\t\tdev_info(dev, \"stub up\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\n\t\tif (sdev->ud.status != SDEV_ST_AVAILABLE) {\n\t\t\tdev_err(dev, \"not ready\\n\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tsocket = sockfd_lookup(sockfd, &err);\n\t\tif (!socket) {\n\t\t\tdev_err(dev, \"failed to lookup sock\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (socket->type != SOCK_STREAM) {\n\t\t\tdev_err(dev, \"Expecting SOCK_STREAM - found %d\",\n\t\t\t\tsocket->type);\n\t\t\tgoto sock_err;\n\t\t}\n\n\t\tsdev->ud.tcp_socket = socket;\n\t\tsdev->ud.sockfd = sockfd;\n\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\tsdev->ud.tcp_rx = kthread_get_run(stub_rx_loop, &sdev->ud,\n\t\t\t\t\t\t  \"stub_rx\");\n\t\tsdev->ud.tcp_tx = kthread_get_run(stub_tx_loop, &sdev->ud,\n\t\t\t\t\t\t  \"stub_tx\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tsdev->ud.status = SDEV_ST_USED;\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t} else {\n\t\tdev_info(dev, \"stub down\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tif (sdev->ud.status != SDEV_ST_USED)\n\t\t\tgoto err;\n\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\tusbip_event_add(&sdev->ud, SDEV_EVENT_DOWN);\n\t}\n\n\treturn count;\n\nsock_err:\n\tsockfd_put(socket);\nerr:\n\tspin_unlock_irq(&sdev->ud.lock);\n\treturn -EINVAL;\n}",
        "code_after_change": "static ssize_t usbip_sockfd_store(struct device *dev, struct device_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\tstruct stub_device *sdev = dev_get_drvdata(dev);\n\tint sockfd = 0;\n\tstruct socket *socket;\n\tint rv;\n\tstruct task_struct *tcp_rx = NULL;\n\tstruct task_struct *tcp_tx = NULL;\n\n\tif (!sdev) {\n\t\tdev_err(dev, \"sdev is null\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\trv = sscanf(buf, \"%d\", &sockfd);\n\tif (rv != 1)\n\t\treturn -EINVAL;\n\n\tif (sockfd != -1) {\n\t\tint err;\n\n\t\tdev_info(dev, \"stub up\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\n\t\tif (sdev->ud.status != SDEV_ST_AVAILABLE) {\n\t\t\tdev_err(dev, \"not ready\\n\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tsocket = sockfd_lookup(sockfd, &err);\n\t\tif (!socket) {\n\t\t\tdev_err(dev, \"failed to lookup sock\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (socket->type != SOCK_STREAM) {\n\t\t\tdev_err(dev, \"Expecting SOCK_STREAM - found %d\",\n\t\t\t\tsocket->type);\n\t\t\tgoto sock_err;\n\t\t}\n\n\t\t/* unlock and create threads and get tasks */\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\t\ttcp_rx = kthread_create(stub_rx_loop, &sdev->ud, \"stub_rx\");\n\t\tif (IS_ERR(tcp_rx)) {\n\t\t\tsockfd_put(socket);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttcp_tx = kthread_create(stub_tx_loop, &sdev->ud, \"stub_tx\");\n\t\tif (IS_ERR(tcp_tx)) {\n\t\t\tkthread_stop(tcp_rx);\n\t\t\tsockfd_put(socket);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* get task structs now */\n\t\tget_task_struct(tcp_rx);\n\t\tget_task_struct(tcp_tx);\n\n\t\t/* lock and update sdev->ud state */\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tsdev->ud.tcp_socket = socket;\n\t\tsdev->ud.sockfd = sockfd;\n\t\tsdev->ud.tcp_rx = tcp_rx;\n\t\tsdev->ud.tcp_tx = tcp_tx;\n\t\tsdev->ud.status = SDEV_ST_USED;\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\twake_up_process(sdev->ud.tcp_rx);\n\t\twake_up_process(sdev->ud.tcp_tx);\n\n\t} else {\n\t\tdev_info(dev, \"stub down\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tif (sdev->ud.status != SDEV_ST_USED)\n\t\t\tgoto err;\n\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\tusbip_event_add(&sdev->ud, SDEV_EVENT_DOWN);\n\t}\n\n\treturn count;\n\nsock_err:\n\tsockfd_put(socket);\nerr:\n\tspin_unlock_irq(&sdev->ud.lock);\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,8 @@\n \tint sockfd = 0;\n \tstruct socket *socket;\n \tint rv;\n+\tstruct task_struct *tcp_rx = NULL;\n+\tstruct task_struct *tcp_tx = NULL;\n \n \tif (!sdev) {\n \t\tdev_err(dev, \"sdev is null\\n\");\n@@ -39,19 +41,35 @@\n \t\t\tgoto sock_err;\n \t\t}\n \n+\t\t/* unlock and create threads and get tasks */\n+\t\tspin_unlock_irq(&sdev->ud.lock);\n+\t\ttcp_rx = kthread_create(stub_rx_loop, &sdev->ud, \"stub_rx\");\n+\t\tif (IS_ERR(tcp_rx)) {\n+\t\t\tsockfd_put(socket);\n+\t\t\treturn -EINVAL;\n+\t\t}\n+\t\ttcp_tx = kthread_create(stub_tx_loop, &sdev->ud, \"stub_tx\");\n+\t\tif (IS_ERR(tcp_tx)) {\n+\t\t\tkthread_stop(tcp_rx);\n+\t\t\tsockfd_put(socket);\n+\t\t\treturn -EINVAL;\n+\t\t}\n+\n+\t\t/* get task structs now */\n+\t\tget_task_struct(tcp_rx);\n+\t\tget_task_struct(tcp_tx);\n+\n+\t\t/* lock and update sdev->ud state */\n+\t\tspin_lock_irq(&sdev->ud.lock);\n \t\tsdev->ud.tcp_socket = socket;\n \t\tsdev->ud.sockfd = sockfd;\n-\n+\t\tsdev->ud.tcp_rx = tcp_rx;\n+\t\tsdev->ud.tcp_tx = tcp_tx;\n+\t\tsdev->ud.status = SDEV_ST_USED;\n \t\tspin_unlock_irq(&sdev->ud.lock);\n \n-\t\tsdev->ud.tcp_rx = kthread_get_run(stub_rx_loop, &sdev->ud,\n-\t\t\t\t\t\t  \"stub_rx\");\n-\t\tsdev->ud.tcp_tx = kthread_get_run(stub_tx_loop, &sdev->ud,\n-\t\t\t\t\t\t  \"stub_tx\");\n-\n-\t\tspin_lock_irq(&sdev->ud.lock);\n-\t\tsdev->ud.status = SDEV_ST_USED;\n-\t\tspin_unlock_irq(&sdev->ud.lock);\n+\t\twake_up_process(sdev->ud.tcp_rx);\n+\t\twake_up_process(sdev->ud.tcp_tx);\n \n \t} else {\n \t\tdev_info(dev, \"stub down\\n\");",
        "function_modified_lines": {
            "added": [
                "\tstruct task_struct *tcp_rx = NULL;",
                "\tstruct task_struct *tcp_tx = NULL;",
                "\t\t/* unlock and create threads and get tasks */",
                "\t\tspin_unlock_irq(&sdev->ud.lock);",
                "\t\ttcp_rx = kthread_create(stub_rx_loop, &sdev->ud, \"stub_rx\");",
                "\t\tif (IS_ERR(tcp_rx)) {",
                "\t\t\tsockfd_put(socket);",
                "\t\t\treturn -EINVAL;",
                "\t\t}",
                "\t\ttcp_tx = kthread_create(stub_tx_loop, &sdev->ud, \"stub_tx\");",
                "\t\tif (IS_ERR(tcp_tx)) {",
                "\t\t\tkthread_stop(tcp_rx);",
                "\t\t\tsockfd_put(socket);",
                "\t\t\treturn -EINVAL;",
                "\t\t}",
                "",
                "\t\t/* get task structs now */",
                "\t\tget_task_struct(tcp_rx);",
                "\t\tget_task_struct(tcp_tx);",
                "",
                "\t\t/* lock and update sdev->ud state */",
                "\t\tspin_lock_irq(&sdev->ud.lock);",
                "\t\tsdev->ud.tcp_rx = tcp_rx;",
                "\t\tsdev->ud.tcp_tx = tcp_tx;",
                "\t\tsdev->ud.status = SDEV_ST_USED;",
                "\t\twake_up_process(sdev->ud.tcp_rx);",
                "\t\twake_up_process(sdev->ud.tcp_tx);"
            ],
            "deleted": [
                "",
                "\t\tsdev->ud.tcp_rx = kthread_get_run(stub_rx_loop, &sdev->ud,",
                "\t\t\t\t\t\t  \"stub_rx\");",
                "\t\tsdev->ud.tcp_tx = kthread_get_run(stub_tx_loop, &sdev->ud,",
                "\t\t\t\t\t\t  \"stub_tx\");",
                "",
                "\t\tspin_lock_irq(&sdev->ud.lock);",
                "\t\tsdev->ud.status = SDEV_ST_USED;",
                "\t\tspin_unlock_irq(&sdev->ud.lock);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.11.7. usbip_sockfd_store in drivers/usb/usbip/stub_dev.c allows attackers to cause a denial of service (GPF) because the stub-up sequence has race conditions during an update of the local and shared status, aka CID-9380afd6df70.",
        "id": 2947
    },
    {
        "cve_id": "CVE-2013-0871",
        "code_before_change": "static int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up(task, 1);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT,\n\t\t\t    ptrace_trapping_sleep_fn, TASK_UNINTERRUPTIBLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}",
        "code_after_change": "static int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT,\n\t\t\t    ptrace_trapping_sleep_fn, TASK_UNINTERRUPTIBLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -81,7 +81,7 @@\n \t */\n \tif (task_is_stopped(task) &&\n \t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n-\t\tsignal_wake_up(task, 1);\n+\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n \n \tspin_unlock(&task->sighand->siglock);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tsignal_wake_up_state(task, __TASK_STOPPED);"
            ],
            "deleted": [
                "\t\tsignal_wake_up(task, 1);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death.",
        "id": 164
    },
    {
        "cve_id": "CVE-2013-0871",
        "code_before_change": "int ptrace_request(struct task_struct *child, long request,\n\t\t   unsigned long addr, unsigned long data)\n{\n\tbool seized = child->ptrace & PT_SEIZED;\n\tint ret = -EIO;\n\tsiginfo_t siginfo, *si;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\tunsigned long flags;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\treturn generic_ptrace_peekdata(child, addr, data);\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\treturn generic_ptrace_pokedata(child, addr, data);\n\n#ifdef PTRACE_OLDSETOPTIONS\n\tcase PTRACE_OLDSETOPTIONS:\n#endif\n\tcase PTRACE_SETOPTIONS:\n\t\tret = ptrace_setoptions(child, data);\n\t\tbreak;\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user(child->ptrace_message, datalp);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user(datavp, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tif (copy_from_user(&siginfo, datavp, sizeof siginfo))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_INTERRUPT:\n\t\t/*\n\t\t * Stop tracee without any side-effect on signal or job\n\t\t * control.  At least one trap is guaranteed to happen\n\t\t * after this request.  If @child is already trapped, the\n\t\t * current trap is not disturbed and another trap will\n\t\t * happen after the current trap is ended with PTRACE_CONT.\n\t\t *\n\t\t * The actual trap might not be PTRACE_EVENT_STOP trap but\n\t\t * the pending condition is cleared regardless.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * INTERRUPT doesn't disturb existing trap sans one\n\t\t * exception.  If ptracer issued LISTEN for the current\n\t\t * STOP, this INTERRUPT should clear LISTEN and re-trap\n\t\t * tracee into STOP.\n\t\t */\n\t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n\t\t\tsignal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n\n\t\tunlock_task_sighand(child, &flags);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase PTRACE_LISTEN:\n\t\t/*\n\t\t * Listen for events.  Tracee must be in STOP.  It's not\n\t\t * resumed per-se but is not considered to be in TRACED by\n\t\t * wait(2) or ptrace(2).  If an async event (e.g. group\n\t\t * stop state change) happens, tracee will enter STOP trap\n\t\t * again.  Alternatively, ptracer can issue INTERRUPT to\n\t\t * finish listening and re-trap tracee into STOP.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\tsi = child->last_siginfo;\n\t\tif (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {\n\t\t\tchild->jobctl |= JOBCTL_LISTENING;\n\t\t\t/*\n\t\t\t * If NOTIFY is set, it means event happened between\n\t\t\t * start of this trap and now.  Trigger re-trap.\n\t\t\t */\n\t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n\t\t\t\tsignal_wake_up(child, true);\n\t\t\tret = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t\tbreak;\n\n\tcase PTRACE_DETACH:\t /* detach a process that was attached. */\n\t\tret = ptrace_detach(child, data);\n\t\tbreak;\n\n#ifdef CONFIG_BINFMT_ELF_FDPIC\n\tcase PTRACE_GETFDPIC: {\n\t\tstruct mm_struct *mm = get_task_mm(child);\n\t\tunsigned long tmp = 0;\n\n\t\tret = -ESRCH;\n\t\tif (!mm)\n\t\t\tbreak;\n\n\t\tswitch (addr) {\n\t\tcase PTRACE_GETFDPIC_EXEC:\n\t\t\ttmp = mm->context.exec_fdpic_loadmap;\n\t\t\tbreak;\n\t\tcase PTRACE_GETFDPIC_INTERP:\n\t\t\ttmp = mm->context.interp_fdpic_loadmap;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tmmput(mm);\n\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n\tcase PTRACE_SINGLESTEP:\n#endif\n#ifdef PTRACE_SINGLEBLOCK\n\tcase PTRACE_SINGLEBLOCK:\n#endif\n#ifdef PTRACE_SYSEMU\n\tcase PTRACE_SYSEMU:\n\tcase PTRACE_SYSEMU_SINGLESTEP:\n#endif\n\tcase PTRACE_SYSCALL:\n\tcase PTRACE_CONT:\n\t\treturn ptrace_resume(child, request, data);\n\n\tcase PTRACE_KILL:\n\t\tif (child->exit_state)\t/* already dead */\n\t\t\treturn 0;\n\t\treturn ptrace_resume(child, request, SIGKILL);\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct iovec __user *uiov = datavp;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(kiov.iov_base, &uiov->iov_base) ||\n\t\t    __get_user(kiov.iov_len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "int ptrace_request(struct task_struct *child, long request,\n\t\t   unsigned long addr, unsigned long data)\n{\n\tbool seized = child->ptrace & PT_SEIZED;\n\tint ret = -EIO;\n\tsiginfo_t siginfo, *si;\n\tvoid __user *datavp = (void __user *) data;\n\tunsigned long __user *datalp = datavp;\n\tunsigned long flags;\n\n\tswitch (request) {\n\tcase PTRACE_PEEKTEXT:\n\tcase PTRACE_PEEKDATA:\n\t\treturn generic_ptrace_peekdata(child, addr, data);\n\tcase PTRACE_POKETEXT:\n\tcase PTRACE_POKEDATA:\n\t\treturn generic_ptrace_pokedata(child, addr, data);\n\n#ifdef PTRACE_OLDSETOPTIONS\n\tcase PTRACE_OLDSETOPTIONS:\n#endif\n\tcase PTRACE_SETOPTIONS:\n\t\tret = ptrace_setoptions(child, data);\n\t\tbreak;\n\tcase PTRACE_GETEVENTMSG:\n\t\tret = put_user(child->ptrace_message, datalp);\n\t\tbreak;\n\n\tcase PTRACE_GETSIGINFO:\n\t\tret = ptrace_getsiginfo(child, &siginfo);\n\t\tif (!ret)\n\t\t\tret = copy_siginfo_to_user(datavp, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_SETSIGINFO:\n\t\tif (copy_from_user(&siginfo, datavp, sizeof siginfo))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = ptrace_setsiginfo(child, &siginfo);\n\t\tbreak;\n\n\tcase PTRACE_INTERRUPT:\n\t\t/*\n\t\t * Stop tracee without any side-effect on signal or job\n\t\t * control.  At least one trap is guaranteed to happen\n\t\t * after this request.  If @child is already trapped, the\n\t\t * current trap is not disturbed and another trap will\n\t\t * happen after the current trap is ended with PTRACE_CONT.\n\t\t *\n\t\t * The actual trap might not be PTRACE_EVENT_STOP trap but\n\t\t * the pending condition is cleared regardless.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * INTERRUPT doesn't disturb existing trap sans one\n\t\t * exception.  If ptracer issued LISTEN for the current\n\t\t * STOP, this INTERRUPT should clear LISTEN and re-trap\n\t\t * tracee into STOP.\n\t\t */\n\t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n\n\t\tunlock_task_sighand(child, &flags);\n\t\tret = 0;\n\t\tbreak;\n\n\tcase PTRACE_LISTEN:\n\t\t/*\n\t\t * Listen for events.  Tracee must be in STOP.  It's not\n\t\t * resumed per-se but is not considered to be in TRACED by\n\t\t * wait(2) or ptrace(2).  If an async event (e.g. group\n\t\t * stop state change) happens, tracee will enter STOP trap\n\t\t * again.  Alternatively, ptracer can issue INTERRUPT to\n\t\t * finish listening and re-trap tracee into STOP.\n\t\t */\n\t\tif (unlikely(!seized || !lock_task_sighand(child, &flags)))\n\t\t\tbreak;\n\n\t\tsi = child->last_siginfo;\n\t\tif (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {\n\t\t\tchild->jobctl |= JOBCTL_LISTENING;\n\t\t\t/*\n\t\t\t * If NOTIFY is set, it means event happened between\n\t\t\t * start of this trap and now.  Trigger re-trap.\n\t\t\t */\n\t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n\t\t\t\tptrace_signal_wake_up(child, true);\n\t\t\tret = 0;\n\t\t}\n\t\tunlock_task_sighand(child, &flags);\n\t\tbreak;\n\n\tcase PTRACE_DETACH:\t /* detach a process that was attached. */\n\t\tret = ptrace_detach(child, data);\n\t\tbreak;\n\n#ifdef CONFIG_BINFMT_ELF_FDPIC\n\tcase PTRACE_GETFDPIC: {\n\t\tstruct mm_struct *mm = get_task_mm(child);\n\t\tunsigned long tmp = 0;\n\n\t\tret = -ESRCH;\n\t\tif (!mm)\n\t\t\tbreak;\n\n\t\tswitch (addr) {\n\t\tcase PTRACE_GETFDPIC_EXEC:\n\t\t\ttmp = mm->context.exec_fdpic_loadmap;\n\t\t\tbreak;\n\t\tcase PTRACE_GETFDPIC_INTERP:\n\t\t\ttmp = mm->context.interp_fdpic_loadmap;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tmmput(mm);\n\n\t\tret = put_user(tmp, datalp);\n\t\tbreak;\n\t}\n#endif\n\n#ifdef PTRACE_SINGLESTEP\n\tcase PTRACE_SINGLESTEP:\n#endif\n#ifdef PTRACE_SINGLEBLOCK\n\tcase PTRACE_SINGLEBLOCK:\n#endif\n#ifdef PTRACE_SYSEMU\n\tcase PTRACE_SYSEMU:\n\tcase PTRACE_SYSEMU_SINGLESTEP:\n#endif\n\tcase PTRACE_SYSCALL:\n\tcase PTRACE_CONT:\n\t\treturn ptrace_resume(child, request, data);\n\n\tcase PTRACE_KILL:\n\t\tif (child->exit_state)\t/* already dead */\n\t\t\treturn 0;\n\t\treturn ptrace_resume(child, request, SIGKILL);\n\n#ifdef CONFIG_HAVE_ARCH_TRACEHOOK\n\tcase PTRACE_GETREGSET:\n\tcase PTRACE_SETREGSET:\n\t{\n\t\tstruct iovec kiov;\n\t\tstruct iovec __user *uiov = datavp;\n\n\t\tif (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\n\t\tif (__get_user(kiov.iov_base, &uiov->iov_base) ||\n\t\t    __get_user(kiov.iov_len, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\n\t\tret = ptrace_regset(child, request, addr, &kiov);\n\t\tif (!ret)\n\t\t\tret = __put_user(kiov.iov_len, &uiov->iov_len);\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -60,7 +60,7 @@\n \t\t * tracee into STOP.\n \t\t */\n \t\tif (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))\n-\t\t\tsignal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n+\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);\n \n \t\tunlock_task_sighand(child, &flags);\n \t\tret = 0;\n@@ -86,7 +86,7 @@\n \t\t\t * start of this trap and now.  Trigger re-trap.\n \t\t\t */\n \t\t\tif (child->jobctl & JOBCTL_TRAP_NOTIFY)\n-\t\t\t\tsignal_wake_up(child, true);\n+\t\t\t\tptrace_signal_wake_up(child, true);\n \t\t\tret = 0;\n \t\t}\n \t\tunlock_task_sighand(child, &flags);",
        "function_modified_lines": {
            "added": [
                "\t\t\tptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);",
                "\t\t\t\tptrace_signal_wake_up(child, true);"
            ],
            "deleted": [
                "\t\t\tsignal_wake_up(child, child->jobctl & JOBCTL_LISTENING);",
                "\t\t\t\tsignal_wake_up(child, true);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the ptrace functionality in the Linux kernel before 3.7.5 allows local users to gain privileges via a PTRACE_SETREGS ptrace system call in a crafted application, as demonstrated by ptrace_death.",
        "id": 162
    },
    {
        "cve_id": "CVE-2019-6133",
        "code_before_change": "static __latent_entropy struct task_struct *copy_process(\n\t\t\t\t\tunsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace,\n\t\t\t\t\tunsigned long tls,\n\t\t\t\t\tint node)\n{\n\tint retval;\n\tstruct task_struct *p;\n\tstruct multiprocess_signals delayed;\n\n\t/*\n\t * Don't allow sharing the root directory with processes in a different\n\t * namespace\n\t */\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid or user namespace\n\t * do not allow it to share a thread group with the forking task.\n\t */\n\tif (clone_flags & CLONE_THREAD) {\n\t\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\n\t\t    (task_active_pid_ns(current) !=\n\t\t\t\tcurrent->nsproxy->pid_ns_for_children))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * Force any signals received before this point to be delivered\n\t * before the fork happens.  Collect up signals sent to multiple\n\t * processes that happen during the fork and delay them so that\n\t * they appear to happen after the fork.\n\t */\n\tsigemptyset(&delayed.signal);\n\tINIT_HLIST_NODE(&delayed.node);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (!(clone_flags & CLONE_THREAD))\n\t\thlist_add_head(&delayed.node, &current->signal->multiprocess);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n\tretval = -ERESTARTNOINTR;\n\tif (signal_pending(current))\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current, node);\n\tif (!p)\n\t\tgoto fork_out;\n\n\t/*\n\t * This _must_ happen before we call free_task(), i.e. before we jump\n\t * to any of the bad_fork_* labels. This is to avoid freeing\n\t * p->set_child_tid which is (ab)used as a kthread's data pointer for\n\t * kernel threads (PF_KTHREAD).\n\t */\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (p->real_cred->user != INIT_USER &&\n\t\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);\n\tp->flags |= PF_FORKNOEXEC;\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tp->utimescaled = p->stimescaled = 0;\n#endif\n\tprev_cputime_init(&p->prev_cputime);\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqcount_init(&p->vtime.seqcount);\n\tp->vtime.starttime = 0;\n\tp->vtime.state = VTIME_INACTIVE;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n#ifdef CONFIG_PSI\n\tp->psi_flags = 0;\n#endif\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tp->start_time = ktime_get_ns();\n\tp->real_start_time = ktime_get_boot_ns();\n\tp->io_context = NULL;\n\taudit_set_context(p, NULL);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_threadgroup_lock;\n\t}\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n\n\tp->pagefault_disabled = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n\tlockdep_init_task(p);\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_BCACHE\n\tp->sequential_io\t= 0;\n\tp->sequential_io_avg\t= 0;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tretval = sched_fork(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_perf;\n\t/* copy all the process information */\n\tshm_init_task(p);\n\tretval = security_task_alloc(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_security;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tstackleak_task_init(p);\n\n\tif (pid != &init_struct_pid) {\n\t\tpid = alloc_pid(p->nsproxy->pid_ns_for_children);\n\t\tif (IS_ERR(pid)) {\n\t\t\tretval = PTR_ERR(pid);\n\t\t\tgoto bad_fork_cleanup_thread;\n\t\t}\n\t}\n\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tsas_ss_reset(p);\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->exit_signal = -1;\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tif (clone_flags & CLONE_PARENT)\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t\telse\n\t\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\tp->pdeath_signal = 0;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\tcgroup_threadgroup_change_begin(current);\n\t/*\n\t * Ensure that the cgroup subsystem policies allow the new process to be\n\t * forked. It should be noted the the new process's css_set can be changed\n\t * between here and cgroup_post_fork() if an organisation operation is in\n\t * progress.\n\t */\n\tretval = cgroup_can_fork(p);\n\tif (retval)\n\t\tgoto bad_fork_free_pid;\n\n\t/*\n\t * Make it visible to the rest of the system, but dont wake it up yet.\n\t * Need tasklist lock for parent etc handling!\n\t */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Copy seccomp details explicitly here, in case they were changed\n\t * before holding sighand lock.\n\t */\n\tcopy_seccomp(p);\n\n\trseq_fork(p, clone_flags);\n\n\t/* Don't start children in a dying pid namespace */\n\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\n\t\tretval = -ENOMEM;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* Let kill terminate clone/fork in the middle */\n\tif (fatal_signal_pending(current)) {\n\t\tretval = -EINTR;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\n\tinit_task_pid_links(p);\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tinit_task_pid(p, PIDTYPE_PID, pid);\n\t\tif (thread_group_leader(p)) {\n\t\t\tinit_task_pid(p, PIDTYPE_TGID, pid);\n\t\t\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\n\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\t\t\tp->signal->shared_pending.signal = delayed.signal;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\t/*\n\t\t\t * Inherit has_child_subreaper flag under the same\n\t\t\t * tasklist_lock with adding child to the process tree\n\t\t\t * for propagate_has_child_subreaper optimization.\n\t\t\t */\n\t\t\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\n\t\t\t\t\t\t\t p->real_parent->signal->is_child_subreaper;\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\tattach_pid(p, PIDTYPE_TGID);\n\t\t\tattach_pid(p, PIDTYPE_PGID);\n\t\t\tattach_pid(p, PIDTYPE_SID);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t} else {\n\t\t\tcurrent->signal->nr_threads++;\n\t\t\tatomic_inc(&current->signal->live);\n\t\t\tatomic_inc(&current->signal->sigcnt);\n\t\t\ttask_join_group_stop(p);\n\t\t\tlist_add_tail_rcu(&p->thread_group,\n\t\t\t\t\t  &p->group_leader->thread_group);\n\t\t\tlist_add_tail_rcu(&p->thread_node,\n\t\t\t\t\t  &p->signal->thread_head);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID);\n\t\tnr_threads++;\n\t}\n\ttotal_forks++;\n\thlist_del_init(&delayed.node);\n\tspin_unlock(&current->sighand->siglock);\n\tsyscall_tracepoint_update(p);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tcgroup_threadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\tuprobe_copy_process(p, clone_flags);\n\n\treturn p;\n\nbad_fork_cancel_cgroup:\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tcgroup_cancel_fork(p);\nbad_fork_free_pid:\n\tcgroup_threadgroup_change_end(current);\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_thread:\n\texit_thread(p);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_security:\n\tsecurity_task_free(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_perf:\n\tperf_event_free_task(p);\nbad_fork_cleanup_policy:\n\tlockdep_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_threadgroup_lock:\n#endif\n\tdelayacct_tsk_free(p);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tp->state = TASK_DEAD;\n\tput_task_stack(p);\n\tfree_task(p);\nfork_out:\n\tspin_lock_irq(&current->sighand->siglock);\n\thlist_del_init(&delayed.node);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ERR_PTR(retval);\n}",
        "code_after_change": "static __latent_entropy struct task_struct *copy_process(\n\t\t\t\t\tunsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace,\n\t\t\t\t\tunsigned long tls,\n\t\t\t\t\tint node)\n{\n\tint retval;\n\tstruct task_struct *p;\n\tstruct multiprocess_signals delayed;\n\n\t/*\n\t * Don't allow sharing the root directory with processes in a different\n\t * namespace\n\t */\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid or user namespace\n\t * do not allow it to share a thread group with the forking task.\n\t */\n\tif (clone_flags & CLONE_THREAD) {\n\t\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\n\t\t    (task_active_pid_ns(current) !=\n\t\t\t\tcurrent->nsproxy->pid_ns_for_children))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * Force any signals received before this point to be delivered\n\t * before the fork happens.  Collect up signals sent to multiple\n\t * processes that happen during the fork and delay them so that\n\t * they appear to happen after the fork.\n\t */\n\tsigemptyset(&delayed.signal);\n\tINIT_HLIST_NODE(&delayed.node);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (!(clone_flags & CLONE_THREAD))\n\t\thlist_add_head(&delayed.node, &current->signal->multiprocess);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n\tretval = -ERESTARTNOINTR;\n\tif (signal_pending(current))\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current, node);\n\tif (!p)\n\t\tgoto fork_out;\n\n\t/*\n\t * This _must_ happen before we call free_task(), i.e. before we jump\n\t * to any of the bad_fork_* labels. This is to avoid freeing\n\t * p->set_child_tid which is (ab)used as a kthread's data pointer for\n\t * kernel threads (PF_KTHREAD).\n\t */\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (p->real_cred->user != INIT_USER &&\n\t\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);\n\tp->flags |= PF_FORKNOEXEC;\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tp->utimescaled = p->stimescaled = 0;\n#endif\n\tprev_cputime_init(&p->prev_cputime);\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqcount_init(&p->vtime.seqcount);\n\tp->vtime.starttime = 0;\n\tp->vtime.state = VTIME_INACTIVE;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n#ifdef CONFIG_PSI\n\tp->psi_flags = 0;\n#endif\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tp->io_context = NULL;\n\taudit_set_context(p, NULL);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_threadgroup_lock;\n\t}\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n\n\tp->pagefault_disabled = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n\tlockdep_init_task(p);\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_BCACHE\n\tp->sequential_io\t= 0;\n\tp->sequential_io_avg\t= 0;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tretval = sched_fork(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_perf;\n\t/* copy all the process information */\n\tshm_init_task(p);\n\tretval = security_task_alloc(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_security;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tstackleak_task_init(p);\n\n\tif (pid != &init_struct_pid) {\n\t\tpid = alloc_pid(p->nsproxy->pid_ns_for_children);\n\t\tif (IS_ERR(pid)) {\n\t\t\tretval = PTR_ERR(pid);\n\t\t\tgoto bad_fork_cleanup_thread;\n\t\t}\n\t}\n\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tsas_ss_reset(p);\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->exit_signal = -1;\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tif (clone_flags & CLONE_PARENT)\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t\telse\n\t\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\tp->pdeath_signal = 0;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\tcgroup_threadgroup_change_begin(current);\n\t/*\n\t * Ensure that the cgroup subsystem policies allow the new process to be\n\t * forked. It should be noted the the new process's css_set can be changed\n\t * between here and cgroup_post_fork() if an organisation operation is in\n\t * progress.\n\t */\n\tretval = cgroup_can_fork(p);\n\tif (retval)\n\t\tgoto bad_fork_free_pid;\n\n\t/*\n\t * From this point on we must avoid any synchronous user-space\n\t * communication until we take the tasklist-lock. In particular, we do\n\t * not want user-space to be able to predict the process start-time by\n\t * stalling fork(2) after we recorded the start_time but before it is\n\t * visible to the system.\n\t */\n\n\tp->start_time = ktime_get_ns();\n\tp->real_start_time = ktime_get_boot_ns();\n\n\t/*\n\t * Make it visible to the rest of the system, but dont wake it up yet.\n\t * Need tasklist lock for parent etc handling!\n\t */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Copy seccomp details explicitly here, in case they were changed\n\t * before holding sighand lock.\n\t */\n\tcopy_seccomp(p);\n\n\trseq_fork(p, clone_flags);\n\n\t/* Don't start children in a dying pid namespace */\n\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\n\t\tretval = -ENOMEM;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* Let kill terminate clone/fork in the middle */\n\tif (fatal_signal_pending(current)) {\n\t\tretval = -EINTR;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\n\tinit_task_pid_links(p);\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tinit_task_pid(p, PIDTYPE_PID, pid);\n\t\tif (thread_group_leader(p)) {\n\t\t\tinit_task_pid(p, PIDTYPE_TGID, pid);\n\t\t\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\n\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\t\t\tp->signal->shared_pending.signal = delayed.signal;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\t/*\n\t\t\t * Inherit has_child_subreaper flag under the same\n\t\t\t * tasklist_lock with adding child to the process tree\n\t\t\t * for propagate_has_child_subreaper optimization.\n\t\t\t */\n\t\t\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\n\t\t\t\t\t\t\t p->real_parent->signal->is_child_subreaper;\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\tattach_pid(p, PIDTYPE_TGID);\n\t\t\tattach_pid(p, PIDTYPE_PGID);\n\t\t\tattach_pid(p, PIDTYPE_SID);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t} else {\n\t\t\tcurrent->signal->nr_threads++;\n\t\t\tatomic_inc(&current->signal->live);\n\t\t\tatomic_inc(&current->signal->sigcnt);\n\t\t\ttask_join_group_stop(p);\n\t\t\tlist_add_tail_rcu(&p->thread_group,\n\t\t\t\t\t  &p->group_leader->thread_group);\n\t\t\tlist_add_tail_rcu(&p->thread_node,\n\t\t\t\t\t  &p->signal->thread_head);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID);\n\t\tnr_threads++;\n\t}\n\ttotal_forks++;\n\thlist_del_init(&delayed.node);\n\tspin_unlock(&current->sighand->siglock);\n\tsyscall_tracepoint_update(p);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tcgroup_threadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\tuprobe_copy_process(p, clone_flags);\n\n\treturn p;\n\nbad_fork_cancel_cgroup:\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tcgroup_cancel_fork(p);\nbad_fork_free_pid:\n\tcgroup_threadgroup_change_end(current);\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_thread:\n\texit_thread(p);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_security:\n\tsecurity_task_free(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_perf:\n\tperf_event_free_task(p);\nbad_fork_cleanup_policy:\n\tlockdep_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_threadgroup_lock:\n#endif\n\tdelayacct_tsk_free(p);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tp->state = TASK_DEAD;\n\tput_task_stack(p);\n\tfree_task(p);\nfork_out:\n\tspin_lock_irq(&current->sighand->siglock);\n\thlist_del_init(&delayed.node);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ERR_PTR(retval);\n}",
        "patch": "--- code before\n+++ code after\n@@ -161,8 +161,6 @@\n \n \tposix_cpu_timers_init(p);\n \n-\tp->start_time = ktime_get_ns();\n-\tp->real_start_time = ktime_get_boot_ns();\n \tp->io_context = NULL;\n \taudit_set_context(p, NULL);\n \tcgroup_fork(p);\n@@ -327,6 +325,17 @@\n \tretval = cgroup_can_fork(p);\n \tif (retval)\n \t\tgoto bad_fork_free_pid;\n+\n+\t/*\n+\t * From this point on we must avoid any synchronous user-space\n+\t * communication until we take the tasklist-lock. In particular, we do\n+\t * not want user-space to be able to predict the process start-time by\n+\t * stalling fork(2) after we recorded the start_time but before it is\n+\t * visible to the system.\n+\t */\n+\n+\tp->start_time = ktime_get_ns();\n+\tp->real_start_time = ktime_get_boot_ns();\n \n \t/*\n \t * Make it visible to the rest of the system, but dont wake it up yet.",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * From this point on we must avoid any synchronous user-space",
                "\t * communication until we take the tasklist-lock. In particular, we do",
                "\t * not want user-space to be able to predict the process start-time by",
                "\t * stalling fork(2) after we recorded the start_time but before it is",
                "\t * visible to the system.",
                "\t */",
                "",
                "\tp->start_time = ktime_get_ns();",
                "\tp->real_start_time = ktime_get_boot_ns();"
            ],
            "deleted": [
                "\tp->start_time = ktime_get_ns();",
                "\tp->real_start_time = ktime_get_boot_ns();"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In PolicyKit (aka polkit) 0.115, the \"start time\" protection mechanism can be bypassed because fork() is not atomic, and therefore authorization decisions are improperly cached. This is related to lack of uid checking in polkitbackend/polkitbackendinteractiveauthority.c.",
        "id": 2341
    },
    {
        "cve_id": "CVE-2022-1462",
        "code_before_change": "static int pty_write(struct tty_struct *tty, const unsigned char *buf, int c)\n{\n\tstruct tty_struct *to = tty->link;\n\tunsigned long flags;\n\n\tif (tty->flow.stopped)\n\t\treturn 0;\n\n\tif (c > 0) {\n\t\tspin_lock_irqsave(&to->port->lock, flags);\n\t\t/* Stuff the data into the input queue of the other end */\n\t\tc = tty_insert_flip_string(to->port, buf, c);\n\t\tspin_unlock_irqrestore(&to->port->lock, flags);\n\t\t/* And shovel */\n\t\tif (c)\n\t\t\ttty_flip_buffer_push(to->port);\n\t}\n\treturn c;\n}",
        "code_after_change": "static int pty_write(struct tty_struct *tty, const unsigned char *buf, int c)\n{\n\tstruct tty_struct *to = tty->link;\n\n\tif (tty->flow.stopped || !c)\n\t\treturn 0;\n\n\treturn tty_insert_flip_string_and_push_buffer(to->port, buf, c);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,19 +1,9 @@\n static int pty_write(struct tty_struct *tty, const unsigned char *buf, int c)\n {\n \tstruct tty_struct *to = tty->link;\n-\tunsigned long flags;\n \n-\tif (tty->flow.stopped)\n+\tif (tty->flow.stopped || !c)\n \t\treturn 0;\n \n-\tif (c > 0) {\n-\t\tspin_lock_irqsave(&to->port->lock, flags);\n-\t\t/* Stuff the data into the input queue of the other end */\n-\t\tc = tty_insert_flip_string(to->port, buf, c);\n-\t\tspin_unlock_irqrestore(&to->port->lock, flags);\n-\t\t/* And shovel */\n-\t\tif (c)\n-\t\t\ttty_flip_buffer_push(to->port);\n-\t}\n-\treturn c;\n+\treturn tty_insert_flip_string_and_push_buffer(to->port, buf, c);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (tty->flow.stopped || !c)",
                "\treturn tty_insert_flip_string_and_push_buffer(to->port, buf, c);"
            ],
            "deleted": [
                "\tunsigned long flags;",
                "\tif (tty->flow.stopped)",
                "\tif (c > 0) {",
                "\t\tspin_lock_irqsave(&to->port->lock, flags);",
                "\t\t/* Stuff the data into the input queue of the other end */",
                "\t\tc = tty_insert_flip_string(to->port, buf, c);",
                "\t\tspin_unlock_irqrestore(&to->port->lock, flags);",
                "\t\t/* And shovel */",
                "\t\tif (c)",
                "\t\t\ttty_flip_buffer_push(to->port);",
                "\t}",
                "\treturn c;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "An out-of-bounds read flaw was found in the Linux kernel’s TeleTYpe subsystem. The issue occurs in how a user triggers a race condition using ioctls TIOCSPTLCK and TIOCGPTPEER and TIOCSTI and TCXONC with leakage of memory in the flush_to_ldisc function. This flaw allows a local user to crash the system or read unauthorized random data from memory.",
        "id": 3261
    },
    {
        "cve_id": "CVE-2021-20261",
        "code_before_change": "static int get_floppy_geometry(int drive, int type, struct floppy_struct **g)\n{\n\tif (type)\n\t\t*g = &floppy_type[type];\n\telse {\n\t\tif (lock_fdc(drive, false))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(false, 0) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\t*g = current_type[drive];\n\t}\n\tif (!*g)\n\t\treturn -ENODEV;\n\treturn 0;\n}",
        "code_after_change": "static int get_floppy_geometry(int drive, int type, struct floppy_struct **g)\n{\n\tif (type)\n\t\t*g = &floppy_type[type];\n\telse {\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(false, 0) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\t*g = current_type[drive];\n\t}\n\tif (!*g)\n\t\treturn -ENODEV;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \tif (type)\n \t\t*g = &floppy_type[type];\n \telse {\n-\t\tif (lock_fdc(drive, false))\n+\t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;\n \t\tif (poll_drive(false, 0) == -EINTR)\n \t\t\treturn -EINTR;",
        "function_modified_lines": {
            "added": [
                "\t\tif (lock_fdc(drive))"
            ],
            "deleted": [
                "\t\tif (lock_fdc(drive, false))"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw.",
        "id": 2864
    },
    {
        "cve_id": "CVE-2021-20261",
        "code_before_change": "static int user_reset_fdc(int drive, int arg, bool interruptible)\n{\n\tint ret;\n\n\tif (lock_fdc(drive, interruptible))\n\t\treturn -EINTR;\n\n\tif (arg == FD_RESET_ALWAYS)\n\t\tFDCS->reset = 1;\n\tif (FDCS->reset) {\n\t\tcont = &reset_cont;\n\t\tret = wait_til_done(reset_fdc, interruptible);\n\t\tif (ret == -EINTR)\n\t\t\treturn -EINTR;\n\t}\n\tprocess_fd_request();\n\treturn 0;\n}",
        "code_after_change": "static int user_reset_fdc(int drive, int arg, bool interruptible)\n{\n\tint ret;\n\n\tif (lock_fdc(drive))\n\t\treturn -EINTR;\n\n\tif (arg == FD_RESET_ALWAYS)\n\t\tFDCS->reset = 1;\n\tif (FDCS->reset) {\n\t\tcont = &reset_cont;\n\t\tret = wait_til_done(reset_fdc, interruptible);\n\t\tif (ret == -EINTR)\n\t\t\treturn -EINTR;\n\t}\n\tprocess_fd_request();\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tint ret;\n \n-\tif (lock_fdc(drive, interruptible))\n+\tif (lock_fdc(drive))\n \t\treturn -EINTR;\n \n \tif (arg == FD_RESET_ALWAYS)",
        "function_modified_lines": {
            "added": [
                "\tif (lock_fdc(drive))"
            ],
            "deleted": [
                "\tif (lock_fdc(drive, interruptible))"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw.",
        "id": 2863
    },
    {
        "cve_id": "CVE-2021-20261",
        "code_before_change": "static int floppy_revalidate(struct gendisk *disk)\n{\n\tint drive = (long)disk->private_data;\n\tint cf;\n\tint res = 0;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive)) {\n\t\tif (WARN(atomic_read(&usage_count) == 0,\n\t\t\t \"VFS: revalidate called on non-open device.\\n\"))\n\t\t\treturn -EFAULT;\n\n\t\tlock_fdc(drive, false);\n\t\tcf = (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t\t      test_bit(FD_VERIFY_BIT, &UDRS->flags));\n\t\tif (!(cf || test_bit(drive, &fake_change) || drive_no_geom(drive))) {\n\t\t\tprocess_fd_request();\t/*already done by another thread */\n\t\t\treturn 0;\n\t\t}\n\t\tUDRS->maxblock = 0;\n\t\tUDRS->maxtrack = 0;\n\t\tif (buffer_drive == drive)\n\t\t\tbuffer_track = -1;\n\t\tclear_bit(drive, &fake_change);\n\t\tclear_bit(FD_DISK_CHANGED_BIT, &UDRS->flags);\n\t\tif (cf)\n\t\t\tUDRS->generation++;\n\t\tif (drive_no_geom(drive)) {\n\t\t\t/* auto-sensing */\n\t\t\tres = __floppy_read_block_0(opened_bdev[drive], drive);\n\t\t} else {\n\t\t\tif (cf)\n\t\t\t\tpoll_drive(false, FD_RAW_NEED_DISK);\n\t\t\tprocess_fd_request();\n\t\t}\n\t}\n\tset_capacity(disk, floppy_sizes[UDRS->fd_device]);\n\treturn res;\n}",
        "code_after_change": "static int floppy_revalidate(struct gendisk *disk)\n{\n\tint drive = (long)disk->private_data;\n\tint cf;\n\tint res = 0;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive)) {\n\t\tif (WARN(atomic_read(&usage_count) == 0,\n\t\t\t \"VFS: revalidate called on non-open device.\\n\"))\n\t\t\treturn -EFAULT;\n\n\t\tres = lock_fdc(drive);\n\t\tif (res)\n\t\t\treturn res;\n\t\tcf = (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t\t      test_bit(FD_VERIFY_BIT, &UDRS->flags));\n\t\tif (!(cf || test_bit(drive, &fake_change) || drive_no_geom(drive))) {\n\t\t\tprocess_fd_request();\t/*already done by another thread */\n\t\t\treturn 0;\n\t\t}\n\t\tUDRS->maxblock = 0;\n\t\tUDRS->maxtrack = 0;\n\t\tif (buffer_drive == drive)\n\t\t\tbuffer_track = -1;\n\t\tclear_bit(drive, &fake_change);\n\t\tclear_bit(FD_DISK_CHANGED_BIT, &UDRS->flags);\n\t\tif (cf)\n\t\t\tUDRS->generation++;\n\t\tif (drive_no_geom(drive)) {\n\t\t\t/* auto-sensing */\n\t\t\tres = __floppy_read_block_0(opened_bdev[drive], drive);\n\t\t} else {\n\t\t\tif (cf)\n\t\t\t\tpoll_drive(false, FD_RAW_NEED_DISK);\n\t\t\tprocess_fd_request();\n\t\t}\n\t}\n\tset_capacity(disk, floppy_sizes[UDRS->fd_device]);\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,9 @@\n \t\t\t \"VFS: revalidate called on non-open device.\\n\"))\n \t\t\treturn -EFAULT;\n \n-\t\tlock_fdc(drive, false);\n+\t\tres = lock_fdc(drive);\n+\t\tif (res)\n+\t\t\treturn res;\n \t\tcf = (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n \t\t      test_bit(FD_VERIFY_BIT, &UDRS->flags));\n \t\tif (!(cf || test_bit(drive, &fake_change) || drive_no_geom(drive))) {",
        "function_modified_lines": {
            "added": [
                "\t\tres = lock_fdc(drive);",
                "\t\tif (res)",
                "\t\t\treturn res;"
            ],
            "deleted": [
                "\t\tlock_fdc(drive, false);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the Linux kernels implementation of the floppy disk drive controller driver software. The impact of this issue is lessened by the fact that the default permissions on the floppy device (/dev/fd0) are restricted to root. If the permissions on the device have changed the impact changes greatly. In the default configuration root (or equivalent) permissions are required to attack this flaw.",
        "id": 2865
    },
    {
        "cve_id": "CVE-2022-2590",
        "code_before_change": "struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\tpmd_t *pmd, int flags, struct dev_pagemap **pgmap)\n{\n\tunsigned long pfn = pmd_pfn(*pmd);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\t/*\n\t * When we COW a devmap PMD entry, we split it into PTEs, so we should\n\t * not be in this function with `flags & FOLL_COW` set.\n\t */\n\tWARN_ONCE(flags & FOLL_COW, \"mm: In follow_devmap_pmd with FOLL_COW set\");\n\n\t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn NULL;\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\treturn NULL;\n\n\tif (pmd_present(*pmd) && pmd_devmap(*pmd))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd, flags & FOLL_WRITE);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & (FOLL_GET | FOLL_PIN)))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PMD_MASK) >> PAGE_SHIFT;\n\t*pgmap = get_dev_pagemap(pfn, *pgmap);\n\tif (!*pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tif (!try_grab_page(page, flags))\n\t\tpage = ERR_PTR(-ENOMEM);\n\n\treturn page;\n}",
        "code_after_change": "struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\tpmd_t *pmd, int flags, struct dev_pagemap **pgmap)\n{\n\tunsigned long pfn = pmd_pfn(*pmd);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn NULL;\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\treturn NULL;\n\n\tif (pmd_present(*pmd) && pmd_devmap(*pmd))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd, flags & FOLL_WRITE);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & (FOLL_GET | FOLL_PIN)))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PMD_MASK) >> PAGE_SHIFT;\n\t*pgmap = get_dev_pagemap(pfn, *pgmap);\n\tif (!*pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tif (!try_grab_page(page, flags))\n\t\tpage = ERR_PTR(-ENOMEM);\n\n\treturn page;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,12 +6,6 @@\n \tstruct page *page;\n \n \tassert_spin_locked(pmd_lockptr(mm, pmd));\n-\n-\t/*\n-\t * When we COW a devmap PMD entry, we split it into PTEs, so we should\n-\t * not be in this function with `flags & FOLL_COW` set.\n-\t */\n-\tWARN_ONCE(flags & FOLL_COW, \"mm: In follow_devmap_pmd with FOLL_COW set\");\n \n \t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n \tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\t/*",
                "\t * When we COW a devmap PMD entry, we split it into PTEs, so we should",
                "\t * not be in this function with `flags & FOLL_COW` set.",
                "\t */",
                "\tWARN_ONCE(flags & FOLL_COW, \"mm: In follow_devmap_pmd with FOLL_COW set\");"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the way the Linux kernel's memory subsystem handled the copy-on-write (COW) breakage of private read-only shared memory mappings. This flaw allows an unprivileged, local user to gain write access to read-only memory mappings, increasing their privileges on the system.",
        "id": 3481
    },
    {
        "cve_id": "CVE-2022-2590",
        "code_before_change": "static int faultin_page(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, bool unshare,\n\t\tint *locked)\n{\n\tunsigned int fault_flags = 0;\n\tvm_fault_t ret;\n\n\tif (*flags & FOLL_NOFAULT)\n\t\treturn -EFAULT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (locked)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\t/*\n\t\t * Note: FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_TRIED\n\t\t * can co-exist\n\t\t */\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\tif (unshare) {\n\t\tfault_flags |= FAULT_FLAG_UNSHARE;\n\t\t/* FAULT_FLAG_WRITE and FAULT_FLAG_UNSHARE are incompatible */\n\t\tVM_BUG_ON(fault_flags & FAULT_FLAG_WRITE);\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags, NULL);\n\n\tif (ret & VM_FAULT_COMPLETED) {\n\t\t/*\n\t\t * With FAULT_FLAG_RETRY_NOWAIT we'll never release the\n\t\t * mmap lock in the page fault handler. Sanity check this.\n\t\t */\n\t\tWARN_ON_ONCE(fault_flags & FAULT_FLAG_RETRY_NOWAIT);\n\t\tif (locked)\n\t\t\t*locked = 0;\n\t\t/*\n\t\t * We should do the same as VM_FAULT_RETRY, but let's not\n\t\t * return -EBUSY since that's not reflecting the reality of\n\t\t * what has happened - we've just fully completed a page\n\t\t * fault, with the mmap lock released.  Use -EAGAIN to show\n\t\t * that we want to take the mmap lock _again_.\n\t\t */\n\t\treturn -EAGAIN;\n\t}\n\n\tif (ret & VM_FAULT_ERROR) {\n\t\tint err = vm_fault_to_errno(ret, *flags);\n\n\t\tif (err)\n\t\t\treturn err;\n\t\tBUG();\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (locked && !(fault_flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\t\t*locked = 0;\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n\t * can thus safely do subsequent page lookups as if they were reads.\n\t * But only do so when looping for pte_write is futile: in some cases\n\t * userspace may also be wanting to write to the gotten user page,\n\t * which a read fault here might prevent (a readonly page might get\n\t * reCOWed by userspace write).\n\t */\n\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n\t\t*flags |= FOLL_COW;\n\treturn 0;\n}",
        "code_after_change": "static int faultin_page(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, bool unshare,\n\t\tint *locked)\n{\n\tunsigned int fault_flags = 0;\n\tvm_fault_t ret;\n\n\tif (*flags & FOLL_NOFAULT)\n\t\treturn -EFAULT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (locked)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\t/*\n\t\t * Note: FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_TRIED\n\t\t * can co-exist\n\t\t */\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\tif (unshare) {\n\t\tfault_flags |= FAULT_FLAG_UNSHARE;\n\t\t/* FAULT_FLAG_WRITE and FAULT_FLAG_UNSHARE are incompatible */\n\t\tVM_BUG_ON(fault_flags & FAULT_FLAG_WRITE);\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags, NULL);\n\n\tif (ret & VM_FAULT_COMPLETED) {\n\t\t/*\n\t\t * With FAULT_FLAG_RETRY_NOWAIT we'll never release the\n\t\t * mmap lock in the page fault handler. Sanity check this.\n\t\t */\n\t\tWARN_ON_ONCE(fault_flags & FAULT_FLAG_RETRY_NOWAIT);\n\t\tif (locked)\n\t\t\t*locked = 0;\n\t\t/*\n\t\t * We should do the same as VM_FAULT_RETRY, but let's not\n\t\t * return -EBUSY since that's not reflecting the reality of\n\t\t * what has happened - we've just fully completed a page\n\t\t * fault, with the mmap lock released.  Use -EAGAIN to show\n\t\t * that we want to take the mmap lock _again_.\n\t\t */\n\t\treturn -EAGAIN;\n\t}\n\n\tif (ret & VM_FAULT_ERROR) {\n\t\tint err = vm_fault_to_errno(ret, *flags);\n\n\t\tif (err)\n\t\t\treturn err;\n\t\tBUG();\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (locked && !(fault_flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\t\t*locked = 0;\n\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -62,16 +62,5 @@\n \t\treturn -EBUSY;\n \t}\n \n-\t/*\n-\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n-\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n-\t * can thus safely do subsequent page lookups as if they were reads.\n-\t * But only do so when looping for pte_write is futile: in some cases\n-\t * userspace may also be wanting to write to the gotten user page,\n-\t * which a read fault here might prevent (a readonly page might get\n-\t * reCOWed by userspace write).\n-\t */\n-\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n-\t\t*flags |= FOLL_COW;\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t/*",
                "\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when",
                "\t * necessary, even if maybe_mkwrite decided not to set pte_write. We",
                "\t * can thus safely do subsequent page lookups as if they were reads.",
                "\t * But only do so when looping for pte_write is futile: in some cases",
                "\t * userspace may also be wanting to write to the gotten user page,",
                "\t * which a read fault here might prevent (a readonly page might get",
                "\t * reCOWed by userspace write).",
                "\t */",
                "\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))",
                "\t\t*flags |= FOLL_COW;"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race condition was found in the way the Linux kernel's memory subsystem handled the copy-on-write (COW) breakage of private read-only shared memory mappings. This flaw allows an unprivileged, local user to gain write access to read-only memory mappings, increasing their privileges on the system.",
        "id": 3480
    },
    {
        "cve_id": "CVE-2021-4203",
        "code_before_change": "static void copy_peercred(struct sock *sk, struct sock *peersk)\n{\n\tput_pid(sk->sk_peer_pid);\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tsk->sk_peer_pid  = get_pid(peersk->sk_peer_pid);\n\tsk->sk_peer_cred = get_cred(peersk->sk_peer_cred);\n}",
        "code_after_change": "static void copy_peercred(struct sock *sk, struct sock *peersk)\n{\n\tconst struct cred *old_cred;\n\tstruct pid *old_pid;\n\n\tif (sk < peersk) {\n\t\tspin_lock(&sk->sk_peer_lock);\n\t\tspin_lock_nested(&peersk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n\t} else {\n\t\tspin_lock(&peersk->sk_peer_lock);\n\t\tspin_lock_nested(&sk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n\t}\n\told_pid = sk->sk_peer_pid;\n\told_cred = sk->sk_peer_cred;\n\tsk->sk_peer_pid  = get_pid(peersk->sk_peer_pid);\n\tsk->sk_peer_cred = get_cred(peersk->sk_peer_cred);\n\n\tspin_unlock(&sk->sk_peer_lock);\n\tspin_unlock(&peersk->sk_peer_lock);\n\n\tput_pid(old_pid);\n\tput_cred(old_cred);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,23 @@\n static void copy_peercred(struct sock *sk, struct sock *peersk)\n {\n-\tput_pid(sk->sk_peer_pid);\n-\tif (sk->sk_peer_cred)\n-\t\tput_cred(sk->sk_peer_cred);\n+\tconst struct cred *old_cred;\n+\tstruct pid *old_pid;\n+\n+\tif (sk < peersk) {\n+\t\tspin_lock(&sk->sk_peer_lock);\n+\t\tspin_lock_nested(&peersk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n+\t} else {\n+\t\tspin_lock(&peersk->sk_peer_lock);\n+\t\tspin_lock_nested(&sk->sk_peer_lock, SINGLE_DEPTH_NESTING);\n+\t}\n+\told_pid = sk->sk_peer_pid;\n+\told_cred = sk->sk_peer_cred;\n \tsk->sk_peer_pid  = get_pid(peersk->sk_peer_pid);\n \tsk->sk_peer_cred = get_cred(peersk->sk_peer_cred);\n+\n+\tspin_unlock(&sk->sk_peer_lock);\n+\tspin_unlock(&peersk->sk_peer_lock);\n+\n+\tput_pid(old_pid);\n+\tput_cred(old_cred);\n }",
        "function_modified_lines": {
            "added": [
                "\tconst struct cred *old_cred;",
                "\tstruct pid *old_pid;",
                "",
                "\tif (sk < peersk) {",
                "\t\tspin_lock(&sk->sk_peer_lock);",
                "\t\tspin_lock_nested(&peersk->sk_peer_lock, SINGLE_DEPTH_NESTING);",
                "\t} else {",
                "\t\tspin_lock(&peersk->sk_peer_lock);",
                "\t\tspin_lock_nested(&sk->sk_peer_lock, SINGLE_DEPTH_NESTING);",
                "\t}",
                "\told_pid = sk->sk_peer_pid;",
                "\told_cred = sk->sk_peer_cred;",
                "",
                "\tspin_unlock(&sk->sk_peer_lock);",
                "\tspin_unlock(&peersk->sk_peer_lock);",
                "",
                "\tput_pid(old_pid);",
                "\tput_cred(old_cred);"
            ],
            "deleted": [
                "\tput_pid(sk->sk_peer_pid);",
                "\tif (sk->sk_peer_cred)",
                "\t\tput_cred(sk->sk_peer_cred);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information.",
        "id": 3149
    },
    {
        "cve_id": "CVE-2021-4203",
        "code_before_change": "static void __sk_destruct(struct rcu_head *head)\n{\n\tstruct sock *sk = container_of(head, struct sock, sk_rcu);\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       refcount_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n#ifdef CONFIG_BPF_SYSCALL\n\tbpf_sk_storage_free(sk);\n#endif\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tpr_debug(\"%s: optmem leakage (%d bytes) detected\\n\",\n\t\t\t __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t}\n\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\tif (likely(sk->sk_net_refcnt))\n\t\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}",
        "code_after_change": "static void __sk_destruct(struct rcu_head *head)\n{\n\tstruct sock *sk = container_of(head, struct sock, sk_rcu);\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       refcount_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n#ifdef CONFIG_BPF_SYSCALL\n\tbpf_sk_storage_free(sk);\n#endif\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tpr_debug(\"%s: optmem leakage (%d bytes) detected\\n\",\n\t\t\t __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t}\n\n\t/* We do not need to acquire sk->sk_peer_lock, we are the last user. */\n\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\n\tif (likely(sk->sk_net_refcnt))\n\t\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,9 +28,10 @@\n \t\tsk->sk_frag.page = NULL;\n \t}\n \n-\tif (sk->sk_peer_cred)\n-\t\tput_cred(sk->sk_peer_cred);\n+\t/* We do not need to acquire sk->sk_peer_lock, we are the last user. */\n+\tput_cred(sk->sk_peer_cred);\n \tput_pid(sk->sk_peer_pid);\n+\n \tif (likely(sk->sk_net_refcnt))\n \t\tput_net(sock_net(sk));\n \tsk_prot_free(sk->sk_prot_creator, sk);",
        "function_modified_lines": {
            "added": [
                "\t/* We do not need to acquire sk->sk_peer_lock, we are the last user. */",
                "\tput_cred(sk->sk_peer_cred);",
                ""
            ],
            "deleted": [
                "\tif (sk->sk_peer_cred)",
                "\t\tput_cred(sk->sk_peer_cred);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information.",
        "id": 3147
    },
    {
        "cve_id": "CVE-2021-4203",
        "code_before_change": "int sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tunion {\n\t\tint val;\n\t\tu64 val64;\n\t\tunsigned long ulval;\n\t\tstruct linger ling;\n\t\tstruct old_timeval32 tm32;\n\t\tstruct __kernel_old_timeval tm;\n\t\tstruct  __kernel_sock_timeval stm;\n\t\tstruct sock_txtime txtime;\n\t\tstruct so_timestamping timestamping;\n\t} v;\n\n\tint lv = sizeof(int);\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tmemset(&v, 0, sizeof(v));\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tv.val = sock_flag(sk, SOCK_DBG);\n\t\tbreak;\n\n\tcase SO_DONTROUTE:\n\t\tv.val = sock_flag(sk, SOCK_LOCALROUTE);\n\t\tbreak;\n\n\tcase SO_BROADCAST:\n\t\tv.val = sock_flag(sk, SOCK_BROADCAST);\n\t\tbreak;\n\n\tcase SO_SNDBUF:\n\t\tv.val = sk->sk_sndbuf;\n\t\tbreak;\n\n\tcase SO_RCVBUF:\n\t\tv.val = sk->sk_rcvbuf;\n\t\tbreak;\n\n\tcase SO_REUSEADDR:\n\t\tv.val = sk->sk_reuse;\n\t\tbreak;\n\n\tcase SO_REUSEPORT:\n\t\tv.val = sk->sk_reuseport;\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tv.val = sock_flag(sk, SOCK_KEEPOPEN);\n\t\tbreak;\n\n\tcase SO_TYPE:\n\t\tv.val = sk->sk_type;\n\t\tbreak;\n\n\tcase SO_PROTOCOL:\n\t\tv.val = sk->sk_protocol;\n\t\tbreak;\n\n\tcase SO_DOMAIN:\n\t\tv.val = sk->sk_family;\n\t\tbreak;\n\n\tcase SO_ERROR:\n\t\tv.val = -sock_error(sk);\n\t\tif (v.val == 0)\n\t\t\tv.val = xchg(&sk->sk_err_soft, 0);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tv.val = sock_flag(sk, SOCK_URGINLINE);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tv.val = sk->sk_no_check_tx;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tv.val = sk->sk_priority;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tlv\t\t= sizeof(v.ling);\n\t\tv.ling.l_onoff\t= sock_flag(sk, SOCK_LINGER);\n\t\tv.ling.l_linger\t= sk->sk_lingertime / HZ;\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) &&\n\t\t\t\t!sock_flag(sk, SOCK_TSTAMP_NEW) &&\n\t\t\t\t!sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && !sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING_OLD:\n\t\tlv = sizeof(v.timestamping);\n\t\tv.timestamping.flags = sk->sk_tsflags;\n\t\tv.timestamping.bind_phc = sk->sk_bind_phc;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO_OLD:\n\tcase SO_RCVTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_rcvtimeo, &v, SO_RCVTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO_OLD:\n\tcase SO_SNDTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_sndtimeo, &v, SO_SNDTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tv.val = sk->sk_rcvlowat;\n\t\tbreak;\n\n\tcase SO_SNDLOWAT:\n\t\tv.val = 1;\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tv.val = !!test_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERCRED:\n\t{\n\t\tstruct ucred peercred;\n\t\tif (len > sizeof(peercred))\n\t\t\tlen = sizeof(peercred);\n\t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n\t\tif (copy_to_user(optval, &peercred, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERGROUPS:\n\t{\n\t\tint ret, n;\n\n\t\tif (!sk->sk_peer_cred)\n\t\t\treturn -ENODATA;\n\n\t\tn = sk->sk_peer_cred->group_info->ngroups;\n\t\tif (len < n * sizeof(gid_t)) {\n\t\t\tlen = n * sizeof(gid_t);\n\t\t\treturn put_user(len, optlen) ? -EFAULT : -ERANGE;\n\t\t}\n\t\tlen = n * sizeof(gid_t);\n\n\t\tret = groups_to_user((gid_t __user *)optval,\n\t\t\t\t     sk->sk_peer_cred->group_info);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERNAME:\n\t{\n\t\tchar address[128];\n\n\t\tlv = sock->ops->getname(sock, (struct sockaddr *)address, 2);\n\t\tif (lv < 0)\n\t\t\treturn -ENOTCONN;\n\t\tif (lv < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(optval, address, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\t/* Dubious BSD thing... Probably nobody even uses it, but\n\t * the UNIX standard wants it for whatever reason... -DaveM\n\t */\n\tcase SO_ACCEPTCONN:\n\t\tv.val = sk->sk_state == TCP_LISTEN;\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tv.val = !!test_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERSEC:\n\t\treturn security_socket_getpeersec_stream(sock, optval, optlen, len);\n\n\tcase SO_MARK:\n\t\tv.val = sk->sk_mark;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tv.val = sock_flag(sk, SOCK_RXQ_OVFL);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tv.val = sock_flag(sk, SOCK_WIFI_STATUS);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (!sock->ops->set_peek_off)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tv.val = sk->sk_peek_off;\n\t\tbreak;\n\tcase SO_NOFCS:\n\t\tv.val = sock_flag(sk, SOCK_NOFCS);\n\t\tbreak;\n\n\tcase SO_BINDTODEVICE:\n\t\treturn sock_getbindtodevice(sk, optval, optlen, len);\n\n\tcase SO_GET_FILTER:\n\t\tlen = sk_get_filter(sk, (struct sock_filter __user *)optval, len);\n\t\tif (len < 0)\n\t\t\treturn len;\n\n\t\tgoto lenout;\n\n\tcase SO_LOCK_FILTER:\n\t\tv.val = sock_flag(sk, SOCK_FILTER_LOCKED);\n\t\tbreak;\n\n\tcase SO_BPF_EXTENSIONS:\n\t\tv.val = bpf_tell_extensions();\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tv.val = sock_flag(sk, SOCK_SELECT_ERR_QUEUE);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\tv.val = sk->sk_ll_usec;\n\t\tbreak;\n\tcase SO_PREFER_BUSY_POLL:\n\t\tv.val = READ_ONCE(sk->sk_prefer_busy_poll);\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\tif (sizeof(v.ulval) != sizeof(v.val) && len >= sizeof(v.ulval)) {\n\t\t\tlv = sizeof(v.ulval);\n\t\t\tv.ulval = sk->sk_max_pacing_rate;\n\t\t} else {\n\t\t\t/* 32bit version */\n\t\t\tv.val = min_t(unsigned long, sk->sk_max_pacing_rate, ~0U);\n\t\t}\n\t\tbreak;\n\n\tcase SO_INCOMING_CPU:\n\t\tv.val = READ_ONCE(sk->sk_incoming_cpu);\n\t\tbreak;\n\n\tcase SO_MEMINFO:\n\t{\n\t\tu32 meminfo[SK_MEMINFO_VARS];\n\n\t\tsk_get_meminfo(sk, meminfo);\n\n\t\tlen = min_t(unsigned int, len, sizeof(meminfo));\n\t\tif (copy_to_user(optval, &meminfo, len))\n\t\t\treturn -EFAULT;\n\n\t\tgoto lenout;\n\t}\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_INCOMING_NAPI_ID:\n\t\tv.val = READ_ONCE(sk->sk_napi_id);\n\n\t\t/* aggregate non-NAPI IDs down to 0 */\n\t\tif (v.val < MIN_NAPI_ID)\n\t\t\tv.val = 0;\n\n\t\tbreak;\n#endif\n\n\tcase SO_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len < lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_gen_cookie(sk);\n\t\tbreak;\n\n\tcase SO_ZEROCOPY:\n\t\tv.val = sock_flag(sk, SOCK_ZEROCOPY);\n\t\tbreak;\n\n\tcase SO_TXTIME:\n\t\tlv = sizeof(v.txtime);\n\t\tv.txtime.clockid = sk->sk_clockid;\n\t\tv.txtime.flags |= sk->sk_txtime_deadline_mode ?\n\t\t\t\t  SOF_TXTIME_DEADLINE_MODE : 0;\n\t\tv.txtime.flags |= sk->sk_txtime_report_errors ?\n\t\t\t\t  SOF_TXTIME_REPORT_ERRORS : 0;\n\t\tbreak;\n\n\tcase SO_BINDTOIFINDEX:\n\t\tv.val = sk->sk_bound_dev_if;\n\t\tbreak;\n\n\tcase SO_NETNS_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len != lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_net(sk)->net_cookie;\n\t\tbreak;\n\n\tcase SO_BUF_LOCK:\n\t\tv.val = sk->sk_userlocks & SOCK_BUF_LOCK_MASK;\n\t\tbreak;\n\n\tdefault:\n\t\t/* We implement the SO_SNDLOWAT etc to not be settable\n\t\t * (1003.1g 7).\n\t\t */\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (copy_to_user(optval, &v, len))\n\t\treturn -EFAULT;\nlenout:\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "code_after_change": "int sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tunion {\n\t\tint val;\n\t\tu64 val64;\n\t\tunsigned long ulval;\n\t\tstruct linger ling;\n\t\tstruct old_timeval32 tm32;\n\t\tstruct __kernel_old_timeval tm;\n\t\tstruct  __kernel_sock_timeval stm;\n\t\tstruct sock_txtime txtime;\n\t\tstruct so_timestamping timestamping;\n\t} v;\n\n\tint lv = sizeof(int);\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tmemset(&v, 0, sizeof(v));\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tv.val = sock_flag(sk, SOCK_DBG);\n\t\tbreak;\n\n\tcase SO_DONTROUTE:\n\t\tv.val = sock_flag(sk, SOCK_LOCALROUTE);\n\t\tbreak;\n\n\tcase SO_BROADCAST:\n\t\tv.val = sock_flag(sk, SOCK_BROADCAST);\n\t\tbreak;\n\n\tcase SO_SNDBUF:\n\t\tv.val = sk->sk_sndbuf;\n\t\tbreak;\n\n\tcase SO_RCVBUF:\n\t\tv.val = sk->sk_rcvbuf;\n\t\tbreak;\n\n\tcase SO_REUSEADDR:\n\t\tv.val = sk->sk_reuse;\n\t\tbreak;\n\n\tcase SO_REUSEPORT:\n\t\tv.val = sk->sk_reuseport;\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tv.val = sock_flag(sk, SOCK_KEEPOPEN);\n\t\tbreak;\n\n\tcase SO_TYPE:\n\t\tv.val = sk->sk_type;\n\t\tbreak;\n\n\tcase SO_PROTOCOL:\n\t\tv.val = sk->sk_protocol;\n\t\tbreak;\n\n\tcase SO_DOMAIN:\n\t\tv.val = sk->sk_family;\n\t\tbreak;\n\n\tcase SO_ERROR:\n\t\tv.val = -sock_error(sk);\n\t\tif (v.val == 0)\n\t\t\tv.val = xchg(&sk->sk_err_soft, 0);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tv.val = sock_flag(sk, SOCK_URGINLINE);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tv.val = sk->sk_no_check_tx;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tv.val = sk->sk_priority;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tlv\t\t= sizeof(v.ling);\n\t\tv.ling.l_onoff\t= sock_flag(sk, SOCK_LINGER);\n\t\tv.ling.l_linger\t= sk->sk_lingertime / HZ;\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) &&\n\t\t\t\t!sock_flag(sk, SOCK_TSTAMP_NEW) &&\n\t\t\t\t!sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && !sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING_OLD:\n\t\tlv = sizeof(v.timestamping);\n\t\tv.timestamping.flags = sk->sk_tsflags;\n\t\tv.timestamping.bind_phc = sk->sk_bind_phc;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO_OLD:\n\tcase SO_RCVTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_rcvtimeo, &v, SO_RCVTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO_OLD:\n\tcase SO_SNDTIMEO_NEW:\n\t\tlv = sock_get_timeout(sk->sk_sndtimeo, &v, SO_SNDTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tv.val = sk->sk_rcvlowat;\n\t\tbreak;\n\n\tcase SO_SNDLOWAT:\n\t\tv.val = 1;\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tv.val = !!test_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERCRED:\n\t{\n\t\tstruct ucred peercred;\n\t\tif (len > sizeof(peercred))\n\t\t\tlen = sizeof(peercred);\n\n\t\tspin_lock(&sk->sk_peer_lock);\n\t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n\t\tspin_unlock(&sk->sk_peer_lock);\n\n\t\tif (copy_to_user(optval, &peercred, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERGROUPS:\n\t{\n\t\tconst struct cred *cred;\n\t\tint ret, n;\n\n\t\tcred = sk_get_peer_cred(sk);\n\t\tif (!cred)\n\t\t\treturn -ENODATA;\n\n\t\tn = cred->group_info->ngroups;\n\t\tif (len < n * sizeof(gid_t)) {\n\t\t\tlen = n * sizeof(gid_t);\n\t\t\tput_cred(cred);\n\t\t\treturn put_user(len, optlen) ? -EFAULT : -ERANGE;\n\t\t}\n\t\tlen = n * sizeof(gid_t);\n\n\t\tret = groups_to_user((gid_t __user *)optval, cred->group_info);\n\t\tput_cred(cred);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERNAME:\n\t{\n\t\tchar address[128];\n\n\t\tlv = sock->ops->getname(sock, (struct sockaddr *)address, 2);\n\t\tif (lv < 0)\n\t\t\treturn -ENOTCONN;\n\t\tif (lv < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(optval, address, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\t/* Dubious BSD thing... Probably nobody even uses it, but\n\t * the UNIX standard wants it for whatever reason... -DaveM\n\t */\n\tcase SO_ACCEPTCONN:\n\t\tv.val = sk->sk_state == TCP_LISTEN;\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tv.val = !!test_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERSEC:\n\t\treturn security_socket_getpeersec_stream(sock, optval, optlen, len);\n\n\tcase SO_MARK:\n\t\tv.val = sk->sk_mark;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tv.val = sock_flag(sk, SOCK_RXQ_OVFL);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tv.val = sock_flag(sk, SOCK_WIFI_STATUS);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (!sock->ops->set_peek_off)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tv.val = sk->sk_peek_off;\n\t\tbreak;\n\tcase SO_NOFCS:\n\t\tv.val = sock_flag(sk, SOCK_NOFCS);\n\t\tbreak;\n\n\tcase SO_BINDTODEVICE:\n\t\treturn sock_getbindtodevice(sk, optval, optlen, len);\n\n\tcase SO_GET_FILTER:\n\t\tlen = sk_get_filter(sk, (struct sock_filter __user *)optval, len);\n\t\tif (len < 0)\n\t\t\treturn len;\n\n\t\tgoto lenout;\n\n\tcase SO_LOCK_FILTER:\n\t\tv.val = sock_flag(sk, SOCK_FILTER_LOCKED);\n\t\tbreak;\n\n\tcase SO_BPF_EXTENSIONS:\n\t\tv.val = bpf_tell_extensions();\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tv.val = sock_flag(sk, SOCK_SELECT_ERR_QUEUE);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\tv.val = sk->sk_ll_usec;\n\t\tbreak;\n\tcase SO_PREFER_BUSY_POLL:\n\t\tv.val = READ_ONCE(sk->sk_prefer_busy_poll);\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\tif (sizeof(v.ulval) != sizeof(v.val) && len >= sizeof(v.ulval)) {\n\t\t\tlv = sizeof(v.ulval);\n\t\t\tv.ulval = sk->sk_max_pacing_rate;\n\t\t} else {\n\t\t\t/* 32bit version */\n\t\t\tv.val = min_t(unsigned long, sk->sk_max_pacing_rate, ~0U);\n\t\t}\n\t\tbreak;\n\n\tcase SO_INCOMING_CPU:\n\t\tv.val = READ_ONCE(sk->sk_incoming_cpu);\n\t\tbreak;\n\n\tcase SO_MEMINFO:\n\t{\n\t\tu32 meminfo[SK_MEMINFO_VARS];\n\n\t\tsk_get_meminfo(sk, meminfo);\n\n\t\tlen = min_t(unsigned int, len, sizeof(meminfo));\n\t\tif (copy_to_user(optval, &meminfo, len))\n\t\t\treturn -EFAULT;\n\n\t\tgoto lenout;\n\t}\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_INCOMING_NAPI_ID:\n\t\tv.val = READ_ONCE(sk->sk_napi_id);\n\n\t\t/* aggregate non-NAPI IDs down to 0 */\n\t\tif (v.val < MIN_NAPI_ID)\n\t\t\tv.val = 0;\n\n\t\tbreak;\n#endif\n\n\tcase SO_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len < lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_gen_cookie(sk);\n\t\tbreak;\n\n\tcase SO_ZEROCOPY:\n\t\tv.val = sock_flag(sk, SOCK_ZEROCOPY);\n\t\tbreak;\n\n\tcase SO_TXTIME:\n\t\tlv = sizeof(v.txtime);\n\t\tv.txtime.clockid = sk->sk_clockid;\n\t\tv.txtime.flags |= sk->sk_txtime_deadline_mode ?\n\t\t\t\t  SOF_TXTIME_DEADLINE_MODE : 0;\n\t\tv.txtime.flags |= sk->sk_txtime_report_errors ?\n\t\t\t\t  SOF_TXTIME_REPORT_ERRORS : 0;\n\t\tbreak;\n\n\tcase SO_BINDTOIFINDEX:\n\t\tv.val = sk->sk_bound_dev_if;\n\t\tbreak;\n\n\tcase SO_NETNS_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len != lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_net(sk)->net_cookie;\n\t\tbreak;\n\n\tcase SO_BUF_LOCK:\n\t\tv.val = sk->sk_userlocks & SOCK_BUF_LOCK_MASK;\n\t\tbreak;\n\n\tdefault:\n\t\t/* We implement the SO_SNDLOWAT etc to not be settable\n\t\t * (1003.1g 7).\n\t\t */\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (copy_to_user(optval, &v, len))\n\t\treturn -EFAULT;\nlenout:\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -148,7 +148,11 @@\n \t\tstruct ucred peercred;\n \t\tif (len > sizeof(peercred))\n \t\t\tlen = sizeof(peercred);\n+\n+\t\tspin_lock(&sk->sk_peer_lock);\n \t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n+\t\tspin_unlock(&sk->sk_peer_lock);\n+\n \t\tif (copy_to_user(optval, &peercred, len))\n \t\t\treturn -EFAULT;\n \t\tgoto lenout;\n@@ -156,20 +160,23 @@\n \n \tcase SO_PEERGROUPS:\n \t{\n+\t\tconst struct cred *cred;\n \t\tint ret, n;\n \n-\t\tif (!sk->sk_peer_cred)\n+\t\tcred = sk_get_peer_cred(sk);\n+\t\tif (!cred)\n \t\t\treturn -ENODATA;\n \n-\t\tn = sk->sk_peer_cred->group_info->ngroups;\n+\t\tn = cred->group_info->ngroups;\n \t\tif (len < n * sizeof(gid_t)) {\n \t\t\tlen = n * sizeof(gid_t);\n+\t\t\tput_cred(cred);\n \t\t\treturn put_user(len, optlen) ? -EFAULT : -ERANGE;\n \t\t}\n \t\tlen = n * sizeof(gid_t);\n \n-\t\tret = groups_to_user((gid_t __user *)optval,\n-\t\t\t\t     sk->sk_peer_cred->group_info);\n+\t\tret = groups_to_user((gid_t __user *)optval, cred->group_info);\n+\t\tput_cred(cred);\n \t\tif (ret)\n \t\t\treturn ret;\n \t\tgoto lenout;",
        "function_modified_lines": {
            "added": [
                "",
                "\t\tspin_lock(&sk->sk_peer_lock);",
                "\t\tspin_unlock(&sk->sk_peer_lock);",
                "",
                "\t\tconst struct cred *cred;",
                "\t\tcred = sk_get_peer_cred(sk);",
                "\t\tif (!cred)",
                "\t\tn = cred->group_info->ngroups;",
                "\t\t\tput_cred(cred);",
                "\t\tret = groups_to_user((gid_t __user *)optval, cred->group_info);",
                "\t\tput_cred(cred);"
            ],
            "deleted": [
                "\t\tif (!sk->sk_peer_cred)",
                "\t\tn = sk->sk_peer_cred->group_info->ngroups;",
                "\t\tret = groups_to_user((gid_t __user *)optval,",
                "\t\t\t\t     sk->sk_peer_cred->group_info);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free read flaw was found in sock_getsockopt() in net/core/sock.c due to SO_PEERCRED and SO_PEERGROUPS race with listen() (and connect()) in the Linux kernel. In this flaw, an attacker with a user privileges may crash the system or leak internal kernel information.",
        "id": 3146
    },
    {
        "cve_id": "CVE-2020-36557",
        "code_before_change": "static int vt_disallocate(unsigned int vc_num)\n{\n\tstruct vc_data *vc = NULL;\n\tint ret = 0;\n\n\tconsole_lock();\n\tif (vt_busy(vc_num))\n\t\tret = -EBUSY;\n\telse if (vc_num)\n\t\tvc = vc_deallocate(vc_num);\n\tconsole_unlock();\n\n\tif (vc && vc_num >= MIN_NR_CONSOLES) {\n\t\ttty_port_destroy(&vc->port);\n\t\tkfree(vc);\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int vt_disallocate(unsigned int vc_num)\n{\n\tstruct vc_data *vc = NULL;\n\tint ret = 0;\n\n\tconsole_lock();\n\tif (vt_busy(vc_num))\n\t\tret = -EBUSY;\n\telse if (vc_num)\n\t\tvc = vc_deallocate(vc_num);\n\tconsole_unlock();\n\n\tif (vc && vc_num >= MIN_NR_CONSOLES)\n\t\ttty_port_put(&vc->port);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,10 +10,8 @@\n \t\tvc = vc_deallocate(vc_num);\n \tconsole_unlock();\n \n-\tif (vc && vc_num >= MIN_NR_CONSOLES) {\n-\t\ttty_port_destroy(&vc->port);\n-\t\tkfree(vc);\n-\t}\n+\tif (vc && vc_num >= MIN_NR_CONSOLES)\n+\t\ttty_port_put(&vc->port);\n \n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (vc && vc_num >= MIN_NR_CONSOLES)",
                "\t\ttty_port_put(&vc->port);"
            ],
            "deleted": [
                "\tif (vc && vc_num >= MIN_NR_CONSOLES) {",
                "\t\ttty_port_destroy(&vc->port);",
                "\t\tkfree(vc);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free.",
        "id": 2765
    },
    {
        "cve_id": "CVE-2020-36557",
        "code_before_change": "static void vt_disallocate_all(void)\n{\n\tstruct vc_data *vc[MAX_NR_CONSOLES];\n\tint i;\n\n\tconsole_lock();\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++)\n\t\tif (!vt_busy(i))\n\t\t\tvc[i] = vc_deallocate(i);\n\t\telse\n\t\t\tvc[i] = NULL;\n\tconsole_unlock();\n\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++) {\n\t\tif (vc[i] && i >= MIN_NR_CONSOLES) {\n\t\t\ttty_port_destroy(&vc[i]->port);\n\t\t\tkfree(vc[i]);\n\t\t}\n\t}\n}",
        "code_after_change": "static void vt_disallocate_all(void)\n{\n\tstruct vc_data *vc[MAX_NR_CONSOLES];\n\tint i;\n\n\tconsole_lock();\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++)\n\t\tif (!vt_busy(i))\n\t\t\tvc[i] = vc_deallocate(i);\n\t\telse\n\t\t\tvc[i] = NULL;\n\tconsole_unlock();\n\n\tfor (i = 1; i < MAX_NR_CONSOLES; i++) {\n\t\tif (vc[i] && i >= MIN_NR_CONSOLES)\n\t\t\ttty_port_put(&vc[i]->port);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,9 +12,7 @@\n \tconsole_unlock();\n \n \tfor (i = 1; i < MAX_NR_CONSOLES; i++) {\n-\t\tif (vc[i] && i >= MIN_NR_CONSOLES) {\n-\t\t\ttty_port_destroy(&vc[i]->port);\n-\t\t\tkfree(vc[i]);\n-\t\t}\n+\t\tif (vc[i] && i >= MIN_NR_CONSOLES)\n+\t\t\ttty_port_put(&vc[i]->port);\n \t}\n }",
        "function_modified_lines": {
            "added": [
                "\t\tif (vc[i] && i >= MIN_NR_CONSOLES)",
                "\t\t\ttty_port_put(&vc[i]->port);"
            ],
            "deleted": [
                "\t\tif (vc[i] && i >= MIN_NR_CONSOLES) {",
                "\t\t\ttty_port_destroy(&vc[i]->port);",
                "\t\t\tkfree(vc[i]);",
                "\t\t}"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free.",
        "id": 2766
    },
    {
        "cve_id": "CVE-2016-2069",
        "code_before_change": "void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, unsigned long vmflag)\n{\n\tunsigned long addr;\n\t/* do a global flush by default */\n\tunsigned long base_pages_to_flush = TLB_FLUSH_ALL;\n\n\tpreempt_disable();\n\tif (current->active_mm != mm)\n\t\tgoto out;\n\n\tif (!current->mm) {\n\t\tleave_mm(smp_processor_id());\n\t\tgoto out;\n\t}\n\n\tif ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))\n\t\tbase_pages_to_flush = (end - start) >> PAGE_SHIFT;\n\n\tif (base_pages_to_flush > tlb_single_page_flush_ceiling) {\n\t\tbase_pages_to_flush = TLB_FLUSH_ALL;\n\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\t\tlocal_flush_tlb();\n\t} else {\n\t\t/* flush range by one by one 'invlpg' */\n\t\tfor (addr = start; addr < end;\taddr += PAGE_SIZE) {\n\t\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);\n\t\t\t__flush_tlb_single(addr);\n\t\t}\n\t}\n\ttrace_tlb_flush(TLB_LOCAL_MM_SHOOTDOWN, base_pages_to_flush);\nout:\n\tif (base_pages_to_flush == TLB_FLUSH_ALL) {\n\t\tstart = 0UL;\n\t\tend = TLB_FLUSH_ALL;\n\t}\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, end);\n\tpreempt_enable();\n}",
        "code_after_change": "void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, unsigned long vmflag)\n{\n\tunsigned long addr;\n\t/* do a global flush by default */\n\tunsigned long base_pages_to_flush = TLB_FLUSH_ALL;\n\n\tpreempt_disable();\n\tif (current->active_mm != mm) {\n\t\t/* Synchronize with switch_mm. */\n\t\tsmp_mb();\n\n\t\tgoto out;\n\t}\n\n\tif (!current->mm) {\n\t\tleave_mm(smp_processor_id());\n\n\t\t/* Synchronize with switch_mm. */\n\t\tsmp_mb();\n\n\t\tgoto out;\n\t}\n\n\tif ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))\n\t\tbase_pages_to_flush = (end - start) >> PAGE_SHIFT;\n\n\t/*\n\t * Both branches below are implicit full barriers (MOV to CR or\n\t * INVLPG) that synchronize with switch_mm.\n\t */\n\tif (base_pages_to_flush > tlb_single_page_flush_ceiling) {\n\t\tbase_pages_to_flush = TLB_FLUSH_ALL;\n\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\t\tlocal_flush_tlb();\n\t} else {\n\t\t/* flush range by one by one 'invlpg' */\n\t\tfor (addr = start; addr < end;\taddr += PAGE_SIZE) {\n\t\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);\n\t\t\t__flush_tlb_single(addr);\n\t\t}\n\t}\n\ttrace_tlb_flush(TLB_LOCAL_MM_SHOOTDOWN, base_pages_to_flush);\nout:\n\tif (base_pages_to_flush == TLB_FLUSH_ALL) {\n\t\tstart = 0UL;\n\t\tend = TLB_FLUSH_ALL;\n\t}\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, start, end);\n\tpreempt_enable();\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,17 +6,29 @@\n \tunsigned long base_pages_to_flush = TLB_FLUSH_ALL;\n \n \tpreempt_disable();\n-\tif (current->active_mm != mm)\n+\tif (current->active_mm != mm) {\n+\t\t/* Synchronize with switch_mm. */\n+\t\tsmp_mb();\n+\n \t\tgoto out;\n+\t}\n \n \tif (!current->mm) {\n \t\tleave_mm(smp_processor_id());\n+\n+\t\t/* Synchronize with switch_mm. */\n+\t\tsmp_mb();\n+\n \t\tgoto out;\n \t}\n \n \tif ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))\n \t\tbase_pages_to_flush = (end - start) >> PAGE_SHIFT;\n \n+\t/*\n+\t * Both branches below are implicit full barriers (MOV to CR or\n+\t * INVLPG) that synchronize with switch_mm.\n+\t */\n \tif (base_pages_to_flush > tlb_single_page_flush_ceiling) {\n \t\tbase_pages_to_flush = TLB_FLUSH_ALL;\n \t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);",
        "function_modified_lines": {
            "added": [
                "\tif (current->active_mm != mm) {",
                "\t\t/* Synchronize with switch_mm. */",
                "\t\tsmp_mb();",
                "",
                "\t}",
                "",
                "\t\t/* Synchronize with switch_mm. */",
                "\t\tsmp_mb();",
                "",
                "\t/*",
                "\t * Both branches below are implicit full barriers (MOV to CR or",
                "\t * INVLPG) that synchronize with switch_mm.",
                "\t */"
            ],
            "deleted": [
                "\tif (current->active_mm != mm)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in arch/x86/mm/tlb.c in the Linux kernel before 4.4.1 allows local users to gain privileges by triggering access to a paging structure by a different CPU.",
        "id": 919
    },
    {
        "cve_id": "CVE-2017-12146",
        "code_before_change": "static ssize_t driver_override_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\n\treturn sprintf(buf, \"%s\\n\", pdev->driver_override);\n}",
        "code_after_change": "static ssize_t driver_override_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tssize_t len;\n\n\tdevice_lock(dev);\n\tlen = sprintf(buf, \"%s\\n\", pdev->driver_override);\n\tdevice_unlock(dev);\n\treturn len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,10 @@\n \t\t\t\t    struct device_attribute *attr, char *buf)\n {\n \tstruct platform_device *pdev = to_platform_device(dev);\n+\tssize_t len;\n \n-\treturn sprintf(buf, \"%s\\n\", pdev->driver_override);\n+\tdevice_lock(dev);\n+\tlen = sprintf(buf, \"%s\\n\", pdev->driver_override);\n+\tdevice_unlock(dev);\n+\treturn len;\n }",
        "function_modified_lines": {
            "added": [
                "\tssize_t len;",
                "\tdevice_lock(dev);",
                "\tlen = sprintf(buf, \"%s\\n\", pdev->driver_override);",
                "\tdevice_unlock(dev);",
                "\treturn len;"
            ],
            "deleted": [
                "\treturn sprintf(buf, \"%s\\n\", pdev->driver_override);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "The driver_override implementation in drivers/base/platform.c in the Linux kernel before 4.12.1 allows local users to gain privileges by leveraging a race condition between a read operation and a store operation that involve different overrides.",
        "id": 1255
    },
    {
        "cve_id": "CVE-2021-3609",
        "code_before_change": "static int bcm_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net;\n\tstruct bcm_sock *bo;\n\tstruct bcm_op *op, *next;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\tbo = bcm_sk(sk);\n\n\t/* remove bcm_ops, timer, rx_unregister(), etc. */\n\n\tspin_lock(&bcm_notifier_lock);\n\twhile (bcm_busy_notifier == bo) {\n\t\tspin_unlock(&bcm_notifier_lock);\n\t\tschedule_timeout_uninterruptible(1);\n\t\tspin_lock(&bcm_notifier_lock);\n\t}\n\tlist_del(&bo->notifier);\n\tspin_unlock(&bcm_notifier_lock);\n\n\tlock_sock(sk);\n\n\tlist_for_each_entry_safe(op, next, &bo->tx_ops, list)\n\t\tbcm_remove_op(op);\n\n\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list) {\n\t\t/*\n\t\t * Don't care if we're bound or not (due to netdev problems)\n\t\t * can_rx_unregister() is always a save thing to do here.\n\t\t */\n\t\tif (op->ifindex) {\n\t\t\t/*\n\t\t\t * Only remove subscriptions that had not\n\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t * in bcm_notifier()\n\t\t\t */\n\t\t\tif (op->rx_reg_dev) {\n\t\t\t\tstruct net_device *dev;\n\n\t\t\t\tdev = dev_get_by_index(net, op->ifindex);\n\t\t\t\tif (dev) {\n\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\tdev_put(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tcan_rx_unregister(net, NULL, op->can_id,\n\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t  bcm_rx_handler, op);\n\n\t\tbcm_remove_op(op);\n\t}\n\n#if IS_ENABLED(CONFIG_PROC_FS)\n\t/* remove procfs entry */\n\tif (net->can.bcmproc_dir && bo->bcm_proc_read)\n\t\tremove_proc_entry(bo->procname, net->can.bcmproc_dir);\n#endif /* CONFIG_PROC_FS */\n\n\t/* remove device reference */\n\tif (bo->bound) {\n\t\tbo->bound   = 0;\n\t\tbo->ifindex = 0;\n\t}\n\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\trelease_sock(sk);\n\tsock_put(sk);\n\n\treturn 0;\n}",
        "code_after_change": "static int bcm_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net;\n\tstruct bcm_sock *bo;\n\tstruct bcm_op *op, *next;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\tbo = bcm_sk(sk);\n\n\t/* remove bcm_ops, timer, rx_unregister(), etc. */\n\n\tspin_lock(&bcm_notifier_lock);\n\twhile (bcm_busy_notifier == bo) {\n\t\tspin_unlock(&bcm_notifier_lock);\n\t\tschedule_timeout_uninterruptible(1);\n\t\tspin_lock(&bcm_notifier_lock);\n\t}\n\tlist_del(&bo->notifier);\n\tspin_unlock(&bcm_notifier_lock);\n\n\tlock_sock(sk);\n\n\tlist_for_each_entry_safe(op, next, &bo->tx_ops, list)\n\t\tbcm_remove_op(op);\n\n\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list) {\n\t\t/*\n\t\t * Don't care if we're bound or not (due to netdev problems)\n\t\t * can_rx_unregister() is always a save thing to do here.\n\t\t */\n\t\tif (op->ifindex) {\n\t\t\t/*\n\t\t\t * Only remove subscriptions that had not\n\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t * in bcm_notifier()\n\t\t\t */\n\t\t\tif (op->rx_reg_dev) {\n\t\t\t\tstruct net_device *dev;\n\n\t\t\t\tdev = dev_get_by_index(net, op->ifindex);\n\t\t\t\tif (dev) {\n\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\tdev_put(dev);\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tcan_rx_unregister(net, NULL, op->can_id,\n\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t  bcm_rx_handler, op);\n\n\t}\n\n\tsynchronize_rcu();\n\n\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list)\n\t\tbcm_remove_op(op);\n\n#if IS_ENABLED(CONFIG_PROC_FS)\n\t/* remove procfs entry */\n\tif (net->can.bcmproc_dir && bo->bcm_proc_read)\n\t\tremove_proc_entry(bo->procname, net->can.bcmproc_dir);\n#endif /* CONFIG_PROC_FS */\n\n\t/* remove device reference */\n\tif (bo->bound) {\n\t\tbo->bound   = 0;\n\t\tbo->ifindex = 0;\n\t}\n\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\trelease_sock(sk);\n\tsock_put(sk);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -52,8 +52,12 @@\n \t\t\t\t\t  REGMASK(op->can_id),\n \t\t\t\t\t  bcm_rx_handler, op);\n \n+\t}\n+\n+\tsynchronize_rcu();\n+\n+\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list)\n \t\tbcm_remove_op(op);\n-\t}\n \n #if IS_ENABLED(CONFIG_PROC_FS)\n \t/* remove procfs entry */",
        "function_modified_lines": {
            "added": [
                "\t}",
                "",
                "\tsynchronize_rcu();",
                "",
                "\tlist_for_each_entry_safe(op, next, &bo->rx_ops, list)"
            ],
            "deleted": [
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": ".A flaw was found in the CAN BCM networking protocol in the Linux kernel, where a local attacker can abuse a flaw in the CAN subsystem to corrupt memory, crash the system or escalate privileges. This race condition in net/can/bcm.c in the Linux kernel allows for local privilege escalation to root.",
        "id": 3021
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "evtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))\n\t\treturn 0;\n\n\treturn info_for_irq(irq)->evtchn;\n}",
        "code_after_change": "evtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tconst struct irq_info *info = NULL;\n\n\tif (likely(irq < nr_irqs))\n\t\tinfo = info_for_irq(irq);\n\tif (!info)\n\t\treturn 0;\n\n\treturn info->evtchn;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,11 @@\n evtchn_port_t evtchn_from_irq(unsigned irq)\n {\n-\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))\n+\tconst struct irq_info *info = NULL;\n+\n+\tif (likely(irq < nr_irqs))\n+\t\tinfo = info_for_irq(irq);\n+\tif (!info)\n \t\treturn 0;\n \n-\treturn info_for_irq(irq)->evtchn;\n+\treturn info->evtchn;\n }",
        "function_modified_lines": {
            "added": [
                "\tconst struct irq_info *info = NULL;",
                "",
                "\tif (likely(irq < nr_irqs))",
                "\t\tinfo = info_for_irq(irq);",
                "\tif (!info)",
                "\treturn info->evtchn;"
            ],
            "deleted": [
                "\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))",
                "\treturn info_for_irq(irq)->evtchn;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "id": 2626
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tevtchn_to_irq[row][col] = -1;\n}",
        "code_after_change": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,5 +3,5 @@\n \tunsigned col;\n \n \tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n-\t\tevtchn_to_irq[row][col] = -1;\n+\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);\n }",
        "function_modified_lines": {
            "added": [
                "\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);"
            ],
            "deleted": [
                "\t\tevtchn_to_irq[row][col] = -1;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "id": 2622
    },
    {
        "cve_id": "CVE-2015-8839",
        "code_before_change": "int ext4_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tint error, rc = 0;\n\tint orphan = 0;\n\tconst unsigned int ia_valid = attr->ia_valid;\n\n\terror = inode_change_ok(inode, attr);\n\tif (error)\n\t\treturn error;\n\n\tif (is_quota_modification(inode, attr)) {\n\t\terror = dquot_initialize(inode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tif ((ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid)) ||\n\t    (ia_valid & ATTR_GID && !gid_eq(attr->ia_gid, inode->i_gid))) {\n\t\thandle_t *handle;\n\n\t\t/* (user+group)*(old+new) structure, inode write (sb,\n\t\t * inode block, ? - but truncate inode update has it) */\n\t\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA,\n\t\t\t(EXT4_MAXQUOTAS_INIT_BLOCKS(inode->i_sb) +\n\t\t\t EXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb)) + 3);\n\t\tif (IS_ERR(handle)) {\n\t\t\terror = PTR_ERR(handle);\n\t\t\tgoto err_out;\n\t\t}\n\t\terror = dquot_transfer(inode, attr);\n\t\tif (error) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn error;\n\t\t}\n\t\t/* Update corresponding info in inode so that everything is in\n\t\t * one transaction */\n\t\tif (attr->ia_valid & ATTR_UID)\n\t\t\tinode->i_uid = attr->ia_uid;\n\t\tif (attr->ia_valid & ATTR_GID)\n\t\t\tinode->i_gid = attr->ia_gid;\n\t\terror = ext4_mark_inode_dirty(handle, inode);\n\t\text4_journal_stop(handle);\n\t}\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\thandle_t *handle;\n\t\tloff_t oldsize = inode->i_size;\n\t\tint shrink = (attr->ia_size <= inode->i_size);\n\n\t\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t\t\tif (attr->ia_size > sbi->s_bitmap_maxbytes)\n\t\t\t\treturn -EFBIG;\n\t\t}\n\t\tif (!S_ISREG(inode->i_mode))\n\t\t\treturn -EINVAL;\n\n\t\tif (IS_I_VERSION(inode) && attr->ia_size != inode->i_size)\n\t\t\tinode_inc_iversion(inode);\n\n\t\tif (ext4_should_order_data(inode) &&\n\t\t    (attr->ia_size < inode->i_size)) {\n\t\t\terror = ext4_begin_ordered_truncate(inode,\n\t\t\t\t\t\t\t    attr->ia_size);\n\t\t\tif (error)\n\t\t\t\tgoto err_out;\n\t\t}\n\t\tif (attr->ia_size != inode->i_size) {\n\t\t\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 3);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\terror = PTR_ERR(handle);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t\tif (ext4_handle_valid(handle) && shrink) {\n\t\t\t\terror = ext4_orphan_add(handle, inode);\n\t\t\t\torphan = 1;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Update c/mtime on truncate up, ext4_truncate() will\n\t\t\t * update c/mtime in shrink case below\n\t\t\t */\n\t\t\tif (!shrink) {\n\t\t\t\tinode->i_mtime = ext4_current_time(inode);\n\t\t\t\tinode->i_ctime = inode->i_mtime;\n\t\t\t}\n\t\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tEXT4_I(inode)->i_disksize = attr->ia_size;\n\t\t\trc = ext4_mark_inode_dirty(handle, inode);\n\t\t\tif (!error)\n\t\t\t\terror = rc;\n\t\t\t/*\n\t\t\t * We have to update i_size under i_data_sem together\n\t\t\t * with i_disksize to avoid races with writeback code\n\t\t\t * running ext4_wb_update_i_disksize().\n\t\t\t */\n\t\t\tif (!error)\n\t\t\t\ti_size_write(inode, attr->ia_size);\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\text4_journal_stop(handle);\n\t\t\tif (error) {\n\t\t\t\tif (orphan)\n\t\t\t\t\text4_orphan_del(NULL, inode);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t}\n\t\tif (!shrink)\n\t\t\tpagecache_isize_extended(inode, oldsize, inode->i_size);\n\n\t\t/*\n\t\t * Blocks are going to be removed from the inode. Wait\n\t\t * for dio in flight.  Temporarily disable\n\t\t * dioread_nolock to prevent livelock.\n\t\t */\n\t\tif (orphan) {\n\t\t\tif (!ext4_should_journal_data(inode)) {\n\t\t\t\text4_inode_block_unlocked_dio(inode);\n\t\t\t\tinode_dio_wait(inode);\n\t\t\t\text4_inode_resume_unlocked_dio(inode);\n\t\t\t} else\n\t\t\t\text4_wait_for_tail_page_commit(inode);\n\t\t}\n\t\t/*\n\t\t * Truncate pagecache after we've waited for commit\n\t\t * in data=journal mode to make pages freeable.\n\t\t */\n\t\ttruncate_pagecache(inode, inode->i_size);\n\t\tif (shrink)\n\t\t\text4_truncate(inode);\n\t}\n\n\tif (!rc) {\n\t\tsetattr_copy(inode, attr);\n\t\tmark_inode_dirty(inode);\n\t}\n\n\t/*\n\t * If the call to ext4_truncate failed to get a transaction handle at\n\t * all, we need to clean up the in-core orphan list manually.\n\t */\n\tif (orphan && inode->i_nlink)\n\t\text4_orphan_del(NULL, inode);\n\n\tif (!rc && (ia_valid & ATTR_MODE))\n\t\trc = posix_acl_chmod(inode, inode->i_mode);\n\nerr_out:\n\text4_std_error(inode->i_sb, error);\n\tif (!error)\n\t\terror = rc;\n\treturn error;\n}",
        "code_after_change": "int ext4_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tint error, rc = 0;\n\tint orphan = 0;\n\tconst unsigned int ia_valid = attr->ia_valid;\n\n\terror = inode_change_ok(inode, attr);\n\tif (error)\n\t\treturn error;\n\n\tif (is_quota_modification(inode, attr)) {\n\t\terror = dquot_initialize(inode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tif ((ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid)) ||\n\t    (ia_valid & ATTR_GID && !gid_eq(attr->ia_gid, inode->i_gid))) {\n\t\thandle_t *handle;\n\n\t\t/* (user+group)*(old+new) structure, inode write (sb,\n\t\t * inode block, ? - but truncate inode update has it) */\n\t\thandle = ext4_journal_start(inode, EXT4_HT_QUOTA,\n\t\t\t(EXT4_MAXQUOTAS_INIT_BLOCKS(inode->i_sb) +\n\t\t\t EXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb)) + 3);\n\t\tif (IS_ERR(handle)) {\n\t\t\terror = PTR_ERR(handle);\n\t\t\tgoto err_out;\n\t\t}\n\t\terror = dquot_transfer(inode, attr);\n\t\tif (error) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn error;\n\t\t}\n\t\t/* Update corresponding info in inode so that everything is in\n\t\t * one transaction */\n\t\tif (attr->ia_valid & ATTR_UID)\n\t\t\tinode->i_uid = attr->ia_uid;\n\t\tif (attr->ia_valid & ATTR_GID)\n\t\t\tinode->i_gid = attr->ia_gid;\n\t\terror = ext4_mark_inode_dirty(handle, inode);\n\t\text4_journal_stop(handle);\n\t}\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\thandle_t *handle;\n\t\tloff_t oldsize = inode->i_size;\n\t\tint shrink = (attr->ia_size <= inode->i_size);\n\n\t\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t\t\tif (attr->ia_size > sbi->s_bitmap_maxbytes)\n\t\t\t\treturn -EFBIG;\n\t\t}\n\t\tif (!S_ISREG(inode->i_mode))\n\t\t\treturn -EINVAL;\n\n\t\tif (IS_I_VERSION(inode) && attr->ia_size != inode->i_size)\n\t\t\tinode_inc_iversion(inode);\n\n\t\tif (ext4_should_order_data(inode) &&\n\t\t    (attr->ia_size < inode->i_size)) {\n\t\t\terror = ext4_begin_ordered_truncate(inode,\n\t\t\t\t\t\t\t    attr->ia_size);\n\t\t\tif (error)\n\t\t\t\tgoto err_out;\n\t\t}\n\t\tif (attr->ia_size != inode->i_size) {\n\t\t\thandle = ext4_journal_start(inode, EXT4_HT_INODE, 3);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\terror = PTR_ERR(handle);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t\tif (ext4_handle_valid(handle) && shrink) {\n\t\t\t\terror = ext4_orphan_add(handle, inode);\n\t\t\t\torphan = 1;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Update c/mtime on truncate up, ext4_truncate() will\n\t\t\t * update c/mtime in shrink case below\n\t\t\t */\n\t\t\tif (!shrink) {\n\t\t\t\tinode->i_mtime = ext4_current_time(inode);\n\t\t\t\tinode->i_ctime = inode->i_mtime;\n\t\t\t}\n\t\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tEXT4_I(inode)->i_disksize = attr->ia_size;\n\t\t\trc = ext4_mark_inode_dirty(handle, inode);\n\t\t\tif (!error)\n\t\t\t\terror = rc;\n\t\t\t/*\n\t\t\t * We have to update i_size under i_data_sem together\n\t\t\t * with i_disksize to avoid races with writeback code\n\t\t\t * running ext4_wb_update_i_disksize().\n\t\t\t */\n\t\t\tif (!error)\n\t\t\t\ti_size_write(inode, attr->ia_size);\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\text4_journal_stop(handle);\n\t\t\tif (error) {\n\t\t\t\tif (orphan)\n\t\t\t\t\text4_orphan_del(NULL, inode);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t}\n\t\tif (!shrink)\n\t\t\tpagecache_isize_extended(inode, oldsize, inode->i_size);\n\n\t\t/*\n\t\t * Blocks are going to be removed from the inode. Wait\n\t\t * for dio in flight.  Temporarily disable\n\t\t * dioread_nolock to prevent livelock.\n\t\t */\n\t\tif (orphan) {\n\t\t\tif (!ext4_should_journal_data(inode)) {\n\t\t\t\text4_inode_block_unlocked_dio(inode);\n\t\t\t\tinode_dio_wait(inode);\n\t\t\t\text4_inode_resume_unlocked_dio(inode);\n\t\t\t} else\n\t\t\t\text4_wait_for_tail_page_commit(inode);\n\t\t}\n\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\t\t/*\n\t\t * Truncate pagecache after we've waited for commit\n\t\t * in data=journal mode to make pages freeable.\n\t\t */\n\t\ttruncate_pagecache(inode, inode->i_size);\n\t\tif (shrink)\n\t\t\text4_truncate(inode);\n\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\t}\n\n\tif (!rc) {\n\t\tsetattr_copy(inode, attr);\n\t\tmark_inode_dirty(inode);\n\t}\n\n\t/*\n\t * If the call to ext4_truncate failed to get a transaction handle at\n\t * all, we need to clean up the in-core orphan list manually.\n\t */\n\tif (orphan && inode->i_nlink)\n\t\text4_orphan_del(NULL, inode);\n\n\tif (!rc && (ia_valid & ATTR_MODE))\n\t\trc = posix_acl_chmod(inode, inode->i_mode);\n\nerr_out:\n\text4_std_error(inode->i_sb, error);\n\tif (!error)\n\t\terror = rc;\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -120,6 +120,7 @@\n \t\t\t} else\n \t\t\t\text4_wait_for_tail_page_commit(inode);\n \t\t}\n+\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n \t\t/*\n \t\t * Truncate pagecache after we've waited for commit\n \t\t * in data=journal mode to make pages freeable.\n@@ -127,6 +128,7 @@\n \t\ttruncate_pagecache(inode, inode->i_size);\n \t\tif (shrink)\n \t\t\text4_truncate(inode);\n+\t\tup_write(&EXT4_I(inode)->i_mmap_sem);\n \t}\n \n \tif (!rc) {",
        "function_modified_lines": {
            "added": [
                "\t\tdown_write(&EXT4_I(inode)->i_mmap_sem);",
                "\t\tup_write(&EXT4_I(inode)->i_mmap_sem);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Multiple race conditions in the ext4 filesystem implementation in the Linux kernel before 4.5 allow local users to cause a denial of service (disk corruption) by writing to a page that is associated with a different user's file after unsynchronized hole punching and page-fault handling.",
        "id": 857
    },
    {
        "cve_id": "CVE-2018-12232",
        "code_before_change": "static int sock_close(struct inode *inode, struct file *filp)\n{\n\tsock_release(SOCKET_I(inode));\n\treturn 0;\n}",
        "code_after_change": "static int sock_close(struct inode *inode, struct file *filp)\n{\n\t__sock_release(SOCKET_I(inode), inode);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,5 @@\n static int sock_close(struct inode *inode, struct file *filp)\n {\n-\tsock_release(SOCKET_I(inode));\n+\t__sock_release(SOCKET_I(inode), inode);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t__sock_release(SOCKET_I(inode), inode);"
            ],
            "deleted": [
                "\tsock_release(SOCKET_I(inode));"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In net/socket.c in the Linux kernel through 4.17.1, there is a race condition between fchownat and close in cases where they target the same socket file descriptor, related to the sock_close and sockfs_setattr functions. fchownat does not increment the file descriptor reference count, which allows close to set the socket to NULL during fchownat's execution, leading to a NULL pointer dereference and system crash.",
        "id": 1650
    },
    {
        "cve_id": "CVE-2023-35823",
        "code_before_change": "void saa7134_video_fini(struct saa7134_dev *dev)\n{\n\t/* free stuff */\n\tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n\tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);\n\tv4l2_ctrl_handler_free(&dev->ctrl_handler);\n\tif (card_has_radio(dev))\n\t\tv4l2_ctrl_handler_free(&dev->radio_ctrl_handler);\n}",
        "code_after_change": "void saa7134_video_fini(struct saa7134_dev *dev)\n{\n\tdel_timer_sync(&dev->video_q.timeout);\n\t/* free stuff */\n\tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n\tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);\n\tv4l2_ctrl_handler_free(&dev->ctrl_handler);\n\tif (card_has_radio(dev))\n\t\tv4l2_ctrl_handler_free(&dev->radio_ctrl_handler);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n void saa7134_video_fini(struct saa7134_dev *dev)\n {\n+\tdel_timer_sync(&dev->video_q.timeout);\n \t/* free stuff */\n \tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n \tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);",
        "function_modified_lines": {
            "added": [
                "\tdel_timer_sync(&dev->video_q.timeout);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in saa7134_finidev in drivers/media/pci/saa7134/saa7134-core.c.",
        "id": 4111
    },
    {
        "cve_id": "CVE-2023-35823",
        "code_before_change": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\treturn 0;\n}",
        "code_after_change": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\tdel_timer_sync(&dev->vbi_q.timeout);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n int saa7134_vbi_fini(struct saa7134_dev *dev)\n {\n \t/* nothing */\n+\tdel_timer_sync(&dev->vbi_q.timeout);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tdel_timer_sync(&dev->vbi_q.timeout);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in saa7134_finidev in drivers/media/pci/saa7134/saa7134-core.c.",
        "id": 4110
    },
    {
        "cve_id": "CVE-2021-39686",
        "code_before_change": "static int binder_open(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc, *itr;\n\tstruct binder_device *binder_dev;\n\tstruct binderfs_info *info;\n\tstruct dentry *binder_binderfs_dir_entry_proc = NULL;\n\tbool existing_pid = false;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE, \"%s: %d:%d\\n\", __func__,\n\t\t     current->group_leader->pid, current->pid);\n\n\tproc = kzalloc(sizeof(*proc), GFP_KERNEL);\n\tif (proc == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&proc->inner_lock);\n\tspin_lock_init(&proc->outer_lock);\n\tget_task_struct(current->group_leader);\n\tproc->tsk = current->group_leader;\n\tINIT_LIST_HEAD(&proc->todo);\n\tinit_waitqueue_head(&proc->freeze_wait);\n\tproc->default_priority = task_nice(current);\n\t/* binderfs stashes devices in i_private */\n\tif (is_binderfs_device(nodp)) {\n\t\tbinder_dev = nodp->i_private;\n\t\tinfo = nodp->i_sb->s_fs_info;\n\t\tbinder_binderfs_dir_entry_proc = info->proc_log_dir;\n\t} else {\n\t\tbinder_dev = container_of(filp->private_data,\n\t\t\t\t\t  struct binder_device, miscdev);\n\t}\n\trefcount_inc(&binder_dev->ref);\n\tproc->context = &binder_dev->context;\n\tbinder_alloc_init(&proc->alloc);\n\n\tbinder_stats_created(BINDER_STAT_PROC);\n\tproc->pid = current->group_leader->pid;\n\tINIT_LIST_HEAD(&proc->delivered_death);\n\tINIT_LIST_HEAD(&proc->waiting_threads);\n\tfilp->private_data = proc;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(itr, &binder_procs, proc_node) {\n\t\tif (itr->pid == proc->pid) {\n\t\t\texisting_pid = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\thlist_add_head(&proc->proc_node, &binder_procs);\n\tmutex_unlock(&binder_procs_lock);\n\n\tif (binder_debugfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * proc debug entries are shared between contexts.\n\t\t * Only create for the first PID to avoid debugfs log spamming\n\t\t * The printing code will anyway print all contexts for a given\n\t\t * PID so this is not a problem.\n\t\t */\n\t\tproc->debugfs_entry = debugfs_create_file(strbuf, 0444,\n\t\t\tbinder_debugfs_dir_entry_proc,\n\t\t\t(void *)(unsigned long)proc->pid,\n\t\t\t&proc_fops);\n\t}\n\n\tif (binder_binderfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\t\tstruct dentry *binderfs_entry;\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * Similar to debugfs, the process specific log file is shared\n\t\t * between contexts. Only create for the first PID.\n\t\t * This is ok since same as debugfs, the log file will contain\n\t\t * information on all contexts of a given PID.\n\t\t */\n\t\tbinderfs_entry = binderfs_create_file(binder_binderfs_dir_entry_proc,\n\t\t\tstrbuf, &proc_fops, (void *)(unsigned long)proc->pid);\n\t\tif (!IS_ERR(binderfs_entry)) {\n\t\t\tproc->binderfs_entry = binderfs_entry;\n\t\t} else {\n\t\t\tint error;\n\n\t\t\terror = PTR_ERR(binderfs_entry);\n\t\t\tpr_warn(\"Unable to create file %s in binderfs (error %d)\\n\",\n\t\t\t\tstrbuf, error);\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int binder_open(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc, *itr;\n\tstruct binder_device *binder_dev;\n\tstruct binderfs_info *info;\n\tstruct dentry *binder_binderfs_dir_entry_proc = NULL;\n\tbool existing_pid = false;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE, \"%s: %d:%d\\n\", __func__,\n\t\t     current->group_leader->pid, current->pid);\n\n\tproc = kzalloc(sizeof(*proc), GFP_KERNEL);\n\tif (proc == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&proc->inner_lock);\n\tspin_lock_init(&proc->outer_lock);\n\tget_task_struct(current->group_leader);\n\tproc->tsk = current->group_leader;\n\tproc->cred = get_cred(filp->f_cred);\n\tINIT_LIST_HEAD(&proc->todo);\n\tinit_waitqueue_head(&proc->freeze_wait);\n\tproc->default_priority = task_nice(current);\n\t/* binderfs stashes devices in i_private */\n\tif (is_binderfs_device(nodp)) {\n\t\tbinder_dev = nodp->i_private;\n\t\tinfo = nodp->i_sb->s_fs_info;\n\t\tbinder_binderfs_dir_entry_proc = info->proc_log_dir;\n\t} else {\n\t\tbinder_dev = container_of(filp->private_data,\n\t\t\t\t\t  struct binder_device, miscdev);\n\t}\n\trefcount_inc(&binder_dev->ref);\n\tproc->context = &binder_dev->context;\n\tbinder_alloc_init(&proc->alloc);\n\n\tbinder_stats_created(BINDER_STAT_PROC);\n\tproc->pid = current->group_leader->pid;\n\tINIT_LIST_HEAD(&proc->delivered_death);\n\tINIT_LIST_HEAD(&proc->waiting_threads);\n\tfilp->private_data = proc;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(itr, &binder_procs, proc_node) {\n\t\tif (itr->pid == proc->pid) {\n\t\t\texisting_pid = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\thlist_add_head(&proc->proc_node, &binder_procs);\n\tmutex_unlock(&binder_procs_lock);\n\n\tif (binder_debugfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * proc debug entries are shared between contexts.\n\t\t * Only create for the first PID to avoid debugfs log spamming\n\t\t * The printing code will anyway print all contexts for a given\n\t\t * PID so this is not a problem.\n\t\t */\n\t\tproc->debugfs_entry = debugfs_create_file(strbuf, 0444,\n\t\t\tbinder_debugfs_dir_entry_proc,\n\t\t\t(void *)(unsigned long)proc->pid,\n\t\t\t&proc_fops);\n\t}\n\n\tif (binder_binderfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\t\tstruct dentry *binderfs_entry;\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * Similar to debugfs, the process specific log file is shared\n\t\t * between contexts. Only create for the first PID.\n\t\t * This is ok since same as debugfs, the log file will contain\n\t\t * information on all contexts of a given PID.\n\t\t */\n\t\tbinderfs_entry = binderfs_create_file(binder_binderfs_dir_entry_proc,\n\t\t\tstrbuf, &proc_fops, (void *)(unsigned long)proc->pid);\n\t\tif (!IS_ERR(binderfs_entry)) {\n\t\t\tproc->binderfs_entry = binderfs_entry;\n\t\t} else {\n\t\t\tint error;\n\n\t\t\terror = PTR_ERR(binderfs_entry);\n\t\t\tpr_warn(\"Unable to create file %s in binderfs (error %d)\\n\",\n\t\t\t\tstrbuf, error);\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,7 @@\n \tspin_lock_init(&proc->outer_lock);\n \tget_task_struct(current->group_leader);\n \tproc->tsk = current->group_leader;\n+\tproc->cred = get_cred(filp->f_cred);\n \tINIT_LIST_HEAD(&proc->todo);\n \tinit_waitqueue_head(&proc->freeze_wait);\n \tproc->default_priority = task_nice(current);",
        "function_modified_lines": {
            "added": [
                "\tproc->cred = get_cred(filp->f_cred);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In several functions of binder.c, there is a possible way to represent the wrong domain to SELinux due to a race condition. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-200688826References: Upstream kernel",
        "id": 3101
    },
    {
        "cve_id": "CVE-2021-3752",
        "code_before_change": "static void l2cap_sock_destruct(struct sock *sk)\n{\n\tBT_DBG(\"sk %p\", sk);\n\n\tif (l2cap_pi(sk)->chan)\n\t\tl2cap_chan_put(l2cap_pi(sk)->chan);\n\n\tif (l2cap_pi(sk)->rx_busy_skb) {\n\t\tkfree_skb(l2cap_pi(sk)->rx_busy_skb);\n\t\tl2cap_pi(sk)->rx_busy_skb = NULL;\n\t}\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tskb_queue_purge(&sk->sk_write_queue);\n}",
        "code_after_change": "static void l2cap_sock_destruct(struct sock *sk)\n{\n\tBT_DBG(\"sk %p\", sk);\n\n\tif (l2cap_pi(sk)->chan) {\n\t\tl2cap_pi(sk)->chan->data = NULL;\n\t\tl2cap_chan_put(l2cap_pi(sk)->chan);\n\t}\n\n\tif (l2cap_pi(sk)->rx_busy_skb) {\n\t\tkfree_skb(l2cap_pi(sk)->rx_busy_skb);\n\t\tl2cap_pi(sk)->rx_busy_skb = NULL;\n\t}\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tskb_queue_purge(&sk->sk_write_queue);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,8 +2,10 @@\n {\n \tBT_DBG(\"sk %p\", sk);\n \n-\tif (l2cap_pi(sk)->chan)\n+\tif (l2cap_pi(sk)->chan) {\n+\t\tl2cap_pi(sk)->chan->data = NULL;\n \t\tl2cap_chan_put(l2cap_pi(sk)->chan);\n+\t}\n \n \tif (l2cap_pi(sk)->rx_busy_skb) {\n \t\tkfree_skb(l2cap_pi(sk)->rx_busy_skb);",
        "function_modified_lines": {
            "added": [
                "\tif (l2cap_pi(sk)->chan) {",
                "\t\tl2cap_pi(sk)->chan->data = NULL;",
                "\t}"
            ],
            "deleted": [
                "\tif (l2cap_pi(sk)->chan)"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s Bluetooth subsystem in the way user calls connect to the socket and disconnect simultaneously due to a race condition. This flaw allows a user to crash the system or escalate their privileges. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability.",
        "id": 3054
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "static bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time.\n\t */\n\tif (q->mq_ops) {\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}",
        "code_after_change": "static bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time. And acquire the tag's\n\t * ownership for flush req.\n\t */\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t\tfq->orig_rq = first_rq;\n\n\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,11 +25,18 @@\n \n \t/*\n \t * Borrow tag from the first request since they can't\n-\t * be in flight at the same time.\n+\t * be in flight at the same time. And acquire the tag's\n+\t * ownership for flush req.\n \t */\n \tif (q->mq_ops) {\n+\t\tstruct blk_mq_hw_ctx *hctx;\n+\n \t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n \t\tflush_rq->tag = first_rq->tag;\n+\t\tfq->orig_rq = first_rq;\n+\n+\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);\n+\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);\n \t}\n \n \tflush_rq->cmd_type = REQ_TYPE_FS;",
        "function_modified_lines": {
            "added": [
                "\t * be in flight at the same time. And acquire the tag's",
                "\t * ownership for flush req.",
                "\t\tstruct blk_mq_hw_ctx *hctx;",
                "",
                "\t\tfq->orig_rq = first_rq;",
                "",
                "\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);",
                "\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);"
            ],
            "deleted": [
                "\t * be in flight at the same time."
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "id": 882
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\tstruct request *rq = tags->rqs[tag];\n\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n\n\tif (!is_flush_request(rq, fq, tag))\n\t\treturn rq;\n\n\treturn fq->flush_rq;\n}",
        "code_after_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\treturn tags->rqs[tag];\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,4 @@\n struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n {\n-\tstruct request *rq = tags->rqs[tag];\n-\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n-\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n-\n-\tif (!is_flush_request(rq, fq, tag))\n-\t\treturn rq;\n-\n-\treturn fq->flush_rq;\n+\treturn tags->rqs[tag];\n }",
        "function_modified_lines": {
            "added": [
                "\treturn tags->rqs[tag];"
            ],
            "deleted": [
                "\tstruct request *rq = tags->rqs[tag];",
                "\t/* mq_ctx of flush rq is always cloned from the corresponding req */",
                "\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);",
                "",
                "\tif (!is_flush_request(rq, fq, tag))",
                "\t\treturn rq;",
                "",
                "\treturn fq->flush_rq;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "id": 885
    },
    {
        "cve_id": "CVE-2018-20836",
        "code_before_change": "static void smp_task_timedout(struct timer_list *t)\n{\n\tstruct sas_task_slow *slow = from_timer(slow, t, timer);\n\tstruct sas_task *task = slow->task;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&task->task_state_lock, flags);\n\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))\n\t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n\tspin_unlock_irqrestore(&task->task_state_lock, flags);\n\n\tcomplete(&task->slow_task->completion);\n}",
        "code_after_change": "static void smp_task_timedout(struct timer_list *t)\n{\n\tstruct sas_task_slow *slow = from_timer(slow, t, timer);\n\tstruct sas_task *task = slow->task;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&task->task_state_lock, flags);\n\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {\n\t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n\t\tcomplete(&task->slow_task->completion);\n\t}\n\tspin_unlock_irqrestore(&task->task_state_lock, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,9 +5,9 @@\n \tunsigned long flags;\n \n \tspin_lock_irqsave(&task->task_state_lock, flags);\n-\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))\n+\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {\n \t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n+\t\tcomplete(&task->slow_task->completion);\n+\t}\n \tspin_unlock_irqrestore(&task->task_state_lock, flags);\n-\n-\tcomplete(&task->slow_task->completion);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {",
                "\t\tcomplete(&task->slow_task->completion);",
                "\t}"
            ],
            "deleted": [
                "\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))",
                "",
                "\tcomplete(&task->slow_task->completion);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.20. There is a race condition in smp_task_timedout() and smp_task_done() in drivers/scsi/libsas/sas_expander.c, leading to a use-after-free.",
        "id": 1782
    },
    {
        "cve_id": "CVE-2023-4732",
        "code_before_change": "void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n{\n\tstruct vm_area_struct *vma = pvmw->vma;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address = pvmw->address;\n\tunsigned long mmun_start = address & HPAGE_PMD_MASK;\n\tpmd_t pmde;\n\tswp_entry_t entry;\n\n\tif (!(pvmw->pmd && !pvmw->pte))\n\t\treturn;\n\n\tentry = pmd_to_swp_entry(*pvmw->pmd);\n\tget_page(new);\n\tpmde = pmd_mkold(mk_huge_pmd(new, vma->vm_page_prot));\n\tif (pmd_swp_soft_dirty(*pvmw->pmd))\n\t\tpmde = pmd_mksoft_dirty(pmde);\n\tif (is_write_migration_entry(entry))\n\t\tpmde = maybe_pmd_mkwrite(pmde, vma);\n\n\tflush_cache_range(vma, mmun_start, mmun_start + HPAGE_PMD_SIZE);\n\tif (PageAnon(new))\n\t\tpage_add_anon_rmap(new, vma, mmun_start, true);\n\telse\n\t\tpage_add_file_rmap(new, true);\n\tset_pmd_at(mm, mmun_start, pvmw->pmd, pmde);\n\tif ((vma->vm_flags & VM_LOCKED) && !PageDoubleMap(new))\n\t\tmlock_vma_page(new);\n\tupdate_mmu_cache_pmd(vma, address, pvmw->pmd);\n}",
        "code_after_change": "void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n{\n\tstruct vm_area_struct *vma = pvmw->vma;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address = pvmw->address;\n\tunsigned long mmun_start = address & HPAGE_PMD_MASK;\n\tpmd_t pmde;\n\tswp_entry_t entry;\n\n\tif (!(pvmw->pmd && !pvmw->pte))\n\t\treturn;\n\n\tentry = pmd_to_swp_entry(*pvmw->pmd);\n\tget_page(new);\n\tpmde = pmd_mkold(mk_huge_pmd(new, vma->vm_page_prot));\n\tif (pmd_swp_soft_dirty(*pvmw->pmd))\n\t\tpmde = pmd_mksoft_dirty(pmde);\n\tif (is_write_migration_entry(entry))\n\t\tpmde = maybe_pmd_mkwrite(pmde, vma);\n\tif (pmd_swp_uffd_wp(*pvmw->pmd))\n\t\tpmde = pmd_wrprotect(pmd_mkuffd_wp(pmde));\n\n\tflush_cache_range(vma, mmun_start, mmun_start + HPAGE_PMD_SIZE);\n\tif (PageAnon(new))\n\t\tpage_add_anon_rmap(new, vma, mmun_start, true);\n\telse\n\t\tpage_add_file_rmap(new, true);\n\tset_pmd_at(mm, mmun_start, pvmw->pmd, pmde);\n\tif ((vma->vm_flags & VM_LOCKED) && !PageDoubleMap(new))\n\t\tmlock_vma_page(new);\n\tupdate_mmu_cache_pmd(vma, address, pvmw->pmd);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,8 @@\n \t\tpmde = pmd_mksoft_dirty(pmde);\n \tif (is_write_migration_entry(entry))\n \t\tpmde = maybe_pmd_mkwrite(pmde, vma);\n+\tif (pmd_swp_uffd_wp(*pvmw->pmd))\n+\t\tpmde = pmd_wrprotect(pmd_mkuffd_wp(pmde));\n \n \tflush_cache_range(vma, mmun_start, mmun_start + HPAGE_PMD_SIZE);\n \tif (PageAnon(new))",
        "function_modified_lines": {
            "added": [
                "\tif (pmd_swp_uffd_wp(*pvmw->pmd))",
                "\t\tpmde = pmd_wrprotect(pmd_mkuffd_wp(pmde));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in pfn_swap_entry_to_page in memory management subsystem in the Linux Kernel. In this flaw, an attacker with a local user privilege may cause a denial of service problem due to a BUG statement referencing pmd_t x.",
        "id": 4244
    },
    {
        "cve_id": "CVE-2023-4732",
        "code_before_change": "int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long addr, pgprot_t newprot, unsigned long cp_flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tspinlock_t *ptl;\n\tpmd_t entry;\n\tbool preserve_write;\n\tint ret;\n\tbool prot_numa = cp_flags & MM_CP_PROT_NUMA;\n\tbool uffd_wp = cp_flags & MM_CP_UFFD_WP;\n\tbool uffd_wp_resolve = cp_flags & MM_CP_UFFD_WP_RESOLVE;\n\n\tptl = __pmd_trans_huge_lock(pmd, vma);\n\tif (!ptl)\n\t\treturn 0;\n\n\tpreserve_write = prot_numa && pmd_write(*pmd);\n\tret = 1;\n\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\tif (is_swap_pmd(*pmd)) {\n\t\tswp_entry_t entry = pmd_to_swp_entry(*pmd);\n\n\t\tVM_BUG_ON(!is_pmd_migration_entry(*pmd));\n\t\tif (is_write_migration_entry(entry)) {\n\t\t\tpmd_t newpmd;\n\t\t\t/*\n\t\t\t * A protection check is difficult so\n\t\t\t * just be safe and disable write\n\t\t\t */\n\t\t\tmake_migration_entry_read(&entry);\n\t\t\tnewpmd = swp_entry_to_pmd(entry);\n\t\t\tif (pmd_swp_soft_dirty(*pmd))\n\t\t\t\tnewpmd = pmd_swp_mksoft_dirty(newpmd);\n\t\t\tset_pmd_at(mm, addr, pmd, newpmd);\n\t\t}\n\t\tgoto unlock;\n\t}\n#endif\n\n\t/*\n\t * Avoid trapping faults against the zero page. The read-only\n\t * data is likely to be read-cached on the local CPU and\n\t * local/remote hits to the zero page are not interesting.\n\t */\n\tif (prot_numa && is_huge_zero_pmd(*pmd))\n\t\tgoto unlock;\n\n\tif (prot_numa && pmd_protnone(*pmd))\n\t\tgoto unlock;\n\n\t/*\n\t * In case prot_numa, we are under mmap_read_lock(mm). It's critical\n\t * to not clear pmd intermittently to avoid race with MADV_DONTNEED\n\t * which is also under mmap_read_lock(mm):\n\t *\n\t *\tCPU0:\t\t\t\tCPU1:\n\t *\t\t\t\tchange_huge_pmd(prot_numa=1)\n\t *\t\t\t\t pmdp_huge_get_and_clear_notify()\n\t * madvise_dontneed()\n\t *  zap_pmd_range()\n\t *   pmd_trans_huge(*pmd) == 0 (without ptl)\n\t *   // skip the pmd\n\t *\t\t\t\t set_pmd_at();\n\t *\t\t\t\t // pmd is re-established\n\t *\n\t * The race makes MADV_DONTNEED miss the huge pmd and don't clear it\n\t * which may break userspace.\n\t *\n\t * pmdp_invalidate() is required to make sure we don't miss\n\t * dirty/young flags set by hardware.\n\t */\n\tentry = pmdp_invalidate(vma, addr, pmd);\n\n\tentry = pmd_modify(entry, newprot);\n\tif (preserve_write)\n\t\tentry = pmd_mk_savedwrite(entry);\n\tif (uffd_wp) {\n\t\tentry = pmd_wrprotect(entry);\n\t\tentry = pmd_mkuffd_wp(entry);\n\t} else if (uffd_wp_resolve) {\n\t\t/*\n\t\t * Leave the write bit to be handled by PF interrupt\n\t\t * handler, then things like COW could be properly\n\t\t * handled.\n\t\t */\n\t\tentry = pmd_clear_uffd_wp(entry);\n\t}\n\tret = HPAGE_PMD_NR;\n\tset_pmd_at(mm, addr, pmd, entry);\n\tBUG_ON(vma_is_anonymous(vma) && !preserve_write && pmd_write(entry));\nunlock:\n\tspin_unlock(ptl);\n\treturn ret;\n}",
        "code_after_change": "int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long addr, pgprot_t newprot, unsigned long cp_flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tspinlock_t *ptl;\n\tpmd_t entry;\n\tbool preserve_write;\n\tint ret;\n\tbool prot_numa = cp_flags & MM_CP_PROT_NUMA;\n\tbool uffd_wp = cp_flags & MM_CP_UFFD_WP;\n\tbool uffd_wp_resolve = cp_flags & MM_CP_UFFD_WP_RESOLVE;\n\n\tptl = __pmd_trans_huge_lock(pmd, vma);\n\tif (!ptl)\n\t\treturn 0;\n\n\tpreserve_write = prot_numa && pmd_write(*pmd);\n\tret = 1;\n\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\tif (is_swap_pmd(*pmd)) {\n\t\tswp_entry_t entry = pmd_to_swp_entry(*pmd);\n\n\t\tVM_BUG_ON(!is_pmd_migration_entry(*pmd));\n\t\tif (is_write_migration_entry(entry)) {\n\t\t\tpmd_t newpmd;\n\t\t\t/*\n\t\t\t * A protection check is difficult so\n\t\t\t * just be safe and disable write\n\t\t\t */\n\t\t\tmake_migration_entry_read(&entry);\n\t\t\tnewpmd = swp_entry_to_pmd(entry);\n\t\t\tif (pmd_swp_soft_dirty(*pmd))\n\t\t\t\tnewpmd = pmd_swp_mksoft_dirty(newpmd);\n\t\t\tif (pmd_swp_uffd_wp(*pmd))\n\t\t\t\tnewpmd = pmd_swp_mkuffd_wp(newpmd);\n\t\t\tset_pmd_at(mm, addr, pmd, newpmd);\n\t\t}\n\t\tgoto unlock;\n\t}\n#endif\n\n\t/*\n\t * Avoid trapping faults against the zero page. The read-only\n\t * data is likely to be read-cached on the local CPU and\n\t * local/remote hits to the zero page are not interesting.\n\t */\n\tif (prot_numa && is_huge_zero_pmd(*pmd))\n\t\tgoto unlock;\n\n\tif (prot_numa && pmd_protnone(*pmd))\n\t\tgoto unlock;\n\n\t/*\n\t * In case prot_numa, we are under mmap_read_lock(mm). It's critical\n\t * to not clear pmd intermittently to avoid race with MADV_DONTNEED\n\t * which is also under mmap_read_lock(mm):\n\t *\n\t *\tCPU0:\t\t\t\tCPU1:\n\t *\t\t\t\tchange_huge_pmd(prot_numa=1)\n\t *\t\t\t\t pmdp_huge_get_and_clear_notify()\n\t * madvise_dontneed()\n\t *  zap_pmd_range()\n\t *   pmd_trans_huge(*pmd) == 0 (without ptl)\n\t *   // skip the pmd\n\t *\t\t\t\t set_pmd_at();\n\t *\t\t\t\t // pmd is re-established\n\t *\n\t * The race makes MADV_DONTNEED miss the huge pmd and don't clear it\n\t * which may break userspace.\n\t *\n\t * pmdp_invalidate() is required to make sure we don't miss\n\t * dirty/young flags set by hardware.\n\t */\n\tentry = pmdp_invalidate(vma, addr, pmd);\n\n\tentry = pmd_modify(entry, newprot);\n\tif (preserve_write)\n\t\tentry = pmd_mk_savedwrite(entry);\n\tif (uffd_wp) {\n\t\tentry = pmd_wrprotect(entry);\n\t\tentry = pmd_mkuffd_wp(entry);\n\t} else if (uffd_wp_resolve) {\n\t\t/*\n\t\t * Leave the write bit to be handled by PF interrupt\n\t\t * handler, then things like COW could be properly\n\t\t * handled.\n\t\t */\n\t\tentry = pmd_clear_uffd_wp(entry);\n\t}\n\tret = HPAGE_PMD_NR;\n\tset_pmd_at(mm, addr, pmd, entry);\n\tBUG_ON(vma_is_anonymous(vma) && !preserve_write && pmd_write(entry));\nunlock:\n\tspin_unlock(ptl);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -32,6 +32,8 @@\n \t\t\tnewpmd = swp_entry_to_pmd(entry);\n \t\t\tif (pmd_swp_soft_dirty(*pmd))\n \t\t\t\tnewpmd = pmd_swp_mksoft_dirty(newpmd);\n+\t\t\tif (pmd_swp_uffd_wp(*pmd))\n+\t\t\t\tnewpmd = pmd_swp_mkuffd_wp(newpmd);\n \t\t\tset_pmd_at(mm, addr, pmd, newpmd);\n \t\t}\n \t\tgoto unlock;",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (pmd_swp_uffd_wp(*pmd))",
                "\t\t\t\tnewpmd = pmd_swp_mkuffd_wp(newpmd);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A flaw was found in pfn_swap_entry_to_page in memory management subsystem in the Linux Kernel. In this flaw, an attacker with a local user privilege may cause a denial of service problem due to a BUG statement referencing pmd_t x.",
        "id": 4243
    },
    {
        "cve_id": "CVE-2019-13233",
        "code_before_change": "unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct *desc;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn -1L;\n\n\tif (v8086_mode(regs))\n\t\t/*\n\t\t * Base is simply the segment selector shifted 4\n\t\t * bits to the right.\n\t\t */\n\t\treturn (unsigned long)(sel << 4);\n\n\tif (user_64bit_mode(regs)) {\n\t\t/*\n\t\t * Only FS or GS will have a base address, the rest of\n\t\t * the segments' bases are forced to 0.\n\t\t */\n\t\tunsigned long base;\n\n\t\tif (seg_reg_idx == INAT_SEG_REG_FS)\n\t\t\trdmsrl(MSR_FS_BASE, base);\n\t\telse if (seg_reg_idx == INAT_SEG_REG_GS)\n\t\t\t/*\n\t\t\t * swapgs was called at the kernel entry point. Thus,\n\t\t\t * MSR_KERNEL_GS_BASE will have the user-space GS base.\n\t\t\t */\n\t\t\trdmsrl(MSR_KERNEL_GS_BASE, base);\n\t\telse\n\t\t\tbase = 0;\n\t\treturn base;\n\t}\n\n\t/* In protected mode the segment selector cannot be null. */\n\tif (!sel)\n\t\treturn -1L;\n\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn -1L;\n\n\treturn get_desc_base(desc);\n}",
        "code_after_change": "unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct desc;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn -1L;\n\n\tif (v8086_mode(regs))\n\t\t/*\n\t\t * Base is simply the segment selector shifted 4\n\t\t * bits to the right.\n\t\t */\n\t\treturn (unsigned long)(sel << 4);\n\n\tif (user_64bit_mode(regs)) {\n\t\t/*\n\t\t * Only FS or GS will have a base address, the rest of\n\t\t * the segments' bases are forced to 0.\n\t\t */\n\t\tunsigned long base;\n\n\t\tif (seg_reg_idx == INAT_SEG_REG_FS)\n\t\t\trdmsrl(MSR_FS_BASE, base);\n\t\telse if (seg_reg_idx == INAT_SEG_REG_GS)\n\t\t\t/*\n\t\t\t * swapgs was called at the kernel entry point. Thus,\n\t\t\t * MSR_KERNEL_GS_BASE will have the user-space GS base.\n\t\t\t */\n\t\t\trdmsrl(MSR_KERNEL_GS_BASE, base);\n\t\telse\n\t\t\tbase = 0;\n\t\treturn base;\n\t}\n\n\t/* In protected mode the segment selector cannot be null. */\n\tif (!sel)\n\t\treturn -1L;\n\n\tif (!get_desc(&desc, sel))\n\t\treturn -1L;\n\n\treturn get_desc_base(&desc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)\n {\n-\tstruct desc_struct *desc;\n+\tstruct desc_struct desc;\n \tshort sel;\n \n \tsel = get_segment_selector(regs, seg_reg_idx);\n@@ -38,9 +38,8 @@\n \tif (!sel)\n \t\treturn -1L;\n \n-\tdesc = get_desc(sel);\n-\tif (!desc)\n+\tif (!get_desc(&desc, sel))\n \t\treturn -1L;\n \n-\treturn get_desc_base(desc);\n+\treturn get_desc_base(&desc);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct desc_struct desc;",
                "\tif (!get_desc(&desc, sel))",
                "\treturn get_desc_base(&desc);"
            ],
            "deleted": [
                "\tstruct desc_struct *desc;",
                "\tdesc = get_desc(sel);",
                "\tif (!desc)",
                "\treturn get_desc_base(desc);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In arch/x86/lib/insn-eval.c in the Linux kernel before 5.1.9, there is a use-after-free for access to an LDT entry because of a race condition between modify_ldt() and a #BR exception for an MPX bounds violation.",
        "id": 1959
    },
    {
        "cve_id": "CVE-2019-13233",
        "code_before_change": "static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct *desc;\n\tunsigned long limit;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn 0;\n\n\tif (user_64bit_mode(regs) || v8086_mode(regs))\n\t\treturn -1L;\n\n\tif (!sel)\n\t\treturn 0;\n\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn 0;\n\n\t/*\n\t * If the granularity bit is set, the limit is given in multiples\n\t * of 4096. This also means that the 12 least significant bits are\n\t * not tested when checking the segment limits. In practice,\n\t * this means that the segment ends in (limit << 12) + 0xfff.\n\t */\n\tlimit = get_desc_limit(desc);\n\tif (desc->g)\n\t\tlimit = (limit << 12) + 0xfff;\n\n\treturn limit;\n}",
        "code_after_change": "static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)\n{\n\tstruct desc_struct desc;\n\tunsigned long limit;\n\tshort sel;\n\n\tsel = get_segment_selector(regs, seg_reg_idx);\n\tif (sel < 0)\n\t\treturn 0;\n\n\tif (user_64bit_mode(regs) || v8086_mode(regs))\n\t\treturn -1L;\n\n\tif (!sel)\n\t\treturn 0;\n\n\tif (!get_desc(&desc, sel))\n\t\treturn 0;\n\n\t/*\n\t * If the granularity bit is set, the limit is given in multiples\n\t * of 4096. This also means that the 12 least significant bits are\n\t * not tested when checking the segment limits. In practice,\n\t * this means that the segment ends in (limit << 12) + 0xfff.\n\t */\n\tlimit = get_desc_limit(&desc);\n\tif (desc.g)\n\t\tlimit = (limit << 12) + 0xfff;\n\n\treturn limit;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)\n {\n-\tstruct desc_struct *desc;\n+\tstruct desc_struct desc;\n \tunsigned long limit;\n \tshort sel;\n \n@@ -14,8 +14,7 @@\n \tif (!sel)\n \t\treturn 0;\n \n-\tdesc = get_desc(sel);\n-\tif (!desc)\n+\tif (!get_desc(&desc, sel))\n \t\treturn 0;\n \n \t/*\n@@ -24,8 +23,8 @@\n \t * not tested when checking the segment limits. In practice,\n \t * this means that the segment ends in (limit << 12) + 0xfff.\n \t */\n-\tlimit = get_desc_limit(desc);\n-\tif (desc->g)\n+\tlimit = get_desc_limit(&desc);\n+\tif (desc.g)\n \t\tlimit = (limit << 12) + 0xfff;\n \n \treturn limit;",
        "function_modified_lines": {
            "added": [
                "\tstruct desc_struct desc;",
                "\tif (!get_desc(&desc, sel))",
                "\tlimit = get_desc_limit(&desc);",
                "\tif (desc.g)"
            ],
            "deleted": [
                "\tstruct desc_struct *desc;",
                "\tdesc = get_desc(sel);",
                "\tif (!desc)",
                "\tlimit = get_desc_limit(desc);",
                "\tif (desc->g)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In arch/x86/lib/insn-eval.c in the Linux kernel before 5.1.9, there is a use-after-free for access to an LDT entry because of a race condition between modify_ldt() and a #BR exception for an MPX bounds violation.",
        "id": 1957
    },
    {
        "cve_id": "CVE-2020-27067",
        "code_before_change": "static void l2tp_eth_dev_uninit(struct net_device *dev)\n{\n\tstruct l2tp_eth *priv = netdev_priv(dev);\n\tstruct l2tp_eth_net *pn = l2tp_eth_pernet(dev_net(dev));\n\n\tspin_lock(&pn->l2tp_eth_lock);\n\tlist_del_init(&priv->list);\n\tspin_unlock(&pn->l2tp_eth_lock);\n\tdev_put(dev);\n}",
        "code_after_change": "static void l2tp_eth_dev_uninit(struct net_device *dev)\n{\n\tdev_put(dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,4 @@\n static void l2tp_eth_dev_uninit(struct net_device *dev)\n {\n-\tstruct l2tp_eth *priv = netdev_priv(dev);\n-\tstruct l2tp_eth_net *pn = l2tp_eth_pernet(dev_net(dev));\n-\n-\tspin_lock(&pn->l2tp_eth_lock);\n-\tlist_del_init(&priv->list);\n-\tspin_unlock(&pn->l2tp_eth_lock);\n \tdev_put(dev);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct l2tp_eth *priv = netdev_priv(dev);",
                "\tstruct l2tp_eth_net *pn = l2tp_eth_pernet(dev_net(dev));",
                "",
                "\tspin_lock(&pn->l2tp_eth_lock);",
                "\tlist_del_init(&priv->list);",
                "\tspin_unlock(&pn->l2tp_eth_lock);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the l2tp subsystem, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152409173",
        "id": 2611
    },
    {
        "cve_id": "CVE-2020-27067",
        "code_before_change": "static int l2tp_eth_create(struct net *net, struct l2tp_tunnel *tunnel,\n\t\t\t   u32 session_id, u32 peer_session_id,\n\t\t\t   struct l2tp_session_cfg *cfg)\n{\n\tunsigned char name_assign_type;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct l2tp_session *session;\n\tstruct l2tp_eth *priv;\n\tstruct l2tp_eth_sess *spriv;\n\tint rc;\n\tstruct l2tp_eth_net *pn;\n\n\tif (cfg->ifname) {\n\t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n\t\tname_assign_type = NET_NAME_USER;\n\t} else {\n\t\tstrcpy(name, L2TP_ETH_DEV_NAME);\n\t\tname_assign_type = NET_NAME_ENUM;\n\t}\n\n\tsession = l2tp_session_create(sizeof(*spriv), tunnel, session_id,\n\t\t\t\t      peer_session_id, cfg);\n\tif (IS_ERR(session)) {\n\t\trc = PTR_ERR(session);\n\t\tgoto out;\n\t}\n\n\tdev = alloc_netdev(sizeof(*priv), name, name_assign_type,\n\t\t\t   l2tp_eth_dev_setup);\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto out_del_session;\n\t}\n\n\tdev_net_set(dev, net);\n\tdev->min_mtu = 0;\n\tdev->max_mtu = ETH_MAX_MTU;\n\tl2tp_eth_adjust_mtu(tunnel, session, dev);\n\n\tpriv = netdev_priv(dev);\n\tpriv->dev = dev;\n\tpriv->session = session;\n\tINIT_LIST_HEAD(&priv->list);\n\n\tpriv->tunnel_sock = tunnel->sock;\n\tsession->recv_skb = l2tp_eth_dev_recv;\n\tsession->session_close = l2tp_eth_delete;\n#if IS_ENABLED(CONFIG_L2TP_DEBUGFS)\n\tsession->show = l2tp_eth_show;\n#endif\n\n\tspriv = l2tp_session_priv(session);\n\tspriv->dev = dev;\n\n\trc = register_netdev(dev);\n\tif (rc < 0)\n\t\tgoto out_del_dev;\n\n\t__module_get(THIS_MODULE);\n\t/* Must be done after register_netdev() */\n\tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n\n\tdev_hold(dev);\n\tpn = l2tp_eth_pernet(dev_net(dev));\n\tspin_lock(&pn->l2tp_eth_lock);\n\tlist_add(&priv->list, &pn->l2tp_eth_dev_list);\n\tspin_unlock(&pn->l2tp_eth_lock);\n\n\treturn 0;\n\nout_del_dev:\n\tfree_netdev(dev);\n\tspriv->dev = NULL;\nout_del_session:\n\tl2tp_session_delete(session);\nout:\n\treturn rc;\n}",
        "code_after_change": "static int l2tp_eth_create(struct net *net, struct l2tp_tunnel *tunnel,\n\t\t\t   u32 session_id, u32 peer_session_id,\n\t\t\t   struct l2tp_session_cfg *cfg)\n{\n\tunsigned char name_assign_type;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct l2tp_session *session;\n\tstruct l2tp_eth *priv;\n\tstruct l2tp_eth_sess *spriv;\n\tint rc;\n\n\tif (cfg->ifname) {\n\t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n\t\tname_assign_type = NET_NAME_USER;\n\t} else {\n\t\tstrcpy(name, L2TP_ETH_DEV_NAME);\n\t\tname_assign_type = NET_NAME_ENUM;\n\t}\n\n\tsession = l2tp_session_create(sizeof(*spriv), tunnel, session_id,\n\t\t\t\t      peer_session_id, cfg);\n\tif (IS_ERR(session)) {\n\t\trc = PTR_ERR(session);\n\t\tgoto out;\n\t}\n\n\tdev = alloc_netdev(sizeof(*priv), name, name_assign_type,\n\t\t\t   l2tp_eth_dev_setup);\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto out_del_session;\n\t}\n\n\tdev_net_set(dev, net);\n\tdev->min_mtu = 0;\n\tdev->max_mtu = ETH_MAX_MTU;\n\tl2tp_eth_adjust_mtu(tunnel, session, dev);\n\n\tpriv = netdev_priv(dev);\n\tpriv->dev = dev;\n\tpriv->session = session;\n\n\tpriv->tunnel_sock = tunnel->sock;\n\tsession->recv_skb = l2tp_eth_dev_recv;\n\tsession->session_close = l2tp_eth_delete;\n#if IS_ENABLED(CONFIG_L2TP_DEBUGFS)\n\tsession->show = l2tp_eth_show;\n#endif\n\n\tspriv = l2tp_session_priv(session);\n\tspriv->dev = dev;\n\n\trc = register_netdev(dev);\n\tif (rc < 0)\n\t\tgoto out_del_dev;\n\n\t__module_get(THIS_MODULE);\n\t/* Must be done after register_netdev() */\n\tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n\n\tdev_hold(dev);\n\n\treturn 0;\n\nout_del_dev:\n\tfree_netdev(dev);\n\tspriv->dev = NULL;\nout_del_session:\n\tl2tp_session_delete(session);\nout:\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,6 @@\n \tstruct l2tp_eth *priv;\n \tstruct l2tp_eth_sess *spriv;\n \tint rc;\n-\tstruct l2tp_eth_net *pn;\n \n \tif (cfg->ifname) {\n \t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n@@ -41,7 +40,6 @@\n \tpriv = netdev_priv(dev);\n \tpriv->dev = dev;\n \tpriv->session = session;\n-\tINIT_LIST_HEAD(&priv->list);\n \n \tpriv->tunnel_sock = tunnel->sock;\n \tsession->recv_skb = l2tp_eth_dev_recv;\n@@ -62,10 +60,6 @@\n \tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n \n \tdev_hold(dev);\n-\tpn = l2tp_eth_pernet(dev_net(dev));\n-\tspin_lock(&pn->l2tp_eth_lock);\n-\tlist_add(&priv->list, &pn->l2tp_eth_dev_list);\n-\tspin_unlock(&pn->l2tp_eth_lock);\n \n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct l2tp_eth_net *pn;",
                "\tINIT_LIST_HEAD(&priv->list);",
                "\tpn = l2tp_eth_pernet(dev_net(dev));",
                "\tspin_lock(&pn->l2tp_eth_lock);",
                "\tlist_add(&priv->list, &pn->l2tp_eth_dev_list);",
                "\tspin_unlock(&pn->l2tp_eth_lock);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the l2tp subsystem, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152409173",
        "id": 2612
    },
    {
        "cve_id": "CVE-2014-0100",
        "code_before_change": "static struct inet_frag_queue *inet_frag_intern(struct netns_frags *nf,\n\t\tstruct inet_frag_queue *qp_in, struct inet_frags *f,\n\t\tvoid *arg)\n{\n\tstruct inet_frag_bucket *hb;\n\tstruct inet_frag_queue *qp;\n\tunsigned int hash;\n\n\tread_lock(&f->lock); /* Protects against hash rebuild */\n\t/*\n\t * While we stayed w/o the lock other CPU could update\n\t * the rnd seed, so we need to re-calculate the hash\n\t * chain. Fortunatelly the qp_in can be used to get one.\n\t */\n\thash = f->hashfn(qp_in);\n\thb = &f->hash[hash];\n\tspin_lock(&hb->chain_lock);\n\n#ifdef CONFIG_SMP\n\t/* With SMP race we have to recheck hash table, because\n\t * such entry could be created on other cpu, while we\n\t * released the hash bucket lock.\n\t */\n\thlist_for_each_entry(qp, &hb->chain, list) {\n\t\tif (qp->net == nf && f->match(qp, arg)) {\n\t\t\tatomic_inc(&qp->refcnt);\n\t\t\tspin_unlock(&hb->chain_lock);\n\t\t\tread_unlock(&f->lock);\n\t\t\tqp_in->last_in |= INET_FRAG_COMPLETE;\n\t\t\tinet_frag_put(qp_in, f);\n\t\t\treturn qp;\n\t\t}\n\t}\n#endif\n\tqp = qp_in;\n\tif (!mod_timer(&qp->timer, jiffies + nf->timeout))\n\t\tatomic_inc(&qp->refcnt);\n\n\tatomic_inc(&qp->refcnt);\n\thlist_add_head(&qp->list, &hb->chain);\n\tspin_unlock(&hb->chain_lock);\n\tread_unlock(&f->lock);\n\tinet_frag_lru_add(nf, qp);\n\treturn qp;\n}",
        "code_after_change": "static struct inet_frag_queue *inet_frag_intern(struct netns_frags *nf,\n\t\tstruct inet_frag_queue *qp_in, struct inet_frags *f,\n\t\tvoid *arg)\n{\n\tstruct inet_frag_bucket *hb;\n\tstruct inet_frag_queue *qp;\n\tunsigned int hash;\n\n\tread_lock(&f->lock); /* Protects against hash rebuild */\n\t/*\n\t * While we stayed w/o the lock other CPU could update\n\t * the rnd seed, so we need to re-calculate the hash\n\t * chain. Fortunatelly the qp_in can be used to get one.\n\t */\n\thash = f->hashfn(qp_in);\n\thb = &f->hash[hash];\n\tspin_lock(&hb->chain_lock);\n\n#ifdef CONFIG_SMP\n\t/* With SMP race we have to recheck hash table, because\n\t * such entry could be created on other cpu, while we\n\t * released the hash bucket lock.\n\t */\n\thlist_for_each_entry(qp, &hb->chain, list) {\n\t\tif (qp->net == nf && f->match(qp, arg)) {\n\t\t\tatomic_inc(&qp->refcnt);\n\t\t\tspin_unlock(&hb->chain_lock);\n\t\t\tread_unlock(&f->lock);\n\t\t\tqp_in->last_in |= INET_FRAG_COMPLETE;\n\t\t\tinet_frag_put(qp_in, f);\n\t\t\treturn qp;\n\t\t}\n\t}\n#endif\n\tqp = qp_in;\n\tif (!mod_timer(&qp->timer, jiffies + nf->timeout))\n\t\tatomic_inc(&qp->refcnt);\n\n\tatomic_inc(&qp->refcnt);\n\thlist_add_head(&qp->list, &hb->chain);\n\tinet_frag_lru_add(nf, qp);\n\tspin_unlock(&hb->chain_lock);\n\tread_unlock(&f->lock);\n\n\treturn qp;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,8 +38,9 @@\n \n \tatomic_inc(&qp->refcnt);\n \thlist_add_head(&qp->list, &hb->chain);\n+\tinet_frag_lru_add(nf, qp);\n \tspin_unlock(&hb->chain_lock);\n \tread_unlock(&f->lock);\n-\tinet_frag_lru_add(nf, qp);\n+\n \treturn qp;\n }",
        "function_modified_lines": {
            "added": [
                "\tinet_frag_lru_add(nf, qp);",
                ""
            ],
            "deleted": [
                "\tinet_frag_lru_add(nf, qp);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the inet_frag_intern function in net/ipv4/inet_fragment.c in the Linux kernel through 3.13.6 allows remote attackers to cause a denial of service (use-after-free error) or possibly have unspecified other impact via a large series of fragmented ICMP Echo Request packets to a system with a heavy CPU load.",
        "id": 428
    },
    {
        "cve_id": "CVE-2015-7613",
        "code_before_change": "static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tkey_t key = params->key;\n\tint shmflg = params->flg;\n\tsize_t size = params->u.size;\n\tint error;\n\tstruct shmid_kernel *shp;\n\tsize_t numpages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstruct file *file;\n\tchar name[13];\n\tint id;\n\tvm_flags_t acctflag = 0;\n\n\tif (size < SHMMIN || size > ns->shm_ctlmax)\n\t\treturn -EINVAL;\n\n\tif (numpages << PAGE_SHIFT < size)\n\t\treturn -ENOSPC;\n\n\tif (ns->shm_tot + numpages < ns->shm_tot ||\n\t\t\tns->shm_tot + numpages > ns->shm_ctlall)\n\t\treturn -ENOSPC;\n\n\tshp = ipc_rcu_alloc(sizeof(*shp));\n\tif (!shp)\n\t\treturn -ENOMEM;\n\n\tshp->shm_perm.key = key;\n\tshp->shm_perm.mode = (shmflg & S_IRWXUGO);\n\tshp->mlock_user = NULL;\n\n\tshp->shm_perm.security = NULL;\n\terror = security_shm_alloc(shp);\n\tif (error) {\n\t\tipc_rcu_putref(shp, ipc_rcu_free);\n\t\treturn error;\n\t}\n\n\tsprintf(name, \"SYSV%08x\", key);\n\tif (shmflg & SHM_HUGETLB) {\n\t\tstruct hstate *hs;\n\t\tsize_t hugesize;\n\n\t\ths = hstate_sizelog((shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t\tif (!hs) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto no_file;\n\t\t}\n\t\thugesize = ALIGN(size, huge_page_size(hs));\n\n\t\t/* hugetlb_file_setup applies strict accounting */\n\t\tif (shmflg & SHM_NORESERVE)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = hugetlb_file_setup(name, hugesize, acctflag,\n\t\t\t\t  &shp->mlock_user, HUGETLB_SHMFS_INODE,\n\t\t\t\t(shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t} else {\n\t\t/*\n\t\t * Do not allow no accounting for OVERCOMMIT_NEVER, even\n\t\t * if it's asked for.\n\t\t */\n\t\tif  ((shmflg & SHM_NORESERVE) &&\n\t\t\t\tsysctl_overcommit_memory != OVERCOMMIT_NEVER)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = shmem_kernel_file_setup(name, size, acctflag);\n\t}\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto no_file;\n\n\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n\tif (id < 0) {\n\t\terror = id;\n\t\tgoto no_id;\n\t}\n\n\tshp->shm_cprid = task_tgid_vnr(current);\n\tshp->shm_lprid = 0;\n\tshp->shm_atim = shp->shm_dtim = 0;\n\tshp->shm_ctim = get_seconds();\n\tshp->shm_segsz = size;\n\tshp->shm_nattch = 0;\n\tshp->shm_file = file;\n\tshp->shm_creator = current;\n\tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n\n\t/*\n\t * shmid gets reported as \"inode#\" in /proc/pid/maps.\n\t * proc-ps tools use this. Changing this will break them.\n\t */\n\tfile_inode(file)->i_ino = shp->shm_perm.id;\n\n\tns->shm_tot += numpages;\n\terror = shp->shm_perm.id;\n\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\treturn error;\n\nno_id:\n\tif (is_file_hugepages(file) && shp->mlock_user)\n\t\tuser_shm_unlock(size, shp->mlock_user);\n\tfput(file);\nno_file:\n\tipc_rcu_putref(shp, shm_rcu_free);\n\treturn error;\n}",
        "code_after_change": "static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tkey_t key = params->key;\n\tint shmflg = params->flg;\n\tsize_t size = params->u.size;\n\tint error;\n\tstruct shmid_kernel *shp;\n\tsize_t numpages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstruct file *file;\n\tchar name[13];\n\tint id;\n\tvm_flags_t acctflag = 0;\n\n\tif (size < SHMMIN || size > ns->shm_ctlmax)\n\t\treturn -EINVAL;\n\n\tif (numpages << PAGE_SHIFT < size)\n\t\treturn -ENOSPC;\n\n\tif (ns->shm_tot + numpages < ns->shm_tot ||\n\t\t\tns->shm_tot + numpages > ns->shm_ctlall)\n\t\treturn -ENOSPC;\n\n\tshp = ipc_rcu_alloc(sizeof(*shp));\n\tif (!shp)\n\t\treturn -ENOMEM;\n\n\tshp->shm_perm.key = key;\n\tshp->shm_perm.mode = (shmflg & S_IRWXUGO);\n\tshp->mlock_user = NULL;\n\n\tshp->shm_perm.security = NULL;\n\terror = security_shm_alloc(shp);\n\tif (error) {\n\t\tipc_rcu_putref(shp, ipc_rcu_free);\n\t\treturn error;\n\t}\n\n\tsprintf(name, \"SYSV%08x\", key);\n\tif (shmflg & SHM_HUGETLB) {\n\t\tstruct hstate *hs;\n\t\tsize_t hugesize;\n\n\t\ths = hstate_sizelog((shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t\tif (!hs) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto no_file;\n\t\t}\n\t\thugesize = ALIGN(size, huge_page_size(hs));\n\n\t\t/* hugetlb_file_setup applies strict accounting */\n\t\tif (shmflg & SHM_NORESERVE)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = hugetlb_file_setup(name, hugesize, acctflag,\n\t\t\t\t  &shp->mlock_user, HUGETLB_SHMFS_INODE,\n\t\t\t\t(shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t} else {\n\t\t/*\n\t\t * Do not allow no accounting for OVERCOMMIT_NEVER, even\n\t\t * if it's asked for.\n\t\t */\n\t\tif  ((shmflg & SHM_NORESERVE) &&\n\t\t\t\tsysctl_overcommit_memory != OVERCOMMIT_NEVER)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = shmem_kernel_file_setup(name, size, acctflag);\n\t}\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto no_file;\n\n\tshp->shm_cprid = task_tgid_vnr(current);\n\tshp->shm_lprid = 0;\n\tshp->shm_atim = shp->shm_dtim = 0;\n\tshp->shm_ctim = get_seconds();\n\tshp->shm_segsz = size;\n\tshp->shm_nattch = 0;\n\tshp->shm_file = file;\n\tshp->shm_creator = current;\n\n\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n\tif (id < 0) {\n\t\terror = id;\n\t\tgoto no_id;\n\t}\n\n\tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n\n\t/*\n\t * shmid gets reported as \"inode#\" in /proc/pid/maps.\n\t * proc-ps tools use this. Changing this will break them.\n\t */\n\tfile_inode(file)->i_ino = shp->shm_perm.id;\n\n\tns->shm_tot += numpages;\n\terror = shp->shm_perm.id;\n\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\treturn error;\n\nno_id:\n\tif (is_file_hugepages(file) && shp->mlock_user)\n\t\tuser_shm_unlock(size, shp->mlock_user);\n\tfput(file);\nno_file:\n\tipc_rcu_putref(shp, shm_rcu_free);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -68,12 +68,6 @@\n \tif (IS_ERR(file))\n \t\tgoto no_file;\n \n-\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n-\tif (id < 0) {\n-\t\terror = id;\n-\t\tgoto no_id;\n-\t}\n-\n \tshp->shm_cprid = task_tgid_vnr(current);\n \tshp->shm_lprid = 0;\n \tshp->shm_atim = shp->shm_dtim = 0;\n@@ -82,6 +76,13 @@\n \tshp->shm_nattch = 0;\n \tshp->shm_file = file;\n \tshp->shm_creator = current;\n+\n+\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n+\tif (id < 0) {\n+\t\terror = id;\n+\t\tgoto no_id;\n+\t}\n+\n \tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "",
                "\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);",
                "\tif (id < 0) {",
                "\t\terror = id;",
                "\t\tgoto no_id;",
                "\t}",
                ""
            ],
            "deleted": [
                "\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);",
                "\tif (id < 0) {",
                "\t\terror = id;",
                "\t\tgoto no_id;",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the IPC object implementation in the Linux kernel through 4.2.3 allows local users to gain privileges by triggering an ipc_addid call that leads to uid and gid comparisons against uninitialized data, related to msg.c, shm.c, and util.c.",
        "id": 789
    },
    {
        "cve_id": "CVE-2018-1000004",
        "code_before_change": "static long snd_seq_ioctl(struct file *file, unsigned int cmd,\n\t\t\t  unsigned long arg)\n{\n\tstruct snd_seq_client *client = file->private_data;\n\t/* To use kernel stack for ioctl data. */\n\tunion {\n\t\tint pversion;\n\t\tint client_id;\n\t\tstruct snd_seq_system_info\tsystem_info;\n\t\tstruct snd_seq_running_info\trunning_info;\n\t\tstruct snd_seq_client_info\tclient_info;\n\t\tstruct snd_seq_port_info\tport_info;\n\t\tstruct snd_seq_port_subscribe\tport_subscribe;\n\t\tstruct snd_seq_queue_info\tqueue_info;\n\t\tstruct snd_seq_queue_status\tqueue_status;\n\t\tstruct snd_seq_queue_tempo\ttempo;\n\t\tstruct snd_seq_queue_timer\tqueue_timer;\n\t\tstruct snd_seq_queue_client\tqueue_client;\n\t\tstruct snd_seq_client_pool\tclient_pool;\n\t\tstruct snd_seq_remove_events\tremove_events;\n\t\tstruct snd_seq_query_subs\tquery_subs;\n\t} buf;\n\tconst struct ioctl_handler *handler;\n\tunsigned long size;\n\tint err;\n\n\tif (snd_BUG_ON(!client))\n\t\treturn -ENXIO;\n\n\tfor (handler = ioctl_handlers; handler->cmd > 0; ++handler) {\n\t\tif (handler->cmd == cmd)\n\t\t\tbreak;\n\t}\n\tif (handler->cmd == 0)\n\t\treturn -ENOTTY;\n\n\tmemset(&buf, 0, sizeof(buf));\n\n\t/*\n\t * All of ioctl commands for ALSA sequencer get an argument of size\n\t * within 13 bits. We can safely pick up the size from the command.\n\t */\n\tsize = _IOC_SIZE(handler->cmd);\n\tif (handler->cmd & IOC_IN) {\n\t\tif (copy_from_user(&buf, (const void __user *)arg, size))\n\t\t\treturn -EFAULT;\n\t}\n\n\terr = handler->func(client, &buf);\n\tif (err >= 0) {\n\t\t/* Some commands includes a bug in 'dir' field. */\n\t\tif (handler->cmd == SNDRV_SEQ_IOCTL_SET_QUEUE_CLIENT ||\n\t\t    handler->cmd == SNDRV_SEQ_IOCTL_SET_CLIENT_POOL ||\n\t\t    (handler->cmd & IOC_OUT))\n\t\t\tif (copy_to_user((void __user *)arg, &buf, size))\n\t\t\t\treturn -EFAULT;\n\t}\n\n\treturn err;\n}",
        "code_after_change": "static long snd_seq_ioctl(struct file *file, unsigned int cmd,\n\t\t\t  unsigned long arg)\n{\n\tstruct snd_seq_client *client = file->private_data;\n\t/* To use kernel stack for ioctl data. */\n\tunion {\n\t\tint pversion;\n\t\tint client_id;\n\t\tstruct snd_seq_system_info\tsystem_info;\n\t\tstruct snd_seq_running_info\trunning_info;\n\t\tstruct snd_seq_client_info\tclient_info;\n\t\tstruct snd_seq_port_info\tport_info;\n\t\tstruct snd_seq_port_subscribe\tport_subscribe;\n\t\tstruct snd_seq_queue_info\tqueue_info;\n\t\tstruct snd_seq_queue_status\tqueue_status;\n\t\tstruct snd_seq_queue_tempo\ttempo;\n\t\tstruct snd_seq_queue_timer\tqueue_timer;\n\t\tstruct snd_seq_queue_client\tqueue_client;\n\t\tstruct snd_seq_client_pool\tclient_pool;\n\t\tstruct snd_seq_remove_events\tremove_events;\n\t\tstruct snd_seq_query_subs\tquery_subs;\n\t} buf;\n\tconst struct ioctl_handler *handler;\n\tunsigned long size;\n\tint err;\n\n\tif (snd_BUG_ON(!client))\n\t\treturn -ENXIO;\n\n\tfor (handler = ioctl_handlers; handler->cmd > 0; ++handler) {\n\t\tif (handler->cmd == cmd)\n\t\t\tbreak;\n\t}\n\tif (handler->cmd == 0)\n\t\treturn -ENOTTY;\n\n\tmemset(&buf, 0, sizeof(buf));\n\n\t/*\n\t * All of ioctl commands for ALSA sequencer get an argument of size\n\t * within 13 bits. We can safely pick up the size from the command.\n\t */\n\tsize = _IOC_SIZE(handler->cmd);\n\tif (handler->cmd & IOC_IN) {\n\t\tif (copy_from_user(&buf, (const void __user *)arg, size))\n\t\t\treturn -EFAULT;\n\t}\n\n\tmutex_lock(&client->ioctl_mutex);\n\terr = handler->func(client, &buf);\n\tmutex_unlock(&client->ioctl_mutex);\n\tif (err >= 0) {\n\t\t/* Some commands includes a bug in 'dir' field. */\n\t\tif (handler->cmd == SNDRV_SEQ_IOCTL_SET_QUEUE_CLIENT ||\n\t\t    handler->cmd == SNDRV_SEQ_IOCTL_SET_CLIENT_POOL ||\n\t\t    (handler->cmd & IOC_OUT))\n\t\t\tif (copy_to_user((void __user *)arg, &buf, size))\n\t\t\t\treturn -EFAULT;\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -46,7 +46,9 @@\n \t\t\treturn -EFAULT;\n \t}\n \n+\tmutex_lock(&client->ioctl_mutex);\n \terr = handler->func(client, &buf);\n+\tmutex_unlock(&client->ioctl_mutex);\n \tif (err >= 0) {\n \t\t/* Some commands includes a bug in 'dir' field. */\n \t\tif (handler->cmd == SNDRV_SEQ_IOCTL_SET_QUEUE_CLIENT ||",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&client->ioctl_mutex);",
                "\tmutex_unlock(&client->ioctl_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "In the Linux kernel 4.12, 3.10, 2.6 and possibly earlier versions a race condition vulnerability exists in the sound system, this can lead to a deadlock and denial of service condition.",
        "id": 1573
    },
    {
        "cve_id": "CVE-2014-4652",
        "code_before_change": "static int snd_ctl_elem_user_put(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tint change;\n\tstruct user_element *ue = kcontrol->private_data;\n\n\tchange = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;\n\tif (change)\n\t\tmemcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);\n\treturn change;\n}",
        "code_after_change": "static int snd_ctl_elem_user_put(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tint change;\n\tstruct user_element *ue = kcontrol->private_data;\n\n\tmutex_lock(&ue->card->user_ctl_lock);\n\tchange = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;\n\tif (change)\n\t\tmemcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);\n\tmutex_unlock(&ue->card->user_ctl_lock);\n\treturn change;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,8 +4,10 @@\n \tint change;\n \tstruct user_element *ue = kcontrol->private_data;\n \n+\tmutex_lock(&ue->card->user_ctl_lock);\n \tchange = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;\n \tif (change)\n \t\tmemcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);\n+\tmutex_unlock(&ue->card->user_ctl_lock);\n \treturn change;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&ue->card->user_ctl_lock);",
                "\tmutex_unlock(&ue->card->user_ctl_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the tlv handler functionality in the snd_ctl_elem_user_tlv function in sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 allows local users to obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "id": 564
    },
    {
        "cve_id": "CVE-2017-15265",
        "code_before_change": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\tsprintf(new_port->name, \"port-%d\", num);\n\n\treturn new_port;\n}",
        "code_after_change": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\tsnd_use_lock_use(&new_port->use_lock);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\tsprintf(new_port->name, \"port-%d\", num);\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\n\treturn new_port;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,6 +26,7 @@\n \tsnd_use_lock_init(&new_port->use_lock);\n \tport_subs_info_init(&new_port->c_src);\n \tport_subs_info_init(&new_port->c_dest);\n+\tsnd_use_lock_use(&new_port->use_lock);\n \n \tnum = port >= 0 ? port : 0;\n \tmutex_lock(&client->ports_mutex);\n@@ -40,9 +41,9 @@\n \tlist_add_tail(&new_port->list, &p->list);\n \tclient->num_ports++;\n \tnew_port->addr.port = num;\t/* store the port number in the port */\n+\tsprintf(new_port->name, \"port-%d\", num);\n \twrite_unlock_irqrestore(&client->ports_lock, flags);\n \tmutex_unlock(&client->ports_mutex);\n-\tsprintf(new_port->name, \"port-%d\", num);\n \n \treturn new_port;\n }",
        "function_modified_lines": {
            "added": [
                "\tsnd_use_lock_use(&new_port->use_lock);",
                "\tsprintf(new_port->name, \"port-%d\", num);"
            ],
            "deleted": [
                "\tsprintf(new_port->name, \"port-%d\", num);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ALSA subsystem in the Linux kernel before 4.13.8 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via crafted /dev/snd/seq ioctl calls, related to sound/core/seq/seq_clientmgr.c and sound/core/seq/seq_ports.c.",
        "id": 1301
    },
    {
        "cve_id": "CVE-2021-3348",
        "code_before_change": "static int nbd_add_socket(struct nbd_device *nbd, unsigned long arg,\n\t\t\t  bool netlink)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct socket *sock;\n\tstruct nbd_sock **socks;\n\tstruct nbd_sock *nsock;\n\tint err;\n\n\tsock = nbd_get_socket(nbd, arg, &err);\n\tif (!sock)\n\t\treturn err;\n\n\tif (!netlink && !nbd->task_setup &&\n\t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n\t\tnbd->task_setup = current;\n\n\tif (!netlink &&\n\t    (nbd->task_setup != current ||\n\t     test_bit(NBD_RT_BOUND, &config->runtime_flags))) {\n\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\"Device being setup by another task\");\n\t\terr = -EBUSY;\n\t\tgoto put_socket;\n\t}\n\n\tnsock = kzalloc(sizeof(*nsock), GFP_KERNEL);\n\tif (!nsock) {\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tsocks = krealloc(config->socks, (config->num_connections + 1) *\n\t\t\t sizeof(struct nbd_sock *), GFP_KERNEL);\n\tif (!socks) {\n\t\tkfree(nsock);\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tconfig->socks = socks;\n\n\tnsock->fallback_index = -1;\n\tnsock->dead = false;\n\tmutex_init(&nsock->tx_lock);\n\tnsock->sock = sock;\n\tnsock->pending = NULL;\n\tnsock->sent = 0;\n\tnsock->cookie = 0;\n\tsocks[config->num_connections++] = nsock;\n\tatomic_inc(&config->live_connections);\n\n\treturn 0;\n\nput_socket:\n\tsockfd_put(sock);\n\treturn err;\n}",
        "code_after_change": "static int nbd_add_socket(struct nbd_device *nbd, unsigned long arg,\n\t\t\t  bool netlink)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct socket *sock;\n\tstruct nbd_sock **socks;\n\tstruct nbd_sock *nsock;\n\tint err;\n\n\tsock = nbd_get_socket(nbd, arg, &err);\n\tif (!sock)\n\t\treturn err;\n\n\t/*\n\t * We need to make sure we don't get any errant requests while we're\n\t * reallocating the ->socks array.\n\t */\n\tblk_mq_freeze_queue(nbd->disk->queue);\n\n\tif (!netlink && !nbd->task_setup &&\n\t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n\t\tnbd->task_setup = current;\n\n\tif (!netlink &&\n\t    (nbd->task_setup != current ||\n\t     test_bit(NBD_RT_BOUND, &config->runtime_flags))) {\n\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\"Device being setup by another task\");\n\t\terr = -EBUSY;\n\t\tgoto put_socket;\n\t}\n\n\tnsock = kzalloc(sizeof(*nsock), GFP_KERNEL);\n\tif (!nsock) {\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tsocks = krealloc(config->socks, (config->num_connections + 1) *\n\t\t\t sizeof(struct nbd_sock *), GFP_KERNEL);\n\tif (!socks) {\n\t\tkfree(nsock);\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tconfig->socks = socks;\n\n\tnsock->fallback_index = -1;\n\tnsock->dead = false;\n\tmutex_init(&nsock->tx_lock);\n\tnsock->sock = sock;\n\tnsock->pending = NULL;\n\tnsock->sent = 0;\n\tnsock->cookie = 0;\n\tsocks[config->num_connections++] = nsock;\n\tatomic_inc(&config->live_connections);\n\tblk_mq_unfreeze_queue(nbd->disk->queue);\n\n\treturn 0;\n\nput_socket:\n\tblk_mq_unfreeze_queue(nbd->disk->queue);\n\tsockfd_put(sock);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,12 @@\n \tsock = nbd_get_socket(nbd, arg, &err);\n \tif (!sock)\n \t\treturn err;\n+\n+\t/*\n+\t * We need to make sure we don't get any errant requests while we're\n+\t * reallocating the ->socks array.\n+\t */\n+\tblk_mq_freeze_queue(nbd->disk->queue);\n \n \tif (!netlink && !nbd->task_setup &&\n \t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n@@ -49,10 +55,12 @@\n \tnsock->cookie = 0;\n \tsocks[config->num_connections++] = nsock;\n \tatomic_inc(&config->live_connections);\n+\tblk_mq_unfreeze_queue(nbd->disk->queue);\n \n \treturn 0;\n \n put_socket:\n+\tblk_mq_unfreeze_queue(nbd->disk->queue);\n \tsockfd_put(sock);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * We need to make sure we don't get any errant requests while we're",
                "\t * reallocating the ->socks array.",
                "\t */",
                "\tblk_mq_freeze_queue(nbd->disk->queue);",
                "\tblk_mq_unfreeze_queue(nbd->disk->queue);",
                "\tblk_mq_unfreeze_queue(nbd->disk->queue);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "nbd_add_socket in drivers/block/nbd.c in the Linux kernel through 5.10.12 has an ndb_queue_rq use-after-free that could be triggered by local attackers (with access to the nbd device) via an I/O request at a certain point during device setup, aka CID-b98e762e3d71.",
        "id": 2980
    },
    {
        "cve_id": "CVE-2023-1582",
        "code_before_change": "static void smaps_pte_entry(pte_t *pte, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page = NULL;\n\n\tif (pte_present(*pte)) {\n\t\tpage = vm_normal_page(vma, addr, *pte);\n\t} else if (is_swap_pte(*pte)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(*pte);\n\n\t\tif (!non_swap_entry(swpent)) {\n\t\t\tint mapcount;\n\n\t\t\tmss->swap += PAGE_SIZE;\n\t\t\tmapcount = swp_swapcount(swpent);\n\t\t\tif (mapcount >= 2) {\n\t\t\t\tu64 pss_delta = (u64)PAGE_SIZE << PSS_SHIFT;\n\n\t\t\t\tdo_div(pss_delta, mapcount);\n\t\t\t\tmss->swap_pss += pss_delta;\n\t\t\t} else {\n\t\t\t\tmss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;\n\t\t\t}\n\t\t} else if (is_pfn_swap_entry(swpent))\n\t\t\tpage = pfn_swap_entry_to_page(swpent);\n\t} else {\n\t\tsmaps_pte_hole_lookup(addr, walk);\n\t\treturn;\n\t}\n\n\tif (!page)\n\t\treturn;\n\n\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte), locked);\n}",
        "code_after_change": "static void smaps_pte_entry(pte_t *pte, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page = NULL;\n\tbool migration = false;\n\n\tif (pte_present(*pte)) {\n\t\tpage = vm_normal_page(vma, addr, *pte);\n\t} else if (is_swap_pte(*pte)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(*pte);\n\n\t\tif (!non_swap_entry(swpent)) {\n\t\t\tint mapcount;\n\n\t\t\tmss->swap += PAGE_SIZE;\n\t\t\tmapcount = swp_swapcount(swpent);\n\t\t\tif (mapcount >= 2) {\n\t\t\t\tu64 pss_delta = (u64)PAGE_SIZE << PSS_SHIFT;\n\n\t\t\t\tdo_div(pss_delta, mapcount);\n\t\t\t\tmss->swap_pss += pss_delta;\n\t\t\t} else {\n\t\t\t\tmss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;\n\t\t\t}\n\t\t} else if (is_pfn_swap_entry(swpent)) {\n\t\t\tif (is_migration_entry(swpent))\n\t\t\t\tmigration = true;\n\t\t\tpage = pfn_swap_entry_to_page(swpent);\n\t\t}\n\t} else {\n\t\tsmaps_pte_hole_lookup(addr, walk);\n\t\treturn;\n\t}\n\n\tif (!page)\n\t\treturn;\n\n\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte),\n\t\t      locked, migration);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tstruct vm_area_struct *vma = walk->vma;\n \tbool locked = !!(vma->vm_flags & VM_LOCKED);\n \tstruct page *page = NULL;\n+\tbool migration = false;\n \n \tif (pte_present(*pte)) {\n \t\tpage = vm_normal_page(vma, addr, *pte);\n@@ -24,8 +25,11 @@\n \t\t\t} else {\n \t\t\t\tmss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;\n \t\t\t}\n-\t\t} else if (is_pfn_swap_entry(swpent))\n+\t\t} else if (is_pfn_swap_entry(swpent)) {\n+\t\t\tif (is_migration_entry(swpent))\n+\t\t\t\tmigration = true;\n \t\t\tpage = pfn_swap_entry_to_page(swpent);\n+\t\t}\n \t} else {\n \t\tsmaps_pte_hole_lookup(addr, walk);\n \t\treturn;\n@@ -34,5 +38,6 @@\n \tif (!page)\n \t\treturn;\n \n-\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte), locked);\n+\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte),\n+\t\t      locked, migration);\n }",
        "function_modified_lines": {
            "added": [
                "\tbool migration = false;",
                "\t\t} else if (is_pfn_swap_entry(swpent)) {",
                "\t\t\tif (is_migration_entry(swpent))",
                "\t\t\t\tmigration = true;",
                "\t\t}",
                "\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte),",
                "\t\t      locked, migration);"
            ],
            "deleted": [
                "\t\t} else if (is_pfn_swap_entry(swpent))",
                "\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte), locked);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A race problem was found in fs/proc/task_mmu.c in the memory management sub-component in the Linux kernel. This issue may allow a local attacker with user privilege to cause a denial of service.",
        "id": 3870
    },
    {
        "cve_id": "CVE-2018-7995",
        "code_before_change": "static ssize_t store_int_with_restart(struct device *s,\n\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t      const char *buf, size_t size)\n{\n\tssize_t ret = device_store_int(s, attr, buf, size);\n\tmce_restart();\n\treturn ret;\n}",
        "code_after_change": "static ssize_t store_int_with_restart(struct device *s,\n\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t      const char *buf, size_t size)\n{\n\tunsigned long old_check_interval = check_interval;\n\tssize_t ret = device_store_ulong(s, attr, buf, size);\n\n\tif (check_interval == old_check_interval)\n\t\treturn ret;\n\n\tif (check_interval < 1)\n\t\tcheck_interval = 1;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tmce_restart();\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,18 @@\n \t\t\t\t      struct device_attribute *attr,\n \t\t\t\t      const char *buf, size_t size)\n {\n-\tssize_t ret = device_store_int(s, attr, buf, size);\n+\tunsigned long old_check_interval = check_interval;\n+\tssize_t ret = device_store_ulong(s, attr, buf, size);\n+\n+\tif (check_interval == old_check_interval)\n+\t\treturn ret;\n+\n+\tif (check_interval < 1)\n+\t\tcheck_interval = 1;\n+\n+\tmutex_lock(&mce_sysfs_mutex);\n \tmce_restart();\n+\tmutex_unlock(&mce_sysfs_mutex);\n+\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tunsigned long old_check_interval = check_interval;",
                "\tssize_t ret = device_store_ulong(s, attr, buf, size);",
                "",
                "\tif (check_interval == old_check_interval)",
                "\t\treturn ret;",
                "",
                "\tif (check_interval < 1)",
                "\t\tcheck_interval = 1;",
                "",
                "\tmutex_lock(&mce_sysfs_mutex);",
                "\tmutex_unlock(&mce_sysfs_mutex);",
                ""
            ],
            "deleted": [
                "\tssize_t ret = device_store_int(s, attr, buf, size);"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the store_int_with_restart() function in arch/x86/kernel/cpu/mcheck/mce.c in the Linux kernel through 4.15.7 allows local users to cause a denial of service (panic) by leveraging root access to write to the check_interval file in a /sys/devices/system/machinecheck/machinecheck<cpu number> directory. NOTE: a third party has indicated that this report is not security relevant",
        "id": 1855
    },
    {
        "cve_id": "CVE-2018-7995",
        "code_before_change": "static ssize_t set_ignore_ce(struct device *s,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tif (mca_cfg.ignore_ce ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable ce features */\n\t\t\tmce_timer_delete_all();\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.ignore_ce = true;\n\t\t} else {\n\t\t\t/* enable ce features */\n\t\t\tmca_cfg.ignore_ce = false;\n\t\t\ton_each_cpu(mce_enable_ce, (void *)1, 1);\n\t\t}\n\t}\n\treturn size;\n}",
        "code_after_change": "static ssize_t set_ignore_ce(struct device *s,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tif (mca_cfg.ignore_ce ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable ce features */\n\t\t\tmce_timer_delete_all();\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.ignore_ce = true;\n\t\t} else {\n\t\t\t/* enable ce features */\n\t\t\tmca_cfg.ignore_ce = false;\n\t\t\ton_each_cpu(mce_enable_ce, (void *)1, 1);\n\t\t}\n\t}\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn size;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tif (kstrtou64(buf, 0, &new) < 0)\n \t\treturn -EINVAL;\n \n+\tmutex_lock(&mce_sysfs_mutex);\n \tif (mca_cfg.ignore_ce ^ !!new) {\n \t\tif (new) {\n \t\t\t/* disable ce features */\n@@ -19,5 +20,7 @@\n \t\t\ton_each_cpu(mce_enable_ce, (void *)1, 1);\n \t\t}\n \t}\n+\tmutex_unlock(&mce_sysfs_mutex);\n+\n \treturn size;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&mce_sysfs_mutex);",
                "\tmutex_unlock(&mce_sysfs_mutex);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "Race condition in the store_int_with_restart() function in arch/x86/kernel/cpu/mcheck/mce.c in the Linux kernel through 4.15.7 allows local users to cause a denial of service (panic) by leveraging root access to write to the check_interval file in a /sys/devices/system/machinecheck/machinecheck<cpu number> directory. NOTE: a third party has indicated that this report is not security relevant",
        "id": 1857
    },
    {
        "cve_id": "CVE-2023-35827",
        "code_before_change": "static int ravb_close(struct net_device *ndev)\n{\n\tstruct device_node *np = ndev->dev.parent->of_node;\n\tstruct ravb_private *priv = netdev_priv(ndev);\n\tconst struct ravb_hw_info *info = priv->info;\n\tstruct ravb_tstamp_skb *ts_skb, *ts_skb2;\n\n\tnetif_tx_stop_all_queues(ndev);\n\n\t/* Disable interrupts by clearing the interrupt masks. */\n\travb_write(ndev, 0, RIC0);\n\travb_write(ndev, 0, RIC2);\n\travb_write(ndev, 0, TIC);\n\n\t/* Stop PTP Clock driver */\n\tif (info->gptp)\n\t\travb_ptp_stop(ndev);\n\n\t/* Set the config mode to stop the AVB-DMAC's processes */\n\tif (ravb_stop_dma(ndev) < 0)\n\t\tnetdev_err(ndev,\n\t\t\t   \"device will be stopped after h/w processes are done.\\n\");\n\n\t/* Clear the timestamp list */\n\tif (info->gptp || info->ccc_gac) {\n\t\tlist_for_each_entry_safe(ts_skb, ts_skb2, &priv->ts_skb_list, list) {\n\t\t\tlist_del(&ts_skb->list);\n\t\t\tkfree_skb(ts_skb->skb);\n\t\t\tkfree(ts_skb);\n\t\t}\n\t}\n\n\t/* PHY disconnect */\n\tif (ndev->phydev) {\n\t\tphy_stop(ndev->phydev);\n\t\tphy_disconnect(ndev->phydev);\n\t\tif (of_phy_is_fixed_link(np))\n\t\t\tof_phy_deregister_fixed_link(np);\n\t}\n\n\tif (info->multi_irqs) {\n\t\tfree_irq(priv->tx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->tx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->emac_irq, ndev);\n\t\tif (info->err_mgmt_irqs) {\n\t\t\tfree_irq(priv->erra_irq, ndev);\n\t\t\tfree_irq(priv->mgmta_irq, ndev);\n\t\t}\n\t}\n\tfree_irq(ndev->irq, ndev);\n\n\tif (info->nc_queues)\n\t\tnapi_disable(&priv->napi[RAVB_NC]);\n\tnapi_disable(&priv->napi[RAVB_BE]);\n\n\t/* Free all the skb's in the RX queue and the DMA buffers. */\n\travb_ring_free(ndev, RAVB_BE);\n\tif (info->nc_queues)\n\t\travb_ring_free(ndev, RAVB_NC);\n\n\treturn 0;\n}",
        "code_after_change": "static int ravb_close(struct net_device *ndev)\n{\n\tstruct device_node *np = ndev->dev.parent->of_node;\n\tstruct ravb_private *priv = netdev_priv(ndev);\n\tconst struct ravb_hw_info *info = priv->info;\n\tstruct ravb_tstamp_skb *ts_skb, *ts_skb2;\n\n\tnetif_tx_stop_all_queues(ndev);\n\n\t/* Disable interrupts by clearing the interrupt masks. */\n\travb_write(ndev, 0, RIC0);\n\travb_write(ndev, 0, RIC2);\n\travb_write(ndev, 0, TIC);\n\n\t/* Stop PTP Clock driver */\n\tif (info->gptp)\n\t\travb_ptp_stop(ndev);\n\n\t/* Set the config mode to stop the AVB-DMAC's processes */\n\tif (ravb_stop_dma(ndev) < 0)\n\t\tnetdev_err(ndev,\n\t\t\t   \"device will be stopped after h/w processes are done.\\n\");\n\n\t/* Clear the timestamp list */\n\tif (info->gptp || info->ccc_gac) {\n\t\tlist_for_each_entry_safe(ts_skb, ts_skb2, &priv->ts_skb_list, list) {\n\t\t\tlist_del(&ts_skb->list);\n\t\t\tkfree_skb(ts_skb->skb);\n\t\t\tkfree(ts_skb);\n\t\t}\n\t}\n\n\t/* PHY disconnect */\n\tif (ndev->phydev) {\n\t\tphy_stop(ndev->phydev);\n\t\tphy_disconnect(ndev->phydev);\n\t\tif (of_phy_is_fixed_link(np))\n\t\t\tof_phy_deregister_fixed_link(np);\n\t}\n\n\tcancel_work_sync(&priv->work);\n\n\tif (info->multi_irqs) {\n\t\tfree_irq(priv->tx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_NC], ndev);\n\t\tfree_irq(priv->tx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->rx_irqs[RAVB_BE], ndev);\n\t\tfree_irq(priv->emac_irq, ndev);\n\t\tif (info->err_mgmt_irqs) {\n\t\t\tfree_irq(priv->erra_irq, ndev);\n\t\t\tfree_irq(priv->mgmta_irq, ndev);\n\t\t}\n\t}\n\tfree_irq(ndev->irq, ndev);\n\n\tif (info->nc_queues)\n\t\tnapi_disable(&priv->napi[RAVB_NC]);\n\tnapi_disable(&priv->napi[RAVB_BE]);\n\n\t/* Free all the skb's in the RX queue and the DMA buffers. */\n\travb_ring_free(ndev, RAVB_BE);\n\tif (info->nc_queues)\n\t\travb_ring_free(ndev, RAVB_NC);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,6 +38,8 @@\n \t\t\tof_phy_deregister_fixed_link(np);\n \t}\n \n+\tcancel_work_sync(&priv->work);\n+\n \tif (info->multi_irqs) {\n \t\tfree_irq(priv->tx_irqs[RAVB_NC], ndev);\n \t\tfree_irq(priv->rx_irqs[RAVB_NC], ndev);",
        "function_modified_lines": {
            "added": [
                "\tcancel_work_sync(&priv->work);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.3.8. A use-after-free was found in ravb_remove in drivers/net/ethernet/renesas/ravb_main.c.",
        "id": 4114
    },
    {
        "cve_id": "CVE-2022-45888",
        "code_before_change": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tkref_put(&xdev->kref, cleanup_dev);\n}",
        "code_after_change": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tmutex_lock(&kref_mutex);\n\tkref_put(&xdev->kref, cleanup_dev);\n\tmutex_unlock(&kref_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -61,5 +61,7 @@\n \n \txdev->dev = NULL;\n \n+\tmutex_lock(&kref_mutex);\n \tkref_put(&xdev->kref, cleanup_dev);\n+\tmutex_unlock(&kref_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&kref_mutex);",
                "\tmutex_unlock(&kref_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/char/xillybus/xillyusb.c has a race condition and use-after-free during physical removal of a USB device.",
        "id": 3754
    },
    {
        "cve_id": "CVE-2021-44733",
        "code_before_change": "struct tee_shm *tee_shm_alloc(struct tee_context *ctx, size_t size, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tstruct tee_shm_pool_mgr *poolm = NULL;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\n\tif (!(flags & TEE_SHM_MAPPED)) {\n\t\tdev_err(teedev->dev.parent,\n\t\t\t\"only mapped allocations supported\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif ((flags & ~(TEE_SHM_MAPPED | TEE_SHM_DMA_BUF | TEE_SHM_PRIV))) {\n\t\tdev_err(teedev->dev.parent, \"invalid shm flags 0x%x\", flags);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->pool) {\n\t\t/* teedev has been detached from driver */\n\t\tret = ERR_PTR(-EINVAL);\n\t\tgoto err_dev_put;\n\t}\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err_dev_put;\n\t}\n\n\tshm->flags = flags | TEE_SHM_POOL;\n\tshm->ctx = ctx;\n\tif (flags & TEE_SHM_DMA_BUF)\n\t\tpoolm = teedev->pool->dma_buf_mgr;\n\telse\n\t\tpoolm = teedev->pool->private_mgr;\n\n\trc = poolm->ops->alloc(poolm, shm, size);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err_kfree;\n\t}\n\n\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\n\t\tmutex_lock(&teedev->mutex);\n\t\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\t\tmutex_unlock(&teedev->mutex);\n\t\tif (shm->id < 0) {\n\t\t\tret = ERR_PTR(shm->id);\n\t\t\tgoto err_pool_free;\n\t\t}\n\n\t\texp_info.ops = &tee_shm_dma_buf_ops;\n\t\texp_info.size = shm->size;\n\t\texp_info.flags = O_RDWR;\n\t\texp_info.priv = shm;\n\n\t\tshm->dmabuf = dma_buf_export(&exp_info);\n\t\tif (IS_ERR(shm->dmabuf)) {\n\t\t\tret = ERR_CAST(shm->dmabuf);\n\t\t\tgoto err_rem;\n\t\t}\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\treturn shm;\nerr_rem:\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tmutex_lock(&teedev->mutex);\n\t\tidr_remove(&teedev->idr, shm->id);\n\t\tmutex_unlock(&teedev->mutex);\n\t}\nerr_pool_free:\n\tpoolm->ops->free(poolm, shm);\nerr_kfree:\n\tkfree(shm);\nerr_dev_put:\n\ttee_device_put(teedev);\n\treturn ret;\n}",
        "code_after_change": "struct tee_shm *tee_shm_alloc(struct tee_context *ctx, size_t size, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tstruct tee_shm_pool_mgr *poolm = NULL;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\n\tif (!(flags & TEE_SHM_MAPPED)) {\n\t\tdev_err(teedev->dev.parent,\n\t\t\t\"only mapped allocations supported\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif ((flags & ~(TEE_SHM_MAPPED | TEE_SHM_DMA_BUF | TEE_SHM_PRIV))) {\n\t\tdev_err(teedev->dev.parent, \"invalid shm flags 0x%x\", flags);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->pool) {\n\t\t/* teedev has been detached from driver */\n\t\tret = ERR_PTR(-EINVAL);\n\t\tgoto err_dev_put;\n\t}\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err_dev_put;\n\t}\n\n\trefcount_set(&shm->refcount, 1);\n\tshm->flags = flags | TEE_SHM_POOL;\n\tshm->ctx = ctx;\n\tif (flags & TEE_SHM_DMA_BUF)\n\t\tpoolm = teedev->pool->dma_buf_mgr;\n\telse\n\t\tpoolm = teedev->pool->private_mgr;\n\n\trc = poolm->ops->alloc(poolm, shm, size);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err_kfree;\n\t}\n\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tmutex_lock(&teedev->mutex);\n\t\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\t\tmutex_unlock(&teedev->mutex);\n\t\tif (shm->id < 0) {\n\t\t\tret = ERR_PTR(shm->id);\n\t\t\tgoto err_pool_free;\n\t\t}\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\treturn shm;\nerr_pool_free:\n\tpoolm->ops->free(poolm, shm);\nerr_kfree:\n\tkfree(shm);\nerr_dev_put:\n\ttee_device_put(teedev);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -32,6 +32,7 @@\n \t\tgoto err_dev_put;\n \t}\n \n+\trefcount_set(&shm->refcount, 1);\n \tshm->flags = flags | TEE_SHM_POOL;\n \tshm->ctx = ctx;\n \tif (flags & TEE_SHM_DMA_BUF)\n@@ -45,10 +46,7 @@\n \t\tgoto err_kfree;\n \t}\n \n-\n \tif (flags & TEE_SHM_DMA_BUF) {\n-\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n-\n \t\tmutex_lock(&teedev->mutex);\n \t\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n \t\tmutex_unlock(&teedev->mutex);\n@@ -56,28 +54,11 @@\n \t\t\tret = ERR_PTR(shm->id);\n \t\t\tgoto err_pool_free;\n \t\t}\n-\n-\t\texp_info.ops = &tee_shm_dma_buf_ops;\n-\t\texp_info.size = shm->size;\n-\t\texp_info.flags = O_RDWR;\n-\t\texp_info.priv = shm;\n-\n-\t\tshm->dmabuf = dma_buf_export(&exp_info);\n-\t\tif (IS_ERR(shm->dmabuf)) {\n-\t\t\tret = ERR_CAST(shm->dmabuf);\n-\t\t\tgoto err_rem;\n-\t\t}\n \t}\n \n \tteedev_ctx_get(ctx);\n \n \treturn shm;\n-err_rem:\n-\tif (flags & TEE_SHM_DMA_BUF) {\n-\t\tmutex_lock(&teedev->mutex);\n-\t\tidr_remove(&teedev->idr, shm->id);\n-\t\tmutex_unlock(&teedev->mutex);\n-\t}\n err_pool_free:\n \tpoolm->ops->free(poolm, shm);\n err_kfree:",
        "function_modified_lines": {
            "added": [
                "\trefcount_set(&shm->refcount, 1);"
            ],
            "deleted": [
                "",
                "\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);",
                "",
                "",
                "\t\texp_info.ops = &tee_shm_dma_buf_ops;",
                "\t\texp_info.size = shm->size;",
                "\t\texp_info.flags = O_RDWR;",
                "\t\texp_info.priv = shm;",
                "",
                "\t\tshm->dmabuf = dma_buf_export(&exp_info);",
                "\t\tif (IS_ERR(shm->dmabuf)) {",
                "\t\t\tret = ERR_CAST(shm->dmabuf);",
                "\t\t\tgoto err_rem;",
                "\t\t}",
                "err_rem:",
                "\tif (flags & TEE_SHM_DMA_BUF) {",
                "\t\tmutex_lock(&teedev->mutex);",
                "\t\tidr_remove(&teedev->idr, shm->id);",
                "\t\tmutex_unlock(&teedev->mutex);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362"
        ],
        "cve_description": "A use-after-free exists in drivers/tee/tee_shm.c in the TEE subsystem in the Linux kernel through 5.15.11. This occurs because of a race condition in tee_shm_get_from_id during an attempt to free a shared memory object.",
        "id": 3171
    },
    {
        "cve_id": "CVE-2016-10200",
        "code_before_change": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "code_after_change": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out_unlock;\n\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,6 @@\n \tint addr_type;\n \tint err;\n \n-\tif (!sock_flag(sk, SOCK_ZAPPED))\n-\t\treturn -EINVAL;\n \tif (addr->l2tp_family != AF_INET6)\n \t\treturn -EINVAL;\n \tif (addr_len < sizeof(*addr))\n@@ -35,6 +33,9 @@\n \tlock_sock(sk);\n \n \terr = -EINVAL;\n+\tif (!sock_flag(sk, SOCK_ZAPPED))\n+\t\tgoto out_unlock;\n+\n \tif (sk->sk_state != TCP_CLOSE)\n \t\tgoto out_unlock;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\tgoto out_unlock;",
                ""
            ],
            "deleted": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the L2TPv3 IP Encapsulation feature in the Linux kernel before 4.8.14 allows local users to gain privileges or cause a denial of service (use-after-free) by making multiple bind system calls without properly ascertaining whether a socket has the SOCK_ZAPPED status, related to net/l2tp/l2tp_ip.c and net/l2tp/l2tp_ip6.c.",
        "id": 899
    }
]