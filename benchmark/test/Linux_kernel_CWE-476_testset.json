[
    {
        "cve_id": "CVE-2022-2153",
        "code_before_change": "bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,\n\t\tstruct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tint i;\n\tbool ret;\n\n\t*r = -1;\n\n\tif (irq->shorthand == APIC_DEST_SELF) {\n\t\t*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);\n\t\treturn true;\n\t}\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);\n\tif (ret) {\n\t\t*r = 0;\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dst[i])\n\t\t\t\tcontinue;\n\t\t\t*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}",
        "code_after_change": "bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,\n\t\tstruct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tint i;\n\tbool ret;\n\n\t*r = -1;\n\n\tif (irq->shorthand == APIC_DEST_SELF) {\n\t\tif (KVM_BUG_ON(!src, kvm)) {\n\t\t\t*r = 0;\n\t\t\treturn true;\n\t\t}\n\t\t*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);\n\t\treturn true;\n\t}\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);\n\tif (ret) {\n\t\t*r = 0;\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dst[i])\n\t\t\t\tcontinue;\n\t\t\t*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,10 @@\n \t*r = -1;\n \n \tif (irq->shorthand == APIC_DEST_SELF) {\n+\t\tif (KVM_BUG_ON(!src, kvm)) {\n+\t\t\t*r = 0;\n+\t\t\treturn true;\n+\t\t}\n \t\t*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);\n \t\treturn true;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (KVM_BUG_ON(!src, kvm)) {",
                "\t\t\t*r = 0;",
                "\t\t\treturn true;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s KVM when attempting to set a SynIC IRQ. This issue makes it possible for a misbehaving VMM to write to SYNIC/STIMER MSRs, causing a NULL pointer dereference. This flaw allows an unprivileged local attacker on the host to issue specific ioctl calls, causing a kernel oops condition that results in a denial of service.",
        "id": 3413
    },
    {
        "cve_id": "CVE-2019-20806",
        "code_before_change": "static void tw5864_handle_frame(struct tw5864_h264_frame *frame)\n{\n#define SKIP_VLCBUF_BYTES 3\n\tstruct tw5864_input *input = frame->input;\n\tstruct tw5864_dev *dev = input->root;\n\tstruct tw5864_buf *vb;\n\tstruct vb2_v4l2_buffer *v4l2_buf;\n\tint frame_len = frame->vlc_len - SKIP_VLCBUF_BYTES;\n\tu8 *dst = input->buf_cur_ptr;\n\tu8 tail_mask, vlc_mask = 0;\n\tint i;\n\tu8 vlc_first_byte = ((u8 *)(frame->vlc.addr + SKIP_VLCBUF_BYTES))[0];\n\tunsigned long flags;\n\tint zero_run;\n\tu8 *src;\n\tu8 *src_end;\n\n#ifdef DEBUG\n\tif (frame->checksum !=\n\t    tw5864_vlc_checksum((u32 *)frame->vlc.addr, frame_len))\n\t\tdev_err(&dev->pci->dev,\n\t\t\t\"Checksum of encoded frame doesn't match!\\n\");\n#endif\n\n\tspin_lock_irqsave(&input->slock, flags);\n\tvb = input->vb;\n\tinput->vb = NULL;\n\tspin_unlock_irqrestore(&input->slock, flags);\n\n\tv4l2_buf = to_vb2_v4l2_buffer(&vb->vb.vb2_buf);\n\n\tif (!vb) { /* Gone because of disabling */\n\t\tdev_dbg(&dev->pci->dev, \"vb is empty, dropping frame\\n\");\n\t\treturn;\n\t}\n\n\t/*\n\t * Check for space.\n\t * Mind the overhead of startcode emulation prevention.\n\t */\n\tif (input->buf_cur_space_left < frame_len * 5 / 4) {\n\t\tdev_err_once(&dev->pci->dev,\n\t\t\t     \"Left space in vb2 buffer, %d bytes, is less than considered safely enough to put frame of length %d. Dropping this frame.\\n\",\n\t\t\t     input->buf_cur_space_left, frame_len);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < 8 - input->tail_nb_bits; i++)\n\t\tvlc_mask |= 1 << i;\n\ttail_mask = (~vlc_mask) & 0xff;\n\n\tdst[0] = (input->tail & tail_mask) | (vlc_first_byte & vlc_mask);\n\tframe_len--;\n\tdst++;\n\n\t/* H.264 startcode emulation prevention */\n\tsrc = frame->vlc.addr + SKIP_VLCBUF_BYTES + 1;\n\tsrc_end = src + frame_len;\n\tzero_run = 0;\n\tfor (; src < src_end; src++) {\n\t\tif (zero_run < 2) {\n\t\t\tif (*src == 0)\n\t\t\t\t++zero_run;\n\t\t\telse\n\t\t\t\tzero_run = 0;\n\t\t} else {\n\t\t\tif ((*src & ~0x03) == 0)\n\t\t\t\t*dst++ = 0x03;\n\t\t\tzero_run = *src == 0;\n\t\t}\n\t\t*dst++ = *src;\n\t}\n\n\tvb2_set_plane_payload(&vb->vb.vb2_buf, 0,\n\t\t\t      dst - (u8 *)vb2_plane_vaddr(&vb->vb.vb2_buf, 0));\n\n\tvb->vb.vb2_buf.timestamp = frame->timestamp;\n\tv4l2_buf->field = V4L2_FIELD_INTERLACED;\n\tv4l2_buf->sequence = frame->seqno;\n\n\t/* Check for motion flags */\n\tif (frame->gop_seqno /* P-frame */ &&\n\t    tw5864_is_motion_triggered(frame)) {\n\t\tstruct v4l2_event ev = {\n\t\t\t.type = V4L2_EVENT_MOTION_DET,\n\t\t\t.u.motion_det = {\n\t\t\t\t.flags = V4L2_EVENT_MD_FL_HAVE_FRAME_SEQ,\n\t\t\t\t.frame_sequence = v4l2_buf->sequence,\n\t\t\t},\n\t\t};\n\n\t\tv4l2_event_queue(&input->vdev, &ev);\n\t}\n\n\tvb2_buffer_done(&vb->vb.vb2_buf, VB2_BUF_STATE_DONE);\n}",
        "code_after_change": "static void tw5864_handle_frame(struct tw5864_h264_frame *frame)\n{\n#define SKIP_VLCBUF_BYTES 3\n\tstruct tw5864_input *input = frame->input;\n\tstruct tw5864_dev *dev = input->root;\n\tstruct tw5864_buf *vb;\n\tstruct vb2_v4l2_buffer *v4l2_buf;\n\tint frame_len = frame->vlc_len - SKIP_VLCBUF_BYTES;\n\tu8 *dst = input->buf_cur_ptr;\n\tu8 tail_mask, vlc_mask = 0;\n\tint i;\n\tu8 vlc_first_byte = ((u8 *)(frame->vlc.addr + SKIP_VLCBUF_BYTES))[0];\n\tunsigned long flags;\n\tint zero_run;\n\tu8 *src;\n\tu8 *src_end;\n\n#ifdef DEBUG\n\tif (frame->checksum !=\n\t    tw5864_vlc_checksum((u32 *)frame->vlc.addr, frame_len))\n\t\tdev_err(&dev->pci->dev,\n\t\t\t\"Checksum of encoded frame doesn't match!\\n\");\n#endif\n\n\tspin_lock_irqsave(&input->slock, flags);\n\tvb = input->vb;\n\tinput->vb = NULL;\n\tspin_unlock_irqrestore(&input->slock, flags);\n\n\tif (!vb) { /* Gone because of disabling */\n\t\tdev_dbg(&dev->pci->dev, \"vb is empty, dropping frame\\n\");\n\t\treturn;\n\t}\n\n\tv4l2_buf = to_vb2_v4l2_buffer(&vb->vb.vb2_buf);\n\n\t/*\n\t * Check for space.\n\t * Mind the overhead of startcode emulation prevention.\n\t */\n\tif (input->buf_cur_space_left < frame_len * 5 / 4) {\n\t\tdev_err_once(&dev->pci->dev,\n\t\t\t     \"Left space in vb2 buffer, %d bytes, is less than considered safely enough to put frame of length %d. Dropping this frame.\\n\",\n\t\t\t     input->buf_cur_space_left, frame_len);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < 8 - input->tail_nb_bits; i++)\n\t\tvlc_mask |= 1 << i;\n\ttail_mask = (~vlc_mask) & 0xff;\n\n\tdst[0] = (input->tail & tail_mask) | (vlc_first_byte & vlc_mask);\n\tframe_len--;\n\tdst++;\n\n\t/* H.264 startcode emulation prevention */\n\tsrc = frame->vlc.addr + SKIP_VLCBUF_BYTES + 1;\n\tsrc_end = src + frame_len;\n\tzero_run = 0;\n\tfor (; src < src_end; src++) {\n\t\tif (zero_run < 2) {\n\t\t\tif (*src == 0)\n\t\t\t\t++zero_run;\n\t\t\telse\n\t\t\t\tzero_run = 0;\n\t\t} else {\n\t\t\tif ((*src & ~0x03) == 0)\n\t\t\t\t*dst++ = 0x03;\n\t\t\tzero_run = *src == 0;\n\t\t}\n\t\t*dst++ = *src;\n\t}\n\n\tvb2_set_plane_payload(&vb->vb.vb2_buf, 0,\n\t\t\t      dst - (u8 *)vb2_plane_vaddr(&vb->vb.vb2_buf, 0));\n\n\tvb->vb.vb2_buf.timestamp = frame->timestamp;\n\tv4l2_buf->field = V4L2_FIELD_INTERLACED;\n\tv4l2_buf->sequence = frame->seqno;\n\n\t/* Check for motion flags */\n\tif (frame->gop_seqno /* P-frame */ &&\n\t    tw5864_is_motion_triggered(frame)) {\n\t\tstruct v4l2_event ev = {\n\t\t\t.type = V4L2_EVENT_MOTION_DET,\n\t\t\t.u.motion_det = {\n\t\t\t\t.flags = V4L2_EVENT_MD_FL_HAVE_FRAME_SEQ,\n\t\t\t\t.frame_sequence = v4l2_buf->sequence,\n\t\t\t},\n\t\t};\n\n\t\tv4l2_event_queue(&input->vdev, &ev);\n\t}\n\n\tvb2_buffer_done(&vb->vb.vb2_buf, VB2_BUF_STATE_DONE);\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,12 +27,12 @@\n \tinput->vb = NULL;\n \tspin_unlock_irqrestore(&input->slock, flags);\n \n-\tv4l2_buf = to_vb2_v4l2_buffer(&vb->vb.vb2_buf);\n-\n \tif (!vb) { /* Gone because of disabling */\n \t\tdev_dbg(&dev->pci->dev, \"vb is empty, dropping frame\\n\");\n \t\treturn;\n \t}\n+\n+\tv4l2_buf = to_vb2_v4l2_buffer(&vb->vb.vb2_buf);\n \n \t/*\n \t * Check for space.",
        "function_modified_lines": {
            "added": [
                "",
                "\tv4l2_buf = to_vb2_v4l2_buffer(&vb->vb.vb2_buf);"
            ],
            "deleted": [
                "\tv4l2_buf = to_vb2_v4l2_buffer(&vb->vb.vb2_buf);",
                ""
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.2. There is a NULL pointer dereference in tw5864_handle_frame() in drivers/media/pci/tw5864/tw5864-video.c, which may cause denial of service, aka CID-2e7682ebfc75.",
        "id": 2283
    },
    {
        "cve_id": "CVE-2022-23222",
        "code_before_change": "static void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id &&\n\t    !WARN_ON_ONCE(!reg->id)) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t\t/* We don't need id and ref_obj_id from this point\n\t\t\t * onwards anymore, thus we should better reset it,\n\t\t\t * so that state pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t\treg->ref_obj_id = 0;\n\n\t\t\treturn;\n\t\t}\n\n\t\tmark_ptr_not_null_reg(reg);\n\n\t\tif (!reg_may_point_to_spin_lock(reg)) {\n\t\t\t/* For not-NULL ptr, reg->ref_obj_id will be reset\n\t\t\t * in release_reg_references().\n\t\t\t *\n\t\t\t * reg->id is still used by spin_lock ptr. Other\n\t\t\t * than spin_lock ptr type, reg->id can be reset.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}",
        "code_after_change": "static void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (type_may_be_null(reg->type) && reg->id == id &&\n\t    !WARN_ON_ONCE(!reg->id)) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t\t/* We don't need id and ref_obj_id from this point\n\t\t\t * onwards anymore, thus we should better reset it,\n\t\t\t * so that state pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t\treg->ref_obj_id = 0;\n\n\t\t\treturn;\n\t\t}\n\n\t\tmark_ptr_not_null_reg(reg);\n\n\t\tif (!reg_may_point_to_spin_lock(reg)) {\n\t\t\t/* For not-NULL ptr, reg->ref_obj_id will be reset\n\t\t\t * in release_reg_references().\n\t\t\t *\n\t\t\t * reg->id is still used by spin_lock ptr. Other\n\t\t\t * than spin_lock ptr type, reg->id can be reset.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n \t\t\t\t struct bpf_reg_state *reg, u32 id,\n \t\t\t\t bool is_null)\n {\n-\tif (reg_type_may_be_null(reg->type) && reg->id == id &&\n+\tif (type_may_be_null(reg->type) && reg->id == id &&\n \t    !WARN_ON_ONCE(!reg->id)) {\n \t\t/* Old offset (both fixed and variable parts) should\n \t\t * have been known-zero, because we don't allow pointer",
        "function_modified_lines": {
            "added": [
                "\tif (type_may_be_null(reg->type) && reg->id == id &&"
            ],
            "deleted": [
                "\tif (reg_type_may_be_null(reg->type) && reg->id == id &&"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "kernel/bpf/verifier.c in the Linux kernel through 5.15.14 allows local users to gain privileges because of the availability of pointer arithmetic via certain *_OR_NULL pointer types.",
        "id": 3460
    },
    {
        "cve_id": "CVE-2022-23222",
        "code_before_change": "static int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off, int size,\n\t\t\t    enum bpf_access_type t, enum bpf_reg_type *reg_type,\n\t\t\t    struct btf **btf, u32 *btf_id)\n{\n\tstruct bpf_insn_access_aux info = {\n\t\t.reg_type = *reg_type,\n\t\t.log = &env->log,\n\t};\n\n\tif (env->ops->is_valid_access &&\n\t    env->ops->is_valid_access(off, size, t, env->prog, &info)) {\n\t\t/* A non zero info.ctx_field_size indicates that this field is a\n\t\t * candidate for later verifier transformation to load the whole\n\t\t * field and then apply a mask when accessed with a narrower\n\t\t * access than actual ctx access size. A zero info.ctx_field_size\n\t\t * will only allow for whole field access and rejects any other\n\t\t * type of narrower access.\n\t\t */\n\t\t*reg_type = info.reg_type;\n\n\t\tif (*reg_type == PTR_TO_BTF_ID || *reg_type == PTR_TO_BTF_ID_OR_NULL) {\n\t\t\t*btf = info.btf;\n\t\t\t*btf_id = info.btf_id;\n\t\t} else {\n\t\t\tenv->insn_aux_data[insn_idx].ctx_field_size = info.ctx_field_size;\n\t\t}\n\t\t/* remember the offset of last byte accessed in ctx */\n\t\tif (env->prog->aux->max_ctx_offset < off + size)\n\t\t\tenv->prog->aux->max_ctx_offset = off + size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"invalid bpf_context access off=%d size=%d\\n\", off, size);\n\treturn -EACCES;\n}",
        "code_after_change": "static int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off, int size,\n\t\t\t    enum bpf_access_type t, enum bpf_reg_type *reg_type,\n\t\t\t    struct btf **btf, u32 *btf_id)\n{\n\tstruct bpf_insn_access_aux info = {\n\t\t.reg_type = *reg_type,\n\t\t.log = &env->log,\n\t};\n\n\tif (env->ops->is_valid_access &&\n\t    env->ops->is_valid_access(off, size, t, env->prog, &info)) {\n\t\t/* A non zero info.ctx_field_size indicates that this field is a\n\t\t * candidate for later verifier transformation to load the whole\n\t\t * field and then apply a mask when accessed with a narrower\n\t\t * access than actual ctx access size. A zero info.ctx_field_size\n\t\t * will only allow for whole field access and rejects any other\n\t\t * type of narrower access.\n\t\t */\n\t\t*reg_type = info.reg_type;\n\n\t\tif (base_type(*reg_type) == PTR_TO_BTF_ID) {\n\t\t\t*btf = info.btf;\n\t\t\t*btf_id = info.btf_id;\n\t\t} else {\n\t\t\tenv->insn_aux_data[insn_idx].ctx_field_size = info.ctx_field_size;\n\t\t}\n\t\t/* remember the offset of last byte accessed in ctx */\n\t\tif (env->prog->aux->max_ctx_offset < off + size)\n\t\t\tenv->prog->aux->max_ctx_offset = off + size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"invalid bpf_context access off=%d size=%d\\n\", off, size);\n\treturn -EACCES;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,7 @@\n \t\t */\n \t\t*reg_type = info.reg_type;\n \n-\t\tif (*reg_type == PTR_TO_BTF_ID || *reg_type == PTR_TO_BTF_ID_OR_NULL) {\n+\t\tif (base_type(*reg_type) == PTR_TO_BTF_ID) {\n \t\t\t*btf = info.btf;\n \t\t\t*btf_id = info.btf_id;\n \t\t} else {",
        "function_modified_lines": {
            "added": [
                "\t\tif (base_type(*reg_type) == PTR_TO_BTF_ID) {"
            ],
            "deleted": [
                "\t\tif (*reg_type == PTR_TO_BTF_ID || *reg_type == PTR_TO_BTF_ID_OR_NULL) {"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "kernel/bpf/verifier.c in the Linux kernel through 5.15.14 allows local users to gain privileges because of the availability of pointer arithmetic via certain *_OR_NULL pointer types.",
        "id": 3449
    },
    {
        "cve_id": "CVE-2022-23222",
        "code_before_change": "static int do_check(struct bpf_verifier_env *env)\n{\n\tbool pop_log = !(env->log.level & BPF_LOG_LEVEL2);\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len;\n\tbool do_print_state = false;\n\tint prev_insn_idx = -1;\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tenv->prev_insn_idx = prev_insn_idx;\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++env->insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tenv->insn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, env->insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d%s: safe\\n\",\n\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level & BPF_LOG_LEVEL2 && do_print_state) {\n\t\t\tverbose(env, \"\\nfrom %d to %d%s:\",\n\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\tprint_verifier_state(env, state->frame[state->curframe], true);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tconst struct bpf_insn_cbs cbs = {\n\t\t\t\t.cb_call\t= disasm_kfunc_name,\n\t\t\t\t.cb_print\t= verbose,\n\t\t\t\t.private_data\t= env,\n\t\t\t};\n\n\t\t\tif (verifier_state_scratched(env))\n\t\t\t\tprint_insn_state(env, state->frame[state->curframe]);\n\n\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\tenv->prev_log_len = env->log.len_used;\n\t\t\tverbose(env, \"%d: \", env->insn_idx);\n\t\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t\t\tenv->prev_insn_print_len = env->log.len_used - env->prev_log_len;\n\t\t\tenv->prev_log_len = env->log.len_used;\n\t\t}\n\n\t\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\t\terr = bpf_prog_offload_verify_insn(env, env->insn_idx,\n\t\t\t\t\t\t\t   env->prev_insn_idx);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tregs = cur_regs(env);\n\t\tsanitize_mark_insn_seen(env);\n\t\tprev_insn_idx = env->insn_idx;\n\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->src_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_READ, insn->dst_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (reg_type_mismatch(src_reg_type, *prev_src_type)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_ATOMIC) {\n\t\t\t\terr = check_atomic(env, env->insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_STX uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, insn->src_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (reg_type_mismatch(dst_reg_type, *prev_dst_type)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tif (is_ctx_reg(env, insn->dst_reg)) {\n\t\t\t\tverbose(env, \"BPF_ST stores into R%d %s is not allowed\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, -1, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP || class == BPF_JMP32) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tenv->jmps_processed++;\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    (insn->src_reg != BPF_PSEUDO_KFUNC_CALL\n\t\t\t\t     && insn->off != 0) ||\n\t\t\t\t    (insn->src_reg != BPF_REG_0 &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_CALL &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_KFUNC_CALL) ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock &&\n\t\t\t\t    (insn->src_reg == BPF_PSEUDO_CALL ||\n\t\t\t\t     insn->imm != BPF_FUNC_spin_unlock)) {\n\t\t\t\t\tverbose(env, \"function calls are not allowed while holding a lock\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\t\terr = check_func_call(env, insn, &env->insn_idx);\n\t\t\t\telse if (insn->src_reg == BPF_PSEUDO_KFUNC_CALL)\n\t\t\t\t\terr = check_kfunc_call(env, insn);\n\t\t\t\telse\n\t\t\t\t\terr = check_helper_call(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tenv->insn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock) {\n\t\t\t\t\tverbose(env, \"bpf_spin_unlock is missing\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (state->curframe) {\n\t\t\t\t\t/* exit from nested function */\n\t\t\t\t\terr = prepare_func_exit(env, &env->insn_idx);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\terr = check_reference_leak(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\tmark_verifier_state_scratched(env);\n\t\t\t\tupdate_branch_counts(env, env->cur_state);\n\t\t\t\terr = pop_stack(env, &prev_insn_idx,\n\t\t\t\t\t\t&env->insn_idx, pop_log);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tsanitize_mark_insn_seen(env);\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tenv->insn_idx++;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int do_check(struct bpf_verifier_env *env)\n{\n\tbool pop_log = !(env->log.level & BPF_LOG_LEVEL2);\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len;\n\tbool do_print_state = false;\n\tint prev_insn_idx = -1;\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tenv->prev_insn_idx = prev_insn_idx;\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++env->insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tenv->insn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, env->insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d%s: safe\\n\",\n\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level & BPF_LOG_LEVEL2 && do_print_state) {\n\t\t\tverbose(env, \"\\nfrom %d to %d%s:\",\n\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\tprint_verifier_state(env, state->frame[state->curframe], true);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tconst struct bpf_insn_cbs cbs = {\n\t\t\t\t.cb_call\t= disasm_kfunc_name,\n\t\t\t\t.cb_print\t= verbose,\n\t\t\t\t.private_data\t= env,\n\t\t\t};\n\n\t\t\tif (verifier_state_scratched(env))\n\t\t\t\tprint_insn_state(env, state->frame[state->curframe]);\n\n\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\tenv->prev_log_len = env->log.len_used;\n\t\t\tverbose(env, \"%d: \", env->insn_idx);\n\t\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t\t\tenv->prev_insn_print_len = env->log.len_used - env->prev_log_len;\n\t\t\tenv->prev_log_len = env->log.len_used;\n\t\t}\n\n\t\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\t\terr = bpf_prog_offload_verify_insn(env, env->insn_idx,\n\t\t\t\t\t\t\t   env->prev_insn_idx);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tregs = cur_regs(env);\n\t\tsanitize_mark_insn_seen(env);\n\t\tprev_insn_idx = env->insn_idx;\n\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->src_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_READ, insn->dst_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (reg_type_mismatch(src_reg_type, *prev_src_type)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_ATOMIC) {\n\t\t\t\terr = check_atomic(env, env->insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_STX uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, insn->src_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (reg_type_mismatch(dst_reg_type, *prev_dst_type)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tif (is_ctx_reg(env, insn->dst_reg)) {\n\t\t\t\tverbose(env, \"BPF_ST stores into R%d %s is not allowed\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\treg_type_str(env, reg_state(env, insn->dst_reg)->type));\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, -1, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP || class == BPF_JMP32) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tenv->jmps_processed++;\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    (insn->src_reg != BPF_PSEUDO_KFUNC_CALL\n\t\t\t\t     && insn->off != 0) ||\n\t\t\t\t    (insn->src_reg != BPF_REG_0 &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_CALL &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_KFUNC_CALL) ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock &&\n\t\t\t\t    (insn->src_reg == BPF_PSEUDO_CALL ||\n\t\t\t\t     insn->imm != BPF_FUNC_spin_unlock)) {\n\t\t\t\t\tverbose(env, \"function calls are not allowed while holding a lock\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\t\terr = check_func_call(env, insn, &env->insn_idx);\n\t\t\t\telse if (insn->src_reg == BPF_PSEUDO_KFUNC_CALL)\n\t\t\t\t\terr = check_kfunc_call(env, insn);\n\t\t\t\telse\n\t\t\t\t\terr = check_helper_call(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tenv->insn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock) {\n\t\t\t\t\tverbose(env, \"bpf_spin_unlock is missing\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (state->curframe) {\n\t\t\t\t\t/* exit from nested function */\n\t\t\t\t\terr = prepare_func_exit(env, &env->insn_idx);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\terr = check_reference_leak(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\tmark_verifier_state_scratched(env);\n\t\t\t\tupdate_branch_counts(env, env->cur_state);\n\t\t\t\terr = pop_stack(env, &prev_insn_idx,\n\t\t\t\t\t\t&env->insn_idx, pop_log);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tsanitize_mark_insn_seen(env);\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tenv->insn_idx++;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -199,7 +199,7 @@\n \t\t\tif (is_ctx_reg(env, insn->dst_reg)) {\n \t\t\t\tverbose(env, \"BPF_ST stores into R%d %s is not allowed\\n\",\n \t\t\t\t\tinsn->dst_reg,\n-\t\t\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);\n+\t\t\t\t\treg_type_str(env, reg_state(env, insn->dst_reg)->type));\n \t\t\t\treturn -EACCES;\n \t\t\t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\treg_type_str(env, reg_state(env, insn->dst_reg)->type));"
            ],
            "deleted": [
                "\t\t\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "kernel/bpf/verifier.c in the Linux kernel through 5.15.14 allows local users to gain privileges because of the availability of pointer arithmetic via certain *_OR_NULL pointer types.",
        "id": 3447
    },
    {
        "cve_id": "CVE-2020-25639",
        "code_before_change": "int\nnouveau_channel_new(struct nouveau_drm *drm, struct nvif_device *device,\n\t\t    u32 arg0, u32 arg1, bool priv,\n\t\t    struct nouveau_channel **pchan)\n{\n\tstruct nouveau_cli *cli = (void *)device->object.client;\n\tbool super;\n\tint ret;\n\n\t/* hack until fencenv50 is fixed, and agp access relaxed */\n\tsuper = cli->base.super;\n\tcli->base.super = true;\n\n\tret = nouveau_channel_ind(drm, device, arg0, priv, pchan);\n\tif (ret) {\n\t\tNV_PRINTK(dbg, cli, \"ib channel create, %d\\n\", ret);\n\t\tret = nouveau_channel_dma(drm, device, pchan);\n\t\tif (ret) {\n\t\t\tNV_PRINTK(dbg, cli, \"dma channel create, %d\\n\", ret);\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tret = nouveau_channel_init(*pchan, arg0, arg1);\n\tif (ret) {\n\t\tNV_PRINTK(err, cli, \"channel failed to initialise, %d\\n\", ret);\n\t\tnouveau_channel_del(pchan);\n\t}\n\n\tret = nouveau_svmm_join((*pchan)->vmm->svmm, (*pchan)->inst);\n\tif (ret)\n\t\tnouveau_channel_del(pchan);\n\ndone:\n\tcli->base.super = super;\n\treturn ret;\n}",
        "code_after_change": "int\nnouveau_channel_new(struct nouveau_drm *drm, struct nvif_device *device,\n\t\t    u32 arg0, u32 arg1, bool priv,\n\t\t    struct nouveau_channel **pchan)\n{\n\tstruct nouveau_cli *cli = (void *)device->object.client;\n\tbool super;\n\tint ret;\n\n\t/* hack until fencenv50 is fixed, and agp access relaxed */\n\tsuper = cli->base.super;\n\tcli->base.super = true;\n\n\tret = nouveau_channel_ind(drm, device, arg0, priv, pchan);\n\tif (ret) {\n\t\tNV_PRINTK(dbg, cli, \"ib channel create, %d\\n\", ret);\n\t\tret = nouveau_channel_dma(drm, device, pchan);\n\t\tif (ret) {\n\t\t\tNV_PRINTK(dbg, cli, \"dma channel create, %d\\n\", ret);\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tret = nouveau_channel_init(*pchan, arg0, arg1);\n\tif (ret) {\n\t\tNV_PRINTK(err, cli, \"channel failed to initialise, %d\\n\", ret);\n\t\tnouveau_channel_del(pchan);\n\t\tgoto done;\n\t}\n\n\tret = nouveau_svmm_join((*pchan)->vmm->svmm, (*pchan)->inst);\n\tif (ret)\n\t\tnouveau_channel_del(pchan);\n\ndone:\n\tcli->base.super = super;\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,6 +25,7 @@\n \tif (ret) {\n \t\tNV_PRINTK(err, cli, \"channel failed to initialise, %d\\n\", ret);\n \t\tnouveau_channel_del(pchan);\n+\t\tgoto done;\n \t}\n \n \tret = nouveau_svmm_join((*pchan)->vmm->svmm, (*pchan)->inst);",
        "function_modified_lines": {
            "added": [
                "\t\tgoto done;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A NULL pointer dereference flaw was found in the Linux kernel's GPU Nouveau driver functionality in versions prior to 5.12-rc1 in the way the user calls ioctl DRM_IOCTL_NOUVEAU_CHANNEL_ALLOC. This flaw allows a local user to crash the system.",
        "id": 2588
    },
    {
        "cve_id": "CVE-2019-12818",
        "code_before_change": "static int nfc_llcp_build_gb(struct nfc_llcp_local *local)\n{\n\tu8 *gb_cur, *version_tlv, version, version_length;\n\tu8 *lto_tlv, lto_length;\n\tu8 *wks_tlv, wks_length;\n\tu8 *miux_tlv, miux_length;\n\t__be16 wks = cpu_to_be16(local->local_wks);\n\tu8 gb_len = 0;\n\tint ret = 0;\n\n\tversion = LLCP_VERSION_11;\n\tversion_tlv = nfc_llcp_build_tlv(LLCP_TLV_VERSION, &version,\n\t\t\t\t\t 1, &version_length);\n\tgb_len += version_length;\n\n\tlto_tlv = nfc_llcp_build_tlv(LLCP_TLV_LTO, &local->lto, 1, &lto_length);\n\tgb_len += lto_length;\n\n\tpr_debug(\"Local wks 0x%lx\\n\", local->local_wks);\n\twks_tlv = nfc_llcp_build_tlv(LLCP_TLV_WKS, (u8 *)&wks, 2, &wks_length);\n\tgb_len += wks_length;\n\n\tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&local->miux, 0,\n\t\t\t\t      &miux_length);\n\tgb_len += miux_length;\n\n\tgb_len += ARRAY_SIZE(llcp_magic);\n\n\tif (gb_len > NFC_MAX_GT_LEN) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tgb_cur = local->gb;\n\n\tmemcpy(gb_cur, llcp_magic, ARRAY_SIZE(llcp_magic));\n\tgb_cur += ARRAY_SIZE(llcp_magic);\n\n\tmemcpy(gb_cur, version_tlv, version_length);\n\tgb_cur += version_length;\n\n\tmemcpy(gb_cur, lto_tlv, lto_length);\n\tgb_cur += lto_length;\n\n\tmemcpy(gb_cur, wks_tlv, wks_length);\n\tgb_cur += wks_length;\n\n\tmemcpy(gb_cur, miux_tlv, miux_length);\n\tgb_cur += miux_length;\n\n\tlocal->gb_len = gb_len;\n\nout:\n\tkfree(version_tlv);\n\tkfree(lto_tlv);\n\tkfree(wks_tlv);\n\tkfree(miux_tlv);\n\n\treturn ret;\n}",
        "code_after_change": "static int nfc_llcp_build_gb(struct nfc_llcp_local *local)\n{\n\tu8 *gb_cur, version, version_length;\n\tu8 lto_length, wks_length, miux_length;\n\tu8 *version_tlv = NULL, *lto_tlv = NULL,\n\t   *wks_tlv = NULL, *miux_tlv = NULL;\n\t__be16 wks = cpu_to_be16(local->local_wks);\n\tu8 gb_len = 0;\n\tint ret = 0;\n\n\tversion = LLCP_VERSION_11;\n\tversion_tlv = nfc_llcp_build_tlv(LLCP_TLV_VERSION, &version,\n\t\t\t\t\t 1, &version_length);\n\tif (!version_tlv) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tgb_len += version_length;\n\n\tlto_tlv = nfc_llcp_build_tlv(LLCP_TLV_LTO, &local->lto, 1, &lto_length);\n\tif (!lto_tlv) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tgb_len += lto_length;\n\n\tpr_debug(\"Local wks 0x%lx\\n\", local->local_wks);\n\twks_tlv = nfc_llcp_build_tlv(LLCP_TLV_WKS, (u8 *)&wks, 2, &wks_length);\n\tif (!wks_tlv) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tgb_len += wks_length;\n\n\tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&local->miux, 0,\n\t\t\t\t      &miux_length);\n\tif (!miux_tlv) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tgb_len += miux_length;\n\n\tgb_len += ARRAY_SIZE(llcp_magic);\n\n\tif (gb_len > NFC_MAX_GT_LEN) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tgb_cur = local->gb;\n\n\tmemcpy(gb_cur, llcp_magic, ARRAY_SIZE(llcp_magic));\n\tgb_cur += ARRAY_SIZE(llcp_magic);\n\n\tmemcpy(gb_cur, version_tlv, version_length);\n\tgb_cur += version_length;\n\n\tmemcpy(gb_cur, lto_tlv, lto_length);\n\tgb_cur += lto_length;\n\n\tmemcpy(gb_cur, wks_tlv, wks_length);\n\tgb_cur += wks_length;\n\n\tmemcpy(gb_cur, miux_tlv, miux_length);\n\tgb_cur += miux_length;\n\n\tlocal->gb_len = gb_len;\n\nout:\n\tkfree(version_tlv);\n\tkfree(lto_tlv);\n\tkfree(wks_tlv);\n\tkfree(miux_tlv);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,9 +1,9 @@\n static int nfc_llcp_build_gb(struct nfc_llcp_local *local)\n {\n-\tu8 *gb_cur, *version_tlv, version, version_length;\n-\tu8 *lto_tlv, lto_length;\n-\tu8 *wks_tlv, wks_length;\n-\tu8 *miux_tlv, miux_length;\n+\tu8 *gb_cur, version, version_length;\n+\tu8 lto_length, wks_length, miux_length;\n+\tu8 *version_tlv = NULL, *lto_tlv = NULL,\n+\t   *wks_tlv = NULL, *miux_tlv = NULL;\n \t__be16 wks = cpu_to_be16(local->local_wks);\n \tu8 gb_len = 0;\n \tint ret = 0;\n@@ -11,17 +11,33 @@\n \tversion = LLCP_VERSION_11;\n \tversion_tlv = nfc_llcp_build_tlv(LLCP_TLV_VERSION, &version,\n \t\t\t\t\t 1, &version_length);\n+\tif (!version_tlv) {\n+\t\tret = -ENOMEM;\n+\t\tgoto out;\n+\t}\n \tgb_len += version_length;\n \n \tlto_tlv = nfc_llcp_build_tlv(LLCP_TLV_LTO, &local->lto, 1, &lto_length);\n+\tif (!lto_tlv) {\n+\t\tret = -ENOMEM;\n+\t\tgoto out;\n+\t}\n \tgb_len += lto_length;\n \n \tpr_debug(\"Local wks 0x%lx\\n\", local->local_wks);\n \twks_tlv = nfc_llcp_build_tlv(LLCP_TLV_WKS, (u8 *)&wks, 2, &wks_length);\n+\tif (!wks_tlv) {\n+\t\tret = -ENOMEM;\n+\t\tgoto out;\n+\t}\n \tgb_len += wks_length;\n \n \tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&local->miux, 0,\n \t\t\t\t      &miux_length);\n+\tif (!miux_tlv) {\n+\t\tret = -ENOMEM;\n+\t\tgoto out;\n+\t}\n \tgb_len += miux_length;\n \n \tgb_len += ARRAY_SIZE(llcp_magic);",
        "function_modified_lines": {
            "added": [
                "\tu8 *gb_cur, version, version_length;",
                "\tu8 lto_length, wks_length, miux_length;",
                "\tu8 *version_tlv = NULL, *lto_tlv = NULL,",
                "\t   *wks_tlv = NULL, *miux_tlv = NULL;",
                "\tif (!version_tlv) {",
                "\t\tret = -ENOMEM;",
                "\t\tgoto out;",
                "\t}",
                "\tif (!lto_tlv) {",
                "\t\tret = -ENOMEM;",
                "\t\tgoto out;",
                "\t}",
                "\tif (!wks_tlv) {",
                "\t\tret = -ENOMEM;",
                "\t\tgoto out;",
                "\t}",
                "\tif (!miux_tlv) {",
                "\t\tret = -ENOMEM;",
                "\t\tgoto out;",
                "\t}"
            ],
            "deleted": [
                "\tu8 *gb_cur, *version_tlv, version, version_length;",
                "\tu8 *lto_tlv, lto_length;",
                "\tu8 *wks_tlv, wks_length;",
                "\tu8 *miux_tlv, miux_length;"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.20.15. The nfc_llcp_build_tlv function in net/nfc/llcp_commands.c may return NULL. If the caller does not check for this, it will trigger a NULL pointer dereference. This will cause denial of service. This affects nfc_llcp_build_gb in net/nfc/llcp_core.c.",
        "id": 1953
    },
    {
        "cve_id": "CVE-2019-12818",
        "code_before_change": "int nfc_llcp_send_cc(struct nfc_llcp_sock *sock)\n{\n\tstruct nfc_llcp_local *local;\n\tstruct sk_buff *skb;\n\tu8 *miux_tlv = NULL, miux_tlv_length;\n\tu8 *rw_tlv = NULL, rw_tlv_length, rw;\n\tint err;\n\tu16 size = 0;\n\t__be16 miux;\n\n\tpr_debug(\"Sending CC\\n\");\n\n\tlocal = sock->local;\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\t/* If the socket parameters are not set, use the local ones */\n\tmiux = be16_to_cpu(sock->miux) > LLCP_MAX_MIUX ?\n\t\tlocal->miux : sock->miux;\n\trw = sock->rw > LLCP_MAX_RW ? local->rw : sock->rw;\n\n\tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,\n\t\t\t\t      &miux_tlv_length);\n\tsize += miux_tlv_length;\n\n\trw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);\n\tsize += rw_tlv_length;\n\n\tskb = llcp_allocate_pdu(sock, LLCP_PDU_CC, size);\n\tif (skb == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\n\tllcp_add_tlv(skb, miux_tlv, miux_tlv_length);\n\tllcp_add_tlv(skb, rw_tlv, rw_tlv_length);\n\n\tskb_queue_tail(&local->tx_queue, skb);\n\n\terr = 0;\n\nerror_tlv:\n\tif (err)\n\t\tpr_err(\"error %d\\n\", err);\n\n\tkfree(miux_tlv);\n\tkfree(rw_tlv);\n\n\treturn err;\n}",
        "code_after_change": "int nfc_llcp_send_cc(struct nfc_llcp_sock *sock)\n{\n\tstruct nfc_llcp_local *local;\n\tstruct sk_buff *skb;\n\tu8 *miux_tlv = NULL, miux_tlv_length;\n\tu8 *rw_tlv = NULL, rw_tlv_length, rw;\n\tint err;\n\tu16 size = 0;\n\t__be16 miux;\n\n\tpr_debug(\"Sending CC\\n\");\n\n\tlocal = sock->local;\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\t/* If the socket parameters are not set, use the local ones */\n\tmiux = be16_to_cpu(sock->miux) > LLCP_MAX_MIUX ?\n\t\tlocal->miux : sock->miux;\n\trw = sock->rw > LLCP_MAX_RW ? local->rw : sock->rw;\n\n\tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,\n\t\t\t\t      &miux_tlv_length);\n\tif (!miux_tlv) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\tsize += miux_tlv_length;\n\n\trw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);\n\tif (!rw_tlv) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\tsize += rw_tlv_length;\n\n\tskb = llcp_allocate_pdu(sock, LLCP_PDU_CC, size);\n\tif (skb == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\n\tllcp_add_tlv(skb, miux_tlv, miux_tlv_length);\n\tllcp_add_tlv(skb, rw_tlv, rw_tlv_length);\n\n\tskb_queue_tail(&local->tx_queue, skb);\n\n\terr = 0;\n\nerror_tlv:\n\tif (err)\n\t\tpr_err(\"error %d\\n\", err);\n\n\tkfree(miux_tlv);\n\tkfree(rw_tlv);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,9 +21,17 @@\n \n \tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,\n \t\t\t\t      &miux_tlv_length);\n+\tif (!miux_tlv) {\n+\t\terr = -ENOMEM;\n+\t\tgoto error_tlv;\n+\t}\n \tsize += miux_tlv_length;\n \n \trw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);\n+\tif (!rw_tlv) {\n+\t\terr = -ENOMEM;\n+\t\tgoto error_tlv;\n+\t}\n \tsize += rw_tlv_length;\n \n \tskb = llcp_allocate_pdu(sock, LLCP_PDU_CC, size);",
        "function_modified_lines": {
            "added": [
                "\tif (!miux_tlv) {",
                "\t\terr = -ENOMEM;",
                "\t\tgoto error_tlv;",
                "\t}",
                "\tif (!rw_tlv) {",
                "\t\terr = -ENOMEM;",
                "\t\tgoto error_tlv;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.20.15. The nfc_llcp_build_tlv function in net/nfc/llcp_commands.c may return NULL. If the caller does not check for this, it will trigger a NULL pointer dereference. This will cause denial of service. This affects nfc_llcp_build_gb in net/nfc/llcp_core.c.",
        "id": 1951
    },
    {
        "cve_id": "CVE-2017-16532",
        "code_before_change": "static int\nget_endpoints(struct usbtest_dev *dev, struct usb_interface *intf)\n{\n\tint\t\t\t\ttmp;\n\tstruct usb_host_interface\t*alt;\n\tstruct usb_host_endpoint\t*in, *out;\n\tstruct usb_host_endpoint\t*iso_in, *iso_out;\n\tstruct usb_host_endpoint\t*int_in, *int_out;\n\tstruct usb_device\t\t*udev;\n\n\tfor (tmp = 0; tmp < intf->num_altsetting; tmp++) {\n\t\tunsigned\tep;\n\n\t\tin = out = NULL;\n\t\tiso_in = iso_out = NULL;\n\t\tint_in = int_out = NULL;\n\t\talt = intf->altsetting + tmp;\n\n\t\tif (override_alt >= 0 &&\n\t\t\t\toverride_alt != alt->desc.bAlternateSetting)\n\t\t\tcontinue;\n\n\t\t/* take the first altsetting with in-bulk + out-bulk;\n\t\t * ignore other endpoints and altsettings.\n\t\t */\n\t\tfor (ep = 0; ep < alt->desc.bNumEndpoints; ep++) {\n\t\t\tstruct usb_host_endpoint\t*e;\n\t\t\tint edi;\n\n\t\t\te = alt->endpoint + ep;\n\t\t\tedi = usb_endpoint_dir_in(&e->desc);\n\n\t\t\tswitch (usb_endpoint_type(&e->desc)) {\n\t\t\tcase USB_ENDPOINT_XFER_BULK:\n\t\t\t\tendpoint_update(edi, &in, &out, e);\n\t\t\t\tcontinue;\n\t\t\tcase USB_ENDPOINT_XFER_INT:\n\t\t\t\tif (dev->info->intr)\n\t\t\t\t\tendpoint_update(edi, &int_in, &int_out, e);\n\t\t\t\tcontinue;\n\t\t\tcase USB_ENDPOINT_XFER_ISOC:\n\t\t\t\tif (dev->info->iso)\n\t\t\t\t\tendpoint_update(edi, &iso_in, &iso_out, e);\n\t\t\t\t/* FALLTHROUGH */\n\t\t\tdefault:\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif ((in && out)  ||  iso_in || iso_out || int_in || int_out)\n\t\t\tgoto found;\n\t}\n\treturn -EINVAL;\n\nfound:\n\tudev = testdev_to_usbdev(dev);\n\tdev->info->alt = alt->desc.bAlternateSetting;\n\tif (alt->desc.bAlternateSetting != 0) {\n\t\ttmp = usb_set_interface(udev,\n\t\t\t\talt->desc.bInterfaceNumber,\n\t\t\t\talt->desc.bAlternateSetting);\n\t\tif (tmp < 0)\n\t\t\treturn tmp;\n\t}\n\n\tif (in) {\n\t\tdev->in_pipe = usb_rcvbulkpipe(udev,\n\t\t\tin->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);\n\t\tdev->out_pipe = usb_sndbulkpipe(udev,\n\t\t\tout->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);\n\t}\n\tif (iso_in) {\n\t\tdev->iso_in = &iso_in->desc;\n\t\tdev->in_iso_pipe = usb_rcvisocpipe(udev,\n\t\t\t\tiso_in->desc.bEndpointAddress\n\t\t\t\t\t& USB_ENDPOINT_NUMBER_MASK);\n\t}\n\n\tif (iso_out) {\n\t\tdev->iso_out = &iso_out->desc;\n\t\tdev->out_iso_pipe = usb_sndisocpipe(udev,\n\t\t\t\tiso_out->desc.bEndpointAddress\n\t\t\t\t\t& USB_ENDPOINT_NUMBER_MASK);\n\t}\n\n\tif (int_in) {\n\t\tdev->int_in = &int_in->desc;\n\t\tdev->in_int_pipe = usb_rcvintpipe(udev,\n\t\t\t\tint_in->desc.bEndpointAddress\n\t\t\t\t\t& USB_ENDPOINT_NUMBER_MASK);\n\t}\n\n\tif (int_out) {\n\t\tdev->int_out = &int_out->desc;\n\t\tdev->out_int_pipe = usb_sndintpipe(udev,\n\t\t\t\tint_out->desc.bEndpointAddress\n\t\t\t\t\t& USB_ENDPOINT_NUMBER_MASK);\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int\nget_endpoints(struct usbtest_dev *dev, struct usb_interface *intf)\n{\n\tint\t\t\t\ttmp;\n\tstruct usb_host_interface\t*alt;\n\tstruct usb_host_endpoint\t*in, *out;\n\tstruct usb_host_endpoint\t*iso_in, *iso_out;\n\tstruct usb_host_endpoint\t*int_in, *int_out;\n\tstruct usb_device\t\t*udev;\n\n\tfor (tmp = 0; tmp < intf->num_altsetting; tmp++) {\n\t\tunsigned\tep;\n\n\t\tin = out = NULL;\n\t\tiso_in = iso_out = NULL;\n\t\tint_in = int_out = NULL;\n\t\talt = intf->altsetting + tmp;\n\n\t\tif (override_alt >= 0 &&\n\t\t\t\toverride_alt != alt->desc.bAlternateSetting)\n\t\t\tcontinue;\n\n\t\t/* take the first altsetting with in-bulk + out-bulk;\n\t\t * ignore other endpoints and altsettings.\n\t\t */\n\t\tfor (ep = 0; ep < alt->desc.bNumEndpoints; ep++) {\n\t\t\tstruct usb_host_endpoint\t*e;\n\t\t\tint edi;\n\n\t\t\te = alt->endpoint + ep;\n\t\t\tedi = usb_endpoint_dir_in(&e->desc);\n\n\t\t\tswitch (usb_endpoint_type(&e->desc)) {\n\t\t\tcase USB_ENDPOINT_XFER_BULK:\n\t\t\t\tendpoint_update(edi, &in, &out, e);\n\t\t\t\tcontinue;\n\t\t\tcase USB_ENDPOINT_XFER_INT:\n\t\t\t\tif (dev->info->intr)\n\t\t\t\t\tendpoint_update(edi, &int_in, &int_out, e);\n\t\t\t\tcontinue;\n\t\t\tcase USB_ENDPOINT_XFER_ISOC:\n\t\t\t\tif (dev->info->iso)\n\t\t\t\t\tendpoint_update(edi, &iso_in, &iso_out, e);\n\t\t\t\t/* FALLTHROUGH */\n\t\t\tdefault:\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif ((in && out)  ||  iso_in || iso_out || int_in || int_out)\n\t\t\tgoto found;\n\t}\n\treturn -EINVAL;\n\nfound:\n\tudev = testdev_to_usbdev(dev);\n\tdev->info->alt = alt->desc.bAlternateSetting;\n\tif (alt->desc.bAlternateSetting != 0) {\n\t\ttmp = usb_set_interface(udev,\n\t\t\t\talt->desc.bInterfaceNumber,\n\t\t\t\talt->desc.bAlternateSetting);\n\t\tif (tmp < 0)\n\t\t\treturn tmp;\n\t}\n\n\tif (in)\n\t\tdev->in_pipe = usb_rcvbulkpipe(udev,\n\t\t\tin->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);\n\tif (out)\n\t\tdev->out_pipe = usb_sndbulkpipe(udev,\n\t\t\tout->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);\n\n\tif (iso_in) {\n\t\tdev->iso_in = &iso_in->desc;\n\t\tdev->in_iso_pipe = usb_rcvisocpipe(udev,\n\t\t\t\tiso_in->desc.bEndpointAddress\n\t\t\t\t\t& USB_ENDPOINT_NUMBER_MASK);\n\t}\n\n\tif (iso_out) {\n\t\tdev->iso_out = &iso_out->desc;\n\t\tdev->out_iso_pipe = usb_sndisocpipe(udev,\n\t\t\t\tiso_out->desc.bEndpointAddress\n\t\t\t\t\t& USB_ENDPOINT_NUMBER_MASK);\n\t}\n\n\tif (int_in) {\n\t\tdev->int_in = &int_in->desc;\n\t\tdev->in_int_pipe = usb_rcvintpipe(udev,\n\t\t\t\tint_in->desc.bEndpointAddress\n\t\t\t\t\t& USB_ENDPOINT_NUMBER_MASK);\n\t}\n\n\tif (int_out) {\n\t\tdev->int_out = &int_out->desc;\n\t\tdev->out_int_pipe = usb_sndintpipe(udev,\n\t\t\t\tint_out->desc.bEndpointAddress\n\t\t\t\t\t& USB_ENDPOINT_NUMBER_MASK);\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -62,12 +62,13 @@\n \t\t\treturn tmp;\n \t}\n \n-\tif (in) {\n+\tif (in)\n \t\tdev->in_pipe = usb_rcvbulkpipe(udev,\n \t\t\tin->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);\n+\tif (out)\n \t\tdev->out_pipe = usb_sndbulkpipe(udev,\n \t\t\tout->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);\n-\t}\n+\n \tif (iso_in) {\n \t\tdev->iso_in = &iso_in->desc;\n \t\tdev->in_iso_pipe = usb_rcvisocpipe(udev,",
        "function_modified_lines": {
            "added": [
                "\tif (in)",
                "\tif (out)",
                ""
            ],
            "deleted": [
                "\tif (in) {",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The get_endpoints function in drivers/usb/misc/usbtest.c in the Linux kernel through 4.13.11 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact via a crafted USB device.",
        "id": 1319
    },
    {
        "cve_id": "CVE-2015-8956",
        "code_before_change": "static int rfcomm_sock_bind(struct socket *sock, struct sockaddr *addr, int addr_len)\n{\n\tstruct sockaddr_rc *sa = (struct sockaddr_rc *) addr;\n\tstruct sock *sk = sock->sk;\n\tint chan = sa->rc_channel;\n\tint err = 0;\n\n\tBT_DBG(\"sk %p %pMR\", sk, &sa->rc_bdaddr);\n\n\tif (!addr || addr->sa_family != AF_BLUETOOTH)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != BT_OPEN) {\n\t\terr = -EBADFD;\n\t\tgoto done;\n\t}\n\n\tif (sk->sk_type != SOCK_STREAM) {\n\t\terr = -EINVAL;\n\t\tgoto done;\n\t}\n\n\twrite_lock(&rfcomm_sk_list.lock);\n\n\tif (chan && __rfcomm_get_listen_sock_by_addr(chan, &sa->rc_bdaddr)) {\n\t\terr = -EADDRINUSE;\n\t} else {\n\t\t/* Save source address */\n\t\tbacpy(&rfcomm_pi(sk)->src, &sa->rc_bdaddr);\n\t\trfcomm_pi(sk)->channel = chan;\n\t\tsk->sk_state = BT_BOUND;\n\t}\n\n\twrite_unlock(&rfcomm_sk_list.lock);\n\ndone:\n\trelease_sock(sk);\n\treturn err;\n}",
        "code_after_change": "static int rfcomm_sock_bind(struct socket *sock, struct sockaddr *addr, int addr_len)\n{\n\tstruct sockaddr_rc sa;\n\tstruct sock *sk = sock->sk;\n\tint len, err = 0;\n\n\tif (!addr || addr->sa_family != AF_BLUETOOTH)\n\t\treturn -EINVAL;\n\n\tmemset(&sa, 0, sizeof(sa));\n\tlen = min_t(unsigned int, sizeof(sa), addr_len);\n\tmemcpy(&sa, addr, len);\n\n\tBT_DBG(\"sk %p %pMR\", sk, &sa.rc_bdaddr);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != BT_OPEN) {\n\t\terr = -EBADFD;\n\t\tgoto done;\n\t}\n\n\tif (sk->sk_type != SOCK_STREAM) {\n\t\terr = -EINVAL;\n\t\tgoto done;\n\t}\n\n\twrite_lock(&rfcomm_sk_list.lock);\n\n\tif (sa.rc_channel &&\n\t    __rfcomm_get_listen_sock_by_addr(sa.rc_channel, &sa.rc_bdaddr)) {\n\t\terr = -EADDRINUSE;\n\t} else {\n\t\t/* Save source address */\n\t\tbacpy(&rfcomm_pi(sk)->src, &sa.rc_bdaddr);\n\t\trfcomm_pi(sk)->channel = sa.rc_channel;\n\t\tsk->sk_state = BT_BOUND;\n\t}\n\n\twrite_unlock(&rfcomm_sk_list.lock);\n\ndone:\n\trelease_sock(sk);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,14 +1,17 @@\n static int rfcomm_sock_bind(struct socket *sock, struct sockaddr *addr, int addr_len)\n {\n-\tstruct sockaddr_rc *sa = (struct sockaddr_rc *) addr;\n+\tstruct sockaddr_rc sa;\n \tstruct sock *sk = sock->sk;\n-\tint chan = sa->rc_channel;\n-\tint err = 0;\n-\n-\tBT_DBG(\"sk %p %pMR\", sk, &sa->rc_bdaddr);\n+\tint len, err = 0;\n \n \tif (!addr || addr->sa_family != AF_BLUETOOTH)\n \t\treturn -EINVAL;\n+\n+\tmemset(&sa, 0, sizeof(sa));\n+\tlen = min_t(unsigned int, sizeof(sa), addr_len);\n+\tmemcpy(&sa, addr, len);\n+\n+\tBT_DBG(\"sk %p %pMR\", sk, &sa.rc_bdaddr);\n \n \tlock_sock(sk);\n \n@@ -24,12 +27,13 @@\n \n \twrite_lock(&rfcomm_sk_list.lock);\n \n-\tif (chan && __rfcomm_get_listen_sock_by_addr(chan, &sa->rc_bdaddr)) {\n+\tif (sa.rc_channel &&\n+\t    __rfcomm_get_listen_sock_by_addr(sa.rc_channel, &sa.rc_bdaddr)) {\n \t\terr = -EADDRINUSE;\n \t} else {\n \t\t/* Save source address */\n-\t\tbacpy(&rfcomm_pi(sk)->src, &sa->rc_bdaddr);\n-\t\trfcomm_pi(sk)->channel = chan;\n+\t\tbacpy(&rfcomm_pi(sk)->src, &sa.rc_bdaddr);\n+\t\trfcomm_pi(sk)->channel = sa.rc_channel;\n \t\tsk->sk_state = BT_BOUND;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct sockaddr_rc sa;",
                "\tint len, err = 0;",
                "",
                "\tmemset(&sa, 0, sizeof(sa));",
                "\tlen = min_t(unsigned int, sizeof(sa), addr_len);",
                "\tmemcpy(&sa, addr, len);",
                "",
                "\tBT_DBG(\"sk %p %pMR\", sk, &sa.rc_bdaddr);",
                "\tif (sa.rc_channel &&",
                "\t    __rfcomm_get_listen_sock_by_addr(sa.rc_channel, &sa.rc_bdaddr)) {",
                "\t\tbacpy(&rfcomm_pi(sk)->src, &sa.rc_bdaddr);",
                "\t\trfcomm_pi(sk)->channel = sa.rc_channel;"
            ],
            "deleted": [
                "\tstruct sockaddr_rc *sa = (struct sockaddr_rc *) addr;",
                "\tint chan = sa->rc_channel;",
                "\tint err = 0;",
                "",
                "\tBT_DBG(\"sk %p %pMR\", sk, &sa->rc_bdaddr);",
                "\tif (chan && __rfcomm_get_listen_sock_by_addr(chan, &sa->rc_bdaddr)) {",
                "\t\tbacpy(&rfcomm_pi(sk)->src, &sa->rc_bdaddr);",
                "\t\trfcomm_pi(sk)->channel = chan;"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The rfcomm_sock_bind function in net/bluetooth/rfcomm/sock.c in the Linux kernel before 4.2 allows local users to obtain sensitive information or cause a denial of service (NULL pointer dereference) via vectors involving a bind system call on a Bluetooth RFCOMM socket.",
        "id": 868
    },
    {
        "cve_id": "CVE-2023-3357",
        "code_before_change": "int amd_sfh_hid_client_init(struct amd_mp2_dev *privdata)\n{\n\tstruct amd_input_data *in_data = &privdata->in_data;\n\tstruct amdtp_cl_data *cl_data = privdata->cl_data;\n\tstruct amd_mp2_ops *mp2_ops = privdata->mp2_ops;\n\tstruct amd_mp2_sensor_info info;\n\tstruct request_list *req_list;\n\tstruct device *dev;\n\tu32 feature_report_size;\n\tu32 input_report_size;\n\tint rc, i, status;\n\tu8 cl_idx;\n\n\treq_list = &cl_data->req_list;\n\tdev = &privdata->pdev->dev;\n\tamd_sfh_set_desc_ops(mp2_ops);\n\n\tmp2_ops->suspend = amd_sfh_suspend;\n\tmp2_ops->resume = amd_sfh_resume;\n\n\tcl_data->num_hid_devices = amd_mp2_get_sensor_num(privdata, &cl_data->sensor_idx[0]);\n\tif (cl_data->num_hid_devices == 0)\n\t\treturn -ENODEV;\n\n\tINIT_DELAYED_WORK(&cl_data->work, amd_sfh_work);\n\tINIT_DELAYED_WORK(&cl_data->work_buffer, amd_sfh_work_buffer);\n\tINIT_LIST_HEAD(&req_list->list);\n\tcl_data->in_data = in_data;\n\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tin_data->sensor_virt_addr[i] = dma_alloc_coherent(dev, sizeof(int) * 8,\n\t\t\t\t\t\t\t\t  &cl_data->sensor_dma_addr[i],\n\t\t\t\t\t\t\t\t  GFP_KERNEL);\n\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\tcl_data->sensor_requested_cnt[i] = 0;\n\t\tcl_data->cur_hid_dev = i;\n\t\tcl_idx = cl_data->sensor_idx[i];\n\t\tcl_data->report_descr_sz[i] = mp2_ops->get_desc_sz(cl_idx, descr_size);\n\t\tif (!cl_data->report_descr_sz[i]) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tfeature_report_size = mp2_ops->get_desc_sz(cl_idx, feature_size);\n\t\tif (!feature_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinput_report_size =  mp2_ops->get_desc_sz(cl_idx, input_size);\n\t\tif (!input_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->feature_report[i] = devm_kzalloc(dev, feature_report_size, GFP_KERNEL);\n\t\tif (!cl_data->feature_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tin_data->input_report[i] = devm_kzalloc(dev, input_report_size, GFP_KERNEL);\n\t\tif (!in_data->input_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinfo.period = AMD_SFH_IDLE_LOOP;\n\t\tinfo.sensor_idx = cl_idx;\n\t\tinfo.dma_address = cl_data->sensor_dma_addr[i];\n\n\t\tcl_data->report_descr[i] =\n\t\t\tdevm_kzalloc(dev, cl_data->report_descr_sz[i], GFP_KERNEL);\n\t\tif (!cl_data->report_descr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\trc = mp2_ops->get_rep_desc(cl_idx, cl_data->report_descr[i]);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\tmp2_ops->start(privdata, info);\n\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_ENABLED);\n\t\tif (status == SENSOR_ENABLED) {\n\t\t\tcl_data->sensor_sts[i] = SENSOR_ENABLED;\n\t\t\trc = amdtp_hid_probe(cl_data->cur_hid_dev, cl_data);\n\t\t\tif (rc) {\n\t\t\t\tmp2_ops->stop(privdata, cl_data->sensor_idx[i]);\n\t\t\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_DISABLED);\n\t\t\t\tif (status != SENSOR_ENABLED)\n\t\t\t\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\t\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\t\t\tcl_data->sensor_idx[i],\n\t\t\t\t\tget_sensor_name(cl_data->sensor_idx[i]),\n\t\t\t\t\tcl_data->sensor_sts[i]);\n\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t}\n\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\tcl_data->sensor_idx[i], get_sensor_name(cl_data->sensor_idx[i]),\n\t\t\tcl_data->sensor_sts[i]);\n\t}\n\tif (mp2_ops->discovery_status && mp2_ops->discovery_status(privdata) == 0) {\n\t\tamd_sfh_hid_client_deinit(privdata);\n\t\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t\t}\n\t\tdev_warn(dev, \"Failed to discover, sensors not enabled\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tschedule_delayed_work(&cl_data->work_buffer, msecs_to_jiffies(AMD_SFH_IDLE_LOOP));\n\treturn 0;\n\ncleanup:\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tif (in_data->sensor_virt_addr[i]) {\n\t\t\tdma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int),\n\t\t\t\t\t  in_data->sensor_virt_addr[i],\n\t\t\t\t\t  cl_data->sensor_dma_addr[i]);\n\t\t}\n\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t}\n\treturn rc;\n}",
        "code_after_change": "int amd_sfh_hid_client_init(struct amd_mp2_dev *privdata)\n{\n\tstruct amd_input_data *in_data = &privdata->in_data;\n\tstruct amdtp_cl_data *cl_data = privdata->cl_data;\n\tstruct amd_mp2_ops *mp2_ops = privdata->mp2_ops;\n\tstruct amd_mp2_sensor_info info;\n\tstruct request_list *req_list;\n\tstruct device *dev;\n\tu32 feature_report_size;\n\tu32 input_report_size;\n\tint rc, i, status;\n\tu8 cl_idx;\n\n\treq_list = &cl_data->req_list;\n\tdev = &privdata->pdev->dev;\n\tamd_sfh_set_desc_ops(mp2_ops);\n\n\tmp2_ops->suspend = amd_sfh_suspend;\n\tmp2_ops->resume = amd_sfh_resume;\n\n\tcl_data->num_hid_devices = amd_mp2_get_sensor_num(privdata, &cl_data->sensor_idx[0]);\n\tif (cl_data->num_hid_devices == 0)\n\t\treturn -ENODEV;\n\n\tINIT_DELAYED_WORK(&cl_data->work, amd_sfh_work);\n\tINIT_DELAYED_WORK(&cl_data->work_buffer, amd_sfh_work_buffer);\n\tINIT_LIST_HEAD(&req_list->list);\n\tcl_data->in_data = in_data;\n\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tin_data->sensor_virt_addr[i] = dma_alloc_coherent(dev, sizeof(int) * 8,\n\t\t\t\t\t\t\t\t  &cl_data->sensor_dma_addr[i],\n\t\t\t\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!in_data->sensor_virt_addr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\tcl_data->sensor_requested_cnt[i] = 0;\n\t\tcl_data->cur_hid_dev = i;\n\t\tcl_idx = cl_data->sensor_idx[i];\n\t\tcl_data->report_descr_sz[i] = mp2_ops->get_desc_sz(cl_idx, descr_size);\n\t\tif (!cl_data->report_descr_sz[i]) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tfeature_report_size = mp2_ops->get_desc_sz(cl_idx, feature_size);\n\t\tif (!feature_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinput_report_size =  mp2_ops->get_desc_sz(cl_idx, input_size);\n\t\tif (!input_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->feature_report[i] = devm_kzalloc(dev, feature_report_size, GFP_KERNEL);\n\t\tif (!cl_data->feature_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tin_data->input_report[i] = devm_kzalloc(dev, input_report_size, GFP_KERNEL);\n\t\tif (!in_data->input_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinfo.period = AMD_SFH_IDLE_LOOP;\n\t\tinfo.sensor_idx = cl_idx;\n\t\tinfo.dma_address = cl_data->sensor_dma_addr[i];\n\n\t\tcl_data->report_descr[i] =\n\t\t\tdevm_kzalloc(dev, cl_data->report_descr_sz[i], GFP_KERNEL);\n\t\tif (!cl_data->report_descr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\trc = mp2_ops->get_rep_desc(cl_idx, cl_data->report_descr[i]);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\tmp2_ops->start(privdata, info);\n\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_ENABLED);\n\t\tif (status == SENSOR_ENABLED) {\n\t\t\tcl_data->sensor_sts[i] = SENSOR_ENABLED;\n\t\t\trc = amdtp_hid_probe(cl_data->cur_hid_dev, cl_data);\n\t\t\tif (rc) {\n\t\t\t\tmp2_ops->stop(privdata, cl_data->sensor_idx[i]);\n\t\t\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_DISABLED);\n\t\t\t\tif (status != SENSOR_ENABLED)\n\t\t\t\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\t\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\t\t\tcl_data->sensor_idx[i],\n\t\t\t\t\tget_sensor_name(cl_data->sensor_idx[i]),\n\t\t\t\t\tcl_data->sensor_sts[i]);\n\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t}\n\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\tcl_data->sensor_idx[i], get_sensor_name(cl_data->sensor_idx[i]),\n\t\t\tcl_data->sensor_sts[i]);\n\t}\n\tif (mp2_ops->discovery_status && mp2_ops->discovery_status(privdata) == 0) {\n\t\tamd_sfh_hid_client_deinit(privdata);\n\t\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t\t}\n\t\tdev_warn(dev, \"Failed to discover, sensors not enabled\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tschedule_delayed_work(&cl_data->work_buffer, msecs_to_jiffies(AMD_SFH_IDLE_LOOP));\n\treturn 0;\n\ncleanup:\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tif (in_data->sensor_virt_addr[i]) {\n\t\t\tdma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int),\n\t\t\t\t\t  in_data->sensor_virt_addr[i],\n\t\t\t\t\t  cl_data->sensor_dma_addr[i]);\n\t\t}\n\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t}\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,6 +31,10 @@\n \t\tin_data->sensor_virt_addr[i] = dma_alloc_coherent(dev, sizeof(int) * 8,\n \t\t\t\t\t\t\t\t  &cl_data->sensor_dma_addr[i],\n \t\t\t\t\t\t\t\t  GFP_KERNEL);\n+\t\tif (!in_data->sensor_virt_addr[i]) {\n+\t\t\trc = -ENOMEM;\n+\t\t\tgoto cleanup;\n+\t\t}\n \t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n \t\tcl_data->sensor_requested_cnt[i] = 0;\n \t\tcl_data->cur_hid_dev = i;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!in_data->sensor_virt_addr[i]) {",
                "\t\t\trc = -ENOMEM;",
                "\t\t\tgoto cleanup;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A NULL pointer dereference flaw was found in the Linux kernel AMD Sensor Fusion Hub driver. This flaw allows a local user to crash the system.",
        "id": 4061
    },
    {
        "cve_id": "CVE-2013-7339",
        "code_before_change": "static int rds_ib_laddr_check(__be32 addr)\n{\n\tint ret;\n\tstruct rdma_cm_id *cm_id;\n\tstruct sockaddr_in sin;\n\n\t/* Create a CMA ID and try to bind it. This catches both\n\t * IB and iWARP capable NICs.\n\t */\n\tcm_id = rdma_create_id(NULL, NULL, RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(cm_id))\n\t\treturn PTR_ERR(cm_id);\n\n\tmemset(&sin, 0, sizeof(sin));\n\tsin.sin_family = AF_INET;\n\tsin.sin_addr.s_addr = addr;\n\n\t/* rdma_bind_addr will only succeed for IB & iWARP devices */\n\tret = rdma_bind_addr(cm_id, (struct sockaddr *)&sin);\n\t/* due to this, we will claim to support iWARP devices unless we\n\t   check node_type. */\n\tif (ret || cm_id->device->node_type != RDMA_NODE_IB_CA)\n\t\tret = -EADDRNOTAVAIL;\n\n\trdsdebug(\"addr %pI4 ret %d node type %d\\n\",\n\t\t&addr, ret,\n\t\tcm_id->device ? cm_id->device->node_type : -1);\n\n\trdma_destroy_id(cm_id);\n\n\treturn ret;\n}",
        "code_after_change": "static int rds_ib_laddr_check(__be32 addr)\n{\n\tint ret;\n\tstruct rdma_cm_id *cm_id;\n\tstruct sockaddr_in sin;\n\n\t/* Create a CMA ID and try to bind it. This catches both\n\t * IB and iWARP capable NICs.\n\t */\n\tcm_id = rdma_create_id(NULL, NULL, RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(cm_id))\n\t\treturn PTR_ERR(cm_id);\n\n\tmemset(&sin, 0, sizeof(sin));\n\tsin.sin_family = AF_INET;\n\tsin.sin_addr.s_addr = addr;\n\n\t/* rdma_bind_addr will only succeed for IB & iWARP devices */\n\tret = rdma_bind_addr(cm_id, (struct sockaddr *)&sin);\n\t/* due to this, we will claim to support iWARP devices unless we\n\t   check node_type. */\n\tif (ret || !cm_id->device ||\n\t    cm_id->device->node_type != RDMA_NODE_IB_CA)\n\t\tret = -EADDRNOTAVAIL;\n\n\trdsdebug(\"addr %pI4 ret %d node type %d\\n\",\n\t\t&addr, ret,\n\t\tcm_id->device ? cm_id->device->node_type : -1);\n\n\trdma_destroy_id(cm_id);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,8 @@\n \tret = rdma_bind_addr(cm_id, (struct sockaddr *)&sin);\n \t/* due to this, we will claim to support iWARP devices unless we\n \t   check node_type. */\n-\tif (ret || cm_id->device->node_type != RDMA_NODE_IB_CA)\n+\tif (ret || !cm_id->device ||\n+\t    cm_id->device->node_type != RDMA_NODE_IB_CA)\n \t\tret = -EADDRNOTAVAIL;\n \n \trdsdebug(\"addr %pI4 ret %d node type %d\\n\",",
        "function_modified_lines": {
            "added": [
                "\tif (ret || !cm_id->device ||",
                "\t    cm_id->device->node_type != RDMA_NODE_IB_CA)"
            ],
            "deleted": [
                "\tif (ret || cm_id->device->node_type != RDMA_NODE_IB_CA)"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The rds_ib_laddr_check function in net/rds/ib.c in the Linux kernel before 3.12.8 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact via a bind system call for an RDS socket on a system that lacks RDS transports.",
        "id": 414
    },
    {
        "cve_id": "CVE-2019-15223",
        "code_before_change": "static int toneport_setup(struct usb_line6_toneport *toneport)\n{\n\tu32 *ticks;\n\tstruct usb_line6 *line6 = &toneport->line6;\n\tstruct usb_device *usbdev = line6->usbdev;\n\n\tticks = kmalloc(sizeof(*ticks), GFP_KERNEL);\n\tif (!ticks)\n\t\treturn -ENOMEM;\n\n\t/* sync time on device with host: */\n\t/* note: 32-bit timestamps overflow in year 2106 */\n\t*ticks = (u32)ktime_get_real_seconds();\n\tline6_write_data(line6, 0x80c6, ticks, 4);\n\tkfree(ticks);\n\n\t/* enable device: */\n\ttoneport_send_cmd(usbdev, 0x0301, 0x0000);\n\n\t/* initialize source select: */\n\tif (toneport_has_source_select(toneport))\n\t\ttoneport_send_cmd(usbdev,\n\t\t\t\t  toneport_source_info[toneport->source].code,\n\t\t\t\t  0x0000);\n\n\tif (toneport_has_led(toneport))\n\t\ttoneport_update_led(toneport);\n\n\tschedule_delayed_work(&toneport->pcm_work,\n\t\t\t      msecs_to_jiffies(TONEPORT_PCM_DELAY * 1000));\n\treturn 0;\n}",
        "code_after_change": "static int toneport_setup(struct usb_line6_toneport *toneport)\n{\n\tu32 *ticks;\n\tstruct usb_line6 *line6 = &toneport->line6;\n\tstruct usb_device *usbdev = line6->usbdev;\n\n\tticks = kmalloc(sizeof(*ticks), GFP_KERNEL);\n\tif (!ticks)\n\t\treturn -ENOMEM;\n\n\t/* sync time on device with host: */\n\t/* note: 32-bit timestamps overflow in year 2106 */\n\t*ticks = (u32)ktime_get_real_seconds();\n\tline6_write_data(line6, 0x80c6, ticks, 4);\n\tkfree(ticks);\n\n\t/* enable device: */\n\ttoneport_send_cmd(usbdev, 0x0301, 0x0000);\n\n\t/* initialize source select: */\n\tif (toneport_has_source_select(toneport))\n\t\ttoneport_send_cmd(usbdev,\n\t\t\t\t  toneport_source_info[toneport->source].code,\n\t\t\t\t  0x0000);\n\n\tif (toneport_has_led(toneport))\n\t\ttoneport_update_led(toneport);\n\n\tschedule_delayed_work(&toneport->line6.startup_work,\n\t\t\t      msecs_to_jiffies(TONEPORT_PCM_DELAY * 1000));\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,7 +26,7 @@\n \tif (toneport_has_led(toneport))\n \t\ttoneport_update_led(toneport);\n \n-\tschedule_delayed_work(&toneport->pcm_work,\n+\tschedule_delayed_work(&toneport->line6.startup_work,\n \t\t\t      msecs_to_jiffies(TONEPORT_PCM_DELAY * 1000));\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tschedule_delayed_work(&toneport->line6.startup_work,"
            ],
            "deleted": [
                "\tschedule_delayed_work(&toneport->pcm_work,"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.1.8. There is a NULL pointer dereference caused by a malicious USB device in the sound/usb/line6/driver.c driver.",
        "id": 2012
    },
    {
        "cve_id": "CVE-2023-3772",
        "code_before_change": "static void xfrm_update_ae_params(struct xfrm_state *x, struct nlattr **attrs,\n\t\t\t\t  int update_esn)\n{\n\tstruct nlattr *rp = attrs[XFRMA_REPLAY_VAL];\n\tstruct nlattr *re = update_esn ? attrs[XFRMA_REPLAY_ESN_VAL] : NULL;\n\tstruct nlattr *lt = attrs[XFRMA_LTIME_VAL];\n\tstruct nlattr *et = attrs[XFRMA_ETIMER_THRESH];\n\tstruct nlattr *rt = attrs[XFRMA_REPLAY_THRESH];\n\tstruct nlattr *mt = attrs[XFRMA_MTIMER_THRESH];\n\n\tif (re) {\n\t\tstruct xfrm_replay_state_esn *replay_esn;\n\t\treplay_esn = nla_data(re);\n\t\tmemcpy(x->replay_esn, replay_esn,\n\t\t       xfrm_replay_state_esn_len(replay_esn));\n\t\tmemcpy(x->preplay_esn, replay_esn,\n\t\t       xfrm_replay_state_esn_len(replay_esn));\n\t}\n\n\tif (rp) {\n\t\tstruct xfrm_replay_state *replay;\n\t\treplay = nla_data(rp);\n\t\tmemcpy(&x->replay, replay, sizeof(*replay));\n\t\tmemcpy(&x->preplay, replay, sizeof(*replay));\n\t}\n\n\tif (lt) {\n\t\tstruct xfrm_lifetime_cur *ltime;\n\t\tltime = nla_data(lt);\n\t\tx->curlft.bytes = ltime->bytes;\n\t\tx->curlft.packets = ltime->packets;\n\t\tx->curlft.add_time = ltime->add_time;\n\t\tx->curlft.use_time = ltime->use_time;\n\t}\n\n\tif (et)\n\t\tx->replay_maxage = nla_get_u32(et);\n\n\tif (rt)\n\t\tx->replay_maxdiff = nla_get_u32(rt);\n\n\tif (mt)\n\t\tx->mapping_maxage = nla_get_u32(mt);\n}",
        "code_after_change": "static void xfrm_update_ae_params(struct xfrm_state *x, struct nlattr **attrs,\n\t\t\t\t  int update_esn)\n{\n\tstruct nlattr *rp = attrs[XFRMA_REPLAY_VAL];\n\tstruct nlattr *re = update_esn ? attrs[XFRMA_REPLAY_ESN_VAL] : NULL;\n\tstruct nlattr *lt = attrs[XFRMA_LTIME_VAL];\n\tstruct nlattr *et = attrs[XFRMA_ETIMER_THRESH];\n\tstruct nlattr *rt = attrs[XFRMA_REPLAY_THRESH];\n\tstruct nlattr *mt = attrs[XFRMA_MTIMER_THRESH];\n\n\tif (re && x->replay_esn && x->preplay_esn) {\n\t\tstruct xfrm_replay_state_esn *replay_esn;\n\t\treplay_esn = nla_data(re);\n\t\tmemcpy(x->replay_esn, replay_esn,\n\t\t       xfrm_replay_state_esn_len(replay_esn));\n\t\tmemcpy(x->preplay_esn, replay_esn,\n\t\t       xfrm_replay_state_esn_len(replay_esn));\n\t}\n\n\tif (rp) {\n\t\tstruct xfrm_replay_state *replay;\n\t\treplay = nla_data(rp);\n\t\tmemcpy(&x->replay, replay, sizeof(*replay));\n\t\tmemcpy(&x->preplay, replay, sizeof(*replay));\n\t}\n\n\tif (lt) {\n\t\tstruct xfrm_lifetime_cur *ltime;\n\t\tltime = nla_data(lt);\n\t\tx->curlft.bytes = ltime->bytes;\n\t\tx->curlft.packets = ltime->packets;\n\t\tx->curlft.add_time = ltime->add_time;\n\t\tx->curlft.use_time = ltime->use_time;\n\t}\n\n\tif (et)\n\t\tx->replay_maxage = nla_get_u32(et);\n\n\tif (rt)\n\t\tx->replay_maxdiff = nla_get_u32(rt);\n\n\tif (mt)\n\t\tx->mapping_maxage = nla_get_u32(mt);\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \tstruct nlattr *rt = attrs[XFRMA_REPLAY_THRESH];\n \tstruct nlattr *mt = attrs[XFRMA_MTIMER_THRESH];\n \n-\tif (re) {\n+\tif (re && x->replay_esn && x->preplay_esn) {\n \t\tstruct xfrm_replay_state_esn *replay_esn;\n \t\treplay_esn = nla_data(re);\n \t\tmemcpy(x->replay_esn, replay_esn,",
        "function_modified_lines": {
            "added": [
                "\tif (re && x->replay_esn && x->preplay_esn) {"
            ],
            "deleted": [
                "\tif (re) {"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s IP framework for transforming packets (XFRM subsystem). This issue may allow a malicious user with CAP_NET_ADMIN privileges to directly dereference a NULL pointer in xfrm_update_ae_params(), leading to a possible kernel crash and denial of service.",
        "id": 4134
    },
    {
        "cve_id": "CVE-2020-11608",
        "code_before_change": "static void ov518_mode_init_regs(struct sd *sd)\n{\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tint hsegs, vsegs, packet_size;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\tov518_reg_w32(sd, R51x_FIFO_PSIZE, packet_size & ~7, 2);\n\n\t/******** Set the mode ********/\n\treg_w(sd, 0x2b, 0);\n\treg_w(sd, 0x2c, 0);\n\treg_w(sd, 0x2d, 0);\n\treg_w(sd, 0x2e, 0);\n\treg_w(sd, 0x3b, 0);\n\treg_w(sd, 0x3c, 0);\n\treg_w(sd, 0x3d, 0);\n\treg_w(sd, 0x3e, 0);\n\n\tif (sd->bridge == BRIDGE_OV518) {\n\t\t/* Set 8-bit (YVYU) input format */\n\t\treg_w_mask(sd, 0x20, 0x08, 0x08);\n\n\t\t/* Set 12-bit (4:2:0) output format */\n\t\treg_w_mask(sd, 0x28, 0x80, 0xf0);\n\t\treg_w_mask(sd, 0x38, 0x80, 0xf0);\n\t} else {\n\t\treg_w(sd, 0x28, 0x80);\n\t\treg_w(sd, 0x38, 0x80);\n\t}\n\n\thsegs = sd->gspca_dev.pixfmt.width / 16;\n\tvsegs = sd->gspca_dev.pixfmt.height / 4;\n\n\treg_w(sd, 0x29, hsegs);\n\treg_w(sd, 0x2a, vsegs);\n\n\treg_w(sd, 0x39, hsegs);\n\treg_w(sd, 0x3a, vsegs);\n\n\t/* Windows driver does this here; who knows why */\n\treg_w(sd, 0x2f, 0x80);\n\n\t/******** Set the framerate ********/\n\tif (sd->bridge == BRIDGE_OV518PLUS && sd->revision == 0 &&\n\t\t\t\t\t      sd->sensor == SEN_OV7620AE)\n\t\tsd->clockdiv = 0;\n\telse\n\t\tsd->clockdiv = 1;\n\n\t/* Mode independent, but framerate dependent, regs */\n\t/* 0x51: Clock divider; Only works on some cams which use 2 crystals */\n\treg_w(sd, 0x51, 0x04);\n\treg_w(sd, 0x22, 0x18);\n\treg_w(sd, 0x23, 0xff);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\tswitch (sd->sensor) {\n\t\tcase SEN_OV7620AE:\n\t\t\t/*\n\t\t\t * HdG: 640x480 needs special handling on device\n\t\t\t * revision 2, we check for device revision > 0 to\n\t\t\t * avoid regressions, as we don't know the correct\n\t\t\t * thing todo for revision 1.\n\t\t\t *\n\t\t\t * Also this likely means we don't need to\n\t\t\t * differentiate between the OV7620 and OV7620AE,\n\t\t\t * earlier testing hitting this same problem likely\n\t\t\t * happened to be with revision < 2 cams using an\n\t\t\t * OV7620 and revision 2 cams using an OV7620AE.\n\t\t\t */\n\t\t\tif (sd->revision > 0 &&\n\t\t\t\t\tsd->gspca_dev.pixfmt.width == 640) {\n\t\t\t\treg_w(sd, 0x20, 0x60);\n\t\t\t\treg_w(sd, 0x21, 0x1f);\n\t\t\t} else {\n\t\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SEN_OV7620:\n\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t}\n\t} else\n\t\treg_w(sd, 0x71, 0x17);\t/* Compression-related? */\n\n\t/* FIXME: Sensor-specific */\n\t/* Bit 5 is what matters here. Of course, it is \"reserved\" */\n\ti2c_w(sd, 0x54, 0x23);\n\n\treg_w(sd, 0x2f, 0x80);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\treg_w(sd, 0x24, 0x94);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc7,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc8,    108, 2);\t/* 6ch    */\n\t\tov518_reg_w32(sd, 0xca, 131098, 3);\t/* 2001ah */\n\t\tov518_reg_w32(sd, 0xcb,    532, 2);\t/* 214h   */\n\t\tov518_reg_w32(sd, 0xcc,   2400, 2);\t/* 960h   */\n\t\tov518_reg_w32(sd, 0xcd,     32, 2);\t/* 20h    */\n\t\tov518_reg_w32(sd, 0xce,    608, 2);\t/* 260h   */\n\t} else {\n\t\treg_w(sd, 0x24, 0x9f);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc7,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc8,    128, 2);\t/* 80h    */\n\t\tov518_reg_w32(sd, 0xca, 183331, 3);\t/* 2cc23h */\n\t\tov518_reg_w32(sd, 0xcb,    746, 2);\t/* 2eah   */\n\t\tov518_reg_w32(sd, 0xcc,   1750, 2);\t/* 6d6h   */\n\t\tov518_reg_w32(sd, 0xcd,     45, 2);\t/* 2dh    */\n\t\tov518_reg_w32(sd, 0xce,    851, 2);\t/* 353h   */\n\t}\n\n\treg_w(sd, 0x2f, 0x80);\n}",
        "code_after_change": "static void ov518_mode_init_regs(struct sd *sd)\n{\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tint hsegs, vsegs, packet_size;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1) {\n\t\tsd->gspca_dev.usb_err = -ENODEV;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\tov518_reg_w32(sd, R51x_FIFO_PSIZE, packet_size & ~7, 2);\n\n\t/******** Set the mode ********/\n\treg_w(sd, 0x2b, 0);\n\treg_w(sd, 0x2c, 0);\n\treg_w(sd, 0x2d, 0);\n\treg_w(sd, 0x2e, 0);\n\treg_w(sd, 0x3b, 0);\n\treg_w(sd, 0x3c, 0);\n\treg_w(sd, 0x3d, 0);\n\treg_w(sd, 0x3e, 0);\n\n\tif (sd->bridge == BRIDGE_OV518) {\n\t\t/* Set 8-bit (YVYU) input format */\n\t\treg_w_mask(sd, 0x20, 0x08, 0x08);\n\n\t\t/* Set 12-bit (4:2:0) output format */\n\t\treg_w_mask(sd, 0x28, 0x80, 0xf0);\n\t\treg_w_mask(sd, 0x38, 0x80, 0xf0);\n\t} else {\n\t\treg_w(sd, 0x28, 0x80);\n\t\treg_w(sd, 0x38, 0x80);\n\t}\n\n\thsegs = sd->gspca_dev.pixfmt.width / 16;\n\tvsegs = sd->gspca_dev.pixfmt.height / 4;\n\n\treg_w(sd, 0x29, hsegs);\n\treg_w(sd, 0x2a, vsegs);\n\n\treg_w(sd, 0x39, hsegs);\n\treg_w(sd, 0x3a, vsegs);\n\n\t/* Windows driver does this here; who knows why */\n\treg_w(sd, 0x2f, 0x80);\n\n\t/******** Set the framerate ********/\n\tif (sd->bridge == BRIDGE_OV518PLUS && sd->revision == 0 &&\n\t\t\t\t\t      sd->sensor == SEN_OV7620AE)\n\t\tsd->clockdiv = 0;\n\telse\n\t\tsd->clockdiv = 1;\n\n\t/* Mode independent, but framerate dependent, regs */\n\t/* 0x51: Clock divider; Only works on some cams which use 2 crystals */\n\treg_w(sd, 0x51, 0x04);\n\treg_w(sd, 0x22, 0x18);\n\treg_w(sd, 0x23, 0xff);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\tswitch (sd->sensor) {\n\t\tcase SEN_OV7620AE:\n\t\t\t/*\n\t\t\t * HdG: 640x480 needs special handling on device\n\t\t\t * revision 2, we check for device revision > 0 to\n\t\t\t * avoid regressions, as we don't know the correct\n\t\t\t * thing todo for revision 1.\n\t\t\t *\n\t\t\t * Also this likely means we don't need to\n\t\t\t * differentiate between the OV7620 and OV7620AE,\n\t\t\t * earlier testing hitting this same problem likely\n\t\t\t * happened to be with revision < 2 cams using an\n\t\t\t * OV7620 and revision 2 cams using an OV7620AE.\n\t\t\t */\n\t\t\tif (sd->revision > 0 &&\n\t\t\t\t\tsd->gspca_dev.pixfmt.width == 640) {\n\t\t\t\treg_w(sd, 0x20, 0x60);\n\t\t\t\treg_w(sd, 0x21, 0x1f);\n\t\t\t} else {\n\t\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SEN_OV7620:\n\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t}\n\t} else\n\t\treg_w(sd, 0x71, 0x17);\t/* Compression-related? */\n\n\t/* FIXME: Sensor-specific */\n\t/* Bit 5 is what matters here. Of course, it is \"reserved\" */\n\ti2c_w(sd, 0x54, 0x23);\n\n\treg_w(sd, 0x2f, 0x80);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\treg_w(sd, 0x24, 0x94);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc7,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc8,    108, 2);\t/* 6ch    */\n\t\tov518_reg_w32(sd, 0xca, 131098, 3);\t/* 2001ah */\n\t\tov518_reg_w32(sd, 0xcb,    532, 2);\t/* 214h   */\n\t\tov518_reg_w32(sd, 0xcc,   2400, 2);\t/* 960h   */\n\t\tov518_reg_w32(sd, 0xcd,     32, 2);\t/* 20h    */\n\t\tov518_reg_w32(sd, 0xce,    608, 2);\t/* 260h   */\n\t} else {\n\t\treg_w(sd, 0x24, 0x9f);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc7,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc8,    128, 2);\t/* 80h    */\n\t\tov518_reg_w32(sd, 0xca, 183331, 3);\t/* 2cc23h */\n\t\tov518_reg_w32(sd, 0xcb,    746, 2);\t/* 2eah   */\n\t\tov518_reg_w32(sd, 0xcc,   1750, 2);\t/* 6d6h   */\n\t\tov518_reg_w32(sd, 0xcd,     45, 2);\t/* 2dh    */\n\t\tov518_reg_w32(sd, 0xce,    851, 2);\t/* 353h   */\n\t}\n\n\treg_w(sd, 0x2f, 0x80);\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,11 @@\n \tif (!alt) {\n \t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n \t\tsd->gspca_dev.usb_err = -EIO;\n+\t\treturn;\n+\t}\n+\n+\tif (alt->desc.bNumEndpoints < 1) {\n+\t\tsd->gspca_dev.usb_err = -ENODEV;\n \t\treturn;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\treturn;",
                "\t}",
                "",
                "\tif (alt->desc.bNumEndpoints < 1) {",
                "\t\tsd->gspca_dev.usb_err = -ENODEV;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.6.1. drivers/media/usb/gspca/ov519.c allows NULL pointer dereferences in ov511_mode_init_regs and ov518_mode_init_regs when there are zero endpoints, aka CID-998912346c0d.",
        "id": 2427
    },
    {
        "cve_id": "CVE-2020-11608",
        "code_before_change": "static void ov511_mode_init_regs(struct sd *sd)\n{\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tint hsegs, vsegs, packet_size, fps, needed;\n\tint interlaced = 0;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\treg_w(sd, R51x_FIFO_PSIZE, packet_size >> 5);\n\n\treg_w(sd, R511_CAM_UV_EN, 0x01);\n\treg_w(sd, R511_SNAP_UV_EN, 0x01);\n\treg_w(sd, R511_SNAP_OPTS, 0x03);\n\n\t/* Here I'm assuming that snapshot size == image size.\n\t * I hope that's always true. --claudio\n\t */\n\thsegs = (sd->gspca_dev.pixfmt.width >> 3) - 1;\n\tvsegs = (sd->gspca_dev.pixfmt.height >> 3) - 1;\n\n\treg_w(sd, R511_CAM_PXCNT, hsegs);\n\treg_w(sd, R511_CAM_LNCNT, vsegs);\n\treg_w(sd, R511_CAM_PXDIV, 0x00);\n\treg_w(sd, R511_CAM_LNDIV, 0x00);\n\n\t/* YUV420, low pass filter on */\n\treg_w(sd, R511_CAM_OPTS, 0x03);\n\n\t/* Snapshot additions */\n\treg_w(sd, R511_SNAP_PXCNT, hsegs);\n\treg_w(sd, R511_SNAP_LNCNT, vsegs);\n\treg_w(sd, R511_SNAP_PXDIV, 0x00);\n\treg_w(sd, R511_SNAP_LNDIV, 0x00);\n\n\t/******** Set the framerate ********/\n\tif (frame_rate > 0)\n\t\tsd->frame_rate = frame_rate;\n\n\tswitch (sd->sensor) {\n\tcase SEN_OV6620:\n\t\t/* No framerate control, doesn't like higher rates yet */\n\t\tsd->clockdiv = 3;\n\t\tbreak;\n\n\t/* Note once the FIXME's in mode_init_ov_sensor_regs() are fixed\n\t   for more sensors we need to do this for them too */\n\tcase SEN_OV7620:\n\tcase SEN_OV7620AE:\n\tcase SEN_OV7640:\n\tcase SEN_OV7648:\n\tcase SEN_OV76BE:\n\t\tif (sd->gspca_dev.pixfmt.width == 320)\n\t\t\tinterlaced = 1;\n\t\t/* Fall through */\n\tcase SEN_OV6630:\n\tcase SEN_OV7610:\n\tcase SEN_OV7670:\n\t\tswitch (sd->frame_rate) {\n\t\tcase 30:\n\t\tcase 25:\n\t\t\t/* Not enough bandwidth to do 640x480 @ 30 fps */\n\t\t\tif (sd->gspca_dev.pixfmt.width != 640) {\n\t\t\t\tsd->clockdiv = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* For 640x480 case */\n\t\t\t/* fall through */\n\t\tdefault:\n/*\t\tcase 20: */\n/*\t\tcase 15: */\n\t\t\tsd->clockdiv = 1;\n\t\t\tbreak;\n\t\tcase 10:\n\t\t\tsd->clockdiv = 2;\n\t\t\tbreak;\n\t\tcase 5:\n\t\t\tsd->clockdiv = 5;\n\t\t\tbreak;\n\t\t}\n\t\tif (interlaced) {\n\t\t\tsd->clockdiv = (sd->clockdiv + 1) * 2 - 1;\n\t\t\t/* Higher then 10 does not work */\n\t\t\tif (sd->clockdiv > 10)\n\t\t\t\tsd->clockdiv = 10;\n\t\t}\n\t\tbreak;\n\n\tcase SEN_OV8610:\n\t\t/* No framerate control ?? */\n\t\tsd->clockdiv = 0;\n\t\tbreak;\n\t}\n\n\t/* Check if we have enough bandwidth to disable compression */\n\tfps = (interlaced ? 60 : 30) / (sd->clockdiv + 1) + 1;\n\tneeded = fps * sd->gspca_dev.pixfmt.width *\n\t\t\tsd->gspca_dev.pixfmt.height * 3 / 2;\n\t/* 1000 isoc packets/sec */\n\tif (needed > 1000 * packet_size) {\n\t\t/* Enable Y and UV quantization and compression */\n\t\treg_w(sd, R511_COMP_EN, 0x07);\n\t\treg_w(sd, R511_COMP_LUT_EN, 0x03);\n\t} else {\n\t\treg_w(sd, R511_COMP_EN, 0x06);\n\t\treg_w(sd, R511_COMP_LUT_EN, 0x00);\n\t}\n\n\treg_w(sd, R51x_SYS_RESET, OV511_RESET_OMNICE);\n\treg_w(sd, R51x_SYS_RESET, 0);\n}",
        "code_after_change": "static void ov511_mode_init_regs(struct sd *sd)\n{\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tint hsegs, vsegs, packet_size, fps, needed;\n\tint interlaced = 0;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1) {\n\t\tsd->gspca_dev.usb_err = -ENODEV;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\treg_w(sd, R51x_FIFO_PSIZE, packet_size >> 5);\n\n\treg_w(sd, R511_CAM_UV_EN, 0x01);\n\treg_w(sd, R511_SNAP_UV_EN, 0x01);\n\treg_w(sd, R511_SNAP_OPTS, 0x03);\n\n\t/* Here I'm assuming that snapshot size == image size.\n\t * I hope that's always true. --claudio\n\t */\n\thsegs = (sd->gspca_dev.pixfmt.width >> 3) - 1;\n\tvsegs = (sd->gspca_dev.pixfmt.height >> 3) - 1;\n\n\treg_w(sd, R511_CAM_PXCNT, hsegs);\n\treg_w(sd, R511_CAM_LNCNT, vsegs);\n\treg_w(sd, R511_CAM_PXDIV, 0x00);\n\treg_w(sd, R511_CAM_LNDIV, 0x00);\n\n\t/* YUV420, low pass filter on */\n\treg_w(sd, R511_CAM_OPTS, 0x03);\n\n\t/* Snapshot additions */\n\treg_w(sd, R511_SNAP_PXCNT, hsegs);\n\treg_w(sd, R511_SNAP_LNCNT, vsegs);\n\treg_w(sd, R511_SNAP_PXDIV, 0x00);\n\treg_w(sd, R511_SNAP_LNDIV, 0x00);\n\n\t/******** Set the framerate ********/\n\tif (frame_rate > 0)\n\t\tsd->frame_rate = frame_rate;\n\n\tswitch (sd->sensor) {\n\tcase SEN_OV6620:\n\t\t/* No framerate control, doesn't like higher rates yet */\n\t\tsd->clockdiv = 3;\n\t\tbreak;\n\n\t/* Note once the FIXME's in mode_init_ov_sensor_regs() are fixed\n\t   for more sensors we need to do this for them too */\n\tcase SEN_OV7620:\n\tcase SEN_OV7620AE:\n\tcase SEN_OV7640:\n\tcase SEN_OV7648:\n\tcase SEN_OV76BE:\n\t\tif (sd->gspca_dev.pixfmt.width == 320)\n\t\t\tinterlaced = 1;\n\t\t/* Fall through */\n\tcase SEN_OV6630:\n\tcase SEN_OV7610:\n\tcase SEN_OV7670:\n\t\tswitch (sd->frame_rate) {\n\t\tcase 30:\n\t\tcase 25:\n\t\t\t/* Not enough bandwidth to do 640x480 @ 30 fps */\n\t\t\tif (sd->gspca_dev.pixfmt.width != 640) {\n\t\t\t\tsd->clockdiv = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* For 640x480 case */\n\t\t\t/* fall through */\n\t\tdefault:\n/*\t\tcase 20: */\n/*\t\tcase 15: */\n\t\t\tsd->clockdiv = 1;\n\t\t\tbreak;\n\t\tcase 10:\n\t\t\tsd->clockdiv = 2;\n\t\t\tbreak;\n\t\tcase 5:\n\t\t\tsd->clockdiv = 5;\n\t\t\tbreak;\n\t\t}\n\t\tif (interlaced) {\n\t\t\tsd->clockdiv = (sd->clockdiv + 1) * 2 - 1;\n\t\t\t/* Higher then 10 does not work */\n\t\t\tif (sd->clockdiv > 10)\n\t\t\t\tsd->clockdiv = 10;\n\t\t}\n\t\tbreak;\n\n\tcase SEN_OV8610:\n\t\t/* No framerate control ?? */\n\t\tsd->clockdiv = 0;\n\t\tbreak;\n\t}\n\n\t/* Check if we have enough bandwidth to disable compression */\n\tfps = (interlaced ? 60 : 30) / (sd->clockdiv + 1) + 1;\n\tneeded = fps * sd->gspca_dev.pixfmt.width *\n\t\t\tsd->gspca_dev.pixfmt.height * 3 / 2;\n\t/* 1000 isoc packets/sec */\n\tif (needed > 1000 * packet_size) {\n\t\t/* Enable Y and UV quantization and compression */\n\t\treg_w(sd, R511_COMP_EN, 0x07);\n\t\treg_w(sd, R511_COMP_LUT_EN, 0x03);\n\t} else {\n\t\treg_w(sd, R511_COMP_EN, 0x06);\n\t\treg_w(sd, R511_COMP_LUT_EN, 0x00);\n\t}\n\n\treg_w(sd, R51x_SYS_RESET, OV511_RESET_OMNICE);\n\treg_w(sd, R51x_SYS_RESET, 0);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,11 @@\n \tif (!alt) {\n \t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n \t\tsd->gspca_dev.usb_err = -EIO;\n+\t\treturn;\n+\t}\n+\n+\tif (alt->desc.bNumEndpoints < 1) {\n+\t\tsd->gspca_dev.usb_err = -ENODEV;\n \t\treturn;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\treturn;",
                "\t}",
                "",
                "\tif (alt->desc.bNumEndpoints < 1) {",
                "\t\tsd->gspca_dev.usb_err = -ENODEV;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.6.1. drivers/media/usb/gspca/ov519.c allows NULL pointer dereferences in ov511_mode_init_regs and ov518_mode_init_regs when there are zero endpoints, aka CID-998912346c0d.",
        "id": 2428
    },
    {
        "cve_id": "CVE-2018-19407",
        "code_before_change": "static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_apic_hw_enabled(vcpu->arch.apic))\n\t\treturn;\n\n\tbitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);\n\n\tif (irqchip_split(vcpu->kvm))\n\t\tkvm_scan_ioapic_routes(vcpu, vcpu->arch.ioapic_handled_vectors);\n\telse {\n\t\tif (vcpu->arch.apicv_active)\n\t\t\tkvm_x86_ops->sync_pir_to_irr(vcpu);\n\t\tkvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);\n\t}\n\n\tif (is_guest_mode(vcpu))\n\t\tvcpu->arch.load_eoi_exitmap_pending = true;\n\telse\n\t\tkvm_make_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu);\n}",
        "code_after_change": "static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_apic_hw_enabled(vcpu->arch.apic))\n\t\treturn;\n\n\tbitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);\n\n\tif (irqchip_split(vcpu->kvm))\n\t\tkvm_scan_ioapic_routes(vcpu, vcpu->arch.ioapic_handled_vectors);\n\telse {\n\t\tif (vcpu->arch.apicv_active)\n\t\t\tkvm_x86_ops->sync_pir_to_irr(vcpu);\n\t\tif (ioapic_in_kernel(vcpu->kvm))\n\t\t\tkvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);\n\t}\n\n\tif (is_guest_mode(vcpu))\n\t\tvcpu->arch.load_eoi_exitmap_pending = true;\n\telse\n\t\tkvm_make_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu);\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,8 @@\n \telse {\n \t\tif (vcpu->arch.apicv_active)\n \t\t\tkvm_x86_ops->sync_pir_to_irr(vcpu);\n-\t\tkvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);\n+\t\tif (ioapic_in_kernel(vcpu->kvm))\n+\t\t\tkvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);\n \t}\n \n \tif (is_guest_mode(vcpu))",
        "function_modified_lines": {
            "added": [
                "\t\tif (ioapic_in_kernel(vcpu->kvm))",
                "\t\t\tkvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);"
            ],
            "deleted": [
                "\t\tkvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The vcpu_scan_ioapic function in arch/x86/kvm/x86.c in the Linux kernel through 4.19.2 allows local users to cause a denial of service (NULL pointer dereference and BUG) via crafted system calls that reach a situation where ioapic is uninitialized.",
        "id": 1744
    },
    {
        "cve_id": "CVE-2018-14613",
        "code_before_change": "static int check_leaf_item(struct btrfs_fs_info *fs_info,\n\t\t\t   struct extent_buffer *leaf,\n\t\t\t   struct btrfs_key *key, int slot)\n{\n\tint ret = 0;\n\n\tswitch (key->type) {\n\tcase BTRFS_EXTENT_DATA_KEY:\n\t\tret = check_extent_data_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\tcase BTRFS_EXTENT_CSUM_KEY:\n\t\tret = check_csum_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\tcase BTRFS_DIR_ITEM_KEY:\n\tcase BTRFS_DIR_INDEX_KEY:\n\tcase BTRFS_XATTR_ITEM_KEY:\n\t\tret = check_dir_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\t}\n\treturn ret;\n}",
        "code_after_change": "static int check_leaf_item(struct btrfs_fs_info *fs_info,\n\t\t\t   struct extent_buffer *leaf,\n\t\t\t   struct btrfs_key *key, int slot)\n{\n\tint ret = 0;\n\n\tswitch (key->type) {\n\tcase BTRFS_EXTENT_DATA_KEY:\n\t\tret = check_extent_data_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\tcase BTRFS_EXTENT_CSUM_KEY:\n\t\tret = check_csum_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\tcase BTRFS_DIR_ITEM_KEY:\n\tcase BTRFS_DIR_INDEX_KEY:\n\tcase BTRFS_XATTR_ITEM_KEY:\n\t\tret = check_dir_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\tcase BTRFS_BLOCK_GROUP_ITEM_KEY:\n\t\tret = check_block_group_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,9 @@\n \tcase BTRFS_XATTR_ITEM_KEY:\n \t\tret = check_dir_item(fs_info, leaf, key, slot);\n \t\tbreak;\n+\tcase BTRFS_BLOCK_GROUP_ITEM_KEY:\n+\t\tret = check_block_group_item(fs_info, leaf, key, slot);\n+\t\tbreak;\n \t}\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tcase BTRFS_BLOCK_GROUP_ITEM_KEY:",
                "\t\tret = check_block_group_item(fs_info, leaf, key, slot);",
                "\t\tbreak;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 4.17.10. There is an invalid pointer dereference in io_ctl_map_page() when mounting and operating a crafted btrfs image, because of a lack of block group item validation in check_leaf_item in fs/btrfs/tree-checker.c.",
        "id": 1682
    },
    {
        "cve_id": "CVE-2018-14613",
        "code_before_change": "static int __btrfs_alloc_chunk(struct btrfs_trans_handle *trans,\n\t\t\t       u64 start, u64 type)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = info->fs_devices;\n\tstruct btrfs_device *device;\n\tstruct map_lookup *map = NULL;\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_device_info *devices_info = NULL;\n\tu64 total_avail;\n\tint num_stripes;\t/* total number of stripes to allocate */\n\tint data_stripes;\t/* number of stripes that count for\n\t\t\t\t   block group size */\n\tint sub_stripes;\t/* sub_stripes info for map */\n\tint dev_stripes;\t/* stripes per dev */\n\tint devs_max;\t\t/* max devs to use */\n\tint devs_min;\t\t/* min devs needed */\n\tint devs_increment;\t/* ndevs has to be a multiple of this */\n\tint ncopies;\t\t/* how many copies to data has */\n\tint ret;\n\tu64 max_stripe_size;\n\tu64 max_chunk_size;\n\tu64 stripe_size;\n\tu64 num_bytes;\n\tint ndevs;\n\tint i;\n\tint j;\n\tint index;\n\n\tBUG_ON(!alloc_profile_is_valid(type, 0));\n\n\tif (list_empty(&fs_devices->alloc_list)) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\tbtrfs_debug(info, \"%s: no writable device\", __func__);\n\t\treturn -ENOSPC;\n\t}\n\n\tindex = btrfs_bg_flags_to_raid_index(type);\n\n\tsub_stripes = btrfs_raid_array[index].sub_stripes;\n\tdev_stripes = btrfs_raid_array[index].dev_stripes;\n\tdevs_max = btrfs_raid_array[index].devs_max;\n\tdevs_min = btrfs_raid_array[index].devs_min;\n\tdevs_increment = btrfs_raid_array[index].devs_increment;\n\tncopies = btrfs_raid_array[index].ncopies;\n\n\tif (type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tmax_stripe_size = SZ_1G;\n\t\tmax_chunk_size = 10 * max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {\n\t\t/* for larger filesystems, use larger metadata chunks */\n\t\tif (fs_devices->total_rw_bytes > 50ULL * SZ_1G)\n\t\t\tmax_stripe_size = SZ_1G;\n\t\telse\n\t\t\tmax_stripe_size = SZ_256M;\n\t\tmax_chunk_size = max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tmax_stripe_size = SZ_32M;\n\t\tmax_chunk_size = 2 * max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS_SYS_CHUNK;\n\t} else {\n\t\tbtrfs_err(info, \"invalid chunk type 0x%llx requested\",\n\t\t       type);\n\t\tBUG_ON(1);\n\t}\n\n\t/* we don't want a chunk larger than 10% of writeable space */\n\tmax_chunk_size = min(div_factor(fs_devices->total_rw_bytes, 1),\n\t\t\t     max_chunk_size);\n\n\tdevices_info = kcalloc(fs_devices->rw_devices, sizeof(*devices_info),\n\t\t\t       GFP_NOFS);\n\tif (!devices_info)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * in the first pass through the devices list, we gather information\n\t * about the available holes on each device.\n\t */\n\tndevs = 0;\n\tlist_for_each_entry(device, &fs_devices->alloc_list, dev_alloc_list) {\n\t\tu64 max_avail;\n\t\tu64 dev_offset;\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tWARN(1, KERN_ERR\n\t\t\t       \"BTRFS: read-only device in alloc_list\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t\t&device->dev_state) ||\n\t\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\t\tcontinue;\n\n\t\tif (device->total_bytes > device->bytes_used)\n\t\t\ttotal_avail = device->total_bytes - device->bytes_used;\n\t\telse\n\t\t\ttotal_avail = 0;\n\n\t\t/* If there is no space on this device, skip it. */\n\t\tif (total_avail == 0)\n\t\t\tcontinue;\n\n\t\tret = find_free_dev_extent(trans, device,\n\t\t\t\t\t   max_stripe_size * dev_stripes,\n\t\t\t\t\t   &dev_offset, &max_avail);\n\t\tif (ret && ret != -ENOSPC)\n\t\t\tgoto error;\n\n\t\tif (ret == 0)\n\t\t\tmax_avail = max_stripe_size * dev_stripes;\n\n\t\tif (max_avail < BTRFS_STRIPE_LEN * dev_stripes) {\n\t\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\t\tbtrfs_debug(info,\n\t\t\t\"%s: devid %llu has no free space, have=%llu want=%u\",\n\t\t\t\t\t    __func__, device->devid, max_avail,\n\t\t\t\t\t    BTRFS_STRIPE_LEN * dev_stripes);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ndevs == fs_devices->rw_devices) {\n\t\t\tWARN(1, \"%s: found more than %llu devices\\n\",\n\t\t\t     __func__, fs_devices->rw_devices);\n\t\t\tbreak;\n\t\t}\n\t\tdevices_info[ndevs].dev_offset = dev_offset;\n\t\tdevices_info[ndevs].max_avail = max_avail;\n\t\tdevices_info[ndevs].total_avail = total_avail;\n\t\tdevices_info[ndevs].dev = device;\n\t\t++ndevs;\n\t}\n\n\t/*\n\t * now sort the devices by hole size / available space\n\t */\n\tsort(devices_info, ndevs, sizeof(struct btrfs_device_info),\n\t     btrfs_cmp_device_info, NULL);\n\n\t/* round down to number of usable stripes */\n\tndevs = round_down(ndevs, devs_increment);\n\n\tif (ndevs < devs_min) {\n\t\tret = -ENOSPC;\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG)) {\n\t\t\tbtrfs_debug(info,\n\t\"%s: not enough devices with free space: have=%d minimum required=%d\",\n\t\t\t\t    __func__, ndevs, devs_min);\n\t\t}\n\t\tgoto error;\n\t}\n\n\tndevs = min(ndevs, devs_max);\n\n\t/*\n\t * The primary goal is to maximize the number of stripes, so use as\n\t * many devices as possible, even if the stripes are not maximum sized.\n\t *\n\t * The DUP profile stores more than one stripe per device, the\n\t * max_avail is the total size so we have to adjust.\n\t */\n\tstripe_size = div_u64(devices_info[ndevs - 1].max_avail, dev_stripes);\n\tnum_stripes = ndevs * dev_stripes;\n\n\t/*\n\t * this will have to be fixed for RAID1 and RAID10 over\n\t * more drives\n\t */\n\tdata_stripes = num_stripes / ncopies;\n\n\tif (type & BTRFS_BLOCK_GROUP_RAID5)\n\t\tdata_stripes = num_stripes - 1;\n\n\tif (type & BTRFS_BLOCK_GROUP_RAID6)\n\t\tdata_stripes = num_stripes - 2;\n\n\t/*\n\t * Use the number of data stripes to figure out how big this chunk\n\t * is really going to be in terms of logical address space,\n\t * and compare that answer with the max chunk size\n\t */\n\tif (stripe_size * data_stripes > max_chunk_size) {\n\t\tstripe_size = div_u64(max_chunk_size, data_stripes);\n\n\t\t/* bump the answer up to a 16MB boundary */\n\t\tstripe_size = round_up(stripe_size, SZ_16M);\n\n\t\t/*\n\t\t * But don't go higher than the limits we found while searching\n\t\t * for free extents\n\t\t */\n\t\tstripe_size = min(devices_info[ndevs - 1].max_avail,\n\t\t\t\t  stripe_size);\n\t}\n\n\t/* align to BTRFS_STRIPE_LEN */\n\tstripe_size = round_down(stripe_size, BTRFS_STRIPE_LEN);\n\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tmap->num_stripes = num_stripes;\n\n\tfor (i = 0; i < ndevs; ++i) {\n\t\tfor (j = 0; j < dev_stripes; ++j) {\n\t\t\tint s = i * dev_stripes + j;\n\t\t\tmap->stripes[s].dev = devices_info[i].dev;\n\t\t\tmap->stripes[s].physical = devices_info[i].dev_offset +\n\t\t\t\t\t\t   j * stripe_size;\n\t\t}\n\t}\n\tmap->stripe_len = BTRFS_STRIPE_LEN;\n\tmap->io_align = BTRFS_STRIPE_LEN;\n\tmap->io_width = BTRFS_STRIPE_LEN;\n\tmap->type = type;\n\tmap->sub_stripes = sub_stripes;\n\n\tnum_bytes = stripe_size * data_stripes;\n\n\ttrace_btrfs_chunk_alloc(info, map, start, num_bytes);\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tkfree(map);\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = start;\n\tem->len = num_bytes;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\tem->orig_block_len = stripe_size;\n\n\tem_tree = &info->mapping_tree.map_tree;\n\twrite_lock(&em_tree->lock);\n\tret = add_extent_mapping(em_tree, em, 0);\n\tif (ret) {\n\t\twrite_unlock(&em_tree->lock);\n\t\tfree_extent_map(em);\n\t\tgoto error;\n\t}\n\n\tlist_add_tail(&em->list, &trans->transaction->pending_chunks);\n\trefcount_inc(&em->refs);\n\twrite_unlock(&em_tree->lock);\n\n\tret = btrfs_make_block_group(trans, 0, type, start, num_bytes);\n\tif (ret)\n\t\tgoto error_del_extent;\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tnum_bytes = map->stripes[i].dev->bytes_used + stripe_size;\n\t\tbtrfs_device_set_bytes_used(map->stripes[i].dev, num_bytes);\n\t}\n\n\tatomic64_sub(stripe_size * map->num_stripes, &info->free_chunk_space);\n\n\tfree_extent_map(em);\n\tcheck_raid56_incompat_flag(info, type);\n\n\tkfree(devices_info);\n\treturn 0;\n\nerror_del_extent:\n\twrite_lock(&em_tree->lock);\n\tremove_extent_mapping(em_tree, em);\n\twrite_unlock(&em_tree->lock);\n\n\t/* One for our allocation */\n\tfree_extent_map(em);\n\t/* One for the tree reference */\n\tfree_extent_map(em);\n\t/* One for the pending_chunks list reference */\n\tfree_extent_map(em);\nerror:\n\tkfree(devices_info);\n\treturn ret;\n}",
        "code_after_change": "static int __btrfs_alloc_chunk(struct btrfs_trans_handle *trans,\n\t\t\t       u64 start, u64 type)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = info->fs_devices;\n\tstruct btrfs_device *device;\n\tstruct map_lookup *map = NULL;\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_device_info *devices_info = NULL;\n\tu64 total_avail;\n\tint num_stripes;\t/* total number of stripes to allocate */\n\tint data_stripes;\t/* number of stripes that count for\n\t\t\t\t   block group size */\n\tint sub_stripes;\t/* sub_stripes info for map */\n\tint dev_stripes;\t/* stripes per dev */\n\tint devs_max;\t\t/* max devs to use */\n\tint devs_min;\t\t/* min devs needed */\n\tint devs_increment;\t/* ndevs has to be a multiple of this */\n\tint ncopies;\t\t/* how many copies to data has */\n\tint ret;\n\tu64 max_stripe_size;\n\tu64 max_chunk_size;\n\tu64 stripe_size;\n\tu64 num_bytes;\n\tint ndevs;\n\tint i;\n\tint j;\n\tint index;\n\n\tBUG_ON(!alloc_profile_is_valid(type, 0));\n\n\tif (list_empty(&fs_devices->alloc_list)) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\tbtrfs_debug(info, \"%s: no writable device\", __func__);\n\t\treturn -ENOSPC;\n\t}\n\n\tindex = btrfs_bg_flags_to_raid_index(type);\n\n\tsub_stripes = btrfs_raid_array[index].sub_stripes;\n\tdev_stripes = btrfs_raid_array[index].dev_stripes;\n\tdevs_max = btrfs_raid_array[index].devs_max;\n\tdevs_min = btrfs_raid_array[index].devs_min;\n\tdevs_increment = btrfs_raid_array[index].devs_increment;\n\tncopies = btrfs_raid_array[index].ncopies;\n\n\tif (type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tmax_stripe_size = SZ_1G;\n\t\tmax_chunk_size = BTRFS_MAX_DATA_CHUNK_SIZE;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {\n\t\t/* for larger filesystems, use larger metadata chunks */\n\t\tif (fs_devices->total_rw_bytes > 50ULL * SZ_1G)\n\t\t\tmax_stripe_size = SZ_1G;\n\t\telse\n\t\t\tmax_stripe_size = SZ_256M;\n\t\tmax_chunk_size = max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tmax_stripe_size = SZ_32M;\n\t\tmax_chunk_size = 2 * max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS_SYS_CHUNK;\n\t} else {\n\t\tbtrfs_err(info, \"invalid chunk type 0x%llx requested\",\n\t\t       type);\n\t\tBUG_ON(1);\n\t}\n\n\t/* we don't want a chunk larger than 10% of writeable space */\n\tmax_chunk_size = min(div_factor(fs_devices->total_rw_bytes, 1),\n\t\t\t     max_chunk_size);\n\n\tdevices_info = kcalloc(fs_devices->rw_devices, sizeof(*devices_info),\n\t\t\t       GFP_NOFS);\n\tif (!devices_info)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * in the first pass through the devices list, we gather information\n\t * about the available holes on each device.\n\t */\n\tndevs = 0;\n\tlist_for_each_entry(device, &fs_devices->alloc_list, dev_alloc_list) {\n\t\tu64 max_avail;\n\t\tu64 dev_offset;\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tWARN(1, KERN_ERR\n\t\t\t       \"BTRFS: read-only device in alloc_list\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t\t&device->dev_state) ||\n\t\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\t\tcontinue;\n\n\t\tif (device->total_bytes > device->bytes_used)\n\t\t\ttotal_avail = device->total_bytes - device->bytes_used;\n\t\telse\n\t\t\ttotal_avail = 0;\n\n\t\t/* If there is no space on this device, skip it. */\n\t\tif (total_avail == 0)\n\t\t\tcontinue;\n\n\t\tret = find_free_dev_extent(trans, device,\n\t\t\t\t\t   max_stripe_size * dev_stripes,\n\t\t\t\t\t   &dev_offset, &max_avail);\n\t\tif (ret && ret != -ENOSPC)\n\t\t\tgoto error;\n\n\t\tif (ret == 0)\n\t\t\tmax_avail = max_stripe_size * dev_stripes;\n\n\t\tif (max_avail < BTRFS_STRIPE_LEN * dev_stripes) {\n\t\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\t\tbtrfs_debug(info,\n\t\t\t\"%s: devid %llu has no free space, have=%llu want=%u\",\n\t\t\t\t\t    __func__, device->devid, max_avail,\n\t\t\t\t\t    BTRFS_STRIPE_LEN * dev_stripes);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ndevs == fs_devices->rw_devices) {\n\t\t\tWARN(1, \"%s: found more than %llu devices\\n\",\n\t\t\t     __func__, fs_devices->rw_devices);\n\t\t\tbreak;\n\t\t}\n\t\tdevices_info[ndevs].dev_offset = dev_offset;\n\t\tdevices_info[ndevs].max_avail = max_avail;\n\t\tdevices_info[ndevs].total_avail = total_avail;\n\t\tdevices_info[ndevs].dev = device;\n\t\t++ndevs;\n\t}\n\n\t/*\n\t * now sort the devices by hole size / available space\n\t */\n\tsort(devices_info, ndevs, sizeof(struct btrfs_device_info),\n\t     btrfs_cmp_device_info, NULL);\n\n\t/* round down to number of usable stripes */\n\tndevs = round_down(ndevs, devs_increment);\n\n\tif (ndevs < devs_min) {\n\t\tret = -ENOSPC;\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG)) {\n\t\t\tbtrfs_debug(info,\n\t\"%s: not enough devices with free space: have=%d minimum required=%d\",\n\t\t\t\t    __func__, ndevs, devs_min);\n\t\t}\n\t\tgoto error;\n\t}\n\n\tndevs = min(ndevs, devs_max);\n\n\t/*\n\t * The primary goal is to maximize the number of stripes, so use as\n\t * many devices as possible, even if the stripes are not maximum sized.\n\t *\n\t * The DUP profile stores more than one stripe per device, the\n\t * max_avail is the total size so we have to adjust.\n\t */\n\tstripe_size = div_u64(devices_info[ndevs - 1].max_avail, dev_stripes);\n\tnum_stripes = ndevs * dev_stripes;\n\n\t/*\n\t * this will have to be fixed for RAID1 and RAID10 over\n\t * more drives\n\t */\n\tdata_stripes = num_stripes / ncopies;\n\n\tif (type & BTRFS_BLOCK_GROUP_RAID5)\n\t\tdata_stripes = num_stripes - 1;\n\n\tif (type & BTRFS_BLOCK_GROUP_RAID6)\n\t\tdata_stripes = num_stripes - 2;\n\n\t/*\n\t * Use the number of data stripes to figure out how big this chunk\n\t * is really going to be in terms of logical address space,\n\t * and compare that answer with the max chunk size\n\t */\n\tif (stripe_size * data_stripes > max_chunk_size) {\n\t\tstripe_size = div_u64(max_chunk_size, data_stripes);\n\n\t\t/* bump the answer up to a 16MB boundary */\n\t\tstripe_size = round_up(stripe_size, SZ_16M);\n\n\t\t/*\n\t\t * But don't go higher than the limits we found while searching\n\t\t * for free extents\n\t\t */\n\t\tstripe_size = min(devices_info[ndevs - 1].max_avail,\n\t\t\t\t  stripe_size);\n\t}\n\n\t/* align to BTRFS_STRIPE_LEN */\n\tstripe_size = round_down(stripe_size, BTRFS_STRIPE_LEN);\n\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tmap->num_stripes = num_stripes;\n\n\tfor (i = 0; i < ndevs; ++i) {\n\t\tfor (j = 0; j < dev_stripes; ++j) {\n\t\t\tint s = i * dev_stripes + j;\n\t\t\tmap->stripes[s].dev = devices_info[i].dev;\n\t\t\tmap->stripes[s].physical = devices_info[i].dev_offset +\n\t\t\t\t\t\t   j * stripe_size;\n\t\t}\n\t}\n\tmap->stripe_len = BTRFS_STRIPE_LEN;\n\tmap->io_align = BTRFS_STRIPE_LEN;\n\tmap->io_width = BTRFS_STRIPE_LEN;\n\tmap->type = type;\n\tmap->sub_stripes = sub_stripes;\n\n\tnum_bytes = stripe_size * data_stripes;\n\n\ttrace_btrfs_chunk_alloc(info, map, start, num_bytes);\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tkfree(map);\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = start;\n\tem->len = num_bytes;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\tem->orig_block_len = stripe_size;\n\n\tem_tree = &info->mapping_tree.map_tree;\n\twrite_lock(&em_tree->lock);\n\tret = add_extent_mapping(em_tree, em, 0);\n\tif (ret) {\n\t\twrite_unlock(&em_tree->lock);\n\t\tfree_extent_map(em);\n\t\tgoto error;\n\t}\n\n\tlist_add_tail(&em->list, &trans->transaction->pending_chunks);\n\trefcount_inc(&em->refs);\n\twrite_unlock(&em_tree->lock);\n\n\tret = btrfs_make_block_group(trans, 0, type, start, num_bytes);\n\tif (ret)\n\t\tgoto error_del_extent;\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tnum_bytes = map->stripes[i].dev->bytes_used + stripe_size;\n\t\tbtrfs_device_set_bytes_used(map->stripes[i].dev, num_bytes);\n\t}\n\n\tatomic64_sub(stripe_size * map->num_stripes, &info->free_chunk_space);\n\n\tfree_extent_map(em);\n\tcheck_raid56_incompat_flag(info, type);\n\n\tkfree(devices_info);\n\treturn 0;\n\nerror_del_extent:\n\twrite_lock(&em_tree->lock);\n\tremove_extent_mapping(em_tree, em);\n\twrite_unlock(&em_tree->lock);\n\n\t/* One for our allocation */\n\tfree_extent_map(em);\n\t/* One for the tree reference */\n\tfree_extent_map(em);\n\t/* One for the pending_chunks list reference */\n\tfree_extent_map(em);\nerror:\n\tkfree(devices_info);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,7 +47,7 @@\n \n \tif (type & BTRFS_BLOCK_GROUP_DATA) {\n \t\tmax_stripe_size = SZ_1G;\n-\t\tmax_chunk_size = 10 * max_stripe_size;\n+\t\tmax_chunk_size = BTRFS_MAX_DATA_CHUNK_SIZE;\n \t\tif (!devs_max)\n \t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n \t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {",
        "function_modified_lines": {
            "added": [
                "\t\tmax_chunk_size = BTRFS_MAX_DATA_CHUNK_SIZE;"
            ],
            "deleted": [
                "\t\tmax_chunk_size = 10 * max_stripe_size;"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 4.17.10. There is an invalid pointer dereference in io_ctl_map_page() when mounting and operating a crafted btrfs image, because of a lack of block group item validation in check_leaf_item in fs/btrfs/tree-checker.c.",
        "id": 1683
    },
    {
        "cve_id": "CVE-2023-6622",
        "code_before_change": "static int nft_dynset_init(const struct nft_ctx *ctx,\n\t\t\t   const struct nft_expr *expr,\n\t\t\t   const struct nlattr * const tb[])\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(ctx->net);\n\tstruct nft_dynset *priv = nft_expr_priv(expr);\n\tu8 genmask = nft_genmask_next(ctx->net);\n\tstruct nft_set *set;\n\tu64 timeout;\n\tint err, i;\n\n\tlockdep_assert_held(&nft_net->commit_mutex);\n\n\tif (tb[NFTA_DYNSET_SET_NAME] == NULL ||\n\t    tb[NFTA_DYNSET_OP] == NULL ||\n\t    tb[NFTA_DYNSET_SREG_KEY] == NULL)\n\t\treturn -EINVAL;\n\n\tif (tb[NFTA_DYNSET_FLAGS]) {\n\t\tu32 flags = ntohl(nla_get_be32(tb[NFTA_DYNSET_FLAGS]));\n\t\tif (flags & ~(NFT_DYNSET_F_INV | NFT_DYNSET_F_EXPR))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (flags & NFT_DYNSET_F_INV)\n\t\t\tpriv->invert = true;\n\t\tif (flags & NFT_DYNSET_F_EXPR)\n\t\t\tpriv->expr = true;\n\t}\n\n\tset = nft_set_lookup_global(ctx->net, ctx->table,\n\t\t\t\t    tb[NFTA_DYNSET_SET_NAME],\n\t\t\t\t    tb[NFTA_DYNSET_SET_ID], genmask);\n\tif (IS_ERR(set))\n\t\treturn PTR_ERR(set);\n\n\tif (set->flags & NFT_SET_OBJECT)\n\t\treturn -EOPNOTSUPP;\n\n\tif (set->ops->update == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (set->flags & NFT_SET_CONSTANT)\n\t\treturn -EBUSY;\n\n\tpriv->op = ntohl(nla_get_be32(tb[NFTA_DYNSET_OP]));\n\tif (priv->op > NFT_DYNSET_OP_DELETE)\n\t\treturn -EOPNOTSUPP;\n\n\ttimeout = 0;\n\tif (tb[NFTA_DYNSET_TIMEOUT] != NULL) {\n\t\tif (!(set->flags & NFT_SET_TIMEOUT))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\terr = nf_msecs_to_jiffies64(tb[NFTA_DYNSET_TIMEOUT], &timeout);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = nft_parse_register_load(tb[NFTA_DYNSET_SREG_KEY], &priv->sreg_key,\n\t\t\t\t      set->klen);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[NFTA_DYNSET_SREG_DATA] != NULL) {\n\t\tif (!(set->flags & NFT_SET_MAP))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (set->dtype == NFT_DATA_VERDICT)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\terr = nft_parse_register_load(tb[NFTA_DYNSET_SREG_DATA],\n\t\t\t\t\t      &priv->sreg_data, set->dlen);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else if (set->flags & NFT_SET_MAP)\n\t\treturn -EINVAL;\n\n\tif ((tb[NFTA_DYNSET_EXPR] || tb[NFTA_DYNSET_EXPRESSIONS]) &&\n\t    !(set->flags & NFT_SET_EVAL))\n\t\treturn -EINVAL;\n\n\tif (tb[NFTA_DYNSET_EXPR]) {\n\t\tstruct nft_expr *dynset_expr;\n\n\t\tdynset_expr = nft_dynset_expr_alloc(ctx, set,\n\t\t\t\t\t\t    tb[NFTA_DYNSET_EXPR], 0);\n\t\tif (IS_ERR(dynset_expr))\n\t\t\treturn PTR_ERR(dynset_expr);\n\n\t\tpriv->num_exprs++;\n\t\tpriv->expr_array[0] = dynset_expr;\n\n\t\tif (set->num_exprs > 1 ||\n\t\t    (set->num_exprs == 1 &&\n\t\t     dynset_expr->ops != set->exprs[0]->ops)) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_expr_free;\n\t\t}\n\t} else if (tb[NFTA_DYNSET_EXPRESSIONS]) {\n\t\tstruct nft_expr *dynset_expr;\n\t\tstruct nlattr *tmp;\n\t\tint left;\n\n\t\tif (!priv->expr)\n\t\t\treturn -EINVAL;\n\n\t\ti = 0;\n\t\tnla_for_each_nested(tmp, tb[NFTA_DYNSET_EXPRESSIONS], left) {\n\t\t\tif (i == NFT_SET_EXPR_MAX) {\n\t\t\t\terr = -E2BIG;\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tif (nla_type(tmp) != NFTA_LIST_ELEM) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tdynset_expr = nft_dynset_expr_alloc(ctx, set, tmp, i);\n\t\t\tif (IS_ERR(dynset_expr)) {\n\t\t\t\terr = PTR_ERR(dynset_expr);\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tpriv->expr_array[i] = dynset_expr;\n\t\t\tpriv->num_exprs++;\n\n\t\t\tif (set->num_exprs &&\n\t\t\t    dynset_expr->ops != set->exprs[i]->ops) {\n\t\t\t\terr = -EOPNOTSUPP;\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\ti++;\n\t\t}\n\t\tif (set->num_exprs && set->num_exprs != i) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_expr_free;\n\t\t}\n\t} else if (set->num_exprs > 0) {\n\t\terr = nft_set_elem_expr_clone(ctx, set, priv->expr_array);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tpriv->num_exprs = set->num_exprs;\n\t}\n\n\tnft_set_ext_prepare(&priv->tmpl);\n\tnft_set_ext_add_length(&priv->tmpl, NFT_SET_EXT_KEY, set->klen);\n\tif (set->flags & NFT_SET_MAP)\n\t\tnft_set_ext_add_length(&priv->tmpl, NFT_SET_EXT_DATA, set->dlen);\n\n\tif (priv->num_exprs)\n\t\tnft_dynset_ext_add_expr(priv);\n\n\tif (set->flags & NFT_SET_TIMEOUT) {\n\t\tif (timeout || set->timeout) {\n\t\t\tnft_set_ext_add(&priv->tmpl, NFT_SET_EXT_TIMEOUT);\n\t\t\tnft_set_ext_add(&priv->tmpl, NFT_SET_EXT_EXPIRATION);\n\t\t}\n\t}\n\n\tpriv->timeout = timeout;\n\n\terr = nf_tables_bind_set(ctx, set, &priv->binding);\n\tif (err < 0)\n\t\tgoto err_expr_free;\n\n\tif (set->size == 0)\n\t\tset->size = 0xffff;\n\n\tpriv->set = set;\n\treturn 0;\n\nerr_expr_free:\n\tfor (i = 0; i < priv->num_exprs; i++)\n\t\tnft_expr_destroy(ctx, priv->expr_array[i]);\n\treturn err;\n}",
        "code_after_change": "static int nft_dynset_init(const struct nft_ctx *ctx,\n\t\t\t   const struct nft_expr *expr,\n\t\t\t   const struct nlattr * const tb[])\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(ctx->net);\n\tstruct nft_dynset *priv = nft_expr_priv(expr);\n\tu8 genmask = nft_genmask_next(ctx->net);\n\tstruct nft_set *set;\n\tu64 timeout;\n\tint err, i;\n\n\tlockdep_assert_held(&nft_net->commit_mutex);\n\n\tif (tb[NFTA_DYNSET_SET_NAME] == NULL ||\n\t    tb[NFTA_DYNSET_OP] == NULL ||\n\t    tb[NFTA_DYNSET_SREG_KEY] == NULL)\n\t\treturn -EINVAL;\n\n\tif (tb[NFTA_DYNSET_FLAGS]) {\n\t\tu32 flags = ntohl(nla_get_be32(tb[NFTA_DYNSET_FLAGS]));\n\t\tif (flags & ~(NFT_DYNSET_F_INV | NFT_DYNSET_F_EXPR))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (flags & NFT_DYNSET_F_INV)\n\t\t\tpriv->invert = true;\n\t\tif (flags & NFT_DYNSET_F_EXPR)\n\t\t\tpriv->expr = true;\n\t}\n\n\tset = nft_set_lookup_global(ctx->net, ctx->table,\n\t\t\t\t    tb[NFTA_DYNSET_SET_NAME],\n\t\t\t\t    tb[NFTA_DYNSET_SET_ID], genmask);\n\tif (IS_ERR(set))\n\t\treturn PTR_ERR(set);\n\n\tif (set->flags & NFT_SET_OBJECT)\n\t\treturn -EOPNOTSUPP;\n\n\tif (set->ops->update == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (set->flags & NFT_SET_CONSTANT)\n\t\treturn -EBUSY;\n\n\tpriv->op = ntohl(nla_get_be32(tb[NFTA_DYNSET_OP]));\n\tif (priv->op > NFT_DYNSET_OP_DELETE)\n\t\treturn -EOPNOTSUPP;\n\n\ttimeout = 0;\n\tif (tb[NFTA_DYNSET_TIMEOUT] != NULL) {\n\t\tif (!(set->flags & NFT_SET_TIMEOUT))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\terr = nf_msecs_to_jiffies64(tb[NFTA_DYNSET_TIMEOUT], &timeout);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = nft_parse_register_load(tb[NFTA_DYNSET_SREG_KEY], &priv->sreg_key,\n\t\t\t\t      set->klen);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[NFTA_DYNSET_SREG_DATA] != NULL) {\n\t\tif (!(set->flags & NFT_SET_MAP))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (set->dtype == NFT_DATA_VERDICT)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\terr = nft_parse_register_load(tb[NFTA_DYNSET_SREG_DATA],\n\t\t\t\t\t      &priv->sreg_data, set->dlen);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else if (set->flags & NFT_SET_MAP)\n\t\treturn -EINVAL;\n\n\tif ((tb[NFTA_DYNSET_EXPR] || tb[NFTA_DYNSET_EXPRESSIONS]) &&\n\t    !(set->flags & NFT_SET_EVAL))\n\t\treturn -EINVAL;\n\n\tif (tb[NFTA_DYNSET_EXPR]) {\n\t\tstruct nft_expr *dynset_expr;\n\n\t\tdynset_expr = nft_dynset_expr_alloc(ctx, set,\n\t\t\t\t\t\t    tb[NFTA_DYNSET_EXPR], 0);\n\t\tif (IS_ERR(dynset_expr))\n\t\t\treturn PTR_ERR(dynset_expr);\n\n\t\tpriv->num_exprs++;\n\t\tpriv->expr_array[0] = dynset_expr;\n\n\t\tif (set->num_exprs > 1 ||\n\t\t    (set->num_exprs == 1 &&\n\t\t     dynset_expr->ops != set->exprs[0]->ops)) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_expr_free;\n\t\t}\n\t} else if (tb[NFTA_DYNSET_EXPRESSIONS]) {\n\t\tstruct nft_expr *dynset_expr;\n\t\tstruct nlattr *tmp;\n\t\tint left;\n\n\t\tif (!priv->expr)\n\t\t\treturn -EINVAL;\n\n\t\ti = 0;\n\t\tnla_for_each_nested(tmp, tb[NFTA_DYNSET_EXPRESSIONS], left) {\n\t\t\tif (i == NFT_SET_EXPR_MAX) {\n\t\t\t\terr = -E2BIG;\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tif (nla_type(tmp) != NFTA_LIST_ELEM) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tdynset_expr = nft_dynset_expr_alloc(ctx, set, tmp, i);\n\t\t\tif (IS_ERR(dynset_expr)) {\n\t\t\t\terr = PTR_ERR(dynset_expr);\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tpriv->expr_array[i] = dynset_expr;\n\t\t\tpriv->num_exprs++;\n\n\t\t\tif (set->num_exprs) {\n\t\t\t\tif (i >= set->num_exprs) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto err_expr_free;\n\t\t\t\t}\n\t\t\t\tif (dynset_expr->ops != set->exprs[i]->ops) {\n\t\t\t\t\terr = -EOPNOTSUPP;\n\t\t\t\t\tgoto err_expr_free;\n\t\t\t\t}\n\t\t\t}\n\t\t\ti++;\n\t\t}\n\t\tif (set->num_exprs && set->num_exprs != i) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_expr_free;\n\t\t}\n\t} else if (set->num_exprs > 0) {\n\t\terr = nft_set_elem_expr_clone(ctx, set, priv->expr_array);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tpriv->num_exprs = set->num_exprs;\n\t}\n\n\tnft_set_ext_prepare(&priv->tmpl);\n\tnft_set_ext_add_length(&priv->tmpl, NFT_SET_EXT_KEY, set->klen);\n\tif (set->flags & NFT_SET_MAP)\n\t\tnft_set_ext_add_length(&priv->tmpl, NFT_SET_EXT_DATA, set->dlen);\n\n\tif (priv->num_exprs)\n\t\tnft_dynset_ext_add_expr(priv);\n\n\tif (set->flags & NFT_SET_TIMEOUT) {\n\t\tif (timeout || set->timeout) {\n\t\t\tnft_set_ext_add(&priv->tmpl, NFT_SET_EXT_TIMEOUT);\n\t\t\tnft_set_ext_add(&priv->tmpl, NFT_SET_EXT_EXPIRATION);\n\t\t}\n\t}\n\n\tpriv->timeout = timeout;\n\n\terr = nf_tables_bind_set(ctx, set, &priv->binding);\n\tif (err < 0)\n\t\tgoto err_expr_free;\n\n\tif (set->size == 0)\n\t\tset->size = 0xffff;\n\n\tpriv->set = set;\n\treturn 0;\n\nerr_expr_free:\n\tfor (i = 0; i < priv->num_exprs; i++)\n\t\tnft_expr_destroy(ctx, priv->expr_array[i]);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -120,10 +120,15 @@\n \t\t\tpriv->expr_array[i] = dynset_expr;\n \t\t\tpriv->num_exprs++;\n \n-\t\t\tif (set->num_exprs &&\n-\t\t\t    dynset_expr->ops != set->exprs[i]->ops) {\n-\t\t\t\terr = -EOPNOTSUPP;\n-\t\t\t\tgoto err_expr_free;\n+\t\t\tif (set->num_exprs) {\n+\t\t\t\tif (i >= set->num_exprs) {\n+\t\t\t\t\terr = -EINVAL;\n+\t\t\t\t\tgoto err_expr_free;\n+\t\t\t\t}\n+\t\t\t\tif (dynset_expr->ops != set->exprs[i]->ops) {\n+\t\t\t\t\terr = -EOPNOTSUPP;\n+\t\t\t\t\tgoto err_expr_free;\n+\t\t\t\t}\n \t\t\t}\n \t\t\ti++;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (set->num_exprs) {",
                "\t\t\t\tif (i >= set->num_exprs) {",
                "\t\t\t\t\terr = -EINVAL;",
                "\t\t\t\t\tgoto err_expr_free;",
                "\t\t\t\t}",
                "\t\t\t\tif (dynset_expr->ops != set->exprs[i]->ops) {",
                "\t\t\t\t\terr = -EOPNOTSUPP;",
                "\t\t\t\t\tgoto err_expr_free;",
                "\t\t\t\t}"
            ],
            "deleted": [
                "\t\t\tif (set->num_exprs &&",
                "\t\t\t    dynset_expr->ops != set->exprs[i]->ops) {",
                "\t\t\t\terr = -EOPNOTSUPP;",
                "\t\t\t\tgoto err_expr_free;"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A null pointer dereference vulnerability was found in nft_dynset_init() in net/netfilter/nft_dynset.c in nf_tables in the Linux kernel. This issue may allow a local attacker with CAP_NET_ADMIN user privilege to trigger a denial of service.",
        "id": 4307
    },
    {
        "cve_id": "CVE-2021-38208",
        "code_before_change": "static int llcp_sock_bind(struct socket *sock, struct sockaddr *addr, int alen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct nfc_llcp_local *local;\n\tstruct nfc_dev *dev;\n\tstruct sockaddr_nfc_llcp llcp_addr;\n\tint len, ret = 0;\n\n\tif (!addr || alen < offsetofend(struct sockaddr, sa_family) ||\n\t    addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"sk %p addr %p family %d\\n\", sk, addr, addr->sa_family);\n\n\tmemset(&llcp_addr, 0, sizeof(llcp_addr));\n\tlen = min_t(unsigned int, sizeof(llcp_addr), alen);\n\tmemcpy(&llcp_addr, addr, len);\n\n\t/* This is going to be a listening socket, dsap must be 0 */\n\tif (llcp_addr.dsap != 0)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != LLCP_CLOSED) {\n\t\tret = -EBADFD;\n\t\tgoto error;\n\t}\n\n\tdev = nfc_get_device(llcp_addr.dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = nfc_llcp_local_get(local);\n\tllcp_sock->nfc_protocol = llcp_addr.nfc_protocol;\n\tllcp_sock->service_name_len = min_t(unsigned int,\n\t\t\t\t\t    llcp_addr.service_name_len,\n\t\t\t\t\t    NFC_LLCP_MAX_SERVICE_NAME);\n\tllcp_sock->service_name = kmemdup(llcp_addr.service_name,\n\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tnfc_llcp_local_put(llcp_sock->local);\n\t\tllcp_sock->local = NULL;\n\t\tret = -ENOMEM;\n\t\tgoto put_dev;\n\t}\n\tllcp_sock->ssap = nfc_llcp_get_sdp_ssap(local, llcp_sock);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tnfc_llcp_local_put(llcp_sock->local);\n\t\tllcp_sock->local = NULL;\n\t\tkfree(llcp_sock->service_name);\n\t\tllcp_sock->service_name = NULL;\n\t\tret = -EADDRINUSE;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;\n\n\tnfc_llcp_sock_link(&local->sockets, sk);\n\n\tpr_debug(\"Socket bound to SAP %d\\n\", llcp_sock->ssap);\n\n\tsk->sk_state = LLCP_BOUND;\n\nput_dev:\n\tnfc_put_device(dev);\n\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "code_after_change": "static int llcp_sock_bind(struct socket *sock, struct sockaddr *addr, int alen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct nfc_llcp_local *local;\n\tstruct nfc_dev *dev;\n\tstruct sockaddr_nfc_llcp llcp_addr;\n\tint len, ret = 0;\n\n\tif (!addr || alen < offsetofend(struct sockaddr, sa_family) ||\n\t    addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"sk %p addr %p family %d\\n\", sk, addr, addr->sa_family);\n\n\tmemset(&llcp_addr, 0, sizeof(llcp_addr));\n\tlen = min_t(unsigned int, sizeof(llcp_addr), alen);\n\tmemcpy(&llcp_addr, addr, len);\n\n\t/* This is going to be a listening socket, dsap must be 0 */\n\tif (llcp_addr.dsap != 0)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != LLCP_CLOSED) {\n\t\tret = -EBADFD;\n\t\tgoto error;\n\t}\n\n\tdev = nfc_get_device(llcp_addr.dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = nfc_llcp_local_get(local);\n\tllcp_sock->nfc_protocol = llcp_addr.nfc_protocol;\n\tllcp_sock->service_name_len = min_t(unsigned int,\n\t\t\t\t\t    llcp_addr.service_name_len,\n\t\t\t\t\t    NFC_LLCP_MAX_SERVICE_NAME);\n\tllcp_sock->service_name = kmemdup(llcp_addr.service_name,\n\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tnfc_llcp_local_put(llcp_sock->local);\n\t\tllcp_sock->local = NULL;\n\t\tllcp_sock->dev = NULL;\n\t\tret = -ENOMEM;\n\t\tgoto put_dev;\n\t}\n\tllcp_sock->ssap = nfc_llcp_get_sdp_ssap(local, llcp_sock);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tnfc_llcp_local_put(llcp_sock->local);\n\t\tllcp_sock->local = NULL;\n\t\tkfree(llcp_sock->service_name);\n\t\tllcp_sock->service_name = NULL;\n\t\tllcp_sock->dev = NULL;\n\t\tret = -EADDRINUSE;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;\n\n\tnfc_llcp_sock_link(&local->sockets, sk);\n\n\tpr_debug(\"Socket bound to SAP %d\\n\", llcp_sock->ssap);\n\n\tsk->sk_state = LLCP_BOUND;\n\nput_dev:\n\tnfc_put_device(dev);\n\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -52,6 +52,7 @@\n \tif (!llcp_sock->service_name) {\n \t\tnfc_llcp_local_put(llcp_sock->local);\n \t\tllcp_sock->local = NULL;\n+\t\tllcp_sock->dev = NULL;\n \t\tret = -ENOMEM;\n \t\tgoto put_dev;\n \t}\n@@ -61,6 +62,7 @@\n \t\tllcp_sock->local = NULL;\n \t\tkfree(llcp_sock->service_name);\n \t\tllcp_sock->service_name = NULL;\n+\t\tllcp_sock->dev = NULL;\n \t\tret = -EADDRINUSE;\n \t\tgoto put_dev;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tllcp_sock->dev = NULL;",
                "\t\tllcp_sock->dev = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "net/nfc/llcp_sock.c in the Linux kernel before 5.12.10 allows local unprivileged users to cause a denial of service (NULL pointer dereference and BUG) by making a getsockname call after a certain type of failure of a bind call.",
        "id": 3085
    },
    {
        "cve_id": "CVE-2019-10207",
        "code_before_change": "static int intel_open(struct hci_uart *hu)\n{\n\tstruct intel_data *intel;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tintel = kzalloc(sizeof(*intel), GFP_KERNEL);\n\tif (!intel)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&intel->txq);\n\tINIT_WORK(&intel->busy_work, intel_busy_work);\n\n\tintel->hu = hu;\n\n\thu->priv = intel;\n\n\tif (!intel_set_power(hu, true))\n\t\tset_bit(STATE_BOOTING, &intel->flags);\n\n\treturn 0;\n}",
        "code_after_change": "static int intel_open(struct hci_uart *hu)\n{\n\tstruct intel_data *intel;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tif (!hci_uart_has_flow_control(hu))\n\t\treturn -EOPNOTSUPP;\n\n\tintel = kzalloc(sizeof(*intel), GFP_KERNEL);\n\tif (!intel)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&intel->txq);\n\tINIT_WORK(&intel->busy_work, intel_busy_work);\n\n\tintel->hu = hu;\n\n\thu->priv = intel;\n\n\tif (!intel_set_power(hu, true))\n\t\tset_bit(STATE_BOOTING, &intel->flags);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,9 @@\n \tstruct intel_data *intel;\n \n \tBT_DBG(\"hu %p\", hu);\n+\n+\tif (!hci_uart_has_flow_control(hu))\n+\t\treturn -EOPNOTSUPP;\n \n \tintel = kzalloc(sizeof(*intel), GFP_KERNEL);\n \tif (!intel)",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (!hci_uart_has_flow_control(hu))",
                "\t\treturn -EOPNOTSUPP;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A flaw was found in the Linux kernel's Bluetooth implementation of UART, all versions kernel 3.x.x before 4.18.0 and kernel 5.x.x. An attacker with local access and write permissions to the Bluetooth hardware could use this flaw to issue a specially crafted ioctl function call and cause the system to crash.",
        "id": 1898
    },
    {
        "cve_id": "CVE-2018-13093",
        "code_before_change": "static int\nxfs_iget_cache_miss(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\txfs_trans_t\t\t*tp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_inode\t**ipp,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\txfs_agino_t\t\tagino = XFS_INO_TO_AGINO(mp, ino);\n\tint\t\t\tiflags;\n\n\tip = xfs_inode_alloc(mp, ino);\n\tif (!ip)\n\t\treturn -ENOMEM;\n\n\terror = xfs_iread(mp, tp, ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\tif (!xfs_inode_verify_forks(ip)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto out_destroy;\n\t}\n\n\ttrace_xfs_iget_miss(ip);\n\n\n\t/*\n\t * If we are allocating a new inode, then check what was returned is\n\t * actually a free, empty inode. If we are not allocating an inode,\n\t * the check we didn't find a free inode.\n\t */\n\tif (flags & XFS_IGET_CREATE) {\n\t\tif (VFS_I(ip)->i_mode != 0) {\n\t\t\txfs_warn(mp,\n\"Corruption detected! Free inode 0x%llx not marked free on disk\",\n\t\t\t\tino);\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto out_destroy;\n\t\t}\n\t\tif (ip->i_d.di_nblocks != 0) {\n\t\t\txfs_warn(mp,\n\"Corruption detected! Free inode 0x%llx has blocks allocated!\",\n\t\t\t\tino);\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto out_destroy;\n\t\t}\n\t} else if (VFS_I(ip)->i_mode == 0) {\n\t\terror = -ENOENT;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Preload the radix tree so we can insert safely under the\n\t * write spinlock. Note that we cannot sleep inside the preload\n\t * region. Since we can be called from transaction context, don't\n\t * recurse into the file system.\n\t */\n\tif (radix_tree_preload(GFP_NOFS)) {\n\t\terror = -EAGAIN;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Because the inode hasn't been added to the radix-tree yet it can't\n\t * be found by another thread, so we can do the non-sleeping lock here.\n\t */\n\tif (lock_flags) {\n\t\tif (!xfs_ilock_nowait(ip, lock_flags))\n\t\t\tBUG();\n\t}\n\n\t/*\n\t * These values must be set before inserting the inode into the radix\n\t * tree as the moment it is inserted a concurrent lookup (allowed by the\n\t * RCU locking mechanism) can find it and that lookup must see that this\n\t * is an inode currently under construction (i.e. that XFS_INEW is set).\n\t * The ip->i_flags_lock that protects the XFS_INEW flag forms the\n\t * memory barrier that ensures this detection works correctly at lookup\n\t * time.\n\t */\n\tiflags = XFS_INEW;\n\tif (flags & XFS_IGET_DONTCACHE)\n\t\tiflags |= XFS_IDONTCACHE;\n\tip->i_udquot = NULL;\n\tip->i_gdquot = NULL;\n\tip->i_pdquot = NULL;\n\txfs_iflags_set(ip, iflags);\n\n\t/* insert the new inode */\n\tspin_lock(&pag->pag_ici_lock);\n\terror = radix_tree_insert(&pag->pag_ici_root, agino, ip);\n\tif (unlikely(error)) {\n\t\tWARN_ON(error != -EEXIST);\n\t\tXFS_STATS_INC(mp, xs_ig_dup);\n\t\terror = -EAGAIN;\n\t\tgoto out_preload_end;\n\t}\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\n\t*ipp = ip;\n\treturn 0;\n\nout_preload_end:\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\nout_destroy:\n\t__destroy_inode(VFS_I(ip));\n\txfs_inode_free(ip);\n\treturn error;\n}",
        "code_after_change": "static int\nxfs_iget_cache_miss(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\txfs_trans_t\t\t*tp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_inode\t**ipp,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\txfs_agino_t\t\tagino = XFS_INO_TO_AGINO(mp, ino);\n\tint\t\t\tiflags;\n\n\tip = xfs_inode_alloc(mp, ino);\n\tif (!ip)\n\t\treturn -ENOMEM;\n\n\terror = xfs_iread(mp, tp, ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\tif (!xfs_inode_verify_forks(ip)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto out_destroy;\n\t}\n\n\ttrace_xfs_iget_miss(ip);\n\n\n\t/*\n\t * Check the inode free state is valid. This also detects lookup\n\t * racing with unlinks.\n\t */\n\terror = xfs_iget_check_free_state(ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\t/*\n\t * Preload the radix tree so we can insert safely under the\n\t * write spinlock. Note that we cannot sleep inside the preload\n\t * region. Since we can be called from transaction context, don't\n\t * recurse into the file system.\n\t */\n\tif (radix_tree_preload(GFP_NOFS)) {\n\t\terror = -EAGAIN;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Because the inode hasn't been added to the radix-tree yet it can't\n\t * be found by another thread, so we can do the non-sleeping lock here.\n\t */\n\tif (lock_flags) {\n\t\tif (!xfs_ilock_nowait(ip, lock_flags))\n\t\t\tBUG();\n\t}\n\n\t/*\n\t * These values must be set before inserting the inode into the radix\n\t * tree as the moment it is inserted a concurrent lookup (allowed by the\n\t * RCU locking mechanism) can find it and that lookup must see that this\n\t * is an inode currently under construction (i.e. that XFS_INEW is set).\n\t * The ip->i_flags_lock that protects the XFS_INEW flag forms the\n\t * memory barrier that ensures this detection works correctly at lookup\n\t * time.\n\t */\n\tiflags = XFS_INEW;\n\tif (flags & XFS_IGET_DONTCACHE)\n\t\tiflags |= XFS_IDONTCACHE;\n\tip->i_udquot = NULL;\n\tip->i_gdquot = NULL;\n\tip->i_pdquot = NULL;\n\txfs_iflags_set(ip, iflags);\n\n\t/* insert the new inode */\n\tspin_lock(&pag->pag_ici_lock);\n\terror = radix_tree_insert(&pag->pag_ici_root, agino, ip);\n\tif (unlikely(error)) {\n\t\tWARN_ON(error != -EEXIST);\n\t\tXFS_STATS_INC(mp, xs_ig_dup);\n\t\terror = -EAGAIN;\n\t\tgoto out_preload_end;\n\t}\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\n\t*ipp = ip;\n\treturn 0;\n\nout_preload_end:\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\nout_destroy:\n\t__destroy_inode(VFS_I(ip));\n\txfs_inode_free(ip);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,29 +30,12 @@\n \n \n \t/*\n-\t * If we are allocating a new inode, then check what was returned is\n-\t * actually a free, empty inode. If we are not allocating an inode,\n-\t * the check we didn't find a free inode.\n+\t * Check the inode free state is valid. This also detects lookup\n+\t * racing with unlinks.\n \t */\n-\tif (flags & XFS_IGET_CREATE) {\n-\t\tif (VFS_I(ip)->i_mode != 0) {\n-\t\t\txfs_warn(mp,\n-\"Corruption detected! Free inode 0x%llx not marked free on disk\",\n-\t\t\t\tino);\n-\t\t\terror = -EFSCORRUPTED;\n-\t\t\tgoto out_destroy;\n-\t\t}\n-\t\tif (ip->i_d.di_nblocks != 0) {\n-\t\t\txfs_warn(mp,\n-\"Corruption detected! Free inode 0x%llx has blocks allocated!\",\n-\t\t\t\tino);\n-\t\t\terror = -EFSCORRUPTED;\n-\t\t\tgoto out_destroy;\n-\t\t}\n-\t} else if (VFS_I(ip)->i_mode == 0) {\n-\t\terror = -ENOENT;\n+\terror = xfs_iget_check_free_state(ip, flags);\n+\tif (error)\n \t\tgoto out_destroy;\n-\t}\n \n \t/*\n \t * Preload the radix tree so we can insert safely under the",
        "function_modified_lines": {
            "added": [
                "\t * Check the inode free state is valid. This also detects lookup",
                "\t * racing with unlinks.",
                "\terror = xfs_iget_check_free_state(ip, flags);",
                "\tif (error)"
            ],
            "deleted": [
                "\t * If we are allocating a new inode, then check what was returned is",
                "\t * actually a free, empty inode. If we are not allocating an inode,",
                "\t * the check we didn't find a free inode.",
                "\tif (flags & XFS_IGET_CREATE) {",
                "\t\tif (VFS_I(ip)->i_mode != 0) {",
                "\t\t\txfs_warn(mp,",
                "\"Corruption detected! Free inode 0x%llx not marked free on disk\",",
                "\t\t\t\tino);",
                "\t\t\terror = -EFSCORRUPTED;",
                "\t\t\tgoto out_destroy;",
                "\t\t}",
                "\t\tif (ip->i_d.di_nblocks != 0) {",
                "\t\t\txfs_warn(mp,",
                "\"Corruption detected! Free inode 0x%llx has blocks allocated!\",",
                "\t\t\t\tino);",
                "\t\t\terror = -EFSCORRUPTED;",
                "\t\t\tgoto out_destroy;",
                "\t\t}",
                "\t} else if (VFS_I(ip)->i_mode == 0) {",
                "\t\terror = -ENOENT;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in fs/xfs/xfs_icache.c in the Linux kernel through 4.17.3. There is a NULL pointer dereference and panic in lookup_slow() on a NULL inode->i_ops pointer when doing pathwalks on a corrupted xfs image. This occurs because of a lack of proper validation that cached inodes are free during allocation.",
        "id": 1667
    },
    {
        "cve_id": "CVE-2018-13093",
        "code_before_change": "static int\nxfs_iget_cache_hit(\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_inode\t*ip,\n\txfs_ino_t\t\tino,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags) __releases(RCU)\n{\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tint\t\t\terror;\n\n\t/*\n\t * check for re-use of an inode within an RCU grace period due to the\n\t * radix tree nodes not being updated yet. We monitor for this by\n\t * setting the inode number to zero before freeing the inode structure.\n\t * If the inode has been reallocated and set up, then the inode number\n\t * will not match, so check for that, too.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (ip->i_ino != ino) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\n\t/*\n\t * If we are racing with another cache hit that is currently\n\t * instantiating this inode or currently recycling it out of\n\t * reclaimabe state, wait for the initialisation to complete\n\t * before continuing.\n\t *\n\t * XXX(hch): eventually we should do something equivalent to\n\t *\t     wait_on_inode to wait for these flags to be cleared\n\t *\t     instead of polling for it.\n\t */\n\tif (ip->i_flags & (XFS_INEW|XFS_IRECLAIM)) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\t/*\n\t * If lookup is racing with unlink return an error immediately.\n\t */\n\tif (VFS_I(ip)->i_mode == 0 && !(flags & XFS_IGET_CREATE)) {\n\t\terror = -ENOENT;\n\t\tgoto out_error;\n\t}\n\n\t/*\n\t * If IRECLAIMABLE is set, we've torn down the VFS inode already.\n\t * Need to carefully get it back into useable state.\n\t */\n\tif (ip->i_flags & XFS_IRECLAIMABLE) {\n\t\ttrace_xfs_iget_reclaim(ip);\n\n\t\tif (flags & XFS_IGET_INCORE) {\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/*\n\t\t * We need to set XFS_IRECLAIM to prevent xfs_reclaim_inode\n\t\t * from stomping over us while we recycle the inode.  We can't\n\t\t * clear the radix tree reclaimable tag yet as it requires\n\t\t * pag_ici_lock to be held exclusive.\n\t\t */\n\t\tip->i_flags |= XFS_IRECLAIM;\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\n\t\terror = xfs_reinit_inode(mp, inode);\n\t\tif (error) {\n\t\t\tbool wake;\n\t\t\t/*\n\t\t\t * Re-initializing the inode failed, and we are in deep\n\t\t\t * trouble.  Try to re-add it to the reclaim list.\n\t\t\t */\n\t\t\trcu_read_lock();\n\t\t\tspin_lock(&ip->i_flags_lock);\n\t\t\twake = !!__xfs_iflags_test(ip, XFS_INEW);\n\t\t\tip->i_flags &= ~(XFS_INEW | XFS_IRECLAIM);\n\t\t\tif (wake)\n\t\t\t\twake_up_bit(&ip->i_flags, __XFS_INEW_BIT);\n\t\t\tASSERT(ip->i_flags & XFS_IRECLAIMABLE);\n\t\t\ttrace_xfs_iget_reclaim_fail(ip);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tspin_lock(&pag->pag_ici_lock);\n\t\tspin_lock(&ip->i_flags_lock);\n\n\t\t/*\n\t\t * Clear the per-lifetime state in the inode as we are now\n\t\t * effectively a new inode and need to return to the initial\n\t\t * state before reuse occurs.\n\t\t */\n\t\tip->i_flags &= ~XFS_IRECLAIM_RESET_FLAGS;\n\t\tip->i_flags |= XFS_INEW;\n\t\txfs_inode_clear_reclaim_tag(pag, ip->i_ino);\n\t\tinode->i_state = I_NEW;\n\n\t\tASSERT(!rwsem_is_locked(&inode->i_rwsem));\n\t\tinit_rwsem(&inode->i_rwsem);\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\tspin_unlock(&pag->pag_ici_lock);\n\t} else {\n\t\t/* If the VFS inode is being torn down, pause and try again. */\n\t\tif (!igrab(inode)) {\n\t\t\ttrace_xfs_iget_skip(ip);\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/* We've got a live one. */\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\t\ttrace_xfs_iget_hit(ip);\n\t}\n\n\tif (lock_flags != 0)\n\t\txfs_ilock(ip, lock_flags);\n\n\tif (!(flags & XFS_IGET_INCORE))\n\t\txfs_iflags_clear(ip, XFS_ISTALE | XFS_IDONTCACHE);\n\tXFS_STATS_INC(mp, xs_ig_found);\n\n\treturn 0;\n\nout_error:\n\tspin_unlock(&ip->i_flags_lock);\n\trcu_read_unlock();\n\treturn error;\n}",
        "code_after_change": "static int\nxfs_iget_cache_hit(\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_inode\t*ip,\n\txfs_ino_t\t\tino,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags) __releases(RCU)\n{\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tint\t\t\terror;\n\n\t/*\n\t * check for re-use of an inode within an RCU grace period due to the\n\t * radix tree nodes not being updated yet. We monitor for this by\n\t * setting the inode number to zero before freeing the inode structure.\n\t * If the inode has been reallocated and set up, then the inode number\n\t * will not match, so check for that, too.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (ip->i_ino != ino) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\n\t/*\n\t * If we are racing with another cache hit that is currently\n\t * instantiating this inode or currently recycling it out of\n\t * reclaimabe state, wait for the initialisation to complete\n\t * before continuing.\n\t *\n\t * XXX(hch): eventually we should do something equivalent to\n\t *\t     wait_on_inode to wait for these flags to be cleared\n\t *\t     instead of polling for it.\n\t */\n\tif (ip->i_flags & (XFS_INEW|XFS_IRECLAIM)) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\t/*\n\t * Check the inode free state is valid. This also detects lookup\n\t * racing with unlinks.\n\t */\n\terror = xfs_iget_check_free_state(ip, flags);\n\tif (error)\n\t\tgoto out_error;\n\n\t/*\n\t * If IRECLAIMABLE is set, we've torn down the VFS inode already.\n\t * Need to carefully get it back into useable state.\n\t */\n\tif (ip->i_flags & XFS_IRECLAIMABLE) {\n\t\ttrace_xfs_iget_reclaim(ip);\n\n\t\tif (flags & XFS_IGET_INCORE) {\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/*\n\t\t * We need to set XFS_IRECLAIM to prevent xfs_reclaim_inode\n\t\t * from stomping over us while we recycle the inode.  We can't\n\t\t * clear the radix tree reclaimable tag yet as it requires\n\t\t * pag_ici_lock to be held exclusive.\n\t\t */\n\t\tip->i_flags |= XFS_IRECLAIM;\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\n\t\terror = xfs_reinit_inode(mp, inode);\n\t\tif (error) {\n\t\t\tbool wake;\n\t\t\t/*\n\t\t\t * Re-initializing the inode failed, and we are in deep\n\t\t\t * trouble.  Try to re-add it to the reclaim list.\n\t\t\t */\n\t\t\trcu_read_lock();\n\t\t\tspin_lock(&ip->i_flags_lock);\n\t\t\twake = !!__xfs_iflags_test(ip, XFS_INEW);\n\t\t\tip->i_flags &= ~(XFS_INEW | XFS_IRECLAIM);\n\t\t\tif (wake)\n\t\t\t\twake_up_bit(&ip->i_flags, __XFS_INEW_BIT);\n\t\t\tASSERT(ip->i_flags & XFS_IRECLAIMABLE);\n\t\t\ttrace_xfs_iget_reclaim_fail(ip);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tspin_lock(&pag->pag_ici_lock);\n\t\tspin_lock(&ip->i_flags_lock);\n\n\t\t/*\n\t\t * Clear the per-lifetime state in the inode as we are now\n\t\t * effectively a new inode and need to return to the initial\n\t\t * state before reuse occurs.\n\t\t */\n\t\tip->i_flags &= ~XFS_IRECLAIM_RESET_FLAGS;\n\t\tip->i_flags |= XFS_INEW;\n\t\txfs_inode_clear_reclaim_tag(pag, ip->i_ino);\n\t\tinode->i_state = I_NEW;\n\n\t\tASSERT(!rwsem_is_locked(&inode->i_rwsem));\n\t\tinit_rwsem(&inode->i_rwsem);\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\tspin_unlock(&pag->pag_ici_lock);\n\t} else {\n\t\t/* If the VFS inode is being torn down, pause and try again. */\n\t\tif (!igrab(inode)) {\n\t\t\ttrace_xfs_iget_skip(ip);\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/* We've got a live one. */\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\t\ttrace_xfs_iget_hit(ip);\n\t}\n\n\tif (lock_flags != 0)\n\t\txfs_ilock(ip, lock_flags);\n\n\tif (!(flags & XFS_IGET_INCORE))\n\t\txfs_iflags_clear(ip, XFS_ISTALE | XFS_IDONTCACHE);\n\tXFS_STATS_INC(mp, xs_ig_found);\n\n\treturn 0;\n\nout_error:\n\tspin_unlock(&ip->i_flags_lock);\n\trcu_read_unlock();\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -44,12 +44,12 @@\n \t}\n \n \t/*\n-\t * If lookup is racing with unlink return an error immediately.\n+\t * Check the inode free state is valid. This also detects lookup\n+\t * racing with unlinks.\n \t */\n-\tif (VFS_I(ip)->i_mode == 0 && !(flags & XFS_IGET_CREATE)) {\n-\t\terror = -ENOENT;\n+\terror = xfs_iget_check_free_state(ip, flags);\n+\tif (error)\n \t\tgoto out_error;\n-\t}\n \n \t/*\n \t * If IRECLAIMABLE is set, we've torn down the VFS inode already.",
        "function_modified_lines": {
            "added": [
                "\t * Check the inode free state is valid. This also detects lookup",
                "\t * racing with unlinks.",
                "\terror = xfs_iget_check_free_state(ip, flags);",
                "\tif (error)"
            ],
            "deleted": [
                "\t * If lookup is racing with unlink return an error immediately.",
                "\tif (VFS_I(ip)->i_mode == 0 && !(flags & XFS_IGET_CREATE)) {",
                "\t\terror = -ENOENT;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in fs/xfs/xfs_icache.c in the Linux kernel through 4.17.3. There is a NULL pointer dereference and panic in lookup_slow() on a NULL inode->i_ops pointer when doing pathwalks on a corrupted xfs image. This occurs because of a lack of proper validation that cached inodes are free during allocation.",
        "id": 1668
    },
    {
        "cve_id": "CVE-2023-32252",
        "code_before_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(work));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(work);\n\treturn 0;\n}",
        "code_after_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(conn));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(conn);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,7 +23,7 @@\n \n \trsp = smb2_get_msg(work->response_buf);\n \n-\tWARN_ON(ksmbd_conn_good(work));\n+\tWARN_ON(ksmbd_conn_good(conn));\n \n \trsp->StructureSize = cpu_to_le16(65);\n \tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n@@ -52,6 +52,6 @@\n \t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n \tconn->use_spnego = true;\n \n-\tksmbd_conn_set_need_negotiate(work);\n+\tksmbd_conn_set_need_negotiate(conn);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tWARN_ON(ksmbd_conn_good(conn));",
                "\tksmbd_conn_set_need_negotiate(conn);"
            ],
            "deleted": [
                "\tWARN_ON(ksmbd_conn_good(work));",
                "\tksmbd_conn_set_need_negotiate(work);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the handling of SMB2_LOGOFF commands. The issue results from the lack of proper validation of a pointer prior to accessing it. An attacker can leverage this vulnerability to create a denial-of-service condition on the system.",
        "id": 4031
    },
    {
        "cve_id": "CVE-2023-32252",
        "code_before_change": "static void ksmbd_conn_lock(struct ksmbd_conn *conn)\n{\n\tmutex_lock(&conn->srv_mutex);\n}",
        "code_after_change": "void ksmbd_conn_lock(struct ksmbd_conn *conn)\n{\n\tmutex_lock(&conn->srv_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-static void ksmbd_conn_lock(struct ksmbd_conn *conn)\n+void ksmbd_conn_lock(struct ksmbd_conn *conn)\n {\n \tmutex_lock(&conn->srv_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "void ksmbd_conn_lock(struct ksmbd_conn *conn)"
            ],
            "deleted": [
                "static void ksmbd_conn_lock(struct ksmbd_conn *conn)"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the handling of SMB2_LOGOFF commands. The issue results from the lack of proper validation of a pointer prior to accessing it. An attacker can leverage this vulnerability to create a denial-of-service condition on the system.",
        "id": 4026
    },
    {
        "cve_id": "CVE-2017-18216",
        "code_before_change": "static ssize_t o2nm_node_ipv4_address_store(struct config_item *item,\n\t\t\t\t\t    const char *page,\n\t\t\t\t\t    size_t count)\n{\n\tstruct o2nm_node *node = to_o2nm_node(item);\n\tstruct o2nm_cluster *cluster = to_o2nm_cluster_from_node(node);\n\tint ret, i;\n\tstruct rb_node **p, *parent;\n\tunsigned int octets[4];\n\t__be32 ipv4_addr = 0;\n\n\tret = sscanf(page, \"%3u.%3u.%3u.%3u\", &octets[3], &octets[2],\n\t\t     &octets[1], &octets[0]);\n\tif (ret != 4)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < ARRAY_SIZE(octets); i++) {\n\t\tif (octets[i] > 255)\n\t\t\treturn -ERANGE;\n\t\tbe32_add_cpu(&ipv4_addr, octets[i] << (i * 8));\n\t}\n\n\tret = 0;\n\twrite_lock(&cluster->cl_nodes_lock);\n\tif (o2nm_node_ip_tree_lookup(cluster, ipv4_addr, &p, &parent))\n\t\tret = -EEXIST;\n\telse if (test_and_set_bit(O2NM_NODE_ATTR_ADDRESS,\n\t\t\t&node->nd_set_attributes))\n\t\tret = -EBUSY;\n\telse {\n\t\trb_link_node(&node->nd_ip_node, parent, p);\n\t\trb_insert_color(&node->nd_ip_node, &cluster->cl_node_ip_tree);\n\t}\n\twrite_unlock(&cluster->cl_nodes_lock);\n\tif (ret)\n\t\treturn ret;\n\n\tmemcpy(&node->nd_ipv4_address, &ipv4_addr, sizeof(ipv4_addr));\n\n\treturn count;\n}",
        "code_after_change": "static ssize_t o2nm_node_ipv4_address_store(struct config_item *item,\n\t\t\t\t\t    const char *page,\n\t\t\t\t\t    size_t count)\n{\n\tstruct o2nm_node *node = to_o2nm_node(item);\n\tstruct o2nm_cluster *cluster;\n\tint ret, i;\n\tstruct rb_node **p, *parent;\n\tunsigned int octets[4];\n\t__be32 ipv4_addr = 0;\n\n\tret = sscanf(page, \"%3u.%3u.%3u.%3u\", &octets[3], &octets[2],\n\t\t     &octets[1], &octets[0]);\n\tif (ret != 4)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < ARRAY_SIZE(octets); i++) {\n\t\tif (octets[i] > 255)\n\t\t\treturn -ERANGE;\n\t\tbe32_add_cpu(&ipv4_addr, octets[i] << (i * 8));\n\t}\n\n\to2nm_lock_subsystem();\n\tcluster = to_o2nm_cluster_from_node(node);\n\tif (!cluster) {\n\t\to2nm_unlock_subsystem();\n\t\treturn -EINVAL;\n\t}\n\n\tret = 0;\n\twrite_lock(&cluster->cl_nodes_lock);\n\tif (o2nm_node_ip_tree_lookup(cluster, ipv4_addr, &p, &parent))\n\t\tret = -EEXIST;\n\telse if (test_and_set_bit(O2NM_NODE_ATTR_ADDRESS,\n\t\t\t&node->nd_set_attributes))\n\t\tret = -EBUSY;\n\telse {\n\t\trb_link_node(&node->nd_ip_node, parent, p);\n\t\trb_insert_color(&node->nd_ip_node, &cluster->cl_node_ip_tree);\n\t}\n\twrite_unlock(&cluster->cl_nodes_lock);\n\to2nm_unlock_subsystem();\n\n\tif (ret)\n\t\treturn ret;\n\n\tmemcpy(&node->nd_ipv4_address, &ipv4_addr, sizeof(ipv4_addr));\n\n\treturn count;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t\t\t\t\t    size_t count)\n {\n \tstruct o2nm_node *node = to_o2nm_node(item);\n-\tstruct o2nm_cluster *cluster = to_o2nm_cluster_from_node(node);\n+\tstruct o2nm_cluster *cluster;\n \tint ret, i;\n \tstruct rb_node **p, *parent;\n \tunsigned int octets[4];\n@@ -20,6 +20,13 @@\n \t\tbe32_add_cpu(&ipv4_addr, octets[i] << (i * 8));\n \t}\n \n+\to2nm_lock_subsystem();\n+\tcluster = to_o2nm_cluster_from_node(node);\n+\tif (!cluster) {\n+\t\to2nm_unlock_subsystem();\n+\t\treturn -EINVAL;\n+\t}\n+\n \tret = 0;\n \twrite_lock(&cluster->cl_nodes_lock);\n \tif (o2nm_node_ip_tree_lookup(cluster, ipv4_addr, &p, &parent))\n@@ -32,6 +39,8 @@\n \t\trb_insert_color(&node->nd_ip_node, &cluster->cl_node_ip_tree);\n \t}\n \twrite_unlock(&cluster->cl_nodes_lock);\n+\to2nm_unlock_subsystem();\n+\n \tif (ret)\n \t\treturn ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct o2nm_cluster *cluster;",
                "\to2nm_lock_subsystem();",
                "\tcluster = to_o2nm_cluster_from_node(node);",
                "\tif (!cluster) {",
                "\t\to2nm_unlock_subsystem();",
                "\t\treturn -EINVAL;",
                "\t}",
                "",
                "\to2nm_unlock_subsystem();",
                ""
            ],
            "deleted": [
                "\tstruct o2nm_cluster *cluster = to_o2nm_cluster_from_node(node);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "In fs/ocfs2/cluster/nodemanager.c in the Linux kernel before 4.15, local users can cause a denial of service (NULL pointer dereference and BUG) because a required mutex is not used.",
        "id": 1402
    },
    {
        "cve_id": "CVE-2018-16871",
        "code_before_change": "static __be32\nnfsd4_verify_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t  stateid_t *src_stateid, struct file **src,\n\t\t  stateid_t *dst_stateid, struct file **dst)\n{\n\t__be32 status;\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->save_fh,\n\t\t\t\t\t    src_stateid, RD_STATE, src, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: %s: couldn't process src stateid!\\n\", __func__);\n\t\tgoto out;\n\t}\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t    dst_stateid, WR_STATE, dst, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: %s: couldn't process dst stateid!\\n\", __func__);\n\t\tgoto out_put_src;\n\t}\n\n\t/* fix up for NFS-specific error code */\n\tif (!S_ISREG(file_inode(*src)->i_mode) ||\n\t    !S_ISREG(file_inode(*dst)->i_mode)) {\n\t\tstatus = nfserr_wrong_type;\n\t\tgoto out_put_dst;\n\t}\n\nout:\n\treturn status;\nout_put_dst:\n\tfput(*dst);\nout_put_src:\n\tfput(*src);\n\tgoto out;\n}",
        "code_after_change": "static __be32\nnfsd4_verify_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t  stateid_t *src_stateid, struct file **src,\n\t\t  stateid_t *dst_stateid, struct file **dst)\n{\n\t__be32 status;\n\n\tif (!cstate->save_fh.fh_dentry)\n\t\treturn nfserr_nofilehandle;\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->save_fh,\n\t\t\t\t\t    src_stateid, RD_STATE, src, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: %s: couldn't process src stateid!\\n\", __func__);\n\t\tgoto out;\n\t}\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t    dst_stateid, WR_STATE, dst, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: %s: couldn't process dst stateid!\\n\", __func__);\n\t\tgoto out_put_src;\n\t}\n\n\t/* fix up for NFS-specific error code */\n\tif (!S_ISREG(file_inode(*src)->i_mode) ||\n\t    !S_ISREG(file_inode(*dst)->i_mode)) {\n\t\tstatus = nfserr_wrong_type;\n\t\tgoto out_put_dst;\n\t}\n\nout:\n\treturn status;\nout_put_dst:\n\tfput(*dst);\nout_put_src:\n\tfput(*src);\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,9 @@\n \t\t  stateid_t *dst_stateid, struct file **dst)\n {\n \t__be32 status;\n+\n+\tif (!cstate->save_fh.fh_dentry)\n+\t\treturn nfserr_nofilehandle;\n \n \tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->save_fh,\n \t\t\t\t\t    src_stateid, RD_STATE, src, NULL);",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (!cstate->save_fh.fh_dentry)",
                "\t\treturn nfserr_nofilehandle;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A flaw was found in the Linux kernel's NFS implementation, all versions 3.x and all versions 4.x up to 4.20. An attacker, who is able to mount an exported NFS filesystem, is able to trigger a null pointer dereference by using an invalid NFS sequence. This can panic the machine and deny access to the NFS server. Any outstanding disk writes to the NFS server will be lost.",
        "id": 1716
    },
    {
        "cve_id": "CVE-2020-12364",
        "code_before_change": "void intel_guc_ads_reset(struct intel_guc *guc)\n{\n\tif (!guc->ads_vma)\n\t\treturn;\n\t__guc_ads_init(guc);\n}",
        "code_after_change": "void intel_guc_ads_reset(struct intel_guc *guc)\n{\n\tif (!guc->ads_vma)\n\t\treturn;\n\n\t__guc_ads_init(guc);\n\n\tguc_ads_private_data_reset(guc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,5 +2,8 @@\n {\n \tif (!guc->ads_vma)\n \t\treturn;\n+\n \t__guc_ads_init(guc);\n+\n+\tguc_ads_private_data_reset(guc);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "",
                "\tguc_ads_private_data_reset(guc);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "Null pointer reference in some Intel(R) Graphics Drivers for Windows* before version 26.20.100.7212 and before version Linux kernel version 5.5 may allow a privileged user to potentially enable a denial of service via local access.",
        "id": 2468
    },
    {
        "cve_id": "CVE-2019-19815",
        "code_before_change": "static int f2fs_set_data_page_dirty(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode = mapping->host;\n\n\ttrace_f2fs_set_page_dirty(page, DATA);\n\n\tif (!PageUptodate(page))\n\t\tSetPageUptodate(page);\n\n\tif (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {\n\t\tif (!IS_ATOMIC_WRITTEN_PAGE(page)) {\n\t\t\tf2fs_register_inmem_page(inode, page);\n\t\t\treturn 1;\n\t\t}\n\t\t/*\n\t\t * Previously, this page has been registered, we just\n\t\t * return here.\n\t\t */\n\t\treturn 0;\n\t}\n\n\tif (!PageDirty(page)) {\n\t\t__set_page_dirty_nobuffers(page);\n\t\tf2fs_update_dirty_page(inode, page);\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int f2fs_set_data_page_dirty(struct page *page)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\n\ttrace_f2fs_set_page_dirty(page, DATA);\n\n\tif (!PageUptodate(page))\n\t\tSetPageUptodate(page);\n\tif (PageSwapCache(page))\n\t\treturn __set_page_dirty_nobuffers(page);\n\n\tif (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {\n\t\tif (!IS_ATOMIC_WRITTEN_PAGE(page)) {\n\t\t\tf2fs_register_inmem_page(inode, page);\n\t\t\treturn 1;\n\t\t}\n\t\t/*\n\t\t * Previously, this page has been registered, we just\n\t\t * return here.\n\t\t */\n\t\treturn 0;\n\t}\n\n\tif (!PageDirty(page)) {\n\t\t__set_page_dirty_nobuffers(page);\n\t\tf2fs_update_dirty_page(inode, page);\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,13 @@\n static int f2fs_set_data_page_dirty(struct page *page)\n {\n-\tstruct address_space *mapping = page->mapping;\n-\tstruct inode *inode = mapping->host;\n+\tstruct inode *inode = page_file_mapping(page)->host;\n \n \ttrace_f2fs_set_page_dirty(page, DATA);\n \n \tif (!PageUptodate(page))\n \t\tSetPageUptodate(page);\n+\tif (PageSwapCache(page))\n+\t\treturn __set_page_dirty_nobuffers(page);\n \n \tif (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {\n \t\tif (!IS_ATOMIC_WRITTEN_PAGE(page)) {",
        "function_modified_lines": {
            "added": [
                "\tstruct inode *inode = page_file_mapping(page)->host;",
                "\tif (PageSwapCache(page))",
                "\t\treturn __set_page_dirty_nobuffers(page);"
            ],
            "deleted": [
                "\tstruct address_space *mapping = page->mapping;",
                "\tstruct inode *inode = mapping->host;"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "In the Linux kernel 5.0.21, mounting a crafted f2fs filesystem image can cause a NULL pointer dereference in f2fs_recover_fsync_data in fs/f2fs/recovery.c. This is related to F2FS_P_SB in fs/f2fs/f2fs.h.",
        "id": 2250
    },
    {
        "cve_id": "CVE-2019-19815",
        "code_before_change": "static inline struct f2fs_sb_info *F2FS_P_SB(struct page *page)\n{\n\treturn F2FS_M_SB(page->mapping);\n}",
        "code_after_change": "static inline struct f2fs_sb_info *F2FS_P_SB(struct page *page)\n{\n\treturn F2FS_M_SB(page_file_mapping(page));\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n static inline struct f2fs_sb_info *F2FS_P_SB(struct page *page)\n {\n-\treturn F2FS_M_SB(page->mapping);\n+\treturn F2FS_M_SB(page_file_mapping(page));\n }",
        "function_modified_lines": {
            "added": [
                "\treturn F2FS_M_SB(page_file_mapping(page));"
            ],
            "deleted": [
                "\treturn F2FS_M_SB(page->mapping);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "In the Linux kernel 5.0.21, mounting a crafted f2fs filesystem image can cause a NULL pointer dereference in f2fs_recover_fsync_data in fs/f2fs/recovery.c. This is related to F2FS_P_SB in fs/f2fs/f2fs.h.",
        "id": 2253
    },
    {
        "cve_id": "CVE-2019-19815",
        "code_before_change": "static int f2fs_read_data_page(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page->mapping->host;\n\tint ret = -EAGAIN;\n\n\ttrace_f2fs_readpage(page, DATA);\n\n\t/* If the file has inline data, try to read it directly */\n\tif (f2fs_has_inline_data(inode))\n\t\tret = f2fs_read_inline_data(inode, page);\n\tif (ret == -EAGAIN)\n\t\tret = f2fs_mpage_readpages(page->mapping, NULL, page, 1, false);\n\treturn ret;\n}",
        "code_after_change": "static int f2fs_read_data_page(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tint ret = -EAGAIN;\n\n\ttrace_f2fs_readpage(page, DATA);\n\n\t/* If the file has inline data, try to read it directly */\n\tif (f2fs_has_inline_data(inode))\n\t\tret = f2fs_read_inline_data(inode, page);\n\tif (ret == -EAGAIN)\n\t\tret = f2fs_mpage_readpages(page_file_mapping(page),\n\t\t\t\t\t\tNULL, page, 1, false);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static int f2fs_read_data_page(struct file *file, struct page *page)\n {\n-\tstruct inode *inode = page->mapping->host;\n+\tstruct inode *inode = page_file_mapping(page)->host;\n \tint ret = -EAGAIN;\n \n \ttrace_f2fs_readpage(page, DATA);\n@@ -9,6 +9,7 @@\n \tif (f2fs_has_inline_data(inode))\n \t\tret = f2fs_read_inline_data(inode, page);\n \tif (ret == -EAGAIN)\n-\t\tret = f2fs_mpage_readpages(page->mapping, NULL, page, 1, false);\n+\t\tret = f2fs_mpage_readpages(page_file_mapping(page),\n+\t\t\t\t\t\tNULL, page, 1, false);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct inode *inode = page_file_mapping(page)->host;",
                "\t\tret = f2fs_mpage_readpages(page_file_mapping(page),",
                "\t\t\t\t\t\tNULL, page, 1, false);"
            ],
            "deleted": [
                "\tstruct inode *inode = page->mapping->host;",
                "\t\tret = f2fs_mpage_readpages(page->mapping, NULL, page, 1, false);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "In the Linux kernel 5.0.21, mounting a crafted f2fs filesystem image can cause a NULL pointer dereference in f2fs_recover_fsync_data in fs/f2fs/recovery.c. This is related to F2FS_P_SB in fs/f2fs/f2fs.h.",
        "id": 2252
    },
    {
        "cve_id": "CVE-2019-16233",
        "code_before_change": "static int\nqla2x00_probe_one(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tint\tret = -ENODEV;\n\tstruct Scsi_Host *host;\n\tscsi_qla_host_t *base_vha = NULL;\n\tstruct qla_hw_data *ha;\n\tchar pci_info[30];\n\tchar fw_str[30], wq_name[30];\n\tstruct scsi_host_template *sht;\n\tint bars, mem_only = 0;\n\tuint16_t req_length = 0, rsp_length = 0;\n\tstruct req_que *req = NULL;\n\tstruct rsp_que *rsp = NULL;\n\tint i;\n\n\tbars = pci_select_bars(pdev, IORESOURCE_MEM | IORESOURCE_IO);\n\tsht = &qla2xxx_driver_template;\n\tif (pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2422 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2432 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP8432 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP5422 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP5432 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2532 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP8001 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP8021 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2031 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP8031 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISPF001 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP8044 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2071 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2271 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2261 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2081 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2281 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2089 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2289) {\n\t\tbars = pci_select_bars(pdev, IORESOURCE_MEM);\n\t\tmem_only = 1;\n\t\tql_dbg_pci(ql_dbg_init, pdev, 0x0007,\n\t\t    \"Mem only adapter.\\n\");\n\t}\n\tql_dbg_pci(ql_dbg_init, pdev, 0x0008,\n\t    \"Bars=%d.\\n\", bars);\n\n\tif (mem_only) {\n\t\tif (pci_enable_device_mem(pdev))\n\t\t\treturn ret;\n\t} else {\n\t\tif (pci_enable_device(pdev))\n\t\t\treturn ret;\n\t}\n\n\t/* This may fail but that's ok */\n\tpci_enable_pcie_error_reporting(pdev);\n\n\t/* Turn off T10-DIF when FC-NVMe is enabled */\n\tif (ql2xnvmeenable)\n\t\tql2xenabledif = 0;\n\n\tha = kzalloc(sizeof(struct qla_hw_data), GFP_KERNEL);\n\tif (!ha) {\n\t\tql_log_pci(ql_log_fatal, pdev, 0x0009,\n\t\t    \"Unable to allocate memory for ha.\\n\");\n\t\tgoto disable_device;\n\t}\n\tql_dbg_pci(ql_dbg_init, pdev, 0x000a,\n\t    \"Memory allocated for ha=%p.\\n\", ha);\n\tha->pdev = pdev;\n\tINIT_LIST_HEAD(&ha->tgt.q_full_list);\n\tspin_lock_init(&ha->tgt.q_full_lock);\n\tspin_lock_init(&ha->tgt.sess_lock);\n\tspin_lock_init(&ha->tgt.atio_lock);\n\n\tatomic_set(&ha->nvme_active_aen_cnt, 0);\n\n\t/* Clear our data area */\n\tha->bars = bars;\n\tha->mem_only = mem_only;\n\tspin_lock_init(&ha->hardware_lock);\n\tspin_lock_init(&ha->vport_slock);\n\tmutex_init(&ha->selflogin_lock);\n\tmutex_init(&ha->optrom_mutex);\n\n\t/* Set ISP-type information. */\n\tqla2x00_set_isp_flags(ha);\n\n\t/* Set EEH reset type to fundamental if required by hba */\n\tif (IS_QLA24XX(ha) || IS_QLA25XX(ha) || IS_QLA81XX(ha) ||\n\t    IS_QLA83XX(ha) || IS_QLA27XX(ha) || IS_QLA28XX(ha))\n\t\tpdev->needs_freset = 1;\n\n\tha->prev_topology = 0;\n\tha->init_cb_size = sizeof(init_cb_t);\n\tha->link_data_rate = PORT_SPEED_UNKNOWN;\n\tha->optrom_size = OPTROM_SIZE_2300;\n\tha->max_exchg = FW_MAX_EXCHANGES_CNT;\n\tatomic_set(&ha->num_pend_mbx_stage1, 0);\n\tatomic_set(&ha->num_pend_mbx_stage2, 0);\n\tatomic_set(&ha->num_pend_mbx_stage3, 0);\n\tatomic_set(&ha->zio_threshold, DEFAULT_ZIO_THRESHOLD);\n\tha->last_zio_threshold = DEFAULT_ZIO_THRESHOLD;\n\n\t/* Assign ISP specific operations. */\n\tif (IS_QLA2100(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2100;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT_2100;\n\t\treq_length = REQUEST_ENTRY_CNT_2100;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2100;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2100;\n\t\tha->gid_list_info_size = 4;\n\t\tha->flash_conf_off = ~0;\n\t\tha->flash_data_off = ~0;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t\tha->isp_ops = &qla2100_isp_ops;\n\t} else if (IS_QLA2200(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2100;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT_2200;\n\t\treq_length = REQUEST_ENTRY_CNT_2200;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2100;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2100;\n\t\tha->gid_list_info_size = 4;\n\t\tha->flash_conf_off = ~0;\n\t\tha->flash_data_off = ~0;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t\tha->isp_ops = &qla2100_isp_ops;\n\t} else if (IS_QLA23XX(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2100;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_2200;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2300;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->gid_list_info_size = 6;\n\t\tif (IS_QLA2322(ha) || IS_QLA6322(ha))\n\t\t\tha->optrom_size = OPTROM_SIZE_2322;\n\t\tha->flash_conf_off = ~0;\n\t\tha->flash_data_off = ~0;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t\tha->isp_ops = &qla2300_isp_ops;\n\t} else if (IS_QLA24XX_TYPE(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_24XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2300;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_24xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_24XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA24XX;\n\t\tha->isp_ops = &qla24xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA;\n\t\tha->nvram_conf_off = FARX_ACCESS_NVRAM_CONF;\n\t\tha->nvram_data_off = FARX_ACCESS_NVRAM_DATA;\n\t} else if (IS_QLA25XX(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_24XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2300;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_24xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_25XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla25xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA;\n\t\tha->nvram_conf_off = FARX_ACCESS_NVRAM_CONF;\n\t\tha->nvram_data_off = FARX_ACCESS_NVRAM_DATA;\n\t} else if (IS_QLA81XX(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_24XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2300;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_81XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla81xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF_81XX;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA_81XX;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t} else if (IS_QLA82XX(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_82XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_82XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_82XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla82xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA;\n\t\tha->nvram_conf_off = FARX_ACCESS_NVRAM_CONF;\n\t\tha->nvram_data_off = FARX_ACCESS_NVRAM_DATA;\n\t} else if (IS_QLA8044(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_82XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_82XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_83XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla8044_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA;\n\t\tha->nvram_conf_off = FARX_ACCESS_NVRAM_CONF;\n\t\tha->nvram_data_off = FARX_ACCESS_NVRAM_DATA;\n\t} else if (IS_QLA83XX(ha)) {\n\t\tha->portnum = PCI_FUNC(ha->pdev->devfn);\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_83XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_83XX;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_83XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla83xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF_81XX;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA_81XX;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t}  else if (IS_QLAFX00(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_FX00;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT_FX00;\n\t\tha->aen_mbx_count = AEN_MAILBOX_REGISTER_COUNT_FX00;\n\t\treq_length = REQUEST_ENTRY_CNT_FX00;\n\t\trsp_length = RESPONSE_ENTRY_CNT_FX00;\n\t\tha->isp_ops = &qlafx00_isp_ops;\n\t\tha->port_down_retry_count = 30; /* default value */\n\t\tha->mr.fw_hbt_cnt = QLAFX00_HEARTBEAT_INTERVAL;\n\t\tha->mr.fw_reset_timer_tick = QLAFX00_RESET_INTERVAL;\n\t\tha->mr.fw_critemp_timer_tick = QLAFX00_CRITEMP_INTERVAL;\n\t\tha->mr.fw_hbt_en = 1;\n\t\tha->mr.host_info_resend = false;\n\t\tha->mr.hinfo_resend_timer_tick = QLAFX00_HINFO_RESEND_INTERVAL;\n\t} else if (IS_QLA27XX(ha)) {\n\t\tha->portnum = PCI_FUNC(ha->pdev->devfn);\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_83XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_83XX;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_83XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla27xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF_81XX;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA_81XX;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t} else if (IS_QLA28XX(ha)) {\n\t\tha->portnum = PCI_FUNC(ha->pdev->devfn);\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_24XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2300;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_28XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla27xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF_28XX;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA_28XX;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t}\n\n\tql_dbg_pci(ql_dbg_init, pdev, 0x001e,\n\t    \"mbx_count=%d, req_length=%d, \"\n\t    \"rsp_length=%d, max_loop_id=%d, init_cb_size=%d, \"\n\t    \"gid_list_info_size=%d, optrom_size=%d, nvram_npiv_size=%d, \"\n\t    \"max_fibre_devices=%d.\\n\",\n\t    ha->mbx_count, req_length, rsp_length, ha->max_loop_id,\n\t    ha->init_cb_size, ha->gid_list_info_size, ha->optrom_size,\n\t    ha->nvram_npiv_size, ha->max_fibre_devices);\n\tql_dbg_pci(ql_dbg_init, pdev, 0x001f,\n\t    \"isp_ops=%p, flash_conf_off=%d, \"\n\t    \"flash_data_off=%d, nvram_conf_off=%d, nvram_data_off=%d.\\n\",\n\t    ha->isp_ops, ha->flash_conf_off, ha->flash_data_off,\n\t    ha->nvram_conf_off, ha->nvram_data_off);\n\n\t/* Configure PCI I/O space */\n\tret = ha->isp_ops->iospace_config(ha);\n\tif (ret)\n\t\tgoto iospace_config_failed;\n\n\tql_log_pci(ql_log_info, pdev, 0x001d,\n\t    \"Found an ISP%04X irq %d iobase 0x%p.\\n\",\n\t    pdev->device, pdev->irq, ha->iobase);\n\tmutex_init(&ha->vport_lock);\n\tmutex_init(&ha->mq_lock);\n\tinit_completion(&ha->mbx_cmd_comp);\n\tcomplete(&ha->mbx_cmd_comp);\n\tinit_completion(&ha->mbx_intr_comp);\n\tinit_completion(&ha->dcbx_comp);\n\tinit_completion(&ha->lb_portup_comp);\n\n\tset_bit(0, (unsigned long *) ha->vp_idx_map);\n\n\tqla2x00_config_dma_addressing(ha);\n\tql_dbg_pci(ql_dbg_init, pdev, 0x0020,\n\t    \"64 Bit addressing is %s.\\n\",\n\t    ha->flags.enable_64bit_addressing ? \"enable\" :\n\t    \"disable\");\n\tret = qla2x00_mem_alloc(ha, req_length, rsp_length, &req, &rsp);\n\tif (ret) {\n\t\tql_log_pci(ql_log_fatal, pdev, 0x0031,\n\t\t    \"Failed to allocate memory for adapter, aborting.\\n\");\n\n\t\tgoto probe_hw_failed;\n\t}\n\n\treq->max_q_depth = MAX_Q_DEPTH;\n\tif (ql2xmaxqdepth != 0 && ql2xmaxqdepth <= 0xffffU)\n\t\treq->max_q_depth = ql2xmaxqdepth;\n\n\n\tbase_vha = qla2x00_create_host(sht, ha);\n\tif (!base_vha) {\n\t\tret = -ENOMEM;\n\t\tgoto probe_hw_failed;\n\t}\n\n\tpci_set_drvdata(pdev, base_vha);\n\tset_bit(PFLG_DRIVER_PROBING, &base_vha->pci_flags);\n\n\thost = base_vha->host;\n\tbase_vha->req = req;\n\tif (IS_QLA2XXX_MIDTYPE(ha))\n\t\tbase_vha->mgmt_svr_loop_id =\n\t\t\tqla2x00_reserve_mgmt_server_loop_id(base_vha);\n\telse\n\t\tbase_vha->mgmt_svr_loop_id = MANAGEMENT_SERVER +\n\t\t\t\t\t\tbase_vha->vp_idx;\n\n\t/* Setup fcport template structure. */\n\tha->mr.fcport.vha = base_vha;\n\tha->mr.fcport.port_type = FCT_UNKNOWN;\n\tha->mr.fcport.loop_id = FC_NO_LOOP_ID;\n\tqla2x00_set_fcport_state(&ha->mr.fcport, FCS_UNCONFIGURED);\n\tha->mr.fcport.supported_classes = FC_COS_UNSPECIFIED;\n\tha->mr.fcport.scan_state = 1;\n\n\t/* Set the SG table size based on ISP type */\n\tif (!IS_FWI2_CAPABLE(ha)) {\n\t\tif (IS_QLA2100(ha))\n\t\t\thost->sg_tablesize = 32;\n\t} else {\n\t\tif (!IS_QLA82XX(ha))\n\t\t\thost->sg_tablesize = QLA_SG_ALL;\n\t}\n\thost->max_id = ha->max_fibre_devices;\n\thost->cmd_per_lun = 3;\n\thost->unique_id = host->host_no;\n\tif (IS_T10_PI_CAPABLE(ha) && ql2xenabledif)\n\t\thost->max_cmd_len = 32;\n\telse\n\t\thost->max_cmd_len = MAX_CMDSZ;\n\thost->max_channel = MAX_BUSES - 1;\n\t/* Older HBAs support only 16-bit LUNs */\n\tif (!IS_QLAFX00(ha) && !IS_FWI2_CAPABLE(ha) &&\n\t    ql2xmaxlun > 0xffff)\n\t\thost->max_lun = 0xffff;\n\telse\n\t\thost->max_lun = ql2xmaxlun;\n\thost->transportt = qla2xxx_transport_template;\n\tsht->vendor_id = (SCSI_NL_VID_TYPE_PCI | PCI_VENDOR_ID_QLOGIC);\n\n\tql_dbg(ql_dbg_init, base_vha, 0x0033,\n\t    \"max_id=%d this_id=%d \"\n\t    \"cmd_per_len=%d unique_id=%d max_cmd_len=%d max_channel=%d \"\n\t    \"max_lun=%llu transportt=%p, vendor_id=%llu.\\n\", host->max_id,\n\t    host->this_id, host->cmd_per_lun, host->unique_id,\n\t    host->max_cmd_len, host->max_channel, host->max_lun,\n\t    host->transportt, sht->vendor_id);\n\n\tINIT_WORK(&base_vha->iocb_work, qla2x00_iocb_work_fn);\n\n\t/* Set up the irqs */\n\tret = qla2x00_request_irqs(ha, rsp);\n\tif (ret)\n\t\tgoto probe_failed;\n\n\t/* Alloc arrays of request and response ring ptrs */\n\tret = qla2x00_alloc_queues(ha, req, rsp);\n\tif (ret) {\n\t\tql_log(ql_log_fatal, base_vha, 0x003d,\n\t\t    \"Failed to allocate memory for queue pointers...\"\n\t\t    \"aborting.\\n\");\n\t\tret = -ENODEV;\n\t\tgoto probe_failed;\n\t}\n\n\tif (ha->mqenable) {\n\t\t/* number of hardware queues supported by blk/scsi-mq*/\n\t\thost->nr_hw_queues = ha->max_qpairs;\n\n\t\tql_dbg(ql_dbg_init, base_vha, 0x0192,\n\t\t\t\"blk/scsi-mq enabled, HW queues = %d.\\n\", host->nr_hw_queues);\n\t} else {\n\t\tif (ql2xnvmeenable) {\n\t\t\thost->nr_hw_queues = ha->max_qpairs;\n\t\t\tql_dbg(ql_dbg_init, base_vha, 0x0194,\n\t\t\t    \"FC-NVMe support is enabled, HW queues=%d\\n\",\n\t\t\t    host->nr_hw_queues);\n\t\t} else {\n\t\t\tql_dbg(ql_dbg_init, base_vha, 0x0193,\n\t\t\t    \"blk/scsi-mq disabled.\\n\");\n\t\t}\n\t}\n\n\tqlt_probe_one_stage1(base_vha, ha);\n\n\tpci_save_state(pdev);\n\n\t/* Assign back pointers */\n\trsp->req = req;\n\treq->rsp = rsp;\n\n\tif (IS_QLAFX00(ha)) {\n\t\tha->rsp_q_map[0] = rsp;\n\t\tha->req_q_map[0] = req;\n\t\tset_bit(0, ha->req_qid_map);\n\t\tset_bit(0, ha->rsp_qid_map);\n\t}\n\n\t/* FWI2-capable only. */\n\treq->req_q_in = &ha->iobase->isp24.req_q_in;\n\treq->req_q_out = &ha->iobase->isp24.req_q_out;\n\trsp->rsp_q_in = &ha->iobase->isp24.rsp_q_in;\n\trsp->rsp_q_out = &ha->iobase->isp24.rsp_q_out;\n\tif (ha->mqenable || IS_QLA83XX(ha) || IS_QLA27XX(ha) ||\n\t    IS_QLA28XX(ha)) {\n\t\treq->req_q_in = &ha->mqiobase->isp25mq.req_q_in;\n\t\treq->req_q_out = &ha->mqiobase->isp25mq.req_q_out;\n\t\trsp->rsp_q_in = &ha->mqiobase->isp25mq.rsp_q_in;\n\t\trsp->rsp_q_out =  &ha->mqiobase->isp25mq.rsp_q_out;\n\t}\n\n\tif (IS_QLAFX00(ha)) {\n\t\treq->req_q_in = &ha->iobase->ispfx00.req_q_in;\n\t\treq->req_q_out = &ha->iobase->ispfx00.req_q_out;\n\t\trsp->rsp_q_in = &ha->iobase->ispfx00.rsp_q_in;\n\t\trsp->rsp_q_out = &ha->iobase->ispfx00.rsp_q_out;\n\t}\n\n\tif (IS_P3P_TYPE(ha)) {\n\t\treq->req_q_out = &ha->iobase->isp82.req_q_out[0];\n\t\trsp->rsp_q_in = &ha->iobase->isp82.rsp_q_in[0];\n\t\trsp->rsp_q_out = &ha->iobase->isp82.rsp_q_out[0];\n\t}\n\n\tql_dbg(ql_dbg_multiq, base_vha, 0xc009,\n\t    \"rsp_q_map=%p req_q_map=%p rsp->req=%p req->rsp=%p.\\n\",\n\t    ha->rsp_q_map, ha->req_q_map, rsp->req, req->rsp);\n\tql_dbg(ql_dbg_multiq, base_vha, 0xc00a,\n\t    \"req->req_q_in=%p req->req_q_out=%p \"\n\t    \"rsp->rsp_q_in=%p rsp->rsp_q_out=%p.\\n\",\n\t    req->req_q_in, req->req_q_out,\n\t    rsp->rsp_q_in, rsp->rsp_q_out);\n\tql_dbg(ql_dbg_init, base_vha, 0x003e,\n\t    \"rsp_q_map=%p req_q_map=%p rsp->req=%p req->rsp=%p.\\n\",\n\t    ha->rsp_q_map, ha->req_q_map, rsp->req, req->rsp);\n\tql_dbg(ql_dbg_init, base_vha, 0x003f,\n\t    \"req->req_q_in=%p req->req_q_out=%p rsp->rsp_q_in=%p rsp->rsp_q_out=%p.\\n\",\n\t    req->req_q_in, req->req_q_out, rsp->rsp_q_in, rsp->rsp_q_out);\n\n\tha->wq = alloc_workqueue(\"qla2xxx_wq\", 0, 0);\n\n\tif (ha->isp_ops->initialize_adapter(base_vha)) {\n\t\tql_log(ql_log_fatal, base_vha, 0x00d6,\n\t\t    \"Failed to initialize adapter - Adapter flags %x.\\n\",\n\t\t    base_vha->device_flags);\n\n\t\tif (IS_QLA82XX(ha)) {\n\t\t\tqla82xx_idc_lock(ha);\n\t\t\tqla82xx_wr_32(ha, QLA82XX_CRB_DEV_STATE,\n\t\t\t\tQLA8XXX_DEV_FAILED);\n\t\t\tqla82xx_idc_unlock(ha);\n\t\t\tql_log(ql_log_fatal, base_vha, 0x00d7,\n\t\t\t    \"HW State: FAILED.\\n\");\n\t\t} else if (IS_QLA8044(ha)) {\n\t\t\tqla8044_idc_lock(ha);\n\t\t\tqla8044_wr_direct(base_vha,\n\t\t\t\tQLA8044_CRB_DEV_STATE_INDEX,\n\t\t\t\tQLA8XXX_DEV_FAILED);\n\t\t\tqla8044_idc_unlock(ha);\n\t\t\tql_log(ql_log_fatal, base_vha, 0x0150,\n\t\t\t    \"HW State: FAILED.\\n\");\n\t\t}\n\n\t\tret = -ENODEV;\n\t\tgoto probe_failed;\n\t}\n\n\tif (IS_QLAFX00(ha))\n\t\thost->can_queue = QLAFX00_MAX_CANQUEUE;\n\telse\n\t\thost->can_queue = req->num_outstanding_cmds - 10;\n\n\tql_dbg(ql_dbg_init, base_vha, 0x0032,\n\t    \"can_queue=%d, req=%p, mgmt_svr_loop_id=%d, sg_tablesize=%d.\\n\",\n\t    host->can_queue, base_vha->req,\n\t    base_vha->mgmt_svr_loop_id, host->sg_tablesize);\n\n\tif (ha->mqenable) {\n\t\tbool startit = false;\n\n\t\tif (QLA_TGT_MODE_ENABLED())\n\t\t\tstartit = false;\n\n\t\tif (ql2x_ini_mode == QLA2XXX_INI_MODE_ENABLED)\n\t\t\tstartit = true;\n\n\t\t/* Create start of day qpairs for Block MQ */\n\t\tfor (i = 0; i < ha->max_qpairs; i++)\n\t\t\tqla2xxx_create_qpair(base_vha, 5, 0, startit);\n\t}\n\n\tif (ha->flags.running_gold_fw)\n\t\tgoto skip_dpc;\n\n\t/*\n\t * Startup the kernel thread for this host adapter\n\t */\n\tha->dpc_thread = kthread_create(qla2x00_do_dpc, ha,\n\t    \"%s_dpc\", base_vha->host_str);\n\tif (IS_ERR(ha->dpc_thread)) {\n\t\tql_log(ql_log_fatal, base_vha, 0x00ed,\n\t\t    \"Failed to start DPC thread.\\n\");\n\t\tret = PTR_ERR(ha->dpc_thread);\n\t\tha->dpc_thread = NULL;\n\t\tgoto probe_failed;\n\t}\n\tql_dbg(ql_dbg_init, base_vha, 0x00ee,\n\t    \"DPC thread started successfully.\\n\");\n\n\t/*\n\t * If we're not coming up in initiator mode, we might sit for\n\t * a while without waking up the dpc thread, which leads to a\n\t * stuck process warning.  So just kick the dpc once here and\n\t * let the kthread start (and go back to sleep in qla2x00_do_dpc).\n\t */\n\tqla2xxx_wake_dpc(base_vha);\n\n\tINIT_WORK(&ha->board_disable, qla2x00_disable_board_on_pci_error);\n\n\tif (IS_QLA8031(ha) || IS_MCTP_CAPABLE(ha)) {\n\t\tsprintf(wq_name, \"qla2xxx_%lu_dpc_lp_wq\", base_vha->host_no);\n\t\tha->dpc_lp_wq = create_singlethread_workqueue(wq_name);\n\t\tINIT_WORK(&ha->idc_aen, qla83xx_service_idc_aen);\n\n\t\tsprintf(wq_name, \"qla2xxx_%lu_dpc_hp_wq\", base_vha->host_no);\n\t\tha->dpc_hp_wq = create_singlethread_workqueue(wq_name);\n\t\tINIT_WORK(&ha->nic_core_reset, qla83xx_nic_core_reset_work);\n\t\tINIT_WORK(&ha->idc_state_handler,\n\t\t    qla83xx_idc_state_handler_work);\n\t\tINIT_WORK(&ha->nic_core_unrecoverable,\n\t\t    qla83xx_nic_core_unrecoverable_work);\n\t}\n\nskip_dpc:\n\tlist_add_tail(&base_vha->list, &ha->vp_list);\n\tbase_vha->host->irq = ha->pdev->irq;\n\n\t/* Initialized the timer */\n\tqla2x00_start_timer(base_vha, WATCH_INTERVAL);\n\tql_dbg(ql_dbg_init, base_vha, 0x00ef,\n\t    \"Started qla2x00_timer with \"\n\t    \"interval=%d.\\n\", WATCH_INTERVAL);\n\tql_dbg(ql_dbg_init, base_vha, 0x00f0,\n\t    \"Detected hba at address=%p.\\n\",\n\t    ha);\n\n\tif (IS_T10_PI_CAPABLE(ha) && ql2xenabledif) {\n\t\tif (ha->fw_attributes & BIT_4) {\n\t\t\tint prot = 0, guard;\n\n\t\t\tbase_vha->flags.difdix_supported = 1;\n\t\t\tql_dbg(ql_dbg_init, base_vha, 0x00f1,\n\t\t\t    \"Registering for DIF/DIX type 1 and 3 protection.\\n\");\n\t\t\tif (ql2xenabledif == 1)\n\t\t\t\tprot = SHOST_DIX_TYPE0_PROTECTION;\n\t\t\tif (ql2xprotmask)\n\t\t\t\tscsi_host_set_prot(host, ql2xprotmask);\n\t\t\telse\n\t\t\t\tscsi_host_set_prot(host,\n\t\t\t\t    prot | SHOST_DIF_TYPE1_PROTECTION\n\t\t\t\t    | SHOST_DIF_TYPE2_PROTECTION\n\t\t\t\t    | SHOST_DIF_TYPE3_PROTECTION\n\t\t\t\t    | SHOST_DIX_TYPE1_PROTECTION\n\t\t\t\t    | SHOST_DIX_TYPE2_PROTECTION\n\t\t\t\t    | SHOST_DIX_TYPE3_PROTECTION);\n\n\t\t\tguard = SHOST_DIX_GUARD_CRC;\n\n\t\t\tif (IS_PI_IPGUARD_CAPABLE(ha) &&\n\t\t\t    (ql2xenabledif > 1 || IS_PI_DIFB_DIX0_CAPABLE(ha)))\n\t\t\t\tguard |= SHOST_DIX_GUARD_IP;\n\n\t\t\tif (ql2xprotguard)\n\t\t\t\tscsi_host_set_guard(host, ql2xprotguard);\n\t\t\telse\n\t\t\t\tscsi_host_set_guard(host, guard);\n\t\t} else\n\t\t\tbase_vha->flags.difdix_supported = 0;\n\t}\n\n\tha->isp_ops->enable_intrs(ha);\n\n\tif (IS_QLAFX00(ha)) {\n\t\tret = qlafx00_fx_disc(base_vha,\n\t\t\t&base_vha->hw->mr.fcport, FXDISC_GET_CONFIG_INFO);\n\t\thost->sg_tablesize = (ha->mr.extended_io_enabled) ?\n\t\t    QLA_SG_ALL : 128;\n\t}\n\n\tret = scsi_add_host(host, &pdev->dev);\n\tif (ret)\n\t\tgoto probe_failed;\n\n\tbase_vha->flags.init_done = 1;\n\tbase_vha->flags.online = 1;\n\tha->prev_minidump_failed = 0;\n\n\tql_dbg(ql_dbg_init, base_vha, 0x00f2,\n\t    \"Init done and hba is online.\\n\");\n\n\tif (qla_ini_mode_enabled(base_vha) ||\n\t\tqla_dual_mode_enabled(base_vha))\n\t\tscsi_scan_host(host);\n\telse\n\t\tql_dbg(ql_dbg_init, base_vha, 0x0122,\n\t\t\t\"skipping scsi_scan_host() for non-initiator port\\n\");\n\n\tqla2x00_alloc_sysfs_attr(base_vha);\n\n\tif (IS_QLAFX00(ha)) {\n\t\tret = qlafx00_fx_disc(base_vha,\n\t\t\t&base_vha->hw->mr.fcport, FXDISC_GET_PORT_INFO);\n\n\t\t/* Register system information */\n\t\tret =  qlafx00_fx_disc(base_vha,\n\t\t\t&base_vha->hw->mr.fcport, FXDISC_REG_HOST_INFO);\n\t}\n\n\tqla2x00_init_host_attr(base_vha);\n\n\tqla2x00_dfs_setup(base_vha);\n\n\tql_log(ql_log_info, base_vha, 0x00fb,\n\t    \"QLogic %s - %s.\\n\", ha->model_number, ha->model_desc);\n\tql_log(ql_log_info, base_vha, 0x00fc,\n\t    \"ISP%04X: %s @ %s hdma%c host#=%ld fw=%s.\\n\",\n\t    pdev->device, ha->isp_ops->pci_info_str(base_vha, pci_info,\n\t\t\t\t\t\t       sizeof(pci_info)),\n\t    pci_name(pdev), ha->flags.enable_64bit_addressing ? '+' : '-',\n\t    base_vha->host_no,\n\t    ha->isp_ops->fw_version_str(base_vha, fw_str, sizeof(fw_str)));\n\n\tqlt_add_target(ha, base_vha);\n\n\tclear_bit(PFLG_DRIVER_PROBING, &base_vha->pci_flags);\n\n\tif (test_bit(UNLOADING, &base_vha->dpc_flags))\n\t\treturn -ENODEV;\n\n\tif (ha->flags.detected_lr_sfp) {\n\t\tql_log(ql_log_info, base_vha, 0xffff,\n\t\t    \"Reset chip to pick up LR SFP setting\\n\");\n\t\tset_bit(ISP_ABORT_NEEDED, &base_vha->dpc_flags);\n\t\tqla2xxx_wake_dpc(base_vha);\n\t}\n\n\treturn 0;\n\nprobe_failed:\n\tif (base_vha->timer_active)\n\t\tqla2x00_stop_timer(base_vha);\n\tbase_vha->flags.online = 0;\n\tif (ha->dpc_thread) {\n\t\tstruct task_struct *t = ha->dpc_thread;\n\n\t\tha->dpc_thread = NULL;\n\t\tkthread_stop(t);\n\t}\n\n\tqla2x00_free_device(base_vha);\n\tscsi_host_put(base_vha->host);\n\t/*\n\t * Need to NULL out local req/rsp after\n\t * qla2x00_free_device => qla2x00_free_queues frees\n\t * what these are pointing to. Or else we'll\n\t * fall over below in qla2x00_free_req/rsp_que.\n\t */\n\treq = NULL;\n\trsp = NULL;\n\nprobe_hw_failed:\n\tqla2x00_mem_free(ha);\n\tqla2x00_free_req_que(ha, req);\n\tqla2x00_free_rsp_que(ha, rsp);\n\tqla2x00_clear_drv_active(ha);\n\niospace_config_failed:\n\tif (IS_P3P_TYPE(ha)) {\n\t\tif (!ha->nx_pcibase)\n\t\t\tiounmap((device_reg_t *)ha->nx_pcibase);\n\t\tif (!ql2xdbwr)\n\t\t\tiounmap((device_reg_t *)ha->nxdb_wr_ptr);\n\t} else {\n\t\tif (ha->iobase)\n\t\t\tiounmap(ha->iobase);\n\t\tif (ha->cregbase)\n\t\t\tiounmap(ha->cregbase);\n\t}\n\tpci_release_selected_regions(ha->pdev, ha->bars);\n\tkfree(ha);\n\ndisable_device:\n\tpci_disable_device(pdev);\n\treturn ret;\n}",
        "code_after_change": "static int\nqla2x00_probe_one(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tint\tret = -ENODEV;\n\tstruct Scsi_Host *host;\n\tscsi_qla_host_t *base_vha = NULL;\n\tstruct qla_hw_data *ha;\n\tchar pci_info[30];\n\tchar fw_str[30], wq_name[30];\n\tstruct scsi_host_template *sht;\n\tint bars, mem_only = 0;\n\tuint16_t req_length = 0, rsp_length = 0;\n\tstruct req_que *req = NULL;\n\tstruct rsp_que *rsp = NULL;\n\tint i;\n\n\tbars = pci_select_bars(pdev, IORESOURCE_MEM | IORESOURCE_IO);\n\tsht = &qla2xxx_driver_template;\n\tif (pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2422 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2432 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP8432 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP5422 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP5432 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2532 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP8001 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP8021 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2031 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP8031 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISPF001 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP8044 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2071 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2271 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2261 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2081 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2281 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2089 ||\n\t    pdev->device == PCI_DEVICE_ID_QLOGIC_ISP2289) {\n\t\tbars = pci_select_bars(pdev, IORESOURCE_MEM);\n\t\tmem_only = 1;\n\t\tql_dbg_pci(ql_dbg_init, pdev, 0x0007,\n\t\t    \"Mem only adapter.\\n\");\n\t}\n\tql_dbg_pci(ql_dbg_init, pdev, 0x0008,\n\t    \"Bars=%d.\\n\", bars);\n\n\tif (mem_only) {\n\t\tif (pci_enable_device_mem(pdev))\n\t\t\treturn ret;\n\t} else {\n\t\tif (pci_enable_device(pdev))\n\t\t\treturn ret;\n\t}\n\n\t/* This may fail but that's ok */\n\tpci_enable_pcie_error_reporting(pdev);\n\n\t/* Turn off T10-DIF when FC-NVMe is enabled */\n\tif (ql2xnvmeenable)\n\t\tql2xenabledif = 0;\n\n\tha = kzalloc(sizeof(struct qla_hw_data), GFP_KERNEL);\n\tif (!ha) {\n\t\tql_log_pci(ql_log_fatal, pdev, 0x0009,\n\t\t    \"Unable to allocate memory for ha.\\n\");\n\t\tgoto disable_device;\n\t}\n\tql_dbg_pci(ql_dbg_init, pdev, 0x000a,\n\t    \"Memory allocated for ha=%p.\\n\", ha);\n\tha->pdev = pdev;\n\tINIT_LIST_HEAD(&ha->tgt.q_full_list);\n\tspin_lock_init(&ha->tgt.q_full_lock);\n\tspin_lock_init(&ha->tgt.sess_lock);\n\tspin_lock_init(&ha->tgt.atio_lock);\n\n\tatomic_set(&ha->nvme_active_aen_cnt, 0);\n\n\t/* Clear our data area */\n\tha->bars = bars;\n\tha->mem_only = mem_only;\n\tspin_lock_init(&ha->hardware_lock);\n\tspin_lock_init(&ha->vport_slock);\n\tmutex_init(&ha->selflogin_lock);\n\tmutex_init(&ha->optrom_mutex);\n\n\t/* Set ISP-type information. */\n\tqla2x00_set_isp_flags(ha);\n\n\t/* Set EEH reset type to fundamental if required by hba */\n\tif (IS_QLA24XX(ha) || IS_QLA25XX(ha) || IS_QLA81XX(ha) ||\n\t    IS_QLA83XX(ha) || IS_QLA27XX(ha) || IS_QLA28XX(ha))\n\t\tpdev->needs_freset = 1;\n\n\tha->prev_topology = 0;\n\tha->init_cb_size = sizeof(init_cb_t);\n\tha->link_data_rate = PORT_SPEED_UNKNOWN;\n\tha->optrom_size = OPTROM_SIZE_2300;\n\tha->max_exchg = FW_MAX_EXCHANGES_CNT;\n\tatomic_set(&ha->num_pend_mbx_stage1, 0);\n\tatomic_set(&ha->num_pend_mbx_stage2, 0);\n\tatomic_set(&ha->num_pend_mbx_stage3, 0);\n\tatomic_set(&ha->zio_threshold, DEFAULT_ZIO_THRESHOLD);\n\tha->last_zio_threshold = DEFAULT_ZIO_THRESHOLD;\n\n\t/* Assign ISP specific operations. */\n\tif (IS_QLA2100(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2100;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT_2100;\n\t\treq_length = REQUEST_ENTRY_CNT_2100;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2100;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2100;\n\t\tha->gid_list_info_size = 4;\n\t\tha->flash_conf_off = ~0;\n\t\tha->flash_data_off = ~0;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t\tha->isp_ops = &qla2100_isp_ops;\n\t} else if (IS_QLA2200(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2100;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT_2200;\n\t\treq_length = REQUEST_ENTRY_CNT_2200;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2100;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2100;\n\t\tha->gid_list_info_size = 4;\n\t\tha->flash_conf_off = ~0;\n\t\tha->flash_data_off = ~0;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t\tha->isp_ops = &qla2100_isp_ops;\n\t} else if (IS_QLA23XX(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2100;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_2200;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2300;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->gid_list_info_size = 6;\n\t\tif (IS_QLA2322(ha) || IS_QLA6322(ha))\n\t\t\tha->optrom_size = OPTROM_SIZE_2322;\n\t\tha->flash_conf_off = ~0;\n\t\tha->flash_data_off = ~0;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t\tha->isp_ops = &qla2300_isp_ops;\n\t} else if (IS_QLA24XX_TYPE(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_24XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2300;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_24xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_24XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA24XX;\n\t\tha->isp_ops = &qla24xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA;\n\t\tha->nvram_conf_off = FARX_ACCESS_NVRAM_CONF;\n\t\tha->nvram_data_off = FARX_ACCESS_NVRAM_DATA;\n\t} else if (IS_QLA25XX(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_24XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2300;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_24xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_25XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla25xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA;\n\t\tha->nvram_conf_off = FARX_ACCESS_NVRAM_CONF;\n\t\tha->nvram_data_off = FARX_ACCESS_NVRAM_DATA;\n\t} else if (IS_QLA81XX(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_24XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2300;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_81XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla81xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF_81XX;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA_81XX;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t} else if (IS_QLA82XX(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_82XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_82XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_82XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla82xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA;\n\t\tha->nvram_conf_off = FARX_ACCESS_NVRAM_CONF;\n\t\tha->nvram_data_off = FARX_ACCESS_NVRAM_DATA;\n\t} else if (IS_QLA8044(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_82XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_82XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_83XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla8044_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA;\n\t\tha->nvram_conf_off = FARX_ACCESS_NVRAM_CONF;\n\t\tha->nvram_data_off = FARX_ACCESS_NVRAM_DATA;\n\t} else if (IS_QLA83XX(ha)) {\n\t\tha->portnum = PCI_FUNC(ha->pdev->devfn);\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_83XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_83XX;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_83XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla83xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF_81XX;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA_81XX;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t}  else if (IS_QLAFX00(ha)) {\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_FX00;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT_FX00;\n\t\tha->aen_mbx_count = AEN_MAILBOX_REGISTER_COUNT_FX00;\n\t\treq_length = REQUEST_ENTRY_CNT_FX00;\n\t\trsp_length = RESPONSE_ENTRY_CNT_FX00;\n\t\tha->isp_ops = &qlafx00_isp_ops;\n\t\tha->port_down_retry_count = 30; /* default value */\n\t\tha->mr.fw_hbt_cnt = QLAFX00_HEARTBEAT_INTERVAL;\n\t\tha->mr.fw_reset_timer_tick = QLAFX00_RESET_INTERVAL;\n\t\tha->mr.fw_critemp_timer_tick = QLAFX00_CRITEMP_INTERVAL;\n\t\tha->mr.fw_hbt_en = 1;\n\t\tha->mr.host_info_resend = false;\n\t\tha->mr.hinfo_resend_timer_tick = QLAFX00_HINFO_RESEND_INTERVAL;\n\t} else if (IS_QLA27XX(ha)) {\n\t\tha->portnum = PCI_FUNC(ha->pdev->devfn);\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_83XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_83XX;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_83XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla27xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF_81XX;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA_81XX;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t} else if (IS_QLA28XX(ha)) {\n\t\tha->portnum = PCI_FUNC(ha->pdev->devfn);\n\t\tha->max_fibre_devices = MAX_FIBRE_DEVICES_2400;\n\t\tha->mbx_count = MAILBOX_REGISTER_COUNT;\n\t\treq_length = REQUEST_ENTRY_CNT_24XX;\n\t\trsp_length = RESPONSE_ENTRY_CNT_2300;\n\t\tha->tgt.atio_q_length = ATIO_ENTRY_CNT_24XX;\n\t\tha->max_loop_id = SNS_LAST_LOOP_ID_2300;\n\t\tha->init_cb_size = sizeof(struct mid_init_cb_81xx);\n\t\tha->gid_list_info_size = 8;\n\t\tha->optrom_size = OPTROM_SIZE_28XX;\n\t\tha->nvram_npiv_size = QLA_MAX_VPORTS_QLA25XX;\n\t\tha->isp_ops = &qla27xx_isp_ops;\n\t\tha->flash_conf_off = FARX_ACCESS_FLASH_CONF_28XX;\n\t\tha->flash_data_off = FARX_ACCESS_FLASH_DATA_28XX;\n\t\tha->nvram_conf_off = ~0;\n\t\tha->nvram_data_off = ~0;\n\t}\n\n\tql_dbg_pci(ql_dbg_init, pdev, 0x001e,\n\t    \"mbx_count=%d, req_length=%d, \"\n\t    \"rsp_length=%d, max_loop_id=%d, init_cb_size=%d, \"\n\t    \"gid_list_info_size=%d, optrom_size=%d, nvram_npiv_size=%d, \"\n\t    \"max_fibre_devices=%d.\\n\",\n\t    ha->mbx_count, req_length, rsp_length, ha->max_loop_id,\n\t    ha->init_cb_size, ha->gid_list_info_size, ha->optrom_size,\n\t    ha->nvram_npiv_size, ha->max_fibre_devices);\n\tql_dbg_pci(ql_dbg_init, pdev, 0x001f,\n\t    \"isp_ops=%p, flash_conf_off=%d, \"\n\t    \"flash_data_off=%d, nvram_conf_off=%d, nvram_data_off=%d.\\n\",\n\t    ha->isp_ops, ha->flash_conf_off, ha->flash_data_off,\n\t    ha->nvram_conf_off, ha->nvram_data_off);\n\n\t/* Configure PCI I/O space */\n\tret = ha->isp_ops->iospace_config(ha);\n\tif (ret)\n\t\tgoto iospace_config_failed;\n\n\tql_log_pci(ql_log_info, pdev, 0x001d,\n\t    \"Found an ISP%04X irq %d iobase 0x%p.\\n\",\n\t    pdev->device, pdev->irq, ha->iobase);\n\tmutex_init(&ha->vport_lock);\n\tmutex_init(&ha->mq_lock);\n\tinit_completion(&ha->mbx_cmd_comp);\n\tcomplete(&ha->mbx_cmd_comp);\n\tinit_completion(&ha->mbx_intr_comp);\n\tinit_completion(&ha->dcbx_comp);\n\tinit_completion(&ha->lb_portup_comp);\n\n\tset_bit(0, (unsigned long *) ha->vp_idx_map);\n\n\tqla2x00_config_dma_addressing(ha);\n\tql_dbg_pci(ql_dbg_init, pdev, 0x0020,\n\t    \"64 Bit addressing is %s.\\n\",\n\t    ha->flags.enable_64bit_addressing ? \"enable\" :\n\t    \"disable\");\n\tret = qla2x00_mem_alloc(ha, req_length, rsp_length, &req, &rsp);\n\tif (ret) {\n\t\tql_log_pci(ql_log_fatal, pdev, 0x0031,\n\t\t    \"Failed to allocate memory for adapter, aborting.\\n\");\n\n\t\tgoto probe_hw_failed;\n\t}\n\n\treq->max_q_depth = MAX_Q_DEPTH;\n\tif (ql2xmaxqdepth != 0 && ql2xmaxqdepth <= 0xffffU)\n\t\treq->max_q_depth = ql2xmaxqdepth;\n\n\n\tbase_vha = qla2x00_create_host(sht, ha);\n\tif (!base_vha) {\n\t\tret = -ENOMEM;\n\t\tgoto probe_hw_failed;\n\t}\n\n\tpci_set_drvdata(pdev, base_vha);\n\tset_bit(PFLG_DRIVER_PROBING, &base_vha->pci_flags);\n\n\thost = base_vha->host;\n\tbase_vha->req = req;\n\tif (IS_QLA2XXX_MIDTYPE(ha))\n\t\tbase_vha->mgmt_svr_loop_id =\n\t\t\tqla2x00_reserve_mgmt_server_loop_id(base_vha);\n\telse\n\t\tbase_vha->mgmt_svr_loop_id = MANAGEMENT_SERVER +\n\t\t\t\t\t\tbase_vha->vp_idx;\n\n\t/* Setup fcport template structure. */\n\tha->mr.fcport.vha = base_vha;\n\tha->mr.fcport.port_type = FCT_UNKNOWN;\n\tha->mr.fcport.loop_id = FC_NO_LOOP_ID;\n\tqla2x00_set_fcport_state(&ha->mr.fcport, FCS_UNCONFIGURED);\n\tha->mr.fcport.supported_classes = FC_COS_UNSPECIFIED;\n\tha->mr.fcport.scan_state = 1;\n\n\t/* Set the SG table size based on ISP type */\n\tif (!IS_FWI2_CAPABLE(ha)) {\n\t\tif (IS_QLA2100(ha))\n\t\t\thost->sg_tablesize = 32;\n\t} else {\n\t\tif (!IS_QLA82XX(ha))\n\t\t\thost->sg_tablesize = QLA_SG_ALL;\n\t}\n\thost->max_id = ha->max_fibre_devices;\n\thost->cmd_per_lun = 3;\n\thost->unique_id = host->host_no;\n\tif (IS_T10_PI_CAPABLE(ha) && ql2xenabledif)\n\t\thost->max_cmd_len = 32;\n\telse\n\t\thost->max_cmd_len = MAX_CMDSZ;\n\thost->max_channel = MAX_BUSES - 1;\n\t/* Older HBAs support only 16-bit LUNs */\n\tif (!IS_QLAFX00(ha) && !IS_FWI2_CAPABLE(ha) &&\n\t    ql2xmaxlun > 0xffff)\n\t\thost->max_lun = 0xffff;\n\telse\n\t\thost->max_lun = ql2xmaxlun;\n\thost->transportt = qla2xxx_transport_template;\n\tsht->vendor_id = (SCSI_NL_VID_TYPE_PCI | PCI_VENDOR_ID_QLOGIC);\n\n\tql_dbg(ql_dbg_init, base_vha, 0x0033,\n\t    \"max_id=%d this_id=%d \"\n\t    \"cmd_per_len=%d unique_id=%d max_cmd_len=%d max_channel=%d \"\n\t    \"max_lun=%llu transportt=%p, vendor_id=%llu.\\n\", host->max_id,\n\t    host->this_id, host->cmd_per_lun, host->unique_id,\n\t    host->max_cmd_len, host->max_channel, host->max_lun,\n\t    host->transportt, sht->vendor_id);\n\n\tINIT_WORK(&base_vha->iocb_work, qla2x00_iocb_work_fn);\n\n\t/* Set up the irqs */\n\tret = qla2x00_request_irqs(ha, rsp);\n\tif (ret)\n\t\tgoto probe_failed;\n\n\t/* Alloc arrays of request and response ring ptrs */\n\tret = qla2x00_alloc_queues(ha, req, rsp);\n\tif (ret) {\n\t\tql_log(ql_log_fatal, base_vha, 0x003d,\n\t\t    \"Failed to allocate memory for queue pointers...\"\n\t\t    \"aborting.\\n\");\n\t\tret = -ENODEV;\n\t\tgoto probe_failed;\n\t}\n\n\tif (ha->mqenable) {\n\t\t/* number of hardware queues supported by blk/scsi-mq*/\n\t\thost->nr_hw_queues = ha->max_qpairs;\n\n\t\tql_dbg(ql_dbg_init, base_vha, 0x0192,\n\t\t\t\"blk/scsi-mq enabled, HW queues = %d.\\n\", host->nr_hw_queues);\n\t} else {\n\t\tif (ql2xnvmeenable) {\n\t\t\thost->nr_hw_queues = ha->max_qpairs;\n\t\t\tql_dbg(ql_dbg_init, base_vha, 0x0194,\n\t\t\t    \"FC-NVMe support is enabled, HW queues=%d\\n\",\n\t\t\t    host->nr_hw_queues);\n\t\t} else {\n\t\t\tql_dbg(ql_dbg_init, base_vha, 0x0193,\n\t\t\t    \"blk/scsi-mq disabled.\\n\");\n\t\t}\n\t}\n\n\tqlt_probe_one_stage1(base_vha, ha);\n\n\tpci_save_state(pdev);\n\n\t/* Assign back pointers */\n\trsp->req = req;\n\treq->rsp = rsp;\n\n\tif (IS_QLAFX00(ha)) {\n\t\tha->rsp_q_map[0] = rsp;\n\t\tha->req_q_map[0] = req;\n\t\tset_bit(0, ha->req_qid_map);\n\t\tset_bit(0, ha->rsp_qid_map);\n\t}\n\n\t/* FWI2-capable only. */\n\treq->req_q_in = &ha->iobase->isp24.req_q_in;\n\treq->req_q_out = &ha->iobase->isp24.req_q_out;\n\trsp->rsp_q_in = &ha->iobase->isp24.rsp_q_in;\n\trsp->rsp_q_out = &ha->iobase->isp24.rsp_q_out;\n\tif (ha->mqenable || IS_QLA83XX(ha) || IS_QLA27XX(ha) ||\n\t    IS_QLA28XX(ha)) {\n\t\treq->req_q_in = &ha->mqiobase->isp25mq.req_q_in;\n\t\treq->req_q_out = &ha->mqiobase->isp25mq.req_q_out;\n\t\trsp->rsp_q_in = &ha->mqiobase->isp25mq.rsp_q_in;\n\t\trsp->rsp_q_out =  &ha->mqiobase->isp25mq.rsp_q_out;\n\t}\n\n\tif (IS_QLAFX00(ha)) {\n\t\treq->req_q_in = &ha->iobase->ispfx00.req_q_in;\n\t\treq->req_q_out = &ha->iobase->ispfx00.req_q_out;\n\t\trsp->rsp_q_in = &ha->iobase->ispfx00.rsp_q_in;\n\t\trsp->rsp_q_out = &ha->iobase->ispfx00.rsp_q_out;\n\t}\n\n\tif (IS_P3P_TYPE(ha)) {\n\t\treq->req_q_out = &ha->iobase->isp82.req_q_out[0];\n\t\trsp->rsp_q_in = &ha->iobase->isp82.rsp_q_in[0];\n\t\trsp->rsp_q_out = &ha->iobase->isp82.rsp_q_out[0];\n\t}\n\n\tql_dbg(ql_dbg_multiq, base_vha, 0xc009,\n\t    \"rsp_q_map=%p req_q_map=%p rsp->req=%p req->rsp=%p.\\n\",\n\t    ha->rsp_q_map, ha->req_q_map, rsp->req, req->rsp);\n\tql_dbg(ql_dbg_multiq, base_vha, 0xc00a,\n\t    \"req->req_q_in=%p req->req_q_out=%p \"\n\t    \"rsp->rsp_q_in=%p rsp->rsp_q_out=%p.\\n\",\n\t    req->req_q_in, req->req_q_out,\n\t    rsp->rsp_q_in, rsp->rsp_q_out);\n\tql_dbg(ql_dbg_init, base_vha, 0x003e,\n\t    \"rsp_q_map=%p req_q_map=%p rsp->req=%p req->rsp=%p.\\n\",\n\t    ha->rsp_q_map, ha->req_q_map, rsp->req, req->rsp);\n\tql_dbg(ql_dbg_init, base_vha, 0x003f,\n\t    \"req->req_q_in=%p req->req_q_out=%p rsp->rsp_q_in=%p rsp->rsp_q_out=%p.\\n\",\n\t    req->req_q_in, req->req_q_out, rsp->rsp_q_in, rsp->rsp_q_out);\n\n\tha->wq = alloc_workqueue(\"qla2xxx_wq\", 0, 0);\n\tif (unlikely(!ha->wq)) {\n\t\tret = -ENOMEM;\n\t\tgoto probe_failed;\n\t}\n\n\tif (ha->isp_ops->initialize_adapter(base_vha)) {\n\t\tql_log(ql_log_fatal, base_vha, 0x00d6,\n\t\t    \"Failed to initialize adapter - Adapter flags %x.\\n\",\n\t\t    base_vha->device_flags);\n\n\t\tif (IS_QLA82XX(ha)) {\n\t\t\tqla82xx_idc_lock(ha);\n\t\t\tqla82xx_wr_32(ha, QLA82XX_CRB_DEV_STATE,\n\t\t\t\tQLA8XXX_DEV_FAILED);\n\t\t\tqla82xx_idc_unlock(ha);\n\t\t\tql_log(ql_log_fatal, base_vha, 0x00d7,\n\t\t\t    \"HW State: FAILED.\\n\");\n\t\t} else if (IS_QLA8044(ha)) {\n\t\t\tqla8044_idc_lock(ha);\n\t\t\tqla8044_wr_direct(base_vha,\n\t\t\t\tQLA8044_CRB_DEV_STATE_INDEX,\n\t\t\t\tQLA8XXX_DEV_FAILED);\n\t\t\tqla8044_idc_unlock(ha);\n\t\t\tql_log(ql_log_fatal, base_vha, 0x0150,\n\t\t\t    \"HW State: FAILED.\\n\");\n\t\t}\n\n\t\tret = -ENODEV;\n\t\tgoto probe_failed;\n\t}\n\n\tif (IS_QLAFX00(ha))\n\t\thost->can_queue = QLAFX00_MAX_CANQUEUE;\n\telse\n\t\thost->can_queue = req->num_outstanding_cmds - 10;\n\n\tql_dbg(ql_dbg_init, base_vha, 0x0032,\n\t    \"can_queue=%d, req=%p, mgmt_svr_loop_id=%d, sg_tablesize=%d.\\n\",\n\t    host->can_queue, base_vha->req,\n\t    base_vha->mgmt_svr_loop_id, host->sg_tablesize);\n\n\tif (ha->mqenable) {\n\t\tbool startit = false;\n\n\t\tif (QLA_TGT_MODE_ENABLED())\n\t\t\tstartit = false;\n\n\t\tif (ql2x_ini_mode == QLA2XXX_INI_MODE_ENABLED)\n\t\t\tstartit = true;\n\n\t\t/* Create start of day qpairs for Block MQ */\n\t\tfor (i = 0; i < ha->max_qpairs; i++)\n\t\t\tqla2xxx_create_qpair(base_vha, 5, 0, startit);\n\t}\n\n\tif (ha->flags.running_gold_fw)\n\t\tgoto skip_dpc;\n\n\t/*\n\t * Startup the kernel thread for this host adapter\n\t */\n\tha->dpc_thread = kthread_create(qla2x00_do_dpc, ha,\n\t    \"%s_dpc\", base_vha->host_str);\n\tif (IS_ERR(ha->dpc_thread)) {\n\t\tql_log(ql_log_fatal, base_vha, 0x00ed,\n\t\t    \"Failed to start DPC thread.\\n\");\n\t\tret = PTR_ERR(ha->dpc_thread);\n\t\tha->dpc_thread = NULL;\n\t\tgoto probe_failed;\n\t}\n\tql_dbg(ql_dbg_init, base_vha, 0x00ee,\n\t    \"DPC thread started successfully.\\n\");\n\n\t/*\n\t * If we're not coming up in initiator mode, we might sit for\n\t * a while without waking up the dpc thread, which leads to a\n\t * stuck process warning.  So just kick the dpc once here and\n\t * let the kthread start (and go back to sleep in qla2x00_do_dpc).\n\t */\n\tqla2xxx_wake_dpc(base_vha);\n\n\tINIT_WORK(&ha->board_disable, qla2x00_disable_board_on_pci_error);\n\n\tif (IS_QLA8031(ha) || IS_MCTP_CAPABLE(ha)) {\n\t\tsprintf(wq_name, \"qla2xxx_%lu_dpc_lp_wq\", base_vha->host_no);\n\t\tha->dpc_lp_wq = create_singlethread_workqueue(wq_name);\n\t\tINIT_WORK(&ha->idc_aen, qla83xx_service_idc_aen);\n\n\t\tsprintf(wq_name, \"qla2xxx_%lu_dpc_hp_wq\", base_vha->host_no);\n\t\tha->dpc_hp_wq = create_singlethread_workqueue(wq_name);\n\t\tINIT_WORK(&ha->nic_core_reset, qla83xx_nic_core_reset_work);\n\t\tINIT_WORK(&ha->idc_state_handler,\n\t\t    qla83xx_idc_state_handler_work);\n\t\tINIT_WORK(&ha->nic_core_unrecoverable,\n\t\t    qla83xx_nic_core_unrecoverable_work);\n\t}\n\nskip_dpc:\n\tlist_add_tail(&base_vha->list, &ha->vp_list);\n\tbase_vha->host->irq = ha->pdev->irq;\n\n\t/* Initialized the timer */\n\tqla2x00_start_timer(base_vha, WATCH_INTERVAL);\n\tql_dbg(ql_dbg_init, base_vha, 0x00ef,\n\t    \"Started qla2x00_timer with \"\n\t    \"interval=%d.\\n\", WATCH_INTERVAL);\n\tql_dbg(ql_dbg_init, base_vha, 0x00f0,\n\t    \"Detected hba at address=%p.\\n\",\n\t    ha);\n\n\tif (IS_T10_PI_CAPABLE(ha) && ql2xenabledif) {\n\t\tif (ha->fw_attributes & BIT_4) {\n\t\t\tint prot = 0, guard;\n\n\t\t\tbase_vha->flags.difdix_supported = 1;\n\t\t\tql_dbg(ql_dbg_init, base_vha, 0x00f1,\n\t\t\t    \"Registering for DIF/DIX type 1 and 3 protection.\\n\");\n\t\t\tif (ql2xenabledif == 1)\n\t\t\t\tprot = SHOST_DIX_TYPE0_PROTECTION;\n\t\t\tif (ql2xprotmask)\n\t\t\t\tscsi_host_set_prot(host, ql2xprotmask);\n\t\t\telse\n\t\t\t\tscsi_host_set_prot(host,\n\t\t\t\t    prot | SHOST_DIF_TYPE1_PROTECTION\n\t\t\t\t    | SHOST_DIF_TYPE2_PROTECTION\n\t\t\t\t    | SHOST_DIF_TYPE3_PROTECTION\n\t\t\t\t    | SHOST_DIX_TYPE1_PROTECTION\n\t\t\t\t    | SHOST_DIX_TYPE2_PROTECTION\n\t\t\t\t    | SHOST_DIX_TYPE3_PROTECTION);\n\n\t\t\tguard = SHOST_DIX_GUARD_CRC;\n\n\t\t\tif (IS_PI_IPGUARD_CAPABLE(ha) &&\n\t\t\t    (ql2xenabledif > 1 || IS_PI_DIFB_DIX0_CAPABLE(ha)))\n\t\t\t\tguard |= SHOST_DIX_GUARD_IP;\n\n\t\t\tif (ql2xprotguard)\n\t\t\t\tscsi_host_set_guard(host, ql2xprotguard);\n\t\t\telse\n\t\t\t\tscsi_host_set_guard(host, guard);\n\t\t} else\n\t\t\tbase_vha->flags.difdix_supported = 0;\n\t}\n\n\tha->isp_ops->enable_intrs(ha);\n\n\tif (IS_QLAFX00(ha)) {\n\t\tret = qlafx00_fx_disc(base_vha,\n\t\t\t&base_vha->hw->mr.fcport, FXDISC_GET_CONFIG_INFO);\n\t\thost->sg_tablesize = (ha->mr.extended_io_enabled) ?\n\t\t    QLA_SG_ALL : 128;\n\t}\n\n\tret = scsi_add_host(host, &pdev->dev);\n\tif (ret)\n\t\tgoto probe_failed;\n\n\tbase_vha->flags.init_done = 1;\n\tbase_vha->flags.online = 1;\n\tha->prev_minidump_failed = 0;\n\n\tql_dbg(ql_dbg_init, base_vha, 0x00f2,\n\t    \"Init done and hba is online.\\n\");\n\n\tif (qla_ini_mode_enabled(base_vha) ||\n\t\tqla_dual_mode_enabled(base_vha))\n\t\tscsi_scan_host(host);\n\telse\n\t\tql_dbg(ql_dbg_init, base_vha, 0x0122,\n\t\t\t\"skipping scsi_scan_host() for non-initiator port\\n\");\n\n\tqla2x00_alloc_sysfs_attr(base_vha);\n\n\tif (IS_QLAFX00(ha)) {\n\t\tret = qlafx00_fx_disc(base_vha,\n\t\t\t&base_vha->hw->mr.fcport, FXDISC_GET_PORT_INFO);\n\n\t\t/* Register system information */\n\t\tret =  qlafx00_fx_disc(base_vha,\n\t\t\t&base_vha->hw->mr.fcport, FXDISC_REG_HOST_INFO);\n\t}\n\n\tqla2x00_init_host_attr(base_vha);\n\n\tqla2x00_dfs_setup(base_vha);\n\n\tql_log(ql_log_info, base_vha, 0x00fb,\n\t    \"QLogic %s - %s.\\n\", ha->model_number, ha->model_desc);\n\tql_log(ql_log_info, base_vha, 0x00fc,\n\t    \"ISP%04X: %s @ %s hdma%c host#=%ld fw=%s.\\n\",\n\t    pdev->device, ha->isp_ops->pci_info_str(base_vha, pci_info,\n\t\t\t\t\t\t       sizeof(pci_info)),\n\t    pci_name(pdev), ha->flags.enable_64bit_addressing ? '+' : '-',\n\t    base_vha->host_no,\n\t    ha->isp_ops->fw_version_str(base_vha, fw_str, sizeof(fw_str)));\n\n\tqlt_add_target(ha, base_vha);\n\n\tclear_bit(PFLG_DRIVER_PROBING, &base_vha->pci_flags);\n\n\tif (test_bit(UNLOADING, &base_vha->dpc_flags))\n\t\treturn -ENODEV;\n\n\tif (ha->flags.detected_lr_sfp) {\n\t\tql_log(ql_log_info, base_vha, 0xffff,\n\t\t    \"Reset chip to pick up LR SFP setting\\n\");\n\t\tset_bit(ISP_ABORT_NEEDED, &base_vha->dpc_flags);\n\t\tqla2xxx_wake_dpc(base_vha);\n\t}\n\n\treturn 0;\n\nprobe_failed:\n\tif (base_vha->timer_active)\n\t\tqla2x00_stop_timer(base_vha);\n\tbase_vha->flags.online = 0;\n\tif (ha->dpc_thread) {\n\t\tstruct task_struct *t = ha->dpc_thread;\n\n\t\tha->dpc_thread = NULL;\n\t\tkthread_stop(t);\n\t}\n\n\tqla2x00_free_device(base_vha);\n\tscsi_host_put(base_vha->host);\n\t/*\n\t * Need to NULL out local req/rsp after\n\t * qla2x00_free_device => qla2x00_free_queues frees\n\t * what these are pointing to. Or else we'll\n\t * fall over below in qla2x00_free_req/rsp_que.\n\t */\n\treq = NULL;\n\trsp = NULL;\n\nprobe_hw_failed:\n\tqla2x00_mem_free(ha);\n\tqla2x00_free_req_que(ha, req);\n\tqla2x00_free_rsp_que(ha, rsp);\n\tqla2x00_clear_drv_active(ha);\n\niospace_config_failed:\n\tif (IS_P3P_TYPE(ha)) {\n\t\tif (!ha->nx_pcibase)\n\t\t\tiounmap((device_reg_t *)ha->nx_pcibase);\n\t\tif (!ql2xdbwr)\n\t\t\tiounmap((device_reg_t *)ha->nxdb_wr_ptr);\n\t} else {\n\t\tif (ha->iobase)\n\t\t\tiounmap(ha->iobase);\n\t\tif (ha->cregbase)\n\t\t\tiounmap(ha->cregbase);\n\t}\n\tpci_release_selected_regions(ha->pdev, ha->bars);\n\tkfree(ha);\n\ndisable_device:\n\tpci_disable_device(pdev);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -486,6 +486,10 @@\n \t    req->req_q_in, req->req_q_out, rsp->rsp_q_in, rsp->rsp_q_out);\n \n \tha->wq = alloc_workqueue(\"qla2xxx_wq\", 0, 0);\n+\tif (unlikely(!ha->wq)) {\n+\t\tret = -ENOMEM;\n+\t\tgoto probe_failed;\n+\t}\n \n \tif (ha->isp_ops->initialize_adapter(base_vha)) {\n \t\tql_log(ql_log_fatal, base_vha, 0x00d6,",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(!ha->wq)) {",
                "\t\tret = -ENOMEM;",
                "\t\tgoto probe_failed;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "drivers/scsi/qla2xxx/qla_os.c in the Linux kernel 5.2.14 does not check the alloc_workqueue return value, leading to a NULL pointer dereference.",
        "id": 2046
    },
    {
        "cve_id": "CVE-2019-15922",
        "code_before_change": "static int __init pf_init(void)\n{\t\t\t\t/* preliminary initialisation */\n\tstruct pf_unit *pf;\n\tint unit;\n\n\tif (disable)\n\t\treturn -EINVAL;\n\n\tpf_init_units();\n\n\tif (pf_detect())\n\t\treturn -ENODEV;\n\tpf_busy = 0;\n\n\tif (register_blkdev(major, name)) {\n\t\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++)\n\t\t\tput_disk(pf->disk);\n\t\treturn -EBUSY;\n\t}\n\n\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {\n\t\tstruct gendisk *disk = pf->disk;\n\n\t\tif (!pf->present)\n\t\t\tcontinue;\n\t\tdisk->private_data = pf;\n\t\tadd_disk(disk);\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int __init pf_init(void)\n{\t\t\t\t/* preliminary initialisation */\n\tstruct pf_unit *pf;\n\tint unit;\n\n\tif (disable)\n\t\treturn -EINVAL;\n\n\tpf_init_units();\n\n\tif (pf_detect())\n\t\treturn -ENODEV;\n\tpf_busy = 0;\n\n\tif (register_blkdev(major, name)) {\n\t\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {\n\t\t\tif (!pf->disk)\n\t\t\t\tcontinue;\n\t\t\tblk_cleanup_queue(pf->disk->queue);\n\t\t\tblk_mq_free_tag_set(&pf->tag_set);\n\t\t\tput_disk(pf->disk);\n\t\t}\n\t\treturn -EBUSY;\n\t}\n\n\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {\n\t\tstruct gendisk *disk = pf->disk;\n\n\t\tif (!pf->present)\n\t\t\tcontinue;\n\t\tdisk->private_data = pf;\n\t\tadd_disk(disk);\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,8 +13,13 @@\n \tpf_busy = 0;\n \n \tif (register_blkdev(major, name)) {\n-\t\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++)\n+\t\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {\n+\t\t\tif (!pf->disk)\n+\t\t\t\tcontinue;\n+\t\t\tblk_cleanup_queue(pf->disk->queue);\n+\t\t\tblk_mq_free_tag_set(&pf->tag_set);\n \t\t\tput_disk(pf->disk);\n+\t\t}\n \t\treturn -EBUSY;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {",
                "\t\t\tif (!pf->disk)",
                "\t\t\t\tcontinue;",
                "\t\t\tblk_cleanup_queue(pf->disk->queue);",
                "\t\t\tblk_mq_free_tag_set(&pf->tag_set);",
                "\t\t}"
            ],
            "deleted": [
                "\t\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++)"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.9. There is a NULL pointer dereference for a pf data structure if alloc_disk fails in drivers/block/paride/pf.c.",
        "id": 2031
    },
    {
        "cve_id": "CVE-2019-18885",
        "code_before_change": "int btrfs_init_dev_replace(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tint ret = 0;\n\tstruct btrfs_path *path = NULL;\n\tint item_size;\n\tstruct btrfs_dev_replace_item *ptr;\n\tu64 src_devid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_DEV_REPLACE_KEY;\n\tkey.offset = 0;\n\tret = btrfs_search_slot(NULL, dev_root, &key, path, 0, 0);\n\tif (ret) {\nno_valid_dev_replace_entry_found:\n\t\tret = 0;\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_DEV_REPLACE_ITEM_STATE_NEVER_STARTED;\n\t\tdev_replace->cont_reading_from_srcdev_mode =\n\t\t    BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_ALWAYS;\n\t\tdev_replace->time_started = 0;\n\t\tdev_replace->time_stopped = 0;\n\t\tatomic64_set(&dev_replace->num_write_errors, 0);\n\t\tatomic64_set(&dev_replace->num_uncorrectable_read_errors, 0);\n\t\tdev_replace->cursor_left = 0;\n\t\tdev_replace->committed_cursor_left = 0;\n\t\tdev_replace->cursor_left_last_write_of_item = 0;\n\t\tdev_replace->cursor_right = 0;\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tdev_replace->is_valid = 0;\n\t\tdev_replace->item_needs_writeback = 0;\n\t\tgoto out;\n\t}\n\tslot = path->slots[0];\n\teb = path->nodes[0];\n\titem_size = btrfs_item_size_nr(eb, slot);\n\tptr = btrfs_item_ptr(eb, slot, struct btrfs_dev_replace_item);\n\n\tif (item_size != sizeof(struct btrfs_dev_replace_item)) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t\"dev_replace entry found has unexpected size, ignore entry\");\n\t\tgoto no_valid_dev_replace_entry_found;\n\t}\n\n\tsrc_devid = btrfs_dev_replace_src_devid(eb, ptr);\n\tdev_replace->cont_reading_from_srcdev_mode =\n\t\tbtrfs_dev_replace_cont_reading_from_srcdev_mode(eb, ptr);\n\tdev_replace->replace_state = btrfs_dev_replace_replace_state(eb, ptr);\n\tdev_replace->time_started = btrfs_dev_replace_time_started(eb, ptr);\n\tdev_replace->time_stopped =\n\t\tbtrfs_dev_replace_time_stopped(eb, ptr);\n\tatomic64_set(&dev_replace->num_write_errors,\n\t\t     btrfs_dev_replace_num_write_errors(eb, ptr));\n\tatomic64_set(&dev_replace->num_uncorrectable_read_errors,\n\t\t     btrfs_dev_replace_num_uncorrectable_read_errors(eb, ptr));\n\tdev_replace->cursor_left = btrfs_dev_replace_cursor_left(eb, ptr);\n\tdev_replace->committed_cursor_left = dev_replace->cursor_left;\n\tdev_replace->cursor_left_last_write_of_item = dev_replace->cursor_left;\n\tdev_replace->cursor_right = btrfs_dev_replace_cursor_right(eb, ptr);\n\tdev_replace->is_valid = 1;\n\n\tdev_replace->item_needs_writeback = 0;\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tdev_replace->srcdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tsrc_devid, NULL, NULL);\n\t\tdev_replace->tgtdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tBTRFS_DEV_REPLACE_DEVID,\n\t\t\t\t\t\t\tNULL, NULL);\n\t\t/*\n\t\t * allow 'btrfs dev replace_cancel' if src/tgt device is\n\t\t * missing\n\t\t */\n\t\tif (!dev_replace->srcdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"srcdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t   src_devid);\n\t\t}\n\t\tif (!dev_replace->tgtdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"tgtdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t\tBTRFS_DEV_REPLACE_DEVID);\n\t\t}\n\t\tif (dev_replace->tgtdev) {\n\t\t\tif (dev_replace->srcdev) {\n\t\t\t\tdev_replace->tgtdev->total_bytes =\n\t\t\t\t\tdev_replace->srcdev->total_bytes;\n\t\t\t\tdev_replace->tgtdev->disk_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->disk_total_bytes;\n\t\t\t\tdev_replace->tgtdev->commit_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->commit_total_bytes;\n\t\t\t\tdev_replace->tgtdev->bytes_used =\n\t\t\t\t\tdev_replace->srcdev->bytes_used;\n\t\t\t\tdev_replace->tgtdev->commit_bytes_used =\n\t\t\t\t\tdev_replace->srcdev->commit_bytes_used;\n\t\t\t}\n\t\t\tset_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\n\t\t\tWARN_ON(fs_info->fs_devices->rw_devices == 0);\n\t\t\tdev_replace->tgtdev->io_width = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->io_align = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->sector_size = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->fs_info = fs_info;\n\t\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\t\t}\n\t\tbreak;\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}",
        "code_after_change": "int btrfs_init_dev_replace(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tint ret = 0;\n\tstruct btrfs_path *path = NULL;\n\tint item_size;\n\tstruct btrfs_dev_replace_item *ptr;\n\tu64 src_devid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_DEV_REPLACE_KEY;\n\tkey.offset = 0;\n\tret = btrfs_search_slot(NULL, dev_root, &key, path, 0, 0);\n\tif (ret) {\nno_valid_dev_replace_entry_found:\n\t\tret = 0;\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_DEV_REPLACE_ITEM_STATE_NEVER_STARTED;\n\t\tdev_replace->cont_reading_from_srcdev_mode =\n\t\t    BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_ALWAYS;\n\t\tdev_replace->time_started = 0;\n\t\tdev_replace->time_stopped = 0;\n\t\tatomic64_set(&dev_replace->num_write_errors, 0);\n\t\tatomic64_set(&dev_replace->num_uncorrectable_read_errors, 0);\n\t\tdev_replace->cursor_left = 0;\n\t\tdev_replace->committed_cursor_left = 0;\n\t\tdev_replace->cursor_left_last_write_of_item = 0;\n\t\tdev_replace->cursor_right = 0;\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tdev_replace->is_valid = 0;\n\t\tdev_replace->item_needs_writeback = 0;\n\t\tgoto out;\n\t}\n\tslot = path->slots[0];\n\teb = path->nodes[0];\n\titem_size = btrfs_item_size_nr(eb, slot);\n\tptr = btrfs_item_ptr(eb, slot, struct btrfs_dev_replace_item);\n\n\tif (item_size != sizeof(struct btrfs_dev_replace_item)) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t\"dev_replace entry found has unexpected size, ignore entry\");\n\t\tgoto no_valid_dev_replace_entry_found;\n\t}\n\n\tsrc_devid = btrfs_dev_replace_src_devid(eb, ptr);\n\tdev_replace->cont_reading_from_srcdev_mode =\n\t\tbtrfs_dev_replace_cont_reading_from_srcdev_mode(eb, ptr);\n\tdev_replace->replace_state = btrfs_dev_replace_replace_state(eb, ptr);\n\tdev_replace->time_started = btrfs_dev_replace_time_started(eb, ptr);\n\tdev_replace->time_stopped =\n\t\tbtrfs_dev_replace_time_stopped(eb, ptr);\n\tatomic64_set(&dev_replace->num_write_errors,\n\t\t     btrfs_dev_replace_num_write_errors(eb, ptr));\n\tatomic64_set(&dev_replace->num_uncorrectable_read_errors,\n\t\t     btrfs_dev_replace_num_uncorrectable_read_errors(eb, ptr));\n\tdev_replace->cursor_left = btrfs_dev_replace_cursor_left(eb, ptr);\n\tdev_replace->committed_cursor_left = dev_replace->cursor_left;\n\tdev_replace->cursor_left_last_write_of_item = dev_replace->cursor_left;\n\tdev_replace->cursor_right = btrfs_dev_replace_cursor_right(eb, ptr);\n\tdev_replace->is_valid = 1;\n\n\tdev_replace->item_needs_writeback = 0;\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tdev_replace->srcdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\tsrc_devid, NULL, NULL, true);\n\t\tdev_replace->tgtdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tBTRFS_DEV_REPLACE_DEVID,\n\t\t\t\t\t\t\tNULL, NULL, true);\n\t\t/*\n\t\t * allow 'btrfs dev replace_cancel' if src/tgt device is\n\t\t * missing\n\t\t */\n\t\tif (!dev_replace->srcdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"srcdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t   src_devid);\n\t\t}\n\t\tif (!dev_replace->tgtdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"tgtdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t\tBTRFS_DEV_REPLACE_DEVID);\n\t\t}\n\t\tif (dev_replace->tgtdev) {\n\t\t\tif (dev_replace->srcdev) {\n\t\t\t\tdev_replace->tgtdev->total_bytes =\n\t\t\t\t\tdev_replace->srcdev->total_bytes;\n\t\t\t\tdev_replace->tgtdev->disk_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->disk_total_bytes;\n\t\t\t\tdev_replace->tgtdev->commit_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->commit_total_bytes;\n\t\t\t\tdev_replace->tgtdev->bytes_used =\n\t\t\t\t\tdev_replace->srcdev->bytes_used;\n\t\t\t\tdev_replace->tgtdev->commit_bytes_used =\n\t\t\t\t\tdev_replace->srcdev->commit_bytes_used;\n\t\t\t}\n\t\t\tset_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\n\t\t\tWARN_ON(fs_info->fs_devices->rw_devices == 0);\n\t\t\tdev_replace->tgtdev->io_width = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->io_align = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->sector_size = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->fs_info = fs_info;\n\t\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\t\t}\n\t\tbreak;\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -81,10 +81,10 @@\n \tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n \tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n \t\tdev_replace->srcdev = btrfs_find_device(fs_info->fs_devices,\n-\t\t\t\t\t\t\tsrc_devid, NULL, NULL);\n+\t\t\t\t\t\tsrc_devid, NULL, NULL, true);\n \t\tdev_replace->tgtdev = btrfs_find_device(fs_info->fs_devices,\n \t\t\t\t\t\t\tBTRFS_DEV_REPLACE_DEVID,\n-\t\t\t\t\t\t\tNULL, NULL);\n+\t\t\t\t\t\t\tNULL, NULL, true);\n \t\t/*\n \t\t * allow 'btrfs dev replace_cancel' if src/tgt device is\n \t\t * missing",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\t\tsrc_devid, NULL, NULL, true);",
                "\t\t\t\t\t\t\tNULL, NULL, true);"
            ],
            "deleted": [
                "\t\t\t\t\t\t\tsrc_devid, NULL, NULL);",
                "\t\t\t\t\t\t\tNULL, NULL);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "fs/btrfs/volumes.c in the Linux kernel before 5.1 allows a btrfs_verify_dev_extents NULL pointer dereference via a crafted btrfs image because fs_devices->devices is mishandled within find_device, aka CID-09ba3bc9dd15.",
        "id": 2107
    },
    {
        "cve_id": "CVE-2019-18885",
        "code_before_change": "static int read_one_dev(struct btrfs_fs_info *fs_info,\n\t\t\tstruct extent_buffer *leaf,\n\t\t\tstruct btrfs_dev_item *dev_item)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tu64 devid;\n\tint ret;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\n\tdevid = btrfs_device_id(leaf, dev_item);\n\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t   BTRFS_UUID_SIZE);\n\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t   BTRFS_FSID_SIZE);\n\n\tif (memcmp(fs_uuid, fs_devices->metadata_uuid, BTRFS_FSID_SIZE)) {\n\t\tfs_devices = open_seed_devices(fs_info, fs_uuid);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn PTR_ERR(fs_devices);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t   fs_uuid);\n\tif (!device) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tdevice = add_missing_dev(fs_devices, devid, dev_uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"failed to add missing dev %llu: %ld\",\n\t\t\t\tdevid, PTR_ERR(device));\n\t\t\treturn PTR_ERR(device);\n\t\t}\n\t\tbtrfs_report_missing_device(fs_info, devid, dev_uuid, false);\n\t} else {\n\t\tif (!device->bdev) {\n\t\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\t\tbtrfs_report_missing_device(fs_info,\n\t\t\t\t\t\tdevid, dev_uuid, true);\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, false);\n\t\t}\n\n\t\tif (!device->bdev &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\t/*\n\t\t\t * this happens when a device that was properly setup\n\t\t\t * in the device info lists suddenly goes bad.\n\t\t\t * device->bdev is NULL, and so we have to set\n\t\t\t * device->missing to one here\n\t\t\t */\n\t\t\tdevice->fs_devices->missing_devices++;\n\t\t\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\n\t\t/* Move the device to its own fs_devices */\n\t\tif (device->fs_devices != fs_devices) {\n\t\t\tASSERT(test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t\t&device->dev_state));\n\n\t\t\tlist_move(&device->dev_list, &fs_devices->devices);\n\t\t\tdevice->fs_devices->num_devices--;\n\t\t\tfs_devices->num_devices++;\n\n\t\t\tdevice->fs_devices->missing_devices--;\n\t\t\tfs_devices->missing_devices++;\n\n\t\t\tdevice->fs_devices = fs_devices;\n\t\t}\n\t}\n\n\tif (device->fs_devices != fs_info->fs_devices) {\n\t\tBUG_ON(test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state));\n\t\tif (device->generation !=\n\t\t    btrfs_device_generation(leaf, dev_item))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfill_device_from_item(leaf, dev_item, device);\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t   !test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes += device->total_bytes;\n\t\tatomic64_add(device->total_bytes - device->bytes_used,\n\t\t\t\t&fs_info->free_chunk_space);\n\t}\n\tret = 0;\n\treturn ret;\n}",
        "code_after_change": "static int read_one_dev(struct btrfs_fs_info *fs_info,\n\t\t\tstruct extent_buffer *leaf,\n\t\t\tstruct btrfs_dev_item *dev_item)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tu64 devid;\n\tint ret;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\n\tdevid = btrfs_device_id(leaf, dev_item);\n\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t   BTRFS_UUID_SIZE);\n\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t   BTRFS_FSID_SIZE);\n\n\tif (memcmp(fs_uuid, fs_devices->metadata_uuid, BTRFS_FSID_SIZE)) {\n\t\tfs_devices = open_seed_devices(fs_info, fs_uuid);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn PTR_ERR(fs_devices);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t   fs_uuid, true);\n\tif (!device) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tdevice = add_missing_dev(fs_devices, devid, dev_uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"failed to add missing dev %llu: %ld\",\n\t\t\t\tdevid, PTR_ERR(device));\n\t\t\treturn PTR_ERR(device);\n\t\t}\n\t\tbtrfs_report_missing_device(fs_info, devid, dev_uuid, false);\n\t} else {\n\t\tif (!device->bdev) {\n\t\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\t\tbtrfs_report_missing_device(fs_info,\n\t\t\t\t\t\tdevid, dev_uuid, true);\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, false);\n\t\t}\n\n\t\tif (!device->bdev &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\t/*\n\t\t\t * this happens when a device that was properly setup\n\t\t\t * in the device info lists suddenly goes bad.\n\t\t\t * device->bdev is NULL, and so we have to set\n\t\t\t * device->missing to one here\n\t\t\t */\n\t\t\tdevice->fs_devices->missing_devices++;\n\t\t\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\n\t\t/* Move the device to its own fs_devices */\n\t\tif (device->fs_devices != fs_devices) {\n\t\t\tASSERT(test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t\t&device->dev_state));\n\n\t\t\tlist_move(&device->dev_list, &fs_devices->devices);\n\t\t\tdevice->fs_devices->num_devices--;\n\t\t\tfs_devices->num_devices++;\n\n\t\t\tdevice->fs_devices->missing_devices--;\n\t\t\tfs_devices->missing_devices++;\n\n\t\t\tdevice->fs_devices = fs_devices;\n\t\t}\n\t}\n\n\tif (device->fs_devices != fs_info->fs_devices) {\n\t\tBUG_ON(test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state));\n\t\tif (device->generation !=\n\t\t    btrfs_device_generation(leaf, dev_item))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfill_device_from_item(leaf, dev_item, device);\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t   !test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes += device->total_bytes;\n\t\tatomic64_add(device->total_bytes - device->bytes_used,\n\t\t\t\t&fs_info->free_chunk_space);\n\t}\n\tret = 0;\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,7 +22,7 @@\n \t}\n \n \tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n-\t\t\t\t   fs_uuid);\n+\t\t\t\t   fs_uuid, true);\n \tif (!device) {\n \t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n \t\t\tbtrfs_report_missing_device(fs_info, devid,",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t   fs_uuid, true);"
            ],
            "deleted": [
                "\t\t\t\t   fs_uuid);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "fs/btrfs/volumes.c in the Linux kernel before 5.1 allows a btrfs_verify_dev_extents NULL pointer dereference via a crafted btrfs image because fs_devices->devices is mishandled within find_device, aka CID-09ba3bc9dd15.",
        "id": 2113
    },
    {
        "cve_id": "CVE-2019-18885",
        "code_before_change": "static noinline int btrfs_ioctl_resize(struct file *file,\n\t\t\t\t\tvoid __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tu64 new_size;\n\tu64 old_size;\n\tu64 devid = 1;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device = NULL;\n\tchar *sizestr;\n\tchar *retptr;\n\tchar *devstr = NULL;\n\tint ret = 0;\n\tint mod = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tmnt_drop_write_file(file);\n\t\treturn BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t}\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tsizestr = vol_args->name;\n\tdevstr = strchr(sizestr, ':');\n\tif (devstr) {\n\t\tsizestr = devstr + 1;\n\t\t*devstr = '\\0';\n\t\tdevstr = vol_args->name;\n\t\tret = kstrtoull(devstr, 10, &devid);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t\tif (!devid) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tbtrfs_info(fs_info, \"resizing devid %llu\", devid);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!device) {\n\t\tbtrfs_info(fs_info, \"resizer unable to find device %llu\",\n\t\t\t   devid);\n\t\tret = -ENODEV;\n\t\tgoto out_free;\n\t}\n\n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"resizer unable to apply on readonly device %llu\",\n\t\t       devid);\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\tif (!strcmp(sizestr, \"max\"))\n\t\tnew_size = device->bdev->bd_inode->i_size;\n\telse {\n\t\tif (sizestr[0] == '-') {\n\t\t\tmod = -1;\n\t\t\tsizestr++;\n\t\t} else if (sizestr[0] == '+') {\n\t\t\tmod = 1;\n\t\t\tsizestr++;\n\t\t}\n\t\tnew_size = memparse(sizestr, &retptr);\n\t\tif (*retptr != '\\0' || new_size == 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\told_size = btrfs_device_get_total_bytes(device);\n\n\tif (mod < 0) {\n\t\tif (new_size > old_size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size - new_size;\n\t} else if (mod > 0) {\n\t\tif (new_size > ULLONG_MAX - old_size) {\n\t\t\tret = -ERANGE;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size + new_size;\n\t}\n\n\tif (new_size < SZ_256M) {\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\tif (new_size > device->bdev->bd_inode->i_size) {\n\t\tret = -EFBIG;\n\t\tgoto out_free;\n\t}\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\n\tbtrfs_info_in_rcu(fs_info, \"new size for %s is %llu\",\n\t\t\t  rcu_str_deref(device->name), new_size);\n\n\tif (new_size > old_size) {\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out_free;\n\t\t}\n\t\tret = btrfs_grow_device(trans, device, new_size);\n\t\tbtrfs_commit_transaction(trans);\n\t} else if (new_size < old_size) {\n\t\tret = btrfs_shrink_device(device, new_size);\n\t} /* equal, nothing need to do */\n\nout_free:\n\tkfree(vol_args);\nout:\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}",
        "code_after_change": "static noinline int btrfs_ioctl_resize(struct file *file,\n\t\t\t\t\tvoid __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tu64 new_size;\n\tu64 old_size;\n\tu64 devid = 1;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device = NULL;\n\tchar *sizestr;\n\tchar *retptr;\n\tchar *devstr = NULL;\n\tint ret = 0;\n\tint mod = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tmnt_drop_write_file(file);\n\t\treturn BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t}\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tsizestr = vol_args->name;\n\tdevstr = strchr(sizestr, ':');\n\tif (devstr) {\n\t\tsizestr = devstr + 1;\n\t\t*devstr = '\\0';\n\t\tdevstr = vol_args->name;\n\t\tret = kstrtoull(devstr, 10, &devid);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t\tif (!devid) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tbtrfs_info(fs_info, \"resizing devid %llu\", devid);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!device) {\n\t\tbtrfs_info(fs_info, \"resizer unable to find device %llu\",\n\t\t\t   devid);\n\t\tret = -ENODEV;\n\t\tgoto out_free;\n\t}\n\n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"resizer unable to apply on readonly device %llu\",\n\t\t       devid);\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\tif (!strcmp(sizestr, \"max\"))\n\t\tnew_size = device->bdev->bd_inode->i_size;\n\telse {\n\t\tif (sizestr[0] == '-') {\n\t\t\tmod = -1;\n\t\t\tsizestr++;\n\t\t} else if (sizestr[0] == '+') {\n\t\t\tmod = 1;\n\t\t\tsizestr++;\n\t\t}\n\t\tnew_size = memparse(sizestr, &retptr);\n\t\tif (*retptr != '\\0' || new_size == 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\told_size = btrfs_device_get_total_bytes(device);\n\n\tif (mod < 0) {\n\t\tif (new_size > old_size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size - new_size;\n\t} else if (mod > 0) {\n\t\tif (new_size > ULLONG_MAX - old_size) {\n\t\t\tret = -ERANGE;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size + new_size;\n\t}\n\n\tif (new_size < SZ_256M) {\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\tif (new_size > device->bdev->bd_inode->i_size) {\n\t\tret = -EFBIG;\n\t\tgoto out_free;\n\t}\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\n\tbtrfs_info_in_rcu(fs_info, \"new size for %s is %llu\",\n\t\t\t  rcu_str_deref(device->name), new_size);\n\n\tif (new_size > old_size) {\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out_free;\n\t\t}\n\t\tret = btrfs_grow_device(trans, device, new_size);\n\t\tbtrfs_commit_transaction(trans);\n\t} else if (new_size < old_size) {\n\t\tret = btrfs_shrink_device(device, new_size);\n\t} /* equal, nothing need to do */\n\nout_free:\n\tkfree(vol_args);\nout:\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -52,7 +52,7 @@\n \t\tbtrfs_info(fs_info, \"resizing devid %llu\", devid);\n \t}\n \n-\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n+\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n \tif (!device) {\n \t\tbtrfs_info(fs_info, \"resizer unable to find device %llu\",\n \t\t\t   devid);",
        "function_modified_lines": {
            "added": [
                "\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);"
            ],
            "deleted": [
                "\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "fs/btrfs/volumes.c in the Linux kernel before 5.1 allows a btrfs_verify_dev_extents NULL pointer dereference via a crafted btrfs image because fs_devices->devices is mishandled within find_device, aka CID-09ba3bc9dd15.",
        "id": 2108
    },
    {
        "cve_id": "CVE-2017-15116",
        "code_before_change": "static unsigned int seedsize(struct crypto_alg *alg)\n{\n\tstruct rng_alg *ralg = container_of(alg, struct rng_alg, base);\n\n\treturn alg->cra_rng.rng_make_random ?\n\t       alg->cra_rng.seedsize : ralg->seedsize;\n}",
        "code_after_change": "static unsigned int seedsize(struct crypto_alg *alg)\n{\n\tstruct rng_alg *ralg = container_of(alg, struct rng_alg, base);\n\n\treturn ralg->seedsize;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,5 @@\n {\n \tstruct rng_alg *ralg = container_of(alg, struct rng_alg, base);\n \n-\treturn alg->cra_rng.rng_make_random ?\n-\t       alg->cra_rng.seedsize : ralg->seedsize;\n+\treturn ralg->seedsize;\n }",
        "function_modified_lines": {
            "added": [
                "\treturn ralg->seedsize;"
            ],
            "deleted": [
                "\treturn alg->cra_rng.rng_make_random ?",
                "\t       alg->cra_rng.seedsize : ralg->seedsize;"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The rngapi_reset function in crypto/rng.c in the Linux kernel before 4.2 allows attackers to cause a denial of service (NULL pointer dereference).",
        "id": 1292
    },
    {
        "cve_id": "CVE-2022-4842",
        "code_before_change": "int attr_punch_hole(struct ntfs_inode *ni, u64 vbo, u64 bytes, u32 *frame_size)\n{\n\tint err = 0;\n\tstruct runs_tree *run = &ni->file.run;\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tstruct ATTRIB *attr = NULL, *attr_b;\n\tstruct ATTR_LIST_ENTRY *le, *le_b;\n\tstruct mft_inode *mi, *mi_b;\n\tCLST svcn, evcn1, vcn, len, end, alen, hole, next_svcn;\n\tu64 total_size, alloc_size;\n\tu32 mask;\n\t__le16 a_flags;\n\tstruct runs_tree run2;\n\n\tif (!bytes)\n\t\treturn 0;\n\n\tle_b = NULL;\n\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL, &mi_b);\n\tif (!attr_b)\n\t\treturn -ENOENT;\n\n\tif (!attr_b->non_res) {\n\t\tu32 data_size = le32_to_cpu(attr->res.data_size);\n\t\tu32 from, to;\n\n\t\tif (vbo > data_size)\n\t\t\treturn 0;\n\n\t\tfrom = vbo;\n\t\tto = min_t(u64, vbo + bytes, data_size);\n\t\tmemset(Add2Ptr(resident_data(attr_b), from), 0, to - from);\n\t\treturn 0;\n\t}\n\n\tif (!is_attr_ext(attr_b))\n\t\treturn -EOPNOTSUPP;\n\n\talloc_size = le64_to_cpu(attr_b->nres.alloc_size);\n\ttotal_size = le64_to_cpu(attr_b->nres.total_size);\n\n\tif (vbo >= alloc_size) {\n\t\t/* NOTE: It is allowed. */\n\t\treturn 0;\n\t}\n\n\tmask = (sbi->cluster_size << attr_b->nres.c_unit) - 1;\n\n\tbytes += vbo;\n\tif (bytes > alloc_size)\n\t\tbytes = alloc_size;\n\tbytes -= vbo;\n\n\tif ((vbo & mask) || (bytes & mask)) {\n\t\t/* We have to zero a range(s). */\n\t\tif (frame_size == NULL) {\n\t\t\t/* Caller insists range is aligned. */\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*frame_size = mask + 1;\n\t\treturn E_NTFS_NOTALIGNED;\n\t}\n\n\tdown_write(&ni->file.run_lock);\n\trun_init(&run2);\n\trun_truncate(run, 0);\n\n\t/*\n\t * Enumerate all attribute segments and punch hole where necessary.\n\t */\n\talen = alloc_size >> sbi->cluster_bits;\n\tvcn = vbo >> sbi->cluster_bits;\n\tlen = bytes >> sbi->cluster_bits;\n\tend = vcn + len;\n\thole = 0;\n\n\tsvcn = le64_to_cpu(attr_b->nres.svcn);\n\tevcn1 = le64_to_cpu(attr_b->nres.evcn) + 1;\n\ta_flags = attr_b->flags;\n\n\tif (svcn <= vcn && vcn < evcn1) {\n\t\tattr = attr_b;\n\t\tle = le_b;\n\t\tmi = mi_b;\n\t} else if (!le_b) {\n\t\terr = -EINVAL;\n\t\tgoto bad_inode;\n\t} else {\n\t\tle = le_b;\n\t\tattr = ni_find_attr(ni, attr_b, &le, ATTR_DATA, NULL, 0, &vcn,\n\t\t\t\t    &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\n\twhile (svcn < end) {\n\t\tCLST vcn1, zero, hole2 = hole;\n\n\t\terr = attr_load_runs(attr, ni, run, &svcn);\n\t\tif (err)\n\t\t\tgoto done;\n\t\tvcn1 = max(vcn, svcn);\n\t\tzero = min(end, evcn1) - vcn1;\n\n\t\t/*\n\t\t * Check range [vcn1 + zero).\n\t\t * Calculate how many clusters there are.\n\t\t * Don't do any destructive actions.\n\t\t */\n\t\terr = run_deallocate_ex(NULL, run, vcn1, zero, &hole2, false);\n\t\tif (err)\n\t\t\tgoto done;\n\n\t\t/* Check if required range is already hole. */\n\t\tif (hole2 == hole)\n\t\t\tgoto next_attr;\n\n\t\t/* Make a clone of run to undo. */\n\t\terr = run_clone(run, &run2);\n\t\tif (err)\n\t\t\tgoto done;\n\n\t\t/* Make a hole range (sparse) [vcn1 + zero). */\n\t\tif (!run_add_entry(run, vcn1, SPARSE_LCN, zero, false)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto done;\n\t\t}\n\n\t\t/* Update run in attribute segment. */\n\t\terr = mi_pack_runs(mi, attr, run, evcn1 - svcn);\n\t\tif (err)\n\t\t\tgoto done;\n\t\tnext_svcn = le64_to_cpu(attr->nres.evcn) + 1;\n\t\tif (next_svcn < evcn1) {\n\t\t\t/* Insert new attribute segment. */\n\t\t\terr = ni_insert_nonresident(ni, ATTR_DATA, NULL, 0, run,\n\t\t\t\t\t\t    next_svcn,\n\t\t\t\t\t\t    evcn1 - next_svcn, a_flags,\n\t\t\t\t\t\t    &attr, &mi, &le);\n\t\t\tif (err)\n\t\t\t\tgoto undo_punch;\n\n\t\t\t/* Layout of records maybe changed. */\n\t\t\tattr_b = NULL;\n\t\t}\n\n\t\t/* Real deallocate. Should not fail. */\n\t\trun_deallocate_ex(sbi, &run2, vcn1, zero, &hole, true);\n\nnext_attr:\n\t\t/* Free all allocated memory. */\n\t\trun_truncate(run, 0);\n\n\t\tif (evcn1 >= alen)\n\t\t\tbreak;\n\n\t\t/* Get next attribute segment. */\n\t\tattr = ni_enum_attr_ex(ni, attr, &le, &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\ndone:\n\tif (!hole)\n\t\tgoto out;\n\n\tif (!attr_b) {\n\t\tattr_b = ni_find_attr(ni, NULL, NULL, ATTR_DATA, NULL, 0, NULL,\n\t\t\t\t      &mi_b);\n\t\tif (!attr_b) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\t}\n\n\ttotal_size -= (u64)hole << sbi->cluster_bits;\n\tattr_b->nres.total_size = cpu_to_le64(total_size);\n\tmi_b->dirty = true;\n\n\t/* Update inode size. */\n\tinode_set_bytes(&ni->vfs_inode, total_size);\n\tni->ni_flags |= NI_FLAG_UPDATE_PARENT;\n\tmark_inode_dirty(&ni->vfs_inode);\n\nout:\n\trun_close(&run2);\n\tup_write(&ni->file.run_lock);\n\treturn err;\n\nbad_inode:\n\t_ntfs_bad_inode(&ni->vfs_inode);\n\tgoto out;\n\nundo_punch:\n\t/*\n\t * Restore packed runs.\n\t * 'mi_pack_runs' should not fail, cause we restore original.\n\t */\n\tif (mi_pack_runs(mi, attr, &run2, evcn1 - svcn))\n\t\tgoto bad_inode;\n\n\tgoto done;\n}",
        "code_after_change": "int attr_punch_hole(struct ntfs_inode *ni, u64 vbo, u64 bytes, u32 *frame_size)\n{\n\tint err = 0;\n\tstruct runs_tree *run = &ni->file.run;\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tstruct ATTRIB *attr = NULL, *attr_b;\n\tstruct ATTR_LIST_ENTRY *le, *le_b;\n\tstruct mft_inode *mi, *mi_b;\n\tCLST svcn, evcn1, vcn, len, end, alen, hole, next_svcn;\n\tu64 total_size, alloc_size;\n\tu32 mask;\n\t__le16 a_flags;\n\tstruct runs_tree run2;\n\n\tif (!bytes)\n\t\treturn 0;\n\n\tle_b = NULL;\n\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL, &mi_b);\n\tif (!attr_b)\n\t\treturn -ENOENT;\n\n\tif (!attr_b->non_res) {\n\t\tu32 data_size = le32_to_cpu(attr_b->res.data_size);\n\t\tu32 from, to;\n\n\t\tif (vbo > data_size)\n\t\t\treturn 0;\n\n\t\tfrom = vbo;\n\t\tto = min_t(u64, vbo + bytes, data_size);\n\t\tmemset(Add2Ptr(resident_data(attr_b), from), 0, to - from);\n\t\treturn 0;\n\t}\n\n\tif (!is_attr_ext(attr_b))\n\t\treturn -EOPNOTSUPP;\n\n\talloc_size = le64_to_cpu(attr_b->nres.alloc_size);\n\ttotal_size = le64_to_cpu(attr_b->nres.total_size);\n\n\tif (vbo >= alloc_size) {\n\t\t/* NOTE: It is allowed. */\n\t\treturn 0;\n\t}\n\n\tmask = (sbi->cluster_size << attr_b->nres.c_unit) - 1;\n\n\tbytes += vbo;\n\tif (bytes > alloc_size)\n\t\tbytes = alloc_size;\n\tbytes -= vbo;\n\n\tif ((vbo & mask) || (bytes & mask)) {\n\t\t/* We have to zero a range(s). */\n\t\tif (frame_size == NULL) {\n\t\t\t/* Caller insists range is aligned. */\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*frame_size = mask + 1;\n\t\treturn E_NTFS_NOTALIGNED;\n\t}\n\n\tdown_write(&ni->file.run_lock);\n\trun_init(&run2);\n\trun_truncate(run, 0);\n\n\t/*\n\t * Enumerate all attribute segments and punch hole where necessary.\n\t */\n\talen = alloc_size >> sbi->cluster_bits;\n\tvcn = vbo >> sbi->cluster_bits;\n\tlen = bytes >> sbi->cluster_bits;\n\tend = vcn + len;\n\thole = 0;\n\n\tsvcn = le64_to_cpu(attr_b->nres.svcn);\n\tevcn1 = le64_to_cpu(attr_b->nres.evcn) + 1;\n\ta_flags = attr_b->flags;\n\n\tif (svcn <= vcn && vcn < evcn1) {\n\t\tattr = attr_b;\n\t\tle = le_b;\n\t\tmi = mi_b;\n\t} else if (!le_b) {\n\t\terr = -EINVAL;\n\t\tgoto bad_inode;\n\t} else {\n\t\tle = le_b;\n\t\tattr = ni_find_attr(ni, attr_b, &le, ATTR_DATA, NULL, 0, &vcn,\n\t\t\t\t    &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\n\twhile (svcn < end) {\n\t\tCLST vcn1, zero, hole2 = hole;\n\n\t\terr = attr_load_runs(attr, ni, run, &svcn);\n\t\tif (err)\n\t\t\tgoto done;\n\t\tvcn1 = max(vcn, svcn);\n\t\tzero = min(end, evcn1) - vcn1;\n\n\t\t/*\n\t\t * Check range [vcn1 + zero).\n\t\t * Calculate how many clusters there are.\n\t\t * Don't do any destructive actions.\n\t\t */\n\t\terr = run_deallocate_ex(NULL, run, vcn1, zero, &hole2, false);\n\t\tif (err)\n\t\t\tgoto done;\n\n\t\t/* Check if required range is already hole. */\n\t\tif (hole2 == hole)\n\t\t\tgoto next_attr;\n\n\t\t/* Make a clone of run to undo. */\n\t\terr = run_clone(run, &run2);\n\t\tif (err)\n\t\t\tgoto done;\n\n\t\t/* Make a hole range (sparse) [vcn1 + zero). */\n\t\tif (!run_add_entry(run, vcn1, SPARSE_LCN, zero, false)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto done;\n\t\t}\n\n\t\t/* Update run in attribute segment. */\n\t\terr = mi_pack_runs(mi, attr, run, evcn1 - svcn);\n\t\tif (err)\n\t\t\tgoto done;\n\t\tnext_svcn = le64_to_cpu(attr->nres.evcn) + 1;\n\t\tif (next_svcn < evcn1) {\n\t\t\t/* Insert new attribute segment. */\n\t\t\terr = ni_insert_nonresident(ni, ATTR_DATA, NULL, 0, run,\n\t\t\t\t\t\t    next_svcn,\n\t\t\t\t\t\t    evcn1 - next_svcn, a_flags,\n\t\t\t\t\t\t    &attr, &mi, &le);\n\t\t\tif (err)\n\t\t\t\tgoto undo_punch;\n\n\t\t\t/* Layout of records maybe changed. */\n\t\t\tattr_b = NULL;\n\t\t}\n\n\t\t/* Real deallocate. Should not fail. */\n\t\trun_deallocate_ex(sbi, &run2, vcn1, zero, &hole, true);\n\nnext_attr:\n\t\t/* Free all allocated memory. */\n\t\trun_truncate(run, 0);\n\n\t\tif (evcn1 >= alen)\n\t\t\tbreak;\n\n\t\t/* Get next attribute segment. */\n\t\tattr = ni_enum_attr_ex(ni, attr, &le, &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\ndone:\n\tif (!hole)\n\t\tgoto out;\n\n\tif (!attr_b) {\n\t\tattr_b = ni_find_attr(ni, NULL, NULL, ATTR_DATA, NULL, 0, NULL,\n\t\t\t\t      &mi_b);\n\t\tif (!attr_b) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\t}\n\n\ttotal_size -= (u64)hole << sbi->cluster_bits;\n\tattr_b->nres.total_size = cpu_to_le64(total_size);\n\tmi_b->dirty = true;\n\n\t/* Update inode size. */\n\tinode_set_bytes(&ni->vfs_inode, total_size);\n\tni->ni_flags |= NI_FLAG_UPDATE_PARENT;\n\tmark_inode_dirty(&ni->vfs_inode);\n\nout:\n\trun_close(&run2);\n\tup_write(&ni->file.run_lock);\n\treturn err;\n\nbad_inode:\n\t_ntfs_bad_inode(&ni->vfs_inode);\n\tgoto out;\n\nundo_punch:\n\t/*\n\t * Restore packed runs.\n\t * 'mi_pack_runs' should not fail, cause we restore original.\n\t */\n\tif (mi_pack_runs(mi, attr, &run2, evcn1 - svcn))\n\t\tgoto bad_inode;\n\n\tgoto done;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,7 +21,7 @@\n \t\treturn -ENOENT;\n \n \tif (!attr_b->non_res) {\n-\t\tu32 data_size = le32_to_cpu(attr->res.data_size);\n+\t\tu32 data_size = le32_to_cpu(attr_b->res.data_size);\n \t\tu32 from, to;\n \n \t\tif (vbo > data_size)",
        "function_modified_lines": {
            "added": [
                "\t\tu32 data_size = le32_to_cpu(attr_b->res.data_size);"
            ],
            "deleted": [
                "\t\tu32 data_size = le32_to_cpu(attr->res.data_size);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A flaw NULL Pointer Dereference in the Linux kernel NTFS3 driver function attr_punch_hole() was found. A local user could use this flaw to crash the system.",
        "id": 3788
    },
    {
        "cve_id": "CVE-2019-12455",
        "code_before_change": "static struct clk ** __init sunxi_divs_clk_setup(struct device_node *node,\n\t\t\t\t\t\t const struct divs_data *data)\n{\n\tstruct clk_onecell_data *clk_data;\n\tconst char *parent;\n\tconst char *clk_name;\n\tstruct clk **clks, *pclk;\n\tstruct clk_hw *gate_hw, *rate_hw;\n\tconst struct clk_ops *rate_ops;\n\tstruct clk_gate *gate = NULL;\n\tstruct clk_fixed_factor *fix_factor;\n\tstruct clk_divider *divider;\n\tstruct factors_data factors = *data->factors;\n\tchar *derived_name = NULL;\n\tvoid __iomem *reg;\n\tint ndivs = SUNXI_DIVS_MAX_QTY, i = 0;\n\tint flags, clkflags;\n\n\t/* if number of children known, use it */\n\tif (data->ndivs)\n\t\tndivs = data->ndivs;\n\n\t/* Try to find a name for base factor clock */\n\tfor (i = 0; i < ndivs; i++) {\n\t\tif (data->div[i].self) {\n\t\t\tof_property_read_string_index(node, \"clock-output-names\",\n\t\t\t\t\t\t      i, &factors.name);\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* If we don't have a .self clk use the first output-name up to '_' */\n\tif (factors.name == NULL) {\n\t\tchar *endp;\n\n\t\tof_property_read_string_index(node, \"clock-output-names\",\n\t\t\t\t\t\t      0, &clk_name);\n\t\tendp = strchr(clk_name, '_');\n\t\tif (endp) {\n\t\t\tderived_name = kstrndup(clk_name, endp - clk_name,\n\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tfactors.name = derived_name;\n\t\t} else {\n\t\t\tfactors.name = clk_name;\n\t\t}\n\t}\n\n\t/* Set up factor clock that we will be dividing */\n\tpclk = sunxi_factors_clk_setup(node, &factors);\n\tif (!pclk)\n\t\treturn NULL;\n\n\tparent = __clk_get_name(pclk);\n\tkfree(derived_name);\n\n\treg = of_iomap(node, 0);\n\tif (!reg) {\n\t\tpr_err(\"Could not map registers for divs-clk: %pOF\\n\", node);\n\t\treturn NULL;\n\t}\n\n\tclk_data = kmalloc(sizeof(struct clk_onecell_data), GFP_KERNEL);\n\tif (!clk_data)\n\t\tgoto out_unmap;\n\n\tclks = kcalloc(ndivs, sizeof(*clks), GFP_KERNEL);\n\tif (!clks)\n\t\tgoto free_clkdata;\n\n\tclk_data->clks = clks;\n\n\t/* It's not a good idea to have automatic reparenting changing\n\t * our RAM clock! */\n\tclkflags = !strcmp(\"pll5\", parent) ? 0 : CLK_SET_RATE_PARENT;\n\n\tfor (i = 0; i < ndivs; i++) {\n\t\tif (of_property_read_string_index(node, \"clock-output-names\",\n\t\t\t\t\t\t  i, &clk_name) != 0)\n\t\t\tbreak;\n\n\t\t/* If this is the base factor clock, only update clks */\n\t\tif (data->div[i].self) {\n\t\t\tclk_data->clks[i] = pclk;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgate_hw = NULL;\n\t\trate_hw = NULL;\n\t\trate_ops = NULL;\n\n\t\t/* If this leaf clock can be gated, create a gate */\n\t\tif (data->div[i].gate) {\n\t\t\tgate = kzalloc(sizeof(*gate), GFP_KERNEL);\n\t\t\tif (!gate)\n\t\t\t\tgoto free_clks;\n\n\t\t\tgate->reg = reg;\n\t\t\tgate->bit_idx = data->div[i].gate;\n\t\t\tgate->lock = &clk_lock;\n\n\t\t\tgate_hw = &gate->hw;\n\t\t}\n\n\t\t/* Leaves can be fixed or configurable divisors */\n\t\tif (data->div[i].fixed) {\n\t\t\tfix_factor = kzalloc(sizeof(*fix_factor), GFP_KERNEL);\n\t\t\tif (!fix_factor)\n\t\t\t\tgoto free_gate;\n\n\t\t\tfix_factor->mult = 1;\n\t\t\tfix_factor->div = data->div[i].fixed;\n\n\t\t\trate_hw = &fix_factor->hw;\n\t\t\trate_ops = &clk_fixed_factor_ops;\n\t\t} else {\n\t\t\tdivider = kzalloc(sizeof(*divider), GFP_KERNEL);\n\t\t\tif (!divider)\n\t\t\t\tgoto free_gate;\n\n\t\t\tflags = data->div[i].pow ? CLK_DIVIDER_POWER_OF_TWO : 0;\n\n\t\t\tdivider->reg = reg;\n\t\t\tdivider->shift = data->div[i].shift;\n\t\t\tdivider->width = SUNXI_DIVISOR_WIDTH;\n\t\t\tdivider->flags = flags;\n\t\t\tdivider->lock = &clk_lock;\n\t\t\tdivider->table = data->div[i].table;\n\n\t\t\trate_hw = &divider->hw;\n\t\t\trate_ops = &clk_divider_ops;\n\t\t}\n\n\t\t/* Wrap the (potential) gate and the divisor on a composite\n\t\t * clock to unify them */\n\t\tclks[i] = clk_register_composite(NULL, clk_name, &parent, 1,\n\t\t\t\t\t\t NULL, NULL,\n\t\t\t\t\t\t rate_hw, rate_ops,\n\t\t\t\t\t\t gate_hw, &clk_gate_ops,\n\t\t\t\t\t\t clkflags |\n\t\t\t\t\t\t data->div[i].critical ?\n\t\t\t\t\t\t\tCLK_IS_CRITICAL : 0);\n\n\t\tWARN_ON(IS_ERR(clk_data->clks[i]));\n\t}\n\n\t/* Adjust to the real max */\n\tclk_data->clk_num = i;\n\n\tif (of_clk_add_provider(node, of_clk_src_onecell_get, clk_data)) {\n\t\tpr_err(\"%s: failed to add clock provider for %s\\n\",\n\t\t       __func__, clk_name);\n\t\tgoto free_gate;\n\t}\n\n\treturn clks;\nfree_gate:\n\tkfree(gate);\nfree_clks:\n\tkfree(clks);\nfree_clkdata:\n\tkfree(clk_data);\nout_unmap:\n\tiounmap(reg);\n\treturn NULL;\n}",
        "code_after_change": "static struct clk ** __init sunxi_divs_clk_setup(struct device_node *node,\n\t\t\t\t\t\t const struct divs_data *data)\n{\n\tstruct clk_onecell_data *clk_data;\n\tconst char *parent;\n\tconst char *clk_name;\n\tstruct clk **clks, *pclk;\n\tstruct clk_hw *gate_hw, *rate_hw;\n\tconst struct clk_ops *rate_ops;\n\tstruct clk_gate *gate = NULL;\n\tstruct clk_fixed_factor *fix_factor;\n\tstruct clk_divider *divider;\n\tstruct factors_data factors = *data->factors;\n\tchar *derived_name = NULL;\n\tvoid __iomem *reg;\n\tint ndivs = SUNXI_DIVS_MAX_QTY, i = 0;\n\tint flags, clkflags;\n\n\t/* if number of children known, use it */\n\tif (data->ndivs)\n\t\tndivs = data->ndivs;\n\n\t/* Try to find a name for base factor clock */\n\tfor (i = 0; i < ndivs; i++) {\n\t\tif (data->div[i].self) {\n\t\t\tof_property_read_string_index(node, \"clock-output-names\",\n\t\t\t\t\t\t      i, &factors.name);\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* If we don't have a .self clk use the first output-name up to '_' */\n\tif (factors.name == NULL) {\n\t\tchar *endp;\n\n\t\tof_property_read_string_index(node, \"clock-output-names\",\n\t\t\t\t\t\t      0, &clk_name);\n\t\tendp = strchr(clk_name, '_');\n\t\tif (endp) {\n\t\t\tderived_name = kstrndup(clk_name, endp - clk_name,\n\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!derived_name)\n\t\t\t\treturn NULL;\n\t\t\tfactors.name = derived_name;\n\t\t} else {\n\t\t\tfactors.name = clk_name;\n\t\t}\n\t}\n\n\t/* Set up factor clock that we will be dividing */\n\tpclk = sunxi_factors_clk_setup(node, &factors);\n\tif (!pclk)\n\t\treturn NULL;\n\n\tparent = __clk_get_name(pclk);\n\tkfree(derived_name);\n\n\treg = of_iomap(node, 0);\n\tif (!reg) {\n\t\tpr_err(\"Could not map registers for divs-clk: %pOF\\n\", node);\n\t\treturn NULL;\n\t}\n\n\tclk_data = kmalloc(sizeof(struct clk_onecell_data), GFP_KERNEL);\n\tif (!clk_data)\n\t\tgoto out_unmap;\n\n\tclks = kcalloc(ndivs, sizeof(*clks), GFP_KERNEL);\n\tif (!clks)\n\t\tgoto free_clkdata;\n\n\tclk_data->clks = clks;\n\n\t/* It's not a good idea to have automatic reparenting changing\n\t * our RAM clock! */\n\tclkflags = !strcmp(\"pll5\", parent) ? 0 : CLK_SET_RATE_PARENT;\n\n\tfor (i = 0; i < ndivs; i++) {\n\t\tif (of_property_read_string_index(node, \"clock-output-names\",\n\t\t\t\t\t\t  i, &clk_name) != 0)\n\t\t\tbreak;\n\n\t\t/* If this is the base factor clock, only update clks */\n\t\tif (data->div[i].self) {\n\t\t\tclk_data->clks[i] = pclk;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgate_hw = NULL;\n\t\trate_hw = NULL;\n\t\trate_ops = NULL;\n\n\t\t/* If this leaf clock can be gated, create a gate */\n\t\tif (data->div[i].gate) {\n\t\t\tgate = kzalloc(sizeof(*gate), GFP_KERNEL);\n\t\t\tif (!gate)\n\t\t\t\tgoto free_clks;\n\n\t\t\tgate->reg = reg;\n\t\t\tgate->bit_idx = data->div[i].gate;\n\t\t\tgate->lock = &clk_lock;\n\n\t\t\tgate_hw = &gate->hw;\n\t\t}\n\n\t\t/* Leaves can be fixed or configurable divisors */\n\t\tif (data->div[i].fixed) {\n\t\t\tfix_factor = kzalloc(sizeof(*fix_factor), GFP_KERNEL);\n\t\t\tif (!fix_factor)\n\t\t\t\tgoto free_gate;\n\n\t\t\tfix_factor->mult = 1;\n\t\t\tfix_factor->div = data->div[i].fixed;\n\n\t\t\trate_hw = &fix_factor->hw;\n\t\t\trate_ops = &clk_fixed_factor_ops;\n\t\t} else {\n\t\t\tdivider = kzalloc(sizeof(*divider), GFP_KERNEL);\n\t\t\tif (!divider)\n\t\t\t\tgoto free_gate;\n\n\t\t\tflags = data->div[i].pow ? CLK_DIVIDER_POWER_OF_TWO : 0;\n\n\t\t\tdivider->reg = reg;\n\t\t\tdivider->shift = data->div[i].shift;\n\t\t\tdivider->width = SUNXI_DIVISOR_WIDTH;\n\t\t\tdivider->flags = flags;\n\t\t\tdivider->lock = &clk_lock;\n\t\t\tdivider->table = data->div[i].table;\n\n\t\t\trate_hw = &divider->hw;\n\t\t\trate_ops = &clk_divider_ops;\n\t\t}\n\n\t\t/* Wrap the (potential) gate and the divisor on a composite\n\t\t * clock to unify them */\n\t\tclks[i] = clk_register_composite(NULL, clk_name, &parent, 1,\n\t\t\t\t\t\t NULL, NULL,\n\t\t\t\t\t\t rate_hw, rate_ops,\n\t\t\t\t\t\t gate_hw, &clk_gate_ops,\n\t\t\t\t\t\t clkflags |\n\t\t\t\t\t\t data->div[i].critical ?\n\t\t\t\t\t\t\tCLK_IS_CRITICAL : 0);\n\n\t\tWARN_ON(IS_ERR(clk_data->clks[i]));\n\t}\n\n\t/* Adjust to the real max */\n\tclk_data->clk_num = i;\n\n\tif (of_clk_add_provider(node, of_clk_src_onecell_get, clk_data)) {\n\t\tpr_err(\"%s: failed to add clock provider for %s\\n\",\n\t\t       __func__, clk_name);\n\t\tgoto free_gate;\n\t}\n\n\treturn clks;\nfree_gate:\n\tkfree(gate);\nfree_clks:\n\tkfree(clks);\nfree_clkdata:\n\tkfree(clk_data);\nout_unmap:\n\tiounmap(reg);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,6 +38,8 @@\n \t\tif (endp) {\n \t\t\tderived_name = kstrndup(clk_name, endp - clk_name,\n \t\t\t\t\t\tGFP_KERNEL);\n+\t\t\tif (!derived_name)\n+\t\t\t\treturn NULL;\n \t\t\tfactors.name = derived_name;\n \t\t} else {\n \t\t\tfactors.name = clk_name;",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (!derived_name)",
                "\t\t\t\treturn NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in sunxi_divs_clk_setup in drivers/clk/sunxi/clk-sunxi.c in the Linux kernel through 5.1.5. There is an unchecked kstrndup of derived_name, which might allow an attacker to cause a denial of service (NULL pointer dereference and system crash). NOTE: This id is disputed as not being an issue because “The memory allocation that was not checked is part of a code that only runs at boot time, before user processes are started. Therefore, there is no possibility for an unprivileged user to control it, and no denial of service.”",
        "id": 1946
    },
    {
        "cve_id": "CVE-2019-15923",
        "code_before_change": "static void __exit pcd_exit(void)\n{\n\tstruct pcd_unit *cd;\n\tint unit;\n\n\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\tif (cd->present) {\n\t\t\tdel_gendisk(cd->disk);\n\t\t\tpi_release(cd->pi);\n\t\t\tunregister_cdrom(&cd->info);\n\t\t}\n\t\tblk_cleanup_queue(cd->disk->queue);\n\t\tblk_mq_free_tag_set(&cd->tag_set);\n\t\tput_disk(cd->disk);\n\t}\n\tunregister_blkdev(major, name);\n\tpi_unregister_driver(par_drv);\n}",
        "code_after_change": "static void __exit pcd_exit(void)\n{\n\tstruct pcd_unit *cd;\n\tint unit;\n\n\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\tif (!cd->disk)\n\t\t\tcontinue;\n\n\t\tif (cd->present) {\n\t\t\tdel_gendisk(cd->disk);\n\t\t\tpi_release(cd->pi);\n\t\t\tunregister_cdrom(&cd->info);\n\t\t}\n\t\tblk_cleanup_queue(cd->disk->queue);\n\t\tblk_mq_free_tag_set(&cd->tag_set);\n\t\tput_disk(cd->disk);\n\t}\n\tunregister_blkdev(major, name);\n\tpi_unregister_driver(par_drv);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,9 @@\n \tint unit;\n \n \tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n+\t\tif (!cd->disk)\n+\t\t\tcontinue;\n+\n \t\tif (cd->present) {\n \t\t\tdel_gendisk(cd->disk);\n \t\t\tpi_release(cd->pi);",
        "function_modified_lines": {
            "added": [
                "\t\tif (!cd->disk)",
                "\t\t\tcontinue;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.9. There is a NULL pointer dereference for a cd data structure if alloc_disk fails in drivers/block/paride/pf.c.",
        "id": 2032
    },
    {
        "cve_id": "CVE-2014-0101",
        "code_before_change": "sctp_disposition_t sctp_sf_do_5_1D_ce(struct net *net,\n\t\t\t\t      const struct sctp_endpoint *ep,\n\t\t\t\t      const struct sctp_association *asoc,\n\t\t\t\t      const sctp_subtype_t type, void *arg,\n\t\t\t\t      sctp_cmd_seq_t *commands)\n{\n\tstruct sctp_chunk *chunk = arg;\n\tstruct sctp_association *new_asoc;\n\tsctp_init_chunk_t *peer_init;\n\tstruct sctp_chunk *repl;\n\tstruct sctp_ulpevent *ev, *ai_ev = NULL;\n\tint error = 0;\n\tstruct sctp_chunk *err_chk_p;\n\tstruct sock *sk;\n\n\t/* If the packet is an OOTB packet which is temporarily on the\n\t * control endpoint, respond with an ABORT.\n\t */\n\tif (ep == sctp_sk(net->sctp.ctl_sock)->ep) {\n\t\tSCTP_INC_STATS(net, SCTP_MIB_OUTOFBLUES);\n\t\treturn sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);\n\t}\n\n\t/* Make sure that the COOKIE_ECHO chunk has a valid length.\n\t * In this case, we check that we have enough for at least a\n\t * chunk header.  More detailed verification is done\n\t * in sctp_unpack_cookie().\n\t */\n\tif (!sctp_chunk_length_valid(chunk, sizeof(sctp_chunkhdr_t)))\n\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\n\t/* If the endpoint is not listening or if the number of associations\n\t * on the TCP-style socket exceed the max backlog, respond with an\n\t * ABORT.\n\t */\n\tsk = ep->base.sk;\n\tif (!sctp_sstate(sk, LISTENING) ||\n\t    (sctp_style(sk, TCP) && sk_acceptq_is_full(sk)))\n\t\treturn sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);\n\n\t/* \"Decode\" the chunk.  We have no optional parameters so we\n\t * are in good shape.\n\t */\n\tchunk->subh.cookie_hdr =\n\t\t(struct sctp_signed_cookie *)chunk->skb->data;\n\tif (!pskb_pull(chunk->skb, ntohs(chunk->chunk_hdr->length) -\n\t\t\t\t\t sizeof(sctp_chunkhdr_t)))\n\t\tgoto nomem;\n\n\t/* 5.1 D) Upon reception of the COOKIE ECHO chunk, Endpoint\n\t * \"Z\" will reply with a COOKIE ACK chunk after building a TCB\n\t * and moving to the ESTABLISHED state.\n\t */\n\tnew_asoc = sctp_unpack_cookie(ep, asoc, chunk, GFP_ATOMIC, &error,\n\t\t\t\t      &err_chk_p);\n\n\t/* FIXME:\n\t * If the re-build failed, what is the proper error path\n\t * from here?\n\t *\n\t * [We should abort the association. --piggy]\n\t */\n\tif (!new_asoc) {\n\t\t/* FIXME: Several errors are possible.  A bad cookie should\n\t\t * be silently discarded, but think about logging it too.\n\t\t */\n\t\tswitch (error) {\n\t\tcase -SCTP_IERROR_NOMEM:\n\t\t\tgoto nomem;\n\n\t\tcase -SCTP_IERROR_STALE_COOKIE:\n\t\t\tsctp_send_stale_cookie_err(net, ep, asoc, chunk, commands,\n\t\t\t\t\t\t   err_chk_p);\n\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\n\t\tcase -SCTP_IERROR_BAD_SIG:\n\t\tdefault:\n\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\t\t}\n\t}\n\n\n\t/* Delay state machine commands until later.\n\t *\n\t * Re-build the bind address for the association is done in\n\t * the sctp_unpack_cookie() already.\n\t */\n\t/* This is a brand-new association, so these are not yet side\n\t * effects--it is safe to run them here.\n\t */\n\tpeer_init = &chunk->subh.cookie_hdr->c.peer_init[0];\n\n\tif (!sctp_process_init(new_asoc, chunk,\n\t\t\t       &chunk->subh.cookie_hdr->c.peer_addr,\n\t\t\t       peer_init, GFP_ATOMIC))\n\t\tgoto nomem_init;\n\n\t/* SCTP-AUTH:  Now that we've populate required fields in\n\t * sctp_process_init, set up the assocaition shared keys as\n\t * necessary so that we can potentially authenticate the ACK\n\t */\n\terror = sctp_auth_asoc_init_active_key(new_asoc, GFP_ATOMIC);\n\tif (error)\n\t\tgoto nomem_init;\n\n\t/* SCTP-AUTH:  auth_chunk pointer is only set when the cookie-echo\n\t * is supposed to be authenticated and we have to do delayed\n\t * authentication.  We've just recreated the association using\n\t * the information in the cookie and now it's much easier to\n\t * do the authentication.\n\t */\n\tif (chunk->auth_chunk) {\n\t\tstruct sctp_chunk auth;\n\t\tsctp_ierror_t ret;\n\n\t\t/* set-up our fake chunk so that we can process it */\n\t\tauth.skb = chunk->auth_chunk;\n\t\tauth.asoc = chunk->asoc;\n\t\tauth.sctp_hdr = chunk->sctp_hdr;\n\t\tauth.chunk_hdr = (sctp_chunkhdr_t *)skb_push(chunk->auth_chunk,\n\t\t\t\t\t    sizeof(sctp_chunkhdr_t));\n\t\tskb_pull(chunk->auth_chunk, sizeof(sctp_chunkhdr_t));\n\t\tauth.transport = chunk->transport;\n\n\t\tret = sctp_sf_authenticate(net, ep, new_asoc, type, &auth);\n\n\t\t/* We can now safely free the auth_chunk clone */\n\t\tkfree_skb(chunk->auth_chunk);\n\n\t\tif (ret != SCTP_IERROR_NO_ERROR) {\n\t\t\tsctp_association_free(new_asoc);\n\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\t\t}\n\t}\n\n\trepl = sctp_make_cookie_ack(new_asoc, chunk);\n\tif (!repl)\n\t\tgoto nomem_init;\n\n\t/* RFC 2960 5.1 Normal Establishment of an Association\n\t *\n\t * D) IMPLEMENTATION NOTE: An implementation may choose to\n\t * send the Communication Up notification to the SCTP user\n\t * upon reception of a valid COOKIE ECHO chunk.\n\t */\n\tev = sctp_ulpevent_make_assoc_change(new_asoc, 0, SCTP_COMM_UP, 0,\n\t\t\t\t\t     new_asoc->c.sinit_num_ostreams,\n\t\t\t\t\t     new_asoc->c.sinit_max_instreams,\n\t\t\t\t\t     NULL, GFP_ATOMIC);\n\tif (!ev)\n\t\tgoto nomem_ev;\n\n\t/* Sockets API Draft Section 5.3.1.6\n\t * When a peer sends a Adaptation Layer Indication parameter , SCTP\n\t * delivers this notification to inform the application that of the\n\t * peers requested adaptation layer.\n\t */\n\tif (new_asoc->peer.adaptation_ind) {\n\t\tai_ev = sctp_ulpevent_make_adaptation_indication(new_asoc,\n\t\t\t\t\t\t\t    GFP_ATOMIC);\n\t\tif (!ai_ev)\n\t\t\tgoto nomem_aiev;\n\t}\n\n\t/* Add all the state machine commands now since we've created\n\t * everything.  This way we don't introduce memory corruptions\n\t * during side-effect processing and correclty count established\n\t * associations.\n\t */\n\tsctp_add_cmd_sf(commands, SCTP_CMD_NEW_ASOC, SCTP_ASOC(new_asoc));\n\tsctp_add_cmd_sf(commands, SCTP_CMD_NEW_STATE,\n\t\t\tSCTP_STATE(SCTP_STATE_ESTABLISHED));\n\tSCTP_INC_STATS(net, SCTP_MIB_CURRESTAB);\n\tSCTP_INC_STATS(net, SCTP_MIB_PASSIVEESTABS);\n\tsctp_add_cmd_sf(commands, SCTP_CMD_HB_TIMERS_START, SCTP_NULL());\n\n\tif (new_asoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE])\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_TIMER_START,\n\t\t\t\tSCTP_TO(SCTP_EVENT_TIMEOUT_AUTOCLOSE));\n\n\t/* This will send the COOKIE ACK */\n\tsctp_add_cmd_sf(commands, SCTP_CMD_REPLY, SCTP_CHUNK(repl));\n\n\t/* Queue the ASSOC_CHANGE event */\n\tsctp_add_cmd_sf(commands, SCTP_CMD_EVENT_ULP, SCTP_ULPEVENT(ev));\n\n\t/* Send up the Adaptation Layer Indication event */\n\tif (ai_ev)\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_EVENT_ULP,\n\t\t\t\tSCTP_ULPEVENT(ai_ev));\n\n\treturn SCTP_DISPOSITION_CONSUME;\n\nnomem_aiev:\n\tsctp_ulpevent_free(ev);\nnomem_ev:\n\tsctp_chunk_free(repl);\nnomem_init:\n\tsctp_association_free(new_asoc);\nnomem:\n\treturn SCTP_DISPOSITION_NOMEM;\n}",
        "code_after_change": "sctp_disposition_t sctp_sf_do_5_1D_ce(struct net *net,\n\t\t\t\t      const struct sctp_endpoint *ep,\n\t\t\t\t      const struct sctp_association *asoc,\n\t\t\t\t      const sctp_subtype_t type, void *arg,\n\t\t\t\t      sctp_cmd_seq_t *commands)\n{\n\tstruct sctp_chunk *chunk = arg;\n\tstruct sctp_association *new_asoc;\n\tsctp_init_chunk_t *peer_init;\n\tstruct sctp_chunk *repl;\n\tstruct sctp_ulpevent *ev, *ai_ev = NULL;\n\tint error = 0;\n\tstruct sctp_chunk *err_chk_p;\n\tstruct sock *sk;\n\n\t/* If the packet is an OOTB packet which is temporarily on the\n\t * control endpoint, respond with an ABORT.\n\t */\n\tif (ep == sctp_sk(net->sctp.ctl_sock)->ep) {\n\t\tSCTP_INC_STATS(net, SCTP_MIB_OUTOFBLUES);\n\t\treturn sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);\n\t}\n\n\t/* Make sure that the COOKIE_ECHO chunk has a valid length.\n\t * In this case, we check that we have enough for at least a\n\t * chunk header.  More detailed verification is done\n\t * in sctp_unpack_cookie().\n\t */\n\tif (!sctp_chunk_length_valid(chunk, sizeof(sctp_chunkhdr_t)))\n\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\n\t/* If the endpoint is not listening or if the number of associations\n\t * on the TCP-style socket exceed the max backlog, respond with an\n\t * ABORT.\n\t */\n\tsk = ep->base.sk;\n\tif (!sctp_sstate(sk, LISTENING) ||\n\t    (sctp_style(sk, TCP) && sk_acceptq_is_full(sk)))\n\t\treturn sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);\n\n\t/* \"Decode\" the chunk.  We have no optional parameters so we\n\t * are in good shape.\n\t */\n\tchunk->subh.cookie_hdr =\n\t\t(struct sctp_signed_cookie *)chunk->skb->data;\n\tif (!pskb_pull(chunk->skb, ntohs(chunk->chunk_hdr->length) -\n\t\t\t\t\t sizeof(sctp_chunkhdr_t)))\n\t\tgoto nomem;\n\n\t/* 5.1 D) Upon reception of the COOKIE ECHO chunk, Endpoint\n\t * \"Z\" will reply with a COOKIE ACK chunk after building a TCB\n\t * and moving to the ESTABLISHED state.\n\t */\n\tnew_asoc = sctp_unpack_cookie(ep, asoc, chunk, GFP_ATOMIC, &error,\n\t\t\t\t      &err_chk_p);\n\n\t/* FIXME:\n\t * If the re-build failed, what is the proper error path\n\t * from here?\n\t *\n\t * [We should abort the association. --piggy]\n\t */\n\tif (!new_asoc) {\n\t\t/* FIXME: Several errors are possible.  A bad cookie should\n\t\t * be silently discarded, but think about logging it too.\n\t\t */\n\t\tswitch (error) {\n\t\tcase -SCTP_IERROR_NOMEM:\n\t\t\tgoto nomem;\n\n\t\tcase -SCTP_IERROR_STALE_COOKIE:\n\t\t\tsctp_send_stale_cookie_err(net, ep, asoc, chunk, commands,\n\t\t\t\t\t\t   err_chk_p);\n\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\n\t\tcase -SCTP_IERROR_BAD_SIG:\n\t\tdefault:\n\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\t\t}\n\t}\n\n\n\t/* Delay state machine commands until later.\n\t *\n\t * Re-build the bind address for the association is done in\n\t * the sctp_unpack_cookie() already.\n\t */\n\t/* This is a brand-new association, so these are not yet side\n\t * effects--it is safe to run them here.\n\t */\n\tpeer_init = &chunk->subh.cookie_hdr->c.peer_init[0];\n\n\tif (!sctp_process_init(new_asoc, chunk,\n\t\t\t       &chunk->subh.cookie_hdr->c.peer_addr,\n\t\t\t       peer_init, GFP_ATOMIC))\n\t\tgoto nomem_init;\n\n\t/* SCTP-AUTH:  Now that we've populate required fields in\n\t * sctp_process_init, set up the assocaition shared keys as\n\t * necessary so that we can potentially authenticate the ACK\n\t */\n\terror = sctp_auth_asoc_init_active_key(new_asoc, GFP_ATOMIC);\n\tif (error)\n\t\tgoto nomem_init;\n\n\t/* SCTP-AUTH:  auth_chunk pointer is only set when the cookie-echo\n\t * is supposed to be authenticated and we have to do delayed\n\t * authentication.  We've just recreated the association using\n\t * the information in the cookie and now it's much easier to\n\t * do the authentication.\n\t */\n\tif (chunk->auth_chunk) {\n\t\tstruct sctp_chunk auth;\n\t\tsctp_ierror_t ret;\n\n\t\t/* Make sure that we and the peer are AUTH capable */\n\t\tif (!net->sctp.auth_enable || !new_asoc->peer.auth_capable) {\n\t\t\tkfree_skb(chunk->auth_chunk);\n\t\t\tsctp_association_free(new_asoc);\n\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\t\t}\n\n\t\t/* set-up our fake chunk so that we can process it */\n\t\tauth.skb = chunk->auth_chunk;\n\t\tauth.asoc = chunk->asoc;\n\t\tauth.sctp_hdr = chunk->sctp_hdr;\n\t\tauth.chunk_hdr = (sctp_chunkhdr_t *)skb_push(chunk->auth_chunk,\n\t\t\t\t\t    sizeof(sctp_chunkhdr_t));\n\t\tskb_pull(chunk->auth_chunk, sizeof(sctp_chunkhdr_t));\n\t\tauth.transport = chunk->transport;\n\n\t\tret = sctp_sf_authenticate(net, ep, new_asoc, type, &auth);\n\n\t\t/* We can now safely free the auth_chunk clone */\n\t\tkfree_skb(chunk->auth_chunk);\n\n\t\tif (ret != SCTP_IERROR_NO_ERROR) {\n\t\t\tsctp_association_free(new_asoc);\n\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n\t\t}\n\t}\n\n\trepl = sctp_make_cookie_ack(new_asoc, chunk);\n\tif (!repl)\n\t\tgoto nomem_init;\n\n\t/* RFC 2960 5.1 Normal Establishment of an Association\n\t *\n\t * D) IMPLEMENTATION NOTE: An implementation may choose to\n\t * send the Communication Up notification to the SCTP user\n\t * upon reception of a valid COOKIE ECHO chunk.\n\t */\n\tev = sctp_ulpevent_make_assoc_change(new_asoc, 0, SCTP_COMM_UP, 0,\n\t\t\t\t\t     new_asoc->c.sinit_num_ostreams,\n\t\t\t\t\t     new_asoc->c.sinit_max_instreams,\n\t\t\t\t\t     NULL, GFP_ATOMIC);\n\tif (!ev)\n\t\tgoto nomem_ev;\n\n\t/* Sockets API Draft Section 5.3.1.6\n\t * When a peer sends a Adaptation Layer Indication parameter , SCTP\n\t * delivers this notification to inform the application that of the\n\t * peers requested adaptation layer.\n\t */\n\tif (new_asoc->peer.adaptation_ind) {\n\t\tai_ev = sctp_ulpevent_make_adaptation_indication(new_asoc,\n\t\t\t\t\t\t\t    GFP_ATOMIC);\n\t\tif (!ai_ev)\n\t\t\tgoto nomem_aiev;\n\t}\n\n\t/* Add all the state machine commands now since we've created\n\t * everything.  This way we don't introduce memory corruptions\n\t * during side-effect processing and correclty count established\n\t * associations.\n\t */\n\tsctp_add_cmd_sf(commands, SCTP_CMD_NEW_ASOC, SCTP_ASOC(new_asoc));\n\tsctp_add_cmd_sf(commands, SCTP_CMD_NEW_STATE,\n\t\t\tSCTP_STATE(SCTP_STATE_ESTABLISHED));\n\tSCTP_INC_STATS(net, SCTP_MIB_CURRESTAB);\n\tSCTP_INC_STATS(net, SCTP_MIB_PASSIVEESTABS);\n\tsctp_add_cmd_sf(commands, SCTP_CMD_HB_TIMERS_START, SCTP_NULL());\n\n\tif (new_asoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE])\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_TIMER_START,\n\t\t\t\tSCTP_TO(SCTP_EVENT_TIMEOUT_AUTOCLOSE));\n\n\t/* This will send the COOKIE ACK */\n\tsctp_add_cmd_sf(commands, SCTP_CMD_REPLY, SCTP_CHUNK(repl));\n\n\t/* Queue the ASSOC_CHANGE event */\n\tsctp_add_cmd_sf(commands, SCTP_CMD_EVENT_ULP, SCTP_ULPEVENT(ev));\n\n\t/* Send up the Adaptation Layer Indication event */\n\tif (ai_ev)\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_EVENT_ULP,\n\t\t\t\tSCTP_ULPEVENT(ai_ev));\n\n\treturn SCTP_DISPOSITION_CONSUME;\n\nnomem_aiev:\n\tsctp_ulpevent_free(ev);\nnomem_ev:\n\tsctp_chunk_free(repl);\nnomem_init:\n\tsctp_association_free(new_asoc);\nnomem:\n\treturn SCTP_DISPOSITION_NOMEM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -113,6 +113,13 @@\n \t\tstruct sctp_chunk auth;\n \t\tsctp_ierror_t ret;\n \n+\t\t/* Make sure that we and the peer are AUTH capable */\n+\t\tif (!net->sctp.auth_enable || !new_asoc->peer.auth_capable) {\n+\t\t\tkfree_skb(chunk->auth_chunk);\n+\t\t\tsctp_association_free(new_asoc);\n+\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);\n+\t\t}\n+\n \t\t/* set-up our fake chunk so that we can process it */\n \t\tauth.skb = chunk->auth_chunk;\n \t\tauth.asoc = chunk->asoc;",
        "function_modified_lines": {
            "added": [
                "\t\t/* Make sure that we and the peer are AUTH capable */",
                "\t\tif (!net->sctp.auth_enable || !new_asoc->peer.auth_capable) {",
                "\t\t\tkfree_skb(chunk->auth_chunk);",
                "\t\t\tsctp_association_free(new_asoc);",
                "\t\t\treturn sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);",
                "\t\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The sctp_sf_do_5_1D_ce function in net/sctp/sm_statefuns.c in the Linux kernel through 3.13.6 does not validate certain auth_enable and auth_capable fields before making an sctp_sf_authenticate call, which allows remote attackers to cause a denial of service (NULL pointer dereference and system crash) via an SCTP handshake with a modified INIT chunk and a crafted AUTH chunk before a COOKIE_ECHO chunk.",
        "id": 429
    },
    {
        "cve_id": "CVE-2020-11609",
        "code_before_change": "static int stv06xx_start(struct gspca_dev *gspca_dev)\n{\n\tstruct sd *sd = (struct sd *) gspca_dev;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\tint err, packet_size;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\treturn -EIO;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\terr = stv06xx_write_bridge(sd, STV_ISO_SIZE_L, packet_size);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* Prepare the sensor for start */\n\terr = sd->sensor->start(sd);\n\tif (err < 0)\n\t\tgoto out;\n\n\t/* Start isochronous streaming */\n\terr = stv06xx_write_bridge(sd, STV_ISO_ENABLE, 1);\n\nout:\n\tif (err < 0)\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Starting stream failed\\n\");\n\telse\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Started streaming\\n\");\n\n\treturn (err < 0) ? err : 0;\n}",
        "code_after_change": "static int stv06xx_start(struct gspca_dev *gspca_dev)\n{\n\tstruct sd *sd = (struct sd *) gspca_dev;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\tint err, packet_size;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\treturn -EIO;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1)\n\t\treturn -ENODEV;\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\terr = stv06xx_write_bridge(sd, STV_ISO_SIZE_L, packet_size);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* Prepare the sensor for start */\n\terr = sd->sensor->start(sd);\n\tif (err < 0)\n\t\tgoto out;\n\n\t/* Start isochronous streaming */\n\terr = stv06xx_write_bridge(sd, STV_ISO_ENABLE, 1);\n\nout:\n\tif (err < 0)\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Starting stream failed\\n\");\n\telse\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Started streaming\\n\");\n\n\treturn (err < 0) ? err : 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,9 @@\n \t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n \t\treturn -EIO;\n \t}\n+\n+\tif (alt->desc.bNumEndpoints < 1)\n+\t\treturn -ENODEV;\n \n \tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n \terr = stv06xx_write_bridge(sd, STV_ISO_SIZE_L, packet_size);",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (alt->desc.bNumEndpoints < 1)",
                "\t\treturn -ENODEV;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the stv06xx subsystem in the Linux kernel before 5.6.1. drivers/media/usb/gspca/stv06xx/stv06xx.c and drivers/media/usb/gspca/stv06xx/stv06xx_pb0100.c mishandle invalid descriptors, as demonstrated by a NULL pointer dereference, aka CID-485b06aadb93.",
        "id": 2431
    },
    {
        "cve_id": "CVE-2018-10323",
        "code_before_change": "STATIC int\t\t\t\t\t/* error */\nxfs_bmap_extents_to_btree(\n\txfs_trans_t\t\t*tp,\t\t/* transaction pointer */\n\txfs_inode_t\t\t*ip,\t\t/* incore inode pointer */\n\txfs_fsblock_t\t\t*firstblock,\t/* first-block-allocated */\n\tstruct xfs_defer_ops\t*dfops,\t\t/* blocks freed in xaction */\n\txfs_btree_cur_t\t\t**curp,\t\t/* cursor returned to caller */\n\tint\t\t\twasdel,\t\t/* converting a delayed alloc */\n\tint\t\t\t*logflagsp,\t/* inode logging flags */\n\tint\t\t\twhichfork)\t/* data or attr fork */\n{\n\tstruct xfs_btree_block\t*ablock;\t/* allocated (child) bt block */\n\txfs_buf_t\t\t*abp;\t\t/* buffer for ablock */\n\txfs_alloc_arg_t\t\targs;\t\t/* allocation arguments */\n\txfs_bmbt_rec_t\t\t*arp;\t\t/* child record pointer */\n\tstruct xfs_btree_block\t*block;\t\t/* btree root block */\n\txfs_btree_cur_t\t\t*cur;\t\t/* bmap btree cursor */\n\tint\t\t\terror;\t\t/* error return value */\n\txfs_ifork_t\t\t*ifp;\t\t/* inode fork pointer */\n\txfs_bmbt_key_t\t\t*kp;\t\t/* root block key pointer */\n\txfs_mount_t\t\t*mp;\t\t/* mount structure */\n\txfs_bmbt_ptr_t\t\t*pp;\t\t/* root block address pointer */\n\tstruct xfs_iext_cursor\ticur;\n\tstruct xfs_bmbt_irec\trec;\n\txfs_extnum_t\t\tcnt = 0;\n\n\tmp = ip->i_mount;\n\tASSERT(whichfork != XFS_COW_FORK);\n\tifp = XFS_IFORK_PTR(ip, whichfork);\n\tASSERT(XFS_IFORK_FORMAT(ip, whichfork) == XFS_DINODE_FMT_EXTENTS);\n\n\t/*\n\t * Make space in the inode incore.\n\t */\n\txfs_iroot_realloc(ip, 1, whichfork);\n\tifp->if_flags |= XFS_IFBROOT;\n\n\t/*\n\t * Fill in the root.\n\t */\n\tblock = ifp->if_broot;\n\txfs_btree_init_block_int(mp, block, XFS_BUF_DADDR_NULL,\n\t\t\t\t XFS_BTNUM_BMAP, 1, 1, ip->i_ino,\n\t\t\t\t XFS_BTREE_LONG_PTRS);\n\t/*\n\t * Need a cursor.  Can't allocate until bb_level is filled in.\n\t */\n\tcur = xfs_bmbt_init_cursor(mp, tp, ip, whichfork);\n\tcur->bc_private.b.firstblock = *firstblock;\n\tcur->bc_private.b.dfops = dfops;\n\tcur->bc_private.b.flags = wasdel ? XFS_BTCUR_BPRV_WASDEL : 0;\n\t/*\n\t * Convert to a btree with two levels, one record in root.\n\t */\n\tXFS_IFORK_FMT_SET(ip, whichfork, XFS_DINODE_FMT_BTREE);\n\tmemset(&args, 0, sizeof(args));\n\targs.tp = tp;\n\targs.mp = mp;\n\txfs_rmap_ino_bmbt_owner(&args.oinfo, ip->i_ino, whichfork);\n\targs.firstblock = *firstblock;\n\tif (*firstblock == NULLFSBLOCK) {\n\t\targs.type = XFS_ALLOCTYPE_START_BNO;\n\t\targs.fsbno = XFS_INO_TO_FSB(mp, ip->i_ino);\n\t} else if (dfops->dop_low) {\n\t\targs.type = XFS_ALLOCTYPE_START_BNO;\n\t\targs.fsbno = *firstblock;\n\t} else {\n\t\targs.type = XFS_ALLOCTYPE_NEAR_BNO;\n\t\targs.fsbno = *firstblock;\n\t}\n\targs.minlen = args.maxlen = args.prod = 1;\n\targs.wasdel = wasdel;\n\t*logflagsp = 0;\n\tif ((error = xfs_alloc_vextent(&args))) {\n\t\txfs_iroot_realloc(ip, -1, whichfork);\n\t\txfs_btree_del_cursor(cur, XFS_BTREE_ERROR);\n\t\treturn error;\n\t}\n\n\tif (WARN_ON_ONCE(args.fsbno == NULLFSBLOCK)) {\n\t\txfs_iroot_realloc(ip, -1, whichfork);\n\t\txfs_btree_del_cursor(cur, XFS_BTREE_ERROR);\n\t\treturn -ENOSPC;\n\t}\n\t/*\n\t * Allocation can't fail, the space was reserved.\n\t */\n\tASSERT(*firstblock == NULLFSBLOCK ||\n\t       args.agno >= XFS_FSB_TO_AGNO(mp, *firstblock));\n\t*firstblock = cur->bc_private.b.firstblock = args.fsbno;\n\tcur->bc_private.b.allocated++;\n\tip->i_d.di_nblocks++;\n\txfs_trans_mod_dquot_byino(tp, ip, XFS_TRANS_DQ_BCOUNT, 1L);\n\tabp = xfs_btree_get_bufl(mp, tp, args.fsbno, 0);\n\t/*\n\t * Fill in the child block.\n\t */\n\tabp->b_ops = &xfs_bmbt_buf_ops;\n\tablock = XFS_BUF_TO_BLOCK(abp);\n\txfs_btree_init_block_int(mp, ablock, abp->b_bn,\n\t\t\t\tXFS_BTNUM_BMAP, 0, 0, ip->i_ino,\n\t\t\t\tXFS_BTREE_LONG_PTRS);\n\n\tfor_each_xfs_iext(ifp, &icur, &rec) {\n\t\tif (isnullstartblock(rec.br_startblock))\n\t\t\tcontinue;\n\t\tarp = XFS_BMBT_REC_ADDR(mp, ablock, 1 + cnt);\n\t\txfs_bmbt_disk_set_all(arp, &rec);\n\t\tcnt++;\n\t}\n\tASSERT(cnt == XFS_IFORK_NEXTENTS(ip, whichfork));\n\txfs_btree_set_numrecs(ablock, cnt);\n\n\t/*\n\t * Fill in the root key and pointer.\n\t */\n\tkp = XFS_BMBT_KEY_ADDR(mp, block, 1);\n\tarp = XFS_BMBT_REC_ADDR(mp, ablock, 1);\n\tkp->br_startoff = cpu_to_be64(xfs_bmbt_disk_get_startoff(arp));\n\tpp = XFS_BMBT_PTR_ADDR(mp, block, 1, xfs_bmbt_get_maxrecs(cur,\n\t\t\t\t\t\tbe16_to_cpu(block->bb_level)));\n\t*pp = cpu_to_be64(args.fsbno);\n\n\t/*\n\t * Do all this logging at the end so that\n\t * the root is at the right level.\n\t */\n\txfs_btree_log_block(cur, abp, XFS_BB_ALL_BITS);\n\txfs_btree_log_recs(cur, abp, 1, be16_to_cpu(ablock->bb_numrecs));\n\tASSERT(*curp == NULL);\n\t*curp = cur;\n\t*logflagsp = XFS_ILOG_CORE | xfs_ilog_fbroot(whichfork);\n\treturn 0;\n}",
        "code_after_change": "STATIC int\t\t\t\t\t/* error */\nxfs_bmap_extents_to_btree(\n\txfs_trans_t\t\t*tp,\t\t/* transaction pointer */\n\txfs_inode_t\t\t*ip,\t\t/* incore inode pointer */\n\txfs_fsblock_t\t\t*firstblock,\t/* first-block-allocated */\n\tstruct xfs_defer_ops\t*dfops,\t\t/* blocks freed in xaction */\n\txfs_btree_cur_t\t\t**curp,\t\t/* cursor returned to caller */\n\tint\t\t\twasdel,\t\t/* converting a delayed alloc */\n\tint\t\t\t*logflagsp,\t/* inode logging flags */\n\tint\t\t\twhichfork)\t/* data or attr fork */\n{\n\tstruct xfs_btree_block\t*ablock;\t/* allocated (child) bt block */\n\txfs_buf_t\t\t*abp;\t\t/* buffer for ablock */\n\txfs_alloc_arg_t\t\targs;\t\t/* allocation arguments */\n\txfs_bmbt_rec_t\t\t*arp;\t\t/* child record pointer */\n\tstruct xfs_btree_block\t*block;\t\t/* btree root block */\n\txfs_btree_cur_t\t\t*cur;\t\t/* bmap btree cursor */\n\tint\t\t\terror;\t\t/* error return value */\n\txfs_ifork_t\t\t*ifp;\t\t/* inode fork pointer */\n\txfs_bmbt_key_t\t\t*kp;\t\t/* root block key pointer */\n\txfs_mount_t\t\t*mp;\t\t/* mount structure */\n\txfs_bmbt_ptr_t\t\t*pp;\t\t/* root block address pointer */\n\tstruct xfs_iext_cursor\ticur;\n\tstruct xfs_bmbt_irec\trec;\n\txfs_extnum_t\t\tcnt = 0;\n\n\tmp = ip->i_mount;\n\tASSERT(whichfork != XFS_COW_FORK);\n\tifp = XFS_IFORK_PTR(ip, whichfork);\n\tASSERT(XFS_IFORK_FORMAT(ip, whichfork) == XFS_DINODE_FMT_EXTENTS);\n\n\t/*\n\t * Make space in the inode incore.\n\t */\n\txfs_iroot_realloc(ip, 1, whichfork);\n\tifp->if_flags |= XFS_IFBROOT;\n\n\t/*\n\t * Fill in the root.\n\t */\n\tblock = ifp->if_broot;\n\txfs_btree_init_block_int(mp, block, XFS_BUF_DADDR_NULL,\n\t\t\t\t XFS_BTNUM_BMAP, 1, 1, ip->i_ino,\n\t\t\t\t XFS_BTREE_LONG_PTRS);\n\t/*\n\t * Need a cursor.  Can't allocate until bb_level is filled in.\n\t */\n\tcur = xfs_bmbt_init_cursor(mp, tp, ip, whichfork);\n\tcur->bc_private.b.firstblock = *firstblock;\n\tcur->bc_private.b.dfops = dfops;\n\tcur->bc_private.b.flags = wasdel ? XFS_BTCUR_BPRV_WASDEL : 0;\n\t/*\n\t * Convert to a btree with two levels, one record in root.\n\t */\n\tXFS_IFORK_FMT_SET(ip, whichfork, XFS_DINODE_FMT_BTREE);\n\tmemset(&args, 0, sizeof(args));\n\targs.tp = tp;\n\targs.mp = mp;\n\txfs_rmap_ino_bmbt_owner(&args.oinfo, ip->i_ino, whichfork);\n\targs.firstblock = *firstblock;\n\tif (*firstblock == NULLFSBLOCK) {\n\t\targs.type = XFS_ALLOCTYPE_START_BNO;\n\t\targs.fsbno = XFS_INO_TO_FSB(mp, ip->i_ino);\n\t} else if (dfops->dop_low) {\n\t\targs.type = XFS_ALLOCTYPE_START_BNO;\n\t\targs.fsbno = *firstblock;\n\t} else {\n\t\targs.type = XFS_ALLOCTYPE_NEAR_BNO;\n\t\targs.fsbno = *firstblock;\n\t}\n\targs.minlen = args.maxlen = args.prod = 1;\n\targs.wasdel = wasdel;\n\t*logflagsp = 0;\n\tif ((error = xfs_alloc_vextent(&args))) {\n\t\txfs_iroot_realloc(ip, -1, whichfork);\n\t\tASSERT(ifp->if_broot == NULL);\n\t\tXFS_IFORK_FMT_SET(ip, whichfork, XFS_DINODE_FMT_EXTENTS);\n\t\txfs_btree_del_cursor(cur, XFS_BTREE_ERROR);\n\t\treturn error;\n\t}\n\n\tif (WARN_ON_ONCE(args.fsbno == NULLFSBLOCK)) {\n\t\txfs_iroot_realloc(ip, -1, whichfork);\n\t\tASSERT(ifp->if_broot == NULL);\n\t\tXFS_IFORK_FMT_SET(ip, whichfork, XFS_DINODE_FMT_EXTENTS);\n\t\txfs_btree_del_cursor(cur, XFS_BTREE_ERROR);\n\t\treturn -ENOSPC;\n\t}\n\t/*\n\t * Allocation can't fail, the space was reserved.\n\t */\n\tASSERT(*firstblock == NULLFSBLOCK ||\n\t       args.agno >= XFS_FSB_TO_AGNO(mp, *firstblock));\n\t*firstblock = cur->bc_private.b.firstblock = args.fsbno;\n\tcur->bc_private.b.allocated++;\n\tip->i_d.di_nblocks++;\n\txfs_trans_mod_dquot_byino(tp, ip, XFS_TRANS_DQ_BCOUNT, 1L);\n\tabp = xfs_btree_get_bufl(mp, tp, args.fsbno, 0);\n\t/*\n\t * Fill in the child block.\n\t */\n\tabp->b_ops = &xfs_bmbt_buf_ops;\n\tablock = XFS_BUF_TO_BLOCK(abp);\n\txfs_btree_init_block_int(mp, ablock, abp->b_bn,\n\t\t\t\tXFS_BTNUM_BMAP, 0, 0, ip->i_ino,\n\t\t\t\tXFS_BTREE_LONG_PTRS);\n\n\tfor_each_xfs_iext(ifp, &icur, &rec) {\n\t\tif (isnullstartblock(rec.br_startblock))\n\t\t\tcontinue;\n\t\tarp = XFS_BMBT_REC_ADDR(mp, ablock, 1 + cnt);\n\t\txfs_bmbt_disk_set_all(arp, &rec);\n\t\tcnt++;\n\t}\n\tASSERT(cnt == XFS_IFORK_NEXTENTS(ip, whichfork));\n\txfs_btree_set_numrecs(ablock, cnt);\n\n\t/*\n\t * Fill in the root key and pointer.\n\t */\n\tkp = XFS_BMBT_KEY_ADDR(mp, block, 1);\n\tarp = XFS_BMBT_REC_ADDR(mp, ablock, 1);\n\tkp->br_startoff = cpu_to_be64(xfs_bmbt_disk_get_startoff(arp));\n\tpp = XFS_BMBT_PTR_ADDR(mp, block, 1, xfs_bmbt_get_maxrecs(cur,\n\t\t\t\t\t\tbe16_to_cpu(block->bb_level)));\n\t*pp = cpu_to_be64(args.fsbno);\n\n\t/*\n\t * Do all this logging at the end so that\n\t * the root is at the right level.\n\t */\n\txfs_btree_log_block(cur, abp, XFS_BB_ALL_BITS);\n\txfs_btree_log_recs(cur, abp, 1, be16_to_cpu(ablock->bb_numrecs));\n\tASSERT(*curp == NULL);\n\t*curp = cur;\n\t*logflagsp = XFS_ILOG_CORE | xfs_ilog_fbroot(whichfork);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -73,12 +73,16 @@\n \t*logflagsp = 0;\n \tif ((error = xfs_alloc_vextent(&args))) {\n \t\txfs_iroot_realloc(ip, -1, whichfork);\n+\t\tASSERT(ifp->if_broot == NULL);\n+\t\tXFS_IFORK_FMT_SET(ip, whichfork, XFS_DINODE_FMT_EXTENTS);\n \t\txfs_btree_del_cursor(cur, XFS_BTREE_ERROR);\n \t\treturn error;\n \t}\n \n \tif (WARN_ON_ONCE(args.fsbno == NULLFSBLOCK)) {\n \t\txfs_iroot_realloc(ip, -1, whichfork);\n+\t\tASSERT(ifp->if_broot == NULL);\n+\t\tXFS_IFORK_FMT_SET(ip, whichfork, XFS_DINODE_FMT_EXTENTS);\n \t\txfs_btree_del_cursor(cur, XFS_BTREE_ERROR);\n \t\treturn -ENOSPC;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tASSERT(ifp->if_broot == NULL);",
                "\t\tXFS_IFORK_FMT_SET(ip, whichfork, XFS_DINODE_FMT_EXTENTS);",
                "\t\tASSERT(ifp->if_broot == NULL);",
                "\t\tXFS_IFORK_FMT_SET(ip, whichfork, XFS_DINODE_FMT_EXTENTS);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The xfs_bmap_extents_to_btree function in fs/xfs/libxfs/xfs_bmap.c in the Linux kernel through 4.16.3 allows local users to cause a denial of service (xfs_bmapi_write NULL pointer dereference) via a crafted xfs image.",
        "id": 1588
    },
    {
        "cve_id": "CVE-2019-18680",
        "code_before_change": "static void rds_tcp_kill_sock(struct net *net)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\tLIST_HEAD(tmp_list);\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\tstruct socket *lsock = rtn->rds_tcp_listen_sock;\n\n\trtn->rds_tcp_listen_sock = NULL;\n\trds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n\t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n\n\t\tif (net != c_net || !tc->t_sock)\n\t\t\tcontinue;\n\t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n\t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);\n\t\t} else {\n\t\t\tlist_del(&tc->t_tcp_node);\n\t\t\ttc->t_tcp_node_detached = true;\n\t\t}\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)\n\t\trds_conn_destroy(tc->t_cpath->cp_conn);\n}",
        "code_after_change": "static void rds_tcp_kill_sock(struct net *net)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\tLIST_HEAD(tmp_list);\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\tstruct socket *lsock = rtn->rds_tcp_listen_sock;\n\n\trtn->rds_tcp_listen_sock = NULL;\n\trds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n\t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n\n\t\tif (net != c_net)\n\t\t\tcontinue;\n\t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n\t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);\n\t\t} else {\n\t\t\tlist_del(&tc->t_tcp_node);\n\t\t\ttc->t_tcp_node_detached = true;\n\t\t}\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)\n\t\trds_conn_destroy(tc->t_cpath->cp_conn);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n \t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n \n-\t\tif (net != c_net || !tc->t_sock)\n+\t\tif (net != c_net)\n \t\t\tcontinue;\n \t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n \t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);",
        "function_modified_lines": {
            "added": [
                "\t\tif (net != c_net)"
            ],
            "deleted": [
                "\t\tif (net != c_net || !tc->t_sock)"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 4.4.x before 4.4.195. There is a NULL pointer dereference in rds_tcp_kill_sock() in net/rds/tcp.c that will cause denial of service, aka CID-91573ae4aed0.",
        "id": 2090
    },
    {
        "cve_id": "CVE-2019-15098",
        "code_before_change": "static void ath6kl_usb_free_urb_to_pipe(struct ath6kl_usb_pipe *pipe,\n\t\t\t\t\tstruct ath6kl_urb_context *urb_context)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tpipe->urb_cnt++;\n\n\tlist_add(&urb_context->link, &pipe->urb_list_head);\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n}",
        "code_after_change": "static void ath6kl_usb_free_urb_to_pipe(struct ath6kl_usb_pipe *pipe,\n\t\t\t\t\tstruct ath6kl_urb_context *urb_context)\n{\n\tunsigned long flags;\n\n\t/* bail if this pipe is not initialized */\n\tif (!pipe->ar_usb)\n\t\treturn;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tpipe->urb_cnt++;\n\n\tlist_add(&urb_context->link, &pipe->urb_list_head);\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,10 @@\n \t\t\t\t\tstruct ath6kl_urb_context *urb_context)\n {\n \tunsigned long flags;\n+\n+\t/* bail if this pipe is not initialized */\n+\tif (!pipe->ar_usb)\n+\t\treturn;\n \n \tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n \tpipe->urb_cnt++;",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* bail if this pipe is not initialized */",
                "\tif (!pipe->ar_usb)",
                "\t\treturn;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "drivers/net/wireless/ath/ath6kl/usb.c in the Linux kernel through 5.2.9 has a NULL pointer dereference via an incomplete address in an endpoint descriptor.",
        "id": 1987
    },
    {
        "cve_id": "CVE-2019-15098",
        "code_before_change": "static struct ath6kl_urb_context *\nath6kl_usb_alloc_urb_from_pipe(struct ath6kl_usb_pipe *pipe)\n{\n\tstruct ath6kl_urb_context *urb_context = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tif (!list_empty(&pipe->urb_list_head)) {\n\t\turb_context =\n\t\t    list_first_entry(&pipe->urb_list_head,\n\t\t\t\t     struct ath6kl_urb_context, link);\n\t\tlist_del(&urb_context->link);\n\t\tpipe->urb_cnt--;\n\t}\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n\n\treturn urb_context;\n}",
        "code_after_change": "static struct ath6kl_urb_context *\nath6kl_usb_alloc_urb_from_pipe(struct ath6kl_usb_pipe *pipe)\n{\n\tstruct ath6kl_urb_context *urb_context = NULL;\n\tunsigned long flags;\n\n\t/* bail if this pipe is not initialized */\n\tif (!pipe->ar_usb)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tif (!list_empty(&pipe->urb_list_head)) {\n\t\turb_context =\n\t\t    list_first_entry(&pipe->urb_list_head,\n\t\t\t\t     struct ath6kl_urb_context, link);\n\t\tlist_del(&urb_context->link);\n\t\tpipe->urb_cnt--;\n\t}\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n\n\treturn urb_context;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,10 @@\n {\n \tstruct ath6kl_urb_context *urb_context = NULL;\n \tunsigned long flags;\n+\n+\t/* bail if this pipe is not initialized */\n+\tif (!pipe->ar_usb)\n+\t\treturn NULL;\n \n \tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n \tif (!list_empty(&pipe->urb_list_head)) {",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* bail if this pipe is not initialized */",
                "\tif (!pipe->ar_usb)",
                "\t\treturn NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "drivers/net/wireless/ath/ath6kl/usb.c in the Linux kernel through 5.2.9 has a NULL pointer dereference via an incomplete address in an endpoint descriptor.",
        "id": 1988
    },
    {
        "cve_id": "CVE-2022-3107",
        "code_before_change": "static void netvsc_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t     struct ethtool_stats *stats, u64 *data)\n{\n\tstruct net_device_context *ndc = netdev_priv(dev);\n\tstruct netvsc_device *nvdev = rtnl_dereference(ndc->nvdev);\n\tconst void *nds = &ndc->eth_stats;\n\tconst struct netvsc_stats *qstats;\n\tstruct netvsc_vf_pcpu_stats sum;\n\tstruct netvsc_ethtool_pcpu_stats *pcpu_sum;\n\tunsigned int start;\n\tu64 packets, bytes;\n\tu64 xdp_drop;\n\tint i, j, cpu;\n\n\tif (!nvdev)\n\t\treturn;\n\n\tfor (i = 0; i < NETVSC_GLOBAL_STATS_LEN; i++)\n\t\tdata[i] = *(unsigned long *)(nds + netvsc_stats[i].offset);\n\n\tnetvsc_get_vf_stats(dev, &sum);\n\tfor (j = 0; j < NETVSC_VF_STATS_LEN; j++)\n\t\tdata[i++] = *(u64 *)((void *)&sum + vf_stats[j].offset);\n\n\tfor (j = 0; j < nvdev->num_chn; j++) {\n\t\tqstats = &nvdev->chan_table[j].tx_stats;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&qstats->syncp);\n\t\t\tpackets = qstats->packets;\n\t\t\tbytes = qstats->bytes;\n\t\t} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));\n\t\tdata[i++] = packets;\n\t\tdata[i++] = bytes;\n\n\t\tqstats = &nvdev->chan_table[j].rx_stats;\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&qstats->syncp);\n\t\t\tpackets = qstats->packets;\n\t\t\tbytes = qstats->bytes;\n\t\t\txdp_drop = qstats->xdp_drop;\n\t\t} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));\n\t\tdata[i++] = packets;\n\t\tdata[i++] = bytes;\n\t\tdata[i++] = xdp_drop;\n\t}\n\n\tpcpu_sum = kvmalloc_array(num_possible_cpus(),\n\t\t\t\t  sizeof(struct netvsc_ethtool_pcpu_stats),\n\t\t\t\t  GFP_KERNEL);\n\tnetvsc_get_pcpu_stats(dev, pcpu_sum);\n\tfor_each_present_cpu(cpu) {\n\t\tstruct netvsc_ethtool_pcpu_stats *this_sum = &pcpu_sum[cpu];\n\n\t\tfor (j = 0; j < ARRAY_SIZE(pcpu_stats); j++)\n\t\t\tdata[i++] = *(u64 *)((void *)this_sum\n\t\t\t\t\t     + pcpu_stats[j].offset);\n\t}\n\tkvfree(pcpu_sum);\n}",
        "code_after_change": "static void netvsc_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t     struct ethtool_stats *stats, u64 *data)\n{\n\tstruct net_device_context *ndc = netdev_priv(dev);\n\tstruct netvsc_device *nvdev = rtnl_dereference(ndc->nvdev);\n\tconst void *nds = &ndc->eth_stats;\n\tconst struct netvsc_stats *qstats;\n\tstruct netvsc_vf_pcpu_stats sum;\n\tstruct netvsc_ethtool_pcpu_stats *pcpu_sum;\n\tunsigned int start;\n\tu64 packets, bytes;\n\tu64 xdp_drop;\n\tint i, j, cpu;\n\n\tif (!nvdev)\n\t\treturn;\n\n\tfor (i = 0; i < NETVSC_GLOBAL_STATS_LEN; i++)\n\t\tdata[i] = *(unsigned long *)(nds + netvsc_stats[i].offset);\n\n\tnetvsc_get_vf_stats(dev, &sum);\n\tfor (j = 0; j < NETVSC_VF_STATS_LEN; j++)\n\t\tdata[i++] = *(u64 *)((void *)&sum + vf_stats[j].offset);\n\n\tfor (j = 0; j < nvdev->num_chn; j++) {\n\t\tqstats = &nvdev->chan_table[j].tx_stats;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&qstats->syncp);\n\t\t\tpackets = qstats->packets;\n\t\t\tbytes = qstats->bytes;\n\t\t} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));\n\t\tdata[i++] = packets;\n\t\tdata[i++] = bytes;\n\n\t\tqstats = &nvdev->chan_table[j].rx_stats;\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&qstats->syncp);\n\t\t\tpackets = qstats->packets;\n\t\t\tbytes = qstats->bytes;\n\t\t\txdp_drop = qstats->xdp_drop;\n\t\t} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));\n\t\tdata[i++] = packets;\n\t\tdata[i++] = bytes;\n\t\tdata[i++] = xdp_drop;\n\t}\n\n\tpcpu_sum = kvmalloc_array(num_possible_cpus(),\n\t\t\t\t  sizeof(struct netvsc_ethtool_pcpu_stats),\n\t\t\t\t  GFP_KERNEL);\n\tif (!pcpu_sum)\n\t\treturn;\n\n\tnetvsc_get_pcpu_stats(dev, pcpu_sum);\n\tfor_each_present_cpu(cpu) {\n\t\tstruct netvsc_ethtool_pcpu_stats *this_sum = &pcpu_sum[cpu];\n\n\t\tfor (j = 0; j < ARRAY_SIZE(pcpu_stats); j++)\n\t\t\tdata[i++] = *(u64 *)((void *)this_sum\n\t\t\t\t\t     + pcpu_stats[j].offset);\n\t}\n\tkvfree(pcpu_sum);\n}",
        "patch": "--- code before\n+++ code after\n@@ -48,6 +48,9 @@\n \tpcpu_sum = kvmalloc_array(num_possible_cpus(),\n \t\t\t\t  sizeof(struct netvsc_ethtool_pcpu_stats),\n \t\t\t\t  GFP_KERNEL);\n+\tif (!pcpu_sum)\n+\t\treturn;\n+\n \tnetvsc_get_pcpu_stats(dev, pcpu_sum);\n \tfor_each_present_cpu(cpu) {\n \t\tstruct netvsc_ethtool_pcpu_stats *this_sum = &pcpu_sum[cpu];",
        "function_modified_lines": {
            "added": [
                "\tif (!pcpu_sum)",
                "\t\treturn;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.16-rc6. netvsc_get_ethtool_stats in drivers/net/hyperv/netvsc_drv.c lacks check of the return value of kvmalloc_array() and will cause the null pointer dereference.",
        "id": 3552
    },
    {
        "cve_id": "CVE-2019-16232",
        "code_before_change": "static int if_sdio_probe(struct sdio_func *func,\n\t\tconst struct sdio_device_id *id)\n{\n\tstruct if_sdio_card *card;\n\tstruct lbs_private *priv;\n\tint ret, i;\n\tunsigned int model;\n\tstruct if_sdio_packet *packet;\n\n\tfor (i = 0;i < func->card->num_info;i++) {\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"802.11 SDIO ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (!strcmp(func->card->info[i], \"IBIS Wireless SDIO Card\")) {\n\t\t\tmodel = MODEL_8385;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == func->card->num_info) {\n\t\tpr_err(\"unable to identify card model\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tcard = kzalloc(sizeof(struct if_sdio_card), GFP_KERNEL);\n\tif (!card)\n\t\treturn -ENOMEM;\n\n\tcard->func = func;\n\tcard->model = model;\n\n\tswitch (card->model) {\n\tcase MODEL_8385:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH_OLD;\n\t\tbreak;\n\tcase MODEL_8686:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH;\n\t\tbreak;\n\tcase MODEL_8688:\n\tdefault: /* for newer chipsets */\n\t\tcard->scratch_reg = IF_SDIO_FW_STATUS;\n\t\tbreak;\n\t}\n\n\tspin_lock_init(&card->lock);\n\tcard->workqueue = alloc_workqueue(\"libertas_sdio\", WQ_MEM_RECLAIM, 0);\n\tINIT_WORK(&card->packet_worker, if_sdio_host_to_card_worker);\n\tinit_waitqueue_head(&card->pwron_waitq);\n\n\t/* Check if we support this card */\n\tfor (i = 0; i < ARRAY_SIZE(fw_table); i++) {\n\t\tif (card->model == fw_table[i].model)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(fw_table)) {\n\t\tpr_err(\"unknown card model 0x%x\\n\", card->model);\n\t\tret = -ENODEV;\n\t\tgoto free;\n\t}\n\n\tsdio_set_drvdata(func, card);\n\n\tlbs_deb_sdio(\"class = 0x%X, vendor = 0x%X, \"\n\t\t\t\"device = 0x%X, model = 0x%X, ioport = 0x%X\\n\",\n\t\t\tfunc->class, func->vendor, func->device,\n\t\t\tmodel, (unsigned)card->ioport);\n\n\n\tpriv = lbs_add_card(card, &func->dev);\n\tif (IS_ERR(priv)) {\n\t\tret = PTR_ERR(priv);\n\t\tgoto free;\n\t}\n\n\tcard->priv = priv;\n\n\tpriv->card = card;\n\tpriv->hw_host_to_card = if_sdio_host_to_card;\n\tpriv->enter_deep_sleep = if_sdio_enter_deep_sleep;\n\tpriv->exit_deep_sleep = if_sdio_exit_deep_sleep;\n\tpriv->reset_deep_sleep_wakeup = if_sdio_reset_deep_sleep_wakeup;\n\tpriv->reset_card = if_sdio_reset_card;\n\tpriv->power_save = if_sdio_power_save;\n\tpriv->power_restore = if_sdio_power_restore;\n\tpriv->is_polling = !(func->card->host->caps & MMC_CAP_SDIO_IRQ);\n\tret = if_sdio_power_on(card);\n\tif (ret)\n\t\tgoto err_activate_card;\n\nout:\n\treturn ret;\n\nerr_activate_card:\n\tflush_workqueue(card->workqueue);\n\tlbs_remove_card(priv);\nfree:\n\tdestroy_workqueue(card->workqueue);\n\twhile (card->packets) {\n\t\tpacket = card->packets;\n\t\tcard->packets = card->packets->next;\n\t\tkfree(packet);\n\t}\n\n\tkfree(card);\n\n\tgoto out;\n}",
        "code_after_change": "static int if_sdio_probe(struct sdio_func *func,\n\t\tconst struct sdio_device_id *id)\n{\n\tstruct if_sdio_card *card;\n\tstruct lbs_private *priv;\n\tint ret, i;\n\tunsigned int model;\n\tstruct if_sdio_packet *packet;\n\n\tfor (i = 0;i < func->card->num_info;i++) {\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"802.11 SDIO ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (!strcmp(func->card->info[i], \"IBIS Wireless SDIO Card\")) {\n\t\t\tmodel = MODEL_8385;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == func->card->num_info) {\n\t\tpr_err(\"unable to identify card model\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tcard = kzalloc(sizeof(struct if_sdio_card), GFP_KERNEL);\n\tif (!card)\n\t\treturn -ENOMEM;\n\n\tcard->func = func;\n\tcard->model = model;\n\n\tswitch (card->model) {\n\tcase MODEL_8385:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH_OLD;\n\t\tbreak;\n\tcase MODEL_8686:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH;\n\t\tbreak;\n\tcase MODEL_8688:\n\tdefault: /* for newer chipsets */\n\t\tcard->scratch_reg = IF_SDIO_FW_STATUS;\n\t\tbreak;\n\t}\n\n\tspin_lock_init(&card->lock);\n\tcard->workqueue = alloc_workqueue(\"libertas_sdio\", WQ_MEM_RECLAIM, 0);\n\tif (unlikely(!card->workqueue)) {\n\t\tret = -ENOMEM;\n\t\tgoto err_queue;\n\t}\n\tINIT_WORK(&card->packet_worker, if_sdio_host_to_card_worker);\n\tinit_waitqueue_head(&card->pwron_waitq);\n\n\t/* Check if we support this card */\n\tfor (i = 0; i < ARRAY_SIZE(fw_table); i++) {\n\t\tif (card->model == fw_table[i].model)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(fw_table)) {\n\t\tpr_err(\"unknown card model 0x%x\\n\", card->model);\n\t\tret = -ENODEV;\n\t\tgoto free;\n\t}\n\n\tsdio_set_drvdata(func, card);\n\n\tlbs_deb_sdio(\"class = 0x%X, vendor = 0x%X, \"\n\t\t\t\"device = 0x%X, model = 0x%X, ioport = 0x%X\\n\",\n\t\t\tfunc->class, func->vendor, func->device,\n\t\t\tmodel, (unsigned)card->ioport);\n\n\n\tpriv = lbs_add_card(card, &func->dev);\n\tif (IS_ERR(priv)) {\n\t\tret = PTR_ERR(priv);\n\t\tgoto free;\n\t}\n\n\tcard->priv = priv;\n\n\tpriv->card = card;\n\tpriv->hw_host_to_card = if_sdio_host_to_card;\n\tpriv->enter_deep_sleep = if_sdio_enter_deep_sleep;\n\tpriv->exit_deep_sleep = if_sdio_exit_deep_sleep;\n\tpriv->reset_deep_sleep_wakeup = if_sdio_reset_deep_sleep_wakeup;\n\tpriv->reset_card = if_sdio_reset_card;\n\tpriv->power_save = if_sdio_power_save;\n\tpriv->power_restore = if_sdio_power_restore;\n\tpriv->is_polling = !(func->card->host->caps & MMC_CAP_SDIO_IRQ);\n\tret = if_sdio_power_on(card);\n\tif (ret)\n\t\tgoto err_activate_card;\n\nout:\n\treturn ret;\n\nerr_activate_card:\n\tflush_workqueue(card->workqueue);\n\tlbs_remove_card(priv);\nfree:\n\tdestroy_workqueue(card->workqueue);\nerr_queue:\n\twhile (card->packets) {\n\t\tpacket = card->packets;\n\t\tcard->packets = card->packets->next;\n\t\tkfree(packet);\n\t}\n\n\tkfree(card);\n\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,6 +47,10 @@\n \n \tspin_lock_init(&card->lock);\n \tcard->workqueue = alloc_workqueue(\"libertas_sdio\", WQ_MEM_RECLAIM, 0);\n+\tif (unlikely(!card->workqueue)) {\n+\t\tret = -ENOMEM;\n+\t\tgoto err_queue;\n+\t}\n \tINIT_WORK(&card->packet_worker, if_sdio_host_to_card_worker);\n \tinit_waitqueue_head(&card->pwron_waitq);\n \n@@ -98,6 +102,7 @@\n \tlbs_remove_card(priv);\n free:\n \tdestroy_workqueue(card->workqueue);\n+err_queue:\n \twhile (card->packets) {\n \t\tpacket = card->packets;\n \t\tcard->packets = card->packets->next;",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(!card->workqueue)) {",
                "\t\tret = -ENOMEM;",
                "\t\tgoto err_queue;",
                "\t}",
                "err_queue:"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "drivers/net/wireless/marvell/libertas/if_sdio.c in the Linux kernel 5.2.14 does not check the alloc_workqueue return value, leading to a NULL pointer dereference.",
        "id": 2045
    },
    {
        "cve_id": "CVE-2016-8630",
        "code_before_change": "int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t/* Legacy prefixes. */\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t/* operand-size override */\n\t\t\top_prefix = true;\n\t\t\t/* switch between 2/4 bytes */\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t/* address-size override */\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t/* switch between 4/8 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t/* switch between 2/4 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t/* ES override */\n\t\tcase 0x2e:\t/* CS override */\n\t\tcase 0x36:\t/* SS override */\n\t\tcase 0x3e:\t/* DS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t/* FS override */\n\t\tcase 0x65:\t/* GS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: /* REX */\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t/* LOCK */\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t/* REPNE/REPNZ */\n\t\tcase 0xf3:\t/* REP/REPE/REPZ */\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t/* Any legacy prefix after a REX prefix nullifies its effect. */\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t/* REX prefix. */\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t/* REX.W */\n\n\t/* Opcode byte(s). */\n\topcode = opcode_table[ctxt->b];\n\t/* Two-byte opcode? */\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t/* 0F_38 opcode map */\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t/* vex-prefix instructions are not implemented */\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 || (ctxt->modrm & 0xc0) == 0xc0)) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tcase InstrDual:\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.idual->mod3;\n\t\t\telse\n\t\t\t\topcode = opcode.u.idual->mod012;\n\t\t\tbreak;\n\t\tcase ModeDual:\n\t\t\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\t\t\topcode = opcode.u.mdual->mode64;\n\t\t\telse\n\t\t\t\topcode = opcode.u.mdual->mode32;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t/* Unrecognised? */\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t    (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm|NearBranch|\n\t     No16))) {\n\t\t/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t */\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64) {\n\t\t\tif (ctxt->op_bytes == 4 && (ctxt->d & Stack))\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse if (ctxt->d & NearBranch)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t}\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif ((ctxt->d & No16) && ctxt->op_bytes == 2)\n\t\t\tctxt->op_bytes = 4;\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t/* ModRM and SIB bytes. */\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/* Decode and fetch the destination operand: register or memory. */\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative)\n\t\tctxt->memopp->addr.mem.ea = address_mask(ctxt,\n\t\t\t\t\tctxt->memopp->addr.mem.ea + ctxt->_eip);\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
        "code_after_change": "int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t/* Legacy prefixes. */\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t/* operand-size override */\n\t\t\top_prefix = true;\n\t\t\t/* switch between 2/4 bytes */\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t/* address-size override */\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t/* switch between 4/8 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t/* switch between 2/4 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t/* ES override */\n\t\tcase 0x2e:\t/* CS override */\n\t\tcase 0x36:\t/* SS override */\n\t\tcase 0x3e:\t/* DS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t/* FS override */\n\t\tcase 0x65:\t/* GS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: /* REX */\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t/* LOCK */\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t/* REPNE/REPNZ */\n\t\tcase 0xf3:\t/* REP/REPE/REPZ */\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t/* Any legacy prefix after a REX prefix nullifies its effect. */\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t/* REX prefix. */\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t/* REX.W */\n\n\t/* Opcode byte(s). */\n\topcode = opcode_table[ctxt->b];\n\t/* Two-byte opcode? */\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t/* 0F_38 opcode map */\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t/* vex-prefix instructions are not implemented */\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 || (ctxt->modrm & 0xc0) == 0xc0)) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tcase InstrDual:\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.idual->mod3;\n\t\t\telse\n\t\t\t\topcode = opcode.u.idual->mod012;\n\t\t\tbreak;\n\t\tcase ModeDual:\n\t\t\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\t\t\topcode = opcode.u.mdual->mode64;\n\t\t\telse\n\t\t\t\topcode = opcode.u.mdual->mode32;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t/* Unrecognised? */\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t    (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm|NearBranch|\n\t     No16))) {\n\t\t/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t */\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64) {\n\t\t\tif (ctxt->op_bytes == 4 && (ctxt->d & Stack))\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse if (ctxt->d & NearBranch)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t}\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif ((ctxt->d & No16) && ctxt->op_bytes == 2)\n\t\t\tctxt->op_bytes = 4;\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t/* ModRM and SIB bytes. */\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/* Decode and fetch the destination operand: register or memory. */\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative && likely(ctxt->memopp))\n\t\tctxt->memopp->addr.mem.ea = address_mask(ctxt,\n\t\t\t\t\tctxt->memopp->addr.mem.ea + ctxt->_eip);\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
        "patch": "--- code before\n+++ code after\n@@ -259,7 +259,7 @@\n \t/* Decode and fetch the destination operand: register or memory. */\n \trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n \n-\tif (ctxt->rip_relative)\n+\tif (ctxt->rip_relative && likely(ctxt->memopp))\n \t\tctxt->memopp->addr.mem.ea = address_mask(ctxt,\n \t\t\t\t\tctxt->memopp->addr.mem.ea + ctxt->_eip);\n ",
        "function_modified_lines": {
            "added": [
                "\tif (ctxt->rip_relative && likely(ctxt->memopp))"
            ],
            "deleted": [
                "\tif (ctxt->rip_relative)"
            ]
        },
        "cwe": [
            "CWE-284",
            "CWE-476"
        ],
        "cve_description": "The x86_decode_insn function in arch/x86/kvm/emulate.c in the Linux kernel before 4.8.7, when KVM is enabled, allows local users to cause a denial of service (host OS crash) via a certain use of a ModR/M byte in an undefined instruction.",
        "id": 1119
    },
    {
        "cve_id": "CVE-2021-38206",
        "code_before_change": "netdev_tx_t ieee80211_monitor_start_xmit(struct sk_buff *skb,\n\t\t\t\t\t struct net_device *dev)\n{\n\tstruct ieee80211_local *local = wdev_priv(dev->ieee80211_ptr);\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_sub_if_data *tmp_sdata, *sdata;\n\tstruct cfg80211_chan_def *chandef;\n\tu16 len_rthdr;\n\tint hdrlen;\n\n\tmemset(info, 0, sizeof(*info));\n\tinfo->flags = IEEE80211_TX_CTL_REQ_TX_STATUS |\n\t\t      IEEE80211_TX_CTL_INJECTED;\n\n\t/* Sanity-check and process the injection radiotap header */\n\tif (!ieee80211_parse_tx_radiotap(skb, dev))\n\t\tgoto fail;\n\n\t/* we now know there is a radiotap header with a length we can use */\n\tlen_rthdr = ieee80211_get_radiotap_len(skb->data);\n\n\t/*\n\t * fix up the pointers accounting for the radiotap\n\t * header still being in there.  We are being given\n\t * a precooked IEEE80211 header so no need for\n\t * normal processing\n\t */\n\tskb_set_mac_header(skb, len_rthdr);\n\t/*\n\t * these are just fixed to the end of the rt area since we\n\t * don't have any better information and at this point, nobody cares\n\t */\n\tskb_set_network_header(skb, len_rthdr);\n\tskb_set_transport_header(skb, len_rthdr);\n\n\tif (skb->len < len_rthdr + 2)\n\t\tgoto fail;\n\n\thdr = (struct ieee80211_hdr *)(skb->data + len_rthdr);\n\thdrlen = ieee80211_hdrlen(hdr->frame_control);\n\n\tif (skb->len < len_rthdr + hdrlen)\n\t\tgoto fail;\n\n\t/*\n\t * Initialize skb->protocol if the injected frame is a data frame\n\t * carrying a rfc1042 header\n\t */\n\tif (ieee80211_is_data(hdr->frame_control) &&\n\t    skb->len >= len_rthdr + hdrlen + sizeof(rfc1042_header) + 2) {\n\t\tu8 *payload = (u8 *)hdr + hdrlen;\n\n\t\tif (ether_addr_equal(payload, rfc1042_header))\n\t\t\tskb->protocol = cpu_to_be16((payload[6] << 8) |\n\t\t\t\t\t\t    payload[7]);\n\t}\n\n\trcu_read_lock();\n\n\t/*\n\t * We process outgoing injected frames that have a local address\n\t * we handle as though they are non-injected frames.\n\t * This code here isn't entirely correct, the local MAC address\n\t * isn't always enough to find the interface to use; for proper\n\t * VLAN support we have an nl80211-based mechanism.\n\t *\n\t * This is necessary, for example, for old hostapd versions that\n\t * don't use nl80211-based management TX/RX.\n\t */\n\tsdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\n\tlist_for_each_entry_rcu(tmp_sdata, &local->interfaces, list) {\n\t\tif (!ieee80211_sdata_running(tmp_sdata))\n\t\t\tcontinue;\n\t\tif (tmp_sdata->vif.type == NL80211_IFTYPE_MONITOR ||\n\t\t    tmp_sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\t\tcontinue;\n\t\tif (ether_addr_equal(tmp_sdata->vif.addr, hdr->addr2)) {\n\t\t\tsdata = tmp_sdata;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\tif (!chanctx_conf) {\n\t\ttmp_sdata = rcu_dereference(local->monitor_sdata);\n\t\tif (tmp_sdata)\n\t\t\tchanctx_conf =\n\t\t\t\trcu_dereference(tmp_sdata->vif.chanctx_conf);\n\t}\n\n\tif (chanctx_conf)\n\t\tchandef = &chanctx_conf->def;\n\telse if (!local->use_chanctx)\n\t\tchandef = &local->_oper_chandef;\n\telse\n\t\tgoto fail_rcu;\n\n\t/*\n\t * Frame injection is not allowed if beaconing is not allowed\n\t * or if we need radar detection. Beaconing is usually not allowed when\n\t * the mode or operation (Adhoc, AP, Mesh) does not support DFS.\n\t * Passive scan is also used in world regulatory domains where\n\t * your country is not known and as such it should be treated as\n\t * NO TX unless the channel is explicitly allowed in which case\n\t * your current regulatory domain would not have the passive scan\n\t * flag.\n\t *\n\t * Since AP mode uses monitor interfaces to inject/TX management\n\t * frames we can make AP mode the exception to this rule once it\n\t * supports radar detection as its implementation can deal with\n\t * radar detection by itself. We can do that later by adding a\n\t * monitor flag interfaces used for AP support.\n\t */\n\tif (!cfg80211_reg_can_beacon(local->hw.wiphy, chandef,\n\t\t\t\t     sdata->vif.type))\n\t\tgoto fail_rcu;\n\n\tinfo->band = chandef->chan->band;\n\n\t/* Initialize skb->priority according to frame type and TID class,\n\t * with respect to the sub interface that the frame will actually\n\t * be transmitted on. If the DONT_REORDER flag is set, the original\n\t * skb-priority is preserved to assure frames injected with this\n\t * flag are not reordered relative to each other.\n\t */\n\tieee80211_select_queue_80211(sdata, skb, hdr);\n\tskb_set_queue_mapping(skb, ieee80211_ac_from_tid(skb->priority));\n\n\t/* remove the injection radiotap header */\n\tskb_pull(skb, len_rthdr);\n\n\tieee80211_xmit(sdata, NULL, skb);\n\trcu_read_unlock();\n\n\treturn NETDEV_TX_OK;\n\nfail_rcu:\n\trcu_read_unlock();\nfail:\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK; /* meaning, we dealt with the skb */\n}",
        "code_after_change": "netdev_tx_t ieee80211_monitor_start_xmit(struct sk_buff *skb,\n\t\t\t\t\t struct net_device *dev)\n{\n\tstruct ieee80211_local *local = wdev_priv(dev->ieee80211_ptr);\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_sub_if_data *tmp_sdata, *sdata;\n\tstruct cfg80211_chan_def *chandef;\n\tu16 len_rthdr;\n\tint hdrlen;\n\n\tmemset(info, 0, sizeof(*info));\n\tinfo->flags = IEEE80211_TX_CTL_REQ_TX_STATUS |\n\t\t      IEEE80211_TX_CTL_INJECTED;\n\n\t/* Sanity-check the length of the radiotap header */\n\tif (!ieee80211_validate_radiotap_len(skb))\n\t\tgoto fail;\n\n\t/* we now know there is a radiotap header with a length we can use */\n\tlen_rthdr = ieee80211_get_radiotap_len(skb->data);\n\n\t/*\n\t * fix up the pointers accounting for the radiotap\n\t * header still being in there.  We are being given\n\t * a precooked IEEE80211 header so no need for\n\t * normal processing\n\t */\n\tskb_set_mac_header(skb, len_rthdr);\n\t/*\n\t * these are just fixed to the end of the rt area since we\n\t * don't have any better information and at this point, nobody cares\n\t */\n\tskb_set_network_header(skb, len_rthdr);\n\tskb_set_transport_header(skb, len_rthdr);\n\n\tif (skb->len < len_rthdr + 2)\n\t\tgoto fail;\n\n\thdr = (struct ieee80211_hdr *)(skb->data + len_rthdr);\n\thdrlen = ieee80211_hdrlen(hdr->frame_control);\n\n\tif (skb->len < len_rthdr + hdrlen)\n\t\tgoto fail;\n\n\t/*\n\t * Initialize skb->protocol if the injected frame is a data frame\n\t * carrying a rfc1042 header\n\t */\n\tif (ieee80211_is_data(hdr->frame_control) &&\n\t    skb->len >= len_rthdr + hdrlen + sizeof(rfc1042_header) + 2) {\n\t\tu8 *payload = (u8 *)hdr + hdrlen;\n\n\t\tif (ether_addr_equal(payload, rfc1042_header))\n\t\t\tskb->protocol = cpu_to_be16((payload[6] << 8) |\n\t\t\t\t\t\t    payload[7]);\n\t}\n\n\trcu_read_lock();\n\n\t/*\n\t * We process outgoing injected frames that have a local address\n\t * we handle as though they are non-injected frames.\n\t * This code here isn't entirely correct, the local MAC address\n\t * isn't always enough to find the interface to use; for proper\n\t * VLAN support we have an nl80211-based mechanism.\n\t *\n\t * This is necessary, for example, for old hostapd versions that\n\t * don't use nl80211-based management TX/RX.\n\t */\n\tsdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\n\tlist_for_each_entry_rcu(tmp_sdata, &local->interfaces, list) {\n\t\tif (!ieee80211_sdata_running(tmp_sdata))\n\t\t\tcontinue;\n\t\tif (tmp_sdata->vif.type == NL80211_IFTYPE_MONITOR ||\n\t\t    tmp_sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\t\tcontinue;\n\t\tif (ether_addr_equal(tmp_sdata->vif.addr, hdr->addr2)) {\n\t\t\tsdata = tmp_sdata;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\tif (!chanctx_conf) {\n\t\ttmp_sdata = rcu_dereference(local->monitor_sdata);\n\t\tif (tmp_sdata)\n\t\t\tchanctx_conf =\n\t\t\t\trcu_dereference(tmp_sdata->vif.chanctx_conf);\n\t}\n\n\tif (chanctx_conf)\n\t\tchandef = &chanctx_conf->def;\n\telse if (!local->use_chanctx)\n\t\tchandef = &local->_oper_chandef;\n\telse\n\t\tgoto fail_rcu;\n\n\t/*\n\t * Frame injection is not allowed if beaconing is not allowed\n\t * or if we need radar detection. Beaconing is usually not allowed when\n\t * the mode or operation (Adhoc, AP, Mesh) does not support DFS.\n\t * Passive scan is also used in world regulatory domains where\n\t * your country is not known and as such it should be treated as\n\t * NO TX unless the channel is explicitly allowed in which case\n\t * your current regulatory domain would not have the passive scan\n\t * flag.\n\t *\n\t * Since AP mode uses monitor interfaces to inject/TX management\n\t * frames we can make AP mode the exception to this rule once it\n\t * supports radar detection as its implementation can deal with\n\t * radar detection by itself. We can do that later by adding a\n\t * monitor flag interfaces used for AP support.\n\t */\n\tif (!cfg80211_reg_can_beacon(local->hw.wiphy, chandef,\n\t\t\t\t     sdata->vif.type))\n\t\tgoto fail_rcu;\n\n\tinfo->band = chandef->chan->band;\n\n\t/* Initialize skb->priority according to frame type and TID class,\n\t * with respect to the sub interface that the frame will actually\n\t * be transmitted on. If the DONT_REORDER flag is set, the original\n\t * skb-priority is preserved to assure frames injected with this\n\t * flag are not reordered relative to each other.\n\t */\n\tieee80211_select_queue_80211(sdata, skb, hdr);\n\tskb_set_queue_mapping(skb, ieee80211_ac_from_tid(skb->priority));\n\n\t/*\n\t * Process the radiotap header. This will now take into account the\n\t * selected chandef above to accurately set injection rates and\n\t * retransmissions.\n\t */\n\tif (!ieee80211_parse_tx_radiotap(skb, dev))\n\t\tgoto fail_rcu;\n\n\t/* remove the injection radiotap header */\n\tskb_pull(skb, len_rthdr);\n\n\tieee80211_xmit(sdata, NULL, skb);\n\trcu_read_unlock();\n\n\treturn NETDEV_TX_OK;\n\nfail_rcu:\n\trcu_read_unlock();\nfail:\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK; /* meaning, we dealt with the skb */\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,8 +14,8 @@\n \tinfo->flags = IEEE80211_TX_CTL_REQ_TX_STATUS |\n \t\t      IEEE80211_TX_CTL_INJECTED;\n \n-\t/* Sanity-check and process the injection radiotap header */\n-\tif (!ieee80211_parse_tx_radiotap(skb, dev))\n+\t/* Sanity-check the length of the radiotap header */\n+\tif (!ieee80211_validate_radiotap_len(skb))\n \t\tgoto fail;\n \n \t/* we now know there is a radiotap header with a length we can use */\n@@ -129,6 +129,14 @@\n \tieee80211_select_queue_80211(sdata, skb, hdr);\n \tskb_set_queue_mapping(skb, ieee80211_ac_from_tid(skb->priority));\n \n+\t/*\n+\t * Process the radiotap header. This will now take into account the\n+\t * selected chandef above to accurately set injection rates and\n+\t * retransmissions.\n+\t */\n+\tif (!ieee80211_parse_tx_radiotap(skb, dev))\n+\t\tgoto fail_rcu;\n+\n \t/* remove the injection radiotap header */\n \tskb_pull(skb, len_rthdr);\n ",
        "function_modified_lines": {
            "added": [
                "\t/* Sanity-check the length of the radiotap header */",
                "\tif (!ieee80211_validate_radiotap_len(skb))",
                "\t/*",
                "\t * Process the radiotap header. This will now take into account the",
                "\t * selected chandef above to accurately set injection rates and",
                "\t * retransmissions.",
                "\t */",
                "\tif (!ieee80211_parse_tx_radiotap(skb, dev))",
                "\t\tgoto fail_rcu;",
                ""
            ],
            "deleted": [
                "\t/* Sanity-check and process the injection radiotap header */",
                "\tif (!ieee80211_parse_tx_radiotap(skb, dev))"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The mac80211 subsystem in the Linux kernel before 5.12.13, when a device supporting only 5 GHz is used, allows attackers to cause a denial of service (NULL pointer dereference in the radiotap parser) by injecting a frame with 802.11a rates.",
        "id": 3083
    },
    {
        "cve_id": "CVE-2017-7374",
        "code_before_change": "static void put_crypt_info(struct fscrypt_info *ci)\n{\n\tif (!ci)\n\t\treturn;\n\n\tkey_put(ci->ci_keyring_key);\n\tcrypto_free_skcipher(ci->ci_ctfm);\n\tkmem_cache_free(fscrypt_info_cachep, ci);\n}",
        "code_after_change": "static void put_crypt_info(struct fscrypt_info *ci)\n{\n\tif (!ci)\n\t\treturn;\n\n\tcrypto_free_skcipher(ci->ci_ctfm);\n\tkmem_cache_free(fscrypt_info_cachep, ci);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,6 @@\n \tif (!ci)\n \t\treturn;\n \n-\tkey_put(ci->ci_keyring_key);\n \tcrypto_free_skcipher(ci->ci_ctfm);\n \tkmem_cache_free(fscrypt_info_cachep, ci);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tkey_put(ci->ci_keyring_key);"
            ]
        },
        "cwe": [
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in fs/crypto/ in the Linux kernel before 4.10.7 allows local users to cause a denial of service (NULL pointer dereference) or possibly gain privileges by revoking keyring keys being used for ext4, f2fs, or ubifs encryption, causing cryptographic transform objects to be freed prematurely.",
        "id": 1501
    },
    {
        "cve_id": "CVE-2014-7826",
        "code_before_change": "static void ftrace_syscall_exit(void *data, struct pt_regs *regs, long ret)\n{\n\tstruct trace_array *tr = data;\n\tstruct ftrace_event_file *ftrace_file;\n\tstruct syscall_trace_exit *entry;\n\tstruct syscall_metadata *sys_data;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tunsigned long irq_flags;\n\tint pc;\n\tint syscall_nr;\n\n\tsyscall_nr = trace_get_syscall_nr(current, regs);\n\tif (syscall_nr < 0)\n\t\treturn;\n\n\t/* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE()) */\n\tftrace_file = rcu_dereference_sched(tr->exit_syscall_files[syscall_nr]);\n\tif (!ftrace_file)\n\t\treturn;\n\n\tif (ftrace_trigger_soft_disabled(ftrace_file))\n\t\treturn;\n\n\tsys_data = syscall_nr_to_meta(syscall_nr);\n\tif (!sys_data)\n\t\treturn;\n\n\tlocal_save_flags(irq_flags);\n\tpc = preempt_count();\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = trace_buffer_lock_reserve(buffer,\n\t\t\tsys_data->exit_event->event.type, sizeof(*entry),\n\t\t\tirq_flags, pc);\n\tif (!event)\n\t\treturn;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->nr = syscall_nr;\n\tentry->ret = syscall_get_return_value(current, regs);\n\n\tevent_trigger_unlock_commit(ftrace_file, buffer, event, entry,\n\t\t\t\t    irq_flags, pc);\n}",
        "code_after_change": "static void ftrace_syscall_exit(void *data, struct pt_regs *regs, long ret)\n{\n\tstruct trace_array *tr = data;\n\tstruct ftrace_event_file *ftrace_file;\n\tstruct syscall_trace_exit *entry;\n\tstruct syscall_metadata *sys_data;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tunsigned long irq_flags;\n\tint pc;\n\tint syscall_nr;\n\n\tsyscall_nr = trace_get_syscall_nr(current, regs);\n\tif (syscall_nr < 0 || syscall_nr >= NR_syscalls)\n\t\treturn;\n\n\t/* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE()) */\n\tftrace_file = rcu_dereference_sched(tr->exit_syscall_files[syscall_nr]);\n\tif (!ftrace_file)\n\t\treturn;\n\n\tif (ftrace_trigger_soft_disabled(ftrace_file))\n\t\treturn;\n\n\tsys_data = syscall_nr_to_meta(syscall_nr);\n\tif (!sys_data)\n\t\treturn;\n\n\tlocal_save_flags(irq_flags);\n\tpc = preempt_count();\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = trace_buffer_lock_reserve(buffer,\n\t\t\tsys_data->exit_event->event.type, sizeof(*entry),\n\t\t\tirq_flags, pc);\n\tif (!event)\n\t\treturn;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->nr = syscall_nr;\n\tentry->ret = syscall_get_return_value(current, regs);\n\n\tevent_trigger_unlock_commit(ftrace_file, buffer, event, entry,\n\t\t\t\t    irq_flags, pc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \tint syscall_nr;\n \n \tsyscall_nr = trace_get_syscall_nr(current, regs);\n-\tif (syscall_nr < 0)\n+\tif (syscall_nr < 0 || syscall_nr >= NR_syscalls)\n \t\treturn;\n \n \t/* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE()) */",
        "function_modified_lines": {
            "added": [
                "\tif (syscall_nr < 0 || syscall_nr >= NR_syscalls)"
            ],
            "deleted": [
                "\tif (syscall_nr < 0)"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "kernel/trace/trace_syscalls.c in the Linux kernel through 3.17.2 does not properly handle private syscall numbers during use of the ftrace subsystem, which allows local users to gain privileges or cause a denial of service (invalid pointer dereference) via a crafted application.",
        "id": 593
    },
    {
        "cve_id": "CVE-2023-3220",
        "code_before_change": "static int dpu_crtc_atomic_check(struct drm_crtc *crtc,\n\t\tstruct drm_atomic_state *state)\n{\n\tstruct drm_crtc_state *crtc_state = drm_atomic_get_new_crtc_state(state,\n\t\t\t\t\t\t\t\t\t  crtc);\n\tstruct dpu_crtc *dpu_crtc = to_dpu_crtc(crtc);\n\tstruct dpu_crtc_state *cstate = to_dpu_crtc_state(crtc_state);\n\tstruct plane_state *pstates;\n\n\tconst struct drm_plane_state *pstate;\n\tstruct drm_plane *plane;\n\tstruct drm_display_mode *mode;\n\n\tint cnt = 0, rc = 0, mixer_width = 0, i, z_pos;\n\n\tstruct dpu_multirect_plane_states multirect_plane[DPU_STAGE_MAX * 2];\n\tint multirect_count = 0;\n\tconst struct drm_plane_state *pipe_staged[SSPP_MAX];\n\tint left_zpos_cnt = 0, right_zpos_cnt = 0;\n\tstruct drm_rect crtc_rect = { 0 };\n\tbool needs_dirtyfb = dpu_crtc_needs_dirtyfb(crtc_state);\n\n\tpstates = kzalloc(sizeof(*pstates) * DPU_STAGE_MAX * 4, GFP_KERNEL);\n\n\tif (!crtc_state->enable || !crtc_state->active) {\n\t\tDRM_DEBUG_ATOMIC(\"crtc%d -> enable %d, active %d, skip atomic_check\\n\",\n\t\t\t\tcrtc->base.id, crtc_state->enable,\n\t\t\t\tcrtc_state->active);\n\t\tmemset(&cstate->new_perf, 0, sizeof(cstate->new_perf));\n\t\tgoto end;\n\t}\n\n\tmode = &crtc_state->adjusted_mode;\n\tDRM_DEBUG_ATOMIC(\"%s: check\\n\", dpu_crtc->name);\n\n\t/* force a full mode set if active state changed */\n\tif (crtc_state->active_changed)\n\t\tcrtc_state->mode_changed = true;\n\n\tmemset(pipe_staged, 0, sizeof(pipe_staged));\n\n\tif (cstate->num_mixers) {\n\t\tmixer_width = mode->hdisplay / cstate->num_mixers;\n\n\t\t_dpu_crtc_setup_lm_bounds(crtc, crtc_state);\n\t}\n\n\tcrtc_rect.x2 = mode->hdisplay;\n\tcrtc_rect.y2 = mode->vdisplay;\n\n\t /* get plane state for all drm planes associated with crtc state */\n\tdrm_atomic_crtc_state_for_each_plane_state(plane, pstate, crtc_state) {\n\t\tstruct dpu_plane_state *dpu_pstate = to_dpu_plane_state(pstate);\n\t\tstruct drm_rect dst, clip = crtc_rect;\n\n\t\tif (IS_ERR_OR_NULL(pstate)) {\n\t\t\trc = PTR_ERR(pstate);\n\t\t\tDPU_ERROR(\"%s: failed to get plane%d state, %d\\n\",\n\t\t\t\t\tdpu_crtc->name, plane->base.id, rc);\n\t\t\tgoto end;\n\t\t}\n\t\tif (cnt >= DPU_STAGE_MAX * 4)\n\t\t\tcontinue;\n\n\t\tif (!pstate->visible)\n\t\t\tcontinue;\n\n\t\tpstates[cnt].dpu_pstate = dpu_pstate;\n\t\tpstates[cnt].drm_pstate = pstate;\n\t\tpstates[cnt].stage = pstate->normalized_zpos;\n\t\tpstates[cnt].pipe_id = dpu_plane_pipe(plane);\n\n\t\tdpu_pstate->needs_dirtyfb = needs_dirtyfb;\n\n\t\tif (pipe_staged[pstates[cnt].pipe_id]) {\n\t\t\tmultirect_plane[multirect_count].r0 =\n\t\t\t\tpipe_staged[pstates[cnt].pipe_id];\n\t\t\tmultirect_plane[multirect_count].r1 = pstate;\n\t\t\tmultirect_count++;\n\n\t\t\tpipe_staged[pstates[cnt].pipe_id] = NULL;\n\t\t} else {\n\t\t\tpipe_staged[pstates[cnt].pipe_id] = pstate;\n\t\t}\n\n\t\tcnt++;\n\n\t\tdst = drm_plane_state_dest(pstate);\n\t\tif (!drm_rect_intersect(&clip, &dst)) {\n\t\t\tDPU_ERROR(\"invalid vertical/horizontal destination\\n\");\n\t\t\tDPU_ERROR(\"display: \" DRM_RECT_FMT \" plane: \"\n\t\t\t\t  DRM_RECT_FMT \"\\n\", DRM_RECT_ARG(&crtc_rect),\n\t\t\t\t  DRM_RECT_ARG(&dst));\n\t\t\trc = -E2BIG;\n\t\t\tgoto end;\n\t\t}\n\t}\n\n\tfor (i = 1; i < SSPP_MAX; i++) {\n\t\tif (pipe_staged[i])\n\t\t\tdpu_plane_clear_multirect(pipe_staged[i]);\n\t}\n\n\tz_pos = -1;\n\tfor (i = 0; i < cnt; i++) {\n\t\t/* reset counts at every new blend stage */\n\t\tif (pstates[i].stage != z_pos) {\n\t\t\tleft_zpos_cnt = 0;\n\t\t\tright_zpos_cnt = 0;\n\t\t\tz_pos = pstates[i].stage;\n\t\t}\n\n\t\t/* verify z_pos setting before using it */\n\t\tif (z_pos >= DPU_STAGE_MAX - DPU_STAGE_0) {\n\t\t\tDPU_ERROR(\"> %d plane stages assigned\\n\",\n\t\t\t\t\tDPU_STAGE_MAX - DPU_STAGE_0);\n\t\t\trc = -EINVAL;\n\t\t\tgoto end;\n\t\t} else if (pstates[i].drm_pstate->crtc_x < mixer_width) {\n\t\t\tif (left_zpos_cnt == 2) {\n\t\t\t\tDPU_ERROR(\"> 2 planes @ stage %d on left\\n\",\n\t\t\t\t\tz_pos);\n\t\t\t\trc = -EINVAL;\n\t\t\t\tgoto end;\n\t\t\t}\n\t\t\tleft_zpos_cnt++;\n\n\t\t} else {\n\t\t\tif (right_zpos_cnt == 2) {\n\t\t\t\tDPU_ERROR(\"> 2 planes @ stage %d on right\\n\",\n\t\t\t\t\tz_pos);\n\t\t\t\trc = -EINVAL;\n\t\t\t\tgoto end;\n\t\t\t}\n\t\t\tright_zpos_cnt++;\n\t\t}\n\n\t\tpstates[i].dpu_pstate->stage = z_pos + DPU_STAGE_0;\n\t\tDRM_DEBUG_ATOMIC(\"%s: zpos %d\\n\", dpu_crtc->name, z_pos);\n\t}\n\n\tfor (i = 0; i < multirect_count; i++) {\n\t\tif (dpu_plane_validate_multirect_v2(&multirect_plane[i])) {\n\t\t\tDPU_ERROR(\n\t\t\t\"multirect validation failed for planes (%d - %d)\\n\",\n\t\t\t\t\tmultirect_plane[i].r0->plane->base.id,\n\t\t\t\t\tmultirect_plane[i].r1->plane->base.id);\n\t\t\trc = -EINVAL;\n\t\t\tgoto end;\n\t\t}\n\t}\n\n\tatomic_inc(&_dpu_crtc_get_kms(crtc)->bandwidth_ref);\n\n\trc = dpu_core_perf_crtc_check(crtc, crtc_state);\n\tif (rc) {\n\t\tDPU_ERROR(\"crtc%d failed performance check %d\\n\",\n\t\t\t\tcrtc->base.id, rc);\n\t\tgoto end;\n\t}\n\n\t/* validate source split:\n\t * use pstates sorted by stage to check planes on same stage\n\t * we assume that all pipes are in source split so its valid to compare\n\t * without taking into account left/right mixer placement\n\t */\n\tfor (i = 1; i < cnt; i++) {\n\t\tstruct plane_state *prv_pstate, *cur_pstate;\n\t\tstruct drm_rect left_rect, right_rect;\n\t\tint32_t left_pid, right_pid;\n\t\tint32_t stage;\n\n\t\tprv_pstate = &pstates[i - 1];\n\t\tcur_pstate = &pstates[i];\n\t\tif (prv_pstate->stage != cur_pstate->stage)\n\t\t\tcontinue;\n\n\t\tstage = cur_pstate->stage;\n\n\t\tleft_pid = prv_pstate->dpu_pstate->base.plane->base.id;\n\t\tleft_rect = drm_plane_state_dest(prv_pstate->drm_pstate);\n\n\t\tright_pid = cur_pstate->dpu_pstate->base.plane->base.id;\n\t\tright_rect = drm_plane_state_dest(cur_pstate->drm_pstate);\n\n\t\tif (right_rect.x1 < left_rect.x1) {\n\t\t\tswap(left_pid, right_pid);\n\t\t\tswap(left_rect, right_rect);\n\t\t}\n\n\t\t/**\n\t\t * - planes are enumerated in pipe-priority order such that\n\t\t *   planes with lower drm_id must be left-most in a shared\n\t\t *   blend-stage when using source split.\n\t\t * - planes in source split must be contiguous in width\n\t\t * - planes in source split must have same dest yoff and height\n\t\t */\n\t\tif (right_pid < left_pid) {\n\t\t\tDPU_ERROR(\n\t\t\t\t\"invalid src split cfg. priority mismatch. stage: %d left: %d right: %d\\n\",\n\t\t\t\tstage, left_pid, right_pid);\n\t\t\trc = -EINVAL;\n\t\t\tgoto end;\n\t\t} else if (right_rect.x1 != drm_rect_width(&left_rect)) {\n\t\t\tDPU_ERROR(\"non-contiguous coordinates for src split. \"\n\t\t\t\t  \"stage: %d left: \" DRM_RECT_FMT \" right: \"\n\t\t\t\t  DRM_RECT_FMT \"\\n\", stage,\n\t\t\t\t  DRM_RECT_ARG(&left_rect),\n\t\t\t\t  DRM_RECT_ARG(&right_rect));\n\t\t\trc = -EINVAL;\n\t\t\tgoto end;\n\t\t} else if (left_rect.y1 != right_rect.y1 ||\n\t\t\t   drm_rect_height(&left_rect) != drm_rect_height(&right_rect)) {\n\t\t\tDPU_ERROR(\"source split at stage: %d. invalid \"\n\t\t\t\t  \"yoff/height: left: \" DRM_RECT_FMT \" right: \"\n\t\t\t\t  DRM_RECT_FMT \"\\n\", stage,\n\t\t\t\t  DRM_RECT_ARG(&left_rect),\n\t\t\t\t  DRM_RECT_ARG(&right_rect));\n\t\t\trc = -EINVAL;\n\t\t\tgoto end;\n\t\t}\n\t}\n\nend:\n\tkfree(pstates);\n\treturn rc;\n}",
        "code_after_change": "static int dpu_crtc_atomic_check(struct drm_crtc *crtc,\n\t\tstruct drm_atomic_state *state)\n{\n\tstruct drm_crtc_state *crtc_state = drm_atomic_get_new_crtc_state(state,\n\t\t\t\t\t\t\t\t\t  crtc);\n\tstruct dpu_crtc *dpu_crtc = to_dpu_crtc(crtc);\n\tstruct dpu_crtc_state *cstate = to_dpu_crtc_state(crtc_state);\n\tstruct plane_state *pstates;\n\n\tconst struct drm_plane_state *pstate;\n\tstruct drm_plane *plane;\n\tstruct drm_display_mode *mode;\n\n\tint cnt = 0, rc = 0, mixer_width = 0, i, z_pos;\n\n\tstruct dpu_multirect_plane_states multirect_plane[DPU_STAGE_MAX * 2];\n\tint multirect_count = 0;\n\tconst struct drm_plane_state *pipe_staged[SSPP_MAX];\n\tint left_zpos_cnt = 0, right_zpos_cnt = 0;\n\tstruct drm_rect crtc_rect = { 0 };\n\tbool needs_dirtyfb = dpu_crtc_needs_dirtyfb(crtc_state);\n\n\tpstates = kzalloc(sizeof(*pstates) * DPU_STAGE_MAX * 4, GFP_KERNEL);\n\tif (!pstates)\n\t\treturn -ENOMEM;\n\n\tif (!crtc_state->enable || !crtc_state->active) {\n\t\tDRM_DEBUG_ATOMIC(\"crtc%d -> enable %d, active %d, skip atomic_check\\n\",\n\t\t\t\tcrtc->base.id, crtc_state->enable,\n\t\t\t\tcrtc_state->active);\n\t\tmemset(&cstate->new_perf, 0, sizeof(cstate->new_perf));\n\t\tgoto end;\n\t}\n\n\tmode = &crtc_state->adjusted_mode;\n\tDRM_DEBUG_ATOMIC(\"%s: check\\n\", dpu_crtc->name);\n\n\t/* force a full mode set if active state changed */\n\tif (crtc_state->active_changed)\n\t\tcrtc_state->mode_changed = true;\n\n\tmemset(pipe_staged, 0, sizeof(pipe_staged));\n\n\tif (cstate->num_mixers) {\n\t\tmixer_width = mode->hdisplay / cstate->num_mixers;\n\n\t\t_dpu_crtc_setup_lm_bounds(crtc, crtc_state);\n\t}\n\n\tcrtc_rect.x2 = mode->hdisplay;\n\tcrtc_rect.y2 = mode->vdisplay;\n\n\t /* get plane state for all drm planes associated with crtc state */\n\tdrm_atomic_crtc_state_for_each_plane_state(plane, pstate, crtc_state) {\n\t\tstruct dpu_plane_state *dpu_pstate = to_dpu_plane_state(pstate);\n\t\tstruct drm_rect dst, clip = crtc_rect;\n\n\t\tif (IS_ERR_OR_NULL(pstate)) {\n\t\t\trc = PTR_ERR(pstate);\n\t\t\tDPU_ERROR(\"%s: failed to get plane%d state, %d\\n\",\n\t\t\t\t\tdpu_crtc->name, plane->base.id, rc);\n\t\t\tgoto end;\n\t\t}\n\t\tif (cnt >= DPU_STAGE_MAX * 4)\n\t\t\tcontinue;\n\n\t\tif (!pstate->visible)\n\t\t\tcontinue;\n\n\t\tpstates[cnt].dpu_pstate = dpu_pstate;\n\t\tpstates[cnt].drm_pstate = pstate;\n\t\tpstates[cnt].stage = pstate->normalized_zpos;\n\t\tpstates[cnt].pipe_id = dpu_plane_pipe(plane);\n\n\t\tdpu_pstate->needs_dirtyfb = needs_dirtyfb;\n\n\t\tif (pipe_staged[pstates[cnt].pipe_id]) {\n\t\t\tmultirect_plane[multirect_count].r0 =\n\t\t\t\tpipe_staged[pstates[cnt].pipe_id];\n\t\t\tmultirect_plane[multirect_count].r1 = pstate;\n\t\t\tmultirect_count++;\n\n\t\t\tpipe_staged[pstates[cnt].pipe_id] = NULL;\n\t\t} else {\n\t\t\tpipe_staged[pstates[cnt].pipe_id] = pstate;\n\t\t}\n\n\t\tcnt++;\n\n\t\tdst = drm_plane_state_dest(pstate);\n\t\tif (!drm_rect_intersect(&clip, &dst)) {\n\t\t\tDPU_ERROR(\"invalid vertical/horizontal destination\\n\");\n\t\t\tDPU_ERROR(\"display: \" DRM_RECT_FMT \" plane: \"\n\t\t\t\t  DRM_RECT_FMT \"\\n\", DRM_RECT_ARG(&crtc_rect),\n\t\t\t\t  DRM_RECT_ARG(&dst));\n\t\t\trc = -E2BIG;\n\t\t\tgoto end;\n\t\t}\n\t}\n\n\tfor (i = 1; i < SSPP_MAX; i++) {\n\t\tif (pipe_staged[i])\n\t\t\tdpu_plane_clear_multirect(pipe_staged[i]);\n\t}\n\n\tz_pos = -1;\n\tfor (i = 0; i < cnt; i++) {\n\t\t/* reset counts at every new blend stage */\n\t\tif (pstates[i].stage != z_pos) {\n\t\t\tleft_zpos_cnt = 0;\n\t\t\tright_zpos_cnt = 0;\n\t\t\tz_pos = pstates[i].stage;\n\t\t}\n\n\t\t/* verify z_pos setting before using it */\n\t\tif (z_pos >= DPU_STAGE_MAX - DPU_STAGE_0) {\n\t\t\tDPU_ERROR(\"> %d plane stages assigned\\n\",\n\t\t\t\t\tDPU_STAGE_MAX - DPU_STAGE_0);\n\t\t\trc = -EINVAL;\n\t\t\tgoto end;\n\t\t} else if (pstates[i].drm_pstate->crtc_x < mixer_width) {\n\t\t\tif (left_zpos_cnt == 2) {\n\t\t\t\tDPU_ERROR(\"> 2 planes @ stage %d on left\\n\",\n\t\t\t\t\tz_pos);\n\t\t\t\trc = -EINVAL;\n\t\t\t\tgoto end;\n\t\t\t}\n\t\t\tleft_zpos_cnt++;\n\n\t\t} else {\n\t\t\tif (right_zpos_cnt == 2) {\n\t\t\t\tDPU_ERROR(\"> 2 planes @ stage %d on right\\n\",\n\t\t\t\t\tz_pos);\n\t\t\t\trc = -EINVAL;\n\t\t\t\tgoto end;\n\t\t\t}\n\t\t\tright_zpos_cnt++;\n\t\t}\n\n\t\tpstates[i].dpu_pstate->stage = z_pos + DPU_STAGE_0;\n\t\tDRM_DEBUG_ATOMIC(\"%s: zpos %d\\n\", dpu_crtc->name, z_pos);\n\t}\n\n\tfor (i = 0; i < multirect_count; i++) {\n\t\tif (dpu_plane_validate_multirect_v2(&multirect_plane[i])) {\n\t\t\tDPU_ERROR(\n\t\t\t\"multirect validation failed for planes (%d - %d)\\n\",\n\t\t\t\t\tmultirect_plane[i].r0->plane->base.id,\n\t\t\t\t\tmultirect_plane[i].r1->plane->base.id);\n\t\t\trc = -EINVAL;\n\t\t\tgoto end;\n\t\t}\n\t}\n\n\tatomic_inc(&_dpu_crtc_get_kms(crtc)->bandwidth_ref);\n\n\trc = dpu_core_perf_crtc_check(crtc, crtc_state);\n\tif (rc) {\n\t\tDPU_ERROR(\"crtc%d failed performance check %d\\n\",\n\t\t\t\tcrtc->base.id, rc);\n\t\tgoto end;\n\t}\n\n\t/* validate source split:\n\t * use pstates sorted by stage to check planes on same stage\n\t * we assume that all pipes are in source split so its valid to compare\n\t * without taking into account left/right mixer placement\n\t */\n\tfor (i = 1; i < cnt; i++) {\n\t\tstruct plane_state *prv_pstate, *cur_pstate;\n\t\tstruct drm_rect left_rect, right_rect;\n\t\tint32_t left_pid, right_pid;\n\t\tint32_t stage;\n\n\t\tprv_pstate = &pstates[i - 1];\n\t\tcur_pstate = &pstates[i];\n\t\tif (prv_pstate->stage != cur_pstate->stage)\n\t\t\tcontinue;\n\n\t\tstage = cur_pstate->stage;\n\n\t\tleft_pid = prv_pstate->dpu_pstate->base.plane->base.id;\n\t\tleft_rect = drm_plane_state_dest(prv_pstate->drm_pstate);\n\n\t\tright_pid = cur_pstate->dpu_pstate->base.plane->base.id;\n\t\tright_rect = drm_plane_state_dest(cur_pstate->drm_pstate);\n\n\t\tif (right_rect.x1 < left_rect.x1) {\n\t\t\tswap(left_pid, right_pid);\n\t\t\tswap(left_rect, right_rect);\n\t\t}\n\n\t\t/**\n\t\t * - planes are enumerated in pipe-priority order such that\n\t\t *   planes with lower drm_id must be left-most in a shared\n\t\t *   blend-stage when using source split.\n\t\t * - planes in source split must be contiguous in width\n\t\t * - planes in source split must have same dest yoff and height\n\t\t */\n\t\tif (right_pid < left_pid) {\n\t\t\tDPU_ERROR(\n\t\t\t\t\"invalid src split cfg. priority mismatch. stage: %d left: %d right: %d\\n\",\n\t\t\t\tstage, left_pid, right_pid);\n\t\t\trc = -EINVAL;\n\t\t\tgoto end;\n\t\t} else if (right_rect.x1 != drm_rect_width(&left_rect)) {\n\t\t\tDPU_ERROR(\"non-contiguous coordinates for src split. \"\n\t\t\t\t  \"stage: %d left: \" DRM_RECT_FMT \" right: \"\n\t\t\t\t  DRM_RECT_FMT \"\\n\", stage,\n\t\t\t\t  DRM_RECT_ARG(&left_rect),\n\t\t\t\t  DRM_RECT_ARG(&right_rect));\n\t\t\trc = -EINVAL;\n\t\t\tgoto end;\n\t\t} else if (left_rect.y1 != right_rect.y1 ||\n\t\t\t   drm_rect_height(&left_rect) != drm_rect_height(&right_rect)) {\n\t\t\tDPU_ERROR(\"source split at stage: %d. invalid \"\n\t\t\t\t  \"yoff/height: left: \" DRM_RECT_FMT \" right: \"\n\t\t\t\t  DRM_RECT_FMT \"\\n\", stage,\n\t\t\t\t  DRM_RECT_ARG(&left_rect),\n\t\t\t\t  DRM_RECT_ARG(&right_rect));\n\t\t\trc = -EINVAL;\n\t\t\tgoto end;\n\t\t}\n\t}\n\nend:\n\tkfree(pstates);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,8 @@\n \tbool needs_dirtyfb = dpu_crtc_needs_dirtyfb(crtc_state);\n \n \tpstates = kzalloc(sizeof(*pstates) * DPU_STAGE_MAX * 4, GFP_KERNEL);\n+\tif (!pstates)\n+\t\treturn -ENOMEM;\n \n \tif (!crtc_state->enable || !crtc_state->active) {\n \t\tDRM_DEBUG_ATOMIC(\"crtc%d -> enable %d, active %d, skip atomic_check\\n\",",
        "function_modified_lines": {
            "added": [
                "\tif (!pstates)",
                "\t\treturn -ENOMEM;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.1-rc8. dpu_crtc_atomic_check in drivers/gpu/drm/msm/disp/dpu1/dpu_crtc.c lacks check of the return value of kzalloc() and will cause the NULL Pointer Dereference.",
        "id": 4005
    },
    {
        "cve_id": "CVE-2018-14646",
        "code_before_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(skb, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
        "code_after_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(NETLINK_CB(skb).sk, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,7 @@\n \n \tif (tb[IFLA_IF_NETNSID]) {\n \t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n-\t\ttgt_net = get_target_net(skb, netnsid);\n+\t\ttgt_net = get_target_net(NETLINK_CB(skb).sk, netnsid);\n \t\tif (IS_ERR(tgt_net))\n \t\t\treturn PTR_ERR(tgt_net);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\ttgt_net = get_target_net(NETLINK_CB(skb).sk, netnsid);"
            ],
            "deleted": [
                "\t\ttgt_net = get_target_net(skb, netnsid);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The Linux kernel before 4.15-rc8 was found to be vulnerable to a NULL pointer dereference bug in the __netlink_ns_capable() function in the net/netlink/af_netlink.c file. A local attacker could exploit this when a net namespace with a netnsid is assigned to cause a kernel panic and a denial of service.",
        "id": 1701
    },
    {
        "cve_id": "CVE-2018-7191",
        "code_before_change": "static int tun_set_iff(struct net *net, struct file *file, struct ifreq *ifr)\n{\n\tstruct tun_struct *tun;\n\tstruct tun_file *tfile = file->private_data;\n\tstruct net_device *dev;\n\tint err;\n\n\tif (tfile->detached)\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_name(net, ifr->ifr_name);\n\tif (dev) {\n\t\tif (ifr->ifr_flags & IFF_TUN_EXCL)\n\t\t\treturn -EBUSY;\n\t\tif ((ifr->ifr_flags & IFF_TUN) && dev->netdev_ops == &tun_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse if ((ifr->ifr_flags & IFF_TAP) && dev->netdev_ops == &tap_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t\tif (!!(ifr->ifr_flags & IFF_MULTI_QUEUE) !=\n\t\t    !!(tun->flags & IFF_MULTI_QUEUE))\n\t\t\treturn -EINVAL;\n\n\t\tif (tun_not_capable(tun))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_open(tun->security);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = tun_attach(tun, file, ifr->ifr_flags & IFF_NOFILTER);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tif (tun->flags & IFF_MULTI_QUEUE &&\n\t\t    (tun->numqueues + tun->numdisabled > 1)) {\n\t\t\t/* One or more queue has already been attached, no need\n\t\t\t * to initialize the device again.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\telse {\n\t\tchar *name;\n\t\tunsigned long flags = 0;\n\t\tint queues = ifr->ifr_flags & IFF_MULTI_QUEUE ?\n\t\t\t     MAX_TAP_QUEUES : 1;\n\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_create();\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\t/* Set dev type */\n\t\tif (ifr->ifr_flags & IFF_TUN) {\n\t\t\t/* TUN device */\n\t\t\tflags |= IFF_TUN;\n\t\t\tname = \"tun%d\";\n\t\t} else if (ifr->ifr_flags & IFF_TAP) {\n\t\t\t/* TAP device */\n\t\t\tflags |= IFF_TAP;\n\t\t\tname = \"tap%d\";\n\t\t} else\n\t\t\treturn -EINVAL;\n\n\t\tif (*ifr->ifr_name)\n\t\t\tname = ifr->ifr_name;\n\n\t\tdev = alloc_netdev_mqs(sizeof(struct tun_struct), name,\n\t\t\t\t       NET_NAME_UNKNOWN, tun_setup, queues,\n\t\t\t\t       queues);\n\n\t\tif (!dev)\n\t\t\treturn -ENOMEM;\n\n\t\tdev_net_set(dev, net);\n\t\tdev->rtnl_link_ops = &tun_link_ops;\n\t\tdev->ifindex = tfile->ifindex;\n\t\tdev->sysfs_groups[0] = &tun_attr_group;\n\n\t\ttun = netdev_priv(dev);\n\t\ttun->dev = dev;\n\t\ttun->flags = flags;\n\t\ttun->txflt.count = 0;\n\t\ttun->vnet_hdr_sz = sizeof(struct virtio_net_hdr);\n\n\t\ttun->align = NET_SKB_PAD;\n\t\ttun->filter_attached = false;\n\t\ttun->sndbuf = tfile->socket.sk->sk_sndbuf;\n\t\ttun->rx_batched = 0;\n\n\t\ttun->pcpu_stats = netdev_alloc_pcpu_stats(struct tun_pcpu_stats);\n\t\tif (!tun->pcpu_stats) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_dev;\n\t\t}\n\n\t\tspin_lock_init(&tun->lock);\n\n\t\terr = security_tun_dev_alloc_security(&tun->security);\n\t\tif (err < 0)\n\t\t\tgoto err_free_stat;\n\n\t\ttun_net_init(dev);\n\t\ttun_flow_init(tun);\n\n\t\tdev->hw_features = NETIF_F_SG | NETIF_F_FRAGLIST |\n\t\t\t\t   TUN_USER_FEATURES | NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t   NETIF_F_HW_VLAN_STAG_TX;\n\t\tdev->features = dev->hw_features | NETIF_F_LLTX;\n\t\tdev->vlan_features = dev->features &\n\t\t\t\t     ~(NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t       NETIF_F_HW_VLAN_STAG_TX);\n\n\t\tINIT_LIST_HEAD(&tun->disabled);\n\t\terr = tun_attach(tun, file, false);\n\t\tif (err < 0)\n\t\t\tgoto err_free_flow;\n\n\t\terr = register_netdevice(tun->dev);\n\t\tif (err < 0)\n\t\t\tgoto err_detach;\n\t}\n\n\tnetif_carrier_on(tun->dev);\n\n\ttun_debug(KERN_INFO, tun, \"tun_set_iff\\n\");\n\n\ttun->flags = (tun->flags & ~TUN_FEATURES) |\n\t\t(ifr->ifr_flags & TUN_FEATURES);\n\n\t/* Make sure persistent devices do not get stuck in\n\t * xoff state.\n\t */\n\tif (netif_running(tun->dev))\n\t\tnetif_tx_wake_all_queues(tun->dev);\n\n\tstrcpy(ifr->ifr_name, tun->dev->name);\n\treturn 0;\n\nerr_detach:\n\ttun_detach_all(dev);\n\t/* register_netdevice() already called tun_free_netdev() */\n\tgoto err_free_dev;\n\nerr_free_flow:\n\ttun_flow_uninit(tun);\n\tsecurity_tun_dev_free_security(tun->security);\nerr_free_stat:\n\tfree_percpu(tun->pcpu_stats);\nerr_free_dev:\n\tfree_netdev(dev);\n\treturn err;\n}",
        "code_after_change": "static int tun_set_iff(struct net *net, struct file *file, struct ifreq *ifr)\n{\n\tstruct tun_struct *tun;\n\tstruct tun_file *tfile = file->private_data;\n\tstruct net_device *dev;\n\tint err;\n\n\tif (tfile->detached)\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_name(net, ifr->ifr_name);\n\tif (dev) {\n\t\tif (ifr->ifr_flags & IFF_TUN_EXCL)\n\t\t\treturn -EBUSY;\n\t\tif ((ifr->ifr_flags & IFF_TUN) && dev->netdev_ops == &tun_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse if ((ifr->ifr_flags & IFF_TAP) && dev->netdev_ops == &tap_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t\tif (!!(ifr->ifr_flags & IFF_MULTI_QUEUE) !=\n\t\t    !!(tun->flags & IFF_MULTI_QUEUE))\n\t\t\treturn -EINVAL;\n\n\t\tif (tun_not_capable(tun))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_open(tun->security);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = tun_attach(tun, file, ifr->ifr_flags & IFF_NOFILTER);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tif (tun->flags & IFF_MULTI_QUEUE &&\n\t\t    (tun->numqueues + tun->numdisabled > 1)) {\n\t\t\t/* One or more queue has already been attached, no need\n\t\t\t * to initialize the device again.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\telse {\n\t\tchar *name;\n\t\tunsigned long flags = 0;\n\t\tint queues = ifr->ifr_flags & IFF_MULTI_QUEUE ?\n\t\t\t     MAX_TAP_QUEUES : 1;\n\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_create();\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\t/* Set dev type */\n\t\tif (ifr->ifr_flags & IFF_TUN) {\n\t\t\t/* TUN device */\n\t\t\tflags |= IFF_TUN;\n\t\t\tname = \"tun%d\";\n\t\t} else if (ifr->ifr_flags & IFF_TAP) {\n\t\t\t/* TAP device */\n\t\t\tflags |= IFF_TAP;\n\t\t\tname = \"tap%d\";\n\t\t} else\n\t\t\treturn -EINVAL;\n\n\t\tif (*ifr->ifr_name)\n\t\t\tname = ifr->ifr_name;\n\n\t\tdev = alloc_netdev_mqs(sizeof(struct tun_struct), name,\n\t\t\t\t       NET_NAME_UNKNOWN, tun_setup, queues,\n\t\t\t\t       queues);\n\n\t\tif (!dev)\n\t\t\treturn -ENOMEM;\n\t\terr = dev_get_valid_name(net, dev, name);\n\t\tif (err)\n\t\t\tgoto err_free_dev;\n\n\t\tdev_net_set(dev, net);\n\t\tdev->rtnl_link_ops = &tun_link_ops;\n\t\tdev->ifindex = tfile->ifindex;\n\t\tdev->sysfs_groups[0] = &tun_attr_group;\n\n\t\ttun = netdev_priv(dev);\n\t\ttun->dev = dev;\n\t\ttun->flags = flags;\n\t\ttun->txflt.count = 0;\n\t\ttun->vnet_hdr_sz = sizeof(struct virtio_net_hdr);\n\n\t\ttun->align = NET_SKB_PAD;\n\t\ttun->filter_attached = false;\n\t\ttun->sndbuf = tfile->socket.sk->sk_sndbuf;\n\t\ttun->rx_batched = 0;\n\n\t\ttun->pcpu_stats = netdev_alloc_pcpu_stats(struct tun_pcpu_stats);\n\t\tif (!tun->pcpu_stats) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_dev;\n\t\t}\n\n\t\tspin_lock_init(&tun->lock);\n\n\t\terr = security_tun_dev_alloc_security(&tun->security);\n\t\tif (err < 0)\n\t\t\tgoto err_free_stat;\n\n\t\ttun_net_init(dev);\n\t\ttun_flow_init(tun);\n\n\t\tdev->hw_features = NETIF_F_SG | NETIF_F_FRAGLIST |\n\t\t\t\t   TUN_USER_FEATURES | NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t   NETIF_F_HW_VLAN_STAG_TX;\n\t\tdev->features = dev->hw_features | NETIF_F_LLTX;\n\t\tdev->vlan_features = dev->features &\n\t\t\t\t     ~(NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t       NETIF_F_HW_VLAN_STAG_TX);\n\n\t\tINIT_LIST_HEAD(&tun->disabled);\n\t\terr = tun_attach(tun, file, false);\n\t\tif (err < 0)\n\t\t\tgoto err_free_flow;\n\n\t\terr = register_netdevice(tun->dev);\n\t\tif (err < 0)\n\t\t\tgoto err_detach;\n\t}\n\n\tnetif_carrier_on(tun->dev);\n\n\ttun_debug(KERN_INFO, tun, \"tun_set_iff\\n\");\n\n\ttun->flags = (tun->flags & ~TUN_FEATURES) |\n\t\t(ifr->ifr_flags & TUN_FEATURES);\n\n\t/* Make sure persistent devices do not get stuck in\n\t * xoff state.\n\t */\n\tif (netif_running(tun->dev))\n\t\tnetif_tx_wake_all_queues(tun->dev);\n\n\tstrcpy(ifr->ifr_name, tun->dev->name);\n\treturn 0;\n\nerr_detach:\n\ttun_detach_all(dev);\n\t/* register_netdevice() already called tun_free_netdev() */\n\tgoto err_free_dev;\n\nerr_free_flow:\n\ttun_flow_uninit(tun);\n\tsecurity_tun_dev_free_security(tun->security);\nerr_free_stat:\n\tfree_percpu(tun->pcpu_stats);\nerr_free_dev:\n\tfree_netdev(dev);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -74,6 +74,9 @@\n \n \t\tif (!dev)\n \t\t\treturn -ENOMEM;\n+\t\terr = dev_get_valid_name(net, dev, name);\n+\t\tif (err)\n+\t\t\tgoto err_free_dev;\n \n \t\tdev_net_set(dev, net);\n \t\tdev->rtnl_link_ops = &tun_link_ops;",
        "function_modified_lines": {
            "added": [
                "\t\terr = dev_get_valid_name(net, dev, name);",
                "\t\tif (err)",
                "\t\t\tgoto err_free_dev;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "In the tun subsystem in the Linux kernel before 4.13.14, dev_get_valid_name is not called before register_netdevice. This allows local users to cause a denial of service (NULL pointer dereference and panic) via an ioctl(TUNSETIFF) call with a dev name containing a / character. This is similar to CVE-2013-4343.",
        "id": 1843
    },
    {
        "cve_id": "CVE-2020-10711",
        "code_before_change": "static int cipso_v4_parsetag_rng(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tret_val = cipso_v4_map_cat_rng_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int cipso_v4_parsetag_rng(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tret_val = cipso_v4_map_cat_rng_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tif (secattr->attr.mls.cat)\n\t\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,7 +22,8 @@\n \t\t\treturn ret_val;\n \t\t}\n \n-\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n+\t\tif (secattr->attr.mls.cat)\n+\t\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n \t}\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\t\tif (secattr->attr.mls.cat)",
                "\t\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;"
            ],
            "deleted": [
                "\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A NULL pointer dereference flaw was found in the Linux kernel's SELinux subsystem in versions before 5.7. This flaw occurs while importing the Commercial IP Security Option (CIPSO) protocol's category bitmap into the SELinux extensible bitmap via the' ebitmap_netlbl_import' routine. While processing the CIPSO restricted bitmap tag in the 'cipso_v4_parsetag_rbm' routine, it sets the security attribute to indicate that the category bitmap is present, even if it has not been allocated. This issue leads to a NULL pointer dereference issue while importing the same category bitmap into SELinux. This flaw allows a remote network user to crash the system kernel, resulting in a denial of service.",
        "id": 2403
    },
    {
        "cve_id": "CVE-2020-10711",
        "code_before_change": "static int cipso_v4_parsetag_rbm(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tret_val = cipso_v4_map_cat_rbm_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int cipso_v4_parsetag_rbm(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tret_val = cipso_v4_map_cat_rbm_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tif (secattr->attr.mls.cat)\n\t\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,7 +22,8 @@\n \t\t\treturn ret_val;\n \t\t}\n \n-\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n+\t\tif (secattr->attr.mls.cat)\n+\t\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n \t}\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\t\tif (secattr->attr.mls.cat)",
                "\t\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;"
            ],
            "deleted": [
                "\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A NULL pointer dereference flaw was found in the Linux kernel's SELinux subsystem in versions before 5.7. This flaw occurs while importing the Commercial IP Security Option (CIPSO) protocol's category bitmap into the SELinux extensible bitmap via the' ebitmap_netlbl_import' routine. While processing the CIPSO restricted bitmap tag in the 'cipso_v4_parsetag_rbm' routine, it sets the security attribute to indicate that the category bitmap is present, even if it has not been allocated. This issue leads to a NULL pointer dereference issue while importing the same category bitmap into SELinux. This flaw allows a remote network user to crash the system kernel, resulting in a denial of service.",
        "id": 2404
    },
    {
        "cve_id": "CVE-2019-19036",
        "code_before_change": "static int check_leaf(struct extent_buffer *leaf, bool check_item_data)\n{\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\t/* No valid key type is 0, so all key should be larger than this key */\n\tstruct btrfs_key prev_key = {0, 0, 0};\n\tstruct btrfs_key key;\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tint slot;\n\n\tif (btrfs_header_level(leaf) != 0) {\n\t\tgeneric_err(leaf, 0,\n\t\t\t\"invalid level for leaf, have %d expect 0\",\n\t\t\tbtrfs_header_level(leaf));\n\t\treturn -EUCLEAN;\n\t}\n\n\t/*\n\t * Extent buffers from a relocation tree have a owner field that\n\t * corresponds to the subvolume tree they are based on. So just from an\n\t * extent buffer alone we can not find out what is the id of the\n\t * corresponding subvolume tree, so we can not figure out if the extent\n\t * buffer corresponds to the root of the relocation tree or not. So\n\t * skip this check for relocation trees.\n\t */\n\tif (nritems == 0 && !btrfs_header_flag(leaf, BTRFS_HEADER_FLAG_RELOC)) {\n\t\tu64 owner = btrfs_header_owner(leaf);\n\n\t\t/* These trees must never be empty */\n\t\tif (owner == BTRFS_ROOT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_CHUNK_TREE_OBJECTID ||\n\t\t    owner == BTRFS_EXTENT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DEV_TREE_OBJECTID ||\n\t\t    owner == BTRFS_FS_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\t\tgeneric_err(leaf, 0,\n\t\t\t\"invalid root, root %llu must never be empty\",\n\t\t\t\t    owner);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (nritems == 0)\n\t\treturn 0;\n\n\t/*\n\t * Check the following things to make sure this is a good leaf, and\n\t * leaf users won't need to bother with similar sanity checks:\n\t *\n\t * 1) key ordering\n\t * 2) item offset and size\n\t *    No overlap, no hole, all inside the leaf.\n\t * 3) item content\n\t *    If possible, do comprehensive sanity check.\n\t *    NOTE: All checks must only rely on the item data itself.\n\t */\n\tfor (slot = 0; slot < nritems; slot++) {\n\t\tu32 item_end_expected;\n\t\tint ret;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t/* Make sure the keys are in the right order */\n\t\tif (btrfs_comp_cpu_keys(&prev_key, &key) >= 0) {\n\t\t\tgeneric_err(leaf, slot,\n\t\"bad key order, prev (%llu %u %llu) current (%llu %u %llu)\",\n\t\t\t\tprev_key.objectid, prev_key.type,\n\t\t\t\tprev_key.offset, key.objectid, key.type,\n\t\t\t\tkey.offset);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Make sure the offset and ends are right, remember that the\n\t\t * item data starts at the end of the leaf and grows towards the\n\t\t * front.\n\t\t */\n\t\tif (slot == 0)\n\t\t\titem_end_expected = BTRFS_LEAF_DATA_SIZE(fs_info);\n\t\telse\n\t\t\titem_end_expected = btrfs_item_offset_nr(leaf,\n\t\t\t\t\t\t\t\t slot - 1);\n\t\tif (btrfs_item_end_nr(leaf, slot) != item_end_expected) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\t\t\"unexpected item end, have %u expect %u\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\titem_end_expected);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Check to make sure that we don't point outside of the leaf,\n\t\t * just in case all the items are consistent to each other, but\n\t\t * all point outside of the leaf.\n\t\t */\n\t\tif (btrfs_item_end_nr(leaf, slot) >\n\t\t    BTRFS_LEAF_DATA_SIZE(fs_info)) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\t\"slot end outside of leaf, have %u expect range [0, %u]\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\tBTRFS_LEAF_DATA_SIZE(fs_info));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/* Also check if the item pointer overlaps with btrfs item. */\n\t\tif (btrfs_item_nr_offset(slot) + sizeof(struct btrfs_item) >\n\t\t    btrfs_item_ptr_offset(leaf, slot)) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\"slot overlaps with its data, item end %lu data start %lu\",\n\t\t\t\tbtrfs_item_nr_offset(slot) +\n\t\t\t\tsizeof(struct btrfs_item),\n\t\t\t\tbtrfs_item_ptr_offset(leaf, slot));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\tif (check_item_data) {\n\t\t\t/*\n\t\t\t * Check if the item size and content meet other\n\t\t\t * criteria\n\t\t\t */\n\t\t\tret = check_leaf_item(leaf, &key, slot, &prev_key);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tprev_key.objectid = key.objectid;\n\t\tprev_key.type = key.type;\n\t\tprev_key.offset = key.offset;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int check_leaf(struct extent_buffer *leaf, bool check_item_data)\n{\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\t/* No valid key type is 0, so all key should be larger than this key */\n\tstruct btrfs_key prev_key = {0, 0, 0};\n\tstruct btrfs_key key;\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tint slot;\n\n\tif (btrfs_header_level(leaf) != 0) {\n\t\tgeneric_err(leaf, 0,\n\t\t\t\"invalid level for leaf, have %d expect 0\",\n\t\t\tbtrfs_header_level(leaf));\n\t\treturn -EUCLEAN;\n\t}\n\n\t/*\n\t * Extent buffers from a relocation tree have a owner field that\n\t * corresponds to the subvolume tree they are based on. So just from an\n\t * extent buffer alone we can not find out what is the id of the\n\t * corresponding subvolume tree, so we can not figure out if the extent\n\t * buffer corresponds to the root of the relocation tree or not. So\n\t * skip this check for relocation trees.\n\t */\n\tif (nritems == 0 && !btrfs_header_flag(leaf, BTRFS_HEADER_FLAG_RELOC)) {\n\t\tu64 owner = btrfs_header_owner(leaf);\n\n\t\t/* These trees must never be empty */\n\t\tif (owner == BTRFS_ROOT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_CHUNK_TREE_OBJECTID ||\n\t\t    owner == BTRFS_EXTENT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DEV_TREE_OBJECTID ||\n\t\t    owner == BTRFS_FS_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\t\tgeneric_err(leaf, 0,\n\t\t\t\"invalid root, root %llu must never be empty\",\n\t\t\t\t    owner);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\t\t/* Unknown tree */\n\t\tif (owner == 0) {\n\t\t\tgeneric_err(leaf, 0,\n\t\t\t\t\"invalid owner, root 0 is not defined\");\n\t\t\treturn -EUCLEAN;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (nritems == 0)\n\t\treturn 0;\n\n\t/*\n\t * Check the following things to make sure this is a good leaf, and\n\t * leaf users won't need to bother with similar sanity checks:\n\t *\n\t * 1) key ordering\n\t * 2) item offset and size\n\t *    No overlap, no hole, all inside the leaf.\n\t * 3) item content\n\t *    If possible, do comprehensive sanity check.\n\t *    NOTE: All checks must only rely on the item data itself.\n\t */\n\tfor (slot = 0; slot < nritems; slot++) {\n\t\tu32 item_end_expected;\n\t\tint ret;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t/* Make sure the keys are in the right order */\n\t\tif (btrfs_comp_cpu_keys(&prev_key, &key) >= 0) {\n\t\t\tgeneric_err(leaf, slot,\n\t\"bad key order, prev (%llu %u %llu) current (%llu %u %llu)\",\n\t\t\t\tprev_key.objectid, prev_key.type,\n\t\t\t\tprev_key.offset, key.objectid, key.type,\n\t\t\t\tkey.offset);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Make sure the offset and ends are right, remember that the\n\t\t * item data starts at the end of the leaf and grows towards the\n\t\t * front.\n\t\t */\n\t\tif (slot == 0)\n\t\t\titem_end_expected = BTRFS_LEAF_DATA_SIZE(fs_info);\n\t\telse\n\t\t\titem_end_expected = btrfs_item_offset_nr(leaf,\n\t\t\t\t\t\t\t\t slot - 1);\n\t\tif (btrfs_item_end_nr(leaf, slot) != item_end_expected) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\t\t\"unexpected item end, have %u expect %u\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\titem_end_expected);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Check to make sure that we don't point outside of the leaf,\n\t\t * just in case all the items are consistent to each other, but\n\t\t * all point outside of the leaf.\n\t\t */\n\t\tif (btrfs_item_end_nr(leaf, slot) >\n\t\t    BTRFS_LEAF_DATA_SIZE(fs_info)) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\t\"slot end outside of leaf, have %u expect range [0, %u]\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\tBTRFS_LEAF_DATA_SIZE(fs_info));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/* Also check if the item pointer overlaps with btrfs item. */\n\t\tif (btrfs_item_nr_offset(slot) + sizeof(struct btrfs_item) >\n\t\t    btrfs_item_ptr_offset(leaf, slot)) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\"slot overlaps with its data, item end %lu data start %lu\",\n\t\t\t\tbtrfs_item_nr_offset(slot) +\n\t\t\t\tsizeof(struct btrfs_item),\n\t\t\t\tbtrfs_item_ptr_offset(leaf, slot));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\tif (check_item_data) {\n\t\t\t/*\n\t\t\t * Check if the item size and content meet other\n\t\t\t * criteria\n\t\t\t */\n\t\t\tret = check_leaf_item(leaf, &key, slot, &prev_key);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tprev_key.objectid = key.objectid;\n\t\tprev_key.type = key.type;\n\t\tprev_key.offset = key.offset;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -35,6 +35,12 @@\n \t\t\tgeneric_err(leaf, 0,\n \t\t\t\"invalid root, root %llu must never be empty\",\n \t\t\t\t    owner);\n+\t\t\treturn -EUCLEAN;\n+\t\t}\n+\t\t/* Unknown tree */\n+\t\tif (owner == 0) {\n+\t\t\tgeneric_err(leaf, 0,\n+\t\t\t\t\"invalid owner, root 0 is not defined\");\n \t\t\treturn -EUCLEAN;\n \t\t}\n \t\treturn 0;",
        "function_modified_lines": {
            "added": [
                "\t\t\treturn -EUCLEAN;",
                "\t\t}",
                "\t\t/* Unknown tree */",
                "\t\tif (owner == 0) {",
                "\t\t\tgeneric_err(leaf, 0,",
                "\t\t\t\t\"invalid owner, root 0 is not defined\");"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "btrfs_root_node in fs/btrfs/ctree.c in the Linux kernel through 5.3.12 allows a NULL pointer dereference because rcu_dereference(root->node) can be zero.",
        "id": 2121
    },
    {
        "cve_id": "CVE-2018-1095",
        "code_before_change": "static int\next4_xattr_check_entries(struct ext4_xattr_entry *entry, void *end,\n\t\t\t void *value_start)\n{\n\tstruct ext4_xattr_entry *e = entry;\n\n\t/* Find the end of the names list */\n\twhile (!IS_LAST_ENTRY(e)) {\n\t\tstruct ext4_xattr_entry *next = EXT4_XATTR_NEXT(e);\n\t\tif ((void *)next >= end)\n\t\t\treturn -EFSCORRUPTED;\n\t\te = next;\n\t}\n\n\t/* Check the values */\n\twhile (!IS_LAST_ENTRY(entry)) {\n\t\tif (entry->e_value_size != 0 &&\n\t\t    entry->e_value_inum == 0) {\n\t\t\tu16 offs = le16_to_cpu(entry->e_value_offs);\n\t\t\tu32 size = le32_to_cpu(entry->e_value_size);\n\t\t\tvoid *value;\n\n\t\t\t/*\n\t\t\t * The value cannot overlap the names, and the value\n\t\t\t * with padding cannot extend beyond 'end'.  Check both\n\t\t\t * the padded and unpadded sizes, since the size may\n\t\t\t * overflow to 0 when adding padding.\n\t\t\t */\n\t\t\tif (offs > end - value_start)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\tvalue = value_start + offs;\n\t\t\tif (value < (void *)e + sizeof(u32) ||\n\t\t\t    size > end - value ||\n\t\t\t    EXT4_XATTR_SIZE(size) > end - value)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\tentry = EXT4_XATTR_NEXT(entry);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int\next4_xattr_check_entries(struct ext4_xattr_entry *entry, void *end,\n\t\t\t void *value_start)\n{\n\tstruct ext4_xattr_entry *e = entry;\n\n\t/* Find the end of the names list */\n\twhile (!IS_LAST_ENTRY(e)) {\n\t\tstruct ext4_xattr_entry *next = EXT4_XATTR_NEXT(e);\n\t\tif ((void *)next >= end)\n\t\t\treturn -EFSCORRUPTED;\n\t\te = next;\n\t}\n\n\t/* Check the values */\n\twhile (!IS_LAST_ENTRY(entry)) {\n\t\tu32 size = le32_to_cpu(entry->e_value_size);\n\n\t\tif (size > INT_MAX)\n\t\t\treturn -EFSCORRUPTED;\n\n\t\tif (size != 0 && entry->e_value_inum == 0) {\n\t\t\tu16 offs = le16_to_cpu(entry->e_value_offs);\n\t\t\tvoid *value;\n\n\t\t\t/*\n\t\t\t * The value cannot overlap the names, and the value\n\t\t\t * with padding cannot extend beyond 'end'.  Check both\n\t\t\t * the padded and unpadded sizes, since the size may\n\t\t\t * overflow to 0 when adding padding.\n\t\t\t */\n\t\t\tif (offs > end - value_start)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\tvalue = value_start + offs;\n\t\t\tif (value < (void *)e + sizeof(u32) ||\n\t\t\t    size > end - value ||\n\t\t\t    EXT4_XATTR_SIZE(size) > end - value)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\tentry = EXT4_XATTR_NEXT(entry);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,10 +14,13 @@\n \n \t/* Check the values */\n \twhile (!IS_LAST_ENTRY(entry)) {\n-\t\tif (entry->e_value_size != 0 &&\n-\t\t    entry->e_value_inum == 0) {\n+\t\tu32 size = le32_to_cpu(entry->e_value_size);\n+\n+\t\tif (size > INT_MAX)\n+\t\t\treturn -EFSCORRUPTED;\n+\n+\t\tif (size != 0 && entry->e_value_inum == 0) {\n \t\t\tu16 offs = le16_to_cpu(entry->e_value_offs);\n-\t\t\tu32 size = le32_to_cpu(entry->e_value_size);\n \t\t\tvoid *value;\n \n \t\t\t/*",
        "function_modified_lines": {
            "added": [
                "\t\tu32 size = le32_to_cpu(entry->e_value_size);",
                "",
                "\t\tif (size > INT_MAX)",
                "\t\t\treturn -EFSCORRUPTED;",
                "",
                "\t\tif (size != 0 && entry->e_value_inum == 0) {"
            ],
            "deleted": [
                "\t\tif (entry->e_value_size != 0 &&",
                "\t\t    entry->e_value_inum == 0) {",
                "\t\t\tu32 size = le32_to_cpu(entry->e_value_size);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The ext4_xattr_check_entries function in fs/ext4/xattr.c in the Linux kernel through 4.15.15 does not properly validate xattr sizes, which causes misinterpretation of a size as an error code, and consequently allows attackers to cause a denial of service (get_acl NULL pointer dereference and system crash) via a crafted ext4 image.",
        "id": 1630
    },
    {
        "cve_id": "CVE-2023-42754",
        "code_before_change": "static void ipv4_send_dest_unreach(struct sk_buff *skb)\n{\n\tstruct ip_options opt;\n\tint res;\n\n\t/* Recompile ip options since IPCB may not be valid anymore.\n\t * Also check we have a reasonable ipv4 header.\n\t */\n\tif (!pskb_network_may_pull(skb, sizeof(struct iphdr)) ||\n\t    ip_hdr(skb)->version != 4 || ip_hdr(skb)->ihl < 5)\n\t\treturn;\n\n\tmemset(&opt, 0, sizeof(opt));\n\tif (ip_hdr(skb)->ihl > 5) {\n\t\tif (!pskb_network_may_pull(skb, ip_hdr(skb)->ihl * 4))\n\t\t\treturn;\n\t\topt.optlen = ip_hdr(skb)->ihl * 4 - sizeof(struct iphdr);\n\n\t\trcu_read_lock();\n\t\tres = __ip_options_compile(dev_net(skb->dev), &opt, skb, NULL);\n\t\trcu_read_unlock();\n\n\t\tif (res)\n\t\t\treturn;\n\t}\n\t__icmp_send(skb, ICMP_DEST_UNREACH, ICMP_HOST_UNREACH, 0, &opt);\n}",
        "code_after_change": "static void ipv4_send_dest_unreach(struct sk_buff *skb)\n{\n\tstruct net_device *dev;\n\tstruct ip_options opt;\n\tint res;\n\n\t/* Recompile ip options since IPCB may not be valid anymore.\n\t * Also check we have a reasonable ipv4 header.\n\t */\n\tif (!pskb_network_may_pull(skb, sizeof(struct iphdr)) ||\n\t    ip_hdr(skb)->version != 4 || ip_hdr(skb)->ihl < 5)\n\t\treturn;\n\n\tmemset(&opt, 0, sizeof(opt));\n\tif (ip_hdr(skb)->ihl > 5) {\n\t\tif (!pskb_network_may_pull(skb, ip_hdr(skb)->ihl * 4))\n\t\t\treturn;\n\t\topt.optlen = ip_hdr(skb)->ihl * 4 - sizeof(struct iphdr);\n\n\t\trcu_read_lock();\n\t\tdev = skb->dev ? skb->dev : skb_rtable(skb)->dst.dev;\n\t\tres = __ip_options_compile(dev_net(dev), &opt, skb, NULL);\n\t\trcu_read_unlock();\n\n\t\tif (res)\n\t\t\treturn;\n\t}\n\t__icmp_send(skb, ICMP_DEST_UNREACH, ICMP_HOST_UNREACH, 0, &opt);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n static void ipv4_send_dest_unreach(struct sk_buff *skb)\n {\n+\tstruct net_device *dev;\n \tstruct ip_options opt;\n \tint res;\n \n@@ -17,7 +18,8 @@\n \t\topt.optlen = ip_hdr(skb)->ihl * 4 - sizeof(struct iphdr);\n \n \t\trcu_read_lock();\n-\t\tres = __ip_options_compile(dev_net(skb->dev), &opt, skb, NULL);\n+\t\tdev = skb->dev ? skb->dev : skb_rtable(skb)->dst.dev;\n+\t\tres = __ip_options_compile(dev_net(dev), &opt, skb, NULL);\n \t\trcu_read_unlock();\n \n \t\tif (res)",
        "function_modified_lines": {
            "added": [
                "\tstruct net_device *dev;",
                "\t\tdev = skb->dev ? skb->dev : skb_rtable(skb)->dst.dev;",
                "\t\tres = __ip_options_compile(dev_net(dev), &opt, skb, NULL);"
            ],
            "deleted": [
                "\t\tres = __ip_options_compile(dev_net(skb->dev), &opt, skb, NULL);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A NULL pointer dereference flaw was found in the Linux kernel ipv4 stack. The socket buffer (skb) was assumed to be associated with a device before calling __ip_options_compile, which is not always the case if the skb is re-routed by ipvs. This issue may allow a local user with CAP_NET_ADMIN privileges to crash the system.",
        "id": 4207
    },
    {
        "cve_id": "CVE-2018-1000200",
        "code_before_change": "void exit_mmap(struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tunsigned long nr_accounted = 0;\n\n\t/* mm's last user has gone, and its about to be pulled down */\n\tmmu_notifier_release(mm);\n\n\tif (mm->locked_vm) {\n\t\tvma = mm->mmap;\n\t\twhile (vma) {\n\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\tmunlock_vma_pages_all(vma);\n\t\t\tvma = vma->vm_next;\n\t\t}\n\t}\n\n\tarch_exit_mmap(mm);\n\n\tvma = mm->mmap;\n\tif (!vma)\t/* Can happen if dup_mmap() received an OOM */\n\t\treturn;\n\n\tlru_add_drain();\n\tflush_cache_mm(mm);\n\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t/* update_hiwater_rss(mm) here? but nobody should be looking */\n\t/* Use -1 here to ensure all VMAs in the mm are unmapped */\n\tunmap_vmas(&tlb, vma, 0, -1);\n\n\tif (unlikely(mm_is_oom_victim(mm))) {\n\t\t/*\n\t\t * Wait for oom_reap_task() to stop working on this\n\t\t * mm. Because MMF_OOM_SKIP is already set before\n\t\t * calling down_read(), oom_reap_task() will not run\n\t\t * on this \"mm\" post up_write().\n\t\t *\n\t\t * mm_is_oom_victim() cannot be set from under us\n\t\t * either because victim->mm is already set to NULL\n\t\t * under task_lock before calling mmput and oom_mm is\n\t\t * set not NULL by the OOM killer only if victim->mm\n\t\t * is found not NULL while holding the task_lock.\n\t\t */\n\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t}\n\tfree_pgtables(&tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);\n\ttlb_finish_mmu(&tlb, 0, -1);\n\n\t/*\n\t * Walk the list again, actually closing and freeing it,\n\t * with preemption enabled, without holding any MM locks.\n\t */\n\twhile (vma) {\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += vma_pages(vma);\n\t\tvma = remove_vma(vma);\n\t}\n\tvm_unacct_memory(nr_accounted);\n}",
        "code_after_change": "void exit_mmap(struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tunsigned long nr_accounted = 0;\n\n\t/* mm's last user has gone, and its about to be pulled down */\n\tmmu_notifier_release(mm);\n\n\tif (unlikely(mm_is_oom_victim(mm))) {\n\t\t/*\n\t\t * Manually reap the mm to free as much memory as possible.\n\t\t * Then, as the oom reaper does, set MMF_OOM_SKIP to disregard\n\t\t * this mm from further consideration.  Taking mm->mmap_sem for\n\t\t * write after setting MMF_OOM_SKIP will guarantee that the oom\n\t\t * reaper will not run on this mm again after mmap_sem is\n\t\t * dropped.\n\t\t *\n\t\t * Nothing can be holding mm->mmap_sem here and the above call\n\t\t * to mmu_notifier_release(mm) ensures mmu notifier callbacks in\n\t\t * __oom_reap_task_mm() will not block.\n\t\t *\n\t\t * This needs to be done before calling munlock_vma_pages_all(),\n\t\t * which clears VM_LOCKED, otherwise the oom reaper cannot\n\t\t * reliably test it.\n\t\t */\n\t\tmutex_lock(&oom_lock);\n\t\t__oom_reap_task_mm(mm);\n\t\tmutex_unlock(&oom_lock);\n\n\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t}\n\n\tif (mm->locked_vm) {\n\t\tvma = mm->mmap;\n\t\twhile (vma) {\n\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\tmunlock_vma_pages_all(vma);\n\t\t\tvma = vma->vm_next;\n\t\t}\n\t}\n\n\tarch_exit_mmap(mm);\n\n\tvma = mm->mmap;\n\tif (!vma)\t/* Can happen if dup_mmap() received an OOM */\n\t\treturn;\n\n\tlru_add_drain();\n\tflush_cache_mm(mm);\n\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t/* update_hiwater_rss(mm) here? but nobody should be looking */\n\t/* Use -1 here to ensure all VMAs in the mm are unmapped */\n\tunmap_vmas(&tlb, vma, 0, -1);\n\tfree_pgtables(&tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);\n\ttlb_finish_mmu(&tlb, 0, -1);\n\n\t/*\n\t * Walk the list again, actually closing and freeing it,\n\t * with preemption enabled, without holding any MM locks.\n\t */\n\twhile (vma) {\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += vma_pages(vma);\n\t\tvma = remove_vma(vma);\n\t}\n\tvm_unacct_memory(nr_accounted);\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,32 @@\n \n \t/* mm's last user has gone, and its about to be pulled down */\n \tmmu_notifier_release(mm);\n+\n+\tif (unlikely(mm_is_oom_victim(mm))) {\n+\t\t/*\n+\t\t * Manually reap the mm to free as much memory as possible.\n+\t\t * Then, as the oom reaper does, set MMF_OOM_SKIP to disregard\n+\t\t * this mm from further consideration.  Taking mm->mmap_sem for\n+\t\t * write after setting MMF_OOM_SKIP will guarantee that the oom\n+\t\t * reaper will not run on this mm again after mmap_sem is\n+\t\t * dropped.\n+\t\t *\n+\t\t * Nothing can be holding mm->mmap_sem here and the above call\n+\t\t * to mmu_notifier_release(mm) ensures mmu notifier callbacks in\n+\t\t * __oom_reap_task_mm() will not block.\n+\t\t *\n+\t\t * This needs to be done before calling munlock_vma_pages_all(),\n+\t\t * which clears VM_LOCKED, otherwise the oom reaper cannot\n+\t\t * reliably test it.\n+\t\t */\n+\t\tmutex_lock(&oom_lock);\n+\t\t__oom_reap_task_mm(mm);\n+\t\tmutex_unlock(&oom_lock);\n+\n+\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n+\t\tdown_write(&mm->mmap_sem);\n+\t\tup_write(&mm->mmap_sem);\n+\t}\n \n \tif (mm->locked_vm) {\n \t\tvma = mm->mmap;\n@@ -28,24 +54,6 @@\n \t/* update_hiwater_rss(mm) here? but nobody should be looking */\n \t/* Use -1 here to ensure all VMAs in the mm are unmapped */\n \tunmap_vmas(&tlb, vma, 0, -1);\n-\n-\tif (unlikely(mm_is_oom_victim(mm))) {\n-\t\t/*\n-\t\t * Wait for oom_reap_task() to stop working on this\n-\t\t * mm. Because MMF_OOM_SKIP is already set before\n-\t\t * calling down_read(), oom_reap_task() will not run\n-\t\t * on this \"mm\" post up_write().\n-\t\t *\n-\t\t * mm_is_oom_victim() cannot be set from under us\n-\t\t * either because victim->mm is already set to NULL\n-\t\t * under task_lock before calling mmput and oom_mm is\n-\t\t * set not NULL by the OOM killer only if victim->mm\n-\t\t * is found not NULL while holding the task_lock.\n-\t\t */\n-\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n-\t\tdown_write(&mm->mmap_sem);\n-\t\tup_write(&mm->mmap_sem);\n-\t}\n \tfree_pgtables(&tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);\n \ttlb_finish_mmu(&tlb, 0, -1);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (unlikely(mm_is_oom_victim(mm))) {",
                "\t\t/*",
                "\t\t * Manually reap the mm to free as much memory as possible.",
                "\t\t * Then, as the oom reaper does, set MMF_OOM_SKIP to disregard",
                "\t\t * this mm from further consideration.  Taking mm->mmap_sem for",
                "\t\t * write after setting MMF_OOM_SKIP will guarantee that the oom",
                "\t\t * reaper will not run on this mm again after mmap_sem is",
                "\t\t * dropped.",
                "\t\t *",
                "\t\t * Nothing can be holding mm->mmap_sem here and the above call",
                "\t\t * to mmu_notifier_release(mm) ensures mmu notifier callbacks in",
                "\t\t * __oom_reap_task_mm() will not block.",
                "\t\t *",
                "\t\t * This needs to be done before calling munlock_vma_pages_all(),",
                "\t\t * which clears VM_LOCKED, otherwise the oom reaper cannot",
                "\t\t * reliably test it.",
                "\t\t */",
                "\t\tmutex_lock(&oom_lock);",
                "\t\t__oom_reap_task_mm(mm);",
                "\t\tmutex_unlock(&oom_lock);",
                "",
                "\t\tset_bit(MMF_OOM_SKIP, &mm->flags);",
                "\t\tdown_write(&mm->mmap_sem);",
                "\t\tup_write(&mm->mmap_sem);",
                "\t}"
            ],
            "deleted": [
                "",
                "\tif (unlikely(mm_is_oom_victim(mm))) {",
                "\t\t/*",
                "\t\t * Wait for oom_reap_task() to stop working on this",
                "\t\t * mm. Because MMF_OOM_SKIP is already set before",
                "\t\t * calling down_read(), oom_reap_task() will not run",
                "\t\t * on this \"mm\" post up_write().",
                "\t\t *",
                "\t\t * mm_is_oom_victim() cannot be set from under us",
                "\t\t * either because victim->mm is already set to NULL",
                "\t\t * under task_lock before calling mmput and oom_mm is",
                "\t\t * set not NULL by the OOM killer only if victim->mm",
                "\t\t * is found not NULL while holding the task_lock.",
                "\t\t */",
                "\t\tset_bit(MMF_OOM_SKIP, &mm->flags);",
                "\t\tdown_write(&mm->mmap_sem);",
                "\t\tup_write(&mm->mmap_sem);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The Linux Kernel versions 4.14, 4.15, and 4.16 has a null pointer dereference which can result in an out of memory (OOM) killing of large mlocked processes. The issue arises from an oom killed process's final thread calling exit_mmap(), which calls munlock_vma_pages_all() for mlocked vmas.This can happen synchronously with the oom reaper's unmap_page_range() since the vma's VM_LOCKED bit is cleared before munlocking (to determine if any other vmas share the memory and are mlocked).",
        "id": 1578
    },
    {
        "cve_id": "CVE-2018-14612",
        "code_before_change": "static int check_leaf(struct btrfs_fs_info *fs_info, struct extent_buffer *leaf,\n\t\t      bool check_item_data)\n{\n\t/* No valid key type is 0, so all key should be larger than this key */\n\tstruct btrfs_key prev_key = {0, 0, 0};\n\tstruct btrfs_key key;\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tint slot;\n\n\t/*\n\t * Extent buffers from a relocation tree have a owner field that\n\t * corresponds to the subvolume tree they are based on. So just from an\n\t * extent buffer alone we can not find out what is the id of the\n\t * corresponding subvolume tree, so we can not figure out if the extent\n\t * buffer corresponds to the root of the relocation tree or not. So\n\t * skip this check for relocation trees.\n\t */\n\tif (nritems == 0 && !btrfs_header_flag(leaf, BTRFS_HEADER_FLAG_RELOC)) {\n\t\tstruct btrfs_root *check_root;\n\n\t\tkey.objectid = btrfs_header_owner(leaf);\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\n\t\tcheck_root = btrfs_get_fs_root(fs_info, &key, false);\n\t\t/*\n\t\t * The only reason we also check NULL here is that during\n\t\t * open_ctree() some roots has not yet been set up.\n\t\t */\n\t\tif (!IS_ERR_OR_NULL(check_root)) {\n\t\t\tstruct extent_buffer *eb;\n\n\t\t\teb = btrfs_root_node(check_root);\n\t\t\t/* if leaf is the root, then it's fine */\n\t\t\tif (leaf != eb) {\n\t\t\t\tgeneric_err(fs_info, leaf, 0,\n\t\t\"invalid nritems, have %u should not be 0 for non-root leaf\",\n\t\t\t\t\tnritems);\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\treturn -EUCLEAN;\n\t\t\t}\n\t\t\tfree_extent_buffer(eb);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (nritems == 0)\n\t\treturn 0;\n\n\t/*\n\t * Check the following things to make sure this is a good leaf, and\n\t * leaf users won't need to bother with similar sanity checks:\n\t *\n\t * 1) key ordering\n\t * 2) item offset and size\n\t *    No overlap, no hole, all inside the leaf.\n\t * 3) item content\n\t *    If possible, do comprehensive sanity check.\n\t *    NOTE: All checks must only rely on the item data itself.\n\t */\n\tfor (slot = 0; slot < nritems; slot++) {\n\t\tu32 item_end_expected;\n\t\tint ret;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t/* Make sure the keys are in the right order */\n\t\tif (btrfs_comp_cpu_keys(&prev_key, &key) >= 0) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\"bad key order, prev (%llu %u %llu) current (%llu %u %llu)\",\n\t\t\t\tprev_key.objectid, prev_key.type,\n\t\t\t\tprev_key.offset, key.objectid, key.type,\n\t\t\t\tkey.offset);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Make sure the offset and ends are right, remember that the\n\t\t * item data starts at the end of the leaf and grows towards the\n\t\t * front.\n\t\t */\n\t\tif (slot == 0)\n\t\t\titem_end_expected = BTRFS_LEAF_DATA_SIZE(fs_info);\n\t\telse\n\t\t\titem_end_expected = btrfs_item_offset_nr(leaf,\n\t\t\t\t\t\t\t\t slot - 1);\n\t\tif (btrfs_item_end_nr(leaf, slot) != item_end_expected) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\t\"unexpected item end, have %u expect %u\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\titem_end_expected);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Check to make sure that we don't point outside of the leaf,\n\t\t * just in case all the items are consistent to each other, but\n\t\t * all point outside of the leaf.\n\t\t */\n\t\tif (btrfs_item_end_nr(leaf, slot) >\n\t\t    BTRFS_LEAF_DATA_SIZE(fs_info)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\"slot end outside of leaf, have %u expect range [0, %u]\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\tBTRFS_LEAF_DATA_SIZE(fs_info));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/* Also check if the item pointer overlaps with btrfs item. */\n\t\tif (btrfs_item_nr_offset(slot) + sizeof(struct btrfs_item) >\n\t\t    btrfs_item_ptr_offset(leaf, slot)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\"slot overlaps with its data, item end %lu data start %lu\",\n\t\t\t\tbtrfs_item_nr_offset(slot) +\n\t\t\t\tsizeof(struct btrfs_item),\n\t\t\t\tbtrfs_item_ptr_offset(leaf, slot));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\tif (check_item_data) {\n\t\t\t/*\n\t\t\t * Check if the item size and content meet other\n\t\t\t * criteria\n\t\t\t */\n\t\t\tret = check_leaf_item(fs_info, leaf, &key, slot);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tprev_key.objectid = key.objectid;\n\t\tprev_key.type = key.type;\n\t\tprev_key.offset = key.offset;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int check_leaf(struct btrfs_fs_info *fs_info, struct extent_buffer *leaf,\n\t\t      bool check_item_data)\n{\n\t/* No valid key type is 0, so all key should be larger than this key */\n\tstruct btrfs_key prev_key = {0, 0, 0};\n\tstruct btrfs_key key;\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tint slot;\n\n\t/*\n\t * Extent buffers from a relocation tree have a owner field that\n\t * corresponds to the subvolume tree they are based on. So just from an\n\t * extent buffer alone we can not find out what is the id of the\n\t * corresponding subvolume tree, so we can not figure out if the extent\n\t * buffer corresponds to the root of the relocation tree or not. So\n\t * skip this check for relocation trees.\n\t */\n\tif (nritems == 0 && !btrfs_header_flag(leaf, BTRFS_HEADER_FLAG_RELOC)) {\n\t\tu64 owner = btrfs_header_owner(leaf);\n\t\tstruct btrfs_root *check_root;\n\n\t\t/* These trees must never be empty */\n\t\tif (owner == BTRFS_ROOT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_CHUNK_TREE_OBJECTID ||\n\t\t    owner == BTRFS_EXTENT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DEV_TREE_OBJECTID ||\n\t\t    owner == BTRFS_FS_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\t\tgeneric_err(fs_info, leaf, 0,\n\t\t\t\"invalid root, root %llu must never be empty\",\n\t\t\t\t    owner);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\t\tkey.objectid = owner;\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\n\t\tcheck_root = btrfs_get_fs_root(fs_info, &key, false);\n\t\t/*\n\t\t * The only reason we also check NULL here is that during\n\t\t * open_ctree() some roots has not yet been set up.\n\t\t */\n\t\tif (!IS_ERR_OR_NULL(check_root)) {\n\t\t\tstruct extent_buffer *eb;\n\n\t\t\teb = btrfs_root_node(check_root);\n\t\t\t/* if leaf is the root, then it's fine */\n\t\t\tif (leaf != eb) {\n\t\t\t\tgeneric_err(fs_info, leaf, 0,\n\t\t\"invalid nritems, have %u should not be 0 for non-root leaf\",\n\t\t\t\t\tnritems);\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\treturn -EUCLEAN;\n\t\t\t}\n\t\t\tfree_extent_buffer(eb);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (nritems == 0)\n\t\treturn 0;\n\n\t/*\n\t * Check the following things to make sure this is a good leaf, and\n\t * leaf users won't need to bother with similar sanity checks:\n\t *\n\t * 1) key ordering\n\t * 2) item offset and size\n\t *    No overlap, no hole, all inside the leaf.\n\t * 3) item content\n\t *    If possible, do comprehensive sanity check.\n\t *    NOTE: All checks must only rely on the item data itself.\n\t */\n\tfor (slot = 0; slot < nritems; slot++) {\n\t\tu32 item_end_expected;\n\t\tint ret;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t/* Make sure the keys are in the right order */\n\t\tif (btrfs_comp_cpu_keys(&prev_key, &key) >= 0) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\"bad key order, prev (%llu %u %llu) current (%llu %u %llu)\",\n\t\t\t\tprev_key.objectid, prev_key.type,\n\t\t\t\tprev_key.offset, key.objectid, key.type,\n\t\t\t\tkey.offset);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Make sure the offset and ends are right, remember that the\n\t\t * item data starts at the end of the leaf and grows towards the\n\t\t * front.\n\t\t */\n\t\tif (slot == 0)\n\t\t\titem_end_expected = BTRFS_LEAF_DATA_SIZE(fs_info);\n\t\telse\n\t\t\titem_end_expected = btrfs_item_offset_nr(leaf,\n\t\t\t\t\t\t\t\t slot - 1);\n\t\tif (btrfs_item_end_nr(leaf, slot) != item_end_expected) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\t\"unexpected item end, have %u expect %u\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\titem_end_expected);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Check to make sure that we don't point outside of the leaf,\n\t\t * just in case all the items are consistent to each other, but\n\t\t * all point outside of the leaf.\n\t\t */\n\t\tif (btrfs_item_end_nr(leaf, slot) >\n\t\t    BTRFS_LEAF_DATA_SIZE(fs_info)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\"slot end outside of leaf, have %u expect range [0, %u]\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\tBTRFS_LEAF_DATA_SIZE(fs_info));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/* Also check if the item pointer overlaps with btrfs item. */\n\t\tif (btrfs_item_nr_offset(slot) + sizeof(struct btrfs_item) >\n\t\t    btrfs_item_ptr_offset(leaf, slot)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\"slot overlaps with its data, item end %lu data start %lu\",\n\t\t\t\tbtrfs_item_nr_offset(slot) +\n\t\t\t\tsizeof(struct btrfs_item),\n\t\t\t\tbtrfs_item_ptr_offset(leaf, slot));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\tif (check_item_data) {\n\t\t\t/*\n\t\t\t * Check if the item size and content meet other\n\t\t\t * criteria\n\t\t\t */\n\t\t\tret = check_leaf_item(fs_info, leaf, &key, slot);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tprev_key.objectid = key.objectid;\n\t\tprev_key.type = key.type;\n\t\tprev_key.offset = key.offset;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,9 +16,22 @@\n \t * skip this check for relocation trees.\n \t */\n \tif (nritems == 0 && !btrfs_header_flag(leaf, BTRFS_HEADER_FLAG_RELOC)) {\n+\t\tu64 owner = btrfs_header_owner(leaf);\n \t\tstruct btrfs_root *check_root;\n \n-\t\tkey.objectid = btrfs_header_owner(leaf);\n+\t\t/* These trees must never be empty */\n+\t\tif (owner == BTRFS_ROOT_TREE_OBJECTID ||\n+\t\t    owner == BTRFS_CHUNK_TREE_OBJECTID ||\n+\t\t    owner == BTRFS_EXTENT_TREE_OBJECTID ||\n+\t\t    owner == BTRFS_DEV_TREE_OBJECTID ||\n+\t\t    owner == BTRFS_FS_TREE_OBJECTID ||\n+\t\t    owner == BTRFS_DATA_RELOC_TREE_OBJECTID) {\n+\t\t\tgeneric_err(fs_info, leaf, 0,\n+\t\t\t\"invalid root, root %llu must never be empty\",\n+\t\t\t\t    owner);\n+\t\t\treturn -EUCLEAN;\n+\t\t}\n+\t\tkey.objectid = owner;\n \t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n \t\tkey.offset = (u64)-1;\n ",
        "function_modified_lines": {
            "added": [
                "\t\tu64 owner = btrfs_header_owner(leaf);",
                "\t\t/* These trees must never be empty */",
                "\t\tif (owner == BTRFS_ROOT_TREE_OBJECTID ||",
                "\t\t    owner == BTRFS_CHUNK_TREE_OBJECTID ||",
                "\t\t    owner == BTRFS_EXTENT_TREE_OBJECTID ||",
                "\t\t    owner == BTRFS_DEV_TREE_OBJECTID ||",
                "\t\t    owner == BTRFS_FS_TREE_OBJECTID ||",
                "\t\t    owner == BTRFS_DATA_RELOC_TREE_OBJECTID) {",
                "\t\t\tgeneric_err(fs_info, leaf, 0,",
                "\t\t\t\"invalid root, root %llu must never be empty\",",
                "\t\t\t\t    owner);",
                "\t\t\treturn -EUCLEAN;",
                "\t\t}",
                "\t\tkey.objectid = owner;"
            ],
            "deleted": [
                "\t\tkey.objectid = btrfs_header_owner(leaf);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 4.17.10. There is an invalid pointer dereference in btrfs_root_node() when mounting a crafted btrfs image, because of a lack of chunk block group mapping validation in btrfs_read_block_groups in fs/btrfs/extent-tree.c, and a lack of empty-tree checks in check_leaf in fs/btrfs/tree-checker.c.",
        "id": 1681
    },
    {
        "cve_id": "CVE-2019-19462",
        "code_before_change": "struct rchan *relay_open(const char *base_filename,\n\t\t\t struct dentry *parent,\n\t\t\t size_t subbuf_size,\n\t\t\t size_t n_subbufs,\n\t\t\t struct rchan_callbacks *cb,\n\t\t\t void *private_data)\n{\n\tunsigned int i;\n\tstruct rchan *chan;\n\tstruct rchan_buf *buf;\n\n\tif (!(subbuf_size && n_subbufs))\n\t\treturn NULL;\n\tif (subbuf_size > UINT_MAX / n_subbufs)\n\t\treturn NULL;\n\n\tchan = kzalloc(sizeof(struct rchan), GFP_KERNEL);\n\tif (!chan)\n\t\treturn NULL;\n\n\tchan->buf = alloc_percpu(struct rchan_buf *);\n\tchan->version = RELAYFS_CHANNEL_VERSION;\n\tchan->n_subbufs = n_subbufs;\n\tchan->subbuf_size = subbuf_size;\n\tchan->alloc_size = PAGE_ALIGN(subbuf_size * n_subbufs);\n\tchan->parent = parent;\n\tchan->private_data = private_data;\n\tif (base_filename) {\n\t\tchan->has_base_filename = 1;\n\t\tstrlcpy(chan->base_filename, base_filename, NAME_MAX);\n\t}\n\tsetup_callbacks(chan, cb);\n\tkref_init(&chan->kref);\n\n\tmutex_lock(&relay_channels_mutex);\n\tfor_each_online_cpu(i) {\n\t\tbuf = relay_open_buf(chan, i);\n\t\tif (!buf)\n\t\t\tgoto free_bufs;\n\t\t*per_cpu_ptr(chan->buf, i) = buf;\n\t}\n\tlist_add(&chan->list, &relay_channels);\n\tmutex_unlock(&relay_channels_mutex);\n\n\treturn chan;\n\nfree_bufs:\n\tfor_each_possible_cpu(i) {\n\t\tif ((buf = *per_cpu_ptr(chan->buf, i)))\n\t\t\trelay_close_buf(buf);\n\t}\n\n\tkref_put(&chan->kref, relay_destroy_channel);\n\tmutex_unlock(&relay_channels_mutex);\n\treturn NULL;\n}",
        "code_after_change": "struct rchan *relay_open(const char *base_filename,\n\t\t\t struct dentry *parent,\n\t\t\t size_t subbuf_size,\n\t\t\t size_t n_subbufs,\n\t\t\t struct rchan_callbacks *cb,\n\t\t\t void *private_data)\n{\n\tunsigned int i;\n\tstruct rchan *chan;\n\tstruct rchan_buf *buf;\n\n\tif (!(subbuf_size && n_subbufs))\n\t\treturn NULL;\n\tif (subbuf_size > UINT_MAX / n_subbufs)\n\t\treturn NULL;\n\n\tchan = kzalloc(sizeof(struct rchan), GFP_KERNEL);\n\tif (!chan)\n\t\treturn NULL;\n\n\tchan->buf = alloc_percpu(struct rchan_buf *);\n\tif (!chan->buf) {\n\t\tkfree(chan);\n\t\treturn NULL;\n\t}\n\n\tchan->version = RELAYFS_CHANNEL_VERSION;\n\tchan->n_subbufs = n_subbufs;\n\tchan->subbuf_size = subbuf_size;\n\tchan->alloc_size = PAGE_ALIGN(subbuf_size * n_subbufs);\n\tchan->parent = parent;\n\tchan->private_data = private_data;\n\tif (base_filename) {\n\t\tchan->has_base_filename = 1;\n\t\tstrlcpy(chan->base_filename, base_filename, NAME_MAX);\n\t}\n\tsetup_callbacks(chan, cb);\n\tkref_init(&chan->kref);\n\n\tmutex_lock(&relay_channels_mutex);\n\tfor_each_online_cpu(i) {\n\t\tbuf = relay_open_buf(chan, i);\n\t\tif (!buf)\n\t\t\tgoto free_bufs;\n\t\t*per_cpu_ptr(chan->buf, i) = buf;\n\t}\n\tlist_add(&chan->list, &relay_channels);\n\tmutex_unlock(&relay_channels_mutex);\n\n\treturn chan;\n\nfree_bufs:\n\tfor_each_possible_cpu(i) {\n\t\tif ((buf = *per_cpu_ptr(chan->buf, i)))\n\t\t\trelay_close_buf(buf);\n\t}\n\n\tkref_put(&chan->kref, relay_destroy_channel);\n\tmutex_unlock(&relay_channels_mutex);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,6 +19,11 @@\n \t\treturn NULL;\n \n \tchan->buf = alloc_percpu(struct rchan_buf *);\n+\tif (!chan->buf) {\n+\t\tkfree(chan);\n+\t\treturn NULL;\n+\t}\n+\n \tchan->version = RELAYFS_CHANNEL_VERSION;\n \tchan->n_subbufs = n_subbufs;\n \tchan->subbuf_size = subbuf_size;",
        "function_modified_lines": {
            "added": [
                "\tif (!chan->buf) {",
                "\t\tkfree(chan);",
                "\t\treturn NULL;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "relay_open in kernel/relay.c in the Linux kernel through 5.4.1 allows local users to cause a denial of service (such as relay blockage) by triggering a NULL alloc_percpu result.",
        "id": 2197
    },
    {
        "cve_id": "CVE-2023-28327",
        "code_before_change": "static int unix_diag_get_exact(struct sk_buff *in_skb,\n\t\t\t       const struct nlmsghdr *nlh,\n\t\t\t       struct unix_diag_req *req)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tunsigned int extra_len;\n\tstruct sk_buff *rep;\n\tstruct sock *sk;\n\tint err;\n\n\terr = -EINVAL;\n\tif (req->udiag_ino == 0)\n\t\tgoto out_nosk;\n\n\tsk = unix_lookup_by_ino(net, req->udiag_ino);\n\terr = -ENOENT;\n\tif (sk == NULL)\n\t\tgoto out_nosk;\n\n\terr = sock_diag_check_cookie(sk, req->udiag_cookie);\n\tif (err)\n\t\tgoto out;\n\n\textra_len = 256;\nagain:\n\terr = -ENOMEM;\n\trep = nlmsg_new(sizeof(struct unix_diag_msg) + extra_len, GFP_KERNEL);\n\tif (!rep)\n\t\tgoto out;\n\n\terr = sk_diag_fill(sk, rep, req, NETLINK_CB(in_skb).portid,\n\t\t\t   nlh->nlmsg_seq, 0, req->udiag_ino);\n\tif (err < 0) {\n\t\tnlmsg_free(rep);\n\t\textra_len += 256;\n\t\tif (extra_len >= PAGE_SIZE)\n\t\t\tgoto out;\n\n\t\tgoto again;\n\t}\n\terr = nlmsg_unicast(net->diag_nlsk, rep, NETLINK_CB(in_skb).portid);\n\nout:\n\tif (sk)\n\t\tsock_put(sk);\nout_nosk:\n\treturn err;\n}",
        "code_after_change": "static int unix_diag_get_exact(struct sk_buff *in_skb,\n\t\t\t       const struct nlmsghdr *nlh,\n\t\t\t       struct unix_diag_req *req)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tunsigned int extra_len;\n\tstruct sk_buff *rep;\n\tstruct sock *sk;\n\tint err;\n\n\terr = -EINVAL;\n\tif (req->udiag_ino == 0)\n\t\tgoto out_nosk;\n\n\tsk = unix_lookup_by_ino(net, req->udiag_ino);\n\terr = -ENOENT;\n\tif (sk == NULL)\n\t\tgoto out_nosk;\n\n\terr = sock_diag_check_cookie(sk, req->udiag_cookie);\n\tif (err)\n\t\tgoto out;\n\n\textra_len = 256;\nagain:\n\terr = -ENOMEM;\n\trep = nlmsg_new(sizeof(struct unix_diag_msg) + extra_len, GFP_KERNEL);\n\tif (!rep)\n\t\tgoto out;\n\n\terr = sk_diag_fill(sk, rep, req, sk_user_ns(NETLINK_CB(in_skb).sk),\n\t\t\t   NETLINK_CB(in_skb).portid,\n\t\t\t   nlh->nlmsg_seq, 0, req->udiag_ino);\n\tif (err < 0) {\n\t\tnlmsg_free(rep);\n\t\textra_len += 256;\n\t\tif (extra_len >= PAGE_SIZE)\n\t\t\tgoto out;\n\n\t\tgoto again;\n\t}\n\terr = nlmsg_unicast(net->diag_nlsk, rep, NETLINK_CB(in_skb).portid);\n\nout:\n\tif (sk)\n\t\tsock_put(sk);\nout_nosk:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,8 @@\n \tif (!rep)\n \t\tgoto out;\n \n-\terr = sk_diag_fill(sk, rep, req, NETLINK_CB(in_skb).portid,\n+\terr = sk_diag_fill(sk, rep, req, sk_user_ns(NETLINK_CB(in_skb).sk),\n+\t\t\t   NETLINK_CB(in_skb).portid,\n \t\t\t   nlh->nlmsg_seq, 0, req->udiag_ino);\n \tif (err < 0) {\n \t\tnlmsg_free(rep);",
        "function_modified_lines": {
            "added": [
                "\terr = sk_diag_fill(sk, rep, req, sk_user_ns(NETLINK_CB(in_skb).sk),",
                "\t\t\t   NETLINK_CB(in_skb).portid,"
            ],
            "deleted": [
                "\terr = sk_diag_fill(sk, rep, req, NETLINK_CB(in_skb).portid,"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A NULL pointer dereference flaw was found in the UNIX protocol in net/unix/diag.c In unix_diag_get_exact in the Linux Kernel. The newly allocated skb does not have sk, leading to a NULL pointer. This flaw allows a local user to crash or potentially cause a denial of service.",
        "id": 3975
    },
    {
        "cve_id": "CVE-2022-40476",
        "code_before_change": "static inline void io_req_track_inflight(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_INFLIGHT)) {\n\t\treq->flags |= REQ_F_INFLIGHT;\n\t\tatomic_inc(&current->io_uring->inflight_tracked);\n\t}\n}",
        "code_after_change": "static inline void io_req_track_inflight(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_INFLIGHT)) {\n\t\treq->flags |= REQ_F_INFLIGHT;\n\t\tatomic_inc(&req->task->io_uring->inflight_tracked);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,6 @@\n {\n \tif (!(req->flags & REQ_F_INFLIGHT)) {\n \t\treq->flags |= REQ_F_INFLIGHT;\n-\t\tatomic_inc(&current->io_uring->inflight_tracked);\n+\t\tatomic_inc(&req->task->io_uring->inflight_tracked);\n \t}\n }",
        "function_modified_lines": {
            "added": [
                "\t\tatomic_inc(&req->task->io_uring->inflight_tracked);"
            ],
            "deleted": [
                "\t\tatomic_inc(&current->io_uring->inflight_tracked);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "A null pointer dereference issue was discovered in fs/io_uring.c in the Linux kernel before 5.15.62. A local user could use this flaw to crash the system or potentially cause a denial of service.",
        "id": 3704
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "int get_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -1;\n\tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n\t\treturn -1;\n\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];\n}",
        "code_after_change": "int get_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -1;\n\tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n\t\treturn -1;\n\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,5 +4,5 @@\n \t\treturn -1;\n \tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n \t\treturn -1;\n-\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];\n+\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);"
            ],
            "deleted": [
                "\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "id": 2621
    },
    {
        "cve_id": "CVE-2016-2782",
        "code_before_change": "static int treo_attach(struct usb_serial *serial)\n{\n\tstruct usb_serial_port *swap_port;\n\n\t/* Only do this endpoint hack for the Handspring devices with\n\t * interrupt in endpoints, which for now are the Treo devices. */\n\tif (!((le16_to_cpu(serial->dev->descriptor.idVendor)\n\t\t\t\t\t\t== HANDSPRING_VENDOR_ID) ||\n\t\t(le16_to_cpu(serial->dev->descriptor.idVendor)\n\t\t\t\t\t\t== KYOCERA_VENDOR_ID)) ||\n\t\t(serial->num_interrupt_in == 0))\n\t\treturn 0;\n\n\t/*\n\t* It appears that Treos and Kyoceras want to use the\n\t* 1st bulk in endpoint to communicate with the 2nd bulk out endpoint,\n\t* so let's swap the 1st and 2nd bulk in and interrupt endpoints.\n\t* Note that swapping the bulk out endpoints would break lots of\n\t* apps that want to communicate on the second port.\n\t*/\n#define COPY_PORT(dest, src)\t\t\t\t\t\t\\\n\tdo { \\\n\t\tint i;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tfor (i = 0; i < ARRAY_SIZE(src->read_urbs); ++i) {\t\\\n\t\t\tdest->read_urbs[i] = src->read_urbs[i];\t\t\\\n\t\t\tdest->read_urbs[i]->context = dest;\t\t\\\n\t\t\tdest->bulk_in_buffers[i] = src->bulk_in_buffers[i]; \\\n\t\t}\t\t\t\t\t\t\t\\\n\t\tdest->read_urb = src->read_urb;\t\t\t\t\\\n\t\tdest->bulk_in_endpointAddress = src->bulk_in_endpointAddress;\\\n\t\tdest->bulk_in_buffer = src->bulk_in_buffer;\t\t\\\n\t\tdest->bulk_in_size = src->bulk_in_size;\t\t\t\\\n\t\tdest->interrupt_in_urb = src->interrupt_in_urb;\t\t\\\n\t\tdest->interrupt_in_urb->context = dest;\t\t\t\\\n\t\tdest->interrupt_in_endpointAddress = \\\n\t\t\t\t\tsrc->interrupt_in_endpointAddress;\\\n\t\tdest->interrupt_in_buffer = src->interrupt_in_buffer;\t\\\n\t} while (0);\n\n\tswap_port = kmalloc(sizeof(*swap_port), GFP_KERNEL);\n\tif (!swap_port)\n\t\treturn -ENOMEM;\n\tCOPY_PORT(swap_port, serial->port[0]);\n\tCOPY_PORT(serial->port[0], serial->port[1]);\n\tCOPY_PORT(serial->port[1], swap_port);\n\tkfree(swap_port);\n\n\treturn 0;\n}",
        "code_after_change": "static int treo_attach(struct usb_serial *serial)\n{\n\tstruct usb_serial_port *swap_port;\n\n\t/* Only do this endpoint hack for the Handspring devices with\n\t * interrupt in endpoints, which for now are the Treo devices. */\n\tif (!((le16_to_cpu(serial->dev->descriptor.idVendor)\n\t\t\t\t\t\t== HANDSPRING_VENDOR_ID) ||\n\t\t(le16_to_cpu(serial->dev->descriptor.idVendor)\n\t\t\t\t\t\t== KYOCERA_VENDOR_ID)) ||\n\t\t(serial->num_interrupt_in == 0))\n\t\treturn 0;\n\n\tif (serial->num_bulk_in < 2 || serial->num_interrupt_in < 2) {\n\t\tdev_err(&serial->interface->dev, \"missing endpoints\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\t/*\n\t* It appears that Treos and Kyoceras want to use the\n\t* 1st bulk in endpoint to communicate with the 2nd bulk out endpoint,\n\t* so let's swap the 1st and 2nd bulk in and interrupt endpoints.\n\t* Note that swapping the bulk out endpoints would break lots of\n\t* apps that want to communicate on the second port.\n\t*/\n#define COPY_PORT(dest, src)\t\t\t\t\t\t\\\n\tdo { \\\n\t\tint i;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tfor (i = 0; i < ARRAY_SIZE(src->read_urbs); ++i) {\t\\\n\t\t\tdest->read_urbs[i] = src->read_urbs[i];\t\t\\\n\t\t\tdest->read_urbs[i]->context = dest;\t\t\\\n\t\t\tdest->bulk_in_buffers[i] = src->bulk_in_buffers[i]; \\\n\t\t}\t\t\t\t\t\t\t\\\n\t\tdest->read_urb = src->read_urb;\t\t\t\t\\\n\t\tdest->bulk_in_endpointAddress = src->bulk_in_endpointAddress;\\\n\t\tdest->bulk_in_buffer = src->bulk_in_buffer;\t\t\\\n\t\tdest->bulk_in_size = src->bulk_in_size;\t\t\t\\\n\t\tdest->interrupt_in_urb = src->interrupt_in_urb;\t\t\\\n\t\tdest->interrupt_in_urb->context = dest;\t\t\t\\\n\t\tdest->interrupt_in_endpointAddress = \\\n\t\t\t\t\tsrc->interrupt_in_endpointAddress;\\\n\t\tdest->interrupt_in_buffer = src->interrupt_in_buffer;\t\\\n\t} while (0);\n\n\tswap_port = kmalloc(sizeof(*swap_port), GFP_KERNEL);\n\tif (!swap_port)\n\t\treturn -ENOMEM;\n\tCOPY_PORT(swap_port, serial->port[0]);\n\tCOPY_PORT(serial->port[0], serial->port[1]);\n\tCOPY_PORT(serial->port[1], swap_port);\n\tkfree(swap_port);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,11 @@\n \t\t\t\t\t\t== KYOCERA_VENDOR_ID)) ||\n \t\t(serial->num_interrupt_in == 0))\n \t\treturn 0;\n+\n+\tif (serial->num_bulk_in < 2 || serial->num_interrupt_in < 2) {\n+\t\tdev_err(&serial->interface->dev, \"missing endpoints\\n\");\n+\t\treturn -ENODEV;\n+\t}\n \n \t/*\n \t* It appears that Treos and Kyoceras want to use the",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (serial->num_bulk_in < 2 || serial->num_interrupt_in < 2) {",
                "\t\tdev_err(&serial->interface->dev, \"missing endpoints\\n\");",
                "\t\treturn -ENODEV;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "The treo_attach function in drivers/usb/serial/visor.c in the Linux kernel before 4.5 allows physically proximate attackers to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by inserting a USB device that lacks a (1) bulk-in or (2) interrupt-in endpoint.",
        "id": 955
    },
    {
        "cve_id": "CVE-2017-16646",
        "code_before_change": "static int stk7070p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7070p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7070p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
        "code_after_change": "static int stk7070p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7070p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7070p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,7 @@\n \t\t\t\t     &dib7070p_dib7000p_config) != 0) {\n \t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n \t\t    __func__);\n-\t\tdvb_detach(&state->dib7000p_ops);\n+\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n \t\treturn -ENODEV;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);"
            ],
            "deleted": [
                "\t\tdvb_detach(&state->dib7000p_ops);"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "drivers/media/usb/dvb-usb/dib0700_devices.c in the Linux kernel through 4.13.11 allows local users to cause a denial of service (BUG and system crash) or possibly have unspecified other impact via a crafted USB device.",
        "id": 1329
    },
    {
        "cve_id": "CVE-2017-18241",
        "code_before_change": "int create_flush_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct flush_cmd_control *fcc;\n\tint err = 0;\n\n\tif (SM_I(sbi)->fcc_info) {\n\t\tfcc = SM_I(sbi)->fcc_info;\n\t\tgoto init_thread;\n\t}\n\n\tfcc = kzalloc(sizeof(struct flush_cmd_control), GFP_KERNEL);\n\tif (!fcc)\n\t\treturn -ENOMEM;\n\tatomic_set(&fcc->issued_flush, 0);\n\tatomic_set(&fcc->issing_flush, 0);\n\tinit_waitqueue_head(&fcc->flush_wait_queue);\n\tinit_llist_head(&fcc->issue_list);\n\tSM_I(sbi)->fcc_info = fcc;\ninit_thread:\n\tfcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,\n\t\t\t\t\"f2fs_flush-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(fcc->f2fs_issue_flush)) {\n\t\terr = PTR_ERR(fcc->f2fs_issue_flush);\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}",
        "code_after_change": "int create_flush_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct flush_cmd_control *fcc;\n\tint err = 0;\n\n\tif (SM_I(sbi)->fcc_info) {\n\t\tfcc = SM_I(sbi)->fcc_info;\n\t\tgoto init_thread;\n\t}\n\n\tfcc = kzalloc(sizeof(struct flush_cmd_control), GFP_KERNEL);\n\tif (!fcc)\n\t\treturn -ENOMEM;\n\tatomic_set(&fcc->issued_flush, 0);\n\tatomic_set(&fcc->issing_flush, 0);\n\tinit_waitqueue_head(&fcc->flush_wait_queue);\n\tinit_llist_head(&fcc->issue_list);\n\tSM_I(sbi)->fcc_info = fcc;\n\tif (!test_opt(sbi, FLUSH_MERGE))\n\t\treturn err;\n\ninit_thread:\n\tfcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,\n\t\t\t\t\"f2fs_flush-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(fcc->f2fs_issue_flush)) {\n\t\terr = PTR_ERR(fcc->f2fs_issue_flush);\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,9 @@\n \tinit_waitqueue_head(&fcc->flush_wait_queue);\n \tinit_llist_head(&fcc->issue_list);\n \tSM_I(sbi)->fcc_info = fcc;\n+\tif (!test_opt(sbi, FLUSH_MERGE))\n+\t\treturn err;\n+\n init_thread:\n \tfcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,\n \t\t\t\t\"f2fs_flush-%u:%u\", MAJOR(dev), MINOR(dev));",
        "function_modified_lines": {
            "added": [
                "\tif (!test_opt(sbi, FLUSH_MERGE))",
                "\t\treturn err;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "fs/f2fs/segment.c in the Linux kernel before 4.13 allows local users to cause a denial of service (NULL pointer dereference and panic) by using a noflush_merge option that triggers a NULL value for a flush_cmd_control data structure.",
        "id": 1423
    },
    {
        "cve_id": "CVE-2017-15274",
        "code_before_change": "long keyctl_update_key(key_serial_t id,\n\t\t       const void __user *_payload,\n\t\t       size_t plen)\n{\n\tkey_ref_t key_ref;\n\tvoid *payload;\n\tlong ret;\n\n\tret = -EINVAL;\n\tif (plen > PAGE_SIZE)\n\t\tgoto error;\n\n\t/* pull the payload in if one was supplied */\n\tpayload = NULL;\n\tif (_payload) {\n\t\tret = -ENOMEM;\n\t\tpayload = kmalloc(plen, GFP_KERNEL);\n\t\tif (!payload)\n\t\t\tgoto error;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(payload, _payload, plen) != 0)\n\t\t\tgoto error2;\n\t}\n\n\t/* find the target key (which must be writable) */\n\tkey_ref = lookup_user_key(id, 0, KEY_NEED_WRITE);\n\tif (IS_ERR(key_ref)) {\n\t\tret = PTR_ERR(key_ref);\n\t\tgoto error2;\n\t}\n\n\t/* update the key */\n\tret = key_update(key_ref, payload, plen);\n\n\tkey_ref_put(key_ref);\nerror2:\n\tkfree(payload);\nerror:\n\treturn ret;\n}",
        "code_after_change": "long keyctl_update_key(key_serial_t id,\n\t\t       const void __user *_payload,\n\t\t       size_t plen)\n{\n\tkey_ref_t key_ref;\n\tvoid *payload;\n\tlong ret;\n\n\tret = -EINVAL;\n\tif (plen > PAGE_SIZE)\n\t\tgoto error;\n\n\t/* pull the payload in if one was supplied */\n\tpayload = NULL;\n\tif (plen) {\n\t\tret = -ENOMEM;\n\t\tpayload = kmalloc(plen, GFP_KERNEL);\n\t\tif (!payload)\n\t\t\tgoto error;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(payload, _payload, plen) != 0)\n\t\t\tgoto error2;\n\t}\n\n\t/* find the target key (which must be writable) */\n\tkey_ref = lookup_user_key(id, 0, KEY_NEED_WRITE);\n\tif (IS_ERR(key_ref)) {\n\t\tret = PTR_ERR(key_ref);\n\t\tgoto error2;\n\t}\n\n\t/* update the key */\n\tret = key_update(key_ref, payload, plen);\n\n\tkey_ref_put(key_ref);\nerror2:\n\tkfree(payload);\nerror:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,7 @@\n \n \t/* pull the payload in if one was supplied */\n \tpayload = NULL;\n-\tif (_payload) {\n+\tif (plen) {\n \t\tret = -ENOMEM;\n \t\tpayload = kmalloc(plen, GFP_KERNEL);\n \t\tif (!payload)",
        "function_modified_lines": {
            "added": [
                "\tif (plen) {"
            ],
            "deleted": [
                "\tif (_payload) {"
            ]
        },
        "cwe": [
            "CWE-476"
        ],
        "cve_description": "security/keys/keyctl.c in the Linux kernel before 4.11.5 does not consider the case of a NULL payload in conjunction with a nonzero length value, which allows local users to cause a denial of service (NULL pointer dereference and OOPS) via a crafted add_key or keyctl system call, a different vulnerability than CVE-2017-12192.",
        "id": 1302
    }
]