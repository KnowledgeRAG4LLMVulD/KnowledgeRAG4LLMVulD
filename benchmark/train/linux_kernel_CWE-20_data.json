[
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int netlink_recvmsg(struct kiocb *kiocb, struct socket *sock,\n\t\t\t   struct msghdr *msg, size_t len,\n\t\t\t   int flags)\n{\n\tstruct sock_iocb *siocb = kiocb_to_siocb(kiocb);\n\tstruct scm_cookie scm;\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint noblock = flags&MSG_DONTWAIT;\n\tsize_t copied;\n\tstruct sk_buff *skb, *data_skb;\n\tint err, ret;\n\n\tif (flags&MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tcopied = 0;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tdata_skb = skb;\n\n#ifdef CONFIG_COMPAT_NETLINK_MESSAGES\n\tif (unlikely(skb_shinfo(skb)->frag_list)) {\n\t\t/*\n\t\t * If this skb has a frag_list, then here that means that we\n\t\t * will have to use the frag_list skb's data for compat tasks\n\t\t * and the regular skb's data for normal (non-compat) tasks.\n\t\t *\n\t\t * If we need to send the compat skb, assign it to the\n\t\t * 'data_skb' variable so that it will be used below for data\n\t\t * copying. We keep 'skb' for everything else, including\n\t\t * freeing both later.\n\t\t */\n\t\tif (flags & MSG_CMSG_COMPAT)\n\t\t\tdata_skb = skb_shinfo(skb)->frag_list;\n\t}\n#endif\n\n\tmsg->msg_namelen = 0;\n\n\tcopied = data_skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\tskb_reset_transport_header(data_skb);\n\terr = skb_copy_datagram_iovec(data_skb, 0, msg->msg_iov, copied);\n\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_nl *addr = (struct sockaddr_nl *)msg->msg_name;\n\t\taddr->nl_family = AF_NETLINK;\n\t\taddr->nl_pad    = 0;\n\t\taddr->nl_pid\t= NETLINK_CB(skb).portid;\n\t\taddr->nl_groups\t= netlink_group_mask(NETLINK_CB(skb).dst_group);\n\t\tmsg->msg_namelen = sizeof(*addr);\n\t}\n\n\tif (nlk->flags & NETLINK_RECV_PKTINFO)\n\t\tnetlink_cmsg_recv_pktinfo(msg, skb);\n\n\tif (NULL == siocb->scm) {\n\t\tmemset(&scm, 0, sizeof(scm));\n\t\tsiocb->scm = &scm;\n\t}\n\tsiocb->scm->creds = *NETLINK_CREDS(skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = data_skb->len;\n\n\tskb_free_datagram(sk, skb);\n\n\tif (nlk->cb_running &&\n\t    atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf / 2) {\n\t\tret = netlink_dump(sk);\n\t\tif (ret) {\n\t\t\tsk->sk_err = ret;\n\t\t\tsk->sk_error_report(sk);\n\t\t}\n\t}\n\n\tscm_recv(sock, msg, siocb->scm, flags);\nout:\n\tnetlink_rcv_wake(sk);\n\treturn err ? : copied;\n}",
        "code_after_change": "static int netlink_recvmsg(struct kiocb *kiocb, struct socket *sock,\n\t\t\t   struct msghdr *msg, size_t len,\n\t\t\t   int flags)\n{\n\tstruct sock_iocb *siocb = kiocb_to_siocb(kiocb);\n\tstruct scm_cookie scm;\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint noblock = flags&MSG_DONTWAIT;\n\tsize_t copied;\n\tstruct sk_buff *skb, *data_skb;\n\tint err, ret;\n\n\tif (flags&MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tcopied = 0;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tdata_skb = skb;\n\n#ifdef CONFIG_COMPAT_NETLINK_MESSAGES\n\tif (unlikely(skb_shinfo(skb)->frag_list)) {\n\t\t/*\n\t\t * If this skb has a frag_list, then here that means that we\n\t\t * will have to use the frag_list skb's data for compat tasks\n\t\t * and the regular skb's data for normal (non-compat) tasks.\n\t\t *\n\t\t * If we need to send the compat skb, assign it to the\n\t\t * 'data_skb' variable so that it will be used below for data\n\t\t * copying. We keep 'skb' for everything else, including\n\t\t * freeing both later.\n\t\t */\n\t\tif (flags & MSG_CMSG_COMPAT)\n\t\t\tdata_skb = skb_shinfo(skb)->frag_list;\n\t}\n#endif\n\n\tcopied = data_skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\tskb_reset_transport_header(data_skb);\n\terr = skb_copy_datagram_iovec(data_skb, 0, msg->msg_iov, copied);\n\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_nl *addr = (struct sockaddr_nl *)msg->msg_name;\n\t\taddr->nl_family = AF_NETLINK;\n\t\taddr->nl_pad    = 0;\n\t\taddr->nl_pid\t= NETLINK_CB(skb).portid;\n\t\taddr->nl_groups\t= netlink_group_mask(NETLINK_CB(skb).dst_group);\n\t\tmsg->msg_namelen = sizeof(*addr);\n\t}\n\n\tif (nlk->flags & NETLINK_RECV_PKTINFO)\n\t\tnetlink_cmsg_recv_pktinfo(msg, skb);\n\n\tif (NULL == siocb->scm) {\n\t\tmemset(&scm, 0, sizeof(scm));\n\t\tsiocb->scm = &scm;\n\t}\n\tsiocb->scm->creds = *NETLINK_CREDS(skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = data_skb->len;\n\n\tskb_free_datagram(sk, skb);\n\n\tif (nlk->cb_running &&\n\t    atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf / 2) {\n\t\tret = netlink_dump(sk);\n\t\tif (ret) {\n\t\t\tsk->sk_err = ret;\n\t\t\tsk->sk_error_report(sk);\n\t\t}\n\t}\n\n\tscm_recv(sock, msg, siocb->scm, flags);\nout:\n\tnetlink_rcv_wake(sk);\n\treturn err ? : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,8 +38,6 @@\n \t\t\tdata_skb = skb_shinfo(skb)->frag_list;\n \t}\n #endif\n-\n-\tmsg->msg_namelen = 0;\n \n \tcopied = data_skb->len;\n \tif (len < copied) {",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 389
    },
    {
        "cve_id": "CVE-2017-15121",
        "code_before_change": "void truncate_pagecache_range(struct inode *inode, loff_t lstart, loff_t lend)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tloff_t unmap_start = round_up(lstart, PAGE_SIZE);\n\tloff_t unmap_end = round_down(1 + lend, PAGE_SIZE) - 1;\n\t/*\n\t * This rounding is currently just for example: unmap_mapping_range\n\t * expands its hole outwards, whereas we want it to contract the hole\n\t * inwards.  However, existing callers of truncate_pagecache_range are\n\t * doing their own page rounding first; and truncate_inode_pages_range\n\t * currently BUGs if lend is not pagealigned-1 (it handles partial\n\t * page at start of hole, but not partial page at end of hole).  Note\n\t * unmap_mapping_range allows holelen 0 for all, and we allow lend -1.\n\t */\n\n\t/*\n\t * Unlike in truncate_pagecache, unmap_mapping_range is called only\n\t * once (before truncating pagecache), and without \"even_cows\" flag:\n\t * hole-punching should not remove private COWed pages from the hole.\n\t */\n\tif ((u64)unmap_end > (u64)unmap_start)\n\t\tunmap_mapping_range(mapping, unmap_start,\n\t\t\t\t    1 + unmap_end - unmap_start, 0);\n\ttruncate_inode_pages_range(mapping, lstart, lend);\n}",
        "code_after_change": "void truncate_pagecache_range(struct inode *inode, loff_t lstart, loff_t lend)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tloff_t unmap_start = round_up(lstart, PAGE_SIZE);\n\tloff_t unmap_end = round_down(1 + lend, PAGE_SIZE) - 1;\n\t/*\n\t * This rounding is currently just for example: unmap_mapping_range\n\t * expands its hole outwards, whereas we want it to contract the hole\n\t * inwards.  However, existing callers of truncate_pagecache_range are\n\t * doing their own page rounding first.  Note that unmap_mapping_range\n\t * allows holelen 0 for all, and we allow lend -1 for end of file.\n\t */\n\n\t/*\n\t * Unlike in truncate_pagecache, unmap_mapping_range is called only\n\t * once (before truncating pagecache), and without \"even_cows\" flag:\n\t * hole-punching should not remove private COWed pages from the hole.\n\t */\n\tif ((u64)unmap_end > (u64)unmap_start)\n\t\tunmap_mapping_range(mapping, unmap_start,\n\t\t\t\t    1 + unmap_end - unmap_start, 0);\n\ttruncate_inode_pages_range(mapping, lstart, lend);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,10 +7,8 @@\n \t * This rounding is currently just for example: unmap_mapping_range\n \t * expands its hole outwards, whereas we want it to contract the hole\n \t * inwards.  However, existing callers of truncate_pagecache_range are\n-\t * doing their own page rounding first; and truncate_inode_pages_range\n-\t * currently BUGs if lend is not pagealigned-1 (it handles partial\n-\t * page at start of hole, but not partial page at end of hole).  Note\n-\t * unmap_mapping_range allows holelen 0 for all, and we allow lend -1.\n+\t * doing their own page rounding first.  Note that unmap_mapping_range\n+\t * allows holelen 0 for all, and we allow lend -1 for end of file.\n \t */\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "\t * doing their own page rounding first.  Note that unmap_mapping_range",
                "\t * allows holelen 0 for all, and we allow lend -1 for end of file."
            ],
            "deleted": [
                "\t * doing their own page rounding first; and truncate_inode_pages_range",
                "\t * currently BUGs if lend is not pagealigned-1 (it handles partial",
                "\t * page at start of hole, but not partial page at end of hole).  Note",
                "\t * unmap_mapping_range allows holelen 0 for all, and we allow lend -1."
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A non-privileged user is able to mount a fuse filesystem on RHEL 6 or 7 and crash a system if an application punches a hole in a file that does not end aligned to a page boundary.",
        "id": 1295
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static void io_req_drop_files(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\tlist_del(&req->inflight_entry);\n\tif (waitqueue_active(&ctx->inflight_wait))\n\t\twake_up(&ctx->inflight_wait);\n\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\treq->flags &= ~REQ_F_INFLIGHT;\n\treq->work.files = NULL;\n}",
        "code_after_change": "static void io_req_drop_files(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\tlist_del(&req->inflight_entry);\n\tif (waitqueue_active(&ctx->inflight_wait))\n\t\twake_up(&ctx->inflight_wait);\n\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\treq->flags &= ~REQ_F_INFLIGHT;\n\tput_files_struct(req->work.files);\n\treq->work.files = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,5 +9,6 @@\n \t\twake_up(&ctx->inflight_wait);\n \tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n \treq->flags &= ~REQ_F_INFLIGHT;\n+\tput_files_struct(req->work.files);\n \treq->work.files = NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\tput_files_struct(req->work.files);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2855
    },
    {
        "cve_id": "CVE-2017-16538",
        "code_before_change": "static int lme2510_return_status(struct dvb_usb_device *d)\n{\n\tint ret = 0;\n\tu8 *data;\n\n\tdata = kzalloc(10, GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tret |= usb_control_msg(d->udev, usb_rcvctrlpipe(d->udev, 0),\n\t\t\t0x06, 0x80, 0x0302, 0x00, data, 0x0006, 200);\n\tinfo(\"Firmware Status: %x (%x)\", ret , data[2]);\n\n\tret = (ret < 0) ? -ENODEV : data[2];\n\tkfree(data);\n\treturn ret;\n}",
        "code_after_change": "static int lme2510_return_status(struct dvb_usb_device *d)\n{\n\tint ret;\n\tu8 *data;\n\n\tdata = kzalloc(6, GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tret = usb_control_msg(d->udev, usb_rcvctrlpipe(d->udev, 0),\n\t\t\t      0x06, 0x80, 0x0302, 0x00,\n\t\t\t      data, 0x6, 200);\n\tif (ret != 6)\n\t\tret = -EINVAL;\n\telse\n\t\tret = data[2];\n\n\tinfo(\"Firmware Status: %6ph\", data);\n\n\tkfree(data);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,17 +1,22 @@\n static int lme2510_return_status(struct dvb_usb_device *d)\n {\n-\tint ret = 0;\n+\tint ret;\n \tu8 *data;\n \n-\tdata = kzalloc(10, GFP_KERNEL);\n+\tdata = kzalloc(6, GFP_KERNEL);\n \tif (!data)\n \t\treturn -ENOMEM;\n \n-\tret |= usb_control_msg(d->udev, usb_rcvctrlpipe(d->udev, 0),\n-\t\t\t0x06, 0x80, 0x0302, 0x00, data, 0x0006, 200);\n-\tinfo(\"Firmware Status: %x (%x)\", ret , data[2]);\n+\tret = usb_control_msg(d->udev, usb_rcvctrlpipe(d->udev, 0),\n+\t\t\t      0x06, 0x80, 0x0302, 0x00,\n+\t\t\t      data, 0x6, 200);\n+\tif (ret != 6)\n+\t\tret = -EINVAL;\n+\telse\n+\t\tret = data[2];\n \n-\tret = (ret < 0) ? -ENODEV : data[2];\n+\tinfo(\"Firmware Status: %6ph\", data);\n+\n \tkfree(data);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tint ret;",
                "\tdata = kzalloc(6, GFP_KERNEL);",
                "\tret = usb_control_msg(d->udev, usb_rcvctrlpipe(d->udev, 0),",
                "\t\t\t      0x06, 0x80, 0x0302, 0x00,",
                "\t\t\t      data, 0x6, 200);",
                "\tif (ret != 6)",
                "\t\tret = -EINVAL;",
                "\telse",
                "\t\tret = data[2];",
                "\tinfo(\"Firmware Status: %6ph\", data);",
                ""
            ],
            "deleted": [
                "\tint ret = 0;",
                "\tdata = kzalloc(10, GFP_KERNEL);",
                "\tret |= usb_control_msg(d->udev, usb_rcvctrlpipe(d->udev, 0),",
                "\t\t\t0x06, 0x80, 0x0302, 0x00, data, 0x0006, 200);",
                "\tinfo(\"Firmware Status: %x (%x)\", ret , data[2]);",
                "\tret = (ret < 0) ? -ENODEV : data[2];"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "drivers/media/usb/dvb-usb-v2/lmedm04.c in the Linux kernel through 4.13.11 allows local users to cause a denial of service (general protection fault and system crash) or possibly have unspecified other impact via a crafted USB device, related to a missing warm-start check and incorrect attach timing (dm04_lme2510_frontend_attach versus dm04_lme2510_tuner).",
        "id": 1324
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int irda_recvmsg_dgram(struct kiocb *iocb, struct socket *sock,\n\t\t\t      struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct irda_sock *self = irda_sk(sk);\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\n\tIRDA_DEBUG(4, \"%s()\\n\", __func__);\n\n\tmsg->msg_namelen = 0;\n\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\tflags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tskb_reset_transport_header(skb);\n\tcopied = skb->len;\n\n\tif (copied > size) {\n\t\tIRDA_DEBUG(2, \"%s(), Received truncated frame (%zd < %zd)!\\n\",\n\t\t\t   __func__, copied, size);\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\tskb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tskb_free_datagram(sk, skb);\n\n\t/*\n\t *  Check if we have previously stopped IrTTP and we know\n\t *  have more free space in our rx_queue. If so tell IrTTP\n\t *  to start delivering frames again before our rx_queue gets\n\t *  empty\n\t */\n\tif (self->rx_flow == FLOW_STOP) {\n\t\tif ((atomic_read(&sk->sk_rmem_alloc) << 2) <= sk->sk_rcvbuf) {\n\t\t\tIRDA_DEBUG(2, \"%s(), Starting IrTTP\\n\", __func__);\n\t\t\tself->rx_flow = FLOW_START;\n\t\t\tirttp_flow_request(self->tsap, FLOW_START);\n\t\t}\n\t}\n\n\treturn copied;\n}",
        "code_after_change": "static int irda_recvmsg_dgram(struct kiocb *iocb, struct socket *sock,\n\t\t\t      struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct irda_sock *self = irda_sk(sk);\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\n\tIRDA_DEBUG(4, \"%s()\\n\", __func__);\n\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\tflags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tskb_reset_transport_header(skb);\n\tcopied = skb->len;\n\n\tif (copied > size) {\n\t\tIRDA_DEBUG(2, \"%s(), Received truncated frame (%zd < %zd)!\\n\",\n\t\t\t   __func__, copied, size);\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\tskb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tskb_free_datagram(sk, skb);\n\n\t/*\n\t *  Check if we have previously stopped IrTTP and we know\n\t *  have more free space in our rx_queue. If so tell IrTTP\n\t *  to start delivering frames again before our rx_queue gets\n\t *  empty\n\t */\n\tif (self->rx_flow == FLOW_STOP) {\n\t\tif ((atomic_read(&sk->sk_rmem_alloc) << 2) <= sk->sk_rcvbuf) {\n\t\t\tIRDA_DEBUG(2, \"%s(), Starting IrTTP\\n\", __func__);\n\t\t\tself->rx_flow = FLOW_START;\n\t\t\tirttp_flow_request(self->tsap, FLOW_START);\n\t\t}\n\t}\n\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,6 @@\n \tint err;\n \n \tIRDA_DEBUG(4, \"%s()\\n\", __func__);\n-\n-\tmsg->msg_namelen = 0;\n \n \tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n \t\t\t\tflags & MSG_DONTWAIT, &err);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 384
    },
    {
        "cve_id": "CVE-2018-5803",
        "code_before_change": "static struct sctp_chunk *_sctp_make_chunk(const struct sctp_association *asoc,\n\t\t\t\t\t   __u8 type, __u8 flags, int paylen,\n\t\t\t\t\t   gfp_t gfp)\n{\n\tstruct sctp_chunkhdr *chunk_hdr;\n\tstruct sctp_chunk *retval;\n\tstruct sk_buff *skb;\n\tstruct sock *sk;\n\n\t/* No need to allocate LL here, as this is only a chunk. */\n\tskb = alloc_skb(SCTP_PAD4(sizeof(*chunk_hdr) + paylen), gfp);\n\tif (!skb)\n\t\tgoto nodata;\n\n\t/* Make room for the chunk header.  */\n\tchunk_hdr = (struct sctp_chunkhdr *)skb_put(skb, sizeof(*chunk_hdr));\n\tchunk_hdr->type\t  = type;\n\tchunk_hdr->flags  = flags;\n\tchunk_hdr->length = htons(sizeof(*chunk_hdr));\n\n\tsk = asoc ? asoc->base.sk : NULL;\n\tretval = sctp_chunkify(skb, asoc, sk, gfp);\n\tif (!retval) {\n\t\tkfree_skb(skb);\n\t\tgoto nodata;\n\t}\n\n\tretval->chunk_hdr = chunk_hdr;\n\tretval->chunk_end = ((__u8 *)chunk_hdr) + sizeof(*chunk_hdr);\n\n\t/* Determine if the chunk needs to be authenticated */\n\tif (sctp_auth_send_cid(type, asoc))\n\t\tretval->auth = 1;\n\n\treturn retval;\nnodata:\n\treturn NULL;\n}",
        "code_after_change": "static struct sctp_chunk *_sctp_make_chunk(const struct sctp_association *asoc,\n\t\t\t\t\t   __u8 type, __u8 flags, int paylen,\n\t\t\t\t\t   gfp_t gfp)\n{\n\tstruct sctp_chunkhdr *chunk_hdr;\n\tstruct sctp_chunk *retval;\n\tstruct sk_buff *skb;\n\tstruct sock *sk;\n\tint chunklen;\n\n\tchunklen = SCTP_PAD4(sizeof(*chunk_hdr) + paylen);\n\tif (chunklen > SCTP_MAX_CHUNK_LEN)\n\t\tgoto nodata;\n\n\t/* No need to allocate LL here, as this is only a chunk. */\n\tskb = alloc_skb(chunklen, gfp);\n\tif (!skb)\n\t\tgoto nodata;\n\n\t/* Make room for the chunk header.  */\n\tchunk_hdr = (struct sctp_chunkhdr *)skb_put(skb, sizeof(*chunk_hdr));\n\tchunk_hdr->type\t  = type;\n\tchunk_hdr->flags  = flags;\n\tchunk_hdr->length = htons(sizeof(*chunk_hdr));\n\n\tsk = asoc ? asoc->base.sk : NULL;\n\tretval = sctp_chunkify(skb, asoc, sk, gfp);\n\tif (!retval) {\n\t\tkfree_skb(skb);\n\t\tgoto nodata;\n\t}\n\n\tretval->chunk_hdr = chunk_hdr;\n\tretval->chunk_end = ((__u8 *)chunk_hdr) + sizeof(*chunk_hdr);\n\n\t/* Determine if the chunk needs to be authenticated */\n\tif (sctp_auth_send_cid(type, asoc))\n\t\tretval->auth = 1;\n\n\treturn retval;\nnodata:\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,9 +6,14 @@\n \tstruct sctp_chunk *retval;\n \tstruct sk_buff *skb;\n \tstruct sock *sk;\n+\tint chunklen;\n+\n+\tchunklen = SCTP_PAD4(sizeof(*chunk_hdr) + paylen);\n+\tif (chunklen > SCTP_MAX_CHUNK_LEN)\n+\t\tgoto nodata;\n \n \t/* No need to allocate LL here, as this is only a chunk. */\n-\tskb = alloc_skb(SCTP_PAD4(sizeof(*chunk_hdr) + paylen), gfp);\n+\tskb = alloc_skb(chunklen, gfp);\n \tif (!skb)\n \t\tgoto nodata;\n ",
        "function_modified_lines": {
            "added": [
                "\tint chunklen;",
                "",
                "\tchunklen = SCTP_PAD4(sizeof(*chunk_hdr) + paylen);",
                "\tif (chunklen > SCTP_MAX_CHUNK_LEN)",
                "\t\tgoto nodata;",
                "\tskb = alloc_skb(chunklen, gfp);"
            ],
            "deleted": [
                "\tskb = alloc_skb(SCTP_PAD4(sizeof(*chunk_hdr) + paylen), gfp);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "In the Linux Kernel before version 4.15.8, 4.14.25, 4.9.87, 4.4.121, 4.1.51, and 3.2.102, an error in the \"_sctp_make_chunk()\" function (net/sctp/sm_make_chunk.c) when handling SCTP packets length can be exploited to cause a kernel crash.",
        "id": 1828
    },
    {
        "cve_id": "CVE-2013-0290",
        "code_before_change": "struct sk_buff *__skb_recv_datagram(struct sock *sk, unsigned int flags,\n\t\t\t\t    int *peeked, int *off, int *err)\n{\n\tstruct sk_buff *skb;\n\tlong timeo;\n\t/*\n\t * Caller is allowed not to check sk->sk_err before skb_recv_datagram()\n\t */\n\tint error = sock_error(sk);\n\n\tif (error)\n\t\tgoto no_packet;\n\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\tdo {\n\t\t/* Again only user level code calls this function, so nothing\n\t\t * interrupt level will suddenly eat the receive_queue.\n\t\t *\n\t\t * Look at current nfs client by the way...\n\t\t * However, this function was correct in any case. 8)\n\t\t */\n\t\tunsigned long cpu_flags;\n\t\tstruct sk_buff_head *queue = &sk->sk_receive_queue;\n\n\t\tspin_lock_irqsave(&queue->lock, cpu_flags);\n\t\tskb_queue_walk(queue, skb) {\n\t\t\t*peeked = skb->peeked;\n\t\t\tif (flags & MSG_PEEK) {\n\t\t\t\tif (*off >= skb->len) {\n\t\t\t\t\t*off -= skb->len;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tskb->peeked = 1;\n\t\t\t\tatomic_inc(&skb->users);\n\t\t\t} else\n\t\t\t\t__skb_unlink(skb, queue);\n\n\t\t\tspin_unlock_irqrestore(&queue->lock, cpu_flags);\n\t\t\treturn skb;\n\t\t}\n\t\tspin_unlock_irqrestore(&queue->lock, cpu_flags);\n\n\t\t/* User doesn't want to wait */\n\t\terror = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto no_packet;\n\n\t} while (!wait_for_packet(sk, err, &timeo));\n\n\treturn NULL;\n\nno_packet:\n\t*err = error;\n\treturn NULL;\n}",
        "code_after_change": "struct sk_buff *__skb_recv_datagram(struct sock *sk, unsigned int flags,\n\t\t\t\t    int *peeked, int *off, int *err)\n{\n\tstruct sk_buff *skb;\n\tlong timeo;\n\t/*\n\t * Caller is allowed not to check sk->sk_err before skb_recv_datagram()\n\t */\n\tint error = sock_error(sk);\n\n\tif (error)\n\t\tgoto no_packet;\n\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\tdo {\n\t\t/* Again only user level code calls this function, so nothing\n\t\t * interrupt level will suddenly eat the receive_queue.\n\t\t *\n\t\t * Look at current nfs client by the way...\n\t\t * However, this function was correct in any case. 8)\n\t\t */\n\t\tunsigned long cpu_flags;\n\t\tstruct sk_buff_head *queue = &sk->sk_receive_queue;\n\n\t\tspin_lock_irqsave(&queue->lock, cpu_flags);\n\t\tskb_queue_walk(queue, skb) {\n\t\t\t*peeked = skb->peeked;\n\t\t\tif (flags & MSG_PEEK) {\n\t\t\t\tif (*off >= skb->len && skb->len) {\n\t\t\t\t\t*off -= skb->len;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tskb->peeked = 1;\n\t\t\t\tatomic_inc(&skb->users);\n\t\t\t} else\n\t\t\t\t__skb_unlink(skb, queue);\n\n\t\t\tspin_unlock_irqrestore(&queue->lock, cpu_flags);\n\t\t\treturn skb;\n\t\t}\n\t\tspin_unlock_irqrestore(&queue->lock, cpu_flags);\n\n\t\t/* User doesn't want to wait */\n\t\terror = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto no_packet;\n\n\t} while (!wait_for_packet(sk, err, &timeo));\n\n\treturn NULL;\n\nno_packet:\n\t*err = error;\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,7 +27,7 @@\n \t\tskb_queue_walk(queue, skb) {\n \t\t\t*peeked = skb->peeked;\n \t\t\tif (flags & MSG_PEEK) {\n-\t\t\t\tif (*off >= skb->len) {\n+\t\t\t\tif (*off >= skb->len && skb->len) {\n \t\t\t\t\t*off -= skb->len;\n \t\t\t\t\tcontinue;\n \t\t\t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tif (*off >= skb->len && skb->len) {"
            ],
            "deleted": [
                "\t\t\t\tif (*off >= skb->len) {"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The __skb_recv_datagram function in net/core/datagram.c in the Linux kernel before 3.8 does not properly handle the MSG_PEEK flag with zero-length data, which allows local users to cause a denial of service (infinite loop and system hang) via a crafted application.",
        "id": 154
    },
    {
        "cve_id": "CVE-2012-4398",
        "code_before_change": "void GPIOChangeRFWorkItemCallBack(struct work_struct *work)\n{\n\tstruct ieee80211_device *ieee = container_of(work, struct ieee80211_device, GPIOChangeRFWorkItem.work);\n\tstruct net_device *dev = ieee->dev;\n\tstruct r8180_priv *priv = ieee80211_priv(dev);\n\tu8 btPSR;\n\tu8 btConfig0;\n\tRT_RF_POWER_STATE\teRfPowerStateToSet;\n\tbool bActuallySet = false;\n\n\tchar *argv[3];\n\tstatic char *RadioPowerPath = \"/etc/acpi/events/RadioPower.sh\";\n\tstatic char *envp[] = {\"HOME=/\", \"TERM=linux\", \"PATH=/usr/bin:/bin\", NULL};\n\tstatic int readf_count = 0;\n\n\tif (readf_count % 10 == 0)\n\t\tpriv->PowerProfile = read_acadapter_file(\"/proc/acpi/ac_adapter/AC0/state\");\n\n\treadf_count = (readf_count+1)%0xffff;\n\t/* We should turn off LED before polling FF51[4]. */\n\n\t/* Turn off LED. */\n\tbtPSR = read_nic_byte(dev, PSR);\n\twrite_nic_byte(dev, PSR, (btPSR & ~BIT3));\n\n\t/* It need to delay 4us suggested by Jong, 2008-01-16 */\n\tudelay(4);\n\n\t/* HW radio On/Off according to the value of FF51[4](config0) */\n\tbtConfig0 = btPSR = read_nic_byte(dev, CONFIG0);\n\n\teRfPowerStateToSet = (btConfig0 & BIT4) ?  eRfOn : eRfOff;\n\n\t/* Turn LED back on when radio enabled */\n\tif (eRfPowerStateToSet == eRfOn)\n\t\twrite_nic_byte(dev, PSR, btPSR | BIT3);\n\n\tif ((priv->ieee80211->bHwRadioOff == true) &&\n\t   (eRfPowerStateToSet == eRfOn)) {\n\t\tpriv->ieee80211->bHwRadioOff = false;\n\t\tbActuallySet = true;\n\t} else if ((priv->ieee80211->bHwRadioOff == false) &&\n\t\t  (eRfPowerStateToSet == eRfOff)) {\n\t\tpriv->ieee80211->bHwRadioOff = true;\n\t\tbActuallySet = true;\n\t}\n\n\tif (bActuallySet) {\n\t\tMgntActSet_RF_State(dev, eRfPowerStateToSet, RF_CHANGE_BY_HW);\n\n\t\t/* To update the UI status for Power status changed */\n\t\tif (priv->ieee80211->bHwRadioOff == true)\n\t\t\targv[1] = \"RFOFF\";\n\t\telse\n\t\t\targv[1] = \"RFON\";\n\t\targv[0] = RadioPowerPath;\n\t\targv[2] = NULL;\n\n\t\tcall_usermodehelper(RadioPowerPath, argv, envp, 1);\n\t}\n}",
        "code_after_change": "void GPIOChangeRFWorkItemCallBack(struct work_struct *work)\n{\n\tstruct ieee80211_device *ieee = container_of(work, struct ieee80211_device, GPIOChangeRFWorkItem.work);\n\tstruct net_device *dev = ieee->dev;\n\tstruct r8180_priv *priv = ieee80211_priv(dev);\n\tu8 btPSR;\n\tu8 btConfig0;\n\tRT_RF_POWER_STATE\teRfPowerStateToSet;\n\tbool bActuallySet = false;\n\n\tchar *argv[3];\n\tstatic char *RadioPowerPath = \"/etc/acpi/events/RadioPower.sh\";\n\tstatic char *envp[] = {\"HOME=/\", \"TERM=linux\", \"PATH=/usr/bin:/bin\", NULL};\n\tstatic int readf_count = 0;\n\n\tif (readf_count % 10 == 0)\n\t\tpriv->PowerProfile = read_acadapter_file(\"/proc/acpi/ac_adapter/AC0/state\");\n\n\treadf_count = (readf_count+1)%0xffff;\n\t/* We should turn off LED before polling FF51[4]. */\n\n\t/* Turn off LED. */\n\tbtPSR = read_nic_byte(dev, PSR);\n\twrite_nic_byte(dev, PSR, (btPSR & ~BIT3));\n\n\t/* It need to delay 4us suggested by Jong, 2008-01-16 */\n\tudelay(4);\n\n\t/* HW radio On/Off according to the value of FF51[4](config0) */\n\tbtConfig0 = btPSR = read_nic_byte(dev, CONFIG0);\n\n\teRfPowerStateToSet = (btConfig0 & BIT4) ?  eRfOn : eRfOff;\n\n\t/* Turn LED back on when radio enabled */\n\tif (eRfPowerStateToSet == eRfOn)\n\t\twrite_nic_byte(dev, PSR, btPSR | BIT3);\n\n\tif ((priv->ieee80211->bHwRadioOff == true) &&\n\t   (eRfPowerStateToSet == eRfOn)) {\n\t\tpriv->ieee80211->bHwRadioOff = false;\n\t\tbActuallySet = true;\n\t} else if ((priv->ieee80211->bHwRadioOff == false) &&\n\t\t  (eRfPowerStateToSet == eRfOff)) {\n\t\tpriv->ieee80211->bHwRadioOff = true;\n\t\tbActuallySet = true;\n\t}\n\n\tif (bActuallySet) {\n\t\tMgntActSet_RF_State(dev, eRfPowerStateToSet, RF_CHANGE_BY_HW);\n\n\t\t/* To update the UI status for Power status changed */\n\t\tif (priv->ieee80211->bHwRadioOff == true)\n\t\t\targv[1] = \"RFOFF\";\n\t\telse\n\t\t\targv[1] = \"RFON\";\n\t\targv[0] = RadioPowerPath;\n\t\targv[2] = NULL;\n\n\t\tcall_usermodehelper(RadioPowerPath, argv, envp, UMH_WAIT_PROC);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -56,6 +56,6 @@\n \t\targv[0] = RadioPowerPath;\n \t\targv[2] = NULL;\n \n-\t\tcall_usermodehelper(RadioPowerPath, argv, envp, 1);\n+\t\tcall_usermodehelper(RadioPowerPath, argv, envp, UMH_WAIT_PROC);\n \t}\n }",
        "function_modified_lines": {
            "added": [
                "\t\tcall_usermodehelper(RadioPowerPath, argv, envp, UMH_WAIT_PROC);"
            ],
            "deleted": [
                "\t\tcall_usermodehelper(RadioPowerPath, argv, envp, 1);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The __request_module function in kernel/kmod.c in the Linux kernel before 3.4 does not set a certain killable attribute, which allows local users to cause a denial of service (memory consumption) via a crafted application.",
        "id": 96
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tunsigned long iovlen;\n\tstruct iovec *iov;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\tmsg->msg_namelen = 0;\n\tfor (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;\n\t     iovlen--, iov++) {\n\t\tunsigned long seglen = iov->iov_len;\n\t\tchar __user *from = iov->iov_base;\n\n\t\twhile (seglen) {\n\t\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t\t       struct skcipher_sg_list, list);\n\t\t\tsg = sgl->sg;\n\n\t\t\twhile (!sg->length)\n\t\t\t\tsg++;\n\n\t\t\tused = ctx->used;\n\t\t\tif (!used) {\n\t\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto unlock;\n\t\t\t}\n\n\t\t\tused = min_t(unsigned long, used, seglen);\n\n\t\t\tused = af_alg_make_sg(&ctx->rsgl, from, used, 1);\n\t\t\terr = used;\n\t\t\tif (err < 0)\n\t\t\t\tgoto unlock;\n\n\t\t\tif (ctx->more || used < ctx->used)\n\t\t\t\tused -= used % bs;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (!used)\n\t\t\t\tgoto free;\n\n\t\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t\t     ctx->iv);\n\n\t\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\n\t\t\tcopied += used;\n\t\t\tfrom += used;\n\t\t\tseglen -= used;\n\t\t\tskcipher_pull_sgl(sk, used);\n\t\t}\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
        "code_after_change": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tunsigned long iovlen;\n\tstruct iovec *iov;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\tfor (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;\n\t     iovlen--, iov++) {\n\t\tunsigned long seglen = iov->iov_len;\n\t\tchar __user *from = iov->iov_base;\n\n\t\twhile (seglen) {\n\t\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t\t       struct skcipher_sg_list, list);\n\t\t\tsg = sgl->sg;\n\n\t\t\twhile (!sg->length)\n\t\t\t\tsg++;\n\n\t\t\tused = ctx->used;\n\t\t\tif (!used) {\n\t\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto unlock;\n\t\t\t}\n\n\t\t\tused = min_t(unsigned long, used, seglen);\n\n\t\t\tused = af_alg_make_sg(&ctx->rsgl, from, used, 1);\n\t\t\terr = used;\n\t\t\tif (err < 0)\n\t\t\t\tgoto unlock;\n\n\t\t\tif (ctx->more || used < ctx->used)\n\t\t\t\tused -= used % bs;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (!used)\n\t\t\t\tgoto free;\n\n\t\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t\t     ctx->iv);\n\n\t\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\n\t\t\tcopied += used;\n\t\t\tfrom += used;\n\t\t\tseglen -= used;\n\t\t\tskcipher_pull_sgl(sk, used);\n\t\t}\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,7 +15,6 @@\n \tlong copied = 0;\n \n \tlock_sock(sk);\n-\tmsg->msg_namelen = 0;\n \tfor (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;\n \t     iovlen--, iov++) {\n \t\tunsigned long seglen = iov->iov_len;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 367
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static void io_req_free_batch(struct req_batch *rb, struct io_kiocb *req)\n{\n\tif (unlikely(io_is_fallback_req(req))) {\n\t\tio_free_req(req);\n\t\treturn;\n\t}\n\tif (req->flags & REQ_F_LINK_HEAD)\n\t\tio_queue_next(req);\n\n\tif (req->task != rb->task) {\n\t\tif (rb->task)\n\t\t\tput_task_struct_many(rb->task, rb->task_refs);\n\t\trb->task = req->task;\n\t\trb->task_refs = 0;\n\t}\n\trb->task_refs++;\n\n\tWARN_ON_ONCE(io_dismantle_req(req));\n\trb->reqs[rb->to_free++] = req;\n\tif (unlikely(rb->to_free == ARRAY_SIZE(rb->reqs)))\n\t\t__io_req_free_batch_flush(req->ctx, rb);\n}",
        "code_after_change": "static void io_req_free_batch(struct req_batch *rb, struct io_kiocb *req)\n{\n\tif (unlikely(io_is_fallback_req(req))) {\n\t\tio_free_req(req);\n\t\treturn;\n\t}\n\tif (req->flags & REQ_F_LINK_HEAD)\n\t\tio_queue_next(req);\n\n\tif (req->task != rb->task) {\n\t\tif (rb->task) {\n\t\t\tatomic_long_add(rb->task_refs, &rb->task->io_uring->req_complete);\n\t\t\tput_task_struct_many(rb->task, rb->task_refs);\n\t\t}\n\t\trb->task = req->task;\n\t\trb->task_refs = 0;\n\t}\n\trb->task_refs++;\n\n\tWARN_ON_ONCE(io_dismantle_req(req));\n\trb->reqs[rb->to_free++] = req;\n\tif (unlikely(rb->to_free == ARRAY_SIZE(rb->reqs)))\n\t\t__io_req_free_batch_flush(req->ctx, rb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,10 @@\n \t\tio_queue_next(req);\n \n \tif (req->task != rb->task) {\n-\t\tif (rb->task)\n+\t\tif (rb->task) {\n+\t\t\tatomic_long_add(rb->task_refs, &rb->task->io_uring->req_complete);\n \t\t\tput_task_struct_many(rb->task, rb->task_refs);\n+\t\t}\n \t\trb->task = req->task;\n \t\trb->task_refs = 0;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (rb->task) {",
                "\t\t\tatomic_long_add(rb->task_refs, &rb->task->io_uring->req_complete);",
                "\t\t}"
            ],
            "deleted": [
                "\t\tif (rb->task)"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2853
    },
    {
        "cve_id": "CVE-2013-1848",
        "code_before_change": "static struct block_device *ext3_blkdev_get(dev_t dev, struct super_block *sb)\n{\n\tstruct block_device *bdev;\n\tchar b[BDEVNAME_SIZE];\n\n\tbdev = blkdev_get_by_dev(dev, FMODE_READ|FMODE_WRITE|FMODE_EXCL, sb);\n\tif (IS_ERR(bdev))\n\t\tgoto fail;\n\treturn bdev;\n\nfail:\n\text3_msg(sb, \"error: failed to open journal device %s: %ld\",\n\t\t__bdevname(dev, b), PTR_ERR(bdev));\n\n\treturn NULL;\n}",
        "code_after_change": "static struct block_device *ext3_blkdev_get(dev_t dev, struct super_block *sb)\n{\n\tstruct block_device *bdev;\n\tchar b[BDEVNAME_SIZE];\n\n\tbdev = blkdev_get_by_dev(dev, FMODE_READ|FMODE_WRITE|FMODE_EXCL, sb);\n\tif (IS_ERR(bdev))\n\t\tgoto fail;\n\treturn bdev;\n\nfail:\n\text3_msg(sb, KERN_ERR, \"error: failed to open journal device %s: %ld\",\n\t\t__bdevname(dev, b), PTR_ERR(bdev));\n\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \treturn bdev;\n \n fail:\n-\text3_msg(sb, \"error: failed to open journal device %s: %ld\",\n+\text3_msg(sb, KERN_ERR, \"error: failed to open journal device %s: %ld\",\n \t\t__bdevname(dev, b), PTR_ERR(bdev));\n \n \treturn NULL;",
        "function_modified_lines": {
            "added": [
                "\text3_msg(sb, KERN_ERR, \"error: failed to open journal device %s: %ld\","
            ],
            "deleted": [
                "\text3_msg(sb, \"error: failed to open journal device %s: %ld\","
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "fs/ext3/super.c in the Linux kernel before 3.8.4 uses incorrect arguments to functions in certain circumstances related to printk input, which allows local users to conduct format-string attacks and possibly gain privileges via a crafted application.",
        "id": 197
    },
    {
        "cve_id": "CVE-2015-3288",
        "code_before_change": "static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tunsigned long address, pte_t *page_table, pmd_t *pmd,\n\t\tunsigned int flags)\n{\n\tstruct mem_cgroup *memcg;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t entry;\n\n\tpte_unmap(page_table);\n\n\t/* Check if we need to add a guard page to the stack */\n\tif (check_stack_guard_page(vma, address) < 0)\n\t\treturn VM_FAULT_SIGSEGV;\n\n\t/* Use the zero-page for reads */\n\tif (!(flags & FAULT_FLAG_WRITE) && !mm_forbids_zeropage(mm)) {\n\t\tentry = pte_mkspecial(pfn_pte(my_zero_pfn(address),\n\t\t\t\t\t\tvma->vm_page_prot));\n\t\tpage_table = pte_offset_map_lock(mm, pmd, address, &ptl);\n\t\tif (!pte_none(*page_table))\n\t\t\tgoto unlock;\n\t\tgoto setpte;\n\t}\n\n\t/* Allocate our own private page. */\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\tgoto oom;\n\tpage = alloc_zeroed_user_highpage_movable(vma, address);\n\tif (!page)\n\t\tgoto oom;\n\n\tif (mem_cgroup_try_charge(page, mm, GFP_KERNEL, &memcg))\n\t\tgoto oom_free_page;\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceeding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\n\tentry = mk_pte(page, vma->vm_page_prot);\n\tif (vma->vm_flags & VM_WRITE)\n\t\tentry = pte_mkwrite(pte_mkdirty(entry));\n\n\tpage_table = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tif (!pte_none(*page_table))\n\t\tgoto release;\n\n\tinc_mm_counter_fast(mm, MM_ANONPAGES);\n\tpage_add_new_anon_rmap(page, vma, address);\n\tmem_cgroup_commit_charge(page, memcg, false);\n\tlru_cache_add_active_or_unevictable(page, vma);\nsetpte:\n\tset_pte_at(mm, address, page_table, entry);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, address, page_table);\nunlock:\n\tpte_unmap_unlock(page_table, ptl);\n\treturn 0;\nrelease:\n\tmem_cgroup_cancel_charge(page, memcg);\n\tpage_cache_release(page);\n\tgoto unlock;\noom_free_page:\n\tpage_cache_release(page);\noom:\n\treturn VM_FAULT_OOM;\n}",
        "code_after_change": "static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tunsigned long address, pte_t *page_table, pmd_t *pmd,\n\t\tunsigned int flags)\n{\n\tstruct mem_cgroup *memcg;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t entry;\n\n\tpte_unmap(page_table);\n\n\t/* File mapping without ->vm_ops ? */\n\tif (vma->vm_flags & VM_SHARED)\n\t\treturn VM_FAULT_SIGBUS;\n\n\t/* Check if we need to add a guard page to the stack */\n\tif (check_stack_guard_page(vma, address) < 0)\n\t\treturn VM_FAULT_SIGSEGV;\n\n\t/* Use the zero-page for reads */\n\tif (!(flags & FAULT_FLAG_WRITE) && !mm_forbids_zeropage(mm)) {\n\t\tentry = pte_mkspecial(pfn_pte(my_zero_pfn(address),\n\t\t\t\t\t\tvma->vm_page_prot));\n\t\tpage_table = pte_offset_map_lock(mm, pmd, address, &ptl);\n\t\tif (!pte_none(*page_table))\n\t\t\tgoto unlock;\n\t\tgoto setpte;\n\t}\n\n\t/* Allocate our own private page. */\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\tgoto oom;\n\tpage = alloc_zeroed_user_highpage_movable(vma, address);\n\tif (!page)\n\t\tgoto oom;\n\n\tif (mem_cgroup_try_charge(page, mm, GFP_KERNEL, &memcg))\n\t\tgoto oom_free_page;\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceeding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\n\tentry = mk_pte(page, vma->vm_page_prot);\n\tif (vma->vm_flags & VM_WRITE)\n\t\tentry = pte_mkwrite(pte_mkdirty(entry));\n\n\tpage_table = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tif (!pte_none(*page_table))\n\t\tgoto release;\n\n\tinc_mm_counter_fast(mm, MM_ANONPAGES);\n\tpage_add_new_anon_rmap(page, vma, address);\n\tmem_cgroup_commit_charge(page, memcg, false);\n\tlru_cache_add_active_or_unevictable(page, vma);\nsetpte:\n\tset_pte_at(mm, address, page_table, entry);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, address, page_table);\nunlock:\n\tpte_unmap_unlock(page_table, ptl);\n\treturn 0;\nrelease:\n\tmem_cgroup_cancel_charge(page, memcg);\n\tpage_cache_release(page);\n\tgoto unlock;\noom_free_page:\n\tpage_cache_release(page);\noom:\n\treturn VM_FAULT_OOM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,10 @@\n \tpte_t entry;\n \n \tpte_unmap(page_table);\n+\n+\t/* File mapping without ->vm_ops ? */\n+\tif (vma->vm_flags & VM_SHARED)\n+\t\treturn VM_FAULT_SIGBUS;\n \n \t/* Check if we need to add a guard page to the stack */\n \tif (check_stack_guard_page(vma, address) < 0)",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* File mapping without ->vm_ops ? */",
                "\tif (vma->vm_flags & VM_SHARED)",
                "\t\treturn VM_FAULT_SIGBUS;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "mm/memory.c in the Linux kernel before 4.1.4 mishandles anonymous pages, which allows local users to gain privileges or cause a denial of service (page tainting) via a crafted application that triggers writing to page zero.",
        "id": 754
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int vmci_transport_dgram_dequeue(struct kiocb *kiocb,\n\t\t\t\t\tstruct vsock_sock *vsk,\n\t\t\t\t\tstruct msghdr *msg, size_t len,\n\t\t\t\t\tint flags)\n{\n\tint err;\n\tint noblock;\n\tstruct vmci_datagram *dg;\n\tsize_t payload_len;\n\tstruct sk_buff *skb;\n\n\tnoblock = flags & MSG_DONTWAIT;\n\n\tif (flags & MSG_OOB || flags & MSG_ERRQUEUE)\n\t\treturn -EOPNOTSUPP;\n\n\tmsg->msg_namelen = 0;\n\n\t/* Retrieve the head sk_buff from the socket's receive queue. */\n\terr = 0;\n\tskb = skb_recv_datagram(&vsk->sk, flags, noblock, &err);\n\tif (err)\n\t\treturn err;\n\n\tif (!skb)\n\t\treturn -EAGAIN;\n\n\tdg = (struct vmci_datagram *)skb->data;\n\tif (!dg)\n\t\t/* err is 0, meaning we read zero bytes. */\n\t\tgoto out;\n\n\tpayload_len = dg->payload_size;\n\t/* Ensure the sk_buff matches the payload size claimed in the packet. */\n\tif (payload_len != skb->len - sizeof(*dg)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (payload_len > len) {\n\t\tpayload_len = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\t/* Place the datagram payload in the user's iovec. */\n\terr = skb_copy_datagram_iovec(skb, sizeof(*dg), msg->msg_iov,\n\t\tpayload_len);\n\tif (err)\n\t\tgoto out;\n\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_vm *vm_addr;\n\n\t\t/* Provide the address of the sender. */\n\t\tvm_addr = (struct sockaddr_vm *)msg->msg_name;\n\t\tvsock_addr_init(vm_addr, dg->src.context, dg->src.resource);\n\t\tmsg->msg_namelen = sizeof(*vm_addr);\n\t}\n\terr = payload_len;\n\nout:\n\tskb_free_datagram(&vsk->sk, skb);\n\treturn err;\n}",
        "code_after_change": "static int vmci_transport_dgram_dequeue(struct kiocb *kiocb,\n\t\t\t\t\tstruct vsock_sock *vsk,\n\t\t\t\t\tstruct msghdr *msg, size_t len,\n\t\t\t\t\tint flags)\n{\n\tint err;\n\tint noblock;\n\tstruct vmci_datagram *dg;\n\tsize_t payload_len;\n\tstruct sk_buff *skb;\n\n\tnoblock = flags & MSG_DONTWAIT;\n\n\tif (flags & MSG_OOB || flags & MSG_ERRQUEUE)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Retrieve the head sk_buff from the socket's receive queue. */\n\terr = 0;\n\tskb = skb_recv_datagram(&vsk->sk, flags, noblock, &err);\n\tif (err)\n\t\treturn err;\n\n\tif (!skb)\n\t\treturn -EAGAIN;\n\n\tdg = (struct vmci_datagram *)skb->data;\n\tif (!dg)\n\t\t/* err is 0, meaning we read zero bytes. */\n\t\tgoto out;\n\n\tpayload_len = dg->payload_size;\n\t/* Ensure the sk_buff matches the payload size claimed in the packet. */\n\tif (payload_len != skb->len - sizeof(*dg)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (payload_len > len) {\n\t\tpayload_len = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\t/* Place the datagram payload in the user's iovec. */\n\terr = skb_copy_datagram_iovec(skb, sizeof(*dg), msg->msg_iov,\n\t\tpayload_len);\n\tif (err)\n\t\tgoto out;\n\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_vm *vm_addr;\n\n\t\t/* Provide the address of the sender. */\n\t\tvm_addr = (struct sockaddr_vm *)msg->msg_name;\n\t\tvsock_addr_init(vm_addr, dg->src.context, dg->src.resource);\n\t\tmsg->msg_namelen = sizeof(*vm_addr);\n\t}\n\terr = payload_len;\n\nout:\n\tskb_free_datagram(&vsk->sk, skb);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,8 +13,6 @@\n \n \tif (flags & MSG_OOB || flags & MSG_ERRQUEUE)\n \t\treturn -EOPNOTSUPP;\n-\n-\tmsg->msg_namelen = 0;\n \n \t/* Retrieve the head sk_buff from the socket's receive queue. */\n \terr = 0;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 405
    },
    {
        "cve_id": "CVE-2018-14641",
        "code_before_change": "static int ip_frag_reasm(struct ipq *qp, struct sk_buff *skb,\n\t\t\t struct sk_buff *prev_tail, struct net_device *dev)\n{\n\tstruct net *net = container_of(qp->q.net, struct net, ipv4.frags);\n\tstruct iphdr *iph;\n\tstruct sk_buff *fp, *head = skb_rb_first(&qp->q.rb_fragments);\n\tstruct sk_buff **nextp; /* To build frag_list. */\n\tstruct rb_node *rbn;\n\tint len;\n\tint ihlen;\n\tint err;\n\tu8 ecn;\n\n\tipq_kill(qp);\n\n\tecn = ip_frag_ecn_table[qp->ecn];\n\tif (unlikely(ecn == 0xff)) {\n\t\terr = -EINVAL;\n\t\tgoto out_fail;\n\t}\n\t/* Make the one we just received the head. */\n\tif (head != skb) {\n\t\tfp = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!fp)\n\t\t\tgoto out_nomem;\n\t\tFRAG_CB(fp)->next_frag = FRAG_CB(skb)->next_frag;\n\t\tif (RB_EMPTY_NODE(&skb->rbnode))\n\t\t\tFRAG_CB(prev_tail)->next_frag = fp;\n\t\telse\n\t\t\trb_replace_node(&skb->rbnode, &fp->rbnode,\n\t\t\t\t\t&qp->q.rb_fragments);\n\t\tif (qp->q.fragments_tail == skb)\n\t\t\tqp->q.fragments_tail = fp;\n\t\tskb_morph(skb, head);\n\t\tFRAG_CB(skb)->next_frag = FRAG_CB(head)->next_frag;\n\t\trb_replace_node(&head->rbnode, &skb->rbnode,\n\t\t\t\t&qp->q.rb_fragments);\n\t\tconsume_skb(head);\n\t\thead = skb;\n\t}\n\n\tWARN_ON(head->ip_defrag_offset != 0);\n\n\t/* Allocate a new buffer for the datagram. */\n\tihlen = ip_hdrlen(head);\n\tlen = ihlen + qp->q.len;\n\n\terr = -E2BIG;\n\tif (len > 65535)\n\t\tgoto out_oversize;\n\n\t/* Head of list must not be cloned. */\n\tif (skb_unclone(head, GFP_ATOMIC))\n\t\tgoto out_nomem;\n\n\t/* If the first fragment is fragmented itself, we split\n\t * it to two chunks: the first with data and paged part\n\t * and the second, holding only fragments. */\n\tif (skb_has_frag_list(head)) {\n\t\tstruct sk_buff *clone;\n\t\tint i, plen = 0;\n\n\t\tclone = alloc_skb(0, GFP_ATOMIC);\n\t\tif (!clone)\n\t\t\tgoto out_nomem;\n\t\tskb_shinfo(clone)->frag_list = skb_shinfo(head)->frag_list;\n\t\tskb_frag_list_init(head);\n\t\tfor (i = 0; i < skb_shinfo(head)->nr_frags; i++)\n\t\t\tplen += skb_frag_size(&skb_shinfo(head)->frags[i]);\n\t\tclone->len = clone->data_len = head->data_len - plen;\n\t\thead->truesize += clone->truesize;\n\t\tclone->csum = 0;\n\t\tclone->ip_summed = head->ip_summed;\n\t\tadd_frag_mem_limit(qp->q.net, clone->truesize);\n\t\tskb_shinfo(head)->frag_list = clone;\n\t\tnextp = &clone->next;\n\t} else {\n\t\tnextp = &skb_shinfo(head)->frag_list;\n\t}\n\n\tskb_push(head, head->data - skb_network_header(head));\n\n\t/* Traverse the tree in order, to build frag_list. */\n\tfp = FRAG_CB(head)->next_frag;\n\trbn = rb_next(&head->rbnode);\n\trb_erase(&head->rbnode, &qp->q.rb_fragments);\n\twhile (rbn || fp) {\n\t\t/* fp points to the next sk_buff in the current run;\n\t\t * rbn points to the next run.\n\t\t */\n\t\t/* Go through the current run. */\n\t\twhile (fp) {\n\t\t\t*nextp = fp;\n\t\t\tnextp = &fp->next;\n\t\t\tfp->prev = NULL;\n\t\t\tmemset(&fp->rbnode, 0, sizeof(fp->rbnode));\n\t\t\thead->data_len += fp->len;\n\t\t\thead->len += fp->len;\n\t\t\tif (head->ip_summed != fp->ip_summed)\n\t\t\t\thead->ip_summed = CHECKSUM_NONE;\n\t\t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n\t\t\t\thead->csum = csum_add(head->csum, fp->csum);\n\t\t\thead->truesize += fp->truesize;\n\t\t\tfp = FRAG_CB(fp)->next_frag;\n\t\t}\n\t\t/* Move to the next run. */\n\t\tif (rbn) {\n\t\t\tstruct rb_node *rbnext = rb_next(rbn);\n\n\t\t\tfp = rb_to_skb(rbn);\n\t\t\trb_erase(rbn, &qp->q.rb_fragments);\n\t\t\trbn = rbnext;\n\t\t}\n\t}\n\tsub_frag_mem_limit(qp->q.net, head->truesize);\n\n\t*nextp = NULL;\n\thead->next = NULL;\n\thead->prev = NULL;\n\thead->dev = dev;\n\thead->tstamp = qp->q.stamp;\n\tIPCB(head)->frag_max_size = max(qp->max_df_size, qp->q.max_size);\n\n\tiph = ip_hdr(head);\n\tiph->tot_len = htons(len);\n\tiph->tos |= ecn;\n\n\t/* When we set IP_DF on a refragmented skb we must also force a\n\t * call to ip_fragment to avoid forwarding a DF-skb of size s while\n\t * original sender only sent fragments of size f (where f < s).\n\t *\n\t * We only set DF/IPSKB_FRAG_PMTU if such DF fragment was the largest\n\t * frag seen to avoid sending tiny DF-fragments in case skb was built\n\t * from one very small df-fragment and one large non-df frag.\n\t */\n\tif (qp->max_df_size == qp->q.max_size) {\n\t\tIPCB(head)->flags |= IPSKB_FRAG_PMTU;\n\t\tiph->frag_off = htons(IP_DF);\n\t} else {\n\t\tiph->frag_off = 0;\n\t}\n\n\tip_send_check(iph);\n\n\t__IP_INC_STATS(net, IPSTATS_MIB_REASMOKS);\n\tqp->q.fragments = NULL;\n\tqp->q.rb_fragments = RB_ROOT;\n\tqp->q.fragments_tail = NULL;\n\tqp->q.last_run_head = NULL;\n\treturn 0;\n\nout_nomem:\n\tnet_dbg_ratelimited(\"queue_glue: no memory for gluing queue %p\\n\", qp);\n\terr = -ENOMEM;\n\tgoto out_fail;\nout_oversize:\n\tnet_info_ratelimited(\"Oversized IP packet from %pI4\\n\", &qp->q.key.v4.saddr);\nout_fail:\n\t__IP_INC_STATS(net, IPSTATS_MIB_REASMFAILS);\n\treturn err;\n}",
        "code_after_change": "static int ip_frag_reasm(struct ipq *qp, struct sk_buff *skb,\n\t\t\t struct sk_buff *prev_tail, struct net_device *dev)\n{\n\tstruct net *net = container_of(qp->q.net, struct net, ipv4.frags);\n\tstruct iphdr *iph;\n\tstruct sk_buff *fp, *head = skb_rb_first(&qp->q.rb_fragments);\n\tstruct sk_buff **nextp; /* To build frag_list. */\n\tstruct rb_node *rbn;\n\tint len;\n\tint ihlen;\n\tint err;\n\tu8 ecn;\n\n\tipq_kill(qp);\n\n\tecn = ip_frag_ecn_table[qp->ecn];\n\tif (unlikely(ecn == 0xff)) {\n\t\terr = -EINVAL;\n\t\tgoto out_fail;\n\t}\n\t/* Make the one we just received the head. */\n\tif (head != skb) {\n\t\tfp = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!fp)\n\t\t\tgoto out_nomem;\n\t\tFRAG_CB(fp)->next_frag = FRAG_CB(skb)->next_frag;\n\t\tif (RB_EMPTY_NODE(&skb->rbnode))\n\t\t\tFRAG_CB(prev_tail)->next_frag = fp;\n\t\telse\n\t\t\trb_replace_node(&skb->rbnode, &fp->rbnode,\n\t\t\t\t\t&qp->q.rb_fragments);\n\t\tif (qp->q.fragments_tail == skb)\n\t\t\tqp->q.fragments_tail = fp;\n\t\tskb_morph(skb, head);\n\t\tFRAG_CB(skb)->next_frag = FRAG_CB(head)->next_frag;\n\t\trb_replace_node(&head->rbnode, &skb->rbnode,\n\t\t\t\t&qp->q.rb_fragments);\n\t\tconsume_skb(head);\n\t\thead = skb;\n\t}\n\n\tWARN_ON(head->ip_defrag_offset != 0);\n\n\t/* Allocate a new buffer for the datagram. */\n\tihlen = ip_hdrlen(head);\n\tlen = ihlen + qp->q.len;\n\n\terr = -E2BIG;\n\tif (len > 65535)\n\t\tgoto out_oversize;\n\n\t/* Head of list must not be cloned. */\n\tif (skb_unclone(head, GFP_ATOMIC))\n\t\tgoto out_nomem;\n\n\t/* If the first fragment is fragmented itself, we split\n\t * it to two chunks: the first with data and paged part\n\t * and the second, holding only fragments. */\n\tif (skb_has_frag_list(head)) {\n\t\tstruct sk_buff *clone;\n\t\tint i, plen = 0;\n\n\t\tclone = alloc_skb(0, GFP_ATOMIC);\n\t\tif (!clone)\n\t\t\tgoto out_nomem;\n\t\tskb_shinfo(clone)->frag_list = skb_shinfo(head)->frag_list;\n\t\tskb_frag_list_init(head);\n\t\tfor (i = 0; i < skb_shinfo(head)->nr_frags; i++)\n\t\t\tplen += skb_frag_size(&skb_shinfo(head)->frags[i]);\n\t\tclone->len = clone->data_len = head->data_len - plen;\n\t\thead->truesize += clone->truesize;\n\t\tclone->csum = 0;\n\t\tclone->ip_summed = head->ip_summed;\n\t\tadd_frag_mem_limit(qp->q.net, clone->truesize);\n\t\tskb_shinfo(head)->frag_list = clone;\n\t\tnextp = &clone->next;\n\t} else {\n\t\tnextp = &skb_shinfo(head)->frag_list;\n\t}\n\n\tskb_push(head, head->data - skb_network_header(head));\n\n\t/* Traverse the tree in order, to build frag_list. */\n\tfp = FRAG_CB(head)->next_frag;\n\trbn = rb_next(&head->rbnode);\n\trb_erase(&head->rbnode, &qp->q.rb_fragments);\n\twhile (rbn || fp) {\n\t\t/* fp points to the next sk_buff in the current run;\n\t\t * rbn points to the next run.\n\t\t */\n\t\t/* Go through the current run. */\n\t\twhile (fp) {\n\t\t\t*nextp = fp;\n\t\t\tnextp = &fp->next;\n\t\t\tfp->prev = NULL;\n\t\t\tmemset(&fp->rbnode, 0, sizeof(fp->rbnode));\n\t\t\tfp->sk = NULL;\n\t\t\thead->data_len += fp->len;\n\t\t\thead->len += fp->len;\n\t\t\tif (head->ip_summed != fp->ip_summed)\n\t\t\t\thead->ip_summed = CHECKSUM_NONE;\n\t\t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n\t\t\t\thead->csum = csum_add(head->csum, fp->csum);\n\t\t\thead->truesize += fp->truesize;\n\t\t\tfp = FRAG_CB(fp)->next_frag;\n\t\t}\n\t\t/* Move to the next run. */\n\t\tif (rbn) {\n\t\t\tstruct rb_node *rbnext = rb_next(rbn);\n\n\t\t\tfp = rb_to_skb(rbn);\n\t\t\trb_erase(rbn, &qp->q.rb_fragments);\n\t\t\trbn = rbnext;\n\t\t}\n\t}\n\tsub_frag_mem_limit(qp->q.net, head->truesize);\n\n\t*nextp = NULL;\n\thead->next = NULL;\n\thead->prev = NULL;\n\thead->dev = dev;\n\thead->tstamp = qp->q.stamp;\n\tIPCB(head)->frag_max_size = max(qp->max_df_size, qp->q.max_size);\n\n\tiph = ip_hdr(head);\n\tiph->tot_len = htons(len);\n\tiph->tos |= ecn;\n\n\t/* When we set IP_DF on a refragmented skb we must also force a\n\t * call to ip_fragment to avoid forwarding a DF-skb of size s while\n\t * original sender only sent fragments of size f (where f < s).\n\t *\n\t * We only set DF/IPSKB_FRAG_PMTU if such DF fragment was the largest\n\t * frag seen to avoid sending tiny DF-fragments in case skb was built\n\t * from one very small df-fragment and one large non-df frag.\n\t */\n\tif (qp->max_df_size == qp->q.max_size) {\n\t\tIPCB(head)->flags |= IPSKB_FRAG_PMTU;\n\t\tiph->frag_off = htons(IP_DF);\n\t} else {\n\t\tiph->frag_off = 0;\n\t}\n\n\tip_send_check(iph);\n\n\t__IP_INC_STATS(net, IPSTATS_MIB_REASMOKS);\n\tqp->q.fragments = NULL;\n\tqp->q.rb_fragments = RB_ROOT;\n\tqp->q.fragments_tail = NULL;\n\tqp->q.last_run_head = NULL;\n\treturn 0;\n\nout_nomem:\n\tnet_dbg_ratelimited(\"queue_glue: no memory for gluing queue %p\\n\", qp);\n\terr = -ENOMEM;\n\tgoto out_fail;\nout_oversize:\n\tnet_info_ratelimited(\"Oversized IP packet from %pI4\\n\", &qp->q.key.v4.saddr);\nout_fail:\n\t__IP_INC_STATS(net, IPSTATS_MIB_REASMFAILS);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -94,6 +94,7 @@\n \t\t\tnextp = &fp->next;\n \t\t\tfp->prev = NULL;\n \t\t\tmemset(&fp->rbnode, 0, sizeof(fp->rbnode));\n+\t\t\tfp->sk = NULL;\n \t\t\thead->data_len += fp->len;\n \t\t\thead->len += fp->len;\n \t\t\tif (head->ip_summed != fp->ip_summed)",
        "function_modified_lines": {
            "added": [
                "\t\t\tfp->sk = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A security flaw was found in the ip_frag_reasm() function in net/ipv4/ip_fragment.c in the Linux kernel from 4.19-rc1 to 4.19-rc3 inclusive, which can cause a later system crash in ip_do_fragment(). With certain non-default, but non-rare, configuration of a victim host, an attacker can trigger this crash remotely, thus leading to a remote denial-of-service.",
        "id": 1699
    },
    {
        "cve_id": "CVE-2018-20669",
        "code_before_change": "int\ni915_gem_execbuffer2_ioctl(struct drm_device *dev, void *data,\n\t\t\t   struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list;\n\tstruct drm_syncobj **fences = NULL;\n\tconst size_t count = args->buffer_count;\n\tint err;\n\n\tif (!check_buffer_count(count)) {\n\t\tDRM_DEBUG(\"execbuf2 with %zd buffers\\n\", count);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!i915_gem_check_execbuffer(args))\n\t\treturn -EINVAL;\n\n\t/* Allocate an extra slot for use by the command parser */\n\texec2_list = kvmalloc_array(count + 1, eb_element_size(),\n\t\t\t\t    __GFP_NOWARN | GFP_KERNEL);\n\tif (exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %zd buffers\\n\",\n\t\t\t  count);\n\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user(exec2_list,\n\t\t\t   u64_to_user_ptr(args->buffers_ptr),\n\t\t\t   sizeof(*exec2_list) * count)) {\n\t\tDRM_DEBUG(\"copy %zd exec entries failed\\n\", count);\n\t\tkvfree(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tif (args->flags & I915_EXEC_FENCE_ARRAY) {\n\t\tfences = get_fence_array(args, file);\n\t\tif (IS_ERR(fences)) {\n\t\t\tkvfree(exec2_list);\n\t\t\treturn PTR_ERR(fences);\n\t\t}\n\t}\n\n\terr = i915_gem_do_execbuffer(dev, file, args, exec2_list, fences);\n\n\t/*\n\t * Now that we have begun execution of the batchbuffer, we ignore\n\t * any new error after this point. Also given that we have already\n\t * updated the associated relocations, we try to write out the current\n\t * object locations irrespective of any error.\n\t */\n\tif (args->flags & __EXEC_HAS_RELOC) {\n\t\tstruct drm_i915_gem_exec_object2 __user *user_exec_list =\n\t\t\tu64_to_user_ptr(args->buffers_ptr);\n\t\tunsigned int i;\n\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tuser_access_begin();\n\t\tfor (i = 0; i < args->buffer_count; i++) {\n\t\t\tif (!(exec2_list[i].offset & UPDATE))\n\t\t\t\tcontinue;\n\n\t\t\texec2_list[i].offset =\n\t\t\t\tgen8_canonical_addr(exec2_list[i].offset & PIN_OFFSET_MASK);\n\t\t\tunsafe_put_user(exec2_list[i].offset,\n\t\t\t\t\t&user_exec_list[i].offset,\n\t\t\t\t\tend_user);\n\t\t}\nend_user:\n\t\tuser_access_end();\n\t}\n\n\targs->flags &= ~__I915_EXEC_UNKNOWN_FLAGS;\n\tput_fence_array(args, fences);\n\tkvfree(exec2_list);\n\treturn err;\n}",
        "code_after_change": "int\ni915_gem_execbuffer2_ioctl(struct drm_device *dev, void *data,\n\t\t\t   struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list;\n\tstruct drm_syncobj **fences = NULL;\n\tconst size_t count = args->buffer_count;\n\tint err;\n\n\tif (!check_buffer_count(count)) {\n\t\tDRM_DEBUG(\"execbuf2 with %zd buffers\\n\", count);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!i915_gem_check_execbuffer(args))\n\t\treturn -EINVAL;\n\n\t/* Allocate an extra slot for use by the command parser */\n\texec2_list = kvmalloc_array(count + 1, eb_element_size(),\n\t\t\t\t    __GFP_NOWARN | GFP_KERNEL);\n\tif (exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %zd buffers\\n\",\n\t\t\t  count);\n\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user(exec2_list,\n\t\t\t   u64_to_user_ptr(args->buffers_ptr),\n\t\t\t   sizeof(*exec2_list) * count)) {\n\t\tDRM_DEBUG(\"copy %zd exec entries failed\\n\", count);\n\t\tkvfree(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tif (args->flags & I915_EXEC_FENCE_ARRAY) {\n\t\tfences = get_fence_array(args, file);\n\t\tif (IS_ERR(fences)) {\n\t\t\tkvfree(exec2_list);\n\t\t\treturn PTR_ERR(fences);\n\t\t}\n\t}\n\n\terr = i915_gem_do_execbuffer(dev, file, args, exec2_list, fences);\n\n\t/*\n\t * Now that we have begun execution of the batchbuffer, we ignore\n\t * any new error after this point. Also given that we have already\n\t * updated the associated relocations, we try to write out the current\n\t * object locations irrespective of any error.\n\t */\n\tif (args->flags & __EXEC_HAS_RELOC) {\n\t\tstruct drm_i915_gem_exec_object2 __user *user_exec_list =\n\t\t\tu64_to_user_ptr(args->buffers_ptr);\n\t\tunsigned int i;\n\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\t/*\n\t\t * Note: count * sizeof(*user_exec_list) does not overflow,\n\t\t * because we checked 'count' in check_buffer_count().\n\t\t *\n\t\t * And this range already got effectively checked earlier\n\t\t * when we did the \"copy_from_user()\" above.\n\t\t */\n\t\tif (!user_access_begin(user_exec_list, count * sizeof(*user_exec_list)))\n\t\t\tgoto end_user;\n\n\t\tfor (i = 0; i < args->buffer_count; i++) {\n\t\t\tif (!(exec2_list[i].offset & UPDATE))\n\t\t\t\tcontinue;\n\n\t\t\texec2_list[i].offset =\n\t\t\t\tgen8_canonical_addr(exec2_list[i].offset & PIN_OFFSET_MASK);\n\t\t\tunsafe_put_user(exec2_list[i].offset,\n\t\t\t\t\t&user_exec_list[i].offset,\n\t\t\t\t\tend_user);\n\t\t}\nend_user:\n\t\tuser_access_end();\n\t}\n\n\targs->flags &= ~__I915_EXEC_UNKNOWN_FLAGS;\n\tput_fence_array(args, fences);\n\tkvfree(exec2_list);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -54,7 +54,16 @@\n \t\tunsigned int i;\n \n \t\t/* Copy the new buffer offsets back to the user's exec list. */\n-\t\tuser_access_begin();\n+\t\t/*\n+\t\t * Note: count * sizeof(*user_exec_list) does not overflow,\n+\t\t * because we checked 'count' in check_buffer_count().\n+\t\t *\n+\t\t * And this range already got effectively checked earlier\n+\t\t * when we did the \"copy_from_user()\" above.\n+\t\t */\n+\t\tif (!user_access_begin(user_exec_list, count * sizeof(*user_exec_list)))\n+\t\t\tgoto end_user;\n+\n \t\tfor (i = 0; i < args->buffer_count; i++) {\n \t\t\tif (!(exec2_list[i].offset & UPDATE))\n \t\t\t\tcontinue;",
        "function_modified_lines": {
            "added": [
                "\t\t/*",
                "\t\t * Note: count * sizeof(*user_exec_list) does not overflow,",
                "\t\t * because we checked 'count' in check_buffer_count().",
                "\t\t *",
                "\t\t * And this range already got effectively checked earlier",
                "\t\t * when we did the \"copy_from_user()\" above.",
                "\t\t */",
                "\t\tif (!user_access_begin(user_exec_list, count * sizeof(*user_exec_list)))",
                "\t\t\tgoto end_user;",
                ""
            ],
            "deleted": [
                "\t\tuser_access_begin();"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "An issue where a provided address with access_ok() is not checked was discovered in i915_gem_execbuffer2_ioctl in drivers/gpu/drm/i915/i915_gem_execbuffer.c in the Linux kernel through 4.19.13. A local attacker can craft a malicious IOCTL function call to overwrite arbitrary kernel memory, resulting in a Denial of Service or privilege escalation.",
        "id": 1773
    },
    {
        "cve_id": "CVE-2013-6368",
        "code_before_change": "void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data, tpr;\n\tint max_irr, max_isr;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tvoid *vapic;\n\n\tapic_sync_pv_eoi_to_guest(vcpu, apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\ttpr = kvm_apic_get_reg(apic, APIC_TASKPRI) & 0xff;\n\tmax_irr = apic_find_highest_irr(apic);\n\tif (max_irr < 0)\n\t\tmax_irr = 0;\n\tmax_isr = apic_find_highest_isr(apic);\n\tif (max_isr < 0)\n\t\tmax_isr = 0;\n\tdata = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);\n\n\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);\n\t*(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr)) = data;\n\tkunmap_atomic(vapic);\n}",
        "code_after_change": "void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data, tpr;\n\tint max_irr, max_isr;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tapic_sync_pv_eoi_to_guest(vcpu, apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\ttpr = kvm_apic_get_reg(apic, APIC_TASKPRI) & 0xff;\n\tmax_irr = apic_find_highest_irr(apic);\n\tif (max_irr < 0)\n\t\tmax_irr = 0;\n\tmax_isr = apic_find_highest_isr(apic);\n\tif (max_isr < 0)\n\t\tmax_isr = 0;\n\tdata = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);\n\n\tkvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n\t\t\t\tsizeof(u32));\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,6 @@\n \tu32 data, tpr;\n \tint max_irr, max_isr;\n \tstruct kvm_lapic *apic = vcpu->arch.apic;\n-\tvoid *vapic;\n \n \tapic_sync_pv_eoi_to_guest(vcpu, apic);\n \n@@ -19,7 +18,6 @@\n \t\tmax_isr = 0;\n \tdata = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);\n \n-\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);\n-\t*(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr)) = data;\n-\tkunmap_atomic(vapic);\n+\tkvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n+\t\t\t\tsizeof(u32));\n }",
        "function_modified_lines": {
            "added": [
                "\tkvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,",
                "\t\t\t\tsizeof(u32));"
            ],
            "deleted": [
                "\tvoid *vapic;",
                "\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);",
                "\t*(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr)) = data;",
                "\tkunmap_atomic(vapic);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The KVM subsystem in the Linux kernel through 3.12.5 allows local users to gain privileges or cause a denial of service (system crash) via a VAPIC synchronization operation involving a page-end address.",
        "id": 341
    },
    {
        "cve_id": "CVE-2008-7316",
        "code_before_change": "static void __iov_iter_advance_iov(struct iov_iter *i, size_t bytes)\n{\n\tif (likely(i->nr_segs == 1)) {\n\t\ti->iov_offset += bytes;\n\t} else {\n\t\tconst struct iovec *iov = i->iov;\n\t\tsize_t base = i->iov_offset;\n\n\t\twhile (bytes) {\n\t\t\tint copy = min(bytes, iov->iov_len - base);\n\n\t\t\tbytes -= copy;\n\t\t\tbase += copy;\n\t\t\tif (iov->iov_len == base) {\n\t\t\t\tiov++;\n\t\t\t\tbase = 0;\n\t\t\t}\n\t\t}\n\t\ti->iov = iov;\n\t\ti->iov_offset = base;\n\t}\n}",
        "code_after_change": "static void __iov_iter_advance_iov(struct iov_iter *i, size_t bytes)\n{\n\tif (likely(i->nr_segs == 1)) {\n\t\ti->iov_offset += bytes;\n\t} else {\n\t\tconst struct iovec *iov = i->iov;\n\t\tsize_t base = i->iov_offset;\n\n\t\t/*\n\t\t * The !iov->iov_len check ensures we skip over unlikely\n\t\t * zero-length segments.\n\t\t */\n\t\twhile (bytes || !iov->iov_len) {\n\t\t\tint copy = min(bytes, iov->iov_len - base);\n\n\t\t\tbytes -= copy;\n\t\t\tbase += copy;\n\t\t\tif (iov->iov_len == base) {\n\t\t\t\tiov++;\n\t\t\t\tbase = 0;\n\t\t\t}\n\t\t}\n\t\ti->iov = iov;\n\t\ti->iov_offset = base;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,11 @@\n \t\tconst struct iovec *iov = i->iov;\n \t\tsize_t base = i->iov_offset;\n \n-\t\twhile (bytes) {\n+\t\t/*\n+\t\t * The !iov->iov_len check ensures we skip over unlikely\n+\t\t * zero-length segments.\n+\t\t */\n+\t\twhile (bytes || !iov->iov_len) {\n \t\t\tint copy = min(bytes, iov->iov_len - base);\n \n \t\t\tbytes -= copy;",
        "function_modified_lines": {
            "added": [
                "\t\t/*",
                "\t\t * The !iov->iov_len check ensures we skip over unlikely",
                "\t\t * zero-length segments.",
                "\t\t */",
                "\t\twhile (bytes || !iov->iov_len) {"
            ],
            "deleted": [
                "\t\twhile (bytes) {"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "mm/filemap.c in the Linux kernel before 2.6.25 allows local users to cause a denial of service (infinite loop) via a writev system call that triggers an iovec of zero length, followed by a page fault for an iovec of nonzero length.",
        "id": 9
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int\nmISDN_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t   struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sk_buff\t\t*skb;\n\tstruct sock\t\t*sk = sock->sk;\n\tstruct sockaddr_mISDN\t*maddr;\n\n\tint\t\tcopied, err;\n\n\tif (*debug & DEBUG_SOCKET)\n\t\tprintk(KERN_DEBUG \"%s: len %d, flags %x ch.nr %d, proto %x\\n\",\n\t\t       __func__, (int)len, flags, _pms(sk)->ch.nr,\n\t\t       sk->sk_protocol);\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\tif (sk->sk_state == MISDN_CLOSED)\n\t\treturn 0;\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tif (msg->msg_namelen >= sizeof(struct sockaddr_mISDN)) {\n\t\tmsg->msg_namelen = sizeof(struct sockaddr_mISDN);\n\t\tmaddr = (struct sockaddr_mISDN *)msg->msg_name;\n\t\tmaddr->family = AF_ISDN;\n\t\tmaddr->dev = _pms(sk)->dev->id;\n\t\tif ((sk->sk_protocol == ISDN_P_LAPD_TE) ||\n\t\t    (sk->sk_protocol == ISDN_P_LAPD_NT)) {\n\t\t\tmaddr->channel = (mISDN_HEAD_ID(skb) >> 16) & 0xff;\n\t\t\tmaddr->tei =  (mISDN_HEAD_ID(skb) >> 8) & 0xff;\n\t\t\tmaddr->sapi = mISDN_HEAD_ID(skb) & 0xff;\n\t\t} else {\n\t\t\tmaddr->channel = _pms(sk)->ch.nr;\n\t\t\tmaddr->sapi = _pms(sk)->ch.addr & 0xFF;\n\t\t\tmaddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;\n\t\t}\n\t} else {\n\t\tif (msg->msg_namelen)\n\t\t\tprintk(KERN_WARNING \"%s: too small namelen %d\\n\",\n\t\t\t       __func__, msg->msg_namelen);\n\t\tmsg->msg_namelen = 0;\n\t}\n\n\tcopied = skb->len + MISDN_HEADER_LEN;\n\tif (len < copied) {\n\t\tif (flags & MSG_PEEK)\n\t\t\tatomic_dec(&skb->users);\n\t\telse\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn -ENOSPC;\n\t}\n\tmemcpy(skb_push(skb, MISDN_HEADER_LEN), mISDN_HEAD_P(skb),\n\t       MISDN_HEADER_LEN);\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tmISDN_sock_cmsg(sk, msg, skb);\n\n\tskb_free_datagram(sk, skb);\n\n\treturn err ? : copied;\n}",
        "code_after_change": "static int\nmISDN_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t   struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sk_buff\t\t*skb;\n\tstruct sock\t\t*sk = sock->sk;\n\n\tint\t\tcopied, err;\n\n\tif (*debug & DEBUG_SOCKET)\n\t\tprintk(KERN_DEBUG \"%s: len %d, flags %x ch.nr %d, proto %x\\n\",\n\t\t       __func__, (int)len, flags, _pms(sk)->ch.nr,\n\t\t       sk->sk_protocol);\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\tif (sk->sk_state == MISDN_CLOSED)\n\t\treturn 0;\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_mISDN *maddr = msg->msg_name;\n\n\t\tmaddr->family = AF_ISDN;\n\t\tmaddr->dev = _pms(sk)->dev->id;\n\t\tif ((sk->sk_protocol == ISDN_P_LAPD_TE) ||\n\t\t    (sk->sk_protocol == ISDN_P_LAPD_NT)) {\n\t\t\tmaddr->channel = (mISDN_HEAD_ID(skb) >> 16) & 0xff;\n\t\t\tmaddr->tei =  (mISDN_HEAD_ID(skb) >> 8) & 0xff;\n\t\t\tmaddr->sapi = mISDN_HEAD_ID(skb) & 0xff;\n\t\t} else {\n\t\t\tmaddr->channel = _pms(sk)->ch.nr;\n\t\t\tmaddr->sapi = _pms(sk)->ch.addr & 0xFF;\n\t\t\tmaddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;\n\t\t}\n\t\tmsg->msg_namelen = sizeof(*maddr);\n\t}\n\n\tcopied = skb->len + MISDN_HEADER_LEN;\n\tif (len < copied) {\n\t\tif (flags & MSG_PEEK)\n\t\t\tatomic_dec(&skb->users);\n\t\telse\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn -ENOSPC;\n\t}\n\tmemcpy(skb_push(skb, MISDN_HEADER_LEN), mISDN_HEAD_P(skb),\n\t       MISDN_HEADER_LEN);\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tmISDN_sock_cmsg(sk, msg, skb);\n\n\tskb_free_datagram(sk, skb);\n\n\treturn err ? : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n {\n \tstruct sk_buff\t\t*skb;\n \tstruct sock\t\t*sk = sock->sk;\n-\tstruct sockaddr_mISDN\t*maddr;\n \n \tint\t\tcopied, err;\n \n@@ -22,9 +21,9 @@\n \tif (!skb)\n \t\treturn err;\n \n-\tif (msg->msg_namelen >= sizeof(struct sockaddr_mISDN)) {\n-\t\tmsg->msg_namelen = sizeof(struct sockaddr_mISDN);\n-\t\tmaddr = (struct sockaddr_mISDN *)msg->msg_name;\n+\tif (msg->msg_name) {\n+\t\tstruct sockaddr_mISDN *maddr = msg->msg_name;\n+\n \t\tmaddr->family = AF_ISDN;\n \t\tmaddr->dev = _pms(sk)->dev->id;\n \t\tif ((sk->sk_protocol == ISDN_P_LAPD_TE) ||\n@@ -37,11 +36,7 @@\n \t\t\tmaddr->sapi = _pms(sk)->ch.addr & 0xFF;\n \t\t\tmaddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;\n \t\t}\n-\t} else {\n-\t\tif (msg->msg_namelen)\n-\t\t\tprintk(KERN_WARNING \"%s: too small namelen %d\\n\",\n-\t\t\t       __func__, msg->msg_namelen);\n-\t\tmsg->msg_namelen = 0;\n+\t\tmsg->msg_namelen = sizeof(*maddr);\n \t}\n \n \tcopied = skb->len + MISDN_HEADER_LEN;",
        "function_modified_lines": {
            "added": [
                "\tif (msg->msg_name) {",
                "\t\tstruct sockaddr_mISDN *maddr = msg->msg_name;",
                "",
                "\t\tmsg->msg_namelen = sizeof(*maddr);"
            ],
            "deleted": [
                "\tstruct sockaddr_mISDN\t*maddr;",
                "\tif (msg->msg_namelen >= sizeof(struct sockaddr_mISDN)) {",
                "\t\tmsg->msg_namelen = sizeof(struct sockaddr_mISDN);",
                "\t\tmaddr = (struct sockaddr_mISDN *)msg->msg_name;",
                "\t} else {",
                "\t\tif (msg->msg_namelen)",
                "\t\t\tprintk(KERN_WARNING \"%s: too small namelen %d\\n\",",
                "\t\t\t       __func__, msg->msg_namelen);",
                "\t\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 368
    },
    {
        "cve_id": "CVE-2012-4398",
        "code_before_change": "int drbd_khelper(struct drbd_conf *mdev, char *cmd)\n{\n\tchar *envp[] = { \"HOME=/\",\n\t\t\t\"TERM=linux\",\n\t\t\t\"PATH=/sbin:/usr/sbin:/bin:/usr/bin\",\n\t\t\tNULL, /* Will be set to address family */\n\t\t\tNULL, /* Will be set to address */\n\t\t\tNULL };\n\n\tchar mb[12], af[20], ad[60], *afs;\n\tchar *argv[] = {usermode_helper, cmd, mb, NULL };\n\tint ret;\n\n\tsnprintf(mb, 12, \"minor-%d\", mdev_to_minor(mdev));\n\n\tif (get_net_conf(mdev)) {\n\t\tswitch (((struct sockaddr *)mdev->net_conf->peer_addr)->sa_family) {\n\t\tcase AF_INET6:\n\t\t\tafs = \"ipv6\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI6\",\n\t\t\t\t &((struct sockaddr_in6 *)mdev->net_conf->peer_addr)->sin6_addr);\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tafs = \"ipv4\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI4\",\n\t\t\t\t &((struct sockaddr_in *)mdev->net_conf->peer_addr)->sin_addr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tafs = \"ssocks\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI4\",\n\t\t\t\t &((struct sockaddr_in *)mdev->net_conf->peer_addr)->sin_addr);\n\t\t}\n\t\tsnprintf(af, 20, \"DRBD_PEER_AF=%s\", afs);\n\t\tenvp[3]=af;\n\t\tenvp[4]=ad;\n\t\tput_net_conf(mdev);\n\t}\n\n\t/* The helper may take some time.\n\t * write out any unsynced meta data changes now */\n\tdrbd_md_sync(mdev);\n\n\tdev_info(DEV, \"helper command: %s %s %s\\n\", usermode_helper, cmd, mb);\n\n\tdrbd_bcast_ev_helper(mdev, cmd);\n\tret = call_usermodehelper(usermode_helper, argv, envp, 1);\n\tif (ret)\n\t\tdev_warn(DEV, \"helper command: %s %s %s exit code %u (0x%x)\\n\",\n\t\t\t\tusermode_helper, cmd, mb,\n\t\t\t\t(ret >> 8) & 0xff, ret);\n\telse\n\t\tdev_info(DEV, \"helper command: %s %s %s exit code %u (0x%x)\\n\",\n\t\t\t\tusermode_helper, cmd, mb,\n\t\t\t\t(ret >> 8) & 0xff, ret);\n\n\tif (ret < 0) /* Ignore any ERRNOs we got. */\n\t\tret = 0;\n\n\treturn ret;\n}",
        "code_after_change": "int drbd_khelper(struct drbd_conf *mdev, char *cmd)\n{\n\tchar *envp[] = { \"HOME=/\",\n\t\t\t\"TERM=linux\",\n\t\t\t\"PATH=/sbin:/usr/sbin:/bin:/usr/bin\",\n\t\t\tNULL, /* Will be set to address family */\n\t\t\tNULL, /* Will be set to address */\n\t\t\tNULL };\n\n\tchar mb[12], af[20], ad[60], *afs;\n\tchar *argv[] = {usermode_helper, cmd, mb, NULL };\n\tint ret;\n\n\tsnprintf(mb, 12, \"minor-%d\", mdev_to_minor(mdev));\n\n\tif (get_net_conf(mdev)) {\n\t\tswitch (((struct sockaddr *)mdev->net_conf->peer_addr)->sa_family) {\n\t\tcase AF_INET6:\n\t\t\tafs = \"ipv6\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI6\",\n\t\t\t\t &((struct sockaddr_in6 *)mdev->net_conf->peer_addr)->sin6_addr);\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tafs = \"ipv4\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI4\",\n\t\t\t\t &((struct sockaddr_in *)mdev->net_conf->peer_addr)->sin_addr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tafs = \"ssocks\";\n\t\t\tsnprintf(ad, 60, \"DRBD_PEER_ADDRESS=%pI4\",\n\t\t\t\t &((struct sockaddr_in *)mdev->net_conf->peer_addr)->sin_addr);\n\t\t}\n\t\tsnprintf(af, 20, \"DRBD_PEER_AF=%s\", afs);\n\t\tenvp[3]=af;\n\t\tenvp[4]=ad;\n\t\tput_net_conf(mdev);\n\t}\n\n\t/* The helper may take some time.\n\t * write out any unsynced meta data changes now */\n\tdrbd_md_sync(mdev);\n\n\tdev_info(DEV, \"helper command: %s %s %s\\n\", usermode_helper, cmd, mb);\n\n\tdrbd_bcast_ev_helper(mdev, cmd);\n\tret = call_usermodehelper(usermode_helper, argv, envp, UMH_WAIT_PROC);\n\tif (ret)\n\t\tdev_warn(DEV, \"helper command: %s %s %s exit code %u (0x%x)\\n\",\n\t\t\t\tusermode_helper, cmd, mb,\n\t\t\t\t(ret >> 8) & 0xff, ret);\n\telse\n\t\tdev_info(DEV, \"helper command: %s %s %s exit code %u (0x%x)\\n\",\n\t\t\t\tusermode_helper, cmd, mb,\n\t\t\t\t(ret >> 8) & 0xff, ret);\n\n\tif (ret < 0) /* Ignore any ERRNOs we got. */\n\t\tret = 0;\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,7 +43,7 @@\n \tdev_info(DEV, \"helper command: %s %s %s\\n\", usermode_helper, cmd, mb);\n \n \tdrbd_bcast_ev_helper(mdev, cmd);\n-\tret = call_usermodehelper(usermode_helper, argv, envp, 1);\n+\tret = call_usermodehelper(usermode_helper, argv, envp, UMH_WAIT_PROC);\n \tif (ret)\n \t\tdev_warn(DEV, \"helper command: %s %s %s exit code %u (0x%x)\\n\",\n \t\t\t\tusermode_helper, cmd, mb,",
        "function_modified_lines": {
            "added": [
                "\tret = call_usermodehelper(usermode_helper, argv, envp, UMH_WAIT_PROC);"
            ],
            "deleted": [
                "\tret = call_usermodehelper(usermode_helper, argv, envp, 1);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The __request_module function in kernel/kmod.c in the Linux kernel before 3.4 does not set a certain killable attribute, which allows local users to cause a denial of service (memory consumption) via a crafted application.",
        "id": 95
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int unix_dgram_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t      struct msghdr *msg, size_t size,\n\t\t\t      int flags)\n{\n\tstruct sock_iocb *siocb = kiocb_to_siocb(iocb);\n\tstruct scm_cookie tmp_scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sk_buff *skb;\n\tint err;\n\tint peeked, skip;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\tmsg->msg_namelen = 0;\n\n\terr = mutex_lock_interruptible(&u->readlock);\n\tif (err) {\n\t\terr = sock_intr_errno(sock_rcvtimeo(sk, noblock));\n\t\tgoto out;\n\t}\n\n\tskip = sk_peek_offset(sk, flags);\n\n\tskb = __skb_recv_datagram(sk, flags, &peeked, &skip, &err);\n\tif (!skb) {\n\t\tunix_state_lock(sk);\n\t\t/* Signal EOF on disconnected non-blocking SEQPACKET socket. */\n\t\tif (sk->sk_type == SOCK_SEQPACKET && err == -EAGAIN &&\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\t\terr = 0;\n\t\tunix_state_unlock(sk);\n\t\tgoto out_unlock;\n\t}\n\n\twake_up_interruptible_sync_poll(&u->peer_wait,\n\t\t\t\t\tPOLLOUT | POLLWRNORM | POLLWRBAND);\n\n\tif (msg->msg_name)\n\t\tunix_copy_addr(msg, skb->sk);\n\n\tif (size > skb->len - skip)\n\t\tsize = skb->len - skip;\n\telse if (size < skb->len - skip)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\terr = skb_copy_datagram_iovec(skb, skip, msg->msg_iov, size);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock_flag(sk, SOCK_RCVTSTAMP))\n\t\t__sock_recv_timestamp(msg, sk, skb);\n\n\tif (!siocb->scm) {\n\t\tsiocb->scm = &tmp_scm;\n\t\tmemset(&tmp_scm, 0, sizeof(tmp_scm));\n\t}\n\tscm_set_cred(siocb->scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\tunix_set_secdata(siocb->scm, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tif (UNIXCB(skb).fp)\n\t\t\tunix_detach_fds(siocb->scm, skb);\n\n\t\tsk_peek_offset_bwd(sk, skb->len);\n\t} else {\n\t\t/* It is questionable: on PEEK we could:\n\t\t   - do not return fds - good, but too simple 8)\n\t\t   - return fds, and do not return them on read (old strategy,\n\t\t     apparently wrong)\n\t\t   - clone fds (I chose it for now, it is the most universal\n\t\t     solution)\n\n\t\t   POSIX 1003.1g does not actually define this clearly\n\t\t   at all. POSIX 1003.1g doesn't define a lot of things\n\t\t   clearly however!\n\n\t\t*/\n\n\t\tsk_peek_offset_fwd(sk, size);\n\n\t\tif (UNIXCB(skb).fp)\n\t\t\tsiocb->scm->fp = scm_fp_dup(UNIXCB(skb).fp);\n\t}\n\terr = (flags & MSG_TRUNC) ? skb->len - skip : size;\n\n\tscm_recv(sock, msg, siocb->scm, flags);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout_unlock:\n\tmutex_unlock(&u->readlock);\nout:\n\treturn err;\n}",
        "code_after_change": "static int unix_dgram_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t      struct msghdr *msg, size_t size,\n\t\t\t      int flags)\n{\n\tstruct sock_iocb *siocb = kiocb_to_siocb(iocb);\n\tstruct scm_cookie tmp_scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sk_buff *skb;\n\tint err;\n\tint peeked, skip;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\terr = mutex_lock_interruptible(&u->readlock);\n\tif (err) {\n\t\terr = sock_intr_errno(sock_rcvtimeo(sk, noblock));\n\t\tgoto out;\n\t}\n\n\tskip = sk_peek_offset(sk, flags);\n\n\tskb = __skb_recv_datagram(sk, flags, &peeked, &skip, &err);\n\tif (!skb) {\n\t\tunix_state_lock(sk);\n\t\t/* Signal EOF on disconnected non-blocking SEQPACKET socket. */\n\t\tif (sk->sk_type == SOCK_SEQPACKET && err == -EAGAIN &&\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\t\terr = 0;\n\t\tunix_state_unlock(sk);\n\t\tgoto out_unlock;\n\t}\n\n\twake_up_interruptible_sync_poll(&u->peer_wait,\n\t\t\t\t\tPOLLOUT | POLLWRNORM | POLLWRBAND);\n\n\tif (msg->msg_name)\n\t\tunix_copy_addr(msg, skb->sk);\n\n\tif (size > skb->len - skip)\n\t\tsize = skb->len - skip;\n\telse if (size < skb->len - skip)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\terr = skb_copy_datagram_iovec(skb, skip, msg->msg_iov, size);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock_flag(sk, SOCK_RCVTSTAMP))\n\t\t__sock_recv_timestamp(msg, sk, skb);\n\n\tif (!siocb->scm) {\n\t\tsiocb->scm = &tmp_scm;\n\t\tmemset(&tmp_scm, 0, sizeof(tmp_scm));\n\t}\n\tscm_set_cred(siocb->scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\tunix_set_secdata(siocb->scm, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tif (UNIXCB(skb).fp)\n\t\t\tunix_detach_fds(siocb->scm, skb);\n\n\t\tsk_peek_offset_bwd(sk, skb->len);\n\t} else {\n\t\t/* It is questionable: on PEEK we could:\n\t\t   - do not return fds - good, but too simple 8)\n\t\t   - return fds, and do not return them on read (old strategy,\n\t\t     apparently wrong)\n\t\t   - clone fds (I chose it for now, it is the most universal\n\t\t     solution)\n\n\t\t   POSIX 1003.1g does not actually define this clearly\n\t\t   at all. POSIX 1003.1g doesn't define a lot of things\n\t\t   clearly however!\n\n\t\t*/\n\n\t\tsk_peek_offset_fwd(sk, size);\n\n\t\tif (UNIXCB(skb).fp)\n\t\t\tsiocb->scm->fp = scm_fp_dup(UNIXCB(skb).fp);\n\t}\n\terr = (flags & MSG_TRUNC) ? skb->len - skip : size;\n\n\tscm_recv(sock, msg, siocb->scm, flags);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout_unlock:\n\tmutex_unlock(&u->readlock);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,8 +14,6 @@\n \terr = -EOPNOTSUPP;\n \tif (flags&MSG_OOB)\n \t\tgoto out;\n-\n-\tmsg->msg_namelen = 0;\n \n \terr = mutex_lock_interruptible(&u->readlock);\n \tif (err) {",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 403
    },
    {
        "cve_id": "CVE-2013-7263",
        "code_before_change": "static int dgram_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\tstruct msghdr *msg, size_t len, int noblock, int flags,\n\t\tint *addr_len)\n{\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sk_buff *skb;\n\tstruct sockaddr_ieee802154 *saddr;\n\n\tsaddr = (struct sockaddr_ieee802154 *)msg->msg_name;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\t/* FIXME: skip headers if necessary ?! */\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (saddr) {\n\t\tsaddr->family = AF_IEEE802154;\n\t\tsaddr->addr = mac_cb(skb)->sa;\n\t}\n\tif (addr_len)\n\t\t*addr_len = sizeof(*saddr);\n\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err)\n\t\treturn err;\n\treturn copied;\n}",
        "code_after_change": "static int dgram_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\tstruct msghdr *msg, size_t len, int noblock, int flags,\n\t\tint *addr_len)\n{\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sk_buff *skb;\n\tstruct sockaddr_ieee802154 *saddr;\n\n\tsaddr = (struct sockaddr_ieee802154 *)msg->msg_name;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\t/* FIXME: skip headers if necessary ?! */\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (saddr) {\n\t\tsaddr->family = AF_IEEE802154;\n\t\tsaddr->addr = mac_cb(skb)->sa;\n\t\t*addr_len = sizeof(*saddr);\n\t}\n\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err)\n\t\treturn err;\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,9 +29,8 @@\n \tif (saddr) {\n \t\tsaddr->family = AF_IEEE802154;\n \t\tsaddr->addr = mac_cb(skb)->sa;\n+\t\t*addr_len = sizeof(*saddr);\n \t}\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(*saddr);\n \n \tif (flags & MSG_TRUNC)\n \t\tcopied = skb->len;",
        "function_modified_lines": {
            "added": [
                "\t\t*addr_len = sizeof(*saddr);"
            ],
            "deleted": [
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(*saddr);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Linux kernel before 3.12.4 updates certain length values before ensuring that associated data structures have been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call, related to net/ipv4/ping.c, net/ipv4/raw.c, net/ipv4/udp.c, net/ipv6/raw.c, and net/ipv6/udp.c.",
        "id": 359
    },
    {
        "cve_id": "CVE-2020-0041",
        "code_before_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\tstrscpy(e->context_name, proc->context->name, BINDERFS_MAX_NAME);\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc->pid == proc->pid) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\n\t\tsecurity_task_getsecid(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tint err;\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\terr = binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  t->buffer, buf_offset,\n\t\t\t\t\t\t  secctx, secctx_sz);\n\t\tif (err) {\n\t\t\tt->security_ctx = 0;\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size -\n\t\tALIGN(secctx_sz, sizeof(u64));\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  &object_offset,\n\t\t\t\t\t\t  t->buffer,\n\t\t\t\t\t\t  buffer_offset,\n\t\t\t\t\t\t  sizeof(object_offset))) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tfp->pad_binder = 0;\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) *\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) *\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tbp, sizeof(*bp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
        "code_after_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\tstrscpy(e->context_name, proc->context->name, BINDERFS_MAX_NAME);\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc->pid == proc->pid) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\n\t\tsecurity_task_getsecid(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tint err;\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\terr = binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  t->buffer, buf_offset,\n\t\t\t\t\t\t  secctx, secctx_sz);\n\t\tif (err) {\n\t\t\tt->security_ctx = 0;\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size -\n\t\tALIGN(secctx_sz, sizeof(u64));\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  &object_offset,\n\t\t\t\t\t\t  t->buffer,\n\t\t\t\t\t\t  buffer_offset,\n\t\t\t\t\t\t  sizeof(object_offset))) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tfp->pad_binder = 0;\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tbp, sizeof(*bp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -474,7 +474,7 @@\n \t\t\tbinder_size_t parent_offset;\n \t\t\tstruct binder_fd_array_object *fda =\n \t\t\t\tto_binder_fd_array_object(hdr);\n-\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) *\n+\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) /\n \t\t\t\t\t\tsizeof(binder_size_t);\n \t\t\tstruct binder_buffer_object *parent =\n \t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n@@ -548,7 +548,7 @@\n \t\t\t\tt->buffer->user_data + sg_buf_offset;\n \t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n \n-\t\t\tnum_valid = (buffer_offset - off_start_offset) *\n+\t\t\tnum_valid = (buffer_offset - off_start_offset) /\n \t\t\t\t\tsizeof(binder_size_t);\n \t\t\tret = binder_fixup_parent(t, thread, bp,\n \t\t\t\t\t\t  off_start_offset,",
        "function_modified_lines": {
            "added": [
                "\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) /",
                "\t\t\tnum_valid = (buffer_offset - off_start_offset) /"
            ],
            "deleted": [
                "\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) *",
                "\t\t\tnum_valid = (buffer_offset - off_start_offset) *"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "In binder_transaction of binder.c, there is a possible out of bounds write due to an incorrect bounds check. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-145988638References: Upstream kernel",
        "id": 2373
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static int io_grab_files(struct io_kiocb *req)\n{\n\tint ret = -EBADF;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->work.files || (req->flags & REQ_F_NO_FILE_TABLE))\n\t\treturn 0;\n\tif (!ctx->ring_file)\n\t\treturn -EBADF;\n\n\trcu_read_lock();\n\tspin_lock_irq(&ctx->inflight_lock);\n\t/*\n\t * We use the f_ops->flush() handler to ensure that we can flush\n\t * out work accessing these files if the fd is closed. Check if\n\t * the fd has changed since we started down this path, and disallow\n\t * this operation if it has.\n\t */\n\tif (fcheck(ctx->ring_fd) == ctx->ring_file) {\n\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n\t\treq->flags |= REQ_F_INFLIGHT;\n\t\treq->work.files = current->files;\n\t\tret = 0;\n\t}\n\tspin_unlock_irq(&ctx->inflight_lock);\n\trcu_read_unlock();\n\n\treturn ret;\n}",
        "code_after_change": "static int io_grab_files(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->work.files || (req->flags & REQ_F_NO_FILE_TABLE))\n\t\treturn 0;\n\n\treq->work.files = get_files_struct(current);\n\treq->flags |= REQ_F_INFLIGHT;\n\n\tspin_lock_irq(&ctx->inflight_lock);\n\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n\tspin_unlock_irq(&ctx->inflight_lock);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,31 +1,17 @@\n static int io_grab_files(struct io_kiocb *req)\n {\n-\tint ret = -EBADF;\n \tstruct io_ring_ctx *ctx = req->ctx;\n \n \tio_req_init_async(req);\n \n \tif (req->work.files || (req->flags & REQ_F_NO_FILE_TABLE))\n \t\treturn 0;\n-\tif (!ctx->ring_file)\n-\t\treturn -EBADF;\n \n-\trcu_read_lock();\n+\treq->work.files = get_files_struct(current);\n+\treq->flags |= REQ_F_INFLIGHT;\n+\n \tspin_lock_irq(&ctx->inflight_lock);\n-\t/*\n-\t * We use the f_ops->flush() handler to ensure that we can flush\n-\t * out work accessing these files if the fd is closed. Check if\n-\t * the fd has changed since we started down this path, and disallow\n-\t * this operation if it has.\n-\t */\n-\tif (fcheck(ctx->ring_fd) == ctx->ring_file) {\n-\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n-\t\treq->flags |= REQ_F_INFLIGHT;\n-\t\treq->work.files = current->files;\n-\t\tret = 0;\n-\t}\n+\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n \tspin_unlock_irq(&ctx->inflight_lock);\n-\trcu_read_unlock();\n-\n-\treturn ret;\n+\treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\treq->work.files = get_files_struct(current);",
                "\treq->flags |= REQ_F_INFLIGHT;",
                "",
                "\tlist_add(&req->inflight_entry, &ctx->inflight_list);",
                "\treturn 0;"
            ],
            "deleted": [
                "\tint ret = -EBADF;",
                "\tif (!ctx->ring_file)",
                "\t\treturn -EBADF;",
                "\trcu_read_lock();",
                "\t/*",
                "\t * We use the f_ops->flush() handler to ensure that we can flush",
                "\t * out work accessing these files if the fd is closed. Check if",
                "\t * the fd has changed since we started down this path, and disallow",
                "\t * this operation if it has.",
                "\t */",
                "\tif (fcheck(ctx->ring_fd) == ctx->ring_file) {",
                "\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);",
                "\t\treq->flags |= REQ_F_INFLIGHT;",
                "\t\treq->work.files = current->files;",
                "\t\tret = 0;",
                "\t}",
                "\trcu_read_unlock();",
                "",
                "\treturn ret;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2849
    },
    {
        "cve_id": "CVE-2015-7872",
        "code_before_change": "static noinline void key_gc_unused_keys(struct list_head *keys)\n{\n\twhile (!list_empty(keys)) {\n\t\tstruct key *key =\n\t\t\tlist_entry(keys->next, struct key, graveyard_link);\n\t\tlist_del(&key->graveyard_link);\n\n\t\tkdebug(\"- %u\", key->serial);\n\t\tkey_check(key);\n\n\t\t/* Throw away the key data */\n\t\tif (key->type->destroy)\n\t\t\tkey->type->destroy(key);\n\n\t\tsecurity_key_free(key);\n\n\t\t/* deal with the user's key tracking and quota */\n\t\tif (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {\n\t\t\tspin_lock(&key->user->lock);\n\t\t\tkey->user->qnkeys--;\n\t\t\tkey->user->qnbytes -= key->quotalen;\n\t\t\tspin_unlock(&key->user->lock);\n\t\t}\n\n\t\tatomic_dec(&key->user->nkeys);\n\t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))\n\t\t\tatomic_dec(&key->user->nikeys);\n\n\t\tkey_user_put(key->user);\n\n\t\tkfree(key->description);\n\n#ifdef KEY_DEBUGGING\n\t\tkey->magic = KEY_DEBUG_MAGIC_X;\n#endif\n\t\tkmem_cache_free(key_jar, key);\n\t}\n}",
        "code_after_change": "static noinline void key_gc_unused_keys(struct list_head *keys)\n{\n\twhile (!list_empty(keys)) {\n\t\tstruct key *key =\n\t\t\tlist_entry(keys->next, struct key, graveyard_link);\n\t\tlist_del(&key->graveyard_link);\n\n\t\tkdebug(\"- %u\", key->serial);\n\t\tkey_check(key);\n\n\t\t/* Throw away the key data if the key is instantiated */\n\t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags) &&\n\t\t    !test_bit(KEY_FLAG_NEGATIVE, &key->flags) &&\n\t\t    key->type->destroy)\n\t\t\tkey->type->destroy(key);\n\n\t\tsecurity_key_free(key);\n\n\t\t/* deal with the user's key tracking and quota */\n\t\tif (test_bit(KEY_FLAG_IN_QUOTA, &key->flags)) {\n\t\t\tspin_lock(&key->user->lock);\n\t\t\tkey->user->qnkeys--;\n\t\t\tkey->user->qnbytes -= key->quotalen;\n\t\t\tspin_unlock(&key->user->lock);\n\t\t}\n\n\t\tatomic_dec(&key->user->nkeys);\n\t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))\n\t\t\tatomic_dec(&key->user->nikeys);\n\n\t\tkey_user_put(key->user);\n\n\t\tkfree(key->description);\n\n#ifdef KEY_DEBUGGING\n\t\tkey->magic = KEY_DEBUG_MAGIC_X;\n#endif\n\t\tkmem_cache_free(key_jar, key);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,10 @@\n \t\tkdebug(\"- %u\", key->serial);\n \t\tkey_check(key);\n \n-\t\t/* Throw away the key data */\n-\t\tif (key->type->destroy)\n+\t\t/* Throw away the key data if the key is instantiated */\n+\t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags) &&\n+\t\t    !test_bit(KEY_FLAG_NEGATIVE, &key->flags) &&\n+\t\t    key->type->destroy)\n \t\t\tkey->type->destroy(key);\n \n \t\tsecurity_key_free(key);",
        "function_modified_lines": {
            "added": [
                "\t\t/* Throw away the key data if the key is instantiated */",
                "\t\tif (test_bit(KEY_FLAG_INSTANTIATED, &key->flags) &&",
                "\t\t    !test_bit(KEY_FLAG_NEGATIVE, &key->flags) &&",
                "\t\t    key->type->destroy)"
            ],
            "deleted": [
                "\t\t/* Throw away the key data */",
                "\t\tif (key->type->destroy)"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The key_gc_unused_keys function in security/keys/gc.c in the Linux kernel through 4.2.6 allows local users to cause a denial of service (OOPS) via crafted keyctl commands.",
        "id": 793
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static __latent_entropy struct task_struct *copy_process(\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace,\n\t\t\t\t\tint node,\n\t\t\t\t\tstruct kernel_clone_args *args)\n{\n\tint pidfd = -1, retval;\n\tstruct task_struct *p;\n\tstruct multiprocess_signals delayed;\n\tstruct file *pidfile = NULL;\n\tu64 clone_flags = args->flags;\n\tstruct nsproxy *nsp = current->nsproxy;\n\n\t/*\n\t * Don't allow sharing the root directory with processes in a different\n\t * namespace\n\t */\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid or user namespace\n\t * do not allow it to share a thread group with the forking task.\n\t */\n\tif (clone_flags & CLONE_THREAD) {\n\t\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\n\t\t    (task_active_pid_ns(current) != nsp->pid_ns_for_children))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * If the new process will be in a different time namespace\n\t * do not allow it to share VM or a thread group with the forking task.\n\t */\n\tif (clone_flags & (CLONE_THREAD | CLONE_VM)) {\n\t\tif (nsp->time_ns != nsp->time_ns_for_children)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (clone_flags & CLONE_PIDFD) {\n\t\t/*\n\t\t * - CLONE_DETACHED is blocked so that we can potentially\n\t\t *   reuse it later for CLONE_PIDFD.\n\t\t * - CLONE_THREAD is blocked until someone really needs it.\n\t\t */\n\t\tif (clone_flags & (CLONE_DETACHED | CLONE_THREAD))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * Force any signals received before this point to be delivered\n\t * before the fork happens.  Collect up signals sent to multiple\n\t * processes that happen during the fork and delay them so that\n\t * they appear to happen after the fork.\n\t */\n\tsigemptyset(&delayed.signal);\n\tINIT_HLIST_NODE(&delayed.node);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (!(clone_flags & CLONE_THREAD))\n\t\thlist_add_head(&delayed.node, &current->signal->multiprocess);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n\tretval = -ERESTARTNOINTR;\n\tif (signal_pending(current))\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current, node);\n\tif (!p)\n\t\tgoto fork_out;\n\n\t/*\n\t * This _must_ happen before we call free_task(), i.e. before we jump\n\t * to any of the bad_fork_* labels. This is to avoid freeing\n\t * p->set_child_tid which is (ab)used as a kthread's data pointer for\n\t * kernel threads (PF_KTHREAD).\n\t */\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? args->child_tid : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? args->child_tid : NULL;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n\tlockdep_assert_irqs_enabled();\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (p->real_cred->user != INIT_USER &&\n\t\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (data_race(nr_threads >= max_threads))\n\t\tgoto bad_fork_cleanup_count;\n\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);\n\tp->flags |= PF_FORKNOEXEC;\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tp->utimescaled = p->stimescaled = 0;\n#endif\n\tprev_cputime_init(&p->prev_cputime);\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqcount_init(&p->vtime.seqcount);\n\tp->vtime.starttime = 0;\n\tp->vtime.state = VTIME_INACTIVE;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n#ifdef CONFIG_PSI\n\tp->psi_flags = 0;\n#endif\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cputimers_init(&p->posix_cputimers);\n\n\tp->io_context = NULL;\n\taudit_set_context(p, NULL);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_threadgroup_lock;\n\t}\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_spinlock_init(&p->mems_allowed_seq, &p->alloc_lock);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tmemset(&p->irqtrace, 0, sizeof(p->irqtrace));\n\tp->irqtrace.hardirq_disable_ip\t= _THIS_IP_;\n\tp->irqtrace.softirq_enable_ip\t= _THIS_IP_;\n\tp->softirqs_enabled\t\t= 1;\n\tp->softirq_context\t\t= 0;\n#endif\n\n\tp->pagefault_disabled = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tlockdep_init_task(p);\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_BCACHE\n\tp->sequential_io\t= 0;\n\tp->sequential_io_avg\t= 0;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tretval = sched_fork(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_perf;\n\t/* copy all the process information */\n\tshm_init_task(p);\n\tretval = security_task_alloc(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_security;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, args->stack, args->stack_size, p, args->tls);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tstackleak_task_init(p);\n\n\tif (pid != &init_struct_pid) {\n\t\tpid = alloc_pid(p->nsproxy->pid_ns_for_children, args->set_tid,\n\t\t\t\targs->set_tid_size);\n\t\tif (IS_ERR(pid)) {\n\t\t\tretval = PTR_ERR(pid);\n\t\t\tgoto bad_fork_cleanup_thread;\n\t\t}\n\t}\n\n\t/*\n\t * This has to happen after we've potentially unshared the file\n\t * descriptor table (so that the pidfd doesn't leak into the child\n\t * if the fd table isn't shared).\n\t */\n\tif (clone_flags & CLONE_PIDFD) {\n\t\tretval = get_unused_fd_flags(O_RDWR | O_CLOEXEC);\n\t\tif (retval < 0)\n\t\t\tgoto bad_fork_free_pid;\n\n\t\tpidfd = retval;\n\n\t\tpidfile = anon_inode_getfile(\"[pidfd]\", &pidfd_fops, pid,\n\t\t\t\t\t      O_RDWR | O_CLOEXEC);\n\t\tif (IS_ERR(pidfile)) {\n\t\t\tput_unused_fd(pidfd);\n\t\t\tretval = PTR_ERR(pidfile);\n\t\t\tgoto bad_fork_free_pid;\n\t\t}\n\t\tget_pid(pid);\t/* held by pidfile now */\n\n\t\tretval = put_user(pidfd, args->pidfd);\n\t\tif (retval)\n\t\t\tgoto bad_fork_put_pidfd;\n\t}\n\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n\tfutex_init_task(p);\n\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tsas_ss_reset(p);\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_tsk_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->exit_signal = -1;\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tif (clone_flags & CLONE_PARENT)\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t\telse\n\t\t\tp->exit_signal = args->exit_signal;\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\tp->pdeath_signal = 0;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/*\n\t * Ensure that the cgroup subsystem policies allow the new process to be\n\t * forked. It should be noted the the new process's css_set can be changed\n\t * between here and cgroup_post_fork() if an organisation operation is in\n\t * progress.\n\t */\n\tretval = cgroup_can_fork(p, args);\n\tif (retval)\n\t\tgoto bad_fork_put_pidfd;\n\n\t/*\n\t * From this point on we must avoid any synchronous user-space\n\t * communication until we take the tasklist-lock. In particular, we do\n\t * not want user-space to be able to predict the process start-time by\n\t * stalling fork(2) after we recorded the start_time but before it is\n\t * visible to the system.\n\t */\n\n\tp->start_time = ktime_get_ns();\n\tp->start_boottime = ktime_get_boottime_ns();\n\n\t/*\n\t * Make it visible to the rest of the system, but dont wake it up yet.\n\t * Need tasklist lock for parent etc handling!\n\t */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Copy seccomp details explicitly here, in case they were changed\n\t * before holding sighand lock.\n\t */\n\tcopy_seccomp(p);\n\n\trseq_fork(p, clone_flags);\n\n\t/* Don't start children in a dying pid namespace */\n\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\n\t\tretval = -ENOMEM;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* Let kill terminate clone/fork in the middle */\n\tif (fatal_signal_pending(current)) {\n\t\tretval = -EINTR;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* past the last point of failure */\n\tif (pidfile)\n\t\tfd_install(pidfd, pidfile);\n\n\tinit_task_pid_links(p);\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tinit_task_pid(p, PIDTYPE_PID, pid);\n\t\tif (thread_group_leader(p)) {\n\t\t\tinit_task_pid(p, PIDTYPE_TGID, pid);\n\t\t\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\n\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\t\t\tp->signal->shared_pending.signal = delayed.signal;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\t/*\n\t\t\t * Inherit has_child_subreaper flag under the same\n\t\t\t * tasklist_lock with adding child to the process tree\n\t\t\t * for propagate_has_child_subreaper optimization.\n\t\t\t */\n\t\t\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\n\t\t\t\t\t\t\t p->real_parent->signal->is_child_subreaper;\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\tattach_pid(p, PIDTYPE_TGID);\n\t\t\tattach_pid(p, PIDTYPE_PGID);\n\t\t\tattach_pid(p, PIDTYPE_SID);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t} else {\n\t\t\tcurrent->signal->nr_threads++;\n\t\t\tatomic_inc(&current->signal->live);\n\t\t\trefcount_inc(&current->signal->sigcnt);\n\t\t\ttask_join_group_stop(p);\n\t\t\tlist_add_tail_rcu(&p->thread_group,\n\t\t\t\t\t  &p->group_leader->thread_group);\n\t\t\tlist_add_tail_rcu(&p->thread_node,\n\t\t\t\t\t  &p->signal->thread_head);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID);\n\t\tnr_threads++;\n\t}\n\ttotal_forks++;\n\thlist_del_init(&delayed.node);\n\tspin_unlock(&current->sighand->siglock);\n\tsyscall_tracepoint_update(p);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_fork_connector(p);\n\tsched_post_fork(p);\n\tcgroup_post_fork(p, args);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\tuprobe_copy_process(p, clone_flags);\n\n\treturn p;\n\nbad_fork_cancel_cgroup:\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tcgroup_cancel_fork(p, args);\nbad_fork_put_pidfd:\n\tif (clone_flags & CLONE_PIDFD) {\n\t\tfput(pidfile);\n\t\tput_unused_fd(pidfd);\n\t}\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_thread:\n\texit_thread(p);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm) {\n\t\tmm_clear_owner(p->mm, p);\n\t\tmmput(p->mm);\n\t}\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_security:\n\tsecurity_task_free(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_perf:\n\tperf_event_free_task(p);\nbad_fork_cleanup_policy:\n\tlockdep_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_threadgroup_lock:\n#endif\n\tdelayacct_tsk_free(p);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tp->state = TASK_DEAD;\n\tput_task_stack(p);\n\tdelayed_free_task(p);\nfork_out:\n\tspin_lock_irq(&current->sighand->siglock);\n\thlist_del_init(&delayed.node);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ERR_PTR(retval);\n}",
        "code_after_change": "static __latent_entropy struct task_struct *copy_process(\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace,\n\t\t\t\t\tint node,\n\t\t\t\t\tstruct kernel_clone_args *args)\n{\n\tint pidfd = -1, retval;\n\tstruct task_struct *p;\n\tstruct multiprocess_signals delayed;\n\tstruct file *pidfile = NULL;\n\tu64 clone_flags = args->flags;\n\tstruct nsproxy *nsp = current->nsproxy;\n\n\t/*\n\t * Don't allow sharing the root directory with processes in a different\n\t * namespace\n\t */\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid or user namespace\n\t * do not allow it to share a thread group with the forking task.\n\t */\n\tif (clone_flags & CLONE_THREAD) {\n\t\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\n\t\t    (task_active_pid_ns(current) != nsp->pid_ns_for_children))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * If the new process will be in a different time namespace\n\t * do not allow it to share VM or a thread group with the forking task.\n\t */\n\tif (clone_flags & (CLONE_THREAD | CLONE_VM)) {\n\t\tif (nsp->time_ns != nsp->time_ns_for_children)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (clone_flags & CLONE_PIDFD) {\n\t\t/*\n\t\t * - CLONE_DETACHED is blocked so that we can potentially\n\t\t *   reuse it later for CLONE_PIDFD.\n\t\t * - CLONE_THREAD is blocked until someone really needs it.\n\t\t */\n\t\tif (clone_flags & (CLONE_DETACHED | CLONE_THREAD))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * Force any signals received before this point to be delivered\n\t * before the fork happens.  Collect up signals sent to multiple\n\t * processes that happen during the fork and delay them so that\n\t * they appear to happen after the fork.\n\t */\n\tsigemptyset(&delayed.signal);\n\tINIT_HLIST_NODE(&delayed.node);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (!(clone_flags & CLONE_THREAD))\n\t\thlist_add_head(&delayed.node, &current->signal->multiprocess);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n\tretval = -ERESTARTNOINTR;\n\tif (signal_pending(current))\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current, node);\n\tif (!p)\n\t\tgoto fork_out;\n\n\t/*\n\t * This _must_ happen before we call free_task(), i.e. before we jump\n\t * to any of the bad_fork_* labels. This is to avoid freeing\n\t * p->set_child_tid which is (ab)used as a kthread's data pointer for\n\t * kernel threads (PF_KTHREAD).\n\t */\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? args->child_tid : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? args->child_tid : NULL;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n\tlockdep_assert_irqs_enabled();\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (p->real_cred->user != INIT_USER &&\n\t\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (data_race(nr_threads >= max_threads))\n\t\tgoto bad_fork_cleanup_count;\n\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);\n\tp->flags |= PF_FORKNOEXEC;\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tp->utimescaled = p->stimescaled = 0;\n#endif\n\tprev_cputime_init(&p->prev_cputime);\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqcount_init(&p->vtime.seqcount);\n\tp->vtime.starttime = 0;\n\tp->vtime.state = VTIME_INACTIVE;\n#endif\n\n#ifdef CONFIG_IO_URING\n\tp->io_uring = NULL;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n#ifdef CONFIG_PSI\n\tp->psi_flags = 0;\n#endif\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cputimers_init(&p->posix_cputimers);\n\n\tp->io_context = NULL;\n\taudit_set_context(p, NULL);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_threadgroup_lock;\n\t}\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_spinlock_init(&p->mems_allowed_seq, &p->alloc_lock);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tmemset(&p->irqtrace, 0, sizeof(p->irqtrace));\n\tp->irqtrace.hardirq_disable_ip\t= _THIS_IP_;\n\tp->irqtrace.softirq_enable_ip\t= _THIS_IP_;\n\tp->softirqs_enabled\t\t= 1;\n\tp->softirq_context\t\t= 0;\n#endif\n\n\tp->pagefault_disabled = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tlockdep_init_task(p);\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_BCACHE\n\tp->sequential_io\t= 0;\n\tp->sequential_io_avg\t= 0;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tretval = sched_fork(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_perf;\n\t/* copy all the process information */\n\tshm_init_task(p);\n\tretval = security_task_alloc(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_security;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, args->stack, args->stack_size, p, args->tls);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tstackleak_task_init(p);\n\n\tif (pid != &init_struct_pid) {\n\t\tpid = alloc_pid(p->nsproxy->pid_ns_for_children, args->set_tid,\n\t\t\t\targs->set_tid_size);\n\t\tif (IS_ERR(pid)) {\n\t\t\tretval = PTR_ERR(pid);\n\t\t\tgoto bad_fork_cleanup_thread;\n\t\t}\n\t}\n\n\t/*\n\t * This has to happen after we've potentially unshared the file\n\t * descriptor table (so that the pidfd doesn't leak into the child\n\t * if the fd table isn't shared).\n\t */\n\tif (clone_flags & CLONE_PIDFD) {\n\t\tretval = get_unused_fd_flags(O_RDWR | O_CLOEXEC);\n\t\tif (retval < 0)\n\t\t\tgoto bad_fork_free_pid;\n\n\t\tpidfd = retval;\n\n\t\tpidfile = anon_inode_getfile(\"[pidfd]\", &pidfd_fops, pid,\n\t\t\t\t\t      O_RDWR | O_CLOEXEC);\n\t\tif (IS_ERR(pidfile)) {\n\t\t\tput_unused_fd(pidfd);\n\t\t\tretval = PTR_ERR(pidfile);\n\t\t\tgoto bad_fork_free_pid;\n\t\t}\n\t\tget_pid(pid);\t/* held by pidfile now */\n\n\t\tretval = put_user(pidfd, args->pidfd);\n\t\tif (retval)\n\t\t\tgoto bad_fork_put_pidfd;\n\t}\n\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n\tfutex_init_task(p);\n\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tsas_ss_reset(p);\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_tsk_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->exit_signal = -1;\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tif (clone_flags & CLONE_PARENT)\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t\telse\n\t\t\tp->exit_signal = args->exit_signal;\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\tp->pdeath_signal = 0;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/*\n\t * Ensure that the cgroup subsystem policies allow the new process to be\n\t * forked. It should be noted the the new process's css_set can be changed\n\t * between here and cgroup_post_fork() if an organisation operation is in\n\t * progress.\n\t */\n\tretval = cgroup_can_fork(p, args);\n\tif (retval)\n\t\tgoto bad_fork_put_pidfd;\n\n\t/*\n\t * From this point on we must avoid any synchronous user-space\n\t * communication until we take the tasklist-lock. In particular, we do\n\t * not want user-space to be able to predict the process start-time by\n\t * stalling fork(2) after we recorded the start_time but before it is\n\t * visible to the system.\n\t */\n\n\tp->start_time = ktime_get_ns();\n\tp->start_boottime = ktime_get_boottime_ns();\n\n\t/*\n\t * Make it visible to the rest of the system, but dont wake it up yet.\n\t * Need tasklist lock for parent etc handling!\n\t */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Copy seccomp details explicitly here, in case they were changed\n\t * before holding sighand lock.\n\t */\n\tcopy_seccomp(p);\n\n\trseq_fork(p, clone_flags);\n\n\t/* Don't start children in a dying pid namespace */\n\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\n\t\tretval = -ENOMEM;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* Let kill terminate clone/fork in the middle */\n\tif (fatal_signal_pending(current)) {\n\t\tretval = -EINTR;\n\t\tgoto bad_fork_cancel_cgroup;\n\t}\n\n\t/* past the last point of failure */\n\tif (pidfile)\n\t\tfd_install(pidfd, pidfile);\n\n\tinit_task_pid_links(p);\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tinit_task_pid(p, PIDTYPE_PID, pid);\n\t\tif (thread_group_leader(p)) {\n\t\t\tinit_task_pid(p, PIDTYPE_TGID, pid);\n\t\t\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\n\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\t\t\tp->signal->shared_pending.signal = delayed.signal;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\t/*\n\t\t\t * Inherit has_child_subreaper flag under the same\n\t\t\t * tasklist_lock with adding child to the process tree\n\t\t\t * for propagate_has_child_subreaper optimization.\n\t\t\t */\n\t\t\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\n\t\t\t\t\t\t\t p->real_parent->signal->is_child_subreaper;\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\tattach_pid(p, PIDTYPE_TGID);\n\t\t\tattach_pid(p, PIDTYPE_PGID);\n\t\t\tattach_pid(p, PIDTYPE_SID);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t} else {\n\t\t\tcurrent->signal->nr_threads++;\n\t\t\tatomic_inc(&current->signal->live);\n\t\t\trefcount_inc(&current->signal->sigcnt);\n\t\t\ttask_join_group_stop(p);\n\t\t\tlist_add_tail_rcu(&p->thread_group,\n\t\t\t\t\t  &p->group_leader->thread_group);\n\t\t\tlist_add_tail_rcu(&p->thread_node,\n\t\t\t\t\t  &p->signal->thread_head);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID);\n\t\tnr_threads++;\n\t}\n\ttotal_forks++;\n\thlist_del_init(&delayed.node);\n\tspin_unlock(&current->sighand->siglock);\n\tsyscall_tracepoint_update(p);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tproc_fork_connector(p);\n\tsched_post_fork(p);\n\tcgroup_post_fork(p, args);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\tuprobe_copy_process(p, clone_flags);\n\n\treturn p;\n\nbad_fork_cancel_cgroup:\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tcgroup_cancel_fork(p, args);\nbad_fork_put_pidfd:\n\tif (clone_flags & CLONE_PIDFD) {\n\t\tfput(pidfile);\n\t\tput_unused_fd(pidfd);\n\t}\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_thread:\n\texit_thread(p);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm) {\n\t\tmm_clear_owner(p->mm, p);\n\t\tmmput(p->mm);\n\t}\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_security:\n\tsecurity_task_free(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_perf:\n\tperf_event_free_task(p);\nbad_fork_cleanup_policy:\n\tlockdep_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_threadgroup_lock:\n#endif\n\tdelayacct_tsk_free(p);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tp->state = TASK_DEAD;\n\tput_task_stack(p);\n\tdelayed_free_task(p);\nfork_out:\n\tspin_lock_irq(&current->sighand->siglock);\n\thlist_del_init(&delayed.node);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ERR_PTR(retval);\n}",
        "patch": "--- code before\n+++ code after\n@@ -161,6 +161,10 @@\n \tseqcount_init(&p->vtime.seqcount);\n \tp->vtime.starttime = 0;\n \tp->vtime.state = VTIME_INACTIVE;\n+#endif\n+\n+#ifdef CONFIG_IO_URING\n+\tp->io_uring = NULL;\n #endif\n \n #if defined(SPLIT_RSS_COUNTING)",
        "function_modified_lines": {
            "added": [
                "#endif",
                "",
                "#ifdef CONFIG_IO_URING",
                "\tp->io_uring = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2859
    },
    {
        "cve_id": "CVE-2012-2669",
        "code_before_change": "int main(void)\n{\n\tint fd, len, sock_opt;\n\tint error;\n\tstruct cn_msg *message;\n\tstruct pollfd pfd;\n\tstruct nlmsghdr *incoming_msg;\n\tstruct cn_msg\t*incoming_cn_msg;\n\tstruct hv_kvp_msg *hv_msg;\n\tchar\t*p;\n\tchar\t*key_value;\n\tchar\t*key_name;\n\n\tdaemon(1, 0);\n\topenlog(\"KVP\", 0, LOG_USER);\n\tsyslog(LOG_INFO, \"KVP starting; pid is:%d\", getpid());\n\t/*\n\t * Retrieve OS release information.\n\t */\n\tkvp_get_os_info();\n\n\tif (kvp_file_init()) {\n\t\tsyslog(LOG_ERR, \"Failed to initialize the pools\");\n\t\texit(-1);\n\t}\n\n\tfd = socket(AF_NETLINK, SOCK_DGRAM, NETLINK_CONNECTOR);\n\tif (fd < 0) {\n\t\tsyslog(LOG_ERR, \"netlink socket creation failed; error:%d\", fd);\n\t\texit(-1);\n\t}\n\taddr.nl_family = AF_NETLINK;\n\taddr.nl_pad = 0;\n\taddr.nl_pid = 0;\n\taddr.nl_groups = CN_KVP_IDX;\n\n\n\terror = bind(fd, (struct sockaddr *)&addr, sizeof(addr));\n\tif (error < 0) {\n\t\tsyslog(LOG_ERR, \"bind failed; error:%d\", error);\n\t\tclose(fd);\n\t\texit(-1);\n\t}\n\tsock_opt = addr.nl_groups;\n\tsetsockopt(fd, 270, 1, &sock_opt, sizeof(sock_opt));\n\t/*\n\t * Register ourselves with the kernel.\n\t */\n\tmessage = (struct cn_msg *)kvp_send_buffer;\n\tmessage->id.idx = CN_KVP_IDX;\n\tmessage->id.val = CN_KVP_VAL;\n\n\thv_msg = (struct hv_kvp_msg *)message->data;\n\thv_msg->kvp_hdr.operation = KVP_OP_REGISTER;\n\tmessage->ack = 0;\n\tmessage->len = sizeof(struct hv_kvp_msg);\n\n\tlen = netlink_send(fd, message);\n\tif (len < 0) {\n\t\tsyslog(LOG_ERR, \"netlink_send failed; error:%d\", len);\n\t\tclose(fd);\n\t\texit(-1);\n\t}\n\n\tpfd.fd = fd;\n\n\twhile (1) {\n\t\tpfd.events = POLLIN;\n\t\tpfd.revents = 0;\n\t\tpoll(&pfd, 1, -1);\n\n\t\tlen = recv(fd, kvp_recv_buffer, sizeof(kvp_recv_buffer), 0);\n\n\t\tif (len < 0) {\n\t\t\tsyslog(LOG_ERR, \"recv failed; error:%d\", len);\n\t\t\tclose(fd);\n\t\t\treturn -1;\n\t\t}\n\n\t\tincoming_msg = (struct nlmsghdr *)kvp_recv_buffer;\n\t\tincoming_cn_msg = (struct cn_msg *)NLMSG_DATA(incoming_msg);\n\t\thv_msg = (struct hv_kvp_msg *)incoming_cn_msg->data;\n\n\t\tswitch (hv_msg->kvp_hdr.operation) {\n\t\tcase KVP_OP_REGISTER:\n\t\t\t/*\n\t\t\t * Driver is registering with us; stash away the version\n\t\t\t * information.\n\t\t\t */\n\t\t\tp = (char *)hv_msg->body.kvp_register.version;\n\t\t\tlic_version = malloc(strlen(p) + 1);\n\t\t\tif (lic_version) {\n\t\t\t\tstrcpy(lic_version, p);\n\t\t\t\tsyslog(LOG_INFO, \"KVP LIC Version: %s\",\n\t\t\t\t\tlic_version);\n\t\t\t} else {\n\t\t\t\tsyslog(LOG_ERR, \"malloc failed\");\n\t\t\t}\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * The current protocol with the kernel component uses a\n\t\t * NULL key name to pass an error condition.\n\t\t * For the SET, GET and DELETE operations,\n\t\t * use the existing protocol to pass back error.\n\t\t */\n\n\t\tcase KVP_OP_SET:\n\t\t\tif (kvp_key_add_or_modify(hv_msg->kvp_hdr.pool,\n\t\t\t\t\thv_msg->body.kvp_set.data.key,\n\t\t\t\t\thv_msg->body.kvp_set.data.key_size,\n\t\t\t\t\thv_msg->body.kvp_set.data.value,\n\t\t\t\t\thv_msg->body.kvp_set.data.value_size))\n\t\t\t\tstrcpy(hv_msg->body.kvp_set.data.key, \"\");\n\t\t\tbreak;\n\n\t\tcase KVP_OP_GET:\n\t\t\tif (kvp_get_value(hv_msg->kvp_hdr.pool,\n\t\t\t\t\thv_msg->body.kvp_set.data.key,\n\t\t\t\t\thv_msg->body.kvp_set.data.key_size,\n\t\t\t\t\thv_msg->body.kvp_set.data.value,\n\t\t\t\t\thv_msg->body.kvp_set.data.value_size))\n\t\t\t\tstrcpy(hv_msg->body.kvp_set.data.key, \"\");\n\t\t\tbreak;\n\n\t\tcase KVP_OP_DELETE:\n\t\t\tif (kvp_key_delete(hv_msg->kvp_hdr.pool,\n\t\t\t\t\thv_msg->body.kvp_delete.key,\n\t\t\t\t\thv_msg->body.kvp_delete.key_size))\n\t\t\t\tstrcpy(hv_msg->body.kvp_delete.key, \"\");\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (hv_msg->kvp_hdr.operation != KVP_OP_ENUMERATE)\n\t\t\tgoto kvp_done;\n\n\t\t/*\n\t\t * If the pool is KVP_POOL_AUTO, dynamically generate\n\t\t * both the key and the value; if not read from the\n\t\t * appropriate pool.\n\t\t */\n\t\tif (hv_msg->kvp_hdr.pool != KVP_POOL_AUTO) {\n\t\t\tkvp_pool_enumerate(hv_msg->kvp_hdr.pool,\n\t\t\t\t\thv_msg->body.kvp_enum_data.index,\n\t\t\t\t\thv_msg->body.kvp_enum_data.data.key,\n\t\t\t\t\tHV_KVP_EXCHANGE_MAX_KEY_SIZE,\n\t\t\t\t\thv_msg->body.kvp_enum_data.data.value,\n\t\t\t\t\tHV_KVP_EXCHANGE_MAX_VALUE_SIZE);\n\t\t\tgoto kvp_done;\n\t\t}\n\n\t\thv_msg = (struct hv_kvp_msg *)incoming_cn_msg->data;\n\t\tkey_name = (char *)hv_msg->body.kvp_enum_data.data.key;\n\t\tkey_value = (char *)hv_msg->body.kvp_enum_data.data.value;\n\n\t\tswitch (hv_msg->body.kvp_enum_data.index) {\n\t\tcase FullyQualifiedDomainName:\n\t\t\tkvp_get_domain_name(key_value,\n\t\t\t\t\tHV_KVP_EXCHANGE_MAX_VALUE_SIZE);\n\t\t\tstrcpy(key_name, \"FullyQualifiedDomainName\");\n\t\t\tbreak;\n\t\tcase IntegrationServicesVersion:\n\t\t\tstrcpy(key_name, \"IntegrationServicesVersion\");\n\t\t\tstrcpy(key_value, lic_version);\n\t\t\tbreak;\n\t\tcase NetworkAddressIPv4:\n\t\t\tkvp_get_ip_address(AF_INET, key_value,\n\t\t\t\t\tHV_KVP_EXCHANGE_MAX_VALUE_SIZE);\n\t\t\tstrcpy(key_name, \"NetworkAddressIPv4\");\n\t\t\tbreak;\n\t\tcase NetworkAddressIPv6:\n\t\t\tkvp_get_ip_address(AF_INET6, key_value,\n\t\t\t\t\tHV_KVP_EXCHANGE_MAX_VALUE_SIZE);\n\t\t\tstrcpy(key_name, \"NetworkAddressIPv6\");\n\t\t\tbreak;\n\t\tcase OSBuildNumber:\n\t\t\tstrcpy(key_value, os_build);\n\t\t\tstrcpy(key_name, \"OSBuildNumber\");\n\t\t\tbreak;\n\t\tcase OSName:\n\t\t\tstrcpy(key_value, os_name);\n\t\t\tstrcpy(key_name, \"OSName\");\n\t\t\tbreak;\n\t\tcase OSMajorVersion:\n\t\t\tstrcpy(key_value, os_major);\n\t\t\tstrcpy(key_name, \"OSMajorVersion\");\n\t\t\tbreak;\n\t\tcase OSMinorVersion:\n\t\t\tstrcpy(key_value, os_minor);\n\t\t\tstrcpy(key_name, \"OSMinorVersion\");\n\t\t\tbreak;\n\t\tcase OSVersion:\n\t\t\tstrcpy(key_value, os_build);\n\t\t\tstrcpy(key_name, \"OSVersion\");\n\t\t\tbreak;\n\t\tcase ProcessorArchitecture:\n\t\t\tstrcpy(key_value, processor_arch);\n\t\t\tstrcpy(key_name, \"ProcessorArchitecture\");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tstrcpy(key_value, \"Unknown Key\");\n\t\t\t/*\n\t\t\t * We use a null key name to terminate enumeration.\n\t\t\t */\n\t\t\tstrcpy(key_name, \"\");\n\t\t\tbreak;\n\t\t}\n\t\t/*\n\t\t * Send the value back to the kernel. The response is\n\t\t * already in the receive buffer. Update the cn_msg header to\n\t\t * reflect the key value that has been added to the message\n\t\t */\nkvp_done:\n\n\t\tincoming_cn_msg->id.idx = CN_KVP_IDX;\n\t\tincoming_cn_msg->id.val = CN_KVP_VAL;\n\t\tincoming_cn_msg->ack = 0;\n\t\tincoming_cn_msg->len = sizeof(struct hv_kvp_msg);\n\n\t\tlen = netlink_send(fd, incoming_cn_msg);\n\t\tif (len < 0) {\n\t\t\tsyslog(LOG_ERR, \"net_link send failed; error:%d\", len);\n\t\t\texit(-1);\n\t\t}\n\t}\n\n}",
        "code_after_change": "int main(void)\n{\n\tint fd, len, sock_opt;\n\tint error;\n\tstruct cn_msg *message;\n\tstruct pollfd pfd;\n\tstruct nlmsghdr *incoming_msg;\n\tstruct cn_msg\t*incoming_cn_msg;\n\tstruct hv_kvp_msg *hv_msg;\n\tchar\t*p;\n\tchar\t*key_value;\n\tchar\t*key_name;\n\n\tdaemon(1, 0);\n\topenlog(\"KVP\", 0, LOG_USER);\n\tsyslog(LOG_INFO, \"KVP starting; pid is:%d\", getpid());\n\t/*\n\t * Retrieve OS release information.\n\t */\n\tkvp_get_os_info();\n\n\tif (kvp_file_init()) {\n\t\tsyslog(LOG_ERR, \"Failed to initialize the pools\");\n\t\texit(-1);\n\t}\n\n\tfd = socket(AF_NETLINK, SOCK_DGRAM, NETLINK_CONNECTOR);\n\tif (fd < 0) {\n\t\tsyslog(LOG_ERR, \"netlink socket creation failed; error:%d\", fd);\n\t\texit(-1);\n\t}\n\taddr.nl_family = AF_NETLINK;\n\taddr.nl_pad = 0;\n\taddr.nl_pid = 0;\n\taddr.nl_groups = CN_KVP_IDX;\n\n\n\terror = bind(fd, (struct sockaddr *)&addr, sizeof(addr));\n\tif (error < 0) {\n\t\tsyslog(LOG_ERR, \"bind failed; error:%d\", error);\n\t\tclose(fd);\n\t\texit(-1);\n\t}\n\tsock_opt = addr.nl_groups;\n\tsetsockopt(fd, 270, 1, &sock_opt, sizeof(sock_opt));\n\t/*\n\t * Register ourselves with the kernel.\n\t */\n\tmessage = (struct cn_msg *)kvp_send_buffer;\n\tmessage->id.idx = CN_KVP_IDX;\n\tmessage->id.val = CN_KVP_VAL;\n\n\thv_msg = (struct hv_kvp_msg *)message->data;\n\thv_msg->kvp_hdr.operation = KVP_OP_REGISTER;\n\tmessage->ack = 0;\n\tmessage->len = sizeof(struct hv_kvp_msg);\n\n\tlen = netlink_send(fd, message);\n\tif (len < 0) {\n\t\tsyslog(LOG_ERR, \"netlink_send failed; error:%d\", len);\n\t\tclose(fd);\n\t\texit(-1);\n\t}\n\n\tpfd.fd = fd;\n\n\twhile (1) {\n\t\tstruct sockaddr *addr_p = (struct sockaddr *) &addr;\n\t\tsocklen_t addr_l = sizeof(addr);\n\t\tpfd.events = POLLIN;\n\t\tpfd.revents = 0;\n\t\tpoll(&pfd, 1, -1);\n\n\t\tlen = recvfrom(fd, kvp_recv_buffer, sizeof(kvp_recv_buffer), 0,\n\t\t\t\taddr_p, &addr_l);\n\n\t\tif (len < 0 || addr.nl_pid) {\n\t\t\tsyslog(LOG_ERR, \"recvfrom failed; pid:%u error:%d %s\",\n\t\t\t\t\taddr.nl_pid, errno, strerror(errno));\n\t\t\tclose(fd);\n\t\t\treturn -1;\n\t\t}\n\n\t\tincoming_msg = (struct nlmsghdr *)kvp_recv_buffer;\n\t\tincoming_cn_msg = (struct cn_msg *)NLMSG_DATA(incoming_msg);\n\t\thv_msg = (struct hv_kvp_msg *)incoming_cn_msg->data;\n\n\t\tswitch (hv_msg->kvp_hdr.operation) {\n\t\tcase KVP_OP_REGISTER:\n\t\t\t/*\n\t\t\t * Driver is registering with us; stash away the version\n\t\t\t * information.\n\t\t\t */\n\t\t\tp = (char *)hv_msg->body.kvp_register.version;\n\t\t\tlic_version = malloc(strlen(p) + 1);\n\t\t\tif (lic_version) {\n\t\t\t\tstrcpy(lic_version, p);\n\t\t\t\tsyslog(LOG_INFO, \"KVP LIC Version: %s\",\n\t\t\t\t\tlic_version);\n\t\t\t} else {\n\t\t\t\tsyslog(LOG_ERR, \"malloc failed\");\n\t\t\t}\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * The current protocol with the kernel component uses a\n\t\t * NULL key name to pass an error condition.\n\t\t * For the SET, GET and DELETE operations,\n\t\t * use the existing protocol to pass back error.\n\t\t */\n\n\t\tcase KVP_OP_SET:\n\t\t\tif (kvp_key_add_or_modify(hv_msg->kvp_hdr.pool,\n\t\t\t\t\thv_msg->body.kvp_set.data.key,\n\t\t\t\t\thv_msg->body.kvp_set.data.key_size,\n\t\t\t\t\thv_msg->body.kvp_set.data.value,\n\t\t\t\t\thv_msg->body.kvp_set.data.value_size))\n\t\t\t\tstrcpy(hv_msg->body.kvp_set.data.key, \"\");\n\t\t\tbreak;\n\n\t\tcase KVP_OP_GET:\n\t\t\tif (kvp_get_value(hv_msg->kvp_hdr.pool,\n\t\t\t\t\thv_msg->body.kvp_set.data.key,\n\t\t\t\t\thv_msg->body.kvp_set.data.key_size,\n\t\t\t\t\thv_msg->body.kvp_set.data.value,\n\t\t\t\t\thv_msg->body.kvp_set.data.value_size))\n\t\t\t\tstrcpy(hv_msg->body.kvp_set.data.key, \"\");\n\t\t\tbreak;\n\n\t\tcase KVP_OP_DELETE:\n\t\t\tif (kvp_key_delete(hv_msg->kvp_hdr.pool,\n\t\t\t\t\thv_msg->body.kvp_delete.key,\n\t\t\t\t\thv_msg->body.kvp_delete.key_size))\n\t\t\t\tstrcpy(hv_msg->body.kvp_delete.key, \"\");\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (hv_msg->kvp_hdr.operation != KVP_OP_ENUMERATE)\n\t\t\tgoto kvp_done;\n\n\t\t/*\n\t\t * If the pool is KVP_POOL_AUTO, dynamically generate\n\t\t * both the key and the value; if not read from the\n\t\t * appropriate pool.\n\t\t */\n\t\tif (hv_msg->kvp_hdr.pool != KVP_POOL_AUTO) {\n\t\t\tkvp_pool_enumerate(hv_msg->kvp_hdr.pool,\n\t\t\t\t\thv_msg->body.kvp_enum_data.index,\n\t\t\t\t\thv_msg->body.kvp_enum_data.data.key,\n\t\t\t\t\tHV_KVP_EXCHANGE_MAX_KEY_SIZE,\n\t\t\t\t\thv_msg->body.kvp_enum_data.data.value,\n\t\t\t\t\tHV_KVP_EXCHANGE_MAX_VALUE_SIZE);\n\t\t\tgoto kvp_done;\n\t\t}\n\n\t\thv_msg = (struct hv_kvp_msg *)incoming_cn_msg->data;\n\t\tkey_name = (char *)hv_msg->body.kvp_enum_data.data.key;\n\t\tkey_value = (char *)hv_msg->body.kvp_enum_data.data.value;\n\n\t\tswitch (hv_msg->body.kvp_enum_data.index) {\n\t\tcase FullyQualifiedDomainName:\n\t\t\tkvp_get_domain_name(key_value,\n\t\t\t\t\tHV_KVP_EXCHANGE_MAX_VALUE_SIZE);\n\t\t\tstrcpy(key_name, \"FullyQualifiedDomainName\");\n\t\t\tbreak;\n\t\tcase IntegrationServicesVersion:\n\t\t\tstrcpy(key_name, \"IntegrationServicesVersion\");\n\t\t\tstrcpy(key_value, lic_version);\n\t\t\tbreak;\n\t\tcase NetworkAddressIPv4:\n\t\t\tkvp_get_ip_address(AF_INET, key_value,\n\t\t\t\t\tHV_KVP_EXCHANGE_MAX_VALUE_SIZE);\n\t\t\tstrcpy(key_name, \"NetworkAddressIPv4\");\n\t\t\tbreak;\n\t\tcase NetworkAddressIPv6:\n\t\t\tkvp_get_ip_address(AF_INET6, key_value,\n\t\t\t\t\tHV_KVP_EXCHANGE_MAX_VALUE_SIZE);\n\t\t\tstrcpy(key_name, \"NetworkAddressIPv6\");\n\t\t\tbreak;\n\t\tcase OSBuildNumber:\n\t\t\tstrcpy(key_value, os_build);\n\t\t\tstrcpy(key_name, \"OSBuildNumber\");\n\t\t\tbreak;\n\t\tcase OSName:\n\t\t\tstrcpy(key_value, os_name);\n\t\t\tstrcpy(key_name, \"OSName\");\n\t\t\tbreak;\n\t\tcase OSMajorVersion:\n\t\t\tstrcpy(key_value, os_major);\n\t\t\tstrcpy(key_name, \"OSMajorVersion\");\n\t\t\tbreak;\n\t\tcase OSMinorVersion:\n\t\t\tstrcpy(key_value, os_minor);\n\t\t\tstrcpy(key_name, \"OSMinorVersion\");\n\t\t\tbreak;\n\t\tcase OSVersion:\n\t\t\tstrcpy(key_value, os_build);\n\t\t\tstrcpy(key_name, \"OSVersion\");\n\t\t\tbreak;\n\t\tcase ProcessorArchitecture:\n\t\t\tstrcpy(key_value, processor_arch);\n\t\t\tstrcpy(key_name, \"ProcessorArchitecture\");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tstrcpy(key_value, \"Unknown Key\");\n\t\t\t/*\n\t\t\t * We use a null key name to terminate enumeration.\n\t\t\t */\n\t\t\tstrcpy(key_name, \"\");\n\t\t\tbreak;\n\t\t}\n\t\t/*\n\t\t * Send the value back to the kernel. The response is\n\t\t * already in the receive buffer. Update the cn_msg header to\n\t\t * reflect the key value that has been added to the message\n\t\t */\nkvp_done:\n\n\t\tincoming_cn_msg->id.idx = CN_KVP_IDX;\n\t\tincoming_cn_msg->id.val = CN_KVP_VAL;\n\t\tincoming_cn_msg->ack = 0;\n\t\tincoming_cn_msg->len = sizeof(struct hv_kvp_msg);\n\n\t\tlen = netlink_send(fd, incoming_cn_msg);\n\t\tif (len < 0) {\n\t\t\tsyslog(LOG_ERR, \"net_link send failed; error:%d\", len);\n\t\t\texit(-1);\n\t\t}\n\t}\n\n}",
        "patch": "--- code before\n+++ code after\n@@ -65,14 +65,18 @@\n \tpfd.fd = fd;\n \n \twhile (1) {\n+\t\tstruct sockaddr *addr_p = (struct sockaddr *) &addr;\n+\t\tsocklen_t addr_l = sizeof(addr);\n \t\tpfd.events = POLLIN;\n \t\tpfd.revents = 0;\n \t\tpoll(&pfd, 1, -1);\n \n-\t\tlen = recv(fd, kvp_recv_buffer, sizeof(kvp_recv_buffer), 0);\n-\n-\t\tif (len < 0) {\n-\t\t\tsyslog(LOG_ERR, \"recv failed; error:%d\", len);\n+\t\tlen = recvfrom(fd, kvp_recv_buffer, sizeof(kvp_recv_buffer), 0,\n+\t\t\t\taddr_p, &addr_l);\n+\n+\t\tif (len < 0 || addr.nl_pid) {\n+\t\t\tsyslog(LOG_ERR, \"recvfrom failed; pid:%u error:%d %s\",\n+\t\t\t\t\taddr.nl_pid, errno, strerror(errno));\n \t\t\tclose(fd);\n \t\t\treturn -1;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\tstruct sockaddr *addr_p = (struct sockaddr *) &addr;",
                "\t\tsocklen_t addr_l = sizeof(addr);",
                "\t\tlen = recvfrom(fd, kvp_recv_buffer, sizeof(kvp_recv_buffer), 0,",
                "\t\t\t\taddr_p, &addr_l);",
                "",
                "\t\tif (len < 0 || addr.nl_pid) {",
                "\t\t\tsyslog(LOG_ERR, \"recvfrom failed; pid:%u error:%d %s\",",
                "\t\t\t\t\taddr.nl_pid, errno, strerror(errno));"
            ],
            "deleted": [
                "\t\tlen = recv(fd, kvp_recv_buffer, sizeof(kvp_recv_buffer), 0);",
                "",
                "\t\tif (len < 0) {",
                "\t\t\tsyslog(LOG_ERR, \"recv failed; error:%d\", len);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The main function in tools/hv/hv_kvp_daemon.c in hypervkvpd, as distributed in the Linux kernel before 3.4.5, does not validate the origin of Netlink messages, which allows local users to spoof Netlink communication via a crafted connector message.",
        "id": 50
    },
    {
        "cve_id": "CVE-2018-12207",
        "code_before_change": "static int kvm_debugfs_open(struct inode *inode, struct file *file,\n\t\t\t   int (*get)(void *, u64 *), int (*set)(void *, u64),\n\t\t\t   const char *fmt)\n{\n\tstruct kvm_stat_data *stat_data = (struct kvm_stat_data *)\n\t\t\t\t\t  inode->i_private;\n\n\t/* The debugfs files are a reference to the kvm struct which\n\t * is still valid when kvm_destroy_vm is called.\n\t * To avoid the race between open and the removal of the debugfs\n\t * directory we test against the users count.\n\t */\n\tif (!refcount_inc_not_zero(&stat_data->kvm->users_count))\n\t\treturn -ENOENT;\n\n\tif (simple_attr_open(inode, file, get, set, fmt)) {\n\t\tkvm_put_kvm(stat_data->kvm);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int kvm_debugfs_open(struct inode *inode, struct file *file,\n\t\t\t   int (*get)(void *, u64 *), int (*set)(void *, u64),\n\t\t\t   const char *fmt)\n{\n\tstruct kvm_stat_data *stat_data = (struct kvm_stat_data *)\n\t\t\t\t\t  inode->i_private;\n\n\t/* The debugfs files are a reference to the kvm struct which\n\t * is still valid when kvm_destroy_vm is called.\n\t * To avoid the race between open and the removal of the debugfs\n\t * directory we test against the users count.\n\t */\n\tif (!refcount_inc_not_zero(&stat_data->kvm->users_count))\n\t\treturn -ENOENT;\n\n\tif (simple_attr_open(inode, file, get,\n\t\t\t     stat_data->mode & S_IWUGO ? set : NULL,\n\t\t\t     fmt)) {\n\t\tkvm_put_kvm(stat_data->kvm);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,7 +13,9 @@\n \tif (!refcount_inc_not_zero(&stat_data->kvm->users_count))\n \t\treturn -ENOENT;\n \n-\tif (simple_attr_open(inode, file, get, set, fmt)) {\n+\tif (simple_attr_open(inode, file, get,\n+\t\t\t     stat_data->mode & S_IWUGO ? set : NULL,\n+\t\t\t     fmt)) {\n \t\tkvm_put_kvm(stat_data->kvm);\n \t\treturn -ENOMEM;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif (simple_attr_open(inode, file, get,",
                "\t\t\t     stat_data->mode & S_IWUGO ? set : NULL,",
                "\t\t\t     fmt)) {"
            ],
            "deleted": [
                "\tif (simple_attr_open(inode, file, get, set, fmt)) {"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Improper invalidation for page table updates by a virtual guest operating system for multiple Intel(R) Processors may allow an authenticated user to potentially enable denial of service of the host system via local access.",
        "id": 1646
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int irda_recvmsg_stream(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct irda_sock *self = irda_sk(sk);\n\tint noblock = flags & MSG_DONTWAIT;\n\tsize_t copied = 0;\n\tint target, err;\n\tlong timeo;\n\n\tIRDA_DEBUG(3, \"%s()\\n\", __func__);\n\n\tif ((err = sock_error(sk)) < 0)\n\t\treturn err;\n\n\tif (sock->flags & __SO_ACCEPTCON)\n\t\treturn -EINVAL;\n\n\terr =-EOPNOTSUPP;\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\terr = 0;\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, noblock);\n\n\tmsg->msg_namelen = 0;\n\n\tdo {\n\t\tint chunk;\n\t\tstruct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);\n\n\t\tif (skb == NULL) {\n\t\t\tDEFINE_WAIT(wait);\n\t\t\terr = 0;\n\n\t\t\tif (copied >= target)\n\t\t\t\tbreak;\n\n\t\t\tprepare_to_wait_exclusive(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\t;\n\t\t\telse if (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\t;\n\t\t\telse if (noblock)\n\t\t\t\terr = -EAGAIN;\n\t\t\telse if (signal_pending(current))\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\telse if (sk->sk_state != TCP_ESTABLISHED)\n\t\t\t\terr = -ENOTCONN;\n\t\t\telse if (skb_peek(&sk->sk_receive_queue) == NULL)\n\t\t\t\t/* Wait process until data arrives */\n\t\t\t\tschedule();\n\n\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, skb->len, size);\n\t\tif (memcpy_toiovec(msg->msg_iov, skb->data, chunk)) {\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tskb_pull(skb, chunk);\n\n\t\t\t/* put the skb back if we didn't use it up.. */\n\t\t\tif (skb->len) {\n\t\t\t\tIRDA_DEBUG(1, \"%s(), back on q!\\n\",\n\t\t\t\t\t   __func__);\n\t\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tkfree_skb(skb);\n\t\t} else {\n\t\t\tIRDA_DEBUG(0, \"%s() questionable!?\\n\", __func__);\n\n\t\t\t/* put message back and return */\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\t/*\n\t *  Check if we have previously stopped IrTTP and we know\n\t *  have more free space in our rx_queue. If so tell IrTTP\n\t *  to start delivering frames again before our rx_queue gets\n\t *  empty\n\t */\n\tif (self->rx_flow == FLOW_STOP) {\n\t\tif ((atomic_read(&sk->sk_rmem_alloc) << 2) <= sk->sk_rcvbuf) {\n\t\t\tIRDA_DEBUG(2, \"%s(), Starting IrTTP\\n\", __func__);\n\t\t\tself->rx_flow = FLOW_START;\n\t\t\tirttp_flow_request(self->tsap, FLOW_START);\n\t\t}\n\t}\n\n\treturn copied;\n}",
        "code_after_change": "static int irda_recvmsg_stream(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct irda_sock *self = irda_sk(sk);\n\tint noblock = flags & MSG_DONTWAIT;\n\tsize_t copied = 0;\n\tint target, err;\n\tlong timeo;\n\n\tIRDA_DEBUG(3, \"%s()\\n\", __func__);\n\n\tif ((err = sock_error(sk)) < 0)\n\t\treturn err;\n\n\tif (sock->flags & __SO_ACCEPTCON)\n\t\treturn -EINVAL;\n\n\terr =-EOPNOTSUPP;\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\terr = 0;\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, noblock);\n\n\tdo {\n\t\tint chunk;\n\t\tstruct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);\n\n\t\tif (skb == NULL) {\n\t\t\tDEFINE_WAIT(wait);\n\t\t\terr = 0;\n\n\t\t\tif (copied >= target)\n\t\t\t\tbreak;\n\n\t\t\tprepare_to_wait_exclusive(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\t;\n\t\t\telse if (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\t;\n\t\t\telse if (noblock)\n\t\t\t\terr = -EAGAIN;\n\t\t\telse if (signal_pending(current))\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\telse if (sk->sk_state != TCP_ESTABLISHED)\n\t\t\t\terr = -ENOTCONN;\n\t\t\telse if (skb_peek(&sk->sk_receive_queue) == NULL)\n\t\t\t\t/* Wait process until data arrives */\n\t\t\t\tschedule();\n\n\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, skb->len, size);\n\t\tif (memcpy_toiovec(msg->msg_iov, skb->data, chunk)) {\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tskb_pull(skb, chunk);\n\n\t\t\t/* put the skb back if we didn't use it up.. */\n\t\t\tif (skb->len) {\n\t\t\t\tIRDA_DEBUG(1, \"%s(), back on q!\\n\",\n\t\t\t\t\t   __func__);\n\t\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tkfree_skb(skb);\n\t\t} else {\n\t\t\tIRDA_DEBUG(0, \"%s() questionable!?\\n\", __func__);\n\n\t\t\t/* put message back and return */\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\t/*\n\t *  Check if we have previously stopped IrTTP and we know\n\t *  have more free space in our rx_queue. If so tell IrTTP\n\t *  to start delivering frames again before our rx_queue gets\n\t *  empty\n\t */\n\tif (self->rx_flow == FLOW_STOP) {\n\t\tif ((atomic_read(&sk->sk_rmem_alloc) << 2) <= sk->sk_rcvbuf) {\n\t\t\tIRDA_DEBUG(2, \"%s(), Starting IrTTP\\n\", __func__);\n\t\t\tself->rx_flow = FLOW_START;\n\t\t\tirttp_flow_request(self->tsap, FLOW_START);\n\t\t}\n\t}\n\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,8 +23,6 @@\n \terr = 0;\n \ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n \ttimeo = sock_rcvtimeo(sk, noblock);\n-\n-\tmsg->msg_namelen = 0;\n \n \tdo {\n \t\tint chunk;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 383
    },
    {
        "cve_id": "CVE-2012-2136",
        "code_before_change": "struct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,\n\t\t\t\t     unsigned long data_len, int noblock,\n\t\t\t\t     int *errcode)\n{\n\tstruct sk_buff *skb;\n\tgfp_t gfp_mask;\n\tlong timeo;\n\tint err;\n\n\tgfp_mask = sk->sk_allocation;\n\tif (gfp_mask & __GFP_WAIT)\n\t\tgfp_mask |= __GFP_REPEAT;\n\n\ttimeo = sock_sndtimeo(sk, noblock);\n\twhile (1) {\n\t\terr = sock_error(sk);\n\t\tif (err != 0)\n\t\t\tgoto failure;\n\n\t\terr = -EPIPE;\n\t\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\t\tgoto failure;\n\n\t\tif (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {\n\t\t\tskb = alloc_skb(header_len, gfp_mask);\n\t\t\tif (skb) {\n\t\t\t\tint npages;\n\t\t\t\tint i;\n\n\t\t\t\t/* No pages, we're done... */\n\t\t\t\tif (!data_len)\n\t\t\t\t\tbreak;\n\n\t\t\t\tnpages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;\n\t\t\t\tskb->truesize += data_len;\n\t\t\t\tskb_shinfo(skb)->nr_frags = npages;\n\t\t\t\tfor (i = 0; i < npages; i++) {\n\t\t\t\t\tstruct page *page;\n\n\t\t\t\t\tpage = alloc_pages(sk->sk_allocation, 0);\n\t\t\t\t\tif (!page) {\n\t\t\t\t\t\terr = -ENOBUFS;\n\t\t\t\t\t\tskb_shinfo(skb)->nr_frags = i;\n\t\t\t\t\t\tkfree_skb(skb);\n\t\t\t\t\t\tgoto failure;\n\t\t\t\t\t}\n\n\t\t\t\t\t__skb_fill_page_desc(skb, i,\n\t\t\t\t\t\t\tpage, 0,\n\t\t\t\t\t\t\t(data_len >= PAGE_SIZE ?\n\t\t\t\t\t\t\t PAGE_SIZE :\n\t\t\t\t\t\t\t data_len));\n\t\t\t\t\tdata_len -= PAGE_SIZE;\n\t\t\t\t}\n\n\t\t\t\t/* Full success... */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto failure;\n\t\t}\n\t\tset_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto failure;\n\t\tif (signal_pending(current))\n\t\t\tgoto interrupted;\n\t\ttimeo = sock_wait_for_wmem(sk, timeo);\n\t}\n\n\tskb_set_owner_w(skb, sk);\n\treturn skb;\n\ninterrupted:\n\terr = sock_intr_errno(timeo);\nfailure:\n\t*errcode = err;\n\treturn NULL;\n}",
        "code_after_change": "struct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,\n\t\t\t\t     unsigned long data_len, int noblock,\n\t\t\t\t     int *errcode)\n{\n\tstruct sk_buff *skb;\n\tgfp_t gfp_mask;\n\tlong timeo;\n\tint err;\n\tint npages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;\n\n\terr = -EMSGSIZE;\n\tif (npages > MAX_SKB_FRAGS)\n\t\tgoto failure;\n\n\tgfp_mask = sk->sk_allocation;\n\tif (gfp_mask & __GFP_WAIT)\n\t\tgfp_mask |= __GFP_REPEAT;\n\n\ttimeo = sock_sndtimeo(sk, noblock);\n\twhile (1) {\n\t\terr = sock_error(sk);\n\t\tif (err != 0)\n\t\t\tgoto failure;\n\n\t\terr = -EPIPE;\n\t\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\t\tgoto failure;\n\n\t\tif (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {\n\t\t\tskb = alloc_skb(header_len, gfp_mask);\n\t\t\tif (skb) {\n\t\t\t\tint i;\n\n\t\t\t\t/* No pages, we're done... */\n\t\t\t\tif (!data_len)\n\t\t\t\t\tbreak;\n\n\t\t\t\tskb->truesize += data_len;\n\t\t\t\tskb_shinfo(skb)->nr_frags = npages;\n\t\t\t\tfor (i = 0; i < npages; i++) {\n\t\t\t\t\tstruct page *page;\n\n\t\t\t\t\tpage = alloc_pages(sk->sk_allocation, 0);\n\t\t\t\t\tif (!page) {\n\t\t\t\t\t\terr = -ENOBUFS;\n\t\t\t\t\t\tskb_shinfo(skb)->nr_frags = i;\n\t\t\t\t\t\tkfree_skb(skb);\n\t\t\t\t\t\tgoto failure;\n\t\t\t\t\t}\n\n\t\t\t\t\t__skb_fill_page_desc(skb, i,\n\t\t\t\t\t\t\tpage, 0,\n\t\t\t\t\t\t\t(data_len >= PAGE_SIZE ?\n\t\t\t\t\t\t\t PAGE_SIZE :\n\t\t\t\t\t\t\t data_len));\n\t\t\t\t\tdata_len -= PAGE_SIZE;\n\t\t\t\t}\n\n\t\t\t\t/* Full success... */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto failure;\n\t\t}\n\t\tset_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto failure;\n\t\tif (signal_pending(current))\n\t\t\tgoto interrupted;\n\t\ttimeo = sock_wait_for_wmem(sk, timeo);\n\t}\n\n\tskb_set_owner_w(skb, sk);\n\treturn skb;\n\ninterrupted:\n\terr = sock_intr_errno(timeo);\nfailure:\n\t*errcode = err;\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,11 @@\n \tgfp_t gfp_mask;\n \tlong timeo;\n \tint err;\n+\tint npages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;\n+\n+\terr = -EMSGSIZE;\n+\tif (npages > MAX_SKB_FRAGS)\n+\t\tgoto failure;\n \n \tgfp_mask = sk->sk_allocation;\n \tif (gfp_mask & __GFP_WAIT)\n@@ -24,14 +29,12 @@\n \t\tif (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {\n \t\t\tskb = alloc_skb(header_len, gfp_mask);\n \t\t\tif (skb) {\n-\t\t\t\tint npages;\n \t\t\t\tint i;\n \n \t\t\t\t/* No pages, we're done... */\n \t\t\t\tif (!data_len)\n \t\t\t\t\tbreak;\n \n-\t\t\t\tnpages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;\n \t\t\t\tskb->truesize += data_len;\n \t\t\t\tskb_shinfo(skb)->nr_frags = npages;\n \t\t\t\tfor (i = 0; i < npages; i++) {",
        "function_modified_lines": {
            "added": [
                "\tint npages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;",
                "",
                "\terr = -EMSGSIZE;",
                "\tif (npages > MAX_SKB_FRAGS)",
                "\t\tgoto failure;"
            ],
            "deleted": [
                "\t\t\t\tint npages;",
                "\t\t\t\tnpages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The sock_alloc_send_pskb function in net/core/sock.c in the Linux kernel before 3.4.5 does not properly validate a certain length value, which allows local users to cause a denial of service (heap-based buffer overflow and system crash) or possibly gain privileges by leveraging access to a TUN/TAP device.",
        "id": 41
    },
    {
        "cve_id": "CVE-2014-0038",
        "code_before_change": "asmlinkage long compat_sys_recvmmsg(int fd, struct compat_mmsghdr __user *mmsg,\n\t\t\t\t    unsigned int vlen, unsigned int flags,\n\t\t\t\t    struct compat_timespec __user *timeout)\n{\n\tint datagrams;\n\tstruct timespec ktspec;\n\n\tif (flags & MSG_CMSG_COMPAT)\n\t\treturn -EINVAL;\n\n\tif (COMPAT_USE_64BIT_TIME)\n\t\treturn __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,\n\t\t\t\t      flags | MSG_CMSG_COMPAT,\n\t\t\t\t      (struct timespec *) timeout);\n\n\tif (timeout == NULL)\n\t\treturn __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,\n\t\t\t\t      flags | MSG_CMSG_COMPAT, NULL);\n\n\tif (get_compat_timespec(&ktspec, timeout))\n\t\treturn -EFAULT;\n\n\tdatagrams = __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,\n\t\t\t\t   flags | MSG_CMSG_COMPAT, &ktspec);\n\tif (datagrams > 0 && put_compat_timespec(&ktspec, timeout))\n\t\tdatagrams = -EFAULT;\n\n\treturn datagrams;\n}",
        "code_after_change": "asmlinkage long compat_sys_recvmmsg(int fd, struct compat_mmsghdr __user *mmsg,\n\t\t\t\t    unsigned int vlen, unsigned int flags,\n\t\t\t\t    struct compat_timespec __user *timeout)\n{\n\tint datagrams;\n\tstruct timespec ktspec;\n\n\tif (flags & MSG_CMSG_COMPAT)\n\t\treturn -EINVAL;\n\n\tif (timeout == NULL)\n\t\treturn __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,\n\t\t\t\t      flags | MSG_CMSG_COMPAT, NULL);\n\n\tif (compat_get_timespec(&ktspec, timeout))\n\t\treturn -EFAULT;\n\n\tdatagrams = __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,\n\t\t\t\t   flags | MSG_CMSG_COMPAT, &ktspec);\n\tif (datagrams > 0 && compat_put_timespec(&ktspec, timeout))\n\t\tdatagrams = -EFAULT;\n\n\treturn datagrams;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,21 +8,16 @@\n \tif (flags & MSG_CMSG_COMPAT)\n \t\treturn -EINVAL;\n \n-\tif (COMPAT_USE_64BIT_TIME)\n-\t\treturn __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,\n-\t\t\t\t      flags | MSG_CMSG_COMPAT,\n-\t\t\t\t      (struct timespec *) timeout);\n-\n \tif (timeout == NULL)\n \t\treturn __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,\n \t\t\t\t      flags | MSG_CMSG_COMPAT, NULL);\n \n-\tif (get_compat_timespec(&ktspec, timeout))\n+\tif (compat_get_timespec(&ktspec, timeout))\n \t\treturn -EFAULT;\n \n \tdatagrams = __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,\n \t\t\t\t   flags | MSG_CMSG_COMPAT, &ktspec);\n-\tif (datagrams > 0 && put_compat_timespec(&ktspec, timeout))\n+\tif (datagrams > 0 && compat_put_timespec(&ktspec, timeout))\n \t\tdatagrams = -EFAULT;\n \n \treturn datagrams;",
        "function_modified_lines": {
            "added": [
                "\tif (compat_get_timespec(&ktspec, timeout))",
                "\tif (datagrams > 0 && compat_put_timespec(&ktspec, timeout))"
            ],
            "deleted": [
                "\tif (COMPAT_USE_64BIT_TIME)",
                "\t\treturn __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,",
                "\t\t\t\t      flags | MSG_CMSG_COMPAT,",
                "\t\t\t\t      (struct timespec *) timeout);",
                "",
                "\tif (get_compat_timespec(&ktspec, timeout))",
                "\tif (datagrams > 0 && put_compat_timespec(&ktspec, timeout))"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The compat_sys_recvmmsg function in net/compat.c in the Linux kernel before 3.13.2, when CONFIG_X86_X32 is enabled, allows local users to gain privileges via a recvmmsg system call with a crafted timeout pointer parameter.",
        "id": 423
    },
    {
        "cve_id": "CVE-2012-4398",
        "code_before_change": "static int uvesafb_helper_start(void)\n{\n\tchar *envp[] = {\n\t\t\"HOME=/\",\n\t\t\"PATH=/sbin:/bin\",\n\t\tNULL,\n\t};\n\n\tchar *argv[] = {\n\t\tv86d_path,\n\t\tNULL,\n\t};\n\n\treturn call_usermodehelper(v86d_path, argv, envp, 1);\n}",
        "code_after_change": "static int uvesafb_helper_start(void)\n{\n\tchar *envp[] = {\n\t\t\"HOME=/\",\n\t\t\"PATH=/sbin:/bin\",\n\t\tNULL,\n\t};\n\n\tchar *argv[] = {\n\t\tv86d_path,\n\t\tNULL,\n\t};\n\n\treturn call_usermodehelper(v86d_path, argv, envp, UMH_WAIT_PROC);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,5 +11,5 @@\n \t\tNULL,\n \t};\n \n-\treturn call_usermodehelper(v86d_path, argv, envp, 1);\n+\treturn call_usermodehelper(v86d_path, argv, envp, UMH_WAIT_PROC);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn call_usermodehelper(v86d_path, argv, envp, UMH_WAIT_PROC);"
            ],
            "deleted": [
                "\treturn call_usermodehelper(v86d_path, argv, envp, 1);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The __request_module function in kernel/kmod.c in the Linux kernel before 3.4 does not set a certain killable attribute, which allows local users to cause a denial of service (memory consumption) via a crafted application.",
        "id": 99
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int packet_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\tstruct sockaddr_ll *sll;\n\tint vnet_hdr_len = 0;\n\n\terr = -EINVAL;\n\tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT|MSG_ERRQUEUE))\n\t\tgoto out;\n\n#if 0\n\t/* What error should we return now? EUNATTACH? */\n\tif (pkt_sk(sk)->ifindex < 0)\n\t\treturn -ENODEV;\n#endif\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = sock_recv_errqueue(sk, msg, len,\n\t\t\t\t\t SOL_PACKET, PACKET_TX_TIMESTAMP);\n\t\tgoto out;\n\t}\n\n\t/*\n\t *\tCall the generic datagram receiver. This handles all sorts\n\t *\tof horrible races and re-entrancy so we can forget about it\n\t *\tin the protocol layers.\n\t *\n\t *\tNow it will return ENETDOWN, if device have just gone down,\n\t *\tbut then it will block.\n\t */\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\n\t/*\n\t *\tAn error occurred so return it. Because skb_recv_datagram()\n\t *\thandles the blocking we don't see and worry about blocking\n\t *\tretries.\n\t */\n\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tif (pkt_sk(sk)->has_vnet_hdr) {\n\t\tstruct virtio_net_hdr vnet_hdr = { 0 };\n\n\t\terr = -EINVAL;\n\t\tvnet_hdr_len = sizeof(vnet_hdr);\n\t\tif (len < vnet_hdr_len)\n\t\t\tgoto out_free;\n\n\t\tlen -= vnet_hdr_len;\n\n\t\tif (skb_is_gso(skb)) {\n\t\t\tstruct skb_shared_info *sinfo = skb_shinfo(skb);\n\n\t\t\t/* This is a hint as to how much should be linear. */\n\t\t\tvnet_hdr.hdr_len = skb_headlen(skb);\n\t\t\tvnet_hdr.gso_size = sinfo->gso_size;\n\t\t\tif (sinfo->gso_type & SKB_GSO_TCPV4)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_TCPV4;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_TCPV6)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_TCPV6;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_UDP)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_UDP;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_FCOE)\n\t\t\t\tgoto out_free;\n\t\t\telse\n\t\t\t\tBUG();\n\t\t\tif (sinfo->gso_type & SKB_GSO_TCP_ECN)\n\t\t\t\tvnet_hdr.gso_type |= VIRTIO_NET_HDR_GSO_ECN;\n\t\t} else\n\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tvnet_hdr.flags = VIRTIO_NET_HDR_F_NEEDS_CSUM;\n\t\t\tvnet_hdr.csum_start = skb_checksum_start_offset(skb);\n\t\t\tvnet_hdr.csum_offset = skb->csum_offset;\n\t\t} else if (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\t\tvnet_hdr.flags = VIRTIO_NET_HDR_F_DATA_VALID;\n\t\t} /* else everything is zero */\n\n\t\terr = memcpy_toiovec(msg->msg_iov, (void *)&vnet_hdr,\n\t\t\t\t     vnet_hdr_len);\n\t\tif (err < 0)\n\t\t\tgoto out_free;\n\t}\n\n\t/*\n\t *\tIf the address length field is there to be filled in, we fill\n\t *\tit in now.\n\t */\n\n\tsll = &PACKET_SKB_CB(skb)->sa.ll;\n\tif (sock->type == SOCK_PACKET)\n\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);\n\telse\n\t\tmsg->msg_namelen = sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr);\n\n\t/*\n\t *\tYou lose any data beyond the buffer you gave. If it worries a\n\t *\tuser program they can ask the device for its MTU anyway.\n\t */\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto out_free;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (msg->msg_name)\n\t\tmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,\n\t\t       msg->msg_namelen);\n\n\tif (pkt_sk(sk)->auxdata) {\n\t\tstruct tpacket_auxdata aux;\n\n\t\taux.tp_status = TP_STATUS_USER;\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\taux.tp_status |= TP_STATUS_CSUMNOTREADY;\n\t\taux.tp_len = PACKET_SKB_CB(skb)->origlen;\n\t\taux.tp_snaplen = skb->len;\n\t\taux.tp_mac = 0;\n\t\taux.tp_net = skb_network_offset(skb);\n\t\tif (vlan_tx_tag_present(skb)) {\n\t\t\taux.tp_vlan_tci = vlan_tx_tag_get(skb);\n\t\t\taux.tp_status |= TP_STATUS_VLAN_VALID;\n\t\t} else {\n\t\t\taux.tp_vlan_tci = 0;\n\t\t}\n\t\taux.tp_padding = 0;\n\t\tput_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);\n\t}\n\n\t/*\n\t *\tFree or return the buffer as appropriate. Again this\n\t *\thides all the races and re-entrancy issues from us.\n\t */\n\terr = vnet_hdr_len + ((flags&MSG_TRUNC) ? skb->len : copied);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n}",
        "code_after_change": "static int packet_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\tint vnet_hdr_len = 0;\n\n\terr = -EINVAL;\n\tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT|MSG_ERRQUEUE))\n\t\tgoto out;\n\n#if 0\n\t/* What error should we return now? EUNATTACH? */\n\tif (pkt_sk(sk)->ifindex < 0)\n\t\treturn -ENODEV;\n#endif\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = sock_recv_errqueue(sk, msg, len,\n\t\t\t\t\t SOL_PACKET, PACKET_TX_TIMESTAMP);\n\t\tgoto out;\n\t}\n\n\t/*\n\t *\tCall the generic datagram receiver. This handles all sorts\n\t *\tof horrible races and re-entrancy so we can forget about it\n\t *\tin the protocol layers.\n\t *\n\t *\tNow it will return ENETDOWN, if device have just gone down,\n\t *\tbut then it will block.\n\t */\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\n\t/*\n\t *\tAn error occurred so return it. Because skb_recv_datagram()\n\t *\thandles the blocking we don't see and worry about blocking\n\t *\tretries.\n\t */\n\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tif (pkt_sk(sk)->has_vnet_hdr) {\n\t\tstruct virtio_net_hdr vnet_hdr = { 0 };\n\n\t\terr = -EINVAL;\n\t\tvnet_hdr_len = sizeof(vnet_hdr);\n\t\tif (len < vnet_hdr_len)\n\t\t\tgoto out_free;\n\n\t\tlen -= vnet_hdr_len;\n\n\t\tif (skb_is_gso(skb)) {\n\t\t\tstruct skb_shared_info *sinfo = skb_shinfo(skb);\n\n\t\t\t/* This is a hint as to how much should be linear. */\n\t\t\tvnet_hdr.hdr_len = skb_headlen(skb);\n\t\t\tvnet_hdr.gso_size = sinfo->gso_size;\n\t\t\tif (sinfo->gso_type & SKB_GSO_TCPV4)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_TCPV4;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_TCPV6)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_TCPV6;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_UDP)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_UDP;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_FCOE)\n\t\t\t\tgoto out_free;\n\t\t\telse\n\t\t\t\tBUG();\n\t\t\tif (sinfo->gso_type & SKB_GSO_TCP_ECN)\n\t\t\t\tvnet_hdr.gso_type |= VIRTIO_NET_HDR_GSO_ECN;\n\t\t} else\n\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tvnet_hdr.flags = VIRTIO_NET_HDR_F_NEEDS_CSUM;\n\t\t\tvnet_hdr.csum_start = skb_checksum_start_offset(skb);\n\t\t\tvnet_hdr.csum_offset = skb->csum_offset;\n\t\t} else if (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\t\tvnet_hdr.flags = VIRTIO_NET_HDR_F_DATA_VALID;\n\t\t} /* else everything is zero */\n\n\t\terr = memcpy_toiovec(msg->msg_iov, (void *)&vnet_hdr,\n\t\t\t\t     vnet_hdr_len);\n\t\tif (err < 0)\n\t\t\tgoto out_free;\n\t}\n\n\t/* You lose any data beyond the buffer you gave. If it worries\n\t * a user program they can ask the device for its MTU\n\t * anyway.\n\t */\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto out_free;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (msg->msg_name) {\n\t\t/* If the address length field is there to be filled\n\t\t * in, we fill it in now.\n\t\t */\n\t\tif (sock->type == SOCK_PACKET) {\n\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);\n\t\t} else {\n\t\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n\t\t\tmsg->msg_namelen = sll->sll_halen +\n\t\t\t\toffsetof(struct sockaddr_ll, sll_addr);\n\t\t}\n\t\tmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,\n\t\t       msg->msg_namelen);\n\t}\n\n\tif (pkt_sk(sk)->auxdata) {\n\t\tstruct tpacket_auxdata aux;\n\n\t\taux.tp_status = TP_STATUS_USER;\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\taux.tp_status |= TP_STATUS_CSUMNOTREADY;\n\t\taux.tp_len = PACKET_SKB_CB(skb)->origlen;\n\t\taux.tp_snaplen = skb->len;\n\t\taux.tp_mac = 0;\n\t\taux.tp_net = skb_network_offset(skb);\n\t\tif (vlan_tx_tag_present(skb)) {\n\t\t\taux.tp_vlan_tci = vlan_tx_tag_get(skb);\n\t\t\taux.tp_status |= TP_STATUS_VLAN_VALID;\n\t\t} else {\n\t\t\taux.tp_vlan_tci = 0;\n\t\t}\n\t\taux.tp_padding = 0;\n\t\tput_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);\n\t}\n\n\t/*\n\t *\tFree or return the buffer as appropriate. Again this\n\t *\thides all the races and re-entrancy issues from us.\n\t */\n\terr = vnet_hdr_len + ((flags&MSG_TRUNC) ? skb->len : copied);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \tstruct sock *sk = sock->sk;\n \tstruct sk_buff *skb;\n \tint copied, err;\n-\tstruct sockaddr_ll *sll;\n \tint vnet_hdr_len = 0;\n \n \terr = -EINVAL;\n@@ -88,22 +87,10 @@\n \t\t\tgoto out_free;\n \t}\n \n-\t/*\n-\t *\tIf the address length field is there to be filled in, we fill\n-\t *\tit in now.\n+\t/* You lose any data beyond the buffer you gave. If it worries\n+\t * a user program they can ask the device for its MTU\n+\t * anyway.\n \t */\n-\n-\tsll = &PACKET_SKB_CB(skb)->sa.ll;\n-\tif (sock->type == SOCK_PACKET)\n-\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);\n-\telse\n-\t\tmsg->msg_namelen = sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr);\n-\n-\t/*\n-\t *\tYou lose any data beyond the buffer you gave. If it worries a\n-\t *\tuser program they can ask the device for its MTU anyway.\n-\t */\n-\n \tcopied = skb->len;\n \tif (copied > len) {\n \t\tcopied = len;\n@@ -116,9 +103,20 @@\n \n \tsock_recv_ts_and_drops(msg, sk, skb);\n \n-\tif (msg->msg_name)\n+\tif (msg->msg_name) {\n+\t\t/* If the address length field is there to be filled\n+\t\t * in, we fill it in now.\n+\t\t */\n+\t\tif (sock->type == SOCK_PACKET) {\n+\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);\n+\t\t} else {\n+\t\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n+\t\t\tmsg->msg_namelen = sll->sll_halen +\n+\t\t\t\toffsetof(struct sockaddr_ll, sll_addr);\n+\t\t}\n \t\tmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,\n \t\t       msg->msg_namelen);\n+\t}\n \n \tif (pkt_sk(sk)->auxdata) {\n \t\tstruct tpacket_auxdata aux;",
        "function_modified_lines": {
            "added": [
                "\t/* You lose any data beyond the buffer you gave. If it worries",
                "\t * a user program they can ask the device for its MTU",
                "\t * anyway.",
                "\tif (msg->msg_name) {",
                "\t\t/* If the address length field is there to be filled",
                "\t\t * in, we fill it in now.",
                "\t\t */",
                "\t\tif (sock->type == SOCK_PACKET) {",
                "\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);",
                "\t\t} else {",
                "\t\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;",
                "\t\t\tmsg->msg_namelen = sll->sll_halen +",
                "\t\t\t\toffsetof(struct sockaddr_ll, sll_addr);",
                "\t\t}",
                "\t}"
            ],
            "deleted": [
                "\tstruct sockaddr_ll *sll;",
                "\t/*",
                "\t *\tIf the address length field is there to be filled in, we fill",
                "\t *\tit in now.",
                "",
                "\tsll = &PACKET_SKB_CB(skb)->sa.ll;",
                "\tif (sock->type == SOCK_PACKET)",
                "\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);",
                "\telse",
                "\t\tmsg->msg_namelen = sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr);",
                "",
                "\t/*",
                "\t *\tYou lose any data beyond the buffer you gave. If it worries a",
                "\t *\tuser program they can ask the device for its MTU anyway.",
                "\t */",
                "",
                "\tif (msg->msg_name)"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 393
    },
    {
        "cve_id": "CVE-2017-18200",
        "code_before_change": "static void f2fs_put_super(struct super_block *sb)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tint i;\n\n\tf2fs_quota_off_umount(sb);\n\n\t/* prevent remaining shrinker jobs */\n\tmutex_lock(&sbi->umount_mutex);\n\n\t/*\n\t * We don't need to do checkpoint when superblock is clean.\n\t * But, the previous checkpoint was not done by umount, it needs to do\n\t * clean checkpoint again.\n\t */\n\tif (is_sbi_flag_set(sbi, SBI_IS_DIRTY) ||\n\t\t\t!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\tstruct cp_control cpc = {\n\t\t\t.reason = CP_UMOUNT,\n\t\t};\n\t\twrite_checkpoint(sbi, &cpc);\n\t}\n\n\t/* be sure to wait for any on-going discard commands */\n\tf2fs_wait_discard_bios(sbi);\n\n\tif (f2fs_discard_en(sbi) && !sbi->discard_blks) {\n\t\tstruct cp_control cpc = {\n\t\t\t.reason = CP_UMOUNT | CP_TRIMMED,\n\t\t};\n\t\twrite_checkpoint(sbi, &cpc);\n\t}\n\n\t/* write_checkpoint can update stat informaion */\n\tf2fs_destroy_stats(sbi);\n\n\t/*\n\t * normally superblock is clean, so we need to release this.\n\t * In addition, EIO will skip do checkpoint, we need this as well.\n\t */\n\trelease_ino_entry(sbi, true);\n\n\tf2fs_leave_shrinker(sbi);\n\tmutex_unlock(&sbi->umount_mutex);\n\n\t/* our cp_error case, we can wait for any writeback page */\n\tf2fs_flush_merged_writes(sbi);\n\n\tiput(sbi->node_inode);\n\tiput(sbi->meta_inode);\n\n\t/* destroy f2fs internal modules */\n\tdestroy_node_manager(sbi);\n\tdestroy_segment_manager(sbi);\n\n\tkfree(sbi->ckpt);\n\n\tf2fs_unregister_sysfs(sbi);\n\n\tsb->s_fs_info = NULL;\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi->raw_super);\n\n\tdestroy_device_list(sbi);\n\tmempool_destroy(sbi->write_io_dummy);\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(sbi->s_qf_names[i]);\n#endif\n\tdestroy_percpu_info(sbi);\n\tfor (i = 0; i < NR_PAGE_TYPE; i++)\n\t\tkfree(sbi->write_io[i]);\n\tkfree(sbi);\n}",
        "code_after_change": "static void f2fs_put_super(struct super_block *sb)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_SB(sb);\n\tint i;\n\n\tf2fs_quota_off_umount(sb);\n\n\t/* prevent remaining shrinker jobs */\n\tmutex_lock(&sbi->umount_mutex);\n\n\t/*\n\t * We don't need to do checkpoint when superblock is clean.\n\t * But, the previous checkpoint was not done by umount, it needs to do\n\t * clean checkpoint again.\n\t */\n\tif (is_sbi_flag_set(sbi, SBI_IS_DIRTY) ||\n\t\t\t!is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG)) {\n\t\tstruct cp_control cpc = {\n\t\t\t.reason = CP_UMOUNT,\n\t\t};\n\t\twrite_checkpoint(sbi, &cpc);\n\t}\n\n\t/* be sure to wait for any on-going discard commands */\n\tf2fs_wait_discard_bios(sbi, true);\n\n\tif (f2fs_discard_en(sbi) && !sbi->discard_blks) {\n\t\tstruct cp_control cpc = {\n\t\t\t.reason = CP_UMOUNT | CP_TRIMMED,\n\t\t};\n\t\twrite_checkpoint(sbi, &cpc);\n\t}\n\n\t/* write_checkpoint can update stat informaion */\n\tf2fs_destroy_stats(sbi);\n\n\t/*\n\t * normally superblock is clean, so we need to release this.\n\t * In addition, EIO will skip do checkpoint, we need this as well.\n\t */\n\trelease_ino_entry(sbi, true);\n\n\tf2fs_leave_shrinker(sbi);\n\tmutex_unlock(&sbi->umount_mutex);\n\n\t/* our cp_error case, we can wait for any writeback page */\n\tf2fs_flush_merged_writes(sbi);\n\n\tiput(sbi->node_inode);\n\tiput(sbi->meta_inode);\n\n\t/* destroy f2fs internal modules */\n\tdestroy_node_manager(sbi);\n\tdestroy_segment_manager(sbi);\n\n\tkfree(sbi->ckpt);\n\n\tf2fs_unregister_sysfs(sbi);\n\n\tsb->s_fs_info = NULL;\n\tif (sbi->s_chksum_driver)\n\t\tcrypto_free_shash(sbi->s_chksum_driver);\n\tkfree(sbi->raw_super);\n\n\tdestroy_device_list(sbi);\n\tmempool_destroy(sbi->write_io_dummy);\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(sbi->s_qf_names[i]);\n#endif\n\tdestroy_percpu_info(sbi);\n\tfor (i = 0; i < NR_PAGE_TYPE; i++)\n\t\tkfree(sbi->write_io[i]);\n\tkfree(sbi);\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,7 +22,7 @@\n \t}\n \n \t/* be sure to wait for any on-going discard commands */\n-\tf2fs_wait_discard_bios(sbi);\n+\tf2fs_wait_discard_bios(sbi, true);\n \n \tif (f2fs_discard_en(sbi) && !sbi->discard_blks) {\n \t\tstruct cp_control cpc = {",
        "function_modified_lines": {
            "added": [
                "\tf2fs_wait_discard_bios(sbi, true);"
            ],
            "deleted": [
                "\tf2fs_wait_discard_bios(sbi);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The f2fs implementation in the Linux kernel before 4.14 mishandles reference counts associated with f2fs_wait_discard_bios calls, which allows local users to cause a denial of service (BUG), as demonstrated by fstrim.",
        "id": 1396
    },
    {
        "cve_id": "CVE-2021-3655",
        "code_before_change": "static int sctp_process_param(struct sctp_association *asoc,\n\t\t\t      union sctp_params param,\n\t\t\t      const union sctp_addr *peer_addr,\n\t\t\t      gfp_t gfp)\n{\n\tstruct sctp_endpoint *ep = asoc->ep;\n\tunion sctp_addr_param *addr_param;\n\tstruct net *net = asoc->base.net;\n\tstruct sctp_transport *t;\n\tenum sctp_scope scope;\n\tunion sctp_addr addr;\n\tstruct sctp_af *af;\n\tint retval = 1, i;\n\tu32 stale;\n\t__u16 sat;\n\n\t/* We maintain all INIT parameters in network byte order all the\n\t * time.  This allows us to not worry about whether the parameters\n\t * came from a fresh INIT, and INIT ACK, or were stored in a cookie.\n\t */\n\tswitch (param.p->type) {\n\tcase SCTP_PARAM_IPV6_ADDRESS:\n\t\tif (PF_INET6 != asoc->base.sk->sk_family)\n\t\t\tbreak;\n\t\tgoto do_addr_param;\n\n\tcase SCTP_PARAM_IPV4_ADDRESS:\n\t\t/* v4 addresses are not allowed on v6-only socket */\n\t\tif (ipv6_only_sock(asoc->base.sk))\n\t\t\tbreak;\ndo_addr_param:\n\t\taf = sctp_get_af_specific(param_type2af(param.p->type));\n\t\taf->from_addr_param(&addr, param.addr, htons(asoc->peer.port), 0);\n\t\tscope = sctp_scope(peer_addr);\n\t\tif (sctp_in_scope(net, &addr, scope))\n\t\t\tif (!sctp_assoc_add_peer(asoc, &addr, gfp, SCTP_UNCONFIRMED))\n\t\t\t\treturn 0;\n\t\tbreak;\n\n\tcase SCTP_PARAM_COOKIE_PRESERVATIVE:\n\t\tif (!net->sctp.cookie_preserve_enable)\n\t\t\tbreak;\n\n\t\tstale = ntohl(param.life->lifespan_increment);\n\n\t\t/* Suggested Cookie Life span increment's unit is msec,\n\t\t * (1/1000sec).\n\t\t */\n\t\tasoc->cookie_life = ktime_add_ms(asoc->cookie_life, stale);\n\t\tbreak;\n\n\tcase SCTP_PARAM_HOST_NAME_ADDRESS:\n\t\tpr_debug(\"%s: unimplemented SCTP_HOST_NAME_ADDRESS\\n\", __func__);\n\t\tbreak;\n\n\tcase SCTP_PARAM_SUPPORTED_ADDRESS_TYPES:\n\t\t/* Turn off the default values first so we'll know which\n\t\t * ones are really set by the peer.\n\t\t */\n\t\tasoc->peer.ipv4_address = 0;\n\t\tasoc->peer.ipv6_address = 0;\n\n\t\t/* Assume that peer supports the address family\n\t\t * by which it sends a packet.\n\t\t */\n\t\tif (peer_addr->sa.sa_family == AF_INET6)\n\t\t\tasoc->peer.ipv6_address = 1;\n\t\telse if (peer_addr->sa.sa_family == AF_INET)\n\t\t\tasoc->peer.ipv4_address = 1;\n\n\t\t/* Cycle through address types; avoid divide by 0. */\n\t\tsat = ntohs(param.p->length) - sizeof(struct sctp_paramhdr);\n\t\tif (sat)\n\t\t\tsat /= sizeof(__u16);\n\n\t\tfor (i = 0; i < sat; ++i) {\n\t\t\tswitch (param.sat->types[i]) {\n\t\t\tcase SCTP_PARAM_IPV4_ADDRESS:\n\t\t\t\tasoc->peer.ipv4_address = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase SCTP_PARAM_IPV6_ADDRESS:\n\t\t\t\tif (PF_INET6 == asoc->base.sk->sk_family)\n\t\t\t\t\tasoc->peer.ipv6_address = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase SCTP_PARAM_HOST_NAME_ADDRESS:\n\t\t\t\tasoc->peer.hostname_address = 1;\n\t\t\t\tbreak;\n\n\t\t\tdefault: /* Just ignore anything else.  */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase SCTP_PARAM_STATE_COOKIE:\n\t\tasoc->peer.cookie_len =\n\t\t\tntohs(param.p->length) - sizeof(struct sctp_paramhdr);\n\t\tkfree(asoc->peer.cookie);\n\t\tasoc->peer.cookie = kmemdup(param.cookie->body, asoc->peer.cookie_len, gfp);\n\t\tif (!asoc->peer.cookie)\n\t\t\tretval = 0;\n\t\tbreak;\n\n\tcase SCTP_PARAM_HEARTBEAT_INFO:\n\t\t/* Would be odd to receive, but it causes no problems. */\n\t\tbreak;\n\n\tcase SCTP_PARAM_UNRECOGNIZED_PARAMETERS:\n\t\t/* Rejected during verify stage. */\n\t\tbreak;\n\n\tcase SCTP_PARAM_ECN_CAPABLE:\n\t\tif (asoc->ep->ecn_enable) {\n\t\t\tasoc->peer.ecn_capable = 1;\n\t\t\tbreak;\n\t\t}\n\t\t/* Fall Through */\n\t\tgoto fall_through;\n\n\n\tcase SCTP_PARAM_ADAPTATION_LAYER_IND:\n\t\tasoc->peer.adaptation_ind = ntohl(param.aind->adaptation_ind);\n\t\tbreak;\n\n\tcase SCTP_PARAM_SET_PRIMARY:\n\t\tif (!ep->asconf_enable)\n\t\t\tgoto fall_through;\n\n\t\taddr_param = param.v + sizeof(struct sctp_addip_param);\n\n\t\taf = sctp_get_af_specific(param_type2af(addr_param->p.type));\n\t\tif (af == NULL)\n\t\t\tbreak;\n\n\t\taf->from_addr_param(&addr, addr_param,\n\t\t\t\t    htons(asoc->peer.port), 0);\n\n\t\t/* if the address is invalid, we can't process it.\n\t\t * XXX: see spec for what to do.\n\t\t */\n\t\tif (!af->addr_valid(&addr, NULL, NULL))\n\t\t\tbreak;\n\n\t\tt = sctp_assoc_lookup_paddr(asoc, &addr);\n\t\tif (!t)\n\t\t\tbreak;\n\n\t\tsctp_assoc_set_primary(asoc, t);\n\t\tbreak;\n\n\tcase SCTP_PARAM_SUPPORTED_EXT:\n\t\tsctp_process_ext_param(asoc, param);\n\t\tbreak;\n\n\tcase SCTP_PARAM_FWD_TSN_SUPPORT:\n\t\tif (asoc->ep->prsctp_enable) {\n\t\t\tasoc->peer.prsctp_capable = 1;\n\t\t\tbreak;\n\t\t}\n\t\t/* Fall Through */\n\t\tgoto fall_through;\n\n\tcase SCTP_PARAM_RANDOM:\n\t\tif (!ep->auth_enable)\n\t\t\tgoto fall_through;\n\n\t\t/* Save peer's random parameter */\n\t\tkfree(asoc->peer.peer_random);\n\t\tasoc->peer.peer_random = kmemdup(param.p,\n\t\t\t\t\t    ntohs(param.p->length), gfp);\n\t\tif (!asoc->peer.peer_random) {\n\t\t\tretval = 0;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase SCTP_PARAM_HMAC_ALGO:\n\t\tif (!ep->auth_enable)\n\t\t\tgoto fall_through;\n\n\t\t/* Save peer's HMAC list */\n\t\tkfree(asoc->peer.peer_hmacs);\n\t\tasoc->peer.peer_hmacs = kmemdup(param.p,\n\t\t\t\t\t    ntohs(param.p->length), gfp);\n\t\tif (!asoc->peer.peer_hmacs) {\n\t\t\tretval = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Set the default HMAC the peer requested*/\n\t\tsctp_auth_asoc_set_default_hmac(asoc, param.hmac_algo);\n\t\tbreak;\n\n\tcase SCTP_PARAM_CHUNKS:\n\t\tif (!ep->auth_enable)\n\t\t\tgoto fall_through;\n\n\t\tkfree(asoc->peer.peer_chunks);\n\t\tasoc->peer.peer_chunks = kmemdup(param.p,\n\t\t\t\t\t    ntohs(param.p->length), gfp);\n\t\tif (!asoc->peer.peer_chunks)\n\t\t\tretval = 0;\n\t\tbreak;\nfall_through:\n\tdefault:\n\t\t/* Any unrecognized parameters should have been caught\n\t\t * and handled by sctp_verify_param() which should be\n\t\t * called prior to this routine.  Simply log the error\n\t\t * here.\n\t\t */\n\t\tpr_debug(\"%s: ignoring param:%d for association:%p.\\n\",\n\t\t\t __func__, ntohs(param.p->type), asoc);\n\t\tbreak;\n\t}\n\n\treturn retval;\n}",
        "code_after_change": "static int sctp_process_param(struct sctp_association *asoc,\n\t\t\t      union sctp_params param,\n\t\t\t      const union sctp_addr *peer_addr,\n\t\t\t      gfp_t gfp)\n{\n\tstruct sctp_endpoint *ep = asoc->ep;\n\tunion sctp_addr_param *addr_param;\n\tstruct net *net = asoc->base.net;\n\tstruct sctp_transport *t;\n\tenum sctp_scope scope;\n\tunion sctp_addr addr;\n\tstruct sctp_af *af;\n\tint retval = 1, i;\n\tu32 stale;\n\t__u16 sat;\n\n\t/* We maintain all INIT parameters in network byte order all the\n\t * time.  This allows us to not worry about whether the parameters\n\t * came from a fresh INIT, and INIT ACK, or were stored in a cookie.\n\t */\n\tswitch (param.p->type) {\n\tcase SCTP_PARAM_IPV6_ADDRESS:\n\t\tif (PF_INET6 != asoc->base.sk->sk_family)\n\t\t\tbreak;\n\t\tgoto do_addr_param;\n\n\tcase SCTP_PARAM_IPV4_ADDRESS:\n\t\t/* v4 addresses are not allowed on v6-only socket */\n\t\tif (ipv6_only_sock(asoc->base.sk))\n\t\t\tbreak;\ndo_addr_param:\n\t\taf = sctp_get_af_specific(param_type2af(param.p->type));\n\t\tif (!af->from_addr_param(&addr, param.addr, htons(asoc->peer.port), 0))\n\t\t\tbreak;\n\t\tscope = sctp_scope(peer_addr);\n\t\tif (sctp_in_scope(net, &addr, scope))\n\t\t\tif (!sctp_assoc_add_peer(asoc, &addr, gfp, SCTP_UNCONFIRMED))\n\t\t\t\treturn 0;\n\t\tbreak;\n\n\tcase SCTP_PARAM_COOKIE_PRESERVATIVE:\n\t\tif (!net->sctp.cookie_preserve_enable)\n\t\t\tbreak;\n\n\t\tstale = ntohl(param.life->lifespan_increment);\n\n\t\t/* Suggested Cookie Life span increment's unit is msec,\n\t\t * (1/1000sec).\n\t\t */\n\t\tasoc->cookie_life = ktime_add_ms(asoc->cookie_life, stale);\n\t\tbreak;\n\n\tcase SCTP_PARAM_HOST_NAME_ADDRESS:\n\t\tpr_debug(\"%s: unimplemented SCTP_HOST_NAME_ADDRESS\\n\", __func__);\n\t\tbreak;\n\n\tcase SCTP_PARAM_SUPPORTED_ADDRESS_TYPES:\n\t\t/* Turn off the default values first so we'll know which\n\t\t * ones are really set by the peer.\n\t\t */\n\t\tasoc->peer.ipv4_address = 0;\n\t\tasoc->peer.ipv6_address = 0;\n\n\t\t/* Assume that peer supports the address family\n\t\t * by which it sends a packet.\n\t\t */\n\t\tif (peer_addr->sa.sa_family == AF_INET6)\n\t\t\tasoc->peer.ipv6_address = 1;\n\t\telse if (peer_addr->sa.sa_family == AF_INET)\n\t\t\tasoc->peer.ipv4_address = 1;\n\n\t\t/* Cycle through address types; avoid divide by 0. */\n\t\tsat = ntohs(param.p->length) - sizeof(struct sctp_paramhdr);\n\t\tif (sat)\n\t\t\tsat /= sizeof(__u16);\n\n\t\tfor (i = 0; i < sat; ++i) {\n\t\t\tswitch (param.sat->types[i]) {\n\t\t\tcase SCTP_PARAM_IPV4_ADDRESS:\n\t\t\t\tasoc->peer.ipv4_address = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase SCTP_PARAM_IPV6_ADDRESS:\n\t\t\t\tif (PF_INET6 == asoc->base.sk->sk_family)\n\t\t\t\t\tasoc->peer.ipv6_address = 1;\n\t\t\t\tbreak;\n\n\t\t\tcase SCTP_PARAM_HOST_NAME_ADDRESS:\n\t\t\t\tasoc->peer.hostname_address = 1;\n\t\t\t\tbreak;\n\n\t\t\tdefault: /* Just ignore anything else.  */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase SCTP_PARAM_STATE_COOKIE:\n\t\tasoc->peer.cookie_len =\n\t\t\tntohs(param.p->length) - sizeof(struct sctp_paramhdr);\n\t\tkfree(asoc->peer.cookie);\n\t\tasoc->peer.cookie = kmemdup(param.cookie->body, asoc->peer.cookie_len, gfp);\n\t\tif (!asoc->peer.cookie)\n\t\t\tretval = 0;\n\t\tbreak;\n\n\tcase SCTP_PARAM_HEARTBEAT_INFO:\n\t\t/* Would be odd to receive, but it causes no problems. */\n\t\tbreak;\n\n\tcase SCTP_PARAM_UNRECOGNIZED_PARAMETERS:\n\t\t/* Rejected during verify stage. */\n\t\tbreak;\n\n\tcase SCTP_PARAM_ECN_CAPABLE:\n\t\tif (asoc->ep->ecn_enable) {\n\t\t\tasoc->peer.ecn_capable = 1;\n\t\t\tbreak;\n\t\t}\n\t\t/* Fall Through */\n\t\tgoto fall_through;\n\n\n\tcase SCTP_PARAM_ADAPTATION_LAYER_IND:\n\t\tasoc->peer.adaptation_ind = ntohl(param.aind->adaptation_ind);\n\t\tbreak;\n\n\tcase SCTP_PARAM_SET_PRIMARY:\n\t\tif (!ep->asconf_enable)\n\t\t\tgoto fall_through;\n\n\t\taddr_param = param.v + sizeof(struct sctp_addip_param);\n\n\t\taf = sctp_get_af_specific(param_type2af(addr_param->p.type));\n\t\tif (!af)\n\t\t\tbreak;\n\n\t\tif (!af->from_addr_param(&addr, addr_param,\n\t\t\t\t\t htons(asoc->peer.port), 0))\n\t\t\tbreak;\n\n\t\tif (!af->addr_valid(&addr, NULL, NULL))\n\t\t\tbreak;\n\n\t\tt = sctp_assoc_lookup_paddr(asoc, &addr);\n\t\tif (!t)\n\t\t\tbreak;\n\n\t\tsctp_assoc_set_primary(asoc, t);\n\t\tbreak;\n\n\tcase SCTP_PARAM_SUPPORTED_EXT:\n\t\tsctp_process_ext_param(asoc, param);\n\t\tbreak;\n\n\tcase SCTP_PARAM_FWD_TSN_SUPPORT:\n\t\tif (asoc->ep->prsctp_enable) {\n\t\t\tasoc->peer.prsctp_capable = 1;\n\t\t\tbreak;\n\t\t}\n\t\t/* Fall Through */\n\t\tgoto fall_through;\n\n\tcase SCTP_PARAM_RANDOM:\n\t\tif (!ep->auth_enable)\n\t\t\tgoto fall_through;\n\n\t\t/* Save peer's random parameter */\n\t\tkfree(asoc->peer.peer_random);\n\t\tasoc->peer.peer_random = kmemdup(param.p,\n\t\t\t\t\t    ntohs(param.p->length), gfp);\n\t\tif (!asoc->peer.peer_random) {\n\t\t\tretval = 0;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase SCTP_PARAM_HMAC_ALGO:\n\t\tif (!ep->auth_enable)\n\t\t\tgoto fall_through;\n\n\t\t/* Save peer's HMAC list */\n\t\tkfree(asoc->peer.peer_hmacs);\n\t\tasoc->peer.peer_hmacs = kmemdup(param.p,\n\t\t\t\t\t    ntohs(param.p->length), gfp);\n\t\tif (!asoc->peer.peer_hmacs) {\n\t\t\tretval = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Set the default HMAC the peer requested*/\n\t\tsctp_auth_asoc_set_default_hmac(asoc, param.hmac_algo);\n\t\tbreak;\n\n\tcase SCTP_PARAM_CHUNKS:\n\t\tif (!ep->auth_enable)\n\t\t\tgoto fall_through;\n\n\t\tkfree(asoc->peer.peer_chunks);\n\t\tasoc->peer.peer_chunks = kmemdup(param.p,\n\t\t\t\t\t    ntohs(param.p->length), gfp);\n\t\tif (!asoc->peer.peer_chunks)\n\t\t\tretval = 0;\n\t\tbreak;\nfall_through:\n\tdefault:\n\t\t/* Any unrecognized parameters should have been caught\n\t\t * and handled by sctp_verify_param() which should be\n\t\t * called prior to this routine.  Simply log the error\n\t\t * here.\n\t\t */\n\t\tpr_debug(\"%s: ignoring param:%d for association:%p.\\n\",\n\t\t\t __func__, ntohs(param.p->type), asoc);\n\t\tbreak;\n\t}\n\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,7 +30,8 @@\n \t\t\tbreak;\n do_addr_param:\n \t\taf = sctp_get_af_specific(param_type2af(param.p->type));\n-\t\taf->from_addr_param(&addr, param.addr, htons(asoc->peer.port), 0);\n+\t\tif (!af->from_addr_param(&addr, param.addr, htons(asoc->peer.port), 0))\n+\t\t\tbreak;\n \t\tscope = sctp_scope(peer_addr);\n \t\tif (sctp_in_scope(net, &addr, scope))\n \t\t\tif (!sctp_assoc_add_peer(asoc, &addr, gfp, SCTP_UNCONFIRMED))\n@@ -131,15 +132,13 @@\n \t\taddr_param = param.v + sizeof(struct sctp_addip_param);\n \n \t\taf = sctp_get_af_specific(param_type2af(addr_param->p.type));\n-\t\tif (af == NULL)\n-\t\t\tbreak;\n-\n-\t\taf->from_addr_param(&addr, addr_param,\n-\t\t\t\t    htons(asoc->peer.port), 0);\n-\n-\t\t/* if the address is invalid, we can't process it.\n-\t\t * XXX: see spec for what to do.\n-\t\t */\n+\t\tif (!af)\n+\t\t\tbreak;\n+\n+\t\tif (!af->from_addr_param(&addr, addr_param,\n+\t\t\t\t\t htons(asoc->peer.port), 0))\n+\t\t\tbreak;\n+\n \t\tif (!af->addr_valid(&addr, NULL, NULL))\n \t\t\tbreak;\n ",
        "function_modified_lines": {
            "added": [
                "\t\tif (!af->from_addr_param(&addr, param.addr, htons(asoc->peer.port), 0))",
                "\t\t\tbreak;",
                "\t\tif (!af)",
                "\t\t\tbreak;",
                "",
                "\t\tif (!af->from_addr_param(&addr, addr_param,",
                "\t\t\t\t\t htons(asoc->peer.port), 0))",
                "\t\t\tbreak;",
                ""
            ],
            "deleted": [
                "\t\taf->from_addr_param(&addr, param.addr, htons(asoc->peer.port), 0);",
                "\t\tif (af == NULL)",
                "\t\t\tbreak;",
                "",
                "\t\taf->from_addr_param(&addr, addr_param,",
                "\t\t\t\t    htons(asoc->peer.port), 0);",
                "",
                "\t\t/* if the address is invalid, we can't process it.",
                "\t\t * XXX: see spec for what to do.",
                "\t\t */"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel in versions prior to v5.14-rc1. Missing size validations on inbound SCTP packets may allow the kernel to read uninitialized memory.",
        "id": 3036
    },
    {
        "cve_id": "CVE-2018-11232",
        "code_before_change": "static void *etm_setup_aux(int event_cpu, void **pages,\n\t\t\t   int nr_pages, bool overwrite)\n{\n\tint cpu;\n\tcpumask_t *mask;\n\tstruct coresight_device *sink;\n\tstruct etm_event_data *event_data = NULL;\n\n\tevent_data = alloc_event_data(event_cpu);\n\tif (!event_data)\n\t\treturn NULL;\n\n\t/*\n\t * In theory nothing prevent tracers in a trace session from being\n\t * associated with different sinks, nor having a sink per tracer.  But\n\t * until we have HW with this kind of topology we need to assume tracers\n\t * in a trace session are using the same sink.  Therefore go through\n\t * the coresight bus and pick the first enabled sink.\n\t *\n\t * When operated from sysFS users are responsible to enable the sink\n\t * while from perf, the perf tools will do it based on the choice made\n\t * on the cmd line.  As such the \"enable_sink\" flag in sysFS is reset.\n\t */\n\tsink = coresight_get_enabled_sink(true);\n\tif (!sink)\n\t\tgoto err;\n\n\tINIT_WORK(&event_data->work, free_event_data);\n\n\tmask = &event_data->mask;\n\n\t/* Setup the path for each CPU in a trace session */\n\tfor_each_cpu(cpu, mask) {\n\t\tstruct coresight_device *csdev;\n\n\t\tcsdev = per_cpu(csdev_src, cpu);\n\t\tif (!csdev)\n\t\t\tgoto err;\n\n\t\t/*\n\t\t * Building a path doesn't enable it, it simply builds a\n\t\t * list of devices from source to sink that can be\n\t\t * referenced later when the path is actually needed.\n\t\t */\n\t\tevent_data->path[cpu] = coresight_build_path(csdev, sink);\n\t\tif (IS_ERR(event_data->path[cpu]))\n\t\t\tgoto err;\n\t}\n\n\tif (!sink_ops(sink)->alloc_buffer)\n\t\tgoto err;\n\n\t/* Get the AUX specific data from the sink buffer */\n\tevent_data->snk_config =\n\t\t\tsink_ops(sink)->alloc_buffer(sink, cpu, pages,\n\t\t\t\t\t\t     nr_pages, overwrite);\n\tif (!event_data->snk_config)\n\t\tgoto err;\n\nout:\n\treturn event_data;\n\nerr:\n\tetm_free_aux(event_data);\n\tevent_data = NULL;\n\tgoto out;\n}",
        "code_after_change": "static void *etm_setup_aux(int event_cpu, void **pages,\n\t\t\t   int nr_pages, bool overwrite)\n{\n\tint cpu;\n\tcpumask_t *mask;\n\tstruct coresight_device *sink;\n\tstruct etm_event_data *event_data = NULL;\n\n\tevent_data = alloc_event_data(event_cpu);\n\tif (!event_data)\n\t\treturn NULL;\n\n\t/*\n\t * In theory nothing prevent tracers in a trace session from being\n\t * associated with different sinks, nor having a sink per tracer.  But\n\t * until we have HW with this kind of topology we need to assume tracers\n\t * in a trace session are using the same sink.  Therefore go through\n\t * the coresight bus and pick the first enabled sink.\n\t *\n\t * When operated from sysFS users are responsible to enable the sink\n\t * while from perf, the perf tools will do it based on the choice made\n\t * on the cmd line.  As such the \"enable_sink\" flag in sysFS is reset.\n\t */\n\tsink = coresight_get_enabled_sink(true);\n\tif (!sink)\n\t\tgoto err;\n\n\tINIT_WORK(&event_data->work, free_event_data);\n\n\tmask = &event_data->mask;\n\n\t/* Setup the path for each CPU in a trace session */\n\tfor_each_cpu(cpu, mask) {\n\t\tstruct coresight_device *csdev;\n\n\t\tcsdev = per_cpu(csdev_src, cpu);\n\t\tif (!csdev)\n\t\t\tgoto err;\n\n\t\t/*\n\t\t * Building a path doesn't enable it, it simply builds a\n\t\t * list of devices from source to sink that can be\n\t\t * referenced later when the path is actually needed.\n\t\t */\n\t\tevent_data->path[cpu] = coresight_build_path(csdev, sink);\n\t\tif (IS_ERR(event_data->path[cpu]))\n\t\t\tgoto err;\n\t}\n\n\tif (!sink_ops(sink)->alloc_buffer)\n\t\tgoto err;\n\n\tcpu = cpumask_first(mask);\n\t/* Get the AUX specific data from the sink buffer */\n\tevent_data->snk_config =\n\t\t\tsink_ops(sink)->alloc_buffer(sink, cpu, pages,\n\t\t\t\t\t\t     nr_pages, overwrite);\n\tif (!event_data->snk_config)\n\t\tgoto err;\n\nout:\n\treturn event_data;\n\nerr:\n\tetm_free_aux(event_data);\n\tevent_data = NULL;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -50,6 +50,7 @@\n \tif (!sink_ops(sink)->alloc_buffer)\n \t\tgoto err;\n \n+\tcpu = cpumask_first(mask);\n \t/* Get the AUX specific data from the sink buffer */\n \tevent_data->snk_config =\n \t\t\tsink_ops(sink)->alloc_buffer(sink, cpu, pages,",
        "function_modified_lines": {
            "added": [
                "\tcpu = cpumask_first(mask);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The etm_setup_aux function in drivers/hwtracing/coresight/coresight-etm-perf.c in the Linux kernel before 4.10.2 allows attackers to cause a denial of service (panic) because a parameter is incorrectly used as a local variable.",
        "id": 1638
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int pppol2tp_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t len,\n\t\t\t    int flags)\n{\n\tint err;\n\tstruct sk_buff *skb;\n\tstruct sock *sk = sock->sk;\n\n\terr = -EIO;\n\tif (sk->sk_state & PPPOX_BOUND)\n\t\tgoto end;\n\n\tmsg->msg_namelen = 0;\n\n\terr = 0;\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\tflags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\tgoto end;\n\n\tif (len > skb->len)\n\t\tlen = skb->len;\n\telse if (len < skb->len)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, len);\n\tif (likely(err == 0))\n\t\terr = len;\n\n\tkfree_skb(skb);\nend:\n\treturn err;\n}",
        "code_after_change": "static int pppol2tp_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t len,\n\t\t\t    int flags)\n{\n\tint err;\n\tstruct sk_buff *skb;\n\tstruct sock *sk = sock->sk;\n\n\terr = -EIO;\n\tif (sk->sk_state & PPPOX_BOUND)\n\t\tgoto end;\n\n\terr = 0;\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\tflags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\tgoto end;\n\n\tif (len > skb->len)\n\t\tlen = skb->len;\n\telse if (len < skb->len)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, len);\n\tif (likely(err == 0))\n\t\terr = len;\n\n\tkfree_skb(skb);\nend:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,8 +9,6 @@\n \terr = -EIO;\n \tif (sk->sk_state & PPPOX_BOUND)\n \t\tgoto end;\n-\n-\tmsg->msg_namelen = 0;\n \n \terr = 0;\n \tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 387
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static int io_sq_thread(void *data)\n{\n\tstruct io_ring_ctx *ctx = data;\n\tconst struct cred *old_cred;\n\tDEFINE_WAIT(wait);\n\tunsigned long timeout;\n\tint ret = 0;\n\n\tcomplete(&ctx->sq_thread_comp);\n\n\told_cred = override_creds(ctx->creds);\n\n\ttimeout = jiffies + ctx->sq_thread_idle;\n\twhile (!kthread_should_park()) {\n\t\tunsigned int to_submit;\n\n\t\tif (!list_empty(&ctx->iopoll_list)) {\n\t\t\tunsigned nr_events = 0;\n\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t\tif (!list_empty(&ctx->iopoll_list) && !need_resched())\n\t\t\t\tio_do_iopoll(ctx, &nr_events, 0);\n\t\t\telse\n\t\t\t\ttimeout = jiffies + ctx->sq_thread_idle;\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t}\n\n\t\tto_submit = io_sqring_entries(ctx);\n\n\t\t/*\n\t\t * If submit got -EBUSY, flag us as needing the application\n\t\t * to enter the kernel to reap and flush events.\n\t\t */\n\t\tif (!to_submit || ret == -EBUSY || need_resched()) {\n\t\t\t/*\n\t\t\t * Drop cur_mm before scheduling, we can't hold it for\n\t\t\t * long periods (or over schedule()). Do this before\n\t\t\t * adding ourselves to the waitqueue, as the unuse/drop\n\t\t\t * may sleep.\n\t\t\t */\n\t\t\tio_sq_thread_drop_mm();\n\n\t\t\t/*\n\t\t\t * We're polling. If we're within the defined idle\n\t\t\t * period, then let us spin without work before going\n\t\t\t * to sleep. The exception is if we got EBUSY doing\n\t\t\t * more IO, we should wait for the application to\n\t\t\t * reap events and wake us up.\n\t\t\t */\n\t\t\tif (!list_empty(&ctx->iopoll_list) || need_resched() ||\n\t\t\t    (!time_after(jiffies, timeout) && ret != -EBUSY &&\n\t\t\t    !percpu_ref_is_dying(&ctx->refs))) {\n\t\t\t\tio_run_task_work();\n\t\t\t\tcond_resched();\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tprepare_to_wait(&ctx->sqo_wait, &wait,\n\t\t\t\t\t\tTASK_INTERRUPTIBLE);\n\n\t\t\t/*\n\t\t\t * While doing polled IO, before going to sleep, we need\n\t\t\t * to check if there are new reqs added to iopoll_list,\n\t\t\t * it is because reqs may have been punted to io worker\n\t\t\t * and will be added to iopoll_list later, hence check\n\t\t\t * the iopoll_list again.\n\t\t\t */\n\t\t\tif ((ctx->flags & IORING_SETUP_IOPOLL) &&\n\t\t\t    !list_empty_careful(&ctx->iopoll_list)) {\n\t\t\t\tfinish_wait(&ctx->sqo_wait, &wait);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tio_ring_set_wakeup_flag(ctx);\n\n\t\t\tto_submit = io_sqring_entries(ctx);\n\t\t\tif (!to_submit || ret == -EBUSY) {\n\t\t\t\tif (kthread_should_park()) {\n\t\t\t\t\tfinish_wait(&ctx->sqo_wait, &wait);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (io_run_task_work()) {\n\t\t\t\t\tfinish_wait(&ctx->sqo_wait, &wait);\n\t\t\t\t\tio_ring_clear_wakeup_flag(ctx);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (signal_pending(current))\n\t\t\t\t\tflush_signals(current);\n\t\t\t\tschedule();\n\t\t\t\tfinish_wait(&ctx->sqo_wait, &wait);\n\n\t\t\t\tio_ring_clear_wakeup_flag(ctx);\n\t\t\t\tret = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tfinish_wait(&ctx->sqo_wait, &wait);\n\n\t\t\tio_ring_clear_wakeup_flag(ctx);\n\t\t}\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tif (likely(!percpu_ref_is_dying(&ctx->refs)))\n\t\t\tret = io_submit_sqes(ctx, to_submit, NULL, -1);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\ttimeout = jiffies + ctx->sq_thread_idle;\n\t}\n\n\tio_run_task_work();\n\n\tio_sq_thread_drop_mm();\n\trevert_creds(old_cred);\n\n\tkthread_parkme();\n\n\treturn 0;\n}",
        "code_after_change": "static int io_sq_thread(void *data)\n{\n\tstruct io_ring_ctx *ctx = data;\n\tconst struct cred *old_cred;\n\tDEFINE_WAIT(wait);\n\tunsigned long timeout;\n\tint ret = 0;\n\n\tcomplete(&ctx->sq_thread_comp);\n\n\told_cred = override_creds(ctx->creds);\n\n\ttimeout = jiffies + ctx->sq_thread_idle;\n\twhile (!kthread_should_park()) {\n\t\tunsigned int to_submit;\n\n\t\tif (!list_empty(&ctx->iopoll_list)) {\n\t\t\tunsigned nr_events = 0;\n\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t\tif (!list_empty(&ctx->iopoll_list) && !need_resched())\n\t\t\t\tio_do_iopoll(ctx, &nr_events, 0);\n\t\t\telse\n\t\t\t\ttimeout = jiffies + ctx->sq_thread_idle;\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t}\n\n\t\tto_submit = io_sqring_entries(ctx);\n\n\t\t/*\n\t\t * If submit got -EBUSY, flag us as needing the application\n\t\t * to enter the kernel to reap and flush events.\n\t\t */\n\t\tif (!to_submit || ret == -EBUSY || need_resched()) {\n\t\t\t/*\n\t\t\t * Drop cur_mm before scheduling, we can't hold it for\n\t\t\t * long periods (or over schedule()). Do this before\n\t\t\t * adding ourselves to the waitqueue, as the unuse/drop\n\t\t\t * may sleep.\n\t\t\t */\n\t\t\tio_sq_thread_drop_mm();\n\n\t\t\t/*\n\t\t\t * We're polling. If we're within the defined idle\n\t\t\t * period, then let us spin without work before going\n\t\t\t * to sleep. The exception is if we got EBUSY doing\n\t\t\t * more IO, we should wait for the application to\n\t\t\t * reap events and wake us up.\n\t\t\t */\n\t\t\tif (!list_empty(&ctx->iopoll_list) || need_resched() ||\n\t\t\t    (!time_after(jiffies, timeout) && ret != -EBUSY &&\n\t\t\t    !percpu_ref_is_dying(&ctx->refs))) {\n\t\t\t\tio_run_task_work();\n\t\t\t\tcond_resched();\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tprepare_to_wait(&ctx->sqo_wait, &wait,\n\t\t\t\t\t\tTASK_INTERRUPTIBLE);\n\n\t\t\t/*\n\t\t\t * While doing polled IO, before going to sleep, we need\n\t\t\t * to check if there are new reqs added to iopoll_list,\n\t\t\t * it is because reqs may have been punted to io worker\n\t\t\t * and will be added to iopoll_list later, hence check\n\t\t\t * the iopoll_list again.\n\t\t\t */\n\t\t\tif ((ctx->flags & IORING_SETUP_IOPOLL) &&\n\t\t\t    !list_empty_careful(&ctx->iopoll_list)) {\n\t\t\t\tfinish_wait(&ctx->sqo_wait, &wait);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tio_ring_set_wakeup_flag(ctx);\n\n\t\t\tto_submit = io_sqring_entries(ctx);\n\t\t\tif (!to_submit || ret == -EBUSY) {\n\t\t\t\tif (kthread_should_park()) {\n\t\t\t\t\tfinish_wait(&ctx->sqo_wait, &wait);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (io_run_task_work()) {\n\t\t\t\t\tfinish_wait(&ctx->sqo_wait, &wait);\n\t\t\t\t\tio_ring_clear_wakeup_flag(ctx);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (signal_pending(current))\n\t\t\t\t\tflush_signals(current);\n\t\t\t\tschedule();\n\t\t\t\tfinish_wait(&ctx->sqo_wait, &wait);\n\n\t\t\t\tio_ring_clear_wakeup_flag(ctx);\n\t\t\t\tret = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tfinish_wait(&ctx->sqo_wait, &wait);\n\n\t\t\tio_ring_clear_wakeup_flag(ctx);\n\t\t}\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tif (likely(!percpu_ref_is_dying(&ctx->refs)))\n\t\t\tret = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\ttimeout = jiffies + ctx->sq_thread_idle;\n\t}\n\n\tio_run_task_work();\n\n\tio_sq_thread_drop_mm();\n\trevert_creds(old_cred);\n\n\tkthread_parkme();\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -100,7 +100,7 @@\n \n \t\tmutex_lock(&ctx->uring_lock);\n \t\tif (likely(!percpu_ref_is_dying(&ctx->refs)))\n-\t\t\tret = io_submit_sqes(ctx, to_submit, NULL, -1);\n+\t\t\tret = io_submit_sqes(ctx, to_submit);\n \t\tmutex_unlock(&ctx->uring_lock);\n \t\ttimeout = jiffies + ctx->sq_thread_idle;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tret = io_submit_sqes(ctx, to_submit);"
            ],
            "deleted": [
                "\t\t\tret = io_submit_sqes(ctx, to_submit, NULL, -1);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2857
    },
    {
        "cve_id": "CVE-2014-2523",
        "code_before_change": "static int dccp_packet(struct nf_conn *ct, const struct sk_buff *skb,\n\t\t       unsigned int dataoff, enum ip_conntrack_info ctinfo,\n\t\t       u_int8_t pf, unsigned int hooknum,\n\t\t       unsigned int *timeouts)\n{\n\tstruct net *net = nf_ct_net(ct);\n\tenum ip_conntrack_dir dir = CTINFO2DIR(ctinfo);\n\tstruct dccp_hdr _dh, *dh;\n\tu_int8_t type, old_state, new_state;\n\tenum ct_dccp_roles role;\n\n\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);\n\tBUG_ON(dh == NULL);\n\ttype = dh->dccph_type;\n\n\tif (type == DCCP_PKT_RESET &&\n\t    !test_bit(IPS_SEEN_REPLY_BIT, &ct->status)) {\n\t\t/* Tear down connection immediately if only reply is a RESET */\n\t\tnf_ct_kill_acct(ct, ctinfo, skb);\n\t\treturn NF_ACCEPT;\n\t}\n\n\tspin_lock_bh(&ct->lock);\n\n\trole = ct->proto.dccp.role[dir];\n\told_state = ct->proto.dccp.state;\n\tnew_state = dccp_state_table[role][type][old_state];\n\n\tswitch (new_state) {\n\tcase CT_DCCP_REQUEST:\n\t\tif (old_state == CT_DCCP_TIMEWAIT &&\n\t\t    role == CT_DCCP_ROLE_SERVER) {\n\t\t\t/* Reincarnation in the reverse direction: reopen and\n\t\t\t * reverse client/server roles. */\n\t\t\tct->proto.dccp.role[dir] = CT_DCCP_ROLE_CLIENT;\n\t\t\tct->proto.dccp.role[!dir] = CT_DCCP_ROLE_SERVER;\n\t\t}\n\t\tbreak;\n\tcase CT_DCCP_RESPOND:\n\t\tif (old_state == CT_DCCP_REQUEST)\n\t\t\tct->proto.dccp.handshake_seq = dccp_hdr_seq(dh);\n\t\tbreak;\n\tcase CT_DCCP_PARTOPEN:\n\t\tif (old_state == CT_DCCP_RESPOND &&\n\t\t    type == DCCP_PKT_ACK &&\n\t\t    dccp_ack_seq(dh) == ct->proto.dccp.handshake_seq)\n\t\t\tset_bit(IPS_ASSURED_BIT, &ct->status);\n\t\tbreak;\n\tcase CT_DCCP_IGNORE:\n\t\t/*\n\t\t * Connection tracking might be out of sync, so we ignore\n\t\t * packets that might establish a new connection and resync\n\t\t * if the server responds with a valid Response.\n\t\t */\n\t\tif (ct->proto.dccp.last_dir == !dir &&\n\t\t    ct->proto.dccp.last_pkt == DCCP_PKT_REQUEST &&\n\t\t    type == DCCP_PKT_RESPONSE) {\n\t\t\tct->proto.dccp.role[!dir] = CT_DCCP_ROLE_CLIENT;\n\t\t\tct->proto.dccp.role[dir] = CT_DCCP_ROLE_SERVER;\n\t\t\tct->proto.dccp.handshake_seq = dccp_hdr_seq(dh);\n\t\t\tnew_state = CT_DCCP_RESPOND;\n\t\t\tbreak;\n\t\t}\n\t\tct->proto.dccp.last_dir = dir;\n\t\tct->proto.dccp.last_pkt = type;\n\n\t\tspin_unlock_bh(&ct->lock);\n\t\tif (LOG_INVALID(net, IPPROTO_DCCP))\n\t\t\tnf_log_packet(net, pf, 0, skb, NULL, NULL, NULL,\n\t\t\t\t      \"nf_ct_dccp: invalid packet ignored \");\n\t\treturn NF_ACCEPT;\n\tcase CT_DCCP_INVALID:\n\t\tspin_unlock_bh(&ct->lock);\n\t\tif (LOG_INVALID(net, IPPROTO_DCCP))\n\t\t\tnf_log_packet(net, pf, 0, skb, NULL, NULL, NULL,\n\t\t\t\t      \"nf_ct_dccp: invalid state transition \");\n\t\treturn -NF_ACCEPT;\n\t}\n\n\tct->proto.dccp.last_dir = dir;\n\tct->proto.dccp.last_pkt = type;\n\tct->proto.dccp.state = new_state;\n\tspin_unlock_bh(&ct->lock);\n\n\tif (new_state != old_state)\n\t\tnf_conntrack_event_cache(IPCT_PROTOINFO, ct);\n\n\tnf_ct_refresh_acct(ct, ctinfo, skb, timeouts[new_state]);\n\n\treturn NF_ACCEPT;\n}",
        "code_after_change": "static int dccp_packet(struct nf_conn *ct, const struct sk_buff *skb,\n\t\t       unsigned int dataoff, enum ip_conntrack_info ctinfo,\n\t\t       u_int8_t pf, unsigned int hooknum,\n\t\t       unsigned int *timeouts)\n{\n\tstruct net *net = nf_ct_net(ct);\n\tenum ip_conntrack_dir dir = CTINFO2DIR(ctinfo);\n\tstruct dccp_hdr _dh, *dh;\n\tu_int8_t type, old_state, new_state;\n\tenum ct_dccp_roles role;\n\n\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);\n\tBUG_ON(dh == NULL);\n\ttype = dh->dccph_type;\n\n\tif (type == DCCP_PKT_RESET &&\n\t    !test_bit(IPS_SEEN_REPLY_BIT, &ct->status)) {\n\t\t/* Tear down connection immediately if only reply is a RESET */\n\t\tnf_ct_kill_acct(ct, ctinfo, skb);\n\t\treturn NF_ACCEPT;\n\t}\n\n\tspin_lock_bh(&ct->lock);\n\n\trole = ct->proto.dccp.role[dir];\n\told_state = ct->proto.dccp.state;\n\tnew_state = dccp_state_table[role][type][old_state];\n\n\tswitch (new_state) {\n\tcase CT_DCCP_REQUEST:\n\t\tif (old_state == CT_DCCP_TIMEWAIT &&\n\t\t    role == CT_DCCP_ROLE_SERVER) {\n\t\t\t/* Reincarnation in the reverse direction: reopen and\n\t\t\t * reverse client/server roles. */\n\t\t\tct->proto.dccp.role[dir] = CT_DCCP_ROLE_CLIENT;\n\t\t\tct->proto.dccp.role[!dir] = CT_DCCP_ROLE_SERVER;\n\t\t}\n\t\tbreak;\n\tcase CT_DCCP_RESPOND:\n\t\tif (old_state == CT_DCCP_REQUEST)\n\t\t\tct->proto.dccp.handshake_seq = dccp_hdr_seq(dh);\n\t\tbreak;\n\tcase CT_DCCP_PARTOPEN:\n\t\tif (old_state == CT_DCCP_RESPOND &&\n\t\t    type == DCCP_PKT_ACK &&\n\t\t    dccp_ack_seq(dh) == ct->proto.dccp.handshake_seq)\n\t\t\tset_bit(IPS_ASSURED_BIT, &ct->status);\n\t\tbreak;\n\tcase CT_DCCP_IGNORE:\n\t\t/*\n\t\t * Connection tracking might be out of sync, so we ignore\n\t\t * packets that might establish a new connection and resync\n\t\t * if the server responds with a valid Response.\n\t\t */\n\t\tif (ct->proto.dccp.last_dir == !dir &&\n\t\t    ct->proto.dccp.last_pkt == DCCP_PKT_REQUEST &&\n\t\t    type == DCCP_PKT_RESPONSE) {\n\t\t\tct->proto.dccp.role[!dir] = CT_DCCP_ROLE_CLIENT;\n\t\t\tct->proto.dccp.role[dir] = CT_DCCP_ROLE_SERVER;\n\t\t\tct->proto.dccp.handshake_seq = dccp_hdr_seq(dh);\n\t\t\tnew_state = CT_DCCP_RESPOND;\n\t\t\tbreak;\n\t\t}\n\t\tct->proto.dccp.last_dir = dir;\n\t\tct->proto.dccp.last_pkt = type;\n\n\t\tspin_unlock_bh(&ct->lock);\n\t\tif (LOG_INVALID(net, IPPROTO_DCCP))\n\t\t\tnf_log_packet(net, pf, 0, skb, NULL, NULL, NULL,\n\t\t\t\t      \"nf_ct_dccp: invalid packet ignored \");\n\t\treturn NF_ACCEPT;\n\tcase CT_DCCP_INVALID:\n\t\tspin_unlock_bh(&ct->lock);\n\t\tif (LOG_INVALID(net, IPPROTO_DCCP))\n\t\t\tnf_log_packet(net, pf, 0, skb, NULL, NULL, NULL,\n\t\t\t\t      \"nf_ct_dccp: invalid state transition \");\n\t\treturn -NF_ACCEPT;\n\t}\n\n\tct->proto.dccp.last_dir = dir;\n\tct->proto.dccp.last_pkt = type;\n\tct->proto.dccp.state = new_state;\n\tspin_unlock_bh(&ct->lock);\n\n\tif (new_state != old_state)\n\t\tnf_conntrack_event_cache(IPCT_PROTOINFO, ct);\n\n\tnf_ct_refresh_acct(ct, ctinfo, skb, timeouts[new_state]);\n\n\treturn NF_ACCEPT;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \tu_int8_t type, old_state, new_state;\n \tenum ct_dccp_roles role;\n \n-\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);\n+\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);\n \tBUG_ON(dh == NULL);\n \ttype = dh->dccph_type;\n ",
        "function_modified_lines": {
            "added": [
                "\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);"
            ],
            "deleted": [
                "\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "net/netfilter/nf_conntrack_proto_dccp.c in the Linux kernel through 3.13.6 uses a DCCP header pointer incorrectly, which allows remote attackers to cause a denial of service (system crash) or possibly execute arbitrary code via a DCCP packet that triggers a call to the (1) dccp_new, (2) dccp_packet, or (3) dccp_error function.",
        "id": 483
    },
    {
        "cve_id": "CVE-2014-4611",
        "code_before_change": "static int lz4_uncompress(const char *source, char *dest, int osize)\n{\n\tconst BYTE *ip = (const BYTE *) source;\n\tconst BYTE *ref;\n\tBYTE *op = (BYTE *) dest;\n\tBYTE * const oend = op + osize;\n\tBYTE *cpy;\n\tunsigned token;\n\tsize_t length;\n\tsize_t dec32table[] = {0, 3, 2, 3, 0, 0, 0, 0};\n#if LZ4_ARCH64\n\tsize_t dec64table[] = {0, 0, 0, -1, 0, 1, 2, 3};\n#endif\n\n\twhile (1) {\n\n\t\t/* get runlength */\n\t\ttoken = *ip++;\n\t\tlength = (token >> ML_BITS);\n\t\tif (length == RUN_MASK) {\n\t\t\tsize_t len;\n\n\t\t\tlen = *ip++;\n\t\t\tfor (; len == 255; length += 255)\n\t\t\t\tlen = *ip++;\n\t\t\tlength += len;\n\t\t}\n\n\t\t/* copy literals */\n\t\tcpy = op + length;\n\t\tif (unlikely(cpy > oend - COPYLENGTH)) {\n\t\t\t/*\n\t\t\t * Error: not enough place for another match\n\t\t\t * (min 4) + 5 literals\n\t\t\t */\n\t\t\tif (cpy != oend)\n\t\t\t\tgoto _output_error;\n\n\t\t\tmemcpy(op, ip, length);\n\t\t\tip += length;\n\t\t\tbreak; /* EOF */\n\t\t}\n\t\tLZ4_WILDCOPY(ip, op, cpy);\n\t\tip -= (op - cpy);\n\t\top = cpy;\n\n\t\t/* get offset */\n\t\tLZ4_READ_LITTLEENDIAN_16(ref, cpy, ip);\n\t\tip += 2;\n\n\t\t/* Error: offset create reference outside destination buffer */\n\t\tif (unlikely(ref < (BYTE *const) dest))\n\t\t\tgoto _output_error;\n\n\t\t/* get matchlength */\n\t\tlength = token & ML_MASK;\n\t\tif (length == ML_MASK) {\n\t\t\tfor (; *ip == 255; length += 255)\n\t\t\t\tip++;\n\t\t\tlength += *ip++;\n\t\t}\n\n\t\t/* copy repeated sequence */\n\t\tif (unlikely((op - ref) < STEPSIZE)) {\n#if LZ4_ARCH64\n\t\t\tsize_t dec64 = dec64table[op - ref];\n#else\n\t\t\tconst int dec64 = 0;\n#endif\n\t\t\top[0] = ref[0];\n\t\t\top[1] = ref[1];\n\t\t\top[2] = ref[2];\n\t\t\top[3] = ref[3];\n\t\t\top += 4;\n\t\t\tref += 4;\n\t\t\tref -= dec32table[op-ref];\n\t\t\tPUT4(ref, op);\n\t\t\top += STEPSIZE - 4;\n\t\t\tref -= dec64;\n\t\t} else {\n\t\t\tLZ4_COPYSTEP(ref, op);\n\t\t}\n\t\tcpy = op + length - (STEPSIZE - 4);\n\t\tif (cpy > (oend - COPYLENGTH)) {\n\n\t\t\t/* Error: request to write beyond destination buffer */\n\t\t\tif (cpy > oend)\n\t\t\t\tgoto _output_error;\n\t\t\tLZ4_SECURECOPY(ref, op, (oend - COPYLENGTH));\n\t\t\twhile (op < cpy)\n\t\t\t\t*op++ = *ref++;\n\t\t\top = cpy;\n\t\t\t/*\n\t\t\t * Check EOF (should never happen, since last 5 bytes\n\t\t\t * are supposed to be literals)\n\t\t\t */\n\t\t\tif (op == oend)\n\t\t\t\tgoto _output_error;\n\t\t\tcontinue;\n\t\t}\n\t\tLZ4_SECURECOPY(ref, op, cpy);\n\t\top = cpy; /* correction */\n\t}\n\t/* end of decoding */\n\treturn (int) (((char *)ip) - source);\n\n\t/* write overflow error detected */\n_output_error:\n\treturn (int) (-(((char *)ip) - source));\n}",
        "code_after_change": "static int lz4_uncompress(const char *source, char *dest, int osize)\n{\n\tconst BYTE *ip = (const BYTE *) source;\n\tconst BYTE *ref;\n\tBYTE *op = (BYTE *) dest;\n\tBYTE * const oend = op + osize;\n\tBYTE *cpy;\n\tunsigned token;\n\tsize_t length;\n\tsize_t dec32table[] = {0, 3, 2, 3, 0, 0, 0, 0};\n#if LZ4_ARCH64\n\tsize_t dec64table[] = {0, 0, 0, -1, 0, 1, 2, 3};\n#endif\n\n\twhile (1) {\n\n\t\t/* get runlength */\n\t\ttoken = *ip++;\n\t\tlength = (token >> ML_BITS);\n\t\tif (length == RUN_MASK) {\n\t\t\tsize_t len;\n\n\t\t\tlen = *ip++;\n\t\t\tfor (; len == 255; length += 255)\n\t\t\t\tlen = *ip++;\n\t\t\tif (unlikely(length > (size_t)(length + len)))\n\t\t\t\tgoto _output_error;\n\t\t\tlength += len;\n\t\t}\n\n\t\t/* copy literals */\n\t\tcpy = op + length;\n\t\tif (unlikely(cpy > oend - COPYLENGTH)) {\n\t\t\t/*\n\t\t\t * Error: not enough place for another match\n\t\t\t * (min 4) + 5 literals\n\t\t\t */\n\t\t\tif (cpy != oend)\n\t\t\t\tgoto _output_error;\n\n\t\t\tmemcpy(op, ip, length);\n\t\t\tip += length;\n\t\t\tbreak; /* EOF */\n\t\t}\n\t\tLZ4_WILDCOPY(ip, op, cpy);\n\t\tip -= (op - cpy);\n\t\top = cpy;\n\n\t\t/* get offset */\n\t\tLZ4_READ_LITTLEENDIAN_16(ref, cpy, ip);\n\t\tip += 2;\n\n\t\t/* Error: offset create reference outside destination buffer */\n\t\tif (unlikely(ref < (BYTE *const) dest))\n\t\t\tgoto _output_error;\n\n\t\t/* get matchlength */\n\t\tlength = token & ML_MASK;\n\t\tif (length == ML_MASK) {\n\t\t\tfor (; *ip == 255; length += 255)\n\t\t\t\tip++;\n\t\t\tlength += *ip++;\n\t\t}\n\n\t\t/* copy repeated sequence */\n\t\tif (unlikely((op - ref) < STEPSIZE)) {\n#if LZ4_ARCH64\n\t\t\tsize_t dec64 = dec64table[op - ref];\n#else\n\t\t\tconst int dec64 = 0;\n#endif\n\t\t\top[0] = ref[0];\n\t\t\top[1] = ref[1];\n\t\t\top[2] = ref[2];\n\t\t\top[3] = ref[3];\n\t\t\top += 4;\n\t\t\tref += 4;\n\t\t\tref -= dec32table[op-ref];\n\t\t\tPUT4(ref, op);\n\t\t\top += STEPSIZE - 4;\n\t\t\tref -= dec64;\n\t\t} else {\n\t\t\tLZ4_COPYSTEP(ref, op);\n\t\t}\n\t\tcpy = op + length - (STEPSIZE - 4);\n\t\tif (cpy > (oend - COPYLENGTH)) {\n\n\t\t\t/* Error: request to write beyond destination buffer */\n\t\t\tif (cpy > oend)\n\t\t\t\tgoto _output_error;\n\t\t\tLZ4_SECURECOPY(ref, op, (oend - COPYLENGTH));\n\t\t\twhile (op < cpy)\n\t\t\t\t*op++ = *ref++;\n\t\t\top = cpy;\n\t\t\t/*\n\t\t\t * Check EOF (should never happen, since last 5 bytes\n\t\t\t * are supposed to be literals)\n\t\t\t */\n\t\t\tif (op == oend)\n\t\t\t\tgoto _output_error;\n\t\t\tcontinue;\n\t\t}\n\t\tLZ4_SECURECOPY(ref, op, cpy);\n\t\top = cpy; /* correction */\n\t}\n\t/* end of decoding */\n\treturn (int) (((char *)ip) - source);\n\n\t/* write overflow error detected */\n_output_error:\n\treturn (int) (-(((char *)ip) - source));\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,6 +23,8 @@\n \t\t\tlen = *ip++;\n \t\t\tfor (; len == 255; length += 255)\n \t\t\t\tlen = *ip++;\n+\t\t\tif (unlikely(length > (size_t)(length + len)))\n+\t\t\t\tgoto _output_error;\n \t\t\tlength += len;\n \t\t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (unlikely(length > (size_t)(length + len)))",
                "\t\t\t\tgoto _output_error;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Integer overflow in the LZ4 algorithm implementation, as used in Yann Collet LZ4 before r118 and in the lz4_uncompress function in lib/lz4/lz4_decompress.c in the Linux kernel before 3.15.2, on 32-bit platforms might allow context-dependent attackers to cause a denial of service (memory corruption) or possibly have unspecified other impact via a crafted Literal Run that would be improperly handled by programs not complying with an API limitation, a different vulnerability than CVE-2014-4715.",
        "id": 562
    },
    {
        "cve_id": "CVE-2019-9453",
        "code_before_change": "static int __f2fs_setxattr(struct inode *inode, int index,\n\t\t\tconst char *name, const void *value, size_t size,\n\t\t\tstruct page *ipage, int flags)\n{\n\tstruct f2fs_xattr_entry *here, *last;\n\tvoid *base_addr;\n\tint found, newsize;\n\tsize_t len;\n\t__u32 new_hsize;\n\tint error = 0;\n\n\tif (name == NULL)\n\t\treturn -EINVAL;\n\n\tif (value == NULL)\n\t\tsize = 0;\n\n\tlen = strlen(name);\n\n\tif (len > F2FS_NAME_LEN)\n\t\treturn -ERANGE;\n\n\tif (size > MAX_VALUE_LEN(inode))\n\t\treturn -E2BIG;\n\n\terror = read_all_xattrs(inode, ipage, &base_addr);\n\tif (error)\n\t\treturn error;\n\n\t/* find entry with wanted name. */\n\there = __find_xattr(base_addr, index, len, name);\n\n\tfound = IS_XATTR_LAST_ENTRY(here) ? 0 : 1;\n\n\tif (found) {\n\t\tif ((flags & XATTR_CREATE)) {\n\t\t\terror = -EEXIST;\n\t\t\tgoto exit;\n\t\t}\n\n\t\tif (value && f2fs_xattr_value_same(here, value, size))\n\t\t\tgoto exit;\n\t} else if ((flags & XATTR_REPLACE)) {\n\t\terror = -ENODATA;\n\t\tgoto exit;\n\t}\n\n\tlast = here;\n\twhile (!IS_XATTR_LAST_ENTRY(last))\n\t\tlast = XATTR_NEXT_ENTRY(last);\n\n\tnewsize = XATTR_ALIGN(sizeof(struct f2fs_xattr_entry) + len + size);\n\n\t/* 1. Check space */\n\tif (value) {\n\t\tint free;\n\t\t/*\n\t\t * If value is NULL, it is remove operation.\n\t\t * In case of update operation, we calculate free.\n\t\t */\n\t\tfree = MIN_OFFSET(inode) - ((char *)last - (char *)base_addr);\n\t\tif (found)\n\t\t\tfree = free + ENTRY_SIZE(here);\n\n\t\tif (unlikely(free < newsize)) {\n\t\t\terror = -E2BIG;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\n\t/* 2. Remove old entry */\n\tif (found) {\n\t\t/*\n\t\t * If entry is found, remove old entry.\n\t\t * If not found, remove operation is not needed.\n\t\t */\n\t\tstruct f2fs_xattr_entry *next = XATTR_NEXT_ENTRY(here);\n\t\tint oldsize = ENTRY_SIZE(here);\n\n\t\tmemmove(here, next, (char *)last - (char *)next);\n\t\tlast = (struct f2fs_xattr_entry *)((char *)last - oldsize);\n\t\tmemset(last, 0, oldsize);\n\t}\n\n\tnew_hsize = (char *)last - (char *)base_addr;\n\n\t/* 3. Write new entry */\n\tif (value) {\n\t\tchar *pval;\n\t\t/*\n\t\t * Before we come here, old entry is removed.\n\t\t * We just write new entry.\n\t\t */\n\t\tlast->e_name_index = index;\n\t\tlast->e_name_len = len;\n\t\tmemcpy(last->e_name, name, len);\n\t\tpval = last->e_name + len;\n\t\tmemcpy(pval, value, size);\n\t\tlast->e_value_size = cpu_to_le16(size);\n\t\tnew_hsize += newsize;\n\t}\n\n\terror = write_all_xattrs(inode, new_hsize, base_addr, ipage);\n\tif (error)\n\t\tgoto exit;\n\n\tif (is_inode_flag_set(inode, FI_ACL_MODE)) {\n\t\tinode->i_mode = F2FS_I(inode)->i_acl_mode;\n\t\tinode->i_ctime = current_time(inode);\n\t\tclear_inode_flag(inode, FI_ACL_MODE);\n\t}\n\tif (index == F2FS_XATTR_INDEX_ENCRYPTION &&\n\t\t\t!strcmp(name, F2FS_XATTR_NAME_ENCRYPTION_CONTEXT))\n\t\tf2fs_set_encrypted_inode(inode);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (!error && S_ISDIR(inode->i_mode))\n\t\tset_sbi_flag(F2FS_I_SB(inode), SBI_NEED_CP);\nexit:\n\tkvfree(base_addr);\n\treturn error;\n}",
        "code_after_change": "static int __f2fs_setxattr(struct inode *inode, int index,\n\t\t\tconst char *name, const void *value, size_t size,\n\t\t\tstruct page *ipage, int flags)\n{\n\tstruct f2fs_xattr_entry *here, *last;\n\tvoid *base_addr, *last_base_addr;\n\tnid_t xnid = F2FS_I(inode)->i_xattr_nid;\n\tint found, newsize;\n\tsize_t len;\n\t__u32 new_hsize;\n\tint error = 0;\n\n\tif (name == NULL)\n\t\treturn -EINVAL;\n\n\tif (value == NULL)\n\t\tsize = 0;\n\n\tlen = strlen(name);\n\n\tif (len > F2FS_NAME_LEN)\n\t\treturn -ERANGE;\n\n\tif (size > MAX_VALUE_LEN(inode))\n\t\treturn -E2BIG;\n\n\terror = read_all_xattrs(inode, ipage, &base_addr);\n\tif (error)\n\t\treturn error;\n\n\tlast_base_addr = (void *)base_addr + XATTR_SIZE(xnid, inode);\n\n\t/* find entry with wanted name. */\n\there = __find_xattr(base_addr, last_base_addr, index, len, name);\n\tif (!here) {\n\t\terror = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\tfound = IS_XATTR_LAST_ENTRY(here) ? 0 : 1;\n\n\tif (found) {\n\t\tif ((flags & XATTR_CREATE)) {\n\t\t\terror = -EEXIST;\n\t\t\tgoto exit;\n\t\t}\n\n\t\tif (value && f2fs_xattr_value_same(here, value, size))\n\t\t\tgoto exit;\n\t} else if ((flags & XATTR_REPLACE)) {\n\t\terror = -ENODATA;\n\t\tgoto exit;\n\t}\n\n\tlast = here;\n\twhile (!IS_XATTR_LAST_ENTRY(last))\n\t\tlast = XATTR_NEXT_ENTRY(last);\n\n\tnewsize = XATTR_ALIGN(sizeof(struct f2fs_xattr_entry) + len + size);\n\n\t/* 1. Check space */\n\tif (value) {\n\t\tint free;\n\t\t/*\n\t\t * If value is NULL, it is remove operation.\n\t\t * In case of update operation, we calculate free.\n\t\t */\n\t\tfree = MIN_OFFSET(inode) - ((char *)last - (char *)base_addr);\n\t\tif (found)\n\t\t\tfree = free + ENTRY_SIZE(here);\n\n\t\tif (unlikely(free < newsize)) {\n\t\t\terror = -E2BIG;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\n\t/* 2. Remove old entry */\n\tif (found) {\n\t\t/*\n\t\t * If entry is found, remove old entry.\n\t\t * If not found, remove operation is not needed.\n\t\t */\n\t\tstruct f2fs_xattr_entry *next = XATTR_NEXT_ENTRY(here);\n\t\tint oldsize = ENTRY_SIZE(here);\n\n\t\tmemmove(here, next, (char *)last - (char *)next);\n\t\tlast = (struct f2fs_xattr_entry *)((char *)last - oldsize);\n\t\tmemset(last, 0, oldsize);\n\t}\n\n\tnew_hsize = (char *)last - (char *)base_addr;\n\n\t/* 3. Write new entry */\n\tif (value) {\n\t\tchar *pval;\n\t\t/*\n\t\t * Before we come here, old entry is removed.\n\t\t * We just write new entry.\n\t\t */\n\t\tlast->e_name_index = index;\n\t\tlast->e_name_len = len;\n\t\tmemcpy(last->e_name, name, len);\n\t\tpval = last->e_name + len;\n\t\tmemcpy(pval, value, size);\n\t\tlast->e_value_size = cpu_to_le16(size);\n\t\tnew_hsize += newsize;\n\t}\n\n\terror = write_all_xattrs(inode, new_hsize, base_addr, ipage);\n\tif (error)\n\t\tgoto exit;\n\n\tif (is_inode_flag_set(inode, FI_ACL_MODE)) {\n\t\tinode->i_mode = F2FS_I(inode)->i_acl_mode;\n\t\tinode->i_ctime = current_time(inode);\n\t\tclear_inode_flag(inode, FI_ACL_MODE);\n\t}\n\tif (index == F2FS_XATTR_INDEX_ENCRYPTION &&\n\t\t\t!strcmp(name, F2FS_XATTR_NAME_ENCRYPTION_CONTEXT))\n\t\tf2fs_set_encrypted_inode(inode);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (!error && S_ISDIR(inode->i_mode))\n\t\tset_sbi_flag(F2FS_I_SB(inode), SBI_NEED_CP);\nexit:\n\tkvfree(base_addr);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,8 @@\n \t\t\tstruct page *ipage, int flags)\n {\n \tstruct f2fs_xattr_entry *here, *last;\n-\tvoid *base_addr;\n+\tvoid *base_addr, *last_base_addr;\n+\tnid_t xnid = F2FS_I(inode)->i_xattr_nid;\n \tint found, newsize;\n \tsize_t len;\n \t__u32 new_hsize;\n@@ -27,8 +28,14 @@\n \tif (error)\n \t\treturn error;\n \n+\tlast_base_addr = (void *)base_addr + XATTR_SIZE(xnid, inode);\n+\n \t/* find entry with wanted name. */\n-\there = __find_xattr(base_addr, index, len, name);\n+\there = __find_xattr(base_addr, last_base_addr, index, len, name);\n+\tif (!here) {\n+\t\terror = -EFAULT;\n+\t\tgoto exit;\n+\t}\n \n \tfound = IS_XATTR_LAST_ENTRY(here) ? 0 : 1;\n ",
        "function_modified_lines": {
            "added": [
                "\tvoid *base_addr, *last_base_addr;",
                "\tnid_t xnid = F2FS_I(inode)->i_xattr_nid;",
                "\tlast_base_addr = (void *)base_addr + XATTR_SIZE(xnid, inode);",
                "",
                "\there = __find_xattr(base_addr, last_base_addr, index, len, name);",
                "\tif (!here) {",
                "\t\terror = -EFAULT;",
                "\t\tgoto exit;",
                "\t}"
            ],
            "deleted": [
                "\tvoid *base_addr;",
                "\there = __find_xattr(base_addr, index, len, name);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "In the Android kernel in F2FS touch driver there is a possible out of bounds read due to improper input validation. This could lead to local information disclosure with system execution privileges needed. User interaction is not needed for exploitation.",
        "id": 2357
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static bool io_uring_cancel_files(struct io_ring_ctx *ctx,\n\t\t\t\t  struct files_struct *files)\n{\n\tif (list_empty_careful(&ctx->inflight_list))\n\t\treturn false;\n\n\tio_cancel_defer_files(ctx, files);\n\t/* cancel all at once, should be faster than doing it one by one*/\n\tio_wq_cancel_cb(ctx->io_wq, io_wq_files_match, files, true);\n\n\twhile (!list_empty_careful(&ctx->inflight_list)) {\n\t\tstruct io_kiocb *cancel_req = NULL, *req;\n\t\tDEFINE_WAIT(wait);\n\n\t\tspin_lock_irq(&ctx->inflight_lock);\n\t\tlist_for_each_entry(req, &ctx->inflight_list, inflight_entry) {\n\t\t\tif (req->work.files != files)\n\t\t\t\tcontinue;\n\t\t\t/* req is being completed, ignore */\n\t\t\tif (!refcount_inc_not_zero(&req->refs))\n\t\t\t\tcontinue;\n\t\t\tcancel_req = req;\n\t\t\tbreak;\n\t\t}\n\t\tif (cancel_req)\n\t\t\tprepare_to_wait(&ctx->inflight_wait, &wait,\n\t\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tspin_unlock_irq(&ctx->inflight_lock);\n\n\t\t/* We need to keep going until we don't find a matching req */\n\t\tif (!cancel_req)\n\t\t\tbreak;\n\t\t/* cancel this request, or head link requests */\n\t\tio_attempt_cancel(ctx, cancel_req);\n\t\tio_put_req(cancel_req);\n\t\t/* cancellations _may_ trigger task work */\n\t\tio_run_task_work();\n\t\tschedule();\n\t\tfinish_wait(&ctx->inflight_wait, &wait);\n\t}\n\n\treturn true;\n}",
        "code_after_change": "static bool io_uring_cancel_files(struct io_ring_ctx *ctx,\n\t\t\t\t  struct files_struct *files)\n{\n\tif (list_empty_careful(&ctx->inflight_list))\n\t\treturn false;\n\n\tio_cancel_defer_files(ctx, files);\n\t/* cancel all at once, should be faster than doing it one by one*/\n\tio_wq_cancel_cb(ctx->io_wq, io_wq_files_match, files, true);\n\n\twhile (!list_empty_careful(&ctx->inflight_list)) {\n\t\tstruct io_kiocb *cancel_req = NULL, *req;\n\t\tDEFINE_WAIT(wait);\n\n\t\tspin_lock_irq(&ctx->inflight_lock);\n\t\tlist_for_each_entry(req, &ctx->inflight_list, inflight_entry) {\n\t\t\tif (files && req->work.files != files)\n\t\t\t\tcontinue;\n\t\t\t/* req is being completed, ignore */\n\t\t\tif (!refcount_inc_not_zero(&req->refs))\n\t\t\t\tcontinue;\n\t\t\tcancel_req = req;\n\t\t\tbreak;\n\t\t}\n\t\tif (cancel_req)\n\t\t\tprepare_to_wait(&ctx->inflight_wait, &wait,\n\t\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tspin_unlock_irq(&ctx->inflight_lock);\n\n\t\t/* We need to keep going until we don't find a matching req */\n\t\tif (!cancel_req)\n\t\t\tbreak;\n\t\t/* cancel this request, or head link requests */\n\t\tio_attempt_cancel(ctx, cancel_req);\n\t\tio_put_req(cancel_req);\n\t\t/* cancellations _may_ trigger task work */\n\t\tio_run_task_work();\n\t\tschedule();\n\t\tfinish_wait(&ctx->inflight_wait, &wait);\n\t}\n\n\treturn true;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,7 +14,7 @@\n \n \t\tspin_lock_irq(&ctx->inflight_lock);\n \t\tlist_for_each_entry(req, &ctx->inflight_list, inflight_entry) {\n-\t\t\tif (req->work.files != files)\n+\t\t\tif (files && req->work.files != files)\n \t\t\t\tcontinue;\n \t\t\t/* req is being completed, ignore */\n \t\t\tif (!refcount_inc_not_zero(&req->refs))",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (files && req->work.files != files)"
            ],
            "deleted": [
                "\t\t\tif (req->work.files != files)"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2844
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static void io_req_free_batch_finish(struct io_ring_ctx *ctx,\n\t\t\t\t     struct req_batch *rb)\n{\n\tif (rb->to_free)\n\t\t__io_req_free_batch_flush(ctx, rb);\n\tif (rb->task) {\n\t\tput_task_struct_many(rb->task, rb->task_refs);\n\t\trb->task = NULL;\n\t}\n}",
        "code_after_change": "static void io_req_free_batch_finish(struct io_ring_ctx *ctx,\n\t\t\t\t     struct req_batch *rb)\n{\n\tif (rb->to_free)\n\t\t__io_req_free_batch_flush(ctx, rb);\n\tif (rb->task) {\n\t\tatomic_long_add(rb->task_refs, &rb->task->io_uring->req_complete);\n\t\tput_task_struct_many(rb->task, rb->task_refs);\n\t\trb->task = NULL;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \tif (rb->to_free)\n \t\t__io_req_free_batch_flush(ctx, rb);\n \tif (rb->task) {\n+\t\tatomic_long_add(rb->task_refs, &rb->task->io_uring->req_complete);\n \t\tput_task_struct_many(rb->task, rb->task_refs);\n \t\trb->task = NULL;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tatomic_long_add(rb->task_refs, &rb->task->io_uring->req_complete);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2845
    },
    {
        "cve_id": "CVE-2018-20669",
        "code_before_change": "\nSYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,\n\t\tinfop, int, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);\n\tint signo = 0;\n\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\tif (!infop)\n\t\treturn err;\n\n\tif (!access_ok(infop, sizeof(*infop)))\n\t\treturn -EFAULT;\n\n\tuser_access_begin();\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "code_after_change": "\nSYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,\n\t\tinfop, int, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);\n\tint signo = 0;\n\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\tif (!infop)\n\t\treturn err;\n\n\tif (!user_access_begin(infop, sizeof(*infop)))\n\t\treturn -EFAULT;\n\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,10 +16,9 @@\n \tif (!infop)\n \t\treturn err;\n \n-\tif (!access_ok(infop, sizeof(*infop)))\n+\tif (!user_access_begin(infop, sizeof(*infop)))\n \t\treturn -EFAULT;\n \n-\tuser_access_begin();\n \tunsafe_put_user(signo, &infop->si_signo, Efault);\n \tunsafe_put_user(0, &infop->si_errno, Efault);\n \tunsafe_put_user(info.cause, &infop->si_code, Efault);",
        "function_modified_lines": {
            "added": [
                "\tif (!user_access_begin(infop, sizeof(*infop)))"
            ],
            "deleted": [
                "\tif (!access_ok(infop, sizeof(*infop)))",
                "\tuser_access_begin();"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "An issue where a provided address with access_ok() is not checked was discovered in i915_gem_execbuffer2_ioctl in drivers/gpu/drm/i915/i915_gem_execbuffer.c in the Linux kernel through 4.19.13. A local attacker can craft a malicious IOCTL function call to overwrite arbitrary kernel memory, resulting in a Denial of Service or privilege escalation.",
        "id": 1778
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int rawsock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t   struct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied;\n\tint rc;\n\n\tpr_debug(\"sock=%p sk=%p len=%zu flags=%d\\n\", sock, sk, len, flags);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &rc);\n\tif (!skb)\n\t\treturn rc;\n\n\tmsg->msg_namelen = 0;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\trc = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tskb_free_datagram(sk, skb);\n\n\treturn rc ? : copied;\n}",
        "code_after_change": "static int rawsock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t   struct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied;\n\tint rc;\n\n\tpr_debug(\"sock=%p sk=%p len=%zu flags=%d\\n\", sock, sk, len, flags);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &rc);\n\tif (!skb)\n\t\treturn rc;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\trc = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tskb_free_datagram(sk, skb);\n\n\treturn rc ? : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,8 +13,6 @@\n \tif (!skb)\n \t\treturn rc;\n \n-\tmsg->msg_namelen = 0;\n-\n \tcopied = skb->len;\n \tif (len < copied) {\n \t\tmsg->msg_flags |= MSG_TRUNC;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmsg->msg_namelen = 0;",
                ""
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 392
    },
    {
        "cve_id": "CVE-2013-2897",
        "code_before_change": "static int mt_touch_input_mapping(struct hid_device *hdev, struct hid_input *hi,\n\t\tstruct hid_field *field, struct hid_usage *usage,\n\t\tunsigned long **bit, int *max)\n{\n\tstruct mt_device *td = hid_get_drvdata(hdev);\n\tstruct mt_class *cls = &td->mtclass;\n\tint code;\n\tstruct hid_usage *prev_usage = NULL;\n\n\tif (field->application == HID_DG_TOUCHSCREEN)\n\t\ttd->mt_flags |= INPUT_MT_DIRECT;\n\n\t/*\n\t * Model touchscreens providing buttons as touchpads.\n\t */\n\tif (field->application == HID_DG_TOUCHPAD ||\n\t    (usage->hid & HID_USAGE_PAGE) == HID_UP_BUTTON)\n\t\ttd->mt_flags |= INPUT_MT_POINTER;\n\n\tif (usage->usage_index)\n\t\tprev_usage = &field->usage[usage->usage_index - 1];\n\n\tswitch (usage->hid & HID_USAGE_PAGE) {\n\n\tcase HID_UP_GENDESK:\n\t\tswitch (usage->hid) {\n\t\tcase HID_GD_X:\n\t\t\tif (prev_usage && (prev_usage->hid == usage->hid)) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOOL_X);\n\t\t\t\tset_abs(hi->input, ABS_MT_TOOL_X, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t} else {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_POSITION_X);\n\t\t\t\tset_abs(hi->input, ABS_MT_POSITION_X, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t}\n\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_GD_Y:\n\t\t\tif (prev_usage && (prev_usage->hid == usage->hid)) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOOL_Y);\n\t\t\t\tset_abs(hi->input, ABS_MT_TOOL_Y, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t} else {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_POSITION_Y);\n\t\t\t\tset_abs(hi->input, ABS_MT_POSITION_Y, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t}\n\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\t}\n\t\treturn 0;\n\n\tcase HID_UP_DIGITIZER:\n\t\tswitch (usage->hid) {\n\t\tcase HID_DG_INRANGE:\n\t\t\tif (cls->quirks & MT_QUIRK_HOVERING) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_DISTANCE);\n\t\t\t\tinput_set_abs_params(hi->input,\n\t\t\t\t\tABS_MT_DISTANCE, 0, 1, 0, 0);\n\t\t\t}\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONFIDENCE:\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_TIPSWITCH:\n\t\t\thid_map_usage(hi, usage, bit, max, EV_KEY, BTN_TOUCH);\n\t\t\tinput_set_capability(hi->input, EV_KEY, BTN_TOUCH);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTID:\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\ttd->touches_by_report++;\n\t\t\ttd->mt_report_id = field->report->id;\n\t\t\treturn 1;\n\t\tcase HID_DG_WIDTH:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOUCH_MAJOR);\n\t\t\tif (!(cls->quirks & MT_QUIRK_NO_AREA))\n\t\t\t\tset_abs(hi->input, ABS_MT_TOUCH_MAJOR, field,\n\t\t\t\t\tcls->sn_width);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_HEIGHT:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOUCH_MINOR);\n\t\t\tif (!(cls->quirks & MT_QUIRK_NO_AREA)) {\n\t\t\t\tset_abs(hi->input, ABS_MT_TOUCH_MINOR, field,\n\t\t\t\t\tcls->sn_height);\n\t\t\t\tinput_set_abs_params(hi->input,\n\t\t\t\t\tABS_MT_ORIENTATION, 0, 1, 0, 0);\n\t\t\t}\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_TIPPRESSURE:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_PRESSURE);\n\t\t\tset_abs(hi->input, ABS_MT_PRESSURE, field,\n\t\t\t\tcls->sn_pressure);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTCOUNT:\n\t\t\ttd->cc_index = field->index;\n\t\t\ttd->cc_value_index = usage->usage_index;\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTMAX:\n\t\t\t/* we don't set td->last_slot_field as contactcount and\n\t\t\t * contact max are global to the report */\n\t\t\treturn -1;\n\t\tcase HID_DG_TOUCH:\n\t\t\t/* Legacy devices use TIPSWITCH and not TOUCH.\n\t\t\t * Let's just ignore this field. */\n\t\t\treturn -1;\n\t\t}\n\t\t/* let hid-input decide for the others */\n\t\treturn 0;\n\n\tcase HID_UP_BUTTON:\n\t\tcode = BTN_MOUSE + ((usage->hid - 1) & HID_USAGE);\n\t\thid_map_usage(hi, usage, bit, max, EV_KEY, code);\n\t\tinput_set_capability(hi->input, EV_KEY, code);\n\t\treturn 1;\n\n\tcase 0xff000000:\n\t\t/* we do not want to map these: no input-oriented meaning */\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int mt_touch_input_mapping(struct hid_device *hdev, struct hid_input *hi,\n\t\tstruct hid_field *field, struct hid_usage *usage,\n\t\tunsigned long **bit, int *max)\n{\n\tstruct mt_device *td = hid_get_drvdata(hdev);\n\tstruct mt_class *cls = &td->mtclass;\n\tint code;\n\tstruct hid_usage *prev_usage = NULL;\n\n\tif (field->application == HID_DG_TOUCHSCREEN)\n\t\ttd->mt_flags |= INPUT_MT_DIRECT;\n\n\t/*\n\t * Model touchscreens providing buttons as touchpads.\n\t */\n\tif (field->application == HID_DG_TOUCHPAD ||\n\t    (usage->hid & HID_USAGE_PAGE) == HID_UP_BUTTON)\n\t\ttd->mt_flags |= INPUT_MT_POINTER;\n\n\tif (usage->usage_index)\n\t\tprev_usage = &field->usage[usage->usage_index - 1];\n\n\tswitch (usage->hid & HID_USAGE_PAGE) {\n\n\tcase HID_UP_GENDESK:\n\t\tswitch (usage->hid) {\n\t\tcase HID_GD_X:\n\t\t\tif (prev_usage && (prev_usage->hid == usage->hid)) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOOL_X);\n\t\t\t\tset_abs(hi->input, ABS_MT_TOOL_X, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t} else {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_POSITION_X);\n\t\t\t\tset_abs(hi->input, ABS_MT_POSITION_X, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t}\n\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_GD_Y:\n\t\t\tif (prev_usage && (prev_usage->hid == usage->hid)) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOOL_Y);\n\t\t\t\tset_abs(hi->input, ABS_MT_TOOL_Y, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t} else {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_POSITION_Y);\n\t\t\t\tset_abs(hi->input, ABS_MT_POSITION_Y, field,\n\t\t\t\t\tcls->sn_move);\n\t\t\t}\n\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\t}\n\t\treturn 0;\n\n\tcase HID_UP_DIGITIZER:\n\t\tswitch (usage->hid) {\n\t\tcase HID_DG_INRANGE:\n\t\t\tif (cls->quirks & MT_QUIRK_HOVERING) {\n\t\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_DISTANCE);\n\t\t\t\tinput_set_abs_params(hi->input,\n\t\t\t\t\tABS_MT_DISTANCE, 0, 1, 0, 0);\n\t\t\t}\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONFIDENCE:\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_TIPSWITCH:\n\t\t\thid_map_usage(hi, usage, bit, max, EV_KEY, BTN_TOUCH);\n\t\t\tinput_set_capability(hi->input, EV_KEY, BTN_TOUCH);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTID:\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\ttd->touches_by_report++;\n\t\t\ttd->mt_report_id = field->report->id;\n\t\t\treturn 1;\n\t\tcase HID_DG_WIDTH:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOUCH_MAJOR);\n\t\t\tif (!(cls->quirks & MT_QUIRK_NO_AREA))\n\t\t\t\tset_abs(hi->input, ABS_MT_TOUCH_MAJOR, field,\n\t\t\t\t\tcls->sn_width);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_HEIGHT:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_TOUCH_MINOR);\n\t\t\tif (!(cls->quirks & MT_QUIRK_NO_AREA)) {\n\t\t\t\tset_abs(hi->input, ABS_MT_TOUCH_MINOR, field,\n\t\t\t\t\tcls->sn_height);\n\t\t\t\tinput_set_abs_params(hi->input,\n\t\t\t\t\tABS_MT_ORIENTATION, 0, 1, 0, 0);\n\t\t\t}\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_TIPPRESSURE:\n\t\t\thid_map_usage(hi, usage, bit, max,\n\t\t\t\t\tEV_ABS, ABS_MT_PRESSURE);\n\t\t\tset_abs(hi->input, ABS_MT_PRESSURE, field,\n\t\t\t\tcls->sn_pressure);\n\t\t\tmt_store_field(usage, td, hi);\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTCOUNT:\n\t\t\t/* Ignore if indexes are out of bounds. */\n\t\t\tif (field->index >= field->report->maxfield ||\n\t\t\t    usage->usage_index >= field->report_count)\n\t\t\t\treturn 1;\n\t\t\ttd->cc_index = field->index;\n\t\t\ttd->cc_value_index = usage->usage_index;\n\t\t\treturn 1;\n\t\tcase HID_DG_CONTACTMAX:\n\t\t\t/* we don't set td->last_slot_field as contactcount and\n\t\t\t * contact max are global to the report */\n\t\t\treturn -1;\n\t\tcase HID_DG_TOUCH:\n\t\t\t/* Legacy devices use TIPSWITCH and not TOUCH.\n\t\t\t * Let's just ignore this field. */\n\t\t\treturn -1;\n\t\t}\n\t\t/* let hid-input decide for the others */\n\t\treturn 0;\n\n\tcase HID_UP_BUTTON:\n\t\tcode = BTN_MOUSE + ((usage->hid - 1) & HID_USAGE);\n\t\thid_map_usage(hi, usage, bit, max, EV_KEY, code);\n\t\tinput_set_capability(hi->input, EV_KEY, code);\n\t\treturn 1;\n\n\tcase 0xff000000:\n\t\t/* we do not want to map these: no input-oriented meaning */\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -108,6 +108,10 @@\n \t\t\tmt_store_field(usage, td, hi);\n \t\t\treturn 1;\n \t\tcase HID_DG_CONTACTCOUNT:\n+\t\t\t/* Ignore if indexes are out of bounds. */\n+\t\t\tif (field->index >= field->report->maxfield ||\n+\t\t\t    usage->usage_index >= field->report_count)\n+\t\t\t\treturn 1;\n \t\t\ttd->cc_index = field->index;\n \t\t\ttd->cc_value_index = usage->usage_index;\n \t\t\treturn 1;",
        "function_modified_lines": {
            "added": [
                "\t\t\t/* Ignore if indexes are out of bounds. */",
                "\t\t\tif (field->index >= field->report->maxfield ||",
                "\t\t\t    usage->usage_index >= field->report_count)",
                "\t\t\t\treturn 1;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Multiple array index errors in drivers/hid/hid-multitouch.c in the Human Interface Device (HID) subsystem in the Linux kernel through 3.11, when CONFIG_HID_MULTITOUCH is enabled, allow physically proximate attackers to cause a denial of service (heap memory corruption, or NULL pointer dereference and OOPS) via a crafted device.",
        "id": 257
    },
    {
        "cve_id": "CVE-2013-2146",
        "code_before_change": "__init int intel_pmu_init(void)\n{\n\tunion cpuid10_edx edx;\n\tunion cpuid10_eax eax;\n\tunion cpuid10_ebx ebx;\n\tstruct event_constraint *c;\n\tunsigned int unused;\n\tint version;\n\n\tif (!cpu_has(&boot_cpu_data, X86_FEATURE_ARCH_PERFMON)) {\n\t\tswitch (boot_cpu_data.x86) {\n\t\tcase 0x6:\n\t\t\treturn p6_pmu_init();\n\t\tcase 0xb:\n\t\t\treturn knc_pmu_init();\n\t\tcase 0xf:\n\t\t\treturn p4_pmu_init();\n\t\t}\n\t\treturn -ENODEV;\n\t}\n\n\t/*\n\t * Check whether the Architectural PerfMon supports\n\t * Branch Misses Retired hw_event or not.\n\t */\n\tcpuid(10, &eax.full, &ebx.full, &unused, &edx.full);\n\tif (eax.split.mask_length < ARCH_PERFMON_EVENTS_COUNT)\n\t\treturn -ENODEV;\n\n\tversion = eax.split.version_id;\n\tif (version < 2)\n\t\tx86_pmu = core_pmu;\n\telse\n\t\tx86_pmu = intel_pmu;\n\n\tx86_pmu.version\t\t\t= version;\n\tx86_pmu.num_counters\t\t= eax.split.num_counters;\n\tx86_pmu.cntval_bits\t\t= eax.split.bit_width;\n\tx86_pmu.cntval_mask\t\t= (1ULL << eax.split.bit_width) - 1;\n\n\tx86_pmu.events_maskl\t\t= ebx.full;\n\tx86_pmu.events_mask_len\t\t= eax.split.mask_length;\n\n\tx86_pmu.max_pebs_events\t\t= min_t(unsigned, MAX_PEBS_EVENTS, x86_pmu.num_counters);\n\n\t/*\n\t * Quirk: v2 perfmon does not report fixed-purpose events, so\n\t * assume at least 3 events:\n\t */\n\tif (version > 1)\n\t\tx86_pmu.num_counters_fixed = max((int)edx.split.num_counters_fixed, 3);\n\n\t/*\n\t * v2 and above have a perf capabilities MSR\n\t */\n\tif (version > 1) {\n\t\tu64 capabilities;\n\n\t\trdmsrl(MSR_IA32_PERF_CAPABILITIES, capabilities);\n\t\tx86_pmu.intel_cap.capabilities = capabilities;\n\t}\n\n\tintel_ds_init();\n\n\tx86_add_quirk(intel_arch_events_quirk); /* Install first, so it runs last */\n\n\t/*\n\t * Install the hw-cache-events table:\n\t */\n\tswitch (boot_cpu_data.x86_model) {\n\tcase 14: /* 65 nm core solo/duo, \"Yonah\" */\n\t\tpr_cont(\"Core events, \");\n\t\tbreak;\n\n\tcase 15: /* original 65 nm celeron/pentium/core2/xeon, \"Merom\"/\"Conroe\" */\n\t\tx86_add_quirk(intel_clovertown_quirk);\n\tcase 22: /* single-core 65 nm celeron/core2solo \"Merom-L\"/\"Conroe-L\" */\n\tcase 23: /* current 45 nm celeron/core2/xeon \"Penryn\"/\"Wolfdale\" */\n\tcase 29: /* six-core 45 nm xeon \"Dunnington\" */\n\t\tmemcpy(hw_cache_event_ids, core2_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_core();\n\n\t\tx86_pmu.event_constraints = intel_core2_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_core2_pebs_event_constraints;\n\t\tpr_cont(\"Core2 events, \");\n\t\tbreak;\n\n\tcase 26: /* 45 nm nehalem, \"Bloomfield\" */\n\tcase 30: /* 45 nm nehalem, \"Lynnfield\" */\n\tcase 46: /* 45 nm nehalem-ex, \"Beckton\" */\n\t\tmemcpy(hw_cache_event_ids, nehalem_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_nehalem_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_nehalem_pebs_event_constraints;\n\t\tx86_pmu.enable_all = intel_pmu_nhm_enable_all;\n\t\tx86_pmu.extra_regs = intel_nehalem_extra_regs;\n\n\t\t/* UOPS_ISSUED.STALLED_CYCLES */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\t\t/* UOPS_EXECUTED.CORE_ACTIVE_CYCLES,c=1,i=1 */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\n\t\t\tX86_CONFIG(.event=0xb1, .umask=0x3f, .inv=1, .cmask=1);\n\n\t\tx86_add_quirk(intel_nehalem_quirk);\n\n\t\tpr_cont(\"Nehalem events, \");\n\t\tbreak;\n\n\tcase 28: /* Atom */\n\tcase 38: /* Lincroft */\n\tcase 39: /* Penwell */\n\tcase 53: /* Cloverview */\n\tcase 54: /* Cedarview */\n\t\tmemcpy(hw_cache_event_ids, atom_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_atom();\n\n\t\tx86_pmu.event_constraints = intel_gen_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_atom_pebs_event_constraints;\n\t\tpr_cont(\"Atom events, \");\n\t\tbreak;\n\n\tcase 37: /* 32 nm nehalem, \"Clarkdale\" */\n\tcase 44: /* 32 nm nehalem, \"Gulftown\" */\n\tcase 47: /* 32 nm Xeon E7 */\n\t\tmemcpy(hw_cache_event_ids, westmere_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_westmere_event_constraints;\n\t\tx86_pmu.enable_all = intel_pmu_nhm_enable_all;\n\t\tx86_pmu.pebs_constraints = intel_westmere_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_westmere_extra_regs;\n\t\tx86_pmu.er_flags |= ERF_HAS_RSP_1;\n\n\t\t/* UOPS_ISSUED.STALLED_CYCLES */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\t\t/* UOPS_EXECUTED.CORE_ACTIVE_CYCLES,c=1,i=1 */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\n\t\t\tX86_CONFIG(.event=0xb1, .umask=0x3f, .inv=1, .cmask=1);\n\n\t\tpr_cont(\"Westmere events, \");\n\t\tbreak;\n\n\tcase 42: /* SandyBridge */\n\tcase 45: /* SandyBridge, \"Romely-EP\" */\n\t\tx86_add_quirk(intel_sandybridge_quirk);\n\t\tmemcpy(hw_cache_event_ids, snb_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, snb_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_snb();\n\n\t\tx86_pmu.event_constraints = intel_snb_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_snb_pebs_event_constraints;\n\t\tx86_pmu.pebs_aliases = intel_pebs_aliases_snb;\n\t\tx86_pmu.extra_regs = intel_snb_extra_regs;\n\t\t/* all extra regs are per-cpu when HT is on */\n\t\tx86_pmu.er_flags |= ERF_HAS_RSP_1;\n\t\tx86_pmu.er_flags |= ERF_NO_HT_SHARING;\n\n\t\t/* UOPS_ISSUED.ANY,c=1,i=1 to count stall cycles */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\t\t/* UOPS_DISPATCHED.THREAD,c=1,i=1 to count stall cycles*/\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\n\t\t\tX86_CONFIG(.event=0xb1, .umask=0x01, .inv=1, .cmask=1);\n\n\t\tpr_cont(\"SandyBridge events, \");\n\t\tbreak;\n\tcase 58: /* IvyBridge */\n\tcase 62: /* IvyBridge EP */\n\t\tmemcpy(hw_cache_event_ids, snb_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, snb_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_snb();\n\n\t\tx86_pmu.event_constraints = intel_ivb_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_ivb_pebs_event_constraints;\n\t\tx86_pmu.pebs_aliases = intel_pebs_aliases_snb;\n\t\tx86_pmu.extra_regs = intel_snb_extra_regs;\n\t\t/* all extra regs are per-cpu when HT is on */\n\t\tx86_pmu.er_flags |= ERF_HAS_RSP_1;\n\t\tx86_pmu.er_flags |= ERF_NO_HT_SHARING;\n\n\t\t/* UOPS_ISSUED.ANY,c=1,i=1 to count stall cycles */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\n\t\tpr_cont(\"IvyBridge events, \");\n\t\tbreak;\n\n\n\tdefault:\n\t\tswitch (x86_pmu.version) {\n\t\tcase 1:\n\t\t\tx86_pmu.event_constraints = intel_v1_event_constraints;\n\t\t\tpr_cont(\"generic architected perfmon v1, \");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/*\n\t\t\t * default constraints for v2 and up\n\t\t\t */\n\t\t\tx86_pmu.event_constraints = intel_gen_event_constraints;\n\t\t\tpr_cont(\"generic architected perfmon, \");\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (x86_pmu.num_counters > INTEL_PMC_MAX_GENERIC) {\n\t\tWARN(1, KERN_ERR \"hw perf events %d > max(%d), clipping!\",\n\t\t     x86_pmu.num_counters, INTEL_PMC_MAX_GENERIC);\n\t\tx86_pmu.num_counters = INTEL_PMC_MAX_GENERIC;\n\t}\n\tx86_pmu.intel_ctrl = (1 << x86_pmu.num_counters) - 1;\n\n\tif (x86_pmu.num_counters_fixed > INTEL_PMC_MAX_FIXED) {\n\t\tWARN(1, KERN_ERR \"hw perf events fixed %d > max(%d), clipping!\",\n\t\t     x86_pmu.num_counters_fixed, INTEL_PMC_MAX_FIXED);\n\t\tx86_pmu.num_counters_fixed = INTEL_PMC_MAX_FIXED;\n\t}\n\n\tx86_pmu.intel_ctrl |=\n\t\t((1LL << x86_pmu.num_counters_fixed)-1) << INTEL_PMC_IDX_FIXED;\n\n\tif (x86_pmu.event_constraints) {\n\t\t/*\n\t\t * event on fixed counter2 (REF_CYCLES) only works on this\n\t\t * counter, so do not extend mask to generic counters\n\t\t */\n\t\tfor_each_event_constraint(c, x86_pmu.event_constraints) {\n\t\t\tif (c->cmask != X86_RAW_EVENT_MASK\n\t\t\t    || c->idxmsk64 == INTEL_PMC_MSK_FIXED_REF_CYCLES) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tc->idxmsk64 |= (1ULL << x86_pmu.num_counters) - 1;\n\t\t\tc->weight += x86_pmu.num_counters;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "__init int intel_pmu_init(void)\n{\n\tunion cpuid10_edx edx;\n\tunion cpuid10_eax eax;\n\tunion cpuid10_ebx ebx;\n\tstruct event_constraint *c;\n\tunsigned int unused;\n\tint version;\n\n\tif (!cpu_has(&boot_cpu_data, X86_FEATURE_ARCH_PERFMON)) {\n\t\tswitch (boot_cpu_data.x86) {\n\t\tcase 0x6:\n\t\t\treturn p6_pmu_init();\n\t\tcase 0xb:\n\t\t\treturn knc_pmu_init();\n\t\tcase 0xf:\n\t\t\treturn p4_pmu_init();\n\t\t}\n\t\treturn -ENODEV;\n\t}\n\n\t/*\n\t * Check whether the Architectural PerfMon supports\n\t * Branch Misses Retired hw_event or not.\n\t */\n\tcpuid(10, &eax.full, &ebx.full, &unused, &edx.full);\n\tif (eax.split.mask_length < ARCH_PERFMON_EVENTS_COUNT)\n\t\treturn -ENODEV;\n\n\tversion = eax.split.version_id;\n\tif (version < 2)\n\t\tx86_pmu = core_pmu;\n\telse\n\t\tx86_pmu = intel_pmu;\n\n\tx86_pmu.version\t\t\t= version;\n\tx86_pmu.num_counters\t\t= eax.split.num_counters;\n\tx86_pmu.cntval_bits\t\t= eax.split.bit_width;\n\tx86_pmu.cntval_mask\t\t= (1ULL << eax.split.bit_width) - 1;\n\n\tx86_pmu.events_maskl\t\t= ebx.full;\n\tx86_pmu.events_mask_len\t\t= eax.split.mask_length;\n\n\tx86_pmu.max_pebs_events\t\t= min_t(unsigned, MAX_PEBS_EVENTS, x86_pmu.num_counters);\n\n\t/*\n\t * Quirk: v2 perfmon does not report fixed-purpose events, so\n\t * assume at least 3 events:\n\t */\n\tif (version > 1)\n\t\tx86_pmu.num_counters_fixed = max((int)edx.split.num_counters_fixed, 3);\n\n\t/*\n\t * v2 and above have a perf capabilities MSR\n\t */\n\tif (version > 1) {\n\t\tu64 capabilities;\n\n\t\trdmsrl(MSR_IA32_PERF_CAPABILITIES, capabilities);\n\t\tx86_pmu.intel_cap.capabilities = capabilities;\n\t}\n\n\tintel_ds_init();\n\n\tx86_add_quirk(intel_arch_events_quirk); /* Install first, so it runs last */\n\n\t/*\n\t * Install the hw-cache-events table:\n\t */\n\tswitch (boot_cpu_data.x86_model) {\n\tcase 14: /* 65 nm core solo/duo, \"Yonah\" */\n\t\tpr_cont(\"Core events, \");\n\t\tbreak;\n\n\tcase 15: /* original 65 nm celeron/pentium/core2/xeon, \"Merom\"/\"Conroe\" */\n\t\tx86_add_quirk(intel_clovertown_quirk);\n\tcase 22: /* single-core 65 nm celeron/core2solo \"Merom-L\"/\"Conroe-L\" */\n\tcase 23: /* current 45 nm celeron/core2/xeon \"Penryn\"/\"Wolfdale\" */\n\tcase 29: /* six-core 45 nm xeon \"Dunnington\" */\n\t\tmemcpy(hw_cache_event_ids, core2_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_core();\n\n\t\tx86_pmu.event_constraints = intel_core2_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_core2_pebs_event_constraints;\n\t\tpr_cont(\"Core2 events, \");\n\t\tbreak;\n\n\tcase 26: /* 45 nm nehalem, \"Bloomfield\" */\n\tcase 30: /* 45 nm nehalem, \"Lynnfield\" */\n\tcase 46: /* 45 nm nehalem-ex, \"Beckton\" */\n\t\tmemcpy(hw_cache_event_ids, nehalem_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_nehalem_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_nehalem_pebs_event_constraints;\n\t\tx86_pmu.enable_all = intel_pmu_nhm_enable_all;\n\t\tx86_pmu.extra_regs = intel_nehalem_extra_regs;\n\n\t\t/* UOPS_ISSUED.STALLED_CYCLES */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\t\t/* UOPS_EXECUTED.CORE_ACTIVE_CYCLES,c=1,i=1 */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\n\t\t\tX86_CONFIG(.event=0xb1, .umask=0x3f, .inv=1, .cmask=1);\n\n\t\tx86_add_quirk(intel_nehalem_quirk);\n\n\t\tpr_cont(\"Nehalem events, \");\n\t\tbreak;\n\n\tcase 28: /* Atom */\n\tcase 38: /* Lincroft */\n\tcase 39: /* Penwell */\n\tcase 53: /* Cloverview */\n\tcase 54: /* Cedarview */\n\t\tmemcpy(hw_cache_event_ids, atom_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_atom();\n\n\t\tx86_pmu.event_constraints = intel_gen_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_atom_pebs_event_constraints;\n\t\tpr_cont(\"Atom events, \");\n\t\tbreak;\n\n\tcase 37: /* 32 nm nehalem, \"Clarkdale\" */\n\tcase 44: /* 32 nm nehalem, \"Gulftown\" */\n\tcase 47: /* 32 nm Xeon E7 */\n\t\tmemcpy(hw_cache_event_ids, westmere_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_westmere_event_constraints;\n\t\tx86_pmu.enable_all = intel_pmu_nhm_enable_all;\n\t\tx86_pmu.pebs_constraints = intel_westmere_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_westmere_extra_regs;\n\t\tx86_pmu.er_flags |= ERF_HAS_RSP_1;\n\n\t\t/* UOPS_ISSUED.STALLED_CYCLES */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\t\t/* UOPS_EXECUTED.CORE_ACTIVE_CYCLES,c=1,i=1 */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\n\t\t\tX86_CONFIG(.event=0xb1, .umask=0x3f, .inv=1, .cmask=1);\n\n\t\tpr_cont(\"Westmere events, \");\n\t\tbreak;\n\n\tcase 42: /* SandyBridge */\n\tcase 45: /* SandyBridge, \"Romely-EP\" */\n\t\tx86_add_quirk(intel_sandybridge_quirk);\n\t\tmemcpy(hw_cache_event_ids, snb_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, snb_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_snb();\n\n\t\tx86_pmu.event_constraints = intel_snb_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_snb_pebs_event_constraints;\n\t\tx86_pmu.pebs_aliases = intel_pebs_aliases_snb;\n\t\tif (boot_cpu_data.x86_model == 45)\n\t\t\tx86_pmu.extra_regs = intel_snbep_extra_regs;\n\t\telse\n\t\t\tx86_pmu.extra_regs = intel_snb_extra_regs;\n\t\t/* all extra regs are per-cpu when HT is on */\n\t\tx86_pmu.er_flags |= ERF_HAS_RSP_1;\n\t\tx86_pmu.er_flags |= ERF_NO_HT_SHARING;\n\n\t\t/* UOPS_ISSUED.ANY,c=1,i=1 to count stall cycles */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\t\t/* UOPS_DISPATCHED.THREAD,c=1,i=1 to count stall cycles*/\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\n\t\t\tX86_CONFIG(.event=0xb1, .umask=0x01, .inv=1, .cmask=1);\n\n\t\tpr_cont(\"SandyBridge events, \");\n\t\tbreak;\n\tcase 58: /* IvyBridge */\n\tcase 62: /* IvyBridge EP */\n\t\tmemcpy(hw_cache_event_ids, snb_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, snb_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_snb();\n\n\t\tx86_pmu.event_constraints = intel_ivb_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_ivb_pebs_event_constraints;\n\t\tx86_pmu.pebs_aliases = intel_pebs_aliases_snb;\n\t\tif (boot_cpu_data.x86_model == 62)\n\t\t\tx86_pmu.extra_regs = intel_snbep_extra_regs;\n\t\telse\n\t\t\tx86_pmu.extra_regs = intel_snb_extra_regs;\n\t\t/* all extra regs are per-cpu when HT is on */\n\t\tx86_pmu.er_flags |= ERF_HAS_RSP_1;\n\t\tx86_pmu.er_flags |= ERF_NO_HT_SHARING;\n\n\t\t/* UOPS_ISSUED.ANY,c=1,i=1 to count stall cycles */\n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\n\t\tpr_cont(\"IvyBridge events, \");\n\t\tbreak;\n\n\n\tdefault:\n\t\tswitch (x86_pmu.version) {\n\t\tcase 1:\n\t\t\tx86_pmu.event_constraints = intel_v1_event_constraints;\n\t\t\tpr_cont(\"generic architected perfmon v1, \");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/*\n\t\t\t * default constraints for v2 and up\n\t\t\t */\n\t\t\tx86_pmu.event_constraints = intel_gen_event_constraints;\n\t\t\tpr_cont(\"generic architected perfmon, \");\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (x86_pmu.num_counters > INTEL_PMC_MAX_GENERIC) {\n\t\tWARN(1, KERN_ERR \"hw perf events %d > max(%d), clipping!\",\n\t\t     x86_pmu.num_counters, INTEL_PMC_MAX_GENERIC);\n\t\tx86_pmu.num_counters = INTEL_PMC_MAX_GENERIC;\n\t}\n\tx86_pmu.intel_ctrl = (1 << x86_pmu.num_counters) - 1;\n\n\tif (x86_pmu.num_counters_fixed > INTEL_PMC_MAX_FIXED) {\n\t\tWARN(1, KERN_ERR \"hw perf events fixed %d > max(%d), clipping!\",\n\t\t     x86_pmu.num_counters_fixed, INTEL_PMC_MAX_FIXED);\n\t\tx86_pmu.num_counters_fixed = INTEL_PMC_MAX_FIXED;\n\t}\n\n\tx86_pmu.intel_ctrl |=\n\t\t((1LL << x86_pmu.num_counters_fixed)-1) << INTEL_PMC_IDX_FIXED;\n\n\tif (x86_pmu.event_constraints) {\n\t\t/*\n\t\t * event on fixed counter2 (REF_CYCLES) only works on this\n\t\t * counter, so do not extend mask to generic counters\n\t\t */\n\t\tfor_each_event_constraint(c, x86_pmu.event_constraints) {\n\t\t\tif (c->cmask != X86_RAW_EVENT_MASK\n\t\t\t    || c->idxmsk64 == INTEL_PMC_MSK_FIXED_REF_CYCLES) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tc->idxmsk64 |= (1ULL << x86_pmu.num_counters) - 1;\n\t\t\tc->weight += x86_pmu.num_counters;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -168,7 +168,10 @@\n \t\tx86_pmu.event_constraints = intel_snb_event_constraints;\n \t\tx86_pmu.pebs_constraints = intel_snb_pebs_event_constraints;\n \t\tx86_pmu.pebs_aliases = intel_pebs_aliases_snb;\n-\t\tx86_pmu.extra_regs = intel_snb_extra_regs;\n+\t\tif (boot_cpu_data.x86_model == 45)\n+\t\t\tx86_pmu.extra_regs = intel_snbep_extra_regs;\n+\t\telse\n+\t\t\tx86_pmu.extra_regs = intel_snb_extra_regs;\n \t\t/* all extra regs are per-cpu when HT is on */\n \t\tx86_pmu.er_flags |= ERF_HAS_RSP_1;\n \t\tx86_pmu.er_flags |= ERF_NO_HT_SHARING;\n@@ -194,7 +197,10 @@\n \t\tx86_pmu.event_constraints = intel_ivb_event_constraints;\n \t\tx86_pmu.pebs_constraints = intel_ivb_pebs_event_constraints;\n \t\tx86_pmu.pebs_aliases = intel_pebs_aliases_snb;\n-\t\tx86_pmu.extra_regs = intel_snb_extra_regs;\n+\t\tif (boot_cpu_data.x86_model == 62)\n+\t\t\tx86_pmu.extra_regs = intel_snbep_extra_regs;\n+\t\telse\n+\t\t\tx86_pmu.extra_regs = intel_snb_extra_regs;\n \t\t/* all extra regs are per-cpu when HT is on */\n \t\tx86_pmu.er_flags |= ERF_HAS_RSP_1;\n \t\tx86_pmu.er_flags |= ERF_NO_HT_SHARING;",
        "function_modified_lines": {
            "added": [
                "\t\tif (boot_cpu_data.x86_model == 45)",
                "\t\t\tx86_pmu.extra_regs = intel_snbep_extra_regs;",
                "\t\telse",
                "\t\t\tx86_pmu.extra_regs = intel_snb_extra_regs;",
                "\t\tif (boot_cpu_data.x86_model == 62)",
                "\t\t\tx86_pmu.extra_regs = intel_snbep_extra_regs;",
                "\t\telse",
                "\t\t\tx86_pmu.extra_regs = intel_snb_extra_regs;"
            ],
            "deleted": [
                "\t\tx86_pmu.extra_regs = intel_snb_extra_regs;",
                "\t\tx86_pmu.extra_regs = intel_snb_extra_regs;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "arch/x86/kernel/cpu/perf_event_intel.c in the Linux kernel before 3.8.9, when the Performance Events Subsystem is enabled, specifies an incorrect bitmask, which allows local users to cause a denial of service (general protection fault and system crash) by attempting to set a reserved bit.",
        "id": 218
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int pppoe_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t  struct msghdr *m, size_t total_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint error = 0;\n\n\tif (sk->sk_state & PPPOX_BOUND) {\n\t\terror = -EIO;\n\t\tgoto end;\n\t}\n\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\tflags & MSG_DONTWAIT, &error);\n\tif (error < 0)\n\t\tgoto end;\n\n\tm->msg_namelen = 0;\n\n\tif (skb) {\n\t\ttotal_len = min_t(size_t, total_len, skb->len);\n\t\terror = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);\n\t\tif (error == 0) {\n\t\t\tconsume_skb(skb);\n\t\t\treturn total_len;\n\t\t}\n\t}\n\n\tkfree_skb(skb);\nend:\n\treturn error;\n}",
        "code_after_change": "static int pppoe_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t  struct msghdr *m, size_t total_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint error = 0;\n\n\tif (sk->sk_state & PPPOX_BOUND) {\n\t\terror = -EIO;\n\t\tgoto end;\n\t}\n\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\tflags & MSG_DONTWAIT, &error);\n\tif (error < 0)\n\t\tgoto end;\n\n\tif (skb) {\n\t\ttotal_len = min_t(size_t, total_len, skb->len);\n\t\terror = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);\n\t\tif (error == 0) {\n\t\t\tconsume_skb(skb);\n\t\t\treturn total_len;\n\t\t}\n\t}\n\n\tkfree_skb(skb);\nend:\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,8 +15,6 @@\n \tif (error < 0)\n \t\tgoto end;\n \n-\tm->msg_namelen = 0;\n-\n \tif (skb) {\n \t\ttotal_len = min_t(size_t, total_len, skb->len);\n \t\terror = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tm->msg_namelen = 0;",
                ""
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 369
    },
    {
        "cve_id": "CVE-2018-12207",
        "code_before_change": "static void kvm_init_debug(void)\n{\n\tstruct kvm_stats_debugfs_item *p;\n\n\tkvm_debugfs_dir = debugfs_create_dir(\"kvm\", NULL);\n\n\tkvm_debugfs_num_entries = 0;\n\tfor (p = debugfs_entries; p->name; ++p, kvm_debugfs_num_entries++) {\n\t\tdebugfs_create_file(p->name, 0644, kvm_debugfs_dir,\n\t\t\t\t    (void *)(long)p->offset,\n\t\t\t\t    stat_fops[p->kind]);\n\t}\n}",
        "code_after_change": "static void kvm_init_debug(void)\n{\n\tstruct kvm_stats_debugfs_item *p;\n\n\tkvm_debugfs_dir = debugfs_create_dir(\"kvm\", NULL);\n\n\tkvm_debugfs_num_entries = 0;\n\tfor (p = debugfs_entries; p->name; ++p, kvm_debugfs_num_entries++) {\n\t\tint mode = p->mode ? p->mode : 0644;\n\t\tdebugfs_create_file(p->name, mode, kvm_debugfs_dir,\n\t\t\t\t    (void *)(long)p->offset,\n\t\t\t\t    stat_fops[p->kind]);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,8 @@\n \n \tkvm_debugfs_num_entries = 0;\n \tfor (p = debugfs_entries; p->name; ++p, kvm_debugfs_num_entries++) {\n-\t\tdebugfs_create_file(p->name, 0644, kvm_debugfs_dir,\n+\t\tint mode = p->mode ? p->mode : 0644;\n+\t\tdebugfs_create_file(p->name, mode, kvm_debugfs_dir,\n \t\t\t\t    (void *)(long)p->offset,\n \t\t\t\t    stat_fops[p->kind]);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tint mode = p->mode ? p->mode : 0644;",
                "\t\tdebugfs_create_file(p->name, mode, kvm_debugfs_dir,"
            ],
            "deleted": [
                "\t\tdebugfs_create_file(p->name, 0644, kvm_debugfs_dir,"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Improper invalidation for page table updates by a virtual guest operating system for multiple Intel(R) Processors may allow an authenticated user to potentially enable denial of service of the host system via local access.",
        "id": 1647
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int hash_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\tstruct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct hash_ctx *ctx = ask->private;\n\tunsigned ds = crypto_ahash_digestsize(crypto_ahash_reqtfm(&ctx->req));\n\tint err;\n\n\tif (len > ds)\n\t\tlen = ds;\n\telse if (len < ds)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tmsg->msg_namelen = 0;\n\n\tlock_sock(sk);\n\tif (ctx->more) {\n\t\tctx->more = 0;\n\t\tahash_request_set_crypt(&ctx->req, NULL, ctx->result, 0);\n\t\terr = af_alg_wait_for_completion(crypto_ahash_final(&ctx->req),\n\t\t\t\t\t\t &ctx->completion);\n\t\tif (err)\n\t\t\tgoto unlock;\n\t}\n\n\terr = memcpy_toiovec(msg->msg_iov, ctx->result, len);\n\nunlock:\n\trelease_sock(sk);\n\n\treturn err ?: len;\n}",
        "code_after_change": "static int hash_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\tstruct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct hash_ctx *ctx = ask->private;\n\tunsigned ds = crypto_ahash_digestsize(crypto_ahash_reqtfm(&ctx->req));\n\tint err;\n\n\tif (len > ds)\n\t\tlen = ds;\n\telse if (len < ds)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tlock_sock(sk);\n\tif (ctx->more) {\n\t\tctx->more = 0;\n\t\tahash_request_set_crypt(&ctx->req, NULL, ctx->result, 0);\n\t\terr = af_alg_wait_for_completion(crypto_ahash_final(&ctx->req),\n\t\t\t\t\t\t &ctx->completion);\n\t\tif (err)\n\t\t\tgoto unlock;\n\t}\n\n\terr = memcpy_toiovec(msg->msg_iov, ctx->result, len);\n\nunlock:\n\trelease_sock(sk);\n\n\treturn err ?: len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,8 +11,6 @@\n \t\tlen = ds;\n \telse if (len < ds)\n \t\tmsg->msg_flags |= MSG_TRUNC;\n-\n-\tmsg->msg_namelen = 0;\n \n \tlock_sock(sk);\n \tif (ctx->more) {",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 366
    },
    {
        "cve_id": "CVE-2017-17862",
        "code_before_change": "static int do_check(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len;\n\tint insn_idx, prev_insn_idx = 0;\n\tint insn_processed = 0;\n\tbool do_print_state = false;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tenv->cur_state = state;\n\tinit_reg_state(env, state->regs);\n\tstate->parent = NULL;\n\tinsn_idx = 0;\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tif (insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tinsn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tinsn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d: safe\\n\",\n\t\t\t\t\t\tprev_insn_idx, insn_idx);\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level > 1 || (env->log.level && do_print_state)) {\n\t\t\tif (env->log.level > 1)\n\t\t\t\tverbose(env, \"%d:\", insn_idx);\n\t\t\telse\n\t\t\t\tverbose(env, \"\\nfrom %d to %d:\",\n\t\t\t\t\tprev_insn_idx, insn_idx);\n\t\t\tprint_verifier_state(env, state);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level) {\n\t\t\tverbose(env, \"%d: \", insn_idx);\n\t\t\tprint_bpf_insn(verbose, env, insn,\n\t\t\t\t       env->allow_ptr_leaks);\n\t\t}\n\n\t\terr = ext_analyzer_insn_hook(env, insn_idx, prev_insn_idx);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tregs = cur_regs(env);\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, insn_idx, insn->src_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_READ,\n\t\t\t\t\t       insn->dst_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (src_reg_type != *prev_src_type &&\n\t\t\t\t   (src_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_src_type == PTR_TO_CTX)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_XADD) {\n\t\t\t\terr = check_xadd(env, insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tinsn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       insn->src_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (dst_reg_type != *prev_dst_type &&\n\t\t\t\t   (dst_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_dst_type == PTR_TO_CTX)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       -1);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->off != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\terr = check_call(env, insn->imm, insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tinsn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\t/* eBPF calling convetion is such that R0 is used\n\t\t\t\t * to return the value from eBPF program.\n\t\t\t\t * Make sure that it's readable at this time\n\t\t\t\t * of bpf_exit, which means that program wrote\n\t\t\t\t * something into it earlier\n\t\t\t\t */\n\t\t\t\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\t\t\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\terr = pop_stack(env, &prev_insn_idx, &insn_idx);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tinsn_idx++;\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinsn_idx++;\n\t}\n\n\tverbose(env, \"processed %d insns, stack depth %d\\n\", insn_processed,\n\t\tenv->prog->aux->stack_depth);\n\treturn 0;\n}",
        "code_after_change": "static int do_check(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len;\n\tint insn_idx, prev_insn_idx = 0;\n\tint insn_processed = 0;\n\tbool do_print_state = false;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tenv->cur_state = state;\n\tinit_reg_state(env, state->regs);\n\tstate->parent = NULL;\n\tinsn_idx = 0;\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tif (insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tinsn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tinsn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d: safe\\n\",\n\t\t\t\t\t\tprev_insn_idx, insn_idx);\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level > 1 || (env->log.level && do_print_state)) {\n\t\t\tif (env->log.level > 1)\n\t\t\t\tverbose(env, \"%d:\", insn_idx);\n\t\t\telse\n\t\t\t\tverbose(env, \"\\nfrom %d to %d:\",\n\t\t\t\t\tprev_insn_idx, insn_idx);\n\t\t\tprint_verifier_state(env, state);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level) {\n\t\t\tverbose(env, \"%d: \", insn_idx);\n\t\t\tprint_bpf_insn(verbose, env, insn,\n\t\t\t\t       env->allow_ptr_leaks);\n\t\t}\n\n\t\terr = ext_analyzer_insn_hook(env, insn_idx, prev_insn_idx);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tregs = cur_regs(env);\n\t\tenv->insn_aux_data[insn_idx].seen = true;\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, insn_idx, insn->src_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_READ,\n\t\t\t\t\t       insn->dst_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (src_reg_type != *prev_src_type &&\n\t\t\t\t   (src_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_src_type == PTR_TO_CTX)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_XADD) {\n\t\t\t\terr = check_xadd(env, insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tinsn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       insn->src_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (dst_reg_type != *prev_dst_type &&\n\t\t\t\t   (dst_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_dst_type == PTR_TO_CTX)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       -1);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->off != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\terr = check_call(env, insn->imm, insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tinsn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\t/* eBPF calling convetion is such that R0 is used\n\t\t\t\t * to return the value from eBPF program.\n\t\t\t\t * Make sure that it's readable at this time\n\t\t\t\t * of bpf_exit, which means that program wrote\n\t\t\t\t * something into it earlier\n\t\t\t\t */\n\t\t\t\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\t\t\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\terr = pop_stack(env, &prev_insn_idx, &insn_idx);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tinsn_idx++;\n\t\t\t\tenv->insn_aux_data[insn_idx].seen = true;\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinsn_idx++;\n\t}\n\n\tverbose(env, \"processed %d insns, stack depth %d\\n\", insn_processed,\n\t\tenv->prog->aux->stack_depth);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -75,6 +75,7 @@\n \t\t\treturn err;\n \n \t\tregs = cur_regs(env);\n+\t\tenv->insn_aux_data[insn_idx].seen = true;\n \t\tif (class == BPF_ALU || class == BPF_ALU64) {\n \t\t\terr = check_alu_op(env, insn);\n \t\t\tif (err)\n@@ -270,6 +271,7 @@\n \t\t\t\t\treturn err;\n \n \t\t\t\tinsn_idx++;\n+\t\t\t\tenv->insn_aux_data[insn_idx].seen = true;\n \t\t\t} else {\n \t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n \t\t\t\treturn -EINVAL;",
        "function_modified_lines": {
            "added": [
                "\t\tenv->insn_aux_data[insn_idx].seen = true;",
                "\t\t\t\tenv->insn_aux_data[insn_idx].seen = true;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "kernel/bpf/verifier.c in the Linux kernel through 4.14.8 ignores unreachable code, even though it would still be processed by JIT compilers. This behavior, also considered an improper branch-pruning logic issue, could possibly be used by local users for denial of service.",
        "id": 1383
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static void __io_cqring_fill_event(struct io_kiocb *req, long res, long cflags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_cqe *cqe;\n\n\ttrace_io_uring_complete(ctx, req->user_data, res);\n\n\t/*\n\t * If we can't get a cq entry, userspace overflowed the\n\t * submission (by quite a lot). Increment the overflow count in\n\t * the ring.\n\t */\n\tcqe = io_get_cqring(ctx);\n\tif (likely(cqe)) {\n\t\tWRITE_ONCE(cqe->user_data, req->user_data);\n\t\tWRITE_ONCE(cqe->res, res);\n\t\tWRITE_ONCE(cqe->flags, cflags);\n\t} else if (ctx->cq_overflow_flushed) {\n\t\tWRITE_ONCE(ctx->rings->cq_overflow,\n\t\t\t\tatomic_inc_return(&ctx->cached_cq_overflow));\n\t} else {\n\t\tif (list_empty(&ctx->cq_overflow_list)) {\n\t\t\tset_bit(0, &ctx->sq_check_overflow);\n\t\t\tset_bit(0, &ctx->cq_check_overflow);\n\t\t\tctx->rings->sq_flags |= IORING_SQ_CQ_OVERFLOW;\n\t\t}\n\t\tio_clean_op(req);\n\t\treq->result = res;\n\t\treq->compl.cflags = cflags;\n\t\trefcount_inc(&req->refs);\n\t\tlist_add_tail(&req->compl.list, &ctx->cq_overflow_list);\n\t}\n}",
        "code_after_change": "static void __io_cqring_fill_event(struct io_kiocb *req, long res, long cflags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_cqe *cqe;\n\n\ttrace_io_uring_complete(ctx, req->user_data, res);\n\n\t/*\n\t * If we can't get a cq entry, userspace overflowed the\n\t * submission (by quite a lot). Increment the overflow count in\n\t * the ring.\n\t */\n\tcqe = io_get_cqring(ctx);\n\tif (likely(cqe)) {\n\t\tWRITE_ONCE(cqe->user_data, req->user_data);\n\t\tWRITE_ONCE(cqe->res, res);\n\t\tWRITE_ONCE(cqe->flags, cflags);\n\t} else if (ctx->cq_overflow_flushed || req->task->io_uring->in_idle) {\n\t\t/*\n\t\t * If we're in ring overflow flush mode, or in task cancel mode,\n\t\t * then we cannot store the request for later flushing, we need\n\t\t * to drop it on the floor.\n\t\t */\n\t\tWRITE_ONCE(ctx->rings->cq_overflow,\n\t\t\t\tatomic_inc_return(&ctx->cached_cq_overflow));\n\t} else {\n\t\tif (list_empty(&ctx->cq_overflow_list)) {\n\t\t\tset_bit(0, &ctx->sq_check_overflow);\n\t\t\tset_bit(0, &ctx->cq_check_overflow);\n\t\t\tctx->rings->sq_flags |= IORING_SQ_CQ_OVERFLOW;\n\t\t}\n\t\tio_clean_op(req);\n\t\treq->result = res;\n\t\treq->compl.cflags = cflags;\n\t\trefcount_inc(&req->refs);\n\t\tlist_add_tail(&req->compl.list, &ctx->cq_overflow_list);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,7 +15,12 @@\n \t\tWRITE_ONCE(cqe->user_data, req->user_data);\n \t\tWRITE_ONCE(cqe->res, res);\n \t\tWRITE_ONCE(cqe->flags, cflags);\n-\t} else if (ctx->cq_overflow_flushed) {\n+\t} else if (ctx->cq_overflow_flushed || req->task->io_uring->in_idle) {\n+\t\t/*\n+\t\t * If we're in ring overflow flush mode, or in task cancel mode,\n+\t\t * then we cannot store the request for later flushing, we need\n+\t\t * to drop it on the floor.\n+\t\t */\n \t\tWRITE_ONCE(ctx->rings->cq_overflow,\n \t\t\t\tatomic_inc_return(&ctx->cached_cq_overflow));\n \t} else {",
        "function_modified_lines": {
            "added": [
                "\t} else if (ctx->cq_overflow_flushed || req->task->io_uring->in_idle) {",
                "\t\t/*",
                "\t\t * If we're in ring overflow flush mode, or in task cancel mode,",
                "\t\t * then we cannot store the request for later flushing, we need",
                "\t\t * to drop it on the floor.",
                "\t\t */"
            ],
            "deleted": [
                "\t} else if (ctx->cq_overflow_flushed) {"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2848
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int caif_seqpkt_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *m, size_t len, int flags)\n\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint ret;\n\tint copylen;\n\n\tret = -EOPNOTSUPP;\n\tif (m->msg_flags&MSG_OOB)\n\t\tgoto read_error;\n\n\tm->msg_namelen = 0;\n\n\tskb = skb_recv_datagram(sk, flags, 0 , &ret);\n\tif (!skb)\n\t\tgoto read_error;\n\tcopylen = skb->len;\n\tif (len < copylen) {\n\t\tm->msg_flags |= MSG_TRUNC;\n\t\tcopylen = len;\n\t}\n\n\tret = skb_copy_datagram_iovec(skb, 0, m->msg_iov, copylen);\n\tif (ret)\n\t\tgoto out_free;\n\n\tret = (flags & MSG_TRUNC) ? skb->len : copylen;\nout_free:\n\tskb_free_datagram(sk, skb);\n\tcaif_check_flow_release(sk);\n\treturn ret;\n\nread_error:\n\treturn ret;\n}",
        "code_after_change": "static int caif_seqpkt_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *m, size_t len, int flags)\n\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint ret;\n\tint copylen;\n\n\tret = -EOPNOTSUPP;\n\tif (m->msg_flags&MSG_OOB)\n\t\tgoto read_error;\n\n\tskb = skb_recv_datagram(sk, flags, 0 , &ret);\n\tif (!skb)\n\t\tgoto read_error;\n\tcopylen = skb->len;\n\tif (len < copylen) {\n\t\tm->msg_flags |= MSG_TRUNC;\n\t\tcopylen = len;\n\t}\n\n\tret = skb_copy_datagram_iovec(skb, 0, m->msg_iov, copylen);\n\tif (ret)\n\t\tgoto out_free;\n\n\tret = (flags & MSG_TRUNC) ? skb->len : copylen;\nout_free:\n\tskb_free_datagram(sk, skb);\n\tcaif_check_flow_release(sk);\n\treturn ret;\n\nread_error:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,8 +10,6 @@\n \tret = -EOPNOTSUPP;\n \tif (m->msg_flags&MSG_OOB)\n \t\tgoto read_error;\n-\n-\tm->msg_namelen = 0;\n \n \tskb = skb_recv_datagram(sk, flags, 0 , &ret);\n \tif (!skb)",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tm->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 379
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\t/*\n\t * If we queue this for async, it must not be cancellable. That would\n\t * leave the 'file' in an undeterminate state, and here need to modify\n\t * io_wq_work.flags, so initialize io_wq_work firstly.\n\t */\n\tio_req_init_async(req);\n\treq->work.flags |= IO_WQ_WORK_NO_CANCEL;\n\n\tif (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||\n\t    sqe->rw_flags || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->close.fd = READ_ONCE(sqe->fd);\n\tif ((req->file && req->file->f_op == &io_uring_fops) ||\n\t    req->close.fd == req->ctx->ring_fd)\n\t\treturn -EBADF;\n\n\treq->close.put_file = NULL;\n\treturn 0;\n}",
        "code_after_change": "static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\t/*\n\t * If we queue this for async, it must not be cancellable. That would\n\t * leave the 'file' in an undeterminate state, and here need to modify\n\t * io_wq_work.flags, so initialize io_wq_work firstly.\n\t */\n\tio_req_init_async(req);\n\treq->work.flags |= IO_WQ_WORK_NO_CANCEL;\n\n\tif (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||\n\t    sqe->rw_flags || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->close.fd = READ_ONCE(sqe->fd);\n\tif ((req->file && req->file->f_op == &io_uring_fops))\n\t\treturn -EBADF;\n\n\treq->close.put_file = NULL;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,8 +17,7 @@\n \t\treturn -EBADF;\n \n \treq->close.fd = READ_ONCE(sqe->fd);\n-\tif ((req->file && req->file->f_op == &io_uring_fops) ||\n-\t    req->close.fd == req->ctx->ring_fd)\n+\tif ((req->file && req->file->f_op == &io_uring_fops))\n \t\treturn -EBADF;\n \n \treq->close.put_file = NULL;",
        "function_modified_lines": {
            "added": [
                "\tif ((req->file && req->file->f_op == &io_uring_fops))"
            ],
            "deleted": [
                "\tif ((req->file && req->file->f_op == &io_uring_fops) ||",
                "\t    req->close.fd == req->ctx->ring_fd)"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2858
    },
    {
        "cve_id": "CVE-2020-12363",
        "code_before_change": "void intel_guc_ads_reset(struct intel_guc *guc)\n{\n\tif (!guc->ads_vma)\n\t\treturn;\n\t__guc_ads_init(guc);\n}",
        "code_after_change": "void intel_guc_ads_reset(struct intel_guc *guc)\n{\n\tif (!guc->ads_vma)\n\t\treturn;\n\n\t__guc_ads_init(guc);\n\n\tguc_ads_private_data_reset(guc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,5 +2,8 @@\n {\n \tif (!guc->ads_vma)\n \t\treturn;\n+\n \t__guc_ads_init(guc);\n+\n+\tguc_ads_private_data_reset(guc);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "",
                "\tguc_ads_private_data_reset(guc);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Improper input validation in some Intel(R) Graphics Drivers for Windows* before version 26.20.100.7212 and before Linux kernel version 5.5 may allow a privileged user to potentially enable a denial of service via local access.",
        "id": 2462
    },
    {
        "cve_id": "CVE-2015-2672",
        "code_before_change": "static inline int xsave_state(struct xsave_struct *fx, u64 mask)\n{\n\tu32 lmask = mask;\n\tu32 hmask = mask >> 32;\n\tint err = 0;\n\n\t/*\n\t * If xsaves is enabled, xsaves replaces xsaveopt because\n\t * it supports compact format and supervisor states in addition to\n\t * modified optimization in xsaveopt.\n\t *\n\t * Otherwise, if xsaveopt is enabled, xsaveopt replaces xsave\n\t * because xsaveopt supports modified optimization which is not\n\t * supported by xsave.\n\t *\n\t * If none of xsaves and xsaveopt is enabled, use xsave.\n\t */\n\talternative_input_2(\n\t\t\"1:\"XSAVE,\n\t\t\"1:\"XSAVEOPT,\n\t\tX86_FEATURE_XSAVEOPT,\n\t\t\"1:\"XSAVES,\n\t\tX86_FEATURE_XSAVES,\n\t\t[fx] \"D\" (fx), \"a\" (lmask), \"d\" (hmask) :\n\t\t\"memory\");\n\tasm volatile(\"2:\\n\\t\"\n\t\t     xstate_fault\n\t\t     : \"0\" (0)\n\t\t     : \"memory\");\n\n\treturn err;\n}",
        "code_after_change": "static inline int xsave_state(struct xsave_struct *fx, u64 mask)\n{\n\tu32 lmask = mask;\n\tu32 hmask = mask >> 32;\n\tint err = 0;\n\n\t/*\n\t * If xsaves is enabled, xsaves replaces xsaveopt because\n\t * it supports compact format and supervisor states in addition to\n\t * modified optimization in xsaveopt.\n\t *\n\t * Otherwise, if xsaveopt is enabled, xsaveopt replaces xsave\n\t * because xsaveopt supports modified optimization which is not\n\t * supported by xsave.\n\t *\n\t * If none of xsaves and xsaveopt is enabled, use xsave.\n\t */\n\talternative_input_2(\n\t\t\"1:\"XSAVE,\n\t\tXSAVEOPT,\n\t\tX86_FEATURE_XSAVEOPT,\n\t\tXSAVES,\n\t\tX86_FEATURE_XSAVES,\n\t\t[fx] \"D\" (fx), \"a\" (lmask), \"d\" (hmask) :\n\t\t\"memory\");\n\tasm volatile(\"2:\\n\\t\"\n\t\t     xstate_fault\n\t\t     : \"0\" (0)\n\t\t     : \"memory\");\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,9 +17,9 @@\n \t */\n \talternative_input_2(\n \t\t\"1:\"XSAVE,\n-\t\t\"1:\"XSAVEOPT,\n+\t\tXSAVEOPT,\n \t\tX86_FEATURE_XSAVEOPT,\n-\t\t\"1:\"XSAVES,\n+\t\tXSAVES,\n \t\tX86_FEATURE_XSAVES,\n \t\t[fx] \"D\" (fx), \"a\" (lmask), \"d\" (hmask) :\n \t\t\"memory\");",
        "function_modified_lines": {
            "added": [
                "\t\tXSAVEOPT,",
                "\t\tXSAVES,"
            ],
            "deleted": [
                "\t\t\"1:\"XSAVEOPT,",
                "\t\t\"1:\"XSAVES,"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The xsave/xrstor implementation in arch/x86/include/asm/xsave.h in the Linux kernel before 3.19.2 creates certain .altinstr_replacement pointers and consequently does not provide any protection against instruction faulting, which allows local users to cause a denial of service (panic) by triggering a fault, as demonstrated by an unaligned memory operand or a non-canonical address memory operand.",
        "id": 743
    },
    {
        "cve_id": "CVE-2016-2549",
        "code_before_change": "static int snd_hrtimer_start(struct snd_timer *t)\n{\n\tstruct snd_hrtimer *stime = t->private_data;\n\n\tatomic_set(&stime->running, 0);\n\thrtimer_cancel(&stime->hrt);\n\thrtimer_start(&stime->hrt, ns_to_ktime(t->sticks * resolution),\n\t\t      HRTIMER_MODE_REL);\n\tatomic_set(&stime->running, 1);\n\treturn 0;\n}",
        "code_after_change": "static int snd_hrtimer_start(struct snd_timer *t)\n{\n\tstruct snd_hrtimer *stime = t->private_data;\n\n\tatomic_set(&stime->running, 0);\n\thrtimer_try_to_cancel(&stime->hrt);\n\thrtimer_start(&stime->hrt, ns_to_ktime(t->sticks * resolution),\n\t\t      HRTIMER_MODE_REL);\n\tatomic_set(&stime->running, 1);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \tstruct snd_hrtimer *stime = t->private_data;\n \n \tatomic_set(&stime->running, 0);\n-\thrtimer_cancel(&stime->hrt);\n+\thrtimer_try_to_cancel(&stime->hrt);\n \thrtimer_start(&stime->hrt, ns_to_ktime(t->sticks * resolution),\n \t\t      HRTIMER_MODE_REL);\n \tatomic_set(&stime->running, 1);",
        "function_modified_lines": {
            "added": [
                "\thrtimer_try_to_cancel(&stime->hrt);"
            ],
            "deleted": [
                "\thrtimer_cancel(&stime->hrt);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "sound/core/hrtimer.c in the Linux kernel before 4.4.1 does not prevent recursive callback access, which allows local users to cause a denial of service (deadlock) via a crafted ioctl call.",
        "id": 949
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static int bprm_execve(struct linux_binprm *bprm,\n\t\t       int fd, struct filename *filename, int flags)\n{\n\tstruct file *file;\n\tstruct files_struct *displaced;\n\tint retval;\n\n\tretval = unshare_files(&displaced);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = prepare_bprm_creds(bprm);\n\tif (retval)\n\t\tgoto out_files;\n\n\tcheck_unsafe_exec(bprm);\n\tcurrent->in_execve = 1;\n\n\tfile = do_open_execat(fd, filename, flags);\n\tretval = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out_unmark;\n\n\tsched_exec();\n\n\tbprm->file = file;\n\t/*\n\t * Record that a name derived from an O_CLOEXEC fd will be\n\t * inaccessible after exec. Relies on having exclusive access to\n\t * current->files (due to unshare_files above).\n\t */\n\tif (bprm->fdpath &&\n\t    close_on_exec(fd, rcu_dereference_raw(current->files->fdt)))\n\t\tbprm->interp_flags |= BINPRM_FLAGS_PATH_INACCESSIBLE;\n\n\t/* Set the unchanging part of bprm->cred */\n\tretval = security_bprm_creds_for_exec(bprm);\n\tif (retval)\n\t\tgoto out;\n\n\tretval = exec_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\t/* execve succeeded */\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\trseq_execve(current);\n\tacct_update_integrals(current);\n\ttask_numa_free(current, false);\n\tif (displaced)\n\t\tput_files_struct(displaced);\n\treturn retval;\n\nout:\n\t/*\n\t * If past the point of no return ensure the the code never\n\t * returns to the userspace process.  Use an existing fatal\n\t * signal if present otherwise terminate the process with\n\t * SIGSEGV.\n\t */\n\tif (bprm->point_of_no_return && !fatal_signal_pending(current))\n\t\tforce_sigsegv(SIGSEGV);\n\nout_unmark:\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\nout_files:\n\tif (displaced)\n\t\treset_files_struct(displaced);\n\n\treturn retval;\n}",
        "code_after_change": "static int bprm_execve(struct linux_binprm *bprm,\n\t\t       int fd, struct filename *filename, int flags)\n{\n\tstruct file *file;\n\tstruct files_struct *displaced;\n\tint retval;\n\n\t/*\n\t * Cancel any io_uring activity across execve\n\t */\n\tio_uring_task_cancel();\n\n\tretval = unshare_files(&displaced);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = prepare_bprm_creds(bprm);\n\tif (retval)\n\t\tgoto out_files;\n\n\tcheck_unsafe_exec(bprm);\n\tcurrent->in_execve = 1;\n\n\tfile = do_open_execat(fd, filename, flags);\n\tretval = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out_unmark;\n\n\tsched_exec();\n\n\tbprm->file = file;\n\t/*\n\t * Record that a name derived from an O_CLOEXEC fd will be\n\t * inaccessible after exec. Relies on having exclusive access to\n\t * current->files (due to unshare_files above).\n\t */\n\tif (bprm->fdpath &&\n\t    close_on_exec(fd, rcu_dereference_raw(current->files->fdt)))\n\t\tbprm->interp_flags |= BINPRM_FLAGS_PATH_INACCESSIBLE;\n\n\t/* Set the unchanging part of bprm->cred */\n\tretval = security_bprm_creds_for_exec(bprm);\n\tif (retval)\n\t\tgoto out;\n\n\tretval = exec_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\t/* execve succeeded */\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\trseq_execve(current);\n\tacct_update_integrals(current);\n\ttask_numa_free(current, false);\n\tif (displaced)\n\t\tput_files_struct(displaced);\n\treturn retval;\n\nout:\n\t/*\n\t * If past the point of no return ensure the the code never\n\t * returns to the userspace process.  Use an existing fatal\n\t * signal if present otherwise terminate the process with\n\t * SIGSEGV.\n\t */\n\tif (bprm->point_of_no_return && !fatal_signal_pending(current))\n\t\tforce_sigsegv(SIGSEGV);\n\nout_unmark:\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\nout_files:\n\tif (displaced)\n\t\treset_files_struct(displaced);\n\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,11 @@\n \tstruct file *file;\n \tstruct files_struct *displaced;\n \tint retval;\n+\n+\t/*\n+\t * Cancel any io_uring activity across execve\n+\t */\n+\tio_uring_task_cancel();\n \n \tretval = unshare_files(&displaced);\n \tif (retval)",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * Cancel any io_uring activity across execve",
                "\t */",
                "\tio_uring_task_cancel();"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2842
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static int io_sq_offload_start(struct io_ring_ctx *ctx,\n\t\t\t       struct io_uring_params *p)\n{\n\tint ret;\n\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tret = -EPERM;\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\tgoto err;\n\n\t\tctx->sq_thread_idle = msecs_to_jiffies(p->sq_thread_idle);\n\t\tif (!ctx->sq_thread_idle)\n\t\t\tctx->sq_thread_idle = HZ;\n\n\t\tif (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t\tint cpu = p->sq_thread_cpu;\n\n\t\t\tret = -EINVAL;\n\t\t\tif (cpu >= nr_cpu_ids)\n\t\t\t\tgoto err;\n\t\t\tif (!cpu_online(cpu))\n\t\t\t\tgoto err;\n\n\t\t\tctx->sqo_thread = kthread_create_on_cpu(io_sq_thread,\n\t\t\t\t\t\t\tctx, cpu,\n\t\t\t\t\t\t\t\"io_uring-sq\");\n\t\t} else {\n\t\t\tctx->sqo_thread = kthread_create(io_sq_thread, ctx,\n\t\t\t\t\t\t\t\"io_uring-sq\");\n\t\t}\n\t\tif (IS_ERR(ctx->sqo_thread)) {\n\t\t\tret = PTR_ERR(ctx->sqo_thread);\n\t\t\tctx->sqo_thread = NULL;\n\t\t\tgoto err;\n\t\t}\n\t\twake_up_process(ctx->sqo_thread);\n\t} else if (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t/* Can't have SQ_AFF without SQPOLL */\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tret = io_init_wq_offload(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tio_finish_async(ctx);\n\treturn ret;\n}",
        "code_after_change": "static int io_sq_offload_start(struct io_ring_ctx *ctx,\n\t\t\t       struct io_uring_params *p)\n{\n\tint ret;\n\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tret = -EPERM;\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\tgoto err;\n\n\t\tctx->sq_thread_idle = msecs_to_jiffies(p->sq_thread_idle);\n\t\tif (!ctx->sq_thread_idle)\n\t\t\tctx->sq_thread_idle = HZ;\n\n\t\tif (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t\tint cpu = p->sq_thread_cpu;\n\n\t\t\tret = -EINVAL;\n\t\t\tif (cpu >= nr_cpu_ids)\n\t\t\t\tgoto err;\n\t\t\tif (!cpu_online(cpu))\n\t\t\t\tgoto err;\n\n\t\t\tctx->sqo_thread = kthread_create_on_cpu(io_sq_thread,\n\t\t\t\t\t\t\tctx, cpu,\n\t\t\t\t\t\t\t\"io_uring-sq\");\n\t\t} else {\n\t\t\tctx->sqo_thread = kthread_create(io_sq_thread, ctx,\n\t\t\t\t\t\t\t\"io_uring-sq\");\n\t\t}\n\t\tif (IS_ERR(ctx->sqo_thread)) {\n\t\t\tret = PTR_ERR(ctx->sqo_thread);\n\t\t\tctx->sqo_thread = NULL;\n\t\t\tgoto err;\n\t\t}\n\t\tret = io_uring_alloc_task_context(ctx->sqo_thread);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\twake_up_process(ctx->sqo_thread);\n\t} else if (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t/* Can't have SQ_AFF without SQPOLL */\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tret = io_init_wq_offload(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tio_finish_async(ctx);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -33,6 +33,9 @@\n \t\t\tctx->sqo_thread = NULL;\n \t\t\tgoto err;\n \t\t}\n+\t\tret = io_uring_alloc_task_context(ctx->sqo_thread);\n+\t\tif (ret)\n+\t\t\tgoto err;\n \t\twake_up_process(ctx->sqo_thread);\n \t} else if (p->flags & IORING_SETUP_SQ_AFF) {\n \t\t/* Can't have SQ_AFF without SQPOLL */",
        "function_modified_lines": {
            "added": [
                "\t\tret = io_uring_alloc_task_context(ctx->sqo_thread);",
                "\t\tif (ret)",
                "\t\t\tgoto err;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2847
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int rfcomm_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rfcomm_dlc *d = rfcomm_pi(sk)->dlc;\n\tint len;\n\n\tif (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {\n\t\trfcomm_dlc_accept(d);\n\t\tmsg->msg_namelen = 0;\n\t\treturn 0;\n\t}\n\n\tlen = bt_sock_stream_recvmsg(iocb, sock, msg, size, flags);\n\n\tlock_sock(sk);\n\tif (!(flags & MSG_PEEK) && len > 0)\n\t\tatomic_sub(len, &sk->sk_rmem_alloc);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= (sk->sk_rcvbuf >> 2))\n\t\trfcomm_dlc_unthrottle(rfcomm_pi(sk)->dlc);\n\trelease_sock(sk);\n\n\treturn len;\n}",
        "code_after_change": "static int rfcomm_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rfcomm_dlc *d = rfcomm_pi(sk)->dlc;\n\tint len;\n\n\tif (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {\n\t\trfcomm_dlc_accept(d);\n\t\treturn 0;\n\t}\n\n\tlen = bt_sock_stream_recvmsg(iocb, sock, msg, size, flags);\n\n\tlock_sock(sk);\n\tif (!(flags & MSG_PEEK) && len > 0)\n\t\tatomic_sub(len, &sk->sk_rmem_alloc);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= (sk->sk_rcvbuf >> 2))\n\t\trfcomm_dlc_unthrottle(rfcomm_pi(sk)->dlc);\n\trelease_sock(sk);\n\n\treturn len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,6 @@\n \n \tif (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {\n \t\trfcomm_dlc_accept(d);\n-\t\tmsg->msg_namelen = 0;\n \t\treturn 0;\n \t}\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 376
    },
    {
        "cve_id": "CVE-2016-5828",
        "code_before_change": "void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)\n{\n#ifdef CONFIG_PPC64\n\tunsigned long load_addr = regs->gpr[2];\t/* saved by ELF_PLAT_INIT */\n#endif\n\n\t/*\n\t * If we exec out of a kernel thread then thread.regs will not be\n\t * set.  Do it now.\n\t */\n\tif (!current->thread.regs) {\n\t\tstruct pt_regs *regs = task_stack_page(current) + THREAD_SIZE;\n\t\tcurrent->thread.regs = regs - 1;\n\t}\n\n\tmemset(regs->gpr, 0, sizeof(regs->gpr));\n\tregs->ctr = 0;\n\tregs->link = 0;\n\tregs->xer = 0;\n\tregs->ccr = 0;\n\tregs->gpr[1] = sp;\n\n\t/*\n\t * We have just cleared all the nonvolatile GPRs, so make\n\t * FULL_REGS(regs) return true.  This is necessary to allow\n\t * ptrace to examine the thread immediately after exec.\n\t */\n\tregs->trap &= ~1UL;\n\n#ifdef CONFIG_PPC32\n\tregs->mq = 0;\n\tregs->nip = start;\n\tregs->msr = MSR_USER;\n#else\n\tif (!is_32bit_task()) {\n\t\tunsigned long entry;\n\n\t\tif (is_elf2_task()) {\n\t\t\t/* Look ma, no function descriptors! */\n\t\t\tentry = start;\n\n\t\t\t/*\n\t\t\t * Ulrich says:\n\t\t\t *   The latest iteration of the ABI requires that when\n\t\t\t *   calling a function (at its global entry point),\n\t\t\t *   the caller must ensure r12 holds the entry point\n\t\t\t *   address (so that the function can quickly\n\t\t\t *   establish addressability).\n\t\t\t */\n\t\t\tregs->gpr[12] = start;\n\t\t\t/* Make sure that's restored on entry to userspace. */\n\t\t\tset_thread_flag(TIF_RESTOREALL);\n\t\t} else {\n\t\t\tunsigned long toc;\n\n\t\t\t/* start is a relocated pointer to the function\n\t\t\t * descriptor for the elf _start routine.  The first\n\t\t\t * entry in the function descriptor is the entry\n\t\t\t * address of _start and the second entry is the TOC\n\t\t\t * value we need to use.\n\t\t\t */\n\t\t\t__get_user(entry, (unsigned long __user *)start);\n\t\t\t__get_user(toc, (unsigned long __user *)start+1);\n\n\t\t\t/* Check whether the e_entry function descriptor entries\n\t\t\t * need to be relocated before we can use them.\n\t\t\t */\n\t\t\tif (load_addr != 0) {\n\t\t\t\tentry += load_addr;\n\t\t\t\ttoc   += load_addr;\n\t\t\t}\n\t\t\tregs->gpr[2] = toc;\n\t\t}\n\t\tregs->nip = entry;\n\t\tregs->msr = MSR_USER64;\n\t} else {\n\t\tregs->nip = start;\n\t\tregs->gpr[2] = 0;\n\t\tregs->msr = MSR_USER32;\n\t}\n#endif\n#ifdef CONFIG_VSX\n\tcurrent->thread.used_vsr = 0;\n#endif\n\tmemset(&current->thread.fp_state, 0, sizeof(current->thread.fp_state));\n\tcurrent->thread.fp_save_area = NULL;\n#ifdef CONFIG_ALTIVEC\n\tmemset(&current->thread.vr_state, 0, sizeof(current->thread.vr_state));\n\tcurrent->thread.vr_state.vscr.u[3] = 0x00010000; /* Java mode disabled */\n\tcurrent->thread.vr_save_area = NULL;\n\tcurrent->thread.vrsave = 0;\n\tcurrent->thread.used_vr = 0;\n#endif /* CONFIG_ALTIVEC */\n#ifdef CONFIG_SPE\n\tmemset(current->thread.evr, 0, sizeof(current->thread.evr));\n\tcurrent->thread.acc = 0;\n\tcurrent->thread.spefscr = 0;\n\tcurrent->thread.used_spe = 0;\n#endif /* CONFIG_SPE */\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n\tif (cpu_has_feature(CPU_FTR_TM))\n\t\tregs->msr |= MSR_TM;\n\tcurrent->thread.tm_tfhar = 0;\n\tcurrent->thread.tm_texasr = 0;\n\tcurrent->thread.tm_tfiar = 0;\n#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */\n}",
        "code_after_change": "void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)\n{\n#ifdef CONFIG_PPC64\n\tunsigned long load_addr = regs->gpr[2];\t/* saved by ELF_PLAT_INIT */\n#endif\n\n\t/*\n\t * If we exec out of a kernel thread then thread.regs will not be\n\t * set.  Do it now.\n\t */\n\tif (!current->thread.regs) {\n\t\tstruct pt_regs *regs = task_stack_page(current) + THREAD_SIZE;\n\t\tcurrent->thread.regs = regs - 1;\n\t}\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n\t/*\n\t * Clear any transactional state, we're exec()ing. The cause is\n\t * not important as there will never be a recheckpoint so it's not\n\t * user visible.\n\t */\n\tif (MSR_TM_SUSPENDED(mfmsr()))\n\t\ttm_reclaim_current(0);\n#endif\n\n\tmemset(regs->gpr, 0, sizeof(regs->gpr));\n\tregs->ctr = 0;\n\tregs->link = 0;\n\tregs->xer = 0;\n\tregs->ccr = 0;\n\tregs->gpr[1] = sp;\n\n\t/*\n\t * We have just cleared all the nonvolatile GPRs, so make\n\t * FULL_REGS(regs) return true.  This is necessary to allow\n\t * ptrace to examine the thread immediately after exec.\n\t */\n\tregs->trap &= ~1UL;\n\n#ifdef CONFIG_PPC32\n\tregs->mq = 0;\n\tregs->nip = start;\n\tregs->msr = MSR_USER;\n#else\n\tif (!is_32bit_task()) {\n\t\tunsigned long entry;\n\n\t\tif (is_elf2_task()) {\n\t\t\t/* Look ma, no function descriptors! */\n\t\t\tentry = start;\n\n\t\t\t/*\n\t\t\t * Ulrich says:\n\t\t\t *   The latest iteration of the ABI requires that when\n\t\t\t *   calling a function (at its global entry point),\n\t\t\t *   the caller must ensure r12 holds the entry point\n\t\t\t *   address (so that the function can quickly\n\t\t\t *   establish addressability).\n\t\t\t */\n\t\t\tregs->gpr[12] = start;\n\t\t\t/* Make sure that's restored on entry to userspace. */\n\t\t\tset_thread_flag(TIF_RESTOREALL);\n\t\t} else {\n\t\t\tunsigned long toc;\n\n\t\t\t/* start is a relocated pointer to the function\n\t\t\t * descriptor for the elf _start routine.  The first\n\t\t\t * entry in the function descriptor is the entry\n\t\t\t * address of _start and the second entry is the TOC\n\t\t\t * value we need to use.\n\t\t\t */\n\t\t\t__get_user(entry, (unsigned long __user *)start);\n\t\t\t__get_user(toc, (unsigned long __user *)start+1);\n\n\t\t\t/* Check whether the e_entry function descriptor entries\n\t\t\t * need to be relocated before we can use them.\n\t\t\t */\n\t\t\tif (load_addr != 0) {\n\t\t\t\tentry += load_addr;\n\t\t\t\ttoc   += load_addr;\n\t\t\t}\n\t\t\tregs->gpr[2] = toc;\n\t\t}\n\t\tregs->nip = entry;\n\t\tregs->msr = MSR_USER64;\n\t} else {\n\t\tregs->nip = start;\n\t\tregs->gpr[2] = 0;\n\t\tregs->msr = MSR_USER32;\n\t}\n#endif\n#ifdef CONFIG_VSX\n\tcurrent->thread.used_vsr = 0;\n#endif\n\tmemset(&current->thread.fp_state, 0, sizeof(current->thread.fp_state));\n\tcurrent->thread.fp_save_area = NULL;\n#ifdef CONFIG_ALTIVEC\n\tmemset(&current->thread.vr_state, 0, sizeof(current->thread.vr_state));\n\tcurrent->thread.vr_state.vscr.u[3] = 0x00010000; /* Java mode disabled */\n\tcurrent->thread.vr_save_area = NULL;\n\tcurrent->thread.vrsave = 0;\n\tcurrent->thread.used_vr = 0;\n#endif /* CONFIG_ALTIVEC */\n#ifdef CONFIG_SPE\n\tmemset(current->thread.evr, 0, sizeof(current->thread.evr));\n\tcurrent->thread.acc = 0;\n\tcurrent->thread.spefscr = 0;\n\tcurrent->thread.used_spe = 0;\n#endif /* CONFIG_SPE */\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n\tif (cpu_has_feature(CPU_FTR_TM))\n\t\tregs->msr |= MSR_TM;\n\tcurrent->thread.tm_tfhar = 0;\n\tcurrent->thread.tm_texasr = 0;\n\tcurrent->thread.tm_tfiar = 0;\n#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,16 @@\n \t\tstruct pt_regs *regs = task_stack_page(current) + THREAD_SIZE;\n \t\tcurrent->thread.regs = regs - 1;\n \t}\n+\n+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n+\t/*\n+\t * Clear any transactional state, we're exec()ing. The cause is\n+\t * not important as there will never be a recheckpoint so it's not\n+\t * user visible.\n+\t */\n+\tif (MSR_TM_SUSPENDED(mfmsr()))\n+\t\ttm_reclaim_current(0);\n+#endif\n \n \tmemset(regs->gpr, 0, sizeof(regs->gpr));\n \tregs->ctr = 0;",
        "function_modified_lines": {
            "added": [
                "",
                "#ifdef CONFIG_PPC_TRANSACTIONAL_MEM",
                "\t/*",
                "\t * Clear any transactional state, we're exec()ing. The cause is",
                "\t * not important as there will never be a recheckpoint so it's not",
                "\t * user visible.",
                "\t */",
                "\tif (MSR_TM_SUSPENDED(mfmsr()))",
                "\t\ttm_reclaim_current(0);",
                "#endif"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The start_thread function in arch/powerpc/kernel/process.c in the Linux kernel through 4.6.3 on powerpc platforms mishandles transactional state, which allows local users to cause a denial of service (invalid process state or TM Bad Thing exception, and system crash) or possibly have unspecified other impact by starting and suspending a transaction before an exec system call.",
        "id": 1057
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int ___sys_recvmsg(struct socket *sock, struct msghdr __user *msg,\n\t\t\t struct msghdr *msg_sys, unsigned int flags, int nosec)\n{\n\tstruct compat_msghdr __user *msg_compat =\n\t    (struct compat_msghdr __user *)msg;\n\tstruct iovec iovstack[UIO_FASTIOV];\n\tstruct iovec *iov = iovstack;\n\tunsigned long cmsg_ptr;\n\tint err, total_len, len;\n\n\t/* kernel mode address */\n\tstruct sockaddr_storage addr;\n\n\t/* user mode address pointers */\n\tstruct sockaddr __user *uaddr;\n\tint __user *uaddr_len;\n\n\tif (MSG_CMSG_COMPAT & flags) {\n\t\tif (get_compat_msghdr(msg_sys, msg_compat))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\terr = copy_msghdr_from_user(msg_sys, msg);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (msg_sys->msg_iovlen > UIO_FASTIOV) {\n\t\terr = -EMSGSIZE;\n\t\tif (msg_sys->msg_iovlen > UIO_MAXIOV)\n\t\t\tgoto out;\n\t\terr = -ENOMEM;\n\t\tiov = kmalloc(msg_sys->msg_iovlen * sizeof(struct iovec),\n\t\t\t      GFP_KERNEL);\n\t\tif (!iov)\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t *      Save the user-mode address (verify_iovec will change the\n\t *      kernel msghdr to use the kernel address space)\n\t */\n\n\tuaddr = (__force void __user *)msg_sys->msg_name;\n\tuaddr_len = COMPAT_NAMELEN(msg);\n\tif (MSG_CMSG_COMPAT & flags) {\n\t\terr = verify_compat_iovec(msg_sys, iov, &addr, VERIFY_WRITE);\n\t} else\n\t\terr = verify_iovec(msg_sys, iov, &addr, VERIFY_WRITE);\n\tif (err < 0)\n\t\tgoto out_freeiov;\n\ttotal_len = err;\n\n\tcmsg_ptr = (unsigned long)msg_sys->msg_control;\n\tmsg_sys->msg_flags = flags & (MSG_CMSG_CLOEXEC|MSG_CMSG_COMPAT);\n\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = (nosec ? sock_recvmsg_nosec : sock_recvmsg)(sock, msg_sys,\n\t\t\t\t\t\t\t  total_len, flags);\n\tif (err < 0)\n\t\tgoto out_freeiov;\n\tlen = err;\n\n\tif (uaddr != NULL) {\n\t\terr = move_addr_to_user(&addr,\n\t\t\t\t\tmsg_sys->msg_namelen, uaddr,\n\t\t\t\t\tuaddr_len);\n\t\tif (err < 0)\n\t\t\tgoto out_freeiov;\n\t}\n\terr = __put_user((msg_sys->msg_flags & ~MSG_CMSG_COMPAT),\n\t\t\t COMPAT_FLAGS(msg));\n\tif (err)\n\t\tgoto out_freeiov;\n\tif (MSG_CMSG_COMPAT & flags)\n\t\terr = __put_user((unsigned long)msg_sys->msg_control - cmsg_ptr,\n\t\t\t\t &msg_compat->msg_controllen);\n\telse\n\t\terr = __put_user((unsigned long)msg_sys->msg_control - cmsg_ptr,\n\t\t\t\t &msg->msg_controllen);\n\tif (err)\n\t\tgoto out_freeiov;\n\terr = len;\n\nout_freeiov:\n\tif (iov != iovstack)\n\t\tkfree(iov);\nout:\n\treturn err;\n}",
        "code_after_change": "static int ___sys_recvmsg(struct socket *sock, struct msghdr __user *msg,\n\t\t\t struct msghdr *msg_sys, unsigned int flags, int nosec)\n{\n\tstruct compat_msghdr __user *msg_compat =\n\t    (struct compat_msghdr __user *)msg;\n\tstruct iovec iovstack[UIO_FASTIOV];\n\tstruct iovec *iov = iovstack;\n\tunsigned long cmsg_ptr;\n\tint err, total_len, len;\n\n\t/* kernel mode address */\n\tstruct sockaddr_storage addr;\n\n\t/* user mode address pointers */\n\tstruct sockaddr __user *uaddr;\n\tint __user *uaddr_len;\n\n\tif (MSG_CMSG_COMPAT & flags) {\n\t\tif (get_compat_msghdr(msg_sys, msg_compat))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\terr = copy_msghdr_from_user(msg_sys, msg);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (msg_sys->msg_iovlen > UIO_FASTIOV) {\n\t\terr = -EMSGSIZE;\n\t\tif (msg_sys->msg_iovlen > UIO_MAXIOV)\n\t\t\tgoto out;\n\t\terr = -ENOMEM;\n\t\tiov = kmalloc(msg_sys->msg_iovlen * sizeof(struct iovec),\n\t\t\t      GFP_KERNEL);\n\t\tif (!iov)\n\t\t\tgoto out;\n\t}\n\n\t/* Save the user-mode address (verify_iovec will change the\n\t * kernel msghdr to use the kernel address space)\n\t */\n\tuaddr = (__force void __user *)msg_sys->msg_name;\n\tuaddr_len = COMPAT_NAMELEN(msg);\n\tif (MSG_CMSG_COMPAT & flags)\n\t\terr = verify_compat_iovec(msg_sys, iov, &addr, VERIFY_WRITE);\n\telse\n\t\terr = verify_iovec(msg_sys, iov, &addr, VERIFY_WRITE);\n\tif (err < 0)\n\t\tgoto out_freeiov;\n\ttotal_len = err;\n\n\tcmsg_ptr = (unsigned long)msg_sys->msg_control;\n\tmsg_sys->msg_flags = flags & (MSG_CMSG_CLOEXEC|MSG_CMSG_COMPAT);\n\n\t/* We assume all kernel code knows the size of sockaddr_storage */\n\tmsg_sys->msg_namelen = 0;\n\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = (nosec ? sock_recvmsg_nosec : sock_recvmsg)(sock, msg_sys,\n\t\t\t\t\t\t\t  total_len, flags);\n\tif (err < 0)\n\t\tgoto out_freeiov;\n\tlen = err;\n\n\tif (uaddr != NULL) {\n\t\terr = move_addr_to_user(&addr,\n\t\t\t\t\tmsg_sys->msg_namelen, uaddr,\n\t\t\t\t\tuaddr_len);\n\t\tif (err < 0)\n\t\t\tgoto out_freeiov;\n\t}\n\terr = __put_user((msg_sys->msg_flags & ~MSG_CMSG_COMPAT),\n\t\t\t COMPAT_FLAGS(msg));\n\tif (err)\n\t\tgoto out_freeiov;\n\tif (MSG_CMSG_COMPAT & flags)\n\t\terr = __put_user((unsigned long)msg_sys->msg_control - cmsg_ptr,\n\t\t\t\t &msg_compat->msg_controllen);\n\telse\n\t\terr = __put_user((unsigned long)msg_sys->msg_control - cmsg_ptr,\n\t\t\t\t &msg->msg_controllen);\n\tif (err)\n\t\tgoto out_freeiov;\n\terr = len;\n\nout_freeiov:\n\tif (iov != iovstack)\n\t\tkfree(iov);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -35,16 +35,14 @@\n \t\t\tgoto out;\n \t}\n \n-\t/*\n-\t *      Save the user-mode address (verify_iovec will change the\n-\t *      kernel msghdr to use the kernel address space)\n+\t/* Save the user-mode address (verify_iovec will change the\n+\t * kernel msghdr to use the kernel address space)\n \t */\n-\n \tuaddr = (__force void __user *)msg_sys->msg_name;\n \tuaddr_len = COMPAT_NAMELEN(msg);\n-\tif (MSG_CMSG_COMPAT & flags) {\n+\tif (MSG_CMSG_COMPAT & flags)\n \t\terr = verify_compat_iovec(msg_sys, iov, &addr, VERIFY_WRITE);\n-\t} else\n+\telse\n \t\terr = verify_iovec(msg_sys, iov, &addr, VERIFY_WRITE);\n \tif (err < 0)\n \t\tgoto out_freeiov;\n@@ -52,6 +50,9 @@\n \n \tcmsg_ptr = (unsigned long)msg_sys->msg_control;\n \tmsg_sys->msg_flags = flags & (MSG_CMSG_CLOEXEC|MSG_CMSG_COMPAT);\n+\n+\t/* We assume all kernel code knows the size of sockaddr_storage */\n+\tmsg_sys->msg_namelen = 0;\n \n \tif (sock->file->f_flags & O_NONBLOCK)\n \t\tflags |= MSG_DONTWAIT;",
        "function_modified_lines": {
            "added": [
                "\t/* Save the user-mode address (verify_iovec will change the",
                "\t * kernel msghdr to use the kernel address space)",
                "\tif (MSG_CMSG_COMPAT & flags)",
                "\telse",
                "",
                "\t/* We assume all kernel code knows the size of sockaddr_storage */",
                "\tmsg_sys->msg_namelen = 0;"
            ],
            "deleted": [
                "\t/*",
                "\t *      Save the user-mode address (verify_iovec will change the",
                "\t *      kernel msghdr to use the kernel address space)",
                "",
                "\tif (MSG_CMSG_COMPAT & flags) {",
                "\t} else"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 398
    },
    {
        "cve_id": "CVE-2019-9503",
        "code_before_change": "static void brcmf_msgbuf_process_event(struct brcmf_msgbuf *msgbuf, void *buf)\n{\n\tstruct msgbuf_rx_event *event;\n\tu32 idx;\n\tu16 buflen;\n\tstruct sk_buff *skb;\n\tstruct brcmf_if *ifp;\n\n\tevent = (struct msgbuf_rx_event *)buf;\n\tidx = le32_to_cpu(event->msg.request_id);\n\tbuflen = le16_to_cpu(event->event_data_len);\n\n\tif (msgbuf->cur_eventbuf)\n\t\tmsgbuf->cur_eventbuf--;\n\tbrcmf_msgbuf_rxbuf_event_post(msgbuf);\n\n\tskb = brcmf_msgbuf_get_pktid(msgbuf->drvr->bus_if->dev,\n\t\t\t\t     msgbuf->rx_pktids, idx);\n\tif (!skb)\n\t\treturn;\n\n\tif (msgbuf->rx_dataoffset)\n\t\tskb_pull(skb, msgbuf->rx_dataoffset);\n\n\tskb_trim(skb, buflen);\n\n\tifp = brcmf_get_ifp(msgbuf->drvr, event->msg.ifidx);\n\tif (!ifp || !ifp->ndev) {\n\t\tbrcmf_err(\"Received pkt for invalid ifidx %d\\n\",\n\t\t\t  event->msg.ifidx);\n\t\tgoto exit;\n\t}\n\n\tskb->protocol = eth_type_trans(skb, ifp->ndev);\n\n\tbrcmf_fweh_process_skb(ifp->drvr, skb);\n\nexit:\n\tbrcmu_pkt_buf_free_skb(skb);\n}",
        "code_after_change": "static void brcmf_msgbuf_process_event(struct brcmf_msgbuf *msgbuf, void *buf)\n{\n\tstruct msgbuf_rx_event *event;\n\tu32 idx;\n\tu16 buflen;\n\tstruct sk_buff *skb;\n\tstruct brcmf_if *ifp;\n\n\tevent = (struct msgbuf_rx_event *)buf;\n\tidx = le32_to_cpu(event->msg.request_id);\n\tbuflen = le16_to_cpu(event->event_data_len);\n\n\tif (msgbuf->cur_eventbuf)\n\t\tmsgbuf->cur_eventbuf--;\n\tbrcmf_msgbuf_rxbuf_event_post(msgbuf);\n\n\tskb = brcmf_msgbuf_get_pktid(msgbuf->drvr->bus_if->dev,\n\t\t\t\t     msgbuf->rx_pktids, idx);\n\tif (!skb)\n\t\treturn;\n\n\tif (msgbuf->rx_dataoffset)\n\t\tskb_pull(skb, msgbuf->rx_dataoffset);\n\n\tskb_trim(skb, buflen);\n\n\tifp = brcmf_get_ifp(msgbuf->drvr, event->msg.ifidx);\n\tif (!ifp || !ifp->ndev) {\n\t\tbrcmf_err(\"Received pkt for invalid ifidx %d\\n\",\n\t\t\t  event->msg.ifidx);\n\t\tgoto exit;\n\t}\n\n\tskb->protocol = eth_type_trans(skb, ifp->ndev);\n\n\tbrcmf_fweh_process_skb(ifp->drvr, skb, 0);\n\nexit:\n\tbrcmu_pkt_buf_free_skb(skb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -33,7 +33,7 @@\n \n \tskb->protocol = eth_type_trans(skb, ifp->ndev);\n \n-\tbrcmf_fweh_process_skb(ifp->drvr, skb);\n+\tbrcmf_fweh_process_skb(ifp->drvr, skb, 0);\n \n exit:\n \tbrcmu_pkt_buf_free_skb(skb);",
        "function_modified_lines": {
            "added": [
                "\tbrcmf_fweh_process_skb(ifp->drvr, skb, 0);"
            ],
            "deleted": [
                "\tbrcmf_fweh_process_skb(ifp->drvr, skb);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Broadcom brcmfmac WiFi driver prior to commit a4176ec356c73a46c07c181c6d04039fafa34a9f is vulnerable to a frame validation bypass. If the brcmfmac driver receives a firmware event frame from a remote source, the is_wlc_event_frame function will cause this frame to be discarded and unprocessed. If the driver receives the firmware event frame from the host, the appropriate handler is called. This frame validation can be bypassed if the bus used is USB (for instance by a wifi dongle). This can allow firmware event frames from a remote source to be processed. In the worst case scenario, by sending specially-crafted WiFi packets, a remote, unauthenticated attacker may be able to execute arbitrary code on a vulnerable system. More typically, this vulnerability will result in denial-of-service conditions.",
        "id": 2368
    },
    {
        "cve_id": "CVE-2013-7263",
        "code_before_change": "static int l2tp_ip_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t\t   size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}",
        "code_after_change": "static int l2tp_ip_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t\t   size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t*addr_len = sizeof(*sin);\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,9 +9,6 @@\n \n \tif (flags & MSG_OOB)\n \t\tgoto out;\n-\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(*sin);\n \n \tskb = skb_recv_datagram(sk, flags, noblock, &err);\n \tif (!skb)\n@@ -35,6 +32,7 @@\n \t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n \t\tsin->sin_port = 0;\n \t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n+\t\t*addr_len = sizeof(*sin);\n \t}\n \tif (inet->cmsg_flags)\n \t\tip_cmsg_recv(msg, skb);",
        "function_modified_lines": {
            "added": [
                "\t\t*addr_len = sizeof(*sin);"
            ],
            "deleted": [
                "",
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(*sin);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Linux kernel before 3.12.4 updates certain length values before ensuring that associated data structures have been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call, related to net/ipv4/ping.c, net/ipv4/raw.c, net/ipv4/udp.c, net/ipv6/raw.c, and net/ipv6/udp.c.",
        "id": 364
    },
    {
        "cve_id": "CVE-2018-5391",
        "code_before_change": "static int ip_frag_queue(struct ipq *qp, struct sk_buff *skb)\n{\n\tstruct sk_buff *prev, *next;\n\tstruct net_device *dev;\n\tunsigned int fragsize;\n\tint flags, offset;\n\tint ihl, end;\n\tint err = -ENOENT;\n\tu8 ecn;\n\n\tif (qp->q.flags & INET_FRAG_COMPLETE)\n\t\tgoto err;\n\n\tif (!(IPCB(skb)->flags & IPSKB_FRAG_COMPLETE) &&\n\t    unlikely(ip_frag_too_far(qp)) &&\n\t    unlikely(err = ip_frag_reinit(qp))) {\n\t\tipq_kill(qp);\n\t\tgoto err;\n\t}\n\n\tecn = ip4_frag_ecn(ip_hdr(skb)->tos);\n\toffset = ntohs(ip_hdr(skb)->frag_off);\n\tflags = offset & ~IP_OFFSET;\n\toffset &= IP_OFFSET;\n\toffset <<= 3;\t\t/* offset is in 8-byte chunks */\n\tihl = ip_hdrlen(skb);\n\n\t/* Determine the position of this fragment. */\n\tend = offset + skb->len - skb_network_offset(skb) - ihl;\n\terr = -EINVAL;\n\n\t/* Is this the final fragment? */\n\tif ((flags & IP_MF) == 0) {\n\t\t/* If we already have some bits beyond end\n\t\t * or have different end, the segment is corrupted.\n\t\t */\n\t\tif (end < qp->q.len ||\n\t\t    ((qp->q.flags & INET_FRAG_LAST_IN) && end != qp->q.len))\n\t\t\tgoto err;\n\t\tqp->q.flags |= INET_FRAG_LAST_IN;\n\t\tqp->q.len = end;\n\t} else {\n\t\tif (end&7) {\n\t\t\tend &= ~7;\n\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t\tif (end > qp->q.len) {\n\t\t\t/* Some bits beyond end -> corruption. */\n\t\t\tif (qp->q.flags & INET_FRAG_LAST_IN)\n\t\t\t\tgoto err;\n\t\t\tqp->q.len = end;\n\t\t}\n\t}\n\tif (end == offset)\n\t\tgoto err;\n\n\terr = -ENOMEM;\n\tif (!pskb_pull(skb, skb_network_offset(skb) + ihl))\n\t\tgoto err;\n\n\terr = pskb_trim_rcsum(skb, end - offset);\n\tif (err)\n\t\tgoto err;\n\n\t/* Find out which fragments are in front and at the back of us\n\t * in the chain of fragments so far.  We must know where to put\n\t * this fragment, right?\n\t */\n\tprev = qp->q.fragments_tail;\n\tif (!prev || prev->ip_defrag_offset < offset) {\n\t\tnext = NULL;\n\t\tgoto found;\n\t}\n\tprev = NULL;\n\tfor (next = qp->q.fragments; next != NULL; next = next->next) {\n\t\tif (next->ip_defrag_offset >= offset)\n\t\t\tbreak;\t/* bingo! */\n\t\tprev = next;\n\t}\n\nfound:\n\t/* We found where to put this one.  Check for overlap with\n\t * preceding fragment, and, if needed, align things so that\n\t * any overlaps are eliminated.\n\t */\n\tif (prev) {\n\t\tint i = (prev->ip_defrag_offset + prev->len) - offset;\n\n\t\tif (i > 0) {\n\t\t\toffset += i;\n\t\t\terr = -EINVAL;\n\t\t\tif (end <= offset)\n\t\t\t\tgoto err;\n\t\t\terr = -ENOMEM;\n\t\t\tif (!pskb_pull(skb, i))\n\t\t\t\tgoto err;\n\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t}\n\n\terr = -ENOMEM;\n\n\twhile (next && next->ip_defrag_offset < end) {\n\t\tint i = end - next->ip_defrag_offset; /* overlap is 'i' bytes */\n\n\t\tif (i < next->len) {\n\t\t\tint delta = -next->truesize;\n\n\t\t\t/* Eat head of the next overlapped fragment\n\t\t\t * and leave the loop. The next ones cannot overlap.\n\t\t\t */\n\t\t\tif (!pskb_pull(next, i))\n\t\t\t\tgoto err;\n\t\t\tdelta += next->truesize;\n\t\t\tif (delta)\n\t\t\t\tadd_frag_mem_limit(qp->q.net, delta);\n\t\t\tnext->ip_defrag_offset += i;\n\t\t\tqp->q.meat -= i;\n\t\t\tif (next->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tnext->ip_summed = CHECKSUM_NONE;\n\t\t\tbreak;\n\t\t} else {\n\t\t\tstruct sk_buff *free_it = next;\n\n\t\t\t/* Old fragment is completely overridden with\n\t\t\t * new one drop it.\n\t\t\t */\n\t\t\tnext = next->next;\n\n\t\t\tif (prev)\n\t\t\t\tprev->next = next;\n\t\t\telse\n\t\t\t\tqp->q.fragments = next;\n\n\t\t\tqp->q.meat -= free_it->len;\n\t\t\tsub_frag_mem_limit(qp->q.net, free_it->truesize);\n\t\t\tkfree_skb(free_it);\n\t\t}\n\t}\n\n\t/* Note : skb->ip_defrag_offset and skb->dev share the same location */\n\tdev = skb->dev;\n\tif (dev)\n\t\tqp->iif = dev->ifindex;\n\t/* Makes sure compiler wont do silly aliasing games */\n\tbarrier();\n\tskb->ip_defrag_offset = offset;\n\n\t/* Insert this fragment in the chain of fragments. */\n\tskb->next = next;\n\tif (!next)\n\t\tqp->q.fragments_tail = skb;\n\tif (prev)\n\t\tprev->next = skb;\n\telse\n\t\tqp->q.fragments = skb;\n\n\tqp->q.stamp = skb->tstamp;\n\tqp->q.meat += skb->len;\n\tqp->ecn |= ecn;\n\tadd_frag_mem_limit(qp->q.net, skb->truesize);\n\tif (offset == 0)\n\t\tqp->q.flags |= INET_FRAG_FIRST_IN;\n\n\tfragsize = skb->len + ihl;\n\n\tif (fragsize > qp->q.max_size)\n\t\tqp->q.max_size = fragsize;\n\n\tif (ip_hdr(skb)->frag_off & htons(IP_DF) &&\n\t    fragsize > qp->max_df_size)\n\t\tqp->max_df_size = fragsize;\n\n\tif (qp->q.flags == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\n\t    qp->q.meat == qp->q.len) {\n\t\tunsigned long orefdst = skb->_skb_refdst;\n\n\t\tskb->_skb_refdst = 0UL;\n\t\terr = ip_frag_reasm(qp, prev, dev);\n\t\tskb->_skb_refdst = orefdst;\n\t\treturn err;\n\t}\n\n\tskb_dst_drop(skb);\n\treturn -EINPROGRESS;\n\nerr:\n\tkfree_skb(skb);\n\treturn err;\n}",
        "code_after_change": "static int ip_frag_queue(struct ipq *qp, struct sk_buff *skb)\n{\n\tstruct net *net = container_of(qp->q.net, struct net, ipv4.frags);\n\tstruct sk_buff *prev, *next;\n\tstruct net_device *dev;\n\tunsigned int fragsize;\n\tint flags, offset;\n\tint ihl, end;\n\tint err = -ENOENT;\n\tu8 ecn;\n\n\tif (qp->q.flags & INET_FRAG_COMPLETE)\n\t\tgoto err;\n\n\tif (!(IPCB(skb)->flags & IPSKB_FRAG_COMPLETE) &&\n\t    unlikely(ip_frag_too_far(qp)) &&\n\t    unlikely(err = ip_frag_reinit(qp))) {\n\t\tipq_kill(qp);\n\t\tgoto err;\n\t}\n\n\tecn = ip4_frag_ecn(ip_hdr(skb)->tos);\n\toffset = ntohs(ip_hdr(skb)->frag_off);\n\tflags = offset & ~IP_OFFSET;\n\toffset &= IP_OFFSET;\n\toffset <<= 3;\t\t/* offset is in 8-byte chunks */\n\tihl = ip_hdrlen(skb);\n\n\t/* Determine the position of this fragment. */\n\tend = offset + skb->len - skb_network_offset(skb) - ihl;\n\terr = -EINVAL;\n\n\t/* Is this the final fragment? */\n\tif ((flags & IP_MF) == 0) {\n\t\t/* If we already have some bits beyond end\n\t\t * or have different end, the segment is corrupted.\n\t\t */\n\t\tif (end < qp->q.len ||\n\t\t    ((qp->q.flags & INET_FRAG_LAST_IN) && end != qp->q.len))\n\t\t\tgoto err;\n\t\tqp->q.flags |= INET_FRAG_LAST_IN;\n\t\tqp->q.len = end;\n\t} else {\n\t\tif (end&7) {\n\t\t\tend &= ~7;\n\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t\tif (end > qp->q.len) {\n\t\t\t/* Some bits beyond end -> corruption. */\n\t\t\tif (qp->q.flags & INET_FRAG_LAST_IN)\n\t\t\t\tgoto err;\n\t\t\tqp->q.len = end;\n\t\t}\n\t}\n\tif (end == offset)\n\t\tgoto err;\n\n\terr = -ENOMEM;\n\tif (!pskb_pull(skb, skb_network_offset(skb) + ihl))\n\t\tgoto err;\n\n\terr = pskb_trim_rcsum(skb, end - offset);\n\tif (err)\n\t\tgoto err;\n\n\t/* Find out which fragments are in front and at the back of us\n\t * in the chain of fragments so far.  We must know where to put\n\t * this fragment, right?\n\t */\n\tprev = qp->q.fragments_tail;\n\tif (!prev || prev->ip_defrag_offset < offset) {\n\t\tnext = NULL;\n\t\tgoto found;\n\t}\n\tprev = NULL;\n\tfor (next = qp->q.fragments; next != NULL; next = next->next) {\n\t\tif (next->ip_defrag_offset >= offset)\n\t\t\tbreak;\t/* bingo! */\n\t\tprev = next;\n\t}\n\nfound:\n\t/* RFC5722, Section 4, amended by Errata ID : 3089\n\t *                          When reassembling an IPv6 datagram, if\n\t *   one or more its constituent fragments is determined to be an\n\t *   overlapping fragment, the entire datagram (and any constituent\n\t *   fragments) MUST be silently discarded.\n\t *\n\t * We do the same here for IPv4.\n\t */\n\n\t/* Is there an overlap with the previous fragment? */\n\tif (prev &&\n\t    (prev->ip_defrag_offset + prev->len) > offset)\n\t\tgoto discard_qp;\n\n\t/* Is there an overlap with the next fragment? */\n\tif (next && next->ip_defrag_offset < end)\n\t\tgoto discard_qp;\n\n\t/* Note : skb->ip_defrag_offset and skb->dev share the same location */\n\tdev = skb->dev;\n\tif (dev)\n\t\tqp->iif = dev->ifindex;\n\t/* Makes sure compiler wont do silly aliasing games */\n\tbarrier();\n\tskb->ip_defrag_offset = offset;\n\n\t/* Insert this fragment in the chain of fragments. */\n\tskb->next = next;\n\tif (!next)\n\t\tqp->q.fragments_tail = skb;\n\tif (prev)\n\t\tprev->next = skb;\n\telse\n\t\tqp->q.fragments = skb;\n\n\tqp->q.stamp = skb->tstamp;\n\tqp->q.meat += skb->len;\n\tqp->ecn |= ecn;\n\tadd_frag_mem_limit(qp->q.net, skb->truesize);\n\tif (offset == 0)\n\t\tqp->q.flags |= INET_FRAG_FIRST_IN;\n\n\tfragsize = skb->len + ihl;\n\n\tif (fragsize > qp->q.max_size)\n\t\tqp->q.max_size = fragsize;\n\n\tif (ip_hdr(skb)->frag_off & htons(IP_DF) &&\n\t    fragsize > qp->max_df_size)\n\t\tqp->max_df_size = fragsize;\n\n\tif (qp->q.flags == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\n\t    qp->q.meat == qp->q.len) {\n\t\tunsigned long orefdst = skb->_skb_refdst;\n\n\t\tskb->_skb_refdst = 0UL;\n\t\terr = ip_frag_reasm(qp, prev, dev);\n\t\tskb->_skb_refdst = orefdst;\n\t\treturn err;\n\t}\n\n\tskb_dst_drop(skb);\n\treturn -EINPROGRESS;\n\ndiscard_qp:\n\tinet_frag_kill(&qp->q);\n\terr = -EINVAL;\n\t__IP_INC_STATS(net, IPSTATS_MIB_REASM_OVERLAPS);\nerr:\n\tkfree_skb(skb);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n static int ip_frag_queue(struct ipq *qp, struct sk_buff *skb)\n {\n+\tstruct net *net = container_of(qp->q.net, struct net, ipv4.frags);\n \tstruct sk_buff *prev, *next;\n \tstruct net_device *dev;\n \tunsigned int fragsize;\n@@ -80,65 +81,23 @@\n \t}\n \n found:\n-\t/* We found where to put this one.  Check for overlap with\n-\t * preceding fragment, and, if needed, align things so that\n-\t * any overlaps are eliminated.\n+\t/* RFC5722, Section 4, amended by Errata ID : 3089\n+\t *                          When reassembling an IPv6 datagram, if\n+\t *   one or more its constituent fragments is determined to be an\n+\t *   overlapping fragment, the entire datagram (and any constituent\n+\t *   fragments) MUST be silently discarded.\n+\t *\n+\t * We do the same here for IPv4.\n \t */\n-\tif (prev) {\n-\t\tint i = (prev->ip_defrag_offset + prev->len) - offset;\n \n-\t\tif (i > 0) {\n-\t\t\toffset += i;\n-\t\t\terr = -EINVAL;\n-\t\t\tif (end <= offset)\n-\t\t\t\tgoto err;\n-\t\t\terr = -ENOMEM;\n-\t\t\tif (!pskb_pull(skb, i))\n-\t\t\t\tgoto err;\n-\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n-\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n-\t\t}\n-\t}\n+\t/* Is there an overlap with the previous fragment? */\n+\tif (prev &&\n+\t    (prev->ip_defrag_offset + prev->len) > offset)\n+\t\tgoto discard_qp;\n \n-\terr = -ENOMEM;\n-\n-\twhile (next && next->ip_defrag_offset < end) {\n-\t\tint i = end - next->ip_defrag_offset; /* overlap is 'i' bytes */\n-\n-\t\tif (i < next->len) {\n-\t\t\tint delta = -next->truesize;\n-\n-\t\t\t/* Eat head of the next overlapped fragment\n-\t\t\t * and leave the loop. The next ones cannot overlap.\n-\t\t\t */\n-\t\t\tif (!pskb_pull(next, i))\n-\t\t\t\tgoto err;\n-\t\t\tdelta += next->truesize;\n-\t\t\tif (delta)\n-\t\t\t\tadd_frag_mem_limit(qp->q.net, delta);\n-\t\t\tnext->ip_defrag_offset += i;\n-\t\t\tqp->q.meat -= i;\n-\t\t\tif (next->ip_summed != CHECKSUM_UNNECESSARY)\n-\t\t\t\tnext->ip_summed = CHECKSUM_NONE;\n-\t\t\tbreak;\n-\t\t} else {\n-\t\t\tstruct sk_buff *free_it = next;\n-\n-\t\t\t/* Old fragment is completely overridden with\n-\t\t\t * new one drop it.\n-\t\t\t */\n-\t\t\tnext = next->next;\n-\n-\t\t\tif (prev)\n-\t\t\t\tprev->next = next;\n-\t\t\telse\n-\t\t\t\tqp->q.fragments = next;\n-\n-\t\t\tqp->q.meat -= free_it->len;\n-\t\t\tsub_frag_mem_limit(qp->q.net, free_it->truesize);\n-\t\t\tkfree_skb(free_it);\n-\t\t}\n-\t}\n+\t/* Is there an overlap with the next fragment? */\n+\tif (next && next->ip_defrag_offset < end)\n+\t\tgoto discard_qp;\n \n \t/* Note : skb->ip_defrag_offset and skb->dev share the same location */\n \tdev = skb->dev;\n@@ -186,6 +145,10 @@\n \tskb_dst_drop(skb);\n \treturn -EINPROGRESS;\n \n+discard_qp:\n+\tinet_frag_kill(&qp->q);\n+\terr = -EINVAL;\n+\t__IP_INC_STATS(net, IPSTATS_MIB_REASM_OVERLAPS);\n err:\n \tkfree_skb(skb);\n \treturn err;",
        "function_modified_lines": {
            "added": [
                "\tstruct net *net = container_of(qp->q.net, struct net, ipv4.frags);",
                "\t/* RFC5722, Section 4, amended by Errata ID : 3089",
                "\t *                          When reassembling an IPv6 datagram, if",
                "\t *   one or more its constituent fragments is determined to be an",
                "\t *   overlapping fragment, the entire datagram (and any constituent",
                "\t *   fragments) MUST be silently discarded.",
                "\t *",
                "\t * We do the same here for IPv4.",
                "\t/* Is there an overlap with the previous fragment? */",
                "\tif (prev &&",
                "\t    (prev->ip_defrag_offset + prev->len) > offset)",
                "\t\tgoto discard_qp;",
                "\t/* Is there an overlap with the next fragment? */",
                "\tif (next && next->ip_defrag_offset < end)",
                "\t\tgoto discard_qp;",
                "discard_qp:",
                "\tinet_frag_kill(&qp->q);",
                "\terr = -EINVAL;",
                "\t__IP_INC_STATS(net, IPSTATS_MIB_REASM_OVERLAPS);"
            ],
            "deleted": [
                "\t/* We found where to put this one.  Check for overlap with",
                "\t * preceding fragment, and, if needed, align things so that",
                "\t * any overlaps are eliminated.",
                "\tif (prev) {",
                "\t\tint i = (prev->ip_defrag_offset + prev->len) - offset;",
                "\t\tif (i > 0) {",
                "\t\t\toffset += i;",
                "\t\t\terr = -EINVAL;",
                "\t\t\tif (end <= offset)",
                "\t\t\t\tgoto err;",
                "\t\t\terr = -ENOMEM;",
                "\t\t\tif (!pskb_pull(skb, i))",
                "\t\t\t\tgoto err;",
                "\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)",
                "\t\t\t\tskb->ip_summed = CHECKSUM_NONE;",
                "\t\t}",
                "\t}",
                "\terr = -ENOMEM;",
                "",
                "\twhile (next && next->ip_defrag_offset < end) {",
                "\t\tint i = end - next->ip_defrag_offset; /* overlap is 'i' bytes */",
                "",
                "\t\tif (i < next->len) {",
                "\t\t\tint delta = -next->truesize;",
                "",
                "\t\t\t/* Eat head of the next overlapped fragment",
                "\t\t\t * and leave the loop. The next ones cannot overlap.",
                "\t\t\t */",
                "\t\t\tif (!pskb_pull(next, i))",
                "\t\t\t\tgoto err;",
                "\t\t\tdelta += next->truesize;",
                "\t\t\tif (delta)",
                "\t\t\t\tadd_frag_mem_limit(qp->q.net, delta);",
                "\t\t\tnext->ip_defrag_offset += i;",
                "\t\t\tqp->q.meat -= i;",
                "\t\t\tif (next->ip_summed != CHECKSUM_UNNECESSARY)",
                "\t\t\t\tnext->ip_summed = CHECKSUM_NONE;",
                "\t\t\tbreak;",
                "\t\t} else {",
                "\t\t\tstruct sk_buff *free_it = next;",
                "",
                "\t\t\t/* Old fragment is completely overridden with",
                "\t\t\t * new one drop it.",
                "\t\t\t */",
                "\t\t\tnext = next->next;",
                "",
                "\t\t\tif (prev)",
                "\t\t\t\tprev->next = next;",
                "\t\t\telse",
                "\t\t\t\tqp->q.fragments = next;",
                "",
                "\t\t\tqp->q.meat -= free_it->len;",
                "\t\t\tsub_frag_mem_limit(qp->q.net, free_it->truesize);",
                "\t\t\tkfree_skb(free_it);",
                "\t\t}",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Linux kernel, versions 3.9+, is vulnerable to a denial of service attack with low rates of specially modified packets targeting IP fragment re-assembly. An attacker may cause a denial of service condition by sending specially crafted IP fragments. Various vulnerabilities in IP fragmentation have been discovered and fixed over the years. The current vulnerability (CVE-2018-5391) became exploitable in the Linux kernel with the increase of the IP fragment reassembly queue size.",
        "id": 1823
    },
    {
        "cve_id": "CVE-2020-25643",
        "code_before_change": "static void ppp_cp_parse_cr(struct net_device *dev, u16 pid, u8 id,\n\t\t\t    unsigned int req_len, const u8 *data)\n{\n\tstatic u8 const valid_accm[6] = { LCP_OPTION_ACCM, 6, 0, 0, 0, 0 };\n\tconst u8 *opt;\n\tu8 *out;\n\tunsigned int len = req_len, nak_len = 0, rej_len = 0;\n\n\tif (!(out = kmalloc(len, GFP_ATOMIC))) {\n\t\tdev->stats.rx_dropped++;\n\t\treturn;\t/* out of memory, ignore CR packet */\n\t}\n\n\tfor (opt = data; len; len -= opt[1], opt += opt[1]) {\n\t\tif (len < 2 || len < opt[1]) {\n\t\t\tdev->stats.rx_errors++;\n\t\t\tkfree(out);\n\t\t\treturn; /* bad packet, drop silently */\n\t\t}\n\n\t\tif (pid == PID_LCP)\n\t\t\tswitch (opt[0]) {\n\t\t\tcase LCP_OPTION_MRU:\n\t\t\t\tcontinue; /* MRU always OK and > 1500 bytes? */\n\n\t\t\tcase LCP_OPTION_ACCM: /* async control character map */\n\t\t\t\tif (!memcmp(opt, valid_accm,\n\t\t\t\t\t    sizeof(valid_accm)))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!rej_len) { /* NAK it */\n\t\t\t\t\tmemcpy(out + nak_len, valid_accm,\n\t\t\t\t\t       sizeof(valid_accm));\n\t\t\t\t\tnak_len += sizeof(valid_accm);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase LCP_OPTION_MAGIC:\n\t\t\t\tif (opt[1] != 6 || (!opt[2] && !opt[3] &&\n\t\t\t\t\t\t    !opt[4] && !opt[5]))\n\t\t\t\t\tbreak; /* reject invalid magic number */\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t/* reject this option */\n\t\tmemcpy(out + rej_len, opt, opt[1]);\n\t\trej_len += opt[1];\n\t}\n\n\tif (rej_len)\n\t\tppp_cp_event(dev, pid, RCR_BAD, CP_CONF_REJ, id, rej_len, out);\n\telse if (nak_len)\n\t\tppp_cp_event(dev, pid, RCR_BAD, CP_CONF_NAK, id, nak_len, out);\n\telse\n\t\tppp_cp_event(dev, pid, RCR_GOOD, CP_CONF_ACK, id, req_len, data);\n\n\tkfree(out);\n}",
        "code_after_change": "static void ppp_cp_parse_cr(struct net_device *dev, u16 pid, u8 id,\n\t\t\t    unsigned int req_len, const u8 *data)\n{\n\tstatic u8 const valid_accm[6] = { LCP_OPTION_ACCM, 6, 0, 0, 0, 0 };\n\tconst u8 *opt;\n\tu8 *out;\n\tunsigned int len = req_len, nak_len = 0, rej_len = 0;\n\n\tif (!(out = kmalloc(len, GFP_ATOMIC))) {\n\t\tdev->stats.rx_dropped++;\n\t\treturn;\t/* out of memory, ignore CR packet */\n\t}\n\n\tfor (opt = data; len; len -= opt[1], opt += opt[1]) {\n\t\tif (len < 2 || opt[1] < 2 || len < opt[1])\n\t\t\tgoto err_out;\n\n\t\tif (pid == PID_LCP)\n\t\t\tswitch (opt[0]) {\n\t\t\tcase LCP_OPTION_MRU:\n\t\t\t\tcontinue; /* MRU always OK and > 1500 bytes? */\n\n\t\t\tcase LCP_OPTION_ACCM: /* async control character map */\n\t\t\t\tif (opt[1] < sizeof(valid_accm))\n\t\t\t\t\tgoto err_out;\n\t\t\t\tif (!memcmp(opt, valid_accm,\n\t\t\t\t\t    sizeof(valid_accm)))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!rej_len) { /* NAK it */\n\t\t\t\t\tmemcpy(out + nak_len, valid_accm,\n\t\t\t\t\t       sizeof(valid_accm));\n\t\t\t\t\tnak_len += sizeof(valid_accm);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase LCP_OPTION_MAGIC:\n\t\t\t\tif (len < 6)\n\t\t\t\t\tgoto err_out;\n\t\t\t\tif (opt[1] != 6 || (!opt[2] && !opt[3] &&\n\t\t\t\t\t\t    !opt[4] && !opt[5]))\n\t\t\t\t\tbreak; /* reject invalid magic number */\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t/* reject this option */\n\t\tmemcpy(out + rej_len, opt, opt[1]);\n\t\trej_len += opt[1];\n\t}\n\n\tif (rej_len)\n\t\tppp_cp_event(dev, pid, RCR_BAD, CP_CONF_REJ, id, rej_len, out);\n\telse if (nak_len)\n\t\tppp_cp_event(dev, pid, RCR_BAD, CP_CONF_NAK, id, nak_len, out);\n\telse\n\t\tppp_cp_event(dev, pid, RCR_GOOD, CP_CONF_ACK, id, req_len, data);\n\n\tkfree(out);\n\treturn;\n\nerr_out:\n\tdev->stats.rx_errors++;\n\tkfree(out);\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,11 +12,8 @@\n \t}\n \n \tfor (opt = data; len; len -= opt[1], opt += opt[1]) {\n-\t\tif (len < 2 || len < opt[1]) {\n-\t\t\tdev->stats.rx_errors++;\n-\t\t\tkfree(out);\n-\t\t\treturn; /* bad packet, drop silently */\n-\t\t}\n+\t\tif (len < 2 || opt[1] < 2 || len < opt[1])\n+\t\t\tgoto err_out;\n \n \t\tif (pid == PID_LCP)\n \t\t\tswitch (opt[0]) {\n@@ -24,6 +21,8 @@\n \t\t\t\tcontinue; /* MRU always OK and > 1500 bytes? */\n \n \t\t\tcase LCP_OPTION_ACCM: /* async control character map */\n+\t\t\t\tif (opt[1] < sizeof(valid_accm))\n+\t\t\t\t\tgoto err_out;\n \t\t\t\tif (!memcmp(opt, valid_accm,\n \t\t\t\t\t    sizeof(valid_accm)))\n \t\t\t\t\tcontinue;\n@@ -35,6 +34,8 @@\n \t\t\t\t}\n \t\t\t\tbreak;\n \t\t\tcase LCP_OPTION_MAGIC:\n+\t\t\t\tif (len < 6)\n+\t\t\t\t\tgoto err_out;\n \t\t\t\tif (opt[1] != 6 || (!opt[2] && !opt[3] &&\n \t\t\t\t\t\t    !opt[4] && !opt[5]))\n \t\t\t\t\tbreak; /* reject invalid magic number */\n@@ -53,4 +54,9 @@\n \t\tppp_cp_event(dev, pid, RCR_GOOD, CP_CONF_ACK, id, req_len, data);\n \n \tkfree(out);\n+\treturn;\n+\n+err_out:\n+\tdev->stats.rx_errors++;\n+\tkfree(out);\n }",
        "function_modified_lines": {
            "added": [
                "\t\tif (len < 2 || opt[1] < 2 || len < opt[1])",
                "\t\t\tgoto err_out;",
                "\t\t\t\tif (opt[1] < sizeof(valid_accm))",
                "\t\t\t\t\tgoto err_out;",
                "\t\t\t\tif (len < 6)",
                "\t\t\t\t\tgoto err_out;",
                "\treturn;",
                "",
                "err_out:",
                "\tdev->stats.rx_errors++;",
                "\tkfree(out);"
            ],
            "deleted": [
                "\t\tif (len < 2 || len < opt[1]) {",
                "\t\t\tdev->stats.rx_errors++;",
                "\t\t\tkfree(out);",
                "\t\t\treturn; /* bad packet, drop silently */",
                "\t\t}"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A flaw was found in the HDLC_PPP module of the Linux kernel in versions before 5.9-rc7. Memory corruption and a read overflow is caused by improper input validation in the ppp_cp_parse_cr function which can cause the system to crash or cause a denial of service. The highest threat from this vulnerability is to data confidentiality and integrity as well as system availability.",
        "id": 2589
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static bool io_wq_files_match(struct io_wq_work *work, void *data)\n{\n\tstruct files_struct *files = data;\n\n\treturn work->files == files;\n}",
        "code_after_change": "static bool io_wq_files_match(struct io_wq_work *work, void *data)\n{\n\tstruct files_struct *files = data;\n\n\treturn !files || work->files == files;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,5 +2,5 @@\n {\n \tstruct files_struct *files = data;\n \n-\treturn work->files == files;\n+\treturn !files || work->files == files;\n }",
        "function_modified_lines": {
            "added": [
                "\treturn !files || work->files == files;"
            ],
            "deleted": [
                "\treturn work->files == files;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2846
    },
    {
        "cve_id": "CVE-2017-18221",
        "code_before_change": "static void __munlock_pagevec(struct pagevec *pvec, struct zone *zone)\n{\n\tint i;\n\tint nr = pagevec_count(pvec);\n\tint delta_munlocked;\n\tstruct pagevec pvec_putback;\n\tint pgrescued = 0;\n\n\tpagevec_init(&pvec_putback, 0);\n\n\t/* Phase 1: page isolation */\n\tspin_lock_irq(zone_lru_lock(zone));\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (TestClearPageMlocked(page)) {\n\t\t\t/*\n\t\t\t * We already have pin from follow_page_mask()\n\t\t\t * so we can spare the get_page() here.\n\t\t\t */\n\t\t\tif (__munlock_isolate_lru_page(page, false))\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\t__munlock_isolation_failed(page);\n\t\t}\n\n\t\t/*\n\t\t * We won't be munlocking this page in the next phase\n\t\t * but we still need to release the follow_page_mask()\n\t\t * pin. We cannot do it under lru_lock however. If it's\n\t\t * the last pin, __page_cache_release() would deadlock.\n\t\t */\n\t\tpagevec_add(&pvec_putback, pvec->pages[i]);\n\t\tpvec->pages[i] = NULL;\n\t}\n\tdelta_munlocked = -nr + pagevec_count(&pvec_putback);\n\t__mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);\n\tspin_unlock_irq(zone_lru_lock(zone));\n\n\t/* Now we can release pins of pages that we are not munlocking */\n\tpagevec_release(&pvec_putback);\n\n\t/* Phase 2: page munlock */\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (page) {\n\t\t\tlock_page(page);\n\t\t\tif (!__putback_lru_fast_prepare(page, &pvec_putback,\n\t\t\t\t\t&pgrescued)) {\n\t\t\t\t/*\n\t\t\t\t * Slow path. We don't want to lose the last\n\t\t\t\t * pin before unlock_page()\n\t\t\t\t */\n\t\t\t\tget_page(page); /* for putback_lru_page() */\n\t\t\t\t__munlock_isolated_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* from follow_page_mask() */\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Phase 3: page putback for pages that qualified for the fast path\n\t * This will also call put_page() to return pin from follow_page_mask()\n\t */\n\tif (pagevec_count(&pvec_putback))\n\t\t__putback_lru_fast(&pvec_putback, pgrescued);\n}",
        "code_after_change": "static void __munlock_pagevec(struct pagevec *pvec, struct zone *zone)\n{\n\tint i;\n\tint nr = pagevec_count(pvec);\n\tint delta_munlocked = -nr;\n\tstruct pagevec pvec_putback;\n\tint pgrescued = 0;\n\n\tpagevec_init(&pvec_putback, 0);\n\n\t/* Phase 1: page isolation */\n\tspin_lock_irq(zone_lru_lock(zone));\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (TestClearPageMlocked(page)) {\n\t\t\t/*\n\t\t\t * We already have pin from follow_page_mask()\n\t\t\t * so we can spare the get_page() here.\n\t\t\t */\n\t\t\tif (__munlock_isolate_lru_page(page, false))\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\t__munlock_isolation_failed(page);\n\t\t} else {\n\t\t\tdelta_munlocked++;\n\t\t}\n\n\t\t/*\n\t\t * We won't be munlocking this page in the next phase\n\t\t * but we still need to release the follow_page_mask()\n\t\t * pin. We cannot do it under lru_lock however. If it's\n\t\t * the last pin, __page_cache_release() would deadlock.\n\t\t */\n\t\tpagevec_add(&pvec_putback, pvec->pages[i]);\n\t\tpvec->pages[i] = NULL;\n\t}\n\t__mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);\n\tspin_unlock_irq(zone_lru_lock(zone));\n\n\t/* Now we can release pins of pages that we are not munlocking */\n\tpagevec_release(&pvec_putback);\n\n\t/* Phase 2: page munlock */\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (page) {\n\t\t\tlock_page(page);\n\t\t\tif (!__putback_lru_fast_prepare(page, &pvec_putback,\n\t\t\t\t\t&pgrescued)) {\n\t\t\t\t/*\n\t\t\t\t * Slow path. We don't want to lose the last\n\t\t\t\t * pin before unlock_page()\n\t\t\t\t */\n\t\t\t\tget_page(page); /* for putback_lru_page() */\n\t\t\t\t__munlock_isolated_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* from follow_page_mask() */\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Phase 3: page putback for pages that qualified for the fast path\n\t * This will also call put_page() to return pin from follow_page_mask()\n\t */\n\tif (pagevec_count(&pvec_putback))\n\t\t__putback_lru_fast(&pvec_putback, pgrescued);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tint i;\n \tint nr = pagevec_count(pvec);\n-\tint delta_munlocked;\n+\tint delta_munlocked = -nr;\n \tstruct pagevec pvec_putback;\n \tint pgrescued = 0;\n \n@@ -22,6 +22,8 @@\n \t\t\t\tcontinue;\n \t\t\telse\n \t\t\t\t__munlock_isolation_failed(page);\n+\t\t} else {\n+\t\t\tdelta_munlocked++;\n \t\t}\n \n \t\t/*\n@@ -33,7 +35,6 @@\n \t\tpagevec_add(&pvec_putback, pvec->pages[i]);\n \t\tpvec->pages[i] = NULL;\n \t}\n-\tdelta_munlocked = -nr + pagevec_count(&pvec_putback);\n \t__mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);\n \tspin_unlock_irq(zone_lru_lock(zone));\n ",
        "function_modified_lines": {
            "added": [
                "\tint delta_munlocked = -nr;",
                "\t\t} else {",
                "\t\t\tdelta_munlocked++;"
            ],
            "deleted": [
                "\tint delta_munlocked;",
                "\tdelta_munlocked = -nr + pagevec_count(&pvec_putback);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The __munlock_pagevec function in mm/mlock.c in the Linux kernel before 4.11.4 allows local users to cause a denial of service (NR_MLOCK accounting corruption) via crafted use of mlockall and munlockall system calls.",
        "id": 1406
    },
    {
        "cve_id": "CVE-2017-14489",
        "code_before_change": "static void\niscsi_if_rx(struct sk_buff *skb)\n{\n\tmutex_lock(&rx_queue_mutex);\n\twhile (skb->len >= NLMSG_HDRLEN) {\n\t\tint err;\n\t\tuint32_t rlen;\n\t\tstruct nlmsghdr\t*nlh;\n\t\tstruct iscsi_uevent *ev;\n\t\tuint32_t group;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\tif (nlh->nlmsg_len < sizeof(*nlh) ||\n\t\t    skb->len < nlh->nlmsg_len) {\n\t\t\tbreak;\n\t\t}\n\n\t\tev = nlmsg_data(nlh);\n\t\trlen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (rlen > skb->len)\n\t\t\trlen = skb->len;\n\n\t\terr = iscsi_if_recv_msg(skb, nlh, &group);\n\t\tif (err) {\n\t\t\tev->type = ISCSI_KEVENT_IF_ERROR;\n\t\t\tev->iferror = err;\n\t\t}\n\t\tdo {\n\t\t\t/*\n\t\t\t * special case for GET_STATS:\n\t\t\t * on success - sending reply and stats from\n\t\t\t * inside of if_recv_msg(),\n\t\t\t * on error - fall through.\n\t\t\t */\n\t\t\tif (ev->type == ISCSI_UEVENT_GET_STATS && !err)\n\t\t\t\tbreak;\n\t\t\tif (ev->type == ISCSI_UEVENT_GET_CHAP && !err)\n\t\t\t\tbreak;\n\t\t\terr = iscsi_if_send_reply(group, nlh->nlmsg_seq,\n\t\t\t\tnlh->nlmsg_type, 0, 0, ev, sizeof(*ev));\n\t\t} while (err < 0 && err != -ECONNREFUSED && err != -ESRCH);\n\t\tskb_pull(skb, rlen);\n\t}\n\tmutex_unlock(&rx_queue_mutex);\n}",
        "code_after_change": "static void\niscsi_if_rx(struct sk_buff *skb)\n{\n\tmutex_lock(&rx_queue_mutex);\n\twhile (skb->len >= NLMSG_HDRLEN) {\n\t\tint err;\n\t\tuint32_t rlen;\n\t\tstruct nlmsghdr\t*nlh;\n\t\tstruct iscsi_uevent *ev;\n\t\tuint32_t group;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\tif (nlh->nlmsg_len < sizeof(*nlh) + sizeof(*ev) ||\n\t\t    skb->len < nlh->nlmsg_len) {\n\t\t\tbreak;\n\t\t}\n\n\t\tev = nlmsg_data(nlh);\n\t\trlen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (rlen > skb->len)\n\t\t\trlen = skb->len;\n\n\t\terr = iscsi_if_recv_msg(skb, nlh, &group);\n\t\tif (err) {\n\t\t\tev->type = ISCSI_KEVENT_IF_ERROR;\n\t\t\tev->iferror = err;\n\t\t}\n\t\tdo {\n\t\t\t/*\n\t\t\t * special case for GET_STATS:\n\t\t\t * on success - sending reply and stats from\n\t\t\t * inside of if_recv_msg(),\n\t\t\t * on error - fall through.\n\t\t\t */\n\t\t\tif (ev->type == ISCSI_UEVENT_GET_STATS && !err)\n\t\t\t\tbreak;\n\t\t\tif (ev->type == ISCSI_UEVENT_GET_CHAP && !err)\n\t\t\t\tbreak;\n\t\t\terr = iscsi_if_send_reply(group, nlh->nlmsg_seq,\n\t\t\t\tnlh->nlmsg_type, 0, 0, ev, sizeof(*ev));\n\t\t} while (err < 0 && err != -ECONNREFUSED && err != -ESRCH);\n\t\tskb_pull(skb, rlen);\n\t}\n\tmutex_unlock(&rx_queue_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,7 @@\n \t\tuint32_t group;\n \n \t\tnlh = nlmsg_hdr(skb);\n-\t\tif (nlh->nlmsg_len < sizeof(*nlh) ||\n+\t\tif (nlh->nlmsg_len < sizeof(*nlh) + sizeof(*ev) ||\n \t\t    skb->len < nlh->nlmsg_len) {\n \t\t\tbreak;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (nlh->nlmsg_len < sizeof(*nlh) + sizeof(*ev) ||"
            ],
            "deleted": [
                "\t\tif (nlh->nlmsg_len < sizeof(*nlh) ||"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The iscsi_if_rx function in drivers/scsi/scsi_transport_iscsi.c in the Linux kernel through 4.13.2 allows local users to cause a denial of service (panic) by leveraging incorrect length validation.",
        "id": 1284
    },
    {
        "cve_id": "CVE-2015-3288",
        "code_before_change": "static int handle_pte_fault(struct mm_struct *mm,\n\t\t     struct vm_area_struct *vma, unsigned long address,\n\t\t     pte_t *pte, pmd_t *pmd, unsigned int flags)\n{\n\tpte_t entry;\n\tspinlock_t *ptl;\n\n\t/*\n\t * some architectures can have larger ptes than wordsize,\n\t * e.g.ppc44x-defconfig has CONFIG_PTE_64BIT=y and CONFIG_32BIT=y,\n\t * so READ_ONCE or ACCESS_ONCE cannot guarantee atomic accesses.\n\t * The code below just needs a consistent view for the ifs and\n\t * we later double check anyway with the ptl lock held. So here\n\t * a barrier will do.\n\t */\n\tentry = *pte;\n\tbarrier();\n\tif (!pte_present(entry)) {\n\t\tif (pte_none(entry)) {\n\t\t\tif (vma->vm_ops) {\n\t\t\t\tif (likely(vma->vm_ops->fault))\n\t\t\t\t\treturn do_fault(mm, vma, address, pte,\n\t\t\t\t\t\t\tpmd, flags, entry);\n\t\t\t}\n\t\t\treturn do_anonymous_page(mm, vma, address,\n\t\t\t\t\t\t pte, pmd, flags);\n\t\t}\n\t\treturn do_swap_page(mm, vma, address,\n\t\t\t\t\tpte, pmd, flags, entry);\n\t}\n\n\tif (pte_protnone(entry))\n\t\treturn do_numa_page(mm, vma, address, entry, pte, pmd);\n\n\tptl = pte_lockptr(mm, pmd);\n\tspin_lock(ptl);\n\tif (unlikely(!pte_same(*pte, entry)))\n\t\tgoto unlock;\n\tif (flags & FAULT_FLAG_WRITE) {\n\t\tif (!pte_write(entry))\n\t\t\treturn do_wp_page(mm, vma, address,\n\t\t\t\t\tpte, pmd, ptl, entry);\n\t\tentry = pte_mkdirty(entry);\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (ptep_set_access_flags(vma, address, pte, entry, flags & FAULT_FLAG_WRITE)) {\n\t\tupdate_mmu_cache(vma, address, pte);\n\t} else {\n\t\t/*\n\t\t * This is needed only for protection faults but the arch code\n\t\t * is not yet telling us if this is a protection fault or not.\n\t\t * This still avoids useless tlb flushes for .text page faults\n\t\t * with threads.\n\t\t */\n\t\tif (flags & FAULT_FLAG_WRITE)\n\t\t\tflush_tlb_fix_spurious_fault(vma, address);\n\t}\nunlock:\n\tpte_unmap_unlock(pte, ptl);\n\treturn 0;\n}",
        "code_after_change": "static int handle_pte_fault(struct mm_struct *mm,\n\t\t     struct vm_area_struct *vma, unsigned long address,\n\t\t     pte_t *pte, pmd_t *pmd, unsigned int flags)\n{\n\tpte_t entry;\n\tspinlock_t *ptl;\n\n\t/*\n\t * some architectures can have larger ptes than wordsize,\n\t * e.g.ppc44x-defconfig has CONFIG_PTE_64BIT=y and CONFIG_32BIT=y,\n\t * so READ_ONCE or ACCESS_ONCE cannot guarantee atomic accesses.\n\t * The code below just needs a consistent view for the ifs and\n\t * we later double check anyway with the ptl lock held. So here\n\t * a barrier will do.\n\t */\n\tentry = *pte;\n\tbarrier();\n\tif (!pte_present(entry)) {\n\t\tif (pte_none(entry)) {\n\t\t\tif (vma->vm_ops)\n\t\t\t\treturn do_fault(mm, vma, address, pte, pmd,\n\t\t\t\t\t\tflags, entry);\n\n\t\t\treturn do_anonymous_page(mm, vma, address, pte, pmd,\n\t\t\t\t\tflags);\n\t\t}\n\t\treturn do_swap_page(mm, vma, address,\n\t\t\t\t\tpte, pmd, flags, entry);\n\t}\n\n\tif (pte_protnone(entry))\n\t\treturn do_numa_page(mm, vma, address, entry, pte, pmd);\n\n\tptl = pte_lockptr(mm, pmd);\n\tspin_lock(ptl);\n\tif (unlikely(!pte_same(*pte, entry)))\n\t\tgoto unlock;\n\tif (flags & FAULT_FLAG_WRITE) {\n\t\tif (!pte_write(entry))\n\t\t\treturn do_wp_page(mm, vma, address,\n\t\t\t\t\tpte, pmd, ptl, entry);\n\t\tentry = pte_mkdirty(entry);\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (ptep_set_access_flags(vma, address, pte, entry, flags & FAULT_FLAG_WRITE)) {\n\t\tupdate_mmu_cache(vma, address, pte);\n\t} else {\n\t\t/*\n\t\t * This is needed only for protection faults but the arch code\n\t\t * is not yet telling us if this is a protection fault or not.\n\t\t * This still avoids useless tlb flushes for .text page faults\n\t\t * with threads.\n\t\t */\n\t\tif (flags & FAULT_FLAG_WRITE)\n\t\t\tflush_tlb_fix_spurious_fault(vma, address);\n\t}\nunlock:\n\tpte_unmap_unlock(pte, ptl);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,13 +17,12 @@\n \tbarrier();\n \tif (!pte_present(entry)) {\n \t\tif (pte_none(entry)) {\n-\t\t\tif (vma->vm_ops) {\n-\t\t\t\tif (likely(vma->vm_ops->fault))\n-\t\t\t\t\treturn do_fault(mm, vma, address, pte,\n-\t\t\t\t\t\t\tpmd, flags, entry);\n-\t\t\t}\n-\t\t\treturn do_anonymous_page(mm, vma, address,\n-\t\t\t\t\t\t pte, pmd, flags);\n+\t\t\tif (vma->vm_ops)\n+\t\t\t\treturn do_fault(mm, vma, address, pte, pmd,\n+\t\t\t\t\t\tflags, entry);\n+\n+\t\t\treturn do_anonymous_page(mm, vma, address, pte, pmd,\n+\t\t\t\t\tflags);\n \t\t}\n \t\treturn do_swap_page(mm, vma, address,\n \t\t\t\t\tpte, pmd, flags, entry);",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (vma->vm_ops)",
                "\t\t\t\treturn do_fault(mm, vma, address, pte, pmd,",
                "\t\t\t\t\t\tflags, entry);",
                "",
                "\t\t\treturn do_anonymous_page(mm, vma, address, pte, pmd,",
                "\t\t\t\t\tflags);"
            ],
            "deleted": [
                "\t\t\tif (vma->vm_ops) {",
                "\t\t\t\tif (likely(vma->vm_ops->fault))",
                "\t\t\t\t\treturn do_fault(mm, vma, address, pte,",
                "\t\t\t\t\t\t\tpmd, flags, entry);",
                "\t\t\t}",
                "\t\t\treturn do_anonymous_page(mm, vma, address,",
                "\t\t\t\t\t\t pte, pmd, flags);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "mm/memory.c in the Linux kernel before 4.1.4 mishandles anonymous pages, which allows local users to gain privileges or cause a denial of service (page tainting) via a crafted application that triggers writing to page zero.",
        "id": 756
    },
    {
        "cve_id": "CVE-2018-18021",
        "code_before_change": "static int get_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t/*\n\t * Because the kvm_regs structure is a mix of 32, 64 and\n\t * 128bit fields, we index it as if it was a 32bit\n\t * array. Hence below, nr_regs is the number of entries, and\n\t * off the index in the \"array\".\n\t */\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\tu32 off;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (copy_to_user(uaddr, ((u32 *)regs) + off, KVM_REG_SIZE(reg->id)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
        "code_after_change": "static int get_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t/*\n\t * Because the kvm_regs structure is a mix of 32, 64 and\n\t * 128bit fields, we index it as if it was a 32bit\n\t * array. Hence below, nr_regs is the number of entries, and\n\t * off the index in the \"array\".\n\t */\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\tu32 off;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (validate_core_offset(reg))\n\t\treturn -EINVAL;\n\n\tif (copy_to_user(uaddr, ((u32 *)regs) + off, KVM_REG_SIZE(reg->id)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,9 @@\n \t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n \t\treturn -ENOENT;\n \n+\tif (validate_core_offset(reg))\n+\t\treturn -EINVAL;\n+\n \tif (copy_to_user(uaddr, ((u32 *)regs) + off, KVM_REG_SIZE(reg->id)))\n \t\treturn -EFAULT;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (validate_core_offset(reg))",
                "\t\treturn -EINVAL;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "arch/arm64/kvm/guest.c in KVM in the Linux kernel before 4.18.12 on the arm64 platform mishandles the KVM_SET_ON_REG ioctl. This is exploitable by attackers who can create virtual machines. An attacker can arbitrarily redirect the hypervisor flow of control (with full register control). An attacker can also cause a denial of service (hypervisor panic) via an illegal exception return. This occurs because of insufficient restrictions on userspace access to the core register file, and because PSTATE.M validation does not prevent unintended execution modes.",
        "id": 1729
    },
    {
        "cve_id": "CVE-2013-2888",
        "code_before_change": "struct hid_report *hid_register_report(struct hid_device *device, unsigned type, unsigned id)\n{\n\tstruct hid_report_enum *report_enum = device->report_enum + type;\n\tstruct hid_report *report;\n\n\tif (report_enum->report_id_hash[id])\n\t\treturn report_enum->report_id_hash[id];\n\n\treport = kzalloc(sizeof(struct hid_report), GFP_KERNEL);\n\tif (!report)\n\t\treturn NULL;\n\n\tif (id != 0)\n\t\treport_enum->numbered = 1;\n\n\treport->id = id;\n\treport->type = type;\n\treport->size = 0;\n\treport->device = device;\n\treport_enum->report_id_hash[id] = report;\n\n\tlist_add_tail(&report->list, &report_enum->report_list);\n\n\treturn report;\n}",
        "code_after_change": "struct hid_report *hid_register_report(struct hid_device *device, unsigned type, unsigned id)\n{\n\tstruct hid_report_enum *report_enum = device->report_enum + type;\n\tstruct hid_report *report;\n\n\tif (id >= HID_MAX_IDS)\n\t\treturn NULL;\n\tif (report_enum->report_id_hash[id])\n\t\treturn report_enum->report_id_hash[id];\n\n\treport = kzalloc(sizeof(struct hid_report), GFP_KERNEL);\n\tif (!report)\n\t\treturn NULL;\n\n\tif (id != 0)\n\t\treport_enum->numbered = 1;\n\n\treport->id = id;\n\treport->type = type;\n\treport->size = 0;\n\treport->device = device;\n\treport_enum->report_id_hash[id] = report;\n\n\tlist_add_tail(&report->list, &report_enum->report_list);\n\n\treturn report;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,8 @@\n \tstruct hid_report_enum *report_enum = device->report_enum + type;\n \tstruct hid_report *report;\n \n+\tif (id >= HID_MAX_IDS)\n+\t\treturn NULL;\n \tif (report_enum->report_id_hash[id])\n \t\treturn report_enum->report_id_hash[id];\n ",
        "function_modified_lines": {
            "added": [
                "\tif (id >= HID_MAX_IDS)",
                "\t\treturn NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Multiple array index errors in drivers/hid/hid-core.c in the Human Interface Device (HID) subsystem in the Linux kernel through 3.11 allow physically proximate attackers to execute arbitrary code or cause a denial of service (heap memory corruption) via a crafted device that provides an invalid Report ID.",
        "id": 246
    },
    {
        "cve_id": "CVE-2013-4254",
        "code_before_change": "static int\narmpmu_map_hw_event(const unsigned (*event_map)[PERF_COUNT_HW_MAX], u64 config)\n{\n\tint mapping = (*event_map)[config];\n\treturn mapping == HW_OP_UNSUPPORTED ? -ENOENT : mapping;\n}",
        "code_after_change": "static int\narmpmu_map_hw_event(const unsigned (*event_map)[PERF_COUNT_HW_MAX], u64 config)\n{\n\tint mapping;\n\n\tif (config >= PERF_COUNT_HW_MAX)\n\t\treturn -EINVAL;\n\n\tmapping = (*event_map)[config];\n\treturn mapping == HW_OP_UNSUPPORTED ? -ENOENT : mapping;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,11 @@\n static int\n armpmu_map_hw_event(const unsigned (*event_map)[PERF_COUNT_HW_MAX], u64 config)\n {\n-\tint mapping = (*event_map)[config];\n+\tint mapping;\n+\n+\tif (config >= PERF_COUNT_HW_MAX)\n+\t\treturn -EINVAL;\n+\n+\tmapping = (*event_map)[config];\n \treturn mapping == HW_OP_UNSUPPORTED ? -ENOENT : mapping;\n }",
        "function_modified_lines": {
            "added": [
                "\tint mapping;",
                "",
                "\tif (config >= PERF_COUNT_HW_MAX)",
                "\t\treturn -EINVAL;",
                "",
                "\tmapping = (*event_map)[config];"
            ],
            "deleted": [
                "\tint mapping = (*event_map)[config];"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The validate_event function in arch/arm/kernel/perf_event.c in the Linux kernel before 3.10.8 on the ARM platform allows local users to gain privileges or cause a denial of service (NULL pointer dereference and system crash) by adding a hardware event to an event group led by a software event.",
        "id": 290
    },
    {
        "cve_id": "CVE-2020-12363",
        "code_before_change": "static void guc_init_params(struct intel_guc *guc)\n{\n\tu32 *params = guc->params;\n\tint i;\n\n\tBUILD_BUG_ON(sizeof(guc->params) != GUC_CTL_MAX_DWORDS * sizeof(u32));\n\n\tparams[GUC_CTL_CTXINFO] = guc_ctl_ctxinfo_flags(guc);\n\tparams[GUC_CTL_LOG_PARAMS] = guc_ctl_log_params_flags(guc);\n\tparams[GUC_CTL_FEATURE] = guc_ctl_feature_flags(guc);\n\tparams[GUC_CTL_DEBUG] = guc_ctl_debug_flags(guc);\n\tparams[GUC_CTL_ADS] = guc_ctl_ads_flags(guc);\n\n\tfor (i = 0; i < GUC_CTL_MAX_DWORDS; i++)\n\t\tDRM_DEBUG_DRIVER(\"param[%2d] = %#x\\n\", i, params[i]);\n}",
        "code_after_change": "static void guc_init_params(struct intel_guc *guc)\n{\n\tu32 *params = guc->params;\n\tint i;\n\n\tBUILD_BUG_ON(sizeof(guc->params) != GUC_CTL_MAX_DWORDS * sizeof(u32));\n\n\tparams[GUC_CTL_LOG_PARAMS] = guc_ctl_log_params_flags(guc);\n\tparams[GUC_CTL_FEATURE] = guc_ctl_feature_flags(guc);\n\tparams[GUC_CTL_DEBUG] = guc_ctl_debug_flags(guc);\n\tparams[GUC_CTL_ADS] = guc_ctl_ads_flags(guc);\n\n\tfor (i = 0; i < GUC_CTL_MAX_DWORDS; i++)\n\t\tDRM_DEBUG_DRIVER(\"param[%2d] = %#x\\n\", i, params[i]);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,6 @@\n \n \tBUILD_BUG_ON(sizeof(guc->params) != GUC_CTL_MAX_DWORDS * sizeof(u32));\n \n-\tparams[GUC_CTL_CTXINFO] = guc_ctl_ctxinfo_flags(guc);\n \tparams[GUC_CTL_LOG_PARAMS] = guc_ctl_log_params_flags(guc);\n \tparams[GUC_CTL_FEATURE] = guc_ctl_feature_flags(guc);\n \tparams[GUC_CTL_DEBUG] = guc_ctl_debug_flags(guc);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tparams[GUC_CTL_CTXINFO] = guc_ctl_ctxinfo_flags(guc);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Improper input validation in some Intel(R) Graphics Drivers for Windows* before version 26.20.100.7212 and before Linux kernel version 5.5 may allow a privileged user to potentially enable a denial of service via local access.",
        "id": 2459
    },
    {
        "cve_id": "CVE-2020-12363",
        "code_before_change": "static void __guc_ads_init(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct __guc_ads_blob *blob = guc->ads_blob;\n\tconst u32 skipped_size = LRC_PPHWSP_SZ * PAGE_SIZE + LR_HW_CONTEXT_SIZE;\n\tu32 base;\n\tu8 engine_class;\n\n\t/* GuC scheduling policies */\n\tguc_policies_init(&blob->policies);\n\n\t/*\n\t * GuC expects a per-engine-class context image and size\n\t * (minus hwsp and ring context). The context image will be\n\t * used to reinitialize engines after a reset. It must exist\n\t * and be pinned in the GGTT, so that the address won't change after\n\t * we have told GuC where to find it. The context size will be used\n\t * to validate that the LRC base + size fall within allowed GGTT.\n\t */\n\tfor (engine_class = 0; engine_class <= MAX_ENGINE_CLASS; ++engine_class) {\n\t\tif (engine_class == OTHER_CLASS)\n\t\t\tcontinue;\n\t\t/*\n\t\t * TODO: Set context pointer to default state to allow\n\t\t * GuC to re-init guilty contexts after internal reset.\n\t\t */\n\t\tblob->ads.golden_context_lrca[engine_class] = 0;\n\t\tblob->ads.eng_state_size[engine_class] =\n\t\t\tintel_engine_context_size(guc_to_gt(guc),\n\t\t\t\t\t\t  engine_class) -\n\t\t\tskipped_size;\n\t}\n\n\t/* System info */\n\tblob->system_info.slice_enabled = hweight8(gt->info.sseu.slice_mask);\n\tblob->system_info.rcs_enabled = 1;\n\tblob->system_info.bcs_enabled = 1;\n\n\tblob->system_info.vdbox_enable_mask = VDBOX_MASK(gt);\n\tblob->system_info.vebox_enable_mask = VEBOX_MASK(gt);\n\tblob->system_info.vdbox_sfc_support_mask = gt->info.vdbox_sfc_access;\n\n\tbase = intel_guc_ggtt_offset(guc, guc->ads_vma);\n\n\t/* Clients info  */\n\tguc_ct_pool_entries_init(blob->ct_pool, ARRAY_SIZE(blob->ct_pool));\n\n\tblob->clients_info.clients_num = 1;\n\tblob->clients_info.ct_pool_addr = base + ptr_offset(blob, ct_pool);\n\tblob->clients_info.ct_pool_count = ARRAY_SIZE(blob->ct_pool);\n\n\t/* ADS */\n\tblob->ads.scheduler_policies = base + ptr_offset(blob, policies);\n\tblob->ads.reg_state_buffer = base + ptr_offset(blob, reg_state_buffer);\n\tblob->ads.reg_state_addr = base + ptr_offset(blob, reg_state);\n\tblob->ads.gt_system_info = base + ptr_offset(blob, system_info);\n\tblob->ads.clients_info = base + ptr_offset(blob, clients_info);\n\n\ti915_gem_object_flush_map(guc->ads_vma->obj);\n}",
        "code_after_change": "static void __guc_ads_init(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct __guc_ads_blob *blob = guc->ads_blob;\n\tconst u32 skipped_size = LRC_PPHWSP_SZ * PAGE_SIZE + LR_HW_CONTEXT_SIZE;\n\tu32 base;\n\tu8 engine_class;\n\n\t/* GuC scheduling policies */\n\tguc_policies_init(&blob->policies);\n\n\t/*\n\t * GuC expects a per-engine-class context image and size\n\t * (minus hwsp and ring context). The context image will be\n\t * used to reinitialize engines after a reset. It must exist\n\t * and be pinned in the GGTT, so that the address won't change after\n\t * we have told GuC where to find it. The context size will be used\n\t * to validate that the LRC base + size fall within allowed GGTT.\n\t */\n\tfor (engine_class = 0; engine_class <= MAX_ENGINE_CLASS; ++engine_class) {\n\t\tif (engine_class == OTHER_CLASS)\n\t\t\tcontinue;\n\t\t/*\n\t\t * TODO: Set context pointer to default state to allow\n\t\t * GuC to re-init guilty contexts after internal reset.\n\t\t */\n\t\tblob->ads.golden_context_lrca[engine_class] = 0;\n\t\tblob->ads.eng_state_size[engine_class] =\n\t\t\tintel_engine_context_size(guc_to_gt(guc),\n\t\t\t\t\t\t  engine_class) -\n\t\t\tskipped_size;\n\t}\n\n\t/* System info */\n\tblob->system_info.engine_enabled_masks[RENDER_CLASS] = 1;\n\tblob->system_info.engine_enabled_masks[COPY_ENGINE_CLASS] = 1;\n\tblob->system_info.engine_enabled_masks[VIDEO_DECODE_CLASS] = VDBOX_MASK(gt);\n\tblob->system_info.engine_enabled_masks[VIDEO_ENHANCEMENT_CLASS] = VEBOX_MASK(gt);\n\n\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_SLICE_ENABLED] =\n\t\thweight8(gt->info.sseu.slice_mask);\n\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_VDBOX_SFC_SUPPORT_MASK] =\n\t\tgt->info.vdbox_sfc_access;\n\n\tif (INTEL_GEN(i915) >= 12 && !IS_DGFX(i915)) {\n\t\tu32 distdbreg = intel_uncore_read(gt->uncore,\n\t\t\t\t\t\t  GEN12_DIST_DBS_POPULATED);\n\t\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_DOORBELL_COUNT_PER_SQIDI] =\n\t\t\t((distdbreg >> GEN12_DOORBELLS_PER_SQIDI_SHIFT) &\n\t\t\t GEN12_DOORBELLS_PER_SQIDI) + 1;\n\t}\n\n\tguc_mapping_table_init(guc_to_gt(guc), &blob->system_info);\n\n\tbase = intel_guc_ggtt_offset(guc, guc->ads_vma);\n\n\t/* Clients info  */\n\tguc_ct_pool_entries_init(blob->ct_pool, ARRAY_SIZE(blob->ct_pool));\n\n\tblob->clients_info.clients_num = 1;\n\tblob->clients_info.ct_pool_addr = base + ptr_offset(blob, ct_pool);\n\tblob->clients_info.ct_pool_count = ARRAY_SIZE(blob->ct_pool);\n\n\t/* ADS */\n\tblob->ads.scheduler_policies = base + ptr_offset(blob, policies);\n\tblob->ads.gt_system_info = base + ptr_offset(blob, system_info);\n\tblob->ads.clients_info = base + ptr_offset(blob, clients_info);\n\n\t/* Private Data */\n\tblob->ads.private_data = base + guc_ads_private_data_offset(guc);\n\n\ti915_gem_object_flush_map(guc->ads_vma->obj);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,7 @@\n static void __guc_ads_init(struct intel_guc *guc)\n {\n \tstruct intel_gt *gt = guc_to_gt(guc);\n+\tstruct drm_i915_private *i915 = gt->i915;\n \tstruct __guc_ads_blob *blob = guc->ads_blob;\n \tconst u32 skipped_size = LRC_PPHWSP_SZ * PAGE_SIZE + LR_HW_CONTEXT_SIZE;\n \tu32 base;\n@@ -32,13 +33,25 @@\n \t}\n \n \t/* System info */\n-\tblob->system_info.slice_enabled = hweight8(gt->info.sseu.slice_mask);\n-\tblob->system_info.rcs_enabled = 1;\n-\tblob->system_info.bcs_enabled = 1;\n+\tblob->system_info.engine_enabled_masks[RENDER_CLASS] = 1;\n+\tblob->system_info.engine_enabled_masks[COPY_ENGINE_CLASS] = 1;\n+\tblob->system_info.engine_enabled_masks[VIDEO_DECODE_CLASS] = VDBOX_MASK(gt);\n+\tblob->system_info.engine_enabled_masks[VIDEO_ENHANCEMENT_CLASS] = VEBOX_MASK(gt);\n \n-\tblob->system_info.vdbox_enable_mask = VDBOX_MASK(gt);\n-\tblob->system_info.vebox_enable_mask = VEBOX_MASK(gt);\n-\tblob->system_info.vdbox_sfc_support_mask = gt->info.vdbox_sfc_access;\n+\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_SLICE_ENABLED] =\n+\t\thweight8(gt->info.sseu.slice_mask);\n+\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_VDBOX_SFC_SUPPORT_MASK] =\n+\t\tgt->info.vdbox_sfc_access;\n+\n+\tif (INTEL_GEN(i915) >= 12 && !IS_DGFX(i915)) {\n+\t\tu32 distdbreg = intel_uncore_read(gt->uncore,\n+\t\t\t\t\t\t  GEN12_DIST_DBS_POPULATED);\n+\t\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_DOORBELL_COUNT_PER_SQIDI] =\n+\t\t\t((distdbreg >> GEN12_DOORBELLS_PER_SQIDI_SHIFT) &\n+\t\t\t GEN12_DOORBELLS_PER_SQIDI) + 1;\n+\t}\n+\n+\tguc_mapping_table_init(guc_to_gt(guc), &blob->system_info);\n \n \tbase = intel_guc_ggtt_offset(guc, guc->ads_vma);\n \n@@ -51,10 +64,11 @@\n \n \t/* ADS */\n \tblob->ads.scheduler_policies = base + ptr_offset(blob, policies);\n-\tblob->ads.reg_state_buffer = base + ptr_offset(blob, reg_state_buffer);\n-\tblob->ads.reg_state_addr = base + ptr_offset(blob, reg_state);\n \tblob->ads.gt_system_info = base + ptr_offset(blob, system_info);\n \tblob->ads.clients_info = base + ptr_offset(blob, clients_info);\n \n+\t/* Private Data */\n+\tblob->ads.private_data = base + guc_ads_private_data_offset(guc);\n+\n \ti915_gem_object_flush_map(guc->ads_vma->obj);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct drm_i915_private *i915 = gt->i915;",
                "\tblob->system_info.engine_enabled_masks[RENDER_CLASS] = 1;",
                "\tblob->system_info.engine_enabled_masks[COPY_ENGINE_CLASS] = 1;",
                "\tblob->system_info.engine_enabled_masks[VIDEO_DECODE_CLASS] = VDBOX_MASK(gt);",
                "\tblob->system_info.engine_enabled_masks[VIDEO_ENHANCEMENT_CLASS] = VEBOX_MASK(gt);",
                "\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_SLICE_ENABLED] =",
                "\t\thweight8(gt->info.sseu.slice_mask);",
                "\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_VDBOX_SFC_SUPPORT_MASK] =",
                "\t\tgt->info.vdbox_sfc_access;",
                "",
                "\tif (INTEL_GEN(i915) >= 12 && !IS_DGFX(i915)) {",
                "\t\tu32 distdbreg = intel_uncore_read(gt->uncore,",
                "\t\t\t\t\t\t  GEN12_DIST_DBS_POPULATED);",
                "\t\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_DOORBELL_COUNT_PER_SQIDI] =",
                "\t\t\t((distdbreg >> GEN12_DOORBELLS_PER_SQIDI_SHIFT) &",
                "\t\t\t GEN12_DOORBELLS_PER_SQIDI) + 1;",
                "\t}",
                "",
                "\tguc_mapping_table_init(guc_to_gt(guc), &blob->system_info);",
                "\t/* Private Data */",
                "\tblob->ads.private_data = base + guc_ads_private_data_offset(guc);",
                ""
            ],
            "deleted": [
                "\tblob->system_info.slice_enabled = hweight8(gt->info.sseu.slice_mask);",
                "\tblob->system_info.rcs_enabled = 1;",
                "\tblob->system_info.bcs_enabled = 1;",
                "\tblob->system_info.vdbox_enable_mask = VDBOX_MASK(gt);",
                "\tblob->system_info.vebox_enable_mask = VEBOX_MASK(gt);",
                "\tblob->system_info.vdbox_sfc_support_mask = gt->info.vdbox_sfc_access;",
                "\tblob->ads.reg_state_buffer = base + ptr_offset(blob, reg_state_buffer);",
                "\tblob->ads.reg_state_addr = base + ptr_offset(blob, reg_state);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Improper input validation in some Intel(R) Graphics Drivers for Windows* before version 26.20.100.7212 and before Linux kernel version 5.5 may allow a privileged user to potentially enable a denial of service via local access.",
        "id": 2460
    },
    {
        "cve_id": "CVE-2018-10087",
        "code_before_change": "long kernel_wait4(pid_t upid, int __user *stat_addr, int options,\n\t\t  struct rusage *ru)\n{\n\tstruct wait_opts wo;\n\tstruct pid *pid = NULL;\n\tenum pid_type type;\n\tlong ret;\n\n\tif (options & ~(WNOHANG|WUNTRACED|WCONTINUED|\n\t\t\t__WNOTHREAD|__WCLONE|__WALL))\n\t\treturn -EINVAL;\n\n\tif (upid == -1)\n\t\ttype = PIDTYPE_MAX;\n\telse if (upid < 0) {\n\t\ttype = PIDTYPE_PGID;\n\t\tpid = find_get_pid(-upid);\n\t} else if (upid == 0) {\n\t\ttype = PIDTYPE_PGID;\n\t\tpid = get_task_pid(current, PIDTYPE_PGID);\n\t} else /* upid > 0 */ {\n\t\ttype = PIDTYPE_PID;\n\t\tpid = find_get_pid(upid);\n\t}\n\n\two.wo_type\t= type;\n\two.wo_pid\t= pid;\n\two.wo_flags\t= options | WEXITED;\n\two.wo_info\t= NULL;\n\two.wo_stat\t= 0;\n\two.wo_rusage\t= ru;\n\tret = do_wait(&wo);\n\tput_pid(pid);\n\tif (ret > 0 && stat_addr && put_user(wo.wo_stat, stat_addr))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}",
        "code_after_change": "long kernel_wait4(pid_t upid, int __user *stat_addr, int options,\n\t\t  struct rusage *ru)\n{\n\tstruct wait_opts wo;\n\tstruct pid *pid = NULL;\n\tenum pid_type type;\n\tlong ret;\n\n\tif (options & ~(WNOHANG|WUNTRACED|WCONTINUED|\n\t\t\t__WNOTHREAD|__WCLONE|__WALL))\n\t\treturn -EINVAL;\n\n\t/* -INT_MIN is not defined */\n\tif (upid == INT_MIN)\n\t\treturn -ESRCH;\n\n\tif (upid == -1)\n\t\ttype = PIDTYPE_MAX;\n\telse if (upid < 0) {\n\t\ttype = PIDTYPE_PGID;\n\t\tpid = find_get_pid(-upid);\n\t} else if (upid == 0) {\n\t\ttype = PIDTYPE_PGID;\n\t\tpid = get_task_pid(current, PIDTYPE_PGID);\n\t} else /* upid > 0 */ {\n\t\ttype = PIDTYPE_PID;\n\t\tpid = find_get_pid(upid);\n\t}\n\n\two.wo_type\t= type;\n\two.wo_pid\t= pid;\n\two.wo_flags\t= options | WEXITED;\n\two.wo_info\t= NULL;\n\two.wo_stat\t= 0;\n\two.wo_rusage\t= ru;\n\tret = do_wait(&wo);\n\tput_pid(pid);\n\tif (ret > 0 && stat_addr && put_user(wo.wo_stat, stat_addr))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,10 @@\n \tif (options & ~(WNOHANG|WUNTRACED|WCONTINUED|\n \t\t\t__WNOTHREAD|__WCLONE|__WALL))\n \t\treturn -EINVAL;\n+\n+\t/* -INT_MIN is not defined */\n+\tif (upid == INT_MIN)\n+\t\treturn -ESRCH;\n \n \tif (upid == -1)\n \t\ttype = PIDTYPE_MAX;",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* -INT_MIN is not defined */",
                "\tif (upid == INT_MIN)",
                "\t\treturn -ESRCH;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The kernel_wait4 function in kernel/exit.c in the Linux kernel before 4.13, when an unspecified architecture and compiler is used, might allow local users to cause a denial of service by triggering an attempted use of the -INT_MIN value.",
        "id": 1585
    },
    {
        "cve_id": "CVE-2017-1000252",
        "code_before_change": "static int vmx_update_pi_irte(struct kvm *kvm, unsigned int host_irq,\n\t\t\t      uint32_t guest_irq, bool set)\n{\n\tstruct kvm_kernel_irq_routing_entry *e;\n\tstruct kvm_irq_routing_table *irq_rt;\n\tstruct kvm_lapic_irq irq;\n\tstruct kvm_vcpu *vcpu;\n\tstruct vcpu_data vcpu_info;\n\tint idx, ret = -EINVAL;\n\n\tif (!kvm_arch_has_assigned_device(kvm) ||\n\t\t!irq_remapping_cap(IRQ_POSTING_CAP) ||\n\t\t!kvm_vcpu_apicv_active(kvm->vcpus[0]))\n\t\treturn 0;\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tirq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);\n\tBUG_ON(guest_irq >= irq_rt->nr_rt_entries);\n\n\thlist_for_each_entry(e, &irq_rt->map[guest_irq], link) {\n\t\tif (e->type != KVM_IRQ_ROUTING_MSI)\n\t\t\tcontinue;\n\t\t/*\n\t\t * VT-d PI cannot support posting multicast/broadcast\n\t\t * interrupts to a vCPU, we still use interrupt remapping\n\t\t * for these kind of interrupts.\n\t\t *\n\t\t * For lowest-priority interrupts, we only support\n\t\t * those with single CPU as the destination, e.g. user\n\t\t * configures the interrupts via /proc/irq or uses\n\t\t * irqbalance to make the interrupts single-CPU.\n\t\t *\n\t\t * We will support full lowest-priority interrupt later.\n\t\t */\n\n\t\tkvm_set_msi_irq(kvm, e, &irq);\n\t\tif (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu)) {\n\t\t\t/*\n\t\t\t * Make sure the IRTE is in remapped mode if\n\t\t\t * we don't handle it in posted mode.\n\t\t\t */\n\t\t\tret = irq_set_vcpu_affinity(host_irq, NULL);\n\t\t\tif (ret < 0) {\n\t\t\t\tprintk(KERN_INFO\n\t\t\t\t   \"failed to back to remapped mode, irq: %u\\n\",\n\t\t\t\t   host_irq);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tvcpu_info.pi_desc_addr = __pa(vcpu_to_pi_desc(vcpu));\n\t\tvcpu_info.vector = irq.vector;\n\n\t\ttrace_kvm_pi_irte_update(vcpu->vcpu_id, host_irq, e->gsi,\n\t\t\t\tvcpu_info.vector, vcpu_info.pi_desc_addr, set);\n\n\t\tif (set)\n\t\t\tret = irq_set_vcpu_affinity(host_irq, &vcpu_info);\n\t\telse {\n\t\t\t/* suppress notification event before unposting */\n\t\t\tpi_set_sn(vcpu_to_pi_desc(vcpu));\n\t\t\tret = irq_set_vcpu_affinity(host_irq, NULL);\n\t\t\tpi_clear_sn(vcpu_to_pi_desc(vcpu));\n\t\t}\n\n\t\tif (ret < 0) {\n\t\t\tprintk(KERN_INFO \"%s: failed to update PI IRTE\\n\",\n\t\t\t\t\t__func__);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = 0;\nout:\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\treturn ret;\n}",
        "code_after_change": "static int vmx_update_pi_irte(struct kvm *kvm, unsigned int host_irq,\n\t\t\t      uint32_t guest_irq, bool set)\n{\n\tstruct kvm_kernel_irq_routing_entry *e;\n\tstruct kvm_irq_routing_table *irq_rt;\n\tstruct kvm_lapic_irq irq;\n\tstruct kvm_vcpu *vcpu;\n\tstruct vcpu_data vcpu_info;\n\tint idx, ret = 0;\n\n\tif (!kvm_arch_has_assigned_device(kvm) ||\n\t\t!irq_remapping_cap(IRQ_POSTING_CAP) ||\n\t\t!kvm_vcpu_apicv_active(kvm->vcpus[0]))\n\t\treturn 0;\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tirq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);\n\tif (guest_irq >= irq_rt->nr_rt_entries ||\n\t    hlist_empty(&irq_rt->map[guest_irq])) {\n\t\tpr_warn_once(\"no route for guest_irq %u/%u (broken user space?)\\n\",\n\t\t\t     guest_irq, irq_rt->nr_rt_entries);\n\t\tgoto out;\n\t}\n\n\thlist_for_each_entry(e, &irq_rt->map[guest_irq], link) {\n\t\tif (e->type != KVM_IRQ_ROUTING_MSI)\n\t\t\tcontinue;\n\t\t/*\n\t\t * VT-d PI cannot support posting multicast/broadcast\n\t\t * interrupts to a vCPU, we still use interrupt remapping\n\t\t * for these kind of interrupts.\n\t\t *\n\t\t * For lowest-priority interrupts, we only support\n\t\t * those with single CPU as the destination, e.g. user\n\t\t * configures the interrupts via /proc/irq or uses\n\t\t * irqbalance to make the interrupts single-CPU.\n\t\t *\n\t\t * We will support full lowest-priority interrupt later.\n\t\t */\n\n\t\tkvm_set_msi_irq(kvm, e, &irq);\n\t\tif (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu)) {\n\t\t\t/*\n\t\t\t * Make sure the IRTE is in remapped mode if\n\t\t\t * we don't handle it in posted mode.\n\t\t\t */\n\t\t\tret = irq_set_vcpu_affinity(host_irq, NULL);\n\t\t\tif (ret < 0) {\n\t\t\t\tprintk(KERN_INFO\n\t\t\t\t   \"failed to back to remapped mode, irq: %u\\n\",\n\t\t\t\t   host_irq);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tvcpu_info.pi_desc_addr = __pa(vcpu_to_pi_desc(vcpu));\n\t\tvcpu_info.vector = irq.vector;\n\n\t\ttrace_kvm_pi_irte_update(vcpu->vcpu_id, host_irq, e->gsi,\n\t\t\t\tvcpu_info.vector, vcpu_info.pi_desc_addr, set);\n\n\t\tif (set)\n\t\t\tret = irq_set_vcpu_affinity(host_irq, &vcpu_info);\n\t\telse {\n\t\t\t/* suppress notification event before unposting */\n\t\t\tpi_set_sn(vcpu_to_pi_desc(vcpu));\n\t\t\tret = irq_set_vcpu_affinity(host_irq, NULL);\n\t\t\tpi_clear_sn(vcpu_to_pi_desc(vcpu));\n\t\t}\n\n\t\tif (ret < 0) {\n\t\t\tprintk(KERN_INFO \"%s: failed to update PI IRTE\\n\",\n\t\t\t\t\t__func__);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = 0;\nout:\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tstruct kvm_lapic_irq irq;\n \tstruct kvm_vcpu *vcpu;\n \tstruct vcpu_data vcpu_info;\n-\tint idx, ret = -EINVAL;\n+\tint idx, ret = 0;\n \n \tif (!kvm_arch_has_assigned_device(kvm) ||\n \t\t!irq_remapping_cap(IRQ_POSTING_CAP) ||\n@@ -15,7 +15,12 @@\n \n \tidx = srcu_read_lock(&kvm->irq_srcu);\n \tirq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);\n-\tBUG_ON(guest_irq >= irq_rt->nr_rt_entries);\n+\tif (guest_irq >= irq_rt->nr_rt_entries ||\n+\t    hlist_empty(&irq_rt->map[guest_irq])) {\n+\t\tpr_warn_once(\"no route for guest_irq %u/%u (broken user space?)\\n\",\n+\t\t\t     guest_irq, irq_rt->nr_rt_entries);\n+\t\tgoto out;\n+\t}\n \n \thlist_for_each_entry(e, &irq_rt->map[guest_irq], link) {\n \t\tif (e->type != KVM_IRQ_ROUTING_MSI)",
        "function_modified_lines": {
            "added": [
                "\tint idx, ret = 0;",
                "\tif (guest_irq >= irq_rt->nr_rt_entries ||",
                "\t    hlist_empty(&irq_rt->map[guest_irq])) {",
                "\t\tpr_warn_once(\"no route for guest_irq %u/%u (broken user space?)\\n\",",
                "\t\t\t     guest_irq, irq_rt->nr_rt_entries);",
                "\t\tgoto out;",
                "\t}"
            ],
            "deleted": [
                "\tint idx, ret = -EINVAL;",
                "\tBUG_ON(guest_irq >= irq_rt->nr_rt_entries);"
            ]
        },
        "cwe": [
            "CWE-20",
            "CWE-617"
        ],
        "cve_description": "The KVM subsystem in the Linux kernel through 4.13.3 allows guest OS users to cause a denial of service (assertion failure, and hypervisor hang or crash) via an out-of bounds guest_irq value, related to arch/x86/kvm/vmx.c and virt/kvm/eventfd.c.",
        "id": 1193
    },
    {
        "cve_id": "CVE-2015-2672",
        "code_before_change": "static inline int xrstor_state_booting(struct xsave_struct *fx, u64 mask)\n{\n\tu32 lmask = mask;\n\tu32 hmask = mask >> 32;\n\tint err = 0;\n\n\tWARN_ON(system_state != SYSTEM_BOOTING);\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVES))\n\t\tasm volatile(\"1:\"XRSTORS\"\\n\\t\"\n\t\t\t\"2:\\n\\t\"\n\t\t\t: : \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n\t\t\t:   \"memory\");\n\telse\n\t\tasm volatile(\"1:\"XRSTOR\"\\n\\t\"\n\t\t\t\"2:\\n\\t\"\n\t\t\t: : \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n\t\t\t:   \"memory\");\n\n\tasm volatile(xstate_fault\n\t\t     : \"0\" (0)\n\t\t     : \"memory\");\n\n\treturn err;\n}",
        "code_after_change": "static inline int xrstor_state_booting(struct xsave_struct *fx, u64 mask)\n{\n\tu32 lmask = mask;\n\tu32 hmask = mask >> 32;\n\tint err = 0;\n\n\tWARN_ON(system_state != SYSTEM_BOOTING);\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVES))\n\t\tasm volatile(\"1:\"XRSTORS\"\\n\\t\"\n\t\t\t\"2:\\n\\t\"\n\t\t\t     xstate_fault\n\t\t\t: \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n\t\t\t:   \"memory\");\n\telse\n\t\tasm volatile(\"1:\"XRSTOR\"\\n\\t\"\n\t\t\t\"2:\\n\\t\"\n\t\t\t     xstate_fault\n\t\t\t: \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n\t\t\t:   \"memory\");\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,17 +9,14 @@\n \tif (boot_cpu_has(X86_FEATURE_XSAVES))\n \t\tasm volatile(\"1:\"XRSTORS\"\\n\\t\"\n \t\t\t\"2:\\n\\t\"\n-\t\t\t: : \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n+\t\t\t     xstate_fault\n+\t\t\t: \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n \t\t\t:   \"memory\");\n \telse\n \t\tasm volatile(\"1:\"XRSTOR\"\\n\\t\"\n \t\t\t\"2:\\n\\t\"\n-\t\t\t: : \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n+\t\t\t     xstate_fault\n+\t\t\t: \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n \t\t\t:   \"memory\");\n-\n-\tasm volatile(xstate_fault\n-\t\t     : \"0\" (0)\n-\t\t     : \"memory\");\n-\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\t\t\t     xstate_fault",
                "\t\t\t: \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)",
                "\t\t\t     xstate_fault",
                "\t\t\t: \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)"
            ],
            "deleted": [
                "\t\t\t: : \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)",
                "\t\t\t: : \"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)",
                "",
                "\tasm volatile(xstate_fault",
                "\t\t     : \"0\" (0)",
                "\t\t     : \"memory\");",
                ""
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The xsave/xrstor implementation in arch/x86/include/asm/xsave.h in the Linux kernel before 3.19.2 creates certain .altinstr_replacement pointers and consequently does not provide any protection against instruction faulting, which allows local users to cause a denial of service (panic) by triggering a fault, as demonstrated by an unaligned memory operand or a non-canonical address memory operand.",
        "id": 742
    },
    {
        "cve_id": "CVE-2016-2548",
        "code_before_change": "static int snd_timer_start_slave(struct snd_timer_instance *timeri)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&slave_active_lock, flags);\n\ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n\tif (timeri->master)\n\t\tlist_add_tail(&timeri->active_list,\n\t\t\t      &timeri->master->slave_active_head);\n\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\treturn 1; /* delayed start */\n}",
        "code_after_change": "static int snd_timer_start_slave(struct snd_timer_instance *timeri)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&slave_active_lock, flags);\n\ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n\tif (timeri->master && timeri->timer) {\n\t\tspin_lock(&timeri->timer->lock);\n\t\tlist_add_tail(&timeri->active_list,\n\t\t\t      &timeri->master->slave_active_head);\n\t\tspin_unlock(&timeri->timer->lock);\n\t}\n\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\treturn 1; /* delayed start */\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,9 +4,12 @@\n \n \tspin_lock_irqsave(&slave_active_lock, flags);\n \ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n-\tif (timeri->master)\n+\tif (timeri->master && timeri->timer) {\n+\t\tspin_lock(&timeri->timer->lock);\n \t\tlist_add_tail(&timeri->active_list,\n \t\t\t      &timeri->master->slave_active_head);\n+\t\tspin_unlock(&timeri->timer->lock);\n+\t}\n \tspin_unlock_irqrestore(&slave_active_lock, flags);\n \treturn 1; /* delayed start */\n }",
        "function_modified_lines": {
            "added": [
                "\tif (timeri->master && timeri->timer) {",
                "\t\tspin_lock(&timeri->timer->lock);",
                "\t\tspin_unlock(&timeri->timer->lock);",
                "\t}"
            ],
            "deleted": [
                "\tif (timeri->master)"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 retains certain linked lists after a close or stop action, which allows local users to cause a denial of service (system crash) via a crafted ioctl call, related to the (1) snd_timer_close and (2) _snd_timer_stop functions.",
        "id": 945
    },
    {
        "cve_id": "CVE-2019-9503",
        "code_before_change": "void brcmf_rx_frame(struct device *dev, struct sk_buff *skb, bool handle_event)\n{\n\tstruct brcmf_if *ifp;\n\tstruct brcmf_bus *bus_if = dev_get_drvdata(dev);\n\tstruct brcmf_pub *drvr = bus_if->drvr;\n\n\tbrcmf_dbg(DATA, \"Enter: %s: rxp=%p\\n\", dev_name(dev), skb);\n\n\tif (brcmf_rx_hdrpull(drvr, skb, &ifp))\n\t\treturn;\n\n\tif (brcmf_proto_is_reorder_skb(skb)) {\n\t\tbrcmf_proto_rxreorder(ifp, skb);\n\t} else {\n\t\t/* Process special event packets */\n\t\tif (handle_event)\n\t\t\tbrcmf_fweh_process_skb(ifp->drvr, skb);\n\n\t\tbrcmf_netif_rx(ifp, skb);\n\t}\n}",
        "code_after_change": "void brcmf_rx_frame(struct device *dev, struct sk_buff *skb, bool handle_event)\n{\n\tstruct brcmf_if *ifp;\n\tstruct brcmf_bus *bus_if = dev_get_drvdata(dev);\n\tstruct brcmf_pub *drvr = bus_if->drvr;\n\n\tbrcmf_dbg(DATA, \"Enter: %s: rxp=%p\\n\", dev_name(dev), skb);\n\n\tif (brcmf_rx_hdrpull(drvr, skb, &ifp))\n\t\treturn;\n\n\tif (brcmf_proto_is_reorder_skb(skb)) {\n\t\tbrcmf_proto_rxreorder(ifp, skb);\n\t} else {\n\t\t/* Process special event packets */\n\t\tif (handle_event)\n\t\t\tbrcmf_fweh_process_skb(ifp->drvr, skb,\n\t\t\t\t\t       BCMILCP_SUBTYPE_VENDOR_LONG);\n\n\t\tbrcmf_netif_rx(ifp, skb);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,7 +14,8 @@\n \t} else {\n \t\t/* Process special event packets */\n \t\tif (handle_event)\n-\t\t\tbrcmf_fweh_process_skb(ifp->drvr, skb);\n+\t\t\tbrcmf_fweh_process_skb(ifp->drvr, skb,\n+\t\t\t\t\t       BCMILCP_SUBTYPE_VENDOR_LONG);\n \n \t\tbrcmf_netif_rx(ifp, skb);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tbrcmf_fweh_process_skb(ifp->drvr, skb,",
                "\t\t\t\t\t       BCMILCP_SUBTYPE_VENDOR_LONG);"
            ],
            "deleted": [
                "\t\t\tbrcmf_fweh_process_skb(ifp->drvr, skb);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Broadcom brcmfmac WiFi driver prior to commit a4176ec356c73a46c07c181c6d04039fafa34a9f is vulnerable to a frame validation bypass. If the brcmfmac driver receives a firmware event frame from a remote source, the is_wlc_event_frame function will cause this frame to be discarded and unprocessed. If the driver receives the firmware event frame from the host, the appropriate handler is called. This frame validation can be bypassed if the bus used is USB (for instance by a wifi dongle). This can allow firmware event frames from a remote source to be processed. In the worst case scenario, by sending specially-crafted WiFi packets, a remote, unauthenticated attacker may be able to execute arbitrary code on a vulnerable system. More typically, this vulnerability will result in denial-of-service conditions.",
        "id": 2367
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "int rxrpc_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct rxrpc_skb_priv *sp;\n\tstruct rxrpc_call *call = NULL, *continue_call = NULL;\n\tstruct rxrpc_sock *rx = rxrpc_sk(sock->sk);\n\tstruct sk_buff *skb;\n\tlong timeo;\n\tint copy, ret, ullen, offset, copied = 0;\n\tu32 abort_code;\n\n\tDEFINE_WAIT(wait);\n\n\t_enter(\",,,%zu,%d\", len, flags);\n\n\tif (flags & (MSG_OOB | MSG_TRUNC))\n\t\treturn -EOPNOTSUPP;\n\n\tullen = msg->msg_flags & MSG_CMSG_COMPAT ? 4 : sizeof(unsigned long);\n\n\ttimeo = sock_rcvtimeo(&rx->sk, flags & MSG_DONTWAIT);\n\tmsg->msg_flags |= MSG_MORE;\n\n\tlock_sock(&rx->sk);\n\n\tfor (;;) {\n\t\t/* return immediately if a client socket has no outstanding\n\t\t * calls */\n\t\tif (RB_EMPTY_ROOT(&rx->calls)) {\n\t\t\tif (copied)\n\t\t\t\tgoto out;\n\t\t\tif (rx->sk.sk_state != RXRPC_SERVER_LISTENING) {\n\t\t\t\trelease_sock(&rx->sk);\n\t\t\t\tif (continue_call)\n\t\t\t\t\trxrpc_put_call(continue_call);\n\t\t\t\treturn -ENODATA;\n\t\t\t}\n\t\t}\n\n\t\t/* get the next message on the Rx queue */\n\t\tskb = skb_peek(&rx->sk.sk_receive_queue);\n\t\tif (!skb) {\n\t\t\t/* nothing remains on the queue */\n\t\t\tif (copied &&\n\t\t\t    (msg->msg_flags & MSG_PEEK || timeo == 0))\n\t\t\t\tgoto out;\n\n\t\t\t/* wait for a message to turn up */\n\t\t\trelease_sock(&rx->sk);\n\t\t\tprepare_to_wait_exclusive(sk_sleep(&rx->sk), &wait,\n\t\t\t\t\t\t  TASK_INTERRUPTIBLE);\n\t\t\tret = sock_error(&rx->sk);\n\t\t\tif (ret)\n\t\t\t\tgoto wait_error;\n\n\t\t\tif (skb_queue_empty(&rx->sk.sk_receive_queue)) {\n\t\t\t\tif (signal_pending(current))\n\t\t\t\t\tgoto wait_interrupted;\n\t\t\t\ttimeo = schedule_timeout(timeo);\n\t\t\t}\n\t\t\tfinish_wait(sk_sleep(&rx->sk), &wait);\n\t\t\tlock_sock(&rx->sk);\n\t\t\tcontinue;\n\t\t}\n\n\tpeek_next_packet:\n\t\tsp = rxrpc_skb(skb);\n\t\tcall = sp->call;\n\t\tASSERT(call != NULL);\n\n\t\t_debug(\"next pkt %s\", rxrpc_pkts[sp->hdr.type]);\n\n\t\t/* make sure we wait for the state to be updated in this call */\n\t\tspin_lock_bh(&call->lock);\n\t\tspin_unlock_bh(&call->lock);\n\n\t\tif (test_bit(RXRPC_CALL_RELEASED, &call->flags)) {\n\t\t\t_debug(\"packet from released call\");\n\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\t\tBUG();\n\t\t\trxrpc_free_skb(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* determine whether to continue last data receive */\n\t\tif (continue_call) {\n\t\t\t_debug(\"maybe cont\");\n\t\t\tif (call != continue_call ||\n\t\t\t    skb->mark != RXRPC_SKB_MARK_DATA) {\n\t\t\t\trelease_sock(&rx->sk);\n\t\t\t\trxrpc_put_call(continue_call);\n\t\t\t\t_leave(\" = %d [noncont]\", copied);\n\t\t\t\treturn copied;\n\t\t\t}\n\t\t}\n\n\t\trxrpc_get_call(call);\n\n\t\t/* copy the peer address and timestamp */\n\t\tif (!continue_call) {\n\t\t\tif (msg->msg_name && msg->msg_namelen > 0)\n\t\t\t\tmemcpy(msg->msg_name,\n\t\t\t\t       &call->conn->trans->peer->srx,\n\t\t\t\t       sizeof(call->conn->trans->peer->srx));\n\t\t\tsock_recv_ts_and_drops(msg, &rx->sk, skb);\n\t\t}\n\n\t\t/* receive the message */\n\t\tif (skb->mark != RXRPC_SKB_MARK_DATA)\n\t\t\tgoto receive_non_data_message;\n\n\t\t_debug(\"recvmsg DATA #%u { %d, %d }\",\n\t\t       ntohl(sp->hdr.seq), skb->len, sp->offset);\n\n\t\tif (!continue_call) {\n\t\t\t/* only set the control data once per recvmsg() */\n\t\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_USER_CALL_ID,\n\t\t\t\t       ullen, &call->user_call_ID);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto copy_error;\n\t\t\tASSERT(test_bit(RXRPC_CALL_HAS_USERID, &call->flags));\n\t\t}\n\n\t\tASSERTCMP(ntohl(sp->hdr.seq), >=, call->rx_data_recv);\n\t\tASSERTCMP(ntohl(sp->hdr.seq), <=, call->rx_data_recv + 1);\n\t\tcall->rx_data_recv = ntohl(sp->hdr.seq);\n\n\t\tASSERTCMP(ntohl(sp->hdr.seq), >, call->rx_data_eaten);\n\n\t\toffset = sp->offset;\n\t\tcopy = skb->len - offset;\n\t\tif (copy > len - copied)\n\t\t\tcopy = len - copied;\n\n\t\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\t\tret = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\t      msg->msg_iov, copy);\n\t\t} else {\n\t\t\tret = skb_copy_and_csum_datagram_iovec(skb, offset,\n\t\t\t\t\t\t\t       msg->msg_iov);\n\t\t\tif (ret == -EINVAL)\n\t\t\t\tgoto csum_copy_error;\n\t\t}\n\n\t\tif (ret < 0)\n\t\t\tgoto copy_error;\n\n\t\t/* handle piecemeal consumption of data packets */\n\t\t_debug(\"copied %d+%d\", copy, copied);\n\n\t\toffset += copy;\n\t\tcopied += copy;\n\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tsp->offset = offset;\n\n\t\tif (sp->offset < skb->len) {\n\t\t\t_debug(\"buffer full\");\n\t\t\tASSERTCMP(copied, ==, len);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* we transferred the whole data packet */\n\t\tif (sp->hdr.flags & RXRPC_LAST_PACKET) {\n\t\t\t_debug(\"last\");\n\t\t\tif (call->conn->out_clientflag) {\n\t\t\t\t /* last byte of reply received */\n\t\t\t\tret = copied;\n\t\t\t\tgoto terminal_message;\n\t\t\t}\n\n\t\t\t/* last bit of request received */\n\t\t\tif (!(flags & MSG_PEEK)) {\n\t\t\t\t_debug(\"eat packet\");\n\t\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) !=\n\t\t\t\t    skb)\n\t\t\t\t\tBUG();\n\t\t\t\trxrpc_free_skb(skb);\n\t\t\t}\n\t\t\tmsg->msg_flags &= ~MSG_MORE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* move on to the next data message */\n\t\t_debug(\"next\");\n\t\tif (!continue_call)\n\t\t\tcontinue_call = sp->call;\n\t\telse\n\t\t\trxrpc_put_call(call);\n\t\tcall = NULL;\n\n\t\tif (flags & MSG_PEEK) {\n\t\t\t_debug(\"peek next\");\n\t\t\tskb = skb->next;\n\t\t\tif (skb == (struct sk_buff *) &rx->sk.sk_receive_queue)\n\t\t\t\tbreak;\n\t\t\tgoto peek_next_packet;\n\t\t}\n\n\t\t_debug(\"eat packet\");\n\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\tBUG();\n\t\trxrpc_free_skb(skb);\n\t}\n\n\t/* end of non-terminal data packet reception for the moment */\n\t_debug(\"end rcv data\");\nout:\n\trelease_sock(&rx->sk);\n\tif (call)\n\t\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d [data]\", copied);\n\treturn copied;\n\n\t/* handle non-DATA messages such as aborts, incoming connections and\n\t * final ACKs */\nreceive_non_data_message:\n\t_debug(\"non-data\");\n\n\tif (skb->mark == RXRPC_SKB_MARK_NEW_CALL) {\n\t\t_debug(\"RECV NEW CALL\");\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_NEW_CALL, 0, &abort_code);\n\t\tif (ret < 0)\n\t\t\tgoto copy_error;\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\t\tBUG();\n\t\t\trxrpc_free_skb(skb);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_USER_CALL_ID,\n\t\t       ullen, &call->user_call_ID);\n\tif (ret < 0)\n\t\tgoto copy_error;\n\tASSERT(test_bit(RXRPC_CALL_HAS_USERID, &call->flags));\n\n\tswitch (skb->mark) {\n\tcase RXRPC_SKB_MARK_DATA:\n\t\tBUG();\n\tcase RXRPC_SKB_MARK_FINAL_ACK:\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_ACK, 0, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_BUSY:\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_BUSY, 0, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_REMOTE_ABORT:\n\t\tabort_code = call->abort_code;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_ABORT, 4, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_NET_ERROR:\n\t\t_debug(\"RECV NET ERROR %d\", sp->error);\n\t\tabort_code = sp->error;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_NET_ERROR, 4, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_LOCAL_ERROR:\n\t\t_debug(\"RECV LOCAL ERROR %d\", sp->error);\n\t\tabort_code = sp->error;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_LOCAL_ERROR, 4,\n\t\t\t       &abort_code);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tif (ret < 0)\n\t\tgoto copy_error;\n\nterminal_message:\n\t_debug(\"terminal\");\n\tmsg->msg_flags &= ~MSG_MORE;\n\tmsg->msg_flags |= MSG_EOR;\n\n\tif (!(flags & MSG_PEEK)) {\n\t\t_net(\"free terminal skb %p\", skb);\n\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\tBUG();\n\t\trxrpc_free_skb(skb);\n\t\trxrpc_remove_user_ID(rx, call);\n\t}\n\n\trelease_sock(&rx->sk);\n\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\ncopy_error:\n\t_debug(\"copy error\");\n\trelease_sock(&rx->sk);\n\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\ncsum_copy_error:\n\t_debug(\"csum error\");\n\trelease_sock(&rx->sk);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\trxrpc_kill_skb(skb);\n\tskb_kill_datagram(&rx->sk, skb, flags);\n\trxrpc_put_call(call);\n\treturn -EAGAIN;\n\nwait_interrupted:\n\tret = sock_intr_errno(timeo);\nwait_error:\n\tfinish_wait(sk_sleep(&rx->sk), &wait);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\tif (copied)\n\t\tcopied = ret;\n\t_leave(\" = %d [waitfail %d]\", copied, ret);\n\treturn copied;\n\n}",
        "code_after_change": "int rxrpc_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct rxrpc_skb_priv *sp;\n\tstruct rxrpc_call *call = NULL, *continue_call = NULL;\n\tstruct rxrpc_sock *rx = rxrpc_sk(sock->sk);\n\tstruct sk_buff *skb;\n\tlong timeo;\n\tint copy, ret, ullen, offset, copied = 0;\n\tu32 abort_code;\n\n\tDEFINE_WAIT(wait);\n\n\t_enter(\",,,%zu,%d\", len, flags);\n\n\tif (flags & (MSG_OOB | MSG_TRUNC))\n\t\treturn -EOPNOTSUPP;\n\n\tullen = msg->msg_flags & MSG_CMSG_COMPAT ? 4 : sizeof(unsigned long);\n\n\ttimeo = sock_rcvtimeo(&rx->sk, flags & MSG_DONTWAIT);\n\tmsg->msg_flags |= MSG_MORE;\n\n\tlock_sock(&rx->sk);\n\n\tfor (;;) {\n\t\t/* return immediately if a client socket has no outstanding\n\t\t * calls */\n\t\tif (RB_EMPTY_ROOT(&rx->calls)) {\n\t\t\tif (copied)\n\t\t\t\tgoto out;\n\t\t\tif (rx->sk.sk_state != RXRPC_SERVER_LISTENING) {\n\t\t\t\trelease_sock(&rx->sk);\n\t\t\t\tif (continue_call)\n\t\t\t\t\trxrpc_put_call(continue_call);\n\t\t\t\treturn -ENODATA;\n\t\t\t}\n\t\t}\n\n\t\t/* get the next message on the Rx queue */\n\t\tskb = skb_peek(&rx->sk.sk_receive_queue);\n\t\tif (!skb) {\n\t\t\t/* nothing remains on the queue */\n\t\t\tif (copied &&\n\t\t\t    (msg->msg_flags & MSG_PEEK || timeo == 0))\n\t\t\t\tgoto out;\n\n\t\t\t/* wait for a message to turn up */\n\t\t\trelease_sock(&rx->sk);\n\t\t\tprepare_to_wait_exclusive(sk_sleep(&rx->sk), &wait,\n\t\t\t\t\t\t  TASK_INTERRUPTIBLE);\n\t\t\tret = sock_error(&rx->sk);\n\t\t\tif (ret)\n\t\t\t\tgoto wait_error;\n\n\t\t\tif (skb_queue_empty(&rx->sk.sk_receive_queue)) {\n\t\t\t\tif (signal_pending(current))\n\t\t\t\t\tgoto wait_interrupted;\n\t\t\t\ttimeo = schedule_timeout(timeo);\n\t\t\t}\n\t\t\tfinish_wait(sk_sleep(&rx->sk), &wait);\n\t\t\tlock_sock(&rx->sk);\n\t\t\tcontinue;\n\t\t}\n\n\tpeek_next_packet:\n\t\tsp = rxrpc_skb(skb);\n\t\tcall = sp->call;\n\t\tASSERT(call != NULL);\n\n\t\t_debug(\"next pkt %s\", rxrpc_pkts[sp->hdr.type]);\n\n\t\t/* make sure we wait for the state to be updated in this call */\n\t\tspin_lock_bh(&call->lock);\n\t\tspin_unlock_bh(&call->lock);\n\n\t\tif (test_bit(RXRPC_CALL_RELEASED, &call->flags)) {\n\t\t\t_debug(\"packet from released call\");\n\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\t\tBUG();\n\t\t\trxrpc_free_skb(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* determine whether to continue last data receive */\n\t\tif (continue_call) {\n\t\t\t_debug(\"maybe cont\");\n\t\t\tif (call != continue_call ||\n\t\t\t    skb->mark != RXRPC_SKB_MARK_DATA) {\n\t\t\t\trelease_sock(&rx->sk);\n\t\t\t\trxrpc_put_call(continue_call);\n\t\t\t\t_leave(\" = %d [noncont]\", copied);\n\t\t\t\treturn copied;\n\t\t\t}\n\t\t}\n\n\t\trxrpc_get_call(call);\n\n\t\t/* copy the peer address and timestamp */\n\t\tif (!continue_call) {\n\t\t\tif (msg->msg_name) {\n\t\t\t\tsize_t len =\n\t\t\t\t\tsizeof(call->conn->trans->peer->srx);\n\t\t\t\tmemcpy(msg->msg_name,\n\t\t\t\t       &call->conn->trans->peer->srx, len);\n\t\t\t\tmsg->msg_namelen = len;\n\t\t\t}\n\t\t\tsock_recv_ts_and_drops(msg, &rx->sk, skb);\n\t\t}\n\n\t\t/* receive the message */\n\t\tif (skb->mark != RXRPC_SKB_MARK_DATA)\n\t\t\tgoto receive_non_data_message;\n\n\t\t_debug(\"recvmsg DATA #%u { %d, %d }\",\n\t\t       ntohl(sp->hdr.seq), skb->len, sp->offset);\n\n\t\tif (!continue_call) {\n\t\t\t/* only set the control data once per recvmsg() */\n\t\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_USER_CALL_ID,\n\t\t\t\t       ullen, &call->user_call_ID);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto copy_error;\n\t\t\tASSERT(test_bit(RXRPC_CALL_HAS_USERID, &call->flags));\n\t\t}\n\n\t\tASSERTCMP(ntohl(sp->hdr.seq), >=, call->rx_data_recv);\n\t\tASSERTCMP(ntohl(sp->hdr.seq), <=, call->rx_data_recv + 1);\n\t\tcall->rx_data_recv = ntohl(sp->hdr.seq);\n\n\t\tASSERTCMP(ntohl(sp->hdr.seq), >, call->rx_data_eaten);\n\n\t\toffset = sp->offset;\n\t\tcopy = skb->len - offset;\n\t\tif (copy > len - copied)\n\t\t\tcopy = len - copied;\n\n\t\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\t\tret = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\t      msg->msg_iov, copy);\n\t\t} else {\n\t\t\tret = skb_copy_and_csum_datagram_iovec(skb, offset,\n\t\t\t\t\t\t\t       msg->msg_iov);\n\t\t\tif (ret == -EINVAL)\n\t\t\t\tgoto csum_copy_error;\n\t\t}\n\n\t\tif (ret < 0)\n\t\t\tgoto copy_error;\n\n\t\t/* handle piecemeal consumption of data packets */\n\t\t_debug(\"copied %d+%d\", copy, copied);\n\n\t\toffset += copy;\n\t\tcopied += copy;\n\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tsp->offset = offset;\n\n\t\tif (sp->offset < skb->len) {\n\t\t\t_debug(\"buffer full\");\n\t\t\tASSERTCMP(copied, ==, len);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* we transferred the whole data packet */\n\t\tif (sp->hdr.flags & RXRPC_LAST_PACKET) {\n\t\t\t_debug(\"last\");\n\t\t\tif (call->conn->out_clientflag) {\n\t\t\t\t /* last byte of reply received */\n\t\t\t\tret = copied;\n\t\t\t\tgoto terminal_message;\n\t\t\t}\n\n\t\t\t/* last bit of request received */\n\t\t\tif (!(flags & MSG_PEEK)) {\n\t\t\t\t_debug(\"eat packet\");\n\t\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) !=\n\t\t\t\t    skb)\n\t\t\t\t\tBUG();\n\t\t\t\trxrpc_free_skb(skb);\n\t\t\t}\n\t\t\tmsg->msg_flags &= ~MSG_MORE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* move on to the next data message */\n\t\t_debug(\"next\");\n\t\tif (!continue_call)\n\t\t\tcontinue_call = sp->call;\n\t\telse\n\t\t\trxrpc_put_call(call);\n\t\tcall = NULL;\n\n\t\tif (flags & MSG_PEEK) {\n\t\t\t_debug(\"peek next\");\n\t\t\tskb = skb->next;\n\t\t\tif (skb == (struct sk_buff *) &rx->sk.sk_receive_queue)\n\t\t\t\tbreak;\n\t\t\tgoto peek_next_packet;\n\t\t}\n\n\t\t_debug(\"eat packet\");\n\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\tBUG();\n\t\trxrpc_free_skb(skb);\n\t}\n\n\t/* end of non-terminal data packet reception for the moment */\n\t_debug(\"end rcv data\");\nout:\n\trelease_sock(&rx->sk);\n\tif (call)\n\t\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d [data]\", copied);\n\treturn copied;\n\n\t/* handle non-DATA messages such as aborts, incoming connections and\n\t * final ACKs */\nreceive_non_data_message:\n\t_debug(\"non-data\");\n\n\tif (skb->mark == RXRPC_SKB_MARK_NEW_CALL) {\n\t\t_debug(\"RECV NEW CALL\");\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_NEW_CALL, 0, &abort_code);\n\t\tif (ret < 0)\n\t\t\tgoto copy_error;\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\t\tBUG();\n\t\t\trxrpc_free_skb(skb);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_USER_CALL_ID,\n\t\t       ullen, &call->user_call_ID);\n\tif (ret < 0)\n\t\tgoto copy_error;\n\tASSERT(test_bit(RXRPC_CALL_HAS_USERID, &call->flags));\n\n\tswitch (skb->mark) {\n\tcase RXRPC_SKB_MARK_DATA:\n\t\tBUG();\n\tcase RXRPC_SKB_MARK_FINAL_ACK:\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_ACK, 0, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_BUSY:\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_BUSY, 0, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_REMOTE_ABORT:\n\t\tabort_code = call->abort_code;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_ABORT, 4, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_NET_ERROR:\n\t\t_debug(\"RECV NET ERROR %d\", sp->error);\n\t\tabort_code = sp->error;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_NET_ERROR, 4, &abort_code);\n\t\tbreak;\n\tcase RXRPC_SKB_MARK_LOCAL_ERROR:\n\t\t_debug(\"RECV LOCAL ERROR %d\", sp->error);\n\t\tabort_code = sp->error;\n\t\tret = put_cmsg(msg, SOL_RXRPC, RXRPC_LOCAL_ERROR, 4,\n\t\t\t       &abort_code);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tif (ret < 0)\n\t\tgoto copy_error;\n\nterminal_message:\n\t_debug(\"terminal\");\n\tmsg->msg_flags &= ~MSG_MORE;\n\tmsg->msg_flags |= MSG_EOR;\n\n\tif (!(flags & MSG_PEEK)) {\n\t\t_net(\"free terminal skb %p\", skb);\n\t\tif (skb_dequeue(&rx->sk.sk_receive_queue) != skb)\n\t\t\tBUG();\n\t\trxrpc_free_skb(skb);\n\t\trxrpc_remove_user_ID(rx, call);\n\t}\n\n\trelease_sock(&rx->sk);\n\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\ncopy_error:\n\t_debug(\"copy error\");\n\trelease_sock(&rx->sk);\n\trxrpc_put_call(call);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\t_leave(\" = %d\", ret);\n\treturn ret;\n\ncsum_copy_error:\n\t_debug(\"csum error\");\n\trelease_sock(&rx->sk);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\trxrpc_kill_skb(skb);\n\tskb_kill_datagram(&rx->sk, skb, flags);\n\trxrpc_put_call(call);\n\treturn -EAGAIN;\n\nwait_interrupted:\n\tret = sock_intr_errno(timeo);\nwait_error:\n\tfinish_wait(sk_sleep(&rx->sk), &wait);\n\tif (continue_call)\n\t\trxrpc_put_call(continue_call);\n\tif (copied)\n\t\tcopied = ret;\n\t_leave(\" = %d [waitfail %d]\", copied, ret);\n\treturn copied;\n\n}",
        "patch": "--- code before\n+++ code after\n@@ -98,10 +98,13 @@\n \n \t\t/* copy the peer address and timestamp */\n \t\tif (!continue_call) {\n-\t\t\tif (msg->msg_name && msg->msg_namelen > 0)\n+\t\t\tif (msg->msg_name) {\n+\t\t\t\tsize_t len =\n+\t\t\t\t\tsizeof(call->conn->trans->peer->srx);\n \t\t\t\tmemcpy(msg->msg_name,\n-\t\t\t\t       &call->conn->trans->peer->srx,\n-\t\t\t\t       sizeof(call->conn->trans->peer->srx));\n+\t\t\t\t       &call->conn->trans->peer->srx, len);\n+\t\t\t\tmsg->msg_namelen = len;\n+\t\t\t}\n \t\t\tsock_recv_ts_and_drops(msg, &rx->sk, skb);\n \t\t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (msg->msg_name) {",
                "\t\t\t\tsize_t len =",
                "\t\t\t\t\tsizeof(call->conn->trans->peer->srx);",
                "\t\t\t\t       &call->conn->trans->peer->srx, len);",
                "\t\t\t\tmsg->msg_namelen = len;",
                "\t\t\t}"
            ],
            "deleted": [
                "\t\t\tif (msg->msg_name && msg->msg_namelen > 0)",
                "\t\t\t\t       &call->conn->trans->peer->srx,",
                "\t\t\t\t       sizeof(call->conn->trans->peer->srx));"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 396
    },
    {
        "cve_id": "CVE-2019-9453",
        "code_before_change": "static int lookup_all_xattrs(struct inode *inode, struct page *ipage,\n\t\t\t\tunsigned int index, unsigned int len,\n\t\t\t\tconst char *name, struct f2fs_xattr_entry **xe,\n\t\t\t\tvoid **base_addr, int *base_size)\n{\n\tvoid *cur_addr, *txattr_addr, *last_addr = NULL;\n\tnid_t xnid = F2FS_I(inode)->i_xattr_nid;\n\tunsigned int size = xnid ? VALID_XATTR_BLOCK_SIZE : 0;\n\tunsigned int inline_size = inline_xattr_size(inode);\n\tint err = 0;\n\n\tif (!size && !inline_size)\n\t\treturn -ENODATA;\n\n\t*base_size = inline_size + size + XATTR_PADDING_SIZE;\n\ttxattr_addr = f2fs_kzalloc(F2FS_I_SB(inode), *base_size, GFP_NOFS);\n\tif (!txattr_addr)\n\t\treturn -ENOMEM;\n\n\t/* read from inline xattr */\n\tif (inline_size) {\n\t\terr = read_inline_xattr(inode, ipage, txattr_addr);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t*xe = __find_inline_xattr(inode, txattr_addr, &last_addr,\n\t\t\t\t\t\tindex, len, name);\n\t\tif (*xe) {\n\t\t\t*base_size = inline_size;\n\t\t\tgoto check;\n\t\t}\n\t}\n\n\t/* read from xattr node block */\n\tif (xnid) {\n\t\terr = read_xattr_block(inode, txattr_addr);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (last_addr)\n\t\tcur_addr = XATTR_HDR(last_addr) - 1;\n\telse\n\t\tcur_addr = txattr_addr;\n\n\t*xe = __find_xattr(cur_addr, index, len, name);\ncheck:\n\tif (IS_XATTR_LAST_ENTRY(*xe)) {\n\t\terr = -ENODATA;\n\t\tgoto out;\n\t}\n\n\t*base_addr = txattr_addr;\n\treturn 0;\nout:\n\tkvfree(txattr_addr);\n\treturn err;\n}",
        "code_after_change": "static int lookup_all_xattrs(struct inode *inode, struct page *ipage,\n\t\t\t\tunsigned int index, unsigned int len,\n\t\t\t\tconst char *name, struct f2fs_xattr_entry **xe,\n\t\t\t\tvoid **base_addr, int *base_size)\n{\n\tvoid *cur_addr, *txattr_addr, *last_txattr_addr;\n\tvoid *last_addr = NULL;\n\tnid_t xnid = F2FS_I(inode)->i_xattr_nid;\n\tunsigned int inline_size = inline_xattr_size(inode);\n\tint err = 0;\n\n\tif (!xnid && !inline_size)\n\t\treturn -ENODATA;\n\n\t*base_size = XATTR_SIZE(xnid, inode) + XATTR_PADDING_SIZE;\n\ttxattr_addr = f2fs_kzalloc(F2FS_I_SB(inode), *base_size, GFP_NOFS);\n\tif (!txattr_addr)\n\t\treturn -ENOMEM;\n\n\tlast_txattr_addr = (void *)txattr_addr + XATTR_SIZE(xnid, inode);\n\n\t/* read from inline xattr */\n\tif (inline_size) {\n\t\terr = read_inline_xattr(inode, ipage, txattr_addr);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t*xe = __find_inline_xattr(inode, txattr_addr, &last_addr,\n\t\t\t\t\t\tindex, len, name);\n\t\tif (*xe) {\n\t\t\t*base_size = inline_size;\n\t\t\tgoto check;\n\t\t}\n\t}\n\n\t/* read from xattr node block */\n\tif (xnid) {\n\t\terr = read_xattr_block(inode, txattr_addr);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (last_addr)\n\t\tcur_addr = XATTR_HDR(last_addr) - 1;\n\telse\n\t\tcur_addr = txattr_addr;\n\n\t*xe = __find_xattr(cur_addr, last_txattr_addr, index, len, name);\n\tif (!*xe) {\n\t\terr = -EFAULT;\n\t\tgoto out;\n\t}\ncheck:\n\tif (IS_XATTR_LAST_ENTRY(*xe)) {\n\t\terr = -ENODATA;\n\t\tgoto out;\n\t}\n\n\t*base_addr = txattr_addr;\n\treturn 0;\nout:\n\tkvfree(txattr_addr);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,19 +3,21 @@\n \t\t\t\tconst char *name, struct f2fs_xattr_entry **xe,\n \t\t\t\tvoid **base_addr, int *base_size)\n {\n-\tvoid *cur_addr, *txattr_addr, *last_addr = NULL;\n+\tvoid *cur_addr, *txattr_addr, *last_txattr_addr;\n+\tvoid *last_addr = NULL;\n \tnid_t xnid = F2FS_I(inode)->i_xattr_nid;\n-\tunsigned int size = xnid ? VALID_XATTR_BLOCK_SIZE : 0;\n \tunsigned int inline_size = inline_xattr_size(inode);\n \tint err = 0;\n \n-\tif (!size && !inline_size)\n+\tif (!xnid && !inline_size)\n \t\treturn -ENODATA;\n \n-\t*base_size = inline_size + size + XATTR_PADDING_SIZE;\n+\t*base_size = XATTR_SIZE(xnid, inode) + XATTR_PADDING_SIZE;\n \ttxattr_addr = f2fs_kzalloc(F2FS_I_SB(inode), *base_size, GFP_NOFS);\n \tif (!txattr_addr)\n \t\treturn -ENOMEM;\n+\n+\tlast_txattr_addr = (void *)txattr_addr + XATTR_SIZE(xnid, inode);\n \n \t/* read from inline xattr */\n \tif (inline_size) {\n@@ -43,7 +45,11 @@\n \telse\n \t\tcur_addr = txattr_addr;\n \n-\t*xe = __find_xattr(cur_addr, index, len, name);\n+\t*xe = __find_xattr(cur_addr, last_txattr_addr, index, len, name);\n+\tif (!*xe) {\n+\t\terr = -EFAULT;\n+\t\tgoto out;\n+\t}\n check:\n \tif (IS_XATTR_LAST_ENTRY(*xe)) {\n \t\terr = -ENODATA;",
        "function_modified_lines": {
            "added": [
                "\tvoid *cur_addr, *txattr_addr, *last_txattr_addr;",
                "\tvoid *last_addr = NULL;",
                "\tif (!xnid && !inline_size)",
                "\t*base_size = XATTR_SIZE(xnid, inode) + XATTR_PADDING_SIZE;",
                "",
                "\tlast_txattr_addr = (void *)txattr_addr + XATTR_SIZE(xnid, inode);",
                "\t*xe = __find_xattr(cur_addr, last_txattr_addr, index, len, name);",
                "\tif (!*xe) {",
                "\t\terr = -EFAULT;",
                "\t\tgoto out;",
                "\t}"
            ],
            "deleted": [
                "\tvoid *cur_addr, *txattr_addr, *last_addr = NULL;",
                "\tunsigned int size = xnid ? VALID_XATTR_BLOCK_SIZE : 0;",
                "\tif (!size && !inline_size)",
                "\t*base_size = inline_size + size + XATTR_PADDING_SIZE;",
                "\t*xe = __find_xattr(cur_addr, index, len, name);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "In the Android kernel in F2FS touch driver there is a possible out of bounds read due to improper input validation. This could lead to local information disclosure with system execution privileges needed. User interaction is not needed for exploitation.",
        "id": 2358
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int atalk_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\t\t size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_at *sat = (struct sockaddr_at *)msg->msg_name;\n\tstruct ddpehdr *ddp;\n\tint copied = 0;\n\tint offset = 0;\n\tint err = 0;\n\tstruct sk_buff *skb;\n\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\t\t\tflags & MSG_DONTWAIT, &err);\n\tlock_sock(sk);\n\n\tif (!skb)\n\t\tgoto out;\n\n\t/* FIXME: use skb->cb to be able to use shared skbs */\n\tddp = ddp_hdr(skb);\n\tcopied = ntohs(ddp->deh_len_hops) & 1023;\n\n\tif (sk->sk_type != SOCK_RAW) {\n\t\toffset = sizeof(*ddp);\n\t\tcopied -= offset;\n\t}\n\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\terr = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);\n\n\tif (!err) {\n\t\tif (sat) {\n\t\t\tsat->sat_family      = AF_APPLETALK;\n\t\t\tsat->sat_port        = ddp->deh_sport;\n\t\t\tsat->sat_addr.s_node = ddp->deh_snode;\n\t\t\tsat->sat_addr.s_net  = ddp->deh_snet;\n\t\t}\n\t\tmsg->msg_namelen = sizeof(*sat);\n\t}\n\n\tskb_free_datagram(sk, skb);\t/* Free the datagram. */\n\nout:\n\trelease_sock(sk);\n\treturn err ? : copied;\n}",
        "code_after_change": "static int atalk_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\t\t size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct ddpehdr *ddp;\n\tint copied = 0;\n\tint offset = 0;\n\tint err = 0;\n\tstruct sk_buff *skb;\n\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\t\t\tflags & MSG_DONTWAIT, &err);\n\tlock_sock(sk);\n\n\tif (!skb)\n\t\tgoto out;\n\n\t/* FIXME: use skb->cb to be able to use shared skbs */\n\tddp = ddp_hdr(skb);\n\tcopied = ntohs(ddp->deh_len_hops) & 1023;\n\n\tif (sk->sk_type != SOCK_RAW) {\n\t\toffset = sizeof(*ddp);\n\t\tcopied -= offset;\n\t}\n\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\terr = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);\n\n\tif (!err && msg->msg_name) {\n\t\tstruct sockaddr_at *sat = msg->msg_name;\n\t\tsat->sat_family      = AF_APPLETALK;\n\t\tsat->sat_port        = ddp->deh_sport;\n\t\tsat->sat_addr.s_node = ddp->deh_snode;\n\t\tsat->sat_addr.s_net  = ddp->deh_snet;\n\t\tmsg->msg_namelen     = sizeof(*sat);\n\t}\n\n\tskb_free_datagram(sk, skb);\t/* Free the datagram. */\n\nout:\n\trelease_sock(sk);\n\treturn err ? : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,6 @@\n \t\t\t size_t size, int flags)\n {\n \tstruct sock *sk = sock->sk;\n-\tstruct sockaddr_at *sat = (struct sockaddr_at *)msg->msg_name;\n \tstruct ddpehdr *ddp;\n \tint copied = 0;\n \tint offset = 0;\n@@ -31,14 +30,13 @@\n \t}\n \terr = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);\n \n-\tif (!err) {\n-\t\tif (sat) {\n-\t\t\tsat->sat_family      = AF_APPLETALK;\n-\t\t\tsat->sat_port        = ddp->deh_sport;\n-\t\t\tsat->sat_addr.s_node = ddp->deh_snode;\n-\t\t\tsat->sat_addr.s_net  = ddp->deh_snet;\n-\t\t}\n-\t\tmsg->msg_namelen = sizeof(*sat);\n+\tif (!err && msg->msg_name) {\n+\t\tstruct sockaddr_at *sat = msg->msg_name;\n+\t\tsat->sat_family      = AF_APPLETALK;\n+\t\tsat->sat_port        = ddp->deh_sport;\n+\t\tsat->sat_addr.s_node = ddp->deh_snode;\n+\t\tsat->sat_addr.s_net  = ddp->deh_snet;\n+\t\tmsg->msg_namelen     = sizeof(*sat);\n \t}\n \n \tskb_free_datagram(sk, skb);\t/* Free the datagram. */",
        "function_modified_lines": {
            "added": [
                "\tif (!err && msg->msg_name) {",
                "\t\tstruct sockaddr_at *sat = msg->msg_name;",
                "\t\tsat->sat_family      = AF_APPLETALK;",
                "\t\tsat->sat_port        = ddp->deh_sport;",
                "\t\tsat->sat_addr.s_node = ddp->deh_snode;",
                "\t\tsat->sat_addr.s_net  = ddp->deh_snet;",
                "\t\tmsg->msg_namelen     = sizeof(*sat);"
            ],
            "deleted": [
                "\tstruct sockaddr_at *sat = (struct sockaddr_at *)msg->msg_name;",
                "\tif (!err) {",
                "\t\tif (sat) {",
                "\t\t\tsat->sat_family      = AF_APPLETALK;",
                "\t\t\tsat->sat_port        = ddp->deh_sport;",
                "\t\t\tsat->sat_addr.s_node = ddp->deh_snode;",
                "\t\t\tsat->sat_addr.s_net  = ddp->deh_snet;",
                "\t\t}",
                "\t\tmsg->msg_namelen = sizeof(*sat);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 370
    },
    {
        "cve_id": "CVE-2013-4270",
        "code_before_change": "static int net_ctl_permissions(struct ctl_table_header *head,\n\t\t\t       struct ctl_table *table)\n{\n\tstruct net *net = container_of(head->set, struct net, sysctls);\n\tkuid_t root_uid = make_kuid(net->user_ns, 0);\n\tkgid_t root_gid = make_kgid(net->user_ns, 0);\n\n\t/* Allow network administrator to have same access as root. */\n\tif (ns_capable(net->user_ns, CAP_NET_ADMIN) ||\n\t    uid_eq(root_uid, current_uid())) {\n\t\tint mode = (table->mode >> 6) & 7;\n\t\treturn (mode << 6) | (mode << 3) | mode;\n\t}\n\t/* Allow netns root group to have the same access as the root group */\n\tif (gid_eq(root_gid, current_gid())) {\n\t\tint mode = (table->mode >> 3) & 7;\n\t\treturn (mode << 3) | mode;\n\t}\n\treturn table->mode;\n}",
        "code_after_change": "static int net_ctl_permissions(struct ctl_table_header *head,\n\t\t\t       struct ctl_table *table)\n{\n\tstruct net *net = container_of(head->set, struct net, sysctls);\n\tkuid_t root_uid = make_kuid(net->user_ns, 0);\n\tkgid_t root_gid = make_kgid(net->user_ns, 0);\n\n\t/* Allow network administrator to have same access as root. */\n\tif (ns_capable(net->user_ns, CAP_NET_ADMIN) ||\n\t    uid_eq(root_uid, current_euid())) {\n\t\tint mode = (table->mode >> 6) & 7;\n\t\treturn (mode << 6) | (mode << 3) | mode;\n\t}\n\t/* Allow netns root group to have the same access as the root group */\n\tif (in_egroup_p(root_gid)) {\n\t\tint mode = (table->mode >> 3) & 7;\n\t\treturn (mode << 3) | mode;\n\t}\n\treturn table->mode;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,12 +7,12 @@\n \n \t/* Allow network administrator to have same access as root. */\n \tif (ns_capable(net->user_ns, CAP_NET_ADMIN) ||\n-\t    uid_eq(root_uid, current_uid())) {\n+\t    uid_eq(root_uid, current_euid())) {\n \t\tint mode = (table->mode >> 6) & 7;\n \t\treturn (mode << 6) | (mode << 3) | mode;\n \t}\n \t/* Allow netns root group to have the same access as the root group */\n-\tif (gid_eq(root_gid, current_gid())) {\n+\tif (in_egroup_p(root_gid)) {\n \t\tint mode = (table->mode >> 3) & 7;\n \t\treturn (mode << 3) | mode;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t    uid_eq(root_uid, current_euid())) {",
                "\tif (in_egroup_p(root_gid)) {"
            ],
            "deleted": [
                "\t    uid_eq(root_uid, current_uid())) {",
                "\tif (gid_eq(root_gid, current_gid())) {"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The net_ctl_permissions function in net/sysctl_net.c in the Linux kernel before 3.11.5 does not properly determine uid and gid values, which allows local users to bypass intended /proc/sys/net restrictions via a crafted application.",
        "id": 291
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int pfkey_recvmsg(struct kiocb *kiocb,\n\t\t\t struct socket *sock, struct msghdr *msg, size_t len,\n\t\t\t int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct pfkey_sock *pfk = pfkey_sk(sk);\n\tstruct sk_buff *skb;\n\tint copied, err;\n\n\terr = -EINVAL;\n\tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))\n\t\tgoto out;\n\n\tmsg->msg_namelen = 0;\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\tskb_reset_transport_header(skb);\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto out_free;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\terr = (flags & MSG_TRUNC) ? skb->len : copied;\n\n\tif (pfk->dump.dump != NULL &&\n\t    3 * atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\tpfkey_do_dump(pfk);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n}",
        "code_after_change": "static int pfkey_recvmsg(struct kiocb *kiocb,\n\t\t\t struct socket *sock, struct msghdr *msg, size_t len,\n\t\t\t int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct pfkey_sock *pfk = pfkey_sk(sk);\n\tstruct sk_buff *skb;\n\tint copied, err;\n\n\terr = -EINVAL;\n\tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))\n\t\tgoto out;\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\tskb_reset_transport_header(skb);\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto out_free;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\terr = (flags & MSG_TRUNC) ? skb->len : copied;\n\n\tif (pfk->dump.dump != NULL &&\n\t    3 * atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\tpfkey_do_dump(pfk);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,6 @@\n \tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))\n \t\tgoto out;\n \n-\tmsg->msg_namelen = 0;\n \tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n \tif (skb == NULL)\n \t\tgoto out;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 386
    },
    {
        "cve_id": "CVE-2017-17805",
        "code_before_change": "static int encrypt(struct blkcipher_desc *desc,\n\t\t   struct scatterlist *dst, struct scatterlist *src,\n\t\t   unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct salsa20_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt_block(desc, &walk, 64);\n\n\tsalsa20_ivsetup(ctx, walk.iv);\n\n\tif (likely(walk.nbytes == nbytes))\n\t{\n\t\tsalsa20_encrypt_bytes(ctx, walk.src.virt.addr,\n\t\t\t\t      walk.dst.virt.addr, nbytes);\n\t\treturn blkcipher_walk_done(desc, &walk, 0);\n\t}\n\n\twhile (walk.nbytes >= 64) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.src.virt.addr,\n\t\t\t\t      walk.dst.virt.addr,\n\t\t\t\t      walk.nbytes - (walk.nbytes % 64));\n\t\terr = blkcipher_walk_done(desc, &walk, walk.nbytes % 64);\n\t}\n\n\tif (walk.nbytes) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.src.virt.addr,\n\t\t\t\t      walk.dst.virt.addr, walk.nbytes);\n\t\terr = blkcipher_walk_done(desc, &walk, 0);\n\t}\n\n\treturn err;\n}",
        "code_after_change": "static int encrypt(struct blkcipher_desc *desc,\n\t\t   struct scatterlist *dst, struct scatterlist *src,\n\t\t   unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct salsa20_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt_block(desc, &walk, 64);\n\n\tsalsa20_ivsetup(ctx, walk.iv);\n\n\twhile (walk.nbytes >= 64) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.src.virt.addr,\n\t\t\t\t      walk.dst.virt.addr,\n\t\t\t\t      walk.nbytes - (walk.nbytes % 64));\n\t\terr = blkcipher_walk_done(desc, &walk, walk.nbytes % 64);\n\t}\n\n\tif (walk.nbytes) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.src.virt.addr,\n\t\t\t\t      walk.dst.virt.addr, walk.nbytes);\n\t\terr = blkcipher_walk_done(desc, &walk, 0);\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,13 +11,6 @@\n \terr = blkcipher_walk_virt_block(desc, &walk, 64);\n \n \tsalsa20_ivsetup(ctx, walk.iv);\n-\n-\tif (likely(walk.nbytes == nbytes))\n-\t{\n-\t\tsalsa20_encrypt_bytes(ctx, walk.src.virt.addr,\n-\t\t\t\t      walk.dst.virt.addr, nbytes);\n-\t\treturn blkcipher_walk_done(desc, &walk, 0);\n-\t}\n \n \twhile (walk.nbytes >= 64) {\n \t\tsalsa20_encrypt_bytes(ctx, walk.src.virt.addr,",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tif (likely(walk.nbytes == nbytes))",
                "\t{",
                "\t\tsalsa20_encrypt_bytes(ctx, walk.src.virt.addr,",
                "\t\t\t\t      walk.dst.virt.addr, nbytes);",
                "\t\treturn blkcipher_walk_done(desc, &walk, 0);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Salsa20 encryption algorithm in the Linux kernel before 4.14.8 does not correctly handle zero-length inputs, allowing a local attacker able to use the AF_ALG-based skcipher interface (CONFIG_CRYPTO_USER_API_SKCIPHER) to cause a denial of service (uninitialized-memory free and kernel crash) or have unspecified other impact by executing a crafted sequence of system calls that use the blkcipher_walk API. Both the generic implementation (crypto/salsa20_generic.c) and x86 implementation (arch/x86/crypto/salsa20_glue.c) of Salsa20 were vulnerable.",
        "id": 1373
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int unix_stream_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size,\n\t\t\t       int flags)\n{\n\tstruct sock_iocb *siocb = kiocb_to_siocb(iocb);\n\tstruct scm_cookie tmp_scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sockaddr_un *sunaddr = msg->msg_name;\n\tint copied = 0;\n\tint check_creds = 0;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\tgoto out;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\ttarget = sock_rcvlowat(sk, flags&MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, flags&MSG_DONTWAIT);\n\n\tmsg->msg_namelen = 0;\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\n\tif (!siocb->scm) {\n\t\tsiocb->scm = &tmp_scm;\n\t\tmemset(&tmp_scm, 0, sizeof(tmp_scm));\n\t}\n\n\terr = mutex_lock_interruptible(&u->readlock);\n\tif (err) {\n\t\terr = sock_intr_errno(timeo);\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tint chunk;\n\t\tstruct sk_buff *skb, *last;\n\n\t\tunix_state_lock(sk);\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\nagain:\n\t\tif (skb == NULL) {\n\t\t\tunix_sk(sk)->recursion_level = 0;\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\t\t\tmutex_unlock(&u->readlock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last);\n\n\t\t\tif (signal_pending(current)\n\t\t\t    ||  mutex_lock_interruptible(&u->readlock)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tcontinue;\n unlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\tskip = sk_peek_offset(sk, flags);\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif ((UNIXCB(skb).pid  != siocb->scm->pid) ||\n\t\t\t    !uid_eq(UNIXCB(skb).uid, siocb->scm->creds.uid) ||\n\t\t\t    !gid_eq(UNIXCB(skb).gid, siocb->scm->creds.gid))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(siocb->scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tcheck_creds = 1;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (sunaddr) {\n\t\t\tunix_copy_addr(msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tif (skb_copy_datagram_iovec(skb, UNIXCB(skb).consumed + skip,\n\t\t\t\t\t    msg->msg_iov, chunk)) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tunix_detach_fds(siocb->scm, skb);\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (siocb->scm->fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tsiocb->scm->fp = scm_fp_dup(UNIXCB(skb).fp);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->readlock);\n\tscm_recv(sock, msg, siocb->scm, flags);\nout:\n\treturn copied ? : err;\n}",
        "code_after_change": "static int unix_stream_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size,\n\t\t\t       int flags)\n{\n\tstruct sock_iocb *siocb = kiocb_to_siocb(iocb);\n\tstruct scm_cookie tmp_scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sockaddr_un *sunaddr = msg->msg_name;\n\tint copied = 0;\n\tint check_creds = 0;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\tgoto out;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\ttarget = sock_rcvlowat(sk, flags&MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, flags&MSG_DONTWAIT);\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\n\tif (!siocb->scm) {\n\t\tsiocb->scm = &tmp_scm;\n\t\tmemset(&tmp_scm, 0, sizeof(tmp_scm));\n\t}\n\n\terr = mutex_lock_interruptible(&u->readlock);\n\tif (err) {\n\t\terr = sock_intr_errno(timeo);\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tint chunk;\n\t\tstruct sk_buff *skb, *last;\n\n\t\tunix_state_lock(sk);\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\nagain:\n\t\tif (skb == NULL) {\n\t\t\tunix_sk(sk)->recursion_level = 0;\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\t\t\tmutex_unlock(&u->readlock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last);\n\n\t\t\tif (signal_pending(current)\n\t\t\t    ||  mutex_lock_interruptible(&u->readlock)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tcontinue;\n unlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\tskip = sk_peek_offset(sk, flags);\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif ((UNIXCB(skb).pid  != siocb->scm->pid) ||\n\t\t\t    !uid_eq(UNIXCB(skb).uid, siocb->scm->creds.uid) ||\n\t\t\t    !gid_eq(UNIXCB(skb).gid, siocb->scm->creds.gid))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(siocb->scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tcheck_creds = 1;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (sunaddr) {\n\t\t\tunix_copy_addr(msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tif (skb_copy_datagram_iovec(skb, UNIXCB(skb).consumed + skip,\n\t\t\t\t\t    msg->msg_iov, chunk)) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tunix_detach_fds(siocb->scm, skb);\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (siocb->scm->fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tsiocb->scm->fp = scm_fp_dup(UNIXCB(skb).fp);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->readlock);\n\tscm_recv(sock, msg, siocb->scm, flags);\nout:\n\treturn copied ? : err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -24,8 +24,6 @@\n \n \ttarget = sock_rcvlowat(sk, flags&MSG_WAITALL, size);\n \ttimeo = sock_rcvtimeo(sk, flags&MSG_DONTWAIT);\n-\n-\tmsg->msg_namelen = 0;\n \n \t/* Lock the socket to prevent queue disordering\n \t * while sleeps in memcpy_tomsg",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 401
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int llc_ui_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sockaddr_llc *uaddr = (struct sockaddr_llc *)msg->msg_name;\n\tconst int nonblock = flags & MSG_DONTWAIT;\n\tstruct sk_buff *skb = NULL;\n\tstruct sock *sk = sock->sk;\n\tstruct llc_sock *llc = llc_sk(sk);\n\tunsigned long cpu_flags;\n\tsize_t copied = 0;\n\tu32 peek_seq = 0;\n\tu32 *seq;\n\tunsigned long used;\n\tint target;\t/* Read at least this many bytes */\n\tlong timeo;\n\n\tmsg->msg_namelen = 0;\n\n\tlock_sock(sk);\n\tcopied = -ENOTCONN;\n\tif (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\tseq = &llc->copied_seq;\n\tif (flags & MSG_PEEK) {\n\t\tpeek_seq = llc->copied_seq;\n\t\tseq = &peek_seq;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tcopied = 0;\n\n\tdo {\n\t\tu32 offset;\n\n\t\t/*\n\t\t * We need to check signals first, to get correct SIGURG\n\t\t * handling. FIXME: Need to check this doesn't impact 1003.1g\n\t\t * and move it down to the bottom of the loop\n\t\t */\n\t\tif (signal_pending(current)) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Next get a buffer. */\n\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb) {\n\t\t\toffset = *seq;\n\t\t\tgoto found_ok_skb;\n\t\t}\n\t\t/* Well, if we have backlog, try to process it now yet. */\n\n\t\tif (copied >= target && !sk->sk_backlog.tail)\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err ||\n\t\t\t    sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    !timeo ||\n\t\t\t    (flags & MSG_PEEK))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_CLOSE) {\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * This occurs when user tries to read\n\t\t\t\t\t * from never connected socket.\n\t\t\t\t\t */\n\t\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (copied >= target) { /* Do not sleep, just process backlog. */\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t} else\n\t\t\tsk_wait_data(sk, &timeo);\n\n\t\tif ((flags & MSG_PEEK) && peek_seq != llc->copied_seq) {\n\t\t\tnet_dbg_ratelimited(\"LLC(%s:%d): Application bug, race in MSG_PEEK\\n\",\n\t\t\t\t\t    current->comm,\n\t\t\t\t\t    task_pid_nr(current));\n\t\t\tpeek_seq = llc->copied_seq;\n\t\t}\n\t\tcontinue;\n\tfound_ok_skb:\n\t\t/* Ok so how much can we use? */\n\t\tused = skb->len - offset;\n\t\tif (len < used)\n\t\t\tused = len;\n\n\t\tif (!(flags & MSG_TRUNC)) {\n\t\t\tint rc = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\t\t msg->msg_iov, used);\n\t\t\tif (rc) {\n\t\t\t\t/* Exception. Bailout! */\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t*seq += used;\n\t\tcopied += used;\n\t\tlen -= used;\n\n\t\t/* For non stream protcols we get one packet per recvmsg call */\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\tgoto copy_uaddr;\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t\t}\n\n\t\t/* Partial read */\n\t\tif (used + offset < skb->len)\n\t\t\tcontinue;\n\t} while (len > 0);\n\nout:\n\trelease_sock(sk);\n\treturn copied;\ncopy_uaddr:\n\tif (uaddr != NULL && skb != NULL) {\n\t\tmemcpy(uaddr, llc_ui_skb_cb(skb), sizeof(*uaddr));\n\t\tmsg->msg_namelen = sizeof(*uaddr);\n\t}\n\tif (llc_sk(sk)->cmsg_flags)\n\t\tllc_cmsg_rcv(msg, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t}\n\n\tgoto out;\n}",
        "code_after_change": "static int llc_ui_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sockaddr_llc *uaddr = (struct sockaddr_llc *)msg->msg_name;\n\tconst int nonblock = flags & MSG_DONTWAIT;\n\tstruct sk_buff *skb = NULL;\n\tstruct sock *sk = sock->sk;\n\tstruct llc_sock *llc = llc_sk(sk);\n\tunsigned long cpu_flags;\n\tsize_t copied = 0;\n\tu32 peek_seq = 0;\n\tu32 *seq;\n\tunsigned long used;\n\tint target;\t/* Read at least this many bytes */\n\tlong timeo;\n\n\tlock_sock(sk);\n\tcopied = -ENOTCONN;\n\tif (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\tseq = &llc->copied_seq;\n\tif (flags & MSG_PEEK) {\n\t\tpeek_seq = llc->copied_seq;\n\t\tseq = &peek_seq;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tcopied = 0;\n\n\tdo {\n\t\tu32 offset;\n\n\t\t/*\n\t\t * We need to check signals first, to get correct SIGURG\n\t\t * handling. FIXME: Need to check this doesn't impact 1003.1g\n\t\t * and move it down to the bottom of the loop\n\t\t */\n\t\tif (signal_pending(current)) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Next get a buffer. */\n\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb) {\n\t\t\toffset = *seq;\n\t\t\tgoto found_ok_skb;\n\t\t}\n\t\t/* Well, if we have backlog, try to process it now yet. */\n\n\t\tif (copied >= target && !sk->sk_backlog.tail)\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err ||\n\t\t\t    sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    !timeo ||\n\t\t\t    (flags & MSG_PEEK))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_CLOSE) {\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * This occurs when user tries to read\n\t\t\t\t\t * from never connected socket.\n\t\t\t\t\t */\n\t\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (copied >= target) { /* Do not sleep, just process backlog. */\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t} else\n\t\t\tsk_wait_data(sk, &timeo);\n\n\t\tif ((flags & MSG_PEEK) && peek_seq != llc->copied_seq) {\n\t\t\tnet_dbg_ratelimited(\"LLC(%s:%d): Application bug, race in MSG_PEEK\\n\",\n\t\t\t\t\t    current->comm,\n\t\t\t\t\t    task_pid_nr(current));\n\t\t\tpeek_seq = llc->copied_seq;\n\t\t}\n\t\tcontinue;\n\tfound_ok_skb:\n\t\t/* Ok so how much can we use? */\n\t\tused = skb->len - offset;\n\t\tif (len < used)\n\t\t\tused = len;\n\n\t\tif (!(flags & MSG_TRUNC)) {\n\t\t\tint rc = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\t\t msg->msg_iov, used);\n\t\t\tif (rc) {\n\t\t\t\t/* Exception. Bailout! */\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t*seq += used;\n\t\tcopied += used;\n\t\tlen -= used;\n\n\t\t/* For non stream protcols we get one packet per recvmsg call */\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\tgoto copy_uaddr;\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t\t}\n\n\t\t/* Partial read */\n\t\tif (used + offset < skb->len)\n\t\t\tcontinue;\n\t} while (len > 0);\n\nout:\n\trelease_sock(sk);\n\treturn copied;\ncopy_uaddr:\n\tif (uaddr != NULL && skb != NULL) {\n\t\tmemcpy(uaddr, llc_ui_skb_cb(skb), sizeof(*uaddr));\n\t\tmsg->msg_namelen = sizeof(*uaddr);\n\t}\n\tif (llc_sk(sk)->cmsg_flags)\n\t\tllc_cmsg_rcv(msg, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t}\n\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,8 +13,6 @@\n \tunsigned long used;\n \tint target;\t/* Read at least this many bytes */\n \tlong timeo;\n-\n-\tmsg->msg_namelen = 0;\n \n \tlock_sock(sk);\n \tcopied = -ENOTCONN;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 388
    },
    {
        "cve_id": "CVE-2018-14656",
        "code_before_change": "static inline void\nshow_signal_msg(struct pt_regs *regs, unsigned long error_code,\n\t\tunsigned long address, struct task_struct *tsk)\n{\n\tconst char *loglvl = task_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG;\n\n\tif (!unhandled_signal(tsk, SIGSEGV))\n\t\treturn;\n\n\tif (!printk_ratelimit())\n\t\treturn;\n\n\tprintk(\"%s%s[%d]: segfault at %lx ip %px sp %px error %lx\",\n\t\tloglvl, tsk->comm, task_pid_nr(tsk), address,\n\t\t(void *)regs->ip, (void *)regs->sp, error_code);\n\n\tprint_vma_addr(KERN_CONT \" in \", regs->ip);\n\n\tprintk(KERN_CONT \"\\n\");\n\n\tshow_opcodes((u8 *)regs->ip, loglvl);\n}",
        "code_after_change": "static inline void\nshow_signal_msg(struct pt_regs *regs, unsigned long error_code,\n\t\tunsigned long address, struct task_struct *tsk)\n{\n\tconst char *loglvl = task_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG;\n\n\tif (!unhandled_signal(tsk, SIGSEGV))\n\t\treturn;\n\n\tif (!printk_ratelimit())\n\t\treturn;\n\n\tprintk(\"%s%s[%d]: segfault at %lx ip %px sp %px error %lx\",\n\t\tloglvl, tsk->comm, task_pid_nr(tsk), address,\n\t\t(void *)regs->ip, (void *)regs->sp, error_code);\n\n\tprint_vma_addr(KERN_CONT \" in \", regs->ip);\n\n\tprintk(KERN_CONT \"\\n\");\n\n\tshow_opcodes(regs, loglvl);\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,5 +18,5 @@\n \n \tprintk(KERN_CONT \"\\n\");\n \n-\tshow_opcodes((u8 *)regs->ip, loglvl);\n+\tshow_opcodes(regs, loglvl);\n }",
        "function_modified_lines": {
            "added": [
                "\tshow_opcodes(regs, loglvl);"
            ],
            "deleted": [
                "\tshow_opcodes((u8 *)regs->ip, loglvl);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A missing address check in the callers of the show_opcodes() in the Linux kernel allows an attacker to dump the kernel memory at an arbitrary kernel address into the dmesg log.",
        "id": 1704
    },
    {
        "cve_id": "CVE-2014-3645",
        "code_before_change": "static bool nested_vmx_exit_handled(struct kvm_vcpu *vcpu)\n{\n\tu32 intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tu32 exit_reason = vmx->exit_reason;\n\n\tif (vmx->nested.nested_run_pending)\n\t\treturn 0;\n\n\tif (unlikely(vmx->fail)) {\n\t\tpr_info_ratelimited(\"%s failed vm entry %x\\n\", __func__,\n\t\t\t\t    vmcs_read32(VM_INSTRUCTION_ERROR));\n\t\treturn 1;\n\t}\n\n\tswitch (exit_reason) {\n\tcase EXIT_REASON_EXCEPTION_NMI:\n\t\tif (!is_exception(intr_info))\n\t\t\treturn 0;\n\t\telse if (is_page_fault(intr_info))\n\t\t\treturn enable_ept;\n\t\treturn vmcs12->exception_bitmap &\n\t\t\t\t(1u << (intr_info & INTR_INFO_VECTOR_MASK));\n\tcase EXIT_REASON_EXTERNAL_INTERRUPT:\n\t\treturn 0;\n\tcase EXIT_REASON_TRIPLE_FAULT:\n\t\treturn 1;\n\tcase EXIT_REASON_PENDING_INTERRUPT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_INTR_PENDING);\n\tcase EXIT_REASON_NMI_WINDOW:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_NMI_PENDING);\n\tcase EXIT_REASON_TASK_SWITCH:\n\t\treturn 1;\n\tcase EXIT_REASON_CPUID:\n\t\treturn 1;\n\tcase EXIT_REASON_HLT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_HLT_EXITING);\n\tcase EXIT_REASON_INVD:\n\t\treturn 1;\n\tcase EXIT_REASON_INVLPG:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);\n\tcase EXIT_REASON_RDPMC:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDPMC_EXITING);\n\tcase EXIT_REASON_RDTSC:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDTSC_EXITING);\n\tcase EXIT_REASON_VMCALL: case EXIT_REASON_VMCLEAR:\n\tcase EXIT_REASON_VMLAUNCH: case EXIT_REASON_VMPTRLD:\n\tcase EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:\n\tcase EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:\n\tcase EXIT_REASON_VMOFF: case EXIT_REASON_VMON:\n\t\t/*\n\t\t * VMX instructions trap unconditionally. This allows L1 to\n\t\t * emulate them for its L2 guest, i.e., allows 3-level nesting!\n\t\t */\n\t\treturn 1;\n\tcase EXIT_REASON_CR_ACCESS:\n\t\treturn nested_vmx_exit_handled_cr(vcpu, vmcs12);\n\tcase EXIT_REASON_DR_ACCESS:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MOV_DR_EXITING);\n\tcase EXIT_REASON_IO_INSTRUCTION:\n\t\treturn nested_vmx_exit_handled_io(vcpu, vmcs12);\n\tcase EXIT_REASON_MSR_READ:\n\tcase EXIT_REASON_MSR_WRITE:\n\t\treturn nested_vmx_exit_handled_msr(vcpu, vmcs12, exit_reason);\n\tcase EXIT_REASON_INVALID_STATE:\n\t\treturn 1;\n\tcase EXIT_REASON_MWAIT_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MWAIT_EXITING);\n\tcase EXIT_REASON_MONITOR_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MONITOR_EXITING);\n\tcase EXIT_REASON_PAUSE_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_PAUSE_EXITING) ||\n\t\t\tnested_cpu_has2(vmcs12,\n\t\t\t\tSECONDARY_EXEC_PAUSE_LOOP_EXITING);\n\tcase EXIT_REASON_MCE_DURING_VMENTRY:\n\t\treturn 0;\n\tcase EXIT_REASON_TPR_BELOW_THRESHOLD:\n\t\treturn 1;\n\tcase EXIT_REASON_APIC_ACCESS:\n\t\treturn nested_cpu_has2(vmcs12,\n\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES);\n\tcase EXIT_REASON_EPT_VIOLATION:\n\tcase EXIT_REASON_EPT_MISCONFIG:\n\t\treturn 0;\n\tcase EXIT_REASON_PREEMPTION_TIMER:\n\t\treturn vmcs12->pin_based_vm_exec_control &\n\t\t\tPIN_BASED_VMX_PREEMPTION_TIMER;\n\tcase EXIT_REASON_WBINVD:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_WBINVD_EXITING);\n\tcase EXIT_REASON_XSETBV:\n\t\treturn 1;\n\tdefault:\n\t\treturn 1;\n\t}\n}",
        "code_after_change": "static bool nested_vmx_exit_handled(struct kvm_vcpu *vcpu)\n{\n\tu32 intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tu32 exit_reason = vmx->exit_reason;\n\n\tif (vmx->nested.nested_run_pending)\n\t\treturn 0;\n\n\tif (unlikely(vmx->fail)) {\n\t\tpr_info_ratelimited(\"%s failed vm entry %x\\n\", __func__,\n\t\t\t\t    vmcs_read32(VM_INSTRUCTION_ERROR));\n\t\treturn 1;\n\t}\n\n\tswitch (exit_reason) {\n\tcase EXIT_REASON_EXCEPTION_NMI:\n\t\tif (!is_exception(intr_info))\n\t\t\treturn 0;\n\t\telse if (is_page_fault(intr_info))\n\t\t\treturn enable_ept;\n\t\treturn vmcs12->exception_bitmap &\n\t\t\t\t(1u << (intr_info & INTR_INFO_VECTOR_MASK));\n\tcase EXIT_REASON_EXTERNAL_INTERRUPT:\n\t\treturn 0;\n\tcase EXIT_REASON_TRIPLE_FAULT:\n\t\treturn 1;\n\tcase EXIT_REASON_PENDING_INTERRUPT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_INTR_PENDING);\n\tcase EXIT_REASON_NMI_WINDOW:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_NMI_PENDING);\n\tcase EXIT_REASON_TASK_SWITCH:\n\t\treturn 1;\n\tcase EXIT_REASON_CPUID:\n\t\treturn 1;\n\tcase EXIT_REASON_HLT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_HLT_EXITING);\n\tcase EXIT_REASON_INVD:\n\t\treturn 1;\n\tcase EXIT_REASON_INVLPG:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);\n\tcase EXIT_REASON_RDPMC:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDPMC_EXITING);\n\tcase EXIT_REASON_RDTSC:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDTSC_EXITING);\n\tcase EXIT_REASON_VMCALL: case EXIT_REASON_VMCLEAR:\n\tcase EXIT_REASON_VMLAUNCH: case EXIT_REASON_VMPTRLD:\n\tcase EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:\n\tcase EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:\n\tcase EXIT_REASON_VMOFF: case EXIT_REASON_VMON:\n\tcase EXIT_REASON_INVEPT:\n\t\t/*\n\t\t * VMX instructions trap unconditionally. This allows L1 to\n\t\t * emulate them for its L2 guest, i.e., allows 3-level nesting!\n\t\t */\n\t\treturn 1;\n\tcase EXIT_REASON_CR_ACCESS:\n\t\treturn nested_vmx_exit_handled_cr(vcpu, vmcs12);\n\tcase EXIT_REASON_DR_ACCESS:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MOV_DR_EXITING);\n\tcase EXIT_REASON_IO_INSTRUCTION:\n\t\treturn nested_vmx_exit_handled_io(vcpu, vmcs12);\n\tcase EXIT_REASON_MSR_READ:\n\tcase EXIT_REASON_MSR_WRITE:\n\t\treturn nested_vmx_exit_handled_msr(vcpu, vmcs12, exit_reason);\n\tcase EXIT_REASON_INVALID_STATE:\n\t\treturn 1;\n\tcase EXIT_REASON_MWAIT_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MWAIT_EXITING);\n\tcase EXIT_REASON_MONITOR_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MONITOR_EXITING);\n\tcase EXIT_REASON_PAUSE_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_PAUSE_EXITING) ||\n\t\t\tnested_cpu_has2(vmcs12,\n\t\t\t\tSECONDARY_EXEC_PAUSE_LOOP_EXITING);\n\tcase EXIT_REASON_MCE_DURING_VMENTRY:\n\t\treturn 0;\n\tcase EXIT_REASON_TPR_BELOW_THRESHOLD:\n\t\treturn 1;\n\tcase EXIT_REASON_APIC_ACCESS:\n\t\treturn nested_cpu_has2(vmcs12,\n\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES);\n\tcase EXIT_REASON_EPT_VIOLATION:\n\tcase EXIT_REASON_EPT_MISCONFIG:\n\t\treturn 0;\n\tcase EXIT_REASON_PREEMPTION_TIMER:\n\t\treturn vmcs12->pin_based_vm_exec_control &\n\t\t\tPIN_BASED_VMX_PREEMPTION_TIMER;\n\tcase EXIT_REASON_WBINVD:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_WBINVD_EXITING);\n\tcase EXIT_REASON_XSETBV:\n\t\treturn 1;\n\tdefault:\n\t\treturn 1;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -49,6 +49,7 @@\n \tcase EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:\n \tcase EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:\n \tcase EXIT_REASON_VMOFF: case EXIT_REASON_VMON:\n+\tcase EXIT_REASON_INVEPT:\n \t\t/*\n \t\t * VMX instructions trap unconditionally. This allows L1 to\n \t\t * emulate them for its L2 guest, i.e., allows 3-level nesting!",
        "function_modified_lines": {
            "added": [
                "\tcase EXIT_REASON_INVEPT:"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "arch/x86/kvm/vmx.c in the KVM subsystem in the Linux kernel before 3.12 does not have an exit handler for the INVEPT instruction, which allows guest OS users to cause a denial of service (guest OS crash) via a crafted application.",
        "id": 530
    },
    {
        "cve_id": "CVE-2016-6197",
        "code_before_change": "static int ovl_rename2(struct inode *olddir, struct dentry *old,\n\t\t       struct inode *newdir, struct dentry *new,\n\t\t       unsigned int flags)\n{\n\tint err;\n\tenum ovl_path_type old_type;\n\tenum ovl_path_type new_type;\n\tstruct dentry *old_upperdir;\n\tstruct dentry *new_upperdir;\n\tstruct dentry *olddentry;\n\tstruct dentry *newdentry;\n\tstruct dentry *trap;\n\tbool old_opaque;\n\tbool new_opaque;\n\tbool new_create = false;\n\tbool cleanup_whiteout = false;\n\tbool overwrite = !(flags & RENAME_EXCHANGE);\n\tbool is_dir = d_is_dir(old);\n\tbool new_is_dir = false;\n\tstruct dentry *opaquedir = NULL;\n\tconst struct cred *old_cred = NULL;\n\tstruct cred *override_cred = NULL;\n\n\terr = -EINVAL;\n\tif (flags & ~(RENAME_EXCHANGE | RENAME_NOREPLACE))\n\t\tgoto out;\n\n\tflags &= ~RENAME_NOREPLACE;\n\n\terr = ovl_check_sticky(old);\n\tif (err)\n\t\tgoto out;\n\n\t/* Don't copy up directory trees */\n\told_type = ovl_path_type(old);\n\terr = -EXDEV;\n\tif (OVL_TYPE_MERGE_OR_LOWER(old_type) && is_dir)\n\t\tgoto out;\n\n\tif (new->d_inode) {\n\t\terr = ovl_check_sticky(new);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (d_is_dir(new))\n\t\t\tnew_is_dir = true;\n\n\t\tnew_type = ovl_path_type(new);\n\t\terr = -EXDEV;\n\t\tif (!overwrite && OVL_TYPE_MERGE_OR_LOWER(new_type) && new_is_dir)\n\t\t\tgoto out;\n\n\t\terr = 0;\n\t\tif (!OVL_TYPE_UPPER(new_type) && !OVL_TYPE_UPPER(old_type)) {\n\t\t\tif (ovl_dentry_lower(old)->d_inode ==\n\t\t\t    ovl_dentry_lower(new)->d_inode)\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (OVL_TYPE_UPPER(new_type) && OVL_TYPE_UPPER(old_type)) {\n\t\t\tif (ovl_dentry_upper(old)->d_inode ==\n\t\t\t    ovl_dentry_upper(new)->d_inode)\n\t\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tif (ovl_dentry_is_opaque(new))\n\t\t\tnew_type = __OVL_PATH_UPPER;\n\t\telse\n\t\t\tnew_type = __OVL_PATH_UPPER | __OVL_PATH_PURE;\n\t}\n\n\terr = ovl_want_write(old);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_copy_up(old);\n\tif (err)\n\t\tgoto out_drop_write;\n\n\terr = ovl_copy_up(new->d_parent);\n\tif (err)\n\t\tgoto out_drop_write;\n\tif (!overwrite) {\n\t\terr = ovl_copy_up(new);\n\t\tif (err)\n\t\t\tgoto out_drop_write;\n\t}\n\n\told_opaque = !OVL_TYPE_PURE_UPPER(old_type);\n\tnew_opaque = !OVL_TYPE_PURE_UPPER(new_type);\n\n\tif (old_opaque || new_opaque) {\n\t\terr = -ENOMEM;\n\t\toverride_cred = prepare_creds();\n\t\tif (!override_cred)\n\t\t\tgoto out_drop_write;\n\n\t\t/*\n\t\t * CAP_SYS_ADMIN for setting xattr on whiteout, opaque dir\n\t\t * CAP_DAC_OVERRIDE for create in workdir\n\t\t * CAP_FOWNER for removing whiteout from sticky dir\n\t\t * CAP_FSETID for chmod of opaque dir\n\t\t * CAP_CHOWN for chown of opaque dir\n\t\t */\n\t\tcap_raise(override_cred->cap_effective, CAP_SYS_ADMIN);\n\t\tcap_raise(override_cred->cap_effective, CAP_DAC_OVERRIDE);\n\t\tcap_raise(override_cred->cap_effective, CAP_FOWNER);\n\t\tcap_raise(override_cred->cap_effective, CAP_FSETID);\n\t\tcap_raise(override_cred->cap_effective, CAP_CHOWN);\n\t\told_cred = override_creds(override_cred);\n\t}\n\n\tif (overwrite && OVL_TYPE_MERGE_OR_LOWER(new_type) && new_is_dir) {\n\t\topaquedir = ovl_check_empty_and_clear(new);\n\t\terr = PTR_ERR(opaquedir);\n\t\tif (IS_ERR(opaquedir)) {\n\t\t\topaquedir = NULL;\n\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\tif (overwrite) {\n\t\tif (old_opaque) {\n\t\t\tif (new->d_inode || !new_opaque) {\n\t\t\t\t/* Whiteout source */\n\t\t\t\tflags |= RENAME_WHITEOUT;\n\t\t\t} else {\n\t\t\t\t/* Switch whiteouts */\n\t\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\t}\n\t\t} else if (is_dir && !new->d_inode && new_opaque) {\n\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\tcleanup_whiteout = true;\n\t\t}\n\t}\n\n\told_upperdir = ovl_dentry_upper(old->d_parent);\n\tnew_upperdir = ovl_dentry_upper(new->d_parent);\n\n\ttrap = lock_rename(new_upperdir, old_upperdir);\n\n\tolddentry = ovl_dentry_upper(old);\n\tnewdentry = ovl_dentry_upper(new);\n\tif (newdentry) {\n\t\tif (opaquedir) {\n\t\t\tnewdentry = opaquedir;\n\t\t\topaquedir = NULL;\n\t\t} else {\n\t\t\tdget(newdentry);\n\t\t}\n\t} else {\n\t\tnew_create = true;\n\t\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n\t\t\t\t\t   new->d_name.len);\n\t\terr = PTR_ERR(newdentry);\n\t\tif (IS_ERR(newdentry))\n\t\t\tgoto out_unlock;\n\t}\n\n\terr = -ESTALE;\n\tif (olddentry->d_parent != old_upperdir)\n\t\tgoto out_dput;\n\tif (newdentry->d_parent != new_upperdir)\n\t\tgoto out_dput;\n\tif (olddentry == trap)\n\t\tgoto out_dput;\n\tif (newdentry == trap)\n\t\tgoto out_dput;\n\n\tif (is_dir && !old_opaque && new_opaque) {\n\t\terr = ovl_set_opaque(olddentry);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\tif (!overwrite && new_is_dir && old_opaque && !new_opaque) {\n\t\terr = ovl_set_opaque(newdentry);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\n\tif (old_opaque || new_opaque) {\n\t\terr = ovl_do_rename(old_upperdir->d_inode, olddentry,\n\t\t\t\t    new_upperdir->d_inode, newdentry,\n\t\t\t\t    flags);\n\t} else {\n\t\t/* No debug for the plain case */\n\t\tBUG_ON(flags & ~RENAME_EXCHANGE);\n\t\terr = vfs_rename(old_upperdir->d_inode, olddentry,\n\t\t\t\t new_upperdir->d_inode, newdentry,\n\t\t\t\t NULL, flags);\n\t}\n\n\tif (err) {\n\t\tif (is_dir && !old_opaque && new_opaque)\n\t\t\tovl_remove_opaque(olddentry);\n\t\tif (!overwrite && new_is_dir && old_opaque && !new_opaque)\n\t\t\tovl_remove_opaque(newdentry);\n\t\tgoto out_dput;\n\t}\n\n\tif (is_dir && old_opaque && !new_opaque)\n\t\tovl_remove_opaque(olddentry);\n\tif (!overwrite && new_is_dir && !old_opaque && new_opaque)\n\t\tovl_remove_opaque(newdentry);\n\n\t/*\n\t * Old dentry now lives in different location. Dentries in\n\t * lowerstack are stale. We cannot drop them here because\n\t * access to them is lockless. This could be only pure upper\n\t * or opaque directory - numlower is zero. Or upper non-dir\n\t * entry - its pureness is tracked by flag opaque.\n\t */\n\tif (old_opaque != new_opaque) {\n\t\tovl_dentry_set_opaque(old, new_opaque);\n\t\tif (!overwrite)\n\t\t\tovl_dentry_set_opaque(new, old_opaque);\n\t}\n\n\tif (cleanup_whiteout)\n\t\tovl_cleanup(old_upperdir->d_inode, newdentry);\n\n\tovl_dentry_version_inc(old->d_parent);\n\tovl_dentry_version_inc(new->d_parent);\n\nout_dput:\n\tdput(newdentry);\nout_unlock:\n\tunlock_rename(new_upperdir, old_upperdir);\nout_revert_creds:\n\tif (old_opaque || new_opaque) {\n\t\trevert_creds(old_cred);\n\t\tput_cred(override_cred);\n\t}\nout_drop_write:\n\tovl_drop_write(old);\nout:\n\tdput(opaquedir);\n\treturn err;\n}",
        "code_after_change": "static int ovl_rename2(struct inode *olddir, struct dentry *old,\n\t\t       struct inode *newdir, struct dentry *new,\n\t\t       unsigned int flags)\n{\n\tint err;\n\tenum ovl_path_type old_type;\n\tenum ovl_path_type new_type;\n\tstruct dentry *old_upperdir;\n\tstruct dentry *new_upperdir;\n\tstruct dentry *olddentry;\n\tstruct dentry *newdentry;\n\tstruct dentry *trap;\n\tbool old_opaque;\n\tbool new_opaque;\n\tbool new_create = false;\n\tbool cleanup_whiteout = false;\n\tbool overwrite = !(flags & RENAME_EXCHANGE);\n\tbool is_dir = d_is_dir(old);\n\tbool new_is_dir = false;\n\tstruct dentry *opaquedir = NULL;\n\tconst struct cred *old_cred = NULL;\n\tstruct cred *override_cred = NULL;\n\n\terr = -EINVAL;\n\tif (flags & ~(RENAME_EXCHANGE | RENAME_NOREPLACE))\n\t\tgoto out;\n\n\tflags &= ~RENAME_NOREPLACE;\n\n\terr = ovl_check_sticky(old);\n\tif (err)\n\t\tgoto out;\n\n\t/* Don't copy up directory trees */\n\told_type = ovl_path_type(old);\n\terr = -EXDEV;\n\tif (OVL_TYPE_MERGE_OR_LOWER(old_type) && is_dir)\n\t\tgoto out;\n\n\tif (new->d_inode) {\n\t\terr = ovl_check_sticky(new);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (d_is_dir(new))\n\t\t\tnew_is_dir = true;\n\n\t\tnew_type = ovl_path_type(new);\n\t\terr = -EXDEV;\n\t\tif (!overwrite && OVL_TYPE_MERGE_OR_LOWER(new_type) && new_is_dir)\n\t\t\tgoto out;\n\n\t\terr = 0;\n\t\tif (!OVL_TYPE_UPPER(new_type) && !OVL_TYPE_UPPER(old_type)) {\n\t\t\tif (ovl_dentry_lower(old)->d_inode ==\n\t\t\t    ovl_dentry_lower(new)->d_inode)\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (OVL_TYPE_UPPER(new_type) && OVL_TYPE_UPPER(old_type)) {\n\t\t\tif (ovl_dentry_upper(old)->d_inode ==\n\t\t\t    ovl_dentry_upper(new)->d_inode)\n\t\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tif (ovl_dentry_is_opaque(new))\n\t\t\tnew_type = __OVL_PATH_UPPER;\n\t\telse\n\t\t\tnew_type = __OVL_PATH_UPPER | __OVL_PATH_PURE;\n\t}\n\n\terr = ovl_want_write(old);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_copy_up(old);\n\tif (err)\n\t\tgoto out_drop_write;\n\n\terr = ovl_copy_up(new->d_parent);\n\tif (err)\n\t\tgoto out_drop_write;\n\tif (!overwrite) {\n\t\terr = ovl_copy_up(new);\n\t\tif (err)\n\t\t\tgoto out_drop_write;\n\t}\n\n\told_opaque = !OVL_TYPE_PURE_UPPER(old_type);\n\tnew_opaque = !OVL_TYPE_PURE_UPPER(new_type);\n\n\tif (old_opaque || new_opaque) {\n\t\terr = -ENOMEM;\n\t\toverride_cred = prepare_creds();\n\t\tif (!override_cred)\n\t\t\tgoto out_drop_write;\n\n\t\t/*\n\t\t * CAP_SYS_ADMIN for setting xattr on whiteout, opaque dir\n\t\t * CAP_DAC_OVERRIDE for create in workdir\n\t\t * CAP_FOWNER for removing whiteout from sticky dir\n\t\t * CAP_FSETID for chmod of opaque dir\n\t\t * CAP_CHOWN for chown of opaque dir\n\t\t */\n\t\tcap_raise(override_cred->cap_effective, CAP_SYS_ADMIN);\n\t\tcap_raise(override_cred->cap_effective, CAP_DAC_OVERRIDE);\n\t\tcap_raise(override_cred->cap_effective, CAP_FOWNER);\n\t\tcap_raise(override_cred->cap_effective, CAP_FSETID);\n\t\tcap_raise(override_cred->cap_effective, CAP_CHOWN);\n\t\told_cred = override_creds(override_cred);\n\t}\n\n\tif (overwrite && OVL_TYPE_MERGE_OR_LOWER(new_type) && new_is_dir) {\n\t\topaquedir = ovl_check_empty_and_clear(new);\n\t\terr = PTR_ERR(opaquedir);\n\t\tif (IS_ERR(opaquedir)) {\n\t\t\topaquedir = NULL;\n\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\tif (overwrite) {\n\t\tif (old_opaque) {\n\t\t\tif (new->d_inode || !new_opaque) {\n\t\t\t\t/* Whiteout source */\n\t\t\t\tflags |= RENAME_WHITEOUT;\n\t\t\t} else {\n\t\t\t\t/* Switch whiteouts */\n\t\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\t}\n\t\t} else if (is_dir && !new->d_inode && new_opaque) {\n\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\tcleanup_whiteout = true;\n\t\t}\n\t}\n\n\told_upperdir = ovl_dentry_upper(old->d_parent);\n\tnew_upperdir = ovl_dentry_upper(new->d_parent);\n\n\ttrap = lock_rename(new_upperdir, old_upperdir);\n\n\n\tolddentry = lookup_one_len(old->d_name.name, old_upperdir,\n\t\t\t\t   old->d_name.len);\n\terr = PTR_ERR(olddentry);\n\tif (IS_ERR(olddentry))\n\t\tgoto out_unlock;\n\n\terr = -ESTALE;\n\tif (olddentry != ovl_dentry_upper(old))\n\t\tgoto out_dput_old;\n\n\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n\t\t\t\t   new->d_name.len);\n\terr = PTR_ERR(newdentry);\n\tif (IS_ERR(newdentry))\n\t\tgoto out_dput_old;\n\n\terr = -ESTALE;\n\tif (ovl_dentry_upper(new)) {\n\t\tif (opaquedir) {\n\t\t\tif (newdentry != opaquedir)\n\t\t\t\tgoto out_dput;\n\t\t} else {\n\t\t\tif (newdentry != ovl_dentry_upper(new))\n\t\t\t\tgoto out_dput;\n\t\t}\n\t} else {\n\t\tnew_create = true;\n\t\tif (!d_is_negative(newdentry) &&\n\t\t    (!new_opaque || !ovl_is_whiteout(newdentry)))\n\t\t\tgoto out_dput;\n\t}\n\n\tif (olddentry == trap)\n\t\tgoto out_dput;\n\tif (newdentry == trap)\n\t\tgoto out_dput;\n\n\tif (is_dir && !old_opaque && new_opaque) {\n\t\terr = ovl_set_opaque(olddentry);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\tif (!overwrite && new_is_dir && old_opaque && !new_opaque) {\n\t\terr = ovl_set_opaque(newdentry);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\n\tif (old_opaque || new_opaque) {\n\t\terr = ovl_do_rename(old_upperdir->d_inode, olddentry,\n\t\t\t\t    new_upperdir->d_inode, newdentry,\n\t\t\t\t    flags);\n\t} else {\n\t\t/* No debug for the plain case */\n\t\tBUG_ON(flags & ~RENAME_EXCHANGE);\n\t\terr = vfs_rename(old_upperdir->d_inode, olddentry,\n\t\t\t\t new_upperdir->d_inode, newdentry,\n\t\t\t\t NULL, flags);\n\t}\n\n\tif (err) {\n\t\tif (is_dir && !old_opaque && new_opaque)\n\t\t\tovl_remove_opaque(olddentry);\n\t\tif (!overwrite && new_is_dir && old_opaque && !new_opaque)\n\t\t\tovl_remove_opaque(newdentry);\n\t\tgoto out_dput;\n\t}\n\n\tif (is_dir && old_opaque && !new_opaque)\n\t\tovl_remove_opaque(olddentry);\n\tif (!overwrite && new_is_dir && !old_opaque && new_opaque)\n\t\tovl_remove_opaque(newdentry);\n\n\t/*\n\t * Old dentry now lives in different location. Dentries in\n\t * lowerstack are stale. We cannot drop them here because\n\t * access to them is lockless. This could be only pure upper\n\t * or opaque directory - numlower is zero. Or upper non-dir\n\t * entry - its pureness is tracked by flag opaque.\n\t */\n\tif (old_opaque != new_opaque) {\n\t\tovl_dentry_set_opaque(old, new_opaque);\n\t\tif (!overwrite)\n\t\t\tovl_dentry_set_opaque(new, old_opaque);\n\t}\n\n\tif (cleanup_whiteout)\n\t\tovl_cleanup(old_upperdir->d_inode, newdentry);\n\n\tovl_dentry_version_inc(old->d_parent);\n\tovl_dentry_version_inc(new->d_parent);\n\nout_dput:\n\tdput(newdentry);\nout_dput_old:\n\tdput(olddentry);\nout_unlock:\n\tunlock_rename(new_upperdir, old_upperdir);\nout_revert_creds:\n\tif (old_opaque || new_opaque) {\n\t\trevert_creds(old_cred);\n\t\tput_cred(override_cred);\n\t}\nout_drop_write:\n\tovl_drop_write(old);\nout:\n\tdput(opaquedir);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -138,29 +138,39 @@\n \n \ttrap = lock_rename(new_upperdir, old_upperdir);\n \n-\tolddentry = ovl_dentry_upper(old);\n-\tnewdentry = ovl_dentry_upper(new);\n-\tif (newdentry) {\n+\n+\tolddentry = lookup_one_len(old->d_name.name, old_upperdir,\n+\t\t\t\t   old->d_name.len);\n+\terr = PTR_ERR(olddentry);\n+\tif (IS_ERR(olddentry))\n+\t\tgoto out_unlock;\n+\n+\terr = -ESTALE;\n+\tif (olddentry != ovl_dentry_upper(old))\n+\t\tgoto out_dput_old;\n+\n+\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n+\t\t\t\t   new->d_name.len);\n+\terr = PTR_ERR(newdentry);\n+\tif (IS_ERR(newdentry))\n+\t\tgoto out_dput_old;\n+\n+\terr = -ESTALE;\n+\tif (ovl_dentry_upper(new)) {\n \t\tif (opaquedir) {\n-\t\t\tnewdentry = opaquedir;\n-\t\t\topaquedir = NULL;\n+\t\t\tif (newdentry != opaquedir)\n+\t\t\t\tgoto out_dput;\n \t\t} else {\n-\t\t\tdget(newdentry);\n+\t\t\tif (newdentry != ovl_dentry_upper(new))\n+\t\t\t\tgoto out_dput;\n \t\t}\n \t} else {\n \t\tnew_create = true;\n-\t\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n-\t\t\t\t\t   new->d_name.len);\n-\t\terr = PTR_ERR(newdentry);\n-\t\tif (IS_ERR(newdentry))\n-\t\t\tgoto out_unlock;\n-\t}\n-\n-\terr = -ESTALE;\n-\tif (olddentry->d_parent != old_upperdir)\n-\t\tgoto out_dput;\n-\tif (newdentry->d_parent != new_upperdir)\n-\t\tgoto out_dput;\n+\t\tif (!d_is_negative(newdentry) &&\n+\t\t    (!new_opaque || !ovl_is_whiteout(newdentry)))\n+\t\t\tgoto out_dput;\n+\t}\n+\n \tif (olddentry == trap)\n \t\tgoto out_dput;\n \tif (newdentry == trap)\n@@ -223,6 +233,8 @@\n \n out_dput:\n \tdput(newdentry);\n+out_dput_old:\n+\tdput(olddentry);\n out_unlock:\n \tunlock_rename(new_upperdir, old_upperdir);\n out_revert_creds:",
        "function_modified_lines": {
            "added": [
                "",
                "\tolddentry = lookup_one_len(old->d_name.name, old_upperdir,",
                "\t\t\t\t   old->d_name.len);",
                "\terr = PTR_ERR(olddentry);",
                "\tif (IS_ERR(olddentry))",
                "\t\tgoto out_unlock;",
                "",
                "\terr = -ESTALE;",
                "\tif (olddentry != ovl_dentry_upper(old))",
                "\t\tgoto out_dput_old;",
                "",
                "\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,",
                "\t\t\t\t   new->d_name.len);",
                "\terr = PTR_ERR(newdentry);",
                "\tif (IS_ERR(newdentry))",
                "\t\tgoto out_dput_old;",
                "",
                "\terr = -ESTALE;",
                "\tif (ovl_dentry_upper(new)) {",
                "\t\t\tif (newdentry != opaquedir)",
                "\t\t\t\tgoto out_dput;",
                "\t\t\tif (newdentry != ovl_dentry_upper(new))",
                "\t\t\t\tgoto out_dput;",
                "\t\tif (!d_is_negative(newdentry) &&",
                "\t\t    (!new_opaque || !ovl_is_whiteout(newdentry)))",
                "\t\t\tgoto out_dput;",
                "\t}",
                "",
                "out_dput_old:",
                "\tdput(olddentry);"
            ],
            "deleted": [
                "\tolddentry = ovl_dentry_upper(old);",
                "\tnewdentry = ovl_dentry_upper(new);",
                "\tif (newdentry) {",
                "\t\t\tnewdentry = opaquedir;",
                "\t\t\topaquedir = NULL;",
                "\t\t\tdget(newdentry);",
                "\t\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,",
                "\t\t\t\t\t   new->d_name.len);",
                "\t\terr = PTR_ERR(newdentry);",
                "\t\tif (IS_ERR(newdentry))",
                "\t\t\tgoto out_unlock;",
                "\t}",
                "",
                "\terr = -ESTALE;",
                "\tif (olddentry->d_parent != old_upperdir)",
                "\t\tgoto out_dput;",
                "\tif (newdentry->d_parent != new_upperdir)",
                "\t\tgoto out_dput;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "fs/overlayfs/dir.c in the OverlayFS filesystem implementation in the Linux kernel before 4.6 does not properly verify the upper dentry before proceeding with unlink and rename system-call processing, which allows local users to cause a denial of service (system crash) via a rename system call that specifies a self-hardlink.",
        "id": 1065
    },
    {
        "cve_id": "CVE-2016-6162",
        "code_before_change": "int udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\t/*\n\t *\tCharge it to the socket, dropping if the queue is full.\n\t */\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\tif (static_key_false(&udp_encap_needed) && up->encap_type) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = ACCESS_ONCE(up->encap_rcv);\n\t\tif (encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\t__UDP_INC_STATS(sock_net(sk),\n\t\t\t\t\t\tUDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\tis_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * \tUDP-Lite specific tests, ignored on UDP sockets\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\t/*\n\t\t * MIB statistics other than incrementing the error count are\n\t\t * disabled for the following two types of errors: these depend\n\t\t * on the application settings, not on the functioning of the\n\t\t * protocol stack as such.\n\t\t *\n\t\t * RFC 3828 here recommends (sec 3.3): \"There should also be a\n\t\t * way ... to ... at least let the receiving application block\n\t\t * delivery of packets with coverage values less than a value\n\t\t * provided by the application.\"\n\t\t */\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLite: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\t/* The next case involves violating the min. coverage requested\n\t\t * by the receiver. This is subtle: if receiver wants x and x is\n\t\t * greater than the buffersize/MTU then receiver will complain\n\t\t * that it wants x while sender emits packets of smaller size y.\n\t\t * Therefore the above ...()->partial_cov statement is essential.\n\t\t */\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLite: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_access_pointer(sk->sk_filter) &&\n\t    udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_error;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto drop;\n\n\tudp_csum_pull_header(skb);\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\t__UDP_INC_STATS(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\tis_udplite);\n\t\tgoto drop;\n\t}\n\n\trc = 0;\n\n\tipv4_pktinfo_prepare(sk, skb);\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udp_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ncsum_error:\n\t__UDP_INC_STATS(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\t__UDP_INC_STATS(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}",
        "code_after_change": "int udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\t/*\n\t *\tCharge it to the socket, dropping if the queue is full.\n\t */\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\tif (static_key_false(&udp_encap_needed) && up->encap_type) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = ACCESS_ONCE(up->encap_rcv);\n\t\tif (encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\t__UDP_INC_STATS(sock_net(sk),\n\t\t\t\t\t\tUDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\tis_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * \tUDP-Lite specific tests, ignored on UDP sockets\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\t/*\n\t\t * MIB statistics other than incrementing the error count are\n\t\t * disabled for the following two types of errors: these depend\n\t\t * on the application settings, not on the functioning of the\n\t\t * protocol stack as such.\n\t\t *\n\t\t * RFC 3828 here recommends (sec 3.3): \"There should also be a\n\t\t * way ... to ... at least let the receiving application block\n\t\t * delivery of packets with coverage values less than a value\n\t\t * provided by the application.\"\n\t\t */\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLite: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\t/* The next case involves violating the min. coverage requested\n\t\t * by the receiver. This is subtle: if receiver wants x and x is\n\t\t * greater than the buffersize/MTU then receiver will complain\n\t\t * that it wants x while sender emits packets of smaller size y.\n\t\t * Therefore the above ...()->partial_cov statement is essential.\n\t\t */\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLite: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_access_pointer(sk->sk_filter) &&\n\t    udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_error;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto drop;\n\tif (unlikely(skb->len < sizeof(struct udphdr)))\n\t\tgoto drop;\n\n\tudp_csum_pull_header(skb);\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\t__UDP_INC_STATS(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\tis_udplite);\n\t\tgoto drop;\n\t}\n\n\trc = 0;\n\n\tipv4_pktinfo_prepare(sk, skb);\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udp_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ncsum_error:\n\t__UDP_INC_STATS(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\t__UDP_INC_STATS(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -86,6 +86,8 @@\n \n \tif (sk_filter(sk, skb))\n \t\tgoto drop;\n+\tif (unlikely(skb->len < sizeof(struct udphdr)))\n+\t\tgoto drop;\n \n \tudp_csum_pull_header(skb);\n \tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(skb->len < sizeof(struct udphdr)))",
                "\t\tgoto drop;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "net/core/skbuff.c in the Linux kernel 4.7-rc6 allows local users to cause a denial of service (panic) or possibly have unspecified other impact via certain IPv6 socket operations.",
        "id": 1062
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static int io_uring_flush(struct file *file, void *data)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\n\tio_uring_cancel_files(ctx, data);\n\n\t/*\n\t * If the task is going away, cancel work it may have pending\n\t */\n\tif (fatal_signal_pending(current) || (current->flags & PF_EXITING))\n\t\tio_wq_cancel_cb(ctx->io_wq, io_cancel_task_cb, current, true);\n\n\treturn 0;\n}",
        "code_after_change": "static int io_uring_flush(struct file *file, void *data)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\n\t/*\n\t * If the task is going away, cancel work it may have pending\n\t */\n\tif (fatal_signal_pending(current) || (current->flags & PF_EXITING))\n\t\tdata = NULL;\n\n\tio_uring_cancel_task_requests(ctx, data);\n\tio_uring_attempt_task_drop(file, !data);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,14 +1,14 @@\n static int io_uring_flush(struct file *file, void *data)\n {\n \tstruct io_ring_ctx *ctx = file->private_data;\n-\n-\tio_uring_cancel_files(ctx, data);\n \n \t/*\n \t * If the task is going away, cancel work it may have pending\n \t */\n \tif (fatal_signal_pending(current) || (current->flags & PF_EXITING))\n-\t\tio_wq_cancel_cb(ctx->io_wq, io_cancel_task_cb, current, true);\n+\t\tdata = NULL;\n \n+\tio_uring_cancel_task_requests(ctx, data);\n+\tio_uring_attempt_task_drop(file, !data);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tdata = NULL;",
                "\tio_uring_cancel_task_requests(ctx, data);",
                "\tio_uring_attempt_task_drop(file, !data);"
            ],
            "deleted": [
                "",
                "\tio_uring_cancel_files(ctx, data);",
                "\t\tio_wq_cancel_cb(ctx->io_wq, io_cancel_task_cb, current, true);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2851
    },
    {
        "cve_id": "CVE-2013-2898",
        "code_before_change": "int sensor_hub_get_feature(struct hid_sensor_hub_device *hsdev, u32 report_id,\n\t\t\t\tu32 field_index, s32 *value)\n{\n\tstruct hid_report *report;\n\tstruct sensor_hub_data *data =  hid_get_drvdata(hsdev->hdev);\n\tint ret = 0;\n\n\tmutex_lock(&data->mutex);\n\treport = sensor_hub_report(report_id, hsdev->hdev, HID_FEATURE_REPORT);\n\tif (!report || (field_index >=  report->maxfield)) {\n\t\tret = -EINVAL;\n\t\tgoto done_proc;\n\t}\n\thid_hw_request(hsdev->hdev, report, HID_REQ_GET_REPORT);\n\thid_hw_wait(hsdev->hdev);\n\t*value = report->field[field_index]->value[0];\n\ndone_proc:\n\tmutex_unlock(&data->mutex);\n\n\treturn ret;\n}",
        "code_after_change": "int sensor_hub_get_feature(struct hid_sensor_hub_device *hsdev, u32 report_id,\n\t\t\t\tu32 field_index, s32 *value)\n{\n\tstruct hid_report *report;\n\tstruct sensor_hub_data *data =  hid_get_drvdata(hsdev->hdev);\n\tint ret = 0;\n\n\tmutex_lock(&data->mutex);\n\treport = sensor_hub_report(report_id, hsdev->hdev, HID_FEATURE_REPORT);\n\tif (!report || (field_index >=  report->maxfield) ||\n\t    report->field[field_index]->report_count < 1) {\n\t\tret = -EINVAL;\n\t\tgoto done_proc;\n\t}\n\thid_hw_request(hsdev->hdev, report, HID_REQ_GET_REPORT);\n\thid_hw_wait(hsdev->hdev);\n\t*value = report->field[field_index]->value[0];\n\ndone_proc:\n\tmutex_unlock(&data->mutex);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,8 @@\n \n \tmutex_lock(&data->mutex);\n \treport = sensor_hub_report(report_id, hsdev->hdev, HID_FEATURE_REPORT);\n-\tif (!report || (field_index >=  report->maxfield)) {\n+\tif (!report || (field_index >=  report->maxfield) ||\n+\t    report->field[field_index]->report_count < 1) {\n \t\tret = -EINVAL;\n \t\tgoto done_proc;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif (!report || (field_index >=  report->maxfield) ||",
                "\t    report->field[field_index]->report_count < 1) {"
            ],
            "deleted": [
                "\tif (!report || (field_index >=  report->maxfield)) {"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "drivers/hid/hid-sensor-hub.c in the Human Interface Device (HID) subsystem in the Linux kernel through 3.11, when CONFIG_HID_SENSOR_HUB is enabled, allows physically proximate attackers to obtain sensitive information from kernel memory via a crafted device.",
        "id": 258
    },
    {
        "cve_id": "CVE-2017-9242",
        "code_before_change": "static int __ip6_append_data(struct sock *sk,\n\t\t\t     struct flowi6 *fl6,\n\t\t\t     struct sk_buff_head *queue,\n\t\t\t     struct inet_cork *cork,\n\t\t\t     struct inet6_cork *v6_cork,\n\t\t\t     struct page_frag *pfrag,\n\t\t\t     int getfrag(void *from, char *to, int offset,\n\t\t\t\t\t int len, int odd, struct sk_buff *skb),\n\t\t\t     void *from, int length, int transhdrlen,\n\t\t\t     unsigned int flags, struct ipcm6_cookie *ipc6,\n\t\t\t     const struct sockcm_cookie *sockc)\n{\n\tstruct sk_buff *skb, *skb_prev = NULL;\n\tunsigned int maxfraglen, fragheaderlen, mtu, orig_mtu;\n\tint exthdrlen = 0;\n\tint dst_exthdrlen = 0;\n\tint hh_len;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\t__u8 tx_flags = 0;\n\tu32 tskey = 0;\n\tstruct rt6_info *rt = (struct rt6_info *)cork->dst;\n\tstruct ipv6_txoptions *opt = v6_cork->opt;\n\tint csummode = CHECKSUM_NONE;\n\tunsigned int maxnonfragsize, headersize;\n\n\tskb = skb_peek_tail(queue);\n\tif (!skb) {\n\t\texthdrlen = opt ? opt->opt_flen : 0;\n\t\tdst_exthdrlen = rt->dst.header_len - rt->rt6i_nfheader_len;\n\t}\n\n\tmtu = cork->fragsize;\n\torig_mtu = mtu;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct ipv6hdr) + rt->rt6i_nfheader_len +\n\t\t\t(opt ? opt->opt_nflen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen -\n\t\t     sizeof(struct frag_hdr);\n\n\theadersize = sizeof(struct ipv6hdr) +\n\t\t     (opt ? opt->opt_flen + opt->opt_nflen : 0) +\n\t\t     (dst_allfrag(&rt->dst) ?\n\t\t      sizeof(struct frag_hdr) : 0) +\n\t\t     rt->rt6i_nfheader_len;\n\n\tif (cork->length + length > mtu - headersize && ipc6->dontfrag &&\n\t    (sk->sk_protocol == IPPROTO_UDP ||\n\t     sk->sk_protocol == IPPROTO_RAW)) {\n\t\tipv6_local_rxpmtu(sk, fl6, mtu - headersize +\n\t\t\t\tsizeof(struct ipv6hdr));\n\t\tgoto emsgsize;\n\t}\n\n\tif (ip6_sk_ignore_df(sk))\n\t\tmaxnonfragsize = sizeof(struct ipv6hdr) + IPV6_MAXPLEN;\n\telse\n\t\tmaxnonfragsize = mtu;\n\n\tif (cork->length + length > maxnonfragsize - headersize) {\nemsgsize:\n\t\tipv6_local_error(sk, EMSGSIZE, fl6,\n\t\t\t\t mtu - headersize +\n\t\t\t\t sizeof(struct ipv6hdr));\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/* CHECKSUM_PARTIAL only with no extension headers and when\n\t * we are not going to fragment\n\t */\n\tif (transhdrlen && sk->sk_protocol == IPPROTO_UDP &&\n\t    headersize == sizeof(struct ipv6hdr) &&\n\t    length <= mtu - headersize &&\n\t    !(flags & MSG_MORE) &&\n\t    rt->dst.dev->features & (NETIF_F_IPV6_CSUM | NETIF_F_HW_CSUM))\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tif (sk->sk_type == SOCK_DGRAM || sk->sk_type == SOCK_RAW) {\n\t\tsock_tx_timestamp(sk, sockc->tsflags, &tx_flags);\n\t\tif (tx_flags & SKBTX_ANY_SW_TSTAMP &&\n\t\t    sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)\n\t\t\ttskey = sk->sk_tskey++;\n\t}\n\n\t/*\n\t * Let's try using as much space as possible.\n\t * Use MTU if total length of the message fits into the MTU.\n\t * Otherwise, we need to reserve fragment header and\n\t * fragment alignment (= 8-15 octects, in total).\n\t *\n\t * Note that we may need to \"move\" the data from the tail of\n\t * of the buffer to the new fragment when we split\n\t * the message.\n\t *\n\t * FIXME: It may be fragmented into multiple chunks\n\t *        at once if non-fragmentable extension headers\n\t *        are too large.\n\t * --yoshfuji\n\t */\n\n\tcork->length += length;\n\tif ((((length + fragheaderlen) > mtu) ||\n\t     (skb && skb_is_gso(skb))) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO) && !dst_xfrm(&rt->dst) &&\n\t    (sk->sk_type == SOCK_DGRAM) && !udp_get_no_check6_tx(sk)) {\n\t\terr = ip6_ufo_append_data(sk, queue, getfrag, from, length,\n\t\t\t\t\t  hh_len, fragheaderlen, exthdrlen,\n\t\t\t\t\t  transhdrlen, mtu, flags, fl6);\n\t\tif (err)\n\t\t\tgoto error;\n\t\treturn 0;\n\t}\n\n\tif (!skb)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\nalloc_new_skb:\n\t\t\t/* There's no room in the current skb */\n\t\t\tif (skb)\n\t\t\t\tfraggap = skb->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\t\t\t/* update mtu and maxfraglen if necessary */\n\t\t\tif (!skb || !skb_prev)\n\t\t\t\tip6_append_data_mtu(&mtu, &maxfraglen,\n\t\t\t\t\t\t    fragheaderlen, skb, rt,\n\t\t\t\t\t\t    orig_mtu);\n\n\t\t\tskb_prev = skb;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\n\t\t\tif (datalen > (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen - rt->dst.trailer_len;\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse\n\t\t\t\talloclen = datalen + fragheaderlen;\n\n\t\t\talloclen += dst_exthdrlen;\n\n\t\t\tif (datalen != length + fraggap) {\n\t\t\t\t/*\n\t\t\t\t * this is not the last fragment, the trailer\n\t\t\t\t * space is regarded as data space.\n\t\t\t\t */\n\t\t\t\tdatalen += rt->dst.trailer_len;\n\t\t\t}\n\n\t\t\talloclen += rt->dst.trailer_len;\n\t\t\tfraglen = datalen + fragheaderlen;\n\n\t\t\t/*\n\t\t\t * We just reserve space for fragment header.\n\t\t\t * Note: this may be overallocation if the message\n\t\t\t * (without MSG_MORE) fits into the MTU.\n\t\t\t */\n\t\t\talloclen += sizeof(struct frag_hdr);\n\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (atomic_read(&sk->sk_wmem_alloc) <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = sock_wmalloc(sk,\n\t\t\t\t\t\t\t   alloclen + hh_len, 1,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\t\tif (unlikely(!skb))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t}\n\t\t\tif (!skb)\n\t\t\t\tgoto error;\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->protocol = htons(ETH_P_IPV6);\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\t/* reserve for fragmentation and ipsec header */\n\t\t\tskb_reserve(skb, hh_len + sizeof(struct frag_hdr) +\n\t\t\t\t    dst_exthdrlen);\n\n\t\t\t/* Only the initial fragment is time stamped */\n\t\t\tskb_shinfo(skb)->tx_flags = tx_flags;\n\t\t\ttx_flags = 0;\n\t\t\tskb_shinfo(skb)->tskey = tskey;\n\t\t\ttskey = 0;\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tdata += fragheaderlen;\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\t\t\tcopy = datalen - transhdrlen - fraggap;\n\n\t\t\tif (copy < 0) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t} else if (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= datalen - fraggap;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tdst_exthdrlen = 0;\n\n\t\t\tif ((flags & MSG_CONFIRM) && !skb_prev)\n\t\t\t\tskb_set_dst_pending_confirm(skb, 1);\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue\n\t\t\t */\n\t\t\t__skb_queue_tail(queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG)) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\n\t\t\terr = -ENOMEM;\n\t\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\t\tgoto error;\n\n\t\t\tif (!skb_can_coalesce(skb, i, pfrag->page,\n\t\t\t\t\t      pfrag->offset)) {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tif (i == MAX_SKB_FRAGS)\n\t\t\t\t\tgoto error;\n\n\t\t\t\t__skb_fill_page_desc(skb, i, pfrag->page,\n\t\t\t\t\t\t     pfrag->offset, 0);\n\t\t\t\tskb_shinfo(skb)->nr_frags = ++i;\n\t\t\t\tget_page(pfrag->page);\n\t\t\t}\n\t\t\tcopy = min_t(int, copy, pfrag->size - pfrag->offset);\n\t\t\tif (getfrag(from,\n\t\t\t\t    page_address(pfrag->page) + pfrag->offset,\n\t\t\t\t    offset, copy, skb->len, skb) < 0)\n\t\t\t\tgoto error_efault;\n\n\t\t\tpfrag->offset += copy;\n\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\tatomic_add(copy, &sk->sk_wmem_alloc);\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\n\treturn 0;\n\nerror_efault:\n\terr = -EFAULT;\nerror:\n\tcork->length -= length;\n\tIP6_INC_STATS(sock_net(sk), rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}",
        "code_after_change": "static int __ip6_append_data(struct sock *sk,\n\t\t\t     struct flowi6 *fl6,\n\t\t\t     struct sk_buff_head *queue,\n\t\t\t     struct inet_cork *cork,\n\t\t\t     struct inet6_cork *v6_cork,\n\t\t\t     struct page_frag *pfrag,\n\t\t\t     int getfrag(void *from, char *to, int offset,\n\t\t\t\t\t int len, int odd, struct sk_buff *skb),\n\t\t\t     void *from, int length, int transhdrlen,\n\t\t\t     unsigned int flags, struct ipcm6_cookie *ipc6,\n\t\t\t     const struct sockcm_cookie *sockc)\n{\n\tstruct sk_buff *skb, *skb_prev = NULL;\n\tunsigned int maxfraglen, fragheaderlen, mtu, orig_mtu;\n\tint exthdrlen = 0;\n\tint dst_exthdrlen = 0;\n\tint hh_len;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\t__u8 tx_flags = 0;\n\tu32 tskey = 0;\n\tstruct rt6_info *rt = (struct rt6_info *)cork->dst;\n\tstruct ipv6_txoptions *opt = v6_cork->opt;\n\tint csummode = CHECKSUM_NONE;\n\tunsigned int maxnonfragsize, headersize;\n\n\tskb = skb_peek_tail(queue);\n\tif (!skb) {\n\t\texthdrlen = opt ? opt->opt_flen : 0;\n\t\tdst_exthdrlen = rt->dst.header_len - rt->rt6i_nfheader_len;\n\t}\n\n\tmtu = cork->fragsize;\n\torig_mtu = mtu;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct ipv6hdr) + rt->rt6i_nfheader_len +\n\t\t\t(opt ? opt->opt_nflen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen -\n\t\t     sizeof(struct frag_hdr);\n\n\theadersize = sizeof(struct ipv6hdr) +\n\t\t     (opt ? opt->opt_flen + opt->opt_nflen : 0) +\n\t\t     (dst_allfrag(&rt->dst) ?\n\t\t      sizeof(struct frag_hdr) : 0) +\n\t\t     rt->rt6i_nfheader_len;\n\n\tif (cork->length + length > mtu - headersize && ipc6->dontfrag &&\n\t    (sk->sk_protocol == IPPROTO_UDP ||\n\t     sk->sk_protocol == IPPROTO_RAW)) {\n\t\tipv6_local_rxpmtu(sk, fl6, mtu - headersize +\n\t\t\t\tsizeof(struct ipv6hdr));\n\t\tgoto emsgsize;\n\t}\n\n\tif (ip6_sk_ignore_df(sk))\n\t\tmaxnonfragsize = sizeof(struct ipv6hdr) + IPV6_MAXPLEN;\n\telse\n\t\tmaxnonfragsize = mtu;\n\n\tif (cork->length + length > maxnonfragsize - headersize) {\nemsgsize:\n\t\tipv6_local_error(sk, EMSGSIZE, fl6,\n\t\t\t\t mtu - headersize +\n\t\t\t\t sizeof(struct ipv6hdr));\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/* CHECKSUM_PARTIAL only with no extension headers and when\n\t * we are not going to fragment\n\t */\n\tif (transhdrlen && sk->sk_protocol == IPPROTO_UDP &&\n\t    headersize == sizeof(struct ipv6hdr) &&\n\t    length <= mtu - headersize &&\n\t    !(flags & MSG_MORE) &&\n\t    rt->dst.dev->features & (NETIF_F_IPV6_CSUM | NETIF_F_HW_CSUM))\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tif (sk->sk_type == SOCK_DGRAM || sk->sk_type == SOCK_RAW) {\n\t\tsock_tx_timestamp(sk, sockc->tsflags, &tx_flags);\n\t\tif (tx_flags & SKBTX_ANY_SW_TSTAMP &&\n\t\t    sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)\n\t\t\ttskey = sk->sk_tskey++;\n\t}\n\n\t/*\n\t * Let's try using as much space as possible.\n\t * Use MTU if total length of the message fits into the MTU.\n\t * Otherwise, we need to reserve fragment header and\n\t * fragment alignment (= 8-15 octects, in total).\n\t *\n\t * Note that we may need to \"move\" the data from the tail of\n\t * of the buffer to the new fragment when we split\n\t * the message.\n\t *\n\t * FIXME: It may be fragmented into multiple chunks\n\t *        at once if non-fragmentable extension headers\n\t *        are too large.\n\t * --yoshfuji\n\t */\n\n\tcork->length += length;\n\tif ((((length + fragheaderlen) > mtu) ||\n\t     (skb && skb_is_gso(skb))) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO) && !dst_xfrm(&rt->dst) &&\n\t    (sk->sk_type == SOCK_DGRAM) && !udp_get_no_check6_tx(sk)) {\n\t\terr = ip6_ufo_append_data(sk, queue, getfrag, from, length,\n\t\t\t\t\t  hh_len, fragheaderlen, exthdrlen,\n\t\t\t\t\t  transhdrlen, mtu, flags, fl6);\n\t\tif (err)\n\t\t\tgoto error;\n\t\treturn 0;\n\t}\n\n\tif (!skb)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\nalloc_new_skb:\n\t\t\t/* There's no room in the current skb */\n\t\t\tif (skb)\n\t\t\t\tfraggap = skb->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\t\t\t/* update mtu and maxfraglen if necessary */\n\t\t\tif (!skb || !skb_prev)\n\t\t\t\tip6_append_data_mtu(&mtu, &maxfraglen,\n\t\t\t\t\t\t    fragheaderlen, skb, rt,\n\t\t\t\t\t\t    orig_mtu);\n\n\t\t\tskb_prev = skb;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\n\t\t\tif (datalen > (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen - rt->dst.trailer_len;\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse\n\t\t\t\talloclen = datalen + fragheaderlen;\n\n\t\t\talloclen += dst_exthdrlen;\n\n\t\t\tif (datalen != length + fraggap) {\n\t\t\t\t/*\n\t\t\t\t * this is not the last fragment, the trailer\n\t\t\t\t * space is regarded as data space.\n\t\t\t\t */\n\t\t\t\tdatalen += rt->dst.trailer_len;\n\t\t\t}\n\n\t\t\talloclen += rt->dst.trailer_len;\n\t\t\tfraglen = datalen + fragheaderlen;\n\n\t\t\t/*\n\t\t\t * We just reserve space for fragment header.\n\t\t\t * Note: this may be overallocation if the message\n\t\t\t * (without MSG_MORE) fits into the MTU.\n\t\t\t */\n\t\t\talloclen += sizeof(struct frag_hdr);\n\n\t\t\tcopy = datalen - transhdrlen - fraggap;\n\t\t\tif (copy < 0) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (atomic_read(&sk->sk_wmem_alloc) <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = sock_wmalloc(sk,\n\t\t\t\t\t\t\t   alloclen + hh_len, 1,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\t\tif (unlikely(!skb))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t}\n\t\t\tif (!skb)\n\t\t\t\tgoto error;\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->protocol = htons(ETH_P_IPV6);\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\t/* reserve for fragmentation and ipsec header */\n\t\t\tskb_reserve(skb, hh_len + sizeof(struct frag_hdr) +\n\t\t\t\t    dst_exthdrlen);\n\n\t\t\t/* Only the initial fragment is time stamped */\n\t\t\tskb_shinfo(skb)->tx_flags = tx_flags;\n\t\t\ttx_flags = 0;\n\t\t\tskb_shinfo(skb)->tskey = tskey;\n\t\t\ttskey = 0;\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tdata += fragheaderlen;\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\t\t\tif (copy > 0 &&\n\t\t\t    getfrag(from, data + transhdrlen, offset,\n\t\t\t\t    copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= datalen - fraggap;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tdst_exthdrlen = 0;\n\n\t\t\tif ((flags & MSG_CONFIRM) && !skb_prev)\n\t\t\t\tskb_set_dst_pending_confirm(skb, 1);\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue\n\t\t\t */\n\t\t\t__skb_queue_tail(queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG)) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\n\t\t\terr = -ENOMEM;\n\t\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\t\tgoto error;\n\n\t\t\tif (!skb_can_coalesce(skb, i, pfrag->page,\n\t\t\t\t\t      pfrag->offset)) {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tif (i == MAX_SKB_FRAGS)\n\t\t\t\t\tgoto error;\n\n\t\t\t\t__skb_fill_page_desc(skb, i, pfrag->page,\n\t\t\t\t\t\t     pfrag->offset, 0);\n\t\t\t\tskb_shinfo(skb)->nr_frags = ++i;\n\t\t\t\tget_page(pfrag->page);\n\t\t\t}\n\t\t\tcopy = min_t(int, copy, pfrag->size - pfrag->offset);\n\t\t\tif (getfrag(from,\n\t\t\t\t    page_address(pfrag->page) + pfrag->offset,\n\t\t\t\t    offset, copy, skb->len, skb) < 0)\n\t\t\t\tgoto error_efault;\n\n\t\t\tpfrag->offset += copy;\n\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\tatomic_add(copy, &sk->sk_wmem_alloc);\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\n\treturn 0;\n\nerror_efault:\n\terr = -EFAULT;\nerror:\n\tcork->length -= length;\n\tIP6_INC_STATS(sock_net(sk), rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -178,6 +178,11 @@\n \t\t\t */\n \t\t\talloclen += sizeof(struct frag_hdr);\n \n+\t\t\tcopy = datalen - transhdrlen - fraggap;\n+\t\t\tif (copy < 0) {\n+\t\t\t\terr = -EINVAL;\n+\t\t\t\tgoto error;\n+\t\t\t}\n \t\t\tif (transhdrlen) {\n \t\t\t\tskb = sock_alloc_send_skb(sk,\n \t\t\t\t\t\talloclen + hh_len,\n@@ -227,13 +232,9 @@\n \t\t\t\tdata += fraggap;\n \t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n \t\t\t}\n-\t\t\tcopy = datalen - transhdrlen - fraggap;\n-\n-\t\t\tif (copy < 0) {\n-\t\t\t\terr = -EINVAL;\n-\t\t\t\tkfree_skb(skb);\n-\t\t\t\tgoto error;\n-\t\t\t} else if (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {\n+\t\t\tif (copy > 0 &&\n+\t\t\t    getfrag(from, data + transhdrlen, offset,\n+\t\t\t\t    copy, fraggap, skb) < 0) {\n \t\t\t\terr = -EFAULT;\n \t\t\t\tkfree_skb(skb);\n \t\t\t\tgoto error;",
        "function_modified_lines": {
            "added": [
                "\t\t\tcopy = datalen - transhdrlen - fraggap;",
                "\t\t\tif (copy < 0) {",
                "\t\t\t\terr = -EINVAL;",
                "\t\t\t\tgoto error;",
                "\t\t\t}",
                "\t\t\tif (copy > 0 &&",
                "\t\t\t    getfrag(from, data + transhdrlen, offset,",
                "\t\t\t\t    copy, fraggap, skb) < 0) {"
            ],
            "deleted": [
                "\t\t\tcopy = datalen - transhdrlen - fraggap;",
                "",
                "\t\t\tif (copy < 0) {",
                "\t\t\t\terr = -EINVAL;",
                "\t\t\t\tkfree_skb(skb);",
                "\t\t\t\tgoto error;",
                "\t\t\t} else if (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The __ip6_append_data function in net/ipv6/ip6_output.c in the Linux kernel through 4.11.3 is too late in checking whether an overwrite of an skb data structure may occur, which allows local users to cause a denial of service (system crash) via crafted system calls.",
        "id": 1569
    },
    {
        "cve_id": "CVE-2016-2548",
        "code_before_change": "static void snd_timer_check_master(struct snd_timer_instance *master)\n{\n\tstruct snd_timer_instance *slave, *tmp;\n\n\t/* check all pending slaves */\n\tlist_for_each_entry_safe(slave, tmp, &snd_timer_slave_list, open_list) {\n\t\tif (slave->slave_class == master->slave_class &&\n\t\t    slave->slave_id == master->slave_id) {\n\t\t\tlist_move_tail(&slave->open_list, &master->slave_list_head);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\tslave->master = master;\n\t\t\tslave->timer = master->timer;\n\t\t\tif (slave->flags & SNDRV_TIMER_IFLG_RUNNING)\n\t\t\t\tlist_add_tail(&slave->active_list,\n\t\t\t\t\t      &master->slave_active_head);\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t}\n\t}\n}",
        "code_after_change": "static void snd_timer_check_master(struct snd_timer_instance *master)\n{\n\tstruct snd_timer_instance *slave, *tmp;\n\n\t/* check all pending slaves */\n\tlist_for_each_entry_safe(slave, tmp, &snd_timer_slave_list, open_list) {\n\t\tif (slave->slave_class == master->slave_class &&\n\t\t    slave->slave_id == master->slave_id) {\n\t\t\tlist_move_tail(&slave->open_list, &master->slave_list_head);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\tspin_lock(&master->timer->lock);\n\t\t\tslave->master = master;\n\t\t\tslave->timer = master->timer;\n\t\t\tif (slave->flags & SNDRV_TIMER_IFLG_RUNNING)\n\t\t\t\tlist_add_tail(&slave->active_list,\n\t\t\t\t\t      &master->slave_active_head);\n\t\t\tspin_unlock(&master->timer->lock);\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t}\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,11 +8,13 @@\n \t\t    slave->slave_id == master->slave_id) {\n \t\t\tlist_move_tail(&slave->open_list, &master->slave_list_head);\n \t\t\tspin_lock_irq(&slave_active_lock);\n+\t\t\tspin_lock(&master->timer->lock);\n \t\t\tslave->master = master;\n \t\t\tslave->timer = master->timer;\n \t\t\tif (slave->flags & SNDRV_TIMER_IFLG_RUNNING)\n \t\t\t\tlist_add_tail(&slave->active_list,\n \t\t\t\t\t      &master->slave_active_head);\n+\t\t\tspin_unlock(&master->timer->lock);\n \t\t\tspin_unlock_irq(&slave_active_lock);\n \t\t}\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tspin_lock(&master->timer->lock);",
                "\t\t\tspin_unlock(&master->timer->lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 retains certain linked lists after a close or stop action, which allows local users to cause a denial of service (system crash) via a crafted ioctl call, related to the (1) snd_timer_close and (2) _snd_timer_stop functions.",
        "id": 944
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int\nvsock_stream_recvmsg(struct kiocb *kiocb,\n\t\t     struct socket *sock,\n\t\t     struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tint err;\n\tsize_t target;\n\tssize_t copied;\n\tlong timeout;\n\tstruct vsock_transport_recv_notify_data recv_data;\n\n\tDEFINE_WAIT(wait);\n\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\terr = 0;\n\n\tmsg->msg_namelen = 0;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != SS_CONNECTED) {\n\t\t/* Recvmsg is supposed to return 0 if a peer performs an\n\t\t * orderly shutdown. Differentiate between that case and when a\n\t\t * peer has not connected or a local shutdown occured with the\n\t\t * SOCK_DONE flag.\n\t\t */\n\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\terr = 0;\n\t\telse\n\t\t\terr = -ENOTCONN;\n\n\t\tgoto out;\n\t}\n\n\tif (flags & MSG_OOB) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\t/* We don't check peer_shutdown flag here since peer may actually shut\n\t * down, but there can be data in the queue that a local socket can\n\t * receive.\n\t */\n\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* It is valid on Linux to pass in a zero-length receive buffer.  This\n\t * is not an error.  We may as well bail out now.\n\t */\n\tif (!len) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* We must not copy less than target bytes into the user's buffer\n\t * before returning successfully, so we wait for the consume queue to\n\t * have that much data to consume before dequeueing.  Note that this\n\t * makes it impossible to handle cases where target is greater than the\n\t * queue size.\n\t */\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tif (target >= transport->stream_rcvhiwat(vsk)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\tcopied = 0;\n\n\terr = transport->notify_recv_init(vsk, target, &recv_data);\n\tif (err < 0)\n\t\tgoto out;\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\twhile (1) {\n\t\ts64 ready = vsock_stream_has_data(vsk);\n\n\t\tif (ready < 0) {\n\t\t\t/* Invalid queue pair content. XXX This should be\n\t\t\t * changed to a connection reset in a later change.\n\t\t\t */\n\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_wait;\n\t\t} else if (ready > 0) {\n\t\t\tssize_t read;\n\n\t\t\terr = transport->notify_recv_pre_dequeue(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\n\t\t\tread = transport->stream_dequeue(\n\t\t\t\t\tvsk, msg->msg_iov,\n\t\t\t\t\tlen - copied, flags);\n\t\t\tif (read < 0) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcopied += read;\n\n\t\t\terr = transport->notify_recv_post_dequeue(\n\t\t\t\t\tvsk, target, read,\n\t\t\t\t\t!(flags & MSG_PEEK), &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_wait;\n\n\t\t\tif (read >= target || flags & MSG_PEEK)\n\t\t\t\tbreak;\n\n\t\t\ttarget -= read;\n\t\t} else {\n\t\t\tif (sk->sk_err != 0 || (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t    || (vsk->peer_shutdown & SEND_SHUTDOWN)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* Don't wait for non-blocking sockets. */\n\t\t\tif (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = transport->notify_recv_pre_block(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\n\t\t\trelease_sock(sk);\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tlock_sock(sk);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeout);\n\t\t\t\tbreak;\n\t\t\t} else if (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tprepare_to_wait(sk_sleep(sk), &wait,\n\t\t\t\t\tTASK_INTERRUPTIBLE);\n\t\t}\n\t}\n\n\tif (sk->sk_err)\n\t\terr = -sk->sk_err;\n\telse if (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\terr = 0;\n\n\tif (copied > 0) {\n\t\t/* We only do these additional bookkeeping/notification steps\n\t\t * if we actually copied something out of the queue pair\n\t\t * instead of just peeking ahead.\n\t\t */\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\t/* If the other side has shutdown for sending and there\n\t\t\t * is nothing more to read, then modify the socket\n\t\t\t * state.\n\t\t\t */\n\t\t\tif (vsk->peer_shutdown & SEND_SHUTDOWN) {\n\t\t\t\tif (vsock_stream_has_data(vsk) <= 0) {\n\t\t\t\t\tsk->sk_state = SS_UNCONNECTED;\n\t\t\t\t\tsock_set_flag(sk, SOCK_DONE);\n\t\t\t\t\tsk->sk_state_change(sk);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr = copied;\n\t}\n\nout_wait:\n\tfinish_wait(sk_sleep(sk), &wait);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "code_after_change": "static int\nvsock_stream_recvmsg(struct kiocb *kiocb,\n\t\t     struct socket *sock,\n\t\t     struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tint err;\n\tsize_t target;\n\tssize_t copied;\n\tlong timeout;\n\tstruct vsock_transport_recv_notify_data recv_data;\n\n\tDEFINE_WAIT(wait);\n\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\terr = 0;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != SS_CONNECTED) {\n\t\t/* Recvmsg is supposed to return 0 if a peer performs an\n\t\t * orderly shutdown. Differentiate between that case and when a\n\t\t * peer has not connected or a local shutdown occured with the\n\t\t * SOCK_DONE flag.\n\t\t */\n\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\terr = 0;\n\t\telse\n\t\t\terr = -ENOTCONN;\n\n\t\tgoto out;\n\t}\n\n\tif (flags & MSG_OOB) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\t/* We don't check peer_shutdown flag here since peer may actually shut\n\t * down, but there can be data in the queue that a local socket can\n\t * receive.\n\t */\n\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* It is valid on Linux to pass in a zero-length receive buffer.  This\n\t * is not an error.  We may as well bail out now.\n\t */\n\tif (!len) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* We must not copy less than target bytes into the user's buffer\n\t * before returning successfully, so we wait for the consume queue to\n\t * have that much data to consume before dequeueing.  Note that this\n\t * makes it impossible to handle cases where target is greater than the\n\t * queue size.\n\t */\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tif (target >= transport->stream_rcvhiwat(vsk)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\tcopied = 0;\n\n\terr = transport->notify_recv_init(vsk, target, &recv_data);\n\tif (err < 0)\n\t\tgoto out;\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\twhile (1) {\n\t\ts64 ready = vsock_stream_has_data(vsk);\n\n\t\tif (ready < 0) {\n\t\t\t/* Invalid queue pair content. XXX This should be\n\t\t\t * changed to a connection reset in a later change.\n\t\t\t */\n\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_wait;\n\t\t} else if (ready > 0) {\n\t\t\tssize_t read;\n\n\t\t\terr = transport->notify_recv_pre_dequeue(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\n\t\t\tread = transport->stream_dequeue(\n\t\t\t\t\tvsk, msg->msg_iov,\n\t\t\t\t\tlen - copied, flags);\n\t\t\tif (read < 0) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcopied += read;\n\n\t\t\terr = transport->notify_recv_post_dequeue(\n\t\t\t\t\tvsk, target, read,\n\t\t\t\t\t!(flags & MSG_PEEK), &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_wait;\n\n\t\t\tif (read >= target || flags & MSG_PEEK)\n\t\t\t\tbreak;\n\n\t\t\ttarget -= read;\n\t\t} else {\n\t\t\tif (sk->sk_err != 0 || (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t    || (vsk->peer_shutdown & SEND_SHUTDOWN)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* Don't wait for non-blocking sockets. */\n\t\t\tif (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = transport->notify_recv_pre_block(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\n\t\t\trelease_sock(sk);\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tlock_sock(sk);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeout);\n\t\t\t\tbreak;\n\t\t\t} else if (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tprepare_to_wait(sk_sleep(sk), &wait,\n\t\t\t\t\tTASK_INTERRUPTIBLE);\n\t\t}\n\t}\n\n\tif (sk->sk_err)\n\t\terr = -sk->sk_err;\n\telse if (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\terr = 0;\n\n\tif (copied > 0) {\n\t\t/* We only do these additional bookkeeping/notification steps\n\t\t * if we actually copied something out of the queue pair\n\t\t * instead of just peeking ahead.\n\t\t */\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\t/* If the other side has shutdown for sending and there\n\t\t\t * is nothing more to read, then modify the socket\n\t\t\t * state.\n\t\t\t */\n\t\t\tif (vsk->peer_shutdown & SEND_SHUTDOWN) {\n\t\t\t\tif (vsock_stream_has_data(vsk) <= 0) {\n\t\t\t\t\tsk->sk_state = SS_UNCONNECTED;\n\t\t\t\t\tsock_set_flag(sk, SOCK_DONE);\n\t\t\t\t\tsk->sk_state_change(sk);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr = copied;\n\t}\n\nout_wait:\n\tfinish_wait(sk_sleep(sk), &wait);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,8 +16,6 @@\n \tsk = sock->sk;\n \tvsk = vsock_sk(sk);\n \terr = 0;\n-\n-\tmsg->msg_namelen = 0;\n \n \tlock_sock(sk);\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 404
    },
    {
        "cve_id": "CVE-2016-9919",
        "code_before_change": "static void icmp6_send(struct sk_buff *skb, u8 type, u8 code, __u32 info,\n\t\t       const struct in6_addr *force_saddr)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tstruct inet6_dev *idev = NULL;\n\tstruct ipv6hdr *hdr = ipv6_hdr(skb);\n\tstruct sock *sk;\n\tstruct ipv6_pinfo *np;\n\tconst struct in6_addr *saddr = NULL;\n\tstruct dst_entry *dst;\n\tstruct icmp6hdr tmp_hdr;\n\tstruct flowi6 fl6;\n\tstruct icmpv6_msg msg;\n\tstruct sockcm_cookie sockc_unused = {0};\n\tstruct ipcm6_cookie ipc6;\n\tint iif = 0;\n\tint addr_type = 0;\n\tint len;\n\tint err = 0;\n\tu32 mark = IP6_REPLY_MARK(net, skb->mark);\n\n\tif ((u8 *)hdr < skb->head ||\n\t    (skb_network_header(skb) + sizeof(*hdr)) > skb_tail_pointer(skb))\n\t\treturn;\n\n\t/*\n\t *\tMake sure we respect the rules\n\t *\ti.e. RFC 1885 2.4(e)\n\t *\tRule (e.1) is enforced by not using icmp6_send\n\t *\tin any code that processes icmp errors.\n\t */\n\taddr_type = ipv6_addr_type(&hdr->daddr);\n\n\tif (ipv6_chk_addr(net, &hdr->daddr, skb->dev, 0) ||\n\t    ipv6_chk_acast_addr_src(net, skb->dev, &hdr->daddr))\n\t\tsaddr = &hdr->daddr;\n\n\t/*\n\t *\tDest addr check\n\t */\n\n\tif (addr_type & IPV6_ADDR_MULTICAST || skb->pkt_type != PACKET_HOST) {\n\t\tif (type != ICMPV6_PKT_TOOBIG &&\n\t\t    !(type == ICMPV6_PARAMPROB &&\n\t\t      code == ICMPV6_UNK_OPTION &&\n\t\t      (opt_unrec(skb, info))))\n\t\t\treturn;\n\n\t\tsaddr = NULL;\n\t}\n\n\taddr_type = ipv6_addr_type(&hdr->saddr);\n\n\t/*\n\t *\tSource addr check\n\t */\n\n\tif (__ipv6_addr_needs_scope_id(addr_type))\n\t\tiif = skb->dev->ifindex;\n\telse\n\t\tiif = l3mdev_master_ifindex(skb_dst(skb)->dev);\n\n\t/*\n\t *\tMust not send error if the source does not uniquely\n\t *\tidentify a single node (RFC2463 Section 2.4).\n\t *\tWe check unspecified / multicast addresses here,\n\t *\tand anycast addresses will be checked later.\n\t */\n\tif ((addr_type == IPV6_ADDR_ANY) || (addr_type & IPV6_ADDR_MULTICAST)) {\n\t\tnet_dbg_ratelimited(\"icmp6_send: addr_any/mcast source [%pI6c > %pI6c]\\n\",\n\t\t\t\t    &hdr->saddr, &hdr->daddr);\n\t\treturn;\n\t}\n\n\t/*\n\t *\tNever answer to a ICMP packet.\n\t */\n\tif (is_ineligible(skb)) {\n\t\tnet_dbg_ratelimited(\"icmp6_send: no reply to icmp error [%pI6c > %pI6c]\\n\",\n\t\t\t\t    &hdr->saddr, &hdr->daddr);\n\t\treturn;\n\t}\n\n\tmip6_addr_swap(skb);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_ICMPV6;\n\tfl6.daddr = hdr->saddr;\n\tif (force_saddr)\n\t\tsaddr = force_saddr;\n\tif (saddr)\n\t\tfl6.saddr = *saddr;\n\tfl6.flowi6_mark = mark;\n\tfl6.flowi6_oif = iif;\n\tfl6.fl6_icmp_type = type;\n\tfl6.fl6_icmp_code = code;\n\tsecurity_skb_classify_flow(skb, flowi6_to_flowi(&fl6));\n\n\tsk = icmpv6_xmit_lock(net);\n\tif (!sk)\n\t\treturn;\n\tsk->sk_mark = mark;\n\tnp = inet6_sk(sk);\n\n\tif (!icmpv6_xrlim_allow(sk, type, &fl6))\n\t\tgoto out;\n\n\ttmp_hdr.icmp6_type = type;\n\ttmp_hdr.icmp6_code = code;\n\ttmp_hdr.icmp6_cksum = 0;\n\ttmp_hdr.icmp6_pointer = htonl(info);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tipc6.tclass = np->tclass;\n\tfl6.flowlabel = ip6_make_flowinfo(ipc6.tclass, fl6.flowlabel);\n\n\tdst = icmpv6_route_lookup(net, skb, sk, &fl6);\n\tif (IS_ERR(dst))\n\t\tgoto out;\n\n\tipc6.hlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\tipc6.dontfrag = np->dontfrag;\n\tipc6.opt = NULL;\n\n\tmsg.skb = skb;\n\tmsg.offset = skb_network_offset(skb);\n\tmsg.type = type;\n\n\tlen = skb->len - msg.offset;\n\tlen = min_t(unsigned int, len, IPV6_MIN_MTU - sizeof(struct ipv6hdr) - sizeof(struct icmp6hdr));\n\tif (len < 0) {\n\t\tnet_dbg_ratelimited(\"icmp: len problem [%pI6c > %pI6c]\\n\",\n\t\t\t\t    &hdr->saddr, &hdr->daddr);\n\t\tgoto out_dst_release;\n\t}\n\n\trcu_read_lock();\n\tidev = __in6_dev_get(skb->dev);\n\n\terr = ip6_append_data(sk, icmpv6_getfrag, &msg,\n\t\t\t      len + sizeof(struct icmp6hdr),\n\t\t\t      sizeof(struct icmp6hdr),\n\t\t\t      &ipc6, &fl6, (struct rt6_info *)dst,\n\t\t\t      MSG_DONTWAIT, &sockc_unused);\n\tif (err) {\n\t\tICMP6_INC_STATS(net, idev, ICMP6_MIB_OUTERRORS);\n\t\tip6_flush_pending_frames(sk);\n\t} else {\n\t\terr = icmpv6_push_pending_frames(sk, &fl6, &tmp_hdr,\n\t\t\t\t\t\t len + sizeof(struct icmp6hdr));\n\t}\n\trcu_read_unlock();\nout_dst_release:\n\tdst_release(dst);\nout:\n\ticmpv6_xmit_unlock(sk);\n}",
        "code_after_change": "static void icmp6_send(struct sk_buff *skb, u8 type, u8 code, __u32 info,\n\t\t       const struct in6_addr *force_saddr)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tstruct inet6_dev *idev = NULL;\n\tstruct ipv6hdr *hdr = ipv6_hdr(skb);\n\tstruct sock *sk;\n\tstruct ipv6_pinfo *np;\n\tconst struct in6_addr *saddr = NULL;\n\tstruct dst_entry *dst;\n\tstruct icmp6hdr tmp_hdr;\n\tstruct flowi6 fl6;\n\tstruct icmpv6_msg msg;\n\tstruct sockcm_cookie sockc_unused = {0};\n\tstruct ipcm6_cookie ipc6;\n\tint iif = 0;\n\tint addr_type = 0;\n\tint len;\n\tint err = 0;\n\tu32 mark = IP6_REPLY_MARK(net, skb->mark);\n\n\tif ((u8 *)hdr < skb->head ||\n\t    (skb_network_header(skb) + sizeof(*hdr)) > skb_tail_pointer(skb))\n\t\treturn;\n\n\t/*\n\t *\tMake sure we respect the rules\n\t *\ti.e. RFC 1885 2.4(e)\n\t *\tRule (e.1) is enforced by not using icmp6_send\n\t *\tin any code that processes icmp errors.\n\t */\n\taddr_type = ipv6_addr_type(&hdr->daddr);\n\n\tif (ipv6_chk_addr(net, &hdr->daddr, skb->dev, 0) ||\n\t    ipv6_chk_acast_addr_src(net, skb->dev, &hdr->daddr))\n\t\tsaddr = &hdr->daddr;\n\n\t/*\n\t *\tDest addr check\n\t */\n\n\tif (addr_type & IPV6_ADDR_MULTICAST || skb->pkt_type != PACKET_HOST) {\n\t\tif (type != ICMPV6_PKT_TOOBIG &&\n\t\t    !(type == ICMPV6_PARAMPROB &&\n\t\t      code == ICMPV6_UNK_OPTION &&\n\t\t      (opt_unrec(skb, info))))\n\t\t\treturn;\n\n\t\tsaddr = NULL;\n\t}\n\n\taddr_type = ipv6_addr_type(&hdr->saddr);\n\n\t/*\n\t *\tSource addr check\n\t */\n\n\tif (__ipv6_addr_needs_scope_id(addr_type))\n\t\tiif = skb->dev->ifindex;\n\telse {\n\t\tdst = skb_dst(skb);\n\t\tiif = l3mdev_master_ifindex(dst ? dst->dev : skb->dev);\n\t}\n\n\t/*\n\t *\tMust not send error if the source does not uniquely\n\t *\tidentify a single node (RFC2463 Section 2.4).\n\t *\tWe check unspecified / multicast addresses here,\n\t *\tand anycast addresses will be checked later.\n\t */\n\tif ((addr_type == IPV6_ADDR_ANY) || (addr_type & IPV6_ADDR_MULTICAST)) {\n\t\tnet_dbg_ratelimited(\"icmp6_send: addr_any/mcast source [%pI6c > %pI6c]\\n\",\n\t\t\t\t    &hdr->saddr, &hdr->daddr);\n\t\treturn;\n\t}\n\n\t/*\n\t *\tNever answer to a ICMP packet.\n\t */\n\tif (is_ineligible(skb)) {\n\t\tnet_dbg_ratelimited(\"icmp6_send: no reply to icmp error [%pI6c > %pI6c]\\n\",\n\t\t\t\t    &hdr->saddr, &hdr->daddr);\n\t\treturn;\n\t}\n\n\tmip6_addr_swap(skb);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_ICMPV6;\n\tfl6.daddr = hdr->saddr;\n\tif (force_saddr)\n\t\tsaddr = force_saddr;\n\tif (saddr)\n\t\tfl6.saddr = *saddr;\n\tfl6.flowi6_mark = mark;\n\tfl6.flowi6_oif = iif;\n\tfl6.fl6_icmp_type = type;\n\tfl6.fl6_icmp_code = code;\n\tsecurity_skb_classify_flow(skb, flowi6_to_flowi(&fl6));\n\n\tsk = icmpv6_xmit_lock(net);\n\tif (!sk)\n\t\treturn;\n\tsk->sk_mark = mark;\n\tnp = inet6_sk(sk);\n\n\tif (!icmpv6_xrlim_allow(sk, type, &fl6))\n\t\tgoto out;\n\n\ttmp_hdr.icmp6_type = type;\n\ttmp_hdr.icmp6_code = code;\n\ttmp_hdr.icmp6_cksum = 0;\n\ttmp_hdr.icmp6_pointer = htonl(info);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tipc6.tclass = np->tclass;\n\tfl6.flowlabel = ip6_make_flowinfo(ipc6.tclass, fl6.flowlabel);\n\n\tdst = icmpv6_route_lookup(net, skb, sk, &fl6);\n\tif (IS_ERR(dst))\n\t\tgoto out;\n\n\tipc6.hlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\tipc6.dontfrag = np->dontfrag;\n\tipc6.opt = NULL;\n\n\tmsg.skb = skb;\n\tmsg.offset = skb_network_offset(skb);\n\tmsg.type = type;\n\n\tlen = skb->len - msg.offset;\n\tlen = min_t(unsigned int, len, IPV6_MIN_MTU - sizeof(struct ipv6hdr) - sizeof(struct icmp6hdr));\n\tif (len < 0) {\n\t\tnet_dbg_ratelimited(\"icmp: len problem [%pI6c > %pI6c]\\n\",\n\t\t\t\t    &hdr->saddr, &hdr->daddr);\n\t\tgoto out_dst_release;\n\t}\n\n\trcu_read_lock();\n\tidev = __in6_dev_get(skb->dev);\n\n\terr = ip6_append_data(sk, icmpv6_getfrag, &msg,\n\t\t\t      len + sizeof(struct icmp6hdr),\n\t\t\t      sizeof(struct icmp6hdr),\n\t\t\t      &ipc6, &fl6, (struct rt6_info *)dst,\n\t\t\t      MSG_DONTWAIT, &sockc_unused);\n\tif (err) {\n\t\tICMP6_INC_STATS(net, idev, ICMP6_MIB_OUTERRORS);\n\t\tip6_flush_pending_frames(sk);\n\t} else {\n\t\terr = icmpv6_push_pending_frames(sk, &fl6, &tmp_hdr,\n\t\t\t\t\t\t len + sizeof(struct icmp6hdr));\n\t}\n\trcu_read_unlock();\nout_dst_release:\n\tdst_release(dst);\nout:\n\ticmpv6_xmit_unlock(sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -57,8 +57,10 @@\n \n \tif (__ipv6_addr_needs_scope_id(addr_type))\n \t\tiif = skb->dev->ifindex;\n-\telse\n-\t\tiif = l3mdev_master_ifindex(skb_dst(skb)->dev);\n+\telse {\n+\t\tdst = skb_dst(skb);\n+\t\tiif = l3mdev_master_ifindex(dst ? dst->dev : skb->dev);\n+\t}\n \n \t/*\n \t *\tMust not send error if the source does not uniquely",
        "function_modified_lines": {
            "added": [
                "\telse {",
                "\t\tdst = skb_dst(skb);",
                "\t\tiif = l3mdev_master_ifindex(dst ? dst->dev : skb->dev);",
                "\t}"
            ],
            "deleted": [
                "\telse",
                "\t\tiif = l3mdev_master_ifindex(skb_dst(skb)->dev);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The icmp6_send function in net/ipv6/icmp.c in the Linux kernel through 4.8.12 omits a certain check of the dst data structure, which allows remote attackers to cause a denial of service (panic) via a fragmented IPv6 packet.",
        "id": 1170
    },
    {
        "cve_id": "CVE-2016-8650",
        "code_before_change": "int mpi_powm(MPI res, MPI base, MPI exp, MPI mod)\n{\n\tmpi_ptr_t mp_marker = NULL, bp_marker = NULL, ep_marker = NULL;\n\tmpi_ptr_t xp_marker = NULL;\n\tmpi_ptr_t tspace = NULL;\n\tmpi_ptr_t rp, ep, mp, bp;\n\tmpi_size_t esize, msize, bsize, rsize;\n\tint esign, msign, bsign, rsign;\n\tmpi_size_t size;\n\tint mod_shift_cnt;\n\tint negative_result;\n\tint assign_rp = 0;\n\tmpi_size_t tsize = 0;\t/* to avoid compiler warning */\n\t/* fixme: we should check that the warning is void */\n\tint rc = -ENOMEM;\n\n\tesize = exp->nlimbs;\n\tmsize = mod->nlimbs;\n\tsize = 2 * msize;\n\tesign = exp->sign;\n\tmsign = mod->sign;\n\n\trp = res->d;\n\tep = exp->d;\n\n\tif (!msize)\n\t\treturn -EINVAL;\n\n\tif (!esize) {\n\t\t/* Exponent is zero, result is 1 mod MOD, i.e., 1 or 0\n\t\t * depending on if MOD equals 1.  */\n\t\trp[0] = 1;\n\t\tres->nlimbs = (msize == 1 && mod->d[0] == 1) ? 0 : 1;\n\t\tres->sign = 0;\n\t\tgoto leave;\n\t}\n\n\t/* Normalize MOD (i.e. make its most significant bit set) as required by\n\t * mpn_divrem.  This will make the intermediate values in the calculation\n\t * slightly larger, but the correct result is obtained after a final\n\t * reduction using the original MOD value.  */\n\tmp = mp_marker = mpi_alloc_limb_space(msize);\n\tif (!mp)\n\t\tgoto enomem;\n\tmod_shift_cnt = count_leading_zeros(mod->d[msize - 1]);\n\tif (mod_shift_cnt)\n\t\tmpihelp_lshift(mp, mod->d, msize, mod_shift_cnt);\n\telse\n\t\tMPN_COPY(mp, mod->d, msize);\n\n\tbsize = base->nlimbs;\n\tbsign = base->sign;\n\tif (bsize > msize) {\t/* The base is larger than the module. Reduce it. */\n\t\t/* Allocate (BSIZE + 1) with space for remainder and quotient.\n\t\t * (The quotient is (bsize - msize + 1) limbs.)  */\n\t\tbp = bp_marker = mpi_alloc_limb_space(bsize + 1);\n\t\tif (!bp)\n\t\t\tgoto enomem;\n\t\tMPN_COPY(bp, base->d, bsize);\n\t\t/* We don't care about the quotient, store it above the remainder,\n\t\t * at BP + MSIZE.  */\n\t\tmpihelp_divrem(bp + msize, 0, bp, bsize, mp, msize);\n\t\tbsize = msize;\n\t\t/* Canonicalize the base, since we are going to multiply with it\n\t\t * quite a few times.  */\n\t\tMPN_NORMALIZE(bp, bsize);\n\t} else\n\t\tbp = base->d;\n\n\tif (!bsize) {\n\t\tres->nlimbs = 0;\n\t\tres->sign = 0;\n\t\tgoto leave;\n\t}\n\n\tif (res->alloced < size) {\n\t\t/* We have to allocate more space for RES.  If any of the input\n\t\t * parameters are identical to RES, defer deallocation of the old\n\t\t * space.  */\n\t\tif (rp == ep || rp == mp || rp == bp) {\n\t\t\trp = mpi_alloc_limb_space(size);\n\t\t\tif (!rp)\n\t\t\t\tgoto enomem;\n\t\t\tassign_rp = 1;\n\t\t} else {\n\t\t\tif (mpi_resize(res, size) < 0)\n\t\t\t\tgoto enomem;\n\t\t\trp = res->d;\n\t\t}\n\t} else {\t\t/* Make BASE, EXP and MOD not overlap with RES.  */\n\t\tif (rp == bp) {\n\t\t\t/* RES and BASE are identical.  Allocate temp. space for BASE.  */\n\t\t\tBUG_ON(bp_marker);\n\t\t\tbp = bp_marker = mpi_alloc_limb_space(bsize);\n\t\t\tif (!bp)\n\t\t\t\tgoto enomem;\n\t\t\tMPN_COPY(bp, rp, bsize);\n\t\t}\n\t\tif (rp == ep) {\n\t\t\t/* RES and EXP are identical.  Allocate temp. space for EXP.  */\n\t\t\tep = ep_marker = mpi_alloc_limb_space(esize);\n\t\t\tif (!ep)\n\t\t\t\tgoto enomem;\n\t\t\tMPN_COPY(ep, rp, esize);\n\t\t}\n\t\tif (rp == mp) {\n\t\t\t/* RES and MOD are identical.  Allocate temporary space for MOD. */\n\t\t\tBUG_ON(mp_marker);\n\t\t\tmp = mp_marker = mpi_alloc_limb_space(msize);\n\t\t\tif (!mp)\n\t\t\t\tgoto enomem;\n\t\t\tMPN_COPY(mp, rp, msize);\n\t\t}\n\t}\n\n\tMPN_COPY(rp, bp, bsize);\n\trsize = bsize;\n\trsign = bsign;\n\n\t{\n\t\tmpi_size_t i;\n\t\tmpi_ptr_t xp;\n\t\tint c;\n\t\tmpi_limb_t e;\n\t\tmpi_limb_t carry_limb;\n\t\tstruct karatsuba_ctx karactx;\n\n\t\txp = xp_marker = mpi_alloc_limb_space(2 * (msize + 1));\n\t\tif (!xp)\n\t\t\tgoto enomem;\n\n\t\tmemset(&karactx, 0, sizeof karactx);\n\t\tnegative_result = (ep[0] & 1) && base->sign;\n\n\t\ti = esize - 1;\n\t\te = ep[i];\n\t\tc = count_leading_zeros(e);\n\t\te = (e << c) << 1;\t/* shift the exp bits to the left, lose msb */\n\t\tc = BITS_PER_MPI_LIMB - 1 - c;\n\n\t\t/* Main loop.\n\t\t *\n\t\t * Make the result be pointed to alternately by XP and RP.  This\n\t\t * helps us avoid block copying, which would otherwise be necessary\n\t\t * with the overlap restrictions of mpihelp_divmod. With 50% probability\n\t\t * the result after this loop will be in the area originally pointed\n\t\t * by RP (==RES->d), and with 50% probability in the area originally\n\t\t * pointed to by XP.\n\t\t */\n\n\t\tfor (;;) {\n\t\t\twhile (c) {\n\t\t\t\tmpi_ptr_t tp;\n\t\t\t\tmpi_size_t xsize;\n\n\t\t\t\t/*if (mpihelp_mul_n(xp, rp, rp, rsize) < 0) goto enomem */\n\t\t\t\tif (rsize < KARATSUBA_THRESHOLD)\n\t\t\t\t\tmpih_sqr_n_basecase(xp, rp, rsize);\n\t\t\t\telse {\n\t\t\t\t\tif (!tspace) {\n\t\t\t\t\t\ttsize = 2 * rsize;\n\t\t\t\t\t\ttspace =\n\t\t\t\t\t\t    mpi_alloc_limb_space(tsize);\n\t\t\t\t\t\tif (!tspace)\n\t\t\t\t\t\t\tgoto enomem;\n\t\t\t\t\t} else if (tsize < (2 * rsize)) {\n\t\t\t\t\t\tmpi_free_limb_space(tspace);\n\t\t\t\t\t\ttsize = 2 * rsize;\n\t\t\t\t\t\ttspace =\n\t\t\t\t\t\t    mpi_alloc_limb_space(tsize);\n\t\t\t\t\t\tif (!tspace)\n\t\t\t\t\t\t\tgoto enomem;\n\t\t\t\t\t}\n\t\t\t\t\tmpih_sqr_n(xp, rp, rsize, tspace);\n\t\t\t\t}\n\n\t\t\t\txsize = 2 * rsize;\n\t\t\t\tif (xsize > msize) {\n\t\t\t\t\tmpihelp_divrem(xp + msize, 0, xp, xsize,\n\t\t\t\t\t\t       mp, msize);\n\t\t\t\t\txsize = msize;\n\t\t\t\t}\n\n\t\t\t\ttp = rp;\n\t\t\t\trp = xp;\n\t\t\t\txp = tp;\n\t\t\t\trsize = xsize;\n\n\t\t\t\tif ((mpi_limb_signed_t) e < 0) {\n\t\t\t\t\t/*mpihelp_mul( xp, rp, rsize, bp, bsize ); */\n\t\t\t\t\tif (bsize < KARATSUBA_THRESHOLD) {\n\t\t\t\t\t\tmpi_limb_t tmp;\n\t\t\t\t\t\tif (mpihelp_mul\n\t\t\t\t\t\t    (xp, rp, rsize, bp, bsize,\n\t\t\t\t\t\t     &tmp) < 0)\n\t\t\t\t\t\t\tgoto enomem;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tif (mpihelp_mul_karatsuba_case\n\t\t\t\t\t\t    (xp, rp, rsize, bp, bsize,\n\t\t\t\t\t\t     &karactx) < 0)\n\t\t\t\t\t\t\tgoto enomem;\n\t\t\t\t\t}\n\n\t\t\t\t\txsize = rsize + bsize;\n\t\t\t\t\tif (xsize > msize) {\n\t\t\t\t\t\tmpihelp_divrem(xp + msize, 0,\n\t\t\t\t\t\t\t       xp, xsize, mp,\n\t\t\t\t\t\t\t       msize);\n\t\t\t\t\t\txsize = msize;\n\t\t\t\t\t}\n\n\t\t\t\t\ttp = rp;\n\t\t\t\t\trp = xp;\n\t\t\t\t\txp = tp;\n\t\t\t\t\trsize = xsize;\n\t\t\t\t}\n\t\t\t\te <<= 1;\n\t\t\t\tc--;\n\t\t\t}\n\n\t\t\ti--;\n\t\t\tif (i < 0)\n\t\t\t\tbreak;\n\t\t\te = ep[i];\n\t\t\tc = BITS_PER_MPI_LIMB;\n\t\t}\n\n\t\t/* We shifted MOD, the modulo reduction argument, left MOD_SHIFT_CNT\n\t\t * steps.  Adjust the result by reducing it with the original MOD.\n\t\t *\n\t\t * Also make sure the result is put in RES->d (where it already\n\t\t * might be, see above).\n\t\t */\n\t\tif (mod_shift_cnt) {\n\t\t\tcarry_limb =\n\t\t\t    mpihelp_lshift(res->d, rp, rsize, mod_shift_cnt);\n\t\t\trp = res->d;\n\t\t\tif (carry_limb) {\n\t\t\t\trp[rsize] = carry_limb;\n\t\t\t\trsize++;\n\t\t\t}\n\t\t} else {\n\t\t\tMPN_COPY(res->d, rp, rsize);\n\t\t\trp = res->d;\n\t\t}\n\n\t\tif (rsize >= msize) {\n\t\t\tmpihelp_divrem(rp + msize, 0, rp, rsize, mp, msize);\n\t\t\trsize = msize;\n\t\t}\n\n\t\t/* Remove any leading zero words from the result.  */\n\t\tif (mod_shift_cnt)\n\t\t\tmpihelp_rshift(rp, rp, rsize, mod_shift_cnt);\n\t\tMPN_NORMALIZE(rp, rsize);\n\n\t\tmpihelp_release_karatsuba_ctx(&karactx);\n\t}\n\n\tif (negative_result && rsize) {\n\t\tif (mod_shift_cnt)\n\t\t\tmpihelp_rshift(mp, mp, msize, mod_shift_cnt);\n\t\tmpihelp_sub(rp, mp, msize, rp, rsize);\n\t\trsize = msize;\n\t\trsign = msign;\n\t\tMPN_NORMALIZE(rp, rsize);\n\t}\n\tres->nlimbs = rsize;\n\tres->sign = rsign;\n\nleave:\n\trc = 0;\nenomem:\n\tif (assign_rp)\n\t\tmpi_assign_limb_space(res, rp, size);\n\tif (mp_marker)\n\t\tmpi_free_limb_space(mp_marker);\n\tif (bp_marker)\n\t\tmpi_free_limb_space(bp_marker);\n\tif (ep_marker)\n\t\tmpi_free_limb_space(ep_marker);\n\tif (xp_marker)\n\t\tmpi_free_limb_space(xp_marker);\n\tif (tspace)\n\t\tmpi_free_limb_space(tspace);\n\treturn rc;\n}",
        "code_after_change": "int mpi_powm(MPI res, MPI base, MPI exp, MPI mod)\n{\n\tmpi_ptr_t mp_marker = NULL, bp_marker = NULL, ep_marker = NULL;\n\tmpi_ptr_t xp_marker = NULL;\n\tmpi_ptr_t tspace = NULL;\n\tmpi_ptr_t rp, ep, mp, bp;\n\tmpi_size_t esize, msize, bsize, rsize;\n\tint esign, msign, bsign, rsign;\n\tmpi_size_t size;\n\tint mod_shift_cnt;\n\tint negative_result;\n\tint assign_rp = 0;\n\tmpi_size_t tsize = 0;\t/* to avoid compiler warning */\n\t/* fixme: we should check that the warning is void */\n\tint rc = -ENOMEM;\n\n\tesize = exp->nlimbs;\n\tmsize = mod->nlimbs;\n\tsize = 2 * msize;\n\tesign = exp->sign;\n\tmsign = mod->sign;\n\n\trp = res->d;\n\tep = exp->d;\n\n\tif (!msize)\n\t\treturn -EINVAL;\n\n\tif (!esize) {\n\t\t/* Exponent is zero, result is 1 mod MOD, i.e., 1 or 0\n\t\t * depending on if MOD equals 1.  */\n\t\tres->nlimbs = (msize == 1 && mod->d[0] == 1) ? 0 : 1;\n\t\tif (res->nlimbs) {\n\t\t\tif (mpi_resize(res, 1) < 0)\n\t\t\t\tgoto enomem;\n\t\t\trp = res->d;\n\t\t\trp[0] = 1;\n\t\t}\n\t\tres->sign = 0;\n\t\tgoto leave;\n\t}\n\n\t/* Normalize MOD (i.e. make its most significant bit set) as required by\n\t * mpn_divrem.  This will make the intermediate values in the calculation\n\t * slightly larger, but the correct result is obtained after a final\n\t * reduction using the original MOD value.  */\n\tmp = mp_marker = mpi_alloc_limb_space(msize);\n\tif (!mp)\n\t\tgoto enomem;\n\tmod_shift_cnt = count_leading_zeros(mod->d[msize - 1]);\n\tif (mod_shift_cnt)\n\t\tmpihelp_lshift(mp, mod->d, msize, mod_shift_cnt);\n\telse\n\t\tMPN_COPY(mp, mod->d, msize);\n\n\tbsize = base->nlimbs;\n\tbsign = base->sign;\n\tif (bsize > msize) {\t/* The base is larger than the module. Reduce it. */\n\t\t/* Allocate (BSIZE + 1) with space for remainder and quotient.\n\t\t * (The quotient is (bsize - msize + 1) limbs.)  */\n\t\tbp = bp_marker = mpi_alloc_limb_space(bsize + 1);\n\t\tif (!bp)\n\t\t\tgoto enomem;\n\t\tMPN_COPY(bp, base->d, bsize);\n\t\t/* We don't care about the quotient, store it above the remainder,\n\t\t * at BP + MSIZE.  */\n\t\tmpihelp_divrem(bp + msize, 0, bp, bsize, mp, msize);\n\t\tbsize = msize;\n\t\t/* Canonicalize the base, since we are going to multiply with it\n\t\t * quite a few times.  */\n\t\tMPN_NORMALIZE(bp, bsize);\n\t} else\n\t\tbp = base->d;\n\n\tif (!bsize) {\n\t\tres->nlimbs = 0;\n\t\tres->sign = 0;\n\t\tgoto leave;\n\t}\n\n\tif (res->alloced < size) {\n\t\t/* We have to allocate more space for RES.  If any of the input\n\t\t * parameters are identical to RES, defer deallocation of the old\n\t\t * space.  */\n\t\tif (rp == ep || rp == mp || rp == bp) {\n\t\t\trp = mpi_alloc_limb_space(size);\n\t\t\tif (!rp)\n\t\t\t\tgoto enomem;\n\t\t\tassign_rp = 1;\n\t\t} else {\n\t\t\tif (mpi_resize(res, size) < 0)\n\t\t\t\tgoto enomem;\n\t\t\trp = res->d;\n\t\t}\n\t} else {\t\t/* Make BASE, EXP and MOD not overlap with RES.  */\n\t\tif (rp == bp) {\n\t\t\t/* RES and BASE are identical.  Allocate temp. space for BASE.  */\n\t\t\tBUG_ON(bp_marker);\n\t\t\tbp = bp_marker = mpi_alloc_limb_space(bsize);\n\t\t\tif (!bp)\n\t\t\t\tgoto enomem;\n\t\t\tMPN_COPY(bp, rp, bsize);\n\t\t}\n\t\tif (rp == ep) {\n\t\t\t/* RES and EXP are identical.  Allocate temp. space for EXP.  */\n\t\t\tep = ep_marker = mpi_alloc_limb_space(esize);\n\t\t\tif (!ep)\n\t\t\t\tgoto enomem;\n\t\t\tMPN_COPY(ep, rp, esize);\n\t\t}\n\t\tif (rp == mp) {\n\t\t\t/* RES and MOD are identical.  Allocate temporary space for MOD. */\n\t\t\tBUG_ON(mp_marker);\n\t\t\tmp = mp_marker = mpi_alloc_limb_space(msize);\n\t\t\tif (!mp)\n\t\t\t\tgoto enomem;\n\t\t\tMPN_COPY(mp, rp, msize);\n\t\t}\n\t}\n\n\tMPN_COPY(rp, bp, bsize);\n\trsize = bsize;\n\trsign = bsign;\n\n\t{\n\t\tmpi_size_t i;\n\t\tmpi_ptr_t xp;\n\t\tint c;\n\t\tmpi_limb_t e;\n\t\tmpi_limb_t carry_limb;\n\t\tstruct karatsuba_ctx karactx;\n\n\t\txp = xp_marker = mpi_alloc_limb_space(2 * (msize + 1));\n\t\tif (!xp)\n\t\t\tgoto enomem;\n\n\t\tmemset(&karactx, 0, sizeof karactx);\n\t\tnegative_result = (ep[0] & 1) && base->sign;\n\n\t\ti = esize - 1;\n\t\te = ep[i];\n\t\tc = count_leading_zeros(e);\n\t\te = (e << c) << 1;\t/* shift the exp bits to the left, lose msb */\n\t\tc = BITS_PER_MPI_LIMB - 1 - c;\n\n\t\t/* Main loop.\n\t\t *\n\t\t * Make the result be pointed to alternately by XP and RP.  This\n\t\t * helps us avoid block copying, which would otherwise be necessary\n\t\t * with the overlap restrictions of mpihelp_divmod. With 50% probability\n\t\t * the result after this loop will be in the area originally pointed\n\t\t * by RP (==RES->d), and with 50% probability in the area originally\n\t\t * pointed to by XP.\n\t\t */\n\n\t\tfor (;;) {\n\t\t\twhile (c) {\n\t\t\t\tmpi_ptr_t tp;\n\t\t\t\tmpi_size_t xsize;\n\n\t\t\t\t/*if (mpihelp_mul_n(xp, rp, rp, rsize) < 0) goto enomem */\n\t\t\t\tif (rsize < KARATSUBA_THRESHOLD)\n\t\t\t\t\tmpih_sqr_n_basecase(xp, rp, rsize);\n\t\t\t\telse {\n\t\t\t\t\tif (!tspace) {\n\t\t\t\t\t\ttsize = 2 * rsize;\n\t\t\t\t\t\ttspace =\n\t\t\t\t\t\t    mpi_alloc_limb_space(tsize);\n\t\t\t\t\t\tif (!tspace)\n\t\t\t\t\t\t\tgoto enomem;\n\t\t\t\t\t} else if (tsize < (2 * rsize)) {\n\t\t\t\t\t\tmpi_free_limb_space(tspace);\n\t\t\t\t\t\ttsize = 2 * rsize;\n\t\t\t\t\t\ttspace =\n\t\t\t\t\t\t    mpi_alloc_limb_space(tsize);\n\t\t\t\t\t\tif (!tspace)\n\t\t\t\t\t\t\tgoto enomem;\n\t\t\t\t\t}\n\t\t\t\t\tmpih_sqr_n(xp, rp, rsize, tspace);\n\t\t\t\t}\n\n\t\t\t\txsize = 2 * rsize;\n\t\t\t\tif (xsize > msize) {\n\t\t\t\t\tmpihelp_divrem(xp + msize, 0, xp, xsize,\n\t\t\t\t\t\t       mp, msize);\n\t\t\t\t\txsize = msize;\n\t\t\t\t}\n\n\t\t\t\ttp = rp;\n\t\t\t\trp = xp;\n\t\t\t\txp = tp;\n\t\t\t\trsize = xsize;\n\n\t\t\t\tif ((mpi_limb_signed_t) e < 0) {\n\t\t\t\t\t/*mpihelp_mul( xp, rp, rsize, bp, bsize ); */\n\t\t\t\t\tif (bsize < KARATSUBA_THRESHOLD) {\n\t\t\t\t\t\tmpi_limb_t tmp;\n\t\t\t\t\t\tif (mpihelp_mul\n\t\t\t\t\t\t    (xp, rp, rsize, bp, bsize,\n\t\t\t\t\t\t     &tmp) < 0)\n\t\t\t\t\t\t\tgoto enomem;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tif (mpihelp_mul_karatsuba_case\n\t\t\t\t\t\t    (xp, rp, rsize, bp, bsize,\n\t\t\t\t\t\t     &karactx) < 0)\n\t\t\t\t\t\t\tgoto enomem;\n\t\t\t\t\t}\n\n\t\t\t\t\txsize = rsize + bsize;\n\t\t\t\t\tif (xsize > msize) {\n\t\t\t\t\t\tmpihelp_divrem(xp + msize, 0,\n\t\t\t\t\t\t\t       xp, xsize, mp,\n\t\t\t\t\t\t\t       msize);\n\t\t\t\t\t\txsize = msize;\n\t\t\t\t\t}\n\n\t\t\t\t\ttp = rp;\n\t\t\t\t\trp = xp;\n\t\t\t\t\txp = tp;\n\t\t\t\t\trsize = xsize;\n\t\t\t\t}\n\t\t\t\te <<= 1;\n\t\t\t\tc--;\n\t\t\t}\n\n\t\t\ti--;\n\t\t\tif (i < 0)\n\t\t\t\tbreak;\n\t\t\te = ep[i];\n\t\t\tc = BITS_PER_MPI_LIMB;\n\t\t}\n\n\t\t/* We shifted MOD, the modulo reduction argument, left MOD_SHIFT_CNT\n\t\t * steps.  Adjust the result by reducing it with the original MOD.\n\t\t *\n\t\t * Also make sure the result is put in RES->d (where it already\n\t\t * might be, see above).\n\t\t */\n\t\tif (mod_shift_cnt) {\n\t\t\tcarry_limb =\n\t\t\t    mpihelp_lshift(res->d, rp, rsize, mod_shift_cnt);\n\t\t\trp = res->d;\n\t\t\tif (carry_limb) {\n\t\t\t\trp[rsize] = carry_limb;\n\t\t\t\trsize++;\n\t\t\t}\n\t\t} else {\n\t\t\tMPN_COPY(res->d, rp, rsize);\n\t\t\trp = res->d;\n\t\t}\n\n\t\tif (rsize >= msize) {\n\t\t\tmpihelp_divrem(rp + msize, 0, rp, rsize, mp, msize);\n\t\t\trsize = msize;\n\t\t}\n\n\t\t/* Remove any leading zero words from the result.  */\n\t\tif (mod_shift_cnt)\n\t\t\tmpihelp_rshift(rp, rp, rsize, mod_shift_cnt);\n\t\tMPN_NORMALIZE(rp, rsize);\n\n\t\tmpihelp_release_karatsuba_ctx(&karactx);\n\t}\n\n\tif (negative_result && rsize) {\n\t\tif (mod_shift_cnt)\n\t\t\tmpihelp_rshift(mp, mp, msize, mod_shift_cnt);\n\t\tmpihelp_sub(rp, mp, msize, rp, rsize);\n\t\trsize = msize;\n\t\trsign = msign;\n\t\tMPN_NORMALIZE(rp, rsize);\n\t}\n\tres->nlimbs = rsize;\n\tres->sign = rsign;\n\nleave:\n\trc = 0;\nenomem:\n\tif (assign_rp)\n\t\tmpi_assign_limb_space(res, rp, size);\n\tif (mp_marker)\n\t\tmpi_free_limb_space(mp_marker);\n\tif (bp_marker)\n\t\tmpi_free_limb_space(bp_marker);\n\tif (ep_marker)\n\t\tmpi_free_limb_space(ep_marker);\n\tif (xp_marker)\n\t\tmpi_free_limb_space(xp_marker);\n\tif (tspace)\n\t\tmpi_free_limb_space(tspace);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,8 +29,13 @@\n \tif (!esize) {\n \t\t/* Exponent is zero, result is 1 mod MOD, i.e., 1 or 0\n \t\t * depending on if MOD equals 1.  */\n-\t\trp[0] = 1;\n \t\tres->nlimbs = (msize == 1 && mod->d[0] == 1) ? 0 : 1;\n+\t\tif (res->nlimbs) {\n+\t\t\tif (mpi_resize(res, 1) < 0)\n+\t\t\t\tgoto enomem;\n+\t\t\trp = res->d;\n+\t\t\trp[0] = 1;\n+\t\t}\n \t\tres->sign = 0;\n \t\tgoto leave;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (res->nlimbs) {",
                "\t\t\tif (mpi_resize(res, 1) < 0)",
                "\t\t\t\tgoto enomem;",
                "\t\t\trp = res->d;",
                "\t\t\trp[0] = 1;",
                "\t\t}"
            ],
            "deleted": [
                "\t\trp[0] = 1;"
            ]
        },
        "cwe": [
            "CWE-20",
            "CWE-399"
        ],
        "cve_description": "The mpi_powm function in lib/mpi/mpi-pow.c in the Linux kernel through 4.8.11 does not ensure that memory is allocated for limb data, which allows local users to cause a denial of service (stack memory corruption and panic) via an add_key system call for an RSA key with a zero exponent.",
        "id": 1130
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe,\n\t\t       struct io_submit_state *state)\n{\n\tunsigned int sqe_flags;\n\tint id;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->io = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->flags = 0;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\tget_task_struct(req->task);\n\treq->result = 0;\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm(ctx, req)))\n\t\treturn -EFAULT;\n\n\tsqe_flags = READ_ONCE(sqe->flags);\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS))\n\t\treturn -EINVAL;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\tio_req_init_async(req);\n\t\treq->work.creds = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!req->work.creds))\n\t\t\treturn -EINVAL;\n\t\tget_cred(req->work.creds);\n\t}\n\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags |= sqe_flags;\n\n\tif (!io_op_defs[req->opcode].needs_file)\n\t\treturn 0;\n\n\treturn io_req_set_file(state, req, READ_ONCE(sqe->fd));\n}",
        "code_after_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe,\n\t\t       struct io_submit_state *state)\n{\n\tunsigned int sqe_flags;\n\tint id;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->io = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->flags = 0;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\tget_task_struct(req->task);\n\tatomic_long_inc(&req->task->io_uring->req_issue);\n\treq->result = 0;\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm(ctx, req)))\n\t\treturn -EFAULT;\n\n\tsqe_flags = READ_ONCE(sqe->flags);\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS))\n\t\treturn -EINVAL;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\tio_req_init_async(req);\n\t\treq->work.creds = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!req->work.creds))\n\t\t\treturn -EINVAL;\n\t\tget_cred(req->work.creds);\n\t}\n\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags |= sqe_flags;\n\n\tif (!io_op_defs[req->opcode].needs_file)\n\t\treturn 0;\n\n\treturn io_req_set_file(state, req, READ_ONCE(sqe->fd));\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,6 +15,7 @@\n \trefcount_set(&req->refs, 2);\n \treq->task = current;\n \tget_task_struct(req->task);\n+\tatomic_long_inc(&req->task->io_uring->req_issue);\n \treq->result = 0;\n \n \tif (unlikely(req->opcode >= IORING_OP_LAST))",
        "function_modified_lines": {
            "added": [
                "\tatomic_long_inc(&req->task->io_uring->req_issue);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2854
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "void __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(refcount_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk, true);\n\tsecurity_task_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}",
        "code_after_change": "void __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(refcount_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\tio_uring_free(tsk);\n\tcgroup_free(tsk);\n\ttask_numa_free(tsk, true);\n\tsecurity_task_free(tsk);\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \tWARN_ON(refcount_read(&tsk->usage));\n \tWARN_ON(tsk == current);\n \n+\tio_uring_free(tsk);\n \tcgroup_free(tsk);\n \ttask_numa_free(tsk, true);\n \tsecurity_task_free(tsk);",
        "function_modified_lines": {
            "added": [
                "\tio_uring_free(tsk);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2860
    },
    {
        "cve_id": "CVE-2019-9503",
        "code_before_change": "void brcmf_rx_event(struct device *dev, struct sk_buff *skb)\n{\n\tstruct brcmf_if *ifp;\n\tstruct brcmf_bus *bus_if = dev_get_drvdata(dev);\n\tstruct brcmf_pub *drvr = bus_if->drvr;\n\n\tbrcmf_dbg(EVENT, \"Enter: %s: rxp=%p\\n\", dev_name(dev), skb);\n\n\tif (brcmf_rx_hdrpull(drvr, skb, &ifp))\n\t\treturn;\n\n\tbrcmf_fweh_process_skb(ifp->drvr, skb);\n\tbrcmu_pkt_buf_free_skb(skb);\n}",
        "code_after_change": "void brcmf_rx_event(struct device *dev, struct sk_buff *skb)\n{\n\tstruct brcmf_if *ifp;\n\tstruct brcmf_bus *bus_if = dev_get_drvdata(dev);\n\tstruct brcmf_pub *drvr = bus_if->drvr;\n\n\tbrcmf_dbg(EVENT, \"Enter: %s: rxp=%p\\n\", dev_name(dev), skb);\n\n\tif (brcmf_rx_hdrpull(drvr, skb, &ifp))\n\t\treturn;\n\n\tbrcmf_fweh_process_skb(ifp->drvr, skb, 0);\n\tbrcmu_pkt_buf_free_skb(skb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,6 @@\n \tif (brcmf_rx_hdrpull(drvr, skb, &ifp))\n \t\treturn;\n \n-\tbrcmf_fweh_process_skb(ifp->drvr, skb);\n+\tbrcmf_fweh_process_skb(ifp->drvr, skb, 0);\n \tbrcmu_pkt_buf_free_skb(skb);\n }",
        "function_modified_lines": {
            "added": [
                "\tbrcmf_fweh_process_skb(ifp->drvr, skb, 0);"
            ],
            "deleted": [
                "\tbrcmf_fweh_process_skb(ifp->drvr, skb);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Broadcom brcmfmac WiFi driver prior to commit a4176ec356c73a46c07c181c6d04039fafa34a9f is vulnerable to a frame validation bypass. If the brcmfmac driver receives a firmware event frame from a remote source, the is_wlc_event_frame function will cause this frame to be discarded and unprocessed. If the driver receives the firmware event frame from the host, the appropriate handler is called. This frame validation can be bypassed if the bus used is USB (for instance by a wifi dongle). This can allow firmware event frames from a remote source to be processed. In the worst case scenario, by sending specially-crafted WiFi packets, a remote, unauthenticated attacker may be able to execute arbitrary code on a vulnerable system. More typically, this vulnerability will result in denial-of-service conditions.",
        "id": 2366
    },
    {
        "cve_id": "CVE-2015-8552",
        "code_before_change": "int xen_pcibk_enable_msi(struct xen_pcibk_device *pdev,\n\t\t\t struct pci_dev *dev, struct xen_pci_op *op)\n{\n\tstruct xen_pcibk_dev_data *dev_data;\n\tint status;\n\n\tif (unlikely(verbose_request))\n\t\tprintk(KERN_DEBUG DRV_NAME \": %s: enable MSI\\n\", pci_name(dev));\n\n\tstatus = pci_enable_msi(dev);\n\n\tif (status) {\n\t\tpr_warn_ratelimited(\"%s: error enabling MSI for guest %u: err %d\\n\",\n\t\t\t\t    pci_name(dev), pdev->xdev->otherend_id,\n\t\t\t\t    status);\n\t\top->value = 0;\n\t\treturn XEN_PCI_ERR_op_failed;\n\t}\n\n\t/* The value the guest needs is actually the IDT vector, not the\n\t * the local domain's IRQ number. */\n\n\top->value = dev->irq ? xen_pirq_from_irq(dev->irq) : 0;\n\tif (unlikely(verbose_request))\n\t\tprintk(KERN_DEBUG DRV_NAME \": %s: MSI: %d\\n\", pci_name(dev),\n\t\t\top->value);\n\n\tdev_data = pci_get_drvdata(dev);\n\tif (dev_data)\n\t\tdev_data->ack_intr = 0;\n\n\treturn 0;\n}",
        "code_after_change": "int xen_pcibk_enable_msi(struct xen_pcibk_device *pdev,\n\t\t\t struct pci_dev *dev, struct xen_pci_op *op)\n{\n\tstruct xen_pcibk_dev_data *dev_data;\n\tint status;\n\n\tif (unlikely(verbose_request))\n\t\tprintk(KERN_DEBUG DRV_NAME \": %s: enable MSI\\n\", pci_name(dev));\n\n\tif (dev->msi_enabled)\n\t\tstatus = -EALREADY;\n\telse if (dev->msix_enabled)\n\t\tstatus = -ENXIO;\n\telse\n\t\tstatus = pci_enable_msi(dev);\n\n\tif (status) {\n\t\tpr_warn_ratelimited(\"%s: error enabling MSI for guest %u: err %d\\n\",\n\t\t\t\t    pci_name(dev), pdev->xdev->otherend_id,\n\t\t\t\t    status);\n\t\top->value = 0;\n\t\treturn XEN_PCI_ERR_op_failed;\n\t}\n\n\t/* The value the guest needs is actually the IDT vector, not the\n\t * the local domain's IRQ number. */\n\n\top->value = dev->irq ? xen_pirq_from_irq(dev->irq) : 0;\n\tif (unlikely(verbose_request))\n\t\tprintk(KERN_DEBUG DRV_NAME \": %s: MSI: %d\\n\", pci_name(dev),\n\t\t\top->value);\n\n\tdev_data = pci_get_drvdata(dev);\n\tif (dev_data)\n\t\tdev_data->ack_intr = 0;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,12 @@\n \tif (unlikely(verbose_request))\n \t\tprintk(KERN_DEBUG DRV_NAME \": %s: enable MSI\\n\", pci_name(dev));\n \n-\tstatus = pci_enable_msi(dev);\n+\tif (dev->msi_enabled)\n+\t\tstatus = -EALREADY;\n+\telse if (dev->msix_enabled)\n+\t\tstatus = -ENXIO;\n+\telse\n+\t\tstatus = pci_enable_msi(dev);\n \n \tif (status) {\n \t\tpr_warn_ratelimited(\"%s: error enabling MSI for guest %u: err %d\\n\",",
        "function_modified_lines": {
            "added": [
                "\tif (dev->msi_enabled)",
                "\t\tstatus = -EALREADY;",
                "\telse if (dev->msix_enabled)",
                "\t\tstatus = -ENXIO;",
                "\telse",
                "\t\tstatus = pci_enable_msi(dev);"
            ],
            "deleted": [
                "\tstatus = pci_enable_msi(dev);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The PCI backend driver in Xen, when running on an x86 system and using Linux 3.1.x through 4.3.x as the driver domain, allows local guest administrators to generate a continuous stream of WARN messages and cause a denial of service (disk consumption) by leveraging a system with access to a passed-through MSI or MSI-X capable physical PCI device and XEN_PCI_OP_enable_msi operations, aka \"Linux pciback missing sanity checks.\"",
        "id": 830
    },
    {
        "cve_id": "CVE-2014-9584",
        "code_before_change": "static int\nparse_rock_ridge_inode_internal(struct iso_directory_record *de,\n\t\t\t\tstruct inode *inode, int flags)\n{\n\tint symlink_len = 0;\n\tint cnt, sig;\n\tunsigned int reloc_block;\n\tstruct inode *reloc;\n\tstruct rock_ridge *rr;\n\tint rootflag;\n\tstruct rock_state rs;\n\tint ret = 0;\n\n\tif (!ISOFS_SB(inode->i_sb)->s_rock)\n\t\treturn 0;\n\n\tinit_rock_state(&rs, inode);\n\tsetup_rock_ridge(de, inode, &rs);\n\tif (flags & RR_REGARD_XA) {\n\t\trs.chr += 14;\n\t\trs.len -= 14;\n\t\tif (rs.len < 0)\n\t\t\trs.len = 0;\n\t}\n\nrepeat:\n\twhile (rs.len > 2) { /* There may be one byte for padding somewhere */\n\t\trr = (struct rock_ridge *)rs.chr;\n\t\t/*\n\t\t * Ignore rock ridge info if rr->len is out of range, but\n\t\t * don't return -EIO because that would make the file\n\t\t * invisible.\n\t\t */\n\t\tif (rr->len < 3)\n\t\t\tgoto out;\t/* Something got screwed up here */\n\t\tsig = isonum_721(rs.chr);\n\t\tif (rock_check_overflow(&rs, sig))\n\t\t\tgoto eio;\n\t\trs.chr += rr->len;\n\t\trs.len -= rr->len;\n\t\t/*\n\t\t * As above, just ignore the rock ridge info if rr->len\n\t\t * is bogus.\n\t\t */\n\t\tif (rs.len < 0)\n\t\t\tgoto out;\t/* Something got screwed up here */\n\n\t\tswitch (sig) {\n#ifndef CONFIG_ZISOFS\t\t/* No flag for SF or ZF */\n\t\tcase SIG('R', 'R'):\n\t\t\tif ((rr->u.RR.flags[0] &\n\t\t\t     (RR_PX | RR_TF | RR_SL | RR_CL)) == 0)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n#endif\n\t\tcase SIG('S', 'P'):\n\t\t\tif (check_sp(rr, inode))\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase SIG('C', 'E'):\n\t\t\trs.cont_extent = isonum_733(rr->u.CE.extent);\n\t\t\trs.cont_offset = isonum_733(rr->u.CE.offset);\n\t\t\trs.cont_size = isonum_733(rr->u.CE.size);\n\t\t\tbreak;\n\t\tcase SIG('E', 'R'):\n\t\t\tISOFS_SB(inode->i_sb)->s_rock = 1;\n\t\t\tprintk(KERN_DEBUG \"ISO 9660 Extensions: \");\n\t\t\t{\n\t\t\t\tint p;\n\t\t\t\tfor (p = 0; p < rr->u.ER.len_id; p++)\n\t\t\t\t\tprintk(\"%c\", rr->u.ER.data[p]);\n\t\t\t}\n\t\t\tprintk(\"\\n\");\n\t\t\tbreak;\n\t\tcase SIG('P', 'X'):\n\t\t\tinode->i_mode = isonum_733(rr->u.PX.mode);\n\t\t\tset_nlink(inode, isonum_733(rr->u.PX.n_links));\n\t\t\ti_uid_write(inode, isonum_733(rr->u.PX.uid));\n\t\t\ti_gid_write(inode, isonum_733(rr->u.PX.gid));\n\t\t\tbreak;\n\t\tcase SIG('P', 'N'):\n\t\t\t{\n\t\t\t\tint high, low;\n\t\t\t\thigh = isonum_733(rr->u.PN.dev_high);\n\t\t\t\tlow = isonum_733(rr->u.PN.dev_low);\n\t\t\t\t/*\n\t\t\t\t * The Rock Ridge standard specifies that if\n\t\t\t\t * sizeof(dev_t) <= 4, then the high field is\n\t\t\t\t * unused, and the device number is completely\n\t\t\t\t * stored in the low field.  Some writers may\n\t\t\t\t * ignore this subtlety,\n\t\t\t\t * and as a result we test to see if the entire\n\t\t\t\t * device number is\n\t\t\t\t * stored in the low field, and use that.\n\t\t\t\t */\n\t\t\t\tif ((low & ~0xff) && high == 0) {\n\t\t\t\t\tinode->i_rdev =\n\t\t\t\t\t    MKDEV(low >> 8, low & 0xff);\n\t\t\t\t} else {\n\t\t\t\t\tinode->i_rdev =\n\t\t\t\t\t    MKDEV(high, low);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SIG('T', 'F'):\n\t\t\t/*\n\t\t\t * Some RRIP writers incorrectly place ctime in the\n\t\t\t * TF_CREATE field. Try to handle this correctly for\n\t\t\t * either case.\n\t\t\t */\n\t\t\t/* Rock ridge never appears on a High Sierra disk */\n\t\t\tcnt = 0;\n\t\t\tif (rr->u.TF.flags & TF_CREATE) {\n\t\t\t\tinode->i_ctime.tv_sec =\n\t\t\t\t    iso_date(rr->u.TF.times[cnt++].time,\n\t\t\t\t\t     0);\n\t\t\t\tinode->i_ctime.tv_nsec = 0;\n\t\t\t}\n\t\t\tif (rr->u.TF.flags & TF_MODIFY) {\n\t\t\t\tinode->i_mtime.tv_sec =\n\t\t\t\t    iso_date(rr->u.TF.times[cnt++].time,\n\t\t\t\t\t     0);\n\t\t\t\tinode->i_mtime.tv_nsec = 0;\n\t\t\t}\n\t\t\tif (rr->u.TF.flags & TF_ACCESS) {\n\t\t\t\tinode->i_atime.tv_sec =\n\t\t\t\t    iso_date(rr->u.TF.times[cnt++].time,\n\t\t\t\t\t     0);\n\t\t\t\tinode->i_atime.tv_nsec = 0;\n\t\t\t}\n\t\t\tif (rr->u.TF.flags & TF_ATTRIBUTES) {\n\t\t\t\tinode->i_ctime.tv_sec =\n\t\t\t\t    iso_date(rr->u.TF.times[cnt++].time,\n\t\t\t\t\t     0);\n\t\t\t\tinode->i_ctime.tv_nsec = 0;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SIG('S', 'L'):\n\t\t\t{\n\t\t\t\tint slen;\n\t\t\t\tstruct SL_component *slp;\n\t\t\t\tstruct SL_component *oldslp;\n\t\t\t\tslen = rr->len - 5;\n\t\t\t\tslp = &rr->u.SL.link;\n\t\t\t\tinode->i_size = symlink_len;\n\t\t\t\twhile (slen > 1) {\n\t\t\t\t\trootflag = 0;\n\t\t\t\t\tswitch (slp->flags & ~1) {\n\t\t\t\t\tcase 0:\n\t\t\t\t\t\tinode->i_size +=\n\t\t\t\t\t\t    slp->len;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase 2:\n\t\t\t\t\t\tinode->i_size += 1;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase 4:\n\t\t\t\t\t\tinode->i_size += 2;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase 8:\n\t\t\t\t\t\trootflag = 1;\n\t\t\t\t\t\tinode->i_size += 1;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tprintk(\"Symlink component flag \"\n\t\t\t\t\t\t\t\"not implemented\\n\");\n\t\t\t\t\t}\n\t\t\t\t\tslen -= slp->len + 2;\n\t\t\t\t\toldslp = slp;\n\t\t\t\t\tslp = (struct SL_component *)\n\t\t\t\t\t\t(((char *)slp) + slp->len + 2);\n\n\t\t\t\t\tif (slen < 2) {\n\t\t\t\t\t\tif (((rr->u.SL.\n\t\t\t\t\t\t      flags & 1) != 0)\n\t\t\t\t\t\t    &&\n\t\t\t\t\t\t    ((oldslp->\n\t\t\t\t\t\t      flags & 1) == 0))\n\t\t\t\t\t\t\tinode->i_size +=\n\t\t\t\t\t\t\t    1;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\n\t\t\t\t\t/*\n\t\t\t\t\t * If this component record isn't\n\t\t\t\t\t * continued, then append a '/'.\n\t\t\t\t\t */\n\t\t\t\t\tif (!rootflag\n\t\t\t\t\t    && (oldslp->flags & 1) == 0)\n\t\t\t\t\t\tinode->i_size += 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tsymlink_len = inode->i_size;\n\t\t\tbreak;\n\t\tcase SIG('R', 'E'):\n\t\t\tprintk(KERN_WARNING \"Attempt to read inode for \"\n\t\t\t\t\t\"relocated directory\\n\");\n\t\t\tgoto out;\n\t\tcase SIG('C', 'L'):\n\t\t\tif (flags & RR_RELOC_DE) {\n\t\t\t\tprintk(KERN_ERR\n\t\t\t\t       \"ISOFS: Recursive directory relocation \"\n\t\t\t\t       \"is not supported\\n\");\n\t\t\t\tgoto eio;\n\t\t\t}\n\t\t\treloc_block = isonum_733(rr->u.CL.location);\n\t\t\tif (reloc_block == ISOFS_I(inode)->i_iget5_block &&\n\t\t\t    ISOFS_I(inode)->i_iget5_offset == 0) {\n\t\t\t\tprintk(KERN_ERR\n\t\t\t\t       \"ISOFS: Directory relocation points to \"\n\t\t\t\t       \"itself\\n\");\n\t\t\t\tgoto eio;\n\t\t\t}\n\t\t\tISOFS_I(inode)->i_first_extent = reloc_block;\n\t\t\treloc = isofs_iget_reloc(inode->i_sb, reloc_block, 0);\n\t\t\tif (IS_ERR(reloc)) {\n\t\t\t\tret = PTR_ERR(reloc);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tinode->i_mode = reloc->i_mode;\n\t\t\tset_nlink(inode, reloc->i_nlink);\n\t\t\tinode->i_uid = reloc->i_uid;\n\t\t\tinode->i_gid = reloc->i_gid;\n\t\t\tinode->i_rdev = reloc->i_rdev;\n\t\t\tinode->i_size = reloc->i_size;\n\t\t\tinode->i_blocks = reloc->i_blocks;\n\t\t\tinode->i_atime = reloc->i_atime;\n\t\t\tinode->i_ctime = reloc->i_ctime;\n\t\t\tinode->i_mtime = reloc->i_mtime;\n\t\t\tiput(reloc);\n\t\t\tbreak;\n#ifdef CONFIG_ZISOFS\n\t\tcase SIG('Z', 'F'): {\n\t\t\tint algo;\n\n\t\t\tif (ISOFS_SB(inode->i_sb)->s_nocompress)\n\t\t\t\tbreak;\n\t\t\talgo = isonum_721(rr->u.ZF.algorithm);\n\t\t\tif (algo == SIG('p', 'z')) {\n\t\t\t\tint block_shift =\n\t\t\t\t\tisonum_711(&rr->u.ZF.parms[1]);\n\t\t\t\tif (block_shift > 17) {\n\t\t\t\t\tprintk(KERN_WARNING \"isofs: \"\n\t\t\t\t\t\t\"Can't handle ZF block \"\n\t\t\t\t\t\t\"size of 2^%d\\n\",\n\t\t\t\t\t\tblock_shift);\n\t\t\t\t} else {\n\t\t\t\t\t/*\n\t\t\t\t\t * Note: we don't change\n\t\t\t\t\t * i_blocks here\n\t\t\t\t\t */\n\t\t\t\t\tISOFS_I(inode)->i_file_format =\n\t\t\t\t\t\tisofs_file_compressed;\n\t\t\t\t\t/*\n\t\t\t\t\t * Parameters to compression\n\t\t\t\t\t * algorithm (header size,\n\t\t\t\t\t * block size)\n\t\t\t\t\t */\n\t\t\t\t\tISOFS_I(inode)->i_format_parm[0] =\n\t\t\t\t\t\tisonum_711(&rr->u.ZF.parms[0]);\n\t\t\t\t\tISOFS_I(inode)->i_format_parm[1] =\n\t\t\t\t\t\tisonum_711(&rr->u.ZF.parms[1]);\n\t\t\t\t\tinode->i_size =\n\t\t\t\t\t    isonum_733(rr->u.ZF.\n\t\t\t\t\t\t       real_size);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tprintk(KERN_WARNING\n\t\t\t\t       \"isofs: Unknown ZF compression \"\n\t\t\t\t\t\t\"algorithm: %c%c\\n\",\n\t\t\t\t       rr->u.ZF.algorithm[0],\n\t\t\t\t       rr->u.ZF.algorithm[1]);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n#endif\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tret = rock_continue(&rs);\n\tif (ret == 0)\n\t\tgoto repeat;\n\tif (ret == 1)\n\t\tret = 0;\nout:\n\tkfree(rs.buffer);\n\treturn ret;\neio:\n\tret = -EIO;\n\tgoto out;\n}",
        "code_after_change": "static int\nparse_rock_ridge_inode_internal(struct iso_directory_record *de,\n\t\t\t\tstruct inode *inode, int flags)\n{\n\tint symlink_len = 0;\n\tint cnt, sig;\n\tunsigned int reloc_block;\n\tstruct inode *reloc;\n\tstruct rock_ridge *rr;\n\tint rootflag;\n\tstruct rock_state rs;\n\tint ret = 0;\n\n\tif (!ISOFS_SB(inode->i_sb)->s_rock)\n\t\treturn 0;\n\n\tinit_rock_state(&rs, inode);\n\tsetup_rock_ridge(de, inode, &rs);\n\tif (flags & RR_REGARD_XA) {\n\t\trs.chr += 14;\n\t\trs.len -= 14;\n\t\tif (rs.len < 0)\n\t\t\trs.len = 0;\n\t}\n\nrepeat:\n\twhile (rs.len > 2) { /* There may be one byte for padding somewhere */\n\t\trr = (struct rock_ridge *)rs.chr;\n\t\t/*\n\t\t * Ignore rock ridge info if rr->len is out of range, but\n\t\t * don't return -EIO because that would make the file\n\t\t * invisible.\n\t\t */\n\t\tif (rr->len < 3)\n\t\t\tgoto out;\t/* Something got screwed up here */\n\t\tsig = isonum_721(rs.chr);\n\t\tif (rock_check_overflow(&rs, sig))\n\t\t\tgoto eio;\n\t\trs.chr += rr->len;\n\t\trs.len -= rr->len;\n\t\t/*\n\t\t * As above, just ignore the rock ridge info if rr->len\n\t\t * is bogus.\n\t\t */\n\t\tif (rs.len < 0)\n\t\t\tgoto out;\t/* Something got screwed up here */\n\n\t\tswitch (sig) {\n#ifndef CONFIG_ZISOFS\t\t/* No flag for SF or ZF */\n\t\tcase SIG('R', 'R'):\n\t\t\tif ((rr->u.RR.flags[0] &\n\t\t\t     (RR_PX | RR_TF | RR_SL | RR_CL)) == 0)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n#endif\n\t\tcase SIG('S', 'P'):\n\t\t\tif (check_sp(rr, inode))\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase SIG('C', 'E'):\n\t\t\trs.cont_extent = isonum_733(rr->u.CE.extent);\n\t\t\trs.cont_offset = isonum_733(rr->u.CE.offset);\n\t\t\trs.cont_size = isonum_733(rr->u.CE.size);\n\t\t\tbreak;\n\t\tcase SIG('E', 'R'):\n\t\t\t/* Invalid length of ER tag id? */\n\t\t\tif (rr->u.ER.len_id + offsetof(struct rock_ridge, u.ER.data) > rr->len)\n\t\t\t\tgoto out;\n\t\t\tISOFS_SB(inode->i_sb)->s_rock = 1;\n\t\t\tprintk(KERN_DEBUG \"ISO 9660 Extensions: \");\n\t\t\t{\n\t\t\t\tint p;\n\t\t\t\tfor (p = 0; p < rr->u.ER.len_id; p++)\n\t\t\t\t\tprintk(\"%c\", rr->u.ER.data[p]);\n\t\t\t}\n\t\t\tprintk(\"\\n\");\n\t\t\tbreak;\n\t\tcase SIG('P', 'X'):\n\t\t\tinode->i_mode = isonum_733(rr->u.PX.mode);\n\t\t\tset_nlink(inode, isonum_733(rr->u.PX.n_links));\n\t\t\ti_uid_write(inode, isonum_733(rr->u.PX.uid));\n\t\t\ti_gid_write(inode, isonum_733(rr->u.PX.gid));\n\t\t\tbreak;\n\t\tcase SIG('P', 'N'):\n\t\t\t{\n\t\t\t\tint high, low;\n\t\t\t\thigh = isonum_733(rr->u.PN.dev_high);\n\t\t\t\tlow = isonum_733(rr->u.PN.dev_low);\n\t\t\t\t/*\n\t\t\t\t * The Rock Ridge standard specifies that if\n\t\t\t\t * sizeof(dev_t) <= 4, then the high field is\n\t\t\t\t * unused, and the device number is completely\n\t\t\t\t * stored in the low field.  Some writers may\n\t\t\t\t * ignore this subtlety,\n\t\t\t\t * and as a result we test to see if the entire\n\t\t\t\t * device number is\n\t\t\t\t * stored in the low field, and use that.\n\t\t\t\t */\n\t\t\t\tif ((low & ~0xff) && high == 0) {\n\t\t\t\t\tinode->i_rdev =\n\t\t\t\t\t    MKDEV(low >> 8, low & 0xff);\n\t\t\t\t} else {\n\t\t\t\t\tinode->i_rdev =\n\t\t\t\t\t    MKDEV(high, low);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SIG('T', 'F'):\n\t\t\t/*\n\t\t\t * Some RRIP writers incorrectly place ctime in the\n\t\t\t * TF_CREATE field. Try to handle this correctly for\n\t\t\t * either case.\n\t\t\t */\n\t\t\t/* Rock ridge never appears on a High Sierra disk */\n\t\t\tcnt = 0;\n\t\t\tif (rr->u.TF.flags & TF_CREATE) {\n\t\t\t\tinode->i_ctime.tv_sec =\n\t\t\t\t    iso_date(rr->u.TF.times[cnt++].time,\n\t\t\t\t\t     0);\n\t\t\t\tinode->i_ctime.tv_nsec = 0;\n\t\t\t}\n\t\t\tif (rr->u.TF.flags & TF_MODIFY) {\n\t\t\t\tinode->i_mtime.tv_sec =\n\t\t\t\t    iso_date(rr->u.TF.times[cnt++].time,\n\t\t\t\t\t     0);\n\t\t\t\tinode->i_mtime.tv_nsec = 0;\n\t\t\t}\n\t\t\tif (rr->u.TF.flags & TF_ACCESS) {\n\t\t\t\tinode->i_atime.tv_sec =\n\t\t\t\t    iso_date(rr->u.TF.times[cnt++].time,\n\t\t\t\t\t     0);\n\t\t\t\tinode->i_atime.tv_nsec = 0;\n\t\t\t}\n\t\t\tif (rr->u.TF.flags & TF_ATTRIBUTES) {\n\t\t\t\tinode->i_ctime.tv_sec =\n\t\t\t\t    iso_date(rr->u.TF.times[cnt++].time,\n\t\t\t\t\t     0);\n\t\t\t\tinode->i_ctime.tv_nsec = 0;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SIG('S', 'L'):\n\t\t\t{\n\t\t\t\tint slen;\n\t\t\t\tstruct SL_component *slp;\n\t\t\t\tstruct SL_component *oldslp;\n\t\t\t\tslen = rr->len - 5;\n\t\t\t\tslp = &rr->u.SL.link;\n\t\t\t\tinode->i_size = symlink_len;\n\t\t\t\twhile (slen > 1) {\n\t\t\t\t\trootflag = 0;\n\t\t\t\t\tswitch (slp->flags & ~1) {\n\t\t\t\t\tcase 0:\n\t\t\t\t\t\tinode->i_size +=\n\t\t\t\t\t\t    slp->len;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase 2:\n\t\t\t\t\t\tinode->i_size += 1;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase 4:\n\t\t\t\t\t\tinode->i_size += 2;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase 8:\n\t\t\t\t\t\trootflag = 1;\n\t\t\t\t\t\tinode->i_size += 1;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tprintk(\"Symlink component flag \"\n\t\t\t\t\t\t\t\"not implemented\\n\");\n\t\t\t\t\t}\n\t\t\t\t\tslen -= slp->len + 2;\n\t\t\t\t\toldslp = slp;\n\t\t\t\t\tslp = (struct SL_component *)\n\t\t\t\t\t\t(((char *)slp) + slp->len + 2);\n\n\t\t\t\t\tif (slen < 2) {\n\t\t\t\t\t\tif (((rr->u.SL.\n\t\t\t\t\t\t      flags & 1) != 0)\n\t\t\t\t\t\t    &&\n\t\t\t\t\t\t    ((oldslp->\n\t\t\t\t\t\t      flags & 1) == 0))\n\t\t\t\t\t\t\tinode->i_size +=\n\t\t\t\t\t\t\t    1;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\n\t\t\t\t\t/*\n\t\t\t\t\t * If this component record isn't\n\t\t\t\t\t * continued, then append a '/'.\n\t\t\t\t\t */\n\t\t\t\t\tif (!rootflag\n\t\t\t\t\t    && (oldslp->flags & 1) == 0)\n\t\t\t\t\t\tinode->i_size += 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tsymlink_len = inode->i_size;\n\t\t\tbreak;\n\t\tcase SIG('R', 'E'):\n\t\t\tprintk(KERN_WARNING \"Attempt to read inode for \"\n\t\t\t\t\t\"relocated directory\\n\");\n\t\t\tgoto out;\n\t\tcase SIG('C', 'L'):\n\t\t\tif (flags & RR_RELOC_DE) {\n\t\t\t\tprintk(KERN_ERR\n\t\t\t\t       \"ISOFS: Recursive directory relocation \"\n\t\t\t\t       \"is not supported\\n\");\n\t\t\t\tgoto eio;\n\t\t\t}\n\t\t\treloc_block = isonum_733(rr->u.CL.location);\n\t\t\tif (reloc_block == ISOFS_I(inode)->i_iget5_block &&\n\t\t\t    ISOFS_I(inode)->i_iget5_offset == 0) {\n\t\t\t\tprintk(KERN_ERR\n\t\t\t\t       \"ISOFS: Directory relocation points to \"\n\t\t\t\t       \"itself\\n\");\n\t\t\t\tgoto eio;\n\t\t\t}\n\t\t\tISOFS_I(inode)->i_first_extent = reloc_block;\n\t\t\treloc = isofs_iget_reloc(inode->i_sb, reloc_block, 0);\n\t\t\tif (IS_ERR(reloc)) {\n\t\t\t\tret = PTR_ERR(reloc);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tinode->i_mode = reloc->i_mode;\n\t\t\tset_nlink(inode, reloc->i_nlink);\n\t\t\tinode->i_uid = reloc->i_uid;\n\t\t\tinode->i_gid = reloc->i_gid;\n\t\t\tinode->i_rdev = reloc->i_rdev;\n\t\t\tinode->i_size = reloc->i_size;\n\t\t\tinode->i_blocks = reloc->i_blocks;\n\t\t\tinode->i_atime = reloc->i_atime;\n\t\t\tinode->i_ctime = reloc->i_ctime;\n\t\t\tinode->i_mtime = reloc->i_mtime;\n\t\t\tiput(reloc);\n\t\t\tbreak;\n#ifdef CONFIG_ZISOFS\n\t\tcase SIG('Z', 'F'): {\n\t\t\tint algo;\n\n\t\t\tif (ISOFS_SB(inode->i_sb)->s_nocompress)\n\t\t\t\tbreak;\n\t\t\talgo = isonum_721(rr->u.ZF.algorithm);\n\t\t\tif (algo == SIG('p', 'z')) {\n\t\t\t\tint block_shift =\n\t\t\t\t\tisonum_711(&rr->u.ZF.parms[1]);\n\t\t\t\tif (block_shift > 17) {\n\t\t\t\t\tprintk(KERN_WARNING \"isofs: \"\n\t\t\t\t\t\t\"Can't handle ZF block \"\n\t\t\t\t\t\t\"size of 2^%d\\n\",\n\t\t\t\t\t\tblock_shift);\n\t\t\t\t} else {\n\t\t\t\t\t/*\n\t\t\t\t\t * Note: we don't change\n\t\t\t\t\t * i_blocks here\n\t\t\t\t\t */\n\t\t\t\t\tISOFS_I(inode)->i_file_format =\n\t\t\t\t\t\tisofs_file_compressed;\n\t\t\t\t\t/*\n\t\t\t\t\t * Parameters to compression\n\t\t\t\t\t * algorithm (header size,\n\t\t\t\t\t * block size)\n\t\t\t\t\t */\n\t\t\t\t\tISOFS_I(inode)->i_format_parm[0] =\n\t\t\t\t\t\tisonum_711(&rr->u.ZF.parms[0]);\n\t\t\t\t\tISOFS_I(inode)->i_format_parm[1] =\n\t\t\t\t\t\tisonum_711(&rr->u.ZF.parms[1]);\n\t\t\t\t\tinode->i_size =\n\t\t\t\t\t    isonum_733(rr->u.ZF.\n\t\t\t\t\t\t       real_size);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tprintk(KERN_WARNING\n\t\t\t\t       \"isofs: Unknown ZF compression \"\n\t\t\t\t\t\t\"algorithm: %c%c\\n\",\n\t\t\t\t       rr->u.ZF.algorithm[0],\n\t\t\t\t       rr->u.ZF.algorithm[1]);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n#endif\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tret = rock_continue(&rs);\n\tif (ret == 0)\n\t\tgoto repeat;\n\tif (ret == 1)\n\t\tret = 0;\nout:\n\tkfree(rs.buffer);\n\treturn ret;\neio:\n\tret = -EIO;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -63,6 +63,9 @@\n \t\t\trs.cont_size = isonum_733(rr->u.CE.size);\n \t\t\tbreak;\n \t\tcase SIG('E', 'R'):\n+\t\t\t/* Invalid length of ER tag id? */\n+\t\t\tif (rr->u.ER.len_id + offsetof(struct rock_ridge, u.ER.data) > rr->len)\n+\t\t\t\tgoto out;\n \t\t\tISOFS_SB(inode->i_sb)->s_rock = 1;\n \t\t\tprintk(KERN_DEBUG \"ISO 9660 Extensions: \");\n \t\t\t{",
        "function_modified_lines": {
            "added": [
                "\t\t\t/* Invalid length of ER tag id? */",
                "\t\t\tif (rr->u.ER.len_id + offsetof(struct rock_ridge, u.ER.data) > rr->len)",
                "\t\t\t\tgoto out;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The parse_rock_ridge_inode_internal function in fs/isofs/rock.c in the Linux kernel before 3.18.2 does not validate a length value in the Extensions Reference (ER) System Use Field, which allows local users to obtain sensitive information from kernel memory via a crafted iso9660 image.",
        "id": 684
    },
    {
        "cve_id": "CVE-2015-8844",
        "code_before_change": "static long restore_tm_sigcontexts(struct pt_regs *regs,\n\t\t\t\t   struct sigcontext __user *sc,\n\t\t\t\t   struct sigcontext __user *tm_sc)\n{\n#ifdef CONFIG_ALTIVEC\n\telf_vrreg_t __user *v_regs, *tm_v_regs;\n#endif\n\tunsigned long err = 0;\n\tunsigned long msr;\n#ifdef CONFIG_VSX\n\tint i;\n#endif\n\t/* copy the GPRs */\n\terr |= __copy_from_user(regs->gpr, tm_sc->gp_regs, sizeof(regs->gpr));\n\terr |= __copy_from_user(&current->thread.ckpt_regs, sc->gp_regs,\n\t\t\t\tsizeof(regs->gpr));\n\n\t/*\n\t * TFHAR is restored from the checkpointed 'wound-back' ucontext's NIP.\n\t * TEXASR was set by the signal delivery reclaim, as was TFIAR.\n\t * Users doing anything abhorrent like thread-switching w/ signals for\n\t * TM-Suspended code will have to back TEXASR/TFIAR up themselves.\n\t * For the case of getting a signal and simply returning from it,\n\t * we don't need to re-copy them here.\n\t */\n\terr |= __get_user(regs->nip, &tm_sc->gp_regs[PT_NIP]);\n\terr |= __get_user(current->thread.tm_tfhar, &sc->gp_regs[PT_NIP]);\n\n\t/* get MSR separately, transfer the LE bit if doing signal return */\n\terr |= __get_user(msr, &sc->gp_regs[PT_MSR]);\n\t/* pull in MSR TM from user context */\n\tregs->msr = (regs->msr & ~MSR_TS_MASK) | (msr & MSR_TS_MASK);\n\n\t/* pull in MSR LE from user context */\n\tregs->msr = (regs->msr & ~MSR_LE) | (msr & MSR_LE);\n\n\t/* The following non-GPR non-FPR non-VR state is also checkpointed: */\n\terr |= __get_user(regs->ctr, &tm_sc->gp_regs[PT_CTR]);\n\terr |= __get_user(regs->link, &tm_sc->gp_regs[PT_LNK]);\n\terr |= __get_user(regs->xer, &tm_sc->gp_regs[PT_XER]);\n\terr |= __get_user(regs->ccr, &tm_sc->gp_regs[PT_CCR]);\n\terr |= __get_user(current->thread.ckpt_regs.ctr,\n\t\t\t  &sc->gp_regs[PT_CTR]);\n\terr |= __get_user(current->thread.ckpt_regs.link,\n\t\t\t  &sc->gp_regs[PT_LNK]);\n\terr |= __get_user(current->thread.ckpt_regs.xer,\n\t\t\t  &sc->gp_regs[PT_XER]);\n\terr |= __get_user(current->thread.ckpt_regs.ccr,\n\t\t\t  &sc->gp_regs[PT_CCR]);\n\n\t/* These regs are not checkpointed; they can go in 'regs'. */\n\terr |= __get_user(regs->trap, &sc->gp_regs[PT_TRAP]);\n\terr |= __get_user(regs->dar, &sc->gp_regs[PT_DAR]);\n\terr |= __get_user(regs->dsisr, &sc->gp_regs[PT_DSISR]);\n\terr |= __get_user(regs->result, &sc->gp_regs[PT_RESULT]);\n\n\t/*\n\t * Do this before updating the thread state in\n\t * current->thread.fpr/vr.  That way, if we get preempted\n\t * and another task grabs the FPU/Altivec, it won't be\n\t * tempted to save the current CPU state into the thread_struct\n\t * and corrupt what we are writing there.\n\t */\n\tdiscard_lazy_cpu_state();\n\n\t/*\n\t * Force reload of FP/VEC.\n\t * This has to be done before copying stuff into current->thread.fpr/vr\n\t * for the reasons explained in the previous comment.\n\t */\n\tregs->msr &= ~(MSR_FP | MSR_FE0 | MSR_FE1 | MSR_VEC | MSR_VSX);\n\n#ifdef CONFIG_ALTIVEC\n\terr |= __get_user(v_regs, &sc->v_regs);\n\terr |= __get_user(tm_v_regs, &tm_sc->v_regs);\n\tif (err)\n\t\treturn err;\n\tif (v_regs && !access_ok(VERIFY_READ, v_regs, 34 * sizeof(vector128)))\n\t\treturn -EFAULT;\n\tif (tm_v_regs && !access_ok(VERIFY_READ,\n\t\t\t\t    tm_v_regs, 34 * sizeof(vector128)))\n\t\treturn -EFAULT;\n\t/* Copy 33 vec registers (vr0..31 and vscr) from the stack */\n\tif (v_regs != NULL && tm_v_regs != NULL && (msr & MSR_VEC) != 0) {\n\t\terr |= __copy_from_user(&current->thread.vr_state, v_regs,\n\t\t\t\t\t33 * sizeof(vector128));\n\t\terr |= __copy_from_user(&current->thread.transact_vr, tm_v_regs,\n\t\t\t\t\t33 * sizeof(vector128));\n\t}\n\telse if (current->thread.used_vr) {\n\t\tmemset(&current->thread.vr_state, 0, 33 * sizeof(vector128));\n\t\tmemset(&current->thread.transact_vr, 0, 33 * sizeof(vector128));\n\t}\n\t/* Always get VRSAVE back */\n\tif (v_regs != NULL && tm_v_regs != NULL) {\n\t\terr |= __get_user(current->thread.vrsave,\n\t\t\t\t  (u32 __user *)&v_regs[33]);\n\t\terr |= __get_user(current->thread.transact_vrsave,\n\t\t\t\t  (u32 __user *)&tm_v_regs[33]);\n\t}\n\telse {\n\t\tcurrent->thread.vrsave = 0;\n\t\tcurrent->thread.transact_vrsave = 0;\n\t}\n\tif (cpu_has_feature(CPU_FTR_ALTIVEC))\n\t\tmtspr(SPRN_VRSAVE, current->thread.vrsave);\n#endif /* CONFIG_ALTIVEC */\n\t/* restore floating point */\n\terr |= copy_fpr_from_user(current, &sc->fp_regs);\n\terr |= copy_transact_fpr_from_user(current, &tm_sc->fp_regs);\n#ifdef CONFIG_VSX\n\t/*\n\t * Get additional VSX data. Update v_regs to point after the\n\t * VMX data.  Copy VSX low doubleword from userspace to local\n\t * buffer for formatting, then into the taskstruct.\n\t */\n\tif (v_regs && ((msr & MSR_VSX) != 0)) {\n\t\tv_regs += ELF_NVRREG;\n\t\ttm_v_regs += ELF_NVRREG;\n\t\terr |= copy_vsx_from_user(current, v_regs);\n\t\terr |= copy_transact_vsx_from_user(current, tm_v_regs);\n\t} else {\n\t\tfor (i = 0; i < 32 ; i++) {\n\t\t\tcurrent->thread.fp_state.fpr[i][TS_VSRLOWOFFSET] = 0;\n\t\t\tcurrent->thread.transact_fp.fpr[i][TS_VSRLOWOFFSET] = 0;\n\t\t}\n\t}\n#endif\n\ttm_enable();\n\t/* Make sure the transaction is marked as failed */\n\tcurrent->thread.tm_texasr |= TEXASR_FS;\n\t/* This loads the checkpointed FP/VEC state, if used */\n\ttm_recheckpoint(&current->thread, msr);\n\n\t/* This loads the speculative FP/VEC state, if used */\n\tif (msr & MSR_FP) {\n\t\tdo_load_up_transact_fpu(&current->thread);\n\t\tregs->msr |= (MSR_FP | current->thread.fpexc_mode);\n\t}\n#ifdef CONFIG_ALTIVEC\n\tif (msr & MSR_VEC) {\n\t\tdo_load_up_transact_altivec(&current->thread);\n\t\tregs->msr |= MSR_VEC;\n\t}\n#endif\n\n\treturn err;\n}",
        "code_after_change": "static long restore_tm_sigcontexts(struct pt_regs *regs,\n\t\t\t\t   struct sigcontext __user *sc,\n\t\t\t\t   struct sigcontext __user *tm_sc)\n{\n#ifdef CONFIG_ALTIVEC\n\telf_vrreg_t __user *v_regs, *tm_v_regs;\n#endif\n\tunsigned long err = 0;\n\tunsigned long msr;\n#ifdef CONFIG_VSX\n\tint i;\n#endif\n\t/* copy the GPRs */\n\terr |= __copy_from_user(regs->gpr, tm_sc->gp_regs, sizeof(regs->gpr));\n\terr |= __copy_from_user(&current->thread.ckpt_regs, sc->gp_regs,\n\t\t\t\tsizeof(regs->gpr));\n\n\t/*\n\t * TFHAR is restored from the checkpointed 'wound-back' ucontext's NIP.\n\t * TEXASR was set by the signal delivery reclaim, as was TFIAR.\n\t * Users doing anything abhorrent like thread-switching w/ signals for\n\t * TM-Suspended code will have to back TEXASR/TFIAR up themselves.\n\t * For the case of getting a signal and simply returning from it,\n\t * we don't need to re-copy them here.\n\t */\n\terr |= __get_user(regs->nip, &tm_sc->gp_regs[PT_NIP]);\n\terr |= __get_user(current->thread.tm_tfhar, &sc->gp_regs[PT_NIP]);\n\n\t/* get MSR separately, transfer the LE bit if doing signal return */\n\terr |= __get_user(msr, &sc->gp_regs[PT_MSR]);\n\t/* Don't allow reserved mode. */\n\tif (MSR_TM_RESV(msr))\n\t\treturn -EINVAL;\n\n\t/* pull in MSR TM from user context */\n\tregs->msr = (regs->msr & ~MSR_TS_MASK) | (msr & MSR_TS_MASK);\n\n\t/* pull in MSR LE from user context */\n\tregs->msr = (regs->msr & ~MSR_LE) | (msr & MSR_LE);\n\n\t/* The following non-GPR non-FPR non-VR state is also checkpointed: */\n\terr |= __get_user(regs->ctr, &tm_sc->gp_regs[PT_CTR]);\n\terr |= __get_user(regs->link, &tm_sc->gp_regs[PT_LNK]);\n\terr |= __get_user(regs->xer, &tm_sc->gp_regs[PT_XER]);\n\terr |= __get_user(regs->ccr, &tm_sc->gp_regs[PT_CCR]);\n\terr |= __get_user(current->thread.ckpt_regs.ctr,\n\t\t\t  &sc->gp_regs[PT_CTR]);\n\terr |= __get_user(current->thread.ckpt_regs.link,\n\t\t\t  &sc->gp_regs[PT_LNK]);\n\terr |= __get_user(current->thread.ckpt_regs.xer,\n\t\t\t  &sc->gp_regs[PT_XER]);\n\terr |= __get_user(current->thread.ckpt_regs.ccr,\n\t\t\t  &sc->gp_regs[PT_CCR]);\n\n\t/* These regs are not checkpointed; they can go in 'regs'. */\n\terr |= __get_user(regs->trap, &sc->gp_regs[PT_TRAP]);\n\terr |= __get_user(regs->dar, &sc->gp_regs[PT_DAR]);\n\terr |= __get_user(regs->dsisr, &sc->gp_regs[PT_DSISR]);\n\terr |= __get_user(regs->result, &sc->gp_regs[PT_RESULT]);\n\n\t/*\n\t * Do this before updating the thread state in\n\t * current->thread.fpr/vr.  That way, if we get preempted\n\t * and another task grabs the FPU/Altivec, it won't be\n\t * tempted to save the current CPU state into the thread_struct\n\t * and corrupt what we are writing there.\n\t */\n\tdiscard_lazy_cpu_state();\n\n\t/*\n\t * Force reload of FP/VEC.\n\t * This has to be done before copying stuff into current->thread.fpr/vr\n\t * for the reasons explained in the previous comment.\n\t */\n\tregs->msr &= ~(MSR_FP | MSR_FE0 | MSR_FE1 | MSR_VEC | MSR_VSX);\n\n#ifdef CONFIG_ALTIVEC\n\terr |= __get_user(v_regs, &sc->v_regs);\n\terr |= __get_user(tm_v_regs, &tm_sc->v_regs);\n\tif (err)\n\t\treturn err;\n\tif (v_regs && !access_ok(VERIFY_READ, v_regs, 34 * sizeof(vector128)))\n\t\treturn -EFAULT;\n\tif (tm_v_regs && !access_ok(VERIFY_READ,\n\t\t\t\t    tm_v_regs, 34 * sizeof(vector128)))\n\t\treturn -EFAULT;\n\t/* Copy 33 vec registers (vr0..31 and vscr) from the stack */\n\tif (v_regs != NULL && tm_v_regs != NULL && (msr & MSR_VEC) != 0) {\n\t\terr |= __copy_from_user(&current->thread.vr_state, v_regs,\n\t\t\t\t\t33 * sizeof(vector128));\n\t\terr |= __copy_from_user(&current->thread.transact_vr, tm_v_regs,\n\t\t\t\t\t33 * sizeof(vector128));\n\t}\n\telse if (current->thread.used_vr) {\n\t\tmemset(&current->thread.vr_state, 0, 33 * sizeof(vector128));\n\t\tmemset(&current->thread.transact_vr, 0, 33 * sizeof(vector128));\n\t}\n\t/* Always get VRSAVE back */\n\tif (v_regs != NULL && tm_v_regs != NULL) {\n\t\terr |= __get_user(current->thread.vrsave,\n\t\t\t\t  (u32 __user *)&v_regs[33]);\n\t\terr |= __get_user(current->thread.transact_vrsave,\n\t\t\t\t  (u32 __user *)&tm_v_regs[33]);\n\t}\n\telse {\n\t\tcurrent->thread.vrsave = 0;\n\t\tcurrent->thread.transact_vrsave = 0;\n\t}\n\tif (cpu_has_feature(CPU_FTR_ALTIVEC))\n\t\tmtspr(SPRN_VRSAVE, current->thread.vrsave);\n#endif /* CONFIG_ALTIVEC */\n\t/* restore floating point */\n\terr |= copy_fpr_from_user(current, &sc->fp_regs);\n\terr |= copy_transact_fpr_from_user(current, &tm_sc->fp_regs);\n#ifdef CONFIG_VSX\n\t/*\n\t * Get additional VSX data. Update v_regs to point after the\n\t * VMX data.  Copy VSX low doubleword from userspace to local\n\t * buffer for formatting, then into the taskstruct.\n\t */\n\tif (v_regs && ((msr & MSR_VSX) != 0)) {\n\t\tv_regs += ELF_NVRREG;\n\t\ttm_v_regs += ELF_NVRREG;\n\t\terr |= copy_vsx_from_user(current, v_regs);\n\t\terr |= copy_transact_vsx_from_user(current, tm_v_regs);\n\t} else {\n\t\tfor (i = 0; i < 32 ; i++) {\n\t\t\tcurrent->thread.fp_state.fpr[i][TS_VSRLOWOFFSET] = 0;\n\t\t\tcurrent->thread.transact_fp.fpr[i][TS_VSRLOWOFFSET] = 0;\n\t\t}\n\t}\n#endif\n\ttm_enable();\n\t/* Make sure the transaction is marked as failed */\n\tcurrent->thread.tm_texasr |= TEXASR_FS;\n\t/* This loads the checkpointed FP/VEC state, if used */\n\ttm_recheckpoint(&current->thread, msr);\n\n\t/* This loads the speculative FP/VEC state, if used */\n\tif (msr & MSR_FP) {\n\t\tdo_load_up_transact_fpu(&current->thread);\n\t\tregs->msr |= (MSR_FP | current->thread.fpexc_mode);\n\t}\n#ifdef CONFIG_ALTIVEC\n\tif (msr & MSR_VEC) {\n\t\tdo_load_up_transact_altivec(&current->thread);\n\t\tregs->msr |= MSR_VEC;\n\t}\n#endif\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,6 +28,10 @@\n \n \t/* get MSR separately, transfer the LE bit if doing signal return */\n \terr |= __get_user(msr, &sc->gp_regs[PT_MSR]);\n+\t/* Don't allow reserved mode. */\n+\tif (MSR_TM_RESV(msr))\n+\t\treturn -EINVAL;\n+\n \t/* pull in MSR TM from user context */\n \tregs->msr = (regs->msr & ~MSR_TS_MASK) | (msr & MSR_TS_MASK);\n ",
        "function_modified_lines": {
            "added": [
                "\t/* Don't allow reserved mode. */",
                "\tif (MSR_TM_RESV(msr))",
                "\t\treturn -EINVAL;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The signal implementation in the Linux kernel before 4.3.5 on powerpc platforms does not check for an MSR with both the S and T bits set, which allows local users to cause a denial of service (TM Bad Thing exception and panic) via a crafted application.",
        "id": 862
    },
    {
        "cve_id": "CVE-2018-1000026",
        "code_before_change": "static netdev_features_t bnx2x_features_check(struct sk_buff *skb,\n\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t      netdev_features_t features)\n{\n\tfeatures = vlan_features_check(skb, features);\n\treturn vxlan_features_check(skb, features);\n}",
        "code_after_change": "static netdev_features_t bnx2x_features_check(struct sk_buff *skb,\n\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t      netdev_features_t features)\n{\n\t/*\n\t * A skb with gso_size + header length > 9700 will cause a\n\t * firmware panic. Drop GSO support.\n\t *\n\t * Eventually the upper layer should not pass these packets down.\n\t *\n\t * For speed, if the gso_size is <= 9000, assume there will\n\t * not be 700 bytes of headers and pass it through. Only do a\n\t * full (slow) validation if the gso_size is > 9000.\n\t *\n\t * (Due to the way SKB_BY_FRAGS works this will also do a full\n\t * validation in that case.)\n\t */\n\tif (unlikely(skb_is_gso(skb) &&\n\t\t     (skb_shinfo(skb)->gso_size > 9000) &&\n\t\t     !skb_gso_validate_mac_len(skb, 9700)))\n\t\tfeatures &= ~NETIF_F_GSO_MASK;\n\n\tfeatures = vlan_features_check(skb, features);\n\treturn vxlan_features_check(skb, features);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,24 @@\n \t\t\t\t\t      struct net_device *dev,\n \t\t\t\t\t      netdev_features_t features)\n {\n+\t/*\n+\t * A skb with gso_size + header length > 9700 will cause a\n+\t * firmware panic. Drop GSO support.\n+\t *\n+\t * Eventually the upper layer should not pass these packets down.\n+\t *\n+\t * For speed, if the gso_size is <= 9000, assume there will\n+\t * not be 700 bytes of headers and pass it through. Only do a\n+\t * full (slow) validation if the gso_size is > 9000.\n+\t *\n+\t * (Due to the way SKB_BY_FRAGS works this will also do a full\n+\t * validation in that case.)\n+\t */\n+\tif (unlikely(skb_is_gso(skb) &&\n+\t\t     (skb_shinfo(skb)->gso_size > 9000) &&\n+\t\t     !skb_gso_validate_mac_len(skb, 9700)))\n+\t\tfeatures &= ~NETIF_F_GSO_MASK;\n+\n \tfeatures = vlan_features_check(skb, features);\n \treturn vxlan_features_check(skb, features);\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * A skb with gso_size + header length > 9700 will cause a",
                "\t * firmware panic. Drop GSO support.",
                "\t *",
                "\t * Eventually the upper layer should not pass these packets down.",
                "\t *",
                "\t * For speed, if the gso_size is <= 9000, assume there will",
                "\t * not be 700 bytes of headers and pass it through. Only do a",
                "\t * full (slow) validation if the gso_size is > 9000.",
                "\t *",
                "\t * (Due to the way SKB_BY_FRAGS works this will also do a full",
                "\t * validation in that case.)",
                "\t */",
                "\tif (unlikely(skb_is_gso(skb) &&",
                "\t\t     (skb_shinfo(skb)->gso_size > 9000) &&",
                "\t\t     !skb_gso_validate_mac_len(skb, 9700)))",
                "\t\tfeatures &= ~NETIF_F_GSO_MASK;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Linux Linux kernel version at least v4.8 onwards, probably well before contains a Insufficient input validation vulnerability in bnx2x network card driver that can result in DoS: Network card firmware assertion takes card off-line. This attack appear to be exploitable via An attacker on a must pass a very large, specially crafted packet to the bnx2x card. This can be done from an untrusted guest VM..",
        "id": 1575
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int recv_msg(struct kiocb *iocb, struct socket *sock,\n\t\t    struct msghdr *m, size_t buf_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tipc_port *tport = tipc_sk_port(sk);\n\tstruct sk_buff *buf;\n\tstruct tipc_msg *msg;\n\tlong timeout;\n\tunsigned int sz;\n\tu32 err;\n\tint res;\n\n\t/* Catch invalid receive requests */\n\tif (unlikely(!buf_len))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (unlikely(sock->state == SS_UNCONNECTED)) {\n\t\tres = -ENOTCONN;\n\t\tgoto exit;\n\t}\n\n\t/* will be updated in set_orig_addr() if needed */\n\tm->msg_namelen = 0;\n\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\nrestart:\n\n\t/* Look for a message in receive queue; wait if necessary */\n\twhile (skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tif (sock->state == SS_DISCONNECTING) {\n\t\t\tres = -ENOTCONN;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (timeout <= 0L) {\n\t\t\tres = timeout ? timeout : -EWOULDBLOCK;\n\t\t\tgoto exit;\n\t\t}\n\t\trelease_sock(sk);\n\t\ttimeout = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t\t\t   tipc_rx_ready(sock),\n\t\t\t\t\t\t\t   timeout);\n\t\tlock_sock(sk);\n\t}\n\n\t/* Look at first message in receive queue */\n\tbuf = skb_peek(&sk->sk_receive_queue);\n\tmsg = buf_msg(buf);\n\tsz = msg_data_sz(msg);\n\terr = msg_errcode(msg);\n\n\t/* Discard an empty non-errored message & try again */\n\tif ((!sz) && (!err)) {\n\t\tadvance_rx_queue(sk);\n\t\tgoto restart;\n\t}\n\n\t/* Capture sender's address (optional) */\n\tset_orig_addr(m, msg);\n\n\t/* Capture ancillary data (optional) */\n\tres = anc_data_recv(m, msg, tport);\n\tif (res)\n\t\tgoto exit;\n\n\t/* Capture message data (if valid) & compute return value (always) */\n\tif (!err) {\n\t\tif (unlikely(buf_len < sz)) {\n\t\t\tsz = buf_len;\n\t\t\tm->msg_flags |= MSG_TRUNC;\n\t\t}\n\t\tres = skb_copy_datagram_iovec(buf, msg_hdr_sz(msg),\n\t\t\t\t\t      m->msg_iov, sz);\n\t\tif (res)\n\t\t\tgoto exit;\n\t\tres = sz;\n\t} else {\n\t\tif ((sock->state == SS_READY) ||\n\t\t    ((err == TIPC_CONN_SHUTDOWN) || m->msg_control))\n\t\t\tres = 0;\n\t\telse\n\t\t\tres = -ECONNRESET;\n\t}\n\n\t/* Consume received message (optional) */\n\tif (likely(!(flags & MSG_PEEK))) {\n\t\tif ((sock->state != SS_READY) &&\n\t\t    (++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))\n\t\t\ttipc_acknowledge(tport->ref, tport->conn_unacked);\n\t\tadvance_rx_queue(sk);\n\t}\nexit:\n\trelease_sock(sk);\n\treturn res;\n}",
        "code_after_change": "static int recv_msg(struct kiocb *iocb, struct socket *sock,\n\t\t    struct msghdr *m, size_t buf_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tipc_port *tport = tipc_sk_port(sk);\n\tstruct sk_buff *buf;\n\tstruct tipc_msg *msg;\n\tlong timeout;\n\tunsigned int sz;\n\tu32 err;\n\tint res;\n\n\t/* Catch invalid receive requests */\n\tif (unlikely(!buf_len))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (unlikely(sock->state == SS_UNCONNECTED)) {\n\t\tres = -ENOTCONN;\n\t\tgoto exit;\n\t}\n\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\nrestart:\n\n\t/* Look for a message in receive queue; wait if necessary */\n\twhile (skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tif (sock->state == SS_DISCONNECTING) {\n\t\t\tres = -ENOTCONN;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (timeout <= 0L) {\n\t\t\tres = timeout ? timeout : -EWOULDBLOCK;\n\t\t\tgoto exit;\n\t\t}\n\t\trelease_sock(sk);\n\t\ttimeout = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t\t\t   tipc_rx_ready(sock),\n\t\t\t\t\t\t\t   timeout);\n\t\tlock_sock(sk);\n\t}\n\n\t/* Look at first message in receive queue */\n\tbuf = skb_peek(&sk->sk_receive_queue);\n\tmsg = buf_msg(buf);\n\tsz = msg_data_sz(msg);\n\terr = msg_errcode(msg);\n\n\t/* Discard an empty non-errored message & try again */\n\tif ((!sz) && (!err)) {\n\t\tadvance_rx_queue(sk);\n\t\tgoto restart;\n\t}\n\n\t/* Capture sender's address (optional) */\n\tset_orig_addr(m, msg);\n\n\t/* Capture ancillary data (optional) */\n\tres = anc_data_recv(m, msg, tport);\n\tif (res)\n\t\tgoto exit;\n\n\t/* Capture message data (if valid) & compute return value (always) */\n\tif (!err) {\n\t\tif (unlikely(buf_len < sz)) {\n\t\t\tsz = buf_len;\n\t\t\tm->msg_flags |= MSG_TRUNC;\n\t\t}\n\t\tres = skb_copy_datagram_iovec(buf, msg_hdr_sz(msg),\n\t\t\t\t\t      m->msg_iov, sz);\n\t\tif (res)\n\t\t\tgoto exit;\n\t\tres = sz;\n\t} else {\n\t\tif ((sock->state == SS_READY) ||\n\t\t    ((err == TIPC_CONN_SHUTDOWN) || m->msg_control))\n\t\t\tres = 0;\n\t\telse\n\t\t\tres = -ECONNRESET;\n\t}\n\n\t/* Consume received message (optional) */\n\tif (likely(!(flags & MSG_PEEK))) {\n\t\tif ((sock->state != SS_READY) &&\n\t\t    (++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))\n\t\t\ttipc_acknowledge(tport->ref, tport->conn_unacked);\n\t\tadvance_rx_queue(sk);\n\t}\nexit:\n\trelease_sock(sk);\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,9 +20,6 @@\n \t\tres = -ENOTCONN;\n \t\tgoto exit;\n \t}\n-\n-\t/* will be updated in set_orig_addr() if needed */\n-\tm->msg_namelen = 0;\n \n \ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n restart:",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\t/* will be updated in set_orig_addr() if needed */",
                "\tm->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 400
    },
    {
        "cve_id": "CVE-2013-1819",
        "code_before_change": "xfs_buf_t *\n_xfs_buf_find(\n\tstruct xfs_buftarg\t*btp,\n\tstruct xfs_buf_map\t*map,\n\tint\t\t\tnmaps,\n\txfs_buf_flags_t\t\tflags,\n\txfs_buf_t\t\t*new_bp)\n{\n\tsize_t\t\t\tnumbytes;\n\tstruct xfs_perag\t*pag;\n\tstruct rb_node\t\t**rbp;\n\tstruct rb_node\t\t*parent;\n\txfs_buf_t\t\t*bp;\n\txfs_daddr_t\t\tblkno = map[0].bm_bn;\n\tint\t\t\tnumblks = 0;\n\tint\t\t\ti;\n\n\tfor (i = 0; i < nmaps; i++)\n\t\tnumblks += map[i].bm_len;\n\tnumbytes = BBTOB(numblks);\n\n\t/* Check for IOs smaller than the sector size / not sector aligned */\n\tASSERT(!(numbytes < (1 << btp->bt_sshift)));\n\tASSERT(!(BBTOB(blkno) & (xfs_off_t)btp->bt_smask));\n\n\t/* get tree root */\n\tpag = xfs_perag_get(btp->bt_mount,\n\t\t\t\txfs_daddr_to_agno(btp->bt_mount, blkno));\n\n\t/* walk tree */\n\tspin_lock(&pag->pag_buf_lock);\n\trbp = &pag->pag_buf_tree.rb_node;\n\tparent = NULL;\n\tbp = NULL;\n\twhile (*rbp) {\n\t\tparent = *rbp;\n\t\tbp = rb_entry(parent, struct xfs_buf, b_rbnode);\n\n\t\tif (blkno < bp->b_bn)\n\t\t\trbp = &(*rbp)->rb_left;\n\t\telse if (blkno > bp->b_bn)\n\t\t\trbp = &(*rbp)->rb_right;\n\t\telse {\n\t\t\t/*\n\t\t\t * found a block number match. If the range doesn't\n\t\t\t * match, the only way this is allowed is if the buffer\n\t\t\t * in the cache is stale and the transaction that made\n\t\t\t * it stale has not yet committed. i.e. we are\n\t\t\t * reallocating a busy extent. Skip this buffer and\n\t\t\t * continue searching to the right for an exact match.\n\t\t\t */\n\t\t\tif (bp->b_length != numblks) {\n\t\t\t\tASSERT(bp->b_flags & XBF_STALE);\n\t\t\t\trbp = &(*rbp)->rb_right;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tatomic_inc(&bp->b_hold);\n\t\t\tgoto found;\n\t\t}\n\t}\n\n\t/* No match found */\n\tif (new_bp) {\n\t\trb_link_node(&new_bp->b_rbnode, parent, rbp);\n\t\trb_insert_color(&new_bp->b_rbnode, &pag->pag_buf_tree);\n\t\t/* the buffer keeps the perag reference until it is freed */\n\t\tnew_bp->b_pag = pag;\n\t\tspin_unlock(&pag->pag_buf_lock);\n\t} else {\n\t\tXFS_STATS_INC(xb_miss_locked);\n\t\tspin_unlock(&pag->pag_buf_lock);\n\t\txfs_perag_put(pag);\n\t}\n\treturn new_bp;\n\nfound:\n\tspin_unlock(&pag->pag_buf_lock);\n\txfs_perag_put(pag);\n\n\tif (!xfs_buf_trylock(bp)) {\n\t\tif (flags & XBF_TRYLOCK) {\n\t\t\txfs_buf_rele(bp);\n\t\t\tXFS_STATS_INC(xb_busy_locked);\n\t\t\treturn NULL;\n\t\t}\n\t\txfs_buf_lock(bp);\n\t\tXFS_STATS_INC(xb_get_locked_waited);\n\t}\n\n\t/*\n\t * if the buffer is stale, clear all the external state associated with\n\t * it. We need to keep flags such as how we allocated the buffer memory\n\t * intact here.\n\t */\n\tif (bp->b_flags & XBF_STALE) {\n\t\tASSERT((bp->b_flags & _XBF_DELWRI_Q) == 0);\n\t\tASSERT(bp->b_iodone == NULL);\n\t\tbp->b_flags &= _XBF_KMEM | _XBF_PAGES;\n\t\tbp->b_ops = NULL;\n\t}\n\n\ttrace_xfs_buf_find(bp, flags, _RET_IP_);\n\tXFS_STATS_INC(xb_get_locked);\n\treturn bp;\n}",
        "code_after_change": "xfs_buf_t *\n_xfs_buf_find(\n\tstruct xfs_buftarg\t*btp,\n\tstruct xfs_buf_map\t*map,\n\tint\t\t\tnmaps,\n\txfs_buf_flags_t\t\tflags,\n\txfs_buf_t\t\t*new_bp)\n{\n\tsize_t\t\t\tnumbytes;\n\tstruct xfs_perag\t*pag;\n\tstruct rb_node\t\t**rbp;\n\tstruct rb_node\t\t*parent;\n\txfs_buf_t\t\t*bp;\n\txfs_daddr_t\t\tblkno = map[0].bm_bn;\n\txfs_daddr_t\t\teofs;\n\tint\t\t\tnumblks = 0;\n\tint\t\t\ti;\n\n\tfor (i = 0; i < nmaps; i++)\n\t\tnumblks += map[i].bm_len;\n\tnumbytes = BBTOB(numblks);\n\n\t/* Check for IOs smaller than the sector size / not sector aligned */\n\tASSERT(!(numbytes < (1 << btp->bt_sshift)));\n\tASSERT(!(BBTOB(blkno) & (xfs_off_t)btp->bt_smask));\n\n\t/*\n\t * Corrupted block numbers can get through to here, unfortunately, so we\n\t * have to check that the buffer falls within the filesystem bounds.\n\t */\n\teofs = XFS_FSB_TO_BB(btp->bt_mount, btp->bt_mount->m_sb.sb_dblocks);\n\tif (blkno >= eofs) {\n\t\t/*\n\t\t * XXX (dgc): we should really be returning EFSCORRUPTED here,\n\t\t * but none of the higher level infrastructure supports\n\t\t * returning a specific error on buffer lookup failures.\n\t\t */\n\t\txfs_alert(btp->bt_mount,\n\t\t\t  \"%s: Block out of range: block 0x%llx, EOFS 0x%llx \",\n\t\t\t  __func__, blkno, eofs);\n\t\treturn NULL;\n\t}\n\n\t/* get tree root */\n\tpag = xfs_perag_get(btp->bt_mount,\n\t\t\t\txfs_daddr_to_agno(btp->bt_mount, blkno));\n\n\t/* walk tree */\n\tspin_lock(&pag->pag_buf_lock);\n\trbp = &pag->pag_buf_tree.rb_node;\n\tparent = NULL;\n\tbp = NULL;\n\twhile (*rbp) {\n\t\tparent = *rbp;\n\t\tbp = rb_entry(parent, struct xfs_buf, b_rbnode);\n\n\t\tif (blkno < bp->b_bn)\n\t\t\trbp = &(*rbp)->rb_left;\n\t\telse if (blkno > bp->b_bn)\n\t\t\trbp = &(*rbp)->rb_right;\n\t\telse {\n\t\t\t/*\n\t\t\t * found a block number match. If the range doesn't\n\t\t\t * match, the only way this is allowed is if the buffer\n\t\t\t * in the cache is stale and the transaction that made\n\t\t\t * it stale has not yet committed. i.e. we are\n\t\t\t * reallocating a busy extent. Skip this buffer and\n\t\t\t * continue searching to the right for an exact match.\n\t\t\t */\n\t\t\tif (bp->b_length != numblks) {\n\t\t\t\tASSERT(bp->b_flags & XBF_STALE);\n\t\t\t\trbp = &(*rbp)->rb_right;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tatomic_inc(&bp->b_hold);\n\t\t\tgoto found;\n\t\t}\n\t}\n\n\t/* No match found */\n\tif (new_bp) {\n\t\trb_link_node(&new_bp->b_rbnode, parent, rbp);\n\t\trb_insert_color(&new_bp->b_rbnode, &pag->pag_buf_tree);\n\t\t/* the buffer keeps the perag reference until it is freed */\n\t\tnew_bp->b_pag = pag;\n\t\tspin_unlock(&pag->pag_buf_lock);\n\t} else {\n\t\tXFS_STATS_INC(xb_miss_locked);\n\t\tspin_unlock(&pag->pag_buf_lock);\n\t\txfs_perag_put(pag);\n\t}\n\treturn new_bp;\n\nfound:\n\tspin_unlock(&pag->pag_buf_lock);\n\txfs_perag_put(pag);\n\n\tif (!xfs_buf_trylock(bp)) {\n\t\tif (flags & XBF_TRYLOCK) {\n\t\t\txfs_buf_rele(bp);\n\t\t\tXFS_STATS_INC(xb_busy_locked);\n\t\t\treturn NULL;\n\t\t}\n\t\txfs_buf_lock(bp);\n\t\tXFS_STATS_INC(xb_get_locked_waited);\n\t}\n\n\t/*\n\t * if the buffer is stale, clear all the external state associated with\n\t * it. We need to keep flags such as how we allocated the buffer memory\n\t * intact here.\n\t */\n\tif (bp->b_flags & XBF_STALE) {\n\t\tASSERT((bp->b_flags & _XBF_DELWRI_Q) == 0);\n\t\tASSERT(bp->b_iodone == NULL);\n\t\tbp->b_flags &= _XBF_KMEM | _XBF_PAGES;\n\t\tbp->b_ops = NULL;\n\t}\n\n\ttrace_xfs_buf_find(bp, flags, _RET_IP_);\n\tXFS_STATS_INC(xb_get_locked);\n\treturn bp;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,7 @@\n \tstruct rb_node\t\t*parent;\n \txfs_buf_t\t\t*bp;\n \txfs_daddr_t\t\tblkno = map[0].bm_bn;\n+\txfs_daddr_t\t\teofs;\n \tint\t\t\tnumblks = 0;\n \tint\t\t\ti;\n \n@@ -22,6 +23,23 @@\n \t/* Check for IOs smaller than the sector size / not sector aligned */\n \tASSERT(!(numbytes < (1 << btp->bt_sshift)));\n \tASSERT(!(BBTOB(blkno) & (xfs_off_t)btp->bt_smask));\n+\n+\t/*\n+\t * Corrupted block numbers can get through to here, unfortunately, so we\n+\t * have to check that the buffer falls within the filesystem bounds.\n+\t */\n+\teofs = XFS_FSB_TO_BB(btp->bt_mount, btp->bt_mount->m_sb.sb_dblocks);\n+\tif (blkno >= eofs) {\n+\t\t/*\n+\t\t * XXX (dgc): we should really be returning EFSCORRUPTED here,\n+\t\t * but none of the higher level infrastructure supports\n+\t\t * returning a specific error on buffer lookup failures.\n+\t\t */\n+\t\txfs_alert(btp->bt_mount,\n+\t\t\t  \"%s: Block out of range: block 0x%llx, EOFS 0x%llx \",\n+\t\t\t  __func__, blkno, eofs);\n+\t\treturn NULL;\n+\t}\n \n \t/* get tree root */\n \tpag = xfs_perag_get(btp->bt_mount,",
        "function_modified_lines": {
            "added": [
                "\txfs_daddr_t\t\teofs;",
                "",
                "\t/*",
                "\t * Corrupted block numbers can get through to here, unfortunately, so we",
                "\t * have to check that the buffer falls within the filesystem bounds.",
                "\t */",
                "\teofs = XFS_FSB_TO_BB(btp->bt_mount, btp->bt_mount->m_sb.sb_dblocks);",
                "\tif (blkno >= eofs) {",
                "\t\t/*",
                "\t\t * XXX (dgc): we should really be returning EFSCORRUPTED here,",
                "\t\t * but none of the higher level infrastructure supports",
                "\t\t * returning a specific error on buffer lookup failures.",
                "\t\t */",
                "\t\txfs_alert(btp->bt_mount,",
                "\t\t\t  \"%s: Block out of range: block 0x%llx, EOFS 0x%llx \",",
                "\t\t\t  __func__, blkno, eofs);",
                "\t\treturn NULL;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The _xfs_buf_find function in fs/xfs/xfs_buf.c in the Linux kernel before 3.7.6 does not validate block numbers, which allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by leveraging the ability to mount an XFS filesystem containing a metadata inode with an invalid extent map.",
        "id": 191
    },
    {
        "cve_id": "CVE-2020-12363",
        "code_before_change": "static int intel_engine_setup(struct intel_gt *gt, enum intel_engine_id id)\n{\n\tconst struct engine_info *info = &intel_engines[id];\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct intel_engine_cs *engine;\n\n\tBUILD_BUG_ON(MAX_ENGINE_CLASS >= BIT(GEN11_ENGINE_CLASS_WIDTH));\n\tBUILD_BUG_ON(MAX_ENGINE_INSTANCE >= BIT(GEN11_ENGINE_INSTANCE_WIDTH));\n\n\tif (GEM_DEBUG_WARN_ON(id >= ARRAY_SIZE(gt->engine)))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->class > MAX_ENGINE_CLASS))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->instance > MAX_ENGINE_INSTANCE))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(gt->engine_class[info->class][info->instance]))\n\t\treturn -EINVAL;\n\n\tengine = kzalloc(sizeof(*engine), GFP_KERNEL);\n\tif (!engine)\n\t\treturn -ENOMEM;\n\n\tBUILD_BUG_ON(BITS_PER_TYPE(engine->mask) < I915_NUM_ENGINES);\n\n\tengine->id = id;\n\tengine->legacy_idx = INVALID_ENGINE;\n\tengine->mask = BIT(id);\n\tengine->i915 = i915;\n\tengine->gt = gt;\n\tengine->uncore = gt->uncore;\n\tengine->hw_id = engine->guc_id = info->hw_id;\n\tengine->mmio_base = __engine_mmio_base(i915, info->mmio_bases);\n\n\tengine->class = info->class;\n\tengine->instance = info->instance;\n\t__sprint_engine_name(engine);\n\n\tengine->props.heartbeat_interval_ms =\n\t\tCONFIG_DRM_I915_HEARTBEAT_INTERVAL;\n\tengine->props.max_busywait_duration_ns =\n\t\tCONFIG_DRM_I915_MAX_REQUEST_BUSYWAIT;\n\tengine->props.preempt_timeout_ms =\n\t\tCONFIG_DRM_I915_PREEMPT_TIMEOUT;\n\tengine->props.stop_timeout_ms =\n\t\tCONFIG_DRM_I915_STOP_TIMEOUT;\n\tengine->props.timeslice_duration_ms =\n\t\tCONFIG_DRM_I915_TIMESLICE_DURATION;\n\n\t/* Override to uninterruptible for OpenCL workloads. */\n\tif (INTEL_GEN(i915) == 12 && engine->class == RENDER_CLASS)\n\t\tengine->props.preempt_timeout_ms = 0;\n\n\tengine->defaults = engine->props; /* never to change again */\n\n\tengine->context_size = intel_engine_context_size(gt, engine->class);\n\tif (WARN_ON(engine->context_size > BIT(20)))\n\t\tengine->context_size = 0;\n\tif (engine->context_size)\n\t\tDRIVER_CAPS(i915)->has_logical_contexts = true;\n\n\t/* Nothing to do here, execute in order of dependencies */\n\tengine->schedule = NULL;\n\n\tewma__engine_latency_init(&engine->latency);\n\tseqlock_init(&engine->stats.lock);\n\n\tATOMIC_INIT_NOTIFIER_HEAD(&engine->context_status_notifier);\n\n\t/* Scrub mmio state on takeover */\n\tintel_engine_sanitize_mmio(engine);\n\n\tgt->engine_class[info->class][info->instance] = engine;\n\tgt->engine[id] = engine;\n\n\treturn 0;\n}",
        "code_after_change": "static int intel_engine_setup(struct intel_gt *gt, enum intel_engine_id id)\n{\n\tconst struct engine_info *info = &intel_engines[id];\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct intel_engine_cs *engine;\n\n\tBUILD_BUG_ON(MAX_ENGINE_CLASS >= BIT(GEN11_ENGINE_CLASS_WIDTH));\n\tBUILD_BUG_ON(MAX_ENGINE_INSTANCE >= BIT(GEN11_ENGINE_INSTANCE_WIDTH));\n\n\tif (GEM_DEBUG_WARN_ON(id >= ARRAY_SIZE(gt->engine)))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->class > MAX_ENGINE_CLASS))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->instance > MAX_ENGINE_INSTANCE))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(gt->engine_class[info->class][info->instance]))\n\t\treturn -EINVAL;\n\n\tengine = kzalloc(sizeof(*engine), GFP_KERNEL);\n\tif (!engine)\n\t\treturn -ENOMEM;\n\n\tBUILD_BUG_ON(BITS_PER_TYPE(engine->mask) < I915_NUM_ENGINES);\n\n\tengine->id = id;\n\tengine->legacy_idx = INVALID_ENGINE;\n\tengine->mask = BIT(id);\n\tengine->i915 = i915;\n\tengine->gt = gt;\n\tengine->uncore = gt->uncore;\n\tengine->mmio_base = __engine_mmio_base(i915, info->mmio_bases);\n\tengine->hw_id = info->hw_id;\n\tengine->guc_id = MAKE_GUC_ID(info->class, info->instance);\n\n\tengine->class = info->class;\n\tengine->instance = info->instance;\n\t__sprint_engine_name(engine);\n\n\tengine->props.heartbeat_interval_ms =\n\t\tCONFIG_DRM_I915_HEARTBEAT_INTERVAL;\n\tengine->props.max_busywait_duration_ns =\n\t\tCONFIG_DRM_I915_MAX_REQUEST_BUSYWAIT;\n\tengine->props.preempt_timeout_ms =\n\t\tCONFIG_DRM_I915_PREEMPT_TIMEOUT;\n\tengine->props.stop_timeout_ms =\n\t\tCONFIG_DRM_I915_STOP_TIMEOUT;\n\tengine->props.timeslice_duration_ms =\n\t\tCONFIG_DRM_I915_TIMESLICE_DURATION;\n\n\t/* Override to uninterruptible for OpenCL workloads. */\n\tif (INTEL_GEN(i915) == 12 && engine->class == RENDER_CLASS)\n\t\tengine->props.preempt_timeout_ms = 0;\n\n\tengine->defaults = engine->props; /* never to change again */\n\n\tengine->context_size = intel_engine_context_size(gt, engine->class);\n\tif (WARN_ON(engine->context_size > BIT(20)))\n\t\tengine->context_size = 0;\n\tif (engine->context_size)\n\t\tDRIVER_CAPS(i915)->has_logical_contexts = true;\n\n\t/* Nothing to do here, execute in order of dependencies */\n\tengine->schedule = NULL;\n\n\tewma__engine_latency_init(&engine->latency);\n\tseqlock_init(&engine->stats.lock);\n\n\tATOMIC_INIT_NOTIFIER_HEAD(&engine->context_status_notifier);\n\n\t/* Scrub mmio state on takeover */\n\tintel_engine_sanitize_mmio(engine);\n\n\tgt->engine_class[info->class][info->instance] = engine;\n\tgt->engine[id] = engine;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,8 +31,9 @@\n \tengine->i915 = i915;\n \tengine->gt = gt;\n \tengine->uncore = gt->uncore;\n-\tengine->hw_id = engine->guc_id = info->hw_id;\n \tengine->mmio_base = __engine_mmio_base(i915, info->mmio_bases);\n+\tengine->hw_id = info->hw_id;\n+\tengine->guc_id = MAKE_GUC_ID(info->class, info->instance);\n \n \tengine->class = info->class;\n \tengine->instance = info->instance;",
        "function_modified_lines": {
            "added": [
                "\tengine->hw_id = info->hw_id;",
                "\tengine->guc_id = MAKE_GUC_ID(info->class, info->instance);"
            ],
            "deleted": [
                "\tengine->hw_id = engine->guc_id = info->hw_id;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Improper input validation in some Intel(R) Graphics Drivers for Windows* before version 26.20.100.7212 and before Linux kernel version 5.5 may allow a privileged user to potentially enable a denial of service via local access.",
        "id": 2458
    },
    {
        "cve_id": "CVE-2013-6368",
        "code_before_change": "void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data;\n\tvoid *vapic;\n\n\tif (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))\n\t\tapic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);\n\tdata = *(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr));\n\tkunmap_atomic(vapic);\n\n\tapic_set_tpr(vcpu->arch.apic, data & 0xff);\n}",
        "code_after_change": "void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data;\n\n\tif (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))\n\t\tapic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\tkvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n\t\t\t\tsizeof(u32));\n\n\tapic_set_tpr(vcpu->arch.apic, data & 0xff);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,6 @@\n void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)\n {\n \tu32 data;\n-\tvoid *vapic;\n \n \tif (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))\n \t\tapic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);\n@@ -9,9 +8,8 @@\n \tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n \t\treturn;\n \n-\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);\n-\tdata = *(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr));\n-\tkunmap_atomic(vapic);\n+\tkvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n+\t\t\t\tsizeof(u32));\n \n \tapic_set_tpr(vcpu->arch.apic, data & 0xff);\n }",
        "function_modified_lines": {
            "added": [
                "\tkvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,",
                "\t\t\t\tsizeof(u32));"
            ],
            "deleted": [
                "\tvoid *vapic;",
                "\tvapic = kmap_atomic(vcpu->arch.apic->vapic_page);",
                "\tdata = *(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr));",
                "\tkunmap_atomic(vapic);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The KVM subsystem in the Linux kernel through 3.12.5 allows local users to gain privileges or cause a denial of service (system crash) via a VAPIC synchronization operation involving a page-end address.",
        "id": 340
    },
    {
        "cve_id": "CVE-2012-4398",
        "code_before_change": "void tomoyo_load_policy(const char *filename)\n{\n\tstatic bool done;\n\tchar *argv[2];\n\tchar *envp[3];\n\n\tif (tomoyo_policy_loaded || done)\n\t\treturn;\n\tif (!tomoyo_trigger)\n\t\ttomoyo_trigger = CONFIG_SECURITY_TOMOYO_ACTIVATION_TRIGGER;\n\tif (strcmp(filename, tomoyo_trigger))\n\t\treturn;\n\tif (!tomoyo_policy_loader_exists())\n\t\treturn;\n\tdone = true;\n\tprintk(KERN_INFO \"Calling %s to load policy. Please wait.\\n\",\n\t       tomoyo_loader);\n\targv[0] = (char *) tomoyo_loader;\n\targv[1] = NULL;\n\tenvp[0] = \"HOME=/\";\n\tenvp[1] = \"PATH=/sbin:/bin:/usr/sbin:/usr/bin\";\n\tenvp[2] = NULL;\n\tcall_usermodehelper(argv[0], argv, envp, 1);\n\ttomoyo_check_profile();\n}",
        "code_after_change": "void tomoyo_load_policy(const char *filename)\n{\n\tstatic bool done;\n\tchar *argv[2];\n\tchar *envp[3];\n\n\tif (tomoyo_policy_loaded || done)\n\t\treturn;\n\tif (!tomoyo_trigger)\n\t\ttomoyo_trigger = CONFIG_SECURITY_TOMOYO_ACTIVATION_TRIGGER;\n\tif (strcmp(filename, tomoyo_trigger))\n\t\treturn;\n\tif (!tomoyo_policy_loader_exists())\n\t\treturn;\n\tdone = true;\n\tprintk(KERN_INFO \"Calling %s to load policy. Please wait.\\n\",\n\t       tomoyo_loader);\n\targv[0] = (char *) tomoyo_loader;\n\targv[1] = NULL;\n\tenvp[0] = \"HOME=/\";\n\tenvp[1] = \"PATH=/sbin:/bin:/usr/sbin:/usr/bin\";\n\tenvp[2] = NULL;\n\tcall_usermodehelper(argv[0], argv, envp, UMH_WAIT_PROC);\n\ttomoyo_check_profile();\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,6 +20,6 @@\n \tenvp[0] = \"HOME=/\";\n \tenvp[1] = \"PATH=/sbin:/bin:/usr/sbin:/usr/bin\";\n \tenvp[2] = NULL;\n-\tcall_usermodehelper(argv[0], argv, envp, 1);\n+\tcall_usermodehelper(argv[0], argv, envp, UMH_WAIT_PROC);\n \ttomoyo_check_profile();\n }",
        "function_modified_lines": {
            "added": [
                "\tcall_usermodehelper(argv[0], argv, envp, UMH_WAIT_PROC);"
            ],
            "deleted": [
                "\tcall_usermodehelper(argv[0], argv, envp, 1);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The __request_module function in kernel/kmod.c in the Linux kernel before 3.4 does not set a certain killable attribute, which allows local users to cause a denial of service (memory consumption) via a crafted application.",
        "id": 100
    },
    {
        "cve_id": "CVE-2012-4398",
        "code_before_change": "void dm_CheckRfCtrlGPIO(void *data)\n{\n\tstruct r8192_priv *priv = container_of_dwork_rsl(data,\n\t\t\t\t  struct r8192_priv, gpio_change_rf_wq);\n\tstruct net_device *dev = priv->rtllib->dev;\n\tu8 tmp1byte;\n\tenum rt_rf_power_state eRfPowerStateToSet;\n\tbool bActuallySet = false;\n\tchar *argv[3];\n\tstatic char *RadioPowerPath = \"/etc/acpi/events/RadioPower.sh\";\n\tstatic char *envp[] = {\"HOME=/\", \"TERM=linux\", \"PATH=/usr/bin:/bin\", NULL};\n\n\tbActuallySet = false;\n\n\tif ((priv->up_first_time == 1) || (priv->being_init_adapter))\n\t\treturn;\n\n\tif (priv->bfirst_after_down) {\n\t\tpriv->bfirst_after_down = 1;\n\t\treturn;\n\t}\n\n\ttmp1byte = read_nic_byte(dev, GPI);\n\n\teRfPowerStateToSet = (tmp1byte&BIT1) ?  eRfOn : eRfOff;\n\n\tif ((priv->bHwRadioOff == true) && (eRfPowerStateToSet == eRfOn)) {\n\t\tRT_TRACE(COMP_RF, \"gpiochangeRF  - HW Radio ON\\n\");\n\t\tprintk(KERN_INFO \"gpiochangeRF  - HW Radio ON\\n\");\n\t\tpriv->bHwRadioOff = false;\n\t\tbActuallySet = true;\n\t} else if ((priv->bHwRadioOff == false) && (eRfPowerStateToSet == eRfOff)) {\n\t\tRT_TRACE(COMP_RF, \"gpiochangeRF  - HW Radio OFF\\n\");\n\t\tprintk(KERN_INFO \"gpiochangeRF  - HW Radio OFF\\n\");\n\t\tpriv->bHwRadioOff = true;\n\t\tbActuallySet = true;\n\t}\n\n\tif (bActuallySet) {\n\t\tmdelay(1000);\n\t\tpriv->bHwRfOffAction = 1;\n\t\tMgntActSet_RF_State(dev, eRfPowerStateToSet, RF_CHANGE_BY_HW, true);\n\t\tif (priv->bHwRadioOff == true)\n\t\t\targv[1] = \"RFOFF\";\n\t\telse\n\t\t\targv[1] = \"RFON\";\n\n\t\targv[0] = RadioPowerPath;\n\t\targv[2] = NULL;\n\t\tcall_usermodehelper(RadioPowerPath, argv, envp, 1);\n\t}\n}",
        "code_after_change": "void dm_CheckRfCtrlGPIO(void *data)\n{\n\tstruct r8192_priv *priv = container_of_dwork_rsl(data,\n\t\t\t\t  struct r8192_priv, gpio_change_rf_wq);\n\tstruct net_device *dev = priv->rtllib->dev;\n\tu8 tmp1byte;\n\tenum rt_rf_power_state eRfPowerStateToSet;\n\tbool bActuallySet = false;\n\tchar *argv[3];\n\tstatic char *RadioPowerPath = \"/etc/acpi/events/RadioPower.sh\";\n\tstatic char *envp[] = {\"HOME=/\", \"TERM=linux\", \"PATH=/usr/bin:/bin\", NULL};\n\n\tbActuallySet = false;\n\n\tif ((priv->up_first_time == 1) || (priv->being_init_adapter))\n\t\treturn;\n\n\tif (priv->bfirst_after_down) {\n\t\tpriv->bfirst_after_down = 1;\n\t\treturn;\n\t}\n\n\ttmp1byte = read_nic_byte(dev, GPI);\n\n\teRfPowerStateToSet = (tmp1byte&BIT1) ?  eRfOn : eRfOff;\n\n\tif ((priv->bHwRadioOff == true) && (eRfPowerStateToSet == eRfOn)) {\n\t\tRT_TRACE(COMP_RF, \"gpiochangeRF  - HW Radio ON\\n\");\n\t\tprintk(KERN_INFO \"gpiochangeRF  - HW Radio ON\\n\");\n\t\tpriv->bHwRadioOff = false;\n\t\tbActuallySet = true;\n\t} else if ((priv->bHwRadioOff == false) && (eRfPowerStateToSet == eRfOff)) {\n\t\tRT_TRACE(COMP_RF, \"gpiochangeRF  - HW Radio OFF\\n\");\n\t\tprintk(KERN_INFO \"gpiochangeRF  - HW Radio OFF\\n\");\n\t\tpriv->bHwRadioOff = true;\n\t\tbActuallySet = true;\n\t}\n\n\tif (bActuallySet) {\n\t\tmdelay(1000);\n\t\tpriv->bHwRfOffAction = 1;\n\t\tMgntActSet_RF_State(dev, eRfPowerStateToSet, RF_CHANGE_BY_HW, true);\n\t\tif (priv->bHwRadioOff == true)\n\t\t\targv[1] = \"RFOFF\";\n\t\telse\n\t\t\targv[1] = \"RFON\";\n\n\t\targv[0] = RadioPowerPath;\n\t\targv[2] = NULL;\n\t\tcall_usermodehelper(RadioPowerPath, argv, envp, UMH_WAIT_PROC);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,6 +47,6 @@\n \n \t\targv[0] = RadioPowerPath;\n \t\targv[2] = NULL;\n-\t\tcall_usermodehelper(RadioPowerPath, argv, envp, 1);\n+\t\tcall_usermodehelper(RadioPowerPath, argv, envp, UMH_WAIT_PROC);\n \t}\n }",
        "function_modified_lines": {
            "added": [
                "\t\tcall_usermodehelper(RadioPowerPath, argv, envp, UMH_WAIT_PROC);"
            ],
            "deleted": [
                "\t\tcall_usermodehelper(RadioPowerPath, argv, envp, 1);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The __request_module function in kernel/kmod.c in the Linux kernel before 3.4 does not set a certain killable attribute, which allows local users to cause a denial of service (memory consumption) via a crafted application.",
        "id": 97
    },
    {
        "cve_id": "CVE-2021-3655",
        "code_before_change": "int sctp_raw_to_bind_addrs(struct sctp_bind_addr *bp, __u8 *raw_addr_list,\n\t\t\t   int addrs_len, __u16 port, gfp_t gfp)\n{\n\tunion sctp_addr_param *rawaddr;\n\tstruct sctp_paramhdr *param;\n\tunion sctp_addr addr;\n\tint retval = 0;\n\tint len;\n\tstruct sctp_af *af;\n\n\t/* Convert the raw address to standard address format */\n\twhile (addrs_len) {\n\t\tparam = (struct sctp_paramhdr *)raw_addr_list;\n\t\trawaddr = (union sctp_addr_param *)raw_addr_list;\n\n\t\taf = sctp_get_af_specific(param_type2af(param->type));\n\t\tif (unlikely(!af)) {\n\t\t\tretval = -EINVAL;\n\t\t\tsctp_bind_addr_clean(bp);\n\t\t\tbreak;\n\t\t}\n\n\t\taf->from_addr_param(&addr, rawaddr, htons(port), 0);\n\t\tif (sctp_bind_addr_state(bp, &addr) != -1)\n\t\t\tgoto next;\n\t\tretval = sctp_add_bind_addr(bp, &addr, sizeof(addr),\n\t\t\t\t\t    SCTP_ADDR_SRC, gfp);\n\t\tif (retval) {\n\t\t\t/* Can't finish building the list, clean up. */\n\t\t\tsctp_bind_addr_clean(bp);\n\t\t\tbreak;\n\t\t}\n\nnext:\n\t\tlen = ntohs(param->length);\n\t\taddrs_len -= len;\n\t\traw_addr_list += len;\n\t}\n\n\treturn retval;\n}",
        "code_after_change": "int sctp_raw_to_bind_addrs(struct sctp_bind_addr *bp, __u8 *raw_addr_list,\n\t\t\t   int addrs_len, __u16 port, gfp_t gfp)\n{\n\tunion sctp_addr_param *rawaddr;\n\tstruct sctp_paramhdr *param;\n\tunion sctp_addr addr;\n\tint retval = 0;\n\tint len;\n\tstruct sctp_af *af;\n\n\t/* Convert the raw address to standard address format */\n\twhile (addrs_len) {\n\t\tparam = (struct sctp_paramhdr *)raw_addr_list;\n\t\trawaddr = (union sctp_addr_param *)raw_addr_list;\n\n\t\taf = sctp_get_af_specific(param_type2af(param->type));\n\t\tif (unlikely(!af) ||\n\t\t    !af->from_addr_param(&addr, rawaddr, htons(port), 0)) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tif (sctp_bind_addr_state(bp, &addr) != -1)\n\t\t\tgoto next;\n\t\tretval = sctp_add_bind_addr(bp, &addr, sizeof(addr),\n\t\t\t\t\t    SCTP_ADDR_SRC, gfp);\n\t\tif (retval)\n\t\t\t/* Can't finish building the list, clean up. */\n\t\t\tgoto out_err;\n\nnext:\n\t\tlen = ntohs(param->length);\n\t\taddrs_len -= len;\n\t\traw_addr_list += len;\n\t}\n\n\treturn retval;\n\nout_err:\n\tif (retval)\n\t\tsctp_bind_addr_clean(bp);\n\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,22 +14,19 @@\n \t\trawaddr = (union sctp_addr_param *)raw_addr_list;\n \n \t\taf = sctp_get_af_specific(param_type2af(param->type));\n-\t\tif (unlikely(!af)) {\n+\t\tif (unlikely(!af) ||\n+\t\t    !af->from_addr_param(&addr, rawaddr, htons(port), 0)) {\n \t\t\tretval = -EINVAL;\n-\t\t\tsctp_bind_addr_clean(bp);\n-\t\t\tbreak;\n+\t\t\tgoto out_err;\n \t\t}\n \n-\t\taf->from_addr_param(&addr, rawaddr, htons(port), 0);\n \t\tif (sctp_bind_addr_state(bp, &addr) != -1)\n \t\t\tgoto next;\n \t\tretval = sctp_add_bind_addr(bp, &addr, sizeof(addr),\n \t\t\t\t\t    SCTP_ADDR_SRC, gfp);\n-\t\tif (retval) {\n+\t\tif (retval)\n \t\t\t/* Can't finish building the list, clean up. */\n-\t\t\tsctp_bind_addr_clean(bp);\n-\t\t\tbreak;\n-\t\t}\n+\t\t\tgoto out_err;\n \n next:\n \t\tlen = ntohs(param->length);\n@@ -38,4 +35,10 @@\n \t}\n \n \treturn retval;\n+\n+out_err:\n+\tif (retval)\n+\t\tsctp_bind_addr_clean(bp);\n+\n+\treturn retval;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tif (unlikely(!af) ||",
                "\t\t    !af->from_addr_param(&addr, rawaddr, htons(port), 0)) {",
                "\t\t\tgoto out_err;",
                "\t\tif (retval)",
                "\t\t\tgoto out_err;",
                "",
                "out_err:",
                "\tif (retval)",
                "\t\tsctp_bind_addr_clean(bp);",
                "",
                "\treturn retval;"
            ],
            "deleted": [
                "\t\tif (unlikely(!af)) {",
                "\t\t\tsctp_bind_addr_clean(bp);",
                "\t\t\tbreak;",
                "\t\taf->from_addr_param(&addr, rawaddr, htons(port), 0);",
                "\t\tif (retval) {",
                "\t\t\tsctp_bind_addr_clean(bp);",
                "\t\t\tbreak;",
                "\t\t}"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel in versions prior to v5.14-rc1. Missing size validations on inbound SCTP packets may allow the kernel to read uninitialized memory.",
        "id": 3029
    },
    {
        "cve_id": "CVE-2013-7263",
        "code_before_change": "static int pn_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t\tstruct msghdr *msg, size_t len, int noblock,\n\t\t\tint flags, int *addr_len)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct sockaddr_pn sa;\n\tint rval = -EOPNOTSUPP;\n\tint copylen;\n\n\tif (flags & ~(MSG_PEEK|MSG_TRUNC|MSG_DONTWAIT|MSG_NOSIGNAL|\n\t\t\tMSG_CMSG_COMPAT))\n\t\tgoto out_nofree;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(sa);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &rval);\n\tif (skb == NULL)\n\t\tgoto out_nofree;\n\n\tpn_skb_get_src_sockaddr(skb, &sa);\n\n\tcopylen = skb->len;\n\tif (len < copylen) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopylen = len;\n\t}\n\n\trval = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copylen);\n\tif (rval) {\n\t\trval = -EFAULT;\n\t\tgoto out;\n\t}\n\n\trval = (flags & MSG_TRUNC) ? skb->len : copylen;\n\n\tif (msg->msg_name != NULL)\n\t\tmemcpy(msg->msg_name, &sa, sizeof(struct sockaddr_pn));\n\nout:\n\tskb_free_datagram(sk, skb);\n\nout_nofree:\n\treturn rval;\n}",
        "code_after_change": "static int pn_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t\tstruct msghdr *msg, size_t len, int noblock,\n\t\t\tint flags, int *addr_len)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct sockaddr_pn sa;\n\tint rval = -EOPNOTSUPP;\n\tint copylen;\n\n\tif (flags & ~(MSG_PEEK|MSG_TRUNC|MSG_DONTWAIT|MSG_NOSIGNAL|\n\t\t\tMSG_CMSG_COMPAT))\n\t\tgoto out_nofree;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &rval);\n\tif (skb == NULL)\n\t\tgoto out_nofree;\n\n\tpn_skb_get_src_sockaddr(skb, &sa);\n\n\tcopylen = skb->len;\n\tif (len < copylen) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopylen = len;\n\t}\n\n\trval = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copylen);\n\tif (rval) {\n\t\trval = -EFAULT;\n\t\tgoto out;\n\t}\n\n\trval = (flags & MSG_TRUNC) ? skb->len : copylen;\n\n\tif (msg->msg_name != NULL) {\n\t\tmemcpy(msg->msg_name, &sa, sizeof(sa));\n\t\t*addr_len = sizeof(sa);\n\t}\n\nout:\n\tskb_free_datagram(sk, skb);\n\nout_nofree:\n\treturn rval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,9 +10,6 @@\n \tif (flags & ~(MSG_PEEK|MSG_TRUNC|MSG_DONTWAIT|MSG_NOSIGNAL|\n \t\t\tMSG_CMSG_COMPAT))\n \t\tgoto out_nofree;\n-\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(sa);\n \n \tskb = skb_recv_datagram(sk, flags, noblock, &rval);\n \tif (skb == NULL)\n@@ -34,8 +31,10 @@\n \n \trval = (flags & MSG_TRUNC) ? skb->len : copylen;\n \n-\tif (msg->msg_name != NULL)\n-\t\tmemcpy(msg->msg_name, &sa, sizeof(struct sockaddr_pn));\n+\tif (msg->msg_name != NULL) {\n+\t\tmemcpy(msg->msg_name, &sa, sizeof(sa));\n+\t\t*addr_len = sizeof(sa);\n+\t}\n \n out:\n \tskb_free_datagram(sk, skb);",
        "function_modified_lines": {
            "added": [
                "\tif (msg->msg_name != NULL) {",
                "\t\tmemcpy(msg->msg_name, &sa, sizeof(sa));",
                "\t\t*addr_len = sizeof(sa);",
                "\t}"
            ],
            "deleted": [
                "",
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(sa);",
                "\tif (msg->msg_name != NULL)",
                "\t\tmemcpy(msg->msg_name, &sa, sizeof(struct sockaddr_pn));"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Linux kernel before 3.12.4 updates certain length values before ensuring that associated data structures have been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call, related to net/ipv4/ping.c, net/ipv4/raw.c, net/ipv4/udp.c, net/ipv6/raw.c, and net/ipv6/udp.c.",
        "id": 365
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static void unix_copy_addr(struct msghdr *msg, struct sock *sk)\n{\n\tstruct unix_sock *u = unix_sk(sk);\n\n\tmsg->msg_namelen = 0;\n\tif (u->addr) {\n\t\tmsg->msg_namelen = u->addr->len;\n\t\tmemcpy(msg->msg_name, u->addr->name, u->addr->len);\n\t}\n}",
        "code_after_change": "static void unix_copy_addr(struct msghdr *msg, struct sock *sk)\n{\n\tstruct unix_sock *u = unix_sk(sk);\n\n\tif (u->addr) {\n\t\tmsg->msg_namelen = u->addr->len;\n\t\tmemcpy(msg->msg_name, u->addr->name, u->addr->len);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,6 @@\n {\n \tstruct unix_sock *u = unix_sk(sk);\n \n-\tmsg->msg_namelen = 0;\n \tif (u->addr) {\n \t\tmsg->msg_namelen = u->addr->len;\n \t\tmemcpy(msg->msg_name, u->addr->name, u->addr->len);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 402
    },
    {
        "cve_id": "CVE-2021-3655",
        "code_before_change": "static struct sctp_association *__sctp_rcv_init_lookup(struct net *net,\n\tstruct sk_buff *skb,\n\tconst union sctp_addr *laddr, struct sctp_transport **transportp)\n{\n\tstruct sctp_association *asoc;\n\tunion sctp_addr addr;\n\tunion sctp_addr *paddr = &addr;\n\tstruct sctphdr *sh = sctp_hdr(skb);\n\tunion sctp_params params;\n\tstruct sctp_init_chunk *init;\n\tstruct sctp_af *af;\n\n\t/*\n\t * This code will NOT touch anything inside the chunk--it is\n\t * strictly READ-ONLY.\n\t *\n\t * RFC 2960 3  SCTP packet Format\n\t *\n\t * Multiple chunks can be bundled into one SCTP packet up to\n\t * the MTU size, except for the INIT, INIT ACK, and SHUTDOWN\n\t * COMPLETE chunks.  These chunks MUST NOT be bundled with any\n\t * other chunk in a packet.  See Section 6.10 for more details\n\t * on chunk bundling.\n\t */\n\n\t/* Find the start of the TLVs and the end of the chunk.  This is\n\t * the region we search for address parameters.\n\t */\n\tinit = (struct sctp_init_chunk *)skb->data;\n\n\t/* Walk the parameters looking for embedded addresses. */\n\tsctp_walk_params(params, init, init_hdr.params) {\n\n\t\t/* Note: Ignoring hostname addresses. */\n\t\taf = sctp_get_af_specific(param_type2af(params.p->type));\n\t\tif (!af)\n\t\t\tcontinue;\n\n\t\taf->from_addr_param(paddr, params.addr, sh->source, 0);\n\n\t\tasoc = __sctp_lookup_association(net, laddr, paddr, transportp);\n\t\tif (asoc)\n\t\t\treturn asoc;\n\t}\n\n\treturn NULL;\n}",
        "code_after_change": "static struct sctp_association *__sctp_rcv_init_lookup(struct net *net,\n\tstruct sk_buff *skb,\n\tconst union sctp_addr *laddr, struct sctp_transport **transportp)\n{\n\tstruct sctp_association *asoc;\n\tunion sctp_addr addr;\n\tunion sctp_addr *paddr = &addr;\n\tstruct sctphdr *sh = sctp_hdr(skb);\n\tunion sctp_params params;\n\tstruct sctp_init_chunk *init;\n\tstruct sctp_af *af;\n\n\t/*\n\t * This code will NOT touch anything inside the chunk--it is\n\t * strictly READ-ONLY.\n\t *\n\t * RFC 2960 3  SCTP packet Format\n\t *\n\t * Multiple chunks can be bundled into one SCTP packet up to\n\t * the MTU size, except for the INIT, INIT ACK, and SHUTDOWN\n\t * COMPLETE chunks.  These chunks MUST NOT be bundled with any\n\t * other chunk in a packet.  See Section 6.10 for more details\n\t * on chunk bundling.\n\t */\n\n\t/* Find the start of the TLVs and the end of the chunk.  This is\n\t * the region we search for address parameters.\n\t */\n\tinit = (struct sctp_init_chunk *)skb->data;\n\n\t/* Walk the parameters looking for embedded addresses. */\n\tsctp_walk_params(params, init, init_hdr.params) {\n\n\t\t/* Note: Ignoring hostname addresses. */\n\t\taf = sctp_get_af_specific(param_type2af(params.p->type));\n\t\tif (!af)\n\t\t\tcontinue;\n\n\t\tif (!af->from_addr_param(paddr, params.addr, sh->source, 0))\n\t\t\tcontinue;\n\n\t\tasoc = __sctp_lookup_association(net, laddr, paddr, transportp);\n\t\tif (asoc)\n\t\t\treturn asoc;\n\t}\n\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -36,7 +36,8 @@\n \t\tif (!af)\n \t\t\tcontinue;\n \n-\t\taf->from_addr_param(paddr, params.addr, sh->source, 0);\n+\t\tif (!af->from_addr_param(paddr, params.addr, sh->source, 0))\n+\t\t\tcontinue;\n \n \t\tasoc = __sctp_lookup_association(net, laddr, paddr, transportp);\n \t\tif (asoc)",
        "function_modified_lines": {
            "added": [
                "\t\tif (!af->from_addr_param(paddr, params.addr, sh->source, 0))",
                "\t\t\tcontinue;"
            ],
            "deleted": [
                "\t\taf->from_addr_param(paddr, params.addr, sh->source, 0);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel in versions prior to v5.14-rc1. Missing size validations on inbound SCTP packets may allow the kernel to read uninitialized memory.",
        "id": 3031
    },
    {
        "cve_id": "CVE-2018-14656",
        "code_before_change": "void show_ip(struct pt_regs *regs, const char *loglvl)\n{\n#ifdef CONFIG_X86_32\n\tprintk(\"%sEIP: %pS\\n\", loglvl, (void *)regs->ip);\n#else\n\tprintk(\"%sRIP: %04x:%pS\\n\", loglvl, (int)regs->cs, (void *)regs->ip);\n#endif\n\tshow_opcodes((u8 *)regs->ip, loglvl);\n}",
        "code_after_change": "void show_ip(struct pt_regs *regs, const char *loglvl)\n{\n#ifdef CONFIG_X86_32\n\tprintk(\"%sEIP: %pS\\n\", loglvl, (void *)regs->ip);\n#else\n\tprintk(\"%sRIP: %04x:%pS\\n\", loglvl, (int)regs->cs, (void *)regs->ip);\n#endif\n\tshow_opcodes(regs, loglvl);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,5 +5,5 @@\n #else\n \tprintk(\"%sRIP: %04x:%pS\\n\", loglvl, (int)regs->cs, (void *)regs->ip);\n #endif\n-\tshow_opcodes((u8 *)regs->ip, loglvl);\n+\tshow_opcodes(regs, loglvl);\n }",
        "function_modified_lines": {
            "added": [
                "\tshow_opcodes(regs, loglvl);"
            ],
            "deleted": [
                "\tshow_opcodes((u8 *)regs->ip, loglvl);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A missing address check in the callers of the show_opcodes() in the Linux kernel allows an attacker to dump the kernel memory at an arbitrary kernel address into the dmesg log.",
        "id": 1703
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int rose_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\tstruct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rose_sock *rose = rose_sk(sk);\n\tstruct sockaddr_rose *srose = (struct sockaddr_rose *)msg->msg_name;\n\tsize_t copied;\n\tunsigned char *asmptr;\n\tstruct sk_buff *skb;\n\tint n, er, qbit;\n\n\t/*\n\t * This works for seqpacket too. The receiver has ordered the queue for\n\t * us! We do one quick check first though\n\t */\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\treturn -ENOTCONN;\n\n\t/* Now we can treat all alike */\n\tif ((skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT, flags & MSG_DONTWAIT, &er)) == NULL)\n\t\treturn er;\n\n\tqbit = (skb->data[0] & ROSE_Q_BIT) == ROSE_Q_BIT;\n\n\tskb_pull(skb, ROSE_MIN_LEN);\n\n\tif (rose->qbitincl) {\n\t\tasmptr  = skb_push(skb, 1);\n\t\t*asmptr = qbit;\n\t}\n\n\tskb_reset_transport_header(skb);\n\tcopied     = skb->len;\n\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\tskb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tif (srose != NULL) {\n\t\tmemset(srose, 0, msg->msg_namelen);\n\t\tsrose->srose_family = AF_ROSE;\n\t\tsrose->srose_addr   = rose->dest_addr;\n\t\tsrose->srose_call   = rose->dest_call;\n\t\tsrose->srose_ndigis = rose->dest_ndigis;\n\t\tif (msg->msg_namelen >= sizeof(struct full_sockaddr_rose)) {\n\t\t\tstruct full_sockaddr_rose *full_srose = (struct full_sockaddr_rose *)msg->msg_name;\n\t\t\tfor (n = 0 ; n < rose->dest_ndigis ; n++)\n\t\t\t\tfull_srose->srose_digis[n] = rose->dest_digis[n];\n\t\t\tmsg->msg_namelen = sizeof(struct full_sockaddr_rose);\n\t\t} else {\n\t\t\tif (rose->dest_ndigis >= 1) {\n\t\t\t\tsrose->srose_ndigis = 1;\n\t\t\t\tsrose->srose_digi = rose->dest_digis[0];\n\t\t\t}\n\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_rose);\n\t\t}\n\t}\n\n\tskb_free_datagram(sk, skb);\n\n\treturn copied;\n}",
        "code_after_change": "static int rose_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\tstruct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rose_sock *rose = rose_sk(sk);\n\tsize_t copied;\n\tunsigned char *asmptr;\n\tstruct sk_buff *skb;\n\tint n, er, qbit;\n\n\t/*\n\t * This works for seqpacket too. The receiver has ordered the queue for\n\t * us! We do one quick check first though\n\t */\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\treturn -ENOTCONN;\n\n\t/* Now we can treat all alike */\n\tif ((skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT, flags & MSG_DONTWAIT, &er)) == NULL)\n\t\treturn er;\n\n\tqbit = (skb->data[0] & ROSE_Q_BIT) == ROSE_Q_BIT;\n\n\tskb_pull(skb, ROSE_MIN_LEN);\n\n\tif (rose->qbitincl) {\n\t\tasmptr  = skb_push(skb, 1);\n\t\t*asmptr = qbit;\n\t}\n\n\tskb_reset_transport_header(skb);\n\tcopied     = skb->len;\n\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\tskb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_rose *srose;\n\n\t\tmemset(msg->msg_name, 0, sizeof(struct full_sockaddr_rose));\n\t\tsrose = msg->msg_name;\n\t\tsrose->srose_family = AF_ROSE;\n\t\tsrose->srose_addr   = rose->dest_addr;\n\t\tsrose->srose_call   = rose->dest_call;\n\t\tsrose->srose_ndigis = rose->dest_ndigis;\n\t\tif (msg->msg_namelen >= sizeof(struct full_sockaddr_rose)) {\n\t\t\tstruct full_sockaddr_rose *full_srose = (struct full_sockaddr_rose *)msg->msg_name;\n\t\t\tfor (n = 0 ; n < rose->dest_ndigis ; n++)\n\t\t\t\tfull_srose->srose_digis[n] = rose->dest_digis[n];\n\t\t\tmsg->msg_namelen = sizeof(struct full_sockaddr_rose);\n\t\t} else {\n\t\t\tif (rose->dest_ndigis >= 1) {\n\t\t\t\tsrose->srose_ndigis = 1;\n\t\t\t\tsrose->srose_digi = rose->dest_digis[0];\n\t\t\t}\n\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_rose);\n\t\t}\n\t}\n\n\tskb_free_datagram(sk, skb);\n\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,6 @@\n {\n \tstruct sock *sk = sock->sk;\n \tstruct rose_sock *rose = rose_sk(sk);\n-\tstruct sockaddr_rose *srose = (struct sockaddr_rose *)msg->msg_name;\n \tsize_t copied;\n \tunsigned char *asmptr;\n \tstruct sk_buff *skb;\n@@ -39,8 +38,11 @@\n \n \tskb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n \n-\tif (srose != NULL) {\n-\t\tmemset(srose, 0, msg->msg_namelen);\n+\tif (msg->msg_name) {\n+\t\tstruct sockaddr_rose *srose;\n+\n+\t\tmemset(msg->msg_name, 0, sizeof(struct full_sockaddr_rose));\n+\t\tsrose = msg->msg_name;\n \t\tsrose->srose_family = AF_ROSE;\n \t\tsrose->srose_addr   = rose->dest_addr;\n \t\tsrose->srose_call   = rose->dest_call;",
        "function_modified_lines": {
            "added": [
                "\tif (msg->msg_name) {",
                "\t\tstruct sockaddr_rose *srose;",
                "",
                "\t\tmemset(msg->msg_name, 0, sizeof(struct full_sockaddr_rose));",
                "\t\tsrose = msg->msg_name;"
            ],
            "deleted": [
                "\tstruct sockaddr_rose *srose = (struct sockaddr_rose *)msg->msg_name;",
                "\tif (srose != NULL) {",
                "\t\tmemset(srose, 0, msg->msg_namelen);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 395
    },
    {
        "cve_id": "CVE-2013-7263",
        "code_before_change": "int udpv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint is_udp4;\n\tbool slow;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(struct sockaddr_in6);\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, sizeof(struct udphdr), msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udpv6_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tif (is_udp4)\n\t\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t   UDP_MIB_INERRORS,\n\t\t\t\t\t\t   is_udplite);\n\t\t\telse\n\t\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t    UDP_MIB_INERRORS,\n\t\t\t\t\t\t    is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\tif (!peeked) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in6 *sin6;\n\n\t\tsin6 = (struct sockaddr_in6 *) msg->msg_name;\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\n\t\tif (is_udp4) {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\t\tsin6->sin6_scope_id = 0;\n\t\t} else {\n\t\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tsin6->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t    IP6CB(skb)->iif);\n\t\t}\n\n\t}\n\tif (is_udp4) {\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\t}\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tif (is_udp4) {\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t} else {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t}\n\tunlock_sock_fast(sk, slow);\n\n\tif (noblock)\n\t\treturn -EAGAIN;\n\n\t/* starting over for a new packet */\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}",
        "code_after_change": "int udpv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint is_udp4;\n\tbool slow;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, sizeof(struct udphdr), msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udpv6_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tif (is_udp4)\n\t\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t   UDP_MIB_INERRORS,\n\t\t\t\t\t\t   is_udplite);\n\t\t\telse\n\t\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t    UDP_MIB_INERRORS,\n\t\t\t\t\t\t    is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\tif (!peeked) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in6 *sin6;\n\n\t\tsin6 = (struct sockaddr_in6 *) msg->msg_name;\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\n\t\tif (is_udp4) {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\t\tsin6->sin6_scope_id = 0;\n\t\t} else {\n\t\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tsin6->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t    IP6CB(skb)->iif);\n\t\t}\n\t\t*addr_len = sizeof(*sin6);\n\t}\n\tif (is_udp4) {\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\t}\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tif (is_udp4) {\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t} else {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t}\n\tunlock_sock_fast(sk, slow);\n\n\tif (noblock)\n\t\treturn -EAGAIN;\n\n\t/* starting over for a new packet */\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,9 +11,6 @@\n \tint is_udplite = IS_UDPLITE(sk);\n \tint is_udp4;\n \tbool slow;\n-\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(struct sockaddr_in6);\n \n \tif (flags & MSG_ERRQUEUE)\n \t\treturn ipv6_recv_error(sk, msg, len);\n@@ -100,7 +97,7 @@\n \t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n \t\t\t\t\t\t    IP6CB(skb)->iif);\n \t\t}\n-\n+\t\t*addr_len = sizeof(*sin6);\n \t}\n \tif (is_udp4) {\n \t\tif (inet->cmsg_flags)",
        "function_modified_lines": {
            "added": [
                "\t\t*addr_len = sizeof(*sin6);"
            ],
            "deleted": [
                "",
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(struct sockaddr_in6);",
                ""
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Linux kernel before 3.12.4 updates certain length values before ensuring that associated data structures have been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call, related to net/ipv4/ping.c, net/ipv4/raw.c, net/ipv4/udp.c, net/ipv6/raw.c, and net/ipv6/udp.c.",
        "id": 363
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int llcp_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t     struct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tunsigned int copied, rlen;\n\tstruct sk_buff *skb, *cskb;\n\tint err = 0;\n\n\tpr_debug(\"%p %zu\\n\", sk, len);\n\n\tmsg->msg_namelen = 0;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == LLCP_CLOSED &&\n\t    skb_queue_empty(&sk->sk_receive_queue)) {\n\t\trelease_sock(sk);\n\t\treturn 0;\n\t}\n\n\trelease_sock(sk);\n\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb) {\n\t\tpr_err(\"Recv datagram failed state %d %d %d\",\n\t\t       sk->sk_state, err, sock_error(sk));\n\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\treturn 0;\n\n\t\treturn err;\n\t}\n\n\trlen = skb->len;\t\t/* real length of skb */\n\tcopied = min_t(unsigned int, rlen, len);\n\n\tcskb = skb;\n\tif (skb_copy_datagram_iovec(cskb, 0, msg->msg_iov, copied)) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn -EFAULT;\n\t}\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tif (sk->sk_type == SOCK_DGRAM && msg->msg_name) {\n\t\tstruct nfc_llcp_ui_cb *ui_cb = nfc_llcp_ui_skb_cb(skb);\n\t\tstruct sockaddr_nfc_llcp *sockaddr =\n\t\t\t(struct sockaddr_nfc_llcp *) msg->msg_name;\n\n\t\tmsg->msg_namelen = sizeof(struct sockaddr_nfc_llcp);\n\n\t\tpr_debug(\"Datagram socket %d %d\\n\", ui_cb->dsap, ui_cb->ssap);\n\n\t\tmemset(sockaddr, 0, sizeof(*sockaddr));\n\t\tsockaddr->sa_family = AF_NFC;\n\t\tsockaddr->nfc_protocol = NFC_PROTO_NFC_DEP;\n\t\tsockaddr->dsap = ui_cb->dsap;\n\t\tsockaddr->ssap = ui_cb->ssap;\n\t}\n\n\t/* Mark read part of skb as used */\n\tif (!(flags & MSG_PEEK)) {\n\n\t\t/* SOCK_STREAM: re-queue skb if it contains unreceived data */\n\t\tif (sk->sk_type == SOCK_STREAM ||\n\t\t    sk->sk_type == SOCK_DGRAM ||\n\t\t    sk->sk_type == SOCK_RAW) {\n\t\t\tskb_pull(skb, copied);\n\t\t\tif (skb->len) {\n\t\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\n\t\tkfree_skb(skb);\n\t}\n\n\t/* XXX Queue backlogged skbs */\n\ndone:\n\t/* SOCK_SEQPACKET: return real length if MSG_TRUNC is set */\n\tif (sk->sk_type == SOCK_SEQPACKET && (flags & MSG_TRUNC))\n\t\tcopied = rlen;\n\n\treturn copied;\n}",
        "code_after_change": "static int llcp_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t     struct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tunsigned int copied, rlen;\n\tstruct sk_buff *skb, *cskb;\n\tint err = 0;\n\n\tpr_debug(\"%p %zu\\n\", sk, len);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == LLCP_CLOSED &&\n\t    skb_queue_empty(&sk->sk_receive_queue)) {\n\t\trelease_sock(sk);\n\t\treturn 0;\n\t}\n\n\trelease_sock(sk);\n\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb) {\n\t\tpr_err(\"Recv datagram failed state %d %d %d\",\n\t\t       sk->sk_state, err, sock_error(sk));\n\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\treturn 0;\n\n\t\treturn err;\n\t}\n\n\trlen = skb->len;\t\t/* real length of skb */\n\tcopied = min_t(unsigned int, rlen, len);\n\n\tcskb = skb;\n\tif (skb_copy_datagram_iovec(cskb, 0, msg->msg_iov, copied)) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn -EFAULT;\n\t}\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tif (sk->sk_type == SOCK_DGRAM && msg->msg_name) {\n\t\tstruct nfc_llcp_ui_cb *ui_cb = nfc_llcp_ui_skb_cb(skb);\n\t\tstruct sockaddr_nfc_llcp *sockaddr =\n\t\t\t(struct sockaddr_nfc_llcp *) msg->msg_name;\n\n\t\tmsg->msg_namelen = sizeof(struct sockaddr_nfc_llcp);\n\n\t\tpr_debug(\"Datagram socket %d %d\\n\", ui_cb->dsap, ui_cb->ssap);\n\n\t\tmemset(sockaddr, 0, sizeof(*sockaddr));\n\t\tsockaddr->sa_family = AF_NFC;\n\t\tsockaddr->nfc_protocol = NFC_PROTO_NFC_DEP;\n\t\tsockaddr->dsap = ui_cb->dsap;\n\t\tsockaddr->ssap = ui_cb->ssap;\n\t}\n\n\t/* Mark read part of skb as used */\n\tif (!(flags & MSG_PEEK)) {\n\n\t\t/* SOCK_STREAM: re-queue skb if it contains unreceived data */\n\t\tif (sk->sk_type == SOCK_STREAM ||\n\t\t    sk->sk_type == SOCK_DGRAM ||\n\t\t    sk->sk_type == SOCK_RAW) {\n\t\t\tskb_pull(skb, copied);\n\t\t\tif (skb->len) {\n\t\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\n\t\tkfree_skb(skb);\n\t}\n\n\t/* XXX Queue backlogged skbs */\n\ndone:\n\t/* SOCK_SEQPACKET: return real length if MSG_TRUNC is set */\n\tif (sk->sk_type == SOCK_SEQPACKET && (flags & MSG_TRUNC))\n\t\tcopied = rlen;\n\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,6 @@\n \tint err = 0;\n \n \tpr_debug(\"%p %zu\\n\", sk, len);\n-\n-\tmsg->msg_namelen = 0;\n \n \tlock_sock(sk);\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 391
    },
    {
        "cve_id": "CVE-2013-7263",
        "code_before_change": "static int rawv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)msg->msg_name;\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tif (addr_len)\n\t\t*addr_len=sizeof(*sin6);\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\tif (skb_csum_unnecessary(skb)) {\n\t\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\t} else if (msg->msg_flags&MSG_TRUNC) {\n\t\tif (__skb_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\t} else {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, 0, msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (err)\n\t\tgoto out_free;\n\n\t/* Copy the address. */\n\tif (sin6) {\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = 0;\n\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\tsin6->sin6_flowinfo = 0;\n\t\tsin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t\t  IP6CB(skb)->iif);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = skb->len;\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tskb_kill_datagram(sk, skb, flags);\n\n\t/* Error for blocking case is chosen to masquerade\n\t   as some normal condition.\n\t */\n\terr = (flags&MSG_DONTWAIT) ? -EAGAIN : -EHOSTUNREACH;\n\tgoto out;\n}",
        "code_after_change": "static int rawv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)msg->msg_name;\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\tif (skb_csum_unnecessary(skb)) {\n\t\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\t} else if (msg->msg_flags&MSG_TRUNC) {\n\t\tif (__skb_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\t} else {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, 0, msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (err)\n\t\tgoto out_free;\n\n\t/* Copy the address. */\n\tif (sin6) {\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = 0;\n\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\tsin6->sin6_flowinfo = 0;\n\t\tsin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t\t  IP6CB(skb)->iif);\n\t\t*addr_len = sizeof(*sin6);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = skb->len;\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tskb_kill_datagram(sk, skb, flags);\n\n\t/* Error for blocking case is chosen to masquerade\n\t   as some normal condition.\n\t */\n\terr = (flags&MSG_DONTWAIT) ? -EAGAIN : -EHOSTUNREACH;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,9 +10,6 @@\n \n \tif (flags & MSG_OOB)\n \t\treturn -EOPNOTSUPP;\n-\n-\tif (addr_len)\n-\t\t*addr_len=sizeof(*sin6);\n \n \tif (flags & MSG_ERRQUEUE)\n \t\treturn ipv6_recv_error(sk, msg, len);\n@@ -52,6 +49,7 @@\n \t\tsin6->sin6_flowinfo = 0;\n \t\tsin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,\n \t\t\t\t\t\t\t  IP6CB(skb)->iif);\n+\t\t*addr_len = sizeof(*sin6);\n \t}\n \n \tsock_recv_ts_and_drops(msg, sk, skb);",
        "function_modified_lines": {
            "added": [
                "\t\t*addr_len = sizeof(*sin6);"
            ],
            "deleted": [
                "",
                "\tif (addr_len)",
                "\t\t*addr_len=sizeof(*sin6);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Linux kernel before 3.12.4 updates certain length values before ensuring that associated data structures have been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call, related to net/ipv4/ping.c, net/ipv4/raw.c, net/ipv4/udp.c, net/ipv6/raw.c, and net/ipv6/udp.c.",
        "id": 362
    },
    {
        "cve_id": "CVE-2020-12363",
        "code_before_change": "int intel_guc_ads_create(struct intel_guc *guc)\n{\n\tconst u32 size = PAGE_ALIGN(sizeof(struct __guc_ads_blob));\n\tint ret;\n\n\tGEM_BUG_ON(guc->ads_vma);\n\n\tret = intel_guc_allocate_and_map_vma(guc, size, &guc->ads_vma,\n\t\t\t\t\t     (void **)&guc->ads_blob);\n\n\tif (ret)\n\t\treturn ret;\n\n\t__guc_ads_init(guc);\n\n\treturn 0;\n}",
        "code_after_change": "int intel_guc_ads_create(struct intel_guc *guc)\n{\n\tu32 size;\n\tint ret;\n\n\tGEM_BUG_ON(guc->ads_vma);\n\n\tsize = guc_ads_blob_size(guc);\n\n\tret = intel_guc_allocate_and_map_vma(guc, size, &guc->ads_vma,\n\t\t\t\t\t     (void **)&guc->ads_blob);\n\tif (ret)\n\t\treturn ret;\n\n\t__guc_ads_init(guc);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,13 +1,14 @@\n int intel_guc_ads_create(struct intel_guc *guc)\n {\n-\tconst u32 size = PAGE_ALIGN(sizeof(struct __guc_ads_blob));\n+\tu32 size;\n \tint ret;\n \n \tGEM_BUG_ON(guc->ads_vma);\n \n+\tsize = guc_ads_blob_size(guc);\n+\n \tret = intel_guc_allocate_and_map_vma(guc, size, &guc->ads_vma,\n \t\t\t\t\t     (void **)&guc->ads_blob);\n-\n \tif (ret)\n \t\treturn ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tu32 size;",
                "\tsize = guc_ads_blob_size(guc);",
                ""
            ],
            "deleted": [
                "\tconst u32 size = PAGE_ALIGN(sizeof(struct __guc_ads_blob));",
                ""
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Improper input validation in some Intel(R) Graphics Drivers for Windows* before version 26.20.100.7212 and before Linux kernel version 5.5 may allow a privileged user to potentially enable a denial of service via local access.",
        "id": 2461
    },
    {
        "cve_id": "CVE-2015-2672",
        "code_before_change": "static inline int xrstor_state(struct xsave_struct *fx, u64 mask)\n{\n\tint err = 0;\n\tu32 lmask = mask;\n\tu32 hmask = mask >> 32;\n\n\t/*\n\t * Use xrstors to restore context if it is enabled. xrstors supports\n\t * compacted format of xsave area which is not supported by xrstor.\n\t */\n\talternative_input(\n\t\t\"1: \" XRSTOR,\n\t\t\"1: \" XRSTORS,\n\t\tX86_FEATURE_XSAVES,\n\t\t\"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n\t\t: \"memory\");\n\n\tasm volatile(\"2:\\n\"\n\t\t     xstate_fault\n\t\t     : \"0\" (0)\n\t\t     : \"memory\");\n\n\treturn err;\n}",
        "code_after_change": "static inline int xrstor_state(struct xsave_struct *fx, u64 mask)\n{\n\tint err = 0;\n\tu32 lmask = mask;\n\tu32 hmask = mask >> 32;\n\n\t/*\n\t * Use xrstors to restore context if it is enabled. xrstors supports\n\t * compacted format of xsave area which is not supported by xrstor.\n\t */\n\talternative_input(\n\t\t\"1: \" XRSTOR,\n\t\tXRSTORS,\n\t\tX86_FEATURE_XSAVES,\n\t\t\"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n\t\t: \"memory\");\n\n\tasm volatile(\"2:\\n\"\n\t\t     xstate_fault\n\t\t     : \"0\" (0)\n\t\t     : \"memory\");\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,7 @@\n \t */\n \talternative_input(\n \t\t\"1: \" XRSTOR,\n-\t\t\"1: \" XRSTORS,\n+\t\tXRSTORS,\n \t\tX86_FEATURE_XSAVES,\n \t\t\"D\" (fx), \"m\" (*fx), \"a\" (lmask), \"d\" (hmask)\n \t\t: \"memory\");",
        "function_modified_lines": {
            "added": [
                "\t\tXRSTORS,"
            ],
            "deleted": [
                "\t\t\"1: \" XRSTORS,"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The xsave/xrstor implementation in arch/x86/include/asm/xsave.h in the Linux kernel before 3.19.2 creates certain .altinstr_replacement pointers and consequently does not provide any protection against instruction faulting, which allows local users to cause a denial of service (panic) by triggering a fault, as demonstrated by an unaligned memory operand or a non-canonical address memory operand.",
        "id": 744
    },
    {
        "cve_id": "CVE-2021-3655",
        "code_before_change": "int sctp_process_init(struct sctp_association *asoc, struct sctp_chunk *chunk,\n\t\t      const union sctp_addr *peer_addr,\n\t\t      struct sctp_init_chunk *peer_init, gfp_t gfp)\n{\n\tstruct sctp_transport *transport;\n\tstruct list_head *pos, *temp;\n\tunion sctp_params param;\n\tunion sctp_addr addr;\n\tstruct sctp_af *af;\n\tint src_match = 0;\n\n\t/* We must include the address that the INIT packet came from.\n\t * This is the only address that matters for an INIT packet.\n\t * When processing a COOKIE ECHO, we retrieve the from address\n\t * of the INIT from the cookie.\n\t */\n\n\t/* This implementation defaults to making the first transport\n\t * added as the primary transport.  The source address seems to\n\t * be a better choice than any of the embedded addresses.\n\t */\n\tasoc->encap_port = SCTP_INPUT_CB(chunk->skb)->encap_port;\n\tif (!sctp_assoc_add_peer(asoc, peer_addr, gfp, SCTP_ACTIVE))\n\t\tgoto nomem;\n\n\tif (sctp_cmp_addr_exact(sctp_source(chunk), peer_addr))\n\t\tsrc_match = 1;\n\n\t/* Process the initialization parameters.  */\n\tsctp_walk_params(param, peer_init, init_hdr.params) {\n\t\tif (!src_match && (param.p->type == SCTP_PARAM_IPV4_ADDRESS ||\n\t\t    param.p->type == SCTP_PARAM_IPV6_ADDRESS)) {\n\t\t\taf = sctp_get_af_specific(param_type2af(param.p->type));\n\t\t\taf->from_addr_param(&addr, param.addr,\n\t\t\t\t\t    chunk->sctp_hdr->source, 0);\n\t\t\tif (sctp_cmp_addr_exact(sctp_source(chunk), &addr))\n\t\t\t\tsrc_match = 1;\n\t\t}\n\n\t\tif (!sctp_process_param(asoc, param, peer_addr, gfp))\n\t\t\tgoto clean_up;\n\t}\n\n\t/* source address of chunk may not match any valid address */\n\tif (!src_match)\n\t\tgoto clean_up;\n\n\t/* AUTH: After processing the parameters, make sure that we\n\t * have all the required info to potentially do authentications.\n\t */\n\tif (asoc->peer.auth_capable && (!asoc->peer.peer_random ||\n\t\t\t\t\t!asoc->peer.peer_hmacs))\n\t\tasoc->peer.auth_capable = 0;\n\n\t/* In a non-backward compatible mode, if the peer claims\n\t * support for ADD-IP but not AUTH,  the ADD-IP spec states\n\t * that we MUST ABORT the association. Section 6.  The section\n\t * also give us an option to silently ignore the packet, which\n\t * is what we'll do here.\n\t */\n\tif (!asoc->base.net->sctp.addip_noauth &&\n\t    (asoc->peer.asconf_capable && !asoc->peer.auth_capable)) {\n\t\tasoc->peer.addip_disabled_mask |= (SCTP_PARAM_ADD_IP |\n\t\t\t\t\t\t  SCTP_PARAM_DEL_IP |\n\t\t\t\t\t\t  SCTP_PARAM_SET_PRIMARY);\n\t\tasoc->peer.asconf_capable = 0;\n\t\tgoto clean_up;\n\t}\n\n\t/* Walk list of transports, removing transports in the UNKNOWN state. */\n\tlist_for_each_safe(pos, temp, &asoc->peer.transport_addr_list) {\n\t\ttransport = list_entry(pos, struct sctp_transport, transports);\n\t\tif (transport->state == SCTP_UNKNOWN) {\n\t\t\tsctp_assoc_rm_peer(asoc, transport);\n\t\t}\n\t}\n\n\t/* The fixed INIT headers are always in network byte\n\t * order.\n\t */\n\tasoc->peer.i.init_tag =\n\t\tntohl(peer_init->init_hdr.init_tag);\n\tasoc->peer.i.a_rwnd =\n\t\tntohl(peer_init->init_hdr.a_rwnd);\n\tasoc->peer.i.num_outbound_streams =\n\t\tntohs(peer_init->init_hdr.num_outbound_streams);\n\tasoc->peer.i.num_inbound_streams =\n\t\tntohs(peer_init->init_hdr.num_inbound_streams);\n\tasoc->peer.i.initial_tsn =\n\t\tntohl(peer_init->init_hdr.initial_tsn);\n\n\tasoc->strreset_inseq = asoc->peer.i.initial_tsn;\n\n\t/* Apply the upper bounds for output streams based on peer's\n\t * number of inbound streams.\n\t */\n\tif (asoc->c.sinit_num_ostreams  >\n\t    ntohs(peer_init->init_hdr.num_inbound_streams)) {\n\t\tasoc->c.sinit_num_ostreams =\n\t\t\tntohs(peer_init->init_hdr.num_inbound_streams);\n\t}\n\n\tif (asoc->c.sinit_max_instreams >\n\t    ntohs(peer_init->init_hdr.num_outbound_streams)) {\n\t\tasoc->c.sinit_max_instreams =\n\t\t\tntohs(peer_init->init_hdr.num_outbound_streams);\n\t}\n\n\t/* Copy Initiation tag from INIT to VT_peer in cookie.   */\n\tasoc->c.peer_vtag = asoc->peer.i.init_tag;\n\n\t/* Peer Rwnd   : Current calculated value of the peer's rwnd.  */\n\tasoc->peer.rwnd = asoc->peer.i.a_rwnd;\n\n\t/* RFC 2960 7.2.1 The initial value of ssthresh MAY be arbitrarily\n\t * high (for example, implementations MAY use the size of the receiver\n\t * advertised window).\n\t */\n\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\ttransports) {\n\t\ttransport->ssthresh = asoc->peer.i.a_rwnd;\n\t}\n\n\t/* Set up the TSN tracking pieces.  */\n\tif (!sctp_tsnmap_init(&asoc->peer.tsn_map, SCTP_TSN_MAP_INITIAL,\n\t\t\t\tasoc->peer.i.initial_tsn, gfp))\n\t\tgoto clean_up;\n\n\t/* RFC 2960 6.5 Stream Identifier and Stream Sequence Number\n\t *\n\t * The stream sequence number in all the streams shall start\n\t * from 0 when the association is established.  Also, when the\n\t * stream sequence number reaches the value 65535 the next\n\t * stream sequence number shall be set to 0.\n\t */\n\n\tif (sctp_stream_init(&asoc->stream, asoc->c.sinit_num_ostreams,\n\t\t\t     asoc->c.sinit_max_instreams, gfp))\n\t\tgoto clean_up;\n\n\t/* Update frag_point when stream_interleave may get changed. */\n\tsctp_assoc_update_frag_point(asoc);\n\n\tif (!asoc->temp && sctp_assoc_set_id(asoc, gfp))\n\t\tgoto clean_up;\n\n\t/* ADDIP Section 4.1 ASCONF Chunk Procedures\n\t *\n\t * When an endpoint has an ASCONF signaled change to be sent to the\n\t * remote endpoint it should do the following:\n\t * ...\n\t * A2) A serial number should be assigned to the Chunk. The serial\n\t * number should be a monotonically increasing number. All serial\n\t * numbers are defined to be initialized at the start of the\n\t * association to the same value as the Initial TSN.\n\t */\n\tasoc->peer.addip_serial = asoc->peer.i.initial_tsn - 1;\n\treturn 1;\n\nclean_up:\n\t/* Release the transport structures. */\n\tlist_for_each_safe(pos, temp, &asoc->peer.transport_addr_list) {\n\t\ttransport = list_entry(pos, struct sctp_transport, transports);\n\t\tif (transport->state != SCTP_ACTIVE)\n\t\t\tsctp_assoc_rm_peer(asoc, transport);\n\t}\n\nnomem:\n\treturn 0;\n}",
        "code_after_change": "int sctp_process_init(struct sctp_association *asoc, struct sctp_chunk *chunk,\n\t\t      const union sctp_addr *peer_addr,\n\t\t      struct sctp_init_chunk *peer_init, gfp_t gfp)\n{\n\tstruct sctp_transport *transport;\n\tstruct list_head *pos, *temp;\n\tunion sctp_params param;\n\tunion sctp_addr addr;\n\tstruct sctp_af *af;\n\tint src_match = 0;\n\n\t/* We must include the address that the INIT packet came from.\n\t * This is the only address that matters for an INIT packet.\n\t * When processing a COOKIE ECHO, we retrieve the from address\n\t * of the INIT from the cookie.\n\t */\n\n\t/* This implementation defaults to making the first transport\n\t * added as the primary transport.  The source address seems to\n\t * be a better choice than any of the embedded addresses.\n\t */\n\tasoc->encap_port = SCTP_INPUT_CB(chunk->skb)->encap_port;\n\tif (!sctp_assoc_add_peer(asoc, peer_addr, gfp, SCTP_ACTIVE))\n\t\tgoto nomem;\n\n\tif (sctp_cmp_addr_exact(sctp_source(chunk), peer_addr))\n\t\tsrc_match = 1;\n\n\t/* Process the initialization parameters.  */\n\tsctp_walk_params(param, peer_init, init_hdr.params) {\n\t\tif (!src_match &&\n\t\t    (param.p->type == SCTP_PARAM_IPV4_ADDRESS ||\n\t\t     param.p->type == SCTP_PARAM_IPV6_ADDRESS)) {\n\t\t\taf = sctp_get_af_specific(param_type2af(param.p->type));\n\t\t\tif (!af->from_addr_param(&addr, param.addr,\n\t\t\t\t\t\t chunk->sctp_hdr->source, 0))\n\t\t\t\tcontinue;\n\t\t\tif (sctp_cmp_addr_exact(sctp_source(chunk), &addr))\n\t\t\t\tsrc_match = 1;\n\t\t}\n\n\t\tif (!sctp_process_param(asoc, param, peer_addr, gfp))\n\t\t\tgoto clean_up;\n\t}\n\n\t/* source address of chunk may not match any valid address */\n\tif (!src_match)\n\t\tgoto clean_up;\n\n\t/* AUTH: After processing the parameters, make sure that we\n\t * have all the required info to potentially do authentications.\n\t */\n\tif (asoc->peer.auth_capable && (!asoc->peer.peer_random ||\n\t\t\t\t\t!asoc->peer.peer_hmacs))\n\t\tasoc->peer.auth_capable = 0;\n\n\t/* In a non-backward compatible mode, if the peer claims\n\t * support for ADD-IP but not AUTH,  the ADD-IP spec states\n\t * that we MUST ABORT the association. Section 6.  The section\n\t * also give us an option to silently ignore the packet, which\n\t * is what we'll do here.\n\t */\n\tif (!asoc->base.net->sctp.addip_noauth &&\n\t    (asoc->peer.asconf_capable && !asoc->peer.auth_capable)) {\n\t\tasoc->peer.addip_disabled_mask |= (SCTP_PARAM_ADD_IP |\n\t\t\t\t\t\t  SCTP_PARAM_DEL_IP |\n\t\t\t\t\t\t  SCTP_PARAM_SET_PRIMARY);\n\t\tasoc->peer.asconf_capable = 0;\n\t\tgoto clean_up;\n\t}\n\n\t/* Walk list of transports, removing transports in the UNKNOWN state. */\n\tlist_for_each_safe(pos, temp, &asoc->peer.transport_addr_list) {\n\t\ttransport = list_entry(pos, struct sctp_transport, transports);\n\t\tif (transport->state == SCTP_UNKNOWN) {\n\t\t\tsctp_assoc_rm_peer(asoc, transport);\n\t\t}\n\t}\n\n\t/* The fixed INIT headers are always in network byte\n\t * order.\n\t */\n\tasoc->peer.i.init_tag =\n\t\tntohl(peer_init->init_hdr.init_tag);\n\tasoc->peer.i.a_rwnd =\n\t\tntohl(peer_init->init_hdr.a_rwnd);\n\tasoc->peer.i.num_outbound_streams =\n\t\tntohs(peer_init->init_hdr.num_outbound_streams);\n\tasoc->peer.i.num_inbound_streams =\n\t\tntohs(peer_init->init_hdr.num_inbound_streams);\n\tasoc->peer.i.initial_tsn =\n\t\tntohl(peer_init->init_hdr.initial_tsn);\n\n\tasoc->strreset_inseq = asoc->peer.i.initial_tsn;\n\n\t/* Apply the upper bounds for output streams based on peer's\n\t * number of inbound streams.\n\t */\n\tif (asoc->c.sinit_num_ostreams  >\n\t    ntohs(peer_init->init_hdr.num_inbound_streams)) {\n\t\tasoc->c.sinit_num_ostreams =\n\t\t\tntohs(peer_init->init_hdr.num_inbound_streams);\n\t}\n\n\tif (asoc->c.sinit_max_instreams >\n\t    ntohs(peer_init->init_hdr.num_outbound_streams)) {\n\t\tasoc->c.sinit_max_instreams =\n\t\t\tntohs(peer_init->init_hdr.num_outbound_streams);\n\t}\n\n\t/* Copy Initiation tag from INIT to VT_peer in cookie.   */\n\tasoc->c.peer_vtag = asoc->peer.i.init_tag;\n\n\t/* Peer Rwnd   : Current calculated value of the peer's rwnd.  */\n\tasoc->peer.rwnd = asoc->peer.i.a_rwnd;\n\n\t/* RFC 2960 7.2.1 The initial value of ssthresh MAY be arbitrarily\n\t * high (for example, implementations MAY use the size of the receiver\n\t * advertised window).\n\t */\n\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\ttransports) {\n\t\ttransport->ssthresh = asoc->peer.i.a_rwnd;\n\t}\n\n\t/* Set up the TSN tracking pieces.  */\n\tif (!sctp_tsnmap_init(&asoc->peer.tsn_map, SCTP_TSN_MAP_INITIAL,\n\t\t\t\tasoc->peer.i.initial_tsn, gfp))\n\t\tgoto clean_up;\n\n\t/* RFC 2960 6.5 Stream Identifier and Stream Sequence Number\n\t *\n\t * The stream sequence number in all the streams shall start\n\t * from 0 when the association is established.  Also, when the\n\t * stream sequence number reaches the value 65535 the next\n\t * stream sequence number shall be set to 0.\n\t */\n\n\tif (sctp_stream_init(&asoc->stream, asoc->c.sinit_num_ostreams,\n\t\t\t     asoc->c.sinit_max_instreams, gfp))\n\t\tgoto clean_up;\n\n\t/* Update frag_point when stream_interleave may get changed. */\n\tsctp_assoc_update_frag_point(asoc);\n\n\tif (!asoc->temp && sctp_assoc_set_id(asoc, gfp))\n\t\tgoto clean_up;\n\n\t/* ADDIP Section 4.1 ASCONF Chunk Procedures\n\t *\n\t * When an endpoint has an ASCONF signaled change to be sent to the\n\t * remote endpoint it should do the following:\n\t * ...\n\t * A2) A serial number should be assigned to the Chunk. The serial\n\t * number should be a monotonically increasing number. All serial\n\t * numbers are defined to be initialized at the start of the\n\t * association to the same value as the Initial TSN.\n\t */\n\tasoc->peer.addip_serial = asoc->peer.i.initial_tsn - 1;\n\treturn 1;\n\nclean_up:\n\t/* Release the transport structures. */\n\tlist_for_each_safe(pos, temp, &asoc->peer.transport_addr_list) {\n\t\ttransport = list_entry(pos, struct sctp_transport, transports);\n\t\tif (transport->state != SCTP_ACTIVE)\n\t\t\tsctp_assoc_rm_peer(asoc, transport);\n\t}\n\nnomem:\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,11 +28,13 @@\n \n \t/* Process the initialization parameters.  */\n \tsctp_walk_params(param, peer_init, init_hdr.params) {\n-\t\tif (!src_match && (param.p->type == SCTP_PARAM_IPV4_ADDRESS ||\n-\t\t    param.p->type == SCTP_PARAM_IPV6_ADDRESS)) {\n+\t\tif (!src_match &&\n+\t\t    (param.p->type == SCTP_PARAM_IPV4_ADDRESS ||\n+\t\t     param.p->type == SCTP_PARAM_IPV6_ADDRESS)) {\n \t\t\taf = sctp_get_af_specific(param_type2af(param.p->type));\n-\t\t\taf->from_addr_param(&addr, param.addr,\n-\t\t\t\t\t    chunk->sctp_hdr->source, 0);\n+\t\t\tif (!af->from_addr_param(&addr, param.addr,\n+\t\t\t\t\t\t chunk->sctp_hdr->source, 0))\n+\t\t\t\tcontinue;\n \t\t\tif (sctp_cmp_addr_exact(sctp_source(chunk), &addr))\n \t\t\t\tsrc_match = 1;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (!src_match &&",
                "\t\t    (param.p->type == SCTP_PARAM_IPV4_ADDRESS ||",
                "\t\t     param.p->type == SCTP_PARAM_IPV6_ADDRESS)) {",
                "\t\t\tif (!af->from_addr_param(&addr, param.addr,",
                "\t\t\t\t\t\t chunk->sctp_hdr->source, 0))",
                "\t\t\t\tcontinue;"
            ],
            "deleted": [
                "\t\tif (!src_match && (param.p->type == SCTP_PARAM_IPV4_ADDRESS ||",
                "\t\t    param.p->type == SCTP_PARAM_IPV6_ADDRESS)) {",
                "\t\t\taf->from_addr_param(&addr, param.addr,",
                "\t\t\t\t\t    chunk->sctp_hdr->source, 0);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel in versions prior to v5.14-rc1. Missing size validations on inbound SCTP packets may allow the kernel to read uninitialized memory.",
        "id": 3037
    },
    {
        "cve_id": "CVE-2015-7509",
        "code_before_change": "int ext4_orphan_add(handle_t *handle, struct inode *inode)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tstruct ext4_iloc iloc;\n\tint err = 0, rc;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\n\tmutex_lock(&EXT4_SB(sb)->s_orphan_lock);\n\tif (!list_empty(&EXT4_I(inode)->i_orphan))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Orphan handling is only valid for files with data blocks\n\t * being truncated, or files being unlinked. Note that we either\n\t * hold i_mutex, or the inode can not be referenced from outside,\n\t * so i_nlink should not be bumped due to race\n\t */\n\tJ_ASSERT((S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t  S_ISLNK(inode->i_mode)) || inode->i_nlink == 0);\n\n\tBUFFER_TRACE(EXT4_SB(sb)->s_sbh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, EXT4_SB(sb)->s_sbh);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = ext4_reserve_inode_write(handle, inode, &iloc);\n\tif (err)\n\t\tgoto out_unlock;\n\t/*\n\t * Due to previous errors inode may be already a part of on-disk\n\t * orphan list. If so skip on-disk list modification.\n\t */\n\tif (NEXT_ORPHAN(inode) && NEXT_ORPHAN(inode) <=\n\t\t(le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count)))\n\t\t\tgoto mem_insert;\n\n\t/* Insert this inode at the head of the on-disk orphan list... */\n\tNEXT_ORPHAN(inode) = le32_to_cpu(EXT4_SB(sb)->s_es->s_last_orphan);\n\tEXT4_SB(sb)->s_es->s_last_orphan = cpu_to_le32(inode->i_ino);\n\terr = ext4_handle_dirty_super(handle, sb);\n\trc = ext4_mark_iloc_dirty(handle, inode, &iloc);\n\tif (!err)\n\t\terr = rc;\n\n\t/* Only add to the head of the in-memory list if all the\n\t * previous operations succeeded.  If the orphan_add is going to\n\t * fail (possibly taking the journal offline), we can't risk\n\t * leaving the inode on the orphan list: stray orphan-list\n\t * entries can cause panics at unmount time.\n\t *\n\t * This is safe: on error we're going to ignore the orphan list\n\t * anyway on the next recovery. */\nmem_insert:\n\tif (!err)\n\t\tlist_add(&EXT4_I(inode)->i_orphan, &EXT4_SB(sb)->s_orphan);\n\n\tjbd_debug(4, \"superblock will point to %lu\\n\", inode->i_ino);\n\tjbd_debug(4, \"orphan inode %lu will point to %d\\n\",\n\t\t\tinode->i_ino, NEXT_ORPHAN(inode));\nout_unlock:\n\tmutex_unlock(&EXT4_SB(sb)->s_orphan_lock);\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}",
        "code_after_change": "int ext4_orphan_add(handle_t *handle, struct inode *inode)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tstruct ext4_iloc iloc;\n\tint err = 0, rc;\n\n\tif (!EXT4_SB(sb)->s_journal)\n\t\treturn 0;\n\n\tmutex_lock(&EXT4_SB(sb)->s_orphan_lock);\n\tif (!list_empty(&EXT4_I(inode)->i_orphan))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Orphan handling is only valid for files with data blocks\n\t * being truncated, or files being unlinked. Note that we either\n\t * hold i_mutex, or the inode can not be referenced from outside,\n\t * so i_nlink should not be bumped due to race\n\t */\n\tJ_ASSERT((S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t  S_ISLNK(inode->i_mode)) || inode->i_nlink == 0);\n\n\tBUFFER_TRACE(EXT4_SB(sb)->s_sbh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, EXT4_SB(sb)->s_sbh);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = ext4_reserve_inode_write(handle, inode, &iloc);\n\tif (err)\n\t\tgoto out_unlock;\n\t/*\n\t * Due to previous errors inode may be already a part of on-disk\n\t * orphan list. If so skip on-disk list modification.\n\t */\n\tif (NEXT_ORPHAN(inode) && NEXT_ORPHAN(inode) <=\n\t\t(le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count)))\n\t\t\tgoto mem_insert;\n\n\t/* Insert this inode at the head of the on-disk orphan list... */\n\tNEXT_ORPHAN(inode) = le32_to_cpu(EXT4_SB(sb)->s_es->s_last_orphan);\n\tEXT4_SB(sb)->s_es->s_last_orphan = cpu_to_le32(inode->i_ino);\n\terr = ext4_handle_dirty_super(handle, sb);\n\trc = ext4_mark_iloc_dirty(handle, inode, &iloc);\n\tif (!err)\n\t\terr = rc;\n\n\t/* Only add to the head of the in-memory list if all the\n\t * previous operations succeeded.  If the orphan_add is going to\n\t * fail (possibly taking the journal offline), we can't risk\n\t * leaving the inode on the orphan list: stray orphan-list\n\t * entries can cause panics at unmount time.\n\t *\n\t * This is safe: on error we're going to ignore the orphan list\n\t * anyway on the next recovery. */\nmem_insert:\n\tif (!err)\n\t\tlist_add(&EXT4_I(inode)->i_orphan, &EXT4_SB(sb)->s_orphan);\n\n\tjbd_debug(4, \"superblock will point to %lu\\n\", inode->i_ino);\n\tjbd_debug(4, \"orphan inode %lu will point to %d\\n\",\n\t\t\tinode->i_ino, NEXT_ORPHAN(inode));\nout_unlock:\n\tmutex_unlock(&EXT4_SB(sb)->s_orphan_lock);\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \tstruct ext4_iloc iloc;\n \tint err = 0, rc;\n \n-\tif (!ext4_handle_valid(handle))\n+\tif (!EXT4_SB(sb)->s_journal)\n \t\treturn 0;\n \n \tmutex_lock(&EXT4_SB(sb)->s_orphan_lock);",
        "function_modified_lines": {
            "added": [
                "\tif (!EXT4_SB(sb)->s_journal)"
            ],
            "deleted": [
                "\tif (!ext4_handle_valid(handle))"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "fs/ext4/namei.c in the Linux kernel before 3.7 allows physically proximate attackers to cause a denial of service (system crash) via a crafted no-journal filesystem, a related issue to CVE-2013-2015.",
        "id": 782
    },
    {
        "cve_id": "CVE-2012-4398",
        "code_before_change": "static void dm_check_ac_dc_power(struct net_device *dev)\n{\n\tstruct r8192_priv *priv = rtllib_priv(dev);\n\tstatic char *ac_dc_check_script_path = \"/etc/acpi/wireless-rtl-ac-dc-power.sh\";\n\tchar *argv[] = {ac_dc_check_script_path, DRV_NAME, NULL};\n\tstatic char *envp[] = {\"HOME=/\",\n\t\t\t\"TERM=linux\",\n\t\t\t\"PATH=/usr/bin:/bin\",\n\t\t\t NULL};\n\n\tif (priv->ResetProgress == RESET_TYPE_SILENT) {\n\t\tRT_TRACE((COMP_INIT | COMP_POWER | COMP_RF),\n\t\t\t \"GPIOChangeRFWorkItemCallBack(): Silent Reseting!!!!!!!\\n\");\n\t\treturn;\n\t}\n\n\tif (priv->rtllib->state != RTLLIB_LINKED)\n\t\treturn;\n\tcall_usermodehelper(ac_dc_check_script_path, argv, envp, 1);\n\n\treturn;\n};",
        "code_after_change": "static void dm_check_ac_dc_power(struct net_device *dev)\n{\n\tstruct r8192_priv *priv = rtllib_priv(dev);\n\tstatic char *ac_dc_check_script_path = \"/etc/acpi/wireless-rtl-ac-dc-power.sh\";\n\tchar *argv[] = {ac_dc_check_script_path, DRV_NAME, NULL};\n\tstatic char *envp[] = {\"HOME=/\",\n\t\t\t\"TERM=linux\",\n\t\t\t\"PATH=/usr/bin:/bin\",\n\t\t\t NULL};\n\n\tif (priv->ResetProgress == RESET_TYPE_SILENT) {\n\t\tRT_TRACE((COMP_INIT | COMP_POWER | COMP_RF),\n\t\t\t \"GPIOChangeRFWorkItemCallBack(): Silent Reseting!!!!!!!\\n\");\n\t\treturn;\n\t}\n\n\tif (priv->rtllib->state != RTLLIB_LINKED)\n\t\treturn;\n\tcall_usermodehelper(ac_dc_check_script_path, argv, envp, UMH_WAIT_PROC);\n\n\treturn;\n};",
        "patch": "--- code before\n+++ code after\n@@ -16,7 +16,7 @@\n \n \tif (priv->rtllib->state != RTLLIB_LINKED)\n \t\treturn;\n-\tcall_usermodehelper(ac_dc_check_script_path, argv, envp, 1);\n+\tcall_usermodehelper(ac_dc_check_script_path, argv, envp, UMH_WAIT_PROC);\n \n \treturn;\n };",
        "function_modified_lines": {
            "added": [
                "\tcall_usermodehelper(ac_dc_check_script_path, argv, envp, UMH_WAIT_PROC);"
            ],
            "deleted": [
                "\tcall_usermodehelper(ac_dc_check_script_path, argv, envp, 1);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The __request_module function in kernel/kmod.c in the Linux kernel before 3.4 does not set a certain killable attribute, which allows local users to cause a denial of service (memory consumption) via a crafted application.",
        "id": 98
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "int vcc_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\tsize_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct atm_vcc *vcc;\n\tstruct sk_buff *skb;\n\tint copied, error = -EINVAL;\n\n\tmsg->msg_namelen = 0;\n\n\tif (sock->state != SS_CONNECTED)\n\t\treturn -ENOTCONN;\n\n\t/* only handle MSG_DONTWAIT and MSG_PEEK */\n\tif (flags & ~(MSG_DONTWAIT | MSG_PEEK))\n\t\treturn -EOPNOTSUPP;\n\n\tvcc = ATM_SD(sock);\n\tif (test_bit(ATM_VF_RELEASED, &vcc->flags) ||\n\t    test_bit(ATM_VF_CLOSE, &vcc->flags) ||\n\t    !test_bit(ATM_VF_READY, &vcc->flags))\n\t\treturn 0;\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &error);\n\tif (!skb)\n\t\treturn error;\n\n\tcopied = skb->len;\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terror = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (error)\n\t\treturn error;\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tpr_debug(\"%d -= %d\\n\", atomic_read(&sk->sk_rmem_alloc),\n\t\t\t skb->truesize);\n\t\tatm_return(vcc, skb->truesize);\n\t}\n\n\tskb_free_datagram(sk, skb);\n\treturn copied;\n}",
        "code_after_change": "int vcc_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\tsize_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct atm_vcc *vcc;\n\tstruct sk_buff *skb;\n\tint copied, error = -EINVAL;\n\n\tif (sock->state != SS_CONNECTED)\n\t\treturn -ENOTCONN;\n\n\t/* only handle MSG_DONTWAIT and MSG_PEEK */\n\tif (flags & ~(MSG_DONTWAIT | MSG_PEEK))\n\t\treturn -EOPNOTSUPP;\n\n\tvcc = ATM_SD(sock);\n\tif (test_bit(ATM_VF_RELEASED, &vcc->flags) ||\n\t    test_bit(ATM_VF_CLOSE, &vcc->flags) ||\n\t    !test_bit(ATM_VF_READY, &vcc->flags))\n\t\treturn 0;\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &error);\n\tif (!skb)\n\t\treturn error;\n\n\tcopied = skb->len;\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terror = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (error)\n\t\treturn error;\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tpr_debug(\"%d -= %d\\n\", atomic_read(&sk->sk_rmem_alloc),\n\t\t\t skb->truesize);\n\t\tatm_return(vcc, skb->truesize);\n\t}\n\n\tskb_free_datagram(sk, skb);\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,6 @@\n \tstruct atm_vcc *vcc;\n \tstruct sk_buff *skb;\n \tint copied, error = -EINVAL;\n-\n-\tmsg->msg_namelen = 0;\n \n \tif (sock->state != SS_CONNECTED)\n \t\treturn -ENOTCONN;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 371
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int hci_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\tif (sk->sk_state == BT_CLOSED)\n\t\treturn 0;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tmsg->msg_namelen = 0;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\tskb_reset_transport_header(skb);\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tswitch (hci_pi(sk)->channel) {\n\tcase HCI_CHANNEL_RAW:\n\t\thci_sock_cmsg(sk, msg, skb);\n\t\tbreak;\n\tcase HCI_CHANNEL_USER:\n\tcase HCI_CHANNEL_CONTROL:\n\tcase HCI_CHANNEL_MONITOR:\n\t\tsock_recv_timestamp(msg, sk, skb);\n\t\tbreak;\n\t}\n\n\tskb_free_datagram(sk, skb);\n\n\treturn err ? : copied;\n}",
        "code_after_change": "static int hci_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\tif (sk->sk_state == BT_CLOSED)\n\t\treturn 0;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\tskb_reset_transport_header(skb);\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tswitch (hci_pi(sk)->channel) {\n\tcase HCI_CHANNEL_RAW:\n\t\thci_sock_cmsg(sk, msg, skb);\n\t\tbreak;\n\tcase HCI_CHANNEL_USER:\n\tcase HCI_CHANNEL_CONTROL:\n\tcase HCI_CHANNEL_MONITOR:\n\t\tsock_recv_timestamp(msg, sk, skb);\n\t\tbreak;\n\t}\n\n\tskb_free_datagram(sk, skb);\n\n\treturn err ? : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,8 +17,6 @@\n \tskb = skb_recv_datagram(sk, flags, noblock, &err);\n \tif (!skb)\n \t\treturn err;\n-\n-\tmsg->msg_namelen = 0;\n \n \tcopied = skb->len;\n \tif (len < copied) {",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 375
    },
    {
        "cve_id": "CVE-2018-20669",
        "code_before_change": "long compat_put_bitmap(compat_ulong_t __user *umask, unsigned long *mask,\n\t\t       unsigned long bitmap_size)\n{\n\tunsigned long nr_compat_longs;\n\n\t/* align bitmap up to nearest compat_long_t boundary */\n\tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);\n\tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);\n\n\tif (!access_ok(umask, bitmap_size / 8))\n\t\treturn -EFAULT;\n\n\tuser_access_begin();\n\twhile (nr_compat_longs > 1) {\n\t\tunsigned long m = *mask++;\n\t\tunsafe_put_user((compat_ulong_t)m, umask++, Efault);\n\t\tunsafe_put_user(m >> BITS_PER_COMPAT_LONG, umask++, Efault);\n\t\tnr_compat_longs -= 2;\n\t}\n\tif (nr_compat_longs)\n\t\tunsafe_put_user((compat_ulong_t)*mask, umask++, Efault);\n\tuser_access_end();\n\treturn 0;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "code_after_change": "long compat_put_bitmap(compat_ulong_t __user *umask, unsigned long *mask,\n\t\t       unsigned long bitmap_size)\n{\n\tunsigned long nr_compat_longs;\n\n\t/* align bitmap up to nearest compat_long_t boundary */\n\tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);\n\tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);\n\n\tif (!user_access_begin(umask, bitmap_size / 8))\n\t\treturn -EFAULT;\n\n\twhile (nr_compat_longs > 1) {\n\t\tunsigned long m = *mask++;\n\t\tunsafe_put_user((compat_ulong_t)m, umask++, Efault);\n\t\tunsafe_put_user(m >> BITS_PER_COMPAT_LONG, umask++, Efault);\n\t\tnr_compat_longs -= 2;\n\t}\n\tif (nr_compat_longs)\n\t\tunsafe_put_user((compat_ulong_t)*mask, umask++, Efault);\n\tuser_access_end();\n\treturn 0;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,10 +7,9 @@\n \tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);\n \tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);\n \n-\tif (!access_ok(umask, bitmap_size / 8))\n+\tif (!user_access_begin(umask, bitmap_size / 8))\n \t\treturn -EFAULT;\n \n-\tuser_access_begin();\n \twhile (nr_compat_longs > 1) {\n \t\tunsigned long m = *mask++;\n \t\tunsafe_put_user((compat_ulong_t)m, umask++, Efault);",
        "function_modified_lines": {
            "added": [
                "\tif (!user_access_begin(umask, bitmap_size / 8))"
            ],
            "deleted": [
                "\tif (!access_ok(umask, bitmap_size / 8))",
                "\tuser_access_begin();"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "An issue where a provided address with access_ok() is not checked was discovered in i915_gem_execbuffer2_ioctl in drivers/gpu/drm/i915/i915_gem_execbuffer.c in the Linux kernel through 4.19.13. A local attacker can craft a malicious IOCTL function call to overwrite arbitrary kernel memory, resulting in a Denial of Service or privilege escalation.",
        "id": 1776
    },
    {
        "cve_id": "CVE-2013-1943",
        "code_before_change": "int __kvm_set_memory_region(struct kvm *kvm,\n\t\t\t    struct kvm_userspace_memory_region *mem,\n\t\t\t    int user_alloc)\n{\n\tint r;\n\tgfn_t base_gfn;\n\tunsigned long npages;\n\tunsigned long i;\n\tstruct kvm_memory_slot *memslot;\n\tstruct kvm_memory_slot old, new;\n\tstruct kvm_memslots *slots, *old_memslots;\n\n\tr = -EINVAL;\n\t/* General sanity checks */\n\tif (mem->memory_size & (PAGE_SIZE - 1))\n\t\tgoto out;\n\tif (mem->guest_phys_addr & (PAGE_SIZE - 1))\n\t\tgoto out;\n\tif (user_alloc && (mem->userspace_addr & (PAGE_SIZE - 1)))\n\t\tgoto out;\n\tif (mem->slot >= KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)\n\t\tgoto out;\n\tif (mem->guest_phys_addr + mem->memory_size < mem->guest_phys_addr)\n\t\tgoto out;\n\n\tmemslot = &kvm->memslots->memslots[mem->slot];\n\tbase_gfn = mem->guest_phys_addr >> PAGE_SHIFT;\n\tnpages = mem->memory_size >> PAGE_SHIFT;\n\n\tr = -EINVAL;\n\tif (npages > KVM_MEM_MAX_NR_PAGES)\n\t\tgoto out;\n\n\tif (!npages)\n\t\tmem->flags &= ~KVM_MEM_LOG_DIRTY_PAGES;\n\n\tnew = old = *memslot;\n\n\tnew.id = mem->slot;\n\tnew.base_gfn = base_gfn;\n\tnew.npages = npages;\n\tnew.flags = mem->flags;\n\n\t/* Disallow changing a memory slot's size. */\n\tr = -EINVAL;\n\tif (npages && old.npages && npages != old.npages)\n\t\tgoto out_free;\n\n\t/* Check for overlaps */\n\tr = -EEXIST;\n\tfor (i = 0; i < KVM_MEMORY_SLOTS; ++i) {\n\t\tstruct kvm_memory_slot *s = &kvm->memslots->memslots[i];\n\n\t\tif (s == memslot || !s->npages)\n\t\t\tcontinue;\n\t\tif (!((base_gfn + npages <= s->base_gfn) ||\n\t\t      (base_gfn >= s->base_gfn + s->npages)))\n\t\t\tgoto out_free;\n\t}\n\n\t/* Free page dirty bitmap if unneeded */\n\tif (!(new.flags & KVM_MEM_LOG_DIRTY_PAGES))\n\t\tnew.dirty_bitmap = NULL;\n\n\tr = -ENOMEM;\n\n\t/* Allocate if a slot is being created */\n#ifndef CONFIG_S390\n\tif (npages && !new.rmap) {\n\t\tnew.rmap = vzalloc(npages * sizeof(*new.rmap));\n\n\t\tif (!new.rmap)\n\t\t\tgoto out_free;\n\n\t\tnew.user_alloc = user_alloc;\n\t\tnew.userspace_addr = mem->userspace_addr;\n\t}\n\tif (!npages)\n\t\tgoto skip_lpage;\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES - 1; ++i) {\n\t\tunsigned long ugfn;\n\t\tunsigned long j;\n\t\tint lpages;\n\t\tint level = i + 2;\n\n\t\t/* Avoid unused variable warning if no large pages */\n\t\t(void)level;\n\n\t\tif (new.lpage_info[i])\n\t\t\tcontinue;\n\n\t\tlpages = 1 + ((base_gfn + npages - 1)\n\t\t\t     >> KVM_HPAGE_GFN_SHIFT(level));\n\t\tlpages -= base_gfn >> KVM_HPAGE_GFN_SHIFT(level);\n\n\t\tnew.lpage_info[i] = vzalloc(lpages * sizeof(*new.lpage_info[i]));\n\n\t\tif (!new.lpage_info[i])\n\t\t\tgoto out_free;\n\n\t\tif (base_gfn & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tnew.lpage_info[i][0].write_count = 1;\n\t\tif ((base_gfn+npages) & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tnew.lpage_info[i][lpages - 1].write_count = 1;\n\t\tugfn = new.userspace_addr >> PAGE_SHIFT;\n\t\t/*\n\t\t * If the gfn and userspace address are not aligned wrt each\n\t\t * other, or if explicitly asked to, disable large page\n\t\t * support for this slot\n\t\t */\n\t\tif ((base_gfn ^ ugfn) & (KVM_PAGES_PER_HPAGE(level) - 1) ||\n\t\t    !largepages_enabled)\n\t\t\tfor (j = 0; j < lpages; ++j)\n\t\t\t\tnew.lpage_info[i][j].write_count = 1;\n\t}\n\nskip_lpage:\n\n\t/* Allocate page dirty bitmap if needed */\n\tif ((new.flags & KVM_MEM_LOG_DIRTY_PAGES) && !new.dirty_bitmap) {\n\t\tif (kvm_create_dirty_bitmap(&new) < 0)\n\t\t\tgoto out_free;\n\t\t/* destroy any largepage mappings for dirty tracking */\n\t}\n#else  /* not defined CONFIG_S390 */\n\tnew.user_alloc = user_alloc;\n\tif (user_alloc)\n\t\tnew.userspace_addr = mem->userspace_addr;\n#endif /* not defined CONFIG_S390 */\n\n\tif (!npages) {\n\t\tr = -ENOMEM;\n\t\tslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\t\tif (!slots)\n\t\t\tgoto out_free;\n\t\tmemcpy(slots, kvm->memslots, sizeof(struct kvm_memslots));\n\t\tif (mem->slot >= slots->nmemslots)\n\t\t\tslots->nmemslots = mem->slot + 1;\n\t\tslots->generation++;\n\t\tslots->memslots[mem->slot].flags |= KVM_MEMSLOT_INVALID;\n\n\t\told_memslots = kvm->memslots;\n\t\trcu_assign_pointer(kvm->memslots, slots);\n\t\tsynchronize_srcu_expedited(&kvm->srcu);\n\t\t/* From this point no new shadow pages pointing to a deleted\n\t\t * memslot will be created.\n\t\t *\n\t\t * validation of sp->gfn happens in:\n\t\t * \t- gfn_to_hva (kvm_read_guest, gfn_to_pfn)\n\t\t * \t- kvm_is_visible_gfn (mmu_check_roots)\n\t\t */\n\t\tkvm_arch_flush_shadow(kvm);\n\t\tkfree(old_memslots);\n\t}\n\n\tr = kvm_arch_prepare_memory_region(kvm, &new, old, mem, user_alloc);\n\tif (r)\n\t\tgoto out_free;\n\n\t/* map the pages in iommu page table */\n\tif (npages) {\n\t\tr = kvm_iommu_map_pages(kvm, &new);\n\t\tif (r)\n\t\t\tgoto out_free;\n\t}\n\n\tr = -ENOMEM;\n\tslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\tif (!slots)\n\t\tgoto out_free;\n\tmemcpy(slots, kvm->memslots, sizeof(struct kvm_memslots));\n\tif (mem->slot >= slots->nmemslots)\n\t\tslots->nmemslots = mem->slot + 1;\n\tslots->generation++;\n\n\t/* actual memory is freed via old in kvm_free_physmem_slot below */\n\tif (!npages) {\n\t\tnew.rmap = NULL;\n\t\tnew.dirty_bitmap = NULL;\n\t\tfor (i = 0; i < KVM_NR_PAGE_SIZES - 1; ++i)\n\t\t\tnew.lpage_info[i] = NULL;\n\t}\n\n\tslots->memslots[mem->slot] = new;\n\told_memslots = kvm->memslots;\n\trcu_assign_pointer(kvm->memslots, slots);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\n\tkvm_arch_commit_memory_region(kvm, mem, old, user_alloc);\n\n\tkvm_free_physmem_slot(&old, &new);\n\tkfree(old_memslots);\n\n\treturn 0;\n\nout_free:\n\tkvm_free_physmem_slot(&new, &old);\nout:\n\treturn r;\n\n}",
        "code_after_change": "int __kvm_set_memory_region(struct kvm *kvm,\n\t\t\t    struct kvm_userspace_memory_region *mem,\n\t\t\t    int user_alloc)\n{\n\tint r;\n\tgfn_t base_gfn;\n\tunsigned long npages;\n\tunsigned long i;\n\tstruct kvm_memory_slot *memslot;\n\tstruct kvm_memory_slot old, new;\n\tstruct kvm_memslots *slots, *old_memslots;\n\n\tr = -EINVAL;\n\t/* General sanity checks */\n\tif (mem->memory_size & (PAGE_SIZE - 1))\n\t\tgoto out;\n\tif (mem->guest_phys_addr & (PAGE_SIZE - 1))\n\t\tgoto out;\n\t/* We can read the guest memory with __xxx_user() later on. */\n\tif (user_alloc &&\n\t    ((mem->userspace_addr & (PAGE_SIZE - 1)) ||\n\t     !access_ok(VERIFY_WRITE, mem->userspace_addr, mem->memory_size)))\n\t\tgoto out;\n\tif (mem->slot >= KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)\n\t\tgoto out;\n\tif (mem->guest_phys_addr + mem->memory_size < mem->guest_phys_addr)\n\t\tgoto out;\n\n\tmemslot = &kvm->memslots->memslots[mem->slot];\n\tbase_gfn = mem->guest_phys_addr >> PAGE_SHIFT;\n\tnpages = mem->memory_size >> PAGE_SHIFT;\n\n\tr = -EINVAL;\n\tif (npages > KVM_MEM_MAX_NR_PAGES)\n\t\tgoto out;\n\n\tif (!npages)\n\t\tmem->flags &= ~KVM_MEM_LOG_DIRTY_PAGES;\n\n\tnew = old = *memslot;\n\n\tnew.id = mem->slot;\n\tnew.base_gfn = base_gfn;\n\tnew.npages = npages;\n\tnew.flags = mem->flags;\n\n\t/* Disallow changing a memory slot's size. */\n\tr = -EINVAL;\n\tif (npages && old.npages && npages != old.npages)\n\t\tgoto out_free;\n\n\t/* Check for overlaps */\n\tr = -EEXIST;\n\tfor (i = 0; i < KVM_MEMORY_SLOTS; ++i) {\n\t\tstruct kvm_memory_slot *s = &kvm->memslots->memslots[i];\n\n\t\tif (s == memslot || !s->npages)\n\t\t\tcontinue;\n\t\tif (!((base_gfn + npages <= s->base_gfn) ||\n\t\t      (base_gfn >= s->base_gfn + s->npages)))\n\t\t\tgoto out_free;\n\t}\n\n\t/* Free page dirty bitmap if unneeded */\n\tif (!(new.flags & KVM_MEM_LOG_DIRTY_PAGES))\n\t\tnew.dirty_bitmap = NULL;\n\n\tr = -ENOMEM;\n\n\t/* Allocate if a slot is being created */\n#ifndef CONFIG_S390\n\tif (npages && !new.rmap) {\n\t\tnew.rmap = vzalloc(npages * sizeof(*new.rmap));\n\n\t\tif (!new.rmap)\n\t\t\tgoto out_free;\n\n\t\tnew.user_alloc = user_alloc;\n\t\tnew.userspace_addr = mem->userspace_addr;\n\t}\n\tif (!npages)\n\t\tgoto skip_lpage;\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES - 1; ++i) {\n\t\tunsigned long ugfn;\n\t\tunsigned long j;\n\t\tint lpages;\n\t\tint level = i + 2;\n\n\t\t/* Avoid unused variable warning if no large pages */\n\t\t(void)level;\n\n\t\tif (new.lpage_info[i])\n\t\t\tcontinue;\n\n\t\tlpages = 1 + ((base_gfn + npages - 1)\n\t\t\t     >> KVM_HPAGE_GFN_SHIFT(level));\n\t\tlpages -= base_gfn >> KVM_HPAGE_GFN_SHIFT(level);\n\n\t\tnew.lpage_info[i] = vzalloc(lpages * sizeof(*new.lpage_info[i]));\n\n\t\tif (!new.lpage_info[i])\n\t\t\tgoto out_free;\n\n\t\tif (base_gfn & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tnew.lpage_info[i][0].write_count = 1;\n\t\tif ((base_gfn+npages) & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tnew.lpage_info[i][lpages - 1].write_count = 1;\n\t\tugfn = new.userspace_addr >> PAGE_SHIFT;\n\t\t/*\n\t\t * If the gfn and userspace address are not aligned wrt each\n\t\t * other, or if explicitly asked to, disable large page\n\t\t * support for this slot\n\t\t */\n\t\tif ((base_gfn ^ ugfn) & (KVM_PAGES_PER_HPAGE(level) - 1) ||\n\t\t    !largepages_enabled)\n\t\t\tfor (j = 0; j < lpages; ++j)\n\t\t\t\tnew.lpage_info[i][j].write_count = 1;\n\t}\n\nskip_lpage:\n\n\t/* Allocate page dirty bitmap if needed */\n\tif ((new.flags & KVM_MEM_LOG_DIRTY_PAGES) && !new.dirty_bitmap) {\n\t\tif (kvm_create_dirty_bitmap(&new) < 0)\n\t\t\tgoto out_free;\n\t\t/* destroy any largepage mappings for dirty tracking */\n\t}\n#else  /* not defined CONFIG_S390 */\n\tnew.user_alloc = user_alloc;\n\tif (user_alloc)\n\t\tnew.userspace_addr = mem->userspace_addr;\n#endif /* not defined CONFIG_S390 */\n\n\tif (!npages) {\n\t\tr = -ENOMEM;\n\t\tslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\t\tif (!slots)\n\t\t\tgoto out_free;\n\t\tmemcpy(slots, kvm->memslots, sizeof(struct kvm_memslots));\n\t\tif (mem->slot >= slots->nmemslots)\n\t\t\tslots->nmemslots = mem->slot + 1;\n\t\tslots->generation++;\n\t\tslots->memslots[mem->slot].flags |= KVM_MEMSLOT_INVALID;\n\n\t\told_memslots = kvm->memslots;\n\t\trcu_assign_pointer(kvm->memslots, slots);\n\t\tsynchronize_srcu_expedited(&kvm->srcu);\n\t\t/* From this point no new shadow pages pointing to a deleted\n\t\t * memslot will be created.\n\t\t *\n\t\t * validation of sp->gfn happens in:\n\t\t * \t- gfn_to_hva (kvm_read_guest, gfn_to_pfn)\n\t\t * \t- kvm_is_visible_gfn (mmu_check_roots)\n\t\t */\n\t\tkvm_arch_flush_shadow(kvm);\n\t\tkfree(old_memslots);\n\t}\n\n\tr = kvm_arch_prepare_memory_region(kvm, &new, old, mem, user_alloc);\n\tif (r)\n\t\tgoto out_free;\n\n\t/* map the pages in iommu page table */\n\tif (npages) {\n\t\tr = kvm_iommu_map_pages(kvm, &new);\n\t\tif (r)\n\t\t\tgoto out_free;\n\t}\n\n\tr = -ENOMEM;\n\tslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);\n\tif (!slots)\n\t\tgoto out_free;\n\tmemcpy(slots, kvm->memslots, sizeof(struct kvm_memslots));\n\tif (mem->slot >= slots->nmemslots)\n\t\tslots->nmemslots = mem->slot + 1;\n\tslots->generation++;\n\n\t/* actual memory is freed via old in kvm_free_physmem_slot below */\n\tif (!npages) {\n\t\tnew.rmap = NULL;\n\t\tnew.dirty_bitmap = NULL;\n\t\tfor (i = 0; i < KVM_NR_PAGE_SIZES - 1; ++i)\n\t\t\tnew.lpage_info[i] = NULL;\n\t}\n\n\tslots->memslots[mem->slot] = new;\n\told_memslots = kvm->memslots;\n\trcu_assign_pointer(kvm->memslots, slots);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\n\tkvm_arch_commit_memory_region(kvm, mem, old, user_alloc);\n\n\tkvm_free_physmem_slot(&old, &new);\n\tkfree(old_memslots);\n\n\treturn 0;\n\nout_free:\n\tkvm_free_physmem_slot(&new, &old);\nout:\n\treturn r;\n\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,7 +16,10 @@\n \t\tgoto out;\n \tif (mem->guest_phys_addr & (PAGE_SIZE - 1))\n \t\tgoto out;\n-\tif (user_alloc && (mem->userspace_addr & (PAGE_SIZE - 1)))\n+\t/* We can read the guest memory with __xxx_user() later on. */\n+\tif (user_alloc &&\n+\t    ((mem->userspace_addr & (PAGE_SIZE - 1)) ||\n+\t     !access_ok(VERIFY_WRITE, mem->userspace_addr, mem->memory_size)))\n \t\tgoto out;\n \tif (mem->slot >= KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)\n \t\tgoto out;",
        "function_modified_lines": {
            "added": [
                "\t/* We can read the guest memory with __xxx_user() later on. */",
                "\tif (user_alloc &&",
                "\t    ((mem->userspace_addr & (PAGE_SIZE - 1)) ||",
                "\t     !access_ok(VERIFY_WRITE, mem->userspace_addr, mem->memory_size)))"
            ],
            "deleted": [
                "\tif (user_alloc && (mem->userspace_addr & (PAGE_SIZE - 1)))"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The KVM subsystem in the Linux kernel before 3.0 does not check whether kernel addresses are specified during allocation of memory slots for use in a guest's physical address space, which allows local users to gain privileges or obtain sensitive information from kernel memory via a crafted application, related to arch/x86/kvm/paging_tmpl.h and virt/kvm/kvm_main.c.",
        "id": 208
    },
    {
        "cve_id": "CVE-2017-6345",
        "code_before_change": "void llc_conn_handler(struct llc_sap *sap, struct sk_buff *skb)\n{\n\tstruct llc_addr saddr, daddr;\n\tstruct sock *sk;\n\n\tllc_pdu_decode_sa(skb, saddr.mac);\n\tllc_pdu_decode_ssap(skb, &saddr.lsap);\n\tllc_pdu_decode_da(skb, daddr.mac);\n\tllc_pdu_decode_dsap(skb, &daddr.lsap);\n\n\tsk = __llc_lookup(sap, &saddr, &daddr);\n\tif (!sk)\n\t\tgoto drop;\n\n\tbh_lock_sock(sk);\n\t/*\n\t * This has to be done here and not at the upper layer ->accept\n\t * method because of the way the PROCOM state machine works:\n\t * it needs to set several state variables (see, for instance,\n\t * llc_adm_actions_2 in net/llc/llc_c_st.c) and send a packet to\n\t * the originator of the new connection, and this state has to be\n\t * in the newly created struct sock private area. -acme\n\t */\n\tif (unlikely(sk->sk_state == TCP_LISTEN)) {\n\t\tstruct sock *newsk = llc_create_incoming_sock(sk, skb->dev,\n\t\t\t\t\t\t\t      &saddr, &daddr);\n\t\tif (!newsk)\n\t\t\tgoto drop_unlock;\n\t\tskb_set_owner_r(skb, newsk);\n\t} else {\n\t\t/*\n\t\t * Can't be skb_set_owner_r, this will be done at the\n\t\t * llc_conn_state_process function, later on, when we will use\n\t\t * skb_queue_rcv_skb to send it to upper layers, this is\n\t\t * another trick required to cope with how the PROCOM state\n\t\t * machine works. -acme\n\t\t */\n\t\tskb->sk = sk;\n\t}\n\tif (!sock_owned_by_user(sk))\n\t\tllc_conn_rcv(sk, skb);\n\telse {\n\t\tdprintk(\"%s: adding to backlog...\\n\", __func__);\n\t\tllc_set_backlog_type(skb, LLC_PACKET);\n\t\tif (sk_add_backlog(sk, skb, sk->sk_rcvbuf))\n\t\t\tgoto drop_unlock;\n\t}\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n\treturn;\ndrop:\n\tkfree_skb(skb);\n\treturn;\ndrop_unlock:\n\tkfree_skb(skb);\n\tgoto out;\n}",
        "code_after_change": "void llc_conn_handler(struct llc_sap *sap, struct sk_buff *skb)\n{\n\tstruct llc_addr saddr, daddr;\n\tstruct sock *sk;\n\n\tllc_pdu_decode_sa(skb, saddr.mac);\n\tllc_pdu_decode_ssap(skb, &saddr.lsap);\n\tllc_pdu_decode_da(skb, daddr.mac);\n\tllc_pdu_decode_dsap(skb, &daddr.lsap);\n\n\tsk = __llc_lookup(sap, &saddr, &daddr);\n\tif (!sk)\n\t\tgoto drop;\n\n\tbh_lock_sock(sk);\n\t/*\n\t * This has to be done here and not at the upper layer ->accept\n\t * method because of the way the PROCOM state machine works:\n\t * it needs to set several state variables (see, for instance,\n\t * llc_adm_actions_2 in net/llc/llc_c_st.c) and send a packet to\n\t * the originator of the new connection, and this state has to be\n\t * in the newly created struct sock private area. -acme\n\t */\n\tif (unlikely(sk->sk_state == TCP_LISTEN)) {\n\t\tstruct sock *newsk = llc_create_incoming_sock(sk, skb->dev,\n\t\t\t\t\t\t\t      &saddr, &daddr);\n\t\tif (!newsk)\n\t\t\tgoto drop_unlock;\n\t\tskb_set_owner_r(skb, newsk);\n\t} else {\n\t\t/*\n\t\t * Can't be skb_set_owner_r, this will be done at the\n\t\t * llc_conn_state_process function, later on, when we will use\n\t\t * skb_queue_rcv_skb to send it to upper layers, this is\n\t\t * another trick required to cope with how the PROCOM state\n\t\t * machine works. -acme\n\t\t */\n\t\tskb_orphan(skb);\n\t\tsock_hold(sk);\n\t\tskb->sk = sk;\n\t\tskb->destructor = sock_efree;\n\t}\n\tif (!sock_owned_by_user(sk))\n\t\tllc_conn_rcv(sk, skb);\n\telse {\n\t\tdprintk(\"%s: adding to backlog...\\n\", __func__);\n\t\tllc_set_backlog_type(skb, LLC_PACKET);\n\t\tif (sk_add_backlog(sk, skb, sk->sk_rcvbuf))\n\t\t\tgoto drop_unlock;\n\t}\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n\treturn;\ndrop:\n\tkfree_skb(skb);\n\treturn;\ndrop_unlock:\n\tkfree_skb(skb);\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -35,7 +35,10 @@\n \t\t * another trick required to cope with how the PROCOM state\n \t\t * machine works. -acme\n \t\t */\n+\t\tskb_orphan(skb);\n+\t\tsock_hold(sk);\n \t\tskb->sk = sk;\n+\t\tskb->destructor = sock_efree;\n \t}\n \tif (!sock_owned_by_user(sk))\n \t\tllc_conn_rcv(sk, skb);",
        "function_modified_lines": {
            "added": [
                "\t\tskb_orphan(skb);",
                "\t\tsock_hold(sk);",
                "\t\tskb->destructor = sock_efree;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The LLC subsystem in the Linux kernel before 4.9.13 does not ensure that a certain destructor exists in required circumstances, which allows local users to cause a denial of service (BUG_ON) or possibly have unspecified other impact via crafted system calls.",
        "id": 1480
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "int bt_sock_stream_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\tsize_t target, copied = 0;\n\tlong timeo;\n\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tmsg->msg_namelen = 0;\n\n\tBT_DBG(\"sk %p size %zu\", sk, size);\n\n\tlock_sock(sk);\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo  = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\tdo {\n\t\tstruct sk_buff *skb;\n\t\tint chunk;\n\n\t\tskb = skb_dequeue(&sk->sk_receive_queue);\n\t\tif (!skb) {\n\t\t\tif (copied >= target)\n\t\t\t\tbreak;\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\n\t\t\ttimeo = bt_sock_data_wait(sk, timeo);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, skb->len, size);\n\t\tif (skb_copy_datagram_iovec(skb, 0, msg->msg_iov, chunk)) {\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tif (!copied)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize   -= chunk;\n\n\t\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tint skb_len = skb_headlen(skb);\n\n\t\t\tif (chunk <= skb_len) {\n\t\t\t\t__skb_pull(skb, chunk);\n\t\t\t} else {\n\t\t\t\tstruct sk_buff *frag;\n\n\t\t\t\t__skb_pull(skb, skb_len);\n\t\t\t\tchunk -= skb_len;\n\n\t\t\t\tskb_walk_frags(skb, frag) {\n\t\t\t\t\tif (chunk <= frag->len) {\n\t\t\t\t\t\t/* Pulling partial data */\n\t\t\t\t\t\tskb->len -= chunk;\n\t\t\t\t\t\tskb->data_len -= chunk;\n\t\t\t\t\t\t__skb_pull(frag, chunk);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t} else if (frag->len) {\n\t\t\t\t\t\t/* Pulling all frag data */\n\t\t\t\t\t\tchunk -= frag->len;\n\t\t\t\t\t\tskb->len -= frag->len;\n\t\t\t\t\t\tskb->data_len -= frag->len;\n\t\t\t\t\t\t__skb_pull(frag, frag->len);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (skb->len) {\n\t\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tkfree_skb(skb);\n\n\t\t} else {\n\t\t\t/* put message back and return */\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\nout:\n\trelease_sock(sk);\n\treturn copied ? : err;\n}",
        "code_after_change": "int bt_sock_stream_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\tsize_t target, copied = 0;\n\tlong timeo;\n\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tBT_DBG(\"sk %p size %zu\", sk, size);\n\n\tlock_sock(sk);\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo  = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\tdo {\n\t\tstruct sk_buff *skb;\n\t\tint chunk;\n\n\t\tskb = skb_dequeue(&sk->sk_receive_queue);\n\t\tif (!skb) {\n\t\t\tif (copied >= target)\n\t\t\t\tbreak;\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\n\t\t\ttimeo = bt_sock_data_wait(sk, timeo);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, skb->len, size);\n\t\tif (skb_copy_datagram_iovec(skb, 0, msg->msg_iov, chunk)) {\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tif (!copied)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize   -= chunk;\n\n\t\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tint skb_len = skb_headlen(skb);\n\n\t\t\tif (chunk <= skb_len) {\n\t\t\t\t__skb_pull(skb, chunk);\n\t\t\t} else {\n\t\t\t\tstruct sk_buff *frag;\n\n\t\t\t\t__skb_pull(skb, skb_len);\n\t\t\t\tchunk -= skb_len;\n\n\t\t\t\tskb_walk_frags(skb, frag) {\n\t\t\t\t\tif (chunk <= frag->len) {\n\t\t\t\t\t\t/* Pulling partial data */\n\t\t\t\t\t\tskb->len -= chunk;\n\t\t\t\t\t\tskb->data_len -= chunk;\n\t\t\t\t\t\t__skb_pull(frag, chunk);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t} else if (frag->len) {\n\t\t\t\t\t\t/* Pulling all frag data */\n\t\t\t\t\t\tchunk -= frag->len;\n\t\t\t\t\t\tskb->len -= frag->len;\n\t\t\t\t\t\tskb->data_len -= frag->len;\n\t\t\t\t\t\t__skb_pull(frag, frag->len);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (skb->len) {\n\t\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tkfree_skb(skb);\n\n\t\t} else {\n\t\t\t/* put message back and return */\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\nout:\n\trelease_sock(sk);\n\treturn copied ? : err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,6 @@\n \n \tif (flags & MSG_OOB)\n \t\treturn -EOPNOTSUPP;\n-\n-\tmsg->msg_namelen = 0;\n \n \tBT_DBG(\"sk %p size %zu\", sk, size);\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 374
    },
    {
        "cve_id": "CVE-2013-1763",
        "code_before_change": "static int __sock_diag_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tint err;\n\tstruct sock_diag_req *req = nlmsg_data(nlh);\n\tconst struct sock_diag_handler *hndl;\n\n\tif (nlmsg_len(nlh) < sizeof(*req))\n\t\treturn -EINVAL;\n\n\thndl = sock_diag_lock_handler(req->sdiag_family);\n\tif (hndl == NULL)\n\t\terr = -ENOENT;\n\telse\n\t\terr = hndl->dump(skb, nlh);\n\tsock_diag_unlock_handler(hndl);\n\n\treturn err;\n}",
        "code_after_change": "static int __sock_diag_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tint err;\n\tstruct sock_diag_req *req = nlmsg_data(nlh);\n\tconst struct sock_diag_handler *hndl;\n\n\tif (nlmsg_len(nlh) < sizeof(*req))\n\t\treturn -EINVAL;\n\n\tif (req->sdiag_family >= AF_MAX)\n\t\treturn -EINVAL;\n\n\thndl = sock_diag_lock_handler(req->sdiag_family);\n\tif (hndl == NULL)\n\t\terr = -ENOENT;\n\telse\n\t\terr = hndl->dump(skb, nlh);\n\tsock_diag_unlock_handler(hndl);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,9 @@\n \tconst struct sock_diag_handler *hndl;\n \n \tif (nlmsg_len(nlh) < sizeof(*req))\n+\t\treturn -EINVAL;\n+\n+\tif (req->sdiag_family >= AF_MAX)\n \t\treturn -EINVAL;\n \n \thndl = sock_diag_lock_handler(req->sdiag_family);",
        "function_modified_lines": {
            "added": [
                "\t\treturn -EINVAL;",
                "",
                "\tif (req->sdiag_family >= AF_MAX)"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Array index error in the __sock_diag_rcv_msg function in net/core/sock_diag.c in the Linux kernel before 3.7.10 allows local users to gain privileges via a large family value in a Netlink message.",
        "id": 168
    },
    {
        "cve_id": "CVE-2013-1828",
        "code_before_change": "static int sctp_getsockopt_assoc_stats(struct sock *sk, int len,\n\t\t\t\t       char __user *optval,\n\t\t\t\t       int __user *optlen)\n{\n\tstruct sctp_assoc_stats sas;\n\tstruct sctp_association *asoc = NULL;\n\n\t/* User must provide at least the assoc id */\n\tif (len < sizeof(sctp_assoc_t))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&sas, optval, len))\n\t\treturn -EFAULT;\n\n\tasoc = sctp_id2assoc(sk, sas.sas_assoc_id);\n\tif (!asoc)\n\t\treturn -EINVAL;\n\n\tsas.sas_rtxchunks = asoc->stats.rtxchunks;\n\tsas.sas_gapcnt = asoc->stats.gapcnt;\n\tsas.sas_outofseqtsns = asoc->stats.outofseqtsns;\n\tsas.sas_osacks = asoc->stats.osacks;\n\tsas.sas_isacks = asoc->stats.isacks;\n\tsas.sas_octrlchunks = asoc->stats.octrlchunks;\n\tsas.sas_ictrlchunks = asoc->stats.ictrlchunks;\n\tsas.sas_oodchunks = asoc->stats.oodchunks;\n\tsas.sas_iodchunks = asoc->stats.iodchunks;\n\tsas.sas_ouodchunks = asoc->stats.ouodchunks;\n\tsas.sas_iuodchunks = asoc->stats.iuodchunks;\n\tsas.sas_idupchunks = asoc->stats.idupchunks;\n\tsas.sas_opackets = asoc->stats.opackets;\n\tsas.sas_ipackets = asoc->stats.ipackets;\n\n\t/* New high max rto observed, will return 0 if not a single\n\t * RTO update took place. obs_rto_ipaddr will be bogus\n\t * in such a case\n\t */\n\tsas.sas_maxrto = asoc->stats.max_obs_rto;\n\tmemcpy(&sas.sas_obs_rto_ipaddr, &asoc->stats.obs_rto_ipaddr,\n\t\tsizeof(struct sockaddr_storage));\n\n\t/* Mark beginning of a new observation period */\n\tasoc->stats.max_obs_rto = asoc->rto_min;\n\n\t/* Allow the struct to grow and fill in as much as possible */\n\tlen = min_t(size_t, len, sizeof(sas));\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tSCTP_DEBUG_PRINTK(\"sctp_getsockopt_assoc_stat(%d): %d\\n\",\n\t\t\t  len, sas.sas_assoc_id);\n\n\tif (copy_to_user(optval, &sas, len))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
        "code_after_change": "static int sctp_getsockopt_assoc_stats(struct sock *sk, int len,\n\t\t\t\t       char __user *optval,\n\t\t\t\t       int __user *optlen)\n{\n\tstruct sctp_assoc_stats sas;\n\tstruct sctp_association *asoc = NULL;\n\n\t/* User must provide at least the assoc id */\n\tif (len < sizeof(sctp_assoc_t))\n\t\treturn -EINVAL;\n\n\t/* Allow the struct to grow and fill in as much as possible */\n\tlen = min_t(size_t, len, sizeof(sas));\n\n\tif (copy_from_user(&sas, optval, len))\n\t\treturn -EFAULT;\n\n\tasoc = sctp_id2assoc(sk, sas.sas_assoc_id);\n\tif (!asoc)\n\t\treturn -EINVAL;\n\n\tsas.sas_rtxchunks = asoc->stats.rtxchunks;\n\tsas.sas_gapcnt = asoc->stats.gapcnt;\n\tsas.sas_outofseqtsns = asoc->stats.outofseqtsns;\n\tsas.sas_osacks = asoc->stats.osacks;\n\tsas.sas_isacks = asoc->stats.isacks;\n\tsas.sas_octrlchunks = asoc->stats.octrlchunks;\n\tsas.sas_ictrlchunks = asoc->stats.ictrlchunks;\n\tsas.sas_oodchunks = asoc->stats.oodchunks;\n\tsas.sas_iodchunks = asoc->stats.iodchunks;\n\tsas.sas_ouodchunks = asoc->stats.ouodchunks;\n\tsas.sas_iuodchunks = asoc->stats.iuodchunks;\n\tsas.sas_idupchunks = asoc->stats.idupchunks;\n\tsas.sas_opackets = asoc->stats.opackets;\n\tsas.sas_ipackets = asoc->stats.ipackets;\n\n\t/* New high max rto observed, will return 0 if not a single\n\t * RTO update took place. obs_rto_ipaddr will be bogus\n\t * in such a case\n\t */\n\tsas.sas_maxrto = asoc->stats.max_obs_rto;\n\tmemcpy(&sas.sas_obs_rto_ipaddr, &asoc->stats.obs_rto_ipaddr,\n\t\tsizeof(struct sockaddr_storage));\n\n\t/* Mark beginning of a new observation period */\n\tasoc->stats.max_obs_rto = asoc->rto_min;\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tSCTP_DEBUG_PRINTK(\"sctp_getsockopt_assoc_stat(%d): %d\\n\",\n\t\t\t  len, sas.sas_assoc_id);\n\n\tif (copy_to_user(optval, &sas, len))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,9 @@\n \t/* User must provide at least the assoc id */\n \tif (len < sizeof(sctp_assoc_t))\n \t\treturn -EINVAL;\n+\n+\t/* Allow the struct to grow and fill in as much as possible */\n+\tlen = min_t(size_t, len, sizeof(sas));\n \n \tif (copy_from_user(&sas, optval, len))\n \t\treturn -EFAULT;\n@@ -42,9 +45,6 @@\n \t/* Mark beginning of a new observation period */\n \tasoc->stats.max_obs_rto = asoc->rto_min;\n \n-\t/* Allow the struct to grow and fill in as much as possible */\n-\tlen = min_t(size_t, len, sizeof(sas));\n-\n \tif (put_user(len, optlen))\n \t\treturn -EFAULT;\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* Allow the struct to grow and fill in as much as possible */",
                "\tlen = min_t(size_t, len, sizeof(sas));"
            ],
            "deleted": [
                "\t/* Allow the struct to grow and fill in as much as possible */",
                "\tlen = min_t(size_t, len, sizeof(sas));",
                ""
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The sctp_getsockopt_assoc_stats function in net/sctp/socket.c in the Linux kernel before 3.8.4 does not validate a size value before proceeding to a copy_from_user operation, which allows local users to gain privileges via a crafted application that contains an SCTP_GET_ASSOC_STATS getsockopt system call.",
        "id": 195
    },
    {
        "cve_id": "CVE-2017-15868",
        "code_before_change": "int bnep_add_connection(struct bnep_connadd_req *req, struct socket *sock)\n{\n\tstruct net_device *dev;\n\tstruct bnep_session *s, *ss;\n\tu8 dst[ETH_ALEN], src[ETH_ALEN];\n\tint err;\n\n\tBT_DBG(\"\");\n\n\tbaswap((void *) dst, &l2cap_pi(sock->sk)->chan->dst);\n\tbaswap((void *) src, &l2cap_pi(sock->sk)->chan->src);\n\n\t/* session struct allocated as private part of net_device */\n\tdev = alloc_netdev(sizeof(struct bnep_session),\n\t\t\t   (*req->device) ? req->device : \"bnep%d\",\n\t\t\t   NET_NAME_UNKNOWN,\n\t\t\t   bnep_net_setup);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdown_write(&bnep_session_sem);\n\n\tss = __bnep_get_session(dst);\n\tif (ss && ss->state == BT_CONNECTED) {\n\t\terr = -EEXIST;\n\t\tgoto failed;\n\t}\n\n\ts = netdev_priv(dev);\n\n\t/* This is rx header therefore addresses are swapped.\n\t * ie. eh.h_dest is our local address. */\n\tmemcpy(s->eh.h_dest,   &src, ETH_ALEN);\n\tmemcpy(s->eh.h_source, &dst, ETH_ALEN);\n\tmemcpy(dev->dev_addr, s->eh.h_dest, ETH_ALEN);\n\n\ts->dev   = dev;\n\ts->sock  = sock;\n\ts->role  = req->role;\n\ts->state = BT_CONNECTED;\n\n\ts->msg.msg_flags = MSG_NOSIGNAL;\n\n#ifdef CONFIG_BT_BNEP_MC_FILTER\n\t/* Set default mc filter */\n\tset_bit(bnep_mc_hash(dev->broadcast), (ulong *) &s->mc_filter);\n#endif\n\n#ifdef CONFIG_BT_BNEP_PROTO_FILTER\n\t/* Set default protocol filter */\n\tbnep_set_default_proto_filter(s);\n#endif\n\n\tSET_NETDEV_DEV(dev, bnep_get_device(s));\n\tSET_NETDEV_DEVTYPE(dev, &bnep_type);\n\n\terr = register_netdev(dev);\n\tif (err)\n\t\tgoto failed;\n\n\t__bnep_link_session(s);\n\n\t__module_get(THIS_MODULE);\n\ts->task = kthread_run(bnep_session, s, \"kbnepd %s\", dev->name);\n\tif (IS_ERR(s->task)) {\n\t\t/* Session thread start failed, gotta cleanup. */\n\t\tmodule_put(THIS_MODULE);\n\t\tunregister_netdev(dev);\n\t\t__bnep_unlink_session(s);\n\t\terr = PTR_ERR(s->task);\n\t\tgoto failed;\n\t}\n\n\tup_write(&bnep_session_sem);\n\tstrcpy(req->device, dev->name);\n\treturn 0;\n\nfailed:\n\tup_write(&bnep_session_sem);\n\tfree_netdev(dev);\n\treturn err;\n}",
        "code_after_change": "int bnep_add_connection(struct bnep_connadd_req *req, struct socket *sock)\n{\n\tstruct net_device *dev;\n\tstruct bnep_session *s, *ss;\n\tu8 dst[ETH_ALEN], src[ETH_ALEN];\n\tint err;\n\n\tBT_DBG(\"\");\n\n\tif (!l2cap_is_socket(sock))\n\t\treturn -EBADFD;\n\n\tbaswap((void *) dst, &l2cap_pi(sock->sk)->chan->dst);\n\tbaswap((void *) src, &l2cap_pi(sock->sk)->chan->src);\n\n\t/* session struct allocated as private part of net_device */\n\tdev = alloc_netdev(sizeof(struct bnep_session),\n\t\t\t   (*req->device) ? req->device : \"bnep%d\",\n\t\t\t   NET_NAME_UNKNOWN,\n\t\t\t   bnep_net_setup);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdown_write(&bnep_session_sem);\n\n\tss = __bnep_get_session(dst);\n\tif (ss && ss->state == BT_CONNECTED) {\n\t\terr = -EEXIST;\n\t\tgoto failed;\n\t}\n\n\ts = netdev_priv(dev);\n\n\t/* This is rx header therefore addresses are swapped.\n\t * ie. eh.h_dest is our local address. */\n\tmemcpy(s->eh.h_dest,   &src, ETH_ALEN);\n\tmemcpy(s->eh.h_source, &dst, ETH_ALEN);\n\tmemcpy(dev->dev_addr, s->eh.h_dest, ETH_ALEN);\n\n\ts->dev   = dev;\n\ts->sock  = sock;\n\ts->role  = req->role;\n\ts->state = BT_CONNECTED;\n\n\ts->msg.msg_flags = MSG_NOSIGNAL;\n\n#ifdef CONFIG_BT_BNEP_MC_FILTER\n\t/* Set default mc filter */\n\tset_bit(bnep_mc_hash(dev->broadcast), (ulong *) &s->mc_filter);\n#endif\n\n#ifdef CONFIG_BT_BNEP_PROTO_FILTER\n\t/* Set default protocol filter */\n\tbnep_set_default_proto_filter(s);\n#endif\n\n\tSET_NETDEV_DEV(dev, bnep_get_device(s));\n\tSET_NETDEV_DEVTYPE(dev, &bnep_type);\n\n\terr = register_netdev(dev);\n\tif (err)\n\t\tgoto failed;\n\n\t__bnep_link_session(s);\n\n\t__module_get(THIS_MODULE);\n\ts->task = kthread_run(bnep_session, s, \"kbnepd %s\", dev->name);\n\tif (IS_ERR(s->task)) {\n\t\t/* Session thread start failed, gotta cleanup. */\n\t\tmodule_put(THIS_MODULE);\n\t\tunregister_netdev(dev);\n\t\t__bnep_unlink_session(s);\n\t\terr = PTR_ERR(s->task);\n\t\tgoto failed;\n\t}\n\n\tup_write(&bnep_session_sem);\n\tstrcpy(req->device, dev->name);\n\treturn 0;\n\nfailed:\n\tup_write(&bnep_session_sem);\n\tfree_netdev(dev);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,9 @@\n \tint err;\n \n \tBT_DBG(\"\");\n+\n+\tif (!l2cap_is_socket(sock))\n+\t\treturn -EBADFD;\n \n \tbaswap((void *) dst, &l2cap_pi(sock->sk)->chan->dst);\n \tbaswap((void *) src, &l2cap_pi(sock->sk)->chan->src);",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (!l2cap_is_socket(sock))",
                "\t\treturn -EBADFD;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The bnep_add_connection function in net/bluetooth/bnep/core.c in the Linux kernel before 3.19 does not ensure that an l2cap socket is available, which allows local users to gain privileges via a crafted application.",
        "id": 1308
    },
    {
        "cve_id": "CVE-2018-14641",
        "code_before_change": "static bool\nnf_ct_frag6_reasm(struct frag_queue *fq, struct sk_buff *prev,  struct net_device *dev)\n{\n\tstruct sk_buff *fp, *head = fq->q.fragments;\n\tint    payload_len;\n\tu8 ecn;\n\n\tinet_frag_kill(&fq->q);\n\n\tWARN_ON(head == NULL);\n\tWARN_ON(head->ip_defrag_offset != 0);\n\n\tecn = ip_frag_ecn_table[fq->ecn];\n\tif (unlikely(ecn == 0xff))\n\t\treturn false;\n\n\t/* Unfragmented part is taken from the first segment. */\n\tpayload_len = ((head->data - skb_network_header(head)) -\n\t\t       sizeof(struct ipv6hdr) + fq->q.len -\n\t\t       sizeof(struct frag_hdr));\n\tif (payload_len > IPV6_MAXPLEN) {\n\t\tnet_dbg_ratelimited(\"nf_ct_frag6_reasm: payload len = %d\\n\",\n\t\t\t\t    payload_len);\n\t\treturn false;\n\t}\n\n\t/* Head of list must not be cloned. */\n\tif (skb_unclone(head, GFP_ATOMIC))\n\t\treturn false;\n\n\t/* If the first fragment is fragmented itself, we split\n\t * it to two chunks: the first with data and paged part\n\t * and the second, holding only fragments. */\n\tif (skb_has_frag_list(head)) {\n\t\tstruct sk_buff *clone;\n\t\tint i, plen = 0;\n\n\t\tclone = alloc_skb(0, GFP_ATOMIC);\n\t\tif (clone == NULL)\n\t\t\treturn false;\n\n\t\tclone->next = head->next;\n\t\thead->next = clone;\n\t\tskb_shinfo(clone)->frag_list = skb_shinfo(head)->frag_list;\n\t\tskb_frag_list_init(head);\n\t\tfor (i = 0; i < skb_shinfo(head)->nr_frags; i++)\n\t\t\tplen += skb_frag_size(&skb_shinfo(head)->frags[i]);\n\t\tclone->len = clone->data_len = head->data_len - plen;\n\t\thead->data_len -= clone->len;\n\t\thead->len -= clone->len;\n\t\tclone->csum = 0;\n\t\tclone->ip_summed = head->ip_summed;\n\n\t\tadd_frag_mem_limit(fq->q.net, clone->truesize);\n\t}\n\n\t/* morph head into last received skb: prev.\n\t *\n\t * This allows callers of ipv6 conntrack defrag to continue\n\t * to use the last skb(frag) passed into the reasm engine.\n\t * The last skb frag 'silently' turns into the full reassembled skb.\n\t *\n\t * Since prev is also part of q->fragments we have to clone it first.\n\t */\n\tif (head != prev) {\n\t\tstruct sk_buff *iter;\n\n\t\tfp = skb_clone(prev, GFP_ATOMIC);\n\t\tif (!fp)\n\t\t\treturn false;\n\n\t\tfp->next = prev->next;\n\n\t\titer = head;\n\t\twhile (iter) {\n\t\t\tif (iter->next == prev) {\n\t\t\t\titer->next = fp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\titer = iter->next;\n\t\t}\n\n\t\tskb_morph(prev, head);\n\t\tprev->next = head->next;\n\t\tconsume_skb(head);\n\t\thead = prev;\n\t}\n\n\t/* We have to remove fragment header from datagram and to relocate\n\t * header in order to calculate ICV correctly. */\n\tskb_network_header(head)[fq->nhoffset] = skb_transport_header(head)[0];\n\tmemmove(head->head + sizeof(struct frag_hdr), head->head,\n\t\t(head->data - head->head) - sizeof(struct frag_hdr));\n\thead->mac_header += sizeof(struct frag_hdr);\n\thead->network_header += sizeof(struct frag_hdr);\n\n\tskb_shinfo(head)->frag_list = head->next;\n\tskb_reset_transport_header(head);\n\tskb_push(head, head->data - skb_network_header(head));\n\n\tfor (fp = head->next; fp; fp = fp->next) {\n\t\thead->data_len += fp->len;\n\t\thead->len += fp->len;\n\t\tif (head->ip_summed != fp->ip_summed)\n\t\t\thead->ip_summed = CHECKSUM_NONE;\n\t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n\t\t\thead->csum = csum_add(head->csum, fp->csum);\n\t\thead->truesize += fp->truesize;\n\t}\n\tsub_frag_mem_limit(fq->q.net, head->truesize);\n\n\thead->ignore_df = 1;\n\thead->next = NULL;\n\thead->dev = dev;\n\thead->tstamp = fq->q.stamp;\n\tipv6_hdr(head)->payload_len = htons(payload_len);\n\tipv6_change_dsfield(ipv6_hdr(head), 0xff, ecn);\n\tIP6CB(head)->frag_max_size = sizeof(struct ipv6hdr) + fq->q.max_size;\n\n\t/* Yes, and fold redundant checksum back. 8) */\n\tif (head->ip_summed == CHECKSUM_COMPLETE)\n\t\thead->csum = csum_partial(skb_network_header(head),\n\t\t\t\t\t  skb_network_header_len(head),\n\t\t\t\t\t  head->csum);\n\n\tfq->q.fragments = NULL;\n\tfq->q.rb_fragments = RB_ROOT;\n\tfq->q.fragments_tail = NULL;\n\n\treturn true;\n}",
        "code_after_change": "static bool\nnf_ct_frag6_reasm(struct frag_queue *fq, struct sk_buff *prev,  struct net_device *dev)\n{\n\tstruct sk_buff *fp, *head = fq->q.fragments;\n\tint    payload_len;\n\tu8 ecn;\n\n\tinet_frag_kill(&fq->q);\n\n\tWARN_ON(head == NULL);\n\tWARN_ON(head->ip_defrag_offset != 0);\n\n\tecn = ip_frag_ecn_table[fq->ecn];\n\tif (unlikely(ecn == 0xff))\n\t\treturn false;\n\n\t/* Unfragmented part is taken from the first segment. */\n\tpayload_len = ((head->data - skb_network_header(head)) -\n\t\t       sizeof(struct ipv6hdr) + fq->q.len -\n\t\t       sizeof(struct frag_hdr));\n\tif (payload_len > IPV6_MAXPLEN) {\n\t\tnet_dbg_ratelimited(\"nf_ct_frag6_reasm: payload len = %d\\n\",\n\t\t\t\t    payload_len);\n\t\treturn false;\n\t}\n\n\t/* Head of list must not be cloned. */\n\tif (skb_unclone(head, GFP_ATOMIC))\n\t\treturn false;\n\n\t/* If the first fragment is fragmented itself, we split\n\t * it to two chunks: the first with data and paged part\n\t * and the second, holding only fragments. */\n\tif (skb_has_frag_list(head)) {\n\t\tstruct sk_buff *clone;\n\t\tint i, plen = 0;\n\n\t\tclone = alloc_skb(0, GFP_ATOMIC);\n\t\tif (clone == NULL)\n\t\t\treturn false;\n\n\t\tclone->next = head->next;\n\t\thead->next = clone;\n\t\tskb_shinfo(clone)->frag_list = skb_shinfo(head)->frag_list;\n\t\tskb_frag_list_init(head);\n\t\tfor (i = 0; i < skb_shinfo(head)->nr_frags; i++)\n\t\t\tplen += skb_frag_size(&skb_shinfo(head)->frags[i]);\n\t\tclone->len = clone->data_len = head->data_len - plen;\n\t\thead->data_len -= clone->len;\n\t\thead->len -= clone->len;\n\t\tclone->csum = 0;\n\t\tclone->ip_summed = head->ip_summed;\n\n\t\tadd_frag_mem_limit(fq->q.net, clone->truesize);\n\t}\n\n\t/* morph head into last received skb: prev.\n\t *\n\t * This allows callers of ipv6 conntrack defrag to continue\n\t * to use the last skb(frag) passed into the reasm engine.\n\t * The last skb frag 'silently' turns into the full reassembled skb.\n\t *\n\t * Since prev is also part of q->fragments we have to clone it first.\n\t */\n\tif (head != prev) {\n\t\tstruct sk_buff *iter;\n\n\t\tfp = skb_clone(prev, GFP_ATOMIC);\n\t\tif (!fp)\n\t\t\treturn false;\n\n\t\tfp->next = prev->next;\n\n\t\titer = head;\n\t\twhile (iter) {\n\t\t\tif (iter->next == prev) {\n\t\t\t\titer->next = fp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\titer = iter->next;\n\t\t}\n\n\t\tskb_morph(prev, head);\n\t\tprev->next = head->next;\n\t\tconsume_skb(head);\n\t\thead = prev;\n\t}\n\n\t/* We have to remove fragment header from datagram and to relocate\n\t * header in order to calculate ICV correctly. */\n\tskb_network_header(head)[fq->nhoffset] = skb_transport_header(head)[0];\n\tmemmove(head->head + sizeof(struct frag_hdr), head->head,\n\t\t(head->data - head->head) - sizeof(struct frag_hdr));\n\thead->mac_header += sizeof(struct frag_hdr);\n\thead->network_header += sizeof(struct frag_hdr);\n\n\tskb_shinfo(head)->frag_list = head->next;\n\tskb_reset_transport_header(head);\n\tskb_push(head, head->data - skb_network_header(head));\n\n\tfor (fp = head->next; fp; fp = fp->next) {\n\t\thead->data_len += fp->len;\n\t\thead->len += fp->len;\n\t\tif (head->ip_summed != fp->ip_summed)\n\t\t\thead->ip_summed = CHECKSUM_NONE;\n\t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n\t\t\thead->csum = csum_add(head->csum, fp->csum);\n\t\thead->truesize += fp->truesize;\n\t\tfp->sk = NULL;\n\t}\n\tsub_frag_mem_limit(fq->q.net, head->truesize);\n\n\thead->ignore_df = 1;\n\thead->next = NULL;\n\thead->dev = dev;\n\thead->tstamp = fq->q.stamp;\n\tipv6_hdr(head)->payload_len = htons(payload_len);\n\tipv6_change_dsfield(ipv6_hdr(head), 0xff, ecn);\n\tIP6CB(head)->frag_max_size = sizeof(struct ipv6hdr) + fq->q.max_size;\n\n\t/* Yes, and fold redundant checksum back. 8) */\n\tif (head->ip_summed == CHECKSUM_COMPLETE)\n\t\thead->csum = csum_partial(skb_network_header(head),\n\t\t\t\t\t  skb_network_header_len(head),\n\t\t\t\t\t  head->csum);\n\n\tfq->q.fragments = NULL;\n\tfq->q.rb_fragments = RB_ROOT;\n\tfq->q.fragments_tail = NULL;\n\n\treturn true;\n}",
        "patch": "--- code before\n+++ code after\n@@ -106,6 +106,7 @@\n \t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n \t\t\thead->csum = csum_add(head->csum, fp->csum);\n \t\thead->truesize += fp->truesize;\n+\t\tfp->sk = NULL;\n \t}\n \tsub_frag_mem_limit(fq->q.net, head->truesize);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tfp->sk = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A security flaw was found in the ip_frag_reasm() function in net/ipv4/ip_fragment.c in the Linux kernel from 4.19-rc1 to 4.19-rc3 inclusive, which can cause a later system crash in ip_do_fragment(). With certain non-default, but non-rare, configuration of a victim host, an attacker can trigger this crash remotely, thus leading to a remote denial-of-service.",
        "id": 1700
    },
    {
        "cve_id": "CVE-2014-2523",
        "code_before_change": "static bool dccp_new(struct nf_conn *ct, const struct sk_buff *skb,\n\t\t     unsigned int dataoff, unsigned int *timeouts)\n{\n\tstruct net *net = nf_ct_net(ct);\n\tstruct dccp_net *dn;\n\tstruct dccp_hdr _dh, *dh;\n\tconst char *msg;\n\tu_int8_t state;\n\n\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);\n\tBUG_ON(dh == NULL);\n\n\tstate = dccp_state_table[CT_DCCP_ROLE_CLIENT][dh->dccph_type][CT_DCCP_NONE];\n\tswitch (state) {\n\tdefault:\n\t\tdn = dccp_pernet(net);\n\t\tif (dn->dccp_loose == 0) {\n\t\t\tmsg = \"nf_ct_dccp: not picking up existing connection \";\n\t\t\tgoto out_invalid;\n\t\t}\n\tcase CT_DCCP_REQUEST:\n\t\tbreak;\n\tcase CT_DCCP_INVALID:\n\t\tmsg = \"nf_ct_dccp: invalid state transition \";\n\t\tgoto out_invalid;\n\t}\n\n\tct->proto.dccp.role[IP_CT_DIR_ORIGINAL] = CT_DCCP_ROLE_CLIENT;\n\tct->proto.dccp.role[IP_CT_DIR_REPLY] = CT_DCCP_ROLE_SERVER;\n\tct->proto.dccp.state = CT_DCCP_NONE;\n\tct->proto.dccp.last_pkt = DCCP_PKT_REQUEST;\n\tct->proto.dccp.last_dir = IP_CT_DIR_ORIGINAL;\n\tct->proto.dccp.handshake_seq = 0;\n\treturn true;\n\nout_invalid:\n\tif (LOG_INVALID(net, IPPROTO_DCCP))\n\t\tnf_log_packet(net, nf_ct_l3num(ct), 0, skb, NULL, NULL,\n\t\t\t      NULL, \"%s\", msg);\n\treturn false;\n}",
        "code_after_change": "static bool dccp_new(struct nf_conn *ct, const struct sk_buff *skb,\n\t\t     unsigned int dataoff, unsigned int *timeouts)\n{\n\tstruct net *net = nf_ct_net(ct);\n\tstruct dccp_net *dn;\n\tstruct dccp_hdr _dh, *dh;\n\tconst char *msg;\n\tu_int8_t state;\n\n\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);\n\tBUG_ON(dh == NULL);\n\n\tstate = dccp_state_table[CT_DCCP_ROLE_CLIENT][dh->dccph_type][CT_DCCP_NONE];\n\tswitch (state) {\n\tdefault:\n\t\tdn = dccp_pernet(net);\n\t\tif (dn->dccp_loose == 0) {\n\t\t\tmsg = \"nf_ct_dccp: not picking up existing connection \";\n\t\t\tgoto out_invalid;\n\t\t}\n\tcase CT_DCCP_REQUEST:\n\t\tbreak;\n\tcase CT_DCCP_INVALID:\n\t\tmsg = \"nf_ct_dccp: invalid state transition \";\n\t\tgoto out_invalid;\n\t}\n\n\tct->proto.dccp.role[IP_CT_DIR_ORIGINAL] = CT_DCCP_ROLE_CLIENT;\n\tct->proto.dccp.role[IP_CT_DIR_REPLY] = CT_DCCP_ROLE_SERVER;\n\tct->proto.dccp.state = CT_DCCP_NONE;\n\tct->proto.dccp.last_pkt = DCCP_PKT_REQUEST;\n\tct->proto.dccp.last_dir = IP_CT_DIR_ORIGINAL;\n\tct->proto.dccp.handshake_seq = 0;\n\treturn true;\n\nout_invalid:\n\tif (LOG_INVALID(net, IPPROTO_DCCP))\n\t\tnf_log_packet(net, nf_ct_l3num(ct), 0, skb, NULL, NULL,\n\t\t\t      NULL, \"%s\", msg);\n\treturn false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \tconst char *msg;\n \tu_int8_t state;\n \n-\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);\n+\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);\n \tBUG_ON(dh == NULL);\n \n \tstate = dccp_state_table[CT_DCCP_ROLE_CLIENT][dh->dccph_type][CT_DCCP_NONE];",
        "function_modified_lines": {
            "added": [
                "\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);"
            ],
            "deleted": [
                "\tdh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "net/netfilter/nf_conntrack_proto_dccp.c in the Linux kernel through 3.13.6 uses a DCCP header pointer incorrectly, which allows remote attackers to cause a denial of service (system crash) or possibly execute arbitrary code via a DCCP packet that triggers a call to the (1) dccp_new, (2) dccp_packet, or (3) dccp_error function.",
        "id": 482
    },
    {
        "cve_id": "CVE-2017-5123",
        "code_before_change": "\nSYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,\n\t\tinfop, int, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);\n\tint signo = 0;\n\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\tif (!infop)\n\t\treturn err;\n\n\tuser_access_begin();\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "code_after_change": "\nSYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,\n\t\tinfop, int, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);\n\tint signo = 0;\n\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\tif (!infop)\n\t\treturn err;\n\n\tif (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))\n\t\tgoto Efault;\n\n\tuser_access_begin();\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,9 @@\n \tif (!infop)\n \t\treturn err;\n \n+\tif (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))\n+\t\tgoto Efault;\n+\n \tuser_access_begin();\n \tunsafe_put_user(signo, &infop->si_signo, Efault);\n \tunsafe_put_user(0, &infop->si_errno, Efault);",
        "function_modified_lines": {
            "added": [
                "\tif (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))",
                "\t\tgoto Efault;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Insufficient data validation in waitid allowed an user to escape sandboxes on Linux.",
        "id": 1457
    },
    {
        "cve_id": "CVE-2018-20669",
        "code_before_change": "static int eb_copy_relocations(const struct i915_execbuffer *eb)\n{\n\tconst unsigned int count = eb->buffer_count;\n\tunsigned int i;\n\tint err;\n\n\tfor (i = 0; i < count; i++) {\n\t\tconst unsigned int nreloc = eb->exec[i].relocation_count;\n\t\tstruct drm_i915_gem_relocation_entry __user *urelocs;\n\t\tstruct drm_i915_gem_relocation_entry *relocs;\n\t\tunsigned long size;\n\t\tunsigned long copied;\n\n\t\tif (nreloc == 0)\n\t\t\tcontinue;\n\n\t\terr = check_relocations(&eb->exec[i]);\n\t\tif (err)\n\t\t\tgoto err;\n\n\t\turelocs = u64_to_user_ptr(eb->exec[i].relocs_ptr);\n\t\tsize = nreloc * sizeof(*relocs);\n\n\t\trelocs = kvmalloc_array(size, 1, GFP_KERNEL);\n\t\tif (!relocs) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\t/* copy_from_user is limited to < 4GiB */\n\t\tcopied = 0;\n\t\tdo {\n\t\t\tunsigned int len =\n\t\t\t\tmin_t(u64, BIT_ULL(31), size - copied);\n\n\t\t\tif (__copy_from_user((char *)relocs + copied,\n\t\t\t\t\t     (char __user *)urelocs + copied,\n\t\t\t\t\t     len)) {\nend_user:\n\t\t\t\tuser_access_end();\n\t\t\t\tkvfree(relocs);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tcopied += len;\n\t\t} while (copied < size);\n\n\t\t/*\n\t\t * As we do not update the known relocation offsets after\n\t\t * relocating (due to the complexities in lock handling),\n\t\t * we need to mark them as invalid now so that we force the\n\t\t * relocation processing next time. Just in case the target\n\t\t * object is evicted and then rebound into its old\n\t\t * presumed_offset before the next execbuffer - if that\n\t\t * happened we would make the mistake of assuming that the\n\t\t * relocations were valid.\n\t\t */\n\t\tuser_access_begin();\n\t\tfor (copied = 0; copied < nreloc; copied++)\n\t\t\tunsafe_put_user(-1,\n\t\t\t\t\t&urelocs[copied].presumed_offset,\n\t\t\t\t\tend_user);\n\t\tuser_access_end();\n\n\t\teb->exec[i].relocs_ptr = (uintptr_t)relocs;\n\t}\n\n\treturn 0;\n\nerr:\n\twhile (i--) {\n\t\tstruct drm_i915_gem_relocation_entry *relocs =\n\t\t\tu64_to_ptr(typeof(*relocs), eb->exec[i].relocs_ptr);\n\t\tif (eb->exec[i].relocation_count)\n\t\t\tkvfree(relocs);\n\t}\n\treturn err;\n}",
        "code_after_change": "static int eb_copy_relocations(const struct i915_execbuffer *eb)\n{\n\tconst unsigned int count = eb->buffer_count;\n\tunsigned int i;\n\tint err;\n\n\tfor (i = 0; i < count; i++) {\n\t\tconst unsigned int nreloc = eb->exec[i].relocation_count;\n\t\tstruct drm_i915_gem_relocation_entry __user *urelocs;\n\t\tstruct drm_i915_gem_relocation_entry *relocs;\n\t\tunsigned long size;\n\t\tunsigned long copied;\n\n\t\tif (nreloc == 0)\n\t\t\tcontinue;\n\n\t\terr = check_relocations(&eb->exec[i]);\n\t\tif (err)\n\t\t\tgoto err;\n\n\t\turelocs = u64_to_user_ptr(eb->exec[i].relocs_ptr);\n\t\tsize = nreloc * sizeof(*relocs);\n\n\t\trelocs = kvmalloc_array(size, 1, GFP_KERNEL);\n\t\tif (!relocs) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\t/* copy_from_user is limited to < 4GiB */\n\t\tcopied = 0;\n\t\tdo {\n\t\t\tunsigned int len =\n\t\t\t\tmin_t(u64, BIT_ULL(31), size - copied);\n\n\t\t\tif (__copy_from_user((char *)relocs + copied,\n\t\t\t\t\t     (char __user *)urelocs + copied,\n\t\t\t\t\t     len)) {\nend_user:\n\t\t\t\tuser_access_end();\n\t\t\t\tkvfree(relocs);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tcopied += len;\n\t\t} while (copied < size);\n\n\t\t/*\n\t\t * As we do not update the known relocation offsets after\n\t\t * relocating (due to the complexities in lock handling),\n\t\t * we need to mark them as invalid now so that we force the\n\t\t * relocation processing next time. Just in case the target\n\t\t * object is evicted and then rebound into its old\n\t\t * presumed_offset before the next execbuffer - if that\n\t\t * happened we would make the mistake of assuming that the\n\t\t * relocations were valid.\n\t\t */\n\t\tif (!user_access_begin(urelocs, size))\n\t\t\tgoto end_user;\n\n\t\tfor (copied = 0; copied < nreloc; copied++)\n\t\t\tunsafe_put_user(-1,\n\t\t\t\t\t&urelocs[copied].presumed_offset,\n\t\t\t\t\tend_user);\n\t\tuser_access_end();\n\n\t\teb->exec[i].relocs_ptr = (uintptr_t)relocs;\n\t}\n\n\treturn 0;\n\nerr:\n\twhile (i--) {\n\t\tstruct drm_i915_gem_relocation_entry *relocs =\n\t\t\tu64_to_ptr(typeof(*relocs), eb->exec[i].relocs_ptr);\n\t\tif (eb->exec[i].relocation_count)\n\t\t\tkvfree(relocs);\n\t}\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -56,7 +56,9 @@\n \t\t * happened we would make the mistake of assuming that the\n \t\t * relocations were valid.\n \t\t */\n-\t\tuser_access_begin();\n+\t\tif (!user_access_begin(urelocs, size))\n+\t\t\tgoto end_user;\n+\n \t\tfor (copied = 0; copied < nreloc; copied++)\n \t\t\tunsafe_put_user(-1,\n \t\t\t\t\t&urelocs[copied].presumed_offset,",
        "function_modified_lines": {
            "added": [
                "\t\tif (!user_access_begin(urelocs, size))",
                "\t\t\tgoto end_user;",
                ""
            ],
            "deleted": [
                "\t\tuser_access_begin();"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "An issue where a provided address with access_ok() is not checked was discovered in i915_gem_execbuffer2_ioctl in drivers/gpu/drm/i915/i915_gem_execbuffer.c in the Linux kernel through 4.19.13. A local attacker can craft a malicious IOCTL function call to overwrite arbitrary kernel memory, resulting in a Denial of Service or privilege escalation.",
        "id": 1774
    },
    {
        "cve_id": "CVE-2014-0155",
        "code_before_change": "static int ioapic_service(struct kvm_ioapic *ioapic, int irq, bool line_status)\n{\n\tunion kvm_ioapic_redirect_entry *entry = &ioapic->redirtbl[irq];\n\tstruct kvm_lapic_irq irqe;\n\tint ret;\n\n\tif (entry->fields.mask)\n\t\treturn -1;\n\n\tioapic_debug(\"dest=%x dest_mode=%x delivery_mode=%x \"\n\t\t     \"vector=%x trig_mode=%x\\n\",\n\t\t     entry->fields.dest_id, entry->fields.dest_mode,\n\t\t     entry->fields.delivery_mode, entry->fields.vector,\n\t\t     entry->fields.trig_mode);\n\n\tirqe.dest_id = entry->fields.dest_id;\n\tirqe.vector = entry->fields.vector;\n\tirqe.dest_mode = entry->fields.dest_mode;\n\tirqe.trig_mode = entry->fields.trig_mode;\n\tirqe.delivery_mode = entry->fields.delivery_mode << 8;\n\tirqe.level = 1;\n\tirqe.shorthand = 0;\n\n\tif (irqe.trig_mode == IOAPIC_EDGE_TRIG)\n\t\tioapic->irr &= ~(1 << irq);\n\n\tif (irq == RTC_GSI && line_status) {\n\t\tBUG_ON(ioapic->rtc_status.pending_eoi != 0);\n\t\tret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe,\n\t\t\t\tioapic->rtc_status.dest_map);\n\t\tioapic->rtc_status.pending_eoi = ret;\n\t} else\n\t\tret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);\n\n\tif (ret && irqe.trig_mode == IOAPIC_LEVEL_TRIG)\n\t\tentry->fields.remote_irr = 1;\n\n\treturn ret;\n}",
        "code_after_change": "static int ioapic_service(struct kvm_ioapic *ioapic, int irq, bool line_status)\n{\n\tunion kvm_ioapic_redirect_entry *entry = &ioapic->redirtbl[irq];\n\tstruct kvm_lapic_irq irqe;\n\tint ret;\n\n\tif (entry->fields.mask)\n\t\treturn -1;\n\n\tioapic_debug(\"dest=%x dest_mode=%x delivery_mode=%x \"\n\t\t     \"vector=%x trig_mode=%x\\n\",\n\t\t     entry->fields.dest_id, entry->fields.dest_mode,\n\t\t     entry->fields.delivery_mode, entry->fields.vector,\n\t\t     entry->fields.trig_mode);\n\n\tirqe.dest_id = entry->fields.dest_id;\n\tirqe.vector = entry->fields.vector;\n\tirqe.dest_mode = entry->fields.dest_mode;\n\tirqe.trig_mode = entry->fields.trig_mode;\n\tirqe.delivery_mode = entry->fields.delivery_mode << 8;\n\tirqe.level = 1;\n\tirqe.shorthand = 0;\n\n\tif (irqe.trig_mode == IOAPIC_EDGE_TRIG)\n\t\tioapic->irr &= ~(1 << irq);\n\n\tif (irq == RTC_GSI && line_status) {\n\t\tBUG_ON(ioapic->rtc_status.pending_eoi != 0);\n\t\tret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe,\n\t\t\t\tioapic->rtc_status.dest_map);\n\t\tioapic->rtc_status.pending_eoi = (ret < 0 ? 0 : ret);\n\t} else\n\t\tret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);\n\n\tif (ret && irqe.trig_mode == IOAPIC_LEVEL_TRIG)\n\t\tentry->fields.remote_irr = 1;\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,7 @@\n \t\tBUG_ON(ioapic->rtc_status.pending_eoi != 0);\n \t\tret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe,\n \t\t\t\tioapic->rtc_status.dest_map);\n-\t\tioapic->rtc_status.pending_eoi = ret;\n+\t\tioapic->rtc_status.pending_eoi = (ret < 0 ? 0 : ret);\n \t} else\n \t\tret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tioapic->rtc_status.pending_eoi = (ret < 0 ? 0 : ret);"
            ],
            "deleted": [
                "\t\tioapic->rtc_status.pending_eoi = ret;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The ioapic_deliver function in virt/kvm/ioapic.c in the Linux kernel through 3.14.1 does not properly validate the kvm_irq_delivery_to_apic return value, which allows guest OS users to cause a denial of service (host OS crash) via a crafted entry in the redirection table of an I/O APIC.  NOTE: the affected code was moved to the ioapic_service function before the vulnerability was announced.",
        "id": 432
    },
    {
        "cve_id": "CVE-2018-20669",
        "code_before_change": "long strnlen_user(const char __user *str, long count)\n{\n\tunsigned long max_addr, src_addr;\n\n\tif (unlikely(count <= 0))\n\t\treturn 0;\n\n\tmax_addr = user_addr_max();\n\tsrc_addr = (unsigned long)str;\n\tif (likely(src_addr < max_addr)) {\n\t\tunsigned long max = max_addr - src_addr;\n\t\tlong retval;\n\n\t\tuser_access_begin();\n\t\tretval = do_strnlen_user(str, count, max);\n\t\tuser_access_end();\n\t\treturn retval;\n\t}\n\treturn 0;\n}",
        "code_after_change": "long strnlen_user(const char __user *str, long count)\n{\n\tunsigned long max_addr, src_addr;\n\n\tif (unlikely(count <= 0))\n\t\treturn 0;\n\n\tmax_addr = user_addr_max();\n\tsrc_addr = (unsigned long)str;\n\tif (likely(src_addr < max_addr)) {\n\t\tunsigned long max = max_addr - src_addr;\n\t\tlong retval;\n\n\t\tif (user_access_begin(str, max)) {\n\t\t\tretval = do_strnlen_user(str, count, max);\n\t\t\tuser_access_end();\n\t\t\treturn retval;\n\t\t}\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,10 +11,11 @@\n \t\tunsigned long max = max_addr - src_addr;\n \t\tlong retval;\n \n-\t\tuser_access_begin();\n-\t\tretval = do_strnlen_user(str, count, max);\n-\t\tuser_access_end();\n-\t\treturn retval;\n+\t\tif (user_access_begin(str, max)) {\n+\t\t\tretval = do_strnlen_user(str, count, max);\n+\t\t\tuser_access_end();\n+\t\t\treturn retval;\n+\t\t}\n \t}\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tif (user_access_begin(str, max)) {",
                "\t\t\tretval = do_strnlen_user(str, count, max);",
                "\t\t\tuser_access_end();",
                "\t\t\treturn retval;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tuser_access_begin();",
                "\t\tretval = do_strnlen_user(str, count, max);",
                "\t\tuser_access_end();",
                "\t\treturn retval;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "An issue where a provided address with access_ok() is not checked was discovered in i915_gem_execbuffer2_ioctl in drivers/gpu/drm/i915/i915_gem_execbuffer.c in the Linux kernel through 4.19.13. A local attacker can craft a malicious IOCTL function call to overwrite arbitrary kernel memory, resulting in a Denial of Service or privilege escalation.",
        "id": 1780
    },
    {
        "cve_id": "CVE-2017-17805",
        "code_before_change": "static int encrypt(struct blkcipher_desc *desc,\n\t\t   struct scatterlist *dst, struct scatterlist *src,\n\t\t   unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct salsa20_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt_block(desc, &walk, 64);\n\n\tsalsa20_ivsetup(ctx, walk.iv);\n\n\tif (likely(walk.nbytes == nbytes))\n\t{\n\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n\t\t\t\t      walk.src.virt.addr, nbytes);\n\t\treturn blkcipher_walk_done(desc, &walk, 0);\n\t}\n\n\twhile (walk.nbytes >= 64) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n\t\t\t\t      walk.src.virt.addr,\n\t\t\t\t      walk.nbytes - (walk.nbytes % 64));\n\t\terr = blkcipher_walk_done(desc, &walk, walk.nbytes % 64);\n\t}\n\n\tif (walk.nbytes) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n\t\t\t\t      walk.src.virt.addr, walk.nbytes);\n\t\terr = blkcipher_walk_done(desc, &walk, 0);\n\t}\n\n\treturn err;\n}",
        "code_after_change": "static int encrypt(struct blkcipher_desc *desc,\n\t\t   struct scatterlist *dst, struct scatterlist *src,\n\t\t   unsigned int nbytes)\n{\n\tstruct blkcipher_walk walk;\n\tstruct crypto_blkcipher *tfm = desc->tfm;\n\tstruct salsa20_ctx *ctx = crypto_blkcipher_ctx(tfm);\n\tint err;\n\n\tblkcipher_walk_init(&walk, dst, src, nbytes);\n\terr = blkcipher_walk_virt_block(desc, &walk, 64);\n\n\tsalsa20_ivsetup(ctx, walk.iv);\n\n\twhile (walk.nbytes >= 64) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n\t\t\t\t      walk.src.virt.addr,\n\t\t\t\t      walk.nbytes - (walk.nbytes % 64));\n\t\terr = blkcipher_walk_done(desc, &walk, walk.nbytes % 64);\n\t}\n\n\tif (walk.nbytes) {\n\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n\t\t\t\t      walk.src.virt.addr, walk.nbytes);\n\t\terr = blkcipher_walk_done(desc, &walk, 0);\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,13 +11,6 @@\n \terr = blkcipher_walk_virt_block(desc, &walk, 64);\n \n \tsalsa20_ivsetup(ctx, walk.iv);\n-\n-\tif (likely(walk.nbytes == nbytes))\n-\t{\n-\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,\n-\t\t\t\t      walk.src.virt.addr, nbytes);\n-\t\treturn blkcipher_walk_done(desc, &walk, 0);\n-\t}\n \n \twhile (walk.nbytes >= 64) {\n \t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tif (likely(walk.nbytes == nbytes))",
                "\t{",
                "\t\tsalsa20_encrypt_bytes(ctx, walk.dst.virt.addr,",
                "\t\t\t\t      walk.src.virt.addr, nbytes);",
                "\t\treturn blkcipher_walk_done(desc, &walk, 0);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Salsa20 encryption algorithm in the Linux kernel before 4.14.8 does not correctly handle zero-length inputs, allowing a local attacker able to use the AF_ALG-based skcipher interface (CONFIG_CRYPTO_USER_API_SKCIPHER) to cause a denial of service (uninitialized-memory free and kernel crash) or have unspecified other impact by executing a crafted sequence of system calls that use the blkcipher_walk API. Both the generic implementation (crypto/salsa20_generic.c) and x86 implementation (arch/x86/crypto/salsa20_glue.c) of Salsa20 were vulnerable.",
        "id": 1374
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int iucv_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t     struct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tunsigned int copied, rlen;\n\tstruct sk_buff *skb, *rskb, *cskb;\n\tint err = 0;\n\tu32 offset;\n\n\tmsg->msg_namelen = 0;\n\n\tif ((sk->sk_state == IUCV_DISCONN) &&\n\t    skb_queue_empty(&iucv->backlog_skb_q) &&\n\t    skb_queue_empty(&sk->sk_receive_queue) &&\n\t    list_empty(&iucv->message_q.list))\n\t\treturn 0;\n\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\t/* receive/dequeue next skb:\n\t * the function understands MSG_PEEK and, thus, does not dequeue skb */\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb) {\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\treturn 0;\n\t\treturn err;\n\t}\n\n\toffset = IUCV_SKB_CB(skb)->offset;\n\trlen   = skb->len - offset;\t\t/* real length of skb */\n\tcopied = min_t(unsigned int, rlen, len);\n\tif (!rlen)\n\t\tsk->sk_shutdown = sk->sk_shutdown | RCV_SHUTDOWN;\n\n\tcskb = skb;\n\tif (skb_copy_datagram_iovec(cskb, offset, msg->msg_iov, copied)) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn -EFAULT;\n\t}\n\n\t/* SOCK_SEQPACKET: set MSG_TRUNC if recv buf size is too small */\n\tif (sk->sk_type == SOCK_SEQPACKET) {\n\t\tif (copied < rlen)\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\t/* each iucv message contains a complete record */\n\t\tmsg->msg_flags |= MSG_EOR;\n\t}\n\n\t/* create control message to store iucv msg target class:\n\t * get the trgcls from the control buffer of the skb due to\n\t * fragmentation of original iucv message. */\n\terr = put_cmsg(msg, SOL_IUCV, SCM_IUCV_TRGCLS,\n\t\t       sizeof(IUCV_SKB_CB(skb)->class),\n\t\t       (void *)&IUCV_SKB_CB(skb)->class);\n\tif (err) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn err;\n\t}\n\n\t/* Mark read part of skb as used */\n\tif (!(flags & MSG_PEEK)) {\n\n\t\t/* SOCK_STREAM: re-queue skb if it contains unreceived data */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tif (copied < rlen) {\n\t\t\t\tIUCV_SKB_CB(skb)->offset = offset + copied;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\n\t\tkfree_skb(skb);\n\t\tif (iucv->transport == AF_IUCV_TRANS_HIPER) {\n\t\t\tatomic_inc(&iucv->msg_recv);\n\t\t\tif (atomic_read(&iucv->msg_recv) > iucv->msglimit) {\n\t\t\t\tWARN_ON(1);\n\t\t\t\tiucv_sock_close(sk);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t}\n\n\t\t/* Queue backlog skbs */\n\t\tspin_lock_bh(&iucv->message_q.lock);\n\t\trskb = skb_dequeue(&iucv->backlog_skb_q);\n\t\twhile (rskb) {\n\t\t\tIUCV_SKB_CB(rskb)->offset = 0;\n\t\t\tif (sock_queue_rcv_skb(sk, rskb)) {\n\t\t\t\tskb_queue_head(&iucv->backlog_skb_q,\n\t\t\t\t\t\trskb);\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\trskb = skb_dequeue(&iucv->backlog_skb_q);\n\t\t\t}\n\t\t}\n\t\tif (skb_queue_empty(&iucv->backlog_skb_q)) {\n\t\t\tif (!list_empty(&iucv->message_q.list))\n\t\t\t\tiucv_process_message_q(sk);\n\t\t\tif (atomic_read(&iucv->msg_recv) >=\n\t\t\t\t\t\t\tiucv->msglimit / 2) {\n\t\t\t\terr = iucv_send_ctrl(sk, AF_IUCV_FLAG_WIN);\n\t\t\t\tif (err) {\n\t\t\t\t\tsk->sk_state = IUCV_DISCONN;\n\t\t\t\t\tsk->sk_state_change(sk);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_unlock_bh(&iucv->message_q.lock);\n\t}\n\ndone:\n\t/* SOCK_SEQPACKET: return real length if MSG_TRUNC is set */\n\tif (sk->sk_type == SOCK_SEQPACKET && (flags & MSG_TRUNC))\n\t\tcopied = rlen;\n\n\treturn copied;\n}",
        "code_after_change": "static int iucv_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t     struct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tunsigned int copied, rlen;\n\tstruct sk_buff *skb, *rskb, *cskb;\n\tint err = 0;\n\tu32 offset;\n\n\tif ((sk->sk_state == IUCV_DISCONN) &&\n\t    skb_queue_empty(&iucv->backlog_skb_q) &&\n\t    skb_queue_empty(&sk->sk_receive_queue) &&\n\t    list_empty(&iucv->message_q.list))\n\t\treturn 0;\n\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\t/* receive/dequeue next skb:\n\t * the function understands MSG_PEEK and, thus, does not dequeue skb */\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb) {\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\treturn 0;\n\t\treturn err;\n\t}\n\n\toffset = IUCV_SKB_CB(skb)->offset;\n\trlen   = skb->len - offset;\t\t/* real length of skb */\n\tcopied = min_t(unsigned int, rlen, len);\n\tif (!rlen)\n\t\tsk->sk_shutdown = sk->sk_shutdown | RCV_SHUTDOWN;\n\n\tcskb = skb;\n\tif (skb_copy_datagram_iovec(cskb, offset, msg->msg_iov, copied)) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn -EFAULT;\n\t}\n\n\t/* SOCK_SEQPACKET: set MSG_TRUNC if recv buf size is too small */\n\tif (sk->sk_type == SOCK_SEQPACKET) {\n\t\tif (copied < rlen)\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\t/* each iucv message contains a complete record */\n\t\tmsg->msg_flags |= MSG_EOR;\n\t}\n\n\t/* create control message to store iucv msg target class:\n\t * get the trgcls from the control buffer of the skb due to\n\t * fragmentation of original iucv message. */\n\terr = put_cmsg(msg, SOL_IUCV, SCM_IUCV_TRGCLS,\n\t\t       sizeof(IUCV_SKB_CB(skb)->class),\n\t\t       (void *)&IUCV_SKB_CB(skb)->class);\n\tif (err) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn err;\n\t}\n\n\t/* Mark read part of skb as used */\n\tif (!(flags & MSG_PEEK)) {\n\n\t\t/* SOCK_STREAM: re-queue skb if it contains unreceived data */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tif (copied < rlen) {\n\t\t\t\tIUCV_SKB_CB(skb)->offset = offset + copied;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\n\t\tkfree_skb(skb);\n\t\tif (iucv->transport == AF_IUCV_TRANS_HIPER) {\n\t\t\tatomic_inc(&iucv->msg_recv);\n\t\t\tif (atomic_read(&iucv->msg_recv) > iucv->msglimit) {\n\t\t\t\tWARN_ON(1);\n\t\t\t\tiucv_sock_close(sk);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t}\n\n\t\t/* Queue backlog skbs */\n\t\tspin_lock_bh(&iucv->message_q.lock);\n\t\trskb = skb_dequeue(&iucv->backlog_skb_q);\n\t\twhile (rskb) {\n\t\t\tIUCV_SKB_CB(rskb)->offset = 0;\n\t\t\tif (sock_queue_rcv_skb(sk, rskb)) {\n\t\t\t\tskb_queue_head(&iucv->backlog_skb_q,\n\t\t\t\t\t\trskb);\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\trskb = skb_dequeue(&iucv->backlog_skb_q);\n\t\t\t}\n\t\t}\n\t\tif (skb_queue_empty(&iucv->backlog_skb_q)) {\n\t\t\tif (!list_empty(&iucv->message_q.list))\n\t\t\t\tiucv_process_message_q(sk);\n\t\t\tif (atomic_read(&iucv->msg_recv) >=\n\t\t\t\t\t\t\tiucv->msglimit / 2) {\n\t\t\t\terr = iucv_send_ctrl(sk, AF_IUCV_FLAG_WIN);\n\t\t\t\tif (err) {\n\t\t\t\t\tsk->sk_state = IUCV_DISCONN;\n\t\t\t\t\tsk->sk_state_change(sk);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_unlock_bh(&iucv->message_q.lock);\n\t}\n\ndone:\n\t/* SOCK_SEQPACKET: return real length if MSG_TRUNC is set */\n\tif (sk->sk_type == SOCK_SEQPACKET && (flags & MSG_TRUNC))\n\t\tcopied = rlen;\n\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,6 @@\n \tstruct sk_buff *skb, *rskb, *cskb;\n \tint err = 0;\n \tu32 offset;\n-\n-\tmsg->msg_namelen = 0;\n \n \tif ((sk->sk_state == IUCV_DISCONN) &&\n \t    skb_queue_empty(&iucv->backlog_skb_q) &&",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 385
    },
    {
        "cve_id": "CVE-2021-3655",
        "code_before_change": "static void sctp_v6_from_addr_param(union sctp_addr *addr,\n\t\t\t\t    union sctp_addr_param *param,\n\t\t\t\t    __be16 port, int iif)\n{\n\taddr->v6.sin6_family = AF_INET6;\n\taddr->v6.sin6_port = port;\n\taddr->v6.sin6_flowinfo = 0; /* BUG */\n\taddr->v6.sin6_addr = param->v6.addr;\n\taddr->v6.sin6_scope_id = iif;\n}",
        "code_after_change": "static bool sctp_v6_from_addr_param(union sctp_addr *addr,\n\t\t\t\t    union sctp_addr_param *param,\n\t\t\t\t    __be16 port, int iif)\n{\n\tif (ntohs(param->v6.param_hdr.length) < sizeof(struct sctp_ipv6addr_param))\n\t\treturn false;\n\n\taddr->v6.sin6_family = AF_INET6;\n\taddr->v6.sin6_port = port;\n\taddr->v6.sin6_flowinfo = 0; /* BUG */\n\taddr->v6.sin6_addr = param->v6.addr;\n\taddr->v6.sin6_scope_id = iif;\n\n\treturn true;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,15 @@\n-static void sctp_v6_from_addr_param(union sctp_addr *addr,\n+static bool sctp_v6_from_addr_param(union sctp_addr *addr,\n \t\t\t\t    union sctp_addr_param *param,\n \t\t\t\t    __be16 port, int iif)\n {\n+\tif (ntohs(param->v6.param_hdr.length) < sizeof(struct sctp_ipv6addr_param))\n+\t\treturn false;\n+\n \taddr->v6.sin6_family = AF_INET6;\n \taddr->v6.sin6_port = port;\n \taddr->v6.sin6_flowinfo = 0; /* BUG */\n \taddr->v6.sin6_addr = param->v6.addr;\n \taddr->v6.sin6_scope_id = iif;\n+\n+\treturn true;\n }",
        "function_modified_lines": {
            "added": [
                "static bool sctp_v6_from_addr_param(union sctp_addr *addr,",
                "\tif (ntohs(param->v6.param_hdr.length) < sizeof(struct sctp_ipv6addr_param))",
                "\t\treturn false;",
                "",
                "",
                "\treturn true;"
            ],
            "deleted": [
                "static void sctp_v6_from_addr_param(union sctp_addr *addr,"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel in versions prior to v5.14-rc1. Missing size validations on inbound SCTP packets may allow the kernel to read uninitialized memory.",
        "id": 3032
    },
    {
        "cve_id": "CVE-2017-15121",
        "code_before_change": "void truncate_inode_pages_range(struct address_space *mapping,\n\t\t\t\tloff_t lstart, loff_t lend)\n{\n\tconst pgoff_t start = (lstart + PAGE_CACHE_SIZE-1) >> PAGE_CACHE_SHIFT;\n\tconst unsigned partial = lstart & (PAGE_CACHE_SIZE - 1);\n\tstruct pagevec pvec;\n\tpgoff_t index;\n\tpgoff_t end;\n\tint i;\n\n\tcleancache_invalidate_inode(mapping);\n\tif (mapping->nrpages == 0)\n\t\treturn;\n\n\tBUG_ON((lend & (PAGE_CACHE_SIZE - 1)) != (PAGE_CACHE_SIZE - 1));\n\tend = (lend >> PAGE_CACHE_SHIFT);\n\n\tpagevec_init(&pvec, 0);\n\tindex = start;\n\twhile (index <= end && pagevec_lookup(&pvec, mapping, index,\n\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\t/* We rely upon deletion not changing page->index */\n\t\t\tindex = page->index;\n\t\t\tif (index > end)\n\t\t\t\tbreak;\n\n\t\t\tif (!trylock_page(page))\n\t\t\t\tcontinue;\n\t\t\tWARN_ON(page->index != index);\n\t\t\tif (PageWriteback(page)) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttruncate_inode_page(mapping, page);\n\t\t\tunlock_page(page);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tcond_resched();\n\t\tindex++;\n\t}\n\n\tif (partial) {\n\t\tstruct page *page = find_lock_page(mapping, start - 1);\n\t\tif (page) {\n\t\t\twait_on_page_writeback(page);\n\t\t\ttruncate_partial_page(page, partial);\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t}\n\t}\n\n\tindex = start;\n\tfor ( ; ; ) {\n\t\tcond_resched();\n\t\tif (!pagevec_lookup(&pvec, mapping, index,\n\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {\n\t\t\tif (index == start)\n\t\t\t\tbreak;\n\t\t\tindex = start;\n\t\t\tcontinue;\n\t\t}\n\t\tif (index == start && pvec.pages[0]->index > end) {\n\t\t\tpagevec_release(&pvec);\n\t\t\tbreak;\n\t\t}\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\t/* We rely upon deletion not changing page->index */\n\t\t\tindex = page->index;\n\t\t\tif (index > end)\n\t\t\t\tbreak;\n\n\t\t\tlock_page(page);\n\t\t\tWARN_ON(page->index != index);\n\t\t\twait_on_page_writeback(page);\n\t\t\ttruncate_inode_page(mapping, page);\n\t\t\tunlock_page(page);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tindex++;\n\t}\n\tcleancache_invalidate_inode(mapping);\n}",
        "code_after_change": "void truncate_inode_pages_range(struct address_space *mapping,\n\t\t\t\tloff_t lstart, loff_t lend)\n{\n\tpgoff_t\t\tstart;\t\t/* inclusive */\n\tpgoff_t\t\tend;\t\t/* exclusive */\n\tunsigned int\tpartial_start;\t/* inclusive */\n\tunsigned int\tpartial_end;\t/* exclusive */\n\tstruct pagevec\tpvec;\n\tpgoff_t\t\tindex;\n\tint\t\ti;\n\n\tcleancache_invalidate_inode(mapping);\n\tif (mapping->nrpages == 0)\n\t\treturn;\n\n\t/* Offsets within partial pages */\n\tpartial_start = lstart & (PAGE_CACHE_SIZE - 1);\n\tpartial_end = (lend + 1) & (PAGE_CACHE_SIZE - 1);\n\n\t/*\n\t * 'start' and 'end' always covers the range of pages to be fully\n\t * truncated. Partial pages are covered with 'partial_start' at the\n\t * start of the range and 'partial_end' at the end of the range.\n\t * Note that 'end' is exclusive while 'lend' is inclusive.\n\t */\n\tstart = (lstart + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\tif (lend == -1)\n\t\t/*\n\t\t * lend == -1 indicates end-of-file so we have to set 'end'\n\t\t * to the highest possible pgoff_t and since the type is\n\t\t * unsigned we're using -1.\n\t\t */\n\t\tend = -1;\n\telse\n\t\tend = (lend + 1) >> PAGE_CACHE_SHIFT;\n\n\tpagevec_init(&pvec, 0);\n\tindex = start;\n\twhile (index < end && pagevec_lookup(&pvec, mapping, index,\n\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\t/* We rely upon deletion not changing page->index */\n\t\t\tindex = page->index;\n\t\t\tif (index >= end)\n\t\t\t\tbreak;\n\n\t\t\tif (!trylock_page(page))\n\t\t\t\tcontinue;\n\t\t\tWARN_ON(page->index != index);\n\t\t\tif (PageWriteback(page)) {\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttruncate_inode_page(mapping, page);\n\t\t\tunlock_page(page);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tcond_resched();\n\t\tindex++;\n\t}\n\n\tif (partial_start) {\n\t\tstruct page *page = find_lock_page(mapping, start - 1);\n\t\tif (page) {\n\t\t\tunsigned int top = PAGE_CACHE_SIZE;\n\t\t\tif (start > end) {\n\t\t\t\t/* Truncation within a single page */\n\t\t\t\ttop = partial_end;\n\t\t\t\tpartial_end = 0;\n\t\t\t}\n\t\t\twait_on_page_writeback(page);\n\t\t\tzero_user_segment(page, partial_start, top);\n\t\t\tcleancache_invalidate_page(mapping, page);\n\t\t\tif (page_has_private(page))\n\t\t\t\tdo_invalidatepage(page, partial_start,\n\t\t\t\t\t\t  top - partial_start);\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t}\n\t}\n\tif (partial_end) {\n\t\tstruct page *page = find_lock_page(mapping, end);\n\t\tif (page) {\n\t\t\twait_on_page_writeback(page);\n\t\t\tzero_user_segment(page, 0, partial_end);\n\t\t\tcleancache_invalidate_page(mapping, page);\n\t\t\tif (page_has_private(page))\n\t\t\t\tdo_invalidatepage(page, 0,\n\t\t\t\t\t\t  partial_end);\n\t\t\tunlock_page(page);\n\t\t\tpage_cache_release(page);\n\t\t}\n\t}\n\t/*\n\t * If the truncation happened within a single page no pages\n\t * will be released, just zeroed, so we can bail out now.\n\t */\n\tif (start >= end)\n\t\treturn;\n\n\tindex = start;\n\tfor ( ; ; ) {\n\t\tcond_resched();\n\t\tif (!pagevec_lookup(&pvec, mapping, index,\n\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {\n\t\t\tif (index == start)\n\t\t\t\tbreak;\n\t\t\tindex = start;\n\t\t\tcontinue;\n\t\t}\n\t\tif (index == start && pvec.pages[0]->index >= end) {\n\t\t\tpagevec_release(&pvec);\n\t\t\tbreak;\n\t\t}\n\t\tmem_cgroup_uncharge_start();\n\t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\t/* We rely upon deletion not changing page->index */\n\t\t\tindex = page->index;\n\t\t\tif (index >= end)\n\t\t\t\tbreak;\n\n\t\t\tlock_page(page);\n\t\t\tWARN_ON(page->index != index);\n\t\t\twait_on_page_writeback(page);\n\t\t\ttruncate_inode_page(mapping, page);\n\t\t\tunlock_page(page);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tmem_cgroup_uncharge_end();\n\t\tindex++;\n\t}\n\tcleancache_invalidate_inode(mapping);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,31 +1,50 @@\n void truncate_inode_pages_range(struct address_space *mapping,\n \t\t\t\tloff_t lstart, loff_t lend)\n {\n-\tconst pgoff_t start = (lstart + PAGE_CACHE_SIZE-1) >> PAGE_CACHE_SHIFT;\n-\tconst unsigned partial = lstart & (PAGE_CACHE_SIZE - 1);\n-\tstruct pagevec pvec;\n-\tpgoff_t index;\n-\tpgoff_t end;\n-\tint i;\n+\tpgoff_t\t\tstart;\t\t/* inclusive */\n+\tpgoff_t\t\tend;\t\t/* exclusive */\n+\tunsigned int\tpartial_start;\t/* inclusive */\n+\tunsigned int\tpartial_end;\t/* exclusive */\n+\tstruct pagevec\tpvec;\n+\tpgoff_t\t\tindex;\n+\tint\t\ti;\n \n \tcleancache_invalidate_inode(mapping);\n \tif (mapping->nrpages == 0)\n \t\treturn;\n \n-\tBUG_ON((lend & (PAGE_CACHE_SIZE - 1)) != (PAGE_CACHE_SIZE - 1));\n-\tend = (lend >> PAGE_CACHE_SHIFT);\n+\t/* Offsets within partial pages */\n+\tpartial_start = lstart & (PAGE_CACHE_SIZE - 1);\n+\tpartial_end = (lend + 1) & (PAGE_CACHE_SIZE - 1);\n+\n+\t/*\n+\t * 'start' and 'end' always covers the range of pages to be fully\n+\t * truncated. Partial pages are covered with 'partial_start' at the\n+\t * start of the range and 'partial_end' at the end of the range.\n+\t * Note that 'end' is exclusive while 'lend' is inclusive.\n+\t */\n+\tstart = (lstart + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n+\tif (lend == -1)\n+\t\t/*\n+\t\t * lend == -1 indicates end-of-file so we have to set 'end'\n+\t\t * to the highest possible pgoff_t and since the type is\n+\t\t * unsigned we're using -1.\n+\t\t */\n+\t\tend = -1;\n+\telse\n+\t\tend = (lend + 1) >> PAGE_CACHE_SHIFT;\n \n \tpagevec_init(&pvec, 0);\n \tindex = start;\n-\twhile (index <= end && pagevec_lookup(&pvec, mapping, index,\n-\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {\n+\twhile (index < end && pagevec_lookup(&pvec, mapping, index,\n+\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {\n \t\tmem_cgroup_uncharge_start();\n \t\tfor (i = 0; i < pagevec_count(&pvec); i++) {\n \t\t\tstruct page *page = pvec.pages[i];\n \n \t\t\t/* We rely upon deletion not changing page->index */\n \t\t\tindex = page->index;\n-\t\t\tif (index > end)\n+\t\t\tif (index >= end)\n \t\t\t\tbreak;\n \n \t\t\tif (!trylock_page(page))\n@@ -44,27 +63,56 @@\n \t\tindex++;\n \t}\n \n-\tif (partial) {\n+\tif (partial_start) {\n \t\tstruct page *page = find_lock_page(mapping, start - 1);\n \t\tif (page) {\n+\t\t\tunsigned int top = PAGE_CACHE_SIZE;\n+\t\t\tif (start > end) {\n+\t\t\t\t/* Truncation within a single page */\n+\t\t\t\ttop = partial_end;\n+\t\t\t\tpartial_end = 0;\n+\t\t\t}\n \t\t\twait_on_page_writeback(page);\n-\t\t\ttruncate_partial_page(page, partial);\n+\t\t\tzero_user_segment(page, partial_start, top);\n+\t\t\tcleancache_invalidate_page(mapping, page);\n+\t\t\tif (page_has_private(page))\n+\t\t\t\tdo_invalidatepage(page, partial_start,\n+\t\t\t\t\t\t  top - partial_start);\n \t\t\tunlock_page(page);\n \t\t\tpage_cache_release(page);\n \t\t}\n \t}\n+\tif (partial_end) {\n+\t\tstruct page *page = find_lock_page(mapping, end);\n+\t\tif (page) {\n+\t\t\twait_on_page_writeback(page);\n+\t\t\tzero_user_segment(page, 0, partial_end);\n+\t\t\tcleancache_invalidate_page(mapping, page);\n+\t\t\tif (page_has_private(page))\n+\t\t\t\tdo_invalidatepage(page, 0,\n+\t\t\t\t\t\t  partial_end);\n+\t\t\tunlock_page(page);\n+\t\t\tpage_cache_release(page);\n+\t\t}\n+\t}\n+\t/*\n+\t * If the truncation happened within a single page no pages\n+\t * will be released, just zeroed, so we can bail out now.\n+\t */\n+\tif (start >= end)\n+\t\treturn;\n \n \tindex = start;\n \tfor ( ; ; ) {\n \t\tcond_resched();\n \t\tif (!pagevec_lookup(&pvec, mapping, index,\n-\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {\n+\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {\n \t\t\tif (index == start)\n \t\t\t\tbreak;\n \t\t\tindex = start;\n \t\t\tcontinue;\n \t\t}\n-\t\tif (index == start && pvec.pages[0]->index > end) {\n+\t\tif (index == start && pvec.pages[0]->index >= end) {\n \t\t\tpagevec_release(&pvec);\n \t\t\tbreak;\n \t\t}\n@@ -74,7 +122,7 @@\n \n \t\t\t/* We rely upon deletion not changing page->index */\n \t\t\tindex = page->index;\n-\t\t\tif (index > end)\n+\t\t\tif (index >= end)\n \t\t\t\tbreak;\n \n \t\t\tlock_page(page);",
        "function_modified_lines": {
            "added": [
                "\tpgoff_t\t\tstart;\t\t/* inclusive */",
                "\tpgoff_t\t\tend;\t\t/* exclusive */",
                "\tunsigned int\tpartial_start;\t/* inclusive */",
                "\tunsigned int\tpartial_end;\t/* exclusive */",
                "\tstruct pagevec\tpvec;",
                "\tpgoff_t\t\tindex;",
                "\tint\t\ti;",
                "\t/* Offsets within partial pages */",
                "\tpartial_start = lstart & (PAGE_CACHE_SIZE - 1);",
                "\tpartial_end = (lend + 1) & (PAGE_CACHE_SIZE - 1);",
                "",
                "\t/*",
                "\t * 'start' and 'end' always covers the range of pages to be fully",
                "\t * truncated. Partial pages are covered with 'partial_start' at the",
                "\t * start of the range and 'partial_end' at the end of the range.",
                "\t * Note that 'end' is exclusive while 'lend' is inclusive.",
                "\t */",
                "\tstart = (lstart + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;",
                "\tif (lend == -1)",
                "\t\t/*",
                "\t\t * lend == -1 indicates end-of-file so we have to set 'end'",
                "\t\t * to the highest possible pgoff_t and since the type is",
                "\t\t * unsigned we're using -1.",
                "\t\t */",
                "\t\tend = -1;",
                "\telse",
                "\t\tend = (lend + 1) >> PAGE_CACHE_SHIFT;",
                "\twhile (index < end && pagevec_lookup(&pvec, mapping, index,",
                "\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {",
                "\t\t\tif (index >= end)",
                "\tif (partial_start) {",
                "\t\t\tunsigned int top = PAGE_CACHE_SIZE;",
                "\t\t\tif (start > end) {",
                "\t\t\t\t/* Truncation within a single page */",
                "\t\t\t\ttop = partial_end;",
                "\t\t\t\tpartial_end = 0;",
                "\t\t\t}",
                "\t\t\tzero_user_segment(page, partial_start, top);",
                "\t\t\tcleancache_invalidate_page(mapping, page);",
                "\t\t\tif (page_has_private(page))",
                "\t\t\t\tdo_invalidatepage(page, partial_start,",
                "\t\t\t\t\t\t  top - partial_start);",
                "\tif (partial_end) {",
                "\t\tstruct page *page = find_lock_page(mapping, end);",
                "\t\tif (page) {",
                "\t\t\twait_on_page_writeback(page);",
                "\t\t\tzero_user_segment(page, 0, partial_end);",
                "\t\t\tcleancache_invalidate_page(mapping, page);",
                "\t\t\tif (page_has_private(page))",
                "\t\t\t\tdo_invalidatepage(page, 0,",
                "\t\t\t\t\t\t  partial_end);",
                "\t\t\tunlock_page(page);",
                "\t\t\tpage_cache_release(page);",
                "\t\t}",
                "\t}",
                "\t/*",
                "\t * If the truncation happened within a single page no pages",
                "\t * will be released, just zeroed, so we can bail out now.",
                "\t */",
                "\tif (start >= end)",
                "\t\treturn;",
                "\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE))) {",
                "\t\tif (index == start && pvec.pages[0]->index >= end) {",
                "\t\t\tif (index >= end)"
            ],
            "deleted": [
                "\tconst pgoff_t start = (lstart + PAGE_CACHE_SIZE-1) >> PAGE_CACHE_SHIFT;",
                "\tconst unsigned partial = lstart & (PAGE_CACHE_SIZE - 1);",
                "\tstruct pagevec pvec;",
                "\tpgoff_t index;",
                "\tpgoff_t end;",
                "\tint i;",
                "\tBUG_ON((lend & (PAGE_CACHE_SIZE - 1)) != (PAGE_CACHE_SIZE - 1));",
                "\tend = (lend >> PAGE_CACHE_SHIFT);",
                "\twhile (index <= end && pagevec_lookup(&pvec, mapping, index,",
                "\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {",
                "\t\t\tif (index > end)",
                "\tif (partial) {",
                "\t\t\ttruncate_partial_page(page, partial);",
                "\t\t\tmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {",
                "\t\tif (index == start && pvec.pages[0]->index > end) {",
                "\t\t\tif (index > end)"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A non-privileged user is able to mount a fuse filesystem on RHEL 6 or 7 and crash a system if an application punches a hole in a file that does not end aligned to a page boundary.",
        "id": 1296
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int x25_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t       struct msghdr *msg, size_t size,\n\t\t       int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct x25_sock *x25 = x25_sk(sk);\n\tstruct sockaddr_x25 *sx25 = (struct sockaddr_x25 *)msg->msg_name;\n\tsize_t copied;\n\tint qbit, header_len;\n\tstruct sk_buff *skb;\n\tunsigned char *asmptr;\n\tint rc = -ENOTCONN;\n\n\tlock_sock(sk);\n\n\tif (x25->neighbour == NULL)\n\t\tgoto out;\n\n\theader_len = x25->neighbour->extended ?\n\t\tX25_EXT_MIN_LEN : X25_STD_MIN_LEN;\n\n\t/*\n\t * This works for seqpacket too. The receiver has ordered the queue for\n\t * us! We do one quick check first though\n\t */\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\tgoto out;\n\n\tif (flags & MSG_OOB) {\n\t\trc = -EINVAL;\n\t\tif (sock_flag(sk, SOCK_URGINLINE) ||\n\t\t    !skb_peek(&x25->interrupt_in_queue))\n\t\t\tgoto out;\n\n\t\tskb = skb_dequeue(&x25->interrupt_in_queue);\n\n\t\tif (!pskb_may_pull(skb, X25_STD_MIN_LEN))\n\t\t\tgoto out_free_dgram;\n\n\t\tskb_pull(skb, X25_STD_MIN_LEN);\n\n\t\t/*\n\t\t *\tNo Q bit information on Interrupt data.\n\t\t */\n\t\tif (test_bit(X25_Q_BIT_FLAG, &x25->flags)) {\n\t\t\tasmptr  = skb_push(skb, 1);\n\t\t\t*asmptr = 0x00;\n\t\t}\n\n\t\tmsg->msg_flags |= MSG_OOB;\n\t} else {\n\t\t/* Now we can treat all alike */\n\t\trelease_sock(sk);\n\t\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\t\tflags & MSG_DONTWAIT, &rc);\n\t\tlock_sock(sk);\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tif (!pskb_may_pull(skb, header_len))\n\t\t\tgoto out_free_dgram;\n\n\t\tqbit = (skb->data[0] & X25_Q_BIT) == X25_Q_BIT;\n\n\t\tskb_pull(skb, header_len);\n\n\t\tif (test_bit(X25_Q_BIT_FLAG, &x25->flags)) {\n\t\t\tasmptr  = skb_push(skb, 1);\n\t\t\t*asmptr = qbit;\n\t\t}\n\t}\n\n\tskb_reset_transport_header(skb);\n\tcopied = skb->len;\n\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\t/* Currently, each datagram always contains a complete record */\n\tmsg->msg_flags |= MSG_EOR;\n\n\trc = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (rc)\n\t\tgoto out_free_dgram;\n\n\tif (sx25) {\n\t\tsx25->sx25_family = AF_X25;\n\t\tsx25->sx25_addr   = x25->dest_addr;\n\t}\n\n\tmsg->msg_namelen = sizeof(struct sockaddr_x25);\n\n\tx25_check_rbuf(sk);\n\trc = copied;\nout_free_dgram:\n\tskb_free_datagram(sk, skb);\nout:\n\trelease_sock(sk);\n\treturn rc;\n}",
        "code_after_change": "static int x25_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t       struct msghdr *msg, size_t size,\n\t\t       int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct x25_sock *x25 = x25_sk(sk);\n\tstruct sockaddr_x25 *sx25 = (struct sockaddr_x25 *)msg->msg_name;\n\tsize_t copied;\n\tint qbit, header_len;\n\tstruct sk_buff *skb;\n\tunsigned char *asmptr;\n\tint rc = -ENOTCONN;\n\n\tlock_sock(sk);\n\n\tif (x25->neighbour == NULL)\n\t\tgoto out;\n\n\theader_len = x25->neighbour->extended ?\n\t\tX25_EXT_MIN_LEN : X25_STD_MIN_LEN;\n\n\t/*\n\t * This works for seqpacket too. The receiver has ordered the queue for\n\t * us! We do one quick check first though\n\t */\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\tgoto out;\n\n\tif (flags & MSG_OOB) {\n\t\trc = -EINVAL;\n\t\tif (sock_flag(sk, SOCK_URGINLINE) ||\n\t\t    !skb_peek(&x25->interrupt_in_queue))\n\t\t\tgoto out;\n\n\t\tskb = skb_dequeue(&x25->interrupt_in_queue);\n\n\t\tif (!pskb_may_pull(skb, X25_STD_MIN_LEN))\n\t\t\tgoto out_free_dgram;\n\n\t\tskb_pull(skb, X25_STD_MIN_LEN);\n\n\t\t/*\n\t\t *\tNo Q bit information on Interrupt data.\n\t\t */\n\t\tif (test_bit(X25_Q_BIT_FLAG, &x25->flags)) {\n\t\t\tasmptr  = skb_push(skb, 1);\n\t\t\t*asmptr = 0x00;\n\t\t}\n\n\t\tmsg->msg_flags |= MSG_OOB;\n\t} else {\n\t\t/* Now we can treat all alike */\n\t\trelease_sock(sk);\n\t\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\t\tflags & MSG_DONTWAIT, &rc);\n\t\tlock_sock(sk);\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tif (!pskb_may_pull(skb, header_len))\n\t\t\tgoto out_free_dgram;\n\n\t\tqbit = (skb->data[0] & X25_Q_BIT) == X25_Q_BIT;\n\n\t\tskb_pull(skb, header_len);\n\n\t\tif (test_bit(X25_Q_BIT_FLAG, &x25->flags)) {\n\t\t\tasmptr  = skb_push(skb, 1);\n\t\t\t*asmptr = qbit;\n\t\t}\n\t}\n\n\tskb_reset_transport_header(skb);\n\tcopied = skb->len;\n\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\t/* Currently, each datagram always contains a complete record */\n\tmsg->msg_flags |= MSG_EOR;\n\n\trc = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (rc)\n\t\tgoto out_free_dgram;\n\n\tif (sx25) {\n\t\tsx25->sx25_family = AF_X25;\n\t\tsx25->sx25_addr   = x25->dest_addr;\n\t\tmsg->msg_namelen = sizeof(*sx25);\n\t}\n\n\tx25_check_rbuf(sk);\n\trc = copied;\nout_free_dgram:\n\tskb_free_datagram(sk, skb);\nout:\n\trelease_sock(sk);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -88,9 +88,8 @@\n \tif (sx25) {\n \t\tsx25->sx25_family = AF_X25;\n \t\tsx25->sx25_addr   = x25->dest_addr;\n+\t\tmsg->msg_namelen = sizeof(*sx25);\n \t}\n-\n-\tmsg->msg_namelen = sizeof(struct sockaddr_x25);\n \n \tx25_check_rbuf(sk);\n \trc = copied;",
        "function_modified_lines": {
            "added": [
                "\t\tmsg->msg_namelen = sizeof(*sx25);"
            ],
            "deleted": [
                "",
                "\tmsg->msg_namelen = sizeof(struct sockaddr_x25);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 406
    },
    {
        "cve_id": "CVE-2017-17862",
        "code_before_change": "int bpf_check(struct bpf_prog **prog, union bpf_attr *attr)\n{\n\tstruct bpf_verifier_env *env;\n\tstruct bpf_verifer_log *log;\n\tint ret = -EINVAL;\n\n\t/* no program is valid */\n\tif (ARRAY_SIZE(bpf_verifier_ops) == 0)\n\t\treturn -EINVAL;\n\n\t/* 'struct bpf_verifier_env' can be global, but since it's not small,\n\t * allocate/free it every time bpf_check() is called\n\t */\n\tenv = kzalloc(sizeof(struct bpf_verifier_env), GFP_KERNEL);\n\tif (!env)\n\t\treturn -ENOMEM;\n\tlog = &env->log;\n\n\tenv->insn_aux_data = vzalloc(sizeof(struct bpf_insn_aux_data) *\n\t\t\t\t     (*prog)->len);\n\tret = -ENOMEM;\n\tif (!env->insn_aux_data)\n\t\tgoto err_free_env;\n\tenv->prog = *prog;\n\tenv->ops = bpf_verifier_ops[env->prog->type];\n\n\t/* grab the mutex to protect few globals used by verifier */\n\tmutex_lock(&bpf_verifier_lock);\n\n\tif (attr->log_level || attr->log_buf || attr->log_size) {\n\t\t/* user requested verbose verifier output\n\t\t * and supplied buffer to store the verification trace\n\t\t */\n\t\tlog->level = attr->log_level;\n\t\tlog->ubuf = (char __user *) (unsigned long) attr->log_buf;\n\t\tlog->len_total = attr->log_size;\n\n\t\tret = -EINVAL;\n\t\t/* log attributes have to be sane */\n\t\tif (log->len_total < 128 || log->len_total > UINT_MAX >> 8 ||\n\t\t    !log->level || !log->ubuf)\n\t\t\tgoto err_unlock;\n\t}\n\n\tenv->strict_alignment = !!(attr->prog_flags & BPF_F_STRICT_ALIGNMENT);\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))\n\t\tenv->strict_alignment = true;\n\n\tif (env->prog->aux->offload) {\n\t\tret = bpf_prog_offload_verifier_prep(env);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\t}\n\n\tret = replace_map_fd_with_map_ptr(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->explored_states = kcalloc(env->prog->len,\n\t\t\t\t       sizeof(struct bpf_verifier_state_list *),\n\t\t\t\t       GFP_USER);\n\tret = -ENOMEM;\n\tif (!env->explored_states)\n\t\tgoto skip_full_check;\n\n\tret = check_cfg(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->allow_ptr_leaks = capable(CAP_SYS_ADMIN);\n\n\tret = do_check(env);\n\tif (env->cur_state) {\n\t\tfree_verifier_state(env->cur_state, true);\n\t\tenv->cur_state = NULL;\n\t}\n\nskip_full_check:\n\twhile (!pop_stack(env, NULL, NULL));\n\tfree_states(env);\n\n\tif (ret == 0)\n\t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n\t\tret = convert_ctx_accesses(env);\n\n\tif (ret == 0)\n\t\tret = fixup_bpf_calls(env);\n\n\tif (log->level && bpf_verifier_log_full(log))\n\t\tret = -ENOSPC;\n\tif (log->level && !log->ubuf) {\n\t\tret = -EFAULT;\n\t\tgoto err_release_maps;\n\t}\n\n\tif (ret == 0 && env->used_map_cnt) {\n\t\t/* if program passed verifier, update used_maps in bpf_prog_info */\n\t\tenv->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_maps[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!env->prog->aux->used_maps) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_maps, env->used_maps,\n\t\t       sizeof(env->used_maps[0]) * env->used_map_cnt);\n\t\tenv->prog->aux->used_map_cnt = env->used_map_cnt;\n\n\t\t/* program is valid. Convert pseudo bpf_ld_imm64 into generic\n\t\t * bpf_ld_imm64 instructions\n\t\t */\n\t\tconvert_pseudo_ld_imm64(env);\n\t}\n\nerr_release_maps:\n\tif (!env->prog->aux->used_maps)\n\t\t/* if we didn't copy map pointers into bpf_prog_info, release\n\t\t * them now. Otherwise free_bpf_prog_info() will release them.\n\t\t */\n\t\trelease_maps(env);\n\t*prog = env->prog;\nerr_unlock:\n\tmutex_unlock(&bpf_verifier_lock);\n\tvfree(env->insn_aux_data);\nerr_free_env:\n\tkfree(env);\n\treturn ret;\n}",
        "code_after_change": "int bpf_check(struct bpf_prog **prog, union bpf_attr *attr)\n{\n\tstruct bpf_verifier_env *env;\n\tstruct bpf_verifer_log *log;\n\tint ret = -EINVAL;\n\n\t/* no program is valid */\n\tif (ARRAY_SIZE(bpf_verifier_ops) == 0)\n\t\treturn -EINVAL;\n\n\t/* 'struct bpf_verifier_env' can be global, but since it's not small,\n\t * allocate/free it every time bpf_check() is called\n\t */\n\tenv = kzalloc(sizeof(struct bpf_verifier_env), GFP_KERNEL);\n\tif (!env)\n\t\treturn -ENOMEM;\n\tlog = &env->log;\n\n\tenv->insn_aux_data = vzalloc(sizeof(struct bpf_insn_aux_data) *\n\t\t\t\t     (*prog)->len);\n\tret = -ENOMEM;\n\tif (!env->insn_aux_data)\n\t\tgoto err_free_env;\n\tenv->prog = *prog;\n\tenv->ops = bpf_verifier_ops[env->prog->type];\n\n\t/* grab the mutex to protect few globals used by verifier */\n\tmutex_lock(&bpf_verifier_lock);\n\n\tif (attr->log_level || attr->log_buf || attr->log_size) {\n\t\t/* user requested verbose verifier output\n\t\t * and supplied buffer to store the verification trace\n\t\t */\n\t\tlog->level = attr->log_level;\n\t\tlog->ubuf = (char __user *) (unsigned long) attr->log_buf;\n\t\tlog->len_total = attr->log_size;\n\n\t\tret = -EINVAL;\n\t\t/* log attributes have to be sane */\n\t\tif (log->len_total < 128 || log->len_total > UINT_MAX >> 8 ||\n\t\t    !log->level || !log->ubuf)\n\t\t\tgoto err_unlock;\n\t}\n\n\tenv->strict_alignment = !!(attr->prog_flags & BPF_F_STRICT_ALIGNMENT);\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))\n\t\tenv->strict_alignment = true;\n\n\tif (env->prog->aux->offload) {\n\t\tret = bpf_prog_offload_verifier_prep(env);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\t}\n\n\tret = replace_map_fd_with_map_ptr(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->explored_states = kcalloc(env->prog->len,\n\t\t\t\t       sizeof(struct bpf_verifier_state_list *),\n\t\t\t\t       GFP_USER);\n\tret = -ENOMEM;\n\tif (!env->explored_states)\n\t\tgoto skip_full_check;\n\n\tret = check_cfg(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tenv->allow_ptr_leaks = capable(CAP_SYS_ADMIN);\n\n\tret = do_check(env);\n\tif (env->cur_state) {\n\t\tfree_verifier_state(env->cur_state, true);\n\t\tenv->cur_state = NULL;\n\t}\n\nskip_full_check:\n\twhile (!pop_stack(env, NULL, NULL));\n\tfree_states(env);\n\n\tif (ret == 0)\n\t\tsanitize_dead_code(env);\n\n\tif (ret == 0)\n\t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n\t\tret = convert_ctx_accesses(env);\n\n\tif (ret == 0)\n\t\tret = fixup_bpf_calls(env);\n\n\tif (log->level && bpf_verifier_log_full(log))\n\t\tret = -ENOSPC;\n\tif (log->level && !log->ubuf) {\n\t\tret = -EFAULT;\n\t\tgoto err_release_maps;\n\t}\n\n\tif (ret == 0 && env->used_map_cnt) {\n\t\t/* if program passed verifier, update used_maps in bpf_prog_info */\n\t\tenv->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_maps[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!env->prog->aux->used_maps) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_maps, env->used_maps,\n\t\t       sizeof(env->used_maps[0]) * env->used_map_cnt);\n\t\tenv->prog->aux->used_map_cnt = env->used_map_cnt;\n\n\t\t/* program is valid. Convert pseudo bpf_ld_imm64 into generic\n\t\t * bpf_ld_imm64 instructions\n\t\t */\n\t\tconvert_pseudo_ld_imm64(env);\n\t}\n\nerr_release_maps:\n\tif (!env->prog->aux->used_maps)\n\t\t/* if we didn't copy map pointers into bpf_prog_info, release\n\t\t * them now. Otherwise free_bpf_prog_info() will release them.\n\t\t */\n\t\trelease_maps(env);\n\t*prog = env->prog;\nerr_unlock:\n\tmutex_unlock(&bpf_verifier_lock);\n\tvfree(env->insn_aux_data);\nerr_free_env:\n\tkfree(env);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -80,6 +80,9 @@\n \tfree_states(env);\n \n \tif (ret == 0)\n+\t\tsanitize_dead_code(env);\n+\n+\tif (ret == 0)\n \t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n \t\tret = convert_ctx_accesses(env);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tsanitize_dead_code(env);",
                "",
                "\tif (ret == 0)"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "kernel/bpf/verifier.c in the Linux kernel through 4.14.8 ignores unreachable code, even though it would still be processed by JIT compilers. This behavior, also considered an improper branch-pruning logic issue, could possibly be used by local users for denial of service.",
        "id": 1384
    },
    {
        "cve_id": "CVE-2013-2897",
        "code_before_change": "static void mt_feature_mapping(struct hid_device *hdev,\n\t\tstruct hid_field *field, struct hid_usage *usage)\n{\n\tstruct mt_device *td = hid_get_drvdata(hdev);\n\tint i;\n\n\tswitch (usage->hid) {\n\tcase HID_DG_INPUTMODE:\n\t\ttd->inputmode = field->report->id;\n\t\ttd->inputmode_index = 0; /* has to be updated below */\n\n\t\tfor (i=0; i < field->maxusage; i++) {\n\t\t\tif (field->usage[i].hid == usage->hid) {\n\t\t\t\ttd->inputmode_index = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tbreak;\n\tcase HID_DG_CONTACTMAX:\n\t\ttd->maxcontact_report_id = field->report->id;\n\t\ttd->maxcontacts = field->value[0];\n\t\tif (!td->maxcontacts &&\n\t\t    field->logical_maximum <= MT_MAX_MAXCONTACT)\n\t\t\ttd->maxcontacts = field->logical_maximum;\n\t\tif (td->mtclass.maxcontacts)\n\t\t\t/* check if the maxcontacts is given by the class */\n\t\t\ttd->maxcontacts = td->mtclass.maxcontacts;\n\n\t\tbreak;\n\t}\n}",
        "code_after_change": "static void mt_feature_mapping(struct hid_device *hdev,\n\t\tstruct hid_field *field, struct hid_usage *usage)\n{\n\tstruct mt_device *td = hid_get_drvdata(hdev);\n\n\tswitch (usage->hid) {\n\tcase HID_DG_INPUTMODE:\n\t\t/* Ignore if value index is out of bounds. */\n\t\tif (usage->usage_index >= field->report_count) {\n\t\t\tdev_err(&hdev->dev, \"HID_DG_INPUTMODE out of range\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\ttd->inputmode = field->report->id;\n\t\ttd->inputmode_index = usage->usage_index;\n\n\t\tbreak;\n\tcase HID_DG_CONTACTMAX:\n\t\ttd->maxcontact_report_id = field->report->id;\n\t\ttd->maxcontacts = field->value[0];\n\t\tif (!td->maxcontacts &&\n\t\t    field->logical_maximum <= MT_MAX_MAXCONTACT)\n\t\t\ttd->maxcontacts = field->logical_maximum;\n\t\tif (td->mtclass.maxcontacts)\n\t\t\t/* check if the maxcontacts is given by the class */\n\t\t\ttd->maxcontacts = td->mtclass.maxcontacts;\n\n\t\tbreak;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,19 +2,17 @@\n \t\tstruct hid_field *field, struct hid_usage *usage)\n {\n \tstruct mt_device *td = hid_get_drvdata(hdev);\n-\tint i;\n \n \tswitch (usage->hid) {\n \tcase HID_DG_INPUTMODE:\n+\t\t/* Ignore if value index is out of bounds. */\n+\t\tif (usage->usage_index >= field->report_count) {\n+\t\t\tdev_err(&hdev->dev, \"HID_DG_INPUTMODE out of range\\n\");\n+\t\t\tbreak;\n+\t\t}\n+\n \t\ttd->inputmode = field->report->id;\n-\t\ttd->inputmode_index = 0; /* has to be updated below */\n-\n-\t\tfor (i=0; i < field->maxusage; i++) {\n-\t\t\tif (field->usage[i].hid == usage->hid) {\n-\t\t\t\ttd->inputmode_index = i;\n-\t\t\t\tbreak;\n-\t\t\t}\n-\t\t}\n+\t\ttd->inputmode_index = usage->usage_index;\n \n \t\tbreak;\n \tcase HID_DG_CONTACTMAX:",
        "function_modified_lines": {
            "added": [
                "\t\t/* Ignore if value index is out of bounds. */",
                "\t\tif (usage->usage_index >= field->report_count) {",
                "\t\t\tdev_err(&hdev->dev, \"HID_DG_INPUTMODE out of range\\n\");",
                "\t\t\tbreak;",
                "\t\t}",
                "",
                "\t\ttd->inputmode_index = usage->usage_index;"
            ],
            "deleted": [
                "\tint i;",
                "\t\ttd->inputmode_index = 0; /* has to be updated below */",
                "",
                "\t\tfor (i=0; i < field->maxusage; i++) {",
                "\t\t\tif (field->usage[i].hid == usage->hid) {",
                "\t\t\t\ttd->inputmode_index = i;",
                "\t\t\t\tbreak;",
                "\t\t\t}",
                "\t\t}"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Multiple array index errors in drivers/hid/hid-multitouch.c in the Human Interface Device (HID) subsystem in the Linux kernel through 3.11, when CONFIG_HID_MULTITOUCH is enabled, allow physically proximate attackers to cause a denial of service (heap memory corruption, or NULL pointer dereference and OOPS) via a crafted device.",
        "id": 256
    },
    {
        "cve_id": "CVE-2012-6647",
        "code_before_change": "static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,\n\t\t\t\t u32 val, ktime_t *abs_time, u32 bitset,\n\t\t\t\t u32 __user *uaddr2)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct rt_mutex *pi_mutex = NULL;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key2 = FUTEX_KEY_INIT;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\n\t/*\n\t * The waiter is allocated on our stack, manipulated by the requeue\n\t * code while we sleep on uaddr.\n\t */\n\tdebug_rt_mutex_init_waiter(&rt_waiter);\n\trt_waiter.task = NULL;\n\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\tq.bitset = bitset;\n\tq.rt_waiter = &rt_waiter;\n\tq.requeue_pi_key = &key2;\n\n\t/*\n\t * Prepare to wait on uaddr. On success, increments q.key (key1) ref\n\t * count.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out_key2;\n\n\t/* Queue the futex_q, drop the hb lock, wait for wakeup. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\tspin_lock(&hb->lock);\n\tret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);\n\tspin_unlock(&hb->lock);\n\tif (ret)\n\t\tgoto out_put_keys;\n\n\t/*\n\t * In order for us to be here, we know our q.key == key2, and since\n\t * we took the hb->lock above, we also know that futex_requeue() has\n\t * completed and we no longer have to concern ourselves with a wakeup\n\t * race with the atomic proxy lock acquisition by the requeue code. The\n\t * futex_requeue dropped our key1 reference and incremented our key2\n\t * reference count.\n\t */\n\n\t/* Check if the requeue code acquired the second futex for us. */\n\tif (!q.rt_waiter) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case.\n\t\t */\n\t\tif (q.pi_state && (q.pi_state->owner != current)) {\n\t\t\tspin_lock(q.lock_ptr);\n\t\t\tret = fixup_pi_state_owner(uaddr2, &q, current);\n\t\t\tspin_unlock(q.lock_ptr);\n\t\t}\n\t} else {\n\t\t/*\n\t\t * We have been woken up by futex_unlock_pi(), a timeout, or a\n\t\t * signal.  futex_unlock_pi() will not destroy the lock_ptr nor\n\t\t * the pi_state.\n\t\t */\n\t\tWARN_ON(!q.pi_state);\n\t\tpi_mutex = &q.pi_state->pi_mutex;\n\t\tret = rt_mutex_finish_proxy_lock(pi_mutex, to, &rt_waiter, 1);\n\t\tdebug_rt_mutex_free_waiter(&rt_waiter);\n\n\t\tspin_lock(q.lock_ptr);\n\t\t/*\n\t\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t\t * haven't already.\n\t\t */\n\t\tres = fixup_owner(uaddr2, &q, !ret);\n\t\t/*\n\t\t * If fixup_owner() returned an error, proprogate that.  If it\n\t\t * acquired the lock, clear -ETIMEDOUT or -EINTR.\n\t\t */\n\t\tif (res)\n\t\t\tret = (res < 0) ? res : 0;\n\n\t\t/* Unqueue and drop the lock. */\n\t\tunqueue_me_pi(&q);\n\t}\n\n\t/*\n\t * If fixup_pi_state_owner() faulted and was unable to handle the\n\t * fault, unlock the rt_mutex and return the fault to userspace.\n\t */\n\tif (ret == -EFAULT) {\n\t\tif (pi_mutex && rt_mutex_owner(pi_mutex) == current)\n\t\t\trt_mutex_unlock(pi_mutex);\n\t} else if (ret == -EINTR) {\n\t\t/*\n\t\t * We've already been requeued, but cannot restart by calling\n\t\t * futex_lock_pi() directly. We could restart this syscall, but\n\t\t * it would detect that the user space \"val\" changed and return\n\t\t * -EWOULDBLOCK.  Save the overhead of the restart and return\n\t\t * -EWOULDBLOCK directly.\n\t\t */\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout_put_keys:\n\tput_futex_key(&q.key);\nout_key2:\n\tput_futex_key(&key2);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}",
        "code_after_change": "static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,\n\t\t\t\t u32 val, ktime_t *abs_time, u32 bitset,\n\t\t\t\t u32 __user *uaddr2)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct rt_mutex *pi_mutex = NULL;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key2 = FUTEX_KEY_INIT;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (uaddr == uaddr2)\n\t\treturn -EINVAL;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\n\t/*\n\t * The waiter is allocated on our stack, manipulated by the requeue\n\t * code while we sleep on uaddr.\n\t */\n\tdebug_rt_mutex_init_waiter(&rt_waiter);\n\trt_waiter.task = NULL;\n\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\tq.bitset = bitset;\n\tq.rt_waiter = &rt_waiter;\n\tq.requeue_pi_key = &key2;\n\n\t/*\n\t * Prepare to wait on uaddr. On success, increments q.key (key1) ref\n\t * count.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out_key2;\n\n\t/* Queue the futex_q, drop the hb lock, wait for wakeup. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\tspin_lock(&hb->lock);\n\tret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);\n\tspin_unlock(&hb->lock);\n\tif (ret)\n\t\tgoto out_put_keys;\n\n\t/*\n\t * In order for us to be here, we know our q.key == key2, and since\n\t * we took the hb->lock above, we also know that futex_requeue() has\n\t * completed and we no longer have to concern ourselves with a wakeup\n\t * race with the atomic proxy lock acquisition by the requeue code. The\n\t * futex_requeue dropped our key1 reference and incremented our key2\n\t * reference count.\n\t */\n\n\t/* Check if the requeue code acquired the second futex for us. */\n\tif (!q.rt_waiter) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case.\n\t\t */\n\t\tif (q.pi_state && (q.pi_state->owner != current)) {\n\t\t\tspin_lock(q.lock_ptr);\n\t\t\tret = fixup_pi_state_owner(uaddr2, &q, current);\n\t\t\tspin_unlock(q.lock_ptr);\n\t\t}\n\t} else {\n\t\t/*\n\t\t * We have been woken up by futex_unlock_pi(), a timeout, or a\n\t\t * signal.  futex_unlock_pi() will not destroy the lock_ptr nor\n\t\t * the pi_state.\n\t\t */\n\t\tWARN_ON(!q.pi_state);\n\t\tpi_mutex = &q.pi_state->pi_mutex;\n\t\tret = rt_mutex_finish_proxy_lock(pi_mutex, to, &rt_waiter, 1);\n\t\tdebug_rt_mutex_free_waiter(&rt_waiter);\n\n\t\tspin_lock(q.lock_ptr);\n\t\t/*\n\t\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t\t * haven't already.\n\t\t */\n\t\tres = fixup_owner(uaddr2, &q, !ret);\n\t\t/*\n\t\t * If fixup_owner() returned an error, proprogate that.  If it\n\t\t * acquired the lock, clear -ETIMEDOUT or -EINTR.\n\t\t */\n\t\tif (res)\n\t\t\tret = (res < 0) ? res : 0;\n\n\t\t/* Unqueue and drop the lock. */\n\t\tunqueue_me_pi(&q);\n\t}\n\n\t/*\n\t * If fixup_pi_state_owner() faulted and was unable to handle the\n\t * fault, unlock the rt_mutex and return the fault to userspace.\n\t */\n\tif (ret == -EFAULT) {\n\t\tif (pi_mutex && rt_mutex_owner(pi_mutex) == current)\n\t\t\trt_mutex_unlock(pi_mutex);\n\t} else if (ret == -EINTR) {\n\t\t/*\n\t\t * We've already been requeued, but cannot restart by calling\n\t\t * futex_lock_pi() directly. We could restart this syscall, but\n\t\t * it would detect that the user space \"val\" changed and return\n\t\t * -EWOULDBLOCK.  Save the overhead of the restart and return\n\t\t * -EWOULDBLOCK directly.\n\t\t */\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout_put_keys:\n\tput_futex_key(&q.key);\nout_key2:\n\tput_futex_key(&key2);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,9 @@\n \tunion futex_key key2 = FUTEX_KEY_INIT;\n \tstruct futex_q q = futex_q_init;\n \tint res, ret;\n+\n+\tif (uaddr == uaddr2)\n+\t\treturn -EINVAL;\n \n \tif (!bitset)\n \t\treturn -EINVAL;",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (uaddr == uaddr2)",
                "\t\treturn -EINVAL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The futex_wait_requeue_pi function in kernel/futex.c in the Linux kernel before 3.5.1 does not ensure that calls have two different futex addresses, which allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact via a crafted FUTEX_WAIT_REQUEUE_PI command.",
        "id": 135
    },
    {
        "cve_id": "CVE-2017-5123",
        "code_before_change": "\nCOMPAT_SYSCALL_DEFINE5(waitid,\n\t\tint, which, compat_pid_t, pid,\n\t\tstruct compat_siginfo __user *, infop, int, options,\n\t\tstruct compat_rusage __user *, uru)\n{\n\tstruct rusage ru;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, pid, &info, options, uru ? &ru : NULL);\n\tint signo = 0;\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (uru) {\n\t\t\t/* kernel_waitid() overwrites everything in ru */\n\t\t\tif (COMPAT_USE_64BIT_TIME)\n\t\t\t\terr = copy_to_user(uru, &ru, sizeof(ru));\n\t\t\telse\n\t\t\t\terr = put_compat_rusage(&ru, uru);\n\t\t\tif (err)\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (!infop)\n\t\treturn err;\n\n\tuser_access_begin();\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "code_after_change": "\nCOMPAT_SYSCALL_DEFINE5(waitid,\n\t\tint, which, compat_pid_t, pid,\n\t\tstruct compat_siginfo __user *, infop, int, options,\n\t\tstruct compat_rusage __user *, uru)\n{\n\tstruct rusage ru;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, pid, &info, options, uru ? &ru : NULL);\n\tint signo = 0;\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (uru) {\n\t\t\t/* kernel_waitid() overwrites everything in ru */\n\t\t\tif (COMPAT_USE_64BIT_TIME)\n\t\t\t\terr = copy_to_user(uru, &ru, sizeof(ru));\n\t\t\telse\n\t\t\t\terr = put_compat_rusage(&ru, uru);\n\t\t\tif (err)\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (!infop)\n\t\treturn err;\n\n\tif (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))\n\t\tgoto Efault;\n\n\tuser_access_begin();\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,6 +25,9 @@\n \tif (!infop)\n \t\treturn err;\n \n+\tif (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))\n+\t\tgoto Efault;\n+\n \tuser_access_begin();\n \tunsafe_put_user(signo, &infop->si_signo, Efault);\n \tunsafe_put_user(0, &infop->si_errno, Efault);",
        "function_modified_lines": {
            "added": [
                "\tif (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))",
                "\t\tgoto Efault;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Insufficient data validation in waitid allowed an user to escape sandboxes on Linux.",
        "id": 1456
    },
    {
        "cve_id": "CVE-2013-0216",
        "code_before_change": "static void netbk_tx_err(struct xenvif *vif,\n\t\t\t struct xen_netif_tx_request *txp, RING_IDX end)\n{\n\tRING_IDX cons = vif->tx.req_cons;\n\n\tdo {\n\t\tmake_tx_response(vif, txp, XEN_NETIF_RSP_ERROR);\n\t\tif (cons >= end)\n\t\t\tbreak;\n\t\ttxp = RING_GET_REQUEST(&vif->tx, cons++);\n\t} while (1);\n\tvif->tx.req_cons = cons;\n\txen_netbk_check_rx_xenvif(vif);\n\txenvif_put(vif);\n}",
        "code_after_change": "static void netbk_tx_err(struct xenvif *vif,\n\t\t\t struct xen_netif_tx_request *txp, RING_IDX end)\n{\n\tRING_IDX cons = vif->tx.req_cons;\n\n\tdo {\n\t\tmake_tx_response(vif, txp, XEN_NETIF_RSP_ERROR);\n\t\tif (cons == end)\n\t\t\tbreak;\n\t\ttxp = RING_GET_REQUEST(&vif->tx, cons++);\n\t} while (1);\n\tvif->tx.req_cons = cons;\n\txen_netbk_check_rx_xenvif(vif);\n\txenvif_put(vif);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \n \tdo {\n \t\tmake_tx_response(vif, txp, XEN_NETIF_RSP_ERROR);\n-\t\tif (cons >= end)\n+\t\tif (cons == end)\n \t\t\tbreak;\n \t\ttxp = RING_GET_REQUEST(&vif->tx, cons++);\n \t} while (1);",
        "function_modified_lines": {
            "added": [
                "\t\tif (cons == end)"
            ],
            "deleted": [
                "\t\tif (cons >= end)"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The Xen netback functionality in the Linux kernel before 3.7.8 allows guest OS users to cause a denial of service (loop) by triggering ring pointer corruption.",
        "id": 147
    },
    {
        "cve_id": "CVE-2016-2549",
        "code_before_change": "static int snd_hrtimer_stop(struct snd_timer *t)\n{\n\tstruct snd_hrtimer *stime = t->private_data;\n\tatomic_set(&stime->running, 0);\n\treturn 0;\n}",
        "code_after_change": "static int snd_hrtimer_stop(struct snd_timer *t)\n{\n\tstruct snd_hrtimer *stime = t->private_data;\n\tatomic_set(&stime->running, 0);\n\thrtimer_try_to_cancel(&stime->hrt);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,5 +2,6 @@\n {\n \tstruct snd_hrtimer *stime = t->private_data;\n \tatomic_set(&stime->running, 0);\n+\thrtimer_try_to_cancel(&stime->hrt);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\thrtimer_try_to_cancel(&stime->hrt);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "sound/core/hrtimer.c in the Linux kernel before 4.4.1 does not prevent recursive callback access, which allows local users to cause a denial of service (deadlock) via a crafted ioctl call.",
        "id": 948
    },
    {
        "cve_id": "CVE-2016-2143",
        "code_before_change": "static inline int init_new_context(struct task_struct *tsk,\n\t\t\t\t   struct mm_struct *mm)\n{\n\tcpumask_clear(&mm->context.cpu_attach_mask);\n\tatomic_set(&mm->context.attach_count, 0);\n\tmm->context.flush_mm = 0;\n\tmm->context.asce_bits = _ASCE_TABLE_LENGTH | _ASCE_USER_BITS;\n\tmm->context.asce_bits |= _ASCE_TYPE_REGION3;\n#ifdef CONFIG_PGSTE\n\tmm->context.alloc_pgste = page_table_allocate_pgste;\n\tmm->context.has_pgste = 0;\n\tmm->context.use_skey = 0;\n#endif\n\tmm->context.asce_limit = STACK_TOP_MAX;\n\tcrst_table_init((unsigned long *) mm->pgd, pgd_entry_type(mm));\n\treturn 0;\n}",
        "code_after_change": "static inline int init_new_context(struct task_struct *tsk,\n\t\t\t\t   struct mm_struct *mm)\n{\n\tspin_lock_init(&mm->context.list_lock);\n\tINIT_LIST_HEAD(&mm->context.pgtable_list);\n\tINIT_LIST_HEAD(&mm->context.gmap_list);\n\tcpumask_clear(&mm->context.cpu_attach_mask);\n\tatomic_set(&mm->context.attach_count, 0);\n\tmm->context.flush_mm = 0;\n#ifdef CONFIG_PGSTE\n\tmm->context.alloc_pgste = page_table_allocate_pgste;\n\tmm->context.has_pgste = 0;\n\tmm->context.use_skey = 0;\n#endif\n\tif (mm->context.asce_limit == 0) {\n\t\t/* context created by exec, set asce limit to 4TB */\n\t\tmm->context.asce_bits = _ASCE_TABLE_LENGTH |\n\t\t\t_ASCE_USER_BITS | _ASCE_TYPE_REGION3;\n\t\tmm->context.asce_limit = STACK_TOP_MAX;\n\t} else if (mm->context.asce_limit == (1UL << 31)) {\n\t\tmm_inc_nr_pmds(mm);\n\t}\n\tcrst_table_init((unsigned long *) mm->pgd, pgd_entry_type(mm));\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,17 +1,25 @@\n static inline int init_new_context(struct task_struct *tsk,\n \t\t\t\t   struct mm_struct *mm)\n {\n+\tspin_lock_init(&mm->context.list_lock);\n+\tINIT_LIST_HEAD(&mm->context.pgtable_list);\n+\tINIT_LIST_HEAD(&mm->context.gmap_list);\n \tcpumask_clear(&mm->context.cpu_attach_mask);\n \tatomic_set(&mm->context.attach_count, 0);\n \tmm->context.flush_mm = 0;\n-\tmm->context.asce_bits = _ASCE_TABLE_LENGTH | _ASCE_USER_BITS;\n-\tmm->context.asce_bits |= _ASCE_TYPE_REGION3;\n #ifdef CONFIG_PGSTE\n \tmm->context.alloc_pgste = page_table_allocate_pgste;\n \tmm->context.has_pgste = 0;\n \tmm->context.use_skey = 0;\n #endif\n-\tmm->context.asce_limit = STACK_TOP_MAX;\n+\tif (mm->context.asce_limit == 0) {\n+\t\t/* context created by exec, set asce limit to 4TB */\n+\t\tmm->context.asce_bits = _ASCE_TABLE_LENGTH |\n+\t\t\t_ASCE_USER_BITS | _ASCE_TYPE_REGION3;\n+\t\tmm->context.asce_limit = STACK_TOP_MAX;\n+\t} else if (mm->context.asce_limit == (1UL << 31)) {\n+\t\tmm_inc_nr_pmds(mm);\n+\t}\n \tcrst_table_init((unsigned long *) mm->pgd, pgd_entry_type(mm));\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tspin_lock_init(&mm->context.list_lock);",
                "\tINIT_LIST_HEAD(&mm->context.pgtable_list);",
                "\tINIT_LIST_HEAD(&mm->context.gmap_list);",
                "\tif (mm->context.asce_limit == 0) {",
                "\t\t/* context created by exec, set asce limit to 4TB */",
                "\t\tmm->context.asce_bits = _ASCE_TABLE_LENGTH |",
                "\t\t\t_ASCE_USER_BITS | _ASCE_TYPE_REGION3;",
                "\t\tmm->context.asce_limit = STACK_TOP_MAX;",
                "\t} else if (mm->context.asce_limit == (1UL << 31)) {",
                "\t\tmm_inc_nr_pmds(mm);",
                "\t}"
            ],
            "deleted": [
                "\tmm->context.asce_bits = _ASCE_TABLE_LENGTH | _ASCE_USER_BITS;",
                "\tmm->context.asce_bits |= _ASCE_TYPE_REGION3;",
                "\tmm->context.asce_limit = STACK_TOP_MAX;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The fork implementation in the Linux kernel before 4.5 on s390 platforms mishandles the case of four page-table levels, which allows local users to cause a denial of service (system crash) or possibly have unspecified other impact via a crafted application, related to arch/s390/include/asm/mmu_context.h and arch/s390/include/asm/pgalloc.h.",
        "id": 924
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int nr_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t      struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_ax25 *sax = (struct sockaddr_ax25 *)msg->msg_name;\n\tsize_t copied;\n\tstruct sk_buff *skb;\n\tint er;\n\n\t/*\n\t * This works for seqpacket too. The receiver has ordered the queue for\n\t * us! We do one quick check first though\n\t */\n\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\trelease_sock(sk);\n\t\treturn -ENOTCONN;\n\t}\n\n\t/* Now we can treat all alike */\n\tif ((skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT, flags & MSG_DONTWAIT, &er)) == NULL) {\n\t\trelease_sock(sk);\n\t\treturn er;\n\t}\n\n\tskb_reset_transport_header(skb);\n\tcopied     = skb->len;\n\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\ter = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (er < 0) {\n\t\tskb_free_datagram(sk, skb);\n\t\trelease_sock(sk);\n\t\treturn er;\n\t}\n\n\tif (sax != NULL) {\n\t\tmemset(sax, 0, sizeof(*sax));\n\t\tsax->sax25_family = AF_NETROM;\n\t\tskb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,\n\t\t\t      AX25_ADDR_LEN);\n\t}\n\n\tmsg->msg_namelen = sizeof(*sax);\n\n\tskb_free_datagram(sk, skb);\n\n\trelease_sock(sk);\n\treturn copied;\n}",
        "code_after_change": "static int nr_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t      struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_ax25 *sax = (struct sockaddr_ax25 *)msg->msg_name;\n\tsize_t copied;\n\tstruct sk_buff *skb;\n\tint er;\n\n\t/*\n\t * This works for seqpacket too. The receiver has ordered the queue for\n\t * us! We do one quick check first though\n\t */\n\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\trelease_sock(sk);\n\t\treturn -ENOTCONN;\n\t}\n\n\t/* Now we can treat all alike */\n\tif ((skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT, flags & MSG_DONTWAIT, &er)) == NULL) {\n\t\trelease_sock(sk);\n\t\treturn er;\n\t}\n\n\tskb_reset_transport_header(skb);\n\tcopied     = skb->len;\n\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\ter = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (er < 0) {\n\t\tskb_free_datagram(sk, skb);\n\t\trelease_sock(sk);\n\t\treturn er;\n\t}\n\n\tif (sax != NULL) {\n\t\tmemset(sax, 0, sizeof(*sax));\n\t\tsax->sax25_family = AF_NETROM;\n\t\tskb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,\n\t\t\t      AX25_ADDR_LEN);\n\t\tmsg->msg_namelen = sizeof(*sax);\n\t}\n\n\tskb_free_datagram(sk, skb);\n\n\trelease_sock(sk);\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -44,9 +44,8 @@\n \t\tsax->sax25_family = AF_NETROM;\n \t\tskb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,\n \t\t\t      AX25_ADDR_LEN);\n+\t\tmsg->msg_namelen = sizeof(*sax);\n \t}\n-\n-\tmsg->msg_namelen = sizeof(*sax);\n \n \tskb_free_datagram(sk, skb);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tmsg->msg_namelen = sizeof(*sax);"
            ],
            "deleted": [
                "",
                "\tmsg->msg_namelen = sizeof(*sax);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 390
    },
    {
        "cve_id": "CVE-2017-18509",
        "code_before_change": "int ip6_mroute_setsockopt(struct sock *sk, int optname, char __user *optval, unsigned int optlen)\n{\n\tint ret, parent = 0;\n\tstruct mif6ctl vif;\n\tstruct mf6cctl mfc;\n\tmifi_t mifi;\n\tstruct net *net = sock_net(sk);\n\tstruct mr6_table *mrt;\n\n\tmrt = ip6mr_get_table(net, raw6_sk(sk)->ip6mr_table ? : RT6_TABLE_DFLT);\n\tif (!mrt)\n\t\treturn -ENOENT;\n\n\tif (optname != MRT6_INIT) {\n\t\tif (sk != mrt->mroute6_sk && !ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tswitch (optname) {\n\tcase MRT6_INIT:\n\t\tif (sk->sk_type != SOCK_RAW ||\n\t\t    inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\n\t\treturn ip6mr_sk_init(mrt, sk);\n\n\tcase MRT6_DONE:\n\t\treturn ip6mr_sk_done(sk);\n\n\tcase MRT6_ADD_MIF:\n\t\tif (optlen < sizeof(vif))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&vif, optval, sizeof(vif)))\n\t\t\treturn -EFAULT;\n\t\tif (vif.mif6c_mifi >= MAXMIFS)\n\t\t\treturn -ENFILE;\n\t\trtnl_lock();\n\t\tret = mif6_add(net, mrt, &vif, sk == mrt->mroute6_sk);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\tcase MRT6_DEL_MIF:\n\t\tif (optlen < sizeof(mifi_t))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&mifi, optval, sizeof(mifi_t)))\n\t\t\treturn -EFAULT;\n\t\trtnl_lock();\n\t\tret = mif6_delete(mrt, mifi, NULL);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\t/*\n\t *\tManipulate the forwarding caches. These live\n\t *\tin a sort of kernel/user symbiosis.\n\t */\n\tcase MRT6_ADD_MFC:\n\tcase MRT6_DEL_MFC:\n\t\tparent = -1;\n\tcase MRT6_ADD_MFC_PROXY:\n\tcase MRT6_DEL_MFC_PROXY:\n\t\tif (optlen < sizeof(mfc))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&mfc, optval, sizeof(mfc)))\n\t\t\treturn -EFAULT;\n\t\tif (parent == 0)\n\t\t\tparent = mfc.mf6cc_parent;\n\t\trtnl_lock();\n\t\tif (optname == MRT6_DEL_MFC || optname == MRT6_DEL_MFC_PROXY)\n\t\t\tret = ip6mr_mfc_delete(mrt, &mfc, parent);\n\t\telse\n\t\t\tret = ip6mr_mfc_add(net, mrt, &mfc,\n\t\t\t\t\t    sk == mrt->mroute6_sk, parent);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\t/*\n\t *\tControl PIM assert (to activate pim will activate assert)\n\t */\n\tcase MRT6_ASSERT:\n\t{\n\t\tint v;\n\n\t\tif (optlen != sizeof(v))\n\t\t\treturn -EINVAL;\n\t\tif (get_user(v, (int __user *)optval))\n\t\t\treturn -EFAULT;\n\t\tmrt->mroute_do_assert = v;\n\t\treturn 0;\n\t}\n\n#ifdef CONFIG_IPV6_PIMSM_V2\n\tcase MRT6_PIM:\n\t{\n\t\tint v;\n\n\t\tif (optlen != sizeof(v))\n\t\t\treturn -EINVAL;\n\t\tif (get_user(v, (int __user *)optval))\n\t\t\treturn -EFAULT;\n\t\tv = !!v;\n\t\trtnl_lock();\n\t\tret = 0;\n\t\tif (v != mrt->mroute_do_pim) {\n\t\t\tmrt->mroute_do_pim = v;\n\t\t\tmrt->mroute_do_assert = v;\n\t\t}\n\t\trtnl_unlock();\n\t\treturn ret;\n\t}\n\n#endif\n#ifdef CONFIG_IPV6_MROUTE_MULTIPLE_TABLES\n\tcase MRT6_TABLE:\n\t{\n\t\tu32 v;\n\n\t\tif (optlen != sizeof(u32))\n\t\t\treturn -EINVAL;\n\t\tif (get_user(v, (u32 __user *)optval))\n\t\t\treturn -EFAULT;\n\t\t/* \"pim6reg%u\" should not exceed 16 bytes (IFNAMSIZ) */\n\t\tif (v != RT_TABLE_DEFAULT && v >= 100000000)\n\t\t\treturn -EINVAL;\n\t\tif (sk == mrt->mroute6_sk)\n\t\t\treturn -EBUSY;\n\n\t\trtnl_lock();\n\t\tret = 0;\n\t\tif (!ip6mr_new_table(net, v))\n\t\t\tret = -ENOMEM;\n\t\traw6_sk(sk)->ip6mr_table = v;\n\t\trtnl_unlock();\n\t\treturn ret;\n\t}\n#endif\n\t/*\n\t *\tSpurious command, or MRT6_VERSION which you cannot\n\t *\tset.\n\t */\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
        "code_after_change": "int ip6_mroute_setsockopt(struct sock *sk, int optname, char __user *optval, unsigned int optlen)\n{\n\tint ret, parent = 0;\n\tstruct mif6ctl vif;\n\tstruct mf6cctl mfc;\n\tmifi_t mifi;\n\tstruct net *net = sock_net(sk);\n\tstruct mr6_table *mrt;\n\n\tif (sk->sk_type != SOCK_RAW ||\n\t    inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n\t\treturn -EOPNOTSUPP;\n\n\tmrt = ip6mr_get_table(net, raw6_sk(sk)->ip6mr_table ? : RT6_TABLE_DFLT);\n\tif (!mrt)\n\t\treturn -ENOENT;\n\n\tif (optname != MRT6_INIT) {\n\t\tif (sk != mrt->mroute6_sk && !ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tswitch (optname) {\n\tcase MRT6_INIT:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\n\t\treturn ip6mr_sk_init(mrt, sk);\n\n\tcase MRT6_DONE:\n\t\treturn ip6mr_sk_done(sk);\n\n\tcase MRT6_ADD_MIF:\n\t\tif (optlen < sizeof(vif))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&vif, optval, sizeof(vif)))\n\t\t\treturn -EFAULT;\n\t\tif (vif.mif6c_mifi >= MAXMIFS)\n\t\t\treturn -ENFILE;\n\t\trtnl_lock();\n\t\tret = mif6_add(net, mrt, &vif, sk == mrt->mroute6_sk);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\tcase MRT6_DEL_MIF:\n\t\tif (optlen < sizeof(mifi_t))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&mifi, optval, sizeof(mifi_t)))\n\t\t\treturn -EFAULT;\n\t\trtnl_lock();\n\t\tret = mif6_delete(mrt, mifi, NULL);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\t/*\n\t *\tManipulate the forwarding caches. These live\n\t *\tin a sort of kernel/user symbiosis.\n\t */\n\tcase MRT6_ADD_MFC:\n\tcase MRT6_DEL_MFC:\n\t\tparent = -1;\n\tcase MRT6_ADD_MFC_PROXY:\n\tcase MRT6_DEL_MFC_PROXY:\n\t\tif (optlen < sizeof(mfc))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&mfc, optval, sizeof(mfc)))\n\t\t\treturn -EFAULT;\n\t\tif (parent == 0)\n\t\t\tparent = mfc.mf6cc_parent;\n\t\trtnl_lock();\n\t\tif (optname == MRT6_DEL_MFC || optname == MRT6_DEL_MFC_PROXY)\n\t\t\tret = ip6mr_mfc_delete(mrt, &mfc, parent);\n\t\telse\n\t\t\tret = ip6mr_mfc_add(net, mrt, &mfc,\n\t\t\t\t\t    sk == mrt->mroute6_sk, parent);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\t/*\n\t *\tControl PIM assert (to activate pim will activate assert)\n\t */\n\tcase MRT6_ASSERT:\n\t{\n\t\tint v;\n\n\t\tif (optlen != sizeof(v))\n\t\t\treturn -EINVAL;\n\t\tif (get_user(v, (int __user *)optval))\n\t\t\treturn -EFAULT;\n\t\tmrt->mroute_do_assert = v;\n\t\treturn 0;\n\t}\n\n#ifdef CONFIG_IPV6_PIMSM_V2\n\tcase MRT6_PIM:\n\t{\n\t\tint v;\n\n\t\tif (optlen != sizeof(v))\n\t\t\treturn -EINVAL;\n\t\tif (get_user(v, (int __user *)optval))\n\t\t\treturn -EFAULT;\n\t\tv = !!v;\n\t\trtnl_lock();\n\t\tret = 0;\n\t\tif (v != mrt->mroute_do_pim) {\n\t\t\tmrt->mroute_do_pim = v;\n\t\t\tmrt->mroute_do_assert = v;\n\t\t}\n\t\trtnl_unlock();\n\t\treturn ret;\n\t}\n\n#endif\n#ifdef CONFIG_IPV6_MROUTE_MULTIPLE_TABLES\n\tcase MRT6_TABLE:\n\t{\n\t\tu32 v;\n\n\t\tif (optlen != sizeof(u32))\n\t\t\treturn -EINVAL;\n\t\tif (get_user(v, (u32 __user *)optval))\n\t\t\treturn -EFAULT;\n\t\t/* \"pim6reg%u\" should not exceed 16 bytes (IFNAMSIZ) */\n\t\tif (v != RT_TABLE_DEFAULT && v >= 100000000)\n\t\t\treturn -EINVAL;\n\t\tif (sk == mrt->mroute6_sk)\n\t\t\treturn -EBUSY;\n\n\t\trtnl_lock();\n\t\tret = 0;\n\t\tif (!ip6mr_new_table(net, v))\n\t\t\tret = -ENOMEM;\n\t\traw6_sk(sk)->ip6mr_table = v;\n\t\trtnl_unlock();\n\t\treturn ret;\n\t}\n#endif\n\t/*\n\t *\tSpurious command, or MRT6_VERSION which you cannot\n\t *\tset.\n\t */\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,10 @@\n \tmifi_t mifi;\n \tstruct net *net = sock_net(sk);\n \tstruct mr6_table *mrt;\n+\n+\tif (sk->sk_type != SOCK_RAW ||\n+\t    inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n+\t\treturn -EOPNOTSUPP;\n \n \tmrt = ip6mr_get_table(net, raw6_sk(sk)->ip6mr_table ? : RT6_TABLE_DFLT);\n \tif (!mrt)\n@@ -18,9 +22,6 @@\n \n \tswitch (optname) {\n \tcase MRT6_INIT:\n-\t\tif (sk->sk_type != SOCK_RAW ||\n-\t\t    inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n-\t\t\treturn -EOPNOTSUPP;\n \t\tif (optlen < sizeof(int))\n \t\t\treturn -EINVAL;\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (sk->sk_type != SOCK_RAW ||",
                "\t    inet_sk(sk)->inet_num != IPPROTO_ICMPV6)",
                "\t\treturn -EOPNOTSUPP;"
            ],
            "deleted": [
                "\t\tif (sk->sk_type != SOCK_RAW ||",
                "\t\t    inet_sk(sk)->inet_num != IPPROTO_ICMPV6)",
                "\t\t\treturn -EOPNOTSUPP;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "An issue was discovered in net/ipv6/ip6mr.c in the Linux kernel before 4.11. By setting a specific socket option, an attacker can control a pointer in kernel land and cause an inet_csk_listen_stop general protection fault, or potentially execute arbitrary code under certain circumstances. The issue can be triggered as root (e.g., inside a default LXC container or with the CAP_NET_ADMIN capability) or after namespace unsharing. This occurs because sk_type and protocol are not checked in the appropriate part of the ip6_mroute_* functions. NOTE: this affects Linux distributions that use 4.9.x longterm kernels before 4.9.187.",
        "id": 1436
    },
    {
        "cve_id": "CVE-2021-33098",
        "code_before_change": "static int ixgbe_rcv_msg_from_vf(struct ixgbe_adapter *adapter, u32 vf)\n{\n\tu32 mbx_size = IXGBE_VFMAILBOX_SIZE;\n\tu32 msgbuf[IXGBE_VFMAILBOX_SIZE];\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\ts32 retval;\n\n\tretval = ixgbe_read_mbx(hw, msgbuf, mbx_size, vf);\n\n\tif (retval) {\n\t\tpr_err(\"Error receiving message from VF\\n\");\n\t\treturn retval;\n\t}\n\n\t/* this is a message we already processed, do nothing */\n\tif (msgbuf[0] & (IXGBE_VT_MSGTYPE_ACK | IXGBE_VT_MSGTYPE_NACK))\n\t\treturn 0;\n\n\t/* flush the ack before we write any messages back */\n\tIXGBE_WRITE_FLUSH(hw);\n\n\tif (msgbuf[0] == IXGBE_VF_RESET)\n\t\treturn ixgbe_vf_reset_msg(adapter, vf);\n\n\t/*\n\t * until the vf completes a virtual function reset it should not be\n\t * allowed to start any configuration.\n\t */\n\tif (!adapter->vfinfo[vf].clear_to_send) {\n\t\tmsgbuf[0] |= IXGBE_VT_MSGTYPE_NACK;\n\t\tixgbe_write_mbx(hw, msgbuf, 1, vf);\n\t\treturn 0;\n\t}\n\n\tswitch ((msgbuf[0] & 0xFFFF)) {\n\tcase IXGBE_VF_SET_MAC_ADDR:\n\t\tretval = ixgbe_set_vf_mac_addr(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_SET_MULTICAST:\n\t\tretval = ixgbe_set_vf_multicasts(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_SET_VLAN:\n\t\tretval = ixgbe_set_vf_vlan_msg(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_SET_LPE:\n\t\tretval = ixgbe_set_vf_lpe(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_SET_MACVLAN:\n\t\tretval = ixgbe_set_vf_macvlan_msg(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_API_NEGOTIATE:\n\t\tretval = ixgbe_negotiate_vf_api(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_GET_QUEUES:\n\t\tretval = ixgbe_get_vf_queues(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_GET_RETA:\n\t\tretval = ixgbe_get_vf_reta(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_GET_RSS_KEY:\n\t\tretval = ixgbe_get_vf_rss_key(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_UPDATE_XCAST_MODE:\n\t\tretval = ixgbe_update_vf_xcast_mode(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_IPSEC_ADD:\n\t\tretval = ixgbe_ipsec_vf_add_sa(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_IPSEC_DEL:\n\t\tretval = ixgbe_ipsec_vf_del_sa(adapter, msgbuf, vf);\n\t\tbreak;\n\tdefault:\n\t\te_err(drv, \"Unhandled Msg %8.8x\\n\", msgbuf[0]);\n\t\tretval = IXGBE_ERR_MBX;\n\t\tbreak;\n\t}\n\n\t/* notify the VF of the results of what it sent us */\n\tif (retval)\n\t\tmsgbuf[0] |= IXGBE_VT_MSGTYPE_NACK;\n\telse\n\t\tmsgbuf[0] |= IXGBE_VT_MSGTYPE_ACK;\n\n\tmsgbuf[0] |= IXGBE_VT_MSGTYPE_CTS;\n\n\tixgbe_write_mbx(hw, msgbuf, mbx_size, vf);\n\n\treturn retval;\n}",
        "code_after_change": "static int ixgbe_rcv_msg_from_vf(struct ixgbe_adapter *adapter, u32 vf)\n{\n\tu32 mbx_size = IXGBE_VFMAILBOX_SIZE;\n\tu32 msgbuf[IXGBE_VFMAILBOX_SIZE];\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\ts32 retval;\n\n\tretval = ixgbe_read_mbx(hw, msgbuf, mbx_size, vf);\n\n\tif (retval) {\n\t\tpr_err(\"Error receiving message from VF\\n\");\n\t\treturn retval;\n\t}\n\n\t/* this is a message we already processed, do nothing */\n\tif (msgbuf[0] & (IXGBE_VT_MSGTYPE_ACK | IXGBE_VT_MSGTYPE_NACK))\n\t\treturn 0;\n\n\t/* flush the ack before we write any messages back */\n\tIXGBE_WRITE_FLUSH(hw);\n\n\tif (msgbuf[0] == IXGBE_VF_RESET)\n\t\treturn ixgbe_vf_reset_msg(adapter, vf);\n\n\t/*\n\t * until the vf completes a virtual function reset it should not be\n\t * allowed to start any configuration.\n\t */\n\tif (!adapter->vfinfo[vf].clear_to_send) {\n\t\tmsgbuf[0] |= IXGBE_VT_MSGTYPE_NACK;\n\t\tixgbe_write_mbx(hw, msgbuf, 1, vf);\n\t\treturn 0;\n\t}\n\n\tswitch ((msgbuf[0] & 0xFFFF)) {\n\tcase IXGBE_VF_SET_MAC_ADDR:\n\t\tretval = ixgbe_set_vf_mac_addr(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_SET_MULTICAST:\n\t\tretval = ixgbe_set_vf_multicasts(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_SET_VLAN:\n\t\tretval = ixgbe_set_vf_vlan_msg(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_SET_LPE:\n\t\tretval = ixgbe_set_vf_lpe(adapter, msgbuf[1], vf);\n\t\tbreak;\n\tcase IXGBE_VF_SET_MACVLAN:\n\t\tretval = ixgbe_set_vf_macvlan_msg(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_API_NEGOTIATE:\n\t\tretval = ixgbe_negotiate_vf_api(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_GET_QUEUES:\n\t\tretval = ixgbe_get_vf_queues(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_GET_RETA:\n\t\tretval = ixgbe_get_vf_reta(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_GET_RSS_KEY:\n\t\tretval = ixgbe_get_vf_rss_key(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_UPDATE_XCAST_MODE:\n\t\tretval = ixgbe_update_vf_xcast_mode(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_IPSEC_ADD:\n\t\tretval = ixgbe_ipsec_vf_add_sa(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase IXGBE_VF_IPSEC_DEL:\n\t\tretval = ixgbe_ipsec_vf_del_sa(adapter, msgbuf, vf);\n\t\tbreak;\n\tdefault:\n\t\te_err(drv, \"Unhandled Msg %8.8x\\n\", msgbuf[0]);\n\t\tretval = IXGBE_ERR_MBX;\n\t\tbreak;\n\t}\n\n\t/* notify the VF of the results of what it sent us */\n\tif (retval)\n\t\tmsgbuf[0] |= IXGBE_VT_MSGTYPE_NACK;\n\telse\n\t\tmsgbuf[0] |= IXGBE_VT_MSGTYPE_ACK;\n\n\tmsgbuf[0] |= IXGBE_VT_MSGTYPE_CTS;\n\n\tixgbe_write_mbx(hw, msgbuf, mbx_size, vf);\n\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,7 +43,7 @@\n \t\tretval = ixgbe_set_vf_vlan_msg(adapter, msgbuf, vf);\n \t\tbreak;\n \tcase IXGBE_VF_SET_LPE:\n-\t\tretval = ixgbe_set_vf_lpe(adapter, msgbuf, vf);\n+\t\tretval = ixgbe_set_vf_lpe(adapter, msgbuf[1], vf);\n \t\tbreak;\n \tcase IXGBE_VF_SET_MACVLAN:\n \t\tretval = ixgbe_set_vf_macvlan_msg(adapter, msgbuf, vf);",
        "function_modified_lines": {
            "added": [
                "\t\tretval = ixgbe_set_vf_lpe(adapter, msgbuf[1], vf);"
            ],
            "deleted": [
                "\t\tretval = ixgbe_set_vf_lpe(adapter, msgbuf, vf);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Improper input validation in the Intel(R) Ethernet ixgbe driver for Linux before version 3.17.3 may allow an authenticated user to potentially enable denial of service via local access.",
        "id": 2974
    },
    {
        "cve_id": "CVE-2008-7316",
        "code_before_change": "static ssize_t generic_perform_write(struct file *file,\n\t\t\t\tstruct iov_iter *i, loff_t pos)\n{\n\tstruct address_space *mapping = file->f_mapping;\n\tconst struct address_space_operations *a_ops = mapping->a_ops;\n\tlong status = 0;\n\tssize_t written = 0;\n\tunsigned int flags = 0;\n\n\t/*\n\t * Copies from kernel address space cannot fail (NFSD is a big user).\n\t */\n\tif (segment_eq(get_fs(), KERNEL_DS))\n\t\tflags |= AOP_FLAG_UNINTERRUPTIBLE;\n\n\tdo {\n\t\tstruct page *page;\n\t\tpgoff_t index;\t\t/* Pagecache index for current page */\n\t\tunsigned long offset;\t/* Offset into pagecache page */\n\t\tunsigned long bytes;\t/* Bytes to write to page */\n\t\tsize_t copied;\t\t/* Bytes copied from user */\n\t\tvoid *fsdata;\n\n\t\toffset = (pos & (PAGE_CACHE_SIZE - 1));\n\t\tindex = pos >> PAGE_CACHE_SHIFT;\n\t\tbytes = min_t(unsigned long, PAGE_CACHE_SIZE - offset,\n\t\t\t\t\t\tiov_iter_count(i));\n\nagain:\n\n\t\t/*\n\t\t * Bring in the user page that we will copy from _first_.\n\t\t * Otherwise there's a nasty deadlock on copying from the\n\t\t * same page as we're writing to, without it being marked\n\t\t * up-to-date.\n\t\t *\n\t\t * Not only is this an optimisation, but it is also required\n\t\t * to check that the address is actually valid, when atomic\n\t\t * usercopies are used, below.\n\t\t */\n\t\tif (unlikely(iov_iter_fault_in_readable(i, bytes))) {\n\t\t\tstatus = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tstatus = a_ops->write_begin(file, mapping, pos, bytes, flags,\n\t\t\t\t\t\t&page, &fsdata);\n\t\tif (unlikely(status))\n\t\t\tbreak;\n\n\t\tpagefault_disable();\n\t\tcopied = iov_iter_copy_from_user_atomic(page, i, offset, bytes);\n\t\tpagefault_enable();\n\t\tflush_dcache_page(page);\n\n\t\tstatus = a_ops->write_end(file, mapping, pos, bytes, copied,\n\t\t\t\t\t\tpage, fsdata);\n\t\tif (unlikely(status < 0))\n\t\t\tbreak;\n\t\tcopied = status;\n\n\t\tcond_resched();\n\n\t\tif (unlikely(copied == 0)) {\n\t\t\t/*\n\t\t\t * If we were unable to copy any data at all, we must\n\t\t\t * fall back to a single segment length write.\n\t\t\t *\n\t\t\t * If we didn't fallback here, we could livelock\n\t\t\t * because not all segments in the iov can be copied at\n\t\t\t * once without a pagefault.\n\t\t\t */\n\t\t\tbytes = min_t(unsigned long, PAGE_CACHE_SIZE - offset,\n\t\t\t\t\t\tiov_iter_single_seg_count(i));\n\t\t\tgoto again;\n\t\t}\n\t\tiov_iter_advance(i, copied);\n\t\tpos += copied;\n\t\twritten += copied;\n\n\t\tbalance_dirty_pages_ratelimited(mapping);\n\n\t} while (iov_iter_count(i));\n\n\treturn written ? written : status;\n}",
        "code_after_change": "static ssize_t generic_perform_write(struct file *file,\n\t\t\t\tstruct iov_iter *i, loff_t pos)\n{\n\tstruct address_space *mapping = file->f_mapping;\n\tconst struct address_space_operations *a_ops = mapping->a_ops;\n\tlong status = 0;\n\tssize_t written = 0;\n\tunsigned int flags = 0;\n\n\t/*\n\t * Copies from kernel address space cannot fail (NFSD is a big user).\n\t */\n\tif (segment_eq(get_fs(), KERNEL_DS))\n\t\tflags |= AOP_FLAG_UNINTERRUPTIBLE;\n\n\tdo {\n\t\tstruct page *page;\n\t\tpgoff_t index;\t\t/* Pagecache index for current page */\n\t\tunsigned long offset;\t/* Offset into pagecache page */\n\t\tunsigned long bytes;\t/* Bytes to write to page */\n\t\tsize_t copied;\t\t/* Bytes copied from user */\n\t\tvoid *fsdata;\n\n\t\toffset = (pos & (PAGE_CACHE_SIZE - 1));\n\t\tindex = pos >> PAGE_CACHE_SHIFT;\n\t\tbytes = min_t(unsigned long, PAGE_CACHE_SIZE - offset,\n\t\t\t\t\t\tiov_iter_count(i));\n\nagain:\n\n\t\t/*\n\t\t * Bring in the user page that we will copy from _first_.\n\t\t * Otherwise there's a nasty deadlock on copying from the\n\t\t * same page as we're writing to, without it being marked\n\t\t * up-to-date.\n\t\t *\n\t\t * Not only is this an optimisation, but it is also required\n\t\t * to check that the address is actually valid, when atomic\n\t\t * usercopies are used, below.\n\t\t */\n\t\tif (unlikely(iov_iter_fault_in_readable(i, bytes))) {\n\t\t\tstatus = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tstatus = a_ops->write_begin(file, mapping, pos, bytes, flags,\n\t\t\t\t\t\t&page, &fsdata);\n\t\tif (unlikely(status))\n\t\t\tbreak;\n\n\t\tpagefault_disable();\n\t\tcopied = iov_iter_copy_from_user_atomic(page, i, offset, bytes);\n\t\tpagefault_enable();\n\t\tflush_dcache_page(page);\n\n\t\tstatus = a_ops->write_end(file, mapping, pos, bytes, copied,\n\t\t\t\t\t\tpage, fsdata);\n\t\tif (unlikely(status < 0))\n\t\t\tbreak;\n\t\tcopied = status;\n\n\t\tcond_resched();\n\n\t\tiov_iter_advance(i, copied);\n\t\tif (unlikely(copied == 0)) {\n\t\t\t/*\n\t\t\t * If we were unable to copy any data at all, we must\n\t\t\t * fall back to a single segment length write.\n\t\t\t *\n\t\t\t * If we didn't fallback here, we could livelock\n\t\t\t * because not all segments in the iov can be copied at\n\t\t\t * once without a pagefault.\n\t\t\t */\n\t\t\tbytes = min_t(unsigned long, PAGE_CACHE_SIZE - offset,\n\t\t\t\t\t\tiov_iter_single_seg_count(i));\n\t\t\tgoto again;\n\t\t}\n\t\tpos += copied;\n\t\twritten += copied;\n\n\t\tbalance_dirty_pages_ratelimited(mapping);\n\n\t} while (iov_iter_count(i));\n\n\treturn written ? written : status;\n}",
        "patch": "--- code before\n+++ code after\n@@ -61,6 +61,7 @@\n \n \t\tcond_resched();\n \n+\t\tiov_iter_advance(i, copied);\n \t\tif (unlikely(copied == 0)) {\n \t\t\t/*\n \t\t\t * If we were unable to copy any data at all, we must\n@@ -74,7 +75,6 @@\n \t\t\t\t\t\tiov_iter_single_seg_count(i));\n \t\t\tgoto again;\n \t\t}\n-\t\tiov_iter_advance(i, copied);\n \t\tpos += copied;\n \t\twritten += copied;\n ",
        "function_modified_lines": {
            "added": [
                "\t\tiov_iter_advance(i, copied);"
            ],
            "deleted": [
                "\t\tiov_iter_advance(i, copied);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "mm/filemap.c in the Linux kernel before 2.6.25 allows local users to cause a denial of service (infinite loop) via a writev system call that triggers an iovec of zero length, followed by a page fault for an iovec of nonzero length.",
        "id": 8
    },
    {
        "cve_id": "CVE-2019-0147",
        "code_before_change": "static int i40e_config_iwarp_qvlist(struct i40e_vf *vf,\n\t\t\t\t    struct virtchnl_iwarp_qvlist_info *qvlist_info)\n{\n\tstruct i40e_pf *pf = vf->pf;\n\tstruct i40e_hw *hw = &pf->hw;\n\tstruct virtchnl_iwarp_qv_info *qv_info;\n\tu32 v_idx, i, reg_idx, reg;\n\tu32 next_q_idx, next_q_type;\n\tu32 msix_vf, size;\n\n\tsize = sizeof(struct virtchnl_iwarp_qvlist_info) +\n\t       (sizeof(struct virtchnl_iwarp_qv_info) *\n\t\t\t\t\t\t(qvlist_info->num_vectors - 1));\n\tvf->qvlist_info = kzalloc(size, GFP_KERNEL);\n\tif (!vf->qvlist_info)\n\t\treturn -ENOMEM;\n\n\tvf->qvlist_info->num_vectors = qvlist_info->num_vectors;\n\n\tmsix_vf = pf->hw.func_caps.num_msix_vectors_vf;\n\tfor (i = 0; i < qvlist_info->num_vectors; i++) {\n\t\tqv_info = &qvlist_info->qv_info[i];\n\t\tif (!qv_info)\n\t\t\tcontinue;\n\t\tv_idx = qv_info->v_idx;\n\n\t\t/* Validate vector id belongs to this vf */\n\t\tif (!i40e_vc_isvalid_vector_id(vf, v_idx))\n\t\t\tgoto err;\n\n\t\tvf->qvlist_info->qv_info[i] = *qv_info;\n\n\t\treg_idx = ((msix_vf - 1) * vf->vf_id) + (v_idx - 1);\n\t\t/* We might be sharing the interrupt, so get the first queue\n\t\t * index and type, push it down the list by adding the new\n\t\t * queue on top. Also link it with the new queue in CEQCTL.\n\t\t */\n\t\treg = rd32(hw, I40E_VPINT_LNKLSTN(reg_idx));\n\t\tnext_q_idx = ((reg & I40E_VPINT_LNKLSTN_FIRSTQ_INDX_MASK) >>\n\t\t\t\tI40E_VPINT_LNKLSTN_FIRSTQ_INDX_SHIFT);\n\t\tnext_q_type = ((reg & I40E_VPINT_LNKLSTN_FIRSTQ_TYPE_MASK) >>\n\t\t\t\tI40E_VPINT_LNKLSTN_FIRSTQ_TYPE_SHIFT);\n\n\t\tif (qv_info->ceq_idx != I40E_QUEUE_INVALID_IDX) {\n\t\t\treg_idx = (msix_vf - 1) * vf->vf_id + qv_info->ceq_idx;\n\t\t\treg = (I40E_VPINT_CEQCTL_CAUSE_ENA_MASK |\n\t\t\t(v_idx << I40E_VPINT_CEQCTL_MSIX_INDX_SHIFT) |\n\t\t\t(qv_info->itr_idx << I40E_VPINT_CEQCTL_ITR_INDX_SHIFT) |\n\t\t\t(next_q_type << I40E_VPINT_CEQCTL_NEXTQ_TYPE_SHIFT) |\n\t\t\t(next_q_idx << I40E_VPINT_CEQCTL_NEXTQ_INDX_SHIFT));\n\t\t\twr32(hw, I40E_VPINT_CEQCTL(reg_idx), reg);\n\n\t\t\treg_idx = ((msix_vf - 1) * vf->vf_id) + (v_idx - 1);\n\t\t\treg = (qv_info->ceq_idx &\n\t\t\t       I40E_VPINT_LNKLSTN_FIRSTQ_INDX_MASK) |\n\t\t\t       (I40E_QUEUE_TYPE_PE_CEQ <<\n\t\t\t       I40E_VPINT_LNKLSTN_FIRSTQ_TYPE_SHIFT);\n\t\t\twr32(hw, I40E_VPINT_LNKLSTN(reg_idx), reg);\n\t\t}\n\n\t\tif (qv_info->aeq_idx != I40E_QUEUE_INVALID_IDX) {\n\t\t\treg = (I40E_VPINT_AEQCTL_CAUSE_ENA_MASK |\n\t\t\t(v_idx << I40E_VPINT_AEQCTL_MSIX_INDX_SHIFT) |\n\t\t\t(qv_info->itr_idx << I40E_VPINT_AEQCTL_ITR_INDX_SHIFT));\n\n\t\t\twr32(hw, I40E_VPINT_AEQCTL(vf->vf_id), reg);\n\t\t}\n\t}\n\n\treturn 0;\nerr:\n\tkfree(vf->qvlist_info);\n\tvf->qvlist_info = NULL;\n\treturn -EINVAL;\n}",
        "code_after_change": "static int i40e_config_iwarp_qvlist(struct i40e_vf *vf,\n\t\t\t\t    struct virtchnl_iwarp_qvlist_info *qvlist_info)\n{\n\tstruct i40e_pf *pf = vf->pf;\n\tstruct i40e_hw *hw = &pf->hw;\n\tstruct virtchnl_iwarp_qv_info *qv_info;\n\tu32 v_idx, i, reg_idx, reg;\n\tu32 next_q_idx, next_q_type;\n\tu32 msix_vf, size;\n\n\tmsix_vf = pf->hw.func_caps.num_msix_vectors_vf;\n\n\tif (qvlist_info->num_vectors > msix_vf) {\n\t\tdev_warn(&pf->pdev->dev,\n\t\t\t \"Incorrect number of iwarp vectors %u. Maximum %u allowed.\\n\",\n\t\t\t qvlist_info->num_vectors,\n\t\t\t msix_vf);\n\t\tgoto err;\n\t}\n\n\tsize = sizeof(struct virtchnl_iwarp_qvlist_info) +\n\t       (sizeof(struct virtchnl_iwarp_qv_info) *\n\t\t\t\t\t\t(qvlist_info->num_vectors - 1));\n\tvf->qvlist_info = kzalloc(size, GFP_KERNEL);\n\tif (!vf->qvlist_info)\n\t\treturn -ENOMEM;\n\n\tvf->qvlist_info->num_vectors = qvlist_info->num_vectors;\n\n\tmsix_vf = pf->hw.func_caps.num_msix_vectors_vf;\n\tfor (i = 0; i < qvlist_info->num_vectors; i++) {\n\t\tqv_info = &qvlist_info->qv_info[i];\n\t\tif (!qv_info)\n\t\t\tcontinue;\n\t\tv_idx = qv_info->v_idx;\n\n\t\t/* Validate vector id belongs to this vf */\n\t\tif (!i40e_vc_isvalid_vector_id(vf, v_idx))\n\t\t\tgoto err;\n\n\t\tvf->qvlist_info->qv_info[i] = *qv_info;\n\n\t\treg_idx = ((msix_vf - 1) * vf->vf_id) + (v_idx - 1);\n\t\t/* We might be sharing the interrupt, so get the first queue\n\t\t * index and type, push it down the list by adding the new\n\t\t * queue on top. Also link it with the new queue in CEQCTL.\n\t\t */\n\t\treg = rd32(hw, I40E_VPINT_LNKLSTN(reg_idx));\n\t\tnext_q_idx = ((reg & I40E_VPINT_LNKLSTN_FIRSTQ_INDX_MASK) >>\n\t\t\t\tI40E_VPINT_LNKLSTN_FIRSTQ_INDX_SHIFT);\n\t\tnext_q_type = ((reg & I40E_VPINT_LNKLSTN_FIRSTQ_TYPE_MASK) >>\n\t\t\t\tI40E_VPINT_LNKLSTN_FIRSTQ_TYPE_SHIFT);\n\n\t\tif (qv_info->ceq_idx != I40E_QUEUE_INVALID_IDX) {\n\t\t\treg_idx = (msix_vf - 1) * vf->vf_id + qv_info->ceq_idx;\n\t\t\treg = (I40E_VPINT_CEQCTL_CAUSE_ENA_MASK |\n\t\t\t(v_idx << I40E_VPINT_CEQCTL_MSIX_INDX_SHIFT) |\n\t\t\t(qv_info->itr_idx << I40E_VPINT_CEQCTL_ITR_INDX_SHIFT) |\n\t\t\t(next_q_type << I40E_VPINT_CEQCTL_NEXTQ_TYPE_SHIFT) |\n\t\t\t(next_q_idx << I40E_VPINT_CEQCTL_NEXTQ_INDX_SHIFT));\n\t\t\twr32(hw, I40E_VPINT_CEQCTL(reg_idx), reg);\n\n\t\t\treg_idx = ((msix_vf - 1) * vf->vf_id) + (v_idx - 1);\n\t\t\treg = (qv_info->ceq_idx &\n\t\t\t       I40E_VPINT_LNKLSTN_FIRSTQ_INDX_MASK) |\n\t\t\t       (I40E_QUEUE_TYPE_PE_CEQ <<\n\t\t\t       I40E_VPINT_LNKLSTN_FIRSTQ_TYPE_SHIFT);\n\t\t\twr32(hw, I40E_VPINT_LNKLSTN(reg_idx), reg);\n\t\t}\n\n\t\tif (qv_info->aeq_idx != I40E_QUEUE_INVALID_IDX) {\n\t\t\treg = (I40E_VPINT_AEQCTL_CAUSE_ENA_MASK |\n\t\t\t(v_idx << I40E_VPINT_AEQCTL_MSIX_INDX_SHIFT) |\n\t\t\t(qv_info->itr_idx << I40E_VPINT_AEQCTL_ITR_INDX_SHIFT));\n\n\t\t\twr32(hw, I40E_VPINT_AEQCTL(vf->vf_id), reg);\n\t\t}\n\t}\n\n\treturn 0;\nerr:\n\tkfree(vf->qvlist_info);\n\tvf->qvlist_info = NULL;\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,16 @@\n \tu32 v_idx, i, reg_idx, reg;\n \tu32 next_q_idx, next_q_type;\n \tu32 msix_vf, size;\n+\n+\tmsix_vf = pf->hw.func_caps.num_msix_vectors_vf;\n+\n+\tif (qvlist_info->num_vectors > msix_vf) {\n+\t\tdev_warn(&pf->pdev->dev,\n+\t\t\t \"Incorrect number of iwarp vectors %u. Maximum %u allowed.\\n\",\n+\t\t\t qvlist_info->num_vectors,\n+\t\t\t msix_vf);\n+\t\tgoto err;\n+\t}\n \n \tsize = sizeof(struct virtchnl_iwarp_qvlist_info) +\n \t       (sizeof(struct virtchnl_iwarp_qv_info) *",
        "function_modified_lines": {
            "added": [
                "",
                "\tmsix_vf = pf->hw.func_caps.num_msix_vectors_vf;",
                "",
                "\tif (qvlist_info->num_vectors > msix_vf) {",
                "\t\tdev_warn(&pf->pdev->dev,",
                "\t\t\t \"Incorrect number of iwarp vectors %u. Maximum %u allowed.\\n\",",
                "\t\t\t qvlist_info->num_vectors,",
                "\t\t\t msix_vf);",
                "\t\tgoto err;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Insufficient input validation in i40e driver for Intel(R) Ethernet 700 Series Controllers versions before 7.0 may allow an authenticated user to potentially enable a denial of service via local access.",
        "id": 1879
    },
    {
        "cve_id": "CVE-2021-3655",
        "code_before_change": "static void sctp_asconf_param_success(struct sctp_association *asoc,\n\t\t\t\t      struct sctp_addip_param *asconf_param)\n{\n\tstruct sctp_bind_addr *bp = &asoc->base.bind_addr;\n\tunion sctp_addr_param *addr_param;\n\tstruct sctp_sockaddr_entry *saddr;\n\tstruct sctp_transport *transport;\n\tunion sctp_addr\taddr;\n\tstruct sctp_af *af;\n\n\taddr_param = (void *)asconf_param + sizeof(*asconf_param);\n\n\t/* We have checked the packet before, so we do not check again.\t*/\n\taf = sctp_get_af_specific(param_type2af(addr_param->p.type));\n\taf->from_addr_param(&addr, addr_param, htons(bp->port), 0);\n\n\tswitch (asconf_param->param_hdr.type) {\n\tcase SCTP_PARAM_ADD_IP:\n\t\t/* This is always done in BH context with a socket lock\n\t\t * held, so the list can not change.\n\t\t */\n\t\tlocal_bh_disable();\n\t\tlist_for_each_entry(saddr, &bp->address_list, list) {\n\t\t\tif (sctp_cmp_addr_exact(&saddr->a, &addr))\n\t\t\t\tsaddr->state = SCTP_ADDR_SRC;\n\t\t}\n\t\tlocal_bh_enable();\n\t\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\t\tsctp_transport_dst_release(transport);\n\t\t}\n\t\tbreak;\n\tcase SCTP_PARAM_DEL_IP:\n\t\tlocal_bh_disable();\n\t\tsctp_del_bind_addr(bp, &addr);\n\t\tif (asoc->asconf_addr_del_pending != NULL &&\n\t\t    sctp_cmp_addr_exact(asoc->asconf_addr_del_pending, &addr)) {\n\t\t\tkfree(asoc->asconf_addr_del_pending);\n\t\t\tasoc->asconf_addr_del_pending = NULL;\n\t\t}\n\t\tlocal_bh_enable();\n\t\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\t\tsctp_transport_dst_release(transport);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}",
        "code_after_change": "static void sctp_asconf_param_success(struct sctp_association *asoc,\n\t\t\t\t      struct sctp_addip_param *asconf_param)\n{\n\tstruct sctp_bind_addr *bp = &asoc->base.bind_addr;\n\tunion sctp_addr_param *addr_param;\n\tstruct sctp_sockaddr_entry *saddr;\n\tstruct sctp_transport *transport;\n\tunion sctp_addr\taddr;\n\tstruct sctp_af *af;\n\n\taddr_param = (void *)asconf_param + sizeof(*asconf_param);\n\n\t/* We have checked the packet before, so we do not check again.\t*/\n\taf = sctp_get_af_specific(param_type2af(addr_param->p.type));\n\tif (!af->from_addr_param(&addr, addr_param, htons(bp->port), 0))\n\t\treturn;\n\n\tswitch (asconf_param->param_hdr.type) {\n\tcase SCTP_PARAM_ADD_IP:\n\t\t/* This is always done in BH context with a socket lock\n\t\t * held, so the list can not change.\n\t\t */\n\t\tlocal_bh_disable();\n\t\tlist_for_each_entry(saddr, &bp->address_list, list) {\n\t\t\tif (sctp_cmp_addr_exact(&saddr->a, &addr))\n\t\t\t\tsaddr->state = SCTP_ADDR_SRC;\n\t\t}\n\t\tlocal_bh_enable();\n\t\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\t\tsctp_transport_dst_release(transport);\n\t\t}\n\t\tbreak;\n\tcase SCTP_PARAM_DEL_IP:\n\t\tlocal_bh_disable();\n\t\tsctp_del_bind_addr(bp, &addr);\n\t\tif (asoc->asconf_addr_del_pending != NULL &&\n\t\t    sctp_cmp_addr_exact(asoc->asconf_addr_del_pending, &addr)) {\n\t\t\tkfree(asoc->asconf_addr_del_pending);\n\t\t\tasoc->asconf_addr_del_pending = NULL;\n\t\t}\n\t\tlocal_bh_enable();\n\t\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\t\tsctp_transport_dst_release(transport);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,8 @@\n \n \t/* We have checked the packet before, so we do not check again.\t*/\n \taf = sctp_get_af_specific(param_type2af(addr_param->p.type));\n-\taf->from_addr_param(&addr, addr_param, htons(bp->port), 0);\n+\tif (!af->from_addr_param(&addr, addr_param, htons(bp->port), 0))\n+\t\treturn;\n \n \tswitch (asconf_param->param_hdr.type) {\n \tcase SCTP_PARAM_ADD_IP:",
        "function_modified_lines": {
            "added": [
                "\tif (!af->from_addr_param(&addr, addr_param, htons(bp->port), 0))",
                "\t\treturn;"
            ],
            "deleted": [
                "\taf->from_addr_param(&addr, addr_param, htons(bp->port), 0);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel in versions prior to v5.14-rc1. Missing size validations on inbound SCTP packets may allow the kernel to read uninitialized memory.",
        "id": 3035
    },
    {
        "cve_id": "CVE-2016-2548",
        "code_before_change": "static int _snd_timer_stop(struct snd_timer_instance * timeri,\n\t\t\t   int keep_flag, int event)\n{\n\tstruct snd_timer *timer;\n\tunsigned long flags;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\tif (!keep_flag) {\n\t\t\tspin_lock_irqsave(&slave_active_lock, flags);\n\t\t\ttimeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\t\t}\n\t\tgoto __end;\n\t}\n\ttimer = timeri->timer;\n\tif (!timer)\n\t\treturn -EINVAL;\n\tspin_lock_irqsave(&timer->lock, flags);\n\tlist_del_init(&timeri->ack_list);\n\tlist_del_init(&timeri->active_list);\n\tif ((timeri->flags & SNDRV_TIMER_IFLG_RUNNING) &&\n\t    !(--timer->running)) {\n\t\ttimer->hw.stop(timer);\n\t\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED) {\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_RESCHED;\n\t\t\tsnd_timer_reschedule(timer, 0);\n\t\t\tif (timer->flags & SNDRV_TIMER_FLG_CHANGE) {\n\t\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\t\ttimer->hw.start(timer);\n\t\t\t}\n\t\t}\n\t}\n\tif (!keep_flag)\n\t\ttimeri->flags &=\n\t\t\t~(SNDRV_TIMER_IFLG_RUNNING | SNDRV_TIMER_IFLG_START);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n      __end:\n\tif (event != SNDRV_TIMER_EVENT_RESOLUTION)\n\t\tsnd_timer_notify1(timeri, event);\n\treturn 0;\n}",
        "code_after_change": "static int _snd_timer_stop(struct snd_timer_instance * timeri,\n\t\t\t   int keep_flag, int event)\n{\n\tstruct snd_timer *timer;\n\tunsigned long flags;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\tif (!keep_flag) {\n\t\t\tspin_lock_irqsave(&slave_active_lock, flags);\n\t\t\ttimeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\tlist_del_init(&timeri->ack_list);\n\t\t\tlist_del_init(&timeri->active_list);\n\t\t\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\t\t}\n\t\tgoto __end;\n\t}\n\ttimer = timeri->timer;\n\tif (!timer)\n\t\treturn -EINVAL;\n\tspin_lock_irqsave(&timer->lock, flags);\n\tlist_del_init(&timeri->ack_list);\n\tlist_del_init(&timeri->active_list);\n\tif ((timeri->flags & SNDRV_TIMER_IFLG_RUNNING) &&\n\t    !(--timer->running)) {\n\t\ttimer->hw.stop(timer);\n\t\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED) {\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_RESCHED;\n\t\t\tsnd_timer_reschedule(timer, 0);\n\t\t\tif (timer->flags & SNDRV_TIMER_FLG_CHANGE) {\n\t\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\t\ttimer->hw.start(timer);\n\t\t\t}\n\t\t}\n\t}\n\tif (!keep_flag)\n\t\ttimeri->flags &=\n\t\t\t~(SNDRV_TIMER_IFLG_RUNNING | SNDRV_TIMER_IFLG_START);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n      __end:\n\tif (event != SNDRV_TIMER_EVENT_RESOLUTION)\n\t\tsnd_timer_notify1(timeri, event);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,8 @@\n \t\tif (!keep_flag) {\n \t\t\tspin_lock_irqsave(&slave_active_lock, flags);\n \t\t\ttimeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n+\t\t\tlist_del_init(&timeri->ack_list);\n+\t\t\tlist_del_init(&timeri->active_list);\n \t\t\tspin_unlock_irqrestore(&slave_active_lock, flags);\n \t\t}\n \t\tgoto __end;",
        "function_modified_lines": {
            "added": [
                "\t\t\tlist_del_init(&timeri->ack_list);",
                "\t\t\tlist_del_init(&timeri->active_list);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 retains certain linked lists after a close or stop action, which allows local users to cause a denial of service (system crash) via a crafted ioctl call, related to the (1) snd_timer_close and (2) _snd_timer_stop functions.",
        "id": 946
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int recv_stream(struct kiocb *iocb, struct socket *sock,\n\t\t       struct msghdr *m, size_t buf_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tipc_port *tport = tipc_sk_port(sk);\n\tstruct sk_buff *buf;\n\tstruct tipc_msg *msg;\n\tlong timeout;\n\tunsigned int sz;\n\tint sz_to_copy, target, needed;\n\tint sz_copied = 0;\n\tu32 err;\n\tint res = 0;\n\n\t/* Catch invalid receive attempts */\n\tif (unlikely(!buf_len))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (unlikely((sock->state == SS_UNCONNECTED))) {\n\t\tres = -ENOTCONN;\n\t\tgoto exit;\n\t}\n\n\t/* will be updated in set_orig_addr() if needed */\n\tm->msg_namelen = 0;\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, buf_len);\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\nrestart:\n\t/* Look for a message in receive queue; wait if necessary */\n\twhile (skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tif (sock->state == SS_DISCONNECTING) {\n\t\t\tres = -ENOTCONN;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (timeout <= 0L) {\n\t\t\tres = timeout ? timeout : -EWOULDBLOCK;\n\t\t\tgoto exit;\n\t\t}\n\t\trelease_sock(sk);\n\t\ttimeout = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t\t\t   tipc_rx_ready(sock),\n\t\t\t\t\t\t\t   timeout);\n\t\tlock_sock(sk);\n\t}\n\n\t/* Look at first message in receive queue */\n\tbuf = skb_peek(&sk->sk_receive_queue);\n\tmsg = buf_msg(buf);\n\tsz = msg_data_sz(msg);\n\terr = msg_errcode(msg);\n\n\t/* Discard an empty non-errored message & try again */\n\tif ((!sz) && (!err)) {\n\t\tadvance_rx_queue(sk);\n\t\tgoto restart;\n\t}\n\n\t/* Optionally capture sender's address & ancillary data of first msg */\n\tif (sz_copied == 0) {\n\t\tset_orig_addr(m, msg);\n\t\tres = anc_data_recv(m, msg, tport);\n\t\tif (res)\n\t\t\tgoto exit;\n\t}\n\n\t/* Capture message data (if valid) & compute return value (always) */\n\tif (!err) {\n\t\tu32 offset = (u32)(unsigned long)(TIPC_SKB_CB(buf)->handle);\n\n\t\tsz -= offset;\n\t\tneeded = (buf_len - sz_copied);\n\t\tsz_to_copy = (sz <= needed) ? sz : needed;\n\n\t\tres = skb_copy_datagram_iovec(buf, msg_hdr_sz(msg) + offset,\n\t\t\t\t\t      m->msg_iov, sz_to_copy);\n\t\tif (res)\n\t\t\tgoto exit;\n\n\t\tsz_copied += sz_to_copy;\n\n\t\tif (sz_to_copy < sz) {\n\t\t\tif (!(flags & MSG_PEEK))\n\t\t\t\tTIPC_SKB_CB(buf)->handle =\n\t\t\t\t(void *)(unsigned long)(offset + sz_to_copy);\n\t\t\tgoto exit;\n\t\t}\n\t} else {\n\t\tif (sz_copied != 0)\n\t\t\tgoto exit; /* can't add error msg to valid data */\n\n\t\tif ((err == TIPC_CONN_SHUTDOWN) || m->msg_control)\n\t\t\tres = 0;\n\t\telse\n\t\t\tres = -ECONNRESET;\n\t}\n\n\t/* Consume received message (optional) */\n\tif (likely(!(flags & MSG_PEEK))) {\n\t\tif (unlikely(++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))\n\t\t\ttipc_acknowledge(tport->ref, tport->conn_unacked);\n\t\tadvance_rx_queue(sk);\n\t}\n\n\t/* Loop around if more data is required */\n\tif ((sz_copied < buf_len) &&\t/* didn't get all requested data */\n\t    (!skb_queue_empty(&sk->sk_receive_queue) ||\n\t    (sz_copied < target)) &&\t/* and more is ready or required */\n\t    (!(flags & MSG_PEEK)) &&\t/* and aren't just peeking at data */\n\t    (!err))\t\t\t/* and haven't reached a FIN */\n\t\tgoto restart;\n\nexit:\n\trelease_sock(sk);\n\treturn sz_copied ? sz_copied : res;\n}",
        "code_after_change": "static int recv_stream(struct kiocb *iocb, struct socket *sock,\n\t\t       struct msghdr *m, size_t buf_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tipc_port *tport = tipc_sk_port(sk);\n\tstruct sk_buff *buf;\n\tstruct tipc_msg *msg;\n\tlong timeout;\n\tunsigned int sz;\n\tint sz_to_copy, target, needed;\n\tint sz_copied = 0;\n\tu32 err;\n\tint res = 0;\n\n\t/* Catch invalid receive attempts */\n\tif (unlikely(!buf_len))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (unlikely((sock->state == SS_UNCONNECTED))) {\n\t\tres = -ENOTCONN;\n\t\tgoto exit;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, buf_len);\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\nrestart:\n\t/* Look for a message in receive queue; wait if necessary */\n\twhile (skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tif (sock->state == SS_DISCONNECTING) {\n\t\t\tres = -ENOTCONN;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (timeout <= 0L) {\n\t\t\tres = timeout ? timeout : -EWOULDBLOCK;\n\t\t\tgoto exit;\n\t\t}\n\t\trelease_sock(sk);\n\t\ttimeout = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t\t\t   tipc_rx_ready(sock),\n\t\t\t\t\t\t\t   timeout);\n\t\tlock_sock(sk);\n\t}\n\n\t/* Look at first message in receive queue */\n\tbuf = skb_peek(&sk->sk_receive_queue);\n\tmsg = buf_msg(buf);\n\tsz = msg_data_sz(msg);\n\terr = msg_errcode(msg);\n\n\t/* Discard an empty non-errored message & try again */\n\tif ((!sz) && (!err)) {\n\t\tadvance_rx_queue(sk);\n\t\tgoto restart;\n\t}\n\n\t/* Optionally capture sender's address & ancillary data of first msg */\n\tif (sz_copied == 0) {\n\t\tset_orig_addr(m, msg);\n\t\tres = anc_data_recv(m, msg, tport);\n\t\tif (res)\n\t\t\tgoto exit;\n\t}\n\n\t/* Capture message data (if valid) & compute return value (always) */\n\tif (!err) {\n\t\tu32 offset = (u32)(unsigned long)(TIPC_SKB_CB(buf)->handle);\n\n\t\tsz -= offset;\n\t\tneeded = (buf_len - sz_copied);\n\t\tsz_to_copy = (sz <= needed) ? sz : needed;\n\n\t\tres = skb_copy_datagram_iovec(buf, msg_hdr_sz(msg) + offset,\n\t\t\t\t\t      m->msg_iov, sz_to_copy);\n\t\tif (res)\n\t\t\tgoto exit;\n\n\t\tsz_copied += sz_to_copy;\n\n\t\tif (sz_to_copy < sz) {\n\t\t\tif (!(flags & MSG_PEEK))\n\t\t\t\tTIPC_SKB_CB(buf)->handle =\n\t\t\t\t(void *)(unsigned long)(offset + sz_to_copy);\n\t\t\tgoto exit;\n\t\t}\n\t} else {\n\t\tif (sz_copied != 0)\n\t\t\tgoto exit; /* can't add error msg to valid data */\n\n\t\tif ((err == TIPC_CONN_SHUTDOWN) || m->msg_control)\n\t\t\tres = 0;\n\t\telse\n\t\t\tres = -ECONNRESET;\n\t}\n\n\t/* Consume received message (optional) */\n\tif (likely(!(flags & MSG_PEEK))) {\n\t\tif (unlikely(++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))\n\t\t\ttipc_acknowledge(tport->ref, tport->conn_unacked);\n\t\tadvance_rx_queue(sk);\n\t}\n\n\t/* Loop around if more data is required */\n\tif ((sz_copied < buf_len) &&\t/* didn't get all requested data */\n\t    (!skb_queue_empty(&sk->sk_receive_queue) ||\n\t    (sz_copied < target)) &&\t/* and more is ready or required */\n\t    (!(flags & MSG_PEEK)) &&\t/* and aren't just peeking at data */\n\t    (!err))\t\t\t/* and haven't reached a FIN */\n\t\tgoto restart;\n\nexit:\n\trelease_sock(sk);\n\treturn sz_copied ? sz_copied : res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,9 +22,6 @@\n \t\tres = -ENOTCONN;\n \t\tgoto exit;\n \t}\n-\n-\t/* will be updated in set_orig_addr() if needed */\n-\tm->msg_namelen = 0;\n \n \ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, buf_len);\n \ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\t/* will be updated in set_orig_addr() if needed */",
                "\tm->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 399
    },
    {
        "cve_id": "CVE-2021-3655",
        "code_before_change": "static __be16 sctp_process_asconf_param(struct sctp_association *asoc,\n\t\t\t\t\tstruct sctp_chunk *asconf,\n\t\t\t\t\tstruct sctp_addip_param *asconf_param)\n{\n\tunion sctp_addr_param *addr_param;\n\tstruct sctp_transport *peer;\n\tunion sctp_addr\taddr;\n\tstruct sctp_af *af;\n\n\taddr_param = (void *)asconf_param + sizeof(*asconf_param);\n\n\tif (asconf_param->param_hdr.type != SCTP_PARAM_ADD_IP &&\n\t    asconf_param->param_hdr.type != SCTP_PARAM_DEL_IP &&\n\t    asconf_param->param_hdr.type != SCTP_PARAM_SET_PRIMARY)\n\t\treturn SCTP_ERROR_UNKNOWN_PARAM;\n\n\tswitch (addr_param->p.type) {\n\tcase SCTP_PARAM_IPV6_ADDRESS:\n\t\tif (!asoc->peer.ipv6_address)\n\t\t\treturn SCTP_ERROR_DNS_FAILED;\n\t\tbreak;\n\tcase SCTP_PARAM_IPV4_ADDRESS:\n\t\tif (!asoc->peer.ipv4_address)\n\t\t\treturn SCTP_ERROR_DNS_FAILED;\n\t\tbreak;\n\tdefault:\n\t\treturn SCTP_ERROR_DNS_FAILED;\n\t}\n\n\taf = sctp_get_af_specific(param_type2af(addr_param->p.type));\n\tif (unlikely(!af))\n\t\treturn SCTP_ERROR_DNS_FAILED;\n\n\taf->from_addr_param(&addr, addr_param, htons(asoc->peer.port), 0);\n\n\t/* ADDIP 4.2.1  This parameter MUST NOT contain a broadcast\n\t * or multicast address.\n\t * (note: wildcard is permitted and requires special handling so\n\t *  make sure we check for that)\n\t */\n\tif (!af->is_any(&addr) && !af->addr_valid(&addr, NULL, asconf->skb))\n\t\treturn SCTP_ERROR_DNS_FAILED;\n\n\tswitch (asconf_param->param_hdr.type) {\n\tcase SCTP_PARAM_ADD_IP:\n\t\t/* Section 4.2.1:\n\t\t * If the address 0.0.0.0 or ::0 is provided, the source\n\t\t * address of the packet MUST be added.\n\t\t */\n\t\tif (af->is_any(&addr))\n\t\t\tmemcpy(&addr, &asconf->source, sizeof(addr));\n\n\t\tif (security_sctp_bind_connect(asoc->ep->base.sk,\n\t\t\t\t\t       SCTP_PARAM_ADD_IP,\n\t\t\t\t\t       (struct sockaddr *)&addr,\n\t\t\t\t\t       af->sockaddr_len))\n\t\t\treturn SCTP_ERROR_REQ_REFUSED;\n\n\t\t/* ADDIP 4.3 D9) If an endpoint receives an ADD IP address\n\t\t * request and does not have the local resources to add this\n\t\t * new address to the association, it MUST return an Error\n\t\t * Cause TLV set to the new error code 'Operation Refused\n\t\t * Due to Resource Shortage'.\n\t\t */\n\n\t\tpeer = sctp_assoc_add_peer(asoc, &addr, GFP_ATOMIC, SCTP_UNCONFIRMED);\n\t\tif (!peer)\n\t\t\treturn SCTP_ERROR_RSRC_LOW;\n\n\t\t/* Start the heartbeat timer. */\n\t\tsctp_transport_reset_hb_timer(peer);\n\t\tasoc->new_transport = peer;\n\t\tbreak;\n\tcase SCTP_PARAM_DEL_IP:\n\t\t/* ADDIP 4.3 D7) If a request is received to delete the\n\t\t * last remaining IP address of a peer endpoint, the receiver\n\t\t * MUST send an Error Cause TLV with the error cause set to the\n\t\t * new error code 'Request to Delete Last Remaining IP Address'.\n\t\t */\n\t\tif (asoc->peer.transport_count == 1)\n\t\t\treturn SCTP_ERROR_DEL_LAST_IP;\n\n\t\t/* ADDIP 4.3 D8) If a request is received to delete an IP\n\t\t * address which is also the source address of the IP packet\n\t\t * which contained the ASCONF chunk, the receiver MUST reject\n\t\t * this request. To reject the request the receiver MUST send\n\t\t * an Error Cause TLV set to the new error code 'Request to\n\t\t * Delete Source IP Address'\n\t\t */\n\t\tif (sctp_cmp_addr_exact(&asconf->source, &addr))\n\t\t\treturn SCTP_ERROR_DEL_SRC_IP;\n\n\t\t/* Section 4.2.2\n\t\t * If the address 0.0.0.0 or ::0 is provided, all\n\t\t * addresses of the peer except\tthe source address of the\n\t\t * packet MUST be deleted.\n\t\t */\n\t\tif (af->is_any(&addr)) {\n\t\t\tsctp_assoc_set_primary(asoc, asconf->transport);\n\t\t\tsctp_assoc_del_nonprimary_peers(asoc,\n\t\t\t\t\t\t\tasconf->transport);\n\t\t\treturn SCTP_ERROR_NO_ERROR;\n\t\t}\n\n\t\t/* If the address is not part of the association, the\n\t\t * ASCONF-ACK with Error Cause Indication Parameter\n\t\t * which including cause of Unresolvable Address should\n\t\t * be sent.\n\t\t */\n\t\tpeer = sctp_assoc_lookup_paddr(asoc, &addr);\n\t\tif (!peer)\n\t\t\treturn SCTP_ERROR_DNS_FAILED;\n\n\t\tsctp_assoc_rm_peer(asoc, peer);\n\t\tbreak;\n\tcase SCTP_PARAM_SET_PRIMARY:\n\t\t/* ADDIP Section 4.2.4\n\t\t * If the address 0.0.0.0 or ::0 is provided, the receiver\n\t\t * MAY mark the source address of the packet as its\n\t\t * primary.\n\t\t */\n\t\tif (af->is_any(&addr))\n\t\t\tmemcpy(&addr, sctp_source(asconf), sizeof(addr));\n\n\t\tif (security_sctp_bind_connect(asoc->ep->base.sk,\n\t\t\t\t\t       SCTP_PARAM_SET_PRIMARY,\n\t\t\t\t\t       (struct sockaddr *)&addr,\n\t\t\t\t\t       af->sockaddr_len))\n\t\t\treturn SCTP_ERROR_REQ_REFUSED;\n\n\t\tpeer = sctp_assoc_lookup_paddr(asoc, &addr);\n\t\tif (!peer)\n\t\t\treturn SCTP_ERROR_DNS_FAILED;\n\n\t\tsctp_assoc_set_primary(asoc, peer);\n\t\tbreak;\n\t}\n\n\treturn SCTP_ERROR_NO_ERROR;\n}",
        "code_after_change": "static __be16 sctp_process_asconf_param(struct sctp_association *asoc,\n\t\t\t\t\tstruct sctp_chunk *asconf,\n\t\t\t\t\tstruct sctp_addip_param *asconf_param)\n{\n\tunion sctp_addr_param *addr_param;\n\tstruct sctp_transport *peer;\n\tunion sctp_addr\taddr;\n\tstruct sctp_af *af;\n\n\taddr_param = (void *)asconf_param + sizeof(*asconf_param);\n\n\tif (asconf_param->param_hdr.type != SCTP_PARAM_ADD_IP &&\n\t    asconf_param->param_hdr.type != SCTP_PARAM_DEL_IP &&\n\t    asconf_param->param_hdr.type != SCTP_PARAM_SET_PRIMARY)\n\t\treturn SCTP_ERROR_UNKNOWN_PARAM;\n\n\tswitch (addr_param->p.type) {\n\tcase SCTP_PARAM_IPV6_ADDRESS:\n\t\tif (!asoc->peer.ipv6_address)\n\t\t\treturn SCTP_ERROR_DNS_FAILED;\n\t\tbreak;\n\tcase SCTP_PARAM_IPV4_ADDRESS:\n\t\tif (!asoc->peer.ipv4_address)\n\t\t\treturn SCTP_ERROR_DNS_FAILED;\n\t\tbreak;\n\tdefault:\n\t\treturn SCTP_ERROR_DNS_FAILED;\n\t}\n\n\taf = sctp_get_af_specific(param_type2af(addr_param->p.type));\n\tif (unlikely(!af))\n\t\treturn SCTP_ERROR_DNS_FAILED;\n\n\tif (!af->from_addr_param(&addr, addr_param, htons(asoc->peer.port), 0))\n\t\treturn SCTP_ERROR_DNS_FAILED;\n\n\t/* ADDIP 4.2.1  This parameter MUST NOT contain a broadcast\n\t * or multicast address.\n\t * (note: wildcard is permitted and requires special handling so\n\t *  make sure we check for that)\n\t */\n\tif (!af->is_any(&addr) && !af->addr_valid(&addr, NULL, asconf->skb))\n\t\treturn SCTP_ERROR_DNS_FAILED;\n\n\tswitch (asconf_param->param_hdr.type) {\n\tcase SCTP_PARAM_ADD_IP:\n\t\t/* Section 4.2.1:\n\t\t * If the address 0.0.0.0 or ::0 is provided, the source\n\t\t * address of the packet MUST be added.\n\t\t */\n\t\tif (af->is_any(&addr))\n\t\t\tmemcpy(&addr, &asconf->source, sizeof(addr));\n\n\t\tif (security_sctp_bind_connect(asoc->ep->base.sk,\n\t\t\t\t\t       SCTP_PARAM_ADD_IP,\n\t\t\t\t\t       (struct sockaddr *)&addr,\n\t\t\t\t\t       af->sockaddr_len))\n\t\t\treturn SCTP_ERROR_REQ_REFUSED;\n\n\t\t/* ADDIP 4.3 D9) If an endpoint receives an ADD IP address\n\t\t * request and does not have the local resources to add this\n\t\t * new address to the association, it MUST return an Error\n\t\t * Cause TLV set to the new error code 'Operation Refused\n\t\t * Due to Resource Shortage'.\n\t\t */\n\n\t\tpeer = sctp_assoc_add_peer(asoc, &addr, GFP_ATOMIC, SCTP_UNCONFIRMED);\n\t\tif (!peer)\n\t\t\treturn SCTP_ERROR_RSRC_LOW;\n\n\t\t/* Start the heartbeat timer. */\n\t\tsctp_transport_reset_hb_timer(peer);\n\t\tasoc->new_transport = peer;\n\t\tbreak;\n\tcase SCTP_PARAM_DEL_IP:\n\t\t/* ADDIP 4.3 D7) If a request is received to delete the\n\t\t * last remaining IP address of a peer endpoint, the receiver\n\t\t * MUST send an Error Cause TLV with the error cause set to the\n\t\t * new error code 'Request to Delete Last Remaining IP Address'.\n\t\t */\n\t\tif (asoc->peer.transport_count == 1)\n\t\t\treturn SCTP_ERROR_DEL_LAST_IP;\n\n\t\t/* ADDIP 4.3 D8) If a request is received to delete an IP\n\t\t * address which is also the source address of the IP packet\n\t\t * which contained the ASCONF chunk, the receiver MUST reject\n\t\t * this request. To reject the request the receiver MUST send\n\t\t * an Error Cause TLV set to the new error code 'Request to\n\t\t * Delete Source IP Address'\n\t\t */\n\t\tif (sctp_cmp_addr_exact(&asconf->source, &addr))\n\t\t\treturn SCTP_ERROR_DEL_SRC_IP;\n\n\t\t/* Section 4.2.2\n\t\t * If the address 0.0.0.0 or ::0 is provided, all\n\t\t * addresses of the peer except\tthe source address of the\n\t\t * packet MUST be deleted.\n\t\t */\n\t\tif (af->is_any(&addr)) {\n\t\t\tsctp_assoc_set_primary(asoc, asconf->transport);\n\t\t\tsctp_assoc_del_nonprimary_peers(asoc,\n\t\t\t\t\t\t\tasconf->transport);\n\t\t\treturn SCTP_ERROR_NO_ERROR;\n\t\t}\n\n\t\t/* If the address is not part of the association, the\n\t\t * ASCONF-ACK with Error Cause Indication Parameter\n\t\t * which including cause of Unresolvable Address should\n\t\t * be sent.\n\t\t */\n\t\tpeer = sctp_assoc_lookup_paddr(asoc, &addr);\n\t\tif (!peer)\n\t\t\treturn SCTP_ERROR_DNS_FAILED;\n\n\t\tsctp_assoc_rm_peer(asoc, peer);\n\t\tbreak;\n\tcase SCTP_PARAM_SET_PRIMARY:\n\t\t/* ADDIP Section 4.2.4\n\t\t * If the address 0.0.0.0 or ::0 is provided, the receiver\n\t\t * MAY mark the source address of the packet as its\n\t\t * primary.\n\t\t */\n\t\tif (af->is_any(&addr))\n\t\t\tmemcpy(&addr, sctp_source(asconf), sizeof(addr));\n\n\t\tif (security_sctp_bind_connect(asoc->ep->base.sk,\n\t\t\t\t\t       SCTP_PARAM_SET_PRIMARY,\n\t\t\t\t\t       (struct sockaddr *)&addr,\n\t\t\t\t\t       af->sockaddr_len))\n\t\t\treturn SCTP_ERROR_REQ_REFUSED;\n\n\t\tpeer = sctp_assoc_lookup_paddr(asoc, &addr);\n\t\tif (!peer)\n\t\t\treturn SCTP_ERROR_DNS_FAILED;\n\n\t\tsctp_assoc_set_primary(asoc, peer);\n\t\tbreak;\n\t}\n\n\treturn SCTP_ERROR_NO_ERROR;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,7 +31,8 @@\n \tif (unlikely(!af))\n \t\treturn SCTP_ERROR_DNS_FAILED;\n \n-\taf->from_addr_param(&addr, addr_param, htons(asoc->peer.port), 0);\n+\tif (!af->from_addr_param(&addr, addr_param, htons(asoc->peer.port), 0))\n+\t\treturn SCTP_ERROR_DNS_FAILED;\n \n \t/* ADDIP 4.2.1  This parameter MUST NOT contain a broadcast\n \t * or multicast address.",
        "function_modified_lines": {
            "added": [
                "\tif (!af->from_addr_param(&addr, addr_param, htons(asoc->peer.port), 0))",
                "\t\treturn SCTP_ERROR_DNS_FAILED;"
            ],
            "deleted": [
                "\taf->from_addr_param(&addr, addr_param, htons(asoc->peer.port), 0);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel in versions prior to v5.14-rc1. Missing size validations on inbound SCTP packets may allow the kernel to read uninitialized memory.",
        "id": 3034
    },
    {
        "cve_id": "CVE-2016-2143",
        "code_before_change": "static inline void arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t struct mm_struct *mm)\n{\n\tif (oldmm->context.asce_limit < mm->context.asce_limit)\n\t\tcrst_table_downgrade(mm, oldmm->context.asce_limit);\n}",
        "code_after_change": "static inline void arch_dup_mmap(struct mm_struct *oldmm,\n\t\t\t\t struct mm_struct *mm)\n{\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,4 @@\n static inline void arch_dup_mmap(struct mm_struct *oldmm,\n \t\t\t\t struct mm_struct *mm)\n {\n-\tif (oldmm->context.asce_limit < mm->context.asce_limit)\n-\t\tcrst_table_downgrade(mm, oldmm->context.asce_limit);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tif (oldmm->context.asce_limit < mm->context.asce_limit)",
                "\t\tcrst_table_downgrade(mm, oldmm->context.asce_limit);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The fork implementation in the Linux kernel before 4.5 on s390 platforms mishandles the case of four page-table levels, which allows local users to cause a denial of service (system crash) or possibly have unspecified other impact via a crafted application, related to arch/s390/include/asm/mmu_context.h and arch/s390/include/asm/pgalloc.h.",
        "id": 923
    },
    {
        "cve_id": "CVE-2016-9191",
        "code_before_change": "static int proc_sys_readdir(struct file *file, struct dir_context *ctx)\n{\n\tstruct ctl_table_header *head = grab_header(file_inode(file));\n\tstruct ctl_table_header *h = NULL;\n\tstruct ctl_table *entry;\n\tstruct ctl_dir *ctl_dir;\n\tunsigned long pos;\n\n\tif (IS_ERR(head))\n\t\treturn PTR_ERR(head);\n\n\tctl_dir = container_of(head, struct ctl_dir, header);\n\n\tif (!dir_emit_dots(file, ctx))\n\t\treturn 0;\n\n\tpos = 2;\n\n\tfor (first_entry(ctl_dir, &h, &entry); h; next_entry(&h, &entry)) {\n\t\tif (!scan(h, entry, &pos, file, ctx)) {\n\t\t\tsysctl_head_finish(h);\n\t\t\tbreak;\n\t\t}\n\t}\n\tsysctl_head_finish(head);\n\treturn 0;\n}",
        "code_after_change": "static int proc_sys_readdir(struct file *file, struct dir_context *ctx)\n{\n\tstruct ctl_table_header *head = grab_header(file_inode(file));\n\tstruct ctl_table_header *h = NULL;\n\tstruct ctl_table *entry;\n\tstruct ctl_dir *ctl_dir;\n\tunsigned long pos;\n\n\tif (IS_ERR(head))\n\t\treturn PTR_ERR(head);\n\n\tctl_dir = container_of(head, struct ctl_dir, header);\n\n\tif (!dir_emit_dots(file, ctx))\n\t\tgoto out;\n\n\tpos = 2;\n\n\tfor (first_entry(ctl_dir, &h, &entry); h; next_entry(&h, &entry)) {\n\t\tif (!scan(h, entry, &pos, file, ctx)) {\n\t\t\tsysctl_head_finish(h);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tsysctl_head_finish(head);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,7 @@\n \tctl_dir = container_of(head, struct ctl_dir, header);\n \n \tif (!dir_emit_dots(file, ctx))\n-\t\treturn 0;\n+\t\tgoto out;\n \n \tpos = 2;\n \n@@ -22,6 +22,7 @@\n \t\t\tbreak;\n \t\t}\n \t}\n+out:\n \tsysctl_head_finish(head);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tgoto out;",
                "out:"
            ],
            "deleted": [
                "\t\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-20",
            "CWE-399"
        ],
        "cve_description": "The cgroup offline implementation in the Linux kernel through 4.8.11 mishandles certain drain operations, which allows local users to cause a denial of service (system hang) by leveraging access to a container environment for executing a crafted application, as demonstrated by trinity.",
        "id": 1143
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int ipx_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\tstruct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct ipx_sock *ipxs = ipx_sk(sk);\n\tstruct sockaddr_ipx *sipx = (struct sockaddr_ipx *)msg->msg_name;\n\tstruct ipxhdr *ipx = NULL;\n\tstruct sk_buff *skb;\n\tint copied, rc;\n\n\tlock_sock(sk);\n\t/* put the autobinding in */\n\tif (!ipxs->port) {\n\t\tstruct sockaddr_ipx uaddr;\n\n\t\tuaddr.sipx_port\t\t= 0;\n\t\tuaddr.sipx_network \t= 0;\n\n#ifdef CONFIG_IPX_INTERN\n\t\trc = -ENETDOWN;\n\t\tif (!ipxs->intrfc)\n\t\t\tgoto out; /* Someone zonked the iface */\n\t\tmemcpy(uaddr.sipx_node, ipxs->intrfc->if_node, IPX_NODE_LEN);\n#endif\t/* CONFIG_IPX_INTERN */\n\n\t\trc = __ipx_bind(sock, (struct sockaddr *)&uaddr,\n\t\t\t      sizeof(struct sockaddr_ipx));\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\trc = -ENOTCONN;\n\tif (sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out;\n\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\tflags & MSG_DONTWAIT, &rc);\n\tif (!skb)\n\t\tgoto out;\n\n\tipx \t= ipx_hdr(skb);\n\tcopied \t= ntohs(ipx->ipx_pktsize) - sizeof(struct ipxhdr);\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\trc = skb_copy_datagram_iovec(skb, sizeof(struct ipxhdr), msg->msg_iov,\n\t\t\t\t     copied);\n\tif (rc)\n\t\tgoto out_free;\n\tif (skb->tstamp.tv64)\n\t\tsk->sk_stamp = skb->tstamp;\n\n\tmsg->msg_namelen = sizeof(*sipx);\n\n\tif (sipx) {\n\t\tsipx->sipx_family\t= AF_IPX;\n\t\tsipx->sipx_port\t\t= ipx->ipx_source.sock;\n\t\tmemcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);\n\t\tsipx->sipx_network\t= IPX_SKB_CB(skb)->ipx_source_net;\n\t\tsipx->sipx_type \t= ipx->ipx_type;\n\t\tsipx->sipx_zero\t\t= 0;\n\t}\n\trc = copied;\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\trelease_sock(sk);\n\treturn rc;\n}",
        "code_after_change": "static int ipx_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\tstruct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct ipx_sock *ipxs = ipx_sk(sk);\n\tstruct sockaddr_ipx *sipx = (struct sockaddr_ipx *)msg->msg_name;\n\tstruct ipxhdr *ipx = NULL;\n\tstruct sk_buff *skb;\n\tint copied, rc;\n\n\tlock_sock(sk);\n\t/* put the autobinding in */\n\tif (!ipxs->port) {\n\t\tstruct sockaddr_ipx uaddr;\n\n\t\tuaddr.sipx_port\t\t= 0;\n\t\tuaddr.sipx_network \t= 0;\n\n#ifdef CONFIG_IPX_INTERN\n\t\trc = -ENETDOWN;\n\t\tif (!ipxs->intrfc)\n\t\t\tgoto out; /* Someone zonked the iface */\n\t\tmemcpy(uaddr.sipx_node, ipxs->intrfc->if_node, IPX_NODE_LEN);\n#endif\t/* CONFIG_IPX_INTERN */\n\n\t\trc = __ipx_bind(sock, (struct sockaddr *)&uaddr,\n\t\t\t      sizeof(struct sockaddr_ipx));\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\trc = -ENOTCONN;\n\tif (sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out;\n\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\tflags & MSG_DONTWAIT, &rc);\n\tif (!skb)\n\t\tgoto out;\n\n\tipx \t= ipx_hdr(skb);\n\tcopied \t= ntohs(ipx->ipx_pktsize) - sizeof(struct ipxhdr);\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\trc = skb_copy_datagram_iovec(skb, sizeof(struct ipxhdr), msg->msg_iov,\n\t\t\t\t     copied);\n\tif (rc)\n\t\tgoto out_free;\n\tif (skb->tstamp.tv64)\n\t\tsk->sk_stamp = skb->tstamp;\n\n\tif (sipx) {\n\t\tsipx->sipx_family\t= AF_IPX;\n\t\tsipx->sipx_port\t\t= ipx->ipx_source.sock;\n\t\tmemcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);\n\t\tsipx->sipx_network\t= IPX_SKB_CB(skb)->ipx_source_net;\n\t\tsipx->sipx_type \t= ipx->ipx_type;\n\t\tsipx->sipx_zero\t\t= 0;\n\t\tmsg->msg_namelen\t= sizeof(*sipx);\n\t}\n\trc = copied;\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\trelease_sock(sk);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -52,8 +52,6 @@\n \tif (skb->tstamp.tv64)\n \t\tsk->sk_stamp = skb->tstamp;\n \n-\tmsg->msg_namelen = sizeof(*sipx);\n-\n \tif (sipx) {\n \t\tsipx->sipx_family\t= AF_IPX;\n \t\tsipx->sipx_port\t\t= ipx->ipx_source.sock;\n@@ -61,6 +59,7 @@\n \t\tsipx->sipx_network\t= IPX_SKB_CB(skb)->ipx_source_net;\n \t\tsipx->sipx_type \t= ipx->ipx_type;\n \t\tsipx->sipx_zero\t\t= 0;\n+\t\tmsg->msg_namelen\t= sizeof(*sipx);\n \t}\n \trc = copied;\n ",
        "function_modified_lines": {
            "added": [
                "\t\tmsg->msg_namelen\t= sizeof(*sipx);"
            ],
            "deleted": [
                "\tmsg->msg_namelen = sizeof(*sipx);",
                ""
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 382
    },
    {
        "cve_id": "CVE-2016-6197",
        "code_before_change": "static int ovl_remove_upper(struct dentry *dentry, bool is_dir)\n{\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct inode *dir = upperdir->d_inode;\n\tstruct dentry *upper = ovl_dentry_upper(dentry);\n\tint err;\n\n\tinode_lock_nested(dir, I_MUTEX_PARENT);\n\terr = -ESTALE;\n\tif (upper->d_parent == upperdir) {\n\t\t/* Don't let d_delete() think it can reset d_inode */\n\t\tdget(upper);\n\t\tif (is_dir)\n\t\t\terr = vfs_rmdir(dir, upper);\n\t\telse\n\t\t\terr = vfs_unlink(dir, upper, NULL);\n\t\tdput(upper);\n\t\tovl_dentry_version_inc(dentry->d_parent);\n\t}\n\n\t/*\n\t * Keeping this dentry hashed would mean having to release\n\t * upperpath/lowerpath, which could only be done if we are the\n\t * sole user of this dentry.  Too tricky...  Just unhash for\n\t * now.\n\t */\n\tif (!err)\n\t\td_drop(dentry);\n\tinode_unlock(dir);\n\n\treturn err;\n}",
        "code_after_change": "static int ovl_remove_upper(struct dentry *dentry, bool is_dir)\n{\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct inode *dir = upperdir->d_inode;\n\tstruct dentry *upper;\n\tint err;\n\n\tinode_lock_nested(dir, I_MUTEX_PARENT);\n\tupper = lookup_one_len(dentry->d_name.name, upperdir,\n\t\t\t       dentry->d_name.len);\n\terr = PTR_ERR(upper);\n\tif (IS_ERR(upper))\n\t\tgoto out_unlock;\n\n\terr = -ESTALE;\n\tif (upper == ovl_dentry_upper(dentry)) {\n\t\tif (is_dir)\n\t\t\terr = vfs_rmdir(dir, upper);\n\t\telse\n\t\t\terr = vfs_unlink(dir, upper, NULL);\n\t\tovl_dentry_version_inc(dentry->d_parent);\n\t}\n\tdput(upper);\n\n\t/*\n\t * Keeping this dentry hashed would mean having to release\n\t * upperpath/lowerpath, which could only be done if we are the\n\t * sole user of this dentry.  Too tricky...  Just unhash for\n\t * now.\n\t */\n\tif (!err)\n\t\td_drop(dentry);\nout_unlock:\n\tinode_unlock(dir);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,21 +2,25 @@\n {\n \tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n \tstruct inode *dir = upperdir->d_inode;\n-\tstruct dentry *upper = ovl_dentry_upper(dentry);\n+\tstruct dentry *upper;\n \tint err;\n \n \tinode_lock_nested(dir, I_MUTEX_PARENT);\n+\tupper = lookup_one_len(dentry->d_name.name, upperdir,\n+\t\t\t       dentry->d_name.len);\n+\terr = PTR_ERR(upper);\n+\tif (IS_ERR(upper))\n+\t\tgoto out_unlock;\n+\n \terr = -ESTALE;\n-\tif (upper->d_parent == upperdir) {\n-\t\t/* Don't let d_delete() think it can reset d_inode */\n-\t\tdget(upper);\n+\tif (upper == ovl_dentry_upper(dentry)) {\n \t\tif (is_dir)\n \t\t\terr = vfs_rmdir(dir, upper);\n \t\telse\n \t\t\terr = vfs_unlink(dir, upper, NULL);\n-\t\tdput(upper);\n \t\tovl_dentry_version_inc(dentry->d_parent);\n \t}\n+\tdput(upper);\n \n \t/*\n \t * Keeping this dentry hashed would mean having to release\n@@ -26,6 +30,7 @@\n \t */\n \tif (!err)\n \t\td_drop(dentry);\n+out_unlock:\n \tinode_unlock(dir);\n \n \treturn err;",
        "function_modified_lines": {
            "added": [
                "\tstruct dentry *upper;",
                "\tupper = lookup_one_len(dentry->d_name.name, upperdir,",
                "\t\t\t       dentry->d_name.len);",
                "\terr = PTR_ERR(upper);",
                "\tif (IS_ERR(upper))",
                "\t\tgoto out_unlock;",
                "",
                "\tif (upper == ovl_dentry_upper(dentry)) {",
                "\tdput(upper);",
                "out_unlock:"
            ],
            "deleted": [
                "\tstruct dentry *upper = ovl_dentry_upper(dentry);",
                "\tif (upper->d_parent == upperdir) {",
                "\t\t/* Don't let d_delete() think it can reset d_inode */",
                "\t\tdget(upper);",
                "\t\tdput(upper);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "fs/overlayfs/dir.c in the OverlayFS filesystem implementation in the Linux kernel before 4.6 does not properly verify the upper dentry before proceeding with unlink and rename system-call processing, which allows local users to cause a denial of service (system crash) via a rename system call that specifies a self-hardlink.",
        "id": 1066
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static void __io_free_req_finish(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tput_task_struct(req->task);\n\n\tif (likely(!io_is_fallback_req(req)))\n\t\tkmem_cache_free(req_cachep, req);\n\telse\n\t\tclear_bit_unlock(0, (unsigned long *) &ctx->fallback_req);\n\tpercpu_ref_put(&ctx->refs);\n}",
        "code_after_change": "static void __io_free_req_finish(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = req->task->io_uring;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tatomic_long_inc(&tctx->req_complete);\n\tif (tctx->in_idle)\n\t\twake_up(&tctx->wait);\n\tput_task_struct(req->task);\n\n\tif (likely(!io_is_fallback_req(req)))\n\t\tkmem_cache_free(req_cachep, req);\n\telse\n\t\tclear_bit_unlock(0, (unsigned long *) &ctx->fallback_req);\n\tpercpu_ref_put(&ctx->refs);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,11 @@\n static void __io_free_req_finish(struct io_kiocb *req)\n {\n+\tstruct io_uring_task *tctx = req->task->io_uring;\n \tstruct io_ring_ctx *ctx = req->ctx;\n \n+\tatomic_long_inc(&tctx->req_complete);\n+\tif (tctx->in_idle)\n+\t\twake_up(&tctx->wait);\n \tput_task_struct(req->task);\n \n \tif (likely(!io_is_fallback_req(req)))",
        "function_modified_lines": {
            "added": [
                "\tstruct io_uring_task *tctx = req->task->io_uring;",
                "\tatomic_long_inc(&tctx->req_complete);",
                "\tif (tctx->in_idle)",
                "\t\twake_up(&tctx->wait);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2850
    },
    {
        "cve_id": "CVE-2017-6345",
        "code_before_change": "static void llc_sap_rcv(struct llc_sap *sap, struct sk_buff *skb,\n\t\t\tstruct sock *sk)\n{\n\tstruct llc_sap_state_ev *ev = llc_sap_ev(skb);\n\n\tev->type   = LLC_SAP_EV_TYPE_PDU;\n\tev->reason = 0;\n\tskb->sk = sk;\n\tllc_sap_state_process(sap, skb);\n}",
        "code_after_change": "static void llc_sap_rcv(struct llc_sap *sap, struct sk_buff *skb,\n\t\t\tstruct sock *sk)\n{\n\tstruct llc_sap_state_ev *ev = llc_sap_ev(skb);\n\n\tev->type   = LLC_SAP_EV_TYPE_PDU;\n\tev->reason = 0;\n\tskb_orphan(skb);\n\tsock_hold(sk);\n\tskb->sk = sk;\n\tskb->destructor = sock_efree;\n\tllc_sap_state_process(sap, skb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,9 @@\n \n \tev->type   = LLC_SAP_EV_TYPE_PDU;\n \tev->reason = 0;\n+\tskb_orphan(skb);\n+\tsock_hold(sk);\n \tskb->sk = sk;\n+\tskb->destructor = sock_efree;\n \tllc_sap_state_process(sap, skb);\n }",
        "function_modified_lines": {
            "added": [
                "\tskb_orphan(skb);",
                "\tsock_hold(sk);",
                "\tskb->destructor = sock_efree;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The LLC subsystem in the Linux kernel before 4.9.13 does not ensure that a certain destructor exists in required circumstances, which allows local users to cause a denial of service (BUG_ON) or possibly have unspecified other impact via crafted system calls.",
        "id": 1481
    },
    {
        "cve_id": "CVE-2020-12351",
        "code_before_change": "static int l2cap_data_rcv(struct l2cap_chan *chan, struct sk_buff *skb)\n{\n\tstruct l2cap_ctrl *control = &bt_cb(skb)->l2cap;\n\tu16 len;\n\tu8 event;\n\n\t__unpack_control(chan, skb);\n\n\tlen = skb->len;\n\n\t/*\n\t * We can just drop the corrupted I-frame here.\n\t * Receiver will miss it and start proper recovery\n\t * procedures and ask for retransmission.\n\t */\n\tif (l2cap_check_fcs(chan, skb))\n\t\tgoto drop;\n\n\tif (!control->sframe && control->sar == L2CAP_SAR_START)\n\t\tlen -= L2CAP_SDULEN_SIZE;\n\n\tif (chan->fcs == L2CAP_FCS_CRC16)\n\t\tlen -= L2CAP_FCS_SIZE;\n\n\tif (len > chan->mps) {\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto drop;\n\t}\n\n\tif ((chan->mode == L2CAP_MODE_ERTM ||\n\t     chan->mode == L2CAP_MODE_STREAMING) && sk_filter(chan->data, skb))\n\t\tgoto drop;\n\n\tif (!control->sframe) {\n\t\tint err;\n\n\t\tBT_DBG(\"iframe sar %d, reqseq %d, final %d, txseq %d\",\n\t\t       control->sar, control->reqseq, control->final,\n\t\t       control->txseq);\n\n\t\t/* Validate F-bit - F=0 always valid, F=1 only\n\t\t * valid in TX WAIT_F\n\t\t */\n\t\tif (control->final && chan->tx_state != L2CAP_TX_STATE_WAIT_F)\n\t\t\tgoto drop;\n\n\t\tif (chan->mode != L2CAP_MODE_STREAMING) {\n\t\t\tevent = L2CAP_EV_RECV_IFRAME;\n\t\t\terr = l2cap_rx(chan, control, skb, event);\n\t\t} else {\n\t\t\terr = l2cap_stream_rx(chan, control, skb);\n\t\t}\n\n\t\tif (err)\n\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t} else {\n\t\tconst u8 rx_func_to_event[4] = {\n\t\t\tL2CAP_EV_RECV_RR, L2CAP_EV_RECV_REJ,\n\t\t\tL2CAP_EV_RECV_RNR, L2CAP_EV_RECV_SREJ\n\t\t};\n\n\t\t/* Only I-frames are expected in streaming mode */\n\t\tif (chan->mode == L2CAP_MODE_STREAMING)\n\t\t\tgoto drop;\n\n\t\tBT_DBG(\"sframe reqseq %d, final %d, poll %d, super %d\",\n\t\t       control->reqseq, control->final, control->poll,\n\t\t       control->super);\n\n\t\tif (len != 0) {\n\t\t\tBT_ERR(\"Trailing bytes: %d in sframe\", len);\n\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* Validate F and P bits */\n\t\tif (control->final && (control->poll ||\n\t\t\t\t       chan->tx_state != L2CAP_TX_STATE_WAIT_F))\n\t\t\tgoto drop;\n\n\t\tevent = rx_func_to_event[control->super];\n\t\tif (l2cap_rx(chan, control, skb, event))\n\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t}\n\n\treturn 0;\n\ndrop:\n\tkfree_skb(skb);\n\treturn 0;\n}",
        "code_after_change": "static int l2cap_data_rcv(struct l2cap_chan *chan, struct sk_buff *skb)\n{\n\tstruct l2cap_ctrl *control = &bt_cb(skb)->l2cap;\n\tu16 len;\n\tu8 event;\n\n\t__unpack_control(chan, skb);\n\n\tlen = skb->len;\n\n\t/*\n\t * We can just drop the corrupted I-frame here.\n\t * Receiver will miss it and start proper recovery\n\t * procedures and ask for retransmission.\n\t */\n\tif (l2cap_check_fcs(chan, skb))\n\t\tgoto drop;\n\n\tif (!control->sframe && control->sar == L2CAP_SAR_START)\n\t\tlen -= L2CAP_SDULEN_SIZE;\n\n\tif (chan->fcs == L2CAP_FCS_CRC16)\n\t\tlen -= L2CAP_FCS_SIZE;\n\n\tif (len > chan->mps) {\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto drop;\n\t}\n\n\tif (chan->ops->filter) {\n\t\tif (chan->ops->filter(chan, skb))\n\t\t\tgoto drop;\n\t}\n\n\tif (!control->sframe) {\n\t\tint err;\n\n\t\tBT_DBG(\"iframe sar %d, reqseq %d, final %d, txseq %d\",\n\t\t       control->sar, control->reqseq, control->final,\n\t\t       control->txseq);\n\n\t\t/* Validate F-bit - F=0 always valid, F=1 only\n\t\t * valid in TX WAIT_F\n\t\t */\n\t\tif (control->final && chan->tx_state != L2CAP_TX_STATE_WAIT_F)\n\t\t\tgoto drop;\n\n\t\tif (chan->mode != L2CAP_MODE_STREAMING) {\n\t\t\tevent = L2CAP_EV_RECV_IFRAME;\n\t\t\terr = l2cap_rx(chan, control, skb, event);\n\t\t} else {\n\t\t\terr = l2cap_stream_rx(chan, control, skb);\n\t\t}\n\n\t\tif (err)\n\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t} else {\n\t\tconst u8 rx_func_to_event[4] = {\n\t\t\tL2CAP_EV_RECV_RR, L2CAP_EV_RECV_REJ,\n\t\t\tL2CAP_EV_RECV_RNR, L2CAP_EV_RECV_SREJ\n\t\t};\n\n\t\t/* Only I-frames are expected in streaming mode */\n\t\tif (chan->mode == L2CAP_MODE_STREAMING)\n\t\t\tgoto drop;\n\n\t\tBT_DBG(\"sframe reqseq %d, final %d, poll %d, super %d\",\n\t\t       control->reqseq, control->final, control->poll,\n\t\t       control->super);\n\n\t\tif (len != 0) {\n\t\t\tBT_ERR(\"Trailing bytes: %d in sframe\", len);\n\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* Validate F and P bits */\n\t\tif (control->final && (control->poll ||\n\t\t\t\t       chan->tx_state != L2CAP_TX_STATE_WAIT_F))\n\t\t\tgoto drop;\n\n\t\tevent = rx_func_to_event[control->super];\n\t\tif (l2cap_rx(chan, control, skb, event))\n\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t}\n\n\treturn 0;\n\ndrop:\n\tkfree_skb(skb);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,9 +27,10 @@\n \t\tgoto drop;\n \t}\n \n-\tif ((chan->mode == L2CAP_MODE_ERTM ||\n-\t     chan->mode == L2CAP_MODE_STREAMING) && sk_filter(chan->data, skb))\n-\t\tgoto drop;\n+\tif (chan->ops->filter) {\n+\t\tif (chan->ops->filter(chan, skb))\n+\t\t\tgoto drop;\n+\t}\n \n \tif (!control->sframe) {\n \t\tint err;",
        "function_modified_lines": {
            "added": [
                "\tif (chan->ops->filter) {",
                "\t\tif (chan->ops->filter(chan, skb))",
                "\t\t\tgoto drop;",
                "\t}"
            ],
            "deleted": [
                "\tif ((chan->mode == L2CAP_MODE_ERTM ||",
                "\t     chan->mode == L2CAP_MODE_STREAMING) && sk_filter(chan->data, skb))",
                "\t\tgoto drop;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Improper input validation in BlueZ may allow an unauthenticated user to potentially enable escalation of privilege via adjacent access.",
        "id": 2447
    },
    {
        "cve_id": "CVE-2016-2548",
        "code_before_change": "int snd_timer_close(struct snd_timer_instance *timeri)\n{\n\tstruct snd_timer *timer = NULL;\n\tstruct snd_timer_instance *slave, *tmp;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\t/* force to stop the timer */\n\tsnd_timer_stop(timeri);\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t}\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tmutex_unlock(&register_mutex);\n\t} else {\n\t\ttimer = timeri->timer;\n\t\tif (snd_BUG_ON(!timer))\n\t\t\tgoto out;\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&timer->lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&timer->lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&timer->lock);\n\t\t}\n\t\tspin_unlock_irq(&timer->lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tif (timer && list_empty(&timer->open_list_head) &&\n\t\t    timer->hw.close)\n\t\t\ttimer->hw.close(timer);\n\t\t/* remove slave links */\n\t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n\t\t\t\t\t open_list) {\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);\n\t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n\t\t\tslave->master = NULL;\n\t\t\tslave->timer = NULL;\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t}\n\t\tmutex_unlock(&register_mutex);\n\t}\n out:\n\tif (timeri->private_free)\n\t\ttimeri->private_free(timeri);\n\tkfree(timeri->owner);\n\tkfree(timeri);\n\tif (timer)\n\t\tmodule_put(timer->module);\n\treturn 0;\n}",
        "code_after_change": "int snd_timer_close(struct snd_timer_instance *timeri)\n{\n\tstruct snd_timer *timer = NULL;\n\tstruct snd_timer_instance *slave, *tmp;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\t/* force to stop the timer */\n\tsnd_timer_stop(timeri);\n\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE) {\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t}\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tmutex_unlock(&register_mutex);\n\t} else {\n\t\ttimer = timeri->timer;\n\t\tif (snd_BUG_ON(!timer))\n\t\t\tgoto out;\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&timer->lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&timer->lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&timer->lock);\n\t\t}\n\t\tspin_unlock_irq(&timer->lock);\n\t\tmutex_lock(&register_mutex);\n\t\tlist_del(&timeri->open_list);\n\t\tif (timer && list_empty(&timer->open_list_head) &&\n\t\t    timer->hw.close)\n\t\t\ttimer->hw.close(timer);\n\t\t/* remove slave links */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\tspin_lock(&timer->lock);\n\t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n\t\t\t\t\t open_list) {\n\t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n\t\t\tslave->master = NULL;\n\t\t\tslave->timer = NULL;\n\t\t\tlist_del_init(&slave->ack_list);\n\t\t\tlist_del_init(&slave->active_list);\n\t\t}\n\t\tspin_unlock(&timer->lock);\n\t\tspin_unlock_irq(&slave_active_lock);\n\t\tmutex_unlock(&register_mutex);\n\t}\n out:\n\tif (timeri->private_free)\n\t\ttimeri->private_free(timeri);\n\tkfree(timeri->owner);\n\tkfree(timeri);\n\tif (timer)\n\t\tmodule_put(timer->module);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,15 +39,18 @@\n \t\t    timer->hw.close)\n \t\t\ttimer->hw.close(timer);\n \t\t/* remove slave links */\n+\t\tspin_lock_irq(&slave_active_lock);\n+\t\tspin_lock(&timer->lock);\n \t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n \t\t\t\t\t open_list) {\n-\t\t\tspin_lock_irq(&slave_active_lock);\n-\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);\n \t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n \t\t\tslave->master = NULL;\n \t\t\tslave->timer = NULL;\n-\t\t\tspin_unlock_irq(&slave_active_lock);\n+\t\t\tlist_del_init(&slave->ack_list);\n+\t\t\tlist_del_init(&slave->active_list);\n \t\t}\n+\t\tspin_unlock(&timer->lock);\n+\t\tspin_unlock_irq(&slave_active_lock);\n \t\tmutex_unlock(&register_mutex);\n \t}\n  out:",
        "function_modified_lines": {
            "added": [
                "\t\tspin_lock_irq(&slave_active_lock);",
                "\t\tspin_lock(&timer->lock);",
                "\t\t\tlist_del_init(&slave->ack_list);",
                "\t\t\tlist_del_init(&slave->active_list);",
                "\t\tspin_unlock(&timer->lock);",
                "\t\tspin_unlock_irq(&slave_active_lock);"
            ],
            "deleted": [
                "\t\t\tspin_lock_irq(&slave_active_lock);",
                "\t\t\t_snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);",
                "\t\t\tspin_unlock_irq(&slave_active_lock);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "sound/core/timer.c in the Linux kernel before 4.4.1 retains certain linked lists after a close or stop action, which allows local users to cause a denial of service (system crash) via a crafted ioctl call, related to the (1) snd_timer_close and (2) _snd_timer_stop functions.",
        "id": 947
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "int verify_compat_iovec(struct msghdr *kern_msg, struct iovec *kern_iov,\n\t\t   struct sockaddr_storage *kern_address, int mode)\n{\n\tint tot_len;\n\n\tif (kern_msg->msg_namelen) {\n\t\tif (mode == VERIFY_READ) {\n\t\t\tint err = move_addr_to_kernel(kern_msg->msg_name,\n\t\t\t\t\t\t      kern_msg->msg_namelen,\n\t\t\t\t\t\t      kern_address);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t}\n\t\tkern_msg->msg_name = kern_address;\n\t} else\n\t\tkern_msg->msg_name = NULL;\n\n\ttot_len = iov_from_user_compat_to_kern(kern_iov,\n\t\t\t\t\t  (struct compat_iovec __user *)kern_msg->msg_iov,\n\t\t\t\t\t  kern_msg->msg_iovlen);\n\tif (tot_len >= 0)\n\t\tkern_msg->msg_iov = kern_iov;\n\n\treturn tot_len;\n}",
        "code_after_change": "int verify_compat_iovec(struct msghdr *kern_msg, struct iovec *kern_iov,\n\t\t   struct sockaddr_storage *kern_address, int mode)\n{\n\tint tot_len;\n\n\tif (kern_msg->msg_namelen) {\n\t\tif (mode == VERIFY_READ) {\n\t\t\tint err = move_addr_to_kernel(kern_msg->msg_name,\n\t\t\t\t\t\t      kern_msg->msg_namelen,\n\t\t\t\t\t\t      kern_address);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t}\n\t\tif (kern_msg->msg_name)\n\t\t\tkern_msg->msg_name = kern_address;\n\t} else\n\t\tkern_msg->msg_name = NULL;\n\n\ttot_len = iov_from_user_compat_to_kern(kern_iov,\n\t\t\t\t\t  (struct compat_iovec __user *)kern_msg->msg_iov,\n\t\t\t\t\t  kern_msg->msg_iovlen);\n\tif (tot_len >= 0)\n\t\tkern_msg->msg_iov = kern_iov;\n\n\treturn tot_len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,8 @@\n \t\t\tif (err < 0)\n \t\t\t\treturn err;\n \t\t}\n-\t\tkern_msg->msg_name = kern_address;\n+\t\tif (kern_msg->msg_name)\n+\t\t\tkern_msg->msg_name = kern_address;\n \t} else\n \t\tkern_msg->msg_name = NULL;\n ",
        "function_modified_lines": {
            "added": [
                "\t\tif (kern_msg->msg_name)",
                "\t\t\tkern_msg->msg_name = kern_address;"
            ],
            "deleted": [
                "\t\tkern_msg->msg_name = kern_address;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 380
    },
    {
        "cve_id": "CVE-2013-4129",
        "code_before_change": "static int __br_mdb_del(struct net_bridge *br, struct br_mdb_entry *entry)\n{\n\tstruct net_bridge_mdb_htable *mdb;\n\tstruct net_bridge_mdb_entry *mp;\n\tstruct net_bridge_port_group *p;\n\tstruct net_bridge_port_group __rcu **pp;\n\tstruct br_ip ip;\n\tint err = -EINVAL;\n\n\tif (!netif_running(br->dev) || br->multicast_disabled)\n\t\treturn -EINVAL;\n\n\tif (timer_pending(&br->multicast_querier_timer))\n\t\treturn -EBUSY;\n\n\tip.proto = entry->addr.proto;\n\tif (ip.proto == htons(ETH_P_IP))\n\t\tip.u.ip4 = entry->addr.u.ip4;\n#if IS_ENABLED(CONFIG_IPV6)\n\telse\n\t\tip.u.ip6 = entry->addr.u.ip6;\n#endif\n\n\tspin_lock_bh(&br->multicast_lock);\n\tmdb = mlock_dereference(br->mdb, br);\n\n\tmp = br_mdb_ip_get(mdb, &ip);\n\tif (!mp)\n\t\tgoto unlock;\n\n\tfor (pp = &mp->ports;\n\t     (p = mlock_dereference(*pp, br)) != NULL;\n\t     pp = &p->next) {\n\t\tif (!p->port || p->port->dev->ifindex != entry->ifindex)\n\t\t\tcontinue;\n\n\t\tif (p->port->state == BR_STATE_DISABLED)\n\t\t\tgoto unlock;\n\n\t\trcu_assign_pointer(*pp, p->next);\n\t\thlist_del_init(&p->mglist);\n\t\tdel_timer(&p->timer);\n\t\tcall_rcu_bh(&p->rcu, br_multicast_free_pg);\n\t\terr = 0;\n\n\t\tif (!mp->ports && !mp->mglist &&\n\t\t    netif_running(br->dev))\n\t\t\tmod_timer(&mp->timer, jiffies);\n\t\tbreak;\n\t}\n\nunlock:\n\tspin_unlock_bh(&br->multicast_lock);\n\treturn err;\n}",
        "code_after_change": "static int __br_mdb_del(struct net_bridge *br, struct br_mdb_entry *entry)\n{\n\tstruct net_bridge_mdb_htable *mdb;\n\tstruct net_bridge_mdb_entry *mp;\n\tstruct net_bridge_port_group *p;\n\tstruct net_bridge_port_group __rcu **pp;\n\tstruct br_ip ip;\n\tint err = -EINVAL;\n\n\tif (!netif_running(br->dev) || br->multicast_disabled)\n\t\treturn -EINVAL;\n\n\tif (timer_pending(&br->multicast_querier_timer))\n\t\treturn -EBUSY;\n\n\tip.proto = entry->addr.proto;\n\tif (ip.proto == htons(ETH_P_IP))\n\t\tip.u.ip4 = entry->addr.u.ip4;\n#if IS_ENABLED(CONFIG_IPV6)\n\telse\n\t\tip.u.ip6 = entry->addr.u.ip6;\n#endif\n\n\tspin_lock_bh(&br->multicast_lock);\n\tmdb = mlock_dereference(br->mdb, br);\n\n\tmp = br_mdb_ip_get(mdb, &ip);\n\tif (!mp)\n\t\tgoto unlock;\n\n\tfor (pp = &mp->ports;\n\t     (p = mlock_dereference(*pp, br)) != NULL;\n\t     pp = &p->next) {\n\t\tif (!p->port || p->port->dev->ifindex != entry->ifindex)\n\t\t\tcontinue;\n\n\t\tif (p->port->state == BR_STATE_DISABLED)\n\t\t\tgoto unlock;\n\n\t\trcu_assign_pointer(*pp, p->next);\n\t\thlist_del_init(&p->mglist);\n\t\tdel_timer(&p->timer);\n\t\tcall_rcu_bh(&p->rcu, br_multicast_free_pg);\n\t\terr = 0;\n\n\t\tif (!mp->ports && !mp->mglist && mp->timer_armed &&\n\t\t    netif_running(br->dev))\n\t\t\tmod_timer(&mp->timer, jiffies);\n\t\tbreak;\n\t}\n\nunlock:\n\tspin_unlock_bh(&br->multicast_lock);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,7 +43,7 @@\n \t\tcall_rcu_bh(&p->rcu, br_multicast_free_pg);\n \t\terr = 0;\n \n-\t\tif (!mp->ports && !mp->mglist &&\n+\t\tif (!mp->ports && !mp->mglist && mp->timer_armed &&\n \t\t    netif_running(br->dev))\n \t\t\tmod_timer(&mp->timer, jiffies);\n \t\tbreak;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!mp->ports && !mp->mglist && mp->timer_armed &&"
            ],
            "deleted": [
                "\t\tif (!mp->ports && !mp->mglist &&"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The bridge multicast implementation in the Linux kernel through 3.10.3 does not check whether a certain timer is armed before modifying the timeout value of that timer, which allows local users to cause a denial of service (BUG and system crash) via vectors involving the shutdown of a KVM virtual machine, related to net/bridge/br_mdb.c and net/bridge/br_multicast.c.",
        "id": 282
    },
    {
        "cve_id": "CVE-2018-14619",
        "code_before_change": "static void aead_sock_destruct(struct sock *sk)\n{\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct af_alg_ctx *ctx = ask->private;\n\tstruct sock *psk = ask->parent;\n\tstruct alg_sock *pask = alg_sk(psk);\n\tstruct aead_tfm *aeadc = pask->private;\n\tstruct crypto_aead *tfm = aeadc->aead;\n\tunsigned int ivlen = crypto_aead_ivsize(tfm);\n\n\taf_alg_pull_tsgl(sk, ctx->used, NULL, 0);\n\tcrypto_put_default_null_skcipher2();\n\tsock_kzfree_s(sk, ctx->iv, ivlen);\n\tsock_kfree_s(sk, ctx, ctx->len);\n\taf_alg_release_parent(sk);\n}",
        "code_after_change": "static void aead_sock_destruct(struct sock *sk)\n{\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct af_alg_ctx *ctx = ask->private;\n\tstruct sock *psk = ask->parent;\n\tstruct alg_sock *pask = alg_sk(psk);\n\tstruct aead_tfm *aeadc = pask->private;\n\tstruct crypto_aead *tfm = aeadc->aead;\n\tunsigned int ivlen = crypto_aead_ivsize(tfm);\n\n\taf_alg_pull_tsgl(sk, ctx->used, NULL, 0);\n\tsock_kzfree_s(sk, ctx->iv, ivlen);\n\tsock_kfree_s(sk, ctx, ctx->len);\n\taf_alg_release_parent(sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,6 @@\n \tunsigned int ivlen = crypto_aead_ivsize(tfm);\n \n \taf_alg_pull_tsgl(sk, ctx->used, NULL, 0);\n-\tcrypto_put_default_null_skcipher2();\n \tsock_kzfree_s(sk, ctx->iv, ivlen);\n \tsock_kfree_s(sk, ctx, ctx->len);\n \taf_alg_release_parent(sk);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tcrypto_put_default_null_skcipher2();"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "A flaw was found in the crypto subsystem of the Linux kernel before version kernel-4.15-rc4. The \"null skcipher\" was being dropped when each af_alg_ctx was freed instead of when the aead_tfm was freed. This can cause the null skcipher to be freed while it is still in use leading to a local user being able to crash the system or possibly escalate privileges.",
        "id": 1692
    },
    {
        "cve_id": "CVE-2018-20669",
        "code_before_change": "long compat_get_bitmap(unsigned long *mask, const compat_ulong_t __user *umask,\n\t\t       unsigned long bitmap_size)\n{\n\tunsigned long nr_compat_longs;\n\n\t/* align bitmap up to nearest compat_long_t boundary */\n\tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);\n\tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);\n\n\tif (!access_ok(umask, bitmap_size / 8))\n\t\treturn -EFAULT;\n\n\tuser_access_begin();\n\twhile (nr_compat_longs > 1) {\n\t\tcompat_ulong_t l1, l2;\n\t\tunsafe_get_user(l1, umask++, Efault);\n\t\tunsafe_get_user(l2, umask++, Efault);\n\t\t*mask++ = ((unsigned long)l2 << BITS_PER_COMPAT_LONG) | l1;\n\t\tnr_compat_longs -= 2;\n\t}\n\tif (nr_compat_longs)\n\t\tunsafe_get_user(*mask, umask++, Efault);\n\tuser_access_end();\n\treturn 0;\n\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "code_after_change": "long compat_get_bitmap(unsigned long *mask, const compat_ulong_t __user *umask,\n\t\t       unsigned long bitmap_size)\n{\n\tunsigned long nr_compat_longs;\n\n\t/* align bitmap up to nearest compat_long_t boundary */\n\tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);\n\tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);\n\n\tif (!user_access_begin(umask, bitmap_size / 8))\n\t\treturn -EFAULT;\n\n\twhile (nr_compat_longs > 1) {\n\t\tcompat_ulong_t l1, l2;\n\t\tunsafe_get_user(l1, umask++, Efault);\n\t\tunsafe_get_user(l2, umask++, Efault);\n\t\t*mask++ = ((unsigned long)l2 << BITS_PER_COMPAT_LONG) | l1;\n\t\tnr_compat_longs -= 2;\n\t}\n\tif (nr_compat_longs)\n\t\tunsafe_get_user(*mask, umask++, Efault);\n\tuser_access_end();\n\treturn 0;\n\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,10 +7,9 @@\n \tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);\n \tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);\n \n-\tif (!access_ok(umask, bitmap_size / 8))\n+\tif (!user_access_begin(umask, bitmap_size / 8))\n \t\treturn -EFAULT;\n \n-\tuser_access_begin();\n \twhile (nr_compat_longs > 1) {\n \t\tcompat_ulong_t l1, l2;\n \t\tunsafe_get_user(l1, umask++, Efault);",
        "function_modified_lines": {
            "added": [
                "\tif (!user_access_begin(umask, bitmap_size / 8))"
            ],
            "deleted": [
                "\tif (!access_ok(umask, bitmap_size / 8))",
                "\tuser_access_begin();"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "An issue where a provided address with access_ok() is not checked was discovered in i915_gem_execbuffer2_ioctl in drivers/gpu/drm/i915/i915_gem_execbuffer.c in the Linux kernel through 4.19.13. A local attacker can craft a malicious IOCTL function call to overwrite arbitrary kernel memory, resulting in a Denial of Service or privilege escalation.",
        "id": 1775
    },
    {
        "cve_id": "CVE-2013-4129",
        "code_before_change": "static void br_multicast_del_pg(struct net_bridge *br,\n\t\t\t\tstruct net_bridge_port_group *pg)\n{\n\tstruct net_bridge_mdb_htable *mdb;\n\tstruct net_bridge_mdb_entry *mp;\n\tstruct net_bridge_port_group *p;\n\tstruct net_bridge_port_group __rcu **pp;\n\n\tmdb = mlock_dereference(br->mdb, br);\n\n\tmp = br_mdb_ip_get(mdb, &pg->addr);\n\tif (WARN_ON(!mp))\n\t\treturn;\n\n\tfor (pp = &mp->ports;\n\t     (p = mlock_dereference(*pp, br)) != NULL;\n\t     pp = &p->next) {\n\t\tif (p != pg)\n\t\t\tcontinue;\n\n\t\trcu_assign_pointer(*pp, p->next);\n\t\thlist_del_init(&p->mglist);\n\t\tdel_timer(&p->timer);\n\t\tcall_rcu_bh(&p->rcu, br_multicast_free_pg);\n\n\t\tif (!mp->ports && !mp->mglist &&\n\t\t    netif_running(br->dev))\n\t\t\tmod_timer(&mp->timer, jiffies);\n\n\t\treturn;\n\t}\n\n\tWARN_ON(1);\n}",
        "code_after_change": "static void br_multicast_del_pg(struct net_bridge *br,\n\t\t\t\tstruct net_bridge_port_group *pg)\n{\n\tstruct net_bridge_mdb_htable *mdb;\n\tstruct net_bridge_mdb_entry *mp;\n\tstruct net_bridge_port_group *p;\n\tstruct net_bridge_port_group __rcu **pp;\n\n\tmdb = mlock_dereference(br->mdb, br);\n\n\tmp = br_mdb_ip_get(mdb, &pg->addr);\n\tif (WARN_ON(!mp))\n\t\treturn;\n\n\tfor (pp = &mp->ports;\n\t     (p = mlock_dereference(*pp, br)) != NULL;\n\t     pp = &p->next) {\n\t\tif (p != pg)\n\t\t\tcontinue;\n\n\t\trcu_assign_pointer(*pp, p->next);\n\t\thlist_del_init(&p->mglist);\n\t\tdel_timer(&p->timer);\n\t\tcall_rcu_bh(&p->rcu, br_multicast_free_pg);\n\n\t\tif (!mp->ports && !mp->mglist && mp->timer_armed &&\n\t\t    netif_running(br->dev))\n\t\t\tmod_timer(&mp->timer, jiffies);\n\n\t\treturn;\n\t}\n\n\tWARN_ON(1);\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,7 +23,7 @@\n \t\tdel_timer(&p->timer);\n \t\tcall_rcu_bh(&p->rcu, br_multicast_free_pg);\n \n-\t\tif (!mp->ports && !mp->mglist &&\n+\t\tif (!mp->ports && !mp->mglist && mp->timer_armed &&\n \t\t    netif_running(br->dev))\n \t\t\tmod_timer(&mp->timer, jiffies);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tif (!mp->ports && !mp->mglist && mp->timer_armed &&"
            ],
            "deleted": [
                "\t\tif (!mp->ports && !mp->mglist &&"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The bridge multicast implementation in the Linux kernel through 3.10.3 does not check whether a certain timer is armed before modifying the timeout value of that timer, which allows local users to cause a denial of service (BUG and system crash) via vectors involving the shutdown of a KVM virtual machine, related to net/bridge/br_mdb.c and net/bridge/br_multicast.c.",
        "id": 283
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "void exit_files(struct task_struct *tsk)\n{\n\tstruct files_struct * files = tsk->files;\n\n\tif (files) {\n\t\ttask_lock(tsk);\n\t\ttsk->files = NULL;\n\t\ttask_unlock(tsk);\n\t\tput_files_struct(files);\n\t}\n}",
        "code_after_change": "void exit_files(struct task_struct *tsk)\n{\n\tstruct files_struct * files = tsk->files;\n\n\tif (files) {\n\t\tio_uring_files_cancel(files);\n\t\ttask_lock(tsk);\n\t\ttsk->files = NULL;\n\t\ttask_unlock(tsk);\n\t\tput_files_struct(files);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n \tstruct files_struct * files = tsk->files;\n \n \tif (files) {\n+\t\tio_uring_files_cancel(files);\n \t\ttask_lock(tsk);\n \t\ttsk->files = NULL;\n \t\ttask_unlock(tsk);",
        "function_modified_lines": {
            "added": [
                "\t\tio_uring_files_cancel(files);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2843
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint __user *, addr_len)\n{\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tstruct msghdr msg;\n\tstruct sockaddr_storage address;\n\tint err, err2;\n\tint fput_needed;\n\n\tif (size > INT_MAX)\n\t\tsize = INT_MAX;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_iovlen = 1;\n\tmsg.msg_iov = &iov;\n\tiov.iov_len = size;\n\tiov.iov_base = ubuf;\n\tmsg.msg_name = (struct sockaddr *)&address;\n\tmsg.msg_namelen = sizeof(address);\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = sock_recvmsg(sock, &msg, size, flags);\n\n\tif (err >= 0 && addr != NULL) {\n\t\terr2 = move_addr_to_user(&address,\n\t\t\t\t\t msg.msg_namelen, addr, addr_len);\n\t\tif (err2 < 0)\n\t\t\terr = err2;\n\t}\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}",
        "code_after_change": "\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint __user *, addr_len)\n{\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tstruct msghdr msg;\n\tstruct sockaddr_storage address;\n\tint err, err2;\n\tint fput_needed;\n\n\tif (size > INT_MAX)\n\t\tsize = INT_MAX;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_iovlen = 1;\n\tmsg.msg_iov = &iov;\n\tiov.iov_len = size;\n\tiov.iov_base = ubuf;\n\t/* Save some cycles and don't copy the address if not needed */\n\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;\n\t/* We assume all kernel code knows the size of sockaddr_storage */\n\tmsg.msg_namelen = 0;\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = sock_recvmsg(sock, &msg, size, flags);\n\n\tif (err >= 0 && addr != NULL) {\n\t\terr2 = move_addr_to_user(&address,\n\t\t\t\t\t msg.msg_namelen, addr, addr_len);\n\t\tif (err2 < 0)\n\t\t\terr = err2;\n\t}\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,8 +22,10 @@\n \tmsg.msg_iov = &iov;\n \tiov.iov_len = size;\n \tiov.iov_base = ubuf;\n-\tmsg.msg_name = (struct sockaddr *)&address;\n-\tmsg.msg_namelen = sizeof(address);\n+\t/* Save some cycles and don't copy the address if not needed */\n+\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;\n+\t/* We assume all kernel code knows the size of sockaddr_storage */\n+\tmsg.msg_namelen = 0;\n \tif (sock->file->f_flags & O_NONBLOCK)\n \t\tflags |= MSG_DONTWAIT;\n \terr = sock_recvmsg(sock, &msg, size, flags);",
        "function_modified_lines": {
            "added": [
                "\t/* Save some cycles and don't copy the address if not needed */",
                "\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;",
                "\t/* We assume all kernel code knows the size of sockaddr_storage */",
                "\tmsg.msg_namelen = 0;"
            ],
            "deleted": [
                "\tmsg.msg_name = (struct sockaddr *)&address;",
                "\tmsg.msg_namelen = sizeof(address);"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 397
    },
    {
        "cve_id": "CVE-2013-2140",
        "code_before_change": "static int dispatch_discard_io(struct xen_blkif *blkif,\n\t\t\t\tstruct blkif_request *req)\n{\n\tint err = 0;\n\tint status = BLKIF_RSP_OKAY;\n\tstruct block_device *bdev = blkif->vbd.bdev;\n\tunsigned long secure;\n\n\tblkif->st_ds_req++;\n\n\txen_blkif_get(blkif);\n\tsecure = (blkif->vbd.discard_secure &&\n\t\t (req->u.discard.flag & BLKIF_DISCARD_SECURE)) ?\n\t\t BLKDEV_DISCARD_SECURE : 0;\n\n\terr = blkdev_issue_discard(bdev, req->u.discard.sector_number,\n\t\t\t\t   req->u.discard.nr_sectors,\n\t\t\t\t   GFP_KERNEL, secure);\n\n\tif (err == -EOPNOTSUPP) {\n\t\tpr_debug(DRV_PFX \"discard op failed, not supported\\n\");\n\t\tstatus = BLKIF_RSP_EOPNOTSUPP;\n\t} else if (err)\n\t\tstatus = BLKIF_RSP_ERROR;\n\n\tmake_response(blkif, req->u.discard.id, req->operation, status);\n\txen_blkif_put(blkif);\n\treturn err;\n}",
        "code_after_change": "static int dispatch_discard_io(struct xen_blkif *blkif,\n\t\t\t\tstruct blkif_request *req)\n{\n\tint err = 0;\n\tint status = BLKIF_RSP_OKAY;\n\tstruct block_device *bdev = blkif->vbd.bdev;\n\tunsigned long secure;\n\tstruct phys_req preq;\n\n\tpreq.sector_number = req->u.discard.sector_number;\n\tpreq.nr_sects      = req->u.discard.nr_sectors;\n\n\terr = xen_vbd_translate(&preq, blkif, WRITE);\n\tif (err) {\n\t\tpr_warn(DRV_PFX \"access denied: DISCARD [%llu->%llu] on dev=%04x\\n\",\n\t\t\tpreq.sector_number,\n\t\t\tpreq.sector_number + preq.nr_sects, blkif->vbd.pdevice);\n\t\tgoto fail_response;\n\t}\n\tblkif->st_ds_req++;\n\n\txen_blkif_get(blkif);\n\tsecure = (blkif->vbd.discard_secure &&\n\t\t (req->u.discard.flag & BLKIF_DISCARD_SECURE)) ?\n\t\t BLKDEV_DISCARD_SECURE : 0;\n\n\terr = blkdev_issue_discard(bdev, req->u.discard.sector_number,\n\t\t\t\t   req->u.discard.nr_sectors,\n\t\t\t\t   GFP_KERNEL, secure);\nfail_response:\n\tif (err == -EOPNOTSUPP) {\n\t\tpr_debug(DRV_PFX \"discard op failed, not supported\\n\");\n\t\tstatus = BLKIF_RSP_EOPNOTSUPP;\n\t} else if (err)\n\t\tstatus = BLKIF_RSP_ERROR;\n\n\tmake_response(blkif, req->u.discard.id, req->operation, status);\n\txen_blkif_put(blkif);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,18 @@\n \tint status = BLKIF_RSP_OKAY;\n \tstruct block_device *bdev = blkif->vbd.bdev;\n \tunsigned long secure;\n+\tstruct phys_req preq;\n \n+\tpreq.sector_number = req->u.discard.sector_number;\n+\tpreq.nr_sects      = req->u.discard.nr_sectors;\n+\n+\terr = xen_vbd_translate(&preq, blkif, WRITE);\n+\tif (err) {\n+\t\tpr_warn(DRV_PFX \"access denied: DISCARD [%llu->%llu] on dev=%04x\\n\",\n+\t\t\tpreq.sector_number,\n+\t\t\tpreq.sector_number + preq.nr_sects, blkif->vbd.pdevice);\n+\t\tgoto fail_response;\n+\t}\n \tblkif->st_ds_req++;\n \n \txen_blkif_get(blkif);\n@@ -16,7 +27,7 @@\n \terr = blkdev_issue_discard(bdev, req->u.discard.sector_number,\n \t\t\t\t   req->u.discard.nr_sectors,\n \t\t\t\t   GFP_KERNEL, secure);\n-\n+fail_response:\n \tif (err == -EOPNOTSUPP) {\n \t\tpr_debug(DRV_PFX \"discard op failed, not supported\\n\");\n \t\tstatus = BLKIF_RSP_EOPNOTSUPP;",
        "function_modified_lines": {
            "added": [
                "\tstruct phys_req preq;",
                "\tpreq.sector_number = req->u.discard.sector_number;",
                "\tpreq.nr_sects      = req->u.discard.nr_sectors;",
                "",
                "\terr = xen_vbd_translate(&preq, blkif, WRITE);",
                "\tif (err) {",
                "\t\tpr_warn(DRV_PFX \"access denied: DISCARD [%llu->%llu] on dev=%04x\\n\",",
                "\t\t\tpreq.sector_number,",
                "\t\t\tpreq.sector_number + preq.nr_sects, blkif->vbd.pdevice);",
                "\t\tgoto fail_response;",
                "\t}",
                "fail_response:"
            ],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The dispatch_discard_io function in drivers/block/xen-blkback/blkback.c in the Xen blkback implementation in the Linux kernel before 3.10.5 allows guest OS users to cause a denial of service (data loss) via filesystem write operations on a read-only disk that supports the (1) BLKIF_OP_DISCARD (aka discard or TRIM) or (2) SCSI UNMAP feature.",
        "id": 216
    },
    {
        "cve_id": "CVE-2019-11085",
        "code_before_change": "static int intel_vgpu_mmap(struct mdev_device *mdev, struct vm_area_struct *vma)\n{\n\tunsigned int index;\n\tu64 virtaddr;\n\tunsigned long req_size, pgoff = 0;\n\tpgprot_t pg_prot;\n\tstruct intel_vgpu *vgpu = mdev_get_drvdata(mdev);\n\n\tindex = vma->vm_pgoff >> (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT);\n\tif (index >= VFIO_PCI_ROM_REGION_INDEX)\n\t\treturn -EINVAL;\n\n\tif (vma->vm_end < vma->vm_start)\n\t\treturn -EINVAL;\n\tif ((vma->vm_flags & VM_SHARED) == 0)\n\t\treturn -EINVAL;\n\tif (index != VFIO_PCI_BAR2_REGION_INDEX)\n\t\treturn -EINVAL;\n\n\tpg_prot = vma->vm_page_prot;\n\tvirtaddr = vma->vm_start;\n\treq_size = vma->vm_end - vma->vm_start;\n\tpgoff = vgpu_aperture_pa_base(vgpu) >> PAGE_SHIFT;\n\n\treturn remap_pfn_range(vma, virtaddr, pgoff, req_size, pg_prot);\n}",
        "code_after_change": "static int intel_vgpu_mmap(struct mdev_device *mdev, struct vm_area_struct *vma)\n{\n\tunsigned int index;\n\tu64 virtaddr;\n\tunsigned long req_size, pgoff, req_start;\n\tpgprot_t pg_prot;\n\tstruct intel_vgpu *vgpu = mdev_get_drvdata(mdev);\n\n\tindex = vma->vm_pgoff >> (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT);\n\tif (index >= VFIO_PCI_ROM_REGION_INDEX)\n\t\treturn -EINVAL;\n\n\tif (vma->vm_end < vma->vm_start)\n\t\treturn -EINVAL;\n\tif ((vma->vm_flags & VM_SHARED) == 0)\n\t\treturn -EINVAL;\n\tif (index != VFIO_PCI_BAR2_REGION_INDEX)\n\t\treturn -EINVAL;\n\n\tpg_prot = vma->vm_page_prot;\n\tvirtaddr = vma->vm_start;\n\treq_size = vma->vm_end - vma->vm_start;\n\tpgoff = vma->vm_pgoff &\n\t\t((1U << (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT)) - 1);\n\treq_start = pgoff << PAGE_SHIFT;\n\n\tif (!intel_vgpu_in_aperture(vgpu, req_start))\n\t\treturn -EINVAL;\n\tif (req_start + req_size >\n\t    vgpu_aperture_offset(vgpu) + vgpu_aperture_sz(vgpu))\n\t\treturn -EINVAL;\n\n\tpgoff = (gvt_aperture_pa_base(vgpu->gvt) >> PAGE_SHIFT) + pgoff;\n\n\treturn remap_pfn_range(vma, virtaddr, pgoff, req_size, pg_prot);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tunsigned int index;\n \tu64 virtaddr;\n-\tunsigned long req_size, pgoff = 0;\n+\tunsigned long req_size, pgoff, req_start;\n \tpgprot_t pg_prot;\n \tstruct intel_vgpu *vgpu = mdev_get_drvdata(mdev);\n \n@@ -20,7 +20,17 @@\n \tpg_prot = vma->vm_page_prot;\n \tvirtaddr = vma->vm_start;\n \treq_size = vma->vm_end - vma->vm_start;\n-\tpgoff = vgpu_aperture_pa_base(vgpu) >> PAGE_SHIFT;\n+\tpgoff = vma->vm_pgoff &\n+\t\t((1U << (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT)) - 1);\n+\treq_start = pgoff << PAGE_SHIFT;\n+\n+\tif (!intel_vgpu_in_aperture(vgpu, req_start))\n+\t\treturn -EINVAL;\n+\tif (req_start + req_size >\n+\t    vgpu_aperture_offset(vgpu) + vgpu_aperture_sz(vgpu))\n+\t\treturn -EINVAL;\n+\n+\tpgoff = (gvt_aperture_pa_base(vgpu->gvt) >> PAGE_SHIFT) + pgoff;\n \n \treturn remap_pfn_range(vma, virtaddr, pgoff, req_size, pg_prot);\n }",
        "function_modified_lines": {
            "added": [
                "\tunsigned long req_size, pgoff, req_start;",
                "\tpgoff = vma->vm_pgoff &",
                "\t\t((1U << (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT)) - 1);",
                "\treq_start = pgoff << PAGE_SHIFT;",
                "",
                "\tif (!intel_vgpu_in_aperture(vgpu, req_start))",
                "\t\treturn -EINVAL;",
                "\tif (req_start + req_size >",
                "\t    vgpu_aperture_offset(vgpu) + vgpu_aperture_sz(vgpu))",
                "\t\treturn -EINVAL;",
                "",
                "\tpgoff = (gvt_aperture_pa_base(vgpu->gvt) >> PAGE_SHIFT) + pgoff;"
            ],
            "deleted": [
                "\tunsigned long req_size, pgoff = 0;",
                "\tpgoff = vgpu_aperture_pa_base(vgpu) >> PAGE_SHIFT;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "Insufficient input validation in Kernel Mode Driver in Intel(R) i915 Graphics for Linux before version 5.0 may allow an authenticated user to potentially enable escalation of privilege via local access.",
        "id": 1908
    },
    {
        "cve_id": "CVE-2021-20194",
        "code_before_change": "static int io_uring_get_fd(struct io_ring_ctx *ctx)\n{\n\tstruct file *file;\n\tint ret;\n\n#if defined(CONFIG_UNIX)\n\tret = sock_create_kern(&init_net, PF_UNIX, SOCK_RAW, IPPROTO_IP,\n\t\t\t\t&ctx->ring_sock);\n\tif (ret)\n\t\treturn ret;\n#endif\n\n\tret = get_unused_fd_flags(O_RDWR | O_CLOEXEC);\n\tif (ret < 0)\n\t\tgoto err;\n\n\tfile = anon_inode_getfile(\"[io_uring]\", &io_uring_fops, ctx,\n\t\t\t\t\tO_RDWR | O_CLOEXEC);\n\tif (IS_ERR(file)) {\n\t\tput_unused_fd(ret);\n\t\tret = PTR_ERR(file);\n\t\tgoto err;\n\t}\n\n#if defined(CONFIG_UNIX)\n\tctx->ring_sock->file = file;\n#endif\n\tfd_install(ret, file);\n\treturn ret;\nerr:\n#if defined(CONFIG_UNIX)\n\tsock_release(ctx->ring_sock);\n\tctx->ring_sock = NULL;\n#endif\n\treturn ret;\n}",
        "code_after_change": "static int io_uring_get_fd(struct io_ring_ctx *ctx)\n{\n\tstruct file *file;\n\tint ret;\n\n#if defined(CONFIG_UNIX)\n\tret = sock_create_kern(&init_net, PF_UNIX, SOCK_RAW, IPPROTO_IP,\n\t\t\t\t&ctx->ring_sock);\n\tif (ret)\n\t\treturn ret;\n#endif\n\n\tret = get_unused_fd_flags(O_RDWR | O_CLOEXEC);\n\tif (ret < 0)\n\t\tgoto err;\n\n\tfile = anon_inode_getfile(\"[io_uring]\", &io_uring_fops, ctx,\n\t\t\t\t\tO_RDWR | O_CLOEXEC);\n\tif (IS_ERR(file)) {\nerr_fd:\n\t\tput_unused_fd(ret);\n\t\tret = PTR_ERR(file);\n\t\tgoto err;\n\t}\n\n#if defined(CONFIG_UNIX)\n\tctx->ring_sock->file = file;\n#endif\n\tif (unlikely(io_uring_add_task_file(file))) {\n\t\tfile = ERR_PTR(-ENOMEM);\n\t\tgoto err_fd;\n\t}\n\tfd_install(ret, file);\n\treturn ret;\nerr:\n#if defined(CONFIG_UNIX)\n\tsock_release(ctx->ring_sock);\n\tctx->ring_sock = NULL;\n#endif\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \tfile = anon_inode_getfile(\"[io_uring]\", &io_uring_fops, ctx,\n \t\t\t\t\tO_RDWR | O_CLOEXEC);\n \tif (IS_ERR(file)) {\n+err_fd:\n \t\tput_unused_fd(ret);\n \t\tret = PTR_ERR(file);\n \t\tgoto err;\n@@ -25,6 +26,10 @@\n #if defined(CONFIG_UNIX)\n \tctx->ring_sock->file = file;\n #endif\n+\tif (unlikely(io_uring_add_task_file(file))) {\n+\t\tfile = ERR_PTR(-ENOMEM);\n+\t\tgoto err_fd;\n+\t}\n \tfd_install(ret, file);\n \treturn ret;\n err:",
        "function_modified_lines": {
            "added": [
                "err_fd:",
                "\tif (unlikely(io_uring_add_task_file(file))) {",
                "\t\tfile = ERR_PTR(-ENOMEM);",
                "\t\tgoto err_fd;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "There is a vulnerability in the linux kernel versions higher than 5.2 (if kernel compiled with config params CONFIG_BPF_SYSCALL=y , CONFIG_BPF=y , CONFIG_CGROUPS=y , CONFIG_CGROUP_BPF=y , CONFIG_HARDENED_USERCOPY not set, and BPF hook to getsockopt is registered). As result of BPF execution, the local user can trigger bug in __cgroup_bpf_run_filter_getsockopt() function that can lead to heap overflow (because of non-hardened usercopy). The impact of attack could be deny of service or possibly privileges escalation.",
        "id": 2852
    },
    {
        "cve_id": "CVE-2016-6162",
        "code_before_change": "int udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\n\tif (static_key_false(&udpv6_encap_needed) && up->encap_type) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = ACCESS_ONCE(up->encap_rcv);\n\t\tif (encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\t__UDP_INC_STATS(sock_net(sk),\n\t\t\t\t\t\tUDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\tis_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * UDP-Lite specific tests, ignored on UDP sockets (see net/ipv4/udp.c).\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_access_pointer(sk->sk_filter) &&\n\t    udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto drop;\n\n\tudp_csum_pull_header(skb);\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\t__UDP6_INC_STATS(sock_net(sk),\n\t\t\t\t UDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tgoto drop;\n\t}\n\n\tskb_dst_drop(skb);\n\n\tbh_lock_sock(sk);\n\trc = 0;\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udpv6_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ncsum_error:\n\t__UDP6_INC_STATS(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\t__UDP6_INC_STATS(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}",
        "code_after_change": "int udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\n\tif (static_key_false(&udpv6_encap_needed) && up->encap_type) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = ACCESS_ONCE(up->encap_rcv);\n\t\tif (encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\t__UDP_INC_STATS(sock_net(sk),\n\t\t\t\t\t\tUDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\tis_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * UDP-Lite specific tests, ignored on UDP sockets (see net/ipv4/udp.c).\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_access_pointer(sk->sk_filter) &&\n\t    udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto drop;\n\tif (unlikely(skb->len < sizeof(struct udphdr)))\n\t\tgoto drop;\n\n\tudp_csum_pull_header(skb);\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\t__UDP6_INC_STATS(sock_net(sk),\n\t\t\t\t UDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tgoto drop;\n\t}\n\n\tskb_dst_drop(skb);\n\n\tbh_lock_sock(sk);\n\trc = 0;\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udpv6_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ncsum_error:\n\t__UDP6_INC_STATS(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\t__UDP6_INC_STATS(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -65,6 +65,8 @@\n \n \tif (sk_filter(sk, skb))\n \t\tgoto drop;\n+\tif (unlikely(skb->len < sizeof(struct udphdr)))\n+\t\tgoto drop;\n \n \tudp_csum_pull_header(skb);\n \tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(skb->len < sizeof(struct udphdr)))",
                "\t\tgoto drop;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "net/core/skbuff.c in the Linux kernel 4.7-rc6 allows local users to cause a denial of service (panic) or possibly have unspecified other impact via certain IPv6 socket operations.",
        "id": 1063
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int caif_stream_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size,\n\t\t\t       int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint copied = 0;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\tmsg->msg_namelen = 0;\n\n\t/*\n\t * Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\terr = -EAGAIN;\n\tif (sk->sk_state == CAIF_CONNECTING)\n\t\tgoto out;\n\n\tcaif_read_lock(sk);\n\ttarget = sock_rcvlowat(sk, flags&MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, flags&MSG_DONTWAIT);\n\n\tdo {\n\t\tint chunk;\n\t\tstruct sk_buff *skb;\n\n\t\tlock_sock(sk);\n\t\tskb = skb_dequeue(&sk->sk_receive_queue);\n\t\tcaif_check_flow_release(sk);\n\n\t\tif (skb == NULL) {\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\terr = -ECONNRESET;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\terr = -EPIPE;\n\t\t\tif (sk->sk_state != CAIF_CONNECTED)\n\t\t\t\tgoto unlock;\n\t\t\tif (sock_flag(sk, SOCK_DEAD))\n\t\t\t\tgoto unlock;\n\n\t\t\trelease_sock(sk);\n\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\n\t\t\tcaif_read_unlock(sk);\n\n\t\t\ttimeo = caif_stream_data_wait(sk, timeo);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcaif_read_lock(sk);\n\t\t\tcontinue;\nunlock:\n\t\t\trelease_sock(sk);\n\t\t\tbreak;\n\t\t}\n\t\trelease_sock(sk);\n\t\tchunk = min_t(unsigned int, skb->len, size);\n\t\tif (memcpy_toiovec(msg->msg_iov, skb->data, chunk)) {\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tskb_pull(skb, chunk);\n\n\t\t\t/* put the skb back if we didn't use it up. */\n\t\t\tif (skb->len) {\n\t\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tkfree_skb(skb);\n\n\t\t} else {\n\t\t\t/*\n\t\t\t * It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\t/* put message back and return */\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\tcaif_read_unlock(sk);\n\nout:\n\treturn copied ? : err;\n}",
        "code_after_change": "static int caif_stream_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size,\n\t\t\t       int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint copied = 0;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\t/*\n\t * Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\terr = -EAGAIN;\n\tif (sk->sk_state == CAIF_CONNECTING)\n\t\tgoto out;\n\n\tcaif_read_lock(sk);\n\ttarget = sock_rcvlowat(sk, flags&MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, flags&MSG_DONTWAIT);\n\n\tdo {\n\t\tint chunk;\n\t\tstruct sk_buff *skb;\n\n\t\tlock_sock(sk);\n\t\tskb = skb_dequeue(&sk->sk_receive_queue);\n\t\tcaif_check_flow_release(sk);\n\n\t\tif (skb == NULL) {\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\terr = -ECONNRESET;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\terr = -EPIPE;\n\t\t\tif (sk->sk_state != CAIF_CONNECTED)\n\t\t\t\tgoto unlock;\n\t\t\tif (sock_flag(sk, SOCK_DEAD))\n\t\t\t\tgoto unlock;\n\n\t\t\trelease_sock(sk);\n\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\n\t\t\tcaif_read_unlock(sk);\n\n\t\t\ttimeo = caif_stream_data_wait(sk, timeo);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcaif_read_lock(sk);\n\t\t\tcontinue;\nunlock:\n\t\t\trelease_sock(sk);\n\t\t\tbreak;\n\t\t}\n\t\trelease_sock(sk);\n\t\tchunk = min_t(unsigned int, skb->len, size);\n\t\tif (memcpy_toiovec(msg->msg_iov, skb->data, chunk)) {\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tskb_pull(skb, chunk);\n\n\t\t\t/* put the skb back if we didn't use it up. */\n\t\t\tif (skb->len) {\n\t\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tkfree_skb(skb);\n\n\t\t} else {\n\t\t\t/*\n\t\t\t * It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\t/* put message back and return */\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\tcaif_read_unlock(sk);\n\nout:\n\treturn copied ? : err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,8 +11,6 @@\n \terr = -EOPNOTSUPP;\n \tif (flags&MSG_OOB)\n \t\tgoto out;\n-\n-\tmsg->msg_namelen = 0;\n \n \t/*\n \t * Lock the socket to prevent queue disordering",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 378
    },
    {
        "cve_id": "CVE-2013-7266",
        "code_before_change": "static int sco_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sco_pinfo *pi = sco_pi(sk);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == BT_CONNECT2 &&\n\t    test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {\n\t\tsco_conn_defer_accept(pi->conn->hcon, pi->setting);\n\t\tsk->sk_state = BT_CONFIG;\n\t\tmsg->msg_namelen = 0;\n\n\t\trelease_sock(sk);\n\t\treturn 0;\n\t}\n\n\trelease_sock(sk);\n\n\treturn bt_sock_recvmsg(iocb, sock, msg, len, flags);\n}",
        "code_after_change": "static int sco_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sco_pinfo *pi = sco_pi(sk);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == BT_CONNECT2 &&\n\t    test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {\n\t\tsco_conn_defer_accept(pi->conn->hcon, pi->setting);\n\t\tsk->sk_state = BT_CONFIG;\n\n\t\trelease_sock(sk);\n\t\treturn 0;\n\t}\n\n\trelease_sock(sk);\n\n\treturn bt_sock_recvmsg(iocb, sock, msg, len, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,6 @@\n \t    test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {\n \t\tsco_conn_defer_accept(pi->conn->hcon, pi->setting);\n \t\tsk->sk_state = BT_CONFIG;\n-\t\tmsg->msg_namelen = 0;\n \n \t\trelease_sock(sk);\n \t\treturn 0;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-20"
        ],
        "cve_description": "The mISDN_sock_recvmsg function in drivers/isdn/mISDN/socket.c in the Linux kernel before 3.12.4 does not ensure that a certain length value is consistent with the size of an associated data structure, which allows local users to obtain sensitive information from kernel memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 377
    }
]