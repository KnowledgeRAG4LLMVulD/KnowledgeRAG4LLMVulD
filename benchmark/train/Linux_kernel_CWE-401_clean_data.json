[
    {
        "cve_id": "CVE-2019-19053",
        "code_before_change": "static ssize_t rpmsg_eptdev_write_iter(struct kiocb *iocb,\n\t\t\t\t       struct iov_iter *from)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tstruct rpmsg_eptdev *eptdev = filp->private_data;\n\tsize_t len = iov_iter_count(from);\n\tvoid *kbuf;\n\tint ret;\n\n\tkbuf = kzalloc(len, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\n\tif (!copy_from_iter_full(kbuf, len, from))\n\t\treturn -EFAULT;\n\n\tif (mutex_lock_interruptible(&eptdev->ept_lock)) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto free_kbuf;\n\t}\n\n\tif (!eptdev->ept) {\n\t\tret = -EPIPE;\n\t\tgoto unlock_eptdev;\n\t}\n\n\tif (filp->f_flags & O_NONBLOCK)\n\t\tret = rpmsg_trysend(eptdev->ept, kbuf, len);\n\telse\n\t\tret = rpmsg_send(eptdev->ept, kbuf, len);\n\nunlock_eptdev:\n\tmutex_unlock(&eptdev->ept_lock);\n\nfree_kbuf:\n\tkfree(kbuf);\n\treturn ret < 0 ? ret : len;\n}",
        "code_after_change": "static ssize_t rpmsg_eptdev_write_iter(struct kiocb *iocb,\n\t\t\t\t       struct iov_iter *from)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tstruct rpmsg_eptdev *eptdev = filp->private_data;\n\tsize_t len = iov_iter_count(from);\n\tvoid *kbuf;\n\tint ret;\n\n\tkbuf = kzalloc(len, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\n\tif (!copy_from_iter_full(kbuf, len, from)) {\n\t\tret = -EFAULT;\n\t\tgoto free_kbuf;\n\t}\n\n\tif (mutex_lock_interruptible(&eptdev->ept_lock)) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto free_kbuf;\n\t}\n\n\tif (!eptdev->ept) {\n\t\tret = -EPIPE;\n\t\tgoto unlock_eptdev;\n\t}\n\n\tif (filp->f_flags & O_NONBLOCK)\n\t\tret = rpmsg_trysend(eptdev->ept, kbuf, len);\n\telse\n\t\tret = rpmsg_send(eptdev->ept, kbuf, len);\n\nunlock_eptdev:\n\tmutex_unlock(&eptdev->ept_lock);\n\nfree_kbuf:\n\tkfree(kbuf);\n\treturn ret < 0 ? ret : len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,8 +11,10 @@\n \tif (!kbuf)\n \t\treturn -ENOMEM;\n \n-\tif (!copy_from_iter_full(kbuf, len, from))\n-\t\treturn -EFAULT;\n+\tif (!copy_from_iter_full(kbuf, len, from)) {\n+\t\tret = -EFAULT;\n+\t\tgoto free_kbuf;\n+\t}\n \n \tif (mutex_lock_interruptible(&eptdev->ept_lock)) {\n \t\tret = -ERESTARTSYS;",
        "function_modified_lines": {
            "added": [
                "\tif (!copy_from_iter_full(kbuf, len, from)) {",
                "\t\tret = -EFAULT;",
                "\t\tgoto free_kbuf;",
                "\t}"
            ],
            "deleted": [
                "\tif (!copy_from_iter_full(kbuf, len, from))",
                "\t\treturn -EFAULT;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the rpmsg_eptdev_write_iter() function in drivers/rpmsg/rpmsg_char.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering copy_from_iter_full() failures, aka CID-bbe692e349e2.",
        "id": 2134
    },
    {
        "cve_id": "CVE-2023-32247",
        "code_before_change": "static struct ksmbd_session *__session_lookup(unsigned long long id)\n{\n\tstruct ksmbd_session *sess;\n\n\thash_for_each_possible(sessions_table, sess, hlist, id) {\n\t\tif (id == sess->id)\n\t\t\treturn sess;\n\t}\n\treturn NULL;\n}",
        "code_after_change": "static struct ksmbd_session *__session_lookup(unsigned long long id)\n{\n\tstruct ksmbd_session *sess;\n\n\thash_for_each_possible(sessions_table, sess, hlist, id) {\n\t\tif (id == sess->id) {\n\t\t\tsess->last_active = jiffies;\n\t\t\treturn sess;\n\t\t}\n\t}\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,8 +3,10 @@\n \tstruct ksmbd_session *sess;\n \n \thash_for_each_possible(sessions_table, sess, hlist, id) {\n-\t\tif (id == sess->id)\n+\t\tif (id == sess->id) {\n+\t\t\tsess->last_active = jiffies;\n \t\t\treturn sess;\n+\t\t}\n \t}\n \treturn NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tif (id == sess->id) {",
                "\t\t\tsess->last_active = jiffies;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tif (id == sess->id)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the handling of SMB2_SESSION_SETUP commands. The issue results from the lack of control of resource consumption. An attacker can leverage this vulnerability to create a denial-of-service condition on the system.",
        "id": 4012
    },
    {
        "cve_id": "CVE-2022-4139",
        "code_before_change": "static void mmio_invalidate_full(struct intel_gt *gt)\n{\n\tstatic const i915_reg_t gen8_regs[] = {\n\t\t[RENDER_CLASS]\t\t\t= GEN8_RTCR,\n\t\t[VIDEO_DECODE_CLASS]\t\t= GEN8_M1TCR, /* , GEN8_M2TCR */\n\t\t[VIDEO_ENHANCEMENT_CLASS]\t= GEN8_VTCR,\n\t\t[COPY_ENGINE_CLASS]\t\t= GEN8_BTCR,\n\t};\n\tstatic const i915_reg_t gen12_regs[] = {\n\t\t[RENDER_CLASS]\t\t\t= GEN12_GFX_TLB_INV_CR,\n\t\t[VIDEO_DECODE_CLASS]\t\t= GEN12_VD_TLB_INV_CR,\n\t\t[VIDEO_ENHANCEMENT_CLASS]\t= GEN12_VE_TLB_INV_CR,\n\t\t[COPY_ENGINE_CLASS]\t\t= GEN12_BLT_TLB_INV_CR,\n\t\t[COMPUTE_CLASS]\t\t\t= GEN12_COMPCTX_TLB_INV_CR,\n\t};\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct intel_uncore *uncore = gt->uncore;\n\tstruct intel_engine_cs *engine;\n\tintel_engine_mask_t awake, tmp;\n\tenum intel_engine_id id;\n\tconst i915_reg_t *regs;\n\tunsigned int num = 0;\n\n\tif (GRAPHICS_VER(i915) == 12) {\n\t\tregs = gen12_regs;\n\t\tnum = ARRAY_SIZE(gen12_regs);\n\t} else if (GRAPHICS_VER(i915) >= 8 && GRAPHICS_VER(i915) <= 11) {\n\t\tregs = gen8_regs;\n\t\tnum = ARRAY_SIZE(gen8_regs);\n\t} else if (GRAPHICS_VER(i915) < 8) {\n\t\treturn;\n\t}\n\n\tif (drm_WARN_ONCE(&i915->drm, !num,\n\t\t\t  \"Platform does not implement TLB invalidation!\"))\n\t\treturn;\n\n\tintel_uncore_forcewake_get(uncore, FORCEWAKE_ALL);\n\n\tspin_lock_irq(&uncore->lock); /* serialise invalidate with GT reset */\n\n\tawake = 0;\n\tfor_each_engine(engine, gt, id) {\n\t\tstruct reg_and_bit rb;\n\n\t\tif (!intel_engine_pm_is_awake(engine))\n\t\t\tcontinue;\n\n\t\trb = get_reg_and_bit(engine, regs == gen8_regs, regs, num);\n\t\tif (!i915_mmio_reg_offset(rb.reg))\n\t\t\tcontinue;\n\n\t\tintel_uncore_write_fw(uncore, rb.reg, rb.bit);\n\t\tawake |= engine->mask;\n\t}\n\n\tGT_TRACE(gt, \"invalidated engines %08x\\n\", awake);\n\n\t/* Wa_2207587034:tgl,dg1,rkl,adl-s,adl-p */\n\tif (awake &&\n\t    (IS_TIGERLAKE(i915) ||\n\t     IS_DG1(i915) ||\n\t     IS_ROCKETLAKE(i915) ||\n\t     IS_ALDERLAKE_S(i915) ||\n\t     IS_ALDERLAKE_P(i915)))\n\t\tintel_uncore_write_fw(uncore, GEN12_OA_TLB_INV_CR, 1);\n\n\tspin_unlock_irq(&uncore->lock);\n\n\tfor_each_engine_masked(engine, gt, awake, tmp) {\n\t\tstruct reg_and_bit rb;\n\n\t\t/*\n\t\t * HW architecture suggest typical invalidation time at 40us,\n\t\t * with pessimistic cases up to 100us and a recommendation to\n\t\t * cap at 1ms. We go a bit higher just in case.\n\t\t */\n\t\tconst unsigned int timeout_us = 100;\n\t\tconst unsigned int timeout_ms = 4;\n\n\t\trb = get_reg_and_bit(engine, regs == gen8_regs, regs, num);\n\t\tif (__intel_wait_for_register_fw(uncore,\n\t\t\t\t\t\t rb.reg, rb.bit, 0,\n\t\t\t\t\t\t timeout_us, timeout_ms,\n\t\t\t\t\t\t NULL))\n\t\t\tdrm_err_ratelimited(&gt->i915->drm,\n\t\t\t\t\t    \"%s TLB invalidation did not complete in %ums!\\n\",\n\t\t\t\t\t    engine->name, timeout_ms);\n\t}\n\n\t/*\n\t * Use delayed put since a) we mostly expect a flurry of TLB\n\t * invalidations so it is good to avoid paying the forcewake cost and\n\t * b) it works around a bug in Icelake which cannot cope with too rapid\n\t * transitions.\n\t */\n\tintel_uncore_forcewake_put_delayed(uncore, FORCEWAKE_ALL);\n}",
        "code_after_change": "static void mmio_invalidate_full(struct intel_gt *gt)\n{\n\tstatic const i915_reg_t gen8_regs[] = {\n\t\t[RENDER_CLASS]\t\t\t= GEN8_RTCR,\n\t\t[VIDEO_DECODE_CLASS]\t\t= GEN8_M1TCR, /* , GEN8_M2TCR */\n\t\t[VIDEO_ENHANCEMENT_CLASS]\t= GEN8_VTCR,\n\t\t[COPY_ENGINE_CLASS]\t\t= GEN8_BTCR,\n\t};\n\tstatic const i915_reg_t gen12_regs[] = {\n\t\t[RENDER_CLASS]\t\t\t= GEN12_GFX_TLB_INV_CR,\n\t\t[VIDEO_DECODE_CLASS]\t\t= GEN12_VD_TLB_INV_CR,\n\t\t[VIDEO_ENHANCEMENT_CLASS]\t= GEN12_VE_TLB_INV_CR,\n\t\t[COPY_ENGINE_CLASS]\t\t= GEN12_BLT_TLB_INV_CR,\n\t\t[COMPUTE_CLASS]\t\t\t= GEN12_COMPCTX_TLB_INV_CR,\n\t};\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct intel_uncore *uncore = gt->uncore;\n\tstruct intel_engine_cs *engine;\n\tintel_engine_mask_t awake, tmp;\n\tenum intel_engine_id id;\n\tconst i915_reg_t *regs;\n\tunsigned int num = 0;\n\n\tif (GRAPHICS_VER(i915) == 12) {\n\t\tregs = gen12_regs;\n\t\tnum = ARRAY_SIZE(gen12_regs);\n\t} else if (GRAPHICS_VER(i915) >= 8 && GRAPHICS_VER(i915) <= 11) {\n\t\tregs = gen8_regs;\n\t\tnum = ARRAY_SIZE(gen8_regs);\n\t} else if (GRAPHICS_VER(i915) < 8) {\n\t\treturn;\n\t}\n\n\tif (drm_WARN_ONCE(&i915->drm, !num,\n\t\t\t  \"Platform does not implement TLB invalidation!\"))\n\t\treturn;\n\n\tintel_uncore_forcewake_get(uncore, FORCEWAKE_ALL);\n\n\tspin_lock_irq(&uncore->lock); /* serialise invalidate with GT reset */\n\n\tawake = 0;\n\tfor_each_engine(engine, gt, id) {\n\t\tstruct reg_and_bit rb;\n\n\t\tif (!intel_engine_pm_is_awake(engine))\n\t\t\tcontinue;\n\n\t\trb = get_reg_and_bit(engine, regs == gen8_regs, regs, num);\n\t\tif (!i915_mmio_reg_offset(rb.reg))\n\t\t\tcontinue;\n\n\t\tif (GRAPHICS_VER(i915) == 12 && (engine->class == VIDEO_DECODE_CLASS ||\n\t\t    engine->class == VIDEO_ENHANCEMENT_CLASS ||\n\t\t    engine->class == COMPUTE_CLASS))\n\t\t\trb.bit = _MASKED_BIT_ENABLE(rb.bit);\n\n\t\tintel_uncore_write_fw(uncore, rb.reg, rb.bit);\n\t\tawake |= engine->mask;\n\t}\n\n\tGT_TRACE(gt, \"invalidated engines %08x\\n\", awake);\n\n\t/* Wa_2207587034:tgl,dg1,rkl,adl-s,adl-p */\n\tif (awake &&\n\t    (IS_TIGERLAKE(i915) ||\n\t     IS_DG1(i915) ||\n\t     IS_ROCKETLAKE(i915) ||\n\t     IS_ALDERLAKE_S(i915) ||\n\t     IS_ALDERLAKE_P(i915)))\n\t\tintel_uncore_write_fw(uncore, GEN12_OA_TLB_INV_CR, 1);\n\n\tspin_unlock_irq(&uncore->lock);\n\n\tfor_each_engine_masked(engine, gt, awake, tmp) {\n\t\tstruct reg_and_bit rb;\n\n\t\t/*\n\t\t * HW architecture suggest typical invalidation time at 40us,\n\t\t * with pessimistic cases up to 100us and a recommendation to\n\t\t * cap at 1ms. We go a bit higher just in case.\n\t\t */\n\t\tconst unsigned int timeout_us = 100;\n\t\tconst unsigned int timeout_ms = 4;\n\n\t\trb = get_reg_and_bit(engine, regs == gen8_regs, regs, num);\n\t\tif (__intel_wait_for_register_fw(uncore,\n\t\t\t\t\t\t rb.reg, rb.bit, 0,\n\t\t\t\t\t\t timeout_us, timeout_ms,\n\t\t\t\t\t\t NULL))\n\t\t\tdrm_err_ratelimited(&gt->i915->drm,\n\t\t\t\t\t    \"%s TLB invalidation did not complete in %ums!\\n\",\n\t\t\t\t\t    engine->name, timeout_ms);\n\t}\n\n\t/*\n\t * Use delayed put since a) we mostly expect a flurry of TLB\n\t * invalidations so it is good to avoid paying the forcewake cost and\n\t * b) it works around a bug in Icelake which cannot cope with too rapid\n\t * transitions.\n\t */\n\tintel_uncore_forcewake_put_delayed(uncore, FORCEWAKE_ALL);\n}",
        "patch": "--- code before\n+++ code after\n@@ -50,6 +50,11 @@\n \t\tif (!i915_mmio_reg_offset(rb.reg))\n \t\t\tcontinue;\n \n+\t\tif (GRAPHICS_VER(i915) == 12 && (engine->class == VIDEO_DECODE_CLASS ||\n+\t\t    engine->class == VIDEO_ENHANCEMENT_CLASS ||\n+\t\t    engine->class == COMPUTE_CLASS))\n+\t\t\trb.bit = _MASKED_BIT_ENABLE(rb.bit);\n+\n \t\tintel_uncore_write_fw(uncore, rb.reg, rb.bit);\n \t\tawake |= engine->mask;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (GRAPHICS_VER(i915) == 12 && (engine->class == VIDEO_DECODE_CLASS ||",
                "\t\t    engine->class == VIDEO_ENHANCEMENT_CLASS ||",
                "\t\t    engine->class == COMPUTE_CLASS))",
                "\t\t\trb.bit = _MASKED_BIT_ENABLE(rb.bit);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An incorrect TLB flush issue was found in the Linux kernel\u2019s GPU i915 kernel driver, potentially leading to random memory corruption or data leaks. This flaw could allow a local user to crash the system or escalate their privileges on the system.",
        "id": 3718
    },
    {
        "cve_id": "CVE-2022-0854",
        "code_before_change": "phys_addr_t swiotlb_tbl_map_single(struct device *dev, phys_addr_t orig_addr,\n\t\tsize_t mapping_size, size_t alloc_size,\n\t\tunsigned int alloc_align_mask, enum dma_data_direction dir,\n\t\tunsigned long attrs)\n{\n\tstruct io_tlb_mem *mem = dev->dma_io_tlb_mem;\n\tunsigned int offset = swiotlb_align_offset(dev, orig_addr);\n\tunsigned int i;\n\tint index;\n\tphys_addr_t tlb_addr;\n\n\tif (!mem)\n\t\tpanic(\"Can not allocate SWIOTLB buffer earlier and can't now provide you with the DMA bounce buffer\");\n\n\tif (cc_platform_has(CC_ATTR_MEM_ENCRYPT))\n\t\tpr_warn_once(\"Memory encryption is active and system is using DMA bounce buffers\\n\");\n\n\tif (mapping_size > alloc_size) {\n\t\tdev_warn_once(dev, \"Invalid sizes (mapping: %zd bytes, alloc: %zd bytes)\",\n\t\t\t      mapping_size, alloc_size);\n\t\treturn (phys_addr_t)DMA_MAPPING_ERROR;\n\t}\n\n\tindex = swiotlb_find_slots(dev, orig_addr,\n\t\t\t\t   alloc_size + offset, alloc_align_mask);\n\tif (index == -1) {\n\t\tif (!(attrs & DMA_ATTR_NO_WARN))\n\t\t\tdev_warn_ratelimited(dev,\n\t\"swiotlb buffer is full (sz: %zd bytes), total %lu (slots), used %lu (slots)\\n\",\n\t\t\t\t alloc_size, mem->nslabs, mem->used);\n\t\treturn (phys_addr_t)DMA_MAPPING_ERROR;\n\t}\n\n\t/*\n\t * Save away the mapping from the original address to the DMA address.\n\t * This is needed when we sync the memory.  Then we sync the buffer if\n\t * needed.\n\t */\n\tfor (i = 0; i < nr_slots(alloc_size + offset); i++)\n\t\tmem->slots[index + i].orig_addr = slot_addr(orig_addr, i);\n\ttlb_addr = slot_addr(mem->start, index) + offset;\n\tif (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&\n\t    (!(attrs & DMA_ATTR_OVERWRITE) || dir == DMA_TO_DEVICE ||\n\t    dir == DMA_BIDIRECTIONAL))\n\t\tswiotlb_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);\n\treturn tlb_addr;\n}",
        "code_after_change": "phys_addr_t swiotlb_tbl_map_single(struct device *dev, phys_addr_t orig_addr,\n\t\tsize_t mapping_size, size_t alloc_size,\n\t\tunsigned int alloc_align_mask, enum dma_data_direction dir,\n\t\tunsigned long attrs)\n{\n\tstruct io_tlb_mem *mem = dev->dma_io_tlb_mem;\n\tunsigned int offset = swiotlb_align_offset(dev, orig_addr);\n\tunsigned int i;\n\tint index;\n\tphys_addr_t tlb_addr;\n\n\tif (!mem)\n\t\tpanic(\"Can not allocate SWIOTLB buffer earlier and can't now provide you with the DMA bounce buffer\");\n\n\tif (cc_platform_has(CC_ATTR_MEM_ENCRYPT))\n\t\tpr_warn_once(\"Memory encryption is active and system is using DMA bounce buffers\\n\");\n\n\tif (mapping_size > alloc_size) {\n\t\tdev_warn_once(dev, \"Invalid sizes (mapping: %zd bytes, alloc: %zd bytes)\",\n\t\t\t      mapping_size, alloc_size);\n\t\treturn (phys_addr_t)DMA_MAPPING_ERROR;\n\t}\n\n\tindex = swiotlb_find_slots(dev, orig_addr,\n\t\t\t\t   alloc_size + offset, alloc_align_mask);\n\tif (index == -1) {\n\t\tif (!(attrs & DMA_ATTR_NO_WARN))\n\t\t\tdev_warn_ratelimited(dev,\n\t\"swiotlb buffer is full (sz: %zd bytes), total %lu (slots), used %lu (slots)\\n\",\n\t\t\t\t alloc_size, mem->nslabs, mem->used);\n\t\treturn (phys_addr_t)DMA_MAPPING_ERROR;\n\t}\n\n\t/*\n\t * Save away the mapping from the original address to the DMA address.\n\t * This is needed when we sync the memory.  Then we sync the buffer if\n\t * needed.\n\t */\n\tfor (i = 0; i < nr_slots(alloc_size + offset); i++)\n\t\tmem->slots[index + i].orig_addr = slot_addr(orig_addr, i);\n\ttlb_addr = slot_addr(mem->start, index) + offset;\n\t/*\n\t * When dir == DMA_FROM_DEVICE we could omit the copy from the orig\n\t * to the tlb buffer, if we knew for sure the device will\n\t * overwirte the entire current content. But we don't. Thus\n\t * unconditional bounce may prevent leaking swiotlb content (i.e.\n\t * kernel memory) to user-space.\n\t */\n\tswiotlb_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);\n\treturn tlb_addr;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,9 +39,13 @@\n \tfor (i = 0; i < nr_slots(alloc_size + offset); i++)\n \t\tmem->slots[index + i].orig_addr = slot_addr(orig_addr, i);\n \ttlb_addr = slot_addr(mem->start, index) + offset;\n-\tif (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&\n-\t    (!(attrs & DMA_ATTR_OVERWRITE) || dir == DMA_TO_DEVICE ||\n-\t    dir == DMA_BIDIRECTIONAL))\n-\t\tswiotlb_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);\n+\t/*\n+\t * When dir == DMA_FROM_DEVICE we could omit the copy from the orig\n+\t * to the tlb buffer, if we knew for sure the device will\n+\t * overwirte the entire current content. But we don't. Thus\n+\t * unconditional bounce may prevent leaking swiotlb content (i.e.\n+\t * kernel memory) to user-space.\n+\t */\n+\tswiotlb_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);\n \treturn tlb_addr;\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * When dir == DMA_FROM_DEVICE we could omit the copy from the orig",
                "\t * to the tlb buffer, if we knew for sure the device will",
                "\t * overwirte the entire current content. But we don't. Thus",
                "\t * unconditional bounce may prevent leaking swiotlb content (i.e.",
                "\t * kernel memory) to user-space.",
                "\t */",
                "\tswiotlb_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);"
            ],
            "deleted": [
                "\tif (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&",
                "\t    (!(attrs & DMA_ATTR_OVERWRITE) || dir == DMA_TO_DEVICE ||",
                "\t    dir == DMA_BIDIRECTIONAL))",
                "\t\tswiotlb_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak flaw was found in the Linux kernel\u2019s DMA subsystem, in the way a user calls DMA_FROM_DEVICE. This flaw allows a local user to read random memory from the kernel space.",
        "id": 3226
    },
    {
        "cve_id": "CVE-2019-19078",
        "code_before_change": "static int ath10k_usb_hif_tx_sg(struct ath10k *ar, u8 pipe_id,\n\t\t\t\tstruct ath10k_hif_sg_item *items, int n_items)\n{\n\tstruct ath10k_usb *ar_usb = ath10k_usb_priv(ar);\n\tstruct ath10k_usb_pipe *pipe = &ar_usb->pipes[pipe_id];\n\tstruct ath10k_urb_context *urb_context;\n\tstruct sk_buff *skb;\n\tstruct urb *urb;\n\tint ret, i;\n\n\tfor (i = 0; i < n_items; i++) {\n\t\turb_context = ath10k_usb_alloc_urb_from_pipe(pipe);\n\t\tif (!urb_context) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tskb = items[i].transfer_context;\n\t\turb_context->skb = skb;\n\n\t\turb = usb_alloc_urb(0, GFP_ATOMIC);\n\t\tif (!urb) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free_urb_to_pipe;\n\t\t}\n\n\t\tusb_fill_bulk_urb(urb,\n\t\t\t\t  ar_usb->udev,\n\t\t\t\t  pipe->usb_pipe_handle,\n\t\t\t\t  skb->data,\n\t\t\t\t  skb->len,\n\t\t\t\t  ath10k_usb_transmit_complete, urb_context);\n\n\t\tif (!(skb->len % pipe->max_packet_size)) {\n\t\t\t/* hit a max packet boundary on this pipe */\n\t\t\turb->transfer_flags |= URB_ZERO_PACKET;\n\t\t}\n\n\t\tusb_anchor_urb(urb, &pipe->urb_submitted);\n\t\tret = usb_submit_urb(urb, GFP_ATOMIC);\n\t\tif (ret) {\n\t\t\tath10k_dbg(ar, ATH10K_DBG_USB_BULK,\n\t\t\t\t   \"usb bulk transmit failed: %d\\n\", ret);\n\t\t\tusb_unanchor_urb(urb);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free_urb_to_pipe;\n\t\t}\n\n\t\tusb_free_urb(urb);\n\t}\n\n\treturn 0;\n\nerr_free_urb_to_pipe:\n\tath10k_usb_free_urb_to_pipe(urb_context->pipe, urb_context);\nerr:\n\treturn ret;\n}",
        "code_after_change": "static int ath10k_usb_hif_tx_sg(struct ath10k *ar, u8 pipe_id,\n\t\t\t\tstruct ath10k_hif_sg_item *items, int n_items)\n{\n\tstruct ath10k_usb *ar_usb = ath10k_usb_priv(ar);\n\tstruct ath10k_usb_pipe *pipe = &ar_usb->pipes[pipe_id];\n\tstruct ath10k_urb_context *urb_context;\n\tstruct sk_buff *skb;\n\tstruct urb *urb;\n\tint ret, i;\n\n\tfor (i = 0; i < n_items; i++) {\n\t\turb_context = ath10k_usb_alloc_urb_from_pipe(pipe);\n\t\tif (!urb_context) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tskb = items[i].transfer_context;\n\t\turb_context->skb = skb;\n\n\t\turb = usb_alloc_urb(0, GFP_ATOMIC);\n\t\tif (!urb) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free_urb_to_pipe;\n\t\t}\n\n\t\tusb_fill_bulk_urb(urb,\n\t\t\t\t  ar_usb->udev,\n\t\t\t\t  pipe->usb_pipe_handle,\n\t\t\t\t  skb->data,\n\t\t\t\t  skb->len,\n\t\t\t\t  ath10k_usb_transmit_complete, urb_context);\n\n\t\tif (!(skb->len % pipe->max_packet_size)) {\n\t\t\t/* hit a max packet boundary on this pipe */\n\t\t\turb->transfer_flags |= URB_ZERO_PACKET;\n\t\t}\n\n\t\tusb_anchor_urb(urb, &pipe->urb_submitted);\n\t\tret = usb_submit_urb(urb, GFP_ATOMIC);\n\t\tif (ret) {\n\t\t\tath10k_dbg(ar, ATH10K_DBG_USB_BULK,\n\t\t\t\t   \"usb bulk transmit failed: %d\\n\", ret);\n\t\t\tusb_unanchor_urb(urb);\n\t\t\tusb_free_urb(urb);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free_urb_to_pipe;\n\t\t}\n\n\t\tusb_free_urb(urb);\n\t}\n\n\treturn 0;\n\nerr_free_urb_to_pipe:\n\tath10k_usb_free_urb_to_pipe(urb_context->pipe, urb_context);\nerr:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -42,6 +42,7 @@\n \t\t\tath10k_dbg(ar, ATH10K_DBG_USB_BULK,\n \t\t\t\t   \"usb bulk transmit failed: %d\\n\", ret);\n \t\t\tusb_unanchor_urb(urb);\n+\t\t\tusb_free_urb(urb);\n \t\t\tret = -EINVAL;\n \t\t\tgoto err_free_urb_to_pipe;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tusb_free_urb(urb);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the ath10k_usb_hif_tx_sg() function in drivers/net/wireless/ath/ath10k/usb.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering usb_submit_urb() failures, aka CID-b8d17e7d93d2.",
        "id": 2161
    },
    {
        "cve_id": "CVE-2023-0615",
        "code_before_change": "void vivid_update_format_cap(struct vivid_dev *dev, bool keep_controls)\n{\n\tstruct v4l2_bt_timings *bt = &dev->dv_timings_cap[dev->input].bt;\n\tu32 dims[V4L2_CTRL_MAX_DIMS] = {};\n\tunsigned size;\n\tu64 pixelclock;\n\n\tswitch (dev->input_type[dev->input]) {\n\tcase WEBCAM:\n\tdefault:\n\t\tdev->src_rect.width = webcam_sizes[dev->webcam_size_idx].width;\n\t\tdev->src_rect.height = webcam_sizes[dev->webcam_size_idx].height;\n\t\tdev->timeperframe_vid_cap = webcam_intervals[dev->webcam_ival_idx];\n\t\tdev->field_cap = V4L2_FIELD_NONE;\n\t\ttpg_s_rgb_range(&dev->tpg, V4L2_DV_RGB_RANGE_AUTO);\n\t\tbreak;\n\tcase TV:\n\tcase SVID:\n\t\tdev->field_cap = dev->tv_field_cap;\n\t\tdev->src_rect.width = 720;\n\t\tif (dev->std_cap[dev->input] & V4L2_STD_525_60) {\n\t\t\tdev->src_rect.height = 480;\n\t\t\tdev->timeperframe_vid_cap = (struct v4l2_fract) { 1001, 30000 };\n\t\t\tdev->service_set_cap = V4L2_SLICED_CAPTION_525;\n\t\t} else {\n\t\t\tdev->src_rect.height = 576;\n\t\t\tdev->timeperframe_vid_cap = (struct v4l2_fract) { 1000, 25000 };\n\t\t\tdev->service_set_cap = V4L2_SLICED_WSS_625 | V4L2_SLICED_TELETEXT_B;\n\t\t}\n\t\ttpg_s_rgb_range(&dev->tpg, V4L2_DV_RGB_RANGE_AUTO);\n\t\tbreak;\n\tcase HDMI:\n\t\tdev->src_rect.width = bt->width;\n\t\tdev->src_rect.height = bt->height;\n\t\tsize = V4L2_DV_BT_FRAME_WIDTH(bt) * V4L2_DV_BT_FRAME_HEIGHT(bt);\n\t\tif (dev->reduced_fps && can_reduce_fps(bt)) {\n\t\t\tpixelclock = div_u64(bt->pixelclock * 1000, 1001);\n\t\t\tbt->flags |= V4L2_DV_FL_REDUCED_FPS;\n\t\t} else {\n\t\t\tpixelclock = bt->pixelclock;\n\t\t\tbt->flags &= ~V4L2_DV_FL_REDUCED_FPS;\n\t\t}\n\t\tdev->timeperframe_vid_cap = (struct v4l2_fract) {\n\t\t\tsize / 100, (u32)pixelclock / 100\n\t\t};\n\t\tif (bt->interlaced)\n\t\t\tdev->field_cap = V4L2_FIELD_ALTERNATE;\n\t\telse\n\t\t\tdev->field_cap = V4L2_FIELD_NONE;\n\n\t\t/*\n\t\t * We can be called from within s_ctrl, in that case we can't\n\t\t * set/get controls. Luckily we don't need to in that case.\n\t\t */\n\t\tif (keep_controls || !dev->colorspace)\n\t\t\tbreak;\n\t\tif (bt->flags & V4L2_DV_FL_IS_CE_VIDEO) {\n\t\t\tif (bt->width == 720 && bt->height <= 576)\n\t\t\t\tv4l2_ctrl_s_ctrl(dev->colorspace, VIVID_CS_170M);\n\t\t\telse\n\t\t\t\tv4l2_ctrl_s_ctrl(dev->colorspace, VIVID_CS_709);\n\t\t\tv4l2_ctrl_s_ctrl(dev->real_rgb_range_cap, 1);\n\t\t} else {\n\t\t\tv4l2_ctrl_s_ctrl(dev->colorspace, VIVID_CS_SRGB);\n\t\t\tv4l2_ctrl_s_ctrl(dev->real_rgb_range_cap, 0);\n\t\t}\n\t\ttpg_s_rgb_range(&dev->tpg, v4l2_ctrl_g_ctrl(dev->rgb_range_cap));\n\t\tbreak;\n\t}\n\tvfree(dev->bitmap_cap);\n\tdev->bitmap_cap = NULL;\n\tvivid_update_quality(dev);\n\ttpg_reset_source(&dev->tpg, dev->src_rect.width, dev->src_rect.height, dev->field_cap);\n\tdev->crop_cap = dev->src_rect;\n\tdev->crop_bounds_cap = dev->src_rect;\n\tdev->compose_cap = dev->crop_cap;\n\tif (V4L2_FIELD_HAS_T_OR_B(dev->field_cap))\n\t\tdev->compose_cap.height /= 2;\n\tdev->fmt_cap_rect = dev->compose_cap;\n\ttpg_s_video_aspect(&dev->tpg, vivid_get_video_aspect(dev));\n\ttpg_s_pixel_aspect(&dev->tpg, vivid_get_pixel_aspect(dev));\n\ttpg_update_mv_step(&dev->tpg);\n\tdims[0] = roundup(dev->src_rect.width, PIXEL_ARRAY_DIV);\n\tdims[1] = roundup(dev->src_rect.height, PIXEL_ARRAY_DIV);\n\tv4l2_ctrl_modify_dimensions(dev->pixel_array, dims);\n}",
        "code_after_change": "void vivid_update_format_cap(struct vivid_dev *dev, bool keep_controls)\n{\n\tstruct v4l2_bt_timings *bt = &dev->dv_timings_cap[dev->input].bt;\n\tu32 dims[V4L2_CTRL_MAX_DIMS] = {};\n\tunsigned size;\n\tu64 pixelclock;\n\n\tswitch (dev->input_type[dev->input]) {\n\tcase WEBCAM:\n\tdefault:\n\t\tdev->src_rect.width = webcam_sizes[dev->webcam_size_idx].width;\n\t\tdev->src_rect.height = webcam_sizes[dev->webcam_size_idx].height;\n\t\tdev->timeperframe_vid_cap = webcam_intervals[dev->webcam_ival_idx];\n\t\tdev->field_cap = V4L2_FIELD_NONE;\n\t\ttpg_s_rgb_range(&dev->tpg, V4L2_DV_RGB_RANGE_AUTO);\n\t\tbreak;\n\tcase TV:\n\tcase SVID:\n\t\tdev->field_cap = dev->tv_field_cap;\n\t\tdev->src_rect.width = 720;\n\t\tif (dev->std_cap[dev->input] & V4L2_STD_525_60) {\n\t\t\tdev->src_rect.height = 480;\n\t\t\tdev->timeperframe_vid_cap = (struct v4l2_fract) { 1001, 30000 };\n\t\t\tdev->service_set_cap = V4L2_SLICED_CAPTION_525;\n\t\t} else {\n\t\t\tdev->src_rect.height = 576;\n\t\t\tdev->timeperframe_vid_cap = (struct v4l2_fract) { 1000, 25000 };\n\t\t\tdev->service_set_cap = V4L2_SLICED_WSS_625 | V4L2_SLICED_TELETEXT_B;\n\t\t}\n\t\ttpg_s_rgb_range(&dev->tpg, V4L2_DV_RGB_RANGE_AUTO);\n\t\tbreak;\n\tcase HDMI:\n\t\tdev->src_rect.width = bt->width;\n\t\tdev->src_rect.height = bt->height;\n\t\tsize = V4L2_DV_BT_FRAME_WIDTH(bt) * V4L2_DV_BT_FRAME_HEIGHT(bt);\n\t\tif (dev->reduced_fps && can_reduce_fps(bt)) {\n\t\t\tpixelclock = div_u64(bt->pixelclock * 1000, 1001);\n\t\t\tbt->flags |= V4L2_DV_FL_REDUCED_FPS;\n\t\t} else {\n\t\t\tpixelclock = bt->pixelclock;\n\t\t\tbt->flags &= ~V4L2_DV_FL_REDUCED_FPS;\n\t\t}\n\t\tdev->timeperframe_vid_cap = (struct v4l2_fract) {\n\t\t\tsize / 100, (u32)pixelclock / 100\n\t\t};\n\t\tif (bt->interlaced)\n\t\t\tdev->field_cap = V4L2_FIELD_ALTERNATE;\n\t\telse\n\t\t\tdev->field_cap = V4L2_FIELD_NONE;\n\n\t\t/*\n\t\t * We can be called from within s_ctrl, in that case we can't\n\t\t * set/get controls. Luckily we don't need to in that case.\n\t\t */\n\t\tif (keep_controls || !dev->colorspace)\n\t\t\tbreak;\n\t\tif (bt->flags & V4L2_DV_FL_IS_CE_VIDEO) {\n\t\t\tif (bt->width == 720 && bt->height <= 576)\n\t\t\t\tv4l2_ctrl_s_ctrl(dev->colorspace, VIVID_CS_170M);\n\t\t\telse\n\t\t\t\tv4l2_ctrl_s_ctrl(dev->colorspace, VIVID_CS_709);\n\t\t\tv4l2_ctrl_s_ctrl(dev->real_rgb_range_cap, 1);\n\t\t} else {\n\t\t\tv4l2_ctrl_s_ctrl(dev->colorspace, VIVID_CS_SRGB);\n\t\t\tv4l2_ctrl_s_ctrl(dev->real_rgb_range_cap, 0);\n\t\t}\n\t\ttpg_s_rgb_range(&dev->tpg, v4l2_ctrl_g_ctrl(dev->rgb_range_cap));\n\t\tbreak;\n\t}\n\tvfree(dev->bitmap_cap);\n\tdev->bitmap_cap = NULL;\n\tvivid_update_quality(dev);\n\ttpg_reset_source(&dev->tpg, dev->src_rect.width, dev->src_rect.height, dev->field_cap);\n\tdev->crop_cap = dev->src_rect;\n\tdev->crop_bounds_cap = dev->src_rect;\n\tif (dev->bitmap_cap &&\n\t    (dev->compose_cap.width != dev->crop_cap.width ||\n\t     dev->compose_cap.height != dev->crop_cap.height)) {\n\t\tvfree(dev->bitmap_cap);\n\t\tdev->bitmap_cap = NULL;\n\t}\n\tdev->compose_cap = dev->crop_cap;\n\tif (V4L2_FIELD_HAS_T_OR_B(dev->field_cap))\n\t\tdev->compose_cap.height /= 2;\n\tdev->fmt_cap_rect = dev->compose_cap;\n\ttpg_s_video_aspect(&dev->tpg, vivid_get_video_aspect(dev));\n\ttpg_s_pixel_aspect(&dev->tpg, vivid_get_pixel_aspect(dev));\n\ttpg_update_mv_step(&dev->tpg);\n\tdims[0] = roundup(dev->src_rect.width, PIXEL_ARRAY_DIV);\n\tdims[1] = roundup(dev->src_rect.height, PIXEL_ARRAY_DIV);\n\tv4l2_ctrl_modify_dimensions(dev->pixel_array, dims);\n}",
        "patch": "--- code before\n+++ code after\n@@ -73,6 +73,12 @@\n \ttpg_reset_source(&dev->tpg, dev->src_rect.width, dev->src_rect.height, dev->field_cap);\n \tdev->crop_cap = dev->src_rect;\n \tdev->crop_bounds_cap = dev->src_rect;\n+\tif (dev->bitmap_cap &&\n+\t    (dev->compose_cap.width != dev->crop_cap.width ||\n+\t     dev->compose_cap.height != dev->crop_cap.height)) {\n+\t\tvfree(dev->bitmap_cap);\n+\t\tdev->bitmap_cap = NULL;\n+\t}\n \tdev->compose_cap = dev->crop_cap;\n \tif (V4L2_FIELD_HAS_T_OR_B(dev->field_cap))\n \t\tdev->compose_cap.height /= 2;",
        "function_modified_lines": {
            "added": [
                "\tif (dev->bitmap_cap &&",
                "\t    (dev->compose_cap.width != dev->crop_cap.width ||",
                "\t     dev->compose_cap.height != dev->crop_cap.height)) {",
                "\t\tvfree(dev->bitmap_cap);",
                "\t\tdev->bitmap_cap = NULL;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-369",
            "CWE-190",
            "CWE-401"
        ],
        "cve_description": "A memory leak flaw and potential divide by zero and Integer overflow was found in the Linux kernel V4L2 and vivid test code functionality. This issue occurs when a user triggers ioctls, such as VIDIOC_S_DV_TIMINGS ioctl. This could allow a local user to crash the system if vivid test code enabled.",
        "id": 3837
    },
    {
        "cve_id": "CVE-2022-3526",
        "code_before_change": "static rx_handler_result_t macvlan_handle_frame(struct sk_buff **pskb)\n{\n\tstruct macvlan_port *port;\n\tstruct sk_buff *skb = *pskb;\n\tconst struct ethhdr *eth = eth_hdr(skb);\n\tconst struct macvlan_dev *vlan;\n\tconst struct macvlan_dev *src;\n\tstruct net_device *dev;\n\tunsigned int len = 0;\n\tint ret;\n\trx_handler_result_t handle_res;\n\n\t/* Packets from dev_loopback_xmit() do not have L2 header, bail out */\n\tif (unlikely(skb->pkt_type == PACKET_LOOPBACK))\n\t\treturn RX_HANDLER_PASS;\n\n\tport = macvlan_port_get_rcu(skb->dev);\n\tif (is_multicast_ether_addr(eth->h_dest)) {\n\t\tunsigned int hash;\n\n\t\tskb = ip_check_defrag(dev_net(skb->dev), skb, IP_DEFRAG_MACVLAN);\n\t\tif (!skb)\n\t\t\treturn RX_HANDLER_CONSUMED;\n\t\t*pskb = skb;\n\t\teth = eth_hdr(skb);\n\t\tif (macvlan_forward_source(skb, port, eth->h_source))\n\t\t\treturn RX_HANDLER_CONSUMED;\n\t\tsrc = macvlan_hash_lookup(port, eth->h_source);\n\t\tif (src && src->mode != MACVLAN_MODE_VEPA &&\n\t\t    src->mode != MACVLAN_MODE_BRIDGE) {\n\t\t\t/* forward to original port. */\n\t\t\tvlan = src;\n\t\t\tret = macvlan_broadcast_one(skb, vlan, eth, 0) ?:\n\t\t\t      __netif_rx(skb);\n\t\t\thandle_res = RX_HANDLER_CONSUMED;\n\t\t\tgoto out;\n\t\t}\n\n\t\thash = mc_hash(NULL, eth->h_dest);\n\t\tif (test_bit(hash, port->mc_filter))\n\t\t\tmacvlan_broadcast_enqueue(port, src, skb);\n\n\t\treturn RX_HANDLER_PASS;\n\t}\n\n\tif (macvlan_forward_source(skb, port, eth->h_source))\n\t\treturn RX_HANDLER_CONSUMED;\n\tif (macvlan_passthru(port))\n\t\tvlan = list_first_or_null_rcu(&port->vlans,\n\t\t\t\t\t      struct macvlan_dev, list);\n\telse\n\t\tvlan = macvlan_hash_lookup(port, eth->h_dest);\n\tif (!vlan || vlan->mode == MACVLAN_MODE_SOURCE)\n\t\treturn RX_HANDLER_PASS;\n\n\tdev = vlan->dev;\n\tif (unlikely(!(dev->flags & IFF_UP))) {\n\t\tkfree_skb(skb);\n\t\treturn RX_HANDLER_CONSUMED;\n\t}\n\tlen = skb->len + ETH_HLEN;\n\tskb = skb_share_check(skb, GFP_ATOMIC);\n\tif (!skb) {\n\t\tret = NET_RX_DROP;\n\t\thandle_res = RX_HANDLER_CONSUMED;\n\t\tgoto out;\n\t}\n\n\t*pskb = skb;\n\tskb->dev = dev;\n\tskb->pkt_type = PACKET_HOST;\n\n\tret = NET_RX_SUCCESS;\n\thandle_res = RX_HANDLER_ANOTHER;\nout:\n\tmacvlan_count_rx(vlan, len, ret == NET_RX_SUCCESS, false);\n\treturn handle_res;\n}",
        "code_after_change": "static rx_handler_result_t macvlan_handle_frame(struct sk_buff **pskb)\n{\n\tstruct macvlan_port *port;\n\tstruct sk_buff *skb = *pskb;\n\tconst struct ethhdr *eth = eth_hdr(skb);\n\tconst struct macvlan_dev *vlan;\n\tconst struct macvlan_dev *src;\n\tstruct net_device *dev;\n\tunsigned int len = 0;\n\tint ret;\n\trx_handler_result_t handle_res;\n\n\t/* Packets from dev_loopback_xmit() do not have L2 header, bail out */\n\tif (unlikely(skb->pkt_type == PACKET_LOOPBACK))\n\t\treturn RX_HANDLER_PASS;\n\n\tport = macvlan_port_get_rcu(skb->dev);\n\tif (is_multicast_ether_addr(eth->h_dest)) {\n\t\tunsigned int hash;\n\n\t\tskb = ip_check_defrag(dev_net(skb->dev), skb, IP_DEFRAG_MACVLAN);\n\t\tif (!skb)\n\t\t\treturn RX_HANDLER_CONSUMED;\n\t\t*pskb = skb;\n\t\teth = eth_hdr(skb);\n\t\tif (macvlan_forward_source(skb, port, eth->h_source)) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn RX_HANDLER_CONSUMED;\n\t\t}\n\t\tsrc = macvlan_hash_lookup(port, eth->h_source);\n\t\tif (src && src->mode != MACVLAN_MODE_VEPA &&\n\t\t    src->mode != MACVLAN_MODE_BRIDGE) {\n\t\t\t/* forward to original port. */\n\t\t\tvlan = src;\n\t\t\tret = macvlan_broadcast_one(skb, vlan, eth, 0) ?:\n\t\t\t      __netif_rx(skb);\n\t\t\thandle_res = RX_HANDLER_CONSUMED;\n\t\t\tgoto out;\n\t\t}\n\n\t\thash = mc_hash(NULL, eth->h_dest);\n\t\tif (test_bit(hash, port->mc_filter))\n\t\t\tmacvlan_broadcast_enqueue(port, src, skb);\n\n\t\treturn RX_HANDLER_PASS;\n\t}\n\n\tif (macvlan_forward_source(skb, port, eth->h_source)) {\n\t\tkfree_skb(skb);\n\t\treturn RX_HANDLER_CONSUMED;\n\t}\n\tif (macvlan_passthru(port))\n\t\tvlan = list_first_or_null_rcu(&port->vlans,\n\t\t\t\t\t      struct macvlan_dev, list);\n\telse\n\t\tvlan = macvlan_hash_lookup(port, eth->h_dest);\n\tif (!vlan || vlan->mode == MACVLAN_MODE_SOURCE)\n\t\treturn RX_HANDLER_PASS;\n\n\tdev = vlan->dev;\n\tif (unlikely(!(dev->flags & IFF_UP))) {\n\t\tkfree_skb(skb);\n\t\treturn RX_HANDLER_CONSUMED;\n\t}\n\tlen = skb->len + ETH_HLEN;\n\tskb = skb_share_check(skb, GFP_ATOMIC);\n\tif (!skb) {\n\t\tret = NET_RX_DROP;\n\t\thandle_res = RX_HANDLER_CONSUMED;\n\t\tgoto out;\n\t}\n\n\t*pskb = skb;\n\tskb->dev = dev;\n\tskb->pkt_type = PACKET_HOST;\n\n\tret = NET_RX_SUCCESS;\n\thandle_res = RX_HANDLER_ANOTHER;\nout:\n\tmacvlan_count_rx(vlan, len, ret == NET_RX_SUCCESS, false);\n\treturn handle_res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,8 +23,10 @@\n \t\t\treturn RX_HANDLER_CONSUMED;\n \t\t*pskb = skb;\n \t\teth = eth_hdr(skb);\n-\t\tif (macvlan_forward_source(skb, port, eth->h_source))\n+\t\tif (macvlan_forward_source(skb, port, eth->h_source)) {\n+\t\t\tkfree_skb(skb);\n \t\t\treturn RX_HANDLER_CONSUMED;\n+\t\t}\n \t\tsrc = macvlan_hash_lookup(port, eth->h_source);\n \t\tif (src && src->mode != MACVLAN_MODE_VEPA &&\n \t\t    src->mode != MACVLAN_MODE_BRIDGE) {\n@@ -43,8 +45,10 @@\n \t\treturn RX_HANDLER_PASS;\n \t}\n \n-\tif (macvlan_forward_source(skb, port, eth->h_source))\n+\tif (macvlan_forward_source(skb, port, eth->h_source)) {\n+\t\tkfree_skb(skb);\n \t\treturn RX_HANDLER_CONSUMED;\n+\t}\n \tif (macvlan_passthru(port))\n \t\tvlan = list_first_or_null_rcu(&port->vlans,\n \t\t\t\t\t      struct macvlan_dev, list);",
        "function_modified_lines": {
            "added": [
                "\t\tif (macvlan_forward_source(skb, port, eth->h_source)) {",
                "\t\t\tkfree_skb(skb);",
                "\t\t}",
                "\tif (macvlan_forward_source(skb, port, eth->h_source)) {",
                "\t\tkfree_skb(skb);",
                "\t}"
            ],
            "deleted": [
                "\t\tif (macvlan_forward_source(skb, port, eth->h_source))",
                "\tif (macvlan_forward_source(skb, port, eth->h_source))"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A vulnerability classified as problematic was found in Linux Kernel. This vulnerability affects the function macvlan_handle_frame of the file drivers/net/macvlan.c of the component skb. The manipulation leads to memory leak. The attack can be initiated remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211024.",
        "id": 3624
    },
    {
        "cve_id": "CVE-2019-19073",
        "code_before_change": "static int htc_setup_complete(struct htc_target *target)\n{\n\tstruct sk_buff *skb;\n\tstruct htc_comp_msg *comp_msg;\n\tint ret = 0;\n\tunsigned long time_left;\n\n\tskb = alloc_skb(50 + sizeof(struct htc_frame_hdr), GFP_ATOMIC);\n\tif (!skb) {\n\t\tdev_err(target->dev, \"failed to allocate send buffer\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tskb_reserve(skb, sizeof(struct htc_frame_hdr));\n\n\tcomp_msg = skb_put(skb, sizeof(struct htc_comp_msg));\n\tcomp_msg->msg_id = cpu_to_be16(HTC_MSG_SETUP_COMPLETE_ID);\n\n\ttarget->htc_flags |= HTC_OP_START_WAIT;\n\n\tret = htc_issue_send(target, skb, skb->len, 0, ENDPOINT0);\n\tif (ret)\n\t\tgoto err;\n\n\ttime_left = wait_for_completion_timeout(&target->cmd_wait, HZ);\n\tif (!time_left) {\n\t\tdev_err(target->dev, \"HTC start timeout\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n\nerr:\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}",
        "code_after_change": "static int htc_setup_complete(struct htc_target *target)\n{\n\tstruct sk_buff *skb;\n\tstruct htc_comp_msg *comp_msg;\n\tint ret = 0;\n\tunsigned long time_left;\n\n\tskb = alloc_skb(50 + sizeof(struct htc_frame_hdr), GFP_ATOMIC);\n\tif (!skb) {\n\t\tdev_err(target->dev, \"failed to allocate send buffer\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tskb_reserve(skb, sizeof(struct htc_frame_hdr));\n\n\tcomp_msg = skb_put(skb, sizeof(struct htc_comp_msg));\n\tcomp_msg->msg_id = cpu_to_be16(HTC_MSG_SETUP_COMPLETE_ID);\n\n\ttarget->htc_flags |= HTC_OP_START_WAIT;\n\n\tret = htc_issue_send(target, skb, skb->len, 0, ENDPOINT0);\n\tif (ret)\n\t\tgoto err;\n\n\ttime_left = wait_for_completion_timeout(&target->cmd_wait, HZ);\n\tif (!time_left) {\n\t\tdev_err(target->dev, \"HTC start timeout\\n\");\n\t\tkfree_skb(skb);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n\nerr:\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -24,6 +24,7 @@\n \ttime_left = wait_for_completion_timeout(&target->cmd_wait, HZ);\n \tif (!time_left) {\n \t\tdev_err(target->dev, \"HTC start timeout\\n\");\n+\t\tkfree_skb(skb);\n \t\treturn -ETIMEDOUT;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tkfree_skb(skb);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leaks in drivers/net/wireless/ath/ath9k/htc_hst.c in the Linux kernel through 5.3.11 allow attackers to cause a denial of service (memory consumption) by triggering wait_for_completion_timeout() failures. This affects the htc_config_pipe_credits() function, the htc_setup_complete() function, and the htc_connect_service() function, aka CID-853acf7caf10.",
        "id": 2156
    },
    {
        "cve_id": "CVE-2022-1012",
        "code_before_change": "u32 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,\n\t\t\t       __be16 dport)\n{\n\tconst struct {\n\t\tstruct in6_addr saddr;\n\t\tstruct in6_addr daddr;\n\t\t__be16 dport;\n\t} __aligned(SIPHASH_ALIGNMENT) combined = {\n\t\t.saddr = *(struct in6_addr *)saddr,\n\t\t.daddr = *(struct in6_addr *)daddr,\n\t\t.dport = dport\n\t};\n\tnet_secret_init();\n\treturn siphash(&combined, offsetofend(typeof(combined), dport),\n\t\t       &net_secret);\n}",
        "code_after_change": "u64 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,\n\t\t\t       __be16 dport)\n{\n\tconst struct {\n\t\tstruct in6_addr saddr;\n\t\tstruct in6_addr daddr;\n\t\t__be16 dport;\n\t} __aligned(SIPHASH_ALIGNMENT) combined = {\n\t\t.saddr = *(struct in6_addr *)saddr,\n\t\t.daddr = *(struct in6_addr *)daddr,\n\t\t.dport = dport\n\t};\n\tnet_secret_init();\n\treturn siphash(&combined, offsetofend(typeof(combined), dport),\n\t\t       &net_secret);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-u32 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,\n+u64 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,\n \t\t\t       __be16 dport)\n {\n \tconst struct {",
        "function_modified_lines": {
            "added": [
                "u64 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,"
            ],
            "deleted": [
                "u32 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak problem was found in the TCP source port generation algorithm in net/ipv4/tcp.c due to the small table perturb size. This flaw may allow an attacker to information leak and may cause a denial of service problem.",
        "id": 3233
    },
    {
        "cve_id": "CVE-2022-1012",
        "code_before_change": "static u32 inet6_sk_port_offset(const struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\treturn secure_ipv6_port_ephemeral(sk->sk_v6_rcv_saddr.s6_addr32,\n\t\t\t\t\t  sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t  inet->inet_dport);\n}",
        "code_after_change": "static u64 inet6_sk_port_offset(const struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\treturn secure_ipv6_port_ephemeral(sk->sk_v6_rcv_saddr.s6_addr32,\n\t\t\t\t\t  sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t  inet->inet_dport);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-static u32 inet6_sk_port_offset(const struct sock *sk)\n+static u64 inet6_sk_port_offset(const struct sock *sk)\n {\n \tconst struct inet_sock *inet = inet_sk(sk);\n ",
        "function_modified_lines": {
            "added": [
                "static u64 inet6_sk_port_offset(const struct sock *sk)"
            ],
            "deleted": [
                "static u32 inet6_sk_port_offset(const struct sock *sk)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak problem was found in the TCP source port generation algorithm in net/ipv4/tcp.c due to the small table perturb size. This flaw may allow an attacker to information leak and may cause a denial of service problem.",
        "id": 3237
    },
    {
        "cve_id": "CVE-2019-20810",
        "code_before_change": "int go7007_snd_init(struct go7007 *go)\n{\n\tstatic int dev;\n\tstruct go7007_snd *gosnd;\n\tint ret;\n\n\tif (dev >= SNDRV_CARDS)\n\t\treturn -ENODEV;\n\tif (!enable[dev]) {\n\t\tdev++;\n\t\treturn -ENOENT;\n\t}\n\tgosnd = kmalloc(sizeof(struct go7007_snd), GFP_KERNEL);\n\tif (gosnd == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&gosnd->lock);\n\tgosnd->hw_ptr = gosnd->w_idx = gosnd->avail = 0;\n\tgosnd->capturing = 0;\n\tret = snd_card_new(go->dev, index[dev], id[dev], THIS_MODULE, 0,\n\t\t\t   &gosnd->card);\n\tif (ret < 0) {\n\t\tkfree(gosnd);\n\t\treturn ret;\n\t}\n\tret = snd_device_new(gosnd->card, SNDRV_DEV_LOWLEVEL, go,\n\t\t\t&go7007_snd_device_ops);\n\tif (ret < 0) {\n\t\tkfree(gosnd);\n\t\treturn ret;\n\t}\n\tret = snd_pcm_new(gosnd->card, \"go7007\", 0, 0, 1, &gosnd->pcm);\n\tif (ret < 0) {\n\t\tsnd_card_free(gosnd->card);\n\t\tkfree(gosnd);\n\t\treturn ret;\n\t}\n\tstrscpy(gosnd->card->driver, \"go7007\", sizeof(gosnd->card->driver));\n\tstrscpy(gosnd->card->shortname, go->name, sizeof(gosnd->card->shortname));\n\tstrscpy(gosnd->card->longname, gosnd->card->shortname,\n\t\tsizeof(gosnd->card->longname));\n\n\tgosnd->pcm->private_data = go;\n\tsnd_pcm_set_ops(gosnd->pcm, SNDRV_PCM_STREAM_CAPTURE,\n\t\t\t&go7007_snd_capture_ops);\n\n\tret = snd_card_register(gosnd->card);\n\tif (ret < 0) {\n\t\tsnd_card_free(gosnd->card);\n\t\tkfree(gosnd);\n\t\treturn ret;\n\t}\n\n\tgosnd->substream = NULL;\n\tgo->snd_context = gosnd;\n\tv4l2_device_get(&go->v4l2_dev);\n\t++dev;\n\n\treturn 0;\n}",
        "code_after_change": "int go7007_snd_init(struct go7007 *go)\n{\n\tstatic int dev;\n\tstruct go7007_snd *gosnd;\n\tint ret;\n\n\tif (dev >= SNDRV_CARDS)\n\t\treturn -ENODEV;\n\tif (!enable[dev]) {\n\t\tdev++;\n\t\treturn -ENOENT;\n\t}\n\tgosnd = kmalloc(sizeof(struct go7007_snd), GFP_KERNEL);\n\tif (gosnd == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&gosnd->lock);\n\tgosnd->hw_ptr = gosnd->w_idx = gosnd->avail = 0;\n\tgosnd->capturing = 0;\n\tret = snd_card_new(go->dev, index[dev], id[dev], THIS_MODULE, 0,\n\t\t\t   &gosnd->card);\n\tif (ret < 0)\n\t\tgoto free_snd;\n\n\tret = snd_device_new(gosnd->card, SNDRV_DEV_LOWLEVEL, go,\n\t\t\t&go7007_snd_device_ops);\n\tif (ret < 0)\n\t\tgoto free_card;\n\n\tret = snd_pcm_new(gosnd->card, \"go7007\", 0, 0, 1, &gosnd->pcm);\n\tif (ret < 0)\n\t\tgoto free_card;\n\n\tstrscpy(gosnd->card->driver, \"go7007\", sizeof(gosnd->card->driver));\n\tstrscpy(gosnd->card->shortname, go->name, sizeof(gosnd->card->shortname));\n\tstrscpy(gosnd->card->longname, gosnd->card->shortname,\n\t\tsizeof(gosnd->card->longname));\n\n\tgosnd->pcm->private_data = go;\n\tsnd_pcm_set_ops(gosnd->pcm, SNDRV_PCM_STREAM_CAPTURE,\n\t\t\t&go7007_snd_capture_ops);\n\n\tret = snd_card_register(gosnd->card);\n\tif (ret < 0)\n\t\tgoto free_card;\n\n\tgosnd->substream = NULL;\n\tgo->snd_context = gosnd;\n\tv4l2_device_get(&go->v4l2_dev);\n\t++dev;\n\n\treturn 0;\n\nfree_card:\n\tsnd_card_free(gosnd->card);\nfree_snd:\n\tkfree(gosnd);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,22 +18,18 @@\n \tgosnd->capturing = 0;\n \tret = snd_card_new(go->dev, index[dev], id[dev], THIS_MODULE, 0,\n \t\t\t   &gosnd->card);\n-\tif (ret < 0) {\n-\t\tkfree(gosnd);\n-\t\treturn ret;\n-\t}\n+\tif (ret < 0)\n+\t\tgoto free_snd;\n+\n \tret = snd_device_new(gosnd->card, SNDRV_DEV_LOWLEVEL, go,\n \t\t\t&go7007_snd_device_ops);\n-\tif (ret < 0) {\n-\t\tkfree(gosnd);\n-\t\treturn ret;\n-\t}\n+\tif (ret < 0)\n+\t\tgoto free_card;\n+\n \tret = snd_pcm_new(gosnd->card, \"go7007\", 0, 0, 1, &gosnd->pcm);\n-\tif (ret < 0) {\n-\t\tsnd_card_free(gosnd->card);\n-\t\tkfree(gosnd);\n-\t\treturn ret;\n-\t}\n+\tif (ret < 0)\n+\t\tgoto free_card;\n+\n \tstrscpy(gosnd->card->driver, \"go7007\", sizeof(gosnd->card->driver));\n \tstrscpy(gosnd->card->shortname, go->name, sizeof(gosnd->card->shortname));\n \tstrscpy(gosnd->card->longname, gosnd->card->shortname,\n@@ -44,11 +40,8 @@\n \t\t\t&go7007_snd_capture_ops);\n \n \tret = snd_card_register(gosnd->card);\n-\tif (ret < 0) {\n-\t\tsnd_card_free(gosnd->card);\n-\t\tkfree(gosnd);\n-\t\treturn ret;\n-\t}\n+\tif (ret < 0)\n+\t\tgoto free_card;\n \n \tgosnd->substream = NULL;\n \tgo->snd_context = gosnd;\n@@ -56,4 +49,10 @@\n \t++dev;\n \n \treturn 0;\n+\n+free_card:\n+\tsnd_card_free(gosnd->card);\n+free_snd:\n+\tkfree(gosnd);\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (ret < 0)",
                "\t\tgoto free_snd;",
                "",
                "\tif (ret < 0)",
                "\t\tgoto free_card;",
                "",
                "\tif (ret < 0)",
                "\t\tgoto free_card;",
                "",
                "\tif (ret < 0)",
                "\t\tgoto free_card;",
                "",
                "free_card:",
                "\tsnd_card_free(gosnd->card);",
                "free_snd:",
                "\tkfree(gosnd);",
                "\treturn ret;"
            ],
            "deleted": [
                "\tif (ret < 0) {",
                "\t\tkfree(gosnd);",
                "\t\treturn ret;",
                "\t}",
                "\tif (ret < 0) {",
                "\t\tkfree(gosnd);",
                "\t\treturn ret;",
                "\t}",
                "\tif (ret < 0) {",
                "\t\tsnd_card_free(gosnd->card);",
                "\t\tkfree(gosnd);",
                "\t\treturn ret;",
                "\t}",
                "\tif (ret < 0) {",
                "\t\tsnd_card_free(gosnd->card);",
                "\t\tkfree(gosnd);",
                "\t\treturn ret;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "go7007_snd_init in drivers/media/usb/go7007/snd-go7007.c in the Linux kernel before 5.6 does not call snd_card_free for a failure path, which causes a memory leak, aka CID-9453264ef586.",
        "id": 2284
    },
    {
        "cve_id": "CVE-2019-15807",
        "code_before_change": "static struct domain_device *sas_ex_discover_expander(\n\tstruct domain_device *parent, int phy_id)\n{\n\tstruct sas_expander_device *parent_ex = rphy_to_expander_device(parent->rphy);\n\tstruct ex_phy *phy = &parent->ex_dev.ex_phy[phy_id];\n\tstruct domain_device *child = NULL;\n\tstruct sas_rphy *rphy;\n\tstruct sas_expander_device *edev;\n\tstruct asd_sas_port *port;\n\tint res;\n\n\tif (phy->routing_attr == DIRECT_ROUTING) {\n\t\tpr_warn(\"ex %016llx:%02d:D <--> ex %016llx:0x%x is not allowed\\n\",\n\t\t\tSAS_ADDR(parent->sas_addr), phy_id,\n\t\t\tSAS_ADDR(phy->attached_sas_addr),\n\t\t\tphy->attached_phy_id);\n\t\treturn NULL;\n\t}\n\tchild = sas_alloc_device();\n\tif (!child)\n\t\treturn NULL;\n\n\tphy->port = sas_port_alloc(&parent->rphy->dev, phy_id);\n\t/* FIXME: better error handling */\n\tBUG_ON(sas_port_add(phy->port) != 0);\n\n\n\tswitch (phy->attached_dev_type) {\n\tcase SAS_EDGE_EXPANDER_DEVICE:\n\t\trphy = sas_expander_alloc(phy->port,\n\t\t\t\t\t  SAS_EDGE_EXPANDER_DEVICE);\n\t\tbreak;\n\tcase SAS_FANOUT_EXPANDER_DEVICE:\n\t\trphy = sas_expander_alloc(phy->port,\n\t\t\t\t\t  SAS_FANOUT_EXPANDER_DEVICE);\n\t\tbreak;\n\tdefault:\n\t\trphy = NULL;\t/* shut gcc up */\n\t\tBUG();\n\t}\n\tport = parent->port;\n\tchild->rphy = rphy;\n\tget_device(&rphy->dev);\n\tedev = rphy_to_expander_device(rphy);\n\tchild->dev_type = phy->attached_dev_type;\n\tkref_get(&parent->kref);\n\tchild->parent = parent;\n\tchild->port = port;\n\tchild->iproto = phy->attached_iproto;\n\tchild->tproto = phy->attached_tproto;\n\tmemcpy(child->sas_addr, phy->attached_sas_addr, SAS_ADDR_SIZE);\n\tsas_hash_addr(child->hashed_sas_addr, child->sas_addr);\n\tsas_ex_get_linkrate(parent, child, phy);\n\tedev->level = parent_ex->level + 1;\n\tparent->port->disc.max_level = max(parent->port->disc.max_level,\n\t\t\t\t\t   edev->level);\n\tsas_init_dev(child);\n\tsas_fill_in_rphy(child, rphy);\n\tsas_rphy_add(rphy);\n\n\tspin_lock_irq(&parent->port->dev_list_lock);\n\tlist_add_tail(&child->dev_list_node, &parent->port->dev_list);\n\tspin_unlock_irq(&parent->port->dev_list_lock);\n\n\tres = sas_discover_expander(child);\n\tif (res) {\n\t\tsas_rphy_delete(rphy);\n\t\tspin_lock_irq(&parent->port->dev_list_lock);\n\t\tlist_del(&child->dev_list_node);\n\t\tspin_unlock_irq(&parent->port->dev_list_lock);\n\t\tsas_put_device(child);\n\t\treturn NULL;\n\t}\n\tlist_add_tail(&child->siblings, &parent->ex_dev.children);\n\treturn child;\n}",
        "code_after_change": "static struct domain_device *sas_ex_discover_expander(\n\tstruct domain_device *parent, int phy_id)\n{\n\tstruct sas_expander_device *parent_ex = rphy_to_expander_device(parent->rphy);\n\tstruct ex_phy *phy = &parent->ex_dev.ex_phy[phy_id];\n\tstruct domain_device *child = NULL;\n\tstruct sas_rphy *rphy;\n\tstruct sas_expander_device *edev;\n\tstruct asd_sas_port *port;\n\tint res;\n\n\tif (phy->routing_attr == DIRECT_ROUTING) {\n\t\tpr_warn(\"ex %016llx:%02d:D <--> ex %016llx:0x%x is not allowed\\n\",\n\t\t\tSAS_ADDR(parent->sas_addr), phy_id,\n\t\t\tSAS_ADDR(phy->attached_sas_addr),\n\t\t\tphy->attached_phy_id);\n\t\treturn NULL;\n\t}\n\tchild = sas_alloc_device();\n\tif (!child)\n\t\treturn NULL;\n\n\tphy->port = sas_port_alloc(&parent->rphy->dev, phy_id);\n\t/* FIXME: better error handling */\n\tBUG_ON(sas_port_add(phy->port) != 0);\n\n\n\tswitch (phy->attached_dev_type) {\n\tcase SAS_EDGE_EXPANDER_DEVICE:\n\t\trphy = sas_expander_alloc(phy->port,\n\t\t\t\t\t  SAS_EDGE_EXPANDER_DEVICE);\n\t\tbreak;\n\tcase SAS_FANOUT_EXPANDER_DEVICE:\n\t\trphy = sas_expander_alloc(phy->port,\n\t\t\t\t\t  SAS_FANOUT_EXPANDER_DEVICE);\n\t\tbreak;\n\tdefault:\n\t\trphy = NULL;\t/* shut gcc up */\n\t\tBUG();\n\t}\n\tport = parent->port;\n\tchild->rphy = rphy;\n\tget_device(&rphy->dev);\n\tedev = rphy_to_expander_device(rphy);\n\tchild->dev_type = phy->attached_dev_type;\n\tkref_get(&parent->kref);\n\tchild->parent = parent;\n\tchild->port = port;\n\tchild->iproto = phy->attached_iproto;\n\tchild->tproto = phy->attached_tproto;\n\tmemcpy(child->sas_addr, phy->attached_sas_addr, SAS_ADDR_SIZE);\n\tsas_hash_addr(child->hashed_sas_addr, child->sas_addr);\n\tsas_ex_get_linkrate(parent, child, phy);\n\tedev->level = parent_ex->level + 1;\n\tparent->port->disc.max_level = max(parent->port->disc.max_level,\n\t\t\t\t\t   edev->level);\n\tsas_init_dev(child);\n\tsas_fill_in_rphy(child, rphy);\n\tsas_rphy_add(rphy);\n\n\tspin_lock_irq(&parent->port->dev_list_lock);\n\tlist_add_tail(&child->dev_list_node, &parent->port->dev_list);\n\tspin_unlock_irq(&parent->port->dev_list_lock);\n\n\tres = sas_discover_expander(child);\n\tif (res) {\n\t\tsas_rphy_delete(rphy);\n\t\tspin_lock_irq(&parent->port->dev_list_lock);\n\t\tlist_del(&child->dev_list_node);\n\t\tspin_unlock_irq(&parent->port->dev_list_lock);\n\t\tsas_put_device(child);\n\t\tsas_port_delete(phy->port);\n\t\tphy->port = NULL;\n\t\treturn NULL;\n\t}\n\tlist_add_tail(&child->siblings, &parent->ex_dev.children);\n\treturn child;\n}",
        "patch": "--- code before\n+++ code after\n@@ -69,6 +69,8 @@\n \t\tlist_del(&child->dev_list_node);\n \t\tspin_unlock_irq(&parent->port->dev_list_lock);\n \t\tsas_put_device(child);\n+\t\tsas_port_delete(phy->port);\n+\t\tphy->port = NULL;\n \t\treturn NULL;\n \t}\n \tlist_add_tail(&child->siblings, &parent->ex_dev.children);",
        "function_modified_lines": {
            "added": [
                "\t\tsas_port_delete(phy->port);",
                "\t\tphy->port = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "In the Linux kernel before 5.1.13, there is a memory leak in drivers/scsi/libsas/sas_expander.c when SAS expander discovery fails. This will cause a BUG and denial of service.",
        "id": 2022
    },
    {
        "cve_id": "CVE-2023-32247",
        "code_before_change": "static struct ksmbd_session *__session_create(int protocol)\n{\n\tstruct ksmbd_session *sess;\n\tint ret;\n\n\tif (protocol != CIFDS_SESSION_FLAG_SMB2)\n\t\treturn NULL;\n\n\tsess = kzalloc(sizeof(struct ksmbd_session), GFP_KERNEL);\n\tif (!sess)\n\t\treturn NULL;\n\n\tif (ksmbd_init_file_table(&sess->file_table))\n\t\tgoto error;\n\n\tsess->state = SMB2_SESSION_IN_PROGRESS;\n\tset_session_flag(sess, protocol);\n\txa_init(&sess->tree_conns);\n\txa_init(&sess->ksmbd_chann_list);\n\txa_init(&sess->rpc_handle_list);\n\tsess->sequence_number = 1;\n\n\tret = __init_smb2_session(sess);\n\tif (ret)\n\t\tgoto error;\n\n\tida_init(&sess->tree_conn_ida);\n\n\tdown_write(&sessions_table_lock);\n\thash_add(sessions_table, &sess->hlist, sess->id);\n\tup_write(&sessions_table_lock);\n\n\treturn sess;\n\nerror:\n\tksmbd_session_destroy(sess);\n\treturn NULL;\n}",
        "code_after_change": "static struct ksmbd_session *__session_create(int protocol)\n{\n\tstruct ksmbd_session *sess;\n\tint ret;\n\n\tif (protocol != CIFDS_SESSION_FLAG_SMB2)\n\t\treturn NULL;\n\n\tsess = kzalloc(sizeof(struct ksmbd_session), GFP_KERNEL);\n\tif (!sess)\n\t\treturn NULL;\n\n\tif (ksmbd_init_file_table(&sess->file_table))\n\t\tgoto error;\n\n\tsess->last_active = jiffies;\n\tsess->state = SMB2_SESSION_IN_PROGRESS;\n\tset_session_flag(sess, protocol);\n\txa_init(&sess->tree_conns);\n\txa_init(&sess->ksmbd_chann_list);\n\txa_init(&sess->rpc_handle_list);\n\tsess->sequence_number = 1;\n\n\tret = __init_smb2_session(sess);\n\tif (ret)\n\t\tgoto error;\n\n\tida_init(&sess->tree_conn_ida);\n\n\tdown_write(&sessions_table_lock);\n\thash_add(sessions_table, &sess->hlist, sess->id);\n\tup_write(&sessions_table_lock);\n\n\treturn sess;\n\nerror:\n\tksmbd_session_destroy(sess);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,7 @@\n \tif (ksmbd_init_file_table(&sess->file_table))\n \t\tgoto error;\n \n+\tsess->last_active = jiffies;\n \tsess->state = SMB2_SESSION_IN_PROGRESS;\n \tset_session_flag(sess, protocol);\n \txa_init(&sess->tree_conns);",
        "function_modified_lines": {
            "added": [
                "\tsess->last_active = jiffies;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the handling of SMB2_SESSION_SETUP commands. The issue results from the lack of control of resource consumption. An attacker can leverage this vulnerability to create a denial-of-service condition on the system.",
        "id": 4011
    },
    {
        "cve_id": "CVE-2022-0742",
        "code_before_change": "int igmp6_event_query(struct sk_buff *skb)\n{\n\tstruct inet6_dev *idev = __in6_dev_get(skb->dev);\n\n\tif (!idev)\n\t\treturn -EINVAL;\n\n\tif (idev->dead) {\n\t\tkfree_skb(skb);\n\t\treturn -ENODEV;\n\t}\n\n\tspin_lock_bh(&idev->mc_query_lock);\n\tif (skb_queue_len(&idev->mc_query_queue) < MLD_MAX_SKBS) {\n\t\t__skb_queue_tail(&idev->mc_query_queue, skb);\n\t\tif (!mod_delayed_work(mld_wq, &idev->mc_query_work, 0))\n\t\t\tin6_dev_hold(idev);\n\t}\n\tspin_unlock_bh(&idev->mc_query_lock);\n\n\treturn 0;\n}",
        "code_after_change": "void igmp6_event_query(struct sk_buff *skb)\n{\n\tstruct inet6_dev *idev = __in6_dev_get(skb->dev);\n\n\tif (!idev || idev->dead)\n\t\tgoto out;\n\n\tspin_lock_bh(&idev->mc_query_lock);\n\tif (skb_queue_len(&idev->mc_query_queue) < MLD_MAX_SKBS) {\n\t\t__skb_queue_tail(&idev->mc_query_queue, skb);\n\t\tif (!mod_delayed_work(mld_wq, &idev->mc_query_work, 0))\n\t\t\tin6_dev_hold(idev);\n\t\tskb = NULL;\n\t}\n\tspin_unlock_bh(&idev->mc_query_lock);\nout:\n\tkfree_skb(skb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,22 +1,18 @@\n-int igmp6_event_query(struct sk_buff *skb)\n+void igmp6_event_query(struct sk_buff *skb)\n {\n \tstruct inet6_dev *idev = __in6_dev_get(skb->dev);\n \n-\tif (!idev)\n-\t\treturn -EINVAL;\n-\n-\tif (idev->dead) {\n-\t\tkfree_skb(skb);\n-\t\treturn -ENODEV;\n-\t}\n+\tif (!idev || idev->dead)\n+\t\tgoto out;\n \n \tspin_lock_bh(&idev->mc_query_lock);\n \tif (skb_queue_len(&idev->mc_query_queue) < MLD_MAX_SKBS) {\n \t\t__skb_queue_tail(&idev->mc_query_queue, skb);\n \t\tif (!mod_delayed_work(mld_wq, &idev->mc_query_work, 0))\n \t\t\tin6_dev_hold(idev);\n+\t\tskb = NULL;\n \t}\n \tspin_unlock_bh(&idev->mc_query_lock);\n-\n-\treturn 0;\n+out:\n+\tkfree_skb(skb);\n }",
        "function_modified_lines": {
            "added": [
                "void igmp6_event_query(struct sk_buff *skb)",
                "\tif (!idev || idev->dead)",
                "\t\tgoto out;",
                "\t\tskb = NULL;",
                "out:",
                "\tkfree_skb(skb);"
            ],
            "deleted": [
                "int igmp6_event_query(struct sk_buff *skb)",
                "\tif (!idev)",
                "\t\treturn -EINVAL;",
                "",
                "\tif (idev->dead) {",
                "\t\tkfree_skb(skb);",
                "\t\treturn -ENODEV;",
                "\t}",
                "",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leak in icmp6 implementation in Linux Kernel 5.13+ allows a remote attacker to DoS a host by making it go out-of-memory via icmp6 packets of type 130 or 131. We recommend upgrading past commit 2d3916f3189172d5c69d33065c3c21119fe539fc.",
        "id": 3218
    },
    {
        "cve_id": "CVE-2019-12379",
        "code_before_change": "static int\ncon_insert_unipair(struct uni_pagedir *p, u_short unicode, u_short fontpos)\n{\n\tint i, n;\n\tu16 **p1, *p2;\n\n\tp1 = p->uni_pgdir[n = unicode >> 11];\n\tif (!p1) {\n\t\tp1 = p->uni_pgdir[n] = kmalloc_array(32, sizeof(u16 *),\n\t\t\t\t\t\t     GFP_KERNEL);\n\t\tif (!p1) return -ENOMEM;\n\t\tfor (i = 0; i < 32; i++)\n\t\t\tp1[i] = NULL;\n\t}\n\n\tp2 = p1[n = (unicode >> 6) & 0x1f];\n\tif (!p2) {\n\t\tp2 = p1[n] = kmalloc_array(64, sizeof(u16), GFP_KERNEL);\n\t\tif (!p2) return -ENOMEM;\n\t\tmemset(p2, 0xff, 64*sizeof(u16)); /* No glyphs for the characters (yet) */\n\t}\n\n\tp2[unicode & 0x3f] = fontpos;\n\t\n\tp->sum += (fontpos << 20) + unicode;\n\n\treturn 0;\n}",
        "code_after_change": "static int\ncon_insert_unipair(struct uni_pagedir *p, u_short unicode, u_short fontpos)\n{\n\tint i, n;\n\tu16 **p1, *p2;\n\n\tp1 = p->uni_pgdir[n = unicode >> 11];\n\tif (!p1) {\n\t\tp1 = p->uni_pgdir[n] = kmalloc_array(32, sizeof(u16 *),\n\t\t\t\t\t\t     GFP_KERNEL);\n\t\tif (!p1) return -ENOMEM;\n\t\tfor (i = 0; i < 32; i++)\n\t\t\tp1[i] = NULL;\n\t}\n\n\tp2 = p1[n = (unicode >> 6) & 0x1f];\n\tif (!p2) {\n\t\tp2 = p1[n] = kmalloc_array(64, sizeof(u16), GFP_KERNEL);\n\t\tif (!p2) {\n\t\t\tkfree(p1);\n\t\t\tp->uni_pgdir[n] = NULL;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tmemset(p2, 0xff, 64*sizeof(u16)); /* No glyphs for the characters (yet) */\n\t}\n\n\tp2[unicode & 0x3f] = fontpos;\n\t\n\tp->sum += (fontpos << 20) + unicode;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,7 +16,11 @@\n \tp2 = p1[n = (unicode >> 6) & 0x1f];\n \tif (!p2) {\n \t\tp2 = p1[n] = kmalloc_array(64, sizeof(u16), GFP_KERNEL);\n-\t\tif (!p2) return -ENOMEM;\n+\t\tif (!p2) {\n+\t\t\tkfree(p1);\n+\t\t\tp->uni_pgdir[n] = NULL;\n+\t\t\treturn -ENOMEM;\n+\t\t}\n \t\tmemset(p2, 0xff, 64*sizeof(u16)); /* No glyphs for the characters (yet) */\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tif (!p2) {",
                "\t\t\tkfree(p1);",
                "\t\t\tp->uni_pgdir[n] = NULL;",
                "\t\t\treturn -ENOMEM;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tif (!p2) return -ENOMEM;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An issue was discovered in con_insert_unipair in drivers/tty/vt/consolemap.c in the Linux kernel through 5.1.5. There is a memory leak in a certain case of an ENOMEM outcome of kmalloc. NOTE: This id is disputed as not being an issue",
        "id": 1940
    },
    {
        "cve_id": "CVE-2022-47941",
        "code_before_change": "int smb2_handle_negotiate(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct smb2_negotiate_req *req = smb2_get_msg(work->request_buf);\n\tstruct smb2_negotiate_rsp *rsp = smb2_get_msg(work->response_buf);\n\tint rc = 0;\n\tunsigned int smb2_buf_len, smb2_neg_size;\n\t__le32 status;\n\n\tksmbd_debug(SMB, \"Received negotiate request\\n\");\n\tconn->need_neg = false;\n\tif (ksmbd_conn_good(work)) {\n\t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n\t\twork->send_no_response = 1;\n\t\treturn rc;\n\t}\n\n\tif (req->DialectCount == 0) {\n\t\tpr_err(\"malformed packet\\n\");\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tsmb2_buf_len = get_rfc1002_len(work->request_buf);\n\tsmb2_neg_size = offsetof(struct smb2_negotiate_req, Dialects);\n\tif (smb2_neg_size > smb2_buf_len) {\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (conn->dialect == SMB311_PROT_ID) {\n\t\tunsigned int nego_ctxt_off = le32_to_cpu(req->NegotiateContextOffset);\n\n\t\tif (smb2_buf_len < nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size > nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t} else {\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    smb2_buf_len) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tconn->cli_cap = le32_to_cpu(req->Capabilities);\n\tswitch (conn->dialect) {\n\tcase SMB311_PROT_ID:\n\t\tconn->preauth_info =\n\t\t\tkzalloc(sizeof(struct preauth_integrity_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conn->preauth_info) {\n\t\t\trc = -ENOMEM;\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tstatus = deassemble_neg_contexts(conn, req,\n\t\t\t\t\t\t get_rfc1002_len(work->request_buf));\n\t\tif (status != STATUS_SUCCESS) {\n\t\t\tpr_err(\"deassemble_neg_contexts error(0x%x)\\n\",\n\t\t\t       status);\n\t\t\trsp->hdr.Status = status;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\trc = init_smb3_11_server(conn);\n\t\tif (rc < 0) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tksmbd_gen_preauth_integrity_hash(conn,\n\t\t\t\t\t\t work->request_buf,\n\t\t\t\t\t\t conn->preauth_info->Preauth_HashValue);\n\t\trsp->NegotiateContextOffset =\n\t\t\t\tcpu_to_le32(OFFSET_OF_NEG_CONTEXT);\n\t\tassemble_neg_contexts(conn, rsp, work->response_buf);\n\t\tbreak;\n\tcase SMB302_PROT_ID:\n\t\tinit_smb3_02_server(conn);\n\t\tbreak;\n\tcase SMB30_PROT_ID:\n\t\tinit_smb3_0_server(conn);\n\t\tbreak;\n\tcase SMB21_PROT_ID:\n\t\tinit_smb2_1_server(conn);\n\t\tbreak;\n\tcase SMB2X_PROT_ID:\n\tcase BAD_PROT_ID:\n\tdefault:\n\t\tksmbd_debug(SMB, \"Server dialect :0x%x not supported\\n\",\n\t\t\t    conn->dialect);\n\t\trsp->hdr.Status = STATUS_NOT_SUPPORTED;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\n\t/* For stats */\n\tconn->connection_type = conn->dialect;\n\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\tmemcpy(conn->ClientGUID, req->ClientGUID,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\tconn->cli_sec_mode = le16_to_cpu(req->SecurityMode);\n\n\trsp->StructureSize = cpu_to_le16(65);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying server\n\t */\n\tmemset(rsp->ServerGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\tksmbd_debug(SMB, \"negotiate context offset %d, count %d\\n\",\n\t\t    le32_to_cpu(rsp->NegotiateContextOffset),\n\t\t    le16_to_cpu(rsp->NegotiateContextCount));\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\t\t\t  le16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf, sizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) - sizeof(rsp->Buffer) +\n\t\t\t AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tconn->use_spnego = true;\n\n\tif ((server_conf.signing == KSMBD_CONFIG_OPT_AUTO ||\n\t     server_conf.signing == KSMBD_CONFIG_OPT_DISABLED) &&\n\t    req->SecurityMode & SMB2_NEGOTIATE_SIGNING_REQUIRED_LE)\n\t\tconn->sign = true;\n\telse if (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY) {\n\t\tserver_conf.enforced_signing = true;\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\t\tconn->sign = true;\n\t}\n\n\tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tksmbd_conn_set_need_negotiate(work);\n\nerr_out:\n\tif (rc < 0)\n\t\tsmb2_set_err_rsp(work);\n\n\treturn rc;\n}",
        "code_after_change": "int smb2_handle_negotiate(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct smb2_negotiate_req *req = smb2_get_msg(work->request_buf);\n\tstruct smb2_negotiate_rsp *rsp = smb2_get_msg(work->response_buf);\n\tint rc = 0;\n\tunsigned int smb2_buf_len, smb2_neg_size;\n\t__le32 status;\n\n\tksmbd_debug(SMB, \"Received negotiate request\\n\");\n\tconn->need_neg = false;\n\tif (ksmbd_conn_good(work)) {\n\t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n\t\twork->send_no_response = 1;\n\t\treturn rc;\n\t}\n\n\tif (req->DialectCount == 0) {\n\t\tpr_err(\"malformed packet\\n\");\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tsmb2_buf_len = get_rfc1002_len(work->request_buf);\n\tsmb2_neg_size = offsetof(struct smb2_negotiate_req, Dialects);\n\tif (smb2_neg_size > smb2_buf_len) {\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (conn->dialect == SMB311_PROT_ID) {\n\t\tunsigned int nego_ctxt_off = le32_to_cpu(req->NegotiateContextOffset);\n\n\t\tif (smb2_buf_len < nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size > nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t} else {\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    smb2_buf_len) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tconn->cli_cap = le32_to_cpu(req->Capabilities);\n\tswitch (conn->dialect) {\n\tcase SMB311_PROT_ID:\n\t\tconn->preauth_info =\n\t\t\tkzalloc(sizeof(struct preauth_integrity_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conn->preauth_info) {\n\t\t\trc = -ENOMEM;\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tstatus = deassemble_neg_contexts(conn, req,\n\t\t\t\t\t\t get_rfc1002_len(work->request_buf));\n\t\tif (status != STATUS_SUCCESS) {\n\t\t\tpr_err(\"deassemble_neg_contexts error(0x%x)\\n\",\n\t\t\t       status);\n\t\t\trsp->hdr.Status = status;\n\t\t\trc = -EINVAL;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\trc = init_smb3_11_server(conn);\n\t\tif (rc < 0) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tksmbd_gen_preauth_integrity_hash(conn,\n\t\t\t\t\t\t work->request_buf,\n\t\t\t\t\t\t conn->preauth_info->Preauth_HashValue);\n\t\trsp->NegotiateContextOffset =\n\t\t\t\tcpu_to_le32(OFFSET_OF_NEG_CONTEXT);\n\t\tassemble_neg_contexts(conn, rsp, work->response_buf);\n\t\tbreak;\n\tcase SMB302_PROT_ID:\n\t\tinit_smb3_02_server(conn);\n\t\tbreak;\n\tcase SMB30_PROT_ID:\n\t\tinit_smb3_0_server(conn);\n\t\tbreak;\n\tcase SMB21_PROT_ID:\n\t\tinit_smb2_1_server(conn);\n\t\tbreak;\n\tcase SMB2X_PROT_ID:\n\tcase BAD_PROT_ID:\n\tdefault:\n\t\tksmbd_debug(SMB, \"Server dialect :0x%x not supported\\n\",\n\t\t\t    conn->dialect);\n\t\trsp->hdr.Status = STATUS_NOT_SUPPORTED;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\n\t/* For stats */\n\tconn->connection_type = conn->dialect;\n\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\tmemcpy(conn->ClientGUID, req->ClientGUID,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\tconn->cli_sec_mode = le16_to_cpu(req->SecurityMode);\n\n\trsp->StructureSize = cpu_to_le16(65);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying server\n\t */\n\tmemset(rsp->ServerGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\tksmbd_debug(SMB, \"negotiate context offset %d, count %d\\n\",\n\t\t    le32_to_cpu(rsp->NegotiateContextOffset),\n\t\t    le16_to_cpu(rsp->NegotiateContextCount));\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\t\t\t  le16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf, sizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) - sizeof(rsp->Buffer) +\n\t\t\t AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tconn->use_spnego = true;\n\n\tif ((server_conf.signing == KSMBD_CONFIG_OPT_AUTO ||\n\t     server_conf.signing == KSMBD_CONFIG_OPT_DISABLED) &&\n\t    req->SecurityMode & SMB2_NEGOTIATE_SIGNING_REQUIRED_LE)\n\t\tconn->sign = true;\n\telse if (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY) {\n\t\tserver_conf.enforced_signing = true;\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\t\tconn->sign = true;\n\t}\n\n\tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tksmbd_conn_set_need_negotiate(work);\n\nerr_out:\n\tif (rc < 0)\n\t\tsmb2_set_err_rsp(work);\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -79,12 +79,16 @@\n \t\t\t       status);\n \t\t\trsp->hdr.Status = status;\n \t\t\trc = -EINVAL;\n+\t\t\tkfree(conn->preauth_info);\n+\t\t\tconn->preauth_info = NULL;\n \t\t\tgoto err_out;\n \t\t}\n \n \t\trc = init_smb3_11_server(conn);\n \t\tif (rc < 0) {\n \t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n+\t\t\tkfree(conn->preauth_info);\n+\t\t\tconn->preauth_info = NULL;\n \t\t\tgoto err_out;\n \t\t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tkfree(conn->preauth_info);",
                "\t\t\tconn->preauth_info = NULL;",
                "\t\t\tkfree(conn->preauth_info);",
                "\t\t\tconn->preauth_info = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An issue was discovered in ksmbd in the Linux kernel 5.15 through 5.19 before 5.19.2. fs/ksmbd/smb2pdu.c omits a kfree call in certain smb2_handle_negotiate error conditions, aka a memory leak.",
        "id": 3769
    },
    {
        "cve_id": "CVE-2023-32247",
        "code_before_change": "struct ksmbd_session *ksmbd_session_lookup(struct ksmbd_conn *conn,\n\t\t\t\t\t   unsigned long long id)\n{\n\treturn xa_load(&conn->sessions, id);\n}",
        "code_after_change": "struct ksmbd_session *ksmbd_session_lookup(struct ksmbd_conn *conn,\n\t\t\t\t\t   unsigned long long id)\n{\n\tstruct ksmbd_session *sess;\n\n\tsess = xa_load(&conn->sessions, id);\n\tif (sess)\n\t\tsess->last_active = jiffies;\n\treturn sess;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,10 @@\n struct ksmbd_session *ksmbd_session_lookup(struct ksmbd_conn *conn,\n \t\t\t\t\t   unsigned long long id)\n {\n-\treturn xa_load(&conn->sessions, id);\n+\tstruct ksmbd_session *sess;\n+\n+\tsess = xa_load(&conn->sessions, id);\n+\tif (sess)\n+\t\tsess->last_active = jiffies;\n+\treturn sess;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct ksmbd_session *sess;",
                "",
                "\tsess = xa_load(&conn->sessions, id);",
                "\tif (sess)",
                "\t\tsess->last_active = jiffies;",
                "\treturn sess;"
            ],
            "deleted": [
                "\treturn xa_load(&conn->sessions, id);"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the handling of SMB2_SESSION_SETUP commands. The issue results from the lack of control of resource consumption. An attacker can leverage this vulnerability to create a denial-of-service condition on the system.",
        "id": 4014
    },
    {
        "cve_id": "CVE-2019-18807",
        "code_before_change": "int sja1105_static_config_upload(struct sja1105_private *priv)\n{\n\tunsigned long port_bitmap = GENMASK_ULL(SJA1105_NUM_PORTS - 1, 0);\n\tstruct sja1105_static_config *config = &priv->static_config;\n\tconst struct sja1105_regs *regs = priv->info->regs;\n\tstruct device *dev = &priv->spidev->dev;\n\tstruct sja1105_status status;\n\tint rc, retries = RETRIES;\n\tu8 *config_buf;\n\tint buf_len;\n\n\tbuf_len = sja1105_static_config_get_length(config);\n\tconfig_buf = kcalloc(buf_len, sizeof(char), GFP_KERNEL);\n\tif (!config_buf)\n\t\treturn -ENOMEM;\n\n\trc = static_config_buf_prepare_for_upload(priv, config_buf, buf_len);\n\tif (rc < 0) {\n\t\tdev_err(dev, \"Invalid config, cannot upload\\n\");\n\t\treturn -EINVAL;\n\t}\n\t/* Prevent PHY jabbering during switch reset by inhibiting\n\t * Tx on all ports and waiting for current packet to drain.\n\t * Otherwise, the PHY will see an unterminated Ethernet packet.\n\t */\n\trc = sja1105_inhibit_tx(priv, port_bitmap, true);\n\tif (rc < 0) {\n\t\tdev_err(dev, \"Failed to inhibit Tx on ports\\n\");\n\t\treturn -ENXIO;\n\t}\n\t/* Wait for an eventual egress packet to finish transmission\n\t * (reach IFG). It is guaranteed that a second one will not\n\t * follow, and that switch cold reset is thus safe\n\t */\n\tusleep_range(500, 1000);\n\tdo {\n\t\t/* Put the SJA1105 in programming mode */\n\t\trc = sja1105_cold_reset(priv);\n\t\tif (rc < 0) {\n\t\t\tdev_err(dev, \"Failed to reset switch, retrying...\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\t/* Wait for the switch to come out of reset */\n\t\tusleep_range(1000, 5000);\n\t\t/* Upload the static config to the device */\n\t\trc = sja1105_spi_send_long_packed_buf(priv, SPI_WRITE,\n\t\t\t\t\t\t      regs->config,\n\t\t\t\t\t\t      config_buf, buf_len);\n\t\tif (rc < 0) {\n\t\t\tdev_err(dev, \"Failed to upload config, retrying...\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\t/* Check that SJA1105 responded well to the config upload */\n\t\trc = sja1105_status_get(priv, &status);\n\t\tif (rc < 0)\n\t\t\tcontinue;\n\n\t\tif (status.ids == 1) {\n\t\t\tdev_err(dev, \"Mismatch between hardware and static config \"\n\t\t\t\t\"device id. Wrote 0x%llx, wants 0x%llx\\n\",\n\t\t\t\tconfig->device_id, priv->info->device_id);\n\t\t\tcontinue;\n\t\t}\n\t\tif (status.crcchkl == 1) {\n\t\t\tdev_err(dev, \"Switch reported invalid local CRC on \"\n\t\t\t\t\"the uploaded config, retrying...\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tif (status.crcchkg == 1) {\n\t\t\tdev_err(dev, \"Switch reported invalid global CRC on \"\n\t\t\t\t\"the uploaded config, retrying...\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tif (status.configs == 0) {\n\t\t\tdev_err(dev, \"Switch reported that configuration is \"\n\t\t\t\t\"invalid, retrying...\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\t/* Success! */\n\t\tbreak;\n\t} while (--retries);\n\n\tif (!retries) {\n\t\trc = -EIO;\n\t\tdev_err(dev, \"Failed to upload config to device, giving up\\n\");\n\t\tgoto out;\n\t} else if (retries != RETRIES) {\n\t\tdev_info(dev, \"Succeeded after %d tried\\n\", RETRIES - retries);\n\t}\n\n\trc = sja1105_ptp_reset(priv);\n\tif (rc < 0)\n\t\tdev_err(dev, \"Failed to reset PTP clock: %d\\n\", rc);\n\n\tdev_info(dev, \"Reset switch and programmed static config\\n\");\n\nout:\n\tkfree(config_buf);\n\treturn rc;\n}",
        "code_after_change": "int sja1105_static_config_upload(struct sja1105_private *priv)\n{\n\tunsigned long port_bitmap = GENMASK_ULL(SJA1105_NUM_PORTS - 1, 0);\n\tstruct sja1105_static_config *config = &priv->static_config;\n\tconst struct sja1105_regs *regs = priv->info->regs;\n\tstruct device *dev = &priv->spidev->dev;\n\tstruct sja1105_status status;\n\tint rc, retries = RETRIES;\n\tu8 *config_buf;\n\tint buf_len;\n\n\tbuf_len = sja1105_static_config_get_length(config);\n\tconfig_buf = kcalloc(buf_len, sizeof(char), GFP_KERNEL);\n\tif (!config_buf)\n\t\treturn -ENOMEM;\n\n\trc = static_config_buf_prepare_for_upload(priv, config_buf, buf_len);\n\tif (rc < 0) {\n\t\tdev_err(dev, \"Invalid config, cannot upload\\n\");\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\t/* Prevent PHY jabbering during switch reset by inhibiting\n\t * Tx on all ports and waiting for current packet to drain.\n\t * Otherwise, the PHY will see an unterminated Ethernet packet.\n\t */\n\trc = sja1105_inhibit_tx(priv, port_bitmap, true);\n\tif (rc < 0) {\n\t\tdev_err(dev, \"Failed to inhibit Tx on ports\\n\");\n\t\trc = -ENXIO;\n\t\tgoto out;\n\t}\n\t/* Wait for an eventual egress packet to finish transmission\n\t * (reach IFG). It is guaranteed that a second one will not\n\t * follow, and that switch cold reset is thus safe\n\t */\n\tusleep_range(500, 1000);\n\tdo {\n\t\t/* Put the SJA1105 in programming mode */\n\t\trc = sja1105_cold_reset(priv);\n\t\tif (rc < 0) {\n\t\t\tdev_err(dev, \"Failed to reset switch, retrying...\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\t/* Wait for the switch to come out of reset */\n\t\tusleep_range(1000, 5000);\n\t\t/* Upload the static config to the device */\n\t\trc = sja1105_spi_send_long_packed_buf(priv, SPI_WRITE,\n\t\t\t\t\t\t      regs->config,\n\t\t\t\t\t\t      config_buf, buf_len);\n\t\tif (rc < 0) {\n\t\t\tdev_err(dev, \"Failed to upload config, retrying...\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\t/* Check that SJA1105 responded well to the config upload */\n\t\trc = sja1105_status_get(priv, &status);\n\t\tif (rc < 0)\n\t\t\tcontinue;\n\n\t\tif (status.ids == 1) {\n\t\t\tdev_err(dev, \"Mismatch between hardware and static config \"\n\t\t\t\t\"device id. Wrote 0x%llx, wants 0x%llx\\n\",\n\t\t\t\tconfig->device_id, priv->info->device_id);\n\t\t\tcontinue;\n\t\t}\n\t\tif (status.crcchkl == 1) {\n\t\t\tdev_err(dev, \"Switch reported invalid local CRC on \"\n\t\t\t\t\"the uploaded config, retrying...\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tif (status.crcchkg == 1) {\n\t\t\tdev_err(dev, \"Switch reported invalid global CRC on \"\n\t\t\t\t\"the uploaded config, retrying...\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tif (status.configs == 0) {\n\t\t\tdev_err(dev, \"Switch reported that configuration is \"\n\t\t\t\t\"invalid, retrying...\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\t/* Success! */\n\t\tbreak;\n\t} while (--retries);\n\n\tif (!retries) {\n\t\trc = -EIO;\n\t\tdev_err(dev, \"Failed to upload config to device, giving up\\n\");\n\t\tgoto out;\n\t} else if (retries != RETRIES) {\n\t\tdev_info(dev, \"Succeeded after %d tried\\n\", RETRIES - retries);\n\t}\n\n\trc = sja1105_ptp_reset(priv);\n\tif (rc < 0)\n\t\tdev_err(dev, \"Failed to reset PTP clock: %d\\n\", rc);\n\n\tdev_info(dev, \"Reset switch and programmed static config\\n\");\n\nout:\n\tkfree(config_buf);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,7 +17,8 @@\n \trc = static_config_buf_prepare_for_upload(priv, config_buf, buf_len);\n \tif (rc < 0) {\n \t\tdev_err(dev, \"Invalid config, cannot upload\\n\");\n-\t\treturn -EINVAL;\n+\t\trc = -EINVAL;\n+\t\tgoto out;\n \t}\n \t/* Prevent PHY jabbering during switch reset by inhibiting\n \t * Tx on all ports and waiting for current packet to drain.\n@@ -26,7 +27,8 @@\n \trc = sja1105_inhibit_tx(priv, port_bitmap, true);\n \tif (rc < 0) {\n \t\tdev_err(dev, \"Failed to inhibit Tx on ports\\n\");\n-\t\treturn -ENXIO;\n+\t\trc = -ENXIO;\n+\t\tgoto out;\n \t}\n \t/* Wait for an eventual egress packet to finish transmission\n \t * (reach IFG). It is guaranteed that a second one will not",
        "function_modified_lines": {
            "added": [
                "\t\trc = -EINVAL;",
                "\t\tgoto out;",
                "\t\trc = -ENXIO;",
                "\t\tgoto out;"
            ],
            "deleted": [
                "\t\treturn -EINVAL;",
                "\t\treturn -ENXIO;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Two memory leaks in the sja1105_static_config_upload() function in drivers/net/dsa/sja1105/sja1105_spi.c in the Linux kernel before 5.3.5 allow attackers to cause a denial of service (memory consumption) by triggering static_config_buf_prepare_for_upload() or sja1105_inhibit_tx() failures, aka CID-68501df92d11.",
        "id": 2099
    },
    {
        "cve_id": "CVE-2022-1012",
        "code_before_change": "static u32 inet_sk_port_offset(const struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\treturn secure_ipv4_port_ephemeral(inet->inet_rcv_saddr,\n\t\t\t\t\t  inet->inet_daddr,\n\t\t\t\t\t  inet->inet_dport);\n}",
        "code_after_change": "static u64 inet_sk_port_offset(const struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\treturn secure_ipv4_port_ephemeral(inet->inet_rcv_saddr,\n\t\t\t\t\t  inet->inet_daddr,\n\t\t\t\t\t  inet->inet_dport);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-static u32 inet_sk_port_offset(const struct sock *sk)\n+static u64 inet_sk_port_offset(const struct sock *sk)\n {\n \tconst struct inet_sock *inet = inet_sk(sk);\n ",
        "function_modified_lines": {
            "added": [
                "static u64 inet_sk_port_offset(const struct sock *sk)"
            ],
            "deleted": [
                "static u32 inet_sk_port_offset(const struct sock *sk)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak problem was found in the TCP source port generation algorithm in net/ipv4/tcp.c due to the small table perturb size. This flaw may allow an attacker to information leak and may cause a denial of service problem.",
        "id": 3234
    },
    {
        "cve_id": "CVE-2019-20096",
        "code_before_change": "static int __feat_register_sp(struct list_head *fn, u8 feat, u8 is_local,\n\t\t\t      u8 mandatory, u8 const *sp_val, u8 sp_len)\n{\n\tdccp_feat_val fval;\n\n\tif (dccp_feat_type(feat) != FEAT_SP ||\n\t    !dccp_feat_sp_list_ok(feat, sp_val, sp_len))\n\t\treturn -EINVAL;\n\n\t/* Avoid negotiating alien CCIDs by only advertising supported ones */\n\tif (feat == DCCPF_CCID && !ccid_support_check(sp_val, sp_len))\n\t\treturn -EOPNOTSUPP;\n\n\tif (dccp_feat_clone_sp_val(&fval, sp_val, sp_len))\n\t\treturn -ENOMEM;\n\n\treturn dccp_feat_push_change(fn, feat, is_local, mandatory, &fval);\n}",
        "code_after_change": "static int __feat_register_sp(struct list_head *fn, u8 feat, u8 is_local,\n\t\t\t      u8 mandatory, u8 const *sp_val, u8 sp_len)\n{\n\tdccp_feat_val fval;\n\n\tif (dccp_feat_type(feat) != FEAT_SP ||\n\t    !dccp_feat_sp_list_ok(feat, sp_val, sp_len))\n\t\treturn -EINVAL;\n\n\t/* Avoid negotiating alien CCIDs by only advertising supported ones */\n\tif (feat == DCCPF_CCID && !ccid_support_check(sp_val, sp_len))\n\t\treturn -EOPNOTSUPP;\n\n\tif (dccp_feat_clone_sp_val(&fval, sp_val, sp_len))\n\t\treturn -ENOMEM;\n\n\tif (dccp_feat_push_change(fn, feat, is_local, mandatory, &fval)) {\n\t\tkfree(fval.sp.vec);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,5 +14,10 @@\n \tif (dccp_feat_clone_sp_val(&fval, sp_val, sp_len))\n \t\treturn -ENOMEM;\n \n-\treturn dccp_feat_push_change(fn, feat, is_local, mandatory, &fval);\n+\tif (dccp_feat_push_change(fn, feat, is_local, mandatory, &fval)) {\n+\t\tkfree(fval.sp.vec);\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (dccp_feat_push_change(fn, feat, is_local, mandatory, &fval)) {",
                "\t\tkfree(fval.sp.vec);",
                "\t\treturn -ENOMEM;",
                "\t}",
                "",
                "\treturn 0;"
            ],
            "deleted": [
                "\treturn dccp_feat_push_change(fn, feat, is_local, mandatory, &fval);"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "In the Linux kernel before 5.1, there is a memory leak in __feat_register_sp() in net/dccp/feat.c, which may cause denial of service, aka CID-1d3ff0950e2b.",
        "id": 2273
    },
    {
        "cve_id": "CVE-2019-19052",
        "code_before_change": "static int gs_can_open(struct net_device *netdev)\n{\n\tstruct gs_can *dev = netdev_priv(netdev);\n\tstruct gs_usb *parent = dev->parent;\n\tint rc, i;\n\tstruct gs_device_mode *dm;\n\tu32 ctrlmode;\n\n\trc = open_candev(netdev);\n\tif (rc)\n\t\treturn rc;\n\n\tif (atomic_add_return(1, &parent->active_channels) == 1) {\n\t\tfor (i = 0; i < GS_MAX_RX_URBS; i++) {\n\t\t\tstruct urb *urb;\n\t\t\tu8 *buf;\n\n\t\t\t/* alloc rx urb */\n\t\t\turb = usb_alloc_urb(0, GFP_KERNEL);\n\t\t\tif (!urb)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\t/* alloc rx buffer */\n\t\t\tbuf = usb_alloc_coherent(dev->udev,\n\t\t\t\t\t\t sizeof(struct gs_host_frame),\n\t\t\t\t\t\t GFP_KERNEL,\n\t\t\t\t\t\t &urb->transfer_dma);\n\t\t\tif (!buf) {\n\t\t\t\tnetdev_err(netdev,\n\t\t\t\t\t   \"No memory left for USB buffer\\n\");\n\t\t\t\tusb_free_urb(urb);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\t/* fill, anchor, and submit rx urb */\n\t\t\tusb_fill_bulk_urb(urb,\n\t\t\t\t\t  dev->udev,\n\t\t\t\t\t  usb_rcvbulkpipe(dev->udev,\n\t\t\t\t\t\t\t  GSUSB_ENDPOINT_IN),\n\t\t\t\t\t  buf,\n\t\t\t\t\t  sizeof(struct gs_host_frame),\n\t\t\t\t\t  gs_usb_receive_bulk_callback,\n\t\t\t\t\t  parent);\n\t\t\turb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\n\t\t\tusb_anchor_urb(urb, &parent->rx_submitted);\n\n\t\t\trc = usb_submit_urb(urb, GFP_KERNEL);\n\t\t\tif (rc) {\n\t\t\t\tif (rc == -ENODEV)\n\t\t\t\t\tnetif_device_detach(dev->netdev);\n\n\t\t\t\tnetdev_err(netdev,\n\t\t\t\t\t   \"usb_submit failed (err=%d)\\n\",\n\t\t\t\t\t   rc);\n\n\t\t\t\tusb_unanchor_urb(urb);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Drop reference,\n\t\t\t * USB core will take care of freeing it\n\t\t\t */\n\t\t\tusb_free_urb(urb);\n\t\t}\n\t}\n\n\tdm = kmalloc(sizeof(*dm), GFP_KERNEL);\n\tif (!dm)\n\t\treturn -ENOMEM;\n\n\t/* flags */\n\tctrlmode = dev->can.ctrlmode;\n\tdm->flags = 0;\n\n\tif (ctrlmode & CAN_CTRLMODE_LOOPBACK)\n\t\tdm->flags |= GS_CAN_MODE_LOOP_BACK;\n\telse if (ctrlmode & CAN_CTRLMODE_LISTENONLY)\n\t\tdm->flags |= GS_CAN_MODE_LISTEN_ONLY;\n\n\t/* Controller is not allowed to retry TX\n\t * this mode is unavailable on atmels uc3c hardware\n\t */\n\tif (ctrlmode & CAN_CTRLMODE_ONE_SHOT)\n\t\tdm->flags |= GS_CAN_MODE_ONE_SHOT;\n\n\tif (ctrlmode & CAN_CTRLMODE_3_SAMPLES)\n\t\tdm->flags |= GS_CAN_MODE_TRIPLE_SAMPLE;\n\n\t/* finally start device */\n\tdm->mode = GS_CAN_MODE_START;\n\trc = usb_control_msg(interface_to_usbdev(dev->iface),\n\t\t\t     usb_sndctrlpipe(interface_to_usbdev(dev->iface), 0),\n\t\t\t     GS_USB_BREQ_MODE,\n\t\t\t     USB_DIR_OUT | USB_TYPE_VENDOR |\n\t\t\t     USB_RECIP_INTERFACE,\n\t\t\t     dev->channel,\n\t\t\t     0,\n\t\t\t     dm,\n\t\t\t     sizeof(*dm),\n\t\t\t     1000);\n\n\tif (rc < 0) {\n\t\tnetdev_err(netdev, \"Couldn't start device (err=%d)\\n\", rc);\n\t\tkfree(dm);\n\t\treturn rc;\n\t}\n\n\tkfree(dm);\n\n\tdev->can.state = CAN_STATE_ERROR_ACTIVE;\n\n\tif (!(dev->can.ctrlmode & CAN_CTRLMODE_LISTENONLY))\n\t\tnetif_start_queue(netdev);\n\n\treturn 0;\n}",
        "code_after_change": "static int gs_can_open(struct net_device *netdev)\n{\n\tstruct gs_can *dev = netdev_priv(netdev);\n\tstruct gs_usb *parent = dev->parent;\n\tint rc, i;\n\tstruct gs_device_mode *dm;\n\tu32 ctrlmode;\n\n\trc = open_candev(netdev);\n\tif (rc)\n\t\treturn rc;\n\n\tif (atomic_add_return(1, &parent->active_channels) == 1) {\n\t\tfor (i = 0; i < GS_MAX_RX_URBS; i++) {\n\t\t\tstruct urb *urb;\n\t\t\tu8 *buf;\n\n\t\t\t/* alloc rx urb */\n\t\t\turb = usb_alloc_urb(0, GFP_KERNEL);\n\t\t\tif (!urb)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\t/* alloc rx buffer */\n\t\t\tbuf = usb_alloc_coherent(dev->udev,\n\t\t\t\t\t\t sizeof(struct gs_host_frame),\n\t\t\t\t\t\t GFP_KERNEL,\n\t\t\t\t\t\t &urb->transfer_dma);\n\t\t\tif (!buf) {\n\t\t\t\tnetdev_err(netdev,\n\t\t\t\t\t   \"No memory left for USB buffer\\n\");\n\t\t\t\tusb_free_urb(urb);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\t/* fill, anchor, and submit rx urb */\n\t\t\tusb_fill_bulk_urb(urb,\n\t\t\t\t\t  dev->udev,\n\t\t\t\t\t  usb_rcvbulkpipe(dev->udev,\n\t\t\t\t\t\t\t  GSUSB_ENDPOINT_IN),\n\t\t\t\t\t  buf,\n\t\t\t\t\t  sizeof(struct gs_host_frame),\n\t\t\t\t\t  gs_usb_receive_bulk_callback,\n\t\t\t\t\t  parent);\n\t\t\turb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\n\t\t\tusb_anchor_urb(urb, &parent->rx_submitted);\n\n\t\t\trc = usb_submit_urb(urb, GFP_KERNEL);\n\t\t\tif (rc) {\n\t\t\t\tif (rc == -ENODEV)\n\t\t\t\t\tnetif_device_detach(dev->netdev);\n\n\t\t\t\tnetdev_err(netdev,\n\t\t\t\t\t   \"usb_submit failed (err=%d)\\n\",\n\t\t\t\t\t   rc);\n\n\t\t\t\tusb_unanchor_urb(urb);\n\t\t\t\tusb_free_urb(urb);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Drop reference,\n\t\t\t * USB core will take care of freeing it\n\t\t\t */\n\t\t\tusb_free_urb(urb);\n\t\t}\n\t}\n\n\tdm = kmalloc(sizeof(*dm), GFP_KERNEL);\n\tif (!dm)\n\t\treturn -ENOMEM;\n\n\t/* flags */\n\tctrlmode = dev->can.ctrlmode;\n\tdm->flags = 0;\n\n\tif (ctrlmode & CAN_CTRLMODE_LOOPBACK)\n\t\tdm->flags |= GS_CAN_MODE_LOOP_BACK;\n\telse if (ctrlmode & CAN_CTRLMODE_LISTENONLY)\n\t\tdm->flags |= GS_CAN_MODE_LISTEN_ONLY;\n\n\t/* Controller is not allowed to retry TX\n\t * this mode is unavailable on atmels uc3c hardware\n\t */\n\tif (ctrlmode & CAN_CTRLMODE_ONE_SHOT)\n\t\tdm->flags |= GS_CAN_MODE_ONE_SHOT;\n\n\tif (ctrlmode & CAN_CTRLMODE_3_SAMPLES)\n\t\tdm->flags |= GS_CAN_MODE_TRIPLE_SAMPLE;\n\n\t/* finally start device */\n\tdm->mode = GS_CAN_MODE_START;\n\trc = usb_control_msg(interface_to_usbdev(dev->iface),\n\t\t\t     usb_sndctrlpipe(interface_to_usbdev(dev->iface), 0),\n\t\t\t     GS_USB_BREQ_MODE,\n\t\t\t     USB_DIR_OUT | USB_TYPE_VENDOR |\n\t\t\t     USB_RECIP_INTERFACE,\n\t\t\t     dev->channel,\n\t\t\t     0,\n\t\t\t     dm,\n\t\t\t     sizeof(*dm),\n\t\t\t     1000);\n\n\tif (rc < 0) {\n\t\tnetdev_err(netdev, \"Couldn't start device (err=%d)\\n\", rc);\n\t\tkfree(dm);\n\t\treturn rc;\n\t}\n\n\tkfree(dm);\n\n\tdev->can.state = CAN_STATE_ERROR_ACTIVE;\n\n\tif (!(dev->can.ctrlmode & CAN_CTRLMODE_LISTENONLY))\n\t\tnetif_start_queue(netdev);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -55,6 +55,7 @@\n \t\t\t\t\t   rc);\n \n \t\t\t\tusb_unanchor_urb(urb);\n+\t\t\t\tusb_free_urb(urb);\n \t\t\t\tbreak;\n \t\t\t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tusb_free_urb(urb);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the gs_can_open() function in drivers/net/can/usb/gs_usb.c in the Linux kernel before 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering usb_submit_urb() failures, aka CID-fb5be6a7b486.",
        "id": 2133
    },
    {
        "cve_id": "CVE-2019-19082",
        "code_before_change": "struct resource_pool *dce100_create_resource_pool(\n\tuint8_t num_virtual_links,\n\tstruct dc  *dc)\n{\n\tstruct dce110_resource_pool *pool =\n\t\tkzalloc(sizeof(struct dce110_resource_pool), GFP_KERNEL);\n\n\tif (!pool)\n\t\treturn NULL;\n\n\tif (construct(num_virtual_links, dc, pool))\n\t\treturn &pool->base;\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "code_after_change": "struct resource_pool *dce100_create_resource_pool(\n\tuint8_t num_virtual_links,\n\tstruct dc  *dc)\n{\n\tstruct dce110_resource_pool *pool =\n\t\tkzalloc(sizeof(struct dce110_resource_pool), GFP_KERNEL);\n\n\tif (!pool)\n\t\treturn NULL;\n\n\tif (construct(num_virtual_links, dc, pool))\n\t\treturn &pool->base;\n\n\tkfree(pool);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,7 @@\n \tif (construct(num_virtual_links, dc, pool))\n \t\treturn &pool->base;\n \n+\tkfree(pool);\n \tBREAK_TO_DEBUGGER();\n \treturn NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\tkfree(pool);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leaks in *create_resource_pool() functions under drivers/gpu/drm/amd/display/dc in the Linux kernel through 5.3.11 allow attackers to cause a denial of service (memory consumption). This affects the dce120_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce120/dce120_resource.c, the dce110_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce110/dce110_resource.c, the dce100_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce100/dce100_resource.c, the dcn10_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dcn10/dcn10_resource.c, and the dce112_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce112/dce112_resource.c, aka CID-104c307147ad.",
        "id": 2165
    },
    {
        "cve_id": "CVE-2019-19083",
        "code_before_change": "struct clock_source *dce80_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "code_after_change": "struct clock_source *dce80_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \t\treturn &clk_src->base;\n \t}\n \n+\tkfree(clk_src);\n \tBREAK_TO_DEBUGGER();\n \treturn NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\tkfree(clk_src);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leaks in *clock_source_create() functions under drivers/gpu/drm/amd/display/dc in the Linux kernel before 5.3.8 allow attackers to cause a denial of service (memory consumption). This affects the dce112_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce112/dce112_resource.c, the dce100_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce100/dce100_resource.c, the dcn10_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dcn10/dcn10_resource.c, the dcn20_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dcn20/dcn20_resource.c, the dce120_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce120/dce120_resource.c, the dce110_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce110/dce110_resource.c, and the dce80_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce80/dce80_resource.c, aka CID-055e547478a1.",
        "id": 2174
    },
    {
        "cve_id": "CVE-2019-19056",
        "code_before_change": "static int mwifiex_pcie_alloc_cmdrsp_buf(struct mwifiex_adapter *adapter)\n{\n\tstruct pcie_service_card *card = adapter->card;\n\tstruct sk_buff *skb;\n\n\t/* Allocate memory for receiving command response data */\n\tskb = dev_alloc_skb(MWIFIEX_UPLD_SIZE);\n\tif (!skb) {\n\t\tmwifiex_dbg(adapter, ERROR,\n\t\t\t    \"Unable to allocate skb for command response data.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tskb_put(skb, MWIFIEX_UPLD_SIZE);\n\tif (mwifiex_map_pci_memory(adapter, skb, MWIFIEX_UPLD_SIZE,\n\t\t\t\t   PCI_DMA_FROMDEVICE))\n\t\treturn -1;\n\n\tcard->cmdrsp_buf = skb;\n\n\treturn 0;\n}",
        "code_after_change": "static int mwifiex_pcie_alloc_cmdrsp_buf(struct mwifiex_adapter *adapter)\n{\n\tstruct pcie_service_card *card = adapter->card;\n\tstruct sk_buff *skb;\n\n\t/* Allocate memory for receiving command response data */\n\tskb = dev_alloc_skb(MWIFIEX_UPLD_SIZE);\n\tif (!skb) {\n\t\tmwifiex_dbg(adapter, ERROR,\n\t\t\t    \"Unable to allocate skb for command response data.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tskb_put(skb, MWIFIEX_UPLD_SIZE);\n\tif (mwifiex_map_pci_memory(adapter, skb, MWIFIEX_UPLD_SIZE,\n\t\t\t\t   PCI_DMA_FROMDEVICE)) {\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\tcard->cmdrsp_buf = skb;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,8 +12,10 @@\n \t}\n \tskb_put(skb, MWIFIEX_UPLD_SIZE);\n \tif (mwifiex_map_pci_memory(adapter, skb, MWIFIEX_UPLD_SIZE,\n-\t\t\t\t   PCI_DMA_FROMDEVICE))\n+\t\t\t\t   PCI_DMA_FROMDEVICE)) {\n+\t\tkfree_skb(skb);\n \t\treturn -1;\n+\t}\n \n \tcard->cmdrsp_buf = skb;\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t   PCI_DMA_FROMDEVICE)) {",
                "\t\tkfree_skb(skb);",
                "\t}"
            ],
            "deleted": [
                "\t\t\t\t   PCI_DMA_FROMDEVICE))"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the mwifiex_pcie_alloc_cmdrsp_buf() function in drivers/net/wireless/marvell/mwifiex/pcie.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering mwifiex_map_pci_memory() failures, aka CID-db8fd2cde932.",
        "id": 2137
    },
    {
        "cve_id": "CVE-2022-1012",
        "code_before_change": "int inet6_hash_connect(struct inet_timewait_death_row *death_row,\n\t\t       struct sock *sk)\n{\n\tu32 port_offset = 0;\n\n\tif (!inet_sk(sk)->inet_num)\n\t\tport_offset = inet6_sk_port_offset(sk);\n\treturn __inet_hash_connect(death_row, sk, port_offset,\n\t\t\t\t   __inet6_check_established);\n}",
        "code_after_change": "int inet6_hash_connect(struct inet_timewait_death_row *death_row,\n\t\t       struct sock *sk)\n{\n\tu64 port_offset = 0;\n\n\tif (!inet_sk(sk)->inet_num)\n\t\tport_offset = inet6_sk_port_offset(sk);\n\treturn __inet_hash_connect(death_row, sk, port_offset,\n\t\t\t\t   __inet6_check_established);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n int inet6_hash_connect(struct inet_timewait_death_row *death_row,\n \t\t       struct sock *sk)\n {\n-\tu32 port_offset = 0;\n+\tu64 port_offset = 0;\n \n \tif (!inet_sk(sk)->inet_num)\n \t\tport_offset = inet6_sk_port_offset(sk);",
        "function_modified_lines": {
            "added": [
                "\tu64 port_offset = 0;"
            ],
            "deleted": [
                "\tu32 port_offset = 0;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak problem was found in the TCP source port generation algorithm in net/ipv4/tcp.c due to the small table perturb size. This flaw may allow an attacker to information leak and may cause a denial of service problem.",
        "id": 3236
    },
    {
        "cve_id": "CVE-2021-20265",
        "code_before_change": "static int unix_stream_read_generic(struct unix_stream_read_state *state)\n{\n\tstruct scm_cookie scm;\n\tstruct socket *sock = state->socket;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tint copied = 0;\n\tint flags = state->flags;\n\tint noblock = flags & MSG_DONTWAIT;\n\tbool check_creds = false;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\tsize_t size = state->size;\n\tunsigned int last_len;\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\tgoto out;\n\n\terr = -EOPNOTSUPP;\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, noblock);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\tmutex_lock(&u->readlock);\n\n\tif (flags & MSG_PEEK)\n\t\tskip = sk_peek_offset(sk, flags);\n\telse\n\t\tskip = 0;\n\n\tdo {\n\t\tint chunk;\n\t\tbool drop_skb;\n\t\tstruct sk_buff *skb, *last;\n\n\t\tunix_state_lock(sk);\n\t\tif (sock_flag(sk, SOCK_DEAD)) {\n\t\t\terr = -ECONNRESET;\n\t\t\tgoto unlock;\n\t\t}\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\n\t\tlast_len = last ? last->len : 0;\nagain:\n\t\tif (skb == NULL) {\n\t\t\tunix_sk(sk)->recursion_level = 0;\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\t\t\tmutex_unlock(&u->readlock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last,\n\t\t\t\t\t\t      last_len);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmutex_lock(&u->readlock);\n\t\t\tcontinue;\nunlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif (!unix_skb_scm_eq(skb, &scm))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tunix_set_secdata(&scm, skb);\n\t\t\tcheck_creds = true;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (state->msg && state->msg->msg_name) {\n\t\t\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr,\n\t\t\t\t\t state->msg->msg_name);\n\t\t\tunix_copy_addr(state->msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tskb_get(skb);\n\t\tchunk = state->recv_actor(skb, skip, chunk, state);\n\t\tdrop_skb = !unix_skb_len(skb);\n\t\t/* skb is only safe to use if !drop_skb */\n\t\tconsume_skb(skb);\n\t\tif (chunk < 0) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\tif (drop_skb) {\n\t\t\t/* the skb was touched by a concurrent reader;\n\t\t\t * we should not expect anything from this skb\n\t\t\t * anymore and assume it invalid - we can be\n\t\t\t * sure it was dropped from the socket queue\n\t\t\t *\n\t\t\t * let's report a short read\n\t\t\t */\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tunix_detach_fds(&scm, skb);\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (scm.fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tbreak;\n\n\t\t\tskip = 0;\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tunix_state_lock(sk);\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (skb)\n\t\t\t\tgoto again;\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->readlock);\n\tif (state->msg)\n\t\tscm_recv(sock, state->msg, &scm, flags);\n\telse\n\t\tscm_destroy(&scm);\nout:\n\treturn copied ? : err;\n}",
        "code_after_change": "static int unix_stream_read_generic(struct unix_stream_read_state *state)\n{\n\tstruct scm_cookie scm;\n\tstruct socket *sock = state->socket;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tint copied = 0;\n\tint flags = state->flags;\n\tint noblock = flags & MSG_DONTWAIT;\n\tbool check_creds = false;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\tsize_t size = state->size;\n\tunsigned int last_len;\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\tgoto out;\n\n\terr = -EOPNOTSUPP;\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, noblock);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\tmutex_lock(&u->readlock);\n\n\tif (flags & MSG_PEEK)\n\t\tskip = sk_peek_offset(sk, flags);\n\telse\n\t\tskip = 0;\n\n\tdo {\n\t\tint chunk;\n\t\tbool drop_skb;\n\t\tstruct sk_buff *skb, *last;\n\n\t\tunix_state_lock(sk);\n\t\tif (sock_flag(sk, SOCK_DEAD)) {\n\t\t\terr = -ECONNRESET;\n\t\t\tgoto unlock;\n\t\t}\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\n\t\tlast_len = last ? last->len : 0;\nagain:\n\t\tif (skb == NULL) {\n\t\t\tunix_sk(sk)->recursion_level = 0;\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\t\t\tmutex_unlock(&u->readlock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last,\n\t\t\t\t\t\t      last_len);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tscm_destroy(&scm);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmutex_lock(&u->readlock);\n\t\t\tcontinue;\nunlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif (!unix_skb_scm_eq(skb, &scm))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tunix_set_secdata(&scm, skb);\n\t\t\tcheck_creds = true;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (state->msg && state->msg->msg_name) {\n\t\t\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr,\n\t\t\t\t\t state->msg->msg_name);\n\t\t\tunix_copy_addr(state->msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tskb_get(skb);\n\t\tchunk = state->recv_actor(skb, skip, chunk, state);\n\t\tdrop_skb = !unix_skb_len(skb);\n\t\t/* skb is only safe to use if !drop_skb */\n\t\tconsume_skb(skb);\n\t\tif (chunk < 0) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\tif (drop_skb) {\n\t\t\t/* the skb was touched by a concurrent reader;\n\t\t\t * we should not expect anything from this skb\n\t\t\t * anymore and assume it invalid - we can be\n\t\t\t * sure it was dropped from the socket queue\n\t\t\t *\n\t\t\t * let's report a short read\n\t\t\t */\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tunix_detach_fds(&scm, skb);\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (scm.fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tbreak;\n\n\t\t\tskip = 0;\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tunix_state_lock(sk);\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (skb)\n\t\t\t\tgoto again;\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->readlock);\n\tif (state->msg)\n\t\tscm_recv(sock, state->msg, &scm, flags);\n\telse\n\t\tscm_destroy(&scm);\nout:\n\treturn copied ? : err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -77,6 +77,7 @@\n \n \t\t\tif (signal_pending(current)) {\n \t\t\t\terr = sock_intr_errno(timeo);\n+\t\t\t\tscm_destroy(&scm);\n \t\t\t\tgoto out;\n \t\t\t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tscm_destroy(&scm);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw was found in the way memory resources were freed in the unix_stream_recvmsg function in the Linux kernel when a signal was pending. This flaw allows an unprivileged local user to crash the system by exhausting available memory. The highest threat from this vulnerability is to system availability.",
        "id": 2866
    },
    {
        "cve_id": "CVE-2021-29649",
        "code_before_change": "static int finish(void)\n{\n\tint magic = BPF_PRELOAD_END;\n\tstruct pid *tgid;\n\tloff_t pos = 0;\n\tssize_t n;\n\n\t/* send the last magic to UMD. It will do a normal exit. */\n\tn = kernel_write(umd_ops.info.pipe_to_umh,\n\t\t\t &magic, sizeof(magic), &pos);\n\tif (n != sizeof(magic))\n\t\treturn -EPIPE;\n\ttgid = umd_ops.info.tgid;\n\twait_event(tgid->wait_pidfd, thread_group_exited(tgid));\n\tumd_ops.info.tgid = NULL;\n\treturn 0;\n}",
        "code_after_change": "static int finish(void)\n{\n\tint magic = BPF_PRELOAD_END;\n\tstruct pid *tgid;\n\tloff_t pos = 0;\n\tssize_t n;\n\n\t/* send the last magic to UMD. It will do a normal exit. */\n\tn = kernel_write(umd_ops.info.pipe_to_umh,\n\t\t\t &magic, sizeof(magic), &pos);\n\tif (n != sizeof(magic))\n\t\treturn -EPIPE;\n\n\ttgid = umd_ops.info.tgid;\n\tif (tgid) {\n\t\twait_event(tgid->wait_pidfd, thread_group_exited(tgid));\n\t\tumd_cleanup_helper(&umd_ops.info);\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,8 +10,11 @@\n \t\t\t &magic, sizeof(magic), &pos);\n \tif (n != sizeof(magic))\n \t\treturn -EPIPE;\n+\n \ttgid = umd_ops.info.tgid;\n-\twait_event(tgid->wait_pidfd, thread_group_exited(tgid));\n-\tumd_ops.info.tgid = NULL;\n+\tif (tgid) {\n+\t\twait_event(tgid->wait_pidfd, thread_group_exited(tgid));\n+\t\tumd_cleanup_helper(&umd_ops.info);\n+\t}\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (tgid) {",
                "\t\twait_event(tgid->wait_pidfd, thread_group_exited(tgid));",
                "\t\tumd_cleanup_helper(&umd_ops.info);",
                "\t}"
            ],
            "deleted": [
                "\twait_event(tgid->wait_pidfd, thread_group_exited(tgid));",
                "\tumd_ops.info.tgid = NULL;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.11.11. The user mode driver (UMD) has a copy_process() memory leak, related to a lack of cleanup steps in kernel/usermode_driver.c and kernel/bpf/preload/bpf_preload_kern.c, aka CID-f60a85cad677.",
        "id": 2952
    },
    {
        "cve_id": "CVE-2021-3744",
        "code_before_change": "static noinline_for_stack int\nccp_run_aes_gcm_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_aes_engine *aes = &cmd->u.aes;\n\tstruct ccp_dm_workarea key, ctx, final_wa, tag;\n\tstruct ccp_data src, dst;\n\tstruct ccp_data aad;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tunsigned int authsize;\n\tunsigned int jobid;\n\tunsigned int ilen;\n\tbool in_place = true; /* Default value */\n\t__be64 *final;\n\tint ret;\n\n\tstruct scatterlist *p_inp, sg_inp[2];\n\tstruct scatterlist *p_tag, sg_tag[2];\n\tstruct scatterlist *p_outp, sg_outp[2];\n\tstruct scatterlist *p_aad;\n\n\tif (!aes->iv)\n\t\treturn -EINVAL;\n\n\tif (!((aes->key_len == AES_KEYSIZE_128) ||\n\t\t(aes->key_len == AES_KEYSIZE_192) ||\n\t\t(aes->key_len == AES_KEYSIZE_256)))\n\t\treturn -EINVAL;\n\n\tif (!aes->key) /* Gotta have a key SGL */\n\t\treturn -EINVAL;\n\n\t/* Zero defaults to 16 bytes, the maximum size */\n\tauthsize = aes->authsize ? aes->authsize : AES_BLOCK_SIZE;\n\tswitch (authsize) {\n\tcase 16:\n\tcase 15:\n\tcase 14:\n\tcase 13:\n\tcase 12:\n\tcase 8:\n\tcase 4:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* First, decompose the source buffer into AAD & PT,\n\t * and the destination buffer into AAD, CT & tag, or\n\t * the input into CT & tag.\n\t * It is expected that the input and output SGs will\n\t * be valid, even if the AAD and input lengths are 0.\n\t */\n\tp_aad = aes->src;\n\tp_inp = scatterwalk_ffwd(sg_inp, aes->src, aes->aad_len);\n\tp_outp = scatterwalk_ffwd(sg_outp, aes->dst, aes->aad_len);\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\tilen = aes->src_len;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_outp, ilen);\n\t} else {\n\t\t/* Input length for decryption includes tag */\n\t\tilen = aes->src_len - authsize;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_inp, ilen);\n\t}\n\n\tjobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\n\t/* Copy the key to the LSB */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tdm_offset = CCP_SB_BYTES - aes->key_len;\n\tret = ccp_set_dm_area(&key, dm_offset, aes->key, 0, aes->key_len);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* Copy the context (IV) to the LSB.\n\t * There is an assumption here that the IV is 96 bits in length, plus\n\t * a nonce of 32 bits. If no IV is present, use a zeroed buffer.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tdm_offset = CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES - aes->iv_len;\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\top.init = 1;\n\tif (aes->aad_len > 0) {\n\t\t/* Step 1: Run a GHASH over the Additional Authenticated Data */\n\t\tret = ccp_init_data(&aad, cmd_q, p_aad, aes->aad_len,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\t\top.u.aes.action = CCP_AES_GHASHAAD;\n\n\t\twhile (aad.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&aad, NULL, &op, AES_BLOCK_SIZE, true);\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_aad;\n\t\t\t}\n\n\t\t\tccp_process_data(&aad, NULL, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\top.u.aes.mode = CCP_AES_MODE_GCTR;\n\top.u.aes.action = aes->action;\n\n\tif (ilen > 0) {\n\t\t/* Step 2: Run a GCTR over the plaintext */\n\t\tin_place = (sg_virt(p_inp) == sg_virt(p_outp)) ? true : false;\n\n\t\tret = ccp_init_data(&src, cmd_q, p_inp, ilen,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    in_place ? DMA_BIDIRECTIONAL\n\t\t\t\t\t     : DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\tif (in_place) {\n\t\t\tdst = src;\n\t\t} else {\n\t\t\tret = ccp_init_data(&dst, cmd_q, p_outp, ilen,\n\t\t\t\t\t    AES_BLOCK_SIZE, DMA_FROM_DEVICE);\n\t\t\tif (ret)\n\t\t\t\tgoto e_src;\n\t\t}\n\n\t\top.soc = 0;\n\t\top.eom = 0;\n\t\top.init = 1;\n\t\twhile (src.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&src, &dst, &op, AES_BLOCK_SIZE, true);\n\t\t\tif (!src.sg_wa.bytes_left) {\n\t\t\t\tunsigned int nbytes = ilen % AES_BLOCK_SIZE;\n\n\t\t\t\tif (nbytes) {\n\t\t\t\t\top.eom = 1;\n\t\t\t\t\top.u.aes.size = (nbytes * 8) - 1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_dst;\n\t\t\t}\n\n\t\t\tccp_process_data(&src, &dst, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\t/* Step 3: Update the IV portion of the context with the original IV */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_dst;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\t/* Step 4: Concatenate the lengths of the AAD and source, and\n\t * hash that 16 byte buffer.\n\t */\n\tret = ccp_init_dm_workarea(&final_wa, cmd_q, AES_BLOCK_SIZE,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_dst;\n\tfinal = (__be64 *)final_wa.address;\n\tfinal[0] = cpu_to_be64(aes->aad_len * 8);\n\tfinal[1] = cpu_to_be64(ilen * 8);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\top.u.aes.action = CCP_AES_GHASHFINAL;\n\top.src.type = CCP_MEMTYPE_SYSTEM;\n\top.src.u.dma.address = final_wa.dma.address;\n\top.src.u.dma.length = AES_BLOCK_SIZE;\n\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\top.dst.u.dma.address = final_wa.dma.address;\n\top.dst.u.dma.length = AES_BLOCK_SIZE;\n\top.eom = 1;\n\top.u.aes.size = 0;\n\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\tif (ret)\n\t\tgoto e_dst;\n\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\t/* Put the ciphered tag after the ciphertext. */\n\t\tccp_get_dm_area(&final_wa, 0, p_tag, 0, authsize);\n\t} else {\n\t\t/* Does this ciphered tag match the input? */\n\t\tret = ccp_init_dm_workarea(&tag, cmd_q, authsize,\n\t\t\t\t\t   DMA_BIDIRECTIONAL);\n\t\tif (ret)\n\t\t\tgoto e_tag;\n\t\tret = ccp_set_dm_area(&tag, 0, p_tag, 0, authsize);\n\t\tif (ret)\n\t\t\tgoto e_tag;\n\n\t\tret = crypto_memneq(tag.address, final_wa.address,\n\t\t\t\t    authsize) ? -EBADMSG : 0;\n\t\tccp_dm_free(&tag);\n\t}\n\ne_tag:\n\tccp_dm_free(&final_wa);\n\ne_dst:\n\tif (ilen > 0 && !in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tif (ilen > 0)\n\t\tccp_free_data(&src, cmd_q);\n\ne_aad:\n\tif (aes->aad_len)\n\t\tccp_free_data(&aad, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}",
        "code_after_change": "static noinline_for_stack int\nccp_run_aes_gcm_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_aes_engine *aes = &cmd->u.aes;\n\tstruct ccp_dm_workarea key, ctx, final_wa, tag;\n\tstruct ccp_data src, dst;\n\tstruct ccp_data aad;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tunsigned int authsize;\n\tunsigned int jobid;\n\tunsigned int ilen;\n\tbool in_place = true; /* Default value */\n\t__be64 *final;\n\tint ret;\n\n\tstruct scatterlist *p_inp, sg_inp[2];\n\tstruct scatterlist *p_tag, sg_tag[2];\n\tstruct scatterlist *p_outp, sg_outp[2];\n\tstruct scatterlist *p_aad;\n\n\tif (!aes->iv)\n\t\treturn -EINVAL;\n\n\tif (!((aes->key_len == AES_KEYSIZE_128) ||\n\t\t(aes->key_len == AES_KEYSIZE_192) ||\n\t\t(aes->key_len == AES_KEYSIZE_256)))\n\t\treturn -EINVAL;\n\n\tif (!aes->key) /* Gotta have a key SGL */\n\t\treturn -EINVAL;\n\n\t/* Zero defaults to 16 bytes, the maximum size */\n\tauthsize = aes->authsize ? aes->authsize : AES_BLOCK_SIZE;\n\tswitch (authsize) {\n\tcase 16:\n\tcase 15:\n\tcase 14:\n\tcase 13:\n\tcase 12:\n\tcase 8:\n\tcase 4:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* First, decompose the source buffer into AAD & PT,\n\t * and the destination buffer into AAD, CT & tag, or\n\t * the input into CT & tag.\n\t * It is expected that the input and output SGs will\n\t * be valid, even if the AAD and input lengths are 0.\n\t */\n\tp_aad = aes->src;\n\tp_inp = scatterwalk_ffwd(sg_inp, aes->src, aes->aad_len);\n\tp_outp = scatterwalk_ffwd(sg_outp, aes->dst, aes->aad_len);\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\tilen = aes->src_len;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_outp, ilen);\n\t} else {\n\t\t/* Input length for decryption includes tag */\n\t\tilen = aes->src_len - authsize;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_inp, ilen);\n\t}\n\n\tjobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\n\t/* Copy the key to the LSB */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tdm_offset = CCP_SB_BYTES - aes->key_len;\n\tret = ccp_set_dm_area(&key, dm_offset, aes->key, 0, aes->key_len);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* Copy the context (IV) to the LSB.\n\t * There is an assumption here that the IV is 96 bits in length, plus\n\t * a nonce of 32 bits. If no IV is present, use a zeroed buffer.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tdm_offset = CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES - aes->iv_len;\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\top.init = 1;\n\tif (aes->aad_len > 0) {\n\t\t/* Step 1: Run a GHASH over the Additional Authenticated Data */\n\t\tret = ccp_init_data(&aad, cmd_q, p_aad, aes->aad_len,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\t\top.u.aes.action = CCP_AES_GHASHAAD;\n\n\t\twhile (aad.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&aad, NULL, &op, AES_BLOCK_SIZE, true);\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_aad;\n\t\t\t}\n\n\t\t\tccp_process_data(&aad, NULL, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\top.u.aes.mode = CCP_AES_MODE_GCTR;\n\top.u.aes.action = aes->action;\n\n\tif (ilen > 0) {\n\t\t/* Step 2: Run a GCTR over the plaintext */\n\t\tin_place = (sg_virt(p_inp) == sg_virt(p_outp)) ? true : false;\n\n\t\tret = ccp_init_data(&src, cmd_q, p_inp, ilen,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    in_place ? DMA_BIDIRECTIONAL\n\t\t\t\t\t     : DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_aad;\n\n\t\tif (in_place) {\n\t\t\tdst = src;\n\t\t} else {\n\t\t\tret = ccp_init_data(&dst, cmd_q, p_outp, ilen,\n\t\t\t\t\t    AES_BLOCK_SIZE, DMA_FROM_DEVICE);\n\t\t\tif (ret)\n\t\t\t\tgoto e_src;\n\t\t}\n\n\t\top.soc = 0;\n\t\top.eom = 0;\n\t\top.init = 1;\n\t\twhile (src.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&src, &dst, &op, AES_BLOCK_SIZE, true);\n\t\t\tif (!src.sg_wa.bytes_left) {\n\t\t\t\tunsigned int nbytes = ilen % AES_BLOCK_SIZE;\n\n\t\t\t\tif (nbytes) {\n\t\t\t\t\top.eom = 1;\n\t\t\t\t\top.u.aes.size = (nbytes * 8) - 1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_dst;\n\t\t\t}\n\n\t\t\tccp_process_data(&src, &dst, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\t/* Step 3: Update the IV portion of the context with the original IV */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_dst;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\t/* Step 4: Concatenate the lengths of the AAD and source, and\n\t * hash that 16 byte buffer.\n\t */\n\tret = ccp_init_dm_workarea(&final_wa, cmd_q, AES_BLOCK_SIZE,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_dst;\n\tfinal = (__be64 *)final_wa.address;\n\tfinal[0] = cpu_to_be64(aes->aad_len * 8);\n\tfinal[1] = cpu_to_be64(ilen * 8);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\top.u.aes.action = CCP_AES_GHASHFINAL;\n\top.src.type = CCP_MEMTYPE_SYSTEM;\n\top.src.u.dma.address = final_wa.dma.address;\n\top.src.u.dma.length = AES_BLOCK_SIZE;\n\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\top.dst.u.dma.address = final_wa.dma.address;\n\top.dst.u.dma.length = AES_BLOCK_SIZE;\n\top.eom = 1;\n\top.u.aes.size = 0;\n\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\tif (ret)\n\t\tgoto e_final_wa;\n\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\t/* Put the ciphered tag after the ciphertext. */\n\t\tccp_get_dm_area(&final_wa, 0, p_tag, 0, authsize);\n\t} else {\n\t\t/* Does this ciphered tag match the input? */\n\t\tret = ccp_init_dm_workarea(&tag, cmd_q, authsize,\n\t\t\t\t\t   DMA_BIDIRECTIONAL);\n\t\tif (ret)\n\t\t\tgoto e_final_wa;\n\t\tret = ccp_set_dm_area(&tag, 0, p_tag, 0, authsize);\n\t\tif (ret) {\n\t\t\tccp_dm_free(&tag);\n\t\t\tgoto e_final_wa;\n\t\t}\n\n\t\tret = crypto_memneq(tag.address, final_wa.address,\n\t\t\t\t    authsize) ? -EBADMSG : 0;\n\t\tccp_dm_free(&tag);\n\t}\n\ne_final_wa:\n\tccp_dm_free(&final_wa);\n\ne_dst:\n\tif (ilen > 0 && !in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tif (ilen > 0)\n\t\tccp_free_data(&src, cmd_q);\n\ne_aad:\n\tif (aes->aad_len)\n\t\tccp_free_data(&aad, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -151,7 +151,7 @@\n \t\t\t\t    in_place ? DMA_BIDIRECTIONAL\n \t\t\t\t\t     : DMA_TO_DEVICE);\n \t\tif (ret)\n-\t\t\tgoto e_ctx;\n+\t\t\tgoto e_aad;\n \n \t\tif (in_place) {\n \t\t\tdst = src;\n@@ -236,7 +236,7 @@\n \top.u.aes.size = 0;\n \tret = cmd_q->ccp->vdata->perform->aes(&op);\n \tif (ret)\n-\t\tgoto e_dst;\n+\t\tgoto e_final_wa;\n \n \tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n \t\t/* Put the ciphered tag after the ciphertext. */\n@@ -246,17 +246,19 @@\n \t\tret = ccp_init_dm_workarea(&tag, cmd_q, authsize,\n \t\t\t\t\t   DMA_BIDIRECTIONAL);\n \t\tif (ret)\n-\t\t\tgoto e_tag;\n+\t\t\tgoto e_final_wa;\n \t\tret = ccp_set_dm_area(&tag, 0, p_tag, 0, authsize);\n-\t\tif (ret)\n-\t\t\tgoto e_tag;\n+\t\tif (ret) {\n+\t\t\tccp_dm_free(&tag);\n+\t\t\tgoto e_final_wa;\n+\t\t}\n \n \t\tret = crypto_memneq(tag.address, final_wa.address,\n \t\t\t\t    authsize) ? -EBADMSG : 0;\n \t\tccp_dm_free(&tag);\n \t}\n \n-e_tag:\n+e_final_wa:\n \tccp_dm_free(&final_wa);\n \n e_dst:",
        "function_modified_lines": {
            "added": [
                "\t\t\tgoto e_aad;",
                "\t\tgoto e_final_wa;",
                "\t\t\tgoto e_final_wa;",
                "\t\tif (ret) {",
                "\t\t\tccp_dm_free(&tag);",
                "\t\t\tgoto e_final_wa;",
                "\t\t}",
                "e_final_wa:"
            ],
            "deleted": [
                "\t\t\tgoto e_ctx;",
                "\t\tgoto e_dst;",
                "\t\t\tgoto e_tag;",
                "\t\tif (ret)",
                "\t\t\tgoto e_tag;",
                "e_tag:"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak flaw was found in the Linux kernel in the ccp_run_aes_gcm_cmd() function in drivers/crypto/ccp/ccp-ops.c, which allows attackers to cause a denial of service (memory consumption). This vulnerability is similar with the older CVE-2019-18808.",
        "id": 3052
    },
    {
        "cve_id": "CVE-2022-3629",
        "code_before_change": "static int vsock_connect(struct socket *sock, struct sockaddr *addr,\n\t\t\t int addr_len, int flags)\n{\n\tint err;\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tconst struct vsock_transport *transport;\n\tstruct sockaddr_vm *remote_addr;\n\tlong timeout;\n\tDEFINE_WAIT(wait);\n\n\terr = 0;\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\n\tlock_sock(sk);\n\n\t/* XXX AF_UNSPEC should make us disconnect like AF_INET. */\n\tswitch (sock->state) {\n\tcase SS_CONNECTED:\n\t\terr = -EISCONN;\n\t\tgoto out;\n\tcase SS_DISCONNECTING:\n\t\terr = -EINVAL;\n\t\tgoto out;\n\tcase SS_CONNECTING:\n\t\t/* This continues on so we can move sock into the SS_CONNECTED\n\t\t * state once the connection has completed (at which point err\n\t\t * will be set to zero also).  Otherwise, we will either wait\n\t\t * for the connection or return -EALREADY should this be a\n\t\t * non-blocking call.\n\t\t */\n\t\terr = -EALREADY;\n\t\tif (flags & O_NONBLOCK)\n\t\t\tgoto out;\n\t\tbreak;\n\tdefault:\n\t\tif ((sk->sk_state == TCP_LISTEN) ||\n\t\t    vsock_addr_cast(addr, addr_len, &remote_addr) != 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* Set the remote address that we are connecting to. */\n\t\tmemcpy(&vsk->remote_addr, remote_addr,\n\t\t       sizeof(vsk->remote_addr));\n\n\t\terr = vsock_assign_transport(vsk, NULL);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\ttransport = vsk->transport;\n\n\t\t/* The hypervisor and well-known contexts do not have socket\n\t\t * endpoints.\n\t\t */\n\t\tif (!transport ||\n\t\t    !transport->stream_allow(remote_addr->svm_cid,\n\t\t\t\t\t     remote_addr->svm_port)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = vsock_auto_bind(vsk);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tsk->sk_state = TCP_SYN_SENT;\n\n\t\terr = transport->connect(vsk);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\t/* Mark sock as connecting and set the error code to in\n\t\t * progress in case this is a non-blocking connect.\n\t\t */\n\t\tsock->state = SS_CONNECTING;\n\t\terr = -EINPROGRESS;\n\t}\n\n\t/* The receive path will handle all communication until we are able to\n\t * enter the connected state.  Here we wait for the connection to be\n\t * completed or a notification of an error.\n\t */\n\ttimeout = vsk->connect_timeout;\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\twhile (sk->sk_state != TCP_ESTABLISHED && sk->sk_err == 0) {\n\t\tif (flags & O_NONBLOCK) {\n\t\t\t/* If we're not going to block, we schedule a timeout\n\t\t\t * function to generate a timeout on the connection\n\t\t\t * attempt, in case the peer doesn't respond in a\n\t\t\t * timely manner. We hold on to the socket until the\n\t\t\t * timeout fires.\n\t\t\t */\n\t\t\tsock_hold(sk);\n\t\t\tschedule_delayed_work(&vsk->connect_work, timeout);\n\n\t\t\t/* Skip ahead to preserve error code set above. */\n\t\t\tgoto out_wait;\n\t\t}\n\n\t\trelease_sock(sk);\n\t\ttimeout = schedule_timeout(timeout);\n\t\tlock_sock(sk);\n\n\t\tif (signal_pending(current)) {\n\t\t\terr = sock_intr_errno(timeout);\n\t\t\tsk->sk_state = sk->sk_state == TCP_ESTABLISHED ? TCP_CLOSING : TCP_CLOSE;\n\t\t\tsock->state = SS_UNCONNECTED;\n\t\t\tvsock_transport_cancel_pkt(vsk);\n\t\t\tvsock_remove_connected(vsk);\n\t\t\tgoto out_wait;\n\t\t} else if (timeout == 0) {\n\t\t\terr = -ETIMEDOUT;\n\t\t\tsk->sk_state = TCP_CLOSE;\n\t\t\tsock->state = SS_UNCONNECTED;\n\t\t\tvsock_transport_cancel_pkt(vsk);\n\t\t\tgoto out_wait;\n\t\t}\n\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t}\n\n\tif (sk->sk_err) {\n\t\terr = -sk->sk_err;\n\t\tsk->sk_state = TCP_CLOSE;\n\t\tsock->state = SS_UNCONNECTED;\n\t} else {\n\t\terr = 0;\n\t}\n\nout_wait:\n\tfinish_wait(sk_sleep(sk), &wait);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "code_after_change": "static int vsock_connect(struct socket *sock, struct sockaddr *addr,\n\t\t\t int addr_len, int flags)\n{\n\tint err;\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tconst struct vsock_transport *transport;\n\tstruct sockaddr_vm *remote_addr;\n\tlong timeout;\n\tDEFINE_WAIT(wait);\n\n\terr = 0;\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\n\tlock_sock(sk);\n\n\t/* XXX AF_UNSPEC should make us disconnect like AF_INET. */\n\tswitch (sock->state) {\n\tcase SS_CONNECTED:\n\t\terr = -EISCONN;\n\t\tgoto out;\n\tcase SS_DISCONNECTING:\n\t\terr = -EINVAL;\n\t\tgoto out;\n\tcase SS_CONNECTING:\n\t\t/* This continues on so we can move sock into the SS_CONNECTED\n\t\t * state once the connection has completed (at which point err\n\t\t * will be set to zero also).  Otherwise, we will either wait\n\t\t * for the connection or return -EALREADY should this be a\n\t\t * non-blocking call.\n\t\t */\n\t\terr = -EALREADY;\n\t\tif (flags & O_NONBLOCK)\n\t\t\tgoto out;\n\t\tbreak;\n\tdefault:\n\t\tif ((sk->sk_state == TCP_LISTEN) ||\n\t\t    vsock_addr_cast(addr, addr_len, &remote_addr) != 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* Set the remote address that we are connecting to. */\n\t\tmemcpy(&vsk->remote_addr, remote_addr,\n\t\t       sizeof(vsk->remote_addr));\n\n\t\terr = vsock_assign_transport(vsk, NULL);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\ttransport = vsk->transport;\n\n\t\t/* The hypervisor and well-known contexts do not have socket\n\t\t * endpoints.\n\t\t */\n\t\tif (!transport ||\n\t\t    !transport->stream_allow(remote_addr->svm_cid,\n\t\t\t\t\t     remote_addr->svm_port)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = vsock_auto_bind(vsk);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tsk->sk_state = TCP_SYN_SENT;\n\n\t\terr = transport->connect(vsk);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\t/* Mark sock as connecting and set the error code to in\n\t\t * progress in case this is a non-blocking connect.\n\t\t */\n\t\tsock->state = SS_CONNECTING;\n\t\terr = -EINPROGRESS;\n\t}\n\n\t/* The receive path will handle all communication until we are able to\n\t * enter the connected state.  Here we wait for the connection to be\n\t * completed or a notification of an error.\n\t */\n\ttimeout = vsk->connect_timeout;\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\twhile (sk->sk_state != TCP_ESTABLISHED && sk->sk_err == 0) {\n\t\tif (flags & O_NONBLOCK) {\n\t\t\t/* If we're not going to block, we schedule a timeout\n\t\t\t * function to generate a timeout on the connection\n\t\t\t * attempt, in case the peer doesn't respond in a\n\t\t\t * timely manner. We hold on to the socket until the\n\t\t\t * timeout fires.\n\t\t\t */\n\t\t\tsock_hold(sk);\n\n\t\t\t/* If the timeout function is already scheduled,\n\t\t\t * reschedule it, then ungrab the socket refcount to\n\t\t\t * keep it balanced.\n\t\t\t */\n\t\t\tif (mod_delayed_work(system_wq, &vsk->connect_work,\n\t\t\t\t\t     timeout))\n\t\t\t\tsock_put(sk);\n\n\t\t\t/* Skip ahead to preserve error code set above. */\n\t\t\tgoto out_wait;\n\t\t}\n\n\t\trelease_sock(sk);\n\t\ttimeout = schedule_timeout(timeout);\n\t\tlock_sock(sk);\n\n\t\tif (signal_pending(current)) {\n\t\t\terr = sock_intr_errno(timeout);\n\t\t\tsk->sk_state = sk->sk_state == TCP_ESTABLISHED ? TCP_CLOSING : TCP_CLOSE;\n\t\t\tsock->state = SS_UNCONNECTED;\n\t\t\tvsock_transport_cancel_pkt(vsk);\n\t\t\tvsock_remove_connected(vsk);\n\t\t\tgoto out_wait;\n\t\t} else if (timeout == 0) {\n\t\t\terr = -ETIMEDOUT;\n\t\t\tsk->sk_state = TCP_CLOSE;\n\t\t\tsock->state = SS_UNCONNECTED;\n\t\t\tvsock_transport_cancel_pkt(vsk);\n\t\t\tgoto out_wait;\n\t\t}\n\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t}\n\n\tif (sk->sk_err) {\n\t\terr = -sk->sk_err;\n\t\tsk->sk_state = TCP_CLOSE;\n\t\tsock->state = SS_UNCONNECTED;\n\t} else {\n\t\terr = 0;\n\t}\n\nout_wait:\n\tfinish_wait(sk_sleep(sk), &wait);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -94,7 +94,14 @@\n \t\t\t * timeout fires.\n \t\t\t */\n \t\t\tsock_hold(sk);\n-\t\t\tschedule_delayed_work(&vsk->connect_work, timeout);\n+\n+\t\t\t/* If the timeout function is already scheduled,\n+\t\t\t * reschedule it, then ungrab the socket refcount to\n+\t\t\t * keep it balanced.\n+\t\t\t */\n+\t\t\tif (mod_delayed_work(system_wq, &vsk->connect_work,\n+\t\t\t\t\t     timeout))\n+\t\t\t\tsock_put(sk);\n \n \t\t\t/* Skip ahead to preserve error code set above. */\n \t\t\tgoto out_wait;",
        "function_modified_lines": {
            "added": [
                "",
                "\t\t\t/* If the timeout function is already scheduled,",
                "\t\t\t * reschedule it, then ungrab the socket refcount to",
                "\t\t\t * keep it balanced.",
                "\t\t\t */",
                "\t\t\tif (mod_delayed_work(system_wq, &vsk->connect_work,",
                "\t\t\t\t\t     timeout))",
                "\t\t\t\tsock_put(sk);"
            ],
            "deleted": [
                "\t\t\tschedule_delayed_work(&vsk->connect_work, timeout);"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been declared as problematic. This vulnerability affects the function vsock_connect of the file net/vmw_vsock/af_vsock.c. The manipulation leads to memory leak. The complexity of an attack is rather high. The exploitation appears to be difficult. It is recommended to apply a patch to fix this issue. VDB-211930 is the identifier assigned to this vulnerability.",
        "id": 3664
    },
    {
        "cve_id": "CVE-2019-18811",
        "code_before_change": "static int sof_set_get_large_ctrl_data(struct snd_sof_dev *sdev,\n\t\t\t\t       struct sof_ipc_ctrl_data *cdata,\n\t\t\t\t       struct sof_ipc_ctrl_data_params *sparams,\n\t\t\t\t       bool send)\n{\n\tstruct sof_ipc_ctrl_data *partdata;\n\tsize_t send_bytes;\n\tsize_t offset = 0;\n\tsize_t msg_bytes;\n\tsize_t pl_size;\n\tint err;\n\tint i;\n\n\t/* allocate max ipc size because we have at least one */\n\tpartdata = kzalloc(SOF_IPC_MSG_MAX_SIZE, GFP_KERNEL);\n\tif (!partdata)\n\t\treturn -ENOMEM;\n\n\tif (send)\n\t\terr = sof_get_ctrl_copy_params(cdata->type, cdata, partdata,\n\t\t\t\t\t       sparams);\n\telse\n\t\terr = sof_get_ctrl_copy_params(cdata->type, partdata, cdata,\n\t\t\t\t\t       sparams);\n\tif (err < 0)\n\t\treturn err;\n\n\tmsg_bytes = sparams->msg_bytes;\n\tpl_size = sparams->pl_size;\n\n\t/* copy the header data */\n\tmemcpy(partdata, cdata, sparams->hdr_bytes);\n\n\t/* Serialise IPC TX */\n\tmutex_lock(&sdev->ipc->tx_mutex);\n\n\t/* copy the payload data in a loop */\n\tfor (i = 0; i < sparams->num_msg; i++) {\n\t\tsend_bytes = min(msg_bytes, pl_size);\n\t\tpartdata->num_elems = send_bytes;\n\t\tpartdata->rhdr.hdr.size = sparams->hdr_bytes + send_bytes;\n\t\tpartdata->msg_index = i;\n\t\tmsg_bytes -= send_bytes;\n\t\tpartdata->elems_remaining = msg_bytes;\n\n\t\tif (send)\n\t\t\tmemcpy(sparams->dst, sparams->src + offset, send_bytes);\n\n\t\terr = sof_ipc_tx_message_unlocked(sdev->ipc,\n\t\t\t\t\t\t  partdata->rhdr.hdr.cmd,\n\t\t\t\t\t\t  partdata,\n\t\t\t\t\t\t  partdata->rhdr.hdr.size,\n\t\t\t\t\t\t  partdata,\n\t\t\t\t\t\t  partdata->rhdr.hdr.size);\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t\tif (!send)\n\t\t\tmemcpy(sparams->dst + offset, sparams->src, send_bytes);\n\n\t\toffset += pl_size;\n\t}\n\n\tmutex_unlock(&sdev->ipc->tx_mutex);\n\n\tkfree(partdata);\n\treturn err;\n}",
        "code_after_change": "static int sof_set_get_large_ctrl_data(struct snd_sof_dev *sdev,\n\t\t\t\t       struct sof_ipc_ctrl_data *cdata,\n\t\t\t\t       struct sof_ipc_ctrl_data_params *sparams,\n\t\t\t\t       bool send)\n{\n\tstruct sof_ipc_ctrl_data *partdata;\n\tsize_t send_bytes;\n\tsize_t offset = 0;\n\tsize_t msg_bytes;\n\tsize_t pl_size;\n\tint err;\n\tint i;\n\n\t/* allocate max ipc size because we have at least one */\n\tpartdata = kzalloc(SOF_IPC_MSG_MAX_SIZE, GFP_KERNEL);\n\tif (!partdata)\n\t\treturn -ENOMEM;\n\n\tif (send)\n\t\terr = sof_get_ctrl_copy_params(cdata->type, cdata, partdata,\n\t\t\t\t\t       sparams);\n\telse\n\t\terr = sof_get_ctrl_copy_params(cdata->type, partdata, cdata,\n\t\t\t\t\t       sparams);\n\tif (err < 0) {\n\t\tkfree(partdata);\n\t\treturn err;\n\t}\n\n\tmsg_bytes = sparams->msg_bytes;\n\tpl_size = sparams->pl_size;\n\n\t/* copy the header data */\n\tmemcpy(partdata, cdata, sparams->hdr_bytes);\n\n\t/* Serialise IPC TX */\n\tmutex_lock(&sdev->ipc->tx_mutex);\n\n\t/* copy the payload data in a loop */\n\tfor (i = 0; i < sparams->num_msg; i++) {\n\t\tsend_bytes = min(msg_bytes, pl_size);\n\t\tpartdata->num_elems = send_bytes;\n\t\tpartdata->rhdr.hdr.size = sparams->hdr_bytes + send_bytes;\n\t\tpartdata->msg_index = i;\n\t\tmsg_bytes -= send_bytes;\n\t\tpartdata->elems_remaining = msg_bytes;\n\n\t\tif (send)\n\t\t\tmemcpy(sparams->dst, sparams->src + offset, send_bytes);\n\n\t\terr = sof_ipc_tx_message_unlocked(sdev->ipc,\n\t\t\t\t\t\t  partdata->rhdr.hdr.cmd,\n\t\t\t\t\t\t  partdata,\n\t\t\t\t\t\t  partdata->rhdr.hdr.size,\n\t\t\t\t\t\t  partdata,\n\t\t\t\t\t\t  partdata->rhdr.hdr.size);\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t\tif (!send)\n\t\t\tmemcpy(sparams->dst + offset, sparams->src, send_bytes);\n\n\t\toffset += pl_size;\n\t}\n\n\tmutex_unlock(&sdev->ipc->tx_mutex);\n\n\tkfree(partdata);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,8 +22,10 @@\n \telse\n \t\terr = sof_get_ctrl_copy_params(cdata->type, partdata, cdata,\n \t\t\t\t\t       sparams);\n-\tif (err < 0)\n+\tif (err < 0) {\n+\t\tkfree(partdata);\n \t\treturn err;\n+\t}\n \n \tmsg_bytes = sparams->msg_bytes;\n \tpl_size = sparams->pl_size;",
        "function_modified_lines": {
            "added": [
                "\tif (err < 0) {",
                "\t\tkfree(partdata);",
                "\t}"
            ],
            "deleted": [
                "\tif (err < 0)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the sof_set_get_large_ctrl_data() function in sound/soc/sof/ipc.c in the Linux kernel through 5.3.9 allows attackers to cause a denial of service (memory consumption) by triggering sof_get_ctrl_copy_params() failures, aka CID-45c1380358b1.",
        "id": 2103
    },
    {
        "cve_id": "CVE-2019-16995",
        "code_before_change": "int hsr_dev_finalize(struct net_device *hsr_dev, struct net_device *slave[2],\n\t\t     unsigned char multicast_spec, u8 protocol_version)\n{\n\tstruct hsr_priv *hsr;\n\tstruct hsr_port *port;\n\tint res;\n\n\thsr = netdev_priv(hsr_dev);\n\tINIT_LIST_HEAD(&hsr->ports);\n\tINIT_LIST_HEAD(&hsr->node_db);\n\tINIT_LIST_HEAD(&hsr->self_node_db);\n\n\tether_addr_copy(hsr_dev->dev_addr, slave[0]->dev_addr);\n\n\t/* Make sure we recognize frames from ourselves in hsr_rcv() */\n\tres = hsr_create_self_node(&hsr->self_node_db, hsr_dev->dev_addr,\n\t\t\t\t   slave[1]->dev_addr);\n\tif (res < 0)\n\t\treturn res;\n\n\tspin_lock_init(&hsr->seqnr_lock);\n\t/* Overflow soon to find bugs easier: */\n\thsr->sequence_nr = HSR_SEQNR_START;\n\thsr->sup_sequence_nr = HSR_SUP_SEQNR_START;\n\n\ttimer_setup(&hsr->announce_timer, hsr_announce, 0);\n\ttimer_setup(&hsr->prune_timer, hsr_prune_nodes, 0);\n\n\tether_addr_copy(hsr->sup_multicast_addr, def_multicast_addr);\n\thsr->sup_multicast_addr[ETH_ALEN - 1] = multicast_spec;\n\n\thsr->protVersion = protocol_version;\n\n\t/* FIXME: should I modify the value of these?\n\t *\n\t * - hsr_dev->flags - i.e.\n\t *\t\t\tIFF_MASTER/SLAVE?\n\t * - hsr_dev->priv_flags - i.e.\n\t *\t\t\tIFF_EBRIDGE?\n\t *\t\t\tIFF_TX_SKB_SHARING?\n\t *\t\t\tIFF_HSR_MASTER/SLAVE?\n\t */\n\n\t/* Make sure the 1st call to netif_carrier_on() gets through */\n\tnetif_carrier_off(hsr_dev);\n\n\tres = hsr_add_port(hsr, hsr_dev, HSR_PT_MASTER);\n\tif (res)\n\t\treturn res;\n\n\tres = register_netdevice(hsr_dev);\n\tif (res)\n\t\tgoto fail;\n\n\tres = hsr_add_port(hsr, slave[0], HSR_PT_SLAVE_A);\n\tif (res)\n\t\tgoto fail;\n\tres = hsr_add_port(hsr, slave[1], HSR_PT_SLAVE_B);\n\tif (res)\n\t\tgoto fail;\n\n\tmod_timer(&hsr->prune_timer, jiffies + msecs_to_jiffies(PRUNE_PERIOD));\n\n\treturn 0;\n\nfail:\n\thsr_for_each_port(hsr, port)\n\t\thsr_del_port(port);\n\n\treturn res;\n}",
        "code_after_change": "int hsr_dev_finalize(struct net_device *hsr_dev, struct net_device *slave[2],\n\t\t     unsigned char multicast_spec, u8 protocol_version)\n{\n\tstruct hsr_priv *hsr;\n\tstruct hsr_port *port;\n\tint res;\n\n\thsr = netdev_priv(hsr_dev);\n\tINIT_LIST_HEAD(&hsr->ports);\n\tINIT_LIST_HEAD(&hsr->node_db);\n\tINIT_LIST_HEAD(&hsr->self_node_db);\n\n\tether_addr_copy(hsr_dev->dev_addr, slave[0]->dev_addr);\n\n\t/* Make sure we recognize frames from ourselves in hsr_rcv() */\n\tres = hsr_create_self_node(&hsr->self_node_db, hsr_dev->dev_addr,\n\t\t\t\t   slave[1]->dev_addr);\n\tif (res < 0)\n\t\treturn res;\n\n\tspin_lock_init(&hsr->seqnr_lock);\n\t/* Overflow soon to find bugs easier: */\n\thsr->sequence_nr = HSR_SEQNR_START;\n\thsr->sup_sequence_nr = HSR_SUP_SEQNR_START;\n\n\ttimer_setup(&hsr->announce_timer, hsr_announce, 0);\n\ttimer_setup(&hsr->prune_timer, hsr_prune_nodes, 0);\n\n\tether_addr_copy(hsr->sup_multicast_addr, def_multicast_addr);\n\thsr->sup_multicast_addr[ETH_ALEN - 1] = multicast_spec;\n\n\thsr->protVersion = protocol_version;\n\n\t/* FIXME: should I modify the value of these?\n\t *\n\t * - hsr_dev->flags - i.e.\n\t *\t\t\tIFF_MASTER/SLAVE?\n\t * - hsr_dev->priv_flags - i.e.\n\t *\t\t\tIFF_EBRIDGE?\n\t *\t\t\tIFF_TX_SKB_SHARING?\n\t *\t\t\tIFF_HSR_MASTER/SLAVE?\n\t */\n\n\t/* Make sure the 1st call to netif_carrier_on() gets through */\n\tnetif_carrier_off(hsr_dev);\n\n\tres = hsr_add_port(hsr, hsr_dev, HSR_PT_MASTER);\n\tif (res)\n\t\tgoto err_add_port;\n\n\tres = register_netdevice(hsr_dev);\n\tif (res)\n\t\tgoto fail;\n\n\tres = hsr_add_port(hsr, slave[0], HSR_PT_SLAVE_A);\n\tif (res)\n\t\tgoto fail;\n\tres = hsr_add_port(hsr, slave[1], HSR_PT_SLAVE_B);\n\tif (res)\n\t\tgoto fail;\n\n\tmod_timer(&hsr->prune_timer, jiffies + msecs_to_jiffies(PRUNE_PERIOD));\n\n\treturn 0;\n\nfail:\n\thsr_for_each_port(hsr, port)\n\t\thsr_del_port(port);\nerr_add_port:\n\thsr_del_node(&hsr->self_node_db);\n\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -46,7 +46,7 @@\n \n \tres = hsr_add_port(hsr, hsr_dev, HSR_PT_MASTER);\n \tif (res)\n-\t\treturn res;\n+\t\tgoto err_add_port;\n \n \tres = register_netdevice(hsr_dev);\n \tif (res)\n@@ -66,6 +66,8 @@\n fail:\n \thsr_for_each_port(hsr, port)\n \t\thsr_del_port(port);\n+err_add_port:\n+\thsr_del_node(&hsr->self_node_db);\n \n \treturn res;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tgoto err_add_port;",
                "err_add_port:",
                "\thsr_del_node(&hsr->self_node_db);"
            ],
            "deleted": [
                "\t\treturn res;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "In the Linux kernel before 5.0.3, a memory leak exits in hsr_dev_finalize() in net/hsr/hsr_device.c if hsr_add_port fails to add a port, which may cause denial of service, aka CID-6caabe7f197d.",
        "id": 2059
    },
    {
        "cve_id": "CVE-2023-0597",
        "code_before_change": "noinstr struct cpu_entry_area *get_cpu_entry_area(int cpu)\n{\n\tunsigned long va = CPU_ENTRY_AREA_PER_CPU + cpu * CPU_ENTRY_AREA_SIZE;\n\tBUILD_BUG_ON(sizeof(struct cpu_entry_area) % PAGE_SIZE != 0);\n\n\treturn (struct cpu_entry_area *) va;\n}",
        "code_after_change": "noinstr struct cpu_entry_area *get_cpu_entry_area(int cpu)\n{\n\tunsigned long va = CPU_ENTRY_AREA_PER_CPU + cea_offset(cpu) * CPU_ENTRY_AREA_SIZE;\n\tBUILD_BUG_ON(sizeof(struct cpu_entry_area) % PAGE_SIZE != 0);\n\n\treturn (struct cpu_entry_area *) va;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n noinstr struct cpu_entry_area *get_cpu_entry_area(int cpu)\n {\n-\tunsigned long va = CPU_ENTRY_AREA_PER_CPU + cpu * CPU_ENTRY_AREA_SIZE;\n+\tunsigned long va = CPU_ENTRY_AREA_PER_CPU + cea_offset(cpu) * CPU_ENTRY_AREA_SIZE;\n \tBUILD_BUG_ON(sizeof(struct cpu_entry_area) % PAGE_SIZE != 0);\n \n \treturn (struct cpu_entry_area *) va;",
        "function_modified_lines": {
            "added": [
                "\tunsigned long va = CPU_ENTRY_AREA_PER_CPU + cea_offset(cpu) * CPU_ENTRY_AREA_SIZE;"
            ],
            "deleted": [
                "\tunsigned long va = CPU_ENTRY_AREA_PER_CPU + cpu * CPU_ENTRY_AREA_SIZE;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw possibility of memory leak in the Linux kernel cpu_entry_area mapping of X86 CPU data to memory was found in the way user can guess location of exception stack(s) or other important data. A local user could use this flaw to get access to some important data with expected location in memory.",
        "id": 3834
    },
    {
        "cve_id": "CVE-2019-19049",
        "code_before_change": "static int __init unittest_data_add(void)\n{\n\tvoid *unittest_data;\n\tstruct device_node *unittest_data_node, *np;\n\t/*\n\t * __dtb_testcases_begin[] and __dtb_testcases_end[] are magically\n\t * created by cmd_dt_S_dtb in scripts/Makefile.lib\n\t */\n\textern uint8_t __dtb_testcases_begin[];\n\textern uint8_t __dtb_testcases_end[];\n\tconst int size = __dtb_testcases_end - __dtb_testcases_begin;\n\tint rc;\n\n\tif (!size) {\n\t\tpr_warn(\"%s: No testcase data to attach; not running tests\\n\",\n\t\t\t__func__);\n\t\treturn -ENODATA;\n\t}\n\n\t/* creating copy */\n\tunittest_data = kmemdup(__dtb_testcases_begin, size, GFP_KERNEL);\n\tif (!unittest_data)\n\t\treturn -ENOMEM;\n\n\tof_fdt_unflatten_tree(unittest_data, NULL, &unittest_data_node);\n\tif (!unittest_data_node) {\n\t\tpr_warn(\"%s: No tree to attach; not running tests\\n\", __func__);\n\t\treturn -ENODATA;\n\t}\n\n\t/*\n\t * This lock normally encloses of_resolve_phandles()\n\t */\n\tof_overlay_mutex_lock();\n\n\trc = of_resolve_phandles(unittest_data_node);\n\tif (rc) {\n\t\tpr_err(\"%s: Failed to resolve phandles (rc=%i)\\n\", __func__, rc);\n\t\tof_overlay_mutex_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\tif (!of_root) {\n\t\tof_root = unittest_data_node;\n\t\tfor_each_of_allnodes(np)\n\t\t\t__of_attach_node_sysfs(np);\n\t\tof_aliases = of_find_node_by_path(\"/aliases\");\n\t\tof_chosen = of_find_node_by_path(\"/chosen\");\n\t\tof_overlay_mutex_unlock();\n\t\treturn 0;\n\t}\n\n\t/* attach the sub-tree to live tree */\n\tnp = unittest_data_node->child;\n\twhile (np) {\n\t\tstruct device_node *next = np->sibling;\n\n\t\tnp->parent = of_root;\n\t\tattach_node_and_children(np);\n\t\tnp = next;\n\t}\n\n\tof_overlay_mutex_unlock();\n\n\treturn 0;\n}",
        "code_after_change": "static int __init unittest_data_add(void)\n{\n\tvoid *unittest_data;\n\tstruct device_node *unittest_data_node, *np;\n\t/*\n\t * __dtb_testcases_begin[] and __dtb_testcases_end[] are magically\n\t * created by cmd_dt_S_dtb in scripts/Makefile.lib\n\t */\n\textern uint8_t __dtb_testcases_begin[];\n\textern uint8_t __dtb_testcases_end[];\n\tconst int size = __dtb_testcases_end - __dtb_testcases_begin;\n\tint rc;\n\n\tif (!size) {\n\t\tpr_warn(\"%s: No testcase data to attach; not running tests\\n\",\n\t\t\t__func__);\n\t\treturn -ENODATA;\n\t}\n\n\t/* creating copy */\n\tunittest_data = kmemdup(__dtb_testcases_begin, size, GFP_KERNEL);\n\tif (!unittest_data)\n\t\treturn -ENOMEM;\n\n\tof_fdt_unflatten_tree(unittest_data, NULL, &unittest_data_node);\n\tif (!unittest_data_node) {\n\t\tpr_warn(\"%s: No tree to attach; not running tests\\n\", __func__);\n\t\tkfree(unittest_data);\n\t\treturn -ENODATA;\n\t}\n\n\t/*\n\t * This lock normally encloses of_resolve_phandles()\n\t */\n\tof_overlay_mutex_lock();\n\n\trc = of_resolve_phandles(unittest_data_node);\n\tif (rc) {\n\t\tpr_err(\"%s: Failed to resolve phandles (rc=%i)\\n\", __func__, rc);\n\t\tof_overlay_mutex_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\tif (!of_root) {\n\t\tof_root = unittest_data_node;\n\t\tfor_each_of_allnodes(np)\n\t\t\t__of_attach_node_sysfs(np);\n\t\tof_aliases = of_find_node_by_path(\"/aliases\");\n\t\tof_chosen = of_find_node_by_path(\"/chosen\");\n\t\tof_overlay_mutex_unlock();\n\t\treturn 0;\n\t}\n\n\t/* attach the sub-tree to live tree */\n\tnp = unittest_data_node->child;\n\twhile (np) {\n\t\tstruct device_node *next = np->sibling;\n\n\t\tnp->parent = of_root;\n\t\tattach_node_and_children(np);\n\t\tnp = next;\n\t}\n\n\tof_overlay_mutex_unlock();\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,6 +25,7 @@\n \tof_fdt_unflatten_tree(unittest_data, NULL, &unittest_data_node);\n \tif (!unittest_data_node) {\n \t\tpr_warn(\"%s: No tree to attach; not running tests\\n\", __func__);\n+\t\tkfree(unittest_data);\n \t\treturn -ENODATA;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tkfree(unittest_data);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the unittest_data_add() function in drivers/of/unittest.c in the Linux kernel before 5.3.10 allows attackers to cause a denial of service (memory consumption) by triggering of_fdt_unflatten_tree() failures, aka CID-e13de8fe0d6a. NOTE: third parties dispute the relevance of this because unittest.c can only be reached during boot",
        "id": 2130
    },
    {
        "cve_id": "CVE-2019-18813",
        "code_before_change": "static int dwc3_pci_probe(struct pci_dev *pci, const struct pci_device_id *id)\n{\n\tstruct property_entry *p = (struct property_entry *)id->driver_data;\n\tstruct dwc3_pci\t\t*dwc;\n\tstruct resource\t\tres[2];\n\tint\t\t\tret;\n\tstruct device\t\t*dev = &pci->dev;\n\n\tret = pcim_enable_device(pci);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to enable pci device\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tpci_set_master(pci);\n\n\tdwc = devm_kzalloc(dev, sizeof(*dwc), GFP_KERNEL);\n\tif (!dwc)\n\t\treturn -ENOMEM;\n\n\tdwc->dwc3 = platform_device_alloc(\"dwc3\", PLATFORM_DEVID_AUTO);\n\tif (!dwc->dwc3)\n\t\treturn -ENOMEM;\n\n\tmemset(res, 0x00, sizeof(struct resource) * ARRAY_SIZE(res));\n\n\tres[0].start\t= pci_resource_start(pci, 0);\n\tres[0].end\t= pci_resource_end(pci, 0);\n\tres[0].name\t= \"dwc_usb3\";\n\tres[0].flags\t= IORESOURCE_MEM;\n\n\tres[1].start\t= pci->irq;\n\tres[1].name\t= \"dwc_usb3\";\n\tres[1].flags\t= IORESOURCE_IRQ;\n\n\tret = platform_device_add_resources(dwc->dwc3, res, ARRAY_SIZE(res));\n\tif (ret) {\n\t\tdev_err(dev, \"couldn't add resources to dwc3 device\\n\");\n\t\tgoto err;\n\t}\n\n\tdwc->pci = pci;\n\tdwc->dwc3->dev.parent = dev;\n\tACPI_COMPANION_SET(&dwc->dwc3->dev, ACPI_COMPANION(dev));\n\n\tret = platform_device_add_properties(dwc->dwc3, p);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = dwc3_pci_quirks(dwc);\n\tif (ret)\n\t\tgoto err;\n\n\tret = platform_device_add(dwc->dwc3);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to register dwc3 device\\n\");\n\t\tgoto err;\n\t}\n\n\tdevice_init_wakeup(dev, true);\n\tpci_set_drvdata(pci, dwc);\n\tpm_runtime_put(dev);\n#ifdef CONFIG_PM\n\tINIT_WORK(&dwc->wakeup_work, dwc3_pci_resume_work);\n#endif\n\n\treturn 0;\nerr:\n\tplatform_device_put(dwc->dwc3);\n\treturn ret;\n}",
        "code_after_change": "static int dwc3_pci_probe(struct pci_dev *pci, const struct pci_device_id *id)\n{\n\tstruct property_entry *p = (struct property_entry *)id->driver_data;\n\tstruct dwc3_pci\t\t*dwc;\n\tstruct resource\t\tres[2];\n\tint\t\t\tret;\n\tstruct device\t\t*dev = &pci->dev;\n\n\tret = pcim_enable_device(pci);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to enable pci device\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tpci_set_master(pci);\n\n\tdwc = devm_kzalloc(dev, sizeof(*dwc), GFP_KERNEL);\n\tif (!dwc)\n\t\treturn -ENOMEM;\n\n\tdwc->dwc3 = platform_device_alloc(\"dwc3\", PLATFORM_DEVID_AUTO);\n\tif (!dwc->dwc3)\n\t\treturn -ENOMEM;\n\n\tmemset(res, 0x00, sizeof(struct resource) * ARRAY_SIZE(res));\n\n\tres[0].start\t= pci_resource_start(pci, 0);\n\tres[0].end\t= pci_resource_end(pci, 0);\n\tres[0].name\t= \"dwc_usb3\";\n\tres[0].flags\t= IORESOURCE_MEM;\n\n\tres[1].start\t= pci->irq;\n\tres[1].name\t= \"dwc_usb3\";\n\tres[1].flags\t= IORESOURCE_IRQ;\n\n\tret = platform_device_add_resources(dwc->dwc3, res, ARRAY_SIZE(res));\n\tif (ret) {\n\t\tdev_err(dev, \"couldn't add resources to dwc3 device\\n\");\n\t\tgoto err;\n\t}\n\n\tdwc->pci = pci;\n\tdwc->dwc3->dev.parent = dev;\n\tACPI_COMPANION_SET(&dwc->dwc3->dev, ACPI_COMPANION(dev));\n\n\tret = platform_device_add_properties(dwc->dwc3, p);\n\tif (ret < 0)\n\t\tgoto err;\n\n\tret = dwc3_pci_quirks(dwc);\n\tif (ret)\n\t\tgoto err;\n\n\tret = platform_device_add(dwc->dwc3);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to register dwc3 device\\n\");\n\t\tgoto err;\n\t}\n\n\tdevice_init_wakeup(dev, true);\n\tpci_set_drvdata(pci, dwc);\n\tpm_runtime_put(dev);\n#ifdef CONFIG_PM\n\tINIT_WORK(&dwc->wakeup_work, dwc3_pci_resume_work);\n#endif\n\n\treturn 0;\nerr:\n\tplatform_device_put(dwc->dwc3);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -45,7 +45,7 @@\n \n \tret = platform_device_add_properties(dwc->dwc3, p);\n \tif (ret < 0)\n-\t\treturn ret;\n+\t\tgoto err;\n \n \tret = dwc3_pci_quirks(dwc);\n \tif (ret)",
        "function_modified_lines": {
            "added": [
                "\t\tgoto err;"
            ],
            "deleted": [
                "\t\treturn ret;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the dwc3_pci_probe() function in drivers/usb/dwc3/dwc3-pci.c in the Linux kernel through 5.3.9 allows attackers to cause a denial of service (memory consumption) by triggering platform_device_add_properties() failures, aka CID-9bbfceea12a8.",
        "id": 2105
    },
    {
        "cve_id": "CVE-2019-19081",
        "code_before_change": "static int\nnfp_flower_spawn_vnic_reprs(struct nfp_app *app,\n\t\t\t    enum nfp_flower_cmsg_port_vnic_type vnic_type,\n\t\t\t    enum nfp_repr_type repr_type, unsigned int cnt)\n{\n\tu8 nfp_pcie = nfp_cppcore_pcie_unit(app->pf->cpp);\n\tstruct nfp_flower_priv *priv = app->priv;\n\tatomic_t *replies = &priv->reify_replies;\n\tstruct nfp_flower_repr_priv *repr_priv;\n\tenum nfp_port_type port_type;\n\tstruct nfp_repr *nfp_repr;\n\tstruct nfp_reprs *reprs;\n\tint i, err, reify_cnt;\n\tconst u8 queue = 0;\n\n\tport_type = repr_type == NFP_REPR_TYPE_PF ? NFP_PORT_PF_PORT :\n\t\t\t\t\t\t    NFP_PORT_VF_PORT;\n\n\treprs = nfp_reprs_alloc(cnt);\n\tif (!reprs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tstruct net_device *repr;\n\t\tstruct nfp_port *port;\n\t\tu32 port_id;\n\n\t\trepr = nfp_repr_alloc(app);\n\t\tif (!repr) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_reprs_clean;\n\t\t}\n\n\t\trepr_priv = kzalloc(sizeof(*repr_priv), GFP_KERNEL);\n\t\tif (!repr_priv) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_reprs_clean;\n\t\t}\n\n\t\tnfp_repr = netdev_priv(repr);\n\t\tnfp_repr->app_priv = repr_priv;\n\t\trepr_priv->nfp_repr = nfp_repr;\n\n\t\t/* For now we only support 1 PF */\n\t\tWARN_ON(repr_type == NFP_REPR_TYPE_PF && i);\n\n\t\tport = nfp_port_alloc(app, port_type, repr);\n\t\tif (IS_ERR(port)) {\n\t\t\terr = PTR_ERR(port);\n\t\t\tnfp_repr_free(repr);\n\t\t\tgoto err_reprs_clean;\n\t\t}\n\t\tif (repr_type == NFP_REPR_TYPE_PF) {\n\t\t\tport->pf_id = i;\n\t\t\tport->vnic = priv->nn->dp.ctrl_bar;\n\t\t} else {\n\t\t\tport->pf_id = 0;\n\t\t\tport->vf_id = i;\n\t\t\tport->vnic =\n\t\t\t\tapp->pf->vf_cfg_mem + i * NFP_NET_CFG_BAR_SZ;\n\t\t}\n\n\t\teth_hw_addr_random(repr);\n\n\t\tport_id = nfp_flower_cmsg_pcie_port(nfp_pcie, vnic_type,\n\t\t\t\t\t\t    i, queue);\n\t\terr = nfp_repr_init(app, repr,\n\t\t\t\t    port_id, port, priv->nn->dp.netdev);\n\t\tif (err) {\n\t\t\tnfp_port_free(port);\n\t\t\tnfp_repr_free(repr);\n\t\t\tgoto err_reprs_clean;\n\t\t}\n\n\t\tRCU_INIT_POINTER(reprs->reprs[i], repr);\n\t\tnfp_info(app->cpp, \"%s%d Representor(%s) created\\n\",\n\t\t\t repr_type == NFP_REPR_TYPE_PF ? \"PF\" : \"VF\", i,\n\t\t\t repr->name);\n\t}\n\n\tnfp_app_reprs_set(app, repr_type, reprs);\n\n\tatomic_set(replies, 0);\n\treify_cnt = nfp_flower_reprs_reify(app, repr_type, true);\n\tif (reify_cnt < 0) {\n\t\terr = reify_cnt;\n\t\tnfp_warn(app->cpp, \"Failed to notify firmware about repr creation\\n\");\n\t\tgoto err_reprs_remove;\n\t}\n\n\terr = nfp_flower_wait_repr_reify(app, replies, reify_cnt);\n\tif (err)\n\t\tgoto err_reprs_remove;\n\n\treturn 0;\nerr_reprs_remove:\n\treprs = nfp_app_reprs_set(app, repr_type, NULL);\nerr_reprs_clean:\n\tnfp_reprs_clean_and_free(app, reprs);\n\treturn err;\n}",
        "code_after_change": "static int\nnfp_flower_spawn_vnic_reprs(struct nfp_app *app,\n\t\t\t    enum nfp_flower_cmsg_port_vnic_type vnic_type,\n\t\t\t    enum nfp_repr_type repr_type, unsigned int cnt)\n{\n\tu8 nfp_pcie = nfp_cppcore_pcie_unit(app->pf->cpp);\n\tstruct nfp_flower_priv *priv = app->priv;\n\tatomic_t *replies = &priv->reify_replies;\n\tstruct nfp_flower_repr_priv *repr_priv;\n\tenum nfp_port_type port_type;\n\tstruct nfp_repr *nfp_repr;\n\tstruct nfp_reprs *reprs;\n\tint i, err, reify_cnt;\n\tconst u8 queue = 0;\n\n\tport_type = repr_type == NFP_REPR_TYPE_PF ? NFP_PORT_PF_PORT :\n\t\t\t\t\t\t    NFP_PORT_VF_PORT;\n\n\treprs = nfp_reprs_alloc(cnt);\n\tif (!reprs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tstruct net_device *repr;\n\t\tstruct nfp_port *port;\n\t\tu32 port_id;\n\n\t\trepr = nfp_repr_alloc(app);\n\t\tif (!repr) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_reprs_clean;\n\t\t}\n\n\t\trepr_priv = kzalloc(sizeof(*repr_priv), GFP_KERNEL);\n\t\tif (!repr_priv) {\n\t\t\terr = -ENOMEM;\n\t\t\tnfp_repr_free(repr);\n\t\t\tgoto err_reprs_clean;\n\t\t}\n\n\t\tnfp_repr = netdev_priv(repr);\n\t\tnfp_repr->app_priv = repr_priv;\n\t\trepr_priv->nfp_repr = nfp_repr;\n\n\t\t/* For now we only support 1 PF */\n\t\tWARN_ON(repr_type == NFP_REPR_TYPE_PF && i);\n\n\t\tport = nfp_port_alloc(app, port_type, repr);\n\t\tif (IS_ERR(port)) {\n\t\t\terr = PTR_ERR(port);\n\t\t\tkfree(repr_priv);\n\t\t\tnfp_repr_free(repr);\n\t\t\tgoto err_reprs_clean;\n\t\t}\n\t\tif (repr_type == NFP_REPR_TYPE_PF) {\n\t\t\tport->pf_id = i;\n\t\t\tport->vnic = priv->nn->dp.ctrl_bar;\n\t\t} else {\n\t\t\tport->pf_id = 0;\n\t\t\tport->vf_id = i;\n\t\t\tport->vnic =\n\t\t\t\tapp->pf->vf_cfg_mem + i * NFP_NET_CFG_BAR_SZ;\n\t\t}\n\n\t\teth_hw_addr_random(repr);\n\n\t\tport_id = nfp_flower_cmsg_pcie_port(nfp_pcie, vnic_type,\n\t\t\t\t\t\t    i, queue);\n\t\terr = nfp_repr_init(app, repr,\n\t\t\t\t    port_id, port, priv->nn->dp.netdev);\n\t\tif (err) {\n\t\t\tkfree(repr_priv);\n\t\t\tnfp_port_free(port);\n\t\t\tnfp_repr_free(repr);\n\t\t\tgoto err_reprs_clean;\n\t\t}\n\n\t\tRCU_INIT_POINTER(reprs->reprs[i], repr);\n\t\tnfp_info(app->cpp, \"%s%d Representor(%s) created\\n\",\n\t\t\t repr_type == NFP_REPR_TYPE_PF ? \"PF\" : \"VF\", i,\n\t\t\t repr->name);\n\t}\n\n\tnfp_app_reprs_set(app, repr_type, reprs);\n\n\tatomic_set(replies, 0);\n\treify_cnt = nfp_flower_reprs_reify(app, repr_type, true);\n\tif (reify_cnt < 0) {\n\t\terr = reify_cnt;\n\t\tnfp_warn(app->cpp, \"Failed to notify firmware about repr creation\\n\");\n\t\tgoto err_reprs_remove;\n\t}\n\n\terr = nfp_flower_wait_repr_reify(app, replies, reify_cnt);\n\tif (err)\n\t\tgoto err_reprs_remove;\n\n\treturn 0;\nerr_reprs_remove:\n\treprs = nfp_app_reprs_set(app, repr_type, NULL);\nerr_reprs_clean:\n\tnfp_reprs_clean_and_free(app, reprs);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -34,6 +34,7 @@\n \t\trepr_priv = kzalloc(sizeof(*repr_priv), GFP_KERNEL);\n \t\tif (!repr_priv) {\n \t\t\terr = -ENOMEM;\n+\t\t\tnfp_repr_free(repr);\n \t\t\tgoto err_reprs_clean;\n \t\t}\n \n@@ -47,6 +48,7 @@\n \t\tport = nfp_port_alloc(app, port_type, repr);\n \t\tif (IS_ERR(port)) {\n \t\t\terr = PTR_ERR(port);\n+\t\t\tkfree(repr_priv);\n \t\t\tnfp_repr_free(repr);\n \t\t\tgoto err_reprs_clean;\n \t\t}\n@@ -67,6 +69,7 @@\n \t\terr = nfp_repr_init(app, repr,\n \t\t\t\t    port_id, port, priv->nn->dp.netdev);\n \t\tif (err) {\n+\t\t\tkfree(repr_priv);\n \t\t\tnfp_port_free(port);\n \t\t\tnfp_repr_free(repr);\n \t\t\tgoto err_reprs_clean;",
        "function_modified_lines": {
            "added": [
                "\t\t\tnfp_repr_free(repr);",
                "\t\t\tkfree(repr_priv);",
                "\t\t\tkfree(repr_priv);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the nfp_flower_spawn_vnic_reprs() function in drivers/net/ethernet/netronome/nfp/flower/main.c in the Linux kernel before 5.3.4 allows attackers to cause a denial of service (memory consumption), aka CID-8ce39eb5a67a.",
        "id": 2164
    },
    {
        "cve_id": "CVE-2019-9857",
        "code_before_change": "static int inotify_update_existing_watch(struct fsnotify_group *group,\n\t\t\t\t\t struct inode *inode,\n\t\t\t\t\t u32 arg)\n{\n\tstruct fsnotify_mark *fsn_mark;\n\tstruct inotify_inode_mark *i_mark;\n\t__u32 old_mask, new_mask;\n\t__u32 mask;\n\tint add = (arg & IN_MASK_ADD);\n\tint create = (arg & IN_MASK_CREATE);\n\tint ret;\n\n\tmask = inotify_arg_to_mask(arg);\n\n\tfsn_mark = fsnotify_find_mark(&inode->i_fsnotify_marks, group);\n\tif (!fsn_mark)\n\t\treturn -ENOENT;\n\telse if (create)\n\t\treturn -EEXIST;\n\n\ti_mark = container_of(fsn_mark, struct inotify_inode_mark, fsn_mark);\n\n\tspin_lock(&fsn_mark->lock);\n\told_mask = fsn_mark->mask;\n\tif (add)\n\t\tfsn_mark->mask |= mask;\n\telse\n\t\tfsn_mark->mask = mask;\n\tnew_mask = fsn_mark->mask;\n\tspin_unlock(&fsn_mark->lock);\n\n\tif (old_mask != new_mask) {\n\t\t/* more bits in old than in new? */\n\t\tint dropped = (old_mask & ~new_mask);\n\t\t/* more bits in this fsn_mark than the inode's mask? */\n\t\tint do_inode = (new_mask & ~inode->i_fsnotify_mask);\n\n\t\t/* update the inode with this new fsn_mark */\n\t\tif (dropped || do_inode)\n\t\t\tfsnotify_recalc_mask(inode->i_fsnotify_marks);\n\n\t}\n\n\t/* return the wd */\n\tret = i_mark->wd;\n\n\t/* match the get from fsnotify_find_mark() */\n\tfsnotify_put_mark(fsn_mark);\n\n\treturn ret;\n}",
        "code_after_change": "static int inotify_update_existing_watch(struct fsnotify_group *group,\n\t\t\t\t\t struct inode *inode,\n\t\t\t\t\t u32 arg)\n{\n\tstruct fsnotify_mark *fsn_mark;\n\tstruct inotify_inode_mark *i_mark;\n\t__u32 old_mask, new_mask;\n\t__u32 mask;\n\tint add = (arg & IN_MASK_ADD);\n\tint create = (arg & IN_MASK_CREATE);\n\tint ret;\n\n\tmask = inotify_arg_to_mask(arg);\n\n\tfsn_mark = fsnotify_find_mark(&inode->i_fsnotify_marks, group);\n\tif (!fsn_mark)\n\t\treturn -ENOENT;\n\telse if (create) {\n\t\tret = -EEXIST;\n\t\tgoto out;\n\t}\n\n\ti_mark = container_of(fsn_mark, struct inotify_inode_mark, fsn_mark);\n\n\tspin_lock(&fsn_mark->lock);\n\told_mask = fsn_mark->mask;\n\tif (add)\n\t\tfsn_mark->mask |= mask;\n\telse\n\t\tfsn_mark->mask = mask;\n\tnew_mask = fsn_mark->mask;\n\tspin_unlock(&fsn_mark->lock);\n\n\tif (old_mask != new_mask) {\n\t\t/* more bits in old than in new? */\n\t\tint dropped = (old_mask & ~new_mask);\n\t\t/* more bits in this fsn_mark than the inode's mask? */\n\t\tint do_inode = (new_mask & ~inode->i_fsnotify_mask);\n\n\t\t/* update the inode with this new fsn_mark */\n\t\tif (dropped || do_inode)\n\t\t\tfsnotify_recalc_mask(inode->i_fsnotify_marks);\n\n\t}\n\n\t/* return the wd */\n\tret = i_mark->wd;\n\nout:\n\t/* match the get from fsnotify_find_mark() */\n\tfsnotify_put_mark(fsn_mark);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,8 +15,10 @@\n \tfsn_mark = fsnotify_find_mark(&inode->i_fsnotify_marks, group);\n \tif (!fsn_mark)\n \t\treturn -ENOENT;\n-\telse if (create)\n-\t\treturn -EEXIST;\n+\telse if (create) {\n+\t\tret = -EEXIST;\n+\t\tgoto out;\n+\t}\n \n \ti_mark = container_of(fsn_mark, struct inotify_inode_mark, fsn_mark);\n \n@@ -44,6 +46,7 @@\n \t/* return the wd */\n \tret = i_mark->wd;\n \n+out:\n \t/* match the get from fsnotify_find_mark() */\n \tfsnotify_put_mark(fsn_mark);\n ",
        "function_modified_lines": {
            "added": [
                "\telse if (create) {",
                "\t\tret = -EEXIST;",
                "\t\tgoto out;",
                "\t}",
                "out:"
            ],
            "deleted": [
                "\telse if (create)",
                "\t\treturn -EEXIST;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "In the Linux kernel through 5.0.2, the function inotify_update_existing_watch() in fs/notify/inotify/inotify_user.c neglects to call fsnotify_put_mark() with IN_MASK_CREATE after fsnotify_find_mark(), which will cause a memory leak (aka refcount leak). Finally, this will cause a denial of service.",
        "id": 2370
    },
    {
        "cve_id": "CVE-2021-29649",
        "code_before_change": "static void umd_cleanup(struct subprocess_info *info)\n{\n\tstruct umd_info *umd_info = info->data;\n\n\t/* cleanup if umh_setup() was successful but exec failed */\n\tif (info->retval) {\n\t\tfput(umd_info->pipe_to_umh);\n\t\tfput(umd_info->pipe_from_umh);\n\t\tput_pid(umd_info->tgid);\n\t\tumd_info->tgid = NULL;\n\t}\n}",
        "code_after_change": "static void umd_cleanup(struct subprocess_info *info)\n{\n\tstruct umd_info *umd_info = info->data;\n\n\t/* cleanup if umh_setup() was successful but exec failed */\n\tif (info->retval)\n\t\tumd_cleanup_helper(umd_info);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,10 +3,6 @@\n \tstruct umd_info *umd_info = info->data;\n \n \t/* cleanup if umh_setup() was successful but exec failed */\n-\tif (info->retval) {\n-\t\tfput(umd_info->pipe_to_umh);\n-\t\tfput(umd_info->pipe_from_umh);\n-\t\tput_pid(umd_info->tgid);\n-\t\tumd_info->tgid = NULL;\n-\t}\n+\tif (info->retval)\n+\t\tumd_cleanup_helper(umd_info);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (info->retval)",
                "\t\tumd_cleanup_helper(umd_info);"
            ],
            "deleted": [
                "\tif (info->retval) {",
                "\t\tfput(umd_info->pipe_to_umh);",
                "\t\tfput(umd_info->pipe_from_umh);",
                "\t\tput_pid(umd_info->tgid);",
                "\t\tumd_info->tgid = NULL;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.11.11. The user mode driver (UMD) has a copy_process() memory leak, related to a lack of cleanup steps in kernel/usermode_driver.c and kernel/bpf/preload/bpf_preload_kern.c, aka CID-f60a85cad677.",
        "id": 2954
    },
    {
        "cve_id": "CVE-2022-0742",
        "code_before_change": "int igmp6_event_report(struct sk_buff *skb)\n{\n\tstruct inet6_dev *idev = __in6_dev_get(skb->dev);\n\n\tif (!idev)\n\t\treturn -EINVAL;\n\n\tif (idev->dead) {\n\t\tkfree_skb(skb);\n\t\treturn -ENODEV;\n\t}\n\n\tspin_lock_bh(&idev->mc_report_lock);\n\tif (skb_queue_len(&idev->mc_report_queue) < MLD_MAX_SKBS) {\n\t\t__skb_queue_tail(&idev->mc_report_queue, skb);\n\t\tif (!mod_delayed_work(mld_wq, &idev->mc_report_work, 0))\n\t\t\tin6_dev_hold(idev);\n\t}\n\tspin_unlock_bh(&idev->mc_report_lock);\n\n\treturn 0;\n}",
        "code_after_change": "void igmp6_event_report(struct sk_buff *skb)\n{\n\tstruct inet6_dev *idev = __in6_dev_get(skb->dev);\n\n\tif (!idev || idev->dead)\n\t\tgoto out;\n\n\tspin_lock_bh(&idev->mc_report_lock);\n\tif (skb_queue_len(&idev->mc_report_queue) < MLD_MAX_SKBS) {\n\t\t__skb_queue_tail(&idev->mc_report_queue, skb);\n\t\tif (!mod_delayed_work(mld_wq, &idev->mc_report_work, 0))\n\t\t\tin6_dev_hold(idev);\n\t\tskb = NULL;\n\t}\n\tspin_unlock_bh(&idev->mc_report_lock);\nout:\n\tkfree_skb(skb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,22 +1,18 @@\n-int igmp6_event_report(struct sk_buff *skb)\n+void igmp6_event_report(struct sk_buff *skb)\n {\n \tstruct inet6_dev *idev = __in6_dev_get(skb->dev);\n \n-\tif (!idev)\n-\t\treturn -EINVAL;\n-\n-\tif (idev->dead) {\n-\t\tkfree_skb(skb);\n-\t\treturn -ENODEV;\n-\t}\n+\tif (!idev || idev->dead)\n+\t\tgoto out;\n \n \tspin_lock_bh(&idev->mc_report_lock);\n \tif (skb_queue_len(&idev->mc_report_queue) < MLD_MAX_SKBS) {\n \t\t__skb_queue_tail(&idev->mc_report_queue, skb);\n \t\tif (!mod_delayed_work(mld_wq, &idev->mc_report_work, 0))\n \t\t\tin6_dev_hold(idev);\n+\t\tskb = NULL;\n \t}\n \tspin_unlock_bh(&idev->mc_report_lock);\n-\n-\treturn 0;\n+out:\n+\tkfree_skb(skb);\n }",
        "function_modified_lines": {
            "added": [
                "void igmp6_event_report(struct sk_buff *skb)",
                "\tif (!idev || idev->dead)",
                "\t\tgoto out;",
                "\t\tskb = NULL;",
                "out:",
                "\tkfree_skb(skb);"
            ],
            "deleted": [
                "int igmp6_event_report(struct sk_buff *skb)",
                "\tif (!idev)",
                "\t\treturn -EINVAL;",
                "",
                "\tif (idev->dead) {",
                "\t\tkfree_skb(skb);",
                "\t\treturn -ENODEV;",
                "\t}",
                "",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leak in icmp6 implementation in Linux Kernel 5.13+ allows a remote attacker to DoS a host by making it go out-of-memory via icmp6 packets of type 130 or 131. We recommend upgrading past commit 2d3916f3189172d5c69d33065c3c21119fe539fc.",
        "id": 3219
    },
    {
        "cve_id": "CVE-2022-1012",
        "code_before_change": "u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)\n{\n\tnet_secret_init();\n\treturn siphash_3u32((__force u32)saddr, (__force u32)daddr,\n\t\t\t    (__force u16)dport, &net_secret);\n}",
        "code_after_change": "u64 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)\n{\n\tnet_secret_init();\n\treturn siphash_3u32((__force u32)saddr, (__force u32)daddr,\n\t\t\t    (__force u16)dport, &net_secret);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)\n+u64 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)\n {\n \tnet_secret_init();\n \treturn siphash_3u32((__force u32)saddr, (__force u32)daddr,",
        "function_modified_lines": {
            "added": [
                "u64 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)"
            ],
            "deleted": [
                "u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak problem was found in the TCP source port generation algorithm in net/ipv4/tcp.c due to the small table perturb size. This flaw may allow an attacker to information leak and may cause a denial of service problem.",
        "id": 3232
    },
    {
        "cve_id": "CVE-2019-19075",
        "code_before_change": "static int ca8210_probe(struct spi_device *spi_device)\n{\n\tstruct ca8210_priv *priv;\n\tstruct ieee802154_hw *hw;\n\tstruct ca8210_platform_data *pdata;\n\tint ret;\n\n\tdev_info(&spi_device->dev, \"Inserting ca8210\\n\");\n\n\t/* allocate ieee802154_hw and private data */\n\thw = ieee802154_alloc_hw(sizeof(struct ca8210_priv), &ca8210_phy_ops);\n\tif (!hw) {\n\t\tdev_crit(&spi_device->dev, \"ieee802154_alloc_hw failed\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\tpriv = hw->priv;\n\tpriv->hw = hw;\n\tpriv->spi = spi_device;\n\thw->parent = &spi_device->dev;\n\tspin_lock_init(&priv->lock);\n\tpriv->async_tx_pending = false;\n\tpriv->hw_registered = false;\n\tpriv->sync_up = 0;\n\tpriv->sync_down = 0;\n\tpriv->promiscuous = false;\n\tpriv->retries = 0;\n\tinit_completion(&priv->ca8210_is_awake);\n\tinit_completion(&priv->spi_transfer_complete);\n\tinit_completion(&priv->sync_exchange_complete);\n\tspi_set_drvdata(priv->spi, priv);\n\tif (IS_ENABLED(CONFIG_IEEE802154_CA8210_DEBUGFS)) {\n\t\tcascoda_api_upstream = ca8210_test_int_driver_write;\n\t\tca8210_test_interface_init(priv);\n\t} else {\n\t\tcascoda_api_upstream = NULL;\n\t}\n\tca8210_hw_setup(hw);\n\tieee802154_random_extended_addr(&hw->phy->perm_extended_addr);\n\n\tpdata = kmalloc(sizeof(*pdata), GFP_KERNEL);\n\tif (!pdata) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\tret = ca8210_get_platform_data(priv->spi, pdata);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_get_platform_data failed\\n\");\n\t\tgoto error;\n\t}\n\tpriv->spi->dev.platform_data = pdata;\n\n\tret = ca8210_dev_com_init(priv);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_dev_com_init failed\\n\");\n\t\tgoto error;\n\t}\n\tret = ca8210_reset_init(priv->spi);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_reset_init failed\\n\");\n\t\tgoto error;\n\t}\n\n\tret = ca8210_interrupt_init(priv->spi);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_interrupt_init failed\\n\");\n\t\tgoto error;\n\t}\n\n\tmsleep(100);\n\n\tca8210_reset_send(priv->spi, 1);\n\n\tret = tdme_chipinit(priv->spi);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"tdme_chipinit failed\\n\");\n\t\tgoto error;\n\t}\n\n\tif (pdata->extclockenable) {\n\t\tret = ca8210_config_extern_clk(pdata, priv->spi, 1);\n\t\tif (ret) {\n\t\t\tdev_crit(\n\t\t\t\t&spi_device->dev,\n\t\t\t\t\"ca8210_config_extern_clk failed\\n\"\n\t\t\t);\n\t\t\tgoto error;\n\t\t}\n\t\tret = ca8210_register_ext_clock(priv->spi);\n\t\tif (ret) {\n\t\t\tdev_crit(\n\t\t\t\t&spi_device->dev,\n\t\t\t\t\"ca8210_register_ext_clock failed\\n\"\n\t\t\t);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tret = ieee802154_register_hw(hw);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ieee802154_register_hw failed\\n\");\n\t\tgoto error;\n\t}\n\tpriv->hw_registered = true;\n\n\treturn 0;\nerror:\n\tmsleep(100); /* wait for pending spi transfers to complete */\n\tca8210_remove(spi_device);\n\treturn link_to_linux_err(ret);\n}",
        "code_after_change": "static int ca8210_probe(struct spi_device *spi_device)\n{\n\tstruct ca8210_priv *priv;\n\tstruct ieee802154_hw *hw;\n\tstruct ca8210_platform_data *pdata;\n\tint ret;\n\n\tdev_info(&spi_device->dev, \"Inserting ca8210\\n\");\n\n\t/* allocate ieee802154_hw and private data */\n\thw = ieee802154_alloc_hw(sizeof(struct ca8210_priv), &ca8210_phy_ops);\n\tif (!hw) {\n\t\tdev_crit(&spi_device->dev, \"ieee802154_alloc_hw failed\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\tpriv = hw->priv;\n\tpriv->hw = hw;\n\tpriv->spi = spi_device;\n\thw->parent = &spi_device->dev;\n\tspin_lock_init(&priv->lock);\n\tpriv->async_tx_pending = false;\n\tpriv->hw_registered = false;\n\tpriv->sync_up = 0;\n\tpriv->sync_down = 0;\n\tpriv->promiscuous = false;\n\tpriv->retries = 0;\n\tinit_completion(&priv->ca8210_is_awake);\n\tinit_completion(&priv->spi_transfer_complete);\n\tinit_completion(&priv->sync_exchange_complete);\n\tspi_set_drvdata(priv->spi, priv);\n\tif (IS_ENABLED(CONFIG_IEEE802154_CA8210_DEBUGFS)) {\n\t\tcascoda_api_upstream = ca8210_test_int_driver_write;\n\t\tca8210_test_interface_init(priv);\n\t} else {\n\t\tcascoda_api_upstream = NULL;\n\t}\n\tca8210_hw_setup(hw);\n\tieee802154_random_extended_addr(&hw->phy->perm_extended_addr);\n\n\tpdata = kmalloc(sizeof(*pdata), GFP_KERNEL);\n\tif (!pdata) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\tpriv->spi->dev.platform_data = pdata;\n\tret = ca8210_get_platform_data(priv->spi, pdata);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_get_platform_data failed\\n\");\n\t\tgoto error;\n\t}\n\n\tret = ca8210_dev_com_init(priv);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_dev_com_init failed\\n\");\n\t\tgoto error;\n\t}\n\tret = ca8210_reset_init(priv->spi);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_reset_init failed\\n\");\n\t\tgoto error;\n\t}\n\n\tret = ca8210_interrupt_init(priv->spi);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_interrupt_init failed\\n\");\n\t\tgoto error;\n\t}\n\n\tmsleep(100);\n\n\tca8210_reset_send(priv->spi, 1);\n\n\tret = tdme_chipinit(priv->spi);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"tdme_chipinit failed\\n\");\n\t\tgoto error;\n\t}\n\n\tif (pdata->extclockenable) {\n\t\tret = ca8210_config_extern_clk(pdata, priv->spi, 1);\n\t\tif (ret) {\n\t\t\tdev_crit(\n\t\t\t\t&spi_device->dev,\n\t\t\t\t\"ca8210_config_extern_clk failed\\n\"\n\t\t\t);\n\t\t\tgoto error;\n\t\t}\n\t\tret = ca8210_register_ext_clock(priv->spi);\n\t\tif (ret) {\n\t\t\tdev_crit(\n\t\t\t\t&spi_device->dev,\n\t\t\t\t\"ca8210_register_ext_clock failed\\n\"\n\t\t\t);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tret = ieee802154_register_hw(hw);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ieee802154_register_hw failed\\n\");\n\t\tgoto error;\n\t}\n\tpriv->hw_registered = true;\n\n\treturn 0;\nerror:\n\tmsleep(100); /* wait for pending spi transfers to complete */\n\tca8210_remove(spi_device);\n\treturn link_to_linux_err(ret);\n}",
        "patch": "--- code before\n+++ code after\n@@ -45,12 +45,12 @@\n \t\tgoto error;\n \t}\n \n+\tpriv->spi->dev.platform_data = pdata;\n \tret = ca8210_get_platform_data(priv->spi, pdata);\n \tif (ret) {\n \t\tdev_crit(&spi_device->dev, \"ca8210_get_platform_data failed\\n\");\n \t\tgoto error;\n \t}\n-\tpriv->spi->dev.platform_data = pdata;\n \n \tret = ca8210_dev_com_init(priv);\n \tif (ret) {",
        "function_modified_lines": {
            "added": [
                "\tpriv->spi->dev.platform_data = pdata;"
            ],
            "deleted": [
                "\tpriv->spi->dev.platform_data = pdata;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the ca8210_probe() function in drivers/net/ieee802154/ca8210.c in the Linux kernel before 5.3.8 allows attackers to cause a denial of service (memory consumption) by triggering ca8210_get_platform_data() failures, aka CID-6402939ec86e.",
        "id": 2158
    },
    {
        "cve_id": "CVE-2019-8980",
        "code_before_change": "int kernel_read_file(struct file *file, void **buf, loff_t *size,\n\t\t     loff_t max_size, enum kernel_read_file_id id)\n{\n\tloff_t i_size, pos;\n\tssize_t bytes = 0;\n\tint ret;\n\n\tif (!S_ISREG(file_inode(file)->i_mode) || max_size < 0)\n\t\treturn -EINVAL;\n\n\tret = deny_write_access(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = security_kernel_read_file(file, id);\n\tif (ret)\n\t\tgoto out;\n\n\ti_size = i_size_read(file_inode(file));\n\tif (i_size <= 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (i_size > SIZE_MAX || (max_size > 0 && i_size > max_size)) {\n\t\tret = -EFBIG;\n\t\tgoto out;\n\t}\n\n\tif (id != READING_FIRMWARE_PREALLOC_BUFFER)\n\t\t*buf = vmalloc(i_size);\n\tif (!*buf) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tpos = 0;\n\twhile (pos < i_size) {\n\t\tbytes = kernel_read(file, *buf + pos, i_size - pos, &pos);\n\t\tif (bytes < 0) {\n\t\t\tret = bytes;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (bytes == 0)\n\t\t\tbreak;\n\t}\n\n\tif (pos != i_size) {\n\t\tret = -EIO;\n\t\tgoto out_free;\n\t}\n\n\tret = security_kernel_post_read_file(file, *buf, i_size, id);\n\tif (!ret)\n\t\t*size = pos;\n\nout_free:\n\tif (ret < 0) {\n\t\tif (id != READING_FIRMWARE_PREALLOC_BUFFER) {\n\t\t\tvfree(*buf);\n\t\t\t*buf = NULL;\n\t\t}\n\t}\n\nout:\n\tallow_write_access(file);\n\treturn ret;\n}",
        "code_after_change": "int kernel_read_file(struct file *file, void **buf, loff_t *size,\n\t\t     loff_t max_size, enum kernel_read_file_id id)\n{\n\tloff_t i_size, pos;\n\tssize_t bytes = 0;\n\tint ret;\n\n\tif (!S_ISREG(file_inode(file)->i_mode) || max_size < 0)\n\t\treturn -EINVAL;\n\n\tret = deny_write_access(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = security_kernel_read_file(file, id);\n\tif (ret)\n\t\tgoto out;\n\n\ti_size = i_size_read(file_inode(file));\n\tif (i_size <= 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (i_size > SIZE_MAX || (max_size > 0 && i_size > max_size)) {\n\t\tret = -EFBIG;\n\t\tgoto out;\n\t}\n\n\tif (id != READING_FIRMWARE_PREALLOC_BUFFER)\n\t\t*buf = vmalloc(i_size);\n\tif (!*buf) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tpos = 0;\n\twhile (pos < i_size) {\n\t\tbytes = kernel_read(file, *buf + pos, i_size - pos, &pos);\n\t\tif (bytes < 0) {\n\t\t\tret = bytes;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tif (bytes == 0)\n\t\t\tbreak;\n\t}\n\n\tif (pos != i_size) {\n\t\tret = -EIO;\n\t\tgoto out_free;\n\t}\n\n\tret = security_kernel_post_read_file(file, *buf, i_size, id);\n\tif (!ret)\n\t\t*size = pos;\n\nout_free:\n\tif (ret < 0) {\n\t\tif (id != READING_FIRMWARE_PREALLOC_BUFFER) {\n\t\t\tvfree(*buf);\n\t\t\t*buf = NULL;\n\t\t}\n\t}\n\nout:\n\tallow_write_access(file);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,7 +38,7 @@\n \t\tbytes = kernel_read(file, *buf + pos, i_size - pos, &pos);\n \t\tif (bytes < 0) {\n \t\t\tret = bytes;\n-\t\t\tgoto out;\n+\t\t\tgoto out_free;\n \t\t}\n \n \t\tif (bytes == 0)",
        "function_modified_lines": {
            "added": [
                "\t\t\tgoto out_free;"
            ],
            "deleted": [
                "\t\t\tgoto out;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the kernel_read_file function in fs/exec.c in the Linux kernel through 4.20.11 allows attackers to cause a denial of service (memory consumption) by triggering vfs_read failures.",
        "id": 2349
    },
    {
        "cve_id": "CVE-2019-19071",
        "code_before_change": "static int rsi_send_beacon(struct rsi_common *common)\n{\n\tstruct sk_buff *skb = NULL;\n\tu8 dword_align_bytes = 0;\n\n\tskb = dev_alloc_skb(MAX_MGMT_PKT_SIZE);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tmemset(skb->data, 0, MAX_MGMT_PKT_SIZE);\n\n\tdword_align_bytes = ((unsigned long)skb->data & 0x3f);\n\tif (dword_align_bytes)\n\t\tskb_pull(skb, (64 - dword_align_bytes));\n\tif (rsi_prepare_beacon(common, skb)) {\n\t\trsi_dbg(ERR_ZONE, \"Failed to prepare beacon\\n\");\n\t\treturn -EINVAL;\n\t}\n\tskb_queue_tail(&common->tx_queue[MGMT_BEACON_Q], skb);\n\trsi_set_event(&common->tx_thread.event);\n\trsi_dbg(DATA_TX_ZONE, \"%s: Added to beacon queue\\n\", __func__);\n\n\treturn 0;\n}",
        "code_after_change": "static int rsi_send_beacon(struct rsi_common *common)\n{\n\tstruct sk_buff *skb = NULL;\n\tu8 dword_align_bytes = 0;\n\n\tskb = dev_alloc_skb(MAX_MGMT_PKT_SIZE);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tmemset(skb->data, 0, MAX_MGMT_PKT_SIZE);\n\n\tdword_align_bytes = ((unsigned long)skb->data & 0x3f);\n\tif (dword_align_bytes)\n\t\tskb_pull(skb, (64 - dword_align_bytes));\n\tif (rsi_prepare_beacon(common, skb)) {\n\t\trsi_dbg(ERR_ZONE, \"Failed to prepare beacon\\n\");\n\t\tdev_kfree_skb(skb);\n\t\treturn -EINVAL;\n\t}\n\tskb_queue_tail(&common->tx_queue[MGMT_BEACON_Q], skb);\n\trsi_set_event(&common->tx_thread.event);\n\trsi_dbg(DATA_TX_ZONE, \"%s: Added to beacon queue\\n\", __func__);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,6 +14,7 @@\n \t\tskb_pull(skb, (64 - dword_align_bytes));\n \tif (rsi_prepare_beacon(common, skb)) {\n \t\trsi_dbg(ERR_ZONE, \"Failed to prepare beacon\\n\");\n+\t\tdev_kfree_skb(skb);\n \t\treturn -EINVAL;\n \t}\n \tskb_queue_tail(&common->tx_queue[MGMT_BEACON_Q], skb);",
        "function_modified_lines": {
            "added": [
                "\t\tdev_kfree_skb(skb);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the rsi_send_beacon() function in drivers/net/wireless/rsi/rsi_91x_mgmt.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering rsi_prepare_beacon() failures, aka CID-d563131ef23c.",
        "id": 2152
    },
    {
        "cve_id": "CVE-2019-19065",
        "code_before_change": "int sdma_init(struct hfi1_devdata *dd, u8 port)\n{\n\tunsigned this_idx;\n\tstruct sdma_engine *sde;\n\tstruct rhashtable *tmp_sdma_rht;\n\tu16 descq_cnt;\n\tvoid *curr_head;\n\tstruct hfi1_pportdata *ppd = dd->pport + port;\n\tu32 per_sdma_credits;\n\tuint idle_cnt = sdma_idle_cnt;\n\tsize_t num_engines = chip_sdma_engines(dd);\n\tint ret = -ENOMEM;\n\n\tif (!HFI1_CAP_IS_KSET(SDMA)) {\n\t\tHFI1_CAP_CLEAR(SDMA_AHG);\n\t\treturn 0;\n\t}\n\tif (mod_num_sdma &&\n\t    /* can't exceed chip support */\n\t    mod_num_sdma <= chip_sdma_engines(dd) &&\n\t    /* count must be >= vls */\n\t    mod_num_sdma >= num_vls)\n\t\tnum_engines = mod_num_sdma;\n\n\tdd_dev_info(dd, \"SDMA mod_num_sdma: %u\\n\", mod_num_sdma);\n\tdd_dev_info(dd, \"SDMA chip_sdma_engines: %u\\n\", chip_sdma_engines(dd));\n\tdd_dev_info(dd, \"SDMA chip_sdma_mem_size: %u\\n\",\n\t\t    chip_sdma_mem_size(dd));\n\n\tper_sdma_credits =\n\t\tchip_sdma_mem_size(dd) / (num_engines * SDMA_BLOCK_SIZE);\n\n\t/* set up freeze waitqueue */\n\tinit_waitqueue_head(&dd->sdma_unfreeze_wq);\n\tatomic_set(&dd->sdma_unfreeze_count, 0);\n\n\tdescq_cnt = sdma_get_descq_cnt();\n\tdd_dev_info(dd, \"SDMA engines %zu descq_cnt %u\\n\",\n\t\t    num_engines, descq_cnt);\n\n\t/* alloc memory for array of send engines */\n\tdd->per_sdma = kcalloc_node(num_engines, sizeof(*dd->per_sdma),\n\t\t\t\t    GFP_KERNEL, dd->node);\n\tif (!dd->per_sdma)\n\t\treturn ret;\n\n\tidle_cnt = ns_to_cclock(dd, idle_cnt);\n\tif (idle_cnt)\n\t\tdd->default_desc1 =\n\t\t\tSDMA_DESC1_HEAD_TO_HOST_FLAG;\n\telse\n\t\tdd->default_desc1 =\n\t\t\tSDMA_DESC1_INT_REQ_FLAG;\n\n\tif (!sdma_desct_intr)\n\t\tsdma_desct_intr = SDMA_DESC_INTR;\n\n\t/* Allocate memory for SendDMA descriptor FIFOs */\n\tfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\n\t\tsde = &dd->per_sdma[this_idx];\n\t\tsde->dd = dd;\n\t\tsde->ppd = ppd;\n\t\tsde->this_idx = this_idx;\n\t\tsde->descq_cnt = descq_cnt;\n\t\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t\tsde->sdma_shift = ilog2(descq_cnt);\n\t\tsde->sdma_mask = (1 << sde->sdma_shift) - 1;\n\n\t\t/* Create a mask specifically for each interrupt source */\n\t\tsde->int_mask = (u64)1 << (0 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t   this_idx);\n\t\tsde->progress_mask = (u64)1 << (1 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t\tthis_idx);\n\t\tsde->idle_mask = (u64)1 << (2 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t    this_idx);\n\t\t/* Create a combined mask to cover all 3 interrupt sources */\n\t\tsde->imask = sde->int_mask | sde->progress_mask |\n\t\t\t     sde->idle_mask;\n\n\t\tspin_lock_init(&sde->tail_lock);\n\t\tseqlock_init(&sde->head_lock);\n\t\tspin_lock_init(&sde->senddmactrl_lock);\n\t\tspin_lock_init(&sde->flushlist_lock);\n\t\tseqlock_init(&sde->waitlock);\n\t\t/* insure there is always a zero bit */\n\t\tsde->ahg_bits = 0xfffffffe00000000ULL;\n\n\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\n\t\t/* set up reference counting */\n\t\tkref_init(&sde->state.kref);\n\t\tinit_completion(&sde->state.comp);\n\n\t\tINIT_LIST_HEAD(&sde->flushlist);\n\t\tINIT_LIST_HEAD(&sde->dmawait);\n\n\t\tsde->tail_csr =\n\t\t\tget_kctxt_csr_addr(dd, this_idx, SD(TAIL));\n\n\t\ttasklet_init(&sde->sdma_hw_clean_up_task, sdma_hw_clean_up_task,\n\t\t\t     (unsigned long)sde);\n\n\t\ttasklet_init(&sde->sdma_sw_clean_up_task, sdma_sw_clean_up_task,\n\t\t\t     (unsigned long)sde);\n\t\tINIT_WORK(&sde->err_halt_worker, sdma_err_halt_wait);\n\t\tINIT_WORK(&sde->flush_worker, sdma_field_flush);\n\n\t\tsde->progress_check_head = 0;\n\n\t\ttimer_setup(&sde->err_progress_check_timer,\n\t\t\t    sdma_err_progress_check, 0);\n\n\t\tsde->descq = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\tdescq_cnt * sizeof(u64[2]),\n\t\t\t\t\t\t&sde->descq_phys, GFP_KERNEL);\n\t\tif (!sde->descq)\n\t\t\tgoto bail;\n\t\tsde->tx_ring =\n\t\t\tkvzalloc_node(array_size(descq_cnt,\n\t\t\t\t\t\t sizeof(struct sdma_txreq *)),\n\t\t\t\t      GFP_KERNEL, dd->node);\n\t\tif (!sde->tx_ring)\n\t\t\tgoto bail;\n\t}\n\n\tdd->sdma_heads_size = L1_CACHE_BYTES * num_engines;\n\t/* Allocate memory for DMA of head registers to memory */\n\tdd->sdma_heads_dma = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\tdd->sdma_heads_size,\n\t\t\t\t\t\t&dd->sdma_heads_phys,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!dd->sdma_heads_dma) {\n\t\tdd_dev_err(dd, \"failed to allocate SendDMA head memory\\n\");\n\t\tgoto bail;\n\t}\n\n\t/* Allocate memory for pad */\n\tdd->sdma_pad_dma = dma_alloc_coherent(&dd->pcidev->dev, sizeof(u32),\n\t\t\t\t\t      &dd->sdma_pad_phys, GFP_KERNEL);\n\tif (!dd->sdma_pad_dma) {\n\t\tdd_dev_err(dd, \"failed to allocate SendDMA pad memory\\n\");\n\t\tgoto bail;\n\t}\n\n\t/* assign each engine to different cacheline and init registers */\n\tcurr_head = (void *)dd->sdma_heads_dma;\n\tfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\n\t\tunsigned long phys_offset;\n\n\t\tsde = &dd->per_sdma[this_idx];\n\n\t\tsde->head_dma = curr_head;\n\t\tcurr_head += L1_CACHE_BYTES;\n\t\tphys_offset = (unsigned long)sde->head_dma -\n\t\t\t      (unsigned long)dd->sdma_heads_dma;\n\t\tsde->head_phys = dd->sdma_heads_phys + phys_offset;\n\t\tinit_sdma_regs(sde, per_sdma_credits, idle_cnt);\n\t}\n\tdd->flags |= HFI1_HAS_SEND_DMA;\n\tdd->flags |= idle_cnt ? HFI1_HAS_SDMA_TIMEOUT : 0;\n\tdd->num_sdma = num_engines;\n\tret = sdma_map_init(dd, port, ppd->vls_operational, NULL);\n\tif (ret < 0)\n\t\tgoto bail;\n\n\ttmp_sdma_rht = kzalloc(sizeof(*tmp_sdma_rht), GFP_KERNEL);\n\tif (!tmp_sdma_rht) {\n\t\tret = -ENOMEM;\n\t\tgoto bail;\n\t}\n\n\tret = rhashtable_init(tmp_sdma_rht, &sdma_rht_params);\n\tif (ret < 0)\n\t\tgoto bail;\n\tdd->sdma_rht = tmp_sdma_rht;\n\n\tdd_dev_info(dd, \"SDMA num_sdma: %u\\n\", dd->num_sdma);\n\treturn 0;\n\nbail:\n\tsdma_clean(dd, num_engines);\n\treturn ret;\n}",
        "code_after_change": "int sdma_init(struct hfi1_devdata *dd, u8 port)\n{\n\tunsigned this_idx;\n\tstruct sdma_engine *sde;\n\tstruct rhashtable *tmp_sdma_rht;\n\tu16 descq_cnt;\n\tvoid *curr_head;\n\tstruct hfi1_pportdata *ppd = dd->pport + port;\n\tu32 per_sdma_credits;\n\tuint idle_cnt = sdma_idle_cnt;\n\tsize_t num_engines = chip_sdma_engines(dd);\n\tint ret = -ENOMEM;\n\n\tif (!HFI1_CAP_IS_KSET(SDMA)) {\n\t\tHFI1_CAP_CLEAR(SDMA_AHG);\n\t\treturn 0;\n\t}\n\tif (mod_num_sdma &&\n\t    /* can't exceed chip support */\n\t    mod_num_sdma <= chip_sdma_engines(dd) &&\n\t    /* count must be >= vls */\n\t    mod_num_sdma >= num_vls)\n\t\tnum_engines = mod_num_sdma;\n\n\tdd_dev_info(dd, \"SDMA mod_num_sdma: %u\\n\", mod_num_sdma);\n\tdd_dev_info(dd, \"SDMA chip_sdma_engines: %u\\n\", chip_sdma_engines(dd));\n\tdd_dev_info(dd, \"SDMA chip_sdma_mem_size: %u\\n\",\n\t\t    chip_sdma_mem_size(dd));\n\n\tper_sdma_credits =\n\t\tchip_sdma_mem_size(dd) / (num_engines * SDMA_BLOCK_SIZE);\n\n\t/* set up freeze waitqueue */\n\tinit_waitqueue_head(&dd->sdma_unfreeze_wq);\n\tatomic_set(&dd->sdma_unfreeze_count, 0);\n\n\tdescq_cnt = sdma_get_descq_cnt();\n\tdd_dev_info(dd, \"SDMA engines %zu descq_cnt %u\\n\",\n\t\t    num_engines, descq_cnt);\n\n\t/* alloc memory for array of send engines */\n\tdd->per_sdma = kcalloc_node(num_engines, sizeof(*dd->per_sdma),\n\t\t\t\t    GFP_KERNEL, dd->node);\n\tif (!dd->per_sdma)\n\t\treturn ret;\n\n\tidle_cnt = ns_to_cclock(dd, idle_cnt);\n\tif (idle_cnt)\n\t\tdd->default_desc1 =\n\t\t\tSDMA_DESC1_HEAD_TO_HOST_FLAG;\n\telse\n\t\tdd->default_desc1 =\n\t\t\tSDMA_DESC1_INT_REQ_FLAG;\n\n\tif (!sdma_desct_intr)\n\t\tsdma_desct_intr = SDMA_DESC_INTR;\n\n\t/* Allocate memory for SendDMA descriptor FIFOs */\n\tfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\n\t\tsde = &dd->per_sdma[this_idx];\n\t\tsde->dd = dd;\n\t\tsde->ppd = ppd;\n\t\tsde->this_idx = this_idx;\n\t\tsde->descq_cnt = descq_cnt;\n\t\tsde->desc_avail = sdma_descq_freecnt(sde);\n\t\tsde->sdma_shift = ilog2(descq_cnt);\n\t\tsde->sdma_mask = (1 << sde->sdma_shift) - 1;\n\n\t\t/* Create a mask specifically for each interrupt source */\n\t\tsde->int_mask = (u64)1 << (0 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t   this_idx);\n\t\tsde->progress_mask = (u64)1 << (1 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t\tthis_idx);\n\t\tsde->idle_mask = (u64)1 << (2 * TXE_NUM_SDMA_ENGINES +\n\t\t\t\t\t    this_idx);\n\t\t/* Create a combined mask to cover all 3 interrupt sources */\n\t\tsde->imask = sde->int_mask | sde->progress_mask |\n\t\t\t     sde->idle_mask;\n\n\t\tspin_lock_init(&sde->tail_lock);\n\t\tseqlock_init(&sde->head_lock);\n\t\tspin_lock_init(&sde->senddmactrl_lock);\n\t\tspin_lock_init(&sde->flushlist_lock);\n\t\tseqlock_init(&sde->waitlock);\n\t\t/* insure there is always a zero bit */\n\t\tsde->ahg_bits = 0xfffffffe00000000ULL;\n\n\t\tsdma_set_state(sde, sdma_state_s00_hw_down);\n\n\t\t/* set up reference counting */\n\t\tkref_init(&sde->state.kref);\n\t\tinit_completion(&sde->state.comp);\n\n\t\tINIT_LIST_HEAD(&sde->flushlist);\n\t\tINIT_LIST_HEAD(&sde->dmawait);\n\n\t\tsde->tail_csr =\n\t\t\tget_kctxt_csr_addr(dd, this_idx, SD(TAIL));\n\n\t\ttasklet_init(&sde->sdma_hw_clean_up_task, sdma_hw_clean_up_task,\n\t\t\t     (unsigned long)sde);\n\n\t\ttasklet_init(&sde->sdma_sw_clean_up_task, sdma_sw_clean_up_task,\n\t\t\t     (unsigned long)sde);\n\t\tINIT_WORK(&sde->err_halt_worker, sdma_err_halt_wait);\n\t\tINIT_WORK(&sde->flush_worker, sdma_field_flush);\n\n\t\tsde->progress_check_head = 0;\n\n\t\ttimer_setup(&sde->err_progress_check_timer,\n\t\t\t    sdma_err_progress_check, 0);\n\n\t\tsde->descq = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\tdescq_cnt * sizeof(u64[2]),\n\t\t\t\t\t\t&sde->descq_phys, GFP_KERNEL);\n\t\tif (!sde->descq)\n\t\t\tgoto bail;\n\t\tsde->tx_ring =\n\t\t\tkvzalloc_node(array_size(descq_cnt,\n\t\t\t\t\t\t sizeof(struct sdma_txreq *)),\n\t\t\t\t      GFP_KERNEL, dd->node);\n\t\tif (!sde->tx_ring)\n\t\t\tgoto bail;\n\t}\n\n\tdd->sdma_heads_size = L1_CACHE_BYTES * num_engines;\n\t/* Allocate memory for DMA of head registers to memory */\n\tdd->sdma_heads_dma = dma_alloc_coherent(&dd->pcidev->dev,\n\t\t\t\t\t\tdd->sdma_heads_size,\n\t\t\t\t\t\t&dd->sdma_heads_phys,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!dd->sdma_heads_dma) {\n\t\tdd_dev_err(dd, \"failed to allocate SendDMA head memory\\n\");\n\t\tgoto bail;\n\t}\n\n\t/* Allocate memory for pad */\n\tdd->sdma_pad_dma = dma_alloc_coherent(&dd->pcidev->dev, sizeof(u32),\n\t\t\t\t\t      &dd->sdma_pad_phys, GFP_KERNEL);\n\tif (!dd->sdma_pad_dma) {\n\t\tdd_dev_err(dd, \"failed to allocate SendDMA pad memory\\n\");\n\t\tgoto bail;\n\t}\n\n\t/* assign each engine to different cacheline and init registers */\n\tcurr_head = (void *)dd->sdma_heads_dma;\n\tfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\n\t\tunsigned long phys_offset;\n\n\t\tsde = &dd->per_sdma[this_idx];\n\n\t\tsde->head_dma = curr_head;\n\t\tcurr_head += L1_CACHE_BYTES;\n\t\tphys_offset = (unsigned long)sde->head_dma -\n\t\t\t      (unsigned long)dd->sdma_heads_dma;\n\t\tsde->head_phys = dd->sdma_heads_phys + phys_offset;\n\t\tinit_sdma_regs(sde, per_sdma_credits, idle_cnt);\n\t}\n\tdd->flags |= HFI1_HAS_SEND_DMA;\n\tdd->flags |= idle_cnt ? HFI1_HAS_SDMA_TIMEOUT : 0;\n\tdd->num_sdma = num_engines;\n\tret = sdma_map_init(dd, port, ppd->vls_operational, NULL);\n\tif (ret < 0)\n\t\tgoto bail;\n\n\ttmp_sdma_rht = kzalloc(sizeof(*tmp_sdma_rht), GFP_KERNEL);\n\tif (!tmp_sdma_rht) {\n\t\tret = -ENOMEM;\n\t\tgoto bail;\n\t}\n\n\tret = rhashtable_init(tmp_sdma_rht, &sdma_rht_params);\n\tif (ret < 0) {\n\t\tkfree(tmp_sdma_rht);\n\t\tgoto bail;\n\t}\n\n\tdd->sdma_rht = tmp_sdma_rht;\n\n\tdd_dev_info(dd, \"SDMA num_sdma: %u\\n\", dd->num_sdma);\n\treturn 0;\n\nbail:\n\tsdma_clean(dd, num_engines);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -170,8 +170,11 @@\n \t}\n \n \tret = rhashtable_init(tmp_sdma_rht, &sdma_rht_params);\n-\tif (ret < 0)\n+\tif (ret < 0) {\n+\t\tkfree(tmp_sdma_rht);\n \t\tgoto bail;\n+\t}\n+\n \tdd->sdma_rht = tmp_sdma_rht;\n \n \tdd_dev_info(dd, \"SDMA num_sdma: %u\\n\", dd->num_sdma);",
        "function_modified_lines": {
            "added": [
                "\tif (ret < 0) {",
                "\t\tkfree(tmp_sdma_rht);",
                "\t}",
                ""
            ],
            "deleted": [
                "\tif (ret < 0)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the sdma_init() function in drivers/infiniband/hw/hfi1/sdma.c in the Linux kernel before 5.3.9 allows attackers to cause a denial of service (memory consumption) by triggering rhashtable_init() failures, aka CID-34b3be18a04e. NOTE: This has been disputed as not a vulnerability because \"rhashtable_init() can only fail if it is passed invalid values in the second parameter's struct, but when invoked from sdma_init() that is a pointer to a static const struct, so an attacker could only trigger failure if they could corrupt kernel memory (in which case a small memory leak is not a significant problem).",
        "id": 2146
    },
    {
        "cve_id": "CVE-2020-36312",
        "code_before_change": "void kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t       struct kvm_io_device *dev)\n{\n\tint i;\n\tstruct kvm_io_bus *new_bus, *bus;\n\n\tbus = kvm_get_bus(kvm, bus_idx);\n\tif (!bus)\n\t\treturn;\n\n\tfor (i = 0; i < bus->dev_count; i++)\n\t\tif (bus->range[i].dev == dev) {\n\t\t\tbreak;\n\t\t}\n\n\tif (i == bus->dev_count)\n\t\treturn;\n\n\tnew_bus = kmalloc(struct_size(bus, range, bus->dev_count - 1),\n\t\t\t  GFP_KERNEL_ACCOUNT);\n\tif (!new_bus)  {\n\t\tpr_err(\"kvm: failed to shrink bus, removing it completely\\n\");\n\t\tgoto broken;\n\t}\n\n\tmemcpy(new_bus, bus, sizeof(*bus) + i * sizeof(struct kvm_io_range));\n\tnew_bus->dev_count--;\n\tmemcpy(new_bus->range + i, bus->range + i + 1,\n\t       (new_bus->dev_count - i) * sizeof(struct kvm_io_range));\n\nbroken:\n\trcu_assign_pointer(kvm->buses[bus_idx], new_bus);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\tkfree(bus);\n\treturn;\n}",
        "code_after_change": "void kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t       struct kvm_io_device *dev)\n{\n\tint i, j;\n\tstruct kvm_io_bus *new_bus, *bus;\n\n\tbus = kvm_get_bus(kvm, bus_idx);\n\tif (!bus)\n\t\treturn;\n\n\tfor (i = 0; i < bus->dev_count; i++)\n\t\tif (bus->range[i].dev == dev) {\n\t\t\tbreak;\n\t\t}\n\n\tif (i == bus->dev_count)\n\t\treturn;\n\n\tnew_bus = kmalloc(struct_size(bus, range, bus->dev_count - 1),\n\t\t\t  GFP_KERNEL_ACCOUNT);\n\tif (new_bus) {\n\t\tmemcpy(new_bus, bus, sizeof(*bus) + i * sizeof(struct kvm_io_range));\n\t\tnew_bus->dev_count--;\n\t\tmemcpy(new_bus->range + i, bus->range + i + 1,\n\t\t       (new_bus->dev_count - i) * sizeof(struct kvm_io_range));\n\t} else {\n\t\tpr_err(\"kvm: failed to shrink bus, removing it completely\\n\");\n\t\tfor (j = 0; j < bus->dev_count; j++) {\n\t\t\tif (j == i)\n\t\t\t\tcontinue;\n\t\t\tkvm_iodevice_destructor(bus->range[j].dev);\n\t\t}\n\t}\n\n\trcu_assign_pointer(kvm->buses[bus_idx], new_bus);\n\tsynchronize_srcu_expedited(&kvm->srcu);\n\tkfree(bus);\n\treturn;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n void kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,\n \t\t\t       struct kvm_io_device *dev)\n {\n-\tint i;\n+\tint i, j;\n \tstruct kvm_io_bus *new_bus, *bus;\n \n \tbus = kvm_get_bus(kvm, bus_idx);\n@@ -18,17 +18,20 @@\n \n \tnew_bus = kmalloc(struct_size(bus, range, bus->dev_count - 1),\n \t\t\t  GFP_KERNEL_ACCOUNT);\n-\tif (!new_bus)  {\n+\tif (new_bus) {\n+\t\tmemcpy(new_bus, bus, sizeof(*bus) + i * sizeof(struct kvm_io_range));\n+\t\tnew_bus->dev_count--;\n+\t\tmemcpy(new_bus->range + i, bus->range + i + 1,\n+\t\t       (new_bus->dev_count - i) * sizeof(struct kvm_io_range));\n+\t} else {\n \t\tpr_err(\"kvm: failed to shrink bus, removing it completely\\n\");\n-\t\tgoto broken;\n+\t\tfor (j = 0; j < bus->dev_count; j++) {\n+\t\t\tif (j == i)\n+\t\t\t\tcontinue;\n+\t\t\tkvm_iodevice_destructor(bus->range[j].dev);\n+\t\t}\n \t}\n \n-\tmemcpy(new_bus, bus, sizeof(*bus) + i * sizeof(struct kvm_io_range));\n-\tnew_bus->dev_count--;\n-\tmemcpy(new_bus->range + i, bus->range + i + 1,\n-\t       (new_bus->dev_count - i) * sizeof(struct kvm_io_range));\n-\n-broken:\n \trcu_assign_pointer(kvm->buses[bus_idx], new_bus);\n \tsynchronize_srcu_expedited(&kvm->srcu);\n \tkfree(bus);",
        "function_modified_lines": {
            "added": [
                "\tint i, j;",
                "\tif (new_bus) {",
                "\t\tmemcpy(new_bus, bus, sizeof(*bus) + i * sizeof(struct kvm_io_range));",
                "\t\tnew_bus->dev_count--;",
                "\t\tmemcpy(new_bus->range + i, bus->range + i + 1,",
                "\t\t       (new_bus->dev_count - i) * sizeof(struct kvm_io_range));",
                "\t} else {",
                "\t\tfor (j = 0; j < bus->dev_count; j++) {",
                "\t\t\tif (j == i)",
                "\t\t\t\tcontinue;",
                "\t\t\tkvm_iodevice_destructor(bus->range[j].dev);",
                "\t\t}"
            ],
            "deleted": [
                "\tint i;",
                "\tif (!new_bus)  {",
                "\t\tgoto broken;",
                "\tmemcpy(new_bus, bus, sizeof(*bus) + i * sizeof(struct kvm_io_range));",
                "\tnew_bus->dev_count--;",
                "\tmemcpy(new_bus->range + i, bus->range + i + 1,",
                "\t       (new_bus->dev_count - i) * sizeof(struct kvm_io_range));",
                "",
                "broken:"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.8.10. virt/kvm/kvm_main.c has a kvm_io_bus_unregister_dev memory leak upon a kmalloc failure, aka CID-f65886606c2d.",
        "id": 2716
    },
    {
        "cve_id": "CVE-2023-0597",
        "code_before_change": "static inline bool within_cpu_entry(unsigned long addr, unsigned long end)\n{\n\tint cpu;\n\n\t/* CPU entry erea is always used for CPU entry */\n\tif (within_area(addr, end, CPU_ENTRY_AREA_BASE,\n\t\t\tCPU_ENTRY_AREA_TOTAL_SIZE))\n\t\treturn true;\n\n\t/*\n\t * When FSGSBASE is enabled, paranoid_entry() fetches the per-CPU\n\t * GSBASE value via __per_cpu_offset or pcpu_unit_offsets.\n\t */\n#ifdef CONFIG_SMP\n\tif (within_area(addr, end, (unsigned long)__per_cpu_offset,\n\t\t\tsizeof(unsigned long) * nr_cpu_ids))\n\t\treturn true;\n#else\n\tif (within_area(addr, end, (unsigned long)&pcpu_unit_offsets,\n\t\t\tsizeof(pcpu_unit_offsets)))\n\t\treturn true;\n#endif\n\n\tfor_each_possible_cpu(cpu) {\n\t\t/* The original rw GDT is being used after load_direct_gdt() */\n\t\tif (within_area(addr, end, (unsigned long)get_cpu_gdt_rw(cpu),\n\t\t\t\tGDT_SIZE))\n\t\t\treturn true;\n\n\t\t/*\n\t\t * cpu_tss_rw is not directly referenced by hardware, but\n\t\t * cpu_tss_rw is also used in CPU entry code,\n\t\t */\n\t\tif (within_area(addr, end,\n\t\t\t\t(unsigned long)&per_cpu(cpu_tss_rw, cpu),\n\t\t\t\tsizeof(struct tss_struct)))\n\t\t\treturn true;\n\n\t\t/*\n\t\t * cpu_tlbstate.user_pcid_flush_mask is used for CPU entry.\n\t\t * If a data breakpoint on it, it will cause an unwanted #DB.\n\t\t * Protect the full cpu_tlbstate structure to be sure.\n\t\t */\n\t\tif (within_area(addr, end,\n\t\t\t\t(unsigned long)&per_cpu(cpu_tlbstate, cpu),\n\t\t\t\tsizeof(struct tlb_state)))\n\t\t\treturn true;\n\n\t\t/*\n\t\t * When in guest (X86_FEATURE_HYPERVISOR), local_db_save()\n\t\t * will read per-cpu cpu_dr7 before clear dr7 register.\n\t\t */\n\t\tif (within_area(addr, end, (unsigned long)&per_cpu(cpu_dr7, cpu),\n\t\t\t\tsizeof(cpu_dr7)))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}",
        "code_after_change": "static inline bool within_cpu_entry(unsigned long addr, unsigned long end)\n{\n\tint cpu;\n\n\t/* CPU entry erea is always used for CPU entry */\n\tif (within_area(addr, end, CPU_ENTRY_AREA_BASE,\n\t\t\tCPU_ENTRY_AREA_MAP_SIZE))\n\t\treturn true;\n\n\t/*\n\t * When FSGSBASE is enabled, paranoid_entry() fetches the per-CPU\n\t * GSBASE value via __per_cpu_offset or pcpu_unit_offsets.\n\t */\n#ifdef CONFIG_SMP\n\tif (within_area(addr, end, (unsigned long)__per_cpu_offset,\n\t\t\tsizeof(unsigned long) * nr_cpu_ids))\n\t\treturn true;\n#else\n\tif (within_area(addr, end, (unsigned long)&pcpu_unit_offsets,\n\t\t\tsizeof(pcpu_unit_offsets)))\n\t\treturn true;\n#endif\n\n\tfor_each_possible_cpu(cpu) {\n\t\t/* The original rw GDT is being used after load_direct_gdt() */\n\t\tif (within_area(addr, end, (unsigned long)get_cpu_gdt_rw(cpu),\n\t\t\t\tGDT_SIZE))\n\t\t\treturn true;\n\n\t\t/*\n\t\t * cpu_tss_rw is not directly referenced by hardware, but\n\t\t * cpu_tss_rw is also used in CPU entry code,\n\t\t */\n\t\tif (within_area(addr, end,\n\t\t\t\t(unsigned long)&per_cpu(cpu_tss_rw, cpu),\n\t\t\t\tsizeof(struct tss_struct)))\n\t\t\treturn true;\n\n\t\t/*\n\t\t * cpu_tlbstate.user_pcid_flush_mask is used for CPU entry.\n\t\t * If a data breakpoint on it, it will cause an unwanted #DB.\n\t\t * Protect the full cpu_tlbstate structure to be sure.\n\t\t */\n\t\tif (within_area(addr, end,\n\t\t\t\t(unsigned long)&per_cpu(cpu_tlbstate, cpu),\n\t\t\t\tsizeof(struct tlb_state)))\n\t\t\treturn true;\n\n\t\t/*\n\t\t * When in guest (X86_FEATURE_HYPERVISOR), local_db_save()\n\t\t * will read per-cpu cpu_dr7 before clear dr7 register.\n\t\t */\n\t\tif (within_area(addr, end, (unsigned long)&per_cpu(cpu_dr7, cpu),\n\t\t\t\tsizeof(cpu_dr7)))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \n \t/* CPU entry erea is always used for CPU entry */\n \tif (within_area(addr, end, CPU_ENTRY_AREA_BASE,\n-\t\t\tCPU_ENTRY_AREA_TOTAL_SIZE))\n+\t\t\tCPU_ENTRY_AREA_MAP_SIZE))\n \t\treturn true;\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "\t\t\tCPU_ENTRY_AREA_MAP_SIZE))"
            ],
            "deleted": [
                "\t\t\tCPU_ENTRY_AREA_TOTAL_SIZE))"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw possibility of memory leak in the Linux kernel cpu_entry_area mapping of X86 CPU data to memory was found in the way user can guess location of exception stack(s) or other important data. A local user could use this flaw to get access to some important data with expected location in memory.",
        "id": 3833
    },
    {
        "cve_id": "CVE-2023-7192",
        "code_before_change": "static struct nf_conn *\nctnetlink_create_conntrack(struct net *net,\n\t\t\t   const struct nf_conntrack_zone *zone,\n\t\t\t   const struct nlattr * const cda[],\n\t\t\t   struct nf_conntrack_tuple *otuple,\n\t\t\t   struct nf_conntrack_tuple *rtuple,\n\t\t\t   u8 u3)\n{\n\tstruct nf_conn *ct;\n\tint err = -EINVAL;\n\tstruct nf_conntrack_helper *helper;\n\tstruct nf_conn_tstamp *tstamp;\n\tu64 timeout;\n\n\tct = nf_conntrack_alloc(net, zone, otuple, rtuple, GFP_ATOMIC);\n\tif (IS_ERR(ct))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!cda[CTA_TIMEOUT])\n\t\tgoto err1;\n\n\ttimeout = (u64)ntohl(nla_get_be32(cda[CTA_TIMEOUT])) * HZ;\n\t__nf_ct_set_timeout(ct, timeout);\n\n\trcu_read_lock();\n \tif (cda[CTA_HELP]) {\n\t\tchar *helpname = NULL;\n\t\tstruct nlattr *helpinfo = NULL;\n\n\t\terr = ctnetlink_parse_help(cda[CTA_HELP], &helpname, &helpinfo);\n \t\tif (err < 0)\n\t\t\tgoto err2;\n\n\t\thelper = __nf_conntrack_helper_find(helpname, nf_ct_l3num(ct),\n\t\t\t\t\t\t    nf_ct_protonum(ct));\n\t\tif (helper == NULL) {\n\t\t\trcu_read_unlock();\n#ifdef CONFIG_MODULES\n\t\t\tif (request_module(\"nfct-helper-%s\", helpname) < 0) {\n\t\t\t\terr = -EOPNOTSUPP;\n\t\t\t\tgoto err1;\n\t\t\t}\n\n\t\t\trcu_read_lock();\n\t\t\thelper = __nf_conntrack_helper_find(helpname,\n\t\t\t\t\t\t\t    nf_ct_l3num(ct),\n\t\t\t\t\t\t\t    nf_ct_protonum(ct));\n\t\t\tif (helper) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto err2;\n\t\t\t}\n\t\t\trcu_read_unlock();\n#endif\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err1;\n\t\t} else {\n\t\t\tstruct nf_conn_help *help;\n\n\t\t\thelp = nf_ct_helper_ext_add(ct, GFP_ATOMIC);\n\t\t\tif (help == NULL) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto err2;\n\t\t\t}\n\t\t\t/* set private helper data if allowed. */\n\t\t\tif (helper->from_nlattr)\n\t\t\t\thelper->from_nlattr(helpinfo, ct);\n\n\t\t\t/* disable helper auto-assignment for this entry */\n\t\t\tct->status |= IPS_HELPER;\n\t\t\tRCU_INIT_POINTER(help->helper, helper);\n\t\t}\n\t}\n\n\terr = ctnetlink_setup_nat(ct, cda);\n\tif (err < 0)\n\t\tgoto err2;\n\n\tnf_ct_acct_ext_add(ct, GFP_ATOMIC);\n\tnf_ct_tstamp_ext_add(ct, GFP_ATOMIC);\n\tnf_ct_ecache_ext_add(ct, 0, 0, GFP_ATOMIC);\n\tnf_ct_labels_ext_add(ct);\n\tnfct_seqadj_ext_add(ct);\n\tnfct_synproxy_ext_add(ct);\n\n\t/* we must add conntrack extensions before confirmation. */\n\tct->status |= IPS_CONFIRMED;\n\n\tif (cda[CTA_STATUS]) {\n\t\terr = ctnetlink_change_status(ct, cda);\n\t\tif (err < 0)\n\t\t\tgoto err2;\n\t}\n\n\tif (cda[CTA_SEQ_ADJ_ORIG] || cda[CTA_SEQ_ADJ_REPLY]) {\n\t\terr = ctnetlink_change_seq_adj(ct, cda);\n\t\tif (err < 0)\n\t\t\tgoto err2;\n\t}\n\n\tmemset(&ct->proto, 0, sizeof(ct->proto));\n\tif (cda[CTA_PROTOINFO]) {\n\t\terr = ctnetlink_change_protoinfo(ct, cda);\n\t\tif (err < 0)\n\t\t\tgoto err2;\n\t}\n\n\tif (cda[CTA_SYNPROXY]) {\n\t\terr = ctnetlink_change_synproxy(ct, cda);\n\t\tif (err < 0)\n\t\t\tgoto err2;\n\t}\n\n#if defined(CONFIG_NF_CONNTRACK_MARK)\n\tif (cda[CTA_MARK])\n\t\tctnetlink_change_mark(ct, cda);\n#endif\n\n\t/* setup master conntrack: this is a confirmed expectation */\n\tif (cda[CTA_TUPLE_MASTER]) {\n\t\tstruct nf_conntrack_tuple master;\n\t\tstruct nf_conntrack_tuple_hash *master_h;\n\t\tstruct nf_conn *master_ct;\n\n\t\terr = ctnetlink_parse_tuple(cda, &master, CTA_TUPLE_MASTER,\n\t\t\t\t\t    u3, NULL);\n\t\tif (err < 0)\n\t\t\tgoto err2;\n\n\t\tmaster_h = nf_conntrack_find_get(net, zone, &master);\n\t\tif (master_h == NULL) {\n\t\t\terr = -ENOENT;\n\t\t\tgoto err2;\n\t\t}\n\t\tmaster_ct = nf_ct_tuplehash_to_ctrack(master_h);\n\t\t__set_bit(IPS_EXPECTED_BIT, &ct->status);\n\t\tct->master = master_ct;\n\t}\n\ttstamp = nf_conn_tstamp_find(ct);\n\tif (tstamp)\n\t\ttstamp->start = ktime_get_real_ns();\n\n\terr = nf_conntrack_hash_check_insert(ct);\n\tif (err < 0)\n\t\tgoto err2;\n\n\trcu_read_unlock();\n\n\treturn ct;\n\nerr2:\n\trcu_read_unlock();\nerr1:\n\tnf_conntrack_free(ct);\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "static struct nf_conn *\nctnetlink_create_conntrack(struct net *net,\n\t\t\t   const struct nf_conntrack_zone *zone,\n\t\t\t   const struct nlattr * const cda[],\n\t\t\t   struct nf_conntrack_tuple *otuple,\n\t\t\t   struct nf_conntrack_tuple *rtuple,\n\t\t\t   u8 u3)\n{\n\tstruct nf_conn *ct;\n\tint err = -EINVAL;\n\tstruct nf_conntrack_helper *helper;\n\tstruct nf_conn_tstamp *tstamp;\n\tu64 timeout;\n\n\tct = nf_conntrack_alloc(net, zone, otuple, rtuple, GFP_ATOMIC);\n\tif (IS_ERR(ct))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!cda[CTA_TIMEOUT])\n\t\tgoto err1;\n\n\ttimeout = (u64)ntohl(nla_get_be32(cda[CTA_TIMEOUT])) * HZ;\n\t__nf_ct_set_timeout(ct, timeout);\n\n\trcu_read_lock();\n \tif (cda[CTA_HELP]) {\n\t\tchar *helpname = NULL;\n\t\tstruct nlattr *helpinfo = NULL;\n\n\t\terr = ctnetlink_parse_help(cda[CTA_HELP], &helpname, &helpinfo);\n \t\tif (err < 0)\n\t\t\tgoto err2;\n\n\t\thelper = __nf_conntrack_helper_find(helpname, nf_ct_l3num(ct),\n\t\t\t\t\t\t    nf_ct_protonum(ct));\n\t\tif (helper == NULL) {\n\t\t\trcu_read_unlock();\n#ifdef CONFIG_MODULES\n\t\t\tif (request_module(\"nfct-helper-%s\", helpname) < 0) {\n\t\t\t\terr = -EOPNOTSUPP;\n\t\t\t\tgoto err1;\n\t\t\t}\n\n\t\t\trcu_read_lock();\n\t\t\thelper = __nf_conntrack_helper_find(helpname,\n\t\t\t\t\t\t\t    nf_ct_l3num(ct),\n\t\t\t\t\t\t\t    nf_ct_protonum(ct));\n\t\t\tif (helper) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto err2;\n\t\t\t}\n\t\t\trcu_read_unlock();\n#endif\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err1;\n\t\t} else {\n\t\t\tstruct nf_conn_help *help;\n\n\t\t\thelp = nf_ct_helper_ext_add(ct, GFP_ATOMIC);\n\t\t\tif (help == NULL) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto err2;\n\t\t\t}\n\t\t\t/* set private helper data if allowed. */\n\t\t\tif (helper->from_nlattr)\n\t\t\t\thelper->from_nlattr(helpinfo, ct);\n\n\t\t\t/* disable helper auto-assignment for this entry */\n\t\t\tct->status |= IPS_HELPER;\n\t\t\tRCU_INIT_POINTER(help->helper, helper);\n\t\t}\n\t}\n\n\terr = ctnetlink_setup_nat(ct, cda);\n\tif (err < 0)\n\t\tgoto err2;\n\n\tnf_ct_acct_ext_add(ct, GFP_ATOMIC);\n\tnf_ct_tstamp_ext_add(ct, GFP_ATOMIC);\n\tnf_ct_ecache_ext_add(ct, 0, 0, GFP_ATOMIC);\n\tnf_ct_labels_ext_add(ct);\n\tnfct_seqadj_ext_add(ct);\n\tnfct_synproxy_ext_add(ct);\n\n\t/* we must add conntrack extensions before confirmation. */\n\tct->status |= IPS_CONFIRMED;\n\n\tif (cda[CTA_STATUS]) {\n\t\terr = ctnetlink_change_status(ct, cda);\n\t\tif (err < 0)\n\t\t\tgoto err2;\n\t}\n\n\tif (cda[CTA_SEQ_ADJ_ORIG] || cda[CTA_SEQ_ADJ_REPLY]) {\n\t\terr = ctnetlink_change_seq_adj(ct, cda);\n\t\tif (err < 0)\n\t\t\tgoto err2;\n\t}\n\n\tmemset(&ct->proto, 0, sizeof(ct->proto));\n\tif (cda[CTA_PROTOINFO]) {\n\t\terr = ctnetlink_change_protoinfo(ct, cda);\n\t\tif (err < 0)\n\t\t\tgoto err2;\n\t}\n\n\tif (cda[CTA_SYNPROXY]) {\n\t\terr = ctnetlink_change_synproxy(ct, cda);\n\t\tif (err < 0)\n\t\t\tgoto err2;\n\t}\n\n#if defined(CONFIG_NF_CONNTRACK_MARK)\n\tif (cda[CTA_MARK])\n\t\tctnetlink_change_mark(ct, cda);\n#endif\n\n\t/* setup master conntrack: this is a confirmed expectation */\n\tif (cda[CTA_TUPLE_MASTER]) {\n\t\tstruct nf_conntrack_tuple master;\n\t\tstruct nf_conntrack_tuple_hash *master_h;\n\t\tstruct nf_conn *master_ct;\n\n\t\terr = ctnetlink_parse_tuple(cda, &master, CTA_TUPLE_MASTER,\n\t\t\t\t\t    u3, NULL);\n\t\tif (err < 0)\n\t\t\tgoto err2;\n\n\t\tmaster_h = nf_conntrack_find_get(net, zone, &master);\n\t\tif (master_h == NULL) {\n\t\t\terr = -ENOENT;\n\t\t\tgoto err2;\n\t\t}\n\t\tmaster_ct = nf_ct_tuplehash_to_ctrack(master_h);\n\t\t__set_bit(IPS_EXPECTED_BIT, &ct->status);\n\t\tct->master = master_ct;\n\t}\n\ttstamp = nf_conn_tstamp_find(ct);\n\tif (tstamp)\n\t\ttstamp->start = ktime_get_real_ns();\n\n\terr = nf_conntrack_hash_check_insert(ct);\n\tif (err < 0)\n\t\tgoto err3;\n\n\trcu_read_unlock();\n\n\treturn ct;\n\nerr3:\n\tif (ct->master)\n\t\tnf_ct_put(ct->master);\nerr2:\n\trcu_read_unlock();\nerr1:\n\tnf_conntrack_free(ct);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -141,12 +141,15 @@\n \n \terr = nf_conntrack_hash_check_insert(ct);\n \tif (err < 0)\n-\t\tgoto err2;\n+\t\tgoto err3;\n \n \trcu_read_unlock();\n \n \treturn ct;\n \n+err3:\n+\tif (ct->master)\n+\t\tnf_ct_put(ct->master);\n err2:\n \trcu_read_unlock();\n err1:",
        "function_modified_lines": {
            "added": [
                "\t\tgoto err3;",
                "err3:",
                "\tif (ct->master)",
                "\t\tnf_ct_put(ct->master);"
            ],
            "deleted": [
                "\t\tgoto err2;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak problem was found in ctnetlink_create_conntrack in net/netfilter/nf_conntrack_netlink.c in the Linux Kernel. This issue may allow a local attacker with CAP_NET_ADMIN privileges to cause a denial of service (DoS) attack due to a refcount overflow.",
        "id": 4312
    },
    {
        "cve_id": "CVE-2019-19077",
        "code_before_change": "int bnxt_re_create_srq(struct ib_srq *ib_srq,\n\t\t       struct ib_srq_init_attr *srq_init_attr,\n\t\t       struct ib_udata *udata)\n{\n\tstruct ib_pd *ib_pd = ib_srq->pd;\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tstruct bnxt_re_srq *srq =\n\t\tcontainer_of(ib_srq, struct bnxt_re_srq, ib_srq);\n\tstruct bnxt_qplib_nq *nq = NULL;\n\tint rc, entries;\n\n\tif (srq_init_attr->attr.max_wr >= dev_attr->max_srq_wqes) {\n\t\tdev_err(rdev_to_dev(rdev), \"Create CQ failed - max exceeded\");\n\t\trc = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tif (srq_init_attr->srq_type != IB_SRQT_BASIC) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto exit;\n\t}\n\n\tsrq->rdev = rdev;\n\tsrq->qplib_srq.pd = &pd->qplib_pd;\n\tsrq->qplib_srq.dpi = &rdev->dpi_privileged;\n\t/* Allocate 1 more than what's provided so posting max doesn't\n\t * mean empty\n\t */\n\tentries = roundup_pow_of_two(srq_init_attr->attr.max_wr + 1);\n\tif (entries > dev_attr->max_srq_wqes + 1)\n\t\tentries = dev_attr->max_srq_wqes + 1;\n\n\tsrq->qplib_srq.max_wqe = entries;\n\tsrq->qplib_srq.max_sge = srq_init_attr->attr.max_sge;\n\tsrq->qplib_srq.threshold = srq_init_attr->attr.srq_limit;\n\tsrq->srq_limit = srq_init_attr->attr.srq_limit;\n\tsrq->qplib_srq.eventq_hw_ring_id = rdev->nq[0].ring_id;\n\tnq = &rdev->nq[0];\n\n\tif (udata) {\n\t\trc = bnxt_re_init_user_srq(rdev, pd, srq, udata);\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\n\trc = bnxt_qplib_create_srq(&rdev->qplib_res, &srq->qplib_srq);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Create HW SRQ failed!\");\n\t\tgoto fail;\n\t}\n\n\tif (udata) {\n\t\tstruct bnxt_re_srq_resp resp;\n\n\t\tresp.srqid = srq->qplib_srq.id;\n\t\trc = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"SRQ copy to udata failed!\");\n\t\t\tbnxt_qplib_destroy_srq(&rdev->qplib_res,\n\t\t\t\t\t       &srq->qplib_srq);\n\t\t\tgoto exit;\n\t\t}\n\t}\n\tif (nq)\n\t\tnq->budget++;\n\tatomic_inc(&rdev->srq_count);\n\n\treturn 0;\n\nfail:\n\tib_umem_release(srq->umem);\nexit:\n\treturn rc;\n}",
        "code_after_change": "int bnxt_re_create_srq(struct ib_srq *ib_srq,\n\t\t       struct ib_srq_init_attr *srq_init_attr,\n\t\t       struct ib_udata *udata)\n{\n\tstruct ib_pd *ib_pd = ib_srq->pd;\n\tstruct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);\n\tstruct bnxt_re_dev *rdev = pd->rdev;\n\tstruct bnxt_qplib_dev_attr *dev_attr = &rdev->dev_attr;\n\tstruct bnxt_re_srq *srq =\n\t\tcontainer_of(ib_srq, struct bnxt_re_srq, ib_srq);\n\tstruct bnxt_qplib_nq *nq = NULL;\n\tint rc, entries;\n\n\tif (srq_init_attr->attr.max_wr >= dev_attr->max_srq_wqes) {\n\t\tdev_err(rdev_to_dev(rdev), \"Create CQ failed - max exceeded\");\n\t\trc = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tif (srq_init_attr->srq_type != IB_SRQT_BASIC) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto exit;\n\t}\n\n\tsrq->rdev = rdev;\n\tsrq->qplib_srq.pd = &pd->qplib_pd;\n\tsrq->qplib_srq.dpi = &rdev->dpi_privileged;\n\t/* Allocate 1 more than what's provided so posting max doesn't\n\t * mean empty\n\t */\n\tentries = roundup_pow_of_two(srq_init_attr->attr.max_wr + 1);\n\tif (entries > dev_attr->max_srq_wqes + 1)\n\t\tentries = dev_attr->max_srq_wqes + 1;\n\n\tsrq->qplib_srq.max_wqe = entries;\n\tsrq->qplib_srq.max_sge = srq_init_attr->attr.max_sge;\n\tsrq->qplib_srq.threshold = srq_init_attr->attr.srq_limit;\n\tsrq->srq_limit = srq_init_attr->attr.srq_limit;\n\tsrq->qplib_srq.eventq_hw_ring_id = rdev->nq[0].ring_id;\n\tnq = &rdev->nq[0];\n\n\tif (udata) {\n\t\trc = bnxt_re_init_user_srq(rdev, pd, srq, udata);\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\n\trc = bnxt_qplib_create_srq(&rdev->qplib_res, &srq->qplib_srq);\n\tif (rc) {\n\t\tdev_err(rdev_to_dev(rdev), \"Create HW SRQ failed!\");\n\t\tgoto fail;\n\t}\n\n\tif (udata) {\n\t\tstruct bnxt_re_srq_resp resp;\n\n\t\tresp.srqid = srq->qplib_srq.id;\n\t\trc = ib_copy_to_udata(udata, &resp, sizeof(resp));\n\t\tif (rc) {\n\t\t\tdev_err(rdev_to_dev(rdev), \"SRQ copy to udata failed!\");\n\t\t\tbnxt_qplib_destroy_srq(&rdev->qplib_res,\n\t\t\t\t\t       &srq->qplib_srq);\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tif (nq)\n\t\tnq->budget++;\n\tatomic_inc(&rdev->srq_count);\n\n\treturn 0;\n\nfail:\n\tib_umem_release(srq->umem);\nexit:\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -60,7 +60,7 @@\n \t\t\tdev_err(rdev_to_dev(rdev), \"SRQ copy to udata failed!\");\n \t\t\tbnxt_qplib_destroy_srq(&rdev->qplib_res,\n \t\t\t\t\t       &srq->qplib_srq);\n-\t\t\tgoto exit;\n+\t\t\tgoto fail;\n \t\t}\n \t}\n \tif (nq)",
        "function_modified_lines": {
            "added": [
                "\t\t\tgoto fail;"
            ],
            "deleted": [
                "\t\t\tgoto exit;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the bnxt_re_create_srq() function in drivers/infiniband/hw/bnxt_re/ib_verbs.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering copy to udata failures, aka CID-4a9d46a9fe14.",
        "id": 2160
    },
    {
        "cve_id": "CVE-2019-19061",
        "code_before_change": "static int adis_update_scan_mode_burst(struct iio_dev *indio_dev,\n\tconst unsigned long *scan_mask)\n{\n\tstruct adis *adis = iio_device_get_drvdata(indio_dev);\n\tunsigned int burst_length;\n\tu8 *tx;\n\n\t/* All but the timestamp channel */\n\tburst_length = (indio_dev->num_channels - 1) * sizeof(u16);\n\tburst_length += adis->burst->extra_len;\n\n\tadis->xfer = kcalloc(2, sizeof(*adis->xfer), GFP_KERNEL);\n\tif (!adis->xfer)\n\t\treturn -ENOMEM;\n\n\tadis->buffer = kzalloc(burst_length + sizeof(u16), GFP_KERNEL);\n\tif (!adis->buffer)\n\t\treturn -ENOMEM;\n\n\ttx = adis->buffer + burst_length;\n\ttx[0] = ADIS_READ_REG(adis->burst->reg_cmd);\n\ttx[1] = 0;\n\n\tadis->xfer[0].tx_buf = tx;\n\tadis->xfer[0].bits_per_word = 8;\n\tadis->xfer[0].len = 2;\n\tadis->xfer[1].rx_buf = adis->buffer;\n\tadis->xfer[1].bits_per_word = 8;\n\tadis->xfer[1].len = burst_length;\n\n\tspi_message_init(&adis->msg);\n\tspi_message_add_tail(&adis->xfer[0], &adis->msg);\n\tspi_message_add_tail(&adis->xfer[1], &adis->msg);\n\n\treturn 0;\n}",
        "code_after_change": "static int adis_update_scan_mode_burst(struct iio_dev *indio_dev,\n\tconst unsigned long *scan_mask)\n{\n\tstruct adis *adis = iio_device_get_drvdata(indio_dev);\n\tunsigned int burst_length;\n\tu8 *tx;\n\n\t/* All but the timestamp channel */\n\tburst_length = (indio_dev->num_channels - 1) * sizeof(u16);\n\tburst_length += adis->burst->extra_len;\n\n\tadis->xfer = kcalloc(2, sizeof(*adis->xfer), GFP_KERNEL);\n\tif (!adis->xfer)\n\t\treturn -ENOMEM;\n\n\tadis->buffer = kzalloc(burst_length + sizeof(u16), GFP_KERNEL);\n\tif (!adis->buffer) {\n\t\tkfree(adis->xfer);\n\t\tadis->xfer = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\ttx = adis->buffer + burst_length;\n\ttx[0] = ADIS_READ_REG(adis->burst->reg_cmd);\n\ttx[1] = 0;\n\n\tadis->xfer[0].tx_buf = tx;\n\tadis->xfer[0].bits_per_word = 8;\n\tadis->xfer[0].len = 2;\n\tadis->xfer[1].rx_buf = adis->buffer;\n\tadis->xfer[1].bits_per_word = 8;\n\tadis->xfer[1].len = burst_length;\n\n\tspi_message_init(&adis->msg);\n\tspi_message_add_tail(&adis->xfer[0], &adis->msg);\n\tspi_message_add_tail(&adis->xfer[1], &adis->msg);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,8 +14,11 @@\n \t\treturn -ENOMEM;\n \n \tadis->buffer = kzalloc(burst_length + sizeof(u16), GFP_KERNEL);\n-\tif (!adis->buffer)\n+\tif (!adis->buffer) {\n+\t\tkfree(adis->xfer);\n+\t\tadis->xfer = NULL;\n \t\treturn -ENOMEM;\n+\t}\n \n \ttx = adis->buffer + burst_length;\n \ttx[0] = ADIS_READ_REG(adis->burst->reg_cmd);",
        "function_modified_lines": {
            "added": [
                "\tif (!adis->buffer) {",
                "\t\tkfree(adis->xfer);",
                "\t\tadis->xfer = NULL;",
                "\t}"
            ],
            "deleted": [
                "\tif (!adis->buffer)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the adis_update_scan_mode_burst() function in drivers/iio/imu/adis_buffer.c in the Linux kernel before 5.3.9 allows attackers to cause a denial of service (memory consumption), aka CID-9c0530e898f3.",
        "id": 2142
    },
    {
        "cve_id": "CVE-2020-15393",
        "code_before_change": "static void usbtest_disconnect(struct usb_interface *intf)\n{\n\tstruct usbtest_dev\t*dev = usb_get_intfdata(intf);\n\n\tusb_set_intfdata(intf, NULL);\n\tdev_dbg(&intf->dev, \"disconnect\\n\");\n\tkfree(dev);\n}",
        "code_after_change": "static void usbtest_disconnect(struct usb_interface *intf)\n{\n\tstruct usbtest_dev\t*dev = usb_get_intfdata(intf);\n\n\tusb_set_intfdata(intf, NULL);\n\tdev_dbg(&intf->dev, \"disconnect\\n\");\n\tkfree(dev->buf);\n\tkfree(dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,5 +4,6 @@\n \n \tusb_set_intfdata(intf, NULL);\n \tdev_dbg(&intf->dev, \"disconnect\\n\");\n+\tkfree(dev->buf);\n \tkfree(dev);\n }",
        "function_modified_lines": {
            "added": [
                "\tkfree(dev->buf);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "In the Linux kernel 4.4 through 5.7.6, usbtest_disconnect in drivers/usb/misc/usbtest.c has a memory leak, aka CID-28ebeb8db770.",
        "id": 2542
    },
    {
        "cve_id": "CVE-2019-19047",
        "code_before_change": "static int\nmlx5_fw_fatal_reporter_dump(struct devlink_health_reporter *reporter,\n\t\t\t    struct devlink_fmsg *fmsg, void *priv_ctx)\n{\n\tstruct mlx5_core_dev *dev = devlink_health_reporter_priv(reporter);\n\tu32 crdump_size = dev->priv.health.crdump_size;\n\tu32 *cr_data;\n\tu32 data_size;\n\tu32 offset;\n\tint err;\n\n\tif (!mlx5_core_is_pf(dev))\n\t\treturn -EPERM;\n\n\tcr_data = kvmalloc(crdump_size, GFP_KERNEL);\n\tif (!cr_data)\n\t\treturn -ENOMEM;\n\terr = mlx5_crdump_collect(dev, cr_data);\n\tif (err)\n\t\treturn err;\n\n\tif (priv_ctx) {\n\t\tstruct mlx5_fw_reporter_ctx *fw_reporter_ctx = priv_ctx;\n\n\t\terr = mlx5_fw_reporter_ctx_pairs_put(fmsg, fw_reporter_ctx);\n\t\tif (err)\n\t\t\tgoto free_data;\n\t}\n\n\terr = devlink_fmsg_arr_pair_nest_start(fmsg, \"crdump_data\");\n\tif (err)\n\t\tgoto free_data;\n\tfor (offset = 0; offset < crdump_size; offset += data_size) {\n\t\tif (crdump_size - offset < MLX5_CR_DUMP_CHUNK_SIZE)\n\t\t\tdata_size = crdump_size - offset;\n\t\telse\n\t\t\tdata_size = MLX5_CR_DUMP_CHUNK_SIZE;\n\t\terr = devlink_fmsg_binary_put(fmsg, (char *)cr_data + offset,\n\t\t\t\t\t      data_size);\n\t\tif (err)\n\t\t\tgoto free_data;\n\t}\n\terr = devlink_fmsg_arr_pair_nest_end(fmsg);\n\nfree_data:\n\tkvfree(cr_data);\n\treturn err;\n}",
        "code_after_change": "static int\nmlx5_fw_fatal_reporter_dump(struct devlink_health_reporter *reporter,\n\t\t\t    struct devlink_fmsg *fmsg, void *priv_ctx)\n{\n\tstruct mlx5_core_dev *dev = devlink_health_reporter_priv(reporter);\n\tu32 crdump_size = dev->priv.health.crdump_size;\n\tu32 *cr_data;\n\tu32 data_size;\n\tu32 offset;\n\tint err;\n\n\tif (!mlx5_core_is_pf(dev))\n\t\treturn -EPERM;\n\n\tcr_data = kvmalloc(crdump_size, GFP_KERNEL);\n\tif (!cr_data)\n\t\treturn -ENOMEM;\n\terr = mlx5_crdump_collect(dev, cr_data);\n\tif (err)\n\t\tgoto free_data;\n\n\tif (priv_ctx) {\n\t\tstruct mlx5_fw_reporter_ctx *fw_reporter_ctx = priv_ctx;\n\n\t\terr = mlx5_fw_reporter_ctx_pairs_put(fmsg, fw_reporter_ctx);\n\t\tif (err)\n\t\t\tgoto free_data;\n\t}\n\n\terr = devlink_fmsg_arr_pair_nest_start(fmsg, \"crdump_data\");\n\tif (err)\n\t\tgoto free_data;\n\tfor (offset = 0; offset < crdump_size; offset += data_size) {\n\t\tif (crdump_size - offset < MLX5_CR_DUMP_CHUNK_SIZE)\n\t\t\tdata_size = crdump_size - offset;\n\t\telse\n\t\t\tdata_size = MLX5_CR_DUMP_CHUNK_SIZE;\n\t\terr = devlink_fmsg_binary_put(fmsg, (char *)cr_data + offset,\n\t\t\t\t\t      data_size);\n\t\tif (err)\n\t\t\tgoto free_data;\n\t}\n\terr = devlink_fmsg_arr_pair_nest_end(fmsg);\n\nfree_data:\n\tkvfree(cr_data);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,7 +17,7 @@\n \t\treturn -ENOMEM;\n \terr = mlx5_crdump_collect(dev, cr_data);\n \tif (err)\n-\t\treturn err;\n+\t\tgoto free_data;\n \n \tif (priv_ctx) {\n \t\tstruct mlx5_fw_reporter_ctx *fw_reporter_ctx = priv_ctx;",
        "function_modified_lines": {
            "added": [
                "\t\tgoto free_data;"
            ],
            "deleted": [
                "\t\treturn err;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the mlx5_fw_fatal_reporter_dump() function in drivers/net/ethernet/mellanox/mlx5/core/health.c in the Linux kernel before 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering mlx5_crdump_collect() failures, aka CID-c7ed6d0183d5.",
        "id": 2128
    },
    {
        "cve_id": "CVE-2023-4569",
        "code_before_change": "static int nft_set_catchall_flush(const struct nft_ctx *ctx,\n\t\t\t\t  struct nft_set *set)\n{\n\tu8 genmask = nft_genmask_next(ctx->net);\n\tstruct nft_set_elem_catchall *catchall;\n\tstruct nft_set_elem elem;\n\tstruct nft_set_ext *ext;\n\tint ret = 0;\n\n\tlist_for_each_entry_rcu(catchall, &set->catchall_list, list) {\n\t\text = nft_set_elem_ext(set, catchall->elem);\n\t\tif (!nft_set_elem_active(ext, genmask))\n\t\t\tcontinue;\n\n\t\telem.priv = catchall->elem;\n\t\tret = __nft_set_catchall_flush(ctx, set, &elem);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int nft_set_catchall_flush(const struct nft_ctx *ctx,\n\t\t\t\t  struct nft_set *set)\n{\n\tu8 genmask = nft_genmask_next(ctx->net);\n\tstruct nft_set_elem_catchall *catchall;\n\tstruct nft_set_elem elem;\n\tstruct nft_set_ext *ext;\n\tint ret = 0;\n\n\tlist_for_each_entry_rcu(catchall, &set->catchall_list, list) {\n\t\text = nft_set_elem_ext(set, catchall->elem);\n\t\tif (!nft_set_elem_active(ext, genmask))\n\t\t\tcontinue;\n\n\t\telem.priv = catchall->elem;\n\t\tret = __nft_set_catchall_flush(ctx, set, &elem);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tnft_set_elem_change_active(ctx->net, set, ext);\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,7 @@\n \t\tret = __nft_set_catchall_flush(ctx, set, &elem);\n \t\tif (ret < 0)\n \t\t\tbreak;\n+\t\tnft_set_elem_change_active(ctx->net, set, ext);\n \t}\n \n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "\t\tnft_set_elem_change_active(ctx->net, set, ext);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak flaw was found in nft_set_catchall_flush in net/netfilter/nf_tables_api.c in the Linux Kernel. This issue may allow a local attacker to cause double-deactivations of catchall elements, which can result in a memory leak.",
        "id": 4222
    },
    {
        "cve_id": "CVE-2019-19051",
        "code_before_change": "int i2400m_op_rfkill_sw_toggle(struct wimax_dev *wimax_dev,\n\t\t\t       enum wimax_rf_state state)\n{\n\tint result;\n\tstruct i2400m *i2400m = wimax_dev_to_i2400m(wimax_dev);\n\tstruct device *dev = i2400m_dev(i2400m);\n\tstruct sk_buff *ack_skb;\n\tstruct {\n\t\tstruct i2400m_l3l4_hdr hdr;\n\t\tstruct i2400m_tlv_rf_operation sw_rf;\n\t} __packed *cmd;\n\tchar strerr[32];\n\n\td_fnstart(4, dev, \"(wimax_dev %p state %d)\\n\", wimax_dev, state);\n\n\tresult = -ENOMEM;\n\tcmd = kzalloc(sizeof(*cmd), GFP_KERNEL);\n\tif (cmd == NULL)\n\t\tgoto error_alloc;\n\tcmd->hdr.type = cpu_to_le16(I2400M_MT_CMD_RF_CONTROL);\n\tcmd->hdr.length = sizeof(cmd->sw_rf);\n\tcmd->hdr.version = cpu_to_le16(I2400M_L3L4_VERSION);\n\tcmd->sw_rf.hdr.type = cpu_to_le16(I2400M_TLV_RF_OPERATION);\n\tcmd->sw_rf.hdr.length = cpu_to_le16(sizeof(cmd->sw_rf.status));\n\tswitch (state) {\n\tcase WIMAX_RF_OFF:\t/* RFKILL ON, radio OFF */\n\t\tcmd->sw_rf.status = cpu_to_le32(2);\n\t\tbreak;\n\tcase WIMAX_RF_ON:\t/* RFKILL OFF, radio ON */\n\t\tcmd->sw_rf.status = cpu_to_le32(1);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tack_skb = i2400m_msg_to_dev(i2400m, cmd, sizeof(*cmd));\n\tresult = PTR_ERR(ack_skb);\n\tif (IS_ERR(ack_skb)) {\n\t\tdev_err(dev, \"Failed to issue 'RF Control' command: %d\\n\",\n\t\t\tresult);\n\t\tgoto error_msg_to_dev;\n\t}\n\tresult = i2400m_msg_check_status(wimax_msg_data(ack_skb),\n\t\t\t\t\t strerr, sizeof(strerr));\n\tif (result < 0) {\n\t\tdev_err(dev, \"'RF Control' (0x%04x) command failed: %d - %s\\n\",\n\t\t\tI2400M_MT_CMD_RF_CONTROL, result, strerr);\n\t\tgoto error_cmd;\n\t}\n\n\t/* Now we wait for the state to change to RADIO_OFF or RADIO_ON */\n\tresult = wait_event_timeout(\n\t\ti2400m->state_wq, i2400m_radio_is(i2400m, state),\n\t\t5 * HZ);\n\tif (result == 0)\n\t\tresult = -ETIMEDOUT;\n\tif (result < 0)\n\t\tdev_err(dev, \"Error waiting for device to toggle RF state: \"\n\t\t\t\"%d\\n\", result);\n\tresult = 0;\nerror_cmd:\n\tkfree(cmd);\n\tkfree_skb(ack_skb);\nerror_msg_to_dev:\nerror_alloc:\n\td_fnend(4, dev, \"(wimax_dev %p state %d) = %d\\n\",\n\t\twimax_dev, state, result);\n\treturn result;\n}",
        "code_after_change": "int i2400m_op_rfkill_sw_toggle(struct wimax_dev *wimax_dev,\n\t\t\t       enum wimax_rf_state state)\n{\n\tint result;\n\tstruct i2400m *i2400m = wimax_dev_to_i2400m(wimax_dev);\n\tstruct device *dev = i2400m_dev(i2400m);\n\tstruct sk_buff *ack_skb;\n\tstruct {\n\t\tstruct i2400m_l3l4_hdr hdr;\n\t\tstruct i2400m_tlv_rf_operation sw_rf;\n\t} __packed *cmd;\n\tchar strerr[32];\n\n\td_fnstart(4, dev, \"(wimax_dev %p state %d)\\n\", wimax_dev, state);\n\n\tresult = -ENOMEM;\n\tcmd = kzalloc(sizeof(*cmd), GFP_KERNEL);\n\tif (cmd == NULL)\n\t\tgoto error_alloc;\n\tcmd->hdr.type = cpu_to_le16(I2400M_MT_CMD_RF_CONTROL);\n\tcmd->hdr.length = sizeof(cmd->sw_rf);\n\tcmd->hdr.version = cpu_to_le16(I2400M_L3L4_VERSION);\n\tcmd->sw_rf.hdr.type = cpu_to_le16(I2400M_TLV_RF_OPERATION);\n\tcmd->sw_rf.hdr.length = cpu_to_le16(sizeof(cmd->sw_rf.status));\n\tswitch (state) {\n\tcase WIMAX_RF_OFF:\t/* RFKILL ON, radio OFF */\n\t\tcmd->sw_rf.status = cpu_to_le32(2);\n\t\tbreak;\n\tcase WIMAX_RF_ON:\t/* RFKILL OFF, radio ON */\n\t\tcmd->sw_rf.status = cpu_to_le32(1);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tack_skb = i2400m_msg_to_dev(i2400m, cmd, sizeof(*cmd));\n\tresult = PTR_ERR(ack_skb);\n\tif (IS_ERR(ack_skb)) {\n\t\tdev_err(dev, \"Failed to issue 'RF Control' command: %d\\n\",\n\t\t\tresult);\n\t\tgoto error_msg_to_dev;\n\t}\n\tresult = i2400m_msg_check_status(wimax_msg_data(ack_skb),\n\t\t\t\t\t strerr, sizeof(strerr));\n\tif (result < 0) {\n\t\tdev_err(dev, \"'RF Control' (0x%04x) command failed: %d - %s\\n\",\n\t\t\tI2400M_MT_CMD_RF_CONTROL, result, strerr);\n\t\tgoto error_cmd;\n\t}\n\n\t/* Now we wait for the state to change to RADIO_OFF or RADIO_ON */\n\tresult = wait_event_timeout(\n\t\ti2400m->state_wq, i2400m_radio_is(i2400m, state),\n\t\t5 * HZ);\n\tif (result == 0)\n\t\tresult = -ETIMEDOUT;\n\tif (result < 0)\n\t\tdev_err(dev, \"Error waiting for device to toggle RF state: \"\n\t\t\t\"%d\\n\", result);\n\tresult = 0;\nerror_cmd:\n\tkfree_skb(ack_skb);\nerror_msg_to_dev:\nerror_alloc:\n\td_fnend(4, dev, \"(wimax_dev %p state %d) = %d\\n\",\n\t\twimax_dev, state, result);\n\tkfree(cmd);\n\treturn result;\n}",
        "patch": "--- code before\n+++ code after\n@@ -59,11 +59,11 @@\n \t\t\t\"%d\\n\", result);\n \tresult = 0;\n error_cmd:\n-\tkfree(cmd);\n \tkfree_skb(ack_skb);\n error_msg_to_dev:\n error_alloc:\n \td_fnend(4, dev, \"(wimax_dev %p state %d) = %d\\n\",\n \t\twimax_dev, state, result);\n+\tkfree(cmd);\n \treturn result;\n }",
        "function_modified_lines": {
            "added": [
                "\tkfree(cmd);"
            ],
            "deleted": [
                "\tkfree(cmd);"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the i2400m_op_rfkill_sw_toggle() function in drivers/net/wimax/i2400m/op-rfkill.c in the Linux kernel before 5.3.11 allows attackers to cause a denial of service (memory consumption), aka CID-6f3ef5c25cc7.",
        "id": 2132
    },
    {
        "cve_id": "CVE-2021-3736",
        "code_before_change": "static int mbochs_probe(struct mdev_device *mdev)\n{\n\tconst struct mbochs_type *type =\n\t\t&mbochs_types[mdev_get_type_group_id(mdev)];\n\tstruct device *dev = mdev_dev(mdev);\n\tstruct mdev_state *mdev_state;\n\tint ret = -ENOMEM;\n\n\tif (type->mbytes + mbochs_used_mbytes > max_mbytes)\n\t\treturn -ENOMEM;\n\n\tmdev_state = kzalloc(sizeof(struct mdev_state), GFP_KERNEL);\n\tif (mdev_state == NULL)\n\t\treturn -ENOMEM;\n\tvfio_init_group_dev(&mdev_state->vdev, &mdev->dev, &mbochs_dev_ops);\n\n\tmdev_state->vconfig = kzalloc(MBOCHS_CONFIG_SPACE_SIZE, GFP_KERNEL);\n\tif (mdev_state->vconfig == NULL)\n\t\tgoto err_mem;\n\n\tmdev_state->memsize = type->mbytes * 1024 * 1024;\n\tmdev_state->pagecount = mdev_state->memsize >> PAGE_SHIFT;\n\tmdev_state->pages = kcalloc(mdev_state->pagecount,\n\t\t\t\t    sizeof(struct page *),\n\t\t\t\t    GFP_KERNEL);\n\tif (!mdev_state->pages)\n\t\tgoto err_mem;\n\n\tdev_info(dev, \"%s: %s, %d MB, %ld pages\\n\", __func__,\n\t\t type->name, type->mbytes, mdev_state->pagecount);\n\n\tmutex_init(&mdev_state->ops_lock);\n\tmdev_state->mdev = mdev;\n\tINIT_LIST_HEAD(&mdev_state->dmabufs);\n\tmdev_state->next_id = 1;\n\n\tmdev_state->type = type;\n\tmdev_state->edid_regs.max_xres = type->max_x;\n\tmdev_state->edid_regs.max_yres = type->max_y;\n\tmdev_state->edid_regs.edid_offset = MBOCHS_EDID_BLOB_OFFSET;\n\tmdev_state->edid_regs.edid_max_size = sizeof(mdev_state->edid_blob);\n\tmbochs_create_config_space(mdev_state);\n\tmbochs_reset(mdev_state);\n\n\tmbochs_used_mbytes += type->mbytes;\n\n\tret = vfio_register_group_dev(&mdev_state->vdev);\n\tif (ret)\n\t\tgoto err_mem;\n\tdev_set_drvdata(&mdev->dev, mdev_state);\n\treturn 0;\n\nerr_mem:\n\tkfree(mdev_state->vconfig);\n\tkfree(mdev_state);\n\treturn ret;\n}",
        "code_after_change": "static int mbochs_probe(struct mdev_device *mdev)\n{\n\tint avail_mbytes = atomic_read(&mbochs_avail_mbytes);\n\tconst struct mbochs_type *type =\n\t\t&mbochs_types[mdev_get_type_group_id(mdev)];\n\tstruct device *dev = mdev_dev(mdev);\n\tstruct mdev_state *mdev_state;\n\tint ret = -ENOMEM;\n\n\tdo {\n\t\tif (avail_mbytes < type->mbytes)\n\t\t\treturn -ENOSPC;\n\t} while (!atomic_try_cmpxchg(&mbochs_avail_mbytes, &avail_mbytes,\n\t\t\t\t     avail_mbytes - type->mbytes));\n\n\tmdev_state = kzalloc(sizeof(struct mdev_state), GFP_KERNEL);\n\tif (mdev_state == NULL)\n\t\tgoto err_avail;\n\tvfio_init_group_dev(&mdev_state->vdev, &mdev->dev, &mbochs_dev_ops);\n\n\tmdev_state->vconfig = kzalloc(MBOCHS_CONFIG_SPACE_SIZE, GFP_KERNEL);\n\tif (mdev_state->vconfig == NULL)\n\t\tgoto err_mem;\n\n\tmdev_state->memsize = type->mbytes * 1024 * 1024;\n\tmdev_state->pagecount = mdev_state->memsize >> PAGE_SHIFT;\n\tmdev_state->pages = kcalloc(mdev_state->pagecount,\n\t\t\t\t    sizeof(struct page *),\n\t\t\t\t    GFP_KERNEL);\n\tif (!mdev_state->pages)\n\t\tgoto err_mem;\n\n\tdev_info(dev, \"%s: %s, %d MB, %ld pages\\n\", __func__,\n\t\t type->name, type->mbytes, mdev_state->pagecount);\n\n\tmutex_init(&mdev_state->ops_lock);\n\tmdev_state->mdev = mdev;\n\tINIT_LIST_HEAD(&mdev_state->dmabufs);\n\tmdev_state->next_id = 1;\n\n\tmdev_state->type = type;\n\tmdev_state->edid_regs.max_xres = type->max_x;\n\tmdev_state->edid_regs.max_yres = type->max_y;\n\tmdev_state->edid_regs.edid_offset = MBOCHS_EDID_BLOB_OFFSET;\n\tmdev_state->edid_regs.edid_max_size = sizeof(mdev_state->edid_blob);\n\tmbochs_create_config_space(mdev_state);\n\tmbochs_reset(mdev_state);\n\n\tret = vfio_register_group_dev(&mdev_state->vdev);\n\tif (ret)\n\t\tgoto err_mem;\n\tdev_set_drvdata(&mdev->dev, mdev_state);\n\treturn 0;\nerr_mem:\n\tkfree(mdev_state->pages);\n\tkfree(mdev_state->vconfig);\n\tkfree(mdev_state);\nerr_avail:\n\tatomic_add(type->mbytes, &mbochs_avail_mbytes);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,17 +1,21 @@\n static int mbochs_probe(struct mdev_device *mdev)\n {\n+\tint avail_mbytes = atomic_read(&mbochs_avail_mbytes);\n \tconst struct mbochs_type *type =\n \t\t&mbochs_types[mdev_get_type_group_id(mdev)];\n \tstruct device *dev = mdev_dev(mdev);\n \tstruct mdev_state *mdev_state;\n \tint ret = -ENOMEM;\n \n-\tif (type->mbytes + mbochs_used_mbytes > max_mbytes)\n-\t\treturn -ENOMEM;\n+\tdo {\n+\t\tif (avail_mbytes < type->mbytes)\n+\t\t\treturn -ENOSPC;\n+\t} while (!atomic_try_cmpxchg(&mbochs_avail_mbytes, &avail_mbytes,\n+\t\t\t\t     avail_mbytes - type->mbytes));\n \n \tmdev_state = kzalloc(sizeof(struct mdev_state), GFP_KERNEL);\n \tif (mdev_state == NULL)\n-\t\treturn -ENOMEM;\n+\t\tgoto err_avail;\n \tvfio_init_group_dev(&mdev_state->vdev, &mdev->dev, &mbochs_dev_ops);\n \n \tmdev_state->vconfig = kzalloc(MBOCHS_CONFIG_SPACE_SIZE, GFP_KERNEL);\n@@ -42,16 +46,16 @@\n \tmbochs_create_config_space(mdev_state);\n \tmbochs_reset(mdev_state);\n \n-\tmbochs_used_mbytes += type->mbytes;\n-\n \tret = vfio_register_group_dev(&mdev_state->vdev);\n \tif (ret)\n \t\tgoto err_mem;\n \tdev_set_drvdata(&mdev->dev, mdev_state);\n \treturn 0;\n-\n err_mem:\n+\tkfree(mdev_state->pages);\n \tkfree(mdev_state->vconfig);\n \tkfree(mdev_state);\n+err_avail:\n+\tatomic_add(type->mbytes, &mbochs_avail_mbytes);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tint avail_mbytes = atomic_read(&mbochs_avail_mbytes);",
                "\tdo {",
                "\t\tif (avail_mbytes < type->mbytes)",
                "\t\t\treturn -ENOSPC;",
                "\t} while (!atomic_try_cmpxchg(&mbochs_avail_mbytes, &avail_mbytes,",
                "\t\t\t\t     avail_mbytes - type->mbytes));",
                "\t\tgoto err_avail;",
                "\tkfree(mdev_state->pages);",
                "err_avail:",
                "\tatomic_add(type->mbytes, &mbochs_avail_mbytes);"
            ],
            "deleted": [
                "\tif (type->mbytes + mbochs_used_mbytes > max_mbytes)",
                "\t\treturn -ENOMEM;",
                "\t\treturn -ENOMEM;",
                "\tmbochs_used_mbytes += type->mbytes;",
                "",
                ""
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw was found in the Linux kernel. A memory leak problem was found in mbochs_ioctl in samples/vfio-mdev/mbochs.c in Virtual Function I/O (VFIO) Mediated devices. This flaw could allow a local attacker to leak internal kernel information.",
        "id": 3049
    },
    {
        "cve_id": "CVE-2022-3619",
        "code_before_change": "void l2cap_recv_acldata(struct hci_conn *hcon, struct sk_buff *skb, u16 flags)\n{\n\tstruct l2cap_conn *conn = hcon->l2cap_data;\n\tint len;\n\n\t/* For AMP controller do not create l2cap conn */\n\tif (!conn && hcon->hdev->dev_type != HCI_PRIMARY)\n\t\tgoto drop;\n\n\tif (!conn)\n\t\tconn = l2cap_conn_add(hcon);\n\n\tif (!conn)\n\t\tgoto drop;\n\n\tBT_DBG(\"conn %p len %u flags 0x%x\", conn, skb->len, flags);\n\n\tswitch (flags) {\n\tcase ACL_START:\n\tcase ACL_START_NO_FLUSH:\n\tcase ACL_COMPLETE:\n\t\tif (conn->rx_skb) {\n\t\t\tBT_ERR(\"Unexpected start frame (len %d)\", skb->len);\n\t\t\tl2cap_recv_reset(conn);\n\t\t\tl2cap_conn_unreliable(conn, ECOMM);\n\t\t}\n\n\t\t/* Start fragment may not contain the L2CAP length so just\n\t\t * copy the initial byte when that happens and use conn->mtu as\n\t\t * expected length.\n\t\t */\n\t\tif (skb->len < L2CAP_LEN_SIZE) {\n\t\t\tif (l2cap_recv_frag(conn, skb, conn->mtu) < 0)\n\t\t\t\tgoto drop;\n\t\t\treturn;\n\t\t}\n\n\t\tlen = get_unaligned_le16(skb->data) + L2CAP_HDR_SIZE;\n\n\t\tif (len == skb->len) {\n\t\t\t/* Complete frame received */\n\t\t\tl2cap_recv_frame(conn, skb);\n\t\t\treturn;\n\t\t}\n\n\t\tBT_DBG(\"Start: total len %d, frag len %u\", len, skb->len);\n\n\t\tif (skb->len > len) {\n\t\t\tBT_ERR(\"Frame is too long (len %u, expected len %d)\",\n\t\t\t       skb->len, len);\n\t\t\tl2cap_conn_unreliable(conn, ECOMM);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* Append fragment into frame (with header) */\n\t\tif (l2cap_recv_frag(conn, skb, len) < 0)\n\t\t\tgoto drop;\n\n\t\tbreak;\n\n\tcase ACL_CONT:\n\t\tBT_DBG(\"Cont: frag len %u (expecting %u)\", skb->len, conn->rx_len);\n\n\t\tif (!conn->rx_skb) {\n\t\t\tBT_ERR(\"Unexpected continuation frame (len %d)\", skb->len);\n\t\t\tl2cap_conn_unreliable(conn, ECOMM);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* Complete the L2CAP length if it has not been read */\n\t\tif (conn->rx_skb->len < L2CAP_LEN_SIZE) {\n\t\t\tif (l2cap_recv_len(conn, skb) < 0) {\n\t\t\t\tl2cap_conn_unreliable(conn, ECOMM);\n\t\t\t\tgoto drop;\n\t\t\t}\n\n\t\t\t/* Header still could not be read just continue */\n\t\t\tif (conn->rx_skb->len < L2CAP_LEN_SIZE)\n\t\t\t\treturn;\n\t\t}\n\n\t\tif (skb->len > conn->rx_len) {\n\t\t\tBT_ERR(\"Fragment is too long (len %u, expected %u)\",\n\t\t\t       skb->len, conn->rx_len);\n\t\t\tl2cap_recv_reset(conn);\n\t\t\tl2cap_conn_unreliable(conn, ECOMM);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* Append fragment into frame (with header) */\n\t\tl2cap_recv_frag(conn, skb, skb->len);\n\n\t\tif (!conn->rx_len) {\n\t\t\t/* Complete frame received. l2cap_recv_frame\n\t\t\t * takes ownership of the skb so set the global\n\t\t\t * rx_skb pointer to NULL first.\n\t\t\t */\n\t\t\tstruct sk_buff *rx_skb = conn->rx_skb;\n\t\t\tconn->rx_skb = NULL;\n\t\t\tl2cap_recv_frame(conn, rx_skb);\n\t\t}\n\t\tbreak;\n\t}\n\ndrop:\n\tkfree_skb(skb);\n}",
        "code_after_change": "void l2cap_recv_acldata(struct hci_conn *hcon, struct sk_buff *skb, u16 flags)\n{\n\tstruct l2cap_conn *conn = hcon->l2cap_data;\n\tint len;\n\n\t/* For AMP controller do not create l2cap conn */\n\tif (!conn && hcon->hdev->dev_type != HCI_PRIMARY)\n\t\tgoto drop;\n\n\tif (!conn)\n\t\tconn = l2cap_conn_add(hcon);\n\n\tif (!conn)\n\t\tgoto drop;\n\n\tBT_DBG(\"conn %p len %u flags 0x%x\", conn, skb->len, flags);\n\n\tswitch (flags) {\n\tcase ACL_START:\n\tcase ACL_START_NO_FLUSH:\n\tcase ACL_COMPLETE:\n\t\tif (conn->rx_skb) {\n\t\t\tBT_ERR(\"Unexpected start frame (len %d)\", skb->len);\n\t\t\tl2cap_recv_reset(conn);\n\t\t\tl2cap_conn_unreliable(conn, ECOMM);\n\t\t}\n\n\t\t/* Start fragment may not contain the L2CAP length so just\n\t\t * copy the initial byte when that happens and use conn->mtu as\n\t\t * expected length.\n\t\t */\n\t\tif (skb->len < L2CAP_LEN_SIZE) {\n\t\t\tl2cap_recv_frag(conn, skb, conn->mtu);\n\t\t\tbreak;\n\t\t}\n\n\t\tlen = get_unaligned_le16(skb->data) + L2CAP_HDR_SIZE;\n\n\t\tif (len == skb->len) {\n\t\t\t/* Complete frame received */\n\t\t\tl2cap_recv_frame(conn, skb);\n\t\t\treturn;\n\t\t}\n\n\t\tBT_DBG(\"Start: total len %d, frag len %u\", len, skb->len);\n\n\t\tif (skb->len > len) {\n\t\t\tBT_ERR(\"Frame is too long (len %u, expected len %d)\",\n\t\t\t       skb->len, len);\n\t\t\tl2cap_conn_unreliable(conn, ECOMM);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* Append fragment into frame (with header) */\n\t\tif (l2cap_recv_frag(conn, skb, len) < 0)\n\t\t\tgoto drop;\n\n\t\tbreak;\n\n\tcase ACL_CONT:\n\t\tBT_DBG(\"Cont: frag len %u (expecting %u)\", skb->len, conn->rx_len);\n\n\t\tif (!conn->rx_skb) {\n\t\t\tBT_ERR(\"Unexpected continuation frame (len %d)\", skb->len);\n\t\t\tl2cap_conn_unreliable(conn, ECOMM);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* Complete the L2CAP length if it has not been read */\n\t\tif (conn->rx_skb->len < L2CAP_LEN_SIZE) {\n\t\t\tif (l2cap_recv_len(conn, skb) < 0) {\n\t\t\t\tl2cap_conn_unreliable(conn, ECOMM);\n\t\t\t\tgoto drop;\n\t\t\t}\n\n\t\t\t/* Header still could not be read just continue */\n\t\t\tif (conn->rx_skb->len < L2CAP_LEN_SIZE)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (skb->len > conn->rx_len) {\n\t\t\tBT_ERR(\"Fragment is too long (len %u, expected %u)\",\n\t\t\t       skb->len, conn->rx_len);\n\t\t\tl2cap_recv_reset(conn);\n\t\t\tl2cap_conn_unreliable(conn, ECOMM);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* Append fragment into frame (with header) */\n\t\tl2cap_recv_frag(conn, skb, skb->len);\n\n\t\tif (!conn->rx_len) {\n\t\t\t/* Complete frame received. l2cap_recv_frame\n\t\t\t * takes ownership of the skb so set the global\n\t\t\t * rx_skb pointer to NULL first.\n\t\t\t */\n\t\t\tstruct sk_buff *rx_skb = conn->rx_skb;\n\t\t\tconn->rx_skb = NULL;\n\t\t\tl2cap_recv_frame(conn, rx_skb);\n\t\t}\n\t\tbreak;\n\t}\n\ndrop:\n\tkfree_skb(skb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,9 +30,8 @@\n \t\t * expected length.\n \t\t */\n \t\tif (skb->len < L2CAP_LEN_SIZE) {\n-\t\t\tif (l2cap_recv_frag(conn, skb, conn->mtu) < 0)\n-\t\t\t\tgoto drop;\n-\t\t\treturn;\n+\t\t\tl2cap_recv_frag(conn, skb, conn->mtu);\n+\t\t\tbreak;\n \t\t}\n \n \t\tlen = get_unaligned_le16(skb->data) + L2CAP_HDR_SIZE;\n@@ -76,7 +75,7 @@\n \n \t\t\t/* Header still could not be read just continue */\n \t\t\tif (conn->rx_skb->len < L2CAP_LEN_SIZE)\n-\t\t\t\treturn;\n+\t\t\t\tbreak;\n \t\t}\n \n \t\tif (skb->len > conn->rx_len) {",
        "function_modified_lines": {
            "added": [
                "\t\t\tl2cap_recv_frag(conn, skb, conn->mtu);",
                "\t\t\tbreak;",
                "\t\t\t\tbreak;"
            ],
            "deleted": [
                "\t\t\tif (l2cap_recv_frag(conn, skb, conn->mtu) < 0)",
                "\t\t\t\tgoto drop;",
                "\t\t\treturn;",
                "\t\t\t\treturn;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A vulnerability has been found in Linux Kernel and classified as problematic. This vulnerability affects the function l2cap_recv_acldata of the file net/bluetooth/l2cap_core.c of the component Bluetooth. The manipulation leads to memory leak. It is recommended to apply a patch to fix this issue. VDB-211918 is the identifier assigned to this vulnerability.",
        "id": 3655
    },
    {
        "cve_id": "CVE-2019-19045",
        "code_before_change": "static int mlx5_fpga_conn_create_cq(struct mlx5_fpga_conn *conn, int cq_size)\n{\n\tstruct mlx5_fpga_device *fdev = conn->fdev;\n\tstruct mlx5_core_dev *mdev = fdev->mdev;\n\tu32 temp_cqc[MLX5_ST_SZ_DW(cqc)] = {0};\n\tu32 out[MLX5_ST_SZ_DW(create_cq_out)];\n\tstruct mlx5_wq_param wqp;\n\tstruct mlx5_cqe64 *cqe;\n\tint inlen, err, eqn;\n\tunsigned int irqn;\n\tvoid *cqc, *in;\n\t__be64 *pas;\n\tu32 i;\n\n\tcq_size = roundup_pow_of_two(cq_size);\n\tMLX5_SET(cqc, temp_cqc, log_cq_size, ilog2(cq_size));\n\n\twqp.buf_numa_node = mdev->priv.numa_node;\n\twqp.db_numa_node  = mdev->priv.numa_node;\n\n\terr = mlx5_cqwq_create(mdev, &wqp, temp_cqc, &conn->cq.wq,\n\t\t\t       &conn->cq.wq_ctrl);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < mlx5_cqwq_get_size(&conn->cq.wq); i++) {\n\t\tcqe = mlx5_cqwq_get_wqe(&conn->cq.wq, i);\n\t\tcqe->op_own = MLX5_CQE_INVALID << 4 | MLX5_CQE_OWNER_MASK;\n\t}\n\n\tinlen = MLX5_ST_SZ_BYTES(create_cq_in) +\n\t\tsizeof(u64) * conn->cq.wq_ctrl.buf.npages;\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_cqwq;\n\t}\n\n\terr = mlx5_vector2eqn(mdev, smp_processor_id(), &eqn, &irqn);\n\tif (err)\n\t\tgoto err_cqwq;\n\n\tcqc = MLX5_ADDR_OF(create_cq_in, in, cq_context);\n\tMLX5_SET(cqc, cqc, log_cq_size, ilog2(cq_size));\n\tMLX5_SET(cqc, cqc, c_eqn, eqn);\n\tMLX5_SET(cqc, cqc, uar_page, fdev->conn_res.uar->index);\n\tMLX5_SET(cqc, cqc, log_page_size, conn->cq.wq_ctrl.buf.page_shift -\n\t\t\t   MLX5_ADAPTER_PAGE_SHIFT);\n\tMLX5_SET64(cqc, cqc, dbr_addr, conn->cq.wq_ctrl.db.dma);\n\n\tpas = (__be64 *)MLX5_ADDR_OF(create_cq_in, in, pas);\n\tmlx5_fill_page_frag_array(&conn->cq.wq_ctrl.buf, pas);\n\n\terr = mlx5_core_create_cq(mdev, &conn->cq.mcq, in, inlen, out, sizeof(out));\n\tkvfree(in);\n\n\tif (err)\n\t\tgoto err_cqwq;\n\n\tconn->cq.mcq.cqe_sz     = 64;\n\tconn->cq.mcq.set_ci_db  = conn->cq.wq_ctrl.db.db;\n\tconn->cq.mcq.arm_db     = conn->cq.wq_ctrl.db.db + 1;\n\t*conn->cq.mcq.set_ci_db = 0;\n\t*conn->cq.mcq.arm_db    = 0;\n\tconn->cq.mcq.vector     = 0;\n\tconn->cq.mcq.comp       = mlx5_fpga_conn_cq_complete;\n\tconn->cq.mcq.event      = mlx5_fpga_conn_cq_event;\n\tconn->cq.mcq.irqn       = irqn;\n\tconn->cq.mcq.uar        = fdev->conn_res.uar;\n\ttasklet_init(&conn->cq.tasklet, mlx5_fpga_conn_cq_tasklet,\n\t\t     (unsigned long)conn);\n\n\tmlx5_fpga_dbg(fdev, \"Created CQ #0x%x\\n\", conn->cq.mcq.cqn);\n\n\tgoto out;\n\nerr_cqwq:\n\tmlx5_wq_destroy(&conn->cq.wq_ctrl);\nout:\n\treturn err;\n}",
        "code_after_change": "static int mlx5_fpga_conn_create_cq(struct mlx5_fpga_conn *conn, int cq_size)\n{\n\tstruct mlx5_fpga_device *fdev = conn->fdev;\n\tstruct mlx5_core_dev *mdev = fdev->mdev;\n\tu32 temp_cqc[MLX5_ST_SZ_DW(cqc)] = {0};\n\tu32 out[MLX5_ST_SZ_DW(create_cq_out)];\n\tstruct mlx5_wq_param wqp;\n\tstruct mlx5_cqe64 *cqe;\n\tint inlen, err, eqn;\n\tunsigned int irqn;\n\tvoid *cqc, *in;\n\t__be64 *pas;\n\tu32 i;\n\n\tcq_size = roundup_pow_of_two(cq_size);\n\tMLX5_SET(cqc, temp_cqc, log_cq_size, ilog2(cq_size));\n\n\twqp.buf_numa_node = mdev->priv.numa_node;\n\twqp.db_numa_node  = mdev->priv.numa_node;\n\n\terr = mlx5_cqwq_create(mdev, &wqp, temp_cqc, &conn->cq.wq,\n\t\t\t       &conn->cq.wq_ctrl);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < mlx5_cqwq_get_size(&conn->cq.wq); i++) {\n\t\tcqe = mlx5_cqwq_get_wqe(&conn->cq.wq, i);\n\t\tcqe->op_own = MLX5_CQE_INVALID << 4 | MLX5_CQE_OWNER_MASK;\n\t}\n\n\tinlen = MLX5_ST_SZ_BYTES(create_cq_in) +\n\t\tsizeof(u64) * conn->cq.wq_ctrl.buf.npages;\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_cqwq;\n\t}\n\n\terr = mlx5_vector2eqn(mdev, smp_processor_id(), &eqn, &irqn);\n\tif (err) {\n\t\tkvfree(in);\n\t\tgoto err_cqwq;\n\t}\n\n\tcqc = MLX5_ADDR_OF(create_cq_in, in, cq_context);\n\tMLX5_SET(cqc, cqc, log_cq_size, ilog2(cq_size));\n\tMLX5_SET(cqc, cqc, c_eqn, eqn);\n\tMLX5_SET(cqc, cqc, uar_page, fdev->conn_res.uar->index);\n\tMLX5_SET(cqc, cqc, log_page_size, conn->cq.wq_ctrl.buf.page_shift -\n\t\t\t   MLX5_ADAPTER_PAGE_SHIFT);\n\tMLX5_SET64(cqc, cqc, dbr_addr, conn->cq.wq_ctrl.db.dma);\n\n\tpas = (__be64 *)MLX5_ADDR_OF(create_cq_in, in, pas);\n\tmlx5_fill_page_frag_array(&conn->cq.wq_ctrl.buf, pas);\n\n\terr = mlx5_core_create_cq(mdev, &conn->cq.mcq, in, inlen, out, sizeof(out));\n\tkvfree(in);\n\n\tif (err)\n\t\tgoto err_cqwq;\n\n\tconn->cq.mcq.cqe_sz     = 64;\n\tconn->cq.mcq.set_ci_db  = conn->cq.wq_ctrl.db.db;\n\tconn->cq.mcq.arm_db     = conn->cq.wq_ctrl.db.db + 1;\n\t*conn->cq.mcq.set_ci_db = 0;\n\t*conn->cq.mcq.arm_db    = 0;\n\tconn->cq.mcq.vector     = 0;\n\tconn->cq.mcq.comp       = mlx5_fpga_conn_cq_complete;\n\tconn->cq.mcq.event      = mlx5_fpga_conn_cq_event;\n\tconn->cq.mcq.irqn       = irqn;\n\tconn->cq.mcq.uar        = fdev->conn_res.uar;\n\ttasklet_init(&conn->cq.tasklet, mlx5_fpga_conn_cq_tasklet,\n\t\t     (unsigned long)conn);\n\n\tmlx5_fpga_dbg(fdev, \"Created CQ #0x%x\\n\", conn->cq.mcq.cqn);\n\n\tgoto out;\n\nerr_cqwq:\n\tmlx5_wq_destroy(&conn->cq.wq_ctrl);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -37,8 +37,10 @@\n \t}\n \n \terr = mlx5_vector2eqn(mdev, smp_processor_id(), &eqn, &irqn);\n-\tif (err)\n+\tif (err) {\n+\t\tkvfree(in);\n \t\tgoto err_cqwq;\n+\t}\n \n \tcqc = MLX5_ADDR_OF(create_cq_in, in, cq_context);\n \tMLX5_SET(cqc, cqc, log_cq_size, ilog2(cq_size));",
        "function_modified_lines": {
            "added": [
                "\tif (err) {",
                "\t\tkvfree(in);",
                "\t}"
            ],
            "deleted": [
                "\tif (err)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the mlx5_fpga_conn_create_cq() function in drivers/net/ethernet/mellanox/mlx5/core/fpga/conn.c in the Linux kernel before 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering mlx5_vector2eqn() failures, aka CID-c8c2a057fdc7.",
        "id": 2126
    },
    {
        "cve_id": "CVE-2019-19083",
        "code_before_change": "static struct clock_source *dce120_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(*clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce112_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\t\t     regs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "code_after_change": "static struct clock_source *dce120_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(*clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce112_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\t\t     regs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \t\treturn &clk_src->base;\n \t}\n \n+\tkfree(clk_src);\n \tBREAK_TO_DEBUGGER();\n \treturn NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\tkfree(clk_src);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leaks in *clock_source_create() functions under drivers/gpu/drm/amd/display/dc in the Linux kernel before 5.3.8 allow attackers to cause a denial of service (memory consumption). This affects the dce112_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce112/dce112_resource.c, the dce100_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce100/dce100_resource.c, the dcn10_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dcn10/dcn10_resource.c, the dcn20_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dcn20/dcn20_resource.c, the dce120_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce120/dce120_resource.c, the dce110_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce110/dce110_resource.c, and the dce80_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce80/dce80_resource.c, aka CID-055e547478a1.",
        "id": 2173
    },
    {
        "cve_id": "CVE-2019-19070",
        "code_before_change": "static int spi_gpio_probe(struct platform_device *pdev)\n{\n\tint\t\t\t\tstatus;\n\tstruct spi_master\t\t*master;\n\tstruct spi_gpio\t\t\t*spi_gpio;\n\tstruct device\t\t\t*dev = &pdev->dev;\n\tstruct spi_bitbang\t\t*bb;\n\tconst struct of_device_id\t*of_id;\n\n\tof_id = of_match_device(spi_gpio_dt_ids, &pdev->dev);\n\n\tmaster = spi_alloc_master(dev, sizeof(*spi_gpio));\n\tif (!master)\n\t\treturn -ENOMEM;\n\n\tstatus = devm_add_action_or_reset(&pdev->dev, spi_gpio_put, master);\n\tif (status)\n\t\treturn status;\n\n\tif (of_id)\n\t\tstatus = spi_gpio_probe_dt(pdev, master);\n\telse\n\t\tstatus = spi_gpio_probe_pdata(pdev, master);\n\n\tif (status)\n\t\treturn status;\n\n\tspi_gpio = spi_master_get_devdata(master);\n\n\tstatus = spi_gpio_request(dev, spi_gpio);\n\tif (status)\n\t\treturn status;\n\n\tmaster->bits_per_word_mask = SPI_BPW_RANGE_MASK(1, 32);\n\tmaster->mode_bits = SPI_3WIRE | SPI_3WIRE_HIZ | SPI_CPHA | SPI_CPOL |\n\t\t\t    SPI_CS_HIGH;\n\tif (!spi_gpio->mosi) {\n\t\t/* HW configuration without MOSI pin\n\t\t *\n\t\t * No setting SPI_MASTER_NO_RX here - if there is only\n\t\t * a MOSI pin connected the host can still do RX by\n\t\t * changing the direction of the line.\n\t\t */\n\t\tmaster->flags = SPI_MASTER_NO_TX;\n\t}\n\n\tmaster->bus_num = pdev->id;\n\tmaster->setup = spi_gpio_setup;\n\tmaster->cleanup = spi_gpio_cleanup;\n\n\tbb = &spi_gpio->bitbang;\n\tbb->master = master;\n\t/*\n\t * There is some additional business, apart from driving the CS GPIO\n\t * line, that we need to do on selection. This makes the local\n\t * callback for chipselect always get called.\n\t */\n\tmaster->flags |= SPI_MASTER_GPIO_SS;\n\tbb->chipselect = spi_gpio_chipselect;\n\tbb->set_line_direction = spi_gpio_set_direction;\n\n\tif (master->flags & SPI_MASTER_NO_TX) {\n\t\tbb->txrx_word[SPI_MODE_0] = spi_gpio_spec_txrx_word_mode0;\n\t\tbb->txrx_word[SPI_MODE_1] = spi_gpio_spec_txrx_word_mode1;\n\t\tbb->txrx_word[SPI_MODE_2] = spi_gpio_spec_txrx_word_mode2;\n\t\tbb->txrx_word[SPI_MODE_3] = spi_gpio_spec_txrx_word_mode3;\n\t} else {\n\t\tbb->txrx_word[SPI_MODE_0] = spi_gpio_txrx_word_mode0;\n\t\tbb->txrx_word[SPI_MODE_1] = spi_gpio_txrx_word_mode1;\n\t\tbb->txrx_word[SPI_MODE_2] = spi_gpio_txrx_word_mode2;\n\t\tbb->txrx_word[SPI_MODE_3] = spi_gpio_txrx_word_mode3;\n\t}\n\tbb->setup_transfer = spi_bitbang_setup_transfer;\n\n\tstatus = spi_bitbang_init(&spi_gpio->bitbang);\n\tif (status)\n\t\treturn status;\n\n\treturn devm_spi_register_master(&pdev->dev, spi_master_get(master));\n}",
        "code_after_change": "static int spi_gpio_probe(struct platform_device *pdev)\n{\n\tint\t\t\t\tstatus;\n\tstruct spi_master\t\t*master;\n\tstruct spi_gpio\t\t\t*spi_gpio;\n\tstruct device\t\t\t*dev = &pdev->dev;\n\tstruct spi_bitbang\t\t*bb;\n\tconst struct of_device_id\t*of_id;\n\n\tof_id = of_match_device(spi_gpio_dt_ids, &pdev->dev);\n\n\tmaster = spi_alloc_master(dev, sizeof(*spi_gpio));\n\tif (!master)\n\t\treturn -ENOMEM;\n\n\tstatus = devm_add_action_or_reset(&pdev->dev, spi_gpio_put, master);\n\tif (status) {\n\t\tspi_master_put(master);\n\t\treturn status;\n\t}\n\n\tif (of_id)\n\t\tstatus = spi_gpio_probe_dt(pdev, master);\n\telse\n\t\tstatus = spi_gpio_probe_pdata(pdev, master);\n\n\tif (status)\n\t\treturn status;\n\n\tspi_gpio = spi_master_get_devdata(master);\n\n\tstatus = spi_gpio_request(dev, spi_gpio);\n\tif (status)\n\t\treturn status;\n\n\tmaster->bits_per_word_mask = SPI_BPW_RANGE_MASK(1, 32);\n\tmaster->mode_bits = SPI_3WIRE | SPI_3WIRE_HIZ | SPI_CPHA | SPI_CPOL |\n\t\t\t    SPI_CS_HIGH;\n\tif (!spi_gpio->mosi) {\n\t\t/* HW configuration without MOSI pin\n\t\t *\n\t\t * No setting SPI_MASTER_NO_RX here - if there is only\n\t\t * a MOSI pin connected the host can still do RX by\n\t\t * changing the direction of the line.\n\t\t */\n\t\tmaster->flags = SPI_MASTER_NO_TX;\n\t}\n\n\tmaster->bus_num = pdev->id;\n\tmaster->setup = spi_gpio_setup;\n\tmaster->cleanup = spi_gpio_cleanup;\n\n\tbb = &spi_gpio->bitbang;\n\tbb->master = master;\n\t/*\n\t * There is some additional business, apart from driving the CS GPIO\n\t * line, that we need to do on selection. This makes the local\n\t * callback for chipselect always get called.\n\t */\n\tmaster->flags |= SPI_MASTER_GPIO_SS;\n\tbb->chipselect = spi_gpio_chipselect;\n\tbb->set_line_direction = spi_gpio_set_direction;\n\n\tif (master->flags & SPI_MASTER_NO_TX) {\n\t\tbb->txrx_word[SPI_MODE_0] = spi_gpio_spec_txrx_word_mode0;\n\t\tbb->txrx_word[SPI_MODE_1] = spi_gpio_spec_txrx_word_mode1;\n\t\tbb->txrx_word[SPI_MODE_2] = spi_gpio_spec_txrx_word_mode2;\n\t\tbb->txrx_word[SPI_MODE_3] = spi_gpio_spec_txrx_word_mode3;\n\t} else {\n\t\tbb->txrx_word[SPI_MODE_0] = spi_gpio_txrx_word_mode0;\n\t\tbb->txrx_word[SPI_MODE_1] = spi_gpio_txrx_word_mode1;\n\t\tbb->txrx_word[SPI_MODE_2] = spi_gpio_txrx_word_mode2;\n\t\tbb->txrx_word[SPI_MODE_3] = spi_gpio_txrx_word_mode3;\n\t}\n\tbb->setup_transfer = spi_bitbang_setup_transfer;\n\n\tstatus = spi_bitbang_init(&spi_gpio->bitbang);\n\tif (status)\n\t\treturn status;\n\n\treturn devm_spi_register_master(&pdev->dev, spi_master_get(master));\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,8 +14,10 @@\n \t\treturn -ENOMEM;\n \n \tstatus = devm_add_action_or_reset(&pdev->dev, spi_gpio_put, master);\n-\tif (status)\n+\tif (status) {\n+\t\tspi_master_put(master);\n \t\treturn status;\n+\t}\n \n \tif (of_id)\n \t\tstatus = spi_gpio_probe_dt(pdev, master);",
        "function_modified_lines": {
            "added": [
                "\tif (status) {",
                "\t\tspi_master_put(master);",
                "\t}"
            ],
            "deleted": [
                "\tif (status)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the spi_gpio_probe() function in drivers/spi/spi-gpio.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering devm_add_action_or_reset() failures, aka CID-d3b0ffa1d75d. NOTE: third parties dispute the relevance of this because the system must have already been out of memory before the probe began",
        "id": 2151
    },
    {
        "cve_id": "CVE-2019-19043",
        "code_before_change": "static int i40e_setup_macvlans(struct i40e_vsi *vsi, u16 macvlan_cnt, u16 qcnt,\n\t\t\t       struct net_device *vdev)\n{\n\tstruct i40e_pf *pf = vsi->back;\n\tstruct i40e_hw *hw = &pf->hw;\n\tstruct i40e_vsi_context ctxt;\n\tu16 sections, qmap, num_qps;\n\tstruct i40e_channel *ch;\n\tint i, pow, ret = 0;\n\tu8 offset = 0;\n\n\tif (vsi->type != I40E_VSI_MAIN || !macvlan_cnt)\n\t\treturn -EINVAL;\n\n\tnum_qps = vsi->num_queue_pairs - (macvlan_cnt * qcnt);\n\n\t/* find the next higher power-of-2 of num queue pairs */\n\tpow = fls(roundup_pow_of_two(num_qps) - 1);\n\n\tqmap = (offset << I40E_AQ_VSI_TC_QUE_OFFSET_SHIFT) |\n\t\t(pow << I40E_AQ_VSI_TC_QUE_NUMBER_SHIFT);\n\n\t/* Setup context bits for the main VSI */\n\tsections = I40E_AQ_VSI_PROP_QUEUE_MAP_VALID;\n\tsections |= I40E_AQ_VSI_PROP_SCHED_VALID;\n\tmemset(&ctxt, 0, sizeof(ctxt));\n\tctxt.seid = vsi->seid;\n\tctxt.pf_num = vsi->back->hw.pf_id;\n\tctxt.vf_num = 0;\n\tctxt.uplink_seid = vsi->uplink_seid;\n\tctxt.info = vsi->info;\n\tctxt.info.tc_mapping[0] = cpu_to_le16(qmap);\n\tctxt.info.mapping_flags |= cpu_to_le16(I40E_AQ_VSI_QUE_MAP_CONTIG);\n\tctxt.info.queue_mapping[0] = cpu_to_le16(vsi->base_queue);\n\tctxt.info.valid_sections |= cpu_to_le16(sections);\n\n\t/* Reconfigure RSS for main VSI with new max queue count */\n\tvsi->rss_size = max_t(u16, num_qps, qcnt);\n\tret = i40e_vsi_config_rss(vsi);\n\tif (ret) {\n\t\tdev_info(&pf->pdev->dev,\n\t\t\t \"Failed to reconfig RSS for num_queues (%u)\\n\",\n\t\t\t vsi->rss_size);\n\t\treturn ret;\n\t}\n\tvsi->reconfig_rss = true;\n\tdev_dbg(&vsi->back->pdev->dev,\n\t\t\"Reconfigured RSS with num_queues (%u)\\n\", vsi->rss_size);\n\tvsi->next_base_queue = num_qps;\n\tvsi->cnt_q_avail = vsi->num_queue_pairs - num_qps;\n\n\t/* Update the VSI after updating the VSI queue-mapping\n\t * information\n\t */\n\tret = i40e_aq_update_vsi_params(hw, &ctxt, NULL);\n\tif (ret) {\n\t\tdev_info(&pf->pdev->dev,\n\t\t\t \"Update vsi tc config failed, err %s aq_err %s\\n\",\n\t\t\t i40e_stat_str(hw, ret),\n\t\t\t i40e_aq_str(hw, hw->aq.asq_last_status));\n\t\treturn ret;\n\t}\n\t/* update the local VSI info with updated queue map */\n\ti40e_vsi_update_queue_map(vsi, &ctxt);\n\tvsi->info.valid_sections = 0;\n\n\t/* Create channels for macvlans */\n\tINIT_LIST_HEAD(&vsi->macvlan_list);\n\tfor (i = 0; i < macvlan_cnt; i++) {\n\t\tch = kzalloc(sizeof(*ch), GFP_KERNEL);\n\t\tif (!ch) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free;\n\t\t}\n\t\tINIT_LIST_HEAD(&ch->list);\n\t\tch->num_queue_pairs = qcnt;\n\t\tif (!i40e_setup_channel(pf, vsi, ch)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t\tch->parent_vsi = vsi;\n\t\tvsi->cnt_q_avail -= ch->num_queue_pairs;\n\t\tvsi->macvlan_cnt++;\n\t\tlist_add_tail(&ch->list, &vsi->macvlan_list);\n\t}\n\n\treturn ret;\n\nerr_free:\n\tdev_info(&pf->pdev->dev, \"Failed to setup macvlans\\n\");\n\ti40e_free_macvlan_channels(vsi);\n\n\treturn ret;\n}",
        "code_after_change": "static int i40e_setup_macvlans(struct i40e_vsi *vsi, u16 macvlan_cnt, u16 qcnt,\n\t\t\t       struct net_device *vdev)\n{\n\tstruct i40e_pf *pf = vsi->back;\n\tstruct i40e_hw *hw = &pf->hw;\n\tstruct i40e_vsi_context ctxt;\n\tu16 sections, qmap, num_qps;\n\tstruct i40e_channel *ch;\n\tint i, pow, ret = 0;\n\tu8 offset = 0;\n\n\tif (vsi->type != I40E_VSI_MAIN || !macvlan_cnt)\n\t\treturn -EINVAL;\n\n\tnum_qps = vsi->num_queue_pairs - (macvlan_cnt * qcnt);\n\n\t/* find the next higher power-of-2 of num queue pairs */\n\tpow = fls(roundup_pow_of_two(num_qps) - 1);\n\n\tqmap = (offset << I40E_AQ_VSI_TC_QUE_OFFSET_SHIFT) |\n\t\t(pow << I40E_AQ_VSI_TC_QUE_NUMBER_SHIFT);\n\n\t/* Setup context bits for the main VSI */\n\tsections = I40E_AQ_VSI_PROP_QUEUE_MAP_VALID;\n\tsections |= I40E_AQ_VSI_PROP_SCHED_VALID;\n\tmemset(&ctxt, 0, sizeof(ctxt));\n\tctxt.seid = vsi->seid;\n\tctxt.pf_num = vsi->back->hw.pf_id;\n\tctxt.vf_num = 0;\n\tctxt.uplink_seid = vsi->uplink_seid;\n\tctxt.info = vsi->info;\n\tctxt.info.tc_mapping[0] = cpu_to_le16(qmap);\n\tctxt.info.mapping_flags |= cpu_to_le16(I40E_AQ_VSI_QUE_MAP_CONTIG);\n\tctxt.info.queue_mapping[0] = cpu_to_le16(vsi->base_queue);\n\tctxt.info.valid_sections |= cpu_to_le16(sections);\n\n\t/* Reconfigure RSS for main VSI with new max queue count */\n\tvsi->rss_size = max_t(u16, num_qps, qcnt);\n\tret = i40e_vsi_config_rss(vsi);\n\tif (ret) {\n\t\tdev_info(&pf->pdev->dev,\n\t\t\t \"Failed to reconfig RSS for num_queues (%u)\\n\",\n\t\t\t vsi->rss_size);\n\t\treturn ret;\n\t}\n\tvsi->reconfig_rss = true;\n\tdev_dbg(&vsi->back->pdev->dev,\n\t\t\"Reconfigured RSS with num_queues (%u)\\n\", vsi->rss_size);\n\tvsi->next_base_queue = num_qps;\n\tvsi->cnt_q_avail = vsi->num_queue_pairs - num_qps;\n\n\t/* Update the VSI after updating the VSI queue-mapping\n\t * information\n\t */\n\tret = i40e_aq_update_vsi_params(hw, &ctxt, NULL);\n\tif (ret) {\n\t\tdev_info(&pf->pdev->dev,\n\t\t\t \"Update vsi tc config failed, err %s aq_err %s\\n\",\n\t\t\t i40e_stat_str(hw, ret),\n\t\t\t i40e_aq_str(hw, hw->aq.asq_last_status));\n\t\treturn ret;\n\t}\n\t/* update the local VSI info with updated queue map */\n\ti40e_vsi_update_queue_map(vsi, &ctxt);\n\tvsi->info.valid_sections = 0;\n\n\t/* Create channels for macvlans */\n\tINIT_LIST_HEAD(&vsi->macvlan_list);\n\tfor (i = 0; i < macvlan_cnt; i++) {\n\t\tch = kzalloc(sizeof(*ch), GFP_KERNEL);\n\t\tif (!ch) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free;\n\t\t}\n\t\tINIT_LIST_HEAD(&ch->list);\n\t\tch->num_queue_pairs = qcnt;\n\t\tif (!i40e_setup_channel(pf, vsi, ch)) {\n\t\t\tret = -EINVAL;\n\t\t\tkfree(ch);\n\t\t\tgoto err_free;\n\t\t}\n\t\tch->parent_vsi = vsi;\n\t\tvsi->cnt_q_avail -= ch->num_queue_pairs;\n\t\tvsi->macvlan_cnt++;\n\t\tlist_add_tail(&ch->list, &vsi->macvlan_list);\n\t}\n\n\treturn ret;\n\nerr_free:\n\tdev_info(&pf->pdev->dev, \"Failed to setup macvlans\\n\");\n\ti40e_free_macvlan_channels(vsi);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -76,6 +76,7 @@\n \t\tch->num_queue_pairs = qcnt;\n \t\tif (!i40e_setup_channel(pf, vsi, ch)) {\n \t\t\tret = -EINVAL;\n+\t\t\tkfree(ch);\n \t\t\tgoto err_free;\n \t\t}\n \t\tch->parent_vsi = vsi;",
        "function_modified_lines": {
            "added": [
                "\t\t\tkfree(ch);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the i40e_setup_macvlans() function in drivers/net/ethernet/intel/i40e/i40e_main.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering i40e_setup_channel() failures, aka CID-27d461333459.",
        "id": 2124
    },
    {
        "cve_id": "CVE-2022-24959",
        "code_before_change": "static int yam_siocdevprivate(struct net_device *dev, struct ifreq *ifr, void __user *data, int cmd)\n{\n\tstruct yam_port *yp = netdev_priv(dev);\n\tstruct yamdrv_ioctl_cfg yi;\n\tstruct yamdrv_ioctl_mcs *ym;\n\tint ioctl_cmd;\n\n\tif (copy_from_user(&ioctl_cmd, data, sizeof(int)))\n\t\treturn -EFAULT;\n\n\tif (yp->magic != YAM_MAGIC)\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (cmd != SIOCDEVPRIVATE)\n\t\treturn -EINVAL;\n\n\tswitch (ioctl_cmd) {\n\n\tcase SIOCYAMRESERVED:\n\t\treturn -EINVAL;\t\t\t/* unused */\n\n\tcase SIOCYAMSMCS:\n\t\tif (netif_running(dev))\n\t\t\treturn -EINVAL;\t\t/* Cannot change this parameter when up */\n\t\tym = memdup_user(data, sizeof(struct yamdrv_ioctl_mcs));\n\t\tif (IS_ERR(ym))\n\t\t\treturn PTR_ERR(ym);\n\t\tif (ym->cmd != SIOCYAMSMCS)\n\t\t\treturn -EINVAL;\n\t\tif (ym->bitrate > YAM_MAXBITRATE) {\n\t\t\tkfree(ym);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t/* setting predef as 0 for loading userdefined mcs data */\n\t\tadd_mcs(ym->bits, ym->bitrate, 0);\n\t\tkfree(ym);\n\t\tbreak;\n\n\tcase SIOCYAMSCFG:\n\t\tif (!capable(CAP_SYS_RAWIO))\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&yi, data, sizeof(struct yamdrv_ioctl_cfg)))\n\t\t\treturn -EFAULT;\n\n\t\tif (yi.cmd != SIOCYAMSCFG)\n\t\t\treturn -EINVAL;\n\t\tif ((yi.cfg.mask & YAM_IOBASE) && netif_running(dev))\n\t\t\treturn -EINVAL;\t\t/* Cannot change this parameter when up */\n\t\tif ((yi.cfg.mask & YAM_IRQ) && netif_running(dev))\n\t\t\treturn -EINVAL;\t\t/* Cannot change this parameter when up */\n\t\tif ((yi.cfg.mask & YAM_BITRATE) && netif_running(dev))\n\t\t\treturn -EINVAL;\t\t/* Cannot change this parameter when up */\n\t\tif ((yi.cfg.mask & YAM_BAUDRATE) && netif_running(dev))\n\t\t\treturn -EINVAL;\t\t/* Cannot change this parameter when up */\n\n\t\tif (yi.cfg.mask & YAM_IOBASE) {\n\t\t\typ->iobase = yi.cfg.iobase;\n\t\t\tdev->base_addr = yi.cfg.iobase;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_IRQ) {\n\t\t\tif (yi.cfg.irq > 15)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->irq = yi.cfg.irq;\n\t\t\tdev->irq = yi.cfg.irq;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_BITRATE) {\n\t\t\tif (yi.cfg.bitrate > YAM_MAXBITRATE)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->bitrate = yi.cfg.bitrate;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_BAUDRATE) {\n\t\t\tif (yi.cfg.baudrate > YAM_MAXBAUDRATE)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->baudrate = yi.cfg.baudrate;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_MODE) {\n\t\t\tif (yi.cfg.mode > YAM_MAXMODE)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->dupmode = yi.cfg.mode;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_HOLDDLY) {\n\t\t\tif (yi.cfg.holddly > YAM_MAXHOLDDLY)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->holdd = yi.cfg.holddly;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_TXDELAY) {\n\t\t\tif (yi.cfg.txdelay > YAM_MAXTXDELAY)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->txd = yi.cfg.txdelay;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_TXTAIL) {\n\t\t\tif (yi.cfg.txtail > YAM_MAXTXTAIL)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->txtail = yi.cfg.txtail;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_PERSIST) {\n\t\t\tif (yi.cfg.persist > YAM_MAXPERSIST)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->pers = yi.cfg.persist;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_SLOTTIME) {\n\t\t\tif (yi.cfg.slottime > YAM_MAXSLOTTIME)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->slot = yi.cfg.slottime;\n\t\t\typ->slotcnt = yp->slot / 10;\n\t\t}\n\t\tbreak;\n\n\tcase SIOCYAMGCFG:\n\t\tmemset(&yi, 0, sizeof(yi));\n\t\tyi.cfg.mask = 0xffffffff;\n\t\tyi.cfg.iobase = yp->iobase;\n\t\tyi.cfg.irq = yp->irq;\n\t\tyi.cfg.bitrate = yp->bitrate;\n\t\tyi.cfg.baudrate = yp->baudrate;\n\t\tyi.cfg.mode = yp->dupmode;\n\t\tyi.cfg.txdelay = yp->txd;\n\t\tyi.cfg.holddly = yp->holdd;\n\t\tyi.cfg.txtail = yp->txtail;\n\t\tyi.cfg.persist = yp->pers;\n\t\tyi.cfg.slottime = yp->slot;\n\t\tif (copy_to_user(data, &yi, sizeof(struct yamdrv_ioctl_cfg)))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int yam_siocdevprivate(struct net_device *dev, struct ifreq *ifr, void __user *data, int cmd)\n{\n\tstruct yam_port *yp = netdev_priv(dev);\n\tstruct yamdrv_ioctl_cfg yi;\n\tstruct yamdrv_ioctl_mcs *ym;\n\tint ioctl_cmd;\n\n\tif (copy_from_user(&ioctl_cmd, data, sizeof(int)))\n\t\treturn -EFAULT;\n\n\tif (yp->magic != YAM_MAGIC)\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (cmd != SIOCDEVPRIVATE)\n\t\treturn -EINVAL;\n\n\tswitch (ioctl_cmd) {\n\n\tcase SIOCYAMRESERVED:\n\t\treturn -EINVAL;\t\t\t/* unused */\n\n\tcase SIOCYAMSMCS:\n\t\tif (netif_running(dev))\n\t\t\treturn -EINVAL;\t\t/* Cannot change this parameter when up */\n\t\tym = memdup_user(data, sizeof(struct yamdrv_ioctl_mcs));\n\t\tif (IS_ERR(ym))\n\t\t\treturn PTR_ERR(ym);\n\t\tif (ym->cmd != SIOCYAMSMCS || ym->bitrate > YAM_MAXBITRATE) {\n\t\t\tkfree(ym);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t/* setting predef as 0 for loading userdefined mcs data */\n\t\tadd_mcs(ym->bits, ym->bitrate, 0);\n\t\tkfree(ym);\n\t\tbreak;\n\n\tcase SIOCYAMSCFG:\n\t\tif (!capable(CAP_SYS_RAWIO))\n\t\t\treturn -EPERM;\n\t\tif (copy_from_user(&yi, data, sizeof(struct yamdrv_ioctl_cfg)))\n\t\t\treturn -EFAULT;\n\n\t\tif (yi.cmd != SIOCYAMSCFG)\n\t\t\treturn -EINVAL;\n\t\tif ((yi.cfg.mask & YAM_IOBASE) && netif_running(dev))\n\t\t\treturn -EINVAL;\t\t/* Cannot change this parameter when up */\n\t\tif ((yi.cfg.mask & YAM_IRQ) && netif_running(dev))\n\t\t\treturn -EINVAL;\t\t/* Cannot change this parameter when up */\n\t\tif ((yi.cfg.mask & YAM_BITRATE) && netif_running(dev))\n\t\t\treturn -EINVAL;\t\t/* Cannot change this parameter when up */\n\t\tif ((yi.cfg.mask & YAM_BAUDRATE) && netif_running(dev))\n\t\t\treturn -EINVAL;\t\t/* Cannot change this parameter when up */\n\n\t\tif (yi.cfg.mask & YAM_IOBASE) {\n\t\t\typ->iobase = yi.cfg.iobase;\n\t\t\tdev->base_addr = yi.cfg.iobase;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_IRQ) {\n\t\t\tif (yi.cfg.irq > 15)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->irq = yi.cfg.irq;\n\t\t\tdev->irq = yi.cfg.irq;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_BITRATE) {\n\t\t\tif (yi.cfg.bitrate > YAM_MAXBITRATE)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->bitrate = yi.cfg.bitrate;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_BAUDRATE) {\n\t\t\tif (yi.cfg.baudrate > YAM_MAXBAUDRATE)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->baudrate = yi.cfg.baudrate;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_MODE) {\n\t\t\tif (yi.cfg.mode > YAM_MAXMODE)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->dupmode = yi.cfg.mode;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_HOLDDLY) {\n\t\t\tif (yi.cfg.holddly > YAM_MAXHOLDDLY)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->holdd = yi.cfg.holddly;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_TXDELAY) {\n\t\t\tif (yi.cfg.txdelay > YAM_MAXTXDELAY)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->txd = yi.cfg.txdelay;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_TXTAIL) {\n\t\t\tif (yi.cfg.txtail > YAM_MAXTXTAIL)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->txtail = yi.cfg.txtail;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_PERSIST) {\n\t\t\tif (yi.cfg.persist > YAM_MAXPERSIST)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->pers = yi.cfg.persist;\n\t\t}\n\t\tif (yi.cfg.mask & YAM_SLOTTIME) {\n\t\t\tif (yi.cfg.slottime > YAM_MAXSLOTTIME)\n\t\t\t\treturn -EINVAL;\n\t\t\typ->slot = yi.cfg.slottime;\n\t\t\typ->slotcnt = yp->slot / 10;\n\t\t}\n\t\tbreak;\n\n\tcase SIOCYAMGCFG:\n\t\tmemset(&yi, 0, sizeof(yi));\n\t\tyi.cfg.mask = 0xffffffff;\n\t\tyi.cfg.iobase = yp->iobase;\n\t\tyi.cfg.irq = yp->irq;\n\t\tyi.cfg.bitrate = yp->bitrate;\n\t\tyi.cfg.baudrate = yp->baudrate;\n\t\tyi.cfg.mode = yp->dupmode;\n\t\tyi.cfg.txdelay = yp->txd;\n\t\tyi.cfg.holddly = yp->holdd;\n\t\tyi.cfg.txtail = yp->txtail;\n\t\tyi.cfg.persist = yp->pers;\n\t\tyi.cfg.slottime = yp->slot;\n\t\tif (copy_to_user(data, &yi, sizeof(struct yamdrv_ioctl_cfg)))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,9 +28,7 @@\n \t\tym = memdup_user(data, sizeof(struct yamdrv_ioctl_mcs));\n \t\tif (IS_ERR(ym))\n \t\t\treturn PTR_ERR(ym);\n-\t\tif (ym->cmd != SIOCYAMSMCS)\n-\t\t\treturn -EINVAL;\n-\t\tif (ym->bitrate > YAM_MAXBITRATE) {\n+\t\tif (ym->cmd != SIOCYAMSMCS || ym->bitrate > YAM_MAXBITRATE) {\n \t\t\tkfree(ym);\n \t\t\treturn -EINVAL;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (ym->cmd != SIOCYAMSMCS || ym->bitrate > YAM_MAXBITRATE) {"
            ],
            "deleted": [
                "\t\tif (ym->cmd != SIOCYAMSMCS)",
                "\t\t\treturn -EINVAL;",
                "\t\tif (ym->bitrate > YAM_MAXBITRATE) {"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.16.5. There is a memory leak in yam_siocdevprivate in drivers/net/hamradio/yam.c.",
        "id": 3474
    },
    {
        "cve_id": "CVE-2022-27950",
        "code_before_change": "static int elo_probe(struct hid_device *hdev, const struct hid_device_id *id)\n{\n\tstruct elo_priv *priv;\n\tint ret;\n\tstruct usb_device *udev;\n\n\tif (!hid_is_usb(hdev))\n\t\treturn -EINVAL;\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tINIT_DELAYED_WORK(&priv->work, elo_work);\n\tudev = interface_to_usbdev(to_usb_interface(hdev->dev.parent));\n\tpriv->usbdev = usb_get_dev(udev);\n\n\thid_set_drvdata(hdev, priv);\n\n\tret = hid_parse(hdev);\n\tif (ret) {\n\t\thid_err(hdev, \"parse failed\\n\");\n\t\tgoto err_free;\n\t}\n\n\tret = hid_hw_start(hdev, HID_CONNECT_DEFAULT);\n\tif (ret) {\n\t\thid_err(hdev, \"hw start failed\\n\");\n\t\tgoto err_free;\n\t}\n\n\tif (elo_broken_firmware(priv->usbdev)) {\n\t\thid_info(hdev, \"broken firmware found, installing workaround\\n\");\n\t\tqueue_delayed_work(wq, &priv->work, ELO_PERIODIC_READ_INTERVAL);\n\t}\n\n\treturn 0;\nerr_free:\n\tkfree(priv);\n\treturn ret;\n}",
        "code_after_change": "static int elo_probe(struct hid_device *hdev, const struct hid_device_id *id)\n{\n\tstruct elo_priv *priv;\n\tint ret;\n\tstruct usb_device *udev;\n\n\tif (!hid_is_usb(hdev))\n\t\treturn -EINVAL;\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tINIT_DELAYED_WORK(&priv->work, elo_work);\n\tudev = interface_to_usbdev(to_usb_interface(hdev->dev.parent));\n\tpriv->usbdev = usb_get_dev(udev);\n\n\thid_set_drvdata(hdev, priv);\n\n\tret = hid_parse(hdev);\n\tif (ret) {\n\t\thid_err(hdev, \"parse failed\\n\");\n\t\tgoto err_free;\n\t}\n\n\tret = hid_hw_start(hdev, HID_CONNECT_DEFAULT);\n\tif (ret) {\n\t\thid_err(hdev, \"hw start failed\\n\");\n\t\tgoto err_free;\n\t}\n\n\tif (elo_broken_firmware(priv->usbdev)) {\n\t\thid_info(hdev, \"broken firmware found, installing workaround\\n\");\n\t\tqueue_delayed_work(wq, &priv->work, ELO_PERIODIC_READ_INTERVAL);\n\t}\n\n\treturn 0;\nerr_free:\n\tusb_put_dev(udev);\n\tkfree(priv);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -36,6 +36,7 @@\n \n \treturn 0;\n err_free:\n+\tusb_put_dev(udev);\n \tkfree(priv);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tusb_put_dev(udev);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "In drivers/hid/hid-elo.c in the Linux kernel before 5.16.11, a memory leak exists for a certain hid_parse error condition.",
        "id": 3498
    },
    {
        "cve_id": "CVE-2022-0854",
        "code_before_change": "void swiotlb_sync_single_for_device(struct device *dev, phys_addr_t tlb_addr,\n\t\tsize_t size, enum dma_data_direction dir)\n{\n\tif (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)\n\t\tswiotlb_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);\n\telse\n\t\tBUG_ON(dir != DMA_FROM_DEVICE);\n}",
        "code_after_change": "void swiotlb_sync_single_for_device(struct device *dev, phys_addr_t tlb_addr,\n\t\tsize_t size, enum dma_data_direction dir)\n{\n\t/*\n\t * Unconditional bounce is necessary to avoid corruption on\n\t * sync_*_for_cpu or dma_ummap_* when the device didn't overwrite\n\t * the whole lengt of the bounce buffer.\n\t */\n\tswiotlb_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);\n\tBUG_ON(!valid_dma_direction(dir));\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,11 @@\n void swiotlb_sync_single_for_device(struct device *dev, phys_addr_t tlb_addr,\n \t\tsize_t size, enum dma_data_direction dir)\n {\n-\tif (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)\n-\t\tswiotlb_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);\n-\telse\n-\t\tBUG_ON(dir != DMA_FROM_DEVICE);\n+\t/*\n+\t * Unconditional bounce is necessary to avoid corruption on\n+\t * sync_*_for_cpu or dma_ummap_* when the device didn't overwrite\n+\t * the whole lengt of the bounce buffer.\n+\t */\n+\tswiotlb_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);\n+\tBUG_ON(!valid_dma_direction(dir));\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * Unconditional bounce is necessary to avoid corruption on",
                "\t * sync_*_for_cpu or dma_ummap_* when the device didn't overwrite",
                "\t * the whole lengt of the bounce buffer.",
                "\t */",
                "\tswiotlb_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);",
                "\tBUG_ON(!valid_dma_direction(dir));"
            ],
            "deleted": [
                "\tif (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)",
                "\t\tswiotlb_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);",
                "\telse",
                "\t\tBUG_ON(dir != DMA_FROM_DEVICE);"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak flaw was found in the Linux kernel\u2019s DMA subsystem, in the way a user calls DMA_FROM_DEVICE. This flaw allows a local user to read random memory from the kernel space.",
        "id": 3227
    },
    {
        "cve_id": "CVE-2020-25704",
        "code_before_change": "static int\nperf_event_parse_addr_filter(struct perf_event *event, char *fstr,\n\t\t\t     struct list_head *filters)\n{\n\tstruct perf_addr_filter *filter = NULL;\n\tchar *start, *orig, *filename = NULL;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tint state = IF_STATE_ACTION, token;\n\tunsigned int kernel = 0;\n\tint ret = -EINVAL;\n\n\torig = fstr = kstrdup(fstr, GFP_KERNEL);\n\tif (!fstr)\n\t\treturn -ENOMEM;\n\n\twhile ((start = strsep(&fstr, \" ,\\n\")) != NULL) {\n\t\tstatic const enum perf_addr_filter_action_t actions[] = {\n\t\t\t[IF_ACT_FILTER]\t= PERF_ADDR_FILTER_ACTION_FILTER,\n\t\t\t[IF_ACT_START]\t= PERF_ADDR_FILTER_ACTION_START,\n\t\t\t[IF_ACT_STOP]\t= PERF_ADDR_FILTER_ACTION_STOP,\n\t\t};\n\t\tret = -EINVAL;\n\n\t\tif (!*start)\n\t\t\tcontinue;\n\n\t\t/* filter definition begins */\n\t\tif (state == IF_STATE_ACTION) {\n\t\t\tfilter = perf_addr_filter_new(event, filters);\n\t\t\tif (!filter)\n\t\t\t\tgoto fail;\n\t\t}\n\n\t\ttoken = match_token(start, if_tokens, args);\n\t\tswitch (token) {\n\t\tcase IF_ACT_FILTER:\n\t\tcase IF_ACT_START:\n\t\tcase IF_ACT_STOP:\n\t\t\tif (state != IF_STATE_ACTION)\n\t\t\t\tgoto fail;\n\n\t\t\tfilter->action = actions[token];\n\t\t\tstate = IF_STATE_SOURCE;\n\t\t\tbreak;\n\n\t\tcase IF_SRC_KERNELADDR:\n\t\tcase IF_SRC_KERNEL:\n\t\t\tkernel = 1;\n\t\t\tfallthrough;\n\n\t\tcase IF_SRC_FILEADDR:\n\t\tcase IF_SRC_FILE:\n\t\t\tif (state != IF_STATE_SOURCE)\n\t\t\t\tgoto fail;\n\n\t\t\t*args[0].to = 0;\n\t\t\tret = kstrtoul(args[0].from, 0, &filter->offset);\n\t\t\tif (ret)\n\t\t\t\tgoto fail;\n\n\t\t\tif (token == IF_SRC_KERNEL || token == IF_SRC_FILE) {\n\t\t\t\t*args[1].to = 0;\n\t\t\t\tret = kstrtoul(args[1].from, 0, &filter->size);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tif (token == IF_SRC_FILE || token == IF_SRC_FILEADDR) {\n\t\t\t\tint fpos = token == IF_SRC_FILE ? 2 : 1;\n\n\t\t\t\tfilename = match_strdup(&args[fpos]);\n\t\t\t\tif (!filename) {\n\t\t\t\t\tret = -ENOMEM;\n\t\t\t\t\tgoto fail;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstate = IF_STATE_END;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto fail;\n\t\t}\n\n\t\t/*\n\t\t * Filter definition is fully parsed, validate and install it.\n\t\t * Make sure that it doesn't contradict itself or the event's\n\t\t * attribute.\n\t\t */\n\t\tif (state == IF_STATE_END) {\n\t\t\tret = -EINVAL;\n\t\t\tif (kernel && event->attr.exclude_kernel)\n\t\t\t\tgoto fail;\n\n\t\t\t/*\n\t\t\t * ACTION \"filter\" must have a non-zero length region\n\t\t\t * specified.\n\t\t\t */\n\t\t\tif (filter->action == PERF_ADDR_FILTER_ACTION_FILTER &&\n\t\t\t    !filter->size)\n\t\t\t\tgoto fail;\n\n\t\t\tif (!kernel) {\n\t\t\t\tif (!filename)\n\t\t\t\t\tgoto fail;\n\n\t\t\t\t/*\n\t\t\t\t * For now, we only support file-based filters\n\t\t\t\t * in per-task events; doing so for CPU-wide\n\t\t\t\t * events requires additional context switching\n\t\t\t\t * trickery, since same object code will be\n\t\t\t\t * mapped at different virtual addresses in\n\t\t\t\t * different processes.\n\t\t\t\t */\n\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\tif (!event->ctx->task)\n\t\t\t\t\tgoto fail_free_name;\n\n\t\t\t\t/* look up the path and grab its inode */\n\t\t\t\tret = kern_path(filename, LOOKUP_FOLLOW,\n\t\t\t\t\t\t&filter->path);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto fail_free_name;\n\n\t\t\t\tkfree(filename);\n\t\t\t\tfilename = NULL;\n\n\t\t\t\tret = -EINVAL;\n\t\t\t\tif (!filter->path.dentry ||\n\t\t\t\t    !S_ISREG(d_inode(filter->path.dentry)\n\t\t\t\t\t     ->i_mode))\n\t\t\t\t\tgoto fail;\n\n\t\t\t\tevent->addr_filters.nr_file_filters++;\n\t\t\t}\n\n\t\t\t/* ready to consume more filters */\n\t\t\tstate = IF_STATE_ACTION;\n\t\t\tfilter = NULL;\n\t\t}\n\t}\n\n\tif (state != IF_STATE_ACTION)\n\t\tgoto fail;\n\n\tkfree(orig);\n\n\treturn 0;\n\nfail_free_name:\n\tkfree(filename);\nfail:\n\tfree_filters_list(filters);\n\tkfree(orig);\n\n\treturn ret;\n}",
        "code_after_change": "static int\nperf_event_parse_addr_filter(struct perf_event *event, char *fstr,\n\t\t\t     struct list_head *filters)\n{\n\tstruct perf_addr_filter *filter = NULL;\n\tchar *start, *orig, *filename = NULL;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tint state = IF_STATE_ACTION, token;\n\tunsigned int kernel = 0;\n\tint ret = -EINVAL;\n\n\torig = fstr = kstrdup(fstr, GFP_KERNEL);\n\tif (!fstr)\n\t\treturn -ENOMEM;\n\n\twhile ((start = strsep(&fstr, \" ,\\n\")) != NULL) {\n\t\tstatic const enum perf_addr_filter_action_t actions[] = {\n\t\t\t[IF_ACT_FILTER]\t= PERF_ADDR_FILTER_ACTION_FILTER,\n\t\t\t[IF_ACT_START]\t= PERF_ADDR_FILTER_ACTION_START,\n\t\t\t[IF_ACT_STOP]\t= PERF_ADDR_FILTER_ACTION_STOP,\n\t\t};\n\t\tret = -EINVAL;\n\n\t\tif (!*start)\n\t\t\tcontinue;\n\n\t\t/* filter definition begins */\n\t\tif (state == IF_STATE_ACTION) {\n\t\t\tfilter = perf_addr_filter_new(event, filters);\n\t\t\tif (!filter)\n\t\t\t\tgoto fail;\n\t\t}\n\n\t\ttoken = match_token(start, if_tokens, args);\n\t\tswitch (token) {\n\t\tcase IF_ACT_FILTER:\n\t\tcase IF_ACT_START:\n\t\tcase IF_ACT_STOP:\n\t\t\tif (state != IF_STATE_ACTION)\n\t\t\t\tgoto fail;\n\n\t\t\tfilter->action = actions[token];\n\t\t\tstate = IF_STATE_SOURCE;\n\t\t\tbreak;\n\n\t\tcase IF_SRC_KERNELADDR:\n\t\tcase IF_SRC_KERNEL:\n\t\t\tkernel = 1;\n\t\t\tfallthrough;\n\n\t\tcase IF_SRC_FILEADDR:\n\t\tcase IF_SRC_FILE:\n\t\t\tif (state != IF_STATE_SOURCE)\n\t\t\t\tgoto fail;\n\n\t\t\t*args[0].to = 0;\n\t\t\tret = kstrtoul(args[0].from, 0, &filter->offset);\n\t\t\tif (ret)\n\t\t\t\tgoto fail;\n\n\t\t\tif (token == IF_SRC_KERNEL || token == IF_SRC_FILE) {\n\t\t\t\t*args[1].to = 0;\n\t\t\t\tret = kstrtoul(args[1].from, 0, &filter->size);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tif (token == IF_SRC_FILE || token == IF_SRC_FILEADDR) {\n\t\t\t\tint fpos = token == IF_SRC_FILE ? 2 : 1;\n\n\t\t\t\tkfree(filename);\n\t\t\t\tfilename = match_strdup(&args[fpos]);\n\t\t\t\tif (!filename) {\n\t\t\t\t\tret = -ENOMEM;\n\t\t\t\t\tgoto fail;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstate = IF_STATE_END;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto fail;\n\t\t}\n\n\t\t/*\n\t\t * Filter definition is fully parsed, validate and install it.\n\t\t * Make sure that it doesn't contradict itself or the event's\n\t\t * attribute.\n\t\t */\n\t\tif (state == IF_STATE_END) {\n\t\t\tret = -EINVAL;\n\t\t\tif (kernel && event->attr.exclude_kernel)\n\t\t\t\tgoto fail;\n\n\t\t\t/*\n\t\t\t * ACTION \"filter\" must have a non-zero length region\n\t\t\t * specified.\n\t\t\t */\n\t\t\tif (filter->action == PERF_ADDR_FILTER_ACTION_FILTER &&\n\t\t\t    !filter->size)\n\t\t\t\tgoto fail;\n\n\t\t\tif (!kernel) {\n\t\t\t\tif (!filename)\n\t\t\t\t\tgoto fail;\n\n\t\t\t\t/*\n\t\t\t\t * For now, we only support file-based filters\n\t\t\t\t * in per-task events; doing so for CPU-wide\n\t\t\t\t * events requires additional context switching\n\t\t\t\t * trickery, since same object code will be\n\t\t\t\t * mapped at different virtual addresses in\n\t\t\t\t * different processes.\n\t\t\t\t */\n\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\tif (!event->ctx->task)\n\t\t\t\t\tgoto fail;\n\n\t\t\t\t/* look up the path and grab its inode */\n\t\t\t\tret = kern_path(filename, LOOKUP_FOLLOW,\n\t\t\t\t\t\t&filter->path);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto fail;\n\n\t\t\t\tret = -EINVAL;\n\t\t\t\tif (!filter->path.dentry ||\n\t\t\t\t    !S_ISREG(d_inode(filter->path.dentry)\n\t\t\t\t\t     ->i_mode))\n\t\t\t\t\tgoto fail;\n\n\t\t\t\tevent->addr_filters.nr_file_filters++;\n\t\t\t}\n\n\t\t\t/* ready to consume more filters */\n\t\t\tstate = IF_STATE_ACTION;\n\t\t\tfilter = NULL;\n\t\t}\n\t}\n\n\tif (state != IF_STATE_ACTION)\n\t\tgoto fail;\n\n\tkfree(filename);\n\tkfree(orig);\n\n\treturn 0;\n\nfail:\n\tkfree(filename);\n\tfree_filters_list(filters);\n\tkfree(orig);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -68,6 +68,7 @@\n \t\t\tif (token == IF_SRC_FILE || token == IF_SRC_FILEADDR) {\n \t\t\t\tint fpos = token == IF_SRC_FILE ? 2 : 1;\n \n+\t\t\t\tkfree(filename);\n \t\t\t\tfilename = match_strdup(&args[fpos]);\n \t\t\t\tif (!filename) {\n \t\t\t\t\tret = -ENOMEM;\n@@ -114,16 +115,13 @@\n \t\t\t\t */\n \t\t\t\tret = -EOPNOTSUPP;\n \t\t\t\tif (!event->ctx->task)\n-\t\t\t\t\tgoto fail_free_name;\n+\t\t\t\t\tgoto fail;\n \n \t\t\t\t/* look up the path and grab its inode */\n \t\t\t\tret = kern_path(filename, LOOKUP_FOLLOW,\n \t\t\t\t\t\t&filter->path);\n \t\t\t\tif (ret)\n-\t\t\t\t\tgoto fail_free_name;\n-\n-\t\t\t\tkfree(filename);\n-\t\t\t\tfilename = NULL;\n+\t\t\t\t\tgoto fail;\n \n \t\t\t\tret = -EINVAL;\n \t\t\t\tif (!filter->path.dentry ||\n@@ -143,13 +141,13 @@\n \tif (state != IF_STATE_ACTION)\n \t\tgoto fail;\n \n+\tkfree(filename);\n \tkfree(orig);\n \n \treturn 0;\n \n-fail_free_name:\n+fail:\n \tkfree(filename);\n-fail:\n \tfree_filters_list(filters);\n \tkfree(orig);\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tkfree(filename);",
                "\t\t\t\t\tgoto fail;",
                "\t\t\t\t\tgoto fail;",
                "\tkfree(filename);",
                "fail:"
            ],
            "deleted": [
                "\t\t\t\t\tgoto fail_free_name;",
                "\t\t\t\t\tgoto fail_free_name;",
                "",
                "\t\t\t\tkfree(filename);",
                "\t\t\t\tfilename = NULL;",
                "fail_free_name:",
                "fail:"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw memory leak in the Linux kernel performance monitoring subsystem was found in the way if using PERF_EVENT_IOC_SET_FILTER. A local user could use this flaw to starve the resources causing denial of service.",
        "id": 2599
    },
    {
        "cve_id": "CVE-2019-18806",
        "code_before_change": "static int ql_alloc_large_buffers(struct ql3_adapter *qdev)\n{\n\tint i;\n\tstruct ql_rcv_buf_cb *lrg_buf_cb;\n\tstruct sk_buff *skb;\n\tdma_addr_t map;\n\tint err;\n\n\tfor (i = 0; i < qdev->num_large_buffers; i++) {\n\t\tskb = netdev_alloc_skb(qdev->ndev,\n\t\t\t\t       qdev->lrg_buffer_len);\n\t\tif (unlikely(!skb)) {\n\t\t\t/* Better luck next round */\n\t\t\tnetdev_err(qdev->ndev,\n\t\t\t\t   \"large buff alloc failed for %d bytes at index %d\\n\",\n\t\t\t\t   qdev->lrg_buffer_len * 2, i);\n\t\t\tql_free_large_buffers(qdev);\n\t\t\treturn -ENOMEM;\n\t\t} else {\n\n\t\t\tlrg_buf_cb = &qdev->lrg_buf[i];\n\t\t\tmemset(lrg_buf_cb, 0, sizeof(struct ql_rcv_buf_cb));\n\t\t\tlrg_buf_cb->index = i;\n\t\t\tlrg_buf_cb->skb = skb;\n\t\t\t/*\n\t\t\t * We save some space to copy the ethhdr from first\n\t\t\t * buffer\n\t\t\t */\n\t\t\tskb_reserve(skb, QL_HEADER_SPACE);\n\t\t\tmap = pci_map_single(qdev->pdev,\n\t\t\t\t\t     skb->data,\n\t\t\t\t\t     qdev->lrg_buffer_len -\n\t\t\t\t\t     QL_HEADER_SPACE,\n\t\t\t\t\t     PCI_DMA_FROMDEVICE);\n\n\t\t\terr = pci_dma_mapping_error(qdev->pdev, map);\n\t\t\tif (err) {\n\t\t\t\tnetdev_err(qdev->ndev,\n\t\t\t\t\t   \"PCI mapping failed with error: %d\\n\",\n\t\t\t\t\t   err);\n\t\t\t\tql_free_large_buffers(qdev);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tdma_unmap_addr_set(lrg_buf_cb, mapaddr, map);\n\t\t\tdma_unmap_len_set(lrg_buf_cb, maplen,\n\t\t\t\t\t  qdev->lrg_buffer_len -\n\t\t\t\t\t  QL_HEADER_SPACE);\n\t\t\tlrg_buf_cb->buf_phy_addr_low =\n\t\t\t    cpu_to_le32(LS_64BITS(map));\n\t\t\tlrg_buf_cb->buf_phy_addr_high =\n\t\t\t    cpu_to_le32(MS_64BITS(map));\n\t\t}\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int ql_alloc_large_buffers(struct ql3_adapter *qdev)\n{\n\tint i;\n\tstruct ql_rcv_buf_cb *lrg_buf_cb;\n\tstruct sk_buff *skb;\n\tdma_addr_t map;\n\tint err;\n\n\tfor (i = 0; i < qdev->num_large_buffers; i++) {\n\t\tskb = netdev_alloc_skb(qdev->ndev,\n\t\t\t\t       qdev->lrg_buffer_len);\n\t\tif (unlikely(!skb)) {\n\t\t\t/* Better luck next round */\n\t\t\tnetdev_err(qdev->ndev,\n\t\t\t\t   \"large buff alloc failed for %d bytes at index %d\\n\",\n\t\t\t\t   qdev->lrg_buffer_len * 2, i);\n\t\t\tql_free_large_buffers(qdev);\n\t\t\treturn -ENOMEM;\n\t\t} else {\n\n\t\t\tlrg_buf_cb = &qdev->lrg_buf[i];\n\t\t\tmemset(lrg_buf_cb, 0, sizeof(struct ql_rcv_buf_cb));\n\t\t\tlrg_buf_cb->index = i;\n\t\t\tlrg_buf_cb->skb = skb;\n\t\t\t/*\n\t\t\t * We save some space to copy the ethhdr from first\n\t\t\t * buffer\n\t\t\t */\n\t\t\tskb_reserve(skb, QL_HEADER_SPACE);\n\t\t\tmap = pci_map_single(qdev->pdev,\n\t\t\t\t\t     skb->data,\n\t\t\t\t\t     qdev->lrg_buffer_len -\n\t\t\t\t\t     QL_HEADER_SPACE,\n\t\t\t\t\t     PCI_DMA_FROMDEVICE);\n\n\t\t\terr = pci_dma_mapping_error(qdev->pdev, map);\n\t\t\tif (err) {\n\t\t\t\tnetdev_err(qdev->ndev,\n\t\t\t\t\t   \"PCI mapping failed with error: %d\\n\",\n\t\t\t\t\t   err);\n\t\t\t\tdev_kfree_skb_irq(skb);\n\t\t\t\tql_free_large_buffers(qdev);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tdma_unmap_addr_set(lrg_buf_cb, mapaddr, map);\n\t\t\tdma_unmap_len_set(lrg_buf_cb, maplen,\n\t\t\t\t\t  qdev->lrg_buffer_len -\n\t\t\t\t\t  QL_HEADER_SPACE);\n\t\t\tlrg_buf_cb->buf_phy_addr_low =\n\t\t\t    cpu_to_le32(LS_64BITS(map));\n\t\t\tlrg_buf_cb->buf_phy_addr_high =\n\t\t\t    cpu_to_le32(MS_64BITS(map));\n\t\t}\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,6 +38,7 @@\n \t\t\t\tnetdev_err(qdev->ndev,\n \t\t\t\t\t   \"PCI mapping failed with error: %d\\n\",\n \t\t\t\t\t   err);\n+\t\t\t\tdev_kfree_skb_irq(skb);\n \t\t\t\tql_free_large_buffers(qdev);\n \t\t\t\treturn -ENOMEM;\n \t\t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tdev_kfree_skb_irq(skb);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the ql_alloc_large_buffers() function in drivers/net/ethernet/qlogic/qla3xxx.c in the Linux kernel before 5.3.5 allows local users to cause a denial of service (memory consumption) by triggering pci_dma_mapping_error() failures, aka CID-1acb8f2a7a9f.",
        "id": 2098
    },
    {
        "cve_id": "CVE-2019-19083",
        "code_before_change": "struct clock_source *dce100_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "code_after_change": "struct clock_source *dce100_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \t\treturn &clk_src->base;\n \t}\n \n+\tkfree(clk_src);\n \tBREAK_TO_DEBUGGER();\n \treturn NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\tkfree(clk_src);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leaks in *clock_source_create() functions under drivers/gpu/drm/amd/display/dc in the Linux kernel before 5.3.8 allow attackers to cause a denial of service (memory consumption). This affects the dce112_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce112/dce112_resource.c, the dce100_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce100/dce100_resource.c, the dcn10_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dcn10/dcn10_resource.c, the dcn20_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dcn20/dcn20_resource.c, the dce120_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce120/dce120_resource.c, the dce110_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce110/dce110_resource.c, and the dce80_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce80/dce80_resource.c, aka CID-055e547478a1.",
        "id": 2170
    },
    {
        "cve_id": "CVE-2019-19079",
        "code_before_change": "static ssize_t qrtr_tun_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tstruct qrtr_tun *tun = filp->private_data;\n\tsize_t len = iov_iter_count(from);\n\tssize_t ret;\n\tvoid *kbuf;\n\n\tkbuf = kzalloc(len, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\n\tif (!copy_from_iter_full(kbuf, len, from))\n\t\treturn -EFAULT;\n\n\tret = qrtr_endpoint_post(&tun->ep, kbuf, len);\n\n\treturn ret < 0 ? ret : len;\n}",
        "code_after_change": "static ssize_t qrtr_tun_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tstruct qrtr_tun *tun = filp->private_data;\n\tsize_t len = iov_iter_count(from);\n\tssize_t ret;\n\tvoid *kbuf;\n\n\tkbuf = kzalloc(len, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\n\tif (!copy_from_iter_full(kbuf, len, from)) {\n\t\tkfree(kbuf);\n\t\treturn -EFAULT;\n\t}\n\n\tret = qrtr_endpoint_post(&tun->ep, kbuf, len);\n\n\tkfree(kbuf);\n\treturn ret < 0 ? ret : len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,10 +10,13 @@\n \tif (!kbuf)\n \t\treturn -ENOMEM;\n \n-\tif (!copy_from_iter_full(kbuf, len, from))\n+\tif (!copy_from_iter_full(kbuf, len, from)) {\n+\t\tkfree(kbuf);\n \t\treturn -EFAULT;\n+\t}\n \n \tret = qrtr_endpoint_post(&tun->ep, kbuf, len);\n \n+\tkfree(kbuf);\n \treturn ret < 0 ? ret : len;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!copy_from_iter_full(kbuf, len, from)) {",
                "\t\tkfree(kbuf);",
                "\t}",
                "\tkfree(kbuf);"
            ],
            "deleted": [
                "\tif (!copy_from_iter_full(kbuf, len, from))"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the qrtr_tun_write_iter() function in net/qrtr/tun.c in the Linux kernel before 5.3 allows attackers to cause a denial of service (memory consumption), aka CID-a21b7f0cff19.",
        "id": 2162
    },
    {
        "cve_id": "CVE-2021-4135",
        "code_before_change": "static int\nnsim_bpf_map_alloc(struct netdevsim *ns, struct bpf_offloaded_map *offmap)\n{\n\tstruct nsim_bpf_bound_map *nmap;\n\tint i, err;\n\n\tif (WARN_ON(offmap->map.map_type != BPF_MAP_TYPE_ARRAY &&\n\t\t    offmap->map.map_type != BPF_MAP_TYPE_HASH))\n\t\treturn -EINVAL;\n\tif (offmap->map.max_entries > NSIM_BPF_MAX_KEYS)\n\t\treturn -ENOMEM;\n\tif (offmap->map.map_flags)\n\t\treturn -EINVAL;\n\n\tnmap = kzalloc(sizeof(*nmap), GFP_USER);\n\tif (!nmap)\n\t\treturn -ENOMEM;\n\n\toffmap->dev_priv = nmap;\n\tnmap->ns = ns;\n\tnmap->map = offmap;\n\tmutex_init(&nmap->mutex);\n\n\tif (offmap->map.map_type == BPF_MAP_TYPE_ARRAY) {\n\t\tfor (i = 0; i < ARRAY_SIZE(nmap->entry); i++) {\n\t\t\tu32 *key;\n\n\t\t\terr = nsim_map_alloc_elem(offmap, i);\n\t\t\tif (err)\n\t\t\t\tgoto err_free;\n\t\t\tkey = nmap->entry[i].key;\n\t\t\t*key = i;\n\t\t}\n\t}\n\n\toffmap->dev_ops = &nsim_bpf_map_ops;\n\tlist_add_tail(&nmap->l, &ns->nsim_dev->bpf_bound_maps);\n\n\treturn 0;\n\nerr_free:\n\twhile (--i >= 0) {\n\t\tkfree(nmap->entry[i].key);\n\t\tkfree(nmap->entry[i].value);\n\t}\n\tkfree(nmap);\n\treturn err;\n}",
        "code_after_change": "static int\nnsim_bpf_map_alloc(struct netdevsim *ns, struct bpf_offloaded_map *offmap)\n{\n\tstruct nsim_bpf_bound_map *nmap;\n\tint i, err;\n\n\tif (WARN_ON(offmap->map.map_type != BPF_MAP_TYPE_ARRAY &&\n\t\t    offmap->map.map_type != BPF_MAP_TYPE_HASH))\n\t\treturn -EINVAL;\n\tif (offmap->map.max_entries > NSIM_BPF_MAX_KEYS)\n\t\treturn -ENOMEM;\n\tif (offmap->map.map_flags)\n\t\treturn -EINVAL;\n\n\tnmap = kzalloc(sizeof(*nmap), GFP_USER);\n\tif (!nmap)\n\t\treturn -ENOMEM;\n\n\toffmap->dev_priv = nmap;\n\tnmap->ns = ns;\n\tnmap->map = offmap;\n\tmutex_init(&nmap->mutex);\n\n\tif (offmap->map.map_type == BPF_MAP_TYPE_ARRAY) {\n\t\tfor (i = 0; i < ARRAY_SIZE(nmap->entry); i++) {\n\t\t\tu32 *key;\n\n\t\t\terr = nsim_map_alloc_elem(offmap, i);\n\t\t\tif (err)\n\t\t\t\tgoto err_free;\n\t\t\tkey = nmap->entry[i].key;\n\t\t\t*key = i;\n\t\t\tmemset(nmap->entry[i].value, 0, offmap->map.value_size);\n\t\t}\n\t}\n\n\toffmap->dev_ops = &nsim_bpf_map_ops;\n\tlist_add_tail(&nmap->l, &ns->nsim_dev->bpf_bound_maps);\n\n\treturn 0;\n\nerr_free:\n\twhile (--i >= 0) {\n\t\tkfree(nmap->entry[i].key);\n\t\tkfree(nmap->entry[i].value);\n\t}\n\tkfree(nmap);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,6 +30,7 @@\n \t\t\t\tgoto err_free;\n \t\t\tkey = nmap->entry[i].key;\n \t\t\t*key = i;\n+\t\t\tmemset(nmap->entry[i].value, 0, offmap->map.value_size);\n \t\t}\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tmemset(nmap->entry[i].value, 0, offmap->map.value_size);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak vulnerability was found in the Linux kernel's eBPF for the Simulated networking device driver in the way user uses BPF for the device such that function nsim_map_alloc_elem being called. A local user could use this flaw to get unauthorized access to some data.",
        "id": 3135
    },
    {
        "cve_id": "CVE-2019-19060",
        "code_before_change": "int adis_update_scan_mode(struct iio_dev *indio_dev,\n\tconst unsigned long *scan_mask)\n{\n\tstruct adis *adis = iio_device_get_drvdata(indio_dev);\n\tconst struct iio_chan_spec *chan;\n\tunsigned int scan_count;\n\tunsigned int i, j;\n\t__be16 *tx, *rx;\n\n\tkfree(adis->xfer);\n\tkfree(adis->buffer);\n\n\tif (adis->burst && adis->burst->en)\n\t\treturn adis_update_scan_mode_burst(indio_dev, scan_mask);\n\n\tscan_count = indio_dev->scan_bytes / 2;\n\n\tadis->xfer = kcalloc(scan_count + 1, sizeof(*adis->xfer), GFP_KERNEL);\n\tif (!adis->xfer)\n\t\treturn -ENOMEM;\n\n\tadis->buffer = kcalloc(indio_dev->scan_bytes, 2, GFP_KERNEL);\n\tif (!adis->buffer)\n\t\treturn -ENOMEM;\n\n\trx = adis->buffer;\n\ttx = rx + scan_count;\n\n\tspi_message_init(&adis->msg);\n\n\tfor (j = 0; j <= scan_count; j++) {\n\t\tadis->xfer[j].bits_per_word = 8;\n\t\tif (j != scan_count)\n\t\t\tadis->xfer[j].cs_change = 1;\n\t\tadis->xfer[j].len = 2;\n\t\tadis->xfer[j].delay_usecs = adis->data->read_delay;\n\t\tif (j < scan_count)\n\t\t\tadis->xfer[j].tx_buf = &tx[j];\n\t\tif (j >= 1)\n\t\t\tadis->xfer[j].rx_buf = &rx[j - 1];\n\t\tspi_message_add_tail(&adis->xfer[j], &adis->msg);\n\t}\n\n\tchan = indio_dev->channels;\n\tfor (i = 0; i < indio_dev->num_channels; i++, chan++) {\n\t\tif (!test_bit(chan->scan_index, scan_mask))\n\t\t\tcontinue;\n\t\tif (chan->scan_type.storagebits == 32)\n\t\t\t*tx++ = cpu_to_be16((chan->address + 2) << 8);\n\t\t*tx++ = cpu_to_be16(chan->address << 8);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int adis_update_scan_mode(struct iio_dev *indio_dev,\n\tconst unsigned long *scan_mask)\n{\n\tstruct adis *adis = iio_device_get_drvdata(indio_dev);\n\tconst struct iio_chan_spec *chan;\n\tunsigned int scan_count;\n\tunsigned int i, j;\n\t__be16 *tx, *rx;\n\n\tkfree(adis->xfer);\n\tkfree(adis->buffer);\n\n\tif (adis->burst && adis->burst->en)\n\t\treturn adis_update_scan_mode_burst(indio_dev, scan_mask);\n\n\tscan_count = indio_dev->scan_bytes / 2;\n\n\tadis->xfer = kcalloc(scan_count + 1, sizeof(*adis->xfer), GFP_KERNEL);\n\tif (!adis->xfer)\n\t\treturn -ENOMEM;\n\n\tadis->buffer = kcalloc(indio_dev->scan_bytes, 2, GFP_KERNEL);\n\tif (!adis->buffer) {\n\t\tkfree(adis->xfer);\n\t\tadis->xfer = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\trx = adis->buffer;\n\ttx = rx + scan_count;\n\n\tspi_message_init(&adis->msg);\n\n\tfor (j = 0; j <= scan_count; j++) {\n\t\tadis->xfer[j].bits_per_word = 8;\n\t\tif (j != scan_count)\n\t\t\tadis->xfer[j].cs_change = 1;\n\t\tadis->xfer[j].len = 2;\n\t\tadis->xfer[j].delay_usecs = adis->data->read_delay;\n\t\tif (j < scan_count)\n\t\t\tadis->xfer[j].tx_buf = &tx[j];\n\t\tif (j >= 1)\n\t\t\tadis->xfer[j].rx_buf = &rx[j - 1];\n\t\tspi_message_add_tail(&adis->xfer[j], &adis->msg);\n\t}\n\n\tchan = indio_dev->channels;\n\tfor (i = 0; i < indio_dev->num_channels; i++, chan++) {\n\t\tif (!test_bit(chan->scan_index, scan_mask))\n\t\t\tcontinue;\n\t\tif (chan->scan_type.storagebits == 32)\n\t\t\t*tx++ = cpu_to_be16((chan->address + 2) << 8);\n\t\t*tx++ = cpu_to_be16(chan->address << 8);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,8 +20,11 @@\n \t\treturn -ENOMEM;\n \n \tadis->buffer = kcalloc(indio_dev->scan_bytes, 2, GFP_KERNEL);\n-\tif (!adis->buffer)\n+\tif (!adis->buffer) {\n+\t\tkfree(adis->xfer);\n+\t\tadis->xfer = NULL;\n \t\treturn -ENOMEM;\n+\t}\n \n \trx = adis->buffer;\n \ttx = rx + scan_count;",
        "function_modified_lines": {
            "added": [
                "\tif (!adis->buffer) {",
                "\t\tkfree(adis->xfer);",
                "\t\tadis->xfer = NULL;",
                "\t}"
            ],
            "deleted": [
                "\tif (!adis->buffer)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the adis_update_scan_mode() function in drivers/iio/imu/adis_buffer.c in the Linux kernel before 5.3.9 allows attackers to cause a denial of service (memory consumption), aka CID-ab612b1daf41.",
        "id": 2141
    },
    {
        "cve_id": "CVE-2020-12656",
        "code_before_change": "static void __exit\ncleanup_sunrpc(void)\n{\n\trpc_cleanup_clids();\n\trpcauth_remove_module();\n\tcleanup_socket_xprt();\n\tsvc_cleanup_xprt_sock();\n\tsunrpc_debugfs_exit();\n\tunregister_rpc_pipefs();\n\trpc_destroy_mempool();\n\tunregister_pernet_subsys(&sunrpc_net_ops);\n#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)\n\trpc_unregister_sysctl();\n#endif\n\trcu_barrier(); /* Wait for completion of call_rcu()'s */\n}",
        "code_after_change": "static void __exit\ncleanup_sunrpc(void)\n{\n\trpc_cleanup_clids();\n\trpcauth_remove_module();\n\tcleanup_socket_xprt();\n\tsvc_cleanup_xprt_sock();\n\tsunrpc_debugfs_exit();\n\tunregister_rpc_pipefs();\n\trpc_destroy_mempool();\n\tunregister_pernet_subsys(&sunrpc_net_ops);\n\tauth_domain_cleanup();\n#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)\n\trpc_unregister_sysctl();\n#endif\n\trcu_barrier(); /* Wait for completion of call_rcu()'s */\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,7 @@\n \tunregister_rpc_pipefs();\n \trpc_destroy_mempool();\n \tunregister_pernet_subsys(&sunrpc_net_ops);\n+\tauth_domain_cleanup();\n #if IS_ENABLED(CONFIG_SUNRPC_DEBUG)\n \trpc_unregister_sysctl();\n #endif",
        "function_modified_lines": {
            "added": [
                "\tauth_domain_cleanup();"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "gss_mech_free in net/sunrpc/auth_gss/gss_mech_switch.c in the rpcsec_gss_krb5 implementation in the Linux kernel through 5.6.10 lacks certain domain_release calls, leading to a memory leak. Note: This was disputed with the assertion that the issue does not grant any access not already available. It is a problem that on unloading a specific kernel module some memory is leaked, but loading kernel modules is a privileged operation. A user could also write a kernel module to consume any amount of memory they like and load that replicating the effect of this bug",
        "id": 2478
    },
    {
        "cve_id": "CVE-2019-15921",
        "code_before_change": "int genl_register_family(struct genl_family *family)\n{\n\tint err, i;\n\tint start = GENL_START_ALLOC, end = GENL_MAX_ID;\n\n\terr = genl_validate_ops(family);\n\tif (err)\n\t\treturn err;\n\n\tgenl_lock_all();\n\n\tif (genl_family_find_byname(family->name)) {\n\t\terr = -EEXIST;\n\t\tgoto errout_locked;\n\t}\n\n\t/*\n\t * Sadly, a few cases need to be special-cased\n\t * due to them having previously abused the API\n\t * and having used their family ID also as their\n\t * multicast group ID, so we use reserved IDs\n\t * for both to be sure we can do that mapping.\n\t */\n\tif (family == &genl_ctrl) {\n\t\t/* and this needs to be special for initial family lookups */\n\t\tstart = end = GENL_ID_CTRL;\n\t} else if (strcmp(family->name, \"pmcraid\") == 0) {\n\t\tstart = end = GENL_ID_PMCRAID;\n\t} else if (strcmp(family->name, \"VFS_DQUOT\") == 0) {\n\t\tstart = end = GENL_ID_VFS_DQUOT;\n\t}\n\n\tif (family->maxattr && !family->parallel_ops) {\n\t\tfamily->attrbuf = kmalloc_array(family->maxattr + 1,\n\t\t\t\t\t\tsizeof(struct nlattr *),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (family->attrbuf == NULL) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto errout_locked;\n\t\t}\n\t} else\n\t\tfamily->attrbuf = NULL;\n\n\tfamily->id = idr_alloc(&genl_fam_idr, family,\n\t\t\t       start, end + 1, GFP_KERNEL);\n\tif (family->id < 0) {\n\t\terr = family->id;\n\t\tgoto errout_locked;\n\t}\n\n\terr = genl_validate_assign_mc_groups(family);\n\tif (err)\n\t\tgoto errout_remove;\n\n\tgenl_unlock_all();\n\n\t/* send all events */\n\tgenl_ctrl_event(CTRL_CMD_NEWFAMILY, family, NULL, 0);\n\tfor (i = 0; i < family->n_mcgrps; i++)\n\t\tgenl_ctrl_event(CTRL_CMD_NEWMCAST_GRP, family,\n\t\t\t\t&family->mcgrps[i], family->mcgrp_offset + i);\n\n\treturn 0;\n\nerrout_remove:\n\tidr_remove(&genl_fam_idr, family->id);\n\tkfree(family->attrbuf);\nerrout_locked:\n\tgenl_unlock_all();\n\treturn err;\n}",
        "code_after_change": "int genl_register_family(struct genl_family *family)\n{\n\tint err, i;\n\tint start = GENL_START_ALLOC, end = GENL_MAX_ID;\n\n\terr = genl_validate_ops(family);\n\tif (err)\n\t\treturn err;\n\n\tgenl_lock_all();\n\n\tif (genl_family_find_byname(family->name)) {\n\t\terr = -EEXIST;\n\t\tgoto errout_locked;\n\t}\n\n\t/*\n\t * Sadly, a few cases need to be special-cased\n\t * due to them having previously abused the API\n\t * and having used their family ID also as their\n\t * multicast group ID, so we use reserved IDs\n\t * for both to be sure we can do that mapping.\n\t */\n\tif (family == &genl_ctrl) {\n\t\t/* and this needs to be special for initial family lookups */\n\t\tstart = end = GENL_ID_CTRL;\n\t} else if (strcmp(family->name, \"pmcraid\") == 0) {\n\t\tstart = end = GENL_ID_PMCRAID;\n\t} else if (strcmp(family->name, \"VFS_DQUOT\") == 0) {\n\t\tstart = end = GENL_ID_VFS_DQUOT;\n\t}\n\n\tif (family->maxattr && !family->parallel_ops) {\n\t\tfamily->attrbuf = kmalloc_array(family->maxattr + 1,\n\t\t\t\t\t\tsizeof(struct nlattr *),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (family->attrbuf == NULL) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto errout_locked;\n\t\t}\n\t} else\n\t\tfamily->attrbuf = NULL;\n\n\tfamily->id = idr_alloc(&genl_fam_idr, family,\n\t\t\t       start, end + 1, GFP_KERNEL);\n\tif (family->id < 0) {\n\t\terr = family->id;\n\t\tgoto errout_free;\n\t}\n\n\terr = genl_validate_assign_mc_groups(family);\n\tif (err)\n\t\tgoto errout_remove;\n\n\tgenl_unlock_all();\n\n\t/* send all events */\n\tgenl_ctrl_event(CTRL_CMD_NEWFAMILY, family, NULL, 0);\n\tfor (i = 0; i < family->n_mcgrps; i++)\n\t\tgenl_ctrl_event(CTRL_CMD_NEWMCAST_GRP, family,\n\t\t\t\t&family->mcgrps[i], family->mcgrp_offset + i);\n\n\treturn 0;\n\nerrout_remove:\n\tidr_remove(&genl_fam_idr, family->id);\nerrout_free:\n\tkfree(family->attrbuf);\nerrout_locked:\n\tgenl_unlock_all();\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -45,7 +45,7 @@\n \t\t\t       start, end + 1, GFP_KERNEL);\n \tif (family->id < 0) {\n \t\terr = family->id;\n-\t\tgoto errout_locked;\n+\t\tgoto errout_free;\n \t}\n \n \terr = genl_validate_assign_mc_groups(family);\n@@ -64,6 +64,7 @@\n \n errout_remove:\n \tidr_remove(&genl_fam_idr, family->id);\n+errout_free:\n \tkfree(family->attrbuf);\n errout_locked:\n \tgenl_unlock_all();",
        "function_modified_lines": {
            "added": [
                "\t\tgoto errout_free;",
                "errout_free:"
            ],
            "deleted": [
                "\t\tgoto errout_locked;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.6. There is a memory leak issue when idr_alloc() fails in genl_register_family() in net/netlink/genetlink.c.",
        "id": 2028
    },
    {
        "cve_id": "CVE-2022-26878",
        "code_before_change": "static void virtbt_rx_handle(struct virtio_bluetooth *vbt, struct sk_buff *skb)\n{\n\t__u8 pkt_type;\n\n\tpkt_type = *((__u8 *) skb->data);\n\tskb_pull(skb, 1);\n\n\tswitch (pkt_type) {\n\tcase HCI_EVENT_PKT:\n\tcase HCI_ACLDATA_PKT:\n\tcase HCI_SCODATA_PKT:\n\tcase HCI_ISODATA_PKT:\n\t\thci_skb_pkt_type(skb) = pkt_type;\n\t\thci_recv_frame(vbt->hdev, skb);\n\t\tbreak;\n\t}\n}",
        "code_after_change": "static void virtbt_rx_handle(struct virtio_bluetooth *vbt, struct sk_buff *skb)\n{\n\t__u8 pkt_type;\n\n\tpkt_type = *((__u8 *) skb->data);\n\tskb_pull(skb, 1);\n\n\tswitch (pkt_type) {\n\tcase HCI_EVENT_PKT:\n\tcase HCI_ACLDATA_PKT:\n\tcase HCI_SCODATA_PKT:\n\tcase HCI_ISODATA_PKT:\n\t\thci_skb_pkt_type(skb) = pkt_type;\n\t\thci_recv_frame(vbt->hdev, skb);\n\t\tbreak;\n\tdefault:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,5 +13,8 @@\n \t\thci_skb_pkt_type(skb) = pkt_type;\n \t\thci_recv_frame(vbt->hdev, skb);\n \t\tbreak;\n+\tdefault:\n+\t\tkfree_skb(skb);\n+\t\tbreak;\n \t}\n }",
        "function_modified_lines": {
            "added": [
                "\tdefault:",
                "\t\tkfree_skb(skb);",
                "\t\tbreak;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "drivers/bluetooth/virtio_bt.c in the Linux kernel before 5.16.3 has a memory leak (socket buffers have memory allocated but not freed).",
        "id": 3491
    },
    {
        "cve_id": "CVE-2019-19073",
        "code_before_change": "static int htc_config_pipe_credits(struct htc_target *target)\n{\n\tstruct sk_buff *skb;\n\tstruct htc_config_pipe_msg *cp_msg;\n\tint ret;\n\tunsigned long time_left;\n\n\tskb = alloc_skb(50 + sizeof(struct htc_frame_hdr), GFP_ATOMIC);\n\tif (!skb) {\n\t\tdev_err(target->dev, \"failed to allocate send buffer\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tskb_reserve(skb, sizeof(struct htc_frame_hdr));\n\n\tcp_msg = skb_put(skb, sizeof(struct htc_config_pipe_msg));\n\n\tcp_msg->message_id = cpu_to_be16(HTC_MSG_CONFIG_PIPE_ID);\n\tcp_msg->pipe_id = USB_WLAN_TX_PIPE;\n\tcp_msg->credits = target->credits;\n\n\ttarget->htc_flags |= HTC_OP_CONFIG_PIPE_CREDITS;\n\n\tret = htc_issue_send(target, skb, skb->len, 0, ENDPOINT0);\n\tif (ret)\n\t\tgoto err;\n\n\ttime_left = wait_for_completion_timeout(&target->cmd_wait, HZ);\n\tif (!time_left) {\n\t\tdev_err(target->dev, \"HTC credit config timeout\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\nerr:\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}",
        "code_after_change": "static int htc_config_pipe_credits(struct htc_target *target)\n{\n\tstruct sk_buff *skb;\n\tstruct htc_config_pipe_msg *cp_msg;\n\tint ret;\n\tunsigned long time_left;\n\n\tskb = alloc_skb(50 + sizeof(struct htc_frame_hdr), GFP_ATOMIC);\n\tif (!skb) {\n\t\tdev_err(target->dev, \"failed to allocate send buffer\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tskb_reserve(skb, sizeof(struct htc_frame_hdr));\n\n\tcp_msg = skb_put(skb, sizeof(struct htc_config_pipe_msg));\n\n\tcp_msg->message_id = cpu_to_be16(HTC_MSG_CONFIG_PIPE_ID);\n\tcp_msg->pipe_id = USB_WLAN_TX_PIPE;\n\tcp_msg->credits = target->credits;\n\n\ttarget->htc_flags |= HTC_OP_CONFIG_PIPE_CREDITS;\n\n\tret = htc_issue_send(target, skb, skb->len, 0, ENDPOINT0);\n\tif (ret)\n\t\tgoto err;\n\n\ttime_left = wait_for_completion_timeout(&target->cmd_wait, HZ);\n\tif (!time_left) {\n\t\tdev_err(target->dev, \"HTC credit config timeout\\n\");\n\t\tkfree_skb(skb);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\nerr:\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,6 +27,7 @@\n \ttime_left = wait_for_completion_timeout(&target->cmd_wait, HZ);\n \tif (!time_left) {\n \t\tdev_err(target->dev, \"HTC credit config timeout\\n\");\n+\t\tkfree_skb(skb);\n \t\treturn -ETIMEDOUT;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tkfree_skb(skb);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leaks in drivers/net/wireless/ath/ath9k/htc_hst.c in the Linux kernel through 5.3.11 allow attackers to cause a denial of service (memory consumption) by triggering wait_for_completion_timeout() failures. This affects the htc_config_pipe_credits() function, the htc_setup_complete() function, and the htc_connect_service() function, aka CID-853acf7caf10.",
        "id": 2155
    },
    {
        "cve_id": "CVE-2019-19083",
        "code_before_change": "struct clock_source *dce112_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce112_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "code_after_change": "struct clock_source *dce112_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce112_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \t\treturn &clk_src->base;\n \t}\n \n+\tkfree(clk_src);\n \tBREAK_TO_DEBUGGER();\n \treturn NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\tkfree(clk_src);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leaks in *clock_source_create() functions under drivers/gpu/drm/amd/display/dc in the Linux kernel before 5.3.8 allow attackers to cause a denial of service (memory consumption). This affects the dce112_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce112/dce112_resource.c, the dce100_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce100/dce100_resource.c, the dcn10_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dcn10/dcn10_resource.c, the dcn20_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dcn20/dcn20_resource.c, the dce120_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce120/dce120_resource.c, the dce110_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce110/dce110_resource.c, and the dce80_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce80/dce80_resource.c, aka CID-055e547478a1.",
        "id": 2172
    },
    {
        "cve_id": "CVE-2023-32247",
        "code_before_change": "struct ksmbd_session *ksmbd_session_lookup_slowpath(unsigned long long id)\n{\n\tstruct ksmbd_session *sess;\n\n\tdown_read(&sessions_table_lock);\n\tsess = __session_lookup(id);\n\tup_read(&sessions_table_lock);\n\n\treturn sess;\n}",
        "code_after_change": "struct ksmbd_session *ksmbd_session_lookup_slowpath(unsigned long long id)\n{\n\tstruct ksmbd_session *sess;\n\n\tdown_read(&sessions_table_lock);\n\tsess = __session_lookup(id);\n\tif (sess)\n\t\tsess->last_active = jiffies;\n\tup_read(&sessions_table_lock);\n\n\treturn sess;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,8 @@\n \n \tdown_read(&sessions_table_lock);\n \tsess = __session_lookup(id);\n+\tif (sess)\n+\t\tsess->last_active = jiffies;\n \tup_read(&sessions_table_lock);\n \n \treturn sess;",
        "function_modified_lines": {
            "added": [
                "\tif (sess)",
                "\t\tsess->last_active = jiffies;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the handling of SMB2_SESSION_SETUP commands. The issue results from the lack of control of resource consumption. An attacker can leverage this vulnerability to create a denial-of-service condition on the system.",
        "id": 4013
    },
    {
        "cve_id": "CVE-2019-19055",
        "code_before_change": "static int nl80211_get_ftm_responder_stats(struct sk_buff *skb,\n\t\t\t\t\t   struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_ftm_responder_stats ftm_stats = {};\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tstruct nlattr *ftm_stats_attr;\n\tint err;\n\n\tif (wdev->iftype != NL80211_IFTYPE_AP || !wdev->beacon_interval)\n\t\treturn -EOPNOTSUPP;\n\n\terr = rdev_get_ftm_responder_stats(rdev, dev, &ftm_stats);\n\tif (err)\n\t\treturn err;\n\n\tif (!ftm_stats.filled)\n\t\treturn -ENODATA;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_GET_FTM_RESPONDER_STATS);\n\tif (!hdr)\n\t\treturn -ENOBUFS;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex))\n\t\tgoto nla_put_failure;\n\n\tftm_stats_attr = nla_nest_start_noflag(msg,\n\t\t\t\t\t       NL80211_ATTR_FTM_RESPONDER_STATS);\n\tif (!ftm_stats_attr)\n\t\tgoto nla_put_failure;\n\n#define SET_FTM(field, name, type)\t\t\t\t\t \\\n\tdo { if ((ftm_stats.filled & BIT(NL80211_FTM_STATS_ ## name)) && \\\n\t    nla_put_ ## type(msg, NL80211_FTM_STATS_ ## name,\t\t \\\n\t\t\t     ftm_stats.field))\t\t\t\t \\\n\t\tgoto nla_put_failure; } while (0)\n#define SET_FTM_U64(field, name)\t\t\t\t\t \\\n\tdo { if ((ftm_stats.filled & BIT(NL80211_FTM_STATS_ ## name)) && \\\n\t    nla_put_u64_64bit(msg, NL80211_FTM_STATS_ ## name,\t\t \\\n\t\t\t      ftm_stats.field, NL80211_FTM_STATS_PAD))\t \\\n\t\tgoto nla_put_failure; } while (0)\n\n\tSET_FTM(success_num, SUCCESS_NUM, u32);\n\tSET_FTM(partial_num, PARTIAL_NUM, u32);\n\tSET_FTM(failed_num, FAILED_NUM, u32);\n\tSET_FTM(asap_num, ASAP_NUM, u32);\n\tSET_FTM(non_asap_num, NON_ASAP_NUM, u32);\n\tSET_FTM_U64(total_duration_ms, TOTAL_DURATION_MSEC);\n\tSET_FTM(unknown_triggers_num, UNKNOWN_TRIGGERS_NUM, u32);\n\tSET_FTM(reschedule_requests_num, RESCHEDULE_REQUESTS_NUM, u32);\n\tSET_FTM(out_of_window_triggers_num, OUT_OF_WINDOW_TRIGGERS_NUM, u32);\n#undef SET_FTM\n\n\tnla_nest_end(msg, ftm_stats_attr);\n\n\tgenlmsg_end(msg, hdr);\n\treturn genlmsg_reply(msg, info);\n\nnla_put_failure:\n\tnlmsg_free(msg);\n\treturn -ENOBUFS;\n}",
        "code_after_change": "static int nl80211_get_ftm_responder_stats(struct sk_buff *skb,\n\t\t\t\t\t   struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_ftm_responder_stats ftm_stats = {};\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tstruct nlattr *ftm_stats_attr;\n\tint err;\n\n\tif (wdev->iftype != NL80211_IFTYPE_AP || !wdev->beacon_interval)\n\t\treturn -EOPNOTSUPP;\n\n\terr = rdev_get_ftm_responder_stats(rdev, dev, &ftm_stats);\n\tif (err)\n\t\treturn err;\n\n\tif (!ftm_stats.filled)\n\t\treturn -ENODATA;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_GET_FTM_RESPONDER_STATS);\n\tif (!hdr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex))\n\t\tgoto nla_put_failure;\n\n\tftm_stats_attr = nla_nest_start_noflag(msg,\n\t\t\t\t\t       NL80211_ATTR_FTM_RESPONDER_STATS);\n\tif (!ftm_stats_attr)\n\t\tgoto nla_put_failure;\n\n#define SET_FTM(field, name, type)\t\t\t\t\t \\\n\tdo { if ((ftm_stats.filled & BIT(NL80211_FTM_STATS_ ## name)) && \\\n\t    nla_put_ ## type(msg, NL80211_FTM_STATS_ ## name,\t\t \\\n\t\t\t     ftm_stats.field))\t\t\t\t \\\n\t\tgoto nla_put_failure; } while (0)\n#define SET_FTM_U64(field, name)\t\t\t\t\t \\\n\tdo { if ((ftm_stats.filled & BIT(NL80211_FTM_STATS_ ## name)) && \\\n\t    nla_put_u64_64bit(msg, NL80211_FTM_STATS_ ## name,\t\t \\\n\t\t\t      ftm_stats.field, NL80211_FTM_STATS_PAD))\t \\\n\t\tgoto nla_put_failure; } while (0)\n\n\tSET_FTM(success_num, SUCCESS_NUM, u32);\n\tSET_FTM(partial_num, PARTIAL_NUM, u32);\n\tSET_FTM(failed_num, FAILED_NUM, u32);\n\tSET_FTM(asap_num, ASAP_NUM, u32);\n\tSET_FTM(non_asap_num, NON_ASAP_NUM, u32);\n\tSET_FTM_U64(total_duration_ms, TOTAL_DURATION_MSEC);\n\tSET_FTM(unknown_triggers_num, UNKNOWN_TRIGGERS_NUM, u32);\n\tSET_FTM(reschedule_requests_num, RESCHEDULE_REQUESTS_NUM, u32);\n\tSET_FTM(out_of_window_triggers_num, OUT_OF_WINDOW_TRIGGERS_NUM, u32);\n#undef SET_FTM\n\n\tnla_nest_end(msg, ftm_stats_attr);\n\n\tgenlmsg_end(msg, hdr);\n\treturn genlmsg_reply(msg, info);\n\nnla_put_failure:\n\tnlmsg_free(msg);\n\treturn -ENOBUFS;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,7 +27,7 @@\n \thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n \t\t\t     NL80211_CMD_GET_FTM_RESPONDER_STATS);\n \tif (!hdr)\n-\t\treturn -ENOBUFS;\n+\t\tgoto nla_put_failure;\n \n \tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex))\n \t\tgoto nla_put_failure;",
        "function_modified_lines": {
            "added": [
                "\t\tgoto nla_put_failure;"
            ],
            "deleted": [
                "\t\treturn -ENOBUFS;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the nl80211_get_ftm_responder_stats() function in net/wireless/nl80211.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering nl80211hdr_put() failures, aka CID-1399c59fa929. NOTE: third parties dispute the relevance of this because it occurs on a code path where a successful allocation has already occurred",
        "id": 2136
    },
    {
        "cve_id": "CVE-2022-3630",
        "code_before_change": "static void fscache_cookie_state_machine(struct fscache_cookie *cookie)\n{\n\tenum fscache_cookie_state state;\n\tbool wake = false;\n\n\t_enter(\"c=%x\", cookie->debug_id);\n\nagain:\n\tspin_lock(&cookie->lock);\nagain_locked:\n\tstate = cookie->state;\n\tswitch (state) {\n\tcase FSCACHE_COOKIE_STATE_QUIESCENT:\n\t\t/* The QUIESCENT state is jumped to the LOOKING_UP state by\n\t\t * fscache_use_cookie().\n\t\t */\n\n\t\tif (atomic_read(&cookie->n_accesses) == 0 &&\n\t\t    test_bit(FSCACHE_COOKIE_DO_RELINQUISH, &cookie->flags)) {\n\t\t\t__fscache_set_cookie_state(cookie,\n\t\t\t\t\t\t   FSCACHE_COOKIE_STATE_RELINQUISHING);\n\t\t\twake = true;\n\t\t\tgoto again_locked;\n\t\t}\n\t\tbreak;\n\n\tcase FSCACHE_COOKIE_STATE_LOOKING_UP:\n\t\tspin_unlock(&cookie->lock);\n\t\tfscache_init_access_gate(cookie);\n\t\tfscache_perform_lookup(cookie);\n\t\tgoto again;\n\n\tcase FSCACHE_COOKIE_STATE_INVALIDATING:\n\t\tspin_unlock(&cookie->lock);\n\t\tfscache_perform_invalidation(cookie);\n\t\tgoto again;\n\n\tcase FSCACHE_COOKIE_STATE_ACTIVE:\n\t\tif (test_and_clear_bit(FSCACHE_COOKIE_DO_PREP_TO_WRITE, &cookie->flags)) {\n\t\t\tspin_unlock(&cookie->lock);\n\t\t\tfscache_prepare_to_write(cookie);\n\t\t\tspin_lock(&cookie->lock);\n\t\t}\n\t\tif (test_bit(FSCACHE_COOKIE_DO_LRU_DISCARD, &cookie->flags)) {\n\t\t\t__fscache_set_cookie_state(cookie,\n\t\t\t\t\t\t   FSCACHE_COOKIE_STATE_LRU_DISCARDING);\n\t\t\twake = true;\n\t\t\tgoto again_locked;\n\t\t}\n\t\tfallthrough;\n\n\tcase FSCACHE_COOKIE_STATE_FAILED:\n\t\tif (atomic_read(&cookie->n_accesses) != 0)\n\t\t\tbreak;\n\t\tif (test_bit(FSCACHE_COOKIE_DO_RELINQUISH, &cookie->flags)) {\n\t\t\t__fscache_set_cookie_state(cookie,\n\t\t\t\t\t\t   FSCACHE_COOKIE_STATE_RELINQUISHING);\n\t\t\twake = true;\n\t\t\tgoto again_locked;\n\t\t}\n\t\tif (test_bit(FSCACHE_COOKIE_DO_WITHDRAW, &cookie->flags)) {\n\t\t\t__fscache_set_cookie_state(cookie,\n\t\t\t\t\t\t   FSCACHE_COOKIE_STATE_WITHDRAWING);\n\t\t\twake = true;\n\t\t\tgoto again_locked;\n\t\t}\n\t\tbreak;\n\n\tcase FSCACHE_COOKIE_STATE_LRU_DISCARDING:\n\tcase FSCACHE_COOKIE_STATE_RELINQUISHING:\n\tcase FSCACHE_COOKIE_STATE_WITHDRAWING:\n\t\tif (cookie->cache_priv) {\n\t\t\tspin_unlock(&cookie->lock);\n\t\t\tcookie->volume->cache->ops->withdraw_cookie(cookie);\n\t\t\tspin_lock(&cookie->lock);\n\t\t}\n\n\t\tif (test_and_clear_bit(FSCACHE_COOKIE_DO_INVALIDATE, &cookie->flags))\n\t\t\tfscache_end_cookie_access(cookie, fscache_access_invalidate_cookie_end);\n\n\t\tswitch (state) {\n\t\tcase FSCACHE_COOKIE_STATE_RELINQUISHING:\n\t\t\tfscache_see_cookie(cookie, fscache_cookie_see_relinquish);\n\t\t\tfscache_unhash_cookie(cookie);\n\t\t\t__fscache_set_cookie_state(cookie,\n\t\t\t\t\t\t   FSCACHE_COOKIE_STATE_DROPPED);\n\t\t\twake = true;\n\t\t\tgoto out;\n\t\tcase FSCACHE_COOKIE_STATE_LRU_DISCARDING:\n\t\t\tfscache_see_cookie(cookie, fscache_cookie_see_lru_discard);\n\t\t\tbreak;\n\t\tcase FSCACHE_COOKIE_STATE_WITHDRAWING:\n\t\t\tfscache_see_cookie(cookie, fscache_cookie_see_withdraw);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\n\t\tclear_bit(FSCACHE_COOKIE_NEEDS_UPDATE, &cookie->flags);\n\t\tclear_bit(FSCACHE_COOKIE_DO_WITHDRAW, &cookie->flags);\n\t\tclear_bit(FSCACHE_COOKIE_DO_LRU_DISCARD, &cookie->flags);\n\t\tclear_bit(FSCACHE_COOKIE_DO_PREP_TO_WRITE, &cookie->flags);\n\t\tset_bit(FSCACHE_COOKIE_NO_DATA_TO_READ, &cookie->flags);\n\t\t__fscache_set_cookie_state(cookie, FSCACHE_COOKIE_STATE_QUIESCENT);\n\t\twake = true;\n\t\tgoto again_locked;\n\n\tcase FSCACHE_COOKIE_STATE_DROPPED:\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_ONCE(1, \"Cookie %x in unexpected state %u\\n\",\n\t\t\t  cookie->debug_id, state);\n\t\tbreak;\n\t}\n\nout:\n\tspin_unlock(&cookie->lock);\n\tif (wake)\n\t\twake_up_cookie_state(cookie);\n\t_leave(\"\");\n}",
        "code_after_change": "static void fscache_cookie_state_machine(struct fscache_cookie *cookie)\n{\n\tenum fscache_cookie_state state;\n\tbool wake = false;\n\n\t_enter(\"c=%x\", cookie->debug_id);\n\nagain:\n\tspin_lock(&cookie->lock);\nagain_locked:\n\tstate = cookie->state;\n\tswitch (state) {\n\tcase FSCACHE_COOKIE_STATE_QUIESCENT:\n\t\t/* The QUIESCENT state is jumped to the LOOKING_UP state by\n\t\t * fscache_use_cookie().\n\t\t */\n\n\t\tif (atomic_read(&cookie->n_accesses) == 0 &&\n\t\t    test_bit(FSCACHE_COOKIE_DO_RELINQUISH, &cookie->flags)) {\n\t\t\t__fscache_set_cookie_state(cookie,\n\t\t\t\t\t\t   FSCACHE_COOKIE_STATE_RELINQUISHING);\n\t\t\twake = true;\n\t\t\tgoto again_locked;\n\t\t}\n\t\tbreak;\n\n\tcase FSCACHE_COOKIE_STATE_LOOKING_UP:\n\t\tspin_unlock(&cookie->lock);\n\t\tfscache_init_access_gate(cookie);\n\t\tfscache_perform_lookup(cookie);\n\t\tgoto again;\n\n\tcase FSCACHE_COOKIE_STATE_INVALIDATING:\n\t\tspin_unlock(&cookie->lock);\n\t\tfscache_perform_invalidation(cookie);\n\t\tgoto again;\n\n\tcase FSCACHE_COOKIE_STATE_ACTIVE:\n\t\tif (test_and_clear_bit(FSCACHE_COOKIE_DO_PREP_TO_WRITE, &cookie->flags)) {\n\t\t\tspin_unlock(&cookie->lock);\n\t\t\tfscache_prepare_to_write(cookie);\n\t\t\tspin_lock(&cookie->lock);\n\t\t}\n\t\tif (test_bit(FSCACHE_COOKIE_DO_LRU_DISCARD, &cookie->flags)) {\n\t\t\t__fscache_set_cookie_state(cookie,\n\t\t\t\t\t\t   FSCACHE_COOKIE_STATE_LRU_DISCARDING);\n\t\t\twake = true;\n\t\t\tgoto again_locked;\n\t\t}\n\t\tfallthrough;\n\n\tcase FSCACHE_COOKIE_STATE_FAILED:\n\t\tif (test_and_clear_bit(FSCACHE_COOKIE_DO_INVALIDATE, &cookie->flags))\n\t\t\tfscache_end_cookie_access(cookie, fscache_access_invalidate_cookie_end);\n\n\t\tif (atomic_read(&cookie->n_accesses) != 0)\n\t\t\tbreak;\n\t\tif (test_bit(FSCACHE_COOKIE_DO_RELINQUISH, &cookie->flags)) {\n\t\t\t__fscache_set_cookie_state(cookie,\n\t\t\t\t\t\t   FSCACHE_COOKIE_STATE_RELINQUISHING);\n\t\t\twake = true;\n\t\t\tgoto again_locked;\n\t\t}\n\t\tif (test_bit(FSCACHE_COOKIE_DO_WITHDRAW, &cookie->flags)) {\n\t\t\t__fscache_set_cookie_state(cookie,\n\t\t\t\t\t\t   FSCACHE_COOKIE_STATE_WITHDRAWING);\n\t\t\twake = true;\n\t\t\tgoto again_locked;\n\t\t}\n\t\tbreak;\n\n\tcase FSCACHE_COOKIE_STATE_LRU_DISCARDING:\n\tcase FSCACHE_COOKIE_STATE_RELINQUISHING:\n\tcase FSCACHE_COOKIE_STATE_WITHDRAWING:\n\t\tif (cookie->cache_priv) {\n\t\t\tspin_unlock(&cookie->lock);\n\t\t\tcookie->volume->cache->ops->withdraw_cookie(cookie);\n\t\t\tspin_lock(&cookie->lock);\n\t\t}\n\n\t\tif (test_and_clear_bit(FSCACHE_COOKIE_DO_INVALIDATE, &cookie->flags))\n\t\t\tfscache_end_cookie_access(cookie, fscache_access_invalidate_cookie_end);\n\n\t\tswitch (state) {\n\t\tcase FSCACHE_COOKIE_STATE_RELINQUISHING:\n\t\t\tfscache_see_cookie(cookie, fscache_cookie_see_relinquish);\n\t\t\tfscache_unhash_cookie(cookie);\n\t\t\t__fscache_set_cookie_state(cookie,\n\t\t\t\t\t\t   FSCACHE_COOKIE_STATE_DROPPED);\n\t\t\twake = true;\n\t\t\tgoto out;\n\t\tcase FSCACHE_COOKIE_STATE_LRU_DISCARDING:\n\t\t\tfscache_see_cookie(cookie, fscache_cookie_see_lru_discard);\n\t\t\tbreak;\n\t\tcase FSCACHE_COOKIE_STATE_WITHDRAWING:\n\t\t\tfscache_see_cookie(cookie, fscache_cookie_see_withdraw);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\n\t\tclear_bit(FSCACHE_COOKIE_NEEDS_UPDATE, &cookie->flags);\n\t\tclear_bit(FSCACHE_COOKIE_DO_WITHDRAW, &cookie->flags);\n\t\tclear_bit(FSCACHE_COOKIE_DO_LRU_DISCARD, &cookie->flags);\n\t\tclear_bit(FSCACHE_COOKIE_DO_PREP_TO_WRITE, &cookie->flags);\n\t\tset_bit(FSCACHE_COOKIE_NO_DATA_TO_READ, &cookie->flags);\n\t\t__fscache_set_cookie_state(cookie, FSCACHE_COOKIE_STATE_QUIESCENT);\n\t\twake = true;\n\t\tgoto again_locked;\n\n\tcase FSCACHE_COOKIE_STATE_DROPPED:\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_ONCE(1, \"Cookie %x in unexpected state %u\\n\",\n\t\t\t  cookie->debug_id, state);\n\t\tbreak;\n\t}\n\nout:\n\tspin_unlock(&cookie->lock);\n\tif (wake)\n\t\twake_up_cookie_state(cookie);\n\t_leave(\"\");\n}",
        "patch": "--- code before\n+++ code after\n@@ -50,6 +50,9 @@\n \t\tfallthrough;\n \n \tcase FSCACHE_COOKIE_STATE_FAILED:\n+\t\tif (test_and_clear_bit(FSCACHE_COOKIE_DO_INVALIDATE, &cookie->flags))\n+\t\t\tfscache_end_cookie_access(cookie, fscache_access_invalidate_cookie_end);\n+\n \t\tif (atomic_read(&cookie->n_accesses) != 0)\n \t\t\tbreak;\n \t\tif (test_bit(FSCACHE_COOKIE_DO_RELINQUISH, &cookie->flags)) {",
        "function_modified_lines": {
            "added": [
                "\t\tif (test_and_clear_bit(FSCACHE_COOKIE_DO_INVALIDATE, &cookie->flags))",
                "\t\t\tfscache_end_cookie_access(cookie, fscache_access_invalidate_cookie_end);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been rated as problematic. This issue affects some unknown processing of the file fs/fscache/cookie.c of the component IPsec. The manipulation leads to memory leak. It is recommended to apply a patch to fix this issue. The associated identifier of this vulnerability is VDB-211931.",
        "id": 3666
    },
    {
        "cve_id": "CVE-2019-19062",
        "code_before_change": "static int crypto_report(struct sk_buff *in_skb, struct nlmsghdr *in_nlh,\n\t\t\t struct nlattr **attrs)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct crypto_user_alg *p = nlmsg_data(in_nlh);\n\tstruct crypto_alg *alg;\n\tstruct sk_buff *skb;\n\tstruct crypto_dump_info info;\n\tint err;\n\n\tif (!null_terminated(p->cru_name) || !null_terminated(p->cru_driver_name))\n\t\treturn -EINVAL;\n\n\talg = crypto_alg_match(p, 0);\n\tif (!alg)\n\t\treturn -ENOENT;\n\n\terr = -ENOMEM;\n\tskb = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!skb)\n\t\tgoto drop_alg;\n\n\tinfo.in_skb = in_skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = in_nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = 0;\n\n\terr = crypto_report_alg(alg, &info);\n\ndrop_alg:\n\tcrypto_mod_put(alg);\n\n\tif (err)\n\t\treturn err;\n\n\treturn nlmsg_unicast(net->crypto_nlsk, skb, NETLINK_CB(in_skb).portid);\n}",
        "code_after_change": "static int crypto_report(struct sk_buff *in_skb, struct nlmsghdr *in_nlh,\n\t\t\t struct nlattr **attrs)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct crypto_user_alg *p = nlmsg_data(in_nlh);\n\tstruct crypto_alg *alg;\n\tstruct sk_buff *skb;\n\tstruct crypto_dump_info info;\n\tint err;\n\n\tif (!null_terminated(p->cru_name) || !null_terminated(p->cru_driver_name))\n\t\treturn -EINVAL;\n\n\talg = crypto_alg_match(p, 0);\n\tif (!alg)\n\t\treturn -ENOENT;\n\n\terr = -ENOMEM;\n\tskb = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!skb)\n\t\tgoto drop_alg;\n\n\tinfo.in_skb = in_skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = in_nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = 0;\n\n\terr = crypto_report_alg(alg, &info);\n\ndrop_alg:\n\tcrypto_mod_put(alg);\n\n\tif (err) {\n\t\tkfree_skb(skb);\n\t\treturn err;\n\t}\n\n\treturn nlmsg_unicast(net->crypto_nlsk, skb, NETLINK_CB(in_skb).portid);\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,8 +30,10 @@\n drop_alg:\n \tcrypto_mod_put(alg);\n \n-\tif (err)\n+\tif (err) {\n+\t\tkfree_skb(skb);\n \t\treturn err;\n+\t}\n \n \treturn nlmsg_unicast(net->crypto_nlsk, skb, NETLINK_CB(in_skb).portid);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (err) {",
                "\t\tkfree_skb(skb);",
                "\t}"
            ],
            "deleted": [
                "\tif (err)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the crypto_report() function in crypto/crypto_user_base.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering crypto_report_alg() failures, aka CID-ffdde5932042.",
        "id": 2143
    },
    {
        "cve_id": "CVE-2019-19066",
        "code_before_change": "static struct fc_host_statistics *\nbfad_im_get_stats(struct Scsi_Host *shost)\n{\n\tstruct bfad_im_port_s *im_port =\n\t\t\t(struct bfad_im_port_s *) shost->hostdata[0];\n\tstruct bfad_s         *bfad = im_port->bfad;\n\tstruct bfad_hal_comp fcomp;\n\tunion bfa_port_stats_u *fcstats;\n\tstruct fc_host_statistics *hstats;\n\tbfa_status_t    rc;\n\tunsigned long   flags;\n\n\tfcstats = kzalloc(sizeof(union bfa_port_stats_u), GFP_KERNEL);\n\tif (fcstats == NULL)\n\t\treturn NULL;\n\n\thstats = &bfad->link_stats;\n\tinit_completion(&fcomp.comp);\n\tspin_lock_irqsave(&bfad->bfad_lock, flags);\n\tmemset(hstats, 0, sizeof(struct fc_host_statistics));\n\trc = bfa_port_get_stats(BFA_FCPORT(&bfad->bfa),\n\t\t\t\tfcstats, bfad_hcb_comp, &fcomp);\n\tspin_unlock_irqrestore(&bfad->bfad_lock, flags);\n\tif (rc != BFA_STATUS_OK)\n\t\treturn NULL;\n\n\twait_for_completion(&fcomp.comp);\n\n\t/* Fill the fc_host_statistics structure */\n\thstats->seconds_since_last_reset = fcstats->fc.secs_reset;\n\thstats->tx_frames = fcstats->fc.tx_frames;\n\thstats->tx_words  = fcstats->fc.tx_words;\n\thstats->rx_frames = fcstats->fc.rx_frames;\n\thstats->rx_words  = fcstats->fc.rx_words;\n\thstats->lip_count = fcstats->fc.lip_count;\n\thstats->nos_count = fcstats->fc.nos_count;\n\thstats->error_frames = fcstats->fc.error_frames;\n\thstats->dumped_frames = fcstats->fc.dropped_frames;\n\thstats->link_failure_count = fcstats->fc.link_failures;\n\thstats->loss_of_sync_count = fcstats->fc.loss_of_syncs;\n\thstats->loss_of_signal_count = fcstats->fc.loss_of_signals;\n\thstats->prim_seq_protocol_err_count = fcstats->fc.primseq_errs;\n\thstats->invalid_crc_count = fcstats->fc.invalid_crcs;\n\n\tkfree(fcstats);\n\treturn hstats;\n}",
        "code_after_change": "static struct fc_host_statistics *\nbfad_im_get_stats(struct Scsi_Host *shost)\n{\n\tstruct bfad_im_port_s *im_port =\n\t\t\t(struct bfad_im_port_s *) shost->hostdata[0];\n\tstruct bfad_s         *bfad = im_port->bfad;\n\tstruct bfad_hal_comp fcomp;\n\tunion bfa_port_stats_u *fcstats;\n\tstruct fc_host_statistics *hstats;\n\tbfa_status_t    rc;\n\tunsigned long   flags;\n\n\tfcstats = kzalloc(sizeof(union bfa_port_stats_u), GFP_KERNEL);\n\tif (fcstats == NULL)\n\t\treturn NULL;\n\n\thstats = &bfad->link_stats;\n\tinit_completion(&fcomp.comp);\n\tspin_lock_irqsave(&bfad->bfad_lock, flags);\n\tmemset(hstats, 0, sizeof(struct fc_host_statistics));\n\trc = bfa_port_get_stats(BFA_FCPORT(&bfad->bfa),\n\t\t\t\tfcstats, bfad_hcb_comp, &fcomp);\n\tspin_unlock_irqrestore(&bfad->bfad_lock, flags);\n\tif (rc != BFA_STATUS_OK) {\n\t\tkfree(fcstats);\n\t\treturn NULL;\n\t}\n\n\twait_for_completion(&fcomp.comp);\n\n\t/* Fill the fc_host_statistics structure */\n\thstats->seconds_since_last_reset = fcstats->fc.secs_reset;\n\thstats->tx_frames = fcstats->fc.tx_frames;\n\thstats->tx_words  = fcstats->fc.tx_words;\n\thstats->rx_frames = fcstats->fc.rx_frames;\n\thstats->rx_words  = fcstats->fc.rx_words;\n\thstats->lip_count = fcstats->fc.lip_count;\n\thstats->nos_count = fcstats->fc.nos_count;\n\thstats->error_frames = fcstats->fc.error_frames;\n\thstats->dumped_frames = fcstats->fc.dropped_frames;\n\thstats->link_failure_count = fcstats->fc.link_failures;\n\thstats->loss_of_sync_count = fcstats->fc.loss_of_syncs;\n\thstats->loss_of_signal_count = fcstats->fc.loss_of_signals;\n\thstats->prim_seq_protocol_err_count = fcstats->fc.primseq_errs;\n\thstats->invalid_crc_count = fcstats->fc.invalid_crcs;\n\n\tkfree(fcstats);\n\treturn hstats;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,8 +21,10 @@\n \trc = bfa_port_get_stats(BFA_FCPORT(&bfad->bfa),\n \t\t\t\tfcstats, bfad_hcb_comp, &fcomp);\n \tspin_unlock_irqrestore(&bfad->bfad_lock, flags);\n-\tif (rc != BFA_STATUS_OK)\n+\tif (rc != BFA_STATUS_OK) {\n+\t\tkfree(fcstats);\n \t\treturn NULL;\n+\t}\n \n \twait_for_completion(&fcomp.comp);\n ",
        "function_modified_lines": {
            "added": [
                "\tif (rc != BFA_STATUS_OK) {",
                "\t\tkfree(fcstats);",
                "\t}"
            ],
            "deleted": [
                "\tif (rc != BFA_STATUS_OK)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the bfad_im_get_stats() function in drivers/scsi/bfa/bfad_attr.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering bfa_port_get_stats() failures, aka CID-0e62395da2bd.",
        "id": 2147
    },
    {
        "cve_id": "CVE-2019-19068",
        "code_before_change": "static int rtl8xxxu_submit_int_urb(struct ieee80211_hw *hw)\n{\n\tstruct rtl8xxxu_priv *priv = hw->priv;\n\tstruct urb *urb;\n\tu32 val32;\n\tint ret;\n\n\turb = usb_alloc_urb(0, GFP_KERNEL);\n\tif (!urb)\n\t\treturn -ENOMEM;\n\n\tusb_fill_int_urb(urb, priv->udev, priv->pipe_interrupt,\n\t\t\t priv->int_buf, USB_INTR_CONTENT_LENGTH,\n\t\t\t rtl8xxxu_int_complete, priv, 1);\n\tusb_anchor_urb(urb, &priv->int_anchor);\n\tret = usb_submit_urb(urb, GFP_KERNEL);\n\tif (ret) {\n\t\tusb_unanchor_urb(urb);\n\t\tgoto error;\n\t}\n\n\tval32 = rtl8xxxu_read32(priv, REG_USB_HIMR);\n\tval32 |= USB_HIMR_CPWM;\n\trtl8xxxu_write32(priv, REG_USB_HIMR, val32);\n\nerror:\n\treturn ret;\n}",
        "code_after_change": "static int rtl8xxxu_submit_int_urb(struct ieee80211_hw *hw)\n{\n\tstruct rtl8xxxu_priv *priv = hw->priv;\n\tstruct urb *urb;\n\tu32 val32;\n\tint ret;\n\n\turb = usb_alloc_urb(0, GFP_KERNEL);\n\tif (!urb)\n\t\treturn -ENOMEM;\n\n\tusb_fill_int_urb(urb, priv->udev, priv->pipe_interrupt,\n\t\t\t priv->int_buf, USB_INTR_CONTENT_LENGTH,\n\t\t\t rtl8xxxu_int_complete, priv, 1);\n\tusb_anchor_urb(urb, &priv->int_anchor);\n\tret = usb_submit_urb(urb, GFP_KERNEL);\n\tif (ret) {\n\t\tusb_unanchor_urb(urb);\n\t\tusb_free_urb(urb);\n\t\tgoto error;\n\t}\n\n\tval32 = rtl8xxxu_read32(priv, REG_USB_HIMR);\n\tval32 |= USB_HIMR_CPWM;\n\trtl8xxxu_write32(priv, REG_USB_HIMR, val32);\n\nerror:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,7 @@\n \tret = usb_submit_urb(urb, GFP_KERNEL);\n \tif (ret) {\n \t\tusb_unanchor_urb(urb);\n+\t\tusb_free_urb(urb);\n \t\tgoto error;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tusb_free_urb(urb);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the rtl8xxxu_submit_int_urb() function in drivers/net/wireless/realtek/rtl8xxxu/rtl8xxxu_core.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering usb_submit_urb() failures, aka CID-a2cdd07488e6.",
        "id": 2149
    },
    {
        "cve_id": "CVE-2021-3736",
        "code_before_change": "static void mbochs_remove(struct mdev_device *mdev)\n{\n\tstruct mdev_state *mdev_state = dev_get_drvdata(&mdev->dev);\n\n\tmbochs_used_mbytes -= mdev_state->type->mbytes;\n\tvfio_unregister_group_dev(&mdev_state->vdev);\n\tkfree(mdev_state->pages);\n\tkfree(mdev_state->vconfig);\n\tkfree(mdev_state);\n}",
        "code_after_change": "static void mbochs_remove(struct mdev_device *mdev)\n{\n\tstruct mdev_state *mdev_state = dev_get_drvdata(&mdev->dev);\n\n\tvfio_unregister_group_dev(&mdev_state->vdev);\n\tatomic_add(mdev_state->type->mbytes, &mbochs_avail_mbytes);\n\tkfree(mdev_state->pages);\n\tkfree(mdev_state->vconfig);\n\tkfree(mdev_state);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,8 +2,8 @@\n {\n \tstruct mdev_state *mdev_state = dev_get_drvdata(&mdev->dev);\n \n-\tmbochs_used_mbytes -= mdev_state->type->mbytes;\n \tvfio_unregister_group_dev(&mdev_state->vdev);\n+\tatomic_add(mdev_state->type->mbytes, &mbochs_avail_mbytes);\n \tkfree(mdev_state->pages);\n \tkfree(mdev_state->vconfig);\n \tkfree(mdev_state);",
        "function_modified_lines": {
            "added": [
                "\tatomic_add(mdev_state->type->mbytes, &mbochs_avail_mbytes);"
            ],
            "deleted": [
                "\tmbochs_used_mbytes -= mdev_state->type->mbytes;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw was found in the Linux kernel. A memory leak problem was found in mbochs_ioctl in samples/vfio-mdev/mbochs.c in Virtual Function I/O (VFIO) Mediated devices. This flaw could allow a local attacker to leak internal kernel information.",
        "id": 3048
    },
    {
        "cve_id": "CVE-2022-1012",
        "code_before_change": "int inet_hash_connect(struct inet_timewait_death_row *death_row,\n\t\t      struct sock *sk)\n{\n\tu32 port_offset = 0;\n\n\tif (!inet_sk(sk)->inet_num)\n\t\tport_offset = inet_sk_port_offset(sk);\n\treturn __inet_hash_connect(death_row, sk, port_offset,\n\t\t\t\t   __inet_check_established);\n}",
        "code_after_change": "int inet_hash_connect(struct inet_timewait_death_row *death_row,\n\t\t      struct sock *sk)\n{\n\tu64 port_offset = 0;\n\n\tif (!inet_sk(sk)->inet_num)\n\t\tport_offset = inet_sk_port_offset(sk);\n\treturn __inet_hash_connect(death_row, sk, port_offset,\n\t\t\t\t   __inet_check_established);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n int inet_hash_connect(struct inet_timewait_death_row *death_row,\n \t\t      struct sock *sk)\n {\n-\tu32 port_offset = 0;\n+\tu64 port_offset = 0;\n \n \tif (!inet_sk(sk)->inet_num)\n \t\tport_offset = inet_sk_port_offset(sk);",
        "function_modified_lines": {
            "added": [
                "\tu64 port_offset = 0;"
            ],
            "deleted": [
                "\tu32 port_offset = 0;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak problem was found in the TCP source port generation algorithm in net/ipv4/tcp.c due to the small table perturb size. This flaw may allow an attacker to information leak and may cause a denial of service problem.",
        "id": 3235
    },
    {
        "cve_id": "CVE-2019-19057",
        "code_before_change": "static int mwifiex_pcie_init_evt_ring(struct mwifiex_adapter *adapter)\n{\n\tstruct pcie_service_card *card = adapter->card;\n\tstruct mwifiex_evt_buf_desc *desc;\n\tstruct sk_buff *skb;\n\tdma_addr_t buf_pa;\n\tint i;\n\n\tfor (i = 0; i < MWIFIEX_MAX_EVT_BD; i++) {\n\t\t/* Allocate skb here so that firmware can DMA data from it */\n\t\tskb = dev_alloc_skb(MAX_EVENT_SIZE);\n\t\tif (!skb) {\n\t\t\tmwifiex_dbg(adapter, ERROR,\n\t\t\t\t    \"Unable to allocate skb for EVENT buf.\\n\");\n\t\t\tkfree(card->evtbd_ring_vbase);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tskb_put(skb, MAX_EVENT_SIZE);\n\n\t\tif (mwifiex_map_pci_memory(adapter, skb, MAX_EVENT_SIZE,\n\t\t\t\t\t   PCI_DMA_FROMDEVICE))\n\t\t\treturn -1;\n\n\t\tbuf_pa = MWIFIEX_SKB_DMA_ADDR(skb);\n\n\t\tmwifiex_dbg(adapter, EVENT,\n\t\t\t    \"info: EVT ring: skb=%p len=%d data=%p buf_pa=%#x:%x\\n\",\n\t\t\t    skb, skb->len, skb->data, (u32)buf_pa,\n\t\t\t    (u32)((u64)buf_pa >> 32));\n\n\t\tcard->evt_buf_list[i] = skb;\n\t\tcard->evtbd_ring[i] = (void *)(card->evtbd_ring_vbase +\n\t\t\t\t      (sizeof(*desc) * i));\n\t\tdesc = card->evtbd_ring[i];\n\t\tdesc->paddr = buf_pa;\n\t\tdesc->len = (u16)skb->len;\n\t\tdesc->flags = 0;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int mwifiex_pcie_init_evt_ring(struct mwifiex_adapter *adapter)\n{\n\tstruct pcie_service_card *card = adapter->card;\n\tstruct mwifiex_evt_buf_desc *desc;\n\tstruct sk_buff *skb;\n\tdma_addr_t buf_pa;\n\tint i;\n\n\tfor (i = 0; i < MWIFIEX_MAX_EVT_BD; i++) {\n\t\t/* Allocate skb here so that firmware can DMA data from it */\n\t\tskb = dev_alloc_skb(MAX_EVENT_SIZE);\n\t\tif (!skb) {\n\t\t\tmwifiex_dbg(adapter, ERROR,\n\t\t\t\t    \"Unable to allocate skb for EVENT buf.\\n\");\n\t\t\tkfree(card->evtbd_ring_vbase);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tskb_put(skb, MAX_EVENT_SIZE);\n\n\t\tif (mwifiex_map_pci_memory(adapter, skb, MAX_EVENT_SIZE,\n\t\t\t\t\t   PCI_DMA_FROMDEVICE)) {\n\t\t\tkfree_skb(skb);\n\t\t\tkfree(card->evtbd_ring_vbase);\n\t\t\treturn -1;\n\t\t}\n\n\t\tbuf_pa = MWIFIEX_SKB_DMA_ADDR(skb);\n\n\t\tmwifiex_dbg(adapter, EVENT,\n\t\t\t    \"info: EVT ring: skb=%p len=%d data=%p buf_pa=%#x:%x\\n\",\n\t\t\t    skb, skb->len, skb->data, (u32)buf_pa,\n\t\t\t    (u32)((u64)buf_pa >> 32));\n\n\t\tcard->evt_buf_list[i] = skb;\n\t\tcard->evtbd_ring[i] = (void *)(card->evtbd_ring_vbase +\n\t\t\t\t      (sizeof(*desc) * i));\n\t\tdesc = card->evtbd_ring[i];\n\t\tdesc->paddr = buf_pa;\n\t\tdesc->len = (u16)skb->len;\n\t\tdesc->flags = 0;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,8 +18,11 @@\n \t\tskb_put(skb, MAX_EVENT_SIZE);\n \n \t\tif (mwifiex_map_pci_memory(adapter, skb, MAX_EVENT_SIZE,\n-\t\t\t\t\t   PCI_DMA_FROMDEVICE))\n+\t\t\t\t\t   PCI_DMA_FROMDEVICE)) {\n+\t\t\tkfree_skb(skb);\n+\t\t\tkfree(card->evtbd_ring_vbase);\n \t\t\treturn -1;\n+\t\t}\n \n \t\tbuf_pa = MWIFIEX_SKB_DMA_ADDR(skb);\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\t   PCI_DMA_FROMDEVICE)) {",
                "\t\t\tkfree_skb(skb);",
                "\t\t\tkfree(card->evtbd_ring_vbase);",
                "\t\t}"
            ],
            "deleted": [
                "\t\t\t\t\t   PCI_DMA_FROMDEVICE))"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Two memory leaks in the mwifiex_pcie_init_evt_ring() function in drivers/net/wireless/marvell/mwifiex/pcie.c in the Linux kernel through 5.3.11 allow attackers to cause a denial of service (memory consumption) by triggering mwifiex_map_pci_memory() failures, aka CID-d10dcb615c8e.",
        "id": 2138
    },
    {
        "cve_id": "CVE-2019-19050",
        "code_before_change": "int crypto_reportstat(struct sk_buff *in_skb, struct nlmsghdr *in_nlh,\n\t\t      struct nlattr **attrs)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct crypto_user_alg *p = nlmsg_data(in_nlh);\n\tstruct crypto_alg *alg;\n\tstruct sk_buff *skb;\n\tstruct crypto_dump_info info;\n\tint err;\n\n\tif (!null_terminated(p->cru_name) || !null_terminated(p->cru_driver_name))\n\t\treturn -EINVAL;\n\n\talg = crypto_alg_match(p, 0);\n\tif (!alg)\n\t\treturn -ENOENT;\n\n\terr = -ENOMEM;\n\tskb = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_ATOMIC);\n\tif (!skb)\n\t\tgoto drop_alg;\n\n\tinfo.in_skb = in_skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = in_nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = 0;\n\n\terr = crypto_reportstat_alg(alg, &info);\n\ndrop_alg:\n\tcrypto_mod_put(alg);\n\n\tif (err)\n\t\treturn err;\n\n\treturn nlmsg_unicast(net->crypto_nlsk, skb, NETLINK_CB(in_skb).portid);\n}",
        "code_after_change": "int crypto_reportstat(struct sk_buff *in_skb, struct nlmsghdr *in_nlh,\n\t\t      struct nlattr **attrs)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct crypto_user_alg *p = nlmsg_data(in_nlh);\n\tstruct crypto_alg *alg;\n\tstruct sk_buff *skb;\n\tstruct crypto_dump_info info;\n\tint err;\n\n\tif (!null_terminated(p->cru_name) || !null_terminated(p->cru_driver_name))\n\t\treturn -EINVAL;\n\n\talg = crypto_alg_match(p, 0);\n\tif (!alg)\n\t\treturn -ENOENT;\n\n\terr = -ENOMEM;\n\tskb = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_ATOMIC);\n\tif (!skb)\n\t\tgoto drop_alg;\n\n\tinfo.in_skb = in_skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = in_nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = 0;\n\n\terr = crypto_reportstat_alg(alg, &info);\n\ndrop_alg:\n\tcrypto_mod_put(alg);\n\n\tif (err) {\n\t\tkfree_skb(skb);\n\t\treturn err;\n\t}\n\n\treturn nlmsg_unicast(net->crypto_nlsk, skb, NETLINK_CB(in_skb).portid);\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,8 +30,10 @@\n drop_alg:\n \tcrypto_mod_put(alg);\n \n-\tif (err)\n+\tif (err) {\n+\t\tkfree_skb(skb);\n \t\treturn err;\n+\t}\n \n \treturn nlmsg_unicast(net->crypto_nlsk, skb, NETLINK_CB(in_skb).portid);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (err) {",
                "\t\tkfree_skb(skb);",
                "\t}"
            ],
            "deleted": [
                "\tif (err)"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the crypto_reportstat() function in crypto/crypto_user_stat.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering crypto_reportstat_alg() failures, aka CID-c03b04dcdba1.",
        "id": 2131
    },
    {
        "cve_id": "CVE-2020-12768",
        "code_before_change": "static int svm_cpu_init(int cpu)\n{\n\tstruct svm_cpu_data *sd;\n\tint r;\n\n\tsd = kzalloc(sizeof(struct svm_cpu_data), GFP_KERNEL);\n\tif (!sd)\n\t\treturn -ENOMEM;\n\tsd->cpu = cpu;\n\tr = -ENOMEM;\n\tsd->save_area = alloc_page(GFP_KERNEL);\n\tif (!sd->save_area)\n\t\tgoto err_1;\n\n\tif (svm_sev_enabled()) {\n\t\tr = -ENOMEM;\n\t\tsd->sev_vmcbs = kmalloc_array(max_sev_asid + 1,\n\t\t\t\t\t      sizeof(void *),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!sd->sev_vmcbs)\n\t\t\tgoto err_1;\n\t}\n\n\tper_cpu(svm_data, cpu) = sd;\n\n\treturn 0;\n\nerr_1:\n\tkfree(sd);\n\treturn r;\n\n}",
        "code_after_change": "static int svm_cpu_init(int cpu)\n{\n\tstruct svm_cpu_data *sd;\n\n\tsd = kzalloc(sizeof(struct svm_cpu_data), GFP_KERNEL);\n\tif (!sd)\n\t\treturn -ENOMEM;\n\tsd->cpu = cpu;\n\tsd->save_area = alloc_page(GFP_KERNEL);\n\tif (!sd->save_area)\n\t\tgoto free_cpu_data;\n\n\tif (svm_sev_enabled()) {\n\t\tsd->sev_vmcbs = kmalloc_array(max_sev_asid + 1,\n\t\t\t\t\t      sizeof(void *),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!sd->sev_vmcbs)\n\t\t\tgoto free_save_area;\n\t}\n\n\tper_cpu(svm_data, cpu) = sd;\n\n\treturn 0;\n\nfree_save_area:\n\t__free_page(sd->save_area);\nfree_cpu_data:\n\tkfree(sd);\n\treturn -ENOMEM;\n\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,32 +1,31 @@\n static int svm_cpu_init(int cpu)\n {\n \tstruct svm_cpu_data *sd;\n-\tint r;\n \n \tsd = kzalloc(sizeof(struct svm_cpu_data), GFP_KERNEL);\n \tif (!sd)\n \t\treturn -ENOMEM;\n \tsd->cpu = cpu;\n-\tr = -ENOMEM;\n \tsd->save_area = alloc_page(GFP_KERNEL);\n \tif (!sd->save_area)\n-\t\tgoto err_1;\n+\t\tgoto free_cpu_data;\n \n \tif (svm_sev_enabled()) {\n-\t\tr = -ENOMEM;\n \t\tsd->sev_vmcbs = kmalloc_array(max_sev_asid + 1,\n \t\t\t\t\t      sizeof(void *),\n \t\t\t\t\t      GFP_KERNEL);\n \t\tif (!sd->sev_vmcbs)\n-\t\t\tgoto err_1;\n+\t\t\tgoto free_save_area;\n \t}\n \n \tper_cpu(svm_data, cpu) = sd;\n \n \treturn 0;\n \n-err_1:\n+free_save_area:\n+\t__free_page(sd->save_area);\n+free_cpu_data:\n \tkfree(sd);\n-\treturn r;\n+\treturn -ENOMEM;\n \n }",
        "function_modified_lines": {
            "added": [
                "\t\tgoto free_cpu_data;",
                "\t\t\tgoto free_save_area;",
                "free_save_area:",
                "\t__free_page(sd->save_area);",
                "free_cpu_data:",
                "\treturn -ENOMEM;"
            ],
            "deleted": [
                "\tint r;",
                "\tr = -ENOMEM;",
                "\t\tgoto err_1;",
                "\t\tr = -ENOMEM;",
                "\t\t\tgoto err_1;",
                "err_1:",
                "\treturn r;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.6. svm_cpu_uninit in arch/x86/kvm/svm.c has a memory leak, aka CID-d80b64ff297e. NOTE: third parties dispute this issue because it's a one-time leak at the boot, the size is negligible, and it can't be triggered at will",
        "id": 2481
    },
    {
        "cve_id": "CVE-2021-30002",
        "code_before_change": "long\nvideo_usercopy(struct file *file, unsigned int orig_cmd, unsigned long arg,\n\t       v4l2_kioctl func)\n{\n\tchar\tsbuf[128];\n\tvoid    *mbuf = NULL;\n\tvoid\t*parg = (void *)arg;\n\tlong\terr  = -EINVAL;\n\tbool\thas_array_args;\n\tbool\talways_copy = false;\n\tsize_t  array_size = 0;\n\tvoid __user *user_ptr = NULL;\n\tvoid\t**kernel_ptr = NULL;\n\tunsigned int cmd = video_translate_cmd(orig_cmd);\n\tconst size_t ioc_size = _IOC_SIZE(cmd);\n\n\t/*  Copy arguments into temp kernel buffer  */\n\tif (_IOC_DIR(cmd) != _IOC_NONE) {\n\t\tif (ioc_size <= sizeof(sbuf)) {\n\t\t\tparg = sbuf;\n\t\t} else {\n\t\t\t/* too big to allocate from stack */\n\t\t\tmbuf = kvmalloc(ioc_size, GFP_KERNEL);\n\t\t\tif (NULL == mbuf)\n\t\t\t\treturn -ENOMEM;\n\t\t\tparg = mbuf;\n\t\t}\n\n\t\terr = video_get_user((void __user *)arg, parg, cmd,\n\t\t\t\t     orig_cmd, &always_copy);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = check_array_args(cmd, parg, &array_size, &user_ptr, &kernel_ptr);\n\tif (err < 0)\n\t\tgoto out;\n\thas_array_args = err;\n\n\tif (has_array_args) {\n\t\t/*\n\t\t * When adding new types of array args, make sure that the\n\t\t * parent argument to ioctl (which contains the pointer to the\n\t\t * array) fits into sbuf (so that mbuf will still remain\n\t\t * unused up to here).\n\t\t */\n\t\tmbuf = kvmalloc(array_size, GFP_KERNEL);\n\t\terr = -ENOMEM;\n\t\tif (NULL == mbuf)\n\t\t\tgoto out_array_args;\n\t\terr = -EFAULT;\n\t\tif (in_compat_syscall())\n\t\t\terr = v4l2_compat_get_array_args(file, mbuf, user_ptr,\n\t\t\t\t\t\t\t array_size, orig_cmd,\n\t\t\t\t\t\t\t parg);\n\t\telse\n\t\t\terr = copy_from_user(mbuf, user_ptr, array_size) ?\n\t\t\t\t\t\t\t\t-EFAULT : 0;\n\t\tif (err)\n\t\t\tgoto out_array_args;\n\t\t*kernel_ptr = mbuf;\n\t}\n\n\t/* Handles IOCTL */\n\terr = func(file, cmd, parg);\n\tif (err == -ENOTTY || err == -ENOIOCTLCMD) {\n\t\terr = -ENOTTY;\n\t\tgoto out;\n\t}\n\n\tif (err == 0) {\n\t\tif (cmd == VIDIOC_DQBUF)\n\t\t\ttrace_v4l2_dqbuf(video_devdata(file)->minor, parg);\n\t\telse if (cmd == VIDIOC_QBUF)\n\t\t\ttrace_v4l2_qbuf(video_devdata(file)->minor, parg);\n\t}\n\n\tif (has_array_args) {\n\t\t*kernel_ptr = (void __force *)user_ptr;\n\t\tif (in_compat_syscall()) {\n\t\t\tint put_err;\n\n\t\t\tput_err = v4l2_compat_put_array_args(file, user_ptr, mbuf,\n\t\t\t\t\t\t\t     array_size, orig_cmd,\n\t\t\t\t\t\t\t     parg);\n\t\t\tif (put_err)\n\t\t\t\terr = put_err;\n\t\t} else if (copy_to_user(user_ptr, mbuf, array_size)) {\n\t\t\terr = -EFAULT;\n\t\t}\n\t\tgoto out_array_args;\n\t}\n\t/*\n\t * Some ioctls can return an error, but still have valid\n\t * results that must be returned.\n\t */\n\tif (err < 0 && !always_copy)\n\t\tgoto out;\n\nout_array_args:\n\tif (video_put_user((void __user *)arg, parg, cmd, orig_cmd))\n\t\terr = -EFAULT;\nout:\n\tkvfree(mbuf);\n\treturn err;\n}",
        "code_after_change": "long\nvideo_usercopy(struct file *file, unsigned int orig_cmd, unsigned long arg,\n\t       v4l2_kioctl func)\n{\n\tchar\tsbuf[128];\n\tvoid    *mbuf = NULL, *array_buf = NULL;\n\tvoid\t*parg = (void *)arg;\n\tlong\terr  = -EINVAL;\n\tbool\thas_array_args;\n\tbool\talways_copy = false;\n\tsize_t  array_size = 0;\n\tvoid __user *user_ptr = NULL;\n\tvoid\t**kernel_ptr = NULL;\n\tunsigned int cmd = video_translate_cmd(orig_cmd);\n\tconst size_t ioc_size = _IOC_SIZE(cmd);\n\n\t/*  Copy arguments into temp kernel buffer  */\n\tif (_IOC_DIR(cmd) != _IOC_NONE) {\n\t\tif (ioc_size <= sizeof(sbuf)) {\n\t\t\tparg = sbuf;\n\t\t} else {\n\t\t\t/* too big to allocate from stack */\n\t\t\tmbuf = kvmalloc(ioc_size, GFP_KERNEL);\n\t\t\tif (NULL == mbuf)\n\t\t\t\treturn -ENOMEM;\n\t\t\tparg = mbuf;\n\t\t}\n\n\t\terr = video_get_user((void __user *)arg, parg, cmd,\n\t\t\t\t     orig_cmd, &always_copy);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = check_array_args(cmd, parg, &array_size, &user_ptr, &kernel_ptr);\n\tif (err < 0)\n\t\tgoto out;\n\thas_array_args = err;\n\n\tif (has_array_args) {\n\t\tarray_buf = kvmalloc(array_size, GFP_KERNEL);\n\t\terr = -ENOMEM;\n\t\tif (array_buf == NULL)\n\t\t\tgoto out_array_args;\n\t\terr = -EFAULT;\n\t\tif (in_compat_syscall())\n\t\t\terr = v4l2_compat_get_array_args(file, array_buf,\n\t\t\t\t\t\t\t user_ptr, array_size,\n\t\t\t\t\t\t\t orig_cmd, parg);\n\t\telse\n\t\t\terr = copy_from_user(array_buf, user_ptr, array_size) ?\n\t\t\t\t\t\t\t\t-EFAULT : 0;\n\t\tif (err)\n\t\t\tgoto out_array_args;\n\t\t*kernel_ptr = array_buf;\n\t}\n\n\t/* Handles IOCTL */\n\terr = func(file, cmd, parg);\n\tif (err == -ENOTTY || err == -ENOIOCTLCMD) {\n\t\terr = -ENOTTY;\n\t\tgoto out;\n\t}\n\n\tif (err == 0) {\n\t\tif (cmd == VIDIOC_DQBUF)\n\t\t\ttrace_v4l2_dqbuf(video_devdata(file)->minor, parg);\n\t\telse if (cmd == VIDIOC_QBUF)\n\t\t\ttrace_v4l2_qbuf(video_devdata(file)->minor, parg);\n\t}\n\n\tif (has_array_args) {\n\t\t*kernel_ptr = (void __force *)user_ptr;\n\t\tif (in_compat_syscall()) {\n\t\t\tint put_err;\n\n\t\t\tput_err = v4l2_compat_put_array_args(file, user_ptr,\n\t\t\t\t\t\t\t     array_buf,\n\t\t\t\t\t\t\t     array_size,\n\t\t\t\t\t\t\t     orig_cmd, parg);\n\t\t\tif (put_err)\n\t\t\t\terr = put_err;\n\t\t} else if (copy_to_user(user_ptr, array_buf, array_size)) {\n\t\t\terr = -EFAULT;\n\t\t}\n\t\tgoto out_array_args;\n\t}\n\t/*\n\t * Some ioctls can return an error, but still have valid\n\t * results that must be returned.\n\t */\n\tif (err < 0 && !always_copy)\n\t\tgoto out;\n\nout_array_args:\n\tif (video_put_user((void __user *)arg, parg, cmd, orig_cmd))\n\t\terr = -EFAULT;\nout:\n\tkvfree(array_buf);\n\tkvfree(mbuf);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t       v4l2_kioctl func)\n {\n \tchar\tsbuf[128];\n-\tvoid    *mbuf = NULL;\n+\tvoid    *mbuf = NULL, *array_buf = NULL;\n \tvoid\t*parg = (void *)arg;\n \tlong\terr  = -EINVAL;\n \tbool\thas_array_args;\n@@ -38,27 +38,21 @@\n \thas_array_args = err;\n \n \tif (has_array_args) {\n-\t\t/*\n-\t\t * When adding new types of array args, make sure that the\n-\t\t * parent argument to ioctl (which contains the pointer to the\n-\t\t * array) fits into sbuf (so that mbuf will still remain\n-\t\t * unused up to here).\n-\t\t */\n-\t\tmbuf = kvmalloc(array_size, GFP_KERNEL);\n+\t\tarray_buf = kvmalloc(array_size, GFP_KERNEL);\n \t\terr = -ENOMEM;\n-\t\tif (NULL == mbuf)\n+\t\tif (array_buf == NULL)\n \t\t\tgoto out_array_args;\n \t\terr = -EFAULT;\n \t\tif (in_compat_syscall())\n-\t\t\terr = v4l2_compat_get_array_args(file, mbuf, user_ptr,\n-\t\t\t\t\t\t\t array_size, orig_cmd,\n-\t\t\t\t\t\t\t parg);\n+\t\t\terr = v4l2_compat_get_array_args(file, array_buf,\n+\t\t\t\t\t\t\t user_ptr, array_size,\n+\t\t\t\t\t\t\t orig_cmd, parg);\n \t\telse\n-\t\t\terr = copy_from_user(mbuf, user_ptr, array_size) ?\n+\t\t\terr = copy_from_user(array_buf, user_ptr, array_size) ?\n \t\t\t\t\t\t\t\t-EFAULT : 0;\n \t\tif (err)\n \t\t\tgoto out_array_args;\n-\t\t*kernel_ptr = mbuf;\n+\t\t*kernel_ptr = array_buf;\n \t}\n \n \t/* Handles IOCTL */\n@@ -80,12 +74,13 @@\n \t\tif (in_compat_syscall()) {\n \t\t\tint put_err;\n \n-\t\t\tput_err = v4l2_compat_put_array_args(file, user_ptr, mbuf,\n-\t\t\t\t\t\t\t     array_size, orig_cmd,\n-\t\t\t\t\t\t\t     parg);\n+\t\t\tput_err = v4l2_compat_put_array_args(file, user_ptr,\n+\t\t\t\t\t\t\t     array_buf,\n+\t\t\t\t\t\t\t     array_size,\n+\t\t\t\t\t\t\t     orig_cmd, parg);\n \t\t\tif (put_err)\n \t\t\t\terr = put_err;\n-\t\t} else if (copy_to_user(user_ptr, mbuf, array_size)) {\n+\t\t} else if (copy_to_user(user_ptr, array_buf, array_size)) {\n \t\t\terr = -EFAULT;\n \t\t}\n \t\tgoto out_array_args;\n@@ -101,6 +96,7 @@\n \tif (video_put_user((void __user *)arg, parg, cmd, orig_cmd))\n \t\terr = -EFAULT;\n out:\n+\tkvfree(array_buf);\n \tkvfree(mbuf);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tvoid    *mbuf = NULL, *array_buf = NULL;",
                "\t\tarray_buf = kvmalloc(array_size, GFP_KERNEL);",
                "\t\tif (array_buf == NULL)",
                "\t\t\terr = v4l2_compat_get_array_args(file, array_buf,",
                "\t\t\t\t\t\t\t user_ptr, array_size,",
                "\t\t\t\t\t\t\t orig_cmd, parg);",
                "\t\t\terr = copy_from_user(array_buf, user_ptr, array_size) ?",
                "\t\t*kernel_ptr = array_buf;",
                "\t\t\tput_err = v4l2_compat_put_array_args(file, user_ptr,",
                "\t\t\t\t\t\t\t     array_buf,",
                "\t\t\t\t\t\t\t     array_size,",
                "\t\t\t\t\t\t\t     orig_cmd, parg);",
                "\t\t} else if (copy_to_user(user_ptr, array_buf, array_size)) {",
                "\tkvfree(array_buf);"
            ],
            "deleted": [
                "\tvoid    *mbuf = NULL;",
                "\t\t/*",
                "\t\t * When adding new types of array args, make sure that the",
                "\t\t * parent argument to ioctl (which contains the pointer to the",
                "\t\t * array) fits into sbuf (so that mbuf will still remain",
                "\t\t * unused up to here).",
                "\t\t */",
                "\t\tmbuf = kvmalloc(array_size, GFP_KERNEL);",
                "\t\tif (NULL == mbuf)",
                "\t\t\terr = v4l2_compat_get_array_args(file, mbuf, user_ptr,",
                "\t\t\t\t\t\t\t array_size, orig_cmd,",
                "\t\t\t\t\t\t\t parg);",
                "\t\t\terr = copy_from_user(mbuf, user_ptr, array_size) ?",
                "\t\t*kernel_ptr = mbuf;",
                "\t\t\tput_err = v4l2_compat_put_array_args(file, user_ptr, mbuf,",
                "\t\t\t\t\t\t\t     array_size, orig_cmd,",
                "\t\t\t\t\t\t\t     parg);",
                "\t\t} else if (copy_to_user(user_ptr, mbuf, array_size)) {"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.11.3 when a webcam device exists. video_usercopy in drivers/media/v4l2-core/v4l2-ioctl.c has a memory leak for large arguments, aka CID-fb18802a338b.",
        "id": 2958
    },
    {
        "cve_id": "CVE-2021-29649",
        "code_before_change": "static void __exit fini_umd(void)\n{\n\tbpf_preload_ops = NULL;\n\t/* kill UMD in case it's still there due to earlier error */\n\tkill_pid(umd_ops.info.tgid, SIGKILL, 1);\n\tumd_ops.info.tgid = NULL;\n\tumd_unload_blob(&umd_ops.info);\n}",
        "code_after_change": "static void __exit fini_umd(void)\n{\n\tstruct pid *tgid;\n\n\tbpf_preload_ops = NULL;\n\n\t/* kill UMD in case it's still there due to earlier error */\n\ttgid = umd_ops.info.tgid;\n\tif (tgid) {\n\t\tkill_pid(tgid, SIGKILL, 1);\n\n\t\twait_event(tgid->wait_pidfd, thread_group_exited(tgid));\n\t\tumd_cleanup_helper(&umd_ops.info);\n\t}\n\tumd_unload_blob(&umd_ops.info);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,16 @@\n static void __exit fini_umd(void)\n {\n+\tstruct pid *tgid;\n+\n \tbpf_preload_ops = NULL;\n+\n \t/* kill UMD in case it's still there due to earlier error */\n-\tkill_pid(umd_ops.info.tgid, SIGKILL, 1);\n-\tumd_ops.info.tgid = NULL;\n+\ttgid = umd_ops.info.tgid;\n+\tif (tgid) {\n+\t\tkill_pid(tgid, SIGKILL, 1);\n+\n+\t\twait_event(tgid->wait_pidfd, thread_group_exited(tgid));\n+\t\tumd_cleanup_helper(&umd_ops.info);\n+\t}\n \tumd_unload_blob(&umd_ops.info);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct pid *tgid;",
                "",
                "",
                "\ttgid = umd_ops.info.tgid;",
                "\tif (tgid) {",
                "\t\tkill_pid(tgid, SIGKILL, 1);",
                "",
                "\t\twait_event(tgid->wait_pidfd, thread_group_exited(tgid));",
                "\t\tumd_cleanup_helper(&umd_ops.info);",
                "\t}"
            ],
            "deleted": [
                "\tkill_pid(umd_ops.info.tgid, SIGKILL, 1);",
                "\tumd_ops.info.tgid = NULL;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.11.11. The user mode driver (UMD) has a copy_process() memory leak, related to a lack of cleanup steps in kernel/usermode_driver.c and kernel/bpf/preload/bpf_preload_kern.c, aka CID-f60a85cad677.",
        "id": 2953
    },
    {
        "cve_id": "CVE-2023-0597",
        "code_before_change": "static __init void setup_cpu_entry_area_ptes(void)\n{\n#ifdef CONFIG_X86_32\n\tunsigned long start, end;\n\n\t/* The +1 is for the readonly IDT: */\n\tBUILD_BUG_ON((CPU_ENTRY_AREA_PAGES+1)*PAGE_SIZE != CPU_ENTRY_AREA_MAP_SIZE);\n\tBUILD_BUG_ON(CPU_ENTRY_AREA_TOTAL_SIZE != CPU_ENTRY_AREA_MAP_SIZE);\n\tBUG_ON(CPU_ENTRY_AREA_BASE & ~PMD_MASK);\n\n\tstart = CPU_ENTRY_AREA_BASE;\n\tend = start + CPU_ENTRY_AREA_MAP_SIZE;\n\n\t/* Careful here: start + PMD_SIZE might wrap around */\n\tfor (; start < end && start >= CPU_ENTRY_AREA_BASE; start += PMD_SIZE)\n\t\tpopulate_extra_pte(start);\n#endif\n}",
        "code_after_change": "static __init void setup_cpu_entry_area_ptes(void)\n{\n#ifdef CONFIG_X86_32\n\tunsigned long start, end;\n\n\t/* The +1 is for the readonly IDT: */\n\tBUILD_BUG_ON((CPU_ENTRY_AREA_PAGES+1)*PAGE_SIZE != CPU_ENTRY_AREA_MAP_SIZE);\n\tBUG_ON(CPU_ENTRY_AREA_BASE & ~PMD_MASK);\n\n\tstart = CPU_ENTRY_AREA_BASE;\n\tend = start + CPU_ENTRY_AREA_MAP_SIZE;\n\n\t/* Careful here: start + PMD_SIZE might wrap around */\n\tfor (; start < end && start >= CPU_ENTRY_AREA_BASE; start += PMD_SIZE)\n\t\tpopulate_extra_pte(start);\n#endif\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,6 @@\n \n \t/* The +1 is for the readonly IDT: */\n \tBUILD_BUG_ON((CPU_ENTRY_AREA_PAGES+1)*PAGE_SIZE != CPU_ENTRY_AREA_MAP_SIZE);\n-\tBUILD_BUG_ON(CPU_ENTRY_AREA_TOTAL_SIZE != CPU_ENTRY_AREA_MAP_SIZE);\n \tBUG_ON(CPU_ENTRY_AREA_BASE & ~PMD_MASK);\n \n \tstart = CPU_ENTRY_AREA_BASE;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tBUILD_BUG_ON(CPU_ENTRY_AREA_TOTAL_SIZE != CPU_ENTRY_AREA_MAP_SIZE);"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw possibility of memory leak in the Linux kernel cpu_entry_area mapping of X86 CPU data to memory was found in the way user can guess location of exception stack(s) or other important data. A local user could use this flaw to get access to some important data with expected location in memory.",
        "id": 3836
    },
    {
        "cve_id": "CVE-2019-19073",
        "code_before_change": "int htc_connect_service(struct htc_target *target,\n\t\t     struct htc_service_connreq *service_connreq,\n\t\t     enum htc_endpoint_id *conn_rsp_epid)\n{\n\tstruct sk_buff *skb;\n\tstruct htc_endpoint *endpoint;\n\tstruct htc_conn_svc_msg *conn_msg;\n\tint ret;\n\tunsigned long time_left;\n\n\t/* Find an available endpoint */\n\tendpoint = get_next_avail_ep(target->endpoint);\n\tif (!endpoint) {\n\t\tdev_err(target->dev, \"Endpoint is not available for service %d\\n\",\n\t\t\tservice_connreq->service_id);\n\t\treturn -EINVAL;\n\t}\n\n\tendpoint->service_id = service_connreq->service_id;\n\tendpoint->max_txqdepth = service_connreq->max_send_qdepth;\n\tendpoint->ul_pipeid = service_to_ulpipe(service_connreq->service_id);\n\tendpoint->dl_pipeid = service_to_dlpipe(service_connreq->service_id);\n\tendpoint->ep_callbacks = service_connreq->ep_callbacks;\n\n\tskb = alloc_skb(sizeof(struct htc_conn_svc_msg) +\n\t\t\t    sizeof(struct htc_frame_hdr), GFP_ATOMIC);\n\tif (!skb) {\n\t\tdev_err(target->dev, \"Failed to allocate buf to send\"\n\t\t\t\"service connect req\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tskb_reserve(skb, sizeof(struct htc_frame_hdr));\n\n\tconn_msg = skb_put(skb, sizeof(struct htc_conn_svc_msg));\n\tconn_msg->service_id = cpu_to_be16(service_connreq->service_id);\n\tconn_msg->msg_id = cpu_to_be16(HTC_MSG_CONNECT_SERVICE_ID);\n\tconn_msg->con_flags = cpu_to_be16(service_connreq->con_flags);\n\tconn_msg->dl_pipeid = endpoint->dl_pipeid;\n\tconn_msg->ul_pipeid = endpoint->ul_pipeid;\n\n\tret = htc_issue_send(target, skb, skb->len, 0, ENDPOINT0);\n\tif (ret)\n\t\tgoto err;\n\n\ttime_left = wait_for_completion_timeout(&target->cmd_wait, HZ);\n\tif (!time_left) {\n\t\tdev_err(target->dev, \"Service connection timeout for: %d\\n\",\n\t\t\tservice_connreq->service_id);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\t*conn_rsp_epid = target->conn_rsp_epid;\n\treturn 0;\nerr:\n\tkfree_skb(skb);\n\treturn ret;\n}",
        "code_after_change": "int htc_connect_service(struct htc_target *target,\n\t\t     struct htc_service_connreq *service_connreq,\n\t\t     enum htc_endpoint_id *conn_rsp_epid)\n{\n\tstruct sk_buff *skb;\n\tstruct htc_endpoint *endpoint;\n\tstruct htc_conn_svc_msg *conn_msg;\n\tint ret;\n\tunsigned long time_left;\n\n\t/* Find an available endpoint */\n\tendpoint = get_next_avail_ep(target->endpoint);\n\tif (!endpoint) {\n\t\tdev_err(target->dev, \"Endpoint is not available for service %d\\n\",\n\t\t\tservice_connreq->service_id);\n\t\treturn -EINVAL;\n\t}\n\n\tendpoint->service_id = service_connreq->service_id;\n\tendpoint->max_txqdepth = service_connreq->max_send_qdepth;\n\tendpoint->ul_pipeid = service_to_ulpipe(service_connreq->service_id);\n\tendpoint->dl_pipeid = service_to_dlpipe(service_connreq->service_id);\n\tendpoint->ep_callbacks = service_connreq->ep_callbacks;\n\n\tskb = alloc_skb(sizeof(struct htc_conn_svc_msg) +\n\t\t\t    sizeof(struct htc_frame_hdr), GFP_ATOMIC);\n\tif (!skb) {\n\t\tdev_err(target->dev, \"Failed to allocate buf to send\"\n\t\t\t\"service connect req\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tskb_reserve(skb, sizeof(struct htc_frame_hdr));\n\n\tconn_msg = skb_put(skb, sizeof(struct htc_conn_svc_msg));\n\tconn_msg->service_id = cpu_to_be16(service_connreq->service_id);\n\tconn_msg->msg_id = cpu_to_be16(HTC_MSG_CONNECT_SERVICE_ID);\n\tconn_msg->con_flags = cpu_to_be16(service_connreq->con_flags);\n\tconn_msg->dl_pipeid = endpoint->dl_pipeid;\n\tconn_msg->ul_pipeid = endpoint->ul_pipeid;\n\n\tret = htc_issue_send(target, skb, skb->len, 0, ENDPOINT0);\n\tif (ret)\n\t\tgoto err;\n\n\ttime_left = wait_for_completion_timeout(&target->cmd_wait, HZ);\n\tif (!time_left) {\n\t\tdev_err(target->dev, \"Service connection timeout for: %d\\n\",\n\t\t\tservice_connreq->service_id);\n\t\tkfree_skb(skb);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\t*conn_rsp_epid = target->conn_rsp_epid;\n\treturn 0;\nerr:\n\tkfree_skb(skb);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,6 +47,7 @@\n \tif (!time_left) {\n \t\tdev_err(target->dev, \"Service connection timeout for: %d\\n\",\n \t\t\tservice_connreq->service_id);\n+\t\tkfree_skb(skb);\n \t\treturn -ETIMEDOUT;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tkfree_skb(skb);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leaks in drivers/net/wireless/ath/ath9k/htc_hst.c in the Linux kernel through 5.3.11 allow attackers to cause a denial of service (memory consumption) by triggering wait_for_completion_timeout() failures. This affects the htc_config_pipe_credits() function, the htc_setup_complete() function, and the htc_connect_service() function, aka CID-853acf7caf10.",
        "id": 2154
    },
    {
        "cve_id": "CVE-2023-0597",
        "code_before_change": "void __init setup_cpu_entry_areas(void)\n{\n\tunsigned int cpu;\n\n\tsetup_cpu_entry_area_ptes();\n\n\tfor_each_possible_cpu(cpu)\n\t\tsetup_cpu_entry_area(cpu);\n\n\t/*\n\t * This is the last essential update to swapper_pgdir which needs\n\t * to be synchronized to initial_page_table on 32bit.\n\t */\n\tsync_initial_page_table();\n}",
        "code_after_change": "void __init setup_cpu_entry_areas(void)\n{\n\tunsigned int cpu;\n\n\tinit_cea_offsets();\n\n\tsetup_cpu_entry_area_ptes();\n\n\tfor_each_possible_cpu(cpu)\n\t\tsetup_cpu_entry_area(cpu);\n\n\t/*\n\t * This is the last essential update to swapper_pgdir which needs\n\t * to be synchronized to initial_page_table on 32bit.\n\t */\n\tsync_initial_page_table();\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,8 @@\n void __init setup_cpu_entry_areas(void)\n {\n \tunsigned int cpu;\n+\n+\tinit_cea_offsets();\n \n \tsetup_cpu_entry_area_ptes();\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tinit_cea_offsets();"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw possibility of memory leak in the Linux kernel cpu_entry_area mapping of X86 CPU data to memory was found in the way user can guess location of exception stack(s) or other important data. A local user could use this flaw to get access to some important data with expected location in memory.",
        "id": 3835
    },
    {
        "cve_id": "CVE-2019-19082",
        "code_before_change": "struct resource_pool *dcn10_create_resource_pool(\n\t\tconst struct dc_init_data *init_data,\n\t\tstruct dc *dc)\n{\n\tstruct dcn10_resource_pool *pool =\n\t\tkzalloc(sizeof(struct dcn10_resource_pool), GFP_KERNEL);\n\n\tif (!pool)\n\t\treturn NULL;\n\n\tif (construct(init_data->num_virtual_links, dc, pool))\n\t\treturn &pool->base;\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "code_after_change": "struct resource_pool *dcn10_create_resource_pool(\n\t\tconst struct dc_init_data *init_data,\n\t\tstruct dc *dc)\n{\n\tstruct dcn10_resource_pool *pool =\n\t\tkzalloc(sizeof(struct dcn10_resource_pool), GFP_KERNEL);\n\n\tif (!pool)\n\t\treturn NULL;\n\n\tif (construct(init_data->num_virtual_links, dc, pool))\n\t\treturn &pool->base;\n\n\tkfree(pool);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,7 @@\n \tif (construct(init_data->num_virtual_links, dc, pool))\n \t\treturn &pool->base;\n \n+\tkfree(pool);\n \tBREAK_TO_DEBUGGER();\n \treturn NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\tkfree(pool);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leaks in *create_resource_pool() functions under drivers/gpu/drm/amd/display/dc in the Linux kernel through 5.3.11 allow attackers to cause a denial of service (memory consumption). This affects the dce120_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce120/dce120_resource.c, the dce110_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce110/dce110_resource.c, the dce100_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce100/dce100_resource.c, the dcn10_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dcn10/dcn10_resource.c, and the dce112_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce112/dce112_resource.c, aka CID-104c307147ad.",
        "id": 2169
    },
    {
        "cve_id": "CVE-2019-19058",
        "code_before_change": "static struct scatterlist *alloc_sgtable(int size)\n{\n\tint alloc_size, nents, i;\n\tstruct page *new_page;\n\tstruct scatterlist *iter;\n\tstruct scatterlist *table;\n\n\tnents = DIV_ROUND_UP(size, PAGE_SIZE);\n\ttable = kcalloc(nents, sizeof(*table), GFP_KERNEL);\n\tif (!table)\n\t\treturn NULL;\n\tsg_init_table(table, nents);\n\titer = table;\n\tfor_each_sg(table, iter, sg_nents(table), i) {\n\t\tnew_page = alloc_page(GFP_KERNEL);\n\t\tif (!new_page) {\n\t\t\t/* release all previous allocated pages in the table */\n\t\t\titer = table;\n\t\t\tfor_each_sg(table, iter, sg_nents(table), i) {\n\t\t\t\tnew_page = sg_page(iter);\n\t\t\t\tif (new_page)\n\t\t\t\t\t__free_page(new_page);\n\t\t\t}\n\t\t\treturn NULL;\n\t\t}\n\t\talloc_size = min_t(int, size, PAGE_SIZE);\n\t\tsize -= PAGE_SIZE;\n\t\tsg_set_page(iter, new_page, alloc_size, 0);\n\t}\n\treturn table;\n}",
        "code_after_change": "static struct scatterlist *alloc_sgtable(int size)\n{\n\tint alloc_size, nents, i;\n\tstruct page *new_page;\n\tstruct scatterlist *iter;\n\tstruct scatterlist *table;\n\n\tnents = DIV_ROUND_UP(size, PAGE_SIZE);\n\ttable = kcalloc(nents, sizeof(*table), GFP_KERNEL);\n\tif (!table)\n\t\treturn NULL;\n\tsg_init_table(table, nents);\n\titer = table;\n\tfor_each_sg(table, iter, sg_nents(table), i) {\n\t\tnew_page = alloc_page(GFP_KERNEL);\n\t\tif (!new_page) {\n\t\t\t/* release all previous allocated pages in the table */\n\t\t\titer = table;\n\t\t\tfor_each_sg(table, iter, sg_nents(table), i) {\n\t\t\t\tnew_page = sg_page(iter);\n\t\t\t\tif (new_page)\n\t\t\t\t\t__free_page(new_page);\n\t\t\t}\n\t\t\tkfree(table);\n\t\t\treturn NULL;\n\t\t}\n\t\talloc_size = min_t(int, size, PAGE_SIZE);\n\t\tsize -= PAGE_SIZE;\n\t\tsg_set_page(iter, new_page, alloc_size, 0);\n\t}\n\treturn table;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,7 @@\n \t\t\t\tif (new_page)\n \t\t\t\t\t__free_page(new_page);\n \t\t\t}\n+\t\t\tkfree(table);\n \t\t\treturn NULL;\n \t\t}\n \t\talloc_size = min_t(int, size, PAGE_SIZE);",
        "function_modified_lines": {
            "added": [
                "\t\t\tkfree(table);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the alloc_sgtable() function in drivers/net/wireless/intel/iwlwifi/fw/dbg.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering alloc_page() failures, aka CID-b4b814fec1a5.",
        "id": 2139
    },
    {
        "cve_id": "CVE-2023-32247",
        "code_before_change": "int ksmbd_session_register(struct ksmbd_conn *conn,\n\t\t\t   struct ksmbd_session *sess)\n{\n\tsess->dialect = conn->dialect;\n\tmemcpy(sess->ClientGUID, conn->ClientGUID, SMB2_CLIENT_GUID_SIZE);\n\treturn xa_err(xa_store(&conn->sessions, sess->id, sess, GFP_KERNEL));\n}",
        "code_after_change": "int ksmbd_session_register(struct ksmbd_conn *conn,\n\t\t\t   struct ksmbd_session *sess)\n{\n\tsess->dialect = conn->dialect;\n\tmemcpy(sess->ClientGUID, conn->ClientGUID, SMB2_CLIENT_GUID_SIZE);\n\tksmbd_expire_session(conn);\n\treturn xa_err(xa_store(&conn->sessions, sess->id, sess, GFP_KERNEL));\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,5 +3,6 @@\n {\n \tsess->dialect = conn->dialect;\n \tmemcpy(sess->ClientGUID, conn->ClientGUID, SMB2_CLIENT_GUID_SIZE);\n+\tksmbd_expire_session(conn);\n \treturn xa_err(xa_store(&conn->sessions, sess->id, sess, GFP_KERNEL));\n }",
        "function_modified_lines": {
            "added": [
                "\tksmbd_expire_session(conn);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ksmbd, a high-performance in-kernel SMB server. The specific flaw exists within the handling of SMB2_SESSION_SETUP commands. The issue results from the lack of control of resource consumption. An attacker can leverage this vulnerability to create a denial-of-service condition on the system.",
        "id": 4010
    },
    {
        "cve_id": "CVE-2019-19082",
        "code_before_change": "struct resource_pool *dce120_create_resource_pool(\n\tuint8_t num_virtual_links,\n\tstruct dc *dc)\n{\n\tstruct dce110_resource_pool *pool =\n\t\tkzalloc(sizeof(struct dce110_resource_pool), GFP_KERNEL);\n\n\tif (!pool)\n\t\treturn NULL;\n\n\tif (construct(num_virtual_links, dc, pool))\n\t\treturn &pool->base;\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "code_after_change": "struct resource_pool *dce120_create_resource_pool(\n\tuint8_t num_virtual_links,\n\tstruct dc *dc)\n{\n\tstruct dce110_resource_pool *pool =\n\t\tkzalloc(sizeof(struct dce110_resource_pool), GFP_KERNEL);\n\n\tif (!pool)\n\t\treturn NULL;\n\n\tif (construct(num_virtual_links, dc, pool))\n\t\treturn &pool->base;\n\n\tkfree(pool);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,7 @@\n \tif (construct(num_virtual_links, dc, pool))\n \t\treturn &pool->base;\n \n+\tkfree(pool);\n \tBREAK_TO_DEBUGGER();\n \treturn NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\tkfree(pool);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leaks in *create_resource_pool() functions under drivers/gpu/drm/amd/display/dc in the Linux kernel through 5.3.11 allow attackers to cause a denial of service (memory consumption). This affects the dce120_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce120/dce120_resource.c, the dce110_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce110/dce110_resource.c, the dce100_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce100/dce100_resource.c, the dcn10_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dcn10/dcn10_resource.c, and the dce112_create_resource_pool() function in drivers/gpu/drm/amd/display/dc/dce112/dce112_resource.c, aka CID-104c307147ad.",
        "id": 2168
    },
    {
        "cve_id": "CVE-2019-19074",
        "code_before_change": "int ath9k_wmi_cmd(struct wmi *wmi, enum wmi_cmd_id cmd_id,\n\t\t  u8 *cmd_buf, u32 cmd_len,\n\t\t  u8 *rsp_buf, u32 rsp_len,\n\t\t  u32 timeout)\n{\n\tstruct ath_hw *ah = wmi->drv_priv->ah;\n\tstruct ath_common *common = ath9k_hw_common(ah);\n\tu16 headroom = sizeof(struct htc_frame_hdr) +\n\t\t       sizeof(struct wmi_cmd_hdr);\n\tstruct sk_buff *skb;\n\tunsigned long time_left;\n\tint ret = 0;\n\n\tif (ah->ah_flags & AH_UNPLUGGED)\n\t\treturn 0;\n\n\tskb = alloc_skb(headroom + cmd_len, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tskb_reserve(skb, headroom);\n\n\tif (cmd_len != 0 && cmd_buf != NULL) {\n\t\tskb_put_data(skb, cmd_buf, cmd_len);\n\t}\n\n\tmutex_lock(&wmi->op_mutex);\n\n\t/* check if wmi stopped flag is set */\n\tif (unlikely(wmi->stopped)) {\n\t\tret = -EPROTO;\n\t\tgoto out;\n\t}\n\n\t/* record the rsp buffer and length */\n\twmi->cmd_rsp_buf = rsp_buf;\n\twmi->cmd_rsp_len = rsp_len;\n\n\tret = ath9k_wmi_cmd_issue(wmi, skb, cmd_id, cmd_len);\n\tif (ret)\n\t\tgoto out;\n\n\ttime_left = wait_for_completion_timeout(&wmi->cmd_wait, timeout);\n\tif (!time_left) {\n\t\tath_dbg(common, WMI, \"Timeout waiting for WMI command: %s\\n\",\n\t\t\twmi_cmd_to_name(cmd_id));\n\t\tmutex_unlock(&wmi->op_mutex);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\tmutex_unlock(&wmi->op_mutex);\n\n\treturn 0;\n\nout:\n\tath_dbg(common, WMI, \"WMI failure for: %s\\n\", wmi_cmd_to_name(cmd_id));\n\tmutex_unlock(&wmi->op_mutex);\n\tkfree_skb(skb);\n\n\treturn ret;\n}",
        "code_after_change": "int ath9k_wmi_cmd(struct wmi *wmi, enum wmi_cmd_id cmd_id,\n\t\t  u8 *cmd_buf, u32 cmd_len,\n\t\t  u8 *rsp_buf, u32 rsp_len,\n\t\t  u32 timeout)\n{\n\tstruct ath_hw *ah = wmi->drv_priv->ah;\n\tstruct ath_common *common = ath9k_hw_common(ah);\n\tu16 headroom = sizeof(struct htc_frame_hdr) +\n\t\t       sizeof(struct wmi_cmd_hdr);\n\tstruct sk_buff *skb;\n\tunsigned long time_left;\n\tint ret = 0;\n\n\tif (ah->ah_flags & AH_UNPLUGGED)\n\t\treturn 0;\n\n\tskb = alloc_skb(headroom + cmd_len, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tskb_reserve(skb, headroom);\n\n\tif (cmd_len != 0 && cmd_buf != NULL) {\n\t\tskb_put_data(skb, cmd_buf, cmd_len);\n\t}\n\n\tmutex_lock(&wmi->op_mutex);\n\n\t/* check if wmi stopped flag is set */\n\tif (unlikely(wmi->stopped)) {\n\t\tret = -EPROTO;\n\t\tgoto out;\n\t}\n\n\t/* record the rsp buffer and length */\n\twmi->cmd_rsp_buf = rsp_buf;\n\twmi->cmd_rsp_len = rsp_len;\n\n\tret = ath9k_wmi_cmd_issue(wmi, skb, cmd_id, cmd_len);\n\tif (ret)\n\t\tgoto out;\n\n\ttime_left = wait_for_completion_timeout(&wmi->cmd_wait, timeout);\n\tif (!time_left) {\n\t\tath_dbg(common, WMI, \"Timeout waiting for WMI command: %s\\n\",\n\t\t\twmi_cmd_to_name(cmd_id));\n\t\tmutex_unlock(&wmi->op_mutex);\n\t\tkfree_skb(skb);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\tmutex_unlock(&wmi->op_mutex);\n\n\treturn 0;\n\nout:\n\tath_dbg(common, WMI, \"WMI failure for: %s\\n\", wmi_cmd_to_name(cmd_id));\n\tmutex_unlock(&wmi->op_mutex);\n\tkfree_skb(skb);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -45,6 +45,7 @@\n \t\tath_dbg(common, WMI, \"Timeout waiting for WMI command: %s\\n\",\n \t\t\twmi_cmd_to_name(cmd_id));\n \t\tmutex_unlock(&wmi->op_mutex);\n+\t\tkfree_skb(skb);\n \t\treturn -ETIMEDOUT;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tkfree_skb(skb);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the ath9k_wmi_cmd() function in drivers/net/wireless/ath/ath9k/wmi.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption), aka CID-728c1e2a05e4.",
        "id": 2157
    },
    {
        "cve_id": "CVE-2022-1651",
        "code_before_change": "static long acrn_dev_ioctl(struct file *filp, unsigned int cmd,\n\t\t\t   unsigned long ioctl_param)\n{\n\tstruct acrn_vm *vm = filp->private_data;\n\tstruct acrn_vm_creation *vm_param;\n\tstruct acrn_vcpu_regs *cpu_regs;\n\tstruct acrn_ioreq_notify notify;\n\tstruct acrn_ptdev_irq *irq_info;\n\tstruct acrn_ioeventfd ioeventfd;\n\tstruct acrn_vm_memmap memmap;\n\tstruct acrn_mmiodev *mmiodev;\n\tstruct acrn_msi_entry *msi;\n\tstruct acrn_pcidev *pcidev;\n\tstruct acrn_irqfd irqfd;\n\tstruct acrn_vdev *vdev;\n\tstruct page *page;\n\tu64 cstate_cmd;\n\tint i, ret = 0;\n\n\tif (vm->vmid == ACRN_INVALID_VMID && cmd != ACRN_IOCTL_CREATE_VM) {\n\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\"ioctl 0x%x: Invalid VM state!\\n\", cmd);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (cmd) {\n\tcase ACRN_IOCTL_CREATE_VM:\n\t\tvm_param = memdup_user((void __user *)ioctl_param,\n\t\t\t\t       sizeof(struct acrn_vm_creation));\n\t\tif (IS_ERR(vm_param))\n\t\t\treturn PTR_ERR(vm_param);\n\n\t\tif ((vm_param->reserved0 | vm_param->reserved1) != 0)\n\t\t\treturn -EINVAL;\n\n\t\tvm = acrn_vm_create(vm, vm_param);\n\t\tif (!vm) {\n\t\t\tret = -EINVAL;\n\t\t\tkfree(vm_param);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_to_user((void __user *)ioctl_param, vm_param,\n\t\t\t\t sizeof(struct acrn_vm_creation))) {\n\t\t\tacrn_vm_destroy(vm);\n\t\t\tret = -EFAULT;\n\t\t}\n\n\t\tkfree(vm_param);\n\t\tbreak;\n\tcase ACRN_IOCTL_START_VM:\n\t\tret = hcall_start_vm(vm->vmid);\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to start VM %u!\\n\", vm->vmid);\n\t\tbreak;\n\tcase ACRN_IOCTL_PAUSE_VM:\n\t\tret = hcall_pause_vm(vm->vmid);\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to pause VM %u!\\n\", vm->vmid);\n\t\tbreak;\n\tcase ACRN_IOCTL_RESET_VM:\n\t\tret = hcall_reset_vm(vm->vmid);\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to restart VM %u!\\n\", vm->vmid);\n\t\tbreak;\n\tcase ACRN_IOCTL_DESTROY_VM:\n\t\tret = acrn_vm_destroy(vm);\n\t\tbreak;\n\tcase ACRN_IOCTL_SET_VCPU_REGS:\n\t\tcpu_regs = memdup_user((void __user *)ioctl_param,\n\t\t\t\t       sizeof(struct acrn_vcpu_regs));\n\t\tif (IS_ERR(cpu_regs))\n\t\t\treturn PTR_ERR(cpu_regs);\n\n\t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->reserved); i++)\n\t\t\tif (cpu_regs->reserved[i])\n\t\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->vcpu_regs.reserved_32); i++)\n\t\t\tif (cpu_regs->vcpu_regs.reserved_32[i])\n\t\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->vcpu_regs.reserved_64); i++)\n\t\t\tif (cpu_regs->vcpu_regs.reserved_64[i])\n\t\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->vcpu_regs.gdt.reserved); i++)\n\t\t\tif (cpu_regs->vcpu_regs.gdt.reserved[i] |\n\t\t\t    cpu_regs->vcpu_regs.idt.reserved[i])\n\t\t\t\treturn -EINVAL;\n\n\t\tret = hcall_set_vcpu_regs(vm->vmid, virt_to_phys(cpu_regs));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to set regs state of VM%u!\\n\",\n\t\t\t\tvm->vmid);\n\t\tkfree(cpu_regs);\n\t\tbreak;\n\tcase ACRN_IOCTL_SET_MEMSEG:\n\t\tif (copy_from_user(&memmap, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(memmap)))\n\t\t\treturn -EFAULT;\n\n\t\tret = acrn_vm_memseg_map(vm, &memmap);\n\t\tbreak;\n\tcase ACRN_IOCTL_UNSET_MEMSEG:\n\t\tif (copy_from_user(&memmap, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(memmap)))\n\t\t\treturn -EFAULT;\n\n\t\tret = acrn_vm_memseg_unmap(vm, &memmap);\n\t\tbreak;\n\tcase ACRN_IOCTL_ASSIGN_MMIODEV:\n\t\tmmiodev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t      sizeof(struct acrn_mmiodev));\n\t\tif (IS_ERR(mmiodev))\n\t\t\treturn PTR_ERR(mmiodev);\n\n\t\tret = hcall_assign_mmiodev(vm->vmid, virt_to_phys(mmiodev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to assign MMIO device!\\n\");\n\t\tkfree(mmiodev);\n\t\tbreak;\n\tcase ACRN_IOCTL_DEASSIGN_MMIODEV:\n\t\tmmiodev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t      sizeof(struct acrn_mmiodev));\n\t\tif (IS_ERR(mmiodev))\n\t\t\treturn PTR_ERR(mmiodev);\n\n\t\tret = hcall_deassign_mmiodev(vm->vmid, virt_to_phys(mmiodev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to deassign MMIO device!\\n\");\n\t\tkfree(mmiodev);\n\t\tbreak;\n\tcase ACRN_IOCTL_ASSIGN_PCIDEV:\n\t\tpcidev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t     sizeof(struct acrn_pcidev));\n\t\tif (IS_ERR(pcidev))\n\t\t\treturn PTR_ERR(pcidev);\n\n\t\tret = hcall_assign_pcidev(vm->vmid, virt_to_phys(pcidev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to assign pci device!\\n\");\n\t\tkfree(pcidev);\n\t\tbreak;\n\tcase ACRN_IOCTL_DEASSIGN_PCIDEV:\n\t\tpcidev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t     sizeof(struct acrn_pcidev));\n\t\tif (IS_ERR(pcidev))\n\t\t\treturn PTR_ERR(pcidev);\n\n\t\tret = hcall_deassign_pcidev(vm->vmid, virt_to_phys(pcidev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to deassign pci device!\\n\");\n\t\tkfree(pcidev);\n\t\tbreak;\n\tcase ACRN_IOCTL_CREATE_VDEV:\n\t\tvdev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t   sizeof(struct acrn_vdev));\n\t\tif (IS_ERR(vdev))\n\t\t\treturn PTR_ERR(vdev);\n\n\t\tret = hcall_create_vdev(vm->vmid, virt_to_phys(vdev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to create virtual device!\\n\");\n\t\tkfree(vdev);\n\t\tbreak;\n\tcase ACRN_IOCTL_DESTROY_VDEV:\n\t\tvdev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t   sizeof(struct acrn_vdev));\n\t\tif (IS_ERR(vdev))\n\t\t\treturn PTR_ERR(vdev);\n\t\tret = hcall_destroy_vdev(vm->vmid, virt_to_phys(vdev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to destroy virtual device!\\n\");\n\t\tkfree(vdev);\n\t\tbreak;\n\tcase ACRN_IOCTL_SET_PTDEV_INTR:\n\t\tirq_info = memdup_user((void __user *)ioctl_param,\n\t\t\t\t       sizeof(struct acrn_ptdev_irq));\n\t\tif (IS_ERR(irq_info))\n\t\t\treturn PTR_ERR(irq_info);\n\n\t\tret = hcall_set_ptdev_intr(vm->vmid, virt_to_phys(irq_info));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to configure intr for ptdev!\\n\");\n\t\tkfree(irq_info);\n\t\tbreak;\n\tcase ACRN_IOCTL_RESET_PTDEV_INTR:\n\t\tirq_info = memdup_user((void __user *)ioctl_param,\n\t\t\t\t       sizeof(struct acrn_ptdev_irq));\n\t\tif (IS_ERR(irq_info))\n\t\t\treturn PTR_ERR(irq_info);\n\n\t\tret = hcall_reset_ptdev_intr(vm->vmid, virt_to_phys(irq_info));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to reset intr for ptdev!\\n\");\n\t\tkfree(irq_info);\n\t\tbreak;\n\tcase ACRN_IOCTL_SET_IRQLINE:\n\t\tret = hcall_set_irqline(vm->vmid, ioctl_param);\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to set interrupt line!\\n\");\n\t\tbreak;\n\tcase ACRN_IOCTL_INJECT_MSI:\n\t\tmsi = memdup_user((void __user *)ioctl_param,\n\t\t\t\t  sizeof(struct acrn_msi_entry));\n\t\tif (IS_ERR(msi))\n\t\t\treturn PTR_ERR(msi);\n\n\t\tret = hcall_inject_msi(vm->vmid, virt_to_phys(msi));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to inject MSI!\\n\");\n\t\tkfree(msi);\n\t\tbreak;\n\tcase ACRN_IOCTL_VM_INTR_MONITOR:\n\t\tret = pin_user_pages_fast(ioctl_param, 1,\n\t\t\t\t\t  FOLL_WRITE | FOLL_LONGTERM, &page);\n\t\tif (unlikely(ret != 1)) {\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to pin intr hdr buffer!\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tret = hcall_vm_intr_monitor(vm->vmid, page_to_phys(page));\n\t\tif (ret < 0) {\n\t\t\tunpin_user_page(page);\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to monitor intr data!\\n\");\n\t\t\treturn ret;\n\t\t}\n\t\tif (vm->monitor_page)\n\t\t\tunpin_user_page(vm->monitor_page);\n\t\tvm->monitor_page = page;\n\t\tbreak;\n\tcase ACRN_IOCTL_CREATE_IOREQ_CLIENT:\n\t\tif (vm->default_client)\n\t\t\treturn -EEXIST;\n\t\tif (!acrn_ioreq_client_create(vm, NULL, NULL, true, \"acrndm\"))\n\t\t\tret = -EINVAL;\n\t\tbreak;\n\tcase ACRN_IOCTL_DESTROY_IOREQ_CLIENT:\n\t\tif (vm->default_client)\n\t\t\tacrn_ioreq_client_destroy(vm->default_client);\n\t\tbreak;\n\tcase ACRN_IOCTL_ATTACH_IOREQ_CLIENT:\n\t\tif (vm->default_client)\n\t\t\tret = acrn_ioreq_client_wait(vm->default_client);\n\t\telse\n\t\t\tret = -ENODEV;\n\t\tbreak;\n\tcase ACRN_IOCTL_NOTIFY_REQUEST_FINISH:\n\t\tif (copy_from_user(&notify, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(struct acrn_ioreq_notify)))\n\t\t\treturn -EFAULT;\n\n\t\tif (notify.reserved != 0)\n\t\t\treturn -EINVAL;\n\n\t\tret = acrn_ioreq_request_default_complete(vm, notify.vcpu);\n\t\tbreak;\n\tcase ACRN_IOCTL_CLEAR_VM_IOREQ:\n\t\tacrn_ioreq_request_clear(vm);\n\t\tbreak;\n\tcase ACRN_IOCTL_PM_GET_CPU_STATE:\n\t\tif (copy_from_user(&cstate_cmd, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(cstate_cmd)))\n\t\t\treturn -EFAULT;\n\n\t\tret = pmcmd_ioctl(cstate_cmd, (void __user *)ioctl_param);\n\t\tbreak;\n\tcase ACRN_IOCTL_IOEVENTFD:\n\t\tif (copy_from_user(&ioeventfd, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(ioeventfd)))\n\t\t\treturn -EFAULT;\n\n\t\tif (ioeventfd.reserved != 0)\n\t\t\treturn -EINVAL;\n\n\t\tret = acrn_ioeventfd_config(vm, &ioeventfd);\n\t\tbreak;\n\tcase ACRN_IOCTL_IRQFD:\n\t\tif (copy_from_user(&irqfd, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(irqfd)))\n\t\t\treturn -EFAULT;\n\t\tret = acrn_irqfd_config(vm, &irqfd);\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(acrn_dev.this_device, \"Unknown IOCTL 0x%x!\\n\", cmd);\n\t\tret = -ENOTTY;\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static long acrn_dev_ioctl(struct file *filp, unsigned int cmd,\n\t\t\t   unsigned long ioctl_param)\n{\n\tstruct acrn_vm *vm = filp->private_data;\n\tstruct acrn_vm_creation *vm_param;\n\tstruct acrn_vcpu_regs *cpu_regs;\n\tstruct acrn_ioreq_notify notify;\n\tstruct acrn_ptdev_irq *irq_info;\n\tstruct acrn_ioeventfd ioeventfd;\n\tstruct acrn_vm_memmap memmap;\n\tstruct acrn_mmiodev *mmiodev;\n\tstruct acrn_msi_entry *msi;\n\tstruct acrn_pcidev *pcidev;\n\tstruct acrn_irqfd irqfd;\n\tstruct acrn_vdev *vdev;\n\tstruct page *page;\n\tu64 cstate_cmd;\n\tint i, ret = 0;\n\n\tif (vm->vmid == ACRN_INVALID_VMID && cmd != ACRN_IOCTL_CREATE_VM) {\n\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\"ioctl 0x%x: Invalid VM state!\\n\", cmd);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (cmd) {\n\tcase ACRN_IOCTL_CREATE_VM:\n\t\tvm_param = memdup_user((void __user *)ioctl_param,\n\t\t\t\t       sizeof(struct acrn_vm_creation));\n\t\tif (IS_ERR(vm_param))\n\t\t\treturn PTR_ERR(vm_param);\n\n\t\tif ((vm_param->reserved0 | vm_param->reserved1) != 0) {\n\t\t\tkfree(vm_param);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tvm = acrn_vm_create(vm, vm_param);\n\t\tif (!vm) {\n\t\t\tret = -EINVAL;\n\t\t\tkfree(vm_param);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_to_user((void __user *)ioctl_param, vm_param,\n\t\t\t\t sizeof(struct acrn_vm_creation))) {\n\t\t\tacrn_vm_destroy(vm);\n\t\t\tret = -EFAULT;\n\t\t}\n\n\t\tkfree(vm_param);\n\t\tbreak;\n\tcase ACRN_IOCTL_START_VM:\n\t\tret = hcall_start_vm(vm->vmid);\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to start VM %u!\\n\", vm->vmid);\n\t\tbreak;\n\tcase ACRN_IOCTL_PAUSE_VM:\n\t\tret = hcall_pause_vm(vm->vmid);\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to pause VM %u!\\n\", vm->vmid);\n\t\tbreak;\n\tcase ACRN_IOCTL_RESET_VM:\n\t\tret = hcall_reset_vm(vm->vmid);\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to restart VM %u!\\n\", vm->vmid);\n\t\tbreak;\n\tcase ACRN_IOCTL_DESTROY_VM:\n\t\tret = acrn_vm_destroy(vm);\n\t\tbreak;\n\tcase ACRN_IOCTL_SET_VCPU_REGS:\n\t\tcpu_regs = memdup_user((void __user *)ioctl_param,\n\t\t\t\t       sizeof(struct acrn_vcpu_regs));\n\t\tif (IS_ERR(cpu_regs))\n\t\t\treturn PTR_ERR(cpu_regs);\n\n\t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->reserved); i++)\n\t\t\tif (cpu_regs->reserved[i]) {\n\t\t\t\tkfree(cpu_regs);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->vcpu_regs.reserved_32); i++)\n\t\t\tif (cpu_regs->vcpu_regs.reserved_32[i]) {\n\t\t\t\tkfree(cpu_regs);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->vcpu_regs.reserved_64); i++)\n\t\t\tif (cpu_regs->vcpu_regs.reserved_64[i]) {\n\t\t\t\tkfree(cpu_regs);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->vcpu_regs.gdt.reserved); i++)\n\t\t\tif (cpu_regs->vcpu_regs.gdt.reserved[i] |\n\t\t\t    cpu_regs->vcpu_regs.idt.reserved[i]) {\n\t\t\t\tkfree(cpu_regs);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\tret = hcall_set_vcpu_regs(vm->vmid, virt_to_phys(cpu_regs));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to set regs state of VM%u!\\n\",\n\t\t\t\tvm->vmid);\n\t\tkfree(cpu_regs);\n\t\tbreak;\n\tcase ACRN_IOCTL_SET_MEMSEG:\n\t\tif (copy_from_user(&memmap, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(memmap)))\n\t\t\treturn -EFAULT;\n\n\t\tret = acrn_vm_memseg_map(vm, &memmap);\n\t\tbreak;\n\tcase ACRN_IOCTL_UNSET_MEMSEG:\n\t\tif (copy_from_user(&memmap, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(memmap)))\n\t\t\treturn -EFAULT;\n\n\t\tret = acrn_vm_memseg_unmap(vm, &memmap);\n\t\tbreak;\n\tcase ACRN_IOCTL_ASSIGN_MMIODEV:\n\t\tmmiodev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t      sizeof(struct acrn_mmiodev));\n\t\tif (IS_ERR(mmiodev))\n\t\t\treturn PTR_ERR(mmiodev);\n\n\t\tret = hcall_assign_mmiodev(vm->vmid, virt_to_phys(mmiodev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to assign MMIO device!\\n\");\n\t\tkfree(mmiodev);\n\t\tbreak;\n\tcase ACRN_IOCTL_DEASSIGN_MMIODEV:\n\t\tmmiodev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t      sizeof(struct acrn_mmiodev));\n\t\tif (IS_ERR(mmiodev))\n\t\t\treturn PTR_ERR(mmiodev);\n\n\t\tret = hcall_deassign_mmiodev(vm->vmid, virt_to_phys(mmiodev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to deassign MMIO device!\\n\");\n\t\tkfree(mmiodev);\n\t\tbreak;\n\tcase ACRN_IOCTL_ASSIGN_PCIDEV:\n\t\tpcidev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t     sizeof(struct acrn_pcidev));\n\t\tif (IS_ERR(pcidev))\n\t\t\treturn PTR_ERR(pcidev);\n\n\t\tret = hcall_assign_pcidev(vm->vmid, virt_to_phys(pcidev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to assign pci device!\\n\");\n\t\tkfree(pcidev);\n\t\tbreak;\n\tcase ACRN_IOCTL_DEASSIGN_PCIDEV:\n\t\tpcidev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t     sizeof(struct acrn_pcidev));\n\t\tif (IS_ERR(pcidev))\n\t\t\treturn PTR_ERR(pcidev);\n\n\t\tret = hcall_deassign_pcidev(vm->vmid, virt_to_phys(pcidev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to deassign pci device!\\n\");\n\t\tkfree(pcidev);\n\t\tbreak;\n\tcase ACRN_IOCTL_CREATE_VDEV:\n\t\tvdev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t   sizeof(struct acrn_vdev));\n\t\tif (IS_ERR(vdev))\n\t\t\treturn PTR_ERR(vdev);\n\n\t\tret = hcall_create_vdev(vm->vmid, virt_to_phys(vdev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to create virtual device!\\n\");\n\t\tkfree(vdev);\n\t\tbreak;\n\tcase ACRN_IOCTL_DESTROY_VDEV:\n\t\tvdev = memdup_user((void __user *)ioctl_param,\n\t\t\t\t   sizeof(struct acrn_vdev));\n\t\tif (IS_ERR(vdev))\n\t\t\treturn PTR_ERR(vdev);\n\t\tret = hcall_destroy_vdev(vm->vmid, virt_to_phys(vdev));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to destroy virtual device!\\n\");\n\t\tkfree(vdev);\n\t\tbreak;\n\tcase ACRN_IOCTL_SET_PTDEV_INTR:\n\t\tirq_info = memdup_user((void __user *)ioctl_param,\n\t\t\t\t       sizeof(struct acrn_ptdev_irq));\n\t\tif (IS_ERR(irq_info))\n\t\t\treturn PTR_ERR(irq_info);\n\n\t\tret = hcall_set_ptdev_intr(vm->vmid, virt_to_phys(irq_info));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to configure intr for ptdev!\\n\");\n\t\tkfree(irq_info);\n\t\tbreak;\n\tcase ACRN_IOCTL_RESET_PTDEV_INTR:\n\t\tirq_info = memdup_user((void __user *)ioctl_param,\n\t\t\t\t       sizeof(struct acrn_ptdev_irq));\n\t\tif (IS_ERR(irq_info))\n\t\t\treturn PTR_ERR(irq_info);\n\n\t\tret = hcall_reset_ptdev_intr(vm->vmid, virt_to_phys(irq_info));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to reset intr for ptdev!\\n\");\n\t\tkfree(irq_info);\n\t\tbreak;\n\tcase ACRN_IOCTL_SET_IRQLINE:\n\t\tret = hcall_set_irqline(vm->vmid, ioctl_param);\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to set interrupt line!\\n\");\n\t\tbreak;\n\tcase ACRN_IOCTL_INJECT_MSI:\n\t\tmsi = memdup_user((void __user *)ioctl_param,\n\t\t\t\t  sizeof(struct acrn_msi_entry));\n\t\tif (IS_ERR(msi))\n\t\t\treturn PTR_ERR(msi);\n\n\t\tret = hcall_inject_msi(vm->vmid, virt_to_phys(msi));\n\t\tif (ret < 0)\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to inject MSI!\\n\");\n\t\tkfree(msi);\n\t\tbreak;\n\tcase ACRN_IOCTL_VM_INTR_MONITOR:\n\t\tret = pin_user_pages_fast(ioctl_param, 1,\n\t\t\t\t\t  FOLL_WRITE | FOLL_LONGTERM, &page);\n\t\tif (unlikely(ret != 1)) {\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to pin intr hdr buffer!\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tret = hcall_vm_intr_monitor(vm->vmid, page_to_phys(page));\n\t\tif (ret < 0) {\n\t\t\tunpin_user_page(page);\n\t\t\tdev_dbg(acrn_dev.this_device,\n\t\t\t\t\"Failed to monitor intr data!\\n\");\n\t\t\treturn ret;\n\t\t}\n\t\tif (vm->monitor_page)\n\t\t\tunpin_user_page(vm->monitor_page);\n\t\tvm->monitor_page = page;\n\t\tbreak;\n\tcase ACRN_IOCTL_CREATE_IOREQ_CLIENT:\n\t\tif (vm->default_client)\n\t\t\treturn -EEXIST;\n\t\tif (!acrn_ioreq_client_create(vm, NULL, NULL, true, \"acrndm\"))\n\t\t\tret = -EINVAL;\n\t\tbreak;\n\tcase ACRN_IOCTL_DESTROY_IOREQ_CLIENT:\n\t\tif (vm->default_client)\n\t\t\tacrn_ioreq_client_destroy(vm->default_client);\n\t\tbreak;\n\tcase ACRN_IOCTL_ATTACH_IOREQ_CLIENT:\n\t\tif (vm->default_client)\n\t\t\tret = acrn_ioreq_client_wait(vm->default_client);\n\t\telse\n\t\t\tret = -ENODEV;\n\t\tbreak;\n\tcase ACRN_IOCTL_NOTIFY_REQUEST_FINISH:\n\t\tif (copy_from_user(&notify, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(struct acrn_ioreq_notify)))\n\t\t\treturn -EFAULT;\n\n\t\tif (notify.reserved != 0)\n\t\t\treturn -EINVAL;\n\n\t\tret = acrn_ioreq_request_default_complete(vm, notify.vcpu);\n\t\tbreak;\n\tcase ACRN_IOCTL_CLEAR_VM_IOREQ:\n\t\tacrn_ioreq_request_clear(vm);\n\t\tbreak;\n\tcase ACRN_IOCTL_PM_GET_CPU_STATE:\n\t\tif (copy_from_user(&cstate_cmd, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(cstate_cmd)))\n\t\t\treturn -EFAULT;\n\n\t\tret = pmcmd_ioctl(cstate_cmd, (void __user *)ioctl_param);\n\t\tbreak;\n\tcase ACRN_IOCTL_IOEVENTFD:\n\t\tif (copy_from_user(&ioeventfd, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(ioeventfd)))\n\t\t\treturn -EFAULT;\n\n\t\tif (ioeventfd.reserved != 0)\n\t\t\treturn -EINVAL;\n\n\t\tret = acrn_ioeventfd_config(vm, &ioeventfd);\n\t\tbreak;\n\tcase ACRN_IOCTL_IRQFD:\n\t\tif (copy_from_user(&irqfd, (void __user *)ioctl_param,\n\t\t\t\t   sizeof(irqfd)))\n\t\t\treturn -EFAULT;\n\t\tret = acrn_irqfd_config(vm, &irqfd);\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(acrn_dev.this_device, \"Unknown IOCTL 0x%x!\\n\", cmd);\n\t\tret = -ENOTTY;\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,8 +30,10 @@\n \t\tif (IS_ERR(vm_param))\n \t\t\treturn PTR_ERR(vm_param);\n \n-\t\tif ((vm_param->reserved0 | vm_param->reserved1) != 0)\n+\t\tif ((vm_param->reserved0 | vm_param->reserved1) != 0) {\n+\t\t\tkfree(vm_param);\n \t\t\treturn -EINVAL;\n+\t\t}\n \n \t\tvm = acrn_vm_create(vm, vm_param);\n \t\tif (!vm) {\n@@ -76,21 +78,29 @@\n \t\t\treturn PTR_ERR(cpu_regs);\n \n \t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->reserved); i++)\n-\t\t\tif (cpu_regs->reserved[i])\n+\t\t\tif (cpu_regs->reserved[i]) {\n+\t\t\t\tkfree(cpu_regs);\n \t\t\t\treturn -EINVAL;\n+\t\t\t}\n \n \t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->vcpu_regs.reserved_32); i++)\n-\t\t\tif (cpu_regs->vcpu_regs.reserved_32[i])\n+\t\t\tif (cpu_regs->vcpu_regs.reserved_32[i]) {\n+\t\t\t\tkfree(cpu_regs);\n \t\t\t\treturn -EINVAL;\n+\t\t\t}\n \n \t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->vcpu_regs.reserved_64); i++)\n-\t\t\tif (cpu_regs->vcpu_regs.reserved_64[i])\n+\t\t\tif (cpu_regs->vcpu_regs.reserved_64[i]) {\n+\t\t\t\tkfree(cpu_regs);\n \t\t\t\treturn -EINVAL;\n+\t\t\t}\n \n \t\tfor (i = 0; i < ARRAY_SIZE(cpu_regs->vcpu_regs.gdt.reserved); i++)\n \t\t\tif (cpu_regs->vcpu_regs.gdt.reserved[i] |\n-\t\t\t    cpu_regs->vcpu_regs.idt.reserved[i])\n+\t\t\t    cpu_regs->vcpu_regs.idt.reserved[i]) {\n+\t\t\t\tkfree(cpu_regs);\n \t\t\t\treturn -EINVAL;\n+\t\t\t}\n \n \t\tret = hcall_set_vcpu_regs(vm->vmid, virt_to_phys(cpu_regs));\n \t\tif (ret < 0)",
        "function_modified_lines": {
            "added": [
                "\t\tif ((vm_param->reserved0 | vm_param->reserved1) != 0) {",
                "\t\t\tkfree(vm_param);",
                "\t\t}",
                "\t\t\tif (cpu_regs->reserved[i]) {",
                "\t\t\t\tkfree(cpu_regs);",
                "\t\t\t}",
                "\t\t\tif (cpu_regs->vcpu_regs.reserved_32[i]) {",
                "\t\t\t\tkfree(cpu_regs);",
                "\t\t\t}",
                "\t\t\tif (cpu_regs->vcpu_regs.reserved_64[i]) {",
                "\t\t\t\tkfree(cpu_regs);",
                "\t\t\t}",
                "\t\t\t    cpu_regs->vcpu_regs.idt.reserved[i]) {",
                "\t\t\t\tkfree(cpu_regs);",
                "\t\t\t}"
            ],
            "deleted": [
                "\t\tif ((vm_param->reserved0 | vm_param->reserved1) != 0)",
                "\t\t\tif (cpu_regs->reserved[i])",
                "\t\t\tif (cpu_regs->vcpu_regs.reserved_32[i])",
                "\t\t\tif (cpu_regs->vcpu_regs.reserved_64[i])",
                "\t\t\t    cpu_regs->vcpu_regs.idt.reserved[i])"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak flaw was found in the Linux kernel in acrn_dev_ioctl in the drivers/virt/acrn/hsm.c function in how the ACRN Device Model emulates virtual NICs in VM. This flaw allows a local privileged attacker to leak unauthorized kernel information, causing a denial of service.",
        "id": 3265
    },
    {
        "cve_id": "CVE-2019-19076",
        "code_before_change": "static int\nnfp_abm_u32_knode_replace(struct nfp_abm_link *alink,\n\t\t\t  struct tc_cls_u32_knode *knode,\n\t\t\t  __be16 proto, struct netlink_ext_ack *extack)\n{\n\tstruct nfp_abm_u32_match *match = NULL, *iter;\n\tunsigned int tos_off;\n\tu8 mask, val;\n\tint err;\n\n\tif (!nfp_abm_u32_check_knode(alink->abm, knode, proto, extack))\n\t\tgoto err_delete;\n\n\ttos_off = proto == htons(ETH_P_IP) ? 16 : 20;\n\n\t/* Extract the DSCP Class Selector bits */\n\tval = be32_to_cpu(knode->sel->keys[0].val) >> tos_off & 0xff;\n\tmask = be32_to_cpu(knode->sel->keys[0].mask) >> tos_off & 0xff;\n\n\t/* Check if there is no conflicting mapping and find match by handle */\n\tlist_for_each_entry(iter, &alink->dscp_map, list) {\n\t\tu32 cmask;\n\n\t\tif (iter->handle == knode->handle) {\n\t\t\tmatch = iter;\n\t\t\tcontinue;\n\t\t}\n\n\t\tcmask = iter->mask & mask;\n\t\tif ((iter->val & cmask) == (val & cmask) &&\n\t\t    iter->band != knode->res->classid) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"conflict with already offloaded filter\");\n\t\t\tgoto err_delete;\n\t\t}\n\t}\n\n\tif (!match) {\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match)\n\t\t\treturn -ENOMEM;\n\t\tlist_add(&match->list, &alink->dscp_map);\n\t}\n\tmatch->handle = knode->handle;\n\tmatch->band = knode->res->classid;\n\tmatch->mask = mask;\n\tmatch->val = val;\n\n\terr = nfp_abm_update_band_map(alink);\n\tif (err)\n\t\tgoto err_delete;\n\n\treturn 0;\n\nerr_delete:\n\tnfp_abm_u32_knode_delete(alink, knode);\n\treturn -EOPNOTSUPP;\n}",
        "code_after_change": "static int\nnfp_abm_u32_knode_replace(struct nfp_abm_link *alink,\n\t\t\t  struct tc_cls_u32_knode *knode,\n\t\t\t  __be16 proto, struct netlink_ext_ack *extack)\n{\n\tstruct nfp_abm_u32_match *match = NULL, *iter;\n\tunsigned int tos_off;\n\tu8 mask, val;\n\tint err;\n\n\tif (!nfp_abm_u32_check_knode(alink->abm, knode, proto, extack)) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto err_delete;\n\t}\n\n\ttos_off = proto == htons(ETH_P_IP) ? 16 : 20;\n\n\t/* Extract the DSCP Class Selector bits */\n\tval = be32_to_cpu(knode->sel->keys[0].val) >> tos_off & 0xff;\n\tmask = be32_to_cpu(knode->sel->keys[0].mask) >> tos_off & 0xff;\n\n\t/* Check if there is no conflicting mapping and find match by handle */\n\tlist_for_each_entry(iter, &alink->dscp_map, list) {\n\t\tu32 cmask;\n\n\t\tif (iter->handle == knode->handle) {\n\t\t\tmatch = iter;\n\t\t\tcontinue;\n\t\t}\n\n\t\tcmask = iter->mask & mask;\n\t\tif ((iter->val & cmask) == (val & cmask) &&\n\t\t    iter->band != knode->res->classid) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"conflict with already offloaded filter\");\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_delete;\n\t\t}\n\t}\n\n\tif (!match) {\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_delete;\n\t\t}\n\n\t\tlist_add(&match->list, &alink->dscp_map);\n\t}\n\tmatch->handle = knode->handle;\n\tmatch->band = knode->res->classid;\n\tmatch->mask = mask;\n\tmatch->val = val;\n\n\terr = nfp_abm_update_band_map(alink);\n\tif (err)\n\t\tgoto err_delete;\n\n\treturn 0;\n\nerr_delete:\n\tnfp_abm_u32_knode_delete(alink, knode);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,10 @@\n \tu8 mask, val;\n \tint err;\n \n-\tif (!nfp_abm_u32_check_knode(alink->abm, knode, proto, extack))\n+\tif (!nfp_abm_u32_check_knode(alink->abm, knode, proto, extack)) {\n+\t\terr = -EOPNOTSUPP;\n \t\tgoto err_delete;\n+\t}\n \n \ttos_off = proto == htons(ETH_P_IP) ? 16 : 20;\n \n@@ -30,14 +32,18 @@\n \t\tif ((iter->val & cmask) == (val & cmask) &&\n \t\t    iter->band != knode->res->classid) {\n \t\t\tNL_SET_ERR_MSG_MOD(extack, \"conflict with already offloaded filter\");\n+\t\t\terr = -EOPNOTSUPP;\n \t\t\tgoto err_delete;\n \t\t}\n \t}\n \n \tif (!match) {\n \t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n-\t\tif (!match)\n-\t\t\treturn -ENOMEM;\n+\t\tif (!match) {\n+\t\t\terr = -ENOMEM;\n+\t\t\tgoto err_delete;\n+\t\t}\n+\n \t\tlist_add(&match->list, &alink->dscp_map);\n \t}\n \tmatch->handle = knode->handle;\n@@ -53,5 +59,5 @@\n \n err_delete:\n \tnfp_abm_u32_knode_delete(alink, knode);\n-\treturn -EOPNOTSUPP;\n+\treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!nfp_abm_u32_check_knode(alink->abm, knode, proto, extack)) {",
                "\t\terr = -EOPNOTSUPP;",
                "\t}",
                "\t\t\terr = -EOPNOTSUPP;",
                "\t\tif (!match) {",
                "\t\t\terr = -ENOMEM;",
                "\t\t\tgoto err_delete;",
                "\t\t}",
                "",
                "\treturn err;"
            ],
            "deleted": [
                "\tif (!nfp_abm_u32_check_knode(alink->abm, knode, proto, extack))",
                "\t\tif (!match)",
                "\t\t\treturn -ENOMEM;",
                "\treturn -EOPNOTSUPP;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the nfp_abm_u32_knode_replace() function in drivers/net/ethernet/netronome/nfp/abm/cls.c in the Linux kernel before 5.3.6 allows attackers to cause a denial of service (memory consumption), aka CID-78beef629fd9. NOTE: This has been argued as not a valid vulnerability. The upstream commit 78beef629fd9 was reverted",
        "id": 2159
    },
    {
        "cve_id": "CVE-2019-16994",
        "code_before_change": "static int __net_init sit_init_net(struct net *net)\n{\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\tstruct ip_tunnel *t;\n\tint err;\n\n\tsitn->tunnels[0] = sitn->tunnels_wc;\n\tsitn->tunnels[1] = sitn->tunnels_l;\n\tsitn->tunnels[2] = sitn->tunnels_r;\n\tsitn->tunnels[3] = sitn->tunnels_r_l;\n\n\tif (!net_has_fallback_tunnels(net))\n\t\treturn 0;\n\n\tsitn->fb_tunnel_dev = alloc_netdev(sizeof(struct ip_tunnel), \"sit0\",\n\t\t\t\t\t   NET_NAME_UNKNOWN,\n\t\t\t\t\t   ipip6_tunnel_setup);\n\tif (!sitn->fb_tunnel_dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_dev;\n\t}\n\tdev_net_set(sitn->fb_tunnel_dev, net);\n\tsitn->fb_tunnel_dev->rtnl_link_ops = &sit_link_ops;\n\t/* FB netdevice is special: we have one, and only one per netns.\n\t * Allowing to move it to another netns is clearly unsafe.\n\t */\n\tsitn->fb_tunnel_dev->features |= NETIF_F_NETNS_LOCAL;\n\n\terr = register_netdev(sitn->fb_tunnel_dev);\n\tif (err)\n\t\tgoto err_reg_dev;\n\n\tipip6_tunnel_clone_6rd(sitn->fb_tunnel_dev, sitn);\n\tipip6_fb_tunnel_init(sitn->fb_tunnel_dev);\n\n\tt = netdev_priv(sitn->fb_tunnel_dev);\n\n\tstrcpy(t->parms.name, sitn->fb_tunnel_dev->name);\n\treturn 0;\n\nerr_reg_dev:\n\tipip6_dev_free(sitn->fb_tunnel_dev);\nerr_alloc_dev:\n\treturn err;\n}",
        "code_after_change": "static int __net_init sit_init_net(struct net *net)\n{\n\tstruct sit_net *sitn = net_generic(net, sit_net_id);\n\tstruct ip_tunnel *t;\n\tint err;\n\n\tsitn->tunnels[0] = sitn->tunnels_wc;\n\tsitn->tunnels[1] = sitn->tunnels_l;\n\tsitn->tunnels[2] = sitn->tunnels_r;\n\tsitn->tunnels[3] = sitn->tunnels_r_l;\n\n\tif (!net_has_fallback_tunnels(net))\n\t\treturn 0;\n\n\tsitn->fb_tunnel_dev = alloc_netdev(sizeof(struct ip_tunnel), \"sit0\",\n\t\t\t\t\t   NET_NAME_UNKNOWN,\n\t\t\t\t\t   ipip6_tunnel_setup);\n\tif (!sitn->fb_tunnel_dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_dev;\n\t}\n\tdev_net_set(sitn->fb_tunnel_dev, net);\n\tsitn->fb_tunnel_dev->rtnl_link_ops = &sit_link_ops;\n\t/* FB netdevice is special: we have one, and only one per netns.\n\t * Allowing to move it to another netns is clearly unsafe.\n\t */\n\tsitn->fb_tunnel_dev->features |= NETIF_F_NETNS_LOCAL;\n\n\terr = register_netdev(sitn->fb_tunnel_dev);\n\tif (err)\n\t\tgoto err_reg_dev;\n\n\tipip6_tunnel_clone_6rd(sitn->fb_tunnel_dev, sitn);\n\tipip6_fb_tunnel_init(sitn->fb_tunnel_dev);\n\n\tt = netdev_priv(sitn->fb_tunnel_dev);\n\n\tstrcpy(t->parms.name, sitn->fb_tunnel_dev->name);\n\treturn 0;\n\nerr_reg_dev:\n\tipip6_dev_free(sitn->fb_tunnel_dev);\n\tfree_netdev(sitn->fb_tunnel_dev);\nerr_alloc_dev:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -40,6 +40,7 @@\n \n err_reg_dev:\n \tipip6_dev_free(sitn->fb_tunnel_dev);\n+\tfree_netdev(sitn->fb_tunnel_dev);\n err_alloc_dev:\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tfree_netdev(sitn->fb_tunnel_dev);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "In the Linux kernel before 5.0, a memory leak exists in sit_init_net() in net/ipv6/sit.c when register_netdev() fails to register sitn->fb_tunnel_dev, which may cause denial of service, aka CID-07f12b26e21a.",
        "id": 2058
    },
    {
        "cve_id": "CVE-2019-19059",
        "code_before_change": "int iwl_pcie_ctxt_info_gen3_init(struct iwl_trans *trans,\n\t\t\t\t const struct fw_img *fw)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_context_info_gen3 *ctxt_info_gen3;\n\tstruct iwl_prph_scratch *prph_scratch;\n\tstruct iwl_prph_scratch_ctrl_cfg *prph_sc_ctrl;\n\tstruct iwl_prph_info *prph_info;\n\tvoid *iml_img;\n\tu32 control_flags = 0;\n\tint ret;\n\tint cmdq_size = max_t(u32, IWL_CMD_QUEUE_SIZE,\n\t\t\t      trans->cfg->min_txq_size);\n\n\t/* Allocate prph scratch */\n\tprph_scratch = dma_alloc_coherent(trans->dev, sizeof(*prph_scratch),\n\t\t\t\t\t  &trans_pcie->prph_scratch_dma_addr,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!prph_scratch)\n\t\treturn -ENOMEM;\n\n\tprph_sc_ctrl = &prph_scratch->ctrl_cfg;\n\n\tprph_sc_ctrl->version.version = 0;\n\tprph_sc_ctrl->version.mac_id =\n\t\tcpu_to_le16((u16)iwl_read32(trans, CSR_HW_REV));\n\tprph_sc_ctrl->version.size = cpu_to_le16(sizeof(*prph_scratch) / 4);\n\n\tcontrol_flags = IWL_PRPH_SCRATCH_RB_SIZE_4K |\n\t\t\tIWL_PRPH_SCRATCH_MTR_MODE |\n\t\t\t(IWL_PRPH_MTR_FORMAT_256B &\n\t\t\t IWL_PRPH_SCRATCH_MTR_FORMAT) |\n\t\t\tIWL_PRPH_SCRATCH_EARLY_DEBUG_EN |\n\t\t\tIWL_PRPH_SCRATCH_EDBG_DEST_DRAM;\n\tprph_sc_ctrl->control.control_flags = cpu_to_le32(control_flags);\n\n\t/* initialize RX default queue */\n\tprph_sc_ctrl->rbd_cfg.free_rbd_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->bd_dma);\n\n\t/* Configure debug, for integration */\n\tif (!iwl_trans_dbg_ini_valid(trans))\n\t\tiwl_pcie_alloc_fw_monitor(trans, 0);\n\tif (trans->dbg.num_blocks) {\n\t\tprph_sc_ctrl->hwm_cfg.hwm_base_addr =\n\t\t\tcpu_to_le64(trans->dbg.fw_mon[0].physical);\n\t\tprph_sc_ctrl->hwm_cfg.hwm_size =\n\t\t\tcpu_to_le32(trans->dbg.fw_mon[0].size);\n\t}\n\n\t/* allocate ucode sections in dram and set addresses */\n\tret = iwl_pcie_init_fw_sec(trans, fw, &prph_scratch->dram);\n\tif (ret) {\n\t\tdma_free_coherent(trans->dev,\n\t\t\t\t  sizeof(*prph_scratch),\n\t\t\t\t  prph_scratch,\n\t\t\t\t  trans_pcie->prph_scratch_dma_addr);\n\t\treturn ret;\n\t}\n\n\t/* Allocate prph information\n\t * currently we don't assign to the prph info anything, but it would get\n\t * assigned later */\n\tprph_info = dma_alloc_coherent(trans->dev, sizeof(*prph_info),\n\t\t\t\t       &trans_pcie->prph_info_dma_addr,\n\t\t\t\t       GFP_KERNEL);\n\tif (!prph_info)\n\t\treturn -ENOMEM;\n\n\t/* Allocate context info */\n\tctxt_info_gen3 = dma_alloc_coherent(trans->dev,\n\t\t\t\t\t    sizeof(*ctxt_info_gen3),\n\t\t\t\t\t    &trans_pcie->ctxt_info_dma_addr,\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!ctxt_info_gen3)\n\t\treturn -ENOMEM;\n\n\tctxt_info_gen3->prph_info_base_addr =\n\t\tcpu_to_le64(trans_pcie->prph_info_dma_addr);\n\tctxt_info_gen3->prph_scratch_base_addr =\n\t\tcpu_to_le64(trans_pcie->prph_scratch_dma_addr);\n\tctxt_info_gen3->prph_scratch_size =\n\t\tcpu_to_le32(sizeof(*prph_scratch));\n\tctxt_info_gen3->cr_head_idx_arr_base_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->rb_stts_dma);\n\tctxt_info_gen3->tr_tail_idx_arr_base_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->tr_tail_dma);\n\tctxt_info_gen3->cr_tail_idx_arr_base_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->cr_tail_dma);\n\tctxt_info_gen3->cr_idx_arr_size =\n\t\tcpu_to_le16(IWL_NUM_OF_COMPLETION_RINGS);\n\tctxt_info_gen3->tr_idx_arr_size =\n\t\tcpu_to_le16(IWL_NUM_OF_TRANSFER_RINGS);\n\tctxt_info_gen3->mtr_base_addr =\n\t\tcpu_to_le64(trans_pcie->txq[trans_pcie->cmd_queue]->dma_addr);\n\tctxt_info_gen3->mcr_base_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->used_bd_dma);\n\tctxt_info_gen3->mtr_size =\n\t\tcpu_to_le16(TFD_QUEUE_CB_SIZE(cmdq_size));\n\tctxt_info_gen3->mcr_size =\n\t\tcpu_to_le16(RX_QUEUE_CB_SIZE(MQ_RX_TABLE_SIZE));\n\n\ttrans_pcie->ctxt_info_gen3 = ctxt_info_gen3;\n\ttrans_pcie->prph_info = prph_info;\n\ttrans_pcie->prph_scratch = prph_scratch;\n\n\t/* Allocate IML */\n\timl_img = dma_alloc_coherent(trans->dev, trans->iml_len,\n\t\t\t\t     &trans_pcie->iml_dma_addr, GFP_KERNEL);\n\tif (!iml_img)\n\t\treturn -ENOMEM;\n\n\tmemcpy(iml_img, trans->iml, trans->iml_len);\n\n\tiwl_enable_fw_load_int_ctx_info(trans);\n\n\t/* kick FW self load */\n\tiwl_write64(trans, CSR_CTXT_INFO_ADDR,\n\t\t    trans_pcie->ctxt_info_dma_addr);\n\tiwl_write64(trans, CSR_IML_DATA_ADDR,\n\t\t    trans_pcie->iml_dma_addr);\n\tiwl_write32(trans, CSR_IML_SIZE_ADDR, trans->iml_len);\n\n\tiwl_set_bit(trans, CSR_CTXT_INFO_BOOT_CTRL,\n\t\t    CSR_AUTO_FUNC_BOOT_ENA);\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210)\n\t\tiwl_write_umac_prph(trans, UREG_CPU_INIT_RUN, 1);\n\telse\n\t\tiwl_set_bit(trans, CSR_GP_CNTRL, CSR_AUTO_FUNC_INIT);\n\n\treturn 0;\n}",
        "code_after_change": "int iwl_pcie_ctxt_info_gen3_init(struct iwl_trans *trans,\n\t\t\t\t const struct fw_img *fw)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_context_info_gen3 *ctxt_info_gen3;\n\tstruct iwl_prph_scratch *prph_scratch;\n\tstruct iwl_prph_scratch_ctrl_cfg *prph_sc_ctrl;\n\tstruct iwl_prph_info *prph_info;\n\tvoid *iml_img;\n\tu32 control_flags = 0;\n\tint ret;\n\tint cmdq_size = max_t(u32, IWL_CMD_QUEUE_SIZE,\n\t\t\t      trans->cfg->min_txq_size);\n\n\t/* Allocate prph scratch */\n\tprph_scratch = dma_alloc_coherent(trans->dev, sizeof(*prph_scratch),\n\t\t\t\t\t  &trans_pcie->prph_scratch_dma_addr,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!prph_scratch)\n\t\treturn -ENOMEM;\n\n\tprph_sc_ctrl = &prph_scratch->ctrl_cfg;\n\n\tprph_sc_ctrl->version.version = 0;\n\tprph_sc_ctrl->version.mac_id =\n\t\tcpu_to_le16((u16)iwl_read32(trans, CSR_HW_REV));\n\tprph_sc_ctrl->version.size = cpu_to_le16(sizeof(*prph_scratch) / 4);\n\n\tcontrol_flags = IWL_PRPH_SCRATCH_RB_SIZE_4K |\n\t\t\tIWL_PRPH_SCRATCH_MTR_MODE |\n\t\t\t(IWL_PRPH_MTR_FORMAT_256B &\n\t\t\t IWL_PRPH_SCRATCH_MTR_FORMAT) |\n\t\t\tIWL_PRPH_SCRATCH_EARLY_DEBUG_EN |\n\t\t\tIWL_PRPH_SCRATCH_EDBG_DEST_DRAM;\n\tprph_sc_ctrl->control.control_flags = cpu_to_le32(control_flags);\n\n\t/* initialize RX default queue */\n\tprph_sc_ctrl->rbd_cfg.free_rbd_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->bd_dma);\n\n\t/* Configure debug, for integration */\n\tif (!iwl_trans_dbg_ini_valid(trans))\n\t\tiwl_pcie_alloc_fw_monitor(trans, 0);\n\tif (trans->dbg.num_blocks) {\n\t\tprph_sc_ctrl->hwm_cfg.hwm_base_addr =\n\t\t\tcpu_to_le64(trans->dbg.fw_mon[0].physical);\n\t\tprph_sc_ctrl->hwm_cfg.hwm_size =\n\t\t\tcpu_to_le32(trans->dbg.fw_mon[0].size);\n\t}\n\n\t/* allocate ucode sections in dram and set addresses */\n\tret = iwl_pcie_init_fw_sec(trans, fw, &prph_scratch->dram);\n\tif (ret)\n\t\tgoto err_free_prph_scratch;\n\n\n\t/* Allocate prph information\n\t * currently we don't assign to the prph info anything, but it would get\n\t * assigned later */\n\tprph_info = dma_alloc_coherent(trans->dev, sizeof(*prph_info),\n\t\t\t\t       &trans_pcie->prph_info_dma_addr,\n\t\t\t\t       GFP_KERNEL);\n\tif (!prph_info) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_prph_scratch;\n\t}\n\n\t/* Allocate context info */\n\tctxt_info_gen3 = dma_alloc_coherent(trans->dev,\n\t\t\t\t\t    sizeof(*ctxt_info_gen3),\n\t\t\t\t\t    &trans_pcie->ctxt_info_dma_addr,\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!ctxt_info_gen3) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_prph_info;\n\t}\n\n\tctxt_info_gen3->prph_info_base_addr =\n\t\tcpu_to_le64(trans_pcie->prph_info_dma_addr);\n\tctxt_info_gen3->prph_scratch_base_addr =\n\t\tcpu_to_le64(trans_pcie->prph_scratch_dma_addr);\n\tctxt_info_gen3->prph_scratch_size =\n\t\tcpu_to_le32(sizeof(*prph_scratch));\n\tctxt_info_gen3->cr_head_idx_arr_base_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->rb_stts_dma);\n\tctxt_info_gen3->tr_tail_idx_arr_base_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->tr_tail_dma);\n\tctxt_info_gen3->cr_tail_idx_arr_base_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->cr_tail_dma);\n\tctxt_info_gen3->cr_idx_arr_size =\n\t\tcpu_to_le16(IWL_NUM_OF_COMPLETION_RINGS);\n\tctxt_info_gen3->tr_idx_arr_size =\n\t\tcpu_to_le16(IWL_NUM_OF_TRANSFER_RINGS);\n\tctxt_info_gen3->mtr_base_addr =\n\t\tcpu_to_le64(trans_pcie->txq[trans_pcie->cmd_queue]->dma_addr);\n\tctxt_info_gen3->mcr_base_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->used_bd_dma);\n\tctxt_info_gen3->mtr_size =\n\t\tcpu_to_le16(TFD_QUEUE_CB_SIZE(cmdq_size));\n\tctxt_info_gen3->mcr_size =\n\t\tcpu_to_le16(RX_QUEUE_CB_SIZE(MQ_RX_TABLE_SIZE));\n\n\ttrans_pcie->ctxt_info_gen3 = ctxt_info_gen3;\n\ttrans_pcie->prph_info = prph_info;\n\ttrans_pcie->prph_scratch = prph_scratch;\n\n\t/* Allocate IML */\n\timl_img = dma_alloc_coherent(trans->dev, trans->iml_len,\n\t\t\t\t     &trans_pcie->iml_dma_addr, GFP_KERNEL);\n\tif (!iml_img)\n\t\treturn -ENOMEM;\n\n\tmemcpy(iml_img, trans->iml, trans->iml_len);\n\n\tiwl_enable_fw_load_int_ctx_info(trans);\n\n\t/* kick FW self load */\n\tiwl_write64(trans, CSR_CTXT_INFO_ADDR,\n\t\t    trans_pcie->ctxt_info_dma_addr);\n\tiwl_write64(trans, CSR_IML_DATA_ADDR,\n\t\t    trans_pcie->iml_dma_addr);\n\tiwl_write32(trans, CSR_IML_SIZE_ADDR, trans->iml_len);\n\n\tiwl_set_bit(trans, CSR_CTXT_INFO_BOOT_CTRL,\n\t\t    CSR_AUTO_FUNC_BOOT_ENA);\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210)\n\t\tiwl_write_umac_prph(trans, UREG_CPU_INIT_RUN, 1);\n\telse\n\t\tiwl_set_bit(trans, CSR_GP_CNTRL, CSR_AUTO_FUNC_INIT);\n\n\treturn 0;\n\nerr_free_prph_info:\n\tdma_free_coherent(trans->dev,\n\t\t\t  sizeof(*prph_info),\n\t\t\tprph_info,\n\t\t\ttrans_pcie->prph_info_dma_addr);\n\nerr_free_prph_scratch:\n\tdma_free_coherent(trans->dev,\n\t\t\t  sizeof(*prph_scratch),\n\t\t\tprph_scratch,\n\t\t\ttrans_pcie->prph_scratch_dma_addr);\n\treturn ret;\n\n}",
        "patch": "--- code before\n+++ code after\n@@ -50,13 +50,9 @@\n \n \t/* allocate ucode sections in dram and set addresses */\n \tret = iwl_pcie_init_fw_sec(trans, fw, &prph_scratch->dram);\n-\tif (ret) {\n-\t\tdma_free_coherent(trans->dev,\n-\t\t\t\t  sizeof(*prph_scratch),\n-\t\t\t\t  prph_scratch,\n-\t\t\t\t  trans_pcie->prph_scratch_dma_addr);\n-\t\treturn ret;\n-\t}\n+\tif (ret)\n+\t\tgoto err_free_prph_scratch;\n+\n \n \t/* Allocate prph information\n \t * currently we don't assign to the prph info anything, but it would get\n@@ -64,16 +60,20 @@\n \tprph_info = dma_alloc_coherent(trans->dev, sizeof(*prph_info),\n \t\t\t\t       &trans_pcie->prph_info_dma_addr,\n \t\t\t\t       GFP_KERNEL);\n-\tif (!prph_info)\n-\t\treturn -ENOMEM;\n+\tif (!prph_info) {\n+\t\tret = -ENOMEM;\n+\t\tgoto err_free_prph_scratch;\n+\t}\n \n \t/* Allocate context info */\n \tctxt_info_gen3 = dma_alloc_coherent(trans->dev,\n \t\t\t\t\t    sizeof(*ctxt_info_gen3),\n \t\t\t\t\t    &trans_pcie->ctxt_info_dma_addr,\n \t\t\t\t\t    GFP_KERNEL);\n-\tif (!ctxt_info_gen3)\n-\t\treturn -ENOMEM;\n+\tif (!ctxt_info_gen3) {\n+\t\tret = -ENOMEM;\n+\t\tgoto err_free_prph_info;\n+\t}\n \n \tctxt_info_gen3->prph_info_base_addr =\n \t\tcpu_to_le64(trans_pcie->prph_info_dma_addr);\n@@ -129,4 +129,18 @@\n \t\tiwl_set_bit(trans, CSR_GP_CNTRL, CSR_AUTO_FUNC_INIT);\n \n \treturn 0;\n+\n+err_free_prph_info:\n+\tdma_free_coherent(trans->dev,\n+\t\t\t  sizeof(*prph_info),\n+\t\t\tprph_info,\n+\t\t\ttrans_pcie->prph_info_dma_addr);\n+\n+err_free_prph_scratch:\n+\tdma_free_coherent(trans->dev,\n+\t\t\t  sizeof(*prph_scratch),\n+\t\t\tprph_scratch,\n+\t\t\ttrans_pcie->prph_scratch_dma_addr);\n+\treturn ret;\n+\n }",
        "function_modified_lines": {
            "added": [
                "\tif (ret)",
                "\t\tgoto err_free_prph_scratch;",
                "",
                "\tif (!prph_info) {",
                "\t\tret = -ENOMEM;",
                "\t\tgoto err_free_prph_scratch;",
                "\t}",
                "\tif (!ctxt_info_gen3) {",
                "\t\tret = -ENOMEM;",
                "\t\tgoto err_free_prph_info;",
                "\t}",
                "",
                "err_free_prph_info:",
                "\tdma_free_coherent(trans->dev,",
                "\t\t\t  sizeof(*prph_info),",
                "\t\t\tprph_info,",
                "\t\t\ttrans_pcie->prph_info_dma_addr);",
                "",
                "err_free_prph_scratch:",
                "\tdma_free_coherent(trans->dev,",
                "\t\t\t  sizeof(*prph_scratch),",
                "\t\t\tprph_scratch,",
                "\t\t\ttrans_pcie->prph_scratch_dma_addr);",
                "\treturn ret;",
                ""
            ],
            "deleted": [
                "\tif (ret) {",
                "\t\tdma_free_coherent(trans->dev,",
                "\t\t\t\t  sizeof(*prph_scratch),",
                "\t\t\t\t  prph_scratch,",
                "\t\t\t\t  trans_pcie->prph_scratch_dma_addr);",
                "\t\treturn ret;",
                "\t}",
                "\tif (!prph_info)",
                "\t\treturn -ENOMEM;",
                "\tif (!ctxt_info_gen3)",
                "\t\treturn -ENOMEM;"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Multiple memory leaks in the iwl_pcie_ctxt_info_gen3_init() function in drivers/net/wireless/intel/iwlwifi/pcie/ctxt-info-gen3.c in the Linux kernel through 5.3.11 allow attackers to cause a denial of service (memory consumption) by triggering iwl_pcie_init_fw_sec() or dma_alloc_coherent() failures, aka CID-0f4f199443fa.",
        "id": 2140
    },
    {
        "cve_id": "CVE-2019-19083",
        "code_before_change": "struct clock_source *dce110_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "code_after_change": "struct clock_source *dce110_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \t\treturn &clk_src->base;\n \t}\n \n+\tkfree(clk_src);\n \tBREAK_TO_DEBUGGER();\n \treturn NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\tkfree(clk_src);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "Memory leaks in *clock_source_create() functions under drivers/gpu/drm/amd/display/dc in the Linux kernel before 5.3.8 allow attackers to cause a denial of service (memory consumption). This affects the dce112_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce112/dce112_resource.c, the dce100_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce100/dce100_resource.c, the dcn10_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dcn10/dcn10_resource.c, the dcn20_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dcn20/dcn20_resource.c, the dce120_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce120/dce120_resource.c, the dce110_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce110/dce110_resource.c, and the dce80_clock_source_create() function in drivers/gpu/drm/amd/display/dc/dce80/dce80_resource.c, aka CID-055e547478a1.",
        "id": 2171
    },
    {
        "cve_id": "CVE-2021-45480",
        "code_before_change": "static struct rds_connection *__rds_conn_create(struct net *net,\n\t\t\t\t\t\tconst struct in6_addr *laddr,\n\t\t\t\t\t\tconst struct in6_addr *faddr,\n\t\t\t\t\t\tstruct rds_transport *trans,\n\t\t\t\t\t\tgfp_t gfp, u8 tos,\n\t\t\t\t\t\tint is_outgoing,\n\t\t\t\t\t\tint dev_if)\n{\n\tstruct rds_connection *conn, *parent = NULL;\n\tstruct hlist_head *head = rds_conn_bucket(laddr, faddr);\n\tstruct rds_transport *loop_trans;\n\tunsigned long flags;\n\tint ret, i;\n\tint npaths = (trans->t_mp_capable ? RDS_MPATH_WORKERS : 1);\n\n\trcu_read_lock();\n\tconn = rds_conn_lookup(net, head, laddr, faddr, trans, tos, dev_if);\n\tif (conn &&\n\t    conn->c_loopback &&\n\t    conn->c_trans != &rds_loop_transport &&\n\t    ipv6_addr_equal(laddr, faddr) &&\n\t    !is_outgoing) {\n\t\t/* This is a looped back IB connection, and we're\n\t\t * called by the code handling the incoming connect.\n\t\t * We need a second connection object into which we\n\t\t * can stick the other QP. */\n\t\tparent = conn;\n\t\tconn = parent->c_passive;\n\t}\n\trcu_read_unlock();\n\tif (conn)\n\t\tgoto out;\n\n\tconn = kmem_cache_zalloc(rds_conn_slab, gfp);\n\tif (!conn) {\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\tconn->c_path = kcalloc(npaths, sizeof(struct rds_conn_path), gfp);\n\tif (!conn->c_path) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tINIT_HLIST_NODE(&conn->c_hash_node);\n\tconn->c_laddr = *laddr;\n\tconn->c_isv6 = !ipv6_addr_v4mapped(laddr);\n\tconn->c_faddr = *faddr;\n\tconn->c_dev_if = dev_if;\n\tconn->c_tos = tos;\n\n#if IS_ENABLED(CONFIG_IPV6)\n\t/* If the local address is link local, set c_bound_if to be the\n\t * index used for this connection.  Otherwise, set it to 0 as\n\t * the socket is not bound to an interface.  c_bound_if is used\n\t * to look up a socket when a packet is received\n\t */\n\tif (ipv6_addr_type(laddr) & IPV6_ADDR_LINKLOCAL)\n\t\tconn->c_bound_if = dev_if;\n\telse\n#endif\n\t\tconn->c_bound_if = 0;\n\n\trds_conn_net_set(conn, net);\n\n\tret = rds_cong_get_maps(conn);\n\tif (ret) {\n\t\tkfree(conn->c_path);\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This is where a connection becomes loopback.  If *any* RDS sockets\n\t * can bind to the destination address then we'd rather the messages\n\t * flow through loopback rather than either transport.\n\t */\n\tloop_trans = rds_trans_get_preferred(net, faddr, conn->c_dev_if);\n\tif (loop_trans) {\n\t\trds_trans_put(loop_trans);\n\t\tconn->c_loopback = 1;\n\t\tif (trans->t_prefer_loopback) {\n\t\t\tif (likely(is_outgoing)) {\n\t\t\t\t/* \"outgoing\" connection to local address.\n\t\t\t\t * Protocol says it wants the connection\n\t\t\t\t * handled by the loopback transport.\n\t\t\t\t * This is what TCP does.\n\t\t\t\t */\n\t\t\t\ttrans = &rds_loop_transport;\n\t\t\t} else {\n\t\t\t\t/* No transport currently in use\n\t\t\t\t * should end up here, but if it\n\t\t\t\t * does, reset/destroy the connection.\n\t\t\t\t */\n\t\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\t\tconn = ERR_PTR(-EOPNOTSUPP);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tconn->c_trans = trans;\n\n\tinit_waitqueue_head(&conn->c_hs_waitq);\n\tfor (i = 0; i < npaths; i++) {\n\t\t__rds_conn_path_init(conn, &conn->c_path[i],\n\t\t\t\t     is_outgoing);\n\t\tconn->c_path[i].cp_index = i;\n\t}\n\trcu_read_lock();\n\tif (rds_destroy_pending(conn))\n\t\tret = -ENETDOWN;\n\telse\n\t\tret = trans->conn_alloc(conn, GFP_ATOMIC);\n\tif (ret) {\n\t\trcu_read_unlock();\n\t\tkfree(conn->c_path);\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"allocated conn %p for %pI6c -> %pI6c over %s %s\\n\",\n\t\t conn, laddr, faddr,\n\t\t strnlen(trans->t_name, sizeof(trans->t_name)) ?\n\t\t trans->t_name : \"[unknown]\", is_outgoing ? \"(outgoing)\" : \"\");\n\n\t/*\n\t * Since we ran without holding the conn lock, someone could\n\t * have created the same conn (either normal or passive) in the\n\t * interim. We check while holding the lock. If we won, we complete\n\t * init and return our conn. If we lost, we rollback and return the\n\t * other one.\n\t */\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\tif (parent) {\n\t\t/* Creating passive conn */\n\t\tif (parent->c_passive) {\n\t\t\ttrans->conn_free(conn->c_path[0].cp_transport_data);\n\t\t\tkfree(conn->c_path);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = parent->c_passive;\n\t\t} else {\n\t\t\tparent->c_passive = conn;\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t} else {\n\t\t/* Creating normal conn */\n\t\tstruct rds_connection *found;\n\n\t\tfound = rds_conn_lookup(net, head, laddr, faddr, trans,\n\t\t\t\t\ttos, dev_if);\n\t\tif (found) {\n\t\t\tstruct rds_conn_path *cp;\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < npaths; i++) {\n\t\t\t\tcp = &conn->c_path[i];\n\t\t\t\t/* The ->conn_alloc invocation may have\n\t\t\t\t * allocated resource for all paths, so all\n\t\t\t\t * of them may have to be freed here.\n\t\t\t\t */\n\t\t\t\tif (cp->cp_transport_data)\n\t\t\t\t\ttrans->conn_free(cp->cp_transport_data);\n\t\t\t}\n\t\t\tkfree(conn->c_path);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = found;\n\t\t} else {\n\t\t\tconn->c_my_gen_num = rds_gen_num;\n\t\t\tconn->c_peer_gen_num = 0;\n\t\t\thlist_add_head_rcu(&conn->c_hash_node, head);\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n\trcu_read_unlock();\n\nout:\n\treturn conn;\n}",
        "code_after_change": "static struct rds_connection *__rds_conn_create(struct net *net,\n\t\t\t\t\t\tconst struct in6_addr *laddr,\n\t\t\t\t\t\tconst struct in6_addr *faddr,\n\t\t\t\t\t\tstruct rds_transport *trans,\n\t\t\t\t\t\tgfp_t gfp, u8 tos,\n\t\t\t\t\t\tint is_outgoing,\n\t\t\t\t\t\tint dev_if)\n{\n\tstruct rds_connection *conn, *parent = NULL;\n\tstruct hlist_head *head = rds_conn_bucket(laddr, faddr);\n\tstruct rds_transport *loop_trans;\n\tunsigned long flags;\n\tint ret, i;\n\tint npaths = (trans->t_mp_capable ? RDS_MPATH_WORKERS : 1);\n\n\trcu_read_lock();\n\tconn = rds_conn_lookup(net, head, laddr, faddr, trans, tos, dev_if);\n\tif (conn &&\n\t    conn->c_loopback &&\n\t    conn->c_trans != &rds_loop_transport &&\n\t    ipv6_addr_equal(laddr, faddr) &&\n\t    !is_outgoing) {\n\t\t/* This is a looped back IB connection, and we're\n\t\t * called by the code handling the incoming connect.\n\t\t * We need a second connection object into which we\n\t\t * can stick the other QP. */\n\t\tparent = conn;\n\t\tconn = parent->c_passive;\n\t}\n\trcu_read_unlock();\n\tif (conn)\n\t\tgoto out;\n\n\tconn = kmem_cache_zalloc(rds_conn_slab, gfp);\n\tif (!conn) {\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\tconn->c_path = kcalloc(npaths, sizeof(struct rds_conn_path), gfp);\n\tif (!conn->c_path) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tINIT_HLIST_NODE(&conn->c_hash_node);\n\tconn->c_laddr = *laddr;\n\tconn->c_isv6 = !ipv6_addr_v4mapped(laddr);\n\tconn->c_faddr = *faddr;\n\tconn->c_dev_if = dev_if;\n\tconn->c_tos = tos;\n\n#if IS_ENABLED(CONFIG_IPV6)\n\t/* If the local address is link local, set c_bound_if to be the\n\t * index used for this connection.  Otherwise, set it to 0 as\n\t * the socket is not bound to an interface.  c_bound_if is used\n\t * to look up a socket when a packet is received\n\t */\n\tif (ipv6_addr_type(laddr) & IPV6_ADDR_LINKLOCAL)\n\t\tconn->c_bound_if = dev_if;\n\telse\n#endif\n\t\tconn->c_bound_if = 0;\n\n\trds_conn_net_set(conn, net);\n\n\tret = rds_cong_get_maps(conn);\n\tif (ret) {\n\t\tkfree(conn->c_path);\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This is where a connection becomes loopback.  If *any* RDS sockets\n\t * can bind to the destination address then we'd rather the messages\n\t * flow through loopback rather than either transport.\n\t */\n\tloop_trans = rds_trans_get_preferred(net, faddr, conn->c_dev_if);\n\tif (loop_trans) {\n\t\trds_trans_put(loop_trans);\n\t\tconn->c_loopback = 1;\n\t\tif (trans->t_prefer_loopback) {\n\t\t\tif (likely(is_outgoing)) {\n\t\t\t\t/* \"outgoing\" connection to local address.\n\t\t\t\t * Protocol says it wants the connection\n\t\t\t\t * handled by the loopback transport.\n\t\t\t\t * This is what TCP does.\n\t\t\t\t */\n\t\t\t\ttrans = &rds_loop_transport;\n\t\t\t} else {\n\t\t\t\t/* No transport currently in use\n\t\t\t\t * should end up here, but if it\n\t\t\t\t * does, reset/destroy the connection.\n\t\t\t\t */\n\t\t\t\tkfree(conn->c_path);\n\t\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\t\tconn = ERR_PTR(-EOPNOTSUPP);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tconn->c_trans = trans;\n\n\tinit_waitqueue_head(&conn->c_hs_waitq);\n\tfor (i = 0; i < npaths; i++) {\n\t\t__rds_conn_path_init(conn, &conn->c_path[i],\n\t\t\t\t     is_outgoing);\n\t\tconn->c_path[i].cp_index = i;\n\t}\n\trcu_read_lock();\n\tif (rds_destroy_pending(conn))\n\t\tret = -ENETDOWN;\n\telse\n\t\tret = trans->conn_alloc(conn, GFP_ATOMIC);\n\tif (ret) {\n\t\trcu_read_unlock();\n\t\tkfree(conn->c_path);\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"allocated conn %p for %pI6c -> %pI6c over %s %s\\n\",\n\t\t conn, laddr, faddr,\n\t\t strnlen(trans->t_name, sizeof(trans->t_name)) ?\n\t\t trans->t_name : \"[unknown]\", is_outgoing ? \"(outgoing)\" : \"\");\n\n\t/*\n\t * Since we ran without holding the conn lock, someone could\n\t * have created the same conn (either normal or passive) in the\n\t * interim. We check while holding the lock. If we won, we complete\n\t * init and return our conn. If we lost, we rollback and return the\n\t * other one.\n\t */\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\tif (parent) {\n\t\t/* Creating passive conn */\n\t\tif (parent->c_passive) {\n\t\t\ttrans->conn_free(conn->c_path[0].cp_transport_data);\n\t\t\tkfree(conn->c_path);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = parent->c_passive;\n\t\t} else {\n\t\t\tparent->c_passive = conn;\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t} else {\n\t\t/* Creating normal conn */\n\t\tstruct rds_connection *found;\n\n\t\tfound = rds_conn_lookup(net, head, laddr, faddr, trans,\n\t\t\t\t\ttos, dev_if);\n\t\tif (found) {\n\t\t\tstruct rds_conn_path *cp;\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < npaths; i++) {\n\t\t\t\tcp = &conn->c_path[i];\n\t\t\t\t/* The ->conn_alloc invocation may have\n\t\t\t\t * allocated resource for all paths, so all\n\t\t\t\t * of them may have to be freed here.\n\t\t\t\t */\n\t\t\t\tif (cp->cp_transport_data)\n\t\t\t\t\ttrans->conn_free(cp->cp_transport_data);\n\t\t\t}\n\t\t\tkfree(conn->c_path);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = found;\n\t\t} else {\n\t\t\tconn->c_my_gen_num = rds_gen_num;\n\t\t\tconn->c_peer_gen_num = 0;\n\t\t\thlist_add_head_rcu(&conn->c_hash_node, head);\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n\trcu_read_unlock();\n\nout:\n\treturn conn;\n}",
        "patch": "--- code before\n+++ code after\n@@ -94,6 +94,7 @@\n \t\t\t\t * should end up here, but if it\n \t\t\t\t * does, reset/destroy the connection.\n \t\t\t\t */\n+\t\t\t\tkfree(conn->c_path);\n \t\t\t\tkmem_cache_free(rds_conn_slab, conn);\n \t\t\t\tconn = ERR_PTR(-EOPNOTSUPP);\n \t\t\t\tgoto out;",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tkfree(conn->c_path);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.15.11. There is a memory leak in the __rds_conn_create() function in net/rds/connection.c in a certain combination of circumstances.",
        "id": 3179
    },
    {
        "cve_id": "CVE-2021-4002",
        "code_before_change": "static void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,\n\t\t\t\t   unsigned long start, unsigned long end,\n\t\t\t\t   struct page *ref_page)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\tstruct mmu_notifier_range range;\n\n\tWARN_ON(!is_vm_hugetlb_page(vma));\n\tBUG_ON(start & ~huge_page_mask(h));\n\tBUG_ON(end & ~huge_page_mask(h));\n\n\t/*\n\t * This is a hugetlb vma, all the pte entries should point\n\t * to huge page.\n\t */\n\ttlb_change_page_size(tlb, sz);\n\ttlb_start_vma(tlb, vma);\n\n\t/*\n\t * If sharing possible, alert mmu notifiers of worst case.\n\t */\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, vma, mm, start,\n\t\t\t\tend);\n\tadjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);\n\tmmu_notifier_invalidate_range_start(&range);\n\taddress = start;\n\tfor (; address < end; address += sz) {\n\t\tptep = huge_pte_offset(mm, address, sz);\n\t\tif (!ptep)\n\t\t\tcontinue;\n\n\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\tif (huge_pmd_unshare(mm, vma, &address, ptep)) {\n\t\t\tspin_unlock(ptl);\n\t\t\t/*\n\t\t\t * We just unmapped a page of PMDs by clearing a PUD.\n\t\t\t * The caller's TLB flush range should cover this area.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (huge_pte_none(pte)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Migrating hugepage or HWPoisoned hugepage is already\n\t\t * unmapped and its refcount is dropped, so just clear pte here.\n\t\t */\n\t\tif (unlikely(!pte_present(pte))) {\n\t\t\thuge_pte_clear(mm, address, ptep, sz);\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpage = pte_page(pte);\n\t\t/*\n\t\t * If a reference page is supplied, it is because a specific\n\t\t * page is being unmapped, not a range. Ensure the page we\n\t\t * are about to unmap is the actual page of interest.\n\t\t */\n\t\tif (ref_page) {\n\t\t\tif (page != ref_page) {\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Mark the VMA as having unmapped its page so that\n\t\t\t * future faults in this VMA will fail rather than\n\t\t\t * looking like data was lost\n\t\t\t */\n\t\t\tset_vma_resv_flags(vma, HPAGE_RESV_UNMAPPED);\n\t\t}\n\n\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\ttlb_remove_huge_tlb_entry(h, tlb, ptep, address);\n\t\tif (huge_pte_dirty(pte))\n\t\t\tset_page_dirty(page);\n\n\t\thugetlb_count_sub(pages_per_huge_page(h), mm);\n\t\tpage_remove_rmap(page, true);\n\n\t\tspin_unlock(ptl);\n\t\ttlb_remove_page_size(tlb, page, huge_page_size(h));\n\t\t/*\n\t\t * Bail out after unmapping reference page if supplied\n\t\t */\n\t\tif (ref_page)\n\t\t\tbreak;\n\t}\n\tmmu_notifier_invalidate_range_end(&range);\n\ttlb_end_vma(tlb, vma);\n}",
        "code_after_change": "static void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,\n\t\t\t\t   unsigned long start, unsigned long end,\n\t\t\t\t   struct page *ref_page)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\tstruct mmu_notifier_range range;\n\tbool force_flush = false;\n\n\tWARN_ON(!is_vm_hugetlb_page(vma));\n\tBUG_ON(start & ~huge_page_mask(h));\n\tBUG_ON(end & ~huge_page_mask(h));\n\n\t/*\n\t * This is a hugetlb vma, all the pte entries should point\n\t * to huge page.\n\t */\n\ttlb_change_page_size(tlb, sz);\n\ttlb_start_vma(tlb, vma);\n\n\t/*\n\t * If sharing possible, alert mmu notifiers of worst case.\n\t */\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, vma, mm, start,\n\t\t\t\tend);\n\tadjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);\n\tmmu_notifier_invalidate_range_start(&range);\n\taddress = start;\n\tfor (; address < end; address += sz) {\n\t\tptep = huge_pte_offset(mm, address, sz);\n\t\tif (!ptep)\n\t\t\tcontinue;\n\n\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\tif (huge_pmd_unshare(mm, vma, &address, ptep)) {\n\t\t\tspin_unlock(ptl);\n\t\t\ttlb_flush_pmd_range(tlb, address & PUD_MASK, PUD_SIZE);\n\t\t\tforce_flush = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (huge_pte_none(pte)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Migrating hugepage or HWPoisoned hugepage is already\n\t\t * unmapped and its refcount is dropped, so just clear pte here.\n\t\t */\n\t\tif (unlikely(!pte_present(pte))) {\n\t\t\thuge_pte_clear(mm, address, ptep, sz);\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpage = pte_page(pte);\n\t\t/*\n\t\t * If a reference page is supplied, it is because a specific\n\t\t * page is being unmapped, not a range. Ensure the page we\n\t\t * are about to unmap is the actual page of interest.\n\t\t */\n\t\tif (ref_page) {\n\t\t\tif (page != ref_page) {\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Mark the VMA as having unmapped its page so that\n\t\t\t * future faults in this VMA will fail rather than\n\t\t\t * looking like data was lost\n\t\t\t */\n\t\t\tset_vma_resv_flags(vma, HPAGE_RESV_UNMAPPED);\n\t\t}\n\n\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\ttlb_remove_huge_tlb_entry(h, tlb, ptep, address);\n\t\tif (huge_pte_dirty(pte))\n\t\t\tset_page_dirty(page);\n\n\t\thugetlb_count_sub(pages_per_huge_page(h), mm);\n\t\tpage_remove_rmap(page, true);\n\n\t\tspin_unlock(ptl);\n\t\ttlb_remove_page_size(tlb, page, huge_page_size(h));\n\t\t/*\n\t\t * Bail out after unmapping reference page if supplied\n\t\t */\n\t\tif (ref_page)\n\t\t\tbreak;\n\t}\n\tmmu_notifier_invalidate_range_end(&range);\n\ttlb_end_vma(tlb, vma);\n\n\t/*\n\t * If we unshared PMDs, the TLB flush was not recorded in mmu_gather. We\n\t * could defer the flush until now, since by holding i_mmap_rwsem we\n\t * guaranteed that the last refernece would not be dropped. But we must\n\t * do the flushing before we return, as otherwise i_mmap_rwsem will be\n\t * dropped and the last reference to the shared PMDs page might be\n\t * dropped as well.\n\t *\n\t * In theory we could defer the freeing of the PMD pages as well, but\n\t * huge_pmd_unshare() relies on the exact page_count for the PMD page to\n\t * detect sharing, so we cannot defer the release of the page either.\n\t * Instead, do flush now.\n\t */\n\tif (force_flush)\n\t\ttlb_flush_mmu_tlbonly(tlb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,7 @@\n \tstruct hstate *h = hstate_vma(vma);\n \tunsigned long sz = huge_page_size(h);\n \tstruct mmu_notifier_range range;\n+\tbool force_flush = false;\n \n \tWARN_ON(!is_vm_hugetlb_page(vma));\n \tBUG_ON(start & ~huge_page_mask(h));\n@@ -39,10 +40,8 @@\n \t\tptl = huge_pte_lock(h, mm, ptep);\n \t\tif (huge_pmd_unshare(mm, vma, &address, ptep)) {\n \t\t\tspin_unlock(ptl);\n-\t\t\t/*\n-\t\t\t * We just unmapped a page of PMDs by clearing a PUD.\n-\t\t\t * The caller's TLB flush range should cover this area.\n-\t\t\t */\n+\t\t\ttlb_flush_pmd_range(tlb, address & PUD_MASK, PUD_SIZE);\n+\t\t\tforce_flush = true;\n \t\t\tcontinue;\n \t\t}\n \n@@ -99,4 +98,20 @@\n \t}\n \tmmu_notifier_invalidate_range_end(&range);\n \ttlb_end_vma(tlb, vma);\n+\n+\t/*\n+\t * If we unshared PMDs, the TLB flush was not recorded in mmu_gather. We\n+\t * could defer the flush until now, since by holding i_mmap_rwsem we\n+\t * guaranteed that the last refernece would not be dropped. But we must\n+\t * do the flushing before we return, as otherwise i_mmap_rwsem will be\n+\t * dropped and the last reference to the shared PMDs page might be\n+\t * dropped as well.\n+\t *\n+\t * In theory we could defer the freeing of the PMD pages as well, but\n+\t * huge_pmd_unshare() relies on the exact page_count for the PMD page to\n+\t * detect sharing, so we cannot defer the release of the page either.\n+\t * Instead, do flush now.\n+\t */\n+\tif (force_flush)\n+\t\ttlb_flush_mmu_tlbonly(tlb);\n }",
        "function_modified_lines": {
            "added": [
                "\tbool force_flush = false;",
                "\t\t\ttlb_flush_pmd_range(tlb, address & PUD_MASK, PUD_SIZE);",
                "\t\t\tforce_flush = true;",
                "",
                "\t/*",
                "\t * If we unshared PMDs, the TLB flush was not recorded in mmu_gather. We",
                "\t * could defer the flush until now, since by holding i_mmap_rwsem we",
                "\t * guaranteed that the last refernece would not be dropped. But we must",
                "\t * do the flushing before we return, as otherwise i_mmap_rwsem will be",
                "\t * dropped and the last reference to the shared PMDs page might be",
                "\t * dropped as well.",
                "\t *",
                "\t * In theory we could defer the freeing of the PMD pages as well, but",
                "\t * huge_pmd_unshare() relies on the exact page_count for the PMD page to",
                "\t * detect sharing, so we cannot defer the release of the page either.",
                "\t * Instead, do flush now.",
                "\t */",
                "\tif (force_flush)",
                "\t\ttlb_flush_mmu_tlbonly(tlb);"
            ],
            "deleted": [
                "\t\t\t/*",
                "\t\t\t * We just unmapped a page of PMDs by clearing a PUD.",
                "\t\t\t * The caller's TLB flush range should cover this area.",
                "\t\t\t */"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak flaw in the Linux kernel's hugetlbfs memory usage was found in the way the user maps some regions of memory twice using shmget() which are aligned to PUD alignment with the fault of some of the memory pages. A local user could use this flaw to get unauthorized access to some data.",
        "id": 3121
    },
    {
        "cve_id": "CVE-2019-19054",
        "code_before_change": "int cx23888_ir_probe(struct cx23885_dev *dev)\n{\n\tstruct cx23888_ir_state *state;\n\tstruct v4l2_subdev *sd;\n\tstruct v4l2_subdev_ir_parameters default_params;\n\tint ret;\n\n\tstate = kzalloc(sizeof(struct cx23888_ir_state), GFP_KERNEL);\n\tif (state == NULL)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&state->rx_kfifo_lock);\n\tif (kfifo_alloc(&state->rx_kfifo, CX23888_IR_RX_KFIFO_SIZE, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tstate->dev = dev;\n\tsd = &state->sd;\n\n\tv4l2_subdev_init(sd, &cx23888_ir_controller_ops);\n\tv4l2_set_subdevdata(sd, state);\n\t/* FIXME - fix the formatting of dev->v4l2_dev.name and use it */\n\tsnprintf(sd->name, sizeof(sd->name), \"%s/888-ir\", dev->name);\n\tsd->grp_id = CX23885_HW_888_IR;\n\n\tret = v4l2_device_register_subdev(&dev->v4l2_dev, sd);\n\tif (ret == 0) {\n\t\t/*\n\t\t * Ensure no interrupts arrive from '888 specific conditions,\n\t\t * since we ignore them in this driver to have commonality with\n\t\t * similar IR controller cores.\n\t\t */\n\t\tcx23888_ir_write4(dev, CX23888_IR_IRQEN_REG, 0);\n\n\t\tmutex_init(&state->rx_params_lock);\n\t\tdefault_params = default_rx_params;\n\t\tv4l2_subdev_call(sd, ir, rx_s_parameters, &default_params);\n\n\t\tmutex_init(&state->tx_params_lock);\n\t\tdefault_params = default_tx_params;\n\t\tv4l2_subdev_call(sd, ir, tx_s_parameters, &default_params);\n\t} else {\n\t\tkfifo_free(&state->rx_kfifo);\n\t}\n\treturn ret;\n}",
        "code_after_change": "int cx23888_ir_probe(struct cx23885_dev *dev)\n{\n\tstruct cx23888_ir_state *state;\n\tstruct v4l2_subdev *sd;\n\tstruct v4l2_subdev_ir_parameters default_params;\n\tint ret;\n\n\tstate = kzalloc(sizeof(struct cx23888_ir_state), GFP_KERNEL);\n\tif (state == NULL)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&state->rx_kfifo_lock);\n\tif (kfifo_alloc(&state->rx_kfifo, CX23888_IR_RX_KFIFO_SIZE,\n\t\t\tGFP_KERNEL)) {\n\t\tkfree(state);\n\t\treturn -ENOMEM;\n\t}\n\n\tstate->dev = dev;\n\tsd = &state->sd;\n\n\tv4l2_subdev_init(sd, &cx23888_ir_controller_ops);\n\tv4l2_set_subdevdata(sd, state);\n\t/* FIXME - fix the formatting of dev->v4l2_dev.name and use it */\n\tsnprintf(sd->name, sizeof(sd->name), \"%s/888-ir\", dev->name);\n\tsd->grp_id = CX23885_HW_888_IR;\n\n\tret = v4l2_device_register_subdev(&dev->v4l2_dev, sd);\n\tif (ret == 0) {\n\t\t/*\n\t\t * Ensure no interrupts arrive from '888 specific conditions,\n\t\t * since we ignore them in this driver to have commonality with\n\t\t * similar IR controller cores.\n\t\t */\n\t\tcx23888_ir_write4(dev, CX23888_IR_IRQEN_REG, 0);\n\n\t\tmutex_init(&state->rx_params_lock);\n\t\tdefault_params = default_rx_params;\n\t\tv4l2_subdev_call(sd, ir, rx_s_parameters, &default_params);\n\n\t\tmutex_init(&state->tx_params_lock);\n\t\tdefault_params = default_tx_params;\n\t\tv4l2_subdev_call(sd, ir, tx_s_parameters, &default_params);\n\t} else {\n\t\tkfifo_free(&state->rx_kfifo);\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,8 +10,11 @@\n \t\treturn -ENOMEM;\n \n \tspin_lock_init(&state->rx_kfifo_lock);\n-\tif (kfifo_alloc(&state->rx_kfifo, CX23888_IR_RX_KFIFO_SIZE, GFP_KERNEL))\n+\tif (kfifo_alloc(&state->rx_kfifo, CX23888_IR_RX_KFIFO_SIZE,\n+\t\t\tGFP_KERNEL)) {\n+\t\tkfree(state);\n \t\treturn -ENOMEM;\n+\t}\n \n \tstate->dev = dev;\n \tsd = &state->sd;",
        "function_modified_lines": {
            "added": [
                "\tif (kfifo_alloc(&state->rx_kfifo, CX23888_IR_RX_KFIFO_SIZE,",
                "\t\t\tGFP_KERNEL)) {",
                "\t\tkfree(state);",
                "\t}"
            ],
            "deleted": [
                "\tif (kfifo_alloc(&state->rx_kfifo, CX23888_IR_RX_KFIFO_SIZE, GFP_KERNEL))"
            ]
        },
        "cwe": [
            "CWE-401"
        ],
        "cve_description": "A memory leak in the cx23888_ir_probe() function in drivers/media/pci/cx23885/cx23888-ir.c in the Linux kernel through 5.3.11 allows attackers to cause a denial of service (memory consumption) by triggering kfifo_alloc() failures, aka CID-a7b2df76b42b.",
        "id": 2135
    }
]