[
    {
        "cve_id": "CVE-2020-14381",
        "code_before_change": "static void drop_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr) {\n\t\t/* If we're here then we tried to put a key we failed to get */\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\n\tif (!IS_ENABLED(CONFIG_MMU))\n\t\treturn;\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tiput(key->shared.inode);\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tmmdrop(key->private.mm);\n\t\tbreak;\n\t}\n}",
        "code_after_change": "static void drop_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr) {\n\t\t/* If we're here then we tried to put a key we failed to get */\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\n\tif (!IS_ENABLED(CONFIG_MMU))\n\t\treturn;\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tmmdrop(key->private.mm);\n\t\tbreak;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,6 @@\n \n \tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n \tcase FUT_OFF_INODE:\n-\t\tiput(key->shared.inode);\n \t\tbreak;\n \tcase FUT_OFF_MMSHARED:\n \t\tmmdrop(key->private.mm);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tiput(key->shared.inode);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s futex implementation. This flaw allows a local attacker to corrupt system memory or escalate their privileges when creating a futex on a filesystem that is about to be unmounted. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability.",
        "id": 2521
    },
    {
        "cve_id": "CVE-2018-10876",
        "code_before_change": "static ext4_group_t ext4_has_uninit_itable(struct super_block *sb)\n{\n\text4_group_t group, ngroups = EXT4_SB(sb)->s_groups_count;\n\tstruct ext4_group_desc *gdp = NULL;\n\n\tfor (group = 0; group < ngroups; group++) {\n\t\tgdp = ext4_get_group_desc(sb, group, NULL);\n\t\tif (!gdp)\n\t\t\tcontinue;\n\n\t\tif (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n\t\t\tbreak;\n\t}\n\n\treturn group;\n}",
        "code_after_change": "static ext4_group_t ext4_has_uninit_itable(struct super_block *sb)\n{\n\text4_group_t group, ngroups = EXT4_SB(sb)->s_groups_count;\n\tstruct ext4_group_desc *gdp = NULL;\n\n\tif (!ext4_has_group_desc_csum(sb))\n\t\treturn ngroups;\n\n\tfor (group = 0; group < ngroups; group++) {\n\t\tgdp = ext4_get_group_desc(sb, group, NULL);\n\t\tif (!gdp)\n\t\t\tcontinue;\n\n\t\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED))\n\t\t\tcontinue;\n\t\tif (group != 0)\n\t\t\tbreak;\n\t\text4_error(sb, \"Inode table for bg 0 marked as \"\n\t\t\t   \"needing zeroing\");\n\t\tif (sb_rdonly(sb))\n\t\t\treturn ngroups;\n\t}\n\n\treturn group;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,14 +2,23 @@\n {\n \text4_group_t group, ngroups = EXT4_SB(sb)->s_groups_count;\n \tstruct ext4_group_desc *gdp = NULL;\n+\n+\tif (!ext4_has_group_desc_csum(sb))\n+\t\treturn ngroups;\n \n \tfor (group = 0; group < ngroups; group++) {\n \t\tgdp = ext4_get_group_desc(sb, group, NULL);\n \t\tif (!gdp)\n \t\t\tcontinue;\n \n-\t\tif (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n+\t\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED))\n+\t\t\tcontinue;\n+\t\tif (group != 0)\n \t\t\tbreak;\n+\t\text4_error(sb, \"Inode table for bg 0 marked as \"\n+\t\t\t   \"needing zeroing\");\n+\t\tif (sb_rdonly(sb))\n+\t\t\treturn ngroups;\n \t}\n \n \treturn group;",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (!ext4_has_group_desc_csum(sb))",
                "\t\treturn ngroups;",
                "\t\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED))",
                "\t\t\tcontinue;",
                "\t\tif (group != 0)",
                "\t\text4_error(sb, \"Inode table for bg 0 marked as \"",
                "\t\t\t   \"needing zeroing\");",
                "\t\tif (sb_rdonly(sb))",
                "\t\t\treturn ngroups;"
            ],
            "deleted": [
                "\t\tif (!(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in Linux kernel in the ext4 filesystem code. A use-after-free is possible in ext4_ext_remove_space() function when mounting and operating a crafted ext4 image.",
        "id": 1609
    },
    {
        "cve_id": "CVE-2016-9576",
        "code_before_change": "int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,\n\t\t\tstruct rq_map_data *map_data,\n\t\t\tconst struct iov_iter *iter, gfp_t gfp_mask)\n{\n\tbool copy = false;\n\tunsigned long align = q->dma_pad_mask | queue_dma_alignment(q);\n\tstruct bio *bio = NULL;\n\tstruct iov_iter i;\n\tint ret;\n\n\tif (map_data)\n\t\tcopy = true;\n\telse if (iov_iter_alignment(iter) & align)\n\t\tcopy = true;\n\telse if (queue_virt_boundary(q))\n\t\tcopy = queue_virt_boundary(q) & iov_iter_gap_alignment(iter);\n\n\ti = *iter;\n\tdo {\n\t\tret =__blk_rq_map_user_iov(rq, map_data, &i, gfp_mask, copy);\n\t\tif (ret)\n\t\t\tgoto unmap_rq;\n\t\tif (!bio)\n\t\t\tbio = rq->bio;\n\t} while (iov_iter_count(&i));\n\n\tif (!bio_flagged(bio, BIO_USER_MAPPED))\n\t\trq->cmd_flags |= REQ_COPY_USER;\n\treturn 0;\n\nunmap_rq:\n\t__blk_rq_unmap_user(bio);\n\trq->bio = NULL;\n\treturn -EINVAL;\n}",
        "code_after_change": "int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,\n\t\t\tstruct rq_map_data *map_data,\n\t\t\tconst struct iov_iter *iter, gfp_t gfp_mask)\n{\n\tbool copy = false;\n\tunsigned long align = q->dma_pad_mask | queue_dma_alignment(q);\n\tstruct bio *bio = NULL;\n\tstruct iov_iter i;\n\tint ret;\n\n\tif (!iter_is_iovec(iter))\n\t\tgoto fail;\n\n\tif (map_data)\n\t\tcopy = true;\n\telse if (iov_iter_alignment(iter) & align)\n\t\tcopy = true;\n\telse if (queue_virt_boundary(q))\n\t\tcopy = queue_virt_boundary(q) & iov_iter_gap_alignment(iter);\n\n\ti = *iter;\n\tdo {\n\t\tret =__blk_rq_map_user_iov(rq, map_data, &i, gfp_mask, copy);\n\t\tif (ret)\n\t\t\tgoto unmap_rq;\n\t\tif (!bio)\n\t\t\tbio = rq->bio;\n\t} while (iov_iter_count(&i));\n\n\tif (!bio_flagged(bio, BIO_USER_MAPPED))\n\t\trq->cmd_flags |= REQ_COPY_USER;\n\treturn 0;\n\nunmap_rq:\n\t__blk_rq_unmap_user(bio);\nfail:\n\trq->bio = NULL;\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,9 @@\n \tstruct bio *bio = NULL;\n \tstruct iov_iter i;\n \tint ret;\n+\n+\tif (!iter_is_iovec(iter))\n+\t\tgoto fail;\n \n \tif (map_data)\n \t\tcopy = true;\n@@ -30,6 +33,7 @@\n \n unmap_rq:\n \t__blk_rq_unmap_user(bio);\n+fail:\n \trq->bio = NULL;\n \treturn -EINVAL;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (!iter_is_iovec(iter))",
                "\t\tgoto fail;",
                "fail:"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The blk_rq_map_user_iov function in block/blk-map.c in the Linux kernel before 4.8.14 does not properly restrict the type of iterator, which allows local users to read or write to arbitrary kernel memory locations or cause a denial of service (use-after-free) by leveraging access to a /dev/sg device.",
        "id": 1146
    },
    {
        "cve_id": "CVE-2018-10876",
        "code_before_change": "struct inode *__ext4_new_inode(handle_t *handle, struct inode *dir,\n\t\t\t       umode_t mode, const struct qstr *qstr,\n\t\t\t       __u32 goal, uid_t *owner, __u32 i_flags,\n\t\t\t       int handle_type, unsigned int line_no,\n\t\t\t       int nblocks)\n{\n\tstruct super_block *sb;\n\tstruct buffer_head *inode_bitmap_bh = NULL;\n\tstruct buffer_head *group_desc_bh;\n\text4_group_t ngroups, group = 0;\n\tunsigned long ino = 0;\n\tstruct inode *inode;\n\tstruct ext4_group_desc *gdp = NULL;\n\tstruct ext4_inode_info *ei;\n\tstruct ext4_sb_info *sbi;\n\tint ret2, err;\n\tstruct inode *ret;\n\text4_group_t i;\n\text4_group_t flex_group;\n\tstruct ext4_group_info *grp;\n\tint encrypt = 0;\n\n\t/* Cannot create files in a deleted directory */\n\tif (!dir || !dir->i_nlink)\n\t\treturn ERR_PTR(-EPERM);\n\n\tsb = dir->i_sb;\n\tsbi = EXT4_SB(sb);\n\n\tif (unlikely(ext4_forced_shutdown(sbi)))\n\t\treturn ERR_PTR(-EIO);\n\n\tif ((ext4_encrypted_inode(dir) || DUMMY_ENCRYPTION_ENABLED(sbi)) &&\n\t    (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode)) &&\n\t    !(i_flags & EXT4_EA_INODE_FL)) {\n\t\terr = fscrypt_get_encryption_info(dir);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t\tif (!fscrypt_has_encryption_key(dir))\n\t\t\treturn ERR_PTR(-ENOKEY);\n\t\tencrypt = 1;\n\t}\n\n\tif (!handle && sbi->s_journal && !(i_flags & EXT4_EA_INODE_FL)) {\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\t\tstruct posix_acl *p = get_acl(dir, ACL_TYPE_DEFAULT);\n\n\t\tif (IS_ERR(p))\n\t\t\treturn ERR_CAST(p);\n\t\tif (p) {\n\t\t\tint acl_size = p->a_count * sizeof(ext4_acl_entry);\n\n\t\t\tnblocks += (S_ISDIR(mode) ? 2 : 1) *\n\t\t\t\t__ext4_xattr_set_credits(sb, NULL /* inode */,\n\t\t\t\t\tNULL /* block_bh */, acl_size,\n\t\t\t\t\ttrue /* is_create */);\n\t\t\tposix_acl_release(p);\n\t\t}\n#endif\n\n#ifdef CONFIG_SECURITY\n\t\t{\n\t\t\tint num_security_xattrs = 1;\n\n#ifdef CONFIG_INTEGRITY\n\t\t\tnum_security_xattrs++;\n#endif\n\t\t\t/*\n\t\t\t * We assume that security xattrs are never\n\t\t\t * more than 1k.  In practice they are under\n\t\t\t * 128 bytes.\n\t\t\t */\n\t\t\tnblocks += num_security_xattrs *\n\t\t\t\t__ext4_xattr_set_credits(sb, NULL /* inode */,\n\t\t\t\t\tNULL /* block_bh */, 1024,\n\t\t\t\t\ttrue /* is_create */);\n\t\t}\n#endif\n\t\tif (encrypt)\n\t\t\tnblocks += __ext4_xattr_set_credits(sb,\n\t\t\t\t\tNULL /* inode */, NULL /* block_bh */,\n\t\t\t\t\tFSCRYPT_SET_CONTEXT_MAX_SIZE,\n\t\t\t\t\ttrue /* is_create */);\n\t}\n\n\tngroups = ext4_get_groups_count(sb);\n\ttrace_ext4_request_inode(dir, mode);\n\tinode = new_inode(sb);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tei = EXT4_I(inode);\n\n\t/*\n\t * Initialize owners and quota early so that we don't have to account\n\t * for quota initialization worst case in standard inode creating\n\t * transaction\n\t */\n\tif (owner) {\n\t\tinode->i_mode = mode;\n\t\ti_uid_write(inode, owner[0]);\n\t\ti_gid_write(inode, owner[1]);\n\t} else if (test_opt(sb, GRPID)) {\n\t\tinode->i_mode = mode;\n\t\tinode->i_uid = current_fsuid();\n\t\tinode->i_gid = dir->i_gid;\n\t} else\n\t\tinode_init_owner(inode, dir, mode);\n\n\tif (ext4_has_feature_project(sb) &&\n\t    ext4_test_inode_flag(dir, EXT4_INODE_PROJINHERIT))\n\t\tei->i_projid = EXT4_I(dir)->i_projid;\n\telse\n\t\tei->i_projid = make_kprojid(&init_user_ns, EXT4_DEF_PROJID);\n\n\terr = dquot_initialize(inode);\n\tif (err)\n\t\tgoto out;\n\n\tif (!goal)\n\t\tgoal = sbi->s_inode_goal;\n\n\tif (goal && goal <= le32_to_cpu(sbi->s_es->s_inodes_count)) {\n\t\tgroup = (goal - 1) / EXT4_INODES_PER_GROUP(sb);\n\t\tino = (goal - 1) % EXT4_INODES_PER_GROUP(sb);\n\t\tret2 = 0;\n\t\tgoto got_group;\n\t}\n\n\tif (S_ISDIR(mode))\n\t\tret2 = find_group_orlov(sb, dir, &group, mode, qstr);\n\telse\n\t\tret2 = find_group_other(sb, dir, &group, mode);\n\ngot_group:\n\tEXT4_I(dir)->i_last_alloc_group = group;\n\terr = -ENOSPC;\n\tif (ret2 == -1)\n\t\tgoto out;\n\n\t/*\n\t * Normally we will only go through one pass of this loop,\n\t * unless we get unlucky and it turns out the group we selected\n\t * had its last inode grabbed by someone else.\n\t */\n\tfor (i = 0; i < ngroups; i++, ino = 0) {\n\t\terr = -EIO;\n\n\t\tgdp = ext4_get_group_desc(sb, group, &group_desc_bh);\n\t\tif (!gdp)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * Check free inodes count before loading bitmap.\n\t\t */\n\t\tif (ext4_free_inodes_count(sb, gdp) == 0)\n\t\t\tgoto next_group;\n\n\t\tgrp = ext4_get_group_info(sb, group);\n\t\t/* Skip groups with already-known suspicious inode tables */\n\t\tif (EXT4_MB_GRP_IBITMAP_CORRUPT(grp))\n\t\t\tgoto next_group;\n\n\t\tbrelse(inode_bitmap_bh);\n\t\tinode_bitmap_bh = ext4_read_inode_bitmap(sb, group);\n\t\t/* Skip groups with suspicious inode tables */\n\t\tif (EXT4_MB_GRP_IBITMAP_CORRUPT(grp) ||\n\t\t    IS_ERR(inode_bitmap_bh)) {\n\t\t\tinode_bitmap_bh = NULL;\n\t\t\tgoto next_group;\n\t\t}\n\nrepeat_in_this_group:\n\t\tret2 = find_inode_bit(sb, group, inode_bitmap_bh, &ino);\n\t\tif (!ret2)\n\t\t\tgoto next_group;\n\n\t\tif (group == 0 && (ino + 1) < EXT4_FIRST_INO(sb)) {\n\t\t\text4_error(sb, \"reserved inode found cleared - \"\n\t\t\t\t   \"inode=%lu\", ino + 1);\n\t\t\text4_mark_group_bitmap_corrupted(sb, group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\t\tgoto next_group;\n\t\t}\n\n\t\tif (!handle) {\n\t\t\tBUG_ON(nblocks <= 0);\n\t\t\thandle = __ext4_journal_start_sb(dir->i_sb, line_no,\n\t\t\t\t\t\t\t handle_type, nblocks,\n\t\t\t\t\t\t\t 0);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\terr = PTR_ERR(handle);\n\t\t\t\text4_std_error(sb, err);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tBUFFER_TRACE(inode_bitmap_bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, inode_bitmap_bh);\n\t\tif (err) {\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\t\text4_lock_group(sb, group);\n\t\tret2 = ext4_test_and_set_bit(ino, inode_bitmap_bh->b_data);\n\t\tif (ret2) {\n\t\t\t/* Someone already took the bit. Repeat the search\n\t\t\t * with lock held.\n\t\t\t */\n\t\t\tret2 = find_inode_bit(sb, group, inode_bitmap_bh, &ino);\n\t\t\tif (ret2) {\n\t\t\t\text4_set_bit(ino, inode_bitmap_bh->b_data);\n\t\t\t\tret2 = 0;\n\t\t\t} else {\n\t\t\t\tret2 = 1; /* we didn't grab the inode */\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, group);\n\t\tino++;\t\t/* the inode bitmap is zero-based */\n\t\tif (!ret2)\n\t\t\tgoto got; /* we grabbed the inode! */\n\n\t\tif (ino < EXT4_INODES_PER_GROUP(sb))\n\t\t\tgoto repeat_in_this_group;\nnext_group:\n\t\tif (++group == ngroups)\n\t\t\tgroup = 0;\n\t}\n\terr = -ENOSPC;\n\tgoto out;\n\ngot:\n\tBUFFER_TRACE(inode_bitmap_bh, \"call ext4_handle_dirty_metadata\");\n\terr = ext4_handle_dirty_metadata(handle, NULL, inode_bitmap_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\tBUFFER_TRACE(group_desc_bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, group_desc_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\t/* We may have to initialize the block bitmap if it isn't already */\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n\t\tstruct buffer_head *block_bitmap_bh;\n\n\t\tblock_bitmap_bh = ext4_read_block_bitmap(sb, group);\n\t\tif (IS_ERR(block_bitmap_bh)) {\n\t\t\terr = PTR_ERR(block_bitmap_bh);\n\t\t\tgoto out;\n\t\t}\n\t\tBUFFER_TRACE(block_bitmap_bh, \"get block bitmap access\");\n\t\terr = ext4_journal_get_write_access(handle, block_bitmap_bh);\n\t\tif (err) {\n\t\t\tbrelse(block_bitmap_bh);\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\n\t\tBUFFER_TRACE(block_bitmap_bh, \"dirty block bitmap\");\n\t\terr = ext4_handle_dirty_metadata(handle, NULL, block_bitmap_bh);\n\n\t\t/* recheck and clear flag under lock if we still need to */\n\t\text4_lock_group(sb, group);\n\t\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n\t\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_BLOCK_UNINIT);\n\t\t\text4_free_group_clusters_set(sb, gdp,\n\t\t\t\text4_free_clusters_after_init(sb, group, gdp));\n\t\t\text4_block_bitmap_csum_set(sb, group, gdp,\n\t\t\t\t\t\t   block_bitmap_bh);\n\t\t\text4_group_desc_csum_set(sb, group, gdp);\n\t\t}\n\t\text4_unlock_group(sb, group);\n\t\tbrelse(block_bitmap_bh);\n\n\t\tif (err) {\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* Update the relevant bg descriptor fields */\n\tif (ext4_has_group_desc_csum(sb)) {\n\t\tint free;\n\t\tstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\n\n\t\tdown_read(&grp->alloc_sem); /* protect vs itable lazyinit */\n\t\text4_lock_group(sb, group); /* while we modify the bg desc */\n\t\tfree = EXT4_INODES_PER_GROUP(sb) -\n\t\t\text4_itable_unused_count(sb, gdp);\n\t\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) {\n\t\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_INODE_UNINIT);\n\t\t\tfree = 0;\n\t\t}\n\t\t/*\n\t\t * Check the relative inode number against the last used\n\t\t * relative inode number in this group. if it is greater\n\t\t * we need to update the bg_itable_unused count\n\t\t */\n\t\tif (ino > free)\n\t\t\text4_itable_unused_set(sb, gdp,\n\t\t\t\t\t(EXT4_INODES_PER_GROUP(sb) - ino));\n\t\tup_read(&grp->alloc_sem);\n\t} else {\n\t\text4_lock_group(sb, group);\n\t}\n\n\text4_free_inodes_set(sb, gdp, ext4_free_inodes_count(sb, gdp) - 1);\n\tif (S_ISDIR(mode)) {\n\t\text4_used_dirs_set(sb, gdp, ext4_used_dirs_count(sb, gdp) + 1);\n\t\tif (sbi->s_log_groups_per_flex) {\n\t\t\text4_group_t f = ext4_flex_group(sbi, group);\n\n\t\t\tatomic_inc(&sbi->s_flex_groups[f].used_dirs);\n\t\t}\n\t}\n\tif (ext4_has_group_desc_csum(sb)) {\n\t\text4_inode_bitmap_csum_set(sb, group, gdp, inode_bitmap_bh,\n\t\t\t\t\t   EXT4_INODES_PER_GROUP(sb) / 8);\n\t\text4_group_desc_csum_set(sb, group, gdp);\n\t}\n\text4_unlock_group(sb, group);\n\n\tBUFFER_TRACE(group_desc_bh, \"call ext4_handle_dirty_metadata\");\n\terr = ext4_handle_dirty_metadata(handle, NULL, group_desc_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\tpercpu_counter_dec(&sbi->s_freeinodes_counter);\n\tif (S_ISDIR(mode))\n\t\tpercpu_counter_inc(&sbi->s_dirs_counter);\n\n\tif (sbi->s_log_groups_per_flex) {\n\t\tflex_group = ext4_flex_group(sbi, group);\n\t\tatomic_dec(&sbi->s_flex_groups[flex_group].free_inodes);\n\t}\n\n\tinode->i_ino = ino + group * EXT4_INODES_PER_GROUP(sb);\n\t/* This is the optimal IO size (for stat), not the fs block size */\n\tinode->i_blocks = 0;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = ei->i_crtime =\n\t\t\t\t\t\t       current_time(inode);\n\n\tmemset(ei->i_data, 0, sizeof(ei->i_data));\n\tei->i_dir_start_lookup = 0;\n\tei->i_disksize = 0;\n\n\t/* Don't inherit extent flag from directory, amongst others. */\n\tei->i_flags =\n\t\text4_mask_flags(mode, EXT4_I(dir)->i_flags & EXT4_FL_INHERITED);\n\tei->i_flags |= i_flags;\n\tei->i_file_acl = 0;\n\tei->i_dtime = 0;\n\tei->i_block_group = group;\n\tei->i_last_alloc_group = ~0;\n\n\text4_set_inode_flags(inode);\n\tif (IS_DIRSYNC(inode))\n\t\text4_handle_sync(handle);\n\tif (insert_inode_locked(inode) < 0) {\n\t\t/*\n\t\t * Likely a bitmap corruption causing inode to be allocated\n\t\t * twice.\n\t\t */\n\t\terr = -EIO;\n\t\text4_error(sb, \"failed to insert inode %lu: doubly allocated?\",\n\t\t\t   inode->i_ino);\n\t\text4_mark_group_bitmap_corrupted(sb, group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\tgoto out;\n\t}\n\tinode->i_generation = prandom_u32();\n\n\t/* Precompute checksum seed for inode metadata */\n\tif (ext4_has_metadata_csum(sb)) {\n\t\t__u32 csum;\n\t\t__le32 inum = cpu_to_le32(inode->i_ino);\n\t\t__le32 gen = cpu_to_le32(inode->i_generation);\n\t\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum,\n\t\t\t\t   sizeof(inum));\n\t\tei->i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&gen,\n\t\t\t\t\t      sizeof(gen));\n\t}\n\n\text4_clear_state_flags(ei); /* Only relevant on 32-bit archs */\n\text4_set_inode_state(inode, EXT4_STATE_NEW);\n\n\tei->i_extra_isize = sbi->s_want_extra_isize;\n\tei->i_inline_off = 0;\n\tif (ext4_has_feature_inline_data(sb))\n\t\text4_set_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\n\tret = inode;\n\terr = dquot_alloc_inode(inode);\n\tif (err)\n\t\tgoto fail_drop;\n\n\t/*\n\t * Since the encryption xattr will always be unique, create it first so\n\t * that it's less likely to end up in an external xattr block and\n\t * prevent its deduplication.\n\t */\n\tif (encrypt) {\n\t\terr = fscrypt_inherit_context(dir, inode, handle, true);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\t}\n\n\tif (!(ei->i_flags & EXT4_EA_INODE_FL)) {\n\t\terr = ext4_init_acl(handle, inode, dir);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\n\t\terr = ext4_init_security(handle, inode, dir, qstr);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\t}\n\n\tif (ext4_has_feature_extents(sb)) {\n\t\t/* set extent flag only for directory, file and normal symlink*/\n\t\tif (S_ISDIR(mode) || S_ISREG(mode) || S_ISLNK(mode)) {\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EXTENTS);\n\t\t\text4_ext_tree_init(handle, inode);\n\t\t}\n\t}\n\n\tif (ext4_handle_valid(handle)) {\n\t\tei->i_sync_tid = handle->h_transaction->t_tid;\n\t\tei->i_datasync_tid = handle->h_transaction->t_tid;\n\t}\n\n\terr = ext4_mark_inode_dirty(handle, inode);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto fail_free_drop;\n\t}\n\n\text4_debug(\"allocating inode %lu\\n\", inode->i_ino);\n\ttrace_ext4_allocate_inode(inode, dir, mode);\n\tbrelse(inode_bitmap_bh);\n\treturn ret;\n\nfail_free_drop:\n\tdquot_free_inode(inode);\nfail_drop:\n\tclear_nlink(inode);\n\tunlock_new_inode(inode);\nout:\n\tdquot_drop(inode);\n\tinode->i_flags |= S_NOQUOTA;\n\tiput(inode);\n\tbrelse(inode_bitmap_bh);\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "struct inode *__ext4_new_inode(handle_t *handle, struct inode *dir,\n\t\t\t       umode_t mode, const struct qstr *qstr,\n\t\t\t       __u32 goal, uid_t *owner, __u32 i_flags,\n\t\t\t       int handle_type, unsigned int line_no,\n\t\t\t       int nblocks)\n{\n\tstruct super_block *sb;\n\tstruct buffer_head *inode_bitmap_bh = NULL;\n\tstruct buffer_head *group_desc_bh;\n\text4_group_t ngroups, group = 0;\n\tunsigned long ino = 0;\n\tstruct inode *inode;\n\tstruct ext4_group_desc *gdp = NULL;\n\tstruct ext4_inode_info *ei;\n\tstruct ext4_sb_info *sbi;\n\tint ret2, err;\n\tstruct inode *ret;\n\text4_group_t i;\n\text4_group_t flex_group;\n\tstruct ext4_group_info *grp;\n\tint encrypt = 0;\n\n\t/* Cannot create files in a deleted directory */\n\tif (!dir || !dir->i_nlink)\n\t\treturn ERR_PTR(-EPERM);\n\n\tsb = dir->i_sb;\n\tsbi = EXT4_SB(sb);\n\n\tif (unlikely(ext4_forced_shutdown(sbi)))\n\t\treturn ERR_PTR(-EIO);\n\n\tif ((ext4_encrypted_inode(dir) || DUMMY_ENCRYPTION_ENABLED(sbi)) &&\n\t    (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode)) &&\n\t    !(i_flags & EXT4_EA_INODE_FL)) {\n\t\terr = fscrypt_get_encryption_info(dir);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t\tif (!fscrypt_has_encryption_key(dir))\n\t\t\treturn ERR_PTR(-ENOKEY);\n\t\tencrypt = 1;\n\t}\n\n\tif (!handle && sbi->s_journal && !(i_flags & EXT4_EA_INODE_FL)) {\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\t\tstruct posix_acl *p = get_acl(dir, ACL_TYPE_DEFAULT);\n\n\t\tif (IS_ERR(p))\n\t\t\treturn ERR_CAST(p);\n\t\tif (p) {\n\t\t\tint acl_size = p->a_count * sizeof(ext4_acl_entry);\n\n\t\t\tnblocks += (S_ISDIR(mode) ? 2 : 1) *\n\t\t\t\t__ext4_xattr_set_credits(sb, NULL /* inode */,\n\t\t\t\t\tNULL /* block_bh */, acl_size,\n\t\t\t\t\ttrue /* is_create */);\n\t\t\tposix_acl_release(p);\n\t\t}\n#endif\n\n#ifdef CONFIG_SECURITY\n\t\t{\n\t\t\tint num_security_xattrs = 1;\n\n#ifdef CONFIG_INTEGRITY\n\t\t\tnum_security_xattrs++;\n#endif\n\t\t\t/*\n\t\t\t * We assume that security xattrs are never\n\t\t\t * more than 1k.  In practice they are under\n\t\t\t * 128 bytes.\n\t\t\t */\n\t\t\tnblocks += num_security_xattrs *\n\t\t\t\t__ext4_xattr_set_credits(sb, NULL /* inode */,\n\t\t\t\t\tNULL /* block_bh */, 1024,\n\t\t\t\t\ttrue /* is_create */);\n\t\t}\n#endif\n\t\tif (encrypt)\n\t\t\tnblocks += __ext4_xattr_set_credits(sb,\n\t\t\t\t\tNULL /* inode */, NULL /* block_bh */,\n\t\t\t\t\tFSCRYPT_SET_CONTEXT_MAX_SIZE,\n\t\t\t\t\ttrue /* is_create */);\n\t}\n\n\tngroups = ext4_get_groups_count(sb);\n\ttrace_ext4_request_inode(dir, mode);\n\tinode = new_inode(sb);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tei = EXT4_I(inode);\n\n\t/*\n\t * Initialize owners and quota early so that we don't have to account\n\t * for quota initialization worst case in standard inode creating\n\t * transaction\n\t */\n\tif (owner) {\n\t\tinode->i_mode = mode;\n\t\ti_uid_write(inode, owner[0]);\n\t\ti_gid_write(inode, owner[1]);\n\t} else if (test_opt(sb, GRPID)) {\n\t\tinode->i_mode = mode;\n\t\tinode->i_uid = current_fsuid();\n\t\tinode->i_gid = dir->i_gid;\n\t} else\n\t\tinode_init_owner(inode, dir, mode);\n\n\tif (ext4_has_feature_project(sb) &&\n\t    ext4_test_inode_flag(dir, EXT4_INODE_PROJINHERIT))\n\t\tei->i_projid = EXT4_I(dir)->i_projid;\n\telse\n\t\tei->i_projid = make_kprojid(&init_user_ns, EXT4_DEF_PROJID);\n\n\terr = dquot_initialize(inode);\n\tif (err)\n\t\tgoto out;\n\n\tif (!goal)\n\t\tgoal = sbi->s_inode_goal;\n\n\tif (goal && goal <= le32_to_cpu(sbi->s_es->s_inodes_count)) {\n\t\tgroup = (goal - 1) / EXT4_INODES_PER_GROUP(sb);\n\t\tino = (goal - 1) % EXT4_INODES_PER_GROUP(sb);\n\t\tret2 = 0;\n\t\tgoto got_group;\n\t}\n\n\tif (S_ISDIR(mode))\n\t\tret2 = find_group_orlov(sb, dir, &group, mode, qstr);\n\telse\n\t\tret2 = find_group_other(sb, dir, &group, mode);\n\ngot_group:\n\tEXT4_I(dir)->i_last_alloc_group = group;\n\terr = -ENOSPC;\n\tif (ret2 == -1)\n\t\tgoto out;\n\n\t/*\n\t * Normally we will only go through one pass of this loop,\n\t * unless we get unlucky and it turns out the group we selected\n\t * had its last inode grabbed by someone else.\n\t */\n\tfor (i = 0; i < ngroups; i++, ino = 0) {\n\t\terr = -EIO;\n\n\t\tgdp = ext4_get_group_desc(sb, group, &group_desc_bh);\n\t\tif (!gdp)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * Check free inodes count before loading bitmap.\n\t\t */\n\t\tif (ext4_free_inodes_count(sb, gdp) == 0)\n\t\t\tgoto next_group;\n\n\t\tgrp = ext4_get_group_info(sb, group);\n\t\t/* Skip groups with already-known suspicious inode tables */\n\t\tif (EXT4_MB_GRP_IBITMAP_CORRUPT(grp))\n\t\t\tgoto next_group;\n\n\t\tbrelse(inode_bitmap_bh);\n\t\tinode_bitmap_bh = ext4_read_inode_bitmap(sb, group);\n\t\t/* Skip groups with suspicious inode tables */\n\t\tif (EXT4_MB_GRP_IBITMAP_CORRUPT(grp) ||\n\t\t    IS_ERR(inode_bitmap_bh)) {\n\t\t\tinode_bitmap_bh = NULL;\n\t\t\tgoto next_group;\n\t\t}\n\nrepeat_in_this_group:\n\t\tret2 = find_inode_bit(sb, group, inode_bitmap_bh, &ino);\n\t\tif (!ret2)\n\t\t\tgoto next_group;\n\n\t\tif (group == 0 && (ino + 1) < EXT4_FIRST_INO(sb)) {\n\t\t\text4_error(sb, \"reserved inode found cleared - \"\n\t\t\t\t   \"inode=%lu\", ino + 1);\n\t\t\text4_mark_group_bitmap_corrupted(sb, group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\t\tgoto next_group;\n\t\t}\n\n\t\tif (!handle) {\n\t\t\tBUG_ON(nblocks <= 0);\n\t\t\thandle = __ext4_journal_start_sb(dir->i_sb, line_no,\n\t\t\t\t\t\t\t handle_type, nblocks,\n\t\t\t\t\t\t\t 0);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\terr = PTR_ERR(handle);\n\t\t\t\text4_std_error(sb, err);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tBUFFER_TRACE(inode_bitmap_bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, inode_bitmap_bh);\n\t\tif (err) {\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\t\text4_lock_group(sb, group);\n\t\tret2 = ext4_test_and_set_bit(ino, inode_bitmap_bh->b_data);\n\t\tif (ret2) {\n\t\t\t/* Someone already took the bit. Repeat the search\n\t\t\t * with lock held.\n\t\t\t */\n\t\t\tret2 = find_inode_bit(sb, group, inode_bitmap_bh, &ino);\n\t\t\tif (ret2) {\n\t\t\t\text4_set_bit(ino, inode_bitmap_bh->b_data);\n\t\t\t\tret2 = 0;\n\t\t\t} else {\n\t\t\t\tret2 = 1; /* we didn't grab the inode */\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, group);\n\t\tino++;\t\t/* the inode bitmap is zero-based */\n\t\tif (!ret2)\n\t\t\tgoto got; /* we grabbed the inode! */\n\n\t\tif (ino < EXT4_INODES_PER_GROUP(sb))\n\t\t\tgoto repeat_in_this_group;\nnext_group:\n\t\tif (++group == ngroups)\n\t\t\tgroup = 0;\n\t}\n\terr = -ENOSPC;\n\tgoto out;\n\ngot:\n\tBUFFER_TRACE(inode_bitmap_bh, \"call ext4_handle_dirty_metadata\");\n\terr = ext4_handle_dirty_metadata(handle, NULL, inode_bitmap_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\tBUFFER_TRACE(group_desc_bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, group_desc_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\t/* We may have to initialize the block bitmap if it isn't already */\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n\t\tstruct buffer_head *block_bitmap_bh;\n\n\t\tblock_bitmap_bh = ext4_read_block_bitmap(sb, group);\n\t\tif (IS_ERR(block_bitmap_bh)) {\n\t\t\terr = PTR_ERR(block_bitmap_bh);\n\t\t\tgoto out;\n\t\t}\n\t\tBUFFER_TRACE(block_bitmap_bh, \"get block bitmap access\");\n\t\terr = ext4_journal_get_write_access(handle, block_bitmap_bh);\n\t\tif (err) {\n\t\t\tbrelse(block_bitmap_bh);\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\n\t\tBUFFER_TRACE(block_bitmap_bh, \"dirty block bitmap\");\n\t\terr = ext4_handle_dirty_metadata(handle, NULL, block_bitmap_bh);\n\n\t\t/* recheck and clear flag under lock if we still need to */\n\t\text4_lock_group(sb, group);\n\t\tif (ext4_has_group_desc_csum(sb) &&\n\t\t    (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {\n\t\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_BLOCK_UNINIT);\n\t\t\text4_free_group_clusters_set(sb, gdp,\n\t\t\t\text4_free_clusters_after_init(sb, group, gdp));\n\t\t\text4_block_bitmap_csum_set(sb, group, gdp,\n\t\t\t\t\t\t   block_bitmap_bh);\n\t\t\text4_group_desc_csum_set(sb, group, gdp);\n\t\t}\n\t\text4_unlock_group(sb, group);\n\t\tbrelse(block_bitmap_bh);\n\n\t\tif (err) {\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* Update the relevant bg descriptor fields */\n\tif (ext4_has_group_desc_csum(sb)) {\n\t\tint free;\n\t\tstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\n\n\t\tdown_read(&grp->alloc_sem); /* protect vs itable lazyinit */\n\t\text4_lock_group(sb, group); /* while we modify the bg desc */\n\t\tfree = EXT4_INODES_PER_GROUP(sb) -\n\t\t\text4_itable_unused_count(sb, gdp);\n\t\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) {\n\t\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_INODE_UNINIT);\n\t\t\tfree = 0;\n\t\t}\n\t\t/*\n\t\t * Check the relative inode number against the last used\n\t\t * relative inode number in this group. if it is greater\n\t\t * we need to update the bg_itable_unused count\n\t\t */\n\t\tif (ino > free)\n\t\t\text4_itable_unused_set(sb, gdp,\n\t\t\t\t\t(EXT4_INODES_PER_GROUP(sb) - ino));\n\t\tup_read(&grp->alloc_sem);\n\t} else {\n\t\text4_lock_group(sb, group);\n\t}\n\n\text4_free_inodes_set(sb, gdp, ext4_free_inodes_count(sb, gdp) - 1);\n\tif (S_ISDIR(mode)) {\n\t\text4_used_dirs_set(sb, gdp, ext4_used_dirs_count(sb, gdp) + 1);\n\t\tif (sbi->s_log_groups_per_flex) {\n\t\t\text4_group_t f = ext4_flex_group(sbi, group);\n\n\t\t\tatomic_inc(&sbi->s_flex_groups[f].used_dirs);\n\t\t}\n\t}\n\tif (ext4_has_group_desc_csum(sb)) {\n\t\text4_inode_bitmap_csum_set(sb, group, gdp, inode_bitmap_bh,\n\t\t\t\t\t   EXT4_INODES_PER_GROUP(sb) / 8);\n\t\text4_group_desc_csum_set(sb, group, gdp);\n\t}\n\text4_unlock_group(sb, group);\n\n\tBUFFER_TRACE(group_desc_bh, \"call ext4_handle_dirty_metadata\");\n\terr = ext4_handle_dirty_metadata(handle, NULL, group_desc_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\tpercpu_counter_dec(&sbi->s_freeinodes_counter);\n\tif (S_ISDIR(mode))\n\t\tpercpu_counter_inc(&sbi->s_dirs_counter);\n\n\tif (sbi->s_log_groups_per_flex) {\n\t\tflex_group = ext4_flex_group(sbi, group);\n\t\tatomic_dec(&sbi->s_flex_groups[flex_group].free_inodes);\n\t}\n\n\tinode->i_ino = ino + group * EXT4_INODES_PER_GROUP(sb);\n\t/* This is the optimal IO size (for stat), not the fs block size */\n\tinode->i_blocks = 0;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = ei->i_crtime =\n\t\t\t\t\t\t       current_time(inode);\n\n\tmemset(ei->i_data, 0, sizeof(ei->i_data));\n\tei->i_dir_start_lookup = 0;\n\tei->i_disksize = 0;\n\n\t/* Don't inherit extent flag from directory, amongst others. */\n\tei->i_flags =\n\t\text4_mask_flags(mode, EXT4_I(dir)->i_flags & EXT4_FL_INHERITED);\n\tei->i_flags |= i_flags;\n\tei->i_file_acl = 0;\n\tei->i_dtime = 0;\n\tei->i_block_group = group;\n\tei->i_last_alloc_group = ~0;\n\n\text4_set_inode_flags(inode);\n\tif (IS_DIRSYNC(inode))\n\t\text4_handle_sync(handle);\n\tif (insert_inode_locked(inode) < 0) {\n\t\t/*\n\t\t * Likely a bitmap corruption causing inode to be allocated\n\t\t * twice.\n\t\t */\n\t\terr = -EIO;\n\t\text4_error(sb, \"failed to insert inode %lu: doubly allocated?\",\n\t\t\t   inode->i_ino);\n\t\text4_mark_group_bitmap_corrupted(sb, group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\tgoto out;\n\t}\n\tinode->i_generation = prandom_u32();\n\n\t/* Precompute checksum seed for inode metadata */\n\tif (ext4_has_metadata_csum(sb)) {\n\t\t__u32 csum;\n\t\t__le32 inum = cpu_to_le32(inode->i_ino);\n\t\t__le32 gen = cpu_to_le32(inode->i_generation);\n\t\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum,\n\t\t\t\t   sizeof(inum));\n\t\tei->i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&gen,\n\t\t\t\t\t      sizeof(gen));\n\t}\n\n\text4_clear_state_flags(ei); /* Only relevant on 32-bit archs */\n\text4_set_inode_state(inode, EXT4_STATE_NEW);\n\n\tei->i_extra_isize = sbi->s_want_extra_isize;\n\tei->i_inline_off = 0;\n\tif (ext4_has_feature_inline_data(sb))\n\t\text4_set_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\n\tret = inode;\n\terr = dquot_alloc_inode(inode);\n\tif (err)\n\t\tgoto fail_drop;\n\n\t/*\n\t * Since the encryption xattr will always be unique, create it first so\n\t * that it's less likely to end up in an external xattr block and\n\t * prevent its deduplication.\n\t */\n\tif (encrypt) {\n\t\terr = fscrypt_inherit_context(dir, inode, handle, true);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\t}\n\n\tif (!(ei->i_flags & EXT4_EA_INODE_FL)) {\n\t\terr = ext4_init_acl(handle, inode, dir);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\n\t\terr = ext4_init_security(handle, inode, dir, qstr);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\t}\n\n\tif (ext4_has_feature_extents(sb)) {\n\t\t/* set extent flag only for directory, file and normal symlink*/\n\t\tif (S_ISDIR(mode) || S_ISREG(mode) || S_ISLNK(mode)) {\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EXTENTS);\n\t\t\text4_ext_tree_init(handle, inode);\n\t\t}\n\t}\n\n\tif (ext4_handle_valid(handle)) {\n\t\tei->i_sync_tid = handle->h_transaction->t_tid;\n\t\tei->i_datasync_tid = handle->h_transaction->t_tid;\n\t}\n\n\terr = ext4_mark_inode_dirty(handle, inode);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto fail_free_drop;\n\t}\n\n\text4_debug(\"allocating inode %lu\\n\", inode->i_ino);\n\ttrace_ext4_allocate_inode(inode, dir, mode);\n\tbrelse(inode_bitmap_bh);\n\treturn ret;\n\nfail_free_drop:\n\tdquot_free_inode(inode);\nfail_drop:\n\tclear_nlink(inode);\n\tunlock_new_inode(inode);\nout:\n\tdquot_drop(inode);\n\tinode->i_flags |= S_NOQUOTA;\n\tiput(inode);\n\tbrelse(inode_bitmap_bh);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -265,7 +265,8 @@\n \n \t\t/* recheck and clear flag under lock if we still need to */\n \t\text4_lock_group(sb, group);\n-\t\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n+\t\tif (ext4_has_group_desc_csum(sb) &&\n+\t\t    (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {\n \t\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_BLOCK_UNINIT);\n \t\t\text4_free_group_clusters_set(sb, gdp,\n \t\t\t\text4_free_clusters_after_init(sb, group, gdp));",
        "function_modified_lines": {
            "added": [
                "\t\tif (ext4_has_group_desc_csum(sb) &&",
                "\t\t    (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {"
            ],
            "deleted": [
                "\t\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in Linux kernel in the ext4 filesystem code. A use-after-free is possible in ext4_ext_remove_space() function when mounting and operating a crafted ext4 image.",
        "id": 1605
    },
    {
        "cve_id": "CVE-2018-9465",
        "code_before_change": "static int task_get_unused_fd_flags(struct binder_proc *proc, int flags)\n{\n\tstruct files_struct *files = proc->files;\n\tunsigned long rlim_cur;\n\tunsigned long irqs;\n\n\tif (files == NULL)\n\t\treturn -ESRCH;\n\n\tif (!lock_task_sighand(proc->tsk, &irqs))\n\t\treturn -EMFILE;\n\n\trlim_cur = task_rlimit(proc->tsk, RLIMIT_NOFILE);\n\tunlock_task_sighand(proc->tsk, &irqs);\n\n\treturn __alloc_fd(files, 0, rlim_cur, flags);\n}",
        "code_after_change": "static int task_get_unused_fd_flags(struct binder_proc *proc, int flags)\n{\n\tunsigned long rlim_cur;\n\tunsigned long irqs;\n\tint ret;\n\n\tmutex_lock(&proc->files_lock);\n\tif (proc->files == NULL) {\n\t\tret = -ESRCH;\n\t\tgoto err;\n\t}\n\tif (!lock_task_sighand(proc->tsk, &irqs)) {\n\t\tret = -EMFILE;\n\t\tgoto err;\n\t}\n\trlim_cur = task_rlimit(proc->tsk, RLIMIT_NOFILE);\n\tunlock_task_sighand(proc->tsk, &irqs);\n\n\tret = __alloc_fd(proc->files, 0, rlim_cur, flags);\nerr:\n\tmutex_unlock(&proc->files_lock);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,17 +1,23 @@\n static int task_get_unused_fd_flags(struct binder_proc *proc, int flags)\n {\n-\tstruct files_struct *files = proc->files;\n \tunsigned long rlim_cur;\n \tunsigned long irqs;\n+\tint ret;\n \n-\tif (files == NULL)\n-\t\treturn -ESRCH;\n-\n-\tif (!lock_task_sighand(proc->tsk, &irqs))\n-\t\treturn -EMFILE;\n-\n+\tmutex_lock(&proc->files_lock);\n+\tif (proc->files == NULL) {\n+\t\tret = -ESRCH;\n+\t\tgoto err;\n+\t}\n+\tif (!lock_task_sighand(proc->tsk, &irqs)) {\n+\t\tret = -EMFILE;\n+\t\tgoto err;\n+\t}\n \trlim_cur = task_rlimit(proc->tsk, RLIMIT_NOFILE);\n \tunlock_task_sighand(proc->tsk, &irqs);\n \n-\treturn __alloc_fd(files, 0, rlim_cur, flags);\n+\tret = __alloc_fd(proc->files, 0, rlim_cur, flags);\n+err:\n+\tmutex_unlock(&proc->files_lock);\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tint ret;",
                "\tmutex_lock(&proc->files_lock);",
                "\tif (proc->files == NULL) {",
                "\t\tret = -ESRCH;",
                "\t\tgoto err;",
                "\t}",
                "\tif (!lock_task_sighand(proc->tsk, &irqs)) {",
                "\t\tret = -EMFILE;",
                "\t\tgoto err;",
                "\t}",
                "\tret = __alloc_fd(proc->files, 0, rlim_cur, flags);",
                "err:",
                "\tmutex_unlock(&proc->files_lock);",
                "\treturn ret;"
            ],
            "deleted": [
                "\tstruct files_struct *files = proc->files;",
                "\tif (files == NULL)",
                "\t\treturn -ESRCH;",
                "",
                "\tif (!lock_task_sighand(proc->tsk, &irqs))",
                "\t\treturn -EMFILE;",
                "",
                "\treturn __alloc_fd(files, 0, rlim_cur, flags);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In task_get_unused_fd_flags of binder.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation. Product: Android Versions: Android kernel Android ID: A-69164715 References: Upstream kernel.",
        "id": 1871
    },
    {
        "cve_id": "CVE-2022-42703",
        "code_before_change": "static inline struct anon_vma *anon_vma_alloc(void)\n{\n\tstruct anon_vma *anon_vma;\n\n\tanon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);\n\tif (anon_vma) {\n\t\tatomic_set(&anon_vma->refcount, 1);\n\t\tanon_vma->degree = 1;\t/* Reference for first vma */\n\t\tanon_vma->parent = anon_vma;\n\t\t/*\n\t\t * Initialise the anon_vma root to point to itself. If called\n\t\t * from fork, the root will be reset to the parents anon_vma.\n\t\t */\n\t\tanon_vma->root = anon_vma;\n\t}\n\n\treturn anon_vma;\n}",
        "code_after_change": "static inline struct anon_vma *anon_vma_alloc(void)\n{\n\tstruct anon_vma *anon_vma;\n\n\tanon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);\n\tif (anon_vma) {\n\t\tatomic_set(&anon_vma->refcount, 1);\n\t\tanon_vma->num_children = 0;\n\t\tanon_vma->num_active_vmas = 0;\n\t\tanon_vma->parent = anon_vma;\n\t\t/*\n\t\t * Initialise the anon_vma root to point to itself. If called\n\t\t * from fork, the root will be reset to the parents anon_vma.\n\t\t */\n\t\tanon_vma->root = anon_vma;\n\t}\n\n\treturn anon_vma;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,8 @@\n \tanon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);\n \tif (anon_vma) {\n \t\tatomic_set(&anon_vma->refcount, 1);\n-\t\tanon_vma->degree = 1;\t/* Reference for first vma */\n+\t\tanon_vma->num_children = 0;\n+\t\tanon_vma->num_active_vmas = 0;\n \t\tanon_vma->parent = anon_vma;\n \t\t/*\n \t\t * Initialise the anon_vma root to point to itself. If called",
        "function_modified_lines": {
            "added": [
                "\t\tanon_vma->num_children = 0;",
                "\t\tanon_vma->num_active_vmas = 0;"
            ],
            "deleted": [
                "\t\tanon_vma->degree = 1;\t/* Reference for first vma */"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "mm/rmap.c in the Linux kernel before 5.19.7 has a use-after-free related to leaf anon_vma double reuse.",
        "id": 3727
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n{\n\tstruct io_identity *iod;\n\n\tiod = idr_remove(&ctx->personality_idr, id);\n\tif (iod) {\n\t\tput_cred(iod->creds);\n\t\tif (refcount_dec_and_test(&iod->count))\n\t\t\tkfree(iod);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}",
        "code_after_change": "static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n{\n\tconst struct cred *creds;\n\n\tcreds = idr_remove(&ctx->personality_idr, id);\n\tif (creds) {\n\t\tput_cred(creds);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,10 @@\n static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n {\n-\tstruct io_identity *iod;\n+\tconst struct cred *creds;\n \n-\tiod = idr_remove(&ctx->personality_idr, id);\n-\tif (iod) {\n-\t\tput_cred(iod->creds);\n-\t\tif (refcount_dec_and_test(&iod->count))\n-\t\t\tkfree(iod);\n+\tcreds = idr_remove(&ctx->personality_idr, id);\n+\tif (creds) {\n+\t\tput_cred(creds);\n \t\treturn 0;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\tconst struct cred *creds;",
                "\tcreds = idr_remove(&ctx->personality_idr, id);",
                "\tif (creds) {",
                "\t\tput_cred(creds);"
            ],
            "deleted": [
                "\tstruct io_identity *iod;",
                "\tiod = idr_remove(&ctx->personality_idr, id);",
                "\tif (iod) {",
                "\t\tput_cred(iod->creds);",
                "\t\tif (refcount_dec_and_test(&iod->count))",
                "\t\t\tkfree(iod);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3287
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static struct dst_entry *inet6_csk_route_socket(struct sock *sk,\n\t\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->daddr = sk->sk_v6_daddr;\n\tfl6->saddr = np->saddr;\n\tfl6->flowlabel = np->flow_label;\n\tIP6_ECN_flow_xmit(sk, fl6->flowlabel);\n\tfl6->flowi6_oif = sk->sk_bound_dev_if;\n\tfl6->flowi6_mark = sk->sk_mark;\n\tfl6->fl6_sport = inet->inet_sport;\n\tfl6->fl6_dport = inet->inet_dport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n\n\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n\n\tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n\tif (!dst) {\n\t\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\n\t\tif (!IS_ERR(dst))\n\t\t\t__inet6_csk_dst_store(sk, dst, NULL, NULL);\n\t}\n\treturn dst;\n}",
        "code_after_change": "static struct dst_entry *inet6_csk_route_socket(struct sock *sk,\n\t\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->daddr = sk->sk_v6_daddr;\n\tfl6->saddr = np->saddr;\n\tfl6->flowlabel = np->flow_label;\n\tIP6_ECN_flow_xmit(sk, fl6->flowlabel);\n\tfl6->flowi6_oif = sk->sk_bound_dev_if;\n\tfl6->flowi6_mark = sk->sk_mark;\n\tfl6->fl6_sport = inet->inet_sport;\n\tfl6->fl6_dport = inet->inet_dport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n\tif (!dst) {\n\t\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\n\t\tif (!IS_ERR(dst))\n\t\t\t__inet6_csk_dst_store(sk, dst, NULL, NULL);\n\t}\n\treturn dst;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,9 @@\n \tfl6->fl6_dport = inet->inet_dport;\n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n \n-\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n+\trcu_read_lock();\n+\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n+\trcu_read_unlock();\n \n \tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n \tif (!dst) {",
        "function_modified_lines": {
            "added": [
                "\trcu_read_lock();",
                "\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(fl6, np->opt, &final);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 997
    },
    {
        "cve_id": "CVE-2023-3439",
        "code_before_change": "void mctp_dev_put(struct mctp_dev *mdev)\n{\n\tif (mdev && refcount_dec_and_test(&mdev->refs)) {\n\t\tdev_put(mdev->dev);\n\t\tkfree_rcu(mdev, rcu);\n\t}\n}",
        "code_after_change": "void mctp_dev_put(struct mctp_dev *mdev)\n{\n\tif (mdev && refcount_dec_and_test(&mdev->refs)) {\n\t\tkfree(mdev->addrs);\n\t\tdev_put(mdev->dev);\n\t\tkfree_rcu(mdev, rcu);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,7 @@\n void mctp_dev_put(struct mctp_dev *mdev)\n {\n \tif (mdev && refcount_dec_and_test(&mdev->refs)) {\n+\t\tkfree(mdev->addrs);\n \t\tdev_put(mdev->dev);\n \t\tkfree_rcu(mdev, rcu);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tkfree(mdev->addrs);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the MCTP protocol in the Linux kernel. The function mctp_unregister() reclaims the device's relevant resource when a netcard detaches. However, a running routine may be unaware of this and cause the use-after-free of the mdev->addrs object, potentially leading to a denial of service.",
        "id": 4105
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "static void io_poll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tret = io_poll_check_events(req, locked);\n\tif (ret > 0)\n\t\treturn;\n\n\tif (!ret) {\n\t\tstruct io_poll *poll = io_kiocb_to_cmd(req);\n\n\t\treq->cqe.res = mangle_poll(req->cqe.res & poll->events);\n\t} else {\n\t\treq->cqe.res = ret;\n\t\treq_set_fail(req);\n\t}\n\n\tio_poll_remove_entries(req);\n\tio_poll_req_delete(req, ctx);\n\tio_req_set_res(req, req->cqe.res, 0);\n\tio_req_task_complete(req, locked);\n}",
        "code_after_change": "static void io_poll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tint ret;\n\n\tret = io_poll_check_events(req, locked);\n\tif (ret > 0)\n\t\treturn;\n\n\tif (!ret) {\n\t\tstruct io_poll *poll = io_kiocb_to_cmd(req);\n\n\t\treq->cqe.res = mangle_poll(req->cqe.res & poll->events);\n\t} else {\n\t\treq->cqe.res = ret;\n\t\treq_set_fail(req);\n\t}\n\n\tio_poll_remove_entries(req);\n\tio_poll_tw_hash_eject(req, locked);\n\n\tio_req_set_res(req, req->cqe.res, 0);\n\tio_req_task_complete(req, locked);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,5 @@\n static void io_poll_task_func(struct io_kiocb *req, bool *locked)\n {\n-\tstruct io_ring_ctx *ctx = req->ctx;\n \tint ret;\n \n \tret = io_poll_check_events(req, locked);\n@@ -17,7 +16,8 @@\n \t}\n \n \tio_poll_remove_entries(req);\n-\tio_poll_req_delete(req, ctx);\n+\tio_poll_tw_hash_eject(req, locked);\n+\n \tio_req_set_res(req, req->cqe.res, 0);\n \tio_req_task_complete(req, locked);\n }",
        "function_modified_lines": {
            "added": [
                "\tio_poll_tw_hash_eject(req, locked);",
                ""
            ],
            "deleted": [
                "\tstruct io_ring_ctx *ctx = req->ctx;",
                "\tio_poll_req_delete(req, ctx);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and 0e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4073
    },
    {
        "cve_id": "CVE-2020-14381",
        "code_before_change": "static void get_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr)\n\t\treturn;\n\n\t/*\n\t * On MMU less systems futexes are always \"private\" as there is no per\n\t * process address space. We need the smp wmb nevertheless - yes,\n\t * arch/blackfin has MMU less SMP ...\n\t */\n\tif (!IS_ENABLED(CONFIG_MMU)) {\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t\treturn;\n\t}\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tihold(key->shared.inode); /* implies smp_mb(); (B) */\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tfutex_get_mm(key); /* implies smp_mb(); (B) */\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * Private futexes do not hold reference on an inode or\n\t\t * mm, therefore the only purpose of calling get_futex_key_refs\n\t\t * is because we need the barrier for the lockless waiter check.\n\t\t */\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t}\n}",
        "code_after_change": "static void get_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr)\n\t\treturn;\n\n\t/*\n\t * On MMU less systems futexes are always \"private\" as there is no per\n\t * process address space. We need the smp wmb nevertheless - yes,\n\t * arch/blackfin has MMU less SMP ...\n\t */\n\tif (!IS_ENABLED(CONFIG_MMU)) {\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t\treturn;\n\t}\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tsmp_mb();\t\t/* explicit smp_mb(); (B) */\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tfutex_get_mm(key); /* implies smp_mb(); (B) */\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * Private futexes do not hold reference on an inode or\n\t\t * mm, therefore the only purpose of calling get_futex_key_refs\n\t\t * is because we need the barrier for the lockless waiter check.\n\t\t */\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,7 +15,7 @@\n \n \tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n \tcase FUT_OFF_INODE:\n-\t\tihold(key->shared.inode); /* implies smp_mb(); (B) */\n+\t\tsmp_mb();\t\t/* explicit smp_mb(); (B) */\n \t\tbreak;\n \tcase FUT_OFF_MMSHARED:\n \t\tfutex_get_mm(key); /* implies smp_mb(); (B) */",
        "function_modified_lines": {
            "added": [
                "\t\tsmp_mb();\t\t/* explicit smp_mb(); (B) */"
            ],
            "deleted": [
                "\t\tihold(key->shared.inode); /* implies smp_mb(); (B) */"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s futex implementation. This flaw allows a local attacker to corrupt system memory or escalate their privileges when creating a futex on a filesystem that is about to be unmounted. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability.",
        "id": 2520
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "static __cold void io_uring_try_cancel_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t\tstruct task_struct *task,\n\t\t\t\t\t\tbool cancel_all)\n{\n\tstruct io_task_cancel cancel = { .task = task, .all = cancel_all, };\n\tstruct io_uring_task *tctx = task ? task->io_uring : NULL;\n\n\t/* failed during ring init, it couldn't have issued any requests */\n\tif (!ctx->rings)\n\t\treturn;\n\n\twhile (1) {\n\t\tenum io_wq_cancel cret;\n\t\tbool ret = false;\n\n\t\tif (!task) {\n\t\t\tret |= io_uring_try_cancel_iowq(ctx);\n\t\t} else if (tctx && tctx->io_wq) {\n\t\t\t/*\n\t\t\t * Cancels requests of all rings, not only @ctx, but\n\t\t\t * it's fine as the task is in exit/exec.\n\t\t\t */\n\t\t\tcret = io_wq_cancel_cb(tctx->io_wq, io_cancel_task_cb,\n\t\t\t\t\t       &cancel, true);\n\t\t\tret |= (cret != IO_WQ_CANCEL_NOTFOUND);\n\t\t}\n\n\t\t/* SQPOLL thread does its own polling */\n\t\tif ((!(ctx->flags & IORING_SETUP_SQPOLL) && cancel_all) ||\n\t\t    (ctx->sq_data && ctx->sq_data->thread == current)) {\n\t\t\twhile (!wq_list_empty(&ctx->iopoll_list)) {\n\t\t\t\tio_iopoll_try_reap_events(ctx);\n\t\t\t\tret = true;\n\t\t\t}\n\t\t}\n\n\t\tret |= io_cancel_defer_files(ctx, task, cancel_all);\n\t\tret |= io_poll_remove_all(ctx, task, cancel_all);\n\t\tret |= io_kill_timeouts(ctx, task, cancel_all);\n\t\tif (task)\n\t\t\tret |= io_run_task_work();\n\t\tif (!ret)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n}",
        "code_after_change": "static __cold void io_uring_try_cancel_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t\tstruct task_struct *task,\n\t\t\t\t\t\tbool cancel_all)\n{\n\tstruct io_task_cancel cancel = { .task = task, .all = cancel_all, };\n\tstruct io_uring_task *tctx = task ? task->io_uring : NULL;\n\n\t/* failed during ring init, it couldn't have issued any requests */\n\tif (!ctx->rings)\n\t\treturn;\n\n\twhile (1) {\n\t\tenum io_wq_cancel cret;\n\t\tbool ret = false;\n\n\t\tif (!task) {\n\t\t\tret |= io_uring_try_cancel_iowq(ctx);\n\t\t} else if (tctx && tctx->io_wq) {\n\t\t\t/*\n\t\t\t * Cancels requests of all rings, not only @ctx, but\n\t\t\t * it's fine as the task is in exit/exec.\n\t\t\t */\n\t\t\tcret = io_wq_cancel_cb(tctx->io_wq, io_cancel_task_cb,\n\t\t\t\t\t       &cancel, true);\n\t\t\tret |= (cret != IO_WQ_CANCEL_NOTFOUND);\n\t\t}\n\n\t\t/* SQPOLL thread does its own polling */\n\t\tif ((!(ctx->flags & IORING_SETUP_SQPOLL) && cancel_all) ||\n\t\t    (ctx->sq_data && ctx->sq_data->thread == current)) {\n\t\t\twhile (!wq_list_empty(&ctx->iopoll_list)) {\n\t\t\t\tio_iopoll_try_reap_events(ctx);\n\t\t\t\tret = true;\n\t\t\t}\n\t\t}\n\n\t\tret |= io_cancel_defer_files(ctx, task, cancel_all);\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tret |= io_poll_remove_all(ctx, task, cancel_all);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\tret |= io_kill_timeouts(ctx, task, cancel_all);\n\t\tif (task)\n\t\t\tret |= io_run_task_work();\n\t\tif (!ret)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -35,7 +35,9 @@\n \t\t}\n \n \t\tret |= io_cancel_defer_files(ctx, task, cancel_all);\n+\t\tmutex_lock(&ctx->uring_lock);\n \t\tret |= io_poll_remove_all(ctx, task, cancel_all);\n+\t\tmutex_unlock(&ctx->uring_lock);\n \t\tret |= io_kill_timeouts(ctx, task, cancel_all);\n \t\tif (task)\n \t\t\tret |= io_run_task_work();",
        "function_modified_lines": {
            "added": [
                "\t\tmutex_lock(&ctx->uring_lock);",
                "\t\tmutex_unlock(&ctx->uring_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and 0e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4066
    },
    {
        "cve_id": "CVE-2022-1679",
        "code_before_change": "int ath9k_htc_probe_device(struct htc_target *htc_handle, struct device *dev,\n\t\t\t   u16 devid, char *product, u32 drv_info)\n{\n\tstruct hif_device_usb *hif_dev;\n\tstruct ath9k_htc_priv *priv;\n\tstruct ieee80211_hw *hw;\n\tint ret;\n\n\thw = ieee80211_alloc_hw(sizeof(struct ath9k_htc_priv), &ath9k_htc_ops);\n\tif (!hw)\n\t\treturn -ENOMEM;\n\n\tpriv = hw->priv;\n\tpriv->hw = hw;\n\tpriv->htc = htc_handle;\n\tpriv->dev = dev;\n\thtc_handle->drv_priv = priv;\n\tSET_IEEE80211_DEV(hw, priv->dev);\n\n\tret = ath9k_htc_wait_for_target(priv);\n\tif (ret)\n\t\tgoto err_free;\n\n\tpriv->wmi = ath9k_init_wmi(priv);\n\tif (!priv->wmi) {\n\t\tret = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tret = ath9k_init_htc_services(priv, devid, drv_info);\n\tif (ret)\n\t\tgoto err_init;\n\n\tret = ath9k_init_device(priv, devid, product, drv_info);\n\tif (ret)\n\t\tgoto err_init;\n\n\treturn 0;\n\nerr_init:\n\tath9k_stop_wmi(priv);\n\thif_dev = (struct hif_device_usb *)htc_handle->hif_dev;\n\tath9k_hif_usb_dealloc_urbs(hif_dev);\n\tath9k_destroy_wmi(priv);\nerr_free:\n\tieee80211_free_hw(hw);\n\treturn ret;\n}",
        "code_after_change": "int ath9k_htc_probe_device(struct htc_target *htc_handle, struct device *dev,\n\t\t\t   u16 devid, char *product, u32 drv_info)\n{\n\tstruct hif_device_usb *hif_dev;\n\tstruct ath9k_htc_priv *priv;\n\tstruct ieee80211_hw *hw;\n\tint ret;\n\n\thw = ieee80211_alloc_hw(sizeof(struct ath9k_htc_priv), &ath9k_htc_ops);\n\tif (!hw)\n\t\treturn -ENOMEM;\n\n\tpriv = hw->priv;\n\tpriv->hw = hw;\n\tpriv->htc = htc_handle;\n\tpriv->dev = dev;\n\tSET_IEEE80211_DEV(hw, priv->dev);\n\n\tret = ath9k_htc_wait_for_target(priv);\n\tif (ret)\n\t\tgoto err_free;\n\n\tpriv->wmi = ath9k_init_wmi(priv);\n\tif (!priv->wmi) {\n\t\tret = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tret = ath9k_init_htc_services(priv, devid, drv_info);\n\tif (ret)\n\t\tgoto err_init;\n\n\tret = ath9k_init_device(priv, devid, product, drv_info);\n\tif (ret)\n\t\tgoto err_init;\n\n\thtc_handle->drv_priv = priv;\n\n\treturn 0;\n\nerr_init:\n\tath9k_stop_wmi(priv);\n\thif_dev = (struct hif_device_usb *)htc_handle->hif_dev;\n\tath9k_hif_usb_dealloc_urbs(hif_dev);\n\tath9k_destroy_wmi(priv);\nerr_free:\n\tieee80211_free_hw(hw);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,7 +14,6 @@\n \tpriv->hw = hw;\n \tpriv->htc = htc_handle;\n \tpriv->dev = dev;\n-\thtc_handle->drv_priv = priv;\n \tSET_IEEE80211_DEV(hw, priv->dev);\n \n \tret = ath9k_htc_wait_for_target(priv);\n@@ -35,6 +34,8 @@\n \tif (ret)\n \t\tgoto err_init;\n \n+\thtc_handle->drv_priv = priv;\n+\n \treturn 0;\n \n err_init:",
        "function_modified_lines": {
            "added": [
                "\thtc_handle->drv_priv = priv;",
                ""
            ],
            "deleted": [
                "\thtc_handle->drv_priv = priv;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s Atheros wireless adapter driver in the way a user forces the ath9k_htc_wait_for_target function to fail with some input messages. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3275
    },
    {
        "cve_id": "CVE-2023-4611",
        "code_before_change": "static long do_mbind(unsigned long start, unsigned long len,\n\t\t     unsigned short mode, unsigned short mode_flags,\n\t\t     nodemask_t *nmask, unsigned long flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct vma_iterator vmi;\n\tstruct mempolicy *new;\n\tunsigned long end;\n\tint err;\n\tint ret;\n\tLIST_HEAD(pagelist);\n\n\tif (flags & ~(unsigned long)MPOL_MF_VALID)\n\t\treturn -EINVAL;\n\tif ((flags & MPOL_MF_MOVE_ALL) && !capable(CAP_SYS_NICE))\n\t\treturn -EPERM;\n\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\n\tif (mode == MPOL_DEFAULT)\n\t\tflags &= ~MPOL_MF_STRICT;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\n\tnew = mpol_new(mode, mode_flags, nmask);\n\tif (IS_ERR(new))\n\t\treturn PTR_ERR(new);\n\n\tif (flags & MPOL_MF_LAZY)\n\t\tnew->flags |= MPOL_F_MOF;\n\n\t/*\n\t * If we are using the default policy then operation\n\t * on discontinuous address spaces is okay after all\n\t */\n\tif (!new)\n\t\tflags |= MPOL_MF_DISCONTIG_OK;\n\n\tpr_debug(\"mbind %lx-%lx mode:%d flags:%d nodes:%lx\\n\",\n\t\t start, start + len, mode, mode_flags,\n\t\t nmask ? nodes_addr(*nmask)[0] : NUMA_NO_NODE);\n\n\tif (flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) {\n\n\t\tlru_cache_disable();\n\t}\n\t{\n\t\tNODEMASK_SCRATCH(scratch);\n\t\tif (scratch) {\n\t\t\tmmap_write_lock(mm);\n\t\t\terr = mpol_set_nodemask(new, nmask, scratch);\n\t\t\tif (err)\n\t\t\t\tmmap_write_unlock(mm);\n\t\t} else\n\t\t\terr = -ENOMEM;\n\t\tNODEMASK_SCRATCH_FREE(scratch);\n\t}\n\tif (err)\n\t\tgoto mpol_out;\n\n\tret = queue_pages_range(mm, start, end, nmask,\n\t\t\t  flags | MPOL_MF_INVERT, &pagelist);\n\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto up_out;\n\t}\n\n\tvma_iter_init(&vmi, mm, start);\n\tprev = vma_prev(&vmi);\n\tfor_each_vma_range(vmi, vma, end) {\n\t\terr = mbind_range(&vmi, vma, &prev, start, end, new);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\tif (!err) {\n\t\tint nr_failed = 0;\n\n\t\tif (!list_empty(&pagelist)) {\n\t\t\tWARN_ON_ONCE(flags & MPOL_MF_LAZY);\n\t\t\tnr_failed = migrate_pages(&pagelist, new_folio, NULL,\n\t\t\t\tstart, MIGRATE_SYNC, MR_MEMPOLICY_MBIND, NULL);\n\t\t\tif (nr_failed)\n\t\t\t\tputback_movable_pages(&pagelist);\n\t\t}\n\n\t\tif ((ret > 0) || (nr_failed && (flags & MPOL_MF_STRICT)))\n\t\t\terr = -EIO;\n\t} else {\nup_out:\n\t\tif (!list_empty(&pagelist))\n\t\t\tputback_movable_pages(&pagelist);\n\t}\n\n\tmmap_write_unlock(mm);\nmpol_out:\n\tmpol_put(new);\n\tif (flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL))\n\t\tlru_cache_enable();\n\treturn err;\n}",
        "code_after_change": "static long do_mbind(unsigned long start, unsigned long len,\n\t\t     unsigned short mode, unsigned short mode_flags,\n\t\t     nodemask_t *nmask, unsigned long flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct vma_iterator vmi;\n\tstruct mempolicy *new;\n\tunsigned long end;\n\tint err;\n\tint ret;\n\tLIST_HEAD(pagelist);\n\n\tif (flags & ~(unsigned long)MPOL_MF_VALID)\n\t\treturn -EINVAL;\n\tif ((flags & MPOL_MF_MOVE_ALL) && !capable(CAP_SYS_NICE))\n\t\treturn -EPERM;\n\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\n\tif (mode == MPOL_DEFAULT)\n\t\tflags &= ~MPOL_MF_STRICT;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\n\tnew = mpol_new(mode, mode_flags, nmask);\n\tif (IS_ERR(new))\n\t\treturn PTR_ERR(new);\n\n\tif (flags & MPOL_MF_LAZY)\n\t\tnew->flags |= MPOL_F_MOF;\n\n\t/*\n\t * If we are using the default policy then operation\n\t * on discontinuous address spaces is okay after all\n\t */\n\tif (!new)\n\t\tflags |= MPOL_MF_DISCONTIG_OK;\n\n\tpr_debug(\"mbind %lx-%lx mode:%d flags:%d nodes:%lx\\n\",\n\t\t start, start + len, mode, mode_flags,\n\t\t nmask ? nodes_addr(*nmask)[0] : NUMA_NO_NODE);\n\n\tif (flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) {\n\n\t\tlru_cache_disable();\n\t}\n\t{\n\t\tNODEMASK_SCRATCH(scratch);\n\t\tif (scratch) {\n\t\t\tmmap_write_lock(mm);\n\t\t\terr = mpol_set_nodemask(new, nmask, scratch);\n\t\t\tif (err)\n\t\t\t\tmmap_write_unlock(mm);\n\t\t} else\n\t\t\terr = -ENOMEM;\n\t\tNODEMASK_SCRATCH_FREE(scratch);\n\t}\n\tif (err)\n\t\tgoto mpol_out;\n\n\t/*\n\t * Lock the VMAs before scanning for pages to migrate, to ensure we don't\n\t * miss a concurrently inserted page.\n\t */\n\tvma_iter_init(&vmi, mm, start);\n\tfor_each_vma_range(vmi, vma, end)\n\t\tvma_start_write(vma);\n\n\tret = queue_pages_range(mm, start, end, nmask,\n\t\t\t  flags | MPOL_MF_INVERT, &pagelist);\n\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto up_out;\n\t}\n\n\tvma_iter_init(&vmi, mm, start);\n\tprev = vma_prev(&vmi);\n\tfor_each_vma_range(vmi, vma, end) {\n\t\terr = mbind_range(&vmi, vma, &prev, start, end, new);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\tif (!err) {\n\t\tint nr_failed = 0;\n\n\t\tif (!list_empty(&pagelist)) {\n\t\t\tWARN_ON_ONCE(flags & MPOL_MF_LAZY);\n\t\t\tnr_failed = migrate_pages(&pagelist, new_folio, NULL,\n\t\t\t\tstart, MIGRATE_SYNC, MR_MEMPOLICY_MBIND, NULL);\n\t\t\tif (nr_failed)\n\t\t\t\tputback_movable_pages(&pagelist);\n\t\t}\n\n\t\tif ((ret > 0) || (nr_failed && (flags & MPOL_MF_STRICT)))\n\t\t\terr = -EIO;\n\t} else {\nup_out:\n\t\tif (!list_empty(&pagelist))\n\t\t\tputback_movable_pages(&pagelist);\n\t}\n\n\tmmap_write_unlock(mm);\nmpol_out:\n\tmpol_put(new);\n\tif (flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL))\n\t\tlru_cache_enable();\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -66,6 +66,14 @@\n \tif (err)\n \t\tgoto mpol_out;\n \n+\t/*\n+\t * Lock the VMAs before scanning for pages to migrate, to ensure we don't\n+\t * miss a concurrently inserted page.\n+\t */\n+\tvma_iter_init(&vmi, mm, start);\n+\tfor_each_vma_range(vmi, vma, end)\n+\t\tvma_start_write(vma);\n+\n \tret = queue_pages_range(mm, start, end, nmask,\n \t\t\t  flags | MPOL_MF_INVERT, &pagelist);\n ",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * Lock the VMAs before scanning for pages to migrate, to ensure we don't",
                "\t * miss a concurrently inserted page.",
                "\t */",
                "\tvma_iter_init(&vmi, mm, start);",
                "\tfor_each_vma_range(vmi, vma, end)",
                "\t\tvma_start_write(vma);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in mm/mempolicy.c in the memory management subsystem in the Linux Kernel. This issue is caused by a race between mbind() and VMA-locked page fault, and may allow a local attacker to crash the system or lead to a kernel information leak.",
        "id": 4234
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->work.identity->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
        "code_after_change": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->task->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,7 @@\n \t\t\tcontinue;\n \t\tif (req->file && req->file->f_op == &io_uring_fops)\n \t\t\treturn true;\n-\t\tif (req->work.identity->files == files)\n+\t\tif (req->task->files == files)\n \t\t\treturn true;\n \t}\n \treturn false;",
        "function_modified_lines": {
            "added": [
                "\t\tif (req->task->files == files)"
            ],
            "deleted": [
                "\t\tif (req->work.identity->files == files)"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3281
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "static int nfc_genl_llc_sdreq(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tstruct nlattr *attr, *sdp_attrs[NFC_SDP_ATTR_MAX+1];\n\tu32 idx;\n\tu8 tid;\n\tchar *uri;\n\tint rc = 0, rem;\n\tsize_t uri_len, tlvs_len;\n\tstruct hlist_head sdreq_list;\n\tstruct nfc_llcp_sdp_tlv *sdreq;\n\n\tif (!info->attrs[NFC_ATTR_DEVICE_INDEX] ||\n\t    !info->attrs[NFC_ATTR_LLC_SDP])\n\t\treturn -EINVAL;\n\n\tidx = nla_get_u32(info->attrs[NFC_ATTR_DEVICE_INDEX]);\n\n\tdev = nfc_get_device(idx);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->dep_link_up == false) {\n\t\trc = -ENOLINK;\n\t\tgoto exit;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (!local) {\n\t\trc = -ENODEV;\n\t\tgoto exit;\n\t}\n\n\tINIT_HLIST_HEAD(&sdreq_list);\n\n\ttlvs_len = 0;\n\n\tnla_for_each_nested(attr, info->attrs[NFC_ATTR_LLC_SDP], rem) {\n\t\trc = nla_parse_nested_deprecated(sdp_attrs, NFC_SDP_ATTR_MAX,\n\t\t\t\t\t\t attr, nfc_sdp_genl_policy,\n\t\t\t\t\t\t info->extack);\n\n\t\tif (rc != 0) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto exit;\n\t\t}\n\n\t\tif (!sdp_attrs[NFC_SDP_ATTR_URI])\n\t\t\tcontinue;\n\n\t\turi_len = nla_len(sdp_attrs[NFC_SDP_ATTR_URI]);\n\t\tif (uri_len == 0)\n\t\t\tcontinue;\n\n\t\turi = nla_data(sdp_attrs[NFC_SDP_ATTR_URI]);\n\t\tif (uri == NULL || *uri == 0)\n\t\t\tcontinue;\n\n\t\ttid = local->sdreq_next_tid++;\n\n\t\tsdreq = nfc_llcp_build_sdreq_tlv(tid, uri, uri_len);\n\t\tif (sdreq == NULL) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\n\t\ttlvs_len += sdreq->tlv_len;\n\n\t\thlist_add_head(&sdreq->node, &sdreq_list);\n\t}\n\n\tif (hlist_empty(&sdreq_list)) {\n\t\trc = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\trc = nfc_llcp_send_snl_sdreq(local, &sdreq_list, tlvs_len);\nexit:\n\tdevice_unlock(&dev->dev);\n\n\tnfc_put_device(dev);\n\n\treturn rc;\n}",
        "code_after_change": "static int nfc_genl_llc_sdreq(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tstruct nlattr *attr, *sdp_attrs[NFC_SDP_ATTR_MAX+1];\n\tu32 idx;\n\tu8 tid;\n\tchar *uri;\n\tint rc = 0, rem;\n\tsize_t uri_len, tlvs_len;\n\tstruct hlist_head sdreq_list;\n\tstruct nfc_llcp_sdp_tlv *sdreq;\n\n\tif (!info->attrs[NFC_ATTR_DEVICE_INDEX] ||\n\t    !info->attrs[NFC_ATTR_LLC_SDP])\n\t\treturn -EINVAL;\n\n\tidx = nla_get_u32(info->attrs[NFC_ATTR_DEVICE_INDEX]);\n\n\tdev = nfc_get_device(idx);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->dep_link_up == false) {\n\t\trc = -ENOLINK;\n\t\tgoto exit;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (!local) {\n\t\trc = -ENODEV;\n\t\tgoto exit;\n\t}\n\n\tINIT_HLIST_HEAD(&sdreq_list);\n\n\ttlvs_len = 0;\n\n\tnla_for_each_nested(attr, info->attrs[NFC_ATTR_LLC_SDP], rem) {\n\t\trc = nla_parse_nested_deprecated(sdp_attrs, NFC_SDP_ATTR_MAX,\n\t\t\t\t\t\t attr, nfc_sdp_genl_policy,\n\t\t\t\t\t\t info->extack);\n\n\t\tif (rc != 0) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto put_local;\n\t\t}\n\n\t\tif (!sdp_attrs[NFC_SDP_ATTR_URI])\n\t\t\tcontinue;\n\n\t\turi_len = nla_len(sdp_attrs[NFC_SDP_ATTR_URI]);\n\t\tif (uri_len == 0)\n\t\t\tcontinue;\n\n\t\turi = nla_data(sdp_attrs[NFC_SDP_ATTR_URI]);\n\t\tif (uri == NULL || *uri == 0)\n\t\t\tcontinue;\n\n\t\ttid = local->sdreq_next_tid++;\n\n\t\tsdreq = nfc_llcp_build_sdreq_tlv(tid, uri, uri_len);\n\t\tif (sdreq == NULL) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto put_local;\n\t\t}\n\n\t\ttlvs_len += sdreq->tlv_len;\n\n\t\thlist_add_head(&sdreq->node, &sdreq_list);\n\t}\n\n\tif (hlist_empty(&sdreq_list)) {\n\t\trc = -EINVAL;\n\t\tgoto put_local;\n\t}\n\n\trc = nfc_llcp_send_snl_sdreq(local, &sdreq_list, tlvs_len);\n\nput_local:\n\tnfc_llcp_local_put(local);\n\nexit:\n\tdevice_unlock(&dev->dev);\n\n\tnfc_put_device(dev);\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -45,7 +45,7 @@\n \n \t\tif (rc != 0) {\n \t\t\trc = -EINVAL;\n-\t\t\tgoto exit;\n+\t\t\tgoto put_local;\n \t\t}\n \n \t\tif (!sdp_attrs[NFC_SDP_ATTR_URI])\n@@ -64,7 +64,7 @@\n \t\tsdreq = nfc_llcp_build_sdreq_tlv(tid, uri, uri_len);\n \t\tif (sdreq == NULL) {\n \t\t\trc = -ENOMEM;\n-\t\t\tgoto exit;\n+\t\t\tgoto put_local;\n \t\t}\n \n \t\ttlvs_len += sdreq->tlv_len;\n@@ -74,10 +74,14 @@\n \n \tif (hlist_empty(&sdreq_list)) {\n \t\trc = -EINVAL;\n-\t\tgoto exit;\n+\t\tgoto put_local;\n \t}\n \n \trc = nfc_llcp_send_snl_sdreq(local, &sdreq_list, tlvs_len);\n+\n+put_local:\n+\tnfc_llcp_local_put(local);\n+\n exit:\n \tdevice_unlock(&dev->dev);\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tgoto put_local;",
                "\t\t\tgoto put_local;",
                "\t\tgoto put_local;",
                "",
                "put_local:",
                "\tnfc_llcp_local_put(local);",
                ""
            ],
            "deleted": [
                "\t\t\tgoto exit;",
                "\t\t\tgoto exit;",
                "\t\tgoto exit;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4157
    },
    {
        "cve_id": "CVE-2019-11487",
        "code_before_change": "void generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n{\n\tget_page(buf->page);\n}",
        "code_after_change": "bool generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n{\n\treturn try_get_page(buf->page);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-void generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n+bool generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n {\n-\tget_page(buf->page);\n+\treturn try_get_page(buf->page);\n }",
        "function_modified_lines": {
            "added": [
                "bool generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)",
                "\treturn try_get_page(buf->page);"
            ],
            "deleted": [
                "void generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)",
                "\tget_page(buf->page);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 5.1-rc5 allows page->_refcount reference count overflow, with resultant use-after-free issues, if about 140 GiB of RAM exists. This is related to fs/fuse/dev.c, fs/pipe.c, fs/splice.c, include/linux/mm.h, include/linux/pipe_fs_i.h, kernel/trace/trace.c, mm/gup.c, and mm/hugetlb.c. It can occur with FUSE requests.",
        "id": 1918
    },
    {
        "cve_id": "CVE-2022-45919",
        "code_before_change": "static int dvb_ca_en50221_io_release(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_ca_private *ca = dvbdev->priv;\n\tint err;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\t/* mark the CA device as closed */\n\tca->open = 0;\n\tdvb_ca_en50221_thread_update_delay(ca);\n\n\terr = dvb_generic_release(inode, file);\n\n\tmodule_put(ca->pub->owner);\n\n\tdvb_ca_private_put(ca);\n\n\treturn err;\n}",
        "code_after_change": "static int dvb_ca_en50221_io_release(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_ca_private *ca = dvbdev->priv;\n\tint err;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\tmutex_lock(&ca->remove_mutex);\n\n\t/* mark the CA device as closed */\n\tca->open = 0;\n\tdvb_ca_en50221_thread_update_delay(ca);\n\n\terr = dvb_generic_release(inode, file);\n\n\tmodule_put(ca->pub->owner);\n\n\tdvb_ca_private_put(ca);\n\n\tif (dvbdev->users == 1 && ca->exit == 1) {\n\t\tmutex_unlock(&ca->remove_mutex);\n\t\twake_up(&dvbdev->wait_queue);\n\t} else {\n\t\tmutex_unlock(&ca->remove_mutex);\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,8 @@\n \tint err;\n \n \tdprintk(\"%s\\n\", __func__);\n+\n+\tmutex_lock(&ca->remove_mutex);\n \n \t/* mark the CA device as closed */\n \tca->open = 0;\n@@ -16,5 +18,12 @@\n \n \tdvb_ca_private_put(ca);\n \n+\tif (dvbdev->users == 1 && ca->exit == 1) {\n+\t\tmutex_unlock(&ca->remove_mutex);\n+\t\twake_up(&dvbdev->wait_queue);\n+\t} else {\n+\t\tmutex_unlock(&ca->remove_mutex);\n+\t}\n+\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tmutex_lock(&ca->remove_mutex);",
                "\tif (dvbdev->users == 1 && ca->exit == 1) {",
                "\t\tmutex_unlock(&ca->remove_mutex);",
                "\t\twake_up(&dvbdev->wait_queue);",
                "\t} else {",
                "\t\tmutex_unlock(&ca->remove_mutex);",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.0.10. In drivers/media/dvb-core/dvb_ca_en50221.c, a use-after-free can occur is there is a disconnect after an open, because of the lack of a wait_event.",
        "id": 3756
    },
    {
        "cve_id": "CVE-2023-32233",
        "code_before_change": "static void nft_objref_map_activate(const struct nft_ctx *ctx,\n\t\t\t\t    const struct nft_expr *expr)\n{\n\tstruct nft_objref_map *priv = nft_expr_priv(expr);\n\n\tpriv->set->use++;\n}",
        "code_after_change": "static void nft_objref_map_activate(const struct nft_ctx *ctx,\n\t\t\t\t    const struct nft_expr *expr)\n{\n\tstruct nft_objref_map *priv = nft_expr_priv(expr);\n\n\tnf_tables_activate_set(ctx, priv->set);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,5 +3,5 @@\n {\n \tstruct nft_objref_map *priv = nft_expr_priv(expr);\n \n-\tpriv->set->use++;\n+\tnf_tables_activate_set(ctx, priv->set);\n }",
        "function_modified_lines": {
            "added": [
                "\tnf_tables_activate_set(ctx, priv->set);"
            ],
            "deleted": [
                "\tpriv->set->use++;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel through 6.3.1, a use-after-free in Netfilter nf_tables when processing batch requests can be abused to perform arbitrary read and write operations on kernel memory. Unprivileged local users can obtain root privileges. This occurs because anonymous sets are mishandled.",
        "id": 4009
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int\ncopy_entries_to_user(unsigned int total_size,\n\t\t     const struct xt_table *table,\n\t\t     void __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct ipt_entry *e;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\tint ret = 0;\n\tconst void *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tunsigned int i;\n\t\tconst struct xt_entry_match *m;\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct ipt_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tfor (i = sizeof(struct ipt_entry);\n\t\t     i < e->target_offset;\n\t\t     i += m->u.match_size) {\n\t\t\tm = (void *)e + i;\n\n\t\t\tif (xt_match_to_user(m, userptr + off + i)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto free_counters;\n\t\t\t}\n\t\t}\n\n\t\tt = ipt_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
        "code_after_change": "static int\ncopy_entries_to_user(unsigned int total_size,\n\t\t     const struct xt_table *table,\n\t\t     void __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct ipt_entry *e;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\tint ret = 0;\n\tconst void *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tunsigned int i;\n\t\tconst struct xt_entry_match *m;\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct ipt_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tfor (i = sizeof(struct ipt_entry);\n\t\t     i < e->target_offset;\n\t\t     i += m->u.match_size) {\n\t\t\tm = (void *)e + i;\n\n\t\t\tif (xt_match_to_user(m, userptr + off + i)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto free_counters;\n\t\t\t}\n\t\t}\n\n\t\tt = ipt_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tunsigned int off, num;\n \tconst struct ipt_entry *e;\n \tstruct xt_counters *counters;\n-\tconst struct xt_table_info *private = table->private;\n+\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n \tint ret = 0;\n \tconst void *loc_cpu_entry;\n ",
        "function_modified_lines": {
            "added": [
                "\tconst struct xt_table_info *private = xt_table_get_private_protected(table);"
            ],
            "deleted": [
                "\tconst struct xt_table_info *private = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2783
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "void hfi1_mmu_rb_unregister(struct mmu_rb_handler *handler)\n{\n\tstruct mmu_rb_node *rbnode;\n\tstruct rb_node *node;\n\tunsigned long flags;\n\tstruct list_head del_list;\n\n\t/* Unregister first so we don't get any more notifications. */\n\tmmu_notifier_unregister(&handler->mn, handler->mm);\n\n\t/*\n\t * Make sure the wq delete handler is finished running.  It will not\n\t * be triggered once the mmu notifiers are unregistered above.\n\t */\n\tflush_work(&handler->del_work);\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\twhile ((node = rb_first_cached(&handler->root))) {\n\t\trbnode = rb_entry(node, struct mmu_rb_node, node);\n\t\trb_erase_cached(node, &handler->root);\n\t\t/* move from LRU list to delete list */\n\t\tlist_move(&rbnode->list, &del_list);\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\tdo_remove(handler, &del_list);\n\n\tkfree(handler);\n}",
        "code_after_change": "void hfi1_mmu_rb_unregister(struct mmu_rb_handler *handler)\n{\n\tstruct mmu_rb_node *rbnode;\n\tstruct rb_node *node;\n\tunsigned long flags;\n\tstruct list_head del_list;\n\n\t/* Unregister first so we don't get any more notifications. */\n\tmmu_notifier_unregister(&handler->mn, handler->mn.mm);\n\n\t/*\n\t * Make sure the wq delete handler is finished running.  It will not\n\t * be triggered once the mmu notifiers are unregistered above.\n\t */\n\tflush_work(&handler->del_work);\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\twhile ((node = rb_first_cached(&handler->root))) {\n\t\trbnode = rb_entry(node, struct mmu_rb_node, node);\n\t\trb_erase_cached(node, &handler->root);\n\t\t/* move from LRU list to delete list */\n\t\tlist_move(&rbnode->list, &del_list);\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\tdo_remove(handler, &del_list);\n\n\tkfree(handler);\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tstruct list_head del_list;\n \n \t/* Unregister first so we don't get any more notifications. */\n-\tmmu_notifier_unregister(&handler->mn, handler->mm);\n+\tmmu_notifier_unregister(&handler->mn, handler->mn.mm);\n \n \t/*\n \t * Make sure the wq delete handler is finished running.  It will not",
        "function_modified_lines": {
            "added": [
                "\tmmu_notifier_unregister(&handler->mn, handler->mn.mm);"
            ],
            "deleted": [
                "\tmmu_notifier_unregister(&handler->mn, handler->mm);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2647
    },
    {
        "cve_id": "CVE-2018-9422",
        "code_before_change": "static int\nget_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(rw, uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\tget_futex_key_refs(key);  /* implies smp_mb(); (B) */\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, 1, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == VERIFY_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\tlock_page(page);\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tmapping = compound_head(page)->mapping;\n\tif (!mapping) {\n\t\tint shmem_swizzled = PageSwapCache(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(fshared)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t} else {\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.inode = mapping->host;\n\t\tkey->shared.pgoff = basepage_index(page);\n\t}\n\n\tget_futex_key_refs(key); /* implies smp_mb(); (B) */\n\nout:\n\tunlock_page(page);\n\tput_page(page);\n\treturn err;\n}",
        "code_after_change": "static int\nget_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(rw, uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\tget_futex_key_refs(key);  /* implies smp_mb(); (B) */\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, 1, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == VERIFY_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t */\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(fshared)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t\tget_futex_key_refs(key); /* implies smp_mb(); (B) */\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\t/*\n\t\t * Take a reference unless it is about to be freed. Previously\n\t\t * this reference was taken by ihold under the page lock\n\t\t * pinning the inode in place so i_lock was unnecessary. The\n\t\t * only way for this check to fail is if the inode was\n\t\t * truncated in parallel so warn for now if this happens.\n\t\t *\n\t\t * We are not calling into get_futex_key_refs() in file-backed\n\t\t * cases, therefore a successful atomic_inc return below will\n\t\t * guarantee that get_futex_key() will still imply smp_mb(); (B).\n\t\t */\n\t\tif (WARN_ON_ONCE(!atomic_inc_not_zero(&inode->i_count))) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\t/* Should be impossible but lets be paranoid for now */\n\t\tif (WARN_ON_ONCE(inode->i_mapping != mapping)) {\n\t\t\terr = -EFAULT;\n\t\t\trcu_read_unlock();\n\t\t\tiput(inode);\n\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.inode = inode;\n\t\tkey->shared.pgoff = basepage_index(page);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -54,7 +54,20 @@\n \telse\n \t\terr = 0;\n \n-\tlock_page(page);\n+\t/*\n+\t * The treatment of mapping from this point on is critical. The page\n+\t * lock protects many things but in this context the page lock\n+\t * stabilizes mapping, prevents inode freeing in the shared\n+\t * file-backed region case and guards against movement to swap cache.\n+\t *\n+\t * Strictly speaking the page lock is not needed in all cases being\n+\t * considered here and page lock forces unnecessarily serialization\n+\t * From this point on, mapping will be re-verified if necessary and\n+\t * page lock will be acquired only if it is unavoidable\n+\t */\n+\tpage = compound_head(page);\n+\tmapping = READ_ONCE(page->mapping);\n+\n \t/*\n \t * If page->mapping is NULL, then it cannot be a PageAnon\n \t * page; but it might be the ZERO_PAGE or in the gate area or\n@@ -70,18 +83,30 @@\n \t * shmem_writepage move it from filecache to swapcache beneath us:\n \t * an unlikely race, but we do need to retry for page->mapping.\n \t */\n-\tmapping = compound_head(page)->mapping;\n-\tif (!mapping) {\n-\t\tint shmem_swizzled = PageSwapCache(page);\n+\tif (unlikely(!mapping)) {\n+\t\tint shmem_swizzled;\n+\n+\t\t/*\n+\t\t * Page lock is required to identify which special case above\n+\t\t * applies. If this is really a shmem page then the page lock\n+\t\t * will prevent unexpected transitions.\n+\t\t */\n+\t\tlock_page(page);\n+\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n \t\tunlock_page(page);\n \t\tput_page(page);\n+\n \t\tif (shmem_swizzled)\n \t\t\tgoto again;\n+\n \t\treturn -EFAULT;\n \t}\n \n \t/*\n \t * Private mappings are handled in a simple way.\n+\t *\n+\t * If the futex key is stored on an anonymous page, then the associated\n+\t * object is the mm which is implicitly pinned by the calling process.\n \t *\n \t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n \t * it's a read-only handle, it's expected that futexes attach to\n@@ -100,16 +125,74 @@\n \t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n \t\tkey->private.mm = mm;\n \t\tkey->private.address = address;\n+\n+\t\tget_futex_key_refs(key); /* implies smp_mb(); (B) */\n+\n \t} else {\n+\t\tstruct inode *inode;\n+\n+\t\t/*\n+\t\t * The associated futex object in this case is the inode and\n+\t\t * the page->mapping must be traversed. Ordinarily this should\n+\t\t * be stabilised under page lock but it's not strictly\n+\t\t * necessary in this case as we just want to pin the inode, not\n+\t\t * update the radix tree or anything like that.\n+\t\t *\n+\t\t * The RCU read lock is taken as the inode is finally freed\n+\t\t * under RCU. If the mapping still matches expectations then the\n+\t\t * mapping->host can be safely accessed as being a valid inode.\n+\t\t */\n+\t\trcu_read_lock();\n+\n+\t\tif (READ_ONCE(page->mapping) != mapping) {\n+\t\t\trcu_read_unlock();\n+\t\t\tput_page(page);\n+\n+\t\t\tgoto again;\n+\t\t}\n+\n+\t\tinode = READ_ONCE(mapping->host);\n+\t\tif (!inode) {\n+\t\t\trcu_read_unlock();\n+\t\t\tput_page(page);\n+\n+\t\t\tgoto again;\n+\t\t}\n+\n+\t\t/*\n+\t\t * Take a reference unless it is about to be freed. Previously\n+\t\t * this reference was taken by ihold under the page lock\n+\t\t * pinning the inode in place so i_lock was unnecessary. The\n+\t\t * only way for this check to fail is if the inode was\n+\t\t * truncated in parallel so warn for now if this happens.\n+\t\t *\n+\t\t * We are not calling into get_futex_key_refs() in file-backed\n+\t\t * cases, therefore a successful atomic_inc return below will\n+\t\t * guarantee that get_futex_key() will still imply smp_mb(); (B).\n+\t\t */\n+\t\tif (WARN_ON_ONCE(!atomic_inc_not_zero(&inode->i_count))) {\n+\t\t\trcu_read_unlock();\n+\t\t\tput_page(page);\n+\n+\t\t\tgoto again;\n+\t\t}\n+\n+\t\t/* Should be impossible but lets be paranoid for now */\n+\t\tif (WARN_ON_ONCE(inode->i_mapping != mapping)) {\n+\t\t\terr = -EFAULT;\n+\t\t\trcu_read_unlock();\n+\t\t\tiput(inode);\n+\n+\t\t\tgoto out;\n+\t\t}\n+\n \t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n-\t\tkey->shared.inode = mapping->host;\n+\t\tkey->shared.inode = inode;\n \t\tkey->shared.pgoff = basepage_index(page);\n+\t\trcu_read_unlock();\n \t}\n \n-\tget_futex_key_refs(key); /* implies smp_mb(); (B) */\n-\n out:\n-\tunlock_page(page);\n \tput_page(page);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * The treatment of mapping from this point on is critical. The page",
                "\t * lock protects many things but in this context the page lock",
                "\t * stabilizes mapping, prevents inode freeing in the shared",
                "\t * file-backed region case and guards against movement to swap cache.",
                "\t *",
                "\t * Strictly speaking the page lock is not needed in all cases being",
                "\t * considered here and page lock forces unnecessarily serialization",
                "\t * From this point on, mapping will be re-verified if necessary and",
                "\t * page lock will be acquired only if it is unavoidable",
                "\t */",
                "\tpage = compound_head(page);",
                "\tmapping = READ_ONCE(page->mapping);",
                "",
                "\tif (unlikely(!mapping)) {",
                "\t\tint shmem_swizzled;",
                "",
                "\t\t/*",
                "\t\t * Page lock is required to identify which special case above",
                "\t\t * applies. If this is really a shmem page then the page lock",
                "\t\t * will prevent unexpected transitions.",
                "\t\t */",
                "\t\tlock_page(page);",
                "\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;",
                "",
                "",
                "\t *",
                "\t * If the futex key is stored on an anonymous page, then the associated",
                "\t * object is the mm which is implicitly pinned by the calling process.",
                "",
                "\t\tget_futex_key_refs(key); /* implies smp_mb(); (B) */",
                "",
                "\t\tstruct inode *inode;",
                "",
                "\t\t/*",
                "\t\t * The associated futex object in this case is the inode and",
                "\t\t * the page->mapping must be traversed. Ordinarily this should",
                "\t\t * be stabilised under page lock but it's not strictly",
                "\t\t * necessary in this case as we just want to pin the inode, not",
                "\t\t * update the radix tree or anything like that.",
                "\t\t *",
                "\t\t * The RCU read lock is taken as the inode is finally freed",
                "\t\t * under RCU. If the mapping still matches expectations then the",
                "\t\t * mapping->host can be safely accessed as being a valid inode.",
                "\t\t */",
                "\t\trcu_read_lock();",
                "",
                "\t\tif (READ_ONCE(page->mapping) != mapping) {",
                "\t\t\trcu_read_unlock();",
                "\t\t\tput_page(page);",
                "",
                "\t\t\tgoto again;",
                "\t\t}",
                "",
                "\t\tinode = READ_ONCE(mapping->host);",
                "\t\tif (!inode) {",
                "\t\t\trcu_read_unlock();",
                "\t\t\tput_page(page);",
                "",
                "\t\t\tgoto again;",
                "\t\t}",
                "",
                "\t\t/*",
                "\t\t * Take a reference unless it is about to be freed. Previously",
                "\t\t * this reference was taken by ihold under the page lock",
                "\t\t * pinning the inode in place so i_lock was unnecessary. The",
                "\t\t * only way for this check to fail is if the inode was",
                "\t\t * truncated in parallel so warn for now if this happens.",
                "\t\t *",
                "\t\t * We are not calling into get_futex_key_refs() in file-backed",
                "\t\t * cases, therefore a successful atomic_inc return below will",
                "\t\t * guarantee that get_futex_key() will still imply smp_mb(); (B).",
                "\t\t */",
                "\t\tif (WARN_ON_ONCE(!atomic_inc_not_zero(&inode->i_count))) {",
                "\t\t\trcu_read_unlock();",
                "\t\t\tput_page(page);",
                "",
                "\t\t\tgoto again;",
                "\t\t}",
                "",
                "\t\t/* Should be impossible but lets be paranoid for now */",
                "\t\tif (WARN_ON_ONCE(inode->i_mapping != mapping)) {",
                "\t\t\terr = -EFAULT;",
                "\t\t\trcu_read_unlock();",
                "\t\t\tiput(inode);",
                "",
                "\t\t\tgoto out;",
                "\t\t}",
                "",
                "\t\tkey->shared.inode = inode;",
                "\t\trcu_read_unlock();"
            ],
            "deleted": [
                "\tlock_page(page);",
                "\tmapping = compound_head(page)->mapping;",
                "\tif (!mapping) {",
                "\t\tint shmem_swizzled = PageSwapCache(page);",
                "\t\tkey->shared.inode = mapping->host;",
                "\tget_futex_key_refs(key); /* implies smp_mb(); (B) */",
                "",
                "\tunlock_page(page);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In get_futex_key of futex.c, there is a use-after-free due to improper locking. This could lead to local escalation of privilege with no additional privileges needed. User interaction is not needed for exploitation. Product: Android Versions: Android kernel Android ID: A-74250718 References: Upstream kernel.",
        "id": 1866
    },
    {
        "cve_id": "CVE-2019-11487",
        "code_before_change": "static int splice_pipe_to_pipe(struct pipe_inode_info *ipipe,\n\t\t\t       struct pipe_inode_info *opipe,\n\t\t\t       size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, nbuf;\n\tbool input_wakeup = false;\n\n\nretry:\n\tret = ipipe_prep(ipipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = opipe_prep(opipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!ipipe->nrbufs && !ipipe->writers)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Cannot make any progress, because either the input\n\t\t * pipe is empty or the output pipe is full.\n\t\t */\n\t\tif (!ipipe->nrbufs || opipe->nrbufs >= opipe->buffers) {\n\t\t\t/* Already processed some buffers, break */\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We raced with another reader/writer and haven't\n\t\t\t * managed to process any buffers.  A zero return\n\t\t\t * value means EOF, so retry instead.\n\t\t\t */\n\t\t\tpipe_unlock(ipipe);\n\t\t\tpipe_unlock(opipe);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tibuf = ipipe->bufs + ipipe->curbuf;\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\t\tobuf = opipe->bufs + nbuf;\n\n\t\tif (len >= ibuf->len) {\n\t\t\t/*\n\t\t\t * Simply move the whole buffer from ipipe to opipe\n\t\t\t */\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\topipe->nrbufs++;\n\t\t\tipipe->curbuf = (ipipe->curbuf + 1) & (ipipe->buffers - 1);\n\t\t\tipipe->nrbufs--;\n\t\t\tinput_wakeup = true;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Get a reference to this pipe buffer,\n\t\t\t * so we can copy the contents over.\n\t\t\t */\n\t\t\tpipe_buf_get(ipipe, ibuf);\n\t\t\t*obuf = *ibuf;\n\n\t\t\t/*\n\t\t\t * Don't inherit the gift flag, we need to\n\t\t\t * prevent multiple steals of this page.\n\t\t\t */\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\t\tobuf->len = len;\n\t\t\topipe->nrbufs++;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t} while (len);\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\tif (input_wakeup)\n\t\twakeup_pipe_writers(ipipe);\n\n\treturn ret;\n}",
        "code_after_change": "static int splice_pipe_to_pipe(struct pipe_inode_info *ipipe,\n\t\t\t       struct pipe_inode_info *opipe,\n\t\t\t       size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, nbuf;\n\tbool input_wakeup = false;\n\n\nretry:\n\tret = ipipe_prep(ipipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = opipe_prep(opipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!ipipe->nrbufs && !ipipe->writers)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Cannot make any progress, because either the input\n\t\t * pipe is empty or the output pipe is full.\n\t\t */\n\t\tif (!ipipe->nrbufs || opipe->nrbufs >= opipe->buffers) {\n\t\t\t/* Already processed some buffers, break */\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We raced with another reader/writer and haven't\n\t\t\t * managed to process any buffers.  A zero return\n\t\t\t * value means EOF, so retry instead.\n\t\t\t */\n\t\t\tpipe_unlock(ipipe);\n\t\t\tpipe_unlock(opipe);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tibuf = ipipe->bufs + ipipe->curbuf;\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\t\tobuf = opipe->bufs + nbuf;\n\n\t\tif (len >= ibuf->len) {\n\t\t\t/*\n\t\t\t * Simply move the whole buffer from ipipe to opipe\n\t\t\t */\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\topipe->nrbufs++;\n\t\t\tipipe->curbuf = (ipipe->curbuf + 1) & (ipipe->buffers - 1);\n\t\t\tipipe->nrbufs--;\n\t\t\tinput_wakeup = true;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Get a reference to this pipe buffer,\n\t\t\t * so we can copy the contents over.\n\t\t\t */\n\t\t\tif (!pipe_buf_get(ipipe, ibuf)) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t*obuf = *ibuf;\n\n\t\t\t/*\n\t\t\t * Don't inherit the gift flag, we need to\n\t\t\t * prevent multiple steals of this page.\n\t\t\t */\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\t\tobuf->len = len;\n\t\t\topipe->nrbufs++;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t} while (len);\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\tif (input_wakeup)\n\t\twakeup_pipe_writers(ipipe);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -77,7 +77,11 @@\n \t\t\t * Get a reference to this pipe buffer,\n \t\t\t * so we can copy the contents over.\n \t\t\t */\n-\t\t\tpipe_buf_get(ipipe, ibuf);\n+\t\t\tif (!pipe_buf_get(ipipe, ibuf)) {\n+\t\t\t\tif (ret == 0)\n+\t\t\t\t\tret = -EFAULT;\n+\t\t\t\tbreak;\n+\t\t\t}\n \t\t\t*obuf = *ibuf;\n \n \t\t\t/*",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (!pipe_buf_get(ipipe, ibuf)) {",
                "\t\t\t\tif (ret == 0)",
                "\t\t\t\t\tret = -EFAULT;",
                "\t\t\t\tbreak;",
                "\t\t\t}"
            ],
            "deleted": [
                "\t\t\tpipe_buf_get(ipipe, ibuf);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 5.1-rc5 allows page->_refcount reference count overflow, with resultant use-after-free issues, if about 140 GiB of RAM exists. This is related to fs/fuse/dev.c, fs/pipe.c, fs/splice.c, include/linux/mm.h, include/linux/pipe_fs_i.h, kernel/trace/trace.c, mm/gup.c, and mm/hugetlb.c. It can occur with FUSE requests.",
        "id": 1919
    },
    {
        "cve_id": "CVE-2023-25012",
        "code_before_change": "static void bigben_set_led(struct led_classdev *led,\n\tenum led_brightness value)\n{\n\tstruct device *dev = led->dev->parent;\n\tstruct hid_device *hid = to_hid_device(dev);\n\tstruct bigben_device *bigben = hid_get_drvdata(hid);\n\tint n;\n\tbool work;\n\tunsigned long flags;\n\n\tif (!bigben) {\n\t\thid_err(hid, \"no device data\\n\");\n\t\treturn;\n\t}\n\n\tfor (n = 0; n < NUM_LEDS; n++) {\n\t\tif (led == bigben->leds[n]) {\n\t\t\tspin_lock_irqsave(&bigben->lock, flags);\n\t\t\tif (value == LED_OFF) {\n\t\t\t\twork = (bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state &= ~BIT(n);\n\t\t\t} else {\n\t\t\t\twork = !(bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state |= BIT(n);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\t\t\tif (work) {\n\t\t\t\tbigben->work_led = true;\n\t\t\t\tschedule_work(&bigben->worker);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n}",
        "code_after_change": "static void bigben_set_led(struct led_classdev *led,\n\tenum led_brightness value)\n{\n\tstruct device *dev = led->dev->parent;\n\tstruct hid_device *hid = to_hid_device(dev);\n\tstruct bigben_device *bigben = hid_get_drvdata(hid);\n\tint n;\n\tbool work;\n\tunsigned long flags;\n\n\tif (!bigben) {\n\t\thid_err(hid, \"no device data\\n\");\n\t\treturn;\n\t}\n\n\tfor (n = 0; n < NUM_LEDS; n++) {\n\t\tif (led == bigben->leds[n]) {\n\t\t\tspin_lock_irqsave(&bigben->lock, flags);\n\t\t\tif (value == LED_OFF) {\n\t\t\t\twork = (bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state &= ~BIT(n);\n\t\t\t} else {\n\t\t\t\twork = !(bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state |= BIT(n);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\t\t\tif (work) {\n\t\t\t\tbigben->work_led = true;\n\t\t\t\tbigben_schedule_work(bigben);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,7 +27,7 @@\n \n \t\t\tif (work) {\n \t\t\t\tbigben->work_led = true;\n-\t\t\t\tschedule_work(&bigben->worker);\n+\t\t\t\tbigben_schedule_work(bigben);\n \t\t\t}\n \t\t\treturn;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tbigben_schedule_work(bigben);"
            ],
            "deleted": [
                "\t\t\t\tschedule_work(&bigben->worker);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel through 6.1.9 has a Use-After-Free in bigben_remove in drivers/hid/hid-bigbenff.c via a crafted USB device because the LED controllers remain registered for too long.",
        "id": 3957
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_buffer_object **vmw_bo_p)\n{\n\tstruct vmw_buffer_object *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tvmw_bo = vmw_user_bo_noref_lookup(sw_context->filp, handle);\n\tif (IS_ERR(vmw_bo)) {\n\t\tVMW_DEBUG_USER(\"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo, false, false);\n\tttm_bo_put(&vmw_bo->base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_buffer_object **vmw_bo_p)\n{\n\tstruct vmw_buffer_object *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo, false, false);\n\tttm_bo_put(&vmw_bo->base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,9 +9,9 @@\n \tint ret;\n \n \tvmw_validation_preload_bo(sw_context->ctx);\n-\tvmw_bo = vmw_user_bo_noref_lookup(sw_context->filp, handle);\n-\tif (IS_ERR(vmw_bo)) {\n-\t\tVMW_DEBUG_USER(\"Could not find or use GMR region.\\n\");\n+\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n+\tif (ret != 0) {\n+\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n \t\treturn PTR_ERR(vmw_bo);\n \t}\n \tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo, false, false);",
        "function_modified_lines": {
            "added": [
                "\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);",
                "\tif (ret != 0) {",
                "\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");"
            ],
            "deleted": [
                "\tvmw_bo = vmw_user_bo_noref_lookup(sw_context->filp, handle);",
                "\tif (IS_ERR(vmw_bo)) {",
                "\t\tVMW_DEBUG_USER(\"Could not find or use GMR region.\\n\");"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3680
    },
    {
        "cve_id": "CVE-2019-15920",
        "code_before_change": "int\nSMB2_read(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t  unsigned int *nbytes, char **buf, int *buf_type)\n{\n\tstruct smb_rqst rqst;\n\tint resp_buftype, rc = -EACCES;\n\tstruct smb2_read_plain_req *req = NULL;\n\tstruct smb2_read_rsp *rsp = NULL;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tunsigned int total_len;\n\tint flags = CIFS_LOG_ERROR;\n\tstruct cifs_ses *ses = io_parms->tcon->ses;\n\n\t*nbytes = 0;\n\trc = smb2_new_read_req((void **)&req, &total_len, io_parms, NULL, 0, 0);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\n\trsp = (struct smb2_read_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\tif (rc != -ENODATA) {\n\t\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_READ_HE);\n\t\t\tcifs_dbg(VFS, \"Send error in read = %d\\n\", rc);\n\t\t\ttrace_smb3_read_err(xid, req->PersistentFileId,\n\t\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t\t    io_parms->offset, io_parms->length,\n\t\t\t\t\t    rc);\n\t\t} else\n\t\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, 0);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t\treturn rc == -ENODATA ? 0 : rc;\n\t} else\n\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, io_parms->length);\n\n\t*nbytes = le32_to_cpu(rsp->DataLength);\n\tif ((*nbytes > CIFS_MAX_MSGSIZE) ||\n\t    (*nbytes > io_parms->length)) {\n\t\tcifs_dbg(FYI, \"bad length %d for count %d\\n\",\n\t\t\t *nbytes, io_parms->length);\n\t\trc = -EIO;\n\t\t*nbytes = 0;\n\t}\n\n\tif (*buf) {\n\t\tmemcpy(*buf, (char *)rsp + rsp->DataOffset, *nbytes);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t} else if (resp_buftype != CIFS_NO_BUFFER) {\n\t\t*buf = rsp_iov.iov_base;\n\t\tif (resp_buftype == CIFS_SMALL_BUFFER)\n\t\t\t*buf_type = CIFS_SMALL_BUFFER;\n\t\telse if (resp_buftype == CIFS_LARGE_BUFFER)\n\t\t\t*buf_type = CIFS_LARGE_BUFFER;\n\t}\n\treturn rc;\n}",
        "code_after_change": "int\nSMB2_read(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t  unsigned int *nbytes, char **buf, int *buf_type)\n{\n\tstruct smb_rqst rqst;\n\tint resp_buftype, rc = -EACCES;\n\tstruct smb2_read_plain_req *req = NULL;\n\tstruct smb2_read_rsp *rsp = NULL;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tunsigned int total_len;\n\tint flags = CIFS_LOG_ERROR;\n\tstruct cifs_ses *ses = io_parms->tcon->ses;\n\n\t*nbytes = 0;\n\trc = smb2_new_read_req((void **)&req, &total_len, io_parms, NULL, 0, 0);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\trsp = (struct smb2_read_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\tif (rc != -ENODATA) {\n\t\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_READ_HE);\n\t\t\tcifs_dbg(VFS, \"Send error in read = %d\\n\", rc);\n\t\t\ttrace_smb3_read_err(xid, req->PersistentFileId,\n\t\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t\t    io_parms->offset, io_parms->length,\n\t\t\t\t\t    rc);\n\t\t} else\n\t\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, 0);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t\treturn rc == -ENODATA ? 0 : rc;\n\t} else\n\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, io_parms->length);\n\n\tcifs_small_buf_release(req);\n\n\t*nbytes = le32_to_cpu(rsp->DataLength);\n\tif ((*nbytes > CIFS_MAX_MSGSIZE) ||\n\t    (*nbytes > io_parms->length)) {\n\t\tcifs_dbg(FYI, \"bad length %d for count %d\\n\",\n\t\t\t *nbytes, io_parms->length);\n\t\trc = -EIO;\n\t\t*nbytes = 0;\n\t}\n\n\tif (*buf) {\n\t\tmemcpy(*buf, (char *)rsp + rsp->DataOffset, *nbytes);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t} else if (resp_buftype != CIFS_NO_BUFFER) {\n\t\t*buf = rsp_iov.iov_base;\n\t\tif (resp_buftype == CIFS_SMALL_BUFFER)\n\t\t\t*buf_type = CIFS_SMALL_BUFFER;\n\t\telse if (resp_buftype == CIFS_LARGE_BUFFER)\n\t\t\t*buf_type = CIFS_LARGE_BUFFER;\n\t}\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,8 +28,6 @@\n \trqst.rq_nvec = 1;\n \n \trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n-\tcifs_small_buf_release(req);\n-\n \trsp = (struct smb2_read_rsp *)rsp_iov.iov_base;\n \n \tif (rc) {\n@@ -50,6 +48,8 @@\n \t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n \t\t\t\t    io_parms->tcon->tid, ses->Suid,\n \t\t\t\t    io_parms->offset, io_parms->length);\n+\n+\tcifs_small_buf_release(req);\n \n \t*nbytes = le32_to_cpu(rsp->DataLength);\n \tif ((*nbytes > CIFS_MAX_MSGSIZE) ||",
        "function_modified_lines": {
            "added": [
                "",
                "\tcifs_small_buf_release(req);"
            ],
            "deleted": [
                "\tcifs_small_buf_release(req);",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.10. SMB2_read in fs/cifs/smb2pdu.c has a use-after-free. NOTE: this was not fixed correctly in 5.0.10; see the 5.0.11 ChangeLog, which documents a memory leak.",
        "id": 2027
    },
    {
        "cve_id": "CVE-2018-10876",
        "code_before_change": "struct buffer_head *\next4_read_block_bitmap_nowait(struct super_block *sb, ext4_group_t block_group)\n{\n\tstruct ext4_group_desc *desc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct buffer_head *bh;\n\text4_fsblk_t bitmap_blk;\n\tint err;\n\n\tdesc = ext4_get_group_desc(sb, block_group, NULL);\n\tif (!desc)\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\tbitmap_blk = ext4_block_bitmap(sb, desc);\n\tif ((bitmap_blk <= le32_to_cpu(sbi->s_es->s_first_data_block)) ||\n\t    (bitmap_blk >= ext4_blocks_count(sbi->s_es))) {\n\t\text4_error(sb, \"Invalid block bitmap block %llu in \"\n\t\t\t   \"block_group %u\", bitmap_blk, block_group);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\t\tEXT4_GROUP_INFO_BBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\t}\n\tbh = sb_getblk(sb, bitmap_blk);\n\tif (unlikely(!bh)) {\n\t\text4_error(sb, \"Cannot get buffer for block bitmap - \"\n\t\t\t   \"block_group = %u, block_bitmap = %llu\",\n\t\t\t   block_group, bitmap_blk);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tif (bitmap_uptodate(bh))\n\t\tgoto verify;\n\n\tlock_buffer(bh);\n\tif (bitmap_uptodate(bh)) {\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\text4_lock_group(sb, block_group);\n\tif (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n\t\terr = ext4_init_block_bitmap(sb, bh, block_group, desc);\n\t\tset_bitmap_uptodate(bh);\n\t\tset_buffer_uptodate(bh);\n\t\tset_buffer_verified(bh);\n\t\text4_unlock_group(sb, block_group);\n\t\tunlock_buffer(bh);\n\t\tif (err) {\n\t\t\text4_error(sb, \"Failed to init block bitmap for group \"\n\t\t\t\t   \"%u: %d\", block_group, err);\n\t\t\tgoto out;\n\t\t}\n\t\tgoto verify;\n\t}\n\text4_unlock_group(sb, block_group);\n\tif (buffer_uptodate(bh)) {\n\t\t/*\n\t\t * if not uninit if bh is uptodate,\n\t\t * bitmap is also uptodate\n\t\t */\n\t\tset_bitmap_uptodate(bh);\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\t/*\n\t * submit the buffer_head for reading\n\t */\n\tset_buffer_new(bh);\n\ttrace_ext4_read_block_bitmap_load(sb, block_group);\n\tbh->b_end_io = ext4_end_bitmap_read;\n\tget_bh(bh);\n\tsubmit_bh(REQ_OP_READ, REQ_META | REQ_PRIO, bh);\n\treturn bh;\nverify:\n\terr = ext4_validate_block_bitmap(sb, desc, block_group, bh);\n\tif (err)\n\t\tgoto out;\n\treturn bh;\nout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "struct buffer_head *\next4_read_block_bitmap_nowait(struct super_block *sb, ext4_group_t block_group)\n{\n\tstruct ext4_group_desc *desc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct buffer_head *bh;\n\text4_fsblk_t bitmap_blk;\n\tint err;\n\n\tdesc = ext4_get_group_desc(sb, block_group, NULL);\n\tif (!desc)\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\tbitmap_blk = ext4_block_bitmap(sb, desc);\n\tif ((bitmap_blk <= le32_to_cpu(sbi->s_es->s_first_data_block)) ||\n\t    (bitmap_blk >= ext4_blocks_count(sbi->s_es))) {\n\t\text4_error(sb, \"Invalid block bitmap block %llu in \"\n\t\t\t   \"block_group %u\", bitmap_blk, block_group);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\t\tEXT4_GROUP_INFO_BBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\t}\n\tbh = sb_getblk(sb, bitmap_blk);\n\tif (unlikely(!bh)) {\n\t\text4_error(sb, \"Cannot get buffer for block bitmap - \"\n\t\t\t   \"block_group = %u, block_bitmap = %llu\",\n\t\t\t   block_group, bitmap_blk);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tif (bitmap_uptodate(bh))\n\t\tgoto verify;\n\n\tlock_buffer(bh);\n\tif (bitmap_uptodate(bh)) {\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\text4_lock_group(sb, block_group);\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {\n\t\tif (block_group == 0) {\n\t\t\text4_unlock_group(sb, block_group);\n\t\t\tunlock_buffer(bh);\n\t\t\text4_error(sb, \"Block bitmap for bg 0 marked \"\n\t\t\t\t   \"uninitialized\");\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t}\n\t\terr = ext4_init_block_bitmap(sb, bh, block_group, desc);\n\t\tset_bitmap_uptodate(bh);\n\t\tset_buffer_uptodate(bh);\n\t\tset_buffer_verified(bh);\n\t\text4_unlock_group(sb, block_group);\n\t\tunlock_buffer(bh);\n\t\tif (err) {\n\t\t\text4_error(sb, \"Failed to init block bitmap for group \"\n\t\t\t\t   \"%u: %d\", block_group, err);\n\t\t\tgoto out;\n\t\t}\n\t\tgoto verify;\n\t}\n\text4_unlock_group(sb, block_group);\n\tif (buffer_uptodate(bh)) {\n\t\t/*\n\t\t * if not uninit if bh is uptodate,\n\t\t * bitmap is also uptodate\n\t\t */\n\t\tset_bitmap_uptodate(bh);\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\t/*\n\t * submit the buffer_head for reading\n\t */\n\tset_buffer_new(bh);\n\ttrace_ext4_read_block_bitmap_load(sb, block_group);\n\tbh->b_end_io = ext4_end_bitmap_read;\n\tget_bh(bh);\n\tsubmit_bh(REQ_OP_READ, REQ_META | REQ_PRIO, bh);\n\treturn bh;\nverify:\n\terr = ext4_validate_block_bitmap(sb, desc, block_group, bh);\n\tif (err)\n\t\tgoto out;\n\treturn bh;\nout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -36,7 +36,16 @@\n \t\tgoto verify;\n \t}\n \text4_lock_group(sb, block_group);\n-\tif (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n+\tif (ext4_has_group_desc_csum(sb) &&\n+\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {\n+\t\tif (block_group == 0) {\n+\t\t\text4_unlock_group(sb, block_group);\n+\t\t\tunlock_buffer(bh);\n+\t\t\text4_error(sb, \"Block bitmap for bg 0 marked \"\n+\t\t\t\t   \"uninitialized\");\n+\t\t\terr = -EFSCORRUPTED;\n+\t\t\tgoto out;\n+\t\t}\n \t\terr = ext4_init_block_bitmap(sb, bh, block_group, desc);\n \t\tset_bitmap_uptodate(bh);\n \t\tset_buffer_uptodate(bh);",
        "function_modified_lines": {
            "added": [
                "\tif (ext4_has_group_desc_csum(sb) &&",
                "\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {",
                "\t\tif (block_group == 0) {",
                "\t\t\text4_unlock_group(sb, block_group);",
                "\t\t\tunlock_buffer(bh);",
                "\t\t\text4_error(sb, \"Block bitmap for bg 0 marked \"",
                "\t\t\t\t   \"uninitialized\");",
                "\t\t\terr = -EFSCORRUPTED;",
                "\t\t\tgoto out;",
                "\t\t}"
            ],
            "deleted": [
                "\tif (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in Linux kernel in the ext4 filesystem code. A use-after-free is possible in ext4_ext_remove_space() function when mounting and operating a crafted ext4 image.",
        "id": 1604
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "void __io_uring_free(struct task_struct *tsk)\n{\n\tstruct io_uring_task *tctx = tsk->io_uring;\n\n\tWARN_ON_ONCE(!xa_empty(&tctx->xa));\n\tWARN_ON_ONCE(refcount_read(&tctx->identity->count) != 1);\n\tif (tctx->identity != &tctx->__identity)\n\t\tkfree(tctx->identity);\n\tpercpu_counter_destroy(&tctx->inflight);\n\tkfree(tctx);\n\ttsk->io_uring = NULL;\n}",
        "code_after_change": "void __io_uring_free(struct task_struct *tsk)\n{\n\tstruct io_uring_task *tctx = tsk->io_uring;\n\n\tWARN_ON_ONCE(!xa_empty(&tctx->xa));\n\tpercpu_counter_destroy(&tctx->inflight);\n\tkfree(tctx);\n\ttsk->io_uring = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,9 +3,6 @@\n \tstruct io_uring_task *tctx = tsk->io_uring;\n \n \tWARN_ON_ONCE(!xa_empty(&tctx->xa));\n-\tWARN_ON_ONCE(refcount_read(&tctx->identity->count) != 1);\n-\tif (tctx->identity != &tctx->__identity)\n-\t\tkfree(tctx->identity);\n \tpercpu_counter_destroy(&tctx->inflight);\n \tkfree(tctx);\n \ttsk->io_uring = NULL;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tWARN_ON_ONCE(refcount_read(&tctx->identity->count) != 1);",
                "\tif (tctx->identity != &tctx->__identity)",
                "\t\tkfree(tctx->identity);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3288
    },
    {
        "cve_id": "CVE-2020-10720",
        "code_before_change": "static struct sk_buff *napi_frags_skb(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\tconst struct ethhdr *eth;\n\tunsigned int hlen = sizeof(*eth);\n\n\tnapi->skb = NULL;\n\n\tskb_reset_mac_header(skb);\n\tskb_gro_reset_offset(skb);\n\n\teth = skb_gro_header_fast(skb, 0);\n\tif (unlikely(skb_gro_header_hard(skb, hlen))) {\n\t\teth = skb_gro_header_slow(skb, hlen, 0);\n\t\tif (unlikely(!eth)) {\n\t\t\tnet_warn_ratelimited(\"%s: dropping impossible skb from %s\\n\",\n\t\t\t\t\t     __func__, napi->dev->name);\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\tgro_pull_from_frag0(skb, hlen);\n\t\tNAPI_GRO_CB(skb)->frag0 += hlen;\n\t\tNAPI_GRO_CB(skb)->frag0_len -= hlen;\n\t}\n\t__skb_pull(skb, hlen);\n\n\t/*\n\t * This works because the only protocols we care about don't require\n\t * special handling.\n\t * We'll fix it up properly in napi_frags_finish()\n\t */\n\tskb->protocol = eth->h_proto;\n\n\treturn skb;\n}",
        "code_after_change": "static struct sk_buff *napi_frags_skb(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\tconst struct ethhdr *eth;\n\tunsigned int hlen = sizeof(*eth);\n\n\tnapi->skb = NULL;\n\n\tskb_reset_mac_header(skb);\n\tskb_gro_reset_offset(skb);\n\n\tif (unlikely(skb_gro_header_hard(skb, hlen))) {\n\t\teth = skb_gro_header_slow(skb, hlen, 0);\n\t\tif (unlikely(!eth)) {\n\t\t\tnet_warn_ratelimited(\"%s: dropping impossible skb from %s\\n\",\n\t\t\t\t\t     __func__, napi->dev->name);\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\teth = (const struct ethhdr *)skb->data;\n\t\tgro_pull_from_frag0(skb, hlen);\n\t\tNAPI_GRO_CB(skb)->frag0 += hlen;\n\t\tNAPI_GRO_CB(skb)->frag0_len -= hlen;\n\t}\n\t__skb_pull(skb, hlen);\n\n\t/*\n\t * This works because the only protocols we care about don't require\n\t * special handling.\n\t * We'll fix it up properly in napi_frags_finish()\n\t */\n\tskb->protocol = eth->h_proto;\n\n\treturn skb;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,6 @@\n \tskb_reset_mac_header(skb);\n \tskb_gro_reset_offset(skb);\n \n-\teth = skb_gro_header_fast(skb, 0);\n \tif (unlikely(skb_gro_header_hard(skb, hlen))) {\n \t\teth = skb_gro_header_slow(skb, hlen, 0);\n \t\tif (unlikely(!eth)) {\n@@ -19,6 +18,7 @@\n \t\t\treturn NULL;\n \t\t}\n \t} else {\n+\t\teth = (const struct ethhdr *)skb->data;\n \t\tgro_pull_from_frag0(skb, hlen);\n \t\tNAPI_GRO_CB(skb)->frag0 += hlen;\n \t\tNAPI_GRO_CB(skb)->frag0_len -= hlen;",
        "function_modified_lines": {
            "added": [
                "\t\teth = (const struct ethhdr *)skb->data;"
            ],
            "deleted": [
                "\teth = skb_gro_header_fast(skb, 0);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's implementation of GRO in versions before 5.2. This flaw allows an attacker with local access to crash the system.",
        "id": 2407
    },
    {
        "cve_id": "CVE-2022-2318",
        "code_before_change": "void rose_start_heartbeat(struct sock *sk)\n{\n\tdel_timer(&sk->sk_timer);\n\n\tsk->sk_timer.function = rose_heartbeat_expiry;\n\tsk->sk_timer.expires  = jiffies + 5 * HZ;\n\n\tadd_timer(&sk->sk_timer);\n}",
        "code_after_change": "void rose_start_heartbeat(struct sock *sk)\n{\n\tsk_stop_timer(sk, &sk->sk_timer);\n\n\tsk->sk_timer.function = rose_heartbeat_expiry;\n\tsk->sk_timer.expires  = jiffies + 5 * HZ;\n\n\tsk_reset_timer(sk, &sk->sk_timer, sk->sk_timer.expires);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,9 +1,9 @@\n void rose_start_heartbeat(struct sock *sk)\n {\n-\tdel_timer(&sk->sk_timer);\n+\tsk_stop_timer(sk, &sk->sk_timer);\n \n \tsk->sk_timer.function = rose_heartbeat_expiry;\n \tsk->sk_timer.expires  = jiffies + 5 * HZ;\n \n-\tadd_timer(&sk->sk_timer);\n+\tsk_reset_timer(sk, &sk->sk_timer, sk->sk_timer.expires);\n }",
        "function_modified_lines": {
            "added": [
                "\tsk_stop_timer(sk, &sk->sk_timer);",
                "\tsk_reset_timer(sk, &sk->sk_timer, sk->sk_timer.expires);"
            ],
            "deleted": [
                "\tdel_timer(&sk->sk_timer);",
                "\tadd_timer(&sk->sk_timer);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities caused by timer handler in net/rose/rose_timer.c of linux that allow attackers to crash linux kernel without any privileges.",
        "id": 3433
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static ssize_t aio_read(struct kiocb *req, const struct iocb *iocb,\n\t\t\tbool vectored, bool compat)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct iov_iter iter;\n\tstruct file *file;\n\tssize_t ret;\n\n\tret = aio_prep_rw(req, iocb);\n\tif (ret)\n\t\treturn ret;\n\tfile = req->ki_filp;\n\n\tret = -EBADF;\n\tif (unlikely(!(file->f_mode & FMODE_READ)))\n\t\tgoto out_fput;\n\tret = -EINVAL;\n\tif (unlikely(!file->f_op->read_iter))\n\t\tgoto out_fput;\n\n\tret = aio_setup_rw(READ, iocb, &iovec, vectored, compat, &iter);\n\tif (ret)\n\t\tgoto out_fput;\n\tret = rw_verify_area(READ, file, &req->ki_pos, iov_iter_count(&iter));\n\tif (!ret)\n\t\taio_rw_done(req, call_read_iter(file, req, &iter));\n\tkfree(iovec);\nout_fput:\n\tif (unlikely(ret))\n\t\tfput(file);\n\treturn ret;\n}",
        "code_after_change": "static ssize_t aio_read(struct kiocb *req, const struct iocb *iocb,\n\t\t\tbool vectored, bool compat)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct iov_iter iter;\n\tstruct file *file;\n\tssize_t ret;\n\n\tret = aio_prep_rw(req, iocb);\n\tif (ret)\n\t\treturn ret;\n\tfile = req->ki_filp;\n\tif (unlikely(!(file->f_mode & FMODE_READ)))\n\t\treturn -EBADF;\n\tret = -EINVAL;\n\tif (unlikely(!file->f_op->read_iter))\n\t\treturn -EINVAL;\n\n\tret = aio_setup_rw(READ, iocb, &iovec, vectored, compat, &iter);\n\tif (ret)\n\t\treturn ret;\n\tret = rw_verify_area(READ, file, &req->ki_pos, iov_iter_count(&iter));\n\tif (!ret)\n\t\taio_rw_done(req, call_read_iter(file, req, &iter));\n\tkfree(iovec);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,23 +10,18 @@\n \tif (ret)\n \t\treturn ret;\n \tfile = req->ki_filp;\n-\n-\tret = -EBADF;\n \tif (unlikely(!(file->f_mode & FMODE_READ)))\n-\t\tgoto out_fput;\n+\t\treturn -EBADF;\n \tret = -EINVAL;\n \tif (unlikely(!file->f_op->read_iter))\n-\t\tgoto out_fput;\n+\t\treturn -EINVAL;\n \n \tret = aio_setup_rw(READ, iocb, &iovec, vectored, compat, &iter);\n \tif (ret)\n-\t\tgoto out_fput;\n+\t\treturn ret;\n \tret = rw_verify_area(READ, file, &req->ki_pos, iov_iter_count(&iter));\n \tif (!ret)\n \t\taio_rw_done(req, call_read_iter(file, req, &iter));\n \tkfree(iovec);\n-out_fput:\n-\tif (unlikely(ret))\n-\t\tfput(file);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t\treturn -EBADF;",
                "\t\treturn -EINVAL;",
                "\t\treturn ret;"
            ],
            "deleted": [
                "",
                "\tret = -EBADF;",
                "\t\tgoto out_fput;",
                "\t\tgoto out_fput;",
                "\t\tgoto out_fput;",
                "out_fput:",
                "\tif (unlikely(ret))",
                "\t\tfput(file);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1893
    },
    {
        "cve_id": "CVE-2019-19767",
        "code_before_change": "static void ext4_clamp_want_extra_isize(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\n\t/* determine the minimum size of new large inodes, if present */\n\tif (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    sbi->s_want_extra_isize == 0) {\n\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\t     EXT4_GOOD_OLD_INODE_SIZE;\n\t\tif (ext4_has_feature_extra_isize(sb)) {\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_want_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_want_extra_isize);\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_min_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_min_extra_isize);\n\t\t}\n\t}\n\t/* Check if enough inode space is available */\n\tif (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >\n\t\t\t\t\t\t\tsbi->s_inode_size) {\n\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\t       EXT4_GOOD_OLD_INODE_SIZE;\n\t\text4_msg(sb, KERN_INFO,\n\t\t\t \"required extra inode space not available\");\n\t}\n}",
        "code_after_change": "static void ext4_clamp_want_extra_isize(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tunsigned def_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\tEXT4_GOOD_OLD_INODE_SIZE;\n\n\tif (sbi->s_inode_size == EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tsbi->s_want_extra_isize = 0;\n\t\treturn;\n\t}\n\tif (sbi->s_want_extra_isize < 4) {\n\t\tsbi->s_want_extra_isize = def_extra_isize;\n\t\tif (ext4_has_feature_extra_isize(sb)) {\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_want_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_want_extra_isize);\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_min_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_min_extra_isize);\n\t\t}\n\t}\n\t/* Check if enough inode space is available */\n\tif ((sbi->s_want_extra_isize > sbi->s_inode_size) ||\n\t    (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >\n\t\t\t\t\t\t\tsbi->s_inode_size)) {\n\t\tsbi->s_want_extra_isize = def_extra_isize;\n\t\text4_msg(sb, KERN_INFO,\n\t\t\t \"required extra inode space not available\");\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,12 +2,15 @@\n {\n \tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n \tstruct ext4_super_block *es = sbi->s_es;\n+\tunsigned def_extra_isize = sizeof(struct ext4_inode) -\n+\t\t\t\t\t\tEXT4_GOOD_OLD_INODE_SIZE;\n \n-\t/* determine the minimum size of new large inodes, if present */\n-\tif (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE &&\n-\t    sbi->s_want_extra_isize == 0) {\n-\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n-\t\t\t\t\t\t     EXT4_GOOD_OLD_INODE_SIZE;\n+\tif (sbi->s_inode_size == EXT4_GOOD_OLD_INODE_SIZE) {\n+\t\tsbi->s_want_extra_isize = 0;\n+\t\treturn;\n+\t}\n+\tif (sbi->s_want_extra_isize < 4) {\n+\t\tsbi->s_want_extra_isize = def_extra_isize;\n \t\tif (ext4_has_feature_extra_isize(sb)) {\n \t\t\tif (sbi->s_want_extra_isize <\n \t\t\t    le16_to_cpu(es->s_want_extra_isize))\n@@ -20,10 +23,10 @@\n \t\t}\n \t}\n \t/* Check if enough inode space is available */\n-\tif (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >\n-\t\t\t\t\t\t\tsbi->s_inode_size) {\n-\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n-\t\t\t\t\t\t       EXT4_GOOD_OLD_INODE_SIZE;\n+\tif ((sbi->s_want_extra_isize > sbi->s_inode_size) ||\n+\t    (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >\n+\t\t\t\t\t\t\tsbi->s_inode_size)) {\n+\t\tsbi->s_want_extra_isize = def_extra_isize;\n \t\text4_msg(sb, KERN_INFO,\n \t\t\t \"required extra inode space not available\");\n \t}",
        "function_modified_lines": {
            "added": [
                "\tunsigned def_extra_isize = sizeof(struct ext4_inode) -",
                "\t\t\t\t\t\tEXT4_GOOD_OLD_INODE_SIZE;",
                "\tif (sbi->s_inode_size == EXT4_GOOD_OLD_INODE_SIZE) {",
                "\t\tsbi->s_want_extra_isize = 0;",
                "\t\treturn;",
                "\t}",
                "\tif (sbi->s_want_extra_isize < 4) {",
                "\t\tsbi->s_want_extra_isize = def_extra_isize;",
                "\tif ((sbi->s_want_extra_isize > sbi->s_inode_size) ||",
                "\t    (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >",
                "\t\t\t\t\t\t\tsbi->s_inode_size)) {",
                "\t\tsbi->s_want_extra_isize = def_extra_isize;"
            ],
            "deleted": [
                "\t/* determine the minimum size of new large inodes, if present */",
                "\tif (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE &&",
                "\t    sbi->s_want_extra_isize == 0) {",
                "\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -",
                "\t\t\t\t\t\t     EXT4_GOOD_OLD_INODE_SIZE;",
                "\tif (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >",
                "\t\t\t\t\t\t\tsbi->s_inode_size) {",
                "\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -",
                "\t\t\t\t\t\t       EXT4_GOOD_OLD_INODE_SIZE;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 5.4.2 mishandles ext4_expand_extra_isize, as demonstrated by use-after-free errors in __ext4_expand_extra_isize and ext4_xattr_set_entry, related to fs/ext4/inode.c and fs/ext4/super.c, aka CID-4ea99936a163.",
        "id": 2224
    },
    {
        "cve_id": "CVE-2020-7053",
        "code_before_change": "static int context_idr_cleanup(int id, void *p, void *data)\n{\n\tstruct i915_gem_context *ctx = p;\n\n\tcontext_close(ctx);\n\treturn 0;\n}",
        "code_after_change": "static int context_idr_cleanup(int id, void *p, void *data)\n{\n\tcontext_close(p);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,5 @@\n static int context_idr_cleanup(int id, void *p, void *data)\n {\n-\tstruct i915_gem_context *ctx = p;\n-\n-\tcontext_close(ctx);\n+\tcontext_close(p);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tcontext_close(p);"
            ],
            "deleted": [
                "\tstruct i915_gem_context *ctx = p;",
                "",
                "\tcontext_close(ctx);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 4.14 longterm through 4.14.165 and 4.19 longterm through 4.19.96 (and 5.x before 5.2), there is a use-after-free (write) in the i915_ppgtt_close function in drivers/gpu/drm/i915/i915_gem_gtt.c, aka CID-7dc40713618c. This is related to i915_gem_context_destroy_ioctl in drivers/gpu/drm/i915/i915_gem_context.c.",
        "id": 2802
    },
    {
        "cve_id": "CVE-2023-3610",
        "code_before_change": "static void nf_tables_rule_destroy(const struct nft_ctx *ctx,\n\t\t\t\t   struct nft_rule *rule)\n{\n\tstruct nft_expr *expr, *next;\n\n\t/*\n\t * Careful: some expressions might not be initialized in case this\n\t * is called on error from nf_tables_newrule().\n\t */\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tnext = nft_expr_next(expr);\n\t\tnf_tables_expr_destroy(ctx, expr);\n\t\texpr = next;\n\t}\n\tkfree(rule);\n}",
        "code_after_change": "void nf_tables_rule_destroy(const struct nft_ctx *ctx, struct nft_rule *rule)\n{\n\tstruct nft_expr *expr, *next;\n\n\t/*\n\t * Careful: some expressions might not be initialized in case this\n\t * is called on error from nf_tables_newrule().\n\t */\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tnext = nft_expr_next(expr);\n\t\tnf_tables_expr_destroy(ctx, expr);\n\t\texpr = next;\n\t}\n\tkfree(rule);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,4 @@\n-static void nf_tables_rule_destroy(const struct nft_ctx *ctx,\n-\t\t\t\t   struct nft_rule *rule)\n+void nf_tables_rule_destroy(const struct nft_ctx *ctx, struct nft_rule *rule)\n {\n \tstruct nft_expr *expr, *next;\n ",
        "function_modified_lines": {
            "added": [
                "void nf_tables_rule_destroy(const struct nft_ctx *ctx, struct nft_rule *rule)"
            ],
            "deleted": [
                "static void nf_tables_rule_destroy(const struct nft_ctx *ctx,",
                "\t\t\t\t   struct nft_rule *rule)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nFlaw in the error handling of bound chains causes a use-after-free in the abort path of NFT_MSG_NEWRULE. The vulnerability requires CAP_NET_ADMIN to be triggered.\n\nWe recommend upgrading past commit 4bedf9eee016286c835e3d8fa981ddece5338795.\n\n",
        "id": 4123
    },
    {
        "cve_id": "CVE-2019-11487",
        "code_before_change": "static inline void pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf)\n{\n\tbuf->ops->get(pipe, buf);\n}",
        "code_after_change": "static inline __must_check bool pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf)\n{\n\treturn buf->ops->get(pipe, buf);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,5 @@\n-static inline void pipe_buf_get(struct pipe_inode_info *pipe,\n+static inline __must_check bool pipe_buf_get(struct pipe_inode_info *pipe,\n \t\t\t\tstruct pipe_buffer *buf)\n {\n-\tbuf->ops->get(pipe, buf);\n+\treturn buf->ops->get(pipe, buf);\n }",
        "function_modified_lines": {
            "added": [
                "static inline __must_check bool pipe_buf_get(struct pipe_inode_info *pipe,",
                "\treturn buf->ops->get(pipe, buf);"
            ],
            "deleted": [
                "static inline void pipe_buf_get(struct pipe_inode_info *pipe,",
                "\tbuf->ops->get(pipe, buf);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 5.1-rc5 allows page->_refcount reference count overflow, with resultant use-after-free issues, if about 140 GiB of RAM exists. This is related to fs/fuse/dev.c, fs/pipe.c, fs/splice.c, include/linux/mm.h, include/linux/pipe_fs_i.h, kernel/trace/trace.c, mm/gup.c, and mm/hugetlb.c. It can occur with FUSE requests.",
        "id": 1921
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n{\n\tstruct io_submit_state *state;\n\tunsigned int sqe_flags;\n\tint id, ret = 0;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags = sqe_flags = READ_ONCE(sqe->flags);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->link = NULL;\n\treq->fixed_rsrc_refs = NULL;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS)) {\n\t\treq->flags = 0;\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm_files(ctx, req)))\n\t\treturn -EFAULT;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\tstruct io_identity *iod;\n\n\t\tiod = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!iod))\n\t\t\treturn -EINVAL;\n\t\trefcount_inc(&iod->count);\n\n\t\t__io_req_init_async(req);\n\t\tget_cred(iod->creds);\n\t\treq->work.identity = iod;\n\t}\n\n\tstate = &ctx->submit_state;\n\n\t/*\n\t * Plug now if we have more than 1 IO left after this, and the target\n\t * is potentially a read/write to block based storage.\n\t */\n\tif (!state->plug_started && state->ios_left > 1 &&\n\t    io_op_defs[req->opcode].plug) {\n\t\tblk_start_plug(&state->plug);\n\t\tstate->plug_started = true;\n\t}\n\n\tif (io_op_defs[req->opcode].needs_file) {\n\t\tbool fixed = req->flags & REQ_F_FIXED_FILE;\n\n\t\treq->file = io_file_get(state, req, READ_ONCE(sqe->fd), fixed);\n\t\tif (unlikely(!req->file))\n\t\t\tret = -EBADF;\n\t}\n\n\tstate->ios_left--;\n\treturn ret;\n}",
        "code_after_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n{\n\tstruct io_submit_state *state;\n\tunsigned int sqe_flags;\n\tint id, ret = 0;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags = sqe_flags = READ_ONCE(sqe->flags);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->link = NULL;\n\treq->fixed_rsrc_refs = NULL;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS)) {\n\t\treq->flags = 0;\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm_files(ctx, req)))\n\t\treturn -EFAULT;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\t__io_req_init_async(req);\n\t\treq->work.creds = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!req->work.creds))\n\t\t\treturn -EINVAL;\n\t\tget_cred(req->work.creds);\n\t}\n\n\tstate = &ctx->submit_state;\n\n\t/*\n\t * Plug now if we have more than 1 IO left after this, and the target\n\t * is potentially a read/write to block based storage.\n\t */\n\tif (!state->plug_started && state->ios_left > 1 &&\n\t    io_op_defs[req->opcode].plug) {\n\t\tblk_start_plug(&state->plug);\n\t\tstate->plug_started = true;\n\t}\n\n\tif (io_op_defs[req->opcode].needs_file) {\n\t\tbool fixed = req->flags & REQ_F_FIXED_FILE;\n\n\t\treq->file = io_file_get(state, req, READ_ONCE(sqe->fd), fixed);\n\t\tif (unlikely(!req->file))\n\t\t\tret = -EBADF;\n\t}\n\n\tstate->ios_left--;\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -40,16 +40,11 @@\n \n \tid = READ_ONCE(sqe->personality);\n \tif (id) {\n-\t\tstruct io_identity *iod;\n-\n-\t\tiod = idr_find(&ctx->personality_idr, id);\n-\t\tif (unlikely(!iod))\n+\t\t__io_req_init_async(req);\n+\t\treq->work.creds = idr_find(&ctx->personality_idr, id);\n+\t\tif (unlikely(!req->work.creds))\n \t\t\treturn -EINVAL;\n-\t\trefcount_inc(&iod->count);\n-\n-\t\t__io_req_init_async(req);\n-\t\tget_cred(iod->creds);\n-\t\treq->work.identity = iod;\n+\t\tget_cred(req->work.creds);\n \t}\n \n \tstate = &ctx->submit_state;",
        "function_modified_lines": {
            "added": [
                "\t\t__io_req_init_async(req);",
                "\t\treq->work.creds = idr_find(&ctx->personality_idr, id);",
                "\t\tif (unlikely(!req->work.creds))",
                "\t\tget_cred(req->work.creds);"
            ],
            "deleted": [
                "\t\tstruct io_identity *iod;",
                "",
                "\t\tiod = idr_find(&ctx->personality_idr, id);",
                "\t\tif (unlikely(!iod))",
                "\t\trefcount_inc(&iod->count);",
                "",
                "\t\t__io_req_init_async(req);",
                "\t\tget_cred(iod->creds);",
                "\t\treq->work.identity = iod;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3286
    },
    {
        "cve_id": "CVE-2021-3348",
        "code_before_change": "static int nbd_add_socket(struct nbd_device *nbd, unsigned long arg,\n\t\t\t  bool netlink)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct socket *sock;\n\tstruct nbd_sock **socks;\n\tstruct nbd_sock *nsock;\n\tint err;\n\n\tsock = nbd_get_socket(nbd, arg, &err);\n\tif (!sock)\n\t\treturn err;\n\n\tif (!netlink && !nbd->task_setup &&\n\t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n\t\tnbd->task_setup = current;\n\n\tif (!netlink &&\n\t    (nbd->task_setup != current ||\n\t     test_bit(NBD_RT_BOUND, &config->runtime_flags))) {\n\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\"Device being setup by another task\");\n\t\terr = -EBUSY;\n\t\tgoto put_socket;\n\t}\n\n\tnsock = kzalloc(sizeof(*nsock), GFP_KERNEL);\n\tif (!nsock) {\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tsocks = krealloc(config->socks, (config->num_connections + 1) *\n\t\t\t sizeof(struct nbd_sock *), GFP_KERNEL);\n\tif (!socks) {\n\t\tkfree(nsock);\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tconfig->socks = socks;\n\n\tnsock->fallback_index = -1;\n\tnsock->dead = false;\n\tmutex_init(&nsock->tx_lock);\n\tnsock->sock = sock;\n\tnsock->pending = NULL;\n\tnsock->sent = 0;\n\tnsock->cookie = 0;\n\tsocks[config->num_connections++] = nsock;\n\tatomic_inc(&config->live_connections);\n\n\treturn 0;\n\nput_socket:\n\tsockfd_put(sock);\n\treturn err;\n}",
        "code_after_change": "static int nbd_add_socket(struct nbd_device *nbd, unsigned long arg,\n\t\t\t  bool netlink)\n{\n\tstruct nbd_config *config = nbd->config;\n\tstruct socket *sock;\n\tstruct nbd_sock **socks;\n\tstruct nbd_sock *nsock;\n\tint err;\n\n\tsock = nbd_get_socket(nbd, arg, &err);\n\tif (!sock)\n\t\treturn err;\n\n\t/*\n\t * We need to make sure we don't get any errant requests while we're\n\t * reallocating the ->socks array.\n\t */\n\tblk_mq_freeze_queue(nbd->disk->queue);\n\n\tif (!netlink && !nbd->task_setup &&\n\t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n\t\tnbd->task_setup = current;\n\n\tif (!netlink &&\n\t    (nbd->task_setup != current ||\n\t     test_bit(NBD_RT_BOUND, &config->runtime_flags))) {\n\t\tdev_err(disk_to_dev(nbd->disk),\n\t\t\t\"Device being setup by another task\");\n\t\terr = -EBUSY;\n\t\tgoto put_socket;\n\t}\n\n\tnsock = kzalloc(sizeof(*nsock), GFP_KERNEL);\n\tif (!nsock) {\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tsocks = krealloc(config->socks, (config->num_connections + 1) *\n\t\t\t sizeof(struct nbd_sock *), GFP_KERNEL);\n\tif (!socks) {\n\t\tkfree(nsock);\n\t\terr = -ENOMEM;\n\t\tgoto put_socket;\n\t}\n\n\tconfig->socks = socks;\n\n\tnsock->fallback_index = -1;\n\tnsock->dead = false;\n\tmutex_init(&nsock->tx_lock);\n\tnsock->sock = sock;\n\tnsock->pending = NULL;\n\tnsock->sent = 0;\n\tnsock->cookie = 0;\n\tsocks[config->num_connections++] = nsock;\n\tatomic_inc(&config->live_connections);\n\tblk_mq_unfreeze_queue(nbd->disk->queue);\n\n\treturn 0;\n\nput_socket:\n\tblk_mq_unfreeze_queue(nbd->disk->queue);\n\tsockfd_put(sock);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,12 @@\n \tsock = nbd_get_socket(nbd, arg, &err);\n \tif (!sock)\n \t\treturn err;\n+\n+\t/*\n+\t * We need to make sure we don't get any errant requests while we're\n+\t * reallocating the ->socks array.\n+\t */\n+\tblk_mq_freeze_queue(nbd->disk->queue);\n \n \tif (!netlink && !nbd->task_setup &&\n \t    !test_bit(NBD_RT_BOUND, &config->runtime_flags))\n@@ -49,10 +55,12 @@\n \tnsock->cookie = 0;\n \tsocks[config->num_connections++] = nsock;\n \tatomic_inc(&config->live_connections);\n+\tblk_mq_unfreeze_queue(nbd->disk->queue);\n \n \treturn 0;\n \n put_socket:\n+\tblk_mq_unfreeze_queue(nbd->disk->queue);\n \tsockfd_put(sock);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * We need to make sure we don't get any errant requests while we're",
                "\t * reallocating the ->socks array.",
                "\t */",
                "\tblk_mq_freeze_queue(nbd->disk->queue);",
                "\tblk_mq_unfreeze_queue(nbd->disk->queue);",
                "\tblk_mq_unfreeze_queue(nbd->disk->queue);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "nbd_add_socket in drivers/block/nbd.c in the Linux kernel through 5.10.12 has an ndb_queue_rq use-after-free that could be triggered by local attackers (with access to the nbd device) via an I/O request at a certain point during device setup, aka CID-b98e762e3d71.",
        "id": 2980
    },
    {
        "cve_id": "CVE-2014-2851",
        "code_before_change": "int ping_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tkgid_t group = current_egid();\n\tstruct group_info *group_info = get_current_groups();\n\tint i, j, count = group_info->ngroups;\n\tkgid_t low, high;\n\n\tinet_get_ping_group_range_net(net, &low, &high);\n\tif (gid_lte(low, group) && gid_lte(group, high))\n\t\treturn 0;\n\n\tfor (i = 0; i < group_info->nblocks; i++) {\n\t\tint cp_count = min_t(int, NGROUPS_PER_BLOCK, count);\n\t\tfor (j = 0; j < cp_count; j++) {\n\t\t\tkgid_t gid = group_info->blocks[i][j];\n\t\t\tif (gid_lte(low, gid) && gid_lte(gid, high))\n\t\t\t\treturn 0;\n\t\t}\n\n\t\tcount -= cp_count;\n\t}\n\n\treturn -EACCES;\n}",
        "code_after_change": "int ping_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tkgid_t group = current_egid();\n\tstruct group_info *group_info;\n\tint i, j, count;\n\tkgid_t low, high;\n\tint ret = 0;\n\n\tinet_get_ping_group_range_net(net, &low, &high);\n\tif (gid_lte(low, group) && gid_lte(group, high))\n\t\treturn 0;\n\n\tgroup_info = get_current_groups();\n\tcount = group_info->ngroups;\n\tfor (i = 0; i < group_info->nblocks; i++) {\n\t\tint cp_count = min_t(int, NGROUPS_PER_BLOCK, count);\n\t\tfor (j = 0; j < cp_count; j++) {\n\t\t\tkgid_t gid = group_info->blocks[i][j];\n\t\t\tif (gid_lte(low, gid) && gid_lte(gid, high))\n\t\t\t\tgoto out_release_group;\n\t\t}\n\n\t\tcount -= cp_count;\n\t}\n\n\tret = -EACCES;\n\nout_release_group:\n\tput_group_info(group_info);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,24 +2,31 @@\n {\n \tstruct net *net = sock_net(sk);\n \tkgid_t group = current_egid();\n-\tstruct group_info *group_info = get_current_groups();\n-\tint i, j, count = group_info->ngroups;\n+\tstruct group_info *group_info;\n+\tint i, j, count;\n \tkgid_t low, high;\n+\tint ret = 0;\n \n \tinet_get_ping_group_range_net(net, &low, &high);\n \tif (gid_lte(low, group) && gid_lte(group, high))\n \t\treturn 0;\n \n+\tgroup_info = get_current_groups();\n+\tcount = group_info->ngroups;\n \tfor (i = 0; i < group_info->nblocks; i++) {\n \t\tint cp_count = min_t(int, NGROUPS_PER_BLOCK, count);\n \t\tfor (j = 0; j < cp_count; j++) {\n \t\t\tkgid_t gid = group_info->blocks[i][j];\n \t\t\tif (gid_lte(low, gid) && gid_lte(gid, high))\n-\t\t\t\treturn 0;\n+\t\t\t\tgoto out_release_group;\n \t\t}\n \n \t\tcount -= cp_count;\n \t}\n \n-\treturn -EACCES;\n+\tret = -EACCES;\n+\n+out_release_group:\n+\tput_group_info(group_info);\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct group_info *group_info;",
                "\tint i, j, count;",
                "\tint ret = 0;",
                "\tgroup_info = get_current_groups();",
                "\tcount = group_info->ngroups;",
                "\t\t\t\tgoto out_release_group;",
                "\tret = -EACCES;",
                "",
                "out_release_group:",
                "\tput_group_info(group_info);",
                "\treturn ret;"
            ],
            "deleted": [
                "\tstruct group_info *group_info = get_current_groups();",
                "\tint i, j, count = group_info->ngroups;",
                "\t\t\t\treturn 0;",
                "\treturn -EACCES;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Integer overflow in the ping_init_sock function in net/ipv4/ping.c in the Linux kernel through 3.14.1 allows local users to cause a denial of service (use-after-free and system crash) or possibly gain privileges via a crafted application that leverages an improperly managed reference counter.",
        "id": 498
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static int io_register_personality(struct io_ring_ctx *ctx)\n{\n\tstruct io_identity *id;\n\tint ret;\n\n\tid = kmalloc(sizeof(*id), GFP_KERNEL);\n\tif (unlikely(!id))\n\t\treturn -ENOMEM;\n\n\tio_init_identity(id);\n\tid->creds = get_current_cred();\n\n\tret = idr_alloc_cyclic(&ctx->personality_idr, id, 1, USHRT_MAX, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tput_cred(id->creds);\n\t\tkfree(id);\n\t}\n\treturn ret;\n}",
        "code_after_change": "static int io_register_personality(struct io_ring_ctx *ctx)\n{\n\tconst struct cred *creds;\n\tint ret;\n\n\tcreds = get_current_cred();\n\n\tret = idr_alloc_cyclic(&ctx->personality_idr, (void *) creds, 1,\n\t\t\t\tUSHRT_MAX, GFP_KERNEL);\n\tif (ret < 0)\n\t\tput_cred(creds);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,19 +1,13 @@\n static int io_register_personality(struct io_ring_ctx *ctx)\n {\n-\tstruct io_identity *id;\n+\tconst struct cred *creds;\n \tint ret;\n \n-\tid = kmalloc(sizeof(*id), GFP_KERNEL);\n-\tif (unlikely(!id))\n-\t\treturn -ENOMEM;\n+\tcreds = get_current_cred();\n \n-\tio_init_identity(id);\n-\tid->creds = get_current_cred();\n-\n-\tret = idr_alloc_cyclic(&ctx->personality_idr, id, 1, USHRT_MAX, GFP_KERNEL);\n-\tif (ret < 0) {\n-\t\tput_cred(id->creds);\n-\t\tkfree(id);\n-\t}\n+\tret = idr_alloc_cyclic(&ctx->personality_idr, (void *) creds, 1,\n+\t\t\t\tUSHRT_MAX, GFP_KERNEL);\n+\tif (ret < 0)\n+\t\tput_cred(creds);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tconst struct cred *creds;",
                "\tcreds = get_current_cred();",
                "\tret = idr_alloc_cyclic(&ctx->personality_idr, (void *) creds, 1,",
                "\t\t\t\tUSHRT_MAX, GFP_KERNEL);",
                "\tif (ret < 0)",
                "\t\tput_cred(creds);"
            ],
            "deleted": [
                "\tstruct io_identity *id;",
                "\tid = kmalloc(sizeof(*id), GFP_KERNEL);",
                "\tif (unlikely(!id))",
                "\t\treturn -ENOMEM;",
                "\tio_init_identity(id);",
                "\tid->creds = get_current_cred();",
                "",
                "\tret = idr_alloc_cyclic(&ctx->personality_idr, id, 1, USHRT_MAX, GFP_KERNEL);",
                "\tif (ret < 0) {",
                "\t\tput_cred(id->creds);",
                "\t\tkfree(id);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3290
    },
    {
        "cve_id": "CVE-2018-9517",
        "code_before_change": "static int l2tp_nl_cmd_session_create(struct sk_buff *skb, struct genl_info *info)\n{\n\tu32 tunnel_id = 0;\n\tu32 session_id;\n\tu32 peer_session_id;\n\tint ret = 0;\n\tstruct l2tp_tunnel *tunnel;\n\tstruct l2tp_session *session;\n\tstruct l2tp_session_cfg cfg = { 0, };\n\tstruct net *net = genl_info_net(info);\n\n\tif (!info->attrs[L2TP_ATTR_CONN_ID]) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttunnel_id = nla_get_u32(info->attrs[L2TP_ATTR_CONN_ID]);\n\ttunnel = l2tp_tunnel_get(net, tunnel_id);\n\tif (!tunnel) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tif (!info->attrs[L2TP_ATTR_SESSION_ID]) {\n\t\tret = -EINVAL;\n\t\tgoto out_tunnel;\n\t}\n\tsession_id = nla_get_u32(info->attrs[L2TP_ATTR_SESSION_ID]);\n\n\tif (!info->attrs[L2TP_ATTR_PEER_SESSION_ID]) {\n\t\tret = -EINVAL;\n\t\tgoto out_tunnel;\n\t}\n\tpeer_session_id = nla_get_u32(info->attrs[L2TP_ATTR_PEER_SESSION_ID]);\n\n\tif (!info->attrs[L2TP_ATTR_PW_TYPE]) {\n\t\tret = -EINVAL;\n\t\tgoto out_tunnel;\n\t}\n\tcfg.pw_type = nla_get_u16(info->attrs[L2TP_ATTR_PW_TYPE]);\n\tif (cfg.pw_type >= __L2TP_PWTYPE_MAX) {\n\t\tret = -EINVAL;\n\t\tgoto out_tunnel;\n\t}\n\n\tif (tunnel->version > 2) {\n\t\tif (info->attrs[L2TP_ATTR_OFFSET])\n\t\t\tcfg.offset = nla_get_u16(info->attrs[L2TP_ATTR_OFFSET]);\n\n\t\tif (info->attrs[L2TP_ATTR_DATA_SEQ])\n\t\t\tcfg.data_seq = nla_get_u8(info->attrs[L2TP_ATTR_DATA_SEQ]);\n\n\t\tcfg.l2specific_type = L2TP_L2SPECTYPE_DEFAULT;\n\t\tif (info->attrs[L2TP_ATTR_L2SPEC_TYPE])\n\t\t\tcfg.l2specific_type = nla_get_u8(info->attrs[L2TP_ATTR_L2SPEC_TYPE]);\n\n\t\tcfg.l2specific_len = 4;\n\t\tif (info->attrs[L2TP_ATTR_L2SPEC_LEN])\n\t\t\tcfg.l2specific_len = nla_get_u8(info->attrs[L2TP_ATTR_L2SPEC_LEN]);\n\n\t\tif (info->attrs[L2TP_ATTR_COOKIE]) {\n\t\t\tu16 len = nla_len(info->attrs[L2TP_ATTR_COOKIE]);\n\t\t\tif (len > 8) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_tunnel;\n\t\t\t}\n\t\t\tcfg.cookie_len = len;\n\t\t\tmemcpy(&cfg.cookie[0], nla_data(info->attrs[L2TP_ATTR_COOKIE]), len);\n\t\t}\n\t\tif (info->attrs[L2TP_ATTR_PEER_COOKIE]) {\n\t\t\tu16 len = nla_len(info->attrs[L2TP_ATTR_PEER_COOKIE]);\n\t\t\tif (len > 8) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_tunnel;\n\t\t\t}\n\t\t\tcfg.peer_cookie_len = len;\n\t\t\tmemcpy(&cfg.peer_cookie[0], nla_data(info->attrs[L2TP_ATTR_PEER_COOKIE]), len);\n\t\t}\n\t\tif (info->attrs[L2TP_ATTR_IFNAME])\n\t\t\tcfg.ifname = nla_data(info->attrs[L2TP_ATTR_IFNAME]);\n\n\t\tif (info->attrs[L2TP_ATTR_VLAN_ID])\n\t\t\tcfg.vlan_id = nla_get_u16(info->attrs[L2TP_ATTR_VLAN_ID]);\n\t}\n\n\tif (info->attrs[L2TP_ATTR_DEBUG])\n\t\tcfg.debug = nla_get_u32(info->attrs[L2TP_ATTR_DEBUG]);\n\n\tif (info->attrs[L2TP_ATTR_RECV_SEQ])\n\t\tcfg.recv_seq = nla_get_u8(info->attrs[L2TP_ATTR_RECV_SEQ]);\n\n\tif (info->attrs[L2TP_ATTR_SEND_SEQ])\n\t\tcfg.send_seq = nla_get_u8(info->attrs[L2TP_ATTR_SEND_SEQ]);\n\n\tif (info->attrs[L2TP_ATTR_LNS_MODE])\n\t\tcfg.lns_mode = nla_get_u8(info->attrs[L2TP_ATTR_LNS_MODE]);\n\n\tif (info->attrs[L2TP_ATTR_RECV_TIMEOUT])\n\t\tcfg.reorder_timeout = nla_get_msecs(info->attrs[L2TP_ATTR_RECV_TIMEOUT]);\n\n\tif (info->attrs[L2TP_ATTR_MTU])\n\t\tcfg.mtu = nla_get_u16(info->attrs[L2TP_ATTR_MTU]);\n\n\tif (info->attrs[L2TP_ATTR_MRU])\n\t\tcfg.mru = nla_get_u16(info->attrs[L2TP_ATTR_MRU]);\n\n#ifdef CONFIG_MODULES\n\tif (l2tp_nl_cmd_ops[cfg.pw_type] == NULL) {\n\t\tgenl_unlock();\n\t\trequest_module(\"net-l2tp-type-%u\", cfg.pw_type);\n\t\tgenl_lock();\n\t}\n#endif\n\tif ((l2tp_nl_cmd_ops[cfg.pw_type] == NULL) ||\n\t    (l2tp_nl_cmd_ops[cfg.pw_type]->session_create == NULL)) {\n\t\tret = -EPROTONOSUPPORT;\n\t\tgoto out_tunnel;\n\t}\n\n\t/* Check that pseudowire-specific params are present */\n\tswitch (cfg.pw_type) {\n\tcase L2TP_PWTYPE_NONE:\n\t\tbreak;\n\tcase L2TP_PWTYPE_ETH_VLAN:\n\t\tif (!info->attrs[L2TP_ATTR_VLAN_ID]) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_tunnel;\n\t\t}\n\t\tbreak;\n\tcase L2TP_PWTYPE_ETH:\n\t\tbreak;\n\tcase L2TP_PWTYPE_PPP:\n\tcase L2TP_PWTYPE_PPP_AC:\n\t\tbreak;\n\tcase L2TP_PWTYPE_IP:\n\tdefault:\n\t\tret = -EPROTONOSUPPORT;\n\t\tbreak;\n\t}\n\n\tret = -EPROTONOSUPPORT;\n\tif (l2tp_nl_cmd_ops[cfg.pw_type]->session_create)\n\t\tret = (*l2tp_nl_cmd_ops[cfg.pw_type]->session_create)(net, tunnel_id,\n\t\t\tsession_id, peer_session_id, &cfg);\n\n\tif (ret >= 0) {\n\t\tsession = l2tp_session_get(net, tunnel, session_id, false);\n\t\tif (session) {\n\t\t\tret = l2tp_session_notify(&l2tp_nl_family, info, session,\n\t\t\t\t\t\t  L2TP_CMD_SESSION_CREATE);\n\t\t\tl2tp_session_dec_refcount(session);\n\t\t}\n\t}\n\nout_tunnel:\n\tl2tp_tunnel_dec_refcount(tunnel);\nout:\n\treturn ret;\n}",
        "code_after_change": "static int l2tp_nl_cmd_session_create(struct sk_buff *skb, struct genl_info *info)\n{\n\tu32 tunnel_id = 0;\n\tu32 session_id;\n\tu32 peer_session_id;\n\tint ret = 0;\n\tstruct l2tp_tunnel *tunnel;\n\tstruct l2tp_session *session;\n\tstruct l2tp_session_cfg cfg = { 0, };\n\tstruct net *net = genl_info_net(info);\n\n\tif (!info->attrs[L2TP_ATTR_CONN_ID]) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttunnel_id = nla_get_u32(info->attrs[L2TP_ATTR_CONN_ID]);\n\ttunnel = l2tp_tunnel_get(net, tunnel_id);\n\tif (!tunnel) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tif (!info->attrs[L2TP_ATTR_SESSION_ID]) {\n\t\tret = -EINVAL;\n\t\tgoto out_tunnel;\n\t}\n\tsession_id = nla_get_u32(info->attrs[L2TP_ATTR_SESSION_ID]);\n\n\tif (!info->attrs[L2TP_ATTR_PEER_SESSION_ID]) {\n\t\tret = -EINVAL;\n\t\tgoto out_tunnel;\n\t}\n\tpeer_session_id = nla_get_u32(info->attrs[L2TP_ATTR_PEER_SESSION_ID]);\n\n\tif (!info->attrs[L2TP_ATTR_PW_TYPE]) {\n\t\tret = -EINVAL;\n\t\tgoto out_tunnel;\n\t}\n\tcfg.pw_type = nla_get_u16(info->attrs[L2TP_ATTR_PW_TYPE]);\n\tif (cfg.pw_type >= __L2TP_PWTYPE_MAX) {\n\t\tret = -EINVAL;\n\t\tgoto out_tunnel;\n\t}\n\n\tif (tunnel->version > 2) {\n\t\tif (info->attrs[L2TP_ATTR_OFFSET])\n\t\t\tcfg.offset = nla_get_u16(info->attrs[L2TP_ATTR_OFFSET]);\n\n\t\tif (info->attrs[L2TP_ATTR_DATA_SEQ])\n\t\t\tcfg.data_seq = nla_get_u8(info->attrs[L2TP_ATTR_DATA_SEQ]);\n\n\t\tcfg.l2specific_type = L2TP_L2SPECTYPE_DEFAULT;\n\t\tif (info->attrs[L2TP_ATTR_L2SPEC_TYPE])\n\t\t\tcfg.l2specific_type = nla_get_u8(info->attrs[L2TP_ATTR_L2SPEC_TYPE]);\n\n\t\tcfg.l2specific_len = 4;\n\t\tif (info->attrs[L2TP_ATTR_L2SPEC_LEN])\n\t\t\tcfg.l2specific_len = nla_get_u8(info->attrs[L2TP_ATTR_L2SPEC_LEN]);\n\n\t\tif (info->attrs[L2TP_ATTR_COOKIE]) {\n\t\t\tu16 len = nla_len(info->attrs[L2TP_ATTR_COOKIE]);\n\t\t\tif (len > 8) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_tunnel;\n\t\t\t}\n\t\t\tcfg.cookie_len = len;\n\t\t\tmemcpy(&cfg.cookie[0], nla_data(info->attrs[L2TP_ATTR_COOKIE]), len);\n\t\t}\n\t\tif (info->attrs[L2TP_ATTR_PEER_COOKIE]) {\n\t\t\tu16 len = nla_len(info->attrs[L2TP_ATTR_PEER_COOKIE]);\n\t\t\tif (len > 8) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_tunnel;\n\t\t\t}\n\t\t\tcfg.peer_cookie_len = len;\n\t\t\tmemcpy(&cfg.peer_cookie[0], nla_data(info->attrs[L2TP_ATTR_PEER_COOKIE]), len);\n\t\t}\n\t\tif (info->attrs[L2TP_ATTR_IFNAME])\n\t\t\tcfg.ifname = nla_data(info->attrs[L2TP_ATTR_IFNAME]);\n\n\t\tif (info->attrs[L2TP_ATTR_VLAN_ID])\n\t\t\tcfg.vlan_id = nla_get_u16(info->attrs[L2TP_ATTR_VLAN_ID]);\n\t}\n\n\tif (info->attrs[L2TP_ATTR_DEBUG])\n\t\tcfg.debug = nla_get_u32(info->attrs[L2TP_ATTR_DEBUG]);\n\n\tif (info->attrs[L2TP_ATTR_RECV_SEQ])\n\t\tcfg.recv_seq = nla_get_u8(info->attrs[L2TP_ATTR_RECV_SEQ]);\n\n\tif (info->attrs[L2TP_ATTR_SEND_SEQ])\n\t\tcfg.send_seq = nla_get_u8(info->attrs[L2TP_ATTR_SEND_SEQ]);\n\n\tif (info->attrs[L2TP_ATTR_LNS_MODE])\n\t\tcfg.lns_mode = nla_get_u8(info->attrs[L2TP_ATTR_LNS_MODE]);\n\n\tif (info->attrs[L2TP_ATTR_RECV_TIMEOUT])\n\t\tcfg.reorder_timeout = nla_get_msecs(info->attrs[L2TP_ATTR_RECV_TIMEOUT]);\n\n\tif (info->attrs[L2TP_ATTR_MTU])\n\t\tcfg.mtu = nla_get_u16(info->attrs[L2TP_ATTR_MTU]);\n\n\tif (info->attrs[L2TP_ATTR_MRU])\n\t\tcfg.mru = nla_get_u16(info->attrs[L2TP_ATTR_MRU]);\n\n#ifdef CONFIG_MODULES\n\tif (l2tp_nl_cmd_ops[cfg.pw_type] == NULL) {\n\t\tgenl_unlock();\n\t\trequest_module(\"net-l2tp-type-%u\", cfg.pw_type);\n\t\tgenl_lock();\n\t}\n#endif\n\tif ((l2tp_nl_cmd_ops[cfg.pw_type] == NULL) ||\n\t    (l2tp_nl_cmd_ops[cfg.pw_type]->session_create == NULL)) {\n\t\tret = -EPROTONOSUPPORT;\n\t\tgoto out_tunnel;\n\t}\n\n\t/* Check that pseudowire-specific params are present */\n\tswitch (cfg.pw_type) {\n\tcase L2TP_PWTYPE_NONE:\n\t\tbreak;\n\tcase L2TP_PWTYPE_ETH_VLAN:\n\t\tif (!info->attrs[L2TP_ATTR_VLAN_ID]) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_tunnel;\n\t\t}\n\t\tbreak;\n\tcase L2TP_PWTYPE_ETH:\n\t\tbreak;\n\tcase L2TP_PWTYPE_PPP:\n\tcase L2TP_PWTYPE_PPP_AC:\n\t\tbreak;\n\tcase L2TP_PWTYPE_IP:\n\tdefault:\n\t\tret = -EPROTONOSUPPORT;\n\t\tbreak;\n\t}\n\n\tret = l2tp_nl_cmd_ops[cfg.pw_type]->session_create(net, tunnel,\n\t\t\t\t\t\t\t   session_id,\n\t\t\t\t\t\t\t   peer_session_id,\n\t\t\t\t\t\t\t   &cfg);\n\n\tif (ret >= 0) {\n\t\tsession = l2tp_session_get(net, tunnel, session_id, false);\n\t\tif (session) {\n\t\t\tret = l2tp_session_notify(&l2tp_nl_family, info, session,\n\t\t\t\t\t\t  L2TP_CMD_SESSION_CREATE);\n\t\t\tl2tp_session_dec_refcount(session);\n\t\t}\n\t}\n\nout_tunnel:\n\tl2tp_tunnel_dec_refcount(tunnel);\nout:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -138,10 +138,10 @@\n \t\tbreak;\n \t}\n \n-\tret = -EPROTONOSUPPORT;\n-\tif (l2tp_nl_cmd_ops[cfg.pw_type]->session_create)\n-\t\tret = (*l2tp_nl_cmd_ops[cfg.pw_type]->session_create)(net, tunnel_id,\n-\t\t\tsession_id, peer_session_id, &cfg);\n+\tret = l2tp_nl_cmd_ops[cfg.pw_type]->session_create(net, tunnel,\n+\t\t\t\t\t\t\t   session_id,\n+\t\t\t\t\t\t\t   peer_session_id,\n+\t\t\t\t\t\t\t   &cfg);\n \n \tif (ret >= 0) {\n \t\tsession = l2tp_session_get(net, tunnel, session_id, false);",
        "function_modified_lines": {
            "added": [
                "\tret = l2tp_nl_cmd_ops[cfg.pw_type]->session_create(net, tunnel,",
                "\t\t\t\t\t\t\t   session_id,",
                "\t\t\t\t\t\t\t   peer_session_id,",
                "\t\t\t\t\t\t\t   &cfg);"
            ],
            "deleted": [
                "\tret = -EPROTONOSUPPORT;",
                "\tif (l2tp_nl_cmd_ops[cfg.pw_type]->session_create)",
                "\t\tret = (*l2tp_nl_cmd_ops[cfg.pw_type]->session_create)(net, tunnel_id,",
                "\t\t\tsession_id, peer_session_id, &cfg);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In pppol2tp_connect, there is possible memory corruption due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation. Product: Android. Versions: Android kernel. Android ID: A-38159931.",
        "id": 1873
    },
    {
        "cve_id": "CVE-2022-1882",
        "code_before_change": "void __post_watch_notification(struct watch_list *wlist,\n\t\t\t       struct watch_notification *n,\n\t\t\t       const struct cred *cred,\n\t\t\t       u64 id)\n{\n\tconst struct watch_filter *wf;\n\tstruct watch_queue *wqueue;\n\tstruct watch *watch;\n\n\tif (((n->info & WATCH_INFO_LENGTH) >> WATCH_INFO_LENGTH__SHIFT) == 0) {\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\trcu_read_lock();\n\n\thlist_for_each_entry_rcu(watch, &wlist->watchers, list_node) {\n\t\tif (watch->id != id)\n\t\t\tcontinue;\n\t\tn->info &= ~WATCH_INFO_ID;\n\t\tn->info |= watch->info_id;\n\n\t\twqueue = rcu_dereference(watch->queue);\n\t\twf = rcu_dereference(wqueue->filter);\n\t\tif (wf && !filter_watch_notification(wf, n))\n\t\t\tcontinue;\n\n\t\tif (security_post_notification(watch->cred, cred, n) < 0)\n\t\t\tcontinue;\n\n\t\tpost_one_notification(wqueue, n);\n\t}\n\n\trcu_read_unlock();\n}",
        "code_after_change": "void __post_watch_notification(struct watch_list *wlist,\n\t\t\t       struct watch_notification *n,\n\t\t\t       const struct cred *cred,\n\t\t\t       u64 id)\n{\n\tconst struct watch_filter *wf;\n\tstruct watch_queue *wqueue;\n\tstruct watch *watch;\n\n\tif (((n->info & WATCH_INFO_LENGTH) >> WATCH_INFO_LENGTH__SHIFT) == 0) {\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\trcu_read_lock();\n\n\thlist_for_each_entry_rcu(watch, &wlist->watchers, list_node) {\n\t\tif (watch->id != id)\n\t\t\tcontinue;\n\t\tn->info &= ~WATCH_INFO_ID;\n\t\tn->info |= watch->info_id;\n\n\t\twqueue = rcu_dereference(watch->queue);\n\t\twf = rcu_dereference(wqueue->filter);\n\t\tif (wf && !filter_watch_notification(wf, n))\n\t\t\tcontinue;\n\n\t\tif (security_post_notification(watch->cred, cred, n) < 0)\n\t\t\tcontinue;\n\n\t\tif (lock_wqueue(wqueue)) {\n\t\t\tpost_one_notification(wqueue, n);\n\t\t\tunlock_wqueue(wqueue);;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,10 @@\n \t\tif (security_post_notification(watch->cred, cred, n) < 0)\n \t\t\tcontinue;\n \n-\t\tpost_one_notification(wqueue, n);\n+\t\tif (lock_wqueue(wqueue)) {\n+\t\t\tpost_one_notification(wqueue, n);\n+\t\t\tunlock_wqueue(wqueue);;\n+\t\t}\n \t}\n \n \trcu_read_unlock();",
        "function_modified_lines": {
            "added": [
                "\t\tif (lock_wqueue(wqueue)) {",
                "\t\t\tpost_one_notification(wqueue, n);",
                "\t\t\tunlock_wqueue(wqueue);;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tpost_one_notification(wqueue, n);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s pipes functionality in how a user performs manipulations with the pipe post_one_notification() after free_pipe_info() that is already called. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3298
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static void blk_add_trace_plug(void *ignore, struct request_queue *q)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\n\tif (bt)\n\t\t__blk_add_trace(bt, 0, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL, 0);\n}",
        "code_after_change": "static void blk_add_trace_plug(void *ignore, struct request_queue *q)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (bt)\n\t\t__blk_add_trace(bt, 0, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL, 0);\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,10 @@\n static void blk_add_trace_plug(void *ignore, struct request_queue *q)\n {\n-\tstruct blk_trace *bt = q->blk_trace;\n+\tstruct blk_trace *bt;\n \n+\trcu_read_lock();\n+\tbt = rcu_dereference(q->blk_trace);\n \tif (bt)\n \t\t__blk_add_trace(bt, 0, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL, 0);\n+\trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct blk_trace *bt;",
                "\trcu_read_lock();",
                "\tbt = rcu_dereference(q->blk_trace);",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tstruct blk_trace *bt = q->blk_trace;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2239
    },
    {
        "cve_id": "CVE-2022-41218",
        "code_before_change": "static int dvb_demux_open(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dmxdev *dmxdev = dvbdev->priv;\n\tint i;\n\tstruct dmxdev_filter *dmxdevfilter;\n\n\tif (!dmxdev->filter)\n\t\treturn -EINVAL;\n\n\tif (mutex_lock_interruptible(&dmxdev->mutex))\n\t\treturn -ERESTARTSYS;\n\n\tfor (i = 0; i < dmxdev->filternum; i++)\n\t\tif (dmxdev->filter[i].state == DMXDEV_STATE_FREE)\n\t\t\tbreak;\n\n\tif (i == dmxdev->filternum) {\n\t\tmutex_unlock(&dmxdev->mutex);\n\t\treturn -EMFILE;\n\t}\n\n\tdmxdevfilter = &dmxdev->filter[i];\n\tmutex_init(&dmxdevfilter->mutex);\n\tfile->private_data = dmxdevfilter;\n\n#ifdef CONFIG_DVB_MMAP\n\tdmxdev->may_do_mmap = 1;\n#else\n\tdmxdev->may_do_mmap = 0;\n#endif\n\n\tdvb_ringbuffer_init(&dmxdevfilter->buffer, NULL, 8192);\n\tdvb_vb2_init(&dmxdevfilter->vb2_ctx, \"demux_filter\",\n\t\t     file->f_flags & O_NONBLOCK);\n\tdmxdevfilter->type = DMXDEV_TYPE_NONE;\n\tdvb_dmxdev_filter_state_set(dmxdevfilter, DMXDEV_STATE_ALLOCATED);\n\ttimer_setup(&dmxdevfilter->timer, dvb_dmxdev_filter_timeout, 0);\n\n\tdvbdev->users++;\n\n\tmutex_unlock(&dmxdev->mutex);\n\treturn 0;\n}",
        "code_after_change": "static int dvb_demux_open(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dmxdev *dmxdev = dvbdev->priv;\n\tint i;\n\tstruct dmxdev_filter *dmxdevfilter;\n\n\tif (!dmxdev->filter)\n\t\treturn -EINVAL;\n\n\tif (mutex_lock_interruptible(&dmxdev->mutex))\n\t\treturn -ERESTARTSYS;\n\n\tif (dmxdev->exit) {\n\t\tmutex_unlock(&dmxdev->mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tfor (i = 0; i < dmxdev->filternum; i++)\n\t\tif (dmxdev->filter[i].state == DMXDEV_STATE_FREE)\n\t\t\tbreak;\n\n\tif (i == dmxdev->filternum) {\n\t\tmutex_unlock(&dmxdev->mutex);\n\t\treturn -EMFILE;\n\t}\n\n\tdmxdevfilter = &dmxdev->filter[i];\n\tmutex_init(&dmxdevfilter->mutex);\n\tfile->private_data = dmxdevfilter;\n\n#ifdef CONFIG_DVB_MMAP\n\tdmxdev->may_do_mmap = 1;\n#else\n\tdmxdev->may_do_mmap = 0;\n#endif\n\n\tdvb_ringbuffer_init(&dmxdevfilter->buffer, NULL, 8192);\n\tdvb_vb2_init(&dmxdevfilter->vb2_ctx, \"demux_filter\",\n\t\t     file->f_flags & O_NONBLOCK);\n\tdmxdevfilter->type = DMXDEV_TYPE_NONE;\n\tdvb_dmxdev_filter_state_set(dmxdevfilter, DMXDEV_STATE_ALLOCATED);\n\ttimer_setup(&dmxdevfilter->timer, dvb_dmxdev_filter_timeout, 0);\n\n\tdvbdev->users++;\n\n\tmutex_unlock(&dmxdev->mutex);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,11 @@\n \n \tif (mutex_lock_interruptible(&dmxdev->mutex))\n \t\treturn -ERESTARTSYS;\n+\n+\tif (dmxdev->exit) {\n+\t\tmutex_unlock(&dmxdev->mutex);\n+\t\treturn -ENODEV;\n+\t}\n \n \tfor (i = 0; i < dmxdev->filternum; i++)\n \t\tif (dmxdev->filter[i].state == DMXDEV_STATE_FREE)",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (dmxdev->exit) {",
                "\t\tmutex_unlock(&dmxdev->mutex);",
                "\t\treturn -ENODEV;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In drivers/media/dvb-core/dmxdev.c in the Linux kernel through 5.19.10, there is a use-after-free caused by refcount races, affecting dvb_demux_open and dvb_dmxdev_release.",
        "id": 3712
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "static int\nvmw_cmd_res_check(struct vmw_private *dev_priv,\n\t\t  struct vmw_sw_context *sw_context,\n\t\t  enum vmw_res_type res_type,\n\t\t  u32 dirty,\n\t\t  const struct vmw_user_resource_conv *converter,\n\t\t  uint32_t *id_loc,\n\t\t  struct vmw_resource **p_res)\n{\n\tstruct vmw_res_cache_entry *rcache = &sw_context->res_cache[res_type];\n\tstruct vmw_resource *res;\n\tint ret;\n\n\tif (p_res)\n\t\t*p_res = NULL;\n\n\tif (*id_loc == SVGA3D_INVALID_ID) {\n\t\tif (res_type == vmw_res_context) {\n\t\t\tVMW_DEBUG_USER(\"Illegal context invalid id.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (likely(rcache->valid_handle && *id_loc == rcache->handle)) {\n\t\tres = rcache->res;\n\t\tif (dirty)\n\t\t\tvmw_validation_res_set_dirty(sw_context->ctx,\n\t\t\t\t\t\t     rcache->private, dirty);\n\t} else {\n\t\tunsigned int size = vmw_execbuf_res_size(dev_priv, res_type);\n\n\t\tret = vmw_validation_preload_res(sw_context->ctx, size);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tres = vmw_user_resource_noref_lookup_handle\n\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter);\n\t\tif (IS_ERR(res)) {\n\t\t\tVMW_DEBUG_USER(\"Could not find/use resource 0x%08x.\\n\",\n\t\t\t\t       (unsigned int) *id_loc);\n\t\t\treturn PTR_ERR(res);\n\t\t}\n\n\t\tret = vmw_execbuf_res_noref_val_add(sw_context, res, dirty);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\n\t\tif (rcache->valid && rcache->res == res) {\n\t\t\trcache->valid_handle = true;\n\t\t\trcache->handle = *id_loc;\n\t\t}\n\t}\n\n\tret = vmw_resource_relocation_add(sw_context, res,\n\t\t\t\t\t  vmw_ptr_diff(sw_context->buf_start,\n\t\t\t\t\t\t       id_loc),\n\t\t\t\t\t  vmw_res_rel_normal);\n\tif (p_res)\n\t\t*p_res = res;\n\n\treturn 0;\n}",
        "code_after_change": "static int\nvmw_cmd_res_check(struct vmw_private *dev_priv,\n\t\t  struct vmw_sw_context *sw_context,\n\t\t  enum vmw_res_type res_type,\n\t\t  u32 dirty,\n\t\t  const struct vmw_user_resource_conv *converter,\n\t\t  uint32_t *id_loc,\n\t\t  struct vmw_resource **p_res)\n{\n\tstruct vmw_res_cache_entry *rcache = &sw_context->res_cache[res_type];\n\tstruct vmw_resource *res;\n\tint ret = 0;\n\tbool needs_unref = false;\n\n\tif (p_res)\n\t\t*p_res = NULL;\n\n\tif (*id_loc == SVGA3D_INVALID_ID) {\n\t\tif (res_type == vmw_res_context) {\n\t\t\tVMW_DEBUG_USER(\"Illegal context invalid id.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (likely(rcache->valid_handle && *id_loc == rcache->handle)) {\n\t\tres = rcache->res;\n\t\tif (dirty)\n\t\t\tvmw_validation_res_set_dirty(sw_context->ctx,\n\t\t\t\t\t\t     rcache->private, dirty);\n\t} else {\n\t\tunsigned int size = vmw_execbuf_res_size(dev_priv, res_type);\n\n\t\tret = vmw_validation_preload_res(sw_context->ctx, size);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = vmw_user_resource_lookup_handle\n\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter, &res);\n\t\tif (ret != 0) {\n\t\t\tVMW_DEBUG_USER(\"Could not find/use resource 0x%08x.\\n\",\n\t\t\t\t       (unsigned int) *id_loc);\n\t\t\treturn ret;\n\t\t}\n\t\tneeds_unref = true;\n\n\t\tret = vmw_execbuf_res_val_add(sw_context, res, dirty, vmw_val_add_flag_none);\n\t\tif (unlikely(ret != 0))\n\t\t\tgoto res_check_done;\n\n\t\tif (rcache->valid && rcache->res == res) {\n\t\t\trcache->valid_handle = true;\n\t\t\trcache->handle = *id_loc;\n\t\t}\n\t}\n\n\tret = vmw_resource_relocation_add(sw_context, res,\n\t\t\t\t\t  vmw_ptr_diff(sw_context->buf_start,\n\t\t\t\t\t\t       id_loc),\n\t\t\t\t\t  vmw_res_rel_normal);\n\tif (p_res)\n\t\t*p_res = res;\n\nres_check_done:\n\tif (needs_unref)\n\t\tvmw_resource_unreference(&res);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,8 @@\n {\n \tstruct vmw_res_cache_entry *rcache = &sw_context->res_cache[res_type];\n \tstruct vmw_resource *res;\n-\tint ret;\n+\tint ret = 0;\n+\tbool needs_unref = false;\n \n \tif (p_res)\n \t\t*p_res = NULL;\n@@ -34,17 +35,18 @@\n \t\tif (ret)\n \t\t\treturn ret;\n \n-\t\tres = vmw_user_resource_noref_lookup_handle\n-\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter);\n-\t\tif (IS_ERR(res)) {\n+\t\tret = vmw_user_resource_lookup_handle\n+\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter, &res);\n+\t\tif (ret != 0) {\n \t\t\tVMW_DEBUG_USER(\"Could not find/use resource 0x%08x.\\n\",\n \t\t\t\t       (unsigned int) *id_loc);\n-\t\t\treturn PTR_ERR(res);\n+\t\t\treturn ret;\n \t\t}\n+\t\tneeds_unref = true;\n \n-\t\tret = vmw_execbuf_res_noref_val_add(sw_context, res, dirty);\n+\t\tret = vmw_execbuf_res_val_add(sw_context, res, dirty, vmw_val_add_flag_none);\n \t\tif (unlikely(ret != 0))\n-\t\t\treturn ret;\n+\t\t\tgoto res_check_done;\n \n \t\tif (rcache->valid && rcache->res == res) {\n \t\t\trcache->valid_handle = true;\n@@ -59,5 +61,9 @@\n \tif (p_res)\n \t\t*p_res = res;\n \n-\treturn 0;\n+res_check_done:\n+\tif (needs_unref)\n+\t\tvmw_resource_unreference(&res);\n+\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tint ret = 0;",
                "\tbool needs_unref = false;",
                "\t\tret = vmw_user_resource_lookup_handle",
                "\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter, &res);",
                "\t\tif (ret != 0) {",
                "\t\t\treturn ret;",
                "\t\tneeds_unref = true;",
                "\t\tret = vmw_execbuf_res_val_add(sw_context, res, dirty, vmw_val_add_flag_none);",
                "\t\t\tgoto res_check_done;",
                "res_check_done:",
                "\tif (needs_unref)",
                "\t\tvmw_resource_unreference(&res);",
                "",
                "\treturn ret;"
            ],
            "deleted": [
                "\tint ret;",
                "\t\tres = vmw_user_resource_noref_lookup_handle",
                "\t\t\t(dev_priv, sw_context->fp->tfile, *id_loc, converter);",
                "\t\tif (IS_ERR(res)) {",
                "\t\t\treturn PTR_ERR(res);",
                "\t\tret = vmw_execbuf_res_noref_val_add(sw_context, res, dirty);",
                "\t\t\treturn ret;",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3686
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}",
        "code_after_change": "struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -87,7 +87,7 @@\n \t\tmemset(&fl6, 0, sizeof(fl6));\n \t\tfl6.flowi6_proto = IPPROTO_TCP;\n \t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n-\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n \t\tfl6.saddr = ireq->ir_v6_loc_addr;\n \t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n \t\tfl6.flowi6_mark = ireq->ir_mark;",
        "function_modified_lines": {
            "added": [
                "\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);"
            ],
            "deleted": [
                "\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1004
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static inline int l2cap_move_channel_confirm_rsp(struct l2cap_conn *conn,\n\t\t\t\t\t\t struct l2cap_cmd_hdr *cmd,\n\t\t\t\t\t\t u16 cmd_len, void *data)\n{\n\tstruct l2cap_move_chan_cfm_rsp *rsp = data;\n\tstruct l2cap_chan *chan;\n\tu16 icid;\n\n\tif (cmd_len != sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\ticid = le16_to_cpu(rsp->icid);\n\n\tBT_DBG(\"icid 0x%4.4x\", icid);\n\n\tchan = l2cap_get_chan_by_scid(conn, icid);\n\tif (!chan)\n\t\treturn 0;\n\n\t__clear_chan_timer(chan);\n\n\tif (chan->move_state == L2CAP_MOVE_WAIT_CONFIRM_RSP) {\n\t\tchan->local_amp_id = chan->move_id;\n\n\t\tif (chan->local_amp_id == AMP_ID_BREDR && chan->hs_hchan)\n\t\t\t__release_logical_link(chan);\n\n\t\tl2cap_move_done(chan);\n\t}\n\n\tl2cap_chan_unlock(chan);\n\n\treturn 0;\n}",
        "code_after_change": "static inline int l2cap_move_channel_confirm_rsp(struct l2cap_conn *conn,\n\t\t\t\t\t\t struct l2cap_cmd_hdr *cmd,\n\t\t\t\t\t\t u16 cmd_len, void *data)\n{\n\tstruct l2cap_move_chan_cfm_rsp *rsp = data;\n\tstruct l2cap_chan *chan;\n\tu16 icid;\n\n\tif (cmd_len != sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\ticid = le16_to_cpu(rsp->icid);\n\n\tBT_DBG(\"icid 0x%4.4x\", icid);\n\n\tchan = l2cap_get_chan_by_scid(conn, icid);\n\tif (!chan)\n\t\treturn 0;\n\n\t__clear_chan_timer(chan);\n\n\tif (chan->move_state == L2CAP_MOVE_WAIT_CONFIRM_RSP) {\n\t\tchan->local_amp_id = chan->move_id;\n\n\t\tif (chan->local_amp_id == AMP_ID_BREDR && chan->hs_hchan)\n\t\t\t__release_logical_link(chan);\n\n\t\tl2cap_move_done(chan);\n\t}\n\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,6 +29,7 @@\n \t}\n \n \tl2cap_chan_unlock(chan);\n+\tl2cap_chan_put(chan);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tl2cap_chan_put(chan);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3388
    },
    {
        "cve_id": "CVE-2018-16884",
        "code_before_change": "static int\nsvc_process_common(struct svc_rqst *rqstp, struct kvec *argv, struct kvec *resv)\n{\n\tstruct svc_program\t*progp;\n\tconst struct svc_version *versp = NULL;\t/* compiler food */\n\tconst struct svc_procedure *procp = NULL;\n\tstruct svc_serv\t\t*serv = rqstp->rq_server;\n\t__be32\t\t\t*statp;\n\tu32\t\t\tprog, vers, proc;\n\t__be32\t\t\tauth_stat, rpc_stat;\n\tint\t\t\tauth_res;\n\t__be32\t\t\t*reply_statp;\n\n\trpc_stat = rpc_success;\n\n\tif (argv->iov_len < 6*4)\n\t\tgoto err_short_len;\n\n\t/* Will be turned off by GSS integrity and privacy services */\n\tset_bit(RQ_SPLICE_OK, &rqstp->rq_flags);\n\t/* Will be turned off only when NFSv4 Sessions are used */\n\tset_bit(RQ_USEDEFERRAL, &rqstp->rq_flags);\n\tclear_bit(RQ_DROPME, &rqstp->rq_flags);\n\n\t/* Setup reply header */\n\trqstp->rq_xprt->xpt_ops->xpo_prep_reply_hdr(rqstp);\n\n\tsvc_putu32(resv, rqstp->rq_xid);\n\n\tvers = svc_getnl(argv);\n\n\t/* First words of reply: */\n\tsvc_putnl(resv, 1);\t\t/* REPLY */\n\n\tif (vers != 2)\t\t/* RPC version number */\n\t\tgoto err_bad_rpc;\n\n\t/* Save position in case we later decide to reject: */\n\treply_statp = resv->iov_base + resv->iov_len;\n\n\tsvc_putnl(resv, 0);\t\t/* ACCEPT */\n\n\trqstp->rq_prog = prog = svc_getnl(argv);\t/* program number */\n\trqstp->rq_vers = vers = svc_getnl(argv);\t/* version number */\n\trqstp->rq_proc = proc = svc_getnl(argv);\t/* procedure number */\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next)\n\t\tif (prog == progp->pg_prog)\n\t\t\tbreak;\n\n\t/*\n\t * Decode auth data, and add verifier to reply buffer.\n\t * We do this before anything else in order to get a decent\n\t * auth verifier.\n\t */\n\tauth_res = svc_authenticate(rqstp, &auth_stat);\n\t/* Also give the program a chance to reject this call: */\n\tif (auth_res == SVC_OK && progp) {\n\t\tauth_stat = rpc_autherr_badcred;\n\t\tauth_res = progp->pg_authenticate(rqstp);\n\t}\n\tswitch (auth_res) {\n\tcase SVC_OK:\n\t\tbreak;\n\tcase SVC_GARBAGE:\n\t\tgoto err_garbage;\n\tcase SVC_SYSERR:\n\t\trpc_stat = rpc_system_err;\n\t\tgoto err_bad;\n\tcase SVC_DENIED:\n\t\tgoto err_bad_auth;\n\tcase SVC_CLOSE:\n\t\tgoto close;\n\tcase SVC_DROP:\n\t\tgoto dropit;\n\tcase SVC_COMPLETE:\n\t\tgoto sendit;\n\t}\n\n\tif (progp == NULL)\n\t\tgoto err_bad_prog;\n\n\tif (vers >= progp->pg_nvers ||\n\t  !(versp = progp->pg_vers[vers]))\n\t\tgoto err_bad_vers;\n\n\t/*\n\t * Some protocol versions (namely NFSv4) require some form of\n\t * congestion control.  (See RFC 7530 section 3.1 paragraph 2)\n\t * In other words, UDP is not allowed. We mark those when setting\n\t * up the svc_xprt, and verify that here.\n\t *\n\t * The spec is not very clear about what error should be returned\n\t * when someone tries to access a server that is listening on UDP\n\t * for lower versions. RPC_PROG_MISMATCH seems to be the closest\n\t * fit.\n\t */\n\tif (versp->vs_need_cong_ctrl &&\n\t    !test_bit(XPT_CONG_CTRL, &rqstp->rq_xprt->xpt_flags))\n\t\tgoto err_bad_vers;\n\n\tprocp = versp->vs_proc + proc;\n\tif (proc >= versp->vs_nproc || !procp->pc_func)\n\t\tgoto err_bad_proc;\n\trqstp->rq_procinfo = procp;\n\n\t/* Syntactic check complete */\n\tserv->sv_stats->rpccnt++;\n\ttrace_svc_process(rqstp, progp->pg_name);\n\n\t/* Build the reply header. */\n\tstatp = resv->iov_base +resv->iov_len;\n\tsvc_putnl(resv, RPC_SUCCESS);\n\n\t/* Bump per-procedure stats counter */\n\tversp->vs_count[proc]++;\n\n\t/* Initialize storage for argp and resp */\n\tmemset(rqstp->rq_argp, 0, procp->pc_argsize);\n\tmemset(rqstp->rq_resp, 0, procp->pc_ressize);\n\n\t/* un-reserve some of the out-queue now that we have a\n\t * better idea of reply size\n\t */\n\tif (procp->pc_xdrressize)\n\t\tsvc_reserve_auth(rqstp, procp->pc_xdrressize<<2);\n\n\t/* Call the function that processes the request. */\n\tif (!versp->vs_dispatch) {\n\t\t/*\n\t\t * Decode arguments\n\t\t * XXX: why do we ignore the return value?\n\t\t */\n\t\tif (procp->pc_decode &&\n\t\t    !procp->pc_decode(rqstp, argv->iov_base))\n\t\t\tgoto err_garbage;\n\n\t\t*statp = procp->pc_func(rqstp);\n\n\t\t/* Encode reply */\n\t\tif (*statp == rpc_drop_reply ||\n\t\t    test_bit(RQ_DROPME, &rqstp->rq_flags)) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto dropit;\n\t\t}\n\t\tif (*statp == rpc_autherr_badcred) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto err_bad_auth;\n\t\t}\n\t\tif (*statp == rpc_success && procp->pc_encode &&\n\t\t    !procp->pc_encode(rqstp, resv->iov_base + resv->iov_len)) {\n\t\t\tdprintk(\"svc: failed to encode reply\\n\");\n\t\t\t/* serv->sv_stats->rpcsystemerr++; */\n\t\t\t*statp = rpc_system_err;\n\t\t}\n\t} else {\n\t\tdprintk(\"svc: calling dispatcher\\n\");\n\t\tif (!versp->vs_dispatch(rqstp, statp)) {\n\t\t\t/* Release reply info */\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto dropit;\n\t\t}\n\t}\n\n\t/* Check RPC status result */\n\tif (*statp != rpc_success)\n\t\tresv->iov_len = ((void*)statp)  - resv->iov_base + 4;\n\n\t/* Release reply info */\n\tif (procp->pc_release)\n\t\tprocp->pc_release(rqstp);\n\n\tif (procp->pc_encode == NULL)\n\t\tgoto dropit;\n\n sendit:\n\tif (svc_authorise(rqstp))\n\t\tgoto close;\n\treturn 1;\t\t/* Caller can now send it */\n\n dropit:\n\tsvc_authorise(rqstp);\t/* doesn't hurt to call this twice */\n\tdprintk(\"svc: svc_process dropit\\n\");\n\treturn 0;\n\n close:\n\tif (test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))\n\t\tsvc_close_xprt(rqstp->rq_xprt);\n\tdprintk(\"svc: svc_process close\\n\");\n\treturn 0;\n\nerr_short_len:\n\tsvc_printk(rqstp, \"short len %zd, dropping request\\n\",\n\t\t\targv->iov_len);\n\tgoto close;\n\nerr_bad_rpc:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 0);\t/* RPC_MISMATCH */\n\tsvc_putnl(resv, 2);\t/* Only RPCv2 supported */\n\tsvc_putnl(resv, 2);\n\tgoto sendit;\n\nerr_bad_auth:\n\tdprintk(\"svc: authentication failed (%d)\\n\", ntohl(auth_stat));\n\tserv->sv_stats->rpcbadauth++;\n\t/* Restore write pointer to location of accept status: */\n\txdr_ressize_check(rqstp, reply_statp);\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 1);\t/* AUTH_ERROR */\n\tsvc_putnl(resv, ntohl(auth_stat));\t/* status */\n\tgoto sendit;\n\nerr_bad_prog:\n\tdprintk(\"svc: unknown program %d\\n\", prog);\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_UNAVAIL);\n\tgoto sendit;\n\nerr_bad_vers:\n\tsvc_printk(rqstp, \"unknown version (%d for prog %d, %s)\\n\",\n\t\t       vers, prog, progp->pg_name);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_MISMATCH);\n\tsvc_putnl(resv, progp->pg_lovers);\n\tsvc_putnl(resv, progp->pg_hivers);\n\tgoto sendit;\n\nerr_bad_proc:\n\tsvc_printk(rqstp, \"unknown procedure (%d)\\n\", proc);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROC_UNAVAIL);\n\tgoto sendit;\n\nerr_garbage:\n\tsvc_printk(rqstp, \"failed to decode args\\n\");\n\n\trpc_stat = rpc_garbage_args;\nerr_bad:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, ntohl(rpc_stat));\n\tgoto sendit;\n}",
        "code_after_change": "static int\nsvc_process_common(struct svc_rqst *rqstp, struct kvec *argv, struct kvec *resv)\n{\n\tstruct svc_program\t*progp;\n\tconst struct svc_version *versp = NULL;\t/* compiler food */\n\tconst struct svc_procedure *procp = NULL;\n\tstruct svc_serv\t\t*serv = rqstp->rq_server;\n\t__be32\t\t\t*statp;\n\tu32\t\t\tprog, vers, proc;\n\t__be32\t\t\tauth_stat, rpc_stat;\n\tint\t\t\tauth_res;\n\t__be32\t\t\t*reply_statp;\n\n\trpc_stat = rpc_success;\n\n\tif (argv->iov_len < 6*4)\n\t\tgoto err_short_len;\n\n\t/* Will be turned off by GSS integrity and privacy services */\n\tset_bit(RQ_SPLICE_OK, &rqstp->rq_flags);\n\t/* Will be turned off only when NFSv4 Sessions are used */\n\tset_bit(RQ_USEDEFERRAL, &rqstp->rq_flags);\n\tclear_bit(RQ_DROPME, &rqstp->rq_flags);\n\n\t/* Setup reply header */\n\tif (rqstp->rq_prot == IPPROTO_TCP)\n\t\tsvc_tcp_prep_reply_hdr(rqstp);\n\n\tsvc_putu32(resv, rqstp->rq_xid);\n\n\tvers = svc_getnl(argv);\n\n\t/* First words of reply: */\n\tsvc_putnl(resv, 1);\t\t/* REPLY */\n\n\tif (vers != 2)\t\t/* RPC version number */\n\t\tgoto err_bad_rpc;\n\n\t/* Save position in case we later decide to reject: */\n\treply_statp = resv->iov_base + resv->iov_len;\n\n\tsvc_putnl(resv, 0);\t\t/* ACCEPT */\n\n\trqstp->rq_prog = prog = svc_getnl(argv);\t/* program number */\n\trqstp->rq_vers = vers = svc_getnl(argv);\t/* version number */\n\trqstp->rq_proc = proc = svc_getnl(argv);\t/* procedure number */\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next)\n\t\tif (prog == progp->pg_prog)\n\t\t\tbreak;\n\n\t/*\n\t * Decode auth data, and add verifier to reply buffer.\n\t * We do this before anything else in order to get a decent\n\t * auth verifier.\n\t */\n\tauth_res = svc_authenticate(rqstp, &auth_stat);\n\t/* Also give the program a chance to reject this call: */\n\tif (auth_res == SVC_OK && progp) {\n\t\tauth_stat = rpc_autherr_badcred;\n\t\tauth_res = progp->pg_authenticate(rqstp);\n\t}\n\tswitch (auth_res) {\n\tcase SVC_OK:\n\t\tbreak;\n\tcase SVC_GARBAGE:\n\t\tgoto err_garbage;\n\tcase SVC_SYSERR:\n\t\trpc_stat = rpc_system_err;\n\t\tgoto err_bad;\n\tcase SVC_DENIED:\n\t\tgoto err_bad_auth;\n\tcase SVC_CLOSE:\n\t\tgoto close;\n\tcase SVC_DROP:\n\t\tgoto dropit;\n\tcase SVC_COMPLETE:\n\t\tgoto sendit;\n\t}\n\n\tif (progp == NULL)\n\t\tgoto err_bad_prog;\n\n\tif (vers >= progp->pg_nvers ||\n\t  !(versp = progp->pg_vers[vers]))\n\t\tgoto err_bad_vers;\n\n\t/*\n\t * Some protocol versions (namely NFSv4) require some form of\n\t * congestion control.  (See RFC 7530 section 3.1 paragraph 2)\n\t * In other words, UDP is not allowed. We mark those when setting\n\t * up the svc_xprt, and verify that here.\n\t *\n\t * The spec is not very clear about what error should be returned\n\t * when someone tries to access a server that is listening on UDP\n\t * for lower versions. RPC_PROG_MISMATCH seems to be the closest\n\t * fit.\n\t */\n\tif (versp->vs_need_cong_ctrl && rqstp->rq_xprt &&\n\t    !test_bit(XPT_CONG_CTRL, &rqstp->rq_xprt->xpt_flags))\n\t\tgoto err_bad_vers;\n\n\tprocp = versp->vs_proc + proc;\n\tif (proc >= versp->vs_nproc || !procp->pc_func)\n\t\tgoto err_bad_proc;\n\trqstp->rq_procinfo = procp;\n\n\t/* Syntactic check complete */\n\tserv->sv_stats->rpccnt++;\n\ttrace_svc_process(rqstp, progp->pg_name);\n\n\t/* Build the reply header. */\n\tstatp = resv->iov_base +resv->iov_len;\n\tsvc_putnl(resv, RPC_SUCCESS);\n\n\t/* Bump per-procedure stats counter */\n\tversp->vs_count[proc]++;\n\n\t/* Initialize storage for argp and resp */\n\tmemset(rqstp->rq_argp, 0, procp->pc_argsize);\n\tmemset(rqstp->rq_resp, 0, procp->pc_ressize);\n\n\t/* un-reserve some of the out-queue now that we have a\n\t * better idea of reply size\n\t */\n\tif (procp->pc_xdrressize)\n\t\tsvc_reserve_auth(rqstp, procp->pc_xdrressize<<2);\n\n\t/* Call the function that processes the request. */\n\tif (!versp->vs_dispatch) {\n\t\t/*\n\t\t * Decode arguments\n\t\t * XXX: why do we ignore the return value?\n\t\t */\n\t\tif (procp->pc_decode &&\n\t\t    !procp->pc_decode(rqstp, argv->iov_base))\n\t\t\tgoto err_garbage;\n\n\t\t*statp = procp->pc_func(rqstp);\n\n\t\t/* Encode reply */\n\t\tif (*statp == rpc_drop_reply ||\n\t\t    test_bit(RQ_DROPME, &rqstp->rq_flags)) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto dropit;\n\t\t}\n\t\tif (*statp == rpc_autherr_badcred) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto err_bad_auth;\n\t\t}\n\t\tif (*statp == rpc_success && procp->pc_encode &&\n\t\t    !procp->pc_encode(rqstp, resv->iov_base + resv->iov_len)) {\n\t\t\tdprintk(\"svc: failed to encode reply\\n\");\n\t\t\t/* serv->sv_stats->rpcsystemerr++; */\n\t\t\t*statp = rpc_system_err;\n\t\t}\n\t} else {\n\t\tdprintk(\"svc: calling dispatcher\\n\");\n\t\tif (!versp->vs_dispatch(rqstp, statp)) {\n\t\t\t/* Release reply info */\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp);\n\t\t\tgoto dropit;\n\t\t}\n\t}\n\n\t/* Check RPC status result */\n\tif (*statp != rpc_success)\n\t\tresv->iov_len = ((void*)statp)  - resv->iov_base + 4;\n\n\t/* Release reply info */\n\tif (procp->pc_release)\n\t\tprocp->pc_release(rqstp);\n\n\tif (procp->pc_encode == NULL)\n\t\tgoto dropit;\n\n sendit:\n\tif (svc_authorise(rqstp))\n\t\tgoto close;\n\treturn 1;\t\t/* Caller can now send it */\n\n dropit:\n\tsvc_authorise(rqstp);\t/* doesn't hurt to call this twice */\n\tdprintk(\"svc: svc_process dropit\\n\");\n\treturn 0;\n\n close:\n\tif (rqstp->rq_xprt && test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))\n\t\tsvc_close_xprt(rqstp->rq_xprt);\n\tdprintk(\"svc: svc_process close\\n\");\n\treturn 0;\n\nerr_short_len:\n\tsvc_printk(rqstp, \"short len %zd, dropping request\\n\",\n\t\t\targv->iov_len);\n\tgoto close;\n\nerr_bad_rpc:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 0);\t/* RPC_MISMATCH */\n\tsvc_putnl(resv, 2);\t/* Only RPCv2 supported */\n\tsvc_putnl(resv, 2);\n\tgoto sendit;\n\nerr_bad_auth:\n\tdprintk(\"svc: authentication failed (%d)\\n\", ntohl(auth_stat));\n\tserv->sv_stats->rpcbadauth++;\n\t/* Restore write pointer to location of accept status: */\n\txdr_ressize_check(rqstp, reply_statp);\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 1);\t/* AUTH_ERROR */\n\tsvc_putnl(resv, ntohl(auth_stat));\t/* status */\n\tgoto sendit;\n\nerr_bad_prog:\n\tdprintk(\"svc: unknown program %d\\n\", prog);\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_UNAVAIL);\n\tgoto sendit;\n\nerr_bad_vers:\n\tsvc_printk(rqstp, \"unknown version (%d for prog %d, %s)\\n\",\n\t\t       vers, prog, progp->pg_name);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_MISMATCH);\n\tsvc_putnl(resv, progp->pg_lovers);\n\tsvc_putnl(resv, progp->pg_hivers);\n\tgoto sendit;\n\nerr_bad_proc:\n\tsvc_printk(rqstp, \"unknown procedure (%d)\\n\", proc);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROC_UNAVAIL);\n\tgoto sendit;\n\nerr_garbage:\n\tsvc_printk(rqstp, \"failed to decode args\\n\");\n\n\trpc_stat = rpc_garbage_args;\nerr_bad:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, ntohl(rpc_stat));\n\tgoto sendit;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,7 +23,8 @@\n \tclear_bit(RQ_DROPME, &rqstp->rq_flags);\n \n \t/* Setup reply header */\n-\trqstp->rq_xprt->xpt_ops->xpo_prep_reply_hdr(rqstp);\n+\tif (rqstp->rq_prot == IPPROTO_TCP)\n+\t\tsvc_tcp_prep_reply_hdr(rqstp);\n \n \tsvc_putu32(resv, rqstp->rq_xid);\n \n@@ -95,7 +96,7 @@\n \t * for lower versions. RPC_PROG_MISMATCH seems to be the closest\n \t * fit.\n \t */\n-\tif (versp->vs_need_cong_ctrl &&\n+\tif (versp->vs_need_cong_ctrl && rqstp->rq_xprt &&\n \t    !test_bit(XPT_CONG_CTRL, &rqstp->rq_xprt->xpt_flags))\n \t\tgoto err_bad_vers;\n \n@@ -187,7 +188,7 @@\n \treturn 0;\n \n  close:\n-\tif (test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))\n+\tif (rqstp->rq_xprt && test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))\n \t\tsvc_close_xprt(rqstp->rq_xprt);\n \tdprintk(\"svc: svc_process close\\n\");\n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tif (rqstp->rq_prot == IPPROTO_TCP)",
                "\t\tsvc_tcp_prep_reply_hdr(rqstp);",
                "\tif (versp->vs_need_cong_ctrl && rqstp->rq_xprt &&",
                "\tif (rqstp->rq_xprt && test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))"
            ],
            "deleted": [
                "\trqstp->rq_xprt->xpt_ops->xpo_prep_reply_hdr(rqstp);",
                "\tif (versp->vs_need_cong_ctrl &&",
                "\tif (test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's NFS41+ subsystem. NFS41+ shares mounted in different network namespaces at the same time can make bc_svc_process() use wrong back-channel IDs and cause a use-after-free vulnerability. Thus a malicious container user can cause a host kernel memory corruption and a system panic. Due to the nature of the flaw, privilege escalation cannot be fully ruled out.",
        "id": 1722
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static int aio_prep_rw(struct kiocb *req, const struct iocb *iocb)\n{\n\tint ret;\n\n\treq->ki_filp = fget(iocb->aio_fildes);\n\tif (unlikely(!req->ki_filp))\n\t\treturn -EBADF;\n\treq->ki_complete = aio_complete_rw;\n\treq->private = NULL;\n\treq->ki_pos = iocb->aio_offset;\n\treq->ki_flags = iocb_flags(req->ki_filp);\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD)\n\t\treq->ki_flags |= IOCB_EVENTFD;\n\treq->ki_hint = ki_hint_validate(file_write_hint(req->ki_filp));\n\tif (iocb->aio_flags & IOCB_FLAG_IOPRIO) {\n\t\t/*\n\t\t * If the IOCB_FLAG_IOPRIO flag of aio_flags is set, then\n\t\t * aio_reqprio is interpreted as an I/O scheduling\n\t\t * class and priority.\n\t\t */\n\t\tret = ioprio_check_cap(iocb->aio_reqprio);\n\t\tif (ret) {\n\t\t\tpr_debug(\"aio ioprio check cap error: %d\\n\", ret);\n\t\t\tgoto out_fput;\n\t\t}\n\n\t\treq->ki_ioprio = iocb->aio_reqprio;\n\t} else\n\t\treq->ki_ioprio = get_current_ioprio();\n\n\tret = kiocb_set_rw_flags(req, iocb->aio_rw_flags);\n\tif (unlikely(ret))\n\t\tgoto out_fput;\n\n\treq->ki_flags &= ~IOCB_HIPRI; /* no one is going to poll for this I/O */\n\treturn 0;\n\nout_fput:\n\tfput(req->ki_filp);\n\treturn ret;\n}",
        "code_after_change": "static int aio_prep_rw(struct kiocb *req, const struct iocb *iocb)\n{\n\tint ret;\n\n\treq->ki_complete = aio_complete_rw;\n\treq->private = NULL;\n\treq->ki_pos = iocb->aio_offset;\n\treq->ki_flags = iocb_flags(req->ki_filp);\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD)\n\t\treq->ki_flags |= IOCB_EVENTFD;\n\treq->ki_hint = ki_hint_validate(file_write_hint(req->ki_filp));\n\tif (iocb->aio_flags & IOCB_FLAG_IOPRIO) {\n\t\t/*\n\t\t * If the IOCB_FLAG_IOPRIO flag of aio_flags is set, then\n\t\t * aio_reqprio is interpreted as an I/O scheduling\n\t\t * class and priority.\n\t\t */\n\t\tret = ioprio_check_cap(iocb->aio_reqprio);\n\t\tif (ret) {\n\t\t\tpr_debug(\"aio ioprio check cap error: %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\n\t\treq->ki_ioprio = iocb->aio_reqprio;\n\t} else\n\t\treq->ki_ioprio = get_current_ioprio();\n\n\tret = kiocb_set_rw_flags(req, iocb->aio_rw_flags);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\treq->ki_flags &= ~IOCB_HIPRI; /* no one is going to poll for this I/O */\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,9 +2,6 @@\n {\n \tint ret;\n \n-\treq->ki_filp = fget(iocb->aio_fildes);\n-\tif (unlikely(!req->ki_filp))\n-\t\treturn -EBADF;\n \treq->ki_complete = aio_complete_rw;\n \treq->private = NULL;\n \treq->ki_pos = iocb->aio_offset;\n@@ -21,7 +18,7 @@\n \t\tret = ioprio_check_cap(iocb->aio_reqprio);\n \t\tif (ret) {\n \t\t\tpr_debug(\"aio ioprio check cap error: %d\\n\", ret);\n-\t\t\tgoto out_fput;\n+\t\t\treturn ret;\n \t\t}\n \n \t\treq->ki_ioprio = iocb->aio_reqprio;\n@@ -30,12 +27,8 @@\n \n \tret = kiocb_set_rw_flags(req, iocb->aio_rw_flags);\n \tif (unlikely(ret))\n-\t\tgoto out_fput;\n+\t\treturn ret;\n \n \treq->ki_flags &= ~IOCB_HIPRI; /* no one is going to poll for this I/O */\n \treturn 0;\n-\n-out_fput:\n-\tfput(req->ki_filp);\n-\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t\t\treturn ret;",
                "\t\treturn ret;"
            ],
            "deleted": [
                "\treq->ki_filp = fget(iocb->aio_fildes);",
                "\tif (unlikely(!req->ki_filp))",
                "\t\treturn -EBADF;",
                "\t\t\tgoto out_fput;",
                "\t\tgoto out_fput;",
                "",
                "out_fput:",
                "\tfput(req->ki_filp);",
                "\treturn ret;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1884
    },
    {
        "cve_id": "CVE-2021-39800",
        "code_before_change": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\tstruct ion_handle *handle;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_buffer *buffer = NULL;\n\tstruct ion_heap *heap;\n\tint ret;\n\n\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n\t\t len, align, heap_id_mask, flags);\n\t/*\n\t * traverse the list of heaps available in this system in priority\n\t * order.  If the heap type is supported by the client, and matches the\n\t * request of the caller allocate from it.  Repeat until allocate has\n\t * succeeded or all heaps have been tried\n\t */\n\tlen = PAGE_ALIGN(len);\n\n\tif (!len)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdown_read(&dev->lock);\n\tplist_for_each_entry(heap, &dev->heaps, node) {\n\t\t/* if the caller didn't specify this heap id */\n\t\tif (!((1 << heap->id) & heap_id_mask))\n\t\t\tcontinue;\n\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n\t\tif (!IS_ERR(buffer))\n\t\t\tbreak;\n\t}\n\tup_read(&dev->lock);\n\n\tif (buffer == NULL)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (IS_ERR(buffer))\n\t\treturn ERR_CAST(buffer);\n\n\thandle = ion_handle_create(client, buffer);\n\n\t/*\n\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n\t * and ion_handle_create will take a second reference, drop one here\n\t */\n\tion_buffer_put(buffer);\n\n\tif (IS_ERR(handle))\n\t\treturn handle;\n\n\tmutex_lock(&client->lock);\n\tret = ion_handle_add(client, handle);\n\tmutex_unlock(&client->lock);\n\tif (ret) {\n\t\tion_handle_put(handle);\n\t\thandle = ERR_PTR(ret);\n\t}\n\n\treturn handle;\n}",
        "code_after_change": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,60 +2,5 @@\n \t\t\t     size_t align, unsigned int heap_id_mask,\n \t\t\t     unsigned int flags)\n {\n-\tstruct ion_handle *handle;\n-\tstruct ion_device *dev = client->dev;\n-\tstruct ion_buffer *buffer = NULL;\n-\tstruct ion_heap *heap;\n-\tint ret;\n-\n-\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n-\t\t len, align, heap_id_mask, flags);\n-\t/*\n-\t * traverse the list of heaps available in this system in priority\n-\t * order.  If the heap type is supported by the client, and matches the\n-\t * request of the caller allocate from it.  Repeat until allocate has\n-\t * succeeded or all heaps have been tried\n-\t */\n-\tlen = PAGE_ALIGN(len);\n-\n-\tif (!len)\n-\t\treturn ERR_PTR(-EINVAL);\n-\n-\tdown_read(&dev->lock);\n-\tplist_for_each_entry(heap, &dev->heaps, node) {\n-\t\t/* if the caller didn't specify this heap id */\n-\t\tif (!((1 << heap->id) & heap_id_mask))\n-\t\t\tcontinue;\n-\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n-\t\tif (!IS_ERR(buffer))\n-\t\t\tbreak;\n-\t}\n-\tup_read(&dev->lock);\n-\n-\tif (buffer == NULL)\n-\t\treturn ERR_PTR(-ENODEV);\n-\n-\tif (IS_ERR(buffer))\n-\t\treturn ERR_CAST(buffer);\n-\n-\thandle = ion_handle_create(client, buffer);\n-\n-\t/*\n-\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n-\t * and ion_handle_create will take a second reference, drop one here\n-\t */\n-\tion_buffer_put(buffer);\n-\n-\tif (IS_ERR(handle))\n-\t\treturn handle;\n-\n-\tmutex_lock(&client->lock);\n-\tret = ion_handle_add(client, handle);\n-\tmutex_unlock(&client->lock);\n-\tif (ret) {\n-\t\tion_handle_put(handle);\n-\t\thandle = ERR_PTR(ret);\n-\t}\n-\n-\treturn handle;\n+\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);"
            ],
            "deleted": [
                "\tstruct ion_handle *handle;",
                "\tstruct ion_device *dev = client->dev;",
                "\tstruct ion_buffer *buffer = NULL;",
                "\tstruct ion_heap *heap;",
                "\tint ret;",
                "",
                "\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,",
                "\t\t len, align, heap_id_mask, flags);",
                "\t/*",
                "\t * traverse the list of heaps available in this system in priority",
                "\t * order.  If the heap type is supported by the client, and matches the",
                "\t * request of the caller allocate from it.  Repeat until allocate has",
                "\t * succeeded or all heaps have been tried",
                "\t */",
                "\tlen = PAGE_ALIGN(len);",
                "",
                "\tif (!len)",
                "\t\treturn ERR_PTR(-EINVAL);",
                "",
                "\tdown_read(&dev->lock);",
                "\tplist_for_each_entry(heap, &dev->heaps, node) {",
                "\t\t/* if the caller didn't specify this heap id */",
                "\t\tif (!((1 << heap->id) & heap_id_mask))",
                "\t\t\tcontinue;",
                "\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);",
                "\t\tif (!IS_ERR(buffer))",
                "\t\t\tbreak;",
                "\t}",
                "\tup_read(&dev->lock);",
                "",
                "\tif (buffer == NULL)",
                "\t\treturn ERR_PTR(-ENODEV);",
                "",
                "\tif (IS_ERR(buffer))",
                "\t\treturn ERR_CAST(buffer);",
                "",
                "\thandle = ion_handle_create(client, buffer);",
                "",
                "\t/*",
                "\t * ion_buffer_create will create a buffer with a ref_cnt of 1,",
                "\t * and ion_handle_create will take a second reference, drop one here",
                "\t */",
                "\tion_buffer_put(buffer);",
                "",
                "\tif (IS_ERR(handle))",
                "\t\treturn handle;",
                "",
                "\tmutex_lock(&client->lock);",
                "\tret = ion_handle_add(client, handle);",
                "\tmutex_unlock(&client->lock);",
                "\tif (ret) {",
                "\t\tion_handle_put(handle);",
                "\t\thandle = ERR_PTR(ret);",
                "\t}",
                "",
                "\treturn handle;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In ion_ioctl of ion-ioctl.c, there is a possible way to leak kernel head data due to a use after free. This could lead to local information disclosure with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-208277166References: Upstream kernel",
        "id": 3109
    },
    {
        "cve_id": "CVE-2022-3977",
        "code_before_change": "static void __mctp_key_done_in(struct mctp_sk_key *key, struct net *net,\n\t\t\t       unsigned long flags, unsigned long reason)\n__releases(&key->lock)\n{\n\tstruct sk_buff *skb;\n\n\ttrace_mctp_key_release(key, reason);\n\tskb = key->reasm_head;\n\tkey->reasm_head = NULL;\n\n\tif (!key->manual_alloc) {\n\t\tkey->reasm_dead = true;\n\t\tkey->valid = false;\n\t\tmctp_dev_release_key(key->dev, key);\n\t}\n\tspin_unlock_irqrestore(&key->lock, flags);\n\n\tif (!key->manual_alloc) {\n\t\tspin_lock_irqsave(&net->mctp.keys_lock, flags);\n\t\thlist_del(&key->hlist);\n\t\thlist_del(&key->sklist);\n\t\tspin_unlock_irqrestore(&net->mctp.keys_lock, flags);\n\n\t\t/* unref for the lists */\n\t\tmctp_key_unref(key);\n\t}\n\n\t/* and one for the local reference */\n\tmctp_key_unref(key);\n\n\tkfree_skb(skb);\n}",
        "code_after_change": "static void __mctp_key_done_in(struct mctp_sk_key *key, struct net *net,\n\t\t\t       unsigned long flags, unsigned long reason)\n__releases(&key->lock)\n{\n\tstruct sk_buff *skb;\n\n\ttrace_mctp_key_release(key, reason);\n\tskb = key->reasm_head;\n\tkey->reasm_head = NULL;\n\n\tif (!key->manual_alloc) {\n\t\tkey->reasm_dead = true;\n\t\tkey->valid = false;\n\t\tmctp_dev_release_key(key->dev, key);\n\t}\n\tspin_unlock_irqrestore(&key->lock, flags);\n\n\tif (!key->manual_alloc) {\n\t\tspin_lock_irqsave(&net->mctp.keys_lock, flags);\n\t\tif (!hlist_unhashed(&key->hlist)) {\n\t\t\thlist_del_init(&key->hlist);\n\t\t\thlist_del_init(&key->sklist);\n\t\t\tmctp_key_unref(key);\n\t\t}\n\t\tspin_unlock_irqrestore(&net->mctp.keys_lock, flags);\n\t}\n\n\t/* and one for the local reference */\n\tmctp_key_unref(key);\n\n\tkfree_skb(skb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,12 +17,12 @@\n \n \tif (!key->manual_alloc) {\n \t\tspin_lock_irqsave(&net->mctp.keys_lock, flags);\n-\t\thlist_del(&key->hlist);\n-\t\thlist_del(&key->sklist);\n+\t\tif (!hlist_unhashed(&key->hlist)) {\n+\t\t\thlist_del_init(&key->hlist);\n+\t\t\thlist_del_init(&key->sklist);\n+\t\t\tmctp_key_unref(key);\n+\t\t}\n \t\tspin_unlock_irqrestore(&net->mctp.keys_lock, flags);\n-\n-\t\t/* unref for the lists */\n-\t\tmctp_key_unref(key);\n \t}\n \n \t/* and one for the local reference */",
        "function_modified_lines": {
            "added": [
                "\t\tif (!hlist_unhashed(&key->hlist)) {",
                "\t\t\thlist_del_init(&key->hlist);",
                "\t\t\thlist_del_init(&key->sklist);",
                "\t\t\tmctp_key_unref(key);",
                "\t\t}"
            ],
            "deleted": [
                "\t\thlist_del(&key->hlist);",
                "\t\thlist_del(&key->sklist);",
                "",
                "\t\t/* unref for the lists */",
                "\t\tmctp_key_unref(key);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel MCTP (Management Component Transport Protocol) functionality. This issue occurs when a user simultaneously calls DROPTAG ioctl and socket close happens, which could allow a local user to crash the system or potentially escalate their privileges on the system.",
        "id": 3701
    },
    {
        "cve_id": "CVE-2020-27786",
        "code_before_change": "static int resize_runtime_buffer(struct snd_rawmidi_runtime *runtime,\n\t\t\t\t struct snd_rawmidi_params *params,\n\t\t\t\t bool is_input)\n{\n\tchar *newbuf, *oldbuf;\n\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L)\n\t\treturn -EINVAL;\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size)\n\t\treturn -EINVAL;\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = kvzalloc(params->buffer_size, GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\tspin_lock_irq(&runtime->lock);\n\t\toldbuf = runtime->buffer;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t\t__reset_runtime_ptrs(runtime, is_input);\n\t\tspin_unlock_irq(&runtime->lock);\n\t\tkvfree(oldbuf);\n\t}\n\truntime->avail_min = params->avail_min;\n\treturn 0;\n}",
        "code_after_change": "static int resize_runtime_buffer(struct snd_rawmidi_runtime *runtime,\n\t\t\t\t struct snd_rawmidi_params *params,\n\t\t\t\t bool is_input)\n{\n\tchar *newbuf, *oldbuf;\n\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L)\n\t\treturn -EINVAL;\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size)\n\t\treturn -EINVAL;\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = kvzalloc(params->buffer_size, GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\tspin_lock_irq(&runtime->lock);\n\t\tif (runtime->buffer_ref) {\n\t\t\tspin_unlock_irq(&runtime->lock);\n\t\t\tkvfree(newbuf);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\toldbuf = runtime->buffer;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t\t__reset_runtime_ptrs(runtime, is_input);\n\t\tspin_unlock_irq(&runtime->lock);\n\t\tkvfree(oldbuf);\n\t}\n\truntime->avail_min = params->avail_min;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,11 @@\n \t\tif (!newbuf)\n \t\t\treturn -ENOMEM;\n \t\tspin_lock_irq(&runtime->lock);\n+\t\tif (runtime->buffer_ref) {\n+\t\t\tspin_unlock_irq(&runtime->lock);\n+\t\t\tkvfree(newbuf);\n+\t\t\treturn -EBUSY;\n+\t\t}\n \t\toldbuf = runtime->buffer;\n \t\truntime->buffer = newbuf;\n \t\truntime->buffer_size = params->buffer_size;",
        "function_modified_lines": {
            "added": [
                "\t\tif (runtime->buffer_ref) {",
                "\t\t\tspin_unlock_irq(&runtime->lock);",
                "\t\t\tkvfree(newbuf);",
                "\t\t\treturn -EBUSY;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s implementation of MIDI, where an attacker with a local account and the permissions to issue ioctl commands to midi devices could trigger a use-after-free issue. A write to this specific memory while freed and before use causes the flow of execution to change and possibly allow for memory corruption or privilege escalation. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability.",
        "id": 2635
    },
    {
        "cve_id": "CVE-2022-45919",
        "code_before_change": "int dvb_ca_en50221_init(struct dvb_adapter *dvb_adapter,\n\t\t\tstruct dvb_ca_en50221 *pubca, int flags, int slot_count)\n{\n\tint ret;\n\tstruct dvb_ca_private *ca = NULL;\n\tint i;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\tif (slot_count < 1)\n\t\treturn -EINVAL;\n\n\t/* initialise the system data */\n\tca = kzalloc(sizeof(*ca), GFP_KERNEL);\n\tif (!ca) {\n\t\tret = -ENOMEM;\n\t\tgoto exit;\n\t}\n\tkref_init(&ca->refcount);\n\tca->pub = pubca;\n\tca->flags = flags;\n\tca->slot_count = slot_count;\n\tca->slot_info = kcalloc(slot_count, sizeof(struct dvb_ca_slot),\n\t\t\t\tGFP_KERNEL);\n\tif (!ca->slot_info) {\n\t\tret = -ENOMEM;\n\t\tgoto free_ca;\n\t}\n\tinit_waitqueue_head(&ca->wait_queue);\n\tca->open = 0;\n\tca->wakeup = 0;\n\tca->next_read_slot = 0;\n\tpubca->private = ca;\n\n\t/* register the DVB device */\n\tret = dvb_register_device(dvb_adapter, &ca->dvbdev, &dvbdev_ca, ca,\n\t\t\t\t  DVB_DEVICE_CA, 0);\n\tif (ret)\n\t\tgoto free_slot_info;\n\n\t/* now initialise each slot */\n\tfor (i = 0; i < slot_count; i++) {\n\t\tstruct dvb_ca_slot *sl = &ca->slot_info[i];\n\n\t\tmemset(sl, 0, sizeof(struct dvb_ca_slot));\n\t\tsl->slot_state = DVB_CA_SLOTSTATE_NONE;\n\t\tatomic_set(&sl->camchange_count, 0);\n\t\tsl->camchange_type = DVB_CA_EN50221_CAMCHANGE_REMOVED;\n\t\tmutex_init(&sl->slot_lock);\n\t}\n\n\tmutex_init(&ca->ioctl_mutex);\n\n\tif (signal_pending(current)) {\n\t\tret = -EINTR;\n\t\tgoto unregister_device;\n\t}\n\tmb();\n\n\t/* create a kthread for monitoring this CA device */\n\tca->thread = kthread_run(dvb_ca_en50221_thread, ca, \"kdvb-ca-%i:%i\",\n\t\t\t\t ca->dvbdev->adapter->num, ca->dvbdev->id);\n\tif (IS_ERR(ca->thread)) {\n\t\tret = PTR_ERR(ca->thread);\n\t\tpr_err(\"dvb_ca_init: failed to start kernel_thread (%d)\\n\",\n\t\t       ret);\n\t\tgoto unregister_device;\n\t}\n\treturn 0;\n\nunregister_device:\n\tdvb_unregister_device(ca->dvbdev);\nfree_slot_info:\n\tkfree(ca->slot_info);\nfree_ca:\n\tkfree(ca);\nexit:\n\tpubca->private = NULL;\n\treturn ret;\n}",
        "code_after_change": "int dvb_ca_en50221_init(struct dvb_adapter *dvb_adapter,\n\t\t\tstruct dvb_ca_en50221 *pubca, int flags, int slot_count)\n{\n\tint ret;\n\tstruct dvb_ca_private *ca = NULL;\n\tint i;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\tif (slot_count < 1)\n\t\treturn -EINVAL;\n\n\t/* initialise the system data */\n\tca = kzalloc(sizeof(*ca), GFP_KERNEL);\n\tif (!ca) {\n\t\tret = -ENOMEM;\n\t\tgoto exit;\n\t}\n\tkref_init(&ca->refcount);\n\tca->pub = pubca;\n\tca->flags = flags;\n\tca->slot_count = slot_count;\n\tca->slot_info = kcalloc(slot_count, sizeof(struct dvb_ca_slot),\n\t\t\t\tGFP_KERNEL);\n\tif (!ca->slot_info) {\n\t\tret = -ENOMEM;\n\t\tgoto free_ca;\n\t}\n\tinit_waitqueue_head(&ca->wait_queue);\n\tca->open = 0;\n\tca->wakeup = 0;\n\tca->next_read_slot = 0;\n\tpubca->private = ca;\n\n\t/* register the DVB device */\n\tret = dvb_register_device(dvb_adapter, &ca->dvbdev, &dvbdev_ca, ca,\n\t\t\t\t  DVB_DEVICE_CA, 0);\n\tif (ret)\n\t\tgoto free_slot_info;\n\n\t/* now initialise each slot */\n\tfor (i = 0; i < slot_count; i++) {\n\t\tstruct dvb_ca_slot *sl = &ca->slot_info[i];\n\n\t\tmemset(sl, 0, sizeof(struct dvb_ca_slot));\n\t\tsl->slot_state = DVB_CA_SLOTSTATE_NONE;\n\t\tatomic_set(&sl->camchange_count, 0);\n\t\tsl->camchange_type = DVB_CA_EN50221_CAMCHANGE_REMOVED;\n\t\tmutex_init(&sl->slot_lock);\n\t}\n\n\tmutex_init(&ca->ioctl_mutex);\n\tmutex_init(&ca->remove_mutex);\n\n\tif (signal_pending(current)) {\n\t\tret = -EINTR;\n\t\tgoto unregister_device;\n\t}\n\tmb();\n\n\t/* create a kthread for monitoring this CA device */\n\tca->thread = kthread_run(dvb_ca_en50221_thread, ca, \"kdvb-ca-%i:%i\",\n\t\t\t\t ca->dvbdev->adapter->num, ca->dvbdev->id);\n\tif (IS_ERR(ca->thread)) {\n\t\tret = PTR_ERR(ca->thread);\n\t\tpr_err(\"dvb_ca_init: failed to start kernel_thread (%d)\\n\",\n\t\t       ret);\n\t\tgoto unregister_device;\n\t}\n\treturn 0;\n\nunregister_device:\n\tdvb_unregister_device(ca->dvbdev);\nfree_slot_info:\n\tkfree(ca->slot_info);\nfree_ca:\n\tkfree(ca);\nexit:\n\tpubca->private = NULL;\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -50,6 +50,7 @@\n \t}\n \n \tmutex_init(&ca->ioctl_mutex);\n+\tmutex_init(&ca->remove_mutex);\n \n \tif (signal_pending(current)) {\n \t\tret = -EINTR;",
        "function_modified_lines": {
            "added": [
                "\tmutex_init(&ca->remove_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.0.10. In drivers/media/dvb-core/dvb_ca_en50221.c, a use-after-free can occur is there is a disconnect after an open, because of the lack of a wait_event.",
        "id": 3755
    },
    {
        "cve_id": "CVE-2023-45898",
        "code_before_change": "void ext4_es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len)\n{\n\text4_lblk_t end;\n\tint err = 0;\n\tint reserved = 0;\n\tstruct extent_status *es = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\ttrace_ext4_es_remove_extent(inode, lblk, len);\n\tes_debug(\"remove [%u/%u) from extent status tree of inode %lu\\n\",\n\t\t lblk, len, inode->i_ino);\n\n\tif (!len)\n\t\treturn;\n\n\tend = lblk + len - 1;\n\tBUG_ON(end < lblk);\n\nretry:\n\tif (err && !es)\n\t\tes = __es_alloc_extent(true);\n\t/*\n\t * ext4_clear_inode() depends on us taking i_es_lock unconditionally\n\t * so that we are sure __es_shrink() is done with the inode before it\n\t * is reclaimed.\n\t */\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr = __es_remove_extent(inode, lblk, end, &reserved, es);\n\tif (es && !es->es_len)\n\t\t__es_free_extent(es);\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\text4_da_release_space(inode, reserved);\n\treturn;\n}",
        "code_after_change": "void ext4_es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len)\n{\n\text4_lblk_t end;\n\tint err = 0;\n\tint reserved = 0;\n\tstruct extent_status *es = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\ttrace_ext4_es_remove_extent(inode, lblk, len);\n\tes_debug(\"remove [%u/%u) from extent status tree of inode %lu\\n\",\n\t\t lblk, len, inode->i_ino);\n\n\tif (!len)\n\t\treturn;\n\n\tend = lblk + len - 1;\n\tBUG_ON(end < lblk);\n\nretry:\n\tif (err && !es)\n\t\tes = __es_alloc_extent(true);\n\t/*\n\t * ext4_clear_inode() depends on us taking i_es_lock unconditionally\n\t * so that we are sure __es_shrink() is done with the inode before it\n\t * is reclaimed.\n\t */\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr = __es_remove_extent(inode, lblk, end, &reserved, es);\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es) {\n\t\tif (!es->es_len)\n\t\t\t__es_free_extent(es);\n\t\tes = NULL;\n\t}\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\text4_da_release_space(inode, reserved);\n\treturn;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,8 +29,12 @@\n \t */\n \twrite_lock(&EXT4_I(inode)->i_es_lock);\n \terr = __es_remove_extent(inode, lblk, end, &reserved, es);\n-\tif (es && !es->es_len)\n-\t\t__es_free_extent(es);\n+\t/* Free preallocated extent if it didn't get used. */\n+\tif (es) {\n+\t\tif (!es->es_len)\n+\t\t\t__es_free_extent(es);\n+\t\tes = NULL;\n+\t}\n \twrite_unlock(&EXT4_I(inode)->i_es_lock);\n \tif (err)\n \t\tgoto retry;",
        "function_modified_lines": {
            "added": [
                "\t/* Free preallocated extent if it didn't get used. */",
                "\tif (es) {",
                "\t\tif (!es->es_len)",
                "\t\t\t__es_free_extent(es);",
                "\t\tes = NULL;",
                "\t}"
            ],
            "deleted": [
                "\tif (es && !es->es_len)",
                "\t\t__es_free_extent(es);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 6.5.4 has an es1 use-after-free in fs/ext4/extents_status.c, related to ext4_es_insert_extent.",
        "id": 4228
    },
    {
        "cve_id": "CVE-2020-0433",
        "code_before_change": "void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,\n\t\tvoid *priv)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tstruct blk_mq_tags *tags = hctx->tags;\n\n\t\t/*\n\t\t * If not software queues are currently mapped to this\n\t\t * hardware queue, there's nothing to check\n\t\t */\n\t\tif (!blk_mq_hw_queue_mapped(hctx))\n\t\t\tcontinue;\n\n\t\tif (tags->nr_reserved_tags)\n\t\t\tbt_for_each(hctx, &tags->breserved_tags, fn, priv, true);\n\t\tbt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);\n\t}\n\n}",
        "code_after_change": "void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,\n\t\tvoid *priv)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\t/*\n\t * __blk_mq_update_nr_hw_queues will update the nr_hw_queues and\n\t * queue_hw_ctx after freeze the queue. So we could use q_usage_counter\n\t * to avoid race with it. __blk_mq_update_nr_hw_queues will users\n\t * synchronize_rcu to ensure all of the users go out of the critical\n\t * section below and see zeroed q_usage_counter.\n\t */\n\trcu_read_lock();\n\tif (percpu_ref_is_zero(&q->q_usage_counter)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tstruct blk_mq_tags *tags = hctx->tags;\n\n\t\t/*\n\t\t * If not software queues are currently mapped to this\n\t\t * hardware queue, there's nothing to check\n\t\t */\n\t\tif (!blk_mq_hw_queue_mapped(hctx))\n\t\t\tcontinue;\n\n\t\tif (tags->nr_reserved_tags)\n\t\t\tbt_for_each(hctx, &tags->breserved_tags, fn, priv, true);\n\t\tbt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);\n\t}\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,18 @@\n \tstruct blk_mq_hw_ctx *hctx;\n \tint i;\n \n+\t/*\n+\t * __blk_mq_update_nr_hw_queues will update the nr_hw_queues and\n+\t * queue_hw_ctx after freeze the queue. So we could use q_usage_counter\n+\t * to avoid race with it. __blk_mq_update_nr_hw_queues will users\n+\t * synchronize_rcu to ensure all of the users go out of the critical\n+\t * section below and see zeroed q_usage_counter.\n+\t */\n+\trcu_read_lock();\n+\tif (percpu_ref_is_zero(&q->q_usage_counter)) {\n+\t\trcu_read_unlock();\n+\t\treturn;\n+\t}\n \n \tqueue_for_each_hw_ctx(q, hctx, i) {\n \t\tstruct blk_mq_tags *tags = hctx->tags;\n@@ -19,5 +31,5 @@\n \t\t\tbt_for_each(hctx, &tags->breserved_tags, fn, priv, true);\n \t\tbt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);\n \t}\n-\n+\trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * __blk_mq_update_nr_hw_queues will update the nr_hw_queues and",
                "\t * queue_hw_ctx after freeze the queue. So we could use q_usage_counter",
                "\t * to avoid race with it. __blk_mq_update_nr_hw_queues will users",
                "\t * synchronize_rcu to ensure all of the users go out of the critical",
                "\t * section below and see zeroed q_usage_counter.",
                "\t */",
                "\trcu_read_lock();",
                "\tif (percpu_ref_is_zero(&q->q_usage_counter)) {",
                "\t\trcu_read_unlock();",
                "\t\treturn;",
                "\t}",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In blk_mq_queue_tag_busy_iter of blk-mq-tag.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-151939299",
        "id": 2387
    },
    {
        "cve_id": "CVE-2023-1249",
        "code_before_change": "void do_coredump(const kernel_siginfo_t *siginfo)\n{\n\tstruct core_state core_state;\n\tstruct core_name cn;\n\tstruct mm_struct *mm = current->mm;\n\tstruct linux_binfmt * binfmt;\n\tconst struct cred *old_cred;\n\tstruct cred *cred;\n\tint retval = 0;\n\tint ispipe;\n\tsize_t *argv = NULL;\n\tint argc = 0;\n\t/* require nonrelative corefile path and be extra careful */\n\tbool need_suid_safe = false;\n\tbool core_dumped = false;\n\tstatic atomic_t core_dump_count = ATOMIC_INIT(0);\n\tstruct coredump_params cprm = {\n\t\t.siginfo = siginfo,\n\t\t.regs = signal_pt_regs(),\n\t\t.limit = rlimit(RLIMIT_CORE),\n\t\t/*\n\t\t * We must use the same mm->flags while dumping core to avoid\n\t\t * inconsistency of bit flags, since this flag is not protected\n\t\t * by any locks.\n\t\t */\n\t\t.mm_flags = mm->flags,\n\t\t.vma_meta = NULL,\n\t};\n\n\taudit_core_dumps(siginfo->si_signo);\n\n\tbinfmt = mm->binfmt;\n\tif (!binfmt || !binfmt->core_dump)\n\t\tgoto fail;\n\tif (!__get_dumpable(cprm.mm_flags))\n\t\tgoto fail;\n\n\tcred = prepare_creds();\n\tif (!cred)\n\t\tgoto fail;\n\t/*\n\t * We cannot trust fsuid as being the \"true\" uid of the process\n\t * nor do we know its entire history. We only know it was tainted\n\t * so we dump it as root in mode 2, and only into a controlled\n\t * environment (pipe handler or fully qualified path).\n\t */\n\tif (__get_dumpable(cprm.mm_flags) == SUID_DUMP_ROOT) {\n\t\t/* Setuid core dump mode */\n\t\tcred->fsuid = GLOBAL_ROOT_UID;\t/* Dump root private */\n\t\tneed_suid_safe = true;\n\t}\n\n\tretval = coredump_wait(siginfo->si_signo, &core_state);\n\tif (retval < 0)\n\t\tgoto fail_creds;\n\n\told_cred = override_creds(cred);\n\n\tispipe = format_corename(&cn, &cprm, &argv, &argc);\n\n\tif (ispipe) {\n\t\tint argi;\n\t\tint dump_count;\n\t\tchar **helper_argv;\n\t\tstruct subprocess_info *sub_info;\n\n\t\tif (ispipe < 0) {\n\t\t\tprintk(KERN_WARNING \"format_corename failed\\n\");\n\t\t\tprintk(KERN_WARNING \"Aborting core\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\n\t\tif (cprm.limit == 1) {\n\t\t\t/* See umh_pipe_setup() which sets RLIMIT_CORE = 1.\n\t\t\t *\n\t\t\t * Normally core limits are irrelevant to pipes, since\n\t\t\t * we're not writing to the file system, but we use\n\t\t\t * cprm.limit of 1 here as a special value, this is a\n\t\t\t * consistent way to catch recursive crashes.\n\t\t\t * We can still crash if the core_pattern binary sets\n\t\t\t * RLIM_CORE = !1, but it runs as root, and can do\n\t\t\t * lots of stupid things.\n\t\t\t *\n\t\t\t * Note that we use task_tgid_vnr here to grab the pid\n\t\t\t * of the process group leader.  That way we get the\n\t\t\t * right pid if a thread in a multi-threaded\n\t\t\t * core_pattern process dies.\n\t\t\t */\n\t\t\tprintk(KERN_WARNING\n\t\t\t\t\"Process %d(%s) has RLIMIT_CORE set to 1\\n\",\n\t\t\t\ttask_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Aborting core\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\t\tcprm.limit = RLIM_INFINITY;\n\n\t\tdump_count = atomic_inc_return(&core_dump_count);\n\t\tif (core_pipe_limit && (core_pipe_limit < dump_count)) {\n\t\t\tprintk(KERN_WARNING \"Pid %d(%s) over core_pipe_limit\\n\",\n\t\t\t       task_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Skipping core dump\\n\");\n\t\t\tgoto fail_dropcount;\n\t\t}\n\n\t\thelper_argv = kmalloc_array(argc + 1, sizeof(*helper_argv),\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!helper_argv) {\n\t\t\tprintk(KERN_WARNING \"%s failed to allocate memory\\n\",\n\t\t\t       __func__);\n\t\t\tgoto fail_dropcount;\n\t\t}\n\t\tfor (argi = 0; argi < argc; argi++)\n\t\t\thelper_argv[argi] = cn.corename + argv[argi];\n\t\thelper_argv[argi] = NULL;\n\n\t\tretval = -ENOMEM;\n\t\tsub_info = call_usermodehelper_setup(helper_argv[0],\n\t\t\t\t\t\thelper_argv, NULL, GFP_KERNEL,\n\t\t\t\t\t\tumh_pipe_setup, NULL, &cprm);\n\t\tif (sub_info)\n\t\t\tretval = call_usermodehelper_exec(sub_info,\n\t\t\t\t\t\t\t  UMH_WAIT_EXEC);\n\n\t\tkfree(helper_argv);\n\t\tif (retval) {\n\t\t\tprintk(KERN_INFO \"Core dump to |%s pipe failed\\n\",\n\t\t\t       cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t} else {\n\t\tstruct user_namespace *mnt_userns;\n\t\tstruct inode *inode;\n\t\tint open_flags = O_CREAT | O_RDWR | O_NOFOLLOW |\n\t\t\t\t O_LARGEFILE | O_EXCL;\n\n\t\tif (cprm.limit < binfmt->min_coredump)\n\t\t\tgoto fail_unlock;\n\n\t\tif (need_suid_safe && cn.corename[0] != '/') {\n\t\t\tprintk(KERN_WARNING \"Pid %d(%s) can only dump core \"\\\n\t\t\t\t\"to fully qualified path!\\n\",\n\t\t\t\ttask_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Skipping core dump\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Unlink the file if it exists unless this is a SUID\n\t\t * binary - in that case, we're running around with root\n\t\t * privs and don't want to unlink another user's coredump.\n\t\t */\n\t\tif (!need_suid_safe) {\n\t\t\t/*\n\t\t\t * If it doesn't exist, that's fine. If there's some\n\t\t\t * other problem, we'll catch it at the filp_open().\n\t\t\t */\n\t\t\tdo_unlinkat(AT_FDCWD, getname_kernel(cn.corename));\n\t\t}\n\n\t\t/*\n\t\t * There is a race between unlinking and creating the\n\t\t * file, but if that causes an EEXIST here, that's\n\t\t * fine - another process raced with us while creating\n\t\t * the corefile, and the other process won. To userspace,\n\t\t * what matters is that at least one of the two processes\n\t\t * writes its coredump successfully, not which one.\n\t\t */\n\t\tif (need_suid_safe) {\n\t\t\t/*\n\t\t\t * Using user namespaces, normal user tasks can change\n\t\t\t * their current->fs->root to point to arbitrary\n\t\t\t * directories. Since the intention of the \"only dump\n\t\t\t * with a fully qualified path\" rule is to control where\n\t\t\t * coredumps may be placed using root privileges,\n\t\t\t * current->fs->root must not be used. Instead, use the\n\t\t\t * root directory of init_task.\n\t\t\t */\n\t\t\tstruct path root;\n\n\t\t\ttask_lock(&init_task);\n\t\t\tget_fs_root(init_task.fs, &root);\n\t\t\ttask_unlock(&init_task);\n\t\t\tcprm.file = file_open_root(&root, cn.corename,\n\t\t\t\t\t\t   open_flags, 0600);\n\t\t\tpath_put(&root);\n\t\t} else {\n\t\t\tcprm.file = filp_open(cn.corename, open_flags, 0600);\n\t\t}\n\t\tif (IS_ERR(cprm.file))\n\t\t\tgoto fail_unlock;\n\n\t\tinode = file_inode(cprm.file);\n\t\tif (inode->i_nlink > 1)\n\t\t\tgoto close_fail;\n\t\tif (d_unhashed(cprm.file->f_path.dentry))\n\t\t\tgoto close_fail;\n\t\t/*\n\t\t * AK: actually i see no reason to not allow this for named\n\t\t * pipes etc, but keep the previous behaviour for now.\n\t\t */\n\t\tif (!S_ISREG(inode->i_mode))\n\t\t\tgoto close_fail;\n\t\t/*\n\t\t * Don't dump core if the filesystem changed owner or mode\n\t\t * of the file during file creation. This is an issue when\n\t\t * a process dumps core while its cwd is e.g. on a vfat\n\t\t * filesystem.\n\t\t */\n\t\tmnt_userns = file_mnt_user_ns(cprm.file);\n\t\tif (!uid_eq(i_uid_into_mnt(mnt_userns, inode),\n\t\t\t    current_fsuid())) {\n\t\t\tpr_info_ratelimited(\"Core dump to %s aborted: cannot preserve file owner\\n\",\n\t\t\t\t\t    cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif ((inode->i_mode & 0677) != 0600) {\n\t\t\tpr_info_ratelimited(\"Core dump to %s aborted: cannot preserve file permissions\\n\",\n\t\t\t\t\t    cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif (!(cprm.file->f_mode & FMODE_CAN_WRITE))\n\t\t\tgoto close_fail;\n\t\tif (do_truncate(mnt_userns, cprm.file->f_path.dentry,\n\t\t\t\t0, 0, cprm.file))\n\t\t\tgoto close_fail;\n\t}\n\n\t/* get us an unshared descriptor table; almost always a no-op */\n\t/* The cell spufs coredump code reads the file descriptor tables */\n\tretval = unshare_files();\n\tif (retval)\n\t\tgoto close_fail;\n\tif (!dump_interrupted()) {\n\t\t/*\n\t\t * umh disabled with CONFIG_STATIC_USERMODEHELPER_PATH=\"\" would\n\t\t * have this set to NULL.\n\t\t */\n\t\tif (!cprm.file) {\n\t\t\tpr_info(\"Core dump to |%s disabled\\n\", cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif (!dump_vma_snapshot(&cprm))\n\t\t\tgoto close_fail;\n\n\t\tfile_start_write(cprm.file);\n\t\tcore_dumped = binfmt->core_dump(&cprm);\n\t\t/*\n\t\t * Ensures that file size is big enough to contain the current\n\t\t * file postion. This prevents gdb from complaining about\n\t\t * a truncated file if the last \"write\" to the file was\n\t\t * dump_skip.\n\t\t */\n\t\tif (cprm.to_skip) {\n\t\t\tcprm.to_skip--;\n\t\t\tdump_emit(&cprm, \"\", 1);\n\t\t}\n\t\tfile_end_write(cprm.file);\n\t\tkvfree(cprm.vma_meta);\n\t}\n\tif (ispipe && core_pipe_limit)\n\t\twait_for_dump_helpers(cprm.file);\nclose_fail:\n\tif (cprm.file)\n\t\tfilp_close(cprm.file, NULL);\nfail_dropcount:\n\tif (ispipe)\n\t\tatomic_dec(&core_dump_count);\nfail_unlock:\n\tkfree(argv);\n\tkfree(cn.corename);\n\tcoredump_finish(core_dumped);\n\trevert_creds(old_cred);\nfail_creds:\n\tput_cred(cred);\nfail:\n\treturn;\n}",
        "code_after_change": "void do_coredump(const kernel_siginfo_t *siginfo)\n{\n\tstruct core_state core_state;\n\tstruct core_name cn;\n\tstruct mm_struct *mm = current->mm;\n\tstruct linux_binfmt * binfmt;\n\tconst struct cred *old_cred;\n\tstruct cred *cred;\n\tint retval = 0;\n\tint ispipe;\n\tsize_t *argv = NULL;\n\tint argc = 0;\n\t/* require nonrelative corefile path and be extra careful */\n\tbool need_suid_safe = false;\n\tbool core_dumped = false;\n\tstatic atomic_t core_dump_count = ATOMIC_INIT(0);\n\tstruct coredump_params cprm = {\n\t\t.siginfo = siginfo,\n\t\t.regs = signal_pt_regs(),\n\t\t.limit = rlimit(RLIMIT_CORE),\n\t\t/*\n\t\t * We must use the same mm->flags while dumping core to avoid\n\t\t * inconsistency of bit flags, since this flag is not protected\n\t\t * by any locks.\n\t\t */\n\t\t.mm_flags = mm->flags,\n\t\t.vma_meta = NULL,\n\t};\n\n\taudit_core_dumps(siginfo->si_signo);\n\n\tbinfmt = mm->binfmt;\n\tif (!binfmt || !binfmt->core_dump)\n\t\tgoto fail;\n\tif (!__get_dumpable(cprm.mm_flags))\n\t\tgoto fail;\n\n\tcred = prepare_creds();\n\tif (!cred)\n\t\tgoto fail;\n\t/*\n\t * We cannot trust fsuid as being the \"true\" uid of the process\n\t * nor do we know its entire history. We only know it was tainted\n\t * so we dump it as root in mode 2, and only into a controlled\n\t * environment (pipe handler or fully qualified path).\n\t */\n\tif (__get_dumpable(cprm.mm_flags) == SUID_DUMP_ROOT) {\n\t\t/* Setuid core dump mode */\n\t\tcred->fsuid = GLOBAL_ROOT_UID;\t/* Dump root private */\n\t\tneed_suid_safe = true;\n\t}\n\n\tretval = coredump_wait(siginfo->si_signo, &core_state);\n\tif (retval < 0)\n\t\tgoto fail_creds;\n\n\told_cred = override_creds(cred);\n\n\tispipe = format_corename(&cn, &cprm, &argv, &argc);\n\n\tif (ispipe) {\n\t\tint argi;\n\t\tint dump_count;\n\t\tchar **helper_argv;\n\t\tstruct subprocess_info *sub_info;\n\n\t\tif (ispipe < 0) {\n\t\t\tprintk(KERN_WARNING \"format_corename failed\\n\");\n\t\t\tprintk(KERN_WARNING \"Aborting core\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\n\t\tif (cprm.limit == 1) {\n\t\t\t/* See umh_pipe_setup() which sets RLIMIT_CORE = 1.\n\t\t\t *\n\t\t\t * Normally core limits are irrelevant to pipes, since\n\t\t\t * we're not writing to the file system, but we use\n\t\t\t * cprm.limit of 1 here as a special value, this is a\n\t\t\t * consistent way to catch recursive crashes.\n\t\t\t * We can still crash if the core_pattern binary sets\n\t\t\t * RLIM_CORE = !1, but it runs as root, and can do\n\t\t\t * lots of stupid things.\n\t\t\t *\n\t\t\t * Note that we use task_tgid_vnr here to grab the pid\n\t\t\t * of the process group leader.  That way we get the\n\t\t\t * right pid if a thread in a multi-threaded\n\t\t\t * core_pattern process dies.\n\t\t\t */\n\t\t\tprintk(KERN_WARNING\n\t\t\t\t\"Process %d(%s) has RLIMIT_CORE set to 1\\n\",\n\t\t\t\ttask_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Aborting core\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\t\tcprm.limit = RLIM_INFINITY;\n\n\t\tdump_count = atomic_inc_return(&core_dump_count);\n\t\tif (core_pipe_limit && (core_pipe_limit < dump_count)) {\n\t\t\tprintk(KERN_WARNING \"Pid %d(%s) over core_pipe_limit\\n\",\n\t\t\t       task_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Skipping core dump\\n\");\n\t\t\tgoto fail_dropcount;\n\t\t}\n\n\t\thelper_argv = kmalloc_array(argc + 1, sizeof(*helper_argv),\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!helper_argv) {\n\t\t\tprintk(KERN_WARNING \"%s failed to allocate memory\\n\",\n\t\t\t       __func__);\n\t\t\tgoto fail_dropcount;\n\t\t}\n\t\tfor (argi = 0; argi < argc; argi++)\n\t\t\thelper_argv[argi] = cn.corename + argv[argi];\n\t\thelper_argv[argi] = NULL;\n\n\t\tretval = -ENOMEM;\n\t\tsub_info = call_usermodehelper_setup(helper_argv[0],\n\t\t\t\t\t\thelper_argv, NULL, GFP_KERNEL,\n\t\t\t\t\t\tumh_pipe_setup, NULL, &cprm);\n\t\tif (sub_info)\n\t\t\tretval = call_usermodehelper_exec(sub_info,\n\t\t\t\t\t\t\t  UMH_WAIT_EXEC);\n\n\t\tkfree(helper_argv);\n\t\tif (retval) {\n\t\t\tprintk(KERN_INFO \"Core dump to |%s pipe failed\\n\",\n\t\t\t       cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t} else {\n\t\tstruct user_namespace *mnt_userns;\n\t\tstruct inode *inode;\n\t\tint open_flags = O_CREAT | O_RDWR | O_NOFOLLOW |\n\t\t\t\t O_LARGEFILE | O_EXCL;\n\n\t\tif (cprm.limit < binfmt->min_coredump)\n\t\t\tgoto fail_unlock;\n\n\t\tif (need_suid_safe && cn.corename[0] != '/') {\n\t\t\tprintk(KERN_WARNING \"Pid %d(%s) can only dump core \"\\\n\t\t\t\t\"to fully qualified path!\\n\",\n\t\t\t\ttask_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Skipping core dump\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Unlink the file if it exists unless this is a SUID\n\t\t * binary - in that case, we're running around with root\n\t\t * privs and don't want to unlink another user's coredump.\n\t\t */\n\t\tif (!need_suid_safe) {\n\t\t\t/*\n\t\t\t * If it doesn't exist, that's fine. If there's some\n\t\t\t * other problem, we'll catch it at the filp_open().\n\t\t\t */\n\t\t\tdo_unlinkat(AT_FDCWD, getname_kernel(cn.corename));\n\t\t}\n\n\t\t/*\n\t\t * There is a race between unlinking and creating the\n\t\t * file, but if that causes an EEXIST here, that's\n\t\t * fine - another process raced with us while creating\n\t\t * the corefile, and the other process won. To userspace,\n\t\t * what matters is that at least one of the two processes\n\t\t * writes its coredump successfully, not which one.\n\t\t */\n\t\tif (need_suid_safe) {\n\t\t\t/*\n\t\t\t * Using user namespaces, normal user tasks can change\n\t\t\t * their current->fs->root to point to arbitrary\n\t\t\t * directories. Since the intention of the \"only dump\n\t\t\t * with a fully qualified path\" rule is to control where\n\t\t\t * coredumps may be placed using root privileges,\n\t\t\t * current->fs->root must not be used. Instead, use the\n\t\t\t * root directory of init_task.\n\t\t\t */\n\t\t\tstruct path root;\n\n\t\t\ttask_lock(&init_task);\n\t\t\tget_fs_root(init_task.fs, &root);\n\t\t\ttask_unlock(&init_task);\n\t\t\tcprm.file = file_open_root(&root, cn.corename,\n\t\t\t\t\t\t   open_flags, 0600);\n\t\t\tpath_put(&root);\n\t\t} else {\n\t\t\tcprm.file = filp_open(cn.corename, open_flags, 0600);\n\t\t}\n\t\tif (IS_ERR(cprm.file))\n\t\t\tgoto fail_unlock;\n\n\t\tinode = file_inode(cprm.file);\n\t\tif (inode->i_nlink > 1)\n\t\t\tgoto close_fail;\n\t\tif (d_unhashed(cprm.file->f_path.dentry))\n\t\t\tgoto close_fail;\n\t\t/*\n\t\t * AK: actually i see no reason to not allow this for named\n\t\t * pipes etc, but keep the previous behaviour for now.\n\t\t */\n\t\tif (!S_ISREG(inode->i_mode))\n\t\t\tgoto close_fail;\n\t\t/*\n\t\t * Don't dump core if the filesystem changed owner or mode\n\t\t * of the file during file creation. This is an issue when\n\t\t * a process dumps core while its cwd is e.g. on a vfat\n\t\t * filesystem.\n\t\t */\n\t\tmnt_userns = file_mnt_user_ns(cprm.file);\n\t\tif (!uid_eq(i_uid_into_mnt(mnt_userns, inode),\n\t\t\t    current_fsuid())) {\n\t\t\tpr_info_ratelimited(\"Core dump to %s aborted: cannot preserve file owner\\n\",\n\t\t\t\t\t    cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif ((inode->i_mode & 0677) != 0600) {\n\t\t\tpr_info_ratelimited(\"Core dump to %s aborted: cannot preserve file permissions\\n\",\n\t\t\t\t\t    cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif (!(cprm.file->f_mode & FMODE_CAN_WRITE))\n\t\t\tgoto close_fail;\n\t\tif (do_truncate(mnt_userns, cprm.file->f_path.dentry,\n\t\t\t\t0, 0, cprm.file))\n\t\t\tgoto close_fail;\n\t}\n\n\t/* get us an unshared descriptor table; almost always a no-op */\n\t/* The cell spufs coredump code reads the file descriptor tables */\n\tretval = unshare_files();\n\tif (retval)\n\t\tgoto close_fail;\n\tif (!dump_interrupted()) {\n\t\t/*\n\t\t * umh disabled with CONFIG_STATIC_USERMODEHELPER_PATH=\"\" would\n\t\t * have this set to NULL.\n\t\t */\n\t\tif (!cprm.file) {\n\t\t\tpr_info(\"Core dump to |%s disabled\\n\", cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif (!dump_vma_snapshot(&cprm))\n\t\t\tgoto close_fail;\n\n\t\tfile_start_write(cprm.file);\n\t\tcore_dumped = binfmt->core_dump(&cprm);\n\t\t/*\n\t\t * Ensures that file size is big enough to contain the current\n\t\t * file postion. This prevents gdb from complaining about\n\t\t * a truncated file if the last \"write\" to the file was\n\t\t * dump_skip.\n\t\t */\n\t\tif (cprm.to_skip) {\n\t\t\tcprm.to_skip--;\n\t\t\tdump_emit(&cprm, \"\", 1);\n\t\t}\n\t\tfile_end_write(cprm.file);\n\t\tfree_vma_snapshot(&cprm);\n\t}\n\tif (ispipe && core_pipe_limit)\n\t\twait_for_dump_helpers(cprm.file);\nclose_fail:\n\tif (cprm.file)\n\t\tfilp_close(cprm.file, NULL);\nfail_dropcount:\n\tif (ispipe)\n\t\tatomic_dec(&core_dump_count);\nfail_unlock:\n\tkfree(argv);\n\tkfree(cn.corename);\n\tcoredump_finish(core_dumped);\n\trevert_creds(old_cred);\nfail_creds:\n\tput_cred(cred);\nfail:\n\treturn;\n}",
        "patch": "--- code before\n+++ code after\n@@ -255,7 +255,7 @@\n \t\t\tdump_emit(&cprm, \"\", 1);\n \t\t}\n \t\tfile_end_write(cprm.file);\n-\t\tkvfree(cprm.vma_meta);\n+\t\tfree_vma_snapshot(&cprm);\n \t}\n \tif (ispipe && core_pipe_limit)\n \t\twait_for_dump_helpers(cprm.file);",
        "function_modified_lines": {
            "added": [
                "\t\tfree_vma_snapshot(&cprm);"
            ],
            "deleted": [
                "\t\tkvfree(cprm.vma_meta);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s core dump subsystem. This flaw allows a local user to crash the system. Only if patch 390031c94211 (\"coredump: Use the vma snapshot in fill_files_note\") not applied yet, then kernel could be affected.",
        "id": 3859
    },
    {
        "cve_id": "CVE-2021-1048",
        "code_before_change": "static int ep_loop_check_proc(void *priv, void *cookie, int call_nests)\n{\n\tint error = 0;\n\tstruct file *file = priv;\n\tstruct eventpoll *ep = file->private_data;\n\tstruct eventpoll *ep_tovisit;\n\tstruct rb_node *rbp;\n\tstruct epitem *epi;\n\n\tmutex_lock_nested(&ep->mtx, call_nests + 1);\n\tep->visited = 1;\n\tlist_add(&ep->visited_list_link, &visited_list);\n\tfor (rbp = rb_first_cached(&ep->rbr); rbp; rbp = rb_next(rbp)) {\n\t\tepi = rb_entry(rbp, struct epitem, rbn);\n\t\tif (unlikely(is_file_epoll(epi->ffd.file))) {\n\t\t\tep_tovisit = epi->ffd.file->private_data;\n\t\t\tif (ep_tovisit->visited)\n\t\t\t\tcontinue;\n\t\t\terror = ep_call_nested(&poll_loop_ncalls,\n\t\t\t\t\tep_loop_check_proc, epi->ffd.file,\n\t\t\t\t\tep_tovisit, current);\n\t\t\tif (error != 0)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If we've reached a file that is not associated with\n\t\t\t * an ep, then we need to check if the newly added\n\t\t\t * links are going to add too many wakeup paths. We do\n\t\t\t * this by adding it to the tfile_check_list, if it's\n\t\t\t * not already there, and calling reverse_path_check()\n\t\t\t * during ep_insert().\n\t\t\t */\n\t\t\tif (list_empty(&epi->ffd.file->f_tfile_llink)) {\n\t\t\t\tget_file(epi->ffd.file);\n\t\t\t\tlist_add(&epi->ffd.file->f_tfile_llink,\n\t\t\t\t\t &tfile_check_list);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&ep->mtx);\n\n\treturn error;\n}",
        "code_after_change": "static int ep_loop_check_proc(void *priv, void *cookie, int call_nests)\n{\n\tint error = 0;\n\tstruct file *file = priv;\n\tstruct eventpoll *ep = file->private_data;\n\tstruct eventpoll *ep_tovisit;\n\tstruct rb_node *rbp;\n\tstruct epitem *epi;\n\n\tmutex_lock_nested(&ep->mtx, call_nests + 1);\n\tep->visited = 1;\n\tlist_add(&ep->visited_list_link, &visited_list);\n\tfor (rbp = rb_first_cached(&ep->rbr); rbp; rbp = rb_next(rbp)) {\n\t\tepi = rb_entry(rbp, struct epitem, rbn);\n\t\tif (unlikely(is_file_epoll(epi->ffd.file))) {\n\t\t\tep_tovisit = epi->ffd.file->private_data;\n\t\t\tif (ep_tovisit->visited)\n\t\t\t\tcontinue;\n\t\t\terror = ep_call_nested(&poll_loop_ncalls,\n\t\t\t\t\tep_loop_check_proc, epi->ffd.file,\n\t\t\t\t\tep_tovisit, current);\n\t\t\tif (error != 0)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If we've reached a file that is not associated with\n\t\t\t * an ep, then we need to check if the newly added\n\t\t\t * links are going to add too many wakeup paths. We do\n\t\t\t * this by adding it to the tfile_check_list, if it's\n\t\t\t * not already there, and calling reverse_path_check()\n\t\t\t * during ep_insert().\n\t\t\t */\n\t\t\tif (list_empty(&epi->ffd.file->f_tfile_llink)) {\n\t\t\t\tif (get_file_rcu(epi->ffd.file))\n\t\t\t\t\tlist_add(&epi->ffd.file->f_tfile_llink,\n\t\t\t\t\t\t &tfile_check_list);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&ep->mtx);\n\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,9 +31,9 @@\n \t\t\t * during ep_insert().\n \t\t\t */\n \t\t\tif (list_empty(&epi->ffd.file->f_tfile_llink)) {\n-\t\t\t\tget_file(epi->ffd.file);\n-\t\t\t\tlist_add(&epi->ffd.file->f_tfile_llink,\n-\t\t\t\t\t &tfile_check_list);\n+\t\t\t\tif (get_file_rcu(epi->ffd.file))\n+\t\t\t\t\tlist_add(&epi->ffd.file->f_tfile_llink,\n+\t\t\t\t\t\t &tfile_check_list);\n \t\t\t}\n \t\t}\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tif (get_file_rcu(epi->ffd.file))",
                "\t\t\t\t\tlist_add(&epi->ffd.file->f_tfile_llink,",
                "\t\t\t\t\t\t &tfile_check_list);"
            ],
            "deleted": [
                "\t\t\t\tget_file(epi->ffd.file);",
                "\t\t\t\tlist_add(&epi->ffd.file->f_tfile_llink,",
                "\t\t\t\t\t &tfile_check_list);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In ep_loop_check_proc of eventpoll.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-204573007References: Upstream kernel",
        "id": 2841
    },
    {
        "cve_id": "CVE-2023-35829",
        "code_before_change": "static int rkvdec_remove(struct platform_device *pdev)\n{\n\tstruct rkvdec_dev *rkvdec = platform_get_drvdata(pdev);\n\n\trkvdec_v4l2_cleanup(rkvdec);\n\tpm_runtime_disable(&pdev->dev);\n\tpm_runtime_dont_use_autosuspend(&pdev->dev);\n\treturn 0;\n}",
        "code_after_change": "static int rkvdec_remove(struct platform_device *pdev)\n{\n\tstruct rkvdec_dev *rkvdec = platform_get_drvdata(pdev);\n\n\tcancel_delayed_work_sync(&rkvdec->watchdog_work);\n\n\trkvdec_v4l2_cleanup(rkvdec);\n\tpm_runtime_disable(&pdev->dev);\n\tpm_runtime_dont_use_autosuspend(&pdev->dev);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,8 @@\n static int rkvdec_remove(struct platform_device *pdev)\n {\n \tstruct rkvdec_dev *rkvdec = platform_get_drvdata(pdev);\n+\n+\tcancel_delayed_work_sync(&rkvdec->watchdog_work);\n \n \trkvdec_v4l2_cleanup(rkvdec);\n \tpm_runtime_disable(&pdev->dev);",
        "function_modified_lines": {
            "added": [
                "",
                "\tcancel_delayed_work_sync(&rkvdec->watchdog_work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in rkvdec_remove in drivers/staging/media/rkvdec/rkvdec.c.",
        "id": 4116
    },
    {
        "cve_id": "CVE-2023-3439",
        "code_before_change": "static void mctp_unregister(struct net_device *dev)\n{\n\tstruct mctp_dev *mdev;\n\n\tmdev = mctp_dev_get_rtnl(dev);\n\tif (mdev && !mctp_known(dev)) {\n\t\t// Sanity check, should match what was set in mctp_register\n\t\tnetdev_warn(dev, \"%s: BUG mctp_ptr set for unknown type %d\",\n\t\t\t    __func__, dev->type);\n\t\treturn;\n\t}\n\tif (!mdev)\n\t\treturn;\n\n\tRCU_INIT_POINTER(mdev->dev->mctp_ptr, NULL);\n\n\tmctp_route_remove_dev(mdev);\n\tmctp_neigh_remove_dev(mdev);\n\tkfree(mdev->addrs);\n\n\tmctp_dev_put(mdev);\n}",
        "code_after_change": "static void mctp_unregister(struct net_device *dev)\n{\n\tstruct mctp_dev *mdev;\n\n\tmdev = mctp_dev_get_rtnl(dev);\n\tif (mdev && !mctp_known(dev)) {\n\t\t// Sanity check, should match what was set in mctp_register\n\t\tnetdev_warn(dev, \"%s: BUG mctp_ptr set for unknown type %d\",\n\t\t\t    __func__, dev->type);\n\t\treturn;\n\t}\n\tif (!mdev)\n\t\treturn;\n\n\tRCU_INIT_POINTER(mdev->dev->mctp_ptr, NULL);\n\n\tmctp_route_remove_dev(mdev);\n\tmctp_neigh_remove_dev(mdev);\n\n\tmctp_dev_put(mdev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,7 +16,6 @@\n \n \tmctp_route_remove_dev(mdev);\n \tmctp_neigh_remove_dev(mdev);\n-\tkfree(mdev->addrs);\n \n \tmctp_dev_put(mdev);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tkfree(mdev->addrs);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the MCTP protocol in the Linux kernel. The function mctp_unregister() reclaims the device's relevant resource when a netcard detaches. However, a running routine may be unaware of this and cause the use-after-free of the mdev->addrs object, potentially leading to a denial of service.",
        "id": 4104
    },
    {
        "cve_id": "CVE-2023-35824",
        "code_before_change": "static void dm1105_remove(struct pci_dev *pdev)\n{\n\tstruct dm1105_dev *dev = pci_get_drvdata(pdev);\n\tstruct dvb_adapter *dvb_adapter = &dev->dvb_adapter;\n\tstruct dvb_demux *dvbdemux = &dev->demux;\n\tstruct dmx_demux *dmx = &dvbdemux->dmx;\n\n\tdm1105_ir_exit(dev);\n\tdmx->close(dmx);\n\tdvb_net_release(&dev->dvbnet);\n\tif (dev->fe)\n\t\tdvb_unregister_frontend(dev->fe);\n\n\tdmx->disconnect_frontend(dmx);\n\tdmx->remove_frontend(dmx, &dev->mem_frontend);\n\tdmx->remove_frontend(dmx, &dev->hw_frontend);\n\tdvb_dmxdev_release(&dev->dmxdev);\n\tdvb_dmx_release(dvbdemux);\n\tdvb_unregister_adapter(dvb_adapter);\n\ti2c_del_adapter(&dev->i2c_adap);\n\n\tdm1105_hw_exit(dev);\n\tfree_irq(pdev->irq, dev);\n\tpci_iounmap(pdev, dev->io_mem);\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tdm1105_devcount--;\n\tkfree(dev);\n}",
        "code_after_change": "static void dm1105_remove(struct pci_dev *pdev)\n{\n\tstruct dm1105_dev *dev = pci_get_drvdata(pdev);\n\tstruct dvb_adapter *dvb_adapter = &dev->dvb_adapter;\n\tstruct dvb_demux *dvbdemux = &dev->demux;\n\tstruct dmx_demux *dmx = &dvbdemux->dmx;\n\n\tcancel_work_sync(&dev->ir.work);\n\tdm1105_ir_exit(dev);\n\tdmx->close(dmx);\n\tdvb_net_release(&dev->dvbnet);\n\tif (dev->fe)\n\t\tdvb_unregister_frontend(dev->fe);\n\n\tdmx->disconnect_frontend(dmx);\n\tdmx->remove_frontend(dmx, &dev->mem_frontend);\n\tdmx->remove_frontend(dmx, &dev->hw_frontend);\n\tdvb_dmxdev_release(&dev->dmxdev);\n\tdvb_dmx_release(dvbdemux);\n\tdvb_unregister_adapter(dvb_adapter);\n\ti2c_del_adapter(&dev->i2c_adap);\n\n\tdm1105_hw_exit(dev);\n\tfree_irq(pdev->irq, dev);\n\tpci_iounmap(pdev, dev->io_mem);\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tdm1105_devcount--;\n\tkfree(dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tstruct dvb_demux *dvbdemux = &dev->demux;\n \tstruct dmx_demux *dmx = &dvbdemux->dmx;\n \n+\tcancel_work_sync(&dev->ir.work);\n \tdm1105_ir_exit(dev);\n \tdmx->close(dmx);\n \tdvb_net_release(&dev->dvbnet);",
        "function_modified_lines": {
            "added": [
                "\tcancel_work_sync(&dev->ir.work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in dm1105_remove in drivers/media/pci/dm1105/dm1105.c.",
        "id": 4112
    },
    {
        "cve_id": "CVE-2017-18218",
        "code_before_change": "static netdev_tx_t hns_nic_net_xmit(struct sk_buff *skb,\n\t\t\t\t    struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tint ret;\n\n\tassert(skb->queue_mapping < ndev->ae_handle->q_num);\n\tret = hns_nic_net_xmit_hw(ndev, skb,\n\t\t\t\t  &tx_ring_data(priv, skb->queue_mapping));\n\tif (ret == NETDEV_TX_OK) {\n\t\tnetif_trans_update(ndev);\n\t\tndev->stats.tx_bytes += skb->len;\n\t\tndev->stats.tx_packets++;\n\t}\n\treturn (netdev_tx_t)ret;\n}",
        "code_after_change": "static netdev_tx_t hns_nic_net_xmit(struct sk_buff *skb,\n\t\t\t\t    struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\tassert(skb->queue_mapping < ndev->ae_handle->q_num);\n\n\treturn hns_nic_net_xmit_hw(ndev, skb,\n\t\t\t\t   &tx_ring_data(priv, skb->queue_mapping));\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,15 +2,9 @@\n \t\t\t\t    struct net_device *ndev)\n {\n \tstruct hns_nic_priv *priv = netdev_priv(ndev);\n-\tint ret;\n \n \tassert(skb->queue_mapping < ndev->ae_handle->q_num);\n-\tret = hns_nic_net_xmit_hw(ndev, skb,\n-\t\t\t\t  &tx_ring_data(priv, skb->queue_mapping));\n-\tif (ret == NETDEV_TX_OK) {\n-\t\tnetif_trans_update(ndev);\n-\t\tndev->stats.tx_bytes += skb->len;\n-\t\tndev->stats.tx_packets++;\n-\t}\n-\treturn (netdev_tx_t)ret;\n+\n+\treturn hns_nic_net_xmit_hw(ndev, skb,\n+\t\t\t\t   &tx_ring_data(priv, skb->queue_mapping));\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\treturn hns_nic_net_xmit_hw(ndev, skb,",
                "\t\t\t\t   &tx_ring_data(priv, skb->queue_mapping));"
            ],
            "deleted": [
                "\tint ret;",
                "\tret = hns_nic_net_xmit_hw(ndev, skb,",
                "\t\t\t\t  &tx_ring_data(priv, skb->queue_mapping));",
                "\tif (ret == NETDEV_TX_OK) {",
                "\t\tnetif_trans_update(ndev);",
                "\t\tndev->stats.tx_bytes += skb->len;",
                "\t\tndev->stats.tx_packets++;",
                "\t}",
                "\treturn (netdev_tx_t)ret;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In drivers/net/ethernet/hisilicon/hns/hns_enet.c in the Linux kernel before 4.13, local users can cause a denial of service (use-after-free and BUG) or possibly have unspecified other impact by leveraging differences in skb handling between hns_nic_net_xmit_hw and hns_nic_net_xmit.",
        "id": 1405
    },
    {
        "cve_id": "CVE-2020-12657",
        "code_before_change": "static enum hrtimer_restart bfq_idle_slice_timer(struct hrtimer *timer)\n{\n\tstruct bfq_data *bfqd = container_of(timer, struct bfq_data,\n\t\t\t\t\t     idle_slice_timer);\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\t/*\n\t * Theoretical race here: the in-service queue can be NULL or\n\t * different from the queue that was idling if a new request\n\t * arrives for the current queue and there is a full dispatch\n\t * cycle that changes the in-service queue.  This can hardly\n\t * happen, but in the worst case we just expire a queue too\n\t * early.\n\t */\n\tif (bfqq)\n\t\tbfq_idle_slice_timer_body(bfqq);\n\n\treturn HRTIMER_NORESTART;\n}",
        "code_after_change": "static enum hrtimer_restart bfq_idle_slice_timer(struct hrtimer *timer)\n{\n\tstruct bfq_data *bfqd = container_of(timer, struct bfq_data,\n\t\t\t\t\t     idle_slice_timer);\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\t/*\n\t * Theoretical race here: the in-service queue can be NULL or\n\t * different from the queue that was idling if a new request\n\t * arrives for the current queue and there is a full dispatch\n\t * cycle that changes the in-service queue.  This can hardly\n\t * happen, but in the worst case we just expire a queue too\n\t * early.\n\t */\n\tif (bfqq)\n\t\tbfq_idle_slice_timer_body(bfqd, bfqq);\n\n\treturn HRTIMER_NORESTART;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,7 +13,7 @@\n \t * early.\n \t */\n \tif (bfqq)\n-\t\tbfq_idle_slice_timer_body(bfqq);\n+\t\tbfq_idle_slice_timer_body(bfqd, bfqq);\n \n \treturn HRTIMER_NORESTART;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tbfq_idle_slice_timer_body(bfqd, bfqq);"
            ],
            "deleted": [
                "\t\tbfq_idle_slice_timer_body(bfqq);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.6.5. There is a use-after-free in block/bfq-iosched.c related to bfq_idle_slice_timer_body.",
        "id": 2479
    },
    {
        "cve_id": "CVE-2022-2318",
        "code_before_change": "void rose_stop_heartbeat(struct sock *sk)\n{\n\tdel_timer(&sk->sk_timer);\n}",
        "code_after_change": "void rose_stop_heartbeat(struct sock *sk)\n{\n\tsk_stop_timer(sk, &sk->sk_timer);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n void rose_stop_heartbeat(struct sock *sk)\n {\n-\tdel_timer(&sk->sk_timer);\n+\tsk_stop_timer(sk, &sk->sk_timer);\n }",
        "function_modified_lines": {
            "added": [
                "\tsk_stop_timer(sk, &sk->sk_timer);"
            ],
            "deleted": [
                "\tdel_timer(&sk->sk_timer);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities caused by timer handler in net/rose/rose_timer.c of linux that allow attackers to crash linux kernel without any privileges.",
        "id": 3434
    },
    {
        "cve_id": "CVE-2023-1872",
        "code_before_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n\t__must_hold(&ctx->uring_lock)\n{\n\tunsigned int sqe_flags;\n\tint personality;\n\tu8 opcode;\n\n\t/* req is partially pre-initialised, see io_preinit_req() */\n\treq->opcode = opcode = READ_ONCE(sqe->opcode);\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags = sqe_flags = READ_ONCE(sqe->flags);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->file = NULL;\n\treq->fixed_rsrc_refs = NULL;\n\treq->task = current;\n\n\tif (unlikely(opcode >= IORING_OP_LAST)) {\n\t\treq->opcode = 0;\n\t\treturn -EINVAL;\n\t}\n\tif (unlikely(sqe_flags & ~SQE_COMMON_FLAGS)) {\n\t\t/* enforce forwards compatibility on users */\n\t\tif (sqe_flags & ~SQE_VALID_FLAGS)\n\t\t\treturn -EINVAL;\n\t\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t\t    !io_op_defs[opcode].buffer_select)\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (sqe_flags & IOSQE_CQE_SKIP_SUCCESS)\n\t\t\tctx->drain_disabled = true;\n\t\tif (sqe_flags & IOSQE_IO_DRAIN) {\n\t\t\tif (ctx->drain_disabled)\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\tio_init_req_drain(req);\n\t\t}\n\t}\n\tif (unlikely(ctx->restricted || ctx->drain_active || ctx->drain_next)) {\n\t\tif (ctx->restricted && !io_check_restriction(ctx, req, sqe_flags))\n\t\t\treturn -EACCES;\n\t\t/* knock it to the slow queue path, will be drained there */\n\t\tif (ctx->drain_active)\n\t\t\treq->flags |= REQ_F_FORCE_ASYNC;\n\t\t/* if there is no link, we're at \"next\" request and need to drain */\n\t\tif (unlikely(ctx->drain_next) && !ctx->submit_state.link.head) {\n\t\t\tctx->drain_next = false;\n\t\t\tctx->drain_active = true;\n\t\t\treq->flags |= REQ_F_IO_DRAIN | REQ_F_FORCE_ASYNC;\n\t\t}\n\t}\n\n\tif (io_op_defs[opcode].needs_file) {\n\t\tstruct io_submit_state *state = &ctx->submit_state;\n\n\t\t/*\n\t\t * Plug now if we have more than 2 IO left after this, and the\n\t\t * target is potentially a read/write to block based storage.\n\t\t */\n\t\tif (state->need_plug && io_op_defs[opcode].plug) {\n\t\t\tstate->plug_started = true;\n\t\t\tstate->need_plug = false;\n\t\t\tblk_start_plug_nr_ios(&state->plug, state->submit_nr);\n\t\t}\n\n\t\treq->file = io_file_get(ctx, req, READ_ONCE(sqe->fd),\n\t\t\t\t\t(sqe_flags & IOSQE_FIXED_FILE));\n\t\tif (unlikely(!req->file))\n\t\t\treturn -EBADF;\n\t}\n\n\tpersonality = READ_ONCE(sqe->personality);\n\tif (personality) {\n\t\tint ret;\n\n\t\treq->creds = xa_load(&ctx->personalities, personality);\n\t\tif (!req->creds)\n\t\t\treturn -EINVAL;\n\t\tget_cred(req->creds);\n\t\tret = security_uring_override_creds(req->creds);\n\t\tif (ret) {\n\t\t\tput_cred(req->creds);\n\t\t\treturn ret;\n\t\t}\n\t\treq->flags |= REQ_F_CREDS;\n\t}\n\n\treturn io_req_prep(req, sqe);\n}",
        "code_after_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n\t__must_hold(&ctx->uring_lock)\n{\n\tunsigned int sqe_flags;\n\tint personality;\n\tu8 opcode;\n\n\t/* req is partially pre-initialised, see io_preinit_req() */\n\treq->opcode = opcode = READ_ONCE(sqe->opcode);\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags = sqe_flags = READ_ONCE(sqe->flags);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->file = NULL;\n\treq->fixed_rsrc_refs = NULL;\n\treq->task = current;\n\n\tif (unlikely(opcode >= IORING_OP_LAST)) {\n\t\treq->opcode = 0;\n\t\treturn -EINVAL;\n\t}\n\tif (unlikely(sqe_flags & ~SQE_COMMON_FLAGS)) {\n\t\t/* enforce forwards compatibility on users */\n\t\tif (sqe_flags & ~SQE_VALID_FLAGS)\n\t\t\treturn -EINVAL;\n\t\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t\t    !io_op_defs[opcode].buffer_select)\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (sqe_flags & IOSQE_CQE_SKIP_SUCCESS)\n\t\t\tctx->drain_disabled = true;\n\t\tif (sqe_flags & IOSQE_IO_DRAIN) {\n\t\t\tif (ctx->drain_disabled)\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\tio_init_req_drain(req);\n\t\t}\n\t}\n\tif (unlikely(ctx->restricted || ctx->drain_active || ctx->drain_next)) {\n\t\tif (ctx->restricted && !io_check_restriction(ctx, req, sqe_flags))\n\t\t\treturn -EACCES;\n\t\t/* knock it to the slow queue path, will be drained there */\n\t\tif (ctx->drain_active)\n\t\t\treq->flags |= REQ_F_FORCE_ASYNC;\n\t\t/* if there is no link, we're at \"next\" request and need to drain */\n\t\tif (unlikely(ctx->drain_next) && !ctx->submit_state.link.head) {\n\t\t\tctx->drain_next = false;\n\t\t\tctx->drain_active = true;\n\t\t\treq->flags |= REQ_F_IO_DRAIN | REQ_F_FORCE_ASYNC;\n\t\t}\n\t}\n\n\tif (io_op_defs[opcode].needs_file) {\n\t\tstruct io_submit_state *state = &ctx->submit_state;\n\n\t\t/*\n\t\t * Plug now if we have more than 2 IO left after this, and the\n\t\t * target is potentially a read/write to block based storage.\n\t\t */\n\t\tif (state->need_plug && io_op_defs[opcode].plug) {\n\t\t\tstate->plug_started = true;\n\t\t\tstate->need_plug = false;\n\t\t\tblk_start_plug_nr_ios(&state->plug, state->submit_nr);\n\t\t}\n\n\t\tif (req->flags & REQ_F_FIXED_FILE)\n\t\t\treq->file = io_file_get_fixed(req, READ_ONCE(sqe->fd), 0);\n\t\telse\n\t\t\treq->file = io_file_get_normal(req, READ_ONCE(sqe->fd));\n\t\tif (unlikely(!req->file))\n\t\t\treturn -EBADF;\n\t}\n\n\tpersonality = READ_ONCE(sqe->personality);\n\tif (personality) {\n\t\tint ret;\n\n\t\treq->creds = xa_load(&ctx->personalities, personality);\n\t\tif (!req->creds)\n\t\t\treturn -EINVAL;\n\t\tget_cred(req->creds);\n\t\tret = security_uring_override_creds(req->creds);\n\t\tif (ret) {\n\t\t\tput_cred(req->creds);\n\t\t\treturn ret;\n\t\t}\n\t\treq->flags |= REQ_F_CREDS;\n\t}\n\n\treturn io_req_prep(req, sqe);\n}",
        "patch": "--- code before\n+++ code after\n@@ -61,8 +61,10 @@\n \t\t\tblk_start_plug_nr_ios(&state->plug, state->submit_nr);\n \t\t}\n \n-\t\treq->file = io_file_get(ctx, req, READ_ONCE(sqe->fd),\n-\t\t\t\t\t(sqe_flags & IOSQE_FIXED_FILE));\n+\t\tif (req->flags & REQ_F_FIXED_FILE)\n+\t\t\treq->file = io_file_get_fixed(req, READ_ONCE(sqe->fd), 0);\n+\t\telse\n+\t\t\treq->file = io_file_get_normal(req, READ_ONCE(sqe->fd));\n \t\tif (unlikely(!req->file))\n \t\t\treturn -EBADF;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (req->flags & REQ_F_FIXED_FILE)",
                "\t\t\treq->file = io_file_get_fixed(req, READ_ONCE(sqe->fd), 0);",
                "\t\telse",
                "\t\t\treq->file = io_file_get_normal(req, READ_ONCE(sqe->fd));"
            ],
            "deleted": [
                "\t\treq->file = io_file_get(ctx, req, READ_ONCE(sqe->fd),",
                "\t\t\t\t\t(sqe_flags & IOSQE_FIXED_FILE));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring system can be exploited to achieve local privilege escalation.\n\nThe io_file_get_fixed function lacks the presence of ctx->uring_lock which can lead to a Use-After-Free vulnerability due a race condition with fixed files getting unregistered.\n\nWe recommend upgrading past commit da24142b1ef9fd5d36b76e36bab328a5b27523e8.\n\n",
        "id": 3887
    },
    {
        "cve_id": "CVE-2019-19525",
        "code_before_change": "static void atusb_disconnect(struct usb_interface *interface)\n{\n\tstruct atusb *atusb = usb_get_intfdata(interface);\n\n\tdev_dbg(&atusb->usb_dev->dev, \"%s\\n\", __func__);\n\n\tatusb->shutdown = 1;\n\tcancel_delayed_work_sync(&atusb->work);\n\n\tusb_kill_anchored_urbs(&atusb->rx_urbs);\n\tatusb_free_urbs(atusb);\n\tusb_kill_urb(atusb->tx_urb);\n\tusb_free_urb(atusb->tx_urb);\n\n\tieee802154_unregister_hw(atusb->hw);\n\n\tieee802154_free_hw(atusb->hw);\n\n\tusb_set_intfdata(interface, NULL);\n\tusb_put_dev(atusb->usb_dev);\n\n\tpr_debug(\"%s done\\n\", __func__);\n}",
        "code_after_change": "static void atusb_disconnect(struct usb_interface *interface)\n{\n\tstruct atusb *atusb = usb_get_intfdata(interface);\n\n\tdev_dbg(&atusb->usb_dev->dev, \"%s\\n\", __func__);\n\n\tatusb->shutdown = 1;\n\tcancel_delayed_work_sync(&atusb->work);\n\n\tusb_kill_anchored_urbs(&atusb->rx_urbs);\n\tatusb_free_urbs(atusb);\n\tusb_kill_urb(atusb->tx_urb);\n\tusb_free_urb(atusb->tx_urb);\n\n\tieee802154_unregister_hw(atusb->hw);\n\n\tusb_put_dev(atusb->usb_dev);\n\n\tieee802154_free_hw(atusb->hw);\n\n\tusb_set_intfdata(interface, NULL);\n\n\tpr_debug(\"%s done\\n\", __func__);\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,10 +14,11 @@\n \n \tieee802154_unregister_hw(atusb->hw);\n \n+\tusb_put_dev(atusb->usb_dev);\n+\n \tieee802154_free_hw(atusb->hw);\n \n \tusb_set_intfdata(interface, NULL);\n-\tusb_put_dev(atusb->usb_dev);\n \n \tpr_debug(\"%s done\\n\", __func__);\n }",
        "function_modified_lines": {
            "added": [
                "\tusb_put_dev(atusb->usb_dev);",
                ""
            ],
            "deleted": [
                "\tusb_put_dev(atusb->usb_dev);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.3.6, there is a use-after-free bug that can be caused by a malicious USB device in the drivers/net/ieee802154/atusb.c driver, aka CID-7fd25e6fc035.",
        "id": 2200
    },
    {
        "cve_id": "CVE-2018-5873",
        "code_before_change": "static void *__ns_get_path(struct path *path, struct ns_common *ns)\n{\n\tstruct vfsmount *mnt = nsfs_mnt;\n\tstruct qstr qname = { .name = \"\", };\n\tstruct dentry *dentry;\n\tstruct inode *inode;\n\tunsigned long d;\n\n\trcu_read_lock();\n\td = atomic_long_read(&ns->stashed);\n\tif (!d)\n\t\tgoto slow;\n\tdentry = (struct dentry *)d;\n\tif (!lockref_get_not_dead(&dentry->d_lockref))\n\t\tgoto slow;\n\trcu_read_unlock();\n\tns->ops->put(ns);\ngot_it:\n\tpath->mnt = mntget(mnt);\n\tpath->dentry = dentry;\n\treturn NULL;\nslow:\n\trcu_read_unlock();\n\tinode = new_inode_pseudo(mnt->mnt_sb);\n\tif (!inode) {\n\t\tns->ops->put(ns);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tinode->i_ino = ns->inum;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = current_time(inode);\n\tinode->i_flags |= S_IMMUTABLE;\n\tinode->i_mode = S_IFREG | S_IRUGO;\n\tinode->i_fop = &ns_file_operations;\n\tinode->i_private = ns;\n\n\tdentry = d_alloc_pseudo(mnt->mnt_sb, &qname);\n\tif (!dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\td_instantiate(dentry, inode);\n\tdentry->d_fsdata = (void *)ns->ops;\n\td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n\tif (d) {\n\t\td_delete(dentry);\t/* make sure ->d_prune() does nothing */\n\t\tdput(dentry);\n\t\tcpu_relax();\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\tgoto got_it;\n}",
        "code_after_change": "static void *__ns_get_path(struct path *path, struct ns_common *ns)\n{\n\tstruct vfsmount *mnt = nsfs_mnt;\n\tstruct qstr qname = { .name = \"\", };\n\tstruct dentry *dentry;\n\tstruct inode *inode;\n\tunsigned long d;\n\n\trcu_read_lock();\n\td = atomic_long_read(&ns->stashed);\n\tif (!d)\n\t\tgoto slow;\n\tdentry = (struct dentry *)d;\n\tif (!lockref_get_not_dead(&dentry->d_lockref))\n\t\tgoto slow;\n\trcu_read_unlock();\n\tns->ops->put(ns);\ngot_it:\n\tpath->mnt = mntget(mnt);\n\tpath->dentry = dentry;\n\treturn NULL;\nslow:\n\trcu_read_unlock();\n\tinode = new_inode_pseudo(mnt->mnt_sb);\n\tif (!inode) {\n\t\tns->ops->put(ns);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tinode->i_ino = ns->inum;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = current_time(inode);\n\tinode->i_flags |= S_IMMUTABLE;\n\tinode->i_mode = S_IFREG | S_IRUGO;\n\tinode->i_fop = &ns_file_operations;\n\tinode->i_private = ns;\n\n\tdentry = d_alloc_pseudo(mnt->mnt_sb, &qname);\n\tif (!dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\td_instantiate(dentry, inode);\n\tdentry->d_flags |= DCACHE_RCUACCESS;\n\tdentry->d_fsdata = (void *)ns->ops;\n\td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n\tif (d) {\n\t\td_delete(dentry);\t/* make sure ->d_prune() does nothing */\n\t\tdput(dentry);\n\t\tcpu_relax();\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\tgoto got_it;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,6 +39,7 @@\n \t\treturn ERR_PTR(-ENOMEM);\n \t}\n \td_instantiate(dentry, inode);\n+\tdentry->d_flags |= DCACHE_RCUACCESS;\n \tdentry->d_fsdata = (void *)ns->ops;\n \td = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);\n \tif (d) {",
        "function_modified_lines": {
            "added": [
                "\tdentry->d_flags |= DCACHE_RCUACCESS;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the __ns_get_path function in fs/nsfs.c in the Linux kernel before 4.11. Due to a race condition when accessing files, a Use After Free condition can occur. This also affects all Android releases from CAF using the Linux kernel (Android for MSM, Firefox OS for MSM, QRD Android) before security patch level 2018-07-05.",
        "id": 1840
    },
    {
        "cve_id": "CVE-2023-51781",
        "code_before_change": "static int atalk_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tint rc = -ENOIOCTLCMD;\n\tstruct sock *sk = sock->sk;\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (cmd) {\n\t/* Protocol layer */\n\tcase TIOCOUTQ: {\n\t\tlong amount = sk->sk_sndbuf - sk_wmem_alloc_get(sk);\n\n\t\tif (amount < 0)\n\t\t\tamount = 0;\n\t\trc = put_user(amount, (int __user *)argp);\n\t\tbreak;\n\t}\n\tcase TIOCINQ: {\n\t\t/*\n\t\t * These two are safe on a single CPU system as only\n\t\t * user tasks fiddle here\n\t\t */\n\t\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n\t\tlong amount = 0;\n\n\t\tif (skb)\n\t\t\tamount = skb->len - sizeof(struct ddpehdr);\n\t\trc = put_user(amount, (int __user *)argp);\n\t\tbreak;\n\t}\n\t/* Routing */\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\t\trc = -EPERM;\n\t\tif (capable(CAP_NET_ADMIN))\n\t\t\trc = atrtr_ioctl(cmd, argp);\n\t\tbreak;\n\t/* Interface */\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCATALKDIFADDR:\n\tcase SIOCDIFADDR:\n\tcase SIOCSARP:\t\t/* proxy AARP */\n\tcase SIOCDARP:\t\t/* proxy AARP */\n\t\trtnl_lock();\n\t\trc = atif_ioctl(cmd, argp);\n\t\trtnl_unlock();\n\t\tbreak;\n\t}\n\n\treturn rc;\n}",
        "code_after_change": "static int atalk_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tint rc = -ENOIOCTLCMD;\n\tstruct sock *sk = sock->sk;\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (cmd) {\n\t/* Protocol layer */\n\tcase TIOCOUTQ: {\n\t\tlong amount = sk->sk_sndbuf - sk_wmem_alloc_get(sk);\n\n\t\tif (amount < 0)\n\t\t\tamount = 0;\n\t\trc = put_user(amount, (int __user *)argp);\n\t\tbreak;\n\t}\n\tcase TIOCINQ: {\n\t\tstruct sk_buff *skb;\n\t\tlong amount = 0;\n\n\t\tspin_lock_irq(&sk->sk_receive_queue.lock);\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tamount = skb->len - sizeof(struct ddpehdr);\n\t\tspin_unlock_irq(&sk->sk_receive_queue.lock);\n\t\trc = put_user(amount, (int __user *)argp);\n\t\tbreak;\n\t}\n\t/* Routing */\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\t\trc = -EPERM;\n\t\tif (capable(CAP_NET_ADMIN))\n\t\t\trc = atrtr_ioctl(cmd, argp);\n\t\tbreak;\n\t/* Interface */\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCATALKDIFADDR:\n\tcase SIOCDIFADDR:\n\tcase SIOCSARP:\t\t/* proxy AARP */\n\tcase SIOCDARP:\t\t/* proxy AARP */\n\t\trtnl_lock();\n\t\trc = atif_ioctl(cmd, argp);\n\t\trtnl_unlock();\n\t\tbreak;\n\t}\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,15 +15,14 @@\n \t\tbreak;\n \t}\n \tcase TIOCINQ: {\n-\t\t/*\n-\t\t * These two are safe on a single CPU system as only\n-\t\t * user tasks fiddle here\n-\t\t */\n-\t\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n+\t\tstruct sk_buff *skb;\n \t\tlong amount = 0;\n \n+\t\tspin_lock_irq(&sk->sk_receive_queue.lock);\n+\t\tskb = skb_peek(&sk->sk_receive_queue);\n \t\tif (skb)\n \t\t\tamount = skb->len - sizeof(struct ddpehdr);\n+\t\tspin_unlock_irq(&sk->sk_receive_queue.lock);\n \t\trc = put_user(amount, (int __user *)argp);\n \t\tbreak;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tstruct sk_buff *skb;",
                "\t\tspin_lock_irq(&sk->sk_receive_queue.lock);",
                "\t\tskb = skb_peek(&sk->sk_receive_queue);",
                "\t\tspin_unlock_irq(&sk->sk_receive_queue.lock);"
            ],
            "deleted": [
                "\t\t/*",
                "\t\t * These two are safe on a single CPU system as only",
                "\t\t * user tasks fiddle here",
                "\t\t */",
                "\t\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.6.8. atalk_ioctl in net/appletalk/ddp.c has a use-after-free because of an atalk_recvmsg race condition.",
        "id": 4259
    },
    {
        "cve_id": "CVE-2021-3483",
        "code_before_change": "static long\nnosy_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct client *client = file->private_data;\n\tspinlock_t *client_list_lock = &client->lynx->client_list_lock;\n\tstruct nosy_stats stats;\n\n\tswitch (cmd) {\n\tcase NOSY_IOC_GET_STATS:\n\t\tspin_lock_irq(client_list_lock);\n\t\tstats.total_packet_count = client->buffer.total_packet_count;\n\t\tstats.lost_packet_count  = client->buffer.lost_packet_count;\n\t\tspin_unlock_irq(client_list_lock);\n\n\t\tif (copy_to_user((void __user *) arg, &stats, sizeof stats))\n\t\t\treturn -EFAULT;\n\t\telse\n\t\t\treturn 0;\n\n\tcase NOSY_IOC_START:\n\t\tspin_lock_irq(client_list_lock);\n\t\tlist_add_tail(&client->link, &client->lynx->client_list);\n\t\tspin_unlock_irq(client_list_lock);\n\n\t\treturn 0;\n\n\tcase NOSY_IOC_STOP:\n\t\tspin_lock_irq(client_list_lock);\n\t\tlist_del_init(&client->link);\n\t\tspin_unlock_irq(client_list_lock);\n\n\t\treturn 0;\n\n\tcase NOSY_IOC_FILTER:\n\t\tspin_lock_irq(client_list_lock);\n\t\tclient->tcode_mask = arg;\n\t\tspin_unlock_irq(client_list_lock);\n\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t\t/* Flush buffer, configure filter. */\n\t}\n}",
        "code_after_change": "static long\nnosy_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct client *client = file->private_data;\n\tspinlock_t *client_list_lock = &client->lynx->client_list_lock;\n\tstruct nosy_stats stats;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase NOSY_IOC_GET_STATS:\n\t\tspin_lock_irq(client_list_lock);\n\t\tstats.total_packet_count = client->buffer.total_packet_count;\n\t\tstats.lost_packet_count  = client->buffer.lost_packet_count;\n\t\tspin_unlock_irq(client_list_lock);\n\n\t\tif (copy_to_user((void __user *) arg, &stats, sizeof stats))\n\t\t\treturn -EFAULT;\n\t\telse\n\t\t\treturn 0;\n\n\tcase NOSY_IOC_START:\n\t\tret = -EBUSY;\n\t\tspin_lock_irq(client_list_lock);\n\t\tif (list_empty(&client->link)) {\n\t\t\tlist_add_tail(&client->link, &client->lynx->client_list);\n\t\t\tret = 0;\n\t\t}\n\t\tspin_unlock_irq(client_list_lock);\n\n\t\treturn ret;\n\n\tcase NOSY_IOC_STOP:\n\t\tspin_lock_irq(client_list_lock);\n\t\tlist_del_init(&client->link);\n\t\tspin_unlock_irq(client_list_lock);\n\n\t\treturn 0;\n\n\tcase NOSY_IOC_FILTER:\n\t\tspin_lock_irq(client_list_lock);\n\t\tclient->tcode_mask = arg;\n\t\tspin_unlock_irq(client_list_lock);\n\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t\t/* Flush buffer, configure filter. */\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \tstruct client *client = file->private_data;\n \tspinlock_t *client_list_lock = &client->lynx->client_list_lock;\n \tstruct nosy_stats stats;\n+\tint ret;\n \n \tswitch (cmd) {\n \tcase NOSY_IOC_GET_STATS:\n@@ -18,11 +19,15 @@\n \t\t\treturn 0;\n \n \tcase NOSY_IOC_START:\n+\t\tret = -EBUSY;\n \t\tspin_lock_irq(client_list_lock);\n-\t\tlist_add_tail(&client->link, &client->lynx->client_list);\n+\t\tif (list_empty(&client->link)) {\n+\t\t\tlist_add_tail(&client->link, &client->lynx->client_list);\n+\t\t\tret = 0;\n+\t\t}\n \t\tspin_unlock_irq(client_list_lock);\n \n-\t\treturn 0;\n+\t\treturn ret;\n \n \tcase NOSY_IOC_STOP:\n \t\tspin_lock_irq(client_list_lock);",
        "function_modified_lines": {
            "added": [
                "\tint ret;",
                "\t\tret = -EBUSY;",
                "\t\tif (list_empty(&client->link)) {",
                "\t\t\tlist_add_tail(&client->link, &client->lynx->client_list);",
                "\t\t\tret = 0;",
                "\t\t}",
                "\t\treturn ret;"
            ],
            "deleted": [
                "\t\tlist_add_tail(&client->link, &client->lynx->client_list);",
                "\t\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Nosy driver in the Linux kernel. This issue allows a device to be inserted twice into a doubly-linked list, leading to a use-after-free when one of these devices is removed. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability. Versions before kernel 5.12-rc6 are affected",
        "id": 3007
    },
    {
        "cve_id": "CVE-2018-10902",
        "code_before_change": "int snd_rawmidi_output_params(struct snd_rawmidi_substream *substream,\n\t\t\t      struct snd_rawmidi_params * params)\n{\n\tchar *newbuf;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\t\n\tif (substream->append && substream->use_count > 1)\n\t\treturn -EBUSY;\n\tsnd_rawmidi_drain_output(substream);\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = krealloc(runtime->buffer, params->buffer_size,\n\t\t\t\t  GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t\truntime->avail = runtime->buffer_size;\n\t}\n\truntime->avail_min = params->avail_min;\n\tsubstream->active_sensing = !params->no_active_sensing;\n\treturn 0;\n}",
        "code_after_change": "int snd_rawmidi_output_params(struct snd_rawmidi_substream *substream,\n\t\t\t      struct snd_rawmidi_params * params)\n{\n\tchar *newbuf, *oldbuf;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\t\n\tif (substream->append && substream->use_count > 1)\n\t\treturn -EBUSY;\n\tsnd_rawmidi_drain_output(substream);\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = kmalloc(params->buffer_size, GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\tspin_lock_irq(&runtime->lock);\n\t\toldbuf = runtime->buffer;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t\truntime->avail = runtime->buffer_size;\n\t\truntime->appl_ptr = runtime->hw_ptr = 0;\n\t\tspin_unlock_irq(&runtime->lock);\n\t\tkfree(oldbuf);\n\t}\n\truntime->avail_min = params->avail_min;\n\tsubstream->active_sensing = !params->no_active_sensing;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n int snd_rawmidi_output_params(struct snd_rawmidi_substream *substream,\n \t\t\t      struct snd_rawmidi_params * params)\n {\n-\tchar *newbuf;\n+\tchar *newbuf, *oldbuf;\n \tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n \t\n \tif (substream->append && substream->use_count > 1)\n@@ -14,13 +14,17 @@\n \t\treturn -EINVAL;\n \t}\n \tif (params->buffer_size != runtime->buffer_size) {\n-\t\tnewbuf = krealloc(runtime->buffer, params->buffer_size,\n-\t\t\t\t  GFP_KERNEL);\n+\t\tnewbuf = kmalloc(params->buffer_size, GFP_KERNEL);\n \t\tif (!newbuf)\n \t\t\treturn -ENOMEM;\n+\t\tspin_lock_irq(&runtime->lock);\n+\t\toldbuf = runtime->buffer;\n \t\truntime->buffer = newbuf;\n \t\truntime->buffer_size = params->buffer_size;\n \t\truntime->avail = runtime->buffer_size;\n+\t\truntime->appl_ptr = runtime->hw_ptr = 0;\n+\t\tspin_unlock_irq(&runtime->lock);\n+\t\tkfree(oldbuf);\n \t}\n \truntime->avail_min = params->avail_min;\n \tsubstream->active_sensing = !params->no_active_sensing;",
        "function_modified_lines": {
            "added": [
                "\tchar *newbuf, *oldbuf;",
                "\t\tnewbuf = kmalloc(params->buffer_size, GFP_KERNEL);",
                "\t\tspin_lock_irq(&runtime->lock);",
                "\t\toldbuf = runtime->buffer;",
                "\t\truntime->appl_ptr = runtime->hw_ptr = 0;",
                "\t\tspin_unlock_irq(&runtime->lock);",
                "\t\tkfree(oldbuf);"
            ],
            "deleted": [
                "\tchar *newbuf;",
                "\t\tnewbuf = krealloc(runtime->buffer, params->buffer_size,",
                "\t\t\t\t  GFP_KERNEL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "It was found that the raw midi kernel driver does not protect against concurrent access which leads to a double realloc (double free) in snd_rawmidi_input_params() and snd_rawmidi_output_status() which are part of snd_rawmidi_ioctl() handler in rawmidi.c file. A malicious local attacker could possibly use this for privilege escalation.",
        "id": 1622
    },
    {
        "cve_id": "CVE-2021-0941",
        "code_before_change": "\nBPF_CALL_4(bpf_skb_adjust_room, struct sk_buff *, skb, s32, len_diff,\n\t   u32, mode, u64, flags)\n{\n\tu32 len_cur, len_diff_abs = abs(len_diff);\n\tu32 len_min = bpf_skb_net_base_len(skb);\n\tu32 len_max = __bpf_skb_max_len(skb);\n\t__be16 proto = skb->protocol;\n\tbool shrink = len_diff < 0;\n\tu32 off;\n\tint ret;\n\n\tif (unlikely(flags & ~(BPF_F_ADJ_ROOM_MASK |\n\t\t\t       BPF_F_ADJ_ROOM_NO_CSUM_RESET)))\n\t\treturn -EINVAL;\n\tif (unlikely(len_diff_abs > 0xfffU))\n\t\treturn -EFAULT;\n\tif (unlikely(proto != htons(ETH_P_IP) &&\n\t\t     proto != htons(ETH_P_IPV6)))\n\t\treturn -ENOTSUPP;\n\n\toff = skb_mac_header_len(skb);\n\tswitch (mode) {\n\tcase BPF_ADJ_ROOM_NET:\n\t\toff += bpf_skb_net_base_len(skb);\n\t\tbreak;\n\tcase BPF_ADJ_ROOM_MAC:\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOTSUPP;\n\t}\n\n\tlen_cur = skb->len - skb_network_offset(skb);\n\tif ((shrink && (len_diff_abs >= len_cur ||\n\t\t\tlen_cur - len_diff_abs < len_min)) ||\n\t    (!shrink && (skb->len + len_diff_abs > len_max &&\n\t\t\t !skb_is_gso(skb))))\n\t\treturn -ENOTSUPP;\n\n\tret = shrink ? bpf_skb_net_shrink(skb, off, len_diff_abs, flags) :\n\t\t       bpf_skb_net_grow(skb, off, len_diff_abs, flags);\n\tif (!ret && !(flags & BPF_F_ADJ_ROOM_NO_CSUM_RESET))\n\t\t__skb_reset_checksum_unnecessary(skb);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}",
        "code_after_change": "\nBPF_CALL_4(bpf_skb_adjust_room, struct sk_buff *, skb, s32, len_diff,\n\t   u32, mode, u64, flags)\n{\n\tu32 len_cur, len_diff_abs = abs(len_diff);\n\tu32 len_min = bpf_skb_net_base_len(skb);\n\tu32 len_max = BPF_SKB_MAX_LEN;\n\t__be16 proto = skb->protocol;\n\tbool shrink = len_diff < 0;\n\tu32 off;\n\tint ret;\n\n\tif (unlikely(flags & ~(BPF_F_ADJ_ROOM_MASK |\n\t\t\t       BPF_F_ADJ_ROOM_NO_CSUM_RESET)))\n\t\treturn -EINVAL;\n\tif (unlikely(len_diff_abs > 0xfffU))\n\t\treturn -EFAULT;\n\tif (unlikely(proto != htons(ETH_P_IP) &&\n\t\t     proto != htons(ETH_P_IPV6)))\n\t\treturn -ENOTSUPP;\n\n\toff = skb_mac_header_len(skb);\n\tswitch (mode) {\n\tcase BPF_ADJ_ROOM_NET:\n\t\toff += bpf_skb_net_base_len(skb);\n\t\tbreak;\n\tcase BPF_ADJ_ROOM_MAC:\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOTSUPP;\n\t}\n\n\tlen_cur = skb->len - skb_network_offset(skb);\n\tif ((shrink && (len_diff_abs >= len_cur ||\n\t\t\tlen_cur - len_diff_abs < len_min)) ||\n\t    (!shrink && (skb->len + len_diff_abs > len_max &&\n\t\t\t !skb_is_gso(skb))))\n\t\treturn -ENOTSUPP;\n\n\tret = shrink ? bpf_skb_net_shrink(skb, off, len_diff_abs, flags) :\n\t\t       bpf_skb_net_grow(skb, off, len_diff_abs, flags);\n\tif (!ret && !(flags & BPF_F_ADJ_ROOM_NO_CSUM_RESET))\n\t\t__skb_reset_checksum_unnecessary(skb);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n {\n \tu32 len_cur, len_diff_abs = abs(len_diff);\n \tu32 len_min = bpf_skb_net_base_len(skb);\n-\tu32 len_max = __bpf_skb_max_len(skb);\n+\tu32 len_max = BPF_SKB_MAX_LEN;\n \t__be16 proto = skb->protocol;\n \tbool shrink = len_diff < 0;\n \tu32 off;",
        "function_modified_lines": {
            "added": [
                "\tu32 len_max = BPF_SKB_MAX_LEN;"
            ],
            "deleted": [
                "\tu32 len_max = __bpf_skb_max_len(skb);"
            ]
        },
        "cwe": [
            "CWE-125",
            "CWE-416"
        ],
        "cve_description": "In bpf_skb_change_head of filter.c, there is a possible out of bounds read due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-154177719References: Upstream kernel",
        "id": 2838
    },
    {
        "cve_id": "CVE-2017-6874",
        "code_before_change": "static void put_ucounts(struct ucounts *ucounts)\n{\n\tunsigned long flags;\n\n\tif (atomic_dec_and_test(&ucounts->count)) {\n\t\tspin_lock_irqsave(&ucounts_lock, flags);\n\t\thlist_del_init(&ucounts->node);\n\t\tspin_unlock_irqrestore(&ucounts_lock, flags);\n\n\t\tkfree(ucounts);\n\t}\n}",
        "code_after_change": "static void put_ucounts(struct ucounts *ucounts)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ucounts_lock, flags);\n\tucounts->count -= 1;\n\tif (!ucounts->count)\n\t\thlist_del_init(&ucounts->node);\n\telse\n\t\tucounts = NULL;\n\tspin_unlock_irqrestore(&ucounts_lock, flags);\n\n\tkfree(ucounts);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,11 +2,13 @@\n {\n \tunsigned long flags;\n \n-\tif (atomic_dec_and_test(&ucounts->count)) {\n-\t\tspin_lock_irqsave(&ucounts_lock, flags);\n+\tspin_lock_irqsave(&ucounts_lock, flags);\n+\tucounts->count -= 1;\n+\tif (!ucounts->count)\n \t\thlist_del_init(&ucounts->node);\n-\t\tspin_unlock_irqrestore(&ucounts_lock, flags);\n+\telse\n+\t\tucounts = NULL;\n+\tspin_unlock_irqrestore(&ucounts_lock, flags);\n \n-\t\tkfree(ucounts);\n-\t}\n+\tkfree(ucounts);\n }",
        "function_modified_lines": {
            "added": [
                "\tspin_lock_irqsave(&ucounts_lock, flags);",
                "\tucounts->count -= 1;",
                "\tif (!ucounts->count)",
                "\telse",
                "\t\tucounts = NULL;",
                "\tspin_unlock_irqrestore(&ucounts_lock, flags);",
                "\tkfree(ucounts);"
            ],
            "deleted": [
                "\tif (atomic_dec_and_test(&ucounts->count)) {",
                "\t\tspin_lock_irqsave(&ucounts_lock, flags);",
                "\t\tspin_unlock_irqrestore(&ucounts_lock, flags);",
                "\t\tkfree(ucounts);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/ucount.c in the Linux kernel through 4.10.2 allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via crafted system calls that leverage certain decrement behavior that causes incorrect interaction between put_ucounts and get_ucounts.",
        "id": 1487
    },
    {
        "cve_id": "CVE-2017-7374",
        "code_before_change": "static int validate_user_key(struct fscrypt_info *crypt_info,\n\t\t\tstruct fscrypt_context *ctx, u8 *raw_key,\n\t\t\tconst char *prefix)\n{\n\tchar *description;\n\tstruct key *keyring_key;\n\tstruct fscrypt_key *master_key;\n\tconst struct user_key_payload *ukp;\n\tint res;\n\n\tdescription = kasprintf(GFP_NOFS, \"%s%*phN\", prefix,\n\t\t\t\tFS_KEY_DESCRIPTOR_SIZE,\n\t\t\t\tctx->master_key_descriptor);\n\tif (!description)\n\t\treturn -ENOMEM;\n\n\tkeyring_key = request_key(&key_type_logon, description, NULL);\n\tkfree(description);\n\tif (IS_ERR(keyring_key))\n\t\treturn PTR_ERR(keyring_key);\n\n\tif (keyring_key->type != &key_type_logon) {\n\t\tprintk_once(KERN_WARNING\n\t\t\t\t\"%s: key type must be logon\\n\", __func__);\n\t\tres = -ENOKEY;\n\t\tgoto out;\n\t}\n\tdown_read(&keyring_key->sem);\n\tukp = user_key_payload(keyring_key);\n\tif (ukp->datalen != sizeof(struct fscrypt_key)) {\n\t\tres = -EINVAL;\n\t\tup_read(&keyring_key->sem);\n\t\tgoto out;\n\t}\n\tmaster_key = (struct fscrypt_key *)ukp->data;\n\tBUILD_BUG_ON(FS_AES_128_ECB_KEY_SIZE != FS_KEY_DERIVATION_NONCE_SIZE);\n\n\tif (master_key->size != FS_AES_256_XTS_KEY_SIZE) {\n\t\tprintk_once(KERN_WARNING\n\t\t\t\t\"%s: key size incorrect: %d\\n\",\n\t\t\t\t__func__, master_key->size);\n\t\tres = -ENOKEY;\n\t\tup_read(&keyring_key->sem);\n\t\tgoto out;\n\t}\n\tres = derive_key_aes(ctx->nonce, master_key->raw, raw_key);\n\tup_read(&keyring_key->sem);\n\tif (res)\n\t\tgoto out;\n\n\tcrypt_info->ci_keyring_key = keyring_key;\n\treturn 0;\nout:\n\tkey_put(keyring_key);\n\treturn res;\n}",
        "code_after_change": "static int validate_user_key(struct fscrypt_info *crypt_info,\n\t\t\tstruct fscrypt_context *ctx, u8 *raw_key,\n\t\t\tconst char *prefix)\n{\n\tchar *description;\n\tstruct key *keyring_key;\n\tstruct fscrypt_key *master_key;\n\tconst struct user_key_payload *ukp;\n\tint res;\n\n\tdescription = kasprintf(GFP_NOFS, \"%s%*phN\", prefix,\n\t\t\t\tFS_KEY_DESCRIPTOR_SIZE,\n\t\t\t\tctx->master_key_descriptor);\n\tif (!description)\n\t\treturn -ENOMEM;\n\n\tkeyring_key = request_key(&key_type_logon, description, NULL);\n\tkfree(description);\n\tif (IS_ERR(keyring_key))\n\t\treturn PTR_ERR(keyring_key);\n\tdown_read(&keyring_key->sem);\n\n\tif (keyring_key->type != &key_type_logon) {\n\t\tprintk_once(KERN_WARNING\n\t\t\t\t\"%s: key type must be logon\\n\", __func__);\n\t\tres = -ENOKEY;\n\t\tgoto out;\n\t}\n\tukp = user_key_payload(keyring_key);\n\tif (ukp->datalen != sizeof(struct fscrypt_key)) {\n\t\tres = -EINVAL;\n\t\tgoto out;\n\t}\n\tmaster_key = (struct fscrypt_key *)ukp->data;\n\tBUILD_BUG_ON(FS_AES_128_ECB_KEY_SIZE != FS_KEY_DERIVATION_NONCE_SIZE);\n\n\tif (master_key->size != FS_AES_256_XTS_KEY_SIZE) {\n\t\tprintk_once(KERN_WARNING\n\t\t\t\t\"%s: key size incorrect: %d\\n\",\n\t\t\t\t__func__, master_key->size);\n\t\tres = -ENOKEY;\n\t\tgoto out;\n\t}\n\tres = derive_key_aes(ctx->nonce, master_key->raw, raw_key);\nout:\n\tup_read(&keyring_key->sem);\n\tkey_put(keyring_key);\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,6 +18,7 @@\n \tkfree(description);\n \tif (IS_ERR(keyring_key))\n \t\treturn PTR_ERR(keyring_key);\n+\tdown_read(&keyring_key->sem);\n \n \tif (keyring_key->type != &key_type_logon) {\n \t\tprintk_once(KERN_WARNING\n@@ -25,11 +26,9 @@\n \t\tres = -ENOKEY;\n \t\tgoto out;\n \t}\n-\tdown_read(&keyring_key->sem);\n \tukp = user_key_payload(keyring_key);\n \tif (ukp->datalen != sizeof(struct fscrypt_key)) {\n \t\tres = -EINVAL;\n-\t\tup_read(&keyring_key->sem);\n \t\tgoto out;\n \t}\n \tmaster_key = (struct fscrypt_key *)ukp->data;\n@@ -40,17 +39,11 @@\n \t\t\t\t\"%s: key size incorrect: %d\\n\",\n \t\t\t\t__func__, master_key->size);\n \t\tres = -ENOKEY;\n-\t\tup_read(&keyring_key->sem);\n \t\tgoto out;\n \t}\n \tres = derive_key_aes(ctx->nonce, master_key->raw, raw_key);\n+out:\n \tup_read(&keyring_key->sem);\n-\tif (res)\n-\t\tgoto out;\n-\n-\tcrypt_info->ci_keyring_key = keyring_key;\n-\treturn 0;\n-out:\n \tkey_put(keyring_key);\n \treturn res;\n }",
        "function_modified_lines": {
            "added": [
                "\tdown_read(&keyring_key->sem);",
                "out:"
            ],
            "deleted": [
                "\tdown_read(&keyring_key->sem);",
                "\t\tup_read(&keyring_key->sem);",
                "\t\tup_read(&keyring_key->sem);",
                "\tif (res)",
                "\t\tgoto out;",
                "",
                "\tcrypt_info->ci_keyring_key = keyring_key;",
                "\treturn 0;",
                "out:"
            ]
        },
        "cwe": [
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in fs/crypto/ in the Linux kernel before 4.10.7 allows local users to cause a denial of service (NULL pointer dereference) or possibly gain privileges by revoking keyring keys being used for ext4, f2fs, or ubifs encryption, causing cryptographic transform objects to be freed prematurely.",
        "id": 1500
    },
    {
        "cve_id": "CVE-2019-11487",
        "code_before_change": "static int link_pipe(struct pipe_inode_info *ipipe,\n\t\t     struct pipe_inode_info *opipe,\n\t\t     size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, i = 0, nbuf;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * If we have iterated all input buffers or ran out of\n\t\t * output room, break.\n\t\t */\n\t\tif (i >= ipipe->nrbufs || opipe->nrbufs >= opipe->buffers)\n\t\t\tbreak;\n\n\t\tibuf = ipipe->bufs + ((ipipe->curbuf + i) & (ipipe->buffers-1));\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\n\t\t/*\n\t\t * Get a reference to this pipe buffer,\n\t\t * so we can copy the contents over.\n\t\t */\n\t\tpipe_buf_get(ipipe, ibuf);\n\n\t\tobuf = opipe->bufs + nbuf;\n\t\t*obuf = *ibuf;\n\n\t\t/*\n\t\t * Don't inherit the gift flag, we need to\n\t\t * prevent multiple steals of this page.\n\t\t */\n\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\tif (obuf->len > len)\n\t\t\tobuf->len = len;\n\n\t\topipe->nrbufs++;\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t\ti++;\n\t} while (len);\n\n\t/*\n\t * return EAGAIN if we have the potential of some data in the\n\t * future, otherwise just return 0\n\t */\n\tif (!ret && ipipe->waiting_writers && (flags & SPLICE_F_NONBLOCK))\n\t\tret = -EAGAIN;\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\treturn ret;\n}",
        "code_after_change": "static int link_pipe(struct pipe_inode_info *ipipe,\n\t\t     struct pipe_inode_info *opipe,\n\t\t     size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, i = 0, nbuf;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * If we have iterated all input buffers or ran out of\n\t\t * output room, break.\n\t\t */\n\t\tif (i >= ipipe->nrbufs || opipe->nrbufs >= opipe->buffers)\n\t\t\tbreak;\n\n\t\tibuf = ipipe->bufs + ((ipipe->curbuf + i) & (ipipe->buffers-1));\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\n\t\t/*\n\t\t * Get a reference to this pipe buffer,\n\t\t * so we can copy the contents over.\n\t\t */\n\t\tif (!pipe_buf_get(ipipe, ibuf)) {\n\t\t\tif (ret == 0)\n\t\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tobuf = opipe->bufs + nbuf;\n\t\t*obuf = *ibuf;\n\n\t\t/*\n\t\t * Don't inherit the gift flag, we need to\n\t\t * prevent multiple steals of this page.\n\t\t */\n\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\tif (obuf->len > len)\n\t\t\tobuf->len = len;\n\n\t\topipe->nrbufs++;\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t\ti++;\n\t} while (len);\n\n\t/*\n\t * return EAGAIN if we have the potential of some data in the\n\t * future, otherwise just return 0\n\t */\n\tif (!ret && ipipe->waiting_writers && (flags & SPLICE_F_NONBLOCK))\n\t\tret = -EAGAIN;\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -34,7 +34,11 @@\n \t\t * Get a reference to this pipe buffer,\n \t\t * so we can copy the contents over.\n \t\t */\n-\t\tpipe_buf_get(ipipe, ibuf);\n+\t\tif (!pipe_buf_get(ipipe, ibuf)) {\n+\t\t\tif (ret == 0)\n+\t\t\t\tret = -EFAULT;\n+\t\t\tbreak;\n+\t\t}\n \n \t\tobuf = opipe->bufs + nbuf;\n \t\t*obuf = *ibuf;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!pipe_buf_get(ipipe, ibuf)) {",
                "\t\t\tif (ret == 0)",
                "\t\t\t\tret = -EFAULT;",
                "\t\t\tbreak;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tpipe_buf_get(ipipe, ibuf);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 5.1-rc5 allows page->_refcount reference count overflow, with resultant use-after-free issues, if about 140 GiB of RAM exists. This is related to fs/fuse/dev.c, fs/pipe.c, fs/splice.c, include/linux/mm.h, include/linux/pipe_fs_i.h, kernel/trace/trace.c, mm/gup.c, and mm/hugetlb.c. It can occur with FUSE requests.",
        "id": 1920
    },
    {
        "cve_id": "CVE-2019-8912",
        "code_before_change": "int af_alg_release(struct socket *sock)\n{\n\tif (sock->sk)\n\t\tsock_put(sock->sk);\n\treturn 0;\n}",
        "code_after_change": "int af_alg_release(struct socket *sock)\n{\n\tif (sock->sk) {\n\t\tsock_put(sock->sk);\n\t\tsock->sk = NULL;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,8 @@\n int af_alg_release(struct socket *sock)\n {\n-\tif (sock->sk)\n+\tif (sock->sk) {\n \t\tsock_put(sock->sk);\n+\t\tsock->sk = NULL;\n+\t}\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (sock->sk) {",
                "\t\tsock->sk = NULL;",
                "\t}"
            ],
            "deleted": [
                "\tif (sock->sk)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel through 4.20.11, af_alg_release() in crypto/af_alg.c neglects to set a NULL value for a certain structure member, which leads to a use-after-free in sockfs_setattr.",
        "id": 2347
    },
    {
        "cve_id": "CVE-2023-0240",
        "code_before_change": "static void __io_free_req(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx;\n\tstruct io_ring_ctx *ctx;\n\n\tio_dismantle_req(req);\n\ttctx = req->task->io_uring;\n\tctx = req->ctx;\n\n\tatomic_long_inc(&tctx->req_complete);\n\tif (tctx->in_idle)\n\t\twake_up(&tctx->wait);\n\tput_task_struct(req->task);\n\n\tif (likely(!io_is_fallback_req(req)))\n\t\tkmem_cache_free(req_cachep, req);\n\telse\n\t\tclear_bit_unlock(0, (unsigned long *) &ctx->fallback_req);\n\tpercpu_ref_put(&ctx->refs);\n}",
        "code_after_change": "static void __io_free_req(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = req->task->io_uring;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_dismantle_req(req);\n\n\tatomic_long_inc(&tctx->req_complete);\n\tif (tctx->in_idle)\n\t\twake_up(&tctx->wait);\n\tput_task_struct(req->task);\n\n\tif (likely(!io_is_fallback_req(req)))\n\t\tkmem_cache_free(req_cachep, req);\n\telse\n\t\tclear_bit_unlock(0, (unsigned long *) &ctx->fallback_req);\n\tpercpu_ref_put(&ctx->refs);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,9 @@\n static void __io_free_req(struct io_kiocb *req)\n {\n-\tstruct io_uring_task *tctx;\n-\tstruct io_ring_ctx *ctx;\n+\tstruct io_uring_task *tctx = req->task->io_uring;\n+\tstruct io_ring_ctx *ctx = req->ctx;\n \n \tio_dismantle_req(req);\n-\ttctx = req->task->io_uring;\n-\tctx = req->ctx;\n \n \tatomic_long_inc(&tctx->req_complete);\n \tif (tctx->in_idle)",
        "function_modified_lines": {
            "added": [
                "\tstruct io_uring_task *tctx = req->task->io_uring;",
                "\tstruct io_ring_ctx *ctx = req->ctx;"
            ],
            "deleted": [
                "\tstruct io_uring_task *tctx;",
                "\tstruct io_ring_ctx *ctx;",
                "\ttctx = req->task->io_uring;",
                "\tctx = req->ctx;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "id": 3818
    },
    {
        "cve_id": "CVE-2022-47946",
        "code_before_change": "static void io_uring_cancel_task_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t  struct files_struct *files)\n{\n\tstruct task_struct *task = current;\n\tbool did_park = false;\n\n\tif ((ctx->flags & IORING_SETUP_SQPOLL) && ctx->sq_data) {\n\t\tio_disable_sqo_submit(ctx);\n\t\tdid_park = io_sq_thread_park(ctx->sq_data);\n\t\tif (did_park) {\n\t\t\ttask = ctx->sq_data->thread;\n\t\t\tatomic_inc(&task->io_uring->in_idle);\n\t\t}\n\t}\n\n\tio_cancel_defer_files(ctx, task, files);\n\n\tio_uring_cancel_files(ctx, task, files);\n\tif (!files)\n\t\tio_uring_try_cancel_requests(ctx, task, NULL);\n\n\tif (did_park) {\n\t\tatomic_dec(&task->io_uring->in_idle);\n\t\tio_sq_thread_unpark(ctx->sq_data);\n\t}\n}",
        "code_after_change": "static void io_uring_cancel_task_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t  struct files_struct *files)\n{\n\tstruct task_struct *task = current;\n\tbool did_park = false;\n\n\tif ((ctx->flags & IORING_SETUP_SQPOLL) && ctx->sq_data) {\n\t\t/* never started, nothing to cancel */\n\t\tif (ctx->flags & IORING_SETUP_R_DISABLED) {\n\t\t\tio_sq_offload_start(ctx);\n\t\t\treturn;\n\t\t}\n\t\tdid_park = io_sq_thread_park(ctx->sq_data);\n\t\tif (did_park) {\n\t\t\ttask = ctx->sq_data->thread;\n\t\t\tatomic_inc(&task->io_uring->in_idle);\n\t\t}\n\t}\n\n\tio_cancel_defer_files(ctx, task, files);\n\n\tio_uring_cancel_files(ctx, task, files);\n\tif (!files)\n\t\tio_uring_try_cancel_requests(ctx, task, NULL);\n\n\tif (did_park) {\n\t\tatomic_dec(&task->io_uring->in_idle);\n\t\tio_sq_thread_unpark(ctx->sq_data);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,11 @@\n \tbool did_park = false;\n \n \tif ((ctx->flags & IORING_SETUP_SQPOLL) && ctx->sq_data) {\n-\t\tio_disable_sqo_submit(ctx);\n+\t\t/* never started, nothing to cancel */\n+\t\tif (ctx->flags & IORING_SETUP_R_DISABLED) {\n+\t\t\tio_sq_offload_start(ctx);\n+\t\t\treturn;\n+\t\t}\n \t\tdid_park = io_sq_thread_park(ctx->sq_data);\n \t\tif (did_park) {\n \t\t\ttask = ctx->sq_data->thread;",
        "function_modified_lines": {
            "added": [
                "\t\t/* never started, nothing to cancel */",
                "\t\tif (ctx->flags & IORING_SETUP_R_DISABLED) {",
                "\t\t\tio_sq_offload_start(ctx);",
                "\t\t\treturn;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tio_disable_sqo_submit(ctx);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 5.10.x before 5.10.155. A use-after-free in io_sqpoll_wait_sq in fs/io_uring.c allows an attacker to crash the kernel, resulting in denial of service. finish_wait can be skipped. An attack can occur in some situations by forking a process and then quickly terminating it. NOTE: later kernel versions, such as the 5.15 longterm series, substantially changed the implementation of io_sqpoll_wait_sq.",
        "id": 3787
    },
    {
        "cve_id": "CVE-2019-20934",
        "code_before_change": "static int __do_execve_file(int fd, struct filename *filename,\n\t\t\t    struct user_arg_ptr argv,\n\t\t\t    struct user_arg_ptr envp,\n\t\t\t    int flags, struct file *file)\n{\n\tchar *pathbuf = NULL;\n\tstruct linux_binprm *bprm;\n\tstruct files_struct *displaced;\n\tint retval;\n\n\tif (IS_ERR(filename))\n\t\treturn PTR_ERR(filename);\n\n\t/*\n\t * We move the actual failure in case of RLIMIT_NPROC excess from\n\t * set*uid() to execve() because too many poorly written programs\n\t * don't check setuid() return code.  Here we additionally recheck\n\t * whether NPROC limit is still exceeded.\n\t */\n\tif ((current->flags & PF_NPROC_EXCEEDED) &&\n\t    atomic_read(&current_user()->processes) > rlimit(RLIMIT_NPROC)) {\n\t\tretval = -EAGAIN;\n\t\tgoto out_ret;\n\t}\n\n\t/* We're below the limit (still or again), so we don't want to make\n\t * further execve() calls fail. */\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = unshare_files(&displaced);\n\tif (retval)\n\t\tgoto out_ret;\n\n\tretval = -ENOMEM;\n\tbprm = kzalloc(sizeof(*bprm), GFP_KERNEL);\n\tif (!bprm)\n\t\tgoto out_files;\n\n\tretval = prepare_bprm_creds(bprm);\n\tif (retval)\n\t\tgoto out_free;\n\n\tcheck_unsafe_exec(bprm);\n\tcurrent->in_execve = 1;\n\n\tif (!file)\n\t\tfile = do_open_execat(fd, filename, flags);\n\tretval = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out_unmark;\n\n\tsched_exec();\n\n\tbprm->file = file;\n\tif (!filename) {\n\t\tbprm->filename = \"none\";\n\t} else if (fd == AT_FDCWD || filename->name[0] == '/') {\n\t\tbprm->filename = filename->name;\n\t} else {\n\t\tif (filename->name[0] == '\\0')\n\t\t\tpathbuf = kasprintf(GFP_KERNEL, \"/dev/fd/%d\", fd);\n\t\telse\n\t\t\tpathbuf = kasprintf(GFP_KERNEL, \"/dev/fd/%d/%s\",\n\t\t\t\t\t    fd, filename->name);\n\t\tif (!pathbuf) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto out_unmark;\n\t\t}\n\t\t/*\n\t\t * Record that a name derived from an O_CLOEXEC fd will be\n\t\t * inaccessible after exec. Relies on having exclusive access to\n\t\t * current->files (due to unshare_files above).\n\t\t */\n\t\tif (close_on_exec(fd, rcu_dereference_raw(current->files->fdt)))\n\t\t\tbprm->interp_flags |= BINPRM_FLAGS_PATH_INACCESSIBLE;\n\t\tbprm->filename = pathbuf;\n\t}\n\tbprm->interp = bprm->filename;\n\n\tretval = bprm_mm_init(bprm);\n\tif (retval)\n\t\tgoto out_unmark;\n\n\tretval = prepare_arg_pages(bprm, argv, envp);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings_kernel(1, &bprm->filename, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tbprm->exec = bprm->p;\n\tretval = copy_strings(bprm->envc, envp, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings(bprm->argc, argv, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\twould_dump(bprm, bprm->file);\n\n\tretval = exec_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\t/* execve succeeded */\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\tmembarrier_execve(current);\n\trseq_execve(current);\n\tacct_update_integrals(current);\n\ttask_numa_free(current);\n\tfree_bprm(bprm);\n\tkfree(pathbuf);\n\tif (filename)\n\t\tputname(filename);\n\tif (displaced)\n\t\tput_files_struct(displaced);\n\treturn retval;\n\nout:\n\tif (bprm->mm) {\n\t\tacct_arg_size(bprm, 0);\n\t\tmmput(bprm->mm);\n\t}\n\nout_unmark:\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\nout_free:\n\tfree_bprm(bprm);\n\tkfree(pathbuf);\n\nout_files:\n\tif (displaced)\n\t\treset_files_struct(displaced);\nout_ret:\n\tif (filename)\n\t\tputname(filename);\n\treturn retval;\n}",
        "code_after_change": "static int __do_execve_file(int fd, struct filename *filename,\n\t\t\t    struct user_arg_ptr argv,\n\t\t\t    struct user_arg_ptr envp,\n\t\t\t    int flags, struct file *file)\n{\n\tchar *pathbuf = NULL;\n\tstruct linux_binprm *bprm;\n\tstruct files_struct *displaced;\n\tint retval;\n\n\tif (IS_ERR(filename))\n\t\treturn PTR_ERR(filename);\n\n\t/*\n\t * We move the actual failure in case of RLIMIT_NPROC excess from\n\t * set*uid() to execve() because too many poorly written programs\n\t * don't check setuid() return code.  Here we additionally recheck\n\t * whether NPROC limit is still exceeded.\n\t */\n\tif ((current->flags & PF_NPROC_EXCEEDED) &&\n\t    atomic_read(&current_user()->processes) > rlimit(RLIMIT_NPROC)) {\n\t\tretval = -EAGAIN;\n\t\tgoto out_ret;\n\t}\n\n\t/* We're below the limit (still or again), so we don't want to make\n\t * further execve() calls fail. */\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = unshare_files(&displaced);\n\tif (retval)\n\t\tgoto out_ret;\n\n\tretval = -ENOMEM;\n\tbprm = kzalloc(sizeof(*bprm), GFP_KERNEL);\n\tif (!bprm)\n\t\tgoto out_files;\n\n\tretval = prepare_bprm_creds(bprm);\n\tif (retval)\n\t\tgoto out_free;\n\n\tcheck_unsafe_exec(bprm);\n\tcurrent->in_execve = 1;\n\n\tif (!file)\n\t\tfile = do_open_execat(fd, filename, flags);\n\tretval = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out_unmark;\n\n\tsched_exec();\n\n\tbprm->file = file;\n\tif (!filename) {\n\t\tbprm->filename = \"none\";\n\t} else if (fd == AT_FDCWD || filename->name[0] == '/') {\n\t\tbprm->filename = filename->name;\n\t} else {\n\t\tif (filename->name[0] == '\\0')\n\t\t\tpathbuf = kasprintf(GFP_KERNEL, \"/dev/fd/%d\", fd);\n\t\telse\n\t\t\tpathbuf = kasprintf(GFP_KERNEL, \"/dev/fd/%d/%s\",\n\t\t\t\t\t    fd, filename->name);\n\t\tif (!pathbuf) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto out_unmark;\n\t\t}\n\t\t/*\n\t\t * Record that a name derived from an O_CLOEXEC fd will be\n\t\t * inaccessible after exec. Relies on having exclusive access to\n\t\t * current->files (due to unshare_files above).\n\t\t */\n\t\tif (close_on_exec(fd, rcu_dereference_raw(current->files->fdt)))\n\t\t\tbprm->interp_flags |= BINPRM_FLAGS_PATH_INACCESSIBLE;\n\t\tbprm->filename = pathbuf;\n\t}\n\tbprm->interp = bprm->filename;\n\n\tretval = bprm_mm_init(bprm);\n\tif (retval)\n\t\tgoto out_unmark;\n\n\tretval = prepare_arg_pages(bprm, argv, envp);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings_kernel(1, &bprm->filename, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tbprm->exec = bprm->p;\n\tretval = copy_strings(bprm->envc, envp, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings(bprm->argc, argv, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\twould_dump(bprm, bprm->file);\n\n\tretval = exec_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\t/* execve succeeded */\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\tmembarrier_execve(current);\n\trseq_execve(current);\n\tacct_update_integrals(current);\n\ttask_numa_free(current, false);\n\tfree_bprm(bprm);\n\tkfree(pathbuf);\n\tif (filename)\n\t\tputname(filename);\n\tif (displaced)\n\t\tput_files_struct(displaced);\n\treturn retval;\n\nout:\n\tif (bprm->mm) {\n\t\tacct_arg_size(bprm, 0);\n\t\tmmput(bprm->mm);\n\t}\n\nout_unmark:\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\nout_free:\n\tfree_bprm(bprm);\n\tkfree(pathbuf);\n\nout_files:\n\tif (displaced)\n\t\treset_files_struct(displaced);\nout_ret:\n\tif (filename)\n\t\tputname(filename);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -114,7 +114,7 @@\n \tmembarrier_execve(current);\n \trseq_execve(current);\n \tacct_update_integrals(current);\n-\ttask_numa_free(current);\n+\ttask_numa_free(current, false);\n \tfree_bprm(bprm);\n \tkfree(pathbuf);\n \tif (filename)",
        "function_modified_lines": {
            "added": [
                "\ttask_numa_free(current, false);"
            ],
            "deleted": [
                "\ttask_numa_free(current);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.2.6. On NUMA systems, the Linux fair scheduler has a use-after-free in show_numa_stats() because NUMA fault statistics are inappropriately freed, aka CID-16d51a590a8c.",
        "id": 2289
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int compat_copy_entries_to_user(unsigned int total_size,\n\t\t\t\t       struct xt_table *table,\n\t\t\t\t       void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct arpt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\tvfree(counters);\n\treturn ret;\n}",
        "code_after_change": "static int compat_copy_entries_to_user(unsigned int total_size,\n\t\t\t\t       struct xt_table *table,\n\t\t\t\t       void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct arpt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\tvfree(counters);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t\t\t\t       void __user *userptr)\n {\n \tstruct xt_counters *counters;\n-\tconst struct xt_table_info *private = table->private;\n+\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n \tvoid __user *pos;\n \tunsigned int size;\n \tint ret = 0;",
        "function_modified_lines": {
            "added": [
                "\tconst struct xt_table_info *private = xt_table_get_private_protected(table);"
            ],
            "deleted": [
                "\tconst struct xt_table_info *private = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2773
    },
    {
        "cve_id": "CVE-2022-1652",
        "code_before_change": "static int set_next_request(void)\n{\n\tcurrent_req = list_first_entry_or_null(&floppy_reqs, struct request,\n\t\t\t\t\t       queuelist);\n\tif (current_req) {\n\t\tcurrent_req->error_count = 0;\n\t\tlist_del_init(&current_req->queuelist);\n\t}\n\treturn current_req != NULL;\n}",
        "code_after_change": "static int set_next_request(void)\n{\n\tcurrent_req = list_first_entry_or_null(&floppy_reqs, struct request,\n\t\t\t\t\t       queuelist);\n\tif (current_req) {\n\t\tfloppy_errors = 0;\n\t\tlist_del_init(&current_req->queuelist);\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,8 +3,9 @@\n \tcurrent_req = list_first_entry_or_null(&floppy_reqs, struct request,\n \t\t\t\t\t       queuelist);\n \tif (current_req) {\n-\t\tcurrent_req->error_count = 0;\n+\t\tfloppy_errors = 0;\n \t\tlist_del_init(&current_req->queuelist);\n+\t\treturn 1;\n \t}\n-\treturn current_req != NULL;\n+\treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tfloppy_errors = 0;",
                "\t\treturn 1;",
                "\treturn 0;"
            ],
            "deleted": [
                "\t\tcurrent_req->error_count = 0;",
                "\treturn current_req != NULL;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Linux Kernel could allow a local attacker to execute arbitrary code on the system, caused by a concurrency use-after-free flaw in the bad_flp_intr function. By executing a specially-crafted program, an attacker could exploit this vulnerability to execute arbitrary code or cause a denial of service condition on the system.",
        "id": 3269
    },
    {
        "cve_id": "CVE-2017-17053",
        "code_before_change": "static inline int init_new_context(struct task_struct *tsk,\n\t\t\t\t   struct mm_struct *mm)\n{\n\t#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\tif (cpu_feature_enabled(X86_FEATURE_OSPKE)) {\n\t\t/* pkey 0 is the default and always allocated */\n\t\tmm->context.pkey_allocation_map = 0x1;\n\t\t/* -1 means unallocated or invalid */\n\t\tmm->context.execute_only_pkey = -1;\n\t}\n\t#endif\n\tinit_new_context_ldt(tsk, mm);\n\n\treturn 0;\n}",
        "code_after_change": "static inline int init_new_context(struct task_struct *tsk,\n\t\t\t\t   struct mm_struct *mm)\n{\n\t#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\tif (cpu_feature_enabled(X86_FEATURE_OSPKE)) {\n\t\t/* pkey 0 is the default and always allocated */\n\t\tmm->context.pkey_allocation_map = 0x1;\n\t\t/* -1 means unallocated or invalid */\n\t\tmm->context.execute_only_pkey = -1;\n\t}\n\t#endif\n\treturn init_new_context_ldt(tsk, mm);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,5 @@\n \t\tmm->context.execute_only_pkey = -1;\n \t}\n \t#endif\n-\tinit_new_context_ldt(tsk, mm);\n-\n-\treturn 0;\n+\treturn init_new_context_ldt(tsk, mm);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn init_new_context_ldt(tsk, mm);"
            ],
            "deleted": [
                "\tinit_new_context_ldt(tsk, mm);",
                "",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The init_new_context function in arch/x86/include/asm/mmu_context.h in the Linux kernel before 4.12.10 does not correctly handle errors from LDT table allocation when forking a new process, allowing a local attacker to achieve a use-after-free or possibly have unspecified other impact by running a specially crafted program. This vulnerability only affected kernels built with CONFIG_MODIFY_LDT_SYSCALL=y.",
        "id": 1358
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int get_info(struct net *net, void __user *user, const int *len)\n{\n\tchar name[XT_TABLE_MAXNAMELEN];\n\tstruct xt_table *t;\n\tint ret;\n\n\tif (*len != sizeof(struct ip6t_getinfo))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(name, user, sizeof(name)) != 0)\n\t\treturn -EFAULT;\n\n\tname[XT_TABLE_MAXNAMELEN-1] = '\\0';\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_lock(AF_INET6);\n#endif\n\tt = xt_request_find_table_lock(net, AF_INET6, name);\n\tif (!IS_ERR(t)) {\n\t\tstruct ip6t_getinfo info;\n\t\tconst struct xt_table_info *private = t->private;\n#ifdef CONFIG_COMPAT\n\t\tstruct xt_table_info tmp;\n\n\t\tif (in_compat_syscall()) {\n\t\t\tret = compat_table_info(private, &tmp);\n\t\t\txt_compat_flush_offsets(AF_INET6);\n\t\t\tprivate = &tmp;\n\t\t}\n#endif\n\t\tmemset(&info, 0, sizeof(info));\n\t\tinfo.valid_hooks = t->valid_hooks;\n\t\tmemcpy(info.hook_entry, private->hook_entry,\n\t\t       sizeof(info.hook_entry));\n\t\tmemcpy(info.underflow, private->underflow,\n\t\t       sizeof(info.underflow));\n\t\tinfo.num_entries = private->number;\n\t\tinfo.size = private->size;\n\t\tstrcpy(info.name, name);\n\n\t\tif (copy_to_user(user, &info, *len) != 0)\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\n\t\txt_table_unlock(t);\n\t\tmodule_put(t->me);\n\t} else\n\t\tret = PTR_ERR(t);\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_unlock(AF_INET6);\n#endif\n\treturn ret;\n}",
        "code_after_change": "static int get_info(struct net *net, void __user *user, const int *len)\n{\n\tchar name[XT_TABLE_MAXNAMELEN];\n\tstruct xt_table *t;\n\tint ret;\n\n\tif (*len != sizeof(struct ip6t_getinfo))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(name, user, sizeof(name)) != 0)\n\t\treturn -EFAULT;\n\n\tname[XT_TABLE_MAXNAMELEN-1] = '\\0';\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_lock(AF_INET6);\n#endif\n\tt = xt_request_find_table_lock(net, AF_INET6, name);\n\tif (!IS_ERR(t)) {\n\t\tstruct ip6t_getinfo info;\n\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n#ifdef CONFIG_COMPAT\n\t\tstruct xt_table_info tmp;\n\n\t\tif (in_compat_syscall()) {\n\t\t\tret = compat_table_info(private, &tmp);\n\t\t\txt_compat_flush_offsets(AF_INET6);\n\t\t\tprivate = &tmp;\n\t\t}\n#endif\n\t\tmemset(&info, 0, sizeof(info));\n\t\tinfo.valid_hooks = t->valid_hooks;\n\t\tmemcpy(info.hook_entry, private->hook_entry,\n\t\t       sizeof(info.hook_entry));\n\t\tmemcpy(info.underflow, private->underflow,\n\t\t       sizeof(info.underflow));\n\t\tinfo.num_entries = private->number;\n\t\tinfo.size = private->size;\n\t\tstrcpy(info.name, name);\n\n\t\tif (copy_to_user(user, &info, *len) != 0)\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\n\t\txt_table_unlock(t);\n\t\tmodule_put(t->me);\n\t} else\n\t\tret = PTR_ERR(t);\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_unlock(AF_INET6);\n#endif\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,7 @@\n \tt = xt_request_find_table_lock(net, AF_INET6, name);\n \tif (!IS_ERR(t)) {\n \t\tstruct ip6t_getinfo info;\n-\t\tconst struct xt_table_info *private = t->private;\n+\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n #ifdef CONFIG_COMPAT\n \t\tstruct xt_table_info tmp;\n ",
        "function_modified_lines": {
            "added": [
                "\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);"
            ],
            "deleted": [
                "\t\tconst struct xt_table_info *private = t->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2784
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "struct nfc_llcp_local *nfc_llcp_find_local(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlist_for_each_entry(local, &llcp_devices, list)\n\t\tif (local->dev == dev)\n\t\t\treturn local;\n\n\tpr_debug(\"No device found\\n\");\n\n\treturn NULL;\n}",
        "code_after_change": "struct nfc_llcp_local *nfc_llcp_find_local(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local;\n\tstruct nfc_llcp_local *res = NULL;\n\n\tspin_lock(&llcp_devices_lock);\n\tlist_for_each_entry(local, &llcp_devices, list)\n\t\tif (local->dev == dev) {\n\t\t\tres = nfc_llcp_local_get(local);\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&llcp_devices_lock);\n\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,15 @@\n struct nfc_llcp_local *nfc_llcp_find_local(struct nfc_dev *dev)\n {\n \tstruct nfc_llcp_local *local;\n+\tstruct nfc_llcp_local *res = NULL;\n \n+\tspin_lock(&llcp_devices_lock);\n \tlist_for_each_entry(local, &llcp_devices, list)\n-\t\tif (local->dev == dev)\n-\t\t\treturn local;\n+\t\tif (local->dev == dev) {\n+\t\t\tres = nfc_llcp_local_get(local);\n+\t\t\tbreak;\n+\t\t}\n+\tspin_unlock(&llcp_devices_lock);\n \n-\tpr_debug(\"No device found\\n\");\n-\n-\treturn NULL;\n+\treturn res;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct nfc_llcp_local *res = NULL;",
                "\tspin_lock(&llcp_devices_lock);",
                "\t\tif (local->dev == dev) {",
                "\t\t\tres = nfc_llcp_local_get(local);",
                "\t\t\tbreak;",
                "\t\t}",
                "\tspin_unlock(&llcp_devices_lock);",
                "\treturn res;"
            ],
            "deleted": [
                "\t\tif (local->dev == dev)",
                "\t\t\treturn local;",
                "\tpr_debug(\"No device found\\n\");",
                "",
                "\treturn NULL;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4145
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "void vmw_resource_unreserve(struct vmw_resource *res,\n\t\t\t    bool dirty_set,\n\t\t\t    bool dirty,\n\t\t\t    bool switch_guest_memory,\n\t\t\t    struct vmw_bo *new_guest_memory_bo,\n\t\t\t    unsigned long new_guest_memory_offset)\n{\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\n\tif (!list_empty(&res->lru_head))\n\t\treturn;\n\n\tif (switch_guest_memory && new_guest_memory_bo != res->guest_memory_bo) {\n\t\tif (res->guest_memory_bo) {\n\t\t\tvmw_resource_mob_detach(res);\n\t\t\tif (res->coherent)\n\t\t\t\tvmw_bo_dirty_release(res->guest_memory_bo);\n\t\t\tvmw_bo_unreference(&res->guest_memory_bo);\n\t\t}\n\n\t\tif (new_guest_memory_bo) {\n\t\t\tres->guest_memory_bo = vmw_bo_reference(new_guest_memory_bo);\n\n\t\t\t/*\n\t\t\t * The validation code should already have added a\n\t\t\t * dirty tracker here.\n\t\t\t */\n\t\t\tWARN_ON(res->coherent && !new_guest_memory_bo->dirty);\n\n\t\t\tvmw_resource_mob_attach(res);\n\t\t} else {\n\t\t\tres->guest_memory_bo = NULL;\n\t\t}\n\t} else if (switch_guest_memory && res->coherent) {\n\t\tvmw_bo_dirty_release(res->guest_memory_bo);\n\t}\n\n\tif (switch_guest_memory)\n\t\tres->guest_memory_offset = new_guest_memory_offset;\n\n\tif (dirty_set)\n\t\tres->res_dirty = dirty;\n\n\tif (!res->func->may_evict || res->id == -1 || res->pin_count)\n\t\treturn;\n\n\tspin_lock(&dev_priv->resource_lock);\n\tlist_add_tail(&res->lru_head,\n\t\t      &res->dev_priv->res_lru[res->func->res_type]);\n\tspin_unlock(&dev_priv->resource_lock);\n}",
        "code_after_change": "void vmw_resource_unreserve(struct vmw_resource *res,\n\t\t\t    bool dirty_set,\n\t\t\t    bool dirty,\n\t\t\t    bool switch_guest_memory,\n\t\t\t    struct vmw_bo *new_guest_memory_bo,\n\t\t\t    unsigned long new_guest_memory_offset)\n{\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\n\tif (!list_empty(&res->lru_head))\n\t\treturn;\n\n\tif (switch_guest_memory && new_guest_memory_bo != res->guest_memory_bo) {\n\t\tif (res->guest_memory_bo) {\n\t\t\tvmw_resource_mob_detach(res);\n\t\t\tif (res->coherent)\n\t\t\t\tvmw_bo_dirty_release(res->guest_memory_bo);\n\t\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n\t\t}\n\n\t\tif (new_guest_memory_bo) {\n\t\t\tres->guest_memory_bo = vmw_user_bo_ref(new_guest_memory_bo);\n\n\t\t\t/*\n\t\t\t * The validation code should already have added a\n\t\t\t * dirty tracker here.\n\t\t\t */\n\t\t\tWARN_ON(res->coherent && !new_guest_memory_bo->dirty);\n\n\t\t\tvmw_resource_mob_attach(res);\n\t\t} else {\n\t\t\tres->guest_memory_bo = NULL;\n\t\t}\n\t} else if (switch_guest_memory && res->coherent) {\n\t\tvmw_bo_dirty_release(res->guest_memory_bo);\n\t}\n\n\tif (switch_guest_memory)\n\t\tres->guest_memory_offset = new_guest_memory_offset;\n\n\tif (dirty_set)\n\t\tres->res_dirty = dirty;\n\n\tif (!res->func->may_evict || res->id == -1 || res->pin_count)\n\t\treturn;\n\n\tspin_lock(&dev_priv->resource_lock);\n\tlist_add_tail(&res->lru_head,\n\t\t      &res->dev_priv->res_lru[res->func->res_type]);\n\tspin_unlock(&dev_priv->resource_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,11 +15,11 @@\n \t\t\tvmw_resource_mob_detach(res);\n \t\t\tif (res->coherent)\n \t\t\t\tvmw_bo_dirty_release(res->guest_memory_bo);\n-\t\t\tvmw_bo_unreference(&res->guest_memory_bo);\n+\t\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n \t\t}\n \n \t\tif (new_guest_memory_bo) {\n-\t\t\tres->guest_memory_bo = vmw_bo_reference(new_guest_memory_bo);\n+\t\t\tres->guest_memory_bo = vmw_user_bo_ref(new_guest_memory_bo);\n \n \t\t\t/*\n \t\t\t * The validation code should already have added a",
        "function_modified_lines": {
            "added": [
                "\t\t\tvmw_user_bo_unref(&res->guest_memory_bo);",
                "\t\t\tres->guest_memory_bo = vmw_user_bo_ref(new_guest_memory_bo);"
            ],
            "deleted": [
                "\t\t\tvmw_bo_unreference(&res->guest_memory_bo);",
                "\t\t\tres->guest_memory_bo = vmw_bo_reference(new_guest_memory_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4282
    },
    {
        "cve_id": "CVE-2019-25044",
        "code_before_change": "int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct elevator_queue *eq;\n\tunsigned int i;\n\tint ret;\n\n\tif (!e) {\n\t\tq->elevator = NULL;\n\t\tq->nr_requests = q->tag_set->queue_depth;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Default to double of smaller one between hw queue_depth and 128,\n\t * since we don't split into sync/async like the old code did.\n\t * Additionally, this is a per-hw queue depth.\n\t */\n\tq->nr_requests = 2 * min_t(unsigned int, q->tag_set->queue_depth,\n\t\t\t\t   BLKDEV_MAX_RQ);\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tret = blk_mq_sched_alloc_tags(q, hctx, i);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\tret = e->ops.init_sched(q, e);\n\tif (ret)\n\t\tgoto err;\n\n\tblk_mq_debugfs_register_sched(q);\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (e->ops.init_hctx) {\n\t\t\tret = e->ops.init_hctx(hctx, i);\n\t\t\tif (ret) {\n\t\t\t\teq = q->elevator;\n\t\t\t\tblk_mq_exit_sched(q, eq);\n\t\t\t\tkobject_put(&eq->kobj);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tblk_mq_debugfs_register_sched_hctx(q, hctx);\n\t}\n\n\treturn 0;\n\nerr:\n\tblk_mq_sched_tags_teardown(q);\n\tq->elevator = NULL;\n\treturn ret;\n}",
        "code_after_change": "int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct elevator_queue *eq;\n\tunsigned int i;\n\tint ret;\n\n\tif (!e) {\n\t\tq->elevator = NULL;\n\t\tq->nr_requests = q->tag_set->queue_depth;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Default to double of smaller one between hw queue_depth and 128,\n\t * since we don't split into sync/async like the old code did.\n\t * Additionally, this is a per-hw queue depth.\n\t */\n\tq->nr_requests = 2 * min_t(unsigned int, q->tag_set->queue_depth,\n\t\t\t\t   BLKDEV_MAX_RQ);\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tret = blk_mq_sched_alloc_tags(q, hctx, i);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\tret = e->ops.init_sched(q, e);\n\tif (ret)\n\t\tgoto err;\n\n\tblk_mq_debugfs_register_sched(q);\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (e->ops.init_hctx) {\n\t\t\tret = e->ops.init_hctx(hctx, i);\n\t\t\tif (ret) {\n\t\t\t\teq = q->elevator;\n\t\t\t\tblk_mq_sched_free_requests(q);\n\t\t\t\tblk_mq_exit_sched(q, eq);\n\t\t\t\tkobject_put(&eq->kobj);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tblk_mq_debugfs_register_sched_hctx(q, hctx);\n\t}\n\n\treturn 0;\n\nerr:\n\tblk_mq_sched_free_requests(q);\n\tblk_mq_sched_tags_teardown(q);\n\tq->elevator = NULL;\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -36,6 +36,7 @@\n \t\t\tret = e->ops.init_hctx(hctx, i);\n \t\t\tif (ret) {\n \t\t\t\teq = q->elevator;\n+\t\t\t\tblk_mq_sched_free_requests(q);\n \t\t\t\tblk_mq_exit_sched(q, eq);\n \t\t\t\tkobject_put(&eq->kobj);\n \t\t\t\treturn ret;\n@@ -47,6 +48,7 @@\n \treturn 0;\n \n err:\n+\tblk_mq_sched_free_requests(q);\n \tblk_mq_sched_tags_teardown(q);\n \tq->elevator = NULL;\n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tblk_mq_sched_free_requests(q);",
                "\tblk_mq_sched_free_requests(q);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The block subsystem in the Linux kernel before 5.2 has a use-after-free that can lead to arbitrary code execution in the kernel context and privilege escalation, aka CID-c3e2219216c9. This is related to blk_mq_free_rqs and blk_cleanup_queue.",
        "id": 2300
    },
    {
        "cve_id": "CVE-2020-36387",
        "code_before_change": "static void io_req_task_submit(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\n\t__io_req_task_submit(req);\n}",
        "code_after_change": "static void io_req_task_submit(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t__io_req_task_submit(req);\n\tpercpu_ref_put(&ctx->refs);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,8 @@\n static void io_req_task_submit(struct callback_head *cb)\n {\n \tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n+\tstruct io_ring_ctx *ctx = req->ctx;\n \n \t__io_req_task_submit(req);\n+\tpercpu_ref_put(&ctx->refs);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct io_ring_ctx *ctx = req->ctx;",
                "\tpercpu_ref_put(&ctx->refs);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.8.2. fs/io_uring.c has a use-after-free related to io_async_task_func and ctx reference holding, aka CID-6d816e088c35.",
        "id": 2761
    },
    {
        "cve_id": "CVE-2014-0100",
        "code_before_change": "static struct inet_frag_queue *inet_frag_intern(struct netns_frags *nf,\n\t\tstruct inet_frag_queue *qp_in, struct inet_frags *f,\n\t\tvoid *arg)\n{\n\tstruct inet_frag_bucket *hb;\n\tstruct inet_frag_queue *qp;\n\tunsigned int hash;\n\n\tread_lock(&f->lock); /* Protects against hash rebuild */\n\t/*\n\t * While we stayed w/o the lock other CPU could update\n\t * the rnd seed, so we need to re-calculate the hash\n\t * chain. Fortunatelly the qp_in can be used to get one.\n\t */\n\thash = f->hashfn(qp_in);\n\thb = &f->hash[hash];\n\tspin_lock(&hb->chain_lock);\n\n#ifdef CONFIG_SMP\n\t/* With SMP race we have to recheck hash table, because\n\t * such entry could be created on other cpu, while we\n\t * released the hash bucket lock.\n\t */\n\thlist_for_each_entry(qp, &hb->chain, list) {\n\t\tif (qp->net == nf && f->match(qp, arg)) {\n\t\t\tatomic_inc(&qp->refcnt);\n\t\t\tspin_unlock(&hb->chain_lock);\n\t\t\tread_unlock(&f->lock);\n\t\t\tqp_in->last_in |= INET_FRAG_COMPLETE;\n\t\t\tinet_frag_put(qp_in, f);\n\t\t\treturn qp;\n\t\t}\n\t}\n#endif\n\tqp = qp_in;\n\tif (!mod_timer(&qp->timer, jiffies + nf->timeout))\n\t\tatomic_inc(&qp->refcnt);\n\n\tatomic_inc(&qp->refcnt);\n\thlist_add_head(&qp->list, &hb->chain);\n\tspin_unlock(&hb->chain_lock);\n\tread_unlock(&f->lock);\n\tinet_frag_lru_add(nf, qp);\n\treturn qp;\n}",
        "code_after_change": "static struct inet_frag_queue *inet_frag_intern(struct netns_frags *nf,\n\t\tstruct inet_frag_queue *qp_in, struct inet_frags *f,\n\t\tvoid *arg)\n{\n\tstruct inet_frag_bucket *hb;\n\tstruct inet_frag_queue *qp;\n\tunsigned int hash;\n\n\tread_lock(&f->lock); /* Protects against hash rebuild */\n\t/*\n\t * While we stayed w/o the lock other CPU could update\n\t * the rnd seed, so we need to re-calculate the hash\n\t * chain. Fortunatelly the qp_in can be used to get one.\n\t */\n\thash = f->hashfn(qp_in);\n\thb = &f->hash[hash];\n\tspin_lock(&hb->chain_lock);\n\n#ifdef CONFIG_SMP\n\t/* With SMP race we have to recheck hash table, because\n\t * such entry could be created on other cpu, while we\n\t * released the hash bucket lock.\n\t */\n\thlist_for_each_entry(qp, &hb->chain, list) {\n\t\tif (qp->net == nf && f->match(qp, arg)) {\n\t\t\tatomic_inc(&qp->refcnt);\n\t\t\tspin_unlock(&hb->chain_lock);\n\t\t\tread_unlock(&f->lock);\n\t\t\tqp_in->last_in |= INET_FRAG_COMPLETE;\n\t\t\tinet_frag_put(qp_in, f);\n\t\t\treturn qp;\n\t\t}\n\t}\n#endif\n\tqp = qp_in;\n\tif (!mod_timer(&qp->timer, jiffies + nf->timeout))\n\t\tatomic_inc(&qp->refcnt);\n\n\tatomic_inc(&qp->refcnt);\n\thlist_add_head(&qp->list, &hb->chain);\n\tinet_frag_lru_add(nf, qp);\n\tspin_unlock(&hb->chain_lock);\n\tread_unlock(&f->lock);\n\n\treturn qp;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,8 +38,9 @@\n \n \tatomic_inc(&qp->refcnt);\n \thlist_add_head(&qp->list, &hb->chain);\n+\tinet_frag_lru_add(nf, qp);\n \tspin_unlock(&hb->chain_lock);\n \tread_unlock(&f->lock);\n-\tinet_frag_lru_add(nf, qp);\n+\n \treturn qp;\n }",
        "function_modified_lines": {
            "added": [
                "\tinet_frag_lru_add(nf, qp);",
                ""
            ],
            "deleted": [
                "\tinet_frag_lru_add(nf, qp);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the inet_frag_intern function in net/ipv4/inet_fragment.c in the Linux kernel through 3.13.6 allows remote attackers to cause a denial of service (use-after-free error) or possibly have unspecified other impact via a large series of fragmented ICMP Echo Request packets to a system with a heavy CPU load.",
        "id": 428
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static struct l2cap_chan *l2cap_global_fixed_chan(struct l2cap_chan *c,\n\t\t\t\t\t\t  struct hci_conn *hcon)\n{\n\tu8 src_type = bdaddr_src_type(hcon);\n\n\tread_lock(&chan_list_lock);\n\n\tif (c)\n\t\tc = list_next_entry(c, global_l);\n\telse\n\t\tc = list_entry(chan_list.next, typeof(*c), global_l);\n\n\tlist_for_each_entry_from(c, &chan_list, global_l) {\n\t\tif (c->chan_type != L2CAP_CHAN_FIXED)\n\t\t\tcontinue;\n\t\tif (c->state != BT_LISTEN)\n\t\t\tcontinue;\n\t\tif (bacmp(&c->src, &hcon->src) && bacmp(&c->src, BDADDR_ANY))\n\t\t\tcontinue;\n\t\tif (src_type != c->src_type)\n\t\t\tcontinue;\n\n\t\tl2cap_chan_hold(c);\n\t\tread_unlock(&chan_list_lock);\n\t\treturn c;\n\t}\n\n\tread_unlock(&chan_list_lock);\n\n\treturn NULL;\n}",
        "code_after_change": "static struct l2cap_chan *l2cap_global_fixed_chan(struct l2cap_chan *c,\n\t\t\t\t\t\t  struct hci_conn *hcon)\n{\n\tu8 src_type = bdaddr_src_type(hcon);\n\n\tread_lock(&chan_list_lock);\n\n\tif (c)\n\t\tc = list_next_entry(c, global_l);\n\telse\n\t\tc = list_entry(chan_list.next, typeof(*c), global_l);\n\n\tlist_for_each_entry_from(c, &chan_list, global_l) {\n\t\tif (c->chan_type != L2CAP_CHAN_FIXED)\n\t\t\tcontinue;\n\t\tif (c->state != BT_LISTEN)\n\t\t\tcontinue;\n\t\tif (bacmp(&c->src, &hcon->src) && bacmp(&c->src, BDADDR_ANY))\n\t\t\tcontinue;\n\t\tif (src_type != c->src_type)\n\t\t\tcontinue;\n\n\t\tc = l2cap_chan_hold_unless_zero(c);\n\t\tread_unlock(&chan_list_lock);\n\t\treturn c;\n\t}\n\n\tread_unlock(&chan_list_lock);\n\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,7 +20,7 @@\n \t\tif (src_type != c->src_type)\n \t\t\tcontinue;\n \n-\t\tl2cap_chan_hold(c);\n+\t\tc = l2cap_chan_hold_unless_zero(c);\n \t\tread_unlock(&chan_list_lock);\n \t\treturn c;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tc = l2cap_chan_hold_unless_zero(c);"
            ],
            "deleted": [
                "\t\tl2cap_chan_hold(c);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3393
    },
    {
        "cve_id": "CVE-2022-3424",
        "code_before_change": "int gru_handle_user_call_os(unsigned long cb)\n{\n\tstruct gru_tlb_fault_handle *tfh;\n\tstruct gru_thread_state *gts;\n\tvoid *cbk;\n\tint ucbnum, cbrnum, ret = -EINVAL;\n\n\tSTAT(call_os);\n\n\t/* sanity check the cb pointer */\n\tucbnum = get_cb_number((void *)cb);\n\tif ((cb & (GRU_HANDLE_STRIDE - 1)) || ucbnum >= GRU_NUM_CB)\n\t\treturn -EINVAL;\n\n\tgts = gru_find_lock_gts(cb);\n\tif (!gts)\n\t\treturn -EINVAL;\n\tgru_dbg(grudev, \"address 0x%lx, gid %d, gts 0x%p\\n\", cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\n\n\tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE)\n\t\tgoto exit;\n\n\tgru_check_context_placement(gts);\n\n\t/*\n\t * CCH may contain stale data if ts_force_cch_reload is set.\n\t */\n\tif (gts->ts_gru && gts->ts_force_cch_reload) {\n\t\tgts->ts_force_cch_reload = 0;\n\t\tgru_update_cch(gts);\n\t}\n\n\tret = -EAGAIN;\n\tcbrnum = thread_cbr_number(gts, ucbnum);\n\tif (gts->ts_gru) {\n\t\ttfh = get_tfh_by_index(gts->ts_gru, cbrnum);\n\t\tcbk = get_gseg_base_address_cb(gts->ts_gru->gs_gru_base_vaddr,\n\t\t\t\tgts->ts_ctxnum, ucbnum);\n\t\tret = gru_user_dropin(gts, tfh, cbk);\n\t}\nexit:\n\tgru_unlock_gts(gts);\n\treturn ret;\n}",
        "code_after_change": "int gru_handle_user_call_os(unsigned long cb)\n{\n\tstruct gru_tlb_fault_handle *tfh;\n\tstruct gru_thread_state *gts;\n\tvoid *cbk;\n\tint ucbnum, cbrnum, ret = -EINVAL;\n\n\tSTAT(call_os);\n\n\t/* sanity check the cb pointer */\n\tucbnum = get_cb_number((void *)cb);\n\tif ((cb & (GRU_HANDLE_STRIDE - 1)) || ucbnum >= GRU_NUM_CB)\n\t\treturn -EINVAL;\n\nagain:\n\tgts = gru_find_lock_gts(cb);\n\tif (!gts)\n\t\treturn -EINVAL;\n\tgru_dbg(grudev, \"address 0x%lx, gid %d, gts 0x%p\\n\", cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\n\n\tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE)\n\t\tgoto exit;\n\n\tif (gru_check_context_placement(gts)) {\n\t\tgru_unlock_gts(gts);\n\t\tgru_unload_context(gts, 1);\n\t\tgoto again;\n\t}\n\n\t/*\n\t * CCH may contain stale data if ts_force_cch_reload is set.\n\t */\n\tif (gts->ts_gru && gts->ts_force_cch_reload) {\n\t\tgts->ts_force_cch_reload = 0;\n\t\tgru_update_cch(gts);\n\t}\n\n\tret = -EAGAIN;\n\tcbrnum = thread_cbr_number(gts, ucbnum);\n\tif (gts->ts_gru) {\n\t\ttfh = get_tfh_by_index(gts->ts_gru, cbrnum);\n\t\tcbk = get_gseg_base_address_cb(gts->ts_gru->gs_gru_base_vaddr,\n\t\t\t\tgts->ts_ctxnum, ucbnum);\n\t\tret = gru_user_dropin(gts, tfh, cbk);\n\t}\nexit:\n\tgru_unlock_gts(gts);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,7 @@\n \tif ((cb & (GRU_HANDLE_STRIDE - 1)) || ucbnum >= GRU_NUM_CB)\n \t\treturn -EINVAL;\n \n+again:\n \tgts = gru_find_lock_gts(cb);\n \tif (!gts)\n \t\treturn -EINVAL;\n@@ -20,7 +21,11 @@\n \tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE)\n \t\tgoto exit;\n \n-\tgru_check_context_placement(gts);\n+\tif (gru_check_context_placement(gts)) {\n+\t\tgru_unlock_gts(gts);\n+\t\tgru_unload_context(gts, 1);\n+\t\tgoto again;\n+\t}\n \n \t/*\n \t * CCH may contain stale data if ts_force_cch_reload is set.",
        "function_modified_lines": {
            "added": [
                "again:",
                "\tif (gru_check_context_placement(gts)) {",
                "\t\tgru_unlock_gts(gts);",
                "\t\tgru_unload_context(gts, 1);",
                "\t\tgoto again;",
                "\t}"
            ],
            "deleted": [
                "\tgru_check_context_placement(gts);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s SGI GRU driver in the way the first gru_file_unlocked_ioctl function is called by the user, where a fail pass occurs in the gru_check_chiplet_assignment function. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3595
    },
    {
        "cve_id": "CVE-2021-43057",
        "code_before_change": "static int smk_curacc_on_task(struct task_struct *p, int access,\n\t\t\t\tconst char *caller)\n{\n\tstruct smk_audit_info ad;\n\tstruct smack_known *skp = smk_of_task_struct_subj(p);\n\tint rc;\n\n\tsmk_ad_init(&ad, caller, LSM_AUDIT_DATA_TASK);\n\tsmk_ad_setfield_u_tsk(&ad, p);\n\trc = smk_curacc(skp, access, &ad);\n\trc = smk_bu_task(p, access, rc);\n\treturn rc;\n}",
        "code_after_change": "static int smk_curacc_on_task(struct task_struct *p, int access,\n\t\t\t\tconst char *caller)\n{\n\tstruct smk_audit_info ad;\n\tstruct smack_known *skp = smk_of_task_struct_obj(p);\n\tint rc;\n\n\tsmk_ad_init(&ad, caller, LSM_AUDIT_DATA_TASK);\n\tsmk_ad_setfield_u_tsk(&ad, p);\n\trc = smk_curacc(skp, access, &ad);\n\trc = smk_bu_task(p, access, rc);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n \t\t\t\tconst char *caller)\n {\n \tstruct smk_audit_info ad;\n-\tstruct smack_known *skp = smk_of_task_struct_subj(p);\n+\tstruct smack_known *skp = smk_of_task_struct_obj(p);\n \tint rc;\n \n \tsmk_ad_init(&ad, caller, LSM_AUDIT_DATA_TASK);",
        "function_modified_lines": {
            "added": [
                "\tstruct smack_known *skp = smk_of_task_struct_obj(p);"
            ],
            "deleted": [
                "\tstruct smack_known *skp = smk_of_task_struct_subj(p);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.14.8. A use-after-free in selinux_ptrace_traceme (aka the SELinux handler for PTRACE_TRACEME) could be used by local attackers to cause memory corruption and escalate privileges, aka CID-a3727a8bac0a. This occurs because of an attempt to access the subjective credentials of another task.",
        "id": 3162
    },
    {
        "cve_id": "CVE-2019-19377",
        "code_before_change": "int btree_write_cache_pages(struct address_space *mapping,\n\t\t\t\t   struct writeback_control *wbc)\n{\n\tstruct extent_buffer *eb, *prev_eb = NULL;\n\tstruct extent_page_data epd = {\n\t\t.bio = NULL,\n\t\t.extent_locked = 0,\n\t\t.sync_io = wbc->sync_mode == WB_SYNC_ALL,\n\t};\n\tint ret = 0;\n\tint done = 0;\n\tint nr_to_write_done = 0;\n\tstruct pagevec pvec;\n\tint nr_pages;\n\tpgoff_t index;\n\tpgoff_t end;\t\t/* Inclusive */\n\tint scanned = 0;\n\txa_mark_t tag;\n\n\tpagevec_init(&pvec);\n\tif (wbc->range_cyclic) {\n\t\tindex = mapping->writeback_index; /* Start from prev offset */\n\t\tend = -1;\n\t\t/*\n\t\t * Start from the beginning does not need to cycle over the\n\t\t * range, mark it as scanned.\n\t\t */\n\t\tscanned = (index == 0);\n\t} else {\n\t\tindex = wbc->range_start >> PAGE_SHIFT;\n\t\tend = wbc->range_end >> PAGE_SHIFT;\n\t\tscanned = 1;\n\t}\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\nretry:\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\ttag_pages_for_writeback(mapping, index, end);\n\twhile (!done && !nr_to_write_done && (index <= end) &&\n\t       (nr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index, end,\n\t\t\ttag))) {\n\t\tunsigned i;\n\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tif (!PagePrivate(page))\n\t\t\t\tcontinue;\n\n\t\t\tspin_lock(&mapping->private_lock);\n\t\t\tif (!PagePrivate(page)) {\n\t\t\t\tspin_unlock(&mapping->private_lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\teb = (struct extent_buffer *)page->private;\n\n\t\t\t/*\n\t\t\t * Shouldn't happen and normally this would be a BUG_ON\n\t\t\t * but no sense in crashing the users box for something\n\t\t\t * we can survive anyway.\n\t\t\t */\n\t\t\tif (WARN_ON(!eb)) {\n\t\t\t\tspin_unlock(&mapping->private_lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (eb == prev_eb) {\n\t\t\t\tspin_unlock(&mapping->private_lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = atomic_inc_not_zero(&eb->refs);\n\t\t\tspin_unlock(&mapping->private_lock);\n\t\t\tif (!ret)\n\t\t\t\tcontinue;\n\n\t\t\tprev_eb = eb;\n\t\t\tret = lock_extent_buffer_for_io(eb, &epd);\n\t\t\tif (!ret) {\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\tcontinue;\n\t\t\t} else if (ret < 0) {\n\t\t\t\tdone = 1;\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tret = write_one_eb(eb, wbc, &epd);\n\t\t\tif (ret) {\n\t\t\t\tdone = 1;\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tfree_extent_buffer(eb);\n\n\t\t\t/*\n\t\t\t * the filesystem may choose to bump up nr_to_write.\n\t\t\t * We have to make sure to honor the new nr_to_write\n\t\t\t * at any time\n\t\t\t */\n\t\t\tnr_to_write_done = wbc->nr_to_write <= 0;\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tcond_resched();\n\t}\n\tif (!scanned && !done) {\n\t\t/*\n\t\t * We hit the last page and there is more work to be done: wrap\n\t\t * back to the start of the file\n\t\t */\n\t\tscanned = 1;\n\t\tindex = 0;\n\t\tgoto retry;\n\t}\n\tASSERT(ret <= 0);\n\tif (ret < 0) {\n\t\tend_write_bio(&epd, ret);\n\t\treturn ret;\n\t}\n\tret = flush_write_bio(&epd);\n\treturn ret;\n}",
        "code_after_change": "int btree_write_cache_pages(struct address_space *mapping,\n\t\t\t\t   struct writeback_control *wbc)\n{\n\tstruct extent_buffer *eb, *prev_eb = NULL;\n\tstruct extent_page_data epd = {\n\t\t.bio = NULL,\n\t\t.extent_locked = 0,\n\t\t.sync_io = wbc->sync_mode == WB_SYNC_ALL,\n\t};\n\tstruct btrfs_fs_info *fs_info = BTRFS_I(mapping->host)->root->fs_info;\n\tint ret = 0;\n\tint done = 0;\n\tint nr_to_write_done = 0;\n\tstruct pagevec pvec;\n\tint nr_pages;\n\tpgoff_t index;\n\tpgoff_t end;\t\t/* Inclusive */\n\tint scanned = 0;\n\txa_mark_t tag;\n\n\tpagevec_init(&pvec);\n\tif (wbc->range_cyclic) {\n\t\tindex = mapping->writeback_index; /* Start from prev offset */\n\t\tend = -1;\n\t\t/*\n\t\t * Start from the beginning does not need to cycle over the\n\t\t * range, mark it as scanned.\n\t\t */\n\t\tscanned = (index == 0);\n\t} else {\n\t\tindex = wbc->range_start >> PAGE_SHIFT;\n\t\tend = wbc->range_end >> PAGE_SHIFT;\n\t\tscanned = 1;\n\t}\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\nretry:\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\ttag_pages_for_writeback(mapping, index, end);\n\twhile (!done && !nr_to_write_done && (index <= end) &&\n\t       (nr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index, end,\n\t\t\ttag))) {\n\t\tunsigned i;\n\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tif (!PagePrivate(page))\n\t\t\t\tcontinue;\n\n\t\t\tspin_lock(&mapping->private_lock);\n\t\t\tif (!PagePrivate(page)) {\n\t\t\t\tspin_unlock(&mapping->private_lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\teb = (struct extent_buffer *)page->private;\n\n\t\t\t/*\n\t\t\t * Shouldn't happen and normally this would be a BUG_ON\n\t\t\t * but no sense in crashing the users box for something\n\t\t\t * we can survive anyway.\n\t\t\t */\n\t\t\tif (WARN_ON(!eb)) {\n\t\t\t\tspin_unlock(&mapping->private_lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (eb == prev_eb) {\n\t\t\t\tspin_unlock(&mapping->private_lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = atomic_inc_not_zero(&eb->refs);\n\t\t\tspin_unlock(&mapping->private_lock);\n\t\t\tif (!ret)\n\t\t\t\tcontinue;\n\n\t\t\tprev_eb = eb;\n\t\t\tret = lock_extent_buffer_for_io(eb, &epd);\n\t\t\tif (!ret) {\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\tcontinue;\n\t\t\t} else if (ret < 0) {\n\t\t\t\tdone = 1;\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tret = write_one_eb(eb, wbc, &epd);\n\t\t\tif (ret) {\n\t\t\t\tdone = 1;\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tfree_extent_buffer(eb);\n\n\t\t\t/*\n\t\t\t * the filesystem may choose to bump up nr_to_write.\n\t\t\t * We have to make sure to honor the new nr_to_write\n\t\t\t * at any time\n\t\t\t */\n\t\t\tnr_to_write_done = wbc->nr_to_write <= 0;\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tcond_resched();\n\t}\n\tif (!scanned && !done) {\n\t\t/*\n\t\t * We hit the last page and there is more work to be done: wrap\n\t\t * back to the start of the file\n\t\t */\n\t\tscanned = 1;\n\t\tindex = 0;\n\t\tgoto retry;\n\t}\n\tASSERT(ret <= 0);\n\tif (ret < 0) {\n\t\tend_write_bio(&epd, ret);\n\t\treturn ret;\n\t}\n\t/*\n\t * If something went wrong, don't allow any metadata write bio to be\n\t * submitted.\n\t *\n\t * This would prevent use-after-free if we had dirty pages not\n\t * cleaned up, which can still happen by fuzzed images.\n\t *\n\t * - Bad extent tree\n\t *   Allowing existing tree block to be allocated for other trees.\n\t *\n\t * - Log tree operations\n\t *   Exiting tree blocks get allocated to log tree, bumps its\n\t *   generation, then get cleaned in tree re-balance.\n\t *   Such tree block will not be written back, since it's clean,\n\t *   thus no WRITTEN flag set.\n\t *   And after log writes back, this tree block is not traced by\n\t *   any dirty extent_io_tree.\n\t *\n\t * - Offending tree block gets re-dirtied from its original owner\n\t *   Since it has bumped generation, no WRITTEN flag, it can be\n\t *   reused without COWing. This tree block will not be traced\n\t *   by btrfs_transaction::dirty_pages.\n\t *\n\t *   Now such dirty tree block will not be cleaned by any dirty\n\t *   extent io tree. Thus we don't want to submit such wild eb\n\t *   if the fs already has error.\n\t */\n\tif (!test_bit(BTRFS_FS_STATE_ERROR, &fs_info->fs_state)) {\n\t\tret = flush_write_bio(&epd);\n\t} else {\n\t\tret = -EUCLEAN;\n\t\tend_write_bio(&epd, ret);\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \t\t.extent_locked = 0,\n \t\t.sync_io = wbc->sync_mode == WB_SYNC_ALL,\n \t};\n+\tstruct btrfs_fs_info *fs_info = BTRFS_I(mapping->host)->root->fs_info;\n \tint ret = 0;\n \tint done = 0;\n \tint nr_to_write_done = 0;\n@@ -120,6 +121,38 @@\n \t\tend_write_bio(&epd, ret);\n \t\treturn ret;\n \t}\n-\tret = flush_write_bio(&epd);\n+\t/*\n+\t * If something went wrong, don't allow any metadata write bio to be\n+\t * submitted.\n+\t *\n+\t * This would prevent use-after-free if we had dirty pages not\n+\t * cleaned up, which can still happen by fuzzed images.\n+\t *\n+\t * - Bad extent tree\n+\t *   Allowing existing tree block to be allocated for other trees.\n+\t *\n+\t * - Log tree operations\n+\t *   Exiting tree blocks get allocated to log tree, bumps its\n+\t *   generation, then get cleaned in tree re-balance.\n+\t *   Such tree block will not be written back, since it's clean,\n+\t *   thus no WRITTEN flag set.\n+\t *   And after log writes back, this tree block is not traced by\n+\t *   any dirty extent_io_tree.\n+\t *\n+\t * - Offending tree block gets re-dirtied from its original owner\n+\t *   Since it has bumped generation, no WRITTEN flag, it can be\n+\t *   reused without COWing. This tree block will not be traced\n+\t *   by btrfs_transaction::dirty_pages.\n+\t *\n+\t *   Now such dirty tree block will not be cleaned by any dirty\n+\t *   extent io tree. Thus we don't want to submit such wild eb\n+\t *   if the fs already has error.\n+\t */\n+\tif (!test_bit(BTRFS_FS_STATE_ERROR, &fs_info->fs_state)) {\n+\t\tret = flush_write_bio(&epd);\n+\t} else {\n+\t\tret = -EUCLEAN;\n+\t\tend_write_bio(&epd, ret);\n+\t}\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct btrfs_fs_info *fs_info = BTRFS_I(mapping->host)->root->fs_info;",
                "\t/*",
                "\t * If something went wrong, don't allow any metadata write bio to be",
                "\t * submitted.",
                "\t *",
                "\t * This would prevent use-after-free if we had dirty pages not",
                "\t * cleaned up, which can still happen by fuzzed images.",
                "\t *",
                "\t * - Bad extent tree",
                "\t *   Allowing existing tree block to be allocated for other trees.",
                "\t *",
                "\t * - Log tree operations",
                "\t *   Exiting tree blocks get allocated to log tree, bumps its",
                "\t *   generation, then get cleaned in tree re-balance.",
                "\t *   Such tree block will not be written back, since it's clean,",
                "\t *   thus no WRITTEN flag set.",
                "\t *   And after log writes back, this tree block is not traced by",
                "\t *   any dirty extent_io_tree.",
                "\t *",
                "\t * - Offending tree block gets re-dirtied from its original owner",
                "\t *   Since it has bumped generation, no WRITTEN flag, it can be",
                "\t *   reused without COWing. This tree block will not be traced",
                "\t *   by btrfs_transaction::dirty_pages.",
                "\t *",
                "\t *   Now such dirty tree block will not be cleaned by any dirty",
                "\t *   extent io tree. Thus we don't want to submit such wild eb",
                "\t *   if the fs already has error.",
                "\t */",
                "\tif (!test_bit(BTRFS_FS_STATE_ERROR, &fs_info->fs_state)) {",
                "\t\tret = flush_write_bio(&epd);",
                "\t} else {",
                "\t\tret = -EUCLEAN;",
                "\t\tend_write_bio(&epd, ret);",
                "\t}"
            ],
            "deleted": [
                "\tret = flush_write_bio(&epd);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.0.21, mounting a crafted btrfs filesystem image, performing some operations, and unmounting can lead to a use-after-free in btrfs_queue_work in fs/btrfs/async-thread.c.",
        "id": 2193
    },
    {
        "cve_id": "CVE-2023-1281",
        "code_before_change": "static int\ntcindex_set_parms(struct net *net, struct tcf_proto *tp, unsigned long base,\n\t\t  u32 handle, struct tcindex_data *p,\n\t\t  struct tcindex_filter_result *r, struct nlattr **tb,\n\t\t  struct nlattr *est, u32 flags, struct netlink_ext_ack *extack)\n{\n\tstruct tcindex_filter_result new_filter_result;\n\tstruct tcindex_data *cp = NULL, *oldp;\n\tstruct tcindex_filter *f = NULL; /* make gcc behave */\n\tstruct tcf_result cr = {};\n\tint err, balloc = 0;\n\tstruct tcf_exts e;\n\n\terr = tcf_exts_init(&e, net, TCA_TCINDEX_ACT, TCA_TCINDEX_POLICE);\n\tif (err < 0)\n\t\treturn err;\n\terr = tcf_exts_validate(net, tp, tb, est, &e, flags, extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENOMEM;\n\t/* tcindex_data attributes must look atomic to classifier/lookup so\n\t * allocate new tcindex data and RCU assign it onto root. Keeping\n\t * perfect hash and hash pointers from old data.\n\t */\n\tcp = kzalloc(sizeof(*cp), GFP_KERNEL);\n\tif (!cp)\n\t\tgoto errout;\n\n\tcp->mask = p->mask;\n\tcp->shift = p->shift;\n\tcp->hash = p->hash;\n\tcp->alloc_hash = p->alloc_hash;\n\tcp->fall_through = p->fall_through;\n\tcp->tp = tp;\n\trefcount_set(&cp->refcnt, 1); /* Paired with tcindex_destroy_work() */\n\n\tif (tb[TCA_TCINDEX_HASH])\n\t\tcp->hash = nla_get_u32(tb[TCA_TCINDEX_HASH]);\n\n\tif (tb[TCA_TCINDEX_MASK])\n\t\tcp->mask = nla_get_u16(tb[TCA_TCINDEX_MASK]);\n\n\tif (tb[TCA_TCINDEX_SHIFT]) {\n\t\tcp->shift = nla_get_u32(tb[TCA_TCINDEX_SHIFT]);\n\t\tif (cp->shift > 16) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout;\n\t\t}\n\t}\n\tif (!cp->hash) {\n\t\t/* Hash not specified, use perfect hash if the upper limit\n\t\t * of the hashing index is below the threshold.\n\t\t */\n\t\tif ((cp->mask >> cp->shift) < PERFECT_HASH_THRESHOLD)\n\t\t\tcp->hash = (cp->mask >> cp->shift) + 1;\n\t\telse\n\t\t\tcp->hash = DEFAULT_HASH_SIZE;\n\t}\n\n\tif (p->perfect) {\n\t\tint i;\n\n\t\tif (tcindex_alloc_perfect_hash(net, cp) < 0)\n\t\t\tgoto errout;\n\t\tcp->alloc_hash = cp->hash;\n\t\tfor (i = 0; i < min(cp->hash, p->hash); i++)\n\t\t\tcp->perfect[i].res = p->perfect[i].res;\n\t\tballoc = 1;\n\t}\n\tcp->h = p->h;\n\n\terr = tcindex_filter_result_init(&new_filter_result, cp, net);\n\tif (err < 0)\n\t\tgoto errout_alloc;\n\tif (r)\n\t\tcr = r->res;\n\n\terr = -EBUSY;\n\n\t/* Hash already allocated, make sure that we still meet the\n\t * requirements for the allocated hash.\n\t */\n\tif (cp->perfect) {\n\t\tif (!valid_perfect_hash(cp) ||\n\t\t    cp->hash > cp->alloc_hash)\n\t\t\tgoto errout_alloc;\n\t} else if (cp->h && cp->hash != cp->alloc_hash) {\n\t\tgoto errout_alloc;\n\t}\n\n\terr = -EINVAL;\n\tif (tb[TCA_TCINDEX_FALL_THROUGH])\n\t\tcp->fall_through = nla_get_u32(tb[TCA_TCINDEX_FALL_THROUGH]);\n\n\tif (!cp->perfect && !cp->h)\n\t\tcp->alloc_hash = cp->hash;\n\n\t/* Note: this could be as restrictive as if (handle & ~(mask >> shift))\n\t * but then, we'd fail handles that may become valid after some future\n\t * mask change. While this is extremely unlikely to ever matter,\n\t * the check below is safer (and also more backwards-compatible).\n\t */\n\tif (cp->perfect || valid_perfect_hash(cp))\n\t\tif (handle >= cp->alloc_hash)\n\t\t\tgoto errout_alloc;\n\n\n\terr = -ENOMEM;\n\tif (!cp->perfect && !cp->h) {\n\t\tif (valid_perfect_hash(cp)) {\n\t\t\tif (tcindex_alloc_perfect_hash(net, cp) < 0)\n\t\t\t\tgoto errout_alloc;\n\t\t\tballoc = 1;\n\t\t} else {\n\t\t\tstruct tcindex_filter __rcu **hash;\n\n\t\t\thash = kcalloc(cp->hash,\n\t\t\t\t       sizeof(struct tcindex_filter *),\n\t\t\t\t       GFP_KERNEL);\n\n\t\t\tif (!hash)\n\t\t\t\tgoto errout_alloc;\n\n\t\t\tcp->h = hash;\n\t\t\tballoc = 2;\n\t\t}\n\t}\n\n\tif (cp->perfect)\n\t\tr = cp->perfect + handle;\n\telse\n\t\tr = tcindex_lookup(cp, handle) ? : &new_filter_result;\n\n\tif (r == &new_filter_result) {\n\t\tf = kzalloc(sizeof(*f), GFP_KERNEL);\n\t\tif (!f)\n\t\t\tgoto errout_alloc;\n\t\tf->key = handle;\n\t\tf->next = NULL;\n\t\terr = tcindex_filter_result_init(&f->result, cp, net);\n\t\tif (err < 0) {\n\t\t\tkfree(f);\n\t\t\tgoto errout_alloc;\n\t\t}\n\t}\n\n\tif (tb[TCA_TCINDEX_CLASSID]) {\n\t\tcr.classid = nla_get_u32(tb[TCA_TCINDEX_CLASSID]);\n\t\ttcf_bind_filter(tp, &cr, base);\n\t}\n\n\toldp = p;\n\tr->res = cr;\n\ttcf_exts_change(&r->exts, &e);\n\n\trcu_assign_pointer(tp->root, cp);\n\n\tif (r == &new_filter_result) {\n\t\tstruct tcindex_filter *nfp;\n\t\tstruct tcindex_filter __rcu **fp;\n\n\t\tf->result.res = r->res;\n\t\ttcf_exts_change(&f->result.exts, &r->exts);\n\n\t\tfp = cp->h + (handle % cp->hash);\n\t\tfor (nfp = rtnl_dereference(*fp);\n\t\t     nfp;\n\t\t     fp = &nfp->next, nfp = rtnl_dereference(*fp))\n\t\t\t\t; /* nothing */\n\n\t\trcu_assign_pointer(*fp, f);\n\t} else {\n\t\ttcf_exts_destroy(&new_filter_result.exts);\n\t}\n\n\tif (oldp)\n\t\ttcf_queue_work(&oldp->rwork, tcindex_partial_destroy_work);\n\treturn 0;\n\nerrout_alloc:\n\tif (balloc == 1)\n\t\ttcindex_free_perfect_hash(cp);\n\telse if (balloc == 2)\n\t\tkfree(cp->h);\n\ttcf_exts_destroy(&new_filter_result.exts);\nerrout:\n\tkfree(cp);\n\ttcf_exts_destroy(&e);\n\treturn err;\n}",
        "code_after_change": "static int\ntcindex_set_parms(struct net *net, struct tcf_proto *tp, unsigned long base,\n\t\t  u32 handle, struct tcindex_data *p,\n\t\t  struct tcindex_filter_result *r, struct nlattr **tb,\n\t\t  struct nlattr *est, u32 flags, struct netlink_ext_ack *extack)\n{\n\tstruct tcindex_filter_result new_filter_result;\n\tstruct tcindex_data *cp = NULL, *oldp;\n\tstruct tcindex_filter *f = NULL; /* make gcc behave */\n\tstruct tcf_result cr = {};\n\tint err, balloc = 0;\n\tstruct tcf_exts e;\n\tbool update_h = false;\n\n\terr = tcf_exts_init(&e, net, TCA_TCINDEX_ACT, TCA_TCINDEX_POLICE);\n\tif (err < 0)\n\t\treturn err;\n\terr = tcf_exts_validate(net, tp, tb, est, &e, flags, extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENOMEM;\n\t/* tcindex_data attributes must look atomic to classifier/lookup so\n\t * allocate new tcindex data and RCU assign it onto root. Keeping\n\t * perfect hash and hash pointers from old data.\n\t */\n\tcp = kzalloc(sizeof(*cp), GFP_KERNEL);\n\tif (!cp)\n\t\tgoto errout;\n\n\tcp->mask = p->mask;\n\tcp->shift = p->shift;\n\tcp->hash = p->hash;\n\tcp->alloc_hash = p->alloc_hash;\n\tcp->fall_through = p->fall_through;\n\tcp->tp = tp;\n\trefcount_set(&cp->refcnt, 1); /* Paired with tcindex_destroy_work() */\n\n\tif (tb[TCA_TCINDEX_HASH])\n\t\tcp->hash = nla_get_u32(tb[TCA_TCINDEX_HASH]);\n\n\tif (tb[TCA_TCINDEX_MASK])\n\t\tcp->mask = nla_get_u16(tb[TCA_TCINDEX_MASK]);\n\n\tif (tb[TCA_TCINDEX_SHIFT]) {\n\t\tcp->shift = nla_get_u32(tb[TCA_TCINDEX_SHIFT]);\n\t\tif (cp->shift > 16) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout;\n\t\t}\n\t}\n\tif (!cp->hash) {\n\t\t/* Hash not specified, use perfect hash if the upper limit\n\t\t * of the hashing index is below the threshold.\n\t\t */\n\t\tif ((cp->mask >> cp->shift) < PERFECT_HASH_THRESHOLD)\n\t\t\tcp->hash = (cp->mask >> cp->shift) + 1;\n\t\telse\n\t\t\tcp->hash = DEFAULT_HASH_SIZE;\n\t}\n\n\tif (p->perfect) {\n\t\tint i;\n\n\t\tif (tcindex_alloc_perfect_hash(net, cp) < 0)\n\t\t\tgoto errout;\n\t\tcp->alloc_hash = cp->hash;\n\t\tfor (i = 0; i < min(cp->hash, p->hash); i++)\n\t\t\tcp->perfect[i].res = p->perfect[i].res;\n\t\tballoc = 1;\n\t}\n\tcp->h = p->h;\n\n\terr = tcindex_filter_result_init(&new_filter_result, cp, net);\n\tif (err < 0)\n\t\tgoto errout_alloc;\n\tif (r)\n\t\tcr = r->res;\n\n\terr = -EBUSY;\n\n\t/* Hash already allocated, make sure that we still meet the\n\t * requirements for the allocated hash.\n\t */\n\tif (cp->perfect) {\n\t\tif (!valid_perfect_hash(cp) ||\n\t\t    cp->hash > cp->alloc_hash)\n\t\t\tgoto errout_alloc;\n\t} else if (cp->h && cp->hash != cp->alloc_hash) {\n\t\tgoto errout_alloc;\n\t}\n\n\terr = -EINVAL;\n\tif (tb[TCA_TCINDEX_FALL_THROUGH])\n\t\tcp->fall_through = nla_get_u32(tb[TCA_TCINDEX_FALL_THROUGH]);\n\n\tif (!cp->perfect && !cp->h)\n\t\tcp->alloc_hash = cp->hash;\n\n\t/* Note: this could be as restrictive as if (handle & ~(mask >> shift))\n\t * but then, we'd fail handles that may become valid after some future\n\t * mask change. While this is extremely unlikely to ever matter,\n\t * the check below is safer (and also more backwards-compatible).\n\t */\n\tif (cp->perfect || valid_perfect_hash(cp))\n\t\tif (handle >= cp->alloc_hash)\n\t\t\tgoto errout_alloc;\n\n\n\terr = -ENOMEM;\n\tif (!cp->perfect && !cp->h) {\n\t\tif (valid_perfect_hash(cp)) {\n\t\t\tif (tcindex_alloc_perfect_hash(net, cp) < 0)\n\t\t\t\tgoto errout_alloc;\n\t\t\tballoc = 1;\n\t\t} else {\n\t\t\tstruct tcindex_filter __rcu **hash;\n\n\t\t\thash = kcalloc(cp->hash,\n\t\t\t\t       sizeof(struct tcindex_filter *),\n\t\t\t\t       GFP_KERNEL);\n\n\t\t\tif (!hash)\n\t\t\t\tgoto errout_alloc;\n\n\t\t\tcp->h = hash;\n\t\t\tballoc = 2;\n\t\t}\n\t}\n\n\tif (cp->perfect) {\n\t\tr = cp->perfect + handle;\n\t} else {\n\t\t/* imperfect area is updated in-place using rcu */\n\t\tupdate_h = !!tcindex_lookup(cp, handle);\n\t\tr = &new_filter_result;\n\t}\n\n\tif (r == &new_filter_result) {\n\t\tf = kzalloc(sizeof(*f), GFP_KERNEL);\n\t\tif (!f)\n\t\t\tgoto errout_alloc;\n\t\tf->key = handle;\n\t\tf->next = NULL;\n\t\terr = tcindex_filter_result_init(&f->result, cp, net);\n\t\tif (err < 0) {\n\t\t\tkfree(f);\n\t\t\tgoto errout_alloc;\n\t\t}\n\t}\n\n\tif (tb[TCA_TCINDEX_CLASSID]) {\n\t\tcr.classid = nla_get_u32(tb[TCA_TCINDEX_CLASSID]);\n\t\ttcf_bind_filter(tp, &cr, base);\n\t}\n\n\toldp = p;\n\tr->res = cr;\n\ttcf_exts_change(&r->exts, &e);\n\n\trcu_assign_pointer(tp->root, cp);\n\n\tif (update_h) {\n\t\tstruct tcindex_filter __rcu **fp;\n\t\tstruct tcindex_filter *cf;\n\n\t\tf->result.res = r->res;\n\t\ttcf_exts_change(&f->result.exts, &r->exts);\n\n\t\t/* imperfect area bucket */\n\t\tfp = cp->h + (handle % cp->hash);\n\n\t\t/* lookup the filter, guaranteed to exist */\n\t\tfor (cf = rcu_dereference_bh_rtnl(*fp); cf;\n\t\t     fp = &cf->next, cf = rcu_dereference_bh_rtnl(*fp))\n\t\t\tif (cf->key == handle)\n\t\t\t\tbreak;\n\n\t\tf->next = cf->next;\n\n\t\tcf = rcu_replace_pointer(*fp, f, 1);\n\t\ttcf_exts_get_net(&cf->result.exts);\n\t\ttcf_queue_work(&cf->rwork, tcindex_destroy_fexts_work);\n\t} else if (r == &new_filter_result) {\n\t\tstruct tcindex_filter *nfp;\n\t\tstruct tcindex_filter __rcu **fp;\n\n\t\tf->result.res = r->res;\n\t\ttcf_exts_change(&f->result.exts, &r->exts);\n\n\t\tfp = cp->h + (handle % cp->hash);\n\t\tfor (nfp = rtnl_dereference(*fp);\n\t\t     nfp;\n\t\t     fp = &nfp->next, nfp = rtnl_dereference(*fp))\n\t\t\t\t; /* nothing */\n\n\t\trcu_assign_pointer(*fp, f);\n\t} else {\n\t\ttcf_exts_destroy(&new_filter_result.exts);\n\t}\n\n\tif (oldp)\n\t\ttcf_queue_work(&oldp->rwork, tcindex_partial_destroy_work);\n\treturn 0;\n\nerrout_alloc:\n\tif (balloc == 1)\n\t\ttcindex_free_perfect_hash(cp);\n\telse if (balloc == 2)\n\t\tkfree(cp->h);\n\ttcf_exts_destroy(&new_filter_result.exts);\nerrout:\n\tkfree(cp);\n\ttcf_exts_destroy(&e);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,7 @@\n \tstruct tcf_result cr = {};\n \tint err, balloc = 0;\n \tstruct tcf_exts e;\n+\tbool update_h = false;\n \n \terr = tcf_exts_init(&e, net, TCA_TCINDEX_ACT, TCA_TCINDEX_POLICE);\n \tif (err < 0)\n@@ -127,10 +128,13 @@\n \t\t}\n \t}\n \n-\tif (cp->perfect)\n+\tif (cp->perfect) {\n \t\tr = cp->perfect + handle;\n-\telse\n-\t\tr = tcindex_lookup(cp, handle) ? : &new_filter_result;\n+\t} else {\n+\t\t/* imperfect area is updated in-place using rcu */\n+\t\tupdate_h = !!tcindex_lookup(cp, handle);\n+\t\tr = &new_filter_result;\n+\t}\n \n \tif (r == &new_filter_result) {\n \t\tf = kzalloc(sizeof(*f), GFP_KERNEL);\n@@ -156,7 +160,28 @@\n \n \trcu_assign_pointer(tp->root, cp);\n \n-\tif (r == &new_filter_result) {\n+\tif (update_h) {\n+\t\tstruct tcindex_filter __rcu **fp;\n+\t\tstruct tcindex_filter *cf;\n+\n+\t\tf->result.res = r->res;\n+\t\ttcf_exts_change(&f->result.exts, &r->exts);\n+\n+\t\t/* imperfect area bucket */\n+\t\tfp = cp->h + (handle % cp->hash);\n+\n+\t\t/* lookup the filter, guaranteed to exist */\n+\t\tfor (cf = rcu_dereference_bh_rtnl(*fp); cf;\n+\t\t     fp = &cf->next, cf = rcu_dereference_bh_rtnl(*fp))\n+\t\t\tif (cf->key == handle)\n+\t\t\t\tbreak;\n+\n+\t\tf->next = cf->next;\n+\n+\t\tcf = rcu_replace_pointer(*fp, f, 1);\n+\t\ttcf_exts_get_net(&cf->result.exts);\n+\t\ttcf_queue_work(&cf->rwork, tcindex_destroy_fexts_work);\n+\t} else if (r == &new_filter_result) {\n \t\tstruct tcindex_filter *nfp;\n \t\tstruct tcindex_filter __rcu **fp;\n ",
        "function_modified_lines": {
            "added": [
                "\tbool update_h = false;",
                "\tif (cp->perfect) {",
                "\t} else {",
                "\t\t/* imperfect area is updated in-place using rcu */",
                "\t\tupdate_h = !!tcindex_lookup(cp, handle);",
                "\t\tr = &new_filter_result;",
                "\t}",
                "\tif (update_h) {",
                "\t\tstruct tcindex_filter __rcu **fp;",
                "\t\tstruct tcindex_filter *cf;",
                "",
                "\t\tf->result.res = r->res;",
                "\t\ttcf_exts_change(&f->result.exts, &r->exts);",
                "",
                "\t\t/* imperfect area bucket */",
                "\t\tfp = cp->h + (handle % cp->hash);",
                "",
                "\t\t/* lookup the filter, guaranteed to exist */",
                "\t\tfor (cf = rcu_dereference_bh_rtnl(*fp); cf;",
                "\t\t     fp = &cf->next, cf = rcu_dereference_bh_rtnl(*fp))",
                "\t\t\tif (cf->key == handle)",
                "\t\t\t\tbreak;",
                "",
                "\t\tf->next = cf->next;",
                "",
                "\t\tcf = rcu_replace_pointer(*fp, f, 1);",
                "\t\ttcf_exts_get_net(&cf->result.exts);",
                "\t\ttcf_queue_work(&cf->rwork, tcindex_destroy_fexts_work);",
                "\t} else if (r == &new_filter_result) {"
            ],
            "deleted": [
                "\tif (cp->perfect)",
                "\telse",
                "\t\tr = tcindex_lookup(cp, handle) ? : &new_filter_result;",
                "\tif (r == &new_filter_result) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use After Free vulnerability in Linux kernel traffic control index filter (tcindex) allows Privilege Escalation. The imperfect hash area can be updated while packets are traversing, which will cause a use-after-free when 'tcf_exts_exec()' is called with the destroyed tcf_ext. A local attacker user can use this vulnerability to elevate its privileges to root.\nThis issue affects Linux Kernel: from 4.14 before git commit ee059170b1f7e94e55fa6cadee544e176a6e59c2.\n\n",
        "id": 3861
    },
    {
        "cve_id": "CVE-2018-21008",
        "code_before_change": "void rsi_mac80211_detach(struct rsi_hw *adapter)\n{\n\tstruct ieee80211_hw *hw = adapter->hw;\n\tenum nl80211_band band;\n\n\tif (hw) {\n\t\tieee80211_stop_queues(hw);\n\t\tieee80211_unregister_hw(hw);\n\t\tieee80211_free_hw(hw);\n\t}\n\n\tfor (band = 0; band < NUM_NL80211_BANDS; band++) {\n\t\tstruct ieee80211_supported_band *sband =\n\t\t\t\t\t&adapter->sbands[band];\n\n\t\tkfree(sband->channels);\n\t}\n\n#ifdef CONFIG_RSI_DEBUGFS\n\trsi_remove_dbgfs(adapter);\n\tkfree(adapter->dfsentry);\n#endif\n}",
        "code_after_change": "void rsi_mac80211_detach(struct rsi_hw *adapter)\n{\n\tstruct ieee80211_hw *hw = adapter->hw;\n\tenum nl80211_band band;\n\n\tif (hw) {\n\t\tieee80211_stop_queues(hw);\n\t\tieee80211_unregister_hw(hw);\n\t\tieee80211_free_hw(hw);\n\t\tadapter->hw = NULL;\n\t}\n\n\tfor (band = 0; band < NUM_NL80211_BANDS; band++) {\n\t\tstruct ieee80211_supported_band *sband =\n\t\t\t\t\t&adapter->sbands[band];\n\n\t\tkfree(sband->channels);\n\t}\n\n#ifdef CONFIG_RSI_DEBUGFS\n\trsi_remove_dbgfs(adapter);\n\tkfree(adapter->dfsentry);\n#endif\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \t\tieee80211_stop_queues(hw);\n \t\tieee80211_unregister_hw(hw);\n \t\tieee80211_free_hw(hw);\n+\t\tadapter->hw = NULL;\n \t}\n \n \tfor (band = 0; band < NUM_NL80211_BANDS; band++) {",
        "function_modified_lines": {
            "added": [
                "\t\tadapter->hw = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.16.7. A use-after-free can be caused by the function rsi_mac80211_detach in the file drivers/net/wireless/rsi/rsi_91x_mac80211.c.",
        "id": 1793
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "static __cold void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tunsigned long index;\n\tstruct creds *creds;\n\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true);\n\txa_for_each(&ctx->personalities, index, creds)\n\t\tio_unregister_personality(ctx, index);\n\tmutex_unlock(&ctx->uring_lock);\n\n\t/* failed during ring init, it couldn't have issued any requests */\n\tif (ctx->rings) {\n\t\tio_kill_timeouts(ctx, NULL, true);\n\t\tio_poll_remove_all(ctx, NULL, true);\n\t\t/* if we failed setting up the ctx, we might not have any rings */\n\t\tio_iopoll_try_reap_events(ctx);\n\t}\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}",
        "code_after_change": "static __cold void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tunsigned long index;\n\tstruct creds *creds;\n\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true);\n\txa_for_each(&ctx->personalities, index, creds)\n\t\tio_unregister_personality(ctx, index);\n\tif (ctx->rings)\n\t\tio_poll_remove_all(ctx, NULL, true);\n\tmutex_unlock(&ctx->uring_lock);\n\n\t/* failed during ring init, it couldn't have issued any requests */\n\tif (ctx->rings) {\n\t\tio_kill_timeouts(ctx, NULL, true);\n\t\t/* if we failed setting up the ctx, we might not have any rings */\n\t\tio_iopoll_try_reap_events(ctx);\n\t}\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,12 +9,13 @@\n \t\t__io_cqring_overflow_flush(ctx, true);\n \txa_for_each(&ctx->personalities, index, creds)\n \t\tio_unregister_personality(ctx, index);\n+\tif (ctx->rings)\n+\t\tio_poll_remove_all(ctx, NULL, true);\n \tmutex_unlock(&ctx->uring_lock);\n \n \t/* failed during ring init, it couldn't have issued any requests */\n \tif (ctx->rings) {\n \t\tio_kill_timeouts(ctx, NULL, true);\n-\t\tio_poll_remove_all(ctx, NULL, true);\n \t\t/* if we failed setting up the ctx, we might not have any rings */\n \t\tio_iopoll_try_reap_events(ctx);\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif (ctx->rings)",
                "\t\tio_poll_remove_all(ctx, NULL, true);"
            ],
            "deleted": [
                "\t\tio_poll_remove_all(ctx, NULL, true);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and 0e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4065
    },
    {
        "cve_id": "CVE-2021-38204",
        "code_before_change": "static int\nmax3421_select_and_start_urb(struct usb_hcd *hcd)\n{\n\tstruct spi_device *spi = to_spi_device(hcd->self.controller);\n\tstruct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);\n\tstruct urb *urb, *curr_urb = NULL;\n\tstruct max3421_ep *max3421_ep;\n\tint epnum, force_toggles = 0;\n\tstruct usb_host_endpoint *ep;\n\tstruct list_head *pos;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&max3421_hcd->lock, flags);\n\n\tfor (;\n\t     max3421_hcd->sched_pass < SCHED_PASS_DONE;\n\t     ++max3421_hcd->sched_pass)\n\t\tlist_for_each(pos, &max3421_hcd->ep_list) {\n\t\t\turb = NULL;\n\t\t\tmax3421_ep = container_of(pos, struct max3421_ep,\n\t\t\t\t\t\t  ep_list);\n\t\t\tep = max3421_ep->ep;\n\n\t\t\tswitch (usb_endpoint_type(&ep->desc)) {\n\t\t\tcase USB_ENDPOINT_XFER_ISOC:\n\t\t\tcase USB_ENDPOINT_XFER_INT:\n\t\t\t\tif (max3421_hcd->sched_pass !=\n\t\t\t\t    SCHED_PASS_PERIODIC)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\t\tcase USB_ENDPOINT_XFER_BULK:\n\t\t\t\tif (max3421_hcd->sched_pass !=\n\t\t\t\t    SCHED_PASS_NON_PERIODIC)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (list_empty(&ep->urb_list))\n\t\t\t\tcontinue;\t/* nothing to do */\n\t\t\turb = list_first_entry(&ep->urb_list, struct urb,\n\t\t\t\t\t       urb_list);\n\t\t\tif (urb->unlinked) {\n\t\t\t\tdev_dbg(&spi->dev, \"%s: URB %p unlinked=%d\",\n\t\t\t\t\t__func__, urb, urb->unlinked);\n\t\t\t\tmax3421_hcd->curr_urb = urb;\n\t\t\t\tmax3421_hcd->urb_done = 1;\n\t\t\t\tspin_unlock_irqrestore(&max3421_hcd->lock,\n\t\t\t\t\t\t       flags);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tswitch (usb_endpoint_type(&ep->desc)) {\n\t\t\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\t\t\t/*\n\t\t\t\t * Allow one control transaction per\n\t\t\t\t * frame per endpoint:\n\t\t\t\t */\n\t\t\t\tif (frame_diff(max3421_ep->last_active,\n\t\t\t\t\t       max3421_hcd->frame_number) == 0)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_BULK:\n\t\t\t\tif (max3421_ep->retransmit\n\t\t\t\t    && (frame_diff(max3421_ep->last_active,\n\t\t\t\t\t\t   max3421_hcd->frame_number)\n\t\t\t\t\t== 0))\n\t\t\t\t\t/*\n\t\t\t\t\t * We already tried this EP\n\t\t\t\t\t * during this frame and got a\n\t\t\t\t\t * NAK or error; wait for next frame\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_ISOC:\n\t\t\tcase USB_ENDPOINT_XFER_INT:\n\t\t\t\tif (frame_diff(max3421_hcd->frame_number,\n\t\t\t\t\t       max3421_ep->last_active)\n\t\t\t\t    < urb->interval)\n\t\t\t\t\t/*\n\t\t\t\t\t * We already processed this\n\t\t\t\t\t * end-point in the current\n\t\t\t\t\t * frame\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* move current ep to tail: */\n\t\t\tlist_move_tail(pos, &max3421_hcd->ep_list);\n\t\t\tcurr_urb = urb;\n\t\t\tgoto done;\n\t\t}\ndone:\n\tif (!curr_urb) {\n\t\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\t\treturn 0;\n\t}\n\n\turb = max3421_hcd->curr_urb = curr_urb;\n\tepnum = usb_endpoint_num(&urb->ep->desc);\n\tif (max3421_ep->retransmit)\n\t\t/* restart (part of) a USB transaction: */\n\t\tmax3421_ep->retransmit = 0;\n\telse {\n\t\t/* start USB transaction: */\n\t\tif (usb_endpoint_xfer_control(&ep->desc)) {\n\t\t\t/*\n\t\t\t * See USB 2.0 spec section 8.6.1\n\t\t\t * Initialization via SETUP Token:\n\t\t\t */\n\t\t\tusb_settoggle(urb->dev, epnum, 0, 1);\n\t\t\tusb_settoggle(urb->dev, epnum, 1, 1);\n\t\t\tmax3421_ep->pkt_state = PKT_STATE_SETUP;\n\t\t\tforce_toggles = 1;\n\t\t} else\n\t\t\tmax3421_ep->pkt_state = PKT_STATE_TRANSFER;\n\t}\n\n\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\n\tmax3421_ep->last_active = max3421_hcd->frame_number;\n\tmax3421_set_address(hcd, urb->dev, epnum, force_toggles);\n\tmax3421_set_speed(hcd, urb->dev);\n\tmax3421_next_transfer(hcd, 0);\n\treturn 1;\n}",
        "code_after_change": "static int\nmax3421_select_and_start_urb(struct usb_hcd *hcd)\n{\n\tstruct spi_device *spi = to_spi_device(hcd->self.controller);\n\tstruct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);\n\tstruct urb *urb, *curr_urb = NULL;\n\tstruct max3421_ep *max3421_ep;\n\tint epnum;\n\tstruct usb_host_endpoint *ep;\n\tstruct list_head *pos;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&max3421_hcd->lock, flags);\n\n\tfor (;\n\t     max3421_hcd->sched_pass < SCHED_PASS_DONE;\n\t     ++max3421_hcd->sched_pass)\n\t\tlist_for_each(pos, &max3421_hcd->ep_list) {\n\t\t\turb = NULL;\n\t\t\tmax3421_ep = container_of(pos, struct max3421_ep,\n\t\t\t\t\t\t  ep_list);\n\t\t\tep = max3421_ep->ep;\n\n\t\t\tswitch (usb_endpoint_type(&ep->desc)) {\n\t\t\tcase USB_ENDPOINT_XFER_ISOC:\n\t\t\tcase USB_ENDPOINT_XFER_INT:\n\t\t\t\tif (max3421_hcd->sched_pass !=\n\t\t\t\t    SCHED_PASS_PERIODIC)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\t\tcase USB_ENDPOINT_XFER_BULK:\n\t\t\t\tif (max3421_hcd->sched_pass !=\n\t\t\t\t    SCHED_PASS_NON_PERIODIC)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (list_empty(&ep->urb_list))\n\t\t\t\tcontinue;\t/* nothing to do */\n\t\t\turb = list_first_entry(&ep->urb_list, struct urb,\n\t\t\t\t\t       urb_list);\n\t\t\tif (urb->unlinked) {\n\t\t\t\tdev_dbg(&spi->dev, \"%s: URB %p unlinked=%d\",\n\t\t\t\t\t__func__, urb, urb->unlinked);\n\t\t\t\tmax3421_hcd->curr_urb = urb;\n\t\t\t\tmax3421_hcd->urb_done = 1;\n\t\t\t\tspin_unlock_irqrestore(&max3421_hcd->lock,\n\t\t\t\t\t\t       flags);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tswitch (usb_endpoint_type(&ep->desc)) {\n\t\t\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\t\t\t/*\n\t\t\t\t * Allow one control transaction per\n\t\t\t\t * frame per endpoint:\n\t\t\t\t */\n\t\t\t\tif (frame_diff(max3421_ep->last_active,\n\t\t\t\t\t       max3421_hcd->frame_number) == 0)\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_BULK:\n\t\t\t\tif (max3421_ep->retransmit\n\t\t\t\t    && (frame_diff(max3421_ep->last_active,\n\t\t\t\t\t\t   max3421_hcd->frame_number)\n\t\t\t\t\t== 0))\n\t\t\t\t\t/*\n\t\t\t\t\t * We already tried this EP\n\t\t\t\t\t * during this frame and got a\n\t\t\t\t\t * NAK or error; wait for next frame\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\n\t\t\tcase USB_ENDPOINT_XFER_ISOC:\n\t\t\tcase USB_ENDPOINT_XFER_INT:\n\t\t\t\tif (frame_diff(max3421_hcd->frame_number,\n\t\t\t\t\t       max3421_ep->last_active)\n\t\t\t\t    < urb->interval)\n\t\t\t\t\t/*\n\t\t\t\t\t * We already processed this\n\t\t\t\t\t * end-point in the current\n\t\t\t\t\t * frame\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* move current ep to tail: */\n\t\t\tlist_move_tail(pos, &max3421_hcd->ep_list);\n\t\t\tcurr_urb = urb;\n\t\t\tgoto done;\n\t\t}\ndone:\n\tif (!curr_urb) {\n\t\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\t\treturn 0;\n\t}\n\n\turb = max3421_hcd->curr_urb = curr_urb;\n\tepnum = usb_endpoint_num(&urb->ep->desc);\n\tif (max3421_ep->retransmit)\n\t\t/* restart (part of) a USB transaction: */\n\t\tmax3421_ep->retransmit = 0;\n\telse {\n\t\t/* start USB transaction: */\n\t\tif (usb_endpoint_xfer_control(&ep->desc)) {\n\t\t\t/*\n\t\t\t * See USB 2.0 spec section 8.6.1\n\t\t\t * Initialization via SETUP Token:\n\t\t\t */\n\t\t\tusb_settoggle(urb->dev, epnum, 0, 1);\n\t\t\tusb_settoggle(urb->dev, epnum, 1, 1);\n\t\t\tmax3421_ep->pkt_state = PKT_STATE_SETUP;\n\t\t} else\n\t\t\tmax3421_ep->pkt_state = PKT_STATE_TRANSFER;\n\t}\n\n\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\n\tmax3421_ep->last_active = max3421_hcd->frame_number;\n\tmax3421_set_address(hcd, urb->dev, epnum);\n\tmax3421_set_speed(hcd, urb->dev);\n\tmax3421_next_transfer(hcd, 0);\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \tstruct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);\n \tstruct urb *urb, *curr_urb = NULL;\n \tstruct max3421_ep *max3421_ep;\n-\tint epnum, force_toggles = 0;\n+\tint epnum;\n \tstruct usb_host_endpoint *ep;\n \tstruct list_head *pos;\n \tunsigned long flags;\n@@ -115,7 +115,6 @@\n \t\t\tusb_settoggle(urb->dev, epnum, 0, 1);\n \t\t\tusb_settoggle(urb->dev, epnum, 1, 1);\n \t\t\tmax3421_ep->pkt_state = PKT_STATE_SETUP;\n-\t\t\tforce_toggles = 1;\n \t\t} else\n \t\t\tmax3421_ep->pkt_state = PKT_STATE_TRANSFER;\n \t}\n@@ -123,7 +122,7 @@\n \tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n \n \tmax3421_ep->last_active = max3421_hcd->frame_number;\n-\tmax3421_set_address(hcd, urb->dev, epnum, force_toggles);\n+\tmax3421_set_address(hcd, urb->dev, epnum);\n \tmax3421_set_speed(hcd, urb->dev);\n \tmax3421_next_transfer(hcd, 0);\n \treturn 1;",
        "function_modified_lines": {
            "added": [
                "\tint epnum;",
                "\tmax3421_set_address(hcd, urb->dev, epnum);"
            ],
            "deleted": [
                "\tint epnum, force_toggles = 0;",
                "\t\t\tforce_toggles = 1;",
                "\tmax3421_set_address(hcd, urb->dev, epnum, force_toggles);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "drivers/usb/host/max3421-hcd.c in the Linux kernel before 5.13.6 allows physically proximate attackers to cause a denial of service (use-after-free and panic) by removing a MAX-3421 USB device in certain situations.",
        "id": 3079
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static int aio_fsync(struct fsync_iocb *req, const struct iocb *iocb,\n\t\t     bool datasync)\n{\n\tif (unlikely(iocb->aio_buf || iocb->aio_offset || iocb->aio_nbytes ||\n\t\t\tiocb->aio_rw_flags))\n\t\treturn -EINVAL;\n\n\treq->file = fget(iocb->aio_fildes);\n\tif (unlikely(!req->file))\n\t\treturn -EBADF;\n\tif (unlikely(!req->file->f_op->fsync)) {\n\t\tfput(req->file);\n\t\treturn -EINVAL;\n\t}\n\n\treq->datasync = datasync;\n\tINIT_WORK(&req->work, aio_fsync_work);\n\tschedule_work(&req->work);\n\treturn 0;\n}",
        "code_after_change": "static int aio_fsync(struct fsync_iocb *req, const struct iocb *iocb,\n\t\t     bool datasync)\n{\n\tif (unlikely(iocb->aio_buf || iocb->aio_offset || iocb->aio_nbytes ||\n\t\t\tiocb->aio_rw_flags))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!req->file->f_op->fsync))\n\t\treturn -EINVAL;\n\n\treq->datasync = datasync;\n\tINIT_WORK(&req->work, aio_fsync_work);\n\tschedule_work(&req->work);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,13 +5,8 @@\n \t\t\tiocb->aio_rw_flags))\n \t\treturn -EINVAL;\n \n-\treq->file = fget(iocb->aio_fildes);\n-\tif (unlikely(!req->file))\n-\t\treturn -EBADF;\n-\tif (unlikely(!req->file->f_op->fsync)) {\n-\t\tfput(req->file);\n+\tif (unlikely(!req->file->f_op->fsync))\n \t\treturn -EINVAL;\n-\t}\n \n \treq->datasync = datasync;\n \tINIT_WORK(&req->work, aio_fsync_work);",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(!req->file->f_op->fsync))"
            ],
            "deleted": [
                "\treq->file = fget(iocb->aio_fildes);",
                "\tif (unlikely(!req->file))",
                "\t\treturn -EBADF;",
                "\tif (unlikely(!req->file->f_op->fsync)) {",
                "\t\tfput(req->file);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1885
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static inline int l2cap_config_rsp(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_conf_rsp *rsp = (struct l2cap_conf_rsp *)data;\n\tu16 scid, flags, result;\n\tstruct l2cap_chan *chan;\n\tint len = cmd_len - sizeof(*rsp);\n\tint err = 0;\n\n\tif (cmd_len < sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\tscid   = __le16_to_cpu(rsp->scid);\n\tflags  = __le16_to_cpu(rsp->flags);\n\tresult = __le16_to_cpu(rsp->result);\n\n\tBT_DBG(\"scid 0x%4.4x flags 0x%2.2x result 0x%2.2x len %d\", scid, flags,\n\t       result, len);\n\n\tchan = l2cap_get_chan_by_scid(conn, scid);\n\tif (!chan)\n\t\treturn 0;\n\n\tswitch (result) {\n\tcase L2CAP_CONF_SUCCESS:\n\t\tl2cap_conf_rfc_get(chan, rsp->data, len);\n\t\tclear_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\t\tbreak;\n\n\tcase L2CAP_CONF_PENDING:\n\t\tset_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\n\t\tif (test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\n\t\t\tchar buf[64];\n\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   buf, sizeof(buf), &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tif (!chan->hs_hcon) {\n\t\t\t\tl2cap_send_efs_conf_rsp(chan, buf, cmd->ident,\n\t\t\t\t\t\t\t0);\n\t\t\t} else {\n\t\t\t\tif (l2cap_check_efs(chan)) {\n\t\t\t\t\tamp_create_logical_link(chan);\n\t\t\t\t\tchan->ident = cmd->ident;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgoto done;\n\n\tcase L2CAP_CONF_UNKNOWN:\n\tcase L2CAP_CONF_UNACCEPT:\n\t\tif (chan->num_conf_rsp <= L2CAP_CONF_MAX_CONF_RSP) {\n\t\t\tchar req[64];\n\n\t\t\tif (len > sizeof(req) - sizeof(struct l2cap_conf_req)) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\t/* throw out any old stored conf requests */\n\t\t\tresult = L2CAP_CONF_SUCCESS;\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   req, sizeof(req), &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn),\n\t\t\t\t       L2CAP_CONF_REQ, len, req);\n\t\t\tchan->num_conf_req++;\n\t\t\tif (result != L2CAP_CONF_SUCCESS)\n\t\t\t\tgoto done;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\n\tdefault:\n\t\tl2cap_chan_set_err(chan, ECONNRESET);\n\n\t\t__set_chan_timer(chan, L2CAP_DISC_REJ_TIMEOUT);\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto done;\n\t}\n\n\tif (flags & L2CAP_CONF_FLAG_CONTINUATION)\n\t\tgoto done;\n\n\tset_bit(CONF_INPUT_DONE, &chan->conf_state);\n\n\tif (test_bit(CONF_OUTPUT_DONE, &chan->conf_state)) {\n\t\tset_default_fcs(chan);\n\n\t\tif (chan->mode == L2CAP_MODE_ERTM ||\n\t\t    chan->mode == L2CAP_MODE_STREAMING)\n\t\t\terr = l2cap_ertm_init(chan);\n\n\t\tif (err < 0)\n\t\t\tl2cap_send_disconn_req(chan, -err);\n\t\telse\n\t\t\tl2cap_chan_ready(chan);\n\t}\n\ndone:\n\tl2cap_chan_unlock(chan);\n\treturn err;\n}",
        "code_after_change": "static inline int l2cap_config_rsp(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_conf_rsp *rsp = (struct l2cap_conf_rsp *)data;\n\tu16 scid, flags, result;\n\tstruct l2cap_chan *chan;\n\tint len = cmd_len - sizeof(*rsp);\n\tint err = 0;\n\n\tif (cmd_len < sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\tscid   = __le16_to_cpu(rsp->scid);\n\tflags  = __le16_to_cpu(rsp->flags);\n\tresult = __le16_to_cpu(rsp->result);\n\n\tBT_DBG(\"scid 0x%4.4x flags 0x%2.2x result 0x%2.2x len %d\", scid, flags,\n\t       result, len);\n\n\tchan = l2cap_get_chan_by_scid(conn, scid);\n\tif (!chan)\n\t\treturn 0;\n\n\tswitch (result) {\n\tcase L2CAP_CONF_SUCCESS:\n\t\tl2cap_conf_rfc_get(chan, rsp->data, len);\n\t\tclear_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\t\tbreak;\n\n\tcase L2CAP_CONF_PENDING:\n\t\tset_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\n\t\tif (test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\n\t\t\tchar buf[64];\n\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   buf, sizeof(buf), &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tif (!chan->hs_hcon) {\n\t\t\t\tl2cap_send_efs_conf_rsp(chan, buf, cmd->ident,\n\t\t\t\t\t\t\t0);\n\t\t\t} else {\n\t\t\t\tif (l2cap_check_efs(chan)) {\n\t\t\t\t\tamp_create_logical_link(chan);\n\t\t\t\t\tchan->ident = cmd->ident;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgoto done;\n\n\tcase L2CAP_CONF_UNKNOWN:\n\tcase L2CAP_CONF_UNACCEPT:\n\t\tif (chan->num_conf_rsp <= L2CAP_CONF_MAX_CONF_RSP) {\n\t\t\tchar req[64];\n\n\t\t\tif (len > sizeof(req) - sizeof(struct l2cap_conf_req)) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\t/* throw out any old stored conf requests */\n\t\t\tresult = L2CAP_CONF_SUCCESS;\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   req, sizeof(req), &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn),\n\t\t\t\t       L2CAP_CONF_REQ, len, req);\n\t\t\tchan->num_conf_req++;\n\t\t\tif (result != L2CAP_CONF_SUCCESS)\n\t\t\t\tgoto done;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\n\tdefault:\n\t\tl2cap_chan_set_err(chan, ECONNRESET);\n\n\t\t__set_chan_timer(chan, L2CAP_DISC_REJ_TIMEOUT);\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto done;\n\t}\n\n\tif (flags & L2CAP_CONF_FLAG_CONTINUATION)\n\t\tgoto done;\n\n\tset_bit(CONF_INPUT_DONE, &chan->conf_state);\n\n\tif (test_bit(CONF_OUTPUT_DONE, &chan->conf_state)) {\n\t\tset_default_fcs(chan);\n\n\t\tif (chan->mode == L2CAP_MODE_ERTM ||\n\t\t    chan->mode == L2CAP_MODE_STREAMING)\n\t\t\terr = l2cap_ertm_init(chan);\n\n\t\tif (err < 0)\n\t\t\tl2cap_send_disconn_req(chan, -err);\n\t\telse\n\t\t\tl2cap_chan_ready(chan);\n\t}\n\ndone:\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -109,5 +109,6 @@\n \n done:\n \tl2cap_chan_unlock(chan);\n+\tl2cap_chan_put(chan);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tl2cap_chan_put(chan);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3390
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "unsigned long\nkvmppc_h_svm_page_out(struct kvm *kvm, unsigned long gpa,\n\t\t      unsigned long flags, unsigned long page_shift)\n{\n\tunsigned long gfn = gpa >> page_shift;\n\tunsigned long start, end;\n\tstruct vm_area_struct *vma;\n\tint srcu_idx;\n\tint ret;\n\n\tif (!(kvm->arch.secure_guest & KVMPPC_SECURE_INIT_START))\n\t\treturn H_UNSUPPORTED;\n\n\tif (page_shift != PAGE_SHIFT)\n\t\treturn H_P3;\n\n\tif (flags)\n\t\treturn H_P2;\n\n\tret = H_PARAMETER;\n\tsrcu_idx = srcu_read_lock(&kvm->srcu);\n\tmmap_read_lock(kvm->mm);\n\tstart = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(start))\n\t\tgoto out;\n\n\tend = start + (1UL << page_shift);\n\tvma = find_vma_intersection(kvm->mm, start, end);\n\tif (!vma || vma->vm_start > start || vma->vm_end < end)\n\t\tgoto out;\n\n\tif (!kvmppc_svm_page_out(vma, start, end, page_shift, kvm, gpa))\n\t\tret = H_SUCCESS;\nout:\n\tmmap_read_unlock(kvm->mm);\n\tsrcu_read_unlock(&kvm->srcu, srcu_idx);\n\treturn ret;\n}",
        "code_after_change": "unsigned long\nkvmppc_h_svm_page_out(struct kvm *kvm, unsigned long gpa,\n\t\t      unsigned long flags, unsigned long page_shift)\n{\n\tunsigned long gfn = gpa >> page_shift;\n\tunsigned long start, end;\n\tstruct vm_area_struct *vma;\n\tint srcu_idx;\n\tint ret;\n\n\tif (!(kvm->arch.secure_guest & KVMPPC_SECURE_INIT_START))\n\t\treturn H_UNSUPPORTED;\n\n\tif (page_shift != PAGE_SHIFT)\n\t\treturn H_P3;\n\n\tif (flags)\n\t\treturn H_P2;\n\n\tret = H_PARAMETER;\n\tsrcu_idx = srcu_read_lock(&kvm->srcu);\n\tmmap_read_lock(kvm->mm);\n\tstart = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(start))\n\t\tgoto out;\n\n\tend = start + (1UL << page_shift);\n\tvma = find_vma_intersection(kvm->mm, start, end);\n\tif (!vma || vma->vm_start > start || vma->vm_end < end)\n\t\tgoto out;\n\n\tif (!kvmppc_svm_page_out(vma, start, end, page_shift, kvm, gpa, NULL))\n\t\tret = H_SUCCESS;\nout:\n\tmmap_read_unlock(kvm->mm);\n\tsrcu_read_unlock(&kvm->srcu, srcu_idx);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,7 +29,7 @@\n \tif (!vma || vma->vm_start > start || vma->vm_end < end)\n \t\tgoto out;\n \n-\tif (!kvmppc_svm_page_out(vma, start, end, page_shift, kvm, gpa))\n+\tif (!kvmppc_svm_page_out(vma, start, end, page_shift, kvm, gpa, NULL))\n \t\tret = H_SUCCESS;\n out:\n \tmmap_read_unlock(kvm->mm);",
        "function_modified_lines": {
            "added": [
                "\tif (!kvmppc_svm_page_out(vma, start, end, page_shift, kvm, gpa, NULL))"
            ],
            "deleted": [
                "\tif (!kvmppc_svm_page_out(vma, start, end, page_shift, kvm, gpa))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3606
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "static int vmw_cmd_dx_set_shader(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGA3dCmdHeader *header)\n{\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXSetShader);\n\tSVGA3dShaderType max_allowed = has_sm5_context(dev_priv) ?\n\t\tSVGA3D_SHADERTYPE_MAX : SVGA3D_SHADERTYPE_DX10_MAX;\n\tstruct vmw_resource *res = NULL;\n\tstruct vmw_ctx_validation_info *ctx_node = VMW_GET_CTX_NODE(sw_context);\n\tstruct vmw_ctx_bindinfo_shader binding;\n\tint ret = 0;\n\n\tif (!ctx_node)\n\t\treturn -EINVAL;\n\n\tcmd = container_of(header, typeof(*cmd), header);\n\n\tif (cmd->body.type >= max_allowed ||\n\t    cmd->body.type < SVGA3D_SHADERTYPE_MIN) {\n\t\tVMW_DEBUG_USER(\"Illegal shader type %u.\\n\",\n\t\t\t       (unsigned int) cmd->body.type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cmd->body.shaderId != SVGA3D_INVALID_ID) {\n\t\tres = vmw_shader_lookup(sw_context->man, cmd->body.shaderId, 0);\n\t\tif (IS_ERR(res)) {\n\t\t\tVMW_DEBUG_USER(\"Could not find shader for binding.\\n\");\n\t\t\treturn PTR_ERR(res);\n\t\t}\n\n\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbinding.bi.ctx = ctx_node->ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_dx_shader;\n\tbinding.shader_slot = cmd->body.type - SVGA3D_SHADERTYPE_MIN;\n\n\tvmw_binding_add(ctx_node->staged, &binding.bi, binding.shader_slot, 0);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_cmd_dx_set_shader(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGA3dCmdHeader *header)\n{\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXSetShader);\n\tSVGA3dShaderType max_allowed = has_sm5_context(dev_priv) ?\n\t\tSVGA3D_SHADERTYPE_MAX : SVGA3D_SHADERTYPE_DX10_MAX;\n\tstruct vmw_resource *res = NULL;\n\tstruct vmw_ctx_validation_info *ctx_node = VMW_GET_CTX_NODE(sw_context);\n\tstruct vmw_ctx_bindinfo_shader binding;\n\tint ret = 0;\n\n\tif (!ctx_node)\n\t\treturn -EINVAL;\n\n\tcmd = container_of(header, typeof(*cmd), header);\n\n\tif (cmd->body.type >= max_allowed ||\n\t    cmd->body.type < SVGA3D_SHADERTYPE_MIN) {\n\t\tVMW_DEBUG_USER(\"Illegal shader type %u.\\n\",\n\t\t\t       (unsigned int) cmd->body.type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cmd->body.shaderId != SVGA3D_INVALID_ID) {\n\t\tres = vmw_shader_lookup(sw_context->man, cmd->body.shaderId, 0);\n\t\tif (IS_ERR(res)) {\n\t\t\tVMW_DEBUG_USER(\"Could not find shader for binding.\\n\");\n\t\t\treturn PTR_ERR(res);\n\t\t}\n\n\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n\t\t\t\t\t      VMW_RES_DIRTY_NONE,\n\t\t\t\t\t      vmw_val_add_flag_noctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbinding.bi.ctx = ctx_node->ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_dx_shader;\n\tbinding.shader_slot = cmd->body.type - SVGA3D_SHADERTYPE_MIN;\n\n\tvmw_binding_add(ctx_node->staged, &binding.bi, binding.shader_slot, 0);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,8 +29,9 @@\n \t\t\treturn PTR_ERR(res);\n \t\t}\n \n-\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n-\t\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n+\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n+\t\t\t\t\t      VMW_RES_DIRTY_NONE,\n+\t\t\t\t\t      vmw_val_add_flag_noctx);\n \t\tif (ret)\n \t\t\treturn ret;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tret = vmw_execbuf_res_val_add(sw_context, res,",
                "\t\t\t\t\t      VMW_RES_DIRTY_NONE,",
                "\t\t\t\t\t      vmw_val_add_flag_noctx);"
            ],
            "deleted": [
                "\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,",
                "\t\t\t\t\t\t    VMW_RES_DIRTY_NONE);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3683
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static int __blk_trace_startstop(struct request_queue *q, int start)\n{\n\tint ret;\n\tstruct blk_trace *bt = q->blk_trace;\n\n\tif (bt == NULL)\n\t\treturn -EINVAL;\n\n\t/*\n\t * For starting a trace, we can transition from a setup or stopped\n\t * trace. For stopping a trace, the state must be running\n\t */\n\tret = -EINVAL;\n\tif (start) {\n\t\tif (bt->trace_state == Blktrace_setup ||\n\t\t    bt->trace_state == Blktrace_stopped) {\n\t\t\tblktrace_seq++;\n\t\t\tsmp_mb();\n\t\t\tbt->trace_state = Blktrace_running;\n\t\t\tspin_lock_irq(&running_trace_lock);\n\t\t\tlist_add(&bt->running_list, &running_trace_list);\n\t\t\tspin_unlock_irq(&running_trace_lock);\n\n\t\t\ttrace_note_time(bt);\n\t\t\tret = 0;\n\t\t}\n\t} else {\n\t\tif (bt->trace_state == Blktrace_running) {\n\t\t\tbt->trace_state = Blktrace_stopped;\n\t\t\tspin_lock_irq(&running_trace_lock);\n\t\t\tlist_del_init(&bt->running_list);\n\t\t\tspin_unlock_irq(&running_trace_lock);\n\t\t\trelay_flush(bt->rchan);\n\t\t\tret = 0;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int __blk_trace_startstop(struct request_queue *q, int start)\n{\n\tint ret;\n\tstruct blk_trace *bt;\n\n\tbt = rcu_dereference_protected(q->blk_trace,\n\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));\n\tif (bt == NULL)\n\t\treturn -EINVAL;\n\n\t/*\n\t * For starting a trace, we can transition from a setup or stopped\n\t * trace. For stopping a trace, the state must be running\n\t */\n\tret = -EINVAL;\n\tif (start) {\n\t\tif (bt->trace_state == Blktrace_setup ||\n\t\t    bt->trace_state == Blktrace_stopped) {\n\t\t\tblktrace_seq++;\n\t\t\tsmp_mb();\n\t\t\tbt->trace_state = Blktrace_running;\n\t\t\tspin_lock_irq(&running_trace_lock);\n\t\t\tlist_add(&bt->running_list, &running_trace_list);\n\t\t\tspin_unlock_irq(&running_trace_lock);\n\n\t\t\ttrace_note_time(bt);\n\t\t\tret = 0;\n\t\t}\n\t} else {\n\t\tif (bt->trace_state == Blktrace_running) {\n\t\t\tbt->trace_state = Blktrace_stopped;\n\t\t\tspin_lock_irq(&running_trace_lock);\n\t\t\tlist_del_init(&bt->running_list);\n\t\t\tspin_unlock_irq(&running_trace_lock);\n\t\t\trelay_flush(bt->rchan);\n\t\t\tret = 0;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,10 @@\n static int __blk_trace_startstop(struct request_queue *q, int start)\n {\n \tint ret;\n-\tstruct blk_trace *bt = q->blk_trace;\n+\tstruct blk_trace *bt;\n \n+\tbt = rcu_dereference_protected(q->blk_trace,\n+\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));\n \tif (bt == NULL)\n \t\treturn -EINVAL;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct blk_trace *bt;",
                "\tbt = rcu_dereference_protected(q->blk_trace,",
                "\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));"
            ],
            "deleted": [
                "\tstruct blk_trace *bt = q->blk_trace;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2240
    },
    {
        "cve_id": "CVE-2021-20292",
        "code_before_change": "struct ttm_tt *\nnouveau_sgdma_create_ttm(struct ttm_buffer_object *bo, uint32_t page_flags)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\n\tstruct nouveau_sgdma_be *nvbe;\n\n\tnvbe = kzalloc(sizeof(*nvbe), GFP_KERNEL);\n\tif (!nvbe)\n\t\treturn NULL;\n\n\tif (drm->client.device.info.family < NV_DEVICE_INFO_V0_TESLA)\n\t\tnvbe->ttm.ttm.func = &nv04_sgdma_backend;\n\telse\n\t\tnvbe->ttm.ttm.func = &nv50_sgdma_backend;\n\n\tif (ttm_dma_tt_init(&nvbe->ttm, bo, page_flags))\n\t\t/*\n\t\t * A failing ttm_dma_tt_init() will call ttm_tt_destroy()\n\t\t * and thus our nouveau_sgdma_destroy() hook, so we don't need\n\t\t * to free nvbe here.\n\t\t */\n\t\treturn NULL;\n\treturn &nvbe->ttm.ttm;\n}",
        "code_after_change": "struct ttm_tt *\nnouveau_sgdma_create_ttm(struct ttm_buffer_object *bo, uint32_t page_flags)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\n\tstruct nouveau_sgdma_be *nvbe;\n\n\tnvbe = kzalloc(sizeof(*nvbe), GFP_KERNEL);\n\tif (!nvbe)\n\t\treturn NULL;\n\n\tif (drm->client.device.info.family < NV_DEVICE_INFO_V0_TESLA)\n\t\tnvbe->ttm.ttm.func = &nv04_sgdma_backend;\n\telse\n\t\tnvbe->ttm.ttm.func = &nv50_sgdma_backend;\n\n\tif (ttm_dma_tt_init(&nvbe->ttm, bo, page_flags)) {\n\t\tkfree(nvbe);\n\t\treturn NULL;\n\t}\n\treturn &nvbe->ttm.ttm;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,12 +13,9 @@\n \telse\n \t\tnvbe->ttm.ttm.func = &nv50_sgdma_backend;\n \n-\tif (ttm_dma_tt_init(&nvbe->ttm, bo, page_flags))\n-\t\t/*\n-\t\t * A failing ttm_dma_tt_init() will call ttm_tt_destroy()\n-\t\t * and thus our nouveau_sgdma_destroy() hook, so we don't need\n-\t\t * to free nvbe here.\n-\t\t */\n+\tif (ttm_dma_tt_init(&nvbe->ttm, bo, page_flags)) {\n+\t\tkfree(nvbe);\n \t\treturn NULL;\n+\t}\n \treturn &nvbe->ttm.ttm;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (ttm_dma_tt_init(&nvbe->ttm, bo, page_flags)) {",
                "\t\tkfree(nvbe);",
                "\t}"
            ],
            "deleted": [
                "\tif (ttm_dma_tt_init(&nvbe->ttm, bo, page_flags))",
                "\t\t/*",
                "\t\t * A failing ttm_dma_tt_init() will call ttm_tt_destroy()",
                "\t\t * and thus our nouveau_sgdma_destroy() hook, so we don't need",
                "\t\t * to free nvbe here.",
                "\t\t */"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a flaw reported in the Linux kernel in versions before 5.9 in drivers/gpu/drm/nouveau/nouveau_sgdma.c in nouveau_sgdma_create_ttm in Nouveau DRM subsystem. The issue results from the lack of validating the existence of an object prior to performing operations on the object. An attacker with a local account with a root privilege, can leverage this vulnerability to escalate privileges and execute code in the context of the kernel.",
        "id": 2868
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n\n\t/* Grab a ref if this isn't our static identity */\n\treq->work.identity = tctx->identity;\n\tif (tctx->identity != &tctx->__identity)\n\t\trefcount_inc(&req->work.identity->count);\n}",
        "code_after_change": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,14 +1,7 @@\n static inline void io_req_init_async(struct io_kiocb *req)\n {\n-\tstruct io_uring_task *tctx = current->io_uring;\n-\n \tif (req->flags & REQ_F_WORK_INITIALIZED)\n \t\treturn;\n \n \t__io_req_init_async(req);\n-\n-\t/* Grab a ref if this isn't our static identity */\n-\treq->work.identity = tctx->identity;\n-\tif (tctx->identity != &tctx->__identity)\n-\t\trefcount_inc(&req->work.identity->count);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct io_uring_task *tctx = current->io_uring;",
                "",
                "",
                "\t/* Grab a ref if this isn't our static identity */",
                "\treq->work.identity = tctx->identity;",
                "\tif (tctx->identity != &tctx->__identity)",
                "\t\trefcount_inc(&req->work.identity->count);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3357
    },
    {
        "cve_id": "CVE-2023-4244",
        "code_before_change": "static int nf_tables_commit(struct net *net, struct sk_buff *skb)\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(net);\n\tstruct nft_trans *trans, *next;\n\tunsigned int base_seq, gc_seq;\n\tLIST_HEAD(set_update_list);\n\tstruct nft_trans_elem *te;\n\tstruct nft_chain *chain;\n\tstruct nft_table *table;\n\tLIST_HEAD(adl);\n\tint err;\n\n\tif (list_empty(&nft_net->commit_list)) {\n\t\tmutex_unlock(&nft_net->commit_mutex);\n\t\treturn 0;\n\t}\n\n\tlist_for_each_entry(trans, &nft_net->binding_list, binding_list) {\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (!nft_trans_set_update(trans) &&\n\t\t\t    nft_set_is_anonymous(nft_trans_set(trans)) &&\n\t\t\t    !nft_trans_set_bound(trans)) {\n\t\t\t\tpr_warn_once(\"nftables ruleset with unbound set\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (!nft_trans_chain_update(trans) &&\n\t\t\t    nft_chain_binding(nft_trans_chain(trans)) &&\n\t\t\t    !nft_trans_chain_bound(trans)) {\n\t\t\t\tpr_warn_once(\"nftables ruleset with unbound chain\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* 0. Validate ruleset, otherwise roll back for error reporting. */\n\tif (nf_tables_validate(net) < 0)\n\t\treturn -EAGAIN;\n\n\terr = nft_flow_rule_offload_commit(net);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* 1.  Allocate space for next generation rules_gen_X[] */\n\tlist_for_each_entry_safe(trans, next, &nft_net->commit_list, list) {\n\t\tint ret;\n\n\t\tret = nf_tables_commit_audit_alloc(&adl, trans->ctx.table);\n\t\tif (ret) {\n\t\t\tnf_tables_commit_chain_prepare_cancel(net);\n\t\t\tnf_tables_commit_audit_free(&adl);\n\t\t\treturn ret;\n\t\t}\n\t\tif (trans->msg_type == NFT_MSG_NEWRULE ||\n\t\t    trans->msg_type == NFT_MSG_DELRULE) {\n\t\t\tchain = trans->ctx.chain;\n\n\t\t\tret = nf_tables_commit_chain_prepare(net, chain);\n\t\t\tif (ret < 0) {\n\t\t\t\tnf_tables_commit_chain_prepare_cancel(net);\n\t\t\t\tnf_tables_commit_audit_free(&adl);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* step 2.  Make rules_gen_X visible to packet path */\n\tlist_for_each_entry(table, &nft_net->tables, list) {\n\t\tlist_for_each_entry(chain, &table->chains, list)\n\t\t\tnf_tables_commit_chain(net, chain);\n\t}\n\n\t/*\n\t * Bump generation counter, invalidate any dump in progress.\n\t * Cannot fail after this point.\n\t */\n\tbase_seq = READ_ONCE(nft_net->base_seq);\n\twhile (++base_seq == 0)\n\t\t;\n\n\tWRITE_ONCE(nft_net->base_seq, base_seq);\n\n\t/* Bump gc counter, it becomes odd, this is the busy mark. */\n\tgc_seq = READ_ONCE(nft_net->gc_seq);\n\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);\n\n\t/* step 3. Start new generation, rules_gen_X now in use. */\n\tnet->nft.gencursor = nft_gencursor_next(net);\n\n\tlist_for_each_entry_safe(trans, next, &nft_net->commit_list, list) {\n\t\tnf_tables_commit_audit_collect(&adl, trans->ctx.table,\n\t\t\t\t\t       trans->msg_type);\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWTABLE:\n\t\t\tif (nft_trans_table_update(trans)) {\n\t\t\t\tif (!(trans->ctx.table->flags & __NFT_TABLE_F_UPDATE)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (trans->ctx.table->flags & NFT_TABLE_F_DORMANT)\n\t\t\t\t\tnf_tables_table_disable(net, trans->ctx.table);\n\n\t\t\t\ttrans->ctx.table->flags &= ~__NFT_TABLE_F_UPDATE;\n\t\t\t} else {\n\t\t\t\tnft_clear(net, trans->ctx.table);\n\t\t\t}\n\t\t\tnf_tables_table_notify(&trans->ctx, NFT_MSG_NEWTABLE);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELTABLE:\n\t\tcase NFT_MSG_DESTROYTABLE:\n\t\t\tlist_del_rcu(&trans->ctx.table->list);\n\t\t\tnf_tables_table_notify(&trans->ctx, trans->msg_type);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnft_chain_commit_update(trans);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_NEWCHAIN,\n\t\t\t\t\t\t       &nft_trans_chain_hooks(trans));\n\t\t\t\tlist_splice(&nft_trans_chain_hooks(trans),\n\t\t\t\t\t    &nft_trans_basechain(trans)->hook_list);\n\t\t\t\t/* trans destroyed after rcu grace period */\n\t\t\t} else {\n\t\t\t\tnft_chain_commit_drop_policy(trans);\n\t\t\t\tnft_clear(net, trans->ctx.chain);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_NEWCHAIN, NULL);\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELCHAIN:\n\t\tcase NFT_MSG_DESTROYCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_DELCHAIN,\n\t\t\t\t\t\t       &nft_trans_chain_hooks(trans));\n\t\t\t\tnft_netdev_unregister_hooks(net,\n\t\t\t\t\t\t\t    &nft_trans_chain_hooks(trans),\n\t\t\t\t\t\t\t    true);\n\t\t\t} else {\n\t\t\t\tnft_chain_del(trans->ctx.chain);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_DELCHAIN,\n\t\t\t\t\t\t       NULL);\n\t\t\t\tnf_tables_unregister_hook(trans->ctx.net,\n\t\t\t\t\t\t\t  trans->ctx.table,\n\t\t\t\t\t\t\t  trans->ctx.chain);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWRULE:\n\t\t\tnft_clear(trans->ctx.net, nft_trans_rule(trans));\n\t\t\tnf_tables_rule_notify(&trans->ctx,\n\t\t\t\t\t      nft_trans_rule(trans),\n\t\t\t\t\t      NFT_MSG_NEWRULE);\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELRULE:\n\t\tcase NFT_MSG_DESTROYRULE:\n\t\t\tlist_del_rcu(&nft_trans_rule(trans)->list);\n\t\t\tnf_tables_rule_notify(&trans->ctx,\n\t\t\t\t\t      nft_trans_rule(trans),\n\t\t\t\t\t      trans->msg_type);\n\t\t\tnft_rule_expr_deactivate(&trans->ctx,\n\t\t\t\t\t\t nft_trans_rule(trans),\n\t\t\t\t\t\t NFT_TRANS_COMMIT);\n\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (nft_trans_set_update(trans)) {\n\t\t\t\tstruct nft_set *set = nft_trans_set(trans);\n\n\t\t\t\tWRITE_ONCE(set->timeout, nft_trans_set_timeout(trans));\n\t\t\t\tWRITE_ONCE(set->gc_int, nft_trans_set_gc_int(trans));\n\n\t\t\t\tif (nft_trans_set_size(trans))\n\t\t\t\t\tWRITE_ONCE(set->size, nft_trans_set_size(trans));\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_set(trans));\n\t\t\t\t/* This avoids hitting -EBUSY when deleting the table\n\t\t\t\t * from the transaction.\n\t\t\t\t */\n\t\t\t\tif (nft_set_is_anonymous(nft_trans_set(trans)) &&\n\t\t\t\t    !list_empty(&nft_trans_set(trans)->bindings))\n\t\t\t\t\tnft_use_dec(&trans->ctx.table->use);\n\t\t\t}\n\t\t\tnf_tables_set_notify(&trans->ctx, nft_trans_set(trans),\n\t\t\t\t\t     NFT_MSG_NEWSET, GFP_KERNEL);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSET:\n\t\tcase NFT_MSG_DESTROYSET:\n\t\t\tnft_trans_set(trans)->dead = 1;\n\t\t\tlist_del_rcu(&nft_trans_set(trans)->list);\n\t\t\tnf_tables_set_notify(&trans->ctx, nft_trans_set(trans),\n\t\t\t\t\t     trans->msg_type, GFP_KERNEL);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSETELEM:\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\n\t\t\tnft_setelem_activate(net, te->set, &te->elem);\n\t\t\tnf_tables_setelem_notify(&trans->ctx, te->set,\n\t\t\t\t\t\t &te->elem,\n\t\t\t\t\t\t NFT_MSG_NEWSETELEM);\n\t\t\tif (te->set->ops->commit &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSETELEM:\n\t\tcase NFT_MSG_DESTROYSETELEM:\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\n\t\t\tnf_tables_setelem_notify(&trans->ctx, te->set,\n\t\t\t\t\t\t &te->elem,\n\t\t\t\t\t\t trans->msg_type);\n\t\t\tnft_setelem_remove(net, te->set, &te->elem);\n\t\t\tif (!nft_setelem_is_catchall(te->set, &te->elem)) {\n\t\t\t\tatomic_dec(&te->set->nelems);\n\t\t\t\tte->set->ndeact--;\n\t\t\t}\n\t\t\tif (te->set->ops->commit &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWOBJ:\n\t\t\tif (nft_trans_obj_update(trans)) {\n\t\t\t\tnft_obj_commit_update(trans);\n\t\t\t\tnf_tables_obj_notify(&trans->ctx,\n\t\t\t\t\t\t     nft_trans_obj(trans),\n\t\t\t\t\t\t     NFT_MSG_NEWOBJ);\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_obj(trans));\n\t\t\t\tnf_tables_obj_notify(&trans->ctx,\n\t\t\t\t\t\t     nft_trans_obj(trans),\n\t\t\t\t\t\t     NFT_MSG_NEWOBJ);\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELOBJ:\n\t\tcase NFT_MSG_DESTROYOBJ:\n\t\t\tnft_obj_del(nft_trans_obj(trans));\n\t\t\tnf_tables_obj_notify(&trans->ctx, nft_trans_obj(trans),\n\t\t\t\t\t     trans->msg_type);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tnft_trans_flowtable(trans)->data.flags =\n\t\t\t\t\tnft_trans_flowtable_flags(trans);\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t\t\t   NFT_MSG_NEWFLOWTABLE);\n\t\t\t\tlist_splice(&nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t    &nft_trans_flowtable(trans)->hook_list);\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_flowtable(trans));\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   NULL,\n\t\t\t\t\t\t\t   NFT_MSG_NEWFLOWTABLE);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELFLOWTABLE:\n\t\tcase NFT_MSG_DESTROYFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t\t\t   trans->msg_type);\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans));\n\t\t\t} else {\n\t\t\t\tlist_del_rcu(&nft_trans_flowtable(trans)->list);\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   NULL,\n\t\t\t\t\t\t\t   trans->msg_type);\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t&nft_trans_flowtable(trans)->hook_list);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tnft_set_commit_update(&set_update_list);\n\n\tnft_commit_notify(net, NETLINK_CB(skb).portid);\n\tnf_tables_gen_notify(net, skb, NFT_MSG_NEWGEN);\n\tnf_tables_commit_audit_log(&adl, nft_net->base_seq);\n\n\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);\n\tnf_tables_commit_release(net);\n\n\treturn 0;\n}",
        "code_after_change": "static int nf_tables_commit(struct net *net, struct sk_buff *skb)\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(net);\n\tstruct nft_trans *trans, *next;\n\tunsigned int base_seq, gc_seq;\n\tLIST_HEAD(set_update_list);\n\tstruct nft_trans_elem *te;\n\tstruct nft_chain *chain;\n\tstruct nft_table *table;\n\tLIST_HEAD(adl);\n\tint err;\n\n\tif (list_empty(&nft_net->commit_list)) {\n\t\tmutex_unlock(&nft_net->commit_mutex);\n\t\treturn 0;\n\t}\n\n\tlist_for_each_entry(trans, &nft_net->binding_list, binding_list) {\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (!nft_trans_set_update(trans) &&\n\t\t\t    nft_set_is_anonymous(nft_trans_set(trans)) &&\n\t\t\t    !nft_trans_set_bound(trans)) {\n\t\t\t\tpr_warn_once(\"nftables ruleset with unbound set\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (!nft_trans_chain_update(trans) &&\n\t\t\t    nft_chain_binding(nft_trans_chain(trans)) &&\n\t\t\t    !nft_trans_chain_bound(trans)) {\n\t\t\t\tpr_warn_once(\"nftables ruleset with unbound chain\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* 0. Validate ruleset, otherwise roll back for error reporting. */\n\tif (nf_tables_validate(net) < 0)\n\t\treturn -EAGAIN;\n\n\terr = nft_flow_rule_offload_commit(net);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* 1.  Allocate space for next generation rules_gen_X[] */\n\tlist_for_each_entry_safe(trans, next, &nft_net->commit_list, list) {\n\t\tint ret;\n\n\t\tret = nf_tables_commit_audit_alloc(&adl, trans->ctx.table);\n\t\tif (ret) {\n\t\t\tnf_tables_commit_chain_prepare_cancel(net);\n\t\t\tnf_tables_commit_audit_free(&adl);\n\t\t\treturn ret;\n\t\t}\n\t\tif (trans->msg_type == NFT_MSG_NEWRULE ||\n\t\t    trans->msg_type == NFT_MSG_DELRULE) {\n\t\t\tchain = trans->ctx.chain;\n\n\t\t\tret = nf_tables_commit_chain_prepare(net, chain);\n\t\t\tif (ret < 0) {\n\t\t\t\tnf_tables_commit_chain_prepare_cancel(net);\n\t\t\t\tnf_tables_commit_audit_free(&adl);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* step 2.  Make rules_gen_X visible to packet path */\n\tlist_for_each_entry(table, &nft_net->tables, list) {\n\t\tlist_for_each_entry(chain, &table->chains, list)\n\t\t\tnf_tables_commit_chain(net, chain);\n\t}\n\n\t/*\n\t * Bump generation counter, invalidate any dump in progress.\n\t * Cannot fail after this point.\n\t */\n\tbase_seq = READ_ONCE(nft_net->base_seq);\n\twhile (++base_seq == 0)\n\t\t;\n\n\tWRITE_ONCE(nft_net->base_seq, base_seq);\n\n\tgc_seq = nft_gc_seq_begin(nft_net);\n\n\t/* step 3. Start new generation, rules_gen_X now in use. */\n\tnet->nft.gencursor = nft_gencursor_next(net);\n\n\tlist_for_each_entry_safe(trans, next, &nft_net->commit_list, list) {\n\t\tnf_tables_commit_audit_collect(&adl, trans->ctx.table,\n\t\t\t\t\t       trans->msg_type);\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWTABLE:\n\t\t\tif (nft_trans_table_update(trans)) {\n\t\t\t\tif (!(trans->ctx.table->flags & __NFT_TABLE_F_UPDATE)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (trans->ctx.table->flags & NFT_TABLE_F_DORMANT)\n\t\t\t\t\tnf_tables_table_disable(net, trans->ctx.table);\n\n\t\t\t\ttrans->ctx.table->flags &= ~__NFT_TABLE_F_UPDATE;\n\t\t\t} else {\n\t\t\t\tnft_clear(net, trans->ctx.table);\n\t\t\t}\n\t\t\tnf_tables_table_notify(&trans->ctx, NFT_MSG_NEWTABLE);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELTABLE:\n\t\tcase NFT_MSG_DESTROYTABLE:\n\t\t\tlist_del_rcu(&trans->ctx.table->list);\n\t\t\tnf_tables_table_notify(&trans->ctx, trans->msg_type);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnft_chain_commit_update(trans);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_NEWCHAIN,\n\t\t\t\t\t\t       &nft_trans_chain_hooks(trans));\n\t\t\t\tlist_splice(&nft_trans_chain_hooks(trans),\n\t\t\t\t\t    &nft_trans_basechain(trans)->hook_list);\n\t\t\t\t/* trans destroyed after rcu grace period */\n\t\t\t} else {\n\t\t\t\tnft_chain_commit_drop_policy(trans);\n\t\t\t\tnft_clear(net, trans->ctx.chain);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_NEWCHAIN, NULL);\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELCHAIN:\n\t\tcase NFT_MSG_DESTROYCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_DELCHAIN,\n\t\t\t\t\t\t       &nft_trans_chain_hooks(trans));\n\t\t\t\tnft_netdev_unregister_hooks(net,\n\t\t\t\t\t\t\t    &nft_trans_chain_hooks(trans),\n\t\t\t\t\t\t\t    true);\n\t\t\t} else {\n\t\t\t\tnft_chain_del(trans->ctx.chain);\n\t\t\t\tnf_tables_chain_notify(&trans->ctx, NFT_MSG_DELCHAIN,\n\t\t\t\t\t\t       NULL);\n\t\t\t\tnf_tables_unregister_hook(trans->ctx.net,\n\t\t\t\t\t\t\t  trans->ctx.table,\n\t\t\t\t\t\t\t  trans->ctx.chain);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWRULE:\n\t\t\tnft_clear(trans->ctx.net, nft_trans_rule(trans));\n\t\t\tnf_tables_rule_notify(&trans->ctx,\n\t\t\t\t\t      nft_trans_rule(trans),\n\t\t\t\t\t      NFT_MSG_NEWRULE);\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELRULE:\n\t\tcase NFT_MSG_DESTROYRULE:\n\t\t\tlist_del_rcu(&nft_trans_rule(trans)->list);\n\t\t\tnf_tables_rule_notify(&trans->ctx,\n\t\t\t\t\t      nft_trans_rule(trans),\n\t\t\t\t\t      trans->msg_type);\n\t\t\tnft_rule_expr_deactivate(&trans->ctx,\n\t\t\t\t\t\t nft_trans_rule(trans),\n\t\t\t\t\t\t NFT_TRANS_COMMIT);\n\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (nft_trans_set_update(trans)) {\n\t\t\t\tstruct nft_set *set = nft_trans_set(trans);\n\n\t\t\t\tWRITE_ONCE(set->timeout, nft_trans_set_timeout(trans));\n\t\t\t\tWRITE_ONCE(set->gc_int, nft_trans_set_gc_int(trans));\n\n\t\t\t\tif (nft_trans_set_size(trans))\n\t\t\t\t\tWRITE_ONCE(set->size, nft_trans_set_size(trans));\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_set(trans));\n\t\t\t\t/* This avoids hitting -EBUSY when deleting the table\n\t\t\t\t * from the transaction.\n\t\t\t\t */\n\t\t\t\tif (nft_set_is_anonymous(nft_trans_set(trans)) &&\n\t\t\t\t    !list_empty(&nft_trans_set(trans)->bindings))\n\t\t\t\t\tnft_use_dec(&trans->ctx.table->use);\n\t\t\t}\n\t\t\tnf_tables_set_notify(&trans->ctx, nft_trans_set(trans),\n\t\t\t\t\t     NFT_MSG_NEWSET, GFP_KERNEL);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSET:\n\t\tcase NFT_MSG_DESTROYSET:\n\t\t\tnft_trans_set(trans)->dead = 1;\n\t\t\tlist_del_rcu(&nft_trans_set(trans)->list);\n\t\t\tnf_tables_set_notify(&trans->ctx, nft_trans_set(trans),\n\t\t\t\t\t     trans->msg_type, GFP_KERNEL);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSETELEM:\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\n\t\t\tnft_setelem_activate(net, te->set, &te->elem);\n\t\t\tnf_tables_setelem_notify(&trans->ctx, te->set,\n\t\t\t\t\t\t &te->elem,\n\t\t\t\t\t\t NFT_MSG_NEWSETELEM);\n\t\t\tif (te->set->ops->commit &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSETELEM:\n\t\tcase NFT_MSG_DESTROYSETELEM:\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\n\t\t\tnf_tables_setelem_notify(&trans->ctx, te->set,\n\t\t\t\t\t\t &te->elem,\n\t\t\t\t\t\t trans->msg_type);\n\t\t\tnft_setelem_remove(net, te->set, &te->elem);\n\t\t\tif (!nft_setelem_is_catchall(te->set, &te->elem)) {\n\t\t\t\tatomic_dec(&te->set->nelems);\n\t\t\t\tte->set->ndeact--;\n\t\t\t}\n\t\t\tif (te->set->ops->commit &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWOBJ:\n\t\t\tif (nft_trans_obj_update(trans)) {\n\t\t\t\tnft_obj_commit_update(trans);\n\t\t\t\tnf_tables_obj_notify(&trans->ctx,\n\t\t\t\t\t\t     nft_trans_obj(trans),\n\t\t\t\t\t\t     NFT_MSG_NEWOBJ);\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_obj(trans));\n\t\t\t\tnf_tables_obj_notify(&trans->ctx,\n\t\t\t\t\t\t     nft_trans_obj(trans),\n\t\t\t\t\t\t     NFT_MSG_NEWOBJ);\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELOBJ:\n\t\tcase NFT_MSG_DESTROYOBJ:\n\t\t\tnft_obj_del(nft_trans_obj(trans));\n\t\t\tnf_tables_obj_notify(&trans->ctx, nft_trans_obj(trans),\n\t\t\t\t\t     trans->msg_type);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tnft_trans_flowtable(trans)->data.flags =\n\t\t\t\t\tnft_trans_flowtable_flags(trans);\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t\t\t   NFT_MSG_NEWFLOWTABLE);\n\t\t\t\tlist_splice(&nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t    &nft_trans_flowtable(trans)->hook_list);\n\t\t\t} else {\n\t\t\t\tnft_clear(net, nft_trans_flowtable(trans));\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   NULL,\n\t\t\t\t\t\t\t   NFT_MSG_NEWFLOWTABLE);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELFLOWTABLE:\n\t\tcase NFT_MSG_DESTROYFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t\t\t   trans->msg_type);\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t\t\t   &nft_trans_flowtable_hooks(trans));\n\t\t\t} else {\n\t\t\t\tlist_del_rcu(&nft_trans_flowtable(trans)->list);\n\t\t\t\tnf_tables_flowtable_notify(&trans->ctx,\n\t\t\t\t\t\t\t   nft_trans_flowtable(trans),\n\t\t\t\t\t\t\t   NULL,\n\t\t\t\t\t\t\t   trans->msg_type);\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t&nft_trans_flowtable(trans)->hook_list);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tnft_set_commit_update(&set_update_list);\n\n\tnft_commit_notify(net, NETLINK_CB(skb).portid);\n\tnf_tables_gen_notify(net, skb, NFT_MSG_NEWGEN);\n\tnf_tables_commit_audit_log(&adl, nft_net->base_seq);\n\n\tnft_gc_seq_end(nft_net, gc_seq);\n\tnf_tables_commit_release(net);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -83,9 +83,7 @@\n \n \tWRITE_ONCE(nft_net->base_seq, base_seq);\n \n-\t/* Bump gc counter, it becomes odd, this is the busy mark. */\n-\tgc_seq = READ_ONCE(nft_net->gc_seq);\n-\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);\n+\tgc_seq = nft_gc_seq_begin(nft_net);\n \n \t/* step 3. Start new generation, rules_gen_X now in use. */\n \tnet->nft.gencursor = nft_gencursor_next(net);\n@@ -298,7 +296,7 @@\n \tnf_tables_gen_notify(net, skb, NFT_MSG_NEWGEN);\n \tnf_tables_commit_audit_log(&adl, nft_net->base_seq);\n \n-\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);\n+\tnft_gc_seq_end(nft_net, gc_seq);\n \tnf_tables_commit_release(net);\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tgc_seq = nft_gc_seq_begin(nft_net);",
                "\tnft_gc_seq_end(nft_net, gc_seq);"
            ],
            "deleted": [
                "\t/* Bump gc counter, it becomes odd, this is the busy mark. */",
                "\tgc_seq = READ_ONCE(nft_net->gc_seq);",
                "\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);",
                "\tWRITE_ONCE(nft_net->gc_seq, ++gc_seq);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nDue to a race condition between nf_tables netlink control plane transaction and nft_set element garbage collection, it is possible to underflow the reference counter causing a use-after-free vulnerability.\n\nWe recommend upgrading past commit 3e91b0ebd994635df2346353322ac51ce84ce6d8.\n\n",
        "id": 4201
    },
    {
        "cve_id": "CVE-2022-1195",
        "code_before_change": "static void mkiss_close(struct tty_struct *tty)\n{\n\tstruct mkiss *ax;\n\n\twrite_lock_irq(&disc_data_lock);\n\tax = tty->disc_data;\n\ttty->disc_data = NULL;\n\twrite_unlock_irq(&disc_data_lock);\n\n\tif (!ax)\n\t\treturn;\n\n\t/*\n\t * We have now ensured that nobody can start using ap from now on, but\n\t * we have to wait for all existing users to finish.\n\t */\n\tif (!refcount_dec_and_test(&ax->refcnt))\n\t\twait_for_completion(&ax->dead);\n\t/*\n\t * Halt the transmit queue so that a new transmit cannot scribble\n\t * on our buffers\n\t */\n\tnetif_stop_queue(ax->dev);\n\n\tax->tty = NULL;\n\n\tunregister_netdev(ax->dev);\n\n\t/* Free all AX25 frame buffers after unreg. */\n\tkfree(ax->rbuff);\n\tkfree(ax->xbuff);\n\n\tfree_netdev(ax->dev);\n}",
        "code_after_change": "static void mkiss_close(struct tty_struct *tty)\n{\n\tstruct mkiss *ax;\n\n\twrite_lock_irq(&disc_data_lock);\n\tax = tty->disc_data;\n\ttty->disc_data = NULL;\n\twrite_unlock_irq(&disc_data_lock);\n\n\tif (!ax)\n\t\treturn;\n\n\t/*\n\t * We have now ensured that nobody can start using ap from now on, but\n\t * we have to wait for all existing users to finish.\n\t */\n\tif (!refcount_dec_and_test(&ax->refcnt))\n\t\twait_for_completion(&ax->dead);\n\t/*\n\t * Halt the transmit queue so that a new transmit cannot scribble\n\t * on our buffers\n\t */\n\tnetif_stop_queue(ax->dev);\n\n\tunregister_netdev(ax->dev);\n\n\t/* Free all AX25 frame buffers after unreg. */\n\tkfree(ax->rbuff);\n\tkfree(ax->xbuff);\n\n\tax->tty = NULL;\n\n\tfree_netdev(ax->dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,13 +22,13 @@\n \t */\n \tnetif_stop_queue(ax->dev);\n \n-\tax->tty = NULL;\n-\n \tunregister_netdev(ax->dev);\n \n \t/* Free all AX25 frame buffers after unreg. */\n \tkfree(ax->rbuff);\n \tkfree(ax->xbuff);\n \n+\tax->tty = NULL;\n+\n \tfree_netdev(ax->dev);\n }",
        "function_modified_lines": {
            "added": [
                "\tax->tty = NULL;",
                ""
            ],
            "deleted": [
                "\tax->tty = NULL;",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in the Linux kernel in drivers/net/hamradio. This flaw allows a local attacker with a user privilege to cause a denial of service (DOS) when the mkiss or sixpack device is detached and reclaim resources early.",
        "id": 3252
    },
    {
        "cve_id": "CVE-2022-2938",
        "code_before_change": "static void cgroup_pressure_release(struct kernfs_open_file *of)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\n\tpsi_trigger_replace(&ctx->psi.trigger, NULL);\n}",
        "code_after_change": "static void cgroup_pressure_release(struct kernfs_open_file *of)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\n\tpsi_trigger_destroy(ctx->psi.trigger);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,5 +2,5 @@\n {\n \tstruct cgroup_file_ctx *ctx = of->priv;\n \n-\tpsi_trigger_replace(&ctx->psi.trigger, NULL);\n+\tpsi_trigger_destroy(ctx->psi.trigger);\n }",
        "function_modified_lines": {
            "added": [
                "\tpsi_trigger_destroy(ctx->psi.trigger);"
            ],
            "deleted": [
                "\tpsi_trigger_replace(&ctx->psi.trigger, NULL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's implementation of Pressure Stall Information. While the feature is disabled by default, it could allow an attacker to crash the system or have other memory-corruption side effects.",
        "id": 3515
    },
    {
        "cve_id": "CVE-2022-1882",
        "code_before_change": "int add_watch_to_object(struct watch *watch, struct watch_list *wlist)\n{\n\tstruct watch_queue *wqueue = rcu_access_pointer(watch->queue);\n\tstruct watch *w;\n\n\thlist_for_each_entry(w, &wlist->watchers, list_node) {\n\t\tstruct watch_queue *wq = rcu_access_pointer(w->queue);\n\t\tif (wqueue == wq && watch->id == w->id)\n\t\t\treturn -EBUSY;\n\t}\n\n\twatch->cred = get_current_cred();\n\trcu_assign_pointer(watch->watch_list, wlist);\n\n\tif (atomic_inc_return(&watch->cred->user->nr_watches) >\n\t    task_rlimit(current, RLIMIT_NOFILE)) {\n\t\tatomic_dec(&watch->cred->user->nr_watches);\n\t\tput_cred(watch->cred);\n\t\treturn -EAGAIN;\n\t}\n\n\tspin_lock_bh(&wqueue->lock);\n\tkref_get(&wqueue->usage);\n\tkref_get(&watch->usage);\n\thlist_add_head(&watch->queue_node, &wqueue->watches);\n\tspin_unlock_bh(&wqueue->lock);\n\n\thlist_add_head(&watch->list_node, &wlist->watchers);\n\treturn 0;\n}",
        "code_after_change": "int add_watch_to_object(struct watch *watch, struct watch_list *wlist)\n{\n\tstruct watch_queue *wqueue = rcu_access_pointer(watch->queue);\n\tstruct watch *w;\n\n\thlist_for_each_entry(w, &wlist->watchers, list_node) {\n\t\tstruct watch_queue *wq = rcu_access_pointer(w->queue);\n\t\tif (wqueue == wq && watch->id == w->id)\n\t\t\treturn -EBUSY;\n\t}\n\n\twatch->cred = get_current_cred();\n\trcu_assign_pointer(watch->watch_list, wlist);\n\n\tif (atomic_inc_return(&watch->cred->user->nr_watches) >\n\t    task_rlimit(current, RLIMIT_NOFILE)) {\n\t\tatomic_dec(&watch->cred->user->nr_watches);\n\t\tput_cred(watch->cred);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (lock_wqueue(wqueue)) {\n\t\tkref_get(&wqueue->usage);\n\t\tkref_get(&watch->usage);\n\t\thlist_add_head(&watch->queue_node, &wqueue->watches);\n\t\tunlock_wqueue(wqueue);\n\t}\n\n\thlist_add_head(&watch->list_node, &wlist->watchers);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,11 +19,12 @@\n \t\treturn -EAGAIN;\n \t}\n \n-\tspin_lock_bh(&wqueue->lock);\n-\tkref_get(&wqueue->usage);\n-\tkref_get(&watch->usage);\n-\thlist_add_head(&watch->queue_node, &wqueue->watches);\n-\tspin_unlock_bh(&wqueue->lock);\n+\tif (lock_wqueue(wqueue)) {\n+\t\tkref_get(&wqueue->usage);\n+\t\tkref_get(&watch->usage);\n+\t\thlist_add_head(&watch->queue_node, &wqueue->watches);\n+\t\tunlock_wqueue(wqueue);\n+\t}\n \n \thlist_add_head(&watch->list_node, &wlist->watchers);\n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tif (lock_wqueue(wqueue)) {",
                "\t\tkref_get(&wqueue->usage);",
                "\t\tkref_get(&watch->usage);",
                "\t\thlist_add_head(&watch->queue_node, &wqueue->watches);",
                "\t\tunlock_wqueue(wqueue);",
                "\t}"
            ],
            "deleted": [
                "\tspin_lock_bh(&wqueue->lock);",
                "\tkref_get(&wqueue->usage);",
                "\tkref_get(&watch->usage);",
                "\thlist_add_head(&watch->queue_node, &wqueue->watches);",
                "\tspin_unlock_bh(&wqueue->lock);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s pipes functionality in how a user performs manipulations with the pipe post_one_notification() after free_pipe_info() that is already called. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3296
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "static bool io_poll_disarm(struct io_kiocb *req)\n{\n\tif (!io_poll_get_ownership(req))\n\t\treturn false;\n\tio_poll_remove_entries(req);\n\thash_del(&req->hash_node);\n\treturn true;\n}",
        "code_after_change": "static int io_poll_disarm(struct io_kiocb *req)\n{\n\tif (!req)\n\t\treturn -ENOENT;\n\tif (!io_poll_get_ownership(req))\n\t\treturn -EALREADY;\n\tio_poll_remove_entries(req);\n\thash_del(&req->hash_node);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,10 @@\n-static bool io_poll_disarm(struct io_kiocb *req)\n+static int io_poll_disarm(struct io_kiocb *req)\n {\n+\tif (!req)\n+\t\treturn -ENOENT;\n \tif (!io_poll_get_ownership(req))\n-\t\treturn false;\n+\t\treturn -EALREADY;\n \tio_poll_remove_entries(req);\n \thash_del(&req->hash_node);\n-\treturn true;\n+\treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "static int io_poll_disarm(struct io_kiocb *req)",
                "\tif (!req)",
                "\t\treturn -ENOENT;",
                "\t\treturn -EALREADY;",
                "\treturn 0;"
            ],
            "deleted": [
                "static bool io_poll_disarm(struct io_kiocb *req)",
                "\t\treturn false;",
                "\treturn true;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and 0e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4074
    },
    {
        "cve_id": "CVE-2017-8824",
        "code_before_change": "int dccp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err = 0;\n\tconst int old_state = sk->sk_state;\n\n\tif (old_state != DCCP_CLOSED)\n\t\tdccp_set_state(sk, DCCP_CLOSED);\n\n\t/*\n\t * This corresponds to the ABORT function of RFC793, sec. 3.8\n\t * TCP uses a RST segment, DCCP a Reset packet with Code 2, \"Aborted\".\n\t */\n\tif (old_state == DCCP_LISTEN) {\n\t\tinet_csk_listen_stop(sk);\n\t} else if (dccp_need_reset(old_state)) {\n\t\tdccp_send_reset(sk, DCCP_RESET_CODE_ABORTED);\n\t\tsk->sk_err = ECONNRESET;\n\t} else if (old_state == DCCP_REQUESTING)\n\t\tsk->sk_err = ECONNRESET;\n\n\tdccp_clear_xmit_timers(sk);\n\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\t__skb_queue_purge(&sk->sk_write_queue);\n\tif (sk->sk_send_head != NULL) {\n\t\t__kfree_skb(sk->sk_send_head);\n\t\tsk->sk_send_head = NULL;\n\t}\n\n\tinet->inet_dport = 0;\n\n\tif (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK))\n\t\tinet_reset_saddr(sk);\n\n\tsk->sk_shutdown = 0;\n\tsock_reset_flag(sk, SOCK_DONE);\n\n\ticsk->icsk_backoff = 0;\n\tinet_csk_delack_init(sk);\n\t__sk_dst_reset(sk);\n\n\tWARN_ON(inet->inet_num && !icsk->icsk_bind_hash);\n\n\tsk->sk_error_report(sk);\n\treturn err;\n}",
        "code_after_change": "int dccp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tint err = 0;\n\tconst int old_state = sk->sk_state;\n\n\tif (old_state != DCCP_CLOSED)\n\t\tdccp_set_state(sk, DCCP_CLOSED);\n\n\t/*\n\t * This corresponds to the ABORT function of RFC793, sec. 3.8\n\t * TCP uses a RST segment, DCCP a Reset packet with Code 2, \"Aborted\".\n\t */\n\tif (old_state == DCCP_LISTEN) {\n\t\tinet_csk_listen_stop(sk);\n\t} else if (dccp_need_reset(old_state)) {\n\t\tdccp_send_reset(sk, DCCP_RESET_CODE_ABORTED);\n\t\tsk->sk_err = ECONNRESET;\n\t} else if (old_state == DCCP_REQUESTING)\n\t\tsk->sk_err = ECONNRESET;\n\n\tdccp_clear_xmit_timers(sk);\n\tccid_hc_rx_delete(dp->dccps_hc_rx_ccid, sk);\n\tccid_hc_tx_delete(dp->dccps_hc_tx_ccid, sk);\n\tdp->dccps_hc_rx_ccid = NULL;\n\tdp->dccps_hc_tx_ccid = NULL;\n\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\t__skb_queue_purge(&sk->sk_write_queue);\n\tif (sk->sk_send_head != NULL) {\n\t\t__kfree_skb(sk->sk_send_head);\n\t\tsk->sk_send_head = NULL;\n\t}\n\n\tinet->inet_dport = 0;\n\n\tif (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK))\n\t\tinet_reset_saddr(sk);\n\n\tsk->sk_shutdown = 0;\n\tsock_reset_flag(sk, SOCK_DONE);\n\n\ticsk->icsk_backoff = 0;\n\tinet_csk_delack_init(sk);\n\t__sk_dst_reset(sk);\n\n\tWARN_ON(inet->inet_num && !icsk->icsk_bind_hash);\n\n\tsk->sk_error_report(sk);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n {\n \tstruct inet_connection_sock *icsk = inet_csk(sk);\n \tstruct inet_sock *inet = inet_sk(sk);\n+\tstruct dccp_sock *dp = dccp_sk(sk);\n \tint err = 0;\n \tconst int old_state = sk->sk_state;\n \n@@ -21,6 +22,10 @@\n \t\tsk->sk_err = ECONNRESET;\n \n \tdccp_clear_xmit_timers(sk);\n+\tccid_hc_rx_delete(dp->dccps_hc_rx_ccid, sk);\n+\tccid_hc_tx_delete(dp->dccps_hc_tx_ccid, sk);\n+\tdp->dccps_hc_rx_ccid = NULL;\n+\tdp->dccps_hc_tx_ccid = NULL;\n \n \t__skb_queue_purge(&sk->sk_receive_queue);\n \t__skb_queue_purge(&sk->sk_write_queue);",
        "function_modified_lines": {
            "added": [
                "\tstruct dccp_sock *dp = dccp_sk(sk);",
                "\tccid_hc_rx_delete(dp->dccps_hc_rx_ccid, sk);",
                "\tccid_hc_tx_delete(dp->dccps_hc_tx_ccid, sk);",
                "\tdp->dccps_hc_rx_ccid = NULL;",
                "\tdp->dccps_hc_tx_ccid = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The dccp_disconnect function in net/dccp/proto.c in the Linux kernel through 4.14.3 allows local users to gain privileges or cause a denial of service (use-after-free) via an AF_UNSPEC connect system call during the DCCP_LISTEN state.",
        "id": 1554
    },
    {
        "cve_id": "CVE-2022-1048",
        "code_before_change": "static int snd_pcm_hw_free(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tint result;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn -ENXIO;\n\truntime = substream->runtime;\n\tsnd_pcm_stream_lock_irq(substream);\n\tswitch (runtime->status->state) {\n\tcase SNDRV_PCM_STATE_SETUP:\n\tcase SNDRV_PCM_STATE_PREPARED:\n\t\tbreak;\n\tdefault:\n\t\tsnd_pcm_stream_unlock_irq(substream);\n\t\treturn -EBADFD;\n\t}\n\tsnd_pcm_stream_unlock_irq(substream);\n\tif (atomic_read(&substream->mmap_count))\n\t\treturn -EBADFD;\n\tresult = do_hw_free(substream);\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n\tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n\treturn result;\n}",
        "code_after_change": "static int snd_pcm_hw_free(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tint result = 0;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn -ENXIO;\n\truntime = substream->runtime;\n\tmutex_lock(&runtime->buffer_mutex);\n\tsnd_pcm_stream_lock_irq(substream);\n\tswitch (runtime->status->state) {\n\tcase SNDRV_PCM_STATE_SETUP:\n\tcase SNDRV_PCM_STATE_PREPARED:\n\t\tif (atomic_read(&substream->mmap_count))\n\t\t\tresult = -EBADFD;\n\t\tbreak;\n\tdefault:\n\t\tresult = -EBADFD;\n\t\tbreak;\n\t}\n\tsnd_pcm_stream_unlock_irq(substream);\n\tif (result)\n\t\tgoto unlock;\n\tresult = do_hw_free(substream);\n\tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n\tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n unlock:\n\tmutex_unlock(&runtime->buffer_mutex);\n\treturn result;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,25 +1,30 @@\n static int snd_pcm_hw_free(struct snd_pcm_substream *substream)\n {\n \tstruct snd_pcm_runtime *runtime;\n-\tint result;\n+\tint result = 0;\n \n \tif (PCM_RUNTIME_CHECK(substream))\n \t\treturn -ENXIO;\n \truntime = substream->runtime;\n+\tmutex_lock(&runtime->buffer_mutex);\n \tsnd_pcm_stream_lock_irq(substream);\n \tswitch (runtime->status->state) {\n \tcase SNDRV_PCM_STATE_SETUP:\n \tcase SNDRV_PCM_STATE_PREPARED:\n+\t\tif (atomic_read(&substream->mmap_count))\n+\t\t\tresult = -EBADFD;\n \t\tbreak;\n \tdefault:\n-\t\tsnd_pcm_stream_unlock_irq(substream);\n-\t\treturn -EBADFD;\n+\t\tresult = -EBADFD;\n+\t\tbreak;\n \t}\n \tsnd_pcm_stream_unlock_irq(substream);\n-\tif (atomic_read(&substream->mmap_count))\n-\t\treturn -EBADFD;\n+\tif (result)\n+\t\tgoto unlock;\n \tresult = do_hw_free(substream);\n \tsnd_pcm_set_state(substream, SNDRV_PCM_STATE_OPEN);\n \tcpu_latency_qos_remove_request(&substream->latency_pm_qos_req);\n+ unlock:\n+\tmutex_unlock(&runtime->buffer_mutex);\n \treturn result;\n }",
        "function_modified_lines": {
            "added": [
                "\tint result = 0;",
                "\tmutex_lock(&runtime->buffer_mutex);",
                "\t\tif (atomic_read(&substream->mmap_count))",
                "\t\t\tresult = -EBADFD;",
                "\t\tresult = -EBADFD;",
                "\t\tbreak;",
                "\tif (result)",
                "\t\tgoto unlock;",
                " unlock:",
                "\tmutex_unlock(&runtime->buffer_mutex);"
            ],
            "deleted": [
                "\tint result;",
                "\t\tsnd_pcm_stream_unlock_irq(substream);",
                "\t\treturn -EBADFD;",
                "\tif (atomic_read(&substream->mmap_count))",
                "\t\treturn -EBADFD;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s sound subsystem in the way a user triggers concurrent calls of PCM hw_params. The hw_free ioctls or similar race condition happens inside ALSA PCM for other ioctls. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3244
    },
    {
        "cve_id": "CVE-2022-32250",
        "code_before_change": "struct nft_expr *nft_set_elem_expr_alloc(const struct nft_ctx *ctx,\n\t\t\t\t\t const struct nft_set *set,\n\t\t\t\t\t const struct nlattr *attr)\n{\n\tstruct nft_expr *expr;\n\tint err;\n\n\texpr = nft_expr_init(ctx, attr);\n\tif (IS_ERR(expr))\n\t\treturn expr;\n\n\terr = -EOPNOTSUPP;\n\tif (!(expr->ops->type->flags & NFT_EXPR_STATEFUL))\n\t\tgoto err_set_elem_expr;\n\n\tif (expr->ops->type->flags & NFT_EXPR_GC) {\n\t\tif (set->flags & NFT_SET_TIMEOUT)\n\t\t\tgoto err_set_elem_expr;\n\t\tif (!set->ops->gc_init)\n\t\t\tgoto err_set_elem_expr;\n\t\tset->ops->gc_init(set);\n\t}\n\n\treturn expr;\n\nerr_set_elem_expr:\n\tnft_expr_destroy(ctx, expr);\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "struct nft_expr *nft_set_elem_expr_alloc(const struct nft_ctx *ctx,\n\t\t\t\t\t const struct nft_set *set,\n\t\t\t\t\t const struct nlattr *attr)\n{\n\tstruct nft_expr *expr;\n\tint err;\n\n\texpr = nft_expr_init(ctx, attr);\n\tif (IS_ERR(expr))\n\t\treturn expr;\n\n\terr = -EOPNOTSUPP;\n\tif (expr->ops->type->flags & NFT_EXPR_GC) {\n\t\tif (set->flags & NFT_SET_TIMEOUT)\n\t\t\tgoto err_set_elem_expr;\n\t\tif (!set->ops->gc_init)\n\t\t\tgoto err_set_elem_expr;\n\t\tset->ops->gc_init(set);\n\t}\n\n\treturn expr;\n\nerr_set_elem_expr:\n\tnft_expr_destroy(ctx, expr);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,9 +10,6 @@\n \t\treturn expr;\n \n \terr = -EOPNOTSUPP;\n-\tif (!(expr->ops->type->flags & NFT_EXPR_STATEFUL))\n-\t\tgoto err_set_elem_expr;\n-\n \tif (expr->ops->type->flags & NFT_EXPR_GC) {\n \t\tif (set->flags & NFT_SET_TIMEOUT)\n \t\t\tgoto err_set_elem_expr;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tif (!(expr->ops->type->flags & NFT_EXPR_STATEFUL))",
                "\t\tgoto err_set_elem_expr;",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "net/netfilter/nf_tables_api.c in the Linux kernel through 5.18.1 allows a local user (able to create user/net namespaces) to escalate privileges to root because an incorrect NFT_STATEFUL_EXPR check leads to a use-after-free.",
        "id": 3568
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "int nfc_llcp_send_symm(struct nfc_dev *dev)\n{\n\tstruct sk_buff *skb;\n\tstruct nfc_llcp_local *local;\n\tu16 size = 0;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\tsize += LLCP_HEADER_SIZE;\n\tsize += dev->tx_headroom + dev->tx_tailroom + NFC_HEADER_SIZE;\n\n\tskb = alloc_skb(size, GFP_KERNEL);\n\tif (skb == NULL)\n\t\treturn -ENOMEM;\n\n\tskb_reserve(skb, dev->tx_headroom + NFC_HEADER_SIZE);\n\n\tskb = llcp_add_header(skb, 0, 0, LLCP_PDU_SYMM);\n\n\t__net_timestamp(skb);\n\n\tnfc_llcp_send_to_raw_sock(local, skb, NFC_DIRECTION_TX);\n\n\treturn nfc_data_exchange(dev, local->target_idx, skb,\n\t\t\t\t nfc_llcp_recv, local);\n}",
        "code_after_change": "int nfc_llcp_send_symm(struct nfc_dev *dev)\n{\n\tstruct sk_buff *skb;\n\tstruct nfc_llcp_local *local;\n\tu16 size = 0;\n\tint err;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\tsize += LLCP_HEADER_SIZE;\n\tsize += dev->tx_headroom + dev->tx_tailroom + NFC_HEADER_SIZE;\n\n\tskb = alloc_skb(size, GFP_KERNEL);\n\tif (skb == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tskb_reserve(skb, dev->tx_headroom + NFC_HEADER_SIZE);\n\n\tskb = llcp_add_header(skb, 0, 0, LLCP_PDU_SYMM);\n\n\t__net_timestamp(skb);\n\n\tnfc_llcp_send_to_raw_sock(local, skb, NFC_DIRECTION_TX);\n\n\terr = nfc_data_exchange(dev, local->target_idx, skb,\n\t\t\t\t nfc_llcp_recv, local);\nout:\n\tnfc_llcp_local_put(local);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n \tstruct sk_buff *skb;\n \tstruct nfc_llcp_local *local;\n \tu16 size = 0;\n+\tint err;\n \n \tlocal = nfc_llcp_find_local(dev);\n \tif (local == NULL)\n@@ -12,8 +13,10 @@\n \tsize += dev->tx_headroom + dev->tx_tailroom + NFC_HEADER_SIZE;\n \n \tskb = alloc_skb(size, GFP_KERNEL);\n-\tif (skb == NULL)\n-\t\treturn -ENOMEM;\n+\tif (skb == NULL) {\n+\t\terr = -ENOMEM;\n+\t\tgoto out;\n+\t}\n \n \tskb_reserve(skb, dev->tx_headroom + NFC_HEADER_SIZE);\n \n@@ -23,6 +26,9 @@\n \n \tnfc_llcp_send_to_raw_sock(local, skb, NFC_DIRECTION_TX);\n \n-\treturn nfc_data_exchange(dev, local->target_idx, skb,\n+\terr = nfc_data_exchange(dev, local->target_idx, skb,\n \t\t\t\t nfc_llcp_recv, local);\n+out:\n+\tnfc_llcp_local_put(local);\n+\treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tint err;",
                "\tif (skb == NULL) {",
                "\t\terr = -ENOMEM;",
                "\t\tgoto out;",
                "\t}",
                "\terr = nfc_data_exchange(dev, local->target_idx, skb,",
                "out:",
                "\tnfc_llcp_local_put(local);",
                "\treturn err;"
            ],
            "deleted": [
                "\tif (skb == NULL)",
                "\t\treturn -ENOMEM;",
                "\treturn nfc_data_exchange(dev, local->target_idx, skb,"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4143
    },
    {
        "cve_id": "CVE-2017-17975",
        "code_before_change": "static int usbtv_probe(struct usb_interface *intf,\n\tconst struct usb_device_id *id)\n{\n\tint ret;\n\tint size;\n\tstruct device *dev = &intf->dev;\n\tstruct usbtv *usbtv;\n\tstruct usb_host_endpoint *ep;\n\n\t/* Checks that the device is what we think it is. */\n\tif (intf->num_altsetting != 2)\n\t\treturn -ENODEV;\n\tif (intf->altsetting[1].desc.bNumEndpoints != 4)\n\t\treturn -ENODEV;\n\n\tep = &intf->altsetting[1].endpoint[0];\n\n\t/* Packet size is split into 11 bits of base size and count of\n\t * extra multiplies of it.*/\n\tsize = usb_endpoint_maxp(&ep->desc);\n\tsize = size * usb_endpoint_maxp_mult(&ep->desc);\n\n\t/* Device structure */\n\tusbtv = kzalloc(sizeof(struct usbtv), GFP_KERNEL);\n\tif (usbtv == NULL)\n\t\treturn -ENOMEM;\n\tusbtv->dev = dev;\n\tusbtv->udev = usb_get_dev(interface_to_usbdev(intf));\n\n\tusbtv->iso_size = size;\n\n\tusb_set_intfdata(intf, usbtv);\n\n\tret = usbtv_video_init(usbtv);\n\tif (ret < 0)\n\t\tgoto usbtv_video_fail;\n\n\tret = usbtv_audio_init(usbtv);\n\tif (ret < 0)\n\t\tgoto usbtv_audio_fail;\n\n\t/* for simplicity we exploit the v4l2_device reference counting */\n\tv4l2_device_get(&usbtv->v4l2_dev);\n\n\tdev_info(dev, \"Fushicai USBTV007 Audio-Video Grabber\\n\");\n\treturn 0;\n\nusbtv_audio_fail:\n\tusbtv_video_free(usbtv);\n\nusbtv_video_fail:\n\tusb_set_intfdata(intf, NULL);\n\tusb_put_dev(usbtv->udev);\n\tkfree(usbtv);\n\n\treturn ret;\n}",
        "code_after_change": "static int usbtv_probe(struct usb_interface *intf,\n\tconst struct usb_device_id *id)\n{\n\tint ret;\n\tint size;\n\tstruct device *dev = &intf->dev;\n\tstruct usbtv *usbtv;\n\tstruct usb_host_endpoint *ep;\n\n\t/* Checks that the device is what we think it is. */\n\tif (intf->num_altsetting != 2)\n\t\treturn -ENODEV;\n\tif (intf->altsetting[1].desc.bNumEndpoints != 4)\n\t\treturn -ENODEV;\n\n\tep = &intf->altsetting[1].endpoint[0];\n\n\t/* Packet size is split into 11 bits of base size and count of\n\t * extra multiplies of it.*/\n\tsize = usb_endpoint_maxp(&ep->desc);\n\tsize = size * usb_endpoint_maxp_mult(&ep->desc);\n\n\t/* Device structure */\n\tusbtv = kzalloc(sizeof(struct usbtv), GFP_KERNEL);\n\tif (usbtv == NULL)\n\t\treturn -ENOMEM;\n\tusbtv->dev = dev;\n\tusbtv->udev = usb_get_dev(interface_to_usbdev(intf));\n\n\tusbtv->iso_size = size;\n\n\tusb_set_intfdata(intf, usbtv);\n\n\tret = usbtv_video_init(usbtv);\n\tif (ret < 0)\n\t\tgoto usbtv_video_fail;\n\n\tret = usbtv_audio_init(usbtv);\n\tif (ret < 0)\n\t\tgoto usbtv_audio_fail;\n\n\t/* for simplicity we exploit the v4l2_device reference counting */\n\tv4l2_device_get(&usbtv->v4l2_dev);\n\n\tdev_info(dev, \"Fushicai USBTV007 Audio-Video Grabber\\n\");\n\treturn 0;\n\nusbtv_audio_fail:\n\t/* we must not free at this point */\n\tusb_get_dev(usbtv->udev);\n\tusbtv_video_free(usbtv);\n\nusbtv_video_fail:\n\tusb_set_intfdata(intf, NULL);\n\tusb_put_dev(usbtv->udev);\n\tkfree(usbtv);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -46,6 +46,8 @@\n \treturn 0;\n \n usbtv_audio_fail:\n+\t/* we must not free at this point */\n+\tusb_get_dev(usbtv->udev);\n \tusbtv_video_free(usbtv);\n \n usbtv_video_fail:",
        "function_modified_lines": {
            "added": [
                "\t/* we must not free at this point */",
                "\tusb_get_dev(usbtv->udev);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free in the usbtv_probe function in drivers/media/usb/usbtv/usbtv-core.c in the Linux kernel through 4.14.10 allows attackers to cause a denial of service (system crash) or possibly have unspecified other impact by triggering failure of audio registration, because a kfree of the usbtv data structure occurs during a usbtv_video_free call, but the usbtv_video_fail label's code attempts to both access and free this data structure.",
        "id": 1386
    },
    {
        "cve_id": "CVE-2019-25044",
        "code_before_change": "void blk_cleanup_queue(struct request_queue *q)\n{\n\t/* mark @q DYING, no new request or merges will be allowed afterwards */\n\tmutex_lock(&q->sysfs_lock);\n\tblk_set_queue_dying(q);\n\n\tblk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);\n\tblk_queue_flag_set(QUEUE_FLAG_NOXMERGES, q);\n\tblk_queue_flag_set(QUEUE_FLAG_DYING, q);\n\tmutex_unlock(&q->sysfs_lock);\n\n\t/*\n\t * Drain all requests queued before DYING marking. Set DEAD flag to\n\t * prevent that q->request_fn() gets invoked after draining finished.\n\t */\n\tblk_freeze_queue(q);\n\n\trq_qos_exit(q);\n\n\tblk_queue_flag_set(QUEUE_FLAG_DEAD, q);\n\n\t/* for synchronous bio-based driver finish in-flight integrity i/o */\n\tblk_flush_integrity();\n\n\t/* @q won't process any more request, flush async actions */\n\tdel_timer_sync(&q->backing_dev_info->laptop_mode_wb_timer);\n\tblk_sync_queue(q);\n\n\tif (queue_is_mq(q))\n\t\tblk_mq_exit_queue(q);\n\n\tpercpu_ref_exit(&q->q_usage_counter);\n\n\t/* @q is and will stay empty, shutdown and put */\n\tblk_put_queue(q);\n}",
        "code_after_change": "void blk_cleanup_queue(struct request_queue *q)\n{\n\t/* mark @q DYING, no new request or merges will be allowed afterwards */\n\tmutex_lock(&q->sysfs_lock);\n\tblk_set_queue_dying(q);\n\n\tblk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);\n\tblk_queue_flag_set(QUEUE_FLAG_NOXMERGES, q);\n\tblk_queue_flag_set(QUEUE_FLAG_DYING, q);\n\tmutex_unlock(&q->sysfs_lock);\n\n\t/*\n\t * Drain all requests queued before DYING marking. Set DEAD flag to\n\t * prevent that q->request_fn() gets invoked after draining finished.\n\t */\n\tblk_freeze_queue(q);\n\n\trq_qos_exit(q);\n\n\tblk_queue_flag_set(QUEUE_FLAG_DEAD, q);\n\n\t/* for synchronous bio-based driver finish in-flight integrity i/o */\n\tblk_flush_integrity();\n\n\t/* @q won't process any more request, flush async actions */\n\tdel_timer_sync(&q->backing_dev_info->laptop_mode_wb_timer);\n\tblk_sync_queue(q);\n\n\tif (queue_is_mq(q))\n\t\tblk_mq_exit_queue(q);\n\n\t/*\n\t * In theory, request pool of sched_tags belongs to request queue.\n\t * However, the current implementation requires tag_set for freeing\n\t * requests, so free the pool now.\n\t *\n\t * Queue has become frozen, there can't be any in-queue requests, so\n\t * it is safe to free requests now.\n\t */\n\tmutex_lock(&q->sysfs_lock);\n\tif (q->elevator)\n\t\tblk_mq_sched_free_requests(q);\n\tmutex_unlock(&q->sysfs_lock);\n\n\tpercpu_ref_exit(&q->q_usage_counter);\n\n\t/* @q is and will stay empty, shutdown and put */\n\tblk_put_queue(q);\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,6 +29,19 @@\n \tif (queue_is_mq(q))\n \t\tblk_mq_exit_queue(q);\n \n+\t/*\n+\t * In theory, request pool of sched_tags belongs to request queue.\n+\t * However, the current implementation requires tag_set for freeing\n+\t * requests, so free the pool now.\n+\t *\n+\t * Queue has become frozen, there can't be any in-queue requests, so\n+\t * it is safe to free requests now.\n+\t */\n+\tmutex_lock(&q->sysfs_lock);\n+\tif (q->elevator)\n+\t\tblk_mq_sched_free_requests(q);\n+\tmutex_unlock(&q->sysfs_lock);\n+\n \tpercpu_ref_exit(&q->q_usage_counter);\n \n \t/* @q is and will stay empty, shutdown and put */",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * In theory, request pool of sched_tags belongs to request queue.",
                "\t * However, the current implementation requires tag_set for freeing",
                "\t * requests, so free the pool now.",
                "\t *",
                "\t * Queue has become frozen, there can't be any in-queue requests, so",
                "\t * it is safe to free requests now.",
                "\t */",
                "\tmutex_lock(&q->sysfs_lock);",
                "\tif (q->elevator)",
                "\t\tblk_mq_sched_free_requests(q);",
                "\tmutex_unlock(&q->sysfs_lock);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The block subsystem in the Linux kernel before 5.2 has a use-after-free that can lead to arbitrary code execution in the kernel context and privilege escalation, aka CID-c3e2219216c9. This is related to blk_mq_free_rqs and blk_cleanup_queue.",
        "id": 2298
    },
    {
        "cve_id": "CVE-2023-0240",
        "code_before_change": "static void io_req_clean_work(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\treturn;\n\n\treq->flags &= ~REQ_F_WORK_INITIALIZED;\n\n\tif (req->work.flags & IO_WQ_WORK_MM) {\n\t\tmmdrop(req->work.identity->mm);\n\t\treq->work.flags &= ~IO_WQ_WORK_MM;\n\t}\n#ifdef CONFIG_BLK_CGROUP\n\tif (req->work.flags & IO_WQ_WORK_BLKCG) {\n\t\tcss_put(req->work.identity->blkcg_css);\n\t\treq->work.flags &= ~IO_WQ_WORK_BLKCG;\n\t}\n#endif\n\tif (req->work.flags & IO_WQ_WORK_CREDS) {\n\t\tput_cred(req->work.identity->creds);\n\t\treq->work.flags &= ~IO_WQ_WORK_CREDS;\n\t}\n\tif (req->work.flags & IO_WQ_WORK_FS) {\n\t\tstruct fs_struct *fs = req->work.identity->fs;\n\n\t\tspin_lock(&req->work.identity->fs->lock);\n\t\tif (--fs->users)\n\t\t\tfs = NULL;\n\t\tspin_unlock(&req->work.identity->fs->lock);\n\t\tif (fs)\n\t\t\tfree_fs_struct(fs);\n\t\treq->work.flags &= ~IO_WQ_WORK_FS;\n\t}\n}",
        "code_after_change": "static void io_req_clean_work(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\treturn;\n\n\treq->flags &= ~REQ_F_WORK_INITIALIZED;\n\n\tif (req->work.flags & IO_WQ_WORK_MM) {\n\t\tmmdrop(req->work.identity->mm);\n\t\treq->work.flags &= ~IO_WQ_WORK_MM;\n\t}\n#ifdef CONFIG_BLK_CGROUP\n\tif (req->work.flags & IO_WQ_WORK_BLKCG) {\n\t\tcss_put(req->work.identity->blkcg_css);\n\t\treq->work.flags &= ~IO_WQ_WORK_BLKCG;\n\t}\n#endif\n\tif (req->work.flags & IO_WQ_WORK_CREDS) {\n\t\tput_cred(req->work.identity->creds);\n\t\treq->work.flags &= ~IO_WQ_WORK_CREDS;\n\t}\n\tif (req->work.flags & IO_WQ_WORK_FS) {\n\t\tstruct fs_struct *fs = req->work.identity->fs;\n\n\t\tspin_lock(&req->work.identity->fs->lock);\n\t\tif (--fs->users)\n\t\t\tfs = NULL;\n\t\tspin_unlock(&req->work.identity->fs->lock);\n\t\tif (fs)\n\t\t\tfree_fs_struct(fs);\n\t\treq->work.flags &= ~IO_WQ_WORK_FS;\n\t}\n\n\tio_put_identity(req);\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,4 +30,6 @@\n \t\t\tfree_fs_struct(fs);\n \t\treq->work.flags &= ~IO_WQ_WORK_FS;\n \t}\n+\n+\tio_put_identity(req);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tio_put_identity(req);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "id": 3820
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static int io_uring_alloc_task_context(struct task_struct *task,\n\t\t\t\t       struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx;\n\tint ret;\n\n\ttctx = kmalloc(sizeof(*tctx), GFP_KERNEL);\n\tif (unlikely(!tctx))\n\t\treturn -ENOMEM;\n\n\tret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);\n\tif (unlikely(ret)) {\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\ttctx->io_wq = io_init_wq_offload(ctx);\n\tif (IS_ERR(tctx->io_wq)) {\n\t\tret = PTR_ERR(tctx->io_wq);\n\t\tpercpu_counter_destroy(&tctx->inflight);\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\txa_init(&tctx->xa);\n\tinit_waitqueue_head(&tctx->wait);\n\ttctx->last = NULL;\n\tatomic_set(&tctx->in_idle, 0);\n\ttctx->sqpoll = false;\n\tio_init_identity(&tctx->__identity);\n\ttctx->identity = &tctx->__identity;\n\ttask->io_uring = tctx;\n\tspin_lock_init(&tctx->task_lock);\n\tINIT_WQ_LIST(&tctx->task_list);\n\ttctx->task_state = 0;\n\tinit_task_work(&tctx->task_work, tctx_task_work);\n\treturn 0;\n}",
        "code_after_change": "static int io_uring_alloc_task_context(struct task_struct *task,\n\t\t\t\t       struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx;\n\tint ret;\n\n\ttctx = kmalloc(sizeof(*tctx), GFP_KERNEL);\n\tif (unlikely(!tctx))\n\t\treturn -ENOMEM;\n\n\tret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);\n\tif (unlikely(ret)) {\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\ttctx->io_wq = io_init_wq_offload(ctx);\n\tif (IS_ERR(tctx->io_wq)) {\n\t\tret = PTR_ERR(tctx->io_wq);\n\t\tpercpu_counter_destroy(&tctx->inflight);\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\txa_init(&tctx->xa);\n\tinit_waitqueue_head(&tctx->wait);\n\ttctx->last = NULL;\n\tatomic_set(&tctx->in_idle, 0);\n\ttctx->sqpoll = false;\n\ttask->io_uring = tctx;\n\tspin_lock_init(&tctx->task_lock);\n\tINIT_WQ_LIST(&tctx->task_list);\n\ttctx->task_state = 0;\n\tinit_task_work(&tctx->task_work, tctx_task_work);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,8 +27,6 @@\n \ttctx->last = NULL;\n \tatomic_set(&tctx->in_idle, 0);\n \ttctx->sqpoll = false;\n-\tio_init_identity(&tctx->__identity);\n-\ttctx->identity = &tctx->__identity;\n \ttask->io_uring = tctx;\n \tspin_lock_init(&tctx->task_lock);\n \tINIT_WQ_LIST(&tctx->task_list);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tio_init_identity(&tctx->__identity);",
                "\ttctx->identity = &tctx->__identity;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3285
    },
    {
        "cve_id": "CVE-2019-11810",
        "code_before_change": "int megasas_alloc_cmds(struct megasas_instance *instance)\n{\n\tint i;\n\tint j;\n\tu16 max_cmd;\n\tstruct megasas_cmd *cmd;\n\n\tmax_cmd = instance->max_mfi_cmds;\n\n\t/*\n\t * instance->cmd_list is an array of struct megasas_cmd pointers.\n\t * Allocate the dynamic array first and then allocate individual\n\t * commands.\n\t */\n\tinstance->cmd_list = kcalloc(max_cmd, sizeof(struct megasas_cmd*), GFP_KERNEL);\n\n\tif (!instance->cmd_list) {\n\t\tdev_printk(KERN_DEBUG, &instance->pdev->dev, \"out of memory\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmemset(instance->cmd_list, 0, sizeof(struct megasas_cmd *) *max_cmd);\n\n\tfor (i = 0; i < max_cmd; i++) {\n\t\tinstance->cmd_list[i] = kmalloc(sizeof(struct megasas_cmd),\n\t\t\t\t\t\tGFP_KERNEL);\n\n\t\tif (!instance->cmd_list[i]) {\n\n\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\tkfree(instance->cmd_list[j]);\n\n\t\t\tkfree(instance->cmd_list);\n\t\t\tinstance->cmd_list = NULL;\n\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tfor (i = 0; i < max_cmd; i++) {\n\t\tcmd = instance->cmd_list[i];\n\t\tmemset(cmd, 0, sizeof(struct megasas_cmd));\n\t\tcmd->index = i;\n\t\tcmd->scmd = NULL;\n\t\tcmd->instance = instance;\n\n\t\tlist_add_tail(&cmd->list, &instance->cmd_pool);\n\t}\n\n\t/*\n\t * Create a frame pool and assign one frame to each cmd\n\t */\n\tif (megasas_create_frame_pool(instance)) {\n\t\tdev_printk(KERN_DEBUG, &instance->pdev->dev, \"Error creating frame DMA pool\\n\");\n\t\tmegasas_free_cmds(instance);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int megasas_alloc_cmds(struct megasas_instance *instance)\n{\n\tint i;\n\tint j;\n\tu16 max_cmd;\n\tstruct megasas_cmd *cmd;\n\n\tmax_cmd = instance->max_mfi_cmds;\n\n\t/*\n\t * instance->cmd_list is an array of struct megasas_cmd pointers.\n\t * Allocate the dynamic array first and then allocate individual\n\t * commands.\n\t */\n\tinstance->cmd_list = kcalloc(max_cmd, sizeof(struct megasas_cmd*), GFP_KERNEL);\n\n\tif (!instance->cmd_list) {\n\t\tdev_printk(KERN_DEBUG, &instance->pdev->dev, \"out of memory\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmemset(instance->cmd_list, 0, sizeof(struct megasas_cmd *) *max_cmd);\n\n\tfor (i = 0; i < max_cmd; i++) {\n\t\tinstance->cmd_list[i] = kmalloc(sizeof(struct megasas_cmd),\n\t\t\t\t\t\tGFP_KERNEL);\n\n\t\tif (!instance->cmd_list[i]) {\n\n\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\tkfree(instance->cmd_list[j]);\n\n\t\t\tkfree(instance->cmd_list);\n\t\t\tinstance->cmd_list = NULL;\n\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tfor (i = 0; i < max_cmd; i++) {\n\t\tcmd = instance->cmd_list[i];\n\t\tmemset(cmd, 0, sizeof(struct megasas_cmd));\n\t\tcmd->index = i;\n\t\tcmd->scmd = NULL;\n\t\tcmd->instance = instance;\n\n\t\tlist_add_tail(&cmd->list, &instance->cmd_pool);\n\t}\n\n\t/*\n\t * Create a frame pool and assign one frame to each cmd\n\t */\n\tif (megasas_create_frame_pool(instance)) {\n\t\tdev_printk(KERN_DEBUG, &instance->pdev->dev, \"Error creating frame DMA pool\\n\");\n\t\tmegasas_free_cmds(instance);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -53,6 +53,7 @@\n \tif (megasas_create_frame_pool(instance)) {\n \t\tdev_printk(KERN_DEBUG, &instance->pdev->dev, \"Error creating frame DMA pool\\n\");\n \t\tmegasas_free_cmds(instance);\n+\t\treturn -ENOMEM;\n \t}\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\t\treturn -ENOMEM;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.7. A NULL pointer dereference can occur when megasas_create_frame_pool() fails in megasas_alloc_cmds() in drivers/scsi/megaraid/megaraid_sas_base.c. This causes a Denial of Service, related to a use-after-free.",
        "id": 1931
    },
    {
        "cve_id": "CVE-2022-1652",
        "code_before_change": "static int do_format(int drive, struct format_descr *tmp_format_req)\n{\n\tint ret;\n\n\tif (lock_fdc(drive))\n\t\treturn -EINTR;\n\n\tset_floppy(drive);\n\tif (!_floppy ||\n\t    _floppy->track > drive_params[current_drive].tracks ||\n\t    tmp_format_req->track >= _floppy->track ||\n\t    tmp_format_req->head >= _floppy->head ||\n\t    (_floppy->sect << 2) % (1 << FD_SIZECODE(_floppy)) ||\n\t    !_floppy->fmt_gap) {\n\t\tprocess_fd_request();\n\t\treturn -EINVAL;\n\t}\n\tformat_req = *tmp_format_req;\n\tformat_errors = 0;\n\tcont = &format_cont;\n\terrors = &format_errors;\n\tret = wait_til_done(redo_format, true);\n\tif (ret == -EINTR)\n\t\treturn -EINTR;\n\tprocess_fd_request();\n\treturn ret;\n}",
        "code_after_change": "static int do_format(int drive, struct format_descr *tmp_format_req)\n{\n\tint ret;\n\n\tif (lock_fdc(drive))\n\t\treturn -EINTR;\n\n\tset_floppy(drive);\n\tif (!_floppy ||\n\t    _floppy->track > drive_params[current_drive].tracks ||\n\t    tmp_format_req->track >= _floppy->track ||\n\t    tmp_format_req->head >= _floppy->head ||\n\t    (_floppy->sect << 2) % (1 << FD_SIZECODE(_floppy)) ||\n\t    !_floppy->fmt_gap) {\n\t\tprocess_fd_request();\n\t\treturn -EINVAL;\n\t}\n\tformat_req = *tmp_format_req;\n\tcont = &format_cont;\n\tfloppy_errors = 0;\n\tret = wait_til_done(redo_format, true);\n\tif (ret == -EINTR)\n\t\treturn -EINTR;\n\tprocess_fd_request();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,9 +16,8 @@\n \t\treturn -EINVAL;\n \t}\n \tformat_req = *tmp_format_req;\n-\tformat_errors = 0;\n \tcont = &format_cont;\n-\terrors = &format_errors;\n+\tfloppy_errors = 0;\n \tret = wait_til_done(redo_format, true);\n \tif (ret == -EINTR)\n \t\treturn -EINTR;",
        "function_modified_lines": {
            "added": [
                "\tfloppy_errors = 0;"
            ],
            "deleted": [
                "\tformat_errors = 0;",
                "\terrors = &format_errors;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Linux Kernel could allow a local attacker to execute arbitrary code on the system, caused by a concurrency use-after-free flaw in the bad_flp_intr function. By executing a specially-crafted program, an attacker could exploit this vulnerability to execute arbitrary code or cause a denial of service condition on the system.",
        "id": 3267
    },
    {
        "cve_id": "CVE-2020-36387",
        "code_before_change": "static int __io_async_wake(struct io_kiocb *req, struct io_poll_iocb *poll,\n\t\t\t   __poll_t mask, task_work_func_t func)\n{\n\tint ret;\n\n\t/* for instances that support it check for an event match first: */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\ttrace_io_uring_task_add(req->ctx, req->opcode, req->user_data, mask);\n\n\tlist_del_init(&poll->wait.entry);\n\n\treq->result = mask;\n\tinit_task_work(&req->task_work, func);\n\t/*\n\t * If this fails, then the task is exiting. When a task exits, the\n\t * work gets canceled, so just cancel this request as well instead\n\t * of executing it. We can't safely execute it anyway, as we may not\n\t * have the needed state needed for it anyway.\n\t */\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\tWRITE_ONCE(poll->canceled, true);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, 0);\n\t\twake_up_process(tsk);\n\t}\n\treturn 1;\n}",
        "code_after_change": "static int __io_async_wake(struct io_kiocb *req, struct io_poll_iocb *poll,\n\t\t\t   __poll_t mask, task_work_func_t func)\n{\n\tint ret;\n\n\t/* for instances that support it check for an event match first: */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\ttrace_io_uring_task_add(req->ctx, req->opcode, req->user_data, mask);\n\n\tlist_del_init(&poll->wait.entry);\n\n\treq->result = mask;\n\tinit_task_work(&req->task_work, func);\n\tpercpu_ref_get(&req->ctx->refs);\n\n\t/*\n\t * If this fails, then the task is exiting. When a task exits, the\n\t * work gets canceled, so just cancel this request as well instead\n\t * of executing it. We can't safely execute it anyway, as we may not\n\t * have the needed state needed for it anyway.\n\t */\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\tWRITE_ONCE(poll->canceled, true);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, 0);\n\t\twake_up_process(tsk);\n\t}\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,8 @@\n \n \treq->result = mask;\n \tinit_task_work(&req->task_work, func);\n+\tpercpu_ref_get(&req->ctx->refs);\n+\n \t/*\n \t * If this fails, then the task is exiting. When a task exits, the\n \t * work gets canceled, so just cancel this request as well instead",
        "function_modified_lines": {
            "added": [
                "\tpercpu_ref_get(&req->ctx->refs);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.8.2. fs/io_uring.c has a use-after-free related to io_async_task_func and ctx reference holding, aka CID-6d816e088c35.",
        "id": 2758
    },
    {
        "cve_id": "CVE-2022-1048",
        "code_before_change": "int snd_pcm_attach_substream(struct snd_pcm *pcm, int stream,\n\t\t\t     struct file *file,\n\t\t\t     struct snd_pcm_substream **rsubstream)\n{\n\tstruct snd_pcm_str * pstr;\n\tstruct snd_pcm_substream *substream;\n\tstruct snd_pcm_runtime *runtime;\n\tstruct snd_card *card;\n\tint prefer_subdevice;\n\tsize_t size;\n\n\tif (snd_BUG_ON(!pcm || !rsubstream))\n\t\treturn -ENXIO;\n\tif (snd_BUG_ON(stream != SNDRV_PCM_STREAM_PLAYBACK &&\n\t\t       stream != SNDRV_PCM_STREAM_CAPTURE))\n\t\treturn -EINVAL;\n\t*rsubstream = NULL;\n\tpstr = &pcm->streams[stream];\n\tif (pstr->substream == NULL || pstr->substream_count == 0)\n\t\treturn -ENODEV;\n\n\tcard = pcm->card;\n\tprefer_subdevice = snd_ctl_get_preferred_subdevice(card, SND_CTL_SUBDEV_PCM);\n\n\tif (pcm->info_flags & SNDRV_PCM_INFO_HALF_DUPLEX) {\n\t\tint opposite = !stream;\n\n\t\tfor (substream = pcm->streams[opposite].substream; substream;\n\t\t     substream = substream->next) {\n\t\t\tif (SUBSTREAM_BUSY(substream))\n\t\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tif (file->f_flags & O_APPEND) {\n\t\tif (prefer_subdevice < 0) {\n\t\t\tif (pstr->substream_count > 1)\n\t\t\t\treturn -EINVAL; /* must be unique */\n\t\t\tsubstream = pstr->substream;\n\t\t} else {\n\t\t\tfor (substream = pstr->substream; substream;\n\t\t\t     substream = substream->next)\n\t\t\t\tif (substream->number == prefer_subdevice)\n\t\t\t\t\tbreak;\n\t\t}\n\t\tif (! substream)\n\t\t\treturn -ENODEV;\n\t\tif (! SUBSTREAM_BUSY(substream))\n\t\t\treturn -EBADFD;\n\t\tsubstream->ref_count++;\n\t\t*rsubstream = substream;\n\t\treturn 0;\n\t}\n\n\tfor (substream = pstr->substream; substream; substream = substream->next) {\n\t\tif (!SUBSTREAM_BUSY(substream) &&\n\t\t    (prefer_subdevice == -1 ||\n\t\t     substream->number == prefer_subdevice))\n\t\t\tbreak;\n\t}\n\tif (substream == NULL)\n\t\treturn -EAGAIN;\n\n\truntime = kzalloc(sizeof(*runtime), GFP_KERNEL);\n\tif (runtime == NULL)\n\t\treturn -ENOMEM;\n\n\tsize = PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status));\n\truntime->status = alloc_pages_exact(size, GFP_KERNEL);\n\tif (runtime->status == NULL) {\n\t\tkfree(runtime);\n\t\treturn -ENOMEM;\n\t}\n\tmemset(runtime->status, 0, size);\n\n\tsize = PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control));\n\truntime->control = alloc_pages_exact(size, GFP_KERNEL);\n\tif (runtime->control == NULL) {\n\t\tfree_pages_exact(runtime->status,\n\t\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\t\tkfree(runtime);\n\t\treturn -ENOMEM;\n\t}\n\tmemset(runtime->control, 0, size);\n\n\tinit_waitqueue_head(&runtime->sleep);\n\tinit_waitqueue_head(&runtime->tsleep);\n\n\truntime->status->state = SNDRV_PCM_STATE_OPEN;\n\n\tsubstream->runtime = runtime;\n\tsubstream->private_data = pcm->private_data;\n\tsubstream->ref_count = 1;\n\tsubstream->f_flags = file->f_flags;\n\tsubstream->pid = get_pid(task_pid(current));\n\tpstr->substream_opened++;\n\t*rsubstream = substream;\n\treturn 0;\n}",
        "code_after_change": "int snd_pcm_attach_substream(struct snd_pcm *pcm, int stream,\n\t\t\t     struct file *file,\n\t\t\t     struct snd_pcm_substream **rsubstream)\n{\n\tstruct snd_pcm_str * pstr;\n\tstruct snd_pcm_substream *substream;\n\tstruct snd_pcm_runtime *runtime;\n\tstruct snd_card *card;\n\tint prefer_subdevice;\n\tsize_t size;\n\n\tif (snd_BUG_ON(!pcm || !rsubstream))\n\t\treturn -ENXIO;\n\tif (snd_BUG_ON(stream != SNDRV_PCM_STREAM_PLAYBACK &&\n\t\t       stream != SNDRV_PCM_STREAM_CAPTURE))\n\t\treturn -EINVAL;\n\t*rsubstream = NULL;\n\tpstr = &pcm->streams[stream];\n\tif (pstr->substream == NULL || pstr->substream_count == 0)\n\t\treturn -ENODEV;\n\n\tcard = pcm->card;\n\tprefer_subdevice = snd_ctl_get_preferred_subdevice(card, SND_CTL_SUBDEV_PCM);\n\n\tif (pcm->info_flags & SNDRV_PCM_INFO_HALF_DUPLEX) {\n\t\tint opposite = !stream;\n\n\t\tfor (substream = pcm->streams[opposite].substream; substream;\n\t\t     substream = substream->next) {\n\t\t\tif (SUBSTREAM_BUSY(substream))\n\t\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tif (file->f_flags & O_APPEND) {\n\t\tif (prefer_subdevice < 0) {\n\t\t\tif (pstr->substream_count > 1)\n\t\t\t\treturn -EINVAL; /* must be unique */\n\t\t\tsubstream = pstr->substream;\n\t\t} else {\n\t\t\tfor (substream = pstr->substream; substream;\n\t\t\t     substream = substream->next)\n\t\t\t\tif (substream->number == prefer_subdevice)\n\t\t\t\t\tbreak;\n\t\t}\n\t\tif (! substream)\n\t\t\treturn -ENODEV;\n\t\tif (! SUBSTREAM_BUSY(substream))\n\t\t\treturn -EBADFD;\n\t\tsubstream->ref_count++;\n\t\t*rsubstream = substream;\n\t\treturn 0;\n\t}\n\n\tfor (substream = pstr->substream; substream; substream = substream->next) {\n\t\tif (!SUBSTREAM_BUSY(substream) &&\n\t\t    (prefer_subdevice == -1 ||\n\t\t     substream->number == prefer_subdevice))\n\t\t\tbreak;\n\t}\n\tif (substream == NULL)\n\t\treturn -EAGAIN;\n\n\truntime = kzalloc(sizeof(*runtime), GFP_KERNEL);\n\tif (runtime == NULL)\n\t\treturn -ENOMEM;\n\n\tsize = PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status));\n\truntime->status = alloc_pages_exact(size, GFP_KERNEL);\n\tif (runtime->status == NULL) {\n\t\tkfree(runtime);\n\t\treturn -ENOMEM;\n\t}\n\tmemset(runtime->status, 0, size);\n\n\tsize = PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control));\n\truntime->control = alloc_pages_exact(size, GFP_KERNEL);\n\tif (runtime->control == NULL) {\n\t\tfree_pages_exact(runtime->status,\n\t\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\t\tkfree(runtime);\n\t\treturn -ENOMEM;\n\t}\n\tmemset(runtime->control, 0, size);\n\n\tinit_waitqueue_head(&runtime->sleep);\n\tinit_waitqueue_head(&runtime->tsleep);\n\n\truntime->status->state = SNDRV_PCM_STATE_OPEN;\n\tmutex_init(&runtime->buffer_mutex);\n\n\tsubstream->runtime = runtime;\n\tsubstream->private_data = pcm->private_data;\n\tsubstream->ref_count = 1;\n\tsubstream->f_flags = file->f_flags;\n\tsubstream->pid = get_pid(task_pid(current));\n\tpstr->substream_opened++;\n\t*rsubstream = substream;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -87,6 +87,7 @@\n \tinit_waitqueue_head(&runtime->tsleep);\n \n \truntime->status->state = SNDRV_PCM_STATE_OPEN;\n+\tmutex_init(&runtime->buffer_mutex);\n \n \tsubstream->runtime = runtime;\n \tsubstream->private_data = pcm->private_data;",
        "function_modified_lines": {
            "added": [
                "\tmutex_init(&runtime->buffer_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s sound subsystem in the way a user triggers concurrent calls of PCM hw_params. The hw_free ioctls or similar race condition happens inside ALSA PCM for other ioctls. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3242
    },
    {
        "cve_id": "CVE-2018-11412",
        "code_before_change": "int ext4_find_inline_data_nolock(struct inode *inode)\n{\n\tstruct ext4_xattr_ibody_find is = {\n\t\t.s = { .not_found = -ENODATA, },\n\t};\n\tstruct ext4_xattr_info i = {\n\t\t.name_index = EXT4_XATTR_INDEX_SYSTEM,\n\t\t.name = EXT4_XATTR_SYSTEM_DATA,\n\t};\n\tint error;\n\n\tif (EXT4_I(inode)->i_extra_isize == 0)\n\t\treturn 0;\n\n\terror = ext4_get_inode_loc(inode, &is.iloc);\n\tif (error)\n\t\treturn error;\n\n\terror = ext4_xattr_ibody_find(inode, &i, &is);\n\tif (error)\n\t\tgoto out;\n\n\tif (!is.s.not_found) {\n\t\tEXT4_I(inode)->i_inline_off = (u16)((void *)is.s.here -\n\t\t\t\t\t(void *)ext4_raw_inode(&is.iloc));\n\t\tEXT4_I(inode)->i_inline_size = EXT4_MIN_INLINE_DATA_SIZE +\n\t\t\t\tle32_to_cpu(is.s.here->e_value_size);\n\t\text4_set_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\n\t}\nout:\n\tbrelse(is.iloc.bh);\n\treturn error;\n}",
        "code_after_change": "int ext4_find_inline_data_nolock(struct inode *inode)\n{\n\tstruct ext4_xattr_ibody_find is = {\n\t\t.s = { .not_found = -ENODATA, },\n\t};\n\tstruct ext4_xattr_info i = {\n\t\t.name_index = EXT4_XATTR_INDEX_SYSTEM,\n\t\t.name = EXT4_XATTR_SYSTEM_DATA,\n\t};\n\tint error;\n\n\tif (EXT4_I(inode)->i_extra_isize == 0)\n\t\treturn 0;\n\n\terror = ext4_get_inode_loc(inode, &is.iloc);\n\tif (error)\n\t\treturn error;\n\n\terror = ext4_xattr_ibody_find(inode, &i, &is);\n\tif (error)\n\t\tgoto out;\n\n\tif (!is.s.not_found) {\n\t\tif (is.s.here->e_value_inum) {\n\t\t\tEXT4_ERROR_INODE(inode, \"inline data xattr refers \"\n\t\t\t\t\t \"to an external xattr inode\");\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t}\n\t\tEXT4_I(inode)->i_inline_off = (u16)((void *)is.s.here -\n\t\t\t\t\t(void *)ext4_raw_inode(&is.iloc));\n\t\tEXT4_I(inode)->i_inline_size = EXT4_MIN_INLINE_DATA_SIZE +\n\t\t\t\tle32_to_cpu(is.s.here->e_value_size);\n\t\text4_set_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\n\t}\nout:\n\tbrelse(is.iloc.bh);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,12 @@\n \t\tgoto out;\n \n \tif (!is.s.not_found) {\n+\t\tif (is.s.here->e_value_inum) {\n+\t\t\tEXT4_ERROR_INODE(inode, \"inline data xattr refers \"\n+\t\t\t\t\t \"to an external xattr inode\");\n+\t\t\terror = -EFSCORRUPTED;\n+\t\t\tgoto out;\n+\t\t}\n \t\tEXT4_I(inode)->i_inline_off = (u16)((void *)is.s.here -\n \t\t\t\t\t(void *)ext4_raw_inode(&is.iloc));\n \t\tEXT4_I(inode)->i_inline_size = EXT4_MIN_INLINE_DATA_SIZE +",
        "function_modified_lines": {
            "added": [
                "\t\tif (is.s.here->e_value_inum) {",
                "\t\t\tEXT4_ERROR_INODE(inode, \"inline data xattr refers \"",
                "\t\t\t\t\t \"to an external xattr inode\");",
                "\t\t\terror = -EFSCORRUPTED;",
                "\t\t\tgoto out;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 4.13 through 4.16.11, ext4_read_inline_data() in fs/ext4/inline.c performs a memcpy with an untrusted length value in certain circumstances involving a crafted filesystem that stores the system.data extended attribute value in a dedicated inode.",
        "id": 1643
    },
    {
        "cve_id": "CVE-2022-1976",
        "code_before_change": "static __cold int io_uring_alloc_task_context(struct task_struct *task,\n\t\t\t\t\t      struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx;\n\tint ret;\n\n\ttctx = kzalloc(sizeof(*tctx), GFP_KERNEL);\n\tif (unlikely(!tctx))\n\t\treturn -ENOMEM;\n\n\ttctx->registered_rings = kcalloc(IO_RINGFD_REG_MAX,\n\t\t\t\t\t sizeof(struct file *), GFP_KERNEL);\n\tif (unlikely(!tctx->registered_rings)) {\n\t\tkfree(tctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);\n\tif (unlikely(ret)) {\n\t\tkfree(tctx->registered_rings);\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\ttctx->io_wq = io_init_wq_offload(ctx, task);\n\tif (IS_ERR(tctx->io_wq)) {\n\t\tret = PTR_ERR(tctx->io_wq);\n\t\tpercpu_counter_destroy(&tctx->inflight);\n\t\tkfree(tctx->registered_rings);\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\txa_init(&tctx->xa);\n\tinit_waitqueue_head(&tctx->wait);\n\tatomic_set(&tctx->in_idle, 0);\n\ttask->io_uring = tctx;\n\tspin_lock_init(&tctx->task_lock);\n\tINIT_WQ_LIST(&tctx->task_list);\n\tINIT_WQ_LIST(&tctx->prio_task_list);\n\tinit_task_work(&tctx->task_work, tctx_task_work);\n\treturn 0;\n}",
        "code_after_change": "static __cold int io_uring_alloc_task_context(struct task_struct *task,\n\t\t\t\t\t      struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx;\n\tint ret;\n\n\ttctx = kzalloc(sizeof(*tctx), GFP_KERNEL);\n\tif (unlikely(!tctx))\n\t\treturn -ENOMEM;\n\n\ttctx->registered_rings = kcalloc(IO_RINGFD_REG_MAX,\n\t\t\t\t\t sizeof(struct file *), GFP_KERNEL);\n\tif (unlikely(!tctx->registered_rings)) {\n\t\tkfree(tctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);\n\tif (unlikely(ret)) {\n\t\tkfree(tctx->registered_rings);\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\ttctx->io_wq = io_init_wq_offload(ctx, task);\n\tif (IS_ERR(tctx->io_wq)) {\n\t\tret = PTR_ERR(tctx->io_wq);\n\t\tpercpu_counter_destroy(&tctx->inflight);\n\t\tkfree(tctx->registered_rings);\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\txa_init(&tctx->xa);\n\tinit_waitqueue_head(&tctx->wait);\n\tatomic_set(&tctx->in_idle, 0);\n\tatomic_set(&tctx->inflight_tracked, 0);\n\ttask->io_uring = tctx;\n\tspin_lock_init(&tctx->task_lock);\n\tINIT_WQ_LIST(&tctx->task_list);\n\tINIT_WQ_LIST(&tctx->prio_task_list);\n\tinit_task_work(&tctx->task_work, tctx_task_work);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -34,6 +34,7 @@\n \txa_init(&tctx->xa);\n \tinit_waitqueue_head(&tctx->wait);\n \tatomic_set(&tctx->in_idle, 0);\n+\tatomic_set(&tctx->inflight_tracked, 0);\n \ttask->io_uring = tctx;\n \tspin_lock_init(&tctx->task_lock);\n \tINIT_WQ_LIST(&tctx->task_list);",
        "function_modified_lines": {
            "added": [
                "\tatomic_set(&tctx->inflight_tracked, 0);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s implementation of IO-URING. This flaw allows an attacker with local executable permission to create a string of requests that can cause a use-after-free flaw within the kernel. This issue leads to memory corruption and possible privilege escalation.",
        "id": 3327
    },
    {
        "cve_id": "CVE-2018-20836",
        "code_before_change": "static void smp_task_timedout(struct timer_list *t)\n{\n\tstruct sas_task_slow *slow = from_timer(slow, t, timer);\n\tstruct sas_task *task = slow->task;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&task->task_state_lock, flags);\n\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))\n\t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n\tspin_unlock_irqrestore(&task->task_state_lock, flags);\n\n\tcomplete(&task->slow_task->completion);\n}",
        "code_after_change": "static void smp_task_timedout(struct timer_list *t)\n{\n\tstruct sas_task_slow *slow = from_timer(slow, t, timer);\n\tstruct sas_task *task = slow->task;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&task->task_state_lock, flags);\n\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {\n\t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n\t\tcomplete(&task->slow_task->completion);\n\t}\n\tspin_unlock_irqrestore(&task->task_state_lock, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,9 +5,9 @@\n \tunsigned long flags;\n \n \tspin_lock_irqsave(&task->task_state_lock, flags);\n-\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))\n+\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {\n \t\ttask->task_state_flags |= SAS_TASK_STATE_ABORTED;\n+\t\tcomplete(&task->slow_task->completion);\n+\t}\n \tspin_unlock_irqrestore(&task->task_state_lock, flags);\n-\n-\tcomplete(&task->slow_task->completion);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {",
                "\t\tcomplete(&task->slow_task->completion);",
                "\t}"
            ],
            "deleted": [
                "\tif (!(task->task_state_flags & SAS_TASK_STATE_DONE))",
                "",
                "\tcomplete(&task->slow_task->completion);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.20. There is a race condition in smp_task_timedout() and smp_task_done() in drivers/scsi/libsas/sas_expander.c, leading to a use-after-free.",
        "id": 1782
    },
    {
        "cve_id": "CVE-2019-25045",
        "code_before_change": "static int validate_tmpl(int nr, struct xfrm_user_tmpl *ut, u16 family)\n{\n\tu16 prev_family;\n\tint i;\n\n\tif (nr > XFRM_MAX_DEPTH)\n\t\treturn -EINVAL;\n\n\tprev_family = family;\n\n\tfor (i = 0; i < nr; i++) {\n\t\t/* We never validated the ut->family value, so many\n\t\t * applications simply leave it at zero.  The check was\n\t\t * never made and ut->family was ignored because all\n\t\t * templates could be assumed to have the same family as\n\t\t * the policy itself.  Now that we will have ipv4-in-ipv6\n\t\t * and ipv6-in-ipv4 tunnels, this is no longer true.\n\t\t */\n\t\tif (!ut[i].family)\n\t\t\tut[i].family = family;\n\n\t\tswitch (ut[i].mode) {\n\t\tcase XFRM_MODE_TUNNEL:\n\t\tcase XFRM_MODE_BEET:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (ut[i].family != prev_family)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (ut[i].mode >= XFRM_MODE_MAX)\n\t\t\treturn -EINVAL;\n\n\t\tprev_family = ut[i].family;\n\n\t\tswitch (ut[i].family) {\n\t\tcase AF_INET:\n\t\t\tbreak;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tcase AF_INET6:\n\t\t\tbreak;\n#endif\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tswitch (ut[i].id.proto) {\n\t\tcase IPPROTO_AH:\n\t\tcase IPPROTO_ESP:\n\t\tcase IPPROTO_COMP:\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tcase IPPROTO_ROUTING:\n\t\tcase IPPROTO_DSTOPTS:\n#endif\n\t\tcase IPSEC_PROTO_ANY:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int validate_tmpl(int nr, struct xfrm_user_tmpl *ut, u16 family)\n{\n\tu16 prev_family;\n\tint i;\n\n\tif (nr > XFRM_MAX_DEPTH)\n\t\treturn -EINVAL;\n\n\tprev_family = family;\n\n\tfor (i = 0; i < nr; i++) {\n\t\t/* We never validated the ut->family value, so many\n\t\t * applications simply leave it at zero.  The check was\n\t\t * never made and ut->family was ignored because all\n\t\t * templates could be assumed to have the same family as\n\t\t * the policy itself.  Now that we will have ipv4-in-ipv6\n\t\t * and ipv6-in-ipv4 tunnels, this is no longer true.\n\t\t */\n\t\tif (!ut[i].family)\n\t\t\tut[i].family = family;\n\n\t\tswitch (ut[i].mode) {\n\t\tcase XFRM_MODE_TUNNEL:\n\t\tcase XFRM_MODE_BEET:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (ut[i].family != prev_family)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (ut[i].mode >= XFRM_MODE_MAX)\n\t\t\treturn -EINVAL;\n\n\t\tprev_family = ut[i].family;\n\n\t\tswitch (ut[i].family) {\n\t\tcase AF_INET:\n\t\t\tbreak;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tcase AF_INET6:\n\t\t\tbreak;\n#endif\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!xfrm_id_proto_valid(ut[i].id.proto))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -44,20 +44,8 @@\n \t\t\treturn -EINVAL;\n \t\t}\n \n-\t\tswitch (ut[i].id.proto) {\n-\t\tcase IPPROTO_AH:\n-\t\tcase IPPROTO_ESP:\n-\t\tcase IPPROTO_COMP:\n-#if IS_ENABLED(CONFIG_IPV6)\n-\t\tcase IPPROTO_ROUTING:\n-\t\tcase IPPROTO_DSTOPTS:\n-#endif\n-\t\tcase IPSEC_PROTO_ANY:\n-\t\t\tbreak;\n-\t\tdefault:\n+\t\tif (!xfrm_id_proto_valid(ut[i].id.proto))\n \t\t\treturn -EINVAL;\n-\t\t}\n-\n \t}\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!xfrm_id_proto_valid(ut[i].id.proto))"
            ],
            "deleted": [
                "\t\tswitch (ut[i].id.proto) {",
                "\t\tcase IPPROTO_AH:",
                "\t\tcase IPPROTO_ESP:",
                "\t\tcase IPPROTO_COMP:",
                "#if IS_ENABLED(CONFIG_IPV6)",
                "\t\tcase IPPROTO_ROUTING:",
                "\t\tcase IPPROTO_DSTOPTS:",
                "#endif",
                "\t\tcase IPSEC_PROTO_ANY:",
                "\t\t\tbreak;",
                "\t\tdefault:",
                "\t\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.19. The XFRM subsystem has a use-after-free, related to an xfrm_state_fini panic, aka CID-dbb2483b2a46.",
        "id": 2305
    },
    {
        "cve_id": "CVE-2023-1079",
        "code_before_change": "static void asus_kbd_backlight_set(struct led_classdev *led_cdev,\n\t\t\t\t   enum led_brightness brightness)\n{\n\tstruct asus_kbd_leds *led = container_of(led_cdev, struct asus_kbd_leds,\n\t\t\t\t\t\t cdev);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&led->lock, flags);\n\tled->brightness = brightness;\n\tspin_unlock_irqrestore(&led->lock, flags);\n\n\tschedule_work(&led->work);\n}",
        "code_after_change": "static void asus_kbd_backlight_set(struct led_classdev *led_cdev,\n\t\t\t\t   enum led_brightness brightness)\n{\n\tstruct asus_kbd_leds *led = container_of(led_cdev, struct asus_kbd_leds,\n\t\t\t\t\t\t cdev);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&led->lock, flags);\n\tled->brightness = brightness;\n\tspin_unlock_irqrestore(&led->lock, flags);\n\n\tasus_schedule_work(led);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,5 +9,5 @@\n \tled->brightness = brightness;\n \tspin_unlock_irqrestore(&led->lock, flags);\n \n-\tschedule_work(&led->work);\n+\tasus_schedule_work(led);\n }",
        "function_modified_lines": {
            "added": [
                "\tasus_schedule_work(led);"
            ],
            "deleted": [
                "\tschedule_work(&led->work);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel. A use-after-free may be triggered in asus_kbd_backlight_set when plugging/disconnecting in a malicious USB device, which advertises itself as an Asus device. Similarly to the previous known CVE-2023-25012, but in asus devices, the work_struct may be scheduled by the LED controller while the device is disconnecting, triggering a use-after-free on the struct asus_kbd_leds *led structure. A malicious USB device may exploit the issue to cause memory corruption with controlled data.",
        "id": 3846
    },
    {
        "cve_id": "CVE-2021-3543",
        "code_before_change": "static long ne_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase NE_CREATE_VM: {\n\t\tint enclave_fd = -1;\n\t\tstruct file *enclave_file = NULL;\n\t\tstruct ne_pci_dev *ne_pci_dev = ne_devs.ne_pci_dev;\n\t\tint rc = -EINVAL;\n\t\tu64 slot_uid = 0;\n\n\t\tmutex_lock(&ne_pci_dev->enclaves_list_mutex);\n\n\t\tenclave_fd = ne_create_vm_ioctl(ne_pci_dev, &slot_uid);\n\t\tif (enclave_fd < 0) {\n\t\t\trc = enclave_fd;\n\n\t\t\tmutex_unlock(&ne_pci_dev->enclaves_list_mutex);\n\n\t\t\treturn rc;\n\t\t}\n\n\t\tmutex_unlock(&ne_pci_dev->enclaves_list_mutex);\n\n\t\tif (copy_to_user((void __user *)arg, &slot_uid, sizeof(slot_uid))) {\n\t\t\tenclave_file = fget(enclave_fd);\n\t\t\t/* Decrement file refs to have release() called. */\n\t\t\tfput(enclave_file);\n\t\t\tfput(enclave_file);\n\t\t\tput_unused_fd(enclave_fd);\n\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\treturn enclave_fd;\n\t}\n\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static long ne_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase NE_CREATE_VM: {\n\t\tint enclave_fd = -1;\n\t\tstruct ne_pci_dev *ne_pci_dev = ne_devs.ne_pci_dev;\n\t\tu64 __user *slot_uid = (void __user *)arg;\n\n\t\tmutex_lock(&ne_pci_dev->enclaves_list_mutex);\n\t\tenclave_fd = ne_create_vm_ioctl(ne_pci_dev, slot_uid);\n\t\tmutex_unlock(&ne_pci_dev->enclaves_list_mutex);\n\n\t\treturn enclave_fd;\n\t}\n\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,33 +3,12 @@\n \tswitch (cmd) {\n \tcase NE_CREATE_VM: {\n \t\tint enclave_fd = -1;\n-\t\tstruct file *enclave_file = NULL;\n \t\tstruct ne_pci_dev *ne_pci_dev = ne_devs.ne_pci_dev;\n-\t\tint rc = -EINVAL;\n-\t\tu64 slot_uid = 0;\n+\t\tu64 __user *slot_uid = (void __user *)arg;\n \n \t\tmutex_lock(&ne_pci_dev->enclaves_list_mutex);\n-\n-\t\tenclave_fd = ne_create_vm_ioctl(ne_pci_dev, &slot_uid);\n-\t\tif (enclave_fd < 0) {\n-\t\t\trc = enclave_fd;\n-\n-\t\t\tmutex_unlock(&ne_pci_dev->enclaves_list_mutex);\n-\n-\t\t\treturn rc;\n-\t\t}\n-\n+\t\tenclave_fd = ne_create_vm_ioctl(ne_pci_dev, slot_uid);\n \t\tmutex_unlock(&ne_pci_dev->enclaves_list_mutex);\n-\n-\t\tif (copy_to_user((void __user *)arg, &slot_uid, sizeof(slot_uid))) {\n-\t\t\tenclave_file = fget(enclave_fd);\n-\t\t\t/* Decrement file refs to have release() called. */\n-\t\t\tfput(enclave_file);\n-\t\t\tfput(enclave_file);\n-\t\t\tput_unused_fd(enclave_fd);\n-\n-\t\t\treturn -EFAULT;\n-\t\t}\n \n \t\treturn enclave_fd;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tu64 __user *slot_uid = (void __user *)arg;",
                "\t\tenclave_fd = ne_create_vm_ioctl(ne_pci_dev, slot_uid);"
            ],
            "deleted": [
                "\t\tstruct file *enclave_file = NULL;",
                "\t\tint rc = -EINVAL;",
                "\t\tu64 slot_uid = 0;",
                "",
                "\t\tenclave_fd = ne_create_vm_ioctl(ne_pci_dev, &slot_uid);",
                "\t\tif (enclave_fd < 0) {",
                "\t\t\trc = enclave_fd;",
                "",
                "\t\t\tmutex_unlock(&ne_pci_dev->enclaves_list_mutex);",
                "",
                "\t\t\treturn rc;",
                "\t\t}",
                "",
                "",
                "\t\tif (copy_to_user((void __user *)arg, &slot_uid, sizeof(slot_uid))) {",
                "\t\t\tenclave_file = fget(enclave_fd);",
                "\t\t\t/* Decrement file refs to have release() called. */",
                "\t\t\tfput(enclave_file);",
                "\t\t\tfput(enclave_file);",
                "\t\t\tput_unused_fd(enclave_fd);",
                "",
                "\t\t\treturn -EFAULT;",
                "\t\t}"
            ]
        },
        "cwe": [
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "A flaw null pointer dereference in the Nitro Enclaves kernel driver was found in the way that Enclaves VMs forces closures on the enclave file descriptor. A local user of a host machine could use this flaw to crash the system or escalate their privileges on the system.",
        "id": 3017
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "unsigned int\nipt_do_table(struct sk_buff *skb,\n\t     const struct nf_hook_state *state,\n\t     struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tconst struct iphdr *ip;\n\t/* Initializing verdict to NF_DROP keeps gcc happy. */\n\tunsigned int verdict = NF_DROP;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tstruct ipt_entry *e, **jumpstack;\n\tunsigned int stackidx, cpu;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\t/* Initialization */\n\tstackidx = 0;\n\tip = ip_hdr(skb);\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\t/* We handle fragments by dealing with the first fragment as\n\t * if it was a normal packet.  All other fragments are treated\n\t * normally, except that they will NEVER match rules that ask\n\t * things we don't know, ie. tcp syn flag or ports).  If the\n\t * rule is also a fragment-specific rule, non-fragments won't\n\t * match it. */\n\tacpar.fragoff = ntohs(ip->frag_off) & IP_OFFSET;\n\tacpar.thoff   = ip_hdrlen(skb);\n\tacpar.hotdrop = false;\n\tacpar.state   = state;\n\n\tWARN_ON(!(table->valid_hooks & (1 << hook)));\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = READ_ONCE(table->private); /* Address dependency. */\n\tcpu        = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct ipt_entry **)private->jumpstack[cpu];\n\n\t/* Switch to alternate jumpstack if we're being invoked via TEE.\n\t * TEE issues XT_CONTINUE verdict on original skb so we must not\n\t * clobber the jumpstack.\n\t *\n\t * For recursion via REJECT or SYNPROXY the stack will be clobbered\n\t * but it is no problem since absolute verdict is issued by these.\n\t */\n\tif (static_key_false(&xt_tee_enabled))\n\t\tjumpstack += private->stacksize * __this_cpu_read(nf_skb_duplicated);\n\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tconst struct xt_entry_match *ematch;\n\t\tstruct xt_counters *counter;\n\n\t\tWARN_ON(!e);\n\t\tif (!ip_packet_match(ip, indev, outdev,\n\t\t    &e->ip, acpar.fragoff)) {\n no_match:\n\t\t\te = ipt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\txt_ematch_foreach(ematch, e) {\n\t\t\tacpar.match     = ematch->u.kernel.match;\n\t\t\tacpar.matchinfo = ematch->data;\n\t\t\tif (!acpar.match->match(skb, &acpar))\n\t\t\t\tgoto no_match;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, skb->len, 1);\n\n\t\tt = ipt_get_target_c(e);\n\t\tWARN_ON(!t->u.kernel.target);\n\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE)\n\t\t/* The packet is traced: log it */\n\t\tif (unlikely(skb->nf_trace))\n\t\t\ttrace_packet(state->net, skb, hook, state->in,\n\t\t\t\t     state->out, table->name, private, e);\n#endif\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t    private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = ipt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v != ipt_next_entry(e) &&\n\t\t\t    !(e->ip.flags & IPT_F_GOTO)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tip = ip_hdr(skb);\n\t\t\te = ipt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse return verdict;\n}",
        "code_after_change": "unsigned int\nipt_do_table(struct sk_buff *skb,\n\t     const struct nf_hook_state *state,\n\t     struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tconst struct iphdr *ip;\n\t/* Initializing verdict to NF_DROP keeps gcc happy. */\n\tunsigned int verdict = NF_DROP;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tstruct ipt_entry *e, **jumpstack;\n\tunsigned int stackidx, cpu;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\t/* Initialization */\n\tstackidx = 0;\n\tip = ip_hdr(skb);\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\t/* We handle fragments by dealing with the first fragment as\n\t * if it was a normal packet.  All other fragments are treated\n\t * normally, except that they will NEVER match rules that ask\n\t * things we don't know, ie. tcp syn flag or ports).  If the\n\t * rule is also a fragment-specific rule, non-fragments won't\n\t * match it. */\n\tacpar.fragoff = ntohs(ip->frag_off) & IP_OFFSET;\n\tacpar.thoff   = ip_hdrlen(skb);\n\tacpar.hotdrop = false;\n\tacpar.state   = state;\n\n\tWARN_ON(!(table->valid_hooks & (1 << hook)));\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = rcu_access_pointer(table->private);\n\tcpu        = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct ipt_entry **)private->jumpstack[cpu];\n\n\t/* Switch to alternate jumpstack if we're being invoked via TEE.\n\t * TEE issues XT_CONTINUE verdict on original skb so we must not\n\t * clobber the jumpstack.\n\t *\n\t * For recursion via REJECT or SYNPROXY the stack will be clobbered\n\t * but it is no problem since absolute verdict is issued by these.\n\t */\n\tif (static_key_false(&xt_tee_enabled))\n\t\tjumpstack += private->stacksize * __this_cpu_read(nf_skb_duplicated);\n\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tconst struct xt_entry_match *ematch;\n\t\tstruct xt_counters *counter;\n\n\t\tWARN_ON(!e);\n\t\tif (!ip_packet_match(ip, indev, outdev,\n\t\t    &e->ip, acpar.fragoff)) {\n no_match:\n\t\t\te = ipt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\txt_ematch_foreach(ematch, e) {\n\t\t\tacpar.match     = ematch->u.kernel.match;\n\t\t\tacpar.matchinfo = ematch->data;\n\t\t\tif (!acpar.match->match(skb, &acpar))\n\t\t\t\tgoto no_match;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, skb->len, 1);\n\n\t\tt = ipt_get_target_c(e);\n\t\tWARN_ON(!t->u.kernel.target);\n\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE)\n\t\t/* The packet is traced: log it */\n\t\tif (unlikely(skb->nf_trace))\n\t\t\ttrace_packet(state->net, skb, hook, state->in,\n\t\t\t\t     state->out, table->name, private, e);\n#endif\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t    private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = ipt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v != ipt_next_entry(e) &&\n\t\t\t    !(e->ip.flags & IPT_F_GOTO)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tip = ip_hdr(skb);\n\t\t\te = ipt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse return verdict;\n}",
        "patch": "--- code before\n+++ code after\n@@ -35,7 +35,7 @@\n \tWARN_ON(!(table->valid_hooks & (1 << hook)));\n \tlocal_bh_disable();\n \taddend = xt_write_recseq_begin();\n-\tprivate = READ_ONCE(table->private); /* Address dependency. */\n+\tprivate = rcu_access_pointer(table->private);\n \tcpu        = smp_processor_id();\n \ttable_base = private->entries;\n \tjumpstack  = (struct ipt_entry **)private->jumpstack[cpu];",
        "function_modified_lines": {
            "added": [
                "\tprivate = rcu_access_pointer(table->private);"
            ],
            "deleted": [
                "\tprivate = READ_ONCE(table->private); /* Address dependency. */"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2779
    },
    {
        "cve_id": "CVE-2020-7053",
        "code_before_change": "static int gem_context_register(struct i915_gem_context *ctx,\n\t\t\t\tstruct drm_i915_file_private *fpriv)\n{\n\tint ret;\n\n\tctx->file_priv = fpriv;\n\tif (ctx->ppgtt)\n\t\tctx->ppgtt->vm.file = fpriv;\n\n\tctx->pid = get_task_pid(current, PIDTYPE_PID);\n\tctx->name = kasprintf(GFP_KERNEL, \"%s[%d]\",\n\t\t\t      current->comm, pid_nr(ctx->pid));\n\tif (!ctx->name) {\n\t\tret = -ENOMEM;\n\t\tgoto err_pid;\n\t}\n\n\t/* And finally expose ourselves to userspace via the idr */\n\tret = idr_alloc(&fpriv->context_idr, ctx,\n\t\t\tDEFAULT_CONTEXT_HANDLE, 0, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto err_name;\n\n\tctx->user_handle = ret;\n\n\treturn 0;\n\nerr_name:\n\tkfree(fetch_and_zero(&ctx->name));\nerr_pid:\n\tput_pid(fetch_and_zero(&ctx->pid));\n\treturn ret;\n}",
        "code_after_change": "static int gem_context_register(struct i915_gem_context *ctx,\n\t\t\t\tstruct drm_i915_file_private *fpriv)\n{\n\tint ret;\n\n\tctx->file_priv = fpriv;\n\tif (ctx->ppgtt)\n\t\tctx->ppgtt->vm.file = fpriv;\n\n\tctx->pid = get_task_pid(current, PIDTYPE_PID);\n\tctx->name = kasprintf(GFP_KERNEL, \"%s[%d]\",\n\t\t\t      current->comm, pid_nr(ctx->pid));\n\tif (!ctx->name) {\n\t\tret = -ENOMEM;\n\t\tgoto err_pid;\n\t}\n\n\t/* And finally expose ourselves to userspace via the idr */\n\tmutex_lock(&fpriv->context_idr_lock);\n\tret = idr_alloc(&fpriv->context_idr, ctx,\n\t\t\tDEFAULT_CONTEXT_HANDLE, 0, GFP_KERNEL);\n\tif (ret >= 0)\n\t\tctx->user_handle = ret;\n\tmutex_unlock(&fpriv->context_idr_lock);\n\tif (ret < 0)\n\t\tgoto err_name;\n\n\treturn 0;\n\nerr_name:\n\tkfree(fetch_and_zero(&ctx->name));\nerr_pid:\n\tput_pid(fetch_and_zero(&ctx->pid));\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,12 +16,14 @@\n \t}\n \n \t/* And finally expose ourselves to userspace via the idr */\n+\tmutex_lock(&fpriv->context_idr_lock);\n \tret = idr_alloc(&fpriv->context_idr, ctx,\n \t\t\tDEFAULT_CONTEXT_HANDLE, 0, GFP_KERNEL);\n+\tif (ret >= 0)\n+\t\tctx->user_handle = ret;\n+\tmutex_unlock(&fpriv->context_idr_lock);\n \tif (ret < 0)\n \t\tgoto err_name;\n-\n-\tctx->user_handle = ret;\n \n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&fpriv->context_idr_lock);",
                "\tif (ret >= 0)",
                "\t\tctx->user_handle = ret;",
                "\tmutex_unlock(&fpriv->context_idr_lock);"
            ],
            "deleted": [
                "",
                "\tctx->user_handle = ret;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 4.14 longterm through 4.14.165 and 4.19 longterm through 4.19.96 (and 5.x before 5.2), there is a use-after-free (write) in the i915_ppgtt_close function in drivers/gpu/drm/i915/i915_gem_gtt.c, aka CID-7dc40713618c. This is related to i915_gem_context_destroy_ioctl in drivers/gpu/drm/i915/i915_gem_context.c.",
        "id": 2799
    },
    {
        "cve_id": "CVE-2019-15220",
        "code_before_change": "static void p54u_disconnect(struct usb_interface *intf)\n{\n\tstruct ieee80211_hw *dev = usb_get_intfdata(intf);\n\tstruct p54u_priv *priv;\n\n\tif (!dev)\n\t\treturn;\n\n\tpriv = dev->priv;\n\twait_for_completion(&priv->fw_wait_load);\n\tp54_unregister_common(dev);\n\n\tusb_put_dev(interface_to_usbdev(intf));\n\trelease_firmware(priv->fw);\n\tp54_free_common(dev);\n}",
        "code_after_change": "static void p54u_disconnect(struct usb_interface *intf)\n{\n\tstruct ieee80211_hw *dev = usb_get_intfdata(intf);\n\tstruct p54u_priv *priv;\n\n\tif (!dev)\n\t\treturn;\n\n\tpriv = dev->priv;\n\twait_for_completion(&priv->fw_wait_load);\n\tp54_unregister_common(dev);\n\n\trelease_firmware(priv->fw);\n\tp54_free_common(dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,6 @@\n \twait_for_completion(&priv->fw_wait_load);\n \tp54_unregister_common(dev);\n \n-\tusb_put_dev(interface_to_usbdev(intf));\n \trelease_firmware(priv->fw);\n \tp54_free_common(dev);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tusb_put_dev(interface_to_usbdev(intf));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.2.1. There is a use-after-free caused by a malicious USB device in the drivers/net/wireless/intersil/p54/p54usb.c driver.",
        "id": 2005
    },
    {
        "cve_id": "CVE-2014-2568",
        "code_before_change": "static int queue_userspace_packet(struct datapath *dp, struct sk_buff *skb,\n\t\t\t\t  const struct dp_upcall_info *upcall_info)\n{\n\tstruct ovs_header *upcall;\n\tstruct sk_buff *nskb = NULL;\n\tstruct sk_buff *user_skb; /* to be queued to userspace */\n\tstruct nlattr *nla;\n\tstruct genl_info info = {\n\t\t.dst_sk = ovs_dp_get_net(dp)->genl_sock,\n\t\t.snd_portid = upcall_info->portid,\n\t};\n\tsize_t len;\n\tunsigned int hlen;\n\tint err, dp_ifindex;\n\n\tdp_ifindex = get_dpifindex(dp);\n\tif (!dp_ifindex)\n\t\treturn -ENODEV;\n\n\tif (vlan_tx_tag_present(skb)) {\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb = __vlan_put_tag(nskb, nskb->vlan_proto, vlan_tx_tag_get(nskb));\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb->vlan_tci = 0;\n\t\tskb = nskb;\n\t}\n\n\tif (nla_attr_size(skb->len) > USHRT_MAX) {\n\t\terr = -EFBIG;\n\t\tgoto out;\n\t}\n\n\t/* Complete checksum if needed */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t    (err = skb_checksum_help(skb)))\n\t\tgoto out;\n\n\t/* Older versions of OVS user space enforce alignment of the last\n\t * Netlink attribute to NLA_ALIGNTO which would require extensive\n\t * padding logic. Only perform zerocopy if padding is not required.\n\t */\n\tif (dp->user_features & OVS_DP_F_UNALIGNED)\n\t\thlen = skb_zerocopy_headlen(skb);\n\telse\n\t\thlen = skb->len;\n\n\tlen = upcall_msg_size(upcall_info->userdata, hlen);\n\tuser_skb = genlmsg_new_unicast(len, &info, GFP_ATOMIC);\n\tif (!user_skb) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tupcall = genlmsg_put(user_skb, 0, 0, &dp_packet_genl_family,\n\t\t\t     0, upcall_info->cmd);\n\tupcall->dp_ifindex = dp_ifindex;\n\n\tnla = nla_nest_start(user_skb, OVS_PACKET_ATTR_KEY);\n\tovs_nla_put_flow(upcall_info->key, upcall_info->key, user_skb);\n\tnla_nest_end(user_skb, nla);\n\n\tif (upcall_info->userdata)\n\t\t__nla_put(user_skb, OVS_PACKET_ATTR_USERDATA,\n\t\t\t  nla_len(upcall_info->userdata),\n\t\t\t  nla_data(upcall_info->userdata));\n\n\t/* Only reserve room for attribute header, packet data is added\n\t * in skb_zerocopy() */\n\tif (!(nla = nla_reserve(user_skb, OVS_PACKET_ATTR_PACKET, 0))) {\n\t\terr = -ENOBUFS;\n\t\tgoto out;\n\t}\n\tnla->nla_len = nla_attr_size(skb->len);\n\n\tskb_zerocopy(user_skb, skb, skb->len, hlen);\n\n\t/* Pad OVS_PACKET_ATTR_PACKET if linear copy was performed */\n\tif (!(dp->user_features & OVS_DP_F_UNALIGNED)) {\n\t\tsize_t plen = NLA_ALIGN(user_skb->len) - user_skb->len;\n\n\t\tif (plen > 0)\n\t\t\tmemset(skb_put(user_skb, plen), 0, plen);\n\t}\n\n\t((struct nlmsghdr *) user_skb->data)->nlmsg_len = user_skb->len;\n\n\terr = genlmsg_unicast(ovs_dp_get_net(dp), user_skb, upcall_info->portid);\nout:\n\tkfree_skb(nskb);\n\treturn err;\n}",
        "code_after_change": "static int queue_userspace_packet(struct datapath *dp, struct sk_buff *skb,\n\t\t\t\t  const struct dp_upcall_info *upcall_info)\n{\n\tstruct ovs_header *upcall;\n\tstruct sk_buff *nskb = NULL;\n\tstruct sk_buff *user_skb; /* to be queued to userspace */\n\tstruct nlattr *nla;\n\tstruct genl_info info = {\n\t\t.dst_sk = ovs_dp_get_net(dp)->genl_sock,\n\t\t.snd_portid = upcall_info->portid,\n\t};\n\tsize_t len;\n\tunsigned int hlen;\n\tint err, dp_ifindex;\n\n\tdp_ifindex = get_dpifindex(dp);\n\tif (!dp_ifindex)\n\t\treturn -ENODEV;\n\n\tif (vlan_tx_tag_present(skb)) {\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb = __vlan_put_tag(nskb, nskb->vlan_proto, vlan_tx_tag_get(nskb));\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb->vlan_tci = 0;\n\t\tskb = nskb;\n\t}\n\n\tif (nla_attr_size(skb->len) > USHRT_MAX) {\n\t\terr = -EFBIG;\n\t\tgoto out;\n\t}\n\n\t/* Complete checksum if needed */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t    (err = skb_checksum_help(skb)))\n\t\tgoto out;\n\n\t/* Older versions of OVS user space enforce alignment of the last\n\t * Netlink attribute to NLA_ALIGNTO which would require extensive\n\t * padding logic. Only perform zerocopy if padding is not required.\n\t */\n\tif (dp->user_features & OVS_DP_F_UNALIGNED)\n\t\thlen = skb_zerocopy_headlen(skb);\n\telse\n\t\thlen = skb->len;\n\n\tlen = upcall_msg_size(upcall_info->userdata, hlen);\n\tuser_skb = genlmsg_new_unicast(len, &info, GFP_ATOMIC);\n\tif (!user_skb) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tupcall = genlmsg_put(user_skb, 0, 0, &dp_packet_genl_family,\n\t\t\t     0, upcall_info->cmd);\n\tupcall->dp_ifindex = dp_ifindex;\n\n\tnla = nla_nest_start(user_skb, OVS_PACKET_ATTR_KEY);\n\tovs_nla_put_flow(upcall_info->key, upcall_info->key, user_skb);\n\tnla_nest_end(user_skb, nla);\n\n\tif (upcall_info->userdata)\n\t\t__nla_put(user_skb, OVS_PACKET_ATTR_USERDATA,\n\t\t\t  nla_len(upcall_info->userdata),\n\t\t\t  nla_data(upcall_info->userdata));\n\n\t/* Only reserve room for attribute header, packet data is added\n\t * in skb_zerocopy() */\n\tif (!(nla = nla_reserve(user_skb, OVS_PACKET_ATTR_PACKET, 0))) {\n\t\terr = -ENOBUFS;\n\t\tgoto out;\n\t}\n\tnla->nla_len = nla_attr_size(skb->len);\n\n\terr = skb_zerocopy(user_skb, skb, skb->len, hlen);\n\tif (err)\n\t\tgoto out;\n\n\t/* Pad OVS_PACKET_ATTR_PACKET if linear copy was performed */\n\tif (!(dp->user_features & OVS_DP_F_UNALIGNED)) {\n\t\tsize_t plen = NLA_ALIGN(user_skb->len) - user_skb->len;\n\n\t\tif (plen > 0)\n\t\t\tmemset(skb_put(user_skb, plen), 0, plen);\n\t}\n\n\t((struct nlmsghdr *) user_skb->data)->nlmsg_len = user_skb->len;\n\n\terr = genlmsg_unicast(ovs_dp_get_net(dp), user_skb, upcall_info->portid);\nout:\n\tif (err)\n\t\tskb_tx_error(skb);\n\tkfree_skb(nskb);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -77,7 +77,9 @@\n \t}\n \tnla->nla_len = nla_attr_size(skb->len);\n \n-\tskb_zerocopy(user_skb, skb, skb->len, hlen);\n+\terr = skb_zerocopy(user_skb, skb, skb->len, hlen);\n+\tif (err)\n+\t\tgoto out;\n \n \t/* Pad OVS_PACKET_ATTR_PACKET if linear copy was performed */\n \tif (!(dp->user_features & OVS_DP_F_UNALIGNED)) {\n@@ -91,6 +93,8 @@\n \n \terr = genlmsg_unicast(ovs_dp_get_net(dp), user_skb, upcall_info->portid);\n out:\n+\tif (err)\n+\t\tskb_tx_error(skb);\n \tkfree_skb(nskb);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\terr = skb_zerocopy(user_skb, skb, skb->len, hlen);",
                "\tif (err)",
                "\t\tgoto out;",
                "\tif (err)",
                "\t\tskb_tx_error(skb);"
            ],
            "deleted": [
                "\tskb_zerocopy(user_skb, skb, skb->len, hlen);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in the nfqnl_zcopy function in net/netfilter/nfnetlink_queue_core.c in the Linux kernel through 3.13.6 allows attackers to obtain sensitive information from kernel memory by leveraging the absence of a certain orphaning operation. NOTE: the affected code was moved to the skb_zerocopy function in net/core/skbuff.c before the vulnerability was announced.",
        "id": 485
    },
    {
        "cve_id": "CVE-2019-11815",
        "code_before_change": "static void rds_tcp_kill_sock(struct net *net)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\tLIST_HEAD(tmp_list);\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\tstruct socket *lsock = rtn->rds_tcp_listen_sock;\n\n\trtn->rds_tcp_listen_sock = NULL;\n\trds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n\t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n\n\t\tif (net != c_net || !tc->t_sock)\n\t\t\tcontinue;\n\t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n\t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);\n\t\t} else {\n\t\t\tlist_del(&tc->t_tcp_node);\n\t\t\ttc->t_tcp_node_detached = true;\n\t\t}\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)\n\t\trds_conn_destroy(tc->t_cpath->cp_conn);\n}",
        "code_after_change": "static void rds_tcp_kill_sock(struct net *net)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\tLIST_HEAD(tmp_list);\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\tstruct socket *lsock = rtn->rds_tcp_listen_sock;\n\n\trtn->rds_tcp_listen_sock = NULL;\n\trds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n\t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n\n\t\tif (net != c_net)\n\t\t\tcontinue;\n\t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n\t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);\n\t\t} else {\n\t\t\tlist_del(&tc->t_tcp_node);\n\t\t\ttc->t_tcp_node_detached = true;\n\t\t}\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)\n\t\trds_conn_destroy(tc->t_cpath->cp_conn);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n \t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n \n-\t\tif (net != c_net || !tc->t_sock)\n+\t\tif (net != c_net)\n \t\t\tcontinue;\n \t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n \t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);",
        "function_modified_lines": {
            "added": [
                "\t\tif (net != c_net)"
            ],
            "deleted": [
                "\t\tif (net != c_net || !tc->t_sock)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in rds_tcp_kill_sock in net/rds/tcp.c in the Linux kernel before 5.0.8. There is a race condition leading to a use-after-free, related to net namespace cleanup.",
        "id": 1935
    },
    {
        "cve_id": "CVE-2019-3896",
        "code_before_change": "void idr_remove_all(struct idr *idp)\n{\n\tint n, id, max;\n\tstruct idr_layer *p;\n\tstruct idr_layer *pa[MAX_LEVEL];\n\tstruct idr_layer **paa = &pa[0];\n\n\tn = idp->layers * IDR_BITS;\n\tp = idp->top;\n\trcu_assign_pointer(idp->top, NULL);\n\tmax = 1 << n;\n\n\tid = 0;\n\twhile (id < max) {\n\t\twhile (n > IDR_BITS && p) {\n\t\t\tn -= IDR_BITS;\n\t\t\t*paa++ = p;\n\t\t\tp = p->ary[(id >> n) & IDR_MASK];\n\t\t}\n\n\t\tid += 1 << n;\n\t\twhile (n < fls(id)) {\n\t\t\tif (p)\n\t\t\t\tfree_layer(p);\n\t\t\tn += IDR_BITS;\n\t\t\tp = *--paa;\n\t\t}\n\t}\n\tidp->layers = 0;\n}",
        "code_after_change": "void idr_remove_all(struct idr *idp)\n{\n\tint n, id, max;\n\tint bt_mask;\n\tstruct idr_layer *p;\n\tstruct idr_layer *pa[MAX_LEVEL];\n\tstruct idr_layer **paa = &pa[0];\n\n\tn = idp->layers * IDR_BITS;\n\tp = idp->top;\n\trcu_assign_pointer(idp->top, NULL);\n\tmax = 1 << n;\n\n\tid = 0;\n\twhile (id < max) {\n\t\twhile (n > IDR_BITS && p) {\n\t\t\tn -= IDR_BITS;\n\t\t\t*paa++ = p;\n\t\t\tp = p->ary[(id >> n) & IDR_MASK];\n\t\t}\n\n\t\tbt_mask = id;\n\t\tid += 1 << n;\n\t\t/* Get the highest bit that the above add changed from 0->1. */\n\t\twhile (n < fls(id ^ bt_mask)) {\n\t\t\tif (p)\n\t\t\t\tfree_layer(p);\n\t\t\tn += IDR_BITS;\n\t\t\tp = *--paa;\n\t\t}\n\t}\n\tidp->layers = 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,7 @@\n void idr_remove_all(struct idr *idp)\n {\n \tint n, id, max;\n+\tint bt_mask;\n \tstruct idr_layer *p;\n \tstruct idr_layer *pa[MAX_LEVEL];\n \tstruct idr_layer **paa = &pa[0];\n@@ -18,8 +19,10 @@\n \t\t\tp = p->ary[(id >> n) & IDR_MASK];\n \t\t}\n \n+\t\tbt_mask = id;\n \t\tid += 1 << n;\n-\t\twhile (n < fls(id)) {\n+\t\t/* Get the highest bit that the above add changed from 0->1. */\n+\t\twhile (n < fls(id ^ bt_mask)) {\n \t\t\tif (p)\n \t\t\t\tfree_layer(p);\n \t\t\tn += IDR_BITS;",
        "function_modified_lines": {
            "added": [
                "\tint bt_mask;",
                "\t\tbt_mask = id;",
                "\t\t/* Get the highest bit that the above add changed from 0->1. */",
                "\t\twhile (n < fls(id ^ bt_mask)) {"
            ],
            "deleted": [
                "\t\twhile (n < fls(id)) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A double-free can happen in idr_remove_all() in lib/idr.c in the Linux kernel 2.6 branch. An unprivileged local attacker can use this flaw for a privilege escalation or for a system crash and a denial of service (DoS).",
        "id": 2331
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static ssize_t sysfs_blk_trace_attr_store(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  const char *buf, size_t count)\n{\n\tstruct block_device *bdev;\n\tstruct request_queue *q;\n\tstruct hd_struct *p;\n\tu64 value;\n\tssize_t ret = -EINVAL;\n\n\tif (count == 0)\n\t\tgoto out;\n\n\tif (attr == &dev_attr_act_mask) {\n\t\tif (kstrtoull(buf, 0, &value)) {\n\t\t\t/* Assume it is a list of trace category names */\n\t\t\tret = blk_trace_str2mask(buf);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tvalue = ret;\n\t\t}\n\t} else if (kstrtoull(buf, 0, &value))\n\t\tgoto out;\n\n\tret = -ENXIO;\n\n\tp = dev_to_part(dev);\n\tbdev = bdget(part_devt(p));\n\tif (bdev == NULL)\n\t\tgoto out;\n\n\tq = blk_trace_get_queue(bdev);\n\tif (q == NULL)\n\t\tgoto out_bdput;\n\n\tmutex_lock(&q->blk_trace_mutex);\n\n\tif (attr == &dev_attr_enable) {\n\t\tif (!!value == !!q->blk_trace) {\n\t\t\tret = 0;\n\t\t\tgoto out_unlock_bdev;\n\t\t}\n\t\tif (value)\n\t\t\tret = blk_trace_setup_queue(q, bdev);\n\t\telse\n\t\t\tret = blk_trace_remove_queue(q);\n\t\tgoto out_unlock_bdev;\n\t}\n\n\tret = 0;\n\tif (q->blk_trace == NULL)\n\t\tret = blk_trace_setup_queue(q, bdev);\n\n\tif (ret == 0) {\n\t\tif (attr == &dev_attr_act_mask)\n\t\t\tq->blk_trace->act_mask = value;\n\t\telse if (attr == &dev_attr_pid)\n\t\t\tq->blk_trace->pid = value;\n\t\telse if (attr == &dev_attr_start_lba)\n\t\t\tq->blk_trace->start_lba = value;\n\t\telse if (attr == &dev_attr_end_lba)\n\t\t\tq->blk_trace->end_lba = value;\n\t}\n\nout_unlock_bdev:\n\tmutex_unlock(&q->blk_trace_mutex);\nout_bdput:\n\tbdput(bdev);\nout:\n\treturn ret ? ret : count;\n}",
        "code_after_change": "static ssize_t sysfs_blk_trace_attr_store(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  const char *buf, size_t count)\n{\n\tstruct block_device *bdev;\n\tstruct request_queue *q;\n\tstruct hd_struct *p;\n\tstruct blk_trace *bt;\n\tu64 value;\n\tssize_t ret = -EINVAL;\n\n\tif (count == 0)\n\t\tgoto out;\n\n\tif (attr == &dev_attr_act_mask) {\n\t\tif (kstrtoull(buf, 0, &value)) {\n\t\t\t/* Assume it is a list of trace category names */\n\t\t\tret = blk_trace_str2mask(buf);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tvalue = ret;\n\t\t}\n\t} else if (kstrtoull(buf, 0, &value))\n\t\tgoto out;\n\n\tret = -ENXIO;\n\n\tp = dev_to_part(dev);\n\tbdev = bdget(part_devt(p));\n\tif (bdev == NULL)\n\t\tgoto out;\n\n\tq = blk_trace_get_queue(bdev);\n\tif (q == NULL)\n\t\tgoto out_bdput;\n\n\tmutex_lock(&q->blk_trace_mutex);\n\n\tbt = rcu_dereference_protected(q->blk_trace,\n\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));\n\tif (attr == &dev_attr_enable) {\n\t\tif (!!value == !!bt) {\n\t\t\tret = 0;\n\t\t\tgoto out_unlock_bdev;\n\t\t}\n\t\tif (value)\n\t\t\tret = blk_trace_setup_queue(q, bdev);\n\t\telse\n\t\t\tret = blk_trace_remove_queue(q);\n\t\tgoto out_unlock_bdev;\n\t}\n\n\tret = 0;\n\tif (bt == NULL)\n\t\tret = blk_trace_setup_queue(q, bdev);\n\n\tif (ret == 0) {\n\t\tif (attr == &dev_attr_act_mask)\n\t\t\tbt->act_mask = value;\n\t\telse if (attr == &dev_attr_pid)\n\t\t\tbt->pid = value;\n\t\telse if (attr == &dev_attr_start_lba)\n\t\t\tbt->start_lba = value;\n\t\telse if (attr == &dev_attr_end_lba)\n\t\t\tbt->end_lba = value;\n\t}\n\nout_unlock_bdev:\n\tmutex_unlock(&q->blk_trace_mutex);\nout_bdput:\n\tbdput(bdev);\nout:\n\treturn ret ? ret : count;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tstruct block_device *bdev;\n \tstruct request_queue *q;\n \tstruct hd_struct *p;\n+\tstruct blk_trace *bt;\n \tu64 value;\n \tssize_t ret = -EINVAL;\n \n@@ -35,8 +36,10 @@\n \n \tmutex_lock(&q->blk_trace_mutex);\n \n+\tbt = rcu_dereference_protected(q->blk_trace,\n+\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));\n \tif (attr == &dev_attr_enable) {\n-\t\tif (!!value == !!q->blk_trace) {\n+\t\tif (!!value == !!bt) {\n \t\t\tret = 0;\n \t\t\tgoto out_unlock_bdev;\n \t\t}\n@@ -48,18 +51,18 @@\n \t}\n \n \tret = 0;\n-\tif (q->blk_trace == NULL)\n+\tif (bt == NULL)\n \t\tret = blk_trace_setup_queue(q, bdev);\n \n \tif (ret == 0) {\n \t\tif (attr == &dev_attr_act_mask)\n-\t\t\tq->blk_trace->act_mask = value;\n+\t\t\tbt->act_mask = value;\n \t\telse if (attr == &dev_attr_pid)\n-\t\t\tq->blk_trace->pid = value;\n+\t\t\tbt->pid = value;\n \t\telse if (attr == &dev_attr_start_lba)\n-\t\t\tq->blk_trace->start_lba = value;\n+\t\t\tbt->start_lba = value;\n \t\telse if (attr == &dev_attr_end_lba)\n-\t\t\tq->blk_trace->end_lba = value;\n+\t\t\tbt->end_lba = value;\n \t}\n \n out_unlock_bdev:",
        "function_modified_lines": {
            "added": [
                "\tstruct blk_trace *bt;",
                "\tbt = rcu_dereference_protected(q->blk_trace,",
                "\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));",
                "\t\tif (!!value == !!bt) {",
                "\tif (bt == NULL)",
                "\t\t\tbt->act_mask = value;",
                "\t\t\tbt->pid = value;",
                "\t\t\tbt->start_lba = value;",
                "\t\t\tbt->end_lba = value;"
            ],
            "deleted": [
                "\t\tif (!!value == !!q->blk_trace) {",
                "\tif (q->blk_trace == NULL)",
                "\t\t\tq->blk_trace->act_mask = value;",
                "\t\t\tq->blk_trace->pid = value;",
                "\t\t\tq->blk_trace->start_lba = value;",
                "\t\t\tq->blk_trace->end_lba = value;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2238
    },
    {
        "cve_id": "CVE-2014-4653",
        "code_before_change": "static int snd_ctl_tlv_ioctl(struct snd_ctl_file *file,\n                             struct snd_ctl_tlv __user *_tlv,\n                             int op_flag)\n{\n\tstruct snd_card *card = file->card;\n\tstruct snd_ctl_tlv tlv;\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int len;\n\tint err = 0;\n\n\tif (copy_from_user(&tlv, _tlv, sizeof(tlv)))\n\t\treturn -EFAULT;\n\tif (tlv.length < sizeof(unsigned int) * 2)\n\t\treturn -EINVAL;\n\tdown_read(&card->controls_rwsem);\n\tkctl = snd_ctl_find_numid(card, tlv.numid);\n\tif (kctl == NULL) {\n\t\terr = -ENOENT;\n\t\tgoto __kctl_end;\n\t}\n\tif (kctl->tlv.p == NULL) {\n\t\terr = -ENXIO;\n\t\tgoto __kctl_end;\n\t}\n\tvd = &kctl->vd[tlv.numid - kctl->id.numid];\n\tif ((op_flag == 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_READ) == 0) ||\n\t    (op_flag > 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_WRITE) == 0) ||\n\t    (op_flag < 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_COMMAND) == 0)) {\n\t    \terr = -ENXIO;\n\t    \tgoto __kctl_end;\n\t}\n\tif (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK) {\n\t\tif (vd->owner != NULL && vd->owner != file) {\n\t\t\terr = -EPERM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\terr = kctl->tlv.c(kctl, op_flag, tlv.length, _tlv->tlv);\n\t\tif (err > 0) {\n\t\t\tup_read(&card->controls_rwsem);\n\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &kctl->id);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tif (op_flag) {\n\t\t\terr = -ENXIO;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tlen = kctl->tlv.p[1] + 2 * sizeof(unsigned int);\n\t\tif (tlv.length < len) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tif (copy_to_user(_tlv->tlv, kctl->tlv.p, len))\n\t\t\terr = -EFAULT;\n\t}\n      __kctl_end:\n\tup_read(&card->controls_rwsem);\n\treturn err;\n}",
        "code_after_change": "static int snd_ctl_tlv_ioctl(struct snd_ctl_file *file,\n                             struct snd_ctl_tlv __user *_tlv,\n                             int op_flag)\n{\n\tstruct snd_card *card = file->card;\n\tstruct snd_ctl_tlv tlv;\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int len;\n\tint err = 0;\n\n\tif (copy_from_user(&tlv, _tlv, sizeof(tlv)))\n\t\treturn -EFAULT;\n\tif (tlv.length < sizeof(unsigned int) * 2)\n\t\treturn -EINVAL;\n\tdown_read(&card->controls_rwsem);\n\tkctl = snd_ctl_find_numid(card, tlv.numid);\n\tif (kctl == NULL) {\n\t\terr = -ENOENT;\n\t\tgoto __kctl_end;\n\t}\n\tif (kctl->tlv.p == NULL) {\n\t\terr = -ENXIO;\n\t\tgoto __kctl_end;\n\t}\n\tvd = &kctl->vd[tlv.numid - kctl->id.numid];\n\tif ((op_flag == 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_READ) == 0) ||\n\t    (op_flag > 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_WRITE) == 0) ||\n\t    (op_flag < 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_COMMAND) == 0)) {\n\t    \terr = -ENXIO;\n\t    \tgoto __kctl_end;\n\t}\n\tif (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK) {\n\t\tif (vd->owner != NULL && vd->owner != file) {\n\t\t\terr = -EPERM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\terr = kctl->tlv.c(kctl, op_flag, tlv.length, _tlv->tlv);\n\t\tif (err > 0) {\n\t\t\tstruct snd_ctl_elem_id id = kctl->id;\n\t\t\tup_read(&card->controls_rwsem);\n\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &id);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tif (op_flag) {\n\t\t\terr = -ENXIO;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tlen = kctl->tlv.p[1] + 2 * sizeof(unsigned int);\n\t\tif (tlv.length < len) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tif (copy_to_user(_tlv->tlv, kctl->tlv.p, len))\n\t\t\terr = -EFAULT;\n\t}\n      __kctl_end:\n\tup_read(&card->controls_rwsem);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -37,8 +37,9 @@\n \t\t}\n \t\terr = kctl->tlv.c(kctl, op_flag, tlv.length, _tlv->tlv);\n \t\tif (err > 0) {\n+\t\t\tstruct snd_ctl_elem_id id = kctl->id;\n \t\t\tup_read(&card->controls_rwsem);\n-\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &kctl->id);\n+\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &id);\n \t\t\treturn 0;\n \t\t}\n \t} else {",
        "function_modified_lines": {
            "added": [
                "\t\t\tstruct snd_ctl_elem_id id = kctl->id;",
                "\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &id);"
            ],
            "deleted": [
                "\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &kctl->id);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 does not ensure possession of a read/write lock, which allows local users to cause a denial of service (use-after-free) and obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "id": 568
    },
    {
        "cve_id": "CVE-2019-19526",
        "code_before_change": "static int pn533_usb_probe(struct usb_interface *interface,\n\t\t\tconst struct usb_device_id *id)\n{\n\tstruct pn533 *priv;\n\tstruct pn533_usb_phy *phy;\n\tstruct usb_host_interface *iface_desc;\n\tstruct usb_endpoint_descriptor *endpoint;\n\tint in_endpoint = 0;\n\tint out_endpoint = 0;\n\tint rc = -ENOMEM;\n\tint i;\n\tu32 protocols;\n\tenum pn533_protocol_type protocol_type = PN533_PROTO_REQ_ACK_RESP;\n\tstruct pn533_frame_ops *fops = NULL;\n\tunsigned char *in_buf;\n\tint in_buf_len = PN533_EXT_FRAME_HEADER_LEN +\n\t\t\t PN533_STD_FRAME_MAX_PAYLOAD_LEN +\n\t\t\t PN533_STD_FRAME_TAIL_LEN;\n\n\tphy = devm_kzalloc(&interface->dev, sizeof(*phy), GFP_KERNEL);\n\tif (!phy)\n\t\treturn -ENOMEM;\n\n\tin_buf = kzalloc(in_buf_len, GFP_KERNEL);\n\tif (!in_buf)\n\t\treturn -ENOMEM;\n\n\tphy->udev = usb_get_dev(interface_to_usbdev(interface));\n\tphy->interface = interface;\n\n\tiface_desc = interface->cur_altsetting;\n\tfor (i = 0; i < iface_desc->desc.bNumEndpoints; ++i) {\n\t\tendpoint = &iface_desc->endpoint[i].desc;\n\n\t\tif (!in_endpoint && usb_endpoint_is_bulk_in(endpoint))\n\t\t\tin_endpoint = endpoint->bEndpointAddress;\n\n\t\tif (!out_endpoint && usb_endpoint_is_bulk_out(endpoint))\n\t\t\tout_endpoint = endpoint->bEndpointAddress;\n\t}\n\n\tif (!in_endpoint || !out_endpoint) {\n\t\tnfc_err(&interface->dev,\n\t\t\t\"Could not find bulk-in or bulk-out endpoint\\n\");\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tphy->in_urb = usb_alloc_urb(0, GFP_KERNEL);\n\tphy->out_urb = usb_alloc_urb(0, GFP_KERNEL);\n\tphy->ack_urb = usb_alloc_urb(0, GFP_KERNEL);\n\n\tif (!phy->in_urb || !phy->out_urb || !phy->ack_urb)\n\t\tgoto error;\n\n\tusb_fill_bulk_urb(phy->in_urb, phy->udev,\n\t\t\t  usb_rcvbulkpipe(phy->udev, in_endpoint),\n\t\t\t  in_buf, in_buf_len, NULL, phy);\n\n\tusb_fill_bulk_urb(phy->out_urb, phy->udev,\n\t\t\t  usb_sndbulkpipe(phy->udev, out_endpoint),\n\t\t\t  NULL, 0, pn533_send_complete, phy);\n\tusb_fill_bulk_urb(phy->ack_urb, phy->udev,\n\t\t\t  usb_sndbulkpipe(phy->udev, out_endpoint),\n\t\t\t  NULL, 0, pn533_send_complete, phy);\n\n\tswitch (id->driver_info) {\n\tcase PN533_DEVICE_STD:\n\t\tprotocols = PN533_ALL_PROTOCOLS;\n\t\tbreak;\n\n\tcase PN533_DEVICE_PASORI:\n\t\tprotocols = PN533_NO_TYPE_B_PROTOCOLS;\n\t\tbreak;\n\n\tcase PN533_DEVICE_ACR122U:\n\t\tprotocols = PN533_NO_TYPE_B_PROTOCOLS;\n\t\tfops = &pn533_acr122_frame_ops;\n\t\tprotocol_type = PN533_PROTO_REQ_RESP,\n\n\t\trc = pn533_acr122_poweron_rdr(phy);\n\t\tif (rc < 0) {\n\t\t\tnfc_err(&interface->dev,\n\t\t\t\t\"Couldn't poweron the reader (error %d)\\n\", rc);\n\t\t\tgoto error;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tnfc_err(&interface->dev, \"Unknown device type %lu\\n\",\n\t\t\tid->driver_info);\n\t\trc = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tpriv = pn533_register_device(id->driver_info, protocols, protocol_type,\n\t\t\t\t\tphy, &usb_phy_ops, fops,\n\t\t\t\t\t&phy->udev->dev, &interface->dev);\n\n\tif (IS_ERR(priv)) {\n\t\trc = PTR_ERR(priv);\n\t\tgoto error;\n\t}\n\n\tphy->priv = priv;\n\n\trc = pn533_finalize_setup(priv);\n\tif (rc)\n\t\tgoto error;\n\n\tusb_set_intfdata(interface, phy);\n\n\treturn 0;\n\nerror:\n\tusb_free_urb(phy->in_urb);\n\tusb_free_urb(phy->out_urb);\n\tusb_free_urb(phy->ack_urb);\n\tusb_put_dev(phy->udev);\n\tkfree(in_buf);\n\n\treturn rc;\n}",
        "code_after_change": "static int pn533_usb_probe(struct usb_interface *interface,\n\t\t\tconst struct usb_device_id *id)\n{\n\tstruct pn533 *priv;\n\tstruct pn533_usb_phy *phy;\n\tstruct usb_host_interface *iface_desc;\n\tstruct usb_endpoint_descriptor *endpoint;\n\tint in_endpoint = 0;\n\tint out_endpoint = 0;\n\tint rc = -ENOMEM;\n\tint i;\n\tu32 protocols;\n\tenum pn533_protocol_type protocol_type = PN533_PROTO_REQ_ACK_RESP;\n\tstruct pn533_frame_ops *fops = NULL;\n\tunsigned char *in_buf;\n\tint in_buf_len = PN533_EXT_FRAME_HEADER_LEN +\n\t\t\t PN533_STD_FRAME_MAX_PAYLOAD_LEN +\n\t\t\t PN533_STD_FRAME_TAIL_LEN;\n\n\tphy = devm_kzalloc(&interface->dev, sizeof(*phy), GFP_KERNEL);\n\tif (!phy)\n\t\treturn -ENOMEM;\n\n\tin_buf = kzalloc(in_buf_len, GFP_KERNEL);\n\tif (!in_buf)\n\t\treturn -ENOMEM;\n\n\tphy->udev = usb_get_dev(interface_to_usbdev(interface));\n\tphy->interface = interface;\n\n\tiface_desc = interface->cur_altsetting;\n\tfor (i = 0; i < iface_desc->desc.bNumEndpoints; ++i) {\n\t\tendpoint = &iface_desc->endpoint[i].desc;\n\n\t\tif (!in_endpoint && usb_endpoint_is_bulk_in(endpoint))\n\t\t\tin_endpoint = endpoint->bEndpointAddress;\n\n\t\tif (!out_endpoint && usb_endpoint_is_bulk_out(endpoint))\n\t\t\tout_endpoint = endpoint->bEndpointAddress;\n\t}\n\n\tif (!in_endpoint || !out_endpoint) {\n\t\tnfc_err(&interface->dev,\n\t\t\t\"Could not find bulk-in or bulk-out endpoint\\n\");\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tphy->in_urb = usb_alloc_urb(0, GFP_KERNEL);\n\tphy->out_urb = usb_alloc_urb(0, GFP_KERNEL);\n\tphy->ack_urb = usb_alloc_urb(0, GFP_KERNEL);\n\n\tif (!phy->in_urb || !phy->out_urb || !phy->ack_urb)\n\t\tgoto error;\n\n\tusb_fill_bulk_urb(phy->in_urb, phy->udev,\n\t\t\t  usb_rcvbulkpipe(phy->udev, in_endpoint),\n\t\t\t  in_buf, in_buf_len, NULL, phy);\n\n\tusb_fill_bulk_urb(phy->out_urb, phy->udev,\n\t\t\t  usb_sndbulkpipe(phy->udev, out_endpoint),\n\t\t\t  NULL, 0, pn533_send_complete, phy);\n\tusb_fill_bulk_urb(phy->ack_urb, phy->udev,\n\t\t\t  usb_sndbulkpipe(phy->udev, out_endpoint),\n\t\t\t  NULL, 0, pn533_send_complete, phy);\n\n\tswitch (id->driver_info) {\n\tcase PN533_DEVICE_STD:\n\t\tprotocols = PN533_ALL_PROTOCOLS;\n\t\tbreak;\n\n\tcase PN533_DEVICE_PASORI:\n\t\tprotocols = PN533_NO_TYPE_B_PROTOCOLS;\n\t\tbreak;\n\n\tcase PN533_DEVICE_ACR122U:\n\t\tprotocols = PN533_NO_TYPE_B_PROTOCOLS;\n\t\tfops = &pn533_acr122_frame_ops;\n\t\tprotocol_type = PN533_PROTO_REQ_RESP,\n\n\t\trc = pn533_acr122_poweron_rdr(phy);\n\t\tif (rc < 0) {\n\t\t\tnfc_err(&interface->dev,\n\t\t\t\t\"Couldn't poweron the reader (error %d)\\n\", rc);\n\t\t\tgoto error;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tnfc_err(&interface->dev, \"Unknown device type %lu\\n\",\n\t\t\tid->driver_info);\n\t\trc = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tpriv = pn533_register_device(id->driver_info, protocols, protocol_type,\n\t\t\t\t\tphy, &usb_phy_ops, fops,\n\t\t\t\t\t&phy->udev->dev, &interface->dev);\n\n\tif (IS_ERR(priv)) {\n\t\trc = PTR_ERR(priv);\n\t\tgoto error;\n\t}\n\n\tphy->priv = priv;\n\n\trc = pn533_finalize_setup(priv);\n\tif (rc)\n\t\tgoto err_deregister;\n\n\tusb_set_intfdata(interface, phy);\n\n\treturn 0;\n\nerr_deregister:\n\tpn533_unregister_device(phy->priv);\nerror:\n\tusb_kill_urb(phy->in_urb);\n\tusb_kill_urb(phy->out_urb);\n\tusb_kill_urb(phy->ack_urb);\n\n\tusb_free_urb(phy->in_urb);\n\tusb_free_urb(phy->out_urb);\n\tusb_free_urb(phy->ack_urb);\n\tusb_put_dev(phy->udev);\n\tkfree(in_buf);\n\tkfree(phy->ack_buffer);\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -106,18 +106,25 @@\n \n \trc = pn533_finalize_setup(priv);\n \tif (rc)\n-\t\tgoto error;\n+\t\tgoto err_deregister;\n \n \tusb_set_intfdata(interface, phy);\n \n \treturn 0;\n \n+err_deregister:\n+\tpn533_unregister_device(phy->priv);\n error:\n+\tusb_kill_urb(phy->in_urb);\n+\tusb_kill_urb(phy->out_urb);\n+\tusb_kill_urb(phy->ack_urb);\n+\n \tusb_free_urb(phy->in_urb);\n \tusb_free_urb(phy->out_urb);\n \tusb_free_urb(phy->ack_urb);\n \tusb_put_dev(phy->udev);\n \tkfree(in_buf);\n+\tkfree(phy->ack_buffer);\n \n \treturn rc;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tgoto err_deregister;",
                "err_deregister:",
                "\tpn533_unregister_device(phy->priv);",
                "\tusb_kill_urb(phy->in_urb);",
                "\tusb_kill_urb(phy->out_urb);",
                "\tusb_kill_urb(phy->ack_urb);",
                "",
                "\tkfree(phy->ack_buffer);"
            ],
            "deleted": [
                "\t\tgoto error;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.3.9, there is a use-after-free bug that can be caused by a malicious USB device in the drivers/nfc/pn533/usb.c driver, aka CID-6af3aa57a098.",
        "id": 2201
    },
    {
        "cve_id": "CVE-2022-3586",
        "code_before_change": "static int sfb_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t       struct sk_buff **to_free)\n{\n\n\tstruct sfb_sched_data *q = qdisc_priv(sch);\n\tstruct Qdisc *child = q->qdisc;\n\tstruct tcf_proto *fl;\n\tint i;\n\tu32 p_min = ~0;\n\tu32 minqlen = ~0;\n\tu32 r, sfbhash;\n\tu32 slot = q->slot;\n\tint ret = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\n\tif (unlikely(sch->q.qlen >= q->limit)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.queuedrop++;\n\t\tgoto drop;\n\t}\n\n\tif (q->rehash_interval > 0) {\n\t\tunsigned long limit = q->rehash_time + q->rehash_interval;\n\n\t\tif (unlikely(time_after(jiffies, limit))) {\n\t\t\tsfb_swap_slot(q);\n\t\t\tq->rehash_time = jiffies;\n\t\t} else if (unlikely(!q->double_buffering && q->warmup_time > 0 &&\n\t\t\t\t    time_after(jiffies, limit - q->warmup_time))) {\n\t\t\tq->double_buffering = true;\n\t\t}\n\t}\n\n\tfl = rcu_dereference_bh(q->filter_list);\n\tif (fl) {\n\t\tu32 salt;\n\n\t\t/* If using external classifiers, get result and record it. */\n\t\tif (!sfb_classify(skb, fl, &ret, &salt))\n\t\t\tgoto other_drop;\n\t\tsfbhash = siphash_1u32(salt, &q->bins[slot].perturbation);\n\t} else {\n\t\tsfbhash = skb_get_hash_perturb(skb, &q->bins[slot].perturbation);\n\t}\n\n\n\tif (!sfbhash)\n\t\tsfbhash = 1;\n\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\tif (b->qlen == 0)\n\t\t\tdecrement_prob(b, q);\n\t\telse if (b->qlen >= q->bin_size)\n\t\t\tincrement_prob(b, q);\n\t\tif (minqlen > b->qlen)\n\t\t\tminqlen = b->qlen;\n\t\tif (p_min > b->p_mark)\n\t\t\tp_min = b->p_mark;\n\t}\n\n\tslot ^= 1;\n\tsfb_skb_cb(skb)->hashes[slot] = 0;\n\n\tif (unlikely(minqlen >= q->max)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.bucketdrop++;\n\t\tgoto drop;\n\t}\n\n\tif (unlikely(p_min >= SFB_MAX_PROB)) {\n\t\t/* Inelastic flow */\n\t\tif (q->double_buffering) {\n\t\t\tsfbhash = skb_get_hash_perturb(skb,\n\t\t\t    &q->bins[slot].perturbation);\n\t\t\tif (!sfbhash)\n\t\t\t\tsfbhash = 1;\n\t\t\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\t\t\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\t\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\t\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\t\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\t\t\tif (b->qlen == 0)\n\t\t\t\t\tdecrement_prob(b, q);\n\t\t\t\telse if (b->qlen >= q->bin_size)\n\t\t\t\t\tincrement_prob(b, q);\n\t\t\t}\n\t\t}\n\t\tif (sfb_rate_limit(skb, q)) {\n\t\t\tqdisc_qstats_overlimit(sch);\n\t\t\tq->stats.penaltydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t\tgoto enqueue;\n\t}\n\n\tr = prandom_u32() & SFB_MAX_PROB;\n\n\tif (unlikely(r < p_min)) {\n\t\tif (unlikely(p_min > SFB_MAX_PROB / 2)) {\n\t\t\t/* If we're marking that many packets, then either\n\t\t\t * this flow is unresponsive, or we're badly congested.\n\t\t\t * In either case, we want to start dropping packets.\n\t\t\t */\n\t\t\tif (r < (p_min - SFB_MAX_PROB / 2) * 2) {\n\t\t\t\tq->stats.earlydrop++;\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\t\tif (INET_ECN_set_ce(skb)) {\n\t\t\tq->stats.marked++;\n\t\t} else {\n\t\t\tq->stats.earlydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t}\n\nenqueue:\n\tret = qdisc_enqueue(skb, child, to_free);\n\tif (likely(ret == NET_XMIT_SUCCESS)) {\n\t\tqdisc_qstats_backlog_inc(sch, skb);\n\t\tsch->q.qlen++;\n\t\tincrement_qlen(skb, q);\n\t} else if (net_xmit_drop_count(ret)) {\n\t\tq->stats.childdrop++;\n\t\tqdisc_qstats_drop(sch);\n\t}\n\treturn ret;\n\ndrop:\n\tqdisc_drop(skb, sch, to_free);\n\treturn NET_XMIT_CN;\nother_drop:\n\tif (ret & __NET_XMIT_BYPASS)\n\t\tqdisc_qstats_drop(sch);\n\tkfree_skb(skb);\n\treturn ret;\n}",
        "code_after_change": "static int sfb_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t       struct sk_buff **to_free)\n{\n\n\tstruct sfb_sched_data *q = qdisc_priv(sch);\n\tstruct Qdisc *child = q->qdisc;\n\tstruct tcf_proto *fl;\n\tstruct sfb_skb_cb cb;\n\tint i;\n\tu32 p_min = ~0;\n\tu32 minqlen = ~0;\n\tu32 r, sfbhash;\n\tu32 slot = q->slot;\n\tint ret = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\n\tif (unlikely(sch->q.qlen >= q->limit)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.queuedrop++;\n\t\tgoto drop;\n\t}\n\n\tif (q->rehash_interval > 0) {\n\t\tunsigned long limit = q->rehash_time + q->rehash_interval;\n\n\t\tif (unlikely(time_after(jiffies, limit))) {\n\t\t\tsfb_swap_slot(q);\n\t\t\tq->rehash_time = jiffies;\n\t\t} else if (unlikely(!q->double_buffering && q->warmup_time > 0 &&\n\t\t\t\t    time_after(jiffies, limit - q->warmup_time))) {\n\t\t\tq->double_buffering = true;\n\t\t}\n\t}\n\n\tfl = rcu_dereference_bh(q->filter_list);\n\tif (fl) {\n\t\tu32 salt;\n\n\t\t/* If using external classifiers, get result and record it. */\n\t\tif (!sfb_classify(skb, fl, &ret, &salt))\n\t\t\tgoto other_drop;\n\t\tsfbhash = siphash_1u32(salt, &q->bins[slot].perturbation);\n\t} else {\n\t\tsfbhash = skb_get_hash_perturb(skb, &q->bins[slot].perturbation);\n\t}\n\n\n\tif (!sfbhash)\n\t\tsfbhash = 1;\n\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\tif (b->qlen == 0)\n\t\t\tdecrement_prob(b, q);\n\t\telse if (b->qlen >= q->bin_size)\n\t\t\tincrement_prob(b, q);\n\t\tif (minqlen > b->qlen)\n\t\t\tminqlen = b->qlen;\n\t\tif (p_min > b->p_mark)\n\t\t\tp_min = b->p_mark;\n\t}\n\n\tslot ^= 1;\n\tsfb_skb_cb(skb)->hashes[slot] = 0;\n\n\tif (unlikely(minqlen >= q->max)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.bucketdrop++;\n\t\tgoto drop;\n\t}\n\n\tif (unlikely(p_min >= SFB_MAX_PROB)) {\n\t\t/* Inelastic flow */\n\t\tif (q->double_buffering) {\n\t\t\tsfbhash = skb_get_hash_perturb(skb,\n\t\t\t    &q->bins[slot].perturbation);\n\t\t\tif (!sfbhash)\n\t\t\t\tsfbhash = 1;\n\t\t\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\t\t\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\t\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\t\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\t\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\t\t\tif (b->qlen == 0)\n\t\t\t\t\tdecrement_prob(b, q);\n\t\t\t\telse if (b->qlen >= q->bin_size)\n\t\t\t\t\tincrement_prob(b, q);\n\t\t\t}\n\t\t}\n\t\tif (sfb_rate_limit(skb, q)) {\n\t\t\tqdisc_qstats_overlimit(sch);\n\t\t\tq->stats.penaltydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t\tgoto enqueue;\n\t}\n\n\tr = prandom_u32() & SFB_MAX_PROB;\n\n\tif (unlikely(r < p_min)) {\n\t\tif (unlikely(p_min > SFB_MAX_PROB / 2)) {\n\t\t\t/* If we're marking that many packets, then either\n\t\t\t * this flow is unresponsive, or we're badly congested.\n\t\t\t * In either case, we want to start dropping packets.\n\t\t\t */\n\t\t\tif (r < (p_min - SFB_MAX_PROB / 2) * 2) {\n\t\t\t\tq->stats.earlydrop++;\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\t\tif (INET_ECN_set_ce(skb)) {\n\t\t\tq->stats.marked++;\n\t\t} else {\n\t\t\tq->stats.earlydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t}\n\nenqueue:\n\tmemcpy(&cb, sfb_skb_cb(skb), sizeof(cb));\n\tret = qdisc_enqueue(skb, child, to_free);\n\tif (likely(ret == NET_XMIT_SUCCESS)) {\n\t\tqdisc_qstats_backlog_inc(sch, skb);\n\t\tsch->q.qlen++;\n\t\tincrement_qlen(&cb, q);\n\t} else if (net_xmit_drop_count(ret)) {\n\t\tq->stats.childdrop++;\n\t\tqdisc_qstats_drop(sch);\n\t}\n\treturn ret;\n\ndrop:\n\tqdisc_drop(skb, sch, to_free);\n\treturn NET_XMIT_CN;\nother_drop:\n\tif (ret & __NET_XMIT_BYPASS)\n\t\tqdisc_qstats_drop(sch);\n\tkfree_skb(skb);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tstruct sfb_sched_data *q = qdisc_priv(sch);\n \tstruct Qdisc *child = q->qdisc;\n \tstruct tcf_proto *fl;\n+\tstruct sfb_skb_cb cb;\n \tint i;\n \tu32 p_min = ~0;\n \tu32 minqlen = ~0;\n@@ -121,11 +122,12 @@\n \t}\n \n enqueue:\n+\tmemcpy(&cb, sfb_skb_cb(skb), sizeof(cb));\n \tret = qdisc_enqueue(skb, child, to_free);\n \tif (likely(ret == NET_XMIT_SUCCESS)) {\n \t\tqdisc_qstats_backlog_inc(sch, skb);\n \t\tsch->q.qlen++;\n-\t\tincrement_qlen(skb, q);\n+\t\tincrement_qlen(&cb, q);\n \t} else if (net_xmit_drop_count(ret)) {\n \t\tq->stats.childdrop++;\n \t\tqdisc_qstats_drop(sch);",
        "function_modified_lines": {
            "added": [
                "\tstruct sfb_skb_cb cb;",
                "\tmemcpy(&cb, sfb_skb_cb(skb), sizeof(cb));",
                "\t\tincrement_qlen(&cb, q);"
            ],
            "deleted": [
                "\t\tincrement_qlen(skb, q);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s networking code. A use-after-free was found in the way the sch_sfb enqueue function used the socket buffer (SKB) cb field after the same SKB had been enqueued (and freed) into a child qdisc. This flaw allows a local, unprivileged user to crash the system, causing a denial of service.",
        "id": 3651
    },
    {
        "cve_id": "CVE-2019-15214",
        "code_before_change": "int snd_card_disconnect(struct snd_card *card)\n{\n\tstruct snd_monitor_file *mfile;\n\n\tif (!card)\n\t\treturn -EINVAL;\n\n\tspin_lock(&card->files_lock);\n\tif (card->shutdown) {\n\t\tspin_unlock(&card->files_lock);\n\t\treturn 0;\n\t}\n\tcard->shutdown = 1;\n\tspin_unlock(&card->files_lock);\n\n\t/* phase 1: disable fops (user space) operations for ALSA API */\n\tmutex_lock(&snd_card_mutex);\n\tsnd_cards[card->number] = NULL;\n\tclear_bit(card->number, snd_cards_lock);\n\tmutex_unlock(&snd_card_mutex);\n\n\t/* phase 2: replace file->f_op with special dummy operations */\n\n\tspin_lock(&card->files_lock);\n\tlist_for_each_entry(mfile, &card->files_list, list) {\n\t\t/* it's critical part, use endless loop */\n\t\t/* we have no room to fail */\n\t\tmfile->disconnected_f_op = mfile->file->f_op;\n\n\t\tspin_lock(&shutdown_lock);\n\t\tlist_add(&mfile->shutdown_list, &shutdown_files);\n\t\tspin_unlock(&shutdown_lock);\n\n\t\tmfile->file->f_op = &snd_shutdown_f_ops;\n\t\tfops_get(mfile->file->f_op);\n\t}\n\tspin_unlock(&card->files_lock);\t\n\n\t/* phase 3: notify all connected devices about disconnection */\n\t/* at this point, they cannot respond to any calls except release() */\n\n#if IS_ENABLED(CONFIG_SND_MIXER_OSS)\n\tif (snd_mixer_oss_notify_callback)\n\t\tsnd_mixer_oss_notify_callback(card, SND_MIXER_OSS_NOTIFY_DISCONNECT);\n#endif\n\n\t/* notify all devices that we are disconnected */\n\tsnd_device_disconnect_all(card);\n\n\tsnd_info_card_disconnect(card);\n\tif (card->registered) {\n\t\tdevice_del(&card->card_dev);\n\t\tcard->registered = false;\n\t}\n#ifdef CONFIG_PM\n\twake_up(&card->power_sleep);\n#endif\n\treturn 0;\t\n}",
        "code_after_change": "int snd_card_disconnect(struct snd_card *card)\n{\n\tstruct snd_monitor_file *mfile;\n\n\tif (!card)\n\t\treturn -EINVAL;\n\n\tspin_lock(&card->files_lock);\n\tif (card->shutdown) {\n\t\tspin_unlock(&card->files_lock);\n\t\treturn 0;\n\t}\n\tcard->shutdown = 1;\n\tspin_unlock(&card->files_lock);\n\n\t/* replace file->f_op with special dummy operations */\n\tspin_lock(&card->files_lock);\n\tlist_for_each_entry(mfile, &card->files_list, list) {\n\t\t/* it's critical part, use endless loop */\n\t\t/* we have no room to fail */\n\t\tmfile->disconnected_f_op = mfile->file->f_op;\n\n\t\tspin_lock(&shutdown_lock);\n\t\tlist_add(&mfile->shutdown_list, &shutdown_files);\n\t\tspin_unlock(&shutdown_lock);\n\n\t\tmfile->file->f_op = &snd_shutdown_f_ops;\n\t\tfops_get(mfile->file->f_op);\n\t}\n\tspin_unlock(&card->files_lock);\t\n\n\t/* notify all connected devices about disconnection */\n\t/* at this point, they cannot respond to any calls except release() */\n\n#if IS_ENABLED(CONFIG_SND_MIXER_OSS)\n\tif (snd_mixer_oss_notify_callback)\n\t\tsnd_mixer_oss_notify_callback(card, SND_MIXER_OSS_NOTIFY_DISCONNECT);\n#endif\n\n\t/* notify all devices that we are disconnected */\n\tsnd_device_disconnect_all(card);\n\n\tsnd_info_card_disconnect(card);\n\tif (card->registered) {\n\t\tdevice_del(&card->card_dev);\n\t\tcard->registered = false;\n\t}\n\n\t/* disable fops (user space) operations for ALSA API */\n\tmutex_lock(&snd_card_mutex);\n\tsnd_cards[card->number] = NULL;\n\tclear_bit(card->number, snd_cards_lock);\n\tmutex_unlock(&snd_card_mutex);\n\n#ifdef CONFIG_PM\n\twake_up(&card->power_sleep);\n#endif\n\treturn 0;\t\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,14 +13,7 @@\n \tcard->shutdown = 1;\n \tspin_unlock(&card->files_lock);\n \n-\t/* phase 1: disable fops (user space) operations for ALSA API */\n-\tmutex_lock(&snd_card_mutex);\n-\tsnd_cards[card->number] = NULL;\n-\tclear_bit(card->number, snd_cards_lock);\n-\tmutex_unlock(&snd_card_mutex);\n-\n-\t/* phase 2: replace file->f_op with special dummy operations */\n-\n+\t/* replace file->f_op with special dummy operations */\n \tspin_lock(&card->files_lock);\n \tlist_for_each_entry(mfile, &card->files_list, list) {\n \t\t/* it's critical part, use endless loop */\n@@ -36,7 +29,7 @@\n \t}\n \tspin_unlock(&card->files_lock);\t\n \n-\t/* phase 3: notify all connected devices about disconnection */\n+\t/* notify all connected devices about disconnection */\n \t/* at this point, they cannot respond to any calls except release() */\n \n #if IS_ENABLED(CONFIG_SND_MIXER_OSS)\n@@ -52,6 +45,13 @@\n \t\tdevice_del(&card->card_dev);\n \t\tcard->registered = false;\n \t}\n+\n+\t/* disable fops (user space) operations for ALSA API */\n+\tmutex_lock(&snd_card_mutex);\n+\tsnd_cards[card->number] = NULL;\n+\tclear_bit(card->number, snd_cards_lock);\n+\tmutex_unlock(&snd_card_mutex);\n+\n #ifdef CONFIG_PM\n \twake_up(&card->power_sleep);\n #endif",
        "function_modified_lines": {
            "added": [
                "\t/* replace file->f_op with special dummy operations */",
                "\t/* notify all connected devices about disconnection */",
                "",
                "\t/* disable fops (user space) operations for ALSA API */",
                "\tmutex_lock(&snd_card_mutex);",
                "\tsnd_cards[card->number] = NULL;",
                "\tclear_bit(card->number, snd_cards_lock);",
                "\tmutex_unlock(&snd_card_mutex);",
                ""
            ],
            "deleted": [
                "\t/* phase 1: disable fops (user space) operations for ALSA API */",
                "\tmutex_lock(&snd_card_mutex);",
                "\tsnd_cards[card->number] = NULL;",
                "\tclear_bit(card->number, snd_cards_lock);",
                "\tmutex_unlock(&snd_card_mutex);",
                "",
                "\t/* phase 2: replace file->f_op with special dummy operations */",
                "",
                "\t/* phase 3: notify all connected devices about disconnection */"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.10. There is a use-after-free in the sound subsystem because card disconnection causes certain data structures to be deleted too early. This is related to sound/core/init.c and sound/core/info.c.",
        "id": 1996
    },
    {
        "cve_id": "CVE-2020-29660",
        "code_before_change": "void disassociate_ctty(int on_exit)\n{\n\tstruct tty_struct *tty;\n\n\tif (!current->signal->leader)\n\t\treturn;\n\n\ttty = get_current_tty();\n\tif (tty) {\n\t\tif (on_exit && tty->driver->type != TTY_DRIVER_TYPE_PTY) {\n\t\t\ttty_vhangup_session(tty);\n\t\t} else {\n\t\t\tstruct pid *tty_pgrp = tty_get_pgrp(tty);\n\t\t\tif (tty_pgrp) {\n\t\t\t\tkill_pgrp(tty_pgrp, SIGHUP, on_exit);\n\t\t\t\tif (!on_exit)\n\t\t\t\t\tkill_pgrp(tty_pgrp, SIGCONT, on_exit);\n\t\t\t\tput_pid(tty_pgrp);\n\t\t\t}\n\t\t}\n\t\ttty_kref_put(tty);\n\n\t} else if (on_exit) {\n\t\tstruct pid *old_pgrp;\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\told_pgrp = current->signal->tty_old_pgrp;\n\t\tcurrent->signal->tty_old_pgrp = NULL;\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tif (old_pgrp) {\n\t\t\tkill_pgrp(old_pgrp, SIGHUP, on_exit);\n\t\t\tkill_pgrp(old_pgrp, SIGCONT, on_exit);\n\t\t\tput_pid(old_pgrp);\n\t\t}\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty_old_pgrp = NULL;\n\n\ttty = tty_kref_get(current->signal->tty);\n\tif (tty) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t\tput_pid(tty->session);\n\t\tput_pid(tty->pgrp);\n\t\ttty->session = NULL;\n\t\ttty->pgrp = NULL;\n\t\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\t\ttty_kref_put(tty);\n\t}\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\t/* Now clear signal->tty under the lock */\n\tread_lock(&tasklist_lock);\n\tsession_clear_tty(task_session(current));\n\tread_unlock(&tasklist_lock);\n}",
        "code_after_change": "void disassociate_ctty(int on_exit)\n{\n\tstruct tty_struct *tty;\n\n\tif (!current->signal->leader)\n\t\treturn;\n\n\ttty = get_current_tty();\n\tif (tty) {\n\t\tif (on_exit && tty->driver->type != TTY_DRIVER_TYPE_PTY) {\n\t\t\ttty_vhangup_session(tty);\n\t\t} else {\n\t\t\tstruct pid *tty_pgrp = tty_get_pgrp(tty);\n\t\t\tif (tty_pgrp) {\n\t\t\t\tkill_pgrp(tty_pgrp, SIGHUP, on_exit);\n\t\t\t\tif (!on_exit)\n\t\t\t\t\tkill_pgrp(tty_pgrp, SIGCONT, on_exit);\n\t\t\t\tput_pid(tty_pgrp);\n\t\t\t}\n\t\t}\n\t\ttty_kref_put(tty);\n\n\t} else if (on_exit) {\n\t\tstruct pid *old_pgrp;\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\told_pgrp = current->signal->tty_old_pgrp;\n\t\tcurrent->signal->tty_old_pgrp = NULL;\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tif (old_pgrp) {\n\t\t\tkill_pgrp(old_pgrp, SIGHUP, on_exit);\n\t\t\tkill_pgrp(old_pgrp, SIGCONT, on_exit);\n\t\t\tput_pid(old_pgrp);\n\t\t}\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty_old_pgrp = NULL;\n\ttty = tty_kref_get(current->signal->tty);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\tif (tty) {\n\t\tunsigned long flags;\n\n\t\ttty_lock(tty);\n\t\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t\tput_pid(tty->session);\n\t\tput_pid(tty->pgrp);\n\t\ttty->session = NULL;\n\t\ttty->pgrp = NULL;\n\t\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\t\ttty_unlock(tty);\n\t\ttty_kref_put(tty);\n\t}\n\n\t/* Now clear signal->tty under the lock */\n\tread_lock(&tasklist_lock);\n\tsession_clear_tty(task_session(current));\n\tread_unlock(&tasklist_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -37,20 +37,23 @@\n \tspin_lock_irq(&current->sighand->siglock);\n \tput_pid(current->signal->tty_old_pgrp);\n \tcurrent->signal->tty_old_pgrp = NULL;\n+\ttty = tty_kref_get(current->signal->tty);\n+\tspin_unlock_irq(&current->sighand->siglock);\n \n-\ttty = tty_kref_get(current->signal->tty);\n \tif (tty) {\n \t\tunsigned long flags;\n+\n+\t\ttty_lock(tty);\n \t\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n \t\tput_pid(tty->session);\n \t\tput_pid(tty->pgrp);\n \t\ttty->session = NULL;\n \t\ttty->pgrp = NULL;\n \t\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n+\t\ttty_unlock(tty);\n \t\ttty_kref_put(tty);\n \t}\n \n-\tspin_unlock_irq(&current->sighand->siglock);\n \t/* Now clear signal->tty under the lock */\n \tread_lock(&tasklist_lock);\n \tsession_clear_tty(task_session(current));",
        "function_modified_lines": {
            "added": [
                "\ttty = tty_kref_get(current->signal->tty);",
                "\tspin_unlock_irq(&current->sighand->siglock);",
                "",
                "\t\ttty_lock(tty);",
                "\t\ttty_unlock(tty);"
            ],
            "deleted": [
                "\ttty = tty_kref_get(current->signal->tty);",
                "\tspin_unlock_irq(&current->sighand->siglock);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "A locking inconsistency issue was discovered in the tty subsystem of the Linux kernel through 5.9.13. drivers/tty/tty_io.c and drivers/tty/tty_jobctrl.c may allow a read-after-free attack against TIOCGSID, aka CID-c8bcd9c5be24.",
        "id": 2701
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "static int llcp_raw_sock_bind(struct socket *sock, struct sockaddr *addr,\n\t\t\t      int alen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct nfc_llcp_local *local;\n\tstruct nfc_dev *dev;\n\tstruct sockaddr_nfc_llcp llcp_addr;\n\tint len, ret = 0;\n\n\tif (!addr || alen < offsetofend(struct sockaddr, sa_family) ||\n\t    addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"sk %p addr %p family %d\\n\", sk, addr, addr->sa_family);\n\n\tmemset(&llcp_addr, 0, sizeof(llcp_addr));\n\tlen = min_t(unsigned int, sizeof(llcp_addr), alen);\n\tmemcpy(&llcp_addr, addr, len);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != LLCP_CLOSED) {\n\t\tret = -EBADFD;\n\t\tgoto error;\n\t}\n\n\tdev = nfc_get_device(llcp_addr.dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = nfc_llcp_local_get(local);\n\tllcp_sock->nfc_protocol = llcp_addr.nfc_protocol;\n\n\tnfc_llcp_sock_link(&local->raw_sockets, sk);\n\n\tsk->sk_state = LLCP_BOUND;\n\nput_dev:\n\tnfc_put_device(dev);\n\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "code_after_change": "static int llcp_raw_sock_bind(struct socket *sock, struct sockaddr *addr,\n\t\t\t      int alen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct nfc_llcp_local *local;\n\tstruct nfc_dev *dev;\n\tstruct sockaddr_nfc_llcp llcp_addr;\n\tint len, ret = 0;\n\n\tif (!addr || alen < offsetofend(struct sockaddr, sa_family) ||\n\t    addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"sk %p addr %p family %d\\n\", sk, addr, addr->sa_family);\n\n\tmemset(&llcp_addr, 0, sizeof(llcp_addr));\n\tlen = min_t(unsigned int, sizeof(llcp_addr), alen);\n\tmemcpy(&llcp_addr, addr, len);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != LLCP_CLOSED) {\n\t\tret = -EBADFD;\n\t\tgoto error;\n\t}\n\n\tdev = nfc_get_device(llcp_addr.dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = local;\n\tllcp_sock->nfc_protocol = llcp_addr.nfc_protocol;\n\n\tnfc_llcp_sock_link(&local->raw_sockets, sk);\n\n\tsk->sk_state = LLCP_BOUND;\n\nput_dev:\n\tnfc_put_device(dev);\n\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,7 +38,7 @@\n \t}\n \n \tllcp_sock->dev = dev;\n-\tllcp_sock->local = nfc_llcp_local_get(local);\n+\tllcp_sock->local = local;\n \tllcp_sock->nfc_protocol = llcp_addr.nfc_protocol;\n \n \tnfc_llcp_sock_link(&local->raw_sockets, sk);",
        "function_modified_lines": {
            "added": [
                "\tllcp_sock->local = local;"
            ],
            "deleted": [
                "\tllcp_sock->local = nfc_llcp_local_get(local);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4154
    },
    {
        "cve_id": "CVE-2023-5197",
        "code_before_change": "static int nf_tables_delchain(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t      const struct nlattr * const nla[])\n{\n\tstruct netlink_ext_ack *extack = info->extack;\n\tu8 genmask = nft_genmask_next(info->net);\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct net *net = info->net;\n\tconst struct nlattr *attr;\n\tstruct nft_table *table;\n\tstruct nft_chain *chain;\n\tstruct nft_rule *rule;\n\tstruct nft_ctx ctx;\n\tu64 handle;\n\tu32 use;\n\tint err;\n\n\ttable = nft_table_lookup(net, nla[NFTA_CHAIN_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_CHAIN_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_CHAIN_HANDLE]) {\n\t\tattr = nla[NFTA_CHAIN_HANDLE];\n\t\thandle = be64_to_cpu(nla_get_be64(attr));\n\t\tchain = nft_chain_lookup_byhandle(table, handle, genmask);\n\t} else {\n\t\tattr = nla[NFTA_CHAIN_NAME];\n\t\tchain = nft_chain_lookup(net, table, attr, genmask);\n\t}\n\tif (IS_ERR(chain)) {\n\t\tif (PTR_ERR(chain) == -ENOENT &&\n\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYCHAIN)\n\t\t\treturn 0;\n\n\t\tNL_SET_BAD_ATTR(extack, attr);\n\t\treturn PTR_ERR(chain);\n\t}\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tif (nla[NFTA_CHAIN_HOOK]) {\n\t\tif (chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (nft_is_base_chain(chain)) {\n\t\t\tstruct nft_base_chain *basechain = nft_base_chain(chain);\n\n\t\t\tif (nft_base_chain_netdev(table->family, basechain->ops.hooknum))\n\t\t\t\treturn nft_delchain_hook(&ctx, basechain, extack);\n\t\t}\n\t}\n\n\tif (info->nlh->nlmsg_flags & NLM_F_NONREC &&\n\t    chain->use > 0)\n\t\treturn -EBUSY;\n\n\tuse = chain->use;\n\tlist_for_each_entry(rule, &chain->rules, list) {\n\t\tif (!nft_is_active_next(net, rule))\n\t\t\tcontinue;\n\t\tuse--;\n\n\t\terr = nft_delrule(&ctx, rule);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\t/* There are rules and elements that are still holding references to us,\n\t * we cannot do a recursive removal in this case.\n\t */\n\tif (use > 0) {\n\t\tNL_SET_BAD_ATTR(extack, attr);\n\t\treturn -EBUSY;\n\t}\n\n\treturn nft_delchain(&ctx);\n}",
        "code_after_change": "static int nf_tables_delchain(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t      const struct nlattr * const nla[])\n{\n\tstruct netlink_ext_ack *extack = info->extack;\n\tu8 genmask = nft_genmask_next(info->net);\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct net *net = info->net;\n\tconst struct nlattr *attr;\n\tstruct nft_table *table;\n\tstruct nft_chain *chain;\n\tstruct nft_rule *rule;\n\tstruct nft_ctx ctx;\n\tu64 handle;\n\tu32 use;\n\tint err;\n\n\ttable = nft_table_lookup(net, nla[NFTA_CHAIN_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_CHAIN_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_CHAIN_HANDLE]) {\n\t\tattr = nla[NFTA_CHAIN_HANDLE];\n\t\thandle = be64_to_cpu(nla_get_be64(attr));\n\t\tchain = nft_chain_lookup_byhandle(table, handle, genmask);\n\t} else {\n\t\tattr = nla[NFTA_CHAIN_NAME];\n\t\tchain = nft_chain_lookup(net, table, attr, genmask);\n\t}\n\tif (IS_ERR(chain)) {\n\t\tif (PTR_ERR(chain) == -ENOENT &&\n\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYCHAIN)\n\t\t\treturn 0;\n\n\t\tNL_SET_BAD_ATTR(extack, attr);\n\t\treturn PTR_ERR(chain);\n\t}\n\n\tif (nft_chain_binding(chain))\n\t\treturn -EOPNOTSUPP;\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tif (nla[NFTA_CHAIN_HOOK]) {\n\t\tif (chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (nft_is_base_chain(chain)) {\n\t\t\tstruct nft_base_chain *basechain = nft_base_chain(chain);\n\n\t\t\tif (nft_base_chain_netdev(table->family, basechain->ops.hooknum))\n\t\t\t\treturn nft_delchain_hook(&ctx, basechain, extack);\n\t\t}\n\t}\n\n\tif (info->nlh->nlmsg_flags & NLM_F_NONREC &&\n\t    chain->use > 0)\n\t\treturn -EBUSY;\n\n\tuse = chain->use;\n\tlist_for_each_entry(rule, &chain->rules, list) {\n\t\tif (!nft_is_active_next(net, rule))\n\t\t\tcontinue;\n\t\tuse--;\n\n\t\terr = nft_delrule(&ctx, rule);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\t/* There are rules and elements that are still holding references to us,\n\t * we cannot do a recursive removal in this case.\n\t */\n\tif (use > 0) {\n\t\tNL_SET_BAD_ATTR(extack, attr);\n\t\treturn -EBUSY;\n\t}\n\n\treturn nft_delchain(&ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -38,6 +38,9 @@\n \t\treturn PTR_ERR(chain);\n \t}\n \n+\tif (nft_chain_binding(chain))\n+\t\treturn -EOPNOTSUPP;\n+\n \tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n \n \tif (nla[NFTA_CHAIN_HOOK]) {",
        "function_modified_lines": {
            "added": [
                "\tif (nft_chain_binding(chain))",
                "\t\treturn -EOPNOTSUPP;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nAddition and removal of rules from chain bindings within the same transaction causes leads to use-after-free.\n\nWe recommend upgrading past commit f15f29fd4779be8a418b66e9d52979bb6d6c2325.\n\n",
        "id": 4265
    },
    {
        "cve_id": "CVE-2019-25044",
        "code_before_change": "static void blk_exit_queue(struct request_queue *q)\n{\n\t/*\n\t * Since the I/O scheduler exit code may access cgroup information,\n\t * perform I/O scheduler exit before disassociating from the block\n\t * cgroup controller.\n\t */\n\tif (q->elevator) {\n\t\tioc_clear_queue(q);\n\t\televator_exit(q, q->elevator);\n\t\tq->elevator = NULL;\n\t}\n\n\t/*\n\t * Remove all references to @q from the block cgroup controller before\n\t * restoring @q->queue_lock to avoid that restoring this pointer causes\n\t * e.g. blkcg_print_blkgs() to crash.\n\t */\n\tblkcg_exit_queue(q);\n\n\t/*\n\t * Since the cgroup code may dereference the @q->backing_dev_info\n\t * pointer, only decrease its reference count after having removed the\n\t * association with the block cgroup controller.\n\t */\n\tbdi_put(q->backing_dev_info);\n}",
        "code_after_change": "static void blk_exit_queue(struct request_queue *q)\n{\n\t/*\n\t * Since the I/O scheduler exit code may access cgroup information,\n\t * perform I/O scheduler exit before disassociating from the block\n\t * cgroup controller.\n\t */\n\tif (q->elevator) {\n\t\tioc_clear_queue(q);\n\t\t__elevator_exit(q, q->elevator);\n\t\tq->elevator = NULL;\n\t}\n\n\t/*\n\t * Remove all references to @q from the block cgroup controller before\n\t * restoring @q->queue_lock to avoid that restoring this pointer causes\n\t * e.g. blkcg_print_blkgs() to crash.\n\t */\n\tblkcg_exit_queue(q);\n\n\t/*\n\t * Since the cgroup code may dereference the @q->backing_dev_info\n\t * pointer, only decrease its reference count after having removed the\n\t * association with the block cgroup controller.\n\t */\n\tbdi_put(q->backing_dev_info);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \t */\n \tif (q->elevator) {\n \t\tioc_clear_queue(q);\n-\t\televator_exit(q, q->elevator);\n+\t\t__elevator_exit(q, q->elevator);\n \t\tq->elevator = NULL;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t__elevator_exit(q, q->elevator);"
            ],
            "deleted": [
                "\t\televator_exit(q, q->elevator);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The block subsystem in the Linux kernel before 5.2 has a use-after-free that can lead to arbitrary code execution in the kernel context and privilege escalation, aka CID-c3e2219216c9. This is related to blk_mq_free_rqs and blk_cleanup_queue.",
        "id": 2301
    },
    {
        "cve_id": "CVE-2020-10690",
        "code_before_change": "struct ptp_clock *ptp_clock_register(struct ptp_clock_info *info,\n\t\t\t\t     struct device *parent)\n{\n\tstruct ptp_clock *ptp;\n\tint err = 0, index, major = MAJOR(ptp_devt);\n\n\tif (info->n_alarm > PTP_MAX_ALARMS)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* Initialize a clock structure. */\n\terr = -ENOMEM;\n\tptp = kzalloc(sizeof(struct ptp_clock), GFP_KERNEL);\n\tif (ptp == NULL)\n\t\tgoto no_memory;\n\n\tindex = ida_simple_get(&ptp_clocks_map, 0, MINORMASK + 1, GFP_KERNEL);\n\tif (index < 0) {\n\t\terr = index;\n\t\tgoto no_slot;\n\t}\n\n\tptp->clock.ops = ptp_clock_ops;\n\tptp->clock.release = delete_ptp_clock;\n\tptp->info = info;\n\tptp->devid = MKDEV(major, index);\n\tptp->index = index;\n\tspin_lock_init(&ptp->tsevq.lock);\n\tmutex_init(&ptp->tsevq_mux);\n\tmutex_init(&ptp->pincfg_mux);\n\tinit_waitqueue_head(&ptp->tsev_wq);\n\n\tif (ptp->info->do_aux_work) {\n\t\tkthread_init_delayed_work(&ptp->aux_work, ptp_aux_kworker);\n\t\tptp->kworker = kthread_create_worker(0, \"ptp%d\", ptp->index);\n\t\tif (IS_ERR(ptp->kworker)) {\n\t\t\terr = PTR_ERR(ptp->kworker);\n\t\t\tpr_err(\"failed to create ptp aux_worker %d\\n\", err);\n\t\t\tgoto kworker_err;\n\t\t}\n\t}\n\n\terr = ptp_populate_pin_groups(ptp);\n\tif (err)\n\t\tgoto no_pin_groups;\n\n\t/* Create a new device in our class. */\n\tptp->dev = device_create_with_groups(ptp_class, parent, ptp->devid,\n\t\t\t\t\t     ptp, ptp->pin_attr_groups,\n\t\t\t\t\t     \"ptp%d\", ptp->index);\n\tif (IS_ERR(ptp->dev)) {\n\t\terr = PTR_ERR(ptp->dev);\n\t\tgoto no_device;\n\t}\n\n\t/* Register a new PPS source. */\n\tif (info->pps) {\n\t\tstruct pps_source_info pps;\n\t\tmemset(&pps, 0, sizeof(pps));\n\t\tsnprintf(pps.name, PPS_MAX_NAME_LEN, \"ptp%d\", index);\n\t\tpps.mode = PTP_PPS_MODE;\n\t\tpps.owner = info->owner;\n\t\tptp->pps_source = pps_register_source(&pps, PTP_PPS_DEFAULTS);\n\t\tif (IS_ERR(ptp->pps_source)) {\n\t\t\terr = PTR_ERR(ptp->pps_source);\n\t\t\tpr_err(\"failed to register pps source\\n\");\n\t\t\tgoto no_pps;\n\t\t}\n\t}\n\n\t/* Create a posix clock. */\n\terr = posix_clock_register(&ptp->clock, ptp->devid);\n\tif (err) {\n\t\tpr_err(\"failed to create posix clock\\n\");\n\t\tgoto no_clock;\n\t}\n\n\treturn ptp;\n\nno_clock:\n\tif (ptp->pps_source)\n\t\tpps_unregister_source(ptp->pps_source);\nno_pps:\n\tdevice_destroy(ptp_class, ptp->devid);\nno_device:\n\tptp_cleanup_pin_groups(ptp);\nno_pin_groups:\n\tif (ptp->kworker)\n\t\tkthread_destroy_worker(ptp->kworker);\nkworker_err:\n\tmutex_destroy(&ptp->tsevq_mux);\n\tmutex_destroy(&ptp->pincfg_mux);\n\tida_simple_remove(&ptp_clocks_map, index);\nno_slot:\n\tkfree(ptp);\nno_memory:\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "struct ptp_clock *ptp_clock_register(struct ptp_clock_info *info,\n\t\t\t\t     struct device *parent)\n{\n\tstruct ptp_clock *ptp;\n\tint err = 0, index, major = MAJOR(ptp_devt);\n\n\tif (info->n_alarm > PTP_MAX_ALARMS)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* Initialize a clock structure. */\n\terr = -ENOMEM;\n\tptp = kzalloc(sizeof(struct ptp_clock), GFP_KERNEL);\n\tif (ptp == NULL)\n\t\tgoto no_memory;\n\n\tindex = ida_simple_get(&ptp_clocks_map, 0, MINORMASK + 1, GFP_KERNEL);\n\tif (index < 0) {\n\t\terr = index;\n\t\tgoto no_slot;\n\t}\n\n\tptp->clock.ops = ptp_clock_ops;\n\tptp->info = info;\n\tptp->devid = MKDEV(major, index);\n\tptp->index = index;\n\tspin_lock_init(&ptp->tsevq.lock);\n\tmutex_init(&ptp->tsevq_mux);\n\tmutex_init(&ptp->pincfg_mux);\n\tinit_waitqueue_head(&ptp->tsev_wq);\n\n\tif (ptp->info->do_aux_work) {\n\t\tkthread_init_delayed_work(&ptp->aux_work, ptp_aux_kworker);\n\t\tptp->kworker = kthread_create_worker(0, \"ptp%d\", ptp->index);\n\t\tif (IS_ERR(ptp->kworker)) {\n\t\t\terr = PTR_ERR(ptp->kworker);\n\t\t\tpr_err(\"failed to create ptp aux_worker %d\\n\", err);\n\t\t\tgoto kworker_err;\n\t\t}\n\t}\n\n\terr = ptp_populate_pin_groups(ptp);\n\tif (err)\n\t\tgoto no_pin_groups;\n\n\t/* Register a new PPS source. */\n\tif (info->pps) {\n\t\tstruct pps_source_info pps;\n\t\tmemset(&pps, 0, sizeof(pps));\n\t\tsnprintf(pps.name, PPS_MAX_NAME_LEN, \"ptp%d\", index);\n\t\tpps.mode = PTP_PPS_MODE;\n\t\tpps.owner = info->owner;\n\t\tptp->pps_source = pps_register_source(&pps, PTP_PPS_DEFAULTS);\n\t\tif (IS_ERR(ptp->pps_source)) {\n\t\t\terr = PTR_ERR(ptp->pps_source);\n\t\t\tpr_err(\"failed to register pps source\\n\");\n\t\t\tgoto no_pps;\n\t\t}\n\t}\n\n\t/* Initialize a new device of our class in our clock structure. */\n\tdevice_initialize(&ptp->dev);\n\tptp->dev.devt = ptp->devid;\n\tptp->dev.class = ptp_class;\n\tptp->dev.parent = parent;\n\tptp->dev.groups = ptp->pin_attr_groups;\n\tptp->dev.release = ptp_clock_release;\n\tdev_set_drvdata(&ptp->dev, ptp);\n\tdev_set_name(&ptp->dev, \"ptp%d\", ptp->index);\n\n\t/* Create a posix clock and link it to the device. */\n\terr = posix_clock_register(&ptp->clock, &ptp->dev);\n\tif (err) {\n\t\tpr_err(\"failed to create posix clock\\n\");\n\t\tgoto no_clock;\n\t}\n\n\treturn ptp;\n\nno_clock:\n\tif (ptp->pps_source)\n\t\tpps_unregister_source(ptp->pps_source);\nno_pps:\n\tptp_cleanup_pin_groups(ptp);\nno_pin_groups:\n\tif (ptp->kworker)\n\t\tkthread_destroy_worker(ptp->kworker);\nkworker_err:\n\tmutex_destroy(&ptp->tsevq_mux);\n\tmutex_destroy(&ptp->pincfg_mux);\n\tida_simple_remove(&ptp_clocks_map, index);\nno_slot:\n\tkfree(ptp);\nno_memory:\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,7 +20,6 @@\n \t}\n \n \tptp->clock.ops = ptp_clock_ops;\n-\tptp->clock.release = delete_ptp_clock;\n \tptp->info = info;\n \tptp->devid = MKDEV(major, index);\n \tptp->index = index;\n@@ -43,15 +42,6 @@\n \tif (err)\n \t\tgoto no_pin_groups;\n \n-\t/* Create a new device in our class. */\n-\tptp->dev = device_create_with_groups(ptp_class, parent, ptp->devid,\n-\t\t\t\t\t     ptp, ptp->pin_attr_groups,\n-\t\t\t\t\t     \"ptp%d\", ptp->index);\n-\tif (IS_ERR(ptp->dev)) {\n-\t\terr = PTR_ERR(ptp->dev);\n-\t\tgoto no_device;\n-\t}\n-\n \t/* Register a new PPS source. */\n \tif (info->pps) {\n \t\tstruct pps_source_info pps;\n@@ -67,8 +57,18 @@\n \t\t}\n \t}\n \n-\t/* Create a posix clock. */\n-\terr = posix_clock_register(&ptp->clock, ptp->devid);\n+\t/* Initialize a new device of our class in our clock structure. */\n+\tdevice_initialize(&ptp->dev);\n+\tptp->dev.devt = ptp->devid;\n+\tptp->dev.class = ptp_class;\n+\tptp->dev.parent = parent;\n+\tptp->dev.groups = ptp->pin_attr_groups;\n+\tptp->dev.release = ptp_clock_release;\n+\tdev_set_drvdata(&ptp->dev, ptp);\n+\tdev_set_name(&ptp->dev, \"ptp%d\", ptp->index);\n+\n+\t/* Create a posix clock and link it to the device. */\n+\terr = posix_clock_register(&ptp->clock, &ptp->dev);\n \tif (err) {\n \t\tpr_err(\"failed to create posix clock\\n\");\n \t\tgoto no_clock;\n@@ -80,8 +80,6 @@\n \tif (ptp->pps_source)\n \t\tpps_unregister_source(ptp->pps_source);\n no_pps:\n-\tdevice_destroy(ptp_class, ptp->devid);\n-no_device:\n \tptp_cleanup_pin_groups(ptp);\n no_pin_groups:\n \tif (ptp->kworker)",
        "function_modified_lines": {
            "added": [
                "\t/* Initialize a new device of our class in our clock structure. */",
                "\tdevice_initialize(&ptp->dev);",
                "\tptp->dev.devt = ptp->devid;",
                "\tptp->dev.class = ptp_class;",
                "\tptp->dev.parent = parent;",
                "\tptp->dev.groups = ptp->pin_attr_groups;",
                "\tptp->dev.release = ptp_clock_release;",
                "\tdev_set_drvdata(&ptp->dev, ptp);",
                "\tdev_set_name(&ptp->dev, \"ptp%d\", ptp->index);",
                "",
                "\t/* Create a posix clock and link it to the device. */",
                "\terr = posix_clock_register(&ptp->clock, &ptp->dev);"
            ],
            "deleted": [
                "\tptp->clock.release = delete_ptp_clock;",
                "\t/* Create a new device in our class. */",
                "\tptp->dev = device_create_with_groups(ptp_class, parent, ptp->devid,",
                "\t\t\t\t\t     ptp, ptp->pin_attr_groups,",
                "\t\t\t\t\t     \"ptp%d\", ptp->index);",
                "\tif (IS_ERR(ptp->dev)) {",
                "\t\terr = PTR_ERR(ptp->dev);",
                "\t\tgoto no_device;",
                "\t}",
                "",
                "\t/* Create a posix clock. */",
                "\terr = posix_clock_register(&ptp->clock, ptp->devid);",
                "\tdevice_destroy(ptp_class, ptp->devid);",
                "no_device:"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a use-after-free in kernel versions before 5.5 due to a race condition between the release of ptp_clock and cdev while resource deallocation. When a (high privileged) process allocates a ptp device file (like /dev/ptpX) and voluntarily goes to sleep. During this time if the underlying device is removed, it can cause an exploitable condition as the process wakes up to terminate and clean all attached files. The system crashes due to the cdev structure being invalid (as already freed) which is pointed to by the inode.",
        "id": 2398
    },
    {
        "cve_id": "CVE-2021-4154",
        "code_before_change": "int cgroup1_parse_param(struct fs_context *fc, struct fs_parameter *param)\n{\n\tstruct cgroup_fs_context *ctx = cgroup_fc2context(fc);\n\tstruct cgroup_subsys *ss;\n\tstruct fs_parse_result result;\n\tint opt, i;\n\n\topt = fs_parse(fc, cgroup1_fs_parameters, param, &result);\n\tif (opt == -ENOPARAM) {\n\t\tif (strcmp(param->key, \"source\") == 0) {\n\t\t\tif (fc->source)\n\t\t\t\treturn invalf(fc, \"Multiple sources not supported\");\n\t\t\tfc->source = param->string;\n\t\t\tparam->string = NULL;\n\t\t\treturn 0;\n\t\t}\n\t\tfor_each_subsys(ss, i) {\n\t\t\tif (strcmp(param->key, ss->legacy_name))\n\t\t\t\tcontinue;\n\t\t\tif (!cgroup_ssid_enabled(i) || cgroup1_ssid_disabled(i))\n\t\t\t\treturn invalfc(fc, \"Disabled controller '%s'\",\n\t\t\t\t\t       param->key);\n\t\t\tctx->subsys_mask |= (1 << i);\n\t\t\treturn 0;\n\t\t}\n\t\treturn invalfc(fc, \"Unknown subsys name '%s'\", param->key);\n\t}\n\tif (opt < 0)\n\t\treturn opt;\n\n\tswitch (opt) {\n\tcase Opt_none:\n\t\t/* Explicitly have no subsystems */\n\t\tctx->none = true;\n\t\tbreak;\n\tcase Opt_all:\n\t\tctx->all_ss = true;\n\t\tbreak;\n\tcase Opt_noprefix:\n\t\tctx->flags |= CGRP_ROOT_NOPREFIX;\n\t\tbreak;\n\tcase Opt_clone_children:\n\t\tctx->cpuset_clone_children = true;\n\t\tbreak;\n\tcase Opt_cpuset_v2_mode:\n\t\tctx->flags |= CGRP_ROOT_CPUSET_V2_MODE;\n\t\tbreak;\n\tcase Opt_xattr:\n\t\tctx->flags |= CGRP_ROOT_XATTR;\n\t\tbreak;\n\tcase Opt_release_agent:\n\t\t/* Specifying two release agents is forbidden */\n\t\tif (ctx->release_agent)\n\t\t\treturn invalfc(fc, \"release_agent respecified\");\n\t\tctx->release_agent = param->string;\n\t\tparam->string = NULL;\n\t\tbreak;\n\tcase Opt_name:\n\t\t/* blocked by boot param? */\n\t\tif (cgroup_no_v1_named)\n\t\t\treturn -ENOENT;\n\t\t/* Can't specify an empty name */\n\t\tif (!param->size)\n\t\t\treturn invalfc(fc, \"Empty name\");\n\t\tif (param->size > MAX_CGROUP_ROOT_NAMELEN - 1)\n\t\t\treturn invalfc(fc, \"Name too long\");\n\t\t/* Must match [\\w.-]+ */\n\t\tfor (i = 0; i < param->size; i++) {\n\t\t\tchar c = param->string[i];\n\t\t\tif (isalnum(c))\n\t\t\t\tcontinue;\n\t\t\tif ((c == '.') || (c == '-') || (c == '_'))\n\t\t\t\tcontinue;\n\t\t\treturn invalfc(fc, \"Invalid name\");\n\t\t}\n\t\t/* Specifying two names is forbidden */\n\t\tif (ctx->name)\n\t\t\treturn invalfc(fc, \"name respecified\");\n\t\tctx->name = param->string;\n\t\tparam->string = NULL;\n\t\tbreak;\n\t}\n\treturn 0;\n}",
        "code_after_change": "int cgroup1_parse_param(struct fs_context *fc, struct fs_parameter *param)\n{\n\tstruct cgroup_fs_context *ctx = cgroup_fc2context(fc);\n\tstruct cgroup_subsys *ss;\n\tstruct fs_parse_result result;\n\tint opt, i;\n\n\topt = fs_parse(fc, cgroup1_fs_parameters, param, &result);\n\tif (opt == -ENOPARAM) {\n\t\tif (strcmp(param->key, \"source\") == 0) {\n\t\t\tif (param->type != fs_value_is_string)\n\t\t\t\treturn invalf(fc, \"Non-string source\");\n\t\t\tif (fc->source)\n\t\t\t\treturn invalf(fc, \"Multiple sources not supported\");\n\t\t\tfc->source = param->string;\n\t\t\tparam->string = NULL;\n\t\t\treturn 0;\n\t\t}\n\t\tfor_each_subsys(ss, i) {\n\t\t\tif (strcmp(param->key, ss->legacy_name))\n\t\t\t\tcontinue;\n\t\t\tif (!cgroup_ssid_enabled(i) || cgroup1_ssid_disabled(i))\n\t\t\t\treturn invalfc(fc, \"Disabled controller '%s'\",\n\t\t\t\t\t       param->key);\n\t\t\tctx->subsys_mask |= (1 << i);\n\t\t\treturn 0;\n\t\t}\n\t\treturn invalfc(fc, \"Unknown subsys name '%s'\", param->key);\n\t}\n\tif (opt < 0)\n\t\treturn opt;\n\n\tswitch (opt) {\n\tcase Opt_none:\n\t\t/* Explicitly have no subsystems */\n\t\tctx->none = true;\n\t\tbreak;\n\tcase Opt_all:\n\t\tctx->all_ss = true;\n\t\tbreak;\n\tcase Opt_noprefix:\n\t\tctx->flags |= CGRP_ROOT_NOPREFIX;\n\t\tbreak;\n\tcase Opt_clone_children:\n\t\tctx->cpuset_clone_children = true;\n\t\tbreak;\n\tcase Opt_cpuset_v2_mode:\n\t\tctx->flags |= CGRP_ROOT_CPUSET_V2_MODE;\n\t\tbreak;\n\tcase Opt_xattr:\n\t\tctx->flags |= CGRP_ROOT_XATTR;\n\t\tbreak;\n\tcase Opt_release_agent:\n\t\t/* Specifying two release agents is forbidden */\n\t\tif (ctx->release_agent)\n\t\t\treturn invalfc(fc, \"release_agent respecified\");\n\t\tctx->release_agent = param->string;\n\t\tparam->string = NULL;\n\t\tbreak;\n\tcase Opt_name:\n\t\t/* blocked by boot param? */\n\t\tif (cgroup_no_v1_named)\n\t\t\treturn -ENOENT;\n\t\t/* Can't specify an empty name */\n\t\tif (!param->size)\n\t\t\treturn invalfc(fc, \"Empty name\");\n\t\tif (param->size > MAX_CGROUP_ROOT_NAMELEN - 1)\n\t\t\treturn invalfc(fc, \"Name too long\");\n\t\t/* Must match [\\w.-]+ */\n\t\tfor (i = 0; i < param->size; i++) {\n\t\t\tchar c = param->string[i];\n\t\t\tif (isalnum(c))\n\t\t\t\tcontinue;\n\t\t\tif ((c == '.') || (c == '-') || (c == '_'))\n\t\t\t\tcontinue;\n\t\t\treturn invalfc(fc, \"Invalid name\");\n\t\t}\n\t\t/* Specifying two names is forbidden */\n\t\tif (ctx->name)\n\t\t\treturn invalfc(fc, \"name respecified\");\n\t\tctx->name = param->string;\n\t\tparam->string = NULL;\n\t\tbreak;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,8 @@\n \topt = fs_parse(fc, cgroup1_fs_parameters, param, &result);\n \tif (opt == -ENOPARAM) {\n \t\tif (strcmp(param->key, \"source\") == 0) {\n+\t\t\tif (param->type != fs_value_is_string)\n+\t\t\t\treturn invalf(fc, \"Non-string source\");\n \t\t\tif (fc->source)\n \t\t\t\treturn invalf(fc, \"Multiple sources not supported\");\n \t\t\tfc->source = param->string;",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (param->type != fs_value_is_string)",
                "\t\t\t\treturn invalf(fc, \"Non-string source\");"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in cgroup1_parse_param in kernel/cgroup/cgroup-v1.c in the Linux kernel's cgroup v1 parser. A local attacker with a user privilege could cause a privilege escalation by exploiting the fsconfig syscall parameter leading to a container breakout and a denial of service on the system.",
        "id": 3137
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "struct nfc_llcp_local *nfc_llcp_local_get(struct nfc_llcp_local *local)\n{\n\tkref_get(&local->ref);\n\n\treturn local;\n}",
        "code_after_change": "static struct nfc_llcp_local *nfc_llcp_local_get(struct nfc_llcp_local *local)\n{\n\tkref_get(&local->ref);\n\n\treturn local;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-struct nfc_llcp_local *nfc_llcp_local_get(struct nfc_llcp_local *local)\n+static struct nfc_llcp_local *nfc_llcp_local_get(struct nfc_llcp_local *local)\n {\n \tkref_get(&local->ref);\n ",
        "function_modified_lines": {
            "added": [
                "static struct nfc_llcp_local *nfc_llcp_local_get(struct nfc_llcp_local *local)"
            ],
            "deleted": [
                "struct nfc_llcp_local *nfc_llcp_local_get(struct nfc_llcp_local *local)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4147
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt != NULL)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "code_after_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tstruct ipv6_pinfo *np = inet6_sk(sk);\n \tstruct dccp_sock *dp = dccp_sk(sk);\n \tstruct in6_addr *saddr = NULL, *final_p, final;\n+\tstruct ipv6_txoptions *opt;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n \tint addr_type;\n@@ -106,7 +107,8 @@\n \tfl6.fl6_sport = inet->inet_sport;\n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n-\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n \n \tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \tif (IS_ERR(dst)) {\n@@ -126,9 +128,8 @@\n \t__ip6_dst_store(sk, dst, NULL, NULL);\n \n \ticsk->icsk_ext_hdr_len = 0;\n-\tif (np->opt != NULL)\n-\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n-\t\t\t\t\t  np->opt->opt_nflen);\n+\tif (opt)\n+\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n \n \tinet->inet_dport = usin->sin6_port;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt;",
                "\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\tfinal_p = fl6_update_dst(&fl6, opt, &final);",
                "\tif (opt)",
                "\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);",
                "\tif (np->opt != NULL)",
                "\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +",
                "\t\t\t\t\t  np->opt->opt_nflen);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 990
    },
    {
        "cve_id": "CVE-2019-2025",
        "code_before_change": "static struct binder_buffer *binder_alloc_prepare_to_free_locked(\n\t\tstruct binder_alloc *alloc,\n\t\tuintptr_t user_ptr)\n{\n\tstruct rb_node *n = alloc->allocated_buffers.rb_node;\n\tstruct binder_buffer *buffer;\n\tvoid *kern_ptr;\n\n\tkern_ptr = (void *)(user_ptr - alloc->user_buffer_offset);\n\n\twhile (n) {\n\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\tBUG_ON(buffer->free);\n\n\t\tif (kern_ptr < buffer->data)\n\t\t\tn = n->rb_left;\n\t\telse if (kern_ptr > buffer->data)\n\t\t\tn = n->rb_right;\n\t\telse {\n\t\t\t/*\n\t\t\t * Guard against user threads attempting to\n\t\t\t * free the buffer twice\n\t\t\t */\n\t\t\tif (buffer->free_in_progress) {\n\t\t\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t\t\t   \"%d:%d FREE_BUFFER u%016llx user freed buffer twice\\n\",\n\t\t\t\t\t\t   alloc->pid, current->pid,\n\t\t\t\t\t\t   (u64)user_ptr);\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t\tbuffer->free_in_progress = 1;\n\t\t\treturn buffer;\n\t\t}\n\t}\n\treturn NULL;\n}",
        "code_after_change": "static struct binder_buffer *binder_alloc_prepare_to_free_locked(\n\t\tstruct binder_alloc *alloc,\n\t\tuintptr_t user_ptr)\n{\n\tstruct rb_node *n = alloc->allocated_buffers.rb_node;\n\tstruct binder_buffer *buffer;\n\tvoid *kern_ptr;\n\n\tkern_ptr = (void *)(user_ptr - alloc->user_buffer_offset);\n\n\twhile (n) {\n\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\tBUG_ON(buffer->free);\n\n\t\tif (kern_ptr < buffer->data)\n\t\t\tn = n->rb_left;\n\t\telse if (kern_ptr > buffer->data)\n\t\t\tn = n->rb_right;\n\t\telse {\n\t\t\t/*\n\t\t\t * Guard against user threads attempting to\n\t\t\t * free the buffer when in use by kernel or\n\t\t\t * after it's already been freed.\n\t\t\t */\n\t\t\tif (!buffer->allow_user_free)\n\t\t\t\treturn ERR_PTR(-EPERM);\n\t\t\tbuffer->allow_user_free = 0;\n\t\t\treturn buffer;\n\t\t}\n\t}\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,16 +19,12 @@\n \t\telse {\n \t\t\t/*\n \t\t\t * Guard against user threads attempting to\n-\t\t\t * free the buffer twice\n+\t\t\t * free the buffer when in use by kernel or\n+\t\t\t * after it's already been freed.\n \t\t\t */\n-\t\t\tif (buffer->free_in_progress) {\n-\t\t\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n-\t\t\t\t\t\t   \"%d:%d FREE_BUFFER u%016llx user freed buffer twice\\n\",\n-\t\t\t\t\t\t   alloc->pid, current->pid,\n-\t\t\t\t\t\t   (u64)user_ptr);\n-\t\t\t\treturn NULL;\n-\t\t\t}\n-\t\t\tbuffer->free_in_progress = 1;\n+\t\t\tif (!buffer->allow_user_free)\n+\t\t\t\treturn ERR_PTR(-EPERM);\n+\t\t\tbuffer->allow_user_free = 0;\n \t\t\treturn buffer;\n \t\t}\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\t * free the buffer when in use by kernel or",
                "\t\t\t * after it's already been freed.",
                "\t\t\tif (!buffer->allow_user_free)",
                "\t\t\t\treturn ERR_PTR(-EPERM);",
                "\t\t\tbuffer->allow_user_free = 0;"
            ],
            "deleted": [
                "\t\t\t * free the buffer twice",
                "\t\t\tif (buffer->free_in_progress) {",
                "\t\t\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,",
                "\t\t\t\t\t\t   \"%d:%d FREE_BUFFER u%016llx user freed buffer twice\\n\",",
                "\t\t\t\t\t\t   alloc->pid, current->pid,",
                "\t\t\t\t\t\t   (u64)user_ptr);",
                "\t\t\t\treturn NULL;",
                "\t\t\t}",
                "\t\t\tbuffer->free_in_progress = 1;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_thread_read of binder.c, there is a possible use-after-free due to improper locking. This could lead to local escalation of privilege in the kernel with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-116855682References: Upstream kernel",
        "id": 2278
    },
    {
        "cve_id": "CVE-2022-41218",
        "code_before_change": "void dvb_dmxdev_release(struct dmxdev *dmxdev)\n{\n\tdmxdev->exit = 1;\n\tif (dmxdev->dvbdev->users > 1) {\n\t\twait_event(dmxdev->dvbdev->wait_queue,\n\t\t\t\tdmxdev->dvbdev->users == 1);\n\t}\n\tif (dmxdev->dvr_dvbdev->users > 1) {\n\t\twait_event(dmxdev->dvr_dvbdev->wait_queue,\n\t\t\t\tdmxdev->dvr_dvbdev->users == 1);\n\t}\n\n\tdvb_unregister_device(dmxdev->dvbdev);\n\tdvb_unregister_device(dmxdev->dvr_dvbdev);\n\n\tvfree(dmxdev->filter);\n\tdmxdev->filter = NULL;\n\tdmxdev->demux->close(dmxdev->demux);\n}",
        "code_after_change": "void dvb_dmxdev_release(struct dmxdev *dmxdev)\n{\n\tmutex_lock(&dmxdev->mutex);\n\tdmxdev->exit = 1;\n\tmutex_unlock(&dmxdev->mutex);\n\n\tif (dmxdev->dvbdev->users > 1) {\n\t\twait_event(dmxdev->dvbdev->wait_queue,\n\t\t\t\tdmxdev->dvbdev->users == 1);\n\t}\n\tif (dmxdev->dvr_dvbdev->users > 1) {\n\t\twait_event(dmxdev->dvr_dvbdev->wait_queue,\n\t\t\t\tdmxdev->dvr_dvbdev->users == 1);\n\t}\n\n\tdvb_unregister_device(dmxdev->dvbdev);\n\tdvb_unregister_device(dmxdev->dvr_dvbdev);\n\n\tvfree(dmxdev->filter);\n\tdmxdev->filter = NULL;\n\tdmxdev->demux->close(dmxdev->demux);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,9 @@\n void dvb_dmxdev_release(struct dmxdev *dmxdev)\n {\n+\tmutex_lock(&dmxdev->mutex);\n \tdmxdev->exit = 1;\n+\tmutex_unlock(&dmxdev->mutex);\n+\n \tif (dmxdev->dvbdev->users > 1) {\n \t\twait_event(dmxdev->dvbdev->wait_queue,\n \t\t\t\tdmxdev->dvbdev->users == 1);",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&dmxdev->mutex);",
                "\tmutex_unlock(&dmxdev->mutex);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In drivers/media/dvb-core/dmxdev.c in the Linux kernel through 5.19.10, there is a use-after-free caused by refcount races, affecting dvb_demux_open and dvb_dmxdev_release.",
        "id": 3711
    },
    {
        "cve_id": "CVE-2016-9120",
        "code_before_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
        "code_after_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,11 +43,15 @@\n \t{\n \t\tstruct ion_handle *handle;\n \n-\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n-\t\tif (IS_ERR(handle))\n+\t\tmutex_lock(&client->lock);\n+\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n+\t\tif (IS_ERR(handle)) {\n+\t\t\tmutex_unlock(&client->lock);\n \t\t\treturn PTR_ERR(handle);\n-\t\tion_free(client, handle);\n-\t\tion_handle_put(handle);\n+\t\t}\n+\t\tion_free_nolock(client, handle);\n+\t\tion_handle_put_nolock(handle);\n+\t\tmutex_unlock(&client->lock);\n \t\tbreak;\n \t}\n \tcase ION_IOC_SHARE:",
        "function_modified_lines": {
            "added": [
                "\t\tmutex_lock(&client->lock);",
                "\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);",
                "\t\tif (IS_ERR(handle)) {",
                "\t\t\tmutex_unlock(&client->lock);",
                "\t\t}",
                "\t\tion_free_nolock(client, handle);",
                "\t\tion_handle_put_nolock(handle);",
                "\t\tmutex_unlock(&client->lock);"
            ],
            "deleted": [
                "\t\thandle = ion_handle_get_by_id(client, data.handle.handle);",
                "\t\tif (IS_ERR(handle))",
                "\t\tion_free(client, handle);",
                "\t\tion_handle_put(handle);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time.",
        "id": 1140
    },
    {
        "cve_id": "CVE-2023-1118",
        "code_before_change": "static void ene_remove(struct pnp_dev *pnp_dev)\n{\n\tstruct ene_device *dev = pnp_get_drvdata(pnp_dev);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->hw_lock, flags);\n\tene_rx_disable(dev);\n\tene_rx_restore_hw_buffer(dev);\n\tspin_unlock_irqrestore(&dev->hw_lock, flags);\n\n\tfree_irq(dev->irq, dev);\n\trelease_region(dev->hw_io, ENE_IO_SIZE);\n\trc_unregister_device(dev->rdev);\n\tkfree(dev);\n}",
        "code_after_change": "static void ene_remove(struct pnp_dev *pnp_dev)\n{\n\tstruct ene_device *dev = pnp_get_drvdata(pnp_dev);\n\tunsigned long flags;\n\n\trc_unregister_device(dev->rdev);\n\tdel_timer_sync(&dev->tx_sim_timer);\n\tspin_lock_irqsave(&dev->hw_lock, flags);\n\tene_rx_disable(dev);\n\tene_rx_restore_hw_buffer(dev);\n\tspin_unlock_irqrestore(&dev->hw_lock, flags);\n\n\tfree_irq(dev->irq, dev);\n\trelease_region(dev->hw_io, ENE_IO_SIZE);\n\tkfree(dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,8 @@\n \tstruct ene_device *dev = pnp_get_drvdata(pnp_dev);\n \tunsigned long flags;\n \n+\trc_unregister_device(dev->rdev);\n+\tdel_timer_sync(&dev->tx_sim_timer);\n \tspin_lock_irqsave(&dev->hw_lock, flags);\n \tene_rx_disable(dev);\n \tene_rx_restore_hw_buffer(dev);\n@@ -10,6 +12,5 @@\n \n \tfree_irq(dev->irq, dev);\n \trelease_region(dev->hw_io, ENE_IO_SIZE);\n-\trc_unregister_device(dev->rdev);\n \tkfree(dev);\n }",
        "function_modified_lines": {
            "added": [
                "\trc_unregister_device(dev->rdev);",
                "\tdel_timer_sync(&dev->tx_sim_timer);"
            ],
            "deleted": [
                "\trc_unregister_device(dev->rdev);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw use after free in the Linux kernel integrated infrared receiver/transceiver driver was found in the way user detaching rc device. A local user could use this flaw to crash the system or potentially escalate their privileges on the system.",
        "id": 3849
    },
    {
        "cve_id": "CVE-2023-35823",
        "code_before_change": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\treturn 0;\n}",
        "code_after_change": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\tdel_timer_sync(&dev->vbi_q.timeout);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n int saa7134_vbi_fini(struct saa7134_dev *dev)\n {\n \t/* nothing */\n+\tdel_timer_sync(&dev->vbi_q.timeout);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tdel_timer_sync(&dev->vbi_q.timeout);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in saa7134_finidev in drivers/media/pci/saa7134/saa7134-core.c.",
        "id": 4110
    },
    {
        "cve_id": "CVE-2022-2318",
        "code_before_change": "static void rose_idletimer_expiry(struct timer_list *t)\n{\n\tstruct rose_sock *rose = from_timer(rose, t, idletimer);\n\tstruct sock *sk = &rose->sock;\n\n\tbh_lock_sock(sk);\n\trose_clear_queues(sk);\n\n\trose_write_internal(sk, ROSE_CLEAR_REQUEST);\n\trose_sk(sk)->state = ROSE_STATE_2;\n\n\trose_start_t3timer(sk);\n\n\tsk->sk_state     = TCP_CLOSE;\n\tsk->sk_err       = 0;\n\tsk->sk_shutdown |= SEND_SHUTDOWN;\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tsk->sk_state_change(sk);\n\t\tsock_set_flag(sk, SOCK_DEAD);\n\t}\n\tbh_unlock_sock(sk);\n}",
        "code_after_change": "static void rose_idletimer_expiry(struct timer_list *t)\n{\n\tstruct rose_sock *rose = from_timer(rose, t, idletimer);\n\tstruct sock *sk = &rose->sock;\n\n\tbh_lock_sock(sk);\n\trose_clear_queues(sk);\n\n\trose_write_internal(sk, ROSE_CLEAR_REQUEST);\n\trose_sk(sk)->state = ROSE_STATE_2;\n\n\trose_start_t3timer(sk);\n\n\tsk->sk_state     = TCP_CLOSE;\n\tsk->sk_err       = 0;\n\tsk->sk_shutdown |= SEND_SHUTDOWN;\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tsk->sk_state_change(sk);\n\t\tsock_set_flag(sk, SOCK_DEAD);\n\t}\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,4 +20,5 @@\n \t\tsock_set_flag(sk, SOCK_DEAD);\n \t}\n \tbh_unlock_sock(sk);\n+\tsock_put(sk);\n }",
        "function_modified_lines": {
            "added": [
                "\tsock_put(sk);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities caused by timer handler in net/rose/rose_timer.c of linux that allow attackers to crash linux kernel without any privileges.",
        "id": 3436
    },
    {
        "cve_id": "CVE-2021-23134",
        "code_before_change": "static int llcp_sock_connect(struct socket *sock, struct sockaddr *_addr,\n\t\t\t     int len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct sockaddr_nfc_llcp *addr = (struct sockaddr_nfc_llcp *)_addr;\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tint ret = 0;\n\n\tpr_debug(\"sock %p sk %p flags 0x%x\\n\", sock, sk, flags);\n\n\tif (!addr || len < sizeof(*addr) || addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\n\tif (addr->service_name_len == 0 && addr->dsap == 0)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"addr dev_idx=%u target_idx=%u protocol=%u\\n\", addr->dev_idx,\n\t\t addr->target_idx, addr->nfc_protocol);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == LLCP_CONNECTED) {\n\t\tret = -EISCONN;\n\t\tgoto error;\n\t}\n\tif (sk->sk_state == LLCP_CONNECTING) {\n\t\tret = -EINPROGRESS;\n\t\tgoto error;\n\t}\n\n\tdev = nfc_get_device(addr->dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\n\tdevice_lock(&dev->dev);\n\tif (dev->dep_link_up == false) {\n\t\tret = -ENOLINK;\n\t\tdevice_unlock(&dev->dev);\n\t\tgoto put_dev;\n\t}\n\tdevice_unlock(&dev->dev);\n\n\tif (local->rf_mode == NFC_RF_INITIATOR &&\n\t    addr->target_idx != local->target_idx) {\n\t\tret = -ENOLINK;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = nfc_llcp_local_get(local);\n\tllcp_sock->ssap = nfc_llcp_get_local_ssap(local);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tnfc_llcp_local_put(llcp_sock->local);\n\t\tret = -ENOMEM;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;\n\n\tif (addr->service_name_len == 0)\n\t\tllcp_sock->dsap = addr->dsap;\n\telse\n\t\tllcp_sock->dsap = LLCP_SAP_SDP;\n\tllcp_sock->nfc_protocol = addr->nfc_protocol;\n\tllcp_sock->service_name_len = min_t(unsigned int,\n\t\t\t\t\t    addr->service_name_len,\n\t\t\t\t\t    NFC_LLCP_MAX_SERVICE_NAME);\n\tllcp_sock->service_name = kmemdup(addr->service_name,\n\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_release;\n\t}\n\n\tnfc_llcp_sock_link(&local->connecting_sockets, sk);\n\n\tret = nfc_llcp_send_connect(llcp_sock);\n\tif (ret)\n\t\tgoto sock_unlink;\n\n\tsk->sk_state = LLCP_CONNECTING;\n\n\tret = sock_wait_state(sk, LLCP_CONNECTED,\n\t\t\t      sock_sndtimeo(sk, flags & O_NONBLOCK));\n\tif (ret && ret != -EINPROGRESS)\n\t\tgoto sock_unlink;\n\n\trelease_sock(sk);\n\n\treturn ret;\n\nsock_unlink:\n\tnfc_llcp_sock_unlink(&local->connecting_sockets, sk);\n\tkfree(llcp_sock->service_name);\n\tllcp_sock->service_name = NULL;\n\nsock_llcp_release:\n\tnfc_llcp_put_ssap(local, llcp_sock->ssap);\n\tnfc_llcp_local_put(llcp_sock->local);\n\nput_dev:\n\tnfc_put_device(dev);\n\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "code_after_change": "static int llcp_sock_connect(struct socket *sock, struct sockaddr *_addr,\n\t\t\t     int len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct sockaddr_nfc_llcp *addr = (struct sockaddr_nfc_llcp *)_addr;\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tint ret = 0;\n\n\tpr_debug(\"sock %p sk %p flags 0x%x\\n\", sock, sk, flags);\n\n\tif (!addr || len < sizeof(*addr) || addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\n\tif (addr->service_name_len == 0 && addr->dsap == 0)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"addr dev_idx=%u target_idx=%u protocol=%u\\n\", addr->dev_idx,\n\t\t addr->target_idx, addr->nfc_protocol);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == LLCP_CONNECTED) {\n\t\tret = -EISCONN;\n\t\tgoto error;\n\t}\n\tif (sk->sk_state == LLCP_CONNECTING) {\n\t\tret = -EINPROGRESS;\n\t\tgoto error;\n\t}\n\n\tdev = nfc_get_device(addr->dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\n\tdevice_lock(&dev->dev);\n\tif (dev->dep_link_up == false) {\n\t\tret = -ENOLINK;\n\t\tdevice_unlock(&dev->dev);\n\t\tgoto put_dev;\n\t}\n\tdevice_unlock(&dev->dev);\n\n\tif (local->rf_mode == NFC_RF_INITIATOR &&\n\t    addr->target_idx != local->target_idx) {\n\t\tret = -ENOLINK;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = nfc_llcp_local_get(local);\n\tllcp_sock->ssap = nfc_llcp_get_local_ssap(local);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tnfc_llcp_local_put(llcp_sock->local);\n\t\tllcp_sock->local = NULL;\n\t\tret = -ENOMEM;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;\n\n\tif (addr->service_name_len == 0)\n\t\tllcp_sock->dsap = addr->dsap;\n\telse\n\t\tllcp_sock->dsap = LLCP_SAP_SDP;\n\tllcp_sock->nfc_protocol = addr->nfc_protocol;\n\tllcp_sock->service_name_len = min_t(unsigned int,\n\t\t\t\t\t    addr->service_name_len,\n\t\t\t\t\t    NFC_LLCP_MAX_SERVICE_NAME);\n\tllcp_sock->service_name = kmemdup(addr->service_name,\n\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_release;\n\t}\n\n\tnfc_llcp_sock_link(&local->connecting_sockets, sk);\n\n\tret = nfc_llcp_send_connect(llcp_sock);\n\tif (ret)\n\t\tgoto sock_unlink;\n\n\tsk->sk_state = LLCP_CONNECTING;\n\n\tret = sock_wait_state(sk, LLCP_CONNECTED,\n\t\t\t      sock_sndtimeo(sk, flags & O_NONBLOCK));\n\tif (ret && ret != -EINPROGRESS)\n\t\tgoto sock_unlink;\n\n\trelease_sock(sk);\n\n\treturn ret;\n\nsock_unlink:\n\tnfc_llcp_sock_unlink(&local->connecting_sockets, sk);\n\tkfree(llcp_sock->service_name);\n\tllcp_sock->service_name = NULL;\n\nsock_llcp_release:\n\tnfc_llcp_put_ssap(local, llcp_sock->ssap);\n\tnfc_llcp_local_put(llcp_sock->local);\n\tllcp_sock->local = NULL;\n\nput_dev:\n\tnfc_put_device(dev);\n\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -61,6 +61,7 @@\n \tllcp_sock->ssap = nfc_llcp_get_local_ssap(local);\n \tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n \t\tnfc_llcp_local_put(llcp_sock->local);\n+\t\tllcp_sock->local = NULL;\n \t\tret = -ENOMEM;\n \t\tgoto put_dev;\n \t}\n@@ -108,6 +109,7 @@\n sock_llcp_release:\n \tnfc_llcp_put_ssap(local, llcp_sock->ssap);\n \tnfc_llcp_local_put(llcp_sock->local);\n+\tllcp_sock->local = NULL;\n \n put_dev:\n \tnfc_put_device(dev);",
        "function_modified_lines": {
            "added": [
                "\t\tllcp_sock->local = NULL;",
                "\tllcp_sock->local = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use After Free vulnerability in nfc sockets in the Linux Kernel before 5.12.4 allows local attackers to elevate their privileges. In typical configurations, the issue can only be triggered by a privileged local user with the CAP_NET_RAW capability.",
        "id": 2890
    },
    {
        "cve_id": "CVE-2020-36387",
        "code_before_change": "static bool io_rw_reissue(struct io_kiocb *req, long res)\n{\n#ifdef CONFIG_BLOCK\n\tint ret;\n\n\tif ((res != -EAGAIN && res != -EOPNOTSUPP) || io_wq_current_is_worker())\n\t\treturn false;\n\n\tinit_task_work(&req->task_work, io_rw_resubmit);\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (!ret)\n\t\treturn true;\n#endif\n\treturn false;\n}",
        "code_after_change": "static bool io_rw_reissue(struct io_kiocb *req, long res)\n{\n#ifdef CONFIG_BLOCK\n\tint ret;\n\n\tif ((res != -EAGAIN && res != -EOPNOTSUPP) || io_wq_current_is_worker())\n\t\treturn false;\n\n\tinit_task_work(&req->task_work, io_rw_resubmit);\n\tpercpu_ref_get(&req->ctx->refs);\n\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (!ret)\n\t\treturn true;\n#endif\n\treturn false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,8 @@\n \t\treturn false;\n \n \tinit_task_work(&req->task_work, io_rw_resubmit);\n+\tpercpu_ref_get(&req->ctx->refs);\n+\n \tret = io_req_task_work_add(req, &req->task_work);\n \tif (!ret)\n \t\treturn true;",
        "function_modified_lines": {
            "added": [
                "\tpercpu_ref_get(&req->ctx->refs);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.8.2. fs/io_uring.c has a use-after-free related to io_async_task_func and ctx reference holding, aka CID-6d816e088c35.",
        "id": 2760
    },
    {
        "cve_id": "CVE-2022-3176",
        "code_before_change": "static int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = container_of(wait, struct io_poll_iocb,\n\t\t\t\t\t\t wait);\n\t__poll_t mask = key_to_poll(key);\n\n\t/* for instances that support it check for an event match first */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\tif (io_poll_get_ownership(req)) {\n\t\t/* optional, saves extra locking for removal in tw handler */\n\t\tif (mask && poll->events & EPOLLONESHOT) {\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\t\tpoll->head = NULL;\n\t\t}\n\t\t__io_poll_execute(req, mask);\n\t}\n\treturn 1;\n}",
        "code_after_change": "static int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = container_of(wait, struct io_poll_iocb,\n\t\t\t\t\t\t wait);\n\t__poll_t mask = key_to_poll(key);\n\n\tif (unlikely(mask & POLLFREE)) {\n\t\tio_poll_mark_cancelled(req);\n\t\t/* we have to kick tw in case it's not already */\n\t\tio_poll_execute(req, 0);\n\n\t\t/*\n\t\t * If the waitqueue is being freed early but someone is already\n\t\t * holds ownership over it, we have to tear down the request as\n\t\t * best we can. That means immediately removing the request from\n\t\t * its waitqueue and preventing all further accesses to the\n\t\t * waitqueue via the request.\n\t\t */\n\t\tlist_del_init(&poll->wait.entry);\n\n\t\t/*\n\t\t * Careful: this *must* be the last step, since as soon\n\t\t * as req->head is NULL'ed out, the request can be\n\t\t * completed and freed, since aio_poll_complete_work()\n\t\t * will no longer need to take the waitqueue lock.\n\t\t */\n\t\tsmp_store_release(&poll->head, NULL);\n\t\treturn 1;\n\t}\n\n\t/* for instances that support it check for an event match first */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\tif (io_poll_get_ownership(req)) {\n\t\t/* optional, saves extra locking for removal in tw handler */\n\t\tif (mask && poll->events & EPOLLONESHOT) {\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\t\tpoll->head = NULL;\n\t\t}\n\t\t__io_poll_execute(req, mask);\n\t}\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,30 @@\n \tstruct io_poll_iocb *poll = container_of(wait, struct io_poll_iocb,\n \t\t\t\t\t\t wait);\n \t__poll_t mask = key_to_poll(key);\n+\n+\tif (unlikely(mask & POLLFREE)) {\n+\t\tio_poll_mark_cancelled(req);\n+\t\t/* we have to kick tw in case it's not already */\n+\t\tio_poll_execute(req, 0);\n+\n+\t\t/*\n+\t\t * If the waitqueue is being freed early but someone is already\n+\t\t * holds ownership over it, we have to tear down the request as\n+\t\t * best we can. That means immediately removing the request from\n+\t\t * its waitqueue and preventing all further accesses to the\n+\t\t * waitqueue via the request.\n+\t\t */\n+\t\tlist_del_init(&poll->wait.entry);\n+\n+\t\t/*\n+\t\t * Careful: this *must* be the last step, since as soon\n+\t\t * as req->head is NULL'ed out, the request can be\n+\t\t * completed and freed, since aio_poll_complete_work()\n+\t\t * will no longer need to take the waitqueue lock.\n+\t\t */\n+\t\tsmp_store_release(&poll->head, NULL);\n+\t\treturn 1;\n+\t}\n \n \t/* for instances that support it check for an event match first */\n \tif (mask && !(mask & poll->events))",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (unlikely(mask & POLLFREE)) {",
                "\t\tio_poll_mark_cancelled(req);",
                "\t\t/* we have to kick tw in case it's not already */",
                "\t\tio_poll_execute(req, 0);",
                "",
                "\t\t/*",
                "\t\t * If the waitqueue is being freed early but someone is already",
                "\t\t * holds ownership over it, we have to tear down the request as",
                "\t\t * best we can. That means immediately removing the request from",
                "\t\t * its waitqueue and preventing all further accesses to the",
                "\t\t * waitqueue via the request.",
                "\t\t */",
                "\t\tlist_del_init(&poll->wait.entry);",
                "",
                "\t\t/*",
                "\t\t * Careful: this *must* be the last step, since as soon",
                "\t\t * as req->head is NULL'ed out, the request can be",
                "\t\t * completed and freed, since aio_poll_complete_work()",
                "\t\t * will no longer need to take the waitqueue lock.",
                "\t\t */",
                "\t\tsmp_store_release(&poll->head, NULL);",
                "\t\treturn 1;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There exists a use-after-free in io_uring in the Linux kernel. Signalfd_poll() and binder_poll() use a waitqueue whose lifetime is the current task. It will send a POLLFREE notification to all waiters before the queue is freed. Unfortunately, the io_uring poll doesn't handle POLLFREE. This allows a use-after-free to occur if a signalfd or binder fd is polled with io_uring poll, and the waitqueue gets freed. We recommend upgrading past commit fc78b2fc21f10c4c9c4d5d659a685710ffa63659",
        "id": 3565
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "int inet6_sk_rebuild_header(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dst_entry *dst;\n\n\tdst = __sk_dst_check(sk, np->dst_cookie);\n\n\tif (!dst) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = sk->sk_protocol;\n\t\tfl6.daddr = sk->sk_v6_daddr;\n\t\tfl6.saddr = np->saddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tfl6.fl6_sport = inet->inet_sport;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst)) {\n\t\t\tsk->sk_route_caps = 0;\n\t\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\t\treturn PTR_ERR(dst);\n\t\t}\n\n\t\t__ip6_dst_store(sk, dst, NULL, NULL);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int inet6_sk_rebuild_header(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dst_entry *dst;\n\n\tdst = __sk_dst_check(sk, np->dst_cookie);\n\n\tif (!dst) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = sk->sk_protocol;\n\t\tfl6.daddr = sk->sk_v6_daddr;\n\t\tfl6.saddr = np->saddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tfl6.fl6_sport = inet->inet_sport;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\trcu_read_lock();\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt),\n\t\t\t\t\t &final);\n\t\trcu_read_unlock();\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst)) {\n\t\t\tsk->sk_route_caps = 0;\n\t\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\t\treturn PTR_ERR(dst);\n\t\t}\n\n\t\t__ip6_dst_store(sk, dst, NULL, NULL);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,7 +21,10 @@\n \t\tfl6.fl6_sport = inet->inet_sport;\n \t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n-\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\t\trcu_read_lock();\n+\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt),\n+\t\t\t\t\t &final);\n+\t\trcu_read_unlock();\n \n \t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \t\tif (IS_ERR(dst)) {",
        "function_modified_lines": {
            "added": [
                "\t\trcu_read_lock();",
                "\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt),",
                "\t\t\t\t\t &final);",
                "\t\trcu_read_unlock();"
            ],
            "deleted": [
                "\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 993
    },
    {
        "cve_id": "CVE-2019-15220",
        "code_before_change": "static int p54u_probe(struct usb_interface *intf,\n\t\t\t\tconst struct usb_device_id *id)\n{\n\tstruct usb_device *udev = interface_to_usbdev(intf);\n\tstruct ieee80211_hw *dev;\n\tstruct p54u_priv *priv;\n\tint err;\n\tunsigned int i, recognized_pipes;\n\n\tdev = p54_init_common(sizeof(*priv));\n\n\tif (!dev) {\n\t\tdev_err(&udev->dev, \"(p54usb) ieee80211 alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpriv = dev->priv;\n\tpriv->hw_type = P54U_INVALID_HW;\n\n\tSET_IEEE80211_DEV(dev, &intf->dev);\n\tusb_set_intfdata(intf, dev);\n\tpriv->udev = udev;\n\tpriv->intf = intf;\n\tskb_queue_head_init(&priv->rx_queue);\n\tinit_usb_anchor(&priv->submitted);\n\n\tusb_get_dev(udev);\n\n\t/* really lazy and simple way of figuring out if we're a 3887 */\n\t/* TODO: should just stick the identification in the device table */\n\ti = intf->altsetting->desc.bNumEndpoints;\n\trecognized_pipes = 0;\n\twhile (i--) {\n\t\tswitch (intf->altsetting->endpoint[i].desc.bEndpointAddress) {\n\t\tcase P54U_PIPE_DATA:\n\t\tcase P54U_PIPE_MGMT:\n\t\tcase P54U_PIPE_BRG:\n\t\tcase P54U_PIPE_DEV:\n\t\tcase P54U_PIPE_DATA | USB_DIR_IN:\n\t\tcase P54U_PIPE_MGMT | USB_DIR_IN:\n\t\tcase P54U_PIPE_BRG | USB_DIR_IN:\n\t\tcase P54U_PIPE_DEV | USB_DIR_IN:\n\t\tcase P54U_PIPE_INT | USB_DIR_IN:\n\t\t\trecognized_pipes++;\n\t\t}\n\t}\n\tpriv->common.open = p54u_open;\n\tpriv->common.stop = p54u_stop;\n\tif (recognized_pipes < P54U_PIPE_NUMBER) {\n#ifdef CONFIG_PM\n\t\t/* ISL3887 needs a full reset on resume */\n\t\tudev->reset_resume = 1;\n#endif /* CONFIG_PM */\n\t\terr = p54u_device_reset(dev);\n\n\t\tpriv->hw_type = P54U_3887;\n\t\tdev->extra_tx_headroom += sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_lm87;\n\t\tpriv->upload_fw = p54u_upload_firmware_3887;\n\t} else {\n\t\tpriv->hw_type = P54U_NET2280;\n\t\tdev->extra_tx_headroom += sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_net2280;\n\t\tpriv->upload_fw = p54u_upload_firmware_net2280;\n\t}\n\terr = p54u_load_firmware(dev, intf);\n\tif (err) {\n\t\tusb_put_dev(udev);\n\t\tp54_free_common(dev);\n\t}\n\treturn err;\n}",
        "code_after_change": "static int p54u_probe(struct usb_interface *intf,\n\t\t\t\tconst struct usb_device_id *id)\n{\n\tstruct usb_device *udev = interface_to_usbdev(intf);\n\tstruct ieee80211_hw *dev;\n\tstruct p54u_priv *priv;\n\tint err;\n\tunsigned int i, recognized_pipes;\n\n\tdev = p54_init_common(sizeof(*priv));\n\n\tif (!dev) {\n\t\tdev_err(&udev->dev, \"(p54usb) ieee80211 alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpriv = dev->priv;\n\tpriv->hw_type = P54U_INVALID_HW;\n\n\tSET_IEEE80211_DEV(dev, &intf->dev);\n\tusb_set_intfdata(intf, dev);\n\tpriv->udev = udev;\n\tpriv->intf = intf;\n\tskb_queue_head_init(&priv->rx_queue);\n\tinit_usb_anchor(&priv->submitted);\n\n\t/* really lazy and simple way of figuring out if we're a 3887 */\n\t/* TODO: should just stick the identification in the device table */\n\ti = intf->altsetting->desc.bNumEndpoints;\n\trecognized_pipes = 0;\n\twhile (i--) {\n\t\tswitch (intf->altsetting->endpoint[i].desc.bEndpointAddress) {\n\t\tcase P54U_PIPE_DATA:\n\t\tcase P54U_PIPE_MGMT:\n\t\tcase P54U_PIPE_BRG:\n\t\tcase P54U_PIPE_DEV:\n\t\tcase P54U_PIPE_DATA | USB_DIR_IN:\n\t\tcase P54U_PIPE_MGMT | USB_DIR_IN:\n\t\tcase P54U_PIPE_BRG | USB_DIR_IN:\n\t\tcase P54U_PIPE_DEV | USB_DIR_IN:\n\t\tcase P54U_PIPE_INT | USB_DIR_IN:\n\t\t\trecognized_pipes++;\n\t\t}\n\t}\n\tpriv->common.open = p54u_open;\n\tpriv->common.stop = p54u_stop;\n\tif (recognized_pipes < P54U_PIPE_NUMBER) {\n#ifdef CONFIG_PM\n\t\t/* ISL3887 needs a full reset on resume */\n\t\tudev->reset_resume = 1;\n#endif /* CONFIG_PM */\n\t\terr = p54u_device_reset(dev);\n\n\t\tpriv->hw_type = P54U_3887;\n\t\tdev->extra_tx_headroom += sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_lm87;\n\t\tpriv->upload_fw = p54u_upload_firmware_3887;\n\t} else {\n\t\tpriv->hw_type = P54U_NET2280;\n\t\tdev->extra_tx_headroom += sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_net2280;\n\t\tpriv->upload_fw = p54u_upload_firmware_net2280;\n\t}\n\terr = p54u_load_firmware(dev, intf);\n\tif (err)\n\t\tp54_free_common(dev);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,8 +23,6 @@\n \tpriv->intf = intf;\n \tskb_queue_head_init(&priv->rx_queue);\n \tinit_usb_anchor(&priv->submitted);\n-\n-\tusb_get_dev(udev);\n \n \t/* really lazy and simple way of figuring out if we're a 3887 */\n \t/* TODO: should just stick the identification in the device table */\n@@ -66,9 +64,7 @@\n \t\tpriv->upload_fw = p54u_upload_firmware_net2280;\n \t}\n \terr = p54u_load_firmware(dev, intf);\n-\tif (err) {\n-\t\tusb_put_dev(udev);\n+\tif (err)\n \t\tp54_free_common(dev);\n-\t}\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (err)"
            ],
            "deleted": [
                "",
                "\tusb_get_dev(udev);",
                "\tif (err) {",
                "\t\tusb_put_dev(udev);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.2.1. There is a use-after-free caused by a malicious USB device in the drivers/net/wireless/intersil/p54/p54usb.c driver.",
        "id": 2003
    },
    {
        "cve_id": "CVE-2016-10200",
        "code_before_change": "static int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tstruct net *net = sock_net(sk);\n\tint ret;\n\tint chk_addr_ret;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(struct sockaddr_l2tpip))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(net, addr->l2tp_addr.s_addr,\n\t\t\t\t  sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tif (addr->l2tp_addr.s_addr)\n\t\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}",
        "code_after_change": "static int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tstruct net *net = sock_net(sk);\n\tint ret;\n\tint chk_addr_ret;\n\n\tif (addr_len < sizeof(struct sockaddr_l2tpip))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(net, addr->l2tp_addr.s_addr,\n\t\t\t\t  sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out;\n\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tif (addr->l2tp_addr.s_addr)\n\t\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,6 @@\n \tint ret;\n \tint chk_addr_ret;\n \n-\tif (!sock_flag(sk, SOCK_ZAPPED))\n-\t\treturn -EINVAL;\n \tif (addr_len < sizeof(struct sockaddr_l2tpip))\n \t\treturn -EINVAL;\n \tif (addr->l2tp_family != AF_INET)\n@@ -22,6 +20,9 @@\n \tread_unlock_bh(&l2tp_ip_lock);\n \n \tlock_sock(sk);\n+\tif (!sock_flag(sk, SOCK_ZAPPED))\n+\t\tgoto out;\n+\n \tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n \t\tgoto out;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\tgoto out;",
                ""
            ],
            "deleted": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the L2TPv3 IP Encapsulation feature in the Linux kernel before 4.8.14 allows local users to gain privileges or cause a denial of service (use-after-free) by making multiple bind system calls without properly ascertaining whether a socket has the SOCK_ZAPPED status, related to net/l2tp/l2tp_ip.c and net/l2tp/l2tp_ip6.c.",
        "id": 898
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static long\nsvm_migrate_vma_to_vram(struct amdgpu_device *adev, struct svm_range *prange,\n\t\t\tstruct vm_area_struct *vma, uint64_t start,\n\t\t\tuint64_t end, uint32_t trigger)\n{\n\tstruct kfd_process *p = container_of(prange->svms, struct kfd_process, svms);\n\tuint64_t npages = (end - start) >> PAGE_SHIFT;\n\tstruct kfd_process_device *pdd;\n\tstruct dma_fence *mfence = NULL;\n\tstruct migrate_vma migrate;\n\tunsigned long cpages = 0;\n\tdma_addr_t *scratch;\n\tvoid *buf;\n\tint r = -ENOMEM;\n\n\tmemset(&migrate, 0, sizeof(migrate));\n\tmigrate.vma = vma;\n\tmigrate.start = start;\n\tmigrate.end = end;\n\tmigrate.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\tmigrate.pgmap_owner = SVM_ADEV_PGMAP_OWNER(adev);\n\n\tbuf = kvcalloc(npages,\n\t\t       2 * sizeof(*migrate.src) + sizeof(uint64_t) + sizeof(dma_addr_t),\n\t\t       GFP_KERNEL);\n\tif (!buf)\n\t\tgoto out;\n\n\tmigrate.src = buf;\n\tmigrate.dst = migrate.src + npages;\n\tscratch = (dma_addr_t *)(migrate.dst + npages);\n\n\tkfd_smi_event_migration_start(adev->kfd.dev, p->lead_thread->pid,\n\t\t\t\t      start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t      0, adev->kfd.dev->id, prange->prefetch_loc,\n\t\t\t\t      prange->preferred_loc, trigger);\n\n\tr = migrate_vma_setup(&migrate);\n\tif (r) {\n\t\tdev_err(adev->dev, \"%s: vma setup fail %d range [0x%lx 0x%lx]\\n\",\n\t\t\t__func__, r, prange->start, prange->last);\n\t\tgoto out_free;\n\t}\n\n\tcpages = migrate.cpages;\n\tif (!cpages) {\n\t\tpr_debug(\"failed collect migrate sys pages [0x%lx 0x%lx]\\n\",\n\t\t\t prange->start, prange->last);\n\t\tgoto out_free;\n\t}\n\tif (cpages != npages)\n\t\tpr_debug(\"partial migration, 0x%lx/0x%llx pages migrated\\n\",\n\t\t\t cpages, npages);\n\telse\n\t\tpr_debug(\"0x%lx pages migrated\\n\", cpages);\n\n\tr = svm_migrate_copy_to_vram(adev, prange, &migrate, &mfence, scratch);\n\tmigrate_vma_pages(&migrate);\n\n\tpr_debug(\"successful/cpages/npages 0x%lx/0x%lx/0x%lx\\n\",\n\t\tsvm_migrate_successful_pages(&migrate), cpages, migrate.npages);\n\n\tsvm_migrate_copy_done(adev, mfence);\n\tmigrate_vma_finalize(&migrate);\n\n\tkfd_smi_event_migration_end(adev->kfd.dev, p->lead_thread->pid,\n\t\t\t\t    start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t    0, adev->kfd.dev->id, trigger);\n\n\tsvm_range_dma_unmap(adev->dev, scratch, 0, npages);\n\tsvm_range_free_dma_mappings(prange);\n\nout_free:\n\tkvfree(buf);\nout:\n\tif (!r && cpages) {\n\t\tpdd = svm_range_get_pdd_by_adev(prange, adev);\n\t\tif (pdd)\n\t\t\tWRITE_ONCE(pdd->page_in, pdd->page_in + cpages);\n\n\t\treturn cpages;\n\t}\n\treturn r;\n}",
        "code_after_change": "static long\nsvm_migrate_vma_to_vram(struct amdgpu_device *adev, struct svm_range *prange,\n\t\t\tstruct vm_area_struct *vma, uint64_t start,\n\t\t\tuint64_t end, uint32_t trigger)\n{\n\tstruct kfd_process *p = container_of(prange->svms, struct kfd_process, svms);\n\tuint64_t npages = (end - start) >> PAGE_SHIFT;\n\tstruct kfd_process_device *pdd;\n\tstruct dma_fence *mfence = NULL;\n\tstruct migrate_vma migrate = { 0 };\n\tunsigned long cpages = 0;\n\tdma_addr_t *scratch;\n\tvoid *buf;\n\tint r = -ENOMEM;\n\n\tmemset(&migrate, 0, sizeof(migrate));\n\tmigrate.vma = vma;\n\tmigrate.start = start;\n\tmigrate.end = end;\n\tmigrate.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\tmigrate.pgmap_owner = SVM_ADEV_PGMAP_OWNER(adev);\n\n\tbuf = kvcalloc(npages,\n\t\t       2 * sizeof(*migrate.src) + sizeof(uint64_t) + sizeof(dma_addr_t),\n\t\t       GFP_KERNEL);\n\tif (!buf)\n\t\tgoto out;\n\n\tmigrate.src = buf;\n\tmigrate.dst = migrate.src + npages;\n\tscratch = (dma_addr_t *)(migrate.dst + npages);\n\n\tkfd_smi_event_migration_start(adev->kfd.dev, p->lead_thread->pid,\n\t\t\t\t      start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t      0, adev->kfd.dev->id, prange->prefetch_loc,\n\t\t\t\t      prange->preferred_loc, trigger);\n\n\tr = migrate_vma_setup(&migrate);\n\tif (r) {\n\t\tdev_err(adev->dev, \"%s: vma setup fail %d range [0x%lx 0x%lx]\\n\",\n\t\t\t__func__, r, prange->start, prange->last);\n\t\tgoto out_free;\n\t}\n\n\tcpages = migrate.cpages;\n\tif (!cpages) {\n\t\tpr_debug(\"failed collect migrate sys pages [0x%lx 0x%lx]\\n\",\n\t\t\t prange->start, prange->last);\n\t\tgoto out_free;\n\t}\n\tif (cpages != npages)\n\t\tpr_debug(\"partial migration, 0x%lx/0x%llx pages migrated\\n\",\n\t\t\t cpages, npages);\n\telse\n\t\tpr_debug(\"0x%lx pages migrated\\n\", cpages);\n\n\tr = svm_migrate_copy_to_vram(adev, prange, &migrate, &mfence, scratch);\n\tmigrate_vma_pages(&migrate);\n\n\tpr_debug(\"successful/cpages/npages 0x%lx/0x%lx/0x%lx\\n\",\n\t\tsvm_migrate_successful_pages(&migrate), cpages, migrate.npages);\n\n\tsvm_migrate_copy_done(adev, mfence);\n\tmigrate_vma_finalize(&migrate);\n\n\tkfd_smi_event_migration_end(adev->kfd.dev, p->lead_thread->pid,\n\t\t\t\t    start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t    0, adev->kfd.dev->id, trigger);\n\n\tsvm_range_dma_unmap(adev->dev, scratch, 0, npages);\n\tsvm_range_free_dma_mappings(prange);\n\nout_free:\n\tkvfree(buf);\nout:\n\tif (!r && cpages) {\n\t\tpdd = svm_range_get_pdd_by_adev(prange, adev);\n\t\tif (pdd)\n\t\t\tWRITE_ONCE(pdd->page_in, pdd->page_in + cpages);\n\n\t\treturn cpages;\n\t}\n\treturn r;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \tuint64_t npages = (end - start) >> PAGE_SHIFT;\n \tstruct kfd_process_device *pdd;\n \tstruct dma_fence *mfence = NULL;\n-\tstruct migrate_vma migrate;\n+\tstruct migrate_vma migrate = { 0 };\n \tunsigned long cpages = 0;\n \tdma_addr_t *scratch;\n \tvoid *buf;",
        "function_modified_lines": {
            "added": [
                "\tstruct migrate_vma migrate = { 0 };"
            ],
            "deleted": [
                "\tstruct migrate_vma migrate;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3610
    },
    {
        "cve_id": "CVE-2019-25045",
        "code_before_change": "static int\nparse_ipsecrequest(struct xfrm_policy *xp, struct sadb_x_ipsecrequest *rq)\n{\n\tstruct net *net = xp_net(xp);\n\tstruct xfrm_tmpl *t = xp->xfrm_vec + xp->xfrm_nr;\n\tint mode;\n\n\tif (xp->xfrm_nr >= XFRM_MAX_DEPTH)\n\t\treturn -ELOOP;\n\n\tif (rq->sadb_x_ipsecrequest_mode == 0)\n\t\treturn -EINVAL;\n\n\tt->id.proto = rq->sadb_x_ipsecrequest_proto; /* XXX check proto */\n\tif ((mode = pfkey_mode_to_xfrm(rq->sadb_x_ipsecrequest_mode)) < 0)\n\t\treturn -EINVAL;\n\tt->mode = mode;\n\tif (rq->sadb_x_ipsecrequest_level == IPSEC_LEVEL_USE)\n\t\tt->optional = 1;\n\telse if (rq->sadb_x_ipsecrequest_level == IPSEC_LEVEL_UNIQUE) {\n\t\tt->reqid = rq->sadb_x_ipsecrequest_reqid;\n\t\tif (t->reqid > IPSEC_MANUAL_REQID_MAX)\n\t\t\tt->reqid = 0;\n\t\tif (!t->reqid && !(t->reqid = gen_reqid(net)))\n\t\t\treturn -ENOBUFS;\n\t}\n\n\t/* addresses present only in tunnel mode */\n\tif (t->mode == XFRM_MODE_TUNNEL) {\n\t\tint err;\n\n\t\terr = parse_sockaddr_pair(\n\t\t\t(struct sockaddr *)(rq + 1),\n\t\t\trq->sadb_x_ipsecrequest_len - sizeof(*rq),\n\t\t\t&t->saddr, &t->id.daddr, &t->encap_family);\n\t\tif (err)\n\t\t\treturn err;\n\t} else\n\t\tt->encap_family = xp->family;\n\n\t/* No way to set this via kame pfkey */\n\tt->allalgs = 1;\n\txp->xfrm_nr++;\n\treturn 0;\n}",
        "code_after_change": "static int\nparse_ipsecrequest(struct xfrm_policy *xp, struct sadb_x_ipsecrequest *rq)\n{\n\tstruct net *net = xp_net(xp);\n\tstruct xfrm_tmpl *t = xp->xfrm_vec + xp->xfrm_nr;\n\tint mode;\n\n\tif (xp->xfrm_nr >= XFRM_MAX_DEPTH)\n\t\treturn -ELOOP;\n\n\tif (rq->sadb_x_ipsecrequest_mode == 0)\n\t\treturn -EINVAL;\n\tif (!xfrm_id_proto_valid(rq->sadb_x_ipsecrequest_proto))\n\t\treturn -EINVAL;\n\n\tt->id.proto = rq->sadb_x_ipsecrequest_proto;\n\tif ((mode = pfkey_mode_to_xfrm(rq->sadb_x_ipsecrequest_mode)) < 0)\n\t\treturn -EINVAL;\n\tt->mode = mode;\n\tif (rq->sadb_x_ipsecrequest_level == IPSEC_LEVEL_USE)\n\t\tt->optional = 1;\n\telse if (rq->sadb_x_ipsecrequest_level == IPSEC_LEVEL_UNIQUE) {\n\t\tt->reqid = rq->sadb_x_ipsecrequest_reqid;\n\t\tif (t->reqid > IPSEC_MANUAL_REQID_MAX)\n\t\t\tt->reqid = 0;\n\t\tif (!t->reqid && !(t->reqid = gen_reqid(net)))\n\t\t\treturn -ENOBUFS;\n\t}\n\n\t/* addresses present only in tunnel mode */\n\tif (t->mode == XFRM_MODE_TUNNEL) {\n\t\tint err;\n\n\t\terr = parse_sockaddr_pair(\n\t\t\t(struct sockaddr *)(rq + 1),\n\t\t\trq->sadb_x_ipsecrequest_len - sizeof(*rq),\n\t\t\t&t->saddr, &t->id.daddr, &t->encap_family);\n\t\tif (err)\n\t\t\treturn err;\n\t} else\n\t\tt->encap_family = xp->family;\n\n\t/* No way to set this via kame pfkey */\n\tt->allalgs = 1;\n\txp->xfrm_nr++;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,8 +10,10 @@\n \n \tif (rq->sadb_x_ipsecrequest_mode == 0)\n \t\treturn -EINVAL;\n+\tif (!xfrm_id_proto_valid(rq->sadb_x_ipsecrequest_proto))\n+\t\treturn -EINVAL;\n \n-\tt->id.proto = rq->sadb_x_ipsecrequest_proto; /* XXX check proto */\n+\tt->id.proto = rq->sadb_x_ipsecrequest_proto;\n \tif ((mode = pfkey_mode_to_xfrm(rq->sadb_x_ipsecrequest_mode)) < 0)\n \t\treturn -EINVAL;\n \tt->mode = mode;",
        "function_modified_lines": {
            "added": [
                "\tif (!xfrm_id_proto_valid(rq->sadb_x_ipsecrequest_proto))",
                "\t\treturn -EINVAL;",
                "\tt->id.proto = rq->sadb_x_ipsecrequest_proto;"
            ],
            "deleted": [
                "\tt->id.proto = rq->sadb_x_ipsecrequest_proto; /* XXX check proto */"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.19. The XFRM subsystem has a use-after-free, related to an xfrm_state_fini panic, aka CID-dbb2483b2a46.",
        "id": 2303
    },
    {
        "cve_id": "CVE-2022-33981",
        "code_before_change": "static int fd_locked_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd,\n\t\t    unsigned long param)\n{\n\tint drive = (long)bdev->bd_disk->private_data;\n\tint type = ITYPE(drive_state[drive].fd_device);\n\tint i;\n\tint ret;\n\tint size;\n\tunion inparam {\n\t\tstruct floppy_struct g;\t/* geometry */\n\t\tstruct format_descr f;\n\t\tstruct floppy_max_errors max_errors;\n\t\tstruct floppy_drive_params dp;\n\t} inparam;\t\t/* parameters coming from user space */\n\tconst void *outparam;\t/* parameters passed back to user space */\n\n\t/* convert compatibility eject ioctls into floppy eject ioctl.\n\t * We do this in order to provide a means to eject floppy disks before\n\t * installing the new fdutils package */\n\tif (cmd == CDROMEJECT ||\t/* CD-ROM eject */\n\t    cmd == 0x6470) {\t\t/* SunOS floppy eject */\n\t\tDPRINT(\"obsolete eject ioctl\\n\");\n\t\tDPRINT(\"please use floppycontrol --eject\\n\");\n\t\tcmd = FDEJECT;\n\t}\n\n\tif (!((cmd & 0xff00) == 0x0200))\n\t\treturn -EINVAL;\n\n\t/* convert the old style command into a new style command */\n\tret = normalize_ioctl(&cmd, &size);\n\tif (ret)\n\t\treturn ret;\n\n\t/* permission checks */\n\tif (((cmd & 0x40) && !(mode & (FMODE_WRITE | FMODE_WRITE_IOCTL))) ||\n\t    ((cmd & 0x80) && !capable(CAP_SYS_ADMIN)))\n\t\treturn -EPERM;\n\n\tif (WARN_ON(size < 0 || size > sizeof(inparam)))\n\t\treturn -EINVAL;\n\n\t/* copyin */\n\tmemset(&inparam, 0, sizeof(inparam));\n\tif (_IOC_DIR(cmd) & _IOC_WRITE) {\n\t\tret = fd_copyin((void __user *)param, &inparam, size);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tswitch (cmd) {\n\tcase FDEJECT:\n\t\tif (drive_state[drive].fd_ref != 1)\n\t\t\t/* somebody else has this drive open */\n\t\t\treturn -EBUSY;\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\n\t\t/* do the actual eject. Fails on\n\t\t * non-Sparc architectures */\n\t\tret = fd_eject(UNIT(drive));\n\n\t\tset_bit(FD_DISK_CHANGED_BIT, &drive_state[drive].flags);\n\t\tset_bit(FD_VERIFY_BIT, &drive_state[drive].flags);\n\t\tprocess_fd_request();\n\t\treturn ret;\n\tcase FDCLRPRM:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tcurrent_type[drive] = NULL;\n\t\tfloppy_sizes[drive] = MAX_DISK_SIZE << 1;\n\t\tdrive_state[drive].keep_data = 0;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETPRM:\n\tcase FDDEFPRM:\n\t\treturn set_geometry(cmd, &inparam.g, drive, type, bdev);\n\tcase FDGETPRM:\n\t\tret = get_floppy_geometry(drive, type,\n\t\t\t\t\t  (struct floppy_struct **)&outparam);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tmemcpy(&inparam.g, outparam,\n\t\t\t\toffsetof(struct floppy_struct, name));\n\t\toutparam = &inparam.g;\n\t\tbreak;\n\tcase FDMSGON:\n\t\tdrive_params[drive].flags |= FTD_MSG;\n\t\treturn 0;\n\tcase FDMSGOFF:\n\t\tdrive_params[drive].flags &= ~FTD_MSG;\n\t\treturn 0;\n\tcase FDFMTBEG:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tret = drive_state[drive].flags;\n\t\tprocess_fd_request();\n\t\tif (ret & FD_VERIFY)\n\t\t\treturn -ENODEV;\n\t\tif (!(ret & FD_DISK_WRITABLE))\n\t\t\treturn -EROFS;\n\t\treturn 0;\n\tcase FDFMTTRK:\n\t\tif (drive_state[drive].fd_ref != 1)\n\t\t\treturn -EBUSY;\n\t\treturn do_format(drive, &inparam.f);\n\tcase FDFMTEND:\n\tcase FDFLUSH:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETEMSGTRESH:\n\t\tdrive_params[drive].max_errors.reporting = (unsigned short)(param & 0x0f);\n\t\treturn 0;\n\tcase FDGETMAXERRS:\n\t\toutparam = &drive_params[drive].max_errors;\n\t\tbreak;\n\tcase FDSETMAXERRS:\n\t\tdrive_params[drive].max_errors = inparam.max_errors;\n\t\tbreak;\n\tcase FDGETDRVTYP:\n\t\toutparam = drive_name(type, drive);\n\t\tSUPBOUND(size, strlen((const char *)outparam) + 1);\n\t\tbreak;\n\tcase FDSETDRVPRM:\n\t\tif (!valid_floppy_drive_params(inparam.dp.autodetect,\n\t\t\t\tinparam.dp.native_format))\n\t\t\treturn -EINVAL;\n\t\tdrive_params[drive] = inparam.dp;\n\t\tbreak;\n\tcase FDGETDRVPRM:\n\t\toutparam = &drive_params[drive];\n\t\tbreak;\n\tcase FDPOLLDRVSTAT:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\tfallthrough;\n\tcase FDGETDRVSTAT:\n\t\toutparam = &drive_state[drive];\n\t\tbreak;\n\tcase FDRESET:\n\t\treturn user_reset_fdc(drive, (int)param, true);\n\tcase FDGETFDCSTAT:\n\t\toutparam = &fdc_state[FDC(drive)];\n\t\tbreak;\n\tcase FDWERRORCLR:\n\t\tmemset(&write_errors[drive], 0, sizeof(write_errors[drive]));\n\t\treturn 0;\n\tcase FDWERRORGET:\n\t\toutparam = &write_errors[drive];\n\t\tbreak;\n\tcase FDRAWCMD:\n\t\tif (type)\n\t\t\treturn -EINVAL;\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tset_floppy(drive);\n\t\ti = raw_cmd_ioctl(cmd, (void __user *)param);\n\t\tif (i == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\treturn i;\n\tcase FDTWADDLE:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\ttwaddle(current_fdc, current_drive);\n\t\tprocess_fd_request();\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (_IOC_DIR(cmd) & _IOC_READ)\n\t\treturn fd_copyout((void __user *)param, outparam, size);\n\n\treturn 0;\n}",
        "code_after_change": "static int fd_locked_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd,\n\t\t    unsigned long param)\n{\n\tint drive = (long)bdev->bd_disk->private_data;\n\tint type = ITYPE(drive_state[drive].fd_device);\n\tint ret;\n\tint size;\n\tunion inparam {\n\t\tstruct floppy_struct g;\t/* geometry */\n\t\tstruct format_descr f;\n\t\tstruct floppy_max_errors max_errors;\n\t\tstruct floppy_drive_params dp;\n\t} inparam;\t\t/* parameters coming from user space */\n\tconst void *outparam;\t/* parameters passed back to user space */\n\n\t/* convert compatibility eject ioctls into floppy eject ioctl.\n\t * We do this in order to provide a means to eject floppy disks before\n\t * installing the new fdutils package */\n\tif (cmd == CDROMEJECT ||\t/* CD-ROM eject */\n\t    cmd == 0x6470) {\t\t/* SunOS floppy eject */\n\t\tDPRINT(\"obsolete eject ioctl\\n\");\n\t\tDPRINT(\"please use floppycontrol --eject\\n\");\n\t\tcmd = FDEJECT;\n\t}\n\n\tif (!((cmd & 0xff00) == 0x0200))\n\t\treturn -EINVAL;\n\n\t/* convert the old style command into a new style command */\n\tret = normalize_ioctl(&cmd, &size);\n\tif (ret)\n\t\treturn ret;\n\n\t/* permission checks */\n\tif (((cmd & 0x40) && !(mode & (FMODE_WRITE | FMODE_WRITE_IOCTL))) ||\n\t    ((cmd & 0x80) && !capable(CAP_SYS_ADMIN)))\n\t\treturn -EPERM;\n\n\tif (WARN_ON(size < 0 || size > sizeof(inparam)))\n\t\treturn -EINVAL;\n\n\t/* copyin */\n\tmemset(&inparam, 0, sizeof(inparam));\n\tif (_IOC_DIR(cmd) & _IOC_WRITE) {\n\t\tret = fd_copyin((void __user *)param, &inparam, size);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tswitch (cmd) {\n\tcase FDEJECT:\n\t\tif (drive_state[drive].fd_ref != 1)\n\t\t\t/* somebody else has this drive open */\n\t\t\treturn -EBUSY;\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\n\t\t/* do the actual eject. Fails on\n\t\t * non-Sparc architectures */\n\t\tret = fd_eject(UNIT(drive));\n\n\t\tset_bit(FD_DISK_CHANGED_BIT, &drive_state[drive].flags);\n\t\tset_bit(FD_VERIFY_BIT, &drive_state[drive].flags);\n\t\tprocess_fd_request();\n\t\treturn ret;\n\tcase FDCLRPRM:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tcurrent_type[drive] = NULL;\n\t\tfloppy_sizes[drive] = MAX_DISK_SIZE << 1;\n\t\tdrive_state[drive].keep_data = 0;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETPRM:\n\tcase FDDEFPRM:\n\t\treturn set_geometry(cmd, &inparam.g, drive, type, bdev);\n\tcase FDGETPRM:\n\t\tret = get_floppy_geometry(drive, type,\n\t\t\t\t\t  (struct floppy_struct **)&outparam);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tmemcpy(&inparam.g, outparam,\n\t\t\t\toffsetof(struct floppy_struct, name));\n\t\toutparam = &inparam.g;\n\t\tbreak;\n\tcase FDMSGON:\n\t\tdrive_params[drive].flags |= FTD_MSG;\n\t\treturn 0;\n\tcase FDMSGOFF:\n\t\tdrive_params[drive].flags &= ~FTD_MSG;\n\t\treturn 0;\n\tcase FDFMTBEG:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tret = drive_state[drive].flags;\n\t\tprocess_fd_request();\n\t\tif (ret & FD_VERIFY)\n\t\t\treturn -ENODEV;\n\t\tif (!(ret & FD_DISK_WRITABLE))\n\t\t\treturn -EROFS;\n\t\treturn 0;\n\tcase FDFMTTRK:\n\t\tif (drive_state[drive].fd_ref != 1)\n\t\t\treturn -EBUSY;\n\t\treturn do_format(drive, &inparam.f);\n\tcase FDFMTEND:\n\tcase FDFLUSH:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETEMSGTRESH:\n\t\tdrive_params[drive].max_errors.reporting = (unsigned short)(param & 0x0f);\n\t\treturn 0;\n\tcase FDGETMAXERRS:\n\t\toutparam = &drive_params[drive].max_errors;\n\t\tbreak;\n\tcase FDSETMAXERRS:\n\t\tdrive_params[drive].max_errors = inparam.max_errors;\n\t\tbreak;\n\tcase FDGETDRVTYP:\n\t\toutparam = drive_name(type, drive);\n\t\tSUPBOUND(size, strlen((const char *)outparam) + 1);\n\t\tbreak;\n\tcase FDSETDRVPRM:\n\t\tif (!valid_floppy_drive_params(inparam.dp.autodetect,\n\t\t\t\tinparam.dp.native_format))\n\t\t\treturn -EINVAL;\n\t\tdrive_params[drive] = inparam.dp;\n\t\tbreak;\n\tcase FDGETDRVPRM:\n\t\toutparam = &drive_params[drive];\n\t\tbreak;\n\tcase FDPOLLDRVSTAT:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\tfallthrough;\n\tcase FDGETDRVSTAT:\n\t\toutparam = &drive_state[drive];\n\t\tbreak;\n\tcase FDRESET:\n\t\treturn user_reset_fdc(drive, (int)param, true);\n\tcase FDGETFDCSTAT:\n\t\toutparam = &fdc_state[FDC(drive)];\n\t\tbreak;\n\tcase FDWERRORCLR:\n\t\tmemset(&write_errors[drive], 0, sizeof(write_errors[drive]));\n\t\treturn 0;\n\tcase FDWERRORGET:\n\t\toutparam = &write_errors[drive];\n\t\tbreak;\n\tcase FDRAWCMD:\n\t\treturn floppy_raw_cmd_ioctl(type, drive, cmd, (void __user *)param);\n\tcase FDTWADDLE:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\ttwaddle(current_fdc, current_drive);\n\t\tprocess_fd_request();\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (_IOC_DIR(cmd) & _IOC_READ)\n\t\treturn fd_copyout((void __user *)param, outparam, size);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,6 @@\n {\n \tint drive = (long)bdev->bd_disk->private_data;\n \tint type = ITYPE(drive_state[drive].fd_device);\n-\tint i;\n \tint ret;\n \tint size;\n \tunion inparam {\n@@ -154,16 +153,7 @@\n \t\toutparam = &write_errors[drive];\n \t\tbreak;\n \tcase FDRAWCMD:\n-\t\tif (type)\n-\t\t\treturn -EINVAL;\n-\t\tif (lock_fdc(drive))\n-\t\t\treturn -EINTR;\n-\t\tset_floppy(drive);\n-\t\ti = raw_cmd_ioctl(cmd, (void __user *)param);\n-\t\tif (i == -EINTR)\n-\t\t\treturn -EINTR;\n-\t\tprocess_fd_request();\n-\t\treturn i;\n+\t\treturn floppy_raw_cmd_ioctl(type, drive, cmd, (void __user *)param);\n \tcase FDTWADDLE:\n \t\tif (lock_fdc(drive))\n \t\t\treturn -EINTR;",
        "function_modified_lines": {
            "added": [
                "\t\treturn floppy_raw_cmd_ioctl(type, drive, cmd, (void __user *)param);"
            ],
            "deleted": [
                "\tint i;",
                "\t\tif (type)",
                "\t\t\treturn -EINVAL;",
                "\t\tif (lock_fdc(drive))",
                "\t\t\treturn -EINTR;",
                "\t\tset_floppy(drive);",
                "\t\ti = raw_cmd_ioctl(cmd, (void __user *)param);",
                "\t\tif (i == -EINTR)",
                "\t\t\treturn -EINTR;",
                "\t\tprocess_fd_request();",
                "\t\treturn i;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "drivers/block/floppy.c in the Linux kernel before 5.17.6 is vulnerable to a denial of service, because of a concurrency use-after-free flaw after deallocating raw_cmd in the raw_cmd_ioctl function.",
        "id": 3594
    },
    {
        "cve_id": "CVE-2022-20158",
        "code_before_change": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, hdrlen;\n\tunsigned int netoff;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\tbool do_vnet = false;\n\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev_has_header(dev)) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\n\t/* If we are flooded, just give up */\n\tif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (netoff > USHRT_MAX) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb)\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tdo_vnet = false;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tdo_vnet = false;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tslot_id = po->rx_ring.head;\n\t\tif (test_bit(slot_id, po->rx_ring.rx_owner_map))\n\t\t\tgoto drop_n_account;\n\t\t__set_bit(slot_id, po->rx_ring.rx_owner_map);\n\t}\n\n\tif (do_vnet &&\n\t    virtio_net_hdr_from_skb(skb, h.raw + macoff -\n\t\t\t\t    sizeof(struct virtio_net_hdr),\n\t\t\t\t    vio_le(), true, 0)) {\n\t\tif (po->tp_version == TPACKET_V3)\n\t\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t\tgoto drop_n_account;\n\t}\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tpacket_increment_rx_head(po, &po->rx_ring);\n\t/*\n\t * LOSING will be reported till you read the stats,\n\t * because it's COR - Clear On Read.\n\t * Anyways, moving it for V1/V2 only as V3 doesn't need this\n\t * at packet level.\n\t */\n\t\tif (atomic_read(&po->tp_drops))\n\t\t\tstatus |= TP_STATUS_LOSING;\n\t}\n\n\tpo->stats.stats1.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\t/* Always timestamp; prefer an existing software timestamp taken\n\t * closer to the time of capture.\n\t */\n\tts_status = tpacket_get_timestamp(skb, &ts,\n\t\t\t\t\t  po->tp_tstamp | SOF_TIMESTAMPING_SOFTWARE);\n\tif (!ts_status)\n\t\tktime_get_real_ts64(&ts);\n\n\tstatus |= ts_status;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\th.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t\th.h2->tp_vlan_tpid = 0;\n\t\t}\n\t\tmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\t/* tp_nxt_offset,vlan are already populated above.\n\t\t * So DONT clear those fields here\n\t\t */\n\t\th.h3->tp_status |= status;\n\t\th.h3->tp_len = skb->len;\n\t\th.h3->tp_snaplen = snaplen;\n\t\th.h3->tp_mac = macoff;\n\t\th.h3->tp_net = netoff;\n\t\th.h3->tp_sec  = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\n\t\thdrlen = sizeof(*h.h3);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsmp_mb();\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\n\t\t\t\t\tmacoff + snaplen);\n\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n\tsmp_wmb();\n#endif\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tspin_lock(&sk->sk_receive_queue.lock);\n\t\t__packet_set_status(po, h.raw, status);\n\t\t__clear_bit(slot_id, po->rx_ring.rx_owner_map);\n\t\tspin_unlock(&sk->sk_receive_queue.lock);\n\t\tsk->sk_data_ready(sk);\n\t} else if (po->tp_version == TPACKET_V3) {\n\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t}\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n\ndrop_n_account:\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tatomic_inc(&po->tp_drops);\n\tis_drop_n_account = true;\n\n\tsk->sk_data_ready(sk);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}",
        "code_after_change": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, hdrlen;\n\tunsigned int netoff;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\tbool do_vnet = false;\n\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev_has_header(dev)) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\n\t/* If we are flooded, just give up */\n\tif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (netoff > USHRT_MAX) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb) {\n\t\t\t\t\tmemset(&PACKET_SKB_CB(copy_skb)->sa.ll, 0,\n\t\t\t\t\t       sizeof(PACKET_SKB_CB(copy_skb)->sa.ll));\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t\t}\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tdo_vnet = false;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tdo_vnet = false;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tslot_id = po->rx_ring.head;\n\t\tif (test_bit(slot_id, po->rx_ring.rx_owner_map))\n\t\t\tgoto drop_n_account;\n\t\t__set_bit(slot_id, po->rx_ring.rx_owner_map);\n\t}\n\n\tif (do_vnet &&\n\t    virtio_net_hdr_from_skb(skb, h.raw + macoff -\n\t\t\t\t    sizeof(struct virtio_net_hdr),\n\t\t\t\t    vio_le(), true, 0)) {\n\t\tif (po->tp_version == TPACKET_V3)\n\t\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t\tgoto drop_n_account;\n\t}\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tpacket_increment_rx_head(po, &po->rx_ring);\n\t/*\n\t * LOSING will be reported till you read the stats,\n\t * because it's COR - Clear On Read.\n\t * Anyways, moving it for V1/V2 only as V3 doesn't need this\n\t * at packet level.\n\t */\n\t\tif (atomic_read(&po->tp_drops))\n\t\t\tstatus |= TP_STATUS_LOSING;\n\t}\n\n\tpo->stats.stats1.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\t/* Always timestamp; prefer an existing software timestamp taken\n\t * closer to the time of capture.\n\t */\n\tts_status = tpacket_get_timestamp(skb, &ts,\n\t\t\t\t\t  po->tp_tstamp | SOF_TIMESTAMPING_SOFTWARE);\n\tif (!ts_status)\n\t\tktime_get_real_ts64(&ts);\n\n\tstatus |= ts_status;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\th.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t\th.h2->tp_vlan_tpid = 0;\n\t\t}\n\t\tmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\t/* tp_nxt_offset,vlan are already populated above.\n\t\t * So DONT clear those fields here\n\t\t */\n\t\th.h3->tp_status |= status;\n\t\th.h3->tp_len = skb->len;\n\t\th.h3->tp_snaplen = snaplen;\n\t\th.h3->tp_mac = macoff;\n\t\th.h3->tp_net = netoff;\n\t\th.h3->tp_sec  = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\n\t\thdrlen = sizeof(*h.h3);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsmp_mb();\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\n\t\t\t\t\tmacoff + snaplen);\n\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n\tsmp_wmb();\n#endif\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tspin_lock(&sk->sk_receive_queue.lock);\n\t\t__packet_set_status(po, h.raw, status);\n\t\t__clear_bit(slot_id, po->rx_ring.rx_owner_map);\n\t\tspin_unlock(&sk->sk_receive_queue.lock);\n\t\tsk->sk_data_ready(sk);\n\t} else if (po->tp_version == TPACKET_V3) {\n\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t}\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n\ndrop_n_account:\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tatomic_inc(&po->tp_drops);\n\tis_drop_n_account = true;\n\n\tsk->sk_data_ready(sk);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}",
        "patch": "--- code before\n+++ code after\n@@ -93,8 +93,11 @@\n \t\t\t\t\tcopy_skb = skb_get(skb);\n \t\t\t\t\tskb_head = skb->data;\n \t\t\t\t}\n-\t\t\t\tif (copy_skb)\n+\t\t\t\tif (copy_skb) {\n+\t\t\t\t\tmemset(&PACKET_SKB_CB(copy_skb)->sa.ll, 0,\n+\t\t\t\t\t       sizeof(PACKET_SKB_CB(copy_skb)->sa.ll));\n \t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n+\t\t\t\t}\n \t\t\t}\n \t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n \t\t\tif ((int)snaplen < 0) {",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tif (copy_skb) {",
                "\t\t\t\t\tmemset(&PACKET_SKB_CB(copy_skb)->sa.ll, 0,",
                "\t\t\t\t\t       sizeof(PACKET_SKB_CB(copy_skb)->sa.ll));",
                "\t\t\t\t}"
            ],
            "deleted": [
                "\t\t\t\tif (copy_skb)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In bdi_put and bdi_unregister of backing-dev.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-182815710References: Upstream kernel",
        "id": 3344
    },
    {
        "cve_id": "CVE-2023-3390",
        "code_before_change": "static int nf_tables_newrule(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t     const struct nlattr * const nla[])\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(info->net);\n\tstruct netlink_ext_ack *extack = info->extack;\n\tunsigned int size, i, n, ulen = 0, usize = 0;\n\tu8 genmask = nft_genmask_next(info->net);\n\tstruct nft_rule *rule, *old_rule = NULL;\n\tstruct nft_expr_info *expr_info = NULL;\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct nft_flow_rule *flow = NULL;\n\tstruct net *net = info->net;\n\tstruct nft_userdata *udata;\n\tstruct nft_table *table;\n\tstruct nft_chain *chain;\n\tstruct nft_trans *trans;\n\tu64 handle, pos_handle;\n\tstruct nft_expr *expr;\n\tstruct nft_ctx ctx;\n\tstruct nlattr *tmp;\n\tint err, rem;\n\n\tlockdep_assert_held(&nft_net->commit_mutex);\n\n\ttable = nft_table_lookup(net, nla[NFTA_RULE_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_RULE_CHAIN]) {\n\t\tchain = nft_chain_lookup(net, table, nla[NFTA_RULE_CHAIN],\n\t\t\t\t\t genmask);\n\t\tif (IS_ERR(chain)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\t\tif (nft_chain_is_bound(chain))\n\t\t\treturn -EOPNOTSUPP;\n\n\t} else if (nla[NFTA_RULE_CHAIN_ID]) {\n\t\tchain = nft_chain_lookup_byid(net, table, nla[NFTA_RULE_CHAIN_ID]);\n\t\tif (IS_ERR(chain)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN_ID]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\tif (nla[NFTA_RULE_HANDLE]) {\n\t\thandle = be64_to_cpu(nla_get_be64(nla[NFTA_RULE_HANDLE]));\n\t\trule = __nft_rule_lookup(chain, handle);\n\t\tif (IS_ERR(rule)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\treturn PTR_ERR(rule);\n\t\t}\n\n\t\tif (info->nlh->nlmsg_flags & NLM_F_EXCL) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\treturn -EEXIST;\n\t\t}\n\t\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\told_rule = rule;\n\t\telse\n\t\t\treturn -EOPNOTSUPP;\n\t} else {\n\t\tif (!(info->nlh->nlmsg_flags & NLM_F_CREATE) ||\n\t\t    info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\treturn -EINVAL;\n\t\thandle = nf_tables_alloc_handle(table);\n\n\t\tif (chain->use == UINT_MAX)\n\t\t\treturn -EOVERFLOW;\n\n\t\tif (nla[NFTA_RULE_POSITION]) {\n\t\t\tpos_handle = be64_to_cpu(nla_get_be64(nla[NFTA_RULE_POSITION]));\n\t\t\told_rule = __nft_rule_lookup(chain, pos_handle);\n\t\t\tif (IS_ERR(old_rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_POSITION]);\n\t\t\t\treturn PTR_ERR(old_rule);\n\t\t\t}\n\t\t} else if (nla[NFTA_RULE_POSITION_ID]) {\n\t\t\told_rule = nft_rule_lookup_byid(net, chain, nla[NFTA_RULE_POSITION_ID]);\n\t\t\tif (IS_ERR(old_rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_POSITION_ID]);\n\t\t\t\treturn PTR_ERR(old_rule);\n\t\t\t}\n\t\t}\n\t}\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tn = 0;\n\tsize = 0;\n\tif (nla[NFTA_RULE_EXPRESSIONS]) {\n\t\texpr_info = kvmalloc_array(NFT_RULE_MAXEXPRS,\n\t\t\t\t\t   sizeof(struct nft_expr_info),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!expr_info)\n\t\t\treturn -ENOMEM;\n\n\t\tnla_for_each_nested(tmp, nla[NFTA_RULE_EXPRESSIONS], rem) {\n\t\t\terr = -EINVAL;\n\t\t\tif (nla_type(tmp) != NFTA_LIST_ELEM)\n\t\t\t\tgoto err_release_expr;\n\t\t\tif (n == NFT_RULE_MAXEXPRS)\n\t\t\t\tgoto err_release_expr;\n\t\t\terr = nf_tables_expr_parse(&ctx, tmp, &expr_info[n]);\n\t\t\tif (err < 0) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, tmp);\n\t\t\t\tgoto err_release_expr;\n\t\t\t}\n\t\t\tsize += expr_info[n].ops->size;\n\t\t\tn++;\n\t\t}\n\t}\n\t/* Check for overflow of dlen field */\n\terr = -EFBIG;\n\tif (size >= 1 << 12)\n\t\tgoto err_release_expr;\n\n\tif (nla[NFTA_RULE_USERDATA]) {\n\t\tulen = nla_len(nla[NFTA_RULE_USERDATA]);\n\t\tif (ulen > 0)\n\t\t\tusize = sizeof(struct nft_userdata) + ulen;\n\t}\n\n\terr = -ENOMEM;\n\trule = kzalloc(sizeof(*rule) + size + usize, GFP_KERNEL_ACCOUNT);\n\tif (rule == NULL)\n\t\tgoto err_release_expr;\n\n\tnft_activate_next(net, rule);\n\n\trule->handle = handle;\n\trule->dlen   = size;\n\trule->udata  = ulen ? 1 : 0;\n\n\tif (ulen) {\n\t\tudata = nft_userdata(rule);\n\t\tudata->len = ulen - 1;\n\t\tnla_memcpy(udata->data, nla[NFTA_RULE_USERDATA], ulen);\n\t}\n\n\texpr = nft_expr_first(rule);\n\tfor (i = 0; i < n; i++) {\n\t\terr = nf_tables_newexpr(&ctx, &expr_info[i], expr);\n\t\tif (err < 0) {\n\t\t\tNL_SET_BAD_ATTR(extack, expr_info[i].attr);\n\t\t\tgoto err_release_rule;\n\t\t}\n\n\t\tif (expr_info[i].ops->validate)\n\t\t\tnft_validate_state_update(table, NFT_VALIDATE_NEED);\n\n\t\texpr_info[i].ops = NULL;\n\t\texpr = nft_expr_next(expr);\n\t}\n\n\tif (chain->flags & NFT_CHAIN_HW_OFFLOAD) {\n\t\tflow = nft_flow_rule_create(net, rule);\n\t\tif (IS_ERR(flow)) {\n\t\t\terr = PTR_ERR(flow);\n\t\t\tgoto err_release_rule;\n\t\t}\n\t}\n\n\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE) {\n\t\terr = nft_delrule(&ctx, old_rule);\n\t\tif (err < 0)\n\t\t\tgoto err_destroy_flow_rule;\n\n\t\ttrans = nft_trans_rule_add(&ctx, NFT_MSG_NEWRULE, rule);\n\t\tif (trans == NULL) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_destroy_flow_rule;\n\t\t}\n\t\tlist_add_tail_rcu(&rule->list, &old_rule->list);\n\t} else {\n\t\ttrans = nft_trans_rule_add(&ctx, NFT_MSG_NEWRULE, rule);\n\t\tif (!trans) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_destroy_flow_rule;\n\t\t}\n\n\t\tif (info->nlh->nlmsg_flags & NLM_F_APPEND) {\n\t\t\tif (old_rule)\n\t\t\t\tlist_add_rcu(&rule->list, &old_rule->list);\n\t\t\telse\n\t\t\t\tlist_add_tail_rcu(&rule->list, &chain->rules);\n\t\t } else {\n\t\t\tif (old_rule)\n\t\t\t\tlist_add_tail_rcu(&rule->list, &old_rule->list);\n\t\t\telse\n\t\t\t\tlist_add_rcu(&rule->list, &chain->rules);\n\t\t}\n\t}\n\tkvfree(expr_info);\n\tchain->use++;\n\n\tif (flow)\n\t\tnft_trans_flow_rule(trans) = flow;\n\n\tif (table->validate_state == NFT_VALIDATE_DO)\n\t\treturn nft_table_validate(net, table);\n\n\treturn 0;\n\nerr_destroy_flow_rule:\n\tif (flow)\n\t\tnft_flow_rule_destroy(flow);\nerr_release_rule:\n\tnf_tables_rule_release(&ctx, rule);\nerr_release_expr:\n\tfor (i = 0; i < n; i++) {\n\t\tif (expr_info[i].ops) {\n\t\t\tmodule_put(expr_info[i].ops->type->owner);\n\t\t\tif (expr_info[i].ops->type->release_ops)\n\t\t\t\texpr_info[i].ops->type->release_ops(expr_info[i].ops);\n\t\t}\n\t}\n\tkvfree(expr_info);\n\n\treturn err;\n}",
        "code_after_change": "static int nf_tables_newrule(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t     const struct nlattr * const nla[])\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(info->net);\n\tstruct netlink_ext_ack *extack = info->extack;\n\tunsigned int size, i, n, ulen = 0, usize = 0;\n\tu8 genmask = nft_genmask_next(info->net);\n\tstruct nft_rule *rule, *old_rule = NULL;\n\tstruct nft_expr_info *expr_info = NULL;\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct nft_flow_rule *flow = NULL;\n\tstruct net *net = info->net;\n\tstruct nft_userdata *udata;\n\tstruct nft_table *table;\n\tstruct nft_chain *chain;\n\tstruct nft_trans *trans;\n\tu64 handle, pos_handle;\n\tstruct nft_expr *expr;\n\tstruct nft_ctx ctx;\n\tstruct nlattr *tmp;\n\tint err, rem;\n\n\tlockdep_assert_held(&nft_net->commit_mutex);\n\n\ttable = nft_table_lookup(net, nla[NFTA_RULE_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_RULE_CHAIN]) {\n\t\tchain = nft_chain_lookup(net, table, nla[NFTA_RULE_CHAIN],\n\t\t\t\t\t genmask);\n\t\tif (IS_ERR(chain)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\t\tif (nft_chain_is_bound(chain))\n\t\t\treturn -EOPNOTSUPP;\n\n\t} else if (nla[NFTA_RULE_CHAIN_ID]) {\n\t\tchain = nft_chain_lookup_byid(net, table, nla[NFTA_RULE_CHAIN_ID]);\n\t\tif (IS_ERR(chain)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN_ID]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\tif (nla[NFTA_RULE_HANDLE]) {\n\t\thandle = be64_to_cpu(nla_get_be64(nla[NFTA_RULE_HANDLE]));\n\t\trule = __nft_rule_lookup(chain, handle);\n\t\tif (IS_ERR(rule)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\treturn PTR_ERR(rule);\n\t\t}\n\n\t\tif (info->nlh->nlmsg_flags & NLM_F_EXCL) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\treturn -EEXIST;\n\t\t}\n\t\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\told_rule = rule;\n\t\telse\n\t\t\treturn -EOPNOTSUPP;\n\t} else {\n\t\tif (!(info->nlh->nlmsg_flags & NLM_F_CREATE) ||\n\t\t    info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\treturn -EINVAL;\n\t\thandle = nf_tables_alloc_handle(table);\n\n\t\tif (chain->use == UINT_MAX)\n\t\t\treturn -EOVERFLOW;\n\n\t\tif (nla[NFTA_RULE_POSITION]) {\n\t\t\tpos_handle = be64_to_cpu(nla_get_be64(nla[NFTA_RULE_POSITION]));\n\t\t\told_rule = __nft_rule_lookup(chain, pos_handle);\n\t\t\tif (IS_ERR(old_rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_POSITION]);\n\t\t\t\treturn PTR_ERR(old_rule);\n\t\t\t}\n\t\t} else if (nla[NFTA_RULE_POSITION_ID]) {\n\t\t\told_rule = nft_rule_lookup_byid(net, chain, nla[NFTA_RULE_POSITION_ID]);\n\t\t\tif (IS_ERR(old_rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_POSITION_ID]);\n\t\t\t\treturn PTR_ERR(old_rule);\n\t\t\t}\n\t\t}\n\t}\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tn = 0;\n\tsize = 0;\n\tif (nla[NFTA_RULE_EXPRESSIONS]) {\n\t\texpr_info = kvmalloc_array(NFT_RULE_MAXEXPRS,\n\t\t\t\t\t   sizeof(struct nft_expr_info),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!expr_info)\n\t\t\treturn -ENOMEM;\n\n\t\tnla_for_each_nested(tmp, nla[NFTA_RULE_EXPRESSIONS], rem) {\n\t\t\terr = -EINVAL;\n\t\t\tif (nla_type(tmp) != NFTA_LIST_ELEM)\n\t\t\t\tgoto err_release_expr;\n\t\t\tif (n == NFT_RULE_MAXEXPRS)\n\t\t\t\tgoto err_release_expr;\n\t\t\terr = nf_tables_expr_parse(&ctx, tmp, &expr_info[n]);\n\t\t\tif (err < 0) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, tmp);\n\t\t\t\tgoto err_release_expr;\n\t\t\t}\n\t\t\tsize += expr_info[n].ops->size;\n\t\t\tn++;\n\t\t}\n\t}\n\t/* Check for overflow of dlen field */\n\terr = -EFBIG;\n\tif (size >= 1 << 12)\n\t\tgoto err_release_expr;\n\n\tif (nla[NFTA_RULE_USERDATA]) {\n\t\tulen = nla_len(nla[NFTA_RULE_USERDATA]);\n\t\tif (ulen > 0)\n\t\t\tusize = sizeof(struct nft_userdata) + ulen;\n\t}\n\n\terr = -ENOMEM;\n\trule = kzalloc(sizeof(*rule) + size + usize, GFP_KERNEL_ACCOUNT);\n\tif (rule == NULL)\n\t\tgoto err_release_expr;\n\n\tnft_activate_next(net, rule);\n\n\trule->handle = handle;\n\trule->dlen   = size;\n\trule->udata  = ulen ? 1 : 0;\n\n\tif (ulen) {\n\t\tudata = nft_userdata(rule);\n\t\tudata->len = ulen - 1;\n\t\tnla_memcpy(udata->data, nla[NFTA_RULE_USERDATA], ulen);\n\t}\n\n\texpr = nft_expr_first(rule);\n\tfor (i = 0; i < n; i++) {\n\t\terr = nf_tables_newexpr(&ctx, &expr_info[i], expr);\n\t\tif (err < 0) {\n\t\t\tNL_SET_BAD_ATTR(extack, expr_info[i].attr);\n\t\t\tgoto err_release_rule;\n\t\t}\n\n\t\tif (expr_info[i].ops->validate)\n\t\t\tnft_validate_state_update(table, NFT_VALIDATE_NEED);\n\n\t\texpr_info[i].ops = NULL;\n\t\texpr = nft_expr_next(expr);\n\t}\n\n\tif (chain->flags & NFT_CHAIN_HW_OFFLOAD) {\n\t\tflow = nft_flow_rule_create(net, rule);\n\t\tif (IS_ERR(flow)) {\n\t\t\terr = PTR_ERR(flow);\n\t\t\tgoto err_release_rule;\n\t\t}\n\t}\n\n\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE) {\n\t\terr = nft_delrule(&ctx, old_rule);\n\t\tif (err < 0)\n\t\t\tgoto err_destroy_flow_rule;\n\n\t\ttrans = nft_trans_rule_add(&ctx, NFT_MSG_NEWRULE, rule);\n\t\tif (trans == NULL) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_destroy_flow_rule;\n\t\t}\n\t\tlist_add_tail_rcu(&rule->list, &old_rule->list);\n\t} else {\n\t\ttrans = nft_trans_rule_add(&ctx, NFT_MSG_NEWRULE, rule);\n\t\tif (!trans) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_destroy_flow_rule;\n\t\t}\n\n\t\tif (info->nlh->nlmsg_flags & NLM_F_APPEND) {\n\t\t\tif (old_rule)\n\t\t\t\tlist_add_rcu(&rule->list, &old_rule->list);\n\t\t\telse\n\t\t\t\tlist_add_tail_rcu(&rule->list, &chain->rules);\n\t\t } else {\n\t\t\tif (old_rule)\n\t\t\t\tlist_add_tail_rcu(&rule->list, &old_rule->list);\n\t\t\telse\n\t\t\t\tlist_add_rcu(&rule->list, &chain->rules);\n\t\t}\n\t}\n\tkvfree(expr_info);\n\tchain->use++;\n\n\tif (flow)\n\t\tnft_trans_flow_rule(trans) = flow;\n\n\tif (table->validate_state == NFT_VALIDATE_DO)\n\t\treturn nft_table_validate(net, table);\n\n\treturn 0;\n\nerr_destroy_flow_rule:\n\tif (flow)\n\t\tnft_flow_rule_destroy(flow);\nerr_release_rule:\n\tnft_rule_expr_deactivate(&ctx, rule, NFT_TRANS_PREPARE);\n\tnf_tables_rule_destroy(&ctx, rule);\nerr_release_expr:\n\tfor (i = 0; i < n; i++) {\n\t\tif (expr_info[i].ops) {\n\t\t\tmodule_put(expr_info[i].ops->type->owner);\n\t\t\tif (expr_info[i].ops->type->release_ops)\n\t\t\t\texpr_info[i].ops->type->release_ops(expr_info[i].ops);\n\t\t}\n\t}\n\tkvfree(expr_info);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -212,7 +212,8 @@\n \tif (flow)\n \t\tnft_flow_rule_destroy(flow);\n err_release_rule:\n-\tnf_tables_rule_release(&ctx, rule);\n+\tnft_rule_expr_deactivate(&ctx, rule, NFT_TRANS_PREPARE);\n+\tnf_tables_rule_destroy(&ctx, rule);\n err_release_expr:\n \tfor (i = 0; i < n; i++) {\n \t\tif (expr_info[i].ops) {",
        "function_modified_lines": {
            "added": [
                "\tnft_rule_expr_deactivate(&ctx, rule, NFT_TRANS_PREPARE);",
                "\tnf_tables_rule_destroy(&ctx, rule);"
            ],
            "deleted": [
                "\tnf_tables_rule_release(&ctx, rule);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in the Linux kernel's netfilter subsystem in net/netfilter/nf_tables_api.c.\n\nMishandled error handling with NFT_MSG_NEWRULE makes it possible to use a dangling pointer in the same transaction causing a use-after-free vulnerability. This flaw allows a local attacker with user access to cause a privilege escalation issue.\n\nWe recommend upgrading past commit 1240eb93f0616b21c675416516ff3d74798fdc97.",
        "id": 4077
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "int\nsvm_range_restore_pages(struct amdgpu_device *adev, unsigned int pasid,\n\t\t\tuint64_t addr, bool write_fault)\n{\n\tstruct mm_struct *mm = NULL;\n\tstruct svm_range_list *svms;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\tktime_t timestamp = ktime_get_boottime();\n\tint32_t best_loc;\n\tint32_t gpuidx = MAX_GPU_INSTANCE;\n\tbool write_locked = false;\n\tstruct vm_area_struct *vma;\n\tbool migration = false;\n\tint r = 0;\n\n\tif (!KFD_IS_SVM_API_SUPPORTED(adev->kfd.dev)) {\n\t\tpr_debug(\"device does not support SVM\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tp = kfd_lookup_process_by_pasid(pasid);\n\tif (!p) {\n\t\tpr_debug(\"kfd process not founded pasid 0x%x\\n\", pasid);\n\t\treturn 0;\n\t}\n\tsvms = &p->svms;\n\n\tpr_debug(\"restoring svms 0x%p fault address 0x%llx\\n\", svms, addr);\n\n\tif (atomic_read(&svms->drain_pagefaults)) {\n\t\tpr_debug(\"draining retry fault, drop fault 0x%llx\\n\", addr);\n\t\tr = 0;\n\t\tgoto out;\n\t}\n\n\tif (!p->xnack_enabled) {\n\t\tpr_debug(\"XNACK not enabled for pasid 0x%x\\n\", pasid);\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* p->lead_thread is available as kfd_process_wq_release flush the work\n\t * before releasing task ref.\n\t */\n\tmm = get_task_mm(p->lead_thread);\n\tif (!mm) {\n\t\tpr_debug(\"svms 0x%p failed to get mm\\n\", svms);\n\t\tr = 0;\n\t\tgoto out;\n\t}\n\n\tmmap_read_lock(mm);\nretry_write_locked:\n\tmutex_lock(&svms->lock);\n\tprange = svm_range_from_addr(svms, addr, NULL);\n\tif (!prange) {\n\t\tpr_debug(\"failed to find prange svms 0x%p address [0x%llx]\\n\",\n\t\t\t svms, addr);\n\t\tif (!write_locked) {\n\t\t\t/* Need the write lock to create new range with MMU notifier.\n\t\t\t * Also flush pending deferred work to make sure the interval\n\t\t\t * tree is up to date before we add a new range\n\t\t\t */\n\t\t\tmutex_unlock(&svms->lock);\n\t\t\tmmap_read_unlock(mm);\n\t\t\tmmap_write_lock(mm);\n\t\t\twrite_locked = true;\n\t\t\tgoto retry_write_locked;\n\t\t}\n\t\tprange = svm_range_create_unregistered_range(adev, p, mm, addr);\n\t\tif (!prange) {\n\t\t\tpr_debug(\"failed to create unregistered range svms 0x%p address [0x%llx]\\n\",\n\t\t\t\t svms, addr);\n\t\t\tmmap_write_downgrade(mm);\n\t\t\tr = -EFAULT;\n\t\t\tgoto out_unlock_svms;\n\t\t}\n\t}\n\tif (write_locked)\n\t\tmmap_write_downgrade(mm);\n\n\tmutex_lock(&prange->migrate_mutex);\n\n\tif (svm_range_skip_recover(prange)) {\n\t\tamdgpu_gmc_filter_faults_remove(adev, addr, pasid);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\t/* skip duplicate vm fault on different pages of same range */\n\tif (ktime_before(timestamp, ktime_add_ns(prange->validate_timestamp,\n\t\t\t\tAMDGPU_SVM_RANGE_RETRY_FAULT_PENDING))) {\n\t\tpr_debug(\"svms 0x%p [0x%lx %lx] already restored\\n\",\n\t\t\t svms, prange->start, prange->last);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\t/* __do_munmap removed VMA, return success as we are handling stale\n\t * retry fault.\n\t */\n\tvma = find_vma(mm, addr << PAGE_SHIFT);\n\tif (!vma || (addr << PAGE_SHIFT) < vma->vm_start) {\n\t\tpr_debug(\"address 0x%llx VMA is removed\\n\", addr);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\tif (!svm_fault_allowed(vma, write_fault)) {\n\t\tpr_debug(\"fault addr 0x%llx no %s permission\\n\", addr,\n\t\t\twrite_fault ? \"write\" : \"read\");\n\t\tr = -EPERM;\n\t\tgoto out_unlock_range;\n\t}\n\n\tbest_loc = svm_range_best_restore_location(prange, adev, &gpuidx);\n\tif (best_loc == -1) {\n\t\tpr_debug(\"svms %p failed get best restore loc [0x%lx 0x%lx]\\n\",\n\t\t\t svms, prange->start, prange->last);\n\t\tr = -EACCES;\n\t\tgoto out_unlock_range;\n\t}\n\n\tpr_debug(\"svms %p [0x%lx 0x%lx] best restore 0x%x, actual loc 0x%x\\n\",\n\t\t svms, prange->start, prange->last, best_loc,\n\t\t prange->actual_loc);\n\n\tkfd_smi_event_page_fault_start(adev->kfd.dev, p->lead_thread->pid, addr,\n\t\t\t\t       write_fault, timestamp);\n\n\tif (prange->actual_loc != best_loc) {\n\t\tmigration = true;\n\t\tif (best_loc) {\n\t\t\tr = svm_migrate_to_vram(prange, best_loc, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n\t\t\tif (r) {\n\t\t\t\tpr_debug(\"svm_migrate_to_vram failed (%d) at %llx, falling back to system memory\\n\",\n\t\t\t\t\t r, addr);\n\t\t\t\t/* Fallback to system memory if migration to\n\t\t\t\t * VRAM failed\n\t\t\t\t */\n\t\t\t\tif (prange->actual_loc)\n\t\t\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n\t\t\t\telse\n\t\t\t\t\tr = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n\t\t}\n\t\tif (r) {\n\t\t\tpr_debug(\"failed %d to migrate svms %p [0x%lx 0x%lx]\\n\",\n\t\t\t\t r, svms, prange->start, prange->last);\n\t\t\tgoto out_unlock_range;\n\t\t}\n\t}\n\n\tr = svm_range_validate_and_map(mm, prange, gpuidx, false, false, false);\n\tif (r)\n\t\tpr_debug(\"failed %d to map svms 0x%p [0x%lx 0x%lx] to gpus\\n\",\n\t\t\t r, svms, prange->start, prange->last);\n\n\tkfd_smi_event_page_fault_end(adev->kfd.dev, p->lead_thread->pid, addr,\n\t\t\t\t     migration);\n\nout_unlock_range:\n\tmutex_unlock(&prange->migrate_mutex);\nout_unlock_svms:\n\tmutex_unlock(&svms->lock);\n\tmmap_read_unlock(mm);\n\n\tsvm_range_count_fault(adev, p, gpuidx);\n\n\tmmput(mm);\nout:\n\tkfd_unref_process(p);\n\n\tif (r == -EAGAIN) {\n\t\tpr_debug(\"recover vm fault later\\n\");\n\t\tamdgpu_gmc_filter_faults_remove(adev, addr, pasid);\n\t\tr = 0;\n\t}\n\treturn r;\n}",
        "code_after_change": "int\nsvm_range_restore_pages(struct amdgpu_device *adev, unsigned int pasid,\n\t\t\tuint64_t addr, bool write_fault)\n{\n\tstruct mm_struct *mm = NULL;\n\tstruct svm_range_list *svms;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\tktime_t timestamp = ktime_get_boottime();\n\tint32_t best_loc;\n\tint32_t gpuidx = MAX_GPU_INSTANCE;\n\tbool write_locked = false;\n\tstruct vm_area_struct *vma;\n\tbool migration = false;\n\tint r = 0;\n\n\tif (!KFD_IS_SVM_API_SUPPORTED(adev->kfd.dev)) {\n\t\tpr_debug(\"device does not support SVM\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tp = kfd_lookup_process_by_pasid(pasid);\n\tif (!p) {\n\t\tpr_debug(\"kfd process not founded pasid 0x%x\\n\", pasid);\n\t\treturn 0;\n\t}\n\tsvms = &p->svms;\n\n\tpr_debug(\"restoring svms 0x%p fault address 0x%llx\\n\", svms, addr);\n\n\tif (atomic_read(&svms->drain_pagefaults)) {\n\t\tpr_debug(\"draining retry fault, drop fault 0x%llx\\n\", addr);\n\t\tr = 0;\n\t\tgoto out;\n\t}\n\n\tif (!p->xnack_enabled) {\n\t\tpr_debug(\"XNACK not enabled for pasid 0x%x\\n\", pasid);\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* p->lead_thread is available as kfd_process_wq_release flush the work\n\t * before releasing task ref.\n\t */\n\tmm = get_task_mm(p->lead_thread);\n\tif (!mm) {\n\t\tpr_debug(\"svms 0x%p failed to get mm\\n\", svms);\n\t\tr = 0;\n\t\tgoto out;\n\t}\n\n\tmmap_read_lock(mm);\nretry_write_locked:\n\tmutex_lock(&svms->lock);\n\tprange = svm_range_from_addr(svms, addr, NULL);\n\tif (!prange) {\n\t\tpr_debug(\"failed to find prange svms 0x%p address [0x%llx]\\n\",\n\t\t\t svms, addr);\n\t\tif (!write_locked) {\n\t\t\t/* Need the write lock to create new range with MMU notifier.\n\t\t\t * Also flush pending deferred work to make sure the interval\n\t\t\t * tree is up to date before we add a new range\n\t\t\t */\n\t\t\tmutex_unlock(&svms->lock);\n\t\t\tmmap_read_unlock(mm);\n\t\t\tmmap_write_lock(mm);\n\t\t\twrite_locked = true;\n\t\t\tgoto retry_write_locked;\n\t\t}\n\t\tprange = svm_range_create_unregistered_range(adev, p, mm, addr);\n\t\tif (!prange) {\n\t\t\tpr_debug(\"failed to create unregistered range svms 0x%p address [0x%llx]\\n\",\n\t\t\t\t svms, addr);\n\t\t\tmmap_write_downgrade(mm);\n\t\t\tr = -EFAULT;\n\t\t\tgoto out_unlock_svms;\n\t\t}\n\t}\n\tif (write_locked)\n\t\tmmap_write_downgrade(mm);\n\n\tmutex_lock(&prange->migrate_mutex);\n\n\tif (svm_range_skip_recover(prange)) {\n\t\tamdgpu_gmc_filter_faults_remove(adev, addr, pasid);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\t/* skip duplicate vm fault on different pages of same range */\n\tif (ktime_before(timestamp, ktime_add_ns(prange->validate_timestamp,\n\t\t\t\tAMDGPU_SVM_RANGE_RETRY_FAULT_PENDING))) {\n\t\tpr_debug(\"svms 0x%p [0x%lx %lx] already restored\\n\",\n\t\t\t svms, prange->start, prange->last);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\t/* __do_munmap removed VMA, return success as we are handling stale\n\t * retry fault.\n\t */\n\tvma = find_vma(mm, addr << PAGE_SHIFT);\n\tif (!vma || (addr << PAGE_SHIFT) < vma->vm_start) {\n\t\tpr_debug(\"address 0x%llx VMA is removed\\n\", addr);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\tif (!svm_fault_allowed(vma, write_fault)) {\n\t\tpr_debug(\"fault addr 0x%llx no %s permission\\n\", addr,\n\t\t\twrite_fault ? \"write\" : \"read\");\n\t\tr = -EPERM;\n\t\tgoto out_unlock_range;\n\t}\n\n\tbest_loc = svm_range_best_restore_location(prange, adev, &gpuidx);\n\tif (best_loc == -1) {\n\t\tpr_debug(\"svms %p failed get best restore loc [0x%lx 0x%lx]\\n\",\n\t\t\t svms, prange->start, prange->last);\n\t\tr = -EACCES;\n\t\tgoto out_unlock_range;\n\t}\n\n\tpr_debug(\"svms %p [0x%lx 0x%lx] best restore 0x%x, actual loc 0x%x\\n\",\n\t\t svms, prange->start, prange->last, best_loc,\n\t\t prange->actual_loc);\n\n\tkfd_smi_event_page_fault_start(adev->kfd.dev, p->lead_thread->pid, addr,\n\t\t\t\t       write_fault, timestamp);\n\n\tif (prange->actual_loc != best_loc) {\n\t\tmigration = true;\n\t\tif (best_loc) {\n\t\t\tr = svm_migrate_to_vram(prange, best_loc, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n\t\t\tif (r) {\n\t\t\t\tpr_debug(\"svm_migrate_to_vram failed (%d) at %llx, falling back to system memory\\n\",\n\t\t\t\t\t r, addr);\n\t\t\t\t/* Fallback to system memory if migration to\n\t\t\t\t * VRAM failed\n\t\t\t\t */\n\t\t\t\tif (prange->actual_loc)\n\t\t\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,\n\t\t\t\t\t   NULL);\n\t\t\t\telse\n\t\t\t\t\tr = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,\n\t\t\t\t\tNULL);\n\t\t}\n\t\tif (r) {\n\t\t\tpr_debug(\"failed %d to migrate svms %p [0x%lx 0x%lx]\\n\",\n\t\t\t\t r, svms, prange->start, prange->last);\n\t\t\tgoto out_unlock_range;\n\t\t}\n\t}\n\n\tr = svm_range_validate_and_map(mm, prange, gpuidx, false, false, false);\n\tif (r)\n\t\tpr_debug(\"failed %d to map svms 0x%p [0x%lx 0x%lx] to gpus\\n\",\n\t\t\t r, svms, prange->start, prange->last);\n\n\tkfd_smi_event_page_fault_end(adev->kfd.dev, p->lead_thread->pid, addr,\n\t\t\t\t     migration);\n\nout_unlock_range:\n\tmutex_unlock(&prange->migrate_mutex);\nout_unlock_svms:\n\tmutex_unlock(&svms->lock);\n\tmmap_read_unlock(mm);\n\n\tsvm_range_count_fault(adev, p, gpuidx);\n\n\tmmput(mm);\nout:\n\tkfd_unref_process(p);\n\n\tif (r == -EAGAIN) {\n\t\tpr_debug(\"recover vm fault later\\n\");\n\t\tamdgpu_gmc_filter_faults_remove(adev, addr, pasid);\n\t\tr = 0;\n\t}\n\treturn r;\n}",
        "patch": "--- code before\n+++ code after\n@@ -142,13 +142,15 @@\n \t\t\t\t */\n \t\t\t\tif (prange->actual_loc)\n \t\t\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n-\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n+\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,\n+\t\t\t\t\t   NULL);\n \t\t\t\telse\n \t\t\t\t\tr = 0;\n \t\t\t}\n \t\t} else {\n \t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n-\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n+\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,\n+\t\t\t\t\tNULL);\n \t\t}\n \t\tif (r) {\n \t\t\tpr_debug(\"failed %d to migrate svms %p [0x%lx 0x%lx]\\n\",",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,",
                "\t\t\t\t\t   NULL);",
                "\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,",
                "\t\t\t\t\tNULL);"
            ],
            "deleted": [
                "\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);",
                "\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3614
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static void blk_add_trace_rq(struct request *rq, int error,\n\t\t\t     unsigned int nr_bytes, u32 what, u64 cgid)\n{\n\tstruct blk_trace *bt = rq->q->blk_trace;\n\n\tif (likely(!bt))\n\t\treturn;\n\n\tif (blk_rq_is_passthrough(rq))\n\t\twhat |= BLK_TC_ACT(BLK_TC_PC);\n\telse\n\t\twhat |= BLK_TC_ACT(BLK_TC_FS);\n\n\t__blk_add_trace(bt, blk_rq_trace_sector(rq), nr_bytes, req_op(rq),\n\t\t\trq->cmd_flags, what, error, 0, NULL, cgid);\n}",
        "code_after_change": "static void blk_add_trace_rq(struct request *rq, int error,\n\t\t\t     unsigned int nr_bytes, u32 what, u64 cgid)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(rq->q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tif (blk_rq_is_passthrough(rq))\n\t\twhat |= BLK_TC_ACT(BLK_TC_PC);\n\telse\n\t\twhat |= BLK_TC_ACT(BLK_TC_FS);\n\n\t__blk_add_trace(bt, blk_rq_trace_sector(rq), nr_bytes, req_op(rq),\n\t\t\trq->cmd_flags, what, error, 0, NULL, cgid);\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,14 @@\n static void blk_add_trace_rq(struct request *rq, int error,\n \t\t\t     unsigned int nr_bytes, u32 what, u64 cgid)\n {\n-\tstruct blk_trace *bt = rq->q->blk_trace;\n+\tstruct blk_trace *bt;\n \n-\tif (likely(!bt))\n+\trcu_read_lock();\n+\tbt = rcu_dereference(rq->q->blk_trace);\n+\tif (likely(!bt)) {\n+\t\trcu_read_unlock();\n \t\treturn;\n+\t}\n \n \tif (blk_rq_is_passthrough(rq))\n \t\twhat |= BLK_TC_ACT(BLK_TC_PC);\n@@ -13,4 +17,5 @@\n \n \t__blk_add_trace(bt, blk_rq_trace_sector(rq), nr_bytes, req_op(rq),\n \t\t\trq->cmd_flags, what, error, 0, NULL, cgid);\n+\trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct blk_trace *bt;",
                "\trcu_read_lock();",
                "\tbt = rcu_dereference(rq->q->blk_trace);",
                "\tif (likely(!bt)) {",
                "\t\trcu_read_unlock();",
                "\t}",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tstruct blk_trace *bt = rq->q->blk_trace;",
                "\tif (likely(!bt))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2226
    },
    {
        "cve_id": "CVE-2023-32233",
        "code_before_change": "void nf_tables_deactivate_set(const struct nft_ctx *ctx, struct nft_set *set,\n\t\t\t      struct nft_set_binding *binding,\n\t\t\t      enum nft_trans_phase phase)\n{\n\tswitch (phase) {\n\tcase NFT_TRANS_PREPARE:\n\t\tset->use--;\n\t\treturn;\n\tcase NFT_TRANS_ABORT:\n\tcase NFT_TRANS_RELEASE:\n\t\tset->use--;\n\t\tfallthrough;\n\tdefault:\n\t\tnf_tables_unbind_set(ctx, set, binding,\n\t\t\t\t     phase == NFT_TRANS_COMMIT);\n\t}\n}",
        "code_after_change": "void nf_tables_deactivate_set(const struct nft_ctx *ctx, struct nft_set *set,\n\t\t\t      struct nft_set_binding *binding,\n\t\t\t      enum nft_trans_phase phase)\n{\n\tswitch (phase) {\n\tcase NFT_TRANS_PREPARE:\n\t\tif (nft_set_is_anonymous(set))\n\t\t\tnft_deactivate_next(ctx->net, set);\n\n\t\tset->use--;\n\t\treturn;\n\tcase NFT_TRANS_ABORT:\n\tcase NFT_TRANS_RELEASE:\n\t\tset->use--;\n\t\tfallthrough;\n\tdefault:\n\t\tnf_tables_unbind_set(ctx, set, binding,\n\t\t\t\t     phase == NFT_TRANS_COMMIT);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,9 @@\n {\n \tswitch (phase) {\n \tcase NFT_TRANS_PREPARE:\n+\t\tif (nft_set_is_anonymous(set))\n+\t\t\tnft_deactivate_next(ctx->net, set);\n+\n \t\tset->use--;\n \t\treturn;\n \tcase NFT_TRANS_ABORT:",
        "function_modified_lines": {
            "added": [
                "\t\tif (nft_set_is_anonymous(set))",
                "\t\t\tnft_deactivate_next(ctx->net, set);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel through 6.3.1, a use-after-free in Netfilter nf_tables when processing batch requests can be abused to perform arbitrary read and write operations on kernel memory. Unprivileged local users can obtain root privileges. This occurs because anonymous sets are mishandled.",
        "id": 4006
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static int io_uring_alloc_task_context(struct task_struct *task,\n\t\t\t\t       struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx;\n\tint ret;\n\n\ttctx = kmalloc(sizeof(*tctx), GFP_KERNEL);\n\tif (unlikely(!tctx))\n\t\treturn -ENOMEM;\n\n\tret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);\n\tif (unlikely(ret)) {\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\ttctx->io_wq = io_init_wq_offload(ctx);\n\tif (IS_ERR(tctx->io_wq)) {\n\t\tret = PTR_ERR(tctx->io_wq);\n\t\tpercpu_counter_destroy(&tctx->inflight);\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\txa_init(&tctx->xa);\n\tinit_waitqueue_head(&tctx->wait);\n\ttctx->last = NULL;\n\tatomic_set(&tctx->in_idle, 0);\n\ttctx->sqpoll = false;\n\tio_init_identity(&tctx->__identity);\n\ttctx->identity = &tctx->__identity;\n\ttask->io_uring = tctx;\n\tspin_lock_init(&tctx->task_lock);\n\tINIT_WQ_LIST(&tctx->task_list);\n\ttctx->task_state = 0;\n\tinit_task_work(&tctx->task_work, tctx_task_work);\n\treturn 0;\n}",
        "code_after_change": "static int io_uring_alloc_task_context(struct task_struct *task,\n\t\t\t\t       struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx;\n\tint ret;\n\n\ttctx = kmalloc(sizeof(*tctx), GFP_KERNEL);\n\tif (unlikely(!tctx))\n\t\treturn -ENOMEM;\n\n\tret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);\n\tif (unlikely(ret)) {\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\ttctx->io_wq = io_init_wq_offload(ctx);\n\tif (IS_ERR(tctx->io_wq)) {\n\t\tret = PTR_ERR(tctx->io_wq);\n\t\tpercpu_counter_destroy(&tctx->inflight);\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\txa_init(&tctx->xa);\n\tinit_waitqueue_head(&tctx->wait);\n\ttctx->last = NULL;\n\tatomic_set(&tctx->in_idle, 0);\n\ttctx->sqpoll = false;\n\ttask->io_uring = tctx;\n\tspin_lock_init(&tctx->task_lock);\n\tINIT_WQ_LIST(&tctx->task_list);\n\ttctx->task_state = 0;\n\tinit_task_work(&tctx->task_work, tctx_task_work);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,8 +27,6 @@\n \ttctx->last = NULL;\n \tatomic_set(&tctx->in_idle, 0);\n \ttctx->sqpoll = false;\n-\tio_init_identity(&tctx->__identity);\n-\ttctx->identity = &tctx->__identity;\n \ttask->io_uring = tctx;\n \tspin_lock_init(&tctx->task_lock);\n \tINIT_WQ_LIST(&tctx->task_list);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tio_init_identity(&tctx->__identity);",
                "\ttctx->identity = &tctx->__identity;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3358
    },
    {
        "cve_id": "CVE-2018-20976",
        "code_before_change": "static long\nxfs_fs_nr_cached_objects(\n\tstruct super_block\t*sb,\n\tstruct shrink_control\t*sc)\n{\n\treturn xfs_reclaim_inodes_count(XFS_M(sb));\n}",
        "code_after_change": "static long\nxfs_fs_nr_cached_objects(\n\tstruct super_block\t*sb,\n\tstruct shrink_control\t*sc)\n{\n\t/* Paranoia: catch incorrect calls during mount setup or teardown */\n\tif (WARN_ON_ONCE(!sb->s_fs_info))\n\t\treturn 0;\n\treturn xfs_reclaim_inodes_count(XFS_M(sb));\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,5 +3,8 @@\n \tstruct super_block\t*sb,\n \tstruct shrink_control\t*sc)\n {\n+\t/* Paranoia: catch incorrect calls during mount setup or teardown */\n+\tif (WARN_ON_ONCE(!sb->s_fs_info))\n+\t\treturn 0;\n \treturn xfs_reclaim_inodes_count(XFS_M(sb));\n }",
        "function_modified_lines": {
            "added": [
                "\t/* Paranoia: catch incorrect calls during mount setup or teardown */",
                "\tif (WARN_ON_ONCE(!sb->s_fs_info))",
                "\t\treturn 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in fs/xfs/xfs_super.c in the Linux kernel before 4.18. A use after free exists, related to xfs_fs_fill_super failure.",
        "id": 1791
    },
    {
        "cve_id": "CVE-2019-19530",
        "code_before_change": "static int acm_probe(struct usb_interface *intf,\n\t\t     const struct usb_device_id *id)\n{\n\tstruct usb_cdc_union_desc *union_header = NULL;\n\tstruct usb_cdc_call_mgmt_descriptor *cmgmd = NULL;\n\tunsigned char *buffer = intf->altsetting->extra;\n\tint buflen = intf->altsetting->extralen;\n\tstruct usb_interface *control_interface;\n\tstruct usb_interface *data_interface;\n\tstruct usb_endpoint_descriptor *epctrl = NULL;\n\tstruct usb_endpoint_descriptor *epread = NULL;\n\tstruct usb_endpoint_descriptor *epwrite = NULL;\n\tstruct usb_device *usb_dev = interface_to_usbdev(intf);\n\tstruct usb_cdc_parsed_header h;\n\tstruct acm *acm;\n\tint minor;\n\tint ctrlsize, readsize;\n\tu8 *buf;\n\tint call_intf_num = -1;\n\tint data_intf_num = -1;\n\tunsigned long quirks;\n\tint num_rx_buf;\n\tint i;\n\tint combined_interfaces = 0;\n\tstruct device *tty_dev;\n\tint rv = -ENOMEM;\n\tint res;\n\n\t/* normal quirks */\n\tquirks = (unsigned long)id->driver_info;\n\n\tif (quirks == IGNORE_DEVICE)\n\t\treturn -ENODEV;\n\n\tmemset(&h, 0x00, sizeof(struct usb_cdc_parsed_header));\n\n\tnum_rx_buf = (quirks == SINGLE_RX_URB) ? 1 : ACM_NR;\n\n\t/* handle quirks deadly to normal probing*/\n\tif (quirks == NO_UNION_NORMAL) {\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, 1);\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\t/* we would crash */\n\t\tif (!data_interface || !control_interface)\n\t\t\treturn -ENODEV;\n\t\tgoto skip_normal_probe;\n\t}\n\n\t/* normal probing*/\n\tif (!buffer) {\n\t\tdev_err(&intf->dev, \"Weird descriptor references\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!intf->cur_altsetting)\n\t\treturn -EINVAL;\n\n\tif (!buflen) {\n\t\tif (intf->cur_altsetting->endpoint &&\n\t\t\t\tintf->cur_altsetting->endpoint->extralen &&\n\t\t\t\tintf->cur_altsetting->endpoint->extra) {\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Seeking extra descriptors on endpoint\\n\");\n\t\t\tbuflen = intf->cur_altsetting->endpoint->extralen;\n\t\t\tbuffer = intf->cur_altsetting->endpoint->extra;\n\t\t} else {\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"Zero length descriptor references\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tcdc_parse_cdc_header(&h, intf, buffer, buflen);\n\tunion_header = h.usb_cdc_union_desc;\n\tcmgmd = h.usb_cdc_call_mgmt_descriptor;\n\tif (cmgmd)\n\t\tcall_intf_num = cmgmd->bDataInterface;\n\n\tif (!union_header) {\n\t\tif (call_intf_num > 0) {\n\t\t\tdev_dbg(&intf->dev, \"No union descriptor, using call management descriptor\\n\");\n\t\t\t/* quirks for Droids MuIn LCD */\n\t\t\tif (quirks & NO_DATA_INTERFACE) {\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\t\t} else {\n\t\t\t\tdata_intf_num = call_intf_num;\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, data_intf_num);\n\t\t\t}\n\t\t\tcontrol_interface = intf;\n\t\t} else {\n\t\t\tif (intf->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\t\tdev_dbg(&intf->dev,\"No union descriptor, giving up\\n\");\n\t\t\t\treturn -ENODEV;\n\t\t\t} else {\n\t\t\t\tdev_warn(&intf->dev,\"No union descriptor, testing for castrated device\\n\");\n\t\t\t\tcombined_interfaces = 1;\n\t\t\t\tcontrol_interface = data_interface = intf;\n\t\t\t\tgoto look_for_collapsed_interface;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tdata_intf_num = union_header->bSlaveInterface0;\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, union_header->bMasterInterface0);\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, data_intf_num);\n\t}\n\n\tif (!control_interface || !data_interface) {\n\t\tdev_dbg(&intf->dev, \"no interfaces\\n\");\n\t\treturn -ENODEV;\n\t}\n\tif (!data_interface->cur_altsetting || !control_interface->cur_altsetting)\n\t\treturn -ENODEV;\n\n\tif (data_intf_num != call_intf_num)\n\t\tdev_dbg(&intf->dev, \"Separate call control interface. That is not fully supported.\\n\");\n\n\tif (control_interface == data_interface) {\n\t\t/* some broken devices designed for windows work this way */\n\t\tdev_warn(&intf->dev,\"Control and data interfaces are not separated!\\n\");\n\t\tcombined_interfaces = 1;\n\t\t/* a popular other OS doesn't use it */\n\t\tquirks |= NO_CAP_LINE;\n\t\tif (data_interface->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\tdev_err(&intf->dev, \"This needs exactly 3 endpoints\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\nlook_for_collapsed_interface:\n\t\tres = usb_find_common_endpoints(data_interface->cur_altsetting,\n\t\t\t\t&epread, &epwrite, &epctrl, NULL);\n\t\tif (res)\n\t\t\treturn res;\n\n\t\tgoto made_compressed_probe;\n\t}\n\nskip_normal_probe:\n\n\t/*workaround for switched interfaces */\n\tif (data_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t!= CDC_DATA_INTERFACE_TYPE) {\n\t\tif (control_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t== CDC_DATA_INTERFACE_TYPE) {\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Your device has switched interfaces.\\n\");\n\t\t\tswap(control_interface, data_interface);\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* Accept probe requests only for the control interface */\n\tif (!combined_interfaces && intf != control_interface)\n\t\treturn -ENODEV;\n\n\tif (!combined_interfaces && usb_interface_claimed(data_interface)) {\n\t\t/* valid in this context */\n\t\tdev_dbg(&intf->dev, \"The data interface isn't available\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\n\tif (data_interface->cur_altsetting->desc.bNumEndpoints < 2 ||\n\t    control_interface->cur_altsetting->desc.bNumEndpoints == 0)\n\t\treturn -EINVAL;\n\n\tepctrl = &control_interface->cur_altsetting->endpoint[0].desc;\n\tepread = &data_interface->cur_altsetting->endpoint[0].desc;\n\tepwrite = &data_interface->cur_altsetting->endpoint[1].desc;\n\n\n\t/* workaround for switched endpoints */\n\tif (!usb_endpoint_dir_in(epread)) {\n\t\t/* descriptors are swapped */\n\t\tdev_dbg(&intf->dev,\n\t\t\t\"The data interface has switched endpoints\\n\");\n\t\tswap(epread, epwrite);\n\t}\nmade_compressed_probe:\n\tdev_dbg(&intf->dev, \"interfaces are valid\\n\");\n\n\tacm = kzalloc(sizeof(struct acm), GFP_KERNEL);\n\tif (acm == NULL)\n\t\tgoto alloc_fail;\n\n\ttty_port_init(&acm->port);\n\tacm->port.ops = &acm_port_ops;\n\n\tminor = acm_alloc_minor(acm);\n\tif (minor < 0)\n\t\tgoto alloc_fail1;\n\n\tctrlsize = usb_endpoint_maxp(epctrl);\n\treadsize = usb_endpoint_maxp(epread) *\n\t\t\t\t(quirks == SINGLE_RX_URB ? 1 : 2);\n\tacm->combined_interfaces = combined_interfaces;\n\tacm->writesize = usb_endpoint_maxp(epwrite) * 20;\n\tacm->control = control_interface;\n\tacm->data = data_interface;\n\tacm->minor = minor;\n\tacm->dev = usb_dev;\n\tif (h.usb_cdc_acm_descriptor)\n\t\tacm->ctrl_caps = h.usb_cdc_acm_descriptor->bmCapabilities;\n\tif (quirks & NO_CAP_LINE)\n\t\tacm->ctrl_caps &= ~USB_CDC_CAP_LINE;\n\tacm->ctrlsize = ctrlsize;\n\tacm->readsize = readsize;\n\tacm->rx_buflimit = num_rx_buf;\n\tINIT_WORK(&acm->work, acm_softint);\n\tinit_waitqueue_head(&acm->wioctl);\n\tspin_lock_init(&acm->write_lock);\n\tspin_lock_init(&acm->read_lock);\n\tmutex_init(&acm->mutex);\n\tif (usb_endpoint_xfer_int(epread)) {\n\t\tacm->bInterval = epread->bInterval;\n\t\tacm->in = usb_rcvintpipe(usb_dev, epread->bEndpointAddress);\n\t} else {\n\t\tacm->in = usb_rcvbulkpipe(usb_dev, epread->bEndpointAddress);\n\t}\n\tif (usb_endpoint_xfer_int(epwrite))\n\t\tacm->out = usb_sndintpipe(usb_dev, epwrite->bEndpointAddress);\n\telse\n\t\tacm->out = usb_sndbulkpipe(usb_dev, epwrite->bEndpointAddress);\n\tinit_usb_anchor(&acm->delayed);\n\tacm->quirks = quirks;\n\n\tbuf = usb_alloc_coherent(usb_dev, ctrlsize, GFP_KERNEL, &acm->ctrl_dma);\n\tif (!buf)\n\t\tgoto alloc_fail1;\n\tacm->ctrl_buffer = buf;\n\n\tif (acm_write_buffers_alloc(acm) < 0)\n\t\tgoto alloc_fail2;\n\n\tacm->ctrlurb = usb_alloc_urb(0, GFP_KERNEL);\n\tif (!acm->ctrlurb)\n\t\tgoto alloc_fail3;\n\n\tfor (i = 0; i < num_rx_buf; i++) {\n\t\tstruct acm_rb *rb = &(acm->read_buffers[i]);\n\t\tstruct urb *urb;\n\n\t\trb->base = usb_alloc_coherent(acm->dev, readsize, GFP_KERNEL,\n\t\t\t\t\t\t\t\t&rb->dma);\n\t\tif (!rb->base)\n\t\t\tgoto alloc_fail4;\n\t\trb->index = i;\n\t\trb->instance = acm;\n\n\t\turb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (!urb)\n\t\t\tgoto alloc_fail4;\n\n\t\turb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\turb->transfer_dma = rb->dma;\n\t\tif (usb_endpoint_xfer_int(epread))\n\t\t\tusb_fill_int_urb(urb, acm->dev, acm->in, rb->base,\n\t\t\t\t\t acm->readsize,\n\t\t\t\t\t acm_read_bulk_callback, rb,\n\t\t\t\t\t acm->bInterval);\n\t\telse\n\t\t\tusb_fill_bulk_urb(urb, acm->dev, acm->in, rb->base,\n\t\t\t\t\t  acm->readsize,\n\t\t\t\t\t  acm_read_bulk_callback, rb);\n\n\t\tacm->read_urbs[i] = urb;\n\t\t__set_bit(i, &acm->read_urbs_free);\n\t}\n\tfor (i = 0; i < ACM_NW; i++) {\n\t\tstruct acm_wb *snd = &(acm->wb[i]);\n\n\t\tsnd->urb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (snd->urb == NULL)\n\t\t\tgoto alloc_fail5;\n\n\t\tif (usb_endpoint_xfer_int(epwrite))\n\t\t\tusb_fill_int_urb(snd->urb, usb_dev, acm->out,\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd, epwrite->bInterval);\n\t\telse\n\t\t\tusb_fill_bulk_urb(snd->urb, usb_dev, acm->out,\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd);\n\t\tsnd->urb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\tif (quirks & SEND_ZERO_PACKET)\n\t\t\tsnd->urb->transfer_flags |= URB_ZERO_PACKET;\n\t\tsnd->instance = acm;\n\t}\n\n\tusb_set_intfdata(intf, acm);\n\n\ti = device_create_file(&intf->dev, &dev_attr_bmCapabilities);\n\tif (i < 0)\n\t\tgoto alloc_fail5;\n\n\tif (h.usb_cdc_country_functional_desc) { /* export the country data */\n\t\tstruct usb_cdc_country_functional_desc * cfd =\n\t\t\t\t\th.usb_cdc_country_functional_desc;\n\n\t\tacm->country_codes = kmalloc(cfd->bLength - 4, GFP_KERNEL);\n\t\tif (!acm->country_codes)\n\t\t\tgoto skip_countries;\n\t\tacm->country_code_size = cfd->bLength - 4;\n\t\tmemcpy(acm->country_codes, (u8 *)&cfd->wCountyCode0,\n\t\t\t\t\t\t\tcfd->bLength - 4);\n\t\tacm->country_rel_date = cfd->iCountryCodeRelDate;\n\n\t\ti = device_create_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\tif (i < 0) {\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\n\t\ti = device_create_file(&intf->dev,\n\t\t\t\t\t\t&dev_attr_iCountryCodeRelDate);\n\t\tif (i < 0) {\n\t\t\tdevice_remove_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\t}\n\nskip_countries:\n\tusb_fill_int_urb(acm->ctrlurb, usb_dev,\n\t\t\t usb_rcvintpipe(usb_dev, epctrl->bEndpointAddress),\n\t\t\t acm->ctrl_buffer, ctrlsize, acm_ctrl_irq, acm,\n\t\t\t /* works around buggy devices */\n\t\t\t epctrl->bInterval ? epctrl->bInterval : 16);\n\tacm->ctrlurb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\tacm->ctrlurb->transfer_dma = acm->ctrl_dma;\n\tacm->notification_buffer = NULL;\n\tacm->nb_index = 0;\n\tacm->nb_size = 0;\n\n\tdev_info(&intf->dev, \"ttyACM%d: USB ACM device\\n\", minor);\n\n\tacm->line.dwDTERate = cpu_to_le32(9600);\n\tacm->line.bDataBits = 8;\n\tacm_set_line(acm, &acm->line);\n\n\tusb_driver_claim_interface(&acm_driver, data_interface, acm);\n\tusb_set_intfdata(data_interface, acm);\n\n\tusb_get_intf(control_interface);\n\ttty_dev = tty_port_register_device(&acm->port, acm_tty_driver, minor,\n\t\t\t&control_interface->dev);\n\tif (IS_ERR(tty_dev)) {\n\t\trv = PTR_ERR(tty_dev);\n\t\tgoto alloc_fail6;\n\t}\n\n\tif (quirks & CLEAR_HALT_CONDITIONS) {\n\t\tusb_clear_halt(usb_dev, acm->in);\n\t\tusb_clear_halt(usb_dev, acm->out);\n\t}\n\n\treturn 0;\nalloc_fail6:\n\tif (acm->country_codes) {\n\t\tdevice_remove_file(&acm->control->dev,\n\t\t\t\t&dev_attr_wCountryCodes);\n\t\tdevice_remove_file(&acm->control->dev,\n\t\t\t\t&dev_attr_iCountryCodeRelDate);\n\t\tkfree(acm->country_codes);\n\t}\n\tdevice_remove_file(&acm->control->dev, &dev_attr_bmCapabilities);\nalloc_fail5:\n\tusb_set_intfdata(intf, NULL);\n\tfor (i = 0; i < ACM_NW; i++)\n\t\tusb_free_urb(acm->wb[i].urb);\nalloc_fail4:\n\tfor (i = 0; i < num_rx_buf; i++)\n\t\tusb_free_urb(acm->read_urbs[i]);\n\tacm_read_buffers_free(acm);\n\tusb_free_urb(acm->ctrlurb);\nalloc_fail3:\n\tacm_write_buffers_free(acm);\nalloc_fail2:\n\tusb_free_coherent(usb_dev, ctrlsize, acm->ctrl_buffer, acm->ctrl_dma);\nalloc_fail1:\n\ttty_port_put(&acm->port);\nalloc_fail:\n\treturn rv;\n}",
        "code_after_change": "static int acm_probe(struct usb_interface *intf,\n\t\t     const struct usb_device_id *id)\n{\n\tstruct usb_cdc_union_desc *union_header = NULL;\n\tstruct usb_cdc_call_mgmt_descriptor *cmgmd = NULL;\n\tunsigned char *buffer = intf->altsetting->extra;\n\tint buflen = intf->altsetting->extralen;\n\tstruct usb_interface *control_interface;\n\tstruct usb_interface *data_interface;\n\tstruct usb_endpoint_descriptor *epctrl = NULL;\n\tstruct usb_endpoint_descriptor *epread = NULL;\n\tstruct usb_endpoint_descriptor *epwrite = NULL;\n\tstruct usb_device *usb_dev = interface_to_usbdev(intf);\n\tstruct usb_cdc_parsed_header h;\n\tstruct acm *acm;\n\tint minor;\n\tint ctrlsize, readsize;\n\tu8 *buf;\n\tint call_intf_num = -1;\n\tint data_intf_num = -1;\n\tunsigned long quirks;\n\tint num_rx_buf;\n\tint i;\n\tint combined_interfaces = 0;\n\tstruct device *tty_dev;\n\tint rv = -ENOMEM;\n\tint res;\n\n\t/* normal quirks */\n\tquirks = (unsigned long)id->driver_info;\n\n\tif (quirks == IGNORE_DEVICE)\n\t\treturn -ENODEV;\n\n\tmemset(&h, 0x00, sizeof(struct usb_cdc_parsed_header));\n\n\tnum_rx_buf = (quirks == SINGLE_RX_URB) ? 1 : ACM_NR;\n\n\t/* handle quirks deadly to normal probing*/\n\tif (quirks == NO_UNION_NORMAL) {\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, 1);\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\t/* we would crash */\n\t\tif (!data_interface || !control_interface)\n\t\t\treturn -ENODEV;\n\t\tgoto skip_normal_probe;\n\t}\n\n\t/* normal probing*/\n\tif (!buffer) {\n\t\tdev_err(&intf->dev, \"Weird descriptor references\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!intf->cur_altsetting)\n\t\treturn -EINVAL;\n\n\tif (!buflen) {\n\t\tif (intf->cur_altsetting->endpoint &&\n\t\t\t\tintf->cur_altsetting->endpoint->extralen &&\n\t\t\t\tintf->cur_altsetting->endpoint->extra) {\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Seeking extra descriptors on endpoint\\n\");\n\t\t\tbuflen = intf->cur_altsetting->endpoint->extralen;\n\t\t\tbuffer = intf->cur_altsetting->endpoint->extra;\n\t\t} else {\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"Zero length descriptor references\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tcdc_parse_cdc_header(&h, intf, buffer, buflen);\n\tunion_header = h.usb_cdc_union_desc;\n\tcmgmd = h.usb_cdc_call_mgmt_descriptor;\n\tif (cmgmd)\n\t\tcall_intf_num = cmgmd->bDataInterface;\n\n\tif (!union_header) {\n\t\tif (call_intf_num > 0) {\n\t\t\tdev_dbg(&intf->dev, \"No union descriptor, using call management descriptor\\n\");\n\t\t\t/* quirks for Droids MuIn LCD */\n\t\t\tif (quirks & NO_DATA_INTERFACE) {\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\t\t} else {\n\t\t\t\tdata_intf_num = call_intf_num;\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, data_intf_num);\n\t\t\t}\n\t\t\tcontrol_interface = intf;\n\t\t} else {\n\t\t\tif (intf->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\t\tdev_dbg(&intf->dev,\"No union descriptor, giving up\\n\");\n\t\t\t\treturn -ENODEV;\n\t\t\t} else {\n\t\t\t\tdev_warn(&intf->dev,\"No union descriptor, testing for castrated device\\n\");\n\t\t\t\tcombined_interfaces = 1;\n\t\t\t\tcontrol_interface = data_interface = intf;\n\t\t\t\tgoto look_for_collapsed_interface;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tdata_intf_num = union_header->bSlaveInterface0;\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, union_header->bMasterInterface0);\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, data_intf_num);\n\t}\n\n\tif (!control_interface || !data_interface) {\n\t\tdev_dbg(&intf->dev, \"no interfaces\\n\");\n\t\treturn -ENODEV;\n\t}\n\tif (!data_interface->cur_altsetting || !control_interface->cur_altsetting)\n\t\treturn -ENODEV;\n\n\tif (data_intf_num != call_intf_num)\n\t\tdev_dbg(&intf->dev, \"Separate call control interface. That is not fully supported.\\n\");\n\n\tif (control_interface == data_interface) {\n\t\t/* some broken devices designed for windows work this way */\n\t\tdev_warn(&intf->dev,\"Control and data interfaces are not separated!\\n\");\n\t\tcombined_interfaces = 1;\n\t\t/* a popular other OS doesn't use it */\n\t\tquirks |= NO_CAP_LINE;\n\t\tif (data_interface->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\tdev_err(&intf->dev, \"This needs exactly 3 endpoints\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\nlook_for_collapsed_interface:\n\t\tres = usb_find_common_endpoints(data_interface->cur_altsetting,\n\t\t\t\t&epread, &epwrite, &epctrl, NULL);\n\t\tif (res)\n\t\t\treturn res;\n\n\t\tgoto made_compressed_probe;\n\t}\n\nskip_normal_probe:\n\n\t/*workaround for switched interfaces */\n\tif (data_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t!= CDC_DATA_INTERFACE_TYPE) {\n\t\tif (control_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t== CDC_DATA_INTERFACE_TYPE) {\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Your device has switched interfaces.\\n\");\n\t\t\tswap(control_interface, data_interface);\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* Accept probe requests only for the control interface */\n\tif (!combined_interfaces && intf != control_interface)\n\t\treturn -ENODEV;\n\n\tif (!combined_interfaces && usb_interface_claimed(data_interface)) {\n\t\t/* valid in this context */\n\t\tdev_dbg(&intf->dev, \"The data interface isn't available\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\n\tif (data_interface->cur_altsetting->desc.bNumEndpoints < 2 ||\n\t    control_interface->cur_altsetting->desc.bNumEndpoints == 0)\n\t\treturn -EINVAL;\n\n\tepctrl = &control_interface->cur_altsetting->endpoint[0].desc;\n\tepread = &data_interface->cur_altsetting->endpoint[0].desc;\n\tepwrite = &data_interface->cur_altsetting->endpoint[1].desc;\n\n\n\t/* workaround for switched endpoints */\n\tif (!usb_endpoint_dir_in(epread)) {\n\t\t/* descriptors are swapped */\n\t\tdev_dbg(&intf->dev,\n\t\t\t\"The data interface has switched endpoints\\n\");\n\t\tswap(epread, epwrite);\n\t}\nmade_compressed_probe:\n\tdev_dbg(&intf->dev, \"interfaces are valid\\n\");\n\n\tacm = kzalloc(sizeof(struct acm), GFP_KERNEL);\n\tif (acm == NULL)\n\t\tgoto alloc_fail;\n\n\ttty_port_init(&acm->port);\n\tacm->port.ops = &acm_port_ops;\n\n\tctrlsize = usb_endpoint_maxp(epctrl);\n\treadsize = usb_endpoint_maxp(epread) *\n\t\t\t\t(quirks == SINGLE_RX_URB ? 1 : 2);\n\tacm->combined_interfaces = combined_interfaces;\n\tacm->writesize = usb_endpoint_maxp(epwrite) * 20;\n\tacm->control = control_interface;\n\tacm->data = data_interface;\n\n\tusb_get_intf(acm->control); /* undone in destruct() */\n\n\tminor = acm_alloc_minor(acm);\n\tif (minor < 0)\n\t\tgoto alloc_fail1;\n\n\tacm->minor = minor;\n\tacm->dev = usb_dev;\n\tif (h.usb_cdc_acm_descriptor)\n\t\tacm->ctrl_caps = h.usb_cdc_acm_descriptor->bmCapabilities;\n\tif (quirks & NO_CAP_LINE)\n\t\tacm->ctrl_caps &= ~USB_CDC_CAP_LINE;\n\tacm->ctrlsize = ctrlsize;\n\tacm->readsize = readsize;\n\tacm->rx_buflimit = num_rx_buf;\n\tINIT_WORK(&acm->work, acm_softint);\n\tinit_waitqueue_head(&acm->wioctl);\n\tspin_lock_init(&acm->write_lock);\n\tspin_lock_init(&acm->read_lock);\n\tmutex_init(&acm->mutex);\n\tif (usb_endpoint_xfer_int(epread)) {\n\t\tacm->bInterval = epread->bInterval;\n\t\tacm->in = usb_rcvintpipe(usb_dev, epread->bEndpointAddress);\n\t} else {\n\t\tacm->in = usb_rcvbulkpipe(usb_dev, epread->bEndpointAddress);\n\t}\n\tif (usb_endpoint_xfer_int(epwrite))\n\t\tacm->out = usb_sndintpipe(usb_dev, epwrite->bEndpointAddress);\n\telse\n\t\tacm->out = usb_sndbulkpipe(usb_dev, epwrite->bEndpointAddress);\n\tinit_usb_anchor(&acm->delayed);\n\tacm->quirks = quirks;\n\n\tbuf = usb_alloc_coherent(usb_dev, ctrlsize, GFP_KERNEL, &acm->ctrl_dma);\n\tif (!buf)\n\t\tgoto alloc_fail1;\n\tacm->ctrl_buffer = buf;\n\n\tif (acm_write_buffers_alloc(acm) < 0)\n\t\tgoto alloc_fail2;\n\n\tacm->ctrlurb = usb_alloc_urb(0, GFP_KERNEL);\n\tif (!acm->ctrlurb)\n\t\tgoto alloc_fail3;\n\n\tfor (i = 0; i < num_rx_buf; i++) {\n\t\tstruct acm_rb *rb = &(acm->read_buffers[i]);\n\t\tstruct urb *urb;\n\n\t\trb->base = usb_alloc_coherent(acm->dev, readsize, GFP_KERNEL,\n\t\t\t\t\t\t\t\t&rb->dma);\n\t\tif (!rb->base)\n\t\t\tgoto alloc_fail4;\n\t\trb->index = i;\n\t\trb->instance = acm;\n\n\t\turb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (!urb)\n\t\t\tgoto alloc_fail4;\n\n\t\turb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\turb->transfer_dma = rb->dma;\n\t\tif (usb_endpoint_xfer_int(epread))\n\t\t\tusb_fill_int_urb(urb, acm->dev, acm->in, rb->base,\n\t\t\t\t\t acm->readsize,\n\t\t\t\t\t acm_read_bulk_callback, rb,\n\t\t\t\t\t acm->bInterval);\n\t\telse\n\t\t\tusb_fill_bulk_urb(urb, acm->dev, acm->in, rb->base,\n\t\t\t\t\t  acm->readsize,\n\t\t\t\t\t  acm_read_bulk_callback, rb);\n\n\t\tacm->read_urbs[i] = urb;\n\t\t__set_bit(i, &acm->read_urbs_free);\n\t}\n\tfor (i = 0; i < ACM_NW; i++) {\n\t\tstruct acm_wb *snd = &(acm->wb[i]);\n\n\t\tsnd->urb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (snd->urb == NULL)\n\t\t\tgoto alloc_fail5;\n\n\t\tif (usb_endpoint_xfer_int(epwrite))\n\t\t\tusb_fill_int_urb(snd->urb, usb_dev, acm->out,\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd, epwrite->bInterval);\n\t\telse\n\t\t\tusb_fill_bulk_urb(snd->urb, usb_dev, acm->out,\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd);\n\t\tsnd->urb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\tif (quirks & SEND_ZERO_PACKET)\n\t\t\tsnd->urb->transfer_flags |= URB_ZERO_PACKET;\n\t\tsnd->instance = acm;\n\t}\n\n\tusb_set_intfdata(intf, acm);\n\n\ti = device_create_file(&intf->dev, &dev_attr_bmCapabilities);\n\tif (i < 0)\n\t\tgoto alloc_fail5;\n\n\tif (h.usb_cdc_country_functional_desc) { /* export the country data */\n\t\tstruct usb_cdc_country_functional_desc * cfd =\n\t\t\t\t\th.usb_cdc_country_functional_desc;\n\n\t\tacm->country_codes = kmalloc(cfd->bLength - 4, GFP_KERNEL);\n\t\tif (!acm->country_codes)\n\t\t\tgoto skip_countries;\n\t\tacm->country_code_size = cfd->bLength - 4;\n\t\tmemcpy(acm->country_codes, (u8 *)&cfd->wCountyCode0,\n\t\t\t\t\t\t\tcfd->bLength - 4);\n\t\tacm->country_rel_date = cfd->iCountryCodeRelDate;\n\n\t\ti = device_create_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\tif (i < 0) {\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\n\t\ti = device_create_file(&intf->dev,\n\t\t\t\t\t\t&dev_attr_iCountryCodeRelDate);\n\t\tif (i < 0) {\n\t\t\tdevice_remove_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\t}\n\nskip_countries:\n\tusb_fill_int_urb(acm->ctrlurb, usb_dev,\n\t\t\t usb_rcvintpipe(usb_dev, epctrl->bEndpointAddress),\n\t\t\t acm->ctrl_buffer, ctrlsize, acm_ctrl_irq, acm,\n\t\t\t /* works around buggy devices */\n\t\t\t epctrl->bInterval ? epctrl->bInterval : 16);\n\tacm->ctrlurb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\tacm->ctrlurb->transfer_dma = acm->ctrl_dma;\n\tacm->notification_buffer = NULL;\n\tacm->nb_index = 0;\n\tacm->nb_size = 0;\n\n\tdev_info(&intf->dev, \"ttyACM%d: USB ACM device\\n\", minor);\n\n\tacm->line.dwDTERate = cpu_to_le32(9600);\n\tacm->line.bDataBits = 8;\n\tacm_set_line(acm, &acm->line);\n\n\tusb_driver_claim_interface(&acm_driver, data_interface, acm);\n\tusb_set_intfdata(data_interface, acm);\n\n\ttty_dev = tty_port_register_device(&acm->port, acm_tty_driver, minor,\n\t\t\t&control_interface->dev);\n\tif (IS_ERR(tty_dev)) {\n\t\trv = PTR_ERR(tty_dev);\n\t\tgoto alloc_fail6;\n\t}\n\n\tif (quirks & CLEAR_HALT_CONDITIONS) {\n\t\tusb_clear_halt(usb_dev, acm->in);\n\t\tusb_clear_halt(usb_dev, acm->out);\n\t}\n\n\treturn 0;\nalloc_fail6:\n\tif (acm->country_codes) {\n\t\tdevice_remove_file(&acm->control->dev,\n\t\t\t\t&dev_attr_wCountryCodes);\n\t\tdevice_remove_file(&acm->control->dev,\n\t\t\t\t&dev_attr_iCountryCodeRelDate);\n\t\tkfree(acm->country_codes);\n\t}\n\tdevice_remove_file(&acm->control->dev, &dev_attr_bmCapabilities);\nalloc_fail5:\n\tusb_set_intfdata(intf, NULL);\n\tfor (i = 0; i < ACM_NW; i++)\n\t\tusb_free_urb(acm->wb[i].urb);\nalloc_fail4:\n\tfor (i = 0; i < num_rx_buf; i++)\n\t\tusb_free_urb(acm->read_urbs[i]);\n\tacm_read_buffers_free(acm);\n\tusb_free_urb(acm->ctrlurb);\nalloc_fail3:\n\tacm_write_buffers_free(acm);\nalloc_fail2:\n\tusb_free_coherent(usb_dev, ctrlsize, acm->ctrl_buffer, acm->ctrl_dma);\nalloc_fail1:\n\ttty_port_put(&acm->port);\nalloc_fail:\n\treturn rv;\n}",
        "patch": "--- code before\n+++ code after\n@@ -185,10 +185,6 @@\n \ttty_port_init(&acm->port);\n \tacm->port.ops = &acm_port_ops;\n \n-\tminor = acm_alloc_minor(acm);\n-\tif (minor < 0)\n-\t\tgoto alloc_fail1;\n-\n \tctrlsize = usb_endpoint_maxp(epctrl);\n \treadsize = usb_endpoint_maxp(epread) *\n \t\t\t\t(quirks == SINGLE_RX_URB ? 1 : 2);\n@@ -196,6 +192,13 @@\n \tacm->writesize = usb_endpoint_maxp(epwrite) * 20;\n \tacm->control = control_interface;\n \tacm->data = data_interface;\n+\n+\tusb_get_intf(acm->control); /* undone in destruct() */\n+\n+\tminor = acm_alloc_minor(acm);\n+\tif (minor < 0)\n+\t\tgoto alloc_fail1;\n+\n \tacm->minor = minor;\n \tacm->dev = usb_dev;\n \tif (h.usb_cdc_acm_descriptor)\n@@ -342,7 +345,6 @@\n \tusb_driver_claim_interface(&acm_driver, data_interface, acm);\n \tusb_set_intfdata(data_interface, acm);\n \n-\tusb_get_intf(control_interface);\n \ttty_dev = tty_port_register_device(&acm->port, acm_tty_driver, minor,\n \t\t\t&control_interface->dev);\n \tif (IS_ERR(tty_dev)) {",
        "function_modified_lines": {
            "added": [
                "",
                "\tusb_get_intf(acm->control); /* undone in destruct() */",
                "",
                "\tminor = acm_alloc_minor(acm);",
                "\tif (minor < 0)",
                "\t\tgoto alloc_fail1;",
                ""
            ],
            "deleted": [
                "\tminor = acm_alloc_minor(acm);",
                "\tif (minor < 0)",
                "\t\tgoto alloc_fail1;",
                "",
                "\tusb_get_intf(control_interface);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.2.10, there is a use-after-free bug that can be caused by a malicious USB device in the drivers/usb/class/cdc-acm.c driver, aka CID-c52873e5a1ef.",
        "id": 2205
    },
    {
        "cve_id": "CVE-2020-14351",
        "code_before_change": "static void perf_mmap_close(struct vm_area_struct *vma)\n{\n\tstruct perf_event *event = vma->vm_file->private_data;\n\n\tstruct perf_buffer *rb = ring_buffer_get(event);\n\tstruct user_struct *mmap_user = rb->mmap_user;\n\tint mmap_locked = rb->mmap_locked;\n\tunsigned long size = perf_data_size(rb);\n\n\tif (event->pmu->event_unmapped)\n\t\tevent->pmu->event_unmapped(event, vma->vm_mm);\n\n\t/*\n\t * rb->aux_mmap_count will always drop before rb->mmap_count and\n\t * event->mmap_count, so it is ok to use event->mmap_mutex to\n\t * serialize with perf_mmap here.\n\t */\n\tif (rb_has_aux(rb) && vma->vm_pgoff == rb->aux_pgoff &&\n\t    atomic_dec_and_mutex_lock(&rb->aux_mmap_count, &event->mmap_mutex)) {\n\t\t/*\n\t\t * Stop all AUX events that are writing to this buffer,\n\t\t * so that we can free its AUX pages and corresponding PMU\n\t\t * data. Note that after rb::aux_mmap_count dropped to zero,\n\t\t * they won't start any more (see perf_aux_output_begin()).\n\t\t */\n\t\tperf_pmu_output_stop(event);\n\n\t\t/* now it's safe to free the pages */\n\t\tatomic_long_sub(rb->aux_nr_pages - rb->aux_mmap_locked, &mmap_user->locked_vm);\n\t\tatomic64_sub(rb->aux_mmap_locked, &vma->vm_mm->pinned_vm);\n\n\t\t/* this has to be the last one */\n\t\trb_free_aux(rb);\n\t\tWARN_ON_ONCE(refcount_read(&rb->aux_refcount));\n\n\t\tmutex_unlock(&event->mmap_mutex);\n\t}\n\n\tatomic_dec(&rb->mmap_count);\n\n\tif (!atomic_dec_and_mutex_lock(&event->mmap_count, &event->mmap_mutex))\n\t\tgoto out_put;\n\n\tring_buffer_attach(event, NULL);\n\tmutex_unlock(&event->mmap_mutex);\n\n\t/* If there's still other mmap()s of this buffer, we're done. */\n\tif (atomic_read(&rb->mmap_count))\n\t\tgoto out_put;\n\n\t/*\n\t * No other mmap()s, detach from all other events that might redirect\n\t * into the now unreachable buffer. Somewhat complicated by the\n\t * fact that rb::event_lock otherwise nests inside mmap_mutex.\n\t */\nagain:\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(event, &rb->event_list, rb_entry) {\n\t\tif (!atomic_long_inc_not_zero(&event->refcount)) {\n\t\t\t/*\n\t\t\t * This event is en-route to free_event() which will\n\t\t\t * detach it and remove it from the list.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tmutex_lock(&event->mmap_mutex);\n\t\t/*\n\t\t * Check we didn't race with perf_event_set_output() which can\n\t\t * swizzle the rb from under us while we were waiting to\n\t\t * acquire mmap_mutex.\n\t\t *\n\t\t * If we find a different rb; ignore this event, a next\n\t\t * iteration will no longer find it on the list. We have to\n\t\t * still restart the iteration to make sure we're not now\n\t\t * iterating the wrong list.\n\t\t */\n\t\tif (event->rb == rb)\n\t\t\tring_buffer_attach(event, NULL);\n\n\t\tmutex_unlock(&event->mmap_mutex);\n\t\tput_event(event);\n\n\t\t/*\n\t\t * Restart the iteration; either we're on the wrong list or\n\t\t * destroyed its integrity by doing a deletion.\n\t\t */\n\t\tgoto again;\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * It could be there's still a few 0-ref events on the list; they'll\n\t * get cleaned up by free_event() -- they'll also still have their\n\t * ref on the rb and will free it whenever they are done with it.\n\t *\n\t * Aside from that, this buffer is 'fully' detached and unmapped,\n\t * undo the VM accounting.\n\t */\n\n\tatomic_long_sub((size >> PAGE_SHIFT) + 1 - mmap_locked,\n\t\t\t&mmap_user->locked_vm);\n\tatomic64_sub(mmap_locked, &vma->vm_mm->pinned_vm);\n\tfree_uid(mmap_user);\n\nout_put:\n\tring_buffer_put(rb); /* could be last */\n}",
        "code_after_change": "static void perf_mmap_close(struct vm_area_struct *vma)\n{\n\tstruct perf_event *event = vma->vm_file->private_data;\n\tstruct perf_buffer *rb = ring_buffer_get(event);\n\tstruct user_struct *mmap_user = rb->mmap_user;\n\tint mmap_locked = rb->mmap_locked;\n\tunsigned long size = perf_data_size(rb);\n\tbool detach_rest = false;\n\n\tif (event->pmu->event_unmapped)\n\t\tevent->pmu->event_unmapped(event, vma->vm_mm);\n\n\t/*\n\t * rb->aux_mmap_count will always drop before rb->mmap_count and\n\t * event->mmap_count, so it is ok to use event->mmap_mutex to\n\t * serialize with perf_mmap here.\n\t */\n\tif (rb_has_aux(rb) && vma->vm_pgoff == rb->aux_pgoff &&\n\t    atomic_dec_and_mutex_lock(&rb->aux_mmap_count, &event->mmap_mutex)) {\n\t\t/*\n\t\t * Stop all AUX events that are writing to this buffer,\n\t\t * so that we can free its AUX pages and corresponding PMU\n\t\t * data. Note that after rb::aux_mmap_count dropped to zero,\n\t\t * they won't start any more (see perf_aux_output_begin()).\n\t\t */\n\t\tperf_pmu_output_stop(event);\n\n\t\t/* now it's safe to free the pages */\n\t\tatomic_long_sub(rb->aux_nr_pages - rb->aux_mmap_locked, &mmap_user->locked_vm);\n\t\tatomic64_sub(rb->aux_mmap_locked, &vma->vm_mm->pinned_vm);\n\n\t\t/* this has to be the last one */\n\t\trb_free_aux(rb);\n\t\tWARN_ON_ONCE(refcount_read(&rb->aux_refcount));\n\n\t\tmutex_unlock(&event->mmap_mutex);\n\t}\n\n\tif (atomic_dec_and_test(&rb->mmap_count))\n\t\tdetach_rest = true;\n\n\tif (!atomic_dec_and_mutex_lock(&event->mmap_count, &event->mmap_mutex))\n\t\tgoto out_put;\n\n\tring_buffer_attach(event, NULL);\n\tmutex_unlock(&event->mmap_mutex);\n\n\t/* If there's still other mmap()s of this buffer, we're done. */\n\tif (!detach_rest)\n\t\tgoto out_put;\n\n\t/*\n\t * No other mmap()s, detach from all other events that might redirect\n\t * into the now unreachable buffer. Somewhat complicated by the\n\t * fact that rb::event_lock otherwise nests inside mmap_mutex.\n\t */\nagain:\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(event, &rb->event_list, rb_entry) {\n\t\tif (!atomic_long_inc_not_zero(&event->refcount)) {\n\t\t\t/*\n\t\t\t * This event is en-route to free_event() which will\n\t\t\t * detach it and remove it from the list.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tmutex_lock(&event->mmap_mutex);\n\t\t/*\n\t\t * Check we didn't race with perf_event_set_output() which can\n\t\t * swizzle the rb from under us while we were waiting to\n\t\t * acquire mmap_mutex.\n\t\t *\n\t\t * If we find a different rb; ignore this event, a next\n\t\t * iteration will no longer find it on the list. We have to\n\t\t * still restart the iteration to make sure we're not now\n\t\t * iterating the wrong list.\n\t\t */\n\t\tif (event->rb == rb)\n\t\t\tring_buffer_attach(event, NULL);\n\n\t\tmutex_unlock(&event->mmap_mutex);\n\t\tput_event(event);\n\n\t\t/*\n\t\t * Restart the iteration; either we're on the wrong list or\n\t\t * destroyed its integrity by doing a deletion.\n\t\t */\n\t\tgoto again;\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * It could be there's still a few 0-ref events on the list; they'll\n\t * get cleaned up by free_event() -- they'll also still have their\n\t * ref on the rb and will free it whenever they are done with it.\n\t *\n\t * Aside from that, this buffer is 'fully' detached and unmapped,\n\t * undo the VM accounting.\n\t */\n\n\tatomic_long_sub((size >> PAGE_SHIFT) + 1 - mmap_locked,\n\t\t\t&mmap_user->locked_vm);\n\tatomic64_sub(mmap_locked, &vma->vm_mm->pinned_vm);\n\tfree_uid(mmap_user);\n\nout_put:\n\tring_buffer_put(rb); /* could be last */\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,11 @@\n static void perf_mmap_close(struct vm_area_struct *vma)\n {\n \tstruct perf_event *event = vma->vm_file->private_data;\n-\n \tstruct perf_buffer *rb = ring_buffer_get(event);\n \tstruct user_struct *mmap_user = rb->mmap_user;\n \tint mmap_locked = rb->mmap_locked;\n \tunsigned long size = perf_data_size(rb);\n+\tbool detach_rest = false;\n \n \tif (event->pmu->event_unmapped)\n \t\tevent->pmu->event_unmapped(event, vma->vm_mm);\n@@ -36,7 +36,8 @@\n \t\tmutex_unlock(&event->mmap_mutex);\n \t}\n \n-\tatomic_dec(&rb->mmap_count);\n+\tif (atomic_dec_and_test(&rb->mmap_count))\n+\t\tdetach_rest = true;\n \n \tif (!atomic_dec_and_mutex_lock(&event->mmap_count, &event->mmap_mutex))\n \t\tgoto out_put;\n@@ -45,7 +46,7 @@\n \tmutex_unlock(&event->mmap_mutex);\n \n \t/* If there's still other mmap()s of this buffer, we're done. */\n-\tif (atomic_read(&rb->mmap_count))\n+\tif (!detach_rest)\n \t\tgoto out_put;\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "\tbool detach_rest = false;",
                "\tif (atomic_dec_and_test(&rb->mmap_count))",
                "\t\tdetach_rest = true;",
                "\tif (!detach_rest)"
            ],
            "deleted": [
                "",
                "\tatomic_dec(&rb->mmap_count);",
                "\tif (atomic_read(&rb->mmap_count))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel. A use-after-free memory flaw was found in the perf subsystem allowing a local attacker with permission to monitor perf events to corrupt memory and possibly escalate privileges. The highest threat from this vulnerability is to data confidentiality and integrity as well as system availability.",
        "id": 2512
    },
    {
        "cve_id": "CVE-2021-29657",
        "code_before_change": "int nested_svm_vmrun(struct vcpu_svm *svm)\n{\n\tint ret;\n\tstruct vmcb *vmcb12;\n\tstruct vmcb *hsave = svm->nested.hsave;\n\tstruct vmcb *vmcb = svm->vmcb;\n\tstruct kvm_host_map map;\n\tu64 vmcb12_gpa;\n\n\tif (is_smm(&svm->vcpu)) {\n\t\tkvm_queue_exception(&svm->vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcb12_gpa = svm->vmcb->save.rax;\n\tret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb12_gpa), &map);\n\tif (ret == -EINVAL) {\n\t\tkvm_inject_gp(&svm->vcpu, 0);\n\t\treturn 1;\n\t} else if (ret) {\n\t\treturn kvm_skip_emulated_instruction(&svm->vcpu);\n\t}\n\n\tret = kvm_skip_emulated_instruction(&svm->vcpu);\n\n\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tif (!nested_vmcb_checks(svm, vmcb12)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;\n\t}\n\n\ttrace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb12_gpa,\n\t\t\t       vmcb12->save.rip,\n\t\t\t       vmcb12->control.int_ctl,\n\t\t\t       vmcb12->control.event_inj,\n\t\t\t       vmcb12->control.nested_ctl);\n\n\ttrace_kvm_nested_intercepts(vmcb12->control.intercepts[INTERCEPT_CR] & 0xffff,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_CR] >> 16,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_EXCEPTION],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD3],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD4],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD5]);\n\n\t/* Clear internal status */\n\tkvm_clear_exception_queue(&svm->vcpu);\n\tkvm_clear_interrupt_queue(&svm->vcpu);\n\n\t/*\n\t * Save the old vmcb, so we don't need to pick what we save, but can\n\t * restore everything when a VMEXIT occurs\n\t */\n\thsave->save.es     = vmcb->save.es;\n\thsave->save.cs     = vmcb->save.cs;\n\thsave->save.ss     = vmcb->save.ss;\n\thsave->save.ds     = vmcb->save.ds;\n\thsave->save.gdtr   = vmcb->save.gdtr;\n\thsave->save.idtr   = vmcb->save.idtr;\n\thsave->save.efer   = svm->vcpu.arch.efer;\n\thsave->save.cr0    = kvm_read_cr0(&svm->vcpu);\n\thsave->save.cr4    = svm->vcpu.arch.cr4;\n\thsave->save.rflags = kvm_get_rflags(&svm->vcpu);\n\thsave->save.rip    = kvm_rip_read(&svm->vcpu);\n\thsave->save.rsp    = vmcb->save.rsp;\n\thsave->save.rax    = vmcb->save.rax;\n\tif (npt_enabled)\n\t\thsave->save.cr3    = vmcb->save.cr3;\n\telse\n\t\thsave->save.cr3    = kvm_read_cr3(&svm->vcpu);\n\n\tcopy_vmcb_control_area(&hsave->control, &vmcb->control);\n\n\tsvm->nested.nested_run_pending = 1;\n\n\tif (enter_svm_guest_mode(svm, vmcb12_gpa, vmcb12))\n\t\tgoto out_exit_err;\n\n\tif (nested_svm_vmrun_msrpm(svm))\n\t\tgoto out;\n\nout_exit_err:\n\tsvm->nested.nested_run_pending = 0;\n\n\tsvm->vmcb->control.exit_code    = SVM_EXIT_ERR;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\tsvm->vmcb->control.exit_info_1  = 0;\n\tsvm->vmcb->control.exit_info_2  = 0;\n\n\tnested_svm_vmexit(svm);\n\nout:\n\tkvm_vcpu_unmap(&svm->vcpu, &map, true);\n\n\treturn ret;\n}",
        "code_after_change": "int nested_svm_vmrun(struct vcpu_svm *svm)\n{\n\tint ret;\n\tstruct vmcb *vmcb12;\n\tstruct vmcb *hsave = svm->nested.hsave;\n\tstruct vmcb *vmcb = svm->vmcb;\n\tstruct kvm_host_map map;\n\tu64 vmcb12_gpa;\n\n\tif (is_smm(&svm->vcpu)) {\n\t\tkvm_queue_exception(&svm->vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcb12_gpa = svm->vmcb->save.rax;\n\tret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb12_gpa), &map);\n\tif (ret == -EINVAL) {\n\t\tkvm_inject_gp(&svm->vcpu, 0);\n\t\treturn 1;\n\t} else if (ret) {\n\t\treturn kvm_skip_emulated_instruction(&svm->vcpu);\n\t}\n\n\tret = kvm_skip_emulated_instruction(&svm->vcpu);\n\n\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tload_nested_vmcb_control(svm, &vmcb12->control);\n\n\tif (!nested_vmcb_check_save(svm, vmcb12) ||\n\t    !nested_vmcb_check_controls(&svm->nested.ctl)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;\n\t}\n\n\ttrace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb12_gpa,\n\t\t\t       vmcb12->save.rip,\n\t\t\t       vmcb12->control.int_ctl,\n\t\t\t       vmcb12->control.event_inj,\n\t\t\t       vmcb12->control.nested_ctl);\n\n\ttrace_kvm_nested_intercepts(vmcb12->control.intercepts[INTERCEPT_CR] & 0xffff,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_CR] >> 16,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_EXCEPTION],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD3],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD4],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD5]);\n\n\t/* Clear internal status */\n\tkvm_clear_exception_queue(&svm->vcpu);\n\tkvm_clear_interrupt_queue(&svm->vcpu);\n\n\t/*\n\t * Save the old vmcb, so we don't need to pick what we save, but can\n\t * restore everything when a VMEXIT occurs\n\t */\n\thsave->save.es     = vmcb->save.es;\n\thsave->save.cs     = vmcb->save.cs;\n\thsave->save.ss     = vmcb->save.ss;\n\thsave->save.ds     = vmcb->save.ds;\n\thsave->save.gdtr   = vmcb->save.gdtr;\n\thsave->save.idtr   = vmcb->save.idtr;\n\thsave->save.efer   = svm->vcpu.arch.efer;\n\thsave->save.cr0    = kvm_read_cr0(&svm->vcpu);\n\thsave->save.cr4    = svm->vcpu.arch.cr4;\n\thsave->save.rflags = kvm_get_rflags(&svm->vcpu);\n\thsave->save.rip    = kvm_rip_read(&svm->vcpu);\n\thsave->save.rsp    = vmcb->save.rsp;\n\thsave->save.rax    = vmcb->save.rax;\n\tif (npt_enabled)\n\t\thsave->save.cr3    = vmcb->save.cr3;\n\telse\n\t\thsave->save.cr3    = kvm_read_cr3(&svm->vcpu);\n\n\tcopy_vmcb_control_area(&hsave->control, &vmcb->control);\n\n\tsvm->nested.nested_run_pending = 1;\n\n\tif (enter_svm_guest_mode(svm, vmcb12_gpa, vmcb12))\n\t\tgoto out_exit_err;\n\n\tif (nested_svm_vmrun_msrpm(svm))\n\t\tgoto out;\n\nout_exit_err:\n\tsvm->nested.nested_run_pending = 0;\n\n\tsvm->vmcb->control.exit_code    = SVM_EXIT_ERR;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\tsvm->vmcb->control.exit_info_1  = 0;\n\tsvm->vmcb->control.exit_info_2  = 0;\n\n\tnested_svm_vmexit(svm);\n\nout:\n\tkvm_vcpu_unmap(&svm->vcpu, &map, true);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,10 @@\n \tif (WARN_ON_ONCE(!svm->nested.initialized))\n \t\treturn -EINVAL;\n \n-\tif (!nested_vmcb_checks(svm, vmcb12)) {\n+\tload_nested_vmcb_control(svm, &vmcb12->control);\n+\n+\tif (!nested_vmcb_check_save(svm, vmcb12) ||\n+\t    !nested_vmcb_check_controls(&svm->nested.ctl)) {\n \t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n \t\tvmcb12->control.exit_code_hi = 0;\n \t\tvmcb12->control.exit_info_1  = 0;",
        "function_modified_lines": {
            "added": [
                "\tload_nested_vmcb_control(svm, &vmcb12->control);",
                "",
                "\tif (!nested_vmcb_check_save(svm, vmcb12) ||",
                "\t    !nested_vmcb_check_controls(&svm->nested.ctl)) {"
            ],
            "deleted": [
                "\tif (!nested_vmcb_checks(svm, vmcb12)) {"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-367"
        ],
        "cve_description": "arch/x86/kvm/svm/nested.c in the Linux kernel before 5.11.12 has a use-after-free in which an AMD KVM guest can bypass access control on host OS MSRs when there are nested guests, aka CID-a58d9166a756. This occurs because of a TOCTOU race condition associated with a VMCB12 double fetch in nested_svm_vmrun.",
        "id": 2957
    },
    {
        "cve_id": "CVE-2017-17052",
        "code_before_change": "static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,\n\tstruct user_namespace *user_ns)\n{\n\tmm->mmap = NULL;\n\tmm->mm_rb = RB_ROOT;\n\tmm->vmacache_seqnum = 0;\n\tatomic_set(&mm->mm_users, 1);\n\tatomic_set(&mm->mm_count, 1);\n\tinit_rwsem(&mm->mmap_sem);\n\tINIT_LIST_HEAD(&mm->mmlist);\n\tmm->core_state = NULL;\n\tatomic_long_set(&mm->nr_ptes, 0);\n\tmm_nr_pmds_init(mm);\n\tmm->map_count = 0;\n\tmm->locked_vm = 0;\n\tmm->pinned_vm = 0;\n\tmemset(&mm->rss_stat, 0, sizeof(mm->rss_stat));\n\tspin_lock_init(&mm->page_table_lock);\n\tmm_init_cpumask(mm);\n\tmm_init_aio(mm);\n\tmm_init_owner(mm, p);\n\tmmu_notifier_mm_init(mm);\n\tinit_tlb_flush_pending(mm);\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS\n\tmm->pmd_huge_pte = NULL;\n#endif\n\n\tif (current->mm) {\n\t\tmm->flags = current->mm->flags & MMF_INIT_MASK;\n\t\tmm->def_flags = current->mm->def_flags & VM_INIT_DEF_MASK;\n\t} else {\n\t\tmm->flags = default_dump_filter;\n\t\tmm->def_flags = 0;\n\t}\n\n\tif (mm_alloc_pgd(mm))\n\t\tgoto fail_nopgd;\n\n\tif (init_new_context(p, mm))\n\t\tgoto fail_nocontext;\n\n\tmm->user_ns = get_user_ns(user_ns);\n\treturn mm;\n\nfail_nocontext:\n\tmm_free_pgd(mm);\nfail_nopgd:\n\tfree_mm(mm);\n\treturn NULL;\n}",
        "code_after_change": "static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,\n\tstruct user_namespace *user_ns)\n{\n\tmm->mmap = NULL;\n\tmm->mm_rb = RB_ROOT;\n\tmm->vmacache_seqnum = 0;\n\tatomic_set(&mm->mm_users, 1);\n\tatomic_set(&mm->mm_count, 1);\n\tinit_rwsem(&mm->mmap_sem);\n\tINIT_LIST_HEAD(&mm->mmlist);\n\tmm->core_state = NULL;\n\tatomic_long_set(&mm->nr_ptes, 0);\n\tmm_nr_pmds_init(mm);\n\tmm->map_count = 0;\n\tmm->locked_vm = 0;\n\tmm->pinned_vm = 0;\n\tmemset(&mm->rss_stat, 0, sizeof(mm->rss_stat));\n\tspin_lock_init(&mm->page_table_lock);\n\tmm_init_cpumask(mm);\n\tmm_init_aio(mm);\n\tmm_init_owner(mm, p);\n\tRCU_INIT_POINTER(mm->exe_file, NULL);\n\tmmu_notifier_mm_init(mm);\n\tinit_tlb_flush_pending(mm);\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS\n\tmm->pmd_huge_pte = NULL;\n#endif\n\n\tif (current->mm) {\n\t\tmm->flags = current->mm->flags & MMF_INIT_MASK;\n\t\tmm->def_flags = current->mm->def_flags & VM_INIT_DEF_MASK;\n\t} else {\n\t\tmm->flags = default_dump_filter;\n\t\tmm->def_flags = 0;\n\t}\n\n\tif (mm_alloc_pgd(mm))\n\t\tgoto fail_nopgd;\n\n\tif (init_new_context(p, mm))\n\t\tgoto fail_nocontext;\n\n\tmm->user_ns = get_user_ns(user_ns);\n\treturn mm;\n\nfail_nocontext:\n\tmm_free_pgd(mm);\nfail_nopgd:\n\tfree_mm(mm);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,6 +19,7 @@\n \tmm_init_cpumask(mm);\n \tmm_init_aio(mm);\n \tmm_init_owner(mm, p);\n+\tRCU_INIT_POINTER(mm->exe_file, NULL);\n \tmmu_notifier_mm_init(mm);\n \tinit_tlb_flush_pending(mm);\n #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS",
        "function_modified_lines": {
            "added": [
                "\tRCU_INIT_POINTER(mm->exe_file, NULL);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The mm_init function in kernel/fork.c in the Linux kernel before 4.12.10 does not clear the ->exe_file member of a new process's mm_struct, allowing a local attacker to achieve a use-after-free or possibly have unspecified other impact by running a specially crafted program.",
        "id": 1357
    },
    {
        "cve_id": "CVE-2023-3609",
        "code_before_change": "static int u32_set_parms(struct net *net, struct tcf_proto *tp,\n\t\t\t unsigned long base,\n\t\t\t struct tc_u_knode *n, struct nlattr **tb,\n\t\t\t struct nlattr *est, u32 flags, u32 fl_flags,\n\t\t\t struct netlink_ext_ack *extack)\n{\n\tint err;\n\n\terr = tcf_exts_validate_ex(net, tp, tb, est, &n->exts, flags,\n\t\t\t\t   fl_flags, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_U32_LINK]) {\n\t\tu32 handle = nla_get_u32(tb[TCA_U32_LINK]);\n\t\tstruct tc_u_hnode *ht_down = NULL, *ht_old;\n\n\t\tif (TC_U32_KEY(handle)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"u32 Link handle must be a hash table\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (handle) {\n\t\t\tht_down = u32_lookup_ht(tp->data, handle);\n\n\t\t\tif (!ht_down) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Link hash table not found\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (ht_down->is_root) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Not linking to root node\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tht_down->refcnt++;\n\t\t}\n\n\t\tht_old = rtnl_dereference(n->ht_down);\n\t\trcu_assign_pointer(n->ht_down, ht_down);\n\n\t\tif (ht_old)\n\t\t\tht_old->refcnt--;\n\t}\n\tif (tb[TCA_U32_CLASSID]) {\n\t\tn->res.classid = nla_get_u32(tb[TCA_U32_CLASSID]);\n\t\ttcf_bind_filter(tp, &n->res, base);\n\t}\n\n\tif (tb[TCA_U32_INDEV]) {\n\t\tint ret;\n\t\tret = tcf_change_indev(net, tb[TCA_U32_INDEV], extack);\n\t\tif (ret < 0)\n\t\t\treturn -EINVAL;\n\t\tn->ifindex = ret;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int u32_set_parms(struct net *net, struct tcf_proto *tp,\n\t\t\t unsigned long base,\n\t\t\t struct tc_u_knode *n, struct nlattr **tb,\n\t\t\t struct nlattr *est, u32 flags, u32 fl_flags,\n\t\t\t struct netlink_ext_ack *extack)\n{\n\tint err, ifindex = -1;\n\n\terr = tcf_exts_validate_ex(net, tp, tb, est, &n->exts, flags,\n\t\t\t\t   fl_flags, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_U32_INDEV]) {\n\t\tifindex = tcf_change_indev(net, tb[TCA_U32_INDEV], extack);\n\t\tif (ifindex < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tb[TCA_U32_LINK]) {\n\t\tu32 handle = nla_get_u32(tb[TCA_U32_LINK]);\n\t\tstruct tc_u_hnode *ht_down = NULL, *ht_old;\n\n\t\tif (TC_U32_KEY(handle)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"u32 Link handle must be a hash table\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (handle) {\n\t\t\tht_down = u32_lookup_ht(tp->data, handle);\n\n\t\t\tif (!ht_down) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Link hash table not found\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (ht_down->is_root) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Not linking to root node\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tht_down->refcnt++;\n\t\t}\n\n\t\tht_old = rtnl_dereference(n->ht_down);\n\t\trcu_assign_pointer(n->ht_down, ht_down);\n\n\t\tif (ht_old)\n\t\t\tht_old->refcnt--;\n\t}\n\tif (tb[TCA_U32_CLASSID]) {\n\t\tn->res.classid = nla_get_u32(tb[TCA_U32_CLASSID]);\n\t\ttcf_bind_filter(tp, &n->res, base);\n\t}\n\n\tif (ifindex >= 0)\n\t\tn->ifindex = ifindex;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,12 +4,18 @@\n \t\t\t struct nlattr *est, u32 flags, u32 fl_flags,\n \t\t\t struct netlink_ext_ack *extack)\n {\n-\tint err;\n+\tint err, ifindex = -1;\n \n \terr = tcf_exts_validate_ex(net, tp, tb, est, &n->exts, flags,\n \t\t\t\t   fl_flags, extack);\n \tif (err < 0)\n \t\treturn err;\n+\n+\tif (tb[TCA_U32_INDEV]) {\n+\t\tifindex = tcf_change_indev(net, tb[TCA_U32_INDEV], extack);\n+\t\tif (ifindex < 0)\n+\t\t\treturn -EINVAL;\n+\t}\n \n \tif (tb[TCA_U32_LINK]) {\n \t\tu32 handle = nla_get_u32(tb[TCA_U32_LINK]);\n@@ -45,12 +51,8 @@\n \t\ttcf_bind_filter(tp, &n->res, base);\n \t}\n \n-\tif (tb[TCA_U32_INDEV]) {\n-\t\tint ret;\n-\t\tret = tcf_change_indev(net, tb[TCA_U32_INDEV], extack);\n-\t\tif (ret < 0)\n-\t\t\treturn -EINVAL;\n-\t\tn->ifindex = ret;\n-\t}\n+\tif (ifindex >= 0)\n+\t\tn->ifindex = ifindex;\n+\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tint err, ifindex = -1;",
                "",
                "\tif (tb[TCA_U32_INDEV]) {",
                "\t\tifindex = tcf_change_indev(net, tb[TCA_U32_INDEV], extack);",
                "\t\tif (ifindex < 0)",
                "\t\t\treturn -EINVAL;",
                "\t}",
                "\tif (ifindex >= 0)",
                "\t\tn->ifindex = ifindex;",
                ""
            ],
            "deleted": [
                "\tint err;",
                "\tif (tb[TCA_U32_INDEV]) {",
                "\t\tint ret;",
                "\t\tret = tcf_change_indev(net, tb[TCA_U32_INDEV], extack);",
                "\t\tif (ret < 0)",
                "\t\t\treturn -EINVAL;",
                "\t\tn->ifindex = ret;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's net/sched: cls_u32 component can be exploited to achieve local privilege escalation.\n\nIf tcf_change_indev() fails, u32_set_parms() will immediately return an error after incrementing or decrementing the reference counter in tcf_bind_filter(). If an attacker can control the reference counter and set it to zero, they can cause the reference to be freed, leading to a use-after-free vulnerability.\n\nWe recommend upgrading past commit 04c55383fa5689357bcdd2c8036725a55ed632bc.\n\n",
        "id": 4117
    },
    {
        "cve_id": "CVE-2019-19448",
        "code_before_change": "static bool try_merge_free_space(struct btrfs_free_space_ctl *ctl,\n\t\t\t  struct btrfs_free_space *info, bool update_stat)\n{\n\tstruct btrfs_free_space *left_info;\n\tstruct btrfs_free_space *right_info;\n\tbool merged = false;\n\tu64 offset = info->offset;\n\tu64 bytes = info->bytes;\n\tconst bool is_trimmed = btrfs_free_space_trimmed(info);\n\n\t/*\n\t * first we want to see if there is free space adjacent to the range we\n\t * are adding, if there is remove that struct and add a new one to\n\t * cover the entire range\n\t */\n\tright_info = tree_search_offset(ctl, offset + bytes, 0, 0);\n\tif (right_info && rb_prev(&right_info->offset_index))\n\t\tleft_info = rb_entry(rb_prev(&right_info->offset_index),\n\t\t\t\t     struct btrfs_free_space, offset_index);\n\telse\n\t\tleft_info = tree_search_offset(ctl, offset - 1, 0, 0);\n\n\t/* See try_merge_free_space() comment. */\n\tif (right_info && !right_info->bitmap &&\n\t    (!is_trimmed || btrfs_free_space_trimmed(right_info))) {\n\t\tif (update_stat)\n\t\t\tunlink_free_space(ctl, right_info);\n\t\telse\n\t\t\t__unlink_free_space(ctl, right_info);\n\t\tinfo->bytes += right_info->bytes;\n\t\tkmem_cache_free(btrfs_free_space_cachep, right_info);\n\t\tmerged = true;\n\t}\n\n\t/* See try_merge_free_space() comment. */\n\tif (left_info && !left_info->bitmap &&\n\t    left_info->offset + left_info->bytes == offset &&\n\t    (!is_trimmed || btrfs_free_space_trimmed(left_info))) {\n\t\tif (update_stat)\n\t\t\tunlink_free_space(ctl, left_info);\n\t\telse\n\t\t\t__unlink_free_space(ctl, left_info);\n\t\tinfo->offset = left_info->offset;\n\t\tinfo->bytes += left_info->bytes;\n\t\tkmem_cache_free(btrfs_free_space_cachep, left_info);\n\t\tmerged = true;\n\t}\n\n\treturn merged;\n}",
        "code_after_change": "static bool try_merge_free_space(struct btrfs_free_space_ctl *ctl,\n\t\t\t  struct btrfs_free_space *info, bool update_stat)\n{\n\tstruct btrfs_free_space *left_info = NULL;\n\tstruct btrfs_free_space *right_info;\n\tbool merged = false;\n\tu64 offset = info->offset;\n\tu64 bytes = info->bytes;\n\tconst bool is_trimmed = btrfs_free_space_trimmed(info);\n\n\t/*\n\t * first we want to see if there is free space adjacent to the range we\n\t * are adding, if there is remove that struct and add a new one to\n\t * cover the entire range\n\t */\n\tright_info = tree_search_offset(ctl, offset + bytes, 0, 0);\n\tif (right_info && rb_prev(&right_info->offset_index))\n\t\tleft_info = rb_entry(rb_prev(&right_info->offset_index),\n\t\t\t\t     struct btrfs_free_space, offset_index);\n\telse if (!right_info)\n\t\tleft_info = tree_search_offset(ctl, offset - 1, 0, 0);\n\n\t/* See try_merge_free_space() comment. */\n\tif (right_info && !right_info->bitmap &&\n\t    (!is_trimmed || btrfs_free_space_trimmed(right_info))) {\n\t\tif (update_stat)\n\t\t\tunlink_free_space(ctl, right_info);\n\t\telse\n\t\t\t__unlink_free_space(ctl, right_info);\n\t\tinfo->bytes += right_info->bytes;\n\t\tkmem_cache_free(btrfs_free_space_cachep, right_info);\n\t\tmerged = true;\n\t}\n\n\t/* See try_merge_free_space() comment. */\n\tif (left_info && !left_info->bitmap &&\n\t    left_info->offset + left_info->bytes == offset &&\n\t    (!is_trimmed || btrfs_free_space_trimmed(left_info))) {\n\t\tif (update_stat)\n\t\t\tunlink_free_space(ctl, left_info);\n\t\telse\n\t\t\t__unlink_free_space(ctl, left_info);\n\t\tinfo->offset = left_info->offset;\n\t\tinfo->bytes += left_info->bytes;\n\t\tkmem_cache_free(btrfs_free_space_cachep, left_info);\n\t\tmerged = true;\n\t}\n\n\treturn merged;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n static bool try_merge_free_space(struct btrfs_free_space_ctl *ctl,\n \t\t\t  struct btrfs_free_space *info, bool update_stat)\n {\n-\tstruct btrfs_free_space *left_info;\n+\tstruct btrfs_free_space *left_info = NULL;\n \tstruct btrfs_free_space *right_info;\n \tbool merged = false;\n \tu64 offset = info->offset;\n@@ -17,7 +17,7 @@\n \tif (right_info && rb_prev(&right_info->offset_index))\n \t\tleft_info = rb_entry(rb_prev(&right_info->offset_index),\n \t\t\t\t     struct btrfs_free_space, offset_index);\n-\telse\n+\telse if (!right_info)\n \t\tleft_info = tree_search_offset(ctl, offset - 1, 0, 0);\n \n \t/* See try_merge_free_space() comment. */",
        "function_modified_lines": {
            "added": [
                "\tstruct btrfs_free_space *left_info = NULL;",
                "\telse if (!right_info)"
            ],
            "deleted": [
                "\tstruct btrfs_free_space *left_info;",
                "\telse"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.0.21 and 5.3.11, mounting a crafted btrfs filesystem image, performing some operations, and then making a syncfs system call can lead to a use-after-free in try_merge_free_space in fs/btrfs/free-space-cache.c because the pointer to a left data structure can be the same as the pointer to a right data structure.",
        "id": 2195
    },
    {
        "cve_id": "CVE-2019-19447",
        "code_before_change": "static int ext4_unlink(struct inode *dir, struct dentry *dentry)\n{\n\tint retval;\n\tstruct inode *inode;\n\tstruct buffer_head *bh;\n\tstruct ext4_dir_entry_2 *de;\n\thandle_t *handle = NULL;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(dir->i_sb))))\n\t\treturn -EIO;\n\n\ttrace_ext4_unlink_enter(dir, dentry);\n\t/* Initialize quotas before so that eventual writes go\n\t * in separate transaction */\n\tretval = dquot_initialize(dir);\n\tif (retval)\n\t\treturn retval;\n\tretval = dquot_initialize(d_inode(dentry));\n\tif (retval)\n\t\treturn retval;\n\n\tretval = -ENOENT;\n\tbh = ext4_find_entry(dir, &dentry->d_name, &de, NULL);\n\tif (IS_ERR(bh))\n\t\treturn PTR_ERR(bh);\n\tif (!bh)\n\t\tgoto end_unlink;\n\n\tinode = d_inode(dentry);\n\n\tretval = -EFSCORRUPTED;\n\tif (le32_to_cpu(de->inode) != inode->i_ino)\n\t\tgoto end_unlink;\n\n\thandle = ext4_journal_start(dir, EXT4_HT_DIR,\n\t\t\t\t    EXT4_DATA_TRANS_BLOCKS(dir->i_sb));\n\tif (IS_ERR(handle)) {\n\t\tretval = PTR_ERR(handle);\n\t\thandle = NULL;\n\t\tgoto end_unlink;\n\t}\n\n\tif (IS_DIRSYNC(dir))\n\t\text4_handle_sync(handle);\n\n\tif (inode->i_nlink == 0) {\n\t\text4_warning_inode(inode, \"Deleting file '%.*s' with no links\",\n\t\t\t\t   dentry->d_name.len, dentry->d_name.name);\n\t\tset_nlink(inode, 1);\n\t}\n\tretval = ext4_delete_entry(handle, dir, de, bh);\n\tif (retval)\n\t\tgoto end_unlink;\n\tdir->i_ctime = dir->i_mtime = current_time(dir);\n\text4_update_dx_flag(dir);\n\text4_mark_inode_dirty(handle, dir);\n\tdrop_nlink(inode);\n\tif (!inode->i_nlink)\n\t\text4_orphan_add(handle, inode);\n\tinode->i_ctime = current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\n#ifdef CONFIG_UNICODE\n\t/* VFS negative dentries are incompatible with Encoding and\n\t * Case-insensitiveness. Eventually we'll want avoid\n\t * invalidating the dentries here, alongside with returning the\n\t * negative dentries at ext4_lookup(), when it is  better\n\t * supported by the VFS for the CI case.\n\t */\n\tif (IS_CASEFOLDED(dir))\n\t\td_invalidate(dentry);\n#endif\n\nend_unlink:\n\tbrelse(bh);\n\tif (handle)\n\t\text4_journal_stop(handle);\n\ttrace_ext4_unlink_exit(dentry, retval);\n\treturn retval;\n}",
        "code_after_change": "static int ext4_unlink(struct inode *dir, struct dentry *dentry)\n{\n\tint retval;\n\tstruct inode *inode;\n\tstruct buffer_head *bh;\n\tstruct ext4_dir_entry_2 *de;\n\thandle_t *handle = NULL;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(dir->i_sb))))\n\t\treturn -EIO;\n\n\ttrace_ext4_unlink_enter(dir, dentry);\n\t/* Initialize quotas before so that eventual writes go\n\t * in separate transaction */\n\tretval = dquot_initialize(dir);\n\tif (retval)\n\t\treturn retval;\n\tretval = dquot_initialize(d_inode(dentry));\n\tif (retval)\n\t\treturn retval;\n\n\tretval = -ENOENT;\n\tbh = ext4_find_entry(dir, &dentry->d_name, &de, NULL);\n\tif (IS_ERR(bh))\n\t\treturn PTR_ERR(bh);\n\tif (!bh)\n\t\tgoto end_unlink;\n\n\tinode = d_inode(dentry);\n\n\tretval = -EFSCORRUPTED;\n\tif (le32_to_cpu(de->inode) != inode->i_ino)\n\t\tgoto end_unlink;\n\n\thandle = ext4_journal_start(dir, EXT4_HT_DIR,\n\t\t\t\t    EXT4_DATA_TRANS_BLOCKS(dir->i_sb));\n\tif (IS_ERR(handle)) {\n\t\tretval = PTR_ERR(handle);\n\t\thandle = NULL;\n\t\tgoto end_unlink;\n\t}\n\n\tif (IS_DIRSYNC(dir))\n\t\text4_handle_sync(handle);\n\n\tretval = ext4_delete_entry(handle, dir, de, bh);\n\tif (retval)\n\t\tgoto end_unlink;\n\tdir->i_ctime = dir->i_mtime = current_time(dir);\n\text4_update_dx_flag(dir);\n\text4_mark_inode_dirty(handle, dir);\n\tif (inode->i_nlink == 0)\n\t\text4_warning_inode(inode, \"Deleting file '%.*s' with no links\",\n\t\t\t\t   dentry->d_name.len, dentry->d_name.name);\n\telse\n\t\tdrop_nlink(inode);\n\tif (!inode->i_nlink)\n\t\text4_orphan_add(handle, inode);\n\tinode->i_ctime = current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\n#ifdef CONFIG_UNICODE\n\t/* VFS negative dentries are incompatible with Encoding and\n\t * Case-insensitiveness. Eventually we'll want avoid\n\t * invalidating the dentries here, alongside with returning the\n\t * negative dentries at ext4_lookup(), when it is  better\n\t * supported by the VFS for the CI case.\n\t */\n\tif (IS_CASEFOLDED(dir))\n\t\td_invalidate(dentry);\n#endif\n\nend_unlink:\n\tbrelse(bh);\n\tif (handle)\n\t\text4_journal_stop(handle);\n\ttrace_ext4_unlink_exit(dentry, retval);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,18 +43,17 @@\n \tif (IS_DIRSYNC(dir))\n \t\text4_handle_sync(handle);\n \n-\tif (inode->i_nlink == 0) {\n-\t\text4_warning_inode(inode, \"Deleting file '%.*s' with no links\",\n-\t\t\t\t   dentry->d_name.len, dentry->d_name.name);\n-\t\tset_nlink(inode, 1);\n-\t}\n \tretval = ext4_delete_entry(handle, dir, de, bh);\n \tif (retval)\n \t\tgoto end_unlink;\n \tdir->i_ctime = dir->i_mtime = current_time(dir);\n \text4_update_dx_flag(dir);\n \text4_mark_inode_dirty(handle, dir);\n-\tdrop_nlink(inode);\n+\tif (inode->i_nlink == 0)\n+\t\text4_warning_inode(inode, \"Deleting file '%.*s' with no links\",\n+\t\t\t\t   dentry->d_name.len, dentry->d_name.name);\n+\telse\n+\t\tdrop_nlink(inode);\n \tif (!inode->i_nlink)\n \t\text4_orphan_add(handle, inode);\n \tinode->i_ctime = current_time(inode);",
        "function_modified_lines": {
            "added": [
                "\tif (inode->i_nlink == 0)",
                "\t\text4_warning_inode(inode, \"Deleting file '%.*s' with no links\",",
                "\t\t\t\t   dentry->d_name.len, dentry->d_name.name);",
                "\telse",
                "\t\tdrop_nlink(inode);"
            ],
            "deleted": [
                "\tif (inode->i_nlink == 0) {",
                "\t\text4_warning_inode(inode, \"Deleting file '%.*s' with no links\",",
                "\t\t\t\t   dentry->d_name.len, dentry->d_name.name);",
                "\t\tset_nlink(inode, 1);",
                "\t}",
                "\tdrop_nlink(inode);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.0.21, mounting a crafted ext4 filesystem image, performing some operations, and unmounting can lead to a use-after-free in ext4_put_super in fs/ext4/super.c, related to dump_orphan_list in fs/ext4/super.c.",
        "id": 2194
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tvmw_user_bo_unref(vmw_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo, *tmp_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\ttmp_bo = vmw_bo;\n\tvmw_user_bo_unref(&tmp_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t\t\t\t SVGAMobId *id,\n \t\t\t\t struct vmw_bo **vmw_bo_p)\n {\n-\tstruct vmw_bo *vmw_bo;\n+\tstruct vmw_bo *vmw_bo, *tmp_bo;\n \tuint32_t handle = *id;\n \tstruct vmw_relocation *reloc;\n \tint ret;\n@@ -16,7 +16,8 @@\n \t}\n \tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n \tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n-\tvmw_user_bo_unref(vmw_bo);\n+\ttmp_bo = vmw_bo;\n+\tvmw_user_bo_unref(&tmp_bo);\n \tif (unlikely(ret != 0))\n \t\treturn ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct vmw_bo *vmw_bo, *tmp_bo;",
                "\ttmp_bo = vmw_bo;",
                "\tvmw_user_bo_unref(&tmp_bo);"
            ],
            "deleted": [
                "\tstruct vmw_bo *vmw_bo;",
                "\tvmw_user_bo_unref(vmw_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4273
    },
    {
        "cve_id": "CVE-2023-3141",
        "code_before_change": "static void r592_remove(struct pci_dev *pdev)\n{\n\tint error = 0;\n\tstruct r592_device *dev = pci_get_drvdata(pdev);\n\n\t/* Stop the processing thread.\n\tThat ensures that we won't take any more requests */\n\tkthread_stop(dev->io_thread);\n\n\tr592_enable_device(dev, false);\n\n\twhile (!error && dev->req) {\n\t\tdev->req->error = -ETIME;\n\t\terror = memstick_next_req(dev->host, &dev->req);\n\t}\n\tmemstick_remove_host(dev->host);\n\n\tif (dev->dummy_dma_page)\n\t\tdma_free_coherent(&pdev->dev, PAGE_SIZE, dev->dummy_dma_page,\n\t\t\tdev->dummy_dma_page_physical_address);\n\n\tfree_irq(dev->irq, dev);\n\tiounmap(dev->mmio);\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tmemstick_free_host(dev->host);\n}",
        "code_after_change": "static void r592_remove(struct pci_dev *pdev)\n{\n\tint error = 0;\n\tstruct r592_device *dev = pci_get_drvdata(pdev);\n\n\t/* Stop the processing thread.\n\tThat ensures that we won't take any more requests */\n\tkthread_stop(dev->io_thread);\n\tdel_timer_sync(&dev->detect_timer);\n\tr592_enable_device(dev, false);\n\n\twhile (!error && dev->req) {\n\t\tdev->req->error = -ETIME;\n\t\terror = memstick_next_req(dev->host, &dev->req);\n\t}\n\tmemstick_remove_host(dev->host);\n\n\tif (dev->dummy_dma_page)\n\t\tdma_free_coherent(&pdev->dev, PAGE_SIZE, dev->dummy_dma_page,\n\t\t\tdev->dummy_dma_page_physical_address);\n\n\tfree_irq(dev->irq, dev);\n\tiounmap(dev->mmio);\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tmemstick_free_host(dev->host);\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \t/* Stop the processing thread.\n \tThat ensures that we won't take any more requests */\n \tkthread_stop(dev->io_thread);\n-\n+\tdel_timer_sync(&dev->detect_timer);\n \tr592_enable_device(dev, false);\n \n \twhile (!error && dev->req) {",
        "function_modified_lines": {
            "added": [
                "\tdel_timer_sync(&dev->detect_timer);"
            ],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in r592_remove in drivers/memstick/host/r592.c in media access in the Linux Kernel. This flaw allows a local attacker to crash the system at device disconnect, possibly leading to a kernel information leak.",
        "id": 4001
    },
    {
        "cve_id": "CVE-2021-0342",
        "code_before_change": "static ssize_t tun_get_user(struct tun_struct *tun, struct tun_file *tfile,\n\t\t\t    void *msg_control, struct iov_iter *from,\n\t\t\t    int noblock, bool more)\n{\n\tstruct tun_pi pi = { 0, cpu_to_be16(ETH_P_IP) };\n\tstruct sk_buff *skb;\n\tsize_t total_len = iov_iter_count(from);\n\tsize_t len = total_len, align = tun->align, linear;\n\tstruct virtio_net_hdr gso = { 0 };\n\tstruct tun_pcpu_stats *stats;\n\tint good_linear;\n\tint copylen;\n\tbool zerocopy = false;\n\tint err;\n\tu32 rxhash = 0;\n\tint skb_xdp = 1;\n\tbool frags = tun_napi_frags_enabled(tfile);\n\n\tif (!(tun->flags & IFF_NO_PI)) {\n\t\tif (len < sizeof(pi))\n\t\t\treturn -EINVAL;\n\t\tlen -= sizeof(pi);\n\n\t\tif (!copy_from_iter_full(&pi, sizeof(pi), from))\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (tun->flags & IFF_VNET_HDR) {\n\t\tint vnet_hdr_sz = READ_ONCE(tun->vnet_hdr_sz);\n\n\t\tif (len < vnet_hdr_sz)\n\t\t\treturn -EINVAL;\n\t\tlen -= vnet_hdr_sz;\n\n\t\tif (!copy_from_iter_full(&gso, sizeof(gso), from))\n\t\t\treturn -EFAULT;\n\n\t\tif ((gso.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\n\t\t    tun16_to_cpu(tun, gso.csum_start) + tun16_to_cpu(tun, gso.csum_offset) + 2 > tun16_to_cpu(tun, gso.hdr_len))\n\t\t\tgso.hdr_len = cpu_to_tun16(tun, tun16_to_cpu(tun, gso.csum_start) + tun16_to_cpu(tun, gso.csum_offset) + 2);\n\n\t\tif (tun16_to_cpu(tun, gso.hdr_len) > len)\n\t\t\treturn -EINVAL;\n\t\tiov_iter_advance(from, vnet_hdr_sz - sizeof(gso));\n\t}\n\n\tif ((tun->flags & TUN_TYPE_MASK) == IFF_TAP) {\n\t\talign += NET_IP_ALIGN;\n\t\tif (unlikely(len < ETH_HLEN ||\n\t\t\t     (gso.hdr_len && tun16_to_cpu(tun, gso.hdr_len) < ETH_HLEN)))\n\t\t\treturn -EINVAL;\n\t}\n\n\tgood_linear = SKB_MAX_HEAD(align);\n\n\tif (msg_control) {\n\t\tstruct iov_iter i = *from;\n\n\t\t/* There are 256 bytes to be copied in skb, so there is\n\t\t * enough room for skb expand head in case it is used.\n\t\t * The rest of the buffer is mapped from userspace.\n\t\t */\n\t\tcopylen = gso.hdr_len ? tun16_to_cpu(tun, gso.hdr_len) : GOODCOPY_LEN;\n\t\tif (copylen > good_linear)\n\t\t\tcopylen = good_linear;\n\t\tlinear = copylen;\n\t\tiov_iter_advance(&i, copylen);\n\t\tif (iov_iter_npages(&i, INT_MAX) <= MAX_SKB_FRAGS)\n\t\t\tzerocopy = true;\n\t}\n\n\tif (!frags && tun_can_build_skb(tun, tfile, len, noblock, zerocopy)) {\n\t\t/* For the packet that is not easy to be processed\n\t\t * (e.g gso or jumbo packet), we will do it at after\n\t\t * skb was created with generic XDP routine.\n\t\t */\n\t\tskb = tun_build_skb(tun, tfile, from, &gso, len, &skb_xdp);\n\t\tif (IS_ERR(skb)) {\n\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\treturn PTR_ERR(skb);\n\t\t}\n\t\tif (!skb)\n\t\t\treturn total_len;\n\t} else {\n\t\tif (!zerocopy) {\n\t\t\tcopylen = len;\n\t\t\tif (tun16_to_cpu(tun, gso.hdr_len) > good_linear)\n\t\t\t\tlinear = good_linear;\n\t\t\telse\n\t\t\t\tlinear = tun16_to_cpu(tun, gso.hdr_len);\n\t\t}\n\n\t\tif (frags) {\n\t\t\tmutex_lock(&tfile->napi_mutex);\n\t\t\tskb = tun_napi_alloc_frags(tfile, copylen, from);\n\t\t\t/* tun_napi_alloc_frags() enforces a layout for the skb.\n\t\t\t * If zerocopy is enabled, then this layout will be\n\t\t\t * overwritten by zerocopy_sg_from_iter().\n\t\t\t */\n\t\t\tzerocopy = false;\n\t\t} else {\n\t\t\tskb = tun_alloc_skb(tfile, align, copylen, linear,\n\t\t\t\t\t    noblock);\n\t\t}\n\n\t\tif (IS_ERR(skb)) {\n\t\t\tif (PTR_ERR(skb) != -EAGAIN)\n\t\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\tif (frags)\n\t\t\t\tmutex_unlock(&tfile->napi_mutex);\n\t\t\treturn PTR_ERR(skb);\n\t\t}\n\n\t\tif (zerocopy)\n\t\t\terr = zerocopy_sg_from_iter(skb, from);\n\t\telse\n\t\t\terr = skb_copy_datagram_from_iter(skb, 0, from, len);\n\n\t\tif (err) {\n\t\t\terr = -EFAULT;\ndrop:\n\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\tkfree_skb(skb);\n\t\t\tif (frags) {\n\t\t\t\ttfile->napi.skb = NULL;\n\t\t\t\tmutex_unlock(&tfile->napi_mutex);\n\t\t\t}\n\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (virtio_net_hdr_to_skb(skb, &gso, tun_is_little_endian(tun))) {\n\t\tthis_cpu_inc(tun->pcpu_stats->rx_frame_errors);\n\t\tkfree_skb(skb);\n\t\tif (frags) {\n\t\t\ttfile->napi.skb = NULL;\n\t\t\tmutex_unlock(&tfile->napi_mutex);\n\t\t}\n\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (tun->flags & TUN_TYPE_MASK) {\n\tcase IFF_TUN:\n\t\tif (tun->flags & IFF_NO_PI) {\n\t\t\tu8 ip_version = skb->len ? (skb->data[0] >> 4) : 0;\n\n\t\t\tswitch (ip_version) {\n\t\t\tcase 4:\n\t\t\t\tpi.proto = htons(ETH_P_IP);\n\t\t\t\tbreak;\n\t\t\tcase 6:\n\t\t\t\tpi.proto = htons(ETH_P_IPV6);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tskb_reset_mac_header(skb);\n\t\tskb->protocol = pi.proto;\n\t\tskb->dev = tun->dev;\n\t\tbreak;\n\tcase IFF_TAP:\n\t\tif (!frags)\n\t\t\tskb->protocol = eth_type_trans(skb, tun->dev);\n\t\tbreak;\n\t}\n\n\t/* copy skb_ubuf_info for callback when skb has no error */\n\tif (zerocopy) {\n\t\tskb_shinfo(skb)->destructor_arg = msg_control;\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_DEV_ZEROCOPY;\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_SHARED_FRAG;\n\t} else if (msg_control) {\n\t\tstruct ubuf_info *uarg = msg_control;\n\t\tuarg->callback(uarg, false);\n\t}\n\n\tskb_reset_network_header(skb);\n\tskb_probe_transport_header(skb);\n\tskb_record_rx_queue(skb, tfile->queue_index);\n\n\tif (skb_xdp) {\n\t\tstruct bpf_prog *xdp_prog;\n\t\tint ret;\n\n\t\tlocal_bh_disable();\n\t\trcu_read_lock();\n\t\txdp_prog = rcu_dereference(tun->xdp_prog);\n\t\tif (xdp_prog) {\n\t\t\tret = do_xdp_generic(xdp_prog, skb);\n\t\t\tif (ret != XDP_PASS) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tif (frags) {\n\t\t\t\t\ttfile->napi.skb = NULL;\n\t\t\t\t\tmutex_unlock(&tfile->napi_mutex);\n\t\t\t\t}\n\t\t\t\treturn total_len;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t\tlocal_bh_enable();\n\t}\n\n\t/* Compute the costly rx hash only if needed for flow updates.\n\t * We may get a very small possibility of OOO during switching, not\n\t * worth to optimize.\n\t */\n\tif (!rcu_access_pointer(tun->steering_prog) && tun->numqueues > 1 &&\n\t    !tfile->detached)\n\t\trxhash = __skb_get_hash_symmetric(skb);\n\n\trcu_read_lock();\n\tif (unlikely(!(tun->dev->flags & IFF_UP))) {\n\t\terr = -EIO;\n\t\trcu_read_unlock();\n\t\tgoto drop;\n\t}\n\n\tif (frags) {\n\t\t/* Exercise flow dissector code path. */\n\t\tu32 headlen = eth_get_headlen(tun->dev, skb->data,\n\t\t\t\t\t      skb_headlen(skb));\n\n\t\tif (unlikely(headlen > skb_headlen(skb))) {\n\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\tnapi_free_frags(&tfile->napi);\n\t\t\trcu_read_unlock();\n\t\t\tmutex_unlock(&tfile->napi_mutex);\n\t\t\tWARN_ON(1);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tlocal_bh_disable();\n\t\tnapi_gro_frags(&tfile->napi);\n\t\tlocal_bh_enable();\n\t\tmutex_unlock(&tfile->napi_mutex);\n\t} else if (tfile->napi_enabled) {\n\t\tstruct sk_buff_head *queue = &tfile->sk.sk_write_queue;\n\t\tint queue_len;\n\n\t\tspin_lock_bh(&queue->lock);\n\t\t__skb_queue_tail(queue, skb);\n\t\tqueue_len = skb_queue_len(queue);\n\t\tspin_unlock(&queue->lock);\n\n\t\tif (!more || queue_len > NAPI_POLL_WEIGHT)\n\t\t\tnapi_schedule(&tfile->napi);\n\n\t\tlocal_bh_enable();\n\t} else if (!IS_ENABLED(CONFIG_4KSTACKS)) {\n\t\ttun_rx_batched(tun, tfile, skb, more);\n\t} else {\n\t\tnetif_rx_ni(skb);\n\t}\n\trcu_read_unlock();\n\n\tstats = get_cpu_ptr(tun->pcpu_stats);\n\tu64_stats_update_begin(&stats->syncp);\n\tu64_stats_inc(&stats->rx_packets);\n\tu64_stats_add(&stats->rx_bytes, len);\n\tu64_stats_update_end(&stats->syncp);\n\tput_cpu_ptr(stats);\n\n\tif (rxhash)\n\t\ttun_flow_update(tun, rxhash, tfile);\n\n\treturn total_len;\n}",
        "code_after_change": "static ssize_t tun_get_user(struct tun_struct *tun, struct tun_file *tfile,\n\t\t\t    void *msg_control, struct iov_iter *from,\n\t\t\t    int noblock, bool more)\n{\n\tstruct tun_pi pi = { 0, cpu_to_be16(ETH_P_IP) };\n\tstruct sk_buff *skb;\n\tsize_t total_len = iov_iter_count(from);\n\tsize_t len = total_len, align = tun->align, linear;\n\tstruct virtio_net_hdr gso = { 0 };\n\tstruct tun_pcpu_stats *stats;\n\tint good_linear;\n\tint copylen;\n\tbool zerocopy = false;\n\tint err;\n\tu32 rxhash = 0;\n\tint skb_xdp = 1;\n\tbool frags = tun_napi_frags_enabled(tfile);\n\n\tif (!(tun->flags & IFF_NO_PI)) {\n\t\tif (len < sizeof(pi))\n\t\t\treturn -EINVAL;\n\t\tlen -= sizeof(pi);\n\n\t\tif (!copy_from_iter_full(&pi, sizeof(pi), from))\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (tun->flags & IFF_VNET_HDR) {\n\t\tint vnet_hdr_sz = READ_ONCE(tun->vnet_hdr_sz);\n\n\t\tif (len < vnet_hdr_sz)\n\t\t\treturn -EINVAL;\n\t\tlen -= vnet_hdr_sz;\n\n\t\tif (!copy_from_iter_full(&gso, sizeof(gso), from))\n\t\t\treturn -EFAULT;\n\n\t\tif ((gso.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\n\t\t    tun16_to_cpu(tun, gso.csum_start) + tun16_to_cpu(tun, gso.csum_offset) + 2 > tun16_to_cpu(tun, gso.hdr_len))\n\t\t\tgso.hdr_len = cpu_to_tun16(tun, tun16_to_cpu(tun, gso.csum_start) + tun16_to_cpu(tun, gso.csum_offset) + 2);\n\n\t\tif (tun16_to_cpu(tun, gso.hdr_len) > len)\n\t\t\treturn -EINVAL;\n\t\tiov_iter_advance(from, vnet_hdr_sz - sizeof(gso));\n\t}\n\n\tif ((tun->flags & TUN_TYPE_MASK) == IFF_TAP) {\n\t\talign += NET_IP_ALIGN;\n\t\tif (unlikely(len < ETH_HLEN ||\n\t\t\t     (gso.hdr_len && tun16_to_cpu(tun, gso.hdr_len) < ETH_HLEN)))\n\t\t\treturn -EINVAL;\n\t}\n\n\tgood_linear = SKB_MAX_HEAD(align);\n\n\tif (msg_control) {\n\t\tstruct iov_iter i = *from;\n\n\t\t/* There are 256 bytes to be copied in skb, so there is\n\t\t * enough room for skb expand head in case it is used.\n\t\t * The rest of the buffer is mapped from userspace.\n\t\t */\n\t\tcopylen = gso.hdr_len ? tun16_to_cpu(tun, gso.hdr_len) : GOODCOPY_LEN;\n\t\tif (copylen > good_linear)\n\t\t\tcopylen = good_linear;\n\t\tlinear = copylen;\n\t\tiov_iter_advance(&i, copylen);\n\t\tif (iov_iter_npages(&i, INT_MAX) <= MAX_SKB_FRAGS)\n\t\t\tzerocopy = true;\n\t}\n\n\tif (!frags && tun_can_build_skb(tun, tfile, len, noblock, zerocopy)) {\n\t\t/* For the packet that is not easy to be processed\n\t\t * (e.g gso or jumbo packet), we will do it at after\n\t\t * skb was created with generic XDP routine.\n\t\t */\n\t\tskb = tun_build_skb(tun, tfile, from, &gso, len, &skb_xdp);\n\t\tif (IS_ERR(skb)) {\n\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\treturn PTR_ERR(skb);\n\t\t}\n\t\tif (!skb)\n\t\t\treturn total_len;\n\t} else {\n\t\tif (!zerocopy) {\n\t\t\tcopylen = len;\n\t\t\tif (tun16_to_cpu(tun, gso.hdr_len) > good_linear)\n\t\t\t\tlinear = good_linear;\n\t\t\telse\n\t\t\t\tlinear = tun16_to_cpu(tun, gso.hdr_len);\n\t\t}\n\n\t\tif (frags) {\n\t\t\tmutex_lock(&tfile->napi_mutex);\n\t\t\tskb = tun_napi_alloc_frags(tfile, copylen, from);\n\t\t\t/* tun_napi_alloc_frags() enforces a layout for the skb.\n\t\t\t * If zerocopy is enabled, then this layout will be\n\t\t\t * overwritten by zerocopy_sg_from_iter().\n\t\t\t */\n\t\t\tzerocopy = false;\n\t\t} else {\n\t\t\tskb = tun_alloc_skb(tfile, align, copylen, linear,\n\t\t\t\t\t    noblock);\n\t\t}\n\n\t\tif (IS_ERR(skb)) {\n\t\t\tif (PTR_ERR(skb) != -EAGAIN)\n\t\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\tif (frags)\n\t\t\t\tmutex_unlock(&tfile->napi_mutex);\n\t\t\treturn PTR_ERR(skb);\n\t\t}\n\n\t\tif (zerocopy)\n\t\t\terr = zerocopy_sg_from_iter(skb, from);\n\t\telse\n\t\t\terr = skb_copy_datagram_from_iter(skb, 0, from, len);\n\n\t\tif (err) {\n\t\t\terr = -EFAULT;\ndrop:\n\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\tkfree_skb(skb);\n\t\t\tif (frags) {\n\t\t\t\ttfile->napi.skb = NULL;\n\t\t\t\tmutex_unlock(&tfile->napi_mutex);\n\t\t\t}\n\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (virtio_net_hdr_to_skb(skb, &gso, tun_is_little_endian(tun))) {\n\t\tthis_cpu_inc(tun->pcpu_stats->rx_frame_errors);\n\t\tkfree_skb(skb);\n\t\tif (frags) {\n\t\t\ttfile->napi.skb = NULL;\n\t\t\tmutex_unlock(&tfile->napi_mutex);\n\t\t}\n\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (tun->flags & TUN_TYPE_MASK) {\n\tcase IFF_TUN:\n\t\tif (tun->flags & IFF_NO_PI) {\n\t\t\tu8 ip_version = skb->len ? (skb->data[0] >> 4) : 0;\n\n\t\t\tswitch (ip_version) {\n\t\t\tcase 4:\n\t\t\t\tpi.proto = htons(ETH_P_IP);\n\t\t\t\tbreak;\n\t\t\tcase 6:\n\t\t\t\tpi.proto = htons(ETH_P_IPV6);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tskb_reset_mac_header(skb);\n\t\tskb->protocol = pi.proto;\n\t\tskb->dev = tun->dev;\n\t\tbreak;\n\tcase IFF_TAP:\n\t\tif (frags && !pskb_may_pull(skb, ETH_HLEN)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto drop;\n\t\t}\n\t\tskb->protocol = eth_type_trans(skb, tun->dev);\n\t\tbreak;\n\t}\n\n\t/* copy skb_ubuf_info for callback when skb has no error */\n\tif (zerocopy) {\n\t\tskb_shinfo(skb)->destructor_arg = msg_control;\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_DEV_ZEROCOPY;\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_SHARED_FRAG;\n\t} else if (msg_control) {\n\t\tstruct ubuf_info *uarg = msg_control;\n\t\tuarg->callback(uarg, false);\n\t}\n\n\tskb_reset_network_header(skb);\n\tskb_probe_transport_header(skb);\n\tskb_record_rx_queue(skb, tfile->queue_index);\n\n\tif (skb_xdp) {\n\t\tstruct bpf_prog *xdp_prog;\n\t\tint ret;\n\n\t\tlocal_bh_disable();\n\t\trcu_read_lock();\n\t\txdp_prog = rcu_dereference(tun->xdp_prog);\n\t\tif (xdp_prog) {\n\t\t\tret = do_xdp_generic(xdp_prog, skb);\n\t\t\tif (ret != XDP_PASS) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tif (frags) {\n\t\t\t\t\ttfile->napi.skb = NULL;\n\t\t\t\t\tmutex_unlock(&tfile->napi_mutex);\n\t\t\t\t}\n\t\t\t\treturn total_len;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t\tlocal_bh_enable();\n\t}\n\n\t/* Compute the costly rx hash only if needed for flow updates.\n\t * We may get a very small possibility of OOO during switching, not\n\t * worth to optimize.\n\t */\n\tif (!rcu_access_pointer(tun->steering_prog) && tun->numqueues > 1 &&\n\t    !tfile->detached)\n\t\trxhash = __skb_get_hash_symmetric(skb);\n\n\trcu_read_lock();\n\tif (unlikely(!(tun->dev->flags & IFF_UP))) {\n\t\terr = -EIO;\n\t\trcu_read_unlock();\n\t\tgoto drop;\n\t}\n\n\tif (frags) {\n\t\tu32 headlen;\n\n\t\t/* Exercise flow dissector code path. */\n\t\tskb_push(skb, ETH_HLEN);\n\t\theadlen = eth_get_headlen(tun->dev, skb->data,\n\t\t\t\t\t  skb_headlen(skb));\n\n\t\tif (unlikely(headlen > skb_headlen(skb))) {\n\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\tnapi_free_frags(&tfile->napi);\n\t\t\trcu_read_unlock();\n\t\t\tmutex_unlock(&tfile->napi_mutex);\n\t\t\tWARN_ON(1);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tlocal_bh_disable();\n\t\tnapi_gro_frags(&tfile->napi);\n\t\tlocal_bh_enable();\n\t\tmutex_unlock(&tfile->napi_mutex);\n\t} else if (tfile->napi_enabled) {\n\t\tstruct sk_buff_head *queue = &tfile->sk.sk_write_queue;\n\t\tint queue_len;\n\n\t\tspin_lock_bh(&queue->lock);\n\t\t__skb_queue_tail(queue, skb);\n\t\tqueue_len = skb_queue_len(queue);\n\t\tspin_unlock(&queue->lock);\n\n\t\tif (!more || queue_len > NAPI_POLL_WEIGHT)\n\t\t\tnapi_schedule(&tfile->napi);\n\n\t\tlocal_bh_enable();\n\t} else if (!IS_ENABLED(CONFIG_4KSTACKS)) {\n\t\ttun_rx_batched(tun, tfile, skb, more);\n\t} else {\n\t\tnetif_rx_ni(skb);\n\t}\n\trcu_read_unlock();\n\n\tstats = get_cpu_ptr(tun->pcpu_stats);\n\tu64_stats_update_begin(&stats->syncp);\n\tu64_stats_inc(&stats->rx_packets);\n\tu64_stats_add(&stats->rx_bytes, len);\n\tu64_stats_update_end(&stats->syncp);\n\tput_cpu_ptr(stats);\n\n\tif (rxhash)\n\t\ttun_flow_update(tun, rxhash, tfile);\n\n\treturn total_len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -165,8 +165,11 @@\n \t\tskb->dev = tun->dev;\n \t\tbreak;\n \tcase IFF_TAP:\n-\t\tif (!frags)\n-\t\t\tskb->protocol = eth_type_trans(skb, tun->dev);\n+\t\tif (frags && !pskb_may_pull(skb, ETH_HLEN)) {\n+\t\t\terr = -ENOMEM;\n+\t\t\tgoto drop;\n+\t\t}\n+\t\tskb->protocol = eth_type_trans(skb, tun->dev);\n \t\tbreak;\n \t}\n \n@@ -223,9 +226,12 @@\n \t}\n \n \tif (frags) {\n+\t\tu32 headlen;\n+\n \t\t/* Exercise flow dissector code path. */\n-\t\tu32 headlen = eth_get_headlen(tun->dev, skb->data,\n-\t\t\t\t\t      skb_headlen(skb));\n+\t\tskb_push(skb, ETH_HLEN);\n+\t\theadlen = eth_get_headlen(tun->dev, skb->data,\n+\t\t\t\t\t  skb_headlen(skb));\n \n \t\tif (unlikely(headlen > skb_headlen(skb))) {\n \t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);",
        "function_modified_lines": {
            "added": [
                "\t\tif (frags && !pskb_may_pull(skb, ETH_HLEN)) {",
                "\t\t\terr = -ENOMEM;",
                "\t\t\tgoto drop;",
                "\t\t}",
                "\t\tskb->protocol = eth_type_trans(skb, tun->dev);",
                "\t\tu32 headlen;",
                "",
                "\t\tskb_push(skb, ETH_HLEN);",
                "\t\theadlen = eth_get_headlen(tun->dev, skb->data,",
                "\t\t\t\t\t  skb_headlen(skb));"
            ],
            "deleted": [
                "\t\tif (!frags)",
                "\t\t\tskb->protocol = eth_type_trans(skb, tun->dev);",
                "\t\tu32 headlen = eth_get_headlen(tun->dev, skb->data,",
                "\t\t\t\t\t      skb_headlen(skb));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In tun_get_user of tun.c, there is possible memory corruption due to a use after free. This could lead to local escalation of privilege with System execution privileges required. User interaction is not required for exploitation. Product: Android; Versions: Android kernel; Android ID: A-146554327.",
        "id": 2816
    },
    {
        "cve_id": "CVE-2022-1419",
        "code_before_change": "static struct drm_gem_object *vgem_gem_create(struct drm_device *dev,\n\t\t\t\t\t      struct drm_file *file,\n\t\t\t\t\t      unsigned int *handle,\n\t\t\t\t\t      unsigned long size)\n{\n\tstruct drm_vgem_gem_object *obj;\n\tint ret;\n\n\tobj = __vgem_gem_create(dev, size);\n\tif (IS_ERR(obj))\n\t\treturn ERR_CAST(obj);\n\n\tret = drm_gem_handle_create(file, &obj->base, handle);\n\tdrm_gem_object_put_unlocked(&obj->base);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\treturn &obj->base;\n}",
        "code_after_change": "static struct drm_gem_object *vgem_gem_create(struct drm_device *dev,\n\t\t\t\t\t      struct drm_file *file,\n\t\t\t\t\t      unsigned int *handle,\n\t\t\t\t\t      unsigned long size)\n{\n\tstruct drm_vgem_gem_object *obj;\n\tint ret;\n\n\tobj = __vgem_gem_create(dev, size);\n\tif (IS_ERR(obj))\n\t\treturn ERR_CAST(obj);\n\n\tret = drm_gem_handle_create(file, &obj->base, handle);\n\tif (ret) {\n\t\tdrm_gem_object_put_unlocked(&obj->base);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn &obj->base;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,9 +11,10 @@\n \t\treturn ERR_CAST(obj);\n \n \tret = drm_gem_handle_create(file, &obj->base, handle);\n-\tdrm_gem_object_put_unlocked(&obj->base);\n-\tif (ret)\n+\tif (ret) {\n+\t\tdrm_gem_object_put_unlocked(&obj->base);\n \t\treturn ERR_PTR(ret);\n+\t}\n \n \treturn &obj->base;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (ret) {",
                "\t\tdrm_gem_object_put_unlocked(&obj->base);",
                "\t}"
            ],
            "deleted": [
                "\tdrm_gem_object_put_unlocked(&obj->base);",
                "\tif (ret)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The root cause of this vulnerability is that the ioctl$DRM_IOCTL_MODE_DESTROY_DUMB can decrease refcount of *drm_vgem_gem_object *(created in *vgem_gem_dumb_create*) concurrently, and *vgem_gem_dumb_create *will access the freed drm_vgem_gem_object.",
        "id": 3259
    },
    {
        "cve_id": "CVE-2015-8963",
        "code_before_change": "static int swevent_hlist_get_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\tint err = 0;\n\n\tmutex_lock(&swhash->hlist_mutex);\n\n\tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc(sizeof(*hlist), GFP_KERNEL);\n\t\tif (!hlist) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tswhash->hlist_refcount++;\nexit:\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\treturn err;\n}",
        "code_after_change": "static int swevent_hlist_get_cpu(struct perf_event *event, int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\tint err = 0;\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc(sizeof(*hlist), GFP_KERNEL);\n\t\tif (!hlist) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tswhash->hlist_refcount++;\nexit:\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \tint err = 0;\n \n \tmutex_lock(&swhash->hlist_mutex);\n-\n \tif (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {\n \t\tstruct swevent_hlist *hlist;\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation.",
        "id": 873
    },
    {
        "cve_id": "CVE-2020-27784",
        "code_before_change": "static int\nprinter_close(struct inode *inode, struct file *fd)\n{\n\tstruct printer_dev\t*dev = fd->private_data;\n\tunsigned long\t\tflags;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\tdev->printer_cdev_open = 0;\n\tfd->private_data = NULL;\n\t/* Change printer status to show that the printer is off-line. */\n\tdev->printer_status &= ~PRINTER_SELECTED;\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\tDBG(dev, \"printer_close\\n\");\n\n\treturn 0;\n}",
        "code_after_change": "static int\nprinter_close(struct inode *inode, struct file *fd)\n{\n\tstruct printer_dev\t*dev = fd->private_data;\n\tunsigned long\t\tflags;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\tdev->printer_cdev_open = 0;\n\tfd->private_data = NULL;\n\t/* Change printer status to show that the printer is off-line. */\n\tdev->printer_status &= ~PRINTER_SELECTED;\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\tkref_put(&dev->kref, printer_dev_free);\n\tDBG(dev, \"printer_close\\n\");\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,7 @@\n \tdev->printer_status &= ~PRINTER_SELECTED;\n \tspin_unlock_irqrestore(&dev->lock, flags);\n \n+\tkref_put(&dev->kref, printer_dev_free);\n \tDBG(dev, \"printer_close\\n\");\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tkref_put(&dev->kref, printer_dev_free);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel, where accessing a deallocated instance in printer_ioctl() printer_ioctl() tries to access of a printer_dev instance. However, use-after-free arises because it had been freed by gprinter_free().",
        "id": 2630
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "void vivid_stop_generating_vid_out(struct vivid_dev *dev, bool *pstreaming)\n{\n\tdprintk(dev, 1, \"%s\\n\", __func__);\n\n\tif (dev->kthread_vid_out == NULL)\n\t\treturn;\n\n\t*pstreaming = false;\n\tif (pstreaming == &dev->vid_out_streaming) {\n\t\t/* Release all active buffers */\n\t\twhile (!list_empty(&dev->vid_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vid_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vid_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vid_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->vbi_out_streaming) {\n\t\twhile (!list_empty(&dev->vbi_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vbi_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vbi_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vbi_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->meta_out_streaming) {\n\t\twhile (!list_empty(&dev->meta_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->meta_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_meta_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"meta_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (dev->vid_out_streaming || dev->vbi_out_streaming ||\n\t    dev->meta_out_streaming)\n\t\treturn;\n\n\t/* shutdown control thread */\n\tvivid_grab_controls(dev, false);\n\tmutex_unlock(&dev->mutex);\n\tkthread_stop(dev->kthread_vid_out);\n\tdev->kthread_vid_out = NULL;\n\tmutex_lock(&dev->mutex);\n}",
        "code_after_change": "void vivid_stop_generating_vid_out(struct vivid_dev *dev, bool *pstreaming)\n{\n\tdprintk(dev, 1, \"%s\\n\", __func__);\n\n\tif (dev->kthread_vid_out == NULL)\n\t\treturn;\n\n\t*pstreaming = false;\n\tif (pstreaming == &dev->vid_out_streaming) {\n\t\t/* Release all active buffers */\n\t\twhile (!list_empty(&dev->vid_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vid_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vid_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vid_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->vbi_out_streaming) {\n\t\twhile (!list_empty(&dev->vbi_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vbi_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vbi_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vbi_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->meta_out_streaming) {\n\t\twhile (!list_empty(&dev->meta_out_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->meta_out_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_meta_out);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"meta_out buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (dev->vid_out_streaming || dev->vbi_out_streaming ||\n\t    dev->meta_out_streaming)\n\t\treturn;\n\n\t/* shutdown control thread */\n\tvivid_grab_controls(dev, false);\n\tkthread_stop(dev->kthread_vid_out);\n\tdev->kthread_vid_out = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -58,8 +58,6 @@\n \n \t/* shutdown control thread */\n \tvivid_grab_controls(dev, false);\n-\tmutex_unlock(&dev->mutex);\n \tkthread_stop(dev->kthread_vid_out);\n \tdev->kthread_vid_out = NULL;\n-\tmutex_lock(&dev->mutex);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmutex_unlock(&dev->mutex);",
                "\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free.",
        "id": 2094
    },
    {
        "cve_id": "CVE-2019-2025",
        "code_before_change": "static int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tint ret;\n\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tconst char *debug_string;\n\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n\t\t\tstruct binder_ref_data rdata;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tret = -1;\n\t\t\tif (increment && !target) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node)\n\t\t\t\t\tret = binder_inc_ref_for_node(\n\t\t\t\t\t\t\tproc, ctx_mgr_node,\n\t\t\t\t\t\t\tstrong, NULL, &rdata);\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tret = binder_update_ref_for_handle(\n\t\t\t\t\t\tproc, target, increment, strong,\n\t\t\t\t\t\t&rdata);\n\t\t\tif (!ret && rdata.desc != target) {\n\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\ttarget, rdata.desc);\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, debug_string,\n\t\t\t\t\tstrong, target, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string,\n\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n\t\t\t\t     rdata.weak);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\t\t\tbool free_node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbinder_put_node(node);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_node_inner_lock(node);\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tfree_node = binder_dec_node_nilocked(node,\n\t\t\t\t\tcmd == BC_ACQUIRE_DONE, 0);\n\t\t\tWARN_ON(free_node);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d tr %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs,\n\t\t\t\t     node->local_weak_refs, node->tmp_refs);\n\t\t\tbinder_node_inner_unlock(node);\n\t\t\tbinder_put_node(node);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (buffer == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!buffer->allow_user_free) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned buffer\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\t\t\tbinder_free_buf(proc, buffer);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\t/*\n\t\t\t\t * Allocate memory for death notification\n\t\t\t\t * before taking lock\n\t\t\t\t */\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tbinder_enqueue_thread_work(\n\t\t\t\t\t\tthread,\n\t\t\t\t\t\t&thread->return_error.work);\n\t\t\t\t\tbinder_debug(\n\t\t\t\t\t\tBINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t\"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->data.debug_id,\n\t\t\t\t     ref->data.desc, ref->data.strong,\n\t\t\t\t     ref->data.weak, ref->node->debug_id);\n\n\t\t\tbinder_node_lock(ref->node);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tkfree(death);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\n\t\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t&ref->death->work, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper &\n\t\t\t\t\t    (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t     BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\t\t\tthread,\n\t\t\t\t\t\t\t\t&death->work);\n\t\t\t\t\telse {\n\t\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\t\tbinder_wakeup_proc_ilocked(\n\t\t\t\t\t\t\t\tproc);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tbinder_node_unlock(ref->node);\n\t\t\tbinder_proc_unlock(proc);\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death,\n\t\t\t\t\t    entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death =\n\t\t\t\t\tcontainer_of(w,\n\t\t\t\t\t\t     struct binder_ref_death,\n\t\t\t\t\t\t     work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_dequeue_work_ilocked(&death->work);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper &\n\t\t\t\t\t(BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\tthread, &death->work);\n\t\t\t\telse {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tint ret;\n\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tconst char *debug_string;\n\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n\t\t\tstruct binder_ref_data rdata;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tret = -1;\n\t\t\tif (increment && !target) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node)\n\t\t\t\t\tret = binder_inc_ref_for_node(\n\t\t\t\t\t\t\tproc, ctx_mgr_node,\n\t\t\t\t\t\t\tstrong, NULL, &rdata);\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tret = binder_update_ref_for_handle(\n\t\t\t\t\t\tproc, target, increment, strong,\n\t\t\t\t\t\t&rdata);\n\t\t\tif (!ret && rdata.desc != target) {\n\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\ttarget, rdata.desc);\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, debug_string,\n\t\t\t\t\tstrong, target, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string,\n\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n\t\t\t\t     rdata.weak);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\t\t\tbool free_node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbinder_put_node(node);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_node_inner_lock(node);\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tfree_node = binder_dec_node_nilocked(node,\n\t\t\t\t\tcmd == BC_ACQUIRE_DONE, 0);\n\t\t\tWARN_ON(free_node);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d tr %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs,\n\t\t\t\t     node->local_weak_refs, node->tmp_refs);\n\t\t\tbinder_node_inner_unlock(node);\n\t\t\tbinder_put_node(node);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (IS_ERR_OR_NULL(buffer)) {\n\t\t\t\tif (PTR_ERR(buffer) == -EPERM) {\n\t\t\t\t\tbinder_user_error(\n\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned or currently freeing buffer\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)data_ptr);\n\t\t\t\t} else {\n\t\t\t\t\tbinder_user_error(\n\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)data_ptr);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\t\t\tbinder_free_buf(proc, buffer);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\t/*\n\t\t\t\t * Allocate memory for death notification\n\t\t\t\t * before taking lock\n\t\t\t\t */\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tbinder_enqueue_thread_work(\n\t\t\t\t\t\tthread,\n\t\t\t\t\t\t&thread->return_error.work);\n\t\t\t\t\tbinder_debug(\n\t\t\t\t\t\tBINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t\"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->data.debug_id,\n\t\t\t\t     ref->data.desc, ref->data.strong,\n\t\t\t\t     ref->data.weak, ref->node->debug_id);\n\n\t\t\tbinder_node_lock(ref->node);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tkfree(death);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\n\t\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t&ref->death->work, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper &\n\t\t\t\t\t    (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t     BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\t\t\tthread,\n\t\t\t\t\t\t\t\t&death->work);\n\t\t\t\t\telse {\n\t\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\t\tbinder_wakeup_proc_ilocked(\n\t\t\t\t\t\t\t\tproc);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tbinder_node_unlock(ref->node);\n\t\t\tbinder_proc_unlock(proc);\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death,\n\t\t\t\t\t    entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death =\n\t\t\t\t\tcontainer_of(w,\n\t\t\t\t\t\t     struct binder_ref_death,\n\t\t\t\t\t\t     work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_dequeue_work_ilocked(&death->work);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper &\n\t\t\t\t\t(BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\tthread, &death->work);\n\t\t\t\telse {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -169,14 +169,18 @@\n \n \t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n \t\t\t\t\t\t\t      data_ptr);\n-\t\t\tif (buffer == NULL) {\n-\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n-\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n-\t\t\t\tbreak;\n-\t\t\t}\n-\t\t\tif (!buffer->allow_user_free) {\n-\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned buffer\\n\",\n-\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n+\t\t\tif (IS_ERR_OR_NULL(buffer)) {\n+\t\t\t\tif (PTR_ERR(buffer) == -EPERM) {\n+\t\t\t\t\tbinder_user_error(\n+\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned or currently freeing buffer\\n\",\n+\t\t\t\t\t\tproc->pid, thread->pid,\n+\t\t\t\t\t\t(u64)data_ptr);\n+\t\t\t\t} else {\n+\t\t\t\t\tbinder_user_error(\n+\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n+\t\t\t\t\t\tproc->pid, thread->pid,\n+\t\t\t\t\t\t(u64)data_ptr);\n+\t\t\t\t}\n \t\t\t\tbreak;\n \t\t\t}\n \t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (IS_ERR_OR_NULL(buffer)) {",
                "\t\t\t\tif (PTR_ERR(buffer) == -EPERM) {",
                "\t\t\t\t\tbinder_user_error(",
                "\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned or currently freeing buffer\\n\",",
                "\t\t\t\t\t\tproc->pid, thread->pid,",
                "\t\t\t\t\t\t(u64)data_ptr);",
                "\t\t\t\t} else {",
                "\t\t\t\t\tbinder_user_error(",
                "\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",",
                "\t\t\t\t\t\tproc->pid, thread->pid,",
                "\t\t\t\t\t\t(u64)data_ptr);",
                "\t\t\t\t}"
            ],
            "deleted": [
                "\t\t\tif (buffer == NULL) {",
                "\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",",
                "\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);",
                "\t\t\t\tbreak;",
                "\t\t\t}",
                "\t\t\tif (!buffer->allow_user_free) {",
                "\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned buffer\\n\",",
                "\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_thread_read of binder.c, there is a possible use-after-free due to improper locking. This could lead to local escalation of privilege in the kernel with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-116855682References: Upstream kernel",
        "id": 2276
    },
    {
        "cve_id": "CVE-2018-5344",
        "code_before_change": "static void lo_release(struct gendisk *disk, fmode_t mode)\n{\n\tstruct loop_device *lo = disk->private_data;\n\tint err;\n\n\tif (atomic_dec_return(&lo->lo_refcnt))\n\t\treturn;\n\n\tmutex_lock(&lo->lo_ctl_mutex);\n\tif (lo->lo_flags & LO_FLAGS_AUTOCLEAR) {\n\t\t/*\n\t\t * In autoclear mode, stop the loop thread\n\t\t * and remove configuration after last close.\n\t\t */\n\t\terr = loop_clr_fd(lo);\n\t\tif (!err)\n\t\t\treturn;\n\t} else if (lo->lo_state == Lo_bound) {\n\t\t/*\n\t\t * Otherwise keep thread (if running) and config,\n\t\t * but flush possible ongoing bios in thread.\n\t\t */\n\t\tblk_mq_freeze_queue(lo->lo_queue);\n\t\tblk_mq_unfreeze_queue(lo->lo_queue);\n\t}\n\n\tmutex_unlock(&lo->lo_ctl_mutex);\n}",
        "code_after_change": "static void lo_release(struct gendisk *disk, fmode_t mode)\n{\n\tmutex_lock(&loop_index_mutex);\n\t__lo_release(disk->private_data);\n\tmutex_unlock(&loop_index_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,28 +1,6 @@\n static void lo_release(struct gendisk *disk, fmode_t mode)\n {\n-\tstruct loop_device *lo = disk->private_data;\n-\tint err;\n-\n-\tif (atomic_dec_return(&lo->lo_refcnt))\n-\t\treturn;\n-\n-\tmutex_lock(&lo->lo_ctl_mutex);\n-\tif (lo->lo_flags & LO_FLAGS_AUTOCLEAR) {\n-\t\t/*\n-\t\t * In autoclear mode, stop the loop thread\n-\t\t * and remove configuration after last close.\n-\t\t */\n-\t\terr = loop_clr_fd(lo);\n-\t\tif (!err)\n-\t\t\treturn;\n-\t} else if (lo->lo_state == Lo_bound) {\n-\t\t/*\n-\t\t * Otherwise keep thread (if running) and config,\n-\t\t * but flush possible ongoing bios in thread.\n-\t\t */\n-\t\tblk_mq_freeze_queue(lo->lo_queue);\n-\t\tblk_mq_unfreeze_queue(lo->lo_queue);\n-\t}\n-\n-\tmutex_unlock(&lo->lo_ctl_mutex);\n+\tmutex_lock(&loop_index_mutex);\n+\t__lo_release(disk->private_data);\n+\tmutex_unlock(&loop_index_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&loop_index_mutex);",
                "\t__lo_release(disk->private_data);",
                "\tmutex_unlock(&loop_index_mutex);"
            ],
            "deleted": [
                "\tstruct loop_device *lo = disk->private_data;",
                "\tint err;",
                "",
                "\tif (atomic_dec_return(&lo->lo_refcnt))",
                "\t\treturn;",
                "",
                "\tmutex_lock(&lo->lo_ctl_mutex);",
                "\tif (lo->lo_flags & LO_FLAGS_AUTOCLEAR) {",
                "\t\t/*",
                "\t\t * In autoclear mode, stop the loop thread",
                "\t\t * and remove configuration after last close.",
                "\t\t */",
                "\t\terr = loop_clr_fd(lo);",
                "\t\tif (!err)",
                "\t\t\treturn;",
                "\t} else if (lo->lo_state == Lo_bound) {",
                "\t\t/*",
                "\t\t * Otherwise keep thread (if running) and config,",
                "\t\t * but flush possible ongoing bios in thread.",
                "\t\t */",
                "\t\tblk_mq_freeze_queue(lo->lo_queue);",
                "\t\tblk_mq_unfreeze_queue(lo->lo_queue);",
                "\t}",
                "",
                "\tmutex_unlock(&lo->lo_ctl_mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel through 4.14.13, drivers/block/loop.c mishandles lo_release serialization, which allows attackers to cause a denial of service (__lock_acquire use-after-free) or possibly have unspecified other impact.",
        "id": 1821
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "struct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}",
        "code_after_change": "struct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,9 @@\n \tmemset(fl6, 0, sizeof(*fl6));\n \tfl6->flowi6_proto = proto;\n \tfl6->daddr = ireq->ir_v6_rmt_addr;\n-\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n+\trcu_read_lock();\n+\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n+\trcu_read_unlock();\n \tfl6->saddr = ireq->ir_v6_loc_addr;\n \tfl6->flowi6_oif = ireq->ir_iif;\n \tfl6->flowi6_mark = ireq->ir_mark;",
        "function_modified_lines": {
            "added": [
                "\trcu_read_lock();",
                "\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(fl6, np->opt, &final);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 998
    },
    {
        "cve_id": "CVE-2020-0429",
        "code_before_change": "void l2tp_tunnel_closeall(struct l2tp_tunnel *tunnel)\n{\n\tint hash;\n\tstruct hlist_node *walk;\n\tstruct hlist_node *tmp;\n\tstruct l2tp_session *session;\n\n\tBUG_ON(tunnel == NULL);\n\n\tl2tp_info(tunnel, L2TP_MSG_CONTROL, \"%s: closing all sessions...\\n\",\n\t\t  tunnel->name);\n\n\twrite_lock_bh(&tunnel->hlist_lock);\n\ttunnel->acpt_newsess = false;\n\tfor (hash = 0; hash < L2TP_HASH_SIZE; hash++) {\nagain:\n\t\thlist_for_each_safe(walk, tmp, &tunnel->session_hlist[hash]) {\n\t\t\tsession = hlist_entry(walk, struct l2tp_session, hlist);\n\n\t\t\tl2tp_info(session, L2TP_MSG_CONTROL,\n\t\t\t\t  \"%s: closing session\\n\", session->name);\n\n\t\t\thlist_del_init(&session->hlist);\n\n\t\t\tif (session->ref != NULL)\n\t\t\t\t(*session->ref)(session);\n\n\t\t\twrite_unlock_bh(&tunnel->hlist_lock);\n\n\t\t\t__l2tp_session_unhash(session);\n\t\t\tl2tp_session_queue_purge(session);\n\n\t\t\tif (session->session_close != NULL)\n\t\t\t\t(*session->session_close)(session);\n\n\t\t\tif (session->deref != NULL)\n\t\t\t\t(*session->deref)(session);\n\n\t\t\tl2tp_session_dec_refcount(session);\n\n\t\t\twrite_lock_bh(&tunnel->hlist_lock);\n\n\t\t\t/* Now restart from the beginning of this hash\n\t\t\t * chain.  We always remove a session from the\n\t\t\t * list so we are guaranteed to make forward\n\t\t\t * progress.\n\t\t\t */\n\t\t\tgoto again;\n\t\t}\n\t}\n\twrite_unlock_bh(&tunnel->hlist_lock);\n}",
        "code_after_change": "void l2tp_tunnel_closeall(struct l2tp_tunnel *tunnel)\n{\n\tint hash;\n\tstruct hlist_node *walk;\n\tstruct hlist_node *tmp;\n\tstruct l2tp_session *session;\n\n\tBUG_ON(tunnel == NULL);\n\n\tl2tp_info(tunnel, L2TP_MSG_CONTROL, \"%s: closing all sessions...\\n\",\n\t\t  tunnel->name);\n\n\twrite_lock_bh(&tunnel->hlist_lock);\n\ttunnel->acpt_newsess = false;\n\tfor (hash = 0; hash < L2TP_HASH_SIZE; hash++) {\nagain:\n\t\thlist_for_each_safe(walk, tmp, &tunnel->session_hlist[hash]) {\n\t\t\tsession = hlist_entry(walk, struct l2tp_session, hlist);\n\n\t\t\tl2tp_info(session, L2TP_MSG_CONTROL,\n\t\t\t\t  \"%s: closing session\\n\", session->name);\n\n\t\t\thlist_del_init(&session->hlist);\n\n\t\t\tif (test_and_set_bit(0, &session->dead))\n\t\t\t\tgoto again;\n\n\t\t\tif (session->ref != NULL)\n\t\t\t\t(*session->ref)(session);\n\n\t\t\twrite_unlock_bh(&tunnel->hlist_lock);\n\n\t\t\t__l2tp_session_unhash(session);\n\t\t\tl2tp_session_queue_purge(session);\n\n\t\t\tif (session->session_close != NULL)\n\t\t\t\t(*session->session_close)(session);\n\n\t\t\tif (session->deref != NULL)\n\t\t\t\t(*session->deref)(session);\n\n\t\t\tl2tp_session_dec_refcount(session);\n\n\t\t\twrite_lock_bh(&tunnel->hlist_lock);\n\n\t\t\t/* Now restart from the beginning of this hash\n\t\t\t * chain.  We always remove a session from the\n\t\t\t * list so we are guaranteed to make forward\n\t\t\t * progress.\n\t\t\t */\n\t\t\tgoto again;\n\t\t}\n\t}\n\twrite_unlock_bh(&tunnel->hlist_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,9 @@\n \t\t\t\t  \"%s: closing session\\n\", session->name);\n \n \t\t\thlist_del_init(&session->hlist);\n+\n+\t\t\tif (test_and_set_bit(0, &session->dead))\n+\t\t\t\tgoto again;\n \n \t\t\tif (session->ref != NULL)\n \t\t\t\t(*session->ref)(session);",
        "function_modified_lines": {
            "added": [
                "",
                "\t\t\tif (test_and_set_bit(0, &session->dead))",
                "\t\t\t\tgoto again;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-787",
            "CWE-416"
        ],
        "cve_description": "In l2tp_session_delete and related functions of l2tp_core.c, there is possible memory corruption due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152735806",
        "id": 2381
    },
    {
        "cve_id": "CVE-2018-14625",
        "code_before_change": "static int\nvhost_transport_cancel_pkt(struct vsock_sock *vsk)\n{\n\tstruct vhost_vsock *vsock;\n\tstruct virtio_vsock_pkt *pkt, *n;\n\tint cnt = 0;\n\tLIST_HEAD(freeme);\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(vsk->remote_addr.svm_cid);\n\tif (!vsock)\n\t\treturn -ENODEV;\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_for_each_entry_safe(pkt, n, &vsock->send_pkt_list, list) {\n\t\tif (pkt->vsk != vsk)\n\t\t\tcontinue;\n\t\tlist_move(&pkt->list, &freeme);\n\t}\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tlist_for_each_entry_safe(pkt, n, &freeme, list) {\n\t\tif (pkt->reply)\n\t\t\tcnt++;\n\t\tlist_del(&pkt->list);\n\t\tvirtio_transport_free_pkt(pkt);\n\t}\n\n\tif (cnt) {\n\t\tstruct vhost_virtqueue *tx_vq = &vsock->vqs[VSOCK_VQ_TX];\n\t\tint new_cnt;\n\n\t\tnew_cnt = atomic_sub_return(cnt, &vsock->queued_replies);\n\t\tif (new_cnt + cnt >= tx_vq->num && new_cnt < tx_vq->num)\n\t\t\tvhost_poll_queue(&tx_vq->poll);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int\nvhost_transport_cancel_pkt(struct vsock_sock *vsk)\n{\n\tstruct vhost_vsock *vsock;\n\tstruct virtio_vsock_pkt *pkt, *n;\n\tint cnt = 0;\n\tint ret = -ENODEV;\n\tLIST_HEAD(freeme);\n\n\trcu_read_lock();\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(vsk->remote_addr.svm_cid);\n\tif (!vsock)\n\t\tgoto out;\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_for_each_entry_safe(pkt, n, &vsock->send_pkt_list, list) {\n\t\tif (pkt->vsk != vsk)\n\t\t\tcontinue;\n\t\tlist_move(&pkt->list, &freeme);\n\t}\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tlist_for_each_entry_safe(pkt, n, &freeme, list) {\n\t\tif (pkt->reply)\n\t\t\tcnt++;\n\t\tlist_del(&pkt->list);\n\t\tvirtio_transport_free_pkt(pkt);\n\t}\n\n\tif (cnt) {\n\t\tstruct vhost_virtqueue *tx_vq = &vsock->vqs[VSOCK_VQ_TX];\n\t\tint new_cnt;\n\n\t\tnew_cnt = atomic_sub_return(cnt, &vsock->queued_replies);\n\t\tif (new_cnt + cnt >= tx_vq->num && new_cnt < tx_vq->num)\n\t\t\tvhost_poll_queue(&tx_vq->poll);\n\t}\n\n\tret = 0;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,12 +4,15 @@\n \tstruct vhost_vsock *vsock;\n \tstruct virtio_vsock_pkt *pkt, *n;\n \tint cnt = 0;\n+\tint ret = -ENODEV;\n \tLIST_HEAD(freeme);\n+\n+\trcu_read_lock();\n \n \t/* Find the vhost_vsock according to guest context id  */\n \tvsock = vhost_vsock_get(vsk->remote_addr.svm_cid);\n \tif (!vsock)\n-\t\treturn -ENODEV;\n+\t\tgoto out;\n \n \tspin_lock_bh(&vsock->send_pkt_list_lock);\n \tlist_for_each_entry_safe(pkt, n, &vsock->send_pkt_list, list) {\n@@ -35,5 +38,8 @@\n \t\t\tvhost_poll_queue(&tx_vq->poll);\n \t}\n \n-\treturn 0;\n+\tret = 0;\n+out:\n+\trcu_read_unlock();\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tint ret = -ENODEV;",
                "",
                "\trcu_read_lock();",
                "\t\tgoto out;",
                "\tret = 0;",
                "out:",
                "\trcu_read_unlock();",
                "\treturn ret;"
            ],
            "deleted": [
                "\t\treturn -ENODEV;",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux Kernel where an attacker may be able to have an uncontrolled read to kernel-memory from within a vm guest. A race condition between connect() and close() function may allow an attacker using the AF_VSOCK protocol to gather a 4 byte information leak or possibly intercept or corrupt AF_VSOCK messages destined to other clients.",
        "id": 1694
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static int io_register_personality(struct io_ring_ctx *ctx)\n{\n\tstruct io_identity *id;\n\tint ret;\n\n\tid = kmalloc(sizeof(*id), GFP_KERNEL);\n\tif (unlikely(!id))\n\t\treturn -ENOMEM;\n\n\tio_init_identity(id);\n\tid->creds = get_current_cred();\n\n\tret = idr_alloc_cyclic(&ctx->personality_idr, id, 1, USHRT_MAX, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tput_cred(id->creds);\n\t\tkfree(id);\n\t}\n\treturn ret;\n}",
        "code_after_change": "static int io_register_personality(struct io_ring_ctx *ctx)\n{\n\tconst struct cred *creds;\n\tint ret;\n\n\tcreds = get_current_cred();\n\n\tret = idr_alloc_cyclic(&ctx->personality_idr, (void *) creds, 1,\n\t\t\t\tUSHRT_MAX, GFP_KERNEL);\n\tif (ret < 0)\n\t\tput_cred(creds);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,19 +1,13 @@\n static int io_register_personality(struct io_ring_ctx *ctx)\n {\n-\tstruct io_identity *id;\n+\tconst struct cred *creds;\n \tint ret;\n \n-\tid = kmalloc(sizeof(*id), GFP_KERNEL);\n-\tif (unlikely(!id))\n-\t\treturn -ENOMEM;\n+\tcreds = get_current_cred();\n \n-\tio_init_identity(id);\n-\tid->creds = get_current_cred();\n-\n-\tret = idr_alloc_cyclic(&ctx->personality_idr, id, 1, USHRT_MAX, GFP_KERNEL);\n-\tif (ret < 0) {\n-\t\tput_cred(id->creds);\n-\t\tkfree(id);\n-\t}\n+\tret = idr_alloc_cyclic(&ctx->personality_idr, (void *) creds, 1,\n+\t\t\t\tUSHRT_MAX, GFP_KERNEL);\n+\tif (ret < 0)\n+\t\tput_cred(creds);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tconst struct cred *creds;",
                "\tcreds = get_current_cred();",
                "\tret = idr_alloc_cyclic(&ctx->personality_idr, (void *) creds, 1,",
                "\t\t\t\tUSHRT_MAX, GFP_KERNEL);",
                "\tif (ret < 0)",
                "\t\tput_cred(creds);"
            ],
            "deleted": [
                "\tstruct io_identity *id;",
                "\tid = kmalloc(sizeof(*id), GFP_KERNEL);",
                "\tif (unlikely(!id))",
                "\t\treturn -ENOMEM;",
                "\tio_init_identity(id);",
                "\tid->creds = get_current_cred();",
                "",
                "\tret = idr_alloc_cyclic(&ctx->personality_idr, id, 1, USHRT_MAX, GFP_KERNEL);",
                "\tif (ret < 0) {",
                "\t\tput_cred(id->creds);",
                "\t\tkfree(id);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3363
    },
    {
        "cve_id": "CVE-2022-3176",
        "code_before_change": "static void io_poll_remove_entries(struct io_kiocb *req)\n{\n\tstruct io_poll_iocb *poll = io_poll_get_single(req);\n\tstruct io_poll_iocb *poll_double = io_poll_get_double(req);\n\n\tif (poll->head)\n\t\tio_poll_remove_entry(poll);\n\tif (poll_double && poll_double->head)\n\t\tio_poll_remove_entry(poll_double);\n}",
        "code_after_change": "static void io_poll_remove_entries(struct io_kiocb *req)\n{\n\tstruct io_poll_iocb *poll = io_poll_get_single(req);\n\tstruct io_poll_iocb *poll_double = io_poll_get_double(req);\n\n\t/*\n\t * While we hold the waitqueue lock and the waitqueue is nonempty,\n\t * wake_up_pollfree() will wait for us.  However, taking the waitqueue\n\t * lock in the first place can race with the waitqueue being freed.\n\t *\n\t * We solve this as eventpoll does: by taking advantage of the fact that\n\t * all users of wake_up_pollfree() will RCU-delay the actual free.  If\n\t * we enter rcu_read_lock() and see that the pointer to the queue is\n\t * non-NULL, we can then lock it without the memory being freed out from\n\t * under us.\n\t *\n\t * Keep holding rcu_read_lock() as long as we hold the queue lock, in\n\t * case the caller deletes the entry from the queue, leaving it empty.\n\t * In that case, only RCU prevents the queue memory from being freed.\n\t */\n\trcu_read_lock();\n\tio_poll_remove_entry(poll);\n\tif (poll_double)\n\t\tio_poll_remove_entry(poll_double);\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,8 +3,24 @@\n \tstruct io_poll_iocb *poll = io_poll_get_single(req);\n \tstruct io_poll_iocb *poll_double = io_poll_get_double(req);\n \n-\tif (poll->head)\n-\t\tio_poll_remove_entry(poll);\n-\tif (poll_double && poll_double->head)\n+\t/*\n+\t * While we hold the waitqueue lock and the waitqueue is nonempty,\n+\t * wake_up_pollfree() will wait for us.  However, taking the waitqueue\n+\t * lock in the first place can race with the waitqueue being freed.\n+\t *\n+\t * We solve this as eventpoll does: by taking advantage of the fact that\n+\t * all users of wake_up_pollfree() will RCU-delay the actual free.  If\n+\t * we enter rcu_read_lock() and see that the pointer to the queue is\n+\t * non-NULL, we can then lock it without the memory being freed out from\n+\t * under us.\n+\t *\n+\t * Keep holding rcu_read_lock() as long as we hold the queue lock, in\n+\t * case the caller deletes the entry from the queue, leaving it empty.\n+\t * In that case, only RCU prevents the queue memory from being freed.\n+\t */\n+\trcu_read_lock();\n+\tio_poll_remove_entry(poll);\n+\tif (poll_double)\n \t\tio_poll_remove_entry(poll_double);\n+\trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * While we hold the waitqueue lock and the waitqueue is nonempty,",
                "\t * wake_up_pollfree() will wait for us.  However, taking the waitqueue",
                "\t * lock in the first place can race with the waitqueue being freed.",
                "\t *",
                "\t * We solve this as eventpoll does: by taking advantage of the fact that",
                "\t * all users of wake_up_pollfree() will RCU-delay the actual free.  If",
                "\t * we enter rcu_read_lock() and see that the pointer to the queue is",
                "\t * non-NULL, we can then lock it without the memory being freed out from",
                "\t * under us.",
                "\t *",
                "\t * Keep holding rcu_read_lock() as long as we hold the queue lock, in",
                "\t * case the caller deletes the entry from the queue, leaving it empty.",
                "\t * In that case, only RCU prevents the queue memory from being freed.",
                "\t */",
                "\trcu_read_lock();",
                "\tio_poll_remove_entry(poll);",
                "\tif (poll_double)",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tif (poll->head)",
                "\t\tio_poll_remove_entry(poll);",
                "\tif (poll_double && poll_double->head)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There exists a use-after-free in io_uring in the Linux kernel. Signalfd_poll() and binder_poll() use a waitqueue whose lifetime is the current task. It will send a POLLFREE notification to all waiters before the queue is freed. Unfortunately, the io_uring poll doesn't handle POLLFREE. This allows a use-after-free to occur if a signalfd or binder fd is polled with io_uring poll, and the waitqueue gets freed. We recommend upgrading past commit fc78b2fc21f10c4c9c4d5d659a685710ffa63659",
        "id": 3564
    },
    {
        "cve_id": "CVE-2022-2586",
        "code_before_change": "struct nft_set *nft_set_lookup_global(const struct net *net,\n\t\t\t\t      const struct nft_table *table,\n\t\t\t\t      const struct nlattr *nla_set_name,\n\t\t\t\t      const struct nlattr *nla_set_id,\n\t\t\t\t      u8 genmask)\n{\n\tstruct nft_set *set;\n\n\tset = nft_set_lookup(table, nla_set_name, genmask);\n\tif (IS_ERR(set)) {\n\t\tif (!nla_set_id)\n\t\t\treturn set;\n\n\t\tset = nft_set_lookup_byid(net, nla_set_id, genmask);\n\t}\n\treturn set;\n}",
        "code_after_change": "struct nft_set *nft_set_lookup_global(const struct net *net,\n\t\t\t\t      const struct nft_table *table,\n\t\t\t\t      const struct nlattr *nla_set_name,\n\t\t\t\t      const struct nlattr *nla_set_id,\n\t\t\t\t      u8 genmask)\n{\n\tstruct nft_set *set;\n\n\tset = nft_set_lookup(table, nla_set_name, genmask);\n\tif (IS_ERR(set)) {\n\t\tif (!nla_set_id)\n\t\t\treturn set;\n\n\t\tset = nft_set_lookup_byid(net, table, nla_set_id, genmask);\n\t}\n\treturn set;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \t\tif (!nla_set_id)\n \t\t\treturn set;\n \n-\t\tset = nft_set_lookup_byid(net, nla_set_id, genmask);\n+\t\tset = nft_set_lookup_byid(net, table, nla_set_id, genmask);\n \t}\n \treturn set;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tset = nft_set_lookup_byid(net, table, nla_set_id, genmask);"
            ],
            "deleted": [
                "\t\tset = nft_set_lookup_byid(net, nla_set_id, genmask);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "It was discovered that a nft object or expression could reference a nft set on a different nft table, leading to a use-after-free once that table was deleted.",
        "id": 3478
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int\nget_entries(struct net *net, struct ipt_get_entries __user *uptr,\n\t    const int *len)\n{\n\tint ret;\n\tstruct ipt_get_entries get;\n\tstruct xt_table *t;\n\n\tif (*len < sizeof(get))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&get, uptr, sizeof(get)) != 0)\n\t\treturn -EFAULT;\n\tif (*len != sizeof(struct ipt_get_entries) + get.size)\n\t\treturn -EINVAL;\n\tget.name[sizeof(get.name) - 1] = '\\0';\n\n\tt = xt_find_table_lock(net, AF_INET, get.name);\n\tif (!IS_ERR(t)) {\n\t\tconst struct xt_table_info *private = t->private;\n\t\tif (get.size == private->size)\n\t\t\tret = copy_entries_to_user(private->size,\n\t\t\t\t\t\t   t, uptr->entrytable);\n\t\telse\n\t\t\tret = -EAGAIN;\n\n\t\tmodule_put(t->me);\n\t\txt_table_unlock(t);\n\t} else\n\t\tret = PTR_ERR(t);\n\n\treturn ret;\n}",
        "code_after_change": "static int\nget_entries(struct net *net, struct ipt_get_entries __user *uptr,\n\t    const int *len)\n{\n\tint ret;\n\tstruct ipt_get_entries get;\n\tstruct xt_table *t;\n\n\tif (*len < sizeof(get))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&get, uptr, sizeof(get)) != 0)\n\t\treturn -EFAULT;\n\tif (*len != sizeof(struct ipt_get_entries) + get.size)\n\t\treturn -EINVAL;\n\tget.name[sizeof(get.name) - 1] = '\\0';\n\n\tt = xt_find_table_lock(net, AF_INET, get.name);\n\tif (!IS_ERR(t)) {\n\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n\t\tif (get.size == private->size)\n\t\t\tret = copy_entries_to_user(private->size,\n\t\t\t\t\t\t   t, uptr->entrytable);\n\t\telse\n\t\t\tret = -EAGAIN;\n\n\t\tmodule_put(t->me);\n\t\txt_table_unlock(t);\n\t} else\n\t\tret = PTR_ERR(t);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,7 +16,7 @@\n \n \tt = xt_find_table_lock(net, AF_INET, get.name);\n \tif (!IS_ERR(t)) {\n-\t\tconst struct xt_table_info *private = t->private;\n+\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n \t\tif (get.size == private->size)\n \t\t\tret = copy_entries_to_user(private->size,\n \t\t\t\t\t\t   t, uptr->entrytable);",
        "function_modified_lines": {
            "added": [
                "\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);"
            ],
            "deleted": [
                "\t\tconst struct xt_table_info *private = t->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2778
    },
    {
        "cve_id": "CVE-2023-3812",
        "code_before_change": "static struct sk_buff *tun_napi_alloc_frags(struct tun_file *tfile,\n\t\t\t\t\t    size_t len,\n\t\t\t\t\t    const struct iov_iter *it)\n{\n\tstruct sk_buff *skb;\n\tsize_t linear;\n\tint err;\n\tint i;\n\n\tif (it->nr_segs > MAX_SKB_FRAGS + 1)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\tlocal_bh_disable();\n\tskb = napi_get_frags(&tfile->napi);\n\tlocal_bh_enable();\n\tif (!skb)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tlinear = iov_iter_single_seg_count(it);\n\terr = __skb_grow(skb, linear);\n\tif (err)\n\t\tgoto free;\n\n\tskb->len = len;\n\tskb->data_len = len - linear;\n\tskb->truesize += skb->data_len;\n\n\tfor (i = 1; i < it->nr_segs; i++) {\n\t\tsize_t fragsz = it->iov[i].iov_len;\n\t\tstruct page *page;\n\t\tvoid *frag;\n\n\t\tif (fragsz == 0 || fragsz > PAGE_SIZE) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\t\tfrag = netdev_alloc_frag(fragsz);\n\t\tif (!frag) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t\tpage = virt_to_head_page(frag);\n\t\tskb_fill_page_desc(skb, i - 1, page,\n\t\t\t\t   frag - page_address(page), fragsz);\n\t}\n\n\treturn skb;\nfree:\n\t/* frees skb and all frags allocated with napi_alloc_frag() */\n\tnapi_free_frags(&tfile->napi);\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "static struct sk_buff *tun_napi_alloc_frags(struct tun_file *tfile,\n\t\t\t\t\t    size_t len,\n\t\t\t\t\t    const struct iov_iter *it)\n{\n\tstruct sk_buff *skb;\n\tsize_t linear;\n\tint err;\n\tint i;\n\n\tif (it->nr_segs > MAX_SKB_FRAGS + 1 ||\n\t    len > (ETH_MAX_MTU - NET_SKB_PAD - NET_IP_ALIGN))\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\tlocal_bh_disable();\n\tskb = napi_get_frags(&tfile->napi);\n\tlocal_bh_enable();\n\tif (!skb)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tlinear = iov_iter_single_seg_count(it);\n\terr = __skb_grow(skb, linear);\n\tif (err)\n\t\tgoto free;\n\n\tskb->len = len;\n\tskb->data_len = len - linear;\n\tskb->truesize += skb->data_len;\n\n\tfor (i = 1; i < it->nr_segs; i++) {\n\t\tsize_t fragsz = it->iov[i].iov_len;\n\t\tstruct page *page;\n\t\tvoid *frag;\n\n\t\tif (fragsz == 0 || fragsz > PAGE_SIZE) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\t\tfrag = netdev_alloc_frag(fragsz);\n\t\tif (!frag) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t\tpage = virt_to_head_page(frag);\n\t\tskb_fill_page_desc(skb, i - 1, page,\n\t\t\t\t   frag - page_address(page), fragsz);\n\t}\n\n\treturn skb;\nfree:\n\t/* frees skb and all frags allocated with napi_alloc_frag() */\n\tnapi_free_frags(&tfile->napi);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,8 @@\n \tint err;\n \tint i;\n \n-\tif (it->nr_segs > MAX_SKB_FRAGS + 1)\n+\tif (it->nr_segs > MAX_SKB_FRAGS + 1 ||\n+\t    len > (ETH_MAX_MTU - NET_SKB_PAD - NET_IP_ALIGN))\n \t\treturn ERR_PTR(-EMSGSIZE);\n \n \tlocal_bh_disable();",
        "function_modified_lines": {
            "added": [
                "\tif (it->nr_segs > MAX_SKB_FRAGS + 1 ||",
                "\t    len > (ETH_MAX_MTU - NET_SKB_PAD - NET_IP_ALIGN))"
            ],
            "deleted": [
                "\tif (it->nr_segs > MAX_SKB_FRAGS + 1)"
            ]
        },
        "cwe": [
            "CWE-787",
            "CWE-416"
        ],
        "cve_description": "An out-of-bounds memory access flaw was found in the Linux kernel’s TUN/TAP device driver functionality in how a user generates a malicious (too big) networking packet when napi frags is enabled. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 4136
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static struct l2cap_chan *l2cap_get_chan_by_ident(struct l2cap_conn *conn,\n\t\t\t\t\t\t  u8 ident)\n{\n\tstruct l2cap_chan *c;\n\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_ident(conn, ident);\n\tif (c)\n\t\tl2cap_chan_lock(c);\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn c;\n}",
        "code_after_change": "static struct l2cap_chan *l2cap_get_chan_by_ident(struct l2cap_conn *conn,\n\t\t\t\t\t\t  u8 ident)\n{\n\tstruct l2cap_chan *c;\n\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_ident(conn, ident);\n\tif (c) {\n\t\t/* Only lock if chan reference is not 0 */\n\t\tc = l2cap_chan_hold_unless_zero(c);\n\t\tif (c)\n\t\t\tl2cap_chan_lock(c);\n\t}\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn c;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,12 @@\n \n \tmutex_lock(&conn->chan_lock);\n \tc = __l2cap_get_chan_by_ident(conn, ident);\n-\tif (c)\n-\t\tl2cap_chan_lock(c);\n+\tif (c) {\n+\t\t/* Only lock if chan reference is not 0 */\n+\t\tc = l2cap_chan_hold_unless_zero(c);\n+\t\tif (c)\n+\t\t\tl2cap_chan_lock(c);\n+\t}\n \tmutex_unlock(&conn->chan_lock);\n \n \treturn c;",
        "function_modified_lines": {
            "added": [
                "\tif (c) {",
                "\t\t/* Only lock if chan reference is not 0 */",
                "\t\tc = l2cap_chan_hold_unless_zero(c);",
                "\t\tif (c)",
                "\t\t\tl2cap_chan_lock(c);",
                "\t}"
            ],
            "deleted": [
                "\tif (c)",
                "\t\tl2cap_chan_lock(c);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3392
    },
    {
        "cve_id": "CVE-2016-10200",
        "code_before_change": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "code_after_change": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out_unlock;\n\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,6 @@\n \tint addr_type;\n \tint err;\n \n-\tif (!sock_flag(sk, SOCK_ZAPPED))\n-\t\treturn -EINVAL;\n \tif (addr->l2tp_family != AF_INET6)\n \t\treturn -EINVAL;\n \tif (addr_len < sizeof(*addr))\n@@ -35,6 +33,9 @@\n \tlock_sock(sk);\n \n \terr = -EINVAL;\n+\tif (!sock_flag(sk, SOCK_ZAPPED))\n+\t\tgoto out_unlock;\n+\n \tif (sk->sk_state != TCP_CLOSE)\n \t\tgoto out_unlock;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\tgoto out_unlock;",
                ""
            ],
            "deleted": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the L2TPv3 IP Encapsulation feature in the Linux kernel before 4.8.14 allows local users to gain privileges or cause a denial of service (use-after-free) by making multiple bind system calls without properly ascertaining whether a socket has the SOCK_ZAPPED status, related to net/l2tp/l2tp_ip.c and net/l2tp/l2tp_ip6.c.",
        "id": 899
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "code_after_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n \tstruct in6_addr *daddr, *final_p, final;\n \tstruct ipv6_pinfo *np = inet6_sk(sk);\n+\tstruct ipv6_txoptions *opt_to_free = NULL;\n \tstruct ipv6_txoptions *opt = NULL;\n \tstruct ip6_flowlabel *flowlabel = NULL;\n \tstruct dst_entry *dst = NULL;\n@@ -93,8 +94,10 @@\n \t\t\topt = NULL;\n \t}\n \n-\tif (opt == NULL)\n-\t\topt = np->opt;\n+\tif (!opt) {\n+\t\topt = txopt_get(np);\n+\t\topt_to_free = opt;\n+\t}\n \tif (flowlabel)\n \t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n \topt = ipv6_fixup_options(&opt_space, opt);\n@@ -149,6 +152,7 @@\n \tdst_release(dst);\n out:\n \tfl6_sock_release(flowlabel);\n+\ttxopt_put(opt_to_free);\n \n \treturn err < 0 ? err : len;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt_to_free = NULL;",
                "\tif (!opt) {",
                "\t\topt = txopt_get(np);",
                "\t\topt_to_free = opt;",
                "\t}",
                "\ttxopt_put(opt_to_free);"
            ],
            "deleted": [
                "\tif (opt == NULL)",
                "\t\topt = np->opt;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1009
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static inline void aio_poll_complete(struct aio_kiocb *iocb, __poll_t mask)\n{\n\tstruct file *file = iocb->poll.file;\n\n\taio_complete(iocb, mangle_poll(mask), 0);\n\tfput(file);\n}",
        "code_after_change": "static inline void aio_poll_complete(struct aio_kiocb *iocb, __poll_t mask)\n{\n\taio_complete(iocb, mangle_poll(mask), 0);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,4 @@\n static inline void aio_poll_complete(struct aio_kiocb *iocb, __poll_t mask)\n {\n-\tstruct file *file = iocb->poll.file;\n-\n \taio_complete(iocb, mangle_poll(mask), 0);\n-\tfput(file);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct file *file = iocb->poll.file;",
                "",
                "\tfput(file);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1886
    },
    {
        "cve_id": "CVE-2023-0469",
        "code_before_change": "static int io_install_fixed_file(struct io_ring_ctx *ctx, struct file *file,\n\t\t\t\t u32 slot_index)\n\t__must_hold(&req->ctx->uring_lock)\n{\n\tbool needs_switch = false;\n\tstruct io_fixed_file *file_slot;\n\tint ret;\n\n\tif (io_is_uring_fops(file))\n\t\treturn -EBADF;\n\tif (!ctx->file_data)\n\t\treturn -ENXIO;\n\tif (slot_index >= ctx->nr_user_files)\n\t\treturn -EINVAL;\n\n\tslot_index = array_index_nospec(slot_index, ctx->nr_user_files);\n\tfile_slot = io_fixed_file_slot(&ctx->file_table, slot_index);\n\n\tif (file_slot->file_ptr) {\n\t\tstruct file *old_file;\n\n\t\tret = io_rsrc_node_switch_start(ctx);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\told_file = (struct file *)(file_slot->file_ptr & FFS_MASK);\n\t\tret = io_queue_rsrc_removal(ctx->file_data, slot_index,\n\t\t\t\t\t    ctx->rsrc_node, old_file);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tfile_slot->file_ptr = 0;\n\t\tio_file_bitmap_clear(&ctx->file_table, slot_index);\n\t\tneeds_switch = true;\n\t}\n\n\tret = io_scm_file_account(ctx, file);\n\tif (!ret) {\n\t\t*io_get_tag_slot(ctx->file_data, slot_index) = 0;\n\t\tio_fixed_file_set(file_slot, file);\n\t\tio_file_bitmap_set(&ctx->file_table, slot_index);\n\t}\nerr:\n\tif (needs_switch)\n\t\tio_rsrc_node_switch(ctx, ctx->file_data);\n\tif (ret)\n\t\tfput(file);\n\treturn ret;\n}",
        "code_after_change": "static int io_install_fixed_file(struct io_ring_ctx *ctx, struct file *file,\n\t\t\t\t u32 slot_index)\n\t__must_hold(&req->ctx->uring_lock)\n{\n\tbool needs_switch = false;\n\tstruct io_fixed_file *file_slot;\n\tint ret;\n\n\tif (io_is_uring_fops(file))\n\t\treturn -EBADF;\n\tif (!ctx->file_data)\n\t\treturn -ENXIO;\n\tif (slot_index >= ctx->nr_user_files)\n\t\treturn -EINVAL;\n\n\tslot_index = array_index_nospec(slot_index, ctx->nr_user_files);\n\tfile_slot = io_fixed_file_slot(&ctx->file_table, slot_index);\n\n\tif (file_slot->file_ptr) {\n\t\tstruct file *old_file;\n\n\t\tret = io_rsrc_node_switch_start(ctx);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\told_file = (struct file *)(file_slot->file_ptr & FFS_MASK);\n\t\tret = io_queue_rsrc_removal(ctx->file_data, slot_index,\n\t\t\t\t\t    ctx->rsrc_node, old_file);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tfile_slot->file_ptr = 0;\n\t\tio_file_bitmap_clear(&ctx->file_table, slot_index);\n\t\tneeds_switch = true;\n\t}\n\n\tret = io_scm_file_account(ctx, file);\n\tif (!ret) {\n\t\t*io_get_tag_slot(ctx->file_data, slot_index) = 0;\n\t\tio_fixed_file_set(file_slot, file);\n\t\tio_file_bitmap_set(&ctx->file_table, slot_index);\n\t}\nerr:\n\tif (needs_switch)\n\t\tio_rsrc_node_switch(ctx, ctx->file_data);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -42,7 +42,5 @@\n err:\n \tif (needs_switch)\n \t\tio_rsrc_node_switch(ctx, ctx->file_data);\n-\tif (ret)\n-\t\tfput(file);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tif (ret)",
                "\t\tfput(file);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in io_uring/filetable.c in io_install_fixed_file in the io_uring subcomponent in the Linux Kernel during call cleanup. This flaw may lead to a denial of service.",
        "id": 3832
    },
    {
        "cve_id": "CVE-2023-4206",
        "code_before_change": "static int route4_change(struct net *net, struct sk_buff *in_skb,\n\t\t\t struct tcf_proto *tp, unsigned long base, u32 handle,\n\t\t\t struct nlattr **tca, void **arg, u32 flags,\n\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct route4_head *head = rtnl_dereference(tp->root);\n\tstruct route4_filter __rcu **fp;\n\tstruct route4_filter *fold, *f1, *pfp, *f = NULL;\n\tstruct route4_bucket *b;\n\tstruct nlattr *opt = tca[TCA_OPTIONS];\n\tstruct nlattr *tb[TCA_ROUTE4_MAX + 1];\n\tunsigned int h, th;\n\tint err;\n\tbool new = true;\n\n\tif (!handle) {\n\t\tNL_SET_ERR_MSG(extack, \"Creating with handle of 0 is invalid\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (opt == NULL)\n\t\treturn -EINVAL;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_ROUTE4_MAX, opt,\n\t\t\t\t\t  route4_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tfold = *arg;\n\tif (fold && fold->handle != handle)\n\t\t\treturn -EINVAL;\n\n\terr = -ENOBUFS;\n\tf = kzalloc(sizeof(struct route4_filter), GFP_KERNEL);\n\tif (!f)\n\t\tgoto errout;\n\n\terr = tcf_exts_init(&f->exts, net, TCA_ROUTE4_ACT, TCA_ROUTE4_POLICE);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tif (fold) {\n\t\tf->id = fold->id;\n\t\tf->iif = fold->iif;\n\t\tf->res = fold->res;\n\t\tf->handle = fold->handle;\n\n\t\tf->tp = fold->tp;\n\t\tf->bkt = fold->bkt;\n\t\tnew = false;\n\t}\n\n\terr = route4_set_parms(net, tp, base, f, handle, head, tb,\n\t\t\t       tca[TCA_RATE], new, flags, extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\th = from_hash(f->handle >> 16);\n\tfp = &f->bkt->ht[h];\n\tfor (pfp = rtnl_dereference(*fp);\n\t     (f1 = rtnl_dereference(*fp)) != NULL;\n\t     fp = &f1->next)\n\t\tif (f->handle < f1->handle)\n\t\t\tbreak;\n\n\ttcf_block_netif_keep_dst(tp->chain->block);\n\trcu_assign_pointer(f->next, f1);\n\trcu_assign_pointer(*fp, f);\n\n\tif (fold) {\n\t\tth = to_hash(fold->handle);\n\t\th = from_hash(fold->handle >> 16);\n\t\tb = rtnl_dereference(head->table[th]);\n\t\tif (b) {\n\t\t\tfp = &b->ht[h];\n\t\t\tfor (pfp = rtnl_dereference(*fp); pfp;\n\t\t\t     fp = &pfp->next, pfp = rtnl_dereference(*fp)) {\n\t\t\t\tif (pfp == fold) {\n\t\t\t\t\trcu_assign_pointer(*fp, fold->next);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\troute4_reset_fastmap(head);\n\t*arg = f;\n\tif (fold) {\n\t\ttcf_unbind_filter(tp, &fold->res);\n\t\ttcf_exts_get_net(&fold->exts);\n\t\ttcf_queue_work(&fold->rwork, route4_delete_filter_work);\n\t}\n\treturn 0;\n\nerrout:\n\tif (f)\n\t\ttcf_exts_destroy(&f->exts);\n\tkfree(f);\n\treturn err;\n}",
        "code_after_change": "static int route4_change(struct net *net, struct sk_buff *in_skb,\n\t\t\t struct tcf_proto *tp, unsigned long base, u32 handle,\n\t\t\t struct nlattr **tca, void **arg, u32 flags,\n\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct route4_head *head = rtnl_dereference(tp->root);\n\tstruct route4_filter __rcu **fp;\n\tstruct route4_filter *fold, *f1, *pfp, *f = NULL;\n\tstruct route4_bucket *b;\n\tstruct nlattr *opt = tca[TCA_OPTIONS];\n\tstruct nlattr *tb[TCA_ROUTE4_MAX + 1];\n\tunsigned int h, th;\n\tint err;\n\tbool new = true;\n\n\tif (!handle) {\n\t\tNL_SET_ERR_MSG(extack, \"Creating with handle of 0 is invalid\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (opt == NULL)\n\t\treturn -EINVAL;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_ROUTE4_MAX, opt,\n\t\t\t\t\t  route4_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tfold = *arg;\n\tif (fold && fold->handle != handle)\n\t\t\treturn -EINVAL;\n\n\terr = -ENOBUFS;\n\tf = kzalloc(sizeof(struct route4_filter), GFP_KERNEL);\n\tif (!f)\n\t\tgoto errout;\n\n\terr = tcf_exts_init(&f->exts, net, TCA_ROUTE4_ACT, TCA_ROUTE4_POLICE);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tif (fold) {\n\t\tf->id = fold->id;\n\t\tf->iif = fold->iif;\n\t\tf->handle = fold->handle;\n\n\t\tf->tp = fold->tp;\n\t\tf->bkt = fold->bkt;\n\t\tnew = false;\n\t}\n\n\terr = route4_set_parms(net, tp, base, f, handle, head, tb,\n\t\t\t       tca[TCA_RATE], new, flags, extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\th = from_hash(f->handle >> 16);\n\tfp = &f->bkt->ht[h];\n\tfor (pfp = rtnl_dereference(*fp);\n\t     (f1 = rtnl_dereference(*fp)) != NULL;\n\t     fp = &f1->next)\n\t\tif (f->handle < f1->handle)\n\t\t\tbreak;\n\n\ttcf_block_netif_keep_dst(tp->chain->block);\n\trcu_assign_pointer(f->next, f1);\n\trcu_assign_pointer(*fp, f);\n\n\tif (fold) {\n\t\tth = to_hash(fold->handle);\n\t\th = from_hash(fold->handle >> 16);\n\t\tb = rtnl_dereference(head->table[th]);\n\t\tif (b) {\n\t\t\tfp = &b->ht[h];\n\t\t\tfor (pfp = rtnl_dereference(*fp); pfp;\n\t\t\t     fp = &pfp->next, pfp = rtnl_dereference(*fp)) {\n\t\t\t\tif (pfp == fold) {\n\t\t\t\t\trcu_assign_pointer(*fp, fold->next);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\troute4_reset_fastmap(head);\n\t*arg = f;\n\tif (fold) {\n\t\ttcf_unbind_filter(tp, &fold->res);\n\t\ttcf_exts_get_net(&fold->exts);\n\t\ttcf_queue_work(&fold->rwork, route4_delete_filter_work);\n\t}\n\treturn 0;\n\nerrout:\n\tif (f)\n\t\ttcf_exts_destroy(&f->exts);\n\tkfree(f);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -42,7 +42,6 @@\n \tif (fold) {\n \t\tf->id = fold->id;\n \t\tf->iif = fold->iif;\n-\t\tf->res = fold->res;\n \t\tf->handle = fold->handle;\n \n \t\tf->tp = fold->tp;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tf->res = fold->res;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's net/sched: cls_route component can be exploited to achieve local privilege escalation.\n\nWhen route4_change() is called on an existing filter, the whole tcf_result struct is always copied into the new instance of the filter. This causes a problem when updating a filter bound to a class, as tcf_unbind_filter() is always called on the old instance in the success path, decreasing filter_cnt of the still referenced class and allowing it to be deleted, leading to a use-after-free.\n\nWe recommend upgrading past commit b80b829e9e2c1b3f7aae34855e04d8f6ecaf13c8.\n\n",
        "id": 4198
    },
    {
        "cve_id": "CVE-2023-6111",
        "code_before_change": "static struct nft_trans_gc *nft_trans_gc_catchall(struct nft_trans_gc *gc,\n\t\t\t\t\t\t  unsigned int gc_seq,\n\t\t\t\t\t\t  bool sync)\n{\n\tstruct nft_set_elem_catchall *catchall;\n\tconst struct nft_set *set = gc->set;\n\tstruct nft_set_ext *ext;\n\n\tlist_for_each_entry_rcu(catchall, &set->catchall_list, list) {\n\t\text = nft_set_elem_ext(set, catchall->elem);\n\n\t\tif (!nft_set_elem_expired(ext))\n\t\t\tcontinue;\n\t\tif (nft_set_elem_is_dead(ext))\n\t\t\tgoto dead_elem;\n\n\t\tnft_set_elem_dead(ext);\ndead_elem:\n\t\tif (sync)\n\t\t\tgc = nft_trans_gc_queue_sync(gc, GFP_ATOMIC);\n\t\telse\n\t\t\tgc = nft_trans_gc_queue_async(gc, gc_seq, GFP_ATOMIC);\n\n\t\tif (!gc)\n\t\t\treturn NULL;\n\n\t\tnft_trans_gc_elem_add(gc, catchall->elem);\n\t}\n\n\treturn gc;\n}",
        "code_after_change": "static struct nft_trans_gc *nft_trans_gc_catchall(struct nft_trans_gc *gc,\n\t\t\t\t\t\t  unsigned int gc_seq,\n\t\t\t\t\t\t  bool sync)\n{\n\tstruct nft_set_elem_catchall *catchall, *next;\n\tconst struct nft_set *set = gc->set;\n\tstruct nft_elem_priv *elem_priv;\n\tstruct nft_set_ext *ext;\n\n\tlist_for_each_entry_safe(catchall, next, &set->catchall_list, list) {\n\t\text = nft_set_elem_ext(set, catchall->elem);\n\n\t\tif (!nft_set_elem_expired(ext))\n\t\t\tcontinue;\n\t\tif (nft_set_elem_is_dead(ext))\n\t\t\tgoto dead_elem;\n\n\t\tnft_set_elem_dead(ext);\ndead_elem:\n\t\tif (sync)\n\t\t\tgc = nft_trans_gc_queue_sync(gc, GFP_ATOMIC);\n\t\telse\n\t\t\tgc = nft_trans_gc_queue_async(gc, gc_seq, GFP_ATOMIC);\n\n\t\tif (!gc)\n\t\t\treturn NULL;\n\n\t\telem_priv = catchall->elem;\n\t\tif (sync) {\n\t\t\tnft_setelem_data_deactivate(gc->net, gc->set, elem_priv);\n\t\t\tnft_setelem_catchall_destroy(catchall);\n\t\t}\n\n\t\tnft_trans_gc_elem_add(gc, elem_priv);\n\t}\n\n\treturn gc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,11 +2,12 @@\n \t\t\t\t\t\t  unsigned int gc_seq,\n \t\t\t\t\t\t  bool sync)\n {\n-\tstruct nft_set_elem_catchall *catchall;\n+\tstruct nft_set_elem_catchall *catchall, *next;\n \tconst struct nft_set *set = gc->set;\n+\tstruct nft_elem_priv *elem_priv;\n \tstruct nft_set_ext *ext;\n \n-\tlist_for_each_entry_rcu(catchall, &set->catchall_list, list) {\n+\tlist_for_each_entry_safe(catchall, next, &set->catchall_list, list) {\n \t\text = nft_set_elem_ext(set, catchall->elem);\n \n \t\tif (!nft_set_elem_expired(ext))\n@@ -24,7 +25,13 @@\n \t\tif (!gc)\n \t\t\treturn NULL;\n \n-\t\tnft_trans_gc_elem_add(gc, catchall->elem);\n+\t\telem_priv = catchall->elem;\n+\t\tif (sync) {\n+\t\t\tnft_setelem_data_deactivate(gc->net, gc->set, elem_priv);\n+\t\t\tnft_setelem_catchall_destroy(catchall);\n+\t\t}\n+\n+\t\tnft_trans_gc_elem_add(gc, elem_priv);\n \t}\n \n \treturn gc;",
        "function_modified_lines": {
            "added": [
                "\tstruct nft_set_elem_catchall *catchall, *next;",
                "\tstruct nft_elem_priv *elem_priv;",
                "\tlist_for_each_entry_safe(catchall, next, &set->catchall_list, list) {",
                "\t\telem_priv = catchall->elem;",
                "\t\tif (sync) {",
                "\t\t\tnft_setelem_data_deactivate(gc->net, gc->set, elem_priv);",
                "\t\t\tnft_setelem_catchall_destroy(catchall);",
                "\t\t}",
                "",
                "\t\tnft_trans_gc_elem_add(gc, elem_priv);"
            ],
            "deleted": [
                "\tstruct nft_set_elem_catchall *catchall;",
                "\tlist_for_each_entry_rcu(catchall, &set->catchall_list, list) {",
                "\t\tnft_trans_gc_elem_add(gc, catchall->elem);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nThe function nft_trans_gc_catchall did not remove the catchall set element from the catchall_list when the argument sync is true, making it possible to free a catchall set element many times.\n\nWe recommend upgrading past commit 93995bf4af2c5a99e2a87f0cd5ce547d31eb7630.\n\n",
        "id": 4296
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "static int nfc_genl_llc_get_params(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tint rc = 0;\n\tstruct sk_buff *msg = NULL;\n\tu32 idx;\n\n\tif (!info->attrs[NFC_ATTR_DEVICE_INDEX] ||\n\t    !info->attrs[NFC_ATTR_FIRMWARE_NAME])\n\t\treturn -EINVAL;\n\n\tidx = nla_get_u32(info->attrs[NFC_ATTR_DEVICE_INDEX]);\n\n\tdev = nfc_get_device(idx);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tdevice_lock(&dev->dev);\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (!local) {\n\t\trc = -ENODEV;\n\t\tgoto exit;\n\t}\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg) {\n\t\trc = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\trc = nfc_genl_send_params(msg, local, info->snd_portid, info->snd_seq);\n\nexit:\n\tdevice_unlock(&dev->dev);\n\n\tnfc_put_device(dev);\n\n\tif (rc < 0) {\n\t\tif (msg)\n\t\t\tnlmsg_free(msg);\n\n\t\treturn rc;\n\t}\n\n\treturn genlmsg_reply(msg, info);\n}",
        "code_after_change": "static int nfc_genl_llc_get_params(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tint rc = 0;\n\tstruct sk_buff *msg = NULL;\n\tu32 idx;\n\n\tif (!info->attrs[NFC_ATTR_DEVICE_INDEX] ||\n\t    !info->attrs[NFC_ATTR_FIRMWARE_NAME])\n\t\treturn -EINVAL;\n\n\tidx = nla_get_u32(info->attrs[NFC_ATTR_DEVICE_INDEX]);\n\n\tdev = nfc_get_device(idx);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tdevice_lock(&dev->dev);\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (!local) {\n\t\trc = -ENODEV;\n\t\tgoto exit;\n\t}\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg) {\n\t\trc = -ENOMEM;\n\t\tgoto put_local;\n\t}\n\n\trc = nfc_genl_send_params(msg, local, info->snd_portid, info->snd_seq);\n\nput_local:\n\tnfc_llcp_local_put(local);\n\nexit:\n\tdevice_unlock(&dev->dev);\n\n\tnfc_put_device(dev);\n\n\tif (rc < 0) {\n\t\tif (msg)\n\t\t\tnlmsg_free(msg);\n\n\t\treturn rc;\n\t}\n\n\treturn genlmsg_reply(msg, info);\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,10 +27,13 @@\n \tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n \tif (!msg) {\n \t\trc = -ENOMEM;\n-\t\tgoto exit;\n+\t\tgoto put_local;\n \t}\n \n \trc = nfc_genl_send_params(msg, local, info->snd_portid, info->snd_seq);\n+\n+put_local:\n+\tnfc_llcp_local_put(local);\n \n exit:\n \tdevice_unlock(&dev->dev);",
        "function_modified_lines": {
            "added": [
                "\t\tgoto put_local;",
                "",
                "put_local:",
                "\tnfc_llcp_local_put(local);"
            ],
            "deleted": [
                "\t\tgoto exit;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4158
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "static void sdr_cap_stop_streaming(struct vb2_queue *vq)\n{\n\tstruct vivid_dev *dev = vb2_get_drv_priv(vq);\n\n\tif (dev->kthread_sdr_cap == NULL)\n\t\treturn;\n\n\twhile (!list_empty(&dev->sdr_cap_active)) {\n\t\tstruct vivid_buffer *buf;\n\n\t\tbuf = list_entry(dev->sdr_cap_active.next,\n\t\t\t\tstruct vivid_buffer, list);\n\t\tlist_del(&buf->list);\n\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t   &dev->ctrl_hdl_sdr_cap);\n\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t}\n\n\t/* shutdown control thread */\n\tmutex_unlock(&dev->mutex);\n\tkthread_stop(dev->kthread_sdr_cap);\n\tdev->kthread_sdr_cap = NULL;\n\tmutex_lock(&dev->mutex);\n}",
        "code_after_change": "static void sdr_cap_stop_streaming(struct vb2_queue *vq)\n{\n\tstruct vivid_dev *dev = vb2_get_drv_priv(vq);\n\n\tif (dev->kthread_sdr_cap == NULL)\n\t\treturn;\n\n\twhile (!list_empty(&dev->sdr_cap_active)) {\n\t\tstruct vivid_buffer *buf;\n\n\t\tbuf = list_entry(dev->sdr_cap_active.next,\n\t\t\t\tstruct vivid_buffer, list);\n\t\tlist_del(&buf->list);\n\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t   &dev->ctrl_hdl_sdr_cap);\n\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t}\n\n\t/* shutdown control thread */\n\tkthread_stop(dev->kthread_sdr_cap);\n\tdev->kthread_sdr_cap = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,8 +17,6 @@\n \t}\n \n \t/* shutdown control thread */\n-\tmutex_unlock(&dev->mutex);\n \tkthread_stop(dev->kthread_sdr_cap);\n \tdev->kthread_sdr_cap = NULL;\n-\tmutex_lock(&dev->mutex);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmutex_unlock(&dev->mutex);",
                "\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free.",
        "id": 2095
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static int l2cap_move_channel_confirm(struct l2cap_conn *conn,\n\t\t\t\t      struct l2cap_cmd_hdr *cmd,\n\t\t\t\t      u16 cmd_len, void *data)\n{\n\tstruct l2cap_move_chan_cfm *cfm = data;\n\tstruct l2cap_chan *chan;\n\tu16 icid, result;\n\n\tif (cmd_len != sizeof(*cfm))\n\t\treturn -EPROTO;\n\n\ticid = le16_to_cpu(cfm->icid);\n\tresult = le16_to_cpu(cfm->result);\n\n\tBT_DBG(\"icid 0x%4.4x, result 0x%4.4x\", icid, result);\n\n\tchan = l2cap_get_chan_by_dcid(conn, icid);\n\tif (!chan) {\n\t\t/* Spec requires a response even if the icid was not found */\n\t\tl2cap_send_move_chan_cfm_rsp(conn, cmd->ident, icid);\n\t\treturn 0;\n\t}\n\n\tif (chan->move_state == L2CAP_MOVE_WAIT_CONFIRM) {\n\t\tif (result == L2CAP_MC_CONFIRMED) {\n\t\t\tchan->local_amp_id = chan->move_id;\n\t\t\tif (chan->local_amp_id == AMP_ID_BREDR)\n\t\t\t\t__release_logical_link(chan);\n\t\t} else {\n\t\t\tchan->move_id = chan->local_amp_id;\n\t\t}\n\n\t\tl2cap_move_done(chan);\n\t}\n\n\tl2cap_send_move_chan_cfm_rsp(conn, cmd->ident, icid);\n\n\tl2cap_chan_unlock(chan);\n\n\treturn 0;\n}",
        "code_after_change": "static int l2cap_move_channel_confirm(struct l2cap_conn *conn,\n\t\t\t\t      struct l2cap_cmd_hdr *cmd,\n\t\t\t\t      u16 cmd_len, void *data)\n{\n\tstruct l2cap_move_chan_cfm *cfm = data;\n\tstruct l2cap_chan *chan;\n\tu16 icid, result;\n\n\tif (cmd_len != sizeof(*cfm))\n\t\treturn -EPROTO;\n\n\ticid = le16_to_cpu(cfm->icid);\n\tresult = le16_to_cpu(cfm->result);\n\n\tBT_DBG(\"icid 0x%4.4x, result 0x%4.4x\", icid, result);\n\n\tchan = l2cap_get_chan_by_dcid(conn, icid);\n\tif (!chan) {\n\t\t/* Spec requires a response even if the icid was not found */\n\t\tl2cap_send_move_chan_cfm_rsp(conn, cmd->ident, icid);\n\t\treturn 0;\n\t}\n\n\tif (chan->move_state == L2CAP_MOVE_WAIT_CONFIRM) {\n\t\tif (result == L2CAP_MC_CONFIRMED) {\n\t\t\tchan->local_amp_id = chan->move_id;\n\t\t\tif (chan->local_amp_id == AMP_ID_BREDR)\n\t\t\t\t__release_logical_link(chan);\n\t\t} else {\n\t\t\tchan->move_id = chan->local_amp_id;\n\t\t}\n\n\t\tl2cap_move_done(chan);\n\t}\n\n\tl2cap_send_move_chan_cfm_rsp(conn, cmd->ident, icid);\n\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -36,6 +36,7 @@\n \tl2cap_send_move_chan_cfm_rsp(conn, cmd->ident, icid);\n \n \tl2cap_chan_unlock(chan);\n+\tl2cap_chan_put(chan);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tl2cap_chan_put(chan);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3395
    },
    {
        "cve_id": "CVE-2019-15292",
        "code_before_change": "void atalk_register_sysctl(void)\n{\n\tatalk_table_header = register_net_sysctl(&init_net, \"net/appletalk\", atalk_table);\n}",
        "code_after_change": "int __init atalk_register_sysctl(void)\n{\n\tatalk_table_header = register_net_sysctl(&init_net, \"net/appletalk\", atalk_table);\n\tif (!atalk_table_header)\n\t\treturn -ENOMEM;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,7 @@\n-void atalk_register_sysctl(void)\n+int __init atalk_register_sysctl(void)\n {\n \tatalk_table_header = register_net_sysctl(&init_net, \"net/appletalk\", atalk_table);\n+\tif (!atalk_table_header)\n+\t\treturn -ENOMEM;\n+\treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "int __init atalk_register_sysctl(void)",
                "\tif (!atalk_table_header)",
                "\t\treturn -ENOMEM;",
                "\treturn 0;"
            ],
            "deleted": [
                "void atalk_register_sysctl(void)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.9. There is a use-after-free in atalk_proc_exit, related to net/appletalk/atalk_proc.c, net/appletalk/ddp.c, and net/appletalk/sysctl_net_atalk.c.",
        "id": 2016
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "static int vmw_cmd_set_shader(struct vmw_private *dev_priv,\n\t\t\t      struct vmw_sw_context *sw_context,\n\t\t\t      SVGA3dCmdHeader *header)\n{\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdSetShader);\n\tstruct vmw_ctx_bindinfo_shader binding;\n\tstruct vmw_resource *ctx, *res = NULL;\n\tstruct vmw_ctx_validation_info *ctx_info;\n\tint ret;\n\n\tcmd = container_of(header, typeof(*cmd), header);\n\n\tif (cmd->body.type >= SVGA3D_SHADERTYPE_PREDX_MAX) {\n\t\tVMW_DEBUG_USER(\"Illegal shader type %u.\\n\",\n\t\t\t       (unsigned int) cmd->body.type);\n\t\treturn -EINVAL;\n\t}\n\n\tret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\n\t\t\t\tVMW_RES_DIRTY_SET, user_context_converter,\n\t\t\t\t&cmd->body.cid, &ctx);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tif (!dev_priv->has_mob)\n\t\treturn 0;\n\n\tif (cmd->body.shid != SVGA3D_INVALID_ID) {\n\t\t/*\n\t\t * This is the compat shader path - Per device guest-backed\n\t\t * shaders, but user-space thinks it's per context host-\n\t\t * backed shaders.\n\t\t */\n\t\tres = vmw_shader_lookup(vmw_context_res_man(ctx),\n\t\t\t\t\tcmd->body.shid, cmd->body.type);\n\t\tif (!IS_ERR(res)) {\n\t\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\t\t\tif (unlikely(ret != 0))\n\t\t\t\treturn ret;\n\n\t\t\tret = vmw_resource_relocation_add\n\t\t\t\t(sw_context, res,\n\t\t\t\t vmw_ptr_diff(sw_context->buf_start,\n\t\t\t\t\t      &cmd->body.shid),\n\t\t\t\t vmw_res_rel_normal);\n\t\t\tif (unlikely(ret != 0))\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (IS_ERR_OR_NULL(res)) {\n\t\tret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_shader,\n\t\t\t\t\tVMW_RES_DIRTY_NONE,\n\t\t\t\t\tuser_shader_converter, &cmd->body.shid,\n\t\t\t\t\t&res);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\t}\n\n\tctx_info = vmw_execbuf_info_from_res(sw_context, ctx);\n\tif (!ctx_info)\n\t\treturn -EINVAL;\n\n\tbinding.bi.ctx = ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_shader;\n\tbinding.shader_slot = cmd->body.type - SVGA3D_SHADERTYPE_MIN;\n\tvmw_binding_add(ctx_info->staged, &binding.bi, binding.shader_slot, 0);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_cmd_set_shader(struct vmw_private *dev_priv,\n\t\t\t      struct vmw_sw_context *sw_context,\n\t\t\t      SVGA3dCmdHeader *header)\n{\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdSetShader);\n\tstruct vmw_ctx_bindinfo_shader binding;\n\tstruct vmw_resource *ctx, *res = NULL;\n\tstruct vmw_ctx_validation_info *ctx_info;\n\tint ret;\n\n\tcmd = container_of(header, typeof(*cmd), header);\n\n\tif (cmd->body.type >= SVGA3D_SHADERTYPE_PREDX_MAX) {\n\t\tVMW_DEBUG_USER(\"Illegal shader type %u.\\n\",\n\t\t\t       (unsigned int) cmd->body.type);\n\t\treturn -EINVAL;\n\t}\n\n\tret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\n\t\t\t\tVMW_RES_DIRTY_SET, user_context_converter,\n\t\t\t\t&cmd->body.cid, &ctx);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tif (!dev_priv->has_mob)\n\t\treturn 0;\n\n\tif (cmd->body.shid != SVGA3D_INVALID_ID) {\n\t\t/*\n\t\t * This is the compat shader path - Per device guest-backed\n\t\t * shaders, but user-space thinks it's per context host-\n\t\t * backed shaders.\n\t\t */\n\t\tres = vmw_shader_lookup(vmw_context_res_man(ctx),\n\t\t\t\t\tcmd->body.shid, cmd->body.type);\n\t\tif (!IS_ERR(res)) {\n\t\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n\t\t\t\t\t\t      VMW_RES_DIRTY_NONE,\n\t\t\t\t\t\t      vmw_val_add_flag_noctx);\n\t\t\tif (unlikely(ret != 0))\n\t\t\t\treturn ret;\n\n\t\t\tret = vmw_resource_relocation_add\n\t\t\t\t(sw_context, res,\n\t\t\t\t vmw_ptr_diff(sw_context->buf_start,\n\t\t\t\t\t      &cmd->body.shid),\n\t\t\t\t vmw_res_rel_normal);\n\t\t\tif (unlikely(ret != 0))\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (IS_ERR_OR_NULL(res)) {\n\t\tret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_shader,\n\t\t\t\t\tVMW_RES_DIRTY_NONE,\n\t\t\t\t\tuser_shader_converter, &cmd->body.shid,\n\t\t\t\t\t&res);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\t}\n\n\tctx_info = vmw_execbuf_info_from_res(sw_context, ctx);\n\tif (!ctx_info)\n\t\treturn -EINVAL;\n\n\tbinding.bi.ctx = ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_shader;\n\tbinding.shader_slot = cmd->body.type - SVGA3D_SHADERTYPE_MIN;\n\tvmw_binding_add(ctx_info->staged, &binding.bi, binding.shader_slot, 0);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -34,8 +34,9 @@\n \t\tres = vmw_shader_lookup(vmw_context_res_man(ctx),\n \t\t\t\t\tcmd->body.shid, cmd->body.type);\n \t\tif (!IS_ERR(res)) {\n-\t\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n-\t\t\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n+\t\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n+\t\t\t\t\t\t      VMW_RES_DIRTY_NONE,\n+\t\t\t\t\t\t      vmw_val_add_flag_noctx);\n \t\t\tif (unlikely(ret != 0))\n \t\t\t\treturn ret;\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tret = vmw_execbuf_res_val_add(sw_context, res,",
                "\t\t\t\t\t\t      VMW_RES_DIRTY_NONE,",
                "\t\t\t\t\t\t      vmw_val_add_flag_noctx);"
            ],
            "deleted": [
                "\t\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,",
                "\t\t\t\t\t\t\t    VMW_RES_DIRTY_NONE);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3685
    },
    {
        "cve_id": "CVE-2023-0240",
        "code_before_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe,\n\t\t       struct io_submit_state *state)\n{\n\tunsigned int sqe_flags;\n\tint id, ret;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->flags = 0;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm(ctx, req)))\n\t\treturn -EFAULT;\n\n\tsqe_flags = READ_ONCE(sqe->flags);\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\tio_req_init_async(req);\n\t\treq->work.identity->creds = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!req->work.identity->creds))\n\t\t\treturn -EINVAL;\n\t\tget_cred(req->work.identity->creds);\n\t\treq->work.flags |= IO_WQ_WORK_CREDS;\n\t}\n\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags |= sqe_flags;\n\n\tif (!io_op_defs[req->opcode].needs_file)\n\t\treturn 0;\n\n\tret = io_req_set_file(state, req, READ_ONCE(sqe->fd));\n\tstate->ios_left--;\n\treturn ret;\n}",
        "code_after_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe,\n\t\t       struct io_submit_state *state)\n{\n\tunsigned int sqe_flags;\n\tint id, ret;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->flags = 0;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm(ctx, req)))\n\t\treturn -EFAULT;\n\n\tsqe_flags = READ_ONCE(sqe->flags);\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\tstruct io_identity *iod;\n\n\t\tio_req_init_async(req);\n\t\tiod = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!iod))\n\t\t\treturn -EINVAL;\n\t\trefcount_inc(&iod->count);\n\t\tio_put_identity(req);\n\t\tget_cred(iod->creds);\n\t\treq->work.identity = iod;\n\t\treq->work.flags |= IO_WQ_WORK_CREDS;\n\t}\n\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags |= sqe_flags;\n\n\tif (!io_op_defs[req->opcode].needs_file)\n\t\treturn 0;\n\n\tret = io_req_set_file(state, req, READ_ONCE(sqe->fd));\n\tstate->ios_left--;\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -36,11 +36,16 @@\n \n \tid = READ_ONCE(sqe->personality);\n \tif (id) {\n+\t\tstruct io_identity *iod;\n+\n \t\tio_req_init_async(req);\n-\t\treq->work.identity->creds = idr_find(&ctx->personality_idr, id);\n-\t\tif (unlikely(!req->work.identity->creds))\n+\t\tiod = idr_find(&ctx->personality_idr, id);\n+\t\tif (unlikely(!iod))\n \t\t\treturn -EINVAL;\n-\t\tget_cred(req->work.identity->creds);\n+\t\trefcount_inc(&iod->count);\n+\t\tio_put_identity(req);\n+\t\tget_cred(iod->creds);\n+\t\treq->work.identity = iod;\n \t\treq->work.flags |= IO_WQ_WORK_CREDS;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tstruct io_identity *iod;",
                "",
                "\t\tiod = idr_find(&ctx->personality_idr, id);",
                "\t\tif (unlikely(!iod))",
                "\t\trefcount_inc(&iod->count);",
                "\t\tio_put_identity(req);",
                "\t\tget_cred(iod->creds);",
                "\t\treq->work.identity = iod;"
            ],
            "deleted": [
                "\t\treq->work.identity->creds = idr_find(&ctx->personality_idr, id);",
                "\t\tif (unlikely(!req->work.identity->creds))",
                "\t\tget_cred(req->work.identity->creds);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "id": 3819
    },
    {
        "cve_id": "CVE-2019-9003",
        "code_before_change": "int ipmi_destroy_user(struct ipmi_user *user)\n{\n\t_ipmi_destroy_user(user);\n\n\tcleanup_srcu_struct(&user->release_barrier);\n\tkref_put(&user->refcount, free_user);\n\n\treturn 0;\n}",
        "code_after_change": "int ipmi_destroy_user(struct ipmi_user *user)\n{\n\t_ipmi_destroy_user(user);\n\n\tkref_put(&user->refcount, free_user);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,6 @@\n {\n \t_ipmi_destroy_user(user);\n \n-\tcleanup_srcu_struct(&user->release_barrier);\n \tkref_put(&user->refcount, free_user);\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tcleanup_srcu_struct(&user->release_barrier);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 4.20.5, attackers can trigger a drivers/char/ipmi/ipmi_msghandler.c use-after-free and OOPS by arranging for certain simultaneous execution of the code, as demonstrated by a \"service ipmievd restart\" loop.",
        "id": 2351
    },
    {
        "cve_id": "CVE-2023-20928",
        "code_before_change": "void binder_alloc_deferred_release(struct binder_alloc *alloc)\n{\n\tstruct rb_node *n;\n\tint buffers, page_count;\n\tstruct binder_buffer *buffer;\n\n\tbuffers = 0;\n\tmutex_lock(&alloc->mutex);\n\tBUG_ON(alloc->vma);\n\n\twhile ((n = rb_first(&alloc->allocated_buffers))) {\n\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\n\t\t/* Transaction should already have been freed */\n\t\tBUG_ON(buffer->transaction);\n\n\t\tif (buffer->clear_on_free) {\n\t\t\tbinder_alloc_clear_buf(alloc, buffer);\n\t\t\tbuffer->clear_on_free = false;\n\t\t}\n\t\tbinder_free_buf_locked(alloc, buffer);\n\t\tbuffers++;\n\t}\n\n\twhile (!list_empty(&alloc->buffers)) {\n\t\tbuffer = list_first_entry(&alloc->buffers,\n\t\t\t\t\t  struct binder_buffer, entry);\n\t\tWARN_ON(!buffer->free);\n\n\t\tlist_del(&buffer->entry);\n\t\tWARN_ON_ONCE(!list_empty(&alloc->buffers));\n\t\tkfree(buffer);\n\t}\n\n\tpage_count = 0;\n\tif (alloc->pages) {\n\t\tint i;\n\n\t\tfor (i = 0; i < alloc->buffer_size / PAGE_SIZE; i++) {\n\t\t\tvoid __user *page_addr;\n\t\t\tbool on_lru;\n\n\t\t\tif (!alloc->pages[i].page_ptr)\n\t\t\t\tcontinue;\n\n\t\t\ton_lru = list_lru_del(&binder_alloc_lru,\n\t\t\t\t\t      &alloc->pages[i].lru);\n\t\t\tpage_addr = alloc->buffer + i * PAGE_SIZE;\n\t\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t     \"%s: %d: page %d at %pK %s\\n\",\n\t\t\t\t     __func__, alloc->pid, i, page_addr,\n\t\t\t\t     on_lru ? \"on lru\" : \"active\");\n\t\t\t__free_page(alloc->pages[i].page_ptr);\n\t\t\tpage_count++;\n\t\t}\n\t\tkfree(alloc->pages);\n\t}\n\tmutex_unlock(&alloc->mutex);\n\tif (alloc->vma_vm_mm)\n\t\tmmdrop(alloc->vma_vm_mm);\n\n\tbinder_alloc_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d buffers %d, pages %d\\n\",\n\t\t     __func__, alloc->pid, buffers, page_count);\n}",
        "code_after_change": "void binder_alloc_deferred_release(struct binder_alloc *alloc)\n{\n\tstruct rb_node *n;\n\tint buffers, page_count;\n\tstruct binder_buffer *buffer;\n\n\tbuffers = 0;\n\tmutex_lock(&alloc->mutex);\n\tBUG_ON(alloc->vma_addr &&\n\t       vma_lookup(alloc->vma_vm_mm, alloc->vma_addr));\n\n\twhile ((n = rb_first(&alloc->allocated_buffers))) {\n\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\n\t\t/* Transaction should already have been freed */\n\t\tBUG_ON(buffer->transaction);\n\n\t\tif (buffer->clear_on_free) {\n\t\t\tbinder_alloc_clear_buf(alloc, buffer);\n\t\t\tbuffer->clear_on_free = false;\n\t\t}\n\t\tbinder_free_buf_locked(alloc, buffer);\n\t\tbuffers++;\n\t}\n\n\twhile (!list_empty(&alloc->buffers)) {\n\t\tbuffer = list_first_entry(&alloc->buffers,\n\t\t\t\t\t  struct binder_buffer, entry);\n\t\tWARN_ON(!buffer->free);\n\n\t\tlist_del(&buffer->entry);\n\t\tWARN_ON_ONCE(!list_empty(&alloc->buffers));\n\t\tkfree(buffer);\n\t}\n\n\tpage_count = 0;\n\tif (alloc->pages) {\n\t\tint i;\n\n\t\tfor (i = 0; i < alloc->buffer_size / PAGE_SIZE; i++) {\n\t\t\tvoid __user *page_addr;\n\t\t\tbool on_lru;\n\n\t\t\tif (!alloc->pages[i].page_ptr)\n\t\t\t\tcontinue;\n\n\t\t\ton_lru = list_lru_del(&binder_alloc_lru,\n\t\t\t\t\t      &alloc->pages[i].lru);\n\t\t\tpage_addr = alloc->buffer + i * PAGE_SIZE;\n\t\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t     \"%s: %d: page %d at %pK %s\\n\",\n\t\t\t\t     __func__, alloc->pid, i, page_addr,\n\t\t\t\t     on_lru ? \"on lru\" : \"active\");\n\t\t\t__free_page(alloc->pages[i].page_ptr);\n\t\t\tpage_count++;\n\t\t}\n\t\tkfree(alloc->pages);\n\t}\n\tmutex_unlock(&alloc->mutex);\n\tif (alloc->vma_vm_mm)\n\t\tmmdrop(alloc->vma_vm_mm);\n\n\tbinder_alloc_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d buffers %d, pages %d\\n\",\n\t\t     __func__, alloc->pid, buffers, page_count);\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,8 @@\n \n \tbuffers = 0;\n \tmutex_lock(&alloc->mutex);\n-\tBUG_ON(alloc->vma);\n+\tBUG_ON(alloc->vma_addr &&\n+\t       vma_lookup(alloc->vma_vm_mm, alloc->vma_addr));\n \n \twhile ((n = rb_first(&alloc->allocated_buffers))) {\n \t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);",
        "function_modified_lines": {
            "added": [
                "\tBUG_ON(alloc->vma_addr &&",
                "\t       vma_lookup(alloc->vma_vm_mm, alloc->vma_addr));"
            ],
            "deleted": [
                "\tBUG_ON(alloc->vma);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_vma_close of binder.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-254837884References: Upstream kernel",
        "id": 3907
    },
    {
        "cve_id": "CVE-2021-32606",
        "code_before_change": "static int isotp_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t    sockptr_t optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct isotp_sock *so = isotp_sk(sk);\n\tint ret = 0;\n\n\tif (level != SOL_CAN_ISOTP)\n\t\treturn -EINVAL;\n\n\tif (so->bound)\n\t\treturn -EISCONN;\n\n\tswitch (optname) {\n\tcase CAN_ISOTP_OPTS:\n\t\tif (optlen != sizeof(struct can_isotp_options))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_sockptr(&so->opt, optval, optlen))\n\t\t\treturn -EFAULT;\n\n\t\t/* no separate rx_ext_address is given => use ext_address */\n\t\tif (!(so->opt.flags & CAN_ISOTP_RX_EXT_ADDR))\n\t\t\tso->opt.rx_ext_address = so->opt.ext_address;\n\t\tbreak;\n\n\tcase CAN_ISOTP_RECV_FC:\n\t\tif (optlen != sizeof(struct can_isotp_fc_options))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_sockptr(&so->rxfc, optval, optlen))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\n\tcase CAN_ISOTP_TX_STMIN:\n\t\tif (optlen != sizeof(u32))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_sockptr(&so->force_tx_stmin, optval, optlen))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\n\tcase CAN_ISOTP_RX_STMIN:\n\t\tif (optlen != sizeof(u32))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_sockptr(&so->force_rx_stmin, optval, optlen))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\n\tcase CAN_ISOTP_LL_OPTS:\n\t\tif (optlen == sizeof(struct can_isotp_ll_options)) {\n\t\t\tstruct can_isotp_ll_options ll;\n\n\t\t\tif (copy_from_sockptr(&ll, optval, optlen))\n\t\t\t\treturn -EFAULT;\n\n\t\t\t/* check for correct ISO 11898-1 DLC data length */\n\t\t\tif (ll.tx_dl != padlen(ll.tx_dl))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (ll.mtu != CAN_MTU && ll.mtu != CANFD_MTU)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (ll.mtu == CAN_MTU &&\n\t\t\t    (ll.tx_dl > CAN_MAX_DLEN || ll.tx_flags != 0))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tmemcpy(&so->ll, &ll, sizeof(ll));\n\n\t\t\t/* set ll_dl for tx path to similar place as for rx */\n\t\t\tso->tx.ll_dl = ll.tx_dl;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int isotp_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t    sockptr_t optval, unsigned int optlen)\n\n{\n\tstruct sock *sk = sock->sk;\n\tint ret;\n\n\tif (level != SOL_CAN_ISOTP)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tret = isotp_setsockopt_locked(sock, level, optname, optval, optlen);\n\trelease_sock(sk);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,83 +1,15 @@\n static int isotp_setsockopt(struct socket *sock, int level, int optname,\n \t\t\t    sockptr_t optval, unsigned int optlen)\n+\n {\n \tstruct sock *sk = sock->sk;\n-\tstruct isotp_sock *so = isotp_sk(sk);\n-\tint ret = 0;\n+\tint ret;\n \n \tif (level != SOL_CAN_ISOTP)\n \t\treturn -EINVAL;\n \n-\tif (so->bound)\n-\t\treturn -EISCONN;\n-\n-\tswitch (optname) {\n-\tcase CAN_ISOTP_OPTS:\n-\t\tif (optlen != sizeof(struct can_isotp_options))\n-\t\t\treturn -EINVAL;\n-\n-\t\tif (copy_from_sockptr(&so->opt, optval, optlen))\n-\t\t\treturn -EFAULT;\n-\n-\t\t/* no separate rx_ext_address is given => use ext_address */\n-\t\tif (!(so->opt.flags & CAN_ISOTP_RX_EXT_ADDR))\n-\t\t\tso->opt.rx_ext_address = so->opt.ext_address;\n-\t\tbreak;\n-\n-\tcase CAN_ISOTP_RECV_FC:\n-\t\tif (optlen != sizeof(struct can_isotp_fc_options))\n-\t\t\treturn -EINVAL;\n-\n-\t\tif (copy_from_sockptr(&so->rxfc, optval, optlen))\n-\t\t\treturn -EFAULT;\n-\t\tbreak;\n-\n-\tcase CAN_ISOTP_TX_STMIN:\n-\t\tif (optlen != sizeof(u32))\n-\t\t\treturn -EINVAL;\n-\n-\t\tif (copy_from_sockptr(&so->force_tx_stmin, optval, optlen))\n-\t\t\treturn -EFAULT;\n-\t\tbreak;\n-\n-\tcase CAN_ISOTP_RX_STMIN:\n-\t\tif (optlen != sizeof(u32))\n-\t\t\treturn -EINVAL;\n-\n-\t\tif (copy_from_sockptr(&so->force_rx_stmin, optval, optlen))\n-\t\t\treturn -EFAULT;\n-\t\tbreak;\n-\n-\tcase CAN_ISOTP_LL_OPTS:\n-\t\tif (optlen == sizeof(struct can_isotp_ll_options)) {\n-\t\t\tstruct can_isotp_ll_options ll;\n-\n-\t\t\tif (copy_from_sockptr(&ll, optval, optlen))\n-\t\t\t\treturn -EFAULT;\n-\n-\t\t\t/* check for correct ISO 11898-1 DLC data length */\n-\t\t\tif (ll.tx_dl != padlen(ll.tx_dl))\n-\t\t\t\treturn -EINVAL;\n-\n-\t\t\tif (ll.mtu != CAN_MTU && ll.mtu != CANFD_MTU)\n-\t\t\t\treturn -EINVAL;\n-\n-\t\t\tif (ll.mtu == CAN_MTU &&\n-\t\t\t    (ll.tx_dl > CAN_MAX_DLEN || ll.tx_flags != 0))\n-\t\t\t\treturn -EINVAL;\n-\n-\t\t\tmemcpy(&so->ll, &ll, sizeof(ll));\n-\n-\t\t\t/* set ll_dl for tx path to similar place as for rx */\n-\t\t\tso->tx.ll_dl = ll.tx_dl;\n-\t\t} else {\n-\t\t\treturn -EINVAL;\n-\t\t}\n-\t\tbreak;\n-\n-\tdefault:\n-\t\tret = -ENOPROTOOPT;\n-\t}\n-\n+\tlock_sock(sk);\n+\tret = isotp_setsockopt_locked(sock, level, optname, optval, optlen);\n+\trelease_sock(sk);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tint ret;",
                "\tlock_sock(sk);",
                "\tret = isotp_setsockopt_locked(sock, level, optname, optval, optlen);",
                "\trelease_sock(sk);"
            ],
            "deleted": [
                "\tstruct isotp_sock *so = isotp_sk(sk);",
                "\tint ret = 0;",
                "\tif (so->bound)",
                "\t\treturn -EISCONN;",
                "",
                "\tswitch (optname) {",
                "\tcase CAN_ISOTP_OPTS:",
                "\t\tif (optlen != sizeof(struct can_isotp_options))",
                "\t\t\treturn -EINVAL;",
                "",
                "\t\tif (copy_from_sockptr(&so->opt, optval, optlen))",
                "\t\t\treturn -EFAULT;",
                "",
                "\t\t/* no separate rx_ext_address is given => use ext_address */",
                "\t\tif (!(so->opt.flags & CAN_ISOTP_RX_EXT_ADDR))",
                "\t\t\tso->opt.rx_ext_address = so->opt.ext_address;",
                "\t\tbreak;",
                "",
                "\tcase CAN_ISOTP_RECV_FC:",
                "\t\tif (optlen != sizeof(struct can_isotp_fc_options))",
                "\t\t\treturn -EINVAL;",
                "",
                "\t\tif (copy_from_sockptr(&so->rxfc, optval, optlen))",
                "\t\t\treturn -EFAULT;",
                "\t\tbreak;",
                "",
                "\tcase CAN_ISOTP_TX_STMIN:",
                "\t\tif (optlen != sizeof(u32))",
                "\t\t\treturn -EINVAL;",
                "",
                "\t\tif (copy_from_sockptr(&so->force_tx_stmin, optval, optlen))",
                "\t\t\treturn -EFAULT;",
                "\t\tbreak;",
                "",
                "\tcase CAN_ISOTP_RX_STMIN:",
                "\t\tif (optlen != sizeof(u32))",
                "\t\t\treturn -EINVAL;",
                "",
                "\t\tif (copy_from_sockptr(&so->force_rx_stmin, optval, optlen))",
                "\t\t\treturn -EFAULT;",
                "\t\tbreak;",
                "",
                "\tcase CAN_ISOTP_LL_OPTS:",
                "\t\tif (optlen == sizeof(struct can_isotp_ll_options)) {",
                "\t\t\tstruct can_isotp_ll_options ll;",
                "",
                "\t\t\tif (copy_from_sockptr(&ll, optval, optlen))",
                "\t\t\t\treturn -EFAULT;",
                "",
                "\t\t\t/* check for correct ISO 11898-1 DLC data length */",
                "\t\t\tif (ll.tx_dl != padlen(ll.tx_dl))",
                "\t\t\t\treturn -EINVAL;",
                "",
                "\t\t\tif (ll.mtu != CAN_MTU && ll.mtu != CANFD_MTU)",
                "\t\t\t\treturn -EINVAL;",
                "",
                "\t\t\tif (ll.mtu == CAN_MTU &&",
                "\t\t\t    (ll.tx_dl > CAN_MAX_DLEN || ll.tx_flags != 0))",
                "\t\t\t\treturn -EINVAL;",
                "",
                "\t\t\tmemcpy(&so->ll, &ll, sizeof(ll));",
                "",
                "\t\t\t/* set ll_dl for tx path to similar place as for rx */",
                "\t\t\tso->tx.ll_dl = ll.tx_dl;",
                "\t\t} else {",
                "\t\t\treturn -EINVAL;",
                "\t\t}",
                "\t\tbreak;",
                "",
                "\tdefault:",
                "\t\tret = -ENOPROTOOPT;",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.11 through 5.12.2, isotp_setsockopt in net/can/isotp.c allows privilege escalation to root by leveraging a use-after-free. (This does not affect earlier versions that lack CAN ISOTP SF_BROADCAST support.)",
        "id": 2966
    },
    {
        "cve_id": "CVE-2019-11811",
        "code_before_change": "static int try_smi_init(struct smi_info *new_smi)\n{\n\tint rv = 0;\n\tint i;\n\tchar *init_name = NULL;\n\n\tpr_info(\"Trying %s-specified %s state machine at %s address 0x%lx, slave address 0x%x, irq %d\\n\",\n\t\tipmi_addr_src_to_str(new_smi->io.addr_source),\n\t\tsi_to_str[new_smi->io.si_type],\n\t\taddr_space_to_str[new_smi->io.addr_type],\n\t\tnew_smi->io.addr_data,\n\t\tnew_smi->io.slave_addr, new_smi->io.irq);\n\n\tswitch (new_smi->io.si_type) {\n\tcase SI_KCS:\n\t\tnew_smi->handlers = &kcs_smi_handlers;\n\t\tbreak;\n\n\tcase SI_SMIC:\n\t\tnew_smi->handlers = &smic_smi_handlers;\n\t\tbreak;\n\n\tcase SI_BT:\n\t\tnew_smi->handlers = &bt_smi_handlers;\n\t\tbreak;\n\n\tdefault:\n\t\t/* No support for anything else yet. */\n\t\trv = -EIO;\n\t\tgoto out_err;\n\t}\n\n\tnew_smi->si_num = smi_num;\n\n\t/* Do this early so it's available for logs. */\n\tif (!new_smi->io.dev) {\n\t\tinit_name = kasprintf(GFP_KERNEL, \"ipmi_si.%d\",\n\t\t\t\t      new_smi->si_num);\n\n\t\t/*\n\t\t * If we don't already have a device from something\n\t\t * else (like PCI), then register a new one.\n\t\t */\n\t\tnew_smi->pdev = platform_device_alloc(\"ipmi_si\",\n\t\t\t\t\t\t      new_smi->si_num);\n\t\tif (!new_smi->pdev) {\n\t\t\tpr_err(\"Unable to allocate platform device\\n\");\n\t\t\trv = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\t\tnew_smi->io.dev = &new_smi->pdev->dev;\n\t\tnew_smi->io.dev->driver = &ipmi_platform_driver.driver;\n\t\t/* Nulled by device_add() */\n\t\tnew_smi->io.dev->init_name = init_name;\n\t}\n\n\t/* Allocate the state machine's data and initialize it. */\n\tnew_smi->si_sm = kmalloc(new_smi->handlers->size(), GFP_KERNEL);\n\tif (!new_smi->si_sm) {\n\t\trv = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\tnew_smi->io.io_size = new_smi->handlers->init_data(new_smi->si_sm,\n\t\t\t\t\t\t\t   &new_smi->io);\n\n\t/* Now that we know the I/O size, we can set up the I/O. */\n\trv = new_smi->io.io_setup(&new_smi->io);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev, \"Could not set up I/O space\\n\");\n\t\tgoto out_err;\n\t}\n\n\t/* Do low-level detection first. */\n\tif (new_smi->handlers->detect(new_smi->si_sm)) {\n\t\tif (new_smi->io.addr_source)\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t\t\"Interface detection failed\\n\");\n\t\trv = -ENODEV;\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * Attempt a get device id command.  If it fails, we probably\n\t * don't have a BMC here.\n\t */\n\trv = try_get_dev_id(new_smi);\n\tif (rv) {\n\t\tif (new_smi->io.addr_source)\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t       \"There appears to be no BMC at this location\\n\");\n\t\tgoto out_err;\n\t}\n\n\tsetup_oem_data_handler(new_smi);\n\tsetup_xaction_handlers(new_smi);\n\tcheck_for_broken_irqs(new_smi);\n\n\tnew_smi->waiting_msg = NULL;\n\tnew_smi->curr_msg = NULL;\n\tatomic_set(&new_smi->req_events, 0);\n\tnew_smi->run_to_completion = false;\n\tfor (i = 0; i < SI_NUM_STATS; i++)\n\t\tatomic_set(&new_smi->stats[i], 0);\n\n\tnew_smi->interrupt_disabled = true;\n\tatomic_set(&new_smi->need_watch, 0);\n\n\trv = try_enable_event_buffer(new_smi);\n\tif (rv == 0)\n\t\tnew_smi->has_event_buffer = true;\n\n\t/*\n\t * Start clearing the flags before we enable interrupts or the\n\t * timer to avoid racing with the timer.\n\t */\n\tstart_clear_flags(new_smi);\n\n\t/*\n\t * IRQ is defined to be set when non-zero.  req_events will\n\t * cause a global flags check that will enable interrupts.\n\t */\n\tif (new_smi->io.irq) {\n\t\tnew_smi->interrupt_disabled = false;\n\t\tatomic_set(&new_smi->req_events, 1);\n\t}\n\n\tif (new_smi->pdev && !new_smi->pdev_registered) {\n\t\trv = platform_device_add(new_smi->pdev);\n\t\tif (rv) {\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t\t\"Unable to register system interface device: %d\\n\",\n\t\t\t\trv);\n\t\t\tgoto out_err;\n\t\t}\n\t\tnew_smi->pdev_registered = true;\n\t}\n\n\tdev_set_drvdata(new_smi->io.dev, new_smi);\n\trv = device_add_group(new_smi->io.dev, &ipmi_si_dev_attr_group);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev,\n\t\t\t\"Unable to add device attributes: error %d\\n\",\n\t\t\trv);\n\t\tgoto out_err;\n\t}\n\tnew_smi->dev_group_added = true;\n\n\trv = ipmi_register_smi(&handlers,\n\t\t\t       new_smi,\n\t\t\t       new_smi->io.dev,\n\t\t\t       new_smi->io.slave_addr);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev,\n\t\t\t\"Unable to register device: error %d\\n\",\n\t\t\trv);\n\t\tgoto out_err;\n\t}\n\n\t/* Don't increment till we know we have succeeded. */\n\tsmi_num++;\n\n\tdev_info(new_smi->io.dev, \"IPMI %s interface initialized\\n\",\n\t\t si_to_str[new_smi->io.si_type]);\n\n\tWARN_ON(new_smi->io.dev->init_name != NULL);\n\n out_err:\n\tkfree(init_name);\n\treturn rv;\n}",
        "code_after_change": "static int try_smi_init(struct smi_info *new_smi)\n{\n\tint rv = 0;\n\tint i;\n\tchar *init_name = NULL;\n\n\tpr_info(\"Trying %s-specified %s state machine at %s address 0x%lx, slave address 0x%x, irq %d\\n\",\n\t\tipmi_addr_src_to_str(new_smi->io.addr_source),\n\t\tsi_to_str[new_smi->io.si_type],\n\t\taddr_space_to_str[new_smi->io.addr_type],\n\t\tnew_smi->io.addr_data,\n\t\tnew_smi->io.slave_addr, new_smi->io.irq);\n\n\tswitch (new_smi->io.si_type) {\n\tcase SI_KCS:\n\t\tnew_smi->handlers = &kcs_smi_handlers;\n\t\tbreak;\n\n\tcase SI_SMIC:\n\t\tnew_smi->handlers = &smic_smi_handlers;\n\t\tbreak;\n\n\tcase SI_BT:\n\t\tnew_smi->handlers = &bt_smi_handlers;\n\t\tbreak;\n\n\tdefault:\n\t\t/* No support for anything else yet. */\n\t\trv = -EIO;\n\t\tgoto out_err;\n\t}\n\n\tnew_smi->si_num = smi_num;\n\n\t/* Do this early so it's available for logs. */\n\tif (!new_smi->io.dev) {\n\t\tinit_name = kasprintf(GFP_KERNEL, \"ipmi_si.%d\",\n\t\t\t\t      new_smi->si_num);\n\n\t\t/*\n\t\t * If we don't already have a device from something\n\t\t * else (like PCI), then register a new one.\n\t\t */\n\t\tnew_smi->pdev = platform_device_alloc(\"ipmi_si\",\n\t\t\t\t\t\t      new_smi->si_num);\n\t\tif (!new_smi->pdev) {\n\t\t\tpr_err(\"Unable to allocate platform device\\n\");\n\t\t\trv = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\t\tnew_smi->io.dev = &new_smi->pdev->dev;\n\t\tnew_smi->io.dev->driver = &ipmi_platform_driver.driver;\n\t\t/* Nulled by device_add() */\n\t\tnew_smi->io.dev->init_name = init_name;\n\t}\n\n\t/* Allocate the state machine's data and initialize it. */\n\tnew_smi->si_sm = kmalloc(new_smi->handlers->size(), GFP_KERNEL);\n\tif (!new_smi->si_sm) {\n\t\trv = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\tnew_smi->io.io_size = new_smi->handlers->init_data(new_smi->si_sm,\n\t\t\t\t\t\t\t   &new_smi->io);\n\n\t/* Now that we know the I/O size, we can set up the I/O. */\n\trv = new_smi->io.io_setup(&new_smi->io);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev, \"Could not set up I/O space\\n\");\n\t\tgoto out_err;\n\t}\n\n\t/* Do low-level detection first. */\n\tif (new_smi->handlers->detect(new_smi->si_sm)) {\n\t\tif (new_smi->io.addr_source)\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t\t\"Interface detection failed\\n\");\n\t\trv = -ENODEV;\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * Attempt a get device id command.  If it fails, we probably\n\t * don't have a BMC here.\n\t */\n\trv = try_get_dev_id(new_smi);\n\tif (rv) {\n\t\tif (new_smi->io.addr_source)\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t       \"There appears to be no BMC at this location\\n\");\n\t\tgoto out_err;\n\t}\n\n\tsetup_oem_data_handler(new_smi);\n\tsetup_xaction_handlers(new_smi);\n\tcheck_for_broken_irqs(new_smi);\n\n\tnew_smi->waiting_msg = NULL;\n\tnew_smi->curr_msg = NULL;\n\tatomic_set(&new_smi->req_events, 0);\n\tnew_smi->run_to_completion = false;\n\tfor (i = 0; i < SI_NUM_STATS; i++)\n\t\tatomic_set(&new_smi->stats[i], 0);\n\n\tnew_smi->interrupt_disabled = true;\n\tatomic_set(&new_smi->need_watch, 0);\n\n\trv = try_enable_event_buffer(new_smi);\n\tif (rv == 0)\n\t\tnew_smi->has_event_buffer = true;\n\n\t/*\n\t * Start clearing the flags before we enable interrupts or the\n\t * timer to avoid racing with the timer.\n\t */\n\tstart_clear_flags(new_smi);\n\n\t/*\n\t * IRQ is defined to be set when non-zero.  req_events will\n\t * cause a global flags check that will enable interrupts.\n\t */\n\tif (new_smi->io.irq) {\n\t\tnew_smi->interrupt_disabled = false;\n\t\tatomic_set(&new_smi->req_events, 1);\n\t}\n\n\tif (new_smi->pdev && !new_smi->pdev_registered) {\n\t\trv = platform_device_add(new_smi->pdev);\n\t\tif (rv) {\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t\t\"Unable to register system interface device: %d\\n\",\n\t\t\t\trv);\n\t\t\tgoto out_err;\n\t\t}\n\t\tnew_smi->pdev_registered = true;\n\t}\n\n\tdev_set_drvdata(new_smi->io.dev, new_smi);\n\trv = device_add_group(new_smi->io.dev, &ipmi_si_dev_attr_group);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev,\n\t\t\t\"Unable to add device attributes: error %d\\n\",\n\t\t\trv);\n\t\tgoto out_err;\n\t}\n\tnew_smi->dev_group_added = true;\n\n\trv = ipmi_register_smi(&handlers,\n\t\t\t       new_smi,\n\t\t\t       new_smi->io.dev,\n\t\t\t       new_smi->io.slave_addr);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev,\n\t\t\t\"Unable to register device: error %d\\n\",\n\t\t\trv);\n\t\tgoto out_err;\n\t}\n\n\t/* Don't increment till we know we have succeeded. */\n\tsmi_num++;\n\n\tdev_info(new_smi->io.dev, \"IPMI %s interface initialized\\n\",\n\t\t si_to_str[new_smi->io.si_type]);\n\n\tWARN_ON(new_smi->io.dev->init_name != NULL);\n\n out_err:\n\tif (rv && new_smi->io.io_cleanup) {\n\t\tnew_smi->io.io_cleanup(&new_smi->io);\n\t\tnew_smi->io.io_cleanup = NULL;\n\t}\n\n\tkfree(init_name);\n\treturn rv;\n}",
        "patch": "--- code before\n+++ code after\n@@ -165,6 +165,11 @@\n \tWARN_ON(new_smi->io.dev->init_name != NULL);\n \n  out_err:\n+\tif (rv && new_smi->io.io_cleanup) {\n+\t\tnew_smi->io.io_cleanup(&new_smi->io);\n+\t\tnew_smi->io.io_cleanup = NULL;\n+\t}\n+\n \tkfree(init_name);\n \treturn rv;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (rv && new_smi->io.io_cleanup) {",
                "\t\tnew_smi->io.io_cleanup(&new_smi->io);",
                "\t\tnew_smi->io.io_cleanup = NULL;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.4. There is a use-after-free upon attempted read access to /proc/ioports after the ipmi_si module is removed, related to drivers/char/ipmi/ipmi_si_intf.c, drivers/char/ipmi/ipmi_si_mem_io.c, and drivers/char/ipmi/ipmi_si_port_io.c.",
        "id": 1932
    },
    {
        "cve_id": "CVE-2023-20928",
        "code_before_change": "static inline struct vm_area_struct *binder_alloc_get_vma(\n\t\tstruct binder_alloc *alloc)\n{\n\tstruct vm_area_struct *vma = NULL;\n\n\tif (alloc->vma) {\n\t\t/* Look at description in binder_alloc_set_vma */\n\t\tsmp_rmb();\n\t\tvma = alloc->vma;\n\t}\n\treturn vma;\n}",
        "code_after_change": "static inline struct vm_area_struct *binder_alloc_get_vma(\n\t\tstruct binder_alloc *alloc)\n{\n\tstruct vm_area_struct *vma = NULL;\n\n\tif (alloc->vma_addr)\n\t\tvma = vma_lookup(alloc->vma_vm_mm, alloc->vma_addr);\n\n\treturn vma;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,10 +3,8 @@\n {\n \tstruct vm_area_struct *vma = NULL;\n \n-\tif (alloc->vma) {\n-\t\t/* Look at description in binder_alloc_set_vma */\n-\t\tsmp_rmb();\n-\t\tvma = alloc->vma;\n-\t}\n+\tif (alloc->vma_addr)\n+\t\tvma = vma_lookup(alloc->vma_vm_mm, alloc->vma_addr);\n+\n \treturn vma;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (alloc->vma_addr)",
                "\t\tvma = vma_lookup(alloc->vma_vm_mm, alloc->vma_addr);",
                ""
            ],
            "deleted": [
                "\tif (alloc->vma) {",
                "\t\t/* Look at description in binder_alloc_set_vma */",
                "\t\tsmp_rmb();",
                "\t\tvma = alloc->vma;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_vma_close of binder.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-254837884References: Upstream kernel",
        "id": 3910
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static struct drm_framebuffer *vmw_kms_fb_create(struct drm_device *dev,\n\t\t\t\t\t\t struct drm_file *file_priv,\n\t\t\t\t\t\t const struct drm_mode_fb_cmd2 *mode_cmd)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_framebuffer *vfb = NULL;\n\tstruct vmw_surface *surface = NULL;\n\tstruct vmw_bo *bo = NULL;\n\tint ret;\n\n\t/* returns either a bo or surface */\n\tret = vmw_user_lookup_handle(dev_priv, file_priv,\n\t\t\t\t     mode_cmd->handles[0],\n\t\t\t\t     &surface, &bo);\n\tif (ret) {\n\t\tDRM_ERROR(\"Invalid buffer object handle %u (0x%x).\\n\",\n\t\t\t  mode_cmd->handles[0], mode_cmd->handles[0]);\n\t\tgoto err_out;\n\t}\n\n\n\tif (!bo &&\n\t    !vmw_kms_srf_ok(dev_priv, mode_cmd->width, mode_cmd->height)) {\n\t\tDRM_ERROR(\"Surface size cannot exceed %dx%d\\n\",\n\t\t\tdev_priv->texture_max_width,\n\t\t\tdev_priv->texture_max_height);\n\t\tgoto err_out;\n\t}\n\n\n\tvfb = vmw_kms_new_framebuffer(dev_priv, bo, surface,\n\t\t\t\t      !(dev_priv->capabilities & SVGA_CAP_3D),\n\t\t\t\t      mode_cmd);\n\tif (IS_ERR(vfb)) {\n\t\tret = PTR_ERR(vfb);\n\t\tgoto err_out;\n\t}\n\nerr_out:\n\t/* vmw_user_lookup_handle takes one ref so does new_fb */\n\tif (bo)\n\t\tvmw_user_bo_unref(bo);\n\tif (surface)\n\t\tvmw_surface_unreference(&surface);\n\n\tif (ret) {\n\t\tDRM_ERROR(\"failed to create vmw_framebuffer: %i\\n\", ret);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn &vfb->base;\n}",
        "code_after_change": "static struct drm_framebuffer *vmw_kms_fb_create(struct drm_device *dev,\n\t\t\t\t\t\t struct drm_file *file_priv,\n\t\t\t\t\t\t const struct drm_mode_fb_cmd2 *mode_cmd)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_framebuffer *vfb = NULL;\n\tstruct vmw_surface *surface = NULL;\n\tstruct vmw_bo *bo = NULL;\n\tint ret;\n\n\t/* returns either a bo or surface */\n\tret = vmw_user_lookup_handle(dev_priv, file_priv,\n\t\t\t\t     mode_cmd->handles[0],\n\t\t\t\t     &surface, &bo);\n\tif (ret) {\n\t\tDRM_ERROR(\"Invalid buffer object handle %u (0x%x).\\n\",\n\t\t\t  mode_cmd->handles[0], mode_cmd->handles[0]);\n\t\tgoto err_out;\n\t}\n\n\n\tif (!bo &&\n\t    !vmw_kms_srf_ok(dev_priv, mode_cmd->width, mode_cmd->height)) {\n\t\tDRM_ERROR(\"Surface size cannot exceed %dx%d\\n\",\n\t\t\tdev_priv->texture_max_width,\n\t\t\tdev_priv->texture_max_height);\n\t\tgoto err_out;\n\t}\n\n\n\tvfb = vmw_kms_new_framebuffer(dev_priv, bo, surface,\n\t\t\t\t      !(dev_priv->capabilities & SVGA_CAP_3D),\n\t\t\t\t      mode_cmd);\n\tif (IS_ERR(vfb)) {\n\t\tret = PTR_ERR(vfb);\n\t\tgoto err_out;\n\t}\n\nerr_out:\n\t/* vmw_user_lookup_handle takes one ref so does new_fb */\n\tif (bo)\n\t\tvmw_user_bo_unref(&bo);\n\tif (surface)\n\t\tvmw_surface_unreference(&surface);\n\n\tif (ret) {\n\t\tDRM_ERROR(\"failed to create vmw_framebuffer: %i\\n\", ret);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn &vfb->base;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,7 +39,7 @@\n err_out:\n \t/* vmw_user_lookup_handle takes one ref so does new_fb */\n \tif (bo)\n-\t\tvmw_user_bo_unref(bo);\n+\t\tvmw_user_bo_unref(&bo);\n \tif (surface)\n \t\tvmw_surface_unreference(&surface);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tvmw_user_bo_unref(&bo);"
            ],
            "deleted": [
                "\t\tvmw_user_bo_unref(bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4276
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "unsigned int\nip6t_do_table(struct sk_buff *skb,\n\t      const struct nf_hook_state *state,\n\t      struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\t/* Initializing verdict to NF_DROP keeps gcc happy. */\n\tunsigned int verdict = NF_DROP;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tstruct ip6t_entry *e, **jumpstack;\n\tunsigned int stackidx, cpu;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\t/* Initialization */\n\tstackidx = 0;\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\t/* We handle fragments by dealing with the first fragment as\n\t * if it was a normal packet.  All other fragments are treated\n\t * normally, except that they will NEVER match rules that ask\n\t * things we don't know, ie. tcp syn flag or ports).  If the\n\t * rule is also a fragment-specific rule, non-fragments won't\n\t * match it. */\n\tacpar.hotdrop = false;\n\tacpar.state   = state;\n\n\tWARN_ON(!(table->valid_hooks & (1 << hook)));\n\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = READ_ONCE(table->private); /* Address dependency. */\n\tcpu        = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct ip6t_entry **)private->jumpstack[cpu];\n\n\t/* Switch to alternate jumpstack if we're being invoked via TEE.\n\t * TEE issues XT_CONTINUE verdict on original skb so we must not\n\t * clobber the jumpstack.\n\t *\n\t * For recursion via REJECT or SYNPROXY the stack will be clobbered\n\t * but it is no problem since absolute verdict is issued by these.\n\t */\n\tif (static_key_false(&xt_tee_enabled))\n\t\tjumpstack += private->stacksize * __this_cpu_read(nf_skb_duplicated);\n\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tconst struct xt_entry_match *ematch;\n\t\tstruct xt_counters *counter;\n\n\t\tWARN_ON(!e);\n\t\tacpar.thoff = 0;\n\t\tif (!ip6_packet_match(skb, indev, outdev, &e->ipv6,\n\t\t    &acpar.thoff, &acpar.fragoff, &acpar.hotdrop)) {\n no_match:\n\t\t\te = ip6t_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\txt_ematch_foreach(ematch, e) {\n\t\t\tacpar.match     = ematch->u.kernel.match;\n\t\t\tacpar.matchinfo = ematch->data;\n\t\t\tif (!acpar.match->match(skb, &acpar))\n\t\t\t\tgoto no_match;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, skb->len, 1);\n\n\t\tt = ip6t_get_target_c(e);\n\t\tWARN_ON(!t->u.kernel.target);\n\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE)\n\t\t/* The packet is traced: log it */\n\t\tif (unlikely(skb->nf_trace))\n\t\t\ttrace_packet(state->net, skb, hook, state->in,\n\t\t\t\t     state->out, table->name, private, e);\n#endif\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0)\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t    private->underflow[hook]);\n\t\t\t\telse\n\t\t\t\t\te = ip6t_next_entry(jumpstack[--stackidx]);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v != ip6t_next_entry(e) &&\n\t\t\t    !(e->ipv6.flags & IP6T_F_GOTO)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\t\tif (verdict == XT_CONTINUE)\n\t\t\te = ip6t_next_entry(e);\n\t\telse\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t} while (!acpar.hotdrop);\n\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse return verdict;\n}",
        "code_after_change": "unsigned int\nip6t_do_table(struct sk_buff *skb,\n\t      const struct nf_hook_state *state,\n\t      struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\t/* Initializing verdict to NF_DROP keeps gcc happy. */\n\tunsigned int verdict = NF_DROP;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tstruct ip6t_entry *e, **jumpstack;\n\tunsigned int stackidx, cpu;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\t/* Initialization */\n\tstackidx = 0;\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\t/* We handle fragments by dealing with the first fragment as\n\t * if it was a normal packet.  All other fragments are treated\n\t * normally, except that they will NEVER match rules that ask\n\t * things we don't know, ie. tcp syn flag or ports).  If the\n\t * rule is also a fragment-specific rule, non-fragments won't\n\t * match it. */\n\tacpar.hotdrop = false;\n\tacpar.state   = state;\n\n\tWARN_ON(!(table->valid_hooks & (1 << hook)));\n\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = rcu_access_pointer(table->private);\n\tcpu        = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct ip6t_entry **)private->jumpstack[cpu];\n\n\t/* Switch to alternate jumpstack if we're being invoked via TEE.\n\t * TEE issues XT_CONTINUE verdict on original skb so we must not\n\t * clobber the jumpstack.\n\t *\n\t * For recursion via REJECT or SYNPROXY the stack will be clobbered\n\t * but it is no problem since absolute verdict is issued by these.\n\t */\n\tif (static_key_false(&xt_tee_enabled))\n\t\tjumpstack += private->stacksize * __this_cpu_read(nf_skb_duplicated);\n\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tconst struct xt_entry_match *ematch;\n\t\tstruct xt_counters *counter;\n\n\t\tWARN_ON(!e);\n\t\tacpar.thoff = 0;\n\t\tif (!ip6_packet_match(skb, indev, outdev, &e->ipv6,\n\t\t    &acpar.thoff, &acpar.fragoff, &acpar.hotdrop)) {\n no_match:\n\t\t\te = ip6t_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\txt_ematch_foreach(ematch, e) {\n\t\t\tacpar.match     = ematch->u.kernel.match;\n\t\t\tacpar.matchinfo = ematch->data;\n\t\t\tif (!acpar.match->match(skb, &acpar))\n\t\t\t\tgoto no_match;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, skb->len, 1);\n\n\t\tt = ip6t_get_target_c(e);\n\t\tWARN_ON(!t->u.kernel.target);\n\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE)\n\t\t/* The packet is traced: log it */\n\t\tif (unlikely(skb->nf_trace))\n\t\t\ttrace_packet(state->net, skb, hook, state->in,\n\t\t\t\t     state->out, table->name, private, e);\n#endif\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0)\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t    private->underflow[hook]);\n\t\t\t\telse\n\t\t\t\t\te = ip6t_next_entry(jumpstack[--stackidx]);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v != ip6t_next_entry(e) &&\n\t\t\t    !(e->ipv6.flags & IP6T_F_GOTO)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\t\tif (verdict == XT_CONTINUE)\n\t\t\te = ip6t_next_entry(e);\n\t\telse\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t} while (!acpar.hotdrop);\n\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse return verdict;\n}",
        "patch": "--- code before\n+++ code after\n@@ -32,7 +32,7 @@\n \n \tlocal_bh_disable();\n \taddend = xt_write_recseq_begin();\n-\tprivate = READ_ONCE(table->private); /* Address dependency. */\n+\tprivate = rcu_access_pointer(table->private);\n \tcpu        = smp_processor_id();\n \ttable_base = private->entries;\n \tjumpstack  = (struct ip6t_entry **)private->jumpstack[cpu];",
        "function_modified_lines": {
            "added": [
                "\tprivate = rcu_access_pointer(table->private);"
            ],
            "deleted": [
                "\tprivate = READ_ONCE(table->private); /* Address dependency. */"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2785
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "void vivid_stop_generating_vid_cap(struct vivid_dev *dev, bool *pstreaming)\n{\n\tdprintk(dev, 1, \"%s\\n\", __func__);\n\n\tif (dev->kthread_vid_cap == NULL)\n\t\treturn;\n\n\t*pstreaming = false;\n\tif (pstreaming == &dev->vid_cap_streaming) {\n\t\t/* Release all active buffers */\n\t\twhile (!list_empty(&dev->vid_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vid_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vid_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vid_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->vbi_cap_streaming) {\n\t\twhile (!list_empty(&dev->vbi_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vbi_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vbi_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vbi_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->meta_cap_streaming) {\n\t\twhile (!list_empty(&dev->meta_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->meta_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_meta_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"meta_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (dev->vid_cap_streaming || dev->vbi_cap_streaming ||\n\t    dev->meta_cap_streaming)\n\t\treturn;\n\n\t/* shutdown control thread */\n\tvivid_grab_controls(dev, false);\n\tmutex_unlock(&dev->mutex);\n\tkthread_stop(dev->kthread_vid_cap);\n\tdev->kthread_vid_cap = NULL;\n\tmutex_lock(&dev->mutex);\n}",
        "code_after_change": "void vivid_stop_generating_vid_cap(struct vivid_dev *dev, bool *pstreaming)\n{\n\tdprintk(dev, 1, \"%s\\n\", __func__);\n\n\tif (dev->kthread_vid_cap == NULL)\n\t\treturn;\n\n\t*pstreaming = false;\n\tif (pstreaming == &dev->vid_cap_streaming) {\n\t\t/* Release all active buffers */\n\t\twhile (!list_empty(&dev->vid_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vid_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vid_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vid_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->vbi_cap_streaming) {\n\t\twhile (!list_empty(&dev->vbi_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->vbi_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_vbi_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"vbi_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (pstreaming == &dev->meta_cap_streaming) {\n\t\twhile (!list_empty(&dev->meta_cap_active)) {\n\t\t\tstruct vivid_buffer *buf;\n\n\t\t\tbuf = list_entry(dev->meta_cap_active.next,\n\t\t\t\t\t struct vivid_buffer, list);\n\t\t\tlist_del(&buf->list);\n\t\t\tv4l2_ctrl_request_complete(buf->vb.vb2_buf.req_obj.req,\n\t\t\t\t\t\t   &dev->ctrl_hdl_meta_cap);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\t\tdprintk(dev, 2, \"meta_cap buffer %d done\\n\",\n\t\t\t\tbuf->vb.vb2_buf.index);\n\t\t}\n\t}\n\n\tif (dev->vid_cap_streaming || dev->vbi_cap_streaming ||\n\t    dev->meta_cap_streaming)\n\t\treturn;\n\n\t/* shutdown control thread */\n\tvivid_grab_controls(dev, false);\n\tkthread_stop(dev->kthread_vid_cap);\n\tdev->kthread_vid_cap = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -58,8 +58,6 @@\n \n \t/* shutdown control thread */\n \tvivid_grab_controls(dev, false);\n-\tmutex_unlock(&dev->mutex);\n \tkthread_stop(dev->kthread_vid_cap);\n \tdev->kthread_vid_cap = NULL;\n-\tmutex_lock(&dev->mutex);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmutex_unlock(&dev->mutex);",
                "\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free.",
        "id": 2091
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "void inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg(&np->opt, NULL);\n\tif (opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n}",
        "code_after_change": "void inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);\n\tif (opt) {\n\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\ttxopt_put(opt);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,9 @@\n \n \t/* Free tx options */\n \n-\topt = xchg(&np->opt, NULL);\n-\tif (opt)\n-\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);\n+\tif (opt) {\n+\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\ttxopt_put(opt);\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);",
                "\tif (opt) {",
                "\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);",
                "\t\ttxopt_put(opt);",
                "\t}"
            ],
            "deleted": [
                "\topt = xchg(&np->opt, NULL);",
                "\tif (opt)",
                "\t\tsock_kfree_s(sk, opt, opt->tot_len);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 992
    },
    {
        "cve_id": "CVE-2023-25012",
        "code_before_change": "static int hid_bigben_play_effect(struct input_dev *dev, void *data,\n\t\t\t struct ff_effect *effect)\n{\n\tstruct hid_device *hid = input_get_drvdata(dev);\n\tstruct bigben_device *bigben = hid_get_drvdata(hid);\n\tu8 right_motor_on;\n\tu8 left_motor_force;\n\tunsigned long flags;\n\n\tif (!bigben) {\n\t\thid_err(hid, \"no device data\\n\");\n\t\treturn 0;\n\t}\n\n\tif (effect->type != FF_RUMBLE)\n\t\treturn 0;\n\n\tright_motor_on   = effect->u.rumble.weak_magnitude ? 1 : 0;\n\tleft_motor_force = effect->u.rumble.strong_magnitude / 256;\n\n\tif (right_motor_on != bigben->right_motor_on ||\n\t\t\tleft_motor_force != bigben->left_motor_force) {\n\t\tspin_lock_irqsave(&bigben->lock, flags);\n\t\tbigben->right_motor_on   = right_motor_on;\n\t\tbigben->left_motor_force = left_motor_force;\n\t\tbigben->work_ff = true;\n\t\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\t\tschedule_work(&bigben->worker);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int hid_bigben_play_effect(struct input_dev *dev, void *data,\n\t\t\t struct ff_effect *effect)\n{\n\tstruct hid_device *hid = input_get_drvdata(dev);\n\tstruct bigben_device *bigben = hid_get_drvdata(hid);\n\tu8 right_motor_on;\n\tu8 left_motor_force;\n\tunsigned long flags;\n\n\tif (!bigben) {\n\t\thid_err(hid, \"no device data\\n\");\n\t\treturn 0;\n\t}\n\n\tif (effect->type != FF_RUMBLE)\n\t\treturn 0;\n\n\tright_motor_on   = effect->u.rumble.weak_magnitude ? 1 : 0;\n\tleft_motor_force = effect->u.rumble.strong_magnitude / 256;\n\n\tif (right_motor_on != bigben->right_motor_on ||\n\t\t\tleft_motor_force != bigben->left_motor_force) {\n\t\tspin_lock_irqsave(&bigben->lock, flags);\n\t\tbigben->right_motor_on   = right_motor_on;\n\t\tbigben->left_motor_force = left_motor_force;\n\t\tbigben->work_ff = true;\n\t\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\t\tbigben_schedule_work(bigben);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,7 +26,7 @@\n \t\tbigben->work_ff = true;\n \t\tspin_unlock_irqrestore(&bigben->lock, flags);\n \n-\t\tschedule_work(&bigben->worker);\n+\t\tbigben_schedule_work(bigben);\n \t}\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\t\tbigben_schedule_work(bigben);"
            ],
            "deleted": [
                "\t\tschedule_work(&bigben->worker);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel through 6.1.9 has a Use-After-Free in bigben_remove in drivers/hid/hid-bigbenff.c via a crafted USB device because the LED controllers remain registered for too long.",
        "id": 3958
    },
    {
        "cve_id": "CVE-2022-4379",
        "code_before_change": "static int nfsd4_do_async_copy(void *data)\n{\n\tstruct nfsd4_copy *copy = (struct nfsd4_copy *)data;\n\t__be32 nfserr;\n\n\tif (nfsd4_ssc_is_inter(copy)) {\n\t\tstruct file *filp;\n\n\t\tfilp = nfs42_ssc_open(copy->ss_mnt, &copy->c_fh,\n\t\t\t\t      &copy->stateid);\n\t\tif (IS_ERR(filp)) {\n\t\t\tswitch (PTR_ERR(filp)) {\n\t\t\tcase -EBADF:\n\t\t\t\tnfserr = nfserr_wrong_type;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tnfserr = nfserr_offload_denied;\n\t\t\t}\n\t\t\tnfsd4_interssc_disconnect(copy->ss_mnt);\n\t\t\tgoto do_callback;\n\t\t}\n\t\tnfserr = nfsd4_do_copy(copy, filp, copy->nf_dst->nf_file,\n\t\t\t\t       false);\n\t\tnfsd4_cleanup_inter_ssc(copy->ss_mnt, filp, copy->nf_dst);\n\t} else {\n\t\tnfserr = nfsd4_do_copy(copy, copy->nf_src->nf_file,\n\t\t\t\t       copy->nf_dst->nf_file, false);\n\t\tnfsd4_cleanup_intra_ssc(copy->nf_src, copy->nf_dst);\n\t}\n\ndo_callback:\n\tnfsd4_send_cb_offload(copy, nfserr);\n\tcleanup_async_copy(copy);\n\treturn 0;\n}",
        "code_after_change": "static int nfsd4_do_async_copy(void *data)\n{\n\tstruct nfsd4_copy *copy = (struct nfsd4_copy *)data;\n\t__be32 nfserr;\n\n\tif (nfsd4_ssc_is_inter(copy)) {\n\t\tstruct file *filp;\n\n\t\tfilp = nfs42_ssc_open(copy->ss_mnt, &copy->c_fh,\n\t\t\t\t      &copy->stateid);\n\t\tif (IS_ERR(filp)) {\n\t\t\tswitch (PTR_ERR(filp)) {\n\t\t\tcase -EBADF:\n\t\t\t\tnfserr = nfserr_wrong_type;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tnfserr = nfserr_offload_denied;\n\t\t\t}\n\t\t\t/* ss_mnt will be unmounted by the laundromat */\n\t\t\tgoto do_callback;\n\t\t}\n\t\tnfserr = nfsd4_do_copy(copy, filp, copy->nf_dst->nf_file,\n\t\t\t\t       false);\n\t\tnfsd4_cleanup_inter_ssc(copy->ss_mnt, filp, copy->nf_dst);\n\t} else {\n\t\tnfserr = nfsd4_do_copy(copy, copy->nf_src->nf_file,\n\t\t\t\t       copy->nf_dst->nf_file, false);\n\t\tnfsd4_cleanup_intra_ssc(copy->nf_src, copy->nf_dst);\n\t}\n\ndo_callback:\n\tnfsd4_send_cb_offload(copy, nfserr);\n\tcleanup_async_copy(copy);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,7 +16,7 @@\n \t\t\tdefault:\n \t\t\t\tnfserr = nfserr_offload_denied;\n \t\t\t}\n-\t\t\tnfsd4_interssc_disconnect(copy->ss_mnt);\n+\t\t\t/* ss_mnt will be unmounted by the laundromat */\n \t\t\tgoto do_callback;\n \t\t}\n \t\tnfserr = nfsd4_do_copy(copy, filp, copy->nf_dst->nf_file,",
        "function_modified_lines": {
            "added": [
                "\t\t\t/* ss_mnt will be unmounted by the laundromat */"
            ],
            "deleted": [
                "\t\t\tnfsd4_interssc_disconnect(copy->ss_mnt);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in __nfs42_ssc_open() in fs/nfs/nfs4file.c in the Linux kernel. This flaw allows an attacker to conduct a remote denial",
        "id": 3745
    },
    {
        "cve_id": "CVE-2021-29266",
        "code_before_change": "static void vhost_vdpa_config_put(struct vhost_vdpa *v)\n{\n\tif (v->config_ctx)\n\t\teventfd_ctx_put(v->config_ctx);\n}",
        "code_after_change": "static void vhost_vdpa_config_put(struct vhost_vdpa *v)\n{\n\tif (v->config_ctx) {\n\t\teventfd_ctx_put(v->config_ctx);\n\t\tv->config_ctx = NULL;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,7 @@\n static void vhost_vdpa_config_put(struct vhost_vdpa *v)\n {\n-\tif (v->config_ctx)\n+\tif (v->config_ctx) {\n \t\teventfd_ctx_put(v->config_ctx);\n+\t\tv->config_ctx = NULL;\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tif (v->config_ctx) {",
                "\t\tv->config_ctx = NULL;",
                "\t}"
            ],
            "deleted": [
                "\tif (v->config_ctx)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.11.9. drivers/vhost/vdpa.c has a use-after-free because v->config_ctx has an invalid value upon re-opening a character device, aka CID-f6bbf0010ba0.",
        "id": 2948
    },
    {
        "cve_id": "CVE-2018-16882",
        "code_before_change": "static void nested_get_vmcs12_pages(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct page *page;\n\tu64 hpa;\n\n\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {\n\t\t/*\n\t\t * Translate L1 physical address to host physical\n\t\t * address for vmcs02. Keep the page pinned, so this\n\t\t * physical address remains valid. We keep a reference\n\t\t * to it so we can release it later.\n\t\t */\n\t\tif (vmx->nested.apic_access_page) { /* shouldn't happen */\n\t\t\tkvm_release_page_dirty(vmx->nested.apic_access_page);\n\t\t\tvmx->nested.apic_access_page = NULL;\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->apic_access_addr);\n\t\t/*\n\t\t * If translation failed, no matter: This feature asks\n\t\t * to exit when accessing the given address, and if it\n\t\t * can never be accessed, this feature won't do\n\t\t * anything anyway.\n\t\t */\n\t\tif (!is_error_page(page)) {\n\t\t\tvmx->nested.apic_access_page = page;\n\t\t\thpa = page_to_phys(vmx->nested.apic_access_page);\n\t\t\tvmcs_write64(APIC_ACCESS_ADDR, hpa);\n\t\t} else {\n\t\t\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES);\n\t\t}\n\t}\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {\n\t\tif (vmx->nested.virtual_apic_page) { /* shouldn't happen */\n\t\t\tkvm_release_page_dirty(vmx->nested.virtual_apic_page);\n\t\t\tvmx->nested.virtual_apic_page = NULL;\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->virtual_apic_page_addr);\n\n\t\t/*\n\t\t * If translation failed, VM entry will fail because\n\t\t * prepare_vmcs02 set VIRTUAL_APIC_PAGE_ADDR to -1ull.\n\t\t * Failing the vm entry is _not_ what the processor\n\t\t * does but it's basically the only possibility we\n\t\t * have.  We could still enter the guest if CR8 load\n\t\t * exits are enabled, CR8 store exits are enabled, and\n\t\t * virtualize APIC access is disabled; in this case\n\t\t * the processor would never use the TPR shadow and we\n\t\t * could simply clear the bit from the execution\n\t\t * control.  But such a configuration is useless, so\n\t\t * let's keep the code simple.\n\t\t */\n\t\tif (!is_error_page(page)) {\n\t\t\tvmx->nested.virtual_apic_page = page;\n\t\t\thpa = page_to_phys(vmx->nested.virtual_apic_page);\n\t\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR, hpa);\n\t\t}\n\t}\n\n\tif (nested_cpu_has_posted_intr(vmcs12)) {\n\t\tif (vmx->nested.pi_desc_page) { /* shouldn't happen */\n\t\t\tkunmap(vmx->nested.pi_desc_page);\n\t\t\tkvm_release_page_dirty(vmx->nested.pi_desc_page);\n\t\t\tvmx->nested.pi_desc_page = NULL;\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->posted_intr_desc_addr);\n\t\tif (is_error_page(page))\n\t\t\treturn;\n\t\tvmx->nested.pi_desc_page = page;\n\t\tvmx->nested.pi_desc = kmap(vmx->nested.pi_desc_page);\n\t\tvmx->nested.pi_desc =\n\t\t\t(struct pi_desc *)((void *)vmx->nested.pi_desc +\n\t\t\t(unsigned long)(vmcs12->posted_intr_desc_addr &\n\t\t\t(PAGE_SIZE - 1)));\n\t\tvmcs_write64(POSTED_INTR_DESC_ADDR,\n\t\t\tpage_to_phys(vmx->nested.pi_desc_page) +\n\t\t\t(unsigned long)(vmcs12->posted_intr_desc_addr &\n\t\t\t(PAGE_SIZE - 1)));\n\t}\n\tif (nested_vmx_prepare_msr_bitmap(vcpu, vmcs12))\n\t\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t      CPU_BASED_USE_MSR_BITMAPS);\n\telse\n\t\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t\tCPU_BASED_USE_MSR_BITMAPS);\n}",
        "code_after_change": "static void nested_get_vmcs12_pages(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct page *page;\n\tu64 hpa;\n\n\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {\n\t\t/*\n\t\t * Translate L1 physical address to host physical\n\t\t * address for vmcs02. Keep the page pinned, so this\n\t\t * physical address remains valid. We keep a reference\n\t\t * to it so we can release it later.\n\t\t */\n\t\tif (vmx->nested.apic_access_page) { /* shouldn't happen */\n\t\t\tkvm_release_page_dirty(vmx->nested.apic_access_page);\n\t\t\tvmx->nested.apic_access_page = NULL;\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->apic_access_addr);\n\t\t/*\n\t\t * If translation failed, no matter: This feature asks\n\t\t * to exit when accessing the given address, and if it\n\t\t * can never be accessed, this feature won't do\n\t\t * anything anyway.\n\t\t */\n\t\tif (!is_error_page(page)) {\n\t\t\tvmx->nested.apic_access_page = page;\n\t\t\thpa = page_to_phys(vmx->nested.apic_access_page);\n\t\t\tvmcs_write64(APIC_ACCESS_ADDR, hpa);\n\t\t} else {\n\t\t\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES);\n\t\t}\n\t}\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {\n\t\tif (vmx->nested.virtual_apic_page) { /* shouldn't happen */\n\t\t\tkvm_release_page_dirty(vmx->nested.virtual_apic_page);\n\t\t\tvmx->nested.virtual_apic_page = NULL;\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->virtual_apic_page_addr);\n\n\t\t/*\n\t\t * If translation failed, VM entry will fail because\n\t\t * prepare_vmcs02 set VIRTUAL_APIC_PAGE_ADDR to -1ull.\n\t\t * Failing the vm entry is _not_ what the processor\n\t\t * does but it's basically the only possibility we\n\t\t * have.  We could still enter the guest if CR8 load\n\t\t * exits are enabled, CR8 store exits are enabled, and\n\t\t * virtualize APIC access is disabled; in this case\n\t\t * the processor would never use the TPR shadow and we\n\t\t * could simply clear the bit from the execution\n\t\t * control.  But such a configuration is useless, so\n\t\t * let's keep the code simple.\n\t\t */\n\t\tif (!is_error_page(page)) {\n\t\t\tvmx->nested.virtual_apic_page = page;\n\t\t\thpa = page_to_phys(vmx->nested.virtual_apic_page);\n\t\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR, hpa);\n\t\t}\n\t}\n\n\tif (nested_cpu_has_posted_intr(vmcs12)) {\n\t\tif (vmx->nested.pi_desc_page) { /* shouldn't happen */\n\t\t\tkunmap(vmx->nested.pi_desc_page);\n\t\t\tkvm_release_page_dirty(vmx->nested.pi_desc_page);\n\t\t\tvmx->nested.pi_desc_page = NULL;\n\t\t\tvmx->nested.pi_desc = NULL;\n\t\t\tvmcs_write64(POSTED_INTR_DESC_ADDR, -1ull);\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->posted_intr_desc_addr);\n\t\tif (is_error_page(page))\n\t\t\treturn;\n\t\tvmx->nested.pi_desc_page = page;\n\t\tvmx->nested.pi_desc = kmap(vmx->nested.pi_desc_page);\n\t\tvmx->nested.pi_desc =\n\t\t\t(struct pi_desc *)((void *)vmx->nested.pi_desc +\n\t\t\t(unsigned long)(vmcs12->posted_intr_desc_addr &\n\t\t\t(PAGE_SIZE - 1)));\n\t\tvmcs_write64(POSTED_INTR_DESC_ADDR,\n\t\t\tpage_to_phys(vmx->nested.pi_desc_page) +\n\t\t\t(unsigned long)(vmcs12->posted_intr_desc_addr &\n\t\t\t(PAGE_SIZE - 1)));\n\t}\n\tif (nested_vmx_prepare_msr_bitmap(vcpu, vmcs12))\n\t\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t      CPU_BASED_USE_MSR_BITMAPS);\n\telse\n\t\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t\tCPU_BASED_USE_MSR_BITMAPS);\n}",
        "patch": "--- code before\n+++ code after\n@@ -65,6 +65,8 @@\n \t\t\tkunmap(vmx->nested.pi_desc_page);\n \t\t\tkvm_release_page_dirty(vmx->nested.pi_desc_page);\n \t\t\tvmx->nested.pi_desc_page = NULL;\n+\t\t\tvmx->nested.pi_desc = NULL;\n+\t\t\tvmcs_write64(POSTED_INTR_DESC_ADDR, -1ull);\n \t\t}\n \t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->posted_intr_desc_addr);\n \t\tif (is_error_page(page))",
        "function_modified_lines": {
            "added": [
                "\t\t\tvmx->nested.pi_desc = NULL;",
                "\t\t\tvmcs_write64(POSTED_INTR_DESC_ADDR, -1ull);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free issue was found in the way the Linux kernel's KVM hypervisor processed posted interrupts when nested(=1) virtualization is enabled. In nested_get_vmcs12_pages(), in case of an error while processing posted interrupt address, it unmaps the 'pi_desc_page' without resetting 'pi_desc' descriptor address, which is later used in pi_test_and_clear_on(). A guest user/process could use this flaw to crash the host kernel resulting in DoS or potentially gain privileged access to a system. Kernel versions before 4.14.91 and before 4.19.13 are vulnerable.",
        "id": 1721
    },
    {
        "cve_id": "CVE-2023-1193",
        "code_before_change": "void ksmbd_conn_enqueue_request(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct list_head *requests_queue = NULL;\n\n\tif (conn->ops->get_cmd_val(work) != SMB2_CANCEL_HE) {\n\t\trequests_queue = &conn->requests;\n\t\twork->synchronous = true;\n\t}\n\n\tif (requests_queue) {\n\t\tatomic_inc(&conn->req_running);\n\t\tspin_lock(&conn->request_lock);\n\t\tlist_add_tail(&work->request_entry, requests_queue);\n\t\tspin_unlock(&conn->request_lock);\n\t}\n}",
        "code_after_change": "void ksmbd_conn_enqueue_request(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct list_head *requests_queue = NULL;\n\n\tif (conn->ops->get_cmd_val(work) != SMB2_CANCEL_HE)\n\t\trequests_queue = &conn->requests;\n\n\tif (requests_queue) {\n\t\tatomic_inc(&conn->req_running);\n\t\tspin_lock(&conn->request_lock);\n\t\tlist_add_tail(&work->request_entry, requests_queue);\n\t\tspin_unlock(&conn->request_lock);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,10 +3,8 @@\n \tstruct ksmbd_conn *conn = work->conn;\n \tstruct list_head *requests_queue = NULL;\n \n-\tif (conn->ops->get_cmd_val(work) != SMB2_CANCEL_HE) {\n+\tif (conn->ops->get_cmd_val(work) != SMB2_CANCEL_HE)\n \t\trequests_queue = &conn->requests;\n-\t\twork->synchronous = true;\n-\t}\n \n \tif (requests_queue) {\n \t\tatomic_inc(&conn->req_running);",
        "function_modified_lines": {
            "added": [
                "\tif (conn->ops->get_cmd_val(work) != SMB2_CANCEL_HE)"
            ],
            "deleted": [
                "\tif (conn->ops->get_cmd_val(work) != SMB2_CANCEL_HE) {",
                "\t\twork->synchronous = true;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in setup_async_work in the KSMBD implementation of the in-kernel samba server and CIFS in the Linux kernel. This issue could allow an attacker to crash the system by accessing freed work.",
        "id": 3852
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "static int set_evtchn_to_irq(evtchn_port_t evtchn, unsigned int irq)\n{\n\tunsigned row;\n\tunsigned col;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\trow = EVTCHN_ROW(evtchn);\n\tcol = EVTCHN_COL(evtchn);\n\n\tif (evtchn_to_irq[row] == NULL) {\n\t\t/* Unallocated irq entries return -1 anyway */\n\t\tif (irq == -1)\n\t\t\treturn 0;\n\n\t\tevtchn_to_irq[row] = (int *)get_zeroed_page(GFP_KERNEL);\n\t\tif (evtchn_to_irq[row] == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tclear_evtchn_to_irq_row(row);\n\t}\n\n\tevtchn_to_irq[row][col] = irq;\n\treturn 0;\n}",
        "code_after_change": "static int set_evtchn_to_irq(evtchn_port_t evtchn, unsigned int irq)\n{\n\tunsigned row;\n\tunsigned col;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\trow = EVTCHN_ROW(evtchn);\n\tcol = EVTCHN_COL(evtchn);\n\n\tif (evtchn_to_irq[row] == NULL) {\n\t\t/* Unallocated irq entries return -1 anyway */\n\t\tif (irq == -1)\n\t\t\treturn 0;\n\n\t\tevtchn_to_irq[row] = (int *)get_zeroed_page(GFP_KERNEL);\n\t\tif (evtchn_to_irq[row] == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tclear_evtchn_to_irq_row(row);\n\t}\n\n\tWRITE_ONCE(evtchn_to_irq[row][col], irq);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,6 @@\n \t\tclear_evtchn_to_irq_row(row);\n \t}\n \n-\tevtchn_to_irq[row][col] = irq;\n+\tWRITE_ONCE(evtchn_to_irq[row][col], irq);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tWRITE_ONCE(evtchn_to_irq[row][col], irq);"
            ],
            "deleted": [
                "\tevtchn_to_irq[row][col] = irq;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "id": 2624
    },
    {
        "cve_id": "CVE-2023-2162",
        "code_before_change": "static int iscsi_sw_tcp_host_get_param(struct Scsi_Host *shost,\n\t\t\t\t       enum iscsi_host_param param, char *buf)\n{\n\tstruct iscsi_sw_tcp_host *tcp_sw_host = iscsi_host_priv(shost);\n\tstruct iscsi_session *session = tcp_sw_host->session;\n\tstruct iscsi_conn *conn;\n\tstruct iscsi_tcp_conn *tcp_conn;\n\tstruct iscsi_sw_tcp_conn *tcp_sw_conn;\n\tstruct sockaddr_in6 addr;\n\tstruct socket *sock;\n\tint rc;\n\n\tswitch (param) {\n\tcase ISCSI_HOST_PARAM_IPADDRESS:\n\t\tif (!session)\n\t\t\treturn -ENOTCONN;\n\n\t\tspin_lock_bh(&session->frwd_lock);\n\t\tconn = session->leadconn;\n\t\tif (!conn) {\n\t\t\tspin_unlock_bh(&session->frwd_lock);\n\t\t\treturn -ENOTCONN;\n\t\t}\n\t\ttcp_conn = conn->dd_data;\n\t\ttcp_sw_conn = tcp_conn->dd_data;\n\t\t/*\n\t\t * The conn has been setup and bound, so just grab a ref\n\t\t * incase a destroy runs while we are in the net layer.\n\t\t */\n\t\tiscsi_get_conn(conn->cls_conn);\n\t\tspin_unlock_bh(&session->frwd_lock);\n\n\t\tmutex_lock(&tcp_sw_conn->sock_lock);\n\t\tsock = tcp_sw_conn->sock;\n\t\tif (!sock)\n\t\t\trc = -ENOTCONN;\n\t\telse\n\t\t\trc = kernel_getsockname(sock, (struct sockaddr *)&addr);\n\t\tmutex_unlock(&tcp_sw_conn->sock_lock);\n\t\tiscsi_put_conn(conn->cls_conn);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\n\t\treturn iscsi_conn_get_addr_param((struct sockaddr_storage *)\n\t\t\t\t\t\t &addr,\n\t\t\t\t\t\t (enum iscsi_param)param, buf);\n\tdefault:\n\t\treturn iscsi_host_get_param(shost, param, buf);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int iscsi_sw_tcp_host_get_param(struct Scsi_Host *shost,\n\t\t\t\t       enum iscsi_host_param param, char *buf)\n{\n\tstruct iscsi_sw_tcp_host *tcp_sw_host = iscsi_host_priv(shost);\n\tstruct iscsi_session *session;\n\tstruct iscsi_conn *conn;\n\tstruct iscsi_tcp_conn *tcp_conn;\n\tstruct iscsi_sw_tcp_conn *tcp_sw_conn;\n\tstruct sockaddr_in6 addr;\n\tstruct socket *sock;\n\tint rc;\n\n\tswitch (param) {\n\tcase ISCSI_HOST_PARAM_IPADDRESS:\n\t\tsession = tcp_sw_host->session;\n\t\tif (!session)\n\t\t\treturn -ENOTCONN;\n\n\t\tspin_lock_bh(&session->frwd_lock);\n\t\tconn = session->leadconn;\n\t\tif (!conn) {\n\t\t\tspin_unlock_bh(&session->frwd_lock);\n\t\t\treturn -ENOTCONN;\n\t\t}\n\t\ttcp_conn = conn->dd_data;\n\t\ttcp_sw_conn = tcp_conn->dd_data;\n\t\t/*\n\t\t * The conn has been setup and bound, so just grab a ref\n\t\t * incase a destroy runs while we are in the net layer.\n\t\t */\n\t\tiscsi_get_conn(conn->cls_conn);\n\t\tspin_unlock_bh(&session->frwd_lock);\n\n\t\tmutex_lock(&tcp_sw_conn->sock_lock);\n\t\tsock = tcp_sw_conn->sock;\n\t\tif (!sock)\n\t\t\trc = -ENOTCONN;\n\t\telse\n\t\t\trc = kernel_getsockname(sock, (struct sockaddr *)&addr);\n\t\tmutex_unlock(&tcp_sw_conn->sock_lock);\n\t\tiscsi_put_conn(conn->cls_conn);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\n\t\treturn iscsi_conn_get_addr_param((struct sockaddr_storage *)\n\t\t\t\t\t\t &addr,\n\t\t\t\t\t\t (enum iscsi_param)param, buf);\n\tdefault:\n\t\treturn iscsi_host_get_param(shost, param, buf);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n \t\t\t\t       enum iscsi_host_param param, char *buf)\n {\n \tstruct iscsi_sw_tcp_host *tcp_sw_host = iscsi_host_priv(shost);\n-\tstruct iscsi_session *session = tcp_sw_host->session;\n+\tstruct iscsi_session *session;\n \tstruct iscsi_conn *conn;\n \tstruct iscsi_tcp_conn *tcp_conn;\n \tstruct iscsi_sw_tcp_conn *tcp_sw_conn;\n@@ -12,6 +12,7 @@\n \n \tswitch (param) {\n \tcase ISCSI_HOST_PARAM_IPADDRESS:\n+\t\tsession = tcp_sw_host->session;\n \t\tif (!session)\n \t\t\treturn -ENOTCONN;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct iscsi_session *session;",
                "\t\tsession = tcp_sw_host->session;"
            ],
            "deleted": [
                "\tstruct iscsi_session *session = tcp_sw_host->session;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in iscsi_sw_tcp_session_create in drivers/scsi/iscsi_tcp.c in SCSI sub-component in the Linux Kernel. In this flaw an attacker could leak kernel internal information.",
        "id": 3923
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_user_bo_synccpu_release(struct drm_file *filp,\n\t\t\t\t       uint32_t handle,\n\t\t\t\t       uint32_t flags)\n{\n\tstruct vmw_bo *vmw_bo;\n\tint ret = vmw_user_bo_lookup(filp, handle, &vmw_bo);\n\n\tif (!ret) {\n\t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n\t\t\tatomic_dec(&vmw_bo->cpu_writers);\n\t\t}\n\t\tvmw_user_bo_unref(vmw_bo);\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int vmw_user_bo_synccpu_release(struct drm_file *filp,\n\t\t\t\t       uint32_t handle,\n\t\t\t\t       uint32_t flags)\n{\n\tstruct vmw_bo *vmw_bo;\n\tint ret = vmw_user_bo_lookup(filp, handle, &vmw_bo);\n\n\tif (!ret) {\n\t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n\t\t\tatomic_dec(&vmw_bo->cpu_writers);\n\t\t}\n\t\tvmw_user_bo_unref(&vmw_bo);\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n \t\t\tatomic_dec(&vmw_bo->cpu_writers);\n \t\t}\n-\t\tvmw_user_bo_unref(vmw_bo);\n+\t\tvmw_user_bo_unref(&vmw_bo);\n \t}\n \n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "\t\tvmw_user_bo_unref(&vmw_bo);"
            ],
            "deleted": [
                "\t\tvmw_user_bo_unref(vmw_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4270
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "int get_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -1;\n\tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n\t\treturn -1;\n\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];\n}",
        "code_after_change": "int get_evtchn_to_irq(evtchn_port_t evtchn)\n{\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -1;\n\tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n\t\treturn -1;\n\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,5 +4,5 @@\n \t\treturn -1;\n \tif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\n \t\treturn -1;\n-\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];\n+\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);"
            ],
            "deleted": [
                "\treturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "id": 2621
    },
    {
        "cve_id": "CVE-2018-14625",
        "code_before_change": "static struct vhost_vsock *vhost_vsock_get(u32 guest_cid)\n{\n\tstruct vhost_vsock *vsock;\n\n\tspin_lock_bh(&vhost_vsock_lock);\n\tvsock = __vhost_vsock_get(guest_cid);\n\tspin_unlock_bh(&vhost_vsock_lock);\n\n\treturn vsock;\n}",
        "code_after_change": "static struct vhost_vsock *vhost_vsock_get(u32 guest_cid)\n{\n\tstruct vhost_vsock *vsock;\n\n\thash_for_each_possible_rcu(vhost_vsock_hash, vsock, hash, guest_cid) {\n\t\tu32 other_cid = vsock->guest_cid;\n\n\t\t/* Skip instances that have no CID yet */\n\t\tif (other_cid == 0)\n\t\t\tcontinue;\n\n\t\tif (other_cid == guest_cid)\n\t\t\treturn vsock;\n\n\t}\n\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,9 +2,17 @@\n {\n \tstruct vhost_vsock *vsock;\n \n-\tspin_lock_bh(&vhost_vsock_lock);\n-\tvsock = __vhost_vsock_get(guest_cid);\n-\tspin_unlock_bh(&vhost_vsock_lock);\n+\thash_for_each_possible_rcu(vhost_vsock_hash, vsock, hash, guest_cid) {\n+\t\tu32 other_cid = vsock->guest_cid;\n \n-\treturn vsock;\n+\t\t/* Skip instances that have no CID yet */\n+\t\tif (other_cid == 0)\n+\t\t\tcontinue;\n+\n+\t\tif (other_cid == guest_cid)\n+\t\t\treturn vsock;\n+\n+\t}\n+\n+\treturn NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\thash_for_each_possible_rcu(vhost_vsock_hash, vsock, hash, guest_cid) {",
                "\t\tu32 other_cid = vsock->guest_cid;",
                "\t\t/* Skip instances that have no CID yet */",
                "\t\tif (other_cid == 0)",
                "\t\t\tcontinue;",
                "",
                "\t\tif (other_cid == guest_cid)",
                "\t\t\treturn vsock;",
                "",
                "\t}",
                "",
                "\treturn NULL;"
            ],
            "deleted": [
                "\tspin_lock_bh(&vhost_vsock_lock);",
                "\tvsock = __vhost_vsock_get(guest_cid);",
                "\tspin_unlock_bh(&vhost_vsock_lock);",
                "\treturn vsock;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux Kernel where an attacker may be able to have an uncontrolled read to kernel-memory from within a vm guest. A race condition between connect() and close() function may allow an attacker using the AF_VSOCK protocol to gather a 4 byte information leak or possibly intercept or corrupt AF_VSOCK messages destined to other clients.",
        "id": 1693
    },
    {
        "cve_id": "CVE-2022-3239",
        "code_before_change": "static int em28xx_usb_probe(struct usb_interface *intf,\n\t\t\t    const struct usb_device_id *id)\n{\n\tstruct usb_device *udev;\n\tstruct em28xx *dev = NULL;\n\tint retval;\n\tbool has_vendor_audio = false, has_video = false, has_dvb = false;\n\tint i, nr, try_bulk;\n\tconst int ifnum = intf->altsetting[0].desc.bInterfaceNumber;\n\tchar *speed;\n\n\tudev = usb_get_dev(interface_to_usbdev(intf));\n\n\t/* Check to see next free device and mark as used */\n\tdo {\n\t\tnr = find_first_zero_bit(em28xx_devused, EM28XX_MAXBOARDS);\n\t\tif (nr >= EM28XX_MAXBOARDS) {\n\t\t\t/* No free device slots */\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"Driver supports up to %i em28xx boards.\\n\",\n\t\t\t       EM28XX_MAXBOARDS);\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto err_no_slot;\n\t\t}\n\t} while (test_and_set_bit(nr, em28xx_devused));\n\n\t/* Don't register audio interfaces */\n\tif (intf->altsetting[0].desc.bInterfaceClass == USB_CLASS_AUDIO) {\n\t\tdev_info(&intf->dev,\n\t\t\t\"audio device (%04x:%04x): interface %i, class %i\\n\",\n\t\t\tle16_to_cpu(udev->descriptor.idVendor),\n\t\t\tle16_to_cpu(udev->descriptor.idProduct),\n\t\t\tifnum,\n\t\t\tintf->altsetting[0].desc.bInterfaceClass);\n\n\t\tretval = -ENODEV;\n\t\tgoto err;\n\t}\n\n\t/* allocate memory for our device state and initialize it */\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev) {\n\t\tretval = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\t/* compute alternate max packet sizes */\n\tdev->alt_max_pkt_size_isoc = kcalloc(intf->num_altsetting,\n\t\t\t\t\t     sizeof(dev->alt_max_pkt_size_isoc[0]),\n\t\t\t\t\t     GFP_KERNEL);\n\tif (!dev->alt_max_pkt_size_isoc) {\n\t\tkfree(dev);\n\t\tretval = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\t/* Get endpoints */\n\tfor (i = 0; i < intf->num_altsetting; i++) {\n\t\tint ep;\n\n\t\tfor (ep = 0;\n\t\t     ep < intf->altsetting[i].desc.bNumEndpoints;\n\t\t     ep++)\n\t\t\tem28xx_check_usb_descriptor(dev, udev, intf,\n\t\t\t\t\t\t    i, ep,\n\t\t\t\t\t\t    &has_vendor_audio,\n\t\t\t\t\t\t    &has_video,\n\t\t\t\t\t\t    &has_dvb);\n\t}\n\n\tif (!(has_vendor_audio || has_video || has_dvb)) {\n\t\tretval = -ENODEV;\n\t\tgoto err_free;\n\t}\n\n\tswitch (udev->speed) {\n\tcase USB_SPEED_LOW:\n\t\tspeed = \"1.5\";\n\t\tbreak;\n\tcase USB_SPEED_UNKNOWN:\n\tcase USB_SPEED_FULL:\n\t\tspeed = \"12\";\n\t\tbreak;\n\tcase USB_SPEED_HIGH:\n\t\tspeed = \"480\";\n\t\tbreak;\n\tdefault:\n\t\tspeed = \"unknown\";\n\t}\n\n\tdev_info(&intf->dev,\n\t\t\"New device %s %s @ %s Mbps (%04x:%04x, interface %d, class %d)\\n\",\n\t\tudev->manufacturer ? udev->manufacturer : \"\",\n\t\tudev->product ? udev->product : \"\",\n\t\tspeed,\n\t\tle16_to_cpu(udev->descriptor.idVendor),\n\t\tle16_to_cpu(udev->descriptor.idProduct),\n\t\tifnum,\n\t\tintf->altsetting->desc.bInterfaceNumber);\n\n\t/*\n\t * Make sure we have 480 Mbps of bandwidth, otherwise things like\n\t * video stream wouldn't likely work, since 12 Mbps is generally\n\t * not enough even for most Digital TV streams.\n\t */\n\tif (udev->speed != USB_SPEED_HIGH && disable_usb_speed_check == 0) {\n\t\tdev_err(&intf->dev, \"Device initialization failed.\\n\");\n\t\tdev_err(&intf->dev,\n\t\t\t\"Device must be connected to a high-speed USB 2.0 port.\\n\");\n\t\tretval = -ENODEV;\n\t\tgoto err_free;\n\t}\n\n\tdev->devno = nr;\n\tdev->model = id->driver_info;\n\tdev->alt   = -1;\n\tdev->is_audio_only = has_vendor_audio && !(has_video || has_dvb);\n\tdev->has_video = has_video;\n\tdev->ifnum = ifnum;\n\n\tdev->ts = PRIMARY_TS;\n\tsnprintf(dev->name, 28, \"em28xx\");\n\tdev->dev_next = NULL;\n\n\tif (has_vendor_audio) {\n\t\tdev_info(&intf->dev,\n\t\t\t\"Audio interface %i found (Vendor Class)\\n\", ifnum);\n\t\tdev->usb_audio_type = EM28XX_USB_AUDIO_VENDOR;\n\t}\n\t/* Checks if audio is provided by a USB Audio Class intf */\n\tfor (i = 0; i < udev->config->desc.bNumInterfaces; i++) {\n\t\tstruct usb_interface *uif = udev->config->interface[i];\n\n\t\tif (uif->altsetting[0].desc.bInterfaceClass == USB_CLASS_AUDIO) {\n\t\t\tif (has_vendor_audio)\n\t\t\t\tdev_err(&intf->dev,\n\t\t\t\t\t\"em28xx: device seems to have vendor AND usb audio class interfaces !\\n\"\n\t\t\t\t\t\"\\t\\tThe vendor interface will be ignored. Please contact the developers <linux-media@vger.kernel.org>\\n\");\n\t\t\tdev->usb_audio_type = EM28XX_USB_AUDIO_CLASS;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (has_video)\n\t\tdev_info(&intf->dev, \"Video interface %i found:%s%s\\n\",\n\t\t\tifnum,\n\t\t\tdev->analog_ep_bulk ? \" bulk\" : \"\",\n\t\t\tdev->analog_ep_isoc ? \" isoc\" : \"\");\n\tif (has_dvb)\n\t\tdev_info(&intf->dev, \"DVB interface %i found:%s%s\\n\",\n\t\t\tifnum,\n\t\t\tdev->dvb_ep_bulk ? \" bulk\" : \"\",\n\t\t\tdev->dvb_ep_isoc ? \" isoc\" : \"\");\n\n\tdev->num_alt = intf->num_altsetting;\n\n\tif ((unsigned int)card[nr] < em28xx_bcount)\n\t\tdev->model = card[nr];\n\n\t/* save our data pointer in this intf device */\n\tusb_set_intfdata(intf, dev);\n\n\t/* allocate device struct and check if the device is a webcam */\n\tmutex_init(&dev->lock);\n\tretval = em28xx_init_dev(dev, udev, intf, nr);\n\tif (retval)\n\t\tgoto err_free;\n\n\tif (usb_xfer_mode < 0) {\n\t\tif (dev->is_webcam)\n\t\t\ttry_bulk = 1;\n\t\telse\n\t\t\ttry_bulk = 0;\n\t} else {\n\t\ttry_bulk = usb_xfer_mode > 0;\n\t}\n\n\t/* Disable V4L2 if the device doesn't have a decoder or image sensor */\n\tif (has_video &&\n\t    dev->board.decoder == EM28XX_NODECODER &&\n\t    dev->em28xx_sensor == EM28XX_NOSENSOR) {\n\t\tdev_err(&intf->dev,\n\t\t\t\"Currently, V4L2 is not supported on this model\\n\");\n\t\thas_video = false;\n\t\tdev->has_video = false;\n\t}\n\n\tif (dev->board.has_dual_ts &&\n\t    (dev->tuner_type != TUNER_ABSENT || INPUT(0)->type)) {\n\t\t/*\n\t\t * The logic with sets alternate is not ready for dual-tuners\n\t\t * which analog modes.\n\t\t */\n\t\tdev_err(&intf->dev,\n\t\t\t\"We currently don't support analog TV or stream capture on dual tuners.\\n\");\n\t\thas_video = false;\n\t}\n\n\t/* Select USB transfer types to use */\n\tif (has_video) {\n\t\tif (!dev->analog_ep_isoc || (try_bulk && dev->analog_ep_bulk))\n\t\t\tdev->analog_xfer_bulk = 1;\n\t\tdev_info(&intf->dev, \"analog set to %s mode.\\n\",\n\t\t\tdev->analog_xfer_bulk ? \"bulk\" : \"isoc\");\n\t}\n\tif (has_dvb) {\n\t\tif (!dev->dvb_ep_isoc || (try_bulk && dev->dvb_ep_bulk))\n\t\t\tdev->dvb_xfer_bulk = 1;\n\t\tdev_info(&intf->dev, \"dvb set to %s mode.\\n\",\n\t\t\tdev->dvb_xfer_bulk ? \"bulk\" : \"isoc\");\n\t}\n\n\tif (dev->board.has_dual_ts && em28xx_duplicate_dev(dev) == 0) {\n\t\tdev->dev_next->ts = SECONDARY_TS;\n\t\tdev->dev_next->alt   = -1;\n\t\tdev->dev_next->is_audio_only = has_vendor_audio &&\n\t\t\t\t\t\t!(has_video || has_dvb);\n\t\tdev->dev_next->has_video = false;\n\t\tdev->dev_next->ifnum = ifnum;\n\t\tdev->dev_next->model = id->driver_info;\n\n\t\tmutex_init(&dev->dev_next->lock);\n\t\tretval = em28xx_init_dev(dev->dev_next, udev, intf,\n\t\t\t\t\t dev->dev_next->devno);\n\t\tif (retval)\n\t\t\tgoto err_free;\n\n\t\tdev->dev_next->board.ir_codes = NULL; /* No IR for 2nd tuner */\n\t\tdev->dev_next->board.has_ir_i2c = 0; /* No IR for 2nd tuner */\n\n\t\tif (usb_xfer_mode < 0) {\n\t\t\tif (dev->dev_next->is_webcam)\n\t\t\t\ttry_bulk = 1;\n\t\t\telse\n\t\t\t\ttry_bulk = 0;\n\t\t} else {\n\t\t\ttry_bulk = usb_xfer_mode > 0;\n\t\t}\n\n\t\t/* Select USB transfer types to use */\n\t\tif (has_dvb) {\n\t\t\tif (!dev->dvb_ep_isoc_ts2 ||\n\t\t\t    (try_bulk && dev->dvb_ep_bulk_ts2))\n\t\t\t\tdev->dev_next->dvb_xfer_bulk = 1;\n\t\t\tdev_info(&dev->intf->dev, \"dvb ts2 set to %s mode.\\n\",\n\t\t\t\t dev->dev_next->dvb_xfer_bulk ? \"bulk\" : \"isoc\");\n\t\t}\n\n\t\tdev->dev_next->dvb_ep_isoc = dev->dvb_ep_isoc_ts2;\n\t\tdev->dev_next->dvb_ep_bulk = dev->dvb_ep_bulk_ts2;\n\t\tdev->dev_next->dvb_max_pkt_size_isoc = dev->dvb_max_pkt_size_isoc_ts2;\n\t\tdev->dev_next->dvb_alt_isoc = dev->dvb_alt_isoc;\n\n\t\t/* Configure hardware to support TS2*/\n\t\tif (dev->dvb_xfer_bulk) {\n\t\t\t/* The ep4 and ep5 are configured for BULK */\n\t\t\tem28xx_write_reg(dev, 0x0b, 0x96);\n\t\t\tmdelay(100);\n\t\t\tem28xx_write_reg(dev, 0x0b, 0x80);\n\t\t\tmdelay(100);\n\t\t} else {\n\t\t\t/* The ep4 and ep5 are configured for ISO */\n\t\t\tem28xx_write_reg(dev, 0x0b, 0x96);\n\t\t\tmdelay(100);\n\t\t\tem28xx_write_reg(dev, 0x0b, 0x82);\n\t\t\tmdelay(100);\n\t\t}\n\n\t\tkref_init(&dev->dev_next->ref);\n\t}\n\n\tkref_init(&dev->ref);\n\n\trequest_modules(dev);\n\n\t/*\n\t * Do it at the end, to reduce dynamic configuration changes during\n\t * the device init. Yet, as request_modules() can be async, the\n\t * topology will likely change after the load of the em28xx subdrivers.\n\t */\n#ifdef CONFIG_MEDIA_CONTROLLER\n\tretval = media_device_register(dev->media_dev);\n#endif\n\n\treturn 0;\n\nerr_free:\n\tkfree(dev->alt_max_pkt_size_isoc);\n\tkfree(dev);\n\nerr:\n\tclear_bit(nr, em28xx_devused);\n\nerr_no_slot:\n\tusb_put_dev(udev);\n\treturn retval;\n}",
        "code_after_change": "static int em28xx_usb_probe(struct usb_interface *intf,\n\t\t\t    const struct usb_device_id *id)\n{\n\tstruct usb_device *udev;\n\tstruct em28xx *dev = NULL;\n\tint retval;\n\tbool has_vendor_audio = false, has_video = false, has_dvb = false;\n\tint i, nr, try_bulk;\n\tconst int ifnum = intf->altsetting[0].desc.bInterfaceNumber;\n\tchar *speed;\n\n\tudev = usb_get_dev(interface_to_usbdev(intf));\n\n\t/* Check to see next free device and mark as used */\n\tdo {\n\t\tnr = find_first_zero_bit(em28xx_devused, EM28XX_MAXBOARDS);\n\t\tif (nr >= EM28XX_MAXBOARDS) {\n\t\t\t/* No free device slots */\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"Driver supports up to %i em28xx boards.\\n\",\n\t\t\t       EM28XX_MAXBOARDS);\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto err_no_slot;\n\t\t}\n\t} while (test_and_set_bit(nr, em28xx_devused));\n\n\t/* Don't register audio interfaces */\n\tif (intf->altsetting[0].desc.bInterfaceClass == USB_CLASS_AUDIO) {\n\t\tdev_info(&intf->dev,\n\t\t\t\"audio device (%04x:%04x): interface %i, class %i\\n\",\n\t\t\tle16_to_cpu(udev->descriptor.idVendor),\n\t\t\tle16_to_cpu(udev->descriptor.idProduct),\n\t\t\tifnum,\n\t\t\tintf->altsetting[0].desc.bInterfaceClass);\n\n\t\tretval = -ENODEV;\n\t\tgoto err;\n\t}\n\n\t/* allocate memory for our device state and initialize it */\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev) {\n\t\tretval = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\t/* compute alternate max packet sizes */\n\tdev->alt_max_pkt_size_isoc = kcalloc(intf->num_altsetting,\n\t\t\t\t\t     sizeof(dev->alt_max_pkt_size_isoc[0]),\n\t\t\t\t\t     GFP_KERNEL);\n\tif (!dev->alt_max_pkt_size_isoc) {\n\t\tkfree(dev);\n\t\tretval = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\t/* Get endpoints */\n\tfor (i = 0; i < intf->num_altsetting; i++) {\n\t\tint ep;\n\n\t\tfor (ep = 0;\n\t\t     ep < intf->altsetting[i].desc.bNumEndpoints;\n\t\t     ep++)\n\t\t\tem28xx_check_usb_descriptor(dev, udev, intf,\n\t\t\t\t\t\t    i, ep,\n\t\t\t\t\t\t    &has_vendor_audio,\n\t\t\t\t\t\t    &has_video,\n\t\t\t\t\t\t    &has_dvb);\n\t}\n\n\tif (!(has_vendor_audio || has_video || has_dvb)) {\n\t\tretval = -ENODEV;\n\t\tgoto err_free;\n\t}\n\n\tswitch (udev->speed) {\n\tcase USB_SPEED_LOW:\n\t\tspeed = \"1.5\";\n\t\tbreak;\n\tcase USB_SPEED_UNKNOWN:\n\tcase USB_SPEED_FULL:\n\t\tspeed = \"12\";\n\t\tbreak;\n\tcase USB_SPEED_HIGH:\n\t\tspeed = \"480\";\n\t\tbreak;\n\tdefault:\n\t\tspeed = \"unknown\";\n\t}\n\n\tdev_info(&intf->dev,\n\t\t\"New device %s %s @ %s Mbps (%04x:%04x, interface %d, class %d)\\n\",\n\t\tudev->manufacturer ? udev->manufacturer : \"\",\n\t\tudev->product ? udev->product : \"\",\n\t\tspeed,\n\t\tle16_to_cpu(udev->descriptor.idVendor),\n\t\tle16_to_cpu(udev->descriptor.idProduct),\n\t\tifnum,\n\t\tintf->altsetting->desc.bInterfaceNumber);\n\n\t/*\n\t * Make sure we have 480 Mbps of bandwidth, otherwise things like\n\t * video stream wouldn't likely work, since 12 Mbps is generally\n\t * not enough even for most Digital TV streams.\n\t */\n\tif (udev->speed != USB_SPEED_HIGH && disable_usb_speed_check == 0) {\n\t\tdev_err(&intf->dev, \"Device initialization failed.\\n\");\n\t\tdev_err(&intf->dev,\n\t\t\t\"Device must be connected to a high-speed USB 2.0 port.\\n\");\n\t\tretval = -ENODEV;\n\t\tgoto err_free;\n\t}\n\n\tkref_init(&dev->ref);\n\n\tdev->devno = nr;\n\tdev->model = id->driver_info;\n\tdev->alt   = -1;\n\tdev->is_audio_only = has_vendor_audio && !(has_video || has_dvb);\n\tdev->has_video = has_video;\n\tdev->ifnum = ifnum;\n\n\tdev->ts = PRIMARY_TS;\n\tsnprintf(dev->name, 28, \"em28xx\");\n\tdev->dev_next = NULL;\n\n\tif (has_vendor_audio) {\n\t\tdev_info(&intf->dev,\n\t\t\t\"Audio interface %i found (Vendor Class)\\n\", ifnum);\n\t\tdev->usb_audio_type = EM28XX_USB_AUDIO_VENDOR;\n\t}\n\t/* Checks if audio is provided by a USB Audio Class intf */\n\tfor (i = 0; i < udev->config->desc.bNumInterfaces; i++) {\n\t\tstruct usb_interface *uif = udev->config->interface[i];\n\n\t\tif (uif->altsetting[0].desc.bInterfaceClass == USB_CLASS_AUDIO) {\n\t\t\tif (has_vendor_audio)\n\t\t\t\tdev_err(&intf->dev,\n\t\t\t\t\t\"em28xx: device seems to have vendor AND usb audio class interfaces !\\n\"\n\t\t\t\t\t\"\\t\\tThe vendor interface will be ignored. Please contact the developers <linux-media@vger.kernel.org>\\n\");\n\t\t\tdev->usb_audio_type = EM28XX_USB_AUDIO_CLASS;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (has_video)\n\t\tdev_info(&intf->dev, \"Video interface %i found:%s%s\\n\",\n\t\t\tifnum,\n\t\t\tdev->analog_ep_bulk ? \" bulk\" : \"\",\n\t\t\tdev->analog_ep_isoc ? \" isoc\" : \"\");\n\tif (has_dvb)\n\t\tdev_info(&intf->dev, \"DVB interface %i found:%s%s\\n\",\n\t\t\tifnum,\n\t\t\tdev->dvb_ep_bulk ? \" bulk\" : \"\",\n\t\t\tdev->dvb_ep_isoc ? \" isoc\" : \"\");\n\n\tdev->num_alt = intf->num_altsetting;\n\n\tif ((unsigned int)card[nr] < em28xx_bcount)\n\t\tdev->model = card[nr];\n\n\t/* save our data pointer in this intf device */\n\tusb_set_intfdata(intf, dev);\n\n\t/* allocate device struct and check if the device is a webcam */\n\tmutex_init(&dev->lock);\n\tretval = em28xx_init_dev(dev, udev, intf, nr);\n\tif (retval)\n\t\tgoto err_free;\n\n\tif (usb_xfer_mode < 0) {\n\t\tif (dev->is_webcam)\n\t\t\ttry_bulk = 1;\n\t\telse\n\t\t\ttry_bulk = 0;\n\t} else {\n\t\ttry_bulk = usb_xfer_mode > 0;\n\t}\n\n\t/* Disable V4L2 if the device doesn't have a decoder or image sensor */\n\tif (has_video &&\n\t    dev->board.decoder == EM28XX_NODECODER &&\n\t    dev->em28xx_sensor == EM28XX_NOSENSOR) {\n\t\tdev_err(&intf->dev,\n\t\t\t\"Currently, V4L2 is not supported on this model\\n\");\n\t\thas_video = false;\n\t\tdev->has_video = false;\n\t}\n\n\tif (dev->board.has_dual_ts &&\n\t    (dev->tuner_type != TUNER_ABSENT || INPUT(0)->type)) {\n\t\t/*\n\t\t * The logic with sets alternate is not ready for dual-tuners\n\t\t * which analog modes.\n\t\t */\n\t\tdev_err(&intf->dev,\n\t\t\t\"We currently don't support analog TV or stream capture on dual tuners.\\n\");\n\t\thas_video = false;\n\t}\n\n\t/* Select USB transfer types to use */\n\tif (has_video) {\n\t\tif (!dev->analog_ep_isoc || (try_bulk && dev->analog_ep_bulk))\n\t\t\tdev->analog_xfer_bulk = 1;\n\t\tdev_info(&intf->dev, \"analog set to %s mode.\\n\",\n\t\t\tdev->analog_xfer_bulk ? \"bulk\" : \"isoc\");\n\t}\n\tif (has_dvb) {\n\t\tif (!dev->dvb_ep_isoc || (try_bulk && dev->dvb_ep_bulk))\n\t\t\tdev->dvb_xfer_bulk = 1;\n\t\tdev_info(&intf->dev, \"dvb set to %s mode.\\n\",\n\t\t\tdev->dvb_xfer_bulk ? \"bulk\" : \"isoc\");\n\t}\n\n\tif (dev->board.has_dual_ts && em28xx_duplicate_dev(dev) == 0) {\n\t\tkref_init(&dev->dev_next->ref);\n\n\t\tdev->dev_next->ts = SECONDARY_TS;\n\t\tdev->dev_next->alt   = -1;\n\t\tdev->dev_next->is_audio_only = has_vendor_audio &&\n\t\t\t\t\t\t!(has_video || has_dvb);\n\t\tdev->dev_next->has_video = false;\n\t\tdev->dev_next->ifnum = ifnum;\n\t\tdev->dev_next->model = id->driver_info;\n\n\t\tmutex_init(&dev->dev_next->lock);\n\t\tretval = em28xx_init_dev(dev->dev_next, udev, intf,\n\t\t\t\t\t dev->dev_next->devno);\n\t\tif (retval)\n\t\t\tgoto err_free;\n\n\t\tdev->dev_next->board.ir_codes = NULL; /* No IR for 2nd tuner */\n\t\tdev->dev_next->board.has_ir_i2c = 0; /* No IR for 2nd tuner */\n\n\t\tif (usb_xfer_mode < 0) {\n\t\t\tif (dev->dev_next->is_webcam)\n\t\t\t\ttry_bulk = 1;\n\t\t\telse\n\t\t\t\ttry_bulk = 0;\n\t\t} else {\n\t\t\ttry_bulk = usb_xfer_mode > 0;\n\t\t}\n\n\t\t/* Select USB transfer types to use */\n\t\tif (has_dvb) {\n\t\t\tif (!dev->dvb_ep_isoc_ts2 ||\n\t\t\t    (try_bulk && dev->dvb_ep_bulk_ts2))\n\t\t\t\tdev->dev_next->dvb_xfer_bulk = 1;\n\t\t\tdev_info(&dev->intf->dev, \"dvb ts2 set to %s mode.\\n\",\n\t\t\t\t dev->dev_next->dvb_xfer_bulk ? \"bulk\" : \"isoc\");\n\t\t}\n\n\t\tdev->dev_next->dvb_ep_isoc = dev->dvb_ep_isoc_ts2;\n\t\tdev->dev_next->dvb_ep_bulk = dev->dvb_ep_bulk_ts2;\n\t\tdev->dev_next->dvb_max_pkt_size_isoc = dev->dvb_max_pkt_size_isoc_ts2;\n\t\tdev->dev_next->dvb_alt_isoc = dev->dvb_alt_isoc;\n\n\t\t/* Configure hardware to support TS2*/\n\t\tif (dev->dvb_xfer_bulk) {\n\t\t\t/* The ep4 and ep5 are configured for BULK */\n\t\t\tem28xx_write_reg(dev, 0x0b, 0x96);\n\t\t\tmdelay(100);\n\t\t\tem28xx_write_reg(dev, 0x0b, 0x80);\n\t\t\tmdelay(100);\n\t\t} else {\n\t\t\t/* The ep4 and ep5 are configured for ISO */\n\t\t\tem28xx_write_reg(dev, 0x0b, 0x96);\n\t\t\tmdelay(100);\n\t\t\tem28xx_write_reg(dev, 0x0b, 0x82);\n\t\t\tmdelay(100);\n\t\t}\n\t}\n\n\trequest_modules(dev);\n\n\t/*\n\t * Do it at the end, to reduce dynamic configuration changes during\n\t * the device init. Yet, as request_modules() can be async, the\n\t * topology will likely change after the load of the em28xx subdrivers.\n\t */\n#ifdef CONFIG_MEDIA_CONTROLLER\n\tretval = media_device_register(dev->media_dev);\n#endif\n\n\treturn 0;\n\nerr_free:\n\tkfree(dev->alt_max_pkt_size_isoc);\n\tkfree(dev);\n\nerr:\n\tclear_bit(nr, em28xx_devused);\n\nerr_no_slot:\n\tusb_put_dev(udev);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -111,6 +111,8 @@\n \t\tgoto err_free;\n \t}\n \n+\tkref_init(&dev->ref);\n+\n \tdev->devno = nr;\n \tdev->model = id->driver_info;\n \tdev->alt   = -1;\n@@ -211,6 +213,8 @@\n \t}\n \n \tif (dev->board.has_dual_ts && em28xx_duplicate_dev(dev) == 0) {\n+\t\tkref_init(&dev->dev_next->ref);\n+\n \t\tdev->dev_next->ts = SECONDARY_TS;\n \t\tdev->dev_next->alt   = -1;\n \t\tdev->dev_next->is_audio_only = has_vendor_audio &&\n@@ -265,11 +269,7 @@\n \t\t\tem28xx_write_reg(dev, 0x0b, 0x82);\n \t\t\tmdelay(100);\n \t\t}\n-\n-\t\tkref_init(&dev->dev_next->ref);\n-\t}\n-\n-\tkref_init(&dev->ref);\n+\t}\n \n \trequest_modules(dev);\n ",
        "function_modified_lines": {
            "added": [
                "\tkref_init(&dev->ref);",
                "",
                "\t\tkref_init(&dev->dev_next->ref);",
                "",
                "\t}"
            ],
            "deleted": [
                "",
                "\t\tkref_init(&dev->dev_next->ref);",
                "\t}",
                "",
                "\tkref_init(&dev->ref);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw use after free in the Linux kernel video4linux driver was found in the way user triggers em28xx_usb_probe() for the Empia 28xx based TV cards. A local user could use this flaw to crash the system or potentially escalate their privileges on the system.",
        "id": 3570
    },
    {
        "cve_id": "CVE-2014-2568",
        "code_before_change": "static struct sk_buff *\nnfqnl_build_packet_message(struct net *net, struct nfqnl_instance *queue,\n\t\t\t   struct nf_queue_entry *entry,\n\t\t\t   __be32 **packet_id_ptr)\n{\n\tsize_t size;\n\tsize_t data_len = 0, cap_len = 0;\n\tunsigned int hlen = 0;\n\tstruct sk_buff *skb;\n\tstruct nlattr *nla;\n\tstruct nfqnl_msg_packet_hdr *pmsg;\n\tstruct nlmsghdr *nlh;\n\tstruct nfgenmsg *nfmsg;\n\tstruct sk_buff *entskb = entry->skb;\n\tstruct net_device *indev;\n\tstruct net_device *outdev;\n\tstruct nf_conn *ct = NULL;\n\tenum ip_conntrack_info uninitialized_var(ctinfo);\n\tbool csum_verify;\n\n\tsize =    nlmsg_total_size(sizeof(struct nfgenmsg))\n\t\t+ nla_total_size(sizeof(struct nfqnl_msg_packet_hdr))\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* ifindex */\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* ifindex */\n#ifdef CONFIG_BRIDGE_NETFILTER\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* ifindex */\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* ifindex */\n#endif\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* mark */\n\t\t+ nla_total_size(sizeof(struct nfqnl_msg_packet_hw))\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* skbinfo */\n\t\t+ nla_total_size(sizeof(u_int32_t));\t/* cap_len */\n\n\tif (entskb->tstamp.tv64)\n\t\tsize += nla_total_size(sizeof(struct nfqnl_msg_packet_timestamp));\n\n\tif (entry->hook <= NF_INET_FORWARD ||\n\t   (entry->hook == NF_INET_POST_ROUTING && entskb->sk == NULL))\n\t\tcsum_verify = !skb_csum_unnecessary(entskb);\n\telse\n\t\tcsum_verify = false;\n\n\toutdev = entry->outdev;\n\n\tswitch ((enum nfqnl_config_mode)ACCESS_ONCE(queue->copy_mode)) {\n\tcase NFQNL_COPY_META:\n\tcase NFQNL_COPY_NONE:\n\t\tbreak;\n\n\tcase NFQNL_COPY_PACKET:\n\t\tif (!(queue->flags & NFQA_CFG_F_GSO) &&\n\t\t    entskb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t    skb_checksum_help(entskb))\n\t\t\treturn NULL;\n\n\t\tdata_len = ACCESS_ONCE(queue->copy_range);\n\t\tif (data_len > entskb->len)\n\t\t\tdata_len = entskb->len;\n\n\t\thlen = skb_zerocopy_headlen(entskb);\n\t\thlen = min_t(unsigned int, hlen, data_len);\n\t\tsize += sizeof(struct nlattr) + hlen;\n\t\tcap_len = entskb->len;\n\t\tbreak;\n\t}\n\n\tif (queue->flags & NFQA_CFG_F_CONNTRACK)\n\t\tct = nfqnl_ct_get(entskb, &size, &ctinfo);\n\n\tif (queue->flags & NFQA_CFG_F_UID_GID) {\n\t\tsize +=  (nla_total_size(sizeof(u_int32_t))\t/* uid */\n\t\t\t+ nla_total_size(sizeof(u_int32_t)));\t/* gid */\n\t}\n\n\tskb = nfnetlink_alloc_skb(net, size, queue->peer_portid,\n\t\t\t\t  GFP_ATOMIC);\n\tif (!skb)\n\t\treturn NULL;\n\n\tnlh = nlmsg_put(skb, 0, 0,\n\t\t\tNFNL_SUBSYS_QUEUE << 8 | NFQNL_MSG_PACKET,\n\t\t\tsizeof(struct nfgenmsg), 0);\n\tif (!nlh) {\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\tnfmsg = nlmsg_data(nlh);\n\tnfmsg->nfgen_family = entry->pf;\n\tnfmsg->version = NFNETLINK_V0;\n\tnfmsg->res_id = htons(queue->queue_num);\n\n\tnla = __nla_reserve(skb, NFQA_PACKET_HDR, sizeof(*pmsg));\n\tpmsg = nla_data(nla);\n\tpmsg->hw_protocol\t= entskb->protocol;\n\tpmsg->hook\t\t= entry->hook;\n\t*packet_id_ptr\t\t= &pmsg->packet_id;\n\n\tindev = entry->indev;\n\tif (indev) {\n#ifndef CONFIG_BRIDGE_NETFILTER\n\t\tif (nla_put_be32(skb, NFQA_IFINDEX_INDEV, htonl(indev->ifindex)))\n\t\t\tgoto nla_put_failure;\n#else\n\t\tif (entry->pf == PF_BRIDGE) {\n\t\t\t/* Case 1: indev is physical input device, we need to\n\t\t\t * look for bridge group (when called from\n\t\t\t * netfilter_bridge) */\n\t\t\tif (nla_put_be32(skb, NFQA_IFINDEX_PHYSINDEV,\n\t\t\t\t\t htonl(indev->ifindex)) ||\n\t\t\t/* this is the bridge group \"brX\" */\n\t\t\t/* rcu_read_lock()ed by __nf_queue */\n\t\t\t    nla_put_be32(skb, NFQA_IFINDEX_INDEV,\n\t\t\t\t\t htonl(br_port_get_rcu(indev)->br->dev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t} else {\n\t\t\t/* Case 2: indev is bridge group, we need to look for\n\t\t\t * physical device (when called from ipv4) */\n\t\t\tif (nla_put_be32(skb, NFQA_IFINDEX_INDEV,\n\t\t\t\t\t htonl(indev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t\tif (entskb->nf_bridge && entskb->nf_bridge->physindev &&\n\t\t\t    nla_put_be32(skb, NFQA_IFINDEX_PHYSINDEV,\n\t\t\t\t\t htonl(entskb->nf_bridge->physindev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n#endif\n\t}\n\n\tif (outdev) {\n#ifndef CONFIG_BRIDGE_NETFILTER\n\t\tif (nla_put_be32(skb, NFQA_IFINDEX_OUTDEV, htonl(outdev->ifindex)))\n\t\t\tgoto nla_put_failure;\n#else\n\t\tif (entry->pf == PF_BRIDGE) {\n\t\t\t/* Case 1: outdev is physical output device, we need to\n\t\t\t * look for bridge group (when called from\n\t\t\t * netfilter_bridge) */\n\t\t\tif (nla_put_be32(skb, NFQA_IFINDEX_PHYSOUTDEV,\n\t\t\t\t\t htonl(outdev->ifindex)) ||\n\t\t\t/* this is the bridge group \"brX\" */\n\t\t\t/* rcu_read_lock()ed by __nf_queue */\n\t\t\t    nla_put_be32(skb, NFQA_IFINDEX_OUTDEV,\n\t\t\t\t\t htonl(br_port_get_rcu(outdev)->br->dev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t} else {\n\t\t\t/* Case 2: outdev is bridge group, we need to look for\n\t\t\t * physical output device (when called from ipv4) */\n\t\t\tif (nla_put_be32(skb, NFQA_IFINDEX_OUTDEV,\n\t\t\t\t\t htonl(outdev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t\tif (entskb->nf_bridge && entskb->nf_bridge->physoutdev &&\n\t\t\t    nla_put_be32(skb, NFQA_IFINDEX_PHYSOUTDEV,\n\t\t\t\t\t htonl(entskb->nf_bridge->physoutdev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n#endif\n\t}\n\n\tif (entskb->mark &&\n\t    nla_put_be32(skb, NFQA_MARK, htonl(entskb->mark)))\n\t\tgoto nla_put_failure;\n\n\tif (indev && entskb->dev &&\n\t    entskb->mac_header != entskb->network_header) {\n\t\tstruct nfqnl_msg_packet_hw phw;\n\t\tint len;\n\n\t\tmemset(&phw, 0, sizeof(phw));\n\t\tlen = dev_parse_header(entskb, phw.hw_addr);\n\t\tif (len) {\n\t\t\tphw.hw_addrlen = htons(len);\n\t\t\tif (nla_put(skb, NFQA_HWADDR, sizeof(phw), &phw))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n\t}\n\n\tif (entskb->tstamp.tv64) {\n\t\tstruct nfqnl_msg_packet_timestamp ts;\n\t\tstruct timeval tv = ktime_to_timeval(entskb->tstamp);\n\t\tts.sec = cpu_to_be64(tv.tv_sec);\n\t\tts.usec = cpu_to_be64(tv.tv_usec);\n\n\t\tif (nla_put(skb, NFQA_TIMESTAMP, sizeof(ts), &ts))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tif ((queue->flags & NFQA_CFG_F_UID_GID) && entskb->sk &&\n\t    nfqnl_put_sk_uidgid(skb, entskb->sk) < 0)\n\t\tgoto nla_put_failure;\n\n\tif (ct && nfqnl_ct_put(skb, ct, ctinfo) < 0)\n\t\tgoto nla_put_failure;\n\n\tif (cap_len > data_len &&\n\t    nla_put_be32(skb, NFQA_CAP_LEN, htonl(cap_len)))\n\t\tgoto nla_put_failure;\n\n\tif (nfqnl_put_packet_info(skb, entskb, csum_verify))\n\t\tgoto nla_put_failure;\n\n\tif (data_len) {\n\t\tstruct nlattr *nla;\n\n\t\tif (skb_tailroom(skb) < sizeof(*nla) + hlen)\n\t\t\tgoto nla_put_failure;\n\n\t\tnla = (struct nlattr *)skb_put(skb, sizeof(*nla));\n\t\tnla->nla_type = NFQA_PAYLOAD;\n\t\tnla->nla_len = nla_attr_size(data_len);\n\n\t\tskb_zerocopy(skb, entskb, data_len, hlen);\n\t}\n\n\tnlh->nlmsg_len = skb->len;\n\treturn skb;\n\nnla_put_failure:\n\tkfree_skb(skb);\n\tnet_err_ratelimited(\"nf_queue: error creating packet message\\n\");\n\treturn NULL;\n}",
        "code_after_change": "static struct sk_buff *\nnfqnl_build_packet_message(struct net *net, struct nfqnl_instance *queue,\n\t\t\t   struct nf_queue_entry *entry,\n\t\t\t   __be32 **packet_id_ptr)\n{\n\tsize_t size;\n\tsize_t data_len = 0, cap_len = 0;\n\tunsigned int hlen = 0;\n\tstruct sk_buff *skb;\n\tstruct nlattr *nla;\n\tstruct nfqnl_msg_packet_hdr *pmsg;\n\tstruct nlmsghdr *nlh;\n\tstruct nfgenmsg *nfmsg;\n\tstruct sk_buff *entskb = entry->skb;\n\tstruct net_device *indev;\n\tstruct net_device *outdev;\n\tstruct nf_conn *ct = NULL;\n\tenum ip_conntrack_info uninitialized_var(ctinfo);\n\tbool csum_verify;\n\n\tsize =    nlmsg_total_size(sizeof(struct nfgenmsg))\n\t\t+ nla_total_size(sizeof(struct nfqnl_msg_packet_hdr))\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* ifindex */\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* ifindex */\n#ifdef CONFIG_BRIDGE_NETFILTER\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* ifindex */\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* ifindex */\n#endif\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* mark */\n\t\t+ nla_total_size(sizeof(struct nfqnl_msg_packet_hw))\n\t\t+ nla_total_size(sizeof(u_int32_t))\t/* skbinfo */\n\t\t+ nla_total_size(sizeof(u_int32_t));\t/* cap_len */\n\n\tif (entskb->tstamp.tv64)\n\t\tsize += nla_total_size(sizeof(struct nfqnl_msg_packet_timestamp));\n\n\tif (entry->hook <= NF_INET_FORWARD ||\n\t   (entry->hook == NF_INET_POST_ROUTING && entskb->sk == NULL))\n\t\tcsum_verify = !skb_csum_unnecessary(entskb);\n\telse\n\t\tcsum_verify = false;\n\n\toutdev = entry->outdev;\n\n\tswitch ((enum nfqnl_config_mode)ACCESS_ONCE(queue->copy_mode)) {\n\tcase NFQNL_COPY_META:\n\tcase NFQNL_COPY_NONE:\n\t\tbreak;\n\n\tcase NFQNL_COPY_PACKET:\n\t\tif (!(queue->flags & NFQA_CFG_F_GSO) &&\n\t\t    entskb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t    skb_checksum_help(entskb))\n\t\t\treturn NULL;\n\n\t\tdata_len = ACCESS_ONCE(queue->copy_range);\n\t\tif (data_len > entskb->len)\n\t\t\tdata_len = entskb->len;\n\n\t\thlen = skb_zerocopy_headlen(entskb);\n\t\thlen = min_t(unsigned int, hlen, data_len);\n\t\tsize += sizeof(struct nlattr) + hlen;\n\t\tcap_len = entskb->len;\n\t\tbreak;\n\t}\n\n\tif (queue->flags & NFQA_CFG_F_CONNTRACK)\n\t\tct = nfqnl_ct_get(entskb, &size, &ctinfo);\n\n\tif (queue->flags & NFQA_CFG_F_UID_GID) {\n\t\tsize +=  (nla_total_size(sizeof(u_int32_t))\t/* uid */\n\t\t\t+ nla_total_size(sizeof(u_int32_t)));\t/* gid */\n\t}\n\n\tskb = nfnetlink_alloc_skb(net, size, queue->peer_portid,\n\t\t\t\t  GFP_ATOMIC);\n\tif (!skb) {\n\t\tskb_tx_error(entskb);\n\t\treturn NULL;\n\t}\n\n\tnlh = nlmsg_put(skb, 0, 0,\n\t\t\tNFNL_SUBSYS_QUEUE << 8 | NFQNL_MSG_PACKET,\n\t\t\tsizeof(struct nfgenmsg), 0);\n\tif (!nlh) {\n\t\tskb_tx_error(entskb);\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\tnfmsg = nlmsg_data(nlh);\n\tnfmsg->nfgen_family = entry->pf;\n\tnfmsg->version = NFNETLINK_V0;\n\tnfmsg->res_id = htons(queue->queue_num);\n\n\tnla = __nla_reserve(skb, NFQA_PACKET_HDR, sizeof(*pmsg));\n\tpmsg = nla_data(nla);\n\tpmsg->hw_protocol\t= entskb->protocol;\n\tpmsg->hook\t\t= entry->hook;\n\t*packet_id_ptr\t\t= &pmsg->packet_id;\n\n\tindev = entry->indev;\n\tif (indev) {\n#ifndef CONFIG_BRIDGE_NETFILTER\n\t\tif (nla_put_be32(skb, NFQA_IFINDEX_INDEV, htonl(indev->ifindex)))\n\t\t\tgoto nla_put_failure;\n#else\n\t\tif (entry->pf == PF_BRIDGE) {\n\t\t\t/* Case 1: indev is physical input device, we need to\n\t\t\t * look for bridge group (when called from\n\t\t\t * netfilter_bridge) */\n\t\t\tif (nla_put_be32(skb, NFQA_IFINDEX_PHYSINDEV,\n\t\t\t\t\t htonl(indev->ifindex)) ||\n\t\t\t/* this is the bridge group \"brX\" */\n\t\t\t/* rcu_read_lock()ed by __nf_queue */\n\t\t\t    nla_put_be32(skb, NFQA_IFINDEX_INDEV,\n\t\t\t\t\t htonl(br_port_get_rcu(indev)->br->dev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t} else {\n\t\t\t/* Case 2: indev is bridge group, we need to look for\n\t\t\t * physical device (when called from ipv4) */\n\t\t\tif (nla_put_be32(skb, NFQA_IFINDEX_INDEV,\n\t\t\t\t\t htonl(indev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t\tif (entskb->nf_bridge && entskb->nf_bridge->physindev &&\n\t\t\t    nla_put_be32(skb, NFQA_IFINDEX_PHYSINDEV,\n\t\t\t\t\t htonl(entskb->nf_bridge->physindev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n#endif\n\t}\n\n\tif (outdev) {\n#ifndef CONFIG_BRIDGE_NETFILTER\n\t\tif (nla_put_be32(skb, NFQA_IFINDEX_OUTDEV, htonl(outdev->ifindex)))\n\t\t\tgoto nla_put_failure;\n#else\n\t\tif (entry->pf == PF_BRIDGE) {\n\t\t\t/* Case 1: outdev is physical output device, we need to\n\t\t\t * look for bridge group (when called from\n\t\t\t * netfilter_bridge) */\n\t\t\tif (nla_put_be32(skb, NFQA_IFINDEX_PHYSOUTDEV,\n\t\t\t\t\t htonl(outdev->ifindex)) ||\n\t\t\t/* this is the bridge group \"brX\" */\n\t\t\t/* rcu_read_lock()ed by __nf_queue */\n\t\t\t    nla_put_be32(skb, NFQA_IFINDEX_OUTDEV,\n\t\t\t\t\t htonl(br_port_get_rcu(outdev)->br->dev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t} else {\n\t\t\t/* Case 2: outdev is bridge group, we need to look for\n\t\t\t * physical output device (when called from ipv4) */\n\t\t\tif (nla_put_be32(skb, NFQA_IFINDEX_OUTDEV,\n\t\t\t\t\t htonl(outdev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t\tif (entskb->nf_bridge && entskb->nf_bridge->physoutdev &&\n\t\t\t    nla_put_be32(skb, NFQA_IFINDEX_PHYSOUTDEV,\n\t\t\t\t\t htonl(entskb->nf_bridge->physoutdev->ifindex)))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n#endif\n\t}\n\n\tif (entskb->mark &&\n\t    nla_put_be32(skb, NFQA_MARK, htonl(entskb->mark)))\n\t\tgoto nla_put_failure;\n\n\tif (indev && entskb->dev &&\n\t    entskb->mac_header != entskb->network_header) {\n\t\tstruct nfqnl_msg_packet_hw phw;\n\t\tint len;\n\n\t\tmemset(&phw, 0, sizeof(phw));\n\t\tlen = dev_parse_header(entskb, phw.hw_addr);\n\t\tif (len) {\n\t\t\tphw.hw_addrlen = htons(len);\n\t\t\tif (nla_put(skb, NFQA_HWADDR, sizeof(phw), &phw))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n\t}\n\n\tif (entskb->tstamp.tv64) {\n\t\tstruct nfqnl_msg_packet_timestamp ts;\n\t\tstruct timeval tv = ktime_to_timeval(entskb->tstamp);\n\t\tts.sec = cpu_to_be64(tv.tv_sec);\n\t\tts.usec = cpu_to_be64(tv.tv_usec);\n\n\t\tif (nla_put(skb, NFQA_TIMESTAMP, sizeof(ts), &ts))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tif ((queue->flags & NFQA_CFG_F_UID_GID) && entskb->sk &&\n\t    nfqnl_put_sk_uidgid(skb, entskb->sk) < 0)\n\t\tgoto nla_put_failure;\n\n\tif (ct && nfqnl_ct_put(skb, ct, ctinfo) < 0)\n\t\tgoto nla_put_failure;\n\n\tif (cap_len > data_len &&\n\t    nla_put_be32(skb, NFQA_CAP_LEN, htonl(cap_len)))\n\t\tgoto nla_put_failure;\n\n\tif (nfqnl_put_packet_info(skb, entskb, csum_verify))\n\t\tgoto nla_put_failure;\n\n\tif (data_len) {\n\t\tstruct nlattr *nla;\n\n\t\tif (skb_tailroom(skb) < sizeof(*nla) + hlen)\n\t\t\tgoto nla_put_failure;\n\n\t\tnla = (struct nlattr *)skb_put(skb, sizeof(*nla));\n\t\tnla->nla_type = NFQA_PAYLOAD;\n\t\tnla->nla_len = nla_attr_size(data_len);\n\n\t\tif (skb_zerocopy(skb, entskb, data_len, hlen))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tnlh->nlmsg_len = skb->len;\n\treturn skb;\n\nnla_put_failure:\n\tskb_tx_error(entskb);\n\tkfree_skb(skb);\n\tnet_err_ratelimited(\"nf_queue: error creating packet message\\n\");\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -74,13 +74,16 @@\n \n \tskb = nfnetlink_alloc_skb(net, size, queue->peer_portid,\n \t\t\t\t  GFP_ATOMIC);\n-\tif (!skb)\n+\tif (!skb) {\n+\t\tskb_tx_error(entskb);\n \t\treturn NULL;\n+\t}\n \n \tnlh = nlmsg_put(skb, 0, 0,\n \t\t\tNFNL_SUBSYS_QUEUE << 8 | NFQNL_MSG_PACKET,\n \t\t\tsizeof(struct nfgenmsg), 0);\n \tif (!nlh) {\n+\t\tskb_tx_error(entskb);\n \t\tkfree_skb(skb);\n \t\treturn NULL;\n \t}\n@@ -208,13 +211,15 @@\n \t\tnla->nla_type = NFQA_PAYLOAD;\n \t\tnla->nla_len = nla_attr_size(data_len);\n \n-\t\tskb_zerocopy(skb, entskb, data_len, hlen);\n+\t\tif (skb_zerocopy(skb, entskb, data_len, hlen))\n+\t\t\tgoto nla_put_failure;\n \t}\n \n \tnlh->nlmsg_len = skb->len;\n \treturn skb;\n \n nla_put_failure:\n+\tskb_tx_error(entskb);\n \tkfree_skb(skb);\n \tnet_err_ratelimited(\"nf_queue: error creating packet message\\n\");\n \treturn NULL;",
        "function_modified_lines": {
            "added": [
                "\tif (!skb) {",
                "\t\tskb_tx_error(entskb);",
                "\t}",
                "\t\tskb_tx_error(entskb);",
                "\t\tif (skb_zerocopy(skb, entskb, data_len, hlen))",
                "\t\t\tgoto nla_put_failure;",
                "\tskb_tx_error(entskb);"
            ],
            "deleted": [
                "\tif (!skb)",
                "\t\tskb_zerocopy(skb, entskb, data_len, hlen);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in the nfqnl_zcopy function in net/netfilter/nfnetlink_queue_core.c in the Linux kernel through 3.13.6 allows attackers to obtain sensitive information from kernel memory by leveraging the absence of a certain orphaning operation. NOTE: the affected code was moved to the skb_zerocopy function in net/core/skbuff.c before the vulnerability was announced.",
        "id": 484
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static int blk_trace_remove_queue(struct request_queue *q)\n{\n\tstruct blk_trace *bt;\n\n\tbt = xchg(&q->blk_trace, NULL);\n\tif (bt == NULL)\n\t\treturn -EINVAL;\n\n\tput_probe_ref();\n\tblk_trace_free(bt);\n\treturn 0;\n}",
        "code_after_change": "static int blk_trace_remove_queue(struct request_queue *q)\n{\n\tstruct blk_trace *bt;\n\n\tbt = xchg(&q->blk_trace, NULL);\n\tif (bt == NULL)\n\t\treturn -EINVAL;\n\n\tput_probe_ref();\n\tsynchronize_rcu();\n\tblk_trace_free(bt);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \t\treturn -EINVAL;\n \n \tput_probe_ref();\n+\tsynchronize_rcu();\n \tblk_trace_free(bt);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tsynchronize_rcu();"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2227
    },
    {
        "cve_id": "CVE-2023-1192",
        "code_before_change": "static struct inode *ntfs_read_mft(struct inode *inode,\n\t\t\t\t   const struct cpu_str *name,\n\t\t\t\t   const struct MFT_REF *ref)\n{\n\tint err = 0;\n\tstruct ntfs_inode *ni = ntfs_i(inode);\n\tstruct super_block *sb = inode->i_sb;\n\tstruct ntfs_sb_info *sbi = sb->s_fs_info;\n\tmode_t mode = 0;\n\tstruct ATTR_STD_INFO5 *std5 = NULL;\n\tstruct ATTR_LIST_ENTRY *le;\n\tstruct ATTRIB *attr;\n\tbool is_match = false;\n\tbool is_root = false;\n\tbool is_dir;\n\tunsigned long ino = inode->i_ino;\n\tu32 rp_fa = 0, asize, t32;\n\tu16 roff, rsize, names = 0;\n\tconst struct ATTR_FILE_NAME *fname = NULL;\n\tconst struct INDEX_ROOT *root;\n\tstruct REPARSE_DATA_BUFFER rp; // 0x18 bytes\n\tu64 t64;\n\tstruct MFT_REC *rec;\n\tstruct runs_tree *run;\n\n\tinode->i_op = NULL;\n\t/* Setup 'uid' and 'gid' */\n\tinode->i_uid = sbi->options->fs_uid;\n\tinode->i_gid = sbi->options->fs_gid;\n\n\terr = mi_init(&ni->mi, sbi, ino);\n\tif (err)\n\t\tgoto out;\n\n\tif (!sbi->mft.ni && ino == MFT_REC_MFT && !sb->s_root) {\n\t\tt64 = sbi->mft.lbo >> sbi->cluster_bits;\n\t\tt32 = bytes_to_cluster(sbi, MFT_REC_VOL * sbi->record_size);\n\t\tsbi->mft.ni = ni;\n\t\tinit_rwsem(&ni->file.run_lock);\n\n\t\tif (!run_add_entry(&ni->file.run, 0, t64, t32, true)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\terr = mi_read(&ni->mi, ino == MFT_REC_MFT);\n\n\tif (err)\n\t\tgoto out;\n\n\trec = ni->mi.mrec;\n\n\tif (sbi->flags & NTFS_FLAGS_LOG_REPLAYING) {\n\t\t;\n\t} else if (ref->seq != rec->seq) {\n\t\terr = -EINVAL;\n\t\tntfs_err(sb, \"MFT: r=%lx, expect seq=%x instead of %x!\", ino,\n\t\t\t le16_to_cpu(ref->seq), le16_to_cpu(rec->seq));\n\t\tgoto out;\n\t} else if (!is_rec_inuse(rec)) {\n\t\terr = -ESTALE;\n\t\tntfs_err(sb, \"Inode r=%x is not in use!\", (u32)ino);\n\t\tgoto out;\n\t}\n\n\tif (le32_to_cpu(rec->total) != sbi->record_size) {\n\t\t/* Bad inode? */\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!is_rec_base(rec)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Record should contain $I30 root. */\n\tis_dir = rec->flags & RECORD_FLAG_DIR;\n\n\tinode->i_generation = le16_to_cpu(rec->seq);\n\n\t/* Enumerate all struct Attributes MFT. */\n\tle = NULL;\n\tattr = NULL;\n\n\t/*\n\t * To reduce tab pressure use goto instead of\n\t * while( (attr = ni_enum_attr_ex(ni, attr, &le, NULL) ))\n\t */\nnext_attr:\n\trun = NULL;\n\terr = -EINVAL;\n\tattr = ni_enum_attr_ex(ni, attr, &le, NULL);\n\tif (!attr)\n\t\tgoto end_enum;\n\n\tif (le && le->vcn) {\n\t\t/* This is non primary attribute segment. Ignore if not MFT. */\n\t\tif (ino != MFT_REC_MFT || attr->type != ATTR_DATA)\n\t\t\tgoto next_attr;\n\n\t\trun = &ni->file.run;\n\t\tasize = le32_to_cpu(attr->size);\n\t\tgoto attr_unpack_run;\n\t}\n\n\troff = attr->non_res ? 0 : le16_to_cpu(attr->res.data_off);\n\trsize = attr->non_res ? 0 : le32_to_cpu(attr->res.data_size);\n\tasize = le32_to_cpu(attr->size);\n\n\tif (le16_to_cpu(attr->name_off) + attr->name_len > asize)\n\t\tgoto out;\n\n\tif (attr->non_res) {\n\t\tt64 = le64_to_cpu(attr->nres.alloc_size);\n\t\tif (le64_to_cpu(attr->nres.data_size) > t64 ||\n\t\t    le64_to_cpu(attr->nres.valid_size) > t64)\n\t\t\tgoto out;\n\t}\n\n\tswitch (attr->type) {\n\tcase ATTR_STD:\n\t\tif (attr->non_res ||\n\t\t    asize < sizeof(struct ATTR_STD_INFO) + roff ||\n\t\t    rsize < sizeof(struct ATTR_STD_INFO))\n\t\t\tgoto out;\n\n\t\tif (std5)\n\t\t\tgoto next_attr;\n\n\t\tstd5 = Add2Ptr(attr, roff);\n\n#ifdef STATX_BTIME\n\t\tnt2kernel(std5->cr_time, &ni->i_crtime);\n#endif\n\t\tnt2kernel(std5->a_time, &inode->i_atime);\n\t\tnt2kernel(std5->c_time, &inode->i_ctime);\n\t\tnt2kernel(std5->m_time, &inode->i_mtime);\n\n\t\tni->std_fa = std5->fa;\n\n\t\tif (asize >= sizeof(struct ATTR_STD_INFO5) + roff &&\n\t\t    rsize >= sizeof(struct ATTR_STD_INFO5))\n\t\t\tni->std_security_id = std5->security_id;\n\t\tgoto next_attr;\n\n\tcase ATTR_LIST:\n\t\tif (attr->name_len || le || ino == MFT_REC_LOG)\n\t\t\tgoto out;\n\n\t\terr = ntfs_load_attr_list(ni, attr);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tle = NULL;\n\t\tattr = NULL;\n\t\tgoto next_attr;\n\n\tcase ATTR_NAME:\n\t\tif (attr->non_res || asize < SIZEOF_ATTRIBUTE_FILENAME + roff ||\n\t\t    rsize < SIZEOF_ATTRIBUTE_FILENAME)\n\t\t\tgoto out;\n\n\t\tfname = Add2Ptr(attr, roff);\n\t\tif (fname->type == FILE_NAME_DOS)\n\t\t\tgoto next_attr;\n\n\t\tnames += 1;\n\t\tif (name && name->len == fname->name_len &&\n\t\t    !ntfs_cmp_names_cpu(name, (struct le_str *)&fname->name_len,\n\t\t\t\t\tNULL, false))\n\t\t\tis_match = true;\n\n\t\tgoto next_attr;\n\n\tcase ATTR_DATA:\n\t\tif (is_dir) {\n\t\t\t/* Ignore data attribute in dir record. */\n\t\t\tgoto next_attr;\n\t\t}\n\n\t\tif (ino == MFT_REC_BADCLUST && !attr->non_res)\n\t\t\tgoto next_attr;\n\n\t\tif (attr->name_len &&\n\t\t    ((ino != MFT_REC_BADCLUST || !attr->non_res ||\n\t\t      attr->name_len != ARRAY_SIZE(BAD_NAME) ||\n\t\t      memcmp(attr_name(attr), BAD_NAME, sizeof(BAD_NAME))) &&\n\t\t     (ino != MFT_REC_SECURE || !attr->non_res ||\n\t\t      attr->name_len != ARRAY_SIZE(SDS_NAME) ||\n\t\t      memcmp(attr_name(attr), SDS_NAME, sizeof(SDS_NAME))))) {\n\t\t\t/* File contains stream attribute. Ignore it. */\n\t\t\tgoto next_attr;\n\t\t}\n\n\t\tif (is_attr_sparsed(attr))\n\t\t\tni->std_fa |= FILE_ATTRIBUTE_SPARSE_FILE;\n\t\telse\n\t\t\tni->std_fa &= ~FILE_ATTRIBUTE_SPARSE_FILE;\n\n\t\tif (is_attr_compressed(attr))\n\t\t\tni->std_fa |= FILE_ATTRIBUTE_COMPRESSED;\n\t\telse\n\t\t\tni->std_fa &= ~FILE_ATTRIBUTE_COMPRESSED;\n\n\t\tif (is_attr_encrypted(attr))\n\t\t\tni->std_fa |= FILE_ATTRIBUTE_ENCRYPTED;\n\t\telse\n\t\t\tni->std_fa &= ~FILE_ATTRIBUTE_ENCRYPTED;\n\n\t\tif (!attr->non_res) {\n\t\t\tni->i_valid = inode->i_size = rsize;\n\t\t\tinode_set_bytes(inode, rsize);\n\t\t}\n\n\t\tmode = S_IFREG | (0777 & sbi->options->fs_fmask_inv);\n\n\t\tif (!attr->non_res) {\n\t\t\tni->ni_flags |= NI_FLAG_RESIDENT;\n\t\t\tgoto next_attr;\n\t\t}\n\n\t\tinode_set_bytes(inode, attr_ondisk_size(attr));\n\n\t\tni->i_valid = le64_to_cpu(attr->nres.valid_size);\n\t\tinode->i_size = le64_to_cpu(attr->nres.data_size);\n\t\tif (!attr->nres.alloc_size)\n\t\t\tgoto next_attr;\n\n\t\trun = ino == MFT_REC_BITMAP ? &sbi->used.bitmap.run\n\t\t\t\t\t    : &ni->file.run;\n\t\tbreak;\n\n\tcase ATTR_ROOT:\n\t\tif (attr->non_res)\n\t\t\tgoto out;\n\n\t\troot = Add2Ptr(attr, roff);\n\n\t\tif (attr->name_len != ARRAY_SIZE(I30_NAME) ||\n\t\t    memcmp(attr_name(attr), I30_NAME, sizeof(I30_NAME)))\n\t\t\tgoto next_attr;\n\n\t\tif (root->type != ATTR_NAME ||\n\t\t    root->rule != NTFS_COLLATION_TYPE_FILENAME)\n\t\t\tgoto out;\n\n\t\tif (!is_dir)\n\t\t\tgoto next_attr;\n\n\t\tis_root = true;\n\t\tni->ni_flags |= NI_FLAG_DIR;\n\n\t\terr = indx_init(&ni->dir, sbi, attr, INDEX_MUTEX_I30);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tmode = sb->s_root\n\t\t\t       ? (S_IFDIR | (0777 & sbi->options->fs_dmask_inv))\n\t\t\t       : (S_IFDIR | 0777);\n\t\tgoto next_attr;\n\n\tcase ATTR_ALLOC:\n\t\tif (!is_root || attr->name_len != ARRAY_SIZE(I30_NAME) ||\n\t\t    memcmp(attr_name(attr), I30_NAME, sizeof(I30_NAME)))\n\t\t\tgoto next_attr;\n\n\t\tinode->i_size = le64_to_cpu(attr->nres.data_size);\n\t\tni->i_valid = le64_to_cpu(attr->nres.valid_size);\n\t\tinode_set_bytes(inode, le64_to_cpu(attr->nres.alloc_size));\n\n\t\trun = &ni->dir.alloc_run;\n\t\tbreak;\n\n\tcase ATTR_BITMAP:\n\t\tif (ino == MFT_REC_MFT) {\n\t\t\tif (!attr->non_res)\n\t\t\t\tgoto out;\n#ifndef CONFIG_NTFS3_64BIT_CLUSTER\n\t\t\t/* 0x20000000 = 2^32 / 8 */\n\t\t\tif (le64_to_cpu(attr->nres.alloc_size) >= 0x20000000)\n\t\t\t\tgoto out;\n#endif\n\t\t\trun = &sbi->mft.bitmap.run;\n\t\t\tbreak;\n\t\t} else if (is_dir && attr->name_len == ARRAY_SIZE(I30_NAME) &&\n\t\t\t   !memcmp(attr_name(attr), I30_NAME,\n\t\t\t\t   sizeof(I30_NAME)) &&\n\t\t\t   attr->non_res) {\n\t\t\trun = &ni->dir.bitmap_run;\n\t\t\tbreak;\n\t\t}\n\t\tgoto next_attr;\n\n\tcase ATTR_REPARSE:\n\t\tif (attr->name_len)\n\t\t\tgoto next_attr;\n\n\t\trp_fa = ni_parse_reparse(ni, attr, &rp);\n\t\tswitch (rp_fa) {\n\t\tcase REPARSE_LINK:\n\t\t\t/*\n\t\t\t * Normal symlink.\n\t\t\t * Assume one unicode symbol == one utf8.\n\t\t\t */\n\t\t\tinode->i_size = le16_to_cpu(rp.SymbolicLinkReparseBuffer\n\t\t\t\t\t\t\t    .PrintNameLength) /\n\t\t\t\t\tsizeof(u16);\n\n\t\t\tni->i_valid = inode->i_size;\n\n\t\t\t/* Clear directory bit. */\n\t\t\tif (ni->ni_flags & NI_FLAG_DIR) {\n\t\t\t\tindx_clear(&ni->dir);\n\t\t\t\tmemset(&ni->dir, 0, sizeof(ni->dir));\n\t\t\t\tni->ni_flags &= ~NI_FLAG_DIR;\n\t\t\t} else {\n\t\t\t\trun_close(&ni->file.run);\n\t\t\t}\n\t\t\tmode = S_IFLNK | 0777;\n\t\t\tis_dir = false;\n\t\t\tif (attr->non_res) {\n\t\t\t\trun = &ni->file.run;\n\t\t\t\tgoto attr_unpack_run; // Double break.\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase REPARSE_COMPRESSED:\n\t\t\tbreak;\n\n\t\tcase REPARSE_DEDUPLICATED:\n\t\t\tbreak;\n\t\t}\n\t\tgoto next_attr;\n\n\tcase ATTR_EA_INFO:\n\t\tif (!attr->name_len &&\n\t\t    resident_data_ex(attr, sizeof(struct EA_INFO))) {\n\t\t\tni->ni_flags |= NI_FLAG_EA;\n\t\t\t/*\n\t\t\t * ntfs_get_wsl_perm updates inode->i_uid, inode->i_gid, inode->i_mode\n\t\t\t */\n\t\t\tinode->i_mode = mode;\n\t\t\tntfs_get_wsl_perm(inode);\n\t\t\tmode = inode->i_mode;\n\t\t}\n\t\tgoto next_attr;\n\n\tdefault:\n\t\tgoto next_attr;\n\t}\n\nattr_unpack_run:\n\troff = le16_to_cpu(attr->nres.run_off);\n\n\tif (roff > asize) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt64 = le64_to_cpu(attr->nres.svcn);\n\n\terr = run_unpack_ex(run, sbi, ino, t64, le64_to_cpu(attr->nres.evcn),\n\t\t\t    t64, Add2Ptr(attr, roff), asize - roff);\n\tif (err < 0)\n\t\tgoto out;\n\terr = 0;\n\tgoto next_attr;\n\nend_enum:\n\n\tif (!std5)\n\t\tgoto out;\n\n\tif (!is_match && name) {\n\t\t/* Reuse rec as buffer for ascii name. */\n\t\terr = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tif (std5->fa & FILE_ATTRIBUTE_READONLY)\n\t\tmode &= ~0222;\n\n\tif (!names) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (names != le16_to_cpu(rec->hard_links)) {\n\t\t/* Correct minor error on the fly. Do not mark inode as dirty. */\n\t\trec->hard_links = cpu_to_le16(names);\n\t\tni->mi.dirty = true;\n\t}\n\n\tset_nlink(inode, names);\n\n\tif (S_ISDIR(mode)) {\n\t\tni->std_fa |= FILE_ATTRIBUTE_DIRECTORY;\n\n\t\t/*\n\t\t * Dot and dot-dot should be included in count but was not\n\t\t * included in enumeration.\n\t\t * Usually a hard links to directories are disabled.\n\t\t */\n\t\tinode->i_op = &ntfs_dir_inode_operations;\n\t\tinode->i_fop = &ntfs_dir_operations;\n\t\tni->i_valid = 0;\n\t} else if (S_ISLNK(mode)) {\n\t\tni->std_fa &= ~FILE_ATTRIBUTE_DIRECTORY;\n\t\tinode->i_op = &ntfs_link_inode_operations;\n\t\tinode->i_fop = NULL;\n\t\tinode_nohighmem(inode);\n\t} else if (S_ISREG(mode)) {\n\t\tni->std_fa &= ~FILE_ATTRIBUTE_DIRECTORY;\n\t\tinode->i_op = &ntfs_file_inode_operations;\n\t\tinode->i_fop = &ntfs_file_operations;\n\t\tinode->i_mapping->a_ops =\n\t\t\tis_compressed(ni) ? &ntfs_aops_cmpr : &ntfs_aops;\n\t\tif (ino != MFT_REC_MFT)\n\t\t\tinit_rwsem(&ni->file.run_lock);\n\t} else if (S_ISCHR(mode) || S_ISBLK(mode) || S_ISFIFO(mode) ||\n\t\t   S_ISSOCK(mode)) {\n\t\tinode->i_op = &ntfs_special_inode_operations;\n\t\tinit_special_inode(inode, mode, inode->i_rdev);\n\t} else if (fname && fname->home.low == cpu_to_le32(MFT_REC_EXTEND) &&\n\t\t   fname->home.seq == cpu_to_le16(MFT_REC_EXTEND)) {\n\t\t/* Records in $Extend are not a files or general directories. */\n\t\tinode->i_op = &ntfs_file_inode_operations;\n\t} else {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif ((sbi->options->sys_immutable &&\n\t     (std5->fa & FILE_ATTRIBUTE_SYSTEM)) &&\n\t    !S_ISFIFO(mode) && !S_ISSOCK(mode) && !S_ISLNK(mode)) {\n\t\tinode->i_flags |= S_IMMUTABLE;\n\t} else {\n\t\tinode->i_flags &= ~S_IMMUTABLE;\n\t}\n\n\tinode->i_mode = mode;\n\tif (!(ni->ni_flags & NI_FLAG_EA)) {\n\t\t/* If no xattr then no security (stored in xattr). */\n\t\tinode->i_flags |= S_NOSEC;\n\t}\n\n\tif (ino == MFT_REC_MFT && !sb->s_root)\n\t\tsbi->mft.ni = NULL;\n\n\tunlock_new_inode(inode);\n\n\treturn inode;\n\nout:\n\tif (ino == MFT_REC_MFT && !sb->s_root)\n\t\tsbi->mft.ni = NULL;\n\n\tiget_failed(inode);\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "static struct inode *ntfs_read_mft(struct inode *inode,\n\t\t\t\t   const struct cpu_str *name,\n\t\t\t\t   const struct MFT_REF *ref)\n{\n\tint err = 0;\n\tstruct ntfs_inode *ni = ntfs_i(inode);\n\tstruct super_block *sb = inode->i_sb;\n\tstruct ntfs_sb_info *sbi = sb->s_fs_info;\n\tmode_t mode = 0;\n\tstruct ATTR_STD_INFO5 *std5 = NULL;\n\tstruct ATTR_LIST_ENTRY *le;\n\tstruct ATTRIB *attr;\n\tbool is_match = false;\n\tbool is_root = false;\n\tbool is_dir;\n\tunsigned long ino = inode->i_ino;\n\tu32 rp_fa = 0, asize, t32;\n\tu16 roff, rsize, names = 0;\n\tconst struct ATTR_FILE_NAME *fname = NULL;\n\tconst struct INDEX_ROOT *root;\n\tstruct REPARSE_DATA_BUFFER rp; // 0x18 bytes\n\tu64 t64;\n\tstruct MFT_REC *rec;\n\tstruct runs_tree *run;\n\n\tinode->i_op = NULL;\n\t/* Setup 'uid' and 'gid' */\n\tinode->i_uid = sbi->options->fs_uid;\n\tinode->i_gid = sbi->options->fs_gid;\n\n\terr = mi_init(&ni->mi, sbi, ino);\n\tif (err)\n\t\tgoto out;\n\n\tif (!sbi->mft.ni && ino == MFT_REC_MFT && !sb->s_root) {\n\t\tt64 = sbi->mft.lbo >> sbi->cluster_bits;\n\t\tt32 = bytes_to_cluster(sbi, MFT_REC_VOL * sbi->record_size);\n\t\tsbi->mft.ni = ni;\n\t\tinit_rwsem(&ni->file.run_lock);\n\n\t\tif (!run_add_entry(&ni->file.run, 0, t64, t32, true)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\terr = mi_read(&ni->mi, ino == MFT_REC_MFT);\n\n\tif (err)\n\t\tgoto out;\n\n\trec = ni->mi.mrec;\n\n\tif (sbi->flags & NTFS_FLAGS_LOG_REPLAYING) {\n\t\t;\n\t} else if (ref->seq != rec->seq) {\n\t\terr = -EINVAL;\n\t\tntfs_err(sb, \"MFT: r=%lx, expect seq=%x instead of %x!\", ino,\n\t\t\t le16_to_cpu(ref->seq), le16_to_cpu(rec->seq));\n\t\tgoto out;\n\t} else if (!is_rec_inuse(rec)) {\n\t\terr = -ESTALE;\n\t\tntfs_err(sb, \"Inode r=%x is not in use!\", (u32)ino);\n\t\tgoto out;\n\t}\n\n\tif (le32_to_cpu(rec->total) != sbi->record_size) {\n\t\t/* Bad inode? */\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!is_rec_base(rec)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Record should contain $I30 root. */\n\tis_dir = rec->flags & RECORD_FLAG_DIR;\n\n\t/* MFT_REC_MFT is not a dir */\n\tif (is_dir && ino == MFT_REC_MFT) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tinode->i_generation = le16_to_cpu(rec->seq);\n\n\t/* Enumerate all struct Attributes MFT. */\n\tle = NULL;\n\tattr = NULL;\n\n\t/*\n\t * To reduce tab pressure use goto instead of\n\t * while( (attr = ni_enum_attr_ex(ni, attr, &le, NULL) ))\n\t */\nnext_attr:\n\trun = NULL;\n\terr = -EINVAL;\n\tattr = ni_enum_attr_ex(ni, attr, &le, NULL);\n\tif (!attr)\n\t\tgoto end_enum;\n\n\tif (le && le->vcn) {\n\t\t/* This is non primary attribute segment. Ignore if not MFT. */\n\t\tif (ino != MFT_REC_MFT || attr->type != ATTR_DATA)\n\t\t\tgoto next_attr;\n\n\t\trun = &ni->file.run;\n\t\tasize = le32_to_cpu(attr->size);\n\t\tgoto attr_unpack_run;\n\t}\n\n\troff = attr->non_res ? 0 : le16_to_cpu(attr->res.data_off);\n\trsize = attr->non_res ? 0 : le32_to_cpu(attr->res.data_size);\n\tasize = le32_to_cpu(attr->size);\n\n\tif (le16_to_cpu(attr->name_off) + attr->name_len > asize)\n\t\tgoto out;\n\n\tif (attr->non_res) {\n\t\tt64 = le64_to_cpu(attr->nres.alloc_size);\n\t\tif (le64_to_cpu(attr->nres.data_size) > t64 ||\n\t\t    le64_to_cpu(attr->nres.valid_size) > t64)\n\t\t\tgoto out;\n\t}\n\n\tswitch (attr->type) {\n\tcase ATTR_STD:\n\t\tif (attr->non_res ||\n\t\t    asize < sizeof(struct ATTR_STD_INFO) + roff ||\n\t\t    rsize < sizeof(struct ATTR_STD_INFO))\n\t\t\tgoto out;\n\n\t\tif (std5)\n\t\t\tgoto next_attr;\n\n\t\tstd5 = Add2Ptr(attr, roff);\n\n#ifdef STATX_BTIME\n\t\tnt2kernel(std5->cr_time, &ni->i_crtime);\n#endif\n\t\tnt2kernel(std5->a_time, &inode->i_atime);\n\t\tnt2kernel(std5->c_time, &inode->i_ctime);\n\t\tnt2kernel(std5->m_time, &inode->i_mtime);\n\n\t\tni->std_fa = std5->fa;\n\n\t\tif (asize >= sizeof(struct ATTR_STD_INFO5) + roff &&\n\t\t    rsize >= sizeof(struct ATTR_STD_INFO5))\n\t\t\tni->std_security_id = std5->security_id;\n\t\tgoto next_attr;\n\n\tcase ATTR_LIST:\n\t\tif (attr->name_len || le || ino == MFT_REC_LOG)\n\t\t\tgoto out;\n\n\t\terr = ntfs_load_attr_list(ni, attr);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tle = NULL;\n\t\tattr = NULL;\n\t\tgoto next_attr;\n\n\tcase ATTR_NAME:\n\t\tif (attr->non_res || asize < SIZEOF_ATTRIBUTE_FILENAME + roff ||\n\t\t    rsize < SIZEOF_ATTRIBUTE_FILENAME)\n\t\t\tgoto out;\n\n\t\tfname = Add2Ptr(attr, roff);\n\t\tif (fname->type == FILE_NAME_DOS)\n\t\t\tgoto next_attr;\n\n\t\tnames += 1;\n\t\tif (name && name->len == fname->name_len &&\n\t\t    !ntfs_cmp_names_cpu(name, (struct le_str *)&fname->name_len,\n\t\t\t\t\tNULL, false))\n\t\t\tis_match = true;\n\n\t\tgoto next_attr;\n\n\tcase ATTR_DATA:\n\t\tif (is_dir) {\n\t\t\t/* Ignore data attribute in dir record. */\n\t\t\tgoto next_attr;\n\t\t}\n\n\t\tif (ino == MFT_REC_BADCLUST && !attr->non_res)\n\t\t\tgoto next_attr;\n\n\t\tif (attr->name_len &&\n\t\t    ((ino != MFT_REC_BADCLUST || !attr->non_res ||\n\t\t      attr->name_len != ARRAY_SIZE(BAD_NAME) ||\n\t\t      memcmp(attr_name(attr), BAD_NAME, sizeof(BAD_NAME))) &&\n\t\t     (ino != MFT_REC_SECURE || !attr->non_res ||\n\t\t      attr->name_len != ARRAY_SIZE(SDS_NAME) ||\n\t\t      memcmp(attr_name(attr), SDS_NAME, sizeof(SDS_NAME))))) {\n\t\t\t/* File contains stream attribute. Ignore it. */\n\t\t\tgoto next_attr;\n\t\t}\n\n\t\tif (is_attr_sparsed(attr))\n\t\t\tni->std_fa |= FILE_ATTRIBUTE_SPARSE_FILE;\n\t\telse\n\t\t\tni->std_fa &= ~FILE_ATTRIBUTE_SPARSE_FILE;\n\n\t\tif (is_attr_compressed(attr))\n\t\t\tni->std_fa |= FILE_ATTRIBUTE_COMPRESSED;\n\t\telse\n\t\t\tni->std_fa &= ~FILE_ATTRIBUTE_COMPRESSED;\n\n\t\tif (is_attr_encrypted(attr))\n\t\t\tni->std_fa |= FILE_ATTRIBUTE_ENCRYPTED;\n\t\telse\n\t\t\tni->std_fa &= ~FILE_ATTRIBUTE_ENCRYPTED;\n\n\t\tif (!attr->non_res) {\n\t\t\tni->i_valid = inode->i_size = rsize;\n\t\t\tinode_set_bytes(inode, rsize);\n\t\t}\n\n\t\tmode = S_IFREG | (0777 & sbi->options->fs_fmask_inv);\n\n\t\tif (!attr->non_res) {\n\t\t\tni->ni_flags |= NI_FLAG_RESIDENT;\n\t\t\tgoto next_attr;\n\t\t}\n\n\t\tinode_set_bytes(inode, attr_ondisk_size(attr));\n\n\t\tni->i_valid = le64_to_cpu(attr->nres.valid_size);\n\t\tinode->i_size = le64_to_cpu(attr->nres.data_size);\n\t\tif (!attr->nres.alloc_size)\n\t\t\tgoto next_attr;\n\n\t\trun = ino == MFT_REC_BITMAP ? &sbi->used.bitmap.run\n\t\t\t\t\t    : &ni->file.run;\n\t\tbreak;\n\n\tcase ATTR_ROOT:\n\t\tif (attr->non_res)\n\t\t\tgoto out;\n\n\t\troot = Add2Ptr(attr, roff);\n\n\t\tif (attr->name_len != ARRAY_SIZE(I30_NAME) ||\n\t\t    memcmp(attr_name(attr), I30_NAME, sizeof(I30_NAME)))\n\t\t\tgoto next_attr;\n\n\t\tif (root->type != ATTR_NAME ||\n\t\t    root->rule != NTFS_COLLATION_TYPE_FILENAME)\n\t\t\tgoto out;\n\n\t\tif (!is_dir)\n\t\t\tgoto next_attr;\n\n\t\tis_root = true;\n\t\tni->ni_flags |= NI_FLAG_DIR;\n\n\t\terr = indx_init(&ni->dir, sbi, attr, INDEX_MUTEX_I30);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tmode = sb->s_root\n\t\t\t       ? (S_IFDIR | (0777 & sbi->options->fs_dmask_inv))\n\t\t\t       : (S_IFDIR | 0777);\n\t\tgoto next_attr;\n\n\tcase ATTR_ALLOC:\n\t\tif (!is_root || attr->name_len != ARRAY_SIZE(I30_NAME) ||\n\t\t    memcmp(attr_name(attr), I30_NAME, sizeof(I30_NAME)))\n\t\t\tgoto next_attr;\n\n\t\tinode->i_size = le64_to_cpu(attr->nres.data_size);\n\t\tni->i_valid = le64_to_cpu(attr->nres.valid_size);\n\t\tinode_set_bytes(inode, le64_to_cpu(attr->nres.alloc_size));\n\n\t\trun = &ni->dir.alloc_run;\n\t\tbreak;\n\n\tcase ATTR_BITMAP:\n\t\tif (ino == MFT_REC_MFT) {\n\t\t\tif (!attr->non_res)\n\t\t\t\tgoto out;\n#ifndef CONFIG_NTFS3_64BIT_CLUSTER\n\t\t\t/* 0x20000000 = 2^32 / 8 */\n\t\t\tif (le64_to_cpu(attr->nres.alloc_size) >= 0x20000000)\n\t\t\t\tgoto out;\n#endif\n\t\t\trun = &sbi->mft.bitmap.run;\n\t\t\tbreak;\n\t\t} else if (is_dir && attr->name_len == ARRAY_SIZE(I30_NAME) &&\n\t\t\t   !memcmp(attr_name(attr), I30_NAME,\n\t\t\t\t   sizeof(I30_NAME)) &&\n\t\t\t   attr->non_res) {\n\t\t\trun = &ni->dir.bitmap_run;\n\t\t\tbreak;\n\t\t}\n\t\tgoto next_attr;\n\n\tcase ATTR_REPARSE:\n\t\tif (attr->name_len)\n\t\t\tgoto next_attr;\n\n\t\trp_fa = ni_parse_reparse(ni, attr, &rp);\n\t\tswitch (rp_fa) {\n\t\tcase REPARSE_LINK:\n\t\t\t/*\n\t\t\t * Normal symlink.\n\t\t\t * Assume one unicode symbol == one utf8.\n\t\t\t */\n\t\t\tinode->i_size = le16_to_cpu(rp.SymbolicLinkReparseBuffer\n\t\t\t\t\t\t\t    .PrintNameLength) /\n\t\t\t\t\tsizeof(u16);\n\n\t\t\tni->i_valid = inode->i_size;\n\n\t\t\t/* Clear directory bit. */\n\t\t\tif (ni->ni_flags & NI_FLAG_DIR) {\n\t\t\t\tindx_clear(&ni->dir);\n\t\t\t\tmemset(&ni->dir, 0, sizeof(ni->dir));\n\t\t\t\tni->ni_flags &= ~NI_FLAG_DIR;\n\t\t\t} else {\n\t\t\t\trun_close(&ni->file.run);\n\t\t\t}\n\t\t\tmode = S_IFLNK | 0777;\n\t\t\tis_dir = false;\n\t\t\tif (attr->non_res) {\n\t\t\t\trun = &ni->file.run;\n\t\t\t\tgoto attr_unpack_run; // Double break.\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase REPARSE_COMPRESSED:\n\t\t\tbreak;\n\n\t\tcase REPARSE_DEDUPLICATED:\n\t\t\tbreak;\n\t\t}\n\t\tgoto next_attr;\n\n\tcase ATTR_EA_INFO:\n\t\tif (!attr->name_len &&\n\t\t    resident_data_ex(attr, sizeof(struct EA_INFO))) {\n\t\t\tni->ni_flags |= NI_FLAG_EA;\n\t\t\t/*\n\t\t\t * ntfs_get_wsl_perm updates inode->i_uid, inode->i_gid, inode->i_mode\n\t\t\t */\n\t\t\tinode->i_mode = mode;\n\t\t\tntfs_get_wsl_perm(inode);\n\t\t\tmode = inode->i_mode;\n\t\t}\n\t\tgoto next_attr;\n\n\tdefault:\n\t\tgoto next_attr;\n\t}\n\nattr_unpack_run:\n\troff = le16_to_cpu(attr->nres.run_off);\n\n\tif (roff > asize) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt64 = le64_to_cpu(attr->nres.svcn);\n\n\terr = run_unpack_ex(run, sbi, ino, t64, le64_to_cpu(attr->nres.evcn),\n\t\t\t    t64, Add2Ptr(attr, roff), asize - roff);\n\tif (err < 0)\n\t\tgoto out;\n\terr = 0;\n\tgoto next_attr;\n\nend_enum:\n\n\tif (!std5)\n\t\tgoto out;\n\n\tif (!is_match && name) {\n\t\t/* Reuse rec as buffer for ascii name. */\n\t\terr = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tif (std5->fa & FILE_ATTRIBUTE_READONLY)\n\t\tmode &= ~0222;\n\n\tif (!names) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (names != le16_to_cpu(rec->hard_links)) {\n\t\t/* Correct minor error on the fly. Do not mark inode as dirty. */\n\t\trec->hard_links = cpu_to_le16(names);\n\t\tni->mi.dirty = true;\n\t}\n\n\tset_nlink(inode, names);\n\n\tif (S_ISDIR(mode)) {\n\t\tni->std_fa |= FILE_ATTRIBUTE_DIRECTORY;\n\n\t\t/*\n\t\t * Dot and dot-dot should be included in count but was not\n\t\t * included in enumeration.\n\t\t * Usually a hard links to directories are disabled.\n\t\t */\n\t\tinode->i_op = &ntfs_dir_inode_operations;\n\t\tinode->i_fop = &ntfs_dir_operations;\n\t\tni->i_valid = 0;\n\t} else if (S_ISLNK(mode)) {\n\t\tni->std_fa &= ~FILE_ATTRIBUTE_DIRECTORY;\n\t\tinode->i_op = &ntfs_link_inode_operations;\n\t\tinode->i_fop = NULL;\n\t\tinode_nohighmem(inode);\n\t} else if (S_ISREG(mode)) {\n\t\tni->std_fa &= ~FILE_ATTRIBUTE_DIRECTORY;\n\t\tinode->i_op = &ntfs_file_inode_operations;\n\t\tinode->i_fop = &ntfs_file_operations;\n\t\tinode->i_mapping->a_ops =\n\t\t\tis_compressed(ni) ? &ntfs_aops_cmpr : &ntfs_aops;\n\t\tif (ino != MFT_REC_MFT)\n\t\t\tinit_rwsem(&ni->file.run_lock);\n\t} else if (S_ISCHR(mode) || S_ISBLK(mode) || S_ISFIFO(mode) ||\n\t\t   S_ISSOCK(mode)) {\n\t\tinode->i_op = &ntfs_special_inode_operations;\n\t\tinit_special_inode(inode, mode, inode->i_rdev);\n\t} else if (fname && fname->home.low == cpu_to_le32(MFT_REC_EXTEND) &&\n\t\t   fname->home.seq == cpu_to_le16(MFT_REC_EXTEND)) {\n\t\t/* Records in $Extend are not a files or general directories. */\n\t\tinode->i_op = &ntfs_file_inode_operations;\n\t} else {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif ((sbi->options->sys_immutable &&\n\t     (std5->fa & FILE_ATTRIBUTE_SYSTEM)) &&\n\t    !S_ISFIFO(mode) && !S_ISSOCK(mode) && !S_ISLNK(mode)) {\n\t\tinode->i_flags |= S_IMMUTABLE;\n\t} else {\n\t\tinode->i_flags &= ~S_IMMUTABLE;\n\t}\n\n\tinode->i_mode = mode;\n\tif (!(ni->ni_flags & NI_FLAG_EA)) {\n\t\t/* If no xattr then no security (stored in xattr). */\n\t\tinode->i_flags |= S_NOSEC;\n\t}\n\n\tif (ino == MFT_REC_MFT && !sb->s_root)\n\t\tsbi->mft.ni = NULL;\n\n\tunlock_new_inode(inode);\n\n\treturn inode;\n\nout:\n\tif (ino == MFT_REC_MFT && !sb->s_root)\n\t\tsbi->mft.ni = NULL;\n\n\tiget_failed(inode);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -77,6 +77,12 @@\n \n \t/* Record should contain $I30 root. */\n \tis_dir = rec->flags & RECORD_FLAG_DIR;\n+\n+\t/* MFT_REC_MFT is not a dir */\n+\tif (is_dir && ino == MFT_REC_MFT) {\n+\t\terr = -EINVAL;\n+\t\tgoto out;\n+\t}\n \n \tinode->i_generation = le16_to_cpu(rec->seq);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* MFT_REC_MFT is not a dir */",
                "\tif (is_dir && ino == MFT_REC_MFT) {",
                "\t\terr = -EINVAL;",
                "\t\tgoto out;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in smb2_is_status_io_timeout() in CIFS in the Linux Kernel. After CIFS transfers response data to a system call, there are still local variable points to the memory region, and if the system call frees it faster than CIFS uses it, CIFS will access a free memory region, leading to a denial of service.",
        "id": 3850
    },
    {
        "cve_id": "CVE-2023-1670",
        "code_before_change": "static void\nxirc2ps_detach(struct pcmcia_device *link)\n{\n    struct net_device *dev = link->priv;\n\n    dev_dbg(&link->dev, \"detach\\n\");\n\n    unregister_netdev(dev);\n\n    xirc2ps_release(link);\n\n    free_netdev(dev);\n} /* xirc2ps_detach */",
        "code_after_change": "static void\nxirc2ps_detach(struct pcmcia_device *link)\n{\n    struct net_device *dev = link->priv;\n    struct local_info *local = netdev_priv(dev);\n\n    netif_carrier_off(dev);\n    netif_tx_disable(dev);\n    cancel_work_sync(&local->tx_timeout_task);\n\n    dev_dbg(&link->dev, \"detach\\n\");\n\n    unregister_netdev(dev);\n\n    xirc2ps_release(link);\n\n    free_netdev(dev);\n} /* xirc2ps_detach */",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,11 @@\n xirc2ps_detach(struct pcmcia_device *link)\n {\n     struct net_device *dev = link->priv;\n+    struct local_info *local = netdev_priv(dev);\n+\n+    netif_carrier_off(dev);\n+    netif_tx_disable(dev);\n+    cancel_work_sync(&local->tx_timeout_task);\n \n     dev_dbg(&link->dev, \"detach\\n\");\n ",
        "function_modified_lines": {
            "added": [
                "    struct local_info *local = netdev_priv(dev);",
                "",
                "    netif_carrier_off(dev);",
                "    netif_tx_disable(dev);",
                "    cancel_work_sync(&local->tx_timeout_task);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw use after free in the Linux kernel Xircom 16-bit PCMCIA (PC-card) Ethernet driver was found.A local user could use this flaw to crash the system or potentially escalate their privileges on the system.",
        "id": 3878
    },
    {
        "cve_id": "CVE-2017-16527",
        "code_before_change": "static void snd_usb_mixer_free(struct usb_mixer_interface *mixer)\n{\n\tkfree(mixer->id_elems);\n\tif (mixer->urb) {\n\t\tkfree(mixer->urb->transfer_buffer);\n\t\tusb_free_urb(mixer->urb);\n\t}\n\tusb_free_urb(mixer->rc_urb);\n\tkfree(mixer->rc_setup_packet);\n\tkfree(mixer);\n}",
        "code_after_change": "static void snd_usb_mixer_free(struct usb_mixer_interface *mixer)\n{\n\t/* kill pending URBs */\n\tsnd_usb_mixer_disconnect(mixer);\n\n\tkfree(mixer->id_elems);\n\tif (mixer->urb) {\n\t\tkfree(mixer->urb->transfer_buffer);\n\t\tusb_free_urb(mixer->urb);\n\t}\n\tusb_free_urb(mixer->rc_urb);\n\tkfree(mixer->rc_setup_packet);\n\tkfree(mixer);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,8 @@\n static void snd_usb_mixer_free(struct usb_mixer_interface *mixer)\n {\n+\t/* kill pending URBs */\n+\tsnd_usb_mixer_disconnect(mixer);\n+\n \tkfree(mixer->id_elems);\n \tif (mixer->urb) {\n \t\tkfree(mixer->urb->transfer_buffer);",
        "function_modified_lines": {
            "added": [
                "\t/* kill pending URBs */",
                "\tsnd_usb_mixer_disconnect(mixer);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "sound/usb/mixer.c in the Linux kernel before 4.13.8 allows local users to cause a denial of service (snd_usb_mixer_interrupt use-after-free and system crash) or possibly have unspecified other impact via a crafted USB device.",
        "id": 1312
    },
    {
        "cve_id": "CVE-2021-39656",
        "code_before_change": "static int __configfs_open_file(struct inode *inode, struct file *file, int type)\n{\n\tstruct dentry *dentry = file->f_path.dentry;\n\tstruct configfs_fragment *frag = to_frag(file);\n\tstruct configfs_attribute *attr;\n\tstruct configfs_buffer *buffer;\n\tint error;\n\n\terror = -ENOMEM;\n\tbuffer = kzalloc(sizeof(struct configfs_buffer), GFP_KERNEL);\n\tif (!buffer)\n\t\tgoto out;\n\n\terror = -ENOENT;\n\tdown_read(&frag->frag_sem);\n\tif (unlikely(frag->frag_dead))\n\t\tgoto out_free_buffer;\n\n\terror = -EINVAL;\n\tbuffer->item = to_item(dentry->d_parent);\n\tif (!buffer->item)\n\t\tgoto out_free_buffer;\n\n\tattr = to_attr(dentry);\n\tif (!attr)\n\t\tgoto out_put_item;\n\n\tif (type & CONFIGFS_ITEM_BIN_ATTR) {\n\t\tbuffer->bin_attr = to_bin_attr(dentry);\n\t\tbuffer->cb_max_size = buffer->bin_attr->cb_max_size;\n\t} else {\n\t\tbuffer->attr = attr;\n\t}\n\n\tbuffer->owner = attr->ca_owner;\n\t/* Grab the module reference for this attribute if we have one */\n\terror = -ENODEV;\n\tif (!try_module_get(buffer->owner))\n\t\tgoto out_put_item;\n\n\terror = -EACCES;\n\tif (!buffer->item->ci_type)\n\t\tgoto out_put_module;\n\n\tbuffer->ops = buffer->item->ci_type->ct_item_ops;\n\n\t/* File needs write support.\n\t * The inode's perms must say it's ok,\n\t * and we must have a store method.\n\t */\n\tif (file->f_mode & FMODE_WRITE) {\n\t\tif (!(inode->i_mode & S_IWUGO))\n\t\t\tgoto out_put_module;\n\t\tif ((type & CONFIGFS_ITEM_ATTR) && !attr->store)\n\t\t\tgoto out_put_module;\n\t\tif ((type & CONFIGFS_ITEM_BIN_ATTR) && !buffer->bin_attr->write)\n\t\t\tgoto out_put_module;\n\t}\n\n\t/* File needs read support.\n\t * The inode's perms must say it's ok, and we there\n\t * must be a show method for it.\n\t */\n\tif (file->f_mode & FMODE_READ) {\n\t\tif (!(inode->i_mode & S_IRUGO))\n\t\t\tgoto out_put_module;\n\t\tif ((type & CONFIGFS_ITEM_ATTR) && !attr->show)\n\t\t\tgoto out_put_module;\n\t\tif ((type & CONFIGFS_ITEM_BIN_ATTR) && !buffer->bin_attr->read)\n\t\t\tgoto out_put_module;\n\t}\n\n\tmutex_init(&buffer->mutex);\n\tbuffer->needs_read_fill = 1;\n\tbuffer->read_in_progress = false;\n\tbuffer->write_in_progress = false;\n\tfile->private_data = buffer;\n\tup_read(&frag->frag_sem);\n\treturn 0;\n\nout_put_module:\n\tmodule_put(buffer->owner);\nout_put_item:\n\tconfig_item_put(buffer->item);\nout_free_buffer:\n\tup_read(&frag->frag_sem);\n\tkfree(buffer);\nout:\n\treturn error;\n}",
        "code_after_change": "static int __configfs_open_file(struct inode *inode, struct file *file, int type)\n{\n\tstruct dentry *dentry = file->f_path.dentry;\n\tstruct configfs_fragment *frag = to_frag(file);\n\tstruct configfs_attribute *attr;\n\tstruct configfs_buffer *buffer;\n\tint error;\n\n\terror = -ENOMEM;\n\tbuffer = kzalloc(sizeof(struct configfs_buffer), GFP_KERNEL);\n\tif (!buffer)\n\t\tgoto out;\n\n\terror = -ENOENT;\n\tdown_read(&frag->frag_sem);\n\tif (unlikely(frag->frag_dead))\n\t\tgoto out_free_buffer;\n\n\terror = -EINVAL;\n\tbuffer->item = to_item(dentry->d_parent);\n\tif (!buffer->item)\n\t\tgoto out_free_buffer;\n\n\tattr = to_attr(dentry);\n\tif (!attr)\n\t\tgoto out_free_buffer;\n\n\tif (type & CONFIGFS_ITEM_BIN_ATTR) {\n\t\tbuffer->bin_attr = to_bin_attr(dentry);\n\t\tbuffer->cb_max_size = buffer->bin_attr->cb_max_size;\n\t} else {\n\t\tbuffer->attr = attr;\n\t}\n\n\tbuffer->owner = attr->ca_owner;\n\t/* Grab the module reference for this attribute if we have one */\n\terror = -ENODEV;\n\tif (!try_module_get(buffer->owner))\n\t\tgoto out_free_buffer;\n\n\terror = -EACCES;\n\tif (!buffer->item->ci_type)\n\t\tgoto out_put_module;\n\n\tbuffer->ops = buffer->item->ci_type->ct_item_ops;\n\n\t/* File needs write support.\n\t * The inode's perms must say it's ok,\n\t * and we must have a store method.\n\t */\n\tif (file->f_mode & FMODE_WRITE) {\n\t\tif (!(inode->i_mode & S_IWUGO))\n\t\t\tgoto out_put_module;\n\t\tif ((type & CONFIGFS_ITEM_ATTR) && !attr->store)\n\t\t\tgoto out_put_module;\n\t\tif ((type & CONFIGFS_ITEM_BIN_ATTR) && !buffer->bin_attr->write)\n\t\t\tgoto out_put_module;\n\t}\n\n\t/* File needs read support.\n\t * The inode's perms must say it's ok, and we there\n\t * must be a show method for it.\n\t */\n\tif (file->f_mode & FMODE_READ) {\n\t\tif (!(inode->i_mode & S_IRUGO))\n\t\t\tgoto out_put_module;\n\t\tif ((type & CONFIGFS_ITEM_ATTR) && !attr->show)\n\t\t\tgoto out_put_module;\n\t\tif ((type & CONFIGFS_ITEM_BIN_ATTR) && !buffer->bin_attr->read)\n\t\t\tgoto out_put_module;\n\t}\n\n\tmutex_init(&buffer->mutex);\n\tbuffer->needs_read_fill = 1;\n\tbuffer->read_in_progress = false;\n\tbuffer->write_in_progress = false;\n\tfile->private_data = buffer;\n\tup_read(&frag->frag_sem);\n\treturn 0;\n\nout_put_module:\n\tmodule_put(buffer->owner);\nout_free_buffer:\n\tup_read(&frag->frag_sem);\n\tkfree(buffer);\nout:\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,7 +23,7 @@\n \n \tattr = to_attr(dentry);\n \tif (!attr)\n-\t\tgoto out_put_item;\n+\t\tgoto out_free_buffer;\n \n \tif (type & CONFIGFS_ITEM_BIN_ATTR) {\n \t\tbuffer->bin_attr = to_bin_attr(dentry);\n@@ -36,7 +36,7 @@\n \t/* Grab the module reference for this attribute if we have one */\n \terror = -ENODEV;\n \tif (!try_module_get(buffer->owner))\n-\t\tgoto out_put_item;\n+\t\tgoto out_free_buffer;\n \n \terror = -EACCES;\n \tif (!buffer->item->ci_type)\n@@ -80,8 +80,6 @@\n \n out_put_module:\n \tmodule_put(buffer->owner);\n-out_put_item:\n-\tconfig_item_put(buffer->item);\n out_free_buffer:\n \tup_read(&frag->frag_sem);\n \tkfree(buffer);",
        "function_modified_lines": {
            "added": [
                "\t\tgoto out_free_buffer;",
                "\t\tgoto out_free_buffer;"
            ],
            "deleted": [
                "\t\tgoto out_put_item;",
                "\t\tgoto out_put_item;",
                "out_put_item:",
                "\tconfig_item_put(buffer->item);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In __configfs_open_file of file.c, there is a possible use-after-free due to improper locking. This could lead to local escalation of privilege in the kernel with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-174049066References: Upstream kernel",
        "id": 3094
    },
    {
        "cve_id": "CVE-2018-20856",
        "code_before_change": "int blk_init_allocated_queue(struct request_queue *q)\n{\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tq->fq = blk_alloc_flush_queue(q, NUMA_NO_NODE, q->cmd_size);\n\tif (!q->fq)\n\t\treturn -ENOMEM;\n\n\tif (q->init_rq_fn && q->init_rq_fn(q, q->fq->flush_rq, GFP_KERNEL))\n\t\tgoto out_free_flush_queue;\n\n\tif (blk_init_rl(&q->root_rl, q, GFP_KERNEL))\n\t\tgoto out_exit_flush_rq;\n\n\tINIT_WORK(&q->timeout_work, blk_timeout_work);\n\tq->queue_flags\t\t|= QUEUE_FLAG_DEFAULT;\n\n\t/*\n\t * This also sets hw/phys segments, boundary and size\n\t */\n\tblk_queue_make_request(q, blk_queue_bio);\n\n\tq->sg_reserved_size = INT_MAX;\n\n\tif (elevator_init(q))\n\t\tgoto out_exit_flush_rq;\n\treturn 0;\n\nout_exit_flush_rq:\n\tif (q->exit_rq_fn)\n\t\tq->exit_rq_fn(q, q->fq->flush_rq);\nout_free_flush_queue:\n\tblk_free_flush_queue(q->fq);\n\treturn -ENOMEM;\n}",
        "code_after_change": "int blk_init_allocated_queue(struct request_queue *q)\n{\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tq->fq = blk_alloc_flush_queue(q, NUMA_NO_NODE, q->cmd_size);\n\tif (!q->fq)\n\t\treturn -ENOMEM;\n\n\tif (q->init_rq_fn && q->init_rq_fn(q, q->fq->flush_rq, GFP_KERNEL))\n\t\tgoto out_free_flush_queue;\n\n\tif (blk_init_rl(&q->root_rl, q, GFP_KERNEL))\n\t\tgoto out_exit_flush_rq;\n\n\tINIT_WORK(&q->timeout_work, blk_timeout_work);\n\tq->queue_flags\t\t|= QUEUE_FLAG_DEFAULT;\n\n\t/*\n\t * This also sets hw/phys segments, boundary and size\n\t */\n\tblk_queue_make_request(q, blk_queue_bio);\n\n\tq->sg_reserved_size = INT_MAX;\n\n\tif (elevator_init(q))\n\t\tgoto out_exit_flush_rq;\n\treturn 0;\n\nout_exit_flush_rq:\n\tif (q->exit_rq_fn)\n\t\tq->exit_rq_fn(q, q->fq->flush_rq);\nout_free_flush_queue:\n\tblk_free_flush_queue(q->fq);\n\tq->fq = NULL;\n\treturn -ENOMEM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,5 +31,6 @@\n \t\tq->exit_rq_fn(q, q->fq->flush_rq);\n out_free_flush_queue:\n \tblk_free_flush_queue(q->fq);\n+\tq->fq = NULL;\n \treturn -ENOMEM;\n }",
        "function_modified_lines": {
            "added": [
                "\tq->fq = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.18.7. In block/blk-core.c, there is an __blk_drain_queue() use-after-free because a certain error case is mishandled.",
        "id": 1787
    },
    {
        "cve_id": "CVE-2017-11176",
        "code_before_change": "static int do_mq_notify(mqd_t mqdes, const struct sigevent *notification)\n{\n\tint ret;\n\tstruct fd f;\n\tstruct sock *sock;\n\tstruct inode *inode;\n\tstruct mqueue_inode_info *info;\n\tstruct sk_buff *nc;\n\n\taudit_mq_notify(mqdes, notification);\n\n\tnc = NULL;\n\tsock = NULL;\n\tif (notification != NULL) {\n\t\tif (unlikely(notification->sigev_notify != SIGEV_NONE &&\n\t\t\t     notification->sigev_notify != SIGEV_SIGNAL &&\n\t\t\t     notification->sigev_notify != SIGEV_THREAD))\n\t\t\treturn -EINVAL;\n\t\tif (notification->sigev_notify == SIGEV_SIGNAL &&\n\t\t\t!valid_signal(notification->sigev_signo)) {\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (notification->sigev_notify == SIGEV_THREAD) {\n\t\t\tlong timeo;\n\n\t\t\t/* create the notify skb */\n\t\t\tnc = alloc_skb(NOTIFY_COOKIE_LEN, GFP_KERNEL);\n\t\t\tif (!nc) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (copy_from_user(nc->data,\n\t\t\t\t\tnotification->sigev_value.sival_ptr,\n\t\t\t\t\tNOTIFY_COOKIE_LEN)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t/* TODO: add a header? */\n\t\t\tskb_put(nc, NOTIFY_COOKIE_LEN);\n\t\t\t/* and attach it to the socket */\nretry:\n\t\t\tf = fdget(notification->sigev_signo);\n\t\t\tif (!f.file) {\n\t\t\t\tret = -EBADF;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsock = netlink_getsockbyfilp(f.file);\n\t\t\tfdput(f);\n\t\t\tif (IS_ERR(sock)) {\n\t\t\t\tret = PTR_ERR(sock);\n\t\t\t\tsock = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\ttimeo = MAX_SCHEDULE_TIMEOUT;\n\t\t\tret = netlink_attachskb(sock, nc, &timeo, NULL);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto retry;\n\t\t\tif (ret) {\n\t\t\t\tsock = NULL;\n\t\t\t\tnc = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tf = fdget(mqdes);\n\tif (!f.file) {\n\t\tret = -EBADF;\n\t\tgoto out;\n\t}\n\n\tinode = file_inode(f.file);\n\tif (unlikely(f.file->f_op != &mqueue_file_operations)) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\tinfo = MQUEUE_I(inode);\n\n\tret = 0;\n\tspin_lock(&info->lock);\n\tif (notification == NULL) {\n\t\tif (info->notify_owner == task_tgid(current)) {\n\t\t\tremove_notification(info);\n\t\t\tinode->i_atime = inode->i_ctime = current_time(inode);\n\t\t}\n\t} else if (info->notify_owner != NULL) {\n\t\tret = -EBUSY;\n\t} else {\n\t\tswitch (notification->sigev_notify) {\n\t\tcase SIGEV_NONE:\n\t\t\tinfo->notify.sigev_notify = SIGEV_NONE;\n\t\t\tbreak;\n\t\tcase SIGEV_THREAD:\n\t\t\tinfo->notify_sock = sock;\n\t\t\tinfo->notify_cookie = nc;\n\t\t\tsock = NULL;\n\t\t\tnc = NULL;\n\t\t\tinfo->notify.sigev_notify = SIGEV_THREAD;\n\t\t\tbreak;\n\t\tcase SIGEV_SIGNAL:\n\t\t\tinfo->notify.sigev_signo = notification->sigev_signo;\n\t\t\tinfo->notify.sigev_value = notification->sigev_value;\n\t\t\tinfo->notify.sigev_notify = SIGEV_SIGNAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tinfo->notify_owner = get_pid(task_tgid(current));\n\t\tinfo->notify_user_ns = get_user_ns(current_user_ns());\n\t\tinode->i_atime = inode->i_ctime = current_time(inode);\n\t}\n\tspin_unlock(&info->lock);\nout_fput:\n\tfdput(f);\nout:\n\tif (sock)\n\t\tnetlink_detachskb(sock, nc);\n\telse if (nc)\n\t\tdev_kfree_skb(nc);\n\n\treturn ret;\n}",
        "code_after_change": "static int do_mq_notify(mqd_t mqdes, const struct sigevent *notification)\n{\n\tint ret;\n\tstruct fd f;\n\tstruct sock *sock;\n\tstruct inode *inode;\n\tstruct mqueue_inode_info *info;\n\tstruct sk_buff *nc;\n\n\taudit_mq_notify(mqdes, notification);\n\n\tnc = NULL;\n\tsock = NULL;\n\tif (notification != NULL) {\n\t\tif (unlikely(notification->sigev_notify != SIGEV_NONE &&\n\t\t\t     notification->sigev_notify != SIGEV_SIGNAL &&\n\t\t\t     notification->sigev_notify != SIGEV_THREAD))\n\t\t\treturn -EINVAL;\n\t\tif (notification->sigev_notify == SIGEV_SIGNAL &&\n\t\t\t!valid_signal(notification->sigev_signo)) {\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (notification->sigev_notify == SIGEV_THREAD) {\n\t\t\tlong timeo;\n\n\t\t\t/* create the notify skb */\n\t\t\tnc = alloc_skb(NOTIFY_COOKIE_LEN, GFP_KERNEL);\n\t\t\tif (!nc) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (copy_from_user(nc->data,\n\t\t\t\t\tnotification->sigev_value.sival_ptr,\n\t\t\t\t\tNOTIFY_COOKIE_LEN)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t/* TODO: add a header? */\n\t\t\tskb_put(nc, NOTIFY_COOKIE_LEN);\n\t\t\t/* and attach it to the socket */\nretry:\n\t\t\tf = fdget(notification->sigev_signo);\n\t\t\tif (!f.file) {\n\t\t\t\tret = -EBADF;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsock = netlink_getsockbyfilp(f.file);\n\t\t\tfdput(f);\n\t\t\tif (IS_ERR(sock)) {\n\t\t\t\tret = PTR_ERR(sock);\n\t\t\t\tsock = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\ttimeo = MAX_SCHEDULE_TIMEOUT;\n\t\t\tret = netlink_attachskb(sock, nc, &timeo, NULL);\n\t\t\tif (ret == 1) {\n\t\t\t\tsock = NULL;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tsock = NULL;\n\t\t\t\tnc = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tf = fdget(mqdes);\n\tif (!f.file) {\n\t\tret = -EBADF;\n\t\tgoto out;\n\t}\n\n\tinode = file_inode(f.file);\n\tif (unlikely(f.file->f_op != &mqueue_file_operations)) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\tinfo = MQUEUE_I(inode);\n\n\tret = 0;\n\tspin_lock(&info->lock);\n\tif (notification == NULL) {\n\t\tif (info->notify_owner == task_tgid(current)) {\n\t\t\tremove_notification(info);\n\t\t\tinode->i_atime = inode->i_ctime = current_time(inode);\n\t\t}\n\t} else if (info->notify_owner != NULL) {\n\t\tret = -EBUSY;\n\t} else {\n\t\tswitch (notification->sigev_notify) {\n\t\tcase SIGEV_NONE:\n\t\t\tinfo->notify.sigev_notify = SIGEV_NONE;\n\t\t\tbreak;\n\t\tcase SIGEV_THREAD:\n\t\t\tinfo->notify_sock = sock;\n\t\t\tinfo->notify_cookie = nc;\n\t\t\tsock = NULL;\n\t\t\tnc = NULL;\n\t\t\tinfo->notify.sigev_notify = SIGEV_THREAD;\n\t\t\tbreak;\n\t\tcase SIGEV_SIGNAL:\n\t\t\tinfo->notify.sigev_signo = notification->sigev_signo;\n\t\t\tinfo->notify.sigev_value = notification->sigev_value;\n\t\t\tinfo->notify.sigev_notify = SIGEV_SIGNAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tinfo->notify_owner = get_pid(task_tgid(current));\n\t\tinfo->notify_user_ns = get_user_ns(current_user_ns());\n\t\tinode->i_atime = inode->i_ctime = current_time(inode);\n\t}\n\tspin_unlock(&info->lock);\nout_fput:\n\tfdput(f);\nout:\n\tif (sock)\n\t\tnetlink_detachskb(sock, nc);\n\telse if (nc)\n\t\tdev_kfree_skb(nc);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -55,8 +55,10 @@\n \n \t\t\ttimeo = MAX_SCHEDULE_TIMEOUT;\n \t\t\tret = netlink_attachskb(sock, nc, &timeo, NULL);\n-\t\t\tif (ret == 1)\n+\t\t\tif (ret == 1) {\n+\t\t\t\tsock = NULL;\n \t\t\t\tgoto retry;\n+\t\t\t}\n \t\t\tif (ret) {\n \t\t\t\tsock = NULL;\n \t\t\t\tnc = NULL;",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (ret == 1) {",
                "\t\t\t\tsock = NULL;",
                "\t\t\t}"
            ],
            "deleted": [
                "\t\t\tif (ret == 1)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The mq_notify function in the Linux kernel through 4.11.9 does not set the sock pointer to NULL upon entry into the retry logic. During a user-space close of a Netlink socket, it allows attackers to cause a denial of service (use-after-free) or possibly have unspecified other impact.",
        "id": 1249
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static void migrate_vma_unmap(struct migrate_vma *migrate)\n{\n\tconst unsigned long npages = migrate->npages;\n\tunsigned long i, restore = 0;\n\tbool allow_drain = true;\n\n\tlru_add_drain();\n\n\tfor (i = 0; i < npages; i++) {\n\t\tstruct page *page = migrate_pfn_to_page(migrate->src[i]);\n\t\tstruct folio *folio;\n\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t/* ZONE_DEVICE pages are not on LRU */\n\t\tif (!is_zone_device_page(page)) {\n\t\t\tif (!PageLRU(page) && allow_drain) {\n\t\t\t\t/* Drain CPU's pagevec */\n\t\t\t\tlru_add_drain_all();\n\t\t\t\tallow_drain = false;\n\t\t\t}\n\n\t\t\tif (isolate_lru_page(page)) {\n\t\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\t\tmigrate->cpages--;\n\t\t\t\trestore++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* Drop the reference we took in collect */\n\t\t\tput_page(page);\n\t\t}\n\n\t\tfolio = page_folio(page);\n\t\tif (folio_mapped(folio))\n\t\t\ttry_to_migrate(folio, 0);\n\n\t\tif (page_mapped(page) || !migrate_vma_check_page(page)) {\n\t\t\tif (!is_zone_device_page(page)) {\n\t\t\t\tget_page(page);\n\t\t\t\tputback_lru_page(page);\n\t\t\t}\n\n\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\tmigrate->cpages--;\n\t\t\trestore++;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tfor (i = 0; i < npages && restore; i++) {\n\t\tstruct page *page = migrate_pfn_to_page(migrate->src[i]);\n\t\tstruct folio *folio;\n\n\t\tif (!page || (migrate->src[i] & MIGRATE_PFN_MIGRATE))\n\t\t\tcontinue;\n\n\t\tfolio = page_folio(page);\n\t\tremove_migration_ptes(folio, folio, false);\n\n\t\tmigrate->src[i] = 0;\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\trestore--;\n\t}\n}",
        "code_after_change": "static void migrate_vma_unmap(struct migrate_vma *migrate)\n{\n\tconst unsigned long npages = migrate->npages;\n\tunsigned long i, restore = 0;\n\tbool allow_drain = true;\n\n\tlru_add_drain();\n\n\tfor (i = 0; i < npages; i++) {\n\t\tstruct page *page = migrate_pfn_to_page(migrate->src[i]);\n\t\tstruct folio *folio;\n\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t/* ZONE_DEVICE pages are not on LRU */\n\t\tif (!is_zone_device_page(page)) {\n\t\t\tif (!PageLRU(page) && allow_drain) {\n\t\t\t\t/* Drain CPU's pagevec */\n\t\t\t\tlru_add_drain_all();\n\t\t\t\tallow_drain = false;\n\t\t\t}\n\n\t\t\tif (isolate_lru_page(page)) {\n\t\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\t\tmigrate->cpages--;\n\t\t\t\trestore++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* Drop the reference we took in collect */\n\t\t\tput_page(page);\n\t\t}\n\n\t\tfolio = page_folio(page);\n\t\tif (folio_mapped(folio))\n\t\t\ttry_to_migrate(folio, 0);\n\n\t\tif (page_mapped(page) ||\n\t\t    !migrate_vma_check_page(page, migrate->fault_page)) {\n\t\t\tif (!is_zone_device_page(page)) {\n\t\t\t\tget_page(page);\n\t\t\t\tputback_lru_page(page);\n\t\t\t}\n\n\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\tmigrate->cpages--;\n\t\t\trestore++;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tfor (i = 0; i < npages && restore; i++) {\n\t\tstruct page *page = migrate_pfn_to_page(migrate->src[i]);\n\t\tstruct folio *folio;\n\n\t\tif (!page || (migrate->src[i] & MIGRATE_PFN_MIGRATE))\n\t\t\tcontinue;\n\n\t\tfolio = page_folio(page);\n\t\tremove_migration_ptes(folio, folio, false);\n\n\t\tmigrate->src[i] = 0;\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\trestore--;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -36,7 +36,8 @@\n \t\tif (folio_mapped(folio))\n \t\t\ttry_to_migrate(folio, 0);\n \n-\t\tif (page_mapped(page) || !migrate_vma_check_page(page)) {\n+\t\tif (page_mapped(page) ||\n+\t\t    !migrate_vma_check_page(page, migrate->fault_page)) {\n \t\t\tif (!is_zone_device_page(page)) {\n \t\t\t\tget_page(page);\n \t\t\t\tputback_lru_page(page);",
        "function_modified_lines": {
            "added": [
                "\t\tif (page_mapped(page) ||",
                "\t\t    !migrate_vma_check_page(page, migrate->fault_page)) {"
            ],
            "deleted": [
                "\t\tif (page_mapped(page) || !migrate_vma_check_page(page)) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3622
    },
    {
        "cve_id": "CVE-2022-1976",
        "code_before_change": "static int io_poll_check_events(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint v, ret;\n\n\t/* req->task == current here, checking PF_EXITING is safe */\n\tif (unlikely(req->task->flags & PF_EXITING))\n\t\treturn -ECANCELED;\n\n\tdo {\n\t\tv = atomic_read(&req->poll_refs);\n\n\t\t/* tw handler should be the owner, and so have some references */\n\t\tif (WARN_ON_ONCE(!(v & IO_POLL_REF_MASK)))\n\t\t\treturn 0;\n\t\tif (v & IO_POLL_CANCEL_FLAG)\n\t\t\treturn -ECANCELED;\n\n\t\tif (!req->cqe.res) {\n\t\t\tstruct poll_table_struct pt = { ._key = req->apoll_events };\n\t\t\tunsigned flags = locked ? 0 : IO_URING_F_UNLOCKED;\n\n\t\t\tif (unlikely(!io_assign_file(req, flags)))\n\t\t\t\treturn -EBADF;\n\t\t\treq->cqe.res = vfs_poll(req->file, &pt) & req->apoll_events;\n\t\t}\n\n\t\tif ((unlikely(!req->cqe.res)))\n\t\t\tcontinue;\n\t\tif (req->apoll_events & EPOLLONESHOT)\n\t\t\treturn 0;\n\n\t\t/* multishot, just fill a CQE and proceed */\n\t\tif (!(req->flags & REQ_F_APOLL_MULTISHOT)) {\n\t\t\t__poll_t mask = mangle_poll(req->cqe.res &\n\t\t\t\t\t\t    req->apoll_events);\n\t\t\tbool filled;\n\n\t\t\tspin_lock(&ctx->completion_lock);\n\t\t\tfilled = io_fill_cqe_aux(ctx, req->cqe.user_data,\n\t\t\t\t\t\t mask, IORING_CQE_F_MORE);\n\t\t\tio_commit_cqring(ctx);\n\t\t\tspin_unlock(&ctx->completion_lock);\n\t\t\tif (filled) {\n\t\t\t\tio_cqring_ev_posted(ctx);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treturn -ECANCELED;\n\t\t}\n\n\t\tio_tw_lock(req->ctx, locked);\n\t\tif (unlikely(req->task->flags & PF_EXITING))\n\t\t\treturn -EFAULT;\n\t\tret = io_issue_sqe(req,\n\t\t\t\t   IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * Release all references, retry if someone tried to restart\n\t\t * task_work while we were executing it.\n\t\t */\n\t} while (atomic_sub_return(v & IO_POLL_REF_MASK, &req->poll_refs));\n\n\treturn 1;\n}",
        "code_after_change": "static int io_poll_check_events(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint v, ret;\n\n\t/* req->task == current here, checking PF_EXITING is safe */\n\tif (unlikely(req->task->flags & PF_EXITING))\n\t\treturn -ECANCELED;\n\n\tdo {\n\t\tv = atomic_read(&req->poll_refs);\n\n\t\t/* tw handler should be the owner, and so have some references */\n\t\tif (WARN_ON_ONCE(!(v & IO_POLL_REF_MASK)))\n\t\t\treturn 0;\n\t\tif (v & IO_POLL_CANCEL_FLAG)\n\t\t\treturn -ECANCELED;\n\n\t\tif (!req->cqe.res) {\n\t\t\tstruct poll_table_struct pt = { ._key = req->apoll_events };\n\t\t\treq->cqe.res = vfs_poll(req->file, &pt) & req->apoll_events;\n\t\t}\n\n\t\tif ((unlikely(!req->cqe.res)))\n\t\t\tcontinue;\n\t\tif (req->apoll_events & EPOLLONESHOT)\n\t\t\treturn 0;\n\n\t\t/* multishot, just fill a CQE and proceed */\n\t\tif (!(req->flags & REQ_F_APOLL_MULTISHOT)) {\n\t\t\t__poll_t mask = mangle_poll(req->cqe.res &\n\t\t\t\t\t\t    req->apoll_events);\n\t\t\tbool filled;\n\n\t\t\tspin_lock(&ctx->completion_lock);\n\t\t\tfilled = io_fill_cqe_aux(ctx, req->cqe.user_data,\n\t\t\t\t\t\t mask, IORING_CQE_F_MORE);\n\t\t\tio_commit_cqring(ctx);\n\t\t\tspin_unlock(&ctx->completion_lock);\n\t\t\tif (filled) {\n\t\t\t\tio_cqring_ev_posted(ctx);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treturn -ECANCELED;\n\t\t}\n\n\t\tio_tw_lock(req->ctx, locked);\n\t\tif (unlikely(req->task->flags & PF_EXITING))\n\t\t\treturn -EFAULT;\n\t\tret = io_issue_sqe(req,\n\t\t\t\t   IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * Release all references, retry if someone tried to restart\n\t\t * task_work while we were executing it.\n\t\t */\n\t} while (atomic_sub_return(v & IO_POLL_REF_MASK, &req->poll_refs));\n\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,10 +18,6 @@\n \n \t\tif (!req->cqe.res) {\n \t\t\tstruct poll_table_struct pt = { ._key = req->apoll_events };\n-\t\t\tunsigned flags = locked ? 0 : IO_URING_F_UNLOCKED;\n-\n-\t\t\tif (unlikely(!io_assign_file(req, flags)))\n-\t\t\t\treturn -EBADF;\n \t\t\treq->cqe.res = vfs_poll(req->file, &pt) & req->apoll_events;\n \t\t}\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\t\tunsigned flags = locked ? 0 : IO_URING_F_UNLOCKED;",
                "",
                "\t\t\tif (unlikely(!io_assign_file(req, flags)))",
                "\t\t\t\treturn -EBADF;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s implementation of IO-URING. This flaw allows an attacker with local executable permission to create a string of requests that can cause a use-after-free flaw within the kernel. This issue leads to memory corruption and possible privilege escalation.",
        "id": 3325
    },
    {
        "cve_id": "CVE-2020-27067",
        "code_before_change": "static int l2tp_eth_create(struct net *net, struct l2tp_tunnel *tunnel,\n\t\t\t   u32 session_id, u32 peer_session_id,\n\t\t\t   struct l2tp_session_cfg *cfg)\n{\n\tunsigned char name_assign_type;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct l2tp_session *session;\n\tstruct l2tp_eth *priv;\n\tstruct l2tp_eth_sess *spriv;\n\tint rc;\n\tstruct l2tp_eth_net *pn;\n\n\tif (cfg->ifname) {\n\t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n\t\tname_assign_type = NET_NAME_USER;\n\t} else {\n\t\tstrcpy(name, L2TP_ETH_DEV_NAME);\n\t\tname_assign_type = NET_NAME_ENUM;\n\t}\n\n\tsession = l2tp_session_create(sizeof(*spriv), tunnel, session_id,\n\t\t\t\t      peer_session_id, cfg);\n\tif (IS_ERR(session)) {\n\t\trc = PTR_ERR(session);\n\t\tgoto out;\n\t}\n\n\tdev = alloc_netdev(sizeof(*priv), name, name_assign_type,\n\t\t\t   l2tp_eth_dev_setup);\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto out_del_session;\n\t}\n\n\tdev_net_set(dev, net);\n\tdev->min_mtu = 0;\n\tdev->max_mtu = ETH_MAX_MTU;\n\tl2tp_eth_adjust_mtu(tunnel, session, dev);\n\n\tpriv = netdev_priv(dev);\n\tpriv->dev = dev;\n\tpriv->session = session;\n\tINIT_LIST_HEAD(&priv->list);\n\n\tpriv->tunnel_sock = tunnel->sock;\n\tsession->recv_skb = l2tp_eth_dev_recv;\n\tsession->session_close = l2tp_eth_delete;\n#if IS_ENABLED(CONFIG_L2TP_DEBUGFS)\n\tsession->show = l2tp_eth_show;\n#endif\n\n\tspriv = l2tp_session_priv(session);\n\tspriv->dev = dev;\n\n\trc = register_netdev(dev);\n\tif (rc < 0)\n\t\tgoto out_del_dev;\n\n\t__module_get(THIS_MODULE);\n\t/* Must be done after register_netdev() */\n\tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n\n\tdev_hold(dev);\n\tpn = l2tp_eth_pernet(dev_net(dev));\n\tspin_lock(&pn->l2tp_eth_lock);\n\tlist_add(&priv->list, &pn->l2tp_eth_dev_list);\n\tspin_unlock(&pn->l2tp_eth_lock);\n\n\treturn 0;\n\nout_del_dev:\n\tfree_netdev(dev);\n\tspriv->dev = NULL;\nout_del_session:\n\tl2tp_session_delete(session);\nout:\n\treturn rc;\n}",
        "code_after_change": "static int l2tp_eth_create(struct net *net, struct l2tp_tunnel *tunnel,\n\t\t\t   u32 session_id, u32 peer_session_id,\n\t\t\t   struct l2tp_session_cfg *cfg)\n{\n\tunsigned char name_assign_type;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct l2tp_session *session;\n\tstruct l2tp_eth *priv;\n\tstruct l2tp_eth_sess *spriv;\n\tint rc;\n\n\tif (cfg->ifname) {\n\t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n\t\tname_assign_type = NET_NAME_USER;\n\t} else {\n\t\tstrcpy(name, L2TP_ETH_DEV_NAME);\n\t\tname_assign_type = NET_NAME_ENUM;\n\t}\n\n\tsession = l2tp_session_create(sizeof(*spriv), tunnel, session_id,\n\t\t\t\t      peer_session_id, cfg);\n\tif (IS_ERR(session)) {\n\t\trc = PTR_ERR(session);\n\t\tgoto out;\n\t}\n\n\tdev = alloc_netdev(sizeof(*priv), name, name_assign_type,\n\t\t\t   l2tp_eth_dev_setup);\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto out_del_session;\n\t}\n\n\tdev_net_set(dev, net);\n\tdev->min_mtu = 0;\n\tdev->max_mtu = ETH_MAX_MTU;\n\tl2tp_eth_adjust_mtu(tunnel, session, dev);\n\n\tpriv = netdev_priv(dev);\n\tpriv->dev = dev;\n\tpriv->session = session;\n\n\tpriv->tunnel_sock = tunnel->sock;\n\tsession->recv_skb = l2tp_eth_dev_recv;\n\tsession->session_close = l2tp_eth_delete;\n#if IS_ENABLED(CONFIG_L2TP_DEBUGFS)\n\tsession->show = l2tp_eth_show;\n#endif\n\n\tspriv = l2tp_session_priv(session);\n\tspriv->dev = dev;\n\n\trc = register_netdev(dev);\n\tif (rc < 0)\n\t\tgoto out_del_dev;\n\n\t__module_get(THIS_MODULE);\n\t/* Must be done after register_netdev() */\n\tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n\n\tdev_hold(dev);\n\n\treturn 0;\n\nout_del_dev:\n\tfree_netdev(dev);\n\tspriv->dev = NULL;\nout_del_session:\n\tl2tp_session_delete(session);\nout:\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,6 @@\n \tstruct l2tp_eth *priv;\n \tstruct l2tp_eth_sess *spriv;\n \tint rc;\n-\tstruct l2tp_eth_net *pn;\n \n \tif (cfg->ifname) {\n \t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n@@ -41,7 +40,6 @@\n \tpriv = netdev_priv(dev);\n \tpriv->dev = dev;\n \tpriv->session = session;\n-\tINIT_LIST_HEAD(&priv->list);\n \n \tpriv->tunnel_sock = tunnel->sock;\n \tsession->recv_skb = l2tp_eth_dev_recv;\n@@ -62,10 +60,6 @@\n \tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n \n \tdev_hold(dev);\n-\tpn = l2tp_eth_pernet(dev_net(dev));\n-\tspin_lock(&pn->l2tp_eth_lock);\n-\tlist_add(&priv->list, &pn->l2tp_eth_dev_list);\n-\tspin_unlock(&pn->l2tp_eth_lock);\n \n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct l2tp_eth_net *pn;",
                "\tINIT_LIST_HEAD(&priv->list);",
                "\tpn = l2tp_eth_pernet(dev_net(dev));",
                "\tspin_lock(&pn->l2tp_eth_lock);",
                "\tlist_add(&priv->list, &pn->l2tp_eth_dev_list);",
                "\tspin_unlock(&pn->l2tp_eth_lock);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the l2tp subsystem, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152409173",
        "id": 2612
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\n\t\tlock_sock(sk);\n\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,\n\t\t\t\t\t     optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "code_after_change": "static int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\tlock_sock(sk);\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -141,10 +141,11 @@\n \tcase IPV6_RTHDR:\n \tcase IPV6_DSTOPTS:\n \t{\n+\t\tstruct ipv6_txoptions *opt;\n \n \t\tlock_sock(sk);\n-\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,\n-\t\t\t\t\t     optname, optval, len);\n+\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n \t\trelease_sock(sk);\n \t\t/* check if ipv6_getsockopt_sticky() returns err code */\n \t\tif (len < 0)",
        "function_modified_lines": {
            "added": [
                "\t\tstruct ipv6_txoptions *opt;",
                "\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);"
            ],
            "deleted": [
                "\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,",
                "\t\t\t\t\t     optname, optval, len);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1002
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "void migrate_vma_pages(struct migrate_vma *migrate)\n{\n\tconst unsigned long npages = migrate->npages;\n\tconst unsigned long start = migrate->start;\n\tstruct mmu_notifier_range range;\n\tunsigned long addr, i;\n\tbool notified = false;\n\n\tfor (i = 0, addr = start; i < npages; addr += PAGE_SIZE, i++) {\n\t\tstruct page *newpage = migrate_pfn_to_page(migrate->dst[i]);\n\t\tstruct page *page = migrate_pfn_to_page(migrate->src[i]);\n\t\tstruct address_space *mapping;\n\t\tint r;\n\n\t\tif (!newpage) {\n\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!page) {\n\t\t\t/*\n\t\t\t * The only time there is no vma is when called from\n\t\t\t * migrate_device_coherent_page(). However this isn't\n\t\t\t * called if the page could not be unmapped.\n\t\t\t */\n\t\t\tVM_BUG_ON(!migrate->vma);\n\t\t\tif (!(migrate->src[i] & MIGRATE_PFN_MIGRATE))\n\t\t\t\tcontinue;\n\t\t\tif (!notified) {\n\t\t\t\tnotified = true;\n\n\t\t\t\tmmu_notifier_range_init_owner(&range,\n\t\t\t\t\tMMU_NOTIFY_MIGRATE, 0, migrate->vma,\n\t\t\t\t\tmigrate->vma->vm_mm, addr, migrate->end,\n\t\t\t\t\tmigrate->pgmap_owner);\n\t\t\t\tmmu_notifier_invalidate_range_start(&range);\n\t\t\t}\n\t\t\tmigrate_vma_insert_page(migrate, addr, newpage,\n\t\t\t\t\t\t&migrate->src[i]);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmapping = page_mapping(page);\n\n\t\tif (is_device_private_page(newpage) ||\n\t\t    is_device_coherent_page(newpage)) {\n\t\t\t/*\n\t\t\t * For now only support anonymous memory migrating to\n\t\t\t * device private or coherent memory.\n\t\t\t */\n\t\t\tif (mapping) {\n\t\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else if (is_zone_device_page(newpage)) {\n\t\t\t/*\n\t\t\t * Other types of ZONE_DEVICE page are not supported.\n\t\t\t */\n\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = migrate_folio(mapping, page_folio(newpage),\n\t\t\t\tpage_folio(page), MIGRATE_SYNC_NO_COPY);\n\t\tif (r != MIGRATEPAGE_SUCCESS)\n\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t}\n\n\t/*\n\t * No need to double call mmu_notifier->invalidate_range() callback as\n\t * the above ptep_clear_flush_notify() inside migrate_vma_insert_page()\n\t * did already call it.\n\t */\n\tif (notified)\n\t\tmmu_notifier_invalidate_range_only_end(&range);\n}",
        "code_after_change": "void migrate_vma_pages(struct migrate_vma *migrate)\n{\n\tconst unsigned long npages = migrate->npages;\n\tconst unsigned long start = migrate->start;\n\tstruct mmu_notifier_range range;\n\tunsigned long addr, i;\n\tbool notified = false;\n\n\tfor (i = 0, addr = start; i < npages; addr += PAGE_SIZE, i++) {\n\t\tstruct page *newpage = migrate_pfn_to_page(migrate->dst[i]);\n\t\tstruct page *page = migrate_pfn_to_page(migrate->src[i]);\n\t\tstruct address_space *mapping;\n\t\tint r;\n\n\t\tif (!newpage) {\n\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!page) {\n\t\t\t/*\n\t\t\t * The only time there is no vma is when called from\n\t\t\t * migrate_device_coherent_page(). However this isn't\n\t\t\t * called if the page could not be unmapped.\n\t\t\t */\n\t\t\tVM_BUG_ON(!migrate->vma);\n\t\t\tif (!(migrate->src[i] & MIGRATE_PFN_MIGRATE))\n\t\t\t\tcontinue;\n\t\t\tif (!notified) {\n\t\t\t\tnotified = true;\n\n\t\t\t\tmmu_notifier_range_init_owner(&range,\n\t\t\t\t\tMMU_NOTIFY_MIGRATE, 0, migrate->vma,\n\t\t\t\t\tmigrate->vma->vm_mm, addr, migrate->end,\n\t\t\t\t\tmigrate->pgmap_owner);\n\t\t\t\tmmu_notifier_invalidate_range_start(&range);\n\t\t\t}\n\t\t\tmigrate_vma_insert_page(migrate, addr, newpage,\n\t\t\t\t\t\t&migrate->src[i]);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmapping = page_mapping(page);\n\n\t\tif (is_device_private_page(newpage) ||\n\t\t    is_device_coherent_page(newpage)) {\n\t\t\t/*\n\t\t\t * For now only support anonymous memory migrating to\n\t\t\t * device private or coherent memory.\n\t\t\t */\n\t\t\tif (mapping) {\n\t\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else if (is_zone_device_page(newpage)) {\n\t\t\t/*\n\t\t\t * Other types of ZONE_DEVICE page are not supported.\n\t\t\t */\n\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (migrate->fault_page == page)\n\t\t\tr = migrate_folio_extra(mapping, page_folio(newpage),\n\t\t\t\t\t\tpage_folio(page),\n\t\t\t\t\t\tMIGRATE_SYNC_NO_COPY, 1);\n\t\telse\n\t\t\tr = migrate_folio(mapping, page_folio(newpage),\n\t\t\t\t\tpage_folio(page), MIGRATE_SYNC_NO_COPY);\n\t\tif (r != MIGRATEPAGE_SUCCESS)\n\t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n\t}\n\n\t/*\n\t * No need to double call mmu_notifier->invalidate_range() callback as\n\t * the above ptep_clear_flush_notify() inside migrate_vma_insert_page()\n\t * did already call it.\n\t */\n\tif (notified)\n\t\tmmu_notifier_invalidate_range_only_end(&range);\n}",
        "patch": "--- code before\n+++ code after\n@@ -60,8 +60,13 @@\n \t\t\tcontinue;\n \t\t}\n \n-\t\tr = migrate_folio(mapping, page_folio(newpage),\n-\t\t\t\tpage_folio(page), MIGRATE_SYNC_NO_COPY);\n+\t\tif (migrate->fault_page == page)\n+\t\t\tr = migrate_folio_extra(mapping, page_folio(newpage),\n+\t\t\t\t\t\tpage_folio(page),\n+\t\t\t\t\t\tMIGRATE_SYNC_NO_COPY, 1);\n+\t\telse\n+\t\t\tr = migrate_folio(mapping, page_folio(newpage),\n+\t\t\t\t\tpage_folio(page), MIGRATE_SYNC_NO_COPY);\n \t\tif (r != MIGRATEPAGE_SUCCESS)\n \t\t\tmigrate->src[i] &= ~MIGRATE_PFN_MIGRATE;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (migrate->fault_page == page)",
                "\t\t\tr = migrate_folio_extra(mapping, page_folio(newpage),",
                "\t\t\t\t\t\tpage_folio(page),",
                "\t\t\t\t\t\tMIGRATE_SYNC_NO_COPY, 1);",
                "\t\telse",
                "\t\t\tr = migrate_folio(mapping, page_folio(newpage),",
                "\t\t\t\t\tpage_folio(page), MIGRATE_SYNC_NO_COPY);"
            ],
            "deleted": [
                "\t\tr = migrate_folio(mapping, page_folio(newpage),",
                "\t\t\t\tpage_folio(page), MIGRATE_SYNC_NO_COPY);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3620
    },
    {
        "cve_id": "CVE-2020-29569",
        "code_before_change": "static int xen_blkif_disconnect(struct xen_blkif *blkif)\n{\n\tstruct pending_req *req, *n;\n\tunsigned int j, r;\n\tbool busy = false;\n\n\tfor (r = 0; r < blkif->nr_rings; r++) {\n\t\tstruct xen_blkif_ring *ring = &blkif->rings[r];\n\t\tunsigned int i = 0;\n\n\t\tif (!ring->active)\n\t\t\tcontinue;\n\n\t\tif (ring->xenblkd) {\n\t\t\tkthread_stop(ring->xenblkd);\n\t\t\twake_up(&ring->shutdown_wq);\n\t\t}\n\n\t\t/* The above kthread_stop() guarantees that at this point we\n\t\t * don't have any discard_io or other_io requests. So, checking\n\t\t * for inflight IO is enough.\n\t\t */\n\t\tif (atomic_read(&ring->inflight) > 0) {\n\t\t\tbusy = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ring->irq) {\n\t\t\tunbind_from_irqhandler(ring->irq, ring);\n\t\t\tring->irq = 0;\n\t\t}\n\n\t\tif (ring->blk_rings.common.sring) {\n\t\t\txenbus_unmap_ring_vfree(blkif->be->dev, ring->blk_ring);\n\t\t\tring->blk_rings.common.sring = NULL;\n\t\t}\n\n\t\t/* Remove all persistent grants and the cache of ballooned pages. */\n\t\txen_blkbk_free_caches(ring);\n\n\t\t/* Check that there is no request in use */\n\t\tlist_for_each_entry_safe(req, n, &ring->pending_free, free_list) {\n\t\t\tlist_del(&req->free_list);\n\n\t\t\tfor (j = 0; j < MAX_INDIRECT_SEGMENTS; j++)\n\t\t\t\tkfree(req->segments[j]);\n\n\t\t\tfor (j = 0; j < MAX_INDIRECT_PAGES; j++)\n\t\t\t\tkfree(req->indirect_pages[j]);\n\n\t\t\tkfree(req);\n\t\t\ti++;\n\t\t}\n\n\t\tBUG_ON(atomic_read(&ring->persistent_gnt_in_use) != 0);\n\t\tBUG_ON(!list_empty(&ring->persistent_purge_list));\n\t\tBUG_ON(!RB_EMPTY_ROOT(&ring->persistent_gnts));\n\t\tBUG_ON(ring->free_pages.num_pages != 0);\n\t\tBUG_ON(ring->persistent_gnt_c != 0);\n\t\tWARN_ON(i != (XEN_BLKIF_REQS_PER_PAGE * blkif->nr_ring_pages));\n\t\tring->active = false;\n\t}\n\tif (busy)\n\t\treturn -EBUSY;\n\n\tblkif->nr_ring_pages = 0;\n\t/*\n\t * blkif->rings was allocated in connect_ring, so we should free it in\n\t * here.\n\t */\n\tkfree(blkif->rings);\n\tblkif->rings = NULL;\n\tblkif->nr_rings = 0;\n\n\treturn 0;\n}",
        "code_after_change": "static int xen_blkif_disconnect(struct xen_blkif *blkif)\n{\n\tstruct pending_req *req, *n;\n\tunsigned int j, r;\n\tbool busy = false;\n\n\tfor (r = 0; r < blkif->nr_rings; r++) {\n\t\tstruct xen_blkif_ring *ring = &blkif->rings[r];\n\t\tunsigned int i = 0;\n\n\t\tif (!ring->active)\n\t\t\tcontinue;\n\n\t\tif (ring->xenblkd) {\n\t\t\tkthread_stop(ring->xenblkd);\n\t\t\tring->xenblkd = NULL;\n\t\t\twake_up(&ring->shutdown_wq);\n\t\t}\n\n\t\t/* The above kthread_stop() guarantees that at this point we\n\t\t * don't have any discard_io or other_io requests. So, checking\n\t\t * for inflight IO is enough.\n\t\t */\n\t\tif (atomic_read(&ring->inflight) > 0) {\n\t\t\tbusy = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ring->irq) {\n\t\t\tunbind_from_irqhandler(ring->irq, ring);\n\t\t\tring->irq = 0;\n\t\t}\n\n\t\tif (ring->blk_rings.common.sring) {\n\t\t\txenbus_unmap_ring_vfree(blkif->be->dev, ring->blk_ring);\n\t\t\tring->blk_rings.common.sring = NULL;\n\t\t}\n\n\t\t/* Remove all persistent grants and the cache of ballooned pages. */\n\t\txen_blkbk_free_caches(ring);\n\n\t\t/* Check that there is no request in use */\n\t\tlist_for_each_entry_safe(req, n, &ring->pending_free, free_list) {\n\t\t\tlist_del(&req->free_list);\n\n\t\t\tfor (j = 0; j < MAX_INDIRECT_SEGMENTS; j++)\n\t\t\t\tkfree(req->segments[j]);\n\n\t\t\tfor (j = 0; j < MAX_INDIRECT_PAGES; j++)\n\t\t\t\tkfree(req->indirect_pages[j]);\n\n\t\t\tkfree(req);\n\t\t\ti++;\n\t\t}\n\n\t\tBUG_ON(atomic_read(&ring->persistent_gnt_in_use) != 0);\n\t\tBUG_ON(!list_empty(&ring->persistent_purge_list));\n\t\tBUG_ON(!RB_EMPTY_ROOT(&ring->persistent_gnts));\n\t\tBUG_ON(ring->free_pages.num_pages != 0);\n\t\tBUG_ON(ring->persistent_gnt_c != 0);\n\t\tWARN_ON(i != (XEN_BLKIF_REQS_PER_PAGE * blkif->nr_ring_pages));\n\t\tring->active = false;\n\t}\n\tif (busy)\n\t\treturn -EBUSY;\n\n\tblkif->nr_ring_pages = 0;\n\t/*\n\t * blkif->rings was allocated in connect_ring, so we should free it in\n\t * here.\n\t */\n\tkfree(blkif->rings);\n\tblkif->rings = NULL;\n\tblkif->nr_rings = 0;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,7 @@\n \n \t\tif (ring->xenblkd) {\n \t\t\tkthread_stop(ring->xenblkd);\n+\t\t\tring->xenblkd = NULL;\n \t\t\twake_up(&ring->shutdown_wq);\n \t\t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tring->xenblkd = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.10.1, as used with Xen through 4.14.x. The Linux kernel PV block backend expects the kernel thread handler to reset ring->xenblkd to NULL when stopped. However, the handler may not have time to run if the frontend quickly toggles between the states connect and disconnect. As a consequence, the block backend may re-use a pointer after it was freed. A misbehaving guest can trigger a dom0 crash by continuously connecting / disconnecting a block frontend. Privilege escalation and information leaks cannot be ruled out. This only affects systems with a Linux blkback.",
        "id": 2699
    },
    {
        "cve_id": "CVE-2018-16884",
        "code_before_change": "static void svc_tcp_prep_reply_hdr(struct svc_rqst *rqstp)\n{\n\tstruct kvec *resv = &rqstp->rq_res.head[0];\n\n\t/* tcp needs a space for the record length... */\n\tsvc_putnl(resv, 0);\n}",
        "code_after_change": "void svc_tcp_prep_reply_hdr(struct svc_rqst *rqstp)\n{\n\tstruct kvec *resv = &rqstp->rq_res.head[0];\n\n\t/* tcp needs a space for the record length... */\n\tsvc_putnl(resv, 0);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-static void svc_tcp_prep_reply_hdr(struct svc_rqst *rqstp)\n+void svc_tcp_prep_reply_hdr(struct svc_rqst *rqstp)\n {\n \tstruct kvec *resv = &rqstp->rq_res.head[0];\n ",
        "function_modified_lines": {
            "added": [
                "void svc_tcp_prep_reply_hdr(struct svc_rqst *rqstp)"
            ],
            "deleted": [
                "static void svc_tcp_prep_reply_hdr(struct svc_rqst *rqstp)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's NFS41+ subsystem. NFS41+ shares mounted in different network namespaces at the same time can make bc_svc_process() use wrong back-channel IDs and cause a use-after-free vulnerability. Thus a malicious container user can cause a host kernel memory corruption and a system panic. Due to the nature of the flaw, privilege escalation cannot be fully ruled out.",
        "id": 1725
    },
    {
        "cve_id": "CVE-2023-20928",
        "code_before_change": "void binder_selftest_alloc(struct binder_alloc *alloc)\n{\n\tsize_t end_offset[BUFFER_NUM];\n\n\tif (!binder_selftest_run)\n\t\treturn;\n\tmutex_lock(&binder_selftest_lock);\n\tif (!binder_selftest_run || !alloc->vma)\n\t\tgoto done;\n\tpr_info(\"STARTED\\n\");\n\tbinder_selftest_alloc_offset(alloc, end_offset, 0);\n\tbinder_selftest_run = false;\n\tif (binder_selftest_failures > 0)\n\t\tpr_info(\"%d tests FAILED\\n\", binder_selftest_failures);\n\telse\n\t\tpr_info(\"PASSED\\n\");\n\ndone:\n\tmutex_unlock(&binder_selftest_lock);\n}",
        "code_after_change": "void binder_selftest_alloc(struct binder_alloc *alloc)\n{\n\tsize_t end_offset[BUFFER_NUM];\n\n\tif (!binder_selftest_run)\n\t\treturn;\n\tmutex_lock(&binder_selftest_lock);\n\tif (!binder_selftest_run || !alloc->vma_addr)\n\t\tgoto done;\n\tpr_info(\"STARTED\\n\");\n\tbinder_selftest_alloc_offset(alloc, end_offset, 0);\n\tbinder_selftest_run = false;\n\tif (binder_selftest_failures > 0)\n\t\tpr_info(\"%d tests FAILED\\n\", binder_selftest_failures);\n\telse\n\t\tpr_info(\"PASSED\\n\");\n\ndone:\n\tmutex_unlock(&binder_selftest_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \tif (!binder_selftest_run)\n \t\treturn;\n \tmutex_lock(&binder_selftest_lock);\n-\tif (!binder_selftest_run || !alloc->vma)\n+\tif (!binder_selftest_run || !alloc->vma_addr)\n \t\tgoto done;\n \tpr_info(\"STARTED\\n\");\n \tbinder_selftest_alloc_offset(alloc, end_offset, 0);",
        "function_modified_lines": {
            "added": [
                "\tif (!binder_selftest_run || !alloc->vma_addr)"
            ],
            "deleted": [
                "\tif (!binder_selftest_run || !alloc->vma)"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_vma_close of binder.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-254837884References: Upstream kernel",
        "id": 3911
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "static void unpin_rcv_pages(struct hfi1_filedata *fd,\n\t\t\t    struct tid_user_buf *tidbuf,\n\t\t\t    struct tid_rb_node *node,\n\t\t\t    unsigned int idx,\n\t\t\t    unsigned int npages,\n\t\t\t    bool mapped)\n{\n\tstruct page **pages;\n\tstruct hfi1_devdata *dd = fd->uctxt->dd;\n\n\tif (mapped) {\n\t\tpci_unmap_single(dd->pcidev, node->dma_addr,\n\t\t\t\t node->npages * PAGE_SIZE, PCI_DMA_FROMDEVICE);\n\t\tpages = &node->pages[idx];\n\t} else {\n\t\tpages = &tidbuf->pages[idx];\n\t}\n\thfi1_release_user_pages(fd->mm, pages, npages, mapped);\n\tfd->tid_n_pinned -= npages;\n}",
        "code_after_change": "static void unpin_rcv_pages(struct hfi1_filedata *fd,\n\t\t\t    struct tid_user_buf *tidbuf,\n\t\t\t    struct tid_rb_node *node,\n\t\t\t    unsigned int idx,\n\t\t\t    unsigned int npages,\n\t\t\t    bool mapped)\n{\n\tstruct page **pages;\n\tstruct hfi1_devdata *dd = fd->uctxt->dd;\n\tstruct mm_struct *mm;\n\n\tif (mapped) {\n\t\tpci_unmap_single(dd->pcidev, node->dma_addr,\n\t\t\t\t node->npages * PAGE_SIZE, PCI_DMA_FROMDEVICE);\n\t\tpages = &node->pages[idx];\n\t\tmm = mm_from_tid_node(node);\n\t} else {\n\t\tpages = &tidbuf->pages[idx];\n\t\tmm = current->mm;\n\t}\n\thfi1_release_user_pages(mm, pages, npages, mapped);\n\tfd->tid_n_pinned -= npages;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,14 +7,17 @@\n {\n \tstruct page **pages;\n \tstruct hfi1_devdata *dd = fd->uctxt->dd;\n+\tstruct mm_struct *mm;\n \n \tif (mapped) {\n \t\tpci_unmap_single(dd->pcidev, node->dma_addr,\n \t\t\t\t node->npages * PAGE_SIZE, PCI_DMA_FROMDEVICE);\n \t\tpages = &node->pages[idx];\n+\t\tmm = mm_from_tid_node(node);\n \t} else {\n \t\tpages = &tidbuf->pages[idx];\n+\t\tmm = current->mm;\n \t}\n-\thfi1_release_user_pages(fd->mm, pages, npages, mapped);\n+\thfi1_release_user_pages(mm, pages, npages, mapped);\n \tfd->tid_n_pinned -= npages;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct mm_struct *mm;",
                "\t\tmm = mm_from_tid_node(node);",
                "\t\tmm = current->mm;",
                "\thfi1_release_user_pages(mm, pages, npages, mapped);"
            ],
            "deleted": [
                "\thfi1_release_user_pages(fd->mm, pages, npages, mapped);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2649
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static void vmw_user_surface_base_release(struct ttm_base_object **p_base)\n{\n\tstruct ttm_base_object *base = *p_base;\n\tstruct vmw_user_surface *user_srf =\n\t    container_of(base, struct vmw_user_surface, prime.base);\n\tstruct vmw_resource *res = &user_srf->srf.res;\n\n\tif (res->guest_memory_bo)\n\t\tdrm_gem_object_put(&res->guest_memory_bo->tbo.base);\n\n\t*p_base = NULL;\n\tvmw_resource_unreference(&res);\n}",
        "code_after_change": "static void vmw_user_surface_base_release(struct ttm_base_object **p_base)\n{\n\tstruct ttm_base_object *base = *p_base;\n\tstruct vmw_user_surface *user_srf =\n\t    container_of(base, struct vmw_user_surface, prime.base);\n\tstruct vmw_resource *res = &user_srf->srf.res;\n\n\t*p_base = NULL;\n\tvmw_resource_unreference(&res);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,9 +5,6 @@\n \t    container_of(base, struct vmw_user_surface, prime.base);\n \tstruct vmw_resource *res = &user_srf->srf.res;\n \n-\tif (res->guest_memory_bo)\n-\t\tdrm_gem_object_put(&res->guest_memory_bo->tbo.base);\n-\n \t*p_base = NULL;\n \tvmw_resource_unreference(&res);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tif (res->guest_memory_bo)",
                "\t\tdrm_gem_object_put(&res->guest_memory_bo->tbo.base);",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4286
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static struct sock *dccp_v6_request_recv_sock(const struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst,\n\t\t\t\t\t      struct request_sock *req_unhash,\n\t\t\t\t\t      bool *own_req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t\t  req_unhash, own_req);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tstruct flowi6 fl6;\n\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_DCCP);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tnewsk->sk_v6_daddr\t= ireq->ir_v6_rmt_addr;\n\tnewnp->saddr\t\t= ireq->ir_v6_loc_addr;\n\tnewsk->sk_v6_rcv_saddr\t= ireq->ir_v6_loc_addr;\n\tnewsk->sk_bound_dev_if\t= ireq->ir_iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\tif (np->opt != NULL)\n\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt != NULL)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tinet_csk_prepare_forced_close(newsk);\n\t\tdccp_done(newsk);\n\t\tgoto out;\n\t}\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));\n\t/* Clone pktoptions received with SYN, if we own the req */\n\tif (*own_req && ireq->pktopts) {\n\t\tnewnp->pktoptions = skb_clone(ireq->pktopts, GFP_ATOMIC);\n\t\tconsume_skb(ireq->pktopts);\n\t\tireq->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "code_after_change": "static struct sock *dccp_v6_request_recv_sock(const struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst,\n\t\t\t\t\t      struct request_sock *req_unhash,\n\t\t\t\t\t      bool *own_req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t\t  req_unhash, own_req);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tstruct flowi6 fl6;\n\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_DCCP);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tnewsk->sk_v6_daddr\t= ireq->ir_v6_rmt_addr;\n\tnewnp->saddr\t\t= ireq->ir_v6_loc_addr;\n\tnewsk->sk_v6_rcv_saddr\t= ireq->ir_v6_loc_addr;\n\tnewsk->sk_bound_dev_if\t= ireq->ir_iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\topt = rcu_dereference(np->opt);\n\tif (opt) {\n\t\topt = ipv6_dup_options(newsk, opt);\n\t\tRCU_INIT_POINTER(newnp->opt, opt);\n\t}\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n\t\t\t\t\t\t    opt->opt_flen;\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tinet_csk_prepare_forced_close(newsk);\n\t\tdccp_done(newsk);\n\t\tgoto out;\n\t}\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));\n\t/* Clone pktoptions received with SYN, if we own the req */\n\tif (*own_req && ireq->pktopts) {\n\t\tnewnp->pktoptions = skb_clone(ireq->pktopts, GFP_ATOMIC);\n\t\tconsume_skb(ireq->pktopts);\n\t\tireq->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,7 @@\n \tstruct inet_request_sock *ireq = inet_rsk(req);\n \tstruct ipv6_pinfo *newnp;\n \tconst struct ipv6_pinfo *np = inet6_sk(sk);\n+\tstruct ipv6_txoptions *opt;\n \tstruct inet_sock *newinet;\n \tstruct dccp6_sock *newdp6;\n \tstruct sock *newsk;\n@@ -109,13 +110,15 @@\n \t * Yes, keeping reference count would be much more clever, but we make\n \t * one more one thing there: reattach optmem to newsk.\n \t */\n-\tif (np->opt != NULL)\n-\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);\n-\n+\topt = rcu_dereference(np->opt);\n+\tif (opt) {\n+\t\topt = ipv6_dup_options(newsk, opt);\n+\t\tRCU_INIT_POINTER(newnp->opt, opt);\n+\t}\n \tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n-\tif (newnp->opt != NULL)\n-\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n-\t\t\t\t\t\t     newnp->opt->opt_flen);\n+\tif (opt)\n+\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n+\t\t\t\t\t\t    opt->opt_flen;\n \n \tdccp_sync_mss(newsk, dst_mtu(dst));\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt;",
                "\topt = rcu_dereference(np->opt);",
                "\tif (opt) {",
                "\t\topt = ipv6_dup_options(newsk, opt);",
                "\t\tRCU_INIT_POINTER(newnp->opt, opt);",
                "\t}",
                "\tif (opt)",
                "\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +",
                "\t\t\t\t\t\t    opt->opt_flen;"
            ],
            "deleted": [
                "\tif (np->opt != NULL)",
                "\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);",
                "",
                "\tif (newnp->opt != NULL)",
                "\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +",
                "\t\t\t\t\t\t     newnp->opt->opt_flen);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 989
    },
    {
        "cve_id": "CVE-2021-29657",
        "code_before_change": "int enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb12_gpa,\n\t\t\t struct vmcb *vmcb12)\n{\n\tint ret;\n\n\tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n\tload_nested_vmcb_control(svm, &vmcb12->control);\n\tnested_prepare_vmcb_save(svm, vmcb12);\n\tnested_prepare_vmcb_control(svm);\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vmcb12->save.cr3,\n\t\t\t\t  nested_npt_enabled(svm));\n\tif (ret)\n\t\treturn ret;\n\n\tsvm_set_gif(svm, true);\n\n\treturn 0;\n}",
        "code_after_change": "int enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb12_gpa,\n\t\t\t struct vmcb *vmcb12)\n{\n\tint ret;\n\n\tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n\tnested_prepare_vmcb_save(svm, vmcb12);\n\tnested_prepare_vmcb_control(svm);\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vmcb12->save.cr3,\n\t\t\t\t  nested_npt_enabled(svm));\n\tif (ret)\n\t\treturn ret;\n\n\tsvm_set_gif(svm, true);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \tint ret;\n \n \tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n-\tload_nested_vmcb_control(svm, &vmcb12->control);\n \tnested_prepare_vmcb_save(svm, vmcb12);\n \tnested_prepare_vmcb_control(svm);\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tload_nested_vmcb_control(svm, &vmcb12->control);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-367"
        ],
        "cve_description": "arch/x86/kvm/svm/nested.c in the Linux kernel before 5.11.12 has a use-after-free in which an AMD KVM guest can bypass access control on host OS MSRs when there are nested guests, aka CID-a58d9166a756. This occurs because of a TOCTOU race condition associated with a VMCB12 double fetch in nested_svm_vmrun.",
        "id": 2956
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
        "code_after_change": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\tif (worker->saved_creds) {\n\t\trevert_creds(worker->saved_creds);\n\t\tworker->cur_creds = worker->saved_creds = NULL;\n\t}\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,11 @@\n \tworker->flags = 0;\n \tpreempt_enable();\n \n+\tif (worker->saved_creds) {\n+\t\trevert_creds(worker->saved_creds);\n+\t\tworker->cur_creds = worker->saved_creds = NULL;\n+\t}\n+\n \traw_spin_lock_irq(&wqe->lock);\n \thlist_nulls_del_rcu(&worker->nulls_node);\n \tlist_del_rcu(&worker->all_list);",
        "function_modified_lines": {
            "added": [
                "\tif (worker->saved_creds) {",
                "\t\trevert_creds(worker->saved_creds);",
                "\t\tworker->cur_creds = worker->saved_creds = NULL;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3279
    },
    {
        "cve_id": "CVE-2021-32606",
        "code_before_change": "static int isotp_bind(struct socket *sock, struct sockaddr *uaddr, int len)\n{\n\tstruct sockaddr_can *addr = (struct sockaddr_can *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct isotp_sock *so = isotp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint ifindex;\n\tstruct net_device *dev;\n\tint err = 0;\n\tint notify_enetdown = 0;\n\tint do_rx_reg = 1;\n\n\tif (len < ISOTP_MIN_NAMELEN)\n\t\treturn -EINVAL;\n\n\t/* do not register frame reception for functional addressing */\n\tif (so->opt.flags & CAN_ISOTP_SF_BROADCAST)\n\t\tdo_rx_reg = 0;\n\n\t/* do not validate rx address for functional addressing */\n\tif (do_rx_reg) {\n\t\tif (addr->can_addr.tp.rx_id == addr->can_addr.tp.tx_id)\n\t\t\treturn -EADDRNOTAVAIL;\n\n\t\tif (addr->can_addr.tp.rx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG))\n\t\t\treturn -EADDRNOTAVAIL;\n\t}\n\n\tif (addr->can_addr.tp.tx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG))\n\t\treturn -EADDRNOTAVAIL;\n\n\tif (!addr->can_ifindex)\n\t\treturn -ENODEV;\n\n\tlock_sock(sk);\n\n\tif (so->bound && addr->can_ifindex == so->ifindex &&\n\t    addr->can_addr.tp.rx_id == so->rxid &&\n\t    addr->can_addr.tp.tx_id == so->txid)\n\t\tgoto out;\n\n\tdev = dev_get_by_index(net, addr->can_ifindex);\n\tif (!dev) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (dev->type != ARPHRD_CAN) {\n\t\tdev_put(dev);\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (dev->mtu < so->ll.mtu) {\n\t\tdev_put(dev);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (!(dev->flags & IFF_UP))\n\t\tnotify_enetdown = 1;\n\n\tifindex = dev->ifindex;\n\n\tif (do_rx_reg)\n\t\tcan_rx_register(net, dev, addr->can_addr.tp.rx_id,\n\t\t\t\tSINGLE_MASK(addr->can_addr.tp.rx_id),\n\t\t\t\tisotp_rcv, sk, \"isotp\", sk);\n\n\tdev_put(dev);\n\n\tif (so->bound && do_rx_reg) {\n\t\t/* unregister old filter */\n\t\tif (so->ifindex) {\n\t\t\tdev = dev_get_by_index(net, so->ifindex);\n\t\t\tif (dev) {\n\t\t\t\tcan_rx_unregister(net, dev, so->rxid,\n\t\t\t\t\t\t  SINGLE_MASK(so->rxid),\n\t\t\t\t\t\t  isotp_rcv, sk);\n\t\t\t\tdev_put(dev);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* switch to new settings */\n\tso->ifindex = ifindex;\n\tso->rxid = addr->can_addr.tp.rx_id;\n\tso->txid = addr->can_addr.tp.tx_id;\n\tso->bound = 1;\n\nout:\n\trelease_sock(sk);\n\n\tif (notify_enetdown) {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\n\treturn err;\n}",
        "code_after_change": "static int isotp_bind(struct socket *sock, struct sockaddr *uaddr, int len)\n{\n\tstruct sockaddr_can *addr = (struct sockaddr_can *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct isotp_sock *so = isotp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint ifindex;\n\tstruct net_device *dev;\n\tint err = 0;\n\tint notify_enetdown = 0;\n\tint do_rx_reg = 1;\n\n\tif (len < ISOTP_MIN_NAMELEN)\n\t\treturn -EINVAL;\n\n\tif (addr->can_addr.tp.tx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG))\n\t\treturn -EADDRNOTAVAIL;\n\n\tif (!addr->can_ifindex)\n\t\treturn -ENODEV;\n\n\tlock_sock(sk);\n\n\t/* do not register frame reception for functional addressing */\n\tif (so->opt.flags & CAN_ISOTP_SF_BROADCAST)\n\t\tdo_rx_reg = 0;\n\n\t/* do not validate rx address for functional addressing */\n\tif (do_rx_reg) {\n\t\tif (addr->can_addr.tp.rx_id == addr->can_addr.tp.tx_id) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (addr->can_addr.tp.rx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG)) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (so->bound && addr->can_ifindex == so->ifindex &&\n\t    addr->can_addr.tp.rx_id == so->rxid &&\n\t    addr->can_addr.tp.tx_id == so->txid)\n\t\tgoto out;\n\n\tdev = dev_get_by_index(net, addr->can_ifindex);\n\tif (!dev) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (dev->type != ARPHRD_CAN) {\n\t\tdev_put(dev);\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (dev->mtu < so->ll.mtu) {\n\t\tdev_put(dev);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (!(dev->flags & IFF_UP))\n\t\tnotify_enetdown = 1;\n\n\tifindex = dev->ifindex;\n\n\tif (do_rx_reg)\n\t\tcan_rx_register(net, dev, addr->can_addr.tp.rx_id,\n\t\t\t\tSINGLE_MASK(addr->can_addr.tp.rx_id),\n\t\t\t\tisotp_rcv, sk, \"isotp\", sk);\n\n\tdev_put(dev);\n\n\tif (so->bound && do_rx_reg) {\n\t\t/* unregister old filter */\n\t\tif (so->ifindex) {\n\t\t\tdev = dev_get_by_index(net, so->ifindex);\n\t\t\tif (dev) {\n\t\t\t\tcan_rx_unregister(net, dev, so->rxid,\n\t\t\t\t\t\t  SINGLE_MASK(so->rxid),\n\t\t\t\t\t\t  isotp_rcv, sk);\n\t\t\t\tdev_put(dev);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* switch to new settings */\n\tso->ifindex = ifindex;\n\tso->rxid = addr->can_addr.tp.rx_id;\n\tso->txid = addr->can_addr.tp.tx_id;\n\tso->bound = 1;\n\nout:\n\trelease_sock(sk);\n\n\tif (notify_enetdown) {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,19 +13,6 @@\n \tif (len < ISOTP_MIN_NAMELEN)\n \t\treturn -EINVAL;\n \n-\t/* do not register frame reception for functional addressing */\n-\tif (so->opt.flags & CAN_ISOTP_SF_BROADCAST)\n-\t\tdo_rx_reg = 0;\n-\n-\t/* do not validate rx address for functional addressing */\n-\tif (do_rx_reg) {\n-\t\tif (addr->can_addr.tp.rx_id == addr->can_addr.tp.tx_id)\n-\t\t\treturn -EADDRNOTAVAIL;\n-\n-\t\tif (addr->can_addr.tp.rx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG))\n-\t\t\treturn -EADDRNOTAVAIL;\n-\t}\n-\n \tif (addr->can_addr.tp.tx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG))\n \t\treturn -EADDRNOTAVAIL;\n \n@@ -33,6 +20,23 @@\n \t\treturn -ENODEV;\n \n \tlock_sock(sk);\n+\n+\t/* do not register frame reception for functional addressing */\n+\tif (so->opt.flags & CAN_ISOTP_SF_BROADCAST)\n+\t\tdo_rx_reg = 0;\n+\n+\t/* do not validate rx address for functional addressing */\n+\tif (do_rx_reg) {\n+\t\tif (addr->can_addr.tp.rx_id == addr->can_addr.tp.tx_id) {\n+\t\t\terr = -EADDRNOTAVAIL;\n+\t\t\tgoto out;\n+\t\t}\n+\n+\t\tif (addr->can_addr.tp.rx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG)) {\n+\t\t\terr = -EADDRNOTAVAIL;\n+\t\t\tgoto out;\n+\t\t}\n+\t}\n \n \tif (so->bound && addr->can_ifindex == so->ifindex &&\n \t    addr->can_addr.tp.rx_id == so->rxid &&",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* do not register frame reception for functional addressing */",
                "\tif (so->opt.flags & CAN_ISOTP_SF_BROADCAST)",
                "\t\tdo_rx_reg = 0;",
                "",
                "\t/* do not validate rx address for functional addressing */",
                "\tif (do_rx_reg) {",
                "\t\tif (addr->can_addr.tp.rx_id == addr->can_addr.tp.tx_id) {",
                "\t\t\terr = -EADDRNOTAVAIL;",
                "\t\t\tgoto out;",
                "\t\t}",
                "",
                "\t\tif (addr->can_addr.tp.rx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG)) {",
                "\t\t\terr = -EADDRNOTAVAIL;",
                "\t\t\tgoto out;",
                "\t\t}",
                "\t}"
            ],
            "deleted": [
                "\t/* do not register frame reception for functional addressing */",
                "\tif (so->opt.flags & CAN_ISOTP_SF_BROADCAST)",
                "\t\tdo_rx_reg = 0;",
                "",
                "\t/* do not validate rx address for functional addressing */",
                "\tif (do_rx_reg) {",
                "\t\tif (addr->can_addr.tp.rx_id == addr->can_addr.tp.tx_id)",
                "\t\t\treturn -EADDRNOTAVAIL;",
                "",
                "\t\tif (addr->can_addr.tp.rx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG))",
                "\t\t\treturn -EADDRNOTAVAIL;",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.11 through 5.12.2, isotp_setsockopt in net/can/isotp.c allows privilege escalation to root by leveraging a use-after-free. (This does not affect earlier versions that lack CAN ISOTP SF_BROADCAST support.)",
        "id": 2967
    },
    {
        "cve_id": "CVE-2016-9120",
        "code_before_change": "static int ion_handle_put(struct ion_handle *handle)\n{\n\tstruct ion_client *client = handle->client;\n\tint ret;\n\n\tmutex_lock(&client->lock);\n\tret = kref_put(&handle->ref, ion_handle_destroy);\n\tmutex_unlock(&client->lock);\n\n\treturn ret;\n}",
        "code_after_change": "int ion_handle_put(struct ion_handle *handle)\n{\n\tstruct ion_client *client = handle->client;\n\tint ret;\n\n\tmutex_lock(&client->lock);\n\tret = ion_handle_put_nolock(handle);\n\tmutex_unlock(&client->lock);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,10 @@\n-static int ion_handle_put(struct ion_handle *handle)\n+int ion_handle_put(struct ion_handle *handle)\n {\n \tstruct ion_client *client = handle->client;\n \tint ret;\n \n \tmutex_lock(&client->lock);\n-\tret = kref_put(&handle->ref, ion_handle_destroy);\n+\tret = ion_handle_put_nolock(handle);\n \tmutex_unlock(&client->lock);\n \n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "int ion_handle_put(struct ion_handle *handle)",
                "\tret = ion_handle_put_nolock(handle);"
            ],
            "deleted": [
                "static int ion_handle_put(struct ion_handle *handle)",
                "\tret = kref_put(&handle->ref, ion_handle_destroy);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time.",
        "id": 1141
    },
    {
        "cve_id": "CVE-2018-10876",
        "code_before_change": "static noinline_for_stack int\next4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,\n\t\t\t\thandle_t *handle, unsigned int reserv_clstrs)\n{\n\tstruct buffer_head *bitmap_bh = NULL;\n\tstruct ext4_group_desc *gdp;\n\tstruct buffer_head *gdp_bh;\n\tstruct ext4_sb_info *sbi;\n\tstruct super_block *sb;\n\text4_fsblk_t block;\n\tint err, len;\n\n\tBUG_ON(ac->ac_status != AC_STATUS_FOUND);\n\tBUG_ON(ac->ac_b_ex.fe_len <= 0);\n\n\tsb = ac->ac_sb;\n\tsbi = EXT4_SB(sb);\n\n\tbitmap_bh = ext4_read_block_bitmap(sb, ac->ac_b_ex.fe_group);\n\tif (IS_ERR(bitmap_bh)) {\n\t\terr = PTR_ERR(bitmap_bh);\n\t\tbitmap_bh = NULL;\n\t\tgoto out_err;\n\t}\n\n\tBUFFER_TRACE(bitmap_bh, \"getting write access\");\n\terr = ext4_journal_get_write_access(handle, bitmap_bh);\n\tif (err)\n\t\tgoto out_err;\n\n\terr = -EIO;\n\tgdp = ext4_get_group_desc(sb, ac->ac_b_ex.fe_group, &gdp_bh);\n\tif (!gdp)\n\t\tgoto out_err;\n\n\text4_debug(\"using block group %u(%d)\\n\", ac->ac_b_ex.fe_group,\n\t\t\text4_free_group_clusters(sb, gdp));\n\n\tBUFFER_TRACE(gdp_bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, gdp_bh);\n\tif (err)\n\t\tgoto out_err;\n\n\tblock = ext4_grp_offs_to_block(sb, &ac->ac_b_ex);\n\n\tlen = EXT4_C2B(sbi, ac->ac_b_ex.fe_len);\n\tif (!ext4_data_block_valid(sbi, block, len)) {\n\t\text4_error(sb, \"Allocating blocks %llu-%llu which overlap \"\n\t\t\t   \"fs metadata\", block, block+len);\n\t\t/* File system mounted not to panic on error\n\t\t * Fix the bitmap and return EFSCORRUPTED\n\t\t * We leak some of the blocks here.\n\t\t */\n\t\text4_lock_group(sb, ac->ac_b_ex.fe_group);\n\t\text4_set_bits(bitmap_bh->b_data, ac->ac_b_ex.fe_start,\n\t\t\t      ac->ac_b_ex.fe_len);\n\t\text4_unlock_group(sb, ac->ac_b_ex.fe_group);\n\t\terr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\n\t\tif (!err)\n\t\t\terr = -EFSCORRUPTED;\n\t\tgoto out_err;\n\t}\n\n\text4_lock_group(sb, ac->ac_b_ex.fe_group);\n#ifdef AGGRESSIVE_CHECK\n\t{\n\t\tint i;\n\t\tfor (i = 0; i < ac->ac_b_ex.fe_len; i++) {\n\t\t\tBUG_ON(mb_test_bit(ac->ac_b_ex.fe_start + i,\n\t\t\t\t\t\tbitmap_bh->b_data));\n\t\t}\n\t}\n#endif\n\text4_set_bits(bitmap_bh->b_data, ac->ac_b_ex.fe_start,\n\t\t      ac->ac_b_ex.fe_len);\n\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_BLOCK_UNINIT);\n\t\text4_free_group_clusters_set(sb, gdp,\n\t\t\t\t\t     ext4_free_clusters_after_init(sb,\n\t\t\t\t\t\tac->ac_b_ex.fe_group, gdp));\n\t}\n\tlen = ext4_free_group_clusters(sb, gdp) - ac->ac_b_ex.fe_len;\n\text4_free_group_clusters_set(sb, gdp, len);\n\text4_block_bitmap_csum_set(sb, ac->ac_b_ex.fe_group, gdp, bitmap_bh);\n\text4_group_desc_csum_set(sb, ac->ac_b_ex.fe_group, gdp);\n\n\text4_unlock_group(sb, ac->ac_b_ex.fe_group);\n\tpercpu_counter_sub(&sbi->s_freeclusters_counter, ac->ac_b_ex.fe_len);\n\t/*\n\t * Now reduce the dirty block count also. Should not go negative\n\t */\n\tif (!(ac->ac_flags & EXT4_MB_DELALLOC_RESERVED))\n\t\t/* release all the reserved blocks if non delalloc */\n\t\tpercpu_counter_sub(&sbi->s_dirtyclusters_counter,\n\t\t\t\t   reserv_clstrs);\n\n\tif (sbi->s_log_groups_per_flex) {\n\t\text4_group_t flex_group = ext4_flex_group(sbi,\n\t\t\t\t\t\t\t  ac->ac_b_ex.fe_group);\n\t\tatomic64_sub(ac->ac_b_ex.fe_len,\n\t\t\t     &sbi->s_flex_groups[flex_group].free_clusters);\n\t}\n\n\terr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\n\tif (err)\n\t\tgoto out_err;\n\terr = ext4_handle_dirty_metadata(handle, NULL, gdp_bh);\n\nout_err:\n\tbrelse(bitmap_bh);\n\treturn err;\n}",
        "code_after_change": "static noinline_for_stack int\next4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,\n\t\t\t\thandle_t *handle, unsigned int reserv_clstrs)\n{\n\tstruct buffer_head *bitmap_bh = NULL;\n\tstruct ext4_group_desc *gdp;\n\tstruct buffer_head *gdp_bh;\n\tstruct ext4_sb_info *sbi;\n\tstruct super_block *sb;\n\text4_fsblk_t block;\n\tint err, len;\n\n\tBUG_ON(ac->ac_status != AC_STATUS_FOUND);\n\tBUG_ON(ac->ac_b_ex.fe_len <= 0);\n\n\tsb = ac->ac_sb;\n\tsbi = EXT4_SB(sb);\n\n\tbitmap_bh = ext4_read_block_bitmap(sb, ac->ac_b_ex.fe_group);\n\tif (IS_ERR(bitmap_bh)) {\n\t\terr = PTR_ERR(bitmap_bh);\n\t\tbitmap_bh = NULL;\n\t\tgoto out_err;\n\t}\n\n\tBUFFER_TRACE(bitmap_bh, \"getting write access\");\n\terr = ext4_journal_get_write_access(handle, bitmap_bh);\n\tif (err)\n\t\tgoto out_err;\n\n\terr = -EIO;\n\tgdp = ext4_get_group_desc(sb, ac->ac_b_ex.fe_group, &gdp_bh);\n\tif (!gdp)\n\t\tgoto out_err;\n\n\text4_debug(\"using block group %u(%d)\\n\", ac->ac_b_ex.fe_group,\n\t\t\text4_free_group_clusters(sb, gdp));\n\n\tBUFFER_TRACE(gdp_bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, gdp_bh);\n\tif (err)\n\t\tgoto out_err;\n\n\tblock = ext4_grp_offs_to_block(sb, &ac->ac_b_ex);\n\n\tlen = EXT4_C2B(sbi, ac->ac_b_ex.fe_len);\n\tif (!ext4_data_block_valid(sbi, block, len)) {\n\t\text4_error(sb, \"Allocating blocks %llu-%llu which overlap \"\n\t\t\t   \"fs metadata\", block, block+len);\n\t\t/* File system mounted not to panic on error\n\t\t * Fix the bitmap and return EFSCORRUPTED\n\t\t * We leak some of the blocks here.\n\t\t */\n\t\text4_lock_group(sb, ac->ac_b_ex.fe_group);\n\t\text4_set_bits(bitmap_bh->b_data, ac->ac_b_ex.fe_start,\n\t\t\t      ac->ac_b_ex.fe_len);\n\t\text4_unlock_group(sb, ac->ac_b_ex.fe_group);\n\t\terr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\n\t\tif (!err)\n\t\t\terr = -EFSCORRUPTED;\n\t\tgoto out_err;\n\t}\n\n\text4_lock_group(sb, ac->ac_b_ex.fe_group);\n#ifdef AGGRESSIVE_CHECK\n\t{\n\t\tint i;\n\t\tfor (i = 0; i < ac->ac_b_ex.fe_len; i++) {\n\t\t\tBUG_ON(mb_test_bit(ac->ac_b_ex.fe_start + i,\n\t\t\t\t\t\tbitmap_bh->b_data));\n\t\t}\n\t}\n#endif\n\text4_set_bits(bitmap_bh->b_data, ac->ac_b_ex.fe_start,\n\t\t      ac->ac_b_ex.fe_len);\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {\n\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_BLOCK_UNINIT);\n\t\text4_free_group_clusters_set(sb, gdp,\n\t\t\t\t\t     ext4_free_clusters_after_init(sb,\n\t\t\t\t\t\tac->ac_b_ex.fe_group, gdp));\n\t}\n\tlen = ext4_free_group_clusters(sb, gdp) - ac->ac_b_ex.fe_len;\n\text4_free_group_clusters_set(sb, gdp, len);\n\text4_block_bitmap_csum_set(sb, ac->ac_b_ex.fe_group, gdp, bitmap_bh);\n\text4_group_desc_csum_set(sb, ac->ac_b_ex.fe_group, gdp);\n\n\text4_unlock_group(sb, ac->ac_b_ex.fe_group);\n\tpercpu_counter_sub(&sbi->s_freeclusters_counter, ac->ac_b_ex.fe_len);\n\t/*\n\t * Now reduce the dirty block count also. Should not go negative\n\t */\n\tif (!(ac->ac_flags & EXT4_MB_DELALLOC_RESERVED))\n\t\t/* release all the reserved blocks if non delalloc */\n\t\tpercpu_counter_sub(&sbi->s_dirtyclusters_counter,\n\t\t\t\t   reserv_clstrs);\n\n\tif (sbi->s_log_groups_per_flex) {\n\t\text4_group_t flex_group = ext4_flex_group(sbi,\n\t\t\t\t\t\t\t  ac->ac_b_ex.fe_group);\n\t\tatomic64_sub(ac->ac_b_ex.fe_len,\n\t\t\t     &sbi->s_flex_groups[flex_group].free_clusters);\n\t}\n\n\terr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\n\tif (err)\n\t\tgoto out_err;\n\terr = ext4_handle_dirty_metadata(handle, NULL, gdp_bh);\n\nout_err:\n\tbrelse(bitmap_bh);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -73,7 +73,8 @@\n #endif\n \text4_set_bits(bitmap_bh->b_data, ac->ac_b_ex.fe_start,\n \t\t      ac->ac_b_ex.fe_len);\n-\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n+\tif (ext4_has_group_desc_csum(sb) &&\n+\t    (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {\n \t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_BLOCK_UNINIT);\n \t\text4_free_group_clusters_set(sb, gdp,\n \t\t\t\t\t     ext4_free_clusters_after_init(sb,",
        "function_modified_lines": {
            "added": [
                "\tif (ext4_has_group_desc_csum(sb) &&",
                "\t    (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {"
            ],
            "deleted": [
                "\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in Linux kernel in the ext4 filesystem code. A use-after-free is possible in ext4_ext_remove_space() function when mounting and operating a crafted ext4 image.",
        "id": 1607
    },
    {
        "cve_id": "CVE-2019-19966",
        "code_before_change": "static int __init cpia2_init(void)\n{\n\tLOG(\"%s v%s\\n\",\n\t    ABOUT, CPIA_VERSION);\n\tcheck_parameters();\n\tcpia2_usb_init();\n\treturn 0;\n}",
        "code_after_change": "static int __init cpia2_init(void)\n{\n\tLOG(\"%s v%s\\n\",\n\t    ABOUT, CPIA_VERSION);\n\tcheck_parameters();\n\treturn cpia2_usb_init();\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,5 @@\n \tLOG(\"%s v%s\\n\",\n \t    ABOUT, CPIA_VERSION);\n \tcheck_parameters();\n-\tcpia2_usb_init();\n-\treturn 0;\n+\treturn cpia2_usb_init();\n }",
        "function_modified_lines": {
            "added": [
                "\treturn cpia2_usb_init();"
            ],
            "deleted": [
                "\tcpia2_usb_init();",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.1.6, there is a use-after-free in cpia2_exit() in drivers/media/usb/cpia2/cpia2_v4l.c that will cause denial of service, aka CID-dea37a972655.",
        "id": 2270
    },
    {
        "cve_id": "CVE-2019-7221",
        "code_before_change": "static void free_nested(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (!vmx->nested.vmxon && !vmx->nested.smm.vmxon)\n\t\treturn;\n\n\tvmx->nested.vmxon = false;\n\tvmx->nested.smm.vmxon = false;\n\tfree_vpid(vmx->nested.vpid02);\n\tvmx->nested.posted_intr_nv = -1;\n\tvmx->nested.current_vmptr = -1ull;\n\tif (enable_shadow_vmcs) {\n\t\tvmx_disable_shadow_vmcs(vmx);\n\t\tvmcs_clear(vmx->vmcs01.shadow_vmcs);\n\t\tfree_vmcs(vmx->vmcs01.shadow_vmcs);\n\t\tvmx->vmcs01.shadow_vmcs = NULL;\n\t}\n\tkfree(vmx->nested.cached_vmcs12);\n\tkfree(vmx->nested.cached_shadow_vmcs12);\n\t/* Unpin physical memory we referred to in the vmcs02 */\n\tif (vmx->nested.apic_access_page) {\n\t\tkvm_release_page_dirty(vmx->nested.apic_access_page);\n\t\tvmx->nested.apic_access_page = NULL;\n\t}\n\tif (vmx->nested.virtual_apic_page) {\n\t\tkvm_release_page_dirty(vmx->nested.virtual_apic_page);\n\t\tvmx->nested.virtual_apic_page = NULL;\n\t}\n\tif (vmx->nested.pi_desc_page) {\n\t\tkunmap(vmx->nested.pi_desc_page);\n\t\tkvm_release_page_dirty(vmx->nested.pi_desc_page);\n\t\tvmx->nested.pi_desc_page = NULL;\n\t\tvmx->nested.pi_desc = NULL;\n\t}\n\n\tkvm_mmu_free_roots(vcpu, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);\n\n\tnested_release_evmcs(vcpu);\n\n\tfree_loaded_vmcs(&vmx->nested.vmcs02);\n}",
        "code_after_change": "static void free_nested(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (!vmx->nested.vmxon && !vmx->nested.smm.vmxon)\n\t\treturn;\n\n\thrtimer_cancel(&vmx->nested.preemption_timer);\n\tvmx->nested.vmxon = false;\n\tvmx->nested.smm.vmxon = false;\n\tfree_vpid(vmx->nested.vpid02);\n\tvmx->nested.posted_intr_nv = -1;\n\tvmx->nested.current_vmptr = -1ull;\n\tif (enable_shadow_vmcs) {\n\t\tvmx_disable_shadow_vmcs(vmx);\n\t\tvmcs_clear(vmx->vmcs01.shadow_vmcs);\n\t\tfree_vmcs(vmx->vmcs01.shadow_vmcs);\n\t\tvmx->vmcs01.shadow_vmcs = NULL;\n\t}\n\tkfree(vmx->nested.cached_vmcs12);\n\tkfree(vmx->nested.cached_shadow_vmcs12);\n\t/* Unpin physical memory we referred to in the vmcs02 */\n\tif (vmx->nested.apic_access_page) {\n\t\tkvm_release_page_dirty(vmx->nested.apic_access_page);\n\t\tvmx->nested.apic_access_page = NULL;\n\t}\n\tif (vmx->nested.virtual_apic_page) {\n\t\tkvm_release_page_dirty(vmx->nested.virtual_apic_page);\n\t\tvmx->nested.virtual_apic_page = NULL;\n\t}\n\tif (vmx->nested.pi_desc_page) {\n\t\tkunmap(vmx->nested.pi_desc_page);\n\t\tkvm_release_page_dirty(vmx->nested.pi_desc_page);\n\t\tvmx->nested.pi_desc_page = NULL;\n\t\tvmx->nested.pi_desc = NULL;\n\t}\n\n\tkvm_mmu_free_roots(vcpu, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);\n\n\tnested_release_evmcs(vcpu);\n\n\tfree_loaded_vmcs(&vmx->nested.vmcs02);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tif (!vmx->nested.vmxon && !vmx->nested.smm.vmxon)\n \t\treturn;\n \n+\thrtimer_cancel(&vmx->nested.preemption_timer);\n \tvmx->nested.vmxon = false;\n \tvmx->nested.smm.vmxon = false;\n \tfree_vpid(vmx->nested.vpid02);",
        "function_modified_lines": {
            "added": [
                "\thrtimer_cancel(&vmx->nested.preemption_timer);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The KVM implementation in the Linux kernel through 4.20.5 has a Use-after-Free.",
        "id": 2343
    },
    {
        "cve_id": "CVE-2022-20421",
        "code_before_change": "static int binder_inc_ref_for_node(struct binder_proc *proc,\n\t\t\tstruct binder_node *node,\n\t\t\tbool strong,\n\t\t\tstruct list_head *target_list,\n\t\t\tstruct binder_ref_data *rdata)\n{\n\tstruct binder_ref *ref;\n\tstruct binder_ref *new_ref = NULL;\n\tint ret = 0;\n\n\tbinder_proc_lock(proc);\n\tref = binder_get_ref_for_node_olocked(proc, node, NULL);\n\tif (!ref) {\n\t\tbinder_proc_unlock(proc);\n\t\tnew_ref = kzalloc(sizeof(*ref), GFP_KERNEL);\n\t\tif (!new_ref)\n\t\t\treturn -ENOMEM;\n\t\tbinder_proc_lock(proc);\n\t\tref = binder_get_ref_for_node_olocked(proc, node, new_ref);\n\t}\n\tret = binder_inc_ref_olocked(ref, strong, target_list);\n\t*rdata = ref->data;\n\tbinder_proc_unlock(proc);\n\tif (new_ref && ref != new_ref)\n\t\t/*\n\t\t * Another thread created the ref first so\n\t\t * free the one we allocated\n\t\t */\n\t\tkfree(new_ref);\n\treturn ret;\n}",
        "code_after_change": "static int binder_inc_ref_for_node(struct binder_proc *proc,\n\t\t\tstruct binder_node *node,\n\t\t\tbool strong,\n\t\t\tstruct list_head *target_list,\n\t\t\tstruct binder_ref_data *rdata)\n{\n\tstruct binder_ref *ref;\n\tstruct binder_ref *new_ref = NULL;\n\tint ret = 0;\n\n\tbinder_proc_lock(proc);\n\tref = binder_get_ref_for_node_olocked(proc, node, NULL);\n\tif (!ref) {\n\t\tbinder_proc_unlock(proc);\n\t\tnew_ref = kzalloc(sizeof(*ref), GFP_KERNEL);\n\t\tif (!new_ref)\n\t\t\treturn -ENOMEM;\n\t\tbinder_proc_lock(proc);\n\t\tref = binder_get_ref_for_node_olocked(proc, node, new_ref);\n\t}\n\tret = binder_inc_ref_olocked(ref, strong, target_list);\n\t*rdata = ref->data;\n\tif (ret && ref == new_ref) {\n\t\t/*\n\t\t * Cleanup the failed reference here as the target\n\t\t * could now be dead and have already released its\n\t\t * references by now. Calling on the new reference\n\t\t * with strong=0 and a tmp_refs will not decrement\n\t\t * the node. The new_ref gets kfree'd below.\n\t\t */\n\t\tbinder_cleanup_ref_olocked(new_ref);\n\t\tref = NULL;\n\t}\n\n\tbinder_proc_unlock(proc);\n\tif (new_ref && ref != new_ref)\n\t\t/*\n\t\t * Another thread created the ref first so\n\t\t * free the one we allocated\n\t\t */\n\t\tkfree(new_ref);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,6 +20,18 @@\n \t}\n \tret = binder_inc_ref_olocked(ref, strong, target_list);\n \t*rdata = ref->data;\n+\tif (ret && ref == new_ref) {\n+\t\t/*\n+\t\t * Cleanup the failed reference here as the target\n+\t\t * could now be dead and have already released its\n+\t\t * references by now. Calling on the new reference\n+\t\t * with strong=0 and a tmp_refs will not decrement\n+\t\t * the node. The new_ref gets kfree'd below.\n+\t\t */\n+\t\tbinder_cleanup_ref_olocked(new_ref);\n+\t\tref = NULL;\n+\t}\n+\n \tbinder_proc_unlock(proc);\n \tif (new_ref && ref != new_ref)\n \t\t/*",
        "function_modified_lines": {
            "added": [
                "\tif (ret && ref == new_ref) {",
                "\t\t/*",
                "\t\t * Cleanup the failed reference here as the target",
                "\t\t * could now be dead and have already released its",
                "\t\t * references by now. Calling on the new reference",
                "\t\t * with strong=0 and a tmp_refs will not decrement",
                "\t\t * the node. The new_ref gets kfree'd below.",
                "\t\t */",
                "\t\tbinder_cleanup_ref_olocked(new_ref);",
                "\t\tref = NULL;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In binder_inc_ref_for_node of binder.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-239630375References: Upstream kernel",
        "id": 3365
    },
    {
        "cve_id": "CVE-2023-2513",
        "code_before_change": "int ext4_xattr_ibody_set(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_xattr_info *i,\n\t\t\t\tstruct ext4_xattr_ibody_find *is)\n{\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_xattr_search *s = &is->s;\n\tint error;\n\n\tif (EXT4_I(inode)->i_extra_isize == 0)\n\t\treturn -ENOSPC;\n\terror = ext4_xattr_set_entry(i, s, handle, inode, false /* is_block */);\n\tif (error)\n\t\treturn error;\n\theader = IHDR(inode, ext4_raw_inode(&is->iloc));\n\tif (!IS_LAST_ENTRY(s->first)) {\n\t\theader->h_magic = cpu_to_le32(EXT4_XATTR_MAGIC);\n\t\text4_set_inode_state(inode, EXT4_STATE_XATTR);\n\t} else {\n\t\theader->h_magic = cpu_to_le32(0);\n\t\text4_clear_inode_state(inode, EXT4_STATE_XATTR);\n\t}\n\treturn 0;\n}",
        "code_after_change": "int ext4_xattr_ibody_set(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_xattr_info *i,\n\t\t\t\tstruct ext4_xattr_ibody_find *is)\n{\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_xattr_search *s = &is->s;\n\tint error;\n\n\tif (!EXT4_INODE_HAS_XATTR_SPACE(inode))\n\t\treturn -ENOSPC;\n\n\terror = ext4_xattr_set_entry(i, s, handle, inode, false /* is_block */);\n\tif (error)\n\t\treturn error;\n\theader = IHDR(inode, ext4_raw_inode(&is->iloc));\n\tif (!IS_LAST_ENTRY(s->first)) {\n\t\theader->h_magic = cpu_to_le32(EXT4_XATTR_MAGIC);\n\t\text4_set_inode_state(inode, EXT4_STATE_XATTR);\n\t} else {\n\t\theader->h_magic = cpu_to_le32(0);\n\t\text4_clear_inode_state(inode, EXT4_STATE_XATTR);\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,9 @@\n \tstruct ext4_xattr_search *s = &is->s;\n \tint error;\n \n-\tif (EXT4_I(inode)->i_extra_isize == 0)\n+\tif (!EXT4_INODE_HAS_XATTR_SPACE(inode))\n \t\treturn -ENOSPC;\n+\n \terror = ext4_xattr_set_entry(i, s, handle, inode, false /* is_block */);\n \tif (error)\n \t\treturn error;",
        "function_modified_lines": {
            "added": [
                "\tif (!EXT4_INODE_HAS_XATTR_SPACE(inode))",
                ""
            ],
            "deleted": [
                "\tif (EXT4_I(inode)->i_extra_isize == 0)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in the Linux kernel's ext4 filesystem in the way it handled the extra inode size for extended attributes. This flaw could allow a privileged local user to cause a system crash or other undefined behaviors.",
        "id": 3961
    },
    {
        "cve_id": "CVE-2022-45919",
        "code_before_change": "void dvb_ca_en50221_release(struct dvb_ca_en50221 *pubca)\n{\n\tstruct dvb_ca_private *ca = pubca->private;\n\tint i;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\t/* shutdown the thread if there was one */\n\tkthread_stop(ca->thread);\n\n\tfor (i = 0; i < ca->slot_count; i++)\n\t\tdvb_ca_en50221_slot_shutdown(ca, i);\n\n\tdvb_remove_device(ca->dvbdev);\n\tdvb_ca_private_put(ca);\n\tpubca->private = NULL;\n}",
        "code_after_change": "void dvb_ca_en50221_release(struct dvb_ca_en50221 *pubca)\n{\n\tstruct dvb_ca_private *ca = pubca->private;\n\tint i;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\tmutex_lock(&ca->remove_mutex);\n\tca->exit = 1;\n\tmutex_unlock(&ca->remove_mutex);\n\n\tif (ca->dvbdev->users < 1)\n\t\twait_event(ca->dvbdev->wait_queue,\n\t\t\t\tca->dvbdev->users == 1);\n\n\t/* shutdown the thread if there was one */\n\tkthread_stop(ca->thread);\n\n\tfor (i = 0; i < ca->slot_count; i++)\n\t\tdvb_ca_en50221_slot_shutdown(ca, i);\n\n\tdvb_remove_device(ca->dvbdev);\n\tdvb_ca_private_put(ca);\n\tpubca->private = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,14 @@\n \tint i;\n \n \tdprintk(\"%s\\n\", __func__);\n+\n+\tmutex_lock(&ca->remove_mutex);\n+\tca->exit = 1;\n+\tmutex_unlock(&ca->remove_mutex);\n+\n+\tif (ca->dvbdev->users < 1)\n+\t\twait_event(ca->dvbdev->wait_queue,\n+\t\t\t\tca->dvbdev->users == 1);\n \n \t/* shutdown the thread if there was one */\n \tkthread_stop(ca->thread);",
        "function_modified_lines": {
            "added": [
                "",
                "\tmutex_lock(&ca->remove_mutex);",
                "\tca->exit = 1;",
                "\tmutex_unlock(&ca->remove_mutex);",
                "",
                "\tif (ca->dvbdev->users < 1)",
                "\t\twait_event(ca->dvbdev->wait_queue,",
                "\t\t\t\tca->dvbdev->users == 1);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.0.10. In drivers/media/dvb-core/dvb_ca_en50221.c, a use-after-free can occur is there is a disconnect after an open, because of the lack of a wait_event.",
        "id": 3757
    },
    {
        "cve_id": "CVE-2014-0131",
        "code_before_change": "struct sk_buff *skb_segment(struct sk_buff *head_skb,\n\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = NULL;\n\tstruct sk_buff *tail = NULL;\n\tstruct sk_buff *list_skb = skb_shinfo(head_skb)->frag_list;\n\tskb_frag_t *frag = skb_shinfo(head_skb)->frags;\n\tunsigned int mss = skb_shinfo(head_skb)->gso_size;\n\tunsigned int doffset = head_skb->data - skb_mac_header(head_skb);\n\tunsigned int offset = doffset;\n\tunsigned int tnl_hlen = skb_tnl_header_len(head_skb);\n\tunsigned int headroom;\n\tunsigned int len;\n\t__be16 proto;\n\tbool csum;\n\tint sg = !!(features & NETIF_F_SG);\n\tint nfrags = skb_shinfo(head_skb)->nr_frags;\n\tint err = -ENOMEM;\n\tint i = 0;\n\tint pos;\n\n\tproto = skb_network_protocol(head_skb);\n\tif (unlikely(!proto))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcsum = !!can_checksum_protocol(features, proto);\n\t__skb_push(head_skb, doffset);\n\theadroom = skb_headroom(head_skb);\n\tpos = skb_headlen(head_skb);\n\n\tdo {\n\t\tstruct sk_buff *nskb;\n\t\tskb_frag_t *nskb_frag;\n\t\tint hsize;\n\t\tint size;\n\n\t\tlen = head_skb->len - offset;\n\t\tif (len > mss)\n\t\t\tlen = mss;\n\n\t\thsize = skb_headlen(head_skb) - offset;\n\t\tif (hsize < 0)\n\t\t\thsize = 0;\n\t\tif (hsize > len || !sg)\n\t\t\thsize = len;\n\n\t\tif (!hsize && i >= nfrags && skb_headlen(list_skb) &&\n\t\t    (skb_headlen(list_skb) == len || sg)) {\n\t\t\tBUG_ON(skb_headlen(list_skb) > len);\n\n\t\t\ti = 0;\n\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\tpos += skb_headlen(list_skb);\n\n\t\t\twhile (pos < offset + len) {\n\t\t\t\tBUG_ON(i >= nfrags);\n\n\t\t\t\tsize = skb_frag_size(frag);\n\t\t\t\tif (pos + size > offset + len)\n\t\t\t\t\tbreak;\n\n\t\t\t\ti++;\n\t\t\t\tpos += size;\n\t\t\t\tfrag++;\n\t\t\t}\n\n\t\t\tnskb = skb_clone(list_skb, GFP_ATOMIC);\n\t\t\tlist_skb = list_skb->next;\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tif (unlikely(pskb_trim(nskb, len))) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\thsize = skb_end_offset(nskb);\n\t\t\tif (skb_cow_head(nskb, doffset + headroom)) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tnskb->truesize += skb_end_offset(nskb) - hsize;\n\t\t\tskb_release_head_state(nskb);\n\t\t\t__skb_push(nskb, doffset);\n\t\t} else {\n\t\t\tnskb = __alloc_skb(hsize + doffset + headroom,\n\t\t\t\t\t   GFP_ATOMIC, skb_alloc_rx_flag(head_skb),\n\t\t\t\t\t   NUMA_NO_NODE);\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tskb_reserve(nskb, headroom);\n\t\t\t__skb_put(nskb, doffset);\n\t\t}\n\n\t\tif (segs)\n\t\t\ttail->next = nskb;\n\t\telse\n\t\t\tsegs = nskb;\n\t\ttail = nskb;\n\n\t\t__copy_skb_header(nskb, head_skb);\n\t\tnskb->mac_len = head_skb->mac_len;\n\n\t\tskb_headers_offset_update(nskb, skb_headroom(nskb) - headroom);\n\n\t\tskb_copy_from_linear_data_offset(head_skb, -tnl_hlen,\n\t\t\t\t\t\t nskb->data - tnl_hlen,\n\t\t\t\t\t\t doffset + tnl_hlen);\n\n\t\tif (nskb->len == len + doffset)\n\t\t\tgoto perform_csum_check;\n\n\t\tif (!sg) {\n\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\tnskb->csum = skb_copy_and_csum_bits(head_skb, offset,\n\t\t\t\t\t\t\t    skb_put(nskb, len),\n\t\t\t\t\t\t\t    len, 0);\n\t\t\tcontinue;\n\t\t}\n\n\t\tnskb_frag = skb_shinfo(nskb)->frags;\n\n\t\tskb_copy_from_linear_data_offset(head_skb, offset,\n\t\t\t\t\t\t skb_put(nskb, hsize), hsize);\n\n\t\tskb_shinfo(nskb)->tx_flags = skb_shinfo(head_skb)->tx_flags &\n\t\t\tSKBTX_SHARED_FRAG;\n\n\t\twhile (pos < offset + len) {\n\t\t\tif (i >= nfrags) {\n\t\t\t\tBUG_ON(skb_headlen(list_skb));\n\n\t\t\t\ti = 0;\n\t\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\n\t\t\t\tBUG_ON(!nfrags);\n\n\t\t\t\tlist_skb = list_skb->next;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_shinfo(nskb)->nr_frags >=\n\t\t\t\t     MAX_SKB_FRAGS)) {\n\t\t\t\tnet_warn_ratelimited(\n\t\t\t\t\t\"skb_segment: too many frags: %u %u\\n\",\n\t\t\t\t\tpos, mss);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\t*nskb_frag = *frag;\n\t\t\t__skb_frag_ref(nskb_frag);\n\t\t\tsize = skb_frag_size(nskb_frag);\n\n\t\t\tif (pos < offset) {\n\t\t\t\tnskb_frag->page_offset += offset - pos;\n\t\t\t\tskb_frag_size_sub(nskb_frag, offset - pos);\n\t\t\t}\n\n\t\t\tskb_shinfo(nskb)->nr_frags++;\n\n\t\t\tif (pos + size <= offset + len) {\n\t\t\t\ti++;\n\t\t\t\tfrag++;\n\t\t\t\tpos += size;\n\t\t\t} else {\n\t\t\t\tskb_frag_size_sub(nskb_frag, pos + size - (offset + len));\n\t\t\t\tgoto skip_fraglist;\n\t\t\t}\n\n\t\t\tnskb_frag++;\n\t\t}\n\nskip_fraglist:\n\t\tnskb->data_len = len - hsize;\n\t\tnskb->len += nskb->data_len;\n\t\tnskb->truesize += nskb->data_len;\n\nperform_csum_check:\n\t\tif (!csum) {\n\t\t\tnskb->csum = skb_checksum(nskb, doffset,\n\t\t\t\t\t\t  nskb->len - doffset, 0);\n\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t} while ((offset += len) < head_skb->len);\n\n\treturn segs;\n\nerr:\n\tkfree_skb_list(segs);\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "struct sk_buff *skb_segment(struct sk_buff *head_skb,\n\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = NULL;\n\tstruct sk_buff *tail = NULL;\n\tstruct sk_buff *list_skb = skb_shinfo(head_skb)->frag_list;\n\tskb_frag_t *frag = skb_shinfo(head_skb)->frags;\n\tunsigned int mss = skb_shinfo(head_skb)->gso_size;\n\tunsigned int doffset = head_skb->data - skb_mac_header(head_skb);\n\tstruct sk_buff *frag_skb = head_skb;\n\tunsigned int offset = doffset;\n\tunsigned int tnl_hlen = skb_tnl_header_len(head_skb);\n\tunsigned int headroom;\n\tunsigned int len;\n\t__be16 proto;\n\tbool csum;\n\tint sg = !!(features & NETIF_F_SG);\n\tint nfrags = skb_shinfo(head_skb)->nr_frags;\n\tint err = -ENOMEM;\n\tint i = 0;\n\tint pos;\n\n\tproto = skb_network_protocol(head_skb);\n\tif (unlikely(!proto))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcsum = !!can_checksum_protocol(features, proto);\n\t__skb_push(head_skb, doffset);\n\theadroom = skb_headroom(head_skb);\n\tpos = skb_headlen(head_skb);\n\n\tdo {\n\t\tstruct sk_buff *nskb;\n\t\tskb_frag_t *nskb_frag;\n\t\tint hsize;\n\t\tint size;\n\n\t\tlen = head_skb->len - offset;\n\t\tif (len > mss)\n\t\t\tlen = mss;\n\n\t\thsize = skb_headlen(head_skb) - offset;\n\t\tif (hsize < 0)\n\t\t\thsize = 0;\n\t\tif (hsize > len || !sg)\n\t\t\thsize = len;\n\n\t\tif (!hsize && i >= nfrags && skb_headlen(list_skb) &&\n\t\t    (skb_headlen(list_skb) == len || sg)) {\n\t\t\tBUG_ON(skb_headlen(list_skb) > len);\n\n\t\t\ti = 0;\n\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\tfrag_skb = list_skb;\n\t\t\tpos += skb_headlen(list_skb);\n\n\t\t\twhile (pos < offset + len) {\n\t\t\t\tBUG_ON(i >= nfrags);\n\n\t\t\t\tsize = skb_frag_size(frag);\n\t\t\t\tif (pos + size > offset + len)\n\t\t\t\t\tbreak;\n\n\t\t\t\ti++;\n\t\t\t\tpos += size;\n\t\t\t\tfrag++;\n\t\t\t}\n\n\t\t\tnskb = skb_clone(list_skb, GFP_ATOMIC);\n\t\t\tlist_skb = list_skb->next;\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tif (unlikely(pskb_trim(nskb, len))) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\thsize = skb_end_offset(nskb);\n\t\t\tif (skb_cow_head(nskb, doffset + headroom)) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tnskb->truesize += skb_end_offset(nskb) - hsize;\n\t\t\tskb_release_head_state(nskb);\n\t\t\t__skb_push(nskb, doffset);\n\t\t} else {\n\t\t\tnskb = __alloc_skb(hsize + doffset + headroom,\n\t\t\t\t\t   GFP_ATOMIC, skb_alloc_rx_flag(head_skb),\n\t\t\t\t\t   NUMA_NO_NODE);\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tskb_reserve(nskb, headroom);\n\t\t\t__skb_put(nskb, doffset);\n\t\t}\n\n\t\tif (segs)\n\t\t\ttail->next = nskb;\n\t\telse\n\t\t\tsegs = nskb;\n\t\ttail = nskb;\n\n\t\t__copy_skb_header(nskb, head_skb);\n\t\tnskb->mac_len = head_skb->mac_len;\n\n\t\tskb_headers_offset_update(nskb, skb_headroom(nskb) - headroom);\n\n\t\tskb_copy_from_linear_data_offset(head_skb, -tnl_hlen,\n\t\t\t\t\t\t nskb->data - tnl_hlen,\n\t\t\t\t\t\t doffset + tnl_hlen);\n\n\t\tif (nskb->len == len + doffset)\n\t\t\tgoto perform_csum_check;\n\n\t\tif (!sg) {\n\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\tnskb->csum = skb_copy_and_csum_bits(head_skb, offset,\n\t\t\t\t\t\t\t    skb_put(nskb, len),\n\t\t\t\t\t\t\t    len, 0);\n\t\t\tcontinue;\n\t\t}\n\n\t\tnskb_frag = skb_shinfo(nskb)->frags;\n\n\t\tskb_copy_from_linear_data_offset(head_skb, offset,\n\t\t\t\t\t\t skb_put(nskb, hsize), hsize);\n\n\t\tskb_shinfo(nskb)->tx_flags = skb_shinfo(head_skb)->tx_flags &\n\t\t\tSKBTX_SHARED_FRAG;\n\n\t\twhile (pos < offset + len) {\n\t\t\tif (i >= nfrags) {\n\t\t\t\tBUG_ON(skb_headlen(list_skb));\n\n\t\t\t\ti = 0;\n\t\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\t\tfrag_skb = list_skb;\n\n\t\t\t\tBUG_ON(!nfrags);\n\n\t\t\t\tlist_skb = list_skb->next;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_shinfo(nskb)->nr_frags >=\n\t\t\t\t     MAX_SKB_FRAGS)) {\n\t\t\t\tnet_warn_ratelimited(\n\t\t\t\t\t\"skb_segment: too many frags: %u %u\\n\",\n\t\t\t\t\tpos, mss);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_orphan_frags(frag_skb, GFP_ATOMIC)))\n\t\t\t\tgoto err;\n\n\t\t\t*nskb_frag = *frag;\n\t\t\t__skb_frag_ref(nskb_frag);\n\t\t\tsize = skb_frag_size(nskb_frag);\n\n\t\t\tif (pos < offset) {\n\t\t\t\tnskb_frag->page_offset += offset - pos;\n\t\t\t\tskb_frag_size_sub(nskb_frag, offset - pos);\n\t\t\t}\n\n\t\t\tskb_shinfo(nskb)->nr_frags++;\n\n\t\t\tif (pos + size <= offset + len) {\n\t\t\t\ti++;\n\t\t\t\tfrag++;\n\t\t\t\tpos += size;\n\t\t\t} else {\n\t\t\t\tskb_frag_size_sub(nskb_frag, pos + size - (offset + len));\n\t\t\t\tgoto skip_fraglist;\n\t\t\t}\n\n\t\t\tnskb_frag++;\n\t\t}\n\nskip_fraglist:\n\t\tnskb->data_len = len - hsize;\n\t\tnskb->len += nskb->data_len;\n\t\tnskb->truesize += nskb->data_len;\n\nperform_csum_check:\n\t\tif (!csum) {\n\t\t\tnskb->csum = skb_checksum(nskb, doffset,\n\t\t\t\t\t\t  nskb->len - doffset, 0);\n\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t} while ((offset += len) < head_skb->len);\n\n\treturn segs;\n\nerr:\n\tkfree_skb_list(segs);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tskb_frag_t *frag = skb_shinfo(head_skb)->frags;\n \tunsigned int mss = skb_shinfo(head_skb)->gso_size;\n \tunsigned int doffset = head_skb->data - skb_mac_header(head_skb);\n+\tstruct sk_buff *frag_skb = head_skb;\n \tunsigned int offset = doffset;\n \tunsigned int tnl_hlen = skb_tnl_header_len(head_skb);\n \tunsigned int headroom;\n@@ -51,6 +52,7 @@\n \t\t\ti = 0;\n \t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n \t\t\tfrag = skb_shinfo(list_skb)->frags;\n+\t\t\tfrag_skb = list_skb;\n \t\t\tpos += skb_headlen(list_skb);\n \n \t\t\twhile (pos < offset + len) {\n@@ -138,6 +140,7 @@\n \t\t\t\ti = 0;\n \t\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n \t\t\t\tfrag = skb_shinfo(list_skb)->frags;\n+\t\t\t\tfrag_skb = list_skb;\n \n \t\t\t\tBUG_ON(!nfrags);\n \n@@ -151,6 +154,9 @@\n \t\t\t\t\tpos, mss);\n \t\t\t\tgoto err;\n \t\t\t}\n+\n+\t\t\tif (unlikely(skb_orphan_frags(frag_skb, GFP_ATOMIC)))\n+\t\t\t\tgoto err;\n \n \t\t\t*nskb_frag = *frag;\n \t\t\t__skb_frag_ref(nskb_frag);",
        "function_modified_lines": {
            "added": [
                "\tstruct sk_buff *frag_skb = head_skb;",
                "\t\t\tfrag_skb = list_skb;",
                "\t\t\t\tfrag_skb = list_skb;",
                "",
                "\t\t\tif (unlikely(skb_orphan_frags(frag_skb, GFP_ATOMIC)))",
                "\t\t\t\tgoto err;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in the skb_segment function in net/core/skbuff.c in the Linux kernel through 3.13.6 allows attackers to obtain sensitive information from kernel memory by leveraging the absence of a certain orphaning operation.",
        "id": 431
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int\nvmw_resource_check_buffer(struct ww_acquire_ctx *ticket,\n\t\t\t  struct vmw_resource *res,\n\t\t\t  bool interruptible,\n\t\t\t  struct ttm_validate_buffer *val_buf)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct list_head val_list;\n\tbool guest_memory_dirty = false;\n\tint ret;\n\n\tif (unlikely(!res->guest_memory_bo)) {\n\t\tret = vmw_resource_buf_alloc(res, interruptible);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\t}\n\n\tINIT_LIST_HEAD(&val_list);\n\tttm_bo_get(&res->guest_memory_bo->tbo);\n\tval_buf->bo = &res->guest_memory_bo->tbo;\n\tval_buf->num_shared = 0;\n\tlist_add_tail(&val_buf->head, &val_list);\n\tret = ttm_eu_reserve_buffers(ticket, &val_list, interruptible, NULL);\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_reserve;\n\n\tif (res->func->needs_guest_memory && !vmw_resource_mob_attached(res))\n\t\treturn 0;\n\n\tguest_memory_dirty = res->guest_memory_dirty;\n\tvmw_bo_placement_set(res->guest_memory_bo, res->func->domain,\n\t\t\t     res->func->busy_domain);\n\tret = ttm_bo_validate(&res->guest_memory_bo->tbo,\n\t\t\t      &res->guest_memory_bo->placement,\n\t\t\t      &ctx);\n\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_validate;\n\n\treturn 0;\n\nout_no_validate:\n\tttm_eu_backoff_reservation(ticket, &val_list);\nout_no_reserve:\n\tttm_bo_put(val_buf->bo);\n\tval_buf->bo = NULL;\n\tif (guest_memory_dirty)\n\t\tvmw_bo_unreference(&res->guest_memory_bo);\n\n\treturn ret;\n}",
        "code_after_change": "static int\nvmw_resource_check_buffer(struct ww_acquire_ctx *ticket,\n\t\t\t  struct vmw_resource *res,\n\t\t\t  bool interruptible,\n\t\t\t  struct ttm_validate_buffer *val_buf)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct list_head val_list;\n\tbool guest_memory_dirty = false;\n\tint ret;\n\n\tif (unlikely(!res->guest_memory_bo)) {\n\t\tret = vmw_resource_buf_alloc(res, interruptible);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\t}\n\n\tINIT_LIST_HEAD(&val_list);\n\tttm_bo_get(&res->guest_memory_bo->tbo);\n\tval_buf->bo = &res->guest_memory_bo->tbo;\n\tval_buf->num_shared = 0;\n\tlist_add_tail(&val_buf->head, &val_list);\n\tret = ttm_eu_reserve_buffers(ticket, &val_list, interruptible, NULL);\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_reserve;\n\n\tif (res->func->needs_guest_memory && !vmw_resource_mob_attached(res))\n\t\treturn 0;\n\n\tguest_memory_dirty = res->guest_memory_dirty;\n\tvmw_bo_placement_set(res->guest_memory_bo, res->func->domain,\n\t\t\t     res->func->busy_domain);\n\tret = ttm_bo_validate(&res->guest_memory_bo->tbo,\n\t\t\t      &res->guest_memory_bo->placement,\n\t\t\t      &ctx);\n\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_validate;\n\n\treturn 0;\n\nout_no_validate:\n\tttm_eu_backoff_reservation(ticket, &val_list);\nout_no_reserve:\n\tttm_bo_put(val_buf->bo);\n\tval_buf->bo = NULL;\n\tif (guest_memory_dirty)\n\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -45,7 +45,7 @@\n \tttm_bo_put(val_buf->bo);\n \tval_buf->bo = NULL;\n \tif (guest_memory_dirty)\n-\t\tvmw_bo_unreference(&res->guest_memory_bo);\n+\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n \n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tvmw_user_bo_unref(&res->guest_memory_bo);"
            ],
            "deleted": [
                "\t\tvmw_bo_unreference(&res->guest_memory_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4280
    },
    {
        "cve_id": "CVE-2017-16939",
        "code_before_change": "static int xfrm_user_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *attrs[XFRMA_MAX+1];\n\tconst struct xfrm_link *link;\n\tint type, err;\n\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\treturn -EOPNOTSUPP;\n#endif\n\n\ttype = nlh->nlmsg_type;\n\tif (type > XFRM_MSG_MAX)\n\t\treturn -EINVAL;\n\n\ttype -= XFRM_MSG_BASE;\n\tlink = &xfrm_dispatch[type];\n\n\t/* All operations require privileges, even GET */\n\tif (!netlink_net_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((type == (XFRM_MSG_GETSA - XFRM_MSG_BASE) ||\n\t     type == (XFRM_MSG_GETPOLICY - XFRM_MSG_BASE)) &&\n\t    (nlh->nlmsg_flags & NLM_F_DUMP)) {\n\t\tif (link->dump == NULL)\n\t\t\treturn -EINVAL;\n\n\t\t{\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.dump = link->dump,\n\t\t\t\t.done = link->done,\n\t\t\t};\n\t\t\treturn netlink_dump_start(net->xfrm.nlsk, skb, nlh, &c);\n\t\t}\n\t}\n\n\terr = nlmsg_parse(nlh, xfrm_msg_min[type], attrs,\n\t\t\t  link->nla_max ? : XFRMA_MAX,\n\t\t\t  link->nla_pol ? : xfrma_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (link->doit == NULL)\n\t\treturn -EINVAL;\n\n\treturn link->doit(skb, nlh, attrs);\n}",
        "code_after_change": "static int xfrm_user_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *attrs[XFRMA_MAX+1];\n\tconst struct xfrm_link *link;\n\tint type, err;\n\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\treturn -EOPNOTSUPP;\n#endif\n\n\ttype = nlh->nlmsg_type;\n\tif (type > XFRM_MSG_MAX)\n\t\treturn -EINVAL;\n\n\ttype -= XFRM_MSG_BASE;\n\tlink = &xfrm_dispatch[type];\n\n\t/* All operations require privileges, even GET */\n\tif (!netlink_net_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((type == (XFRM_MSG_GETSA - XFRM_MSG_BASE) ||\n\t     type == (XFRM_MSG_GETPOLICY - XFRM_MSG_BASE)) &&\n\t    (nlh->nlmsg_flags & NLM_F_DUMP)) {\n\t\tif (link->dump == NULL)\n\t\t\treturn -EINVAL;\n\n\t\t{\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.start = link->start,\n\t\t\t\t.dump = link->dump,\n\t\t\t\t.done = link->done,\n\t\t\t};\n\t\t\treturn netlink_dump_start(net->xfrm.nlsk, skb, nlh, &c);\n\t\t}\n\t}\n\n\terr = nlmsg_parse(nlh, xfrm_msg_min[type], attrs,\n\t\t\t  link->nla_max ? : XFRMA_MAX,\n\t\t\t  link->nla_pol ? : xfrma_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (link->doit == NULL)\n\t\treturn -EINVAL;\n\n\treturn link->doit(skb, nlh, attrs);\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,6 +30,7 @@\n \n \t\t{\n \t\t\tstruct netlink_dump_control c = {\n+\t\t\t\t.start = link->start,\n \t\t\t\t.dump = link->dump,\n \t\t\t\t.done = link->done,\n \t\t\t};",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t.start = link->start,"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The XFRM dump policy implementation in net/xfrm/xfrm_user.c in the Linux kernel before 4.13.11 allows local users to gain privileges or cause a denial of service (use-after-free) via a crafted SO_RCVBUF setsockopt system call in conjunction with XFRM_MSG_GETPOLICY Netlink messages.",
        "id": 1354
    },
    {
        "cve_id": "CVE-2023-3610",
        "code_before_change": "void nft_data_hold(const struct nft_data *data, enum nft_data_types type)\n{\n\tstruct nft_chain *chain;\n\tstruct nft_rule *rule;\n\n\tif (type == NFT_DATA_VERDICT) {\n\t\tswitch (data->verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\tchain = data->verdict.chain;\n\t\t\tchain->use++;\n\n\t\t\tif (!nft_chain_is_bound(chain))\n\t\t\t\tbreak;\n\n\t\t\tchain->table->use++;\n\t\t\tlist_for_each_entry(rule, &chain->rules, list)\n\t\t\t\tchain->use++;\n\n\t\t\tnft_chain_add(chain->table, chain);\n\t\t\tbreak;\n\t\t}\n\t}\n}",
        "code_after_change": "void nft_data_hold(const struct nft_data *data, enum nft_data_types type)\n{\n\tstruct nft_chain *chain;\n\n\tif (type == NFT_DATA_VERDICT) {\n\t\tswitch (data->verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\tchain = data->verdict.chain;\n\t\t\tchain->use++;\n\t\t\tbreak;\n\t\t}\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,6 @@\n void nft_data_hold(const struct nft_data *data, enum nft_data_types type)\n {\n \tstruct nft_chain *chain;\n-\tstruct nft_rule *rule;\n \n \tif (type == NFT_DATA_VERDICT) {\n \t\tswitch (data->verdict.code) {\n@@ -9,15 +8,6 @@\n \t\tcase NFT_GOTO:\n \t\t\tchain = data->verdict.chain;\n \t\t\tchain->use++;\n-\n-\t\t\tif (!nft_chain_is_bound(chain))\n-\t\t\t\tbreak;\n-\n-\t\t\tchain->table->use++;\n-\t\t\tlist_for_each_entry(rule, &chain->rules, list)\n-\t\t\t\tchain->use++;\n-\n-\t\t\tnft_chain_add(chain->table, chain);\n \t\t\tbreak;\n \t\t}\n \t}",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct nft_rule *rule;",
                "",
                "\t\t\tif (!nft_chain_is_bound(chain))",
                "\t\t\t\tbreak;",
                "",
                "\t\t\tchain->table->use++;",
                "\t\t\tlist_for_each_entry(rule, &chain->rules, list)",
                "\t\t\t\tchain->use++;",
                "",
                "\t\t\tnft_chain_add(chain->table, chain);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nFlaw in the error handling of bound chains causes a use-after-free in the abort path of NFT_MSG_NEWRULE. The vulnerability requires CAP_NET_ADMIN to be triggered.\n\nWe recommend upgrading past commit 4bedf9eee016286c835e3d8fa981ddece5338795.\n\n",
        "id": 4121
    },
    {
        "cve_id": "CVE-2017-15265",
        "code_before_change": "static int snd_seq_ioctl_create_port(struct snd_seq_client *client, void *arg)\n{\n\tstruct snd_seq_port_info *info = arg;\n\tstruct snd_seq_client_port *port;\n\tstruct snd_seq_port_callback *callback;\n\n\t/* it is not allowed to create the port for an another client */\n\tif (info->addr.client != client->number)\n\t\treturn -EPERM;\n\n\tport = snd_seq_create_port(client, (info->flags & SNDRV_SEQ_PORT_FLG_GIVEN_PORT) ? info->addr.port : -1);\n\tif (port == NULL)\n\t\treturn -ENOMEM;\n\n\tif (client->type == USER_CLIENT && info->kernel) {\n\t\tsnd_seq_delete_port(client, port->addr.port);\n\t\treturn -EINVAL;\n\t}\n\tif (client->type == KERNEL_CLIENT) {\n\t\tif ((callback = info->kernel) != NULL) {\n\t\t\tif (callback->owner)\n\t\t\t\tport->owner = callback->owner;\n\t\t\tport->private_data = callback->private_data;\n\t\t\tport->private_free = callback->private_free;\n\t\t\tport->event_input = callback->event_input;\n\t\t\tport->c_src.open = callback->subscribe;\n\t\t\tport->c_src.close = callback->unsubscribe;\n\t\t\tport->c_dest.open = callback->use;\n\t\t\tport->c_dest.close = callback->unuse;\n\t\t}\n\t}\n\n\tinfo->addr = port->addr;\n\n\tsnd_seq_set_port_info(port, info);\n\tsnd_seq_system_client_ev_port_start(port->addr.client, port->addr.port);\n\n\treturn 0;\n}",
        "code_after_change": "static int snd_seq_ioctl_create_port(struct snd_seq_client *client, void *arg)\n{\n\tstruct snd_seq_port_info *info = arg;\n\tstruct snd_seq_client_port *port;\n\tstruct snd_seq_port_callback *callback;\n\tint port_idx;\n\n\t/* it is not allowed to create the port for an another client */\n\tif (info->addr.client != client->number)\n\t\treturn -EPERM;\n\n\tport = snd_seq_create_port(client, (info->flags & SNDRV_SEQ_PORT_FLG_GIVEN_PORT) ? info->addr.port : -1);\n\tif (port == NULL)\n\t\treturn -ENOMEM;\n\n\tif (client->type == USER_CLIENT && info->kernel) {\n\t\tport_idx = port->addr.port;\n\t\tsnd_seq_port_unlock(port);\n\t\tsnd_seq_delete_port(client, port_idx);\n\t\treturn -EINVAL;\n\t}\n\tif (client->type == KERNEL_CLIENT) {\n\t\tif ((callback = info->kernel) != NULL) {\n\t\t\tif (callback->owner)\n\t\t\t\tport->owner = callback->owner;\n\t\t\tport->private_data = callback->private_data;\n\t\t\tport->private_free = callback->private_free;\n\t\t\tport->event_input = callback->event_input;\n\t\t\tport->c_src.open = callback->subscribe;\n\t\t\tport->c_src.close = callback->unsubscribe;\n\t\t\tport->c_dest.open = callback->use;\n\t\t\tport->c_dest.close = callback->unuse;\n\t\t}\n\t}\n\n\tinfo->addr = port->addr;\n\n\tsnd_seq_set_port_info(port, info);\n\tsnd_seq_system_client_ev_port_start(port->addr.client, port->addr.port);\n\tsnd_seq_port_unlock(port);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n \tstruct snd_seq_port_info *info = arg;\n \tstruct snd_seq_client_port *port;\n \tstruct snd_seq_port_callback *callback;\n+\tint port_idx;\n \n \t/* it is not allowed to create the port for an another client */\n \tif (info->addr.client != client->number)\n@@ -13,7 +14,9 @@\n \t\treturn -ENOMEM;\n \n \tif (client->type == USER_CLIENT && info->kernel) {\n-\t\tsnd_seq_delete_port(client, port->addr.port);\n+\t\tport_idx = port->addr.port;\n+\t\tsnd_seq_port_unlock(port);\n+\t\tsnd_seq_delete_port(client, port_idx);\n \t\treturn -EINVAL;\n \t}\n \tif (client->type == KERNEL_CLIENT) {\n@@ -34,6 +37,7 @@\n \n \tsnd_seq_set_port_info(port, info);\n \tsnd_seq_system_client_ev_port_start(port->addr.client, port->addr.port);\n+\tsnd_seq_port_unlock(port);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tint port_idx;",
                "\t\tport_idx = port->addr.port;",
                "\t\tsnd_seq_port_unlock(port);",
                "\t\tsnd_seq_delete_port(client, port_idx);",
                "\tsnd_seq_port_unlock(port);"
            ],
            "deleted": [
                "\t\tsnd_seq_delete_port(client, port->addr.port);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ALSA subsystem in the Linux kernel before 4.13.8 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via crafted /dev/snd/seq ioctl calls, related to sound/core/seq/seq_clientmgr.c and sound/core/seq/seq_ports.c.",
        "id": 1300
    },
    {
        "cve_id": "CVE-2022-47946",
        "code_before_change": "static int io_sq_thread_fork(struct io_sq_data *sqd, struct io_ring_ctx *ctx)\n{\n\tint ret;\n\n\tclear_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n\treinit_completion(&sqd->completion);\n\tctx->sqo_dead = ctx->sqo_exec = 0;\n\tsqd->task_pid = current->pid;\n\tcurrent->flags |= PF_IO_WORKER;\n\tret = io_wq_fork_thread(io_sq_thread, sqd);\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (ret < 0) {\n\t\tsqd->thread = NULL;\n\t\treturn ret;\n\t}\n\twait_for_completion(&sqd->completion);\n\treturn io_uring_alloc_task_context(sqd->thread, ctx);\n}",
        "code_after_change": "static int io_sq_thread_fork(struct io_sq_data *sqd, struct io_ring_ctx *ctx)\n{\n\tint ret;\n\n\tclear_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n\treinit_completion(&sqd->completion);\n\tctx->sqo_exec = 0;\n\tsqd->task_pid = current->pid;\n\tcurrent->flags |= PF_IO_WORKER;\n\tret = io_wq_fork_thread(io_sq_thread, sqd);\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (ret < 0) {\n\t\tsqd->thread = NULL;\n\t\treturn ret;\n\t}\n\twait_for_completion(&sqd->completion);\n\treturn io_uring_alloc_task_context(sqd->thread, ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \n \tclear_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n \treinit_completion(&sqd->completion);\n-\tctx->sqo_dead = ctx->sqo_exec = 0;\n+\tctx->sqo_exec = 0;\n \tsqd->task_pid = current->pid;\n \tcurrent->flags |= PF_IO_WORKER;\n \tret = io_wq_fork_thread(io_sq_thread, sqd);",
        "function_modified_lines": {
            "added": [
                "\tctx->sqo_exec = 0;"
            ],
            "deleted": [
                "\tctx->sqo_dead = ctx->sqo_exec = 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 5.10.x before 5.10.155. A use-after-free in io_sqpoll_wait_sq in fs/io_uring.c allows an attacker to crash the kernel, resulting in denial of service. finish_wait can be skipped. An attack can occur in some situations by forking a process and then quickly terminating it. NOTE: later kernel versions, such as the 5.15 longterm series, substantially changed the implementation of io_sqpoll_wait_sq.",
        "id": 3781
    },
    {
        "cve_id": "CVE-2022-42703",
        "code_before_change": "int __anon_vma_prepare(struct vm_area_struct *vma)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct anon_vma *anon_vma, *allocated;\n\tstruct anon_vma_chain *avc;\n\n\tmight_sleep();\n\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_enomem;\n\n\tanon_vma = find_mergeable_anon_vma(vma);\n\tallocated = NULL;\n\tif (!anon_vma) {\n\t\tanon_vma = anon_vma_alloc();\n\t\tif (unlikely(!anon_vma))\n\t\t\tgoto out_enomem_free_avc;\n\t\tallocated = anon_vma;\n\t}\n\n\tanon_vma_lock_write(anon_vma);\n\t/* page_table_lock to protect against threads */\n\tspin_lock(&mm->page_table_lock);\n\tif (likely(!vma->anon_vma)) {\n\t\tvma->anon_vma = anon_vma;\n\t\tanon_vma_chain_link(vma, avc, anon_vma);\n\t\t/* vma reference or self-parent link for new root */\n\t\tanon_vma->degree++;\n\t\tallocated = NULL;\n\t\tavc = NULL;\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\tanon_vma_unlock_write(anon_vma);\n\n\tif (unlikely(allocated))\n\t\tput_anon_vma(allocated);\n\tif (unlikely(avc))\n\t\tanon_vma_chain_free(avc);\n\n\treturn 0;\n\n out_enomem_free_avc:\n\tanon_vma_chain_free(avc);\n out_enomem:\n\treturn -ENOMEM;\n}",
        "code_after_change": "int __anon_vma_prepare(struct vm_area_struct *vma)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct anon_vma *anon_vma, *allocated;\n\tstruct anon_vma_chain *avc;\n\n\tmight_sleep();\n\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_enomem;\n\n\tanon_vma = find_mergeable_anon_vma(vma);\n\tallocated = NULL;\n\tif (!anon_vma) {\n\t\tanon_vma = anon_vma_alloc();\n\t\tif (unlikely(!anon_vma))\n\t\t\tgoto out_enomem_free_avc;\n\t\tanon_vma->num_children++; /* self-parent link for new root */\n\t\tallocated = anon_vma;\n\t}\n\n\tanon_vma_lock_write(anon_vma);\n\t/* page_table_lock to protect against threads */\n\tspin_lock(&mm->page_table_lock);\n\tif (likely(!vma->anon_vma)) {\n\t\tvma->anon_vma = anon_vma;\n\t\tanon_vma_chain_link(vma, avc, anon_vma);\n\t\tanon_vma->num_active_vmas++;\n\t\tallocated = NULL;\n\t\tavc = NULL;\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\tanon_vma_unlock_write(anon_vma);\n\n\tif (unlikely(allocated))\n\t\tput_anon_vma(allocated);\n\tif (unlikely(avc))\n\t\tanon_vma_chain_free(avc);\n\n\treturn 0;\n\n out_enomem_free_avc:\n\tanon_vma_chain_free(avc);\n out_enomem:\n\treturn -ENOMEM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,7 @@\n \t\tanon_vma = anon_vma_alloc();\n \t\tif (unlikely(!anon_vma))\n \t\t\tgoto out_enomem_free_avc;\n+\t\tanon_vma->num_children++; /* self-parent link for new root */\n \t\tallocated = anon_vma;\n \t}\n \n@@ -25,8 +26,7 @@\n \tif (likely(!vma->anon_vma)) {\n \t\tvma->anon_vma = anon_vma;\n \t\tanon_vma_chain_link(vma, avc, anon_vma);\n-\t\t/* vma reference or self-parent link for new root */\n-\t\tanon_vma->degree++;\n+\t\tanon_vma->num_active_vmas++;\n \t\tallocated = NULL;\n \t\tavc = NULL;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tanon_vma->num_children++; /* self-parent link for new root */",
                "\t\tanon_vma->num_active_vmas++;"
            ],
            "deleted": [
                "\t\t/* vma reference or self-parent link for new root */",
                "\t\tanon_vma->degree++;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "mm/rmap.c in the Linux kernel before 5.19.7 has a use-after-free related to leaf anon_vma double reuse.",
        "id": 3729
    },
    {
        "cve_id": "CVE-2022-1882",
        "code_before_change": "int remove_watch_from_object(struct watch_list *wlist, struct watch_queue *wq,\n\t\t\t     u64 id, bool all)\n{\n\tstruct watch_notification_removal n;\n\tstruct watch_queue *wqueue;\n\tstruct watch *watch;\n\tint ret = -EBADSLT;\n\n\trcu_read_lock();\n\nagain:\n\tspin_lock(&wlist->lock);\n\thlist_for_each_entry(watch, &wlist->watchers, list_node) {\n\t\tif (all ||\n\t\t    (watch->id == id && rcu_access_pointer(watch->queue) == wq))\n\t\t\tgoto found;\n\t}\n\tspin_unlock(&wlist->lock);\n\tgoto out;\n\nfound:\n\tret = 0;\n\thlist_del_init_rcu(&watch->list_node);\n\trcu_assign_pointer(watch->watch_list, NULL);\n\tspin_unlock(&wlist->lock);\n\n\t/* We now own the reference on watch that used to belong to wlist. */\n\n\tn.watch.type = WATCH_TYPE_META;\n\tn.watch.subtype = WATCH_META_REMOVAL_NOTIFICATION;\n\tn.watch.info = watch->info_id | watch_sizeof(n.watch);\n\tn.id = id;\n\tif (id != 0)\n\t\tn.watch.info = watch->info_id | watch_sizeof(n);\n\n\twqueue = rcu_dereference(watch->queue);\n\n\t/* We don't need the watch list lock for the next bit as RCU is\n\t * protecting *wqueue from deallocation.\n\t */\n\tif (wqueue) {\n\t\tpost_one_notification(wqueue, &n.watch);\n\n\t\tspin_lock_bh(&wqueue->lock);\n\n\t\tif (!hlist_unhashed(&watch->queue_node)) {\n\t\t\thlist_del_init_rcu(&watch->queue_node);\n\t\t\tput_watch(watch);\n\t\t}\n\n\t\tspin_unlock_bh(&wqueue->lock);\n\t}\n\n\tif (wlist->release_watch) {\n\t\tvoid (*release_watch)(struct watch *);\n\n\t\trelease_watch = wlist->release_watch;\n\t\trcu_read_unlock();\n\t\t(*release_watch)(watch);\n\t\trcu_read_lock();\n\t}\n\tput_watch(watch);\n\n\tif (all && !hlist_empty(&wlist->watchers))\n\t\tgoto again;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}",
        "code_after_change": "int remove_watch_from_object(struct watch_list *wlist, struct watch_queue *wq,\n\t\t\t     u64 id, bool all)\n{\n\tstruct watch_notification_removal n;\n\tstruct watch_queue *wqueue;\n\tstruct watch *watch;\n\tint ret = -EBADSLT;\n\n\trcu_read_lock();\n\nagain:\n\tspin_lock(&wlist->lock);\n\thlist_for_each_entry(watch, &wlist->watchers, list_node) {\n\t\tif (all ||\n\t\t    (watch->id == id && rcu_access_pointer(watch->queue) == wq))\n\t\t\tgoto found;\n\t}\n\tspin_unlock(&wlist->lock);\n\tgoto out;\n\nfound:\n\tret = 0;\n\thlist_del_init_rcu(&watch->list_node);\n\trcu_assign_pointer(watch->watch_list, NULL);\n\tspin_unlock(&wlist->lock);\n\n\t/* We now own the reference on watch that used to belong to wlist. */\n\n\tn.watch.type = WATCH_TYPE_META;\n\tn.watch.subtype = WATCH_META_REMOVAL_NOTIFICATION;\n\tn.watch.info = watch->info_id | watch_sizeof(n.watch);\n\tn.id = id;\n\tif (id != 0)\n\t\tn.watch.info = watch->info_id | watch_sizeof(n);\n\n\twqueue = rcu_dereference(watch->queue);\n\n\tif (lock_wqueue(wqueue)) {\n\t\tpost_one_notification(wqueue, &n.watch);\n\n\t\tif (!hlist_unhashed(&watch->queue_node)) {\n\t\t\thlist_del_init_rcu(&watch->queue_node);\n\t\t\tput_watch(watch);\n\t\t}\n\n\t\tunlock_wqueue(wqueue);\n\t}\n\n\tif (wlist->release_watch) {\n\t\tvoid (*release_watch)(struct watch *);\n\n\t\trelease_watch = wlist->release_watch;\n\t\trcu_read_unlock();\n\t\t(*release_watch)(watch);\n\t\trcu_read_lock();\n\t}\n\tput_watch(watch);\n\n\tif (all && !hlist_empty(&wlist->watchers))\n\t\tgoto again;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -35,20 +35,15 @@\n \n \twqueue = rcu_dereference(watch->queue);\n \n-\t/* We don't need the watch list lock for the next bit as RCU is\n-\t * protecting *wqueue from deallocation.\n-\t */\n-\tif (wqueue) {\n+\tif (lock_wqueue(wqueue)) {\n \t\tpost_one_notification(wqueue, &n.watch);\n-\n-\t\tspin_lock_bh(&wqueue->lock);\n \n \t\tif (!hlist_unhashed(&watch->queue_node)) {\n \t\t\thlist_del_init_rcu(&watch->queue_node);\n \t\t\tput_watch(watch);\n \t\t}\n \n-\t\tspin_unlock_bh(&wqueue->lock);\n+\t\tunlock_wqueue(wqueue);\n \t}\n \n \tif (wlist->release_watch) {",
        "function_modified_lines": {
            "added": [
                "\tif (lock_wqueue(wqueue)) {",
                "\t\tunlock_wqueue(wqueue);"
            ],
            "deleted": [
                "\t/* We don't need the watch list lock for the next bit as RCU is",
                "\t * protecting *wqueue from deallocation.",
                "\t */",
                "\tif (wqueue) {",
                "",
                "\t\tspin_lock_bh(&wqueue->lock);",
                "\t\tspin_unlock_bh(&wqueue->lock);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s pipes functionality in how a user performs manipulations with the pipe post_one_notification() after free_pipe_info() that is already called. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3299
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t struct request_sock *req,\n\t\t\t\t\t struct dst_entry *dst,\n\t\t\t\t\t struct request_sock *req_unhash,\n\t\t\t\t\t bool *own_req)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp6_sock *newtcp6sk;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\tstruct flowi6 fl6;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t     req_unhash, own_req);\n\n\t\tif (!newsk)\n\t\t\treturn NULL;\n\n\t\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\t\tnewinet = inet_sk(newsk);\n\t\tnewnp = inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->ipv6_ac_list = NULL;\n\t\tnewnp->ipv6_fl_list = NULL;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = tcp_v6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\t\tnewnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(skb));\n\t\tif (np->repflow)\n\t\t\tnewnp->flow_label = ip6_flowlabel(ipv6_hdr(skb));\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\tireq = inet_rsk(req);\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_TCP);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (!newsk)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tinet6_sk_rx_dst_set(newsk, skb);\n\n\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tnewsk->sk_v6_daddr = ireq->ir_v6_rmt_addr;\n\tnewnp->saddr = ireq->ir_v6_loc_addr;\n\tnewsk->sk_v6_rcv_saddr = ireq->ir_v6_loc_addr;\n\tnewsk->sk_bound_dev_if = ireq->ir_iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\tnewnp->ipv6_ac_list = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = tcp_v6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\tnewnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(skb));\n\tif (np->repflow)\n\t\tnewnp->flow_label = ip6_flowlabel(ipv6_hdr(skb));\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\tif (np->opt)\n\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\ttcp_ca_openreq_child(newsk, dst);\n\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\tif (tcp_sk(sk)->rx_opt.user_mss &&\n\t    tcp_sk(sk)->rx_opt.user_mss < newtp->advmss)\n\t\tnewtp->advmss = tcp_sk(sk)->rx_opt.user_mss;\n\n\ttcp_initialize_rcv_mss(newsk);\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tkey = tcp_v6_md5_do_lookup(sk, &newsk->sk_v6_daddr);\n\tif (key) {\n\t\t/* We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\ttcp_md5_do_add(newsk, (union tcp_md5_addr *)&newsk->sk_v6_daddr,\n\t\t\t       AF_INET6, key->key, key->keylen,\n\t\t\t       sk_gfp_atomic(sk, GFP_ATOMIC));\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tinet_csk_prepare_forced_close(newsk);\n\t\ttcp_done(newsk);\n\t\tgoto out;\n\t}\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));\n\tif (*own_req) {\n\t\ttcp_move_syn(newtp, req);\n\n\t\t/* Clone pktoptions received with SYN, if we own the req */\n\t\tif (ireq->pktopts) {\n\t\t\tnewnp->pktoptions = skb_clone(ireq->pktopts,\n\t\t\t\t\t\t      sk_gfp_atomic(sk, GFP_ATOMIC));\n\t\t\tconsume_skb(ireq->pktopts);\n\t\t\tireq->pktopts = NULL;\n\t\t\tif (newnp->pktoptions)\n\t\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t\t}\n\t}\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "code_after_change": "static struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t struct request_sock *req,\n\t\t\t\t\t struct dst_entry *dst,\n\t\t\t\t\t struct request_sock *req_unhash,\n\t\t\t\t\t bool *own_req)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct tcp6_sock *newtcp6sk;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\tstruct flowi6 fl6;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t     req_unhash, own_req);\n\n\t\tif (!newsk)\n\t\t\treturn NULL;\n\n\t\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\t\tnewinet = inet_sk(newsk);\n\t\tnewnp = inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->ipv6_ac_list = NULL;\n\t\tnewnp->ipv6_fl_list = NULL;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = tcp_v6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\t\tnewnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(skb));\n\t\tif (np->repflow)\n\t\t\tnewnp->flow_label = ip6_flowlabel(ipv6_hdr(skb));\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\tireq = inet_rsk(req);\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_TCP);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (!newsk)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tinet6_sk_rx_dst_set(newsk, skb);\n\n\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tnewsk->sk_v6_daddr = ireq->ir_v6_rmt_addr;\n\tnewnp->saddr = ireq->ir_v6_loc_addr;\n\tnewsk->sk_v6_rcv_saddr = ireq->ir_v6_loc_addr;\n\tnewsk->sk_bound_dev_if = ireq->ir_iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\tnewnp->ipv6_ac_list = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = tcp_v6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\tnewnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(skb));\n\tif (np->repflow)\n\t\tnewnp->flow_label = ip6_flowlabel(ipv6_hdr(skb));\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\topt = rcu_dereference(np->opt);\n\tif (opt) {\n\t\topt = ipv6_dup_options(newsk, opt);\n\t\tRCU_INIT_POINTER(newnp->opt, opt);\n\t}\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n\t\t\t\t\t\t    opt->opt_flen;\n\n\ttcp_ca_openreq_child(newsk, dst);\n\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\tif (tcp_sk(sk)->rx_opt.user_mss &&\n\t    tcp_sk(sk)->rx_opt.user_mss < newtp->advmss)\n\t\tnewtp->advmss = tcp_sk(sk)->rx_opt.user_mss;\n\n\ttcp_initialize_rcv_mss(newsk);\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tkey = tcp_v6_md5_do_lookup(sk, &newsk->sk_v6_daddr);\n\tif (key) {\n\t\t/* We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\ttcp_md5_do_add(newsk, (union tcp_md5_addr *)&newsk->sk_v6_daddr,\n\t\t\t       AF_INET6, key->key, key->keylen,\n\t\t\t       sk_gfp_atomic(sk, GFP_ATOMIC));\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tinet_csk_prepare_forced_close(newsk);\n\t\ttcp_done(newsk);\n\t\tgoto out;\n\t}\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));\n\tif (*own_req) {\n\t\ttcp_move_syn(newtp, req);\n\n\t\t/* Clone pktoptions received with SYN, if we own the req */\n\t\tif (ireq->pktopts) {\n\t\t\tnewnp->pktoptions = skb_clone(ireq->pktopts,\n\t\t\t\t\t\t      sk_gfp_atomic(sk, GFP_ATOMIC));\n\t\t\tconsume_skb(ireq->pktopts);\n\t\t\tireq->pktopts = NULL;\n\t\t\tif (newnp->pktoptions)\n\t\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t\t}\n\t}\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tstruct inet_request_sock *ireq;\n \tstruct ipv6_pinfo *newnp;\n \tconst struct ipv6_pinfo *np = inet6_sk(sk);\n+\tstruct ipv6_txoptions *opt;\n \tstruct tcp6_sock *newtcp6sk;\n \tstruct inet_sock *newinet;\n \tstruct tcp_sock *newtp;\n@@ -133,13 +134,15 @@\n \t   but we make one more one thing there: reattach optmem\n \t   to newsk.\n \t */\n-\tif (np->opt)\n-\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);\n-\n+\topt = rcu_dereference(np->opt);\n+\tif (opt) {\n+\t\topt = ipv6_dup_options(newsk, opt);\n+\t\tRCU_INIT_POINTER(newnp->opt, opt);\n+\t}\n \tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n-\tif (newnp->opt)\n-\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n-\t\t\t\t\t\t     newnp->opt->opt_flen);\n+\tif (opt)\n+\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n+\t\t\t\t\t\t    opt->opt_flen;\n \n \ttcp_ca_openreq_child(newsk, dst);\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt;",
                "\topt = rcu_dereference(np->opt);",
                "\tif (opt) {",
                "\t\topt = ipv6_dup_options(newsk, opt);",
                "\t\tRCU_INIT_POINTER(newnp->opt, opt);",
                "\t}",
                "\tif (opt)",
                "\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +",
                "\t\t\t\t\t\t    opt->opt_flen;"
            ],
            "deleted": [
                "\tif (np->opt)",
                "\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);",
                "",
                "\tif (newnp->opt)",
                "\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +",
                "\t\t\t\t\t\t     newnp->opt->opt_flen);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1006
    },
    {
        "cve_id": "CVE-2018-10879",
        "code_before_change": "static int ext4_check_descriptors(struct super_block *sb,\n\t\t\t\t  ext4_fsblk_t sb_block,\n\t\t\t\t  ext4_group_t *first_not_zeroed)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t first_block = le32_to_cpu(sbi->s_es->s_first_data_block);\n\text4_fsblk_t last_block;\n\text4_fsblk_t block_bitmap;\n\text4_fsblk_t inode_bitmap;\n\text4_fsblk_t inode_table;\n\tint flexbg_flag = 0;\n\text4_group_t i, grp = sbi->s_groups_count;\n\n\tif (ext4_has_feature_flex_bg(sb))\n\t\tflexbg_flag = 1;\n\n\text4_debug(\"Checking group descriptors\");\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tif (i == sbi->s_groups_count - 1 || flexbg_flag)\n\t\t\tlast_block = ext4_blocks_count(sbi->s_es) - 1;\n\t\telse\n\t\t\tlast_block = first_block +\n\t\t\t\t(EXT4_BLOCKS_PER_GROUP(sb) - 1);\n\n\t\tif ((grp == sbi->s_groups_count) &&\n\t\t   !(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n\t\t\tgrp = i;\n\n\t\tblock_bitmap = ext4_block_bitmap(sb, gdp);\n\t\tif (block_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Block bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (block_bitmap < first_block || block_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Block bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, block_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_bitmap = ext4_inode_bitmap(sb, gdp);\n\t\tif (inode_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_bitmap < first_block || inode_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_table = ext4_inode_table(sb, gdp);\n\t\tif (inode_table == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode table for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_table < first_block ||\n\t\t    inode_table + sbi->s_itb_per_group - 1 > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode table for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_table);\n\t\t\treturn 0;\n\t\t}\n\t\text4_lock_group(sb, i);\n\t\tif (!ext4_group_desc_csum_verify(sb, i, gdp)) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Checksum for group %u failed (%u!=%u)\",\n\t\t\t\t i, le16_to_cpu(ext4_group_desc_csum(sb, i,\n\t\t\t\t     gdp)), le16_to_cpu(gdp->bg_checksum));\n\t\t\tif (!sb_rdonly(sb)) {\n\t\t\t\text4_unlock_group(sb, i);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, i);\n\t\tif (!flexbg_flag)\n\t\t\tfirst_block += EXT4_BLOCKS_PER_GROUP(sb);\n\t}\n\tif (NULL != first_not_zeroed)\n\t\t*first_not_zeroed = grp;\n\treturn 1;\n}",
        "code_after_change": "static int ext4_check_descriptors(struct super_block *sb,\n\t\t\t\t  ext4_fsblk_t sb_block,\n\t\t\t\t  ext4_group_t *first_not_zeroed)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t first_block = le32_to_cpu(sbi->s_es->s_first_data_block);\n\text4_fsblk_t last_block;\n\text4_fsblk_t last_bg_block = sb_block + ext4_bg_num_gdb(sb, 0) + 1;\n\text4_fsblk_t block_bitmap;\n\text4_fsblk_t inode_bitmap;\n\text4_fsblk_t inode_table;\n\tint flexbg_flag = 0;\n\text4_group_t i, grp = sbi->s_groups_count;\n\n\tif (ext4_has_feature_flex_bg(sb))\n\t\tflexbg_flag = 1;\n\n\text4_debug(\"Checking group descriptors\");\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tif (i == sbi->s_groups_count - 1 || flexbg_flag)\n\t\t\tlast_block = ext4_blocks_count(sbi->s_es) - 1;\n\t\telse\n\t\t\tlast_block = first_block +\n\t\t\t\t(EXT4_BLOCKS_PER_GROUP(sb) - 1);\n\n\t\tif ((grp == sbi->s_groups_count) &&\n\t\t   !(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n\t\t\tgrp = i;\n\n\t\tblock_bitmap = ext4_block_bitmap(sb, gdp);\n\t\tif (block_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Block bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (block_bitmap >= sb_block + 1 &&\n\t\t    block_bitmap <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Block bitmap for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (block_bitmap < first_block || block_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Block bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, block_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_bitmap = ext4_inode_bitmap(sb, gdp);\n\t\tif (inode_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_bitmap >= sb_block + 1 &&\n\t\t    inode_bitmap <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode bitmap for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_bitmap < first_block || inode_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_table = ext4_inode_table(sb, gdp);\n\t\tif (inode_table == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode table for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_table >= sb_block + 1 &&\n\t\t    inode_table <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode table for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_table < first_block ||\n\t\t    inode_table + sbi->s_itb_per_group - 1 > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode table for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_table);\n\t\t\treturn 0;\n\t\t}\n\t\text4_lock_group(sb, i);\n\t\tif (!ext4_group_desc_csum_verify(sb, i, gdp)) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Checksum for group %u failed (%u!=%u)\",\n\t\t\t\t i, le16_to_cpu(ext4_group_desc_csum(sb, i,\n\t\t\t\t     gdp)), le16_to_cpu(gdp->bg_checksum));\n\t\t\tif (!sb_rdonly(sb)) {\n\t\t\t\text4_unlock_group(sb, i);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, i);\n\t\tif (!flexbg_flag)\n\t\t\tfirst_block += EXT4_BLOCKS_PER_GROUP(sb);\n\t}\n\tif (NULL != first_not_zeroed)\n\t\t*first_not_zeroed = grp;\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n \text4_fsblk_t first_block = le32_to_cpu(sbi->s_es->s_first_data_block);\n \text4_fsblk_t last_block;\n+\text4_fsblk_t last_bg_block = sb_block + ext4_bg_num_gdb(sb, 0) + 1;\n \text4_fsblk_t block_bitmap;\n \text4_fsblk_t inode_bitmap;\n \text4_fsblk_t inode_table;\n@@ -37,6 +38,14 @@\n \t\t\tif (!sb_rdonly(sb))\n \t\t\t\treturn 0;\n \t\t}\n+\t\tif (block_bitmap >= sb_block + 1 &&\n+\t\t    block_bitmap <= last_bg_block) {\n+\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n+\t\t\t\t \"Block bitmap for group %u overlaps \"\n+\t\t\t\t \"block group descriptors\", i);\n+\t\t\tif (!sb_rdonly(sb))\n+\t\t\t\treturn 0;\n+\t\t}\n \t\tif (block_bitmap < first_block || block_bitmap > last_block) {\n \t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n \t\t\t       \"Block bitmap for group %u not in group \"\n@@ -51,6 +60,14 @@\n \t\t\tif (!sb_rdonly(sb))\n \t\t\t\treturn 0;\n \t\t}\n+\t\tif (inode_bitmap >= sb_block + 1 &&\n+\t\t    inode_bitmap <= last_bg_block) {\n+\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n+\t\t\t\t \"Inode bitmap for group %u overlaps \"\n+\t\t\t\t \"block group descriptors\", i);\n+\t\t\tif (!sb_rdonly(sb))\n+\t\t\t\treturn 0;\n+\t\t}\n \t\tif (inode_bitmap < first_block || inode_bitmap > last_block) {\n \t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n \t\t\t       \"Inode bitmap for group %u not in group \"\n@@ -62,6 +79,14 @@\n \t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n \t\t\t\t \"Inode table for group %u overlaps \"\n \t\t\t\t \"superblock\", i);\n+\t\t\tif (!sb_rdonly(sb))\n+\t\t\t\treturn 0;\n+\t\t}\n+\t\tif (inode_table >= sb_block + 1 &&\n+\t\t    inode_table <= last_bg_block) {\n+\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n+\t\t\t\t \"Inode table for group %u overlaps \"\n+\t\t\t\t \"block group descriptors\", i);\n \t\t\tif (!sb_rdonly(sb))\n \t\t\t\treturn 0;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\text4_fsblk_t last_bg_block = sb_block + ext4_bg_num_gdb(sb, 0) + 1;",
                "\t\tif (block_bitmap >= sb_block + 1 &&",
                "\t\t    block_bitmap <= last_bg_block) {",
                "\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"",
                "\t\t\t\t \"Block bitmap for group %u overlaps \"",
                "\t\t\t\t \"block group descriptors\", i);",
                "\t\t\tif (!sb_rdonly(sb))",
                "\t\t\t\treturn 0;",
                "\t\t}",
                "\t\tif (inode_bitmap >= sb_block + 1 &&",
                "\t\t    inode_bitmap <= last_bg_block) {",
                "\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"",
                "\t\t\t\t \"Inode bitmap for group %u overlaps \"",
                "\t\t\t\t \"block group descriptors\", i);",
                "\t\t\tif (!sb_rdonly(sb))",
                "\t\t\t\treturn 0;",
                "\t\t}",
                "\t\t\tif (!sb_rdonly(sb))",
                "\t\t\t\treturn 0;",
                "\t\t}",
                "\t\tif (inode_table >= sb_block + 1 &&",
                "\t\t    inode_table <= last_bg_block) {",
                "\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"",
                "\t\t\t\t \"Inode table for group %u overlaps \"",
                "\t\t\t\t \"block group descriptors\", i);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's ext4 filesystem. A local user can cause a use-after-free in ext4_xattr_set_entry function and a denial of service or unspecified other impact may occur by renaming a file in a crafted ext4 filesystem image.",
        "id": 1612
    },
    {
        "cve_id": "CVE-2023-2513",
        "code_before_change": "int ext4_xattr_ibody_find(struct inode *inode, struct ext4_xattr_info *i,\n\t\t\t  struct ext4_xattr_ibody_find *is)\n{\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_inode *raw_inode;\n\tint error;\n\n\tif (EXT4_I(inode)->i_extra_isize == 0)\n\t\treturn 0;\n\traw_inode = ext4_raw_inode(&is->iloc);\n\theader = IHDR(inode, raw_inode);\n\tis->s.base = is->s.first = IFIRST(header);\n\tis->s.here = is->s.first;\n\tis->s.end = (void *)raw_inode + EXT4_SB(inode->i_sb)->s_inode_size;\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\terror = xattr_check_inode(inode, header, is->s.end);\n\t\tif (error)\n\t\t\treturn error;\n\t\t/* Find the named attribute. */\n\t\terror = xattr_find_entry(inode, &is->s.here, is->s.end,\n\t\t\t\t\t i->name_index, i->name, 0);\n\t\tif (error && error != -ENODATA)\n\t\t\treturn error;\n\t\tis->s.not_found = error;\n\t}\n\treturn 0;\n}",
        "code_after_change": "int ext4_xattr_ibody_find(struct inode *inode, struct ext4_xattr_info *i,\n\t\t\t  struct ext4_xattr_ibody_find *is)\n{\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_inode *raw_inode;\n\tint error;\n\n\tif (!EXT4_INODE_HAS_XATTR_SPACE(inode))\n\t\treturn 0;\n\n\traw_inode = ext4_raw_inode(&is->iloc);\n\theader = IHDR(inode, raw_inode);\n\tis->s.base = is->s.first = IFIRST(header);\n\tis->s.here = is->s.first;\n\tis->s.end = (void *)raw_inode + EXT4_SB(inode->i_sb)->s_inode_size;\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\terror = xattr_check_inode(inode, header, is->s.end);\n\t\tif (error)\n\t\t\treturn error;\n\t\t/* Find the named attribute. */\n\t\terror = xattr_find_entry(inode, &is->s.here, is->s.end,\n\t\t\t\t\t i->name_index, i->name, 0);\n\t\tif (error && error != -ENODATA)\n\t\t\treturn error;\n\t\tis->s.not_found = error;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,9 @@\n \tstruct ext4_inode *raw_inode;\n \tint error;\n \n-\tif (EXT4_I(inode)->i_extra_isize == 0)\n+\tif (!EXT4_INODE_HAS_XATTR_SPACE(inode))\n \t\treturn 0;\n+\n \traw_inode = ext4_raw_inode(&is->iloc);\n \theader = IHDR(inode, raw_inode);\n \tis->s.base = is->s.first = IFIRST(header);",
        "function_modified_lines": {
            "added": [
                "\tif (!EXT4_INODE_HAS_XATTR_SPACE(inode))",
                ""
            ],
            "deleted": [
                "\tif (EXT4_I(inode)->i_extra_isize == 0)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in the Linux kernel's ext4 filesystem in the way it handled the extra inode size for extended attributes. This flaw could allow a privileged local user to cause a system crash or other undefined behaviors.",
        "id": 3960
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static struct l2cap_chan *l2cap_global_chan_by_psm(int state, __le16 psm,\n\t\t\t\t\t\t   bdaddr_t *src,\n\t\t\t\t\t\t   bdaddr_t *dst,\n\t\t\t\t\t\t   u8 link_type)\n{\n\tstruct l2cap_chan *c, *c1 = NULL;\n\n\tread_lock(&chan_list_lock);\n\n\tlist_for_each_entry(c, &chan_list, global_l) {\n\t\tif (state && c->state != state)\n\t\t\tcontinue;\n\n\t\tif (link_type == ACL_LINK && c->src_type != BDADDR_BREDR)\n\t\t\tcontinue;\n\n\t\tif (link_type == LE_LINK && c->src_type == BDADDR_BREDR)\n\t\t\tcontinue;\n\n\t\tif (c->psm == psm) {\n\t\t\tint src_match, dst_match;\n\t\t\tint src_any, dst_any;\n\n\t\t\t/* Exact match. */\n\t\t\tsrc_match = !bacmp(&c->src, src);\n\t\t\tdst_match = !bacmp(&c->dst, dst);\n\t\t\tif (src_match && dst_match) {\n\t\t\t\tl2cap_chan_hold(c);\n\t\t\t\tread_unlock(&chan_list_lock);\n\t\t\t\treturn c;\n\t\t\t}\n\n\t\t\t/* Closest match */\n\t\t\tsrc_any = !bacmp(&c->src, BDADDR_ANY);\n\t\t\tdst_any = !bacmp(&c->dst, BDADDR_ANY);\n\t\t\tif ((src_match && dst_any) || (src_any && dst_match) ||\n\t\t\t    (src_any && dst_any))\n\t\t\t\tc1 = c;\n\t\t}\n\t}\n\n\tif (c1)\n\t\tl2cap_chan_hold(c1);\n\n\tread_unlock(&chan_list_lock);\n\n\treturn c1;\n}",
        "code_after_change": "static struct l2cap_chan *l2cap_global_chan_by_psm(int state, __le16 psm,\n\t\t\t\t\t\t   bdaddr_t *src,\n\t\t\t\t\t\t   bdaddr_t *dst,\n\t\t\t\t\t\t   u8 link_type)\n{\n\tstruct l2cap_chan *c, *c1 = NULL;\n\n\tread_lock(&chan_list_lock);\n\n\tlist_for_each_entry(c, &chan_list, global_l) {\n\t\tif (state && c->state != state)\n\t\t\tcontinue;\n\n\t\tif (link_type == ACL_LINK && c->src_type != BDADDR_BREDR)\n\t\t\tcontinue;\n\n\t\tif (link_type == LE_LINK && c->src_type == BDADDR_BREDR)\n\t\t\tcontinue;\n\n\t\tif (c->psm == psm) {\n\t\t\tint src_match, dst_match;\n\t\t\tint src_any, dst_any;\n\n\t\t\t/* Exact match. */\n\t\t\tsrc_match = !bacmp(&c->src, src);\n\t\t\tdst_match = !bacmp(&c->dst, dst);\n\t\t\tif (src_match && dst_match) {\n\t\t\t\tc = l2cap_chan_hold_unless_zero(c);\n\t\t\t\tif (!c)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tread_unlock(&chan_list_lock);\n\t\t\t\treturn c;\n\t\t\t}\n\n\t\t\t/* Closest match */\n\t\t\tsrc_any = !bacmp(&c->src, BDADDR_ANY);\n\t\t\tdst_any = !bacmp(&c->dst, BDADDR_ANY);\n\t\t\tif ((src_match && dst_any) || (src_any && dst_match) ||\n\t\t\t    (src_any && dst_any))\n\t\t\t\tc1 = c;\n\t\t}\n\t}\n\n\tif (c1)\n\t\tc1 = l2cap_chan_hold_unless_zero(c1);\n\n\tread_unlock(&chan_list_lock);\n\n\treturn c1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,7 +25,10 @@\n \t\t\tsrc_match = !bacmp(&c->src, src);\n \t\t\tdst_match = !bacmp(&c->dst, dst);\n \t\t\tif (src_match && dst_match) {\n-\t\t\t\tl2cap_chan_hold(c);\n+\t\t\t\tc = l2cap_chan_hold_unless_zero(c);\n+\t\t\t\tif (!c)\n+\t\t\t\t\tcontinue;\n+\n \t\t\t\tread_unlock(&chan_list_lock);\n \t\t\t\treturn c;\n \t\t\t}\n@@ -40,7 +43,7 @@\n \t}\n \n \tif (c1)\n-\t\tl2cap_chan_hold(c1);\n+\t\tc1 = l2cap_chan_hold_unless_zero(c1);\n \n \tread_unlock(&chan_list_lock);\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tc = l2cap_chan_hold_unless_zero(c);",
                "\t\t\t\tif (!c)",
                "\t\t\t\t\tcontinue;",
                "",
                "\t\tc1 = l2cap_chan_hold_unless_zero(c1);"
            ],
            "deleted": [
                "\t\t\t\tl2cap_chan_hold(c);",
                "\t\tl2cap_chan_hold(c1);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3394
    },
    {
        "cve_id": "CVE-2023-32269",
        "code_before_change": "static int nr_listen(struct socket *sock, int backlog)\n{\n\tstruct sock *sk = sock->sk;\n\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_LISTEN) {\n\t\tmemset(&nr_sk(sk)->user_addr, 0, AX25_ADDR_LEN);\n\t\tsk->sk_max_ack_backlog = backlog;\n\t\tsk->sk_state           = TCP_LISTEN;\n\t\trelease_sock(sk);\n\t\treturn 0;\n\t}\n\trelease_sock(sk);\n\n\treturn -EOPNOTSUPP;\n}",
        "code_after_change": "static int nr_listen(struct socket *sock, int backlog)\n{\n\tstruct sock *sk = sock->sk;\n\n\tlock_sock(sk);\n\tif (sock->state != SS_UNCONNECTED) {\n\t\trelease_sock(sk);\n\t\treturn -EINVAL;\n\t}\n\n\tif (sk->sk_state != TCP_LISTEN) {\n\t\tmemset(&nr_sk(sk)->user_addr, 0, AX25_ADDR_LEN);\n\t\tsk->sk_max_ack_backlog = backlog;\n\t\tsk->sk_state           = TCP_LISTEN;\n\t\trelease_sock(sk);\n\t\treturn 0;\n\t}\n\trelease_sock(sk);\n\n\treturn -EOPNOTSUPP;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,11 @@\n \tstruct sock *sk = sock->sk;\n \n \tlock_sock(sk);\n+\tif (sock->state != SS_UNCONNECTED) {\n+\t\trelease_sock(sk);\n+\t\treturn -EINVAL;\n+\t}\n+\n \tif (sk->sk_state != TCP_LISTEN) {\n \t\tmemset(&nr_sk(sk)->user_addr, 0, AX25_ADDR_LEN);\n \t\tsk->sk_max_ack_backlog = backlog;",
        "function_modified_lines": {
            "added": [
                "\tif (sock->state != SS_UNCONNECTED) {",
                "\t\trelease_sock(sk);",
                "\t\treturn -EINVAL;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.1.11. In net/netrom/af_netrom.c, there is a use-after-free because accept is also allowed for a successfully connected AF_NETROM socket. However, in order for an attacker to exploit this, the system must have netrom routing configured or the attacker must have the CAP_NET_ADMIN capability.",
        "id": 4051
    },
    {
        "cve_id": "CVE-2023-0030",
        "code_before_change": "void\nnvkm_vmm_put_locked(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tstruct nvkm_vma *next = vma;\n\n\tBUG_ON(vma->part);\n\n\tif (vma->mapref || !vma->sparse) {\n\t\tdo {\n\t\t\tconst bool map = next->memory != NULL;\n\t\t\tconst u8  refd = next->refd;\n\t\t\tconst u64 addr = next->addr;\n\t\t\tu64 size = next->size;\n\n\t\t\t/* Merge regions that are in the same state. */\n\t\t\twhile ((next = node(next, next)) && next->part &&\n\t\t\t       (next->memory != NULL) == map &&\n\t\t\t       (next->refd == refd))\n\t\t\t\tsize += next->size;\n\n\t\t\tif (map) {\n\t\t\t\t/* Region(s) are mapped, merge the unmap\n\t\t\t\t * and dereference into a single walk of\n\t\t\t\t * the page tree.\n\t\t\t\t */\n\t\t\t\tnvkm_vmm_ptes_unmap_put(vmm, &page[refd], addr,\n\t\t\t\t\t\t\tsize, vma->sparse);\n\t\t\t} else\n\t\t\tif (refd != NVKM_VMA_PAGE_NONE) {\n\t\t\t\t/* Drop allocation-time PTE references. */\n\t\t\t\tnvkm_vmm_ptes_put(vmm, &page[refd], addr, size);\n\t\t\t}\n\t\t} while (next && next->part);\n\t}\n\n\t/* Merge any mapped regions that were split from the initial\n\t * address-space allocation back into the allocated VMA, and\n\t * release memory/compression resources.\n\t */\n\tnext = vma;\n\tdo {\n\t\tif (next->memory)\n\t\t\tnvkm_vmm_unmap_region(vmm, next);\n\t} while ((next = node(vma, next)) && next->part);\n\n\tif (vma->sparse && !vma->mapref) {\n\t\t/* Sparse region that was allocated with a fixed page size,\n\t\t * meaning all relevant PTEs were referenced once when the\n\t\t * region was allocated, and remained that way, regardless\n\t\t * of whether memory was mapped into it afterwards.\n\t\t *\n\t\t * The process of unmapping, unsparsing, and dereferencing\n\t\t * PTEs can be done in a single page tree walk.\n\t\t */\n\t\tnvkm_vmm_ptes_sparse_put(vmm, &page[vma->refd], vma->addr, vma->size);\n\t} else\n\tif (vma->sparse) {\n\t\t/* Sparse region that wasn't allocated with a fixed page size,\n\t\t * PTE references were taken both at allocation time (to make\n\t\t * the GPU see the region as sparse), and when mapping memory\n\t\t * into the region.\n\t\t *\n\t\t * The latter was handled above, and the remaining references\n\t\t * are dealt with here.\n\t\t */\n\t\tnvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, false);\n\t}\n\n\t/* Remove VMA from the list of allocated nodes. */\n\trb_erase(&vma->tree, &vmm->root);\n\n\t/* Merge VMA back into the free list. */\n\tvma->page = NVKM_VMA_PAGE_NONE;\n\tvma->refd = NVKM_VMA_PAGE_NONE;\n\tvma->used = false;\n\tvma->user = false;\n\tnvkm_vmm_put_region(vmm, vma);\n}",
        "code_after_change": "void\nnvkm_vmm_put_locked(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tstruct nvkm_vma *next = vma;\n\n\tBUG_ON(vma->part);\n\n\tif (vma->mapref || !vma->sparse) {\n\t\tdo {\n\t\t\tconst bool map = next->memory != NULL;\n\t\t\tconst u8  refd = next->refd;\n\t\t\tconst u64 addr = next->addr;\n\t\t\tu64 size = next->size;\n\n\t\t\t/* Merge regions that are in the same state. */\n\t\t\twhile ((next = node(next, next)) && next->part &&\n\t\t\t       (next->memory != NULL) == map &&\n\t\t\t       (next->refd == refd))\n\t\t\t\tsize += next->size;\n\n\t\t\tif (map) {\n\t\t\t\t/* Region(s) are mapped, merge the unmap\n\t\t\t\t * and dereference into a single walk of\n\t\t\t\t * the page tree.\n\t\t\t\t */\n\t\t\t\tnvkm_vmm_ptes_unmap_put(vmm, &page[refd], addr,\n\t\t\t\t\t\t\tsize, vma->sparse);\n\t\t\t} else\n\t\t\tif (refd != NVKM_VMA_PAGE_NONE) {\n\t\t\t\t/* Drop allocation-time PTE references. */\n\t\t\t\tnvkm_vmm_ptes_put(vmm, &page[refd], addr, size);\n\t\t\t}\n\t\t} while (next && next->part);\n\t}\n\n\t/* Merge any mapped regions that were split from the initial\n\t * address-space allocation back into the allocated VMA, and\n\t * release memory/compression resources.\n\t */\n\tnext = vma;\n\tdo {\n\t\tif (next->memory)\n\t\t\tnvkm_vmm_unmap_region(vmm, next);\n\t} while ((next = node(vma, next)) && next->part);\n\n\tif (vma->sparse && !vma->mapref) {\n\t\t/* Sparse region that was allocated with a fixed page size,\n\t\t * meaning all relevant PTEs were referenced once when the\n\t\t * region was allocated, and remained that way, regardless\n\t\t * of whether memory was mapped into it afterwards.\n\t\t *\n\t\t * The process of unmapping, unsparsing, and dereferencing\n\t\t * PTEs can be done in a single page tree walk.\n\t\t */\n\t\tnvkm_vmm_ptes_sparse_put(vmm, &page[vma->refd], vma->addr, vma->size);\n\t} else\n\tif (vma->sparse) {\n\t\t/* Sparse region that wasn't allocated with a fixed page size,\n\t\t * PTE references were taken both at allocation time (to make\n\t\t * the GPU see the region as sparse), and when mapping memory\n\t\t * into the region.\n\t\t *\n\t\t * The latter was handled above, and the remaining references\n\t\t * are dealt with here.\n\t\t */\n\t\tnvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, false);\n\t}\n\n\t/* Remove VMA from the list of allocated nodes. */\n\tnvkm_vmm_node_remove(vmm, vma);\n\n\t/* Merge VMA back into the free list. */\n\tvma->page = NVKM_VMA_PAGE_NONE;\n\tvma->refd = NVKM_VMA_PAGE_NONE;\n\tvma->used = false;\n\tvma->user = false;\n\tnvkm_vmm_put_region(vmm, vma);\n}",
        "patch": "--- code before\n+++ code after\n@@ -68,7 +68,7 @@\n \t}\n \n \t/* Remove VMA from the list of allocated nodes. */\n-\trb_erase(&vma->tree, &vmm->root);\n+\tnvkm_vmm_node_remove(vmm, vma);\n \n \t/* Merge VMA back into the free list. */\n \tvma->page = NVKM_VMA_PAGE_NONE;",
        "function_modified_lines": {
            "added": [
                "\tnvkm_vmm_node_remove(vmm, vma);"
            ],
            "deleted": [
                "\trb_erase(&vma->tree, &vmm->root);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s nouveau driver in how a user triggers a memory overflow that causes the nvkm_vma_tail function to fail. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3807
    },
    {
        "cve_id": "CVE-2020-8648",
        "code_before_change": "int paste_selection(struct tty_struct *tty)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tint\tpasted = 0;\n\tunsigned int count;\n\tstruct  tty_ldisc *ld;\n\tDECLARE_WAITQUEUE(wait, current);\n\tint ret = 0;\n\n\tconsole_lock();\n\tpoke_blanked_console();\n\tconsole_unlock();\n\n\tld = tty_ldisc_ref_wait(tty);\n\tif (!ld)\n\t\treturn -EIO;\t/* ldisc was hung up */\n\ttty_buffer_lock_exclusive(&vc->port);\n\n\tadd_wait_queue(&vc->paste_wait, &wait);\n\twhile (sel_buffer && sel_buffer_lth > pasted) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\t\tif (tty_throttled(tty)) {\n\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\t\t__set_current_state(TASK_RUNNING);\n\t\tcount = sel_buffer_lth - pasted;\n\t\tcount = tty_ldisc_receive_buf(ld, sel_buffer + pasted, NULL,\n\t\t\t\t\t      count);\n\t\tpasted += count;\n\t}\n\tremove_wait_queue(&vc->paste_wait, &wait);\n\t__set_current_state(TASK_RUNNING);\n\n\ttty_buffer_unlock_exclusive(&vc->port);\n\ttty_ldisc_deref(ld);\n\treturn ret;\n}",
        "code_after_change": "int paste_selection(struct tty_struct *tty)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tint\tpasted = 0;\n\tunsigned int count;\n\tstruct  tty_ldisc *ld;\n\tDECLARE_WAITQUEUE(wait, current);\n\tint ret = 0;\n\n\tconsole_lock();\n\tpoke_blanked_console();\n\tconsole_unlock();\n\n\tld = tty_ldisc_ref_wait(tty);\n\tif (!ld)\n\t\treturn -EIO;\t/* ldisc was hung up */\n\ttty_buffer_lock_exclusive(&vc->port);\n\n\tadd_wait_queue(&vc->paste_wait, &wait);\n\tmutex_lock(&sel_lock);\n\twhile (sel_buffer && sel_buffer_lth > pasted) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\t\tif (tty_throttled(tty)) {\n\t\t\tmutex_unlock(&sel_lock);\n\t\t\tschedule();\n\t\t\tmutex_lock(&sel_lock);\n\t\t\tcontinue;\n\t\t}\n\t\t__set_current_state(TASK_RUNNING);\n\t\tcount = sel_buffer_lth - pasted;\n\t\tcount = tty_ldisc_receive_buf(ld, sel_buffer + pasted, NULL,\n\t\t\t\t\t      count);\n\t\tpasted += count;\n\t}\n\tmutex_unlock(&sel_lock);\n\tremove_wait_queue(&vc->paste_wait, &wait);\n\t__set_current_state(TASK_RUNNING);\n\n\ttty_buffer_unlock_exclusive(&vc->port);\n\ttty_ldisc_deref(ld);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \ttty_buffer_lock_exclusive(&vc->port);\n \n \tadd_wait_queue(&vc->paste_wait, &wait);\n+\tmutex_lock(&sel_lock);\n \twhile (sel_buffer && sel_buffer_lth > pasted) {\n \t\tset_current_state(TASK_INTERRUPTIBLE);\n \t\tif (signal_pending(current)) {\n@@ -24,7 +25,9 @@\n \t\t\tbreak;\n \t\t}\n \t\tif (tty_throttled(tty)) {\n+\t\t\tmutex_unlock(&sel_lock);\n \t\t\tschedule();\n+\t\t\tmutex_lock(&sel_lock);\n \t\t\tcontinue;\n \t\t}\n \t\t__set_current_state(TASK_RUNNING);\n@@ -33,6 +36,7 @@\n \t\t\t\t\t      count);\n \t\tpasted += count;\n \t}\n+\tmutex_unlock(&sel_lock);\n \tremove_wait_queue(&vc->paste_wait, &wait);\n \t__set_current_state(TASK_RUNNING);\n ",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&sel_lock);",
                "\t\t\tmutex_unlock(&sel_lock);",
                "\t\t\tmutex_lock(&sel_lock);",
                "\tmutex_unlock(&sel_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a use-after-free vulnerability in the Linux kernel through 5.5.2 in the n_tty_receive_buf_common function in drivers/tty/n_tty.c.",
        "id": 2806
    },
    {
        "cve_id": "CVE-2023-35826",
        "code_before_change": "static int cedrus_remove(struct platform_device *pdev)\n{\n\tstruct cedrus_dev *dev = platform_get_drvdata(pdev);\n\n\tif (media_devnode_is_registered(dev->mdev.devnode)) {\n\t\tmedia_device_unregister(&dev->mdev);\n\t\tv4l2_m2m_unregister_media_controller(dev->m2m_dev);\n\t\tmedia_device_cleanup(&dev->mdev);\n\t}\n\n\tv4l2_m2m_release(dev->m2m_dev);\n\tvideo_unregister_device(&dev->vfd);\n\tv4l2_device_unregister(&dev->v4l2_dev);\n\n\tcedrus_hw_remove(dev);\n\n\treturn 0;\n}",
        "code_after_change": "static int cedrus_remove(struct platform_device *pdev)\n{\n\tstruct cedrus_dev *dev = platform_get_drvdata(pdev);\n\n\tcancel_delayed_work_sync(&dev->watchdog_work);\n\tif (media_devnode_is_registered(dev->mdev.devnode)) {\n\t\tmedia_device_unregister(&dev->mdev);\n\t\tv4l2_m2m_unregister_media_controller(dev->m2m_dev);\n\t\tmedia_device_cleanup(&dev->mdev);\n\t}\n\n\tv4l2_m2m_release(dev->m2m_dev);\n\tvideo_unregister_device(&dev->vfd);\n\tv4l2_device_unregister(&dev->v4l2_dev);\n\n\tcedrus_hw_remove(dev);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n {\n \tstruct cedrus_dev *dev = platform_get_drvdata(pdev);\n \n+\tcancel_delayed_work_sync(&dev->watchdog_work);\n \tif (media_devnode_is_registered(dev->mdev.devnode)) {\n \t\tmedia_device_unregister(&dev->mdev);\n \t\tv4l2_m2m_unregister_media_controller(dev->m2m_dev);",
        "function_modified_lines": {
            "added": [
                "\tcancel_delayed_work_sync(&dev->watchdog_work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in cedrus_remove in drivers/staging/media/sunxi/cedrus/cedrus.c.",
        "id": 4113
    },
    {
        "cve_id": "CVE-2023-1611",
        "code_before_change": "int btrfs_run_qgroups(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint ret = 0;\n\n\tif (!fs_info->quota_root)\n\t\treturn ret;\n\n\tspin_lock(&fs_info->qgroup_lock);\n\twhile (!list_empty(&fs_info->dirty_qgroups)) {\n\t\tstruct btrfs_qgroup *qgroup;\n\t\tqgroup = list_first_entry(&fs_info->dirty_qgroups,\n\t\t\t\t\t  struct btrfs_qgroup, dirty);\n\t\tlist_del_init(&qgroup->dirty);\n\t\tspin_unlock(&fs_info->qgroup_lock);\n\t\tret = update_qgroup_info_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tret = update_qgroup_limit_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tspin_lock(&fs_info->qgroup_lock);\n\t}\n\tif (test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags))\n\t\tfs_info->qgroup_flags |= BTRFS_QGROUP_STATUS_FLAG_ON;\n\telse\n\t\tfs_info->qgroup_flags &= ~BTRFS_QGROUP_STATUS_FLAG_ON;\n\tspin_unlock(&fs_info->qgroup_lock);\n\n\tret = update_qgroup_status_item(trans);\n\tif (ret)\n\t\tqgroup_mark_inconsistent(fs_info);\n\n\treturn ret;\n}",
        "code_after_change": "int btrfs_run_qgroups(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint ret = 0;\n\n\t/*\n\t * In case we are called from the qgroup assign ioctl, assert that we\n\t * are holding the qgroup_ioctl_lock, otherwise we can race with a quota\n\t * disable operation (ioctl) and access a freed quota root.\n\t */\n\tif (trans->transaction->state != TRANS_STATE_COMMIT_DOING)\n\t\tlockdep_assert_held(&fs_info->qgroup_ioctl_lock);\n\n\tif (!fs_info->quota_root)\n\t\treturn ret;\n\n\tspin_lock(&fs_info->qgroup_lock);\n\twhile (!list_empty(&fs_info->dirty_qgroups)) {\n\t\tstruct btrfs_qgroup *qgroup;\n\t\tqgroup = list_first_entry(&fs_info->dirty_qgroups,\n\t\t\t\t\t  struct btrfs_qgroup, dirty);\n\t\tlist_del_init(&qgroup->dirty);\n\t\tspin_unlock(&fs_info->qgroup_lock);\n\t\tret = update_qgroup_info_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tret = update_qgroup_limit_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tspin_lock(&fs_info->qgroup_lock);\n\t}\n\tif (test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags))\n\t\tfs_info->qgroup_flags |= BTRFS_QGROUP_STATUS_FLAG_ON;\n\telse\n\t\tfs_info->qgroup_flags &= ~BTRFS_QGROUP_STATUS_FLAG_ON;\n\tspin_unlock(&fs_info->qgroup_lock);\n\n\tret = update_qgroup_status_item(trans);\n\tif (ret)\n\t\tqgroup_mark_inconsistent(fs_info);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,14 @@\n {\n \tstruct btrfs_fs_info *fs_info = trans->fs_info;\n \tint ret = 0;\n+\n+\t/*\n+\t * In case we are called from the qgroup assign ioctl, assert that we\n+\t * are holding the qgroup_ioctl_lock, otherwise we can race with a quota\n+\t * disable operation (ioctl) and access a freed quota root.\n+\t */\n+\tif (trans->transaction->state != TRANS_STATE_COMMIT_DOING)\n+\t\tlockdep_assert_held(&fs_info->qgroup_ioctl_lock);\n \n \tif (!fs_info->quota_root)\n \t\treturn ret;",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * In case we are called from the qgroup assign ioctl, assert that we",
                "\t * are holding the qgroup_ioctl_lock, otherwise we can race with a quota",
                "\t * disable operation (ioctl) and access a freed quota root.",
                "\t */",
                "\tif (trans->transaction->state != TRANS_STATE_COMMIT_DOING)",
                "\t\tlockdep_assert_held(&fs_info->qgroup_ioctl_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in btrfs_search_slot in fs/btrfs/ctree.c in btrfs in the Linux Kernel.This flaw allows an attacker to crash the system and possibly cause a kernel information lea",
        "id": 3875
    },
    {
        "cve_id": "CVE-2023-1193",
        "code_before_change": "int ksmbd_conn_try_dequeue_request(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tint ret = 1;\n\n\tif (list_empty(&work->request_entry) &&\n\t    list_empty(&work->async_request_entry))\n\t\treturn 0;\n\n\tif (!work->multiRsp)\n\t\tatomic_dec(&conn->req_running);\n\tspin_lock(&conn->request_lock);\n\tif (!work->multiRsp) {\n\t\tlist_del_init(&work->request_entry);\n\t\tif (!work->synchronous)\n\t\t\tlist_del_init(&work->async_request_entry);\n\t\tret = 0;\n\t}\n\tspin_unlock(&conn->request_lock);\n\n\twake_up_all(&conn->req_running_q);\n\treturn ret;\n}",
        "code_after_change": "int ksmbd_conn_try_dequeue_request(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tint ret = 1;\n\n\tif (list_empty(&work->request_entry) &&\n\t    list_empty(&work->async_request_entry))\n\t\treturn 0;\n\n\tif (!work->multiRsp)\n\t\tatomic_dec(&conn->req_running);\n\tif (!work->multiRsp) {\n\t\tspin_lock(&conn->request_lock);\n\t\tlist_del_init(&work->request_entry);\n\t\tspin_unlock(&conn->request_lock);\n\t\tif (work->asynchronous)\n\t\t\trelease_async_work(work);\n\t\tret = 0;\n\t}\n\n\twake_up_all(&conn->req_running_q);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,14 +9,14 @@\n \n \tif (!work->multiRsp)\n \t\tatomic_dec(&conn->req_running);\n-\tspin_lock(&conn->request_lock);\n \tif (!work->multiRsp) {\n+\t\tspin_lock(&conn->request_lock);\n \t\tlist_del_init(&work->request_entry);\n-\t\tif (!work->synchronous)\n-\t\t\tlist_del_init(&work->async_request_entry);\n+\t\tspin_unlock(&conn->request_lock);\n+\t\tif (work->asynchronous)\n+\t\t\trelease_async_work(work);\n \t\tret = 0;\n \t}\n-\tspin_unlock(&conn->request_lock);\n \n \twake_up_all(&conn->req_running_q);\n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "\t\tspin_lock(&conn->request_lock);",
                "\t\tspin_unlock(&conn->request_lock);",
                "\t\tif (work->asynchronous)",
                "\t\t\trelease_async_work(work);"
            ],
            "deleted": [
                "\tspin_lock(&conn->request_lock);",
                "\t\tif (!work->synchronous)",
                "\t\t\tlist_del_init(&work->async_request_entry);",
                "\tspin_unlock(&conn->request_lock);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in setup_async_work in the KSMBD implementation of the in-kernel samba server and CIFS in the Linux kernel. This issue could allow an attacker to crash the system by accessing freed work.",
        "id": 3851
    },
    {
        "cve_id": "CVE-2021-3347",
        "code_before_change": "static int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\tstruct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tu32 uval, curval, newval;\n\tstruct task_struct *oldowner, *newowner;\n\tu32 newtid;\n\tint ret, err = 0;\n\n\tlockdep_assert_held(q->lock_ptr);\n\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\toldowner = pi_state->owner;\n\n\t/*\n\t * We are here because either:\n\t *\n\t *  - we stole the lock and pi_state->owner needs updating to reflect\n\t *    that (@argowner == current),\n\t *\n\t * or:\n\t *\n\t *  - someone stole our lock and we need to fix things to point to the\n\t *    new owner (@argowner == NULL).\n\t *\n\t * Either way, we have to replace the TID in the user space variable.\n\t * This must be atomic as we have to preserve the owner died bit here.\n\t *\n\t * Note: We write the user space value _before_ changing the pi_state\n\t * because we can fault here. Imagine swapped out pages or a fork\n\t * that marked all the anonymous memory readonly for cow.\n\t *\n\t * Modifying pi_state _before_ the user space value would leave the\n\t * pi_state in an inconsistent state when we fault here, because we\n\t * need to drop the locks to handle the fault. This might be observed\n\t * in the PID check in lookup_pi_state.\n\t */\nretry:\n\tif (!argowner) {\n\t\tif (oldowner != current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (__rt_mutex_futex_trylock(&pi_state->pi_mutex)) {\n\t\t\t/* We got the lock after all, nothing to fix. */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * The trylock just failed, so either there is an owner or\n\t\t * there is a higher priority waiter than this one.\n\t\t */\n\t\tnewowner = rt_mutex_owner(&pi_state->pi_mutex);\n\t\t/*\n\t\t * If the higher priority waiter has not yet taken over the\n\t\t * rtmutex then newowner is NULL. We can't return here with\n\t\t * that state because it's inconsistent vs. the user space\n\t\t * state. So drop the locks and try again. It's a valid\n\t\t * situation and not any different from the other retry\n\t\t * conditions.\n\t\t */\n\t\tif (unlikely(!newowner)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto handle_err;\n\t\t}\n\t} else {\n\t\tWARN_ON_ONCE(argowner != current);\n\t\tif (oldowner == current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tnewowner = argowner;\n\t}\n\n\tnewtid = task_pid_vnr(newowner) | FUTEX_WAITERS;\n\t/* Owner died? */\n\tif (!pi_state->owner)\n\t\tnewtid |= FUTEX_OWNER_DIED;\n\n\terr = get_futex_value_locked(&uval, uaddr);\n\tif (err)\n\t\tgoto handle_err;\n\n\tfor (;;) {\n\t\tnewval = (uval & FUTEX_OWNER_DIED) | newtid;\n\n\t\terr = cmpxchg_futex_value_locked(&curval, uaddr, uval, newval);\n\t\tif (err)\n\t\t\tgoto handle_err;\n\n\t\tif (curval == uval)\n\t\t\tbreak;\n\t\tuval = curval;\n\t}\n\n\t/*\n\t * We fixed up user space. Now we need to fix the pi_state\n\t * itself.\n\t */\n\tif (pi_state->owner != NULL) {\n\t\traw_spin_lock(&pi_state->owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&pi_state->owner->pi_lock);\n\t}\n\n\tpi_state->owner = newowner;\n\n\traw_spin_lock(&newowner->pi_lock);\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &newowner->pi_state_list);\n\traw_spin_unlock(&newowner->pi_lock);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\n\treturn 0;\n\n\t/*\n\t * In order to reschedule or handle a page fault, we need to drop the\n\t * locks here. In the case of a fault, this gives the other task\n\t * (either the highest priority waiter itself or the task which stole\n\t * the rtmutex) the chance to try the fixup of the pi_state. So once we\n\t * are back from handling the fault we need to check the pi_state after\n\t * reacquiring the locks and before trying to do another fixup. When\n\t * the fixup has been done already we simply return.\n\t *\n\t * Note: we hold both hb->lock and pi_mutex->wait_lock. We can safely\n\t * drop hb->lock since the caller owns the hb -> futex_q relation.\n\t * Dropping the pi_mutex->wait_lock requires the state revalidate.\n\t */\nhandle_err:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q->lock_ptr);\n\n\tswitch (err) {\n\tcase -EFAULT:\n\t\tret = fault_in_user_writeable(uaddr);\n\t\tbreak;\n\n\tcase -EAGAIN:\n\t\tcond_resched();\n\t\tret = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tret = err;\n\t\tbreak;\n\t}\n\n\tspin_lock(q->lock_ptr);\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Check if someone else fixed it for us:\n\t */\n\tif (pi_state->owner != oldowner) {\n\t\tret = 0;\n\t\tgoto out_unlock;\n\t}\n\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tgoto retry;\n\nout_unlock:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}",
        "code_after_change": "static int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\tstruct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tu32 uval, curval, newval;\n\tstruct task_struct *oldowner, *newowner;\n\tu32 newtid;\n\tint ret, err = 0;\n\n\tlockdep_assert_held(q->lock_ptr);\n\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\toldowner = pi_state->owner;\n\n\t/*\n\t * We are here because either:\n\t *\n\t *  - we stole the lock and pi_state->owner needs updating to reflect\n\t *    that (@argowner == current),\n\t *\n\t * or:\n\t *\n\t *  - someone stole our lock and we need to fix things to point to the\n\t *    new owner (@argowner == NULL).\n\t *\n\t * Either way, we have to replace the TID in the user space variable.\n\t * This must be atomic as we have to preserve the owner died bit here.\n\t *\n\t * Note: We write the user space value _before_ changing the pi_state\n\t * because we can fault here. Imagine swapped out pages or a fork\n\t * that marked all the anonymous memory readonly for cow.\n\t *\n\t * Modifying pi_state _before_ the user space value would leave the\n\t * pi_state in an inconsistent state when we fault here, because we\n\t * need to drop the locks to handle the fault. This might be observed\n\t * in the PID check in lookup_pi_state.\n\t */\nretry:\n\tif (!argowner) {\n\t\tif (oldowner != current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (__rt_mutex_futex_trylock(&pi_state->pi_mutex)) {\n\t\t\t/* We got the lock. pi_state is correct. Tell caller. */\n\t\t\tret = 1;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * The trylock just failed, so either there is an owner or\n\t\t * there is a higher priority waiter than this one.\n\t\t */\n\t\tnewowner = rt_mutex_owner(&pi_state->pi_mutex);\n\t\t/*\n\t\t * If the higher priority waiter has not yet taken over the\n\t\t * rtmutex then newowner is NULL. We can't return here with\n\t\t * that state because it's inconsistent vs. the user space\n\t\t * state. So drop the locks and try again. It's a valid\n\t\t * situation and not any different from the other retry\n\t\t * conditions.\n\t\t */\n\t\tif (unlikely(!newowner)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto handle_err;\n\t\t}\n\t} else {\n\t\tWARN_ON_ONCE(argowner != current);\n\t\tif (oldowner == current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\tret = 1;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tnewowner = argowner;\n\t}\n\n\tnewtid = task_pid_vnr(newowner) | FUTEX_WAITERS;\n\t/* Owner died? */\n\tif (!pi_state->owner)\n\t\tnewtid |= FUTEX_OWNER_DIED;\n\n\terr = get_futex_value_locked(&uval, uaddr);\n\tif (err)\n\t\tgoto handle_err;\n\n\tfor (;;) {\n\t\tnewval = (uval & FUTEX_OWNER_DIED) | newtid;\n\n\t\terr = cmpxchg_futex_value_locked(&curval, uaddr, uval, newval);\n\t\tif (err)\n\t\t\tgoto handle_err;\n\n\t\tif (curval == uval)\n\t\t\tbreak;\n\t\tuval = curval;\n\t}\n\n\t/*\n\t * We fixed up user space. Now we need to fix the pi_state\n\t * itself.\n\t */\n\tif (pi_state->owner != NULL) {\n\t\traw_spin_lock(&pi_state->owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&pi_state->owner->pi_lock);\n\t}\n\n\tpi_state->owner = newowner;\n\n\traw_spin_lock(&newowner->pi_lock);\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &newowner->pi_state_list);\n\traw_spin_unlock(&newowner->pi_lock);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\n\treturn argowner == current;\n\n\t/*\n\t * In order to reschedule or handle a page fault, we need to drop the\n\t * locks here. In the case of a fault, this gives the other task\n\t * (either the highest priority waiter itself or the task which stole\n\t * the rtmutex) the chance to try the fixup of the pi_state. So once we\n\t * are back from handling the fault we need to check the pi_state after\n\t * reacquiring the locks and before trying to do another fixup. When\n\t * the fixup has been done already we simply return.\n\t *\n\t * Note: we hold both hb->lock and pi_mutex->wait_lock. We can safely\n\t * drop hb->lock since the caller owns the hb -> futex_q relation.\n\t * Dropping the pi_mutex->wait_lock requires the state revalidate.\n\t */\nhandle_err:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q->lock_ptr);\n\n\tswitch (err) {\n\tcase -EFAULT:\n\t\tret = fault_in_user_writeable(uaddr);\n\t\tbreak;\n\n\tcase -EAGAIN:\n\t\tcond_resched();\n\t\tret = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tret = err;\n\t\tbreak;\n\t}\n\n\tspin_lock(q->lock_ptr);\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Check if someone else fixed it for us:\n\t */\n\tif (pi_state->owner != oldowner) {\n\t\tret = argowner == current;\n\t\tgoto out_unlock;\n\t}\n\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tgoto retry;\n\nout_unlock:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -48,8 +48,8 @@\n \t\t}\n \n \t\tif (__rt_mutex_futex_trylock(&pi_state->pi_mutex)) {\n-\t\t\t/* We got the lock after all, nothing to fix. */\n-\t\t\tret = 0;\n+\t\t\t/* We got the lock. pi_state is correct. Tell caller. */\n+\t\t\tret = 1;\n \t\t\tgoto out_unlock;\n \t\t}\n \n@@ -77,7 +77,7 @@\n \t\t\t * We raced against a concurrent self; things are\n \t\t\t * already fixed up. Nothing to do.\n \t\t\t */\n-\t\t\tret = 0;\n+\t\t\tret = 1;\n \t\t\tgoto out_unlock;\n \t\t}\n \t\tnewowner = argowner;\n@@ -123,7 +123,7 @@\n \traw_spin_unlock(&newowner->pi_lock);\n \traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n \n-\treturn 0;\n+\treturn argowner == current;\n \n \t/*\n \t * In order to reschedule or handle a page fault, we need to drop the\n@@ -165,7 +165,7 @@\n \t * Check if someone else fixed it for us:\n \t */\n \tif (pi_state->owner != oldowner) {\n-\t\tret = 0;\n+\t\tret = argowner == current;\n \t\tgoto out_unlock;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t/* We got the lock. pi_state is correct. Tell caller. */",
                "\t\t\tret = 1;",
                "\t\t\tret = 1;",
                "\treturn argowner == current;",
                "\t\tret = argowner == current;"
            ],
            "deleted": [
                "\t\t\t/* We got the lock after all, nothing to fix. */",
                "\t\t\tret = 0;",
                "\t\t\tret = 0;",
                "\treturn 0;",
                "\t\tret = 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.10.11. PI futexes have a kernel stack use-after-free during fault handling, allowing local users to execute code in the kernel, aka CID-34b1a1ce1458.",
        "id": 2977
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "int io_poll_cancel(struct io_ring_ctx *ctx, struct io_cancel_data *cd,\n\t\t   unsigned issue_flags)\n{\n\treturn __io_poll_cancel(ctx, cd, &ctx->cancel_table);\n}",
        "code_after_change": "int io_poll_cancel(struct io_ring_ctx *ctx, struct io_cancel_data *cd,\n\t\t   unsigned issue_flags)\n{\n\tint ret;\n\n\tret = __io_poll_cancel(ctx, cd, &ctx->cancel_table);\n\tif (ret != -ENOENT)\n\t\treturn ret;\n\n\tio_ring_submit_lock(ctx, issue_flags);\n\tret = __io_poll_cancel(ctx, cd, &ctx->cancel_table_locked);\n\tio_ring_submit_unlock(ctx, issue_flags);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,14 @@\n int io_poll_cancel(struct io_ring_ctx *ctx, struct io_cancel_data *cd,\n \t\t   unsigned issue_flags)\n {\n-\treturn __io_poll_cancel(ctx, cd, &ctx->cancel_table);\n+\tint ret;\n+\n+\tret = __io_poll_cancel(ctx, cd, &ctx->cancel_table);\n+\tif (ret != -ENOENT)\n+\t\treturn ret;\n+\n+\tio_ring_submit_lock(ctx, issue_flags);\n+\tret = __io_poll_cancel(ctx, cd, &ctx->cancel_table_locked);\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tint ret;",
                "",
                "\tret = __io_poll_cancel(ctx, cd, &ctx->cancel_table);",
                "\tif (ret != -ENOENT)",
                "\t\treturn ret;",
                "",
                "\tio_ring_submit_lock(ctx, issue_flags);",
                "\tret = __io_poll_cancel(ctx, cd, &ctx->cancel_table_locked);",
                "\tio_ring_submit_unlock(ctx, issue_flags);",
                "\treturn ret;"
            ],
            "deleted": [
                "\treturn __io_poll_cancel(ctx, cd, &ctx->cancel_table);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and 0e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4075
    },
    {
        "cve_id": "CVE-2023-26606",
        "code_before_change": "int ntfs_trim_fs(struct ntfs_sb_info *sbi, struct fstrim_range *range)\n{\n\tint err = 0;\n\tstruct super_block *sb = sbi->sb;\n\tstruct wnd_bitmap *wnd = &sbi->used.bitmap;\n\tu32 wbits = 8 * sb->s_blocksize;\n\tCLST len = 0, lcn = 0, done = 0;\n\tCLST minlen = bytes_to_cluster(sbi, range->minlen);\n\tCLST lcn_from = bytes_to_cluster(sbi, range->start);\n\tsize_t iw = lcn_from >> (sb->s_blocksize_bits + 3);\n\tu32 wbit = lcn_from & (wbits - 1);\n\tconst ulong *buf;\n\tCLST lcn_to;\n\n\tif (!minlen)\n\t\tminlen = 1;\n\n\tif (range->len == (u64)-1)\n\t\tlcn_to = wnd->nbits;\n\telse\n\t\tlcn_to = bytes_to_cluster(sbi, range->start + range->len);\n\n\tdown_read_nested(&wnd->rw_lock, BITMAP_MUTEX_CLUSTERS);\n\n\tfor (; iw < wnd->nbits; iw++, wbit = 0) {\n\t\tCLST lcn_wnd = iw * wbits;\n\t\tstruct buffer_head *bh;\n\n\t\tif (lcn_wnd > lcn_to)\n\t\t\tbreak;\n\n\t\tif (!wnd->free_bits[iw])\n\t\t\tcontinue;\n\n\t\tif (iw + 1 == wnd->nwnd)\n\t\t\twbits = wnd->bits_last;\n\n\t\tif (lcn_wnd + wbits > lcn_to)\n\t\t\twbits = lcn_to - lcn_wnd;\n\n\t\tbh = wnd_map(wnd, iw);\n\t\tif (IS_ERR(bh)) {\n\t\t\terr = PTR_ERR(bh);\n\t\t\tbreak;\n\t\t}\n\n\t\tbuf = (ulong *)bh->b_data;\n\n\t\tfor (; wbit < wbits; wbit++) {\n\t\t\tif (!test_bit(wbit, buf)) {\n\t\t\t\tif (!len)\n\t\t\t\t\tlcn = lcn_wnd + wbit;\n\t\t\t\tlen += 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (len >= minlen) {\n\t\t\t\terr = ntfs_discard(sbi, lcn, len);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\t\t\t\tdone += len;\n\t\t\t}\n\t\t\tlen = 0;\n\t\t}\n\t\tput_bh(bh);\n\t}\n\n\t/* Process the last fragment. */\n\tif (len >= minlen) {\n\t\terr = ntfs_discard(sbi, lcn, len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tdone += len;\n\t}\n\nout:\n\trange->len = (u64)done << sbi->cluster_bits;\n\n\tup_read(&wnd->rw_lock);\n\n\treturn err;\n}",
        "code_after_change": "int ntfs_trim_fs(struct ntfs_sb_info *sbi, struct fstrim_range *range)\n{\n\tint err = 0;\n\tstruct super_block *sb = sbi->sb;\n\tstruct wnd_bitmap *wnd = &sbi->used.bitmap;\n\tu32 wbits = 8 * sb->s_blocksize;\n\tCLST len = 0, lcn = 0, done = 0;\n\tCLST minlen = bytes_to_cluster(sbi, range->minlen);\n\tCLST lcn_from = bytes_to_cluster(sbi, range->start);\n\tsize_t iw = lcn_from >> (sb->s_blocksize_bits + 3);\n\tu32 wbit = lcn_from & (wbits - 1);\n\tconst ulong *buf;\n\tCLST lcn_to;\n\n\tif (!minlen)\n\t\tminlen = 1;\n\n\tif (range->len == (u64)-1)\n\t\tlcn_to = wnd->nbits;\n\telse\n\t\tlcn_to = bytes_to_cluster(sbi, range->start + range->len);\n\n\tdown_read_nested(&wnd->rw_lock, BITMAP_MUTEX_CLUSTERS);\n\n\tfor (; iw < wnd->nwnd; iw++, wbit = 0) {\n\t\tCLST lcn_wnd = iw * wbits;\n\t\tstruct buffer_head *bh;\n\n\t\tif (lcn_wnd > lcn_to)\n\t\t\tbreak;\n\n\t\tif (!wnd->free_bits[iw])\n\t\t\tcontinue;\n\n\t\tif (iw + 1 == wnd->nwnd)\n\t\t\twbits = wnd->bits_last;\n\n\t\tif (lcn_wnd + wbits > lcn_to)\n\t\t\twbits = lcn_to - lcn_wnd;\n\n\t\tbh = wnd_map(wnd, iw);\n\t\tif (IS_ERR(bh)) {\n\t\t\terr = PTR_ERR(bh);\n\t\t\tbreak;\n\t\t}\n\n\t\tbuf = (ulong *)bh->b_data;\n\n\t\tfor (; wbit < wbits; wbit++) {\n\t\t\tif (!test_bit(wbit, buf)) {\n\t\t\t\tif (!len)\n\t\t\t\t\tlcn = lcn_wnd + wbit;\n\t\t\t\tlen += 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (len >= minlen) {\n\t\t\t\terr = ntfs_discard(sbi, lcn, len);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\t\t\t\tdone += len;\n\t\t\t}\n\t\t\tlen = 0;\n\t\t}\n\t\tput_bh(bh);\n\t}\n\n\t/* Process the last fragment. */\n\tif (len >= minlen) {\n\t\terr = ntfs_discard(sbi, lcn, len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tdone += len;\n\t}\n\nout:\n\trange->len = (u64)done << sbi->cluster_bits;\n\n\tup_read(&wnd->rw_lock);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,7 +22,7 @@\n \n \tdown_read_nested(&wnd->rw_lock, BITMAP_MUTEX_CLUSTERS);\n \n-\tfor (; iw < wnd->nbits; iw++, wbit = 0) {\n+\tfor (; iw < wnd->nwnd; iw++, wbit = 0) {\n \t\tCLST lcn_wnd = iw * wbits;\n \t\tstruct buffer_head *bh;\n ",
        "function_modified_lines": {
            "added": [
                "\tfor (; iw < wnd->nwnd; iw++, wbit = 0) {"
            ],
            "deleted": [
                "\tfor (; iw < wnd->nbits; iw++, wbit = 0) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 6.0.8, there is a use-after-free in ntfs_trim_fs in fs/ntfs3/bitmap.c.",
        "id": 3972
    },
    {
        "cve_id": "CVE-2020-29660",
        "code_before_change": "void __do_SAK(struct tty_struct *tty)\n{\n#ifdef TTY_SOFT_SAK\n\ttty_hangup(tty);\n#else\n\tstruct task_struct *g, *p;\n\tstruct pid *session;\n\tint\t\ti;\n\n\tif (!tty)\n\t\treturn;\n\tsession = tty->session;\n\n\ttty_ldisc_flush(tty);\n\n\ttty_driver_flush_buffer(tty);\n\n\tread_lock(&tasklist_lock);\n\t/* Kill the entire session */\n\tdo_each_pid_task(session, PIDTYPE_SID, p) {\n\t\ttty_notice(tty, \"SAK: killed process %d (%s): by session\\n\",\n\t\t\t   task_pid_nr(p), p->comm);\n\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t} while_each_pid_task(session, PIDTYPE_SID, p);\n\n\t/* Now kill any processes that happen to have the tty open */\n\tdo_each_thread(g, p) {\n\t\tif (p->signal->tty == tty) {\n\t\t\ttty_notice(tty, \"SAK: killed process %d (%s): by controlling tty\\n\",\n\t\t\t\t   task_pid_nr(p), p->comm);\n\t\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t\t\tcontinue;\n\t\t}\n\t\ttask_lock(p);\n\t\ti = iterate_fd(p->files, 0, this_tty, tty);\n\t\tif (i != 0) {\n\t\t\ttty_notice(tty, \"SAK: killed process %d (%s): by fd#%d\\n\",\n\t\t\t\t   task_pid_nr(p), p->comm, i - 1);\n\t\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t\t}\n\t\ttask_unlock(p);\n\t} while_each_thread(g, p);\n\tread_unlock(&tasklist_lock);\n#endif\n}",
        "code_after_change": "void __do_SAK(struct tty_struct *tty)\n{\n#ifdef TTY_SOFT_SAK\n\ttty_hangup(tty);\n#else\n\tstruct task_struct *g, *p;\n\tstruct pid *session;\n\tint\t\ti;\n\tunsigned long flags;\n\n\tif (!tty)\n\t\treturn;\n\n\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\tsession = get_pid(tty->session);\n\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\n\ttty_ldisc_flush(tty);\n\n\ttty_driver_flush_buffer(tty);\n\n\tread_lock(&tasklist_lock);\n\t/* Kill the entire session */\n\tdo_each_pid_task(session, PIDTYPE_SID, p) {\n\t\ttty_notice(tty, \"SAK: killed process %d (%s): by session\\n\",\n\t\t\t   task_pid_nr(p), p->comm);\n\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t} while_each_pid_task(session, PIDTYPE_SID, p);\n\n\t/* Now kill any processes that happen to have the tty open */\n\tdo_each_thread(g, p) {\n\t\tif (p->signal->tty == tty) {\n\t\t\ttty_notice(tty, \"SAK: killed process %d (%s): by controlling tty\\n\",\n\t\t\t\t   task_pid_nr(p), p->comm);\n\t\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t\t\tcontinue;\n\t\t}\n\t\ttask_lock(p);\n\t\ti = iterate_fd(p->files, 0, this_tty, tty);\n\t\tif (i != 0) {\n\t\t\ttty_notice(tty, \"SAK: killed process %d (%s): by fd#%d\\n\",\n\t\t\t\t   task_pid_nr(p), p->comm, i - 1);\n\t\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t\t}\n\t\ttask_unlock(p);\n\t} while_each_thread(g, p);\n\tread_unlock(&tasklist_lock);\n\tput_pid(session);\n#endif\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,10 +6,14 @@\n \tstruct task_struct *g, *p;\n \tstruct pid *session;\n \tint\t\ti;\n+\tunsigned long flags;\n \n \tif (!tty)\n \t\treturn;\n-\tsession = tty->session;\n+\n+\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n+\tsession = get_pid(tty->session);\n+\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n \n \ttty_ldisc_flush(tty);\n \n@@ -41,5 +45,6 @@\n \t\ttask_unlock(p);\n \t} while_each_thread(g, p);\n \tread_unlock(&tasklist_lock);\n+\tput_pid(session);\n #endif\n }",
        "function_modified_lines": {
            "added": [
                "\tunsigned long flags;",
                "",
                "\tspin_lock_irqsave(&tty->ctrl_lock, flags);",
                "\tsession = get_pid(tty->session);",
                "\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);",
                "\tput_pid(session);"
            ],
            "deleted": [
                "\tsession = tty->session;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "A locking inconsistency issue was discovered in the tty subsystem of the Linux kernel through 5.9.13. drivers/tty/tty_io.c and drivers/tty/tty_jobctrl.c may allow a read-after-free attack against TIOCGSID, aka CID-c8bcd9c5be24.",
        "id": 2700
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int get_info(struct net *net, void __user *user, const int *len)\n{\n\tchar name[XT_TABLE_MAXNAMELEN];\n\tstruct xt_table *t;\n\tint ret;\n\n\tif (*len != sizeof(struct arpt_getinfo))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(name, user, sizeof(name)) != 0)\n\t\treturn -EFAULT;\n\n\tname[XT_TABLE_MAXNAMELEN-1] = '\\0';\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_lock(NFPROTO_ARP);\n#endif\n\tt = xt_request_find_table_lock(net, NFPROTO_ARP, name);\n\tif (!IS_ERR(t)) {\n\t\tstruct arpt_getinfo info;\n\t\tconst struct xt_table_info *private = t->private;\n#ifdef CONFIG_COMPAT\n\t\tstruct xt_table_info tmp;\n\n\t\tif (in_compat_syscall()) {\n\t\t\tret = compat_table_info(private, &tmp);\n\t\t\txt_compat_flush_offsets(NFPROTO_ARP);\n\t\t\tprivate = &tmp;\n\t\t}\n#endif\n\t\tmemset(&info, 0, sizeof(info));\n\t\tinfo.valid_hooks = t->valid_hooks;\n\t\tmemcpy(info.hook_entry, private->hook_entry,\n\t\t       sizeof(info.hook_entry));\n\t\tmemcpy(info.underflow, private->underflow,\n\t\t       sizeof(info.underflow));\n\t\tinfo.num_entries = private->number;\n\t\tinfo.size = private->size;\n\t\tstrcpy(info.name, name);\n\n\t\tif (copy_to_user(user, &info, *len) != 0)\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\t\txt_table_unlock(t);\n\t\tmodule_put(t->me);\n\t} else\n\t\tret = PTR_ERR(t);\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_unlock(NFPROTO_ARP);\n#endif\n\treturn ret;\n}",
        "code_after_change": "static int get_info(struct net *net, void __user *user, const int *len)\n{\n\tchar name[XT_TABLE_MAXNAMELEN];\n\tstruct xt_table *t;\n\tint ret;\n\n\tif (*len != sizeof(struct arpt_getinfo))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(name, user, sizeof(name)) != 0)\n\t\treturn -EFAULT;\n\n\tname[XT_TABLE_MAXNAMELEN-1] = '\\0';\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_lock(NFPROTO_ARP);\n#endif\n\tt = xt_request_find_table_lock(net, NFPROTO_ARP, name);\n\tif (!IS_ERR(t)) {\n\t\tstruct arpt_getinfo info;\n\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n#ifdef CONFIG_COMPAT\n\t\tstruct xt_table_info tmp;\n\n\t\tif (in_compat_syscall()) {\n\t\t\tret = compat_table_info(private, &tmp);\n\t\t\txt_compat_flush_offsets(NFPROTO_ARP);\n\t\t\tprivate = &tmp;\n\t\t}\n#endif\n\t\tmemset(&info, 0, sizeof(info));\n\t\tinfo.valid_hooks = t->valid_hooks;\n\t\tmemcpy(info.hook_entry, private->hook_entry,\n\t\t       sizeof(info.hook_entry));\n\t\tmemcpy(info.underflow, private->underflow,\n\t\t       sizeof(info.underflow));\n\t\tinfo.num_entries = private->number;\n\t\tinfo.size = private->size;\n\t\tstrcpy(info.name, name);\n\n\t\tif (copy_to_user(user, &info, *len) != 0)\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\t\txt_table_unlock(t);\n\t\tmodule_put(t->me);\n\t} else\n\t\tret = PTR_ERR(t);\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_unlock(NFPROTO_ARP);\n#endif\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,7 @@\n \tt = xt_request_find_table_lock(net, NFPROTO_ARP, name);\n \tif (!IS_ERR(t)) {\n \t\tstruct arpt_getinfo info;\n-\t\tconst struct xt_table_info *private = t->private;\n+\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n #ifdef CONFIG_COMPAT\n \t\tstruct xt_table_info tmp;\n ",
        "function_modified_lines": {
            "added": [
                "\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);"
            ],
            "deleted": [
                "\t\tconst struct xt_table_info *private = t->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2772
    },
    {
        "cve_id": "CVE-2020-27820",
        "code_before_change": "void\nnouveau_drm_device_remove(struct drm_device *dev)\n{\n\tstruct nouveau_drm *drm = nouveau_drm(dev);\n\tstruct nvkm_client *client;\n\tstruct nvkm_device *device;\n\n\tdrm_dev_unregister(dev);\n\n\tclient = nvxx_client(&drm->client.base);\n\tdevice = nvkm_device_find(client->device);\n\n\tnouveau_drm_device_fini(dev);\n\tdrm_dev_put(dev);\n\tnvkm_device_del(&device);\n}",
        "code_after_change": "void\nnouveau_drm_device_remove(struct drm_device *dev)\n{\n\tstruct nouveau_drm *drm = nouveau_drm(dev);\n\tstruct nvkm_client *client;\n\tstruct nvkm_device *device;\n\n\tdrm_dev_unplug(dev);\n\n\tclient = nvxx_client(&drm->client.base);\n\tdevice = nvkm_device_find(client->device);\n\n\tnouveau_drm_device_fini(dev);\n\tdrm_dev_put(dev);\n\tnvkm_device_del(&device);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \tstruct nvkm_client *client;\n \tstruct nvkm_device *device;\n \n-\tdrm_dev_unregister(dev);\n+\tdrm_dev_unplug(dev);\n \n \tclient = nvxx_client(&drm->client.base);\n \tdevice = nvkm_device_find(client->device);",
        "function_modified_lines": {
            "added": [
                "\tdrm_dev_unplug(dev);"
            ],
            "deleted": [
                "\tdrm_dev_unregister(dev);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux kernel, where a use-after-frees in nouveau's postclose() handler could happen if removing device (that is not common to remove video card physically without power-off, but same happens if \"unbind\" the driver).",
        "id": 2636
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int get_info(struct net *net, void __user *user, const int *len)\n{\n\tchar name[XT_TABLE_MAXNAMELEN];\n\tstruct xt_table *t;\n\tint ret;\n\n\tif (*len != sizeof(struct ipt_getinfo))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(name, user, sizeof(name)) != 0)\n\t\treturn -EFAULT;\n\n\tname[XT_TABLE_MAXNAMELEN-1] = '\\0';\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_lock(AF_INET);\n#endif\n\tt = xt_request_find_table_lock(net, AF_INET, name);\n\tif (!IS_ERR(t)) {\n\t\tstruct ipt_getinfo info;\n\t\tconst struct xt_table_info *private = t->private;\n#ifdef CONFIG_COMPAT\n\t\tstruct xt_table_info tmp;\n\n\t\tif (in_compat_syscall()) {\n\t\t\tret = compat_table_info(private, &tmp);\n\t\t\txt_compat_flush_offsets(AF_INET);\n\t\t\tprivate = &tmp;\n\t\t}\n#endif\n\t\tmemset(&info, 0, sizeof(info));\n\t\tinfo.valid_hooks = t->valid_hooks;\n\t\tmemcpy(info.hook_entry, private->hook_entry,\n\t\t       sizeof(info.hook_entry));\n\t\tmemcpy(info.underflow, private->underflow,\n\t\t       sizeof(info.underflow));\n\t\tinfo.num_entries = private->number;\n\t\tinfo.size = private->size;\n\t\tstrcpy(info.name, name);\n\n\t\tif (copy_to_user(user, &info, *len) != 0)\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\n\t\txt_table_unlock(t);\n\t\tmodule_put(t->me);\n\t} else\n\t\tret = PTR_ERR(t);\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_unlock(AF_INET);\n#endif\n\treturn ret;\n}",
        "code_after_change": "static int get_info(struct net *net, void __user *user, const int *len)\n{\n\tchar name[XT_TABLE_MAXNAMELEN];\n\tstruct xt_table *t;\n\tint ret;\n\n\tif (*len != sizeof(struct ipt_getinfo))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(name, user, sizeof(name)) != 0)\n\t\treturn -EFAULT;\n\n\tname[XT_TABLE_MAXNAMELEN-1] = '\\0';\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_lock(AF_INET);\n#endif\n\tt = xt_request_find_table_lock(net, AF_INET, name);\n\tif (!IS_ERR(t)) {\n\t\tstruct ipt_getinfo info;\n\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n#ifdef CONFIG_COMPAT\n\t\tstruct xt_table_info tmp;\n\n\t\tif (in_compat_syscall()) {\n\t\t\tret = compat_table_info(private, &tmp);\n\t\t\txt_compat_flush_offsets(AF_INET);\n\t\t\tprivate = &tmp;\n\t\t}\n#endif\n\t\tmemset(&info, 0, sizeof(info));\n\t\tinfo.valid_hooks = t->valid_hooks;\n\t\tmemcpy(info.hook_entry, private->hook_entry,\n\t\t       sizeof(info.hook_entry));\n\t\tmemcpy(info.underflow, private->underflow,\n\t\t       sizeof(info.underflow));\n\t\tinfo.num_entries = private->number;\n\t\tinfo.size = private->size;\n\t\tstrcpy(info.name, name);\n\n\t\tif (copy_to_user(user, &info, *len) != 0)\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\n\t\txt_table_unlock(t);\n\t\tmodule_put(t->me);\n\t} else\n\t\tret = PTR_ERR(t);\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_unlock(AF_INET);\n#endif\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,7 @@\n \tt = xt_request_find_table_lock(net, AF_INET, name);\n \tif (!IS_ERR(t)) {\n \t\tstruct ipt_getinfo info;\n-\t\tconst struct xt_table_info *private = t->private;\n+\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n #ifdef CONFIG_COMPAT\n \t\tstruct xt_table_info tmp;\n ",
        "function_modified_lines": {
            "added": [
                "\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);"
            ],
            "deleted": [
                "\t\tconst struct xt_table_info *private = t->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2780
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static int io_uring_show_cred(int id, void *p, void *data)\n{\n\tstruct io_identity *iod = p;\n\tconst struct cred *cred = iod->creds;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}",
        "code_after_change": "static int io_uring_show_cred(int id, void *p, void *data)\n{\n\tconst struct cred *cred = p;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,6 @@\n static int io_uring_show_cred(int id, void *p, void *data)\n {\n-\tstruct io_identity *iod = p;\n-\tconst struct cred *cred = iod->creds;\n+\tconst struct cred *cred = p;\n \tstruct seq_file *m = data;\n \tstruct user_namespace *uns = seq_user_ns(m);\n \tstruct group_info *gi;",
        "function_modified_lines": {
            "added": [
                "\tconst struct cred *cred = p;"
            ],
            "deleted": [
                "\tstruct io_identity *iod = p;",
                "\tconst struct cred *cred = iod->creds;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3356
    },
    {
        "cve_id": "CVE-2019-15220",
        "code_before_change": "static void p54u_load_firmware_cb(const struct firmware *firmware,\n\t\t\t\t  void *context)\n{\n\tstruct p54u_priv *priv = context;\n\tstruct usb_device *udev = priv->udev;\n\tint err;\n\n\tcomplete(&priv->fw_wait_load);\n\tif (firmware) {\n\t\tpriv->fw = firmware;\n\t\terr = p54u_start_ops(priv);\n\t} else {\n\t\terr = -ENOENT;\n\t\tdev_err(&udev->dev, \"Firmware not found.\\n\");\n\t}\n\n\tif (err) {\n\t\tstruct device *parent = priv->udev->dev.parent;\n\n\t\tdev_err(&udev->dev, \"failed to initialize device (%d)\\n\", err);\n\n\t\tif (parent)\n\t\t\tdevice_lock(parent);\n\n\t\tdevice_release_driver(&udev->dev);\n\t\t/*\n\t\t * At this point p54u_disconnect has already freed\n\t\t * the \"priv\" context. Do not use it anymore!\n\t\t */\n\t\tpriv = NULL;\n\n\t\tif (parent)\n\t\t\tdevice_unlock(parent);\n\t}\n\n\tusb_put_dev(udev);\n}",
        "code_after_change": "static void p54u_load_firmware_cb(const struct firmware *firmware,\n\t\t\t\t  void *context)\n{\n\tstruct p54u_priv *priv = context;\n\tstruct usb_device *udev = priv->udev;\n\tstruct usb_interface *intf = priv->intf;\n\tint err;\n\n\tif (firmware) {\n\t\tpriv->fw = firmware;\n\t\terr = p54u_start_ops(priv);\n\t} else {\n\t\terr = -ENOENT;\n\t\tdev_err(&udev->dev, \"Firmware not found.\\n\");\n\t}\n\n\tcomplete(&priv->fw_wait_load);\n\t/*\n\t * At this point p54u_disconnect may have already freed\n\t * the \"priv\" context. Do not use it anymore!\n\t */\n\tpriv = NULL;\n\n\tif (err) {\n\t\tdev_err(&intf->dev, \"failed to initialize device (%d)\\n\", err);\n\n\t\tusb_lock_device(udev);\n\t\tusb_driver_release_interface(&p54u_driver, intf);\n\t\tusb_unlock_device(udev);\n\t}\n\n\tusb_put_intf(intf);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,9 +3,9 @@\n {\n \tstruct p54u_priv *priv = context;\n \tstruct usb_device *udev = priv->udev;\n+\tstruct usb_interface *intf = priv->intf;\n \tint err;\n \n-\tcomplete(&priv->fw_wait_load);\n \tif (firmware) {\n \t\tpriv->fw = firmware;\n \t\terr = p54u_start_ops(priv);\n@@ -14,24 +14,20 @@\n \t\tdev_err(&udev->dev, \"Firmware not found.\\n\");\n \t}\n \n+\tcomplete(&priv->fw_wait_load);\n+\t/*\n+\t * At this point p54u_disconnect may have already freed\n+\t * the \"priv\" context. Do not use it anymore!\n+\t */\n+\tpriv = NULL;\n+\n \tif (err) {\n-\t\tstruct device *parent = priv->udev->dev.parent;\n+\t\tdev_err(&intf->dev, \"failed to initialize device (%d)\\n\", err);\n \n-\t\tdev_err(&udev->dev, \"failed to initialize device (%d)\\n\", err);\n-\n-\t\tif (parent)\n-\t\t\tdevice_lock(parent);\n-\n-\t\tdevice_release_driver(&udev->dev);\n-\t\t/*\n-\t\t * At this point p54u_disconnect has already freed\n-\t\t * the \"priv\" context. Do not use it anymore!\n-\t\t */\n-\t\tpriv = NULL;\n-\n-\t\tif (parent)\n-\t\t\tdevice_unlock(parent);\n+\t\tusb_lock_device(udev);\n+\t\tusb_driver_release_interface(&p54u_driver, intf);\n+\t\tusb_unlock_device(udev);\n \t}\n \n-\tusb_put_dev(udev);\n+\tusb_put_intf(intf);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct usb_interface *intf = priv->intf;",
                "\tcomplete(&priv->fw_wait_load);",
                "\t/*",
                "\t * At this point p54u_disconnect may have already freed",
                "\t * the \"priv\" context. Do not use it anymore!",
                "\t */",
                "\tpriv = NULL;",
                "",
                "\t\tdev_err(&intf->dev, \"failed to initialize device (%d)\\n\", err);",
                "\t\tusb_lock_device(udev);",
                "\t\tusb_driver_release_interface(&p54u_driver, intf);",
                "\t\tusb_unlock_device(udev);",
                "\tusb_put_intf(intf);"
            ],
            "deleted": [
                "\tcomplete(&priv->fw_wait_load);",
                "\t\tstruct device *parent = priv->udev->dev.parent;",
                "\t\tdev_err(&udev->dev, \"failed to initialize device (%d)\\n\", err);",
                "",
                "\t\tif (parent)",
                "\t\t\tdevice_lock(parent);",
                "",
                "\t\tdevice_release_driver(&udev->dev);",
                "\t\t/*",
                "\t\t * At this point p54u_disconnect has already freed",
                "\t\t * the \"priv\" context. Do not use it anymore!",
                "\t\t */",
                "\t\tpriv = NULL;",
                "",
                "\t\tif (parent)",
                "\t\t\tdevice_unlock(parent);",
                "\tusb_put_dev(udev);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.2.1. There is a use-after-free caused by a malicious USB device in the drivers/net/wireless/intersil/p54/p54usb.c driver.",
        "id": 2002
    },
    {
        "cve_id": "CVE-2016-9120",
        "code_before_change": "static struct ion_handle *ion_handle_get_by_id(struct ion_client *client,\n\t\t\t\t\t\tint id)\n{\n\tstruct ion_handle *handle;\n\n\tmutex_lock(&client->lock);\n\thandle = idr_find(&client->idr, id);\n\tif (handle)\n\t\tion_handle_get(handle);\n\tmutex_unlock(&client->lock);\n\n\treturn handle ? handle : ERR_PTR(-EINVAL);\n}",
        "code_after_change": "struct ion_handle *ion_handle_get_by_id(struct ion_client *client,\n\t\t\t\t\t\tint id)\n{\n\tstruct ion_handle *handle;\n\n\tmutex_lock(&client->lock);\n\thandle = ion_handle_get_by_id_nolock(client, id);\n\tmutex_unlock(&client->lock);\n\n\treturn handle;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,13 +1,11 @@\n-static struct ion_handle *ion_handle_get_by_id(struct ion_client *client,\n+struct ion_handle *ion_handle_get_by_id(struct ion_client *client,\n \t\t\t\t\t\tint id)\n {\n \tstruct ion_handle *handle;\n \n \tmutex_lock(&client->lock);\n-\thandle = idr_find(&client->idr, id);\n-\tif (handle)\n-\t\tion_handle_get(handle);\n+\thandle = ion_handle_get_by_id_nolock(client, id);\n \tmutex_unlock(&client->lock);\n \n-\treturn handle ? handle : ERR_PTR(-EINVAL);\n+\treturn handle;\n }",
        "function_modified_lines": {
            "added": [
                "struct ion_handle *ion_handle_get_by_id(struct ion_client *client,",
                "\thandle = ion_handle_get_by_id_nolock(client, id);",
                "\treturn handle;"
            ],
            "deleted": [
                "static struct ion_handle *ion_handle_get_by_id(struct ion_client *client,",
                "\thandle = idr_find(&client->idr, id);",
                "\tif (handle)",
                "\t\tion_handle_get(handle);",
                "\treturn handle ? handle : ERR_PTR(-EINVAL);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time.",
        "id": 1142
    },
    {
        "cve_id": "CVE-2019-19813",
        "code_before_change": "struct extent_map *btrfs_get_extent(struct btrfs_inode *inode,\n\t\t\t\t    struct page *page,\n\t\t\t\t    size_t pg_offset, u64 start, u64 len,\n\t\t\t\t    int create)\n{\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tint ret;\n\tint err = 0;\n\tu64 extent_start = 0;\n\tu64 extent_end = 0;\n\tu64 objectid = btrfs_ino(inode);\n\tu8 extent_type;\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_root *root = inode->root;\n\tstruct btrfs_file_extent_item *item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key found_key;\n\tstruct extent_map *em = NULL;\n\tstruct extent_map_tree *em_tree = &inode->extent_tree;\n\tstruct extent_io_tree *io_tree = &inode->io_tree;\n\tconst bool new_inline = !page || create;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, start, len);\n\tif (em)\n\t\tem->bdev = fs_info->fs_devices->latest_bdev;\n\tread_unlock(&em_tree->lock);\n\n\tif (em) {\n\t\tif (em->start > start || em->start + em->len <= start)\n\t\t\tfree_extent_map(em);\n\t\telse if (em->block_start == EXTENT_MAP_INLINE && page)\n\t\t\tfree_extent_map(em);\n\t\telse\n\t\t\tgoto out;\n\t}\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tem->bdev = fs_info->fs_devices->latest_bdev;\n\tem->start = EXTENT_MAP_HOLE;\n\tem->orig_start = EXTENT_MAP_HOLE;\n\tem->len = (u64)-1;\n\tem->block_len = (u64)-1;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Chances are we'll be called again, so go ahead and do readahead */\n\tpath->reada = READA_FORWARD;\n\n\t/*\n\t * Unless we're going to uncompress the inline extent, no sleep would\n\t * happen.\n\t */\n\tpath->leave_spinning = 1;\n\n\tret = btrfs_lookup_file_extent(NULL, root, path, objectid, start, 0);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t} else if (ret > 0) {\n\t\tif (path->slots[0] == 0)\n\t\t\tgoto not_found;\n\t\tpath->slots[0]--;\n\t}\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t      struct btrfs_file_extent_item);\n\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\tif (found_key.objectid != objectid ||\n\t    found_key.type != BTRFS_EXTENT_DATA_KEY) {\n\t\t/*\n\t\t * If we backup past the first extent we want to move forward\n\t\t * and see if there is an extent in front of us, otherwise we'll\n\t\t * say there is a hole for our whole search range which can\n\t\t * cause problems.\n\t\t */\n\t\textent_end = start;\n\t\tgoto next;\n\t}\n\n\textent_type = btrfs_file_extent_type(leaf, item);\n\textent_start = found_key.offset;\n\tif (extent_type == BTRFS_FILE_EXTENT_REG ||\n\t    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\textent_end = extent_start +\n\t\t       btrfs_file_extent_num_bytes(leaf, item);\n\n\t\ttrace_btrfs_get_extent_show_fi_regular(inode, leaf, item,\n\t\t\t\t\t\t       extent_start);\n\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tsize_t size;\n\n\t\tsize = btrfs_file_extent_ram_bytes(leaf, item);\n\t\textent_end = ALIGN(extent_start + size,\n\t\t\t\t   fs_info->sectorsize);\n\n\t\ttrace_btrfs_get_extent_show_fi_inline(inode, leaf, item,\n\t\t\t\t\t\t      path->slots[0],\n\t\t\t\t\t\t      extent_start);\n\t}\nnext:\n\tif (start >= extent_end) {\n\t\tpath->slots[0]++;\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0) {\n\t\t\t\terr = ret;\n\t\t\t\tgoto out;\n\t\t\t} else if (ret > 0) {\n\t\t\t\tgoto not_found;\n\t\t\t}\n\t\t\tleaf = path->nodes[0];\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\tif (found_key.objectid != objectid ||\n\t\t    found_key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto not_found;\n\t\tif (start + len <= found_key.offset)\n\t\t\tgoto not_found;\n\t\tif (start > found_key.offset)\n\t\t\tgoto next;\n\n\t\t/* New extent overlaps with existing one */\n\t\tem->start = start;\n\t\tem->orig_start = start;\n\t\tem->len = found_key.offset - start;\n\t\tem->block_start = EXTENT_MAP_HOLE;\n\t\tgoto insert;\n\t}\n\n\tbtrfs_extent_item_to_extent_map(inode, path, item,\n\t\t\tnew_inline, em);\n\n\tif (extent_type == BTRFS_FILE_EXTENT_REG ||\n\t    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\tgoto insert;\n\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tunsigned long ptr;\n\t\tchar *map;\n\t\tsize_t size;\n\t\tsize_t extent_offset;\n\t\tsize_t copy_size;\n\n\t\tif (new_inline)\n\t\t\tgoto out;\n\n\t\tsize = btrfs_file_extent_ram_bytes(leaf, item);\n\t\textent_offset = page_offset(page) + pg_offset - extent_start;\n\t\tcopy_size = min_t(u64, PAGE_SIZE - pg_offset,\n\t\t\t\t  size - extent_offset);\n\t\tem->start = extent_start + extent_offset;\n\t\tem->len = ALIGN(copy_size, fs_info->sectorsize);\n\t\tem->orig_block_len = em->len;\n\t\tem->orig_start = em->start;\n\t\tptr = btrfs_file_extent_inline_start(item) + extent_offset;\n\n\t\tbtrfs_set_path_blocking(path);\n\t\tif (!PageUptodate(page)) {\n\t\t\tif (btrfs_file_extent_compression(leaf, item) !=\n\t\t\t    BTRFS_COMPRESS_NONE) {\n\t\t\t\tret = uncompress_inline(path, page, pg_offset,\n\t\t\t\t\t\t\textent_offset, item);\n\t\t\t\tif (ret) {\n\t\t\t\t\terr = ret;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tmap = kmap(page);\n\t\t\t\tread_extent_buffer(leaf, map + pg_offset, ptr,\n\t\t\t\t\t\t   copy_size);\n\t\t\t\tif (pg_offset + copy_size < PAGE_SIZE) {\n\t\t\t\t\tmemset(map + pg_offset + copy_size, 0,\n\t\t\t\t\t       PAGE_SIZE - pg_offset -\n\t\t\t\t\t       copy_size);\n\t\t\t\t}\n\t\t\t\tkunmap(page);\n\t\t\t}\n\t\t\tflush_dcache_page(page);\n\t\t}\n\t\tset_extent_uptodate(io_tree, em->start,\n\t\t\t\t    extent_map_end(em) - 1, NULL, GFP_NOFS);\n\t\tgoto insert;\n\t}\nnot_found:\n\tem->start = start;\n\tem->orig_start = start;\n\tem->len = len;\n\tem->block_start = EXTENT_MAP_HOLE;\ninsert:\n\tbtrfs_release_path(path);\n\tif (em->start > start || extent_map_end(em) <= start) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"bad extent! em: [%llu %llu] passed [%llu %llu]\",\n\t\t\t  em->start, em->len, start, len);\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\terr = 0;\n\twrite_lock(&em_tree->lock);\n\terr = btrfs_add_extent_mapping(fs_info, em_tree, &em, start, len);\n\twrite_unlock(&em_tree->lock);\nout:\n\tbtrfs_free_path(path);\n\n\ttrace_btrfs_get_extent(root, inode, em);\n\n\tif (err) {\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(err);\n\t}\n\tBUG_ON(!em); /* Error is always set */\n\treturn em;\n}",
        "code_after_change": "struct extent_map *btrfs_get_extent(struct btrfs_inode *inode,\n\t\t\t\t    struct page *page,\n\t\t\t\t    size_t pg_offset, u64 start, u64 len,\n\t\t\t\t    int create)\n{\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tint ret;\n\tint err = 0;\n\tu64 extent_start = 0;\n\tu64 extent_end = 0;\n\tu64 objectid = btrfs_ino(inode);\n\tu8 extent_type;\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_root *root = inode->root;\n\tstruct btrfs_file_extent_item *item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key found_key;\n\tstruct extent_map *em = NULL;\n\tstruct extent_map_tree *em_tree = &inode->extent_tree;\n\tstruct extent_io_tree *io_tree = &inode->io_tree;\n\tconst bool new_inline = !page || create;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, start, len);\n\tif (em)\n\t\tem->bdev = fs_info->fs_devices->latest_bdev;\n\tread_unlock(&em_tree->lock);\n\n\tif (em) {\n\t\tif (em->start > start || em->start + em->len <= start)\n\t\t\tfree_extent_map(em);\n\t\telse if (em->block_start == EXTENT_MAP_INLINE && page)\n\t\t\tfree_extent_map(em);\n\t\telse\n\t\t\tgoto out;\n\t}\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tem->bdev = fs_info->fs_devices->latest_bdev;\n\tem->start = EXTENT_MAP_HOLE;\n\tem->orig_start = EXTENT_MAP_HOLE;\n\tem->len = (u64)-1;\n\tem->block_len = (u64)-1;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Chances are we'll be called again, so go ahead and do readahead */\n\tpath->reada = READA_FORWARD;\n\n\t/*\n\t * Unless we're going to uncompress the inline extent, no sleep would\n\t * happen.\n\t */\n\tpath->leave_spinning = 1;\n\n\tret = btrfs_lookup_file_extent(NULL, root, path, objectid, start, 0);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t} else if (ret > 0) {\n\t\tif (path->slots[0] == 0)\n\t\t\tgoto not_found;\n\t\tpath->slots[0]--;\n\t}\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t      struct btrfs_file_extent_item);\n\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\tif (found_key.objectid != objectid ||\n\t    found_key.type != BTRFS_EXTENT_DATA_KEY) {\n\t\t/*\n\t\t * If we backup past the first extent we want to move forward\n\t\t * and see if there is an extent in front of us, otherwise we'll\n\t\t * say there is a hole for our whole search range which can\n\t\t * cause problems.\n\t\t */\n\t\textent_end = start;\n\t\tgoto next;\n\t}\n\n\textent_type = btrfs_file_extent_type(leaf, item);\n\textent_start = found_key.offset;\n\tif (extent_type == BTRFS_FILE_EXTENT_REG ||\n\t    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t/* Only regular file could have regular/prealloc extent */\n\t\tif (!S_ISREG(inode->vfs_inode.i_mode)) {\n\t\t\tret = -EUCLEAN;\n\t\t\tbtrfs_crit(fs_info,\n\t\t\"regular/prealloc extent found for non-regular inode %llu\",\n\t\t\t\t   btrfs_ino(inode));\n\t\t\tgoto out;\n\t\t}\n\t\textent_end = extent_start +\n\t\t       btrfs_file_extent_num_bytes(leaf, item);\n\n\t\ttrace_btrfs_get_extent_show_fi_regular(inode, leaf, item,\n\t\t\t\t\t\t       extent_start);\n\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tsize_t size;\n\n\t\tsize = btrfs_file_extent_ram_bytes(leaf, item);\n\t\textent_end = ALIGN(extent_start + size,\n\t\t\t\t   fs_info->sectorsize);\n\n\t\ttrace_btrfs_get_extent_show_fi_inline(inode, leaf, item,\n\t\t\t\t\t\t      path->slots[0],\n\t\t\t\t\t\t      extent_start);\n\t}\nnext:\n\tif (start >= extent_end) {\n\t\tpath->slots[0]++;\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0) {\n\t\t\t\terr = ret;\n\t\t\t\tgoto out;\n\t\t\t} else if (ret > 0) {\n\t\t\t\tgoto not_found;\n\t\t\t}\n\t\t\tleaf = path->nodes[0];\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\tif (found_key.objectid != objectid ||\n\t\t    found_key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto not_found;\n\t\tif (start + len <= found_key.offset)\n\t\t\tgoto not_found;\n\t\tif (start > found_key.offset)\n\t\t\tgoto next;\n\n\t\t/* New extent overlaps with existing one */\n\t\tem->start = start;\n\t\tem->orig_start = start;\n\t\tem->len = found_key.offset - start;\n\t\tem->block_start = EXTENT_MAP_HOLE;\n\t\tgoto insert;\n\t}\n\n\tbtrfs_extent_item_to_extent_map(inode, path, item,\n\t\t\tnew_inline, em);\n\n\tif (extent_type == BTRFS_FILE_EXTENT_REG ||\n\t    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\tgoto insert;\n\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tunsigned long ptr;\n\t\tchar *map;\n\t\tsize_t size;\n\t\tsize_t extent_offset;\n\t\tsize_t copy_size;\n\n\t\tif (new_inline)\n\t\t\tgoto out;\n\n\t\tsize = btrfs_file_extent_ram_bytes(leaf, item);\n\t\textent_offset = page_offset(page) + pg_offset - extent_start;\n\t\tcopy_size = min_t(u64, PAGE_SIZE - pg_offset,\n\t\t\t\t  size - extent_offset);\n\t\tem->start = extent_start + extent_offset;\n\t\tem->len = ALIGN(copy_size, fs_info->sectorsize);\n\t\tem->orig_block_len = em->len;\n\t\tem->orig_start = em->start;\n\t\tptr = btrfs_file_extent_inline_start(item) + extent_offset;\n\n\t\tbtrfs_set_path_blocking(path);\n\t\tif (!PageUptodate(page)) {\n\t\t\tif (btrfs_file_extent_compression(leaf, item) !=\n\t\t\t    BTRFS_COMPRESS_NONE) {\n\t\t\t\tret = uncompress_inline(path, page, pg_offset,\n\t\t\t\t\t\t\textent_offset, item);\n\t\t\t\tif (ret) {\n\t\t\t\t\terr = ret;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tmap = kmap(page);\n\t\t\t\tread_extent_buffer(leaf, map + pg_offset, ptr,\n\t\t\t\t\t\t   copy_size);\n\t\t\t\tif (pg_offset + copy_size < PAGE_SIZE) {\n\t\t\t\t\tmemset(map + pg_offset + copy_size, 0,\n\t\t\t\t\t       PAGE_SIZE - pg_offset -\n\t\t\t\t\t       copy_size);\n\t\t\t\t}\n\t\t\t\tkunmap(page);\n\t\t\t}\n\t\t\tflush_dcache_page(page);\n\t\t}\n\t\tset_extent_uptodate(io_tree, em->start,\n\t\t\t\t    extent_map_end(em) - 1, NULL, GFP_NOFS);\n\t\tgoto insert;\n\t}\nnot_found:\n\tem->start = start;\n\tem->orig_start = start;\n\tem->len = len;\n\tem->block_start = EXTENT_MAP_HOLE;\ninsert:\n\tbtrfs_release_path(path);\n\tif (em->start > start || extent_map_end(em) <= start) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"bad extent! em: [%llu %llu] passed [%llu %llu]\",\n\t\t\t  em->start, em->len, start, len);\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\terr = 0;\n\twrite_lock(&em_tree->lock);\n\terr = btrfs_add_extent_mapping(fs_info, em_tree, &em, start, len);\n\twrite_unlock(&em_tree->lock);\nout:\n\tbtrfs_free_path(path);\n\n\ttrace_btrfs_get_extent(root, inode, em);\n\n\tif (err) {\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(err);\n\t}\n\tBUG_ON(!em); /* Error is always set */\n\treturn em;\n}",
        "patch": "--- code before\n+++ code after\n@@ -90,6 +90,14 @@\n \textent_start = found_key.offset;\n \tif (extent_type == BTRFS_FILE_EXTENT_REG ||\n \t    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n+\t\t/* Only regular file could have regular/prealloc extent */\n+\t\tif (!S_ISREG(inode->vfs_inode.i_mode)) {\n+\t\t\tret = -EUCLEAN;\n+\t\t\tbtrfs_crit(fs_info,\n+\t\t\"regular/prealloc extent found for non-regular inode %llu\",\n+\t\t\t\t   btrfs_ino(inode));\n+\t\t\tgoto out;\n+\t\t}\n \t\textent_end = extent_start +\n \t\t       btrfs_file_extent_num_bytes(leaf, item);\n ",
        "function_modified_lines": {
            "added": [
                "\t\t/* Only regular file could have regular/prealloc extent */",
                "\t\tif (!S_ISREG(inode->vfs_inode.i_mode)) {",
                "\t\t\tret = -EUCLEAN;",
                "\t\t\tbtrfs_crit(fs_info,",
                "\t\t\"regular/prealloc extent found for non-regular inode %llu\",",
                "\t\t\t\t   btrfs_ino(inode));",
                "\t\t\tgoto out;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.0.21, mounting a crafted btrfs filesystem image, performing some operations, and then making a syncfs system call can lead to a use-after-free in __mutex_lock in kernel/locking/mutex.c. This is related to mutex_can_spin_on_owner in kernel/locking/mutex.c, __btrfs_qgroup_free_meta in fs/btrfs/qgroup.c, and btrfs_insert_delayed_items in fs/btrfs/delayed-inode.c.",
        "id": 2246
    },
    {
        "cve_id": "CVE-2020-0433",
        "code_before_change": "static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,\n\t\t\t\t\t\t\tint nr_hw_queues)\n{\n\tstruct request_queue *q;\n\tLIST_HEAD(head);\n\n\tlockdep_assert_held(&set->tag_list_lock);\n\n\tif (nr_hw_queues > nr_cpu_ids)\n\t\tnr_hw_queues = nr_cpu_ids;\n\tif (nr_hw_queues < 1 || nr_hw_queues == set->nr_hw_queues)\n\t\treturn;\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_freeze_queue(q);\n\t/*\n\t * Switch IO scheduler to 'none', cleaning up the data associated\n\t * with the previous scheduler. We will switch back once we are done\n\t * updating the new sw to hw queue mappings.\n\t */\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tif (!blk_mq_elv_switch_none(&head, q))\n\t\t\tgoto switch_back;\n\n\tset->nr_hw_queues = nr_hw_queues;\n\tblk_mq_update_queue_map(set);\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_realloc_hw_ctxs(set, q);\n\t\tblk_mq_queue_reinit(q);\n\t}\n\nswitch_back:\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_elv_switch_back(&head, q);\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_unfreeze_queue(q);\n}",
        "code_after_change": "static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,\n\t\t\t\t\t\t\tint nr_hw_queues)\n{\n\tstruct request_queue *q;\n\tLIST_HEAD(head);\n\n\tlockdep_assert_held(&set->tag_list_lock);\n\n\tif (nr_hw_queues > nr_cpu_ids)\n\t\tnr_hw_queues = nr_cpu_ids;\n\tif (nr_hw_queues < 1 || nr_hw_queues == set->nr_hw_queues)\n\t\treturn;\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_freeze_queue(q);\n\t/*\n\t * Sync with blk_mq_queue_tag_busy_iter.\n\t */\n\tsynchronize_rcu();\n\t/*\n\t * Switch IO scheduler to 'none', cleaning up the data associated\n\t * with the previous scheduler. We will switch back once we are done\n\t * updating the new sw to hw queue mappings.\n\t */\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tif (!blk_mq_elv_switch_none(&head, q))\n\t\t\tgoto switch_back;\n\n\tset->nr_hw_queues = nr_hw_queues;\n\tblk_mq_update_queue_map(set);\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_realloc_hw_ctxs(set, q);\n\t\tblk_mq_queue_reinit(q);\n\t}\n\nswitch_back:\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_elv_switch_back(&head, q);\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_unfreeze_queue(q);\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,10 @@\n \n \tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n \t\tblk_mq_freeze_queue(q);\n+\t/*\n+\t * Sync with blk_mq_queue_tag_busy_iter.\n+\t */\n+\tsynchronize_rcu();\n \t/*\n \t * Switch IO scheduler to 'none', cleaning up the data associated\n \t * with the previous scheduler. We will switch back once we are done",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * Sync with blk_mq_queue_tag_busy_iter.",
                "\t */",
                "\tsynchronize_rcu();"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In blk_mq_queue_tag_busy_iter of blk-mq-tag.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-151939299",
        "id": 2388
    },
    {
        "cve_id": "CVE-2021-43057",
        "code_before_change": "static int selinux_ptrace_traceme(struct task_struct *parent)\n{\n\treturn avc_has_perm(&selinux_state,\n\t\t\t    task_sid_subj(parent), task_sid_obj(current),\n\t\t\t    SECCLASS_PROCESS, PROCESS__PTRACE, NULL);\n}",
        "code_after_change": "static int selinux_ptrace_traceme(struct task_struct *parent)\n{\n\treturn avc_has_perm(&selinux_state,\n\t\t\t    task_sid_obj(parent), task_sid_obj(current),\n\t\t\t    SECCLASS_PROCESS, PROCESS__PTRACE, NULL);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static int selinux_ptrace_traceme(struct task_struct *parent)\n {\n \treturn avc_has_perm(&selinux_state,\n-\t\t\t    task_sid_subj(parent), task_sid_obj(current),\n+\t\t\t    task_sid_obj(parent), task_sid_obj(current),\n \t\t\t    SECCLASS_PROCESS, PROCESS__PTRACE, NULL);\n }",
        "function_modified_lines": {
            "added": [
                "\t\t\t    task_sid_obj(parent), task_sid_obj(current),"
            ],
            "deleted": [
                "\t\t\t    task_sid_subj(parent), task_sid_obj(current),"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.14.8. A use-after-free in selinux_ptrace_traceme (aka the SELinux handler for PTRACE_TRACEME) could be used by local attackers to cause memory corruption and escalate privileges, aka CID-a3727a8bac0a. This occurs because of an attempt to access the subjective credentials of another task.",
        "id": 3159
    },
    {
        "cve_id": "CVE-2023-3776",
        "code_before_change": "static int fw_set_parms(struct net *net, struct tcf_proto *tp,\n\t\t\tstruct fw_filter *f, struct nlattr **tb,\n\t\t\tstruct nlattr **tca, unsigned long base, u32 flags,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct fw_head *head = rtnl_dereference(tp->root);\n\tu32 mask;\n\tint err;\n\n\terr = tcf_exts_validate(net, tp, tb, tca[TCA_RATE], &f->exts, flags,\n\t\t\t\textack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_FW_CLASSID]) {\n\t\tf->res.classid = nla_get_u32(tb[TCA_FW_CLASSID]);\n\t\ttcf_bind_filter(tp, &f->res, base);\n\t}\n\n\tif (tb[TCA_FW_INDEV]) {\n\t\tint ret;\n\t\tret = tcf_change_indev(net, tb[TCA_FW_INDEV], extack);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tf->ifindex = ret;\n\t}\n\n\terr = -EINVAL;\n\tif (tb[TCA_FW_MASK]) {\n\t\tmask = nla_get_u32(tb[TCA_FW_MASK]);\n\t\tif (mask != head->mask)\n\t\t\treturn err;\n\t} else if (head->mask != 0xFFFFFFFF)\n\t\treturn err;\n\n\treturn 0;\n}",
        "code_after_change": "static int fw_set_parms(struct net *net, struct tcf_proto *tp,\n\t\t\tstruct fw_filter *f, struct nlattr **tb,\n\t\t\tstruct nlattr **tca, unsigned long base, u32 flags,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct fw_head *head = rtnl_dereference(tp->root);\n\tu32 mask;\n\tint err;\n\n\terr = tcf_exts_validate(net, tp, tb, tca[TCA_RATE], &f->exts, flags,\n\t\t\t\textack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_FW_INDEV]) {\n\t\tint ret;\n\t\tret = tcf_change_indev(net, tb[TCA_FW_INDEV], extack);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tf->ifindex = ret;\n\t}\n\n\terr = -EINVAL;\n\tif (tb[TCA_FW_MASK]) {\n\t\tmask = nla_get_u32(tb[TCA_FW_MASK]);\n\t\tif (mask != head->mask)\n\t\t\treturn err;\n\t} else if (head->mask != 0xFFFFFFFF)\n\t\treturn err;\n\n\tif (tb[TCA_FW_CLASSID]) {\n\t\tf->res.classid = nla_get_u32(tb[TCA_FW_CLASSID]);\n\t\ttcf_bind_filter(tp, &f->res, base);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,11 +11,6 @@\n \t\t\t\textack);\n \tif (err < 0)\n \t\treturn err;\n-\n-\tif (tb[TCA_FW_CLASSID]) {\n-\t\tf->res.classid = nla_get_u32(tb[TCA_FW_CLASSID]);\n-\t\ttcf_bind_filter(tp, &f->res, base);\n-\t}\n \n \tif (tb[TCA_FW_INDEV]) {\n \t\tint ret;\n@@ -33,5 +28,10 @@\n \t} else if (head->mask != 0xFFFFFFFF)\n \t\treturn err;\n \n+\tif (tb[TCA_FW_CLASSID]) {\n+\t\tf->res.classid = nla_get_u32(tb[TCA_FW_CLASSID]);\n+\t\ttcf_bind_filter(tp, &f->res, base);\n+\t}\n+\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (tb[TCA_FW_CLASSID]) {",
                "\t\tf->res.classid = nla_get_u32(tb[TCA_FW_CLASSID]);",
                "\t\ttcf_bind_filter(tp, &f->res, base);",
                "\t}",
                ""
            ],
            "deleted": [
                "",
                "\tif (tb[TCA_FW_CLASSID]) {",
                "\t\tf->res.classid = nla_get_u32(tb[TCA_FW_CLASSID]);",
                "\t\ttcf_bind_filter(tp, &f->res, base);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's net/sched: cls_fw component can be exploited to achieve local privilege escalation.\n\nIf tcf_change_indev() fails, fw_set_parms() will immediately return an error after incrementing or decrementing the reference counter in tcf_bind_filter(). If an attacker can control the reference counter and set it to zero, they can cause the reference to be freed, leading to a use-after-free vulnerability.\n\nWe recommend upgrading past commit 0323bce598eea038714f941ce2b22541c46d488f.\n\n",
        "id": 4135
    },
    {
        "cve_id": "CVE-2017-6346",
        "code_before_change": "static int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f, *match;\n\tu8 type = type_flags & 0xff;\n\tu8 flags = type_flags >> 8;\n\tint err;\n\n\tswitch (type) {\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\n\t\t\treturn -EINVAL;\n\tcase PACKET_FANOUT_HASH:\n\tcase PACKET_FANOUT_LB:\n\tcase PACKET_FANOUT_CPU:\n\tcase PACKET_FANOUT_RND:\n\tcase PACKET_FANOUT_QM:\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (!po->running)\n\t\treturn -EINVAL;\n\n\tif (po->fanout)\n\t\treturn -EALREADY;\n\n\tif (type == PACKET_FANOUT_ROLLOVER ||\n\t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n\t\tpo->rollover = kzalloc(sizeof(*po->rollover), GFP_KERNEL);\n\t\tif (!po->rollover)\n\t\t\treturn -ENOMEM;\n\t\tatomic_long_set(&po->rollover->num, 0);\n\t\tatomic_long_set(&po->rollover->num_huge, 0);\n\t\tatomic_long_set(&po->rollover->num_failed, 0);\n\t}\n\n\tmutex_lock(&fanout_mutex);\n\tmatch = NULL;\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\tmatch = f;\n\t\t\tbreak;\n\t\t}\n\t}\n\terr = -EINVAL;\n\tif (match && match->flags != flags)\n\t\tgoto out;\n\tif (!match) {\n\t\terr = -ENOMEM;\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match)\n\t\t\tgoto out;\n\t\twrite_pnet(&match->net, sock_net(sk));\n\t\tmatch->id = id;\n\t\tmatch->type = type;\n\t\tmatch->flags = flags;\n\t\tINIT_LIST_HEAD(&match->list);\n\t\tspin_lock_init(&match->lock);\n\t\tatomic_set(&match->sk_ref, 0);\n\t\tfanout_init_data(match);\n\t\tmatch->prot_hook.type = po->prot_hook.type;\n\t\tmatch->prot_hook.dev = po->prot_hook.dev;\n\t\tmatch->prot_hook.func = packet_rcv_fanout;\n\t\tmatch->prot_hook.af_packet_priv = match;\n\t\tmatch->prot_hook.id_match = match_fanout_group;\n\t\tdev_add_pack(&match->prot_hook);\n\t\tlist_add(&match->list, &fanout_list);\n\t}\n\terr = -EINVAL;\n\tif (match->type == type &&\n\t    match->prot_hook.type == po->prot_hook.type &&\n\t    match->prot_hook.dev == po->prot_hook.dev) {\n\t\terr = -ENOSPC;\n\t\tif (atomic_read(&match->sk_ref) < PACKET_FANOUT_MAX) {\n\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\tpo->fanout = match;\n\t\t\tatomic_inc(&match->sk_ref);\n\t\t\t__fanout_link(sk, po);\n\t\t\terr = 0;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&fanout_mutex);\n\tif (err) {\n\t\tkfree(po->rollover);\n\t\tpo->rollover = NULL;\n\t}\n\treturn err;\n}",
        "code_after_change": "static int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n{\n\tstruct packet_rollover *rollover = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f, *match;\n\tu8 type = type_flags & 0xff;\n\tu8 flags = type_flags >> 8;\n\tint err;\n\n\tswitch (type) {\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\n\t\t\treturn -EINVAL;\n\tcase PACKET_FANOUT_HASH:\n\tcase PACKET_FANOUT_LB:\n\tcase PACKET_FANOUT_CPU:\n\tcase PACKET_FANOUT_RND:\n\tcase PACKET_FANOUT_QM:\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&fanout_mutex);\n\n\terr = -EINVAL;\n\tif (!po->running)\n\t\tgoto out;\n\n\terr = -EALREADY;\n\tif (po->fanout)\n\t\tgoto out;\n\n\tif (type == PACKET_FANOUT_ROLLOVER ||\n\t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n\t\terr = -ENOMEM;\n\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);\n\t\tif (!rollover)\n\t\t\tgoto out;\n\t\tatomic_long_set(&rollover->num, 0);\n\t\tatomic_long_set(&rollover->num_huge, 0);\n\t\tatomic_long_set(&rollover->num_failed, 0);\n\t\tpo->rollover = rollover;\n\t}\n\n\tmatch = NULL;\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\tmatch = f;\n\t\t\tbreak;\n\t\t}\n\t}\n\terr = -EINVAL;\n\tif (match && match->flags != flags)\n\t\tgoto out;\n\tif (!match) {\n\t\terr = -ENOMEM;\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match)\n\t\t\tgoto out;\n\t\twrite_pnet(&match->net, sock_net(sk));\n\t\tmatch->id = id;\n\t\tmatch->type = type;\n\t\tmatch->flags = flags;\n\t\tINIT_LIST_HEAD(&match->list);\n\t\tspin_lock_init(&match->lock);\n\t\tatomic_set(&match->sk_ref, 0);\n\t\tfanout_init_data(match);\n\t\tmatch->prot_hook.type = po->prot_hook.type;\n\t\tmatch->prot_hook.dev = po->prot_hook.dev;\n\t\tmatch->prot_hook.func = packet_rcv_fanout;\n\t\tmatch->prot_hook.af_packet_priv = match;\n\t\tmatch->prot_hook.id_match = match_fanout_group;\n\t\tdev_add_pack(&match->prot_hook);\n\t\tlist_add(&match->list, &fanout_list);\n\t}\n\terr = -EINVAL;\n\tif (match->type == type &&\n\t    match->prot_hook.type == po->prot_hook.type &&\n\t    match->prot_hook.dev == po->prot_hook.dev) {\n\t\terr = -ENOSPC;\n\t\tif (atomic_read(&match->sk_ref) < PACKET_FANOUT_MAX) {\n\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\tpo->fanout = match;\n\t\t\tatomic_inc(&match->sk_ref);\n\t\t\t__fanout_link(sk, po);\n\t\t\terr = 0;\n\t\t}\n\t}\nout:\n\tif (err && rollover) {\n\t\tkfree(rollover);\n\t\tpo->rollover = NULL;\n\t}\n\tmutex_unlock(&fanout_mutex);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n static int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n {\n+\tstruct packet_rollover *rollover = NULL;\n \tstruct packet_sock *po = pkt_sk(sk);\n \tstruct packet_fanout *f, *match;\n \tu8 type = type_flags & 0xff;\n@@ -22,23 +23,28 @@\n \t\treturn -EINVAL;\n \t}\n \n+\tmutex_lock(&fanout_mutex);\n+\n+\terr = -EINVAL;\n \tif (!po->running)\n-\t\treturn -EINVAL;\n+\t\tgoto out;\n \n+\terr = -EALREADY;\n \tif (po->fanout)\n-\t\treturn -EALREADY;\n+\t\tgoto out;\n \n \tif (type == PACKET_FANOUT_ROLLOVER ||\n \t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n-\t\tpo->rollover = kzalloc(sizeof(*po->rollover), GFP_KERNEL);\n-\t\tif (!po->rollover)\n-\t\t\treturn -ENOMEM;\n-\t\tatomic_long_set(&po->rollover->num, 0);\n-\t\tatomic_long_set(&po->rollover->num_huge, 0);\n-\t\tatomic_long_set(&po->rollover->num_failed, 0);\n+\t\terr = -ENOMEM;\n+\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);\n+\t\tif (!rollover)\n+\t\t\tgoto out;\n+\t\tatomic_long_set(&rollover->num, 0);\n+\t\tatomic_long_set(&rollover->num_huge, 0);\n+\t\tatomic_long_set(&rollover->num_failed, 0);\n+\t\tpo->rollover = rollover;\n \t}\n \n-\tmutex_lock(&fanout_mutex);\n \tmatch = NULL;\n \tlist_for_each_entry(f, &fanout_list, list) {\n \t\tif (f->id == id &&\n@@ -85,10 +91,10 @@\n \t\t}\n \t}\n out:\n-\tmutex_unlock(&fanout_mutex);\n-\tif (err) {\n-\t\tkfree(po->rollover);\n+\tif (err && rollover) {\n+\t\tkfree(rollover);\n \t\tpo->rollover = NULL;\n \t}\n+\tmutex_unlock(&fanout_mutex);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct packet_rollover *rollover = NULL;",
                "\tmutex_lock(&fanout_mutex);",
                "",
                "\terr = -EINVAL;",
                "\t\tgoto out;",
                "\terr = -EALREADY;",
                "\t\tgoto out;",
                "\t\terr = -ENOMEM;",
                "\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);",
                "\t\tif (!rollover)",
                "\t\t\tgoto out;",
                "\t\tatomic_long_set(&rollover->num, 0);",
                "\t\tatomic_long_set(&rollover->num_huge, 0);",
                "\t\tatomic_long_set(&rollover->num_failed, 0);",
                "\t\tpo->rollover = rollover;",
                "\tif (err && rollover) {",
                "\t\tkfree(rollover);",
                "\tmutex_unlock(&fanout_mutex);"
            ],
            "deleted": [
                "\t\treturn -EINVAL;",
                "\t\treturn -EALREADY;",
                "\t\tpo->rollover = kzalloc(sizeof(*po->rollover), GFP_KERNEL);",
                "\t\tif (!po->rollover)",
                "\t\t\treturn -ENOMEM;",
                "\t\tatomic_long_set(&po->rollover->num, 0);",
                "\t\tatomic_long_set(&po->rollover->num_huge, 0);",
                "\t\tatomic_long_set(&po->rollover->num_failed, 0);",
                "\tmutex_lock(&fanout_mutex);",
                "\tmutex_unlock(&fanout_mutex);",
                "\tif (err) {",
                "\t\tkfree(po->rollover);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in net/packet/af_packet.c in the Linux kernel before 4.9.13 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via a multithreaded application that makes PACKET_FANOUT setsockopt system calls.",
        "id": 1482
    },
    {
        "cve_id": "CVE-2023-4611",
        "code_before_change": "void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)\n{\n\tstruct vm_area_struct *vma;\n\tVMA_ITERATOR(vmi, mm, 0);\n\n\tmmap_write_lock(mm);\n\tfor_each_vma(vmi, vma)\n\t\tmpol_rebind_policy(vma->vm_policy, new);\n\tmmap_write_unlock(mm);\n}",
        "code_after_change": "void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)\n{\n\tstruct vm_area_struct *vma;\n\tVMA_ITERATOR(vmi, mm, 0);\n\n\tmmap_write_lock(mm);\n\tfor_each_vma(vmi, vma) {\n\t\tvma_start_write(vma);\n\t\tmpol_rebind_policy(vma->vm_policy, new);\n\t}\n\tmmap_write_unlock(mm);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,9 @@\n \tVMA_ITERATOR(vmi, mm, 0);\n \n \tmmap_write_lock(mm);\n-\tfor_each_vma(vmi, vma)\n+\tfor_each_vma(vmi, vma) {\n+\t\tvma_start_write(vma);\n \t\tmpol_rebind_policy(vma->vm_policy, new);\n+\t}\n \tmmap_write_unlock(mm);\n }",
        "function_modified_lines": {
            "added": [
                "\tfor_each_vma(vmi, vma) {",
                "\t\tvma_start_write(vma);",
                "\t}"
            ],
            "deleted": [
                "\tfor_each_vma(vmi, vma)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in mm/mempolicy.c in the memory management subsystem in the Linux Kernel. This issue is caused by a race between mbind() and VMA-locked page fault, and may allow a local attacker to crash the system or lead to a kernel information leak.",
        "id": 4237
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "static int vivid_thread_sdr_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 samples_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\n\tdprintk(dev, 1, \"SDR Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->sdr_cap_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->sdr_cap_seq_offset = 0xffffff80U;\n\tdev->jiffies_sdr_cap = jiffies;\n\tdev->sdr_cap_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->mutex);\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->sdr_cap_seq_resync) {\n\t\t\tdev->jiffies_sdr_cap = cur_jiffies;\n\t\t\tdev->sdr_cap_seq_offset = dev->sdr_cap_seq_count + 1;\n\t\t\tdev->sdr_cap_seq_count = 0;\n\t\t\tdev->sdr_cap_seq_resync = false;\n\t\t}\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_sdr_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start =\n\t\t\t(u64)jiffies_since_start * dev->sdr_adc_freq +\n\t\t\t\t      (HZ * SDR_CAP_SAMPLES_PER_BUF) / 2;\n\t\tdo_div(buffers_since_start, HZ * SDR_CAP_SAMPLES_PER_BUF);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_sdr_cap = cur_jiffies;\n\t\t\tdev->sdr_cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->sdr_cap_seq_count =\n\t\t\tbuffers_since_start + dev->sdr_cap_seq_offset;\n\n\t\tvivid_thread_sdr_cap_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of samples streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tsamples_since_start = buffers_since_start * SDR_CAP_SAMPLES_PER_BUF;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_sdr_cap;\n\n\t\t/* Increase by the number of samples in one buffer */\n\t\tsamples_since_start += SDR_CAP_SAMPLES_PER_BUF;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = samples_since_start * HZ +\n\t\t\t\t\t   dev->sdr_adc_freq / 2;\n\t\tdo_div(next_jiffies_since_start, dev->sdr_adc_freq);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"SDR Capture Thread End\\n\");\n\treturn 0;\n}",
        "code_after_change": "static int vivid_thread_sdr_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 samples_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\n\tdprintk(dev, 1, \"SDR Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->sdr_cap_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->sdr_cap_seq_offset = 0xffffff80U;\n\tdev->jiffies_sdr_cap = jiffies;\n\tdev->sdr_cap_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (!mutex_trylock(&dev->mutex)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->sdr_cap_seq_resync) {\n\t\t\tdev->jiffies_sdr_cap = cur_jiffies;\n\t\t\tdev->sdr_cap_seq_offset = dev->sdr_cap_seq_count + 1;\n\t\t\tdev->sdr_cap_seq_count = 0;\n\t\t\tdev->sdr_cap_seq_resync = false;\n\t\t}\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_sdr_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start =\n\t\t\t(u64)jiffies_since_start * dev->sdr_adc_freq +\n\t\t\t\t      (HZ * SDR_CAP_SAMPLES_PER_BUF) / 2;\n\t\tdo_div(buffers_since_start, HZ * SDR_CAP_SAMPLES_PER_BUF);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_sdr_cap = cur_jiffies;\n\t\t\tdev->sdr_cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->sdr_cap_seq_count =\n\t\t\tbuffers_since_start + dev->sdr_cap_seq_offset;\n\n\t\tvivid_thread_sdr_cap_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of samples streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tsamples_since_start = buffers_since_start * SDR_CAP_SAMPLES_PER_BUF;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_sdr_cap;\n\n\t\t/* Increase by the number of samples in one buffer */\n\t\tsamples_since_start += SDR_CAP_SAMPLES_PER_BUF;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = samples_since_start * HZ +\n\t\t\t\t\t   dev->sdr_adc_freq / 2;\n\t\tdo_div(next_jiffies_since_start, dev->sdr_adc_freq);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"SDR Capture Thread End\\n\");\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -24,7 +24,11 @@\n \t\tif (kthread_should_stop())\n \t\t\tbreak;\n \n-\t\tmutex_lock(&dev->mutex);\n+\t\tif (!mutex_trylock(&dev->mutex)) {\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tcontinue;\n+\t\t}\n+\n \t\tcur_jiffies = jiffies;\n \t\tif (dev->sdr_cap_seq_resync) {\n \t\t\tdev->jiffies_sdr_cap = cur_jiffies;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!mutex_trylock(&dev->mutex)) {",
                "\t\t\tschedule_timeout_uninterruptible(1);",
                "\t\t\tcontinue;",
                "\t\t}",
                ""
            ],
            "deleted": [
                "\t\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free.",
        "id": 2096
    },
    {
        "cve_id": "CVE-2023-35823",
        "code_before_change": "void saa7134_video_fini(struct saa7134_dev *dev)\n{\n\t/* free stuff */\n\tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n\tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);\n\tv4l2_ctrl_handler_free(&dev->ctrl_handler);\n\tif (card_has_radio(dev))\n\t\tv4l2_ctrl_handler_free(&dev->radio_ctrl_handler);\n}",
        "code_after_change": "void saa7134_video_fini(struct saa7134_dev *dev)\n{\n\tdel_timer_sync(&dev->video_q.timeout);\n\t/* free stuff */\n\tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n\tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);\n\tv4l2_ctrl_handler_free(&dev->ctrl_handler);\n\tif (card_has_radio(dev))\n\t\tv4l2_ctrl_handler_free(&dev->radio_ctrl_handler);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n void saa7134_video_fini(struct saa7134_dev *dev)\n {\n+\tdel_timer_sync(&dev->video_q.timeout);\n \t/* free stuff */\n \tsaa7134_pgtable_free(dev->pci, &dev->video_q.pt);\n \tsaa7134_pgtable_free(dev->pci, &dev->vbi_q.pt);",
        "function_modified_lines": {
            "added": [
                "\tdel_timer_sync(&dev->video_q.timeout);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in saa7134_finidev in drivers/media/pci/saa7134/saa7134-core.c.",
        "id": 4111
    },
    {
        "cve_id": "CVE-2019-19319",
        "code_before_change": "static int __check_block_validity(struct inode *inode, const char *func,\n\t\t\t\tunsigned int line,\n\t\t\t\tstruct ext4_map_blocks *map)\n{\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk,\n\t\t\t\t   map->m_len)) {\n\t\text4_error_inode(inode, func, line, map->m_pblk,\n\t\t\t\t \"lblock %lu mapped to illegal pblock %llu \"\n\t\t\t\t \"(length %d)\", (unsigned long) map->m_lblk,\n\t\t\t\t map->m_pblk, map->m_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int __check_block_validity(struct inode *inode, const char *func,\n\t\t\t\tunsigned int line,\n\t\t\t\tstruct ext4_map_blocks *map)\n{\n\tif (ext4_has_feature_journal(inode->i_sb) &&\n\t    (inode->i_ino ==\n\t     le32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_journal_inum)))\n\t\treturn 0;\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk,\n\t\t\t\t   map->m_len)) {\n\t\text4_error_inode(inode, func, line, map->m_pblk,\n\t\t\t\t \"lblock %lu mapped to illegal pblock %llu \"\n\t\t\t\t \"(length %d)\", (unsigned long) map->m_lblk,\n\t\t\t\t map->m_pblk, map->m_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,10 @@\n \t\t\t\tunsigned int line,\n \t\t\t\tstruct ext4_map_blocks *map)\n {\n+\tif (ext4_has_feature_journal(inode->i_sb) &&\n+\t    (inode->i_ino ==\n+\t     le32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_journal_inum)))\n+\t\treturn 0;\n \tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk,\n \t\t\t\t   map->m_len)) {\n \t\text4_error_inode(inode, func, line, map->m_pblk,",
        "function_modified_lines": {
            "added": [
                "\tif (ext4_has_feature_journal(inode->i_sb) &&",
                "\t    (inode->i_ino ==",
                "\t     le32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_journal_inum)))",
                "\t\treturn 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-787",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.2, a setxattr operation, after a mount of a crafted ext4 image, can cause a slab-out-of-bounds write access because of an ext4_xattr_set_entry use-after-free in fs/ext4/xattr.c when a large old_size value is used in a memset call, aka CID-345c0dbf3a30.",
        "id": 2189
    },
    {
        "cve_id": "CVE-2020-14381",
        "code_before_change": "static int\nget_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\tget_futex_key_refs(key);  /* implies smp_mb(); (B) */\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(fshared)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t\tget_futex_key_refs(key); /* implies smp_mb(); (B) */\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\t/*\n\t\t * Take a reference unless it is about to be freed. Previously\n\t\t * this reference was taken by ihold under the page lock\n\t\t * pinning the inode in place so i_lock was unnecessary. The\n\t\t * only way for this check to fail is if the inode was\n\t\t * truncated in parallel which is almost certainly an\n\t\t * application bug. In such a case, just retry.\n\t\t *\n\t\t * We are not calling into get_futex_key_refs() in file-backed\n\t\t * cases, therefore a successful atomic_inc return below will\n\t\t * guarantee that get_futex_key() will still imply smp_mb(); (B).\n\t\t */\n\t\tif (!atomic_inc_not_zero(&inode->i_count)) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\t/* Should be impossible but lets be paranoid for now */\n\t\tif (WARN_ON_ONCE(inode->i_mapping != mapping)) {\n\t\t\terr = -EFAULT;\n\t\t\trcu_read_unlock();\n\t\t\tiput(inode);\n\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.inode = inode;\n\t\tkey->shared.pgoff = basepage_index(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}",
        "code_after_change": "static int\nget_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\tget_futex_key_refs(key);  /* implies smp_mb(); (B) */\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(fshared)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = basepage_index(tail);\n\t\trcu_read_unlock();\n\t}\n\n\tget_futex_key_refs(key); /* implies smp_mb(); (B) */\n\nout:\n\tput_page(page);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -134,8 +134,6 @@\n \t\tkey->private.mm = mm;\n \t\tkey->private.address = address;\n \n-\t\tget_futex_key_refs(key); /* implies smp_mb(); (B) */\n-\n \t} else {\n \t\tstruct inode *inode;\n \n@@ -167,39 +165,13 @@\n \t\t\tgoto again;\n \t\t}\n \n-\t\t/*\n-\t\t * Take a reference unless it is about to be freed. Previously\n-\t\t * this reference was taken by ihold under the page lock\n-\t\t * pinning the inode in place so i_lock was unnecessary. The\n-\t\t * only way for this check to fail is if the inode was\n-\t\t * truncated in parallel which is almost certainly an\n-\t\t * application bug. In such a case, just retry.\n-\t\t *\n-\t\t * We are not calling into get_futex_key_refs() in file-backed\n-\t\t * cases, therefore a successful atomic_inc return below will\n-\t\t * guarantee that get_futex_key() will still imply smp_mb(); (B).\n-\t\t */\n-\t\tif (!atomic_inc_not_zero(&inode->i_count)) {\n-\t\t\trcu_read_unlock();\n-\t\t\tput_page(page);\n-\n-\t\t\tgoto again;\n-\t\t}\n-\n-\t\t/* Should be impossible but lets be paranoid for now */\n-\t\tif (WARN_ON_ONCE(inode->i_mapping != mapping)) {\n-\t\t\terr = -EFAULT;\n-\t\t\trcu_read_unlock();\n-\t\t\tiput(inode);\n-\n-\t\t\tgoto out;\n-\t\t}\n-\n \t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n-\t\tkey->shared.inode = inode;\n+\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n \t\tkey->shared.pgoff = basepage_index(tail);\n \t\trcu_read_unlock();\n \t}\n+\n+\tget_futex_key_refs(key); /* implies smp_mb(); (B) */\n \n out:\n \tput_page(page);",
        "function_modified_lines": {
            "added": [
                "\t\tkey->shared.i_seq = get_inode_sequence_number(inode);",
                "",
                "\tget_futex_key_refs(key); /* implies smp_mb(); (B) */"
            ],
            "deleted": [
                "\t\tget_futex_key_refs(key); /* implies smp_mb(); (B) */",
                "",
                "\t\t/*",
                "\t\t * Take a reference unless it is about to be freed. Previously",
                "\t\t * this reference was taken by ihold under the page lock",
                "\t\t * pinning the inode in place so i_lock was unnecessary. The",
                "\t\t * only way for this check to fail is if the inode was",
                "\t\t * truncated in parallel which is almost certainly an",
                "\t\t * application bug. In such a case, just retry.",
                "\t\t *",
                "\t\t * We are not calling into get_futex_key_refs() in file-backed",
                "\t\t * cases, therefore a successful atomic_inc return below will",
                "\t\t * guarantee that get_futex_key() will still imply smp_mb(); (B).",
                "\t\t */",
                "\t\tif (!atomic_inc_not_zero(&inode->i_count)) {",
                "\t\t\trcu_read_unlock();",
                "\t\t\tput_page(page);",
                "",
                "\t\t\tgoto again;",
                "\t\t}",
                "",
                "\t\t/* Should be impossible but lets be paranoid for now */",
                "\t\tif (WARN_ON_ONCE(inode->i_mapping != mapping)) {",
                "\t\t\terr = -EFAULT;",
                "\t\t\trcu_read_unlock();",
                "\t\t\tiput(inode);",
                "",
                "\t\t\tgoto out;",
                "\t\t}",
                "",
                "\t\tkey->shared.inode = inode;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s futex implementation. This flaw allows a local attacker to corrupt system memory or escalate their privileges when creating a futex on a filesystem that is about to be unmounted. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability.",
        "id": 2522
    },
    {
        "cve_id": "CVE-2020-25656",
        "code_before_change": "int vt_do_kdgkb_ioctl(int cmd, struct kbsentry __user *user_kdgkb, int perm)\n{\n\tstruct kbsentry *kbs;\n\tu_char *q;\n\tint sz, fnw_sz;\n\tint delta;\n\tchar *first_free, *fj, *fnw;\n\tint i, j, k;\n\tint ret;\n\tunsigned long flags;\n\n\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\tperm = 0;\n\n\tkbs = kmalloc(sizeof(*kbs), GFP_KERNEL);\n\tif (!kbs) {\n\t\tret = -ENOMEM;\n\t\tgoto reterr;\n\t}\n\n\t/* we mostly copy too much here (512bytes), but who cares ;) */\n\tif (copy_from_user(kbs, user_kdgkb, sizeof(struct kbsentry))) {\n\t\tret = -EFAULT;\n\t\tgoto reterr;\n\t}\n\tkbs->kb_string[sizeof(kbs->kb_string)-1] = '\\0';\n\ti = array_index_nospec(kbs->kb_func, MAX_NR_FUNC);\n\n\tswitch (cmd) {\n\tcase KDGKBSENT: {\n\t\t/* size should have been a struct member */\n\t\tunsigned char *from = func_table[i] ? : \"\";\n\n\t\tret = copy_to_user(user_kdgkb->kb_string, from,\n\t\t\t\tstrlen(from) + 1) ? -EFAULT : 0;\n\n\t\tgoto reterr;\n\t}\n\tcase KDSKBSENT:\n\t\tif (!perm) {\n\t\t\tret = -EPERM;\n\t\t\tgoto reterr;\n\t\t}\n\n\t\tfnw = NULL;\n\t\tfnw_sz = 0;\n\t\t/* race aginst other writers */\n\t\tagain:\n\t\tspin_lock_irqsave(&func_buf_lock, flags);\n\t\tq = func_table[i];\n\n\t\t/* fj pointer to next entry after 'q' */\n\t\tfirst_free = funcbufptr + (funcbufsize - funcbufleft);\n\t\tfor (j = i+1; j < MAX_NR_FUNC && !func_table[j]; j++)\n\t\t\t;\n\t\tif (j < MAX_NR_FUNC)\n\t\t\tfj = func_table[j];\n\t\telse\n\t\t\tfj = first_free;\n\t\t/* buffer usage increase by new entry */\n\t\tdelta = (q ? -strlen(q) : 1) + strlen(kbs->kb_string);\n\n\t\tif (delta <= funcbufleft) { \t/* it fits in current buf */\n\t\t    if (j < MAX_NR_FUNC) {\n\t\t\t/* make enough space for new entry at 'fj' */\n\t\t\tmemmove(fj + delta, fj, first_free - fj);\n\t\t\tfor (k = j; k < MAX_NR_FUNC; k++)\n\t\t\t    if (func_table[k])\n\t\t\t\tfunc_table[k] += delta;\n\t\t    }\n\t\t    if (!q)\n\t\t      func_table[i] = fj;\n\t\t    funcbufleft -= delta;\n\t\t} else {\t\t\t/* allocate a larger buffer */\n\t\t    sz = 256;\n\t\t    while (sz < funcbufsize - funcbufleft + delta)\n\t\t      sz <<= 1;\n\t\t    if (fnw_sz != sz) {\n\t\t      spin_unlock_irqrestore(&func_buf_lock, flags);\n\t\t      kfree(fnw);\n\t\t      fnw = kmalloc(sz, GFP_KERNEL);\n\t\t      fnw_sz = sz;\n\t\t      if (!fnw) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto reterr;\n\t\t      }\n\t\t      goto again;\n\t\t    }\n\n\t\t    if (!q)\n\t\t      func_table[i] = fj;\n\t\t    /* copy data before insertion point to new location */\n\t\t    if (fj > funcbufptr)\n\t\t\tmemmove(fnw, funcbufptr, fj - funcbufptr);\n\t\t    for (k = 0; k < j; k++)\n\t\t      if (func_table[k])\n\t\t\tfunc_table[k] = fnw + (func_table[k] - funcbufptr);\n\n\t\t    /* copy data after insertion point to new location */\n\t\t    if (first_free > fj) {\n\t\t\tmemmove(fnw + (fj - funcbufptr) + delta, fj, first_free - fj);\n\t\t\tfor (k = j; k < MAX_NR_FUNC; k++)\n\t\t\t  if (func_table[k])\n\t\t\t    func_table[k] = fnw + (func_table[k] - funcbufptr) + delta;\n\t\t    }\n\t\t    if (funcbufptr != func_buf)\n\t\t      kfree(funcbufptr);\n\t\t    funcbufptr = fnw;\n\t\t    funcbufleft = funcbufleft - delta + sz - funcbufsize;\n\t\t    funcbufsize = sz;\n\t\t}\n\t\t/* finally insert item itself */\n\t\tstrcpy(func_table[i], kbs->kb_string);\n\t\tspin_unlock_irqrestore(&func_buf_lock, flags);\n\t\tbreak;\n\t}\n\tret = 0;\nreterr:\n\tkfree(kbs);\n\treturn ret;\n}",
        "code_after_change": "int vt_do_kdgkb_ioctl(int cmd, struct kbsentry __user *user_kdgkb, int perm)\n{\n\tstruct kbsentry *kbs;\n\tu_char *q;\n\tint sz, fnw_sz;\n\tint delta;\n\tchar *first_free, *fj, *fnw;\n\tint i, j, k;\n\tint ret;\n\tunsigned long flags;\n\n\tif (!capable(CAP_SYS_TTY_CONFIG))\n\t\tperm = 0;\n\n\tkbs = kmalloc(sizeof(*kbs), GFP_KERNEL);\n\tif (!kbs) {\n\t\tret = -ENOMEM;\n\t\tgoto reterr;\n\t}\n\n\t/* we mostly copy too much here (512bytes), but who cares ;) */\n\tif (copy_from_user(kbs, user_kdgkb, sizeof(struct kbsentry))) {\n\t\tret = -EFAULT;\n\t\tgoto reterr;\n\t}\n\tkbs->kb_string[sizeof(kbs->kb_string)-1] = '\\0';\n\ti = array_index_nospec(kbs->kb_func, MAX_NR_FUNC);\n\n\tswitch (cmd) {\n\tcase KDGKBSENT: {\n\t\t/* size should have been a struct member */\n\t\tssize_t len = sizeof(user_kdgkb->kb_string);\n\n\t\tspin_lock_irqsave(&func_buf_lock, flags);\n\t\tlen = strlcpy(kbs->kb_string, func_table[i] ? : \"\", len);\n\t\tspin_unlock_irqrestore(&func_buf_lock, flags);\n\n\t\tret = copy_to_user(user_kdgkb->kb_string, kbs->kb_string,\n\t\t\t\tlen + 1) ? -EFAULT : 0;\n\n\t\tgoto reterr;\n\t}\n\tcase KDSKBSENT:\n\t\tif (!perm) {\n\t\t\tret = -EPERM;\n\t\t\tgoto reterr;\n\t\t}\n\n\t\tfnw = NULL;\n\t\tfnw_sz = 0;\n\t\t/* race aginst other writers */\n\t\tagain:\n\t\tspin_lock_irqsave(&func_buf_lock, flags);\n\t\tq = func_table[i];\n\n\t\t/* fj pointer to next entry after 'q' */\n\t\tfirst_free = funcbufptr + (funcbufsize - funcbufleft);\n\t\tfor (j = i+1; j < MAX_NR_FUNC && !func_table[j]; j++)\n\t\t\t;\n\t\tif (j < MAX_NR_FUNC)\n\t\t\tfj = func_table[j];\n\t\telse\n\t\t\tfj = first_free;\n\t\t/* buffer usage increase by new entry */\n\t\tdelta = (q ? -strlen(q) : 1) + strlen(kbs->kb_string);\n\n\t\tif (delta <= funcbufleft) { \t/* it fits in current buf */\n\t\t    if (j < MAX_NR_FUNC) {\n\t\t\t/* make enough space for new entry at 'fj' */\n\t\t\tmemmove(fj + delta, fj, first_free - fj);\n\t\t\tfor (k = j; k < MAX_NR_FUNC; k++)\n\t\t\t    if (func_table[k])\n\t\t\t\tfunc_table[k] += delta;\n\t\t    }\n\t\t    if (!q)\n\t\t      func_table[i] = fj;\n\t\t    funcbufleft -= delta;\n\t\t} else {\t\t\t/* allocate a larger buffer */\n\t\t    sz = 256;\n\t\t    while (sz < funcbufsize - funcbufleft + delta)\n\t\t      sz <<= 1;\n\t\t    if (fnw_sz != sz) {\n\t\t      spin_unlock_irqrestore(&func_buf_lock, flags);\n\t\t      kfree(fnw);\n\t\t      fnw = kmalloc(sz, GFP_KERNEL);\n\t\t      fnw_sz = sz;\n\t\t      if (!fnw) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto reterr;\n\t\t      }\n\t\t      goto again;\n\t\t    }\n\n\t\t    if (!q)\n\t\t      func_table[i] = fj;\n\t\t    /* copy data before insertion point to new location */\n\t\t    if (fj > funcbufptr)\n\t\t\tmemmove(fnw, funcbufptr, fj - funcbufptr);\n\t\t    for (k = 0; k < j; k++)\n\t\t      if (func_table[k])\n\t\t\tfunc_table[k] = fnw + (func_table[k] - funcbufptr);\n\n\t\t    /* copy data after insertion point to new location */\n\t\t    if (first_free > fj) {\n\t\t\tmemmove(fnw + (fj - funcbufptr) + delta, fj, first_free - fj);\n\t\t\tfor (k = j; k < MAX_NR_FUNC; k++)\n\t\t\t  if (func_table[k])\n\t\t\t    func_table[k] = fnw + (func_table[k] - funcbufptr) + delta;\n\t\t    }\n\t\t    if (funcbufptr != func_buf)\n\t\t      kfree(funcbufptr);\n\t\t    funcbufptr = fnw;\n\t\t    funcbufleft = funcbufleft - delta + sz - funcbufsize;\n\t\t    funcbufsize = sz;\n\t\t}\n\t\t/* finally insert item itself */\n\t\tstrcpy(func_table[i], kbs->kb_string);\n\t\tspin_unlock_irqrestore(&func_buf_lock, flags);\n\t\tbreak;\n\t}\n\tret = 0;\nreterr:\n\tkfree(kbs);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,10 +29,14 @@\n \tswitch (cmd) {\n \tcase KDGKBSENT: {\n \t\t/* size should have been a struct member */\n-\t\tunsigned char *from = func_table[i] ? : \"\";\n+\t\tssize_t len = sizeof(user_kdgkb->kb_string);\n \n-\t\tret = copy_to_user(user_kdgkb->kb_string, from,\n-\t\t\t\tstrlen(from) + 1) ? -EFAULT : 0;\n+\t\tspin_lock_irqsave(&func_buf_lock, flags);\n+\t\tlen = strlcpy(kbs->kb_string, func_table[i] ? : \"\", len);\n+\t\tspin_unlock_irqrestore(&func_buf_lock, flags);\n+\n+\t\tret = copy_to_user(user_kdgkb->kb_string, kbs->kb_string,\n+\t\t\t\tlen + 1) ? -EFAULT : 0;\n \n \t\tgoto reterr;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tssize_t len = sizeof(user_kdgkb->kb_string);",
                "\t\tspin_lock_irqsave(&func_buf_lock, flags);",
                "\t\tlen = strlcpy(kbs->kb_string, func_table[i] ? : \"\", len);",
                "\t\tspin_unlock_irqrestore(&func_buf_lock, flags);",
                "",
                "\t\tret = copy_to_user(user_kdgkb->kb_string, kbs->kb_string,",
                "\t\t\t\tlen + 1) ? -EFAULT : 0;"
            ],
            "deleted": [
                "\t\tunsigned char *from = func_table[i] ? : \"\";",
                "\t\tret = copy_to_user(user_kdgkb->kb_string, from,",
                "\t\t\t\tstrlen(from) + 1) ? -EFAULT : 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel. A use-after-free was found in the way the console subsystem was using ioctls KDGKBSENT and KDSKBSENT. A local user could use this flaw to get read memory access out of bounds. The highest threat from this vulnerability is to data confidentiality.",
        "id": 2593
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "int vmw_resource_validate(struct vmw_resource *res, bool intr,\n\t\t\t  bool dirtying)\n{\n\tint ret;\n\tstruct vmw_resource *evict_res;\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tstruct list_head *lru_list = &dev_priv->res_lru[res->func->res_type];\n\tstruct ttm_validate_buffer val_buf;\n\tunsigned err_count = 0;\n\n\tif (!res->func->create)\n\t\treturn 0;\n\n\tval_buf.bo = NULL;\n\tval_buf.num_shared = 0;\n\tif (res->guest_memory_bo)\n\t\tval_buf.bo = &res->guest_memory_bo->tbo;\n\tdo {\n\t\tret = vmw_resource_do_validate(res, &val_buf, dirtying);\n\t\tif (likely(ret != -EBUSY))\n\t\t\tbreak;\n\n\t\tspin_lock(&dev_priv->resource_lock);\n\t\tif (list_empty(lru_list) || !res->func->may_evict) {\n\t\t\tDRM_ERROR(\"Out of device device resources \"\n\t\t\t\t  \"for %s.\\n\", res->func->type_name);\n\t\t\tret = -EBUSY;\n\t\t\tspin_unlock(&dev_priv->resource_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tevict_res = vmw_resource_reference\n\t\t\t(list_first_entry(lru_list, struct vmw_resource,\n\t\t\t\t\t  lru_head));\n\t\tlist_del_init(&evict_res->lru_head);\n\n\t\tspin_unlock(&dev_priv->resource_lock);\n\n\t\t/* Trylock backup buffers with a NULL ticket. */\n\t\tret = vmw_resource_do_evict(NULL, evict_res, intr);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tspin_lock(&dev_priv->resource_lock);\n\t\t\tlist_add_tail(&evict_res->lru_head, lru_list);\n\t\t\tspin_unlock(&dev_priv->resource_lock);\n\t\t\tif (ret == -ERESTARTSYS ||\n\t\t\t    ++err_count > VMW_RES_EVICT_ERR_COUNT) {\n\t\t\t\tvmw_resource_unreference(&evict_res);\n\t\t\t\tgoto out_no_validate;\n\t\t\t}\n\t\t}\n\n\t\tvmw_resource_unreference(&evict_res);\n\t} while (1);\n\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_validate;\n\telse if (!res->func->needs_guest_memory && res->guest_memory_bo) {\n\t\tWARN_ON_ONCE(vmw_resource_mob_attached(res));\n\t\tvmw_bo_unreference(&res->guest_memory_bo);\n\t}\n\n\treturn 0;\n\nout_no_validate:\n\treturn ret;\n}",
        "code_after_change": "int vmw_resource_validate(struct vmw_resource *res, bool intr,\n\t\t\t  bool dirtying)\n{\n\tint ret;\n\tstruct vmw_resource *evict_res;\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tstruct list_head *lru_list = &dev_priv->res_lru[res->func->res_type];\n\tstruct ttm_validate_buffer val_buf;\n\tunsigned err_count = 0;\n\n\tif (!res->func->create)\n\t\treturn 0;\n\n\tval_buf.bo = NULL;\n\tval_buf.num_shared = 0;\n\tif (res->guest_memory_bo)\n\t\tval_buf.bo = &res->guest_memory_bo->tbo;\n\tdo {\n\t\tret = vmw_resource_do_validate(res, &val_buf, dirtying);\n\t\tif (likely(ret != -EBUSY))\n\t\t\tbreak;\n\n\t\tspin_lock(&dev_priv->resource_lock);\n\t\tif (list_empty(lru_list) || !res->func->may_evict) {\n\t\t\tDRM_ERROR(\"Out of device device resources \"\n\t\t\t\t  \"for %s.\\n\", res->func->type_name);\n\t\t\tret = -EBUSY;\n\t\t\tspin_unlock(&dev_priv->resource_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tevict_res = vmw_resource_reference\n\t\t\t(list_first_entry(lru_list, struct vmw_resource,\n\t\t\t\t\t  lru_head));\n\t\tlist_del_init(&evict_res->lru_head);\n\n\t\tspin_unlock(&dev_priv->resource_lock);\n\n\t\t/* Trylock backup buffers with a NULL ticket. */\n\t\tret = vmw_resource_do_evict(NULL, evict_res, intr);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tspin_lock(&dev_priv->resource_lock);\n\t\t\tlist_add_tail(&evict_res->lru_head, lru_list);\n\t\t\tspin_unlock(&dev_priv->resource_lock);\n\t\t\tif (ret == -ERESTARTSYS ||\n\t\t\t    ++err_count > VMW_RES_EVICT_ERR_COUNT) {\n\t\t\t\tvmw_resource_unreference(&evict_res);\n\t\t\t\tgoto out_no_validate;\n\t\t\t}\n\t\t}\n\n\t\tvmw_resource_unreference(&evict_res);\n\t} while (1);\n\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_validate;\n\telse if (!res->func->needs_guest_memory && res->guest_memory_bo) {\n\t\tWARN_ON_ONCE(vmw_resource_mob_attached(res));\n\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n\t}\n\n\treturn 0;\n\nout_no_validate:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -56,7 +56,7 @@\n \t\tgoto out_no_validate;\n \telse if (!res->func->needs_guest_memory && res->guest_memory_bo) {\n \t\tWARN_ON_ONCE(vmw_resource_mob_attached(res));\n-\t\tvmw_bo_unreference(&res->guest_memory_bo);\n+\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n \t}\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\t\tvmw_user_bo_unref(&res->guest_memory_bo);"
            ],
            "deleted": [
                "\t\tvmw_bo_unreference(&res->guest_memory_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4279
    },
    {
        "cve_id": "CVE-2019-25045",
        "code_before_change": "static void __net_exit xfrm6_tunnel_net_exit(struct net *net)\n{\n\tstruct xfrm6_tunnel_net *xfrm6_tn = xfrm6_tunnel_pernet(net);\n\tunsigned int i;\n\n\txfrm_flush_gc();\n\txfrm_state_flush(net, IPSEC_PROTO_ANY, false, true);\n\n\tfor (i = 0; i < XFRM6_TUNNEL_SPI_BYADDR_HSIZE; i++)\n\t\tWARN_ON_ONCE(!hlist_empty(&xfrm6_tn->spi_byaddr[i]));\n\n\tfor (i = 0; i < XFRM6_TUNNEL_SPI_BYSPI_HSIZE; i++)\n\t\tWARN_ON_ONCE(!hlist_empty(&xfrm6_tn->spi_byspi[i]));\n}",
        "code_after_change": "static void __net_exit xfrm6_tunnel_net_exit(struct net *net)\n{\n\tstruct xfrm6_tunnel_net *xfrm6_tn = xfrm6_tunnel_pernet(net);\n\tunsigned int i;\n\n\txfrm_flush_gc();\n\txfrm_state_flush(net, 0, false, true);\n\n\tfor (i = 0; i < XFRM6_TUNNEL_SPI_BYADDR_HSIZE; i++)\n\t\tWARN_ON_ONCE(!hlist_empty(&xfrm6_tn->spi_byaddr[i]));\n\n\tfor (i = 0; i < XFRM6_TUNNEL_SPI_BYSPI_HSIZE; i++)\n\t\tWARN_ON_ONCE(!hlist_empty(&xfrm6_tn->spi_byspi[i]));\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \tunsigned int i;\n \n \txfrm_flush_gc();\n-\txfrm_state_flush(net, IPSEC_PROTO_ANY, false, true);\n+\txfrm_state_flush(net, 0, false, true);\n \n \tfor (i = 0; i < XFRM6_TUNNEL_SPI_BYADDR_HSIZE; i++)\n \t\tWARN_ON_ONCE(!hlist_empty(&xfrm6_tn->spi_byaddr[i]));",
        "function_modified_lines": {
            "added": [
                "\txfrm_state_flush(net, 0, false, true);"
            ],
            "deleted": [
                "\txfrm_state_flush(net, IPSEC_PROTO_ANY, false, true);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.19. The XFRM subsystem has a use-after-free, related to an xfrm_state_fini panic, aka CID-dbb2483b2a46.",
        "id": 2302
    },
    {
        "cve_id": "CVE-2018-9465",
        "code_before_change": "static long task_close_fd(struct binder_proc *proc, unsigned int fd)\n{\n\tint retval;\n\n\tif (proc->files == NULL)\n\t\treturn -ESRCH;\n\n\tretval = __close_fd(proc->files, fd);\n\t/* can't restart close syscall because file table entry was cleared */\n\tif (unlikely(retval == -ERESTARTSYS ||\n\t\t     retval == -ERESTARTNOINTR ||\n\t\t     retval == -ERESTARTNOHAND ||\n\t\t     retval == -ERESTART_RESTARTBLOCK))\n\t\tretval = -EINTR;\n\n\treturn retval;\n}",
        "code_after_change": "static long task_close_fd(struct binder_proc *proc, unsigned int fd)\n{\n\tint retval;\n\n\tmutex_lock(&proc->files_lock);\n\tif (proc->files == NULL) {\n\t\tretval = -ESRCH;\n\t\tgoto err;\n\t}\n\tretval = __close_fd(proc->files, fd);\n\t/* can't restart close syscall because file table entry was cleared */\n\tif (unlikely(retval == -ERESTARTSYS ||\n\t\t     retval == -ERESTARTNOINTR ||\n\t\t     retval == -ERESTARTNOHAND ||\n\t\t     retval == -ERESTART_RESTARTBLOCK))\n\t\tretval = -EINTR;\nerr:\n\tmutex_unlock(&proc->files_lock);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,9 +2,11 @@\n {\n \tint retval;\n \n-\tif (proc->files == NULL)\n-\t\treturn -ESRCH;\n-\n+\tmutex_lock(&proc->files_lock);\n+\tif (proc->files == NULL) {\n+\t\tretval = -ESRCH;\n+\t\tgoto err;\n+\t}\n \tretval = __close_fd(proc->files, fd);\n \t/* can't restart close syscall because file table entry was cleared */\n \tif (unlikely(retval == -ERESTARTSYS ||\n@@ -12,6 +14,7 @@\n \t\t     retval == -ERESTARTNOHAND ||\n \t\t     retval == -ERESTART_RESTARTBLOCK))\n \t\tretval = -EINTR;\n-\n+err:\n+\tmutex_unlock(&proc->files_lock);\n \treturn retval;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&proc->files_lock);",
                "\tif (proc->files == NULL) {",
                "\t\tretval = -ESRCH;",
                "\t\tgoto err;",
                "\t}",
                "err:",
                "\tmutex_unlock(&proc->files_lock);"
            ],
            "deleted": [
                "\tif (proc->files == NULL)",
                "\t\treturn -ESRCH;",
                "",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In task_get_unused_fd_flags of binder.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation. Product: Android Versions: Android kernel Android ID: A-69164715 References: Upstream kernel.",
        "id": 1872
    },
    {
        "cve_id": "CVE-2021-0707",
        "code_before_change": "static void dma_buf_release(struct dentry *dentry)\n{\n\tstruct dma_buf *dmabuf;\n\n\tdmabuf = dentry->d_fsdata;\n\tif (unlikely(!dmabuf))\n\t\treturn;\n\n\tBUG_ON(dmabuf->vmapping_counter);\n\n\t/*\n\t * Any fences that a dma-buf poll can wait on should be signaled\n\t * before releasing dma-buf. This is the responsibility of each\n\t * driver that uses the reservation objects.\n\t *\n\t * If you hit this BUG() it means someone dropped their ref to the\n\t * dma-buf while still having pending operation to the buffer.\n\t */\n\tBUG_ON(dmabuf->cb_shared.active || dmabuf->cb_excl.active);\n\n\tdmabuf->ops->release(dmabuf);\n\n\tmutex_lock(&db_list.lock);\n\tlist_del(&dmabuf->list_node);\n\tmutex_unlock(&db_list.lock);\n\n\tif (dmabuf->resv == (struct dma_resv *)&dmabuf[1])\n\t\tdma_resv_fini(dmabuf->resv);\n\n\tmodule_put(dmabuf->owner);\n\tkfree(dmabuf->name);\n\tkfree(dmabuf);\n}",
        "code_after_change": "static void dma_buf_release(struct dentry *dentry)\n{\n\tstruct dma_buf *dmabuf;\n\n\tdmabuf = dentry->d_fsdata;\n\tif (unlikely(!dmabuf))\n\t\treturn;\n\n\tBUG_ON(dmabuf->vmapping_counter);\n\n\t/*\n\t * Any fences that a dma-buf poll can wait on should be signaled\n\t * before releasing dma-buf. This is the responsibility of each\n\t * driver that uses the reservation objects.\n\t *\n\t * If you hit this BUG() it means someone dropped their ref to the\n\t * dma-buf while still having pending operation to the buffer.\n\t */\n\tBUG_ON(dmabuf->cb_shared.active || dmabuf->cb_excl.active);\n\n\tdmabuf->ops->release(dmabuf);\n\n\tif (dmabuf->resv == (struct dma_resv *)&dmabuf[1])\n\t\tdma_resv_fini(dmabuf->resv);\n\n\tmodule_put(dmabuf->owner);\n\tkfree(dmabuf->name);\n\tkfree(dmabuf);\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,10 +20,6 @@\n \n \tdmabuf->ops->release(dmabuf);\n \n-\tmutex_lock(&db_list.lock);\n-\tlist_del(&dmabuf->list_node);\n-\tmutex_unlock(&db_list.lock);\n-\n \tif (dmabuf->resv == (struct dma_resv *)&dmabuf[1])\n \t\tdma_resv_fini(dmabuf->resv);\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmutex_lock(&db_list.lock);",
                "\tlist_del(&dmabuf->list_node);",
                "\tmutex_unlock(&db_list.lock);",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In dma_buf_release of dma-buf.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-155756045References: Upstream kernel",
        "id": 2829
    },
    {
        "cve_id": "CVE-2017-6346",
        "code_before_change": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tf = po->fanout;\n\tif (!f)\n\t\treturn;\n\n\tmutex_lock(&fanout_mutex);\n\tpo->fanout = NULL;\n\n\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\tlist_del(&f->list);\n\t\tdev_remove_pack(&f->prot_hook);\n\t\tfanout_release_data(f);\n\t\tkfree(f);\n\t}\n\tmutex_unlock(&fanout_mutex);\n\n\tif (po->rollover)\n\t\tkfree_rcu(po->rollover, rcu);\n}",
        "code_after_change": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tmutex_lock(&fanout_mutex);\n\tf = po->fanout;\n\tif (f) {\n\t\tpo->fanout = NULL;\n\n\t\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\t\tlist_del(&f->list);\n\t\t\tdev_remove_pack(&f->prot_hook);\n\t\t\tfanout_release_data(f);\n\t\t\tkfree(f);\n\t\t}\n\n\t\tif (po->rollover)\n\t\t\tkfree_rcu(po->rollover, rcu);\n\t}\n\tmutex_unlock(&fanout_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,21 +3,20 @@\n \tstruct packet_sock *po = pkt_sk(sk);\n \tstruct packet_fanout *f;\n \n+\tmutex_lock(&fanout_mutex);\n \tf = po->fanout;\n-\tif (!f)\n-\t\treturn;\n+\tif (f) {\n+\t\tpo->fanout = NULL;\n \n-\tmutex_lock(&fanout_mutex);\n-\tpo->fanout = NULL;\n+\t\tif (atomic_dec_and_test(&f->sk_ref)) {\n+\t\t\tlist_del(&f->list);\n+\t\t\tdev_remove_pack(&f->prot_hook);\n+\t\t\tfanout_release_data(f);\n+\t\t\tkfree(f);\n+\t\t}\n \n-\tif (atomic_dec_and_test(&f->sk_ref)) {\n-\t\tlist_del(&f->list);\n-\t\tdev_remove_pack(&f->prot_hook);\n-\t\tfanout_release_data(f);\n-\t\tkfree(f);\n+\t\tif (po->rollover)\n+\t\t\tkfree_rcu(po->rollover, rcu);\n \t}\n \tmutex_unlock(&fanout_mutex);\n-\n-\tif (po->rollover)\n-\t\tkfree_rcu(po->rollover, rcu);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&fanout_mutex);",
                "\tif (f) {",
                "\t\tpo->fanout = NULL;",
                "\t\tif (atomic_dec_and_test(&f->sk_ref)) {",
                "\t\t\tlist_del(&f->list);",
                "\t\t\tdev_remove_pack(&f->prot_hook);",
                "\t\t\tfanout_release_data(f);",
                "\t\t\tkfree(f);",
                "\t\t}",
                "\t\tif (po->rollover)",
                "\t\t\tkfree_rcu(po->rollover, rcu);"
            ],
            "deleted": [
                "\tif (!f)",
                "\t\treturn;",
                "\tmutex_lock(&fanout_mutex);",
                "\tpo->fanout = NULL;",
                "\tif (atomic_dec_and_test(&f->sk_ref)) {",
                "\t\tlist_del(&f->list);",
                "\t\tdev_remove_pack(&f->prot_hook);",
                "\t\tfanout_release_data(f);",
                "\t\tkfree(f);",
                "",
                "\tif (po->rollover)",
                "\t\tkfree_rcu(po->rollover, rcu);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in net/packet/af_packet.c in the Linux kernel before 4.9.13 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via a multithreaded application that makes PACKET_FANOUT setsockopt system calls.",
        "id": 1483
    },
    {
        "cve_id": "CVE-2020-10690",
        "code_before_change": "static int posix_clock_open(struct inode *inode, struct file *fp)\n{\n\tint err;\n\tstruct posix_clock *clk =\n\t\tcontainer_of(inode->i_cdev, struct posix_clock, cdev);\n\n\tdown_read(&clk->rwsem);\n\n\tif (clk->zombie) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (clk->ops.open)\n\t\terr = clk->ops.open(clk, fp->f_mode);\n\telse\n\t\terr = 0;\n\n\tif (!err) {\n\t\tkref_get(&clk->kref);\n\t\tfp->private_data = clk;\n\t}\nout:\n\tup_read(&clk->rwsem);\n\treturn err;\n}",
        "code_after_change": "static int posix_clock_open(struct inode *inode, struct file *fp)\n{\n\tint err;\n\tstruct posix_clock *clk =\n\t\tcontainer_of(inode->i_cdev, struct posix_clock, cdev);\n\n\tdown_read(&clk->rwsem);\n\n\tif (clk->zombie) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (clk->ops.open)\n\t\terr = clk->ops.open(clk, fp->f_mode);\n\telse\n\t\terr = 0;\n\n\tif (!err) {\n\t\tget_device(clk->dev);\n\t\tfp->private_data = clk;\n\t}\nout:\n\tup_read(&clk->rwsem);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,7 +16,7 @@\n \t\terr = 0;\n \n \tif (!err) {\n-\t\tkref_get(&clk->kref);\n+\t\tget_device(clk->dev);\n \t\tfp->private_data = clk;\n \t}\n out:",
        "function_modified_lines": {
            "added": [
                "\t\tget_device(clk->dev);"
            ],
            "deleted": [
                "\t\tkref_get(&clk->kref);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a use-after-free in kernel versions before 5.5 due to a race condition between the release of ptp_clock and cdev while resource deallocation. When a (high privileged) process allocates a ptp device file (like /dev/ptpX) and voluntarily goes to sleep. During this time if the underlying device is removed, it can cause an exploitable condition as the process wakes up to terminate and clean all attached files. The system crashes due to the cdev structure being invalid (as already freed) which is pointed to by the inode.",
        "id": 2402
    },
    {
        "cve_id": "CVE-2022-47946",
        "code_before_change": "static void io_uring_cancel_sqpoll(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\tstruct io_uring_task *tctx;\n\ts64 inflight;\n\tDEFINE_WAIT(wait);\n\n\tif (!sqd)\n\t\treturn;\n\tio_disable_sqo_submit(ctx);\n\tif (!io_sq_thread_park(sqd))\n\t\treturn;\n\ttctx = ctx->sq_data->thread->io_uring;\n\t/* can happen on fork/alloc failure, just ignore that state */\n\tif (!tctx) {\n\t\tio_sq_thread_unpark(sqd);\n\t\treturn;\n\t}\n\n\tatomic_inc(&tctx->in_idle);\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\tio_uring_cancel_task_requests(ctx, NULL);\n\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\t\t/*\n\t\t * If we've seen completions, retry without waiting. This\n\t\t * avoids a race where a completion comes in before we did\n\t\t * prepare_to_wait().\n\t\t */\n\t\tif (inflight == tctx_inflight(tctx))\n\t\t\tschedule();\n\t\tfinish_wait(&tctx->wait, &wait);\n\t} while (1);\n\tatomic_dec(&tctx->in_idle);\n\tio_sq_thread_unpark(sqd);\n}",
        "code_after_change": "static void io_uring_cancel_sqpoll(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\tstruct io_uring_task *tctx;\n\ts64 inflight;\n\tDEFINE_WAIT(wait);\n\n\tif (!sqd)\n\t\treturn;\n\tif (!io_sq_thread_park(sqd))\n\t\treturn;\n\ttctx = ctx->sq_data->thread->io_uring;\n\t/* can happen on fork/alloc failure, just ignore that state */\n\tif (!tctx) {\n\t\tio_sq_thread_unpark(sqd);\n\t\treturn;\n\t}\n\n\tatomic_inc(&tctx->in_idle);\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\tio_uring_cancel_task_requests(ctx, NULL);\n\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\t\t/*\n\t\t * If we've seen completions, retry without waiting. This\n\t\t * avoids a race where a completion comes in before we did\n\t\t * prepare_to_wait().\n\t\t */\n\t\tif (inflight == tctx_inflight(tctx))\n\t\t\tschedule();\n\t\tfinish_wait(&tctx->wait, &wait);\n\t} while (1);\n\tatomic_dec(&tctx->in_idle);\n\tio_sq_thread_unpark(sqd);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,6 @@\n \n \tif (!sqd)\n \t\treturn;\n-\tio_disable_sqo_submit(ctx);\n \tif (!io_sq_thread_park(sqd))\n \t\treturn;\n \ttctx = ctx->sq_data->thread->io_uring;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tio_disable_sqo_submit(ctx);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 5.10.x before 5.10.155. A use-after-free in io_sqpoll_wait_sq in fs/io_uring.c allows an attacker to crash the kernel, resulting in denial of service. finish_wait can be skipped. An attack can occur in some situations by forking a process and then quickly terminating it. NOTE: later kernel versions, such as the 5.15 longterm series, substantially changed the implementation of io_sqpoll_wait_sq.",
        "id": 3783
    },
    {
        "cve_id": "CVE-2020-29660",
        "code_before_change": "static void __proc_set_tty(struct tty_struct *tty)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t/*\n\t * The session and fg pgrp references will be non-NULL if\n\t * tiocsctty() is stealing the controlling tty\n\t */\n\tput_pid(tty->session);\n\tput_pid(tty->pgrp);\n\ttty->pgrp = get_pid(task_pgrp(current));\n\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\ttty->session = get_pid(task_session(current));\n\tif (current->signal->tty) {\n\t\ttty_debug(tty, \"current tty %s not NULL!!\\n\",\n\t\t\t  current->signal->tty->name);\n\t\ttty_kref_put(current->signal->tty);\n\t}\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty = tty_kref_get(tty);\n\tcurrent->signal->tty_old_pgrp = NULL;\n}",
        "code_after_change": "static void __proc_set_tty(struct tty_struct *tty)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t/*\n\t * The session and fg pgrp references will be non-NULL if\n\t * tiocsctty() is stealing the controlling tty\n\t */\n\tput_pid(tty->session);\n\tput_pid(tty->pgrp);\n\ttty->pgrp = get_pid(task_pgrp(current));\n\ttty->session = get_pid(task_session(current));\n\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\tif (current->signal->tty) {\n\t\ttty_debug(tty, \"current tty %s not NULL!!\\n\",\n\t\t\t  current->signal->tty->name);\n\t\ttty_kref_put(current->signal->tty);\n\t}\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty = tty_kref_get(tty);\n\tcurrent->signal->tty_old_pgrp = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,8 +10,8 @@\n \tput_pid(tty->session);\n \tput_pid(tty->pgrp);\n \ttty->pgrp = get_pid(task_pgrp(current));\n+\ttty->session = get_pid(task_session(current));\n \tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n-\ttty->session = get_pid(task_session(current));\n \tif (current->signal->tty) {\n \t\ttty_debug(tty, \"current tty %s not NULL!!\\n\",\n \t\t\t  current->signal->tty->name);",
        "function_modified_lines": {
            "added": [
                "\ttty->session = get_pid(task_session(current));"
            ],
            "deleted": [
                "\ttty->session = get_pid(task_session(current));"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "A locking inconsistency issue was discovered in the tty subsystem of the Linux kernel through 5.9.13. drivers/tty/tty_io.c and drivers/tty/tty_jobctrl.c may allow a read-after-free attack against TIOCGSID, aka CID-c8bcd9c5be24.",
        "id": 2702
    },
    {
        "cve_id": "CVE-2014-4653",
        "code_before_change": "static int snd_ctl_elem_write(struct snd_card *card, struct snd_ctl_file *file,\n\t\t\t      struct snd_ctl_elem_value *control)\n{\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int index_offset;\n\tint result;\n\n\tdown_read(&card->controls_rwsem);\n\tkctl = snd_ctl_find_id(card, &control->id);\n\tif (kctl == NULL) {\n\t\tresult = -ENOENT;\n\t} else {\n\t\tindex_offset = snd_ctl_get_ioff(kctl, &control->id);\n\t\tvd = &kctl->vd[index_offset];\n\t\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_WRITE) ||\n\t\t    kctl->put == NULL ||\n\t\t    (file && vd->owner && vd->owner != file)) {\n\t\t\tresult = -EPERM;\n\t\t} else {\n\t\t\tsnd_ctl_build_ioff(&control->id, kctl, index_offset);\n\t\t\tresult = kctl->put(kctl, control);\n\t\t}\n\t\tif (result > 0) {\n\t\t\tup_read(&card->controls_rwsem);\n\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_VALUE,\n\t\t\t\t       &control->id);\n\t\t\treturn 0;\n\t\t}\n\t}\n\tup_read(&card->controls_rwsem);\n\treturn result;\n}",
        "code_after_change": "static int snd_ctl_elem_write(struct snd_card *card, struct snd_ctl_file *file,\n\t\t\t      struct snd_ctl_elem_value *control)\n{\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int index_offset;\n\tint result;\n\n\tdown_read(&card->controls_rwsem);\n\tkctl = snd_ctl_find_id(card, &control->id);\n\tif (kctl == NULL) {\n\t\tresult = -ENOENT;\n\t} else {\n\t\tindex_offset = snd_ctl_get_ioff(kctl, &control->id);\n\t\tvd = &kctl->vd[index_offset];\n\t\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_WRITE) ||\n\t\t    kctl->put == NULL ||\n\t\t    (file && vd->owner && vd->owner != file)) {\n\t\t\tresult = -EPERM;\n\t\t} else {\n\t\t\tsnd_ctl_build_ioff(&control->id, kctl, index_offset);\n\t\t\tresult = kctl->put(kctl, control);\n\t\t}\n\t\tif (result > 0) {\n\t\t\tstruct snd_ctl_elem_id id = control->id;\n\t\t\tup_read(&card->controls_rwsem);\n\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_VALUE, &id);\n\t\t\treturn 0;\n\t\t}\n\t}\n\tup_read(&card->controls_rwsem);\n\treturn result;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,9 +22,9 @@\n \t\t\tresult = kctl->put(kctl, control);\n \t\t}\n \t\tif (result > 0) {\n+\t\t\tstruct snd_ctl_elem_id id = control->id;\n \t\t\tup_read(&card->controls_rwsem);\n-\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_VALUE,\n-\t\t\t\t       &control->id);\n+\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_VALUE, &id);\n \t\t\treturn 0;\n \t\t}\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tstruct snd_ctl_elem_id id = control->id;",
                "\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_VALUE, &id);"
            ],
            "deleted": [
                "\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_VALUE,",
                "\t\t\t\t       &control->id);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 does not ensure possession of a read/write lock, which allows local users to cause a denial of service (use-after-free) and obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "id": 569
    },
    {
        "cve_id": "CVE-2020-27067",
        "code_before_change": "static void __exit l2tp_eth_exit(void)\n{\n\tunregister_pernet_device(&l2tp_eth_net_ops);\n\tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);\n}",
        "code_after_change": "static void __exit l2tp_eth_exit(void)\n{\n\tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,4 @@\n static void __exit l2tp_eth_exit(void)\n {\n-\tunregister_pernet_device(&l2tp_eth_net_ops);\n \tl2tp_nl_unregister_ops(L2TP_PWTYPE_ETH);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tunregister_pernet_device(&l2tp_eth_net_ops);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the l2tp subsystem, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152409173",
        "id": 2614
    },
    {
        "cve_id": "CVE-2020-36387",
        "code_before_change": "static void io_poll_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_kiocb *nxt = NULL;\n\n\tio_poll_task_handler(req, &nxt);\n\tif (nxt)\n\t\t__io_req_task_submit(nxt);\n}",
        "code_after_change": "static void io_poll_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *nxt = NULL;\n\n\tio_poll_task_handler(req, &nxt);\n\tif (nxt)\n\t\t__io_req_task_submit(nxt);\n\tpercpu_ref_put(&ctx->refs);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,9 +1,11 @@\n static void io_poll_task_func(struct callback_head *cb)\n {\n \tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n+\tstruct io_ring_ctx *ctx = req->ctx;\n \tstruct io_kiocb *nxt = NULL;\n \n \tio_poll_task_handler(req, &nxt);\n \tif (nxt)\n \t\t__io_req_task_submit(nxt);\n+\tpercpu_ref_put(&ctx->refs);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct io_ring_ctx *ctx = req->ctx;",
                "\tpercpu_ref_put(&ctx->refs);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.8.2. fs/io_uring.c has a use-after-free related to io_async_task_func and ctx reference holding, aka CID-6d816e088c35.",
        "id": 2755
    },
    {
        "cve_id": "CVE-2023-0266",
        "code_before_change": "static int snd_ctl_elem_read(struct snd_card *card,\n\t\t\t     struct snd_ctl_elem_value *control)\n{\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int index_offset;\n\tstruct snd_ctl_elem_info info;\n\tconst u32 pattern = 0xdeadbeef;\n\tint ret;\n\n\tkctl = snd_ctl_find_id(card, &control->id);\n\tif (kctl == NULL)\n\t\treturn -ENOENT;\n\n\tindex_offset = snd_ctl_get_ioff(kctl, &control->id);\n\tvd = &kctl->vd[index_offset];\n\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL)\n\t\treturn -EPERM;\n\n\tsnd_ctl_build_ioff(&control->id, kctl, index_offset);\n\n#ifdef CONFIG_SND_CTL_DEBUG\n\t/* info is needed only for validation */\n\tmemset(&info, 0, sizeof(info));\n\tinfo.id = control->id;\n\tret = __snd_ctl_elem_info(card, kctl, &info, NULL);\n\tif (ret < 0)\n\t\treturn ret;\n#endif\n\n\tif (!snd_ctl_skip_validation(&info))\n\t\tfill_remaining_elem_value(control, &info, pattern);\n\tret = snd_power_ref_and_wait(card);\n\tif (!ret)\n\t\tret = kctl->get(kctl, control);\n\tsnd_power_unref(card);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (!snd_ctl_skip_validation(&info) &&\n\t    sanity_check_elem_value(card, control, &info, pattern) < 0) {\n\t\tdev_err(card->dev,\n\t\t\t\"control %i:%i:%i:%s:%i: access overflow\\n\",\n\t\t\tcontrol->id.iface, control->id.device,\n\t\t\tcontrol->id.subdevice, control->id.name,\n\t\t\tcontrol->id.index);\n\t\treturn -EINVAL;\n\t}\n\treturn ret;\n}",
        "code_after_change": "static int snd_ctl_elem_read(struct snd_card *card,\n\t\t\t     struct snd_ctl_elem_value *control)\n{\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int index_offset;\n\tstruct snd_ctl_elem_info info;\n\tconst u32 pattern = 0xdeadbeef;\n\tint ret;\n\n\tdown_read(&card->controls_rwsem);\n\tkctl = snd_ctl_find_id(card, &control->id);\n\tif (kctl == NULL) {\n\t\tret = -ENOENT;\n\t\tgoto unlock;\n\t}\n\n\tindex_offset = snd_ctl_get_ioff(kctl, &control->id);\n\tvd = &kctl->vd[index_offset];\n\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL) {\n\t\tret = -EPERM;\n\t\tgoto unlock;\n\t}\n\n\tsnd_ctl_build_ioff(&control->id, kctl, index_offset);\n\n#ifdef CONFIG_SND_CTL_DEBUG\n\t/* info is needed only for validation */\n\tmemset(&info, 0, sizeof(info));\n\tinfo.id = control->id;\n\tret = __snd_ctl_elem_info(card, kctl, &info, NULL);\n\tif (ret < 0)\n\t\tgoto unlock;\n#endif\n\n\tif (!snd_ctl_skip_validation(&info))\n\t\tfill_remaining_elem_value(control, &info, pattern);\n\tret = snd_power_ref_and_wait(card);\n\tif (!ret)\n\t\tret = kctl->get(kctl, control);\n\tsnd_power_unref(card);\n\tif (ret < 0)\n\t\tgoto unlock;\n\tif (!snd_ctl_skip_validation(&info) &&\n\t    sanity_check_elem_value(card, control, &info, pattern) < 0) {\n\t\tdev_err(card->dev,\n\t\t\t\"control %i:%i:%i:%s:%i: access overflow\\n\",\n\t\t\tcontrol->id.iface, control->id.device,\n\t\t\tcontrol->id.subdevice, control->id.name,\n\t\t\tcontrol->id.index);\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\nunlock:\n\tup_read(&card->controls_rwsem);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,14 +8,19 @@\n \tconst u32 pattern = 0xdeadbeef;\n \tint ret;\n \n+\tdown_read(&card->controls_rwsem);\n \tkctl = snd_ctl_find_id(card, &control->id);\n-\tif (kctl == NULL)\n-\t\treturn -ENOENT;\n+\tif (kctl == NULL) {\n+\t\tret = -ENOENT;\n+\t\tgoto unlock;\n+\t}\n \n \tindex_offset = snd_ctl_get_ioff(kctl, &control->id);\n \tvd = &kctl->vd[index_offset];\n-\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL)\n-\t\treturn -EPERM;\n+\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL) {\n+\t\tret = -EPERM;\n+\t\tgoto unlock;\n+\t}\n \n \tsnd_ctl_build_ioff(&control->id, kctl, index_offset);\n \n@@ -25,7 +30,7 @@\n \tinfo.id = control->id;\n \tret = __snd_ctl_elem_info(card, kctl, &info, NULL);\n \tif (ret < 0)\n-\t\treturn ret;\n+\t\tgoto unlock;\n #endif\n \n \tif (!snd_ctl_skip_validation(&info))\n@@ -35,7 +40,7 @@\n \t\tret = kctl->get(kctl, control);\n \tsnd_power_unref(card);\n \tif (ret < 0)\n-\t\treturn ret;\n+\t\tgoto unlock;\n \tif (!snd_ctl_skip_validation(&info) &&\n \t    sanity_check_elem_value(card, control, &info, pattern) < 0) {\n \t\tdev_err(card->dev,\n@@ -43,7 +48,10 @@\n \t\t\tcontrol->id.iface, control->id.device,\n \t\t\tcontrol->id.subdevice, control->id.name,\n \t\t\tcontrol->id.index);\n-\t\treturn -EINVAL;\n+\t\tret = -EINVAL;\n+\t\tgoto unlock;\n \t}\n+unlock:\n+\tup_read(&card->controls_rwsem);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tdown_read(&card->controls_rwsem);",
                "\tif (kctl == NULL) {",
                "\t\tret = -ENOENT;",
                "\t\tgoto unlock;",
                "\t}",
                "\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL) {",
                "\t\tret = -EPERM;",
                "\t\tgoto unlock;",
                "\t}",
                "\t\tgoto unlock;",
                "\t\tgoto unlock;",
                "\t\tret = -EINVAL;",
                "\t\tgoto unlock;",
                "unlock:",
                "\tup_read(&card->controls_rwsem);"
            ],
            "deleted": [
                "\tif (kctl == NULL)",
                "\t\treturn -ENOENT;",
                "\tif (!(vd->access & SNDRV_CTL_ELEM_ACCESS_READ) || kctl->get == NULL)",
                "\t\treturn -EPERM;",
                "\t\treturn ret;",
                "\t\treturn ret;",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free vulnerability exists in the ALSA PCM package in the Linux Kernel. SNDRV_CTL_IOCTL_ELEM_{READ|WRITE}32 is missing locks that can be used in a use-after-free that can result in a priviledge escalation to gain ring0 access from the system user. We recommend upgrading past commit 56b88b50565cd8b946a2d00b0c83927b7ebb055e\n",
        "id": 3821
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "int migrate_folio(struct address_space *mapping, struct folio *dst,\n\t\tstruct folio *src, enum migrate_mode mode)\n{\n\tint rc;\n\n\tBUG_ON(folio_test_writeback(src));\t/* Writeback must be complete */\n\n\trc = folio_migrate_mapping(mapping, dst, src, 0);\n\n\tif (rc != MIGRATEPAGE_SUCCESS)\n\t\treturn rc;\n\n\tif (mode != MIGRATE_SYNC_NO_COPY)\n\t\tfolio_migrate_copy(dst, src);\n\telse\n\t\tfolio_migrate_flags(dst, src);\n\treturn MIGRATEPAGE_SUCCESS;\n}",
        "code_after_change": "int migrate_folio(struct address_space *mapping, struct folio *dst,\n\t\tstruct folio *src, enum migrate_mode mode)\n{\n\treturn migrate_folio_extra(mapping, dst, src, mode, 0);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,18 +1,5 @@\n int migrate_folio(struct address_space *mapping, struct folio *dst,\n \t\tstruct folio *src, enum migrate_mode mode)\n {\n-\tint rc;\n-\n-\tBUG_ON(folio_test_writeback(src));\t/* Writeback must be complete */\n-\n-\trc = folio_migrate_mapping(mapping, dst, src, 0);\n-\n-\tif (rc != MIGRATEPAGE_SUCCESS)\n-\t\treturn rc;\n-\n-\tif (mode != MIGRATE_SYNC_NO_COPY)\n-\t\tfolio_migrate_copy(dst, src);\n-\telse\n-\t\tfolio_migrate_flags(dst, src);\n-\treturn MIGRATEPAGE_SUCCESS;\n+\treturn migrate_folio_extra(mapping, dst, src, mode, 0);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn migrate_folio_extra(mapping, dst, src, mode, 0);"
            ],
            "deleted": [
                "\tint rc;",
                "",
                "\tBUG_ON(folio_test_writeback(src));\t/* Writeback must be complete */",
                "",
                "\trc = folio_migrate_mapping(mapping, dst, src, 0);",
                "",
                "\tif (rc != MIGRATEPAGE_SUCCESS)",
                "\t\treturn rc;",
                "",
                "\tif (mode != MIGRATE_SYNC_NO_COPY)",
                "\t\tfolio_migrate_copy(dst, src);",
                "\telse",
                "\t\tfolio_migrate_flags(dst, src);",
                "\treturn MIGRATEPAGE_SUCCESS;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3619
    },
    {
        "cve_id": "CVE-2023-6932",
        "code_before_change": "static void igmp_start_timer(struct ip_mc_list *im, int max_delay)\n{\n\tint tv = get_random_u32_below(max_delay);\n\n\tim->tm_running = 1;\n\tif (!mod_timer(&im->timer, jiffies+tv+2))\n\t\trefcount_inc(&im->refcnt);\n}",
        "code_after_change": "static void igmp_start_timer(struct ip_mc_list *im, int max_delay)\n{\n\tint tv = get_random_u32_below(max_delay);\n\n\tim->tm_running = 1;\n\tif (refcount_inc_not_zero(&im->refcnt)) {\n\t\tif (mod_timer(&im->timer, jiffies + tv + 2))\n\t\t\tip_ma_put(im);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,8 @@\n \tint tv = get_random_u32_below(max_delay);\n \n \tim->tm_running = 1;\n-\tif (!mod_timer(&im->timer, jiffies+tv+2))\n-\t\trefcount_inc(&im->refcnt);\n+\tif (refcount_inc_not_zero(&im->refcnt)) {\n+\t\tif (mod_timer(&im->timer, jiffies + tv + 2))\n+\t\t\tip_ma_put(im);\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tif (refcount_inc_not_zero(&im->refcnt)) {",
                "\t\tif (mod_timer(&im->timer, jiffies + tv + 2))",
                "\t\t\tip_ma_put(im);",
                "\t}"
            ],
            "deleted": [
                "\tif (!mod_timer(&im->timer, jiffies+tv+2))",
                "\t\trefcount_inc(&im->refcnt);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's ipv4: igmp component can be exploited to achieve local privilege escalation.\n\nA race condition can be exploited to cause a timer be mistakenly registered on a RCU read locked object which is freed by another thread.\n\nWe recommend upgrading past commit e2b706c691905fe78468c361aaabc719d0a496f1.\n\n",
        "id": 4311
    },
    {
        "cve_id": "CVE-2018-14734",
        "code_before_change": "static ssize_t ucma_process_join(struct ucma_file *file,\n\t\t\t\t struct rdma_ucm_join_mcast *cmd,  int out_len)\n{\n\tstruct rdma_ucm_create_id_resp resp;\n\tstruct ucma_context *ctx;\n\tstruct ucma_multicast *mc;\n\tstruct sockaddr *addr;\n\tint ret;\n\tu8 join_state;\n\n\tif (out_len < sizeof(resp))\n\t\treturn -ENOSPC;\n\n\taddr = (struct sockaddr *) &cmd->addr;\n\tif (cmd->addr_size != rdma_addr_size(addr))\n\t\treturn -EINVAL;\n\n\tif (cmd->join_flags == RDMA_MC_JOIN_FLAG_FULLMEMBER)\n\t\tjoin_state = BIT(FULLMEMBER_JOIN);\n\telse if (cmd->join_flags == RDMA_MC_JOIN_FLAG_SENDONLY_FULLMEMBER)\n\t\tjoin_state = BIT(SENDONLY_FULLMEMBER_JOIN);\n\telse\n\t\treturn -EINVAL;\n\n\tctx = ucma_get_ctx_dev(file, cmd->id);\n\tif (IS_ERR(ctx))\n\t\treturn PTR_ERR(ctx);\n\n\tmutex_lock(&file->mut);\n\tmc = ucma_alloc_multicast(ctx);\n\tif (!mc) {\n\t\tret = -ENOMEM;\n\t\tgoto err1;\n\t}\n\tmc->join_state = join_state;\n\tmc->uid = cmd->uid;\n\tmemcpy(&mc->addr, addr, cmd->addr_size);\n\tret = rdma_join_multicast(ctx->cm_id, (struct sockaddr *)&mc->addr,\n\t\t\t\t  join_state, mc);\n\tif (ret)\n\t\tgoto err2;\n\n\tresp.id = mc->id;\n\tif (copy_to_user(u64_to_user_ptr(cmd->response),\n\t\t\t &resp, sizeof(resp))) {\n\t\tret = -EFAULT;\n\t\tgoto err3;\n\t}\n\n\tmutex_unlock(&file->mut);\n\tucma_put_ctx(ctx);\n\treturn 0;\n\nerr3:\n\trdma_leave_multicast(ctx->cm_id, (struct sockaddr *) &mc->addr);\n\tucma_cleanup_mc_events(mc);\nerr2:\n\tmutex_lock(&mut);\n\tidr_remove(&multicast_idr, mc->id);\n\tmutex_unlock(&mut);\n\tlist_del(&mc->list);\n\tkfree(mc);\nerr1:\n\tmutex_unlock(&file->mut);\n\tucma_put_ctx(ctx);\n\treturn ret;\n}",
        "code_after_change": "static ssize_t ucma_process_join(struct ucma_file *file,\n\t\t\t\t struct rdma_ucm_join_mcast *cmd,  int out_len)\n{\n\tstruct rdma_ucm_create_id_resp resp;\n\tstruct ucma_context *ctx;\n\tstruct ucma_multicast *mc;\n\tstruct sockaddr *addr;\n\tint ret;\n\tu8 join_state;\n\n\tif (out_len < sizeof(resp))\n\t\treturn -ENOSPC;\n\n\taddr = (struct sockaddr *) &cmd->addr;\n\tif (cmd->addr_size != rdma_addr_size(addr))\n\t\treturn -EINVAL;\n\n\tif (cmd->join_flags == RDMA_MC_JOIN_FLAG_FULLMEMBER)\n\t\tjoin_state = BIT(FULLMEMBER_JOIN);\n\telse if (cmd->join_flags == RDMA_MC_JOIN_FLAG_SENDONLY_FULLMEMBER)\n\t\tjoin_state = BIT(SENDONLY_FULLMEMBER_JOIN);\n\telse\n\t\treturn -EINVAL;\n\n\tctx = ucma_get_ctx_dev(file, cmd->id);\n\tif (IS_ERR(ctx))\n\t\treturn PTR_ERR(ctx);\n\n\tmutex_lock(&file->mut);\n\tmc = ucma_alloc_multicast(ctx);\n\tif (!mc) {\n\t\tret = -ENOMEM;\n\t\tgoto err1;\n\t}\n\tmc->join_state = join_state;\n\tmc->uid = cmd->uid;\n\tmemcpy(&mc->addr, addr, cmd->addr_size);\n\tret = rdma_join_multicast(ctx->cm_id, (struct sockaddr *)&mc->addr,\n\t\t\t\t  join_state, mc);\n\tif (ret)\n\t\tgoto err2;\n\n\tresp.id = mc->id;\n\tif (copy_to_user(u64_to_user_ptr(cmd->response),\n\t\t\t &resp, sizeof(resp))) {\n\t\tret = -EFAULT;\n\t\tgoto err3;\n\t}\n\n\tmutex_lock(&mut);\n\tidr_replace(&multicast_idr, mc, mc->id);\n\tmutex_unlock(&mut);\n\n\tmutex_unlock(&file->mut);\n\tucma_put_ctx(ctx);\n\treturn 0;\n\nerr3:\n\trdma_leave_multicast(ctx->cm_id, (struct sockaddr *) &mc->addr);\n\tucma_cleanup_mc_events(mc);\nerr2:\n\tmutex_lock(&mut);\n\tidr_remove(&multicast_idr, mc->id);\n\tmutex_unlock(&mut);\n\tlist_del(&mc->list);\n\tkfree(mc);\nerr1:\n\tmutex_unlock(&file->mut);\n\tucma_put_ctx(ctx);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,6 +47,10 @@\n \t\tgoto err3;\n \t}\n \n+\tmutex_lock(&mut);\n+\tidr_replace(&multicast_idr, mc, mc->id);\n+\tmutex_unlock(&mut);\n+\n \tmutex_unlock(&file->mut);\n \tucma_put_ctx(ctx);\n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&mut);",
                "\tidr_replace(&multicast_idr, mc, mc->id);",
                "\tmutex_unlock(&mut);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "drivers/infiniband/core/ucma.c in the Linux kernel through 4.17.11 allows ucma_leave_multicast to access a certain data structure after a cleanup step in ucma_process_join, which allows attackers to cause a denial of service (use-after-free).",
        "id": 1705
    },
    {
        "cve_id": "CVE-2019-2215",
        "code_before_change": "static int binder_thread_release(struct binder_proc *proc,\n\t\t\t\t struct binder_thread *thread)\n{\n\tstruct binder_transaction *t;\n\tstruct binder_transaction *send_reply = NULL;\n\tint active_transactions = 0;\n\tstruct binder_transaction *last_t = NULL;\n\n\tbinder_inner_proc_lock(thread->proc);\n\t/*\n\t * take a ref on the proc so it survives\n\t * after we remove this thread from proc->threads.\n\t * The corresponding dec is when we actually\n\t * free the thread in binder_free_thread()\n\t */\n\tproc->tmp_ref++;\n\t/*\n\t * take a ref on this thread to ensure it\n\t * survives while we are releasing it\n\t */\n\tatomic_inc(&thread->tmp_ref);\n\trb_erase(&thread->rb_node, &proc->threads);\n\tt = thread->transaction_stack;\n\tif (t) {\n\t\tspin_lock(&t->lock);\n\t\tif (t->to_thread == thread)\n\t\t\tsend_reply = t;\n\t}\n\tthread->is_dead = true;\n\n\twhile (t) {\n\t\tlast_t = t;\n\t\tactive_transactions++;\n\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t     \"release %d:%d transaction %d %s, still active\\n\",\n\t\t\t      proc->pid, thread->pid,\n\t\t\t     t->debug_id,\n\t\t\t     (t->to_thread == thread) ? \"in\" : \"out\");\n\n\t\tif (t->to_thread == thread) {\n\t\t\tt->to_proc = NULL;\n\t\t\tt->to_thread = NULL;\n\t\t\tif (t->buffer) {\n\t\t\t\tt->buffer->transaction = NULL;\n\t\t\t\tt->buffer = NULL;\n\t\t\t}\n\t\t\tt = t->to_parent;\n\t\t} else if (t->from == thread) {\n\t\t\tt->from = NULL;\n\t\t\tt = t->from_parent;\n\t\t} else\n\t\t\tBUG();\n\t\tspin_unlock(&last_t->lock);\n\t\tif (t)\n\t\t\tspin_lock(&t->lock);\n\t}\n\tbinder_inner_proc_unlock(thread->proc);\n\n\tif (send_reply)\n\t\tbinder_send_failed_reply(send_reply, BR_DEAD_REPLY);\n\tbinder_release_work(proc, &thread->todo);\n\tbinder_thread_dec_tmpref(thread);\n\treturn active_transactions;\n}",
        "code_after_change": "static int binder_thread_release(struct binder_proc *proc,\n\t\t\t\t struct binder_thread *thread)\n{\n\tstruct binder_transaction *t;\n\tstruct binder_transaction *send_reply = NULL;\n\tint active_transactions = 0;\n\tstruct binder_transaction *last_t = NULL;\n\n\tbinder_inner_proc_lock(thread->proc);\n\t/*\n\t * take a ref on the proc so it survives\n\t * after we remove this thread from proc->threads.\n\t * The corresponding dec is when we actually\n\t * free the thread in binder_free_thread()\n\t */\n\tproc->tmp_ref++;\n\t/*\n\t * take a ref on this thread to ensure it\n\t * survives while we are releasing it\n\t */\n\tatomic_inc(&thread->tmp_ref);\n\trb_erase(&thread->rb_node, &proc->threads);\n\tt = thread->transaction_stack;\n\tif (t) {\n\t\tspin_lock(&t->lock);\n\t\tif (t->to_thread == thread)\n\t\t\tsend_reply = t;\n\t}\n\tthread->is_dead = true;\n\n\twhile (t) {\n\t\tlast_t = t;\n\t\tactive_transactions++;\n\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t     \"release %d:%d transaction %d %s, still active\\n\",\n\t\t\t      proc->pid, thread->pid,\n\t\t\t     t->debug_id,\n\t\t\t     (t->to_thread == thread) ? \"in\" : \"out\");\n\n\t\tif (t->to_thread == thread) {\n\t\t\tt->to_proc = NULL;\n\t\t\tt->to_thread = NULL;\n\t\t\tif (t->buffer) {\n\t\t\t\tt->buffer->transaction = NULL;\n\t\t\t\tt->buffer = NULL;\n\t\t\t}\n\t\t\tt = t->to_parent;\n\t\t} else if (t->from == thread) {\n\t\t\tt->from = NULL;\n\t\t\tt = t->from_parent;\n\t\t} else\n\t\t\tBUG();\n\t\tspin_unlock(&last_t->lock);\n\t\tif (t)\n\t\t\tspin_lock(&t->lock);\n\t}\n\n\t/*\n\t * If this thread used poll, make sure we remove the waitqueue\n\t * from any epoll data structures holding it with POLLFREE.\n\t * waitqueue_active() is safe to use here because we're holding\n\t * the inner lock.\n\t */\n\tif ((thread->looper & BINDER_LOOPER_STATE_POLL) &&\n\t    waitqueue_active(&thread->wait)) {\n\t\twake_up_poll(&thread->wait, POLLHUP | POLLFREE);\n\t}\n\n\tbinder_inner_proc_unlock(thread->proc);\n\n\tif (send_reply)\n\t\tbinder_send_failed_reply(send_reply, BR_DEAD_REPLY);\n\tbinder_release_work(proc, &thread->todo);\n\tbinder_thread_dec_tmpref(thread);\n\treturn active_transactions;\n}",
        "patch": "--- code before\n+++ code after\n@@ -54,6 +54,18 @@\n \t\tif (t)\n \t\t\tspin_lock(&t->lock);\n \t}\n+\n+\t/*\n+\t * If this thread used poll, make sure we remove the waitqueue\n+\t * from any epoll data structures holding it with POLLFREE.\n+\t * waitqueue_active() is safe to use here because we're holding\n+\t * the inner lock.\n+\t */\n+\tif ((thread->looper & BINDER_LOOPER_STATE_POLL) &&\n+\t    waitqueue_active(&thread->wait)) {\n+\t\twake_up_poll(&thread->wait, POLLHUP | POLLFREE);\n+\t}\n+\n \tbinder_inner_proc_unlock(thread->proc);\n \n \tif (send_reply)",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * If this thread used poll, make sure we remove the waitqueue",
                "\t * from any epoll data structures holding it with POLLFREE.",
                "\t * waitqueue_active() is safe to use here because we're holding",
                "\t * the inner lock.",
                "\t */",
                "\tif ((thread->looper & BINDER_LOOPER_STATE_POLL) &&",
                "\t    waitqueue_active(&thread->wait)) {",
                "\t\twake_up_poll(&thread->wait, POLLHUP | POLLFREE);",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free in binder.c allows an elevation of privilege from an application to the Linux Kernel. No user interaction is required to exploit this vulnerability, however exploitation does require either the installation of a malicious local application or a separate vulnerability in a network facing application.Product: AndroidAndroid ID: A-141720095",
        "id": 2297
    },
    {
        "cve_id": "CVE-2023-4623",
        "code_before_change": "static int\nhfsc_change_class(struct Qdisc *sch, u32 classid, u32 parentid,\n\t\t  struct nlattr **tca, unsigned long *arg,\n\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct hfsc_sched *q = qdisc_priv(sch);\n\tstruct hfsc_class *cl = (struct hfsc_class *)*arg;\n\tstruct hfsc_class *parent = NULL;\n\tstruct nlattr *opt = tca[TCA_OPTIONS];\n\tstruct nlattr *tb[TCA_HFSC_MAX + 1];\n\tstruct tc_service_curve *rsc = NULL, *fsc = NULL, *usc = NULL;\n\tu64 cur_time;\n\tint err;\n\n\tif (opt == NULL)\n\t\treturn -EINVAL;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_HFSC_MAX, opt, hfsc_policy,\n\t\t\t\t\t  NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_HFSC_RSC]) {\n\t\trsc = nla_data(tb[TCA_HFSC_RSC]);\n\t\tif (rsc->m1 == 0 && rsc->m2 == 0)\n\t\t\trsc = NULL;\n\t}\n\n\tif (tb[TCA_HFSC_FSC]) {\n\t\tfsc = nla_data(tb[TCA_HFSC_FSC]);\n\t\tif (fsc->m1 == 0 && fsc->m2 == 0)\n\t\t\tfsc = NULL;\n\t}\n\n\tif (tb[TCA_HFSC_USC]) {\n\t\tusc = nla_data(tb[TCA_HFSC_USC]);\n\t\tif (usc->m1 == 0 && usc->m2 == 0)\n\t\t\tusc = NULL;\n\t}\n\n\tif (cl != NULL) {\n\t\tint old_flags;\n\n\t\tif (parentid) {\n\t\t\tif (cl->cl_parent &&\n\t\t\t    cl->cl_parent->cl_common.classid != parentid)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (cl->cl_parent == NULL && parentid != TC_H_ROOT)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tcur_time = psched_get_time();\n\n\t\tif (tca[TCA_RATE]) {\n\t\t\terr = gen_replace_estimator(&cl->bstats, NULL,\n\t\t\t\t\t\t    &cl->rate_est,\n\t\t\t\t\t\t    NULL,\n\t\t\t\t\t\t    true,\n\t\t\t\t\t\t    tca[TCA_RATE]);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tsch_tree_lock(sch);\n\t\told_flags = cl->cl_flags;\n\n\t\tif (rsc != NULL)\n\t\t\thfsc_change_rsc(cl, rsc, cur_time);\n\t\tif (fsc != NULL)\n\t\t\thfsc_change_fsc(cl, fsc);\n\t\tif (usc != NULL)\n\t\t\thfsc_change_usc(cl, usc, cur_time);\n\n\t\tif (cl->qdisc->q.qlen != 0) {\n\t\t\tint len = qdisc_peek_len(cl->qdisc);\n\n\t\t\tif (cl->cl_flags & HFSC_RSC) {\n\t\t\t\tif (old_flags & HFSC_RSC)\n\t\t\t\t\tupdate_ed(cl, len);\n\t\t\t\telse\n\t\t\t\t\tinit_ed(cl, len);\n\t\t\t}\n\n\t\t\tif (cl->cl_flags & HFSC_FSC) {\n\t\t\t\tif (old_flags & HFSC_FSC)\n\t\t\t\t\tupdate_vf(cl, 0, cur_time);\n\t\t\t\telse\n\t\t\t\t\tinit_vf(cl, len);\n\t\t\t}\n\t\t}\n\t\tsch_tree_unlock(sch);\n\n\t\treturn 0;\n\t}\n\n\tif (parentid == TC_H_ROOT)\n\t\treturn -EEXIST;\n\n\tparent = &q->root;\n\tif (parentid) {\n\t\tparent = hfsc_find_class(parentid, sch);\n\t\tif (parent == NULL)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (classid == 0 || TC_H_MAJ(classid ^ sch->handle) != 0)\n\t\treturn -EINVAL;\n\tif (hfsc_find_class(classid, sch))\n\t\treturn -EEXIST;\n\n\tif (rsc == NULL && fsc == NULL)\n\t\treturn -EINVAL;\n\n\tcl = kzalloc(sizeof(struct hfsc_class), GFP_KERNEL);\n\tif (cl == NULL)\n\t\treturn -ENOBUFS;\n\n\terr = tcf_block_get(&cl->block, &cl->filter_list, sch, extack);\n\tif (err) {\n\t\tkfree(cl);\n\t\treturn err;\n\t}\n\n\tif (tca[TCA_RATE]) {\n\t\terr = gen_new_estimator(&cl->bstats, NULL, &cl->rate_est,\n\t\t\t\t\tNULL, true, tca[TCA_RATE]);\n\t\tif (err) {\n\t\t\ttcf_block_put(cl->block);\n\t\t\tkfree(cl);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (rsc != NULL)\n\t\thfsc_change_rsc(cl, rsc, 0);\n\tif (fsc != NULL)\n\t\thfsc_change_fsc(cl, fsc);\n\tif (usc != NULL)\n\t\thfsc_change_usc(cl, usc, 0);\n\n\tcl->cl_common.classid = classid;\n\tcl->sched     = q;\n\tcl->cl_parent = parent;\n\tcl->qdisc = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t      classid, NULL);\n\tif (cl->qdisc == NULL)\n\t\tcl->qdisc = &noop_qdisc;\n\telse\n\t\tqdisc_hash_add(cl->qdisc, true);\n\tINIT_LIST_HEAD(&cl->children);\n\tcl->vt_tree = RB_ROOT;\n\tcl->cf_tree = RB_ROOT;\n\n\tsch_tree_lock(sch);\n\tqdisc_class_hash_insert(&q->clhash, &cl->cl_common);\n\tlist_add_tail(&cl->siblings, &parent->children);\n\tif (parent->level == 0)\n\t\tqdisc_purge_queue(parent->qdisc);\n\thfsc_adjust_levels(parent);\n\tsch_tree_unlock(sch);\n\n\tqdisc_class_hash_grow(sch, &q->clhash);\n\n\t*arg = (unsigned long)cl;\n\treturn 0;\n}",
        "code_after_change": "static int\nhfsc_change_class(struct Qdisc *sch, u32 classid, u32 parentid,\n\t\t  struct nlattr **tca, unsigned long *arg,\n\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct hfsc_sched *q = qdisc_priv(sch);\n\tstruct hfsc_class *cl = (struct hfsc_class *)*arg;\n\tstruct hfsc_class *parent = NULL;\n\tstruct nlattr *opt = tca[TCA_OPTIONS];\n\tstruct nlattr *tb[TCA_HFSC_MAX + 1];\n\tstruct tc_service_curve *rsc = NULL, *fsc = NULL, *usc = NULL;\n\tu64 cur_time;\n\tint err;\n\n\tif (opt == NULL)\n\t\treturn -EINVAL;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_HFSC_MAX, opt, hfsc_policy,\n\t\t\t\t\t  NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_HFSC_RSC]) {\n\t\trsc = nla_data(tb[TCA_HFSC_RSC]);\n\t\tif (rsc->m1 == 0 && rsc->m2 == 0)\n\t\t\trsc = NULL;\n\t}\n\n\tif (tb[TCA_HFSC_FSC]) {\n\t\tfsc = nla_data(tb[TCA_HFSC_FSC]);\n\t\tif (fsc->m1 == 0 && fsc->m2 == 0)\n\t\t\tfsc = NULL;\n\t}\n\n\tif (tb[TCA_HFSC_USC]) {\n\t\tusc = nla_data(tb[TCA_HFSC_USC]);\n\t\tif (usc->m1 == 0 && usc->m2 == 0)\n\t\t\tusc = NULL;\n\t}\n\n\tif (cl != NULL) {\n\t\tint old_flags;\n\n\t\tif (parentid) {\n\t\t\tif (cl->cl_parent &&\n\t\t\t    cl->cl_parent->cl_common.classid != parentid)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (cl->cl_parent == NULL && parentid != TC_H_ROOT)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tcur_time = psched_get_time();\n\n\t\tif (tca[TCA_RATE]) {\n\t\t\terr = gen_replace_estimator(&cl->bstats, NULL,\n\t\t\t\t\t\t    &cl->rate_est,\n\t\t\t\t\t\t    NULL,\n\t\t\t\t\t\t    true,\n\t\t\t\t\t\t    tca[TCA_RATE]);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tsch_tree_lock(sch);\n\t\told_flags = cl->cl_flags;\n\n\t\tif (rsc != NULL)\n\t\t\thfsc_change_rsc(cl, rsc, cur_time);\n\t\tif (fsc != NULL)\n\t\t\thfsc_change_fsc(cl, fsc);\n\t\tif (usc != NULL)\n\t\t\thfsc_change_usc(cl, usc, cur_time);\n\n\t\tif (cl->qdisc->q.qlen != 0) {\n\t\t\tint len = qdisc_peek_len(cl->qdisc);\n\n\t\t\tif (cl->cl_flags & HFSC_RSC) {\n\t\t\t\tif (old_flags & HFSC_RSC)\n\t\t\t\t\tupdate_ed(cl, len);\n\t\t\t\telse\n\t\t\t\t\tinit_ed(cl, len);\n\t\t\t}\n\n\t\t\tif (cl->cl_flags & HFSC_FSC) {\n\t\t\t\tif (old_flags & HFSC_FSC)\n\t\t\t\t\tupdate_vf(cl, 0, cur_time);\n\t\t\t\telse\n\t\t\t\t\tinit_vf(cl, len);\n\t\t\t}\n\t\t}\n\t\tsch_tree_unlock(sch);\n\n\t\treturn 0;\n\t}\n\n\tif (parentid == TC_H_ROOT)\n\t\treturn -EEXIST;\n\n\tparent = &q->root;\n\tif (parentid) {\n\t\tparent = hfsc_find_class(parentid, sch);\n\t\tif (parent == NULL)\n\t\t\treturn -ENOENT;\n\t}\n\tif (!(parent->cl_flags & HFSC_FSC) && parent != &q->root) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid parent - parent class must have FSC\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (classid == 0 || TC_H_MAJ(classid ^ sch->handle) != 0)\n\t\treturn -EINVAL;\n\tif (hfsc_find_class(classid, sch))\n\t\treturn -EEXIST;\n\n\tif (rsc == NULL && fsc == NULL)\n\t\treturn -EINVAL;\n\n\tcl = kzalloc(sizeof(struct hfsc_class), GFP_KERNEL);\n\tif (cl == NULL)\n\t\treturn -ENOBUFS;\n\n\terr = tcf_block_get(&cl->block, &cl->filter_list, sch, extack);\n\tif (err) {\n\t\tkfree(cl);\n\t\treturn err;\n\t}\n\n\tif (tca[TCA_RATE]) {\n\t\terr = gen_new_estimator(&cl->bstats, NULL, &cl->rate_est,\n\t\t\t\t\tNULL, true, tca[TCA_RATE]);\n\t\tif (err) {\n\t\t\ttcf_block_put(cl->block);\n\t\t\tkfree(cl);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (rsc != NULL)\n\t\thfsc_change_rsc(cl, rsc, 0);\n\tif (fsc != NULL)\n\t\thfsc_change_fsc(cl, fsc);\n\tif (usc != NULL)\n\t\thfsc_change_usc(cl, usc, 0);\n\n\tcl->cl_common.classid = classid;\n\tcl->sched     = q;\n\tcl->cl_parent = parent;\n\tcl->qdisc = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t      classid, NULL);\n\tif (cl->qdisc == NULL)\n\t\tcl->qdisc = &noop_qdisc;\n\telse\n\t\tqdisc_hash_add(cl->qdisc, true);\n\tINIT_LIST_HEAD(&cl->children);\n\tcl->vt_tree = RB_ROOT;\n\tcl->cf_tree = RB_ROOT;\n\n\tsch_tree_lock(sch);\n\tqdisc_class_hash_insert(&q->clhash, &cl->cl_common);\n\tlist_add_tail(&cl->siblings, &parent->children);\n\tif (parent->level == 0)\n\t\tqdisc_purge_queue(parent->qdisc);\n\thfsc_adjust_levels(parent);\n\tsch_tree_unlock(sch);\n\n\tqdisc_class_hash_grow(sch, &q->clhash);\n\n\t*arg = (unsigned long)cl;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -101,6 +101,10 @@\n \t\tif (parent == NULL)\n \t\t\treturn -ENOENT;\n \t}\n+\tif (!(parent->cl_flags & HFSC_FSC) && parent != &q->root) {\n+\t\tNL_SET_ERR_MSG(extack, \"Invalid parent - parent class must have FSC\");\n+\t\treturn -EINVAL;\n+\t}\n \n \tif (classid == 0 || TC_H_MAJ(classid ^ sch->handle) != 0)\n \t\treturn -EINVAL;",
        "function_modified_lines": {
            "added": [
                "\tif (!(parent->cl_flags & HFSC_FSC) && parent != &q->root) {",
                "\t\tNL_SET_ERR_MSG(extack, \"Invalid parent - parent class must have FSC\");",
                "\t\treturn -EINVAL;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's net/sched: sch_hfsc (HFSC qdisc traffic control) component can be exploited to achieve local privilege escalation.\n\nIf a class with a link-sharing curve (i.e. with the HFSC_FSC flag set) has a parent without a link-sharing curve, then init_vf() will call vttree_insert() on the parent, but vttree_remove() will be skipped in update_vf(). This leaves a dangling pointer that can cause a use-after-free.\n\nWe recommend upgrading past commit b3d26c5702c7d6c45456326e56d2ccf3f103e60f.\n\n",
        "id": 4239
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int\nvmw_gb_surface_define_internal(struct drm_device *dev,\n\t\t\t       struct drm_vmw_gb_surface_create_ext_req *req,\n\t\t\t       struct drm_vmw_gb_surface_create_rep *rep,\n\t\t\t       struct drm_file *file_priv)\n{\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_user_surface *user_srf;\n\tstruct vmw_surface_metadata metadata = {0};\n\tstruct vmw_surface *srf;\n\tstruct vmw_resource *res;\n\tstruct vmw_resource *tmp;\n\tint ret = 0;\n\tuint32_t backup_handle = 0;\n\tSVGA3dSurfaceAllFlags svga3d_flags_64 =\n\t\tSVGA3D_FLAGS_64(req->svga3d_flags_upper_32_bits,\n\t\t\t\treq->base.svga3d_flags);\n\n\t/* array_size must be null for non-GL3 host. */\n\tif (req->base.array_size > 0 && !has_sm4_context(dev_priv)) {\n\t\tVMW_DEBUG_USER(\"SM4 surface not supported.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!has_sm4_1_context(dev_priv)) {\n\t\tif (req->svga3d_flags_upper_32_bits != 0)\n\t\t\tret = -EINVAL;\n\n\t\tif (req->base.multisample_count != 0)\n\t\t\tret = -EINVAL;\n\n\t\tif (req->multisample_pattern != SVGA3D_MS_PATTERN_NONE)\n\t\t\tret = -EINVAL;\n\n\t\tif (req->quality_level != SVGA3D_MS_QUALITY_NONE)\n\t\t\tret = -EINVAL;\n\n\t\tif (ret) {\n\t\t\tVMW_DEBUG_USER(\"SM4.1 surface not supported.\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (req->buffer_byte_stride > 0 && !has_sm5_context(dev_priv)) {\n\t\tVMW_DEBUG_USER(\"SM5 surface not supported.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif ((svga3d_flags_64 & SVGA3D_SURFACE_MULTISAMPLE) &&\n\t    req->base.multisample_count == 0) {\n\t\tVMW_DEBUG_USER(\"Invalid sample count.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (req->base.mip_levels > DRM_VMW_MAX_MIP_LEVELS) {\n\t\tVMW_DEBUG_USER(\"Invalid mip level.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmetadata.flags = svga3d_flags_64;\n\tmetadata.format = req->base.format;\n\tmetadata.mip_levels[0] = req->base.mip_levels;\n\tmetadata.multisample_count = req->base.multisample_count;\n\tmetadata.multisample_pattern = req->multisample_pattern;\n\tmetadata.quality_level = req->quality_level;\n\tmetadata.array_size = req->base.array_size;\n\tmetadata.buffer_byte_stride = req->buffer_byte_stride;\n\tmetadata.num_sizes = 1;\n\tmetadata.base_size = req->base.base_size;\n\tmetadata.scanout = req->base.drm_surface_flags &\n\t\tdrm_vmw_surface_flag_scanout;\n\n\t/* Define a surface based on the parameters. */\n\tret = vmw_gb_surface_define(dev_priv, &metadata, &srf);\n\tif (ret != 0) {\n\t\tVMW_DEBUG_USER(\"Failed to define surface.\\n\");\n\t\treturn ret;\n\t}\n\n\tuser_srf = container_of(srf, struct vmw_user_surface, srf);\n\tif (drm_is_primary_client(file_priv))\n\t\tuser_srf->master = drm_file_get_master(file_priv);\n\n\tres = &user_srf->srf.res;\n\n\tif (req->base.buffer_handle != SVGA3D_INVALID_ID) {\n\t\tret = vmw_user_bo_lookup(file_priv, req->base.buffer_handle,\n\t\t\t\t\t &res->guest_memory_bo);\n\t\tif (ret == 0) {\n\t\t\tif (res->guest_memory_bo->tbo.base.size < res->guest_memory_size) {\n\t\t\t\tVMW_DEBUG_USER(\"Surface backup buffer too small.\\n\");\n\t\t\t\tvmw_bo_unreference(&res->guest_memory_bo);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_unlock;\n\t\t\t} else {\n\t\t\t\tbackup_handle = req->base.buffer_handle;\n\t\t\t}\n\t\t}\n\t} else if (req->base.drm_surface_flags &\n\t\t   (drm_vmw_surface_flag_create_buffer |\n\t\t    drm_vmw_surface_flag_coherent)) {\n\t\tret = vmw_gem_object_create_with_handle(dev_priv, file_priv,\n\t\t\t\t\t\t\tres->guest_memory_size,\n\t\t\t\t\t\t\t&backup_handle,\n\t\t\t\t\t\t\t&res->guest_memory_bo);\n\t\tif (ret == 0)\n\t\t\tvmw_bo_reference(res->guest_memory_bo);\n\t}\n\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\n\tif (req->base.drm_surface_flags & drm_vmw_surface_flag_coherent) {\n\t\tstruct vmw_bo *backup = res->guest_memory_bo;\n\n\t\tttm_bo_reserve(&backup->tbo, false, false, NULL);\n\t\tif (!res->func->dirty_alloc)\n\t\t\tret = -EINVAL;\n\t\tif (!ret)\n\t\t\tret = vmw_bo_dirty_add(backup);\n\t\tif (!ret) {\n\t\t\tres->coherent = true;\n\t\t\tret = res->func->dirty_alloc(res);\n\t\t}\n\t\tttm_bo_unreserve(&backup->tbo);\n\t\tif (ret) {\n\t\t\tvmw_resource_unreference(&res);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t}\n\n\ttmp = vmw_resource_reference(res);\n\tret = ttm_prime_object_init(tfile, res->guest_memory_size, &user_srf->prime,\n\t\t\t\t    req->base.drm_surface_flags &\n\t\t\t\t    drm_vmw_surface_flag_shareable,\n\t\t\t\t    VMW_RES_SURFACE,\n\t\t\t\t    &vmw_user_surface_base_release);\n\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&tmp);\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\n\trep->handle      = user_srf->prime.base.handle;\n\trep->backup_size = res->guest_memory_size;\n\tif (res->guest_memory_bo) {\n\t\trep->buffer_map_handle =\n\t\t\tdrm_vma_node_offset_addr(&res->guest_memory_bo->tbo.base.vma_node);\n\t\trep->buffer_size = res->guest_memory_bo->tbo.base.size;\n\t\trep->buffer_handle = backup_handle;\n\t} else {\n\t\trep->buffer_map_handle = 0;\n\t\trep->buffer_size = 0;\n\t\trep->buffer_handle = SVGA3D_INVALID_ID;\n\t}\n\tvmw_resource_unreference(&res);\n\nout_unlock:\n\treturn ret;\n}",
        "code_after_change": "static int\nvmw_gb_surface_define_internal(struct drm_device *dev,\n\t\t\t       struct drm_vmw_gb_surface_create_ext_req *req,\n\t\t\t       struct drm_vmw_gb_surface_create_rep *rep,\n\t\t\t       struct drm_file *file_priv)\n{\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_user_surface *user_srf;\n\tstruct vmw_surface_metadata metadata = {0};\n\tstruct vmw_surface *srf;\n\tstruct vmw_resource *res;\n\tstruct vmw_resource *tmp;\n\tint ret = 0;\n\tuint32_t backup_handle = 0;\n\tSVGA3dSurfaceAllFlags svga3d_flags_64 =\n\t\tSVGA3D_FLAGS_64(req->svga3d_flags_upper_32_bits,\n\t\t\t\treq->base.svga3d_flags);\n\n\t/* array_size must be null for non-GL3 host. */\n\tif (req->base.array_size > 0 && !has_sm4_context(dev_priv)) {\n\t\tVMW_DEBUG_USER(\"SM4 surface not supported.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!has_sm4_1_context(dev_priv)) {\n\t\tif (req->svga3d_flags_upper_32_bits != 0)\n\t\t\tret = -EINVAL;\n\n\t\tif (req->base.multisample_count != 0)\n\t\t\tret = -EINVAL;\n\n\t\tif (req->multisample_pattern != SVGA3D_MS_PATTERN_NONE)\n\t\t\tret = -EINVAL;\n\n\t\tif (req->quality_level != SVGA3D_MS_QUALITY_NONE)\n\t\t\tret = -EINVAL;\n\n\t\tif (ret) {\n\t\t\tVMW_DEBUG_USER(\"SM4.1 surface not supported.\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (req->buffer_byte_stride > 0 && !has_sm5_context(dev_priv)) {\n\t\tVMW_DEBUG_USER(\"SM5 surface not supported.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif ((svga3d_flags_64 & SVGA3D_SURFACE_MULTISAMPLE) &&\n\t    req->base.multisample_count == 0) {\n\t\tVMW_DEBUG_USER(\"Invalid sample count.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (req->base.mip_levels > DRM_VMW_MAX_MIP_LEVELS) {\n\t\tVMW_DEBUG_USER(\"Invalid mip level.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmetadata.flags = svga3d_flags_64;\n\tmetadata.format = req->base.format;\n\tmetadata.mip_levels[0] = req->base.mip_levels;\n\tmetadata.multisample_count = req->base.multisample_count;\n\tmetadata.multisample_pattern = req->multisample_pattern;\n\tmetadata.quality_level = req->quality_level;\n\tmetadata.array_size = req->base.array_size;\n\tmetadata.buffer_byte_stride = req->buffer_byte_stride;\n\tmetadata.num_sizes = 1;\n\tmetadata.base_size = req->base.base_size;\n\tmetadata.scanout = req->base.drm_surface_flags &\n\t\tdrm_vmw_surface_flag_scanout;\n\n\t/* Define a surface based on the parameters. */\n\tret = vmw_gb_surface_define(dev_priv, &metadata, &srf);\n\tif (ret != 0) {\n\t\tVMW_DEBUG_USER(\"Failed to define surface.\\n\");\n\t\treturn ret;\n\t}\n\n\tuser_srf = container_of(srf, struct vmw_user_surface, srf);\n\tif (drm_is_primary_client(file_priv))\n\t\tuser_srf->master = drm_file_get_master(file_priv);\n\n\tres = &user_srf->srf.res;\n\n\tif (req->base.buffer_handle != SVGA3D_INVALID_ID) {\n\t\tret = vmw_user_bo_lookup(file_priv, req->base.buffer_handle,\n\t\t\t\t\t &res->guest_memory_bo);\n\t\tif (ret == 0) {\n\t\t\tif (res->guest_memory_bo->tbo.base.size < res->guest_memory_size) {\n\t\t\t\tVMW_DEBUG_USER(\"Surface backup buffer too small.\\n\");\n\t\t\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_unlock;\n\t\t\t} else {\n\t\t\t\tbackup_handle = req->base.buffer_handle;\n\t\t\t}\n\t\t}\n\t} else if (req->base.drm_surface_flags &\n\t\t   (drm_vmw_surface_flag_create_buffer |\n\t\t    drm_vmw_surface_flag_coherent)) {\n\t\tret = vmw_gem_object_create_with_handle(dev_priv, file_priv,\n\t\t\t\t\t\t\tres->guest_memory_size,\n\t\t\t\t\t\t\t&backup_handle,\n\t\t\t\t\t\t\t&res->guest_memory_bo);\n\t}\n\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\n\tif (req->base.drm_surface_flags & drm_vmw_surface_flag_coherent) {\n\t\tstruct vmw_bo *backup = res->guest_memory_bo;\n\n\t\tttm_bo_reserve(&backup->tbo, false, false, NULL);\n\t\tif (!res->func->dirty_alloc)\n\t\t\tret = -EINVAL;\n\t\tif (!ret)\n\t\t\tret = vmw_bo_dirty_add(backup);\n\t\tif (!ret) {\n\t\t\tres->coherent = true;\n\t\t\tret = res->func->dirty_alloc(res);\n\t\t}\n\t\tttm_bo_unreserve(&backup->tbo);\n\t\tif (ret) {\n\t\t\tvmw_resource_unreference(&res);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t}\n\n\ttmp = vmw_resource_reference(res);\n\tret = ttm_prime_object_init(tfile, res->guest_memory_size, &user_srf->prime,\n\t\t\t\t    req->base.drm_surface_flags &\n\t\t\t\t    drm_vmw_surface_flag_shareable,\n\t\t\t\t    VMW_RES_SURFACE,\n\t\t\t\t    &vmw_user_surface_base_release);\n\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&tmp);\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\n\trep->handle      = user_srf->prime.base.handle;\n\trep->backup_size = res->guest_memory_size;\n\tif (res->guest_memory_bo) {\n\t\trep->buffer_map_handle =\n\t\t\tdrm_vma_node_offset_addr(&res->guest_memory_bo->tbo.base.vma_node);\n\t\trep->buffer_size = res->guest_memory_bo->tbo.base.size;\n\t\trep->buffer_handle = backup_handle;\n\t} else {\n\t\trep->buffer_map_handle = 0;\n\t\trep->buffer_size = 0;\n\t\trep->buffer_handle = SVGA3D_INVALID_ID;\n\t}\n\tvmw_resource_unreference(&res);\n\nout_unlock:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -90,7 +90,7 @@\n \t\tif (ret == 0) {\n \t\t\tif (res->guest_memory_bo->tbo.base.size < res->guest_memory_size) {\n \t\t\t\tVMW_DEBUG_USER(\"Surface backup buffer too small.\\n\");\n-\t\t\t\tvmw_bo_unreference(&res->guest_memory_bo);\n+\t\t\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n \t\t\t\tret = -EINVAL;\n \t\t\t\tgoto out_unlock;\n \t\t\t} else {\n@@ -104,8 +104,6 @@\n \t\t\t\t\t\t\tres->guest_memory_size,\n \t\t\t\t\t\t\t&backup_handle,\n \t\t\t\t\t\t\t&res->guest_memory_bo);\n-\t\tif (ret == 0)\n-\t\t\tvmw_bo_reference(res->guest_memory_bo);\n \t}\n \n \tif (unlikely(ret != 0)) {",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tvmw_user_bo_unref(&res->guest_memory_bo);"
            ],
            "deleted": [
                "\t\t\t\tvmw_bo_unreference(&res->guest_memory_bo);",
                "\t\tif (ret == 0)",
                "\t\t\tvmw_bo_reference(res->guest_memory_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4285
    },
    {
        "cve_id": "CVE-2022-1158",
        "code_before_change": "static int FNAME(cmpxchg_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t       pt_element_t __user *ptep_user, unsigned index,\n\t\t\t       pt_element_t orig_pte, pt_element_t new_pte)\n{\n\tint npages;\n\tpt_element_t ret;\n\tpt_element_t *table;\n\tstruct page *page;\n\n\tnpages = get_user_pages_fast((unsigned long)ptep_user, 1, FOLL_WRITE, &page);\n\tif (likely(npages == 1)) {\n\t\ttable = kmap_atomic(page);\n\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n\t\tkunmap_atomic(table);\n\n\t\tkvm_release_page_dirty(page);\n\t} else {\n\t\tstruct vm_area_struct *vma;\n\t\tunsigned long vaddr = (unsigned long)ptep_user & PAGE_MASK;\n\t\tunsigned long pfn;\n\t\tunsigned long paddr;\n\n\t\tmmap_read_lock(current->mm);\n\t\tvma = find_vma_intersection(current->mm, vaddr, vaddr + PAGE_SIZE);\n\t\tif (!vma || !(vma->vm_flags & VM_PFNMAP)) {\n\t\t\tmmap_read_unlock(current->mm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tpfn = ((vaddr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\n\t\tpaddr = pfn << PAGE_SHIFT;\n\t\ttable = memremap(paddr, PAGE_SIZE, MEMREMAP_WB);\n\t\tif (!table) {\n\t\t\tmmap_read_unlock(current->mm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n\t\tmemunmap(table);\n\t\tmmap_read_unlock(current->mm);\n\t}\n\n\treturn (ret != orig_pte);\n}",
        "code_after_change": "static int FNAME(cmpxchg_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t       pt_element_t __user *ptep_user, unsigned index,\n\t\t\t       pt_element_t orig_pte, pt_element_t new_pte)\n{\n\tsigned char r;\n\n\tif (!user_access_begin(ptep_user, sizeof(pt_element_t)))\n\t\treturn -EFAULT;\n\n#ifdef CMPXCHG\n\tasm volatile(\"1:\" LOCK_PREFIX CMPXCHG \" %[new], %[ptr]\\n\"\n\t\t     \"setnz %b[r]\\n\"\n\t\t     \"2:\"\n\t\t     _ASM_EXTABLE_TYPE_REG(1b, 2b, EX_TYPE_EFAULT_REG, %k[r])\n\t\t     : [ptr] \"+m\" (*ptep_user),\n\t\t       [old] \"+a\" (orig_pte),\n\t\t       [r] \"=q\" (r)\n\t\t     : [new] \"r\" (new_pte)\n\t\t     : \"memory\");\n#else\n\tasm volatile(\"1:\" LOCK_PREFIX \"cmpxchg8b %[ptr]\\n\"\n\t\t     \"setnz %b[r]\\n\"\n\t\t     \"2:\"\n\t\t     _ASM_EXTABLE_TYPE_REG(1b, 2b, EX_TYPE_EFAULT_REG, %k[r])\n\t\t     : [ptr] \"+m\" (*ptep_user),\n\t\t       [old] \"+A\" (orig_pte),\n\t\t       [r] \"=q\" (r)\n\t\t     : [new_lo] \"b\" ((u32)new_pte),\n\t\t       [new_hi] \"c\" ((u32)(new_pte >> 32))\n\t\t     : \"memory\");\n#endif\n\n\tuser_access_end();\n\treturn r;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,41 +2,34 @@\n \t\t\t       pt_element_t __user *ptep_user, unsigned index,\n \t\t\t       pt_element_t orig_pte, pt_element_t new_pte)\n {\n-\tint npages;\n-\tpt_element_t ret;\n-\tpt_element_t *table;\n-\tstruct page *page;\n+\tsigned char r;\n \n-\tnpages = get_user_pages_fast((unsigned long)ptep_user, 1, FOLL_WRITE, &page);\n-\tif (likely(npages == 1)) {\n-\t\ttable = kmap_atomic(page);\n-\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n-\t\tkunmap_atomic(table);\n+\tif (!user_access_begin(ptep_user, sizeof(pt_element_t)))\n+\t\treturn -EFAULT;\n \n-\t\tkvm_release_page_dirty(page);\n-\t} else {\n-\t\tstruct vm_area_struct *vma;\n-\t\tunsigned long vaddr = (unsigned long)ptep_user & PAGE_MASK;\n-\t\tunsigned long pfn;\n-\t\tunsigned long paddr;\n+#ifdef CMPXCHG\n+\tasm volatile(\"1:\" LOCK_PREFIX CMPXCHG \" %[new], %[ptr]\\n\"\n+\t\t     \"setnz %b[r]\\n\"\n+\t\t     \"2:\"\n+\t\t     _ASM_EXTABLE_TYPE_REG(1b, 2b, EX_TYPE_EFAULT_REG, %k[r])\n+\t\t     : [ptr] \"+m\" (*ptep_user),\n+\t\t       [old] \"+a\" (orig_pte),\n+\t\t       [r] \"=q\" (r)\n+\t\t     : [new] \"r\" (new_pte)\n+\t\t     : \"memory\");\n+#else\n+\tasm volatile(\"1:\" LOCK_PREFIX \"cmpxchg8b %[ptr]\\n\"\n+\t\t     \"setnz %b[r]\\n\"\n+\t\t     \"2:\"\n+\t\t     _ASM_EXTABLE_TYPE_REG(1b, 2b, EX_TYPE_EFAULT_REG, %k[r])\n+\t\t     : [ptr] \"+m\" (*ptep_user),\n+\t\t       [old] \"+A\" (orig_pte),\n+\t\t       [r] \"=q\" (r)\n+\t\t     : [new_lo] \"b\" ((u32)new_pte),\n+\t\t       [new_hi] \"c\" ((u32)(new_pte >> 32))\n+\t\t     : \"memory\");\n+#endif\n \n-\t\tmmap_read_lock(current->mm);\n-\t\tvma = find_vma_intersection(current->mm, vaddr, vaddr + PAGE_SIZE);\n-\t\tif (!vma || !(vma->vm_flags & VM_PFNMAP)) {\n-\t\t\tmmap_read_unlock(current->mm);\n-\t\t\treturn -EFAULT;\n-\t\t}\n-\t\tpfn = ((vaddr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\n-\t\tpaddr = pfn << PAGE_SHIFT;\n-\t\ttable = memremap(paddr, PAGE_SIZE, MEMREMAP_WB);\n-\t\tif (!table) {\n-\t\t\tmmap_read_unlock(current->mm);\n-\t\t\treturn -EFAULT;\n-\t\t}\n-\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);\n-\t\tmemunmap(table);\n-\t\tmmap_read_unlock(current->mm);\n-\t}\n-\n-\treturn (ret != orig_pte);\n+\tuser_access_end();\n+\treturn r;\n }",
        "function_modified_lines": {
            "added": [
                "\tsigned char r;",
                "\tif (!user_access_begin(ptep_user, sizeof(pt_element_t)))",
                "\t\treturn -EFAULT;",
                "#ifdef CMPXCHG",
                "\tasm volatile(\"1:\" LOCK_PREFIX CMPXCHG \" %[new], %[ptr]\\n\"",
                "\t\t     \"setnz %b[r]\\n\"",
                "\t\t     \"2:\"",
                "\t\t     _ASM_EXTABLE_TYPE_REG(1b, 2b, EX_TYPE_EFAULT_REG, %k[r])",
                "\t\t     : [ptr] \"+m\" (*ptep_user),",
                "\t\t       [old] \"+a\" (orig_pte),",
                "\t\t       [r] \"=q\" (r)",
                "\t\t     : [new] \"r\" (new_pte)",
                "\t\t     : \"memory\");",
                "#else",
                "\tasm volatile(\"1:\" LOCK_PREFIX \"cmpxchg8b %[ptr]\\n\"",
                "\t\t     \"setnz %b[r]\\n\"",
                "\t\t     \"2:\"",
                "\t\t     _ASM_EXTABLE_TYPE_REG(1b, 2b, EX_TYPE_EFAULT_REG, %k[r])",
                "\t\t     : [ptr] \"+m\" (*ptep_user),",
                "\t\t       [old] \"+A\" (orig_pte),",
                "\t\t       [r] \"=q\" (r)",
                "\t\t     : [new_lo] \"b\" ((u32)new_pte),",
                "\t\t       [new_hi] \"c\" ((u32)(new_pte >> 32))",
                "\t\t     : \"memory\");",
                "#endif",
                "\tuser_access_end();",
                "\treturn r;"
            ],
            "deleted": [
                "\tint npages;",
                "\tpt_element_t ret;",
                "\tpt_element_t *table;",
                "\tstruct page *page;",
                "\tnpages = get_user_pages_fast((unsigned long)ptep_user, 1, FOLL_WRITE, &page);",
                "\tif (likely(npages == 1)) {",
                "\t\ttable = kmap_atomic(page);",
                "\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);",
                "\t\tkunmap_atomic(table);",
                "\t\tkvm_release_page_dirty(page);",
                "\t} else {",
                "\t\tstruct vm_area_struct *vma;",
                "\t\tunsigned long vaddr = (unsigned long)ptep_user & PAGE_MASK;",
                "\t\tunsigned long pfn;",
                "\t\tunsigned long paddr;",
                "\t\tmmap_read_lock(current->mm);",
                "\t\tvma = find_vma_intersection(current->mm, vaddr, vaddr + PAGE_SIZE);",
                "\t\tif (!vma || !(vma->vm_flags & VM_PFNMAP)) {",
                "\t\t\tmmap_read_unlock(current->mm);",
                "\t\t\treturn -EFAULT;",
                "\t\t}",
                "\t\tpfn = ((vaddr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;",
                "\t\tpaddr = pfn << PAGE_SHIFT;",
                "\t\ttable = memremap(paddr, PAGE_SIZE, MEMREMAP_WB);",
                "\t\tif (!table) {",
                "\t\t\tmmap_read_unlock(current->mm);",
                "\t\t\treturn -EFAULT;",
                "\t\t}",
                "\t\tret = CMPXCHG(&table[index], orig_pte, new_pte);",
                "\t\tmemunmap(table);",
                "\t\tmmap_read_unlock(current->mm);",
                "\t}",
                "",
                "\treturn (ret != orig_pte);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in KVM. When updating a guest's page table entry, vm_pgoff was improperly used as the offset to get the page's pfn. As vaddr and vm_pgoff are controllable by user-mode processes, this flaw allows unprivileged local users on the host to write outside the userspace region and potentially corrupt the kernel, resulting in a denial of service condition.",
        "id": 3250
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "void nfc_llcp_mac_is_up(struct nfc_dev *dev, u32 target_idx,\n\t\t\tu8 comm_mode, u8 rf_mode)\n{\n\tstruct nfc_llcp_local *local;\n\n\tpr_debug(\"rf mode %d\\n\", rf_mode);\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn;\n\n\tlocal->target_idx = target_idx;\n\tlocal->comm_mode = comm_mode;\n\tlocal->rf_mode = rf_mode;\n\n\tif (rf_mode == NFC_RF_INITIATOR) {\n\t\tpr_debug(\"Queueing Tx work\\n\");\n\n\t\tschedule_work(&local->tx_work);\n\t} else {\n\t\tmod_timer(&local->link_timer,\n\t\t\t  jiffies + msecs_to_jiffies(local->remote_lto));\n\t}\n}",
        "code_after_change": "void nfc_llcp_mac_is_up(struct nfc_dev *dev, u32 target_idx,\n\t\t\tu8 comm_mode, u8 rf_mode)\n{\n\tstruct nfc_llcp_local *local;\n\n\tpr_debug(\"rf mode %d\\n\", rf_mode);\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn;\n\n\tlocal->target_idx = target_idx;\n\tlocal->comm_mode = comm_mode;\n\tlocal->rf_mode = rf_mode;\n\n\tif (rf_mode == NFC_RF_INITIATOR) {\n\t\tpr_debug(\"Queueing Tx work\\n\");\n\n\t\tschedule_work(&local->tx_work);\n\t} else {\n\t\tmod_timer(&local->link_timer,\n\t\t\t  jiffies + msecs_to_jiffies(local->remote_lto));\n\t}\n\n\tnfc_llcp_local_put(local);\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,4 +21,6 @@\n \t\tmod_timer(&local->link_timer,\n \t\t\t  jiffies + msecs_to_jiffies(local->remote_lto));\n \t}\n+\n+\tnfc_llcp_local_put(local);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tnfc_llcp_local_put(local);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4144
    },
    {
        "cve_id": "CVE-2016-7911",
        "code_before_change": "static int get_task_ioprio(struct task_struct *p)\n{\n\tint ret;\n\n\tret = security_task_getioprio(p);\n\tif (ret)\n\t\tgoto out;\n\tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n\tif (p->io_context)\n\t\tret = p->io_context->ioprio;\nout:\n\treturn ret;\n}",
        "code_after_change": "static int get_task_ioprio(struct task_struct *p)\n{\n\tint ret;\n\n\tret = security_task_getioprio(p);\n\tif (ret)\n\t\tgoto out;\n\tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n\ttask_lock(p);\n\tif (p->io_context)\n\t\tret = p->io_context->ioprio;\n\ttask_unlock(p);\nout:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,10 @@\n \tif (ret)\n \t\tgoto out;\n \tret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);\n+\ttask_lock(p);\n \tif (p->io_context)\n \t\tret = p->io_context->ioprio;\n+\ttask_unlock(p);\n out:\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\ttask_lock(p);",
                "\ttask_unlock(p);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the get_task_ioprio function in block/ioprio.c in the Linux kernel before 4.6.6 allows local users to gain privileges or cause a denial of service (use-after-free) via a crafted ioprio_get system call.",
        "id": 1110
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "static __cold struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx;\n\tint hash_bits;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\txa_init(&ctx->io_bl_xa);\n\n\t/*\n\t * Use 5 bits less than the max cq entries, that should give us around\n\t * 32 entries per hash list if totally full and uniformly spread, but\n\t * don't keep too many buckets to not overconsume memory.\n\t */\n\thash_bits = ilog2(p->cq_entries) - 5;\n\thash_bits = clamp(hash_bits, 1, 8);\n\tif (io_alloc_hash_table(&ctx->cancel_table, hash_bits))\n\t\tgoto err;\n\n\tctx->dummy_ubuf = kzalloc(sizeof(*ctx->dummy_ubuf), GFP_KERNEL);\n\tif (!ctx->dummy_ubuf)\n\t\tgoto err;\n\t/* set invalid range, so io_import_fixed() fails meeting it */\n\tctx->dummy_ubuf->ubuf = -1UL;\n\n\tif (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free,\n\t\t\t    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->flags = p->flags;\n\tinit_waitqueue_head(&ctx->sqo_sq_wait);\n\tINIT_LIST_HEAD(&ctx->sqd_list);\n\tINIT_LIST_HEAD(&ctx->cq_overflow_list);\n\tINIT_LIST_HEAD(&ctx->io_buffers_cache);\n\tINIT_LIST_HEAD(&ctx->apoll_cache);\n\tinit_completion(&ctx->ref_comp);\n\txa_init_flags(&ctx->personalities, XA_FLAGS_ALLOC1);\n\tmutex_init(&ctx->uring_lock);\n\tinit_waitqueue_head(&ctx->cq_wait);\n\tspin_lock_init(&ctx->completion_lock);\n\tspin_lock_init(&ctx->timeout_lock);\n\tINIT_WQ_LIST(&ctx->iopoll_list);\n\tINIT_LIST_HEAD(&ctx->io_buffers_pages);\n\tINIT_LIST_HEAD(&ctx->io_buffers_comp);\n\tINIT_LIST_HEAD(&ctx->defer_list);\n\tINIT_LIST_HEAD(&ctx->timeout_list);\n\tINIT_LIST_HEAD(&ctx->ltimeout_list);\n\tspin_lock_init(&ctx->rsrc_ref_lock);\n\tINIT_LIST_HEAD(&ctx->rsrc_ref_list);\n\tINIT_DELAYED_WORK(&ctx->rsrc_put_work, io_rsrc_put_work);\n\tinit_llist_head(&ctx->rsrc_put_llist);\n\tINIT_LIST_HEAD(&ctx->tctx_list);\n\tctx->submit_state.free_list.next = NULL;\n\tINIT_WQ_LIST(&ctx->locked_free_list);\n\tINIT_DELAYED_WORK(&ctx->fallback_work, io_fallback_req_func);\n\tINIT_WQ_LIST(&ctx->submit_state.compl_reqs);\n\treturn ctx;\nerr:\n\tkfree(ctx->dummy_ubuf);\n\tkfree(ctx->cancel_table.hbs);\n\tkfree(ctx->io_bl);\n\txa_destroy(&ctx->io_bl_xa);\n\tkfree(ctx);\n\treturn NULL;\n}",
        "code_after_change": "static __cold struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx;\n\tint hash_bits;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\txa_init(&ctx->io_bl_xa);\n\n\t/*\n\t * Use 5 bits less than the max cq entries, that should give us around\n\t * 32 entries per hash list if totally full and uniformly spread, but\n\t * don't keep too many buckets to not overconsume memory.\n\t */\n\thash_bits = ilog2(p->cq_entries) - 5;\n\thash_bits = clamp(hash_bits, 1, 8);\n\tif (io_alloc_hash_table(&ctx->cancel_table, hash_bits))\n\t\tgoto err;\n\tif (io_alloc_hash_table(&ctx->cancel_table_locked, hash_bits))\n\t\tgoto err;\n\n\tctx->dummy_ubuf = kzalloc(sizeof(*ctx->dummy_ubuf), GFP_KERNEL);\n\tif (!ctx->dummy_ubuf)\n\t\tgoto err;\n\t/* set invalid range, so io_import_fixed() fails meeting it */\n\tctx->dummy_ubuf->ubuf = -1UL;\n\n\tif (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free,\n\t\t\t    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->flags = p->flags;\n\tinit_waitqueue_head(&ctx->sqo_sq_wait);\n\tINIT_LIST_HEAD(&ctx->sqd_list);\n\tINIT_LIST_HEAD(&ctx->cq_overflow_list);\n\tINIT_LIST_HEAD(&ctx->io_buffers_cache);\n\tINIT_LIST_HEAD(&ctx->apoll_cache);\n\tinit_completion(&ctx->ref_comp);\n\txa_init_flags(&ctx->personalities, XA_FLAGS_ALLOC1);\n\tmutex_init(&ctx->uring_lock);\n\tinit_waitqueue_head(&ctx->cq_wait);\n\tspin_lock_init(&ctx->completion_lock);\n\tspin_lock_init(&ctx->timeout_lock);\n\tINIT_WQ_LIST(&ctx->iopoll_list);\n\tINIT_LIST_HEAD(&ctx->io_buffers_pages);\n\tINIT_LIST_HEAD(&ctx->io_buffers_comp);\n\tINIT_LIST_HEAD(&ctx->defer_list);\n\tINIT_LIST_HEAD(&ctx->timeout_list);\n\tINIT_LIST_HEAD(&ctx->ltimeout_list);\n\tspin_lock_init(&ctx->rsrc_ref_lock);\n\tINIT_LIST_HEAD(&ctx->rsrc_ref_list);\n\tINIT_DELAYED_WORK(&ctx->rsrc_put_work, io_rsrc_put_work);\n\tinit_llist_head(&ctx->rsrc_put_llist);\n\tINIT_LIST_HEAD(&ctx->tctx_list);\n\tctx->submit_state.free_list.next = NULL;\n\tINIT_WQ_LIST(&ctx->locked_free_list);\n\tINIT_DELAYED_WORK(&ctx->fallback_work, io_fallback_req_func);\n\tINIT_WQ_LIST(&ctx->submit_state.compl_reqs);\n\treturn ctx;\nerr:\n\tkfree(ctx->dummy_ubuf);\n\tkfree(ctx->cancel_table.hbs);\n\tkfree(ctx->cancel_table_locked.hbs);\n\tkfree(ctx->io_bl);\n\txa_destroy(&ctx->io_bl_xa);\n\tkfree(ctx);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,8 @@\n \thash_bits = ilog2(p->cq_entries) - 5;\n \thash_bits = clamp(hash_bits, 1, 8);\n \tif (io_alloc_hash_table(&ctx->cancel_table, hash_bits))\n+\t\tgoto err;\n+\tif (io_alloc_hash_table(&ctx->cancel_table_locked, hash_bits))\n \t\tgoto err;\n \n \tctx->dummy_ubuf = kzalloc(sizeof(*ctx->dummy_ubuf), GFP_KERNEL);\n@@ -60,6 +62,7 @@\n err:\n \tkfree(ctx->dummy_ubuf);\n \tkfree(ctx->cancel_table.hbs);\n+\tkfree(ctx->cancel_table_locked.hbs);\n \tkfree(ctx->io_bl);\n \txa_destroy(&ctx->io_bl_xa);\n \tkfree(ctx);",
        "function_modified_lines": {
            "added": [
                "\t\tgoto err;",
                "\tif (io_alloc_hash_table(&ctx->cancel_table_locked, hash_bits))",
                "\tkfree(ctx->cancel_table_locked.hbs);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and 0e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4067
    },
    {
        "cve_id": "CVE-2023-0240",
        "code_before_change": "static int io_remove_personalities(int id, void *p, void *data)\n{\n\tstruct io_ring_ctx *ctx = data;\n\tconst struct cred *cred;\n\n\tcred = idr_remove(&ctx->personality_idr, id);\n\tif (cred)\n\t\tput_cred(cred);\n\treturn 0;\n}",
        "code_after_change": "static int io_remove_personalities(int id, void *p, void *data)\n{\n\tstruct io_ring_ctx *ctx = data;\n\tstruct io_identity *iod;\n\n\tiod = idr_remove(&ctx->personality_idr, id);\n\tif (iod) {\n\t\tput_cred(iod->creds);\n\t\tif (refcount_dec_and_test(&iod->count))\n\t\t\tkfree(iod);\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,13 @@\n static int io_remove_personalities(int id, void *p, void *data)\n {\n \tstruct io_ring_ctx *ctx = data;\n-\tconst struct cred *cred;\n+\tstruct io_identity *iod;\n \n-\tcred = idr_remove(&ctx->personality_idr, id);\n-\tif (cred)\n-\t\tput_cred(cred);\n+\tiod = idr_remove(&ctx->personality_idr, id);\n+\tif (iod) {\n+\t\tput_cred(iod->creds);\n+\t\tif (refcount_dec_and_test(&iod->count))\n+\t\t\tkfree(iod);\n+\t}\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct io_identity *iod;",
                "\tiod = idr_remove(&ctx->personality_idr, id);",
                "\tif (iod) {",
                "\t\tput_cred(iod->creds);",
                "\t\tif (refcount_dec_and_test(&iod->count))",
                "\t\t\tkfree(iod);",
                "\t}"
            ],
            "deleted": [
                "\tconst struct cred *cred;",
                "\tcred = idr_remove(&ctx->personality_idr, id);",
                "\tif (cred)",
                "\t\tput_cred(cred);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "id": 3815
    },
    {
        "cve_id": "CVE-2022-47946",
        "code_before_change": "static int io_uring_create(unsigned entries, struct io_uring_params *p,\n\t\t\t   struct io_uring_params __user *params)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct file *file;\n\tint ret;\n\n\tif (!entries)\n\t\treturn -EINVAL;\n\tif (entries > IORING_MAX_ENTRIES) {\n\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\treturn -EINVAL;\n\t\tentries = IORING_MAX_ENTRIES;\n\t}\n\n\t/*\n\t * Use twice as many entries for the CQ ring. It's possible for the\n\t * application to drive a higher depth than the size of the SQ ring,\n\t * since the sqes are only used at submission time. This allows for\n\t * some flexibility in overcommitting a bit. If the application has\n\t * set IORING_SETUP_CQSIZE, it will have passed in the desired number\n\t * of CQ ring entries manually.\n\t */\n\tp->sq_entries = roundup_pow_of_two(entries);\n\tif (p->flags & IORING_SETUP_CQSIZE) {\n\t\t/*\n\t\t * If IORING_SETUP_CQSIZE is set, we do the same roundup\n\t\t * to a power-of-two, if it isn't already. We do NOT impose\n\t\t * any cq vs sq ring sizing.\n\t\t */\n\t\tif (!p->cq_entries)\n\t\t\treturn -EINVAL;\n\t\tif (p->cq_entries > IORING_MAX_CQ_ENTRIES) {\n\t\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\t\treturn -EINVAL;\n\t\t\tp->cq_entries = IORING_MAX_CQ_ENTRIES;\n\t\t}\n\t\tp->cq_entries = roundup_pow_of_two(p->cq_entries);\n\t\tif (p->cq_entries < p->sq_entries)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tp->cq_entries = 2 * p->sq_entries;\n\t}\n\n\tctx = io_ring_ctx_alloc(p);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tctx->compat = in_compat_syscall();\n\tif (!capable(CAP_IPC_LOCK))\n\t\tctx->user = get_uid(current_user());\n\tctx->sqo_task = current;\n\n\t/*\n\t * This is just grabbed for accounting purposes. When a process exits,\n\t * the mm is exited and dropped before the files, hence we need to hang\n\t * on to this mm purely for the purposes of being able to unaccount\n\t * memory (locked/pinned vm). It's not used for anything else.\n\t */\n\tmmgrab(current->mm);\n\tctx->mm_account = current->mm;\n\n\tret = io_allocate_scq_urings(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tret = io_sq_offload_create(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tif (!(p->flags & IORING_SETUP_R_DISABLED))\n\t\tio_sq_offload_start(ctx);\n\n\tmemset(&p->sq_off, 0, sizeof(p->sq_off));\n\tp->sq_off.head = offsetof(struct io_rings, sq.head);\n\tp->sq_off.tail = offsetof(struct io_rings, sq.tail);\n\tp->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);\n\tp->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);\n\tp->sq_off.flags = offsetof(struct io_rings, sq_flags);\n\tp->sq_off.dropped = offsetof(struct io_rings, sq_dropped);\n\tp->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;\n\n\tmemset(&p->cq_off, 0, sizeof(p->cq_off));\n\tp->cq_off.head = offsetof(struct io_rings, cq.head);\n\tp->cq_off.tail = offsetof(struct io_rings, cq.tail);\n\tp->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);\n\tp->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);\n\tp->cq_off.overflow = offsetof(struct io_rings, cq_overflow);\n\tp->cq_off.cqes = offsetof(struct io_rings, cqes);\n\tp->cq_off.flags = offsetof(struct io_rings, cq_flags);\n\n\tp->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |\n\t\t\tIORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |\n\t\t\tIORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |\n\t\t\tIORING_FEAT_POLL_32BITS | IORING_FEAT_SQPOLL_NONFIXED |\n\t\t\tIORING_FEAT_EXT_ARG | IORING_FEAT_NATIVE_WORKERS;\n\n\tif (copy_to_user(params, p, sizeof(*p))) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\tfile = io_uring_get_file(ctx);\n\tif (IS_ERR(file)) {\n\t\tret = PTR_ERR(file);\n\t\tgoto err;\n\t}\n\n\t/*\n\t * Install ring fd as the very last thing, so we don't risk someone\n\t * having closed it before we finish setup\n\t */\n\tret = io_uring_install_fd(ctx, file);\n\tif (ret < 0) {\n\t\tio_disable_sqo_submit(ctx);\n\t\t/* fput will clean it up */\n\t\tfput(file);\n\t\treturn ret;\n\t}\n\n\ttrace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);\n\treturn ret;\nerr:\n\tio_disable_sqo_submit(ctx);\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn ret;\n}",
        "code_after_change": "static int io_uring_create(unsigned entries, struct io_uring_params *p,\n\t\t\t   struct io_uring_params __user *params)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct file *file;\n\tint ret;\n\n\tif (!entries)\n\t\treturn -EINVAL;\n\tif (entries > IORING_MAX_ENTRIES) {\n\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\treturn -EINVAL;\n\t\tentries = IORING_MAX_ENTRIES;\n\t}\n\n\t/*\n\t * Use twice as many entries for the CQ ring. It's possible for the\n\t * application to drive a higher depth than the size of the SQ ring,\n\t * since the sqes are only used at submission time. This allows for\n\t * some flexibility in overcommitting a bit. If the application has\n\t * set IORING_SETUP_CQSIZE, it will have passed in the desired number\n\t * of CQ ring entries manually.\n\t */\n\tp->sq_entries = roundup_pow_of_two(entries);\n\tif (p->flags & IORING_SETUP_CQSIZE) {\n\t\t/*\n\t\t * If IORING_SETUP_CQSIZE is set, we do the same roundup\n\t\t * to a power-of-two, if it isn't already. We do NOT impose\n\t\t * any cq vs sq ring sizing.\n\t\t */\n\t\tif (!p->cq_entries)\n\t\t\treturn -EINVAL;\n\t\tif (p->cq_entries > IORING_MAX_CQ_ENTRIES) {\n\t\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\t\treturn -EINVAL;\n\t\t\tp->cq_entries = IORING_MAX_CQ_ENTRIES;\n\t\t}\n\t\tp->cq_entries = roundup_pow_of_two(p->cq_entries);\n\t\tif (p->cq_entries < p->sq_entries)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tp->cq_entries = 2 * p->sq_entries;\n\t}\n\n\tctx = io_ring_ctx_alloc(p);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tctx->compat = in_compat_syscall();\n\tif (!capable(CAP_IPC_LOCK))\n\t\tctx->user = get_uid(current_user());\n\tctx->sqo_task = current;\n\n\t/*\n\t * This is just grabbed for accounting purposes. When a process exits,\n\t * the mm is exited and dropped before the files, hence we need to hang\n\t * on to this mm purely for the purposes of being able to unaccount\n\t * memory (locked/pinned vm). It's not used for anything else.\n\t */\n\tmmgrab(current->mm);\n\tctx->mm_account = current->mm;\n\n\tret = io_allocate_scq_urings(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tret = io_sq_offload_create(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tif (!(p->flags & IORING_SETUP_R_DISABLED))\n\t\tio_sq_offload_start(ctx);\n\n\tmemset(&p->sq_off, 0, sizeof(p->sq_off));\n\tp->sq_off.head = offsetof(struct io_rings, sq.head);\n\tp->sq_off.tail = offsetof(struct io_rings, sq.tail);\n\tp->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);\n\tp->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);\n\tp->sq_off.flags = offsetof(struct io_rings, sq_flags);\n\tp->sq_off.dropped = offsetof(struct io_rings, sq_dropped);\n\tp->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;\n\n\tmemset(&p->cq_off, 0, sizeof(p->cq_off));\n\tp->cq_off.head = offsetof(struct io_rings, cq.head);\n\tp->cq_off.tail = offsetof(struct io_rings, cq.tail);\n\tp->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);\n\tp->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);\n\tp->cq_off.overflow = offsetof(struct io_rings, cq_overflow);\n\tp->cq_off.cqes = offsetof(struct io_rings, cqes);\n\tp->cq_off.flags = offsetof(struct io_rings, cq_flags);\n\n\tp->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |\n\t\t\tIORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |\n\t\t\tIORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |\n\t\t\tIORING_FEAT_POLL_32BITS | IORING_FEAT_SQPOLL_NONFIXED |\n\t\t\tIORING_FEAT_EXT_ARG | IORING_FEAT_NATIVE_WORKERS;\n\n\tif (copy_to_user(params, p, sizeof(*p))) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\tfile = io_uring_get_file(ctx);\n\tif (IS_ERR(file)) {\n\t\tret = PTR_ERR(file);\n\t\tgoto err;\n\t}\n\n\t/*\n\t * Install ring fd as the very last thing, so we don't risk someone\n\t * having closed it before we finish setup\n\t */\n\tret = io_uring_install_fd(ctx, file);\n\tif (ret < 0) {\n\t\t/* fput will clean it up */\n\t\tfput(file);\n\t\treturn ret;\n\t}\n\n\ttrace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);\n\treturn ret;\nerr:\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -111,7 +111,6 @@\n \t */\n \tret = io_uring_install_fd(ctx, file);\n \tif (ret < 0) {\n-\t\tio_disable_sqo_submit(ctx);\n \t\t/* fput will clean it up */\n \t\tfput(file);\n \t\treturn ret;\n@@ -120,7 +119,6 @@\n \ttrace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);\n \treturn ret;\n err:\n-\tio_disable_sqo_submit(ctx);\n \tio_ring_ctx_wait_and_kill(ctx);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tio_disable_sqo_submit(ctx);",
                "\tio_disable_sqo_submit(ctx);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 5.10.x before 5.10.155. A use-after-free in io_sqpoll_wait_sq in fs/io_uring.c allows an attacker to crash the kernel, resulting in denial of service. finish_wait can be skipped. An attack can occur in some situations by forking a process and then quickly terminating it. NOTE: later kernel versions, such as the 5.15 longterm series, substantially changed the implementation of io_sqpoll_wait_sq.",
        "id": 3782
    },
    {
        "cve_id": "CVE-2023-4387",
        "code_before_change": "static int\nvmxnet3_rq_alloc_rx_buf(struct vmxnet3_rx_queue *rq, u32 ring_idx,\n\t\t\tint num_to_alloc, struct vmxnet3_adapter *adapter)\n{\n\tint num_allocated = 0;\n\tstruct vmxnet3_rx_buf_info *rbi_base = rq->buf_info[ring_idx];\n\tstruct vmxnet3_cmd_ring *ring = &rq->rx_ring[ring_idx];\n\tu32 val;\n\n\twhile (num_allocated <= num_to_alloc) {\n\t\tstruct vmxnet3_rx_buf_info *rbi;\n\t\tunion Vmxnet3_GenericDesc *gd;\n\n\t\trbi = rbi_base + ring->next2fill;\n\t\tgd = ring->base + ring->next2fill;\n\n\t\tif (rbi->buf_type == VMXNET3_RX_BUF_SKB) {\n\t\t\tif (rbi->skb == NULL) {\n\t\t\t\trbi->skb = __netdev_alloc_skb_ip_align(adapter->netdev,\n\t\t\t\t\t\t\t\t       rbi->len,\n\t\t\t\t\t\t\t\t       GFP_KERNEL);\n\t\t\t\tif (unlikely(rbi->skb == NULL)) {\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\trbi->dma_addr = dma_map_single(\n\t\t\t\t\t\t&adapter->pdev->dev,\n\t\t\t\t\t\trbi->skb->data, rbi->len,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      rbi->dma_addr)) {\n\t\t\t\t\tdev_kfree_skb_any(rbi->skb);\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* rx buffer skipped by the device */\n\t\t\t}\n\t\t\tval = VMXNET3_RXD_BTYPE_HEAD << VMXNET3_RXD_BTYPE_SHIFT;\n\t\t} else {\n\t\t\tBUG_ON(rbi->buf_type != VMXNET3_RX_BUF_PAGE ||\n\t\t\t       rbi->len  != PAGE_SIZE);\n\n\t\t\tif (rbi->page == NULL) {\n\t\t\t\trbi->page = alloc_page(GFP_ATOMIC);\n\t\t\t\tif (unlikely(rbi->page == NULL)) {\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\trbi->dma_addr = dma_map_page(\n\t\t\t\t\t\t&adapter->pdev->dev,\n\t\t\t\t\t\trbi->page, 0, PAGE_SIZE,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      rbi->dma_addr)) {\n\t\t\t\t\tput_page(rbi->page);\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* rx buffers skipped by the device */\n\t\t\t}\n\t\t\tval = VMXNET3_RXD_BTYPE_BODY << VMXNET3_RXD_BTYPE_SHIFT;\n\t\t}\n\n\t\tgd->rxd.addr = cpu_to_le64(rbi->dma_addr);\n\t\tgd->dword[2] = cpu_to_le32((!ring->gen << VMXNET3_RXD_GEN_SHIFT)\n\t\t\t\t\t   | val | rbi->len);\n\n\t\t/* Fill the last buffer but dont mark it ready, or else the\n\t\t * device will think that the queue is full */\n\t\tif (num_allocated == num_to_alloc)\n\t\t\tbreak;\n\n\t\tgd->dword[2] |= cpu_to_le32(ring->gen << VMXNET3_RXD_GEN_SHIFT);\n\t\tnum_allocated++;\n\t\tvmxnet3_cmd_ring_adv_next2fill(ring);\n\t}\n\n\tnetdev_dbg(adapter->netdev,\n\t\t\"alloc_rx_buf: %d allocated, next2fill %u, next2comp %u\\n\",\n\t\tnum_allocated, ring->next2fill, ring->next2comp);\n\n\t/* so that the device can distinguish a full ring and an empty ring */\n\tBUG_ON(num_allocated != 0 && ring->next2fill == ring->next2comp);\n\n\treturn num_allocated;\n}",
        "code_after_change": "static int\nvmxnet3_rq_alloc_rx_buf(struct vmxnet3_rx_queue *rq, u32 ring_idx,\n\t\t\tint num_to_alloc, struct vmxnet3_adapter *adapter)\n{\n\tint num_allocated = 0;\n\tstruct vmxnet3_rx_buf_info *rbi_base = rq->buf_info[ring_idx];\n\tstruct vmxnet3_cmd_ring *ring = &rq->rx_ring[ring_idx];\n\tu32 val;\n\n\twhile (num_allocated <= num_to_alloc) {\n\t\tstruct vmxnet3_rx_buf_info *rbi;\n\t\tunion Vmxnet3_GenericDesc *gd;\n\n\t\trbi = rbi_base + ring->next2fill;\n\t\tgd = ring->base + ring->next2fill;\n\n\t\tif (rbi->buf_type == VMXNET3_RX_BUF_SKB) {\n\t\t\tif (rbi->skb == NULL) {\n\t\t\t\trbi->skb = __netdev_alloc_skb_ip_align(adapter->netdev,\n\t\t\t\t\t\t\t\t       rbi->len,\n\t\t\t\t\t\t\t\t       GFP_KERNEL);\n\t\t\t\tif (unlikely(rbi->skb == NULL)) {\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\trbi->dma_addr = dma_map_single(\n\t\t\t\t\t\t&adapter->pdev->dev,\n\t\t\t\t\t\trbi->skb->data, rbi->len,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      rbi->dma_addr)) {\n\t\t\t\t\tdev_kfree_skb_any(rbi->skb);\n\t\t\t\t\trbi->skb = NULL;\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* rx buffer skipped by the device */\n\t\t\t}\n\t\t\tval = VMXNET3_RXD_BTYPE_HEAD << VMXNET3_RXD_BTYPE_SHIFT;\n\t\t} else {\n\t\t\tBUG_ON(rbi->buf_type != VMXNET3_RX_BUF_PAGE ||\n\t\t\t       rbi->len  != PAGE_SIZE);\n\n\t\t\tif (rbi->page == NULL) {\n\t\t\t\trbi->page = alloc_page(GFP_ATOMIC);\n\t\t\t\tif (unlikely(rbi->page == NULL)) {\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\trbi->dma_addr = dma_map_page(\n\t\t\t\t\t\t&adapter->pdev->dev,\n\t\t\t\t\t\trbi->page, 0, PAGE_SIZE,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      rbi->dma_addr)) {\n\t\t\t\t\tput_page(rbi->page);\n\t\t\t\t\trbi->page = NULL;\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* rx buffers skipped by the device */\n\t\t\t}\n\t\t\tval = VMXNET3_RXD_BTYPE_BODY << VMXNET3_RXD_BTYPE_SHIFT;\n\t\t}\n\n\t\tgd->rxd.addr = cpu_to_le64(rbi->dma_addr);\n\t\tgd->dword[2] = cpu_to_le32((!ring->gen << VMXNET3_RXD_GEN_SHIFT)\n\t\t\t\t\t   | val | rbi->len);\n\n\t\t/* Fill the last buffer but dont mark it ready, or else the\n\t\t * device will think that the queue is full */\n\t\tif (num_allocated == num_to_alloc)\n\t\t\tbreak;\n\n\t\tgd->dword[2] |= cpu_to_le32(ring->gen << VMXNET3_RXD_GEN_SHIFT);\n\t\tnum_allocated++;\n\t\tvmxnet3_cmd_ring_adv_next2fill(ring);\n\t}\n\n\tnetdev_dbg(adapter->netdev,\n\t\t\"alloc_rx_buf: %d allocated, next2fill %u, next2comp %u\\n\",\n\t\tnum_allocated, ring->next2fill, ring->next2comp);\n\n\t/* so that the device can distinguish a full ring and an empty ring */\n\tBUG_ON(num_allocated != 0 && ring->next2fill == ring->next2comp);\n\n\treturn num_allocated;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,6 +31,7 @@\n \t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n \t\t\t\t\t\t      rbi->dma_addr)) {\n \t\t\t\t\tdev_kfree_skb_any(rbi->skb);\n+\t\t\t\t\trbi->skb = NULL;\n \t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n \t\t\t\t\tbreak;\n \t\t\t\t}\n@@ -55,6 +56,7 @@\n \t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n \t\t\t\t\t\t      rbi->dma_addr)) {\n \t\t\t\t\tput_page(rbi->page);\n+\t\t\t\t\trbi->page = NULL;\n \t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n \t\t\t\t\tbreak;\n \t\t\t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\trbi->skb = NULL;",
                "\t\t\t\t\trbi->page = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in vmxnet3_rq_alloc_rx_buf in drivers/net/vmxnet3/vmxnet3_drv.c in VMware's vmxnet3 ethernet NIC driver in the Linux Kernel. This issue could allow a local attacker to crash the system due to a double-free while cleaning up vmxnet3_rq_cleanup_all, which could also lead to a kernel information leak problem.",
        "id": 4210
    },
    {
        "cve_id": "CVE-2018-14625",
        "code_before_change": "static int vhost_vsock_dev_release(struct inode *inode, struct file *file)\n{\n\tstruct vhost_vsock *vsock = file->private_data;\n\n\tspin_lock_bh(&vhost_vsock_lock);\n\tlist_del(&vsock->list);\n\tspin_unlock_bh(&vhost_vsock_lock);\n\n\t/* Iterating over all connections for all CIDs to find orphans is\n\t * inefficient.  Room for improvement here. */\n\tvsock_for_each_connected_socket(vhost_vsock_reset_orphans);\n\n\tvhost_vsock_stop(vsock);\n\tvhost_vsock_flush(vsock);\n\tvhost_dev_stop(&vsock->dev);\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\twhile (!list_empty(&vsock->send_pkt_list)) {\n\t\tstruct virtio_vsock_pkt *pkt;\n\n\t\tpkt = list_first_entry(&vsock->send_pkt_list,\n\t\t\t\tstruct virtio_vsock_pkt, list);\n\t\tlist_del_init(&pkt->list);\n\t\tvirtio_transport_free_pkt(pkt);\n\t}\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tvhost_dev_cleanup(&vsock->dev);\n\tkfree(vsock->dev.vqs);\n\tvhost_vsock_free(vsock);\n\treturn 0;\n}",
        "code_after_change": "static int vhost_vsock_dev_release(struct inode *inode, struct file *file)\n{\n\tstruct vhost_vsock *vsock = file->private_data;\n\n\tspin_lock_bh(&vhost_vsock_lock);\n\tif (vsock->guest_cid)\n\t\thash_del_rcu(&vsock->hash);\n\tspin_unlock_bh(&vhost_vsock_lock);\n\n\t/* Wait for other CPUs to finish using vsock */\n\tsynchronize_rcu();\n\n\t/* Iterating over all connections for all CIDs to find orphans is\n\t * inefficient.  Room for improvement here. */\n\tvsock_for_each_connected_socket(vhost_vsock_reset_orphans);\n\n\tvhost_vsock_stop(vsock);\n\tvhost_vsock_flush(vsock);\n\tvhost_dev_stop(&vsock->dev);\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\twhile (!list_empty(&vsock->send_pkt_list)) {\n\t\tstruct virtio_vsock_pkt *pkt;\n\n\t\tpkt = list_first_entry(&vsock->send_pkt_list,\n\t\t\t\tstruct virtio_vsock_pkt, list);\n\t\tlist_del_init(&pkt->list);\n\t\tvirtio_transport_free_pkt(pkt);\n\t}\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tvhost_dev_cleanup(&vsock->dev);\n\tkfree(vsock->dev.vqs);\n\tvhost_vsock_free(vsock);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,8 +3,12 @@\n \tstruct vhost_vsock *vsock = file->private_data;\n \n \tspin_lock_bh(&vhost_vsock_lock);\n-\tlist_del(&vsock->list);\n+\tif (vsock->guest_cid)\n+\t\thash_del_rcu(&vsock->hash);\n \tspin_unlock_bh(&vhost_vsock_lock);\n+\n+\t/* Wait for other CPUs to finish using vsock */\n+\tsynchronize_rcu();\n \n \t/* Iterating over all connections for all CIDs to find orphans is\n \t * inefficient.  Room for improvement here. */",
        "function_modified_lines": {
            "added": [
                "\tif (vsock->guest_cid)",
                "\t\thash_del_rcu(&vsock->hash);",
                "",
                "\t/* Wait for other CPUs to finish using vsock */",
                "\tsynchronize_rcu();"
            ],
            "deleted": [
                "\tlist_del(&vsock->list);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux Kernel where an attacker may be able to have an uncontrolled read to kernel-memory from within a vm guest. A race condition between connect() and close() function may allow an attacker using the AF_VSOCK protocol to gather a 4 byte information leak or possibly intercept or corrupt AF_VSOCK messages destined to other clients.",
        "id": 1697
    },
    {
        "cve_id": "CVE-2023-0461",
        "code_before_change": "static int __tcp_set_ulp(struct sock *sk, const struct tcp_ulp_ops *ulp_ops)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint err;\n\n\terr = -EEXIST;\n\tif (icsk->icsk_ulp_ops)\n\t\tgoto out_err;\n\n\tif (sk->sk_socket)\n\t\tclear_bit(SOCK_SUPPORT_ZC, &sk->sk_socket->flags);\n\n\terr = ulp_ops->init(sk);\n\tif (err)\n\t\tgoto out_err;\n\n\ticsk->icsk_ulp_ops = ulp_ops;\n\treturn 0;\nout_err:\n\tmodule_put(ulp_ops->owner);\n\treturn err;\n}",
        "code_after_change": "static int __tcp_set_ulp(struct sock *sk, const struct tcp_ulp_ops *ulp_ops)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint err;\n\n\terr = -EEXIST;\n\tif (icsk->icsk_ulp_ops)\n\t\tgoto out_err;\n\n\tif (sk->sk_socket)\n\t\tclear_bit(SOCK_SUPPORT_ZC, &sk->sk_socket->flags);\n\n\terr = -EINVAL;\n\tif (!ulp_ops->clone && sk->sk_state == TCP_LISTEN)\n\t\tgoto out_err;\n\n\terr = ulp_ops->init(sk);\n\tif (err)\n\t\tgoto out_err;\n\n\ticsk->icsk_ulp_ops = ulp_ops;\n\treturn 0;\nout_err:\n\tmodule_put(ulp_ops->owner);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,10 @@\n \tif (sk->sk_socket)\n \t\tclear_bit(SOCK_SUPPORT_ZC, &sk->sk_socket->flags);\n \n+\terr = -EINVAL;\n+\tif (!ulp_ops->clone && sk->sk_state == TCP_LISTEN)\n+\t\tgoto out_err;\n+\n \terr = ulp_ops->init(sk);\n \tif (err)\n \t\tgoto out_err;",
        "function_modified_lines": {
            "added": [
                "\terr = -EINVAL;",
                "\tif (!ulp_ops->clone && sk->sk_state == TCP_LISTEN)",
                "\t\tgoto out_err;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a use-after-free vulnerability in the Linux Kernel which can be exploited to achieve local privilege escalation. To reach the vulnerability kernel configuration flag CONFIG_TLS or CONFIG_XFRM_ESPINTCP has to be configured, but the operation does not require any privilege.\n\nThere is a use-after-free bug of icsk_ulp_data of a struct inet_connection_sock.\n\nWhen CONFIG_TLS is enabled, user can install a tls context (struct tls_context) on a connected tcp socket. The context is not cleared if this socket is disconnected and reused as a listener. If a new socket is created from the listener, the context is inherited and vulnerable.\n\nThe setsockopt TCP_ULP operation does not require any privilege.\n\nWe recommend upgrading past commit 2c02d41d71f90a5168391b6a5f2954112ba2307c",
        "id": 3829
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_shader_define(struct drm_device *dev, struct drm_file *file_priv,\n\t\t\t     enum drm_vmw_shader_type shader_type_drm,\n\t\t\t     u32 buffer_handle, size_t size, size_t offset,\n\t\t\t     uint8_t num_input_sig, uint8_t num_output_sig,\n\t\t\t     uint32_t *shader_handle)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_bo *buffer = NULL;\n\tSVGA3dShaderType shader_type;\n\tint ret;\n\n\tif (buffer_handle != SVGA3D_INVALID_ID) {\n\t\tret = vmw_user_bo_lookup(file_priv, buffer_handle, &buffer);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tVMW_DEBUG_USER(\"Couldn't find buffer for shader creation.\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tif ((u64)buffer->tbo.base.size < (u64)size + (u64)offset) {\n\t\t\tVMW_DEBUG_USER(\"Illegal buffer- or shader size.\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_bad_arg;\n\t\t}\n\t}\n\n\tswitch (shader_type_drm) {\n\tcase drm_vmw_shader_type_vs:\n\t\tshader_type = SVGA3D_SHADERTYPE_VS;\n\t\tbreak;\n\tcase drm_vmw_shader_type_ps:\n\t\tshader_type = SVGA3D_SHADERTYPE_PS;\n\t\tbreak;\n\tdefault:\n\t\tVMW_DEBUG_USER(\"Illegal shader type.\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out_bad_arg;\n\t}\n\n\tret = vmw_user_shader_alloc(dev_priv, buffer, size, offset,\n\t\t\t\t    shader_type, num_input_sig,\n\t\t\t\t    num_output_sig, tfile, shader_handle);\nout_bad_arg:\n\tvmw_user_bo_unref(buffer);\n\treturn ret;\n}",
        "code_after_change": "static int vmw_shader_define(struct drm_device *dev, struct drm_file *file_priv,\n\t\t\t     enum drm_vmw_shader_type shader_type_drm,\n\t\t\t     u32 buffer_handle, size_t size, size_t offset,\n\t\t\t     uint8_t num_input_sig, uint8_t num_output_sig,\n\t\t\t     uint32_t *shader_handle)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_bo *buffer = NULL;\n\tSVGA3dShaderType shader_type;\n\tint ret;\n\n\tif (buffer_handle != SVGA3D_INVALID_ID) {\n\t\tret = vmw_user_bo_lookup(file_priv, buffer_handle, &buffer);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tVMW_DEBUG_USER(\"Couldn't find buffer for shader creation.\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tif ((u64)buffer->tbo.base.size < (u64)size + (u64)offset) {\n\t\t\tVMW_DEBUG_USER(\"Illegal buffer- or shader size.\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_bad_arg;\n\t\t}\n\t}\n\n\tswitch (shader_type_drm) {\n\tcase drm_vmw_shader_type_vs:\n\t\tshader_type = SVGA3D_SHADERTYPE_VS;\n\t\tbreak;\n\tcase drm_vmw_shader_type_ps:\n\t\tshader_type = SVGA3D_SHADERTYPE_PS;\n\t\tbreak;\n\tdefault:\n\t\tVMW_DEBUG_USER(\"Illegal shader type.\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out_bad_arg;\n\t}\n\n\tret = vmw_user_shader_alloc(dev_priv, buffer, size, offset,\n\t\t\t\t    shader_type, num_input_sig,\n\t\t\t\t    num_output_sig, tfile, shader_handle);\nout_bad_arg:\n\tvmw_user_bo_unref(&buffer);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -41,6 +41,6 @@\n \t\t\t\t    shader_type, num_input_sig,\n \t\t\t\t    num_output_sig, tfile, shader_handle);\n out_bad_arg:\n-\tvmw_user_bo_unref(buffer);\n+\tvmw_user_bo_unref(&buffer);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tvmw_user_bo_unref(&buffer);"
            ],
            "deleted": [
                "\tvmw_user_bo_unref(buffer);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4283
    },
    {
        "cve_id": "CVE-2022-2938",
        "code_before_change": "static ssize_t cgroup_pressure_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t\t  size_t nbytes, enum psi_res res)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\tstruct psi_trigger *new;\n\tstruct cgroup *cgrp;\n\tstruct psi_group *psi;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\tcgroup_get(cgrp);\n\tcgroup_kn_unlock(of->kn);\n\n\tpsi = cgroup_ino(cgrp) == 1 ? &psi_system : &cgrp->psi;\n\tnew = psi_trigger_create(psi, buf, nbytes, res);\n\tif (IS_ERR(new)) {\n\t\tcgroup_put(cgrp);\n\t\treturn PTR_ERR(new);\n\t}\n\n\tpsi_trigger_replace(&ctx->psi.trigger, new);\n\n\tcgroup_put(cgrp);\n\n\treturn nbytes;\n}",
        "code_after_change": "static ssize_t cgroup_pressure_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t\t  size_t nbytes, enum psi_res res)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\tstruct psi_trigger *new;\n\tstruct cgroup *cgrp;\n\tstruct psi_group *psi;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\tcgroup_get(cgrp);\n\tcgroup_kn_unlock(of->kn);\n\n\t/* Allow only one trigger per file descriptor */\n\tif (ctx->psi.trigger) {\n\t\tcgroup_put(cgrp);\n\t\treturn -EBUSY;\n\t}\n\n\tpsi = cgroup_ino(cgrp) == 1 ? &psi_system : &cgrp->psi;\n\tnew = psi_trigger_create(psi, buf, nbytes, res);\n\tif (IS_ERR(new)) {\n\t\tcgroup_put(cgrp);\n\t\treturn PTR_ERR(new);\n\t}\n\n\tsmp_store_release(&ctx->psi.trigger, new);\n\tcgroup_put(cgrp);\n\n\treturn nbytes;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,12 @@\n \tcgroup_get(cgrp);\n \tcgroup_kn_unlock(of->kn);\n \n+\t/* Allow only one trigger per file descriptor */\n+\tif (ctx->psi.trigger) {\n+\t\tcgroup_put(cgrp);\n+\t\treturn -EBUSY;\n+\t}\n+\n \tpsi = cgroup_ino(cgrp) == 1 ? &psi_system : &cgrp->psi;\n \tnew = psi_trigger_create(psi, buf, nbytes, res);\n \tif (IS_ERR(new)) {\n@@ -20,8 +26,7 @@\n \t\treturn PTR_ERR(new);\n \t}\n \n-\tpsi_trigger_replace(&ctx->psi.trigger, new);\n-\n+\tsmp_store_release(&ctx->psi.trigger, new);\n \tcgroup_put(cgrp);\n \n \treturn nbytes;",
        "function_modified_lines": {
            "added": [
                "\t/* Allow only one trigger per file descriptor */",
                "\tif (ctx->psi.trigger) {",
                "\t\tcgroup_put(cgrp);",
                "\t\treturn -EBUSY;",
                "\t}",
                "",
                "\tsmp_store_release(&ctx->psi.trigger, new);"
            ],
            "deleted": [
                "\tpsi_trigger_replace(&ctx->psi.trigger, new);",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's implementation of Pressure Stall Information. While the feature is disabled by default, it could allow an attacker to crash the system or have other memory-corruption side effects.",
        "id": 3514
    },
    {
        "cve_id": "CVE-2021-38204",
        "code_before_change": "static int\nmax3421_urb_done(struct usb_hcd *hcd)\n{\n\tstruct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);\n\tunsigned long flags;\n\tstruct urb *urb;\n\tint status;\n\n\tstatus = max3421_hcd->urb_done;\n\tmax3421_hcd->urb_done = 0;\n\tif (status > 0)\n\t\tstatus = 0;\n\turb = max3421_hcd->curr_urb;\n\tif (urb) {\n\t\tmax3421_hcd->curr_urb = NULL;\n\t\tspin_lock_irqsave(&max3421_hcd->lock, flags);\n\t\tusb_hcd_unlink_urb_from_ep(hcd, urb);\n\t\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\n\t\t/* must be called without the HCD spinlock: */\n\t\tusb_hcd_giveback_urb(hcd, urb, status);\n\t}\n\treturn 1;\n}",
        "code_after_change": "static int\nmax3421_urb_done(struct usb_hcd *hcd)\n{\n\tstruct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);\n\tunsigned long flags;\n\tstruct urb *urb;\n\tint status;\n\n\tstatus = max3421_hcd->urb_done;\n\tmax3421_hcd->urb_done = 0;\n\tif (status > 0)\n\t\tstatus = 0;\n\turb = max3421_hcd->curr_urb;\n\tif (urb) {\n\t\t/* save the old end-points toggles: */\n\t\tu8 hrsl = spi_rd8(hcd, MAX3421_REG_HRSL);\n\t\tint rcvtog = (hrsl >> MAX3421_HRSL_RCVTOGRD_BIT) & 1;\n\t\tint sndtog = (hrsl >> MAX3421_HRSL_SNDTOGRD_BIT) & 1;\n\t\tint epnum = usb_endpoint_num(&urb->ep->desc);\n\n\t\t/* no locking: HCD (i.e., we) own toggles, don't we? */\n\t\tusb_settoggle(urb->dev, epnum, 0, rcvtog);\n\t\tusb_settoggle(urb->dev, epnum, 1, sndtog);\n\n\t\tmax3421_hcd->curr_urb = NULL;\n\t\tspin_lock_irqsave(&max3421_hcd->lock, flags);\n\t\tusb_hcd_unlink_urb_from_ep(hcd, urb);\n\t\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\n\t\t/* must be called without the HCD spinlock: */\n\t\tusb_hcd_giveback_urb(hcd, urb, status);\n\t}\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,16 @@\n \t\tstatus = 0;\n \turb = max3421_hcd->curr_urb;\n \tif (urb) {\n+\t\t/* save the old end-points toggles: */\n+\t\tu8 hrsl = spi_rd8(hcd, MAX3421_REG_HRSL);\n+\t\tint rcvtog = (hrsl >> MAX3421_HRSL_RCVTOGRD_BIT) & 1;\n+\t\tint sndtog = (hrsl >> MAX3421_HRSL_SNDTOGRD_BIT) & 1;\n+\t\tint epnum = usb_endpoint_num(&urb->ep->desc);\n+\n+\t\t/* no locking: HCD (i.e., we) own toggles, don't we? */\n+\t\tusb_settoggle(urb->dev, epnum, 0, rcvtog);\n+\t\tusb_settoggle(urb->dev, epnum, 1, sndtog);\n+\n \t\tmax3421_hcd->curr_urb = NULL;\n \t\tspin_lock_irqsave(&max3421_hcd->lock, flags);\n \t\tusb_hcd_unlink_urb_from_ep(hcd, urb);",
        "function_modified_lines": {
            "added": [
                "\t\t/* save the old end-points toggles: */",
                "\t\tu8 hrsl = spi_rd8(hcd, MAX3421_REG_HRSL);",
                "\t\tint rcvtog = (hrsl >> MAX3421_HRSL_RCVTOGRD_BIT) & 1;",
                "\t\tint sndtog = (hrsl >> MAX3421_HRSL_SNDTOGRD_BIT) & 1;",
                "\t\tint epnum = usb_endpoint_num(&urb->ep->desc);",
                "",
                "\t\t/* no locking: HCD (i.e., we) own toggles, don't we? */",
                "\t\tusb_settoggle(urb->dev, epnum, 0, rcvtog);",
                "\t\tusb_settoggle(urb->dev, epnum, 1, sndtog);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "drivers/usb/host/max3421-hcd.c in the Linux kernel before 5.13.6 allows physically proximate attackers to cause a denial of service (use-after-free and panic) by removing a MAX-3421 USB device in certain situations.",
        "id": 3080
    },
    {
        "cve_id": "CVE-2023-51780",
        "code_before_change": "static int do_vcc_ioctl(struct socket *sock, unsigned int cmd,\n\t\t\tunsigned long arg, int compat)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct atm_vcc *vcc;\n\tint error;\n\tstruct list_head *pos;\n\tvoid __user *argp = (void __user *)arg;\n\tvoid __user *buf;\n\tint __user *len;\n\n\tvcc = ATM_SD(sock);\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t\tif (sock->state != SS_CONNECTED ||\n\t\t    !test_bit(ATM_VF_READY, &vcc->flags)) {\n\t\t\terror =  -EINVAL;\n\t\t\tgoto done;\n\t\t}\n\t\terror = put_user(sk->sk_sndbuf - sk_wmem_alloc_get(sk),\n\t\t\t\t (int __user *)argp) ? -EFAULT : 0;\n\t\tgoto done;\n\tcase SIOCINQ:\n\t{\n\t\tstruct sk_buff *skb;\n\n\t\tif (sock->state != SS_CONNECTED) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto done;\n\t\t}\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\terror = put_user(skb ? skb->len : 0,\n\t\t\t\t (int __user *)argp) ? -EFAULT : 0;\n\t\tgoto done;\n\t}\n\tcase ATM_SETSC:\n\t\tnet_warn_ratelimited(\"ATM_SETSC is obsolete; used by %s:%d\\n\",\n\t\t\t\t     current->comm, task_pid_nr(current));\n\t\terror = 0;\n\t\tgoto done;\n\tcase ATMSIGD_CTRL:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\terror = -EPERM;\n\t\t\tgoto done;\n\t\t}\n\t\t/*\n\t\t * The user/kernel protocol for exchanging signalling\n\t\t * info uses kernel pointers as opaque references,\n\t\t * so the holder of the file descriptor can scribble\n\t\t * on the kernel... so we should make sure that we\n\t\t * have the same privileges that /proc/kcore needs\n\t\t */\n\t\tif (!capable(CAP_SYS_RAWIO)) {\n\t\t\terror = -EPERM;\n\t\t\tgoto done;\n\t\t}\n#ifdef CONFIG_COMPAT\n\t\t/* WTF? I don't even want to _think_ about making this\n\t\t   work for 32-bit userspace. TBH I don't really want\n\t\t   to think about it at all. dwmw2. */\n\t\tif (compat) {\n\t\t\tnet_warn_ratelimited(\"32-bit task cannot be atmsigd\\n\");\n\t\t\terror = -EINVAL;\n\t\t\tgoto done;\n\t\t}\n#endif\n\t\terror = sigd_attach(vcc);\n\t\tif (!error)\n\t\t\tsock->state = SS_CONNECTED;\n\t\tgoto done;\n\tcase ATM_SETBACKEND:\n\tcase ATM_NEWBACKENDIF:\n\t{\n\t\tatm_backend_t backend;\n\t\terror = get_user(backend, (atm_backend_t __user *)argp);\n\t\tif (error)\n\t\t\tgoto done;\n\t\tswitch (backend) {\n\t\tcase ATM_BACKEND_PPP:\n\t\t\trequest_module(\"pppoatm\");\n\t\t\tbreak;\n\t\tcase ATM_BACKEND_BR2684:\n\t\t\trequest_module(\"br2684\");\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\tcase ATMMPC_CTRL:\n\tcase ATMMPC_DATA:\n\t\trequest_module(\"mpoa\");\n\t\tbreak;\n\tcase ATMARPD_CTRL:\n\t\trequest_module(\"clip\");\n\t\tbreak;\n\tcase ATMLEC_CTRL:\n\t\trequest_module(\"lec\");\n\t\tbreak;\n\t}\n\n\terror = -ENOIOCTLCMD;\n\n\tmutex_lock(&ioctl_mutex);\n\tlist_for_each(pos, &ioctl_list) {\n\t\tstruct atm_ioctl *ic = list_entry(pos, struct atm_ioctl, list);\n\t\tif (try_module_get(ic->owner)) {\n\t\t\terror = ic->ioctl(sock, cmd, arg);\n\t\t\tmodule_put(ic->owner);\n\t\t\tif (error != -ENOIOCTLCMD)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&ioctl_mutex);\n\n\tif (error != -ENOIOCTLCMD)\n\t\tgoto done;\n\n\tif (cmd == ATM_GETNAMES) {\n\t\tif (IS_ENABLED(CONFIG_COMPAT) && compat) {\n#ifdef CONFIG_COMPAT\n\t\t\tstruct compat_atm_iobuf __user *ciobuf = argp;\n\t\t\tcompat_uptr_t cbuf;\n\t\t\tlen = &ciobuf->length;\n\t\t\tif (get_user(cbuf, &ciobuf->buffer))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf = compat_ptr(cbuf);\n#endif\n\t\t} else {\n\t\t\tstruct atm_iobuf __user *iobuf = argp;\n\t\t\tlen = &iobuf->length;\n\t\t\tif (get_user(buf, &iobuf->buffer))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t\terror = atm_getnames(buf, len);\n\t} else {\n\t\tint number;\n\n\t\tif (IS_ENABLED(CONFIG_COMPAT) && compat) {\n#ifdef CONFIG_COMPAT\n\t\t\tstruct compat_atmif_sioc __user *csioc = argp;\n\t\t\tcompat_uptr_t carg;\n\n\t\t\tlen = &csioc->length;\n\t\t\tif (get_user(carg, &csioc->arg))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf = compat_ptr(carg);\n\t\t\tif (get_user(number, &csioc->number))\n\t\t\t\treturn -EFAULT;\n#endif\n\t\t} else {\n\t\t\tstruct atmif_sioc __user *sioc = argp;\n\n\t\t\tlen = &sioc->length;\n\t\t\tif (get_user(buf, &sioc->arg))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (get_user(number, &sioc->number))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t\terror = atm_dev_ioctl(cmd, buf, len, number, compat);\n\t}\n\ndone:\n\treturn error;\n}",
        "code_after_change": "static int do_vcc_ioctl(struct socket *sock, unsigned int cmd,\n\t\t\tunsigned long arg, int compat)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct atm_vcc *vcc;\n\tint error;\n\tstruct list_head *pos;\n\tvoid __user *argp = (void __user *)arg;\n\tvoid __user *buf;\n\tint __user *len;\n\n\tvcc = ATM_SD(sock);\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t\tif (sock->state != SS_CONNECTED ||\n\t\t    !test_bit(ATM_VF_READY, &vcc->flags)) {\n\t\t\terror =  -EINVAL;\n\t\t\tgoto done;\n\t\t}\n\t\terror = put_user(sk->sk_sndbuf - sk_wmem_alloc_get(sk),\n\t\t\t\t (int __user *)argp) ? -EFAULT : 0;\n\t\tgoto done;\n\tcase SIOCINQ:\n\t{\n\t\tstruct sk_buff *skb;\n\t\tint amount;\n\n\t\tif (sock->state != SS_CONNECTED) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto done;\n\t\t}\n\t\tspin_lock_irq(&sk->sk_receive_queue.lock);\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tamount = skb ? skb->len : 0;\n\t\tspin_unlock_irq(&sk->sk_receive_queue.lock);\n\t\terror = put_user(amount, (int __user *)argp) ? -EFAULT : 0;\n\t\tgoto done;\n\t}\n\tcase ATM_SETSC:\n\t\tnet_warn_ratelimited(\"ATM_SETSC is obsolete; used by %s:%d\\n\",\n\t\t\t\t     current->comm, task_pid_nr(current));\n\t\terror = 0;\n\t\tgoto done;\n\tcase ATMSIGD_CTRL:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\terror = -EPERM;\n\t\t\tgoto done;\n\t\t}\n\t\t/*\n\t\t * The user/kernel protocol for exchanging signalling\n\t\t * info uses kernel pointers as opaque references,\n\t\t * so the holder of the file descriptor can scribble\n\t\t * on the kernel... so we should make sure that we\n\t\t * have the same privileges that /proc/kcore needs\n\t\t */\n\t\tif (!capable(CAP_SYS_RAWIO)) {\n\t\t\terror = -EPERM;\n\t\t\tgoto done;\n\t\t}\n#ifdef CONFIG_COMPAT\n\t\t/* WTF? I don't even want to _think_ about making this\n\t\t   work for 32-bit userspace. TBH I don't really want\n\t\t   to think about it at all. dwmw2. */\n\t\tif (compat) {\n\t\t\tnet_warn_ratelimited(\"32-bit task cannot be atmsigd\\n\");\n\t\t\terror = -EINVAL;\n\t\t\tgoto done;\n\t\t}\n#endif\n\t\terror = sigd_attach(vcc);\n\t\tif (!error)\n\t\t\tsock->state = SS_CONNECTED;\n\t\tgoto done;\n\tcase ATM_SETBACKEND:\n\tcase ATM_NEWBACKENDIF:\n\t{\n\t\tatm_backend_t backend;\n\t\terror = get_user(backend, (atm_backend_t __user *)argp);\n\t\tif (error)\n\t\t\tgoto done;\n\t\tswitch (backend) {\n\t\tcase ATM_BACKEND_PPP:\n\t\t\trequest_module(\"pppoatm\");\n\t\t\tbreak;\n\t\tcase ATM_BACKEND_BR2684:\n\t\t\trequest_module(\"br2684\");\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\tcase ATMMPC_CTRL:\n\tcase ATMMPC_DATA:\n\t\trequest_module(\"mpoa\");\n\t\tbreak;\n\tcase ATMARPD_CTRL:\n\t\trequest_module(\"clip\");\n\t\tbreak;\n\tcase ATMLEC_CTRL:\n\t\trequest_module(\"lec\");\n\t\tbreak;\n\t}\n\n\terror = -ENOIOCTLCMD;\n\n\tmutex_lock(&ioctl_mutex);\n\tlist_for_each(pos, &ioctl_list) {\n\t\tstruct atm_ioctl *ic = list_entry(pos, struct atm_ioctl, list);\n\t\tif (try_module_get(ic->owner)) {\n\t\t\terror = ic->ioctl(sock, cmd, arg);\n\t\t\tmodule_put(ic->owner);\n\t\t\tif (error != -ENOIOCTLCMD)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&ioctl_mutex);\n\n\tif (error != -ENOIOCTLCMD)\n\t\tgoto done;\n\n\tif (cmd == ATM_GETNAMES) {\n\t\tif (IS_ENABLED(CONFIG_COMPAT) && compat) {\n#ifdef CONFIG_COMPAT\n\t\t\tstruct compat_atm_iobuf __user *ciobuf = argp;\n\t\t\tcompat_uptr_t cbuf;\n\t\t\tlen = &ciobuf->length;\n\t\t\tif (get_user(cbuf, &ciobuf->buffer))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf = compat_ptr(cbuf);\n#endif\n\t\t} else {\n\t\t\tstruct atm_iobuf __user *iobuf = argp;\n\t\t\tlen = &iobuf->length;\n\t\t\tif (get_user(buf, &iobuf->buffer))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t\terror = atm_getnames(buf, len);\n\t} else {\n\t\tint number;\n\n\t\tif (IS_ENABLED(CONFIG_COMPAT) && compat) {\n#ifdef CONFIG_COMPAT\n\t\t\tstruct compat_atmif_sioc __user *csioc = argp;\n\t\t\tcompat_uptr_t carg;\n\n\t\t\tlen = &csioc->length;\n\t\t\tif (get_user(carg, &csioc->arg))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf = compat_ptr(carg);\n\t\t\tif (get_user(number, &csioc->number))\n\t\t\t\treturn -EFAULT;\n#endif\n\t\t} else {\n\t\t\tstruct atmif_sioc __user *sioc = argp;\n\n\t\t\tlen = &sioc->length;\n\t\t\tif (get_user(buf, &sioc->arg))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (get_user(number, &sioc->number))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t\terror = atm_dev_ioctl(cmd, buf, len, number, compat);\n\t}\n\ndone:\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,14 +23,17 @@\n \tcase SIOCINQ:\n \t{\n \t\tstruct sk_buff *skb;\n+\t\tint amount;\n \n \t\tif (sock->state != SS_CONNECTED) {\n \t\t\terror = -EINVAL;\n \t\t\tgoto done;\n \t\t}\n+\t\tspin_lock_irq(&sk->sk_receive_queue.lock);\n \t\tskb = skb_peek(&sk->sk_receive_queue);\n-\t\terror = put_user(skb ? skb->len : 0,\n-\t\t\t\t (int __user *)argp) ? -EFAULT : 0;\n+\t\tamount = skb ? skb->len : 0;\n+\t\tspin_unlock_irq(&sk->sk_receive_queue.lock);\n+\t\terror = put_user(amount, (int __user *)argp) ? -EFAULT : 0;\n \t\tgoto done;\n \t}\n \tcase ATM_SETSC:",
        "function_modified_lines": {
            "added": [
                "\t\tint amount;",
                "\t\tspin_lock_irq(&sk->sk_receive_queue.lock);",
                "\t\tamount = skb ? skb->len : 0;",
                "\t\tspin_unlock_irq(&sk->sk_receive_queue.lock);",
                "\t\terror = put_user(amount, (int __user *)argp) ? -EFAULT : 0;"
            ],
            "deleted": [
                "\t\terror = put_user(skb ? skb->len : 0,",
                "\t\t\t\t (int __user *)argp) ? -EFAULT : 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.6.8. do_vcc_ioctl in net/atm/ioctl.c has a use-after-free because of a vcc_recvmsg race condition.",
        "id": 4258
    },
    {
        "cve_id": "CVE-2023-1249",
        "code_before_change": "static int fill_note_info(struct elfhdr *elf, int phdrs,\n\t\t\t  struct elf_note_info *info,\n\t\t\t  struct coredump_params *cprm)\n{\n\tstruct core_thread *ct;\n\tstruct elf_thread_status *ets;\n\n\tif (!elf_note_info_init(info))\n\t\treturn 0;\n\n\tfor (ct = current->signal->core_state->dumper.next;\n\t\t\t\t\tct; ct = ct->next) {\n\t\tets = kzalloc(sizeof(*ets), GFP_KERNEL);\n\t\tif (!ets)\n\t\t\treturn 0;\n\n\t\tets->thread = ct->task;\n\t\tlist_add(&ets->list, &info->thread_list);\n\t}\n\n\tlist_for_each_entry(ets, &info->thread_list, list) {\n\t\tint sz;\n\n\t\tsz = elf_dump_thread_status(cprm->siginfo->si_signo, ets);\n\t\tinfo->thread_status_size += sz;\n\t}\n\t/* now collect the dump for the current */\n\tmemset(info->prstatus, 0, sizeof(*info->prstatus));\n\tfill_prstatus(&info->prstatus->common, current, cprm->siginfo->si_signo);\n\telf_core_copy_regs(&info->prstatus->pr_reg, cprm->regs);\n\n\t/* Set up header */\n\tfill_elf_header(elf, phdrs, ELF_ARCH, ELF_CORE_EFLAGS);\n\n\t/*\n\t * Set up the notes in similar form to SVR4 core dumps made\n\t * with info from their /proc.\n\t */\n\n\tfill_note(info->notes + 0, \"CORE\", NT_PRSTATUS,\n\t\t  sizeof(*info->prstatus), info->prstatus);\n\tfill_psinfo(info->psinfo, current->group_leader, current->mm);\n\tfill_note(info->notes + 1, \"CORE\", NT_PRPSINFO,\n\t\t  sizeof(*info->psinfo), info->psinfo);\n\n\tfill_siginfo_note(info->notes + 2, &info->csigdata, cprm->siginfo);\n\tfill_auxv_note(info->notes + 3, current->mm);\n\tinfo->numnote = 4;\n\n\tif (fill_files_note(info->notes + info->numnote) == 0) {\n\t\tinfo->notes_files = info->notes + info->numnote;\n\t\tinfo->numnote++;\n\t}\n\n\t/* Try to dump the FPU. */\n\tinfo->prstatus->pr_fpvalid =\n\t\telf_core_copy_task_fpregs(current, cprm->regs, info->fpu);\n\tif (info->prstatus->pr_fpvalid)\n\t\tfill_note(info->notes + info->numnote++,\n\t\t\t  \"CORE\", NT_PRFPREG, sizeof(*info->fpu), info->fpu);\n\treturn 1;\n}",
        "code_after_change": "static int fill_note_info(struct elfhdr *elf, int phdrs,\n\t\t\t  struct elf_note_info *info,\n\t\t\t  struct coredump_params *cprm)\n{\n\tstruct core_thread *ct;\n\tstruct elf_thread_status *ets;\n\n\tif (!elf_note_info_init(info))\n\t\treturn 0;\n\n\tfor (ct = current->signal->core_state->dumper.next;\n\t\t\t\t\tct; ct = ct->next) {\n\t\tets = kzalloc(sizeof(*ets), GFP_KERNEL);\n\t\tif (!ets)\n\t\t\treturn 0;\n\n\t\tets->thread = ct->task;\n\t\tlist_add(&ets->list, &info->thread_list);\n\t}\n\n\tlist_for_each_entry(ets, &info->thread_list, list) {\n\t\tint sz;\n\n\t\tsz = elf_dump_thread_status(cprm->siginfo->si_signo, ets);\n\t\tinfo->thread_status_size += sz;\n\t}\n\t/* now collect the dump for the current */\n\tmemset(info->prstatus, 0, sizeof(*info->prstatus));\n\tfill_prstatus(&info->prstatus->common, current, cprm->siginfo->si_signo);\n\telf_core_copy_regs(&info->prstatus->pr_reg, cprm->regs);\n\n\t/* Set up header */\n\tfill_elf_header(elf, phdrs, ELF_ARCH, ELF_CORE_EFLAGS);\n\n\t/*\n\t * Set up the notes in similar form to SVR4 core dumps made\n\t * with info from their /proc.\n\t */\n\n\tfill_note(info->notes + 0, \"CORE\", NT_PRSTATUS,\n\t\t  sizeof(*info->prstatus), info->prstatus);\n\tfill_psinfo(info->psinfo, current->group_leader, current->mm);\n\tfill_note(info->notes + 1, \"CORE\", NT_PRPSINFO,\n\t\t  sizeof(*info->psinfo), info->psinfo);\n\n\tfill_siginfo_note(info->notes + 2, &info->csigdata, cprm->siginfo);\n\tfill_auxv_note(info->notes + 3, current->mm);\n\tinfo->numnote = 4;\n\n\tif (fill_files_note(info->notes + info->numnote, cprm) == 0) {\n\t\tinfo->notes_files = info->notes + info->numnote;\n\t\tinfo->numnote++;\n\t}\n\n\t/* Try to dump the FPU. */\n\tinfo->prstatus->pr_fpvalid =\n\t\telf_core_copy_task_fpregs(current, cprm->regs, info->fpu);\n\tif (info->prstatus->pr_fpvalid)\n\t\tfill_note(info->notes + info->numnote++,\n\t\t\t  \"CORE\", NT_PRFPREG, sizeof(*info->fpu), info->fpu);\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,7 +47,7 @@\n \tfill_auxv_note(info->notes + 3, current->mm);\n \tinfo->numnote = 4;\n \n-\tif (fill_files_note(info->notes + info->numnote) == 0) {\n+\tif (fill_files_note(info->notes + info->numnote, cprm) == 0) {\n \t\tinfo->notes_files = info->notes + info->numnote;\n \t\tinfo->numnote++;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif (fill_files_note(info->notes + info->numnote, cprm) == 0) {"
            ],
            "deleted": [
                "\tif (fill_files_note(info->notes + info->numnote) == 0) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s core dump subsystem. This flaw allows a local user to crash the system. Only if patch 390031c94211 (\"coredump: Use the vma snapshot in fill_files_note\") not applied yet, then kernel could be affected.",
        "id": 3858
    },
    {
        "cve_id": "CVE-2022-42720",
        "code_before_change": "struct cfg80211_internal_bss *\ncfg80211_bss_update(struct cfg80211_registered_device *rdev,\n\t\t    struct cfg80211_internal_bss *tmp,\n\t\t    bool signal_valid, unsigned long ts)\n{\n\tstruct cfg80211_internal_bss *found = NULL;\n\n\tif (WARN_ON(!tmp->pub.channel))\n\t\treturn NULL;\n\n\ttmp->ts = ts;\n\n\tspin_lock_bh(&rdev->bss_lock);\n\n\tif (WARN_ON(!rcu_access_pointer(tmp->pub.ies))) {\n\t\tspin_unlock_bh(&rdev->bss_lock);\n\t\treturn NULL;\n\t}\n\n\tfound = rb_find_bss(rdev, tmp, BSS_CMP_REGULAR);\n\n\tif (found) {\n\t\tif (!cfg80211_update_known_bss(rdev, found, tmp, signal_valid))\n\t\t\tgoto drop;\n\t} else {\n\t\tstruct cfg80211_internal_bss *new;\n\t\tstruct cfg80211_internal_bss *hidden;\n\t\tstruct cfg80211_bss_ies *ies;\n\n\t\t/*\n\t\t * create a copy -- the \"res\" variable that is passed in\n\t\t * is allocated on the stack since it's not needed in the\n\t\t * more common case of an update\n\t\t */\n\t\tnew = kzalloc(sizeof(*new) + rdev->wiphy.bss_priv_size,\n\t\t\t      GFP_ATOMIC);\n\t\tif (!new) {\n\t\t\ties = (void *)rcu_dereference(tmp->pub.beacon_ies);\n\t\t\tif (ies)\n\t\t\t\tkfree_rcu(ies, rcu_head);\n\t\t\ties = (void *)rcu_dereference(tmp->pub.proberesp_ies);\n\t\t\tif (ies)\n\t\t\t\tkfree_rcu(ies, rcu_head);\n\t\t\tgoto drop;\n\t\t}\n\t\tmemcpy(new, tmp, sizeof(*new));\n\t\tnew->refcount = 1;\n\t\tINIT_LIST_HEAD(&new->hidden_list);\n\t\tINIT_LIST_HEAD(&new->pub.nontrans_list);\n\n\t\tif (rcu_access_pointer(tmp->pub.proberesp_ies)) {\n\t\t\thidden = rb_find_bss(rdev, tmp, BSS_CMP_HIDE_ZLEN);\n\t\t\tif (!hidden)\n\t\t\t\thidden = rb_find_bss(rdev, tmp,\n\t\t\t\t\t\t     BSS_CMP_HIDE_NUL);\n\t\t\tif (hidden) {\n\t\t\t\tnew->pub.hidden_beacon_bss = &hidden->pub;\n\t\t\t\tlist_add(&new->hidden_list,\n\t\t\t\t\t &hidden->hidden_list);\n\t\t\t\thidden->refcount++;\n\t\t\t\trcu_assign_pointer(new->pub.beacon_ies,\n\t\t\t\t\t\t   hidden->pub.beacon_ies);\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * Ok so we found a beacon, and don't have an entry. If\n\t\t\t * it's a beacon with hidden SSID, we might be in for an\n\t\t\t * expensive search for any probe responses that should\n\t\t\t * be grouped with this beacon for updates ...\n\t\t\t */\n\t\t\tif (!cfg80211_combine_bsses(rdev, new)) {\n\t\t\t\tbss_ref_put(rdev, new);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\n\t\tif (rdev->bss_entries >= bss_entries_limit &&\n\t\t    !cfg80211_bss_expire_oldest(rdev)) {\n\t\t\tbss_ref_put(rdev, new);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* This must be before the call to bss_ref_get */\n\t\tif (tmp->pub.transmitted_bss) {\n\t\t\tstruct cfg80211_internal_bss *pbss =\n\t\t\t\tcontainer_of(tmp->pub.transmitted_bss,\n\t\t\t\t\t     struct cfg80211_internal_bss,\n\t\t\t\t\t     pub);\n\n\t\t\tnew->pub.transmitted_bss = tmp->pub.transmitted_bss;\n\t\t\tbss_ref_get(rdev, pbss);\n\t\t}\n\n\t\tlist_add_tail(&new->list, &rdev->bss_list);\n\t\trdev->bss_entries++;\n\t\trb_insert_bss(rdev, new);\n\t\tfound = new;\n\t}\n\n\trdev->bss_generation++;\n\tbss_ref_get(rdev, found);\n\tspin_unlock_bh(&rdev->bss_lock);\n\n\treturn found;\n drop:\n\tspin_unlock_bh(&rdev->bss_lock);\n\treturn NULL;\n}",
        "code_after_change": "struct cfg80211_internal_bss *\ncfg80211_bss_update(struct cfg80211_registered_device *rdev,\n\t\t    struct cfg80211_internal_bss *tmp,\n\t\t    bool signal_valid, unsigned long ts)\n{\n\tstruct cfg80211_internal_bss *found = NULL;\n\n\tif (WARN_ON(!tmp->pub.channel))\n\t\treturn NULL;\n\n\ttmp->ts = ts;\n\n\tspin_lock_bh(&rdev->bss_lock);\n\n\tif (WARN_ON(!rcu_access_pointer(tmp->pub.ies))) {\n\t\tspin_unlock_bh(&rdev->bss_lock);\n\t\treturn NULL;\n\t}\n\n\tfound = rb_find_bss(rdev, tmp, BSS_CMP_REGULAR);\n\n\tif (found) {\n\t\tif (!cfg80211_update_known_bss(rdev, found, tmp, signal_valid))\n\t\t\tgoto drop;\n\t} else {\n\t\tstruct cfg80211_internal_bss *new;\n\t\tstruct cfg80211_internal_bss *hidden;\n\t\tstruct cfg80211_bss_ies *ies;\n\n\t\t/*\n\t\t * create a copy -- the \"res\" variable that is passed in\n\t\t * is allocated on the stack since it's not needed in the\n\t\t * more common case of an update\n\t\t */\n\t\tnew = kzalloc(sizeof(*new) + rdev->wiphy.bss_priv_size,\n\t\t\t      GFP_ATOMIC);\n\t\tif (!new) {\n\t\t\ties = (void *)rcu_dereference(tmp->pub.beacon_ies);\n\t\t\tif (ies)\n\t\t\t\tkfree_rcu(ies, rcu_head);\n\t\t\ties = (void *)rcu_dereference(tmp->pub.proberesp_ies);\n\t\t\tif (ies)\n\t\t\t\tkfree_rcu(ies, rcu_head);\n\t\t\tgoto drop;\n\t\t}\n\t\tmemcpy(new, tmp, sizeof(*new));\n\t\tnew->refcount = 1;\n\t\tINIT_LIST_HEAD(&new->hidden_list);\n\t\tINIT_LIST_HEAD(&new->pub.nontrans_list);\n\t\t/* we'll set this later if it was non-NULL */\n\t\tnew->pub.transmitted_bss = NULL;\n\n\t\tif (rcu_access_pointer(tmp->pub.proberesp_ies)) {\n\t\t\thidden = rb_find_bss(rdev, tmp, BSS_CMP_HIDE_ZLEN);\n\t\t\tif (!hidden)\n\t\t\t\thidden = rb_find_bss(rdev, tmp,\n\t\t\t\t\t\t     BSS_CMP_HIDE_NUL);\n\t\t\tif (hidden) {\n\t\t\t\tnew->pub.hidden_beacon_bss = &hidden->pub;\n\t\t\t\tlist_add(&new->hidden_list,\n\t\t\t\t\t &hidden->hidden_list);\n\t\t\t\thidden->refcount++;\n\t\t\t\trcu_assign_pointer(new->pub.beacon_ies,\n\t\t\t\t\t\t   hidden->pub.beacon_ies);\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * Ok so we found a beacon, and don't have an entry. If\n\t\t\t * it's a beacon with hidden SSID, we might be in for an\n\t\t\t * expensive search for any probe responses that should\n\t\t\t * be grouped with this beacon for updates ...\n\t\t\t */\n\t\t\tif (!cfg80211_combine_bsses(rdev, new)) {\n\t\t\t\tbss_ref_put(rdev, new);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\n\t\tif (rdev->bss_entries >= bss_entries_limit &&\n\t\t    !cfg80211_bss_expire_oldest(rdev)) {\n\t\t\tbss_ref_put(rdev, new);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t/* This must be before the call to bss_ref_get */\n\t\tif (tmp->pub.transmitted_bss) {\n\t\t\tstruct cfg80211_internal_bss *pbss =\n\t\t\t\tcontainer_of(tmp->pub.transmitted_bss,\n\t\t\t\t\t     struct cfg80211_internal_bss,\n\t\t\t\t\t     pub);\n\n\t\t\tnew->pub.transmitted_bss = tmp->pub.transmitted_bss;\n\t\t\tbss_ref_get(rdev, pbss);\n\t\t}\n\n\t\tlist_add_tail(&new->list, &rdev->bss_list);\n\t\trdev->bss_entries++;\n\t\trb_insert_bss(rdev, new);\n\t\tfound = new;\n\t}\n\n\trdev->bss_generation++;\n\tbss_ref_get(rdev, found);\n\tspin_unlock_bh(&rdev->bss_lock);\n\n\treturn found;\n drop:\n\tspin_unlock_bh(&rdev->bss_lock);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,6 +47,8 @@\n \t\tnew->refcount = 1;\n \t\tINIT_LIST_HEAD(&new->hidden_list);\n \t\tINIT_LIST_HEAD(&new->pub.nontrans_list);\n+\t\t/* we'll set this later if it was non-NULL */\n+\t\tnew->pub.transmitted_bss = NULL;\n \n \t\tif (rcu_access_pointer(tmp->pub.proberesp_ies)) {\n \t\t\thidden = rb_find_bss(rdev, tmp, BSS_CMP_HIDE_ZLEN);",
        "function_modified_lines": {
            "added": [
                "\t\t/* we'll set this later if it was non-NULL */",
                "\t\tnew->pub.transmitted_bss = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Various refcounting bugs in the multi-BSS handling in the mac80211 stack in the Linux kernel 5.1 through 5.19.x before 5.19.16 could be used by local attackers (able to inject WLAN frames) to trigger use-after-free conditions to potentially execute code.",
        "id": 3733
    },
    {
        "cve_id": "CVE-2018-14611",
        "code_before_change": "static int btrfs_check_chunk_valid(struct btrfs_fs_info *fs_info,\n\t\t\t\t   struct extent_buffer *leaf,\n\t\t\t\t   struct btrfs_chunk *chunk, u64 logical)\n{\n\tu64 length;\n\tu64 stripe_len;\n\tu16 num_stripes;\n\tu16 sub_stripes;\n\tu64 type;\n\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tstripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tsub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\ttype = btrfs_chunk_type(leaf, chunk);\n\n\tif (!num_stripes) {\n\t\tbtrfs_err(fs_info, \"invalid chunk num_stripes: %u\",\n\t\t\t  num_stripes);\n\t\treturn -EIO;\n\t}\n\tif (!IS_ALIGNED(logical, fs_info->sectorsize)) {\n\t\tbtrfs_err(fs_info, \"invalid chunk logical %llu\", logical);\n\t\treturn -EIO;\n\t}\n\tif (btrfs_chunk_sector_size(leaf, chunk) != fs_info->sectorsize) {\n\t\tbtrfs_err(fs_info, \"invalid chunk sectorsize %u\",\n\t\t\t  btrfs_chunk_sector_size(leaf, chunk));\n\t\treturn -EIO;\n\t}\n\tif (!length || !IS_ALIGNED(length, fs_info->sectorsize)) {\n\t\tbtrfs_err(fs_info, \"invalid chunk length %llu\", length);\n\t\treturn -EIO;\n\t}\n\tif (!is_power_of_2(stripe_len) || stripe_len != BTRFS_STRIPE_LEN) {\n\t\tbtrfs_err(fs_info, \"invalid chunk stripe length: %llu\",\n\t\t\t  stripe_len);\n\t\treturn -EIO;\n\t}\n\tif (~(BTRFS_BLOCK_GROUP_TYPE_MASK | BTRFS_BLOCK_GROUP_PROFILE_MASK) &\n\t    type) {\n\t\tbtrfs_err(fs_info, \"unrecognized chunk type: %llu\",\n\t\t\t  ~(BTRFS_BLOCK_GROUP_TYPE_MASK |\n\t\t\t    BTRFS_BLOCK_GROUP_PROFILE_MASK) &\n\t\t\t  btrfs_chunk_type(leaf, chunk));\n\t\treturn -EIO;\n\t}\n\tif ((type & BTRFS_BLOCK_GROUP_RAID10 && sub_stripes != 2) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID1 && num_stripes < 1) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID5 && num_stripes < 2) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID6 && num_stripes < 3) ||\n\t    (type & BTRFS_BLOCK_GROUP_DUP && num_stripes > 2) ||\n\t    ((type & BTRFS_BLOCK_GROUP_PROFILE_MASK) == 0 &&\n\t     num_stripes != 1)) {\n\t\tbtrfs_err(fs_info,\n\t\t\t\"invalid num_stripes:sub_stripes %u:%u for profile %llu\",\n\t\t\tnum_stripes, sub_stripes,\n\t\t\ttype & BTRFS_BLOCK_GROUP_PROFILE_MASK);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int btrfs_check_chunk_valid(struct btrfs_fs_info *fs_info,\n\t\t\t\t   struct extent_buffer *leaf,\n\t\t\t\t   struct btrfs_chunk *chunk, u64 logical)\n{\n\tu64 length;\n\tu64 stripe_len;\n\tu16 num_stripes;\n\tu16 sub_stripes;\n\tu64 type;\n\tu64 features;\n\tbool mixed = false;\n\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tstripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tsub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\ttype = btrfs_chunk_type(leaf, chunk);\n\n\tif (!num_stripes) {\n\t\tbtrfs_err(fs_info, \"invalid chunk num_stripes: %u\",\n\t\t\t  num_stripes);\n\t\treturn -EIO;\n\t}\n\tif (!IS_ALIGNED(logical, fs_info->sectorsize)) {\n\t\tbtrfs_err(fs_info, \"invalid chunk logical %llu\", logical);\n\t\treturn -EIO;\n\t}\n\tif (btrfs_chunk_sector_size(leaf, chunk) != fs_info->sectorsize) {\n\t\tbtrfs_err(fs_info, \"invalid chunk sectorsize %u\",\n\t\t\t  btrfs_chunk_sector_size(leaf, chunk));\n\t\treturn -EIO;\n\t}\n\tif (!length || !IS_ALIGNED(length, fs_info->sectorsize)) {\n\t\tbtrfs_err(fs_info, \"invalid chunk length %llu\", length);\n\t\treturn -EIO;\n\t}\n\tif (!is_power_of_2(stripe_len) || stripe_len != BTRFS_STRIPE_LEN) {\n\t\tbtrfs_err(fs_info, \"invalid chunk stripe length: %llu\",\n\t\t\t  stripe_len);\n\t\treturn -EIO;\n\t}\n\tif (~(BTRFS_BLOCK_GROUP_TYPE_MASK | BTRFS_BLOCK_GROUP_PROFILE_MASK) &\n\t    type) {\n\t\tbtrfs_err(fs_info, \"unrecognized chunk type: %llu\",\n\t\t\t  ~(BTRFS_BLOCK_GROUP_TYPE_MASK |\n\t\t\t    BTRFS_BLOCK_GROUP_PROFILE_MASK) &\n\t\t\t  btrfs_chunk_type(leaf, chunk));\n\t\treturn -EIO;\n\t}\n\n\tif ((type & BTRFS_BLOCK_GROUP_TYPE_MASK) == 0) {\n\t\tbtrfs_err(fs_info, \"missing chunk type flag: 0x%llx\", type);\n\t\treturn -EIO;\n\t}\n\n\tif ((type & BTRFS_BLOCK_GROUP_SYSTEM) &&\n\t    (type & (BTRFS_BLOCK_GROUP_METADATA | BTRFS_BLOCK_GROUP_DATA))) {\n\t\tbtrfs_err(fs_info,\n\t\t\t\"system chunk with data or metadata type: 0x%llx\", type);\n\t\treturn -EIO;\n\t}\n\n\tfeatures = btrfs_super_incompat_flags(fs_info->super_copy);\n\tif (features & BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS)\n\t\tmixed = true;\n\n\tif (!mixed) {\n\t\tif ((type & BTRFS_BLOCK_GROUP_METADATA) &&\n\t\t    (type & BTRFS_BLOCK_GROUP_DATA)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"mixed chunk type in non-mixed mode: 0x%llx\", type);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\tif ((type & BTRFS_BLOCK_GROUP_RAID10 && sub_stripes != 2) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID1 && num_stripes < 1) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID5 && num_stripes < 2) ||\n\t    (type & BTRFS_BLOCK_GROUP_RAID6 && num_stripes < 3) ||\n\t    (type & BTRFS_BLOCK_GROUP_DUP && num_stripes > 2) ||\n\t    ((type & BTRFS_BLOCK_GROUP_PROFILE_MASK) == 0 &&\n\t     num_stripes != 1)) {\n\t\tbtrfs_err(fs_info,\n\t\t\t\"invalid num_stripes:sub_stripes %u:%u for profile %llu\",\n\t\t\tnum_stripes, sub_stripes,\n\t\t\ttype & BTRFS_BLOCK_GROUP_PROFILE_MASK);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,8 @@\n \tu16 num_stripes;\n \tu16 sub_stripes;\n \tu64 type;\n+\tu64 features;\n+\tbool mixed = false;\n \n \tlength = btrfs_chunk_length(leaf, chunk);\n \tstripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n@@ -45,6 +47,32 @@\n \t\t\t  btrfs_chunk_type(leaf, chunk));\n \t\treturn -EIO;\n \t}\n+\n+\tif ((type & BTRFS_BLOCK_GROUP_TYPE_MASK) == 0) {\n+\t\tbtrfs_err(fs_info, \"missing chunk type flag: 0x%llx\", type);\n+\t\treturn -EIO;\n+\t}\n+\n+\tif ((type & BTRFS_BLOCK_GROUP_SYSTEM) &&\n+\t    (type & (BTRFS_BLOCK_GROUP_METADATA | BTRFS_BLOCK_GROUP_DATA))) {\n+\t\tbtrfs_err(fs_info,\n+\t\t\t\"system chunk with data or metadata type: 0x%llx\", type);\n+\t\treturn -EIO;\n+\t}\n+\n+\tfeatures = btrfs_super_incompat_flags(fs_info->super_copy);\n+\tif (features & BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS)\n+\t\tmixed = true;\n+\n+\tif (!mixed) {\n+\t\tif ((type & BTRFS_BLOCK_GROUP_METADATA) &&\n+\t\t    (type & BTRFS_BLOCK_GROUP_DATA)) {\n+\t\t\tbtrfs_err(fs_info,\n+\t\t\t\"mixed chunk type in non-mixed mode: 0x%llx\", type);\n+\t\t\treturn -EIO;\n+\t\t}\n+\t}\n+\n \tif ((type & BTRFS_BLOCK_GROUP_RAID10 && sub_stripes != 2) ||\n \t    (type & BTRFS_BLOCK_GROUP_RAID1 && num_stripes < 1) ||\n \t    (type & BTRFS_BLOCK_GROUP_RAID5 && num_stripes < 2) ||",
        "function_modified_lines": {
            "added": [
                "\tu64 features;",
                "\tbool mixed = false;",
                "",
                "\tif ((type & BTRFS_BLOCK_GROUP_TYPE_MASK) == 0) {",
                "\t\tbtrfs_err(fs_info, \"missing chunk type flag: 0x%llx\", type);",
                "\t\treturn -EIO;",
                "\t}",
                "",
                "\tif ((type & BTRFS_BLOCK_GROUP_SYSTEM) &&",
                "\t    (type & (BTRFS_BLOCK_GROUP_METADATA | BTRFS_BLOCK_GROUP_DATA))) {",
                "\t\tbtrfs_err(fs_info,",
                "\t\t\t\"system chunk with data or metadata type: 0x%llx\", type);",
                "\t\treturn -EIO;",
                "\t}",
                "",
                "\tfeatures = btrfs_super_incompat_flags(fs_info->super_copy);",
                "\tif (features & BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS)",
                "\t\tmixed = true;",
                "",
                "\tif (!mixed) {",
                "\t\tif ((type & BTRFS_BLOCK_GROUP_METADATA) &&",
                "\t\t    (type & BTRFS_BLOCK_GROUP_DATA)) {",
                "\t\t\tbtrfs_err(fs_info,",
                "\t\t\t\"mixed chunk type in non-mixed mode: 0x%llx\", type);",
                "\t\t\treturn -EIO;",
                "\t\t}",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 4.17.10. There is a use-after-free in try_merge_free_space() when mounting a crafted btrfs image, because of a lack of chunk type flag checks in btrfs_check_chunk_valid in fs/btrfs/volumes.c.",
        "id": 1680
    },
    {
        "cve_id": "CVE-2018-9465",
        "code_before_change": "static void binder_deferred_func(struct work_struct *work)\n{\n\tstruct binder_proc *proc;\n\tstruct files_struct *files;\n\n\tint defer;\n\n\tdo {\n\t\tmutex_lock(&binder_deferred_lock);\n\t\tif (!hlist_empty(&binder_deferred_list)) {\n\t\t\tproc = hlist_entry(binder_deferred_list.first,\n\t\t\t\t\tstruct binder_proc, deferred_work_node);\n\t\t\thlist_del_init(&proc->deferred_work_node);\n\t\t\tdefer = proc->deferred_work;\n\t\t\tproc->deferred_work = 0;\n\t\t} else {\n\t\t\tproc = NULL;\n\t\t\tdefer = 0;\n\t\t}\n\t\tmutex_unlock(&binder_deferred_lock);\n\n\t\tfiles = NULL;\n\t\tif (defer & BINDER_DEFERRED_PUT_FILES) {\n\t\t\tfiles = proc->files;\n\t\t\tif (files)\n\t\t\t\tproc->files = NULL;\n\t\t}\n\n\t\tif (defer & BINDER_DEFERRED_FLUSH)\n\t\t\tbinder_deferred_flush(proc);\n\n\t\tif (defer & BINDER_DEFERRED_RELEASE)\n\t\t\tbinder_deferred_release(proc); /* frees proc */\n\n\t\tif (files)\n\t\t\tput_files_struct(files);\n\t} while (proc);\n}",
        "code_after_change": "static void binder_deferred_func(struct work_struct *work)\n{\n\tstruct binder_proc *proc;\n\tstruct files_struct *files;\n\n\tint defer;\n\n\tdo {\n\t\tmutex_lock(&binder_deferred_lock);\n\t\tif (!hlist_empty(&binder_deferred_list)) {\n\t\t\tproc = hlist_entry(binder_deferred_list.first,\n\t\t\t\t\tstruct binder_proc, deferred_work_node);\n\t\t\thlist_del_init(&proc->deferred_work_node);\n\t\t\tdefer = proc->deferred_work;\n\t\t\tproc->deferred_work = 0;\n\t\t} else {\n\t\t\tproc = NULL;\n\t\t\tdefer = 0;\n\t\t}\n\t\tmutex_unlock(&binder_deferred_lock);\n\n\t\tfiles = NULL;\n\t\tif (defer & BINDER_DEFERRED_PUT_FILES) {\n\t\t\tmutex_lock(&proc->files_lock);\n\t\t\tfiles = proc->files;\n\t\t\tif (files)\n\t\t\t\tproc->files = NULL;\n\t\t\tmutex_unlock(&proc->files_lock);\n\t\t}\n\n\t\tif (defer & BINDER_DEFERRED_FLUSH)\n\t\t\tbinder_deferred_flush(proc);\n\n\t\tif (defer & BINDER_DEFERRED_RELEASE)\n\t\t\tbinder_deferred_release(proc); /* frees proc */\n\n\t\tif (files)\n\t\t\tput_files_struct(files);\n\t} while (proc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,9 +21,11 @@\n \n \t\tfiles = NULL;\n \t\tif (defer & BINDER_DEFERRED_PUT_FILES) {\n+\t\t\tmutex_lock(&proc->files_lock);\n \t\t\tfiles = proc->files;\n \t\t\tif (files)\n \t\t\t\tproc->files = NULL;\n+\t\t\tmutex_unlock(&proc->files_lock);\n \t\t}\n \n \t\tif (defer & BINDER_DEFERRED_FLUSH)",
        "function_modified_lines": {
            "added": [
                "\t\t\tmutex_lock(&proc->files_lock);",
                "\t\t\tmutex_unlock(&proc->files_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In task_get_unused_fd_flags of binder.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation. Product: Android Versions: Android kernel Android ID: A-69164715 References: Upstream kernel.",
        "id": 1867
    },
    {
        "cve_id": "CVE-2023-5197",
        "code_before_change": "static int nf_tables_delrule(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t     const struct nlattr * const nla[])\n{\n\tstruct netlink_ext_ack *extack = info->extack;\n\tu8 genmask = nft_genmask_next(info->net);\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct nft_chain *chain = NULL;\n\tstruct net *net = info->net;\n\tstruct nft_table *table;\n\tstruct nft_rule *rule;\n\tstruct nft_ctx ctx;\n\tint err = 0;\n\n\ttable = nft_table_lookup(net, nla[NFTA_RULE_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_RULE_CHAIN]) {\n\t\tchain = nft_chain_lookup(net, table, nla[NFTA_RULE_CHAIN],\n\t\t\t\t\t genmask);\n\t\tif (IS_ERR(chain)) {\n\t\t\tif (PTR_ERR(chain) == -ENOENT &&\n\t\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYRULE)\n\t\t\t\treturn 0;\n\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\t\tif (nft_chain_is_bound(chain))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tif (chain) {\n\t\tif (nla[NFTA_RULE_HANDLE]) {\n\t\t\trule = nft_rule_lookup(chain, nla[NFTA_RULE_HANDLE]);\n\t\t\tif (IS_ERR(rule)) {\n\t\t\t\tif (PTR_ERR(rule) == -ENOENT &&\n\t\t\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYRULE)\n\t\t\t\t\treturn 0;\n\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\t\treturn PTR_ERR(rule);\n\t\t\t}\n\n\t\t\terr = nft_delrule(&ctx, rule);\n\t\t} else if (nla[NFTA_RULE_ID]) {\n\t\t\trule = nft_rule_lookup_byid(net, chain, nla[NFTA_RULE_ID]);\n\t\t\tif (IS_ERR(rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_ID]);\n\t\t\t\treturn PTR_ERR(rule);\n\t\t\t}\n\n\t\t\terr = nft_delrule(&ctx, rule);\n\t\t} else {\n\t\t\terr = nft_delrule_by_chain(&ctx);\n\t\t}\n\t} else {\n\t\tlist_for_each_entry(chain, &table->chains, list) {\n\t\t\tif (!nft_is_active_next(net, chain))\n\t\t\t\tcontinue;\n\t\t\tif (nft_chain_is_bound(chain))\n\t\t\t\tcontinue;\n\n\t\t\tctx.chain = chain;\n\t\t\terr = nft_delrule_by_chain(&ctx);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}",
        "code_after_change": "static int nf_tables_delrule(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t     const struct nlattr * const nla[])\n{\n\tstruct netlink_ext_ack *extack = info->extack;\n\tu8 genmask = nft_genmask_next(info->net);\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct nft_chain *chain = NULL;\n\tstruct net *net = info->net;\n\tstruct nft_table *table;\n\tstruct nft_rule *rule;\n\tstruct nft_ctx ctx;\n\tint err = 0;\n\n\ttable = nft_table_lookup(net, nla[NFTA_RULE_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_RULE_CHAIN]) {\n\t\tchain = nft_chain_lookup(net, table, nla[NFTA_RULE_CHAIN],\n\t\t\t\t\t genmask);\n\t\tif (IS_ERR(chain)) {\n\t\t\tif (PTR_ERR(chain) == -ENOENT &&\n\t\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYRULE)\n\t\t\t\treturn 0;\n\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\t\tif (nft_chain_binding(chain))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tif (chain) {\n\t\tif (nla[NFTA_RULE_HANDLE]) {\n\t\t\trule = nft_rule_lookup(chain, nla[NFTA_RULE_HANDLE]);\n\t\t\tif (IS_ERR(rule)) {\n\t\t\t\tif (PTR_ERR(rule) == -ENOENT &&\n\t\t\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYRULE)\n\t\t\t\t\treturn 0;\n\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\t\treturn PTR_ERR(rule);\n\t\t\t}\n\n\t\t\terr = nft_delrule(&ctx, rule);\n\t\t} else if (nla[NFTA_RULE_ID]) {\n\t\t\trule = nft_rule_lookup_byid(net, chain, nla[NFTA_RULE_ID]);\n\t\t\tif (IS_ERR(rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_ID]);\n\t\t\t\treturn PTR_ERR(rule);\n\t\t\t}\n\n\t\t\terr = nft_delrule(&ctx, rule);\n\t\t} else {\n\t\t\terr = nft_delrule_by_chain(&ctx);\n\t\t}\n\t} else {\n\t\tlist_for_each_entry(chain, &table->chains, list) {\n\t\t\tif (!nft_is_active_next(net, chain))\n\t\t\t\tcontinue;\n\t\t\tif (nft_chain_binding(chain))\n\t\t\t\tcontinue;\n\n\t\t\tctx.chain = chain;\n\t\t\terr = nft_delrule_by_chain(&ctx);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,7 +29,7 @@\n \t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN]);\n \t\t\treturn PTR_ERR(chain);\n \t\t}\n-\t\tif (nft_chain_is_bound(chain))\n+\t\tif (nft_chain_binding(chain))\n \t\t\treturn -EOPNOTSUPP;\n \t}\n \n@@ -63,7 +63,7 @@\n \t\tlist_for_each_entry(chain, &table->chains, list) {\n \t\t\tif (!nft_is_active_next(net, chain))\n \t\t\t\tcontinue;\n-\t\t\tif (nft_chain_is_bound(chain))\n+\t\t\tif (nft_chain_binding(chain))\n \t\t\t\tcontinue;\n \n \t\t\tctx.chain = chain;",
        "function_modified_lines": {
            "added": [
                "\t\tif (nft_chain_binding(chain))",
                "\t\t\tif (nft_chain_binding(chain))"
            ],
            "deleted": [
                "\t\tif (nft_chain_is_bound(chain))",
                "\t\t\tif (nft_chain_is_bound(chain))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nAddition and removal of rules from chain bindings within the same transaction causes leads to use-after-free.\n\nWe recommend upgrading past commit f15f29fd4779be8a418b66e9d52979bb6d6c2325.\n\n",
        "id": 4263
    },
    {
        "cve_id": "CVE-2021-33034",
        "code_before_change": "static void hci_disconn_loglink_complete_evt(struct hci_dev *hdev,\n\t\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct hci_ev_disconn_logical_link_complete *ev = (void *) skb->data;\n\tstruct hci_chan *hchan;\n\n\tBT_DBG(\"%s log handle 0x%4.4x status 0x%2.2x\", hdev->name,\n\t       le16_to_cpu(ev->handle), ev->status);\n\n\tif (ev->status)\n\t\treturn;\n\n\thci_dev_lock(hdev);\n\n\thchan = hci_chan_lookup_handle(hdev, le16_to_cpu(ev->handle));\n\tif (!hchan)\n\t\tgoto unlock;\n\n\tamp_destroy_logical_link(hchan, ev->reason);\n\nunlock:\n\thci_dev_unlock(hdev);\n}",
        "code_after_change": "static void hci_disconn_loglink_complete_evt(struct hci_dev *hdev,\n\t\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct hci_ev_disconn_logical_link_complete *ev = (void *) skb->data;\n\tstruct hci_chan *hchan;\n\n\tBT_DBG(\"%s log handle 0x%4.4x status 0x%2.2x\", hdev->name,\n\t       le16_to_cpu(ev->handle), ev->status);\n\n\tif (ev->status)\n\t\treturn;\n\n\thci_dev_lock(hdev);\n\n\thchan = hci_chan_lookup_handle(hdev, le16_to_cpu(ev->handle));\n\tif (!hchan || !hchan->amp)\n\t\tgoto unlock;\n\n\tamp_destroy_logical_link(hchan, ev->reason);\n\nunlock:\n\thci_dev_unlock(hdev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,7 +13,7 @@\n \thci_dev_lock(hdev);\n \n \thchan = hci_chan_lookup_handle(hdev, le16_to_cpu(ev->handle));\n-\tif (!hchan)\n+\tif (!hchan || !hchan->amp)\n \t\tgoto unlock;\n \n \tamp_destroy_logical_link(hchan, ev->reason);",
        "function_modified_lines": {
            "added": [
                "\tif (!hchan || !hchan->amp)"
            ],
            "deleted": [
                "\tif (!hchan)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.12.4, net/bluetooth/hci_event.c has a use-after-free when destroying an hci_chan, aka CID-5c4c8c954409. This leads to writing an arbitrary value.",
        "id": 2969
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static int dmirror_migrate_to_system(struct dmirror *dmirror,\n\t\t\t\t     struct hmm_dmirror_cmd *cmd)\n{\n\tunsigned long start, end, addr;\n\tunsigned long size = cmd->npages << PAGE_SHIFT;\n\tstruct mm_struct *mm = dmirror->notifier.mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long src_pfns[64] = { 0 };\n\tunsigned long dst_pfns[64] = { 0 };\n\tstruct migrate_vma args;\n\tunsigned long next;\n\tint ret;\n\n\tstart = cmd->addr;\n\tend = start + size;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\t/* Since the mm is for the mirrored process, get a reference first. */\n\tif (!mmget_not_zero(mm))\n\t\treturn -EINVAL;\n\n\tcmd->cpages = 0;\n\tmmap_read_lock(mm);\n\tfor (addr = start; addr < end; addr = next) {\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (!vma || !(vma->vm_flags & VM_READ)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tnext = min(end, addr + (ARRAY_SIZE(src_pfns) << PAGE_SHIFT));\n\t\tif (next > vma->vm_end)\n\t\t\tnext = vma->vm_end;\n\n\t\targs.vma = vma;\n\t\targs.src = src_pfns;\n\t\targs.dst = dst_pfns;\n\t\targs.start = addr;\n\t\targs.end = next;\n\t\targs.pgmap_owner = dmirror->mdevice;\n\t\targs.flags = dmirror_select_device(dmirror);\n\n\t\tret = migrate_vma_setup(&args);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tpr_debug(\"Migrating from device mem to sys mem\\n\");\n\t\tdmirror_devmem_fault_alloc_and_copy(&args, dmirror);\n\n\t\tmigrate_vma_pages(&args);\n\t\tcmd->cpages += dmirror_successful_migrated_pages(&args);\n\t\tmigrate_vma_finalize(&args);\n\t}\nout:\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\treturn ret;\n}",
        "code_after_change": "static int dmirror_migrate_to_system(struct dmirror *dmirror,\n\t\t\t\t     struct hmm_dmirror_cmd *cmd)\n{\n\tunsigned long start, end, addr;\n\tunsigned long size = cmd->npages << PAGE_SHIFT;\n\tstruct mm_struct *mm = dmirror->notifier.mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long src_pfns[64] = { 0 };\n\tunsigned long dst_pfns[64] = { 0 };\n\tstruct migrate_vma args = { 0 };\n\tunsigned long next;\n\tint ret;\n\n\tstart = cmd->addr;\n\tend = start + size;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\t/* Since the mm is for the mirrored process, get a reference first. */\n\tif (!mmget_not_zero(mm))\n\t\treturn -EINVAL;\n\n\tcmd->cpages = 0;\n\tmmap_read_lock(mm);\n\tfor (addr = start; addr < end; addr = next) {\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (!vma || !(vma->vm_flags & VM_READ)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tnext = min(end, addr + (ARRAY_SIZE(src_pfns) << PAGE_SHIFT));\n\t\tif (next > vma->vm_end)\n\t\t\tnext = vma->vm_end;\n\n\t\targs.vma = vma;\n\t\targs.src = src_pfns;\n\t\targs.dst = dst_pfns;\n\t\targs.start = addr;\n\t\targs.end = next;\n\t\targs.pgmap_owner = dmirror->mdevice;\n\t\targs.flags = dmirror_select_device(dmirror);\n\n\t\tret = migrate_vma_setup(&args);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tpr_debug(\"Migrating from device mem to sys mem\\n\");\n\t\tdmirror_devmem_fault_alloc_and_copy(&args, dmirror);\n\n\t\tmigrate_vma_pages(&args);\n\t\tcmd->cpages += dmirror_successful_migrated_pages(&args);\n\t\tmigrate_vma_finalize(&args);\n\t}\nout:\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \tstruct vm_area_struct *vma;\n \tunsigned long src_pfns[64] = { 0 };\n \tunsigned long dst_pfns[64] = { 0 };\n-\tstruct migrate_vma args;\n+\tstruct migrate_vma args = { 0 };\n \tunsigned long next;\n \tint ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct migrate_vma args = { 0 };"
            ],
            "deleted": [
                "\tstruct migrate_vma args;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3616
    },
    {
        "cve_id": "CVE-2020-25669",
        "code_before_change": "static void sunkbd_enable(struct sunkbd *sunkbd, bool enable)\n{\n\tserio_pause_rx(sunkbd->serio);\n\tsunkbd->enabled = enable;\n\tserio_continue_rx(sunkbd->serio);\n}",
        "code_after_change": "static void sunkbd_enable(struct sunkbd *sunkbd, bool enable)\n{\n\tserio_pause_rx(sunkbd->serio);\n\tsunkbd->enabled = enable;\n\tserio_continue_rx(sunkbd->serio);\n\n\tif (!enable) {\n\t\twake_up_interruptible(&sunkbd->wait);\n\t\tcancel_work_sync(&sunkbd->tq);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,4 +3,9 @@\n \tserio_pause_rx(sunkbd->serio);\n \tsunkbd->enabled = enable;\n \tserio_continue_rx(sunkbd->serio);\n+\n+\tif (!enable) {\n+\t\twake_up_interruptible(&sunkbd->wait);\n+\t\tcancel_work_sync(&sunkbd->tq);\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (!enable) {",
                "\t\twake_up_interruptible(&sunkbd->wait);",
                "\t\tcancel_work_sync(&sunkbd->tq);",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in the Linux Kernel where the function sunkbd_reinit having been scheduled by sunkbd_interrupt before sunkbd being freed. Though the dangling pointer is set to NULL in sunkbd_disconnect, there is still an alias in sunkbd_reinit causing Use After Free.",
        "id": 2597
    },
    {
        "cve_id": "CVE-2023-5197",
        "code_before_change": "static int nft_flush_table(struct nft_ctx *ctx)\n{\n\tstruct nft_flowtable *flowtable, *nft;\n\tstruct nft_chain *chain, *nc;\n\tstruct nft_object *obj, *ne;\n\tstruct nft_set *set, *ns;\n\tint err;\n\n\tlist_for_each_entry(chain, &ctx->table->chains, list) {\n\t\tif (!nft_is_active_next(ctx->net, chain))\n\t\t\tcontinue;\n\n\t\tif (nft_chain_is_bound(chain))\n\t\t\tcontinue;\n\n\t\tctx->chain = chain;\n\n\t\terr = nft_delrule_by_chain(ctx);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(set, ns, &ctx->table->sets, list) {\n\t\tif (!nft_is_active_next(ctx->net, set))\n\t\t\tcontinue;\n\n\t\tif (nft_set_is_anonymous(set) &&\n\t\t    !list_empty(&set->bindings))\n\t\t\tcontinue;\n\n\t\terr = nft_delset(ctx, set);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(flowtable, nft, &ctx->table->flowtables, list) {\n\t\tif (!nft_is_active_next(ctx->net, flowtable))\n\t\t\tcontinue;\n\n\t\terr = nft_delflowtable(ctx, flowtable);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(obj, ne, &ctx->table->objects, list) {\n\t\tif (!nft_is_active_next(ctx->net, obj))\n\t\t\tcontinue;\n\n\t\terr = nft_delobj(ctx, obj);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(chain, nc, &ctx->table->chains, list) {\n\t\tif (!nft_is_active_next(ctx->net, chain))\n\t\t\tcontinue;\n\n\t\tif (nft_chain_is_bound(chain))\n\t\t\tcontinue;\n\n\t\tctx->chain = chain;\n\n\t\terr = nft_delchain(ctx);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\terr = nft_deltable(ctx);\nout:\n\treturn err;\n}",
        "code_after_change": "static int nft_flush_table(struct nft_ctx *ctx)\n{\n\tstruct nft_flowtable *flowtable, *nft;\n\tstruct nft_chain *chain, *nc;\n\tstruct nft_object *obj, *ne;\n\tstruct nft_set *set, *ns;\n\tint err;\n\n\tlist_for_each_entry(chain, &ctx->table->chains, list) {\n\t\tif (!nft_is_active_next(ctx->net, chain))\n\t\t\tcontinue;\n\n\t\tif (nft_chain_binding(chain))\n\t\t\tcontinue;\n\n\t\tctx->chain = chain;\n\n\t\terr = nft_delrule_by_chain(ctx);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(set, ns, &ctx->table->sets, list) {\n\t\tif (!nft_is_active_next(ctx->net, set))\n\t\t\tcontinue;\n\n\t\tif (nft_set_is_anonymous(set) &&\n\t\t    !list_empty(&set->bindings))\n\t\t\tcontinue;\n\n\t\terr = nft_delset(ctx, set);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(flowtable, nft, &ctx->table->flowtables, list) {\n\t\tif (!nft_is_active_next(ctx->net, flowtable))\n\t\t\tcontinue;\n\n\t\terr = nft_delflowtable(ctx, flowtable);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(obj, ne, &ctx->table->objects, list) {\n\t\tif (!nft_is_active_next(ctx->net, obj))\n\t\t\tcontinue;\n\n\t\terr = nft_delobj(ctx, obj);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(chain, nc, &ctx->table->chains, list) {\n\t\tif (!nft_is_active_next(ctx->net, chain))\n\t\t\tcontinue;\n\n\t\tif (nft_chain_binding(chain))\n\t\t\tcontinue;\n\n\t\tctx->chain = chain;\n\n\t\terr = nft_delchain(ctx);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\terr = nft_deltable(ctx);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,7 @@\n \t\tif (!nft_is_active_next(ctx->net, chain))\n \t\t\tcontinue;\n \n-\t\tif (nft_chain_is_bound(chain))\n+\t\tif (nft_chain_binding(chain))\n \t\t\tcontinue;\n \n \t\tctx->chain = chain;\n@@ -55,7 +55,7 @@\n \t\tif (!nft_is_active_next(ctx->net, chain))\n \t\t\tcontinue;\n \n-\t\tif (nft_chain_is_bound(chain))\n+\t\tif (nft_chain_binding(chain))\n \t\t\tcontinue;\n \n \t\tctx->chain = chain;",
        "function_modified_lines": {
            "added": [
                "\t\tif (nft_chain_binding(chain))",
                "\t\tif (nft_chain_binding(chain))"
            ],
            "deleted": [
                "\t\tif (nft_chain_is_bound(chain))",
                "\t\tif (nft_chain_is_bound(chain))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nAddition and removal of rules from chain bindings within the same transaction causes leads to use-after-free.\n\nWe recommend upgrading past commit f15f29fd4779be8a418b66e9d52979bb6d6c2325.\n\n",
        "id": 4262
    },
    {
        "cve_id": "CVE-2022-47946",
        "code_before_change": "static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\n\tif (WARN_ON_ONCE((ctx->flags & IORING_SETUP_SQPOLL) && !ctx->sqo_dead))\n\t\tctx->sqo_dead = 1;\n\n\t/* if force is set, the ring is going away. always drop after that */\n\tctx->cq_overflow_flushed = 1;\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true, NULL, NULL);\n\tidr_for_each(&ctx->personality_idr, io_remove_personalities, ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tio_kill_timeouts(ctx, NULL, NULL);\n\tio_poll_remove_all(ctx, NULL, NULL);\n\n\t/* if we failed setting up the ctx, we might not have any rings */\n\tio_iopoll_try_reap_events(ctx);\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}",
        "code_after_change": "static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\t/* if force is set, the ring is going away. always drop after that */\n\tctx->cq_overflow_flushed = 1;\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true, NULL, NULL);\n\tidr_for_each(&ctx->personality_idr, io_remove_personalities, ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tio_kill_timeouts(ctx, NULL, NULL);\n\tio_poll_remove_all(ctx, NULL, NULL);\n\n\t/* if we failed setting up the ctx, we might not have any rings */\n\tio_iopoll_try_reap_events(ctx);\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,10 +2,6 @@\n {\n \tmutex_lock(&ctx->uring_lock);\n \tpercpu_ref_kill(&ctx->refs);\n-\n-\tif (WARN_ON_ONCE((ctx->flags & IORING_SETUP_SQPOLL) && !ctx->sqo_dead))\n-\t\tctx->sqo_dead = 1;\n-\n \t/* if force is set, the ring is going away. always drop after that */\n \tctx->cq_overflow_flushed = 1;\n \tif (ctx->rings)",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tif (WARN_ON_ONCE((ctx->flags & IORING_SETUP_SQPOLL) && !ctx->sqo_dead))",
                "\t\tctx->sqo_dead = 1;",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 5.10.x before 5.10.155. A use-after-free in io_sqpoll_wait_sq in fs/io_uring.c allows an attacker to crash the kernel, resulting in denial of service. finish_wait can be skipped. An attack can occur in some situations by forking a process and then quickly terminating it. NOTE: later kernel versions, such as the 5.15 longterm series, substantially changed the implementation of io_sqpoll_wait_sq.",
        "id": 3779
    },
    {
        "cve_id": "CVE-2020-7053",
        "code_before_change": "void i915_gem_context_close(struct drm_file *file)\n{\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\n\tlockdep_assert_held(&file_priv->dev_priv->drm.struct_mutex);\n\n\tidr_for_each(&file_priv->context_idr, context_idr_cleanup, NULL);\n\tidr_destroy(&file_priv->context_idr);\n}",
        "code_after_change": "void i915_gem_context_close(struct drm_file *file)\n{\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\n\tlockdep_assert_held(&file_priv->dev_priv->drm.struct_mutex);\n\n\tidr_for_each(&file_priv->context_idr, context_idr_cleanup, NULL);\n\tidr_destroy(&file_priv->context_idr);\n\tmutex_destroy(&file_priv->context_idr_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,4 +6,5 @@\n \n \tidr_for_each(&file_priv->context_idr, context_idr_cleanup, NULL);\n \tidr_destroy(&file_priv->context_idr);\n+\tmutex_destroy(&file_priv->context_idr_lock);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_destroy(&file_priv->context_idr_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 4.14 longterm through 4.14.165 and 4.19 longterm through 4.19.96 (and 5.x before 5.2), there is a use-after-free (write) in the i915_ppgtt_close function in drivers/gpu/drm/i915/i915_gem_gtt.c, aka CID-7dc40713618c. This is related to i915_gem_context_destroy_ioctl in drivers/gpu/drm/i915/i915_gem_context.c.",
        "id": 2801
    },
    {
        "cve_id": "CVE-2014-0203",
        "code_before_change": "static __always_inline int __do_follow_link(struct path *path, struct nameidata *nd)\n{\n\tint error;\n\tvoid *cookie;\n\tstruct dentry *dentry = path->dentry;\n\n\ttouch_atime(path->mnt, dentry);\n\tnd_set_link(nd, NULL);\n\n\tif (path->mnt != nd->path.mnt) {\n\t\tpath_to_nameidata(path, nd);\n\t\tdget(dentry);\n\t}\n\tmntget(path->mnt);\n\tcookie = dentry->d_inode->i_op->follow_link(dentry, nd);\n\terror = PTR_ERR(cookie);\n\tif (!IS_ERR(cookie)) {\n\t\tchar *s = nd_get_link(nd);\n\t\terror = 0;\n\t\tif (s)\n\t\t\terror = __vfs_follow_link(nd, s);\n\t\telse if (nd->last_type == LAST_BIND) {\n\t\t\terror = force_reval_path(&nd->path, nd);\n\t\t\tif (error)\n\t\t\t\tpath_put(&nd->path);\n\t\t}\n\t\tif (dentry->d_inode->i_op->put_link)\n\t\t\tdentry->d_inode->i_op->put_link(dentry, nd, cookie);\n\t}\n\treturn error;\n}",
        "code_after_change": "static __always_inline int __do_follow_link(struct path *path, struct nameidata *nd)\n{\n\tint error;\n\tvoid *cookie;\n\tstruct dentry *dentry = path->dentry;\n\n\ttouch_atime(path->mnt, dentry);\n\tnd_set_link(nd, NULL);\n\n\tif (path->mnt != nd->path.mnt) {\n\t\tpath_to_nameidata(path, nd);\n\t\tdget(dentry);\n\t}\n\tmntget(path->mnt);\n\tnd->last_type = LAST_BIND;\n\tcookie = dentry->d_inode->i_op->follow_link(dentry, nd);\n\terror = PTR_ERR(cookie);\n\tif (!IS_ERR(cookie)) {\n\t\tchar *s = nd_get_link(nd);\n\t\terror = 0;\n\t\tif (s)\n\t\t\terror = __vfs_follow_link(nd, s);\n\t\telse if (nd->last_type == LAST_BIND) {\n\t\t\terror = force_reval_path(&nd->path, nd);\n\t\t\tif (error)\n\t\t\t\tpath_put(&nd->path);\n\t\t}\n\t\tif (dentry->d_inode->i_op->put_link)\n\t\t\tdentry->d_inode->i_op->put_link(dentry, nd, cookie);\n\t}\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,7 @@\n \t\tdget(dentry);\n \t}\n \tmntget(path->mnt);\n+\tnd->last_type = LAST_BIND;\n \tcookie = dentry->d_inode->i_op->follow_link(dentry, nd);\n \terror = PTR_ERR(cookie);\n \tif (!IS_ERR(cookie)) {",
        "function_modified_lines": {
            "added": [
                "\tnd->last_type = LAST_BIND;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The __do_follow_link function in fs/namei.c in the Linux kernel before 2.6.33 does not properly handle the last pathname component during use of certain filesystems, which allows local users to cause a denial of service (incorrect free operations and system crash) via an open system call.",
        "id": 462
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static void vmw_bo_release(struct vmw_bo *vbo)\n{\n\tvmw_bo_unmap(vbo);\n\tdrm_gem_object_release(&vbo->tbo.base);\n}",
        "code_after_change": "static void vmw_bo_release(struct vmw_bo *vbo)\n{\n\tWARN_ON(vbo->tbo.base.funcs &&\n\t\tkref_read(&vbo->tbo.base.refcount) != 0);\n\tvmw_bo_unmap(vbo);\n\tdrm_gem_object_release(&vbo->tbo.base);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,7 @@\n static void vmw_bo_release(struct vmw_bo *vbo)\n {\n+\tWARN_ON(vbo->tbo.base.funcs &&\n+\t\tkref_read(&vbo->tbo.base.refcount) != 0);\n \tvmw_bo_unmap(vbo);\n \tdrm_gem_object_release(&vbo->tbo.base);\n }",
        "function_modified_lines": {
            "added": [
                "\tWARN_ON(vbo->tbo.base.funcs &&",
                "\t\tkref_read(&vbo->tbo.base.refcount) != 0);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4267
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static inline int l2cap_le_credits(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_le_credits *pkt;\n\tstruct l2cap_chan *chan;\n\tu16 cid, credits, max_credits;\n\n\tif (cmd_len != sizeof(*pkt))\n\t\treturn -EPROTO;\n\n\tpkt = (struct l2cap_le_credits *) data;\n\tcid\t= __le16_to_cpu(pkt->cid);\n\tcredits\t= __le16_to_cpu(pkt->credits);\n\n\tBT_DBG(\"cid 0x%4.4x credits 0x%4.4x\", cid, credits);\n\n\tchan = l2cap_get_chan_by_dcid(conn, cid);\n\tif (!chan)\n\t\treturn -EBADSLT;\n\n\tmax_credits = LE_FLOWCTL_MAX_CREDITS - chan->tx_credits;\n\tif (credits > max_credits) {\n\t\tBT_ERR(\"LE credits overflow\");\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tl2cap_chan_unlock(chan);\n\n\t\t/* Return 0 so that we don't trigger an unnecessary\n\t\t * command reject packet.\n\t\t */\n\t\treturn 0;\n\t}\n\n\tchan->tx_credits += credits;\n\n\t/* Resume sending */\n\tl2cap_le_flowctl_send(chan);\n\n\tif (chan->tx_credits)\n\t\tchan->ops->resume(chan);\n\n\tl2cap_chan_unlock(chan);\n\n\treturn 0;\n}",
        "code_after_change": "static inline int l2cap_le_credits(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_le_credits *pkt;\n\tstruct l2cap_chan *chan;\n\tu16 cid, credits, max_credits;\n\n\tif (cmd_len != sizeof(*pkt))\n\t\treturn -EPROTO;\n\n\tpkt = (struct l2cap_le_credits *) data;\n\tcid\t= __le16_to_cpu(pkt->cid);\n\tcredits\t= __le16_to_cpu(pkt->credits);\n\n\tBT_DBG(\"cid 0x%4.4x credits 0x%4.4x\", cid, credits);\n\n\tchan = l2cap_get_chan_by_dcid(conn, cid);\n\tif (!chan)\n\t\treturn -EBADSLT;\n\n\tmax_credits = LE_FLOWCTL_MAX_CREDITS - chan->tx_credits;\n\tif (credits > max_credits) {\n\t\tBT_ERR(\"LE credits overflow\");\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\n\t\t/* Return 0 so that we don't trigger an unnecessary\n\t\t * command reject packet.\n\t\t */\n\t\tgoto unlock;\n\t}\n\n\tchan->tx_credits += credits;\n\n\t/* Resume sending */\n\tl2cap_le_flowctl_send(chan);\n\n\tif (chan->tx_credits)\n\t\tchan->ops->resume(chan);\n\nunlock:\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,12 +23,11 @@\n \tif (credits > max_credits) {\n \t\tBT_ERR(\"LE credits overflow\");\n \t\tl2cap_send_disconn_req(chan, ECONNRESET);\n-\t\tl2cap_chan_unlock(chan);\n \n \t\t/* Return 0 so that we don't trigger an unnecessary\n \t\t * command reject packet.\n \t\t */\n-\t\treturn 0;\n+\t\tgoto unlock;\n \t}\n \n \tchan->tx_credits += credits;\n@@ -39,7 +38,9 @@\n \tif (chan->tx_credits)\n \t\tchan->ops->resume(chan);\n \n+unlock:\n \tl2cap_chan_unlock(chan);\n+\tl2cap_chan_put(chan);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tgoto unlock;",
                "unlock:",
                "\tl2cap_chan_put(chan);"
            ],
            "deleted": [
                "\t\tl2cap_chan_unlock(chan);",
                "\t\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3385
    },
    {
        "cve_id": "CVE-2018-18559",
        "code_before_change": "static int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n\t\t\t  __be16 proto)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct net_device *dev_curr;\n\t__be16 proto_curr;\n\tbool need_rehook;\n\tstruct net_device *dev = NULL;\n\tint ret = 0;\n\tbool unlisted = false;\n\n\tlock_sock(sk);\n\tspin_lock(&po->bind_lock);\n\trcu_read_lock();\n\n\tif (po->fanout) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (name) {\n\t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (ifindex) {\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), ifindex);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (dev)\n\t\tdev_hold(dev);\n\n\tproto_curr = po->prot_hook.type;\n\tdev_curr = po->prot_hook.dev;\n\n\tneed_rehook = proto_curr != proto || dev_curr != dev;\n\n\tif (need_rehook) {\n\t\tif (po->running) {\n\t\t\trcu_read_unlock();\n\t\t\t__unregister_prot_hook(sk, true);\n\t\t\trcu_read_lock();\n\t\t\tdev_curr = po->prot_hook.dev;\n\t\t\tif (dev)\n\t\t\t\tunlisted = !dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t\t\t dev->ifindex);\n\t\t}\n\n\t\tpo->num = proto;\n\t\tpo->prot_hook.type = proto;\n\n\t\tif (unlikely(unlisted)) {\n\t\t\tdev_put(dev);\n\t\t\tpo->prot_hook.dev = NULL;\n\t\t\tpo->ifindex = -1;\n\t\t\tpacket_cached_dev_reset(po);\n\t\t} else {\n\t\t\tpo->prot_hook.dev = dev;\n\t\t\tpo->ifindex = dev ? dev->ifindex : 0;\n\t\t\tpacket_cached_dev_assign(po, dev);\n\t\t}\n\t}\n\tif (dev_curr)\n\t\tdev_put(dev_curr);\n\n\tif (proto == 0 || !need_rehook)\n\t\tgoto out_unlock;\n\n\tif (!unlisted && (!dev || (dev->flags & IFF_UP))) {\n\t\tregister_prot_hook(sk);\n\t} else {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\tspin_unlock(&po->bind_lock);\n\trelease_sock(sk);\n\treturn ret;\n}",
        "code_after_change": "static int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n\t\t\t  __be16 proto)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct net_device *dev_curr;\n\t__be16 proto_curr;\n\tbool need_rehook;\n\tstruct net_device *dev = NULL;\n\tint ret = 0;\n\tbool unlisted = false;\n\n\tlock_sock(sk);\n\tspin_lock(&po->bind_lock);\n\trcu_read_lock();\n\n\tif (po->fanout) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (name) {\n\t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (ifindex) {\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), ifindex);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (dev)\n\t\tdev_hold(dev);\n\n\tproto_curr = po->prot_hook.type;\n\tdev_curr = po->prot_hook.dev;\n\n\tneed_rehook = proto_curr != proto || dev_curr != dev;\n\n\tif (need_rehook) {\n\t\tif (po->running) {\n\t\t\trcu_read_unlock();\n\t\t\t/* prevents packet_notifier() from calling\n\t\t\t * register_prot_hook()\n\t\t\t */\n\t\t\tpo->num = 0;\n\t\t\t__unregister_prot_hook(sk, true);\n\t\t\trcu_read_lock();\n\t\t\tdev_curr = po->prot_hook.dev;\n\t\t\tif (dev)\n\t\t\t\tunlisted = !dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t\t\t dev->ifindex);\n\t\t}\n\n\t\tBUG_ON(po->running);\n\t\tpo->num = proto;\n\t\tpo->prot_hook.type = proto;\n\n\t\tif (unlikely(unlisted)) {\n\t\t\tdev_put(dev);\n\t\t\tpo->prot_hook.dev = NULL;\n\t\t\tpo->ifindex = -1;\n\t\t\tpacket_cached_dev_reset(po);\n\t\t} else {\n\t\t\tpo->prot_hook.dev = dev;\n\t\t\tpo->ifindex = dev ? dev->ifindex : 0;\n\t\t\tpacket_cached_dev_assign(po, dev);\n\t\t}\n\t}\n\tif (dev_curr)\n\t\tdev_put(dev_curr);\n\n\tif (proto == 0 || !need_rehook)\n\t\tgoto out_unlock;\n\n\tif (!unlisted && (!dev || (dev->flags & IFF_UP))) {\n\t\tregister_prot_hook(sk);\n\t} else {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\tspin_unlock(&po->bind_lock);\n\trelease_sock(sk);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,6 +43,10 @@\n \tif (need_rehook) {\n \t\tif (po->running) {\n \t\t\trcu_read_unlock();\n+\t\t\t/* prevents packet_notifier() from calling\n+\t\t\t * register_prot_hook()\n+\t\t\t */\n+\t\t\tpo->num = 0;\n \t\t\t__unregister_prot_hook(sk, true);\n \t\t\trcu_read_lock();\n \t\t\tdev_curr = po->prot_hook.dev;\n@@ -51,6 +55,7 @@\n \t\t\t\t\t\t\t\t dev->ifindex);\n \t\t}\n \n+\t\tBUG_ON(po->running);\n \t\tpo->num = proto;\n \t\tpo->prot_hook.type = proto;\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\t/* prevents packet_notifier() from calling",
                "\t\t\t * register_prot_hook()",
                "\t\t\t */",
                "\t\t\tpo->num = 0;",
                "\t\tBUG_ON(po->running);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel through 4.19, a use-after-free can occur due to a race condition between fanout_add from setsockopt and bind on an AF_PACKET socket. This issue exists because of the 15fe076edea787807a7cdc168df832544b58eba6 incomplete fix for a race condition. The code mishandles a certain multithreaded case involving a packet_do_bind unregister action followed by a packet_notifier register action. Later, packet_release operates on only one of the two applicable linked lists. The attacker can achieve Program Counter control.",
        "id": 1739
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_resource_buf_alloc(struct vmw_resource *res,\n\t\t\t\t  bool interruptible)\n{\n\tunsigned long size = PFN_ALIGN(res->guest_memory_size);\n\tstruct vmw_bo *gbo;\n\tstruct vmw_bo_params bo_params = {\n\t\t.domain = res->func->domain,\n\t\t.busy_domain = res->func->busy_domain,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = res->guest_memory_size,\n\t\t.pin = false\n\t};\n\tint ret;\n\n\tif (likely(res->guest_memory_bo)) {\n\t\tBUG_ON(res->guest_memory_bo->tbo.base.size < size);\n\t\treturn 0;\n\t}\n\n\tret = vmw_bo_create(res->dev_priv, &bo_params, &gbo);\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_bo;\n\n\tres->guest_memory_bo = gbo;\n\nout_no_bo:\n\treturn ret;\n}",
        "code_after_change": "static int vmw_resource_buf_alloc(struct vmw_resource *res,\n\t\t\t\t  bool interruptible)\n{\n\tunsigned long size = PFN_ALIGN(res->guest_memory_size);\n\tstruct vmw_bo *gbo;\n\tstruct vmw_bo_params bo_params = {\n\t\t.domain = res->func->domain,\n\t\t.busy_domain = res->func->busy_domain,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = res->guest_memory_size,\n\t\t.pin = false\n\t};\n\tint ret;\n\n\tif (likely(res->guest_memory_bo)) {\n\t\tBUG_ON(res->guest_memory_bo->tbo.base.size < size);\n\t\treturn 0;\n\t}\n\n\tret = vmw_gem_object_create(res->dev_priv, &bo_params, &gbo);\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_bo;\n\n\tres->guest_memory_bo = gbo;\n\nout_no_bo:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,7 +17,7 @@\n \t\treturn 0;\n \t}\n \n-\tret = vmw_bo_create(res->dev_priv, &bo_params, &gbo);\n+\tret = vmw_gem_object_create(res->dev_priv, &bo_params, &gbo);\n \tif (unlikely(ret != 0))\n \t\tgoto out_no_bo;\n ",
        "function_modified_lines": {
            "added": [
                "\tret = vmw_gem_object_create(res->dev_priv, &bo_params, &gbo);"
            ],
            "deleted": [
                "\tret = vmw_bo_create(res->dev_priv, &bo_params, &gbo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4281
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "static int llcp_sock_connect(struct socket *sock, struct sockaddr *_addr,\n\t\t\t     int len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct sockaddr_nfc_llcp *addr = (struct sockaddr_nfc_llcp *)_addr;\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tint ret = 0;\n\n\tpr_debug(\"sock %p sk %p flags 0x%x\\n\", sock, sk, flags);\n\n\tif (!addr || len < sizeof(*addr) || addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\n\tif (addr->service_name_len == 0 && addr->dsap == 0)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"addr dev_idx=%u target_idx=%u protocol=%u\\n\", addr->dev_idx,\n\t\t addr->target_idx, addr->nfc_protocol);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == LLCP_CONNECTED) {\n\t\tret = -EISCONN;\n\t\tgoto error;\n\t}\n\tif (sk->sk_state == LLCP_CONNECTING) {\n\t\tret = -EINPROGRESS;\n\t\tgoto error;\n\t}\n\n\tdev = nfc_get_device(addr->dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\n\tdevice_lock(&dev->dev);\n\tif (dev->dep_link_up == false) {\n\t\tret = -ENOLINK;\n\t\tdevice_unlock(&dev->dev);\n\t\tgoto put_dev;\n\t}\n\tdevice_unlock(&dev->dev);\n\n\tif (local->rf_mode == NFC_RF_INITIATOR &&\n\t    addr->target_idx != local->target_idx) {\n\t\tret = -ENOLINK;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = nfc_llcp_local_get(local);\n\tllcp_sock->ssap = nfc_llcp_get_local_ssap(local);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_put_local;\n\t}\n\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;\n\n\tif (addr->service_name_len == 0)\n\t\tllcp_sock->dsap = addr->dsap;\n\telse\n\t\tllcp_sock->dsap = LLCP_SAP_SDP;\n\tllcp_sock->nfc_protocol = addr->nfc_protocol;\n\tllcp_sock->service_name_len = min_t(unsigned int,\n\t\t\t\t\t    addr->service_name_len,\n\t\t\t\t\t    NFC_LLCP_MAX_SERVICE_NAME);\n\tllcp_sock->service_name = kmemdup(addr->service_name,\n\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_release;\n\t}\n\n\tnfc_llcp_sock_link(&local->connecting_sockets, sk);\n\n\tret = nfc_llcp_send_connect(llcp_sock);\n\tif (ret)\n\t\tgoto sock_unlink;\n\n\tsk->sk_state = LLCP_CONNECTING;\n\n\tret = sock_wait_state(sk, LLCP_CONNECTED,\n\t\t\t      sock_sndtimeo(sk, flags & O_NONBLOCK));\n\tif (ret && ret != -EINPROGRESS)\n\t\tgoto sock_unlink;\n\n\trelease_sock(sk);\n\n\treturn ret;\n\nsock_unlink:\n\tnfc_llcp_sock_unlink(&local->connecting_sockets, sk);\n\tkfree(llcp_sock->service_name);\n\tllcp_sock->service_name = NULL;\n\nsock_llcp_release:\n\tnfc_llcp_put_ssap(local, llcp_sock->ssap);\n\nsock_llcp_put_local:\n\tnfc_llcp_local_put(llcp_sock->local);\n\tllcp_sock->local = NULL;\n\tllcp_sock->dev = NULL;\n\nput_dev:\n\tnfc_put_device(dev);\n\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "code_after_change": "static int llcp_sock_connect(struct socket *sock, struct sockaddr *_addr,\n\t\t\t     int len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct nfc_llcp_sock *llcp_sock = nfc_llcp_sock(sk);\n\tstruct sockaddr_nfc_llcp *addr = (struct sockaddr_nfc_llcp *)_addr;\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tint ret = 0;\n\n\tpr_debug(\"sock %p sk %p flags 0x%x\\n\", sock, sk, flags);\n\n\tif (!addr || len < sizeof(*addr) || addr->sa_family != AF_NFC)\n\t\treturn -EINVAL;\n\n\tif (addr->service_name_len == 0 && addr->dsap == 0)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"addr dev_idx=%u target_idx=%u protocol=%u\\n\", addr->dev_idx,\n\t\t addr->target_idx, addr->nfc_protocol);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == LLCP_CONNECTED) {\n\t\tret = -EISCONN;\n\t\tgoto error;\n\t}\n\tif (sk->sk_state == LLCP_CONNECTING) {\n\t\tret = -EINPROGRESS;\n\t\tgoto error;\n\t}\n\n\tdev = nfc_get_device(addr->dev_idx);\n\tif (dev == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tret = -ENODEV;\n\t\tgoto put_dev;\n\t}\n\n\tdevice_lock(&dev->dev);\n\tif (dev->dep_link_up == false) {\n\t\tret = -ENOLINK;\n\t\tdevice_unlock(&dev->dev);\n\t\tgoto sock_llcp_put_local;\n\t}\n\tdevice_unlock(&dev->dev);\n\n\tif (local->rf_mode == NFC_RF_INITIATOR &&\n\t    addr->target_idx != local->target_idx) {\n\t\tret = -ENOLINK;\n\t\tgoto sock_llcp_put_local;\n\t}\n\n\tllcp_sock->dev = dev;\n\tllcp_sock->local = local;\n\tllcp_sock->ssap = nfc_llcp_get_local_ssap(local);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_nullify;\n\t}\n\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;\n\n\tif (addr->service_name_len == 0)\n\t\tllcp_sock->dsap = addr->dsap;\n\telse\n\t\tllcp_sock->dsap = LLCP_SAP_SDP;\n\tllcp_sock->nfc_protocol = addr->nfc_protocol;\n\tllcp_sock->service_name_len = min_t(unsigned int,\n\t\t\t\t\t    addr->service_name_len,\n\t\t\t\t\t    NFC_LLCP_MAX_SERVICE_NAME);\n\tllcp_sock->service_name = kmemdup(addr->service_name,\n\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tret = -ENOMEM;\n\t\tgoto sock_llcp_release;\n\t}\n\n\tnfc_llcp_sock_link(&local->connecting_sockets, sk);\n\n\tret = nfc_llcp_send_connect(llcp_sock);\n\tif (ret)\n\t\tgoto sock_unlink;\n\n\tsk->sk_state = LLCP_CONNECTING;\n\n\tret = sock_wait_state(sk, LLCP_CONNECTED,\n\t\t\t      sock_sndtimeo(sk, flags & O_NONBLOCK));\n\tif (ret && ret != -EINPROGRESS)\n\t\tgoto sock_unlink;\n\n\trelease_sock(sk);\n\n\treturn ret;\n\nsock_unlink:\n\tnfc_llcp_sock_unlink(&local->connecting_sockets, sk);\n\tkfree(llcp_sock->service_name);\n\tllcp_sock->service_name = NULL;\n\nsock_llcp_release:\n\tnfc_llcp_put_ssap(local, llcp_sock->ssap);\n\nsock_llcp_nullify:\n\tllcp_sock->local = NULL;\n\tllcp_sock->dev = NULL;\n\nsock_llcp_put_local:\n\tnfc_llcp_local_put(local);\n\nput_dev:\n\tnfc_put_device(dev);\n\nerror:\n\trelease_sock(sk);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -46,22 +46,22 @@\n \tif (dev->dep_link_up == false) {\n \t\tret = -ENOLINK;\n \t\tdevice_unlock(&dev->dev);\n-\t\tgoto put_dev;\n+\t\tgoto sock_llcp_put_local;\n \t}\n \tdevice_unlock(&dev->dev);\n \n \tif (local->rf_mode == NFC_RF_INITIATOR &&\n \t    addr->target_idx != local->target_idx) {\n \t\tret = -ENOLINK;\n-\t\tgoto put_dev;\n+\t\tgoto sock_llcp_put_local;\n \t}\n \n \tllcp_sock->dev = dev;\n-\tllcp_sock->local = nfc_llcp_local_get(local);\n+\tllcp_sock->local = local;\n \tllcp_sock->ssap = nfc_llcp_get_local_ssap(local);\n \tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n \t\tret = -ENOMEM;\n-\t\tgoto sock_llcp_put_local;\n+\t\tgoto sock_llcp_nullify;\n \t}\n \n \tllcp_sock->reserved_ssap = llcp_sock->ssap;\n@@ -107,10 +107,12 @@\n sock_llcp_release:\n \tnfc_llcp_put_ssap(local, llcp_sock->ssap);\n \n-sock_llcp_put_local:\n-\tnfc_llcp_local_put(llcp_sock->local);\n+sock_llcp_nullify:\n \tllcp_sock->local = NULL;\n \tllcp_sock->dev = NULL;\n+\n+sock_llcp_put_local:\n+\tnfc_llcp_local_put(local);\n \n put_dev:\n \tnfc_put_device(dev);",
        "function_modified_lines": {
            "added": [
                "\t\tgoto sock_llcp_put_local;",
                "\t\tgoto sock_llcp_put_local;",
                "\tllcp_sock->local = local;",
                "\t\tgoto sock_llcp_nullify;",
                "sock_llcp_nullify:",
                "",
                "sock_llcp_put_local:",
                "\tnfc_llcp_local_put(local);"
            ],
            "deleted": [
                "\t\tgoto put_dev;",
                "\t\tgoto put_dev;",
                "\tllcp_sock->local = nfc_llcp_local_get(local);",
                "\t\tgoto sock_llcp_put_local;",
                "sock_llcp_put_local:",
                "\tnfc_llcp_local_put(llcp_sock->local);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4153
    },
    {
        "cve_id": "CVE-2020-36387",
        "code_before_change": "static void io_req_task_queue(struct io_kiocb *req)\n{\n\tint ret;\n\n\tinit_task_work(&req->task_work, io_req_task_submit);\n\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\tinit_task_work(&req->task_work, io_req_task_cancel);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, 0);\n\t\twake_up_process(tsk);\n\t}\n}",
        "code_after_change": "static void io_req_task_queue(struct io_kiocb *req)\n{\n\tint ret;\n\n\tinit_task_work(&req->task_work, io_req_task_submit);\n\tpercpu_ref_get(&req->ctx->refs);\n\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\tinit_task_work(&req->task_work, io_req_task_cancel);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, 0);\n\t\twake_up_process(tsk);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n \tint ret;\n \n \tinit_task_work(&req->task_work, io_req_task_submit);\n+\tpercpu_ref_get(&req->ctx->refs);\n \n \tret = io_req_task_work_add(req, &req->task_work);\n \tif (unlikely(ret)) {",
        "function_modified_lines": {
            "added": [
                "\tpercpu_ref_get(&req->ctx->refs);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.8.2. fs/io_uring.c has a use-after-free related to io_async_task_func and ctx reference holding, aka CID-6d816e088c35.",
        "id": 2759
    },
    {
        "cve_id": "CVE-2023-1872",
        "code_before_change": "static int io_splice(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tloff_t *poff_in, *poff_out;\n\tstruct file *in;\n\tlong ret = 0;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tin = io_file_get(req->ctx, req, sp->splice_fd_in,\n\t\t\t\t  (sp->flags & SPLICE_F_FD_IN_FIXED));\n\tif (!in) {\n\t\tret = -EBADF;\n\t\tgoto done;\n\t}\n\n\tpoff_in = (sp->off_in == -1) ? NULL : &sp->off_in;\n\tpoff_out = (sp->off_out == -1) ? NULL : &sp->off_out;\n\n\tif (sp->len)\n\t\tret = do_splice(in, poff_in, out, poff_out, sp->len, flags);\n\n\tif (!(sp->flags & SPLICE_F_FD_IN_FIXED))\n\t\tio_put_file(in);\ndone:\n\tif (ret != sp->len)\n\t\treq_set_fail(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}",
        "code_after_change": "static int io_splice(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tloff_t *poff_in, *poff_out;\n\tstruct file *in;\n\tlong ret = 0;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tif (sp->flags & SPLICE_F_FD_IN_FIXED)\n\t\tin = io_file_get_fixed(req, sp->splice_fd_in, IO_URING_F_UNLOCKED);\n\telse\n\t\tin = io_file_get_normal(req, sp->splice_fd_in);\n\tif (!in) {\n\t\tret = -EBADF;\n\t\tgoto done;\n\t}\n\n\tpoff_in = (sp->off_in == -1) ? NULL : &sp->off_in;\n\tpoff_out = (sp->off_out == -1) ? NULL : &sp->off_out;\n\n\tif (sp->len)\n\t\tret = do_splice(in, poff_in, out, poff_out, sp->len, flags);\n\n\tif (!(sp->flags & SPLICE_F_FD_IN_FIXED))\n\t\tio_put_file(in);\ndone:\n\tif (ret != sp->len)\n\t\treq_set_fail(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,8 +10,10 @@\n \tif (issue_flags & IO_URING_F_NONBLOCK)\n \t\treturn -EAGAIN;\n \n-\tin = io_file_get(req->ctx, req, sp->splice_fd_in,\n-\t\t\t\t  (sp->flags & SPLICE_F_FD_IN_FIXED));\n+\tif (sp->flags & SPLICE_F_FD_IN_FIXED)\n+\t\tin = io_file_get_fixed(req, sp->splice_fd_in, IO_URING_F_UNLOCKED);\n+\telse\n+\t\tin = io_file_get_normal(req, sp->splice_fd_in);\n \tif (!in) {\n \t\tret = -EBADF;\n \t\tgoto done;",
        "function_modified_lines": {
            "added": [
                "\tif (sp->flags & SPLICE_F_FD_IN_FIXED)",
                "\t\tin = io_file_get_fixed(req, sp->splice_fd_in, IO_URING_F_UNLOCKED);",
                "\telse",
                "\t\tin = io_file_get_normal(req, sp->splice_fd_in);"
            ],
            "deleted": [
                "\tin = io_file_get(req->ctx, req, sp->splice_fd_in,",
                "\t\t\t\t  (sp->flags & SPLICE_F_FD_IN_FIXED));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring system can be exploited to achieve local privilege escalation.\n\nThe io_file_get_fixed function lacks the presence of ctx->uring_lock which can lead to a Use-After-Free vulnerability due a race condition with fixed files getting unregistered.\n\nWe recommend upgrading past commit da24142b1ef9fd5d36b76e36bab328a5b27523e8.\n\n",
        "id": 3882
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "static void __xen_evtchn_do_upcall(void)\n{\n\tstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\n\tint cpu = smp_processor_id();\n\n\tdo {\n\t\tvcpu_info->evtchn_upcall_pending = 0;\n\n\t\txen_evtchn_handle_events(cpu);\n\n\t\tBUG_ON(!irqs_disabled());\n\n\t\tvirt_rmb(); /* Hypervisor can set upcall pending. */\n\n\t} while (vcpu_info->evtchn_upcall_pending);\n}",
        "code_after_change": "static void __xen_evtchn_do_upcall(void)\n{\n\tstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\n\tint cpu = smp_processor_id();\n\n\tread_lock(&evtchn_rwlock);\n\n\tdo {\n\t\tvcpu_info->evtchn_upcall_pending = 0;\n\n\t\txen_evtchn_handle_events(cpu);\n\n\t\tBUG_ON(!irqs_disabled());\n\n\t\tvirt_rmb(); /* Hypervisor can set upcall pending. */\n\n\t} while (vcpu_info->evtchn_upcall_pending);\n\n\tread_unlock(&evtchn_rwlock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,8 @@\n {\n \tstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\n \tint cpu = smp_processor_id();\n+\n+\tread_lock(&evtchn_rwlock);\n \n \tdo {\n \t\tvcpu_info->evtchn_upcall_pending = 0;\n@@ -13,4 +15,6 @@\n \t\tvirt_rmb(); /* Hypervisor can set upcall pending. */\n \n \t} while (vcpu_info->evtchn_upcall_pending);\n+\n+\tread_unlock(&evtchn_rwlock);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tread_lock(&evtchn_rwlock);",
                "",
                "\tread_unlock(&evtchn_rwlock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "id": 2625
    },
    {
        "cve_id": "CVE-2022-28893",
        "code_before_change": "static void xs_reset_transport(struct sock_xprt *transport)\n{\n\tstruct socket *sock = transport->sock;\n\tstruct sock *sk = transport->inet;\n\tstruct rpc_xprt *xprt = &transport->xprt;\n\tstruct file *filp = transport->file;\n\n\tif (sk == NULL)\n\t\treturn;\n\n\tif (atomic_read(&transport->xprt.swapper))\n\t\tsk_clear_memalloc(sk);\n\n\tkernel_sock_shutdown(sock, SHUT_RDWR);\n\n\tmutex_lock(&transport->recv_mutex);\n\tlock_sock(sk);\n\ttransport->inet = NULL;\n\ttransport->sock = NULL;\n\ttransport->file = NULL;\n\n\tsk->sk_user_data = NULL;\n\n\txs_restore_old_callbacks(transport, sk);\n\txprt_clear_connected(xprt);\n\txs_sock_reset_connection_flags(xprt);\n\t/* Reset stream record info */\n\txs_stream_reset_connect(transport);\n\trelease_sock(sk);\n\tmutex_unlock(&transport->recv_mutex);\n\n\ttrace_rpc_socket_close(xprt, sock);\n\tfput(filp);\n\n\txprt_disconnect_done(xprt);\n}",
        "code_after_change": "static void xs_reset_transport(struct sock_xprt *transport)\n{\n\tstruct socket *sock = transport->sock;\n\tstruct sock *sk = transport->inet;\n\tstruct rpc_xprt *xprt = &transport->xprt;\n\tstruct file *filp = transport->file;\n\n\tif (sk == NULL)\n\t\treturn;\n\t/*\n\t * Make sure we're calling this in a context from which it is safe\n\t * to call __fput_sync(). In practice that means rpciod and the\n\t * system workqueue.\n\t */\n\tif (!(current->flags & PF_WQ_WORKER)) {\n\t\tWARN_ON_ONCE(1);\n\t\tset_bit(XPRT_CLOSE_WAIT, &xprt->state);\n\t\treturn;\n\t}\n\n\tif (atomic_read(&transport->xprt.swapper))\n\t\tsk_clear_memalloc(sk);\n\n\tkernel_sock_shutdown(sock, SHUT_RDWR);\n\n\tmutex_lock(&transport->recv_mutex);\n\tlock_sock(sk);\n\ttransport->inet = NULL;\n\ttransport->sock = NULL;\n\ttransport->file = NULL;\n\n\tsk->sk_user_data = NULL;\n\n\txs_restore_old_callbacks(transport, sk);\n\txprt_clear_connected(xprt);\n\txs_sock_reset_connection_flags(xprt);\n\t/* Reset stream record info */\n\txs_stream_reset_connect(transport);\n\trelease_sock(sk);\n\tmutex_unlock(&transport->recv_mutex);\n\n\ttrace_rpc_socket_close(xprt, sock);\n\t__fput_sync(filp);\n\n\txprt_disconnect_done(xprt);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,16 @@\n \n \tif (sk == NULL)\n \t\treturn;\n+\t/*\n+\t * Make sure we're calling this in a context from which it is safe\n+\t * to call __fput_sync(). In practice that means rpciod and the\n+\t * system workqueue.\n+\t */\n+\tif (!(current->flags & PF_WQ_WORKER)) {\n+\t\tWARN_ON_ONCE(1);\n+\t\tset_bit(XPRT_CLOSE_WAIT, &xprt->state);\n+\t\treturn;\n+\t}\n \n \tif (atomic_read(&transport->xprt.swapper))\n \t\tsk_clear_memalloc(sk);\n@@ -30,7 +40,7 @@\n \tmutex_unlock(&transport->recv_mutex);\n \n \ttrace_rpc_socket_close(xprt, sock);\n-\tfput(filp);\n+\t__fput_sync(filp);\n \n \txprt_disconnect_done(xprt);\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * Make sure we're calling this in a context from which it is safe",
                "\t * to call __fput_sync(). In practice that means rpciod and the",
                "\t * system workqueue.",
                "\t */",
                "\tif (!(current->flags & PF_WQ_WORKER)) {",
                "\t\tWARN_ON_ONCE(1);",
                "\t\tset_bit(XPRT_CLOSE_WAIT, &xprt->state);",
                "\t\treturn;",
                "\t}",
                "\t__fput_sync(filp);"
            ],
            "deleted": [
                "\tfput(filp);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The SUNRPC subsystem in the Linux kernel through 5.17.2 can call xs_xprt_free before ensuring that sockets are in the intended state.",
        "id": 3509
    },
    {
        "cve_id": "CVE-2023-1990",
        "code_before_change": "void ndlc_remove(struct llt_ndlc *ndlc)\n{\n\tst_nci_remove(ndlc->ndev);\n\n\t/* cancel timers */\n\tdel_timer_sync(&ndlc->t1_timer);\n\tdel_timer_sync(&ndlc->t2_timer);\n\tndlc->t2_active = false;\n\tndlc->t1_active = false;\n\n\tskb_queue_purge(&ndlc->rcv_q);\n\tskb_queue_purge(&ndlc->send_q);\n}",
        "code_after_change": "void ndlc_remove(struct llt_ndlc *ndlc)\n{\n\t/* cancel timers */\n\tdel_timer_sync(&ndlc->t1_timer);\n\tdel_timer_sync(&ndlc->t2_timer);\n\tndlc->t2_active = false;\n\tndlc->t1_active = false;\n\t/* cancel work */\n\tcancel_work_sync(&ndlc->sm_work);\n\n\tst_nci_remove(ndlc->ndev);\n\n\tskb_queue_purge(&ndlc->rcv_q);\n\tskb_queue_purge(&ndlc->send_q);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,14 @@\n void ndlc_remove(struct llt_ndlc *ndlc)\n {\n-\tst_nci_remove(ndlc->ndev);\n-\n \t/* cancel timers */\n \tdel_timer_sync(&ndlc->t1_timer);\n \tdel_timer_sync(&ndlc->t2_timer);\n \tndlc->t2_active = false;\n \tndlc->t1_active = false;\n+\t/* cancel work */\n+\tcancel_work_sync(&ndlc->sm_work);\n+\n+\tst_nci_remove(ndlc->ndev);\n \n \tskb_queue_purge(&ndlc->rcv_q);\n \tskb_queue_purge(&ndlc->send_q);",
        "function_modified_lines": {
            "added": [
                "\t/* cancel work */",
                "\tcancel_work_sync(&ndlc->sm_work);",
                "",
                "\tst_nci_remove(ndlc->ndev);"
            ],
            "deleted": [
                "\tst_nci_remove(ndlc->ndev);",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in ndlc_remove in drivers/nfc/st-nci/ndlc.c in the Linux Kernel. This flaw could allow an attacker to crash the system due to a race problem.",
        "id": 3889
    },
    {
        "cve_id": "CVE-2021-3760",
        "code_before_change": "static void nci_core_conn_close_rsp_packet(struct nci_dev *ndev,\n\t\t\t\t\t   const struct sk_buff *skb)\n{\n\tstruct nci_conn_info *conn_info;\n\t__u8 status = skb->data[0];\n\n\tpr_debug(\"status 0x%x\\n\", status);\n\tif (status == NCI_STATUS_OK) {\n\t\tconn_info = nci_get_conn_info_by_conn_id(ndev,\n\t\t\t\t\t\t\t ndev->cur_conn_id);\n\t\tif (conn_info) {\n\t\t\tlist_del(&conn_info->list);\n\t\t\tdevm_kfree(&ndev->nfc_dev->dev, conn_info);\n\t\t}\n\t}\n\tnci_req_complete(ndev, status);\n}",
        "code_after_change": "static void nci_core_conn_close_rsp_packet(struct nci_dev *ndev,\n\t\t\t\t\t   const struct sk_buff *skb)\n{\n\tstruct nci_conn_info *conn_info;\n\t__u8 status = skb->data[0];\n\n\tpr_debug(\"status 0x%x\\n\", status);\n\tif (status == NCI_STATUS_OK) {\n\t\tconn_info = nci_get_conn_info_by_conn_id(ndev,\n\t\t\t\t\t\t\t ndev->cur_conn_id);\n\t\tif (conn_info) {\n\t\t\tlist_del(&conn_info->list);\n\t\t\tif (conn_info == ndev->rf_conn_info)\n\t\t\t\tndev->rf_conn_info = NULL;\n\t\t\tdevm_kfree(&ndev->nfc_dev->dev, conn_info);\n\t\t}\n\t}\n\tnci_req_complete(ndev, status);\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,8 @@\n \t\t\t\t\t\t\t ndev->cur_conn_id);\n \t\tif (conn_info) {\n \t\t\tlist_del(&conn_info->list);\n+\t\t\tif (conn_info == ndev->rf_conn_info)\n+\t\t\t\tndev->rf_conn_info = NULL;\n \t\t\tdevm_kfree(&ndev->nfc_dev->dev, conn_info);\n \t\t}\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (conn_info == ndev->rf_conn_info)",
                "\t\t\t\tndev->rf_conn_info = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel. A use-after-free vulnerability in the NFC stack can lead to a threat to confidentiality, integrity, and system availability.",
        "id": 3065
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static inline int l2cap_move_channel_req(struct l2cap_conn *conn,\n\t\t\t\t\t struct l2cap_cmd_hdr *cmd,\n\t\t\t\t\t u16 cmd_len, void *data)\n{\n\tstruct l2cap_move_chan_req *req = data;\n\tstruct l2cap_move_chan_rsp rsp;\n\tstruct l2cap_chan *chan;\n\tu16 icid = 0;\n\tu16 result = L2CAP_MR_NOT_ALLOWED;\n\n\tif (cmd_len != sizeof(*req))\n\t\treturn -EPROTO;\n\n\ticid = le16_to_cpu(req->icid);\n\n\tBT_DBG(\"icid 0x%4.4x, dest_amp_id %d\", icid, req->dest_amp_id);\n\n\tif (!(conn->local_fixed_chan & L2CAP_FC_A2MP))\n\t\treturn -EINVAL;\n\n\tchan = l2cap_get_chan_by_dcid(conn, icid);\n\tif (!chan) {\n\t\trsp.icid = cpu_to_le16(icid);\n\t\trsp.result = cpu_to_le16(L2CAP_MR_NOT_ALLOWED);\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_MOVE_CHAN_RSP,\n\t\t\t       sizeof(rsp), &rsp);\n\t\treturn 0;\n\t}\n\n\tchan->ident = cmd->ident;\n\n\tif (chan->scid < L2CAP_CID_DYN_START ||\n\t    chan->chan_policy == BT_CHANNEL_POLICY_BREDR_ONLY ||\n\t    (chan->mode != L2CAP_MODE_ERTM &&\n\t     chan->mode != L2CAP_MODE_STREAMING)) {\n\t\tresult = L2CAP_MR_NOT_ALLOWED;\n\t\tgoto send_move_response;\n\t}\n\n\tif (chan->local_amp_id == req->dest_amp_id) {\n\t\tresult = L2CAP_MR_SAME_ID;\n\t\tgoto send_move_response;\n\t}\n\n\tif (req->dest_amp_id != AMP_ID_BREDR) {\n\t\tstruct hci_dev *hdev;\n\t\thdev = hci_dev_get(req->dest_amp_id);\n\t\tif (!hdev || hdev->dev_type != HCI_AMP ||\n\t\t    !test_bit(HCI_UP, &hdev->flags)) {\n\t\t\tif (hdev)\n\t\t\t\thci_dev_put(hdev);\n\n\t\t\tresult = L2CAP_MR_BAD_ID;\n\t\t\tgoto send_move_response;\n\t\t}\n\t\thci_dev_put(hdev);\n\t}\n\n\t/* Detect a move collision.  Only send a collision response\n\t * if this side has \"lost\", otherwise proceed with the move.\n\t * The winner has the larger bd_addr.\n\t */\n\tif ((__chan_is_moving(chan) ||\n\t     chan->move_role != L2CAP_MOVE_ROLE_NONE) &&\n\t    bacmp(&conn->hcon->src, &conn->hcon->dst) > 0) {\n\t\tresult = L2CAP_MR_COLLISION;\n\t\tgoto send_move_response;\n\t}\n\n\tchan->move_role = L2CAP_MOVE_ROLE_RESPONDER;\n\tl2cap_move_setup(chan);\n\tchan->move_id = req->dest_amp_id;\n\n\tif (req->dest_amp_id == AMP_ID_BREDR) {\n\t\t/* Moving to BR/EDR */\n\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOCAL_BUSY;\n\t\t\tresult = L2CAP_MR_PEND;\n\t\t} else {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_CONFIRM;\n\t\t\tresult = L2CAP_MR_SUCCESS;\n\t\t}\n\t} else {\n\t\tchan->move_state = L2CAP_MOVE_WAIT_PREPARE;\n\t\t/* Placeholder - uncomment when amp functions are available */\n\t\t/*amp_accept_physical(chan, req->dest_amp_id);*/\n\t\tresult = L2CAP_MR_PEND;\n\t}\n\nsend_move_response:\n\tl2cap_send_move_chan_rsp(chan, result);\n\n\tl2cap_chan_unlock(chan);\n\n\treturn 0;\n}",
        "code_after_change": "static inline int l2cap_move_channel_req(struct l2cap_conn *conn,\n\t\t\t\t\t struct l2cap_cmd_hdr *cmd,\n\t\t\t\t\t u16 cmd_len, void *data)\n{\n\tstruct l2cap_move_chan_req *req = data;\n\tstruct l2cap_move_chan_rsp rsp;\n\tstruct l2cap_chan *chan;\n\tu16 icid = 0;\n\tu16 result = L2CAP_MR_NOT_ALLOWED;\n\n\tif (cmd_len != sizeof(*req))\n\t\treturn -EPROTO;\n\n\ticid = le16_to_cpu(req->icid);\n\n\tBT_DBG(\"icid 0x%4.4x, dest_amp_id %d\", icid, req->dest_amp_id);\n\n\tif (!(conn->local_fixed_chan & L2CAP_FC_A2MP))\n\t\treturn -EINVAL;\n\n\tchan = l2cap_get_chan_by_dcid(conn, icid);\n\tif (!chan) {\n\t\trsp.icid = cpu_to_le16(icid);\n\t\trsp.result = cpu_to_le16(L2CAP_MR_NOT_ALLOWED);\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_MOVE_CHAN_RSP,\n\t\t\t       sizeof(rsp), &rsp);\n\t\treturn 0;\n\t}\n\n\tchan->ident = cmd->ident;\n\n\tif (chan->scid < L2CAP_CID_DYN_START ||\n\t    chan->chan_policy == BT_CHANNEL_POLICY_BREDR_ONLY ||\n\t    (chan->mode != L2CAP_MODE_ERTM &&\n\t     chan->mode != L2CAP_MODE_STREAMING)) {\n\t\tresult = L2CAP_MR_NOT_ALLOWED;\n\t\tgoto send_move_response;\n\t}\n\n\tif (chan->local_amp_id == req->dest_amp_id) {\n\t\tresult = L2CAP_MR_SAME_ID;\n\t\tgoto send_move_response;\n\t}\n\n\tif (req->dest_amp_id != AMP_ID_BREDR) {\n\t\tstruct hci_dev *hdev;\n\t\thdev = hci_dev_get(req->dest_amp_id);\n\t\tif (!hdev || hdev->dev_type != HCI_AMP ||\n\t\t    !test_bit(HCI_UP, &hdev->flags)) {\n\t\t\tif (hdev)\n\t\t\t\thci_dev_put(hdev);\n\n\t\t\tresult = L2CAP_MR_BAD_ID;\n\t\t\tgoto send_move_response;\n\t\t}\n\t\thci_dev_put(hdev);\n\t}\n\n\t/* Detect a move collision.  Only send a collision response\n\t * if this side has \"lost\", otherwise proceed with the move.\n\t * The winner has the larger bd_addr.\n\t */\n\tif ((__chan_is_moving(chan) ||\n\t     chan->move_role != L2CAP_MOVE_ROLE_NONE) &&\n\t    bacmp(&conn->hcon->src, &conn->hcon->dst) > 0) {\n\t\tresult = L2CAP_MR_COLLISION;\n\t\tgoto send_move_response;\n\t}\n\n\tchan->move_role = L2CAP_MOVE_ROLE_RESPONDER;\n\tl2cap_move_setup(chan);\n\tchan->move_id = req->dest_amp_id;\n\n\tif (req->dest_amp_id == AMP_ID_BREDR) {\n\t\t/* Moving to BR/EDR */\n\t\tif (test_bit(CONN_LOCAL_BUSY, &chan->conn_state)) {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOCAL_BUSY;\n\t\t\tresult = L2CAP_MR_PEND;\n\t\t} else {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_CONFIRM;\n\t\t\tresult = L2CAP_MR_SUCCESS;\n\t\t}\n\t} else {\n\t\tchan->move_state = L2CAP_MOVE_WAIT_PREPARE;\n\t\t/* Placeholder - uncomment when amp functions are available */\n\t\t/*amp_accept_physical(chan, req->dest_amp_id);*/\n\t\tresult = L2CAP_MR_PEND;\n\t}\n\nsend_move_response:\n\tl2cap_send_move_chan_rsp(chan, result);\n\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -91,6 +91,7 @@\n \tl2cap_send_move_chan_rsp(chan, result);\n \n \tl2cap_chan_unlock(chan);\n+\tl2cap_chan_put(chan);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tl2cap_chan_put(chan);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3391
    },
    {
        "cve_id": "CVE-2023-0030",
        "code_before_change": "void\nnvkm_vmm_unmap_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *next;\n\n\tnvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);\n\tnvkm_memory_unref(&vma->memory);\n\n\tif (vma->part) {\n\t\tstruct nvkm_vma *prev = node(vma, prev);\n\t\tif (!prev->memory) {\n\t\t\tprev->size += vma->size;\n\t\t\trb_erase(&vma->tree, &vmm->root);\n\t\t\tlist_del(&vma->head);\n\t\t\tkfree(vma);\n\t\t\tvma = prev;\n\t\t}\n\t}\n\n\tnext = node(vma, next);\n\tif (next && next->part) {\n\t\tif (!next->memory) {\n\t\t\tvma->size += next->size;\n\t\t\trb_erase(&next->tree, &vmm->root);\n\t\t\tlist_del(&next->head);\n\t\t\tkfree(next);\n\t\t}\n\t}\n}",
        "code_after_change": "void\nnvkm_vmm_unmap_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *next = node(vma, next);\n\tstruct nvkm_vma *prev = NULL;\n\n\tnvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);\n\tnvkm_memory_unref(&vma->memory);\n\n\tif (!vma->part || ((prev = node(vma, prev)), prev->memory))\n\t\tprev = NULL;\n\tif (!next->part || next->memory)\n\t\tnext = NULL;\n\tnvkm_vmm_node_merge(vmm, prev, vma, next, vma->size);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,29 +1,15 @@\n void\n nvkm_vmm_unmap_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n {\n-\tstruct nvkm_vma *next;\n+\tstruct nvkm_vma *next = node(vma, next);\n+\tstruct nvkm_vma *prev = NULL;\n \n \tnvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);\n \tnvkm_memory_unref(&vma->memory);\n \n-\tif (vma->part) {\n-\t\tstruct nvkm_vma *prev = node(vma, prev);\n-\t\tif (!prev->memory) {\n-\t\t\tprev->size += vma->size;\n-\t\t\trb_erase(&vma->tree, &vmm->root);\n-\t\t\tlist_del(&vma->head);\n-\t\t\tkfree(vma);\n-\t\t\tvma = prev;\n-\t\t}\n-\t}\n-\n-\tnext = node(vma, next);\n-\tif (next && next->part) {\n-\t\tif (!next->memory) {\n-\t\t\tvma->size += next->size;\n-\t\t\trb_erase(&next->tree, &vmm->root);\n-\t\t\tlist_del(&next->head);\n-\t\t\tkfree(next);\n-\t\t}\n-\t}\n+\tif (!vma->part || ((prev = node(vma, prev)), prev->memory))\n+\t\tprev = NULL;\n+\tif (!next->part || next->memory)\n+\t\tnext = NULL;\n+\tnvkm_vmm_node_merge(vmm, prev, vma, next, vma->size);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct nvkm_vma *next = node(vma, next);",
                "\tstruct nvkm_vma *prev = NULL;",
                "\tif (!vma->part || ((prev = node(vma, prev)), prev->memory))",
                "\t\tprev = NULL;",
                "\tif (!next->part || next->memory)",
                "\t\tnext = NULL;",
                "\tnvkm_vmm_node_merge(vmm, prev, vma, next, vma->size);"
            ],
            "deleted": [
                "\tstruct nvkm_vma *next;",
                "\tif (vma->part) {",
                "\t\tstruct nvkm_vma *prev = node(vma, prev);",
                "\t\tif (!prev->memory) {",
                "\t\t\tprev->size += vma->size;",
                "\t\t\trb_erase(&vma->tree, &vmm->root);",
                "\t\t\tlist_del(&vma->head);",
                "\t\t\tkfree(vma);",
                "\t\t\tvma = prev;",
                "\t\t}",
                "\t}",
                "",
                "\tnext = node(vma, next);",
                "\tif (next && next->part) {",
                "\t\tif (!next->memory) {",
                "\t\t\tvma->size += next->size;",
                "\t\t\trb_erase(&next->tree, &vmm->root);",
                "\t\t\tlist_del(&next->head);",
                "\t\t\tkfree(next);",
                "\t\t}",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s nouveau driver in how a user triggers a memory overflow that causes the nvkm_vma_tail function to fail. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3805
    },
    {
        "cve_id": "CVE-2020-0427",
        "code_before_change": "static void dt_free_map(struct pinctrl_dev *pctldev,\n\t\t     struct pinctrl_map *map, unsigned num_maps)\n{\n\tif (pctldev) {\n\t\tconst struct pinctrl_ops *ops = pctldev->desc->pctlops;\n\t\tif (ops->dt_free_map)\n\t\t\tops->dt_free_map(pctldev, map, num_maps);\n\t} else {\n\t\t/* There is no pctldev for PIN_MAP_TYPE_DUMMY_STATE */\n\t\tkfree(map);\n\t}\n}",
        "code_after_change": "static void dt_free_map(struct pinctrl_dev *pctldev,\n\t\t     struct pinctrl_map *map, unsigned num_maps)\n{\n\tint i;\n\n\tfor (i = 0; i < num_maps; ++i) {\n\t\tkfree_const(map[i].dev_name);\n\t\tmap[i].dev_name = NULL;\n\t}\n\n\tif (pctldev) {\n\t\tconst struct pinctrl_ops *ops = pctldev->desc->pctlops;\n\t\tif (ops->dt_free_map)\n\t\t\tops->dt_free_map(pctldev, map, num_maps);\n\t} else {\n\t\t/* There is no pctldev for PIN_MAP_TYPE_DUMMY_STATE */\n\t\tkfree(map);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,13 @@\n static void dt_free_map(struct pinctrl_dev *pctldev,\n \t\t     struct pinctrl_map *map, unsigned num_maps)\n {\n+\tint i;\n+\n+\tfor (i = 0; i < num_maps; ++i) {\n+\t\tkfree_const(map[i].dev_name);\n+\t\tmap[i].dev_name = NULL;\n+\t}\n+\n \tif (pctldev) {\n \t\tconst struct pinctrl_ops *ops = pctldev->desc->pctlops;\n \t\tif (ops->dt_free_map)",
        "function_modified_lines": {
            "added": [
                "\tint i;",
                "",
                "\tfor (i = 0; i < num_maps; ++i) {",
                "\t\tkfree_const(map[i].dev_name);",
                "\t\tmap[i].dev_name = NULL;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-125",
            "CWE-416"
        ],
        "cve_description": "In create_pinctrl of core.c, there is a possible out of bounds read due to a use after free. This could lead to local information disclosure with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-140550171",
        "id": 2379
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "vm_fault_t do_swap_page(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct folio *swapcache, *folio = NULL;\n\tstruct page *page;\n\tstruct swap_info_struct *si = NULL;\n\trmap_t rmap_flags = RMAP_NONE;\n\tbool exclusive = false;\n\tswp_entry_t entry;\n\tpte_t pte;\n\tint locked;\n\tvm_fault_t ret = 0;\n\tvoid *shadow = NULL;\n\n\tif (!pte_unmap_same(vmf))\n\t\tgoto out;\n\n\tentry = pte_to_swp_entry(vmf->orig_pte);\n\tif (unlikely(non_swap_entry(entry))) {\n\t\tif (is_migration_entry(entry)) {\n\t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,\n\t\t\t\t\t     vmf->address);\n\t\t} else if (is_device_exclusive_entry(entry)) {\n\t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n\t\t\tret = remove_device_exclusive_entry(vmf);\n\t\t} else if (is_device_private_entry(entry)) {\n\t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);\n\t\t} else if (is_hwpoison_entry(entry)) {\n\t\t\tret = VM_FAULT_HWPOISON;\n\t\t} else if (is_swapin_error_entry(entry)) {\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t} else if (is_pte_marker_entry(entry)) {\n\t\t\tret = handle_pte_marker(vmf);\n\t\t} else {\n\t\t\tprint_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* Prevent swapoff from happening to us. */\n\tsi = get_swap_device(entry);\n\tif (unlikely(!si))\n\t\tgoto out;\n\n\tfolio = swap_cache_get_folio(entry, vma, vmf->address);\n\tif (folio)\n\t\tpage = folio_file_page(folio, swp_offset(entry));\n\tswapcache = folio;\n\n\tif (!folio) {\n\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&\n\t\t    __swap_count(entry) == 1) {\n\t\t\t/* skip swapcache */\n\t\t\tfolio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0,\n\t\t\t\t\t\tvma, vmf->address, false);\n\t\t\tpage = &folio->page;\n\t\t\tif (folio) {\n\t\t\t\t__folio_set_locked(folio);\n\t\t\t\t__folio_set_swapbacked(folio);\n\n\t\t\t\tif (mem_cgroup_swapin_charge_folio(folio,\n\t\t\t\t\t\t\tvma->vm_mm, GFP_KERNEL,\n\t\t\t\t\t\t\tentry)) {\n\t\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\t\tgoto out_page;\n\t\t\t\t}\n\t\t\t\tmem_cgroup_swapin_uncharge_swap(entry);\n\n\t\t\t\tshadow = get_shadow_from_swap_cache(entry);\n\t\t\t\tif (shadow)\n\t\t\t\t\tworkingset_refault(folio, shadow);\n\n\t\t\t\tfolio_add_lru(folio);\n\n\t\t\t\t/* To provide entry to swap_readpage() */\n\t\t\t\tfolio_set_swap_entry(folio, entry);\n\t\t\t\tswap_readpage(page, true, NULL);\n\t\t\t\tfolio->private = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\tpage = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,\n\t\t\t\t\t\tvmf);\n\t\t\tif (page)\n\t\t\t\tfolio = page_folio(page);\n\t\t\tswapcache = folio;\n\t\t}\n\n\t\tif (!folio) {\n\t\t\t/*\n\t\t\t * Back out if somebody else faulted in this pte\n\t\t\t * while we released the pte lock.\n\t\t\t */\n\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\t\tvmf->address, &vmf->ptl);\n\t\t\tif (likely(pte_same(*vmf->pte, vmf->orig_pte)))\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\t/* Had to read the page from swap area: Major fault */\n\t\tret = VM_FAULT_MAJOR;\n\t\tcount_vm_event(PGMAJFAULT);\n\t\tcount_memcg_event_mm(vma->vm_mm, PGMAJFAULT);\n\t} else if (PageHWPoison(page)) {\n\t\t/*\n\t\t * hwpoisoned dirty swapcache pages are kept for killing\n\t\t * owner processes (which may be unknown at hwpoison time)\n\t\t */\n\t\tret = VM_FAULT_HWPOISON;\n\t\tgoto out_release;\n\t}\n\n\tlocked = folio_lock_or_retry(folio, vma->vm_mm, vmf->flags);\n\n\tif (!locked) {\n\t\tret |= VM_FAULT_RETRY;\n\t\tgoto out_release;\n\t}\n\n\tif (swapcache) {\n\t\t/*\n\t\t * Make sure folio_free_swap() or swapoff did not release the\n\t\t * swapcache from under us.  The page pin, and pte_same test\n\t\t * below, are not enough to exclude that.  Even if it is still\n\t\t * swapcache, we need to check that the page's swap has not\n\t\t * changed.\n\t\t */\n\t\tif (unlikely(!folio_test_swapcache(folio) ||\n\t\t\t     page_private(page) != entry.val))\n\t\t\tgoto out_page;\n\n\t\t/*\n\t\t * KSM sometimes has to copy on read faults, for example, if\n\t\t * page->index of !PageKSM() pages would be nonlinear inside the\n\t\t * anon VMA -- PageKSM() is lost on actual swapout.\n\t\t */\n\t\tpage = ksm_might_need_to_copy(page, vma, vmf->address);\n\t\tif (unlikely(!page)) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_page;\n\t\t}\n\t\tfolio = page_folio(page);\n\n\t\t/*\n\t\t * If we want to map a page that's in the swapcache writable, we\n\t\t * have to detect via the refcount if we're really the exclusive\n\t\t * owner. Try removing the extra reference from the local LRU\n\t\t * pagevecs if required.\n\t\t */\n\t\tif ((vmf->flags & FAULT_FLAG_WRITE) && folio == swapcache &&\n\t\t    !folio_test_ksm(folio) && !folio_test_lru(folio))\n\t\t\tlru_add_drain();\n\t}\n\n\tcgroup_throttle_swaprate(page, GFP_KERNEL);\n\n\t/*\n\t * Back out if somebody else already faulted in this pte.\n\t */\n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t&vmf->ptl);\n\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte)))\n\t\tgoto out_nomap;\n\n\tif (unlikely(!folio_test_uptodate(folio))) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out_nomap;\n\t}\n\n\t/*\n\t * PG_anon_exclusive reuses PG_mappedtodisk for anon pages. A swap pte\n\t * must never point at an anonymous page in the swapcache that is\n\t * PG_anon_exclusive. Sanity check that this holds and especially, that\n\t * no filesystem set PG_mappedtodisk on a page in the swapcache. Sanity\n\t * check after taking the PT lock and making sure that nobody\n\t * concurrently faulted in this page and set PG_anon_exclusive.\n\t */\n\tBUG_ON(!folio_test_anon(folio) && folio_test_mappedtodisk(folio));\n\tBUG_ON(folio_test_anon(folio) && PageAnonExclusive(page));\n\n\t/*\n\t * Check under PT lock (to protect against concurrent fork() sharing\n\t * the swap entry concurrently) for certainly exclusive pages.\n\t */\n\tif (!folio_test_ksm(folio)) {\n\t\t/*\n\t\t * Note that pte_swp_exclusive() == false for architectures\n\t\t * without __HAVE_ARCH_PTE_SWP_EXCLUSIVE.\n\t\t */\n\t\texclusive = pte_swp_exclusive(vmf->orig_pte);\n\t\tif (folio != swapcache) {\n\t\t\t/*\n\t\t\t * We have a fresh page that is not exposed to the\n\t\t\t * swapcache -> certainly exclusive.\n\t\t\t */\n\t\t\texclusive = true;\n\t\t} else if (exclusive && folio_test_writeback(folio) &&\n\t\t\t  data_race(si->flags & SWP_STABLE_WRITES)) {\n\t\t\t/*\n\t\t\t * This is tricky: not all swap backends support\n\t\t\t * concurrent page modifications while under writeback.\n\t\t\t *\n\t\t\t * So if we stumble over such a page in the swapcache\n\t\t\t * we must not set the page exclusive, otherwise we can\n\t\t\t * map it writable without further checks and modify it\n\t\t\t * while still under writeback.\n\t\t\t *\n\t\t\t * For these problematic swap backends, simply drop the\n\t\t\t * exclusive marker: this is perfectly fine as we start\n\t\t\t * writeback only if we fully unmapped the page and\n\t\t\t * there are no unexpected references on the page after\n\t\t\t * unmapping succeeded. After fully unmapped, no\n\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can\n\t\t\t * appear, so dropping the exclusive marker and mapping\n\t\t\t * it only R/O is fine.\n\t\t\t */\n\t\t\texclusive = false;\n\t\t}\n\t}\n\n\t/*\n\t * Remove the swap entry and conditionally try to free up the swapcache.\n\t * We're already holding a reference on the page but haven't mapped it\n\t * yet.\n\t */\n\tswap_free(entry);\n\tif (should_try_to_free_swap(folio, vma, vmf->flags))\n\t\tfolio_free_swap(folio);\n\n\tinc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);\n\tdec_mm_counter_fast(vma->vm_mm, MM_SWAPENTS);\n\tpte = mk_pte(page, vma->vm_page_prot);\n\n\t/*\n\t * Same logic as in do_wp_page(); however, optimize for pages that are\n\t * certainly not shared either because we just allocated them without\n\t * exposing them to the swapcache or because the swap entry indicates\n\t * exclusivity.\n\t */\n\tif (!folio_test_ksm(folio) &&\n\t    (exclusive || folio_ref_count(folio) == 1)) {\n\t\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);\n\t\t\tvmf->flags &= ~FAULT_FLAG_WRITE;\n\t\t\tret |= VM_FAULT_WRITE;\n\t\t}\n\t\trmap_flags |= RMAP_EXCLUSIVE;\n\t}\n\tflush_icache_page(vma, page);\n\tif (pte_swp_soft_dirty(vmf->orig_pte))\n\t\tpte = pte_mksoft_dirty(pte);\n\tif (pte_swp_uffd_wp(vmf->orig_pte)) {\n\t\tpte = pte_mkuffd_wp(pte);\n\t\tpte = pte_wrprotect(pte);\n\t}\n\tvmf->orig_pte = pte;\n\n\t/* ksm created a completely new copy */\n\tif (unlikely(folio != swapcache && swapcache)) {\n\t\tpage_add_new_anon_rmap(page, vma, vmf->address);\n\t\tfolio_add_lru_vma(folio, vma);\n\t} else {\n\t\tpage_add_anon_rmap(page, vma, vmf->address, rmap_flags);\n\t}\n\n\tVM_BUG_ON(!folio_test_anon(folio) ||\n\t\t\t(pte_write(pte) && !PageAnonExclusive(page)));\n\tset_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);\n\tarch_do_swap_page(vma->vm_mm, vma, vmf->address, pte, vmf->orig_pte);\n\n\tfolio_unlock(folio);\n\tif (folio != swapcache && swapcache) {\n\t\t/*\n\t\t * Hold the lock to avoid the swap entry to be reused\n\t\t * until we take the PT lock for the pte_same() check\n\t\t * (to avoid false positives from pte_same). For\n\t\t * further safety release the lock after the swap_free\n\t\t * so that the swap count won't change under a\n\t\t * parallel locked swapcache.\n\t\t */\n\t\tfolio_unlock(swapcache);\n\t\tfolio_put(swapcache);\n\t}\n\n\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\tret |= do_wp_page(vmf);\n\t\tif (ret & VM_FAULT_ERROR)\n\t\t\tret &= VM_FAULT_ERROR;\n\t\tgoto out;\n\t}\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, vmf->address, vmf->pte);\nunlock:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout:\n\tif (si)\n\t\tput_swap_device(si);\n\treturn ret;\nout_nomap:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout_page:\n\tfolio_unlock(folio);\nout_release:\n\tfolio_put(folio);\n\tif (folio != swapcache && swapcache) {\n\t\tfolio_unlock(swapcache);\n\t\tfolio_put(swapcache);\n\t}\n\tif (si)\n\t\tput_swap_device(si);\n\treturn ret;\n}",
        "code_after_change": "vm_fault_t do_swap_page(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct folio *swapcache, *folio = NULL;\n\tstruct page *page;\n\tstruct swap_info_struct *si = NULL;\n\trmap_t rmap_flags = RMAP_NONE;\n\tbool exclusive = false;\n\tswp_entry_t entry;\n\tpte_t pte;\n\tint locked;\n\tvm_fault_t ret = 0;\n\tvoid *shadow = NULL;\n\n\tif (!pte_unmap_same(vmf))\n\t\tgoto out;\n\n\tentry = pte_to_swp_entry(vmf->orig_pte);\n\tif (unlikely(non_swap_entry(entry))) {\n\t\tif (is_migration_entry(entry)) {\n\t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,\n\t\t\t\t\t     vmf->address);\n\t\t} else if (is_device_exclusive_entry(entry)) {\n\t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n\t\t\tret = remove_device_exclusive_entry(vmf);\n\t\t} else if (is_device_private_entry(entry)) {\n\t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\t\tvmf->address, &vmf->ptl);\n\t\t\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte))) {\n\t\t\t\tspin_unlock(vmf->ptl);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Get a page reference while we know the page can't be\n\t\t\t * freed.\n\t\t\t */\n\t\t\tget_page(vmf->page);\n\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\t\tvmf->page->pgmap->ops->migrate_to_ram(vmf);\n\t\t\tput_page(vmf->page);\n\t\t} else if (is_hwpoison_entry(entry)) {\n\t\t\tret = VM_FAULT_HWPOISON;\n\t\t} else if (is_swapin_error_entry(entry)) {\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t} else if (is_pte_marker_entry(entry)) {\n\t\t\tret = handle_pte_marker(vmf);\n\t\t} else {\n\t\t\tprint_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* Prevent swapoff from happening to us. */\n\tsi = get_swap_device(entry);\n\tif (unlikely(!si))\n\t\tgoto out;\n\n\tfolio = swap_cache_get_folio(entry, vma, vmf->address);\n\tif (folio)\n\t\tpage = folio_file_page(folio, swp_offset(entry));\n\tswapcache = folio;\n\n\tif (!folio) {\n\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&\n\t\t    __swap_count(entry) == 1) {\n\t\t\t/* skip swapcache */\n\t\t\tfolio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0,\n\t\t\t\t\t\tvma, vmf->address, false);\n\t\t\tpage = &folio->page;\n\t\t\tif (folio) {\n\t\t\t\t__folio_set_locked(folio);\n\t\t\t\t__folio_set_swapbacked(folio);\n\n\t\t\t\tif (mem_cgroup_swapin_charge_folio(folio,\n\t\t\t\t\t\t\tvma->vm_mm, GFP_KERNEL,\n\t\t\t\t\t\t\tentry)) {\n\t\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\t\tgoto out_page;\n\t\t\t\t}\n\t\t\t\tmem_cgroup_swapin_uncharge_swap(entry);\n\n\t\t\t\tshadow = get_shadow_from_swap_cache(entry);\n\t\t\t\tif (shadow)\n\t\t\t\t\tworkingset_refault(folio, shadow);\n\n\t\t\t\tfolio_add_lru(folio);\n\n\t\t\t\t/* To provide entry to swap_readpage() */\n\t\t\t\tfolio_set_swap_entry(folio, entry);\n\t\t\t\tswap_readpage(page, true, NULL);\n\t\t\t\tfolio->private = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\tpage = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,\n\t\t\t\t\t\tvmf);\n\t\t\tif (page)\n\t\t\t\tfolio = page_folio(page);\n\t\t\tswapcache = folio;\n\t\t}\n\n\t\tif (!folio) {\n\t\t\t/*\n\t\t\t * Back out if somebody else faulted in this pte\n\t\t\t * while we released the pte lock.\n\t\t\t */\n\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\t\tvmf->address, &vmf->ptl);\n\t\t\tif (likely(pte_same(*vmf->pte, vmf->orig_pte)))\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\t/* Had to read the page from swap area: Major fault */\n\t\tret = VM_FAULT_MAJOR;\n\t\tcount_vm_event(PGMAJFAULT);\n\t\tcount_memcg_event_mm(vma->vm_mm, PGMAJFAULT);\n\t} else if (PageHWPoison(page)) {\n\t\t/*\n\t\t * hwpoisoned dirty swapcache pages are kept for killing\n\t\t * owner processes (which may be unknown at hwpoison time)\n\t\t */\n\t\tret = VM_FAULT_HWPOISON;\n\t\tgoto out_release;\n\t}\n\n\tlocked = folio_lock_or_retry(folio, vma->vm_mm, vmf->flags);\n\n\tif (!locked) {\n\t\tret |= VM_FAULT_RETRY;\n\t\tgoto out_release;\n\t}\n\n\tif (swapcache) {\n\t\t/*\n\t\t * Make sure folio_free_swap() or swapoff did not release the\n\t\t * swapcache from under us.  The page pin, and pte_same test\n\t\t * below, are not enough to exclude that.  Even if it is still\n\t\t * swapcache, we need to check that the page's swap has not\n\t\t * changed.\n\t\t */\n\t\tif (unlikely(!folio_test_swapcache(folio) ||\n\t\t\t     page_private(page) != entry.val))\n\t\t\tgoto out_page;\n\n\t\t/*\n\t\t * KSM sometimes has to copy on read faults, for example, if\n\t\t * page->index of !PageKSM() pages would be nonlinear inside the\n\t\t * anon VMA -- PageKSM() is lost on actual swapout.\n\t\t */\n\t\tpage = ksm_might_need_to_copy(page, vma, vmf->address);\n\t\tif (unlikely(!page)) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_page;\n\t\t}\n\t\tfolio = page_folio(page);\n\n\t\t/*\n\t\t * If we want to map a page that's in the swapcache writable, we\n\t\t * have to detect via the refcount if we're really the exclusive\n\t\t * owner. Try removing the extra reference from the local LRU\n\t\t * pagevecs if required.\n\t\t */\n\t\tif ((vmf->flags & FAULT_FLAG_WRITE) && folio == swapcache &&\n\t\t    !folio_test_ksm(folio) && !folio_test_lru(folio))\n\t\t\tlru_add_drain();\n\t}\n\n\tcgroup_throttle_swaprate(page, GFP_KERNEL);\n\n\t/*\n\t * Back out if somebody else already faulted in this pte.\n\t */\n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t&vmf->ptl);\n\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte)))\n\t\tgoto out_nomap;\n\n\tif (unlikely(!folio_test_uptodate(folio))) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out_nomap;\n\t}\n\n\t/*\n\t * PG_anon_exclusive reuses PG_mappedtodisk for anon pages. A swap pte\n\t * must never point at an anonymous page in the swapcache that is\n\t * PG_anon_exclusive. Sanity check that this holds and especially, that\n\t * no filesystem set PG_mappedtodisk on a page in the swapcache. Sanity\n\t * check after taking the PT lock and making sure that nobody\n\t * concurrently faulted in this page and set PG_anon_exclusive.\n\t */\n\tBUG_ON(!folio_test_anon(folio) && folio_test_mappedtodisk(folio));\n\tBUG_ON(folio_test_anon(folio) && PageAnonExclusive(page));\n\n\t/*\n\t * Check under PT lock (to protect against concurrent fork() sharing\n\t * the swap entry concurrently) for certainly exclusive pages.\n\t */\n\tif (!folio_test_ksm(folio)) {\n\t\t/*\n\t\t * Note that pte_swp_exclusive() == false for architectures\n\t\t * without __HAVE_ARCH_PTE_SWP_EXCLUSIVE.\n\t\t */\n\t\texclusive = pte_swp_exclusive(vmf->orig_pte);\n\t\tif (folio != swapcache) {\n\t\t\t/*\n\t\t\t * We have a fresh page that is not exposed to the\n\t\t\t * swapcache -> certainly exclusive.\n\t\t\t */\n\t\t\texclusive = true;\n\t\t} else if (exclusive && folio_test_writeback(folio) &&\n\t\t\t  data_race(si->flags & SWP_STABLE_WRITES)) {\n\t\t\t/*\n\t\t\t * This is tricky: not all swap backends support\n\t\t\t * concurrent page modifications while under writeback.\n\t\t\t *\n\t\t\t * So if we stumble over such a page in the swapcache\n\t\t\t * we must not set the page exclusive, otherwise we can\n\t\t\t * map it writable without further checks and modify it\n\t\t\t * while still under writeback.\n\t\t\t *\n\t\t\t * For these problematic swap backends, simply drop the\n\t\t\t * exclusive marker: this is perfectly fine as we start\n\t\t\t * writeback only if we fully unmapped the page and\n\t\t\t * there are no unexpected references on the page after\n\t\t\t * unmapping succeeded. After fully unmapped, no\n\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can\n\t\t\t * appear, so dropping the exclusive marker and mapping\n\t\t\t * it only R/O is fine.\n\t\t\t */\n\t\t\texclusive = false;\n\t\t}\n\t}\n\n\t/*\n\t * Remove the swap entry and conditionally try to free up the swapcache.\n\t * We're already holding a reference on the page but haven't mapped it\n\t * yet.\n\t */\n\tswap_free(entry);\n\tif (should_try_to_free_swap(folio, vma, vmf->flags))\n\t\tfolio_free_swap(folio);\n\n\tinc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);\n\tdec_mm_counter_fast(vma->vm_mm, MM_SWAPENTS);\n\tpte = mk_pte(page, vma->vm_page_prot);\n\n\t/*\n\t * Same logic as in do_wp_page(); however, optimize for pages that are\n\t * certainly not shared either because we just allocated them without\n\t * exposing them to the swapcache or because the swap entry indicates\n\t * exclusivity.\n\t */\n\tif (!folio_test_ksm(folio) &&\n\t    (exclusive || folio_ref_count(folio) == 1)) {\n\t\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);\n\t\t\tvmf->flags &= ~FAULT_FLAG_WRITE;\n\t\t\tret |= VM_FAULT_WRITE;\n\t\t}\n\t\trmap_flags |= RMAP_EXCLUSIVE;\n\t}\n\tflush_icache_page(vma, page);\n\tif (pte_swp_soft_dirty(vmf->orig_pte))\n\t\tpte = pte_mksoft_dirty(pte);\n\tif (pte_swp_uffd_wp(vmf->orig_pte)) {\n\t\tpte = pte_mkuffd_wp(pte);\n\t\tpte = pte_wrprotect(pte);\n\t}\n\tvmf->orig_pte = pte;\n\n\t/* ksm created a completely new copy */\n\tif (unlikely(folio != swapcache && swapcache)) {\n\t\tpage_add_new_anon_rmap(page, vma, vmf->address);\n\t\tfolio_add_lru_vma(folio, vma);\n\t} else {\n\t\tpage_add_anon_rmap(page, vma, vmf->address, rmap_flags);\n\t}\n\n\tVM_BUG_ON(!folio_test_anon(folio) ||\n\t\t\t(pte_write(pte) && !PageAnonExclusive(page)));\n\tset_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);\n\tarch_do_swap_page(vma->vm_mm, vma, vmf->address, pte, vmf->orig_pte);\n\n\tfolio_unlock(folio);\n\tif (folio != swapcache && swapcache) {\n\t\t/*\n\t\t * Hold the lock to avoid the swap entry to be reused\n\t\t * until we take the PT lock for the pte_same() check\n\t\t * (to avoid false positives from pte_same). For\n\t\t * further safety release the lock after the swap_free\n\t\t * so that the swap count won't change under a\n\t\t * parallel locked swapcache.\n\t\t */\n\t\tfolio_unlock(swapcache);\n\t\tfolio_put(swapcache);\n\t}\n\n\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\tret |= do_wp_page(vmf);\n\t\tif (ret & VM_FAULT_ERROR)\n\t\t\tret &= VM_FAULT_ERROR;\n\t\tgoto out;\n\t}\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, vmf->address, vmf->pte);\nunlock:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout:\n\tif (si)\n\t\tput_swap_device(si);\n\treturn ret;\nout_nomap:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout_page:\n\tfolio_unlock(folio);\nout_release:\n\tfolio_put(folio);\n\tif (folio != swapcache && swapcache) {\n\t\tfolio_unlock(swapcache);\n\t\tfolio_put(swapcache);\n\t}\n\tif (si)\n\t\tput_swap_device(si);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,7 +25,21 @@\n \t\t\tret = remove_device_exclusive_entry(vmf);\n \t\t} else if (is_device_private_entry(entry)) {\n \t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n-\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);\n+\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n+\t\t\t\t\tvmf->address, &vmf->ptl);\n+\t\t\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte))) {\n+\t\t\t\tspin_unlock(vmf->ptl);\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\n+\t\t\t/*\n+\t\t\t * Get a page reference while we know the page can't be\n+\t\t\t * freed.\n+\t\t\t */\n+\t\t\tget_page(vmf->page);\n+\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\t\t\tvmf->page->pgmap->ops->migrate_to_ram(vmf);\n+\t\t\tput_page(vmf->page);\n \t\t} else if (is_hwpoison_entry(entry)) {\n \t\t\tret = VM_FAULT_HWPOISON;\n \t\t} else if (is_swapin_error_entry(entry)) {",
        "function_modified_lines": {
            "added": [
                "\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,",
                "\t\t\t\t\tvmf->address, &vmf->ptl);",
                "\t\t\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte))) {",
                "\t\t\t\tspin_unlock(vmf->ptl);",
                "\t\t\t\tgoto out;",
                "\t\t\t}",
                "",
                "\t\t\t/*",
                "\t\t\t * Get a page reference while we know the page can't be",
                "\t\t\t * freed.",
                "\t\t\t */",
                "\t\t\tget_page(vmf->page);",
                "\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
                "\t\t\tvmf->page->pgmap->ops->migrate_to_ram(vmf);",
                "\t\t\tput_page(vmf->page);"
            ],
            "deleted": [
                "\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3618
    },
    {
        "cve_id": "CVE-2023-39198",
        "code_before_change": "int qxl_alloc_surf_ioctl(struct drm_device *dev, void *data, struct drm_file *file)\n{\n\tstruct qxl_device *qdev = to_qxl(dev);\n\tstruct drm_qxl_alloc_surf *param = data;\n\tstruct qxl_bo *qobj;\n\tint handle;\n\tint ret;\n\tint size, actual_stride;\n\tstruct qxl_surface surf;\n\n\t/* work out size allocate bo with handle */\n\tactual_stride = param->stride < 0 ? -param->stride : param->stride;\n\tsize = actual_stride * param->height + actual_stride;\n\n\tsurf.format = param->format;\n\tsurf.width = param->width;\n\tsurf.height = param->height;\n\tsurf.stride = param->stride;\n\tsurf.data = 0;\n\n\tret = qxl_gem_object_create_with_handle(qdev, file,\n\t\t\t\t\t\tQXL_GEM_DOMAIN_SURFACE,\n\t\t\t\t\t\tsize,\n\t\t\t\t\t\t&surf,\n\t\t\t\t\t\t&qobj, &handle);\n\tif (ret) {\n\t\tDRM_ERROR(\"%s: failed to create gem ret=%d\\n\",\n\t\t\t  __func__, ret);\n\t\treturn -ENOMEM;\n\t} else\n\t\tparam->handle = handle;\n\treturn ret;\n}",
        "code_after_change": "int qxl_alloc_surf_ioctl(struct drm_device *dev, void *data, struct drm_file *file)\n{\n\tstruct qxl_device *qdev = to_qxl(dev);\n\tstruct drm_qxl_alloc_surf *param = data;\n\tint handle;\n\tint ret;\n\tint size, actual_stride;\n\tstruct qxl_surface surf;\n\n\t/* work out size allocate bo with handle */\n\tactual_stride = param->stride < 0 ? -param->stride : param->stride;\n\tsize = actual_stride * param->height + actual_stride;\n\n\tsurf.format = param->format;\n\tsurf.width = param->width;\n\tsurf.height = param->height;\n\tsurf.stride = param->stride;\n\tsurf.data = 0;\n\n\tret = qxl_gem_object_create_with_handle(qdev, file,\n\t\t\t\t\t\tQXL_GEM_DOMAIN_SURFACE,\n\t\t\t\t\t\tsize,\n\t\t\t\t\t\t&surf,\n\t\t\t\t\t\tNULL, &handle);\n\tif (ret) {\n\t\tDRM_ERROR(\"%s: failed to create gem ret=%d\\n\",\n\t\t\t  __func__, ret);\n\t\treturn -ENOMEM;\n\t} else\n\t\tparam->handle = handle;\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,6 @@\n {\n \tstruct qxl_device *qdev = to_qxl(dev);\n \tstruct drm_qxl_alloc_surf *param = data;\n-\tstruct qxl_bo *qobj;\n \tint handle;\n \tint ret;\n \tint size, actual_stride;\n@@ -22,7 +21,7 @@\n \t\t\t\t\t\tQXL_GEM_DOMAIN_SURFACE,\n \t\t\t\t\t\tsize,\n \t\t\t\t\t\t&surf,\n-\t\t\t\t\t\t&qobj, &handle);\n+\t\t\t\t\t\tNULL, &handle);\n \tif (ret) {\n \t\tDRM_ERROR(\"%s: failed to create gem ret=%d\\n\",\n \t\t\t  __func__, ret);",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\t\tNULL, &handle);"
            ],
            "deleted": [
                "\tstruct qxl_bo *qobj;",
                "\t\t\t\t\t\t&qobj, &handle);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A race condition was found in the QXL driver in the Linux kernel. The qxl_mode_dumb_create() function dereferences the qobj returned by the qxl_gem_object_create_with_handle(), but the handle is the only one holding a reference to it. This flaw allows an attacker to guess the returned handle value and trigger a use-after-free issue, potentially leading to a denial of service or privilege escalation.",
        "id": 4186
    },
    {
        "cve_id": "CVE-2022-2938",
        "code_before_change": "static int psi_fop_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\n\tpsi_trigger_replace(&seq->private, NULL);\n\treturn single_release(inode, file);\n}",
        "code_after_change": "static int psi_fop_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\n\tpsi_trigger_destroy(seq->private);\n\treturn single_release(inode, file);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,6 @@\n {\n \tstruct seq_file *seq = file->private_data;\n \n-\tpsi_trigger_replace(&seq->private, NULL);\n+\tpsi_trigger_destroy(seq->private);\n \treturn single_release(inode, file);\n }",
        "function_modified_lines": {
            "added": [
                "\tpsi_trigger_destroy(seq->private);"
            ],
            "deleted": [
                "\tpsi_trigger_replace(&seq->private, NULL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's implementation of Pressure Stall Information. While the feature is disabled by default, it could allow an attacker to crash the system or have other memory-corruption side effects.",
        "id": 3516
    },
    {
        "cve_id": "CVE-2022-20158",
        "code_before_change": "static int packet_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\n\t\t\t  int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\tint vnet_hdr_len = 0;\n\tunsigned int origlen = 0;\n\n\terr = -EINVAL;\n\tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT|MSG_ERRQUEUE))\n\t\tgoto out;\n\n#if 0\n\t/* What error should we return now? EUNATTACH? */\n\tif (pkt_sk(sk)->ifindex < 0)\n\t\treturn -ENODEV;\n#endif\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = sock_recv_errqueue(sk, msg, len,\n\t\t\t\t\t SOL_PACKET, PACKET_TX_TIMESTAMP);\n\t\tgoto out;\n\t}\n\n\t/*\n\t *\tCall the generic datagram receiver. This handles all sorts\n\t *\tof horrible races and re-entrancy so we can forget about it\n\t *\tin the protocol layers.\n\t *\n\t *\tNow it will return ENETDOWN, if device have just gone down,\n\t *\tbut then it will block.\n\t */\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\n\t/*\n\t *\tAn error occurred so return it. Because skb_recv_datagram()\n\t *\thandles the blocking we don't see and worry about blocking\n\t *\tretries.\n\t */\n\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tpacket_rcv_try_clear_pressure(pkt_sk(sk));\n\n\tif (pkt_sk(sk)->has_vnet_hdr) {\n\t\terr = packet_rcv_vnet(msg, skb, &len);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tvnet_hdr_len = sizeof(struct virtio_net_hdr);\n\t}\n\n\t/* You lose any data beyond the buffer you gave. If it worries\n\t * a user program they can ask the device for its MTU\n\t * anyway.\n\t */\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock->type != SOCK_PACKET) {\n\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n\n\t\t/* Original length was stored in sockaddr_ll fields */\n\t\toriglen = PACKET_SKB_CB(skb)->sa.origlen;\n\t\tsll->sll_family = AF_PACKET;\n\t\tsll->sll_protocol = skb->protocol;\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (msg->msg_name) {\n\t\tint copy_len;\n\n\t\t/* If the address length field is there to be filled\n\t\t * in, we fill it in now.\n\t\t */\n\t\tif (sock->type == SOCK_PACKET) {\n\t\t\t__sockaddr_check_size(sizeof(struct sockaddr_pkt));\n\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);\n\t\t\tcopy_len = msg->msg_namelen;\n\t\t} else {\n\t\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n\n\t\t\tmsg->msg_namelen = sll->sll_halen +\n\t\t\t\toffsetof(struct sockaddr_ll, sll_addr);\n\t\t\tcopy_len = msg->msg_namelen;\n\t\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll)) {\n\t\t\t\tmemset(msg->msg_name +\n\t\t\t\t       offsetof(struct sockaddr_ll, sll_addr),\n\t\t\t\t       0, sizeof(sll->sll_addr));\n\t\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_ll);\n\t\t\t}\n\t\t}\n\t\tmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa, copy_len);\n\t}\n\n\tif (pkt_sk(sk)->auxdata) {\n\t\tstruct tpacket_auxdata aux;\n\n\t\taux.tp_status = TP_STATUS_USER;\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\taux.tp_status |= TP_STATUS_CSUMNOTREADY;\n\t\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t\t  skb_csum_unnecessary(skb)))\n\t\t\taux.tp_status |= TP_STATUS_CSUM_VALID;\n\n\t\taux.tp_len = origlen;\n\t\taux.tp_snaplen = skb->len;\n\t\taux.tp_mac = 0;\n\t\taux.tp_net = skb_network_offset(skb);\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\taux.tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\taux.tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\taux.tp_status |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\taux.tp_vlan_tci = 0;\n\t\t\taux.tp_vlan_tpid = 0;\n\t\t}\n\t\tput_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);\n\t}\n\n\t/*\n\t *\tFree or return the buffer as appropriate. Again this\n\t *\thides all the races and re-entrancy issues from us.\n\t */\n\terr = vnet_hdr_len + ((flags&MSG_TRUNC) ? skb->len : copied);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n}",
        "code_after_change": "static int packet_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\n\t\t\t  int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\tint vnet_hdr_len = 0;\n\tunsigned int origlen = 0;\n\n\terr = -EINVAL;\n\tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT|MSG_ERRQUEUE))\n\t\tgoto out;\n\n#if 0\n\t/* What error should we return now? EUNATTACH? */\n\tif (pkt_sk(sk)->ifindex < 0)\n\t\treturn -ENODEV;\n#endif\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = sock_recv_errqueue(sk, msg, len,\n\t\t\t\t\t SOL_PACKET, PACKET_TX_TIMESTAMP);\n\t\tgoto out;\n\t}\n\n\t/*\n\t *\tCall the generic datagram receiver. This handles all sorts\n\t *\tof horrible races and re-entrancy so we can forget about it\n\t *\tin the protocol layers.\n\t *\n\t *\tNow it will return ENETDOWN, if device have just gone down,\n\t *\tbut then it will block.\n\t */\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\n\t/*\n\t *\tAn error occurred so return it. Because skb_recv_datagram()\n\t *\thandles the blocking we don't see and worry about blocking\n\t *\tretries.\n\t */\n\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tpacket_rcv_try_clear_pressure(pkt_sk(sk));\n\n\tif (pkt_sk(sk)->has_vnet_hdr) {\n\t\terr = packet_rcv_vnet(msg, skb, &len);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tvnet_hdr_len = sizeof(struct virtio_net_hdr);\n\t}\n\n\t/* You lose any data beyond the buffer you gave. If it worries\n\t * a user program they can ask the device for its MTU\n\t * anyway.\n\t */\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock->type != SOCK_PACKET) {\n\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n\n\t\t/* Original length was stored in sockaddr_ll fields */\n\t\toriglen = PACKET_SKB_CB(skb)->sa.origlen;\n\t\tsll->sll_family = AF_PACKET;\n\t\tsll->sll_protocol = skb->protocol;\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (msg->msg_name) {\n\t\tconst size_t max_len = min(sizeof(skb->cb),\n\t\t\t\t\t   sizeof(struct sockaddr_storage));\n\t\tint copy_len;\n\n\t\t/* If the address length field is there to be filled\n\t\t * in, we fill it in now.\n\t\t */\n\t\tif (sock->type == SOCK_PACKET) {\n\t\t\t__sockaddr_check_size(sizeof(struct sockaddr_pkt));\n\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);\n\t\t\tcopy_len = msg->msg_namelen;\n\t\t} else {\n\t\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n\n\t\t\tmsg->msg_namelen = sll->sll_halen +\n\t\t\t\toffsetof(struct sockaddr_ll, sll_addr);\n\t\t\tcopy_len = msg->msg_namelen;\n\t\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll)) {\n\t\t\t\tmemset(msg->msg_name +\n\t\t\t\t       offsetof(struct sockaddr_ll, sll_addr),\n\t\t\t\t       0, sizeof(sll->sll_addr));\n\t\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_ll);\n\t\t\t}\n\t\t}\n\t\tif (WARN_ON_ONCE(copy_len > max_len)) {\n\t\t\tcopy_len = max_len;\n\t\t\tmsg->msg_namelen = copy_len;\n\t\t}\n\t\tmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa, copy_len);\n\t}\n\n\tif (pkt_sk(sk)->auxdata) {\n\t\tstruct tpacket_auxdata aux;\n\n\t\taux.tp_status = TP_STATUS_USER;\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\taux.tp_status |= TP_STATUS_CSUMNOTREADY;\n\t\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t\t  skb_csum_unnecessary(skb)))\n\t\t\taux.tp_status |= TP_STATUS_CSUM_VALID;\n\n\t\taux.tp_len = origlen;\n\t\taux.tp_snaplen = skb->len;\n\t\taux.tp_mac = 0;\n\t\taux.tp_net = skb_network_offset(skb);\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\taux.tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\taux.tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\taux.tp_status |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\taux.tp_vlan_tci = 0;\n\t\t\taux.tp_vlan_tpid = 0;\n\t\t}\n\t\tput_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);\n\t}\n\n\t/*\n\t *\tFree or return the buffer as appropriate. Again this\n\t *\thides all the races and re-entrancy issues from us.\n\t */\n\terr = vnet_hdr_len + ((flags&MSG_TRUNC) ? skb->len : copied);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -78,6 +78,8 @@\n \tsock_recv_ts_and_drops(msg, sk, skb);\n \n \tif (msg->msg_name) {\n+\t\tconst size_t max_len = min(sizeof(skb->cb),\n+\t\t\t\t\t   sizeof(struct sockaddr_storage));\n \t\tint copy_len;\n \n \t\t/* If the address length field is there to be filled\n@@ -99,6 +101,10 @@\n \t\t\t\t       0, sizeof(sll->sll_addr));\n \t\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_ll);\n \t\t\t}\n+\t\t}\n+\t\tif (WARN_ON_ONCE(copy_len > max_len)) {\n+\t\t\tcopy_len = max_len;\n+\t\t\tmsg->msg_namelen = copy_len;\n \t\t}\n \t\tmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa, copy_len);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tconst size_t max_len = min(sizeof(skb->cb),",
                "\t\t\t\t\t   sizeof(struct sockaddr_storage));",
                "\t\t}",
                "\t\tif (WARN_ON_ONCE(copy_len > max_len)) {",
                "\t\t\tcopy_len = max_len;",
                "\t\t\tmsg->msg_namelen = copy_len;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In bdi_put and bdi_unregister of backing-dev.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-182815710References: Upstream kernel",
        "id": 3343
    },
    {
        "cve_id": "CVE-2023-21255",
        "code_before_change": "static void\nbinder_free_buf(struct binder_proc *proc,\n\t\tstruct binder_thread *thread,\n\t\tstruct binder_buffer *buffer, bool is_failure)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
        "code_after_change": "static void\nbinder_free_buf(struct binder_proc *proc,\n\t\tstruct binder_thread *thread,\n\t\tstruct binder_buffer *buffer, bool is_failure)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_release_entire_buffer(proc, thread, buffer, is_failure);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,6 +29,6 @@\n \t\tbinder_node_inner_unlock(buf_node);\n \t}\n \ttrace_binder_transaction_buffer_release(buffer);\n-\tbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);\n+\tbinder_release_entire_buffer(proc, thread, buffer, is_failure);\n \tbinder_alloc_free_buf(&proc->alloc, buffer);\n }",
        "function_modified_lines": {
            "added": [
                "\tbinder_release_entire_buffer(proc, thread, buffer, is_failure);"
            ],
            "deleted": [
                "\tbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);"
            ]
        },
        "cwe": [
            "CWE-787",
            "CWE-416"
        ],
        "cve_description": "In multiple functions of binder.c, there is a possible memory corruption due to a use after free. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.\n\n",
        "id": 3916
    },
    {
        "cve_id": "CVE-2016-7912",
        "code_before_change": "static void ffs_user_copy_worker(struct work_struct *work)\n{\n\tstruct ffs_io_data *io_data = container_of(work, struct ffs_io_data,\n\t\t\t\t\t\t   work);\n\tint ret = io_data->req->status ? io_data->req->status :\n\t\t\t\t\t io_data->req->actual;\n\n\tif (io_data->read && ret > 0) {\n\t\tuse_mm(io_data->mm);\n\t\tret = copy_to_iter(io_data->buf, ret, &io_data->data);\n\t\tif (iov_iter_count(&io_data->data))\n\t\t\tret = -EFAULT;\n\t\tunuse_mm(io_data->mm);\n\t}\n\n\tio_data->kiocb->ki_complete(io_data->kiocb, ret, ret);\n\n\tif (io_data->ffs->ffs_eventfd &&\n\t    !(io_data->kiocb->ki_flags & IOCB_EVENTFD))\n\t\teventfd_signal(io_data->ffs->ffs_eventfd, 1);\n\n\tusb_ep_free_request(io_data->ep, io_data->req);\n\n\tio_data->kiocb->private = NULL;\n\tif (io_data->read)\n\t\tkfree(io_data->to_free);\n\tkfree(io_data->buf);\n\tkfree(io_data);\n}",
        "code_after_change": "static void ffs_user_copy_worker(struct work_struct *work)\n{\n\tstruct ffs_io_data *io_data = container_of(work, struct ffs_io_data,\n\t\t\t\t\t\t   work);\n\tint ret = io_data->req->status ? io_data->req->status :\n\t\t\t\t\t io_data->req->actual;\n\tbool kiocb_has_eventfd = io_data->kiocb->ki_flags & IOCB_EVENTFD;\n\n\tif (io_data->read && ret > 0) {\n\t\tuse_mm(io_data->mm);\n\t\tret = copy_to_iter(io_data->buf, ret, &io_data->data);\n\t\tif (iov_iter_count(&io_data->data))\n\t\t\tret = -EFAULT;\n\t\tunuse_mm(io_data->mm);\n\t}\n\n\tio_data->kiocb->ki_complete(io_data->kiocb, ret, ret);\n\n\tif (io_data->ffs->ffs_eventfd && !kiocb_has_eventfd)\n\t\teventfd_signal(io_data->ffs->ffs_eventfd, 1);\n\n\tusb_ep_free_request(io_data->ep, io_data->req);\n\n\tif (io_data->read)\n\t\tkfree(io_data->to_free);\n\tkfree(io_data->buf);\n\tkfree(io_data);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \t\t\t\t\t\t   work);\n \tint ret = io_data->req->status ? io_data->req->status :\n \t\t\t\t\t io_data->req->actual;\n+\tbool kiocb_has_eventfd = io_data->kiocb->ki_flags & IOCB_EVENTFD;\n \n \tif (io_data->read && ret > 0) {\n \t\tuse_mm(io_data->mm);\n@@ -15,13 +16,11 @@\n \n \tio_data->kiocb->ki_complete(io_data->kiocb, ret, ret);\n \n-\tif (io_data->ffs->ffs_eventfd &&\n-\t    !(io_data->kiocb->ki_flags & IOCB_EVENTFD))\n+\tif (io_data->ffs->ffs_eventfd && !kiocb_has_eventfd)\n \t\teventfd_signal(io_data->ffs->ffs_eventfd, 1);\n \n \tusb_ep_free_request(io_data->ep, io_data->req);\n \n-\tio_data->kiocb->private = NULL;\n \tif (io_data->read)\n \t\tkfree(io_data->to_free);\n \tkfree(io_data->buf);",
        "function_modified_lines": {
            "added": [
                "\tbool kiocb_has_eventfd = io_data->kiocb->ki_flags & IOCB_EVENTFD;",
                "\tif (io_data->ffs->ffs_eventfd && !kiocb_has_eventfd)"
            ],
            "deleted": [
                "\tif (io_data->ffs->ffs_eventfd &&",
                "\t    !(io_data->kiocb->ki_flags & IOCB_EVENTFD))",
                "\tio_data->kiocb->private = NULL;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in the ffs_user_copy_worker function in drivers/usb/gadget/function/f_fs.c in the Linux kernel before 4.5.3 allows local users to gain privileges by accessing an I/O data structure after a certain callback call.",
        "id": 1111
    },
    {
        "cve_id": "CVE-2022-28893",
        "code_before_change": "static int xs_local_send_request(struct rpc_rqst *req)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tstruct sock_xprt *transport =\n\t\t\t\tcontainer_of(xprt, struct sock_xprt, xprt);\n\tstruct xdr_buf *xdr = &req->rq_snd_buf;\n\trpc_fraghdr rm = xs_stream_record_marker(xdr);\n\tunsigned int msglen = rm ? req->rq_slen + sizeof(rm) : req->rq_slen;\n\tstruct msghdr msg = {\n\t\t.msg_flags\t= XS_SENDMSG_FLAGS,\n\t};\n\tbool vm_wait;\n\tunsigned int sent;\n\tint status;\n\n\t/* Close the stream if the previous transmission was incomplete */\n\tif (xs_send_request_was_aborted(transport, req)) {\n\t\txs_close(xprt);\n\t\treturn -ENOTCONN;\n\t}\n\n\txs_pktdump(\"packet data:\",\n\t\t\treq->rq_svec->iov_base, req->rq_svec->iov_len);\n\n\tvm_wait = sk_stream_is_writeable(transport->inet) ? true : false;\n\n\treq->rq_xtime = ktime_get();\n\tstatus = xprt_sock_sendmsg(transport->sock, &msg, xdr,\n\t\t\t\t   transport->xmit.offset, rm, &sent);\n\tdprintk(\"RPC:       %s(%u) = %d\\n\",\n\t\t\t__func__, xdr->len - transport->xmit.offset, status);\n\n\tif (likely(sent > 0) || status == 0) {\n\t\ttransport->xmit.offset += sent;\n\t\treq->rq_bytes_sent = transport->xmit.offset;\n\t\tif (likely(req->rq_bytes_sent >= msglen)) {\n\t\t\treq->rq_xmit_bytes_sent += transport->xmit.offset;\n\t\t\ttransport->xmit.offset = 0;\n\t\t\treturn 0;\n\t\t}\n\t\tstatus = -EAGAIN;\n\t\tvm_wait = false;\n\t}\n\n\tswitch (status) {\n\tcase -EAGAIN:\n\t\tstatus = xs_stream_nospace(req, vm_wait);\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"RPC:       sendmsg returned unrecognized error %d\\n\",\n\t\t\t-status);\n\t\tfallthrough;\n\tcase -EPIPE:\n\t\txs_close(xprt);\n\t\tstatus = -ENOTCONN;\n\t}\n\n\treturn status;\n}",
        "code_after_change": "static int xs_local_send_request(struct rpc_rqst *req)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tstruct sock_xprt *transport =\n\t\t\t\tcontainer_of(xprt, struct sock_xprt, xprt);\n\tstruct xdr_buf *xdr = &req->rq_snd_buf;\n\trpc_fraghdr rm = xs_stream_record_marker(xdr);\n\tunsigned int msglen = rm ? req->rq_slen + sizeof(rm) : req->rq_slen;\n\tstruct msghdr msg = {\n\t\t.msg_flags\t= XS_SENDMSG_FLAGS,\n\t};\n\tbool vm_wait;\n\tunsigned int sent;\n\tint status;\n\n\t/* Close the stream if the previous transmission was incomplete */\n\tif (xs_send_request_was_aborted(transport, req)) {\n\t\txprt_force_disconnect(xprt);\n\t\treturn -ENOTCONN;\n\t}\n\n\txs_pktdump(\"packet data:\",\n\t\t\treq->rq_svec->iov_base, req->rq_svec->iov_len);\n\n\tvm_wait = sk_stream_is_writeable(transport->inet) ? true : false;\n\n\treq->rq_xtime = ktime_get();\n\tstatus = xprt_sock_sendmsg(transport->sock, &msg, xdr,\n\t\t\t\t   transport->xmit.offset, rm, &sent);\n\tdprintk(\"RPC:       %s(%u) = %d\\n\",\n\t\t\t__func__, xdr->len - transport->xmit.offset, status);\n\n\tif (likely(sent > 0) || status == 0) {\n\t\ttransport->xmit.offset += sent;\n\t\treq->rq_bytes_sent = transport->xmit.offset;\n\t\tif (likely(req->rq_bytes_sent >= msglen)) {\n\t\t\treq->rq_xmit_bytes_sent += transport->xmit.offset;\n\t\t\ttransport->xmit.offset = 0;\n\t\t\treturn 0;\n\t\t}\n\t\tstatus = -EAGAIN;\n\t\tvm_wait = false;\n\t}\n\n\tswitch (status) {\n\tcase -EAGAIN:\n\t\tstatus = xs_stream_nospace(req, vm_wait);\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"RPC:       sendmsg returned unrecognized error %d\\n\",\n\t\t\t-status);\n\t\tfallthrough;\n\tcase -EPIPE:\n\t\txprt_force_disconnect(xprt);\n\t\tstatus = -ENOTCONN;\n\t}\n\n\treturn status;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,7 +15,7 @@\n \n \t/* Close the stream if the previous transmission was incomplete */\n \tif (xs_send_request_was_aborted(transport, req)) {\n-\t\txs_close(xprt);\n+\t\txprt_force_disconnect(xprt);\n \t\treturn -ENOTCONN;\n \t}\n \n@@ -51,7 +51,7 @@\n \t\t\t-status);\n \t\tfallthrough;\n \tcase -EPIPE:\n-\t\txs_close(xprt);\n+\t\txprt_force_disconnect(xprt);\n \t\tstatus = -ENOTCONN;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\txprt_force_disconnect(xprt);",
                "\t\txprt_force_disconnect(xprt);"
            ],
            "deleted": [
                "\t\txs_close(xprt);",
                "\t\txs_close(xprt);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The SUNRPC subsystem in the Linux kernel through 5.17.2 can call xs_xprt_free before ensuring that sockets are in the intended state.",
        "id": 3508
    },
    {
        "cve_id": "CVE-2019-19770",
        "code_before_change": "static int do_blk_trace_setup(struct request_queue *q, char *name, dev_t dev,\n\t\t\t      struct block_device *bdev,\n\t\t\t      struct blk_user_trace_setup *buts)\n{\n\tstruct blk_trace *bt = NULL;\n\tstruct dentry *dir = NULL;\n\tint ret;\n\n\tlockdep_assert_held(&q->blk_trace_mutex);\n\n\tif (!buts->buf_size || !buts->buf_nr)\n\t\treturn -EINVAL;\n\n\tif (!blk_debugfs_root)\n\t\treturn -ENOENT;\n\n\tstrncpy(buts->name, name, BLKTRACE_BDEV_SIZE);\n\tbuts->name[BLKTRACE_BDEV_SIZE - 1] = '\\0';\n\n\t/*\n\t * some device names have larger paths - convert the slashes\n\t * to underscores for this to work as expected\n\t */\n\tstrreplace(buts->name, '/', '_');\n\n\t/*\n\t * bdev can be NULL, as with scsi-generic, this is a helpful as\n\t * we can be.\n\t */\n\tif (rcu_dereference_protected(q->blk_trace,\n\t\t\t\t      lockdep_is_held(&q->blk_trace_mutex))) {\n\t\tpr_warn(\"Concurrent blktraces are not allowed on %s\\n\",\n\t\t\tbuts->name);\n\t\treturn -EBUSY;\n\t}\n\n\tbt = kzalloc(sizeof(*bt), GFP_KERNEL);\n\tif (!bt)\n\t\treturn -ENOMEM;\n\n\tret = -ENOMEM;\n\tbt->sequence = alloc_percpu(unsigned long);\n\tif (!bt->sequence)\n\t\tgoto err;\n\n\tbt->msg_data = __alloc_percpu(BLK_TN_MAX_MSG, __alignof__(char));\n\tif (!bt->msg_data)\n\t\tgoto err;\n\n\tret = -ENOENT;\n\n\tdir = debugfs_lookup(buts->name, blk_debugfs_root);\n\tif (!dir)\n\t\tbt->dir = dir = debugfs_create_dir(buts->name, blk_debugfs_root);\n\n\tbt->dev = dev;\n\tatomic_set(&bt->dropped, 0);\n\tINIT_LIST_HEAD(&bt->running_list);\n\n\tret = -EIO;\n\tbt->dropped_file = debugfs_create_file(\"dropped\", 0444, dir, bt,\n\t\t\t\t\t       &blk_dropped_fops);\n\n\tbt->msg_file = debugfs_create_file(\"msg\", 0222, dir, bt, &blk_msg_fops);\n\n\tbt->rchan = relay_open(\"trace\", dir, buts->buf_size,\n\t\t\t\tbuts->buf_nr, &blk_relay_callbacks, bt);\n\tif (!bt->rchan)\n\t\tgoto err;\n\n\tbt->act_mask = buts->act_mask;\n\tif (!bt->act_mask)\n\t\tbt->act_mask = (u16) -1;\n\n\tblk_trace_setup_lba(bt, bdev);\n\n\t/* overwrite with user settings */\n\tif (buts->start_lba)\n\t\tbt->start_lba = buts->start_lba;\n\tif (buts->end_lba)\n\t\tbt->end_lba = buts->end_lba;\n\n\tbt->pid = buts->pid;\n\tbt->trace_state = Blktrace_setup;\n\n\trcu_assign_pointer(q->blk_trace, bt);\n\tget_probe_ref();\n\n\tret = 0;\nerr:\n\tif (dir && !bt->dir)\n\t\tdput(dir);\n\tif (ret)\n\t\tblk_trace_free(bt);\n\treturn ret;\n}",
        "code_after_change": "static int do_blk_trace_setup(struct request_queue *q, char *name, dev_t dev,\n\t\t\t      struct block_device *bdev,\n\t\t\t      struct blk_user_trace_setup *buts)\n{\n\tstruct blk_trace *bt = NULL;\n\tstruct dentry *dir = NULL;\n\tint ret;\n\n\tlockdep_assert_held(&q->blk_trace_mutex);\n\n\tif (!buts->buf_size || !buts->buf_nr)\n\t\treturn -EINVAL;\n\n\tif (!blk_debugfs_root)\n\t\treturn -ENOENT;\n\n\tstrncpy(buts->name, name, BLKTRACE_BDEV_SIZE);\n\tbuts->name[BLKTRACE_BDEV_SIZE - 1] = '\\0';\n\n\t/*\n\t * some device names have larger paths - convert the slashes\n\t * to underscores for this to work as expected\n\t */\n\tstrreplace(buts->name, '/', '_');\n\n\t/*\n\t * bdev can be NULL, as with scsi-generic, this is a helpful as\n\t * we can be.\n\t */\n\tif (rcu_dereference_protected(q->blk_trace,\n\t\t\t\t      lockdep_is_held(&q->blk_trace_mutex))) {\n\t\tpr_warn(\"Concurrent blktraces are not allowed on %s\\n\",\n\t\t\tbuts->name);\n\t\treturn -EBUSY;\n\t}\n\n\tbt = kzalloc(sizeof(*bt), GFP_KERNEL);\n\tif (!bt)\n\t\treturn -ENOMEM;\n\n\tret = -ENOMEM;\n\tbt->sequence = alloc_percpu(unsigned long);\n\tif (!bt->sequence)\n\t\tgoto err;\n\n\tbt->msg_data = __alloc_percpu(BLK_TN_MAX_MSG, __alignof__(char));\n\tif (!bt->msg_data)\n\t\tgoto err;\n\n#ifdef CONFIG_BLK_DEBUG_FS\n\t/*\n\t * When tracing whole make_request drivers (multiqueue) block devices,\n\t * reuse the existing debugfs directory created by the block layer on\n\t * init. For request-based block devices, all partitions block devices,\n\t * and scsi-generic block devices we create a temporary new debugfs\n\t * directory that will be removed once the trace ends.\n\t */\n\tif (queue_is_mq(q) && bdev && bdev == bdev->bd_contains)\n\t\tdir = q->debugfs_dir;\n\telse\n#endif\n\t\tbt->dir = dir = debugfs_create_dir(buts->name, blk_debugfs_root);\n\n\tbt->dev = dev;\n\tatomic_set(&bt->dropped, 0);\n\tINIT_LIST_HEAD(&bt->running_list);\n\n\tret = -EIO;\n\tbt->dropped_file = debugfs_create_file(\"dropped\", 0444, dir, bt,\n\t\t\t\t\t       &blk_dropped_fops);\n\n\tbt->msg_file = debugfs_create_file(\"msg\", 0222, dir, bt, &blk_msg_fops);\n\n\tbt->rchan = relay_open(\"trace\", dir, buts->buf_size,\n\t\t\t\tbuts->buf_nr, &blk_relay_callbacks, bt);\n\tif (!bt->rchan)\n\t\tgoto err;\n\n\tbt->act_mask = buts->act_mask;\n\tif (!bt->act_mask)\n\t\tbt->act_mask = (u16) -1;\n\n\tblk_trace_setup_lba(bt, bdev);\n\n\t/* overwrite with user settings */\n\tif (buts->start_lba)\n\t\tbt->start_lba = buts->start_lba;\n\tif (buts->end_lba)\n\t\tbt->end_lba = buts->end_lba;\n\n\tbt->pid = buts->pid;\n\tbt->trace_state = Blktrace_setup;\n\n\trcu_assign_pointer(q->blk_trace, bt);\n\tget_probe_ref();\n\n\tret = 0;\nerr:\n\tif (ret)\n\t\tblk_trace_free(bt);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,10 +47,18 @@\n \tif (!bt->msg_data)\n \t\tgoto err;\n \n-\tret = -ENOENT;\n-\n-\tdir = debugfs_lookup(buts->name, blk_debugfs_root);\n-\tif (!dir)\n+#ifdef CONFIG_BLK_DEBUG_FS\n+\t/*\n+\t * When tracing whole make_request drivers (multiqueue) block devices,\n+\t * reuse the existing debugfs directory created by the block layer on\n+\t * init. For request-based block devices, all partitions block devices,\n+\t * and scsi-generic block devices we create a temporary new debugfs\n+\t * directory that will be removed once the trace ends.\n+\t */\n+\tif (queue_is_mq(q) && bdev && bdev == bdev->bd_contains)\n+\t\tdir = q->debugfs_dir;\n+\telse\n+#endif\n \t\tbt->dir = dir = debugfs_create_dir(buts->name, blk_debugfs_root);\n \n \tbt->dev = dev;\n@@ -88,8 +96,6 @@\n \n \tret = 0;\n err:\n-\tif (dir && !bt->dir)\n-\t\tdput(dir);\n \tif (ret)\n \t\tblk_trace_free(bt);\n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "#ifdef CONFIG_BLK_DEBUG_FS",
                "\t/*",
                "\t * When tracing whole make_request drivers (multiqueue) block devices,",
                "\t * reuse the existing debugfs directory created by the block layer on",
                "\t * init. For request-based block devices, all partitions block devices,",
                "\t * and scsi-generic block devices we create a temporary new debugfs",
                "\t * directory that will be removed once the trace ends.",
                "\t */",
                "\tif (queue_is_mq(q) && bdev && bdev == bdev->bd_contains)",
                "\t\tdir = q->debugfs_dir;",
                "\telse",
                "#endif"
            ],
            "deleted": [
                "\tret = -ENOENT;",
                "",
                "\tdir = debugfs_lookup(buts->name, blk_debugfs_root);",
                "\tif (!dir)",
                "\tif (dir && !bt->dir)",
                "\t\tdput(dir);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 4.19.83, there is a use-after-free (read) in the debugfs_remove function in fs/debugfs/inode.c (which is used to remove a file or directory in debugfs that was previously created with a call to another debugfs function such as debugfs_create_file). NOTE: Linux kernel developers dispute this issue as not being an issue with debugfs, instead this is an issue with misuse of debugfs within blktrace",
        "id": 2243
    },
    {
        "cve_id": "CVE-2023-51782",
        "code_before_change": "static int rose_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rose_sock *rose = rose_sk(sk);\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (cmd) {\n\tcase TIOCOUTQ: {\n\t\tlong amount;\n\n\t\tamount = sk->sk_sndbuf - sk_wmem_alloc_get(sk);\n\t\tif (amount < 0)\n\t\t\tamount = 0;\n\t\treturn put_user(amount, (unsigned int __user *) argp);\n\t}\n\n\tcase TIOCINQ: {\n\t\tstruct sk_buff *skb;\n\t\tlong amount = 0L;\n\t\t/* These two are safe on a single CPU system as only user tasks fiddle here */\n\t\tif ((skb = skb_peek(&sk->sk_receive_queue)) != NULL)\n\t\t\tamount = skb->len;\n\t\treturn put_user(amount, (unsigned int __user *) argp);\n\t}\n\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCGIFMETRIC:\n\tcase SIOCSIFMETRIC:\n\t\treturn -EINVAL;\n\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\tcase SIOCRSCLRRT:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\treturn rose_rt_ioctl(cmd, argp);\n\n\tcase SIOCRSGCAUSE: {\n\t\tstruct rose_cause_struct rose_cause;\n\t\trose_cause.cause      = rose->cause;\n\t\trose_cause.diagnostic = rose->diagnostic;\n\t\treturn copy_to_user(argp, &rose_cause, sizeof(struct rose_cause_struct)) ? -EFAULT : 0;\n\t}\n\n\tcase SIOCRSSCAUSE: {\n\t\tstruct rose_cause_struct rose_cause;\n\t\tif (copy_from_user(&rose_cause, argp, sizeof(struct rose_cause_struct)))\n\t\t\treturn -EFAULT;\n\t\trose->cause      = rose_cause.cause;\n\t\trose->diagnostic = rose_cause.diagnostic;\n\t\treturn 0;\n\t}\n\n\tcase SIOCRSSL2CALL:\n\t\tif (!capable(CAP_NET_ADMIN)) return -EPERM;\n\t\tif (ax25cmp(&rose_callsign, &null_ax25_address) != 0)\n\t\t\tax25_listen_release(&rose_callsign, NULL);\n\t\tif (copy_from_user(&rose_callsign, argp, sizeof(ax25_address)))\n\t\t\treturn -EFAULT;\n\t\tif (ax25cmp(&rose_callsign, &null_ax25_address) != 0)\n\t\t\treturn ax25_listen_register(&rose_callsign, NULL);\n\n\t\treturn 0;\n\n\tcase SIOCRSGL2CALL:\n\t\treturn copy_to_user(argp, &rose_callsign, sizeof(ax25_address)) ? -EFAULT : 0;\n\n\tcase SIOCRSACCEPT:\n\t\tif (rose->state == ROSE_STATE_5) {\n\t\t\trose_write_internal(sk, ROSE_CALL_ACCEPTED);\n\t\t\trose_start_idletimer(sk);\n\t\t\trose->condition = 0x00;\n\t\t\trose->vs        = 0;\n\t\t\trose->va        = 0;\n\t\t\trose->vr        = 0;\n\t\t\trose->vl        = 0;\n\t\t\trose->state     = ROSE_STATE_3;\n\t\t}\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int rose_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rose_sock *rose = rose_sk(sk);\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (cmd) {\n\tcase TIOCOUTQ: {\n\t\tlong amount;\n\n\t\tamount = sk->sk_sndbuf - sk_wmem_alloc_get(sk);\n\t\tif (amount < 0)\n\t\t\tamount = 0;\n\t\treturn put_user(amount, (unsigned int __user *) argp);\n\t}\n\n\tcase TIOCINQ: {\n\t\tstruct sk_buff *skb;\n\t\tlong amount = 0L;\n\n\t\tspin_lock_irq(&sk->sk_receive_queue.lock);\n\t\tif ((skb = skb_peek(&sk->sk_receive_queue)) != NULL)\n\t\t\tamount = skb->len;\n\t\tspin_unlock_irq(&sk->sk_receive_queue.lock);\n\t\treturn put_user(amount, (unsigned int __user *) argp);\n\t}\n\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCGIFMETRIC:\n\tcase SIOCSIFMETRIC:\n\t\treturn -EINVAL;\n\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\tcase SIOCRSCLRRT:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\treturn rose_rt_ioctl(cmd, argp);\n\n\tcase SIOCRSGCAUSE: {\n\t\tstruct rose_cause_struct rose_cause;\n\t\trose_cause.cause      = rose->cause;\n\t\trose_cause.diagnostic = rose->diagnostic;\n\t\treturn copy_to_user(argp, &rose_cause, sizeof(struct rose_cause_struct)) ? -EFAULT : 0;\n\t}\n\n\tcase SIOCRSSCAUSE: {\n\t\tstruct rose_cause_struct rose_cause;\n\t\tif (copy_from_user(&rose_cause, argp, sizeof(struct rose_cause_struct)))\n\t\t\treturn -EFAULT;\n\t\trose->cause      = rose_cause.cause;\n\t\trose->diagnostic = rose_cause.diagnostic;\n\t\treturn 0;\n\t}\n\n\tcase SIOCRSSL2CALL:\n\t\tif (!capable(CAP_NET_ADMIN)) return -EPERM;\n\t\tif (ax25cmp(&rose_callsign, &null_ax25_address) != 0)\n\t\t\tax25_listen_release(&rose_callsign, NULL);\n\t\tif (copy_from_user(&rose_callsign, argp, sizeof(ax25_address)))\n\t\t\treturn -EFAULT;\n\t\tif (ax25cmp(&rose_callsign, &null_ax25_address) != 0)\n\t\t\treturn ax25_listen_register(&rose_callsign, NULL);\n\n\t\treturn 0;\n\n\tcase SIOCRSGL2CALL:\n\t\treturn copy_to_user(argp, &rose_callsign, sizeof(ax25_address)) ? -EFAULT : 0;\n\n\tcase SIOCRSACCEPT:\n\t\tif (rose->state == ROSE_STATE_5) {\n\t\t\trose_write_internal(sk, ROSE_CALL_ACCEPTED);\n\t\t\trose_start_idletimer(sk);\n\t\t\trose->condition = 0x00;\n\t\t\trose->vs        = 0;\n\t\t\trose->va        = 0;\n\t\t\trose->vr        = 0;\n\t\t\trose->vl        = 0;\n\t\t\trose->state     = ROSE_STATE_3;\n\t\t}\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,9 +17,11 @@\n \tcase TIOCINQ: {\n \t\tstruct sk_buff *skb;\n \t\tlong amount = 0L;\n-\t\t/* These two are safe on a single CPU system as only user tasks fiddle here */\n+\n+\t\tspin_lock_irq(&sk->sk_receive_queue.lock);\n \t\tif ((skb = skb_peek(&sk->sk_receive_queue)) != NULL)\n \t\t\tamount = skb->len;\n+\t\tspin_unlock_irq(&sk->sk_receive_queue.lock);\n \t\treturn put_user(amount, (unsigned int __user *) argp);\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\t\tspin_lock_irq(&sk->sk_receive_queue.lock);",
                "\t\tspin_unlock_irq(&sk->sk_receive_queue.lock);"
            ],
            "deleted": [
                "\t\t/* These two are safe on a single CPU system as only user tasks fiddle here */"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.6.8. rose_ioctl in net/rose/af_rose.c has a use-after-free because of a rose_accept race condition.",
        "id": 4260
    },
    {
        "cve_id": "CVE-2023-1989",
        "code_before_change": "static void btsdio_remove(struct sdio_func *func)\n{\n\tstruct btsdio_data *data = sdio_get_drvdata(func);\n\tstruct hci_dev *hdev;\n\n\tBT_DBG(\"func %p\", func);\n\n\tif (!data)\n\t\treturn;\n\n\thdev = data->hdev;\n\n\tsdio_set_drvdata(func, NULL);\n\n\thci_unregister_dev(hdev);\n\n\thci_free_dev(hdev);\n}",
        "code_after_change": "static void btsdio_remove(struct sdio_func *func)\n{\n\tstruct btsdio_data *data = sdio_get_drvdata(func);\n\tstruct hci_dev *hdev;\n\n\tBT_DBG(\"func %p\", func);\n\n\tcancel_work_sync(&data->work);\n\tif (!data)\n\t\treturn;\n\n\thdev = data->hdev;\n\n\tsdio_set_drvdata(func, NULL);\n\n\thci_unregister_dev(hdev);\n\n\thci_free_dev(hdev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \n \tBT_DBG(\"func %p\", func);\n \n+\tcancel_work_sync(&data->work);\n \tif (!data)\n \t\treturn;\n ",
        "function_modified_lines": {
            "added": [
                "\tcancel_work_sync(&data->work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in btsdio_remove in drivers\\bluetooth\\btsdio.c in the Linux Kernel. In this flaw, a call to btsdio_remove with an unfinished job, may cause a race problem leading to a UAF on hdev devices.",
        "id": 3888
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "int vmw_user_bo_lookup(struct drm_file *filp,\n\t\t       u32 handle,\n\t\t       struct vmw_bo **out)\n{\n\tstruct drm_gem_object *gobj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj) {\n\t\tDRM_ERROR(\"Invalid buffer object handle 0x%08lx.\\n\",\n\t\t\t  (unsigned long)handle);\n\t\treturn -ESRCH;\n\t}\n\n\t*out = to_vmw_bo(gobj);\n\tttm_bo_get(&(*out)->tbo);\n\n\treturn 0;\n}",
        "code_after_change": "int vmw_user_bo_lookup(struct drm_file *filp,\n\t\t       u32 handle,\n\t\t       struct vmw_bo **out)\n{\n\tstruct drm_gem_object *gobj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj) {\n\t\tDRM_ERROR(\"Invalid buffer object handle 0x%08lx.\\n\",\n\t\t\t  (unsigned long)handle);\n\t\treturn -ESRCH;\n\t}\n\n\t*out = to_vmw_bo(gobj);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,6 @@\n \t}\n \n \t*out = to_vmw_bo(gobj);\n-\tttm_bo_get(&(*out)->tbo);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tttm_bo_get(&(*out)->tbo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4269
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n\n\t/* Grab a ref if this isn't our static identity */\n\treq->work.identity = tctx->identity;\n\tif (tctx->identity != &tctx->__identity)\n\t\trefcount_inc(&req->work.identity->count);\n}",
        "code_after_change": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,14 +1,7 @@\n static inline void io_req_init_async(struct io_kiocb *req)\n {\n-\tstruct io_uring_task *tctx = current->io_uring;\n-\n \tif (req->flags & REQ_F_WORK_INITIALIZED)\n \t\treturn;\n \n \t__io_req_init_async(req);\n-\n-\t/* Grab a ref if this isn't our static identity */\n-\treq->work.identity = tctx->identity;\n-\tif (tctx->identity != &tctx->__identity)\n-\t\trefcount_inc(&req->work.identity->count);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct io_uring_task *tctx = current->io_uring;",
                "",
                "",
                "\t/* Grab a ref if this isn't our static identity */",
                "\treq->work.identity = tctx->identity;",
                "\tif (tctx->identity != &tctx->__identity)",
                "\t\trefcount_inc(&req->work.identity->count);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3284
    },
    {
        "cve_id": "CVE-2023-20928",
        "code_before_change": "static inline void binder_alloc_set_vma(struct binder_alloc *alloc,\n\t\tstruct vm_area_struct *vma)\n{\n\tif (vma)\n\t\talloc->vma_vm_mm = vma->vm_mm;\n\t/*\n\t * If we see alloc->vma is not NULL, buffer data structures set up\n\t * completely. Look at smp_rmb side binder_alloc_get_vma.\n\t * We also want to guarantee new alloc->vma_vm_mm is always visible\n\t * if alloc->vma is set.\n\t */\n\tsmp_wmb();\n\talloc->vma = vma;\n}",
        "code_after_change": "static inline void binder_alloc_set_vma(struct binder_alloc *alloc,\n\t\tstruct vm_area_struct *vma)\n{\n\tunsigned long vm_start = 0;\n\n\tif (vma) {\n\t\tvm_start = vma->vm_start;\n\t\talloc->vma_vm_mm = vma->vm_mm;\n\t}\n\n\tmmap_assert_write_locked(alloc->vma_vm_mm);\n\talloc->vma_addr = vm_start;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,14 +1,13 @@\n static inline void binder_alloc_set_vma(struct binder_alloc *alloc,\n \t\tstruct vm_area_struct *vma)\n {\n-\tif (vma)\n+\tunsigned long vm_start = 0;\n+\n+\tif (vma) {\n+\t\tvm_start = vma->vm_start;\n \t\talloc->vma_vm_mm = vma->vm_mm;\n-\t/*\n-\t * If we see alloc->vma is not NULL, buffer data structures set up\n-\t * completely. Look at smp_rmb side binder_alloc_get_vma.\n-\t * We also want to guarantee new alloc->vma_vm_mm is always visible\n-\t * if alloc->vma is set.\n-\t */\n-\tsmp_wmb();\n-\talloc->vma = vma;\n+\t}\n+\n+\tmmap_assert_write_locked(alloc->vma_vm_mm);\n+\talloc->vma_addr = vm_start;\n }",
        "function_modified_lines": {
            "added": [
                "\tunsigned long vm_start = 0;",
                "",
                "\tif (vma) {",
                "\t\tvm_start = vma->vm_start;",
                "\t}",
                "",
                "\tmmap_assert_write_locked(alloc->vma_vm_mm);",
                "\talloc->vma_addr = vm_start;"
            ],
            "deleted": [
                "\tif (vma)",
                "\t/*",
                "\t * If we see alloc->vma is not NULL, buffer data structures set up",
                "\t * completely. Look at smp_rmb side binder_alloc_get_vma.",
                "\t * We also want to guarantee new alloc->vma_vm_mm is always visible",
                "\t * if alloc->vma is set.",
                "\t */",
                "\tsmp_wmb();",
                "\talloc->vma = vma;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_vma_close of binder.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-254837884References: Upstream kernel",
        "id": 3908
    },
    {
        "cve_id": "CVE-2016-4805",
        "code_before_change": "int ppp_register_net_channel(struct net *net, struct ppp_channel *chan)\n{\n\tstruct channel *pch;\n\tstruct ppp_net *pn;\n\n\tpch = kzalloc(sizeof(struct channel), GFP_KERNEL);\n\tif (!pch)\n\t\treturn -ENOMEM;\n\n\tpn = ppp_pernet(net);\n\n\tpch->ppp = NULL;\n\tpch->chan = chan;\n\tpch->chan_net = net;\n\tchan->ppp = pch;\n\tinit_ppp_file(&pch->file, CHANNEL);\n\tpch->file.hdrlen = chan->hdrlen;\n#ifdef CONFIG_PPP_MULTILINK\n\tpch->lastseq = -1;\n#endif /* CONFIG_PPP_MULTILINK */\n\tinit_rwsem(&pch->chan_sem);\n\tspin_lock_init(&pch->downl);\n\trwlock_init(&pch->upl);\n\n\tspin_lock_bh(&pn->all_channels_lock);\n\tpch->file.index = ++pn->last_channel_index;\n\tlist_add(&pch->list, &pn->new_channels);\n\tatomic_inc(&channel_count);\n\tspin_unlock_bh(&pn->all_channels_lock);\n\n\treturn 0;\n}",
        "code_after_change": "int ppp_register_net_channel(struct net *net, struct ppp_channel *chan)\n{\n\tstruct channel *pch;\n\tstruct ppp_net *pn;\n\n\tpch = kzalloc(sizeof(struct channel), GFP_KERNEL);\n\tif (!pch)\n\t\treturn -ENOMEM;\n\n\tpn = ppp_pernet(net);\n\n\tpch->ppp = NULL;\n\tpch->chan = chan;\n\tpch->chan_net = get_net(net);\n\tchan->ppp = pch;\n\tinit_ppp_file(&pch->file, CHANNEL);\n\tpch->file.hdrlen = chan->hdrlen;\n#ifdef CONFIG_PPP_MULTILINK\n\tpch->lastseq = -1;\n#endif /* CONFIG_PPP_MULTILINK */\n\tinit_rwsem(&pch->chan_sem);\n\tspin_lock_init(&pch->downl);\n\trwlock_init(&pch->upl);\n\n\tspin_lock_bh(&pn->all_channels_lock);\n\tpch->file.index = ++pn->last_channel_index;\n\tlist_add(&pch->list, &pn->new_channels);\n\tatomic_inc(&channel_count);\n\tspin_unlock_bh(&pn->all_channels_lock);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \n \tpch->ppp = NULL;\n \tpch->chan = chan;\n-\tpch->chan_net = net;\n+\tpch->chan_net = get_net(net);\n \tchan->ppp = pch;\n \tinit_ppp_file(&pch->file, CHANNEL);\n \tpch->file.hdrlen = chan->hdrlen;",
        "function_modified_lines": {
            "added": [
                "\tpch->chan_net = get_net(net);"
            ],
            "deleted": [
                "\tpch->chan_net = net;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in drivers/net/ppp/ppp_generic.c in the Linux kernel before 4.5.2 allows local users to cause a denial of service (memory corruption and system crash, or spinlock) or possibly have unspecified other impact by removing a network namespace, related to the ppp_register_net_channel and ppp_unregister_channel functions.",
        "id": 1036
    },
    {
        "cve_id": "CVE-2024-0193",
        "code_before_change": "static void nft_set_commit_update(struct list_head *set_update_list)\n{\n\tstruct nft_set *set, *next;\n\n\tlist_for_each_entry_safe(set, next, set_update_list, pending_update) {\n\t\tlist_del_init(&set->pending_update);\n\n\t\tif (!set->ops->commit)\n\t\t\tcontinue;\n\n\t\tset->ops->commit(set);\n\t}\n}",
        "code_after_change": "static void nft_set_commit_update(struct list_head *set_update_list)\n{\n\tstruct nft_set *set, *next;\n\n\tlist_for_each_entry_safe(set, next, set_update_list, pending_update) {\n\t\tlist_del_init(&set->pending_update);\n\n\t\tif (!set->ops->commit || set->dead)\n\t\t\tcontinue;\n\n\t\tset->ops->commit(set);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \tlist_for_each_entry_safe(set, next, set_update_list, pending_update) {\n \t\tlist_del_init(&set->pending_update);\n \n-\t\tif (!set->ops->commit)\n+\t\tif (!set->ops->commit || set->dead)\n \t\t\tcontinue;\n \n \t\tset->ops->commit(set);",
        "function_modified_lines": {
            "added": [
                "\t\tif (!set->ops->commit || set->dead)"
            ],
            "deleted": [
                "\t\tif (!set->ops->commit)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the netfilter subsystem of the Linux kernel. If the catchall element is garbage-collected when the pipapo set is removed, the element can be deactivated twice. This can cause a use-after-free issue on an NFT_CHAIN object or NFT_OBJECT object, allowing a local unprivileged user with CAP_NET_ADMIN capability to escalate their privileges on the system.",
        "id": 4313
    },
    {
        "cve_id": "CVE-2017-7374",
        "code_before_change": "int fscrypt_setup_filename(struct inode *dir, const struct qstr *iname,\n\t\t\t      int lookup, struct fscrypt_name *fname)\n{\n\tint ret = 0, bigname = 0;\n\n\tmemset(fname, 0, sizeof(struct fscrypt_name));\n\tfname->usr_fname = iname;\n\n\tif (!dir->i_sb->s_cop->is_encrypted(dir) ||\n\t\t\t\tfscrypt_is_dot_dotdot(iname)) {\n\t\tfname->disk_name.name = (unsigned char *)iname->name;\n\t\tfname->disk_name.len = iname->len;\n\t\treturn 0;\n\t}\n\tret = fscrypt_get_crypt_info(dir);\n\tif (ret && ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\tif (dir->i_crypt_info) {\n\t\tret = fscrypt_fname_alloc_buffer(dir, iname->len,\n\t\t\t\t\t\t\t&fname->crypto_buf);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = fname_encrypt(dir, iname, &fname->crypto_buf);\n\t\tif (ret)\n\t\t\tgoto errout;\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t\treturn 0;\n\t}\n\tif (!lookup)\n\t\treturn -ENOKEY;\n\n\t/*\n\t * We don't have the key and we are doing a lookup; decode the\n\t * user-supplied name\n\t */\n\tif (iname->name[0] == '_')\n\t\tbigname = 1;\n\tif ((bigname && (iname->len != 33)) || (!bigname && (iname->len > 43)))\n\t\treturn -ENOENT;\n\n\tfname->crypto_buf.name = kmalloc(32, GFP_KERNEL);\n\tif (fname->crypto_buf.name == NULL)\n\t\treturn -ENOMEM;\n\n\tret = digest_decode(iname->name + bigname, iname->len - bigname,\n\t\t\t\tfname->crypto_buf.name);\n\tif (ret < 0) {\n\t\tret = -ENOENT;\n\t\tgoto errout;\n\t}\n\tfname->crypto_buf.len = ret;\n\tif (bigname) {\n\t\tmemcpy(&fname->hash, fname->crypto_buf.name, 4);\n\t\tmemcpy(&fname->minor_hash, fname->crypto_buf.name + 4, 4);\n\t} else {\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t}\n\treturn 0;\n\nerrout:\n\tfscrypt_fname_free_buffer(&fname->crypto_buf);\n\treturn ret;\n}",
        "code_after_change": "int fscrypt_setup_filename(struct inode *dir, const struct qstr *iname,\n\t\t\t      int lookup, struct fscrypt_name *fname)\n{\n\tint ret = 0, bigname = 0;\n\n\tmemset(fname, 0, sizeof(struct fscrypt_name));\n\tfname->usr_fname = iname;\n\n\tif (!dir->i_sb->s_cop->is_encrypted(dir) ||\n\t\t\t\tfscrypt_is_dot_dotdot(iname)) {\n\t\tfname->disk_name.name = (unsigned char *)iname->name;\n\t\tfname->disk_name.len = iname->len;\n\t\treturn 0;\n\t}\n\tret = fscrypt_get_encryption_info(dir);\n\tif (ret && ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\tif (dir->i_crypt_info) {\n\t\tret = fscrypt_fname_alloc_buffer(dir, iname->len,\n\t\t\t\t\t\t\t&fname->crypto_buf);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = fname_encrypt(dir, iname, &fname->crypto_buf);\n\t\tif (ret)\n\t\t\tgoto errout;\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t\treturn 0;\n\t}\n\tif (!lookup)\n\t\treturn -ENOKEY;\n\n\t/*\n\t * We don't have the key and we are doing a lookup; decode the\n\t * user-supplied name\n\t */\n\tif (iname->name[0] == '_')\n\t\tbigname = 1;\n\tif ((bigname && (iname->len != 33)) || (!bigname && (iname->len > 43)))\n\t\treturn -ENOENT;\n\n\tfname->crypto_buf.name = kmalloc(32, GFP_KERNEL);\n\tif (fname->crypto_buf.name == NULL)\n\t\treturn -ENOMEM;\n\n\tret = digest_decode(iname->name + bigname, iname->len - bigname,\n\t\t\t\tfname->crypto_buf.name);\n\tif (ret < 0) {\n\t\tret = -ENOENT;\n\t\tgoto errout;\n\t}\n\tfname->crypto_buf.len = ret;\n\tif (bigname) {\n\t\tmemcpy(&fname->hash, fname->crypto_buf.name, 4);\n\t\tmemcpy(&fname->minor_hash, fname->crypto_buf.name + 4, 4);\n\t} else {\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t}\n\treturn 0;\n\nerrout:\n\tfscrypt_fname_free_buffer(&fname->crypto_buf);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,7 @@\n \t\tfname->disk_name.len = iname->len;\n \t\treturn 0;\n \t}\n-\tret = fscrypt_get_crypt_info(dir);\n+\tret = fscrypt_get_encryption_info(dir);\n \tif (ret && ret != -EOPNOTSUPP)\n \t\treturn ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tret = fscrypt_get_encryption_info(dir);"
            ],
            "deleted": [
                "\tret = fscrypt_get_crypt_info(dir);"
            ]
        },
        "cwe": [
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in fs/crypto/ in the Linux kernel before 4.10.7 allows local users to cause a denial of service (NULL pointer dereference) or possibly gain privileges by revoking keyring keys being used for ext4, f2fs, or ubifs encryption, causing cryptographic transform objects to be freed prematurely.",
        "id": 1498
    },
    {
        "cve_id": "CVE-2023-1079",
        "code_before_change": "static void asus_kbd_backlight_work(struct work_struct *work)\n{\n\tstruct asus_kbd_leds *led = container_of(work, struct asus_kbd_leds, work);\n\tu8 buf[] = { FEATURE_KBD_REPORT_ID, 0xba, 0xc5, 0xc4, 0x00 };\n\tint ret;\n\tunsigned long flags;\n\n\tif (led->removed)\n\t\treturn;\n\n\tspin_lock_irqsave(&led->lock, flags);\n\tbuf[4] = led->brightness;\n\tspin_unlock_irqrestore(&led->lock, flags);\n\n\tret = asus_kbd_set_report(led->hdev, buf, sizeof(buf));\n\tif (ret < 0)\n\t\thid_err(led->hdev, \"Asus failed to set keyboard backlight: %d\\n\", ret);\n}",
        "code_after_change": "static void asus_kbd_backlight_work(struct work_struct *work)\n{\n\tstruct asus_kbd_leds *led = container_of(work, struct asus_kbd_leds, work);\n\tu8 buf[] = { FEATURE_KBD_REPORT_ID, 0xba, 0xc5, 0xc4, 0x00 };\n\tint ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&led->lock, flags);\n\tbuf[4] = led->brightness;\n\tspin_unlock_irqrestore(&led->lock, flags);\n\n\tret = asus_kbd_set_report(led->hdev, buf, sizeof(buf));\n\tif (ret < 0)\n\t\thid_err(led->hdev, \"Asus failed to set keyboard backlight: %d\\n\", ret);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,9 +4,6 @@\n \tu8 buf[] = { FEATURE_KBD_REPORT_ID, 0xba, 0xc5, 0xc4, 0x00 };\n \tint ret;\n \tunsigned long flags;\n-\n-\tif (led->removed)\n-\t\treturn;\n \n \tspin_lock_irqsave(&led->lock, flags);\n \tbuf[4] = led->brightness;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tif (led->removed)",
                "\t\treturn;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel. A use-after-free may be triggered in asus_kbd_backlight_set when plugging/disconnecting in a malicious USB device, which advertises itself as an Asus device. Similarly to the previous known CVE-2023-25012, but in asus devices, the work_struct may be scheduled by the LED controller while the device is disconnecting, triggering a use-after-free on the struct asus_kbd_leds *led structure. A malicious USB device may exploit the issue to cause memory corruption with controlled data.",
        "id": 3847
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "int io_poll_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_poll_update *poll_update = io_kiocb_to_cmd(req);\n\tstruct io_cancel_data cd = { .data = poll_update->old_user_data, };\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_hash_bucket *bucket;\n\tstruct io_kiocb *preq;\n\tint ret2, ret = 0;\n\tbool locked;\n\n\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table, &bucket);\n\tif (preq)\n\t\tret2 = io_poll_disarm(preq);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\n\tif (!preq) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\tif (!ret2) {\n\t\tret = -EALREADY;\n\t\tgoto out;\n\t}\n\n\tif (poll_update->update_events || poll_update->update_user_data) {\n\t\t/* only mask one event flags, keep behavior flags */\n\t\tif (poll_update->update_events) {\n\t\t\tstruct io_poll *poll = io_kiocb_to_cmd(preq);\n\n\t\t\tpoll->events &= ~0xffff;\n\t\t\tpoll->events |= poll_update->events & 0xffff;\n\t\t\tpoll->events |= IO_POLL_UNMASK;\n\t\t}\n\t\tif (poll_update->update_user_data)\n\t\t\tpreq->cqe.user_data = poll_update->new_user_data;\n\n\t\tret2 = io_poll_add(preq, issue_flags);\n\t\t/* successfully updated, don't complete poll request */\n\t\tif (!ret2 || ret2 == -EIOCBQUEUED)\n\t\t\tgoto out;\n\t}\n\n\treq_set_fail(preq);\n\tio_req_set_res(preq, -ECANCELED, 0);\n\tlocked = !(issue_flags & IO_URING_F_UNLOCKED);\n\tio_req_task_complete(preq, &locked);\nout:\n\tif (ret < 0) {\n\t\treq_set_fail(req);\n\t\treturn ret;\n\t}\n\t/* complete update request, we're done with it */\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}",
        "code_after_change": "int io_poll_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_poll_update *poll_update = io_kiocb_to_cmd(req);\n\tstruct io_cancel_data cd = { .data = poll_update->old_user_data, };\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_hash_bucket *bucket;\n\tstruct io_kiocb *preq;\n\tint ret2, ret = 0;\n\tbool locked;\n\n\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table, &bucket);\n\tret2 = io_poll_disarm(preq);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\tif (!ret2)\n\t\tgoto found;\n\tif (ret2 != -ENOENT) {\n\t\tret = ret2;\n\t\tgoto out;\n\t}\n\n\tio_ring_submit_lock(ctx, issue_flags);\n\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table_locked, &bucket);\n\tret2 = io_poll_disarm(preq);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\tio_ring_submit_unlock(ctx, issue_flags);\n\tif (ret2) {\n\t\tret = ret2;\n\t\tgoto out;\n\t}\n\nfound:\n\tif (poll_update->update_events || poll_update->update_user_data) {\n\t\t/* only mask one event flags, keep behavior flags */\n\t\tif (poll_update->update_events) {\n\t\t\tstruct io_poll *poll = io_kiocb_to_cmd(preq);\n\n\t\t\tpoll->events &= ~0xffff;\n\t\t\tpoll->events |= poll_update->events & 0xffff;\n\t\t\tpoll->events |= IO_POLL_UNMASK;\n\t\t}\n\t\tif (poll_update->update_user_data)\n\t\t\tpreq->cqe.user_data = poll_update->new_user_data;\n\n\t\tret2 = io_poll_add(preq, issue_flags);\n\t\t/* successfully updated, don't complete poll request */\n\t\tif (!ret2 || ret2 == -EIOCBQUEUED)\n\t\t\tgoto out;\n\t}\n\n\treq_set_fail(preq);\n\tio_req_set_res(preq, -ECANCELED, 0);\n\tlocked = !(issue_flags & IO_URING_F_UNLOCKED);\n\tio_req_task_complete(preq, &locked);\nout:\n\tif (ret < 0) {\n\t\treq_set_fail(req);\n\t\treturn ret;\n\t}\n\t/* complete update request, we're done with it */\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,20 +9,28 @@\n \tbool locked;\n \n \tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table, &bucket);\n-\tif (preq)\n-\t\tret2 = io_poll_disarm(preq);\n+\tret2 = io_poll_disarm(preq);\n \tif (bucket)\n \t\tspin_unlock(&bucket->lock);\n-\n-\tif (!preq) {\n-\t\tret = -ENOENT;\n-\t\tgoto out;\n-\t}\n-\tif (!ret2) {\n-\t\tret = -EALREADY;\n+\tif (!ret2)\n+\t\tgoto found;\n+\tif (ret2 != -ENOENT) {\n+\t\tret = ret2;\n \t\tgoto out;\n \t}\n \n+\tio_ring_submit_lock(ctx, issue_flags);\n+\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table_locked, &bucket);\n+\tret2 = io_poll_disarm(preq);\n+\tif (bucket)\n+\t\tspin_unlock(&bucket->lock);\n+\tio_ring_submit_unlock(ctx, issue_flags);\n+\tif (ret2) {\n+\t\tret = ret2;\n+\t\tgoto out;\n+\t}\n+\n+found:\n \tif (poll_update->update_events || poll_update->update_user_data) {\n \t\t/* only mask one event flags, keep behavior flags */\n \t\tif (poll_update->update_events) {",
        "function_modified_lines": {
            "added": [
                "\tret2 = io_poll_disarm(preq);",
                "\tif (!ret2)",
                "\t\tgoto found;",
                "\tif (ret2 != -ENOENT) {",
                "\t\tret = ret2;",
                "\tio_ring_submit_lock(ctx, issue_flags);",
                "\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table_locked, &bucket);",
                "\tret2 = io_poll_disarm(preq);",
                "\tif (bucket)",
                "\t\tspin_unlock(&bucket->lock);",
                "\tio_ring_submit_unlock(ctx, issue_flags);",
                "\tif (ret2) {",
                "\t\tret = ret2;",
                "\t\tgoto out;",
                "\t}",
                "",
                "found:"
            ],
            "deleted": [
                "\tif (preq)",
                "\t\tret2 = io_poll_disarm(preq);",
                "",
                "\tif (!preq) {",
                "\t\tret = -ENOENT;",
                "\t\tgoto out;",
                "\t}",
                "\tif (!ret2) {",
                "\t\tret = -EALREADY;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and 0e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4069
    },
    {
        "cve_id": "CVE-2023-39198",
        "code_before_change": "int qxl_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct qxl_device *qdev = to_qxl(dev);\n\tstruct qxl_bo *qobj;\n\tuint32_t handle;\n\tint r;\n\tstruct qxl_surface surf;\n\tuint32_t pitch, format;\n\n\tpitch = args->width * ((args->bpp + 1) / 8);\n\targs->size = pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\n\tswitch (args->bpp) {\n\tcase 16:\n\t\tformat = SPICE_SURFACE_FMT_16_565;\n\t\tbreak;\n\tcase 32:\n\t\tformat = SPICE_SURFACE_FMT_32_xRGB;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tsurf.width = args->width;\n\tsurf.height = args->height;\n\tsurf.stride = pitch;\n\tsurf.format = format;\n\tsurf.data = 0;\n\n\tr = qxl_gem_object_create_with_handle(qdev, file_priv,\n\t\t\t\t\t      QXL_GEM_DOMAIN_CPU,\n\t\t\t\t\t      args->size, &surf, &qobj,\n\t\t\t\t\t      &handle);\n\tif (r)\n\t\treturn r;\n\tqobj->is_dumb = true;\n\targs->pitch = pitch;\n\targs->handle = handle;\n\treturn 0;\n}",
        "code_after_change": "int qxl_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct qxl_device *qdev = to_qxl(dev);\n\tstruct qxl_bo *qobj;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tint r;\n\tstruct qxl_surface surf;\n\tuint32_t pitch, format;\n\n\tpitch = args->width * ((args->bpp + 1) / 8);\n\targs->size = pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\n\tswitch (args->bpp) {\n\tcase 16:\n\t\tformat = SPICE_SURFACE_FMT_16_565;\n\t\tbreak;\n\tcase 32:\n\t\tformat = SPICE_SURFACE_FMT_32_xRGB;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tsurf.width = args->width;\n\tsurf.height = args->height;\n\tsurf.stride = pitch;\n\tsurf.format = format;\n\tsurf.data = 0;\n\n\tr = qxl_gem_object_create_with_handle(qdev, file_priv,\n\t\t\t\t\t      QXL_GEM_DOMAIN_CPU,\n\t\t\t\t\t      args->size, &surf, &gobj,\n\t\t\t\t\t      &handle);\n\tif (r)\n\t\treturn r;\n\tqobj = gem_to_qxl_bo(gobj);\n\tqobj->is_dumb = true;\n\tdrm_gem_object_put(gobj);\n\targs->pitch = pitch;\n\targs->handle = handle;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n {\n \tstruct qxl_device *qdev = to_qxl(dev);\n \tstruct qxl_bo *qobj;\n+\tstruct drm_gem_object *gobj;\n \tuint32_t handle;\n \tint r;\n \tstruct qxl_surface surf;\n@@ -32,11 +33,13 @@\n \n \tr = qxl_gem_object_create_with_handle(qdev, file_priv,\n \t\t\t\t\t      QXL_GEM_DOMAIN_CPU,\n-\t\t\t\t\t      args->size, &surf, &qobj,\n+\t\t\t\t\t      args->size, &surf, &gobj,\n \t\t\t\t\t      &handle);\n \tif (r)\n \t\treturn r;\n+\tqobj = gem_to_qxl_bo(gobj);\n \tqobj->is_dumb = true;\n+\tdrm_gem_object_put(gobj);\n \targs->pitch = pitch;\n \targs->handle = handle;\n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tstruct drm_gem_object *gobj;",
                "\t\t\t\t\t      args->size, &surf, &gobj,",
                "\tqobj = gem_to_qxl_bo(gobj);",
                "\tdrm_gem_object_put(gobj);"
            ],
            "deleted": [
                "\t\t\t\t\t      args->size, &surf, &qobj,"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A race condition was found in the QXL driver in the Linux kernel. The qxl_mode_dumb_create() function dereferences the qobj returned by the qxl_gem_object_create_with_handle(), but the handle is the only one holding a reference to it. This flaw allows an attacker to guess the returned handle value and trigger a use-after-free issue, potentially leading to a denial of service or privilege escalation.",
        "id": 4185
    },
    {
        "cve_id": "CVE-2021-39801",
        "code_before_change": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\tstruct ion_handle *handle;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_buffer *buffer = NULL;\n\tstruct ion_heap *heap;\n\tint ret;\n\n\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n\t\t len, align, heap_id_mask, flags);\n\t/*\n\t * traverse the list of heaps available in this system in priority\n\t * order.  If the heap type is supported by the client, and matches the\n\t * request of the caller allocate from it.  Repeat until allocate has\n\t * succeeded or all heaps have been tried\n\t */\n\tlen = PAGE_ALIGN(len);\n\n\tif (!len)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdown_read(&dev->lock);\n\tplist_for_each_entry(heap, &dev->heaps, node) {\n\t\t/* if the caller didn't specify this heap id */\n\t\tif (!((1 << heap->id) & heap_id_mask))\n\t\t\tcontinue;\n\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n\t\tif (!IS_ERR(buffer))\n\t\t\tbreak;\n\t}\n\tup_read(&dev->lock);\n\n\tif (buffer == NULL)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (IS_ERR(buffer))\n\t\treturn ERR_CAST(buffer);\n\n\thandle = ion_handle_create(client, buffer);\n\n\t/*\n\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n\t * and ion_handle_create will take a second reference, drop one here\n\t */\n\tion_buffer_put(buffer);\n\n\tif (IS_ERR(handle))\n\t\treturn handle;\n\n\tmutex_lock(&client->lock);\n\tret = ion_handle_add(client, handle);\n\tmutex_unlock(&client->lock);\n\tif (ret) {\n\t\tion_handle_put(handle);\n\t\thandle = ERR_PTR(ret);\n\t}\n\n\treturn handle;\n}",
        "code_after_change": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,60 +2,5 @@\n \t\t\t     size_t align, unsigned int heap_id_mask,\n \t\t\t     unsigned int flags)\n {\n-\tstruct ion_handle *handle;\n-\tstruct ion_device *dev = client->dev;\n-\tstruct ion_buffer *buffer = NULL;\n-\tstruct ion_heap *heap;\n-\tint ret;\n-\n-\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n-\t\t len, align, heap_id_mask, flags);\n-\t/*\n-\t * traverse the list of heaps available in this system in priority\n-\t * order.  If the heap type is supported by the client, and matches the\n-\t * request of the caller allocate from it.  Repeat until allocate has\n-\t * succeeded or all heaps have been tried\n-\t */\n-\tlen = PAGE_ALIGN(len);\n-\n-\tif (!len)\n-\t\treturn ERR_PTR(-EINVAL);\n-\n-\tdown_read(&dev->lock);\n-\tplist_for_each_entry(heap, &dev->heaps, node) {\n-\t\t/* if the caller didn't specify this heap id */\n-\t\tif (!((1 << heap->id) & heap_id_mask))\n-\t\t\tcontinue;\n-\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n-\t\tif (!IS_ERR(buffer))\n-\t\t\tbreak;\n-\t}\n-\tup_read(&dev->lock);\n-\n-\tif (buffer == NULL)\n-\t\treturn ERR_PTR(-ENODEV);\n-\n-\tif (IS_ERR(buffer))\n-\t\treturn ERR_CAST(buffer);\n-\n-\thandle = ion_handle_create(client, buffer);\n-\n-\t/*\n-\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n-\t * and ion_handle_create will take a second reference, drop one here\n-\t */\n-\tion_buffer_put(buffer);\n-\n-\tif (IS_ERR(handle))\n-\t\treturn handle;\n-\n-\tmutex_lock(&client->lock);\n-\tret = ion_handle_add(client, handle);\n-\tmutex_unlock(&client->lock);\n-\tif (ret) {\n-\t\tion_handle_put(handle);\n-\t\thandle = ERR_PTR(ret);\n-\t}\n-\n-\treturn handle;\n+\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);"
            ],
            "deleted": [
                "\tstruct ion_handle *handle;",
                "\tstruct ion_device *dev = client->dev;",
                "\tstruct ion_buffer *buffer = NULL;",
                "\tstruct ion_heap *heap;",
                "\tint ret;",
                "",
                "\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,",
                "\t\t len, align, heap_id_mask, flags);",
                "\t/*",
                "\t * traverse the list of heaps available in this system in priority",
                "\t * order.  If the heap type is supported by the client, and matches the",
                "\t * request of the caller allocate from it.  Repeat until allocate has",
                "\t * succeeded or all heaps have been tried",
                "\t */",
                "\tlen = PAGE_ALIGN(len);",
                "",
                "\tif (!len)",
                "\t\treturn ERR_PTR(-EINVAL);",
                "",
                "\tdown_read(&dev->lock);",
                "\tplist_for_each_entry(heap, &dev->heaps, node) {",
                "\t\t/* if the caller didn't specify this heap id */",
                "\t\tif (!((1 << heap->id) & heap_id_mask))",
                "\t\t\tcontinue;",
                "\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);",
                "\t\tif (!IS_ERR(buffer))",
                "\t\t\tbreak;",
                "\t}",
                "\tup_read(&dev->lock);",
                "",
                "\tif (buffer == NULL)",
                "\t\treturn ERR_PTR(-ENODEV);",
                "",
                "\tif (IS_ERR(buffer))",
                "\t\treturn ERR_CAST(buffer);",
                "",
                "\thandle = ion_handle_create(client, buffer);",
                "",
                "\t/*",
                "\t * ion_buffer_create will create a buffer with a ref_cnt of 1,",
                "\t * and ion_handle_create will take a second reference, drop one here",
                "\t */",
                "\tion_buffer_put(buffer);",
                "",
                "\tif (IS_ERR(handle))",
                "\t\treturn handle;",
                "",
                "\tmutex_lock(&client->lock);",
                "\tret = ion_handle_add(client, handle);",
                "\tmutex_unlock(&client->lock);",
                "\tif (ret) {",
                "\t\tion_handle_put(handle);",
                "\t\thandle = ERR_PTR(ret);",
                "\t}",
                "",
                "\treturn handle;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In ion_ioctl of ion-ioctl.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-209791720References: Upstream kernel",
        "id": 3111
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "int inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint res;\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\tif (IS_ERR(dst)) {\n\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\tsk->sk_route_caps = 0;\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(dst);\n\t}\n\n\trcu_read_lock();\n\tskb_dst_set_noref(skb, dst);\n\n\t/* Restore final destination back after routing done */\n\tfl6.daddr = sk->sk_v6_daddr;\n\n\tres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n\trcu_read_unlock();\n\treturn res;\n}",
        "code_after_change": "int inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint res;\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\tif (IS_ERR(dst)) {\n\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\tsk->sk_route_caps = 0;\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(dst);\n\t}\n\n\trcu_read_lock();\n\tskb_dst_set_noref(skb, dst);\n\n\t/* Restore final destination back after routing done */\n\tfl6.daddr = sk->sk_v6_daddr;\n\n\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t       np->tclass);\n\trcu_read_unlock();\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,8 @@\n \t/* Restore final destination back after routing done */\n \tfl6.daddr = sk->sk_v6_daddr;\n \n-\tres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n+\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n+\t\t       np->tclass);\n \trcu_read_unlock();\n \treturn res;\n }",
        "function_modified_lines": {
            "added": [
                "\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),",
                "\t\t       np->tclass);"
            ],
            "deleted": [
                "\tres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 999
    },
    {
        "cve_id": "CVE-2016-10905",
        "code_before_change": "void gfs2_clear_rgrpd(struct gfs2_sbd *sdp)\n{\n\tstruct rb_node *n;\n\tstruct gfs2_rgrpd *rgd;\n\tstruct gfs2_glock *gl;\n\n\twhile ((n = rb_first(&sdp->sd_rindex_tree))) {\n\t\trgd = rb_entry(n, struct gfs2_rgrpd, rd_node);\n\t\tgl = rgd->rd_gl;\n\n\t\trb_erase(n, &sdp->sd_rindex_tree);\n\n\t\tif (gl) {\n\t\t\tspin_lock(&gl->gl_lockref.lock);\n\t\t\tgl->gl_object = NULL;\n\t\t\tspin_unlock(&gl->gl_lockref.lock);\n\t\t\tgfs2_glock_add_to_lru(gl);\n\t\t\tgfs2_glock_put(gl);\n\t\t}\n\n\t\tgfs2_free_clones(rgd);\n\t\tkfree(rgd->rd_bits);\n\t\treturn_all_reservations(rgd);\n\t\tkmem_cache_free(gfs2_rgrpd_cachep, rgd);\n\t}\n}",
        "code_after_change": "void gfs2_clear_rgrpd(struct gfs2_sbd *sdp)\n{\n\tstruct rb_node *n;\n\tstruct gfs2_rgrpd *rgd;\n\tstruct gfs2_glock *gl;\n\n\twhile ((n = rb_first(&sdp->sd_rindex_tree))) {\n\t\trgd = rb_entry(n, struct gfs2_rgrpd, rd_node);\n\t\tgl = rgd->rd_gl;\n\n\t\trb_erase(n, &sdp->sd_rindex_tree);\n\n\t\tif (gl) {\n\t\t\tspin_lock(&gl->gl_lockref.lock);\n\t\t\tgl->gl_object = NULL;\n\t\t\tspin_unlock(&gl->gl_lockref.lock);\n\t\t\tgfs2_glock_add_to_lru(gl);\n\t\t\tgfs2_glock_put(gl);\n\t\t}\n\n\t\tgfs2_free_clones(rgd);\n\t\tkfree(rgd->rd_bits);\n\t\trgd->rd_bits = NULL;\n\t\treturn_all_reservations(rgd);\n\t\tkmem_cache_free(gfs2_rgrpd_cachep, rgd);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,6 +20,7 @@\n \n \t\tgfs2_free_clones(rgd);\n \t\tkfree(rgd->rd_bits);\n+\t\trgd->rd_bits = NULL;\n \t\treturn_all_reservations(rgd);\n \t\tkmem_cache_free(gfs2_rgrpd_cachep, rgd);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\trgd->rd_bits = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in fs/gfs2/rgrp.c in the Linux kernel before 4.8. A use-after-free is caused by the functions gfs2_clear_rgrpd and read_rindex_entry.",
        "id": 907
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "void nfc_llcp_mac_is_down(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn;\n\n\tlocal->remote_miu = LLCP_DEFAULT_MIU;\n\tlocal->remote_lto = LLCP_DEFAULT_LTO;\n\n\t/* Close and purge all existing sockets */\n\tnfc_llcp_socket_release(local, true, 0);\n}",
        "code_after_change": "void nfc_llcp_mac_is_down(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn;\n\n\tlocal->remote_miu = LLCP_DEFAULT_MIU;\n\tlocal->remote_lto = LLCP_DEFAULT_LTO;\n\n\t/* Close and purge all existing sockets */\n\tnfc_llcp_socket_release(local, true, 0);\n\n\tnfc_llcp_local_put(local);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,4 +11,6 @@\n \n \t/* Close and purge all existing sockets */\n \tnfc_llcp_socket_release(local, true, 0);\n+\n+\tnfc_llcp_local_put(local);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tnfc_llcp_local_put(local);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4152
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "int hfi1_mmu_rb_insert(struct mmu_rb_handler *handler,\n\t\t       struct mmu_rb_node *mnode)\n{\n\tstruct mmu_rb_node *node;\n\tunsigned long flags;\n\tint ret = 0;\n\n\ttrace_hfi1_mmu_rb_insert(mnode->addr, mnode->len);\n\tspin_lock_irqsave(&handler->lock, flags);\n\tnode = __mmu_rb_search(handler, mnode->addr, mnode->len);\n\tif (node) {\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\t__mmu_int_rb_insert(mnode, &handler->root);\n\tlist_add(&mnode->list, &handler->lru_list);\n\n\tret = handler->ops->insert(handler->ops_arg, mnode);\n\tif (ret) {\n\t\t__mmu_int_rb_remove(mnode, &handler->root);\n\t\tlist_del(&mnode->list); /* remove from LRU list */\n\t}\nunlock:\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\treturn ret;\n}",
        "code_after_change": "int hfi1_mmu_rb_insert(struct mmu_rb_handler *handler,\n\t\t       struct mmu_rb_node *mnode)\n{\n\tstruct mmu_rb_node *node;\n\tunsigned long flags;\n\tint ret = 0;\n\n\ttrace_hfi1_mmu_rb_insert(mnode->addr, mnode->len);\n\n\tif (current->mm != handler->mn.mm)\n\t\treturn -EPERM;\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\tnode = __mmu_rb_search(handler, mnode->addr, mnode->len);\n\tif (node) {\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\t__mmu_int_rb_insert(mnode, &handler->root);\n\tlist_add(&mnode->list, &handler->lru_list);\n\n\tret = handler->ops->insert(handler->ops_arg, mnode);\n\tif (ret) {\n\t\t__mmu_int_rb_remove(mnode, &handler->root);\n\t\tlist_del(&mnode->list); /* remove from LRU list */\n\t}\n\tmnode->handler = handler;\nunlock:\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,10 @@\n \tint ret = 0;\n \n \ttrace_hfi1_mmu_rb_insert(mnode->addr, mnode->len);\n+\n+\tif (current->mm != handler->mn.mm)\n+\t\treturn -EPERM;\n+\n \tspin_lock_irqsave(&handler->lock, flags);\n \tnode = __mmu_rb_search(handler, mnode->addr, mnode->len);\n \tif (node) {\n@@ -20,6 +24,7 @@\n \t\t__mmu_int_rb_remove(mnode, &handler->root);\n \t\tlist_del(&mnode->list); /* remove from LRU list */\n \t}\n+\tmnode->handler = handler;\n unlock:\n \tspin_unlock_irqrestore(&handler->lock, flags);\n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (current->mm != handler->mn.mm)",
                "\t\treturn -EPERM;",
                "",
                "\tmnode->handler = handler;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2643
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\trfv.msg = msg;\n\trfv.hlen = 0;\n\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\tif (err)\n\t\tgoto out;\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tif (inet->hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (inet->hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst, msg->msg_flags);\n\telse {\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, hlimit, tclass, opt, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags, dontfrag);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "code_after_change": "static int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\trfv.msg = msg;\n\trfv.hlen = 0;\n\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\tif (err)\n\t\tgoto out;\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tif (inet->hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (inet->hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst, msg->msg_flags);\n\telse {\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, hlimit, tclass, opt, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags, dontfrag);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n static int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n {\n+\tstruct ipv6_txoptions *opt_to_free = NULL;\n \tstruct ipv6_txoptions opt_space;\n \tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n \tstruct in6_addr *daddr, *final_p, final;\n@@ -106,8 +107,10 @@\n \t\tif (!(opt->opt_nflen|opt->opt_flen))\n \t\t\topt = NULL;\n \t}\n-\tif (!opt)\n-\t\topt = np->opt;\n+\tif (!opt) {\n+\t\topt = txopt_get(np);\n+\t\topt_to_free = opt;\n+\t\t}\n \tif (flowlabel)\n \t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n \topt = ipv6_fixup_options(&opt_space, opt);\n@@ -173,6 +176,7 @@\n \tdst_release(dst);\n out:\n \tfl6_sock_release(flowlabel);\n+\ttxopt_put(opt_to_free);\n \treturn err < 0 ? err : len;\n do_confirm:\n \tdst_confirm(dst);",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt_to_free = NULL;",
                "\tif (!opt) {",
                "\t\topt = txopt_get(np);",
                "\t\topt_to_free = opt;",
                "\t\t}",
                "\ttxopt_put(opt_to_free);"
            ],
            "deleted": [
                "\tif (!opt)",
                "\t\topt = np->opt;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1003
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static void blk_add_trace_bio(struct request_queue *q, struct bio *bio,\n\t\t\t      u32 what, int error)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\n\tif (likely(!bt))\n\t\treturn;\n\n\t__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,\n\t\t\tbio_op(bio), bio->bi_opf, what, error, 0, NULL,\n\t\t\tblk_trace_bio_get_cgid(q, bio));\n}",
        "code_after_change": "static void blk_add_trace_bio(struct request_queue *q, struct bio *bio,\n\t\t\t      u32 what, int error)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\t__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,\n\t\t\tbio_op(bio), bio->bi_opf, what, error, 0, NULL,\n\t\t\tblk_trace_bio_get_cgid(q, bio));\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,17 @@\n static void blk_add_trace_bio(struct request_queue *q, struct bio *bio,\n \t\t\t      u32 what, int error)\n {\n-\tstruct blk_trace *bt = q->blk_trace;\n+\tstruct blk_trace *bt;\n \n-\tif (likely(!bt))\n+\trcu_read_lock();\n+\tbt = rcu_dereference(q->blk_trace);\n+\tif (likely(!bt)) {\n+\t\trcu_read_unlock();\n \t\treturn;\n+\t}\n \n \t__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,\n \t\t\tbio_op(bio), bio->bi_opf, what, error, 0, NULL,\n \t\t\tblk_trace_bio_get_cgid(q, bio));\n+\trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct blk_trace *bt;",
                "\trcu_read_lock();",
                "\tbt = rcu_dereference(q->blk_trace);",
                "\tif (likely(!bt)) {",
                "\t\trcu_read_unlock();",
                "\t}",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tstruct blk_trace *bt = q->blk_trace;",
                "\tif (likely(!bt))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2229
    },
    {
        "cve_id": "CVE-2019-15211",
        "code_before_change": "static int usb_raremono_probe(struct usb_interface *intf,\n\t\t\t\tconst struct usb_device_id *id)\n{\n\tstruct raremono_device *radio;\n\tint retval = 0;\n\n\tradio = devm_kzalloc(&intf->dev, sizeof(struct raremono_device), GFP_KERNEL);\n\tif (radio)\n\t\tradio->buffer = devm_kmalloc(&intf->dev, BUFFER_LENGTH, GFP_KERNEL);\n\n\tif (!radio || !radio->buffer)\n\t\treturn -ENOMEM;\n\n\tradio->usbdev = interface_to_usbdev(intf);\n\tradio->intf = intf;\n\n\t/*\n\t * This device uses the same USB IDs as the si470x SiLabs reference\n\t * design. So do an additional check: attempt to read the device ID\n\t * from the si470x: the lower 12 bits are 0x0242 for the si470x. The\n\t * Raremono always returns 0x0800 (the meaning of that is unknown, but\n\t * at least it works).\n\t *\n\t * We use this check to determine which device we are dealing with.\n\t */\n\tmsleep(20);\n\tretval = usb_control_msg(radio->usbdev,\n\t\tusb_rcvctrlpipe(radio->usbdev, 0),\n\t\tHID_REQ_GET_REPORT,\n\t\tUSB_TYPE_CLASS | USB_RECIP_INTERFACE | USB_DIR_IN,\n\t\t1, 2,\n\t\tradio->buffer, 3, 500);\n\tif (retval != 3 ||\n\t    (get_unaligned_be16(&radio->buffer[1]) & 0xfff) == 0x0242) {\n\t\tdev_info(&intf->dev, \"this is not Thanko's Raremono.\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tdev_info(&intf->dev, \"Thanko's Raremono connected: (%04X:%04X)\\n\",\n\t\t\tid->idVendor, id->idProduct);\n\n\tretval = v4l2_device_register(&intf->dev, &radio->v4l2_dev);\n\tif (retval < 0) {\n\t\tdev_err(&intf->dev, \"couldn't register v4l2_device\\n\");\n\t\treturn retval;\n\t}\n\n\tmutex_init(&radio->lock);\n\n\tstrscpy(radio->vdev.name, radio->v4l2_dev.name,\n\t\tsizeof(radio->vdev.name));\n\tradio->vdev.v4l2_dev = &radio->v4l2_dev;\n\tradio->vdev.fops = &usb_raremono_fops;\n\tradio->vdev.ioctl_ops = &usb_raremono_ioctl_ops;\n\tradio->vdev.lock = &radio->lock;\n\tradio->vdev.release = video_device_release_empty;\n\tradio->vdev.device_caps = V4L2_CAP_TUNER | V4L2_CAP_RADIO;\n\n\tusb_set_intfdata(intf, &radio->v4l2_dev);\n\n\tvideo_set_drvdata(&radio->vdev, radio);\n\n\traremono_cmd_main(radio, BAND_FM, 95160);\n\n\tretval = video_register_device(&radio->vdev, VFL_TYPE_RADIO, -1);\n\tif (retval == 0) {\n\t\tdev_info(&intf->dev, \"V4L2 device registered as %s\\n\",\n\t\t\t\tvideo_device_node_name(&radio->vdev));\n\t\treturn 0;\n\t}\n\tdev_err(&intf->dev, \"could not register video device\\n\");\n\tv4l2_device_unregister(&radio->v4l2_dev);\n\treturn retval;\n}",
        "code_after_change": "static int usb_raremono_probe(struct usb_interface *intf,\n\t\t\t\tconst struct usb_device_id *id)\n{\n\tstruct raremono_device *radio;\n\tint retval = 0;\n\n\tradio = kzalloc(sizeof(*radio), GFP_KERNEL);\n\tif (!radio)\n\t\treturn -ENOMEM;\n\tradio->buffer = kmalloc(BUFFER_LENGTH, GFP_KERNEL);\n\tif (!radio->buffer) {\n\t\tkfree(radio);\n\t\treturn -ENOMEM;\n\t}\n\n\tradio->usbdev = interface_to_usbdev(intf);\n\tradio->intf = intf;\n\n\t/*\n\t * This device uses the same USB IDs as the si470x SiLabs reference\n\t * design. So do an additional check: attempt to read the device ID\n\t * from the si470x: the lower 12 bits are 0x0242 for the si470x. The\n\t * Raremono always returns 0x0800 (the meaning of that is unknown, but\n\t * at least it works).\n\t *\n\t * We use this check to determine which device we are dealing with.\n\t */\n\tmsleep(20);\n\tretval = usb_control_msg(radio->usbdev,\n\t\tusb_rcvctrlpipe(radio->usbdev, 0),\n\t\tHID_REQ_GET_REPORT,\n\t\tUSB_TYPE_CLASS | USB_RECIP_INTERFACE | USB_DIR_IN,\n\t\t1, 2,\n\t\tradio->buffer, 3, 500);\n\tif (retval != 3 ||\n\t    (get_unaligned_be16(&radio->buffer[1]) & 0xfff) == 0x0242) {\n\t\tdev_info(&intf->dev, \"this is not Thanko's Raremono.\\n\");\n\t\tretval = -ENODEV;\n\t\tgoto free_mem;\n\t}\n\n\tdev_info(&intf->dev, \"Thanko's Raremono connected: (%04X:%04X)\\n\",\n\t\t\tid->idVendor, id->idProduct);\n\n\tretval = v4l2_device_register(&intf->dev, &radio->v4l2_dev);\n\tif (retval < 0) {\n\t\tdev_err(&intf->dev, \"couldn't register v4l2_device\\n\");\n\t\tgoto free_mem;\n\t}\n\n\tmutex_init(&radio->lock);\n\n\tstrscpy(radio->vdev.name, radio->v4l2_dev.name,\n\t\tsizeof(radio->vdev.name));\n\tradio->vdev.v4l2_dev = &radio->v4l2_dev;\n\tradio->vdev.fops = &usb_raremono_fops;\n\tradio->vdev.ioctl_ops = &usb_raremono_ioctl_ops;\n\tradio->vdev.lock = &radio->lock;\n\tradio->vdev.release = video_device_release_empty;\n\tradio->vdev.device_caps = V4L2_CAP_TUNER | V4L2_CAP_RADIO;\n\tradio->v4l2_dev.release = raremono_device_release;\n\n\tusb_set_intfdata(intf, &radio->v4l2_dev);\n\n\tvideo_set_drvdata(&radio->vdev, radio);\n\n\traremono_cmd_main(radio, BAND_FM, 95160);\n\n\tretval = video_register_device(&radio->vdev, VFL_TYPE_RADIO, -1);\n\tif (retval == 0) {\n\t\tdev_info(&intf->dev, \"V4L2 device registered as %s\\n\",\n\t\t\t\tvideo_device_node_name(&radio->vdev));\n\t\treturn 0;\n\t}\n\tdev_err(&intf->dev, \"could not register video device\\n\");\n\tv4l2_device_unregister(&radio->v4l2_dev);\n\nfree_mem:\n\tkfree(radio->buffer);\n\tkfree(radio);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,12 +4,14 @@\n \tstruct raremono_device *radio;\n \tint retval = 0;\n \n-\tradio = devm_kzalloc(&intf->dev, sizeof(struct raremono_device), GFP_KERNEL);\n-\tif (radio)\n-\t\tradio->buffer = devm_kmalloc(&intf->dev, BUFFER_LENGTH, GFP_KERNEL);\n-\n-\tif (!radio || !radio->buffer)\n+\tradio = kzalloc(sizeof(*radio), GFP_KERNEL);\n+\tif (!radio)\n \t\treturn -ENOMEM;\n+\tradio->buffer = kmalloc(BUFFER_LENGTH, GFP_KERNEL);\n+\tif (!radio->buffer) {\n+\t\tkfree(radio);\n+\t\treturn -ENOMEM;\n+\t}\n \n \tradio->usbdev = interface_to_usbdev(intf);\n \tradio->intf = intf;\n@@ -33,7 +35,8 @@\n \tif (retval != 3 ||\n \t    (get_unaligned_be16(&radio->buffer[1]) & 0xfff) == 0x0242) {\n \t\tdev_info(&intf->dev, \"this is not Thanko's Raremono.\\n\");\n-\t\treturn -ENODEV;\n+\t\tretval = -ENODEV;\n+\t\tgoto free_mem;\n \t}\n \n \tdev_info(&intf->dev, \"Thanko's Raremono connected: (%04X:%04X)\\n\",\n@@ -42,7 +45,7 @@\n \tretval = v4l2_device_register(&intf->dev, &radio->v4l2_dev);\n \tif (retval < 0) {\n \t\tdev_err(&intf->dev, \"couldn't register v4l2_device\\n\");\n-\t\treturn retval;\n+\t\tgoto free_mem;\n \t}\n \n \tmutex_init(&radio->lock);\n@@ -55,6 +58,7 @@\n \tradio->vdev.lock = &radio->lock;\n \tradio->vdev.release = video_device_release_empty;\n \tradio->vdev.device_caps = V4L2_CAP_TUNER | V4L2_CAP_RADIO;\n+\tradio->v4l2_dev.release = raremono_device_release;\n \n \tusb_set_intfdata(intf, &radio->v4l2_dev);\n \n@@ -70,5 +74,9 @@\n \t}\n \tdev_err(&intf->dev, \"could not register video device\\n\");\n \tv4l2_device_unregister(&radio->v4l2_dev);\n+\n+free_mem:\n+\tkfree(radio->buffer);\n+\tkfree(radio);\n \treturn retval;\n }",
        "function_modified_lines": {
            "added": [
                "\tradio = kzalloc(sizeof(*radio), GFP_KERNEL);",
                "\tif (!radio)",
                "\tradio->buffer = kmalloc(BUFFER_LENGTH, GFP_KERNEL);",
                "\tif (!radio->buffer) {",
                "\t\tkfree(radio);",
                "\t\treturn -ENOMEM;",
                "\t}",
                "\t\tretval = -ENODEV;",
                "\t\tgoto free_mem;",
                "\t\tgoto free_mem;",
                "\tradio->v4l2_dev.release = raremono_device_release;",
                "",
                "free_mem:",
                "\tkfree(radio->buffer);",
                "\tkfree(radio);"
            ],
            "deleted": [
                "\tradio = devm_kzalloc(&intf->dev, sizeof(struct raremono_device), GFP_KERNEL);",
                "\tif (radio)",
                "\t\tradio->buffer = devm_kmalloc(&intf->dev, BUFFER_LENGTH, GFP_KERNEL);",
                "",
                "\tif (!radio || !radio->buffer)",
                "\t\treturn -ENODEV;",
                "\t\treturn retval;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.2.6. There is a use-after-free caused by a malicious USB device in the drivers/media/v4l2-core/v4l2-dev.c driver because drivers/media/radio/radio-raremono.c does not properly allocate memory.",
        "id": 1993
    },
    {
        "cve_id": "CVE-2022-3424",
        "code_before_change": "int gru_set_context_option(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_set_context_option_req req;\n\tint ret = 0;\n\n\tSTAT(set_context_option);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\tgru_dbg(grudev, \"op %d, gseg 0x%lx, value1 0x%lx\\n\", req.op, req.gseg, req.val1);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts) {\n\t\tgts = gru_alloc_locked_gts(req.gseg);\n\t\tif (IS_ERR(gts))\n\t\t\treturn PTR_ERR(gts);\n\t}\n\n\tswitch (req.op) {\n\tcase sco_blade_chiplet:\n\t\t/* Select blade/chiplet for GRU context */\n\t\tif (req.val0 < -1 || req.val0 >= GRU_CHIPLETS_PER_HUB ||\n\t\t    req.val1 < -1 || req.val1 >= GRU_MAX_BLADES ||\n\t\t    (req.val1 >= 0 && !gru_base[req.val1])) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tgts->ts_user_blade_id = req.val1;\n\t\t\tgts->ts_user_chiplet_id = req.val0;\n\t\t\tgru_check_context_placement(gts);\n\t\t}\n\t\tbreak;\n\tcase sco_gseg_owner:\n \t\t/* Register the current task as the GSEG owner */\n\t\tgts->ts_tgid_owner = current->tgid;\n\t\tbreak;\n\tcase sco_cch_req_slice:\n \t\t/* Set the CCH slice option */\n\t\tgts->ts_cch_req_slice = req.val1 & 3;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\tgru_unlock_gts(gts);\n\n\treturn ret;\n}",
        "code_after_change": "int gru_set_context_option(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_set_context_option_req req;\n\tint ret = 0;\n\n\tSTAT(set_context_option);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\tgru_dbg(grudev, \"op %d, gseg 0x%lx, value1 0x%lx\\n\", req.op, req.gseg, req.val1);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts) {\n\t\tgts = gru_alloc_locked_gts(req.gseg);\n\t\tif (IS_ERR(gts))\n\t\t\treturn PTR_ERR(gts);\n\t}\n\n\tswitch (req.op) {\n\tcase sco_blade_chiplet:\n\t\t/* Select blade/chiplet for GRU context */\n\t\tif (req.val0 < -1 || req.val0 >= GRU_CHIPLETS_PER_HUB ||\n\t\t    req.val1 < -1 || req.val1 >= GRU_MAX_BLADES ||\n\t\t    (req.val1 >= 0 && !gru_base[req.val1])) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tgts->ts_user_blade_id = req.val1;\n\t\t\tgts->ts_user_chiplet_id = req.val0;\n\t\t\tif (gru_check_context_placement(gts)) {\n\t\t\t\tgru_unlock_gts(gts);\n\t\t\t\tgru_unload_context(gts, 1);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase sco_gseg_owner:\n \t\t/* Register the current task as the GSEG owner */\n\t\tgts->ts_tgid_owner = current->tgid;\n\t\tbreak;\n\tcase sco_cch_req_slice:\n \t\t/* Set the CCH slice option */\n\t\tgts->ts_cch_req_slice = req.val1 & 3;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\tgru_unlock_gts(gts);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,7 +26,11 @@\n \t\t} else {\n \t\t\tgts->ts_user_blade_id = req.val1;\n \t\t\tgts->ts_user_chiplet_id = req.val0;\n-\t\t\tgru_check_context_placement(gts);\n+\t\t\tif (gru_check_context_placement(gts)) {\n+\t\t\t\tgru_unlock_gts(gts);\n+\t\t\t\tgru_unload_context(gts, 1);\n+\t\t\t\treturn ret;\n+\t\t\t}\n \t\t}\n \t\tbreak;\n \tcase sco_gseg_owner:",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (gru_check_context_placement(gts)) {",
                "\t\t\t\tgru_unlock_gts(gts);",
                "\t\t\t\tgru_unload_context(gts, 1);",
                "\t\t\t\treturn ret;",
                "\t\t\t}"
            ],
            "deleted": [
                "\t\t\tgru_check_context_placement(gts);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s SGI GRU driver in the way the first gru_file_unlocked_ioctl function is called by the user, where a fail pass occurs in the gru_check_chiplet_assignment function. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3596
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = flowlabel ? flowlabel->opt : np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
        "code_after_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -125,8 +125,10 @@\n \n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n-\topt = flowlabel ? flowlabel->opt : np->opt;\n+\trcu_read_lock();\n+\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n \tfinal_p = fl6_update_dst(&fl6, opt, &final);\n+\trcu_read_unlock();\n \n \tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \terr = 0;",
        "function_modified_lines": {
            "added": [
                "\trcu_read_lock();",
                "\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\topt = flowlabel ? flowlabel->opt : np->opt;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 994
    },
    {
        "cve_id": "CVE-2019-15919",
        "code_before_change": "int\nSMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec)\n{\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}",
        "code_after_change": "int\nSMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec)\n{\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tcifs_small_buf_release(req);\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -54,7 +54,6 @@\n \n \trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n \t\t\t    &resp_buftype, flags, &rsp_iov);\n-\tcifs_small_buf_release(req);\n \trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n \n \tif (rc) {\n@@ -72,6 +71,7 @@\n \t\t\t\t     io_parms->offset, *nbytes);\n \t}\n \n+\tcifs_small_buf_release(req);\n \tfree_rsp_buf(resp_buftype, rsp);\n \treturn rc;\n }",
        "function_modified_lines": {
            "added": [
                "\tcifs_small_buf_release(req);"
            ],
            "deleted": [
                "\tcifs_small_buf_release(req);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.10. SMB2_write in fs/cifs/smb2pdu.c has a use-after-free.",
        "id": 2026
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "static int vmw_resource_context_res_add(struct vmw_private *dev_priv,\n\t\t\t\t\tstruct vmw_sw_context *sw_context,\n\t\t\t\t\tstruct vmw_resource *ctx)\n{\n\tstruct list_head *binding_list;\n\tstruct vmw_ctx_bindinfo *entry;\n\tint ret = 0;\n\tstruct vmw_resource *res;\n\tu32 i;\n\tu32 cotable_max = has_sm5_context(ctx->dev_priv) ?\n\t\tSVGA_COTABLE_MAX : SVGA_COTABLE_DX10_MAX;\n\n\t/* Add all cotables to the validation list. */\n\tif (has_sm4_context(dev_priv) &&\n\t    vmw_res_type(ctx) == vmw_res_dx_context) {\n\t\tfor (i = 0; i < cotable_max; ++i) {\n\t\t\tres = vmw_context_cotable(ctx, i);\n\t\t\tif (IS_ERR(res))\n\t\t\t\tcontinue;\n\n\t\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t\t\t    VMW_RES_DIRTY_SET);\n\t\t\tif (unlikely(ret != 0))\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/* Add all resources bound to the context to the validation list */\n\tmutex_lock(&dev_priv->binding_mutex);\n\tbinding_list = vmw_context_binding_list(ctx);\n\n\tlist_for_each_entry(entry, binding_list, ctx_list) {\n\t\tif (vmw_res_type(entry->res) == vmw_res_view)\n\t\t\tret = vmw_view_res_val_add(sw_context, entry->res);\n\t\telse\n\t\t\tret = vmw_execbuf_res_noctx_val_add\n\t\t\t\t(sw_context, entry->res,\n\t\t\t\t vmw_binding_dirtying(entry->bt));\n\t\tif (unlikely(ret != 0))\n\t\t\tbreak;\n\t}\n\n\tif (has_sm4_context(dev_priv) &&\n\t    vmw_res_type(ctx) == vmw_res_dx_context) {\n\t\tstruct vmw_buffer_object *dx_query_mob;\n\n\t\tdx_query_mob = vmw_context_get_dx_query_mob(ctx);\n\t\tif (dx_query_mob)\n\t\t\tret = vmw_validation_add_bo(sw_context->ctx,\n\t\t\t\t\t\t    dx_query_mob, true, false);\n\t}\n\n\tmutex_unlock(&dev_priv->binding_mutex);\n\treturn ret;\n}",
        "code_after_change": "static int vmw_resource_context_res_add(struct vmw_private *dev_priv,\n\t\t\t\t\tstruct vmw_sw_context *sw_context,\n\t\t\t\t\tstruct vmw_resource *ctx)\n{\n\tstruct list_head *binding_list;\n\tstruct vmw_ctx_bindinfo *entry;\n\tint ret = 0;\n\tstruct vmw_resource *res;\n\tu32 i;\n\tu32 cotable_max = has_sm5_context(ctx->dev_priv) ?\n\t\tSVGA_COTABLE_MAX : SVGA_COTABLE_DX10_MAX;\n\n\t/* Add all cotables to the validation list. */\n\tif (has_sm4_context(dev_priv) &&\n\t    vmw_res_type(ctx) == vmw_res_dx_context) {\n\t\tfor (i = 0; i < cotable_max; ++i) {\n\t\t\tres = vmw_context_cotable(ctx, i);\n\t\t\tif (IS_ERR(res))\n\t\t\t\tcontinue;\n\n\t\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n\t\t\t\t\t\t      VMW_RES_DIRTY_SET,\n\t\t\t\t\t\t      vmw_val_add_flag_noctx);\n\t\t\tif (unlikely(ret != 0))\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/* Add all resources bound to the context to the validation list */\n\tmutex_lock(&dev_priv->binding_mutex);\n\tbinding_list = vmw_context_binding_list(ctx);\n\n\tlist_for_each_entry(entry, binding_list, ctx_list) {\n\t\tif (vmw_res_type(entry->res) == vmw_res_view)\n\t\t\tret = vmw_view_res_val_add(sw_context, entry->res);\n\t\telse\n\t\t\tret = vmw_execbuf_res_val_add(sw_context, entry->res,\n\t\t\t\t\t\t      vmw_binding_dirtying(entry->bt),\n\t\t\t\t\t\t      vmw_val_add_flag_noctx);\n\t\tif (unlikely(ret != 0))\n\t\t\tbreak;\n\t}\n\n\tif (has_sm4_context(dev_priv) &&\n\t    vmw_res_type(ctx) == vmw_res_dx_context) {\n\t\tstruct vmw_buffer_object *dx_query_mob;\n\n\t\tdx_query_mob = vmw_context_get_dx_query_mob(ctx);\n\t\tif (dx_query_mob)\n\t\t\tret = vmw_validation_add_bo(sw_context->ctx,\n\t\t\t\t\t\t    dx_query_mob, true, false);\n\t}\n\n\tmutex_unlock(&dev_priv->binding_mutex);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,8 +18,9 @@\n \t\t\tif (IS_ERR(res))\n \t\t\t\tcontinue;\n \n-\t\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n-\t\t\t\t\t\t\t    VMW_RES_DIRTY_SET);\n+\t\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n+\t\t\t\t\t\t      VMW_RES_DIRTY_SET,\n+\t\t\t\t\t\t      vmw_val_add_flag_noctx);\n \t\t\tif (unlikely(ret != 0))\n \t\t\t\treturn ret;\n \t\t}\n@@ -33,9 +34,9 @@\n \t\tif (vmw_res_type(entry->res) == vmw_res_view)\n \t\t\tret = vmw_view_res_val_add(sw_context, entry->res);\n \t\telse\n-\t\t\tret = vmw_execbuf_res_noctx_val_add\n-\t\t\t\t(sw_context, entry->res,\n-\t\t\t\t vmw_binding_dirtying(entry->bt));\n+\t\t\tret = vmw_execbuf_res_val_add(sw_context, entry->res,\n+\t\t\t\t\t\t      vmw_binding_dirtying(entry->bt),\n+\t\t\t\t\t\t      vmw_val_add_flag_noctx);\n \t\tif (unlikely(ret != 0))\n \t\t\tbreak;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tret = vmw_execbuf_res_val_add(sw_context, res,",
                "\t\t\t\t\t\t      VMW_RES_DIRTY_SET,",
                "\t\t\t\t\t\t      vmw_val_add_flag_noctx);",
                "\t\t\tret = vmw_execbuf_res_val_add(sw_context, entry->res,",
                "\t\t\t\t\t\t      vmw_binding_dirtying(entry->bt),",
                "\t\t\t\t\t\t      vmw_val_add_flag_noctx);"
            ],
            "deleted": [
                "\t\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,",
                "\t\t\t\t\t\t\t    VMW_RES_DIRTY_SET);",
                "\t\t\tret = vmw_execbuf_res_noctx_val_add",
                "\t\t\t\t(sw_context, entry->res,",
                "\t\t\t\t vmw_binding_dirtying(entry->bt));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3687
    },
    {
        "cve_id": "CVE-2023-4244",
        "code_before_change": "static int nft_rcv_nl_event(struct notifier_block *this, unsigned long event,\n\t\t\t    void *ptr)\n{\n\tstruct nft_table *table, *to_delete[8];\n\tstruct nftables_pernet *nft_net;\n\tstruct netlink_notify *n = ptr;\n\tstruct net *net = n->net;\n\tunsigned int deleted;\n\tbool restart = false;\n\n\tif (event != NETLINK_URELEASE || n->protocol != NETLINK_NETFILTER)\n\t\treturn NOTIFY_DONE;\n\n\tnft_net = nft_pernet(net);\n\tdeleted = 0;\n\tmutex_lock(&nft_net->commit_mutex);\n\tif (!list_empty(&nf_tables_destroy_list))\n\t\trcu_barrier();\nagain:\n\tlist_for_each_entry(table, &nft_net->tables, list) {\n\t\tif (nft_table_has_owner(table) &&\n\t\t    n->portid == table->nlpid) {\n\t\t\t__nft_release_hook(net, table);\n\t\t\tlist_del_rcu(&table->list);\n\t\t\tto_delete[deleted++] = table;\n\t\t\tif (deleted >= ARRAY_SIZE(to_delete))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (deleted) {\n\t\trestart = deleted >= ARRAY_SIZE(to_delete);\n\t\tsynchronize_rcu();\n\t\twhile (deleted)\n\t\t\t__nft_release_table(net, to_delete[--deleted]);\n\n\t\tif (restart)\n\t\t\tgoto again;\n\t}\n\tmutex_unlock(&nft_net->commit_mutex);\n\n\treturn NOTIFY_DONE;\n}",
        "code_after_change": "static int nft_rcv_nl_event(struct notifier_block *this, unsigned long event,\n\t\t\t    void *ptr)\n{\n\tstruct nft_table *table, *to_delete[8];\n\tstruct nftables_pernet *nft_net;\n\tstruct netlink_notify *n = ptr;\n\tstruct net *net = n->net;\n\tunsigned int deleted;\n\tbool restart = false;\n\tunsigned int gc_seq;\n\n\tif (event != NETLINK_URELEASE || n->protocol != NETLINK_NETFILTER)\n\t\treturn NOTIFY_DONE;\n\n\tnft_net = nft_pernet(net);\n\tdeleted = 0;\n\tmutex_lock(&nft_net->commit_mutex);\n\n\tgc_seq = nft_gc_seq_begin(nft_net);\n\n\tif (!list_empty(&nf_tables_destroy_list))\n\t\trcu_barrier();\nagain:\n\tlist_for_each_entry(table, &nft_net->tables, list) {\n\t\tif (nft_table_has_owner(table) &&\n\t\t    n->portid == table->nlpid) {\n\t\t\t__nft_release_hook(net, table);\n\t\t\tlist_del_rcu(&table->list);\n\t\t\tto_delete[deleted++] = table;\n\t\t\tif (deleted >= ARRAY_SIZE(to_delete))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (deleted) {\n\t\trestart = deleted >= ARRAY_SIZE(to_delete);\n\t\tsynchronize_rcu();\n\t\twhile (deleted)\n\t\t\t__nft_release_table(net, to_delete[--deleted]);\n\n\t\tif (restart)\n\t\t\tgoto again;\n\t}\n\tnft_gc_seq_end(nft_net, gc_seq);\n\n\tmutex_unlock(&nft_net->commit_mutex);\n\n\treturn NOTIFY_DONE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tstruct net *net = n->net;\n \tunsigned int deleted;\n \tbool restart = false;\n+\tunsigned int gc_seq;\n \n \tif (event != NETLINK_URELEASE || n->protocol != NETLINK_NETFILTER)\n \t\treturn NOTIFY_DONE;\n@@ -14,6 +15,9 @@\n \tnft_net = nft_pernet(net);\n \tdeleted = 0;\n \tmutex_lock(&nft_net->commit_mutex);\n+\n+\tgc_seq = nft_gc_seq_begin(nft_net);\n+\n \tif (!list_empty(&nf_tables_destroy_list))\n \t\trcu_barrier();\n again:\n@@ -36,6 +40,8 @@\n \t\tif (restart)\n \t\t\tgoto again;\n \t}\n+\tnft_gc_seq_end(nft_net, gc_seq);\n+\n \tmutex_unlock(&nft_net->commit_mutex);\n \n \treturn NOTIFY_DONE;",
        "function_modified_lines": {
            "added": [
                "\tunsigned int gc_seq;",
                "",
                "\tgc_seq = nft_gc_seq_begin(nft_net);",
                "",
                "\tnft_gc_seq_end(nft_net, gc_seq);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nDue to a race condition between nf_tables netlink control plane transaction and nft_set element garbage collection, it is possible to underflow the reference counter causing a use-after-free vulnerability.\n\nWe recommend upgrading past commit 3e91b0ebd994635df2346353322ac51ce84ce6d8.\n\n",
        "id": 4202
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int\ncompat_copy_entries_to_user(unsigned int total_size, struct xt_table *table,\n\t\t\t    void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct ipt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\n\tvfree(counters);\n\treturn ret;\n}",
        "code_after_change": "static int\ncompat_copy_entries_to_user(unsigned int total_size, struct xt_table *table,\n\t\t\t    void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct ipt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\n\tvfree(counters);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t\t\t    void __user *userptr)\n {\n \tstruct xt_counters *counters;\n-\tconst struct xt_table_info *private = table->private;\n+\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n \tvoid __user *pos;\n \tunsigned int size;\n \tint ret = 0;",
        "function_modified_lines": {
            "added": [
                "\tconst struct xt_table_info *private = xt_table_get_private_protected(table);"
            ],
            "deleted": [
                "\tconst struct xt_table_info *private = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2781
    },
    {
        "cve_id": "CVE-2019-2025",
        "code_before_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t *offp, *off_end, *off_start;\n\tbinder_size_t off_min;\n\tu8 *sg_bufp, *sg_buf_end;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tstruct binder_buffer_object *last_fixup_obj = NULL;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\te->context_name = proc->context->name;\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc == proc) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tt->buffer->allow_user_free = 0;\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\toff_start = (binder_size_t *)(t->buffer->data +\n\t\t\t\t      ALIGN(tr->data_size, sizeof(void *)));\n\toffp = off_start;\n\n\tif (copy_from_user(t->buffer->data, (const void __user *)(uintptr_t)\n\t\t\t   tr->data.ptr.buffer, tr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (copy_from_user(offp, (const void __user *)(uintptr_t)\n\t\t\t   tr->data.ptr.offsets, tr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_end = (void *)off_start + tr->offsets_size;\n\tsg_bufp = (u8 *)(PTR_ALIGN(off_end, sizeof(void *)));\n\tsg_buf_end = sg_bufp + extra_buffers_size;\n\toff_min = 0;\n\tfor (; offp < off_end; offp++) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size = binder_validate_object(t->buffer, *offp);\n\n\t\tif (object_size == 0 || *offp < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid, (u64)*offp,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = (struct binder_object_header *)(t->buffer->data + *offp);\n\t\toff_min = *offp + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tint ret = binder_translate_fd(&fp->fd, t, thread,\n\t\t\t\t\t\t      in_reply_to);\n\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tfp->pad_binder = 0;\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(t->buffer, fda->parent,\n\t\t\t\t\t\t    off_start,\n\t\t\t\t\t\t    offp - off_start);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(t->buffer, off_start,\n\t\t\t\t\t\t   parent, fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj = parent;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end - sg_bufp;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (copy_from_user(sg_bufp,\n\t\t\t\t\t   (const void __user *)(uintptr_t)\n\t\t\t\t\t   bp->buffer, bp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)sg_bufp +\n\t\t\t\tbinder_alloc_get_user_buffer_offset(\n\t\t\t\t\t\t&target_proc->alloc);\n\t\t\tsg_bufp += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tret = binder_fixup_parent(t, thread, bp, off_start,\n\t\t\t\t\t\t  offp - off_start,\n\t\t\t\t\t\t  last_fixup_obj,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj = bp;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer, offp);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
        "code_after_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t *offp, *off_end, *off_start;\n\tbinder_size_t off_min;\n\tu8 *sg_bufp, *sg_buf_end;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tstruct binder_buffer_object *last_fixup_obj = NULL;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\te->context_name = proc->context->name;\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc == proc) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\toff_start = (binder_size_t *)(t->buffer->data +\n\t\t\t\t      ALIGN(tr->data_size, sizeof(void *)));\n\toffp = off_start;\n\n\tif (copy_from_user(t->buffer->data, (const void __user *)(uintptr_t)\n\t\t\t   tr->data.ptr.buffer, tr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (copy_from_user(offp, (const void __user *)(uintptr_t)\n\t\t\t   tr->data.ptr.offsets, tr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_end = (void *)off_start + tr->offsets_size;\n\tsg_bufp = (u8 *)(PTR_ALIGN(off_end, sizeof(void *)));\n\tsg_buf_end = sg_bufp + extra_buffers_size;\n\toff_min = 0;\n\tfor (; offp < off_end; offp++) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size = binder_validate_object(t->buffer, *offp);\n\n\t\tif (object_size == 0 || *offp < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid, (u64)*offp,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = (struct binder_object_header *)(t->buffer->data + *offp);\n\t\toff_min = *offp + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tint ret = binder_translate_fd(&fp->fd, t, thread,\n\t\t\t\t\t\t      in_reply_to);\n\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tfp->pad_binder = 0;\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(t->buffer, fda->parent,\n\t\t\t\t\t\t    off_start,\n\t\t\t\t\t\t    offp - off_start);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(t->buffer, off_start,\n\t\t\t\t\t\t   parent, fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj = parent;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end - sg_bufp;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (copy_from_user(sg_bufp,\n\t\t\t\t\t   (const void __user *)(uintptr_t)\n\t\t\t\t\t   bp->buffer, bp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)sg_bufp +\n\t\t\t\tbinder_alloc_get_user_buffer_offset(\n\t\t\t\t\t\t&target_proc->alloc);\n\t\t\tsg_bufp += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tret = binder_fixup_parent(t, thread, bp, off_start,\n\t\t\t\t\t\t  offp - off_start,\n\t\t\t\t\t\t  last_fixup_obj,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj = bp;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer, offp);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -278,7 +278,6 @@\n \t\tt->buffer = NULL;\n \t\tgoto err_binder_alloc_buf_failed;\n \t}\n-\tt->buffer->allow_user_free = 0;\n \tt->buffer->debug_id = t->debug_id;\n \tt->buffer->transaction = t;\n \tt->buffer->target_node = target_node;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tt->buffer->allow_user_free = 0;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_thread_read of binder.c, there is a possible use-after-free due to improper locking. This could lead to local escalation of privilege in the kernel with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-116855682References: Upstream kernel",
        "id": 2275
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "void *xt_unregister_table(struct xt_table *table)\n{\n\tstruct xt_table_info *private;\n\n\tmutex_lock(&xt[table->af].mutex);\n\tprivate = table->private;\n\tlist_del(&table->list);\n\tmutex_unlock(&xt[table->af].mutex);\n\taudit_log_nfcfg(table->name, table->af, private->number,\n\t\t\tAUDIT_XT_OP_UNREGISTER, GFP_KERNEL);\n\tkfree(table);\n\n\treturn private;\n}",
        "code_after_change": "void *xt_unregister_table(struct xt_table *table)\n{\n\tstruct xt_table_info *private;\n\n\tmutex_lock(&xt[table->af].mutex);\n\tprivate = xt_table_get_private_protected(table);\n\tRCU_INIT_POINTER(table->private, NULL);\n\tlist_del(&table->list);\n\tmutex_unlock(&xt[table->af].mutex);\n\taudit_log_nfcfg(table->name, table->af, private->number,\n\t\t\tAUDIT_XT_OP_UNREGISTER, GFP_KERNEL);\n\tkfree(table);\n\n\treturn private;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,8 @@\n \tstruct xt_table_info *private;\n \n \tmutex_lock(&xt[table->af].mutex);\n-\tprivate = table->private;\n+\tprivate = xt_table_get_private_protected(table);\n+\tRCU_INIT_POINTER(table->private, NULL);\n \tlist_del(&table->list);\n \tmutex_unlock(&xt[table->af].mutex);\n \taudit_log_nfcfg(table->name, table->af, private->number,",
        "function_modified_lines": {
            "added": [
                "\tprivate = xt_table_get_private_protected(table);",
                "\tRCU_INIT_POINTER(table->private, NULL);"
            ],
            "deleted": [
                "\tprivate = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2791
    },
    {
        "cve_id": "CVE-2023-6039",
        "code_before_change": "static void lan78xx_disconnect(struct usb_interface *intf)\n{\n\tstruct lan78xx_net *dev;\n\tstruct usb_device *udev;\n\tstruct net_device *net;\n\tstruct phy_device *phydev;\n\n\tdev = usb_get_intfdata(intf);\n\tusb_set_intfdata(intf, NULL);\n\tif (!dev)\n\t\treturn;\n\n\tset_bit(EVENT_DEV_DISCONNECT, &dev->flags);\n\n\tnetif_napi_del(&dev->napi);\n\n\tudev = interface_to_usbdev(intf);\n\tnet = dev->net;\n\n\tunregister_netdev(net);\n\n\tcancel_delayed_work_sync(&dev->wq);\n\n\tphydev = net->phydev;\n\n\tphy_unregister_fixup_for_uid(PHY_KSZ9031RNX, 0xfffffff0);\n\tphy_unregister_fixup_for_uid(PHY_LAN8835, 0xfffffff0);\n\n\tphy_disconnect(net->phydev);\n\n\tif (phy_is_pseudo_fixed_link(phydev))\n\t\tfixed_phy_unregister(phydev);\n\n\tusb_scuttle_anchored_urbs(&dev->deferred);\n\n\tif (timer_pending(&dev->stat_monitor))\n\t\tdel_timer_sync(&dev->stat_monitor);\n\n\tlan78xx_unbind(dev, intf);\n\n\tlan78xx_free_tx_resources(dev);\n\tlan78xx_free_rx_resources(dev);\n\n\tusb_kill_urb(dev->urb_intr);\n\tusb_free_urb(dev->urb_intr);\n\n\tfree_netdev(net);\n\tusb_put_dev(udev);\n}",
        "code_after_change": "static void lan78xx_disconnect(struct usb_interface *intf)\n{\n\tstruct lan78xx_net *dev;\n\tstruct usb_device *udev;\n\tstruct net_device *net;\n\tstruct phy_device *phydev;\n\n\tdev = usb_get_intfdata(intf);\n\tusb_set_intfdata(intf, NULL);\n\tif (!dev)\n\t\treturn;\n\n\tnetif_napi_del(&dev->napi);\n\n\tudev = interface_to_usbdev(intf);\n\tnet = dev->net;\n\n\tunregister_netdev(net);\n\n\ttimer_shutdown_sync(&dev->stat_monitor);\n\tset_bit(EVENT_DEV_DISCONNECT, &dev->flags);\n\tcancel_delayed_work_sync(&dev->wq);\n\n\tphydev = net->phydev;\n\n\tphy_unregister_fixup_for_uid(PHY_KSZ9031RNX, 0xfffffff0);\n\tphy_unregister_fixup_for_uid(PHY_LAN8835, 0xfffffff0);\n\n\tphy_disconnect(net->phydev);\n\n\tif (phy_is_pseudo_fixed_link(phydev))\n\t\tfixed_phy_unregister(phydev);\n\n\tusb_scuttle_anchored_urbs(&dev->deferred);\n\n\tlan78xx_unbind(dev, intf);\n\n\tlan78xx_free_tx_resources(dev);\n\tlan78xx_free_rx_resources(dev);\n\n\tusb_kill_urb(dev->urb_intr);\n\tusb_free_urb(dev->urb_intr);\n\n\tfree_netdev(net);\n\tusb_put_dev(udev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,8 +10,6 @@\n \tif (!dev)\n \t\treturn;\n \n-\tset_bit(EVENT_DEV_DISCONNECT, &dev->flags);\n-\n \tnetif_napi_del(&dev->napi);\n \n \tudev = interface_to_usbdev(intf);\n@@ -19,6 +17,8 @@\n \n \tunregister_netdev(net);\n \n+\ttimer_shutdown_sync(&dev->stat_monitor);\n+\tset_bit(EVENT_DEV_DISCONNECT, &dev->flags);\n \tcancel_delayed_work_sync(&dev->wq);\n \n \tphydev = net->phydev;\n@@ -33,9 +33,6 @@\n \n \tusb_scuttle_anchored_urbs(&dev->deferred);\n \n-\tif (timer_pending(&dev->stat_monitor))\n-\t\tdel_timer_sync(&dev->stat_monitor);\n-\n \tlan78xx_unbind(dev, intf);\n \n \tlan78xx_free_tx_resources(dev);",
        "function_modified_lines": {
            "added": [
                "\ttimer_shutdown_sync(&dev->stat_monitor);",
                "\tset_bit(EVENT_DEV_DISCONNECT, &dev->flags);"
            ],
            "deleted": [
                "\tset_bit(EVENT_DEV_DISCONNECT, &dev->flags);",
                "",
                "\tif (timer_pending(&dev->stat_monitor))",
                "\t\tdel_timer_sync(&dev->stat_monitor);",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in lan78xx_disconnect in drivers/net/usb/lan78xx.c in the network sub-component, net/usb/lan78xx in the Linux Kernel. This flaw allows a local attacker to crash the system when the LAN78XX USB device detaches.",
        "id": 4294
    },
    {
        "cve_id": "CVE-2020-27786",
        "code_before_change": "static long snd_rawmidi_kernel_write1(struct snd_rawmidi_substream *substream,\n\t\t\t\t      const unsigned char __user *userbuf,\n\t\t\t\t      const unsigned char *kernelbuf,\n\t\t\t\t      long count)\n{\n\tunsigned long flags;\n\tlong count1, result;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\tunsigned long appl_ptr;\n\n\tif (!kernelbuf && !userbuf)\n\t\treturn -EINVAL;\n\tif (snd_BUG_ON(!runtime->buffer))\n\t\treturn -EINVAL;\n\n\tresult = 0;\n\tspin_lock_irqsave(&runtime->lock, flags);\n\tif (substream->append) {\n\t\tif ((long)runtime->avail < count) {\n\t\t\tspin_unlock_irqrestore(&runtime->lock, flags);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\twhile (count > 0 && runtime->avail > 0) {\n\t\tcount1 = runtime->buffer_size - runtime->appl_ptr;\n\t\tif (count1 > count)\n\t\t\tcount1 = count;\n\t\tif (count1 > (long)runtime->avail)\n\t\t\tcount1 = runtime->avail;\n\n\t\t/* update runtime->appl_ptr before unlocking for userbuf */\n\t\tappl_ptr = runtime->appl_ptr;\n\t\truntime->appl_ptr += count1;\n\t\truntime->appl_ptr %= runtime->buffer_size;\n\t\truntime->avail -= count1;\n\n\t\tif (kernelbuf)\n\t\t\tmemcpy(runtime->buffer + appl_ptr,\n\t\t\t       kernelbuf + result, count1);\n\t\telse if (userbuf) {\n\t\t\tspin_unlock_irqrestore(&runtime->lock, flags);\n\t\t\tif (copy_from_user(runtime->buffer + appl_ptr,\n\t\t\t\t\t   userbuf + result, count1)) {\n\t\t\t\tspin_lock_irqsave(&runtime->lock, flags);\n\t\t\t\tresult = result > 0 ? result : -EFAULT;\n\t\t\t\tgoto __end;\n\t\t\t}\n\t\t\tspin_lock_irqsave(&runtime->lock, flags);\n\t\t}\n\t\tresult += count1;\n\t\tcount -= count1;\n\t}\n      __end:\n\tcount1 = runtime->avail < runtime->buffer_size;\n\tspin_unlock_irqrestore(&runtime->lock, flags);\n\tif (count1)\n\t\tsnd_rawmidi_output_trigger(substream, 1);\n\treturn result;\n}",
        "code_after_change": "static long snd_rawmidi_kernel_write1(struct snd_rawmidi_substream *substream,\n\t\t\t\t      const unsigned char __user *userbuf,\n\t\t\t\t      const unsigned char *kernelbuf,\n\t\t\t\t      long count)\n{\n\tunsigned long flags;\n\tlong count1, result;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\tunsigned long appl_ptr;\n\n\tif (!kernelbuf && !userbuf)\n\t\treturn -EINVAL;\n\tif (snd_BUG_ON(!runtime->buffer))\n\t\treturn -EINVAL;\n\n\tresult = 0;\n\tspin_lock_irqsave(&runtime->lock, flags);\n\tif (substream->append) {\n\t\tif ((long)runtime->avail < count) {\n\t\t\tspin_unlock_irqrestore(&runtime->lock, flags);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\tsnd_rawmidi_buffer_ref(runtime);\n\twhile (count > 0 && runtime->avail > 0) {\n\t\tcount1 = runtime->buffer_size - runtime->appl_ptr;\n\t\tif (count1 > count)\n\t\t\tcount1 = count;\n\t\tif (count1 > (long)runtime->avail)\n\t\t\tcount1 = runtime->avail;\n\n\t\t/* update runtime->appl_ptr before unlocking for userbuf */\n\t\tappl_ptr = runtime->appl_ptr;\n\t\truntime->appl_ptr += count1;\n\t\truntime->appl_ptr %= runtime->buffer_size;\n\t\truntime->avail -= count1;\n\n\t\tif (kernelbuf)\n\t\t\tmemcpy(runtime->buffer + appl_ptr,\n\t\t\t       kernelbuf + result, count1);\n\t\telse if (userbuf) {\n\t\t\tspin_unlock_irqrestore(&runtime->lock, flags);\n\t\t\tif (copy_from_user(runtime->buffer + appl_ptr,\n\t\t\t\t\t   userbuf + result, count1)) {\n\t\t\t\tspin_lock_irqsave(&runtime->lock, flags);\n\t\t\t\tresult = result > 0 ? result : -EFAULT;\n\t\t\t\tgoto __end;\n\t\t\t}\n\t\t\tspin_lock_irqsave(&runtime->lock, flags);\n\t\t}\n\t\tresult += count1;\n\t\tcount -= count1;\n\t}\n      __end:\n\tcount1 = runtime->avail < runtime->buffer_size;\n\tsnd_rawmidi_buffer_unref(runtime);\n\tspin_unlock_irqrestore(&runtime->lock, flags);\n\tif (count1)\n\t\tsnd_rawmidi_output_trigger(substream, 1);\n\treturn result;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,7 @@\n \t\t\treturn -EAGAIN;\n \t\t}\n \t}\n+\tsnd_rawmidi_buffer_ref(runtime);\n \twhile (count > 0 && runtime->avail > 0) {\n \t\tcount1 = runtime->buffer_size - runtime->appl_ptr;\n \t\tif (count1 > count)\n@@ -52,6 +53,7 @@\n \t}\n       __end:\n \tcount1 = runtime->avail < runtime->buffer_size;\n+\tsnd_rawmidi_buffer_unref(runtime);\n \tspin_unlock_irqrestore(&runtime->lock, flags);\n \tif (count1)\n \t\tsnd_rawmidi_output_trigger(substream, 1);",
        "function_modified_lines": {
            "added": [
                "\tsnd_rawmidi_buffer_ref(runtime);",
                "\tsnd_rawmidi_buffer_unref(runtime);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s implementation of MIDI, where an attacker with a local account and the permissions to issue ioctl commands to midi devices could trigger a use-after-free issue. A write to this specific memory while freed and before use causes the flow of execution to change and possibly allow for memory corruption or privilege escalation. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability.",
        "id": 2634
    },
    {
        "cve_id": "CVE-2019-19527",
        "code_before_change": "static int hiddev_open(struct inode *inode, struct file *file)\n{\n\tstruct hiddev_list *list;\n\tstruct usb_interface *intf;\n\tstruct hid_device *hid;\n\tstruct hiddev *hiddev;\n\tint res;\n\n\tintf = usbhid_find_interface(iminor(inode));\n\tif (!intf)\n\t\treturn -ENODEV;\n\thid = usb_get_intfdata(intf);\n\thiddev = hid->hiddev;\n\n\tif (!(list = vzalloc(sizeof(struct hiddev_list))))\n\t\treturn -ENOMEM;\n\tmutex_init(&list->thread_lock);\n\tlist->hiddev = hiddev;\n\tfile->private_data = list;\n\n\t/*\n\t * no need for locking because the USB major number\n\t * is shared which usbcore guards against disconnect\n\t */\n\tif (list->hiddev->exist) {\n\t\tif (!list->hiddev->open++) {\n\t\t\tres = hid_hw_open(hiddev->hid);\n\t\t\tif (res < 0)\n\t\t\t\tgoto bail;\n\t\t}\n\t} else {\n\t\tres = -ENODEV;\n\t\tgoto bail;\n\t}\n\n\tspin_lock_irq(&list->hiddev->list_lock);\n\tlist_add_tail(&list->node, &hiddev->list);\n\tspin_unlock_irq(&list->hiddev->list_lock);\n\n\tmutex_lock(&hiddev->existancelock);\n\t/*\n\t * recheck exist with existance lock held to\n\t * avoid opening a disconnected device\n\t */\n\tif (!list->hiddev->exist) {\n\t\tres = -ENODEV;\n\t\tgoto bail_unlock;\n\t}\n\tif (!list->hiddev->open++)\n\t\tif (list->hiddev->exist) {\n\t\t\tstruct hid_device *hid = hiddev->hid;\n\t\t\tres = hid_hw_power(hid, PM_HINT_FULLON);\n\t\t\tif (res < 0)\n\t\t\t\tgoto bail_unlock;\n\t\t\tres = hid_hw_open(hid);\n\t\t\tif (res < 0)\n\t\t\t\tgoto bail_normal_power;\n\t\t}\n\tmutex_unlock(&hiddev->existancelock);\n\treturn 0;\nbail_normal_power:\n\thid_hw_power(hid, PM_HINT_NORMAL);\nbail_unlock:\n\tmutex_unlock(&hiddev->existancelock);\nbail:\n\tfile->private_data = NULL;\n\tvfree(list);\n\treturn res;\n}",
        "code_after_change": "static int hiddev_open(struct inode *inode, struct file *file)\n{\n\tstruct hiddev_list *list;\n\tstruct usb_interface *intf;\n\tstruct hid_device *hid;\n\tstruct hiddev *hiddev;\n\tint res;\n\n\tintf = usbhid_find_interface(iminor(inode));\n\tif (!intf)\n\t\treturn -ENODEV;\n\thid = usb_get_intfdata(intf);\n\thiddev = hid->hiddev;\n\n\tif (!(list = vzalloc(sizeof(struct hiddev_list))))\n\t\treturn -ENOMEM;\n\tmutex_init(&list->thread_lock);\n\tlist->hiddev = hiddev;\n\tfile->private_data = list;\n\n\t/*\n\t * no need for locking because the USB major number\n\t * is shared which usbcore guards against disconnect\n\t */\n\tif (list->hiddev->exist) {\n\t\tif (!list->hiddev->open++) {\n\t\t\tres = hid_hw_open(hiddev->hid);\n\t\t\tif (res < 0)\n\t\t\t\tgoto bail;\n\t\t}\n\t} else {\n\t\tres = -ENODEV;\n\t\tgoto bail;\n\t}\n\n\tspin_lock_irq(&list->hiddev->list_lock);\n\tlist_add_tail(&list->node, &hiddev->list);\n\tspin_unlock_irq(&list->hiddev->list_lock);\n\n\tmutex_lock(&hiddev->existancelock);\n\t/*\n\t * recheck exist with existance lock held to\n\t * avoid opening a disconnected device\n\t */\n\tif (!list->hiddev->exist) {\n\t\tres = -ENODEV;\n\t\tgoto bail_unlock;\n\t}\n\tif (!list->hiddev->open++)\n\t\tif (list->hiddev->exist) {\n\t\t\tstruct hid_device *hid = hiddev->hid;\n\t\t\tres = hid_hw_power(hid, PM_HINT_FULLON);\n\t\t\tif (res < 0)\n\t\t\t\tgoto bail_unlock;\n\t\t\tres = hid_hw_open(hid);\n\t\t\tif (res < 0)\n\t\t\t\tgoto bail_normal_power;\n\t\t}\n\tmutex_unlock(&hiddev->existancelock);\n\treturn 0;\nbail_normal_power:\n\thid_hw_power(hid, PM_HINT_NORMAL);\nbail_unlock:\n\tmutex_unlock(&hiddev->existancelock);\n\n\tspin_lock_irq(&list->hiddev->list_lock);\n\tlist_del(&list->node);\n\tspin_unlock_irq(&list->hiddev->list_lock);\nbail:\n\tfile->private_data = NULL;\n\tvfree(list);\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -62,6 +62,10 @@\n \thid_hw_power(hid, PM_HINT_NORMAL);\n bail_unlock:\n \tmutex_unlock(&hiddev->existancelock);\n+\n+\tspin_lock_irq(&list->hiddev->list_lock);\n+\tlist_del(&list->node);\n+\tspin_unlock_irq(&list->hiddev->list_lock);\n bail:\n \tfile->private_data = NULL;\n \tvfree(list);",
        "function_modified_lines": {
            "added": [
                "",
                "\tspin_lock_irq(&list->hiddev->list_lock);",
                "\tlist_del(&list->node);",
                "\tspin_unlock_irq(&list->hiddev->list_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.2.10, there is a use-after-free bug that can be caused by a malicious USB device in the drivers/hid/usbhid/hiddev.c driver, aka CID-9c09b214f30e.",
        "id": 2202
    },
    {
        "cve_id": "CVE-2022-42719",
        "code_before_change": "struct ieee802_11_elems *\nieee802_11_parse_elems_full(struct ieee80211_elems_parse_params *params)\n{\n\tstruct ieee802_11_elems *elems;\n\tconst struct element *non_inherit = NULL;\n\tu8 *nontransmitted_profile;\n\tint nontransmitted_profile_len = 0;\n\n\telems = kzalloc(sizeof(*elems), GFP_ATOMIC);\n\tif (!elems)\n\t\treturn NULL;\n\telems->ie_start = params->start;\n\telems->total_len = params->len;\n\n\tnontransmitted_profile = kmalloc(params->len, GFP_ATOMIC);\n\tif (nontransmitted_profile) {\n\t\tnontransmitted_profile_len =\n\t\t\tieee802_11_find_bssid_profile(params->start, params->len,\n\t\t\t\t\t\t      elems, params->bss,\n\t\t\t\t\t\t      nontransmitted_profile);\n\t\tnon_inherit =\n\t\t\tcfg80211_find_ext_elem(WLAN_EID_EXT_NON_INHERITANCE,\n\t\t\t\t\t       nontransmitted_profile,\n\t\t\t\t\t       nontransmitted_profile_len);\n\t}\n\n\telems->crc = _ieee802_11_parse_elems_full(params, elems, non_inherit);\n\n\t/* Override with nontransmitted profile, if found */\n\tif (nontransmitted_profile_len) {\n\t\tstruct ieee80211_elems_parse_params sub = {\n\t\t\t.start = nontransmitted_profile,\n\t\t\t.len = nontransmitted_profile_len,\n\t\t\t.action = params->action,\n\t\t\t.link_id = params->link_id,\n\t\t};\n\n\t\t_ieee802_11_parse_elems_full(&sub, elems, NULL);\n\t}\n\n\tif (elems->tim && !elems->parse_error) {\n\t\tconst struct ieee80211_tim_ie *tim_ie = elems->tim;\n\n\t\telems->dtim_period = tim_ie->dtim_period;\n\t\telems->dtim_count = tim_ie->dtim_count;\n\t}\n\n\t/* Override DTIM period and count if needed */\n\tif (elems->bssid_index &&\n\t    elems->bssid_index_len >=\n\t    offsetofend(struct ieee80211_bssid_index, dtim_period))\n\t\telems->dtim_period = elems->bssid_index->dtim_period;\n\n\tif (elems->bssid_index &&\n\t    elems->bssid_index_len >=\n\t    offsetofend(struct ieee80211_bssid_index, dtim_count))\n\t\telems->dtim_count = elems->bssid_index->dtim_count;\n\n\tkfree(nontransmitted_profile);\n\n\treturn elems;\n}",
        "code_after_change": "struct ieee802_11_elems *\nieee802_11_parse_elems_full(struct ieee80211_elems_parse_params *params)\n{\n\tstruct ieee802_11_elems *elems;\n\tconst struct element *non_inherit = NULL;\n\tu8 *nontransmitted_profile;\n\tint nontransmitted_profile_len = 0;\n\tsize_t scratch_len = params->len;\n\n\telems = kzalloc(sizeof(*elems) + scratch_len, GFP_ATOMIC);\n\tif (!elems)\n\t\treturn NULL;\n\telems->ie_start = params->start;\n\telems->total_len = params->len;\n\telems->scratch_len = scratch_len;\n\telems->scratch_pos = elems->scratch;\n\n\tnontransmitted_profile = elems->scratch_pos;\n\tnontransmitted_profile_len =\n\t\tieee802_11_find_bssid_profile(params->start, params->len,\n\t\t\t\t\t      elems, params->bss,\n\t\t\t\t\t      nontransmitted_profile);\n\telems->scratch_pos += nontransmitted_profile_len;\n\telems->scratch_len -= nontransmitted_profile_len;\n\tnon_inherit = cfg80211_find_ext_elem(WLAN_EID_EXT_NON_INHERITANCE,\n\t\t\t\t\t     nontransmitted_profile,\n\t\t\t\t\t     nontransmitted_profile_len);\n\n\telems->crc = _ieee802_11_parse_elems_full(params, elems, non_inherit);\n\n\t/* Override with nontransmitted profile, if found */\n\tif (nontransmitted_profile_len) {\n\t\tstruct ieee80211_elems_parse_params sub = {\n\t\t\t.start = nontransmitted_profile,\n\t\t\t.len = nontransmitted_profile_len,\n\t\t\t.action = params->action,\n\t\t\t.link_id = params->link_id,\n\t\t};\n\n\t\t_ieee802_11_parse_elems_full(&sub, elems, NULL);\n\t}\n\n\tif (elems->tim && !elems->parse_error) {\n\t\tconst struct ieee80211_tim_ie *tim_ie = elems->tim;\n\n\t\telems->dtim_period = tim_ie->dtim_period;\n\t\telems->dtim_count = tim_ie->dtim_count;\n\t}\n\n\t/* Override DTIM period and count if needed */\n\tif (elems->bssid_index &&\n\t    elems->bssid_index_len >=\n\t    offsetofend(struct ieee80211_bssid_index, dtim_period))\n\t\telems->dtim_period = elems->bssid_index->dtim_period;\n\n\tif (elems->bssid_index &&\n\t    elems->bssid_index_len >=\n\t    offsetofend(struct ieee80211_bssid_index, dtim_count))\n\t\telems->dtim_count = elems->bssid_index->dtim_count;\n\n\treturn elems;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,24 +5,26 @@\n \tconst struct element *non_inherit = NULL;\n \tu8 *nontransmitted_profile;\n \tint nontransmitted_profile_len = 0;\n+\tsize_t scratch_len = params->len;\n \n-\telems = kzalloc(sizeof(*elems), GFP_ATOMIC);\n+\telems = kzalloc(sizeof(*elems) + scratch_len, GFP_ATOMIC);\n \tif (!elems)\n \t\treturn NULL;\n \telems->ie_start = params->start;\n \telems->total_len = params->len;\n+\telems->scratch_len = scratch_len;\n+\telems->scratch_pos = elems->scratch;\n \n-\tnontransmitted_profile = kmalloc(params->len, GFP_ATOMIC);\n-\tif (nontransmitted_profile) {\n-\t\tnontransmitted_profile_len =\n-\t\t\tieee802_11_find_bssid_profile(params->start, params->len,\n-\t\t\t\t\t\t      elems, params->bss,\n-\t\t\t\t\t\t      nontransmitted_profile);\n-\t\tnon_inherit =\n-\t\t\tcfg80211_find_ext_elem(WLAN_EID_EXT_NON_INHERITANCE,\n-\t\t\t\t\t       nontransmitted_profile,\n-\t\t\t\t\t       nontransmitted_profile_len);\n-\t}\n+\tnontransmitted_profile = elems->scratch_pos;\n+\tnontransmitted_profile_len =\n+\t\tieee802_11_find_bssid_profile(params->start, params->len,\n+\t\t\t\t\t      elems, params->bss,\n+\t\t\t\t\t      nontransmitted_profile);\n+\telems->scratch_pos += nontransmitted_profile_len;\n+\telems->scratch_len -= nontransmitted_profile_len;\n+\tnon_inherit = cfg80211_find_ext_elem(WLAN_EID_EXT_NON_INHERITANCE,\n+\t\t\t\t\t     nontransmitted_profile,\n+\t\t\t\t\t     nontransmitted_profile_len);\n \n \telems->crc = _ieee802_11_parse_elems_full(params, elems, non_inherit);\n \n@@ -56,7 +58,5 @@\n \t    offsetofend(struct ieee80211_bssid_index, dtim_count))\n \t\telems->dtim_count = elems->bssid_index->dtim_count;\n \n-\tkfree(nontransmitted_profile);\n-\n \treturn elems;\n }",
        "function_modified_lines": {
            "added": [
                "\tsize_t scratch_len = params->len;",
                "\telems = kzalloc(sizeof(*elems) + scratch_len, GFP_ATOMIC);",
                "\telems->scratch_len = scratch_len;",
                "\telems->scratch_pos = elems->scratch;",
                "\tnontransmitted_profile = elems->scratch_pos;",
                "\tnontransmitted_profile_len =",
                "\t\tieee802_11_find_bssid_profile(params->start, params->len,",
                "\t\t\t\t\t      elems, params->bss,",
                "\t\t\t\t\t      nontransmitted_profile);",
                "\telems->scratch_pos += nontransmitted_profile_len;",
                "\telems->scratch_len -= nontransmitted_profile_len;",
                "\tnon_inherit = cfg80211_find_ext_elem(WLAN_EID_EXT_NON_INHERITANCE,",
                "\t\t\t\t\t     nontransmitted_profile,",
                "\t\t\t\t\t     nontransmitted_profile_len);"
            ],
            "deleted": [
                "\telems = kzalloc(sizeof(*elems), GFP_ATOMIC);",
                "\tnontransmitted_profile = kmalloc(params->len, GFP_ATOMIC);",
                "\tif (nontransmitted_profile) {",
                "\t\tnontransmitted_profile_len =",
                "\t\t\tieee802_11_find_bssid_profile(params->start, params->len,",
                "\t\t\t\t\t\t      elems, params->bss,",
                "\t\t\t\t\t\t      nontransmitted_profile);",
                "\t\tnon_inherit =",
                "\t\t\tcfg80211_find_ext_elem(WLAN_EID_EXT_NON_INHERITANCE,",
                "\t\t\t\t\t       nontransmitted_profile,",
                "\t\t\t\t\t       nontransmitted_profile_len);",
                "\t}",
                "\tkfree(nontransmitted_profile);",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free in the mac80211 stack when parsing a multi-BSSID element in the Linux kernel 5.2 through 5.19.x before 5.19.16 could be used by attackers (able to inject WLAN frames) to crash the kernel and potentially execute code.",
        "id": 3732
    },
    {
        "cve_id": "CVE-2018-16884",
        "code_before_change": "void svc_reserve(struct svc_rqst *rqstp, int space)\n{\n\tspace += rqstp->rq_res.head[0].iov_len;\n\n\tif (space < rqstp->rq_reserved) {\n\t\tstruct svc_xprt *xprt = rqstp->rq_xprt;\n\t\tatomic_sub((rqstp->rq_reserved - space), &xprt->xpt_reserved);\n\t\trqstp->rq_reserved = space;\n\n\t\tsvc_xprt_enqueue(xprt);\n\t}\n}",
        "code_after_change": "void svc_reserve(struct svc_rqst *rqstp, int space)\n{\n\tstruct svc_xprt *xprt = rqstp->rq_xprt;\n\n\tspace += rqstp->rq_res.head[0].iov_len;\n\n\tif (xprt && space < rqstp->rq_reserved) {\n\t\tatomic_sub((rqstp->rq_reserved - space), &xprt->xpt_reserved);\n\t\trqstp->rq_reserved = space;\n\n\t\tsvc_xprt_enqueue(xprt);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,9 +1,10 @@\n void svc_reserve(struct svc_rqst *rqstp, int space)\n {\n+\tstruct svc_xprt *xprt = rqstp->rq_xprt;\n+\n \tspace += rqstp->rq_res.head[0].iov_len;\n \n-\tif (space < rqstp->rq_reserved) {\n-\t\tstruct svc_xprt *xprt = rqstp->rq_xprt;\n+\tif (xprt && space < rqstp->rq_reserved) {\n \t\tatomic_sub((rqstp->rq_reserved - space), &xprt->xpt_reserved);\n \t\trqstp->rq_reserved = space;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct svc_xprt *xprt = rqstp->rq_xprt;",
                "",
                "\tif (xprt && space < rqstp->rq_reserved) {"
            ],
            "deleted": [
                "\tif (space < rqstp->rq_reserved) {",
                "\t\tstruct svc_xprt *xprt = rqstp->rq_xprt;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's NFS41+ subsystem. NFS41+ shares mounted in different network namespaces at the same time can make bc_svc_process() use wrong back-channel IDs and cause a use-after-free vulnerability. Thus a malicious container user can cause a host kernel memory corruption and a system panic. Due to the nature of the flaw, privilege escalation cannot be fully ruled out.",
        "id": 1724
    },
    {
        "cve_id": "CVE-2022-1043",
        "code_before_change": "static int io_register_personality(struct io_ring_ctx *ctx)\n{\n\tconst struct cred *creds;\n\tu32 id;\n\tint ret;\n\n\tcreds = get_current_cred();\n\n\tret = xa_alloc_cyclic(&ctx->personalities, &id, (void *)creds,\n\t\t\tXA_LIMIT(0, USHRT_MAX), &ctx->pers_next, GFP_KERNEL);\n\tif (!ret)\n\t\treturn id;\n\tput_cred(creds);\n\treturn ret;\n}",
        "code_after_change": "static int io_register_personality(struct io_ring_ctx *ctx)\n{\n\tconst struct cred *creds;\n\tu32 id;\n\tint ret;\n\n\tcreds = get_current_cred();\n\n\tret = xa_alloc_cyclic(&ctx->personalities, &id, (void *)creds,\n\t\t\tXA_LIMIT(0, USHRT_MAX), &ctx->pers_next, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tput_cred(creds);\n\t\treturn ret;\n\t}\n\treturn id;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,9 @@\n \n \tret = xa_alloc_cyclic(&ctx->personalities, &id, (void *)creds,\n \t\t\tXA_LIMIT(0, USHRT_MAX), &ctx->pers_next, GFP_KERNEL);\n-\tif (!ret)\n-\t\treturn id;\n-\tput_cred(creds);\n-\treturn ret;\n+\tif (ret < 0) {\n+\t\tput_cred(creds);\n+\t\treturn ret;\n+\t}\n+\treturn id;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (ret < 0) {",
                "\t\tput_cred(creds);",
                "\t\treturn ret;",
                "\t}",
                "\treturn id;"
            ],
            "deleted": [
                "\tif (!ret)",
                "\t\treturn id;",
                "\tput_cred(creds);",
                "\treturn ret;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s io_uring implementation. This flaw allows an attacker with a local account to corrupt system memory, crash the system or escalate privileges.",
        "id": 3241
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static void l2cap_move_fail(struct l2cap_conn *conn, u8 ident, u16 icid,\n\t\t\t    u16 result)\n{\n\tstruct l2cap_chan *chan;\n\n\tchan = l2cap_get_chan_by_ident(conn, ident);\n\tif (!chan) {\n\t\t/* Could not locate channel, icid is best guess */\n\t\tl2cap_send_move_chan_cfm_icid(conn, icid);\n\t\treturn;\n\t}\n\n\t__clear_chan_timer(chan);\n\n\tif (chan->move_role == L2CAP_MOVE_ROLE_INITIATOR) {\n\t\tif (result == L2CAP_MR_COLLISION) {\n\t\t\tchan->move_role = L2CAP_MOVE_ROLE_RESPONDER;\n\t\t} else {\n\t\t\t/* Cleanup - cancel move */\n\t\t\tchan->move_id = chan->local_amp_id;\n\t\t\tl2cap_move_done(chan);\n\t\t}\n\t}\n\n\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_UNCONFIRMED);\n\n\tl2cap_chan_unlock(chan);\n}",
        "code_after_change": "static void l2cap_move_fail(struct l2cap_conn *conn, u8 ident, u16 icid,\n\t\t\t    u16 result)\n{\n\tstruct l2cap_chan *chan;\n\n\tchan = l2cap_get_chan_by_ident(conn, ident);\n\tif (!chan) {\n\t\t/* Could not locate channel, icid is best guess */\n\t\tl2cap_send_move_chan_cfm_icid(conn, icid);\n\t\treturn;\n\t}\n\n\t__clear_chan_timer(chan);\n\n\tif (chan->move_role == L2CAP_MOVE_ROLE_INITIATOR) {\n\t\tif (result == L2CAP_MR_COLLISION) {\n\t\t\tchan->move_role = L2CAP_MOVE_ROLE_RESPONDER;\n\t\t} else {\n\t\t\t/* Cleanup - cancel move */\n\t\t\tchan->move_id = chan->local_amp_id;\n\t\t\tl2cap_move_done(chan);\n\t\t}\n\t}\n\n\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_UNCONFIRMED);\n\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,4 +25,5 @@\n \tl2cap_send_move_chan_cfm(chan, L2CAP_MC_UNCONFIRMED);\n \n \tl2cap_chan_unlock(chan);\n+\tl2cap_chan_put(chan);\n }",
        "function_modified_lines": {
            "added": [
                "\tl2cap_chan_put(chan);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3396
    },
    {
        "cve_id": "CVE-2019-19543",
        "code_before_change": "static int __init serial_ir_init_module(void)\n{\n\tint result;\n\n\tswitch (type) {\n\tcase IR_HOMEBREW:\n\tcase IR_IRDEO:\n\tcase IR_IRDEO_REMOTE:\n\tcase IR_ANIMAX:\n\tcase IR_IGOR:\n\t\t/* if nothing specified, use ttyS0/com1 and irq 4 */\n\t\tio = io ? io : 0x3f8;\n\t\tirq = irq ? irq : 4;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tif (!softcarrier) {\n\t\tswitch (type) {\n\t\tcase IR_HOMEBREW:\n\t\tcase IR_IGOR:\n\t\t\thardware[type].set_send_carrier = false;\n\t\t\thardware[type].set_duty_cycle = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* make sure sense is either -1, 0, or 1 */\n\tif (sense != -1)\n\t\tsense = !!sense;\n\n\tresult = serial_ir_init();\n\tif (!result)\n\t\treturn 0;\n\n\tserial_ir_exit();\n\treturn result;\n}",
        "code_after_change": "static int __init serial_ir_init_module(void)\n{\n\tswitch (type) {\n\tcase IR_HOMEBREW:\n\tcase IR_IRDEO:\n\tcase IR_IRDEO_REMOTE:\n\tcase IR_ANIMAX:\n\tcase IR_IGOR:\n\t\t/* if nothing specified, use ttyS0/com1 and irq 4 */\n\t\tio = io ? io : 0x3f8;\n\t\tirq = irq ? irq : 4;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tif (!softcarrier) {\n\t\tswitch (type) {\n\t\tcase IR_HOMEBREW:\n\t\tcase IR_IGOR:\n\t\t\thardware[type].set_send_carrier = false;\n\t\t\thardware[type].set_duty_cycle = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* make sure sense is either -1, 0, or 1 */\n\tif (sense != -1)\n\t\tsense = !!sense;\n\n\treturn serial_ir_init();\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,5 @@\n static int __init serial_ir_init_module(void)\n {\n-\tint result;\n-\n \tswitch (type) {\n \tcase IR_HOMEBREW:\n \tcase IR_IRDEO:\n@@ -29,10 +27,5 @@\n \tif (sense != -1)\n \t\tsense = !!sense;\n \n-\tresult = serial_ir_init();\n-\tif (!result)\n-\t\treturn 0;\n-\n-\tserial_ir_exit();\n-\treturn result;\n+\treturn serial_ir_init();\n }",
        "function_modified_lines": {
            "added": [
                "\treturn serial_ir_init();"
            ],
            "deleted": [
                "\tint result;",
                "",
                "\tresult = serial_ir_init();",
                "\tif (!result)",
                "\t\treturn 0;",
                "",
                "\tserial_ir_exit();",
                "\treturn result;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.1.6, there is a use-after-free in serial_ir_init_module() in drivers/media/rc/serial_ir.c.",
        "id": 2221
    },
    {
        "cve_id": "CVE-2021-4028",
        "code_before_change": "int rdma_listen(struct rdma_cm_id *id, int backlog)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\tint ret;\n\n\tif (!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND, RDMA_CM_LISTEN)) {\n\t\t/* For a well behaved ULP state will be RDMA_CM_IDLE */\n\t\tid->route.addr.src_addr.ss_family = AF_INET;\n\t\tret = rdma_bind_addr(id, cma_src_addr(id_priv));\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tif (WARN_ON(!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND,\n\t\t\t\t\t   RDMA_CM_LISTEN)))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Once the ID reaches RDMA_CM_LISTEN it is not allowed to be reusable\n\t * any more, and has to be unique in the bind list.\n\t */\n\tif (id_priv->reuseaddr) {\n\t\tmutex_lock(&lock);\n\t\tret = cma_check_port(id_priv->bind_list, id_priv, 0);\n\t\tif (!ret)\n\t\t\tid_priv->reuseaddr = 0;\n\t\tmutex_unlock(&lock);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\tid_priv->backlog = backlog;\n\tif (id_priv->cma_dev) {\n\t\tif (rdma_cap_ib_cm(id->device, 1)) {\n\t\t\tret = cma_ib_listen(id_priv);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t} else if (rdma_cap_iw_cm(id->device, 1)) {\n\t\t\tret = cma_iw_listen(id_priv, backlog);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t} else {\n\t\t\tret = -ENOSYS;\n\t\t\tgoto err;\n\t\t}\n\t} else {\n\t\tret = cma_listen_on_all(id_priv);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tid_priv->backlog = 0;\n\t/*\n\t * All the failure paths that lead here will not allow the req_handler's\n\t * to have run.\n\t */\n\tcma_comp_exch(id_priv, RDMA_CM_LISTEN, RDMA_CM_ADDR_BOUND);\n\treturn ret;\n}",
        "code_after_change": "int rdma_listen(struct rdma_cm_id *id, int backlog)\n{\n\tstruct rdma_id_private *id_priv =\n\t\tcontainer_of(id, struct rdma_id_private, id);\n\tint ret;\n\n\tif (!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND, RDMA_CM_LISTEN)) {\n\t\tstruct sockaddr_in any_in = {\n\t\t\t.sin_family = AF_INET,\n\t\t\t.sin_addr.s_addr = htonl(INADDR_ANY),\n\t\t};\n\n\t\t/* For a well behaved ULP state will be RDMA_CM_IDLE */\n\t\tret = rdma_bind_addr(id, (struct sockaddr *)&any_in);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tif (WARN_ON(!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND,\n\t\t\t\t\t   RDMA_CM_LISTEN)))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Once the ID reaches RDMA_CM_LISTEN it is not allowed to be reusable\n\t * any more, and has to be unique in the bind list.\n\t */\n\tif (id_priv->reuseaddr) {\n\t\tmutex_lock(&lock);\n\t\tret = cma_check_port(id_priv->bind_list, id_priv, 0);\n\t\tif (!ret)\n\t\t\tid_priv->reuseaddr = 0;\n\t\tmutex_unlock(&lock);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\tid_priv->backlog = backlog;\n\tif (id_priv->cma_dev) {\n\t\tif (rdma_cap_ib_cm(id->device, 1)) {\n\t\t\tret = cma_ib_listen(id_priv);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t} else if (rdma_cap_iw_cm(id->device, 1)) {\n\t\t\tret = cma_iw_listen(id_priv, backlog);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t} else {\n\t\t\tret = -ENOSYS;\n\t\t\tgoto err;\n\t\t}\n\t} else {\n\t\tret = cma_listen_on_all(id_priv);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tid_priv->backlog = 0;\n\t/*\n\t * All the failure paths that lead here will not allow the req_handler's\n\t * to have run.\n\t */\n\tcma_comp_exch(id_priv, RDMA_CM_LISTEN, RDMA_CM_ADDR_BOUND);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,9 +5,13 @@\n \tint ret;\n \n \tif (!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND, RDMA_CM_LISTEN)) {\n+\t\tstruct sockaddr_in any_in = {\n+\t\t\t.sin_family = AF_INET,\n+\t\t\t.sin_addr.s_addr = htonl(INADDR_ANY),\n+\t\t};\n+\n \t\t/* For a well behaved ULP state will be RDMA_CM_IDLE */\n-\t\tid->route.addr.src_addr.ss_family = AF_INET;\n-\t\tret = rdma_bind_addr(id, cma_src_addr(id_priv));\n+\t\tret = rdma_bind_addr(id, (struct sockaddr *)&any_in);\n \t\tif (ret)\n \t\t\treturn ret;\n \t\tif (WARN_ON(!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND,",
        "function_modified_lines": {
            "added": [
                "\t\tstruct sockaddr_in any_in = {",
                "\t\t\t.sin_family = AF_INET,",
                "\t\t\t.sin_addr.s_addr = htonl(INADDR_ANY),",
                "\t\t};",
                "",
                "\t\tret = rdma_bind_addr(id, (struct sockaddr *)&any_in);"
            ],
            "deleted": [
                "\t\tid->route.addr.src_addr.ss_family = AF_INET;",
                "\t\tret = rdma_bind_addr(id, cma_src_addr(id_priv));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw in the Linux kernel's implementation of RDMA communications manager listener code allowed an attacker with local access to setup a socket to listen on a high port allowing for a list element to be used after free. Given the ability to execute code, a local attacker could leverage this use-after-free to crash the system or possibly escalate privileges on the system.",
        "id": 3123
    },
    {
        "cve_id": "CVE-2021-3347",
        "code_before_change": "static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,\n\t\t\t\t u32 val, ktime_t *abs_time, u32 bitset,\n\t\t\t\t u32 __user *uaddr2)\n{\n\tstruct hrtimer_sleeper timeout, *to;\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key2 = FUTEX_KEY_INIT;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\n\tif (uaddr == uaddr2)\n\t\treturn -EINVAL;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tto = futex_setup_timer(abs_time, &timeout, flags,\n\t\t\t       current->timer_slack_ns);\n\n\t/*\n\t * The waiter is allocated on our stack, manipulated by the requeue\n\t * code while we sleep on uaddr.\n\t */\n\trt_mutex_init_waiter(&rt_waiter);\n\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, FUTEX_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\tq.bitset = bitset;\n\tq.rt_waiter = &rt_waiter;\n\tq.requeue_pi_key = &key2;\n\n\t/*\n\t * Prepare to wait on uaddr. On success, increments q.key (key1) ref\n\t * count.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/*\n\t * The check above which compares uaddrs is not sufficient for\n\t * shared futexes. We need to compare the keys:\n\t */\n\tif (match_futex(&q.key, &key2)) {\n\t\tqueue_unlock(hb);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Queue the futex_q, drop the hb lock, wait for wakeup. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\tspin_lock(&hb->lock);\n\tret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);\n\tspin_unlock(&hb->lock);\n\tif (ret)\n\t\tgoto out;\n\n\t/*\n\t * In order for us to be here, we know our q.key == key2, and since\n\t * we took the hb->lock above, we also know that futex_requeue() has\n\t * completed and we no longer have to concern ourselves with a wakeup\n\t * race with the atomic proxy lock acquisition by the requeue code. The\n\t * futex_requeue dropped our key1 reference and incremented our key2\n\t * reference count.\n\t */\n\n\t/* Check if the requeue code acquired the second futex for us. */\n\tif (!q.rt_waiter) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case.\n\t\t */\n\t\tif (q.pi_state && (q.pi_state->owner != current)) {\n\t\t\tspin_lock(q.lock_ptr);\n\t\t\tret = fixup_pi_state_owner(uaddr2, &q, current);\n\t\t\tif (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {\n\t\t\t\tpi_state = q.pi_state;\n\t\t\t\tget_pi_state(pi_state);\n\t\t\t}\n\t\t\t/*\n\t\t\t * Drop the reference to the pi state which\n\t\t\t * the requeue_pi() code acquired for us.\n\t\t\t */\n\t\t\tput_pi_state(q.pi_state);\n\t\t\tspin_unlock(q.lock_ptr);\n\t\t}\n\t} else {\n\t\tstruct rt_mutex *pi_mutex;\n\n\t\t/*\n\t\t * We have been woken up by futex_unlock_pi(), a timeout, or a\n\t\t * signal.  futex_unlock_pi() will not destroy the lock_ptr nor\n\t\t * the pi_state.\n\t\t */\n\t\tWARN_ON(!q.pi_state);\n\t\tpi_mutex = &q.pi_state->pi_mutex;\n\t\tret = rt_mutex_wait_proxy_lock(pi_mutex, to, &rt_waiter);\n\n\t\tspin_lock(q.lock_ptr);\n\t\tif (ret && !rt_mutex_cleanup_proxy_lock(pi_mutex, &rt_waiter))\n\t\t\tret = 0;\n\n\t\tdebug_rt_mutex_free_waiter(&rt_waiter);\n\t\t/*\n\t\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t\t * haven't already.\n\t\t */\n\t\tres = fixup_owner(uaddr2, &q, !ret);\n\t\t/*\n\t\t * If fixup_owner() returned an error, proprogate that.  If it\n\t\t * acquired the lock, clear -ETIMEDOUT or -EINTR.\n\t\t */\n\t\tif (res)\n\t\t\tret = (res < 0) ? res : 0;\n\n\t\t/*\n\t\t * If fixup_pi_state_owner() faulted and was unable to handle\n\t\t * the fault, unlock the rt_mutex and return the fault to\n\t\t * userspace.\n\t\t */\n\t\tif (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {\n\t\t\tpi_state = q.pi_state;\n\t\t\tget_pi_state(pi_state);\n\t\t}\n\n\t\t/* Unqueue and drop the lock. */\n\t\tunqueue_me_pi(&q);\n\t}\n\n\tif (pi_state) {\n\t\trt_mutex_futex_unlock(&pi_state->pi_mutex);\n\t\tput_pi_state(pi_state);\n\t}\n\n\tif (ret == -EINTR) {\n\t\t/*\n\t\t * We've already been requeued, but cannot restart by calling\n\t\t * futex_lock_pi() directly. We could restart this syscall, but\n\t\t * it would detect that the user space \"val\" changed and return\n\t\t * -EWOULDBLOCK.  Save the overhead of the restart and return\n\t\t * -EWOULDBLOCK directly.\n\t\t */\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}",
        "code_after_change": "static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,\n\t\t\t\t u32 val, ktime_t *abs_time, u32 bitset,\n\t\t\t\t u32 __user *uaddr2)\n{\n\tstruct hrtimer_sleeper timeout, *to;\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key2 = FUTEX_KEY_INIT;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\n\tif (uaddr == uaddr2)\n\t\treturn -EINVAL;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tto = futex_setup_timer(abs_time, &timeout, flags,\n\t\t\t       current->timer_slack_ns);\n\n\t/*\n\t * The waiter is allocated on our stack, manipulated by the requeue\n\t * code while we sleep on uaddr.\n\t */\n\trt_mutex_init_waiter(&rt_waiter);\n\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, FUTEX_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\tq.bitset = bitset;\n\tq.rt_waiter = &rt_waiter;\n\tq.requeue_pi_key = &key2;\n\n\t/*\n\t * Prepare to wait on uaddr. On success, increments q.key (key1) ref\n\t * count.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/*\n\t * The check above which compares uaddrs is not sufficient for\n\t * shared futexes. We need to compare the keys:\n\t */\n\tif (match_futex(&q.key, &key2)) {\n\t\tqueue_unlock(hb);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Queue the futex_q, drop the hb lock, wait for wakeup. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\tspin_lock(&hb->lock);\n\tret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);\n\tspin_unlock(&hb->lock);\n\tif (ret)\n\t\tgoto out;\n\n\t/*\n\t * In order for us to be here, we know our q.key == key2, and since\n\t * we took the hb->lock above, we also know that futex_requeue() has\n\t * completed and we no longer have to concern ourselves with a wakeup\n\t * race with the atomic proxy lock acquisition by the requeue code. The\n\t * futex_requeue dropped our key1 reference and incremented our key2\n\t * reference count.\n\t */\n\n\t/* Check if the requeue code acquired the second futex for us. */\n\tif (!q.rt_waiter) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case.\n\t\t */\n\t\tif (q.pi_state && (q.pi_state->owner != current)) {\n\t\t\tspin_lock(q.lock_ptr);\n\t\t\tret = fixup_pi_state_owner(uaddr2, &q, current);\n\t\t\tif (ret < 0 && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {\n\t\t\t\tpi_state = q.pi_state;\n\t\t\t\tget_pi_state(pi_state);\n\t\t\t}\n\t\t\t/*\n\t\t\t * Drop the reference to the pi state which\n\t\t\t * the requeue_pi() code acquired for us.\n\t\t\t */\n\t\t\tput_pi_state(q.pi_state);\n\t\t\tspin_unlock(q.lock_ptr);\n\t\t\t/*\n\t\t\t * Adjust the return value. It's either -EFAULT or\n\t\t\t * success (1) but the caller expects 0 for success.\n\t\t\t */\n\t\t\tret = ret < 0 ? ret : 0;\n\t\t}\n\t} else {\n\t\tstruct rt_mutex *pi_mutex;\n\n\t\t/*\n\t\t * We have been woken up by futex_unlock_pi(), a timeout, or a\n\t\t * signal.  futex_unlock_pi() will not destroy the lock_ptr nor\n\t\t * the pi_state.\n\t\t */\n\t\tWARN_ON(!q.pi_state);\n\t\tpi_mutex = &q.pi_state->pi_mutex;\n\t\tret = rt_mutex_wait_proxy_lock(pi_mutex, to, &rt_waiter);\n\n\t\tspin_lock(q.lock_ptr);\n\t\tif (ret && !rt_mutex_cleanup_proxy_lock(pi_mutex, &rt_waiter))\n\t\t\tret = 0;\n\n\t\tdebug_rt_mutex_free_waiter(&rt_waiter);\n\t\t/*\n\t\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t\t * haven't already.\n\t\t */\n\t\tres = fixup_owner(uaddr2, &q, !ret);\n\t\t/*\n\t\t * If fixup_owner() returned an error, proprogate that.  If it\n\t\t * acquired the lock, clear -ETIMEDOUT or -EINTR.\n\t\t */\n\t\tif (res)\n\t\t\tret = (res < 0) ? res : 0;\n\n\t\t/*\n\t\t * If fixup_pi_state_owner() faulted and was unable to handle\n\t\t * the fault, unlock the rt_mutex and return the fault to\n\t\t * userspace.\n\t\t */\n\t\tif (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {\n\t\t\tpi_state = q.pi_state;\n\t\t\tget_pi_state(pi_state);\n\t\t}\n\n\t\t/* Unqueue and drop the lock. */\n\t\tunqueue_me_pi(&q);\n\t}\n\n\tif (pi_state) {\n\t\trt_mutex_futex_unlock(&pi_state->pi_mutex);\n\t\tput_pi_state(pi_state);\n\t}\n\n\tif (ret == -EINTR) {\n\t\t/*\n\t\t * We've already been requeued, but cannot restart by calling\n\t\t * futex_lock_pi() directly. We could restart this syscall, but\n\t\t * it would detect that the user space \"val\" changed and return\n\t\t * -EWOULDBLOCK.  Save the overhead of the restart and return\n\t\t * -EWOULDBLOCK directly.\n\t\t */\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -81,7 +81,7 @@\n \t\tif (q.pi_state && (q.pi_state->owner != current)) {\n \t\t\tspin_lock(q.lock_ptr);\n \t\t\tret = fixup_pi_state_owner(uaddr2, &q, current);\n-\t\t\tif (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {\n+\t\t\tif (ret < 0 && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {\n \t\t\t\tpi_state = q.pi_state;\n \t\t\t\tget_pi_state(pi_state);\n \t\t\t}\n@@ -91,6 +91,11 @@\n \t\t\t */\n \t\t\tput_pi_state(q.pi_state);\n \t\t\tspin_unlock(q.lock_ptr);\n+\t\t\t/*\n+\t\t\t * Adjust the return value. It's either -EFAULT or\n+\t\t\t * success (1) but the caller expects 0 for success.\n+\t\t\t */\n+\t\t\tret = ret < 0 ? ret : 0;\n \t\t}\n \t} else {\n \t\tstruct rt_mutex *pi_mutex;",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (ret < 0 && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {",
                "\t\t\t/*",
                "\t\t\t * Adjust the return value. It's either -EFAULT or",
                "\t\t\t * success (1) but the caller expects 0 for success.",
                "\t\t\t */",
                "\t\t\tret = ret < 0 ? ret : 0;"
            ],
            "deleted": [
                "\t\t\tif (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.10.11. PI futexes have a kernel stack use-after-free during fault handling, allowing local users to execute code in the kernel, aka CID-34b1a1ce1458.",
        "id": 2979
    },
    {
        "cve_id": "CVE-2022-42703",
        "code_before_change": "int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)\n{\n\tstruct anon_vma_chain *avc;\n\tstruct anon_vma *anon_vma;\n\tint error;\n\n\t/* Don't bother if the parent process has no anon_vma here. */\n\tif (!pvma->anon_vma)\n\t\treturn 0;\n\n\t/* Drop inherited anon_vma, we'll reuse existing or allocate new. */\n\tvma->anon_vma = NULL;\n\n\t/*\n\t * First, attach the new VMA to the parent VMA's anon_vmas,\n\t * so rmap can find non-COWed pages in child processes.\n\t */\n\terror = anon_vma_clone(vma, pvma);\n\tif (error)\n\t\treturn error;\n\n\t/* An existing anon_vma has been reused, all done then. */\n\tif (vma->anon_vma)\n\t\treturn 0;\n\n\t/* Then add our own anon_vma. */\n\tanon_vma = anon_vma_alloc();\n\tif (!anon_vma)\n\t\tgoto out_error;\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_error_free_anon_vma;\n\n\t/*\n\t * The root anon_vma's rwsem is the lock actually used when we\n\t * lock any of the anon_vmas in this anon_vma tree.\n\t */\n\tanon_vma->root = pvma->anon_vma->root;\n\tanon_vma->parent = pvma->anon_vma;\n\t/*\n\t * With refcounts, an anon_vma can stay around longer than the\n\t * process it belongs to. The root anon_vma needs to be pinned until\n\t * this anon_vma is freed, because the lock lives in the root.\n\t */\n\tget_anon_vma(anon_vma->root);\n\t/* Mark this anon_vma as the one where our new (COWed) pages go. */\n\tvma->anon_vma = anon_vma;\n\tanon_vma_lock_write(anon_vma);\n\tanon_vma_chain_link(vma, avc, anon_vma);\n\tanon_vma->parent->degree++;\n\tanon_vma_unlock_write(anon_vma);\n\n\treturn 0;\n\n out_error_free_anon_vma:\n\tput_anon_vma(anon_vma);\n out_error:\n\tunlink_anon_vmas(vma);\n\treturn -ENOMEM;\n}",
        "code_after_change": "int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)\n{\n\tstruct anon_vma_chain *avc;\n\tstruct anon_vma *anon_vma;\n\tint error;\n\n\t/* Don't bother if the parent process has no anon_vma here. */\n\tif (!pvma->anon_vma)\n\t\treturn 0;\n\n\t/* Drop inherited anon_vma, we'll reuse existing or allocate new. */\n\tvma->anon_vma = NULL;\n\n\t/*\n\t * First, attach the new VMA to the parent VMA's anon_vmas,\n\t * so rmap can find non-COWed pages in child processes.\n\t */\n\terror = anon_vma_clone(vma, pvma);\n\tif (error)\n\t\treturn error;\n\n\t/* An existing anon_vma has been reused, all done then. */\n\tif (vma->anon_vma)\n\t\treturn 0;\n\n\t/* Then add our own anon_vma. */\n\tanon_vma = anon_vma_alloc();\n\tif (!anon_vma)\n\t\tgoto out_error;\n\tanon_vma->num_active_vmas++;\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_error_free_anon_vma;\n\n\t/*\n\t * The root anon_vma's rwsem is the lock actually used when we\n\t * lock any of the anon_vmas in this anon_vma tree.\n\t */\n\tanon_vma->root = pvma->anon_vma->root;\n\tanon_vma->parent = pvma->anon_vma;\n\t/*\n\t * With refcounts, an anon_vma can stay around longer than the\n\t * process it belongs to. The root anon_vma needs to be pinned until\n\t * this anon_vma is freed, because the lock lives in the root.\n\t */\n\tget_anon_vma(anon_vma->root);\n\t/* Mark this anon_vma as the one where our new (COWed) pages go. */\n\tvma->anon_vma = anon_vma;\n\tanon_vma_lock_write(anon_vma);\n\tanon_vma_chain_link(vma, avc, anon_vma);\n\tanon_vma->parent->num_children++;\n\tanon_vma_unlock_write(anon_vma);\n\n\treturn 0;\n\n out_error_free_anon_vma:\n\tput_anon_vma(anon_vma);\n out_error:\n\tunlink_anon_vmas(vma);\n\treturn -ENOMEM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,6 +27,7 @@\n \tanon_vma = anon_vma_alloc();\n \tif (!anon_vma)\n \t\tgoto out_error;\n+\tanon_vma->num_active_vmas++;\n \tavc = anon_vma_chain_alloc(GFP_KERNEL);\n \tif (!avc)\n \t\tgoto out_error_free_anon_vma;\n@@ -47,7 +48,7 @@\n \tvma->anon_vma = anon_vma;\n \tanon_vma_lock_write(anon_vma);\n \tanon_vma_chain_link(vma, avc, anon_vma);\n-\tanon_vma->parent->degree++;\n+\tanon_vma->parent->num_children++;\n \tanon_vma_unlock_write(anon_vma);\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tanon_vma->num_active_vmas++;",
                "\tanon_vma->parent->num_children++;"
            ],
            "deleted": [
                "\tanon_vma->parent->degree++;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "mm/rmap.c in the Linux kernel before 5.19.7 has a use-after-free related to leaf anon_vma double reuse.",
        "id": 3728
    },
    {
        "cve_id": "CVE-2022-45888",
        "code_before_change": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tkref_put(&xdev->kref, cleanup_dev);\n}",
        "code_after_change": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tmutex_lock(&kref_mutex);\n\tkref_put(&xdev->kref, cleanup_dev);\n\tmutex_unlock(&kref_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -61,5 +61,7 @@\n \n \txdev->dev = NULL;\n \n+\tmutex_lock(&kref_mutex);\n \tkref_put(&xdev->kref, cleanup_dev);\n+\tmutex_unlock(&kref_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&kref_mutex);",
                "\tmutex_unlock(&kref_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.0.9. drivers/char/xillybus/xillyusb.c has a race condition and use-after-free during physical removal of a USB device.",
        "id": 3754
    },
    {
        "cve_id": "CVE-2020-8648",
        "code_before_change": "int set_selection_kernel(struct tiocl_selection *v, struct tty_struct *tty)\n{\n\tstruct vc_data *vc = vc_cons[fg_console].d;\n\tint new_sel_start, new_sel_end, spc;\n\tchar *bp, *obp;\n\tint i, ps, pe, multiplier;\n\tu32 c;\n\tint mode;\n\n\tpoke_blanked_console();\n\n\tv->xs = min_t(u16, v->xs - 1, vc->vc_cols - 1);\n\tv->ys = min_t(u16, v->ys - 1, vc->vc_rows - 1);\n\tv->xe = min_t(u16, v->xe - 1, vc->vc_cols - 1);\n\tv->ye = min_t(u16, v->ye - 1, vc->vc_rows - 1);\n\tps = v->ys * vc->vc_size_row + (v->xs << 1);\n\tpe = v->ye * vc->vc_size_row + (v->xe << 1);\n\n\tif (v->sel_mode == TIOCL_SELCLEAR) {\n\t\t/* useful for screendump without selection highlights */\n\t\tclear_selection();\n\t\treturn 0;\n\t}\n\n\tif (mouse_reporting() && (v->sel_mode & TIOCL_SELMOUSEREPORT)) {\n\t\tmouse_report(tty, v->sel_mode & TIOCL_SELBUTTONMASK, v->xs,\n\t\t\t     v->ys);\n\t\treturn 0;\n\t}\n\n\tif (ps > pe)\t/* make sel_start <= sel_end */\n\t\tswap(ps, pe);\n\n\tif (sel_cons != vc_cons[fg_console].d) {\n\t\tclear_selection();\n\t\tsel_cons = vc_cons[fg_console].d;\n\t}\n\tmode = vt_do_kdgkbmode(fg_console);\n\tif (mode == K_UNICODE)\n\t\tuse_unicode = 1;\n\telse\n\t\tuse_unicode = 0;\n\n\tswitch (v->sel_mode)\n\t{\n\t\tcase TIOCL_SELCHAR:\t/* character-by-character selection */\n\t\t\tnew_sel_start = ps;\n\t\t\tnew_sel_end = pe;\n\t\t\tbreak;\n\t\tcase TIOCL_SELWORD:\t/* word-by-word selection */\n\t\t\tspc = isspace(sel_pos(ps));\n\t\t\tfor (new_sel_start = ps; ; ps -= 2)\n\t\t\t{\n\t\t\t\tif ((spc && !isspace(sel_pos(ps))) ||\n\t\t\t\t    (!spc && !inword(sel_pos(ps))))\n\t\t\t\t\tbreak;\n\t\t\t\tnew_sel_start = ps;\n\t\t\t\tif (!(ps % vc->vc_size_row))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tspc = isspace(sel_pos(pe));\n\t\t\tfor (new_sel_end = pe; ; pe += 2)\n\t\t\t{\n\t\t\t\tif ((spc && !isspace(sel_pos(pe))) ||\n\t\t\t\t    (!spc && !inword(sel_pos(pe))))\n\t\t\t\t\tbreak;\n\t\t\t\tnew_sel_end = pe;\n\t\t\t\tif (!((pe + 2) % vc->vc_size_row))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase TIOCL_SELLINE:\t/* line-by-line selection */\n\t\t\tnew_sel_start = ps - ps % vc->vc_size_row;\n\t\t\tnew_sel_end = pe + vc->vc_size_row\n\t\t\t\t    - pe % vc->vc_size_row - 2;\n\t\t\tbreak;\n\t\tcase TIOCL_SELPOINTER:\n\t\t\thighlight_pointer(pe);\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* remove the pointer */\n\thighlight_pointer(-1);\n\n\t/* select to end of line if on trailing space */\n\tif (new_sel_end > new_sel_start &&\n\t\t!atedge(new_sel_end, vc->vc_size_row) &&\n\t\tisspace(sel_pos(new_sel_end))) {\n\t\tfor (pe = new_sel_end + 2; ; pe += 2)\n\t\t\tif (!isspace(sel_pos(pe)) ||\n\t\t\t    atedge(pe, vc->vc_size_row))\n\t\t\t\tbreak;\n\t\tif (isspace(sel_pos(pe)))\n\t\t\tnew_sel_end = pe;\n\t}\n\tif (sel_start == -1)\t/* no current selection */\n\t\thighlight(new_sel_start, new_sel_end);\n\telse if (new_sel_start == sel_start)\n\t{\n\t\tif (new_sel_end == sel_end)\t/* no action required */\n\t\t\treturn 0;\n\t\telse if (new_sel_end > sel_end)\t/* extend to right */\n\t\t\thighlight(sel_end + 2, new_sel_end);\n\t\telse\t\t\t\t/* contract from right */\n\t\t\thighlight(new_sel_end + 2, sel_end);\n\t}\n\telse if (new_sel_end == sel_end)\n\t{\n\t\tif (new_sel_start < sel_start)\t/* extend to left */\n\t\t\thighlight(new_sel_start, sel_start - 2);\n\t\telse\t\t\t\t/* contract from left */\n\t\t\thighlight(sel_start, new_sel_start - 2);\n\t}\n\telse\t/* some other case; start selection from scratch */\n\t{\n\t\tclear_selection();\n\t\thighlight(new_sel_start, new_sel_end);\n\t}\n\tsel_start = new_sel_start;\n\tsel_end = new_sel_end;\n\n\t/* Allocate a new buffer before freeing the old one ... */\n\tmultiplier = use_unicode ? 4 : 1;  /* chars can take up to 4 bytes */\n\tbp = kmalloc_array((sel_end - sel_start) / 2 + 1, multiplier,\n\t\t\t   GFP_KERNEL);\n\tif (!bp) {\n\t\tprintk(KERN_WARNING \"selection: kmalloc() failed\\n\");\n\t\tclear_selection();\n\t\treturn -ENOMEM;\n\t}\n\tkfree(sel_buffer);\n\tsel_buffer = bp;\n\n\tobp = bp;\n\tfor (i = sel_start; i <= sel_end; i += 2) {\n\t\tc = sel_pos(i);\n\t\tif (use_unicode)\n\t\t\tbp += store_utf8(c, bp);\n\t\telse\n\t\t\t*bp++ = c;\n\t\tif (!isspace(c))\n\t\t\tobp = bp;\n\t\tif (! ((i + 2) % vc->vc_size_row)) {\n\t\t\t/* strip trailing blanks from line and add newline,\n\t\t\t   unless non-space at end of line. */\n\t\t\tif (obp != bp) {\n\t\t\t\tbp = obp;\n\t\t\t\t*bp++ = '\\r';\n\t\t\t}\n\t\t\tobp = bp;\n\t\t}\n\t}\n\tsel_buffer_lth = bp - sel_buffer;\n\treturn 0;\n}",
        "code_after_change": "int set_selection_kernel(struct tiocl_selection *v, struct tty_struct *tty)\n{\n\tstruct vc_data *vc = vc_cons[fg_console].d;\n\tint new_sel_start, new_sel_end, spc;\n\tchar *bp, *obp;\n\tint i, ps, pe, multiplier;\n\tu32 c;\n\tint mode, ret = 0;\n\n\tpoke_blanked_console();\n\n\tv->xs = min_t(u16, v->xs - 1, vc->vc_cols - 1);\n\tv->ys = min_t(u16, v->ys - 1, vc->vc_rows - 1);\n\tv->xe = min_t(u16, v->xe - 1, vc->vc_cols - 1);\n\tv->ye = min_t(u16, v->ye - 1, vc->vc_rows - 1);\n\tps = v->ys * vc->vc_size_row + (v->xs << 1);\n\tpe = v->ye * vc->vc_size_row + (v->xe << 1);\n\n\tif (v->sel_mode == TIOCL_SELCLEAR) {\n\t\t/* useful for screendump without selection highlights */\n\t\tclear_selection();\n\t\treturn 0;\n\t}\n\n\tif (mouse_reporting() && (v->sel_mode & TIOCL_SELMOUSEREPORT)) {\n\t\tmouse_report(tty, v->sel_mode & TIOCL_SELBUTTONMASK, v->xs,\n\t\t\t     v->ys);\n\t\treturn 0;\n\t}\n\n\tif (ps > pe)\t/* make sel_start <= sel_end */\n\t\tswap(ps, pe);\n\n\tmutex_lock(&sel_lock);\n\tif (sel_cons != vc_cons[fg_console].d) {\n\t\tclear_selection();\n\t\tsel_cons = vc_cons[fg_console].d;\n\t}\n\tmode = vt_do_kdgkbmode(fg_console);\n\tif (mode == K_UNICODE)\n\t\tuse_unicode = 1;\n\telse\n\t\tuse_unicode = 0;\n\n\tswitch (v->sel_mode)\n\t{\n\t\tcase TIOCL_SELCHAR:\t/* character-by-character selection */\n\t\t\tnew_sel_start = ps;\n\t\t\tnew_sel_end = pe;\n\t\t\tbreak;\n\t\tcase TIOCL_SELWORD:\t/* word-by-word selection */\n\t\t\tspc = isspace(sel_pos(ps));\n\t\t\tfor (new_sel_start = ps; ; ps -= 2)\n\t\t\t{\n\t\t\t\tif ((spc && !isspace(sel_pos(ps))) ||\n\t\t\t\t    (!spc && !inword(sel_pos(ps))))\n\t\t\t\t\tbreak;\n\t\t\t\tnew_sel_start = ps;\n\t\t\t\tif (!(ps % vc->vc_size_row))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tspc = isspace(sel_pos(pe));\n\t\t\tfor (new_sel_end = pe; ; pe += 2)\n\t\t\t{\n\t\t\t\tif ((spc && !isspace(sel_pos(pe))) ||\n\t\t\t\t    (!spc && !inword(sel_pos(pe))))\n\t\t\t\t\tbreak;\n\t\t\t\tnew_sel_end = pe;\n\t\t\t\tif (!((pe + 2) % vc->vc_size_row))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase TIOCL_SELLINE:\t/* line-by-line selection */\n\t\t\tnew_sel_start = ps - ps % vc->vc_size_row;\n\t\t\tnew_sel_end = pe + vc->vc_size_row\n\t\t\t\t    - pe % vc->vc_size_row - 2;\n\t\t\tbreak;\n\t\tcase TIOCL_SELPOINTER:\n\t\t\thighlight_pointer(pe);\n\t\t\tgoto unlock;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto unlock;\n\t}\n\n\t/* remove the pointer */\n\thighlight_pointer(-1);\n\n\t/* select to end of line if on trailing space */\n\tif (new_sel_end > new_sel_start &&\n\t\t!atedge(new_sel_end, vc->vc_size_row) &&\n\t\tisspace(sel_pos(new_sel_end))) {\n\t\tfor (pe = new_sel_end + 2; ; pe += 2)\n\t\t\tif (!isspace(sel_pos(pe)) ||\n\t\t\t    atedge(pe, vc->vc_size_row))\n\t\t\t\tbreak;\n\t\tif (isspace(sel_pos(pe)))\n\t\t\tnew_sel_end = pe;\n\t}\n\tif (sel_start == -1)\t/* no current selection */\n\t\thighlight(new_sel_start, new_sel_end);\n\telse if (new_sel_start == sel_start)\n\t{\n\t\tif (new_sel_end == sel_end)\t/* no action required */\n\t\t\tgoto unlock;\n\t\telse if (new_sel_end > sel_end)\t/* extend to right */\n\t\t\thighlight(sel_end + 2, new_sel_end);\n\t\telse\t\t\t\t/* contract from right */\n\t\t\thighlight(new_sel_end + 2, sel_end);\n\t}\n\telse if (new_sel_end == sel_end)\n\t{\n\t\tif (new_sel_start < sel_start)\t/* extend to left */\n\t\t\thighlight(new_sel_start, sel_start - 2);\n\t\telse\t\t\t\t/* contract from left */\n\t\t\thighlight(sel_start, new_sel_start - 2);\n\t}\n\telse\t/* some other case; start selection from scratch */\n\t{\n\t\tclear_selection();\n\t\thighlight(new_sel_start, new_sel_end);\n\t}\n\tsel_start = new_sel_start;\n\tsel_end = new_sel_end;\n\n\t/* Allocate a new buffer before freeing the old one ... */\n\tmultiplier = use_unicode ? 4 : 1;  /* chars can take up to 4 bytes */\n\tbp = kmalloc_array((sel_end - sel_start) / 2 + 1, multiplier,\n\t\t\t   GFP_KERNEL);\n\tif (!bp) {\n\t\tprintk(KERN_WARNING \"selection: kmalloc() failed\\n\");\n\t\tclear_selection();\n\t\tret = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\tkfree(sel_buffer);\n\tsel_buffer = bp;\n\n\tobp = bp;\n\tfor (i = sel_start; i <= sel_end; i += 2) {\n\t\tc = sel_pos(i);\n\t\tif (use_unicode)\n\t\t\tbp += store_utf8(c, bp);\n\t\telse\n\t\t\t*bp++ = c;\n\t\tif (!isspace(c))\n\t\t\tobp = bp;\n\t\tif (! ((i + 2) % vc->vc_size_row)) {\n\t\t\t/* strip trailing blanks from line and add newline,\n\t\t\t   unless non-space at end of line. */\n\t\t\tif (obp != bp) {\n\t\t\t\tbp = obp;\n\t\t\t\t*bp++ = '\\r';\n\t\t\t}\n\t\t\tobp = bp;\n\t\t}\n\t}\n\tsel_buffer_lth = bp - sel_buffer;\nunlock:\n\tmutex_unlock(&sel_lock);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \tchar *bp, *obp;\n \tint i, ps, pe, multiplier;\n \tu32 c;\n-\tint mode;\n+\tint mode, ret = 0;\n \n \tpoke_blanked_console();\n \n@@ -31,6 +31,7 @@\n \tif (ps > pe)\t/* make sel_start <= sel_end */\n \t\tswap(ps, pe);\n \n+\tmutex_lock(&sel_lock);\n \tif (sel_cons != vc_cons[fg_console].d) {\n \t\tclear_selection();\n \t\tsel_cons = vc_cons[fg_console].d;\n@@ -76,9 +77,10 @@\n \t\t\tbreak;\n \t\tcase TIOCL_SELPOINTER:\n \t\t\thighlight_pointer(pe);\n-\t\t\treturn 0;\n+\t\t\tgoto unlock;\n \t\tdefault:\n-\t\t\treturn -EINVAL;\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto unlock;\n \t}\n \n \t/* remove the pointer */\n@@ -100,7 +102,7 @@\n \telse if (new_sel_start == sel_start)\n \t{\n \t\tif (new_sel_end == sel_end)\t/* no action required */\n-\t\t\treturn 0;\n+\t\t\tgoto unlock;\n \t\telse if (new_sel_end > sel_end)\t/* extend to right */\n \t\t\thighlight(sel_end + 2, new_sel_end);\n \t\telse\t\t\t\t/* contract from right */\n@@ -128,7 +130,8 @@\n \tif (!bp) {\n \t\tprintk(KERN_WARNING \"selection: kmalloc() failed\\n\");\n \t\tclear_selection();\n-\t\treturn -ENOMEM;\n+\t\tret = -ENOMEM;\n+\t\tgoto unlock;\n \t}\n \tkfree(sel_buffer);\n \tsel_buffer = bp;\n@@ -153,5 +156,7 @@\n \t\t}\n \t}\n \tsel_buffer_lth = bp - sel_buffer;\n-\treturn 0;\n+unlock:\n+\tmutex_unlock(&sel_lock);\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tint mode, ret = 0;",
                "\tmutex_lock(&sel_lock);",
                "\t\t\tgoto unlock;",
                "\t\t\tret = -EINVAL;",
                "\t\t\tgoto unlock;",
                "\t\t\tgoto unlock;",
                "\t\tret = -ENOMEM;",
                "\t\tgoto unlock;",
                "unlock:",
                "\tmutex_unlock(&sel_lock);",
                "\treturn ret;"
            ],
            "deleted": [
                "\tint mode;",
                "\t\t\treturn 0;",
                "\t\t\treturn -EINVAL;",
                "\t\t\treturn 0;",
                "\t\treturn -ENOMEM;",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a use-after-free vulnerability in the Linux kernel through 5.5.2 in the n_tty_receive_buf_common function in drivers/tty/n_tty.c.",
        "id": 2805
    },
    {
        "cve_id": "CVE-2016-10088",
        "code_before_change": "static ssize_t\nsg_write(struct file *filp, const char __user *buf, size_t count, loff_t * ppos)\n{\n\tint mxsize, cmd_size, k;\n\tint input_size, blocking;\n\tunsigned char opcode;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tstruct sg_header old_hdr;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_write: count=%d\\n\", (int) count));\n\tif (atomic_read(&sdp->detaching))\n\t\treturn -ENODEV;\n\tif (!((filp->f_flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device)))\n\t\treturn -ENXIO;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\tif (count < SZ_SG_HEADER)\n\t\treturn -EIO;\n\tif (__copy_from_user(&old_hdr, buf, SZ_SG_HEADER))\n\t\treturn -EFAULT;\n\tblocking = !(filp->f_flags & O_NONBLOCK);\n\tif (old_hdr.reply_len < 0)\n\t\treturn sg_new_write(sfp, filp, buf, count,\n\t\t\t\t    blocking, 0, 0, NULL);\n\tif (count < (SZ_SG_HEADER + 6))\n\t\treturn -EIO;\t/* The minimum scsi command length is 6 bytes. */\n\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\t      \"sg_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tbuf += SZ_SG_HEADER;\n\t__get_user(opcode, buf);\n\tif (sfp->next_cmd_len > 0) {\n\t\tcmd_size = sfp->next_cmd_len;\n\t\tsfp->next_cmd_len = 0;\t/* reset so only this write() effected */\n\t} else {\n\t\tcmd_size = COMMAND_SIZE(opcode);\t/* based on SCSI command group */\n\t\tif ((opcode >= 0xc0) && old_hdr.twelve_byte)\n\t\t\tcmd_size = 12;\n\t}\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\"sg_write:   scsi opcode=0x%02x, cmd_size=%d\\n\", (int) opcode, cmd_size));\n/* Determine buffer size.  */\n\tinput_size = count - cmd_size;\n\tmxsize = (input_size > old_hdr.reply_len) ? input_size : old_hdr.reply_len;\n\tmxsize -= SZ_SG_HEADER;\n\tinput_size -= SZ_SG_HEADER;\n\tif (input_size < 0) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EIO;\t/* User did not pass enough bytes for this command. */\n\t}\n\thp = &srp->header;\n\thp->interface_id = '\\0';\t/* indicator of old interface tunnelled */\n\thp->cmd_len = (unsigned char) cmd_size;\n\thp->iovec_count = 0;\n\thp->mx_sb_len = 0;\n\tif (input_size > 0)\n\t\thp->dxfer_direction = (old_hdr.reply_len > SZ_SG_HEADER) ?\n\t\t    SG_DXFER_TO_FROM_DEV : SG_DXFER_TO_DEV;\n\telse\n\t\thp->dxfer_direction = (mxsize > 0) ? SG_DXFER_FROM_DEV : SG_DXFER_NONE;\n\thp->dxfer_len = mxsize;\n\tif ((hp->dxfer_direction == SG_DXFER_TO_DEV) ||\n\t    (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV))\n\t\thp->dxferp = (char __user *)buf + cmd_size;\n\telse\n\t\thp->dxferp = NULL;\n\thp->sbp = NULL;\n\thp->timeout = old_hdr.reply_len;\t/* structure abuse ... */\n\thp->flags = input_size;\t/* structure abuse ... */\n\thp->pack_id = old_hdr.pack_id;\n\thp->usr_ptr = NULL;\n\tif (__copy_from_user(cmnd, buf, cmd_size))\n\t\treturn -EFAULT;\n\t/*\n\t * SG_DXFER_TO_FROM_DEV is functionally equivalent to SG_DXFER_FROM_DEV,\n\t * but is is possible that the app intended SG_DXFER_TO_DEV, because there\n\t * is a non-zero input_size, so emit a warning.\n\t */\n\tif (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV) {\n\t\tstatic char cmd[TASK_COMM_LEN];\n\t\tif (strcmp(current->comm, cmd)) {\n\t\t\tprintk_ratelimited(KERN_WARNING\n\t\t\t\t\t   \"sg_write: data in/out %d/%d bytes \"\n\t\t\t\t\t   \"for SCSI command 0x%x-- guessing \"\n\t\t\t\t\t   \"data in;\\n   program %s not setting \"\n\t\t\t\t\t   \"count and/or reply_len properly\\n\",\n\t\t\t\t\t   old_hdr.reply_len - (int)SZ_SG_HEADER,\n\t\t\t\t\t   input_size, (unsigned int) cmnd[0],\n\t\t\t\t\t   current->comm);\n\t\t\tstrcpy(cmd, current->comm);\n\t\t}\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, sfp->timeout, blocking);\n\treturn (k < 0) ? k : count;\n}",
        "code_after_change": "static ssize_t\nsg_write(struct file *filp, const char __user *buf, size_t count, loff_t * ppos)\n{\n\tint mxsize, cmd_size, k;\n\tint input_size, blocking;\n\tunsigned char opcode;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tstruct sg_header old_hdr;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\n\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))\n\t\treturn -EINVAL;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_write: count=%d\\n\", (int) count));\n\tif (atomic_read(&sdp->detaching))\n\t\treturn -ENODEV;\n\tif (!((filp->f_flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device)))\n\t\treturn -ENXIO;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\tif (count < SZ_SG_HEADER)\n\t\treturn -EIO;\n\tif (__copy_from_user(&old_hdr, buf, SZ_SG_HEADER))\n\t\treturn -EFAULT;\n\tblocking = !(filp->f_flags & O_NONBLOCK);\n\tif (old_hdr.reply_len < 0)\n\t\treturn sg_new_write(sfp, filp, buf, count,\n\t\t\t\t    blocking, 0, 0, NULL);\n\tif (count < (SZ_SG_HEADER + 6))\n\t\treturn -EIO;\t/* The minimum scsi command length is 6 bytes. */\n\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\t      \"sg_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tbuf += SZ_SG_HEADER;\n\t__get_user(opcode, buf);\n\tif (sfp->next_cmd_len > 0) {\n\t\tcmd_size = sfp->next_cmd_len;\n\t\tsfp->next_cmd_len = 0;\t/* reset so only this write() effected */\n\t} else {\n\t\tcmd_size = COMMAND_SIZE(opcode);\t/* based on SCSI command group */\n\t\tif ((opcode >= 0xc0) && old_hdr.twelve_byte)\n\t\t\tcmd_size = 12;\n\t}\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\"sg_write:   scsi opcode=0x%02x, cmd_size=%d\\n\", (int) opcode, cmd_size));\n/* Determine buffer size.  */\n\tinput_size = count - cmd_size;\n\tmxsize = (input_size > old_hdr.reply_len) ? input_size : old_hdr.reply_len;\n\tmxsize -= SZ_SG_HEADER;\n\tinput_size -= SZ_SG_HEADER;\n\tif (input_size < 0) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EIO;\t/* User did not pass enough bytes for this command. */\n\t}\n\thp = &srp->header;\n\thp->interface_id = '\\0';\t/* indicator of old interface tunnelled */\n\thp->cmd_len = (unsigned char) cmd_size;\n\thp->iovec_count = 0;\n\thp->mx_sb_len = 0;\n\tif (input_size > 0)\n\t\thp->dxfer_direction = (old_hdr.reply_len > SZ_SG_HEADER) ?\n\t\t    SG_DXFER_TO_FROM_DEV : SG_DXFER_TO_DEV;\n\telse\n\t\thp->dxfer_direction = (mxsize > 0) ? SG_DXFER_FROM_DEV : SG_DXFER_NONE;\n\thp->dxfer_len = mxsize;\n\tif ((hp->dxfer_direction == SG_DXFER_TO_DEV) ||\n\t    (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV))\n\t\thp->dxferp = (char __user *)buf + cmd_size;\n\telse\n\t\thp->dxferp = NULL;\n\thp->sbp = NULL;\n\thp->timeout = old_hdr.reply_len;\t/* structure abuse ... */\n\thp->flags = input_size;\t/* structure abuse ... */\n\thp->pack_id = old_hdr.pack_id;\n\thp->usr_ptr = NULL;\n\tif (__copy_from_user(cmnd, buf, cmd_size))\n\t\treturn -EFAULT;\n\t/*\n\t * SG_DXFER_TO_FROM_DEV is functionally equivalent to SG_DXFER_FROM_DEV,\n\t * but is is possible that the app intended SG_DXFER_TO_DEV, because there\n\t * is a non-zero input_size, so emit a warning.\n\t */\n\tif (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV) {\n\t\tstatic char cmd[TASK_COMM_LEN];\n\t\tif (strcmp(current->comm, cmd)) {\n\t\t\tprintk_ratelimited(KERN_WARNING\n\t\t\t\t\t   \"sg_write: data in/out %d/%d bytes \"\n\t\t\t\t\t   \"for SCSI command 0x%x-- guessing \"\n\t\t\t\t\t   \"data in;\\n   program %s not setting \"\n\t\t\t\t\t   \"count and/or reply_len properly\\n\",\n\t\t\t\t\t   old_hdr.reply_len - (int)SZ_SG_HEADER,\n\t\t\t\t\t   input_size, (unsigned int) cmnd[0],\n\t\t\t\t\t   current->comm);\n\t\t\tstrcpy(cmd, current->comm);\n\t\t}\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, sfp->timeout, blocking);\n\treturn (k < 0) ? k : count;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,9 @@\n \tstruct sg_header old_hdr;\n \tsg_io_hdr_t *hp;\n \tunsigned char cmnd[SG_MAX_CDB_SIZE];\n+\n+\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))\n+\t\treturn -EINVAL;\n \n \tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n \t\treturn -ENXIO;",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))",
                "\t\treturn -EINVAL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The sg implementation in the Linux kernel through 4.9 does not properly restrict write operations in situations where the KERNEL_DS option is set, which allows local users to read or write to arbitrary kernel memory locations or cause a denial of service (use-after-free) by leveraging access to a /dev/sg device, related to block/bsg.c and drivers/scsi/sg.c.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2016-9576.",
        "id": 894
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n{\n\tstruct io_identity *iod;\n\n\tiod = idr_remove(&ctx->personality_idr, id);\n\tif (iod) {\n\t\tput_cred(iod->creds);\n\t\tif (refcount_dec_and_test(&iod->count))\n\t\t\tkfree(iod);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}",
        "code_after_change": "static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n{\n\tconst struct cred *creds;\n\n\tcreds = idr_remove(&ctx->personality_idr, id);\n\tif (creds) {\n\t\tput_cred(creds);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,10 @@\n static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n {\n-\tstruct io_identity *iod;\n+\tconst struct cred *creds;\n \n-\tiod = idr_remove(&ctx->personality_idr, id);\n-\tif (iod) {\n-\t\tput_cred(iod->creds);\n-\t\tif (refcount_dec_and_test(&iod->count))\n-\t\t\tkfree(iod);\n+\tcreds = idr_remove(&ctx->personality_idr, id);\n+\tif (creds) {\n+\t\tput_cred(creds);\n \t\treturn 0;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\tconst struct cred *creds;",
                "\tcreds = idr_remove(&ctx->personality_idr, id);",
                "\tif (creds) {",
                "\t\tput_cred(creds);"
            ],
            "deleted": [
                "\tstruct io_identity *iod;",
                "\tiod = idr_remove(&ctx->personality_idr, id);",
                "\tif (iod) {",
                "\t\tput_cred(iod->creds);",
                "\t\tif (refcount_dec_and_test(&iod->count))",
                "\t\t\tkfree(iod);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3360
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "int vmw_overlay_ioctl(struct drm_device *dev, void *data,\n\t\t      struct drm_file *file_priv)\n{\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_overlay *overlay = dev_priv->overlay_priv;\n\tstruct drm_vmw_control_stream_arg *arg =\n\t    (struct drm_vmw_control_stream_arg *)data;\n\tstruct vmw_bo *buf;\n\tstruct vmw_resource *res;\n\tint ret;\n\n\tif (!vmw_overlay_available(dev_priv))\n\t\treturn -ENOSYS;\n\n\tret = vmw_user_stream_lookup(dev_priv, tfile, &arg->stream_id, &res);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&overlay->mutex);\n\n\tif (!arg->enabled) {\n\t\tret = vmw_overlay_stop(dev_priv, arg->stream_id, false, true);\n\t\tgoto out_unlock;\n\t}\n\n\tret = vmw_user_bo_lookup(file_priv, arg->handle, &buf);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = vmw_overlay_update_stream(dev_priv, buf, arg, true);\n\n\tvmw_user_bo_unref(buf);\n\nout_unlock:\n\tmutex_unlock(&overlay->mutex);\n\tvmw_resource_unreference(&res);\n\n\treturn ret;\n}",
        "code_after_change": "int vmw_overlay_ioctl(struct drm_device *dev, void *data,\n\t\t      struct drm_file *file_priv)\n{\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_overlay *overlay = dev_priv->overlay_priv;\n\tstruct drm_vmw_control_stream_arg *arg =\n\t    (struct drm_vmw_control_stream_arg *)data;\n\tstruct vmw_bo *buf;\n\tstruct vmw_resource *res;\n\tint ret;\n\n\tif (!vmw_overlay_available(dev_priv))\n\t\treturn -ENOSYS;\n\n\tret = vmw_user_stream_lookup(dev_priv, tfile, &arg->stream_id, &res);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&overlay->mutex);\n\n\tif (!arg->enabled) {\n\t\tret = vmw_overlay_stop(dev_priv, arg->stream_id, false, true);\n\t\tgoto out_unlock;\n\t}\n\n\tret = vmw_user_bo_lookup(file_priv, arg->handle, &buf);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = vmw_overlay_update_stream(dev_priv, buf, arg, true);\n\n\tvmw_user_bo_unref(&buf);\n\nout_unlock:\n\tmutex_unlock(&overlay->mutex);\n\tvmw_resource_unreference(&res);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,7 +30,7 @@\n \n \tret = vmw_overlay_update_stream(dev_priv, buf, arg, true);\n \n-\tvmw_user_bo_unref(buf);\n+\tvmw_user_bo_unref(&buf);\n \n out_unlock:\n \tmutex_unlock(&overlay->mutex);",
        "function_modified_lines": {
            "added": [
                "\tvmw_user_bo_unref(&buf);"
            ],
            "deleted": [
                "\tvmw_user_bo_unref(buf);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4277
    },
    {
        "cve_id": "CVE-2018-25015",
        "code_before_change": "static int sctp_sendmsg(struct sock *sk, struct msghdr *msg, size_t msg_len)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_association *new_asoc = NULL, *asoc = NULL;\n\tstruct sctp_transport *transport, *chunk_tp;\n\tstruct sctp_chunk *chunk;\n\tunion sctp_addr to;\n\tstruct sockaddr *msg_name = NULL;\n\tstruct sctp_sndrcvinfo default_sinfo;\n\tstruct sctp_sndrcvinfo *sinfo;\n\tstruct sctp_initmsg *sinit;\n\tsctp_assoc_t associd = 0;\n\tstruct sctp_cmsgs cmsgs = { NULL };\n\tenum sctp_scope scope;\n\tbool fill_sinfo_ttl = false, wait_connect = false;\n\tstruct sctp_datamsg *datamsg;\n\tint msg_flags = msg->msg_flags;\n\t__u16 sinfo_flags = 0;\n\tlong timeo;\n\tint err;\n\n\terr = 0;\n\tsp = sctp_sk(sk);\n\tep = sp->ep;\n\n\tpr_debug(\"%s: sk:%p, msg:%p, msg_len:%zu ep:%p\\n\", __func__, sk,\n\t\t msg, msg_len, ep);\n\n\t/* We cannot send a message over a TCP-style listening socket. */\n\tif (sctp_style(sk, TCP) && sctp_sstate(sk, LISTENING)) {\n\t\terr = -EPIPE;\n\t\tgoto out_nounlock;\n\t}\n\n\t/* Parse out the SCTP CMSGs.  */\n\terr = sctp_msghdr_parse(msg, &cmsgs);\n\tif (err) {\n\t\tpr_debug(\"%s: msghdr parse err:%x\\n\", __func__, err);\n\t\tgoto out_nounlock;\n\t}\n\n\t/* Fetch the destination address for this packet.  This\n\t * address only selects the association--it is not necessarily\n\t * the address we will send to.\n\t * For a peeled-off socket, msg_name is ignored.\n\t */\n\tif (!sctp_style(sk, UDP_HIGH_BANDWIDTH) && msg->msg_name) {\n\t\tint msg_namelen = msg->msg_namelen;\n\n\t\terr = sctp_verify_addr(sk, (union sctp_addr *)msg->msg_name,\n\t\t\t\t       msg_namelen);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (msg_namelen > sizeof(to))\n\t\t\tmsg_namelen = sizeof(to);\n\t\tmemcpy(&to, msg->msg_name, msg_namelen);\n\t\tmsg_name = msg->msg_name;\n\t}\n\n\tsinit = cmsgs.init;\n\tif (cmsgs.sinfo != NULL) {\n\t\tmemset(&default_sinfo, 0, sizeof(default_sinfo));\n\t\tdefault_sinfo.sinfo_stream = cmsgs.sinfo->snd_sid;\n\t\tdefault_sinfo.sinfo_flags = cmsgs.sinfo->snd_flags;\n\t\tdefault_sinfo.sinfo_ppid = cmsgs.sinfo->snd_ppid;\n\t\tdefault_sinfo.sinfo_context = cmsgs.sinfo->snd_context;\n\t\tdefault_sinfo.sinfo_assoc_id = cmsgs.sinfo->snd_assoc_id;\n\n\t\tsinfo = &default_sinfo;\n\t\tfill_sinfo_ttl = true;\n\t} else {\n\t\tsinfo = cmsgs.srinfo;\n\t}\n\t/* Did the user specify SNDINFO/SNDRCVINFO? */\n\tif (sinfo) {\n\t\tsinfo_flags = sinfo->sinfo_flags;\n\t\tassocid = sinfo->sinfo_assoc_id;\n\t}\n\n\tpr_debug(\"%s: msg_len:%zu, sinfo_flags:0x%x\\n\", __func__,\n\t\t msg_len, sinfo_flags);\n\n\t/* SCTP_EOF or SCTP_ABORT cannot be set on a TCP-style socket. */\n\tif (sctp_style(sk, TCP) && (sinfo_flags & (SCTP_EOF | SCTP_ABORT))) {\n\t\terr = -EINVAL;\n\t\tgoto out_nounlock;\n\t}\n\n\t/* If SCTP_EOF is set, no data can be sent. Disallow sending zero\n\t * length messages when SCTP_EOF|SCTP_ABORT is not set.\n\t * If SCTP_ABORT is set, the message length could be non zero with\n\t * the msg_iov set to the user abort reason.\n\t */\n\tif (((sinfo_flags & SCTP_EOF) && (msg_len > 0)) ||\n\t    (!(sinfo_flags & (SCTP_EOF|SCTP_ABORT)) && (msg_len == 0))) {\n\t\terr = -EINVAL;\n\t\tgoto out_nounlock;\n\t}\n\n\t/* If SCTP_ADDR_OVER is set, there must be an address\n\t * specified in msg_name.\n\t */\n\tif ((sinfo_flags & SCTP_ADDR_OVER) && (!msg->msg_name)) {\n\t\terr = -EINVAL;\n\t\tgoto out_nounlock;\n\t}\n\n\ttransport = NULL;\n\n\tpr_debug(\"%s: about to look up association\\n\", __func__);\n\n\tlock_sock(sk);\n\n\t/* If a msg_name has been specified, assume this is to be used.  */\n\tif (msg_name) {\n\t\t/* Look for a matching association on the endpoint. */\n\t\tasoc = sctp_endpoint_lookup_assoc(ep, &to, &transport);\n\n\t\t/* If we could not find a matching association on the\n\t\t * endpoint, make sure that it is not a TCP-style\n\t\t * socket that already has an association or there is\n\t\t * no peeled-off association on another socket.\n\t\t */\n\t\tif (!asoc &&\n\t\t    ((sctp_style(sk, TCP) &&\n\t\t      (sctp_sstate(sk, ESTABLISHED) ||\n\t\t       sctp_sstate(sk, CLOSING))) ||\n\t\t     sctp_endpoint_is_peeled_off(ep, &to))) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else {\n\t\tasoc = sctp_id2assoc(sk, associd);\n\t\tif (!asoc) {\n\t\t\terr = -EPIPE;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (asoc) {\n\t\tpr_debug(\"%s: just looked up association:%p\\n\", __func__, asoc);\n\n\t\t/* We cannot send a message on a TCP-style SCTP_SS_ESTABLISHED\n\t\t * socket that has an association in CLOSED state. This can\n\t\t * happen when an accepted socket has an association that is\n\t\t * already CLOSED.\n\t\t */\n\t\tif (sctp_state(asoc, CLOSED) && sctp_style(sk, TCP)) {\n\t\t\terr = -EPIPE;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (sinfo_flags & SCTP_EOF) {\n\t\t\tpr_debug(\"%s: shutting down association:%p\\n\",\n\t\t\t\t __func__, asoc);\n\n\t\t\tsctp_primitive_SHUTDOWN(net, asoc, NULL);\n\t\t\terr = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (sinfo_flags & SCTP_ABORT) {\n\n\t\t\tchunk = sctp_make_abort_user(asoc, msg, msg_len);\n\t\t\tif (!chunk) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\n\t\t\tpr_debug(\"%s: aborting association:%p\\n\",\n\t\t\t\t __func__, asoc);\n\n\t\t\tsctp_primitive_ABORT(net, asoc, chunk);\n\t\t\terr = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\t/* Do we need to create the association?  */\n\tif (!asoc) {\n\t\tpr_debug(\"%s: there is no association yet\\n\", __func__);\n\n\t\tif (sinfo_flags & (SCTP_EOF | SCTP_ABORT)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/* Check for invalid stream against the stream counts,\n\t\t * either the default or the user specified stream counts.\n\t\t */\n\t\tif (sinfo) {\n\t\t\tif (!sinit || !sinit->sinit_num_ostreams) {\n\t\t\t\t/* Check against the defaults. */\n\t\t\t\tif (sinfo->sinfo_stream >=\n\t\t\t\t    sp->initmsg.sinit_num_ostreams) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* Check against the requested.  */\n\t\t\t\tif (sinfo->sinfo_stream >=\n\t\t\t\t    sinit->sinit_num_ostreams) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * API 3.1.2 bind() - UDP Style Syntax\n\t\t * If a bind() or sctp_bindx() is not called prior to a\n\t\t * sendmsg() call that initiates a new association, the\n\t\t * system picks an ephemeral port and will choose an address\n\t\t * set equivalent to binding with a wildcard address.\n\t\t */\n\t\tif (!ep->base.bind_addr.port) {\n\t\t\tif (sctp_autobind(sk)) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * If an unprivileged user inherits a one-to-many\n\t\t\t * style socket with open associations on a privileged\n\t\t\t * port, it MAY be permitted to accept new associations,\n\t\t\t * but it SHOULD NOT be permitted to open new\n\t\t\t * associations.\n\t\t\t */\n\t\t\tif (ep->base.bind_addr.port < inet_prot_sock(net) &&\n\t\t\t    !ns_capable(net->user_ns, CAP_NET_BIND_SERVICE)) {\n\t\t\t\terr = -EACCES;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\n\t\tscope = sctp_scope(&to);\n\t\tnew_asoc = sctp_association_new(ep, sk, scope, GFP_KERNEL);\n\t\tif (!new_asoc) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tasoc = new_asoc;\n\t\terr = sctp_assoc_set_bind_addr_from_ep(asoc, scope, GFP_KERNEL);\n\t\tif (err < 0) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\t/* If the SCTP_INIT ancillary data is specified, set all\n\t\t * the association init values accordingly.\n\t\t */\n\t\tif (sinit) {\n\t\t\tif (sinit->sinit_num_ostreams) {\n\t\t\t\t__u16 outcnt = sinit->sinit_num_ostreams;\n\n\t\t\t\tasoc->c.sinit_num_ostreams = outcnt;\n\t\t\t\t/* outcnt has been changed, so re-init stream */\n\t\t\t\terr = sctp_stream_init(&asoc->stream, outcnt, 0,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t\tif (sinit->sinit_max_instreams) {\n\t\t\t\tasoc->c.sinit_max_instreams =\n\t\t\t\t\tsinit->sinit_max_instreams;\n\t\t\t}\n\t\t\tif (sinit->sinit_max_attempts) {\n\t\t\t\tasoc->max_init_attempts\n\t\t\t\t\t= sinit->sinit_max_attempts;\n\t\t\t}\n\t\t\tif (sinit->sinit_max_init_timeo) {\n\t\t\t\tasoc->max_init_timeo =\n\t\t\t\t msecs_to_jiffies(sinit->sinit_max_init_timeo);\n\t\t\t}\n\t\t}\n\n\t\t/* Prime the peer's transport structures.  */\n\t\ttransport = sctp_assoc_add_peer(asoc, &to, GFP_KERNEL, SCTP_UNKNOWN);\n\t\tif (!transport) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\t/* ASSERT: we have a valid association at this point.  */\n\tpr_debug(\"%s: we have a valid association\\n\", __func__);\n\n\tif (!sinfo) {\n\t\t/* If the user didn't specify SNDINFO/SNDRCVINFO, make up\n\t\t * one with some defaults.\n\t\t */\n\t\tmemset(&default_sinfo, 0, sizeof(default_sinfo));\n\t\tdefault_sinfo.sinfo_stream = asoc->default_stream;\n\t\tdefault_sinfo.sinfo_flags = asoc->default_flags;\n\t\tdefault_sinfo.sinfo_ppid = asoc->default_ppid;\n\t\tdefault_sinfo.sinfo_context = asoc->default_context;\n\t\tdefault_sinfo.sinfo_timetolive = asoc->default_timetolive;\n\t\tdefault_sinfo.sinfo_assoc_id = sctp_assoc2id(asoc);\n\n\t\tsinfo = &default_sinfo;\n\t} else if (fill_sinfo_ttl) {\n\t\t/* In case SNDINFO was specified, we still need to fill\n\t\t * it with a default ttl from the assoc here.\n\t\t */\n\t\tsinfo->sinfo_timetolive = asoc->default_timetolive;\n\t}\n\n\t/* API 7.1.7, the sndbuf size per association bounds the\n\t * maximum size of data that can be sent in a single send call.\n\t */\n\tif (msg_len > sk->sk_sndbuf) {\n\t\terr = -EMSGSIZE;\n\t\tgoto out_free;\n\t}\n\n\tif (asoc->pmtu_pending)\n\t\tsctp_assoc_pending_pmtu(asoc);\n\n\t/* If fragmentation is disabled and the message length exceeds the\n\t * association fragmentation point, return EMSGSIZE.  The I-D\n\t * does not specify what this error is, but this looks like\n\t * a great fit.\n\t */\n\tif (sctp_sk(sk)->disable_fragments && (msg_len > asoc->frag_point)) {\n\t\terr = -EMSGSIZE;\n\t\tgoto out_free;\n\t}\n\n\t/* Check for invalid stream. */\n\tif (sinfo->sinfo_stream >= asoc->stream.outcnt) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\t/* Allocate sctp_stream_out_ext if not already done */\n\tif (unlikely(!asoc->stream.out[sinfo->sinfo_stream].ext)) {\n\t\terr = sctp_stream_init_ext(&asoc->stream, sinfo->sinfo_stream);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t}\n\n\tif (sctp_wspace(asoc) < msg_len)\n\t\tsctp_prsctp_prune(asoc, sinfo, msg_len - sctp_wspace(asoc));\n\n\ttimeo = sock_sndtimeo(sk, msg->msg_flags & MSG_DONTWAIT);\n\tif (!sctp_wspace(asoc)) {\n\t\t/* sk can be changed by peel off when waiting for buf. */\n\t\terr = sctp_wait_for_sndbuf(asoc, &timeo, msg_len, &sk);\n\t\tif (err) {\n\t\t\tif (err == -ESRCH) {\n\t\t\t\t/* asoc is already dead. */\n\t\t\t\tnew_asoc = NULL;\n\t\t\t\terr = -EPIPE;\n\t\t\t}\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\t/* If an address is passed with the sendto/sendmsg call, it is used\n\t * to override the primary destination address in the TCP model, or\n\t * when SCTP_ADDR_OVER flag is set in the UDP model.\n\t */\n\tif ((sctp_style(sk, TCP) && msg_name) ||\n\t    (sinfo_flags & SCTP_ADDR_OVER)) {\n\t\tchunk_tp = sctp_assoc_lookup_paddr(asoc, &to);\n\t\tif (!chunk_tp) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t} else\n\t\tchunk_tp = NULL;\n\n\t/* Auto-connect, if we aren't connected already. */\n\tif (sctp_state(asoc, CLOSED)) {\n\t\terr = sctp_primitive_ASSOCIATE(net, asoc, NULL);\n\t\tif (err < 0)\n\t\t\tgoto out_free;\n\n\t\twait_connect = true;\n\t\tpr_debug(\"%s: we associated primitively\\n\", __func__);\n\t}\n\n\t/* Break the message into multiple chunks of maximum size. */\n\tdatamsg = sctp_datamsg_from_user(asoc, sinfo, &msg->msg_iter);\n\tif (IS_ERR(datamsg)) {\n\t\terr = PTR_ERR(datamsg);\n\t\tgoto out_free;\n\t}\n\tasoc->force_delay = !!(msg->msg_flags & MSG_MORE);\n\n\t/* Now send the (possibly) fragmented message. */\n\tlist_for_each_entry(chunk, &datamsg->chunks, frag_list) {\n\t\tsctp_chunk_hold(chunk);\n\n\t\t/* Do accounting for the write space.  */\n\t\tsctp_set_owner_w(chunk);\n\n\t\tchunk->transport = chunk_tp;\n\t}\n\n\t/* Send it to the lower layers.  Note:  all chunks\n\t * must either fail or succeed.   The lower layer\n\t * works that way today.  Keep it that way or this\n\t * breaks.\n\t */\n\terr = sctp_primitive_SEND(net, asoc, datamsg);\n\t/* Did the lower layer accept the chunk? */\n\tif (err) {\n\t\tsctp_datamsg_free(datamsg);\n\t\tgoto out_free;\n\t}\n\n\tpr_debug(\"%s: we sent primitively\\n\", __func__);\n\n\tsctp_datamsg_put(datamsg);\n\terr = msg_len;\n\n\tif (unlikely(wait_connect)) {\n\t\ttimeo = sock_sndtimeo(sk, msg_flags & MSG_DONTWAIT);\n\t\tsctp_wait_for_connect(asoc, &timeo);\n\t}\n\n\t/* If we are already past ASSOCIATE, the lower\n\t * layers are responsible for association cleanup.\n\t */\n\tgoto out_unlock;\n\nout_free:\n\tif (new_asoc)\n\t\tsctp_association_free(asoc);\nout_unlock:\n\trelease_sock(sk);\n\nout_nounlock:\n\treturn sctp_error(sk, msg_flags, err);\n\n#if 0\ndo_sock_err:\n\tif (msg_len)\n\t\terr = msg_len;\n\telse\n\t\terr = sock_error(sk);\n\tgoto out;\n\ndo_interrupted:\n\tif (msg_len)\n\t\terr = msg_len;\n\tgoto out;\n#endif /* 0 */\n}",
        "code_after_change": "static int sctp_sendmsg(struct sock *sk, struct msghdr *msg, size_t msg_len)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_association *new_asoc = NULL, *asoc = NULL;\n\tstruct sctp_transport *transport, *chunk_tp;\n\tstruct sctp_chunk *chunk;\n\tunion sctp_addr to;\n\tstruct sockaddr *msg_name = NULL;\n\tstruct sctp_sndrcvinfo default_sinfo;\n\tstruct sctp_sndrcvinfo *sinfo;\n\tstruct sctp_initmsg *sinit;\n\tsctp_assoc_t associd = 0;\n\tstruct sctp_cmsgs cmsgs = { NULL };\n\tenum sctp_scope scope;\n\tbool fill_sinfo_ttl = false, wait_connect = false;\n\tstruct sctp_datamsg *datamsg;\n\tint msg_flags = msg->msg_flags;\n\t__u16 sinfo_flags = 0;\n\tlong timeo;\n\tint err;\n\n\terr = 0;\n\tsp = sctp_sk(sk);\n\tep = sp->ep;\n\n\tpr_debug(\"%s: sk:%p, msg:%p, msg_len:%zu ep:%p\\n\", __func__, sk,\n\t\t msg, msg_len, ep);\n\n\t/* We cannot send a message over a TCP-style listening socket. */\n\tif (sctp_style(sk, TCP) && sctp_sstate(sk, LISTENING)) {\n\t\terr = -EPIPE;\n\t\tgoto out_nounlock;\n\t}\n\n\t/* Parse out the SCTP CMSGs.  */\n\terr = sctp_msghdr_parse(msg, &cmsgs);\n\tif (err) {\n\t\tpr_debug(\"%s: msghdr parse err:%x\\n\", __func__, err);\n\t\tgoto out_nounlock;\n\t}\n\n\t/* Fetch the destination address for this packet.  This\n\t * address only selects the association--it is not necessarily\n\t * the address we will send to.\n\t * For a peeled-off socket, msg_name is ignored.\n\t */\n\tif (!sctp_style(sk, UDP_HIGH_BANDWIDTH) && msg->msg_name) {\n\t\tint msg_namelen = msg->msg_namelen;\n\n\t\terr = sctp_verify_addr(sk, (union sctp_addr *)msg->msg_name,\n\t\t\t\t       msg_namelen);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (msg_namelen > sizeof(to))\n\t\t\tmsg_namelen = sizeof(to);\n\t\tmemcpy(&to, msg->msg_name, msg_namelen);\n\t\tmsg_name = msg->msg_name;\n\t}\n\n\tsinit = cmsgs.init;\n\tif (cmsgs.sinfo != NULL) {\n\t\tmemset(&default_sinfo, 0, sizeof(default_sinfo));\n\t\tdefault_sinfo.sinfo_stream = cmsgs.sinfo->snd_sid;\n\t\tdefault_sinfo.sinfo_flags = cmsgs.sinfo->snd_flags;\n\t\tdefault_sinfo.sinfo_ppid = cmsgs.sinfo->snd_ppid;\n\t\tdefault_sinfo.sinfo_context = cmsgs.sinfo->snd_context;\n\t\tdefault_sinfo.sinfo_assoc_id = cmsgs.sinfo->snd_assoc_id;\n\n\t\tsinfo = &default_sinfo;\n\t\tfill_sinfo_ttl = true;\n\t} else {\n\t\tsinfo = cmsgs.srinfo;\n\t}\n\t/* Did the user specify SNDINFO/SNDRCVINFO? */\n\tif (sinfo) {\n\t\tsinfo_flags = sinfo->sinfo_flags;\n\t\tassocid = sinfo->sinfo_assoc_id;\n\t}\n\n\tpr_debug(\"%s: msg_len:%zu, sinfo_flags:0x%x\\n\", __func__,\n\t\t msg_len, sinfo_flags);\n\n\t/* SCTP_EOF or SCTP_ABORT cannot be set on a TCP-style socket. */\n\tif (sctp_style(sk, TCP) && (sinfo_flags & (SCTP_EOF | SCTP_ABORT))) {\n\t\terr = -EINVAL;\n\t\tgoto out_nounlock;\n\t}\n\n\t/* If SCTP_EOF is set, no data can be sent. Disallow sending zero\n\t * length messages when SCTP_EOF|SCTP_ABORT is not set.\n\t * If SCTP_ABORT is set, the message length could be non zero with\n\t * the msg_iov set to the user abort reason.\n\t */\n\tif (((sinfo_flags & SCTP_EOF) && (msg_len > 0)) ||\n\t    (!(sinfo_flags & (SCTP_EOF|SCTP_ABORT)) && (msg_len == 0))) {\n\t\terr = -EINVAL;\n\t\tgoto out_nounlock;\n\t}\n\n\t/* If SCTP_ADDR_OVER is set, there must be an address\n\t * specified in msg_name.\n\t */\n\tif ((sinfo_flags & SCTP_ADDR_OVER) && (!msg->msg_name)) {\n\t\terr = -EINVAL;\n\t\tgoto out_nounlock;\n\t}\n\n\ttransport = NULL;\n\n\tpr_debug(\"%s: about to look up association\\n\", __func__);\n\n\tlock_sock(sk);\n\n\t/* If a msg_name has been specified, assume this is to be used.  */\n\tif (msg_name) {\n\t\t/* Look for a matching association on the endpoint. */\n\t\tasoc = sctp_endpoint_lookup_assoc(ep, &to, &transport);\n\n\t\t/* If we could not find a matching association on the\n\t\t * endpoint, make sure that it is not a TCP-style\n\t\t * socket that already has an association or there is\n\t\t * no peeled-off association on another socket.\n\t\t */\n\t\tif (!asoc &&\n\t\t    ((sctp_style(sk, TCP) &&\n\t\t      (sctp_sstate(sk, ESTABLISHED) ||\n\t\t       sctp_sstate(sk, CLOSING))) ||\n\t\t     sctp_endpoint_is_peeled_off(ep, &to))) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else {\n\t\tasoc = sctp_id2assoc(sk, associd);\n\t\tif (!asoc) {\n\t\t\terr = -EPIPE;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (asoc) {\n\t\tpr_debug(\"%s: just looked up association:%p\\n\", __func__, asoc);\n\n\t\t/* We cannot send a message on a TCP-style SCTP_SS_ESTABLISHED\n\t\t * socket that has an association in CLOSED state. This can\n\t\t * happen when an accepted socket has an association that is\n\t\t * already CLOSED.\n\t\t */\n\t\tif (sctp_state(asoc, CLOSED) && sctp_style(sk, TCP)) {\n\t\t\terr = -EPIPE;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (sinfo_flags & SCTP_EOF) {\n\t\t\tpr_debug(\"%s: shutting down association:%p\\n\",\n\t\t\t\t __func__, asoc);\n\n\t\t\tsctp_primitive_SHUTDOWN(net, asoc, NULL);\n\t\t\terr = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (sinfo_flags & SCTP_ABORT) {\n\n\t\t\tchunk = sctp_make_abort_user(asoc, msg, msg_len);\n\t\t\tif (!chunk) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\n\t\t\tpr_debug(\"%s: aborting association:%p\\n\",\n\t\t\t\t __func__, asoc);\n\n\t\t\tsctp_primitive_ABORT(net, asoc, chunk);\n\t\t\terr = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\t/* Do we need to create the association?  */\n\tif (!asoc) {\n\t\tpr_debug(\"%s: there is no association yet\\n\", __func__);\n\n\t\tif (sinfo_flags & (SCTP_EOF | SCTP_ABORT)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/* Check for invalid stream against the stream counts,\n\t\t * either the default or the user specified stream counts.\n\t\t */\n\t\tif (sinfo) {\n\t\t\tif (!sinit || !sinit->sinit_num_ostreams) {\n\t\t\t\t/* Check against the defaults. */\n\t\t\t\tif (sinfo->sinfo_stream >=\n\t\t\t\t    sp->initmsg.sinit_num_ostreams) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* Check against the requested.  */\n\t\t\t\tif (sinfo->sinfo_stream >=\n\t\t\t\t    sinit->sinit_num_ostreams) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * API 3.1.2 bind() - UDP Style Syntax\n\t\t * If a bind() or sctp_bindx() is not called prior to a\n\t\t * sendmsg() call that initiates a new association, the\n\t\t * system picks an ephemeral port and will choose an address\n\t\t * set equivalent to binding with a wildcard address.\n\t\t */\n\t\tif (!ep->base.bind_addr.port) {\n\t\t\tif (sctp_autobind(sk)) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * If an unprivileged user inherits a one-to-many\n\t\t\t * style socket with open associations on a privileged\n\t\t\t * port, it MAY be permitted to accept new associations,\n\t\t\t * but it SHOULD NOT be permitted to open new\n\t\t\t * associations.\n\t\t\t */\n\t\t\tif (ep->base.bind_addr.port < inet_prot_sock(net) &&\n\t\t\t    !ns_capable(net->user_ns, CAP_NET_BIND_SERVICE)) {\n\t\t\t\terr = -EACCES;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\n\t\tscope = sctp_scope(&to);\n\t\tnew_asoc = sctp_association_new(ep, sk, scope, GFP_KERNEL);\n\t\tif (!new_asoc) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tasoc = new_asoc;\n\t\terr = sctp_assoc_set_bind_addr_from_ep(asoc, scope, GFP_KERNEL);\n\t\tif (err < 0) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\t/* If the SCTP_INIT ancillary data is specified, set all\n\t\t * the association init values accordingly.\n\t\t */\n\t\tif (sinit) {\n\t\t\tif (sinit->sinit_num_ostreams) {\n\t\t\t\t__u16 outcnt = sinit->sinit_num_ostreams;\n\n\t\t\t\tasoc->c.sinit_num_ostreams = outcnt;\n\t\t\t\t/* outcnt has been changed, so re-init stream */\n\t\t\t\terr = sctp_stream_init(&asoc->stream, outcnt, 0,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t\tif (sinit->sinit_max_instreams) {\n\t\t\t\tasoc->c.sinit_max_instreams =\n\t\t\t\t\tsinit->sinit_max_instreams;\n\t\t\t}\n\t\t\tif (sinit->sinit_max_attempts) {\n\t\t\t\tasoc->max_init_attempts\n\t\t\t\t\t= sinit->sinit_max_attempts;\n\t\t\t}\n\t\t\tif (sinit->sinit_max_init_timeo) {\n\t\t\t\tasoc->max_init_timeo =\n\t\t\t\t msecs_to_jiffies(sinit->sinit_max_init_timeo);\n\t\t\t}\n\t\t}\n\n\t\t/* Prime the peer's transport structures.  */\n\t\ttransport = sctp_assoc_add_peer(asoc, &to, GFP_KERNEL, SCTP_UNKNOWN);\n\t\tif (!transport) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\t/* ASSERT: we have a valid association at this point.  */\n\tpr_debug(\"%s: we have a valid association\\n\", __func__);\n\n\tif (!sinfo) {\n\t\t/* If the user didn't specify SNDINFO/SNDRCVINFO, make up\n\t\t * one with some defaults.\n\t\t */\n\t\tmemset(&default_sinfo, 0, sizeof(default_sinfo));\n\t\tdefault_sinfo.sinfo_stream = asoc->default_stream;\n\t\tdefault_sinfo.sinfo_flags = asoc->default_flags;\n\t\tdefault_sinfo.sinfo_ppid = asoc->default_ppid;\n\t\tdefault_sinfo.sinfo_context = asoc->default_context;\n\t\tdefault_sinfo.sinfo_timetolive = asoc->default_timetolive;\n\t\tdefault_sinfo.sinfo_assoc_id = sctp_assoc2id(asoc);\n\n\t\tsinfo = &default_sinfo;\n\t} else if (fill_sinfo_ttl) {\n\t\t/* In case SNDINFO was specified, we still need to fill\n\t\t * it with a default ttl from the assoc here.\n\t\t */\n\t\tsinfo->sinfo_timetolive = asoc->default_timetolive;\n\t}\n\n\t/* API 7.1.7, the sndbuf size per association bounds the\n\t * maximum size of data that can be sent in a single send call.\n\t */\n\tif (msg_len > sk->sk_sndbuf) {\n\t\terr = -EMSGSIZE;\n\t\tgoto out_free;\n\t}\n\n\tif (asoc->pmtu_pending)\n\t\tsctp_assoc_pending_pmtu(asoc);\n\n\t/* If fragmentation is disabled and the message length exceeds the\n\t * association fragmentation point, return EMSGSIZE.  The I-D\n\t * does not specify what this error is, but this looks like\n\t * a great fit.\n\t */\n\tif (sctp_sk(sk)->disable_fragments && (msg_len > asoc->frag_point)) {\n\t\terr = -EMSGSIZE;\n\t\tgoto out_free;\n\t}\n\n\t/* Check for invalid stream. */\n\tif (sinfo->sinfo_stream >= asoc->stream.outcnt) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\t/* Allocate sctp_stream_out_ext if not already done */\n\tif (unlikely(!asoc->stream.out[sinfo->sinfo_stream].ext)) {\n\t\terr = sctp_stream_init_ext(&asoc->stream, sinfo->sinfo_stream);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t}\n\n\tif (sctp_wspace(asoc) < msg_len)\n\t\tsctp_prsctp_prune(asoc, sinfo, msg_len - sctp_wspace(asoc));\n\n\ttimeo = sock_sndtimeo(sk, msg->msg_flags & MSG_DONTWAIT);\n\tif (!sctp_wspace(asoc)) {\n\t\t/* sk can be changed by peel off when waiting for buf. */\n\t\terr = sctp_wait_for_sndbuf(asoc, &timeo, msg_len);\n\t\tif (err) {\n\t\t\tif (err == -ESRCH) {\n\t\t\t\t/* asoc is already dead. */\n\t\t\t\tnew_asoc = NULL;\n\t\t\t\terr = -EPIPE;\n\t\t\t}\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\t/* If an address is passed with the sendto/sendmsg call, it is used\n\t * to override the primary destination address in the TCP model, or\n\t * when SCTP_ADDR_OVER flag is set in the UDP model.\n\t */\n\tif ((sctp_style(sk, TCP) && msg_name) ||\n\t    (sinfo_flags & SCTP_ADDR_OVER)) {\n\t\tchunk_tp = sctp_assoc_lookup_paddr(asoc, &to);\n\t\tif (!chunk_tp) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t} else\n\t\tchunk_tp = NULL;\n\n\t/* Auto-connect, if we aren't connected already. */\n\tif (sctp_state(asoc, CLOSED)) {\n\t\terr = sctp_primitive_ASSOCIATE(net, asoc, NULL);\n\t\tif (err < 0)\n\t\t\tgoto out_free;\n\n\t\twait_connect = true;\n\t\tpr_debug(\"%s: we associated primitively\\n\", __func__);\n\t}\n\n\t/* Break the message into multiple chunks of maximum size. */\n\tdatamsg = sctp_datamsg_from_user(asoc, sinfo, &msg->msg_iter);\n\tif (IS_ERR(datamsg)) {\n\t\terr = PTR_ERR(datamsg);\n\t\tgoto out_free;\n\t}\n\tasoc->force_delay = !!(msg->msg_flags & MSG_MORE);\n\n\t/* Now send the (possibly) fragmented message. */\n\tlist_for_each_entry(chunk, &datamsg->chunks, frag_list) {\n\t\tsctp_chunk_hold(chunk);\n\n\t\t/* Do accounting for the write space.  */\n\t\tsctp_set_owner_w(chunk);\n\n\t\tchunk->transport = chunk_tp;\n\t}\n\n\t/* Send it to the lower layers.  Note:  all chunks\n\t * must either fail or succeed.   The lower layer\n\t * works that way today.  Keep it that way or this\n\t * breaks.\n\t */\n\terr = sctp_primitive_SEND(net, asoc, datamsg);\n\t/* Did the lower layer accept the chunk? */\n\tif (err) {\n\t\tsctp_datamsg_free(datamsg);\n\t\tgoto out_free;\n\t}\n\n\tpr_debug(\"%s: we sent primitively\\n\", __func__);\n\n\tsctp_datamsg_put(datamsg);\n\terr = msg_len;\n\n\tif (unlikely(wait_connect)) {\n\t\ttimeo = sock_sndtimeo(sk, msg_flags & MSG_DONTWAIT);\n\t\tsctp_wait_for_connect(asoc, &timeo);\n\t}\n\n\t/* If we are already past ASSOCIATE, the lower\n\t * layers are responsible for association cleanup.\n\t */\n\tgoto out_unlock;\n\nout_free:\n\tif (new_asoc)\n\t\tsctp_association_free(asoc);\nout_unlock:\n\trelease_sock(sk);\n\nout_nounlock:\n\treturn sctp_error(sk, msg_flags, err);\n\n#if 0\ndo_sock_err:\n\tif (msg_len)\n\t\terr = msg_len;\n\telse\n\t\terr = sock_error(sk);\n\tgoto out;\n\ndo_interrupted:\n\tif (msg_len)\n\t\terr = msg_len;\n\tgoto out;\n#endif /* 0 */\n}",
        "patch": "--- code before\n+++ code after\n@@ -347,7 +347,7 @@\n \ttimeo = sock_sndtimeo(sk, msg->msg_flags & MSG_DONTWAIT);\n \tif (!sctp_wspace(asoc)) {\n \t\t/* sk can be changed by peel off when waiting for buf. */\n-\t\terr = sctp_wait_for_sndbuf(asoc, &timeo, msg_len, &sk);\n+\t\terr = sctp_wait_for_sndbuf(asoc, &timeo, msg_len);\n \t\tif (err) {\n \t\t\tif (err == -ESRCH) {\n \t\t\t\t/* asoc is already dead. */",
        "function_modified_lines": {
            "added": [
                "\t\terr = sctp_wait_for_sndbuf(asoc, &timeo, msg_len);"
            ],
            "deleted": [
                "\t\terr = sctp_wait_for_sndbuf(asoc, &timeo, msg_len, &sk);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.14.16. There is a use-after-free in net/sctp/socket.c for a held lock after a peel off, aka CID-a0ff660058b8.",
        "id": 1794
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static void blk_add_trace_sleeprq(void *ignore,\n\t\t\t\t  struct request_queue *q,\n\t\t\t\t  struct bio *bio, int rw)\n{\n\tif (bio)\n\t\tblk_add_trace_bio(q, bio, BLK_TA_SLEEPRQ, 0);\n\telse {\n\t\tstruct blk_trace *bt = q->blk_trace;\n\n\t\tif (bt)\n\t\t\t__blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_SLEEPRQ,\n\t\t\t\t\t0, 0, NULL, 0);\n\t}\n}",
        "code_after_change": "static void blk_add_trace_sleeprq(void *ignore,\n\t\t\t\t  struct request_queue *q,\n\t\t\t\t  struct bio *bio, int rw)\n{\n\tif (bio)\n\t\tblk_add_trace_bio(q, bio, BLK_TA_SLEEPRQ, 0);\n\telse {\n\t\tstruct blk_trace *bt;\n\n\t\trcu_read_lock();\n\t\tbt = rcu_dereference(q->blk_trace);\n\t\tif (bt)\n\t\t\t__blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_SLEEPRQ,\n\t\t\t\t\t0, 0, NULL, 0);\n\t\trcu_read_unlock();\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,10 +5,13 @@\n \tif (bio)\n \t\tblk_add_trace_bio(q, bio, BLK_TA_SLEEPRQ, 0);\n \telse {\n-\t\tstruct blk_trace *bt = q->blk_trace;\n+\t\tstruct blk_trace *bt;\n \n+\t\trcu_read_lock();\n+\t\tbt = rcu_dereference(q->blk_trace);\n \t\tif (bt)\n \t\t\t__blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_SLEEPRQ,\n \t\t\t\t\t0, 0, NULL, 0);\n+\t\trcu_read_unlock();\n \t}\n }",
        "function_modified_lines": {
            "added": [
                "\t\tstruct blk_trace *bt;",
                "\t\trcu_read_lock();",
                "\t\tbt = rcu_dereference(q->blk_trace);",
                "\t\trcu_read_unlock();"
            ],
            "deleted": [
                "\t\tstruct blk_trace *bt = q->blk_trace;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2241
    },
    {
        "cve_id": "CVE-2019-2213",
        "code_before_change": "static void binder_free_transaction(struct binder_transaction *t)\n{\n\tif (t->buffer)\n\t\tt->buffer->transaction = NULL;\n\tbinder_free_txn_fixups(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n}",
        "code_after_change": "static void binder_free_transaction(struct binder_transaction *t)\n{\n\tstruct binder_proc *target_proc = t->to_proc;\n\n\tif (target_proc) {\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (t->buffer)\n\t\t\tt->buffer->transaction = NULL;\n\t\tbinder_inner_proc_unlock(target_proc);\n\t}\n\t/*\n\t * If the transaction has no target_proc, then\n\t * t->buffer->transaction has already been cleared.\n\t */\n\tbinder_free_txn_fixups(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,17 @@\n static void binder_free_transaction(struct binder_transaction *t)\n {\n-\tif (t->buffer)\n-\t\tt->buffer->transaction = NULL;\n+\tstruct binder_proc *target_proc = t->to_proc;\n+\n+\tif (target_proc) {\n+\t\tbinder_inner_proc_lock(target_proc);\n+\t\tif (t->buffer)\n+\t\t\tt->buffer->transaction = NULL;\n+\t\tbinder_inner_proc_unlock(target_proc);\n+\t}\n+\t/*\n+\t * If the transaction has no target_proc, then\n+\t * t->buffer->transaction has already been cleared.\n+\t */\n \tbinder_free_txn_fixups(t);\n \tkfree(t);\n \tbinder_stats_deleted(BINDER_STAT_TRANSACTION);",
        "function_modified_lines": {
            "added": [
                "\tstruct binder_proc *target_proc = t->to_proc;",
                "",
                "\tif (target_proc) {",
                "\t\tbinder_inner_proc_lock(target_proc);",
                "\t\tif (t->buffer)",
                "\t\t\tt->buffer->transaction = NULL;",
                "\t\tbinder_inner_proc_unlock(target_proc);",
                "\t}",
                "\t/*",
                "\t * If the transaction has no target_proc, then",
                "\t * t->buffer->transaction has already been cleared.",
                "\t */"
            ],
            "deleted": [
                "\tif (t->buffer)",
                "\t\tt->buffer->transaction = NULL;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In binder_free_transaction of binder.c, there is a possible use-after-free due to a race condition. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-133758011References: Upstream kernel",
        "id": 2294
    },
    {
        "cve_id": "CVE-2017-7374",
        "code_before_change": "static void put_crypt_info(struct fscrypt_info *ci)\n{\n\tif (!ci)\n\t\treturn;\n\n\tkey_put(ci->ci_keyring_key);\n\tcrypto_free_skcipher(ci->ci_ctfm);\n\tkmem_cache_free(fscrypt_info_cachep, ci);\n}",
        "code_after_change": "static void put_crypt_info(struct fscrypt_info *ci)\n{\n\tif (!ci)\n\t\treturn;\n\n\tcrypto_free_skcipher(ci->ci_ctfm);\n\tkmem_cache_free(fscrypt_info_cachep, ci);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,6 @@\n \tif (!ci)\n \t\treturn;\n \n-\tkey_put(ci->ci_keyring_key);\n \tcrypto_free_skcipher(ci->ci_ctfm);\n \tkmem_cache_free(fscrypt_info_cachep, ci);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tkey_put(ci->ci_keyring_key);"
            ]
        },
        "cwe": [
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in fs/crypto/ in the Linux kernel before 4.10.7 allows local users to cause a denial of service (NULL pointer dereference) or possibly gain privileges by revoking keyring keys being used for ext4, f2fs, or ubifs encryption, causing cryptographic transform objects to be freed prematurely.",
        "id": 1501
    },
    {
        "cve_id": "CVE-2023-0240",
        "code_before_change": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\tif (!(req->work.flags & IO_WQ_WORK_FILES) &&\n\t    (io_op_defs[req->opcode].work_flags & IO_WQ_WORK_FILES) &&\n\t    !(req->flags & REQ_F_NO_FILE_TABLE)) {\n\t\treq->work.identity->files = get_files_struct(current);\n\t\tget_nsproxy(current->nsproxy);\n\t\treq->work.identity->nsproxy = current->nsproxy;\n\t\treq->flags |= REQ_F_INFLIGHT;\n\n\t\tspin_lock_irq(&ctx->inflight_lock);\n\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n\t\tspin_unlock_irq(&ctx->inflight_lock);\n\t\treq->work.flags |= IO_WQ_WORK_FILES;\n\t}\n\tif (!(req->work.flags & IO_WQ_WORK_MM) &&\n\t    (def->work_flags & IO_WQ_WORK_MM)) {\n\t\tmmgrab(current->mm);\n\t\treq->work.identity->mm = current->mm;\n\t\treq->work.flags |= IO_WQ_WORK_MM;\n\t}\n#ifdef CONFIG_BLK_CGROUP\n\tif (!(req->work.flags & IO_WQ_WORK_BLKCG) &&\n\t    (def->work_flags & IO_WQ_WORK_BLKCG)) {\n\t\trcu_read_lock();\n\t\treq->work.identity->blkcg_css = blkcg_css();\n\t\t/*\n\t\t * This should be rare, either the cgroup is dying or the task\n\t\t * is moving cgroups. Just punt to root for the handful of ios.\n\t\t */\n\t\tif (css_tryget_online(req->work.identity->blkcg_css))\n\t\t\treq->work.flags |= IO_WQ_WORK_BLKCG;\n\t\trcu_read_unlock();\n\t}\n#endif\n\tif (!(req->work.flags & IO_WQ_WORK_CREDS)) {\n\t\treq->work.identity->creds = get_current_cred();\n\t\treq->work.flags |= IO_WQ_WORK_CREDS;\n\t}\n\tif (!(req->work.flags & IO_WQ_WORK_FS) &&\n\t    (def->work_flags & IO_WQ_WORK_FS)) {\n\t\tspin_lock(&current->fs->lock);\n\t\tif (!current->fs->in_exec) {\n\t\t\treq->work.identity->fs = current->fs;\n\t\t\treq->work.identity->fs->users++;\n\t\t\treq->work.flags |= IO_WQ_WORK_FS;\n\t\t} else {\n\t\t\treq->work.flags |= IO_WQ_WORK_CANCEL;\n\t\t}\n\t\tspin_unlock(&current->fs->lock);\n\t}\n\tif (def->needs_fsize)\n\t\treq->work.identity->fsize = rlimit(RLIMIT_FSIZE);\n\telse\n\t\treq->work.identity->fsize = RLIM_INFINITY;\n}",
        "code_after_change": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_identity *id = &req->identity;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\n\t/* ->mm can never change on us */\n\tif (!(req->work.flags & IO_WQ_WORK_MM) &&\n\t    (def->work_flags & IO_WQ_WORK_MM)) {\n\t\tmmgrab(id->mm);\n\t\treq->work.flags |= IO_WQ_WORK_MM;\n\t}\n\n\t/* if we fail grabbing identity, we must COW, regrab, and retry */\n\tif (io_grab_identity(req))\n\t\treturn;\n\n\tif (!io_identity_cow(req))\n\t\treturn;\n\n\t/* can't fail at this point */\n\tif (!io_grab_identity(req))\n\t\tWARN_ON(1);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,7 @@\n static void io_prep_async_work(struct io_kiocb *req)\n {\n \tconst struct io_op_def *def = &io_op_defs[req->opcode];\n+\tstruct io_identity *id = &req->identity;\n \tstruct io_ring_ctx *ctx = req->ctx;\n \n \tio_req_init_async(req);\n@@ -12,57 +13,22 @@\n \t\tif (def->unbound_nonreg_file)\n \t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n \t}\n-\tif (!(req->work.flags & IO_WQ_WORK_FILES) &&\n-\t    (io_op_defs[req->opcode].work_flags & IO_WQ_WORK_FILES) &&\n-\t    !(req->flags & REQ_F_NO_FILE_TABLE)) {\n-\t\treq->work.identity->files = get_files_struct(current);\n-\t\tget_nsproxy(current->nsproxy);\n-\t\treq->work.identity->nsproxy = current->nsproxy;\n-\t\treq->flags |= REQ_F_INFLIGHT;\n \n-\t\tspin_lock_irq(&ctx->inflight_lock);\n-\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n-\t\tspin_unlock_irq(&ctx->inflight_lock);\n-\t\treq->work.flags |= IO_WQ_WORK_FILES;\n-\t}\n+\t/* ->mm can never change on us */\n \tif (!(req->work.flags & IO_WQ_WORK_MM) &&\n \t    (def->work_flags & IO_WQ_WORK_MM)) {\n-\t\tmmgrab(current->mm);\n-\t\treq->work.identity->mm = current->mm;\n+\t\tmmgrab(id->mm);\n \t\treq->work.flags |= IO_WQ_WORK_MM;\n \t}\n-#ifdef CONFIG_BLK_CGROUP\n-\tif (!(req->work.flags & IO_WQ_WORK_BLKCG) &&\n-\t    (def->work_flags & IO_WQ_WORK_BLKCG)) {\n-\t\trcu_read_lock();\n-\t\treq->work.identity->blkcg_css = blkcg_css();\n-\t\t/*\n-\t\t * This should be rare, either the cgroup is dying or the task\n-\t\t * is moving cgroups. Just punt to root for the handful of ios.\n-\t\t */\n-\t\tif (css_tryget_online(req->work.identity->blkcg_css))\n-\t\t\treq->work.flags |= IO_WQ_WORK_BLKCG;\n-\t\trcu_read_unlock();\n-\t}\n-#endif\n-\tif (!(req->work.flags & IO_WQ_WORK_CREDS)) {\n-\t\treq->work.identity->creds = get_current_cred();\n-\t\treq->work.flags |= IO_WQ_WORK_CREDS;\n-\t}\n-\tif (!(req->work.flags & IO_WQ_WORK_FS) &&\n-\t    (def->work_flags & IO_WQ_WORK_FS)) {\n-\t\tspin_lock(&current->fs->lock);\n-\t\tif (!current->fs->in_exec) {\n-\t\t\treq->work.identity->fs = current->fs;\n-\t\t\treq->work.identity->fs->users++;\n-\t\t\treq->work.flags |= IO_WQ_WORK_FS;\n-\t\t} else {\n-\t\t\treq->work.flags |= IO_WQ_WORK_CANCEL;\n-\t\t}\n-\t\tspin_unlock(&current->fs->lock);\n-\t}\n-\tif (def->needs_fsize)\n-\t\treq->work.identity->fsize = rlimit(RLIMIT_FSIZE);\n-\telse\n-\t\treq->work.identity->fsize = RLIM_INFINITY;\n+\n+\t/* if we fail grabbing identity, we must COW, regrab, and retry */\n+\tif (io_grab_identity(req))\n+\t\treturn;\n+\n+\tif (!io_identity_cow(req))\n+\t\treturn;\n+\n+\t/* can't fail at this point */\n+\tif (!io_grab_identity(req))\n+\t\tWARN_ON(1);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct io_identity *id = &req->identity;",
                "\t/* ->mm can never change on us */",
                "\t\tmmgrab(id->mm);",
                "",
                "\t/* if we fail grabbing identity, we must COW, regrab, and retry */",
                "\tif (io_grab_identity(req))",
                "\t\treturn;",
                "",
                "\tif (!io_identity_cow(req))",
                "\t\treturn;",
                "",
                "\t/* can't fail at this point */",
                "\tif (!io_grab_identity(req))",
                "\t\tWARN_ON(1);"
            ],
            "deleted": [
                "\tif (!(req->work.flags & IO_WQ_WORK_FILES) &&",
                "\t    (io_op_defs[req->opcode].work_flags & IO_WQ_WORK_FILES) &&",
                "\t    !(req->flags & REQ_F_NO_FILE_TABLE)) {",
                "\t\treq->work.identity->files = get_files_struct(current);",
                "\t\tget_nsproxy(current->nsproxy);",
                "\t\treq->work.identity->nsproxy = current->nsproxy;",
                "\t\treq->flags |= REQ_F_INFLIGHT;",
                "\t\tspin_lock_irq(&ctx->inflight_lock);",
                "\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);",
                "\t\tspin_unlock_irq(&ctx->inflight_lock);",
                "\t\treq->work.flags |= IO_WQ_WORK_FILES;",
                "\t}",
                "\t\tmmgrab(current->mm);",
                "\t\treq->work.identity->mm = current->mm;",
                "#ifdef CONFIG_BLK_CGROUP",
                "\tif (!(req->work.flags & IO_WQ_WORK_BLKCG) &&",
                "\t    (def->work_flags & IO_WQ_WORK_BLKCG)) {",
                "\t\trcu_read_lock();",
                "\t\treq->work.identity->blkcg_css = blkcg_css();",
                "\t\t/*",
                "\t\t * This should be rare, either the cgroup is dying or the task",
                "\t\t * is moving cgroups. Just punt to root for the handful of ios.",
                "\t\t */",
                "\t\tif (css_tryget_online(req->work.identity->blkcg_css))",
                "\t\t\treq->work.flags |= IO_WQ_WORK_BLKCG;",
                "\t\trcu_read_unlock();",
                "\t}",
                "#endif",
                "\tif (!(req->work.flags & IO_WQ_WORK_CREDS)) {",
                "\t\treq->work.identity->creds = get_current_cred();",
                "\t\treq->work.flags |= IO_WQ_WORK_CREDS;",
                "\t}",
                "\tif (!(req->work.flags & IO_WQ_WORK_FS) &&",
                "\t    (def->work_flags & IO_WQ_WORK_FS)) {",
                "\t\tspin_lock(&current->fs->lock);",
                "\t\tif (!current->fs->in_exec) {",
                "\t\t\treq->work.identity->fs = current->fs;",
                "\t\t\treq->work.identity->fs->users++;",
                "\t\t\treq->work.flags |= IO_WQ_WORK_FS;",
                "\t\t} else {",
                "\t\t\treq->work.flags |= IO_WQ_WORK_CANCEL;",
                "\t\t}",
                "\t\tspin_unlock(&current->fs->lock);",
                "\t}",
                "\tif (def->needs_fsize)",
                "\t\treq->work.identity->fsize = rlimit(RLIMIT_FSIZE);",
                "\telse",
                "\t\treq->work.identity->fsize = RLIM_INFINITY;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "id": 3816
    },
    {
        "cve_id": "CVE-2020-16119",
        "code_before_change": "struct sock *dccp_create_openreq_child(const struct sock *sk,\n\t\t\t\t       const struct request_sock *req,\n\t\t\t\t       const struct sk_buff *skb)\n{\n\t/*\n\t * Step 3: Process LISTEN state\n\t *\n\t *   (* Generate a new socket and switch to that socket *)\n\t *   Set S := new socket for this port pair\n\t */\n\tstruct sock *newsk = inet_csk_clone_lock(sk, req, GFP_ATOMIC);\n\n\tif (newsk != NULL) {\n\t\tstruct dccp_request_sock *dreq = dccp_rsk(req);\n\t\tstruct inet_connection_sock *newicsk = inet_csk(newsk);\n\t\tstruct dccp_sock *newdp = dccp_sk(newsk);\n\n\t\tnewdp->dccps_role\t    = DCCP_ROLE_SERVER;\n\t\tnewdp->dccps_hc_rx_ackvec   = NULL;\n\t\tnewdp->dccps_service_list   = NULL;\n\t\tnewdp->dccps_service\t    = dreq->dreq_service;\n\t\tnewdp->dccps_timestamp_echo = dreq->dreq_timestamp_echo;\n\t\tnewdp->dccps_timestamp_time = dreq->dreq_timestamp_time;\n\t\tnewicsk->icsk_rto\t    = DCCP_TIMEOUT_INIT;\n\n\t\tINIT_LIST_HEAD(&newdp->dccps_featneg);\n\t\t/*\n\t\t * Step 3: Process LISTEN state\n\t\t *\n\t\t *    Choose S.ISS (initial seqno) or set from Init Cookies\n\t\t *    Initialize S.GAR := S.ISS\n\t\t *    Set S.ISR, S.GSR from packet (or Init Cookies)\n\t\t *\n\t\t *    Setting AWL/AWH and SWL/SWH happens as part of the feature\n\t\t *    activation below, as these windows all depend on the local\n\t\t *    and remote Sequence Window feature values (7.5.2).\n\t\t */\n\t\tnewdp->dccps_iss = dreq->dreq_iss;\n\t\tnewdp->dccps_gss = dreq->dreq_gss;\n\t\tnewdp->dccps_gar = newdp->dccps_iss;\n\t\tnewdp->dccps_isr = dreq->dreq_isr;\n\t\tnewdp->dccps_gsr = dreq->dreq_gsr;\n\n\t\t/*\n\t\t * Activate features: initialise CCIDs, sequence windows etc.\n\t\t */\n\t\tif (dccp_feat_activate_values(newsk, &dreq->dreq_featneg)) {\n\t\t\tsk_free_unlock_clone(newsk);\n\t\t\treturn NULL;\n\t\t}\n\t\tdccp_init_xmit_timers(newsk);\n\n\t\t__DCCP_INC_STATS(DCCP_MIB_PASSIVEOPENS);\n\t}\n\treturn newsk;\n}",
        "code_after_change": "struct sock *dccp_create_openreq_child(const struct sock *sk,\n\t\t\t\t       const struct request_sock *req,\n\t\t\t\t       const struct sk_buff *skb)\n{\n\t/*\n\t * Step 3: Process LISTEN state\n\t *\n\t *   (* Generate a new socket and switch to that socket *)\n\t *   Set S := new socket for this port pair\n\t */\n\tstruct sock *newsk = inet_csk_clone_lock(sk, req, GFP_ATOMIC);\n\n\tif (newsk != NULL) {\n\t\tstruct dccp_request_sock *dreq = dccp_rsk(req);\n\t\tstruct inet_connection_sock *newicsk = inet_csk(newsk);\n\t\tstruct dccp_sock *newdp = dccp_sk(newsk);\n\n\t\tnewdp->dccps_role\t    = DCCP_ROLE_SERVER;\n\t\tnewdp->dccps_hc_rx_ackvec   = NULL;\n\t\tnewdp->dccps_service_list   = NULL;\n\t\tnewdp->dccps_hc_rx_ccid     = NULL;\n\t\tnewdp->dccps_hc_tx_ccid     = NULL;\n\t\tnewdp->dccps_service\t    = dreq->dreq_service;\n\t\tnewdp->dccps_timestamp_echo = dreq->dreq_timestamp_echo;\n\t\tnewdp->dccps_timestamp_time = dreq->dreq_timestamp_time;\n\t\tnewicsk->icsk_rto\t    = DCCP_TIMEOUT_INIT;\n\n\t\tINIT_LIST_HEAD(&newdp->dccps_featneg);\n\t\t/*\n\t\t * Step 3: Process LISTEN state\n\t\t *\n\t\t *    Choose S.ISS (initial seqno) or set from Init Cookies\n\t\t *    Initialize S.GAR := S.ISS\n\t\t *    Set S.ISR, S.GSR from packet (or Init Cookies)\n\t\t *\n\t\t *    Setting AWL/AWH and SWL/SWH happens as part of the feature\n\t\t *    activation below, as these windows all depend on the local\n\t\t *    and remote Sequence Window feature values (7.5.2).\n\t\t */\n\t\tnewdp->dccps_iss = dreq->dreq_iss;\n\t\tnewdp->dccps_gss = dreq->dreq_gss;\n\t\tnewdp->dccps_gar = newdp->dccps_iss;\n\t\tnewdp->dccps_isr = dreq->dreq_isr;\n\t\tnewdp->dccps_gsr = dreq->dreq_gsr;\n\n\t\t/*\n\t\t * Activate features: initialise CCIDs, sequence windows etc.\n\t\t */\n\t\tif (dccp_feat_activate_values(newsk, &dreq->dreq_featneg)) {\n\t\t\tsk_free_unlock_clone(newsk);\n\t\t\treturn NULL;\n\t\t}\n\t\tdccp_init_xmit_timers(newsk);\n\n\t\t__DCCP_INC_STATS(DCCP_MIB_PASSIVEOPENS);\n\t}\n\treturn newsk;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,6 +18,8 @@\n \t\tnewdp->dccps_role\t    = DCCP_ROLE_SERVER;\n \t\tnewdp->dccps_hc_rx_ackvec   = NULL;\n \t\tnewdp->dccps_service_list   = NULL;\n+\t\tnewdp->dccps_hc_rx_ccid     = NULL;\n+\t\tnewdp->dccps_hc_tx_ccid     = NULL;\n \t\tnewdp->dccps_service\t    = dreq->dreq_service;\n \t\tnewdp->dccps_timestamp_echo = dreq->dreq_timestamp_echo;\n \t\tnewdp->dccps_timestamp_time = dreq->dreq_timestamp_time;",
        "function_modified_lines": {
            "added": [
                "\t\tnewdp->dccps_hc_rx_ccid     = NULL;",
                "\t\tnewdp->dccps_hc_tx_ccid     = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in the Linux kernel exploitable by a local attacker due to reuse of a DCCP socket with an attached dccps_hc_tx_ccid object as a listener after being released. Fixed in Ubuntu Linux kernel 5.4.0-51.56, 5.3.0-68.63, 4.15.0-121.123, 4.4.0-193.224, 3.13.0.182.191 and 3.2.0-149.196.",
        "id": 2548
    },
    {
        "cve_id": "CVE-2019-11487",
        "code_before_change": "static void buffer_pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tref->ref++;\n}",
        "code_after_change": "static bool buffer_pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tif (ref->ref > INT_MAX/2)\n\t\treturn false;\n\n\tref->ref++;\n\treturn true;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,11 @@\n-static void buffer_pipe_buf_get(struct pipe_inode_info *pipe,\n+static bool buffer_pipe_buf_get(struct pipe_inode_info *pipe,\n \t\t\t\tstruct pipe_buffer *buf)\n {\n \tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n \n+\tif (ref->ref > INT_MAX/2)\n+\t\treturn false;\n+\n \tref->ref++;\n+\treturn true;\n }",
        "function_modified_lines": {
            "added": [
                "static bool buffer_pipe_buf_get(struct pipe_inode_info *pipe,",
                "\tif (ref->ref > INT_MAX/2)",
                "\t\treturn false;",
                "",
                "\treturn true;"
            ],
            "deleted": [
                "static void buffer_pipe_buf_get(struct pipe_inode_info *pipe,"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 5.1-rc5 allows page->_refcount reference count overflow, with resultant use-after-free issues, if about 140 GiB of RAM exists. This is related to fs/fuse/dev.c, fs/pipe.c, fs/splice.c, include/linux/mm.h, include/linux/pipe_fs_i.h, kernel/trace/trace.c, mm/gup.c, and mm/hugetlb.c. It can occur with FUSE requests.",
        "id": 1922
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static void blk_trace_cleanup(struct blk_trace *bt)\n{\n\tblk_trace_free(bt);\n\tput_probe_ref();\n}",
        "code_after_change": "static void blk_trace_cleanup(struct blk_trace *bt)\n{\n\tsynchronize_rcu();\n\tblk_trace_free(bt);\n\tput_probe_ref();\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n static void blk_trace_cleanup(struct blk_trace *bt)\n {\n+\tsynchronize_rcu();\n \tblk_trace_free(bt);\n \tput_probe_ref();\n }",
        "function_modified_lines": {
            "added": [
                "\tsynchronize_rcu();"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2232
    },
    {
        "cve_id": "CVE-2018-10876",
        "code_before_change": "static struct buffer_head *\next4_read_inode_bitmap(struct super_block *sb, ext4_group_t block_group)\n{\n\tstruct ext4_group_desc *desc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct buffer_head *bh = NULL;\n\text4_fsblk_t bitmap_blk;\n\tint err;\n\n\tdesc = ext4_get_group_desc(sb, block_group, NULL);\n\tif (!desc)\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\n\tbitmap_blk = ext4_inode_bitmap(sb, desc);\n\tif ((bitmap_blk <= le32_to_cpu(sbi->s_es->s_first_data_block)) ||\n\t    (bitmap_blk >= ext4_blocks_count(sbi->s_es))) {\n\t\text4_error(sb, \"Invalid inode bitmap blk %llu in \"\n\t\t\t   \"block_group %u\", bitmap_blk, block_group);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\t}\n\tbh = sb_getblk(sb, bitmap_blk);\n\tif (unlikely(!bh)) {\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t    \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t    block_group, bitmap_blk);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tif (bitmap_uptodate(bh))\n\t\tgoto verify;\n\n\tlock_buffer(bh);\n\tif (bitmap_uptodate(bh)) {\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\n\text4_lock_group(sb, block_group);\n\tif (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) {\n\t\tmemset(bh->b_data, 0, (EXT4_INODES_PER_GROUP(sb) + 7) / 8);\n\t\text4_mark_bitmap_end(EXT4_INODES_PER_GROUP(sb),\n\t\t\t\t     sb->s_blocksize * 8, bh->b_data);\n\t\tset_bitmap_uptodate(bh);\n\t\tset_buffer_uptodate(bh);\n\t\tset_buffer_verified(bh);\n\t\text4_unlock_group(sb, block_group);\n\t\tunlock_buffer(bh);\n\t\treturn bh;\n\t}\n\text4_unlock_group(sb, block_group);\n\n\tif (buffer_uptodate(bh)) {\n\t\t/*\n\t\t * if not uninit if bh is uptodate,\n\t\t * bitmap is also uptodate\n\t\t */\n\t\tset_bitmap_uptodate(bh);\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\t/*\n\t * submit the buffer_head for reading\n\t */\n\ttrace_ext4_load_inode_bitmap(sb, block_group);\n\tbh->b_end_io = ext4_end_bitmap_read;\n\tget_bh(bh);\n\tsubmit_bh(REQ_OP_READ, REQ_META | REQ_PRIO, bh);\n\twait_on_buffer(bh);\n\tif (!buffer_uptodate(bh)) {\n\t\tput_bh(bh);\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t   \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t   block_group, bitmap_blk);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\nverify:\n\terr = ext4_validate_inode_bitmap(sb, desc, block_group, bh);\n\tif (err)\n\t\tgoto out;\n\treturn bh;\nout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "static struct buffer_head *\next4_read_inode_bitmap(struct super_block *sb, ext4_group_t block_group)\n{\n\tstruct ext4_group_desc *desc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct buffer_head *bh = NULL;\n\text4_fsblk_t bitmap_blk;\n\tint err;\n\n\tdesc = ext4_get_group_desc(sb, block_group, NULL);\n\tif (!desc)\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\n\tbitmap_blk = ext4_inode_bitmap(sb, desc);\n\tif ((bitmap_blk <= le32_to_cpu(sbi->s_es->s_first_data_block)) ||\n\t    (bitmap_blk >= ext4_blocks_count(sbi->s_es))) {\n\t\text4_error(sb, \"Invalid inode bitmap blk %llu in \"\n\t\t\t   \"block_group %u\", bitmap_blk, block_group);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\t}\n\tbh = sb_getblk(sb, bitmap_blk);\n\tif (unlikely(!bh)) {\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t    \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t    block_group, bitmap_blk);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tif (bitmap_uptodate(bh))\n\t\tgoto verify;\n\n\tlock_buffer(bh);\n\tif (bitmap_uptodate(bh)) {\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\n\text4_lock_group(sb, block_group);\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT))) {\n\t\tif (block_group == 0) {\n\t\t\text4_unlock_group(sb, block_group);\n\t\t\tunlock_buffer(bh);\n\t\t\text4_error(sb, \"Inode bitmap for bg 0 marked \"\n\t\t\t\t   \"uninitialized\");\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t}\n\t\tmemset(bh->b_data, 0, (EXT4_INODES_PER_GROUP(sb) + 7) / 8);\n\t\text4_mark_bitmap_end(EXT4_INODES_PER_GROUP(sb),\n\t\t\t\t     sb->s_blocksize * 8, bh->b_data);\n\t\tset_bitmap_uptodate(bh);\n\t\tset_buffer_uptodate(bh);\n\t\tset_buffer_verified(bh);\n\t\text4_unlock_group(sb, block_group);\n\t\tunlock_buffer(bh);\n\t\treturn bh;\n\t}\n\text4_unlock_group(sb, block_group);\n\n\tif (buffer_uptodate(bh)) {\n\t\t/*\n\t\t * if not uninit if bh is uptodate,\n\t\t * bitmap is also uptodate\n\t\t */\n\t\tset_bitmap_uptodate(bh);\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\t/*\n\t * submit the buffer_head for reading\n\t */\n\ttrace_ext4_load_inode_bitmap(sb, block_group);\n\tbh->b_end_io = ext4_end_bitmap_read;\n\tget_bh(bh);\n\tsubmit_bh(REQ_OP_READ, REQ_META | REQ_PRIO, bh);\n\twait_on_buffer(bh);\n\tif (!buffer_uptodate(bh)) {\n\t\tput_bh(bh);\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t   \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t   block_group, bitmap_blk);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\nverify:\n\terr = ext4_validate_inode_bitmap(sb, desc, block_group, bh);\n\tif (err)\n\t\tgoto out;\n\treturn bh;\nout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -37,7 +37,16 @@\n \t}\n \n \text4_lock_group(sb, block_group);\n-\tif (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) {\n+\tif (ext4_has_group_desc_csum(sb) &&\n+\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT))) {\n+\t\tif (block_group == 0) {\n+\t\t\text4_unlock_group(sb, block_group);\n+\t\t\tunlock_buffer(bh);\n+\t\t\text4_error(sb, \"Inode bitmap for bg 0 marked \"\n+\t\t\t\t   \"uninitialized\");\n+\t\t\terr = -EFSCORRUPTED;\n+\t\t\tgoto out;\n+\t\t}\n \t\tmemset(bh->b_data, 0, (EXT4_INODES_PER_GROUP(sb) + 7) / 8);\n \t\text4_mark_bitmap_end(EXT4_INODES_PER_GROUP(sb),\n \t\t\t\t     sb->s_blocksize * 8, bh->b_data);",
        "function_modified_lines": {
            "added": [
                "\tif (ext4_has_group_desc_csum(sb) &&",
                "\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT))) {",
                "\t\tif (block_group == 0) {",
                "\t\t\text4_unlock_group(sb, block_group);",
                "\t\t\tunlock_buffer(bh);",
                "\t\t\text4_error(sb, \"Inode bitmap for bg 0 marked \"",
                "\t\t\t\t   \"uninitialized\");",
                "\t\t\terr = -EFSCORRUPTED;",
                "\t\t\tgoto out;",
                "\t\t}"
            ],
            "deleted": [
                "\tif (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in Linux kernel in the ext4 filesystem code. A use-after-free is possible in ext4_ext_remove_space() function when mounting and operating a crafted ext4 image.",
        "id": 1606
    },
    {
        "cve_id": "CVE-2023-3610",
        "code_before_change": "static int nft_immediate_init(const struct nft_ctx *ctx,\n\t\t\t      const struct nft_expr *expr,\n\t\t\t      const struct nlattr * const tb[])\n{\n\tstruct nft_immediate_expr *priv = nft_expr_priv(expr);\n\tstruct nft_data_desc desc = {\n\t\t.size\t= sizeof(priv->data),\n\t};\n\tint err;\n\n\tif (tb[NFTA_IMMEDIATE_DREG] == NULL ||\n\t    tb[NFTA_IMMEDIATE_DATA] == NULL)\n\t\treturn -EINVAL;\n\n\tdesc.type = nft_reg_to_type(tb[NFTA_IMMEDIATE_DREG]);\n\terr = nft_data_init(ctx, &priv->data, &desc, tb[NFTA_IMMEDIATE_DATA]);\n\tif (err < 0)\n\t\treturn err;\n\n\tpriv->dlen = desc.len;\n\n\terr = nft_parse_register_store(ctx, tb[NFTA_IMMEDIATE_DREG],\n\t\t\t\t       &priv->dreg, &priv->data, desc.type,\n\t\t\t\t       desc.len);\n\tif (err < 0)\n\t\tgoto err1;\n\n\tif (priv->dreg == NFT_REG_VERDICT) {\n\t\tstruct nft_chain *chain = priv->data.verdict.chain;\n\n\t\tswitch (priv->data.verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\tif (nft_chain_is_bound(chain)) {\n\t\t\t\terr = -EBUSY;\n\t\t\t\tgoto err1;\n\t\t\t}\n\t\t\tchain->bound = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr1:\n\tnft_data_release(&priv->data, desc.type);\n\treturn err;\n}",
        "code_after_change": "static int nft_immediate_init(const struct nft_ctx *ctx,\n\t\t\t      const struct nft_expr *expr,\n\t\t\t      const struct nlattr * const tb[])\n{\n\tstruct nft_immediate_expr *priv = nft_expr_priv(expr);\n\tstruct nft_data_desc desc = {\n\t\t.size\t= sizeof(priv->data),\n\t};\n\tint err;\n\n\tif (tb[NFTA_IMMEDIATE_DREG] == NULL ||\n\t    tb[NFTA_IMMEDIATE_DATA] == NULL)\n\t\treturn -EINVAL;\n\n\tdesc.type = nft_reg_to_type(tb[NFTA_IMMEDIATE_DREG]);\n\terr = nft_data_init(ctx, &priv->data, &desc, tb[NFTA_IMMEDIATE_DATA]);\n\tif (err < 0)\n\t\treturn err;\n\n\tpriv->dlen = desc.len;\n\n\terr = nft_parse_register_store(ctx, tb[NFTA_IMMEDIATE_DREG],\n\t\t\t\t       &priv->dreg, &priv->data, desc.type,\n\t\t\t\t       desc.len);\n\tif (err < 0)\n\t\tgoto err1;\n\n\tif (priv->dreg == NFT_REG_VERDICT) {\n\t\tstruct nft_chain *chain = priv->data.verdict.chain;\n\n\t\tswitch (priv->data.verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\terr = nf_tables_bind_chain(ctx, chain);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr1:\n\tnft_data_release(&priv->data, desc.type);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,11 +31,9 @@\n \t\tswitch (priv->data.verdict.code) {\n \t\tcase NFT_JUMP:\n \t\tcase NFT_GOTO:\n-\t\t\tif (nft_chain_is_bound(chain)) {\n-\t\t\t\terr = -EBUSY;\n-\t\t\t\tgoto err1;\n-\t\t\t}\n-\t\t\tchain->bound = true;\n+\t\t\terr = nf_tables_bind_chain(ctx, chain);\n+\t\t\tif (err < 0)\n+\t\t\t\treturn err;\n \t\t\tbreak;\n \t\tdefault:\n \t\t\tbreak;",
        "function_modified_lines": {
            "added": [
                "\t\t\terr = nf_tables_bind_chain(ctx, chain);",
                "\t\t\tif (err < 0)",
                "\t\t\t\treturn err;"
            ],
            "deleted": [
                "\t\t\tif (nft_chain_is_bound(chain)) {",
                "\t\t\t\terr = -EBUSY;",
                "\t\t\t\tgoto err1;",
                "\t\t\t}",
                "\t\t\tchain->bound = true;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nFlaw in the error handling of bound chains causes a use-after-free in the abort path of NFT_MSG_NEWRULE. The vulnerability requires CAP_NET_ADMIN to be triggered.\n\nWe recommend upgrading past commit 4bedf9eee016286c835e3d8fa981ddece5338795.\n\n",
        "id": 4129
    },
    {
        "cve_id": "CVE-2022-24122",
        "code_before_change": "struct ucounts *alloc_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\tbool wrapped;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tatomic_set(&new->count, 1);\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tspin_unlock_irq(&ucounts_lock);\n\t\t\treturn new;\n\t\t}\n\t}\n\twrapped = !get_ucounts_or_wrap(ucounts);\n\tspin_unlock_irq(&ucounts_lock);\n\tif (wrapped) {\n\t\tput_ucounts(ucounts);\n\t\treturn NULL;\n\t}\n\treturn ucounts;\n}",
        "code_after_change": "struct ucounts *alloc_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\tbool wrapped;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tatomic_set(&new->count, 1);\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tget_user_ns(new->ns);\n\t\t\tspin_unlock_irq(&ucounts_lock);\n\t\t\treturn new;\n\t\t}\n\t}\n\twrapped = !get_ucounts_or_wrap(ucounts);\n\tspin_unlock_irq(&ucounts_lock);\n\tif (wrapped) {\n\t\tput_ucounts(ucounts);\n\t\treturn NULL;\n\t}\n\treturn ucounts;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,6 +23,7 @@\n \t\t\tkfree(new);\n \t\t} else {\n \t\t\thlist_add_head(&new->node, hashent);\n+\t\t\tget_user_ns(new->ns);\n \t\t\tspin_unlock_irq(&ucounts_lock);\n \t\t\treturn new;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tget_user_ns(new->ns);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "kernel/ucount.c in the Linux kernel 5.14 through 5.16.4, when unprivileged user namespaces are enabled, allows a use-after-free and privilege escalation because a ucounts object can outlive its namespace.",
        "id": 3470
    },
    {
        "cve_id": "CVE-2020-27418",
        "code_before_change": "static int vgacon_resize(struct vc_data *c, unsigned int width,\n\t\t\t unsigned int height, unsigned int user)\n{\n\tif (width % 2 || width > screen_info.orig_video_cols ||\n\t    height > (screen_info.orig_video_lines * vga_default_font_height)/\n\t    c->vc_font.height)\n\t\t/* let svgatextmode tinker with video timings and\n\t\t   return success */\n\t\treturn (user) ? 0 : -EINVAL;\n\n\tif (con_is_visible(c) && !vga_is_gfx) /* who knows */\n\t\tvgacon_doresize(c, width, height);\n\treturn 0;\n}",
        "code_after_change": "static int vgacon_resize(struct vc_data *c, unsigned int width,\n\t\t\t unsigned int height, unsigned int user)\n{\n\tif ((width << 1) * height > vga_vram_size)\n\t\treturn -EINVAL;\n\n\tif (width % 2 || width > screen_info.orig_video_cols ||\n\t    height > (screen_info.orig_video_lines * vga_default_font_height)/\n\t    c->vc_font.height)\n\t\t/* let svgatextmode tinker with video timings and\n\t\t   return success */\n\t\treturn (user) ? 0 : -EINVAL;\n\n\tif (con_is_visible(c) && !vga_is_gfx) /* who knows */\n\t\tvgacon_doresize(c, width, height);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,9 @@\n static int vgacon_resize(struct vc_data *c, unsigned int width,\n \t\t\t unsigned int height, unsigned int user)\n {\n+\tif ((width << 1) * height > vga_vram_size)\n+\t\treturn -EINVAL;\n+\n \tif (width % 2 || width > screen_info.orig_video_cols ||\n \t    height > (screen_info.orig_video_lines * vga_default_font_height)/\n \t    c->vc_font.height)",
        "function_modified_lines": {
            "added": [
                "\tif ((width << 1) * height > vga_vram_size)",
                "\t\treturn -EINVAL;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A Use After Free vulnerability in Fedora Linux kernel 5.9.0-rc9 allows attackers to obatin sensitive information via vgacon_invert_region() function.",
        "id": 2619
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "static int hfi1_file_close(struct inode *inode, struct file *fp)\n{\n\tstruct hfi1_filedata *fdata = fp->private_data;\n\tstruct hfi1_ctxtdata *uctxt = fdata->uctxt;\n\tstruct hfi1_devdata *dd = container_of(inode->i_cdev,\n\t\t\t\t\t       struct hfi1_devdata,\n\t\t\t\t\t       user_cdev);\n\tunsigned long flags, *ev;\n\n\tfp->private_data = NULL;\n\n\tif (!uctxt)\n\t\tgoto done;\n\n\thfi1_cdbg(PROC, \"closing ctxt %u:%u\", uctxt->ctxt, fdata->subctxt);\n\n\tflush_wc();\n\t/* drain user sdma queue */\n\thfi1_user_sdma_free_queues(fdata, uctxt);\n\n\t/* release the cpu */\n\thfi1_put_proc_affinity(fdata->rec_cpu_num);\n\n\t/* clean up rcv side */\n\thfi1_user_exp_rcv_free(fdata);\n\n\t/*\n\t * fdata->uctxt is used in the above cleanup.  It is not ready to be\n\t * removed until here.\n\t */\n\tfdata->uctxt = NULL;\n\thfi1_rcd_put(uctxt);\n\n\t/*\n\t * Clear any left over, unhandled events so the next process that\n\t * gets this context doesn't get confused.\n\t */\n\tev = dd->events + uctxt_offset(uctxt) + fdata->subctxt;\n\t*ev = 0;\n\n\tspin_lock_irqsave(&dd->uctxt_lock, flags);\n\t__clear_bit(fdata->subctxt, uctxt->in_use_ctxts);\n\tif (!bitmap_empty(uctxt->in_use_ctxts, HFI1_MAX_SHARED_CTXTS)) {\n\t\tspin_unlock_irqrestore(&dd->uctxt_lock, flags);\n\t\tgoto done;\n\t}\n\tspin_unlock_irqrestore(&dd->uctxt_lock, flags);\n\n\t/*\n\t * Disable receive context and interrupt available, reset all\n\t * RcvCtxtCtrl bits to default values.\n\t */\n\thfi1_rcvctrl(dd, HFI1_RCVCTRL_CTXT_DIS |\n\t\t     HFI1_RCVCTRL_TIDFLOW_DIS |\n\t\t     HFI1_RCVCTRL_INTRAVAIL_DIS |\n\t\t     HFI1_RCVCTRL_TAILUPD_DIS |\n\t\t     HFI1_RCVCTRL_ONE_PKT_EGR_DIS |\n\t\t     HFI1_RCVCTRL_NO_RHQ_DROP_DIS |\n\t\t     HFI1_RCVCTRL_NO_EGR_DROP_DIS |\n\t\t     HFI1_RCVCTRL_URGENT_DIS, uctxt);\n\t/* Clear the context's J_KEY */\n\thfi1_clear_ctxt_jkey(dd, uctxt);\n\t/*\n\t * If a send context is allocated, reset context integrity\n\t * checks to default and disable the send context.\n\t */\n\tif (uctxt->sc) {\n\t\tsc_disable(uctxt->sc);\n\t\tset_pio_integrity(uctxt->sc);\n\t}\n\n\thfi1_free_ctxt_rcv_groups(uctxt);\n\thfi1_clear_ctxt_pkey(dd, uctxt);\n\n\tuctxt->event_flags = 0;\n\n\tdeallocate_ctxt(uctxt);\ndone:\n\tmmdrop(fdata->mm);\n\n\tif (atomic_dec_and_test(&dd->user_refcount))\n\t\tcomplete(&dd->user_comp);\n\n\tcleanup_srcu_struct(&fdata->pq_srcu);\n\tkfree(fdata);\n\treturn 0;\n}",
        "code_after_change": "static int hfi1_file_close(struct inode *inode, struct file *fp)\n{\n\tstruct hfi1_filedata *fdata = fp->private_data;\n\tstruct hfi1_ctxtdata *uctxt = fdata->uctxt;\n\tstruct hfi1_devdata *dd = container_of(inode->i_cdev,\n\t\t\t\t\t       struct hfi1_devdata,\n\t\t\t\t\t       user_cdev);\n\tunsigned long flags, *ev;\n\n\tfp->private_data = NULL;\n\n\tif (!uctxt)\n\t\tgoto done;\n\n\thfi1_cdbg(PROC, \"closing ctxt %u:%u\", uctxt->ctxt, fdata->subctxt);\n\n\tflush_wc();\n\t/* drain user sdma queue */\n\thfi1_user_sdma_free_queues(fdata, uctxt);\n\n\t/* release the cpu */\n\thfi1_put_proc_affinity(fdata->rec_cpu_num);\n\n\t/* clean up rcv side */\n\thfi1_user_exp_rcv_free(fdata);\n\n\t/*\n\t * fdata->uctxt is used in the above cleanup.  It is not ready to be\n\t * removed until here.\n\t */\n\tfdata->uctxt = NULL;\n\thfi1_rcd_put(uctxt);\n\n\t/*\n\t * Clear any left over, unhandled events so the next process that\n\t * gets this context doesn't get confused.\n\t */\n\tev = dd->events + uctxt_offset(uctxt) + fdata->subctxt;\n\t*ev = 0;\n\n\tspin_lock_irqsave(&dd->uctxt_lock, flags);\n\t__clear_bit(fdata->subctxt, uctxt->in_use_ctxts);\n\tif (!bitmap_empty(uctxt->in_use_ctxts, HFI1_MAX_SHARED_CTXTS)) {\n\t\tspin_unlock_irqrestore(&dd->uctxt_lock, flags);\n\t\tgoto done;\n\t}\n\tspin_unlock_irqrestore(&dd->uctxt_lock, flags);\n\n\t/*\n\t * Disable receive context and interrupt available, reset all\n\t * RcvCtxtCtrl bits to default values.\n\t */\n\thfi1_rcvctrl(dd, HFI1_RCVCTRL_CTXT_DIS |\n\t\t     HFI1_RCVCTRL_TIDFLOW_DIS |\n\t\t     HFI1_RCVCTRL_INTRAVAIL_DIS |\n\t\t     HFI1_RCVCTRL_TAILUPD_DIS |\n\t\t     HFI1_RCVCTRL_ONE_PKT_EGR_DIS |\n\t\t     HFI1_RCVCTRL_NO_RHQ_DROP_DIS |\n\t\t     HFI1_RCVCTRL_NO_EGR_DROP_DIS |\n\t\t     HFI1_RCVCTRL_URGENT_DIS, uctxt);\n\t/* Clear the context's J_KEY */\n\thfi1_clear_ctxt_jkey(dd, uctxt);\n\t/*\n\t * If a send context is allocated, reset context integrity\n\t * checks to default and disable the send context.\n\t */\n\tif (uctxt->sc) {\n\t\tsc_disable(uctxt->sc);\n\t\tset_pio_integrity(uctxt->sc);\n\t}\n\n\thfi1_free_ctxt_rcv_groups(uctxt);\n\thfi1_clear_ctxt_pkey(dd, uctxt);\n\n\tuctxt->event_flags = 0;\n\n\tdeallocate_ctxt(uctxt);\ndone:\n\n\tif (atomic_dec_and_test(&dd->user_refcount))\n\t\tcomplete(&dd->user_comp);\n\n\tcleanup_srcu_struct(&fdata->pq_srcu);\n\tkfree(fdata);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -76,7 +76,6 @@\n \n \tdeallocate_ctxt(uctxt);\n done:\n-\tmmdrop(fdata->mm);\n \n \tif (atomic_dec_and_test(&dd->user_refcount))\n \t\tcomplete(&dd->user_comp);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tmmdrop(fdata->mm);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2641
    },
    {
        "cve_id": "CVE-2017-2584",
        "code_before_change": "static int em_fxsave(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct fxregs_state fx_state;\n\tsize_t size;\n\tint rc;\n\n\trc = check_fxsr(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tctxt->ops->get_fpu(ctxt);\n\n\trc = asm_safe(\"fxsave %[fx]\", , [fx] \"+m\"(fx_state));\n\n\tctxt->ops->put_fpu(ctxt);\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tif (ctxt->ops->get_cr(ctxt, 4) & X86_CR4_OSFXSR)\n\t\tsize = offsetof(struct fxregs_state, xmm_space[8 * 16/4]);\n\telse\n\t\tsize = offsetof(struct fxregs_state, xmm_space[0]);\n\n\treturn segmented_write(ctxt, ctxt->memop.addr.mem, &fx_state, size);\n}",
        "code_after_change": "static int em_fxsave(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct fxregs_state fx_state;\n\tsize_t size;\n\tint rc;\n\n\trc = check_fxsr(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tctxt->ops->get_fpu(ctxt);\n\n\trc = asm_safe(\"fxsave %[fx]\", , [fx] \"+m\"(fx_state));\n\n\tctxt->ops->put_fpu(ctxt);\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tif (ctxt->ops->get_cr(ctxt, 4) & X86_CR4_OSFXSR)\n\t\tsize = offsetof(struct fxregs_state, xmm_space[8 * 16/4]);\n\telse\n\t\tsize = offsetof(struct fxregs_state, xmm_space[0]);\n\n\treturn segmented_write_std(ctxt, ctxt->memop.addr.mem, &fx_state, size);\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,5 +22,5 @@\n \telse\n \t\tsize = offsetof(struct fxregs_state, xmm_space[0]);\n \n-\treturn segmented_write(ctxt, ctxt->memop.addr.mem, &fx_state, size);\n+\treturn segmented_write_std(ctxt, ctxt->memop.addr.mem, &fx_state, size);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn segmented_write_std(ctxt, ctxt->memop.addr.mem, &fx_state, size);"
            ],
            "deleted": [
                "\treturn segmented_write(ctxt, ctxt->memop.addr.mem, &fx_state, size);"
            ]
        },
        "cwe": [
            "CWE-200",
            "CWE-416"
        ],
        "cve_description": "arch/x86/kvm/emulate.c in the Linux kernel through 4.9.3 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free) via a crafted application that leverages instruction emulation for fxrstor, fxsave, sgdt, and sidt.",
        "id": 1446
    },
    {
        "cve_id": "CVE-2021-39801",
        "code_before_change": "long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\tunion ion_ioctl_arg data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The copy_from_user is unconditional here for both read and write\n\t * to do the validate. If there is no write for the ioctl, the\n\t * buffer is cleared\n\t */\n\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\treturn -EFAULT;\n\n\tret = validate_ioctl_arg(cmd, &data);\n\tif (ret) {\n\t\tpr_warn_once(\"%s: ioctl validate failed\\n\", __func__);\n\t\treturn ret;\n\t}\n\n\tif (!(dir & _IOC_WRITE))\n\t\tmemset(&data, 0, sizeof(data));\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tdata.fd.fd = ion_share_dma_buf_fd_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tcase ION_IOC_HEAP_QUERY:\n\t\tret = ion_query_heaps(client, &data.query);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
        "code_after_change": "long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\tunion ion_ioctl_arg data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The copy_from_user is unconditional here for both read and write\n\t * to do the validate. If there is no write for the ioctl, the\n\t * buffer is cleared\n\t */\n\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\treturn -EFAULT;\n\n\tret = validate_ioctl_arg(cmd, &data);\n\tif (ret) {\n\t\tpr_warn_once(\"%s: ioctl validate failed\\n\", __func__);\n\t\treturn ret;\n\t}\n\n\tif (!(dir & _IOC_WRITE))\n\t\tmemset(&data, 0, sizeof(data));\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = __ion_alloc(client, data.allocation.len,\n\t\t\t\t     data.allocation.align,\n\t\t\t\t     data.allocation.heap_id_mask,\n\t\t\t\t     data.allocation.flags, true);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tdata.fd.fd = ion_share_dma_buf_fd_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tcase ION_IOC_HEAP_QUERY:\n\t\tret = ion_query_heaps(client, &data.query);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle) {\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\t\tion_handle_put(cleanup_handle);\n\t\t\t}\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\tif (cleanup_handle)\n\t\tion_handle_put(cleanup_handle);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -34,10 +34,10 @@\n \t{\n \t\tstruct ion_handle *handle;\n \n-\t\thandle = ion_alloc(client, data.allocation.len,\n-\t\t\t\t\t\tdata.allocation.align,\n-\t\t\t\t\t\tdata.allocation.heap_id_mask,\n-\t\t\t\t\t\tdata.allocation.flags);\n+\t\thandle = __ion_alloc(client, data.allocation.len,\n+\t\t\t\t     data.allocation.align,\n+\t\t\t\t     data.allocation.heap_id_mask,\n+\t\t\t\t     data.allocation.flags, true);\n \t\tif (IS_ERR(handle))\n \t\t\treturn PTR_ERR(handle);\n \n@@ -112,10 +112,14 @@\n \n \tif (dir & _IOC_READ) {\n \t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n-\t\t\tif (cleanup_handle)\n+\t\t\tif (cleanup_handle) {\n \t\t\t\tion_free(client, cleanup_handle);\n+\t\t\t\tion_handle_put(cleanup_handle);\n+\t\t\t}\n \t\t\treturn -EFAULT;\n \t\t}\n \t}\n+\tif (cleanup_handle)\n+\t\tion_handle_put(cleanup_handle);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t\thandle = __ion_alloc(client, data.allocation.len,",
                "\t\t\t\t     data.allocation.align,",
                "\t\t\t\t     data.allocation.heap_id_mask,",
                "\t\t\t\t     data.allocation.flags, true);",
                "\t\t\tif (cleanup_handle) {",
                "\t\t\t\tion_handle_put(cleanup_handle);",
                "\t\t\t}",
                "\tif (cleanup_handle)",
                "\t\tion_handle_put(cleanup_handle);"
            ],
            "deleted": [
                "\t\thandle = ion_alloc(client, data.allocation.len,",
                "\t\t\t\t\t\tdata.allocation.align,",
                "\t\t\t\t\t\tdata.allocation.heap_id_mask,",
                "\t\t\t\t\t\tdata.allocation.flags);",
                "\t\t\tif (cleanup_handle)"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In ion_ioctl of ion-ioctl.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-209791720References: Upstream kernel",
        "id": 3110
    },
    {
        "cve_id": "CVE-2023-3610",
        "code_before_change": "void nf_tables_rule_release(const struct nft_ctx *ctx, struct nft_rule *rule)\n{\n\tnft_rule_expr_deactivate(ctx, rule, NFT_TRANS_RELEASE);\n\tnf_tables_rule_destroy(ctx, rule);\n}",
        "code_after_change": "static void nf_tables_rule_release(const struct nft_ctx *ctx, struct nft_rule *rule)\n{\n\tnft_rule_expr_deactivate(ctx, rule, NFT_TRANS_RELEASE);\n\tnf_tables_rule_destroy(ctx, rule);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-void nf_tables_rule_release(const struct nft_ctx *ctx, struct nft_rule *rule)\n+static void nf_tables_rule_release(const struct nft_ctx *ctx, struct nft_rule *rule)\n {\n \tnft_rule_expr_deactivate(ctx, rule, NFT_TRANS_RELEASE);\n \tnf_tables_rule_destroy(ctx, rule);",
        "function_modified_lines": {
            "added": [
                "static void nf_tables_rule_release(const struct nft_ctx *ctx, struct nft_rule *rule)"
            ],
            "deleted": [
                "void nf_tables_rule_release(const struct nft_ctx *ctx, struct nft_rule *rule)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nFlaw in the error handling of bound chains causes a use-after-free in the abort path of NFT_MSG_NEWRULE. The vulnerability requires CAP_NET_ADMIN to be triggered.\n\nWe recommend upgrading past commit 4bedf9eee016286c835e3d8fa981ddece5338795.\n\n",
        "id": 4118
    },
    {
        "cve_id": "CVE-2020-25220",
        "code_before_change": "void cgroup_sk_clone(struct sock_cgroup_data *skcd)\n{\n\t/* Socket clone path */\n\tif (skcd->val) {\n\t\t/*\n\t\t * We might be cloning a socket which is left in an empty\n\t\t * cgroup and the cgroup might have already been rmdir'd.\n\t\t * Don't use cgroup_get_live().\n\t\t */\n\t\tcgroup_get(sock_cgroup_ptr(skcd));\n\t}\n}",
        "code_after_change": "void cgroup_sk_clone(struct sock_cgroup_data *skcd)\n{\n\t/* Socket clone path */\n\tif (skcd->val) {\n\t\tif (skcd->no_refcnt)\n\t\t\treturn;\n\t\t/*\n\t\t * We might be cloning a socket which is left in an empty\n\t\t * cgroup and the cgroup might have already been rmdir'd.\n\t\t * Don't use cgroup_get_live().\n\t\t */\n\t\tcgroup_get(sock_cgroup_ptr(skcd));\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,8 @@\n {\n \t/* Socket clone path */\n \tif (skcd->val) {\n+\t\tif (skcd->no_refcnt)\n+\t\t\treturn;\n \t\t/*\n \t\t * We might be cloning a socket which is left in an empty\n \t\t * cgroup and the cgroup might have already been rmdir'd.",
        "function_modified_lines": {
            "added": [
                "\t\tif (skcd->no_refcnt)",
                "\t\t\treturn;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel 4.9.x before 4.9.233, 4.14.x before 4.14.194, and 4.19.x before 4.19.140 has a use-after-free because skcd->no_refcnt was not considered during a backport of a CVE-2020-14356 patch. This is related to the cgroups feature.",
        "id": 2580
    },
    {
        "cve_id": "CVE-2023-4015",
        "code_before_change": "static void nft_immediate_deactivate(const struct nft_ctx *ctx,\n\t\t\t\t     const struct nft_expr *expr,\n\t\t\t\t     enum nft_trans_phase phase)\n{\n\tconst struct nft_immediate_expr *priv = nft_expr_priv(expr);\n\tconst struct nft_data *data = &priv->data;\n\tstruct nft_ctx chain_ctx;\n\tstruct nft_chain *chain;\n\tstruct nft_rule *rule;\n\n\tif (priv->dreg == NFT_REG_VERDICT) {\n\t\tswitch (data->verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\tchain = data->verdict.chain;\n\t\t\tif (!nft_chain_binding(chain))\n\t\t\t\tbreak;\n\n\t\t\tchain_ctx = *ctx;\n\t\t\tchain_ctx.chain = chain;\n\n\t\t\tlist_for_each_entry(rule, &chain->rules, list)\n\t\t\t\tnft_rule_expr_deactivate(&chain_ctx, rule, phase);\n\n\t\t\tswitch (phase) {\n\t\t\tcase NFT_TRANS_PREPARE_ERROR:\n\t\t\t\tnf_tables_unbind_chain(ctx, chain);\n\t\t\t\tfallthrough;\n\t\t\tcase NFT_TRANS_PREPARE:\n\t\t\t\tnft_deactivate_next(ctx->net, chain);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tnft_chain_del(chain);\n\t\t\t\tchain->bound = false;\n\t\t\t\tnft_use_dec(&chain->table->use);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (phase == NFT_TRANS_COMMIT)\n\t\treturn;\n\n\treturn nft_data_release(&priv->data, nft_dreg_to_type(priv->dreg));\n}",
        "code_after_change": "static void nft_immediate_deactivate(const struct nft_ctx *ctx,\n\t\t\t\t     const struct nft_expr *expr,\n\t\t\t\t     enum nft_trans_phase phase)\n{\n\tconst struct nft_immediate_expr *priv = nft_expr_priv(expr);\n\tconst struct nft_data *data = &priv->data;\n\tstruct nft_chain *chain;\n\n\tif (priv->dreg == NFT_REG_VERDICT) {\n\t\tswitch (data->verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\tchain = data->verdict.chain;\n\t\t\tif (!nft_chain_binding(chain))\n\t\t\t\tbreak;\n\n\t\t\tswitch (phase) {\n\t\t\tcase NFT_TRANS_PREPARE_ERROR:\n\t\t\t\tnf_tables_unbind_chain(ctx, chain);\n\t\t\t\tnft_deactivate_next(ctx->net, chain);\n\t\t\t\tbreak;\n\t\t\tcase NFT_TRANS_PREPARE:\n\t\t\t\tnft_immediate_chain_deactivate(ctx, chain, phase);\n\t\t\t\tnft_deactivate_next(ctx->net, chain);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tnft_immediate_chain_deactivate(ctx, chain, phase);\n\t\t\t\tnft_chain_del(chain);\n\t\t\t\tchain->bound = false;\n\t\t\t\tnft_use_dec(&chain->table->use);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (phase == NFT_TRANS_COMMIT)\n\t\treturn;\n\n\treturn nft_data_release(&priv->data, nft_dreg_to_type(priv->dreg));\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,9 +4,7 @@\n {\n \tconst struct nft_immediate_expr *priv = nft_expr_priv(expr);\n \tconst struct nft_data *data = &priv->data;\n-\tstruct nft_ctx chain_ctx;\n \tstruct nft_chain *chain;\n-\tstruct nft_rule *rule;\n \n \tif (priv->dreg == NFT_REG_VERDICT) {\n \t\tswitch (data->verdict.code) {\n@@ -16,20 +14,17 @@\n \t\t\tif (!nft_chain_binding(chain))\n \t\t\t\tbreak;\n \n-\t\t\tchain_ctx = *ctx;\n-\t\t\tchain_ctx.chain = chain;\n-\n-\t\t\tlist_for_each_entry(rule, &chain->rules, list)\n-\t\t\t\tnft_rule_expr_deactivate(&chain_ctx, rule, phase);\n-\n \t\t\tswitch (phase) {\n \t\t\tcase NFT_TRANS_PREPARE_ERROR:\n \t\t\t\tnf_tables_unbind_chain(ctx, chain);\n-\t\t\t\tfallthrough;\n+\t\t\t\tnft_deactivate_next(ctx->net, chain);\n+\t\t\t\tbreak;\n \t\t\tcase NFT_TRANS_PREPARE:\n+\t\t\t\tnft_immediate_chain_deactivate(ctx, chain, phase);\n \t\t\t\tnft_deactivate_next(ctx->net, chain);\n \t\t\t\tbreak;\n \t\t\tdefault:\n+\t\t\t\tnft_immediate_chain_deactivate(ctx, chain, phase);\n \t\t\t\tnft_chain_del(chain);\n \t\t\t\tchain->bound = false;\n \t\t\t\tnft_use_dec(&chain->table->use);",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tnft_deactivate_next(ctx->net, chain);",
                "\t\t\t\tbreak;",
                "\t\t\t\tnft_immediate_chain_deactivate(ctx, chain, phase);",
                "\t\t\t\tnft_immediate_chain_deactivate(ctx, chain, phase);"
            ],
            "deleted": [
                "\tstruct nft_ctx chain_ctx;",
                "\tstruct nft_rule *rule;",
                "\t\t\tchain_ctx = *ctx;",
                "\t\t\tchain_ctx.chain = chain;",
                "",
                "\t\t\tlist_for_each_entry(rule, &chain->rules, list)",
                "\t\t\t\tnft_rule_expr_deactivate(&chain_ctx, rule, phase);",
                "",
                "\t\t\t\tfallthrough;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nOn an error when building a nftables rule, deactivating immediate expressions in nft_immediate_deactivate() can lead unbinding the chain and objects be deactivated but later used.\n\nWe recommend upgrading past commit 0a771f7b266b02d262900c75f1e175c7fe76fec2.\n\n",
        "id": 4188
    },
    {
        "cve_id": "CVE-2023-4622",
        "code_before_change": "static ssize_t unix_stream_sendpage(struct socket *socket, struct page *page,\n\t\t\t\t    int offset, size_t size, int flags)\n{\n\tint err;\n\tbool send_sigpipe = false;\n\tbool init_scm = true;\n\tstruct scm_cookie scm;\n\tstruct sock *other, *sk = socket->sk;\n\tstruct sk_buff *skb, *newskb = NULL, *tail = NULL;\n\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tother = unix_peer(sk);\n\tif (!other || sk->sk_state != TCP_ESTABLISHED)\n\t\treturn -ENOTCONN;\n\n\tif (false) {\nalloc_skb:\n\t\tunix_state_unlock(other);\n\t\tmutex_unlock(&unix_sk(other)->iolock);\n\t\tnewskb = sock_alloc_send_pskb(sk, 0, 0, flags & MSG_DONTWAIT,\n\t\t\t\t\t      &err, 0);\n\t\tif (!newskb)\n\t\t\tgoto err;\n\t}\n\n\t/* we must acquire iolock as we modify already present\n\t * skbs in the sk_receive_queue and mess with skb->len\n\t */\n\terr = mutex_lock_interruptible(&unix_sk(other)->iolock);\n\tif (err) {\n\t\terr = flags & MSG_DONTWAIT ? -EAGAIN : -ERESTARTSYS;\n\t\tgoto err;\n\t}\n\n\tif (sk->sk_shutdown & SEND_SHUTDOWN) {\n\t\terr = -EPIPE;\n\t\tsend_sigpipe = true;\n\t\tgoto err_unlock;\n\t}\n\n\tunix_state_lock(other);\n\n\tif (sock_flag(other, SOCK_DEAD) ||\n\t    other->sk_shutdown & RCV_SHUTDOWN) {\n\t\terr = -EPIPE;\n\t\tsend_sigpipe = true;\n\t\tgoto err_state_unlock;\n\t}\n\n\tif (init_scm) {\n\t\terr = maybe_init_creds(&scm, socket, other);\n\t\tif (err)\n\t\t\tgoto err_state_unlock;\n\t\tinit_scm = false;\n\t}\n\n\tskb = skb_peek_tail(&other->sk_receive_queue);\n\tif (tail && tail == skb) {\n\t\tskb = newskb;\n\t} else if (!skb || !unix_skb_scm_eq(skb, &scm)) {\n\t\tif (newskb) {\n\t\t\tskb = newskb;\n\t\t} else {\n\t\t\ttail = skb;\n\t\t\tgoto alloc_skb;\n\t\t}\n\t} else if (newskb) {\n\t\t/* this is fast path, we don't necessarily need to\n\t\t * call to kfree_skb even though with newskb == NULL\n\t\t * this - does no harm\n\t\t */\n\t\tconsume_skb(newskb);\n\t\tnewskb = NULL;\n\t}\n\n\tif (skb_append_pagefrags(skb, page, offset, size, MAX_SKB_FRAGS)) {\n\t\ttail = skb;\n\t\tgoto alloc_skb;\n\t}\n\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += size;\n\trefcount_add(size, &sk->sk_wmem_alloc);\n\n\tif (newskb) {\n\t\terr = unix_scm_to_skb(&scm, skb, false);\n\t\tif (err)\n\t\t\tgoto err_state_unlock;\n\t\tspin_lock(&other->sk_receive_queue.lock);\n\t\t__skb_queue_tail(&other->sk_receive_queue, newskb);\n\t\tspin_unlock(&other->sk_receive_queue.lock);\n\t}\n\n\tunix_state_unlock(other);\n\tmutex_unlock(&unix_sk(other)->iolock);\n\n\tother->sk_data_ready(other);\n\tscm_destroy(&scm);\n\treturn size;\n\nerr_state_unlock:\n\tunix_state_unlock(other);\nerr_unlock:\n\tmutex_unlock(&unix_sk(other)->iolock);\nerr:\n\tkfree_skb(newskb);\n\tif (send_sigpipe && !(flags & MSG_NOSIGNAL))\n\t\tsend_sig(SIGPIPE, current, 0);\n\tif (!init_scm)\n\t\tscm_destroy(&scm);\n\treturn err;\n}",
        "code_after_change": "static ssize_t unix_stream_sendpage(struct socket *socket, struct page *page,\n\t\t\t\t    int offset, size_t size, int flags)\n{\n\tstruct bio_vec bvec;\n\tstruct msghdr msg = { .msg_flags = flags | MSG_SPLICE_PAGES };\n\n\tif (flags & MSG_SENDPAGE_NOTLAST)\n\t\tmsg.msg_flags |= MSG_MORE;\n\n\tbvec_set_page(&bvec, page, size, offset);\n\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, size);\n\treturn unix_stream_sendmsg(socket, &msg, size);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,115 +1,13 @@\n static ssize_t unix_stream_sendpage(struct socket *socket, struct page *page,\n \t\t\t\t    int offset, size_t size, int flags)\n {\n-\tint err;\n-\tbool send_sigpipe = false;\n-\tbool init_scm = true;\n-\tstruct scm_cookie scm;\n-\tstruct sock *other, *sk = socket->sk;\n-\tstruct sk_buff *skb, *newskb = NULL, *tail = NULL;\n+\tstruct bio_vec bvec;\n+\tstruct msghdr msg = { .msg_flags = flags | MSG_SPLICE_PAGES };\n \n-\tif (flags & MSG_OOB)\n-\t\treturn -EOPNOTSUPP;\n+\tif (flags & MSG_SENDPAGE_NOTLAST)\n+\t\tmsg.msg_flags |= MSG_MORE;\n \n-\tother = unix_peer(sk);\n-\tif (!other || sk->sk_state != TCP_ESTABLISHED)\n-\t\treturn -ENOTCONN;\n-\n-\tif (false) {\n-alloc_skb:\n-\t\tunix_state_unlock(other);\n-\t\tmutex_unlock(&unix_sk(other)->iolock);\n-\t\tnewskb = sock_alloc_send_pskb(sk, 0, 0, flags & MSG_DONTWAIT,\n-\t\t\t\t\t      &err, 0);\n-\t\tif (!newskb)\n-\t\t\tgoto err;\n-\t}\n-\n-\t/* we must acquire iolock as we modify already present\n-\t * skbs in the sk_receive_queue and mess with skb->len\n-\t */\n-\terr = mutex_lock_interruptible(&unix_sk(other)->iolock);\n-\tif (err) {\n-\t\terr = flags & MSG_DONTWAIT ? -EAGAIN : -ERESTARTSYS;\n-\t\tgoto err;\n-\t}\n-\n-\tif (sk->sk_shutdown & SEND_SHUTDOWN) {\n-\t\terr = -EPIPE;\n-\t\tsend_sigpipe = true;\n-\t\tgoto err_unlock;\n-\t}\n-\n-\tunix_state_lock(other);\n-\n-\tif (sock_flag(other, SOCK_DEAD) ||\n-\t    other->sk_shutdown & RCV_SHUTDOWN) {\n-\t\terr = -EPIPE;\n-\t\tsend_sigpipe = true;\n-\t\tgoto err_state_unlock;\n-\t}\n-\n-\tif (init_scm) {\n-\t\terr = maybe_init_creds(&scm, socket, other);\n-\t\tif (err)\n-\t\t\tgoto err_state_unlock;\n-\t\tinit_scm = false;\n-\t}\n-\n-\tskb = skb_peek_tail(&other->sk_receive_queue);\n-\tif (tail && tail == skb) {\n-\t\tskb = newskb;\n-\t} else if (!skb || !unix_skb_scm_eq(skb, &scm)) {\n-\t\tif (newskb) {\n-\t\t\tskb = newskb;\n-\t\t} else {\n-\t\t\ttail = skb;\n-\t\t\tgoto alloc_skb;\n-\t\t}\n-\t} else if (newskb) {\n-\t\t/* this is fast path, we don't necessarily need to\n-\t\t * call to kfree_skb even though with newskb == NULL\n-\t\t * this - does no harm\n-\t\t */\n-\t\tconsume_skb(newskb);\n-\t\tnewskb = NULL;\n-\t}\n-\n-\tif (skb_append_pagefrags(skb, page, offset, size, MAX_SKB_FRAGS)) {\n-\t\ttail = skb;\n-\t\tgoto alloc_skb;\n-\t}\n-\n-\tskb->len += size;\n-\tskb->data_len += size;\n-\tskb->truesize += size;\n-\trefcount_add(size, &sk->sk_wmem_alloc);\n-\n-\tif (newskb) {\n-\t\terr = unix_scm_to_skb(&scm, skb, false);\n-\t\tif (err)\n-\t\t\tgoto err_state_unlock;\n-\t\tspin_lock(&other->sk_receive_queue.lock);\n-\t\t__skb_queue_tail(&other->sk_receive_queue, newskb);\n-\t\tspin_unlock(&other->sk_receive_queue.lock);\n-\t}\n-\n-\tunix_state_unlock(other);\n-\tmutex_unlock(&unix_sk(other)->iolock);\n-\n-\tother->sk_data_ready(other);\n-\tscm_destroy(&scm);\n-\treturn size;\n-\n-err_state_unlock:\n-\tunix_state_unlock(other);\n-err_unlock:\n-\tmutex_unlock(&unix_sk(other)->iolock);\n-err:\n-\tkfree_skb(newskb);\n-\tif (send_sigpipe && !(flags & MSG_NOSIGNAL))\n-\t\tsend_sig(SIGPIPE, current, 0);\n-\tif (!init_scm)\n-\t\tscm_destroy(&scm);\n-\treturn err;\n+\tbvec_set_page(&bvec, page, size, offset);\n+\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, size);\n+\treturn unix_stream_sendmsg(socket, &msg, size);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct bio_vec bvec;",
                "\tstruct msghdr msg = { .msg_flags = flags | MSG_SPLICE_PAGES };",
                "\tif (flags & MSG_SENDPAGE_NOTLAST)",
                "\t\tmsg.msg_flags |= MSG_MORE;",
                "\tbvec_set_page(&bvec, page, size, offset);",
                "\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, size);",
                "\treturn unix_stream_sendmsg(socket, &msg, size);"
            ],
            "deleted": [
                "\tint err;",
                "\tbool send_sigpipe = false;",
                "\tbool init_scm = true;",
                "\tstruct scm_cookie scm;",
                "\tstruct sock *other, *sk = socket->sk;",
                "\tstruct sk_buff *skb, *newskb = NULL, *tail = NULL;",
                "\tif (flags & MSG_OOB)",
                "\t\treturn -EOPNOTSUPP;",
                "\tother = unix_peer(sk);",
                "\tif (!other || sk->sk_state != TCP_ESTABLISHED)",
                "\t\treturn -ENOTCONN;",
                "",
                "\tif (false) {",
                "alloc_skb:",
                "\t\tunix_state_unlock(other);",
                "\t\tmutex_unlock(&unix_sk(other)->iolock);",
                "\t\tnewskb = sock_alloc_send_pskb(sk, 0, 0, flags & MSG_DONTWAIT,",
                "\t\t\t\t\t      &err, 0);",
                "\t\tif (!newskb)",
                "\t\t\tgoto err;",
                "\t}",
                "",
                "\t/* we must acquire iolock as we modify already present",
                "\t * skbs in the sk_receive_queue and mess with skb->len",
                "\t */",
                "\terr = mutex_lock_interruptible(&unix_sk(other)->iolock);",
                "\tif (err) {",
                "\t\terr = flags & MSG_DONTWAIT ? -EAGAIN : -ERESTARTSYS;",
                "\t\tgoto err;",
                "\t}",
                "",
                "\tif (sk->sk_shutdown & SEND_SHUTDOWN) {",
                "\t\terr = -EPIPE;",
                "\t\tsend_sigpipe = true;",
                "\t\tgoto err_unlock;",
                "\t}",
                "",
                "\tunix_state_lock(other);",
                "",
                "\tif (sock_flag(other, SOCK_DEAD) ||",
                "\t    other->sk_shutdown & RCV_SHUTDOWN) {",
                "\t\terr = -EPIPE;",
                "\t\tsend_sigpipe = true;",
                "\t\tgoto err_state_unlock;",
                "\t}",
                "",
                "\tif (init_scm) {",
                "\t\terr = maybe_init_creds(&scm, socket, other);",
                "\t\tif (err)",
                "\t\t\tgoto err_state_unlock;",
                "\t\tinit_scm = false;",
                "\t}",
                "",
                "\tskb = skb_peek_tail(&other->sk_receive_queue);",
                "\tif (tail && tail == skb) {",
                "\t\tskb = newskb;",
                "\t} else if (!skb || !unix_skb_scm_eq(skb, &scm)) {",
                "\t\tif (newskb) {",
                "\t\t\tskb = newskb;",
                "\t\t} else {",
                "\t\t\ttail = skb;",
                "\t\t\tgoto alloc_skb;",
                "\t\t}",
                "\t} else if (newskb) {",
                "\t\t/* this is fast path, we don't necessarily need to",
                "\t\t * call to kfree_skb even though with newskb == NULL",
                "\t\t * this - does no harm",
                "\t\t */",
                "\t\tconsume_skb(newskb);",
                "\t\tnewskb = NULL;",
                "\t}",
                "",
                "\tif (skb_append_pagefrags(skb, page, offset, size, MAX_SKB_FRAGS)) {",
                "\t\ttail = skb;",
                "\t\tgoto alloc_skb;",
                "\t}",
                "",
                "\tskb->len += size;",
                "\tskb->data_len += size;",
                "\tskb->truesize += size;",
                "\trefcount_add(size, &sk->sk_wmem_alloc);",
                "",
                "\tif (newskb) {",
                "\t\terr = unix_scm_to_skb(&scm, skb, false);",
                "\t\tif (err)",
                "\t\t\tgoto err_state_unlock;",
                "\t\tspin_lock(&other->sk_receive_queue.lock);",
                "\t\t__skb_queue_tail(&other->sk_receive_queue, newskb);",
                "\t\tspin_unlock(&other->sk_receive_queue.lock);",
                "\t}",
                "",
                "\tunix_state_unlock(other);",
                "\tmutex_unlock(&unix_sk(other)->iolock);",
                "",
                "\tother->sk_data_ready(other);",
                "\tscm_destroy(&scm);",
                "\treturn size;",
                "",
                "err_state_unlock:",
                "\tunix_state_unlock(other);",
                "err_unlock:",
                "\tmutex_unlock(&unix_sk(other)->iolock);",
                "err:",
                "\tkfree_skb(newskb);",
                "\tif (send_sigpipe && !(flags & MSG_NOSIGNAL))",
                "\t\tsend_sig(SIGPIPE, current, 0);",
                "\tif (!init_scm)",
                "\t\tscm_destroy(&scm);",
                "\treturn err;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's af_unix component can be exploited to achieve local privilege escalation.\n\nThe unix_stream_sendpage() function tries to add data to the last skb in the peer's recv queue without locking the queue. Thus there is a race where unix_stream_sendpage() could access an skb locklessly that is being released by garbage collection, resulting in use-after-free.\n\nWe recommend upgrading past commit 790c2f9d15b594350ae9bca7b236f2b1859de02c.\n\n",
        "id": 4238
    },
    {
        "cve_id": "CVE-2018-17182",
        "code_before_change": "void dump_mm(const struct mm_struct *mm)\n{\n\tpr_emerg(\"mm %px mmap %px seqnum %d task_size %lu\\n\"\n#ifdef CONFIG_MMU\n\t\t\"get_unmapped_area %px\\n\"\n#endif\n\t\t\"mmap_base %lu mmap_legacy_base %lu highest_vm_end %lu\\n\"\n\t\t\"pgd %px mm_users %d mm_count %d pgtables_bytes %lu map_count %d\\n\"\n\t\t\"hiwater_rss %lx hiwater_vm %lx total_vm %lx locked_vm %lx\\n\"\n\t\t\"pinned_vm %lx data_vm %lx exec_vm %lx stack_vm %lx\\n\"\n\t\t\"start_code %lx end_code %lx start_data %lx end_data %lx\\n\"\n\t\t\"start_brk %lx brk %lx start_stack %lx\\n\"\n\t\t\"arg_start %lx arg_end %lx env_start %lx env_end %lx\\n\"\n\t\t\"binfmt %px flags %lx core_state %px\\n\"\n#ifdef CONFIG_AIO\n\t\t\"ioctx_table %px\\n\"\n#endif\n#ifdef CONFIG_MEMCG\n\t\t\"owner %px \"\n#endif\n\t\t\"exe_file %px\\n\"\n#ifdef CONFIG_MMU_NOTIFIER\n\t\t\"mmu_notifier_mm %px\\n\"\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\t\"numa_next_scan %lu numa_scan_offset %lu numa_scan_seq %d\\n\"\n#endif\n\t\t\"tlb_flush_pending %d\\n\"\n\t\t\"def_flags: %#lx(%pGv)\\n\",\n\n\t\tmm, mm->mmap, mm->vmacache_seqnum, mm->task_size,\n#ifdef CONFIG_MMU\n\t\tmm->get_unmapped_area,\n#endif\n\t\tmm->mmap_base, mm->mmap_legacy_base, mm->highest_vm_end,\n\t\tmm->pgd, atomic_read(&mm->mm_users),\n\t\tatomic_read(&mm->mm_count),\n\t\tmm_pgtables_bytes(mm),\n\t\tmm->map_count,\n\t\tmm->hiwater_rss, mm->hiwater_vm, mm->total_vm, mm->locked_vm,\n\t\tmm->pinned_vm, mm->data_vm, mm->exec_vm, mm->stack_vm,\n\t\tmm->start_code, mm->end_code, mm->start_data, mm->end_data,\n\t\tmm->start_brk, mm->brk, mm->start_stack,\n\t\tmm->arg_start, mm->arg_end, mm->env_start, mm->env_end,\n\t\tmm->binfmt, mm->flags, mm->core_state,\n#ifdef CONFIG_AIO\n\t\tmm->ioctx_table,\n#endif\n#ifdef CONFIG_MEMCG\n\t\tmm->owner,\n#endif\n\t\tmm->exe_file,\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tmm->mmu_notifier_mm,\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\tmm->numa_next_scan, mm->numa_scan_offset, mm->numa_scan_seq,\n#endif\n\t\tatomic_read(&mm->tlb_flush_pending),\n\t\tmm->def_flags, &mm->def_flags\n\t);\n}",
        "code_after_change": "void dump_mm(const struct mm_struct *mm)\n{\n\tpr_emerg(\"mm %px mmap %px seqnum %llu task_size %lu\\n\"\n#ifdef CONFIG_MMU\n\t\t\"get_unmapped_area %px\\n\"\n#endif\n\t\t\"mmap_base %lu mmap_legacy_base %lu highest_vm_end %lu\\n\"\n\t\t\"pgd %px mm_users %d mm_count %d pgtables_bytes %lu map_count %d\\n\"\n\t\t\"hiwater_rss %lx hiwater_vm %lx total_vm %lx locked_vm %lx\\n\"\n\t\t\"pinned_vm %lx data_vm %lx exec_vm %lx stack_vm %lx\\n\"\n\t\t\"start_code %lx end_code %lx start_data %lx end_data %lx\\n\"\n\t\t\"start_brk %lx brk %lx start_stack %lx\\n\"\n\t\t\"arg_start %lx arg_end %lx env_start %lx env_end %lx\\n\"\n\t\t\"binfmt %px flags %lx core_state %px\\n\"\n#ifdef CONFIG_AIO\n\t\t\"ioctx_table %px\\n\"\n#endif\n#ifdef CONFIG_MEMCG\n\t\t\"owner %px \"\n#endif\n\t\t\"exe_file %px\\n\"\n#ifdef CONFIG_MMU_NOTIFIER\n\t\t\"mmu_notifier_mm %px\\n\"\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\t\"numa_next_scan %lu numa_scan_offset %lu numa_scan_seq %d\\n\"\n#endif\n\t\t\"tlb_flush_pending %d\\n\"\n\t\t\"def_flags: %#lx(%pGv)\\n\",\n\n\t\tmm, mm->mmap, (long long) mm->vmacache_seqnum, mm->task_size,\n#ifdef CONFIG_MMU\n\t\tmm->get_unmapped_area,\n#endif\n\t\tmm->mmap_base, mm->mmap_legacy_base, mm->highest_vm_end,\n\t\tmm->pgd, atomic_read(&mm->mm_users),\n\t\tatomic_read(&mm->mm_count),\n\t\tmm_pgtables_bytes(mm),\n\t\tmm->map_count,\n\t\tmm->hiwater_rss, mm->hiwater_vm, mm->total_vm, mm->locked_vm,\n\t\tmm->pinned_vm, mm->data_vm, mm->exec_vm, mm->stack_vm,\n\t\tmm->start_code, mm->end_code, mm->start_data, mm->end_data,\n\t\tmm->start_brk, mm->brk, mm->start_stack,\n\t\tmm->arg_start, mm->arg_end, mm->env_start, mm->env_end,\n\t\tmm->binfmt, mm->flags, mm->core_state,\n#ifdef CONFIG_AIO\n\t\tmm->ioctx_table,\n#endif\n#ifdef CONFIG_MEMCG\n\t\tmm->owner,\n#endif\n\t\tmm->exe_file,\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tmm->mmu_notifier_mm,\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\tmm->numa_next_scan, mm->numa_scan_offset, mm->numa_scan_seq,\n#endif\n\t\tatomic_read(&mm->tlb_flush_pending),\n\t\tmm->def_flags, &mm->def_flags\n\t);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n void dump_mm(const struct mm_struct *mm)\n {\n-\tpr_emerg(\"mm %px mmap %px seqnum %d task_size %lu\\n\"\n+\tpr_emerg(\"mm %px mmap %px seqnum %llu task_size %lu\\n\"\n #ifdef CONFIG_MMU\n \t\t\"get_unmapped_area %px\\n\"\n #endif\n@@ -28,7 +28,7 @@\n \t\t\"tlb_flush_pending %d\\n\"\n \t\t\"def_flags: %#lx(%pGv)\\n\",\n \n-\t\tmm, mm->mmap, mm->vmacache_seqnum, mm->task_size,\n+\t\tmm, mm->mmap, (long long) mm->vmacache_seqnum, mm->task_size,\n #ifdef CONFIG_MMU\n \t\tmm->get_unmapped_area,\n #endif",
        "function_modified_lines": {
            "added": [
                "\tpr_emerg(\"mm %px mmap %px seqnum %llu task_size %lu\\n\"",
                "\t\tmm, mm->mmap, (long long) mm->vmacache_seqnum, mm->task_size,"
            ],
            "deleted": [
                "\tpr_emerg(\"mm %px mmap %px seqnum %d task_size %lu\\n\"",
                "\t\tmm, mm->mmap, mm->vmacache_seqnum, mm->task_size,"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 4.18.8. The vmacache_flush_all function in mm/vmacache.c mishandles sequence number overflows. An attacker can trigger a use-after-free (and possibly gain privileges) via certain thread creation, map, unmap, invalidation, and dereference operations.",
        "id": 1727
    },
    {
        "cve_id": "CVE-2017-16648",
        "code_before_change": "static void __dvb_frontend_free(struct dvb_frontend *fe)\n{\n\tstruct dvb_frontend_private *fepriv = fe->frontend_priv;\n\n\tif (fepriv)\n\t\tdvb_free_device(fepriv->dvbdev);\n\n\tdvb_frontend_invoke_release(fe, fe->ops.release);\n\n\tif (!fepriv)\n\t\treturn;\n\n\tkfree(fepriv);\n\tfe->frontend_priv = NULL;\n}",
        "code_after_change": "static void __dvb_frontend_free(struct dvb_frontend *fe)\n{\n\tstruct dvb_frontend_private *fepriv = fe->frontend_priv;\n\n\tif (fepriv)\n\t\tdvb_free_device(fepriv->dvbdev);\n\n\tdvb_frontend_invoke_release(fe, fe->ops.release);\n\n\tif (fepriv)\n\t\tkfree(fepriv);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,9 +7,6 @@\n \n \tdvb_frontend_invoke_release(fe, fe->ops.release);\n \n-\tif (!fepriv)\n-\t\treturn;\n-\n-\tkfree(fepriv);\n-\tfe->frontend_priv = NULL;\n+\tif (fepriv)\n+\t\tkfree(fepriv);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (fepriv)",
                "\t\tkfree(fepriv);"
            ],
            "deleted": [
                "\tif (!fepriv)",
                "\t\treturn;",
                "",
                "\tkfree(fepriv);",
                "\tfe->frontend_priv = NULL;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The dvb_frontend_free function in drivers/media/dvb-core/dvb_frontend.c in the Linux kernel through 4.13.11 allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via a crafted USB device. NOTE: the function was later renamed __dvb_frontend_free.",
        "id": 1342
    },
    {
        "cve_id": "CVE-2016-8655",
        "code_before_change": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n\tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n\t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n\t\tgoto out;\n\t}\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    (int)(req->tp_block_size -\n\t\t\t  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t/* Transmit path is not supported. We checked\n\t\t * it above but just being paranoid\n\t\t */\n\t\t\tif (!tx_ring)\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (closing && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\trelease_sock(sk);\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\treturn err;\n}",
        "code_after_change": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\tlock_sock(sk);\n\t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n\tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n\t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n\t\tgoto out;\n\t}\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    (int)(req->tp_block_size -\n\t\t\t  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t/* Transmit path is not supported. We checked\n\t\t * it above but just being paranoid\n\t\t */\n\t\t\tif (!tx_ring)\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (closing && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,7 @@\n \t/* Added to avoid minimal code churn */\n \tstruct tpacket_req *req = &req_u->req;\n \n+\tlock_sock(sk);\n \t/* Opening a Tx-ring is NOT supported in TPACKET_V3 */\n \tif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\n \t\tnet_warn_ratelimited(\"Tx-ring is not supported.\\n\");\n@@ -92,7 +93,6 @@\n \t\t\tgoto out;\n \t}\n \n-\tlock_sock(sk);\n \n \t/* Detach socket from network */\n \tspin_lock(&po->bind_lock);\n@@ -141,10 +141,10 @@\n \t\tif (!tx_ring)\n \t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n \t}\n-\trelease_sock(sk);\n \n \tif (pg_vec)\n \t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\n out:\n+\trelease_sock(sk);\n \treturn err;\n }",
        "function_modified_lines": {
            "added": [
                "\tlock_sock(sk);",
                "\trelease_sock(sk);"
            ],
            "deleted": [
                "\tlock_sock(sk);",
                "\trelease_sock(sk);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in net/packet/af_packet.c in the Linux kernel through 4.8.12 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging the CAP_NET_RAW capability to change a socket version, related to the packet_set_ring and packet_setsockopt functions.",
        "id": 1131
    },
    {
        "cve_id": "CVE-2022-2938",
        "code_before_change": "__poll_t psi_trigger_poll(void **trigger_ptr,\n\t\t\t\tstruct file *file, poll_table *wait)\n{\n\t__poll_t ret = DEFAULT_POLLMASK;\n\tstruct psi_trigger *t;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n\n\trcu_read_lock();\n\n\tt = rcu_dereference(*(void __rcu __force **)trigger_ptr);\n\tif (!t) {\n\t\trcu_read_unlock();\n\t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n\t}\n\tkref_get(&t->refcount);\n\n\trcu_read_unlock();\n\n\tpoll_wait(file, &t->event_wait, wait);\n\n\tif (cmpxchg(&t->event, 1, 0) == 1)\n\t\tret |= EPOLLPRI;\n\n\tkref_put(&t->refcount, psi_trigger_destroy);\n\n\treturn ret;\n}",
        "code_after_change": "__poll_t psi_trigger_poll(void **trigger_ptr,\n\t\t\t\tstruct file *file, poll_table *wait)\n{\n\t__poll_t ret = DEFAULT_POLLMASK;\n\tstruct psi_trigger *t;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n\n\tt = smp_load_acquire(trigger_ptr);\n\tif (!t)\n\t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n\n\tpoll_wait(file, &t->event_wait, wait);\n\n\tif (cmpxchg(&t->event, 1, 0) == 1)\n\t\tret |= EPOLLPRI;\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,23 +7,14 @@\n \tif (static_branch_likely(&psi_disabled))\n \t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n \n-\trcu_read_lock();\n-\n-\tt = rcu_dereference(*(void __rcu __force **)trigger_ptr);\n-\tif (!t) {\n-\t\trcu_read_unlock();\n+\tt = smp_load_acquire(trigger_ptr);\n+\tif (!t)\n \t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n-\t}\n-\tkref_get(&t->refcount);\n-\n-\trcu_read_unlock();\n \n \tpoll_wait(file, &t->event_wait, wait);\n \n \tif (cmpxchg(&t->event, 1, 0) == 1)\n \t\tret |= EPOLLPRI;\n \n-\tkref_put(&t->refcount, psi_trigger_destroy);\n-\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tt = smp_load_acquire(trigger_ptr);",
                "\tif (!t)"
            ],
            "deleted": [
                "\trcu_read_lock();",
                "",
                "\tt = rcu_dereference(*(void __rcu __force **)trigger_ptr);",
                "\tif (!t) {",
                "\t\trcu_read_unlock();",
                "\t}",
                "\tkref_get(&t->refcount);",
                "",
                "\trcu_read_unlock();",
                "\tkref_put(&t->refcount, psi_trigger_destroy);",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's implementation of Pressure Stall Information. While the feature is disabled by default, it could allow an attacker to crash the system or have other memory-corruption side effects.",
        "id": 3519
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "void hfi1_mmu_rb_remove(struct mmu_rb_handler *handler,\n\t\t\tstruct mmu_rb_node *node)\n{\n\tunsigned long flags;\n\n\t/* Validity of handler and node pointers has been checked by caller. */\n\ttrace_hfi1_mmu_rb_remove(node->addr, node->len);\n\tspin_lock_irqsave(&handler->lock, flags);\n\t__mmu_int_rb_remove(node, &handler->root);\n\tlist_del(&node->list); /* remove from LRU list */\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\thandler->ops->remove(handler->ops_arg, node);\n}",
        "code_after_change": "void hfi1_mmu_rb_remove(struct mmu_rb_handler *handler,\n\t\t\tstruct mmu_rb_node *node)\n{\n\tunsigned long flags;\n\n\tif (current->mm != handler->mn.mm)\n\t\treturn;\n\n\t/* Validity of handler and node pointers has been checked by caller. */\n\ttrace_hfi1_mmu_rb_remove(node->addr, node->len);\n\tspin_lock_irqsave(&handler->lock, flags);\n\t__mmu_int_rb_remove(node, &handler->root);\n\tlist_del(&node->list); /* remove from LRU list */\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\thandler->ops->remove(handler->ops_arg, node);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,9 @@\n \t\t\tstruct mmu_rb_node *node)\n {\n \tunsigned long flags;\n+\n+\tif (current->mm != handler->mn.mm)\n+\t\treturn;\n \n \t/* Validity of handler and node pointers has been checked by caller. */\n \ttrace_hfi1_mmu_rb_remove(node->addr, node->len);",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (current->mm != handler->mn.mm)",
                "\t\treturn;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2646
    },
    {
        "cve_id": "CVE-2023-33288",
        "code_before_change": "static void bq24190_remove(struct i2c_client *client)\n{\n\tstruct bq24190_dev_info *bdi = i2c_get_clientdata(client);\n\tint error;\n\n\terror = pm_runtime_resume_and_get(bdi->dev);\n\tif (error < 0)\n\t\tdev_warn(bdi->dev, \"pm_runtime_get failed: %i\\n\", error);\n\n\tbq24190_register_reset(bdi);\n\tif (bdi->battery)\n\t\tpower_supply_unregister(bdi->battery);\n\tpower_supply_unregister(bdi->charger);\n\tif (error >= 0)\n\t\tpm_runtime_put_sync(bdi->dev);\n\tpm_runtime_dont_use_autosuspend(bdi->dev);\n\tpm_runtime_disable(bdi->dev);\n}",
        "code_after_change": "static void bq24190_remove(struct i2c_client *client)\n{\n\tstruct bq24190_dev_info *bdi = i2c_get_clientdata(client);\n\tint error;\n\n\tcancel_delayed_work_sync(&bdi->input_current_limit_work);\n\terror = pm_runtime_resume_and_get(bdi->dev);\n\tif (error < 0)\n\t\tdev_warn(bdi->dev, \"pm_runtime_get failed: %i\\n\", error);\n\n\tbq24190_register_reset(bdi);\n\tif (bdi->battery)\n\t\tpower_supply_unregister(bdi->battery);\n\tpower_supply_unregister(bdi->charger);\n\tif (error >= 0)\n\t\tpm_runtime_put_sync(bdi->dev);\n\tpm_runtime_dont_use_autosuspend(bdi->dev);\n\tpm_runtime_disable(bdi->dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n \tstruct bq24190_dev_info *bdi = i2c_get_clientdata(client);\n \tint error;\n \n+\tcancel_delayed_work_sync(&bdi->input_current_limit_work);\n \terror = pm_runtime_resume_and_get(bdi->dev);\n \tif (error < 0)\n \t\tdev_warn(bdi->dev, \"pm_runtime_get failed: %i\\n\", error);",
        "function_modified_lines": {
            "added": [
                "\tcancel_delayed_work_sync(&bdi->input_current_limit_work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.2.9. A use-after-free was found in bq24190_remove in drivers/power/supply/bq24190_charger.c. It could allow a local attacker to crash the system due to a race condition.",
        "id": 4059
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "int vmw_gem_object_create_with_handle(struct vmw_private *dev_priv,\n\t\t\t\t      struct drm_file *filp,\n\t\t\t\t      uint32_t size,\n\t\t\t\t      uint32_t *handle,\n\t\t\t\t      struct vmw_bo **p_vbo)\n{\n\tint ret;\n\tstruct vmw_bo_params params = {\n\t\t.domain = (dev_priv->has_mob) ? VMW_BO_DOMAIN_SYS : VMW_BO_DOMAIN_VRAM,\n\t\t.busy_domain = VMW_BO_DOMAIN_SYS,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = size,\n\t\t.pin = false\n\t};\n\n\tret = vmw_bo_create(dev_priv, &params, p_vbo);\n\tif (ret != 0)\n\t\tgoto out_no_bo;\n\n\t(*p_vbo)->tbo.base.funcs = &vmw_gem_object_funcs;\n\n\tret = drm_gem_handle_create(filp, &(*p_vbo)->tbo.base, handle);\nout_no_bo:\n\treturn ret;\n}",
        "code_after_change": "int vmw_gem_object_create_with_handle(struct vmw_private *dev_priv,\n\t\t\t\t      struct drm_file *filp,\n\t\t\t\t      uint32_t size,\n\t\t\t\t      uint32_t *handle,\n\t\t\t\t      struct vmw_bo **p_vbo)\n{\n\tint ret;\n\tstruct vmw_bo_params params = {\n\t\t.domain = (dev_priv->has_mob) ? VMW_BO_DOMAIN_SYS : VMW_BO_DOMAIN_VRAM,\n\t\t.busy_domain = VMW_BO_DOMAIN_SYS,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = size,\n\t\t.pin = false\n\t};\n\n\tret = vmw_gem_object_create(dev_priv, &params, p_vbo);\n\tif (ret != 0)\n\t\tgoto out_no_bo;\n\n\tret = drm_gem_handle_create(filp, &(*p_vbo)->tbo.base, handle);\nout_no_bo:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,11 +13,9 @@\n \t\t.pin = false\n \t};\n \n-\tret = vmw_bo_create(dev_priv, &params, p_vbo);\n+\tret = vmw_gem_object_create(dev_priv, &params, p_vbo);\n \tif (ret != 0)\n \t\tgoto out_no_bo;\n-\n-\t(*p_vbo)->tbo.base.funcs = &vmw_gem_object_funcs;\n \n \tret = drm_gem_handle_create(filp, &(*p_vbo)->tbo.base, handle);\n out_no_bo:",
        "function_modified_lines": {
            "added": [
                "\tret = vmw_gem_object_create(dev_priv, &params, p_vbo);"
            ],
            "deleted": [
                "\tret = vmw_bo_create(dev_priv, &params, p_vbo);",
                "",
                "\t(*p_vbo)->tbo.base.funcs = &vmw_gem_object_funcs;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4274
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static void __io_queue_sqe(struct io_kiocb *req)\n{\n\tstruct io_kiocb *linked_timeout = io_prep_linked_timeout(req);\n\tconst struct cred *old_creds = NULL;\n\tint ret;\n\n\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&\n\t    req->work.identity->creds != current_cred())\n\t\told_creds = override_creds(req->work.identity->creds);\n\n\tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\n\tif (old_creds)\n\t\trevert_creds(old_creds);\n\n\t/*\n\t * We async punt it if the file wasn't marked NOWAIT, or if the file\n\t * doesn't support non-blocking read/write attempts\n\t */\n\tif (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {\n\t\tif (!io_arm_poll_handler(req)) {\n\t\t\t/*\n\t\t\t * Queued up for async execution, worker will release\n\t\t\t * submit reference when the iocb is actually submitted.\n\t\t\t */\n\t\t\tio_queue_async_work(req);\n\t\t}\n\t} else if (likely(!ret)) {\n\t\t/* drop submission reference */\n\t\tif (req->flags & REQ_F_COMPLETE_INLINE) {\n\t\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\t\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\t\t\tcs->reqs[cs->nr++] = req;\n\t\t\tif (cs->nr == ARRAY_SIZE(cs->reqs))\n\t\t\t\tio_submit_flush_completions(cs, ctx);\n\t\t} else {\n\t\t\tio_put_req(req);\n\t\t}\n\t} else {\n\t\treq_set_fail_links(req);\n\t\tio_put_req(req);\n\t\tio_req_complete(req, ret);\n\t}\n\tif (linked_timeout)\n\t\tio_queue_linked_timeout(linked_timeout);\n}",
        "code_after_change": "static void __io_queue_sqe(struct io_kiocb *req)\n{\n\tstruct io_kiocb *linked_timeout = io_prep_linked_timeout(req);\n\tconst struct cred *old_creds = NULL;\n\tint ret;\n\n\tif ((req->flags & REQ_F_WORK_INITIALIZED) && req->work.creds &&\n\t    req->work.creds != current_cred())\n\t\told_creds = override_creds(req->work.creds);\n\n\tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\n\tif (old_creds)\n\t\trevert_creds(old_creds);\n\n\t/*\n\t * We async punt it if the file wasn't marked NOWAIT, or if the file\n\t * doesn't support non-blocking read/write attempts\n\t */\n\tif (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {\n\t\tif (!io_arm_poll_handler(req)) {\n\t\t\t/*\n\t\t\t * Queued up for async execution, worker will release\n\t\t\t * submit reference when the iocb is actually submitted.\n\t\t\t */\n\t\t\tio_queue_async_work(req);\n\t\t}\n\t} else if (likely(!ret)) {\n\t\t/* drop submission reference */\n\t\tif (req->flags & REQ_F_COMPLETE_INLINE) {\n\t\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\t\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\t\t\tcs->reqs[cs->nr++] = req;\n\t\t\tif (cs->nr == ARRAY_SIZE(cs->reqs))\n\t\t\t\tio_submit_flush_completions(cs, ctx);\n\t\t} else {\n\t\t\tio_put_req(req);\n\t\t}\n\t} else {\n\t\treq_set_fail_links(req);\n\t\tio_put_req(req);\n\t\tio_req_complete(req, ret);\n\t}\n\tif (linked_timeout)\n\t\tio_queue_linked_timeout(linked_timeout);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,9 +4,9 @@\n \tconst struct cred *old_creds = NULL;\n \tint ret;\n \n-\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&\n-\t    req->work.identity->creds != current_cred())\n-\t\told_creds = override_creds(req->work.identity->creds);\n+\tif ((req->flags & REQ_F_WORK_INITIALIZED) && req->work.creds &&\n+\t    req->work.creds != current_cred())\n+\t\told_creds = override_creds(req->work.creds);\n \n \tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n ",
        "function_modified_lines": {
            "added": [
                "\tif ((req->flags & REQ_F_WORK_INITIALIZED) && req->work.creds &&",
                "\t    req->work.creds != current_cred())",
                "\t\told_creds = override_creds(req->work.creds);"
            ],
            "deleted": [
                "\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&",
                "\t    req->work.identity->creds != current_cred())",
                "\t\told_creds = override_creds(req->work.identity->creds);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3362
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "static int vmw_cmd_dx_bind_shader(struct vmw_private *dev_priv,\n\t\t\t\t  struct vmw_sw_context *sw_context,\n\t\t\t\t  SVGA3dCmdHeader *header)\n{\n\tstruct vmw_resource *ctx;\n\tstruct vmw_resource *res;\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXBindShader) =\n\t\tcontainer_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (cmd->body.cid != SVGA3D_INVALID_ID) {\n\t\tret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\n\t\t\t\t\tVMW_RES_DIRTY_SET,\n\t\t\t\t\tuser_context_converter, &cmd->body.cid,\n\t\t\t\t\t&ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tstruct vmw_ctx_validation_info *ctx_node =\n\t\t\tVMW_GET_CTX_NODE(sw_context);\n\n\t\tif (!ctx_node)\n\t\t\treturn -EINVAL;\n\n\t\tctx = ctx_node->ctx;\n\t}\n\n\tres = vmw_shader_lookup(vmw_context_res_man(ctx), cmd->body.shid, 0);\n\tif (IS_ERR(res)) {\n\t\tVMW_DEBUG_USER(\"Could not find shader to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\tif (ret) {\n\t\tVMW_DEBUG_USER(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
        "code_after_change": "static int vmw_cmd_dx_bind_shader(struct vmw_private *dev_priv,\n\t\t\t\t  struct vmw_sw_context *sw_context,\n\t\t\t\t  SVGA3dCmdHeader *header)\n{\n\tstruct vmw_resource *ctx;\n\tstruct vmw_resource *res;\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXBindShader) =\n\t\tcontainer_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (cmd->body.cid != SVGA3D_INVALID_ID) {\n\t\tret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\n\t\t\t\t\tVMW_RES_DIRTY_SET,\n\t\t\t\t\tuser_context_converter, &cmd->body.cid,\n\t\t\t\t\t&ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tstruct vmw_ctx_validation_info *ctx_node =\n\t\t\tVMW_GET_CTX_NODE(sw_context);\n\n\t\tif (!ctx_node)\n\t\t\treturn -EINVAL;\n\n\t\tctx = ctx_node->ctx;\n\t}\n\n\tres = vmw_shader_lookup(vmw_context_res_man(ctx), cmd->body.shid, 0);\n\tif (IS_ERR(res)) {\n\t\tVMW_DEBUG_USER(\"Could not find shader to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_NONE,\n\t\t\t\t      vmw_val_add_flag_noctx);\n\tif (ret) {\n\t\tVMW_DEBUG_USER(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,8 +31,8 @@\n \t\treturn PTR_ERR(res);\n \t}\n \n-\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n-\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n+\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_NONE,\n+\t\t\t\t      vmw_val_add_flag_noctx);\n \tif (ret) {\n \t\tVMW_DEBUG_USER(\"Error creating resource validation node.\\n\");\n \t\treturn ret;",
        "function_modified_lines": {
            "added": [
                "\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_NONE,",
                "\t\t\t\t      vmw_val_add_flag_noctx);"
            ],
            "deleted": [
                "\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,",
                "\t\t\t\t\t    VMW_RES_DIRTY_NONE);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3682
    },
    {
        "cve_id": "CVE-2020-8428",
        "code_before_change": "static int do_last(struct nameidata *nd,\n\t\t   struct file *file, const struct open_flags *op)\n{\n\tstruct dentry *dir = nd->path.dentry;\n\tint open_flag = op->open_flag;\n\tbool will_truncate = (open_flag & O_TRUNC) != 0;\n\tbool got_write = false;\n\tint acc_mode = op->acc_mode;\n\tunsigned seq;\n\tstruct inode *inode;\n\tstruct path path;\n\tint error;\n\n\tnd->flags &= ~LOOKUP_PARENT;\n\tnd->flags |= op->intent;\n\n\tif (nd->last_type != LAST_NORM) {\n\t\terror = handle_dots(nd, nd->last_type);\n\t\tif (unlikely(error))\n\t\t\treturn error;\n\t\tgoto finish_open;\n\t}\n\n\tif (!(open_flag & O_CREAT)) {\n\t\tif (nd->last.name[nd->last.len])\n\t\t\tnd->flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;\n\t\t/* we _can_ be in RCU mode here */\n\t\terror = lookup_fast(nd, &path, &inode, &seq);\n\t\tif (likely(error > 0))\n\t\t\tgoto finish_lookup;\n\n\t\tif (error < 0)\n\t\t\treturn error;\n\n\t\tBUG_ON(nd->inode != dir->d_inode);\n\t\tBUG_ON(nd->flags & LOOKUP_RCU);\n\t} else {\n\t\t/* create side of things */\n\t\t/*\n\t\t * This will *only* deal with leaving RCU mode - LOOKUP_JUMPED\n\t\t * has been cleared when we got to the last component we are\n\t\t * about to look up\n\t\t */\n\t\terror = complete_walk(nd);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\taudit_inode(nd->name, dir, AUDIT_INODE_PARENT);\n\t\t/* trailing slashes? */\n\t\tif (unlikely(nd->last.name[nd->last.len]))\n\t\t\treturn -EISDIR;\n\t}\n\n\tif (open_flag & (O_CREAT | O_TRUNC | O_WRONLY | O_RDWR)) {\n\t\terror = mnt_want_write(nd->path.mnt);\n\t\tif (!error)\n\t\t\tgot_write = true;\n\t\t/*\n\t\t * do _not_ fail yet - we might not need that or fail with\n\t\t * a different error; let lookup_open() decide; we'll be\n\t\t * dropping this one anyway.\n\t\t */\n\t}\n\tif (open_flag & O_CREAT)\n\t\tinode_lock(dir->d_inode);\n\telse\n\t\tinode_lock_shared(dir->d_inode);\n\terror = lookup_open(nd, &path, file, op, got_write);\n\tif (open_flag & O_CREAT)\n\t\tinode_unlock(dir->d_inode);\n\telse\n\t\tinode_unlock_shared(dir->d_inode);\n\n\tif (error)\n\t\tgoto out;\n\n\tif (file->f_mode & FMODE_OPENED) {\n\t\tif ((file->f_mode & FMODE_CREATED) ||\n\t\t    !S_ISREG(file_inode(file)->i_mode))\n\t\t\twill_truncate = false;\n\n\t\taudit_inode(nd->name, file->f_path.dentry, 0);\n\t\tgoto opened;\n\t}\n\n\tif (file->f_mode & FMODE_CREATED) {\n\t\t/* Don't check for write permission, don't truncate */\n\t\topen_flag &= ~O_TRUNC;\n\t\twill_truncate = false;\n\t\tacc_mode = 0;\n\t\tpath_to_nameidata(&path, nd);\n\t\tgoto finish_open_created;\n\t}\n\n\t/*\n\t * If atomic_open() acquired write access it is dropped now due to\n\t * possible mount and symlink following (this might be optimized away if\n\t * necessary...)\n\t */\n\tif (got_write) {\n\t\tmnt_drop_write(nd->path.mnt);\n\t\tgot_write = false;\n\t}\n\n\terror = follow_managed(&path, nd);\n\tif (unlikely(error < 0))\n\t\treturn error;\n\n\t/*\n\t * create/update audit record if it already exists.\n\t */\n\taudit_inode(nd->name, path.dentry, 0);\n\n\tif (unlikely((open_flag & (O_EXCL | O_CREAT)) == (O_EXCL | O_CREAT))) {\n\t\tpath_to_nameidata(&path, nd);\n\t\treturn -EEXIST;\n\t}\n\n\tseq = 0;\t/* out of RCU mode, so the value doesn't matter */\n\tinode = d_backing_inode(path.dentry);\nfinish_lookup:\n\terror = step_into(nd, &path, 0, inode, seq);\n\tif (unlikely(error))\n\t\treturn error;\nfinish_open:\n\t/* Why this, you ask?  _Now_ we might have grown LOOKUP_JUMPED... */\n\terror = complete_walk(nd);\n\tif (error)\n\t\treturn error;\n\taudit_inode(nd->name, nd->path.dentry, 0);\n\tif (open_flag & O_CREAT) {\n\t\terror = -EISDIR;\n\t\tif (d_is_dir(nd->path.dentry))\n\t\t\tgoto out;\n\t\terror = may_create_in_sticky(dir,\n\t\t\t\t\t     d_backing_inode(nd->path.dentry));\n\t\tif (unlikely(error))\n\t\t\tgoto out;\n\t}\n\terror = -ENOTDIR;\n\tif ((nd->flags & LOOKUP_DIRECTORY) && !d_can_lookup(nd->path.dentry))\n\t\tgoto out;\n\tif (!d_is_reg(nd->path.dentry))\n\t\twill_truncate = false;\n\n\tif (will_truncate) {\n\t\terror = mnt_want_write(nd->path.mnt);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tgot_write = true;\n\t}\nfinish_open_created:\n\terror = may_open(&nd->path, acc_mode, open_flag);\n\tif (error)\n\t\tgoto out;\n\tBUG_ON(file->f_mode & FMODE_OPENED); /* once it's opened, it's opened */\n\terror = vfs_open(&nd->path, file);\n\tif (error)\n\t\tgoto out;\nopened:\n\terror = ima_file_check(file, op->acc_mode);\n\tif (!error && will_truncate)\n\t\terror = handle_truncate(file);\nout:\n\tif (unlikely(error > 0)) {\n\t\tWARN_ON(1);\n\t\terror = -EINVAL;\n\t}\n\tif (got_write)\n\t\tmnt_drop_write(nd->path.mnt);\n\treturn error;\n}",
        "code_after_change": "static int do_last(struct nameidata *nd,\n\t\t   struct file *file, const struct open_flags *op)\n{\n\tstruct dentry *dir = nd->path.dentry;\n\tkuid_t dir_uid = dir->d_inode->i_uid;\n\tumode_t dir_mode = dir->d_inode->i_mode;\n\tint open_flag = op->open_flag;\n\tbool will_truncate = (open_flag & O_TRUNC) != 0;\n\tbool got_write = false;\n\tint acc_mode = op->acc_mode;\n\tunsigned seq;\n\tstruct inode *inode;\n\tstruct path path;\n\tint error;\n\n\tnd->flags &= ~LOOKUP_PARENT;\n\tnd->flags |= op->intent;\n\n\tif (nd->last_type != LAST_NORM) {\n\t\terror = handle_dots(nd, nd->last_type);\n\t\tif (unlikely(error))\n\t\t\treturn error;\n\t\tgoto finish_open;\n\t}\n\n\tif (!(open_flag & O_CREAT)) {\n\t\tif (nd->last.name[nd->last.len])\n\t\t\tnd->flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;\n\t\t/* we _can_ be in RCU mode here */\n\t\terror = lookup_fast(nd, &path, &inode, &seq);\n\t\tif (likely(error > 0))\n\t\t\tgoto finish_lookup;\n\n\t\tif (error < 0)\n\t\t\treturn error;\n\n\t\tBUG_ON(nd->inode != dir->d_inode);\n\t\tBUG_ON(nd->flags & LOOKUP_RCU);\n\t} else {\n\t\t/* create side of things */\n\t\t/*\n\t\t * This will *only* deal with leaving RCU mode - LOOKUP_JUMPED\n\t\t * has been cleared when we got to the last component we are\n\t\t * about to look up\n\t\t */\n\t\terror = complete_walk(nd);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\taudit_inode(nd->name, dir, AUDIT_INODE_PARENT);\n\t\t/* trailing slashes? */\n\t\tif (unlikely(nd->last.name[nd->last.len]))\n\t\t\treturn -EISDIR;\n\t}\n\n\tif (open_flag & (O_CREAT | O_TRUNC | O_WRONLY | O_RDWR)) {\n\t\terror = mnt_want_write(nd->path.mnt);\n\t\tif (!error)\n\t\t\tgot_write = true;\n\t\t/*\n\t\t * do _not_ fail yet - we might not need that or fail with\n\t\t * a different error; let lookup_open() decide; we'll be\n\t\t * dropping this one anyway.\n\t\t */\n\t}\n\tif (open_flag & O_CREAT)\n\t\tinode_lock(dir->d_inode);\n\telse\n\t\tinode_lock_shared(dir->d_inode);\n\terror = lookup_open(nd, &path, file, op, got_write);\n\tif (open_flag & O_CREAT)\n\t\tinode_unlock(dir->d_inode);\n\telse\n\t\tinode_unlock_shared(dir->d_inode);\n\n\tif (error)\n\t\tgoto out;\n\n\tif (file->f_mode & FMODE_OPENED) {\n\t\tif ((file->f_mode & FMODE_CREATED) ||\n\t\t    !S_ISREG(file_inode(file)->i_mode))\n\t\t\twill_truncate = false;\n\n\t\taudit_inode(nd->name, file->f_path.dentry, 0);\n\t\tgoto opened;\n\t}\n\n\tif (file->f_mode & FMODE_CREATED) {\n\t\t/* Don't check for write permission, don't truncate */\n\t\topen_flag &= ~O_TRUNC;\n\t\twill_truncate = false;\n\t\tacc_mode = 0;\n\t\tpath_to_nameidata(&path, nd);\n\t\tgoto finish_open_created;\n\t}\n\n\t/*\n\t * If atomic_open() acquired write access it is dropped now due to\n\t * possible mount and symlink following (this might be optimized away if\n\t * necessary...)\n\t */\n\tif (got_write) {\n\t\tmnt_drop_write(nd->path.mnt);\n\t\tgot_write = false;\n\t}\n\n\terror = follow_managed(&path, nd);\n\tif (unlikely(error < 0))\n\t\treturn error;\n\n\t/*\n\t * create/update audit record if it already exists.\n\t */\n\taudit_inode(nd->name, path.dentry, 0);\n\n\tif (unlikely((open_flag & (O_EXCL | O_CREAT)) == (O_EXCL | O_CREAT))) {\n\t\tpath_to_nameidata(&path, nd);\n\t\treturn -EEXIST;\n\t}\n\n\tseq = 0;\t/* out of RCU mode, so the value doesn't matter */\n\tinode = d_backing_inode(path.dentry);\nfinish_lookup:\n\terror = step_into(nd, &path, 0, inode, seq);\n\tif (unlikely(error))\n\t\treturn error;\nfinish_open:\n\t/* Why this, you ask?  _Now_ we might have grown LOOKUP_JUMPED... */\n\terror = complete_walk(nd);\n\tif (error)\n\t\treturn error;\n\taudit_inode(nd->name, nd->path.dentry, 0);\n\tif (open_flag & O_CREAT) {\n\t\terror = -EISDIR;\n\t\tif (d_is_dir(nd->path.dentry))\n\t\t\tgoto out;\n\t\terror = may_create_in_sticky(dir_mode, dir_uid,\n\t\t\t\t\t     d_backing_inode(nd->path.dentry));\n\t\tif (unlikely(error))\n\t\t\tgoto out;\n\t}\n\terror = -ENOTDIR;\n\tif ((nd->flags & LOOKUP_DIRECTORY) && !d_can_lookup(nd->path.dentry))\n\t\tgoto out;\n\tif (!d_is_reg(nd->path.dentry))\n\t\twill_truncate = false;\n\n\tif (will_truncate) {\n\t\terror = mnt_want_write(nd->path.mnt);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tgot_write = true;\n\t}\nfinish_open_created:\n\terror = may_open(&nd->path, acc_mode, open_flag);\n\tif (error)\n\t\tgoto out;\n\tBUG_ON(file->f_mode & FMODE_OPENED); /* once it's opened, it's opened */\n\terror = vfs_open(&nd->path, file);\n\tif (error)\n\t\tgoto out;\nopened:\n\terror = ima_file_check(file, op->acc_mode);\n\tif (!error && will_truncate)\n\t\terror = handle_truncate(file);\nout:\n\tif (unlikely(error > 0)) {\n\t\tWARN_ON(1);\n\t\terror = -EINVAL;\n\t}\n\tif (got_write)\n\t\tmnt_drop_write(nd->path.mnt);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,8 @@\n \t\t   struct file *file, const struct open_flags *op)\n {\n \tstruct dentry *dir = nd->path.dentry;\n+\tkuid_t dir_uid = dir->d_inode->i_uid;\n+\tumode_t dir_mode = dir->d_inode->i_mode;\n \tint open_flag = op->open_flag;\n \tbool will_truncate = (open_flag & O_TRUNC) != 0;\n \tbool got_write = false;\n@@ -132,7 +134,7 @@\n \t\terror = -EISDIR;\n \t\tif (d_is_dir(nd->path.dentry))\n \t\t\tgoto out;\n-\t\terror = may_create_in_sticky(dir,\n+\t\terror = may_create_in_sticky(dir_mode, dir_uid,\n \t\t\t\t\t     d_backing_inode(nd->path.dentry));\n \t\tif (unlikely(error))\n \t\t\tgoto out;",
        "function_modified_lines": {
            "added": [
                "\tkuid_t dir_uid = dir->d_inode->i_uid;",
                "\tumode_t dir_mode = dir->d_inode->i_mode;",
                "\t\terror = may_create_in_sticky(dir_mode, dir_uid,"
            ],
            "deleted": [
                "\t\terror = may_create_in_sticky(dir,"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "fs/namei.c in the Linux kernel before 5.5 has a may_create_in_sticky use-after-free, which allows local users to cause a denial of service (OOPS) or possibly obtain sensitive information from kernel memory, aka CID-d0cb50185ae9. One attack vector may be an open system call for a UNIX domain socket, if the socket is being moved to a new parent directory and its old parent directory is being removed.",
        "id": 2804
    },
    {
        "cve_id": "CVE-2023-3610",
        "code_before_change": "static void nft_rule_expr_deactivate(const struct nft_ctx *ctx,\n\t\t\t\t     struct nft_rule *rule,\n\t\t\t\t     enum nft_trans_phase phase)\n{\n\tstruct nft_expr *expr;\n\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tif (expr->ops->deactivate)\n\t\t\texpr->ops->deactivate(ctx, expr, phase);\n\n\t\texpr = nft_expr_next(expr);\n\t}\n}",
        "code_after_change": "void nft_rule_expr_deactivate(const struct nft_ctx *ctx, struct nft_rule *rule,\n\t\t\t      enum nft_trans_phase phase)\n{\n\tstruct nft_expr *expr;\n\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tif (expr->ops->deactivate)\n\t\t\texpr->ops->deactivate(ctx, expr, phase);\n\n\t\texpr = nft_expr_next(expr);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,5 @@\n-static void nft_rule_expr_deactivate(const struct nft_ctx *ctx,\n-\t\t\t\t     struct nft_rule *rule,\n-\t\t\t\t     enum nft_trans_phase phase)\n+void nft_rule_expr_deactivate(const struct nft_ctx *ctx, struct nft_rule *rule,\n+\t\t\t      enum nft_trans_phase phase)\n {\n \tstruct nft_expr *expr;\n ",
        "function_modified_lines": {
            "added": [
                "void nft_rule_expr_deactivate(const struct nft_ctx *ctx, struct nft_rule *rule,",
                "\t\t\t      enum nft_trans_phase phase)"
            ],
            "deleted": [
                "static void nft_rule_expr_deactivate(const struct nft_ctx *ctx,",
                "\t\t\t\t     struct nft_rule *rule,",
                "\t\t\t\t     enum nft_trans_phase phase)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nFlaw in the error handling of bound chains causes a use-after-free in the abort path of NFT_MSG_NEWRULE. The vulnerability requires CAP_NET_ADMIN to be triggered.\n\nWe recommend upgrading past commit 4bedf9eee016286c835e3d8fa981ddece5338795.\n\n",
        "id": 4122
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int copy_entries_to_user(unsigned int total_size,\n\t\t\t\tconst struct xt_table *table,\n\t\t\t\tvoid __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct arpt_entry *e;\n\tstruct xt_counters *counters;\n\tstruct xt_table_info *private = table->private;\n\tint ret = 0;\n\tvoid *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct arpt_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tt = arpt_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
        "code_after_change": "static int copy_entries_to_user(unsigned int total_size,\n\t\t\t\tconst struct xt_table *table,\n\t\t\t\tvoid __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct arpt_entry *e;\n\tstruct xt_counters *counters;\n\tstruct xt_table_info *private = xt_table_get_private_protected(table);\n\tint ret = 0;\n\tvoid *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct arpt_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tt = arpt_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \tunsigned int off, num;\n \tconst struct arpt_entry *e;\n \tstruct xt_counters *counters;\n-\tstruct xt_table_info *private = table->private;\n+\tstruct xt_table_info *private = xt_table_get_private_protected(table);\n \tint ret = 0;\n \tvoid *loc_cpu_entry;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct xt_table_info *private = xt_table_get_private_protected(table);"
            ],
            "deleted": [
                "\tstruct xt_table_info *private = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2776
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int\ncompat_copy_entries_to_user(unsigned int total_size, struct xt_table *table,\n\t\t\t    void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct ip6t_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\n\tvfree(counters);\n\treturn ret;\n}",
        "code_after_change": "static int\ncompat_copy_entries_to_user(unsigned int total_size, struct xt_table *table,\n\t\t\t    void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct ip6t_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\n\tvfree(counters);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t\t\t    void __user *userptr)\n {\n \tstruct xt_counters *counters;\n-\tconst struct xt_table_info *private = table->private;\n+\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n \tvoid __user *pos;\n \tunsigned int size;\n \tint ret = 0;",
        "function_modified_lines": {
            "added": [
                "\tconst struct xt_table_info *private = xt_table_get_private_protected(table);"
            ],
            "deleted": [
                "\tconst struct xt_table_info *private = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2787
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static void vmw_resource_release(struct kref *kref)\n{\n\tstruct vmw_resource *res =\n\t    container_of(kref, struct vmw_resource, kref);\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tint id;\n\tint ret;\n\tstruct idr *idr = &dev_priv->res_idr[res->func->res_type];\n\n\tspin_lock(&dev_priv->resource_lock);\n\tlist_del_init(&res->lru_head);\n\tspin_unlock(&dev_priv->resource_lock);\n\tif (res->guest_memory_bo) {\n\t\tstruct ttm_buffer_object *bo = &res->guest_memory_bo->tbo;\n\n\t\tret = ttm_bo_reserve(bo, false, false, NULL);\n\t\tBUG_ON(ret);\n\t\tif (vmw_resource_mob_attached(res) &&\n\t\t    res->func->unbind != NULL) {\n\t\t\tstruct ttm_validate_buffer val_buf;\n\n\t\t\tval_buf.bo = bo;\n\t\t\tval_buf.num_shared = 0;\n\t\t\tres->func->unbind(res, false, &val_buf);\n\t\t}\n\t\tres->guest_memory_size = false;\n\t\tvmw_resource_mob_detach(res);\n\t\tif (res->dirty)\n\t\t\tres->func->dirty_free(res);\n\t\tif (res->coherent)\n\t\t\tvmw_bo_dirty_release(res->guest_memory_bo);\n\t\tttm_bo_unreserve(bo);\n\t\tvmw_bo_unreference(&res->guest_memory_bo);\n\t}\n\n\tif (likely(res->hw_destroy != NULL)) {\n\t\tmutex_lock(&dev_priv->binding_mutex);\n\t\tvmw_binding_res_list_kill(&res->binding_head);\n\t\tmutex_unlock(&dev_priv->binding_mutex);\n\t\tres->hw_destroy(res);\n\t}\n\n\tid = res->id;\n\tif (res->res_free != NULL)\n\t\tres->res_free(res);\n\telse\n\t\tkfree(res);\n\n\tspin_lock(&dev_priv->resource_lock);\n\tif (id != -1)\n\t\tidr_remove(idr, id);\n\tspin_unlock(&dev_priv->resource_lock);\n}",
        "code_after_change": "static void vmw_resource_release(struct kref *kref)\n{\n\tstruct vmw_resource *res =\n\t    container_of(kref, struct vmw_resource, kref);\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tint id;\n\tint ret;\n\tstruct idr *idr = &dev_priv->res_idr[res->func->res_type];\n\n\tspin_lock(&dev_priv->resource_lock);\n\tlist_del_init(&res->lru_head);\n\tspin_unlock(&dev_priv->resource_lock);\n\tif (res->guest_memory_bo) {\n\t\tstruct ttm_buffer_object *bo = &res->guest_memory_bo->tbo;\n\n\t\tret = ttm_bo_reserve(bo, false, false, NULL);\n\t\tBUG_ON(ret);\n\t\tif (vmw_resource_mob_attached(res) &&\n\t\t    res->func->unbind != NULL) {\n\t\t\tstruct ttm_validate_buffer val_buf;\n\n\t\t\tval_buf.bo = bo;\n\t\t\tval_buf.num_shared = 0;\n\t\t\tres->func->unbind(res, false, &val_buf);\n\t\t}\n\t\tres->guest_memory_size = false;\n\t\tvmw_resource_mob_detach(res);\n\t\tif (res->dirty)\n\t\t\tres->func->dirty_free(res);\n\t\tif (res->coherent)\n\t\t\tvmw_bo_dirty_release(res->guest_memory_bo);\n\t\tttm_bo_unreserve(bo);\n\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n\t}\n\n\tif (likely(res->hw_destroy != NULL)) {\n\t\tmutex_lock(&dev_priv->binding_mutex);\n\t\tvmw_binding_res_list_kill(&res->binding_head);\n\t\tmutex_unlock(&dev_priv->binding_mutex);\n\t\tres->hw_destroy(res);\n\t}\n\n\tid = res->id;\n\tif (res->res_free != NULL)\n\t\tres->res_free(res);\n\telse\n\t\tkfree(res);\n\n\tspin_lock(&dev_priv->resource_lock);\n\tif (id != -1)\n\t\tidr_remove(idr, id);\n\tspin_unlock(&dev_priv->resource_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,7 +30,7 @@\n \t\tif (res->coherent)\n \t\t\tvmw_bo_dirty_release(res->guest_memory_bo);\n \t\tttm_bo_unreserve(bo);\n-\t\tvmw_bo_unreference(&res->guest_memory_bo);\n+\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n \t}\n \n \tif (likely(res->hw_destroy != NULL)) {",
        "function_modified_lines": {
            "added": [
                "\t\tvmw_user_bo_unref(&res->guest_memory_bo);"
            ],
            "deleted": [
                "\t\tvmw_bo_unreference(&res->guest_memory_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4278
    },
    {
        "cve_id": "CVE-2018-14625",
        "code_before_change": "static int\nvhost_transport_send_pkt(struct virtio_vsock_pkt *pkt)\n{\n\tstruct vhost_vsock *vsock;\n\tint len = pkt->len;\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(le64_to_cpu(pkt->hdr.dst_cid));\n\tif (!vsock) {\n\t\tvirtio_transport_free_pkt(pkt);\n\t\treturn -ENODEV;\n\t}\n\n\tif (pkt->reply)\n\t\tatomic_inc(&vsock->queued_replies);\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_add_tail(&pkt->list, &vsock->send_pkt_list);\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tvhost_work_queue(&vsock->dev, &vsock->send_pkt_work);\n\treturn len;\n}",
        "code_after_change": "static int\nvhost_transport_send_pkt(struct virtio_vsock_pkt *pkt)\n{\n\tstruct vhost_vsock *vsock;\n\tint len = pkt->len;\n\n\trcu_read_lock();\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(le64_to_cpu(pkt->hdr.dst_cid));\n\tif (!vsock) {\n\t\trcu_read_unlock();\n\t\tvirtio_transport_free_pkt(pkt);\n\t\treturn -ENODEV;\n\t}\n\n\tif (pkt->reply)\n\t\tatomic_inc(&vsock->queued_replies);\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_add_tail(&pkt->list, &vsock->send_pkt_list);\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tvhost_work_queue(&vsock->dev, &vsock->send_pkt_work);\n\n\trcu_read_unlock();\n\treturn len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,9 +4,12 @@\n \tstruct vhost_vsock *vsock;\n \tint len = pkt->len;\n \n+\trcu_read_lock();\n+\n \t/* Find the vhost_vsock according to guest context id  */\n \tvsock = vhost_vsock_get(le64_to_cpu(pkt->hdr.dst_cid));\n \tif (!vsock) {\n+\t\trcu_read_unlock();\n \t\tvirtio_transport_free_pkt(pkt);\n \t\treturn -ENODEV;\n \t}\n@@ -19,5 +22,7 @@\n \tspin_unlock_bh(&vsock->send_pkt_list_lock);\n \n \tvhost_work_queue(&vsock->dev, &vsock->send_pkt_work);\n+\n+\trcu_read_unlock();\n \treturn len;\n }",
        "function_modified_lines": {
            "added": [
                "\trcu_read_lock();",
                "",
                "\t\trcu_read_unlock();",
                "",
                "\trcu_read_unlock();"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux Kernel where an attacker may be able to have an uncontrolled read to kernel-memory from within a vm guest. A race condition between connect() and close() function may allow an attacker using the AF_VSOCK protocol to gather a 4 byte information leak or possibly intercept or corrupt AF_VSOCK messages destined to other clients.",
        "id": 1695
    },
    {
        "cve_id": "CVE-2020-10690",
        "code_before_change": "void posix_clock_unregister(struct posix_clock *clk)\n{\n\tcdev_del(&clk->cdev);\n\n\tdown_write(&clk->rwsem);\n\tclk->zombie = true;\n\tup_write(&clk->rwsem);\n\n\tkref_put(&clk->kref, delete_clock);\n}",
        "code_after_change": "void posix_clock_unregister(struct posix_clock *clk)\n{\n\tcdev_device_del(&clk->cdev, clk->dev);\n\n\tdown_write(&clk->rwsem);\n\tclk->zombie = true;\n\tup_write(&clk->rwsem);\n\n\tput_device(clk->dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,10 @@\n void posix_clock_unregister(struct posix_clock *clk)\n {\n-\tcdev_del(&clk->cdev);\n+\tcdev_device_del(&clk->cdev, clk->dev);\n \n \tdown_write(&clk->rwsem);\n \tclk->zombie = true;\n \tup_write(&clk->rwsem);\n \n-\tkref_put(&clk->kref, delete_clock);\n+\tput_device(clk->dev);\n }",
        "function_modified_lines": {
            "added": [
                "\tcdev_device_del(&clk->cdev, clk->dev);",
                "\tput_device(clk->dev);"
            ],
            "deleted": [
                "\tcdev_del(&clk->cdev);",
                "\tkref_put(&clk->kref, delete_clock);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a use-after-free in kernel versions before 5.5 due to a race condition between the release of ptp_clock and cdev while resource deallocation. When a (high privileged) process allocates a ptp device file (like /dev/ptpX) and voluntarily goes to sleep. During this time if the underlying device is removed, it can cause an exploitable condition as the process wakes up to terminate and clean all attached files. The system crashes due to the cdev structure being invalid (as already freed) which is pointed to by the inode.",
        "id": 2401
    },
    {
        "cve_id": "CVE-2017-6874",
        "code_before_change": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tatomic_set(&new->count, 0);\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n\t\tucounts = NULL;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
        "code_after_change": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tnew->count = 0;\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (ucounts->count == INT_MAX)\n\t\tucounts = NULL;\n\telse\n\t\tucounts->count += 1;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,7 +14,7 @@\n \n \t\tnew->ns = ns;\n \t\tnew->uid = uid;\n-\t\tatomic_set(&new->count, 0);\n+\t\tnew->count = 0;\n \n \t\tspin_lock_irq(&ucounts_lock);\n \t\tucounts = find_ucounts(ns, uid, hashent);\n@@ -25,8 +25,10 @@\n \t\t\tucounts = new;\n \t\t}\n \t}\n-\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n+\tif (ucounts->count == INT_MAX)\n \t\tucounts = NULL;\n+\telse\n+\t\tucounts->count += 1;\n \tspin_unlock_irq(&ucounts_lock);\n \treturn ucounts;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tnew->count = 0;",
                "\tif (ucounts->count == INT_MAX)",
                "\telse",
                "\t\tucounts->count += 1;"
            ],
            "deleted": [
                "\t\tatomic_set(&new->count, 0);",
                "\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/ucount.c in the Linux kernel through 4.10.2 allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via crafted system calls that leverage certain decrement behavior that causes incorrect interaction between put_ucounts and get_ucounts.",
        "id": 1488
    },
    {
        "cve_id": "CVE-2020-15436",
        "code_before_change": "static int __blkdev_get(struct block_device *bdev, fmode_t mode, int for_part)\n{\n\tstruct gendisk *disk;\n\tint ret;\n\tint partno;\n\tint perm = 0;\n\tbool first_open = false;\n\n\tif (mode & FMODE_READ)\n\t\tperm |= MAY_READ;\n\tif (mode & FMODE_WRITE)\n\t\tperm |= MAY_WRITE;\n\t/*\n\t * hooks: /n/, see \"layering violations\".\n\t */\n\tif (!for_part) {\n\t\tret = devcgroup_inode_permission(bdev->bd_inode, perm);\n\t\tif (ret != 0) {\n\t\t\tbdput(bdev);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n restart:\n\n\tret = -ENXIO;\n\tdisk = bdev_get_gendisk(bdev, &partno);\n\tif (!disk)\n\t\tgoto out;\n\n\tdisk_block_events(disk);\n\tmutex_lock_nested(&bdev->bd_mutex, for_part);\n\tif (!bdev->bd_openers) {\n\t\tfirst_open = true;\n\t\tbdev->bd_disk = disk;\n\t\tbdev->bd_queue = disk->queue;\n\t\tbdev->bd_contains = bdev;\n\t\tbdev->bd_partno = partno;\n\n\t\tif (!partno) {\n\t\t\tret = -ENXIO;\n\t\t\tbdev->bd_part = disk_get_part(disk, partno);\n\t\t\tif (!bdev->bd_part)\n\t\t\t\tgoto out_clear;\n\n\t\t\tret = 0;\n\t\t\tif (disk->fops->open) {\n\t\t\t\tret = disk->fops->open(bdev, mode);\n\t\t\t\tif (ret == -ERESTARTSYS) {\n\t\t\t\t\t/* Lost a race with 'disk' being\n\t\t\t\t\t * deleted, try again.\n\t\t\t\t\t * See md.c\n\t\t\t\t\t */\n\t\t\t\t\tdisk_put_part(bdev->bd_part);\n\t\t\t\t\tbdev->bd_part = NULL;\n\t\t\t\t\tbdev->bd_disk = NULL;\n\t\t\t\t\tbdev->bd_queue = NULL;\n\t\t\t\t\tmutex_unlock(&bdev->bd_mutex);\n\t\t\t\t\tdisk_unblock_events(disk);\n\t\t\t\t\tput_disk_and_module(disk);\n\t\t\t\t\tgoto restart;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!ret) {\n\t\t\t\tbd_set_size(bdev,(loff_t)get_capacity(disk)<<9);\n\t\t\t\tset_init_blocksize(bdev);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If the device is invalidated, rescan partition\n\t\t\t * if open succeeded or failed with -ENOMEDIUM.\n\t\t\t * The latter is necessary to prevent ghost\n\t\t\t * partitions on a removed medium.\n\t\t\t */\n\t\t\tif (bdev->bd_invalidated &&\n\t\t\t    (!ret || ret == -ENOMEDIUM))\n\t\t\t\tbdev_disk_changed(bdev, ret == -ENOMEDIUM);\n\n\t\t\tif (ret)\n\t\t\t\tgoto out_clear;\n\t\t} else {\n\t\t\tstruct block_device *whole;\n\t\t\twhole = bdget_disk(disk, 0);\n\t\t\tret = -ENOMEM;\n\t\t\tif (!whole)\n\t\t\t\tgoto out_clear;\n\t\t\tBUG_ON(for_part);\n\t\t\tret = __blkdev_get(whole, mode, 1);\n\t\t\tif (ret)\n\t\t\t\tgoto out_clear;\n\t\t\tbdev->bd_contains = whole;\n\t\t\tbdev->bd_part = disk_get_part(disk, partno);\n\t\t\tif (!(disk->flags & GENHD_FL_UP) ||\n\t\t\t    !bdev->bd_part || !bdev->bd_part->nr_sects) {\n\t\t\t\tret = -ENXIO;\n\t\t\t\tgoto out_clear;\n\t\t\t}\n\t\t\tbd_set_size(bdev, (loff_t)bdev->bd_part->nr_sects << 9);\n\t\t\tset_init_blocksize(bdev);\n\t\t}\n\n\t\tif (bdev->bd_bdi == &noop_backing_dev_info)\n\t\t\tbdev->bd_bdi = bdi_get(disk->queue->backing_dev_info);\n\t} else {\n\t\tif (bdev->bd_contains == bdev) {\n\t\t\tret = 0;\n\t\t\tif (bdev->bd_disk->fops->open)\n\t\t\t\tret = bdev->bd_disk->fops->open(bdev, mode);\n\t\t\t/* the same as first opener case, read comment there */\n\t\t\tif (bdev->bd_invalidated &&\n\t\t\t    (!ret || ret == -ENOMEDIUM))\n\t\t\t\tbdev_disk_changed(bdev, ret == -ENOMEDIUM);\n\t\t\tif (ret)\n\t\t\t\tgoto out_unlock_bdev;\n\t\t}\n\t}\n\tbdev->bd_openers++;\n\tif (for_part)\n\t\tbdev->bd_part_count++;\n\tmutex_unlock(&bdev->bd_mutex);\n\tdisk_unblock_events(disk);\n\t/* only one opener holds refs to the module and disk */\n\tif (!first_open)\n\t\tput_disk_and_module(disk);\n\treturn 0;\n\n out_clear:\n\tdisk_put_part(bdev->bd_part);\n\tbdev->bd_disk = NULL;\n\tbdev->bd_part = NULL;\n\tbdev->bd_queue = NULL;\n\tif (bdev != bdev->bd_contains)\n\t\t__blkdev_put(bdev->bd_contains, mode, 1);\n\tbdev->bd_contains = NULL;\n out_unlock_bdev:\n\tmutex_unlock(&bdev->bd_mutex);\n\tdisk_unblock_events(disk);\n\tput_disk_and_module(disk);\n out:\n\tbdput(bdev);\n\n\treturn ret;\n}",
        "code_after_change": "static int __blkdev_get(struct block_device *bdev, fmode_t mode, int for_part)\n{\n\tstruct gendisk *disk;\n\tint ret;\n\tint partno;\n\tint perm = 0;\n\tbool first_open = false;\n\n\tif (mode & FMODE_READ)\n\t\tperm |= MAY_READ;\n\tif (mode & FMODE_WRITE)\n\t\tperm |= MAY_WRITE;\n\t/*\n\t * hooks: /n/, see \"layering violations\".\n\t */\n\tif (!for_part) {\n\t\tret = devcgroup_inode_permission(bdev->bd_inode, perm);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\n restart:\n\n\tret = -ENXIO;\n\tdisk = bdev_get_gendisk(bdev, &partno);\n\tif (!disk)\n\t\tgoto out;\n\n\tdisk_block_events(disk);\n\tmutex_lock_nested(&bdev->bd_mutex, for_part);\n\tif (!bdev->bd_openers) {\n\t\tfirst_open = true;\n\t\tbdev->bd_disk = disk;\n\t\tbdev->bd_queue = disk->queue;\n\t\tbdev->bd_contains = bdev;\n\t\tbdev->bd_partno = partno;\n\n\t\tif (!partno) {\n\t\t\tret = -ENXIO;\n\t\t\tbdev->bd_part = disk_get_part(disk, partno);\n\t\t\tif (!bdev->bd_part)\n\t\t\t\tgoto out_clear;\n\n\t\t\tret = 0;\n\t\t\tif (disk->fops->open) {\n\t\t\t\tret = disk->fops->open(bdev, mode);\n\t\t\t\tif (ret == -ERESTARTSYS) {\n\t\t\t\t\t/* Lost a race with 'disk' being\n\t\t\t\t\t * deleted, try again.\n\t\t\t\t\t * See md.c\n\t\t\t\t\t */\n\t\t\t\t\tdisk_put_part(bdev->bd_part);\n\t\t\t\t\tbdev->bd_part = NULL;\n\t\t\t\t\tbdev->bd_disk = NULL;\n\t\t\t\t\tbdev->bd_queue = NULL;\n\t\t\t\t\tmutex_unlock(&bdev->bd_mutex);\n\t\t\t\t\tdisk_unblock_events(disk);\n\t\t\t\t\tput_disk_and_module(disk);\n\t\t\t\t\tgoto restart;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!ret) {\n\t\t\t\tbd_set_size(bdev,(loff_t)get_capacity(disk)<<9);\n\t\t\t\tset_init_blocksize(bdev);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If the device is invalidated, rescan partition\n\t\t\t * if open succeeded or failed with -ENOMEDIUM.\n\t\t\t * The latter is necessary to prevent ghost\n\t\t\t * partitions on a removed medium.\n\t\t\t */\n\t\t\tif (bdev->bd_invalidated &&\n\t\t\t    (!ret || ret == -ENOMEDIUM))\n\t\t\t\tbdev_disk_changed(bdev, ret == -ENOMEDIUM);\n\n\t\t\tif (ret)\n\t\t\t\tgoto out_clear;\n\t\t} else {\n\t\t\tstruct block_device *whole;\n\t\t\twhole = bdget_disk(disk, 0);\n\t\t\tret = -ENOMEM;\n\t\t\tif (!whole)\n\t\t\t\tgoto out_clear;\n\t\t\tBUG_ON(for_part);\n\t\t\tret = __blkdev_get(whole, mode, 1);\n\t\t\tif (ret) {\n\t\t\t\tbdput(whole);\n\t\t\t\tgoto out_clear;\n\t\t\t}\n\t\t\tbdev->bd_contains = whole;\n\t\t\tbdev->bd_part = disk_get_part(disk, partno);\n\t\t\tif (!(disk->flags & GENHD_FL_UP) ||\n\t\t\t    !bdev->bd_part || !bdev->bd_part->nr_sects) {\n\t\t\t\tret = -ENXIO;\n\t\t\t\tgoto out_clear;\n\t\t\t}\n\t\t\tbd_set_size(bdev, (loff_t)bdev->bd_part->nr_sects << 9);\n\t\t\tset_init_blocksize(bdev);\n\t\t}\n\n\t\tif (bdev->bd_bdi == &noop_backing_dev_info)\n\t\t\tbdev->bd_bdi = bdi_get(disk->queue->backing_dev_info);\n\t} else {\n\t\tif (bdev->bd_contains == bdev) {\n\t\t\tret = 0;\n\t\t\tif (bdev->bd_disk->fops->open)\n\t\t\t\tret = bdev->bd_disk->fops->open(bdev, mode);\n\t\t\t/* the same as first opener case, read comment there */\n\t\t\tif (bdev->bd_invalidated &&\n\t\t\t    (!ret || ret == -ENOMEDIUM))\n\t\t\t\tbdev_disk_changed(bdev, ret == -ENOMEDIUM);\n\t\t\tif (ret)\n\t\t\t\tgoto out_unlock_bdev;\n\t\t}\n\t}\n\tbdev->bd_openers++;\n\tif (for_part)\n\t\tbdev->bd_part_count++;\n\tmutex_unlock(&bdev->bd_mutex);\n\tdisk_unblock_events(disk);\n\t/* only one opener holds refs to the module and disk */\n\tif (!first_open)\n\t\tput_disk_and_module(disk);\n\treturn 0;\n\n out_clear:\n\tdisk_put_part(bdev->bd_part);\n\tbdev->bd_disk = NULL;\n\tbdev->bd_part = NULL;\n\tbdev->bd_queue = NULL;\n\tif (bdev != bdev->bd_contains)\n\t\t__blkdev_put(bdev->bd_contains, mode, 1);\n\tbdev->bd_contains = NULL;\n out_unlock_bdev:\n\tmutex_unlock(&bdev->bd_mutex);\n\tdisk_unblock_events(disk);\n\tput_disk_and_module(disk);\n out:\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,10 +15,8 @@\n \t */\n \tif (!for_part) {\n \t\tret = devcgroup_inode_permission(bdev->bd_inode, perm);\n-\t\tif (ret != 0) {\n-\t\t\tbdput(bdev);\n+\t\tif (ret != 0)\n \t\t\treturn ret;\n-\t\t}\n \t}\n \n  restart:\n@@ -87,8 +85,10 @@\n \t\t\t\tgoto out_clear;\n \t\t\tBUG_ON(for_part);\n \t\t\tret = __blkdev_get(whole, mode, 1);\n-\t\t\tif (ret)\n+\t\t\tif (ret) {\n+\t\t\t\tbdput(whole);\n \t\t\t\tgoto out_clear;\n+\t\t\t}\n \t\t\tbdev->bd_contains = whole;\n \t\t\tbdev->bd_part = disk_get_part(disk, partno);\n \t\t\tif (!(disk->flags & GENHD_FL_UP) ||\n@@ -138,7 +138,6 @@\n \tdisk_unblock_events(disk);\n \tput_disk_and_module(disk);\n  out:\n-\tbdput(bdev);\n \n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tif (ret != 0)",
                "\t\t\tif (ret) {",
                "\t\t\t\tbdput(whole);",
                "\t\t\t}"
            ],
            "deleted": [
                "\t\tif (ret != 0) {",
                "\t\t\tbdput(bdev);",
                "\t\t}",
                "\t\t\tif (ret)",
                "\tbdput(bdev);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in fs/block_dev.c in the Linux kernel before 5.8 allows local users to gain privileges or cause a denial of service by leveraging improper access to a certain error field.",
        "id": 2543
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static int\nsvm_migrate_vram_to_vram(struct svm_range *prange, uint32_t best_loc,\n\t\t\t struct mm_struct *mm, uint32_t trigger)\n{\n\tint r, retries = 3;\n\n\t/*\n\t * TODO: for both devices with PCIe large bar or on same xgmi hive, skip\n\t * system memory as migration bridge\n\t */\n\n\tpr_debug(\"from gpu 0x%x to gpu 0x%x\\n\", prange->actual_loc, best_loc);\n\n\tdo {\n\t\tr = svm_migrate_vram_to_ram(prange, mm, trigger);\n\t\tif (r)\n\t\t\treturn r;\n\t} while (prange->actual_loc && --retries);\n\n\tif (prange->actual_loc)\n\t\treturn -EDEADLK;\n\n\treturn svm_migrate_ram_to_vram(prange, best_loc, mm, trigger);\n}",
        "code_after_change": "static int\nsvm_migrate_vram_to_vram(struct svm_range *prange, uint32_t best_loc,\n\t\t\t struct mm_struct *mm, uint32_t trigger)\n{\n\tint r, retries = 3;\n\n\t/*\n\t * TODO: for both devices with PCIe large bar or on same xgmi hive, skip\n\t * system memory as migration bridge\n\t */\n\n\tpr_debug(\"from gpu 0x%x to gpu 0x%x\\n\", prange->actual_loc, best_loc);\n\n\tdo {\n\t\tr = svm_migrate_vram_to_ram(prange, mm, trigger, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t} while (prange->actual_loc && --retries);\n\n\tif (prange->actual_loc)\n\t\treturn -EDEADLK;\n\n\treturn svm_migrate_ram_to_vram(prange, best_loc, mm, trigger);\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,7 @@\n \tpr_debug(\"from gpu 0x%x to gpu 0x%x\\n\", prange->actual_loc, best_loc);\n \n \tdo {\n-\t\tr = svm_migrate_vram_to_ram(prange, mm, trigger);\n+\t\tr = svm_migrate_vram_to_ram(prange, mm, trigger, NULL);\n \t\tif (r)\n \t\t\treturn r;\n \t} while (prange->actual_loc && --retries);",
        "function_modified_lines": {
            "added": [
                "\t\tr = svm_migrate_vram_to_ram(prange, mm, trigger, NULL);"
            ],
            "deleted": [
                "\t\tr = svm_migrate_vram_to_ram(prange, mm, trigger);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3611
    },
    {
        "cve_id": "CVE-2017-10661",
        "code_before_change": "\nSYSCALL_DEFINE2(timerfd_create, int, clockid, int, flags)\n{\n\tint ufd;\n\tstruct timerfd_ctx *ctx;\n\n\t/* Check the TFD_* constants for consistency.  */\n\tBUILD_BUG_ON(TFD_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON(TFD_NONBLOCK != O_NONBLOCK);\n\n\tif ((flags & ~TFD_CREATE_FLAGS) ||\n\t    (clockid != CLOCK_MONOTONIC &&\n\t     clockid != CLOCK_REALTIME &&\n\t     clockid != CLOCK_REALTIME_ALARM &&\n\t     clockid != CLOCK_BOOTTIME &&\n\t     clockid != CLOCK_BOOTTIME_ALARM))\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_WAKE_ALARM) &&\n\t    (clockid == CLOCK_REALTIME_ALARM ||\n\t     clockid == CLOCK_BOOTTIME_ALARM))\n\t\treturn -EPERM;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tinit_waitqueue_head(&ctx->wqh);\n\tctx->clockid = clockid;\n\n\tif (isalarm(ctx))\n\t\talarm_init(&ctx->t.alarm,\n\t\t\t   ctx->clockid == CLOCK_REALTIME_ALARM ?\n\t\t\t   ALARM_REALTIME : ALARM_BOOTTIME,\n\t\t\t   timerfd_alarmproc);\n\telse\n\t\thrtimer_init(&ctx->t.tmr, clockid, HRTIMER_MODE_ABS);\n\n\tctx->moffs = ktime_mono_to_real(0);\n\n\tufd = anon_inode_getfd(\"[timerfd]\", &timerfd_fops, ctx,\n\t\t\t       O_RDWR | (flags & TFD_SHARED_FCNTL_FLAGS));\n\tif (ufd < 0)\n\t\tkfree(ctx);\n\n\treturn ufd;\n}",
        "code_after_change": "\nSYSCALL_DEFINE2(timerfd_create, int, clockid, int, flags)\n{\n\tint ufd;\n\tstruct timerfd_ctx *ctx;\n\n\t/* Check the TFD_* constants for consistency.  */\n\tBUILD_BUG_ON(TFD_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON(TFD_NONBLOCK != O_NONBLOCK);\n\n\tif ((flags & ~TFD_CREATE_FLAGS) ||\n\t    (clockid != CLOCK_MONOTONIC &&\n\t     clockid != CLOCK_REALTIME &&\n\t     clockid != CLOCK_REALTIME_ALARM &&\n\t     clockid != CLOCK_BOOTTIME &&\n\t     clockid != CLOCK_BOOTTIME_ALARM))\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_WAKE_ALARM) &&\n\t    (clockid == CLOCK_REALTIME_ALARM ||\n\t     clockid == CLOCK_BOOTTIME_ALARM))\n\t\treturn -EPERM;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tinit_waitqueue_head(&ctx->wqh);\n\tspin_lock_init(&ctx->cancel_lock);\n\tctx->clockid = clockid;\n\n\tif (isalarm(ctx))\n\t\talarm_init(&ctx->t.alarm,\n\t\t\t   ctx->clockid == CLOCK_REALTIME_ALARM ?\n\t\t\t   ALARM_REALTIME : ALARM_BOOTTIME,\n\t\t\t   timerfd_alarmproc);\n\telse\n\t\thrtimer_init(&ctx->t.tmr, clockid, HRTIMER_MODE_ABS);\n\n\tctx->moffs = ktime_mono_to_real(0);\n\n\tufd = anon_inode_getfd(\"[timerfd]\", &timerfd_fops, ctx,\n\t\t\t       O_RDWR | (flags & TFD_SHARED_FCNTL_FLAGS));\n\tif (ufd < 0)\n\t\tkfree(ctx);\n\n\treturn ufd;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,6 +26,7 @@\n \t\treturn -ENOMEM;\n \n \tinit_waitqueue_head(&ctx->wqh);\n+\tspin_lock_init(&ctx->cancel_lock);\n \tctx->clockid = clockid;\n \n \tif (isalarm(ctx))",
        "function_modified_lines": {
            "added": [
                "\tspin_lock_init(&ctx->cancel_lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Race condition in fs/timerfd.c in the Linux kernel before 4.10.15 allows local users to gain privileges or cause a denial of service (list corruption or use-after-free) via simultaneous file-descriptor operations that leverage improper might_cancel queueing.",
        "id": 1243
    },
    {
        "cve_id": "CVE-2022-2318",
        "code_before_change": "void rose_stop_idletimer(struct sock *sk)\n{\n\tdel_timer(&rose_sk(sk)->idletimer);\n}",
        "code_after_change": "void rose_stop_idletimer(struct sock *sk)\n{\n\tsk_stop_timer(sk, &rose_sk(sk)->idletimer);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n void rose_stop_idletimer(struct sock *sk)\n {\n-\tdel_timer(&rose_sk(sk)->idletimer);\n+\tsk_stop_timer(sk, &rose_sk(sk)->idletimer);\n }",
        "function_modified_lines": {
            "added": [
                "\tsk_stop_timer(sk, &rose_sk(sk)->idletimer);"
            ],
            "deleted": [
                "\tdel_timer(&rose_sk(sk)->idletimer);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities caused by timer handler in net/rose/rose_timer.c of linux that allow attackers to crash linux kernel without any privileges.",
        "id": 3435
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n\t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tvmw_user_bo_unref(vmw_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo, *tmp_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n\t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\ttmp_bo = vmw_bo;\n\tvmw_user_bo_unref(&tmp_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \t\t\t\t   SVGAGuestPtr *ptr,\n \t\t\t\t   struct vmw_bo **vmw_bo_p)\n {\n-\tstruct vmw_bo *vmw_bo;\n+\tstruct vmw_bo *vmw_bo, *tmp_bo;\n \tuint32_t handle = ptr->gmrId;\n \tstruct vmw_relocation *reloc;\n \tint ret;\n@@ -17,7 +17,8 @@\n \tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n \t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n \tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n-\tvmw_user_bo_unref(vmw_bo);\n+\ttmp_bo = vmw_bo;\n+\tvmw_user_bo_unref(&tmp_bo);\n \tif (unlikely(ret != 0))\n \t\treturn ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct vmw_bo *vmw_bo, *tmp_bo;",
                "\ttmp_bo = vmw_bo;",
                "\tvmw_user_bo_unref(&tmp_bo);"
            ],
            "deleted": [
                "\tstruct vmw_bo *vmw_bo;",
                "\tvmw_user_bo_unref(vmw_bo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4272
    },
    {
        "cve_id": "CVE-2016-9794",
        "code_before_change": "void snd_pcm_period_elapsed(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tunsigned long flags;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\n\tsnd_pcm_stream_lock_irqsave(substream, flags);\n\tif (!snd_pcm_running(substream) ||\n\t    snd_pcm_update_hw_ptr0(substream, 1) < 0)\n\t\tgoto _end;\n\n#ifdef CONFIG_SND_PCM_TIMER\n\tif (substream->timer_running)\n\t\tsnd_timer_interrupt(substream->timer, 1);\n#endif\n _end:\n\tsnd_pcm_stream_unlock_irqrestore(substream, flags);\n\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n}",
        "code_after_change": "void snd_pcm_period_elapsed(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\tunsigned long flags;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\n\tsnd_pcm_stream_lock_irqsave(substream, flags);\n\tif (!snd_pcm_running(substream) ||\n\t    snd_pcm_update_hw_ptr0(substream, 1) < 0)\n\t\tgoto _end;\n\n#ifdef CONFIG_SND_PCM_TIMER\n\tif (substream->timer_running)\n\t\tsnd_timer_interrupt(substream->timer, 1);\n#endif\n _end:\n\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n\tsnd_pcm_stream_unlock_irqrestore(substream, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,6 @@\n \t\tsnd_timer_interrupt(substream->timer, 1);\n #endif\n  _end:\n+\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n \tsnd_pcm_stream_unlock_irqrestore(substream, flags);\n-\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);\n }",
        "function_modified_lines": {
            "added": [
                "\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);"
            ],
            "deleted": [
                "\tkill_fasync(&runtime->fasync, SIGIO, POLL_IN);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the snd_pcm_period_elapsed function in sound/core/pcm_lib.c in the ALSA subsystem in the Linux kernel before 4.7 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via a crafted SNDRV_PCM_TRIGGER_START command.",
        "id": 1168
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "static void local_release(struct kref *ref)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = container_of(ref, struct nfc_llcp_local, ref);\n\n\tlist_del(&local->list);\n\tlocal_cleanup(local);\n\tkfree(local);\n}",
        "code_after_change": "static void local_release(struct kref *ref)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = container_of(ref, struct nfc_llcp_local, ref);\n\n\tlocal_cleanup(local);\n\tkfree(local);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \n \tlocal = container_of(ref, struct nfc_llcp_local, ref);\n \n-\tlist_del(&local->list);\n \tlocal_cleanup(local);\n \tkfree(local);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tlist_del(&local->list);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4149
    },
    {
        "cve_id": "CVE-2021-0941",
        "code_before_change": "static inline int __bpf_skb_change_tail(struct sk_buff *skb, u32 new_len,\n\t\t\t\t\tu64 flags)\n{\n\tu32 max_len = __bpf_skb_max_len(skb);\n\tu32 min_len = __bpf_skb_min_len(skb);\n\tint ret;\n\n\tif (unlikely(flags || new_len > max_len || new_len < min_len))\n\t\treturn -EINVAL;\n\tif (skb->encapsulation)\n\t\treturn -ENOTSUPP;\n\n\t/* The basic idea of this helper is that it's performing the\n\t * needed work to either grow or trim an skb, and eBPF program\n\t * rewrites the rest via helpers like bpf_skb_store_bytes(),\n\t * bpf_lX_csum_replace() and others rather than passing a raw\n\t * buffer here. This one is a slow path helper and intended\n\t * for replies with control messages.\n\t *\n\t * Like in bpf_skb_change_proto(), we want to keep this rather\n\t * minimal and without protocol specifics so that we are able\n\t * to separate concerns as in bpf_skb_store_bytes() should only\n\t * be the one responsible for writing buffers.\n\t *\n\t * It's really expected to be a slow path operation here for\n\t * control message replies, so we're implicitly linearizing,\n\t * uncloning and drop offloads from the skb by this.\n\t */\n\tret = __bpf_try_make_writable(skb, skb->len);\n\tif (!ret) {\n\t\tif (new_len > skb->len)\n\t\t\tret = bpf_skb_grow_rcsum(skb, new_len);\n\t\telse if (new_len < skb->len)\n\t\t\tret = bpf_skb_trim_rcsum(skb, new_len);\n\t\tif (!ret && skb_is_gso(skb))\n\t\t\tskb_gso_reset(skb);\n\t}\n\treturn ret;\n}",
        "code_after_change": "static inline int __bpf_skb_change_tail(struct sk_buff *skb, u32 new_len,\n\t\t\t\t\tu64 flags)\n{\n\tu32 max_len = BPF_SKB_MAX_LEN;\n\tu32 min_len = __bpf_skb_min_len(skb);\n\tint ret;\n\n\tif (unlikely(flags || new_len > max_len || new_len < min_len))\n\t\treturn -EINVAL;\n\tif (skb->encapsulation)\n\t\treturn -ENOTSUPP;\n\n\t/* The basic idea of this helper is that it's performing the\n\t * needed work to either grow or trim an skb, and eBPF program\n\t * rewrites the rest via helpers like bpf_skb_store_bytes(),\n\t * bpf_lX_csum_replace() and others rather than passing a raw\n\t * buffer here. This one is a slow path helper and intended\n\t * for replies with control messages.\n\t *\n\t * Like in bpf_skb_change_proto(), we want to keep this rather\n\t * minimal and without protocol specifics so that we are able\n\t * to separate concerns as in bpf_skb_store_bytes() should only\n\t * be the one responsible for writing buffers.\n\t *\n\t * It's really expected to be a slow path operation here for\n\t * control message replies, so we're implicitly linearizing,\n\t * uncloning and drop offloads from the skb by this.\n\t */\n\tret = __bpf_try_make_writable(skb, skb->len);\n\tif (!ret) {\n\t\tif (new_len > skb->len)\n\t\t\tret = bpf_skb_grow_rcsum(skb, new_len);\n\t\telse if (new_len < skb->len)\n\t\t\tret = bpf_skb_trim_rcsum(skb, new_len);\n\t\tif (!ret && skb_is_gso(skb))\n\t\t\tskb_gso_reset(skb);\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n static inline int __bpf_skb_change_tail(struct sk_buff *skb, u32 new_len,\n \t\t\t\t\tu64 flags)\n {\n-\tu32 max_len = __bpf_skb_max_len(skb);\n+\tu32 max_len = BPF_SKB_MAX_LEN;\n \tu32 min_len = __bpf_skb_min_len(skb);\n \tint ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tu32 max_len = BPF_SKB_MAX_LEN;"
            ],
            "deleted": [
                "\tu32 max_len = __bpf_skb_max_len(skb);"
            ]
        },
        "cwe": [
            "CWE-125",
            "CWE-416"
        ],
        "cve_description": "In bpf_skb_change_head of filter.c, there is a possible out of bounds read due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-154177719References: Upstream kernel",
        "id": 2840
    },
    {
        "cve_id": "CVE-2022-2318",
        "code_before_change": "void rose_stop_timer(struct sock *sk)\n{\n\tdel_timer(&rose_sk(sk)->timer);\n}",
        "code_after_change": "void rose_stop_timer(struct sock *sk)\n{\n\tsk_stop_timer(sk, &rose_sk(sk)->timer);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n void rose_stop_timer(struct sock *sk)\n {\n-\tdel_timer(&rose_sk(sk)->timer);\n+\tsk_stop_timer(sk, &rose_sk(sk)->timer);\n }",
        "function_modified_lines": {
            "added": [
                "\tsk_stop_timer(sk, &rose_sk(sk)->timer);"
            ],
            "deleted": [
                "\tdel_timer(&rose_sk(sk)->timer);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities caused by timer handler in net/rose/rose_timer.c of linux that allow attackers to crash linux kernel without any privileges.",
        "id": 3438
    },
    {
        "cve_id": "CVE-2022-2938",
        "code_before_change": "static ssize_t psi_write(struct file *file, const char __user *user_buf,\n\t\t\t size_t nbytes, enum psi_res res)\n{\n\tchar buf[32];\n\tsize_t buf_size;\n\tstruct seq_file *seq;\n\tstruct psi_trigger *new;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!nbytes)\n\t\treturn -EINVAL;\n\n\tbuf_size = min(nbytes, sizeof(buf));\n\tif (copy_from_user(buf, user_buf, buf_size))\n\t\treturn -EFAULT;\n\n\tbuf[buf_size - 1] = '\\0';\n\n\tnew = psi_trigger_create(&psi_system, buf, nbytes, res);\n\tif (IS_ERR(new))\n\t\treturn PTR_ERR(new);\n\n\tseq = file->private_data;\n\t/* Take seq->lock to protect seq->private from concurrent writes */\n\tmutex_lock(&seq->lock);\n\tpsi_trigger_replace(&seq->private, new);\n\tmutex_unlock(&seq->lock);\n\n\treturn nbytes;\n}",
        "code_after_change": "static ssize_t psi_write(struct file *file, const char __user *user_buf,\n\t\t\t size_t nbytes, enum psi_res res)\n{\n\tchar buf[32];\n\tsize_t buf_size;\n\tstruct seq_file *seq;\n\tstruct psi_trigger *new;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!nbytes)\n\t\treturn -EINVAL;\n\n\tbuf_size = min(nbytes, sizeof(buf));\n\tif (copy_from_user(buf, user_buf, buf_size))\n\t\treturn -EFAULT;\n\n\tbuf[buf_size - 1] = '\\0';\n\n\tseq = file->private_data;\n\n\t/* Take seq->lock to protect seq->private from concurrent writes */\n\tmutex_lock(&seq->lock);\n\n\t/* Allow only one trigger per file descriptor */\n\tif (seq->private) {\n\t\tmutex_unlock(&seq->lock);\n\t\treturn -EBUSY;\n\t}\n\n\tnew = psi_trigger_create(&psi_system, buf, nbytes, res);\n\tif (IS_ERR(new)) {\n\t\tmutex_unlock(&seq->lock);\n\t\treturn PTR_ERR(new);\n\t}\n\n\tsmp_store_release(&seq->private, new);\n\tmutex_unlock(&seq->lock);\n\n\treturn nbytes;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,14 +18,24 @@\n \n \tbuf[buf_size - 1] = '\\0';\n \n-\tnew = psi_trigger_create(&psi_system, buf, nbytes, res);\n-\tif (IS_ERR(new))\n-\t\treturn PTR_ERR(new);\n+\tseq = file->private_data;\n \n-\tseq = file->private_data;\n \t/* Take seq->lock to protect seq->private from concurrent writes */\n \tmutex_lock(&seq->lock);\n-\tpsi_trigger_replace(&seq->private, new);\n+\n+\t/* Allow only one trigger per file descriptor */\n+\tif (seq->private) {\n+\t\tmutex_unlock(&seq->lock);\n+\t\treturn -EBUSY;\n+\t}\n+\n+\tnew = psi_trigger_create(&psi_system, buf, nbytes, res);\n+\tif (IS_ERR(new)) {\n+\t\tmutex_unlock(&seq->lock);\n+\t\treturn PTR_ERR(new);\n+\t}\n+\n+\tsmp_store_release(&seq->private, new);\n \tmutex_unlock(&seq->lock);\n \n \treturn nbytes;",
        "function_modified_lines": {
            "added": [
                "\tseq = file->private_data;",
                "",
                "\t/* Allow only one trigger per file descriptor */",
                "\tif (seq->private) {",
                "\t\tmutex_unlock(&seq->lock);",
                "\t\treturn -EBUSY;",
                "\t}",
                "",
                "\tnew = psi_trigger_create(&psi_system, buf, nbytes, res);",
                "\tif (IS_ERR(new)) {",
                "\t\tmutex_unlock(&seq->lock);",
                "\t\treturn PTR_ERR(new);",
                "\t}",
                "",
                "\tsmp_store_release(&seq->private, new);"
            ],
            "deleted": [
                "\tnew = psi_trigger_create(&psi_system, buf, nbytes, res);",
                "\tif (IS_ERR(new))",
                "\t\treturn PTR_ERR(new);",
                "\tseq = file->private_data;",
                "\tpsi_trigger_replace(&seq->private, new);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's implementation of Pressure Stall Information. While the feature is disabled by default, it could allow an attacker to crash the system or have other memory-corruption side effects.",
        "id": 3518
    },
    {
        "cve_id": "CVE-2020-7053",
        "code_before_change": "int i915_gem_context_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t  struct drm_file *file)\n{\n\tstruct drm_i915_private *i915 = to_i915(dev);\n\tstruct drm_i915_gem_context_create *args = data;\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\tstruct i915_gem_context *ctx;\n\tint ret;\n\n\tif (!DRIVER_CAPS(i915)->has_logical_contexts)\n\t\treturn -ENODEV;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tret = i915_terminally_wedged(i915);\n\tif (ret)\n\t\treturn ret;\n\n\tif (client_is_banned(file_priv)) {\n\t\tDRM_DEBUG(\"client %s[%d] banned from creating ctx\\n\",\n\t\t\t  current->comm,\n\t\t\t  pid_nr(get_task_pid(current, PIDTYPE_PID)));\n\n\t\treturn -EIO;\n\t}\n\n\tret = i915_mutex_lock_interruptible(dev);\n\tif (ret)\n\t\treturn ret;\n\n\tctx = i915_gem_create_context(i915);\n\tif (IS_ERR(ctx)) {\n\t\tret = PTR_ERR(ctx);\n\t\tgoto err_unlock;\n\t}\n\n\tret = gem_context_register(ctx, file_priv);\n\tif (ret)\n\t\tgoto err_ctx;\n\n\tmutex_unlock(&dev->struct_mutex);\n\n\targs->ctx_id = ctx->user_handle;\n\tDRM_DEBUG(\"HW context %d created\\n\", args->ctx_id);\n\n\treturn 0;\n\nerr_ctx:\n\tcontext_close(ctx);\nerr_unlock:\n\tmutex_unlock(&dev->struct_mutex);\n\treturn ret;\n}",
        "code_after_change": "int i915_gem_context_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t  struct drm_file *file)\n{\n\tstruct drm_i915_private *i915 = to_i915(dev);\n\tstruct drm_i915_gem_context_create *args = data;\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\tstruct i915_gem_context *ctx;\n\tint ret;\n\n\tif (!DRIVER_CAPS(i915)->has_logical_contexts)\n\t\treturn -ENODEV;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tret = i915_terminally_wedged(i915);\n\tif (ret)\n\t\treturn ret;\n\n\tif (client_is_banned(file_priv)) {\n\t\tDRM_DEBUG(\"client %s[%d] banned from creating ctx\\n\",\n\t\t\t  current->comm,\n\t\t\t  pid_nr(get_task_pid(current, PIDTYPE_PID)));\n\n\t\treturn -EIO;\n\t}\n\n\tret = i915_mutex_lock_interruptible(dev);\n\tif (ret)\n\t\treturn ret;\n\n\tctx = i915_gem_create_context(i915);\n\tmutex_unlock(&dev->struct_mutex);\n\tif (IS_ERR(ctx))\n\t\treturn PTR_ERR(ctx);\n\n\tret = gem_context_register(ctx, file_priv);\n\tif (ret)\n\t\tgoto err_ctx;\n\n\targs->ctx_id = ctx->user_handle;\n\tDRM_DEBUG(\"HW context %d created\\n\", args->ctx_id);\n\n\treturn 0;\n\nerr_ctx:\n\tmutex_lock(&dev->struct_mutex);\n\tcontext_close(ctx);\n\tmutex_unlock(&dev->struct_mutex);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,16 +30,13 @@\n \t\treturn ret;\n \n \tctx = i915_gem_create_context(i915);\n-\tif (IS_ERR(ctx)) {\n-\t\tret = PTR_ERR(ctx);\n-\t\tgoto err_unlock;\n-\t}\n+\tmutex_unlock(&dev->struct_mutex);\n+\tif (IS_ERR(ctx))\n+\t\treturn PTR_ERR(ctx);\n \n \tret = gem_context_register(ctx, file_priv);\n \tif (ret)\n \t\tgoto err_ctx;\n-\n-\tmutex_unlock(&dev->struct_mutex);\n \n \targs->ctx_id = ctx->user_handle;\n \tDRM_DEBUG(\"HW context %d created\\n\", args->ctx_id);\n@@ -47,8 +44,8 @@\n \treturn 0;\n \n err_ctx:\n+\tmutex_lock(&dev->struct_mutex);\n \tcontext_close(ctx);\n-err_unlock:\n \tmutex_unlock(&dev->struct_mutex);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_unlock(&dev->struct_mutex);",
                "\tif (IS_ERR(ctx))",
                "\t\treturn PTR_ERR(ctx);",
                "\tmutex_lock(&dev->struct_mutex);"
            ],
            "deleted": [
                "\tif (IS_ERR(ctx)) {",
                "\t\tret = PTR_ERR(ctx);",
                "\t\tgoto err_unlock;",
                "\t}",
                "",
                "\tmutex_unlock(&dev->struct_mutex);",
                "err_unlock:"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 4.14 longterm through 4.14.165 and 4.19 longterm through 4.19.96 (and 5.x before 5.2), there is a use-after-free (write) in the i915_ppgtt_close function in drivers/gpu/drm/i915/i915_gem_gtt.c, aka CID-7dc40713618c. This is related to i915_gem_context_destroy_ioctl in drivers/gpu/drm/i915/i915_gem_context.c.",
        "id": 2800
    },
    {
        "cve_id": "CVE-2022-1652",
        "code_before_change": "static int interpret_errors(void)\n{\n\tchar bad;\n\n\tif (inr != 7) {\n\t\tDPRINT(\"-- FDC reply error\\n\");\n\t\tfdc_state[current_fdc].reset = 1;\n\t\treturn 1;\n\t}\n\n\t/* check IC to find cause of interrupt */\n\tswitch (reply_buffer[ST0] & ST0_INTR) {\n\tcase 0x40:\t\t/* error occurred during command execution */\n\t\tif (reply_buffer[ST1] & ST1_EOC)\n\t\t\treturn 0;\t/* occurs with pseudo-DMA */\n\t\tbad = 1;\n\t\tif (reply_buffer[ST1] & ST1_WP) {\n\t\t\tDPRINT(\"Drive is write protected\\n\");\n\t\t\tclear_bit(FD_DISK_WRITABLE_BIT,\n\t\t\t\t  &drive_state[current_drive].flags);\n\t\t\tcont->done(0);\n\t\t\tbad = 2;\n\t\t} else if (reply_buffer[ST1] & ST1_ND) {\n\t\t\tset_bit(FD_NEED_TWADDLE_BIT,\n\t\t\t\t&drive_state[current_drive].flags);\n\t\t} else if (reply_buffer[ST1] & ST1_OR) {\n\t\t\tif (drive_params[current_drive].flags & FTD_MSG)\n\t\t\t\tDPRINT(\"Over/Underrun - retrying\\n\");\n\t\t\tbad = 0;\n\t\t} else if (*errors >= drive_params[current_drive].max_errors.reporting) {\n\t\t\tprint_errors();\n\t\t}\n\t\tif (reply_buffer[ST2] & ST2_WC || reply_buffer[ST2] & ST2_BC)\n\t\t\t/* wrong cylinder => recal */\n\t\t\tdrive_state[current_drive].track = NEED_2_RECAL;\n\t\treturn bad;\n\tcase 0x80:\t\t/* invalid command given */\n\t\tDPRINT(\"Invalid FDC command given!\\n\");\n\t\tcont->done(0);\n\t\treturn 2;\n\tcase 0xc0:\n\t\tDPRINT(\"Abnormal termination caused by polling\\n\");\n\t\tcont->error();\n\t\treturn 2;\n\tdefault:\t\t/* (0) Normal command termination */\n\t\treturn 0;\n\t}\n}",
        "code_after_change": "static int interpret_errors(void)\n{\n\tchar bad;\n\n\tif (inr != 7) {\n\t\tDPRINT(\"-- FDC reply error\\n\");\n\t\tfdc_state[current_fdc].reset = 1;\n\t\treturn 1;\n\t}\n\n\t/* check IC to find cause of interrupt */\n\tswitch (reply_buffer[ST0] & ST0_INTR) {\n\tcase 0x40:\t\t/* error occurred during command execution */\n\t\tif (reply_buffer[ST1] & ST1_EOC)\n\t\t\treturn 0;\t/* occurs with pseudo-DMA */\n\t\tbad = 1;\n\t\tif (reply_buffer[ST1] & ST1_WP) {\n\t\t\tDPRINT(\"Drive is write protected\\n\");\n\t\t\tclear_bit(FD_DISK_WRITABLE_BIT,\n\t\t\t\t  &drive_state[current_drive].flags);\n\t\t\tcont->done(0);\n\t\t\tbad = 2;\n\t\t} else if (reply_buffer[ST1] & ST1_ND) {\n\t\t\tset_bit(FD_NEED_TWADDLE_BIT,\n\t\t\t\t&drive_state[current_drive].flags);\n\t\t} else if (reply_buffer[ST1] & ST1_OR) {\n\t\t\tif (drive_params[current_drive].flags & FTD_MSG)\n\t\t\t\tDPRINT(\"Over/Underrun - retrying\\n\");\n\t\t\tbad = 0;\n\t\t} else if (floppy_errors >= drive_params[current_drive].max_errors.reporting) {\n\t\t\tprint_errors();\n\t\t}\n\t\tif (reply_buffer[ST2] & ST2_WC || reply_buffer[ST2] & ST2_BC)\n\t\t\t/* wrong cylinder => recal */\n\t\t\tdrive_state[current_drive].track = NEED_2_RECAL;\n\t\treturn bad;\n\tcase 0x80:\t\t/* invalid command given */\n\t\tDPRINT(\"Invalid FDC command given!\\n\");\n\t\tcont->done(0);\n\t\treturn 2;\n\tcase 0xc0:\n\t\tDPRINT(\"Abnormal termination caused by polling\\n\");\n\t\tcont->error();\n\t\treturn 2;\n\tdefault:\t\t/* (0) Normal command termination */\n\t\treturn 0;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,7 +27,7 @@\n \t\t\tif (drive_params[current_drive].flags & FTD_MSG)\n \t\t\t\tDPRINT(\"Over/Underrun - retrying\\n\");\n \t\t\tbad = 0;\n-\t\t} else if (*errors >= drive_params[current_drive].max_errors.reporting) {\n+\t\t} else if (floppy_errors >= drive_params[current_drive].max_errors.reporting) {\n \t\t\tprint_errors();\n \t\t}\n \t\tif (reply_buffer[ST2] & ST2_WC || reply_buffer[ST2] & ST2_BC)",
        "function_modified_lines": {
            "added": [
                "\t\t} else if (floppy_errors >= drive_params[current_drive].max_errors.reporting) {"
            ],
            "deleted": [
                "\t\t} else if (*errors >= drive_params[current_drive].max_errors.reporting) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Linux Kernel could allow a local attacker to execute arbitrary code on the system, caused by a concurrency use-after-free flaw in the bad_flp_intr function. By executing a specially-crafted program, an attacker could exploit this vulnerability to execute arbitrary code or cause a denial of service condition on the system.",
        "id": 3270
    },
    {
        "cve_id": "CVE-2019-15215",
        "code_before_change": "static void cpia2_usb_disconnect(struct usb_interface *intf)\n{\n\tstruct camera_data *cam = usb_get_intfdata(intf);\n\tusb_set_intfdata(intf, NULL);\n\n\tDBG(\"Stopping stream\\n\");\n\tcpia2_usb_stream_stop(cam);\n\n\tmutex_lock(&cam->v4l2_lock);\n\tDBG(\"Unregistering camera\\n\");\n\tcpia2_unregister_camera(cam);\n\tv4l2_device_disconnect(&cam->v4l2_dev);\n\tmutex_unlock(&cam->v4l2_lock);\n\tv4l2_device_put(&cam->v4l2_dev);\n\n\tif(cam->buffers) {\n\t\tDBG(\"Wakeup waiting processes\\n\");\n\t\tcam->curbuff->status = FRAME_READY;\n\t\tcam->curbuff->length = 0;\n\t\twake_up_interruptible(&cam->wq_stream);\n\t}\n\n\tLOG(\"CPiA2 camera disconnected.\\n\");\n}",
        "code_after_change": "static void cpia2_usb_disconnect(struct usb_interface *intf)\n{\n\tstruct camera_data *cam = usb_get_intfdata(intf);\n\tusb_set_intfdata(intf, NULL);\n\n\tDBG(\"Stopping stream\\n\");\n\tcpia2_usb_stream_stop(cam);\n\n\tmutex_lock(&cam->v4l2_lock);\n\tDBG(\"Unregistering camera\\n\");\n\tcpia2_unregister_camera(cam);\n\tv4l2_device_disconnect(&cam->v4l2_dev);\n\tmutex_unlock(&cam->v4l2_lock);\n\n\tif(cam->buffers) {\n\t\tDBG(\"Wakeup waiting processes\\n\");\n\t\tcam->curbuff->status = FRAME_READY;\n\t\tcam->curbuff->length = 0;\n\t\twake_up_interruptible(&cam->wq_stream);\n\t}\n\n\tv4l2_device_put(&cam->v4l2_dev);\n\n\tLOG(\"CPiA2 camera disconnected.\\n\");\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,6 @@\n \tcpia2_unregister_camera(cam);\n \tv4l2_device_disconnect(&cam->v4l2_dev);\n \tmutex_unlock(&cam->v4l2_lock);\n-\tv4l2_device_put(&cam->v4l2_dev);\n \n \tif(cam->buffers) {\n \t\tDBG(\"Wakeup waiting processes\\n\");\n@@ -20,5 +19,7 @@\n \t\twake_up_interruptible(&cam->wq_stream);\n \t}\n \n+\tv4l2_device_put(&cam->v4l2_dev);\n+\n \tLOG(\"CPiA2 camera disconnected.\\n\");\n }",
        "function_modified_lines": {
            "added": [
                "\tv4l2_device_put(&cam->v4l2_dev);",
                ""
            ],
            "deleted": [
                "\tv4l2_device_put(&cam->v4l2_dev);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.2.6. There is a use-after-free caused by a malicious USB device in the drivers/media/usb/cpia2/cpia2_usb.c driver.",
        "id": 1997
    },
    {
        "cve_id": "CVE-2016-7913",
        "code_before_change": "static int xc2028_set_config(struct dvb_frontend *fe, void *priv_cfg)\n{\n\tstruct xc2028_data *priv = fe->tuner_priv;\n\tstruct xc2028_ctrl *p    = priv_cfg;\n\tint                 rc   = 0;\n\n\ttuner_dbg(\"%s called\\n\", __func__);\n\n\tmutex_lock(&priv->lock);\n\n\t/*\n\t * Copy the config data.\n\t * For the firmware name, keep a local copy of the string,\n\t * in order to avoid troubles during device release.\n\t */\n\tkfree(priv->ctrl.fname);\n\tmemcpy(&priv->ctrl, p, sizeof(priv->ctrl));\n\tif (p->fname) {\n\t\tpriv->ctrl.fname = kstrdup(p->fname, GFP_KERNEL);\n\t\tif (priv->ctrl.fname == NULL)\n\t\t\trc = -ENOMEM;\n\t}\n\n\t/*\n\t * If firmware name changed, frees firmware. As free_firmware will\n\t * reset the status to NO_FIRMWARE, this forces a new request_firmware\n\t */\n\tif (!firmware_name[0] && p->fname &&\n\t    priv->fname && strcmp(p->fname, priv->fname))\n\t\tfree_firmware(priv);\n\n\tif (priv->ctrl.max_len < 9)\n\t\tpriv->ctrl.max_len = 13;\n\n\tif (priv->state == XC2028_NO_FIRMWARE) {\n\t\tif (!firmware_name[0])\n\t\t\tpriv->fname = priv->ctrl.fname;\n\t\telse\n\t\t\tpriv->fname = firmware_name;\n\n\t\trc = request_firmware_nowait(THIS_MODULE, 1,\n\t\t\t\t\t     priv->fname,\n\t\t\t\t\t     priv->i2c_props.adap->dev.parent,\n\t\t\t\t\t     GFP_KERNEL,\n\t\t\t\t\t     fe, load_firmware_cb);\n\t\tif (rc < 0) {\n\t\t\ttuner_err(\"Failed to request firmware %s\\n\",\n\t\t\t\t  priv->fname);\n\t\t\tpriv->state = XC2028_NODEV;\n\t\t} else\n\t\t\tpriv->state = XC2028_WAITING_FIRMWARE;\n\t}\n\tmutex_unlock(&priv->lock);\n\n\treturn rc;\n}",
        "code_after_change": "static int xc2028_set_config(struct dvb_frontend *fe, void *priv_cfg)\n{\n\tstruct xc2028_data *priv = fe->tuner_priv;\n\tstruct xc2028_ctrl *p    = priv_cfg;\n\tint                 rc   = 0;\n\n\ttuner_dbg(\"%s called\\n\", __func__);\n\n\tmutex_lock(&priv->lock);\n\n\t/*\n\t * Copy the config data.\n\t * For the firmware name, keep a local copy of the string,\n\t * in order to avoid troubles during device release.\n\t */\n\tkfree(priv->ctrl.fname);\n\tpriv->ctrl.fname = NULL;\n\tmemcpy(&priv->ctrl, p, sizeof(priv->ctrl));\n\tif (p->fname) {\n\t\tpriv->ctrl.fname = kstrdup(p->fname, GFP_KERNEL);\n\t\tif (priv->ctrl.fname == NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * If firmware name changed, frees firmware. As free_firmware will\n\t * reset the status to NO_FIRMWARE, this forces a new request_firmware\n\t */\n\tif (!firmware_name[0] && p->fname &&\n\t    priv->fname && strcmp(p->fname, priv->fname))\n\t\tfree_firmware(priv);\n\n\tif (priv->ctrl.max_len < 9)\n\t\tpriv->ctrl.max_len = 13;\n\n\tif (priv->state == XC2028_NO_FIRMWARE) {\n\t\tif (!firmware_name[0])\n\t\t\tpriv->fname = priv->ctrl.fname;\n\t\telse\n\t\t\tpriv->fname = firmware_name;\n\n\t\trc = request_firmware_nowait(THIS_MODULE, 1,\n\t\t\t\t\t     priv->fname,\n\t\t\t\t\t     priv->i2c_props.adap->dev.parent,\n\t\t\t\t\t     GFP_KERNEL,\n\t\t\t\t\t     fe, load_firmware_cb);\n\t\tif (rc < 0) {\n\t\t\ttuner_err(\"Failed to request firmware %s\\n\",\n\t\t\t\t  priv->fname);\n\t\t\tpriv->state = XC2028_NODEV;\n\t\t} else\n\t\t\tpriv->state = XC2028_WAITING_FIRMWARE;\n\t}\n\tmutex_unlock(&priv->lock);\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,11 +14,12 @@\n \t * in order to avoid troubles during device release.\n \t */\n \tkfree(priv->ctrl.fname);\n+\tpriv->ctrl.fname = NULL;\n \tmemcpy(&priv->ctrl, p, sizeof(priv->ctrl));\n \tif (p->fname) {\n \t\tpriv->ctrl.fname = kstrdup(p->fname, GFP_KERNEL);\n \t\tif (priv->ctrl.fname == NULL)\n-\t\t\trc = -ENOMEM;\n+\t\t\treturn -ENOMEM;\n \t}\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "\tpriv->ctrl.fname = NULL;",
                "\t\t\treturn -ENOMEM;"
            ],
            "deleted": [
                "\t\t\trc = -ENOMEM;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The xc2028_set_config function in drivers/media/tuners/tuner-xc2028.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) via vectors involving omission of the firmware name from a certain data structure.",
        "id": 1112
    },
    {
        "cve_id": "CVE-2017-15115",
        "code_before_change": "int sctp_do_peeloff(struct sock *sk, sctp_assoc_t id, struct socket **sockp)\n{\n\tstruct sctp_association *asoc = sctp_id2assoc(sk, id);\n\tstruct sctp_sock *sp = sctp_sk(sk);\n\tstruct socket *sock;\n\tint err = 0;\n\n\tif (!asoc)\n\t\treturn -EINVAL;\n\n\t/* If there is a thread waiting on more sndbuf space for\n\t * sending on this asoc, it cannot be peeled.\n\t */\n\tif (waitqueue_active(&asoc->wait))\n\t\treturn -EBUSY;\n\n\t/* An association cannot be branched off from an already peeled-off\n\t * socket, nor is this supported for tcp style sockets.\n\t */\n\tif (!sctp_style(sk, UDP))\n\t\treturn -EINVAL;\n\n\t/* Create a new socket.  */\n\terr = sock_create(sk->sk_family, SOCK_SEQPACKET, IPPROTO_SCTP, &sock);\n\tif (err < 0)\n\t\treturn err;\n\n\tsctp_copy_sock(sock->sk, sk, asoc);\n\n\t/* Make peeled-off sockets more like 1-1 accepted sockets.\n\t * Set the daddr and initialize id to something more random\n\t */\n\tsp->pf->to_sk_daddr(&asoc->peer.primary_addr, sk);\n\n\t/* Populate the fields of the newsk from the oldsk and migrate the\n\t * asoc to the newsk.\n\t */\n\tsctp_sock_migrate(sk, sock->sk, asoc, SCTP_SOCKET_UDP_HIGH_BANDWIDTH);\n\n\t*sockp = sock;\n\n\treturn err;\n}",
        "code_after_change": "int sctp_do_peeloff(struct sock *sk, sctp_assoc_t id, struct socket **sockp)\n{\n\tstruct sctp_association *asoc = sctp_id2assoc(sk, id);\n\tstruct sctp_sock *sp = sctp_sk(sk);\n\tstruct socket *sock;\n\tint err = 0;\n\n\t/* Do not peel off from one netns to another one. */\n\tif (!net_eq(current->nsproxy->net_ns, sock_net(sk)))\n\t\treturn -EINVAL;\n\n\tif (!asoc)\n\t\treturn -EINVAL;\n\n\t/* If there is a thread waiting on more sndbuf space for\n\t * sending on this asoc, it cannot be peeled.\n\t */\n\tif (waitqueue_active(&asoc->wait))\n\t\treturn -EBUSY;\n\n\t/* An association cannot be branched off from an already peeled-off\n\t * socket, nor is this supported for tcp style sockets.\n\t */\n\tif (!sctp_style(sk, UDP))\n\t\treturn -EINVAL;\n\n\t/* Create a new socket.  */\n\terr = sock_create(sk->sk_family, SOCK_SEQPACKET, IPPROTO_SCTP, &sock);\n\tif (err < 0)\n\t\treturn err;\n\n\tsctp_copy_sock(sock->sk, sk, asoc);\n\n\t/* Make peeled-off sockets more like 1-1 accepted sockets.\n\t * Set the daddr and initialize id to something more random\n\t */\n\tsp->pf->to_sk_daddr(&asoc->peer.primary_addr, sk);\n\n\t/* Populate the fields of the newsk from the oldsk and migrate the\n\t * asoc to the newsk.\n\t */\n\tsctp_sock_migrate(sk, sock->sk, asoc, SCTP_SOCKET_UDP_HIGH_BANDWIDTH);\n\n\t*sockp = sock;\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,10 @@\n \tstruct sctp_sock *sp = sctp_sk(sk);\n \tstruct socket *sock;\n \tint err = 0;\n+\n+\t/* Do not peel off from one netns to another one. */\n+\tif (!net_eq(current->nsproxy->net_ns, sock_net(sk)))\n+\t\treturn -EINVAL;\n \n \tif (!asoc)\n \t\treturn -EINVAL;",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* Do not peel off from one netns to another one. */",
                "\tif (!net_eq(current->nsproxy->net_ns, sock_net(sk)))",
                "\t\treturn -EINVAL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The sctp_do_peeloff function in net/sctp/socket.c in the Linux kernel before 4.14 does not check whether the intended netns is used in a peel-off action, which allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via crafted system calls.",
        "id": 1289
    },
    {
        "cve_id": "CVE-2023-0468",
        "code_before_change": "static inline bool io_poll_get_ownership(struct io_kiocb *req)\n{\n\treturn !(atomic_fetch_inc(&req->poll_refs) & IO_POLL_REF_MASK);\n}",
        "code_after_change": "static inline bool io_poll_get_ownership(struct io_kiocb *req)\n{\n\tif (unlikely(atomic_read(&req->poll_refs) >= IO_POLL_REF_BIAS))\n\t\treturn io_poll_get_ownership_slowpath(req);\n\treturn !(atomic_fetch_inc(&req->poll_refs) & IO_POLL_REF_MASK);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,6 @@\n static inline bool io_poll_get_ownership(struct io_kiocb *req)\n {\n+\tif (unlikely(atomic_read(&req->poll_refs) >= IO_POLL_REF_BIAS))\n+\t\treturn io_poll_get_ownership_slowpath(req);\n \treturn !(atomic_fetch_inc(&req->poll_refs) & IO_POLL_REF_MASK);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(atomic_read(&req->poll_refs) >= IO_POLL_REF_BIAS))",
                "\t\treturn io_poll_get_ownership_slowpath(req);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in io_uring/poll.c in io_poll_check_events in the io_uring subcomponent in the Linux Kernel due to a race condition of poll_refs. This flaw may cause a NULL pointer dereference.",
        "id": 3831
    },
    {
        "cve_id": "CVE-2022-22942",
        "code_before_change": "void vmw_kms_helper_validation_finish(struct vmw_private *dev_priv,\n\t\t\t\t      struct drm_file *file_priv,\n\t\t\t\t      struct vmw_validation_context *ctx,\n\t\t\t\t      struct vmw_fence_obj **out_fence,\n\t\t\t\t      struct drm_vmw_fence_rep __user *\n\t\t\t\t      user_fence_rep)\n{\n\tstruct vmw_fence_obj *fence = NULL;\n\tuint32_t handle = 0;\n\tint ret = 0;\n\n\tif (file_priv || user_fence_rep || vmw_validation_has_bos(ctx) ||\n\t    out_fence)\n\t\tret = vmw_execbuf_fence_commands(file_priv, dev_priv, &fence,\n\t\t\t\t\t\t file_priv ? &handle : NULL);\n\tvmw_validation_done(ctx, fence);\n\tif (file_priv)\n\t\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv),\n\t\t\t\t\t    ret, user_fence_rep, fence,\n\t\t\t\t\t    handle, -1, NULL);\n\tif (out_fence)\n\t\t*out_fence = fence;\n\telse\n\t\tvmw_fence_obj_unreference(&fence);\n}",
        "code_after_change": "void vmw_kms_helper_validation_finish(struct vmw_private *dev_priv,\n\t\t\t\t      struct drm_file *file_priv,\n\t\t\t\t      struct vmw_validation_context *ctx,\n\t\t\t\t      struct vmw_fence_obj **out_fence,\n\t\t\t\t      struct drm_vmw_fence_rep __user *\n\t\t\t\t      user_fence_rep)\n{\n\tstruct vmw_fence_obj *fence = NULL;\n\tuint32_t handle = 0;\n\tint ret = 0;\n\n\tif (file_priv || user_fence_rep || vmw_validation_has_bos(ctx) ||\n\t    out_fence)\n\t\tret = vmw_execbuf_fence_commands(file_priv, dev_priv, &fence,\n\t\t\t\t\t\t file_priv ? &handle : NULL);\n\tvmw_validation_done(ctx, fence);\n\tif (file_priv)\n\t\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv),\n\t\t\t\t\t    ret, user_fence_rep, fence,\n\t\t\t\t\t    handle, -1);\n\tif (out_fence)\n\t\t*out_fence = fence;\n\telse\n\t\tvmw_fence_obj_unreference(&fence);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,7 +17,7 @@\n \tif (file_priv)\n \t\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv),\n \t\t\t\t\t    ret, user_fence_rep, fence,\n-\t\t\t\t\t    handle, -1, NULL);\n+\t\t\t\t\t    handle, -1);\n \tif (out_fence)\n \t\t*out_fence = fence;\n \telse",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\t    handle, -1);"
            ],
            "deleted": [
                "\t\t\t\t\t    handle, -1, NULL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The vmwgfx driver contains a local privilege escalation vulnerability that allows unprivileged users to gain access to files opened by other processes on the system through a dangling 'file' pointer.",
        "id": 3418
    },
    {
        "cve_id": "CVE-2018-20976",
        "code_before_change": "STATIC void\nxfs_fs_put_super(\n\tstruct super_block\t*sb)\n{\n\tstruct xfs_mount\t*mp = XFS_M(sb);\n\n\txfs_notice(mp, \"Unmounting Filesystem\");\n\txfs_filestream_unmount(mp);\n\txfs_unmountfs(mp);\n\n\txfs_freesb(mp);\n\tfree_percpu(mp->m_stats.xs_stats);\n\txfs_destroy_percpu_counters(mp);\n\txfs_destroy_mount_workqueues(mp);\n\txfs_close_devices(mp);\n\txfs_free_fsname(mp);\n\tkfree(mp);\n}",
        "code_after_change": "STATIC void\nxfs_fs_put_super(\n\tstruct super_block\t*sb)\n{\n\tstruct xfs_mount\t*mp = XFS_M(sb);\n\n\t/* if ->fill_super failed, we have no mount to tear down */\n\tif (!sb->s_fs_info)\n\t\treturn;\n\n\txfs_notice(mp, \"Unmounting Filesystem\");\n\txfs_filestream_unmount(mp);\n\txfs_unmountfs(mp);\n\n\txfs_freesb(mp);\n\tfree_percpu(mp->m_stats.xs_stats);\n\txfs_destroy_percpu_counters(mp);\n\txfs_destroy_mount_workqueues(mp);\n\txfs_close_devices(mp);\n\n\tsb->s_fs_info = NULL;\n\txfs_free_fsname(mp);\n\tkfree(mp);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,10 @@\n \tstruct super_block\t*sb)\n {\n \tstruct xfs_mount\t*mp = XFS_M(sb);\n+\n+\t/* if ->fill_super failed, we have no mount to tear down */\n+\tif (!sb->s_fs_info)\n+\t\treturn;\n \n \txfs_notice(mp, \"Unmounting Filesystem\");\n \txfs_filestream_unmount(mp);\n@@ -13,6 +17,8 @@\n \txfs_destroy_percpu_counters(mp);\n \txfs_destroy_mount_workqueues(mp);\n \txfs_close_devices(mp);\n+\n+\tsb->s_fs_info = NULL;\n \txfs_free_fsname(mp);\n \tkfree(mp);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* if ->fill_super failed, we have no mount to tear down */",
                "\tif (!sb->s_fs_info)",
                "\t\treturn;",
                "",
                "\tsb->s_fs_info = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in fs/xfs/xfs_super.c in the Linux kernel before 4.18. A use after free exists, related to xfs_fs_fill_super failure.",
        "id": 1790
    },
    {
        "cve_id": "CVE-2021-0935",
        "code_before_change": "int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t\t*daddr;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\t__be32\t\t\tfl6_flowlabel = 0;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tif (np->sndflow)\n\t\tfl6_flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\n\tif (ipv6_addr_any(&usin->sin6_addr)) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\n\t\t\tipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n\t\t\t\t\t       &usin->sin6_addr);\n\t\telse\n\t\t\tusin->sin6_addr = in6addr_loopback;\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6_flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\terr = ip6_datagram_dst_update(sk, true);\n\tif (err) {\n\t\t/* Reset daddr and dport so that udp_v6_early_demux()\n\t\t * fails to find this socket\n\t\t */\n\t\tmemset(&sk->sk_v6_daddr, 0, sizeof(sk->sk_v6_daddr));\n\t\tinet->inet_dport = 0;\n\t\tgoto out;\n\t}\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\treturn err;\n}",
        "code_after_change": "int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t\t*daddr, old_daddr;\n\t__be32\t\t\tfl6_flowlabel = 0;\n\t__be32\t\t\told_fl6_flowlabel;\n\t__be32\t\t\told_dport;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tif (np->sndflow)\n\t\tfl6_flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\n\tif (ipv6_addr_any(&usin->sin6_addr)) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\n\t\t\tipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n\t\t\t\t\t       &usin->sin6_addr);\n\t\telse\n\t\t\tusin->sin6_addr = in6addr_loopback;\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* save the current peer information before updating it */\n\told_daddr = sk->sk_v6_daddr;\n\told_fl6_flowlabel = np->flow_label;\n\told_dport = inet->inet_dport;\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6_flowlabel;\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\terr = ip6_datagram_dst_update(sk, true);\n\tif (err) {\n\t\t/* Restore the socket peer info, to keep it consistent with\n\t\t * the old socket state\n\t\t */\n\t\tsk->sk_v6_daddr = old_daddr;\n\t\tnp->flow_label = old_fl6_flowlabel;\n\t\tinet->inet_dport = old_dport;\n\t\tgoto out;\n\t}\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,10 +4,12 @@\n \tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n \tstruct inet_sock\t*inet = inet_sk(sk);\n \tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n-\tstruct in6_addr\t\t*daddr;\n+\tstruct in6_addr\t\t*daddr, old_daddr;\n+\t__be32\t\t\tfl6_flowlabel = 0;\n+\t__be32\t\t\told_fl6_flowlabel;\n+\t__be32\t\t\told_dport;\n \tint\t\t\taddr_type;\n \tint\t\t\terr;\n-\t__be32\t\t\tfl6_flowlabel = 0;\n \n \tif (usin->sin6_family == AF_INET) {\n \t\tif (__ipv6_only_sock(sk))\n@@ -96,9 +98,13 @@\n \t\t}\n \t}\n \n+\t/* save the current peer information before updating it */\n+\told_daddr = sk->sk_v6_daddr;\n+\told_fl6_flowlabel = np->flow_label;\n+\told_dport = inet->inet_dport;\n+\n \tsk->sk_v6_daddr = *daddr;\n \tnp->flow_label = fl6_flowlabel;\n-\n \tinet->inet_dport = usin->sin6_port;\n \n \t/*\n@@ -108,11 +114,12 @@\n \n \terr = ip6_datagram_dst_update(sk, true);\n \tif (err) {\n-\t\t/* Reset daddr and dport so that udp_v6_early_demux()\n-\t\t * fails to find this socket\n+\t\t/* Restore the socket peer info, to keep it consistent with\n+\t\t * the old socket state\n \t\t */\n-\t\tmemset(&sk->sk_v6_daddr, 0, sizeof(sk->sk_v6_daddr));\n-\t\tinet->inet_dport = 0;\n+\t\tsk->sk_v6_daddr = old_daddr;\n+\t\tnp->flow_label = old_fl6_flowlabel;\n+\t\tinet->inet_dport = old_dport;\n \t\tgoto out;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct in6_addr\t\t*daddr, old_daddr;",
                "\t__be32\t\t\tfl6_flowlabel = 0;",
                "\t__be32\t\t\told_fl6_flowlabel;",
                "\t__be32\t\t\told_dport;",
                "\t/* save the current peer information before updating it */",
                "\told_daddr = sk->sk_v6_daddr;",
                "\told_fl6_flowlabel = np->flow_label;",
                "\told_dport = inet->inet_dport;",
                "",
                "\t\t/* Restore the socket peer info, to keep it consistent with",
                "\t\t * the old socket state",
                "\t\tsk->sk_v6_daddr = old_daddr;",
                "\t\tnp->flow_label = old_fl6_flowlabel;",
                "\t\tinet->inet_dport = old_dport;"
            ],
            "deleted": [
                "\tstruct in6_addr\t\t*daddr;",
                "\t__be32\t\t\tfl6_flowlabel = 0;",
                "",
                "\t\t/* Reset daddr and dport so that udp_v6_early_demux()",
                "\t\t * fails to find this socket",
                "\t\tmemset(&sk->sk_v6_daddr, 0, sizeof(sk->sk_v6_daddr));",
                "\t\tinet->inet_dport = 0;"
            ]
        },
        "cwe": [
            "CWE-787",
            "CWE-416"
        ],
        "cve_description": "In ip6_xmit of ip6_output.c, there is a possible out of bounds write due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-168607263References: Upstream kernel",
        "id": 2832
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static void __io_worker_idle(struct io_wqe *wqe, struct io_worker *worker)\n\t__must_hold(wqe->lock)\n{\n\tif (!(worker->flags & IO_WORKER_F_FREE)) {\n\t\tworker->flags |= IO_WORKER_F_FREE;\n\t\thlist_nulls_add_head_rcu(&worker->nulls_node, &wqe->free_list);\n\t}\n}",
        "code_after_change": "static void __io_worker_idle(struct io_wqe *wqe, struct io_worker *worker)\n\t__must_hold(wqe->lock)\n{\n\tif (!(worker->flags & IO_WORKER_F_FREE)) {\n\t\tworker->flags |= IO_WORKER_F_FREE;\n\t\thlist_nulls_add_head_rcu(&worker->nulls_node, &wqe->free_list);\n\t}\n\tif (worker->saved_creds) {\n\t\trevert_creds(worker->saved_creds);\n\t\tworker->cur_creds = worker->saved_creds = NULL;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,4 +5,8 @@\n \t\tworker->flags |= IO_WORKER_F_FREE;\n \t\thlist_nulls_add_head_rcu(&worker->nulls_node, &wqe->free_list);\n \t}\n+\tif (worker->saved_creds) {\n+\t\trevert_creds(worker->saved_creds);\n+\t\tworker->cur_creds = worker->saved_creds = NULL;\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tif (worker->saved_creds) {",
                "\t\trevert_creds(worker->saved_creds);",
                "\t\tworker->cur_creds = worker->saved_creds = NULL;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3278
    },
    {
        "cve_id": "CVE-2020-27784",
        "code_before_change": "static void gprinter_free(struct usb_function *f)\n{\n\tstruct printer_dev *dev = func_to_printer(f);\n\tstruct f_printer_opts *opts;\n\n\topts = container_of(f->fi, struct f_printer_opts, func_inst);\n\tkfree(dev);\n\tmutex_lock(&opts->lock);\n\t--opts->refcnt;\n\tmutex_unlock(&opts->lock);\n}",
        "code_after_change": "static void gprinter_free(struct usb_function *f)\n{\n\tstruct printer_dev *dev = func_to_printer(f);\n\tstruct f_printer_opts *opts;\n\n\topts = container_of(f->fi, struct f_printer_opts, func_inst);\n\n\tkref_put(&dev->kref, printer_dev_free);\n\tmutex_lock(&opts->lock);\n\t--opts->refcnt;\n\tmutex_unlock(&opts->lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,8 @@\n \tstruct f_printer_opts *opts;\n \n \topts = container_of(f->fi, struct f_printer_opts, func_inst);\n-\tkfree(dev);\n+\n+\tkref_put(&dev->kref, printer_dev_free);\n \tmutex_lock(&opts->lock);\n \t--opts->refcnt;\n \tmutex_unlock(&opts->lock);",
        "function_modified_lines": {
            "added": [
                "",
                "\tkref_put(&dev->kref, printer_dev_free);"
            ],
            "deleted": [
                "\tkfree(dev);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel, where accessing a deallocated instance in printer_ioctl() printer_ioctl() tries to access of a printer_dev instance. However, use-after-free arises because it had been freed by gprinter_free().",
        "id": 2632
    },
    {
        "cve_id": "CVE-2023-20938",
        "code_before_change": "static int binder_translate_fd_array(struct list_head *pf_head,\n\t\t\t\t     struct binder_fd_array_object *fda,\n\t\t\t\t     const void __user *sender_ubuffer,\n\t\t\t\t     struct binder_buffer_object *parent,\n\t\t\t\t     struct binder_buffer_object *sender_uparent,\n\t\t\t\t     struct binder_transaction *t,\n\t\t\t\t     struct binder_thread *thread,\n\t\t\t\t     struct binder_transaction *in_reply_to)\n{\n\tbinder_size_t fdi, fd_buf_size;\n\tbinder_size_t fda_offset;\n\tconst void __user *sender_ufda_base;\n\tstruct binder_proc *proc = thread->proc;\n\tint ret;\n\n\tfd_buf_size = sizeof(u32) * fda->num_fds;\n\tif (fda->num_fds >= SIZE_MAX / sizeof(u32)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid number of fds (%lld)\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fda->num_fds);\n\t\treturn -EINVAL;\n\t}\n\tif (fd_buf_size > parent->length ||\n\t    fda->parent_offset > parent->length - fd_buf_size) {\n\t\t/* No space for all file descriptors here. */\n\t\tbinder_user_error(\"%d:%d not enough space to store %lld fds in buffer\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fda->num_fds);\n\t\treturn -EINVAL;\n\t}\n\t/*\n\t * the source data for binder_buffer_object is visible\n\t * to user-space and the @buffer element is the user\n\t * pointer to the buffer_object containing the fd_array.\n\t * Convert the address to an offset relative to\n\t * the base of the transaction buffer.\n\t */\n\tfda_offset = (parent->buffer - (uintptr_t)t->buffer->user_data) +\n\t\tfda->parent_offset;\n\tsender_ufda_base = (void __user *)(uintptr_t)sender_uparent->buffer +\n\t\t\t\tfda->parent_offset;\n\n\tif (!IS_ALIGNED((unsigned long)fda_offset, sizeof(u32)) ||\n\t    !IS_ALIGNED((unsigned long)sender_ufda_base, sizeof(u32))) {\n\t\tbinder_user_error(\"%d:%d parent offset not aligned correctly.\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\tret = binder_add_fixup(pf_head, fda_offset, 0, fda->num_fds * sizeof(u32));\n\tif (ret)\n\t\treturn ret;\n\n\tfor (fdi = 0; fdi < fda->num_fds; fdi++) {\n\t\tu32 fd;\n\t\tbinder_size_t offset = fda_offset + fdi * sizeof(fd);\n\t\tbinder_size_t sender_uoffset = fdi * sizeof(fd);\n\n\t\tret = copy_from_user(&fd, sender_ufda_base + sender_uoffset, sizeof(fd));\n\t\tif (!ret)\n\t\t\tret = binder_translate_fd(fd, offset, t, thread,\n\t\t\t\t\t\t  in_reply_to);\n\t\tif (ret)\n\t\t\treturn ret > 0 ? -EINVAL : ret;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int binder_translate_fd_array(struct list_head *pf_head,\n\t\t\t\t     struct binder_fd_array_object *fda,\n\t\t\t\t     const void __user *sender_ubuffer,\n\t\t\t\t     struct binder_buffer_object *parent,\n\t\t\t\t     struct binder_buffer_object *sender_uparent,\n\t\t\t\t     struct binder_transaction *t,\n\t\t\t\t     struct binder_thread *thread,\n\t\t\t\t     struct binder_transaction *in_reply_to)\n{\n\tbinder_size_t fdi, fd_buf_size;\n\tbinder_size_t fda_offset;\n\tconst void __user *sender_ufda_base;\n\tstruct binder_proc *proc = thread->proc;\n\tint ret;\n\n\tif (fda->num_fds == 0)\n\t\treturn 0;\n\n\tfd_buf_size = sizeof(u32) * fda->num_fds;\n\tif (fda->num_fds >= SIZE_MAX / sizeof(u32)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid number of fds (%lld)\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fda->num_fds);\n\t\treturn -EINVAL;\n\t}\n\tif (fd_buf_size > parent->length ||\n\t    fda->parent_offset > parent->length - fd_buf_size) {\n\t\t/* No space for all file descriptors here. */\n\t\tbinder_user_error(\"%d:%d not enough space to store %lld fds in buffer\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fda->num_fds);\n\t\treturn -EINVAL;\n\t}\n\t/*\n\t * the source data for binder_buffer_object is visible\n\t * to user-space and the @buffer element is the user\n\t * pointer to the buffer_object containing the fd_array.\n\t * Convert the address to an offset relative to\n\t * the base of the transaction buffer.\n\t */\n\tfda_offset = (parent->buffer - (uintptr_t)t->buffer->user_data) +\n\t\tfda->parent_offset;\n\tsender_ufda_base = (void __user *)(uintptr_t)sender_uparent->buffer +\n\t\t\t\tfda->parent_offset;\n\n\tif (!IS_ALIGNED((unsigned long)fda_offset, sizeof(u32)) ||\n\t    !IS_ALIGNED((unsigned long)sender_ufda_base, sizeof(u32))) {\n\t\tbinder_user_error(\"%d:%d parent offset not aligned correctly.\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\tret = binder_add_fixup(pf_head, fda_offset, 0, fda->num_fds * sizeof(u32));\n\tif (ret)\n\t\treturn ret;\n\n\tfor (fdi = 0; fdi < fda->num_fds; fdi++) {\n\t\tu32 fd;\n\t\tbinder_size_t offset = fda_offset + fdi * sizeof(fd);\n\t\tbinder_size_t sender_uoffset = fdi * sizeof(fd);\n\n\t\tret = copy_from_user(&fd, sender_ufda_base + sender_uoffset, sizeof(fd));\n\t\tif (!ret)\n\t\t\tret = binder_translate_fd(fd, offset, t, thread,\n\t\t\t\t\t\t  in_reply_to);\n\t\tif (ret)\n\t\t\treturn ret > 0 ? -EINVAL : ret;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,9 @@\n \tconst void __user *sender_ufda_base;\n \tstruct binder_proc *proc = thread->proc;\n \tint ret;\n+\n+\tif (fda->num_fds == 0)\n+\t\treturn 0;\n \n \tfd_buf_size = sizeof(u32) * fda->num_fds;\n \tif (fda->num_fds >= SIZE_MAX / sizeof(u32)) {",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (fda->num_fds == 0)",
                "\t\treturn 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In binder_transaction_buffer_release of binder.c, there is a possible use after free due to improper input validation. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-257685302References: Upstream kernel",
        "id": 3912
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "static int vivid_thread_vid_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\tint dropped_bufs;\n\n\tdprintk(dev, 1, \"Video Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->cap_seq_offset = 0;\n\tdev->cap_seq_count = 0;\n\tdev->cap_seq_resync = false;\n\tdev->jiffies_vid_cap = jiffies;\n\tdev->cap_stream_start = ktime_get_ns();\n\tvivid_cap_update_frame_period(dev);\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->mutex);\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->cap_seq_resync) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = dev->cap_seq_count + 1;\n\t\t\tdev->cap_seq_count = 0;\n\t\t\tdev->cap_stream_start += dev->cap_frame_period *\n\t\t\t\t\t\t dev->cap_seq_offset;\n\t\t\tvivid_cap_update_frame_period(dev);\n\t\t\tdev->cap_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_cap.numerator;\n\t\tdenominator = dev->timeperframe_vid_cap.denominator;\n\n\t\tif (dev->field_cap == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdropped_bufs = buffers_since_start + dev->cap_seq_offset - dev->cap_seq_count;\n\t\tdev->cap_seq_count = buffers_since_start + dev->cap_seq_offset;\n\t\tdev->vid_cap_seq_count = dev->cap_seq_count - dev->vid_cap_seq_start;\n\t\tdev->vbi_cap_seq_count = dev->cap_seq_count - dev->vbi_cap_seq_start;\n\t\tdev->meta_cap_seq_count = dev->cap_seq_count - dev->meta_cap_seq_start;\n\n\t\tvivid_thread_vid_cap_tick(dev, dropped_bufs);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * including the current buffer.\n\t\t */\n\t\tnumerators_since_start = ++buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_cap;\n\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Capture Thread End\\n\");\n\treturn 0;\n}",
        "code_after_change": "static int vivid_thread_vid_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\tint dropped_bufs;\n\n\tdprintk(dev, 1, \"Video Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->cap_seq_offset = 0;\n\tdev->cap_seq_count = 0;\n\tdev->cap_seq_resync = false;\n\tdev->jiffies_vid_cap = jiffies;\n\tdev->cap_stream_start = ktime_get_ns();\n\tvivid_cap_update_frame_period(dev);\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (!mutex_trylock(&dev->mutex)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->cap_seq_resync) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = dev->cap_seq_count + 1;\n\t\t\tdev->cap_seq_count = 0;\n\t\t\tdev->cap_stream_start += dev->cap_frame_period *\n\t\t\t\t\t\t dev->cap_seq_offset;\n\t\t\tvivid_cap_update_frame_period(dev);\n\t\t\tdev->cap_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_cap.numerator;\n\t\tdenominator = dev->timeperframe_vid_cap.denominator;\n\n\t\tif (dev->field_cap == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdropped_bufs = buffers_since_start + dev->cap_seq_offset - dev->cap_seq_count;\n\t\tdev->cap_seq_count = buffers_since_start + dev->cap_seq_offset;\n\t\tdev->vid_cap_seq_count = dev->cap_seq_count - dev->vid_cap_seq_start;\n\t\tdev->vbi_cap_seq_count = dev->cap_seq_count - dev->vbi_cap_seq_start;\n\t\tdev->meta_cap_seq_count = dev->cap_seq_count - dev->meta_cap_seq_start;\n\n\t\tvivid_thread_vid_cap_tick(dev, dropped_bufs);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * including the current buffer.\n\t\t */\n\t\tnumerators_since_start = ++buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_cap;\n\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Capture Thread End\\n\");\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,11 @@\n \t\tif (kthread_should_stop())\n \t\t\tbreak;\n \n-\t\tmutex_lock(&dev->mutex);\n+\t\tif (!mutex_trylock(&dev->mutex)) {\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tcontinue;\n+\t\t}\n+\n \t\tcur_jiffies = jiffies;\n \t\tif (dev->cap_seq_resync) {\n \t\t\tdev->jiffies_vid_cap = cur_jiffies;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!mutex_trylock(&dev->mutex)) {",
                "\t\t\tschedule_timeout_uninterruptible(1);",
                "\t\t\tcontinue;",
                "\t\t}",
                ""
            ],
            "deleted": [
                "\t\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free.",
        "id": 2092
    },
    {
        "cve_id": "CVE-2017-10661",
        "code_before_change": "static void timerfd_setup_cancel(struct timerfd_ctx *ctx, int flags)\n{\n\tif ((ctx->clockid == CLOCK_REALTIME ||\n\t     ctx->clockid == CLOCK_REALTIME_ALARM) &&\n\t    (flags & TFD_TIMER_ABSTIME) && (flags & TFD_TIMER_CANCEL_ON_SET)) {\n\t\tif (!ctx->might_cancel) {\n\t\t\tctx->might_cancel = true;\n\t\t\tspin_lock(&cancel_lock);\n\t\t\tlist_add_rcu(&ctx->clist, &cancel_list);\n\t\t\tspin_unlock(&cancel_lock);\n\t\t}\n\t} else if (ctx->might_cancel) {\n\t\ttimerfd_remove_cancel(ctx);\n\t}\n}",
        "code_after_change": "static void timerfd_setup_cancel(struct timerfd_ctx *ctx, int flags)\n{\n\tspin_lock(&ctx->cancel_lock);\n\tif ((ctx->clockid == CLOCK_REALTIME ||\n\t     ctx->clockid == CLOCK_REALTIME_ALARM) &&\n\t    (flags & TFD_TIMER_ABSTIME) && (flags & TFD_TIMER_CANCEL_ON_SET)) {\n\t\tif (!ctx->might_cancel) {\n\t\t\tctx->might_cancel = true;\n\t\t\tspin_lock(&cancel_lock);\n\t\t\tlist_add_rcu(&ctx->clist, &cancel_list);\n\t\t\tspin_unlock(&cancel_lock);\n\t\t}\n\t} else {\n\t\t__timerfd_remove_cancel(ctx);\n\t}\n\tspin_unlock(&ctx->cancel_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n static void timerfd_setup_cancel(struct timerfd_ctx *ctx, int flags)\n {\n+\tspin_lock(&ctx->cancel_lock);\n \tif ((ctx->clockid == CLOCK_REALTIME ||\n \t     ctx->clockid == CLOCK_REALTIME_ALARM) &&\n \t    (flags & TFD_TIMER_ABSTIME) && (flags & TFD_TIMER_CANCEL_ON_SET)) {\n@@ -9,7 +10,8 @@\n \t\t\tlist_add_rcu(&ctx->clist, &cancel_list);\n \t\t\tspin_unlock(&cancel_lock);\n \t\t}\n-\t} else if (ctx->might_cancel) {\n-\t\ttimerfd_remove_cancel(ctx);\n+\t} else {\n+\t\t__timerfd_remove_cancel(ctx);\n \t}\n+\tspin_unlock(&ctx->cancel_lock);\n }",
        "function_modified_lines": {
            "added": [
                "\tspin_lock(&ctx->cancel_lock);",
                "\t} else {",
                "\t\t__timerfd_remove_cancel(ctx);",
                "\tspin_unlock(&ctx->cancel_lock);"
            ],
            "deleted": [
                "\t} else if (ctx->might_cancel) {",
                "\t\ttimerfd_remove_cancel(ctx);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Race condition in fs/timerfd.c in the Linux kernel before 4.10.15 allows local users to gain privileges or cause a denial of service (list corruption or use-after-free) via simultaneous file-descriptor operations that leverage improper might_cancel queueing.",
        "id": 1242
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int\ncopy_entries_to_user(unsigned int total_size,\n\t\t     const struct xt_table *table,\n\t\t     void __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct ip6t_entry *e;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\tint ret = 0;\n\tconst void *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tunsigned int i;\n\t\tconst struct xt_entry_match *m;\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct ip6t_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tfor (i = sizeof(struct ip6t_entry);\n\t\t     i < e->target_offset;\n\t\t     i += m->u.match_size) {\n\t\t\tm = (void *)e + i;\n\n\t\t\tif (xt_match_to_user(m, userptr + off + i)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto free_counters;\n\t\t\t}\n\t\t}\n\n\t\tt = ip6t_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
        "code_after_change": "static int\ncopy_entries_to_user(unsigned int total_size,\n\t\t     const struct xt_table *table,\n\t\t     void __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct ip6t_entry *e;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\tint ret = 0;\n\tconst void *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tunsigned int i;\n\t\tconst struct xt_entry_match *m;\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct ip6t_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tfor (i = sizeof(struct ip6t_entry);\n\t\t     i < e->target_offset;\n\t\t     i += m->u.match_size) {\n\t\t\tm = (void *)e + i;\n\n\t\t\tif (xt_match_to_user(m, userptr + off + i)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto free_counters;\n\t\t\t}\n\t\t}\n\n\t\tt = ip6t_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tunsigned int off, num;\n \tconst struct ip6t_entry *e;\n \tstruct xt_counters *counters;\n-\tconst struct xt_table_info *private = table->private;\n+\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n \tint ret = 0;\n \tconst void *loc_cpu_entry;\n ",
        "function_modified_lines": {
            "added": [
                "\tconst struct xt_table_info *private = xt_table_get_private_protected(table);"
            ],
            "deleted": [
                "\tconst struct xt_table_info *private = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2789
    },
    {
        "cve_id": "CVE-2022-2318",
        "code_before_change": "void rose_start_t1timer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tdel_timer(&rose->timer);\n\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->t1;\n\n\tadd_timer(&rose->timer);\n}",
        "code_after_change": "void rose_start_t1timer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tsk_stop_timer(sk, &rose->timer);\n\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->t1;\n\n\tsk_reset_timer(sk, &rose->timer, rose->timer.expires);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,10 +2,10 @@\n {\n \tstruct rose_sock *rose = rose_sk(sk);\n \n-\tdel_timer(&rose->timer);\n+\tsk_stop_timer(sk, &rose->timer);\n \n \trose->timer.function = rose_timer_expiry;\n \trose->timer.expires  = jiffies + rose->t1;\n \n-\tadd_timer(&rose->timer);\n+\tsk_reset_timer(sk, &rose->timer, rose->timer.expires);\n }",
        "function_modified_lines": {
            "added": [
                "\tsk_stop_timer(sk, &rose->timer);",
                "\tsk_reset_timer(sk, &rose->timer, rose->timer.expires);"
            ],
            "deleted": [
                "\tdel_timer(&rose->timer);",
                "\tadd_timer(&rose->timer);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities caused by timer handler in net/rose/rose_timer.c of linux that allow attackers to crash linux kernel without any privileges.",
        "id": 3430
    },
    {
        "cve_id": "CVE-2022-42703",
        "code_before_change": "int anon_vma_clone(struct vm_area_struct *dst, struct vm_area_struct *src)\n{\n\tstruct anon_vma_chain *avc, *pavc;\n\tstruct anon_vma *root = NULL;\n\n\tlist_for_each_entry_reverse(pavc, &src->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma;\n\n\t\tavc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);\n\t\tif (unlikely(!avc)) {\n\t\t\tunlock_anon_vma_root(root);\n\t\t\troot = NULL;\n\t\t\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\t\t\tif (!avc)\n\t\t\t\tgoto enomem_failure;\n\t\t}\n\t\tanon_vma = pavc->anon_vma;\n\t\troot = lock_anon_vma_root(root, anon_vma);\n\t\tanon_vma_chain_link(dst, avc, anon_vma);\n\n\t\t/*\n\t\t * Reuse existing anon_vma if its degree lower than two,\n\t\t * that means it has no vma and only one anon_vma child.\n\t\t *\n\t\t * Do not choose parent anon_vma, otherwise first child\n\t\t * will always reuse it. Root anon_vma is never reused:\n\t\t * it has self-parent reference and at least one child.\n\t\t */\n\t\tif (!dst->anon_vma && src->anon_vma &&\n\t\t    anon_vma != src->anon_vma && anon_vma->degree < 2)\n\t\t\tdst->anon_vma = anon_vma;\n\t}\n\tif (dst->anon_vma)\n\t\tdst->anon_vma->degree++;\n\tunlock_anon_vma_root(root);\n\treturn 0;\n\n enomem_failure:\n\t/*\n\t * dst->anon_vma is dropped here otherwise its degree can be incorrectly\n\t * decremented in unlink_anon_vmas().\n\t * We can safely do this because callers of anon_vma_clone() don't care\n\t * about dst->anon_vma if anon_vma_clone() failed.\n\t */\n\tdst->anon_vma = NULL;\n\tunlink_anon_vmas(dst);\n\treturn -ENOMEM;\n}",
        "code_after_change": "int anon_vma_clone(struct vm_area_struct *dst, struct vm_area_struct *src)\n{\n\tstruct anon_vma_chain *avc, *pavc;\n\tstruct anon_vma *root = NULL;\n\n\tlist_for_each_entry_reverse(pavc, &src->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma;\n\n\t\tavc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);\n\t\tif (unlikely(!avc)) {\n\t\t\tunlock_anon_vma_root(root);\n\t\t\troot = NULL;\n\t\t\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\t\t\tif (!avc)\n\t\t\t\tgoto enomem_failure;\n\t\t}\n\t\tanon_vma = pavc->anon_vma;\n\t\troot = lock_anon_vma_root(root, anon_vma);\n\t\tanon_vma_chain_link(dst, avc, anon_vma);\n\n\t\t/*\n\t\t * Reuse existing anon_vma if it has no vma and only one\n\t\t * anon_vma child.\n\t\t *\n\t\t * Root anon_vma is never reused:\n\t\t * it has self-parent reference and at least one child.\n\t\t */\n\t\tif (!dst->anon_vma && src->anon_vma &&\n\t\t    anon_vma->num_children < 2 &&\n\t\t    anon_vma->num_active_vmas == 0)\n\t\t\tdst->anon_vma = anon_vma;\n\t}\n\tif (dst->anon_vma)\n\t\tdst->anon_vma->num_active_vmas++;\n\tunlock_anon_vma_root(root);\n\treturn 0;\n\n enomem_failure:\n\t/*\n\t * dst->anon_vma is dropped here otherwise its degree can be incorrectly\n\t * decremented in unlink_anon_vmas().\n\t * We can safely do this because callers of anon_vma_clone() don't care\n\t * about dst->anon_vma if anon_vma_clone() failed.\n\t */\n\tdst->anon_vma = NULL;\n\tunlink_anon_vmas(dst);\n\treturn -ENOMEM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,19 +19,19 @@\n \t\tanon_vma_chain_link(dst, avc, anon_vma);\n \n \t\t/*\n-\t\t * Reuse existing anon_vma if its degree lower than two,\n-\t\t * that means it has no vma and only one anon_vma child.\n+\t\t * Reuse existing anon_vma if it has no vma and only one\n+\t\t * anon_vma child.\n \t\t *\n-\t\t * Do not choose parent anon_vma, otherwise first child\n-\t\t * will always reuse it. Root anon_vma is never reused:\n+\t\t * Root anon_vma is never reused:\n \t\t * it has self-parent reference and at least one child.\n \t\t */\n \t\tif (!dst->anon_vma && src->anon_vma &&\n-\t\t    anon_vma != src->anon_vma && anon_vma->degree < 2)\n+\t\t    anon_vma->num_children < 2 &&\n+\t\t    anon_vma->num_active_vmas == 0)\n \t\t\tdst->anon_vma = anon_vma;\n \t}\n \tif (dst->anon_vma)\n-\t\tdst->anon_vma->degree++;\n+\t\tdst->anon_vma->num_active_vmas++;\n \tunlock_anon_vma_root(root);\n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [
                "\t\t * Reuse existing anon_vma if it has no vma and only one",
                "\t\t * anon_vma child.",
                "\t\t * Root anon_vma is never reused:",
                "\t\t    anon_vma->num_children < 2 &&",
                "\t\t    anon_vma->num_active_vmas == 0)",
                "\t\tdst->anon_vma->num_active_vmas++;"
            ],
            "deleted": [
                "\t\t * Reuse existing anon_vma if its degree lower than two,",
                "\t\t * that means it has no vma and only one anon_vma child.",
                "\t\t * Do not choose parent anon_vma, otherwise first child",
                "\t\t * will always reuse it. Root anon_vma is never reused:",
                "\t\t    anon_vma != src->anon_vma && anon_vma->degree < 2)",
                "\t\tdst->anon_vma->degree++;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "mm/rmap.c in the Linux kernel before 5.19.7 has a use-after-free related to leaf anon_vma double reuse.",
        "id": 3731
    },
    {
        "cve_id": "CVE-2023-4207",
        "code_before_change": "static int fw_change(struct net *net, struct sk_buff *in_skb,\n\t\t     struct tcf_proto *tp, unsigned long base,\n\t\t     u32 handle, struct nlattr **tca, void **arg,\n\t\t     u32 flags, struct netlink_ext_ack *extack)\n{\n\tstruct fw_head *head = rtnl_dereference(tp->root);\n\tstruct fw_filter *f = *arg;\n\tstruct nlattr *opt = tca[TCA_OPTIONS];\n\tstruct nlattr *tb[TCA_FW_MAX + 1];\n\tint err;\n\n\tif (!opt)\n\t\treturn handle ? -EINVAL : 0; /* Succeed if it is old method. */\n\n\terr = nla_parse_nested_deprecated(tb, TCA_FW_MAX, opt, fw_policy,\n\t\t\t\t\t  NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (f) {\n\t\tstruct fw_filter *pfp, *fnew;\n\t\tstruct fw_filter __rcu **fp;\n\n\t\tif (f->id != handle && handle)\n\t\t\treturn -EINVAL;\n\n\t\tfnew = kzalloc(sizeof(struct fw_filter), GFP_KERNEL);\n\t\tif (!fnew)\n\t\t\treturn -ENOBUFS;\n\n\t\tfnew->id = f->id;\n\t\tfnew->res = f->res;\n\t\tfnew->ifindex = f->ifindex;\n\t\tfnew->tp = f->tp;\n\n\t\terr = tcf_exts_init(&fnew->exts, net, TCA_FW_ACT,\n\t\t\t\t    TCA_FW_POLICE);\n\t\tif (err < 0) {\n\t\t\tkfree(fnew);\n\t\t\treturn err;\n\t\t}\n\n\t\terr = fw_set_parms(net, tp, fnew, tb, tca, base, flags, extack);\n\t\tif (err < 0) {\n\t\t\ttcf_exts_destroy(&fnew->exts);\n\t\t\tkfree(fnew);\n\t\t\treturn err;\n\t\t}\n\n\t\tfp = &head->ht[fw_hash(fnew->id)];\n\t\tfor (pfp = rtnl_dereference(*fp); pfp;\n\t\t     fp = &pfp->next, pfp = rtnl_dereference(*fp))\n\t\t\tif (pfp == f)\n\t\t\t\tbreak;\n\n\t\tRCU_INIT_POINTER(fnew->next, rtnl_dereference(pfp->next));\n\t\trcu_assign_pointer(*fp, fnew);\n\t\ttcf_unbind_filter(tp, &f->res);\n\t\ttcf_exts_get_net(&f->exts);\n\t\ttcf_queue_work(&f->rwork, fw_delete_filter_work);\n\n\t\t*arg = fnew;\n\t\treturn err;\n\t}\n\n\tif (!handle)\n\t\treturn -EINVAL;\n\n\tif (!head) {\n\t\tu32 mask = 0xFFFFFFFF;\n\t\tif (tb[TCA_FW_MASK])\n\t\t\tmask = nla_get_u32(tb[TCA_FW_MASK]);\n\n\t\thead = kzalloc(sizeof(*head), GFP_KERNEL);\n\t\tif (!head)\n\t\t\treturn -ENOBUFS;\n\t\thead->mask = mask;\n\n\t\trcu_assign_pointer(tp->root, head);\n\t}\n\n\tf = kzalloc(sizeof(struct fw_filter), GFP_KERNEL);\n\tif (f == NULL)\n\t\treturn -ENOBUFS;\n\n\terr = tcf_exts_init(&f->exts, net, TCA_FW_ACT, TCA_FW_POLICE);\n\tif (err < 0)\n\t\tgoto errout;\n\tf->id = handle;\n\tf->tp = tp;\n\n\terr = fw_set_parms(net, tp, f, tb, tca, base, flags, extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tRCU_INIT_POINTER(f->next, head->ht[fw_hash(handle)]);\n\trcu_assign_pointer(head->ht[fw_hash(handle)], f);\n\n\t*arg = f;\n\treturn 0;\n\nerrout:\n\ttcf_exts_destroy(&f->exts);\n\tkfree(f);\n\treturn err;\n}",
        "code_after_change": "static int fw_change(struct net *net, struct sk_buff *in_skb,\n\t\t     struct tcf_proto *tp, unsigned long base,\n\t\t     u32 handle, struct nlattr **tca, void **arg,\n\t\t     u32 flags, struct netlink_ext_ack *extack)\n{\n\tstruct fw_head *head = rtnl_dereference(tp->root);\n\tstruct fw_filter *f = *arg;\n\tstruct nlattr *opt = tca[TCA_OPTIONS];\n\tstruct nlattr *tb[TCA_FW_MAX + 1];\n\tint err;\n\n\tif (!opt)\n\t\treturn handle ? -EINVAL : 0; /* Succeed if it is old method. */\n\n\terr = nla_parse_nested_deprecated(tb, TCA_FW_MAX, opt, fw_policy,\n\t\t\t\t\t  NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (f) {\n\t\tstruct fw_filter *pfp, *fnew;\n\t\tstruct fw_filter __rcu **fp;\n\n\t\tif (f->id != handle && handle)\n\t\t\treturn -EINVAL;\n\n\t\tfnew = kzalloc(sizeof(struct fw_filter), GFP_KERNEL);\n\t\tif (!fnew)\n\t\t\treturn -ENOBUFS;\n\n\t\tfnew->id = f->id;\n\t\tfnew->ifindex = f->ifindex;\n\t\tfnew->tp = f->tp;\n\n\t\terr = tcf_exts_init(&fnew->exts, net, TCA_FW_ACT,\n\t\t\t\t    TCA_FW_POLICE);\n\t\tif (err < 0) {\n\t\t\tkfree(fnew);\n\t\t\treturn err;\n\t\t}\n\n\t\terr = fw_set_parms(net, tp, fnew, tb, tca, base, flags, extack);\n\t\tif (err < 0) {\n\t\t\ttcf_exts_destroy(&fnew->exts);\n\t\t\tkfree(fnew);\n\t\t\treturn err;\n\t\t}\n\n\t\tfp = &head->ht[fw_hash(fnew->id)];\n\t\tfor (pfp = rtnl_dereference(*fp); pfp;\n\t\t     fp = &pfp->next, pfp = rtnl_dereference(*fp))\n\t\t\tif (pfp == f)\n\t\t\t\tbreak;\n\n\t\tRCU_INIT_POINTER(fnew->next, rtnl_dereference(pfp->next));\n\t\trcu_assign_pointer(*fp, fnew);\n\t\ttcf_unbind_filter(tp, &f->res);\n\t\ttcf_exts_get_net(&f->exts);\n\t\ttcf_queue_work(&f->rwork, fw_delete_filter_work);\n\n\t\t*arg = fnew;\n\t\treturn err;\n\t}\n\n\tif (!handle)\n\t\treturn -EINVAL;\n\n\tif (!head) {\n\t\tu32 mask = 0xFFFFFFFF;\n\t\tif (tb[TCA_FW_MASK])\n\t\t\tmask = nla_get_u32(tb[TCA_FW_MASK]);\n\n\t\thead = kzalloc(sizeof(*head), GFP_KERNEL);\n\t\tif (!head)\n\t\t\treturn -ENOBUFS;\n\t\thead->mask = mask;\n\n\t\trcu_assign_pointer(tp->root, head);\n\t}\n\n\tf = kzalloc(sizeof(struct fw_filter), GFP_KERNEL);\n\tif (f == NULL)\n\t\treturn -ENOBUFS;\n\n\terr = tcf_exts_init(&f->exts, net, TCA_FW_ACT, TCA_FW_POLICE);\n\tif (err < 0)\n\t\tgoto errout;\n\tf->id = handle;\n\tf->tp = tp;\n\n\terr = fw_set_parms(net, tp, f, tb, tca, base, flags, extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tRCU_INIT_POINTER(f->next, head->ht[fw_hash(handle)]);\n\trcu_assign_pointer(head->ht[fw_hash(handle)], f);\n\n\t*arg = f;\n\treturn 0;\n\nerrout:\n\ttcf_exts_destroy(&f->exts);\n\tkfree(f);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,7 +29,6 @@\n \t\t\treturn -ENOBUFS;\n \n \t\tfnew->id = f->id;\n-\t\tfnew->res = f->res;\n \t\tfnew->ifindex = f->ifindex;\n \t\tfnew->tp = f->tp;\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tfnew->res = f->res;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's net/sched: cls_fw component can be exploited to achieve local privilege escalation.\n\nWhen fw_change() is called on an existing filter, the whole tcf_result struct is always copied into the new instance of the filter. This causes a problem when updating a filter bound to a class, as tcf_unbind_filter() is always called on the old instance in the success path, decreasing filter_cnt of the still referenced class and allowing it to be deleted, leading to a use-after-free.\n\nWe recommend upgrading past commit 76e42ae831991c828cffa8c37736ebfb831ad5ec.\n\n",
        "id": 4199
    },
    {
        "cve_id": "CVE-2018-16884",
        "code_before_change": "int\nbc_svc_process(struct svc_serv *serv, struct rpc_rqst *req,\n\t       struct svc_rqst *rqstp)\n{\n\tstruct kvec\t*argv = &rqstp->rq_arg.head[0];\n\tstruct kvec\t*resv = &rqstp->rq_res.head[0];\n\tstruct rpc_task *task;\n\tint proc_error;\n\tint error;\n\n\tdprintk(\"svc: %s(%p)\\n\", __func__, req);\n\n\t/* Build the svc_rqst used by the common processing routine */\n\trqstp->rq_xprt = serv->sv_bc_xprt;\n\trqstp->rq_xid = req->rq_xid;\n\trqstp->rq_prot = req->rq_xprt->prot;\n\trqstp->rq_server = serv;\n\n\trqstp->rq_addrlen = sizeof(req->rq_xprt->addr);\n\tmemcpy(&rqstp->rq_addr, &req->rq_xprt->addr, rqstp->rq_addrlen);\n\tmemcpy(&rqstp->rq_arg, &req->rq_rcv_buf, sizeof(rqstp->rq_arg));\n\tmemcpy(&rqstp->rq_res, &req->rq_snd_buf, sizeof(rqstp->rq_res));\n\n\t/* Adjust the argument buffer length */\n\trqstp->rq_arg.len = req->rq_private_buf.len;\n\tif (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len) {\n\t\trqstp->rq_arg.head[0].iov_len = rqstp->rq_arg.len;\n\t\trqstp->rq_arg.page_len = 0;\n\t} else if (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len)\n\t\trqstp->rq_arg.page_len = rqstp->rq_arg.len -\n\t\t\trqstp->rq_arg.head[0].iov_len;\n\telse\n\t\trqstp->rq_arg.len = rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len;\n\n\t/* reset result send buffer \"put\" position */\n\tresv->iov_len = 0;\n\n\t/*\n\t * Skip the next two words because they've already been\n\t * processed in the transport\n\t */\n\tsvc_getu32(argv);\t/* XID */\n\tsvc_getnl(argv);\t/* CALLDIR */\n\n\t/* Parse and execute the bc call */\n\tproc_error = svc_process_common(rqstp, argv, resv);\n\n\tatomic_inc(&req->rq_xprt->bc_free_slots);\n\tif (!proc_error) {\n\t\t/* Processing error: drop the request */\n\t\txprt_free_bc_request(req);\n\t\treturn 0;\n\t}\n\n\t/* Finally, send the reply synchronously */\n\tmemcpy(&req->rq_snd_buf, &rqstp->rq_res, sizeof(req->rq_snd_buf));\n\ttask = rpc_run_bc_task(req);\n\tif (IS_ERR(task)) {\n\t\terror = PTR_ERR(task);\n\t\tgoto out;\n\t}\n\n\tWARN_ON_ONCE(atomic_read(&task->tk_count) != 1);\n\terror = task->tk_status;\n\trpc_put_task(task);\n\nout:\n\tdprintk(\"svc: %s(), error=%d\\n\", __func__, error);\n\treturn error;\n}",
        "code_after_change": "int\nbc_svc_process(struct svc_serv *serv, struct rpc_rqst *req,\n\t       struct svc_rqst *rqstp)\n{\n\tstruct kvec\t*argv = &rqstp->rq_arg.head[0];\n\tstruct kvec\t*resv = &rqstp->rq_res.head[0];\n\tstruct rpc_task *task;\n\tint proc_error;\n\tint error;\n\n\tdprintk(\"svc: %s(%p)\\n\", __func__, req);\n\n\t/* Build the svc_rqst used by the common processing routine */\n\trqstp->rq_xid = req->rq_xid;\n\trqstp->rq_prot = req->rq_xprt->prot;\n\trqstp->rq_server = serv;\n\trqstp->rq_bc_net = req->rq_xprt->xprt_net;\n\n\trqstp->rq_addrlen = sizeof(req->rq_xprt->addr);\n\tmemcpy(&rqstp->rq_addr, &req->rq_xprt->addr, rqstp->rq_addrlen);\n\tmemcpy(&rqstp->rq_arg, &req->rq_rcv_buf, sizeof(rqstp->rq_arg));\n\tmemcpy(&rqstp->rq_res, &req->rq_snd_buf, sizeof(rqstp->rq_res));\n\n\t/* Adjust the argument buffer length */\n\trqstp->rq_arg.len = req->rq_private_buf.len;\n\tif (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len) {\n\t\trqstp->rq_arg.head[0].iov_len = rqstp->rq_arg.len;\n\t\trqstp->rq_arg.page_len = 0;\n\t} else if (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len)\n\t\trqstp->rq_arg.page_len = rqstp->rq_arg.len -\n\t\t\trqstp->rq_arg.head[0].iov_len;\n\telse\n\t\trqstp->rq_arg.len = rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len;\n\n\t/* reset result send buffer \"put\" position */\n\tresv->iov_len = 0;\n\n\t/*\n\t * Skip the next two words because they've already been\n\t * processed in the transport\n\t */\n\tsvc_getu32(argv);\t/* XID */\n\tsvc_getnl(argv);\t/* CALLDIR */\n\n\t/* Parse and execute the bc call */\n\tproc_error = svc_process_common(rqstp, argv, resv);\n\n\tatomic_inc(&req->rq_xprt->bc_free_slots);\n\tif (!proc_error) {\n\t\t/* Processing error: drop the request */\n\t\txprt_free_bc_request(req);\n\t\treturn 0;\n\t}\n\n\t/* Finally, send the reply synchronously */\n\tmemcpy(&req->rq_snd_buf, &rqstp->rq_res, sizeof(req->rq_snd_buf));\n\ttask = rpc_run_bc_task(req);\n\tif (IS_ERR(task)) {\n\t\terror = PTR_ERR(task);\n\t\tgoto out;\n\t}\n\n\tWARN_ON_ONCE(atomic_read(&task->tk_count) != 1);\n\terror = task->tk_status;\n\trpc_put_task(task);\n\nout:\n\tdprintk(\"svc: %s(), error=%d\\n\", __func__, error);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,10 +11,10 @@\n \tdprintk(\"svc: %s(%p)\\n\", __func__, req);\n \n \t/* Build the svc_rqst used by the common processing routine */\n-\trqstp->rq_xprt = serv->sv_bc_xprt;\n \trqstp->rq_xid = req->rq_xid;\n \trqstp->rq_prot = req->rq_xprt->prot;\n \trqstp->rq_server = serv;\n+\trqstp->rq_bc_net = req->rq_xprt->xprt_net;\n \n \trqstp->rq_addrlen = sizeof(req->rq_xprt->addr);\n \tmemcpy(&rqstp->rq_addr, &req->rq_xprt->addr, rqstp->rq_addrlen);",
        "function_modified_lines": {
            "added": [
                "\trqstp->rq_bc_net = req->rq_xprt->xprt_net;"
            ],
            "deleted": [
                "\trqstp->rq_xprt = serv->sv_bc_xprt;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel's NFS41+ subsystem. NFS41+ shares mounted in different network namespaces at the same time can make bc_svc_process() use wrong back-channel IDs and cause a use-after-free vulnerability. Thus a malicious container user can cause a host kernel memory corruption and a system panic. Due to the nature of the flaw, privilege escalation cannot be fully ruled out.",
        "id": 1723
    },
    {
        "cve_id": "CVE-2017-2584",
        "code_before_change": "static int emulate_store_desc_ptr(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  void (*get)(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\t      struct desc_ptr *ptr))\n{\n\tstruct desc_ptr desc_ptr;\n\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tctxt->op_bytes = 8;\n\tget(ctxt, &desc_ptr);\n\tif (ctxt->op_bytes == 2) {\n\t\tctxt->op_bytes = 4;\n\t\tdesc_ptr.address &= 0x00ffffff;\n\t}\n\t/* Disable writeback. */\n\tctxt->dst.type = OP_NONE;\n\treturn segmented_write(ctxt, ctxt->dst.addr.mem,\n\t\t\t       &desc_ptr, 2 + ctxt->op_bytes);\n}",
        "code_after_change": "static int emulate_store_desc_ptr(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  void (*get)(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\t      struct desc_ptr *ptr))\n{\n\tstruct desc_ptr desc_ptr;\n\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tctxt->op_bytes = 8;\n\tget(ctxt, &desc_ptr);\n\tif (ctxt->op_bytes == 2) {\n\t\tctxt->op_bytes = 4;\n\t\tdesc_ptr.address &= 0x00ffffff;\n\t}\n\t/* Disable writeback. */\n\tctxt->dst.type = OP_NONE;\n\treturn segmented_write_std(ctxt, ctxt->dst.addr.mem,\n\t\t\t\t   &desc_ptr, 2 + ctxt->op_bytes);\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,6 @@\n \t}\n \t/* Disable writeback. */\n \tctxt->dst.type = OP_NONE;\n-\treturn segmented_write(ctxt, ctxt->dst.addr.mem,\n-\t\t\t       &desc_ptr, 2 + ctxt->op_bytes);\n+\treturn segmented_write_std(ctxt, ctxt->dst.addr.mem,\n+\t\t\t\t   &desc_ptr, 2 + ctxt->op_bytes);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn segmented_write_std(ctxt, ctxt->dst.addr.mem,",
                "\t\t\t\t   &desc_ptr, 2 + ctxt->op_bytes);"
            ],
            "deleted": [
                "\treturn segmented_write(ctxt, ctxt->dst.addr.mem,",
                "\t\t\t       &desc_ptr, 2 + ctxt->op_bytes);"
            ]
        },
        "cwe": [
            "CWE-200",
            "CWE-416"
        ],
        "cve_description": "arch/x86/kvm/emulate.c in the Linux kernel through 4.9.3 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free) via a crafted application that leverages instruction emulation for fxrstor, fxsave, sgdt, and sidt.",
        "id": 1445
    },
    {
        "cve_id": "CVE-2022-1976",
        "code_before_change": "static void io_clean_op(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_BUFFER_SELECTED) {\n\t\tspin_lock(&req->ctx->completion_lock);\n\t\tio_put_kbuf_comp(req);\n\t\tspin_unlock(&req->ctx->completion_lock);\n\t}\n\n\tif (req->flags & REQ_F_NEED_CLEANUP) {\n\t\tswitch (req->opcode) {\n\t\tcase IORING_OP_READV:\n\t\tcase IORING_OP_READ_FIXED:\n\t\tcase IORING_OP_READ:\n\t\tcase IORING_OP_WRITEV:\n\t\tcase IORING_OP_WRITE_FIXED:\n\t\tcase IORING_OP_WRITE: {\n\t\t\tstruct io_async_rw *io = req->async_data;\n\n\t\t\tkfree(io->free_iovec);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_RECVMSG:\n\t\tcase IORING_OP_SENDMSG: {\n\t\t\tstruct io_async_msghdr *io = req->async_data;\n\n\t\t\tkfree(io->free_iov);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_OPENAT:\n\t\tcase IORING_OP_OPENAT2:\n\t\t\tif (req->open.filename)\n\t\t\t\tputname(req->open.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_RENAMEAT:\n\t\t\tputname(req->rename.oldpath);\n\t\t\tputname(req->rename.newpath);\n\t\t\tbreak;\n\t\tcase IORING_OP_UNLINKAT:\n\t\t\tputname(req->unlink.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_MKDIRAT:\n\t\t\tputname(req->mkdir.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_SYMLINKAT:\n\t\t\tputname(req->symlink.oldpath);\n\t\t\tputname(req->symlink.newpath);\n\t\t\tbreak;\n\t\tcase IORING_OP_LINKAT:\n\t\t\tputname(req->hardlink.oldpath);\n\t\t\tputname(req->hardlink.newpath);\n\t\t\tbreak;\n\t\tcase IORING_OP_STATX:\n\t\t\tif (req->statx.filename)\n\t\t\t\tputname(req->statx.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_SETXATTR:\n\t\tcase IORING_OP_FSETXATTR:\n\t\tcase IORING_OP_GETXATTR:\n\t\tcase IORING_OP_FGETXATTR:\n\t\t\t__io_xattr_finish(req);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif ((req->flags & REQ_F_POLLED) && req->apoll) {\n\t\tkfree(req->apoll->double_poll);\n\t\tkfree(req->apoll);\n\t\treq->apoll = NULL;\n\t}\n\tif (req->flags & REQ_F_CREDS)\n\t\tput_cred(req->creds);\n\tif (req->flags & REQ_F_ASYNC_DATA) {\n\t\tkfree(req->async_data);\n\t\treq->async_data = NULL;\n\t}\n\treq->flags &= ~IO_REQ_CLEAN_FLAGS;\n}",
        "code_after_change": "static void io_clean_op(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_BUFFER_SELECTED) {\n\t\tspin_lock(&req->ctx->completion_lock);\n\t\tio_put_kbuf_comp(req);\n\t\tspin_unlock(&req->ctx->completion_lock);\n\t}\n\n\tif (req->flags & REQ_F_NEED_CLEANUP) {\n\t\tswitch (req->opcode) {\n\t\tcase IORING_OP_READV:\n\t\tcase IORING_OP_READ_FIXED:\n\t\tcase IORING_OP_READ:\n\t\tcase IORING_OP_WRITEV:\n\t\tcase IORING_OP_WRITE_FIXED:\n\t\tcase IORING_OP_WRITE: {\n\t\t\tstruct io_async_rw *io = req->async_data;\n\n\t\t\tkfree(io->free_iovec);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_RECVMSG:\n\t\tcase IORING_OP_SENDMSG: {\n\t\t\tstruct io_async_msghdr *io = req->async_data;\n\n\t\t\tkfree(io->free_iov);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_OPENAT:\n\t\tcase IORING_OP_OPENAT2:\n\t\t\tif (req->open.filename)\n\t\t\t\tputname(req->open.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_RENAMEAT:\n\t\t\tputname(req->rename.oldpath);\n\t\t\tputname(req->rename.newpath);\n\t\t\tbreak;\n\t\tcase IORING_OP_UNLINKAT:\n\t\t\tputname(req->unlink.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_MKDIRAT:\n\t\t\tputname(req->mkdir.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_SYMLINKAT:\n\t\t\tputname(req->symlink.oldpath);\n\t\t\tputname(req->symlink.newpath);\n\t\t\tbreak;\n\t\tcase IORING_OP_LINKAT:\n\t\t\tputname(req->hardlink.oldpath);\n\t\t\tputname(req->hardlink.newpath);\n\t\t\tbreak;\n\t\tcase IORING_OP_STATX:\n\t\t\tif (req->statx.filename)\n\t\t\t\tputname(req->statx.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_SETXATTR:\n\t\tcase IORING_OP_FSETXATTR:\n\t\tcase IORING_OP_GETXATTR:\n\t\tcase IORING_OP_FGETXATTR:\n\t\t\t__io_xattr_finish(req);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif ((req->flags & REQ_F_POLLED) && req->apoll) {\n\t\tkfree(req->apoll->double_poll);\n\t\tkfree(req->apoll);\n\t\treq->apoll = NULL;\n\t}\n\tif (req->flags & REQ_F_INFLIGHT) {\n\t\tstruct io_uring_task *tctx = req->task->io_uring;\n\n\t\tatomic_dec(&tctx->inflight_tracked);\n\t}\n\tif (req->flags & REQ_F_CREDS)\n\t\tput_cred(req->creds);\n\tif (req->flags & REQ_F_ASYNC_DATA) {\n\t\tkfree(req->async_data);\n\t\treq->async_data = NULL;\n\t}\n\treq->flags &= ~IO_REQ_CLEAN_FLAGS;\n}",
        "patch": "--- code before\n+++ code after\n@@ -66,6 +66,11 @@\n \t\tkfree(req->apoll);\n \t\treq->apoll = NULL;\n \t}\n+\tif (req->flags & REQ_F_INFLIGHT) {\n+\t\tstruct io_uring_task *tctx = req->task->io_uring;\n+\n+\t\tatomic_dec(&tctx->inflight_tracked);\n+\t}\n \tif (req->flags & REQ_F_CREDS)\n \t\tput_cred(req->creds);\n \tif (req->flags & REQ_F_ASYNC_DATA) {",
        "function_modified_lines": {
            "added": [
                "\tif (req->flags & REQ_F_INFLIGHT) {",
                "\t\tstruct io_uring_task *tctx = req->task->io_uring;",
                "",
                "\t\tatomic_dec(&tctx->inflight_tracked);",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s implementation of IO-URING. This flaw allows an attacker with local executable permission to create a string of requests that can cause a use-after-free flaw within the kernel. This issue leads to memory corruption and possible privilege escalation.",
        "id": 3326
    },
    {
        "cve_id": "CVE-2019-9003",
        "code_before_change": "static void free_user(struct kref *ref)\n{\n\tstruct ipmi_user *user = container_of(ref, struct ipmi_user, refcount);\n\tkfree(user);\n}",
        "code_after_change": "static void free_user(struct kref *ref)\n{\n\tstruct ipmi_user *user = container_of(ref, struct ipmi_user, refcount);\n\tcleanup_srcu_struct(&user->release_barrier);\n\tkfree(user);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n static void free_user(struct kref *ref)\n {\n \tstruct ipmi_user *user = container_of(ref, struct ipmi_user, refcount);\n+\tcleanup_srcu_struct(&user->release_barrier);\n \tkfree(user);\n }",
        "function_modified_lines": {
            "added": [
                "\tcleanup_srcu_struct(&user->release_barrier);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 4.20.5, attackers can trigger a drivers/char/ipmi/ipmi_msghandler.c use-after-free and OOPS by arranging for certain simultaneous execution of the code, as demonstrated by a \"service ipmievd restart\" loop.",
        "id": 2350
    },
    {
        "cve_id": "CVE-2022-22942",
        "code_before_change": "int vmw_execbuf_process(struct drm_file *file_priv,\n\t\t\tstruct vmw_private *dev_priv,\n\t\t\tvoid __user *user_commands, void *kernel_commands,\n\t\t\tuint32_t command_size, uint64_t throttle_us,\n\t\t\tuint32_t dx_context_handle,\n\t\t\tstruct drm_vmw_fence_rep __user *user_fence_rep,\n\t\t\tstruct vmw_fence_obj **out_fence, uint32_t flags)\n{\n\tstruct vmw_sw_context *sw_context = &dev_priv->ctx;\n\tstruct vmw_fence_obj *fence = NULL;\n\tstruct vmw_cmdbuf_header *header;\n\tuint32_t handle = 0;\n\tint ret;\n\tint32_t out_fence_fd = -1;\n\tstruct sync_file *sync_file = NULL;\n\tDECLARE_VAL_CONTEXT(val_ctx, &sw_context->res_ht, 1);\n\n\tif (flags & DRM_VMW_EXECBUF_FLAG_EXPORT_FENCE_FD) {\n\t\tout_fence_fd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (out_fence_fd < 0) {\n\t\t\tVMW_DEBUG_USER(\"Failed to get a fence fd.\\n\");\n\t\t\treturn out_fence_fd;\n\t\t}\n\t}\n\n\tif (throttle_us) {\n\t\tVMW_DEBUG_USER(\"Throttling is no longer supported.\\n\");\n\t}\n\n\tkernel_commands = vmw_execbuf_cmdbuf(dev_priv, user_commands,\n\t\t\t\t\t     kernel_commands, command_size,\n\t\t\t\t\t     &header);\n\tif (IS_ERR(kernel_commands)) {\n\t\tret = PTR_ERR(kernel_commands);\n\t\tgoto out_free_fence_fd;\n\t}\n\n\tret = mutex_lock_interruptible(&dev_priv->cmdbuf_mutex);\n\tif (ret) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto out_free_header;\n\t}\n\n\tsw_context->kernel = false;\n\tif (kernel_commands == NULL) {\n\t\tret = vmw_resize_cmd_bounce(sw_context, command_size);\n\t\tif (unlikely(ret != 0))\n\t\t\tgoto out_unlock;\n\n\t\tret = copy_from_user(sw_context->cmd_bounce, user_commands,\n\t\t\t\t     command_size);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tret = -EFAULT;\n\t\t\tVMW_DEBUG_USER(\"Failed copying commands.\\n\");\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tkernel_commands = sw_context->cmd_bounce;\n\t} else if (!header) {\n\t\tsw_context->kernel = true;\n\t}\n\n\tsw_context->filp = file_priv;\n\tsw_context->fp = vmw_fpriv(file_priv);\n\tINIT_LIST_HEAD(&sw_context->ctx_list);\n\tsw_context->cur_query_bo = dev_priv->pinned_bo;\n\tsw_context->last_query_ctx = NULL;\n\tsw_context->needs_post_query_barrier = false;\n\tsw_context->dx_ctx_node = NULL;\n\tsw_context->dx_query_mob = NULL;\n\tsw_context->dx_query_ctx = NULL;\n\tmemset(sw_context->res_cache, 0, sizeof(sw_context->res_cache));\n\tINIT_LIST_HEAD(&sw_context->res_relocations);\n\tINIT_LIST_HEAD(&sw_context->bo_relocations);\n\n\tif (sw_context->staged_bindings)\n\t\tvmw_binding_state_reset(sw_context->staged_bindings);\n\n\tif (!sw_context->res_ht_initialized) {\n\t\tret = vmwgfx_ht_create(&sw_context->res_ht, VMW_RES_HT_ORDER);\n\t\tif (unlikely(ret != 0))\n\t\t\tgoto out_unlock;\n\n\t\tsw_context->res_ht_initialized = true;\n\t}\n\n\tINIT_LIST_HEAD(&sw_context->staged_cmd_res);\n\tsw_context->ctx = &val_ctx;\n\tret = vmw_execbuf_tie_context(dev_priv, sw_context, dx_context_handle);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err_nores;\n\n\tret = vmw_cmd_check_all(dev_priv, sw_context, kernel_commands,\n\t\t\t\tcommand_size);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err_nores;\n\n\tret = vmw_resources_reserve(sw_context);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err_nores;\n\n\tret = vmw_validation_bo_reserve(&val_ctx, true);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err_nores;\n\n\tret = vmw_validation_bo_validate(&val_ctx, true);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err;\n\n\tret = vmw_validation_res_validate(&val_ctx, true);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err;\n\n\tvmw_validation_drop_ht(&val_ctx);\n\n\tret = mutex_lock_interruptible(&dev_priv->binding_mutex);\n\tif (unlikely(ret != 0)) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto out_err;\n\t}\n\n\tif (dev_priv->has_mob) {\n\t\tret = vmw_rebind_contexts(sw_context);\n\t\tif (unlikely(ret != 0))\n\t\t\tgoto out_unlock_binding;\n\t}\n\n\tif (!header) {\n\t\tret = vmw_execbuf_submit_fifo(dev_priv, kernel_commands,\n\t\t\t\t\t      command_size, sw_context);\n\t} else {\n\t\tret = vmw_execbuf_submit_cmdbuf(dev_priv, header, command_size,\n\t\t\t\t\t\tsw_context);\n\t\theader = NULL;\n\t}\n\tmutex_unlock(&dev_priv->binding_mutex);\n\tif (ret)\n\t\tgoto out_err;\n\n\tvmw_query_bo_switch_commit(dev_priv, sw_context);\n\tret = vmw_execbuf_fence_commands(file_priv, dev_priv, &fence,\n\t\t\t\t\t (user_fence_rep) ? &handle : NULL);\n\t/*\n\t * This error is harmless, because if fence submission fails,\n\t * vmw_fifo_send_fence will sync. The error will be propagated to\n\t * user-space in @fence_rep\n\t */\n\tif (ret != 0)\n\t\tVMW_DEBUG_USER(\"Fence submission error. Syncing.\\n\");\n\n\tvmw_execbuf_bindings_commit(sw_context, false);\n\tvmw_bind_dx_query_mob(sw_context);\n\tvmw_validation_res_unreserve(&val_ctx, false);\n\n\tvmw_validation_bo_fence(sw_context->ctx, fence);\n\n\tif (unlikely(dev_priv->pinned_bo != NULL && !dev_priv->query_cid_valid))\n\t\t__vmw_execbuf_release_pinned_bo(dev_priv, fence);\n\n\t/*\n\t * If anything fails here, give up trying to export the fence and do a\n\t * sync since the user mode will not be able to sync the fence itself.\n\t * This ensures we are still functionally correct.\n\t */\n\tif (flags & DRM_VMW_EXECBUF_FLAG_EXPORT_FENCE_FD) {\n\n\t\tsync_file = sync_file_create(&fence->base);\n\t\tif (!sync_file) {\n\t\t\tVMW_DEBUG_USER(\"Sync file create failed for fence\\n\");\n\t\t\tput_unused_fd(out_fence_fd);\n\t\t\tout_fence_fd = -1;\n\n\t\t\t(void) vmw_fence_obj_wait(fence, false, false,\n\t\t\t\t\t\t  VMW_FENCE_WAIT_TIMEOUT);\n\t\t} else {\n\t\t\t/* Link the fence with the FD created earlier */\n\t\t\tfd_install(out_fence_fd, sync_file->file);\n\t\t}\n\t}\n\n\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv), ret,\n\t\t\t\t    user_fence_rep, fence, handle, out_fence_fd,\n\t\t\t\t    sync_file);\n\n\t/* Don't unreference when handing fence out */\n\tif (unlikely(out_fence != NULL)) {\n\t\t*out_fence = fence;\n\t\tfence = NULL;\n\t} else if (likely(fence != NULL)) {\n\t\tvmw_fence_obj_unreference(&fence);\n\t}\n\n\tvmw_cmdbuf_res_commit(&sw_context->staged_cmd_res);\n\tmutex_unlock(&dev_priv->cmdbuf_mutex);\n\n\t/*\n\t * Unreference resources outside of the cmdbuf_mutex to avoid deadlocks\n\t * in resource destruction paths.\n\t */\n\tvmw_validation_unref_lists(&val_ctx);\n\n\treturn 0;\n\nout_unlock_binding:\n\tmutex_unlock(&dev_priv->binding_mutex);\nout_err:\n\tvmw_validation_bo_backoff(&val_ctx);\nout_err_nores:\n\tvmw_execbuf_bindings_commit(sw_context, true);\n\tvmw_validation_res_unreserve(&val_ctx, true);\n\tvmw_resource_relocations_free(&sw_context->res_relocations);\n\tvmw_free_relocations(sw_context);\n\tif (unlikely(dev_priv->pinned_bo != NULL && !dev_priv->query_cid_valid))\n\t\t__vmw_execbuf_release_pinned_bo(dev_priv, NULL);\nout_unlock:\n\tvmw_cmdbuf_res_revert(&sw_context->staged_cmd_res);\n\tvmw_validation_drop_ht(&val_ctx);\n\tWARN_ON(!list_empty(&sw_context->ctx_list));\n\tmutex_unlock(&dev_priv->cmdbuf_mutex);\n\n\t/*\n\t * Unreference resources outside of the cmdbuf_mutex to avoid deadlocks\n\t * in resource destruction paths.\n\t */\n\tvmw_validation_unref_lists(&val_ctx);\nout_free_header:\n\tif (header)\n\t\tvmw_cmdbuf_header_free(header);\nout_free_fence_fd:\n\tif (out_fence_fd >= 0)\n\t\tput_unused_fd(out_fence_fd);\n\n\treturn ret;\n}",
        "code_after_change": "int vmw_execbuf_process(struct drm_file *file_priv,\n\t\t\tstruct vmw_private *dev_priv,\n\t\t\tvoid __user *user_commands, void *kernel_commands,\n\t\t\tuint32_t command_size, uint64_t throttle_us,\n\t\t\tuint32_t dx_context_handle,\n\t\t\tstruct drm_vmw_fence_rep __user *user_fence_rep,\n\t\t\tstruct vmw_fence_obj **out_fence, uint32_t flags)\n{\n\tstruct vmw_sw_context *sw_context = &dev_priv->ctx;\n\tstruct vmw_fence_obj *fence = NULL;\n\tstruct vmw_cmdbuf_header *header;\n\tuint32_t handle = 0;\n\tint ret;\n\tint32_t out_fence_fd = -1;\n\tstruct sync_file *sync_file = NULL;\n\tDECLARE_VAL_CONTEXT(val_ctx, &sw_context->res_ht, 1);\n\n\tif (flags & DRM_VMW_EXECBUF_FLAG_EXPORT_FENCE_FD) {\n\t\tout_fence_fd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (out_fence_fd < 0) {\n\t\t\tVMW_DEBUG_USER(\"Failed to get a fence fd.\\n\");\n\t\t\treturn out_fence_fd;\n\t\t}\n\t}\n\n\tif (throttle_us) {\n\t\tVMW_DEBUG_USER(\"Throttling is no longer supported.\\n\");\n\t}\n\n\tkernel_commands = vmw_execbuf_cmdbuf(dev_priv, user_commands,\n\t\t\t\t\t     kernel_commands, command_size,\n\t\t\t\t\t     &header);\n\tif (IS_ERR(kernel_commands)) {\n\t\tret = PTR_ERR(kernel_commands);\n\t\tgoto out_free_fence_fd;\n\t}\n\n\tret = mutex_lock_interruptible(&dev_priv->cmdbuf_mutex);\n\tif (ret) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto out_free_header;\n\t}\n\n\tsw_context->kernel = false;\n\tif (kernel_commands == NULL) {\n\t\tret = vmw_resize_cmd_bounce(sw_context, command_size);\n\t\tif (unlikely(ret != 0))\n\t\t\tgoto out_unlock;\n\n\t\tret = copy_from_user(sw_context->cmd_bounce, user_commands,\n\t\t\t\t     command_size);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tret = -EFAULT;\n\t\t\tVMW_DEBUG_USER(\"Failed copying commands.\\n\");\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tkernel_commands = sw_context->cmd_bounce;\n\t} else if (!header) {\n\t\tsw_context->kernel = true;\n\t}\n\n\tsw_context->filp = file_priv;\n\tsw_context->fp = vmw_fpriv(file_priv);\n\tINIT_LIST_HEAD(&sw_context->ctx_list);\n\tsw_context->cur_query_bo = dev_priv->pinned_bo;\n\tsw_context->last_query_ctx = NULL;\n\tsw_context->needs_post_query_barrier = false;\n\tsw_context->dx_ctx_node = NULL;\n\tsw_context->dx_query_mob = NULL;\n\tsw_context->dx_query_ctx = NULL;\n\tmemset(sw_context->res_cache, 0, sizeof(sw_context->res_cache));\n\tINIT_LIST_HEAD(&sw_context->res_relocations);\n\tINIT_LIST_HEAD(&sw_context->bo_relocations);\n\n\tif (sw_context->staged_bindings)\n\t\tvmw_binding_state_reset(sw_context->staged_bindings);\n\n\tif (!sw_context->res_ht_initialized) {\n\t\tret = vmwgfx_ht_create(&sw_context->res_ht, VMW_RES_HT_ORDER);\n\t\tif (unlikely(ret != 0))\n\t\t\tgoto out_unlock;\n\n\t\tsw_context->res_ht_initialized = true;\n\t}\n\n\tINIT_LIST_HEAD(&sw_context->staged_cmd_res);\n\tsw_context->ctx = &val_ctx;\n\tret = vmw_execbuf_tie_context(dev_priv, sw_context, dx_context_handle);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err_nores;\n\n\tret = vmw_cmd_check_all(dev_priv, sw_context, kernel_commands,\n\t\t\t\tcommand_size);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err_nores;\n\n\tret = vmw_resources_reserve(sw_context);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err_nores;\n\n\tret = vmw_validation_bo_reserve(&val_ctx, true);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err_nores;\n\n\tret = vmw_validation_bo_validate(&val_ctx, true);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err;\n\n\tret = vmw_validation_res_validate(&val_ctx, true);\n\tif (unlikely(ret != 0))\n\t\tgoto out_err;\n\n\tvmw_validation_drop_ht(&val_ctx);\n\n\tret = mutex_lock_interruptible(&dev_priv->binding_mutex);\n\tif (unlikely(ret != 0)) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto out_err;\n\t}\n\n\tif (dev_priv->has_mob) {\n\t\tret = vmw_rebind_contexts(sw_context);\n\t\tif (unlikely(ret != 0))\n\t\t\tgoto out_unlock_binding;\n\t}\n\n\tif (!header) {\n\t\tret = vmw_execbuf_submit_fifo(dev_priv, kernel_commands,\n\t\t\t\t\t      command_size, sw_context);\n\t} else {\n\t\tret = vmw_execbuf_submit_cmdbuf(dev_priv, header, command_size,\n\t\t\t\t\t\tsw_context);\n\t\theader = NULL;\n\t}\n\tmutex_unlock(&dev_priv->binding_mutex);\n\tif (ret)\n\t\tgoto out_err;\n\n\tvmw_query_bo_switch_commit(dev_priv, sw_context);\n\tret = vmw_execbuf_fence_commands(file_priv, dev_priv, &fence,\n\t\t\t\t\t (user_fence_rep) ? &handle : NULL);\n\t/*\n\t * This error is harmless, because if fence submission fails,\n\t * vmw_fifo_send_fence will sync. The error will be propagated to\n\t * user-space in @fence_rep\n\t */\n\tif (ret != 0)\n\t\tVMW_DEBUG_USER(\"Fence submission error. Syncing.\\n\");\n\n\tvmw_execbuf_bindings_commit(sw_context, false);\n\tvmw_bind_dx_query_mob(sw_context);\n\tvmw_validation_res_unreserve(&val_ctx, false);\n\n\tvmw_validation_bo_fence(sw_context->ctx, fence);\n\n\tif (unlikely(dev_priv->pinned_bo != NULL && !dev_priv->query_cid_valid))\n\t\t__vmw_execbuf_release_pinned_bo(dev_priv, fence);\n\n\t/*\n\t * If anything fails here, give up trying to export the fence and do a\n\t * sync since the user mode will not be able to sync the fence itself.\n\t * This ensures we are still functionally correct.\n\t */\n\tif (flags & DRM_VMW_EXECBUF_FLAG_EXPORT_FENCE_FD) {\n\n\t\tsync_file = sync_file_create(&fence->base);\n\t\tif (!sync_file) {\n\t\t\tVMW_DEBUG_USER(\"Sync file create failed for fence\\n\");\n\t\t\tput_unused_fd(out_fence_fd);\n\t\t\tout_fence_fd = -1;\n\n\t\t\t(void) vmw_fence_obj_wait(fence, false, false,\n\t\t\t\t\t\t  VMW_FENCE_WAIT_TIMEOUT);\n\t\t}\n\t}\n\n\tret = vmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv), ret,\n\t\t\t\t    user_fence_rep, fence, handle, out_fence_fd);\n\n\tif (sync_file) {\n\t\tif (ret) {\n\t\t\t/* usercopy of fence failed, put the file object */\n\t\t\tfput(sync_file->file);\n\t\t\tput_unused_fd(out_fence_fd);\n\t\t} else {\n\t\t\t/* Link the fence with the FD created earlier */\n\t\t\tfd_install(out_fence_fd, sync_file->file);\n\t\t}\n\t}\n\n\t/* Don't unreference when handing fence out */\n\tif (unlikely(out_fence != NULL)) {\n\t\t*out_fence = fence;\n\t\tfence = NULL;\n\t} else if (likely(fence != NULL)) {\n\t\tvmw_fence_obj_unreference(&fence);\n\t}\n\n\tvmw_cmdbuf_res_commit(&sw_context->staged_cmd_res);\n\tmutex_unlock(&dev_priv->cmdbuf_mutex);\n\n\t/*\n\t * Unreference resources outside of the cmdbuf_mutex to avoid deadlocks\n\t * in resource destruction paths.\n\t */\n\tvmw_validation_unref_lists(&val_ctx);\n\n\treturn ret;\n\nout_unlock_binding:\n\tmutex_unlock(&dev_priv->binding_mutex);\nout_err:\n\tvmw_validation_bo_backoff(&val_ctx);\nout_err_nores:\n\tvmw_execbuf_bindings_commit(sw_context, true);\n\tvmw_validation_res_unreserve(&val_ctx, true);\n\tvmw_resource_relocations_free(&sw_context->res_relocations);\n\tvmw_free_relocations(sw_context);\n\tif (unlikely(dev_priv->pinned_bo != NULL && !dev_priv->query_cid_valid))\n\t\t__vmw_execbuf_release_pinned_bo(dev_priv, NULL);\nout_unlock:\n\tvmw_cmdbuf_res_revert(&sw_context->staged_cmd_res);\n\tvmw_validation_drop_ht(&val_ctx);\n\tWARN_ON(!list_empty(&sw_context->ctx_list));\n\tmutex_unlock(&dev_priv->cmdbuf_mutex);\n\n\t/*\n\t * Unreference resources outside of the cmdbuf_mutex to avoid deadlocks\n\t * in resource destruction paths.\n\t */\n\tvmw_validation_unref_lists(&val_ctx);\nout_free_header:\n\tif (header)\n\t\tvmw_cmdbuf_header_free(header);\nout_free_fence_fd:\n\tif (out_fence_fd >= 0)\n\t\tput_unused_fd(out_fence_fd);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -172,15 +172,22 @@\n \n \t\t\t(void) vmw_fence_obj_wait(fence, false, false,\n \t\t\t\t\t\t  VMW_FENCE_WAIT_TIMEOUT);\n+\t\t}\n+\t}\n+\n+\tret = vmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv), ret,\n+\t\t\t\t    user_fence_rep, fence, handle, out_fence_fd);\n+\n+\tif (sync_file) {\n+\t\tif (ret) {\n+\t\t\t/* usercopy of fence failed, put the file object */\n+\t\t\tfput(sync_file->file);\n+\t\t\tput_unused_fd(out_fence_fd);\n \t\t} else {\n \t\t\t/* Link the fence with the FD created earlier */\n \t\t\tfd_install(out_fence_fd, sync_file->file);\n \t\t}\n \t}\n-\n-\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv), ret,\n-\t\t\t\t    user_fence_rep, fence, handle, out_fence_fd,\n-\t\t\t\t    sync_file);\n \n \t/* Don't unreference when handing fence out */\n \tif (unlikely(out_fence != NULL)) {\n@@ -199,7 +206,7 @@\n \t */\n \tvmw_validation_unref_lists(&val_ctx);\n \n-\treturn 0;\n+\treturn ret;\n \n out_unlock_binding:\n \tmutex_unlock(&dev_priv->binding_mutex);",
        "function_modified_lines": {
            "added": [
                "\t\t}",
                "\t}",
                "",
                "\tret = vmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv), ret,",
                "\t\t\t\t    user_fence_rep, fence, handle, out_fence_fd);",
                "",
                "\tif (sync_file) {",
                "\t\tif (ret) {",
                "\t\t\t/* usercopy of fence failed, put the file object */",
                "\t\t\tfput(sync_file->file);",
                "\t\t\tput_unused_fd(out_fence_fd);",
                "\treturn ret;"
            ],
            "deleted": [
                "",
                "\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv), ret,",
                "\t\t\t\t    user_fence_rep, fence, handle, out_fence_fd,",
                "\t\t\t\t    sync_file);",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The vmwgfx driver contains a local privilege escalation vulnerability that allows unprivileged users to gain access to files opened by other processes on the system through a dangling 'file' pointer.",
        "id": 3416
    },
    {
        "cve_id": "CVE-2023-3610",
        "code_before_change": "static void nft_immediate_deactivate(const struct nft_ctx *ctx,\n\t\t\t\t     const struct nft_expr *expr,\n\t\t\t\t     enum nft_trans_phase phase)\n{\n\tconst struct nft_immediate_expr *priv = nft_expr_priv(expr);\n\n\tif (phase == NFT_TRANS_COMMIT)\n\t\treturn;\n\n\treturn nft_data_release(&priv->data, nft_dreg_to_type(priv->dreg));\n}",
        "code_after_change": "static void nft_immediate_deactivate(const struct nft_ctx *ctx,\n\t\t\t\t     const struct nft_expr *expr,\n\t\t\t\t     enum nft_trans_phase phase)\n{\n\tconst struct nft_immediate_expr *priv = nft_expr_priv(expr);\n\tconst struct nft_data *data = &priv->data;\n\tstruct nft_ctx chain_ctx;\n\tstruct nft_chain *chain;\n\tstruct nft_rule *rule;\n\n\tif (priv->dreg == NFT_REG_VERDICT) {\n\t\tswitch (data->verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\tchain = data->verdict.chain;\n\t\t\tif (!nft_chain_binding(chain))\n\t\t\t\tbreak;\n\n\t\t\tchain_ctx = *ctx;\n\t\t\tchain_ctx.chain = chain;\n\n\t\t\tlist_for_each_entry(rule, &chain->rules, list)\n\t\t\t\tnft_rule_expr_deactivate(&chain_ctx, rule, phase);\n\n\t\t\tswitch (phase) {\n\t\t\tcase NFT_TRANS_PREPARE:\n\t\t\t\tnft_deactivate_next(ctx->net, chain);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tnft_chain_del(chain);\n\t\t\t\tchain->bound = false;\n\t\t\t\tchain->table->use--;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (phase == NFT_TRANS_COMMIT)\n\t\treturn;\n\n\treturn nft_data_release(&priv->data, nft_dreg_to_type(priv->dreg));\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,40 @@\n \t\t\t\t     enum nft_trans_phase phase)\n {\n \tconst struct nft_immediate_expr *priv = nft_expr_priv(expr);\n+\tconst struct nft_data *data = &priv->data;\n+\tstruct nft_ctx chain_ctx;\n+\tstruct nft_chain *chain;\n+\tstruct nft_rule *rule;\n+\n+\tif (priv->dreg == NFT_REG_VERDICT) {\n+\t\tswitch (data->verdict.code) {\n+\t\tcase NFT_JUMP:\n+\t\tcase NFT_GOTO:\n+\t\t\tchain = data->verdict.chain;\n+\t\t\tif (!nft_chain_binding(chain))\n+\t\t\t\tbreak;\n+\n+\t\t\tchain_ctx = *ctx;\n+\t\t\tchain_ctx.chain = chain;\n+\n+\t\t\tlist_for_each_entry(rule, &chain->rules, list)\n+\t\t\t\tnft_rule_expr_deactivate(&chain_ctx, rule, phase);\n+\n+\t\t\tswitch (phase) {\n+\t\t\tcase NFT_TRANS_PREPARE:\n+\t\t\t\tnft_deactivate_next(ctx->net, chain);\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\tnft_chain_del(chain);\n+\t\t\t\tchain->bound = false;\n+\t\t\t\tchain->table->use--;\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tbreak;\n+\t\t}\n+\t}\n \n \tif (phase == NFT_TRANS_COMMIT)\n \t\treturn;",
        "function_modified_lines": {
            "added": [
                "\tconst struct nft_data *data = &priv->data;",
                "\tstruct nft_ctx chain_ctx;",
                "\tstruct nft_chain *chain;",
                "\tstruct nft_rule *rule;",
                "",
                "\tif (priv->dreg == NFT_REG_VERDICT) {",
                "\t\tswitch (data->verdict.code) {",
                "\t\tcase NFT_JUMP:",
                "\t\tcase NFT_GOTO:",
                "\t\t\tchain = data->verdict.chain;",
                "\t\t\tif (!nft_chain_binding(chain))",
                "\t\t\t\tbreak;",
                "",
                "\t\t\tchain_ctx = *ctx;",
                "\t\t\tchain_ctx.chain = chain;",
                "",
                "\t\t\tlist_for_each_entry(rule, &chain->rules, list)",
                "\t\t\t\tnft_rule_expr_deactivate(&chain_ctx, rule, phase);",
                "",
                "\t\t\tswitch (phase) {",
                "\t\t\tcase NFT_TRANS_PREPARE:",
                "\t\t\t\tnft_deactivate_next(ctx->net, chain);",
                "\t\t\t\tbreak;",
                "\t\t\tdefault:",
                "\t\t\t\tnft_chain_del(chain);",
                "\t\t\t\tchain->bound = false;",
                "\t\t\t\tchain->table->use--;",
                "\t\t\t\tbreak;",
                "\t\t\t}",
                "\t\t\tbreak;",
                "\t\tdefault:",
                "\t\t\tbreak;",
                "\t\t}",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nFlaw in the error handling of bound chains causes a use-after-free in the abort path of NFT_MSG_NEWRULE. The vulnerability requires CAP_NET_ADMIN to be triggered.\n\nWe recommend upgrading past commit 4bedf9eee016286c835e3d8fa981ddece5338795.\n\n",
        "id": 4128
    },
    {
        "cve_id": "CVE-2022-41849",
        "code_before_change": "static void ufx_usb_disconnect(struct usb_interface *interface)\n{\n\tstruct ufx_data *dev;\n\n\tdev = usb_get_intfdata(interface);\n\n\tpr_debug(\"USB disconnect starting\\n\");\n\n\t/* we virtualize until all fb clients release. Then we free */\n\tdev->virtualized = true;\n\n\t/* When non-active we'll update virtual framebuffer, but no new urbs */\n\tatomic_set(&dev->usb_active, 0);\n\n\tusb_set_intfdata(interface, NULL);\n\n\t/* if clients still have us open, will be freed on last close */\n\tif (dev->fb_count == 0)\n\t\tschedule_delayed_work(&dev->free_framebuffer_work, 0);\n\n\t/* release reference taken by kref_init in probe() */\n\tkref_put(&dev->kref, ufx_free);\n\n\t/* consider ufx_data freed */\n}",
        "code_after_change": "static void ufx_usb_disconnect(struct usb_interface *interface)\n{\n\tstruct ufx_data *dev;\n\n\tmutex_lock(&disconnect_mutex);\n\n\tdev = usb_get_intfdata(interface);\n\n\tpr_debug(\"USB disconnect starting\\n\");\n\n\t/* we virtualize until all fb clients release. Then we free */\n\tdev->virtualized = true;\n\n\t/* When non-active we'll update virtual framebuffer, but no new urbs */\n\tatomic_set(&dev->usb_active, 0);\n\n\tusb_set_intfdata(interface, NULL);\n\n\t/* if clients still have us open, will be freed on last close */\n\tif (dev->fb_count == 0)\n\t\tschedule_delayed_work(&dev->free_framebuffer_work, 0);\n\n\t/* release reference taken by kref_init in probe() */\n\tkref_put(&dev->kref, ufx_free);\n\n\t/* consider ufx_data freed */\n\n\tmutex_unlock(&disconnect_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,8 @@\n static void ufx_usb_disconnect(struct usb_interface *interface)\n {\n \tstruct ufx_data *dev;\n+\n+\tmutex_lock(&disconnect_mutex);\n \n \tdev = usb_get_intfdata(interface);\n \n@@ -22,4 +24,6 @@\n \tkref_put(&dev->kref, ufx_free);\n \n \t/* consider ufx_data freed */\n+\n+\tmutex_unlock(&disconnect_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tmutex_lock(&disconnect_mutex);",
                "",
                "\tmutex_unlock(&disconnect_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "drivers/video/fbdev/smscufx.c in the Linux kernel through 5.19.12 has a race condition and resultant use-after-free if a physically proximate attacker removes a USB device while calling open(), aka a race condition between ufx_ops_open and ufx_usb_disconnect.",
        "id": 3720
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static void io_req_clean_work(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\treturn;\n\n\tif (req->flags & REQ_F_INFLIGHT) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\tstruct io_uring_task *tctx = req->task->io_uring;\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\t\tlist_del(&req->inflight_entry);\n\t\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\t\treq->flags &= ~REQ_F_INFLIGHT;\n\t\tif (atomic_read(&tctx->in_idle))\n\t\t\twake_up(&tctx->wait);\n\t}\n\n\treq->flags &= ~REQ_F_WORK_INITIALIZED;\n\tio_put_identity(req->task->io_uring, req);\n}",
        "code_after_change": "static void io_req_clean_work(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\treturn;\n\n\tif (req->work.creds) {\n\t\tput_cred(req->work.creds);\n\t\treq->work.creds = NULL;\n\t}\n\tif (req->flags & REQ_F_INFLIGHT) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\tstruct io_uring_task *tctx = req->task->io_uring;\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\t\tlist_del(&req->inflight_entry);\n\t\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\t\treq->flags &= ~REQ_F_INFLIGHT;\n\t\tif (atomic_read(&tctx->in_idle))\n\t\t\twake_up(&tctx->wait);\n\t}\n\n\treq->flags &= ~REQ_F_WORK_INITIALIZED;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,10 @@\n \tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n \t\treturn;\n \n+\tif (req->work.creds) {\n+\t\tput_cred(req->work.creds);\n+\t\treq->work.creds = NULL;\n+\t}\n \tif (req->flags & REQ_F_INFLIGHT) {\n \t\tstruct io_ring_ctx *ctx = req->ctx;\n \t\tstruct io_uring_task *tctx = req->task->io_uring;\n@@ -17,5 +21,4 @@\n \t}\n \n \treq->flags &= ~REQ_F_WORK_INITIALIZED;\n-\tio_put_identity(req->task->io_uring, req);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (req->work.creds) {",
                "\t\tput_cred(req->work.creds);",
                "\t\treq->work.creds = NULL;",
                "\t}"
            ],
            "deleted": [
                "\tio_put_identity(req->task->io_uring, req);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3291
    },
    {
        "cve_id": "CVE-2022-1204",
        "code_before_change": "static int ax25_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tax25_cb *ax25;\n\n\tif (sk == NULL)\n\t\treturn 0;\n\n\tsock_hold(sk);\n\tsock_orphan(sk);\n\tlock_sock(sk);\n\tax25 = sk_to_ax25(sk);\n\n\tif (sk->sk_type == SOCK_SEQPACKET) {\n\t\tswitch (ax25->state) {\n\t\tcase AX25_STATE_0:\n\t\t\trelease_sock(sk);\n\t\t\tax25_disconnect(ax25, 0);\n\t\t\tlock_sock(sk);\n\t\t\tax25_destroy_socket(ax25);\n\t\t\tbreak;\n\n\t\tcase AX25_STATE_1:\n\t\tcase AX25_STATE_2:\n\t\t\tax25_send_control(ax25, AX25_DISC, AX25_POLLON, AX25_COMMAND);\n\t\t\trelease_sock(sk);\n\t\t\tax25_disconnect(ax25, 0);\n\t\t\tlock_sock(sk);\n\t\t\tif (!sock_flag(ax25->sk, SOCK_DESTROY))\n\t\t\t\tax25_destroy_socket(ax25);\n\t\t\tbreak;\n\n\t\tcase AX25_STATE_3:\n\t\tcase AX25_STATE_4:\n\t\t\tax25_clear_queues(ax25);\n\t\t\tax25->n2count = 0;\n\n\t\t\tswitch (ax25->ax25_dev->values[AX25_VALUES_PROTOCOL]) {\n\t\t\tcase AX25_PROTO_STD_SIMPLEX:\n\t\t\tcase AX25_PROTO_STD_DUPLEX:\n\t\t\t\tax25_send_control(ax25,\n\t\t\t\t\t\t  AX25_DISC,\n\t\t\t\t\t\t  AX25_POLLON,\n\t\t\t\t\t\t  AX25_COMMAND);\n\t\t\t\tax25_stop_t2timer(ax25);\n\t\t\t\tax25_stop_t3timer(ax25);\n\t\t\t\tax25_stop_idletimer(ax25);\n\t\t\t\tbreak;\n#ifdef CONFIG_AX25_DAMA_SLAVE\n\t\t\tcase AX25_PROTO_DAMA_SLAVE:\n\t\t\t\tax25_stop_t3timer(ax25);\n\t\t\t\tax25_stop_idletimer(ax25);\n\t\t\t\tbreak;\n#endif\n\t\t\t}\n\t\t\tax25_calculate_t1(ax25);\n\t\t\tax25_start_t1timer(ax25);\n\t\t\tax25->state = AX25_STATE_2;\n\t\t\tsk->sk_state                = TCP_CLOSE;\n\t\t\tsk->sk_shutdown            |= SEND_SHUTDOWN;\n\t\t\tsk->sk_state_change(sk);\n\t\t\tsock_set_flag(sk, SOCK_DESTROY);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tsk->sk_state     = TCP_CLOSE;\n\t\tsk->sk_shutdown |= SEND_SHUTDOWN;\n\t\tsk->sk_state_change(sk);\n\t\tax25_destroy_socket(ax25);\n\t}\n\n\tsock->sk   = NULL;\n\trelease_sock(sk);\n\tsock_put(sk);\n\n\treturn 0;\n}",
        "code_after_change": "static int ax25_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tax25_cb *ax25;\n\tax25_dev *ax25_dev;\n\n\tif (sk == NULL)\n\t\treturn 0;\n\n\tsock_hold(sk);\n\tlock_sock(sk);\n\tsock_orphan(sk);\n\tax25 = sk_to_ax25(sk);\n\tax25_dev = ax25->ax25_dev;\n\tif (ax25_dev) {\n\t\tdev_put_track(ax25_dev->dev, &ax25_dev->dev_tracker);\n\t\tax25_dev_put(ax25_dev);\n\t}\n\n\tif (sk->sk_type == SOCK_SEQPACKET) {\n\t\tswitch (ax25->state) {\n\t\tcase AX25_STATE_0:\n\t\t\trelease_sock(sk);\n\t\t\tax25_disconnect(ax25, 0);\n\t\t\tlock_sock(sk);\n\t\t\tax25_destroy_socket(ax25);\n\t\t\tbreak;\n\n\t\tcase AX25_STATE_1:\n\t\tcase AX25_STATE_2:\n\t\t\tax25_send_control(ax25, AX25_DISC, AX25_POLLON, AX25_COMMAND);\n\t\t\trelease_sock(sk);\n\t\t\tax25_disconnect(ax25, 0);\n\t\t\tlock_sock(sk);\n\t\t\tif (!sock_flag(ax25->sk, SOCK_DESTROY))\n\t\t\t\tax25_destroy_socket(ax25);\n\t\t\tbreak;\n\n\t\tcase AX25_STATE_3:\n\t\tcase AX25_STATE_4:\n\t\t\tax25_clear_queues(ax25);\n\t\t\tax25->n2count = 0;\n\n\t\t\tswitch (ax25->ax25_dev->values[AX25_VALUES_PROTOCOL]) {\n\t\t\tcase AX25_PROTO_STD_SIMPLEX:\n\t\t\tcase AX25_PROTO_STD_DUPLEX:\n\t\t\t\tax25_send_control(ax25,\n\t\t\t\t\t\t  AX25_DISC,\n\t\t\t\t\t\t  AX25_POLLON,\n\t\t\t\t\t\t  AX25_COMMAND);\n\t\t\t\tax25_stop_t2timer(ax25);\n\t\t\t\tax25_stop_t3timer(ax25);\n\t\t\t\tax25_stop_idletimer(ax25);\n\t\t\t\tbreak;\n#ifdef CONFIG_AX25_DAMA_SLAVE\n\t\t\tcase AX25_PROTO_DAMA_SLAVE:\n\t\t\t\tax25_stop_t3timer(ax25);\n\t\t\t\tax25_stop_idletimer(ax25);\n\t\t\t\tbreak;\n#endif\n\t\t\t}\n\t\t\tax25_calculate_t1(ax25);\n\t\t\tax25_start_t1timer(ax25);\n\t\t\tax25->state = AX25_STATE_2;\n\t\t\tsk->sk_state                = TCP_CLOSE;\n\t\t\tsk->sk_shutdown            |= SEND_SHUTDOWN;\n\t\t\tsk->sk_state_change(sk);\n\t\t\tsock_set_flag(sk, SOCK_DESTROY);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tsk->sk_state     = TCP_CLOSE;\n\t\tsk->sk_shutdown |= SEND_SHUTDOWN;\n\t\tsk->sk_state_change(sk);\n\t\tax25_destroy_socket(ax25);\n\t}\n\n\tsock->sk   = NULL;\n\trelease_sock(sk);\n\tsock_put(sk);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,14 +2,20 @@\n {\n \tstruct sock *sk = sock->sk;\n \tax25_cb *ax25;\n+\tax25_dev *ax25_dev;\n \n \tif (sk == NULL)\n \t\treturn 0;\n \n \tsock_hold(sk);\n+\tlock_sock(sk);\n \tsock_orphan(sk);\n-\tlock_sock(sk);\n \tax25 = sk_to_ax25(sk);\n+\tax25_dev = ax25->ax25_dev;\n+\tif (ax25_dev) {\n+\t\tdev_put_track(ax25_dev->dev, &ax25_dev->dev_tracker);\n+\t\tax25_dev_put(ax25_dev);\n+\t}\n \n \tif (sk->sk_type == SOCK_SEQPACKET) {\n \t\tswitch (ax25->state) {",
        "function_modified_lines": {
            "added": [
                "\tax25_dev *ax25_dev;",
                "\tlock_sock(sk);",
                "\tax25_dev = ax25->ax25_dev;",
                "\tif (ax25_dev) {",
                "\t\tdev_put_track(ax25_dev->dev, &ax25_dev->dev_tracker);",
                "\t\tax25_dev_put(ax25_dev);",
                "\t}"
            ],
            "deleted": [
                "\tlock_sock(sk);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s Amateur Radio AX.25 protocol functionality in the way a user connects with the protocol. This flaw allows a local user to crash the system.",
        "id": 3254
    },
    {
        "cve_id": "CVE-2023-33250",
        "code_before_change": "void iommufd_access_unpin_pages(struct iommufd_access *access,\n\t\t\t\tunsigned long iova, unsigned long length)\n{\n\tstruct io_pagetable *iopt = &access->ioas->iopt;\n\tstruct iopt_area_contig_iter iter;\n\tunsigned long last_iova;\n\tstruct iopt_area *area;\n\n\tif (WARN_ON(!length) ||\n\t    WARN_ON(check_add_overflow(iova, length - 1, &last_iova)))\n\t\treturn;\n\n\tdown_read(&iopt->iova_rwsem);\n\tiopt_for_each_contig_area(&iter, area, iopt, iova, last_iova)\n\t\tiopt_area_remove_access(\n\t\t\tarea, iopt_area_iova_to_index(area, iter.cur_iova),\n\t\t\tiopt_area_iova_to_index(\n\t\t\t\tarea,\n\t\t\t\tmin(last_iova, iopt_area_last_iova(area))));\n\tup_read(&iopt->iova_rwsem);\n\tWARN_ON(!iopt_area_contig_done(&iter));\n}",
        "code_after_change": "void iommufd_access_unpin_pages(struct iommufd_access *access,\n\t\t\t\tunsigned long iova, unsigned long length)\n{\n\tstruct io_pagetable *iopt = &access->ioas->iopt;\n\tstruct iopt_area_contig_iter iter;\n\tunsigned long last_iova;\n\tstruct iopt_area *area;\n\n\tif (WARN_ON(!length) ||\n\t    WARN_ON(check_add_overflow(iova, length - 1, &last_iova)))\n\t\treturn;\n\n\tdown_read(&iopt->iova_rwsem);\n\tiopt_for_each_contig_area(&iter, area, iopt, iova, last_iova)\n\t\tiopt_area_remove_access(\n\t\t\tarea, iopt_area_iova_to_index(area, iter.cur_iova),\n\t\t\tiopt_area_iova_to_index(\n\t\t\t\tarea,\n\t\t\t\tmin(last_iova, iopt_area_last_iova(area))));\n\tWARN_ON(!iopt_area_contig_done(&iter));\n\tup_read(&iopt->iova_rwsem);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,6 @@\n \t\t\tiopt_area_iova_to_index(\n \t\t\t\tarea,\n \t\t\t\tmin(last_iova, iopt_area_last_iova(area))));\n+\tWARN_ON(!iopt_area_contig_done(&iter));\n \tup_read(&iopt->iova_rwsem);\n-\tWARN_ON(!iopt_area_contig_done(&iter));\n }",
        "function_modified_lines": {
            "added": [
                "\tWARN_ON(!iopt_area_contig_done(&iter));"
            ],
            "deleted": [
                "\tWARN_ON(!iopt_area_contig_done(&iter));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel 6.3 has a use-after-free in iopt_unmap_iova_range in drivers/iommu/iommufd/io_pagetable.c.",
        "id": 4058
    },
    {
        "cve_id": "CVE-2021-20292",
        "code_before_change": "int ttm_tt_init(struct ttm_tt *ttm, struct ttm_buffer_object *bo,\n\t\tuint32_t page_flags)\n{\n\tttm_tt_init_fields(ttm, bo, page_flags);\n\n\tif (ttm_tt_alloc_page_directory(ttm)) {\n\t\tttm_tt_destroy(ttm);\n\t\tpr_err(\"Failed allocating page table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}",
        "code_after_change": "int ttm_tt_init(struct ttm_tt *ttm, struct ttm_buffer_object *bo,\n\t\tuint32_t page_flags)\n{\n\tttm_tt_init_fields(ttm, bo, page_flags);\n\n\tif (ttm_tt_alloc_page_directory(ttm)) {\n\t\tpr_err(\"Failed allocating page table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \tttm_tt_init_fields(ttm, bo, page_flags);\n \n \tif (ttm_tt_alloc_page_directory(ttm)) {\n-\t\tttm_tt_destroy(ttm);\n \t\tpr_err(\"Failed allocating page table\\n\");\n \t\treturn -ENOMEM;\n \t}",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tttm_tt_destroy(ttm);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a flaw reported in the Linux kernel in versions before 5.9 in drivers/gpu/drm/nouveau/nouveau_sgdma.c in nouveau_sgdma_create_ttm in Nouveau DRM subsystem. The issue results from the lack of validating the existence of an object prior to performing operations on the object. An attacker with a local account with a root privilege, can leverage this vulnerability to escalate privileges and execute code in the context of the kernel.",
        "id": 2870
    },
    {
        "cve_id": "CVE-2021-28691",
        "code_before_change": "static void xenvif_disconnect_queue(struct xenvif_queue *queue)\n{\n\tif (queue->task) {\n\t\tkthread_stop(queue->task);\n\t\tqueue->task = NULL;\n\t}\n\n\tif (queue->dealloc_task) {\n\t\tkthread_stop(queue->dealloc_task);\n\t\tqueue->dealloc_task = NULL;\n\t}\n\n\tif (queue->napi.poll) {\n\t\tnetif_napi_del(&queue->napi);\n\t\tqueue->napi.poll = NULL;\n\t}\n\n\tif (queue->tx_irq) {\n\t\tunbind_from_irqhandler(queue->tx_irq, queue);\n\t\tif (queue->tx_irq == queue->rx_irq)\n\t\t\tqueue->rx_irq = 0;\n\t\tqueue->tx_irq = 0;\n\t}\n\n\tif (queue->rx_irq) {\n\t\tunbind_from_irqhandler(queue->rx_irq, queue);\n\t\tqueue->rx_irq = 0;\n\t}\n\n\txenvif_unmap_frontend_data_rings(queue);\n}",
        "code_after_change": "static void xenvif_disconnect_queue(struct xenvif_queue *queue)\n{\n\tif (queue->task) {\n\t\tkthread_stop(queue->task);\n\t\tput_task_struct(queue->task);\n\t\tqueue->task = NULL;\n\t}\n\n\tif (queue->dealloc_task) {\n\t\tkthread_stop(queue->dealloc_task);\n\t\tqueue->dealloc_task = NULL;\n\t}\n\n\tif (queue->napi.poll) {\n\t\tnetif_napi_del(&queue->napi);\n\t\tqueue->napi.poll = NULL;\n\t}\n\n\tif (queue->tx_irq) {\n\t\tunbind_from_irqhandler(queue->tx_irq, queue);\n\t\tif (queue->tx_irq == queue->rx_irq)\n\t\t\tqueue->rx_irq = 0;\n\t\tqueue->tx_irq = 0;\n\t}\n\n\tif (queue->rx_irq) {\n\t\tunbind_from_irqhandler(queue->rx_irq, queue);\n\t\tqueue->rx_irq = 0;\n\t}\n\n\txenvif_unmap_frontend_data_rings(queue);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n {\n \tif (queue->task) {\n \t\tkthread_stop(queue->task);\n+\t\tput_task_struct(queue->task);\n \t\tqueue->task = NULL;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tput_task_struct(queue->task);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Guest triggered use-after-free in Linux xen-netback A malicious or buggy network PV frontend can force Linux netback to disable the interface and terminate the receive kernel thread associated with queue 0 in response to the frontend sending a malformed packet. Such kernel thread termination will lead to a use-after-free in Linux netback when the backend is destroyed, as the kernel thread associated with queue 0 will have already exited and thus the call to kthread_stop will be performed against a stale pointer.",
        "id": 2917
    },
    {
        "cve_id": "CVE-2023-3610",
        "code_before_change": "static struct nft_trans *nft_trans_chain_add(struct nft_ctx *ctx, int msg_type)\n{\n\tstruct nft_trans *trans;\n\n\ttrans = nft_trans_alloc(ctx, msg_type, sizeof(struct nft_trans_chain));\n\tif (trans == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (msg_type == NFT_MSG_NEWCHAIN) {\n\t\tnft_activate_next(ctx->net, ctx->chain);\n\n\t\tif (ctx->nla[NFTA_CHAIN_ID]) {\n\t\t\tnft_trans_chain_id(trans) =\n\t\t\t\tntohl(nla_get_be32(ctx->nla[NFTA_CHAIN_ID]));\n\t\t}\n\t}\n\n\tnft_trans_commit_list_add_tail(ctx->net, trans);\n\treturn trans;\n}",
        "code_after_change": "static struct nft_trans *nft_trans_chain_add(struct nft_ctx *ctx, int msg_type)\n{\n\tstruct nft_trans *trans;\n\n\ttrans = nft_trans_alloc(ctx, msg_type, sizeof(struct nft_trans_chain));\n\tif (trans == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (msg_type == NFT_MSG_NEWCHAIN) {\n\t\tnft_activate_next(ctx->net, ctx->chain);\n\n\t\tif (ctx->nla[NFTA_CHAIN_ID]) {\n\t\t\tnft_trans_chain_id(trans) =\n\t\t\t\tntohl(nla_get_be32(ctx->nla[NFTA_CHAIN_ID]));\n\t\t}\n\t}\n\tnft_trans_chain(trans) = ctx->chain;\n\tnft_trans_commit_list_add_tail(ctx->net, trans);\n\n\treturn trans;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,7 +14,8 @@\n \t\t\t\tntohl(nla_get_be32(ctx->nla[NFTA_CHAIN_ID]));\n \t\t}\n \t}\n+\tnft_trans_chain(trans) = ctx->chain;\n+\tnft_trans_commit_list_add_tail(ctx->net, trans);\n \n-\tnft_trans_commit_list_add_tail(ctx->net, trans);\n \treturn trans;\n }",
        "function_modified_lines": {
            "added": [
                "\tnft_trans_chain(trans) = ctx->chain;",
                "\tnft_trans_commit_list_add_tail(ctx->net, trans);"
            ],
            "deleted": [
                "\tnft_trans_commit_list_add_tail(ctx->net, trans);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nFlaw in the error handling of bound chains causes a use-after-free in the abort path of NFT_MSG_NEWRULE. The vulnerability requires CAP_NET_ADMIN to be triggered.\n\nWe recommend upgrading past commit 4bedf9eee016286c835e3d8fa981ddece5338795.\n\n",
        "id": 4120
    },
    {
        "cve_id": "CVE-2023-1872",
        "code_before_change": "static void io_poll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tret = io_poll_check_events(req);\n\tif (ret > 0)\n\t\treturn;\n\n\tif (!ret) {\n\t\treq->result = mangle_poll(req->result & req->poll.events);\n\t} else {\n\t\treq->result = ret;\n\t\treq_set_fail(req);\n\t}\n\n\tio_poll_remove_entries(req);\n\tspin_lock(&ctx->completion_lock);\n\thash_del(&req->hash_node);\n\t__io_req_complete_post(req, req->result, 0);\n\tio_commit_cqring(ctx);\n\tspin_unlock(&ctx->completion_lock);\n\tio_cqring_ev_posted(ctx);\n}",
        "code_after_change": "static void io_poll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tret = io_poll_check_events(req, *locked);\n\tif (ret > 0)\n\t\treturn;\n\n\tif (!ret) {\n\t\treq->result = mangle_poll(req->result & req->poll.events);\n\t} else {\n\t\treq->result = ret;\n\t\treq_set_fail(req);\n\t}\n\n\tio_poll_remove_entries(req);\n\tspin_lock(&ctx->completion_lock);\n\thash_del(&req->hash_node);\n\t__io_req_complete_post(req, req->result, 0);\n\tio_commit_cqring(ctx);\n\tspin_unlock(&ctx->completion_lock);\n\tio_cqring_ev_posted(ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \tstruct io_ring_ctx *ctx = req->ctx;\n \tint ret;\n \n-\tret = io_poll_check_events(req);\n+\tret = io_poll_check_events(req, *locked);\n \tif (ret > 0)\n \t\treturn;\n ",
        "function_modified_lines": {
            "added": [
                "\tret = io_poll_check_events(req, *locked);"
            ],
            "deleted": [
                "\tret = io_poll_check_events(req);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring system can be exploited to achieve local privilege escalation.\n\nThe io_file_get_fixed function lacks the presence of ctx->uring_lock which can lead to a Use-After-Free vulnerability due a race condition with fixed files getting unregistered.\n\nWe recommend upgrading past commit da24142b1ef9fd5d36b76e36bab328a5b27523e8.\n\n",
        "id": 3884
    },
    {
        "cve_id": "CVE-2019-18683",
        "code_before_change": "static int vivid_thread_vid_out(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\n\tdprintk(dev, 1, \"Video Output Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->out_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->out_seq_count = 0xffffff80U;\n\tdev->jiffies_vid_out = jiffies;\n\tdev->vid_out_seq_start = dev->vbi_out_seq_start = 0;\n\tdev->meta_out_seq_start = 0;\n\tdev->out_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->mutex);\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->out_seq_resync) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = dev->out_seq_count + 1;\n\t\t\tdev->out_seq_count = 0;\n\t\t\tdev->out_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_out.numerator;\n\t\tdenominator = dev->timeperframe_vid_out.denominator;\n\n\t\tif (dev->field_out == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_out;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->out_seq_count = buffers_since_start + dev->out_seq_offset;\n\t\tdev->vid_out_seq_count = dev->out_seq_count - dev->vid_out_seq_start;\n\t\tdev->vbi_out_seq_count = dev->out_seq_count - dev->vbi_out_seq_start;\n\t\tdev->meta_out_seq_count = dev->out_seq_count - dev->meta_out_seq_start;\n\n\t\tvivid_thread_vid_out_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tnumerators_since_start = buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_out;\n\n\t\t/* Increase by the 'numerator' of one buffer */\n\t\tnumerators_since_start += numerator;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Output Thread End\\n\");\n\treturn 0;\n}",
        "code_after_change": "static int vivid_thread_vid_out(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\n\tdprintk(dev, 1, \"Video Output Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->out_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->out_seq_count = 0xffffff80U;\n\tdev->jiffies_vid_out = jiffies;\n\tdev->vid_out_seq_start = dev->vbi_out_seq_start = 0;\n\tdev->meta_out_seq_start = 0;\n\tdev->out_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (!mutex_trylock(&dev->mutex)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->out_seq_resync) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = dev->out_seq_count + 1;\n\t\t\tdev->out_seq_count = 0;\n\t\t\tdev->out_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_out.numerator;\n\t\tdenominator = dev->timeperframe_vid_out.denominator;\n\n\t\tif (dev->field_out == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_out;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->out_seq_count = buffers_since_start + dev->out_seq_offset;\n\t\tdev->vid_out_seq_count = dev->out_seq_count - dev->vid_out_seq_start;\n\t\tdev->vbi_out_seq_count = dev->out_seq_count - dev->vbi_out_seq_start;\n\t\tdev->meta_out_seq_count = dev->out_seq_count - dev->meta_out_seq_start;\n\n\t\tvivid_thread_vid_out_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tnumerators_since_start = buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_out;\n\n\t\t/* Increase by the 'numerator' of one buffer */\n\t\tnumerators_since_start += numerator;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Output Thread End\\n\");\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,7 +28,11 @@\n \t\tif (kthread_should_stop())\n \t\t\tbreak;\n \n-\t\tmutex_lock(&dev->mutex);\n+\t\tif (!mutex_trylock(&dev->mutex)) {\n+\t\t\tschedule_timeout_uninterruptible(1);\n+\t\t\tcontinue;\n+\t\t}\n+\n \t\tcur_jiffies = jiffies;\n \t\tif (dev->out_seq_resync) {\n \t\t\tdev->jiffies_vid_out = cur_jiffies;",
        "function_modified_lines": {
            "added": [
                "\t\tif (!mutex_trylock(&dev->mutex)) {",
                "\t\t\tschedule_timeout_uninterruptible(1);",
                "\t\t\tcontinue;",
                "\t\t}",
                ""
            ],
            "deleted": [
                "\t\tmutex_lock(&dev->mutex);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/media/platform/vivid in the Linux kernel through 5.3.8. It is exploitable for privilege escalation on some Linux distributions where local users have /dev/video0 access, but only if the driver happens to be loaded. There are multiple race conditions during streaming stopping in this driver (part of the V4L2 subsystem). These issues are caused by wrong mutex locking in vivid_stop_generating_vid_cap(), vivid_stop_generating_vid_out(), sdr_cap_stop_streaming(), and the corresponding kthreads. At least one of these race conditions leads to a use-after-free.",
        "id": 2093
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "void hfi1_mmu_rb_evict(struct mmu_rb_handler *handler, void *evict_arg)\n{\n\tstruct mmu_rb_node *rbnode, *ptr;\n\tstruct list_head del_list;\n\tunsigned long flags;\n\tbool stop = false;\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\tlist_for_each_entry_safe_reverse(rbnode, ptr, &handler->lru_list,\n\t\t\t\t\t list) {\n\t\tif (handler->ops->evict(handler->ops_arg, rbnode, evict_arg,\n\t\t\t\t\t&stop)) {\n\t\t\t__mmu_int_rb_remove(rbnode, &handler->root);\n\t\t\t/* move from LRU list to delete list */\n\t\t\tlist_move(&rbnode->list, &del_list);\n\t\t}\n\t\tif (stop)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\twhile (!list_empty(&del_list)) {\n\t\trbnode = list_first_entry(&del_list, struct mmu_rb_node, list);\n\t\tlist_del(&rbnode->list);\n\t\thandler->ops->remove(handler->ops_arg, rbnode);\n\t}\n}",
        "code_after_change": "void hfi1_mmu_rb_evict(struct mmu_rb_handler *handler, void *evict_arg)\n{\n\tstruct mmu_rb_node *rbnode, *ptr;\n\tstruct list_head del_list;\n\tunsigned long flags;\n\tbool stop = false;\n\n\tif (current->mm != handler->mn.mm)\n\t\treturn;\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\tlist_for_each_entry_safe_reverse(rbnode, ptr, &handler->lru_list,\n\t\t\t\t\t list) {\n\t\tif (handler->ops->evict(handler->ops_arg, rbnode, evict_arg,\n\t\t\t\t\t&stop)) {\n\t\t\t__mmu_int_rb_remove(rbnode, &handler->root);\n\t\t\t/* move from LRU list to delete list */\n\t\t\tlist_move(&rbnode->list, &del_list);\n\t\t}\n\t\tif (stop)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\twhile (!list_empty(&del_list)) {\n\t\trbnode = list_first_entry(&del_list, struct mmu_rb_node, list);\n\t\tlist_del(&rbnode->list);\n\t\thandler->ops->remove(handler->ops_arg, rbnode);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,9 @@\n \tstruct list_head del_list;\n \tunsigned long flags;\n \tbool stop = false;\n+\n+\tif (current->mm != handler->mn.mm)\n+\t\treturn;\n \n \tINIT_LIST_HEAD(&del_list);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (current->mm != handler->mn.mm)",
                "\t\treturn;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2644
    },
    {
        "cve_id": "CVE-2023-1872",
        "code_before_change": "static int io_tee(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tstruct file *in;\n\tlong ret = 0;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tin = io_file_get(req->ctx, req, sp->splice_fd_in,\n\t\t\t\t  (sp->flags & SPLICE_F_FD_IN_FIXED));\n\tif (!in) {\n\t\tret = -EBADF;\n\t\tgoto done;\n\t}\n\n\tif (sp->len)\n\t\tret = do_tee(in, out, sp->len, flags);\n\n\tif (!(sp->flags & SPLICE_F_FD_IN_FIXED))\n\t\tio_put_file(in);\ndone:\n\tif (ret != sp->len)\n\t\treq_set_fail(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}",
        "code_after_change": "static int io_tee(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tstruct file *in;\n\tlong ret = 0;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tif (sp->flags & SPLICE_F_FD_IN_FIXED)\n\t\tin = io_file_get_fixed(req, sp->splice_fd_in, IO_URING_F_UNLOCKED);\n\telse\n\t\tin = io_file_get_normal(req, sp->splice_fd_in);\n\tif (!in) {\n\t\tret = -EBADF;\n\t\tgoto done;\n\t}\n\n\tif (sp->len)\n\t\tret = do_tee(in, out, sp->len, flags);\n\n\tif (!(sp->flags & SPLICE_F_FD_IN_FIXED))\n\t\tio_put_file(in);\ndone:\n\tif (ret != sp->len)\n\t\treq_set_fail(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,8 +9,10 @@\n \tif (issue_flags & IO_URING_F_NONBLOCK)\n \t\treturn -EAGAIN;\n \n-\tin = io_file_get(req->ctx, req, sp->splice_fd_in,\n-\t\t\t\t  (sp->flags & SPLICE_F_FD_IN_FIXED));\n+\tif (sp->flags & SPLICE_F_FD_IN_FIXED)\n+\t\tin = io_file_get_fixed(req, sp->splice_fd_in, IO_URING_F_UNLOCKED);\n+\telse\n+\t\tin = io_file_get_normal(req, sp->splice_fd_in);\n \tif (!in) {\n \t\tret = -EBADF;\n \t\tgoto done;",
        "function_modified_lines": {
            "added": [
                "\tif (sp->flags & SPLICE_F_FD_IN_FIXED)",
                "\t\tin = io_file_get_fixed(req, sp->splice_fd_in, IO_URING_F_UNLOCKED);",
                "\telse",
                "\t\tin = io_file_get_normal(req, sp->splice_fd_in);"
            ],
            "deleted": [
                "\tin = io_file_get(req->ctx, req, sp->splice_fd_in,",
                "\t\t\t\t  (sp->flags & SPLICE_F_FD_IN_FIXED));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring system can be exploited to achieve local privilege escalation.\n\nThe io_file_get_fixed function lacks the presence of ctx->uring_lock which can lead to a Use-After-Free vulnerability due a race condition with fixed files getting unregistered.\n\nWe recommend upgrading past commit da24142b1ef9fd5d36b76e36bab328a5b27523e8.\n\n",
        "id": 3886
    },
    {
        "cve_id": "CVE-2023-3317",
        "code_before_change": "u8 mt7921_check_offload_capability(struct device *dev, const char *fw_wm)\n{\n\tstruct mt7921_fw_features *features = NULL;\n\tconst struct mt76_connac2_fw_trailer *hdr;\n\tstruct mt7921_realease_info *rel_info;\n\tconst struct firmware *fw;\n\tint ret, i, offset = 0;\n\tconst u8 *data, *end;\n\n\tret = request_firmware(&fw, fw_wm, dev);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!fw || !fw->data || fw->size < sizeof(*hdr)) {\n\t\tdev_err(dev, \"Invalid firmware\\n\");\n\t\tgoto out;\n\t}\n\n\tdata = fw->data;\n\thdr = (const void *)(fw->data + fw->size - sizeof(*hdr));\n\n\tfor (i = 0; i < hdr->n_region; i++) {\n\t\tconst struct mt76_connac2_fw_region *region;\n\n\t\tregion = (const void *)((const u8 *)hdr -\n\t\t\t\t\t(hdr->n_region - i) * sizeof(*region));\n\t\toffset += le32_to_cpu(region->len);\n\t}\n\n\tdata += offset + 16;\n\trel_info = (struct mt7921_realease_info *)data;\n\tdata += sizeof(*rel_info);\n\tend = data + le16_to_cpu(rel_info->len);\n\n\twhile (data < end) {\n\t\trel_info = (struct mt7921_realease_info *)data;\n\t\tdata += sizeof(*rel_info);\n\n\t\tif (rel_info->tag == MT7921_FW_TAG_FEATURE) {\n\t\t\tfeatures = (struct mt7921_fw_features *)data;\n\t\t\tbreak;\n\t\t}\n\n\t\tdata += le16_to_cpu(rel_info->len) + rel_info->pad_len;\n\t}\n\nout:\n\trelease_firmware(fw);\n\n\treturn features ? features->data : 0;\n}",
        "code_after_change": "u8 mt7921_check_offload_capability(struct device *dev, const char *fw_wm)\n{\n\tconst struct mt76_connac2_fw_trailer *hdr;\n\tstruct mt7921_realease_info *rel_info;\n\tconst struct firmware *fw;\n\tint ret, i, offset = 0;\n\tconst u8 *data, *end;\n\tu8 offload_caps = 0;\n\n\tret = request_firmware(&fw, fw_wm, dev);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!fw || !fw->data || fw->size < sizeof(*hdr)) {\n\t\tdev_err(dev, \"Invalid firmware\\n\");\n\t\tgoto out;\n\t}\n\n\tdata = fw->data;\n\thdr = (const void *)(fw->data + fw->size - sizeof(*hdr));\n\n\tfor (i = 0; i < hdr->n_region; i++) {\n\t\tconst struct mt76_connac2_fw_region *region;\n\n\t\tregion = (const void *)((const u8 *)hdr -\n\t\t\t\t\t(hdr->n_region - i) * sizeof(*region));\n\t\toffset += le32_to_cpu(region->len);\n\t}\n\n\tdata += offset + 16;\n\trel_info = (struct mt7921_realease_info *)data;\n\tdata += sizeof(*rel_info);\n\tend = data + le16_to_cpu(rel_info->len);\n\n\twhile (data < end) {\n\t\trel_info = (struct mt7921_realease_info *)data;\n\t\tdata += sizeof(*rel_info);\n\n\t\tif (rel_info->tag == MT7921_FW_TAG_FEATURE) {\n\t\t\tstruct mt7921_fw_features *features;\n\n\t\t\tfeatures = (struct mt7921_fw_features *)data;\n\t\t\toffload_caps = features->data;\n\t\t\tbreak;\n\t\t}\n\n\t\tdata += le16_to_cpu(rel_info->len) + rel_info->pad_len;\n\t}\n\nout:\n\trelease_firmware(fw);\n\n\treturn offload_caps;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,11 @@\n u8 mt7921_check_offload_capability(struct device *dev, const char *fw_wm)\n {\n-\tstruct mt7921_fw_features *features = NULL;\n \tconst struct mt76_connac2_fw_trailer *hdr;\n \tstruct mt7921_realease_info *rel_info;\n \tconst struct firmware *fw;\n \tint ret, i, offset = 0;\n \tconst u8 *data, *end;\n+\tu8 offload_caps = 0;\n \n \tret = request_firmware(&fw, fw_wm, dev);\n \tif (ret)\n@@ -37,7 +37,10 @@\n \t\tdata += sizeof(*rel_info);\n \n \t\tif (rel_info->tag == MT7921_FW_TAG_FEATURE) {\n+\t\t\tstruct mt7921_fw_features *features;\n+\n \t\t\tfeatures = (struct mt7921_fw_features *)data;\n+\t\t\toffload_caps = features->data;\n \t\t\tbreak;\n \t\t}\n \n@@ -47,5 +50,5 @@\n out:\n \trelease_firmware(fw);\n \n-\treturn features ? features->data : 0;\n+\treturn offload_caps;\n }",
        "function_modified_lines": {
            "added": [
                "\tu8 offload_caps = 0;",
                "\t\t\tstruct mt7921_fw_features *features;",
                "",
                "\t\t\toffload_caps = features->data;",
                "\treturn offload_caps;"
            ],
            "deleted": [
                "\tstruct mt7921_fw_features *features = NULL;",
                "\treturn features ? features->data : 0;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in mt7921_check_offload_capability in drivers/net/wireless/mediatek/mt76/mt7921/init.c in wifi mt76/mt7921 sub-component in the Linux Kernel. This flaw could allow an attacker to crash the system after 'features' memory release. This vulnerability could even lead to a kernel information leak problem.",
        "id": 4056
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static inline bool blk_trace_note_message_enabled(struct request_queue *q)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\tif (likely(!bt))\n\t\treturn false;\n\treturn bt->act_mask & BLK_TC_NOTIFY;\n}",
        "code_after_change": "static inline bool blk_trace_note_message_enabled(struct request_queue *q)\n{\n\tstruct blk_trace *bt;\n\tbool ret;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tret = bt && (bt->act_mask & BLK_TC_NOTIFY);\n\trcu_read_unlock();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,11 @@\n static inline bool blk_trace_note_message_enabled(struct request_queue *q)\n {\n-\tstruct blk_trace *bt = q->blk_trace;\n-\tif (likely(!bt))\n-\t\treturn false;\n-\treturn bt->act_mask & BLK_TC_NOTIFY;\n+\tstruct blk_trace *bt;\n+\tbool ret;\n+\n+\trcu_read_lock();\n+\tbt = rcu_dereference(q->blk_trace);\n+\tret = bt && (bt->act_mask & BLK_TC_NOTIFY);\n+\trcu_read_unlock();\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct blk_trace *bt;",
                "\tbool ret;",
                "",
                "\trcu_read_lock();",
                "\tbt = rcu_dereference(q->blk_trace);",
                "\tret = bt && (bt->act_mask & BLK_TC_NOTIFY);",
                "\trcu_read_unlock();",
                "\treturn ret;"
            ],
            "deleted": [
                "\tstruct blk_trace *bt = q->blk_trace;",
                "\tif (likely(!bt))",
                "\t\treturn false;",
                "\treturn bt->act_mask & BLK_TC_NOTIFY;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2225
    },
    {
        "cve_id": "CVE-2016-9120",
        "code_before_change": "void ion_free(struct ion_client *client, struct ion_handle *handle)\n{\n\tbool valid_handle;\n\n\tBUG_ON(client != handle->client);\n\n\tmutex_lock(&client->lock);\n\tvalid_handle = ion_handle_validate(client, handle);\n\n\tif (!valid_handle) {\n\t\tWARN(1, \"%s: invalid handle passed to free.\\n\", __func__);\n\t\tmutex_unlock(&client->lock);\n\t\treturn;\n\t}\n\tmutex_unlock(&client->lock);\n\tion_handle_put(handle);\n}",
        "code_after_change": "void ion_free(struct ion_client *client, struct ion_handle *handle)\n{\n\tBUG_ON(client != handle->client);\n\n\tmutex_lock(&client->lock);\n\tion_free_nolock(client, handle);\n\tmutex_unlock(&client->lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,17 +1,8 @@\n void ion_free(struct ion_client *client, struct ion_handle *handle)\n {\n-\tbool valid_handle;\n-\n \tBUG_ON(client != handle->client);\n \n \tmutex_lock(&client->lock);\n-\tvalid_handle = ion_handle_validate(client, handle);\n-\n-\tif (!valid_handle) {\n-\t\tWARN(1, \"%s: invalid handle passed to free.\\n\", __func__);\n-\t\tmutex_unlock(&client->lock);\n-\t\treturn;\n-\t}\n+\tion_free_nolock(client, handle);\n \tmutex_unlock(&client->lock);\n-\tion_handle_put(handle);\n }",
        "function_modified_lines": {
            "added": [
                "\tion_free_nolock(client, handle);"
            ],
            "deleted": [
                "\tbool valid_handle;",
                "",
                "\tvalid_handle = ion_handle_validate(client, handle);",
                "",
                "\tif (!valid_handle) {",
                "\t\tWARN(1, \"%s: invalid handle passed to free.\\n\", __func__);",
                "\t\tmutex_unlock(&client->lock);",
                "\t\treturn;",
                "\t}",
                "\tion_handle_put(handle);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time.",
        "id": 1139
    },
    {
        "cve_id": "CVE-2020-36387",
        "code_before_change": "static void io_async_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct async_poll *apoll = req->apoll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\ttrace_io_uring_task_run(req->ctx, req->opcode, req->user_data);\n\n\tif (io_poll_rewait(req, &apoll->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\treturn;\n\t}\n\n\t/* If req is still hashed, it cannot have been canceled. Don't check. */\n\tif (hash_hashed(&req->hash_node))\n\t\thash_del(&req->hash_node);\n\n\tio_poll_remove_double(req, apoll->double_poll);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (!READ_ONCE(apoll->poll.canceled))\n\t\t__io_req_task_submit(req);\n\telse\n\t\t__io_req_task_cancel(req, -ECANCELED);\n\n\tkfree(apoll->double_poll);\n\tkfree(apoll);\n}",
        "code_after_change": "static void io_async_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct async_poll *apoll = req->apoll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\ttrace_io_uring_task_run(req->ctx, req->opcode, req->user_data);\n\n\tif (io_poll_rewait(req, &apoll->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tpercpu_ref_put(&ctx->refs);\n\t\treturn;\n\t}\n\n\t/* If req is still hashed, it cannot have been canceled. Don't check. */\n\tif (hash_hashed(&req->hash_node))\n\t\thash_del(&req->hash_node);\n\n\tio_poll_remove_double(req, apoll->double_poll);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (!READ_ONCE(apoll->poll.canceled))\n\t\t__io_req_task_submit(req);\n\telse\n\t\t__io_req_task_cancel(req, -ECANCELED);\n\n\tpercpu_ref_put(&ctx->refs);\n\tkfree(apoll->double_poll);\n\tkfree(apoll);\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,7 @@\n \n \tif (io_poll_rewait(req, &apoll->poll)) {\n \t\tspin_unlock_irq(&ctx->completion_lock);\n+\t\tpercpu_ref_put(&ctx->refs);\n \t\treturn;\n \t}\n \n@@ -23,6 +24,7 @@\n \telse\n \t\t__io_req_task_cancel(req, -ECANCELED);\n \n+\tpercpu_ref_put(&ctx->refs);\n \tkfree(apoll->double_poll);\n \tkfree(apoll);\n }",
        "function_modified_lines": {
            "added": [
                "\t\tpercpu_ref_put(&ctx->refs);",
                "\tpercpu_ref_put(&ctx->refs);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.8.2. fs/io_uring.c has a use-after-free related to io_async_task_func and ctx reference holding, aka CID-6d816e088c35.",
        "id": 2756
    },
    {
        "cve_id": "CVE-2020-10690",
        "code_before_change": "static int posix_clock_release(struct inode *inode, struct file *fp)\n{\n\tstruct posix_clock *clk = fp->private_data;\n\tint err = 0;\n\n\tif (clk->ops.release)\n\t\terr = clk->ops.release(clk);\n\n\tkref_put(&clk->kref, delete_clock);\n\n\tfp->private_data = NULL;\n\n\treturn err;\n}",
        "code_after_change": "static int posix_clock_release(struct inode *inode, struct file *fp)\n{\n\tstruct posix_clock *clk = fp->private_data;\n\tint err = 0;\n\n\tif (clk->ops.release)\n\t\terr = clk->ops.release(clk);\n\n\tput_device(clk->dev);\n\n\tfp->private_data = NULL;\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tif (clk->ops.release)\n \t\terr = clk->ops.release(clk);\n \n-\tkref_put(&clk->kref, delete_clock);\n+\tput_device(clk->dev);\n \n \tfp->private_data = NULL;\n ",
        "function_modified_lines": {
            "added": [
                "\tput_device(clk->dev);"
            ],
            "deleted": [
                "\tkref_put(&clk->kref, delete_clock);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a use-after-free in kernel versions before 5.5 due to a race condition between the release of ptp_clock and cdev while resource deallocation. When a (high privileged) process allocates a ptp device file (like /dev/ptpX) and voluntarily goes to sleep. During this time if the underlying device is removed, it can cause an exploitable condition as the process wakes up to terminate and clean all attached files. The system crashes due to the cdev structure being invalid (as already freed) which is pointed to by the inode.",
        "id": 2400
    },
    {
        "cve_id": "CVE-2023-5197",
        "code_before_change": "static int nf_tables_newrule(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t     const struct nlattr * const nla[])\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(info->net);\n\tstruct netlink_ext_ack *extack = info->extack;\n\tunsigned int size, i, n, ulen = 0, usize = 0;\n\tu8 genmask = nft_genmask_next(info->net);\n\tstruct nft_rule *rule, *old_rule = NULL;\n\tstruct nft_expr_info *expr_info = NULL;\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct nft_flow_rule *flow = NULL;\n\tstruct net *net = info->net;\n\tstruct nft_userdata *udata;\n\tstruct nft_table *table;\n\tstruct nft_chain *chain;\n\tstruct nft_trans *trans;\n\tu64 handle, pos_handle;\n\tstruct nft_expr *expr;\n\tstruct nft_ctx ctx;\n\tstruct nlattr *tmp;\n\tint err, rem;\n\n\tlockdep_assert_held(&nft_net->commit_mutex);\n\n\ttable = nft_table_lookup(net, nla[NFTA_RULE_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_RULE_CHAIN]) {\n\t\tchain = nft_chain_lookup(net, table, nla[NFTA_RULE_CHAIN],\n\t\t\t\t\t genmask);\n\t\tif (IS_ERR(chain)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\n\t} else if (nla[NFTA_RULE_CHAIN_ID]) {\n\t\tchain = nft_chain_lookup_byid(net, table, nla[NFTA_RULE_CHAIN_ID],\n\t\t\t\t\t      genmask);\n\t\tif (IS_ERR(chain)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN_ID]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\tif (nft_chain_is_bound(chain))\n\t\treturn -EOPNOTSUPP;\n\n\tif (nla[NFTA_RULE_HANDLE]) {\n\t\thandle = be64_to_cpu(nla_get_be64(nla[NFTA_RULE_HANDLE]));\n\t\trule = __nft_rule_lookup(chain, handle);\n\t\tif (IS_ERR(rule)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\treturn PTR_ERR(rule);\n\t\t}\n\n\t\tif (info->nlh->nlmsg_flags & NLM_F_EXCL) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\treturn -EEXIST;\n\t\t}\n\t\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\told_rule = rule;\n\t\telse\n\t\t\treturn -EOPNOTSUPP;\n\t} else {\n\t\tif (!(info->nlh->nlmsg_flags & NLM_F_CREATE) ||\n\t\t    info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\treturn -EINVAL;\n\t\thandle = nf_tables_alloc_handle(table);\n\n\t\tif (nla[NFTA_RULE_POSITION]) {\n\t\t\tpos_handle = be64_to_cpu(nla_get_be64(nla[NFTA_RULE_POSITION]));\n\t\t\told_rule = __nft_rule_lookup(chain, pos_handle);\n\t\t\tif (IS_ERR(old_rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_POSITION]);\n\t\t\t\treturn PTR_ERR(old_rule);\n\t\t\t}\n\t\t} else if (nla[NFTA_RULE_POSITION_ID]) {\n\t\t\told_rule = nft_rule_lookup_byid(net, chain, nla[NFTA_RULE_POSITION_ID]);\n\t\t\tif (IS_ERR(old_rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_POSITION_ID]);\n\t\t\t\treturn PTR_ERR(old_rule);\n\t\t\t}\n\t\t}\n\t}\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tn = 0;\n\tsize = 0;\n\tif (nla[NFTA_RULE_EXPRESSIONS]) {\n\t\texpr_info = kvmalloc_array(NFT_RULE_MAXEXPRS,\n\t\t\t\t\t   sizeof(struct nft_expr_info),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!expr_info)\n\t\t\treturn -ENOMEM;\n\n\t\tnla_for_each_nested(tmp, nla[NFTA_RULE_EXPRESSIONS], rem) {\n\t\t\terr = -EINVAL;\n\t\t\tif (nla_type(tmp) != NFTA_LIST_ELEM)\n\t\t\t\tgoto err_release_expr;\n\t\t\tif (n == NFT_RULE_MAXEXPRS)\n\t\t\t\tgoto err_release_expr;\n\t\t\terr = nf_tables_expr_parse(&ctx, tmp, &expr_info[n]);\n\t\t\tif (err < 0) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, tmp);\n\t\t\t\tgoto err_release_expr;\n\t\t\t}\n\t\t\tsize += expr_info[n].ops->size;\n\t\t\tn++;\n\t\t}\n\t}\n\t/* Check for overflow of dlen field */\n\terr = -EFBIG;\n\tif (size >= 1 << 12)\n\t\tgoto err_release_expr;\n\n\tif (nla[NFTA_RULE_USERDATA]) {\n\t\tulen = nla_len(nla[NFTA_RULE_USERDATA]);\n\t\tif (ulen > 0)\n\t\t\tusize = sizeof(struct nft_userdata) + ulen;\n\t}\n\n\terr = -ENOMEM;\n\trule = kzalloc(sizeof(*rule) + size + usize, GFP_KERNEL_ACCOUNT);\n\tif (rule == NULL)\n\t\tgoto err_release_expr;\n\n\tnft_activate_next(net, rule);\n\n\trule->handle = handle;\n\trule->dlen   = size;\n\trule->udata  = ulen ? 1 : 0;\n\n\tif (ulen) {\n\t\tudata = nft_userdata(rule);\n\t\tudata->len = ulen - 1;\n\t\tnla_memcpy(udata->data, nla[NFTA_RULE_USERDATA], ulen);\n\t}\n\n\texpr = nft_expr_first(rule);\n\tfor (i = 0; i < n; i++) {\n\t\terr = nf_tables_newexpr(&ctx, &expr_info[i], expr);\n\t\tif (err < 0) {\n\t\t\tNL_SET_BAD_ATTR(extack, expr_info[i].attr);\n\t\t\tgoto err_release_rule;\n\t\t}\n\n\t\tif (expr_info[i].ops->validate)\n\t\t\tnft_validate_state_update(table, NFT_VALIDATE_NEED);\n\n\t\texpr_info[i].ops = NULL;\n\t\texpr = nft_expr_next(expr);\n\t}\n\n\tif (chain->flags & NFT_CHAIN_HW_OFFLOAD) {\n\t\tflow = nft_flow_rule_create(net, rule);\n\t\tif (IS_ERR(flow)) {\n\t\t\terr = PTR_ERR(flow);\n\t\t\tgoto err_release_rule;\n\t\t}\n\t}\n\n\tif (!nft_use_inc(&chain->use)) {\n\t\terr = -EMFILE;\n\t\tgoto err_release_rule;\n\t}\n\n\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE) {\n\t\terr = nft_delrule(&ctx, old_rule);\n\t\tif (err < 0)\n\t\t\tgoto err_destroy_flow_rule;\n\n\t\ttrans = nft_trans_rule_add(&ctx, NFT_MSG_NEWRULE, rule);\n\t\tif (trans == NULL) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_destroy_flow_rule;\n\t\t}\n\t\tlist_add_tail_rcu(&rule->list, &old_rule->list);\n\t} else {\n\t\ttrans = nft_trans_rule_add(&ctx, NFT_MSG_NEWRULE, rule);\n\t\tif (!trans) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_destroy_flow_rule;\n\t\t}\n\n\t\tif (info->nlh->nlmsg_flags & NLM_F_APPEND) {\n\t\t\tif (old_rule)\n\t\t\t\tlist_add_rcu(&rule->list, &old_rule->list);\n\t\t\telse\n\t\t\t\tlist_add_tail_rcu(&rule->list, &chain->rules);\n\t\t } else {\n\t\t\tif (old_rule)\n\t\t\t\tlist_add_tail_rcu(&rule->list, &old_rule->list);\n\t\t\telse\n\t\t\t\tlist_add_rcu(&rule->list, &chain->rules);\n\t\t}\n\t}\n\tkvfree(expr_info);\n\n\tif (flow)\n\t\tnft_trans_flow_rule(trans) = flow;\n\n\tif (table->validate_state == NFT_VALIDATE_DO)\n\t\treturn nft_table_validate(net, table);\n\n\treturn 0;\n\nerr_destroy_flow_rule:\n\tnft_use_dec_restore(&chain->use);\n\tif (flow)\n\t\tnft_flow_rule_destroy(flow);\nerr_release_rule:\n\tnft_rule_expr_deactivate(&ctx, rule, NFT_TRANS_PREPARE_ERROR);\n\tnf_tables_rule_destroy(&ctx, rule);\nerr_release_expr:\n\tfor (i = 0; i < n; i++) {\n\t\tif (expr_info[i].ops) {\n\t\t\tmodule_put(expr_info[i].ops->type->owner);\n\t\t\tif (expr_info[i].ops->type->release_ops)\n\t\t\t\texpr_info[i].ops->type->release_ops(expr_info[i].ops);\n\t\t}\n\t}\n\tkvfree(expr_info);\n\n\treturn err;\n}",
        "code_after_change": "static int nf_tables_newrule(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t     const struct nlattr * const nla[])\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(info->net);\n\tstruct netlink_ext_ack *extack = info->extack;\n\tunsigned int size, i, n, ulen = 0, usize = 0;\n\tu8 genmask = nft_genmask_next(info->net);\n\tstruct nft_rule *rule, *old_rule = NULL;\n\tstruct nft_expr_info *expr_info = NULL;\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct nft_flow_rule *flow = NULL;\n\tstruct net *net = info->net;\n\tstruct nft_userdata *udata;\n\tstruct nft_table *table;\n\tstruct nft_chain *chain;\n\tstruct nft_trans *trans;\n\tu64 handle, pos_handle;\n\tstruct nft_expr *expr;\n\tstruct nft_ctx ctx;\n\tstruct nlattr *tmp;\n\tint err, rem;\n\n\tlockdep_assert_held(&nft_net->commit_mutex);\n\n\ttable = nft_table_lookup(net, nla[NFTA_RULE_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_RULE_CHAIN]) {\n\t\tchain = nft_chain_lookup(net, table, nla[NFTA_RULE_CHAIN],\n\t\t\t\t\t genmask);\n\t\tif (IS_ERR(chain)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\n\t} else if (nla[NFTA_RULE_CHAIN_ID]) {\n\t\tchain = nft_chain_lookup_byid(net, table, nla[NFTA_RULE_CHAIN_ID],\n\t\t\t\t\t      genmask);\n\t\tif (IS_ERR(chain)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN_ID]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\tif (nft_chain_is_bound(chain))\n\t\treturn -EOPNOTSUPP;\n\n\tif (nla[NFTA_RULE_HANDLE]) {\n\t\thandle = be64_to_cpu(nla_get_be64(nla[NFTA_RULE_HANDLE]));\n\t\trule = __nft_rule_lookup(chain, handle);\n\t\tif (IS_ERR(rule)) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\treturn PTR_ERR(rule);\n\t\t}\n\n\t\tif (info->nlh->nlmsg_flags & NLM_F_EXCL) {\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\treturn -EEXIST;\n\t\t}\n\t\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\told_rule = rule;\n\t\telse\n\t\t\treturn -EOPNOTSUPP;\n\t} else {\n\t\tif (!(info->nlh->nlmsg_flags & NLM_F_CREATE) ||\n\t\t    info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\treturn -EINVAL;\n\t\thandle = nf_tables_alloc_handle(table);\n\n\t\tif (nla[NFTA_RULE_POSITION]) {\n\t\t\tpos_handle = be64_to_cpu(nla_get_be64(nla[NFTA_RULE_POSITION]));\n\t\t\told_rule = __nft_rule_lookup(chain, pos_handle);\n\t\t\tif (IS_ERR(old_rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_POSITION]);\n\t\t\t\treturn PTR_ERR(old_rule);\n\t\t\t}\n\t\t} else if (nla[NFTA_RULE_POSITION_ID]) {\n\t\t\told_rule = nft_rule_lookup_byid(net, chain, nla[NFTA_RULE_POSITION_ID]);\n\t\t\tif (IS_ERR(old_rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_POSITION_ID]);\n\t\t\t\treturn PTR_ERR(old_rule);\n\t\t\t}\n\t\t}\n\t}\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tn = 0;\n\tsize = 0;\n\tif (nla[NFTA_RULE_EXPRESSIONS]) {\n\t\texpr_info = kvmalloc_array(NFT_RULE_MAXEXPRS,\n\t\t\t\t\t   sizeof(struct nft_expr_info),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!expr_info)\n\t\t\treturn -ENOMEM;\n\n\t\tnla_for_each_nested(tmp, nla[NFTA_RULE_EXPRESSIONS], rem) {\n\t\t\terr = -EINVAL;\n\t\t\tif (nla_type(tmp) != NFTA_LIST_ELEM)\n\t\t\t\tgoto err_release_expr;\n\t\t\tif (n == NFT_RULE_MAXEXPRS)\n\t\t\t\tgoto err_release_expr;\n\t\t\terr = nf_tables_expr_parse(&ctx, tmp, &expr_info[n]);\n\t\t\tif (err < 0) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, tmp);\n\t\t\t\tgoto err_release_expr;\n\t\t\t}\n\t\t\tsize += expr_info[n].ops->size;\n\t\t\tn++;\n\t\t}\n\t}\n\t/* Check for overflow of dlen field */\n\terr = -EFBIG;\n\tif (size >= 1 << 12)\n\t\tgoto err_release_expr;\n\n\tif (nla[NFTA_RULE_USERDATA]) {\n\t\tulen = nla_len(nla[NFTA_RULE_USERDATA]);\n\t\tif (ulen > 0)\n\t\t\tusize = sizeof(struct nft_userdata) + ulen;\n\t}\n\n\terr = -ENOMEM;\n\trule = kzalloc(sizeof(*rule) + size + usize, GFP_KERNEL_ACCOUNT);\n\tif (rule == NULL)\n\t\tgoto err_release_expr;\n\n\tnft_activate_next(net, rule);\n\n\trule->handle = handle;\n\trule->dlen   = size;\n\trule->udata  = ulen ? 1 : 0;\n\n\tif (ulen) {\n\t\tudata = nft_userdata(rule);\n\t\tudata->len = ulen - 1;\n\t\tnla_memcpy(udata->data, nla[NFTA_RULE_USERDATA], ulen);\n\t}\n\n\texpr = nft_expr_first(rule);\n\tfor (i = 0; i < n; i++) {\n\t\terr = nf_tables_newexpr(&ctx, &expr_info[i], expr);\n\t\tif (err < 0) {\n\t\t\tNL_SET_BAD_ATTR(extack, expr_info[i].attr);\n\t\t\tgoto err_release_rule;\n\t\t}\n\n\t\tif (expr_info[i].ops->validate)\n\t\t\tnft_validate_state_update(table, NFT_VALIDATE_NEED);\n\n\t\texpr_info[i].ops = NULL;\n\t\texpr = nft_expr_next(expr);\n\t}\n\n\tif (chain->flags & NFT_CHAIN_HW_OFFLOAD) {\n\t\tflow = nft_flow_rule_create(net, rule);\n\t\tif (IS_ERR(flow)) {\n\t\t\terr = PTR_ERR(flow);\n\t\t\tgoto err_release_rule;\n\t\t}\n\t}\n\n\tif (!nft_use_inc(&chain->use)) {\n\t\terr = -EMFILE;\n\t\tgoto err_release_rule;\n\t}\n\n\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE) {\n\t\tif (nft_chain_binding(chain)) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_destroy_flow_rule;\n\t\t}\n\n\t\terr = nft_delrule(&ctx, old_rule);\n\t\tif (err < 0)\n\t\t\tgoto err_destroy_flow_rule;\n\n\t\ttrans = nft_trans_rule_add(&ctx, NFT_MSG_NEWRULE, rule);\n\t\tif (trans == NULL) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_destroy_flow_rule;\n\t\t}\n\t\tlist_add_tail_rcu(&rule->list, &old_rule->list);\n\t} else {\n\t\ttrans = nft_trans_rule_add(&ctx, NFT_MSG_NEWRULE, rule);\n\t\tif (!trans) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_destroy_flow_rule;\n\t\t}\n\n\t\tif (info->nlh->nlmsg_flags & NLM_F_APPEND) {\n\t\t\tif (old_rule)\n\t\t\t\tlist_add_rcu(&rule->list, &old_rule->list);\n\t\t\telse\n\t\t\t\tlist_add_tail_rcu(&rule->list, &chain->rules);\n\t\t } else {\n\t\t\tif (old_rule)\n\t\t\t\tlist_add_tail_rcu(&rule->list, &old_rule->list);\n\t\t\telse\n\t\t\t\tlist_add_rcu(&rule->list, &chain->rules);\n\t\t}\n\t}\n\tkvfree(expr_info);\n\n\tif (flow)\n\t\tnft_trans_flow_rule(trans) = flow;\n\n\tif (table->validate_state == NFT_VALIDATE_DO)\n\t\treturn nft_table_validate(net, table);\n\n\treturn 0;\n\nerr_destroy_flow_rule:\n\tnft_use_dec_restore(&chain->use);\n\tif (flow)\n\t\tnft_flow_rule_destroy(flow);\nerr_release_rule:\n\tnft_rule_expr_deactivate(&ctx, rule, NFT_TRANS_PREPARE_ERROR);\n\tnf_tables_rule_destroy(&ctx, rule);\nerr_release_expr:\n\tfor (i = 0; i < n; i++) {\n\t\tif (expr_info[i].ops) {\n\t\t\tmodule_put(expr_info[i].ops->type->owner);\n\t\t\tif (expr_info[i].ops->type->release_ops)\n\t\t\t\texpr_info[i].ops->type->release_ops(expr_info[i].ops);\n\t\t}\n\t}\n\tkvfree(expr_info);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -172,6 +172,11 @@\n \t}\n \n \tif (info->nlh->nlmsg_flags & NLM_F_REPLACE) {\n+\t\tif (nft_chain_binding(chain)) {\n+\t\t\terr = -EOPNOTSUPP;\n+\t\t\tgoto err_destroy_flow_rule;\n+\t\t}\n+\n \t\terr = nft_delrule(&ctx, old_rule);\n \t\tif (err < 0)\n \t\t\tgoto err_destroy_flow_rule;",
        "function_modified_lines": {
            "added": [
                "\t\tif (nft_chain_binding(chain)) {",
                "\t\t\terr = -EOPNOTSUPP;",
                "\t\t\tgoto err_destroy_flow_rule;",
                "\t\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nAddition and removal of rules from chain bindings within the same transaction causes leads to use-after-free.\n\nWe recommend upgrading past commit f15f29fd4779be8a418b66e9d52979bb6d6c2325.\n\n",
        "id": 4261
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "struct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "struct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\tatomic_set(&opt2->refcnt, 1);\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,7 +31,7 @@\n \t\treturn ERR_PTR(-ENOBUFS);\n \n \tmemset(opt2, 0, tot_len);\n-\n+\tatomic_set(&opt2->refcnt, 1);\n \topt2->tot_len = tot_len;\n \tp = (char *)(opt2 + 1);\n ",
        "function_modified_lines": {
            "added": [
                "\tatomic_set(&opt2->refcnt, 1);"
            ],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 996
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "static int set_rcvarray_entry(struct hfi1_filedata *fd,\n\t\t\t      struct tid_user_buf *tbuf,\n\t\t\t      u32 rcventry, struct tid_group *grp,\n\t\t\t      u16 pageidx, unsigned int npages)\n{\n\tint ret;\n\tstruct hfi1_ctxtdata *uctxt = fd->uctxt;\n\tstruct tid_rb_node *node;\n\tstruct hfi1_devdata *dd = uctxt->dd;\n\tdma_addr_t phys;\n\tstruct page **pages = tbuf->pages + pageidx;\n\n\t/*\n\t * Allocate the node first so we can handle a potential\n\t * failure before we've programmed anything.\n\t */\n\tnode = kzalloc(sizeof(*node) + (sizeof(struct page *) * npages),\n\t\t       GFP_KERNEL);\n\tif (!node)\n\t\treturn -ENOMEM;\n\n\tphys = pci_map_single(dd->pcidev,\n\t\t\t      __va(page_to_phys(pages[0])),\n\t\t\t      npages * PAGE_SIZE, PCI_DMA_FROMDEVICE);\n\tif (dma_mapping_error(&dd->pcidev->dev, phys)) {\n\t\tdd_dev_err(dd, \"Failed to DMA map Exp Rcv pages 0x%llx\\n\",\n\t\t\t   phys);\n\t\tkfree(node);\n\t\treturn -EFAULT;\n\t}\n\n\tnode->fdata = fd;\n\tnode->phys = page_to_phys(pages[0]);\n\tnode->npages = npages;\n\tnode->rcventry = rcventry;\n\tnode->dma_addr = phys;\n\tnode->grp = grp;\n\tnode->freed = false;\n\tmemcpy(node->pages, pages, sizeof(struct page *) * npages);\n\n\tif (fd->use_mn) {\n\t\tret = mmu_interval_notifier_insert(\n\t\t\t&node->notifier, fd->mm,\n\t\t\ttbuf->vaddr + (pageidx * PAGE_SIZE), npages * PAGE_SIZE,\n\t\t\t&tid_mn_ops);\n\t\tif (ret)\n\t\t\tgoto out_unmap;\n\t\t/*\n\t\t * FIXME: This is in the wrong order, the notifier should be\n\t\t * established before the pages are pinned by pin_rcv_pages.\n\t\t */\n\t\tmmu_interval_read_begin(&node->notifier);\n\t}\n\tfd->entry_to_rb[node->rcventry - uctxt->expected_base] = node;\n\n\thfi1_put_tid(dd, rcventry, PT_EXPECTED, phys, ilog2(npages) + 1);\n\ttrace_hfi1_exp_tid_reg(uctxt->ctxt, fd->subctxt, rcventry, npages,\n\t\t\t       node->notifier.interval_tree.start, node->phys,\n\t\t\t       phys);\n\treturn 0;\n\nout_unmap:\n\thfi1_cdbg(TID, \"Failed to insert RB node %u 0x%lx, 0x%lx %d\",\n\t\t  node->rcventry, node->notifier.interval_tree.start,\n\t\t  node->phys, ret);\n\tpci_unmap_single(dd->pcidev, phys, npages * PAGE_SIZE,\n\t\t\t PCI_DMA_FROMDEVICE);\n\tkfree(node);\n\treturn -EFAULT;\n}",
        "code_after_change": "static int set_rcvarray_entry(struct hfi1_filedata *fd,\n\t\t\t      struct tid_user_buf *tbuf,\n\t\t\t      u32 rcventry, struct tid_group *grp,\n\t\t\t      u16 pageidx, unsigned int npages)\n{\n\tint ret;\n\tstruct hfi1_ctxtdata *uctxt = fd->uctxt;\n\tstruct tid_rb_node *node;\n\tstruct hfi1_devdata *dd = uctxt->dd;\n\tdma_addr_t phys;\n\tstruct page **pages = tbuf->pages + pageidx;\n\n\t/*\n\t * Allocate the node first so we can handle a potential\n\t * failure before we've programmed anything.\n\t */\n\tnode = kzalloc(sizeof(*node) + (sizeof(struct page *) * npages),\n\t\t       GFP_KERNEL);\n\tif (!node)\n\t\treturn -ENOMEM;\n\n\tphys = pci_map_single(dd->pcidev,\n\t\t\t      __va(page_to_phys(pages[0])),\n\t\t\t      npages * PAGE_SIZE, PCI_DMA_FROMDEVICE);\n\tif (dma_mapping_error(&dd->pcidev->dev, phys)) {\n\t\tdd_dev_err(dd, \"Failed to DMA map Exp Rcv pages 0x%llx\\n\",\n\t\t\t   phys);\n\t\tkfree(node);\n\t\treturn -EFAULT;\n\t}\n\n\tnode->fdata = fd;\n\tnode->phys = page_to_phys(pages[0]);\n\tnode->npages = npages;\n\tnode->rcventry = rcventry;\n\tnode->dma_addr = phys;\n\tnode->grp = grp;\n\tnode->freed = false;\n\tmemcpy(node->pages, pages, sizeof(struct page *) * npages);\n\n\tif (fd->use_mn) {\n\t\tret = mmu_interval_notifier_insert(\n\t\t\t&node->notifier, current->mm,\n\t\t\ttbuf->vaddr + (pageidx * PAGE_SIZE), npages * PAGE_SIZE,\n\t\t\t&tid_mn_ops);\n\t\tif (ret)\n\t\t\tgoto out_unmap;\n\t\t/*\n\t\t * FIXME: This is in the wrong order, the notifier should be\n\t\t * established before the pages are pinned by pin_rcv_pages.\n\t\t */\n\t\tmmu_interval_read_begin(&node->notifier);\n\t}\n\tfd->entry_to_rb[node->rcventry - uctxt->expected_base] = node;\n\n\thfi1_put_tid(dd, rcventry, PT_EXPECTED, phys, ilog2(npages) + 1);\n\ttrace_hfi1_exp_tid_reg(uctxt->ctxt, fd->subctxt, rcventry, npages,\n\t\t\t       node->notifier.interval_tree.start, node->phys,\n\t\t\t       phys);\n\treturn 0;\n\nout_unmap:\n\thfi1_cdbg(TID, \"Failed to insert RB node %u 0x%lx, 0x%lx %d\",\n\t\t  node->rcventry, node->notifier.interval_tree.start,\n\t\t  node->phys, ret);\n\tpci_unmap_single(dd->pcidev, phys, npages * PAGE_SIZE,\n\t\t\t PCI_DMA_FROMDEVICE);\n\tkfree(node);\n\treturn -EFAULT;\n}",
        "patch": "--- code before\n+++ code after\n@@ -40,7 +40,7 @@\n \n \tif (fd->use_mn) {\n \t\tret = mmu_interval_notifier_insert(\n-\t\t\t&node->notifier, fd->mm,\n+\t\t\t&node->notifier, current->mm,\n \t\t\ttbuf->vaddr + (pageidx * PAGE_SIZE), npages * PAGE_SIZE,\n \t\t\t&tid_mn_ops);\n \t\tif (ret)",
        "function_modified_lines": {
            "added": [
                "\t\t\t&node->notifier, current->mm,"
            ],
            "deleted": [
                "\t\t\t&node->notifier, fd->mm,"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2648
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "int udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}",
        "code_after_change": "int udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n \tstruct in6_addr *daddr, *final_p, final;\n \tstruct ipv6_txoptions *opt = NULL;\n+\tstruct ipv6_txoptions *opt_to_free = NULL;\n \tstruct ip6_flowlabel *flowlabel = NULL;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n@@ -160,8 +161,10 @@\n \t\t\topt = NULL;\n \t\tconnected = 0;\n \t}\n-\tif (!opt)\n-\t\topt = np->opt;\n+\tif (!opt) {\n+\t\topt = txopt_get(np);\n+\t\topt_to_free = opt;\n+\t}\n \tif (flowlabel)\n \t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n \topt = ipv6_fixup_options(&opt_space, opt);\n@@ -270,6 +273,7 @@\n out:\n \tdst_release(dst);\n \tfl6_sock_release(flowlabel);\n+\ttxopt_put(opt_to_free);\n \tif (!err)\n \t\treturn len;\n \t/*",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt_to_free = NULL;",
                "\tif (!opt) {",
                "\t\topt = txopt_get(np);",
                "\t\topt_to_free = opt;",
                "\t}",
                "\ttxopt_put(opt_to_free);"
            ],
            "deleted": [
                "\tif (!opt)",
                "\t\topt = np->opt;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1008
    },
    {
        "cve_id": "CVE-2022-2602",
        "code_before_change": "void unix_gc(void)\n{\n\tstruct unix_sock *u;\n\tstruct unix_sock *next;\n\tstruct sk_buff_head hitlist;\n\tstruct list_head cursor;\n\tLIST_HEAD(not_cycle_list);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* Avoid a recursive GC. */\n\tif (gc_in_progress)\n\t\tgoto out;\n\n\t/* Paired with READ_ONCE() in wait_for_unix_gc(). */\n\tWRITE_ONCE(gc_in_progress, true);\n\n\t/* First, select candidates for garbage collection.  Only\n\t * in-flight sockets are considered, and from those only ones\n\t * which don't have any external reference.\n\t *\n\t * Holding unix_gc_lock will protect these candidates from\n\t * being detached, and hence from gaining an external\n\t * reference.  Since there are no possible receivers, all\n\t * buffers currently on the candidates' queues stay there\n\t * during the garbage collection.\n\t *\n\t * We also know that no new candidate can be added onto the\n\t * receive queues.  Other, non candidate sockets _can_ be\n\t * added to queue, so we must make sure only to touch\n\t * candidates.\n\t */\n\tlist_for_each_entry_safe(u, next, &gc_inflight_list, link) {\n\t\tlong total_refs;\n\t\tlong inflight_refs;\n\n\t\ttotal_refs = file_count(u->sk.sk_socket->file);\n\t\tinflight_refs = atomic_long_read(&u->inflight);\n\n\t\tBUG_ON(inflight_refs < 1);\n\t\tBUG_ON(total_refs < inflight_refs);\n\t\tif (total_refs == inflight_refs) {\n\t\t\tlist_move_tail(&u->link, &gc_candidates);\n\t\t\t__set_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\t\t__set_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t}\n\t}\n\n\t/* Now remove all internal in-flight reference to children of\n\t * the candidates.\n\t */\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, dec_inflight, NULL);\n\n\t/* Restore the references for children of all candidates,\n\t * which have remaining references.  Do this recursively, so\n\t * only those remain, which form cyclic references.\n\t *\n\t * Use a \"cursor\" link, to make the list traversal safe, even\n\t * though elements might be moved about.\n\t */\n\tlist_add(&cursor, &gc_candidates);\n\twhile (cursor.next != &gc_candidates) {\n\t\tu = list_entry(cursor.next, struct unix_sock, link);\n\n\t\t/* Move cursor to after the current position. */\n\t\tlist_move(&cursor, &u->link);\n\n\t\tif (atomic_long_read(&u->inflight) > 0) {\n\t\t\tlist_move_tail(&u->link, &not_cycle_list);\n\t\t\t__clear_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t\tscan_children(&u->sk, inc_inflight_move_tail, NULL);\n\t\t}\n\t}\n\tlist_del(&cursor);\n\n\t/* Now gc_candidates contains only garbage.  Restore original\n\t * inflight counters for these as well, and remove the skbuffs\n\t * which are creating the cycle(s).\n\t */\n\tskb_queue_head_init(&hitlist);\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, inc_inflight, &hitlist);\n\n\t/* not_cycle_list contains those sockets which do not make up a\n\t * cycle.  Restore these to the inflight list.\n\t */\n\twhile (!list_empty(&not_cycle_list)) {\n\t\tu = list_entry(not_cycle_list.next, struct unix_sock, link);\n\t\t__clear_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\tlist_move_tail(&u->link, &gc_inflight_list);\n\t}\n\n\tspin_unlock(&unix_gc_lock);\n\n\t/* Here we are. Hitlist is filled. Die. */\n\t__skb_queue_purge(&hitlist);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* All candidates should have been detached by now. */\n\tBUG_ON(!list_empty(&gc_candidates));\n\n\t/* Paired with READ_ONCE() in wait_for_unix_gc(). */\n\tWRITE_ONCE(gc_in_progress, false);\n\n\twake_up(&unix_gc_wait);\n\n out:\n\tspin_unlock(&unix_gc_lock);\n}",
        "code_after_change": "void unix_gc(void)\n{\n\tstruct sk_buff *next_skb, *skb;\n\tstruct unix_sock *u;\n\tstruct unix_sock *next;\n\tstruct sk_buff_head hitlist;\n\tstruct list_head cursor;\n\tLIST_HEAD(not_cycle_list);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* Avoid a recursive GC. */\n\tif (gc_in_progress)\n\t\tgoto out;\n\n\t/* Paired with READ_ONCE() in wait_for_unix_gc(). */\n\tWRITE_ONCE(gc_in_progress, true);\n\n\t/* First, select candidates for garbage collection.  Only\n\t * in-flight sockets are considered, and from those only ones\n\t * which don't have any external reference.\n\t *\n\t * Holding unix_gc_lock will protect these candidates from\n\t * being detached, and hence from gaining an external\n\t * reference.  Since there are no possible receivers, all\n\t * buffers currently on the candidates' queues stay there\n\t * during the garbage collection.\n\t *\n\t * We also know that no new candidate can be added onto the\n\t * receive queues.  Other, non candidate sockets _can_ be\n\t * added to queue, so we must make sure only to touch\n\t * candidates.\n\t */\n\tlist_for_each_entry_safe(u, next, &gc_inflight_list, link) {\n\t\tlong total_refs;\n\t\tlong inflight_refs;\n\n\t\ttotal_refs = file_count(u->sk.sk_socket->file);\n\t\tinflight_refs = atomic_long_read(&u->inflight);\n\n\t\tBUG_ON(inflight_refs < 1);\n\t\tBUG_ON(total_refs < inflight_refs);\n\t\tif (total_refs == inflight_refs) {\n\t\t\tlist_move_tail(&u->link, &gc_candidates);\n\t\t\t__set_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\t\t__set_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t}\n\t}\n\n\t/* Now remove all internal in-flight reference to children of\n\t * the candidates.\n\t */\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, dec_inflight, NULL);\n\n\t/* Restore the references for children of all candidates,\n\t * which have remaining references.  Do this recursively, so\n\t * only those remain, which form cyclic references.\n\t *\n\t * Use a \"cursor\" link, to make the list traversal safe, even\n\t * though elements might be moved about.\n\t */\n\tlist_add(&cursor, &gc_candidates);\n\twhile (cursor.next != &gc_candidates) {\n\t\tu = list_entry(cursor.next, struct unix_sock, link);\n\n\t\t/* Move cursor to after the current position. */\n\t\tlist_move(&cursor, &u->link);\n\n\t\tif (atomic_long_read(&u->inflight) > 0) {\n\t\t\tlist_move_tail(&u->link, &not_cycle_list);\n\t\t\t__clear_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t\tscan_children(&u->sk, inc_inflight_move_tail, NULL);\n\t\t}\n\t}\n\tlist_del(&cursor);\n\n\t/* Now gc_candidates contains only garbage.  Restore original\n\t * inflight counters for these as well, and remove the skbuffs\n\t * which are creating the cycle(s).\n\t */\n\tskb_queue_head_init(&hitlist);\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, inc_inflight, &hitlist);\n\n\t/* not_cycle_list contains those sockets which do not make up a\n\t * cycle.  Restore these to the inflight list.\n\t */\n\twhile (!list_empty(&not_cycle_list)) {\n\t\tu = list_entry(not_cycle_list.next, struct unix_sock, link);\n\t\t__clear_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\tlist_move_tail(&u->link, &gc_inflight_list);\n\t}\n\n\tspin_unlock(&unix_gc_lock);\n\n\t/* We need io_uring to clean its registered files, ignore all io_uring\n\t * originated skbs. It's fine as io_uring doesn't keep references to\n\t * other io_uring instances and so killing all other files in the cycle\n\t * will put all io_uring references forcing it to go through normal\n\t * release.path eventually putting registered files.\n\t */\n\tskb_queue_walk_safe(&hitlist, skb, next_skb) {\n\t\tif (skb->scm_io_uring) {\n\t\t\t__skb_unlink(skb, &hitlist);\n\t\t\tskb_queue_tail(&skb->sk->sk_receive_queue, skb);\n\t\t}\n\t}\n\n\t/* Here we are. Hitlist is filled. Die. */\n\t__skb_queue_purge(&hitlist);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* There could be io_uring registered files, just push them back to\n\t * the inflight list\n\t */\n\tlist_for_each_entry_safe(u, next, &gc_candidates, link)\n\t\tlist_move_tail(&u->link, &gc_inflight_list);\n\n\t/* All candidates should have been detached by now. */\n\tBUG_ON(!list_empty(&gc_candidates));\n\n\t/* Paired with READ_ONCE() in wait_for_unix_gc(). */\n\tWRITE_ONCE(gc_in_progress, false);\n\n\twake_up(&unix_gc_wait);\n\n out:\n\tspin_unlock(&unix_gc_lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,6 @@\n void unix_gc(void)\n {\n+\tstruct sk_buff *next_skb, *skb;\n \tstruct unix_sock *u;\n \tstruct unix_sock *next;\n \tstruct sk_buff_head hitlist;\n@@ -93,10 +94,29 @@\n \n \tspin_unlock(&unix_gc_lock);\n \n+\t/* We need io_uring to clean its registered files, ignore all io_uring\n+\t * originated skbs. It's fine as io_uring doesn't keep references to\n+\t * other io_uring instances and so killing all other files in the cycle\n+\t * will put all io_uring references forcing it to go through normal\n+\t * release.path eventually putting registered files.\n+\t */\n+\tskb_queue_walk_safe(&hitlist, skb, next_skb) {\n+\t\tif (skb->scm_io_uring) {\n+\t\t\t__skb_unlink(skb, &hitlist);\n+\t\t\tskb_queue_tail(&skb->sk->sk_receive_queue, skb);\n+\t\t}\n+\t}\n+\n \t/* Here we are. Hitlist is filled. Die. */\n \t__skb_queue_purge(&hitlist);\n \n \tspin_lock(&unix_gc_lock);\n+\n+\t/* There could be io_uring registered files, just push them back to\n+\t * the inflight list\n+\t */\n+\tlist_for_each_entry_safe(u, next, &gc_candidates, link)\n+\t\tlist_move_tail(&u->link, &gc_inflight_list);\n \n \t/* All candidates should have been detached by now. */\n \tBUG_ON(!list_empty(&gc_candidates));",
        "function_modified_lines": {
            "added": [
                "\tstruct sk_buff *next_skb, *skb;",
                "\t/* We need io_uring to clean its registered files, ignore all io_uring",
                "\t * originated skbs. It's fine as io_uring doesn't keep references to",
                "\t * other io_uring instances and so killing all other files in the cycle",
                "\t * will put all io_uring references forcing it to go through normal",
                "\t * release.path eventually putting registered files.",
                "\t */",
                "\tskb_queue_walk_safe(&hitlist, skb, next_skb) {",
                "\t\tif (skb->scm_io_uring) {",
                "\t\t\t__skb_unlink(skb, &hitlist);",
                "\t\t\tskb_queue_tail(&skb->sk->sk_receive_queue, skb);",
                "\t\t}",
                "\t}",
                "",
                "",
                "\t/* There could be io_uring registered files, just push them back to",
                "\t * the inflight list",
                "\t */",
                "\tlist_for_each_entry_safe(u, next, &gc_candidates, link)",
                "\t\tlist_move_tail(&u->link, &gc_inflight_list);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "io_uring UAF, Unix SCM garbage collection",
        "id": 3484
    },
    {
        "cve_id": "CVE-2022-41849",
        "code_before_change": "static int ufx_ops_open(struct fb_info *info, int user)\n{\n\tstruct ufx_data *dev = info->par;\n\n\t/* fbcon aggressively connects to first framebuffer it finds,\n\t * preventing other clients (X) from working properly. Usually\n\t * not what the user wants. Fail by default with option to enable. */\n\tif (user == 0 && !console)\n\t\treturn -EBUSY;\n\n\t/* If the USB device is gone, we don't accept new opens */\n\tif (dev->virtualized)\n\t\treturn -ENODEV;\n\n\tdev->fb_count++;\n\n\tkref_get(&dev->kref);\n\n\tif (fb_defio && (info->fbdefio == NULL)) {\n\t\t/* enable defio at last moment if not disabled by client */\n\n\t\tstruct fb_deferred_io *fbdefio;\n\n\t\tfbdefio = kzalloc(sizeof(*fbdefio), GFP_KERNEL);\n\t\tif (fbdefio) {\n\t\t\tfbdefio->delay = UFX_DEFIO_WRITE_DELAY;\n\t\t\tfbdefio->deferred_io = ufx_dpy_deferred_io;\n\t\t}\n\n\t\tinfo->fbdefio = fbdefio;\n\t\tfb_deferred_io_init(info);\n\t}\n\n\tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n\t\tinfo->node, user, info, dev->fb_count);\n\n\treturn 0;\n}",
        "code_after_change": "static int ufx_ops_open(struct fb_info *info, int user)\n{\n\tstruct ufx_data *dev = info->par;\n\n\t/* fbcon aggressively connects to first framebuffer it finds,\n\t * preventing other clients (X) from working properly. Usually\n\t * not what the user wants. Fail by default with option to enable. */\n\tif (user == 0 && !console)\n\t\treturn -EBUSY;\n\n\tmutex_lock(&disconnect_mutex);\n\n\t/* If the USB device is gone, we don't accept new opens */\n\tif (dev->virtualized) {\n\t\tmutex_unlock(&disconnect_mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tdev->fb_count++;\n\n\tkref_get(&dev->kref);\n\n\tif (fb_defio && (info->fbdefio == NULL)) {\n\t\t/* enable defio at last moment if not disabled by client */\n\n\t\tstruct fb_deferred_io *fbdefio;\n\n\t\tfbdefio = kzalloc(sizeof(*fbdefio), GFP_KERNEL);\n\t\tif (fbdefio) {\n\t\t\tfbdefio->delay = UFX_DEFIO_WRITE_DELAY;\n\t\t\tfbdefio->deferred_io = ufx_dpy_deferred_io;\n\t\t}\n\n\t\tinfo->fbdefio = fbdefio;\n\t\tfb_deferred_io_init(info);\n\t}\n\n\tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n\t\tinfo->node, user, info, dev->fb_count);\n\n\tmutex_unlock(&disconnect_mutex);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,9 +8,13 @@\n \tif (user == 0 && !console)\n \t\treturn -EBUSY;\n \n+\tmutex_lock(&disconnect_mutex);\n+\n \t/* If the USB device is gone, we don't accept new opens */\n-\tif (dev->virtualized)\n+\tif (dev->virtualized) {\n+\t\tmutex_unlock(&disconnect_mutex);\n \t\treturn -ENODEV;\n+\t}\n \n \tdev->fb_count++;\n \n@@ -34,5 +38,7 @@\n \tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n \t\tinfo->node, user, info, dev->fb_count);\n \n+\tmutex_unlock(&disconnect_mutex);\n+\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&disconnect_mutex);",
                "",
                "\tif (dev->virtualized) {",
                "\t\tmutex_unlock(&disconnect_mutex);",
                "\t}",
                "\tmutex_unlock(&disconnect_mutex);",
                ""
            ],
            "deleted": [
                "\tif (dev->virtualized)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "drivers/video/fbdev/smscufx.c in the Linux kernel through 5.19.12 has a race condition and resultant use-after-free if a physically proximate attacker removes a USB device while calling open(), aka a race condition between ufx_ops_open and ufx_usb_disconnect.",
        "id": 3721
    },
    {
        "cve_id": "CVE-2021-20292",
        "code_before_change": "int ttm_dma_tt_init(struct ttm_dma_tt *ttm_dma, struct ttm_buffer_object *bo,\n\t\t    uint32_t page_flags)\n{\n\tstruct ttm_tt *ttm = &ttm_dma->ttm;\n\n\tttm_tt_init_fields(ttm, bo, page_flags);\n\n\tINIT_LIST_HEAD(&ttm_dma->pages_list);\n\tif (ttm_dma_tt_alloc_page_directory(ttm_dma)) {\n\t\tttm_tt_destroy(ttm);\n\t\tpr_err(\"Failed allocating page table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}",
        "code_after_change": "int ttm_dma_tt_init(struct ttm_dma_tt *ttm_dma, struct ttm_buffer_object *bo,\n\t\t    uint32_t page_flags)\n{\n\tstruct ttm_tt *ttm = &ttm_dma->ttm;\n\n\tttm_tt_init_fields(ttm, bo, page_flags);\n\n\tINIT_LIST_HEAD(&ttm_dma->pages_list);\n\tif (ttm_dma_tt_alloc_page_directory(ttm_dma)) {\n\t\tpr_err(\"Failed allocating page table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,6 @@\n \n \tINIT_LIST_HEAD(&ttm_dma->pages_list);\n \tif (ttm_dma_tt_alloc_page_directory(ttm_dma)) {\n-\t\tttm_tt_destroy(ttm);\n \t\tpr_err(\"Failed allocating page table\\n\");\n \t\treturn -ENOMEM;\n \t}",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tttm_tt_destroy(ttm);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a flaw reported in the Linux kernel in versions before 5.9 in drivers/gpu/drm/nouveau/nouveau_sgdma.c in nouveau_sgdma_create_ttm in Nouveau DRM subsystem. The issue results from the lack of validating the existence of an object prior to performing operations on the object. An attacker with a local account with a root privilege, can leverage this vulnerability to escalate privileges and execute code in the context of the kernel.",
        "id": 2869
    },
    {
        "cve_id": "CVE-2022-1055",
        "code_before_change": "static int tc_new_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tchar name[IFNAMSIZ];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tbool prio_allocate;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp;\n\tunsigned long cl;\n\tvoid *fh;\n\tint err;\n\tint tp_created;\n\tbool rtnl_held = false;\n\tu32 flags;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\ttp_created = 0;\n\n\terr = nlmsg_parse_deprecated(n, sizeof(*t), tca, TCA_MAX,\n\t\t\t\t     rtm_tca_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tprio_allocate = false;\n\tparent = t->tcm_parent;\n\ttp = NULL;\n\tcl = 0;\n\tblock = NULL;\n\tflags = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is provided by the user,\n\t\t * we allocate one.\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_CREATE) {\n\t\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t\t\tprio_allocate = true;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid filter command with priority of zero\");\n\t\t\treturn -ENOENT;\n\t\t}\n\t}\n\n\t/* Find head of filter chain. */\n\n\terr = __tcf_qdisc_find(net, &q, &parent, t->tcm_ifindex, false, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (tcf_proto_check_kind(tca[TCA_KIND], name)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified TC filter name too long\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\t/* Take rtnl mutex if rtnl_held was set to true on previous iteration,\n\t * block is shared (no qdisc found), qdisc is not unlocked, classifier\n\t * type is not specified, classifier is not unlocked.\n\t */\n\tif (rtnl_held ||\n\t    (q && !(q->ops->cl_ops->flags & QDISC_CLASS_OPS_DOIT_UNLOCKED)) ||\n\t    !tcf_proto_is_unlocked(name)) {\n\t\trtnl_held = true;\n\t\trtnl_lock();\n\t}\n\n\terr = __tcf_qdisc_cl_find(q, parent, &cl, t->tcm_ifindex, extack);\n\tif (err)\n\t\tgoto errout;\n\n\tblock = __tcf_block_find(net, q, cl, t->tcm_ifindex, t->tcm_block_index,\n\t\t\t\t extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\tblock->classid = parent;\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, true);\n\tif (!chain) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot create specified filter chain\");\n\t\terr = -ENOMEM;\n\t\tgoto errout;\n\t}\n\n\tmutex_lock(&chain->filter_chain_lock);\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, prio_allocate);\n\tif (IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = PTR_ERR(tp);\n\t\tgoto errout_locked;\n\t}\n\n\tif (tp == NULL) {\n\t\tstruct tcf_proto *tp_new = NULL;\n\n\t\tif (chain->flushing) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Filter kind and protocol must be specified\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (prio_allocate)\n\t\t\tprio = tcf_auto_prio(tcf_chain_tp_prev(chain,\n\t\t\t\t\t\t\t       &chain_info));\n\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t\ttp_new = tcf_proto_create(name, protocol, prio, chain,\n\t\t\t\t\t  rtnl_held, extack);\n\t\tif (IS_ERR(tp_new)) {\n\t\t\terr = PTR_ERR(tp_new);\n\t\t\tgoto errout_tp;\n\t\t}\n\n\t\ttp_created = 1;\n\t\ttp = tcf_chain_tp_insert_unique(chain, tp_new, protocol, prio,\n\t\t\t\t\t\trtnl_held);\n\t\tif (IS_ERR(tp)) {\n\t\t\terr = PTR_ERR(tp);\n\t\t\tgoto errout_tp;\n\t\t}\n\t} else {\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout;\n\t\t}\n\t} else if (n->nlmsg_flags & NLM_F_EXCL) {\n\t\ttfilter_put(tp, fh);\n\t\tNL_SET_ERR_MSG(extack, \"Filter already exists\");\n\t\terr = -EEXIST;\n\t\tgoto errout;\n\t}\n\n\tif (chain->tmplt_ops && chain->tmplt_ops != tp->ops) {\n\t\tNL_SET_ERR_MSG(extack, \"Chain template is set to a different filter kind\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\tflags |= TCA_ACT_FLAGS_REPLACE;\n\tif (!rtnl_held)\n\t\tflags |= TCA_ACT_FLAGS_NO_RTNL;\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh,\n\t\t\t      flags, extack);\n\tif (err == 0) {\n\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t       RTM_NEWTFILTER, false, rtnl_held);\n\t\ttfilter_put(tp, fh);\n\t\t/* q pointer is NULL for shared blocks */\n\t\tif (q)\n\t\t\tq->flags &= ~TCQ_F_CAN_BYPASS;\n\t}\n\nerrout:\n\tif (err && tp_created)\n\t\ttcf_chain_tp_delete_empty(chain, tp, rtnl_held, NULL);\nerrout_tp:\n\tif (chain) {\n\t\tif (tp && !IS_ERR(tp))\n\t\t\ttcf_proto_put(tp, rtnl_held, NULL);\n\t\tif (!tp_created)\n\t\t\ttcf_chain_put(chain);\n\t}\n\ttcf_block_release(q, block, rtnl_held);\n\n\tif (rtnl_held)\n\t\trtnl_unlock();\n\n\tif (err == -EAGAIN) {\n\t\t/* Take rtnl lock in case EAGAIN is caused by concurrent flush\n\t\t * of target chain.\n\t\t */\n\t\trtnl_held = true;\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\t}\n\treturn err;\n\nerrout_locked:\n\tmutex_unlock(&chain->filter_chain_lock);\n\tgoto errout;\n}",
        "code_after_change": "static int tc_new_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tchar name[IFNAMSIZ];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tbool prio_allocate;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp;\n\tunsigned long cl;\n\tvoid *fh;\n\tint err;\n\tint tp_created;\n\tbool rtnl_held = false;\n\tu32 flags;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\ttp_created = 0;\n\n\terr = nlmsg_parse_deprecated(n, sizeof(*t), tca, TCA_MAX,\n\t\t\t\t     rtm_tca_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tprio_allocate = false;\n\tparent = t->tcm_parent;\n\ttp = NULL;\n\tcl = 0;\n\tblock = NULL;\n\tq = NULL;\n\tchain = NULL;\n\tflags = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is provided by the user,\n\t\t * we allocate one.\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_CREATE) {\n\t\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t\t\tprio_allocate = true;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid filter command with priority of zero\");\n\t\t\treturn -ENOENT;\n\t\t}\n\t}\n\n\t/* Find head of filter chain. */\n\n\terr = __tcf_qdisc_find(net, &q, &parent, t->tcm_ifindex, false, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (tcf_proto_check_kind(tca[TCA_KIND], name)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified TC filter name too long\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\t/* Take rtnl mutex if rtnl_held was set to true on previous iteration,\n\t * block is shared (no qdisc found), qdisc is not unlocked, classifier\n\t * type is not specified, classifier is not unlocked.\n\t */\n\tif (rtnl_held ||\n\t    (q && !(q->ops->cl_ops->flags & QDISC_CLASS_OPS_DOIT_UNLOCKED)) ||\n\t    !tcf_proto_is_unlocked(name)) {\n\t\trtnl_held = true;\n\t\trtnl_lock();\n\t}\n\n\terr = __tcf_qdisc_cl_find(q, parent, &cl, t->tcm_ifindex, extack);\n\tif (err)\n\t\tgoto errout;\n\n\tblock = __tcf_block_find(net, q, cl, t->tcm_ifindex, t->tcm_block_index,\n\t\t\t\t extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\tblock->classid = parent;\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, true);\n\tif (!chain) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot create specified filter chain\");\n\t\terr = -ENOMEM;\n\t\tgoto errout;\n\t}\n\n\tmutex_lock(&chain->filter_chain_lock);\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, prio_allocate);\n\tif (IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = PTR_ERR(tp);\n\t\tgoto errout_locked;\n\t}\n\n\tif (tp == NULL) {\n\t\tstruct tcf_proto *tp_new = NULL;\n\n\t\tif (chain->flushing) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Filter kind and protocol must be specified\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (prio_allocate)\n\t\t\tprio = tcf_auto_prio(tcf_chain_tp_prev(chain,\n\t\t\t\t\t\t\t       &chain_info));\n\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t\ttp_new = tcf_proto_create(name, protocol, prio, chain,\n\t\t\t\t\t  rtnl_held, extack);\n\t\tif (IS_ERR(tp_new)) {\n\t\t\terr = PTR_ERR(tp_new);\n\t\t\tgoto errout_tp;\n\t\t}\n\n\t\ttp_created = 1;\n\t\ttp = tcf_chain_tp_insert_unique(chain, tp_new, protocol, prio,\n\t\t\t\t\t\trtnl_held);\n\t\tif (IS_ERR(tp)) {\n\t\t\terr = PTR_ERR(tp);\n\t\t\tgoto errout_tp;\n\t\t}\n\t} else {\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout;\n\t\t}\n\t} else if (n->nlmsg_flags & NLM_F_EXCL) {\n\t\ttfilter_put(tp, fh);\n\t\tNL_SET_ERR_MSG(extack, \"Filter already exists\");\n\t\terr = -EEXIST;\n\t\tgoto errout;\n\t}\n\n\tif (chain->tmplt_ops && chain->tmplt_ops != tp->ops) {\n\t\tNL_SET_ERR_MSG(extack, \"Chain template is set to a different filter kind\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\tflags |= TCA_ACT_FLAGS_REPLACE;\n\tif (!rtnl_held)\n\t\tflags |= TCA_ACT_FLAGS_NO_RTNL;\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh,\n\t\t\t      flags, extack);\n\tif (err == 0) {\n\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t       RTM_NEWTFILTER, false, rtnl_held);\n\t\ttfilter_put(tp, fh);\n\t\t/* q pointer is NULL for shared blocks */\n\t\tif (q)\n\t\t\tq->flags &= ~TCQ_F_CAN_BYPASS;\n\t}\n\nerrout:\n\tif (err && tp_created)\n\t\ttcf_chain_tp_delete_empty(chain, tp, rtnl_held, NULL);\nerrout_tp:\n\tif (chain) {\n\t\tif (tp && !IS_ERR(tp))\n\t\t\ttcf_proto_put(tp, rtnl_held, NULL);\n\t\tif (!tp_created)\n\t\t\ttcf_chain_put(chain);\n\t}\n\ttcf_block_release(q, block, rtnl_held);\n\n\tif (rtnl_held)\n\t\trtnl_unlock();\n\n\tif (err == -EAGAIN) {\n\t\t/* Take rtnl lock in case EAGAIN is caused by concurrent flush\n\t\t * of target chain.\n\t\t */\n\t\trtnl_held = true;\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\t}\n\treturn err;\n\nerrout_locked:\n\tmutex_unlock(&chain->filter_chain_lock);\n\tgoto errout;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,9 +10,9 @@\n \tbool prio_allocate;\n \tu32 parent;\n \tu32 chain_index;\n-\tstruct Qdisc *q = NULL;\n+\tstruct Qdisc *q;\n \tstruct tcf_chain_info chain_info;\n-\tstruct tcf_chain *chain = NULL;\n+\tstruct tcf_chain *chain;\n \tstruct tcf_block *block;\n \tstruct tcf_proto *tp;\n \tunsigned long cl;\n@@ -41,6 +41,8 @@\n \ttp = NULL;\n \tcl = 0;\n \tblock = NULL;\n+\tq = NULL;\n+\tchain = NULL;\n \tflags = 0;\n \n \tif (prio == 0) {",
        "function_modified_lines": {
            "added": [
                "\tstruct Qdisc *q;",
                "\tstruct tcf_chain *chain;",
                "\tq = NULL;",
                "\tchain = NULL;"
            ],
            "deleted": [
                "\tstruct Qdisc *q = NULL;",
                "\tstruct tcf_chain *chain = NULL;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free exists in the Linux Kernel in tc_new_tfilter that could allow a local attacker to gain privilege escalation. The exploit requires unprivileged user namespaces. We recommend upgrading past commit 04c2a47ffb13c29778e2a14e414ad4cb5a5db4b5",
        "id": 3247
    },
    {
        "cve_id": "CVE-2019-25044",
        "code_before_change": "static void blk_mq_sched_tags_teardown(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_sched_free_tags(set, hctx, i);\n}",
        "code_after_change": "static void blk_mq_sched_tags_teardown(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (hctx->sched_tags) {\n\t\t\tblk_mq_free_rq_map(hctx->sched_tags);\n\t\t\thctx->sched_tags = NULL;\n\t\t}\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,9 +1,12 @@\n static void blk_mq_sched_tags_teardown(struct request_queue *q)\n {\n-\tstruct blk_mq_tag_set *set = q->tag_set;\n \tstruct blk_mq_hw_ctx *hctx;\n \tint i;\n \n-\tqueue_for_each_hw_ctx(q, hctx, i)\n-\t\tblk_mq_sched_free_tags(set, hctx, i);\n+\tqueue_for_each_hw_ctx(q, hctx, i) {\n+\t\tif (hctx->sched_tags) {\n+\t\t\tblk_mq_free_rq_map(hctx->sched_tags);\n+\t\t\thctx->sched_tags = NULL;\n+\t\t}\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tqueue_for_each_hw_ctx(q, hctx, i) {",
                "\t\tif (hctx->sched_tags) {",
                "\t\t\tblk_mq_free_rq_map(hctx->sched_tags);",
                "\t\t\thctx->sched_tags = NULL;",
                "\t\t}",
                "\t}"
            ],
            "deleted": [
                "\tstruct blk_mq_tag_set *set = q->tag_set;",
                "\tqueue_for_each_hw_ctx(q, hctx, i)",
                "\t\tblk_mq_sched_free_tags(set, hctx, i);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The block subsystem in the Linux kernel before 5.2 has a use-after-free that can lead to arbitrary code execution in the kernel context and privilege escalation, aka CID-c3e2219216c9. This is related to blk_mq_free_rqs and blk_cleanup_queue.",
        "id": 2299
    },
    {
        "cve_id": "CVE-2020-0423",
        "code_before_change": "static void binder_release_work(struct binder_proc *proc,\n\t\t\t\tstruct list_head *list)\n{\n\tstruct binder_work *w;\n\n\twhile (1) {\n\t\tw = binder_dequeue_work_head(proc, list);\n\t\tif (!w)\n\t\t\treturn;\n\n\t\tswitch (w->type) {\n\t\tcase BINDER_WORK_TRANSACTION: {\n\t\t\tstruct binder_transaction *t;\n\n\t\t\tt = container_of(w, struct binder_transaction, work);\n\n\t\t\tbinder_cleanup_transaction(t, \"process died.\",\n\t\t\t\t\t\t   BR_DEAD_REPLY);\n\t\t} break;\n\t\tcase BINDER_WORK_RETURN_ERROR: {\n\t\t\tstruct binder_error *e = container_of(\n\t\t\t\t\tw, struct binder_error, work);\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered TRANSACTION_ERROR: %u\\n\",\n\t\t\t\te->cmd);\n\t\t} break;\n\t\tcase BINDER_WORK_TRANSACTION_COMPLETE: {\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered TRANSACTION_COMPLETE\\n\");\n\t\t\tkfree(w);\n\t\t\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\n\t\t} break;\n\t\tcase BINDER_WORK_DEAD_BINDER_AND_CLEAR:\n\t\tcase BINDER_WORK_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tstruct binder_ref_death *death;\n\n\t\t\tdeath = container_of(w, struct binder_ref_death, work);\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered death notification, %016llx\\n\",\n\t\t\t\t(u64)death->cookie);\n\t\t\tkfree(death);\n\t\t\tbinder_stats_deleted(BINDER_STAT_DEATH);\n\t\t} break;\n\t\tdefault:\n\t\t\tpr_err(\"unexpected work type, %d, not freed\\n\",\n\t\t\t       w->type);\n\t\t\tbreak;\n\t\t}\n\t}\n\n}",
        "code_after_change": "static void binder_release_work(struct binder_proc *proc,\n\t\t\t\tstruct list_head *list)\n{\n\tstruct binder_work *w;\n\tenum binder_work_type wtype;\n\n\twhile (1) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tw = binder_dequeue_work_head_ilocked(list);\n\t\twtype = w ? w->type : 0;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!w)\n\t\t\treturn;\n\n\t\tswitch (wtype) {\n\t\tcase BINDER_WORK_TRANSACTION: {\n\t\t\tstruct binder_transaction *t;\n\n\t\t\tt = container_of(w, struct binder_transaction, work);\n\n\t\t\tbinder_cleanup_transaction(t, \"process died.\",\n\t\t\t\t\t\t   BR_DEAD_REPLY);\n\t\t} break;\n\t\tcase BINDER_WORK_RETURN_ERROR: {\n\t\t\tstruct binder_error *e = container_of(\n\t\t\t\t\tw, struct binder_error, work);\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered TRANSACTION_ERROR: %u\\n\",\n\t\t\t\te->cmd);\n\t\t} break;\n\t\tcase BINDER_WORK_TRANSACTION_COMPLETE: {\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered TRANSACTION_COMPLETE\\n\");\n\t\t\tkfree(w);\n\t\t\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\n\t\t} break;\n\t\tcase BINDER_WORK_DEAD_BINDER_AND_CLEAR:\n\t\tcase BINDER_WORK_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tstruct binder_ref_death *death;\n\n\t\t\tdeath = container_of(w, struct binder_ref_death, work);\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered death notification, %016llx\\n\",\n\t\t\t\t(u64)death->cookie);\n\t\t\tkfree(death);\n\t\t\tbinder_stats_deleted(BINDER_STAT_DEATH);\n\t\t} break;\n\t\tcase BINDER_WORK_NODE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_err(\"unexpected work type, %d, not freed\\n\",\n\t\t\t       wtype);\n\t\t\tbreak;\n\t\t}\n\t}\n\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,13 +2,17 @@\n \t\t\t\tstruct list_head *list)\n {\n \tstruct binder_work *w;\n+\tenum binder_work_type wtype;\n \n \twhile (1) {\n-\t\tw = binder_dequeue_work_head(proc, list);\n+\t\tbinder_inner_proc_lock(proc);\n+\t\tw = binder_dequeue_work_head_ilocked(list);\n+\t\twtype = w ? w->type : 0;\n+\t\tbinder_inner_proc_unlock(proc);\n \t\tif (!w)\n \t\t\treturn;\n \n-\t\tswitch (w->type) {\n+\t\tswitch (wtype) {\n \t\tcase BINDER_WORK_TRANSACTION: {\n \t\t\tstruct binder_transaction *t;\n \n@@ -42,9 +46,11 @@\n \t\t\tkfree(death);\n \t\t\tbinder_stats_deleted(BINDER_STAT_DEATH);\n \t\t} break;\n+\t\tcase BINDER_WORK_NODE:\n+\t\t\tbreak;\n \t\tdefault:\n \t\t\tpr_err(\"unexpected work type, %d, not freed\\n\",\n-\t\t\t       w->type);\n+\t\t\t       wtype);\n \t\t\tbreak;\n \t\t}\n \t}",
        "function_modified_lines": {
            "added": [
                "\tenum binder_work_type wtype;",
                "\t\tbinder_inner_proc_lock(proc);",
                "\t\tw = binder_dequeue_work_head_ilocked(list);",
                "\t\twtype = w ? w->type : 0;",
                "\t\tbinder_inner_proc_unlock(proc);",
                "\t\tswitch (wtype) {",
                "\t\tcase BINDER_WORK_NODE:",
                "\t\t\tbreak;",
                "\t\t\t       wtype);"
            ],
            "deleted": [
                "\t\tw = binder_dequeue_work_head(proc, list);",
                "\t\tswitch (w->type) {",
                "\t\t\t       w->type);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_release_work of binder.c, there is a possible use-after-free due to improper locking. This could lead to local escalation of privilege in the kernel with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-161151868References: N/A",
        "id": 2378
    },
    {
        "cve_id": "CVE-2020-25669",
        "code_before_change": "static void sunkbd_reinit(struct work_struct *work)\n{\n\tstruct sunkbd *sunkbd = container_of(work, struct sunkbd, tq);\n\n\twait_event_interruptible_timeout(sunkbd->wait, sunkbd->reset >= 0, HZ);\n\n\tserio_write(sunkbd->serio, SUNKBD_CMD_SETLED);\n\tserio_write(sunkbd->serio,\n\t\t(!!test_bit(LED_CAPSL,   sunkbd->dev->led) << 3) |\n\t\t(!!test_bit(LED_SCROLLL, sunkbd->dev->led) << 2) |\n\t\t(!!test_bit(LED_COMPOSE, sunkbd->dev->led) << 1) |\n\t\t !!test_bit(LED_NUML,    sunkbd->dev->led));\n\tserio_write(sunkbd->serio,\n\t\tSUNKBD_CMD_NOCLICK - !!test_bit(SND_CLICK, sunkbd->dev->snd));\n\tserio_write(sunkbd->serio,\n\t\tSUNKBD_CMD_BELLOFF - !!test_bit(SND_BELL, sunkbd->dev->snd));\n}",
        "code_after_change": "static void sunkbd_reinit(struct work_struct *work)\n{\n\tstruct sunkbd *sunkbd = container_of(work, struct sunkbd, tq);\n\n\t/*\n\t * It is OK that we check sunkbd->enabled without pausing serio,\n\t * as we only want to catch true->false transition that will\n\t * happen once and we will be woken up for it.\n\t */\n\twait_event_interruptible_timeout(sunkbd->wait,\n\t\t\t\t\t sunkbd->reset >= 0 || !sunkbd->enabled,\n\t\t\t\t\t HZ);\n\n\tif (sunkbd->reset >= 0 && sunkbd->enabled)\n\t\tsunkbd_set_leds_beeps(sunkbd);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,16 +2,15 @@\n {\n \tstruct sunkbd *sunkbd = container_of(work, struct sunkbd, tq);\n \n-\twait_event_interruptible_timeout(sunkbd->wait, sunkbd->reset >= 0, HZ);\n+\t/*\n+\t * It is OK that we check sunkbd->enabled without pausing serio,\n+\t * as we only want to catch true->false transition that will\n+\t * happen once and we will be woken up for it.\n+\t */\n+\twait_event_interruptible_timeout(sunkbd->wait,\n+\t\t\t\t\t sunkbd->reset >= 0 || !sunkbd->enabled,\n+\t\t\t\t\t HZ);\n \n-\tserio_write(sunkbd->serio, SUNKBD_CMD_SETLED);\n-\tserio_write(sunkbd->serio,\n-\t\t(!!test_bit(LED_CAPSL,   sunkbd->dev->led) << 3) |\n-\t\t(!!test_bit(LED_SCROLLL, sunkbd->dev->led) << 2) |\n-\t\t(!!test_bit(LED_COMPOSE, sunkbd->dev->led) << 1) |\n-\t\t !!test_bit(LED_NUML,    sunkbd->dev->led));\n-\tserio_write(sunkbd->serio,\n-\t\tSUNKBD_CMD_NOCLICK - !!test_bit(SND_CLICK, sunkbd->dev->snd));\n-\tserio_write(sunkbd->serio,\n-\t\tSUNKBD_CMD_BELLOFF - !!test_bit(SND_BELL, sunkbd->dev->snd));\n+\tif (sunkbd->reset >= 0 && sunkbd->enabled)\n+\t\tsunkbd_set_leds_beeps(sunkbd);\n }",
        "function_modified_lines": {
            "added": [
                "\t/*",
                "\t * It is OK that we check sunkbd->enabled without pausing serio,",
                "\t * as we only want to catch true->false transition that will",
                "\t * happen once and we will be woken up for it.",
                "\t */",
                "\twait_event_interruptible_timeout(sunkbd->wait,",
                "\t\t\t\t\t sunkbd->reset >= 0 || !sunkbd->enabled,",
                "\t\t\t\t\t HZ);",
                "\tif (sunkbd->reset >= 0 && sunkbd->enabled)",
                "\t\tsunkbd_set_leds_beeps(sunkbd);"
            ],
            "deleted": [
                "\twait_event_interruptible_timeout(sunkbd->wait, sunkbd->reset >= 0, HZ);",
                "\tserio_write(sunkbd->serio, SUNKBD_CMD_SETLED);",
                "\tserio_write(sunkbd->serio,",
                "\t\t(!!test_bit(LED_CAPSL,   sunkbd->dev->led) << 3) |",
                "\t\t(!!test_bit(LED_SCROLLL, sunkbd->dev->led) << 2) |",
                "\t\t(!!test_bit(LED_COMPOSE, sunkbd->dev->led) << 1) |",
                "\t\t !!test_bit(LED_NUML,    sunkbd->dev->led));",
                "\tserio_write(sunkbd->serio,",
                "\t\tSUNKBD_CMD_NOCLICK - !!test_bit(SND_CLICK, sunkbd->dev->snd));",
                "\tserio_write(sunkbd->serio,",
                "\t\tSUNKBD_CMD_BELLOFF - !!test_bit(SND_BELL, sunkbd->dev->snd));"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in the Linux Kernel where the function sunkbd_reinit having been scheduled by sunkbd_interrupt before sunkbd being freed. Though the dangling pointer is set to NULL in sunkbd_disconnect, there is still an alias in sunkbd_reinit causing Use After Free.",
        "id": 2598
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg(&np->opt, NULL);\n\t\t\tif (opt)\n\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = ipv6_renew_options(sk, np->opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "code_after_change": "static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n\t\t\t\t   NULL);\n\t\t\tif (opt) {\n\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\t\ttxopt_put(opt);\n\t\t\t}\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\topt = ipv6_renew_options(sk, opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\tatomic_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -93,9 +93,12 @@\n \t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n \t\t\t\tsk->sk_family = PF_INET;\n \t\t\t}\n-\t\t\topt = xchg(&np->opt, NULL);\n-\t\t\tif (opt)\n-\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n+\t\t\t\t   NULL);\n+\t\t\tif (opt) {\n+\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\t\ttxopt_put(opt);\n+\t\t\t}\n \t\t\tpktopt = xchg(&np->pktoptions, NULL);\n \t\t\tkfree_skb(pktopt);\n \n@@ -265,7 +268,8 @@\n \t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n \t\t\tbreak;\n \n-\t\topt = ipv6_renew_options(sk, np->opt, optname,\n+\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\t\topt = ipv6_renew_options(sk, opt, optname,\n \t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n \t\t\t\t\t optlen);\n \t\tif (IS_ERR(opt)) {\n@@ -294,8 +298,10 @@\n \t\tretv = 0;\n \t\topt = ipv6_update_options(sk, opt);\n sticky_done:\n-\t\tif (opt)\n-\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\tif (opt) {\n+\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\ttxopt_put(opt);\n+\t\t}\n \t\tbreak;\n \t}\n \n@@ -348,6 +354,7 @@\n \t\t\tbreak;\n \n \t\tmemset(opt, 0, sizeof(*opt));\n+\t\tatomic_set(&opt->refcnt, 1);\n \t\topt->tot_len = sizeof(*opt) + optlen;\n \t\tretv = -EFAULT;\n \t\tif (copy_from_user(opt+1, optval, optlen))\n@@ -364,8 +371,10 @@\n \t\tretv = 0;\n \t\topt = ipv6_update_options(sk, opt);\n done:\n-\t\tif (opt)\n-\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\tif (opt) {\n+\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\ttxopt_put(opt);\n+\t\t}\n \t\tbreak;\n \t}\n \tcase IPV6_UNICAST_HOPS:",
        "function_modified_lines": {
            "added": [
                "\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,",
                "\t\t\t\t   NULL);",
                "\t\t\tif (opt) {",
                "\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);",
                "\t\t\t\ttxopt_put(opt);",
                "\t\t\t}",
                "\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\t\topt = ipv6_renew_options(sk, opt, optname,",
                "\t\tif (opt) {",
                "\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);",
                "\t\t\ttxopt_put(opt);",
                "\t\t}",
                "\t\tatomic_set(&opt->refcnt, 1);",
                "\t\tif (opt) {",
                "\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);",
                "\t\t\ttxopt_put(opt);",
                "\t\t}"
            ],
            "deleted": [
                "\t\t\topt = xchg(&np->opt, NULL);",
                "\t\t\tif (opt)",
                "\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);",
                "\t\topt = ipv6_renew_options(sk, np->opt, optname,",
                "\t\tif (opt)",
                "\t\t\tsock_kfree_s(sk, opt, opt->tot_len);",
                "\t\tif (opt)",
                "\t\t\tsock_kfree_s(sk, opt, opt->tot_len);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1001
    },
    {
        "cve_id": "CVE-2017-7374",
        "code_before_change": "int fscrypt_get_encryption_info(struct inode *inode)\n{\n\tstruct fscrypt_info *ci = inode->i_crypt_info;\n\n\tif (!ci ||\n\t\t(ci->ci_keyring_key &&\n\t\t (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |\n\t\t\t\t\t       (1 << KEY_FLAG_REVOKED) |\n\t\t\t\t\t       (1 << KEY_FLAG_DEAD)))))\n\t\treturn fscrypt_get_crypt_info(inode);\n\treturn 0;\n}",
        "code_after_change": "int fscrypt_get_encryption_info(struct inode *inode)\n{\n\tstruct fscrypt_info *crypt_info;\n\tstruct fscrypt_context ctx;\n\tstruct crypto_skcipher *ctfm;\n\tconst char *cipher_str;\n\tint keysize;\n\tu8 *raw_key = NULL;\n\tint res;\n\n\tif (inode->i_crypt_info)\n\t\treturn 0;\n\n\tres = fscrypt_initialize(inode->i_sb->s_cop->flags);\n\tif (res)\n\t\treturn res;\n\n\tif (!inode->i_sb->s_cop->get_context)\n\t\treturn -EOPNOTSUPP;\n\n\tres = inode->i_sb->s_cop->get_context(inode, &ctx, sizeof(ctx));\n\tif (res < 0) {\n\t\tif (!fscrypt_dummy_context_enabled(inode) ||\n\t\t    inode->i_sb->s_cop->is_encrypted(inode))\n\t\t\treturn res;\n\t\t/* Fake up a context for an unencrypted directory */\n\t\tmemset(&ctx, 0, sizeof(ctx));\n\t\tctx.format = FS_ENCRYPTION_CONTEXT_FORMAT_V1;\n\t\tctx.contents_encryption_mode = FS_ENCRYPTION_MODE_AES_256_XTS;\n\t\tctx.filenames_encryption_mode = FS_ENCRYPTION_MODE_AES_256_CTS;\n\t\tmemset(ctx.master_key_descriptor, 0x42, FS_KEY_DESCRIPTOR_SIZE);\n\t} else if (res != sizeof(ctx)) {\n\t\treturn -EINVAL;\n\t}\n\n\tif (ctx.format != FS_ENCRYPTION_CONTEXT_FORMAT_V1)\n\t\treturn -EINVAL;\n\n\tif (ctx.flags & ~FS_POLICY_FLAGS_VALID)\n\t\treturn -EINVAL;\n\n\tcrypt_info = kmem_cache_alloc(fscrypt_info_cachep, GFP_NOFS);\n\tif (!crypt_info)\n\t\treturn -ENOMEM;\n\n\tcrypt_info->ci_flags = ctx.flags;\n\tcrypt_info->ci_data_mode = ctx.contents_encryption_mode;\n\tcrypt_info->ci_filename_mode = ctx.filenames_encryption_mode;\n\tcrypt_info->ci_ctfm = NULL;\n\tmemcpy(crypt_info->ci_master_key, ctx.master_key_descriptor,\n\t\t\t\tsizeof(crypt_info->ci_master_key));\n\n\tres = determine_cipher_type(crypt_info, inode, &cipher_str, &keysize);\n\tif (res)\n\t\tgoto out;\n\n\t/*\n\t * This cannot be a stack buffer because it is passed to the scatterlist\n\t * crypto API as part of key derivation.\n\t */\n\tres = -ENOMEM;\n\traw_key = kmalloc(FS_MAX_KEY_SIZE, GFP_NOFS);\n\tif (!raw_key)\n\t\tgoto out;\n\n\tres = validate_user_key(crypt_info, &ctx, raw_key, FS_KEY_DESC_PREFIX);\n\tif (res && inode->i_sb->s_cop->key_prefix) {\n\t\tint res2 = validate_user_key(crypt_info, &ctx, raw_key,\n\t\t\t\t\t     inode->i_sb->s_cop->key_prefix);\n\t\tif (res2) {\n\t\t\tif (res2 == -ENOKEY)\n\t\t\t\tres = -ENOKEY;\n\t\t\tgoto out;\n\t\t}\n\t} else if (res) {\n\t\tgoto out;\n\t}\n\tctfm = crypto_alloc_skcipher(cipher_str, 0, 0);\n\tif (!ctfm || IS_ERR(ctfm)) {\n\t\tres = ctfm ? PTR_ERR(ctfm) : -ENOMEM;\n\t\tprintk(KERN_DEBUG\n\t\t       \"%s: error %d (inode %u) allocating crypto tfm\\n\",\n\t\t       __func__, res, (unsigned) inode->i_ino);\n\t\tgoto out;\n\t}\n\tcrypt_info->ci_ctfm = ctfm;\n\tcrypto_skcipher_clear_flags(ctfm, ~0);\n\tcrypto_skcipher_set_flags(ctfm, CRYPTO_TFM_REQ_WEAK_KEY);\n\tres = crypto_skcipher_setkey(ctfm, raw_key, keysize);\n\tif (res)\n\t\tgoto out;\n\n\tif (cmpxchg(&inode->i_crypt_info, NULL, crypt_info) == NULL)\n\t\tcrypt_info = NULL;\nout:\n\tif (res == -ENOKEY)\n\t\tres = 0;\n\tput_crypt_info(crypt_info);\n\tkzfree(raw_key);\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,101 @@\n int fscrypt_get_encryption_info(struct inode *inode)\n {\n-\tstruct fscrypt_info *ci = inode->i_crypt_info;\n+\tstruct fscrypt_info *crypt_info;\n+\tstruct fscrypt_context ctx;\n+\tstruct crypto_skcipher *ctfm;\n+\tconst char *cipher_str;\n+\tint keysize;\n+\tu8 *raw_key = NULL;\n+\tint res;\n \n-\tif (!ci ||\n-\t\t(ci->ci_keyring_key &&\n-\t\t (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |\n-\t\t\t\t\t       (1 << KEY_FLAG_REVOKED) |\n-\t\t\t\t\t       (1 << KEY_FLAG_DEAD)))))\n-\t\treturn fscrypt_get_crypt_info(inode);\n-\treturn 0;\n+\tif (inode->i_crypt_info)\n+\t\treturn 0;\n+\n+\tres = fscrypt_initialize(inode->i_sb->s_cop->flags);\n+\tif (res)\n+\t\treturn res;\n+\n+\tif (!inode->i_sb->s_cop->get_context)\n+\t\treturn -EOPNOTSUPP;\n+\n+\tres = inode->i_sb->s_cop->get_context(inode, &ctx, sizeof(ctx));\n+\tif (res < 0) {\n+\t\tif (!fscrypt_dummy_context_enabled(inode) ||\n+\t\t    inode->i_sb->s_cop->is_encrypted(inode))\n+\t\t\treturn res;\n+\t\t/* Fake up a context for an unencrypted directory */\n+\t\tmemset(&ctx, 0, sizeof(ctx));\n+\t\tctx.format = FS_ENCRYPTION_CONTEXT_FORMAT_V1;\n+\t\tctx.contents_encryption_mode = FS_ENCRYPTION_MODE_AES_256_XTS;\n+\t\tctx.filenames_encryption_mode = FS_ENCRYPTION_MODE_AES_256_CTS;\n+\t\tmemset(ctx.master_key_descriptor, 0x42, FS_KEY_DESCRIPTOR_SIZE);\n+\t} else if (res != sizeof(ctx)) {\n+\t\treturn -EINVAL;\n+\t}\n+\n+\tif (ctx.format != FS_ENCRYPTION_CONTEXT_FORMAT_V1)\n+\t\treturn -EINVAL;\n+\n+\tif (ctx.flags & ~FS_POLICY_FLAGS_VALID)\n+\t\treturn -EINVAL;\n+\n+\tcrypt_info = kmem_cache_alloc(fscrypt_info_cachep, GFP_NOFS);\n+\tif (!crypt_info)\n+\t\treturn -ENOMEM;\n+\n+\tcrypt_info->ci_flags = ctx.flags;\n+\tcrypt_info->ci_data_mode = ctx.contents_encryption_mode;\n+\tcrypt_info->ci_filename_mode = ctx.filenames_encryption_mode;\n+\tcrypt_info->ci_ctfm = NULL;\n+\tmemcpy(crypt_info->ci_master_key, ctx.master_key_descriptor,\n+\t\t\t\tsizeof(crypt_info->ci_master_key));\n+\n+\tres = determine_cipher_type(crypt_info, inode, &cipher_str, &keysize);\n+\tif (res)\n+\t\tgoto out;\n+\n+\t/*\n+\t * This cannot be a stack buffer because it is passed to the scatterlist\n+\t * crypto API as part of key derivation.\n+\t */\n+\tres = -ENOMEM;\n+\traw_key = kmalloc(FS_MAX_KEY_SIZE, GFP_NOFS);\n+\tif (!raw_key)\n+\t\tgoto out;\n+\n+\tres = validate_user_key(crypt_info, &ctx, raw_key, FS_KEY_DESC_PREFIX);\n+\tif (res && inode->i_sb->s_cop->key_prefix) {\n+\t\tint res2 = validate_user_key(crypt_info, &ctx, raw_key,\n+\t\t\t\t\t     inode->i_sb->s_cop->key_prefix);\n+\t\tif (res2) {\n+\t\t\tif (res2 == -ENOKEY)\n+\t\t\t\tres = -ENOKEY;\n+\t\t\tgoto out;\n+\t\t}\n+\t} else if (res) {\n+\t\tgoto out;\n+\t}\n+\tctfm = crypto_alloc_skcipher(cipher_str, 0, 0);\n+\tif (!ctfm || IS_ERR(ctfm)) {\n+\t\tres = ctfm ? PTR_ERR(ctfm) : -ENOMEM;\n+\t\tprintk(KERN_DEBUG\n+\t\t       \"%s: error %d (inode %u) allocating crypto tfm\\n\",\n+\t\t       __func__, res, (unsigned) inode->i_ino);\n+\t\tgoto out;\n+\t}\n+\tcrypt_info->ci_ctfm = ctfm;\n+\tcrypto_skcipher_clear_flags(ctfm, ~0);\n+\tcrypto_skcipher_set_flags(ctfm, CRYPTO_TFM_REQ_WEAK_KEY);\n+\tres = crypto_skcipher_setkey(ctfm, raw_key, keysize);\n+\tif (res)\n+\t\tgoto out;\n+\n+\tif (cmpxchg(&inode->i_crypt_info, NULL, crypt_info) == NULL)\n+\t\tcrypt_info = NULL;\n+out:\n+\tif (res == -ENOKEY)\n+\t\tres = 0;\n+\tput_crypt_info(crypt_info);\n+\tkzfree(raw_key);\n+\treturn res;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct fscrypt_info *crypt_info;",
                "\tstruct fscrypt_context ctx;",
                "\tstruct crypto_skcipher *ctfm;",
                "\tconst char *cipher_str;",
                "\tint keysize;",
                "\tu8 *raw_key = NULL;",
                "\tint res;",
                "\tif (inode->i_crypt_info)",
                "\t\treturn 0;",
                "",
                "\tres = fscrypt_initialize(inode->i_sb->s_cop->flags);",
                "\tif (res)",
                "\t\treturn res;",
                "",
                "\tif (!inode->i_sb->s_cop->get_context)",
                "\t\treturn -EOPNOTSUPP;",
                "",
                "\tres = inode->i_sb->s_cop->get_context(inode, &ctx, sizeof(ctx));",
                "\tif (res < 0) {",
                "\t\tif (!fscrypt_dummy_context_enabled(inode) ||",
                "\t\t    inode->i_sb->s_cop->is_encrypted(inode))",
                "\t\t\treturn res;",
                "\t\t/* Fake up a context for an unencrypted directory */",
                "\t\tmemset(&ctx, 0, sizeof(ctx));",
                "\t\tctx.format = FS_ENCRYPTION_CONTEXT_FORMAT_V1;",
                "\t\tctx.contents_encryption_mode = FS_ENCRYPTION_MODE_AES_256_XTS;",
                "\t\tctx.filenames_encryption_mode = FS_ENCRYPTION_MODE_AES_256_CTS;",
                "\t\tmemset(ctx.master_key_descriptor, 0x42, FS_KEY_DESCRIPTOR_SIZE);",
                "\t} else if (res != sizeof(ctx)) {",
                "\t\treturn -EINVAL;",
                "\t}",
                "",
                "\tif (ctx.format != FS_ENCRYPTION_CONTEXT_FORMAT_V1)",
                "\t\treturn -EINVAL;",
                "",
                "\tif (ctx.flags & ~FS_POLICY_FLAGS_VALID)",
                "\t\treturn -EINVAL;",
                "",
                "\tcrypt_info = kmem_cache_alloc(fscrypt_info_cachep, GFP_NOFS);",
                "\tif (!crypt_info)",
                "\t\treturn -ENOMEM;",
                "",
                "\tcrypt_info->ci_flags = ctx.flags;",
                "\tcrypt_info->ci_data_mode = ctx.contents_encryption_mode;",
                "\tcrypt_info->ci_filename_mode = ctx.filenames_encryption_mode;",
                "\tcrypt_info->ci_ctfm = NULL;",
                "\tmemcpy(crypt_info->ci_master_key, ctx.master_key_descriptor,",
                "\t\t\t\tsizeof(crypt_info->ci_master_key));",
                "",
                "\tres = determine_cipher_type(crypt_info, inode, &cipher_str, &keysize);",
                "\tif (res)",
                "\t\tgoto out;",
                "",
                "\t/*",
                "\t * This cannot be a stack buffer because it is passed to the scatterlist",
                "\t * crypto API as part of key derivation.",
                "\t */",
                "\tres = -ENOMEM;",
                "\traw_key = kmalloc(FS_MAX_KEY_SIZE, GFP_NOFS);",
                "\tif (!raw_key)",
                "\t\tgoto out;",
                "",
                "\tres = validate_user_key(crypt_info, &ctx, raw_key, FS_KEY_DESC_PREFIX);",
                "\tif (res && inode->i_sb->s_cop->key_prefix) {",
                "\t\tint res2 = validate_user_key(crypt_info, &ctx, raw_key,",
                "\t\t\t\t\t     inode->i_sb->s_cop->key_prefix);",
                "\t\tif (res2) {",
                "\t\t\tif (res2 == -ENOKEY)",
                "\t\t\t\tres = -ENOKEY;",
                "\t\t\tgoto out;",
                "\t\t}",
                "\t} else if (res) {",
                "\t\tgoto out;",
                "\t}",
                "\tctfm = crypto_alloc_skcipher(cipher_str, 0, 0);",
                "\tif (!ctfm || IS_ERR(ctfm)) {",
                "\t\tres = ctfm ? PTR_ERR(ctfm) : -ENOMEM;",
                "\t\tprintk(KERN_DEBUG",
                "\t\t       \"%s: error %d (inode %u) allocating crypto tfm\\n\",",
                "\t\t       __func__, res, (unsigned) inode->i_ino);",
                "\t\tgoto out;",
                "\t}",
                "\tcrypt_info->ci_ctfm = ctfm;",
                "\tcrypto_skcipher_clear_flags(ctfm, ~0);",
                "\tcrypto_skcipher_set_flags(ctfm, CRYPTO_TFM_REQ_WEAK_KEY);",
                "\tres = crypto_skcipher_setkey(ctfm, raw_key, keysize);",
                "\tif (res)",
                "\t\tgoto out;",
                "",
                "\tif (cmpxchg(&inode->i_crypt_info, NULL, crypt_info) == NULL)",
                "\t\tcrypt_info = NULL;",
                "out:",
                "\tif (res == -ENOKEY)",
                "\t\tres = 0;",
                "\tput_crypt_info(crypt_info);",
                "\tkzfree(raw_key);",
                "\treturn res;"
            ],
            "deleted": [
                "\tstruct fscrypt_info *ci = inode->i_crypt_info;",
                "\tif (!ci ||",
                "\t\t(ci->ci_keyring_key &&",
                "\t\t (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |",
                "\t\t\t\t\t       (1 << KEY_FLAG_REVOKED) |",
                "\t\t\t\t\t       (1 << KEY_FLAG_DEAD)))))",
                "\t\treturn fscrypt_get_crypt_info(inode);",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in fs/crypto/ in the Linux kernel before 4.10.7 allows local users to cause a denial of service (NULL pointer dereference) or possibly gain privileges by revoking keyring keys being used for ext4, f2fs, or ubifs encryption, causing cryptographic transform objects to be freed prematurely.",
        "id": 1499
    },
    {
        "cve_id": "CVE-2023-1195",
        "code_before_change": "void\ncifs_put_tcp_session(struct TCP_Server_Info *server, int from_reconnect)\n{\n\tstruct task_struct *task;\n\n\tspin_lock(&cifs_tcp_ses_lock);\n\tif (--server->srv_count > 0) {\n\t\tspin_unlock(&cifs_tcp_ses_lock);\n\t\treturn;\n\t}\n\n\t/* srv_count can never go negative */\n\tWARN_ON(server->srv_count < 0);\n\n\tput_net(cifs_net_ns(server));\n\n\tlist_del_init(&server->tcp_ses_list);\n\tspin_unlock(&cifs_tcp_ses_lock);\n\n\t/* For secondary channels, we pick up ref-count on the primary server */\n\tif (CIFS_SERVER_IS_CHAN(server))\n\t\tcifs_put_tcp_session(server->primary_server, from_reconnect);\n\n\tcancel_delayed_work_sync(&server->echo);\n\tcancel_delayed_work_sync(&server->resolve);\n\n\tif (from_reconnect)\n\t\t/*\n\t\t * Avoid deadlock here: reconnect work calls\n\t\t * cifs_put_tcp_session() at its end. Need to be sure\n\t\t * that reconnect work does nothing with server pointer after\n\t\t * that step.\n\t\t */\n\t\tcancel_delayed_work(&server->reconnect);\n\telse\n\t\tcancel_delayed_work_sync(&server->reconnect);\n\n\tspin_lock(&server->srv_lock);\n\tserver->tcpStatus = CifsExiting;\n\tspin_unlock(&server->srv_lock);\n\n\tcifs_crypto_secmech_release(server);\n\n\tkfree_sensitive(server->session_key.response);\n\tserver->session_key.response = NULL;\n\tserver->session_key.len = 0;\n\tkfree(server->hostname);\n\n\ttask = xchg(&server->tsk, NULL);\n\tif (task)\n\t\tsend_sig(SIGKILL, task, 1);\n}",
        "code_after_change": "void\ncifs_put_tcp_session(struct TCP_Server_Info *server, int from_reconnect)\n{\n\tstruct task_struct *task;\n\n\tspin_lock(&cifs_tcp_ses_lock);\n\tif (--server->srv_count > 0) {\n\t\tspin_unlock(&cifs_tcp_ses_lock);\n\t\treturn;\n\t}\n\n\t/* srv_count can never go negative */\n\tWARN_ON(server->srv_count < 0);\n\n\tput_net(cifs_net_ns(server));\n\n\tlist_del_init(&server->tcp_ses_list);\n\tspin_unlock(&cifs_tcp_ses_lock);\n\n\t/* For secondary channels, we pick up ref-count on the primary server */\n\tif (CIFS_SERVER_IS_CHAN(server))\n\t\tcifs_put_tcp_session(server->primary_server, from_reconnect);\n\n\tcancel_delayed_work_sync(&server->echo);\n\tcancel_delayed_work_sync(&server->resolve);\n\n\tif (from_reconnect)\n\t\t/*\n\t\t * Avoid deadlock here: reconnect work calls\n\t\t * cifs_put_tcp_session() at its end. Need to be sure\n\t\t * that reconnect work does nothing with server pointer after\n\t\t * that step.\n\t\t */\n\t\tcancel_delayed_work(&server->reconnect);\n\telse\n\t\tcancel_delayed_work_sync(&server->reconnect);\n\n\tspin_lock(&server->srv_lock);\n\tserver->tcpStatus = CifsExiting;\n\tspin_unlock(&server->srv_lock);\n\n\tcifs_crypto_secmech_release(server);\n\n\tkfree_sensitive(server->session_key.response);\n\tserver->session_key.response = NULL;\n\tserver->session_key.len = 0;\n\tkfree(server->hostname);\n\tserver->hostname = NULL;\n\n\ttask = xchg(&server->tsk, NULL);\n\tif (task)\n\t\tsend_sig(SIGKILL, task, 1);\n}",
        "patch": "--- code before\n+++ code after\n@@ -45,6 +45,7 @@\n \tserver->session_key.response = NULL;\n \tserver->session_key.len = 0;\n \tkfree(server->hostname);\n+\tserver->hostname = NULL;\n \n \ttask = xchg(&server->tsk, NULL);\n \tif (task)",
        "function_modified_lines": {
            "added": [
                "\tserver->hostname = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in reconn_set_ipaddr_from_hostname in fs/cifs/connect.c in the Linux kernel. The issue occurs when it forgets to set the free pointer server->hostname to NULL, leading to an invalid pointer request.",
        "id": 3856
    },
    {
        "cve_id": "CVE-2016-10088",
        "code_before_change": "static ssize_t\nbsg_write(struct file *file, const char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct bsg_device *bd = file->private_data;\n\tssize_t bytes_written;\n\tint ret;\n\n\tdprintk(\"%s: write %Zd bytes\\n\", bd->name, count);\n\n\tbsg_set_block(bd, file);\n\n\tbytes_written = 0;\n\tret = __bsg_write(bd, buf, count, &bytes_written,\n\t\t\t  file->f_mode & FMODE_WRITE);\n\n\t*ppos = bytes_written;\n\n\t/*\n\t * return bytes written on non-fatal errors\n\t */\n\tif (!bytes_written || err_block_err(ret))\n\t\tbytes_written = ret;\n\n\tdprintk(\"%s: returning %Zd\\n\", bd->name, bytes_written);\n\treturn bytes_written;\n}",
        "code_after_change": "static ssize_t\nbsg_write(struct file *file, const char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct bsg_device *bd = file->private_data;\n\tssize_t bytes_written;\n\tint ret;\n\n\tdprintk(\"%s: write %Zd bytes\\n\", bd->name, count);\n\n\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))\n\t\treturn -EINVAL;\n\n\tbsg_set_block(bd, file);\n\n\tbytes_written = 0;\n\tret = __bsg_write(bd, buf, count, &bytes_written,\n\t\t\t  file->f_mode & FMODE_WRITE);\n\n\t*ppos = bytes_written;\n\n\t/*\n\t * return bytes written on non-fatal errors\n\t */\n\tif (!bytes_written || err_block_err(ret))\n\t\tbytes_written = ret;\n\n\tdprintk(\"%s: returning %Zd\\n\", bd->name, bytes_written);\n\treturn bytes_written;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,9 @@\n \tint ret;\n \n \tdprintk(\"%s: write %Zd bytes\\n\", bd->name, count);\n+\n+\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))\n+\t\treturn -EINVAL;\n \n \tbsg_set_block(bd, file);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))",
                "\t\treturn -EINVAL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The sg implementation in the Linux kernel through 4.9 does not properly restrict write operations in situations where the KERNEL_DS option is set, which allows local users to read or write to arbitrary kernel memory locations or cause a denial of service (use-after-free) by leveraging access to a /dev/sg device, related to block/bsg.c and drivers/scsi/sg.c.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2016-9576.",
        "id": 893
    },
    {
        "cve_id": "CVE-2022-28893",
        "code_before_change": "void xprt_connect(struct rpc_task *task)\n{\n\tstruct rpc_xprt\t*xprt = task->tk_rqstp->rq_xprt;\n\n\ttrace_xprt_connect(xprt);\n\n\tif (!xprt_bound(xprt)) {\n\t\ttask->tk_status = -EAGAIN;\n\t\treturn;\n\t}\n\tif (!xprt_lock_write(xprt, task))\n\t\treturn;\n\n\tif (test_and_clear_bit(XPRT_CLOSE_WAIT, &xprt->state)) {\n\t\ttrace_xprt_disconnect_cleanup(xprt);\n\t\txprt->ops->close(xprt);\n\t}\n\n\tif (!xprt_connected(xprt)) {\n\t\ttask->tk_rqstp->rq_connect_cookie = xprt->connect_cookie;\n\t\trpc_sleep_on_timeout(&xprt->pending, task, NULL,\n\t\t\t\txprt_request_timeout(task->tk_rqstp));\n\n\t\tif (test_bit(XPRT_CLOSING, &xprt->state))\n\t\t\treturn;\n\t\tif (xprt_test_and_set_connecting(xprt))\n\t\t\treturn;\n\t\t/* Race breaker */\n\t\tif (!xprt_connected(xprt)) {\n\t\t\txprt->stat.connect_start = jiffies;\n\t\t\txprt->ops->connect(xprt, task);\n\t\t} else {\n\t\t\txprt_clear_connecting(xprt);\n\t\t\ttask->tk_status = 0;\n\t\t\trpc_wake_up_queued_task(&xprt->pending, task);\n\t\t}\n\t}\n\txprt_release_write(xprt, task);\n}",
        "code_after_change": "void xprt_connect(struct rpc_task *task)\n{\n\tstruct rpc_xprt\t*xprt = task->tk_rqstp->rq_xprt;\n\n\ttrace_xprt_connect(xprt);\n\n\tif (!xprt_bound(xprt)) {\n\t\ttask->tk_status = -EAGAIN;\n\t\treturn;\n\t}\n\tif (!xprt_lock_write(xprt, task))\n\t\treturn;\n\n\tif (!xprt_connected(xprt) && !test_bit(XPRT_CLOSE_WAIT, &xprt->state)) {\n\t\ttask->tk_rqstp->rq_connect_cookie = xprt->connect_cookie;\n\t\trpc_sleep_on_timeout(&xprt->pending, task, NULL,\n\t\t\t\txprt_request_timeout(task->tk_rqstp));\n\n\t\tif (test_bit(XPRT_CLOSING, &xprt->state))\n\t\t\treturn;\n\t\tif (xprt_test_and_set_connecting(xprt))\n\t\t\treturn;\n\t\t/* Race breaker */\n\t\tif (!xprt_connected(xprt)) {\n\t\t\txprt->stat.connect_start = jiffies;\n\t\t\txprt->ops->connect(xprt, task);\n\t\t} else {\n\t\t\txprt_clear_connecting(xprt);\n\t\t\ttask->tk_status = 0;\n\t\t\trpc_wake_up_queued_task(&xprt->pending, task);\n\t\t}\n\t}\n\txprt_release_write(xprt, task);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,12 +11,7 @@\n \tif (!xprt_lock_write(xprt, task))\n \t\treturn;\n \n-\tif (test_and_clear_bit(XPRT_CLOSE_WAIT, &xprt->state)) {\n-\t\ttrace_xprt_disconnect_cleanup(xprt);\n-\t\txprt->ops->close(xprt);\n-\t}\n-\n-\tif (!xprt_connected(xprt)) {\n+\tif (!xprt_connected(xprt) && !test_bit(XPRT_CLOSE_WAIT, &xprt->state)) {\n \t\ttask->tk_rqstp->rq_connect_cookie = xprt->connect_cookie;\n \t\trpc_sleep_on_timeout(&xprt->pending, task, NULL,\n \t\t\t\txprt_request_timeout(task->tk_rqstp));",
        "function_modified_lines": {
            "added": [
                "\tif (!xprt_connected(xprt) && !test_bit(XPRT_CLOSE_WAIT, &xprt->state)) {"
            ],
            "deleted": [
                "\tif (test_and_clear_bit(XPRT_CLOSE_WAIT, &xprt->state)) {",
                "\t\ttrace_xprt_disconnect_cleanup(xprt);",
                "\t\txprt->ops->close(xprt);",
                "\t}",
                "",
                "\tif (!xprt_connected(xprt)) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The SUNRPC subsystem in the Linux kernel through 5.17.2 can call xs_xprt_free before ensuring that sockets are in the intended state.",
        "id": 3507
    },
    {
        "cve_id": "CVE-2022-2978",
        "code_before_change": "int inode_init_always(struct super_block *sb, struct inode *inode)\n{\n\tstatic const struct inode_operations empty_iops;\n\tstatic const struct file_operations no_open_fops = {.open = no_open};\n\tstruct address_space *const mapping = &inode->i_data;\n\n\tinode->i_sb = sb;\n\tinode->i_blkbits = sb->s_blocksize_bits;\n\tinode->i_flags = 0;\n\tatomic64_set(&inode->i_sequence, 0);\n\tatomic_set(&inode->i_count, 1);\n\tinode->i_op = &empty_iops;\n\tinode->i_fop = &no_open_fops;\n\tinode->i_ino = 0;\n\tinode->__i_nlink = 1;\n\tinode->i_opflags = 0;\n\tif (sb->s_xattr)\n\t\tinode->i_opflags |= IOP_XATTR;\n\ti_uid_write(inode, 0);\n\ti_gid_write(inode, 0);\n\tatomic_set(&inode->i_writecount, 0);\n\tinode->i_size = 0;\n\tinode->i_write_hint = WRITE_LIFE_NOT_SET;\n\tinode->i_blocks = 0;\n\tinode->i_bytes = 0;\n\tinode->i_generation = 0;\n\tinode->i_pipe = NULL;\n\tinode->i_cdev = NULL;\n\tinode->i_link = NULL;\n\tinode->i_dir_seq = 0;\n\tinode->i_rdev = 0;\n\tinode->dirtied_when = 0;\n\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tinode->i_wb_frn_winner = 0;\n\tinode->i_wb_frn_avg_time = 0;\n\tinode->i_wb_frn_history = 0;\n#endif\n\n\tif (security_inode_alloc(inode))\n\t\tgoto out;\n\tspin_lock_init(&inode->i_lock);\n\tlockdep_set_class(&inode->i_lock, &sb->s_type->i_lock_key);\n\n\tinit_rwsem(&inode->i_rwsem);\n\tlockdep_set_class(&inode->i_rwsem, &sb->s_type->i_mutex_key);\n\n\tatomic_set(&inode->i_dio_count, 0);\n\n\tmapping->a_ops = &empty_aops;\n\tmapping->host = inode;\n\tmapping->flags = 0;\n\tmapping->wb_err = 0;\n\tatomic_set(&mapping->i_mmap_writable, 0);\n#ifdef CONFIG_READ_ONLY_THP_FOR_FS\n\tatomic_set(&mapping->nr_thps, 0);\n#endif\n\tmapping_set_gfp_mask(mapping, GFP_HIGHUSER_MOVABLE);\n\tmapping->private_data = NULL;\n\tmapping->writeback_index = 0;\n\tinit_rwsem(&mapping->invalidate_lock);\n\tlockdep_set_class_and_name(&mapping->invalidate_lock,\n\t\t\t\t   &sb->s_type->invalidate_lock_key,\n\t\t\t\t   \"mapping.invalidate_lock\");\n\tinode->i_private = NULL;\n\tinode->i_mapping = mapping;\n\tINIT_HLIST_HEAD(&inode->i_dentry);\t/* buggered by rcu freeing */\n#ifdef CONFIG_FS_POSIX_ACL\n\tinode->i_acl = inode->i_default_acl = ACL_NOT_CACHED;\n#endif\n\n#ifdef CONFIG_FSNOTIFY\n\tinode->i_fsnotify_mask = 0;\n#endif\n\tinode->i_flctx = NULL;\n\tthis_cpu_inc(nr_inodes);\n\n\treturn 0;\nout:\n\treturn -ENOMEM;\n}",
        "code_after_change": "int inode_init_always(struct super_block *sb, struct inode *inode)\n{\n\tstatic const struct inode_operations empty_iops;\n\tstatic const struct file_operations no_open_fops = {.open = no_open};\n\tstruct address_space *const mapping = &inode->i_data;\n\n\tinode->i_sb = sb;\n\tinode->i_blkbits = sb->s_blocksize_bits;\n\tinode->i_flags = 0;\n\tatomic64_set(&inode->i_sequence, 0);\n\tatomic_set(&inode->i_count, 1);\n\tinode->i_op = &empty_iops;\n\tinode->i_fop = &no_open_fops;\n\tinode->i_ino = 0;\n\tinode->__i_nlink = 1;\n\tinode->i_opflags = 0;\n\tif (sb->s_xattr)\n\t\tinode->i_opflags |= IOP_XATTR;\n\ti_uid_write(inode, 0);\n\ti_gid_write(inode, 0);\n\tatomic_set(&inode->i_writecount, 0);\n\tinode->i_size = 0;\n\tinode->i_write_hint = WRITE_LIFE_NOT_SET;\n\tinode->i_blocks = 0;\n\tinode->i_bytes = 0;\n\tinode->i_generation = 0;\n\tinode->i_pipe = NULL;\n\tinode->i_cdev = NULL;\n\tinode->i_link = NULL;\n\tinode->i_dir_seq = 0;\n\tinode->i_rdev = 0;\n\tinode->dirtied_when = 0;\n\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tinode->i_wb_frn_winner = 0;\n\tinode->i_wb_frn_avg_time = 0;\n\tinode->i_wb_frn_history = 0;\n#endif\n\n\tspin_lock_init(&inode->i_lock);\n\tlockdep_set_class(&inode->i_lock, &sb->s_type->i_lock_key);\n\n\tinit_rwsem(&inode->i_rwsem);\n\tlockdep_set_class(&inode->i_rwsem, &sb->s_type->i_mutex_key);\n\n\tatomic_set(&inode->i_dio_count, 0);\n\n\tmapping->a_ops = &empty_aops;\n\tmapping->host = inode;\n\tmapping->flags = 0;\n\tmapping->wb_err = 0;\n\tatomic_set(&mapping->i_mmap_writable, 0);\n#ifdef CONFIG_READ_ONLY_THP_FOR_FS\n\tatomic_set(&mapping->nr_thps, 0);\n#endif\n\tmapping_set_gfp_mask(mapping, GFP_HIGHUSER_MOVABLE);\n\tmapping->private_data = NULL;\n\tmapping->writeback_index = 0;\n\tinit_rwsem(&mapping->invalidate_lock);\n\tlockdep_set_class_and_name(&mapping->invalidate_lock,\n\t\t\t\t   &sb->s_type->invalidate_lock_key,\n\t\t\t\t   \"mapping.invalidate_lock\");\n\tinode->i_private = NULL;\n\tinode->i_mapping = mapping;\n\tINIT_HLIST_HEAD(&inode->i_dentry);\t/* buggered by rcu freeing */\n#ifdef CONFIG_FS_POSIX_ACL\n\tinode->i_acl = inode->i_default_acl = ACL_NOT_CACHED;\n#endif\n\n#ifdef CONFIG_FSNOTIFY\n\tinode->i_fsnotify_mask = 0;\n#endif\n\tinode->i_flctx = NULL;\n\n\tif (unlikely(security_inode_alloc(inode)))\n\t\treturn -ENOMEM;\n\tthis_cpu_inc(nr_inodes);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -37,8 +37,6 @@\n \tinode->i_wb_frn_history = 0;\n #endif\n \n-\tif (security_inode_alloc(inode))\n-\t\tgoto out;\n \tspin_lock_init(&inode->i_lock);\n \tlockdep_set_class(&inode->i_lock, &sb->s_type->i_lock_key);\n \n@@ -73,9 +71,10 @@\n \tinode->i_fsnotify_mask = 0;\n #endif\n \tinode->i_flctx = NULL;\n+\n+\tif (unlikely(security_inode_alloc(inode)))\n+\t\treturn -ENOMEM;\n \tthis_cpu_inc(nr_inodes);\n \n \treturn 0;\n-out:\n-\treturn -ENOMEM;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (unlikely(security_inode_alloc(inode)))",
                "\t\treturn -ENOMEM;"
            ],
            "deleted": [
                "\tif (security_inode_alloc(inode))",
                "\t\tgoto out;",
                "out:",
                "\treturn -ENOMEM;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw use after free in the Linux kernel NILFS file system was found in the way user triggers function security_inode_alloc to fail with following call to function nilfs_mdt_destroy. A local user could use this flaw to crash the system or potentially escalate their privileges on the system.",
        "id": 3529
    },
    {
        "cve_id": "CVE-2023-30772",
        "code_before_change": "static int da9150_charger_remove(struct platform_device *pdev)\n{\n\tstruct da9150_charger *charger = platform_get_drvdata(pdev);\n\tint irq;\n\n\t/* Make sure IRQs are released before unregistering power supplies */\n\tirq = platform_get_irq_byname(pdev, \"CHG_VBUS\");\n\tfree_irq(irq, charger);\n\n\tirq = platform_get_irq_byname(pdev, \"CHG_VFAULT\");\n\tfree_irq(irq, charger);\n\n\tirq = platform_get_irq_byname(pdev, \"CHG_TJUNC\");\n\tfree_irq(irq, charger);\n\n\tirq = platform_get_irq_byname(pdev, \"CHG_STATUS\");\n\tfree_irq(irq, charger);\n\n\tif (!IS_ERR_OR_NULL(charger->usb_phy))\n\t\tusb_unregister_notifier(charger->usb_phy, &charger->otg_nb);\n\n\tpower_supply_unregister(charger->battery);\n\tpower_supply_unregister(charger->usb);\n\n\t/* Release ADC channels */\n\tiio_channel_release(charger->ibus_chan);\n\tiio_channel_release(charger->vbus_chan);\n\tiio_channel_release(charger->tjunc_chan);\n\tiio_channel_release(charger->vbat_chan);\n\n\treturn 0;\n}",
        "code_after_change": "static int da9150_charger_remove(struct platform_device *pdev)\n{\n\tstruct da9150_charger *charger = platform_get_drvdata(pdev);\n\tint irq;\n\n\t/* Make sure IRQs are released before unregistering power supplies */\n\tirq = platform_get_irq_byname(pdev, \"CHG_VBUS\");\n\tfree_irq(irq, charger);\n\n\tirq = platform_get_irq_byname(pdev, \"CHG_VFAULT\");\n\tfree_irq(irq, charger);\n\n\tirq = platform_get_irq_byname(pdev, \"CHG_TJUNC\");\n\tfree_irq(irq, charger);\n\n\tirq = platform_get_irq_byname(pdev, \"CHG_STATUS\");\n\tfree_irq(irq, charger);\n\n\tif (!IS_ERR_OR_NULL(charger->usb_phy))\n\t\tusb_unregister_notifier(charger->usb_phy, &charger->otg_nb);\n\tcancel_work_sync(&charger->otg_work);\n\n\tpower_supply_unregister(charger->battery);\n\tpower_supply_unregister(charger->usb);\n\n\t/* Release ADC channels */\n\tiio_channel_release(charger->ibus_chan);\n\tiio_channel_release(charger->vbus_chan);\n\tiio_channel_release(charger->tjunc_chan);\n\tiio_channel_release(charger->vbat_chan);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,6 +18,7 @@\n \n \tif (!IS_ERR_OR_NULL(charger->usb_phy))\n \t\tusb_unregister_notifier(charger->usb_phy, &charger->otg_nb);\n+\tcancel_work_sync(&charger->otg_work);\n \n \tpower_supply_unregister(charger->battery);\n \tpower_supply_unregister(charger->usb);",
        "function_modified_lines": {
            "added": [
                "\tcancel_work_sync(&charger->otg_work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 6.2.9 has a race condition and resultant use-after-free in drivers/power/supply/da9150-charger.c if a physically proximate attacker unplugs a device.",
        "id": 3989
    },
    {
        "cve_id": "CVE-2023-1872",
        "code_before_change": "static struct iovec *__io_import_iovec(int rw, struct io_kiocb *req,\n\t\t\t\t       struct io_rw_state *s,\n\t\t\t\t       unsigned int issue_flags)\n{\n\tstruct iov_iter *iter = &s->iter;\n\tu8 opcode = req->opcode;\n\tstruct iovec *iovec;\n\tvoid __user *buf;\n\tsize_t sqe_len;\n\tssize_t ret;\n\n\tif (opcode == IORING_OP_READ_FIXED || opcode == IORING_OP_WRITE_FIXED) {\n\t\tret = io_import_fixed(req, rw, iter);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\treturn NULL;\n\t}\n\n\t/* buffer index only valid with fixed read/write, or buffer select  */\n\tif (unlikely(req->buf_index && !(req->flags & REQ_F_BUFFER_SELECT)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbuf = u64_to_user_ptr(req->rw.addr);\n\tsqe_len = req->rw.len;\n\n\tif (opcode == IORING_OP_READ || opcode == IORING_OP_WRITE) {\n\t\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\t\tbuf = io_rw_buffer_select(req, &sqe_len, issue_flags);\n\t\t\tif (IS_ERR(buf))\n\t\t\t\treturn ERR_CAST(buf);\n\t\t\treq->rw.len = sqe_len;\n\t\t}\n\n\t\tret = import_single_range(rw, buf, sqe_len, s->fast_iov, iter);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\treturn NULL;\n\t}\n\n\tiovec = s->fast_iov;\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tret = io_iov_buffer_select(req, iovec, issue_flags);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\tiov_iter_init(iter, rw, iovec, 1, iovec->iov_len);\n\t\treturn NULL;\n\t}\n\n\tret = __import_iovec(rw, buf, sqe_len, UIO_FASTIOV, &iovec, iter,\n\t\t\t      req->ctx->compat);\n\tif (unlikely(ret < 0))\n\t\treturn ERR_PTR(ret);\n\treturn iovec;\n}",
        "code_after_change": "static struct iovec *__io_import_iovec(int rw, struct io_kiocb *req,\n\t\t\t\t       struct io_rw_state *s,\n\t\t\t\t       unsigned int issue_flags)\n{\n\tstruct iov_iter *iter = &s->iter;\n\tu8 opcode = req->opcode;\n\tstruct iovec *iovec;\n\tvoid __user *buf;\n\tsize_t sqe_len;\n\tssize_t ret;\n\n\tif (opcode == IORING_OP_READ_FIXED || opcode == IORING_OP_WRITE_FIXED) {\n\t\tret = io_import_fixed(req, rw, iter, issue_flags);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\treturn NULL;\n\t}\n\n\t/* buffer index only valid with fixed read/write, or buffer select  */\n\tif (unlikely(req->buf_index && !(req->flags & REQ_F_BUFFER_SELECT)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbuf = u64_to_user_ptr(req->rw.addr);\n\tsqe_len = req->rw.len;\n\n\tif (opcode == IORING_OP_READ || opcode == IORING_OP_WRITE) {\n\t\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\t\tbuf = io_rw_buffer_select(req, &sqe_len, issue_flags);\n\t\t\tif (IS_ERR(buf))\n\t\t\t\treturn ERR_CAST(buf);\n\t\t\treq->rw.len = sqe_len;\n\t\t}\n\n\t\tret = import_single_range(rw, buf, sqe_len, s->fast_iov, iter);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\treturn NULL;\n\t}\n\n\tiovec = s->fast_iov;\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tret = io_iov_buffer_select(req, iovec, issue_flags);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\tiov_iter_init(iter, rw, iovec, 1, iovec->iov_len);\n\t\treturn NULL;\n\t}\n\n\tret = __import_iovec(rw, buf, sqe_len, UIO_FASTIOV, &iovec, iter,\n\t\t\t      req->ctx->compat);\n\tif (unlikely(ret < 0))\n\t\treturn ERR_PTR(ret);\n\treturn iovec;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,7 @@\n \tssize_t ret;\n \n \tif (opcode == IORING_OP_READ_FIXED || opcode == IORING_OP_WRITE_FIXED) {\n-\t\tret = io_import_fixed(req, rw, iter);\n+\t\tret = io_import_fixed(req, rw, iter, issue_flags);\n \t\tif (ret)\n \t\t\treturn ERR_PTR(ret);\n \t\treturn NULL;",
        "function_modified_lines": {
            "added": [
                "\t\tret = io_import_fixed(req, rw, iter, issue_flags);"
            ],
            "deleted": [
                "\t\tret = io_import_fixed(req, rw, iter);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring system can be exploited to achieve local privilege escalation.\n\nThe io_file_get_fixed function lacks the presence of ctx->uring_lock which can lead to a Use-After-Free vulnerability due a race condition with fixed files getting unregistered.\n\nWe recommend upgrading past commit da24142b1ef9fd5d36b76e36bab328a5b27523e8.\n\n",
        "id": 3885
    },
    {
        "cve_id": "CVE-2023-1838",
        "code_before_change": "static long vhost_net_set_backend(struct vhost_net *n, unsigned index, int fd)\n{\n\tstruct socket *sock, *oldsock;\n\tstruct vhost_virtqueue *vq;\n\tstruct vhost_net_virtqueue *nvq;\n\tstruct vhost_net_ubuf_ref *ubufs, *oldubufs = NULL;\n\tint r;\n\n\tmutex_lock(&n->dev.mutex);\n\tr = vhost_dev_check_owner(&n->dev);\n\tif (r)\n\t\tgoto err;\n\n\tif (index >= VHOST_NET_VQ_MAX) {\n\t\tr = -ENOBUFS;\n\t\tgoto err;\n\t}\n\tvq = &n->vqs[index].vq;\n\tnvq = &n->vqs[index];\n\tmutex_lock(&vq->mutex);\n\n\t/* Verify that ring has been setup correctly. */\n\tif (!vhost_vq_access_ok(vq)) {\n\t\tr = -EFAULT;\n\t\tgoto err_vq;\n\t}\n\tsock = get_socket(fd);\n\tif (IS_ERR(sock)) {\n\t\tr = PTR_ERR(sock);\n\t\tgoto err_vq;\n\t}\n\n\t/* start polling new socket */\n\toldsock = vhost_vq_get_backend(vq);\n\tif (sock != oldsock) {\n\t\tubufs = vhost_net_ubuf_alloc(vq,\n\t\t\t\t\t     sock && vhost_sock_zcopy(sock));\n\t\tif (IS_ERR(ubufs)) {\n\t\t\tr = PTR_ERR(ubufs);\n\t\t\tgoto err_ubufs;\n\t\t}\n\n\t\tvhost_net_disable_vq(n, vq);\n\t\tvhost_vq_set_backend(vq, sock);\n\t\tvhost_net_buf_unproduce(nvq);\n\t\tr = vhost_vq_init_access(vq);\n\t\tif (r)\n\t\t\tgoto err_used;\n\t\tr = vhost_net_enable_vq(n, vq);\n\t\tif (r)\n\t\t\tgoto err_used;\n\t\tif (index == VHOST_NET_VQ_RX)\n\t\t\tnvq->rx_ring = get_tap_ptr_ring(fd);\n\n\t\toldubufs = nvq->ubufs;\n\t\tnvq->ubufs = ubufs;\n\n\t\tn->tx_packets = 0;\n\t\tn->tx_zcopy_err = 0;\n\t\tn->tx_flush = false;\n\t}\n\n\tmutex_unlock(&vq->mutex);\n\n\tif (oldubufs) {\n\t\tvhost_net_ubuf_put_wait_and_free(oldubufs);\n\t\tmutex_lock(&vq->mutex);\n\t\tvhost_zerocopy_signal_used(n, vq);\n\t\tmutex_unlock(&vq->mutex);\n\t}\n\n\tif (oldsock) {\n\t\tvhost_net_flush_vq(n, index);\n\t\tsockfd_put(oldsock);\n\t}\n\n\tmutex_unlock(&n->dev.mutex);\n\treturn 0;\n\nerr_used:\n\tvhost_vq_set_backend(vq, oldsock);\n\tvhost_net_enable_vq(n, vq);\n\tif (ubufs)\n\t\tvhost_net_ubuf_put_wait_and_free(ubufs);\nerr_ubufs:\n\tif (sock)\n\t\tsockfd_put(sock);\nerr_vq:\n\tmutex_unlock(&vq->mutex);\nerr:\n\tmutex_unlock(&n->dev.mutex);\n\treturn r;\n}",
        "code_after_change": "static long vhost_net_set_backend(struct vhost_net *n, unsigned index, int fd)\n{\n\tstruct socket *sock, *oldsock;\n\tstruct vhost_virtqueue *vq;\n\tstruct vhost_net_virtqueue *nvq;\n\tstruct vhost_net_ubuf_ref *ubufs, *oldubufs = NULL;\n\tint r;\n\n\tmutex_lock(&n->dev.mutex);\n\tr = vhost_dev_check_owner(&n->dev);\n\tif (r)\n\t\tgoto err;\n\n\tif (index >= VHOST_NET_VQ_MAX) {\n\t\tr = -ENOBUFS;\n\t\tgoto err;\n\t}\n\tvq = &n->vqs[index].vq;\n\tnvq = &n->vqs[index];\n\tmutex_lock(&vq->mutex);\n\n\t/* Verify that ring has been setup correctly. */\n\tif (!vhost_vq_access_ok(vq)) {\n\t\tr = -EFAULT;\n\t\tgoto err_vq;\n\t}\n\tsock = get_socket(fd);\n\tif (IS_ERR(sock)) {\n\t\tr = PTR_ERR(sock);\n\t\tgoto err_vq;\n\t}\n\n\t/* start polling new socket */\n\toldsock = vhost_vq_get_backend(vq);\n\tif (sock != oldsock) {\n\t\tubufs = vhost_net_ubuf_alloc(vq,\n\t\t\t\t\t     sock && vhost_sock_zcopy(sock));\n\t\tif (IS_ERR(ubufs)) {\n\t\t\tr = PTR_ERR(ubufs);\n\t\t\tgoto err_ubufs;\n\t\t}\n\n\t\tvhost_net_disable_vq(n, vq);\n\t\tvhost_vq_set_backend(vq, sock);\n\t\tvhost_net_buf_unproduce(nvq);\n\t\tr = vhost_vq_init_access(vq);\n\t\tif (r)\n\t\t\tgoto err_used;\n\t\tr = vhost_net_enable_vq(n, vq);\n\t\tif (r)\n\t\t\tgoto err_used;\n\t\tif (index == VHOST_NET_VQ_RX) {\n\t\t\tif (sock)\n\t\t\t\tnvq->rx_ring = get_tap_ptr_ring(sock->file);\n\t\t\telse\n\t\t\t\tnvq->rx_ring = NULL;\n\t\t}\n\n\t\toldubufs = nvq->ubufs;\n\t\tnvq->ubufs = ubufs;\n\n\t\tn->tx_packets = 0;\n\t\tn->tx_zcopy_err = 0;\n\t\tn->tx_flush = false;\n\t}\n\n\tmutex_unlock(&vq->mutex);\n\n\tif (oldubufs) {\n\t\tvhost_net_ubuf_put_wait_and_free(oldubufs);\n\t\tmutex_lock(&vq->mutex);\n\t\tvhost_zerocopy_signal_used(n, vq);\n\t\tmutex_unlock(&vq->mutex);\n\t}\n\n\tif (oldsock) {\n\t\tvhost_net_flush_vq(n, index);\n\t\tsockfd_put(oldsock);\n\t}\n\n\tmutex_unlock(&n->dev.mutex);\n\treturn 0;\n\nerr_used:\n\tvhost_vq_set_backend(vq, oldsock);\n\tvhost_net_enable_vq(n, vq);\n\tif (ubufs)\n\t\tvhost_net_ubuf_put_wait_and_free(ubufs);\nerr_ubufs:\n\tif (sock)\n\t\tsockfd_put(sock);\nerr_vq:\n\tmutex_unlock(&vq->mutex);\nerr:\n\tmutex_unlock(&n->dev.mutex);\n\treturn r;\n}",
        "patch": "--- code before\n+++ code after\n@@ -49,8 +49,12 @@\n \t\tr = vhost_net_enable_vq(n, vq);\n \t\tif (r)\n \t\t\tgoto err_used;\n-\t\tif (index == VHOST_NET_VQ_RX)\n-\t\t\tnvq->rx_ring = get_tap_ptr_ring(fd);\n+\t\tif (index == VHOST_NET_VQ_RX) {\n+\t\t\tif (sock)\n+\t\t\t\tnvq->rx_ring = get_tap_ptr_ring(sock->file);\n+\t\t\telse\n+\t\t\t\tnvq->rx_ring = NULL;\n+\t\t}\n \n \t\toldubufs = nvq->ubufs;\n \t\tnvq->ubufs = ubufs;",
        "function_modified_lines": {
            "added": [
                "\t\tif (index == VHOST_NET_VQ_RX) {",
                "\t\t\tif (sock)",
                "\t\t\t\tnvq->rx_ring = get_tap_ptr_ring(sock->file);",
                "\t\t\telse",
                "\t\t\t\tnvq->rx_ring = NULL;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tif (index == VHOST_NET_VQ_RX)",
                "\t\t\tnvq->rx_ring = get_tap_ptr_ring(fd);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in vhost_net_set_backend in drivers/vhost/net.c in virtio network subcomponent in the Linux kernel due to a double fget. This flaw could allow a local attacker to crash the system, and could even lead to a kernel information leak problem.",
        "id": 3879
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "code_after_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tstruct ipv6_pinfo *np = inet6_sk(sk);\n \tstruct tcp_sock *tp = tcp_sk(sk);\n \tstruct in6_addr *saddr = NULL, *final_p, final;\n+\tstruct ipv6_txoptions *opt;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n \tint addr_type;\n@@ -122,7 +123,8 @@\n \tfl6.fl6_dport = usin->sin6_port;\n \tfl6.fl6_sport = inet->inet_sport;\n \n-\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n \n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n@@ -150,9 +152,9 @@\n \t\ttcp_fetch_timewait_stamp(sk, dst);\n \n \ticsk->icsk_ext_hdr_len = 0;\n-\tif (np->opt)\n-\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n-\t\t\t\t\t  np->opt->opt_nflen);\n+\tif (opt)\n+\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n+\t\t\t\t\t opt->opt_nflen;\n \n \ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt;",
                "\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\tfinal_p = fl6_update_dst(&fl6, opt, &final);",
                "\tif (opt)",
                "\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +",
                "\t\t\t\t\t opt->opt_nflen;"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);",
                "\tif (np->opt)",
                "\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +",
                "\t\t\t\t\t  np->opt->opt_nflen);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1007
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_cotable_resize(struct vmw_resource *res, size_t new_size)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tstruct vmw_cotable *vcotbl = vmw_cotable(res);\n\tstruct vmw_bo *buf, *old_buf = res->guest_memory_bo;\n\tstruct ttm_buffer_object *bo, *old_bo = &res->guest_memory_bo->tbo;\n\tsize_t old_size = res->guest_memory_size;\n\tsize_t old_size_read_back = vcotbl->size_read_back;\n\tsize_t cur_size_read_back;\n\tstruct ttm_bo_kmap_obj old_map, new_map;\n\tint ret;\n\tsize_t i;\n\tstruct vmw_bo_params bo_params = {\n\t\t.domain = VMW_BO_DOMAIN_MOB,\n\t\t.busy_domain = VMW_BO_DOMAIN_MOB,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = new_size,\n\t\t.pin = true\n\t};\n\n\tMKS_STAT_TIME_DECL(MKSSTAT_KERN_COTABLE_RESIZE);\n\tMKS_STAT_TIME_PUSH(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\tret = vmw_cotable_readback(res);\n\tif (ret)\n\t\tgoto out_done;\n\n\tcur_size_read_back = vcotbl->size_read_back;\n\tvcotbl->size_read_back = old_size_read_back;\n\n\t/*\n\t * While device is processing, Allocate and reserve a buffer object\n\t * for the new COTable. Initially pin the buffer object to make sure\n\t * we can use tryreserve without failure.\n\t */\n\tret = vmw_bo_create(dev_priv, &bo_params, &buf);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed initializing new cotable MOB.\\n\");\n\t\tgoto out_done;\n\t}\n\n\tbo = &buf->tbo;\n\tWARN_ON_ONCE(ttm_bo_reserve(bo, false, true, NULL));\n\n\tret = ttm_bo_wait(old_bo, false, false);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed waiting for cotable unbind.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\t/*\n\t * Do a page by page copy of COTables. This eliminates slow vmap()s.\n\t * This should really be a TTM utility.\n\t */\n\tfor (i = 0; i < PFN_UP(old_bo->resource->size); ++i) {\n\t\tbool dummy;\n\n\t\tret = ttm_bo_kmap(old_bo, i, 1, &old_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping old COTable on resize.\\n\");\n\t\t\tgoto out_wait;\n\t\t}\n\t\tret = ttm_bo_kmap(bo, i, 1, &new_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping new COTable on resize.\\n\");\n\t\t\tgoto out_map_new;\n\t\t}\n\t\tmemcpy(ttm_kmap_obj_virtual(&new_map, &dummy),\n\t\t       ttm_kmap_obj_virtual(&old_map, &dummy),\n\t\t       PAGE_SIZE);\n\t\tttm_bo_kunmap(&new_map);\n\t\tttm_bo_kunmap(&old_map);\n\t}\n\n\t/* Unpin new buffer, and switch backup buffers. */\n\tvmw_bo_placement_set(buf,\n\t\t\t     VMW_BO_DOMAIN_MOB,\n\t\t\t     VMW_BO_DOMAIN_MOB);\n\tret = ttm_bo_validate(bo, &buf->placement, &ctx);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed validating new COTable backup buffer.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_detach(res);\n\tres->guest_memory_bo = buf;\n\tres->guest_memory_size = new_size;\n\tvcotbl->size_read_back = cur_size_read_back;\n\n\t/*\n\t * Now tell the device to switch. If this fails, then we need to\n\t * revert the full resize.\n\t */\n\tret = vmw_cotable_unscrub(res);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed switching COTable backup buffer.\\n\");\n\t\tres->guest_memory_bo = old_buf;\n\t\tres->guest_memory_size = old_size;\n\t\tvcotbl->size_read_back = old_size_read_back;\n\t\tvmw_resource_mob_attach(res);\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_attach(res);\n\t/* Let go of the old mob. */\n\tvmw_bo_unreference(&old_buf);\n\tres->id = vcotbl->type;\n\n\tret = dma_resv_reserve_fences(bo->base.resv, 1);\n\tif (unlikely(ret))\n\t\tgoto out_wait;\n\n\t/* Release the pin acquired in vmw_bo_create */\n\tttm_bo_unpin(bo);\n\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn 0;\n\nout_map_new:\n\tttm_bo_kunmap(&old_map);\nout_wait:\n\tttm_bo_unpin(bo);\n\tttm_bo_unreserve(bo);\n\tvmw_bo_unreference(&buf);\n\nout_done:\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn ret;\n}",
        "code_after_change": "static int vmw_cotable_resize(struct vmw_resource *res, size_t new_size)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tstruct vmw_cotable *vcotbl = vmw_cotable(res);\n\tstruct vmw_bo *buf, *old_buf = res->guest_memory_bo;\n\tstruct ttm_buffer_object *bo, *old_bo = &res->guest_memory_bo->tbo;\n\tsize_t old_size = res->guest_memory_size;\n\tsize_t old_size_read_back = vcotbl->size_read_back;\n\tsize_t cur_size_read_back;\n\tstruct ttm_bo_kmap_obj old_map, new_map;\n\tint ret;\n\tsize_t i;\n\tstruct vmw_bo_params bo_params = {\n\t\t.domain = VMW_BO_DOMAIN_MOB,\n\t\t.busy_domain = VMW_BO_DOMAIN_MOB,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = new_size,\n\t\t.pin = true\n\t};\n\n\tMKS_STAT_TIME_DECL(MKSSTAT_KERN_COTABLE_RESIZE);\n\tMKS_STAT_TIME_PUSH(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\tret = vmw_cotable_readback(res);\n\tif (ret)\n\t\tgoto out_done;\n\n\tcur_size_read_back = vcotbl->size_read_back;\n\tvcotbl->size_read_back = old_size_read_back;\n\n\t/*\n\t * While device is processing, Allocate and reserve a buffer object\n\t * for the new COTable. Initially pin the buffer object to make sure\n\t * we can use tryreserve without failure.\n\t */\n\tret = vmw_gem_object_create(dev_priv, &bo_params, &buf);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed initializing new cotable MOB.\\n\");\n\t\tgoto out_done;\n\t}\n\n\tbo = &buf->tbo;\n\tWARN_ON_ONCE(ttm_bo_reserve(bo, false, true, NULL));\n\n\tret = ttm_bo_wait(old_bo, false, false);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed waiting for cotable unbind.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\t/*\n\t * Do a page by page copy of COTables. This eliminates slow vmap()s.\n\t * This should really be a TTM utility.\n\t */\n\tfor (i = 0; i < PFN_UP(old_bo->resource->size); ++i) {\n\t\tbool dummy;\n\n\t\tret = ttm_bo_kmap(old_bo, i, 1, &old_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping old COTable on resize.\\n\");\n\t\t\tgoto out_wait;\n\t\t}\n\t\tret = ttm_bo_kmap(bo, i, 1, &new_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping new COTable on resize.\\n\");\n\t\t\tgoto out_map_new;\n\t\t}\n\t\tmemcpy(ttm_kmap_obj_virtual(&new_map, &dummy),\n\t\t       ttm_kmap_obj_virtual(&old_map, &dummy),\n\t\t       PAGE_SIZE);\n\t\tttm_bo_kunmap(&new_map);\n\t\tttm_bo_kunmap(&old_map);\n\t}\n\n\t/* Unpin new buffer, and switch backup buffers. */\n\tvmw_bo_placement_set(buf,\n\t\t\t     VMW_BO_DOMAIN_MOB,\n\t\t\t     VMW_BO_DOMAIN_MOB);\n\tret = ttm_bo_validate(bo, &buf->placement, &ctx);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed validating new COTable backup buffer.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_detach(res);\n\tres->guest_memory_bo = buf;\n\tres->guest_memory_size = new_size;\n\tvcotbl->size_read_back = cur_size_read_back;\n\n\t/*\n\t * Now tell the device to switch. If this fails, then we need to\n\t * revert the full resize.\n\t */\n\tret = vmw_cotable_unscrub(res);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed switching COTable backup buffer.\\n\");\n\t\tres->guest_memory_bo = old_buf;\n\t\tres->guest_memory_size = old_size;\n\t\tvcotbl->size_read_back = old_size_read_back;\n\t\tvmw_resource_mob_attach(res);\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_attach(res);\n\t/* Let go of the old mob. */\n\tvmw_user_bo_unref(&old_buf);\n\tres->id = vcotbl->type;\n\n\tret = dma_resv_reserve_fences(bo->base.resv, 1);\n\tif (unlikely(ret))\n\t\tgoto out_wait;\n\n\t/* Release the pin acquired in vmw_bo_create */\n\tttm_bo_unpin(bo);\n\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn 0;\n\nout_map_new:\n\tttm_bo_kunmap(&old_map);\nout_wait:\n\tttm_bo_unpin(bo);\n\tttm_bo_unreserve(bo);\n\tvmw_user_bo_unref(&buf);\n\nout_done:\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -34,7 +34,7 @@\n \t * for the new COTable. Initially pin the buffer object to make sure\n \t * we can use tryreserve without failure.\n \t */\n-\tret = vmw_bo_create(dev_priv, &bo_params, &buf);\n+\tret = vmw_gem_object_create(dev_priv, &bo_params, &buf);\n \tif (ret) {\n \t\tDRM_ERROR(\"Failed initializing new cotable MOB.\\n\");\n \t\tgoto out_done;\n@@ -104,7 +104,7 @@\n \n \tvmw_resource_mob_attach(res);\n \t/* Let go of the old mob. */\n-\tvmw_bo_unreference(&old_buf);\n+\tvmw_user_bo_unref(&old_buf);\n \tres->id = vcotbl->type;\n \n \tret = dma_resv_reserve_fences(bo->base.resv, 1);\n@@ -123,7 +123,7 @@\n out_wait:\n \tttm_bo_unpin(bo);\n \tttm_bo_unreserve(bo);\n-\tvmw_bo_unreference(&buf);\n+\tvmw_user_bo_unref(&buf);\n \n out_done:\n \tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);",
        "function_modified_lines": {
            "added": [
                "\tret = vmw_gem_object_create(dev_priv, &bo_params, &buf);",
                "\tvmw_user_bo_unref(&old_buf);",
                "\tvmw_user_bo_unref(&buf);"
            ],
            "deleted": [
                "\tret = vmw_bo_create(dev_priv, &bo_params, &buf);",
                "\tvmw_bo_unreference(&old_buf);",
                "\tvmw_bo_unreference(&buf);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4271
    },
    {
        "cve_id": "CVE-2018-10902",
        "code_before_change": "int snd_rawmidi_input_params(struct snd_rawmidi_substream *substream,\n\t\t\t     struct snd_rawmidi_params * params)\n{\n\tchar *newbuf;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\n\tsnd_rawmidi_drain_input(substream);\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = krealloc(runtime->buffer, params->buffer_size,\n\t\t\t\t  GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t}\n\truntime->avail_min = params->avail_min;\n\treturn 0;\n}",
        "code_after_change": "int snd_rawmidi_input_params(struct snd_rawmidi_substream *substream,\n\t\t\t     struct snd_rawmidi_params * params)\n{\n\tchar *newbuf, *oldbuf;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\n\tsnd_rawmidi_drain_input(substream);\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = kmalloc(params->buffer_size, GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\tspin_lock_irq(&runtime->lock);\n\t\toldbuf = runtime->buffer;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t\truntime->appl_ptr = runtime->hw_ptr = 0;\n\t\tspin_unlock_irq(&runtime->lock);\n\t\tkfree(oldbuf);\n\t}\n\truntime->avail_min = params->avail_min;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n int snd_rawmidi_input_params(struct snd_rawmidi_substream *substream,\n \t\t\t     struct snd_rawmidi_params * params)\n {\n-\tchar *newbuf;\n+\tchar *newbuf, *oldbuf;\n \tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n \n \tsnd_rawmidi_drain_input(substream);\n@@ -12,12 +12,16 @@\n \t\treturn -EINVAL;\n \t}\n \tif (params->buffer_size != runtime->buffer_size) {\n-\t\tnewbuf = krealloc(runtime->buffer, params->buffer_size,\n-\t\t\t\t  GFP_KERNEL);\n+\t\tnewbuf = kmalloc(params->buffer_size, GFP_KERNEL);\n \t\tif (!newbuf)\n \t\t\treturn -ENOMEM;\n+\t\tspin_lock_irq(&runtime->lock);\n+\t\toldbuf = runtime->buffer;\n \t\truntime->buffer = newbuf;\n \t\truntime->buffer_size = params->buffer_size;\n+\t\truntime->appl_ptr = runtime->hw_ptr = 0;\n+\t\tspin_unlock_irq(&runtime->lock);\n+\t\tkfree(oldbuf);\n \t}\n \truntime->avail_min = params->avail_min;\n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tchar *newbuf, *oldbuf;",
                "\t\tnewbuf = kmalloc(params->buffer_size, GFP_KERNEL);",
                "\t\tspin_lock_irq(&runtime->lock);",
                "\t\toldbuf = runtime->buffer;",
                "\t\truntime->appl_ptr = runtime->hw_ptr = 0;",
                "\t\tspin_unlock_irq(&runtime->lock);",
                "\t\tkfree(oldbuf);"
            ],
            "deleted": [
                "\tchar *newbuf;",
                "\t\tnewbuf = krealloc(runtime->buffer, params->buffer_size,",
                "\t\t\t\t  GFP_KERNEL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "It was found that the raw midi kernel driver does not protect against concurrent access which leads to a double realloc (double free) in snd_rawmidi_input_params() and snd_rawmidi_output_status() which are part of snd_rawmidi_ioctl() handler in rawmidi.c file. A malicious local attacker could possibly use this for privilege escalation.",
        "id": 1621
    },
    {
        "cve_id": "CVE-2023-0266",
        "code_before_change": "static int snd_ctl_elem_read_user(struct snd_card *card,\n\t\t\t\t  struct snd_ctl_elem_value __user *_control)\n{\n\tstruct snd_ctl_elem_value *control;\n\tint result;\n\n\tcontrol = memdup_user(_control, sizeof(*control));\n\tif (IS_ERR(control))\n\t\treturn PTR_ERR(control);\n\n\tdown_read(&card->controls_rwsem);\n\tresult = snd_ctl_elem_read(card, control);\n\tup_read(&card->controls_rwsem);\n\tif (result < 0)\n\t\tgoto error;\n\n\tif (copy_to_user(_control, control, sizeof(*control)))\n\t\tresult = -EFAULT;\n error:\n\tkfree(control);\n\treturn result;\n}",
        "code_after_change": "static int snd_ctl_elem_read_user(struct snd_card *card,\n\t\t\t\t  struct snd_ctl_elem_value __user *_control)\n{\n\tstruct snd_ctl_elem_value *control;\n\tint result;\n\n\tcontrol = memdup_user(_control, sizeof(*control));\n\tif (IS_ERR(control))\n\t\treturn PTR_ERR(control);\n\n\tresult = snd_ctl_elem_read(card, control);\n\tif (result < 0)\n\t\tgoto error;\n\n\tif (copy_to_user(_control, control, sizeof(*control)))\n\t\tresult = -EFAULT;\n error:\n\tkfree(control);\n\treturn result;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,9 +8,7 @@\n \tif (IS_ERR(control))\n \t\treturn PTR_ERR(control);\n \n-\tdown_read(&card->controls_rwsem);\n \tresult = snd_ctl_elem_read(card, control);\n-\tup_read(&card->controls_rwsem);\n \tif (result < 0)\n \t\tgoto error;\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tdown_read(&card->controls_rwsem);",
                "\tup_read(&card->controls_rwsem);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free vulnerability exists in the ALSA PCM package in the Linux Kernel. SNDRV_CTL_IOCTL_ELEM_{READ|WRITE}32 is missing locks that can be used in a use-after-free that can result in a priviledge escalation to gain ring0 access from the system user. We recommend upgrading past commit 56b88b50565cd8b946a2d00b0c83927b7ebb055e\n",
        "id": 3822
    },
    {
        "cve_id": "CVE-2019-11487",
        "code_before_change": "static ssize_t fuse_dev_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t     struct file *out, loff_t *ppos,\n\t\t\t\t     size_t len, unsigned int flags)\n{\n\tunsigned nbuf;\n\tunsigned idx;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud;\n\tsize_t rem;\n\tssize_t ret;\n\n\tfud = fuse_get_dev(out);\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tpipe_lock(pipe);\n\n\tbufs = kvmalloc_array(pipe->nrbufs, sizeof(struct pipe_buffer),\n\t\t\t      GFP_KERNEL);\n\tif (!bufs) {\n\t\tpipe_unlock(pipe);\n\t\treturn -ENOMEM;\n\t}\n\n\tnbuf = 0;\n\trem = 0;\n\tfor (idx = 0; idx < pipe->nrbufs && rem < len; idx++)\n\t\trem += pipe->bufs[(pipe->curbuf + idx) & (pipe->buffers - 1)].len;\n\n\tret = -EINVAL;\n\tif (rem < len) {\n\t\tpipe_unlock(pipe);\n\t\tgoto out;\n\t}\n\n\trem = len;\n\twhile (rem) {\n\t\tstruct pipe_buffer *ibuf;\n\t\tstruct pipe_buffer *obuf;\n\n\t\tBUG_ON(nbuf >= pipe->buffers);\n\t\tBUG_ON(!pipe->nrbufs);\n\t\tibuf = &pipe->bufs[pipe->curbuf];\n\t\tobuf = &bufs[nbuf];\n\n\t\tif (rem >= ibuf->len) {\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\tpipe->nrbufs--;\n\t\t} else {\n\t\t\tpipe_buf_get(pipe, ibuf);\n\t\t\t*obuf = *ibuf;\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\t\t\tobuf->len = rem;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tnbuf++;\n\t\trem -= obuf->len;\n\t}\n\tpipe_unlock(pipe);\n\n\tfuse_copy_init(&cs, 0, NULL);\n\tcs.pipebufs = bufs;\n\tcs.nr_segs = nbuf;\n\tcs.pipe = pipe;\n\n\tif (flags & SPLICE_F_MOVE)\n\t\tcs.move_pages = 1;\n\n\tret = fuse_dev_do_write(fud, &cs, len);\n\n\tpipe_lock(pipe);\n\tfor (idx = 0; idx < nbuf; idx++)\n\t\tpipe_buf_release(pipe, &bufs[idx]);\n\tpipe_unlock(pipe);\n\nout:\n\tkvfree(bufs);\n\treturn ret;\n}",
        "code_after_change": "static ssize_t fuse_dev_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t     struct file *out, loff_t *ppos,\n\t\t\t\t     size_t len, unsigned int flags)\n{\n\tunsigned nbuf;\n\tunsigned idx;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud;\n\tsize_t rem;\n\tssize_t ret;\n\n\tfud = fuse_get_dev(out);\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tpipe_lock(pipe);\n\n\tbufs = kvmalloc_array(pipe->nrbufs, sizeof(struct pipe_buffer),\n\t\t\t      GFP_KERNEL);\n\tif (!bufs) {\n\t\tpipe_unlock(pipe);\n\t\treturn -ENOMEM;\n\t}\n\n\tnbuf = 0;\n\trem = 0;\n\tfor (idx = 0; idx < pipe->nrbufs && rem < len; idx++)\n\t\trem += pipe->bufs[(pipe->curbuf + idx) & (pipe->buffers - 1)].len;\n\n\tret = -EINVAL;\n\tif (rem < len)\n\t\tgoto out_free;\n\n\trem = len;\n\twhile (rem) {\n\t\tstruct pipe_buffer *ibuf;\n\t\tstruct pipe_buffer *obuf;\n\n\t\tBUG_ON(nbuf >= pipe->buffers);\n\t\tBUG_ON(!pipe->nrbufs);\n\t\tibuf = &pipe->bufs[pipe->curbuf];\n\t\tobuf = &bufs[nbuf];\n\n\t\tif (rem >= ibuf->len) {\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\tpipe->nrbufs--;\n\t\t} else {\n\t\t\tif (!pipe_buf_get(pipe, ibuf))\n\t\t\t\tgoto out_free;\n\n\t\t\t*obuf = *ibuf;\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\t\t\tobuf->len = rem;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tnbuf++;\n\t\trem -= obuf->len;\n\t}\n\tpipe_unlock(pipe);\n\n\tfuse_copy_init(&cs, 0, NULL);\n\tcs.pipebufs = bufs;\n\tcs.nr_segs = nbuf;\n\tcs.pipe = pipe;\n\n\tif (flags & SPLICE_F_MOVE)\n\t\tcs.move_pages = 1;\n\n\tret = fuse_dev_do_write(fud, &cs, len);\n\n\tpipe_lock(pipe);\nout_free:\n\tfor (idx = 0; idx < nbuf; idx++)\n\t\tpipe_buf_release(pipe, &bufs[idx]);\n\tpipe_unlock(pipe);\n\n\tkvfree(bufs);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,10 +29,8 @@\n \t\trem += pipe->bufs[(pipe->curbuf + idx) & (pipe->buffers - 1)].len;\n \n \tret = -EINVAL;\n-\tif (rem < len) {\n-\t\tpipe_unlock(pipe);\n-\t\tgoto out;\n-\t}\n+\tif (rem < len)\n+\t\tgoto out_free;\n \n \trem = len;\n \twhile (rem) {\n@@ -50,7 +48,9 @@\n \t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n \t\t\tpipe->nrbufs--;\n \t\t} else {\n-\t\t\tpipe_buf_get(pipe, ibuf);\n+\t\t\tif (!pipe_buf_get(pipe, ibuf))\n+\t\t\t\tgoto out_free;\n+\n \t\t\t*obuf = *ibuf;\n \t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n \t\t\tobuf->len = rem;\n@@ -73,11 +73,11 @@\n \tret = fuse_dev_do_write(fud, &cs, len);\n \n \tpipe_lock(pipe);\n+out_free:\n \tfor (idx = 0; idx < nbuf; idx++)\n \t\tpipe_buf_release(pipe, &bufs[idx]);\n \tpipe_unlock(pipe);\n \n-out:\n \tkvfree(bufs);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (rem < len)",
                "\t\tgoto out_free;",
                "\t\t\tif (!pipe_buf_get(pipe, ibuf))",
                "\t\t\t\tgoto out_free;",
                "",
                "out_free:"
            ],
            "deleted": [
                "\tif (rem < len) {",
                "\t\tpipe_unlock(pipe);",
                "\t\tgoto out;",
                "\t}",
                "\t\t\tpipe_buf_get(pipe, ibuf);",
                "out:"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 5.1-rc5 allows page->_refcount reference count overflow, with resultant use-after-free issues, if about 140 GiB of RAM exists. This is related to fs/fuse/dev.c, fs/pipe.c, fs/splice.c, include/linux/mm.h, include/linux/pipe_fs_i.h, kernel/trace/trace.c, mm/gup.c, and mm/hugetlb.c. It can occur with FUSE requests.",
        "id": 1917
    },
    {
        "cve_id": "CVE-2022-1280",
        "code_before_change": "int drm_mode_getconnector(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *file_priv)\n{\n\tstruct drm_mode_get_connector *out_resp = data;\n\tstruct drm_connector *connector;\n\tstruct drm_encoder *encoder;\n\tstruct drm_display_mode *mode;\n\tint mode_count = 0;\n\tint encoders_count = 0;\n\tint ret = 0;\n\tint copied = 0;\n\tstruct drm_mode_modeinfo u_mode;\n\tstruct drm_mode_modeinfo __user *mode_ptr;\n\tuint32_t __user *encoder_ptr;\n\n\tif (!drm_core_check_feature(dev, DRIVER_MODESET))\n\t\treturn -EOPNOTSUPP;\n\n\tmemset(&u_mode, 0, sizeof(struct drm_mode_modeinfo));\n\n\tconnector = drm_connector_lookup(dev, file_priv, out_resp->connector_id);\n\tif (!connector)\n\t\treturn -ENOENT;\n\n\tencoders_count = hweight32(connector->possible_encoders);\n\n\tif ((out_resp->count_encoders >= encoders_count) && encoders_count) {\n\t\tcopied = 0;\n\t\tencoder_ptr = (uint32_t __user *)(unsigned long)(out_resp->encoders_ptr);\n\n\t\tdrm_connector_for_each_possible_encoder(connector, encoder) {\n\t\t\tif (put_user(encoder->base.id, encoder_ptr + copied)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcopied++;\n\t\t}\n\t}\n\tout_resp->count_encoders = encoders_count;\n\n\tout_resp->connector_id = connector->base.id;\n\tout_resp->connector_type = connector->connector_type;\n\tout_resp->connector_type_id = connector->connector_type_id;\n\n\tmutex_lock(&dev->mode_config.mutex);\n\tif (out_resp->count_modes == 0) {\n\t\tif (drm_is_current_master(file_priv))\n\t\t\tconnector->funcs->fill_modes(connector,\n\t\t\t\t\t\t     dev->mode_config.max_width,\n\t\t\t\t\t\t     dev->mode_config.max_height);\n\t\telse\n\t\t\tdrm_dbg_kms(dev, \"User-space requested a forced probe on [CONNECTOR:%d:%s] but is not the DRM master, demoting to read-only probe\",\n\t\t\t\t    connector->base.id, connector->name);\n\t}\n\n\tout_resp->mm_width = connector->display_info.width_mm;\n\tout_resp->mm_height = connector->display_info.height_mm;\n\tout_resp->subpixel = connector->display_info.subpixel_order;\n\tout_resp->connection = connector->status;\n\n\t/* delayed so we get modes regardless of pre-fill_modes state */\n\tlist_for_each_entry(mode, &connector->modes, head) {\n\t\tWARN_ON(mode->expose_to_userspace);\n\n\t\tif (drm_mode_expose_to_userspace(mode, &connector->modes,\n\t\t\t\t\t\t file_priv)) {\n\t\t\tmode->expose_to_userspace = true;\n\t\t\tmode_count++;\n\t\t}\n\t}\n\n\t/*\n\t * This ioctl is called twice, once to determine how much space is\n\t * needed, and the 2nd time to fill it.\n\t */\n\tif ((out_resp->count_modes >= mode_count) && mode_count) {\n\t\tcopied = 0;\n\t\tmode_ptr = (struct drm_mode_modeinfo __user *)(unsigned long)out_resp->modes_ptr;\n\t\tlist_for_each_entry(mode, &connector->modes, head) {\n\t\t\tif (!mode->expose_to_userspace)\n\t\t\t\tcontinue;\n\n\t\t\t/* Clear the tag for the next time around */\n\t\t\tmode->expose_to_userspace = false;\n\n\t\t\tdrm_mode_convert_to_umode(&u_mode, mode);\n\t\t\t/*\n\t\t\t * Reset aspect ratio flags of user-mode, if modes with\n\t\t\t * aspect-ratio are not supported.\n\t\t\t */\n\t\t\tif (!file_priv->aspect_ratio_allowed)\n\t\t\t\tu_mode.flags &= ~DRM_MODE_FLAG_PIC_AR_MASK;\n\t\t\tif (copy_to_user(mode_ptr + copied,\n\t\t\t\t\t &u_mode, sizeof(u_mode))) {\n\t\t\t\tret = -EFAULT;\n\n\t\t\t\t/*\n\t\t\t\t * Clear the tag for the rest of\n\t\t\t\t * the modes for the next time around.\n\t\t\t\t */\n\t\t\t\tlist_for_each_entry_continue(mode, &connector->modes, head)\n\t\t\t\t\tmode->expose_to_userspace = false;\n\n\t\t\t\tmutex_unlock(&dev->mode_config.mutex);\n\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcopied++;\n\t\t}\n\t} else {\n\t\t/* Clear the tag for the next time around */\n\t\tlist_for_each_entry(mode, &connector->modes, head)\n\t\t\tmode->expose_to_userspace = false;\n\t}\n\n\tout_resp->count_modes = mode_count;\n\tmutex_unlock(&dev->mode_config.mutex);\n\n\tdrm_modeset_lock(&dev->mode_config.connection_mutex, NULL);\n\tencoder = drm_connector_get_encoder(connector);\n\tif (encoder)\n\t\tout_resp->encoder_id = encoder->base.id;\n\telse\n\t\tout_resp->encoder_id = 0;\n\n\t/* Only grab properties after probing, to make sure EDID and other\n\t * properties reflect the latest status.\n\t */\n\tret = drm_mode_object_get_properties(&connector->base, file_priv->atomic,\n\t\t\t(uint32_t __user *)(unsigned long)(out_resp->props_ptr),\n\t\t\t(uint64_t __user *)(unsigned long)(out_resp->prop_values_ptr),\n\t\t\t&out_resp->count_props);\n\tdrm_modeset_unlock(&dev->mode_config.connection_mutex);\n\nout:\n\tdrm_connector_put(connector);\n\n\treturn ret;\n}",
        "code_after_change": "int drm_mode_getconnector(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *file_priv)\n{\n\tstruct drm_mode_get_connector *out_resp = data;\n\tstruct drm_connector *connector;\n\tstruct drm_encoder *encoder;\n\tstruct drm_display_mode *mode;\n\tint mode_count = 0;\n\tint encoders_count = 0;\n\tint ret = 0;\n\tint copied = 0;\n\tstruct drm_mode_modeinfo u_mode;\n\tstruct drm_mode_modeinfo __user *mode_ptr;\n\tuint32_t __user *encoder_ptr;\n\tbool is_current_master;\n\n\tif (!drm_core_check_feature(dev, DRIVER_MODESET))\n\t\treturn -EOPNOTSUPP;\n\n\tmemset(&u_mode, 0, sizeof(struct drm_mode_modeinfo));\n\n\tconnector = drm_connector_lookup(dev, file_priv, out_resp->connector_id);\n\tif (!connector)\n\t\treturn -ENOENT;\n\n\tencoders_count = hweight32(connector->possible_encoders);\n\n\tif ((out_resp->count_encoders >= encoders_count) && encoders_count) {\n\t\tcopied = 0;\n\t\tencoder_ptr = (uint32_t __user *)(unsigned long)(out_resp->encoders_ptr);\n\n\t\tdrm_connector_for_each_possible_encoder(connector, encoder) {\n\t\t\tif (put_user(encoder->base.id, encoder_ptr + copied)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcopied++;\n\t\t}\n\t}\n\tout_resp->count_encoders = encoders_count;\n\n\tout_resp->connector_id = connector->base.id;\n\tout_resp->connector_type = connector->connector_type;\n\tout_resp->connector_type_id = connector->connector_type_id;\n\n\tis_current_master = drm_is_current_master(file_priv);\n\n\tmutex_lock(&dev->mode_config.mutex);\n\tif (out_resp->count_modes == 0) {\n\t\tif (is_current_master)\n\t\t\tconnector->funcs->fill_modes(connector,\n\t\t\t\t\t\t     dev->mode_config.max_width,\n\t\t\t\t\t\t     dev->mode_config.max_height);\n\t\telse\n\t\t\tdrm_dbg_kms(dev, \"User-space requested a forced probe on [CONNECTOR:%d:%s] but is not the DRM master, demoting to read-only probe\",\n\t\t\t\t    connector->base.id, connector->name);\n\t}\n\n\tout_resp->mm_width = connector->display_info.width_mm;\n\tout_resp->mm_height = connector->display_info.height_mm;\n\tout_resp->subpixel = connector->display_info.subpixel_order;\n\tout_resp->connection = connector->status;\n\n\t/* delayed so we get modes regardless of pre-fill_modes state */\n\tlist_for_each_entry(mode, &connector->modes, head) {\n\t\tWARN_ON(mode->expose_to_userspace);\n\n\t\tif (drm_mode_expose_to_userspace(mode, &connector->modes,\n\t\t\t\t\t\t file_priv)) {\n\t\t\tmode->expose_to_userspace = true;\n\t\t\tmode_count++;\n\t\t}\n\t}\n\n\t/*\n\t * This ioctl is called twice, once to determine how much space is\n\t * needed, and the 2nd time to fill it.\n\t */\n\tif ((out_resp->count_modes >= mode_count) && mode_count) {\n\t\tcopied = 0;\n\t\tmode_ptr = (struct drm_mode_modeinfo __user *)(unsigned long)out_resp->modes_ptr;\n\t\tlist_for_each_entry(mode, &connector->modes, head) {\n\t\t\tif (!mode->expose_to_userspace)\n\t\t\t\tcontinue;\n\n\t\t\t/* Clear the tag for the next time around */\n\t\t\tmode->expose_to_userspace = false;\n\n\t\t\tdrm_mode_convert_to_umode(&u_mode, mode);\n\t\t\t/*\n\t\t\t * Reset aspect ratio flags of user-mode, if modes with\n\t\t\t * aspect-ratio are not supported.\n\t\t\t */\n\t\t\tif (!file_priv->aspect_ratio_allowed)\n\t\t\t\tu_mode.flags &= ~DRM_MODE_FLAG_PIC_AR_MASK;\n\t\t\tif (copy_to_user(mode_ptr + copied,\n\t\t\t\t\t &u_mode, sizeof(u_mode))) {\n\t\t\t\tret = -EFAULT;\n\n\t\t\t\t/*\n\t\t\t\t * Clear the tag for the rest of\n\t\t\t\t * the modes for the next time around.\n\t\t\t\t */\n\t\t\t\tlist_for_each_entry_continue(mode, &connector->modes, head)\n\t\t\t\t\tmode->expose_to_userspace = false;\n\n\t\t\t\tmutex_unlock(&dev->mode_config.mutex);\n\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tcopied++;\n\t\t}\n\t} else {\n\t\t/* Clear the tag for the next time around */\n\t\tlist_for_each_entry(mode, &connector->modes, head)\n\t\t\tmode->expose_to_userspace = false;\n\t}\n\n\tout_resp->count_modes = mode_count;\n\tmutex_unlock(&dev->mode_config.mutex);\n\n\tdrm_modeset_lock(&dev->mode_config.connection_mutex, NULL);\n\tencoder = drm_connector_get_encoder(connector);\n\tif (encoder)\n\t\tout_resp->encoder_id = encoder->base.id;\n\telse\n\t\tout_resp->encoder_id = 0;\n\n\t/* Only grab properties after probing, to make sure EDID and other\n\t * properties reflect the latest status.\n\t */\n\tret = drm_mode_object_get_properties(&connector->base, file_priv->atomic,\n\t\t\t(uint32_t __user *)(unsigned long)(out_resp->props_ptr),\n\t\t\t(uint64_t __user *)(unsigned long)(out_resp->prop_values_ptr),\n\t\t\t&out_resp->count_props);\n\tdrm_modeset_unlock(&dev->mode_config.connection_mutex);\n\nout:\n\tdrm_connector_put(connector);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,7 @@\n \tstruct drm_mode_modeinfo u_mode;\n \tstruct drm_mode_modeinfo __user *mode_ptr;\n \tuint32_t __user *encoder_ptr;\n+\tbool is_current_master;\n \n \tif (!drm_core_check_feature(dev, DRIVER_MODESET))\n \t\treturn -EOPNOTSUPP;\n@@ -42,9 +43,11 @@\n \tout_resp->connector_type = connector->connector_type;\n \tout_resp->connector_type_id = connector->connector_type_id;\n \n+\tis_current_master = drm_is_current_master(file_priv);\n+\n \tmutex_lock(&dev->mode_config.mutex);\n \tif (out_resp->count_modes == 0) {\n-\t\tif (drm_is_current_master(file_priv))\n+\t\tif (is_current_master)\n \t\t\tconnector->funcs->fill_modes(connector,\n \t\t\t\t\t\t     dev->mode_config.max_width,\n \t\t\t\t\t\t     dev->mode_config.max_height);",
        "function_modified_lines": {
            "added": [
                "\tbool is_current_master;",
                "\tis_current_master = drm_is_current_master(file_priv);",
                "",
                "\t\tif (is_current_master)"
            ],
            "deleted": [
                "\t\tif (drm_is_current_master(file_priv))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in drm_lease_held in drivers/gpu/drm/drm_lease.c in the Linux kernel due to a race problem. This flaw allows a local user privilege attacker to cause a denial of service (DoS) or a kernel information leak.",
        "id": 3258
    },
    {
        "cve_id": "CVE-2022-3977",
        "code_before_change": "static int mctp_ioctl_alloctag(struct mctp_sock *msk, unsigned long arg)\n{\n\tstruct net *net = sock_net(&msk->sk);\n\tstruct mctp_sk_key *key = NULL;\n\tstruct mctp_ioc_tag_ctl ctl;\n\tunsigned long flags;\n\tu8 tag;\n\n\tif (copy_from_user(&ctl, (void __user *)arg, sizeof(ctl)))\n\t\treturn -EFAULT;\n\n\tif (ctl.tag)\n\t\treturn -EINVAL;\n\n\tif (ctl.flags)\n\t\treturn -EINVAL;\n\n\tkey = mctp_alloc_local_tag(msk, ctl.peer_addr, MCTP_ADDR_ANY,\n\t\t\t\t   true, &tag);\n\tif (IS_ERR(key))\n\t\treturn PTR_ERR(key);\n\n\tctl.tag = tag | MCTP_TAG_OWNER | MCTP_TAG_PREALLOC;\n\tif (copy_to_user((void __user *)arg, &ctl, sizeof(ctl))) {\n\t\tspin_lock_irqsave(&key->lock, flags);\n\t\t__mctp_key_remove(key, net, flags, MCTP_TRACE_KEY_DROPPED);\n\t\tmctp_key_unref(key);\n\t\treturn -EFAULT;\n\t}\n\n\tmctp_key_unref(key);\n\treturn 0;\n}",
        "code_after_change": "static int mctp_ioctl_alloctag(struct mctp_sock *msk, unsigned long arg)\n{\n\tstruct net *net = sock_net(&msk->sk);\n\tstruct mctp_sk_key *key = NULL;\n\tstruct mctp_ioc_tag_ctl ctl;\n\tunsigned long flags;\n\tu8 tag;\n\n\tif (copy_from_user(&ctl, (void __user *)arg, sizeof(ctl)))\n\t\treturn -EFAULT;\n\n\tif (ctl.tag)\n\t\treturn -EINVAL;\n\n\tif (ctl.flags)\n\t\treturn -EINVAL;\n\n\tkey = mctp_alloc_local_tag(msk, ctl.peer_addr, MCTP_ADDR_ANY,\n\t\t\t\t   true, &tag);\n\tif (IS_ERR(key))\n\t\treturn PTR_ERR(key);\n\n\tctl.tag = tag | MCTP_TAG_OWNER | MCTP_TAG_PREALLOC;\n\tif (copy_to_user((void __user *)arg, &ctl, sizeof(ctl))) {\n\t\tunsigned long fl2;\n\t\t/* Unwind our key allocation: the keys list lock needs to be\n\t\t * taken before the individual key locks, and we need a valid\n\t\t * flags value (fl2) to pass to __mctp_key_remove, hence the\n\t\t * second spin_lock_irqsave() rather than a plain spin_lock().\n\t\t */\n\t\tspin_lock_irqsave(&net->mctp.keys_lock, flags);\n\t\tspin_lock_irqsave(&key->lock, fl2);\n\t\t__mctp_key_remove(key, net, fl2, MCTP_TRACE_KEY_DROPPED);\n\t\tmctp_key_unref(key);\n\t\tspin_unlock_irqrestore(&net->mctp.keys_lock, flags);\n\t\treturn -EFAULT;\n\t}\n\n\tmctp_key_unref(key);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,9 +22,17 @@\n \n \tctl.tag = tag | MCTP_TAG_OWNER | MCTP_TAG_PREALLOC;\n \tif (copy_to_user((void __user *)arg, &ctl, sizeof(ctl))) {\n-\t\tspin_lock_irqsave(&key->lock, flags);\n-\t\t__mctp_key_remove(key, net, flags, MCTP_TRACE_KEY_DROPPED);\n+\t\tunsigned long fl2;\n+\t\t/* Unwind our key allocation: the keys list lock needs to be\n+\t\t * taken before the individual key locks, and we need a valid\n+\t\t * flags value (fl2) to pass to __mctp_key_remove, hence the\n+\t\t * second spin_lock_irqsave() rather than a plain spin_lock().\n+\t\t */\n+\t\tspin_lock_irqsave(&net->mctp.keys_lock, flags);\n+\t\tspin_lock_irqsave(&key->lock, fl2);\n+\t\t__mctp_key_remove(key, net, fl2, MCTP_TRACE_KEY_DROPPED);\n \t\tmctp_key_unref(key);\n+\t\tspin_unlock_irqrestore(&net->mctp.keys_lock, flags);\n \t\treturn -EFAULT;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tunsigned long fl2;",
                "\t\t/* Unwind our key allocation: the keys list lock needs to be",
                "\t\t * taken before the individual key locks, and we need a valid",
                "\t\t * flags value (fl2) to pass to __mctp_key_remove, hence the",
                "\t\t * second spin_lock_irqsave() rather than a plain spin_lock().",
                "\t\t */",
                "\t\tspin_lock_irqsave(&net->mctp.keys_lock, flags);",
                "\t\tspin_lock_irqsave(&key->lock, fl2);",
                "\t\t__mctp_key_remove(key, net, fl2, MCTP_TRACE_KEY_DROPPED);",
                "\t\tspin_unlock_irqrestore(&net->mctp.keys_lock, flags);"
            ],
            "deleted": [
                "\t\tspin_lock_irqsave(&key->lock, flags);",
                "\t\t__mctp_key_remove(key, net, flags, MCTP_TRACE_KEY_DROPPED);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel MCTP (Management Component Transport Protocol) functionality. This issue occurs when a user simultaneously calls DROPTAG ioctl and socket close happens, which could allow a local user to crash the system or potentially escalate their privileges on the system.",
        "id": 3699
    },
    {
        "cve_id": "CVE-2019-2024",
        "code_before_change": "static int em28xx_dvb_fini(struct em28xx *dev)\n{\n\tstruct em28xx_dvb *dvb;\n\tstruct i2c_client *client;\n\n\tif (dev->is_audio_only) {\n\t\t/* Shouldn't initialize IR for this interface */\n\t\treturn 0;\n\t}\n\n\tif (!dev->board.has_dvb) {\n\t\t/* This device does not support the extension */\n\t\treturn 0;\n\t}\n\n\tif (!dev->dvb)\n\t\treturn 0;\n\n\tdev_info(&dev->intf->dev, \"Closing DVB extension\\n\");\n\n\tdvb = dev->dvb;\n\n\tem28xx_uninit_usb_xfer(dev, EM28XX_DIGITAL_MODE);\n\n\tif (dev->disconnected) {\n\t\t/* We cannot tell the device to sleep\n\t\t * once it has been unplugged. */\n\t\tif (dvb->fe[0]) {\n\t\t\tprevent_sleep(&dvb->fe[0]->ops);\n\t\t\tdvb->fe[0]->exit = DVB_FE_DEVICE_REMOVED;\n\t\t}\n\t\tif (dvb->fe[1]) {\n\t\t\tprevent_sleep(&dvb->fe[1]->ops);\n\t\t\tdvb->fe[1]->exit = DVB_FE_DEVICE_REMOVED;\n\t\t}\n\t}\n\n\t/* remove I2C SEC */\n\tclient = dvb->i2c_client_sec;\n\tif (client) {\n\t\tmodule_put(client->dev.driver->owner);\n\t\ti2c_unregister_device(client);\n\t}\n\n\t/* remove I2C tuner */\n\tclient = dvb->i2c_client_tuner;\n\tif (client) {\n\t\tmodule_put(client->dev.driver->owner);\n\t\ti2c_unregister_device(client);\n\t}\n\n\t/* remove I2C demod */\n\tclient = dvb->i2c_client_demod;\n\tif (client) {\n\t\tmodule_put(client->dev.driver->owner);\n\t\ti2c_unregister_device(client);\n\t}\n\n\tem28xx_unregister_dvb(dvb);\n\tkfree(dvb);\n\tdev->dvb = NULL;\n\tkref_put(&dev->ref, em28xx_free_device);\n\n\treturn 0;\n}",
        "code_after_change": "static int em28xx_dvb_fini(struct em28xx *dev)\n{\n\tstruct em28xx_dvb *dvb;\n\tstruct i2c_client *client;\n\n\tif (dev->is_audio_only) {\n\t\t/* Shouldn't initialize IR for this interface */\n\t\treturn 0;\n\t}\n\n\tif (!dev->board.has_dvb) {\n\t\t/* This device does not support the extension */\n\t\treturn 0;\n\t}\n\n\tif (!dev->dvb)\n\t\treturn 0;\n\n\tdev_info(&dev->intf->dev, \"Closing DVB extension\\n\");\n\n\tdvb = dev->dvb;\n\n\tem28xx_uninit_usb_xfer(dev, EM28XX_DIGITAL_MODE);\n\n\tif (dev->disconnected) {\n\t\t/* We cannot tell the device to sleep\n\t\t * once it has been unplugged. */\n\t\tif (dvb->fe[0]) {\n\t\t\tprevent_sleep(&dvb->fe[0]->ops);\n\t\t\tdvb->fe[0]->exit = DVB_FE_DEVICE_REMOVED;\n\t\t}\n\t\tif (dvb->fe[1]) {\n\t\t\tprevent_sleep(&dvb->fe[1]->ops);\n\t\t\tdvb->fe[1]->exit = DVB_FE_DEVICE_REMOVED;\n\t\t}\n\t}\n\n\tem28xx_unregister_dvb(dvb);\n\n\t/* remove I2C SEC */\n\tclient = dvb->i2c_client_sec;\n\tif (client) {\n\t\tmodule_put(client->dev.driver->owner);\n\t\ti2c_unregister_device(client);\n\t}\n\n\t/* remove I2C tuner */\n\tclient = dvb->i2c_client_tuner;\n\tif (client) {\n\t\tmodule_put(client->dev.driver->owner);\n\t\ti2c_unregister_device(client);\n\t}\n\n\t/* remove I2C demod */\n\tclient = dvb->i2c_client_demod;\n\tif (client) {\n\t\tmodule_put(client->dev.driver->owner);\n\t\ti2c_unregister_device(client);\n\t}\n\n\tkfree(dvb);\n\tdev->dvb = NULL;\n\tkref_put(&dev->ref, em28xx_free_device);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -35,6 +35,8 @@\n \t\t}\n \t}\n \n+\tem28xx_unregister_dvb(dvb);\n+\n \t/* remove I2C SEC */\n \tclient = dvb->i2c_client_sec;\n \tif (client) {\n@@ -56,7 +58,6 @@\n \t\ti2c_unregister_device(client);\n \t}\n \n-\tem28xx_unregister_dvb(dvb);\n \tkfree(dvb);\n \tdev->dvb = NULL;\n \tkref_put(&dev->ref, em28xx_free_device);",
        "function_modified_lines": {
            "added": [
                "\tem28xx_unregister_dvb(dvb);",
                ""
            ],
            "deleted": [
                "\tem28xx_unregister_dvb(dvb);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In em28xx_unregister_dvb of em28xx-dvb.c, there is a possible use after free issue. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-111761954References: Upstream kernel",
        "id": 2274
    },
    {
        "cve_id": "CVE-2015-5706",
        "code_before_change": "static struct file *path_openat(int dfd, struct filename *pathname,\n\t\tstruct nameidata *nd, const struct open_flags *op, int flags)\n{\n\tstruct file *file;\n\tstruct path path;\n\tint opened = 0;\n\tint error;\n\n\tfile = get_empty_filp();\n\tif (IS_ERR(file))\n\t\treturn file;\n\n\tfile->f_flags = op->open_flag;\n\n\tif (unlikely(file->f_flags & __O_TMPFILE)) {\n\t\terror = do_tmpfile(dfd, pathname, nd, flags, op, file, &opened);\n\t\tgoto out;\n\t}\n\n\terror = path_init(dfd, pathname, flags, nd);\n\tif (unlikely(error))\n\t\tgoto out;\n\n\terror = do_last(nd, &path, file, op, &opened, pathname);\n\twhile (unlikely(error > 0)) { /* trailing symlink */\n\t\tstruct path link = path;\n\t\tvoid *cookie;\n\t\tif (!(nd->flags & LOOKUP_FOLLOW)) {\n\t\t\tpath_put_conditional(&path, nd);\n\t\t\tpath_put(&nd->path);\n\t\t\terror = -ELOOP;\n\t\t\tbreak;\n\t\t}\n\t\terror = may_follow_link(&link, nd);\n\t\tif (unlikely(error))\n\t\t\tbreak;\n\t\tnd->flags |= LOOKUP_PARENT;\n\t\tnd->flags &= ~(LOOKUP_OPEN|LOOKUP_CREATE|LOOKUP_EXCL);\n\t\terror = follow_link(&link, nd, &cookie);\n\t\tif (unlikely(error))\n\t\t\tbreak;\n\t\terror = do_last(nd, &path, file, op, &opened, pathname);\n\t\tput_link(nd, &link, cookie);\n\t}\nout:\n\tpath_cleanup(nd);\n\tif (!(opened & FILE_OPENED)) {\n\t\tBUG_ON(!error);\n\t\tput_filp(file);\n\t}\n\tif (unlikely(error)) {\n\t\tif (error == -EOPENSTALE) {\n\t\t\tif (flags & LOOKUP_RCU)\n\t\t\t\terror = -ECHILD;\n\t\t\telse\n\t\t\t\terror = -ESTALE;\n\t\t}\n\t\tfile = ERR_PTR(error);\n\t}\n\treturn file;\n}",
        "code_after_change": "static struct file *path_openat(int dfd, struct filename *pathname,\n\t\tstruct nameidata *nd, const struct open_flags *op, int flags)\n{\n\tstruct file *file;\n\tstruct path path;\n\tint opened = 0;\n\tint error;\n\n\tfile = get_empty_filp();\n\tif (IS_ERR(file))\n\t\treturn file;\n\n\tfile->f_flags = op->open_flag;\n\n\tif (unlikely(file->f_flags & __O_TMPFILE)) {\n\t\terror = do_tmpfile(dfd, pathname, nd, flags, op, file, &opened);\n\t\tgoto out2;\n\t}\n\n\terror = path_init(dfd, pathname, flags, nd);\n\tif (unlikely(error))\n\t\tgoto out;\n\n\terror = do_last(nd, &path, file, op, &opened, pathname);\n\twhile (unlikely(error > 0)) { /* trailing symlink */\n\t\tstruct path link = path;\n\t\tvoid *cookie;\n\t\tif (!(nd->flags & LOOKUP_FOLLOW)) {\n\t\t\tpath_put_conditional(&path, nd);\n\t\t\tpath_put(&nd->path);\n\t\t\terror = -ELOOP;\n\t\t\tbreak;\n\t\t}\n\t\terror = may_follow_link(&link, nd);\n\t\tif (unlikely(error))\n\t\t\tbreak;\n\t\tnd->flags |= LOOKUP_PARENT;\n\t\tnd->flags &= ~(LOOKUP_OPEN|LOOKUP_CREATE|LOOKUP_EXCL);\n\t\terror = follow_link(&link, nd, &cookie);\n\t\tif (unlikely(error))\n\t\t\tbreak;\n\t\terror = do_last(nd, &path, file, op, &opened, pathname);\n\t\tput_link(nd, &link, cookie);\n\t}\nout:\n\tpath_cleanup(nd);\nout2:\n\tif (!(opened & FILE_OPENED)) {\n\t\tBUG_ON(!error);\n\t\tput_filp(file);\n\t}\n\tif (unlikely(error)) {\n\t\tif (error == -EOPENSTALE) {\n\t\t\tif (flags & LOOKUP_RCU)\n\t\t\t\terror = -ECHILD;\n\t\t\telse\n\t\t\t\terror = -ESTALE;\n\t\t}\n\t\tfile = ERR_PTR(error);\n\t}\n\treturn file;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,7 +14,7 @@\n \n \tif (unlikely(file->f_flags & __O_TMPFILE)) {\n \t\terror = do_tmpfile(dfd, pathname, nd, flags, op, file, &opened);\n-\t\tgoto out;\n+\t\tgoto out2;\n \t}\n \n \terror = path_init(dfd, pathname, flags, nd);\n@@ -44,6 +44,7 @@\n \t}\n out:\n \tpath_cleanup(nd);\n+out2:\n \tif (!(opened & FILE_OPENED)) {\n \t\tBUG_ON(!error);\n \t\tput_filp(file);",
        "function_modified_lines": {
            "added": [
                "\t\tgoto out2;",
                "out2:"
            ],
            "deleted": [
                "\t\tgoto out;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in the path_openat function in fs/namei.c in the Linux kernel 3.x and 4.x before 4.0.4 allows local users to cause a denial of service or possibly have unspecified other impact via O_TMPFILE filesystem operations that leverage a duplicate cleanup operation.",
        "id": 778
    },
    {
        "cve_id": "CVE-2016-10906",
        "code_before_change": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
        "code_after_change": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\t\ttx_buff->skb = NULL;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \t\tstruct sk_buff *skb = tx_buff->skb;\n \t\tunsigned int info = le32_to_cpu(txbd->info);\n \n-\t\tif ((info & FOR_EMAC) || !txbd->data)\n+\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)\n \t\t\tbreak;\n \n \t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n@@ -39,6 +39,7 @@\n \n \t\ttxbd->data = 0;\n \t\ttxbd->info = 0;\n+\t\ttx_buff->skb = NULL;\n \n \t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)",
                "\t\ttx_buff->skb = NULL;"
            ],
            "deleted": [
                "\t\tif ((info & FOR_EMAC) || !txbd->data)"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/net/ethernet/arc/emac_main.c in the Linux kernel before 4.5. A use-after-free is caused by a race condition between the functions arc_emac_tx and arc_emac_tx_clean.",
        "id": 910
    },
    {
        "cve_id": "CVE-2019-15917",
        "code_before_change": "static int hci_uart_set_proto(struct hci_uart *hu, int id)\n{\n\tconst struct hci_uart_proto *p;\n\tint err;\n\n\tp = hci_uart_get_proto(id);\n\tif (!p)\n\t\treturn -EPROTONOSUPPORT;\n\n\thu->proto = p;\n\tset_bit(HCI_UART_PROTO_READY, &hu->flags);\n\n\terr = hci_uart_register_dev(hu);\n\tif (err) {\n\t\tclear_bit(HCI_UART_PROTO_READY, &hu->flags);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int hci_uart_set_proto(struct hci_uart *hu, int id)\n{\n\tconst struct hci_uart_proto *p;\n\tint err;\n\n\tp = hci_uart_get_proto(id);\n\tif (!p)\n\t\treturn -EPROTONOSUPPORT;\n\n\thu->proto = p;\n\n\terr = hci_uart_register_dev(hu);\n\tif (err) {\n\t\treturn err;\n\t}\n\n\tset_bit(HCI_UART_PROTO_READY, &hu->flags);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,13 +8,12 @@\n \t\treturn -EPROTONOSUPPORT;\n \n \thu->proto = p;\n-\tset_bit(HCI_UART_PROTO_READY, &hu->flags);\n \n \terr = hci_uart_register_dev(hu);\n \tif (err) {\n-\t\tclear_bit(HCI_UART_PROTO_READY, &hu->flags);\n \t\treturn err;\n \t}\n \n+\tset_bit(HCI_UART_PROTO_READY, &hu->flags);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tset_bit(HCI_UART_PROTO_READY, &hu->flags);"
            ],
            "deleted": [
                "\tset_bit(HCI_UART_PROTO_READY, &hu->flags);",
                "\t\tclear_bit(HCI_UART_PROTO_READY, &hu->flags);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.5. There is a use-after-free issue when hci_uart_register_dev() fails in hci_uart_set_proto() in drivers/bluetooth/hci_ldisc.c.",
        "id": 2024
    },
    {
        "cve_id": "CVE-2021-33034",
        "code_before_change": "static void hci_loglink_complete_evt(struct hci_dev *hdev, struct sk_buff *skb)\n{\n\tstruct hci_ev_logical_link_complete *ev = (void *) skb->data;\n\tstruct hci_conn *hcon;\n\tstruct hci_chan *hchan;\n\tstruct amp_mgr *mgr;\n\n\tBT_DBG(\"%s log_handle 0x%4.4x phy_handle 0x%2.2x status 0x%2.2x\",\n\t       hdev->name, le16_to_cpu(ev->handle), ev->phy_handle,\n\t       ev->status);\n\n\thcon = hci_conn_hash_lookup_handle(hdev, ev->phy_handle);\n\tif (!hcon)\n\t\treturn;\n\n\t/* Create AMP hchan */\n\thchan = hci_chan_create(hcon);\n\tif (!hchan)\n\t\treturn;\n\n\thchan->handle = le16_to_cpu(ev->handle);\n\n\tBT_DBG(\"hcon %p mgr %p hchan %p\", hcon, hcon->amp_mgr, hchan);\n\n\tmgr = hcon->amp_mgr;\n\tif (mgr && mgr->bredr_chan) {\n\t\tstruct l2cap_chan *bredr_chan = mgr->bredr_chan;\n\n\t\tl2cap_chan_lock(bredr_chan);\n\n\t\tbredr_chan->conn->mtu = hdev->block_mtu;\n\t\tl2cap_logical_cfm(bredr_chan, hchan, 0);\n\t\thci_conn_hold(hcon);\n\n\t\tl2cap_chan_unlock(bredr_chan);\n\t}\n}",
        "code_after_change": "static void hci_loglink_complete_evt(struct hci_dev *hdev, struct sk_buff *skb)\n{\n\tstruct hci_ev_logical_link_complete *ev = (void *) skb->data;\n\tstruct hci_conn *hcon;\n\tstruct hci_chan *hchan;\n\tstruct amp_mgr *mgr;\n\n\tBT_DBG(\"%s log_handle 0x%4.4x phy_handle 0x%2.2x status 0x%2.2x\",\n\t       hdev->name, le16_to_cpu(ev->handle), ev->phy_handle,\n\t       ev->status);\n\n\thcon = hci_conn_hash_lookup_handle(hdev, ev->phy_handle);\n\tif (!hcon)\n\t\treturn;\n\n\t/* Create AMP hchan */\n\thchan = hci_chan_create(hcon);\n\tif (!hchan)\n\t\treturn;\n\n\thchan->handle = le16_to_cpu(ev->handle);\n\thchan->amp = true;\n\n\tBT_DBG(\"hcon %p mgr %p hchan %p\", hcon, hcon->amp_mgr, hchan);\n\n\tmgr = hcon->amp_mgr;\n\tif (mgr && mgr->bredr_chan) {\n\t\tstruct l2cap_chan *bredr_chan = mgr->bredr_chan;\n\n\t\tl2cap_chan_lock(bredr_chan);\n\n\t\tbredr_chan->conn->mtu = hdev->block_mtu;\n\t\tl2cap_logical_cfm(bredr_chan, hchan, 0);\n\t\thci_conn_hold(hcon);\n\n\t\tl2cap_chan_unlock(bredr_chan);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,6 +19,7 @@\n \t\treturn;\n \n \thchan->handle = le16_to_cpu(ev->handle);\n+\thchan->amp = true;\n \n \tBT_DBG(\"hcon %p mgr %p hchan %p\", hcon, hcon->amp_mgr, hchan);\n ",
        "function_modified_lines": {
            "added": [
                "\thchan->amp = true;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.12.4, net/bluetooth/hci_event.c has a use-after-free when destroying an hci_chan, aka CID-5c4c8c954409. This leads to writing an arbitrary value.",
        "id": 2968
    },
    {
        "cve_id": "CVE-2020-14416",
        "code_before_change": "static void slcan_close(struct tty_struct *tty)\n{\n\tstruct slcan *sl = (struct slcan *) tty->disc_data;\n\n\t/* First make sure we're connected. */\n\tif (!sl || sl->magic != SLCAN_MAGIC || sl->tty != tty)\n\t\treturn;\n\n\tspin_lock_bh(&sl->lock);\n\ttty->disc_data = NULL;\n\tsl->tty = NULL;\n\tspin_unlock_bh(&sl->lock);\n\n\tflush_work(&sl->tx_work);\n\n\t/* Flush network side */\n\tunregister_netdev(sl->dev);\n\t/* This will complete via sl_free_netdev */\n}",
        "code_after_change": "static void slcan_close(struct tty_struct *tty)\n{\n\tstruct slcan *sl = (struct slcan *) tty->disc_data;\n\n\t/* First make sure we're connected. */\n\tif (!sl || sl->magic != SLCAN_MAGIC || sl->tty != tty)\n\t\treturn;\n\n\tspin_lock_bh(&sl->lock);\n\trcu_assign_pointer(tty->disc_data, NULL);\n\tsl->tty = NULL;\n\tspin_unlock_bh(&sl->lock);\n\n\tsynchronize_rcu();\n\tflush_work(&sl->tx_work);\n\n\t/* Flush network side */\n\tunregister_netdev(sl->dev);\n\t/* This will complete via sl_free_netdev */\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,10 +7,11 @@\n \t\treturn;\n \n \tspin_lock_bh(&sl->lock);\n-\ttty->disc_data = NULL;\n+\trcu_assign_pointer(tty->disc_data, NULL);\n \tsl->tty = NULL;\n \tspin_unlock_bh(&sl->lock);\n \n+\tsynchronize_rcu();\n \tflush_work(&sl->tx_work);\n \n \t/* Flush network side */",
        "function_modified_lines": {
            "added": [
                "\trcu_assign_pointer(tty->disc_data, NULL);",
                "\tsynchronize_rcu();"
            ],
            "deleted": [
                "\ttty->disc_data = NULL;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.4.16, a race condition in tty->disc_data handling in the slip and slcan line discipline could lead to a use-after-free, aka CID-0ace17d56824. This affects drivers/net/slip/slip.c and drivers/net/can/slcan.c.",
        "id": 2540
    },
    {
        "cve_id": "CVE-2017-18202",
        "code_before_change": "static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tbool ret = true;\n\n\t/*\n\t * We have to make sure to not race with the victim exit path\n\t * and cause premature new oom victim selection:\n\t * __oom_reap_task_mm\t\texit_mm\n\t *   mmget_not_zero\n\t *\t\t\t\t  mmput\n\t *\t\t\t\t    atomic_dec_and_test\n\t *\t\t\t\t  exit_oom_victim\n\t *\t\t\t\t[...]\n\t *\t\t\t\tout_of_memory\n\t *\t\t\t\t  select_bad_process\n\t *\t\t\t\t    # no TIF_MEMDIE task selects new victim\n\t *  unmap_page_range # frees some memory\n\t */\n\tmutex_lock(&oom_lock);\n\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tret = false;\n\t\ttrace_skip_task_reaping(tsk->pid);\n\t\tgoto unlock_oom;\n\t}\n\n\t/*\n\t * If the mm has notifiers then we would need to invalidate them around\n\t * unmap_page_range and that is risky because notifiers can sleep and\n\t * what they do is basically undeterministic.  So let's have a short\n\t * sleep to give the oom victim some more time.\n\t * TODO: we really want to get rid of this ugly hack and make sure that\n\t * notifiers cannot block for unbounded amount of time and add\n\t * mmu_notifier_invalidate_range_{start,end} around unmap_page_range\n\t */\n\tif (mm_has_notifiers(mm)) {\n\t\tup_read(&mm->mmap_sem);\n\t\tschedule_timeout_idle(HZ);\n\t\tgoto unlock_oom;\n\t}\n\n\t/*\n\t * MMF_OOM_SKIP is set by exit_mmap when the OOM reaper can't\n\t * work on the mm anymore. The check for MMF_OOM_SKIP must run\n\t * under mmap_sem for reading because it serializes against the\n\t * down_write();up_write() cycle in exit_mmap().\n\t */\n\tif (test_bit(MMF_OOM_SKIP, &mm->flags)) {\n\t\tup_read(&mm->mmap_sem);\n\t\ttrace_skip_task_reaping(tsk->pid);\n\t\tgoto unlock_oom;\n\t}\n\n\ttrace_start_task_reaping(tsk->pid);\n\n\t/*\n\t * Tell all users of get_user/copy_from_user etc... that the content\n\t * is no longer stable. No barriers really needed because unmapping\n\t * should imply barriers already and the reader would hit a page fault\n\t * if it stumbled over a reaped memory.\n\t */\n\tset_bit(MMF_UNSTABLE, &mm->flags);\n\n\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\tfor (vma = mm->mmap ; vma; vma = vma->vm_next) {\n\t\tif (!can_madv_dontneed_vma(vma))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Only anonymous pages have a good chance to be dropped\n\t\t * without additional steps which we cannot afford as we\n\t\t * are OOM already.\n\t\t *\n\t\t * We do not even care about fs backed pages because all\n\t\t * which are reclaimable have already been reclaimed and\n\t\t * we do not want to block exit_mmap by keeping mm ref\n\t\t * count elevated without a good reason.\n\t\t */\n\t\tif (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED))\n\t\t\tunmap_page_range(&tlb, vma, vma->vm_start, vma->vm_end,\n\t\t\t\t\t NULL);\n\t}\n\ttlb_finish_mmu(&tlb, 0, -1);\n\tpr_info(\"oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\\n\",\n\t\t\ttask_pid_nr(tsk), tsk->comm,\n\t\t\tK(get_mm_counter(mm, MM_ANONPAGES)),\n\t\t\tK(get_mm_counter(mm, MM_FILEPAGES)),\n\t\t\tK(get_mm_counter(mm, MM_SHMEMPAGES)));\n\tup_read(&mm->mmap_sem);\n\n\ttrace_finish_task_reaping(tsk->pid);\nunlock_oom:\n\tmutex_unlock(&oom_lock);\n\treturn ret;\n}",
        "code_after_change": "static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tbool ret = true;\n\n\t/*\n\t * We have to make sure to not race with the victim exit path\n\t * and cause premature new oom victim selection:\n\t * __oom_reap_task_mm\t\texit_mm\n\t *   mmget_not_zero\n\t *\t\t\t\t  mmput\n\t *\t\t\t\t    atomic_dec_and_test\n\t *\t\t\t\t  exit_oom_victim\n\t *\t\t\t\t[...]\n\t *\t\t\t\tout_of_memory\n\t *\t\t\t\t  select_bad_process\n\t *\t\t\t\t    # no TIF_MEMDIE task selects new victim\n\t *  unmap_page_range # frees some memory\n\t */\n\tmutex_lock(&oom_lock);\n\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tret = false;\n\t\ttrace_skip_task_reaping(tsk->pid);\n\t\tgoto unlock_oom;\n\t}\n\n\t/*\n\t * If the mm has notifiers then we would need to invalidate them around\n\t * unmap_page_range and that is risky because notifiers can sleep and\n\t * what they do is basically undeterministic.  So let's have a short\n\t * sleep to give the oom victim some more time.\n\t * TODO: we really want to get rid of this ugly hack and make sure that\n\t * notifiers cannot block for unbounded amount of time and add\n\t * mmu_notifier_invalidate_range_{start,end} around unmap_page_range\n\t */\n\tif (mm_has_notifiers(mm)) {\n\t\tup_read(&mm->mmap_sem);\n\t\tschedule_timeout_idle(HZ);\n\t\tgoto unlock_oom;\n\t}\n\n\t/*\n\t * MMF_OOM_SKIP is set by exit_mmap when the OOM reaper can't\n\t * work on the mm anymore. The check for MMF_OOM_SKIP must run\n\t * under mmap_sem for reading because it serializes against the\n\t * down_write();up_write() cycle in exit_mmap().\n\t */\n\tif (test_bit(MMF_OOM_SKIP, &mm->flags)) {\n\t\tup_read(&mm->mmap_sem);\n\t\ttrace_skip_task_reaping(tsk->pid);\n\t\tgoto unlock_oom;\n\t}\n\n\ttrace_start_task_reaping(tsk->pid);\n\n\t/*\n\t * Tell all users of get_user/copy_from_user etc... that the content\n\t * is no longer stable. No barriers really needed because unmapping\n\t * should imply barriers already and the reader would hit a page fault\n\t * if it stumbled over a reaped memory.\n\t */\n\tset_bit(MMF_UNSTABLE, &mm->flags);\n\n\tfor (vma = mm->mmap ; vma; vma = vma->vm_next) {\n\t\tif (!can_madv_dontneed_vma(vma))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Only anonymous pages have a good chance to be dropped\n\t\t * without additional steps which we cannot afford as we\n\t\t * are OOM already.\n\t\t *\n\t\t * We do not even care about fs backed pages because all\n\t\t * which are reclaimable have already been reclaimed and\n\t\t * we do not want to block exit_mmap by keeping mm ref\n\t\t * count elevated without a good reason.\n\t\t */\n\t\tif (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED)) {\n\t\t\ttlb_gather_mmu(&tlb, mm, vma->vm_start, vma->vm_end);\n\t\t\tunmap_page_range(&tlb, vma, vma->vm_start, vma->vm_end,\n\t\t\t\t\t NULL);\n\t\t\ttlb_finish_mmu(&tlb, vma->vm_start, vma->vm_end);\n\t\t}\n\t}\n\tpr_info(\"oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\\n\",\n\t\t\ttask_pid_nr(tsk), tsk->comm,\n\t\t\tK(get_mm_counter(mm, MM_ANONPAGES)),\n\t\t\tK(get_mm_counter(mm, MM_FILEPAGES)),\n\t\t\tK(get_mm_counter(mm, MM_SHMEMPAGES)));\n\tup_read(&mm->mmap_sem);\n\n\ttrace_finish_task_reaping(tsk->pid);\nunlock_oom:\n\tmutex_unlock(&oom_lock);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -63,7 +63,6 @@\n \t */\n \tset_bit(MMF_UNSTABLE, &mm->flags);\n \n-\ttlb_gather_mmu(&tlb, mm, 0, -1);\n \tfor (vma = mm->mmap ; vma; vma = vma->vm_next) {\n \t\tif (!can_madv_dontneed_vma(vma))\n \t\t\tcontinue;\n@@ -78,11 +77,13 @@\n \t\t * we do not want to block exit_mmap by keeping mm ref\n \t\t * count elevated without a good reason.\n \t\t */\n-\t\tif (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED))\n+\t\tif (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED)) {\n+\t\t\ttlb_gather_mmu(&tlb, mm, vma->vm_start, vma->vm_end);\n \t\t\tunmap_page_range(&tlb, vma, vma->vm_start, vma->vm_end,\n \t\t\t\t\t NULL);\n+\t\t\ttlb_finish_mmu(&tlb, vma->vm_start, vma->vm_end);\n+\t\t}\n \t}\n-\ttlb_finish_mmu(&tlb, 0, -1);\n \tpr_info(\"oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\\n\",\n \t\t\ttask_pid_nr(tsk), tsk->comm,\n \t\t\tK(get_mm_counter(mm, MM_ANONPAGES)),",
        "function_modified_lines": {
            "added": [
                "\t\tif (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED)) {",
                "\t\t\ttlb_gather_mmu(&tlb, mm, vma->vm_start, vma->vm_end);",
                "\t\t\ttlb_finish_mmu(&tlb, vma->vm_start, vma->vm_end);",
                "\t\t}"
            ],
            "deleted": [
                "\ttlb_gather_mmu(&tlb, mm, 0, -1);",
                "\t\tif (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED))",
                "\ttlb_finish_mmu(&tlb, 0, -1);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The __oom_reap_task_mm function in mm/oom_kill.c in the Linux kernel before 4.14.4 mishandles gather operations, which allows attackers to cause a denial of service (TLB entry leak or use-after-free) or possibly have unspecified other impact by triggering a copy_to_user call within a certain time window.",
        "id": 1397
    },
    {
        "cve_id": "CVE-2022-3176",
        "code_before_change": "static inline void io_poll_remove_entry(struct io_poll_iocb *poll)\n{\n\tstruct wait_queue_head *head = poll->head;\n\n\tspin_lock_irq(&head->lock);\n\tlist_del_init(&poll->wait.entry);\n\tpoll->head = NULL;\n\tspin_unlock_irq(&head->lock);\n}",
        "code_after_change": "static inline void io_poll_remove_entry(struct io_poll_iocb *poll)\n{\n\tstruct wait_queue_head *head = smp_load_acquire(&poll->head);\n\n\tif (head) {\n\t\tspin_lock_irq(&head->lock);\n\t\tlist_del_init(&poll->wait.entry);\n\t\tpoll->head = NULL;\n\t\tspin_unlock_irq(&head->lock);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,9 +1,11 @@\n static inline void io_poll_remove_entry(struct io_poll_iocb *poll)\n {\n-\tstruct wait_queue_head *head = poll->head;\n+\tstruct wait_queue_head *head = smp_load_acquire(&poll->head);\n \n-\tspin_lock_irq(&head->lock);\n-\tlist_del_init(&poll->wait.entry);\n-\tpoll->head = NULL;\n-\tspin_unlock_irq(&head->lock);\n+\tif (head) {\n+\t\tspin_lock_irq(&head->lock);\n+\t\tlist_del_init(&poll->wait.entry);\n+\t\tpoll->head = NULL;\n+\t\tspin_unlock_irq(&head->lock);\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct wait_queue_head *head = smp_load_acquire(&poll->head);",
                "\tif (head) {",
                "\t\tspin_lock_irq(&head->lock);",
                "\t\tlist_del_init(&poll->wait.entry);",
                "\t\tpoll->head = NULL;",
                "\t\tspin_unlock_irq(&head->lock);",
                "\t}"
            ],
            "deleted": [
                "\tstruct wait_queue_head *head = poll->head;",
                "\tspin_lock_irq(&head->lock);",
                "\tlist_del_init(&poll->wait.entry);",
                "\tpoll->head = NULL;",
                "\tspin_unlock_irq(&head->lock);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There exists a use-after-free in io_uring in the Linux kernel. Signalfd_poll() and binder_poll() use a waitqueue whose lifetime is the current task. It will send a POLLFREE notification to all waiters before the queue is freed. Unfortunately, the io_uring poll doesn't handle POLLFREE. This allows a use-after-free to occur if a signalfd or binder fd is polled with io_uring poll, and the waitqueue gets freed. We recommend upgrading past commit fc78b2fc21f10c4c9c4d5d659a685710ffa63659",
        "id": 3566
    },
    {
        "cve_id": "CVE-2022-3534",
        "code_before_change": "static size_t btf_dump_name_dups(struct btf_dump *d, struct hashmap *name_map,\n\t\t\t\t const char *orig_name)\n{\n\tsize_t dup_cnt = 0;\n\n\thashmap__find(name_map, orig_name, (void **)&dup_cnt);\n\tdup_cnt++;\n\thashmap__set(name_map, orig_name, (void *)dup_cnt, NULL, NULL);\n\n\treturn dup_cnt;\n}",
        "code_after_change": "static size_t btf_dump_name_dups(struct btf_dump *d, struct hashmap *name_map,\n\t\t\t\t const char *orig_name)\n{\n\tchar *old_name, *new_name;\n\tsize_t dup_cnt = 0;\n\tint err;\n\n\tnew_name = strdup(orig_name);\n\tif (!new_name)\n\t\treturn 1;\n\n\thashmap__find(name_map, orig_name, (void **)&dup_cnt);\n\tdup_cnt++;\n\n\terr = hashmap__set(name_map, new_name, (void *)dup_cnt,\n\t\t\t   (const void **)&old_name, NULL);\n\tif (err)\n\t\tfree(new_name);\n\n\tfree(old_name);\n\n\treturn dup_cnt;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,23 @@\n static size_t btf_dump_name_dups(struct btf_dump *d, struct hashmap *name_map,\n \t\t\t\t const char *orig_name)\n {\n+\tchar *old_name, *new_name;\n \tsize_t dup_cnt = 0;\n+\tint err;\n+\n+\tnew_name = strdup(orig_name);\n+\tif (!new_name)\n+\t\treturn 1;\n \n \thashmap__find(name_map, orig_name, (void **)&dup_cnt);\n \tdup_cnt++;\n-\thashmap__set(name_map, orig_name, (void *)dup_cnt, NULL, NULL);\n+\n+\terr = hashmap__set(name_map, new_name, (void *)dup_cnt,\n+\t\t\t   (const void **)&old_name, NULL);\n+\tif (err)\n+\t\tfree(new_name);\n+\n+\tfree(old_name);\n \n \treturn dup_cnt;\n }",
        "function_modified_lines": {
            "added": [
                "\tchar *old_name, *new_name;",
                "\tint err;",
                "",
                "\tnew_name = strdup(orig_name);",
                "\tif (!new_name)",
                "\t\treturn 1;",
                "",
                "\terr = hashmap__set(name_map, new_name, (void *)dup_cnt,",
                "\t\t\t   (const void **)&old_name, NULL);",
                "\tif (err)",
                "\t\tfree(new_name);",
                "",
                "\tfree(old_name);"
            ],
            "deleted": [
                "\thashmap__set(name_map, orig_name, (void *)dup_cnt, NULL, NULL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability classified as critical has been found in Linux Kernel. Affected is the function btf_dump_name_dups of the file tools/lib/bpf/btf_dump.c of the component libbpf. The manipulation leads to use after free. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211032.",
        "id": 3628
    },
    {
        "cve_id": "CVE-2023-3159",
        "code_before_change": "static void outbound_phy_packet_callback(struct fw_packet *packet,\n\t\t\t\t\t struct fw_card *card, int status)\n{\n\tstruct outbound_phy_packet_event *e =\n\t\tcontainer_of(packet, struct outbound_phy_packet_event, p);\n\n\tswitch (status) {\n\t/* expected: */\n\tcase ACK_COMPLETE:\te->phy_packet.rcode = RCODE_COMPLETE;\tbreak;\n\t/* should never happen with PHY packets: */\n\tcase ACK_PENDING:\te->phy_packet.rcode = RCODE_COMPLETE;\tbreak;\n\tcase ACK_BUSY_X:\n\tcase ACK_BUSY_A:\n\tcase ACK_BUSY_B:\te->phy_packet.rcode = RCODE_BUSY;\tbreak;\n\tcase ACK_DATA_ERROR:\te->phy_packet.rcode = RCODE_DATA_ERROR;\tbreak;\n\tcase ACK_TYPE_ERROR:\te->phy_packet.rcode = RCODE_TYPE_ERROR;\tbreak;\n\t/* stale generation; cancelled; on certain controllers: no ack */\n\tdefault:\t\te->phy_packet.rcode = status;\t\tbreak;\n\t}\n\te->phy_packet.data[0] = packet->timestamp;\n\n\tqueue_event(e->client, &e->event, &e->phy_packet,\n\t\t    sizeof(e->phy_packet) + e->phy_packet.length, NULL, 0);\n\tclient_put(e->client);\n}",
        "code_after_change": "static void outbound_phy_packet_callback(struct fw_packet *packet,\n\t\t\t\t\t struct fw_card *card, int status)\n{\n\tstruct outbound_phy_packet_event *e =\n\t\tcontainer_of(packet, struct outbound_phy_packet_event, p);\n\tstruct client *e_client;\n\n\tswitch (status) {\n\t/* expected: */\n\tcase ACK_COMPLETE:\te->phy_packet.rcode = RCODE_COMPLETE;\tbreak;\n\t/* should never happen with PHY packets: */\n\tcase ACK_PENDING:\te->phy_packet.rcode = RCODE_COMPLETE;\tbreak;\n\tcase ACK_BUSY_X:\n\tcase ACK_BUSY_A:\n\tcase ACK_BUSY_B:\te->phy_packet.rcode = RCODE_BUSY;\tbreak;\n\tcase ACK_DATA_ERROR:\te->phy_packet.rcode = RCODE_DATA_ERROR;\tbreak;\n\tcase ACK_TYPE_ERROR:\te->phy_packet.rcode = RCODE_TYPE_ERROR;\tbreak;\n\t/* stale generation; cancelled; on certain controllers: no ack */\n\tdefault:\t\te->phy_packet.rcode = status;\t\tbreak;\n\t}\n\te->phy_packet.data[0] = packet->timestamp;\n\n\te_client = e->client;\n\tqueue_event(e->client, &e->event, &e->phy_packet,\n\t\t    sizeof(e->phy_packet) + e->phy_packet.length, NULL, 0);\n\tclient_put(e_client);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n {\n \tstruct outbound_phy_packet_event *e =\n \t\tcontainer_of(packet, struct outbound_phy_packet_event, p);\n+\tstruct client *e_client;\n \n \tswitch (status) {\n \t/* expected: */\n@@ -19,7 +20,8 @@\n \t}\n \te->phy_packet.data[0] = packet->timestamp;\n \n+\te_client = e->client;\n \tqueue_event(e->client, &e->event, &e->phy_packet,\n \t\t    sizeof(e->phy_packet) + e->phy_packet.length, NULL, 0);\n-\tclient_put(e->client);\n+\tclient_put(e_client);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct client *e_client;",
                "\te_client = e->client;",
                "\tclient_put(e_client);"
            ],
            "deleted": [
                "\tclient_put(e->client);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free issue was discovered in driver/firewire in outbound_phy_packet_callback in the Linux Kernel. In this flaw a local attacker with special privilege may cause a use after free problem when queue_event() fails.",
        "id": 4003
    },
    {
        "cve_id": "CVE-2023-0030",
        "code_before_change": "static int\nnvkm_uvmm_mthd_map(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tunion {\n\t\tstruct nvif_vmm_map_v0 v0;\n\t} *args = argv;\n\tu64 addr, size, handle, offset;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tstruct nvkm_memory *memory;\n\tint ret = -ENOSYS;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, true))) {\n\t\taddr = args->v0.addr;\n\t\tsize = args->v0.size;\n\t\thandle = args->v0.memory;\n\t\toffset = args->v0.offset;\n\t} else\n\t\treturn ret;\n\n\tmemory = nvkm_umem_search(client, handle);\n\tif (IS_ERR(memory)) {\n\t\tVMM_DEBUG(vmm, \"memory %016llx %ld\\n\", handle, PTR_ERR(memory));\n\t\treturn PTR_ERR(memory);\n\t}\n\n\tmutex_lock(&vmm->mutex);\n\tif (ret = -ENOENT, !(vma = nvkm_vmm_node_search(vmm, addr))) {\n\t\tVMM_DEBUG(vmm, \"lookup %016llx\", addr);\n\t\tgoto fail;\n\t}\n\n\tif (ret = -ENOENT, (!vma->user && !client->super) || vma->busy) {\n\t\tVMM_DEBUG(vmm, \"denied %016llx: %d %d %d\", addr,\n\t\t\t  vma->user, !client->super, vma->busy);\n\t\tgoto fail;\n\t}\n\n\tif (ret = -EINVAL, vma->addr != addr || vma->size != size) {\n\t\tif (addr + size > vma->addr + vma->size || vma->memory ||\n\t\t    (vma->refd == NVKM_VMA_PAGE_NONE && !vma->mapref)) {\n\t\t\tVMM_DEBUG(vmm, \"split %d %d %d \"\n\t\t\t\t       \"%016llx %016llx %016llx %016llx\",\n\t\t\t\t  !!vma->memory, vma->refd, vma->mapref,\n\t\t\t\t  addr, size, vma->addr, (u64)vma->size);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tif (vma->addr != addr) {\n\t\t\tconst u64 tail = vma->size + vma->addr - addr;\n\t\t\tif (ret = -ENOMEM, !(vma = nvkm_vma_tail(vma, tail)))\n\t\t\t\tgoto fail;\n\t\t\tvma->part = true;\n\t\t\tnvkm_vmm_node_insert(vmm, vma);\n\t\t}\n\n\t\tif (vma->size != size) {\n\t\t\tconst u64 tail = vma->size - size;\n\t\t\tstruct nvkm_vma *tmp;\n\t\t\tif (ret = -ENOMEM, !(tmp = nvkm_vma_tail(vma, tail))) {\n\t\t\t\tnvkm_vmm_unmap_region(vmm, vma);\n\t\t\t\tgoto fail;\n\t\t\t}\n\t\t\ttmp->part = true;\n\t\t\tnvkm_vmm_node_insert(vmm, tmp);\n\t\t}\n\t}\n\tvma->busy = true;\n\tmutex_unlock(&vmm->mutex);\n\n\tret = nvkm_memory_map(memory, offset, vmm, vma, argv, argc);\n\tif (ret == 0) {\n\t\t/* Successful map will clear vma->busy. */\n\t\tnvkm_memory_unref(&memory);\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&vmm->mutex);\n\tvma->busy = false;\n\tnvkm_vmm_unmap_region(vmm, vma);\nfail:\n\tmutex_unlock(&vmm->mutex);\n\tnvkm_memory_unref(&memory);\n\treturn ret;\n}",
        "code_after_change": "static int\nnvkm_uvmm_mthd_map(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tunion {\n\t\tstruct nvif_vmm_map_v0 v0;\n\t} *args = argv;\n\tu64 addr, size, handle, offset;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tstruct nvkm_memory *memory;\n\tint ret = -ENOSYS;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, true))) {\n\t\taddr = args->v0.addr;\n\t\tsize = args->v0.size;\n\t\thandle = args->v0.memory;\n\t\toffset = args->v0.offset;\n\t} else\n\t\treturn ret;\n\n\tmemory = nvkm_umem_search(client, handle);\n\tif (IS_ERR(memory)) {\n\t\tVMM_DEBUG(vmm, \"memory %016llx %ld\\n\", handle, PTR_ERR(memory));\n\t\treturn PTR_ERR(memory);\n\t}\n\n\tmutex_lock(&vmm->mutex);\n\tif (ret = -ENOENT, !(vma = nvkm_vmm_node_search(vmm, addr))) {\n\t\tVMM_DEBUG(vmm, \"lookup %016llx\", addr);\n\t\tgoto fail;\n\t}\n\n\tif (ret = -ENOENT, (!vma->user && !client->super) || vma->busy) {\n\t\tVMM_DEBUG(vmm, \"denied %016llx: %d %d %d\", addr,\n\t\t\t  vma->user, !client->super, vma->busy);\n\t\tgoto fail;\n\t}\n\n\tif (ret = -EINVAL, vma->addr != addr || vma->size != size) {\n\t\tif (addr + size > vma->addr + vma->size || vma->memory ||\n\t\t    (vma->refd == NVKM_VMA_PAGE_NONE && !vma->mapref)) {\n\t\t\tVMM_DEBUG(vmm, \"split %d %d %d \"\n\t\t\t\t       \"%016llx %016llx %016llx %016llx\",\n\t\t\t\t  !!vma->memory, vma->refd, vma->mapref,\n\t\t\t\t  addr, size, vma->addr, (u64)vma->size);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tvma = nvkm_vmm_node_split(vmm, vma, addr, size);\n\t\tif (!vma) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tvma->busy = true;\n\tmutex_unlock(&vmm->mutex);\n\n\tret = nvkm_memory_map(memory, offset, vmm, vma, argv, argc);\n\tif (ret == 0) {\n\t\t/* Successful map will clear vma->busy. */\n\t\tnvkm_memory_unref(&memory);\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&vmm->mutex);\n\tvma->busy = false;\n\tnvkm_vmm_unmap_region(vmm, vma);\nfail:\n\tmutex_unlock(&vmm->mutex);\n\tnvkm_memory_unref(&memory);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,23 +47,10 @@\n \t\t\tgoto fail;\n \t\t}\n \n-\t\tif (vma->addr != addr) {\n-\t\t\tconst u64 tail = vma->size + vma->addr - addr;\n-\t\t\tif (ret = -ENOMEM, !(vma = nvkm_vma_tail(vma, tail)))\n-\t\t\t\tgoto fail;\n-\t\t\tvma->part = true;\n-\t\t\tnvkm_vmm_node_insert(vmm, vma);\n-\t\t}\n-\n-\t\tif (vma->size != size) {\n-\t\t\tconst u64 tail = vma->size - size;\n-\t\t\tstruct nvkm_vma *tmp;\n-\t\t\tif (ret = -ENOMEM, !(tmp = nvkm_vma_tail(vma, tail))) {\n-\t\t\t\tnvkm_vmm_unmap_region(vmm, vma);\n-\t\t\t\tgoto fail;\n-\t\t\t}\n-\t\t\ttmp->part = true;\n-\t\t\tnvkm_vmm_node_insert(vmm, tmp);\n+\t\tvma = nvkm_vmm_node_split(vmm, vma, addr, size);\n+\t\tif (!vma) {\n+\t\t\tret = -ENOMEM;\n+\t\t\tgoto fail;\n \t\t}\n \t}\n \tvma->busy = true;",
        "function_modified_lines": {
            "added": [
                "\t\tvma = nvkm_vmm_node_split(vmm, vma, addr, size);",
                "\t\tif (!vma) {",
                "\t\t\tret = -ENOMEM;",
                "\t\t\tgoto fail;"
            ],
            "deleted": [
                "\t\tif (vma->addr != addr) {",
                "\t\t\tconst u64 tail = vma->size + vma->addr - addr;",
                "\t\t\tif (ret = -ENOMEM, !(vma = nvkm_vma_tail(vma, tail)))",
                "\t\t\t\tgoto fail;",
                "\t\t\tvma->part = true;",
                "\t\t\tnvkm_vmm_node_insert(vmm, vma);",
                "\t\t}",
                "",
                "\t\tif (vma->size != size) {",
                "\t\t\tconst u64 tail = vma->size - size;",
                "\t\t\tstruct nvkm_vma *tmp;",
                "\t\t\tif (ret = -ENOMEM, !(tmp = nvkm_vma_tail(vma, tail))) {",
                "\t\t\t\tnvkm_vmm_unmap_region(vmm, vma);",
                "\t\t\t\tgoto fail;",
                "\t\t\t}",
                "\t\t\ttmp->part = true;",
                "\t\t\tnvkm_vmm_node_insert(vmm, tmp);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s nouveau driver in how a user triggers a memory overflow that causes the nvkm_vma_tail function to fail. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3803
    },
    {
        "cve_id": "CVE-2022-2318",
        "code_before_change": "static void rose_timer_expiry(struct timer_list *t)\n{\n\tstruct rose_sock *rose = from_timer(rose, t, timer);\n\tstruct sock *sk = &rose->sock;\n\n\tbh_lock_sock(sk);\n\tswitch (rose->state) {\n\tcase ROSE_STATE_1:\t/* T1 */\n\tcase ROSE_STATE_4:\t/* T2 */\n\t\trose_write_internal(sk, ROSE_CLEAR_REQUEST);\n\t\trose->state = ROSE_STATE_2;\n\t\trose_start_t3timer(sk);\n\t\tbreak;\n\n\tcase ROSE_STATE_2:\t/* T3 */\n\t\trose->neighbour->use--;\n\t\trose_disconnect(sk, ETIMEDOUT, -1, -1);\n\t\tbreak;\n\n\tcase ROSE_STATE_3:\t/* HB */\n\t\tif (rose->condition & ROSE_COND_ACK_PENDING) {\n\t\t\trose->condition &= ~ROSE_COND_ACK_PENDING;\n\t\t\trose_enquiry_response(sk);\n\t\t}\n\t\tbreak;\n\t}\n\tbh_unlock_sock(sk);\n}",
        "code_after_change": "static void rose_timer_expiry(struct timer_list *t)\n{\n\tstruct rose_sock *rose = from_timer(rose, t, timer);\n\tstruct sock *sk = &rose->sock;\n\n\tbh_lock_sock(sk);\n\tswitch (rose->state) {\n\tcase ROSE_STATE_1:\t/* T1 */\n\tcase ROSE_STATE_4:\t/* T2 */\n\t\trose_write_internal(sk, ROSE_CLEAR_REQUEST);\n\t\trose->state = ROSE_STATE_2;\n\t\trose_start_t3timer(sk);\n\t\tbreak;\n\n\tcase ROSE_STATE_2:\t/* T3 */\n\t\trose->neighbour->use--;\n\t\trose_disconnect(sk, ETIMEDOUT, -1, -1);\n\t\tbreak;\n\n\tcase ROSE_STATE_3:\t/* HB */\n\t\tif (rose->condition & ROSE_COND_ACK_PENDING) {\n\t\t\trose->condition &= ~ROSE_COND_ACK_PENDING;\n\t\t\trose_enquiry_response(sk);\n\t\t}\n\t\tbreak;\n\t}\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,4 +25,5 @@\n \t\tbreak;\n \t}\n \tbh_unlock_sock(sk);\n+\tsock_put(sk);\n }",
        "function_modified_lines": {
            "added": [
                "\tsock_put(sk);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities caused by timer handler in net/rose/rose_timer.c of linux that allow attackers to crash linux kernel without any privileges.",
        "id": 3439
    },
    {
        "cve_id": "CVE-2014-9940",
        "code_before_change": "static void regulator_ena_gpio_free(struct regulator_dev *rdev)\n{\n\tstruct regulator_enable_gpio *pin, *n;\n\n\tif (!rdev->ena_pin)\n\t\treturn;\n\n\t/* Free the GPIO only in case of no use */\n\tlist_for_each_entry_safe(pin, n, &regulator_ena_gpio_list, list) {\n\t\tif (pin->gpiod == rdev->ena_pin->gpiod) {\n\t\t\tif (pin->request_count <= 1) {\n\t\t\t\tpin->request_count = 0;\n\t\t\t\tgpiod_put(pin->gpiod);\n\t\t\t\tlist_del(&pin->list);\n\t\t\t\tkfree(pin);\n\t\t\t} else {\n\t\t\t\tpin->request_count--;\n\t\t\t}\n\t\t}\n\t}\n}",
        "code_after_change": "static void regulator_ena_gpio_free(struct regulator_dev *rdev)\n{\n\tstruct regulator_enable_gpio *pin, *n;\n\n\tif (!rdev->ena_pin)\n\t\treturn;\n\n\t/* Free the GPIO only in case of no use */\n\tlist_for_each_entry_safe(pin, n, &regulator_ena_gpio_list, list) {\n\t\tif (pin->gpiod == rdev->ena_pin->gpiod) {\n\t\t\tif (pin->request_count <= 1) {\n\t\t\t\tpin->request_count = 0;\n\t\t\t\tgpiod_put(pin->gpiod);\n\t\t\t\tlist_del(&pin->list);\n\t\t\t\tkfree(pin);\n\t\t\t\trdev->ena_pin = NULL;\n\t\t\t\treturn;\n\t\t\t} else {\n\t\t\t\tpin->request_count--;\n\t\t\t}\n\t\t}\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,8 @@\n \t\t\t\tgpiod_put(pin->gpiod);\n \t\t\t\tlist_del(&pin->list);\n \t\t\t\tkfree(pin);\n+\t\t\t\trdev->ena_pin = NULL;\n+\t\t\t\treturn;\n \t\t\t} else {\n \t\t\t\tpin->request_count--;\n \t\t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\t\trdev->ena_pin = NULL;",
                "\t\t\t\treturn;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The regulator_ena_gpio_free function in drivers/regulator/core.c in the Linux kernel before 3.19 allows local users to gain privileges or cause a denial of service (use-after-free) via a crafted application.",
        "id": 714
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int\nget_entries(struct net *net, struct ip6t_get_entries __user *uptr,\n\t    const int *len)\n{\n\tint ret;\n\tstruct ip6t_get_entries get;\n\tstruct xt_table *t;\n\n\tif (*len < sizeof(get))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&get, uptr, sizeof(get)) != 0)\n\t\treturn -EFAULT;\n\tif (*len != sizeof(struct ip6t_get_entries) + get.size)\n\t\treturn -EINVAL;\n\n\tget.name[sizeof(get.name) - 1] = '\\0';\n\n\tt = xt_find_table_lock(net, AF_INET6, get.name);\n\tif (!IS_ERR(t)) {\n\t\tstruct xt_table_info *private = t->private;\n\t\tif (get.size == private->size)\n\t\t\tret = copy_entries_to_user(private->size,\n\t\t\t\t\t\t   t, uptr->entrytable);\n\t\telse\n\t\t\tret = -EAGAIN;\n\n\t\tmodule_put(t->me);\n\t\txt_table_unlock(t);\n\t} else\n\t\tret = PTR_ERR(t);\n\n\treturn ret;\n}",
        "code_after_change": "static int\nget_entries(struct net *net, struct ip6t_get_entries __user *uptr,\n\t    const int *len)\n{\n\tint ret;\n\tstruct ip6t_get_entries get;\n\tstruct xt_table *t;\n\n\tif (*len < sizeof(get))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&get, uptr, sizeof(get)) != 0)\n\t\treturn -EFAULT;\n\tif (*len != sizeof(struct ip6t_get_entries) + get.size)\n\t\treturn -EINVAL;\n\n\tget.name[sizeof(get.name) - 1] = '\\0';\n\n\tt = xt_find_table_lock(net, AF_INET6, get.name);\n\tif (!IS_ERR(t)) {\n\t\tstruct xt_table_info *private = xt_table_get_private_protected(t);\n\t\tif (get.size == private->size)\n\t\t\tret = copy_entries_to_user(private->size,\n\t\t\t\t\t\t   t, uptr->entrytable);\n\t\telse\n\t\t\tret = -EAGAIN;\n\n\t\tmodule_put(t->me);\n\t\txt_table_unlock(t);\n\t} else\n\t\tret = PTR_ERR(t);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,7 +17,7 @@\n \n \tt = xt_find_table_lock(net, AF_INET6, get.name);\n \tif (!IS_ERR(t)) {\n-\t\tstruct xt_table_info *private = t->private;\n+\t\tstruct xt_table_info *private = xt_table_get_private_protected(t);\n \t\tif (get.size == private->size)\n \t\t\tret = copy_entries_to_user(private->size,\n \t\t\t\t\t\t   t, uptr->entrytable);",
        "function_modified_lines": {
            "added": [
                "\t\tstruct xt_table_info *private = xt_table_get_private_protected(t);"
            ],
            "deleted": [
                "\t\tstruct xt_table_info *private = t->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2786
    },
    {
        "cve_id": "CVE-2023-39198",
        "code_before_change": "int qxl_alloc_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)\n{\n\tstruct qxl_device *qdev = to_qxl(dev);\n\tstruct drm_qxl_alloc *qxl_alloc = data;\n\tint ret;\n\tstruct qxl_bo *qobj;\n\tuint32_t handle;\n\tu32 domain = QXL_GEM_DOMAIN_VRAM;\n\n\tif (qxl_alloc->size == 0) {\n\t\tDRM_ERROR(\"invalid size %d\\n\", qxl_alloc->size);\n\t\treturn -EINVAL;\n\t}\n\tret = qxl_gem_object_create_with_handle(qdev, file_priv,\n\t\t\t\t\t\tdomain,\n\t\t\t\t\t\tqxl_alloc->size,\n\t\t\t\t\t\tNULL,\n\t\t\t\t\t\t&qobj, &handle);\n\tif (ret) {\n\t\tDRM_ERROR(\"%s: failed to create gem ret=%d\\n\",\n\t\t\t  __func__, ret);\n\t\treturn -ENOMEM;\n\t}\n\tqxl_alloc->handle = handle;\n\treturn 0;\n}",
        "code_after_change": "int qxl_alloc_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)\n{\n\tstruct qxl_device *qdev = to_qxl(dev);\n\tstruct drm_qxl_alloc *qxl_alloc = data;\n\tint ret;\n\tuint32_t handle;\n\tu32 domain = QXL_GEM_DOMAIN_VRAM;\n\n\tif (qxl_alloc->size == 0) {\n\t\tDRM_ERROR(\"invalid size %d\\n\", qxl_alloc->size);\n\t\treturn -EINVAL;\n\t}\n\tret = qxl_gem_object_create_with_handle(qdev, file_priv,\n\t\t\t\t\t\tdomain,\n\t\t\t\t\t\tqxl_alloc->size,\n\t\t\t\t\t\tNULL,\n\t\t\t\t\t\tNULL, &handle);\n\tif (ret) {\n\t\tDRM_ERROR(\"%s: failed to create gem ret=%d\\n\",\n\t\t\t  __func__, ret);\n\t\treturn -ENOMEM;\n\t}\n\tqxl_alloc->handle = handle;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,6 @@\n \tstruct qxl_device *qdev = to_qxl(dev);\n \tstruct drm_qxl_alloc *qxl_alloc = data;\n \tint ret;\n-\tstruct qxl_bo *qobj;\n \tuint32_t handle;\n \tu32 domain = QXL_GEM_DOMAIN_VRAM;\n \n@@ -15,7 +14,7 @@\n \t\t\t\t\t\tdomain,\n \t\t\t\t\t\tqxl_alloc->size,\n \t\t\t\t\t\tNULL,\n-\t\t\t\t\t\t&qobj, &handle);\n+\t\t\t\t\t\tNULL, &handle);\n \tif (ret) {\n \t\tDRM_ERROR(\"%s: failed to create gem ret=%d\\n\",\n \t\t\t  __func__, ret);",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\t\tNULL, &handle);"
            ],
            "deleted": [
                "\tstruct qxl_bo *qobj;",
                "\t\t\t\t\t\t&qobj, &handle);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A race condition was found in the QXL driver in the Linux kernel. The qxl_mode_dumb_create() function dereferences the qobj returned by the qxl_gem_object_create_with_handle(), but the handle is the only one holding a reference to it. This flaw allows an attacker to guess the returned handle value and trigger a use-after-free issue, potentially leading to a denial of service or privilege escalation.",
        "id": 4187
    },
    {
        "cve_id": "CVE-2019-19319",
        "code_before_change": "int ext4_setup_system_zone(struct super_block *sb)\n{\n\text4_group_t ngroups = ext4_get_groups_count(sb);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp;\n\text4_group_t i;\n\tint flex_size = ext4_flex_bg_size(sbi);\n\tint ret;\n\n\tif (!test_opt(sb, BLOCK_VALIDITY)) {\n\t\tif (sbi->system_blks.rb_node)\n\t\t\text4_release_system_zone(sb);\n\t\treturn 0;\n\t}\n\tif (sbi->system_blks.rb_node)\n\t\treturn 0;\n\n\tfor (i=0; i < ngroups; i++) {\n\t\tif (ext4_bg_has_super(sb, i) &&\n\t\t    ((i < 5) || ((i % flex_size) == 0)))\n\t\t\tadd_system_zone(sbi, ext4_group_first_block_no(sb, i),\n\t\t\t\t\text4_bg_num_gdb(sb, i) + 1);\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\t\tret = add_system_zone(sbi, ext4_block_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_table(sb, gdp),\n\t\t\t\tsbi->s_itb_per_group);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (test_opt(sb, DEBUG))\n\t\tdebug_print_tree(sbi);\n\treturn 0;\n}",
        "code_after_change": "int ext4_setup_system_zone(struct super_block *sb)\n{\n\text4_group_t ngroups = ext4_get_groups_count(sb);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp;\n\text4_group_t i;\n\tint flex_size = ext4_flex_bg_size(sbi);\n\tint ret;\n\n\tif (!test_opt(sb, BLOCK_VALIDITY)) {\n\t\tif (sbi->system_blks.rb_node)\n\t\t\text4_release_system_zone(sb);\n\t\treturn 0;\n\t}\n\tif (sbi->system_blks.rb_node)\n\t\treturn 0;\n\n\tfor (i=0; i < ngroups; i++) {\n\t\tif (ext4_bg_has_super(sb, i) &&\n\t\t    ((i < 5) || ((i % flex_size) == 0)))\n\t\t\tadd_system_zone(sbi, ext4_group_first_block_no(sb, i),\n\t\t\t\t\text4_bg_num_gdb(sb, i) + 1);\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\t\tret = add_system_zone(sbi, ext4_block_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_table(sb, gdp),\n\t\t\t\tsbi->s_itb_per_group);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tif (ext4_has_feature_journal(sb) && sbi->s_es->s_journal_inum) {\n\t\tret = ext4_protect_reserved_inode(sb,\n\t\t\t\tle32_to_cpu(sbi->s_es->s_journal_inum));\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (test_opt(sb, DEBUG))\n\t\tdebug_print_tree(sbi);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -32,6 +32,12 @@\n \t\tif (ret)\n \t\t\treturn ret;\n \t}\n+\tif (ext4_has_feature_journal(sb) && sbi->s_es->s_journal_inum) {\n+\t\tret = ext4_protect_reserved_inode(sb,\n+\t\t\t\tle32_to_cpu(sbi->s_es->s_journal_inum));\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t}\n \n \tif (test_opt(sb, DEBUG))\n \t\tdebug_print_tree(sbi);",
        "function_modified_lines": {
            "added": [
                "\tif (ext4_has_feature_journal(sb) && sbi->s_es->s_journal_inum) {",
                "\t\tret = ext4_protect_reserved_inode(sb,",
                "\t\t\t\tle32_to_cpu(sbi->s_es->s_journal_inum));",
                "\t\tif (ret)",
                "\t\t\treturn ret;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-787",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.2, a setxattr operation, after a mount of a crafted ext4 image, can cause a slab-out-of-bounds write access because of an ext4_xattr_set_entry use-after-free in fs/ext4/xattr.c when a large old_size value is used in a memset call, aka CID-345c0dbf3a30.",
        "id": 2188
    },
    {
        "cve_id": "CVE-2022-1998",
        "code_before_change": "static ssize_t copy_event_to_user(struct fsnotify_group *group,\n\t\t\t\t  struct fanotify_event *event,\n\t\t\t\t  char __user *buf, size_t count)\n{\n\tstruct fanotify_event_metadata metadata;\n\tstruct path *path = fanotify_event_path(event);\n\tstruct fanotify_info *info = fanotify_event_info(event);\n\tunsigned int info_mode = FAN_GROUP_FLAG(group, FANOTIFY_INFO_MODES);\n\tunsigned int pidfd_mode = info_mode & FAN_REPORT_PIDFD;\n\tstruct file *f = NULL;\n\tint ret, pidfd = FAN_NOPIDFD, fd = FAN_NOFD;\n\n\tpr_debug(\"%s: group=%p event=%p\\n\", __func__, group, event);\n\n\tmetadata.event_len = fanotify_event_len(info_mode, event);\n\tmetadata.metadata_len = FAN_EVENT_METADATA_LEN;\n\tmetadata.vers = FANOTIFY_METADATA_VERSION;\n\tmetadata.reserved = 0;\n\tmetadata.mask = event->mask & FANOTIFY_OUTGOING_EVENTS;\n\tmetadata.pid = pid_vnr(event->pid);\n\t/*\n\t * For an unprivileged listener, event->pid can be used to identify the\n\t * events generated by the listener process itself, without disclosing\n\t * the pids of other processes.\n\t */\n\tif (FAN_GROUP_FLAG(group, FANOTIFY_UNPRIV) &&\n\t    task_tgid(current) != event->pid)\n\t\tmetadata.pid = 0;\n\n\t/*\n\t * For now, fid mode is required for an unprivileged listener and\n\t * fid mode does not report fd in events.  Keep this check anyway\n\t * for safety in case fid mode requirement is relaxed in the future\n\t * to allow unprivileged listener to get events with no fd and no fid.\n\t */\n\tif (!FAN_GROUP_FLAG(group, FANOTIFY_UNPRIV) &&\n\t    path && path->mnt && path->dentry) {\n\t\tfd = create_fd(group, path, &f);\n\t\tif (fd < 0)\n\t\t\treturn fd;\n\t}\n\tmetadata.fd = fd;\n\n\tif (pidfd_mode) {\n\t\t/*\n\t\t * Complain if the FAN_REPORT_PIDFD and FAN_REPORT_TID mutual\n\t\t * exclusion is ever lifted. At the time of incoporating pidfd\n\t\t * support within fanotify, the pidfd API only supported the\n\t\t * creation of pidfds for thread-group leaders.\n\t\t */\n\t\tWARN_ON_ONCE(FAN_GROUP_FLAG(group, FAN_REPORT_TID));\n\n\t\t/*\n\t\t * The PIDTYPE_TGID check for an event->pid is performed\n\t\t * preemptively in an attempt to catch out cases where the event\n\t\t * listener reads events after the event generating process has\n\t\t * already terminated. Report FAN_NOPIDFD to the event listener\n\t\t * in those cases, with all other pidfd creation errors being\n\t\t * reported as FAN_EPIDFD.\n\t\t */\n\t\tif (metadata.pid == 0 ||\n\t\t    !pid_has_task(event->pid, PIDTYPE_TGID)) {\n\t\t\tpidfd = FAN_NOPIDFD;\n\t\t} else {\n\t\t\tpidfd = pidfd_create(event->pid, 0);\n\t\t\tif (pidfd < 0)\n\t\t\t\tpidfd = FAN_EPIDFD;\n\t\t}\n\t}\n\n\tret = -EFAULT;\n\t/*\n\t * Sanity check copy size in case get_one_event() and\n\t * event_len sizes ever get out of sync.\n\t */\n\tif (WARN_ON_ONCE(metadata.event_len > count))\n\t\tgoto out_close_fd;\n\n\tif (copy_to_user(buf, &metadata, FAN_EVENT_METADATA_LEN))\n\t\tgoto out_close_fd;\n\n\tbuf += FAN_EVENT_METADATA_LEN;\n\tcount -= FAN_EVENT_METADATA_LEN;\n\n\tif (fanotify_is_perm_event(event->mask))\n\t\tFANOTIFY_PERM(event)->fd = fd;\n\n\tif (f)\n\t\tfd_install(fd, f);\n\n\tif (info_mode) {\n\t\tret = copy_info_records_to_user(event, info, info_mode, pidfd,\n\t\t\t\t\t\tbuf, count);\n\t\tif (ret < 0)\n\t\t\tgoto out_close_fd;\n\t}\n\n\treturn metadata.event_len;\n\nout_close_fd:\n\tif (fd != FAN_NOFD) {\n\t\tput_unused_fd(fd);\n\t\tfput(f);\n\t}\n\n\tif (pidfd >= 0)\n\t\tclose_fd(pidfd);\n\n\treturn ret;\n}",
        "code_after_change": "static ssize_t copy_event_to_user(struct fsnotify_group *group,\n\t\t\t\t  struct fanotify_event *event,\n\t\t\t\t  char __user *buf, size_t count)\n{\n\tstruct fanotify_event_metadata metadata;\n\tstruct path *path = fanotify_event_path(event);\n\tstruct fanotify_info *info = fanotify_event_info(event);\n\tunsigned int info_mode = FAN_GROUP_FLAG(group, FANOTIFY_INFO_MODES);\n\tunsigned int pidfd_mode = info_mode & FAN_REPORT_PIDFD;\n\tstruct file *f = NULL;\n\tint ret, pidfd = FAN_NOPIDFD, fd = FAN_NOFD;\n\n\tpr_debug(\"%s: group=%p event=%p\\n\", __func__, group, event);\n\n\tmetadata.event_len = fanotify_event_len(info_mode, event);\n\tmetadata.metadata_len = FAN_EVENT_METADATA_LEN;\n\tmetadata.vers = FANOTIFY_METADATA_VERSION;\n\tmetadata.reserved = 0;\n\tmetadata.mask = event->mask & FANOTIFY_OUTGOING_EVENTS;\n\tmetadata.pid = pid_vnr(event->pid);\n\t/*\n\t * For an unprivileged listener, event->pid can be used to identify the\n\t * events generated by the listener process itself, without disclosing\n\t * the pids of other processes.\n\t */\n\tif (FAN_GROUP_FLAG(group, FANOTIFY_UNPRIV) &&\n\t    task_tgid(current) != event->pid)\n\t\tmetadata.pid = 0;\n\n\t/*\n\t * For now, fid mode is required for an unprivileged listener and\n\t * fid mode does not report fd in events.  Keep this check anyway\n\t * for safety in case fid mode requirement is relaxed in the future\n\t * to allow unprivileged listener to get events with no fd and no fid.\n\t */\n\tif (!FAN_GROUP_FLAG(group, FANOTIFY_UNPRIV) &&\n\t    path && path->mnt && path->dentry) {\n\t\tfd = create_fd(group, path, &f);\n\t\tif (fd < 0)\n\t\t\treturn fd;\n\t}\n\tmetadata.fd = fd;\n\n\tif (pidfd_mode) {\n\t\t/*\n\t\t * Complain if the FAN_REPORT_PIDFD and FAN_REPORT_TID mutual\n\t\t * exclusion is ever lifted. At the time of incoporating pidfd\n\t\t * support within fanotify, the pidfd API only supported the\n\t\t * creation of pidfds for thread-group leaders.\n\t\t */\n\t\tWARN_ON_ONCE(FAN_GROUP_FLAG(group, FAN_REPORT_TID));\n\n\t\t/*\n\t\t * The PIDTYPE_TGID check for an event->pid is performed\n\t\t * preemptively in an attempt to catch out cases where the event\n\t\t * listener reads events after the event generating process has\n\t\t * already terminated. Report FAN_NOPIDFD to the event listener\n\t\t * in those cases, with all other pidfd creation errors being\n\t\t * reported as FAN_EPIDFD.\n\t\t */\n\t\tif (metadata.pid == 0 ||\n\t\t    !pid_has_task(event->pid, PIDTYPE_TGID)) {\n\t\t\tpidfd = FAN_NOPIDFD;\n\t\t} else {\n\t\t\tpidfd = pidfd_create(event->pid, 0);\n\t\t\tif (pidfd < 0)\n\t\t\t\tpidfd = FAN_EPIDFD;\n\t\t}\n\t}\n\n\tret = -EFAULT;\n\t/*\n\t * Sanity check copy size in case get_one_event() and\n\t * event_len sizes ever get out of sync.\n\t */\n\tif (WARN_ON_ONCE(metadata.event_len > count))\n\t\tgoto out_close_fd;\n\n\tif (copy_to_user(buf, &metadata, FAN_EVENT_METADATA_LEN))\n\t\tgoto out_close_fd;\n\n\tbuf += FAN_EVENT_METADATA_LEN;\n\tcount -= FAN_EVENT_METADATA_LEN;\n\n\tif (fanotify_is_perm_event(event->mask))\n\t\tFANOTIFY_PERM(event)->fd = fd;\n\n\tif (info_mode) {\n\t\tret = copy_info_records_to_user(event, info, info_mode, pidfd,\n\t\t\t\t\t\tbuf, count);\n\t\tif (ret < 0)\n\t\t\tgoto out_close_fd;\n\t}\n\n\tif (f)\n\t\tfd_install(fd, f);\n\n\treturn metadata.event_len;\n\nout_close_fd:\n\tif (fd != FAN_NOFD) {\n\t\tput_unused_fd(fd);\n\t\tfput(f);\n\t}\n\n\tif (pidfd >= 0)\n\t\tclose_fd(pidfd);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -85,15 +85,15 @@\n \tif (fanotify_is_perm_event(event->mask))\n \t\tFANOTIFY_PERM(event)->fd = fd;\n \n-\tif (f)\n-\t\tfd_install(fd, f);\n-\n \tif (info_mode) {\n \t\tret = copy_info_records_to_user(event, info, info_mode, pidfd,\n \t\t\t\t\t\tbuf, count);\n \t\tif (ret < 0)\n \t\t\tgoto out_close_fd;\n \t}\n+\n+\tif (f)\n+\t\tfd_install(fd, f);\n \n \treturn metadata.event_len;\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (f)",
                "\t\tfd_install(fd, f);"
            ],
            "deleted": [
                "\tif (f)",
                "\t\tfd_install(fd, f);",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel File System notify functionality was found in the way user triggers copy_info_records_to_user() call to fail in copy_event_to_user(). A local user could use this flaw to crash the system or potentially escalate their privileges on the system.",
        "id": 3330
    },
    {
        "cve_id": "CVE-2016-10906",
        "code_before_change": "static int arc_emac_tx(struct sk_buff *skb, struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tunsigned int len, *txbd_curr = &priv->txbd_curr;\n\tstruct net_device_stats *stats = &ndev->stats;\n\t__le32 *info = &priv->txbd[*txbd_curr].info;\n\tdma_addr_t addr;\n\n\tif (skb_padto(skb, ETH_ZLEN))\n\t\treturn NETDEV_TX_OK;\n\n\tlen = max_t(unsigned int, ETH_ZLEN, skb->len);\n\n\tif (unlikely(!arc_emac_tx_avail(priv))) {\n\t\tnetif_stop_queue(ndev);\n\t\tnetdev_err(ndev, \"BUG! Tx Ring full when queue awake!\\n\");\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\taddr = dma_map_single(&ndev->dev, (void *)skb->data, len,\n\t\t\t      DMA_TO_DEVICE);\n\n\tif (unlikely(dma_mapping_error(&ndev->dev, addr))) {\n\t\tstats->tx_dropped++;\n\t\tstats->tx_errors++;\n\t\tdev_kfree_skb(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\tdma_unmap_addr_set(&priv->tx_buff[*txbd_curr], addr, addr);\n\tdma_unmap_len_set(&priv->tx_buff[*txbd_curr], len, len);\n\n\tpriv->tx_buff[*txbd_curr].skb = skb;\n\tpriv->txbd[*txbd_curr].data = cpu_to_le32(addr);\n\n\t/* Make sure pointer to data buffer is set */\n\twmb();\n\n\tskb_tx_timestamp(skb);\n\n\t*info = cpu_to_le32(FOR_EMAC | FIRST_OR_LAST_MASK | len);\n\n\t/* Increment index to point to the next BD */\n\t*txbd_curr = (*txbd_curr + 1) % TX_BD_NUM;\n\n\t/* Ensure that tx_clean() sees the new txbd_curr before\n\t * checking the queue status. This prevents an unneeded wake\n\t * of the queue in tx_clean().\n\t */\n\tsmp_mb();\n\n\tif (!arc_emac_tx_avail(priv)) {\n\t\tnetif_stop_queue(ndev);\n\t\t/* Refresh tx_dirty */\n\t\tsmp_mb();\n\t\tif (arc_emac_tx_avail(priv))\n\t\t\tnetif_start_queue(ndev);\n\t}\n\n\tarc_reg_set(priv, R_STATUS, TXPL_MASK);\n\n\treturn NETDEV_TX_OK;\n}",
        "code_after_change": "static int arc_emac_tx(struct sk_buff *skb, struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tunsigned int len, *txbd_curr = &priv->txbd_curr;\n\tstruct net_device_stats *stats = &ndev->stats;\n\t__le32 *info = &priv->txbd[*txbd_curr].info;\n\tdma_addr_t addr;\n\n\tif (skb_padto(skb, ETH_ZLEN))\n\t\treturn NETDEV_TX_OK;\n\n\tlen = max_t(unsigned int, ETH_ZLEN, skb->len);\n\n\tif (unlikely(!arc_emac_tx_avail(priv))) {\n\t\tnetif_stop_queue(ndev);\n\t\tnetdev_err(ndev, \"BUG! Tx Ring full when queue awake!\\n\");\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\taddr = dma_map_single(&ndev->dev, (void *)skb->data, len,\n\t\t\t      DMA_TO_DEVICE);\n\n\tif (unlikely(dma_mapping_error(&ndev->dev, addr))) {\n\t\tstats->tx_dropped++;\n\t\tstats->tx_errors++;\n\t\tdev_kfree_skb(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\tdma_unmap_addr_set(&priv->tx_buff[*txbd_curr], addr, addr);\n\tdma_unmap_len_set(&priv->tx_buff[*txbd_curr], len, len);\n\n\tpriv->txbd[*txbd_curr].data = cpu_to_le32(addr);\n\n\t/* Make sure pointer to data buffer is set */\n\twmb();\n\n\tskb_tx_timestamp(skb);\n\n\t*info = cpu_to_le32(FOR_EMAC | FIRST_OR_LAST_MASK | len);\n\n\t/* Make sure info word is set */\n\twmb();\n\n\tpriv->tx_buff[*txbd_curr].skb = skb;\n\n\t/* Increment index to point to the next BD */\n\t*txbd_curr = (*txbd_curr + 1) % TX_BD_NUM;\n\n\t/* Ensure that tx_clean() sees the new txbd_curr before\n\t * checking the queue status. This prevents an unneeded wake\n\t * of the queue in tx_clean().\n\t */\n\tsmp_mb();\n\n\tif (!arc_emac_tx_avail(priv)) {\n\t\tnetif_stop_queue(ndev);\n\t\t/* Refresh tx_dirty */\n\t\tsmp_mb();\n\t\tif (arc_emac_tx_avail(priv))\n\t\t\tnetif_start_queue(ndev);\n\t}\n\n\tarc_reg_set(priv, R_STATUS, TXPL_MASK);\n\n\treturn NETDEV_TX_OK;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,7 +29,6 @@\n \tdma_unmap_addr_set(&priv->tx_buff[*txbd_curr], addr, addr);\n \tdma_unmap_len_set(&priv->tx_buff[*txbd_curr], len, len);\n \n-\tpriv->tx_buff[*txbd_curr].skb = skb;\n \tpriv->txbd[*txbd_curr].data = cpu_to_le32(addr);\n \n \t/* Make sure pointer to data buffer is set */\n@@ -38,6 +37,11 @@\n \tskb_tx_timestamp(skb);\n \n \t*info = cpu_to_le32(FOR_EMAC | FIRST_OR_LAST_MASK | len);\n+\n+\t/* Make sure info word is set */\n+\twmb();\n+\n+\tpriv->tx_buff[*txbd_curr].skb = skb;\n \n \t/* Increment index to point to the next BD */\n \t*txbd_curr = (*txbd_curr + 1) % TX_BD_NUM;",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* Make sure info word is set */",
                "\twmb();",
                "",
                "\tpriv->tx_buff[*txbd_curr].skb = skb;"
            ],
            "deleted": [
                "\tpriv->tx_buff[*txbd_curr].skb = skb;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in drivers/net/ethernet/arc/emac_main.c in the Linux kernel before 4.5. A use-after-free is caused by a race condition between the functions arc_emac_tx and arc_emac_tx_clean.",
        "id": 909
    },
    {
        "cve_id": "CVE-2022-1055",
        "code_before_change": "static int tc_ctl_chain(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tunsigned long cl;\n\tint err;\n\n\tif (n->nlmsg_type != RTM_GETCHAIN &&\n\t    !netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\terr = nlmsg_parse_deprecated(n, sizeof(*t), tca, TCA_MAX,\n\t\t\t\t     rtm_tca_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block))\n\t\treturn PTR_ERR(block);\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout_block;\n\t}\n\n\tmutex_lock(&block->lock);\n\tchain = tcf_chain_lookup(block, chain_index);\n\tif (n->nlmsg_type == RTM_NEWCHAIN) {\n\t\tif (chain) {\n\t\t\tif (tcf_chain_held_by_acts_only(chain)) {\n\t\t\t\t/* The chain exists only because there is\n\t\t\t\t * some action referencing it.\n\t\t\t\t */\n\t\t\t\ttcf_chain_hold(chain);\n\t\t\t} else {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Filter chain already exists\");\n\t\t\t\terr = -EEXIST;\n\t\t\t\tgoto errout_block_locked;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWCHAIN and NLM_F_CREATE to create a new chain\");\n\t\t\t\terr = -ENOENT;\n\t\t\t\tgoto errout_block_locked;\n\t\t\t}\n\t\t\tchain = tcf_chain_create(block, chain_index);\n\t\t\tif (!chain) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Failed to create filter chain\");\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto errout_block_locked;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!chain || tcf_chain_held_by_acts_only(chain)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Cannot find specified filter chain\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_block_locked;\n\t\t}\n\t\ttcf_chain_hold(chain);\n\t}\n\n\tif (n->nlmsg_type == RTM_NEWCHAIN) {\n\t\t/* Modifying chain requires holding parent block lock. In case\n\t\t * the chain was successfully added, take a reference to the\n\t\t * chain. This ensures that an empty chain does not disappear at\n\t\t * the end of this function.\n\t\t */\n\t\ttcf_chain_hold(chain);\n\t\tchain->explicitly_created = true;\n\t}\n\tmutex_unlock(&block->lock);\n\n\tswitch (n->nlmsg_type) {\n\tcase RTM_NEWCHAIN:\n\t\terr = tc_chain_tmplt_add(chain, net, tca, extack);\n\t\tif (err) {\n\t\t\ttcf_chain_put_explicitly_created(chain);\n\t\t\tgoto errout;\n\t\t}\n\n\t\ttc_chain_notify(chain, NULL, 0, NLM_F_CREATE | NLM_F_EXCL,\n\t\t\t\tRTM_NEWCHAIN, false);\n\t\tbreak;\n\tcase RTM_DELCHAIN:\n\t\ttfilter_notify_chain(net, skb, block, q, parent, n,\n\t\t\t\t     chain, RTM_DELTFILTER);\n\t\t/* Flush the chain first as the user requested chain removal. */\n\t\ttcf_chain_flush(chain, true);\n\t\t/* In case the chain was successfully deleted, put a reference\n\t\t * to the chain previously taken during addition.\n\t\t */\n\t\ttcf_chain_put_explicitly_created(chain);\n\t\tbreak;\n\tcase RTM_GETCHAIN:\n\t\terr = tc_chain_notify(chain, skb, n->nlmsg_seq,\n\t\t\t\t      n->nlmsg_flags, n->nlmsg_type, true);\n\t\tif (err < 0)\n\t\t\tNL_SET_ERR_MSG(extack, \"Failed to send chain notify message\");\n\t\tbreak;\n\tdefault:\n\t\terr = -EOPNOTSUPP;\n\t\tNL_SET_ERR_MSG(extack, \"Unsupported message type\");\n\t\tgoto errout;\n\t}\n\nerrout:\n\ttcf_chain_put(chain);\nerrout_block:\n\ttcf_block_release(q, block, true);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n\nerrout_block_locked:\n\tmutex_unlock(&block->lock);\n\tgoto errout_block;\n}",
        "code_after_change": "static int tc_ctl_chain(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q;\n\tstruct tcf_chain *chain;\n\tstruct tcf_block *block;\n\tunsigned long cl;\n\tint err;\n\n\tif (n->nlmsg_type != RTM_GETCHAIN &&\n\t    !netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\tq = NULL;\n\terr = nlmsg_parse_deprecated(n, sizeof(*t), tca, TCA_MAX,\n\t\t\t\t     rtm_tca_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block))\n\t\treturn PTR_ERR(block);\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout_block;\n\t}\n\n\tmutex_lock(&block->lock);\n\tchain = tcf_chain_lookup(block, chain_index);\n\tif (n->nlmsg_type == RTM_NEWCHAIN) {\n\t\tif (chain) {\n\t\t\tif (tcf_chain_held_by_acts_only(chain)) {\n\t\t\t\t/* The chain exists only because there is\n\t\t\t\t * some action referencing it.\n\t\t\t\t */\n\t\t\t\ttcf_chain_hold(chain);\n\t\t\t} else {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Filter chain already exists\");\n\t\t\t\terr = -EEXIST;\n\t\t\t\tgoto errout_block_locked;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWCHAIN and NLM_F_CREATE to create a new chain\");\n\t\t\t\terr = -ENOENT;\n\t\t\t\tgoto errout_block_locked;\n\t\t\t}\n\t\t\tchain = tcf_chain_create(block, chain_index);\n\t\t\tif (!chain) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Failed to create filter chain\");\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto errout_block_locked;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!chain || tcf_chain_held_by_acts_only(chain)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Cannot find specified filter chain\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_block_locked;\n\t\t}\n\t\ttcf_chain_hold(chain);\n\t}\n\n\tif (n->nlmsg_type == RTM_NEWCHAIN) {\n\t\t/* Modifying chain requires holding parent block lock. In case\n\t\t * the chain was successfully added, take a reference to the\n\t\t * chain. This ensures that an empty chain does not disappear at\n\t\t * the end of this function.\n\t\t */\n\t\ttcf_chain_hold(chain);\n\t\tchain->explicitly_created = true;\n\t}\n\tmutex_unlock(&block->lock);\n\n\tswitch (n->nlmsg_type) {\n\tcase RTM_NEWCHAIN:\n\t\terr = tc_chain_tmplt_add(chain, net, tca, extack);\n\t\tif (err) {\n\t\t\ttcf_chain_put_explicitly_created(chain);\n\t\t\tgoto errout;\n\t\t}\n\n\t\ttc_chain_notify(chain, NULL, 0, NLM_F_CREATE | NLM_F_EXCL,\n\t\t\t\tRTM_NEWCHAIN, false);\n\t\tbreak;\n\tcase RTM_DELCHAIN:\n\t\ttfilter_notify_chain(net, skb, block, q, parent, n,\n\t\t\t\t     chain, RTM_DELTFILTER);\n\t\t/* Flush the chain first as the user requested chain removal. */\n\t\ttcf_chain_flush(chain, true);\n\t\t/* In case the chain was successfully deleted, put a reference\n\t\t * to the chain previously taken during addition.\n\t\t */\n\t\ttcf_chain_put_explicitly_created(chain);\n\t\tbreak;\n\tcase RTM_GETCHAIN:\n\t\terr = tc_chain_notify(chain, skb, n->nlmsg_seq,\n\t\t\t\t      n->nlmsg_flags, n->nlmsg_type, true);\n\t\tif (err < 0)\n\t\t\tNL_SET_ERR_MSG(extack, \"Failed to send chain notify message\");\n\t\tbreak;\n\tdefault:\n\t\terr = -EOPNOTSUPP;\n\t\tNL_SET_ERR_MSG(extack, \"Unsupported message type\");\n\t\tgoto errout;\n\t}\n\nerrout:\n\ttcf_chain_put(chain);\nerrout_block:\n\ttcf_block_release(q, block, true);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n\nerrout_block_locked:\n\tmutex_unlock(&block->lock);\n\tgoto errout_block;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,8 @@\n \tstruct tcmsg *t;\n \tu32 parent;\n \tu32 chain_index;\n-\tstruct Qdisc *q = NULL;\n-\tstruct tcf_chain *chain = NULL;\n+\tstruct Qdisc *q;\n+\tstruct tcf_chain *chain;\n \tstruct tcf_block *block;\n \tunsigned long cl;\n \tint err;\n@@ -17,6 +17,7 @@\n \t\treturn -EPERM;\n \n replay:\n+\tq = NULL;\n \terr = nlmsg_parse_deprecated(n, sizeof(*t), tca, TCA_MAX,\n \t\t\t\t     rtm_tca_policy, extack);\n \tif (err < 0)",
        "function_modified_lines": {
            "added": [
                "\tstruct Qdisc *q;",
                "\tstruct tcf_chain *chain;",
                "\tq = NULL;"
            ],
            "deleted": [
                "\tstruct Qdisc *q = NULL;",
                "\tstruct tcf_chain *chain = NULL;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free exists in the Linux Kernel in tc_new_tfilter that could allow a local attacker to gain privilege escalation. The exploit requires unprivileged user namespaces. We recommend upgrading past commit 04c2a47ffb13c29778e2a14e414ad4cb5a5db4b5",
        "id": 3246
    },
    {
        "cve_id": "CVE-2017-0861",
        "code_before_change": "static int snd_pcm_control_ioctl(struct snd_card *card,\n\t\t\t\t struct snd_ctl_file *control,\n\t\t\t\t unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SNDRV_CTL_IOCTL_PCM_NEXT_DEVICE:\n\t\t{\n\t\t\tint device;\n\n\t\t\tif (get_user(device, (int __user *)arg))\n\t\t\t\treturn -EFAULT;\n\t\t\tmutex_lock(&register_mutex);\n\t\t\tdevice = snd_pcm_next(card, device);\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\tif (put_user(device, (int __user *)arg))\n\t\t\t\treturn -EFAULT;\n\t\t\treturn 0;\n\t\t}\n\tcase SNDRV_CTL_IOCTL_PCM_INFO:\n\t\t{\n\t\t\tstruct snd_pcm_info __user *info;\n\t\t\tunsigned int device, subdevice;\n\t\t\tint stream;\n\t\t\tstruct snd_pcm *pcm;\n\t\t\tstruct snd_pcm_str *pstr;\n\t\t\tstruct snd_pcm_substream *substream;\n\t\t\tint err;\n\n\t\t\tinfo = (struct snd_pcm_info __user *)arg;\n\t\t\tif (get_user(device, &info->device))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (get_user(stream, &info->stream))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (stream < 0 || stream > 1)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (get_user(subdevice, &info->subdevice))\n\t\t\t\treturn -EFAULT;\n\t\t\tmutex_lock(&register_mutex);\n\t\t\tpcm = snd_pcm_get(card, device);\n\t\t\tif (pcm == NULL) {\n\t\t\t\terr = -ENXIO;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\tpstr = &pcm->streams[stream];\n\t\t\tif (pstr->substream_count == 0) {\n\t\t\t\terr = -ENOENT;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\tif (subdevice >= pstr->substream_count) {\n\t\t\t\terr = -ENXIO;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\tfor (substream = pstr->substream; substream;\n\t\t\t     substream = substream->next)\n\t\t\t\tif (substream->number == (int)subdevice)\n\t\t\t\t\tbreak;\n\t\t\tif (substream == NULL) {\n\t\t\t\terr = -ENXIO;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\terr = snd_pcm_info_user(substream, info);\n\t\t_error:\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\treturn err;\n\t\t}\n\tcase SNDRV_CTL_IOCTL_PCM_PREFER_SUBDEVICE:\n\t\t{\n\t\t\tint val;\n\t\t\t\n\t\t\tif (get_user(val, (int __user *)arg))\n\t\t\t\treturn -EFAULT;\n\t\t\tcontrol->preferred_subdevice[SND_CTL_SUBDEV_PCM] = val;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -ENOIOCTLCMD;\n}",
        "code_after_change": "static int snd_pcm_control_ioctl(struct snd_card *card,\n\t\t\t\t struct snd_ctl_file *control,\n\t\t\t\t unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SNDRV_CTL_IOCTL_PCM_NEXT_DEVICE:\n\t\t{\n\t\t\tint device;\n\n\t\t\tif (get_user(device, (int __user *)arg))\n\t\t\t\treturn -EFAULT;\n\t\t\tmutex_lock(&register_mutex);\n\t\t\tdevice = snd_pcm_next(card, device);\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\tif (put_user(device, (int __user *)arg))\n\t\t\t\treturn -EFAULT;\n\t\t\treturn 0;\n\t\t}\n\tcase SNDRV_CTL_IOCTL_PCM_INFO:\n\t\t{\n\t\t\tstruct snd_pcm_info __user *info;\n\t\t\tunsigned int device, subdevice;\n\t\t\tint stream;\n\t\t\tstruct snd_pcm *pcm;\n\t\t\tstruct snd_pcm_str *pstr;\n\t\t\tstruct snd_pcm_substream *substream;\n\t\t\tint err;\n\n\t\t\tinfo = (struct snd_pcm_info __user *)arg;\n\t\t\tif (get_user(device, &info->device))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (get_user(stream, &info->stream))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (stream < 0 || stream > 1)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (get_user(subdevice, &info->subdevice))\n\t\t\t\treturn -EFAULT;\n\t\t\tmutex_lock(&register_mutex);\n\t\t\tpcm = snd_pcm_get(card, device);\n\t\t\tif (pcm == NULL) {\n\t\t\t\terr = -ENXIO;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\tpstr = &pcm->streams[stream];\n\t\t\tif (pstr->substream_count == 0) {\n\t\t\t\terr = -ENOENT;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\tif (subdevice >= pstr->substream_count) {\n\t\t\t\terr = -ENXIO;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\tfor (substream = pstr->substream; substream;\n\t\t\t     substream = substream->next)\n\t\t\t\tif (substream->number == (int)subdevice)\n\t\t\t\t\tbreak;\n\t\t\tif (substream == NULL) {\n\t\t\t\terr = -ENXIO;\n\t\t\t\tgoto _error;\n\t\t\t}\n\t\t\tmutex_lock(&pcm->open_mutex);\n\t\t\terr = snd_pcm_info_user(substream, info);\n\t\t\tmutex_unlock(&pcm->open_mutex);\n\t\t_error:\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\treturn err;\n\t\t}\n\tcase SNDRV_CTL_IOCTL_PCM_PREFER_SUBDEVICE:\n\t\t{\n\t\t\tint val;\n\t\t\t\n\t\t\tif (get_user(val, (int __user *)arg))\n\t\t\t\treturn -EFAULT;\n\t\t\tcontrol->preferred_subdevice[SND_CTL_SUBDEV_PCM] = val;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -ENOIOCTLCMD;\n}",
        "patch": "--- code before\n+++ code after\n@@ -58,7 +58,9 @@\n \t\t\t\terr = -ENXIO;\n \t\t\t\tgoto _error;\n \t\t\t}\n+\t\t\tmutex_lock(&pcm->open_mutex);\n \t\t\terr = snd_pcm_info_user(substream, info);\n+\t\t\tmutex_unlock(&pcm->open_mutex);\n \t\t_error:\n \t\t\tmutex_unlock(&register_mutex);\n \t\t\treturn err;",
        "function_modified_lines": {
            "added": [
                "\t\t\tmutex_lock(&pcm->open_mutex);",
                "\t\t\tmutex_unlock(&pcm->open_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in the snd_pcm_info function in the ALSA subsystem in the Linux kernel allows attackers to gain privileges via unspecified vectors.",
        "id": 1175
    },
    {
        "cve_id": "CVE-2022-4696",
        "code_before_change": "static int __io_sq_thread_acquire_mm_files(struct io_ring_ctx *ctx,\n\t\t\t\t\t   struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tint ret;\n\n\tif (def->work_flags & IO_WQ_WORK_MM) {\n\t\tret = __io_sq_thread_acquire_mm(ctx);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\tif (def->needs_file || (def->work_flags & IO_WQ_WORK_FILES)) {\n\t\tret = __io_sq_thread_acquire_files(ctx);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int __io_sq_thread_acquire_mm_files(struct io_ring_ctx *ctx,\n\t\t\t\t\t   struct io_kiocb *req)\n{\n\tint ret;\n\n\tret = __io_sq_thread_acquire_mm(ctx);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tret = __io_sq_thread_acquire_files(ctx);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,20 +1,15 @@\n static int __io_sq_thread_acquire_mm_files(struct io_ring_ctx *ctx,\n \t\t\t\t\t   struct io_kiocb *req)\n {\n-\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n \tint ret;\n \n-\tif (def->work_flags & IO_WQ_WORK_MM) {\n-\t\tret = __io_sq_thread_acquire_mm(ctx);\n-\t\tif (unlikely(ret))\n-\t\t\treturn ret;\n-\t}\n+\tret = __io_sq_thread_acquire_mm(ctx);\n+\tif (unlikely(ret))\n+\t\treturn ret;\n \n-\tif (def->needs_file || (def->work_flags & IO_WQ_WORK_FILES)) {\n-\t\tret = __io_sq_thread_acquire_files(ctx);\n-\t\tif (unlikely(ret))\n-\t\t\treturn ret;\n-\t}\n+\tret = __io_sq_thread_acquire_files(ctx);\n+\tif (unlikely(ret))\n+\t\treturn ret;\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tret = __io_sq_thread_acquire_mm(ctx);",
                "\tif (unlikely(ret))",
                "\t\treturn ret;",
                "\tret = __io_sq_thread_acquire_files(ctx);",
                "\tif (unlikely(ret))",
                "\t\treturn ret;"
            ],
            "deleted": [
                "\tconst struct io_op_def *def = &io_op_defs[req->opcode];",
                "\tif (def->work_flags & IO_WQ_WORK_MM) {",
                "\t\tret = __io_sq_thread_acquire_mm(ctx);",
                "\t\tif (unlikely(ret))",
                "\t\t\treturn ret;",
                "\t}",
                "\tif (def->needs_file || (def->work_flags & IO_WQ_WORK_FILES)) {",
                "\t\tret = __io_sq_thread_acquire_files(ctx);",
                "\t\tif (unlikely(ret))",
                "\t\t\treturn ret;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There exists a use-after-free vulnerability in the Linux kernel through io_uring and the IORING_OP_SPLICE operation. If IORING_OP_SPLICE is missing the IO_WQ_WORK_FILES flag, which signals that the operation won't use current->nsproxy, so its reference counter is not increased. This assumption is not always true as calling io_splice on specific files will call the get_uts function which will use current->nsproxy leading to invalidly decreasing its reference counter later causing the use-after-free vulnerability. We recommend upgrading to version 5.10.160 or above\n",
        "id": 3761
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static void svm_range_evict_svm_bo_worker(struct work_struct *work)\n{\n\tstruct svm_range_bo *svm_bo;\n\tstruct mm_struct *mm;\n\tint r = 0;\n\n\tsvm_bo = container_of(work, struct svm_range_bo, eviction_work);\n\tif (!svm_bo_ref_unless_zero(svm_bo))\n\t\treturn; /* svm_bo was freed while eviction was pending */\n\n\tif (mmget_not_zero(svm_bo->eviction_fence->mm)) {\n\t\tmm = svm_bo->eviction_fence->mm;\n\t} else {\n\t\tsvm_range_bo_unref(svm_bo);\n\t\treturn;\n\t}\n\n\tmmap_read_lock(mm);\n\tspin_lock(&svm_bo->list_lock);\n\twhile (!list_empty(&svm_bo->range_list) && !r) {\n\t\tstruct svm_range *prange =\n\t\t\t\tlist_first_entry(&svm_bo->range_list,\n\t\t\t\t\t\tstruct svm_range, svm_bo_list);\n\t\tint retries = 3;\n\n\t\tlist_del_init(&prange->svm_bo_list);\n\t\tspin_unlock(&svm_bo->list_lock);\n\n\t\tpr_debug(\"svms 0x%p [0x%lx 0x%lx]\\n\", prange->svms,\n\t\t\t prange->start, prange->last);\n\n\t\tmutex_lock(&prange->migrate_mutex);\n\t\tdo {\n\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\t\tKFD_MIGRATE_TRIGGER_TTM_EVICTION);\n\t\t} while (!r && prange->actual_loc && --retries);\n\n\t\tif (!r && prange->actual_loc)\n\t\t\tpr_info_once(\"Migration failed during eviction\");\n\n\t\tif (!prange->actual_loc) {\n\t\t\tmutex_lock(&prange->lock);\n\t\t\tprange->svm_bo = NULL;\n\t\t\tmutex_unlock(&prange->lock);\n\t\t}\n\t\tmutex_unlock(&prange->migrate_mutex);\n\n\t\tspin_lock(&svm_bo->list_lock);\n\t}\n\tspin_unlock(&svm_bo->list_lock);\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\tdma_fence_signal(&svm_bo->eviction_fence->base);\n\n\t/* This is the last reference to svm_bo, after svm_range_vram_node_free\n\t * has been called in svm_migrate_vram_to_ram\n\t */\n\tWARN_ONCE(!r && kref_read(&svm_bo->kref) != 1, \"This was not the last reference\\n\");\n\tsvm_range_bo_unref(svm_bo);\n}",
        "code_after_change": "static void svm_range_evict_svm_bo_worker(struct work_struct *work)\n{\n\tstruct svm_range_bo *svm_bo;\n\tstruct mm_struct *mm;\n\tint r = 0;\n\n\tsvm_bo = container_of(work, struct svm_range_bo, eviction_work);\n\tif (!svm_bo_ref_unless_zero(svm_bo))\n\t\treturn; /* svm_bo was freed while eviction was pending */\n\n\tif (mmget_not_zero(svm_bo->eviction_fence->mm)) {\n\t\tmm = svm_bo->eviction_fence->mm;\n\t} else {\n\t\tsvm_range_bo_unref(svm_bo);\n\t\treturn;\n\t}\n\n\tmmap_read_lock(mm);\n\tspin_lock(&svm_bo->list_lock);\n\twhile (!list_empty(&svm_bo->range_list) && !r) {\n\t\tstruct svm_range *prange =\n\t\t\t\tlist_first_entry(&svm_bo->range_list,\n\t\t\t\t\t\tstruct svm_range, svm_bo_list);\n\t\tint retries = 3;\n\n\t\tlist_del_init(&prange->svm_bo_list);\n\t\tspin_unlock(&svm_bo->list_lock);\n\n\t\tpr_debug(\"svms 0x%p [0x%lx 0x%lx]\\n\", prange->svms,\n\t\t\t prange->start, prange->last);\n\n\t\tmutex_lock(&prange->migrate_mutex);\n\t\tdo {\n\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_TTM_EVICTION, NULL);\n\t\t} while (!r && prange->actual_loc && --retries);\n\n\t\tif (!r && prange->actual_loc)\n\t\t\tpr_info_once(\"Migration failed during eviction\");\n\n\t\tif (!prange->actual_loc) {\n\t\t\tmutex_lock(&prange->lock);\n\t\t\tprange->svm_bo = NULL;\n\t\t\tmutex_unlock(&prange->lock);\n\t\t}\n\t\tmutex_unlock(&prange->migrate_mutex);\n\n\t\tspin_lock(&svm_bo->list_lock);\n\t}\n\tspin_unlock(&svm_bo->list_lock);\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\tdma_fence_signal(&svm_bo->eviction_fence->base);\n\n\t/* This is the last reference to svm_bo, after svm_range_vram_node_free\n\t * has been called in svm_migrate_vram_to_ram\n\t */\n\tWARN_ONCE(!r && kref_read(&svm_bo->kref) != 1, \"This was not the last reference\\n\");\n\tsvm_range_bo_unref(svm_bo);\n}",
        "patch": "--- code before\n+++ code after\n@@ -32,7 +32,7 @@\n \t\tmutex_lock(&prange->migrate_mutex);\n \t\tdo {\n \t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n-\t\t\t\t\t\tKFD_MIGRATE_TRIGGER_TTM_EVICTION);\n+\t\t\t\t\tKFD_MIGRATE_TRIGGER_TTM_EVICTION, NULL);\n \t\t} while (!r && prange->actual_loc && --retries);\n \n \t\tif (!r && prange->actual_loc)",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t\tKFD_MIGRATE_TRIGGER_TTM_EVICTION, NULL);"
            ],
            "deleted": [
                "\t\t\t\t\t\tKFD_MIGRATE_TRIGGER_TTM_EVICTION);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3612
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "struct ipv6_txoptions *ipv6_update_options(struct sock *sk,\n\t\t\t\t\t   struct ipv6_txoptions *opt)\n{\n\tif (inet_sk(sk)->is_icsk) {\n\t\tif (opt &&\n\t\t    !((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t    inet_sk(sk)->inet_daddr != LOOPBACK4_IPV6) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t}\n\t}\n\topt = xchg(&inet6_sk(sk)->opt, opt);\n\tsk_dst_reset(sk);\n\n\treturn opt;\n}",
        "code_after_change": "struct ipv6_txoptions *ipv6_update_options(struct sock *sk,\n\t\t\t\t\t   struct ipv6_txoptions *opt)\n{\n\tif (inet_sk(sk)->is_icsk) {\n\t\tif (opt &&\n\t\t    !((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t    inet_sk(sk)->inet_daddr != LOOPBACK4_IPV6) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t}\n\t}\n\topt = xchg((__force struct ipv6_txoptions **)&inet6_sk(sk)->opt,\n\t\t   opt);\n\tsk_dst_reset(sk);\n\n\treturn opt;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,8 @@\n \t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n \t\t}\n \t}\n-\topt = xchg(&inet6_sk(sk)->opt, opt);\n+\topt = xchg((__force struct ipv6_txoptions **)&inet6_sk(sk)->opt,\n+\t\t   opt);\n \tsk_dst_reset(sk);\n \n \treturn opt;",
        "function_modified_lines": {
            "added": [
                "\topt = xchg((__force struct ipv6_txoptions **)&inet6_sk(sk)->opt,",
                "\t\t   opt);"
            ],
            "deleted": [
                "\topt = xchg(&inet6_sk(sk)->opt, opt);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1000
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "unsigned int arpt_do_table(struct sk_buff *skb,\n\t\t\t   const struct nf_hook_state *state,\n\t\t\t   struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tunsigned int verdict = NF_DROP;\n\tconst struct arphdr *arp;\n\tstruct arpt_entry *e, **jumpstack;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tunsigned int cpu, stackidx = 0;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\tif (!pskb_may_pull(skb, arp_hdr_len(skb->dev)))\n\t\treturn NF_DROP;\n\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = READ_ONCE(table->private); /* Address dependency. */\n\tcpu     = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct arpt_entry **)private->jumpstack[cpu];\n\n\t/* No TEE support for arptables, so no need to switch to alternate\n\t * stack.  All targets that reenter must return absolute verdicts.\n\t */\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tacpar.state   = state;\n\tacpar.hotdrop = false;\n\n\tarp = arp_hdr(skb);\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tstruct xt_counters *counter;\n\n\t\tif (!arp_packet_match(arp, skb->dev, indev, outdev, &e->arp)) {\n\t\t\te = arpt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, arp_hdr_len(skb->dev), 1);\n\n\t\tt = arpt_get_target_c(e);\n\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t\t      private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = arpt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v\n\t\t\t    != arpt_next_entry(e)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tarp = arp_hdr(skb);\n\t\t\te = arpt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse\n\t\treturn verdict;\n}",
        "code_after_change": "unsigned int arpt_do_table(struct sk_buff *skb,\n\t\t\t   const struct nf_hook_state *state,\n\t\t\t   struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tunsigned int verdict = NF_DROP;\n\tconst struct arphdr *arp;\n\tstruct arpt_entry *e, **jumpstack;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tunsigned int cpu, stackidx = 0;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\tif (!pskb_may_pull(skb, arp_hdr_len(skb->dev)))\n\t\treturn NF_DROP;\n\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = rcu_access_pointer(table->private);\n\tcpu     = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct arpt_entry **)private->jumpstack[cpu];\n\n\t/* No TEE support for arptables, so no need to switch to alternate\n\t * stack.  All targets that reenter must return absolute verdicts.\n\t */\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tacpar.state   = state;\n\tacpar.hotdrop = false;\n\n\tarp = arp_hdr(skb);\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tstruct xt_counters *counter;\n\n\t\tif (!arp_packet_match(arp, skb->dev, indev, outdev, &e->arp)) {\n\t\t\te = arpt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, arp_hdr_len(skb->dev), 1);\n\n\t\tt = arpt_get_target_c(e);\n\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t\t      private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = arpt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v\n\t\t\t    != arpt_next_entry(e)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tarp = arp_hdr(skb);\n\t\t\te = arpt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse\n\t\treturn verdict;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,7 +22,7 @@\n \n \tlocal_bh_disable();\n \taddend = xt_write_recseq_begin();\n-\tprivate = READ_ONCE(table->private); /* Address dependency. */\n+\tprivate = rcu_access_pointer(table->private);\n \tcpu     = smp_processor_id();\n \ttable_base = private->entries;\n \tjumpstack  = (struct arpt_entry **)private->jumpstack[cpu];",
        "function_modified_lines": {
            "added": [
                "\tprivate = rcu_access_pointer(table->private);"
            ],
            "deleted": [
                "\tprivate = READ_ONCE(table->private); /* Address dependency. */"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2771
    },
    {
        "cve_id": "CVE-2023-5197",
        "code_before_change": "static void __nft_release_table(struct net *net, struct nft_table *table)\n{\n\tstruct nft_flowtable *flowtable, *nf;\n\tstruct nft_chain *chain, *nc;\n\tstruct nft_object *obj, *ne;\n\tstruct nft_rule *rule, *nr;\n\tstruct nft_set *set, *ns;\n\tstruct nft_ctx ctx = {\n\t\t.net\t= net,\n\t\t.family\t= NFPROTO_NETDEV,\n\t};\n\n\tctx.family = table->family;\n\tctx.table = table;\n\tlist_for_each_entry(chain, &table->chains, list) {\n\t\tif (nft_chain_is_bound(chain))\n\t\t\tcontinue;\n\n\t\tctx.chain = chain;\n\t\tlist_for_each_entry_safe(rule, nr, &chain->rules, list) {\n\t\t\tlist_del(&rule->list);\n\t\t\tnft_use_dec(&chain->use);\n\t\t\tnf_tables_rule_release(&ctx, rule);\n\t\t}\n\t}\n\tlist_for_each_entry_safe(flowtable, nf, &table->flowtables, list) {\n\t\tlist_del(&flowtable->list);\n\t\tnft_use_dec(&table->use);\n\t\tnf_tables_flowtable_destroy(flowtable);\n\t}\n\tlist_for_each_entry_safe(set, ns, &table->sets, list) {\n\t\tlist_del(&set->list);\n\t\tnft_use_dec(&table->use);\n\t\tif (set->flags & (NFT_SET_MAP | NFT_SET_OBJECT))\n\t\t\tnft_map_deactivate(&ctx, set);\n\n\t\tnft_set_destroy(&ctx, set);\n\t}\n\tlist_for_each_entry_safe(obj, ne, &table->objects, list) {\n\t\tnft_obj_del(obj);\n\t\tnft_use_dec(&table->use);\n\t\tnft_obj_destroy(&ctx, obj);\n\t}\n\tlist_for_each_entry_safe(chain, nc, &table->chains, list) {\n\t\tctx.chain = chain;\n\t\tnft_chain_del(chain);\n\t\tnft_use_dec(&table->use);\n\t\tnf_tables_chain_destroy(&ctx);\n\t}\n\tnf_tables_table_destroy(&ctx);\n}",
        "code_after_change": "static void __nft_release_table(struct net *net, struct nft_table *table)\n{\n\tstruct nft_flowtable *flowtable, *nf;\n\tstruct nft_chain *chain, *nc;\n\tstruct nft_object *obj, *ne;\n\tstruct nft_rule *rule, *nr;\n\tstruct nft_set *set, *ns;\n\tstruct nft_ctx ctx = {\n\t\t.net\t= net,\n\t\t.family\t= NFPROTO_NETDEV,\n\t};\n\n\tctx.family = table->family;\n\tctx.table = table;\n\tlist_for_each_entry(chain, &table->chains, list) {\n\t\tif (nft_chain_binding(chain))\n\t\t\tcontinue;\n\n\t\tctx.chain = chain;\n\t\tlist_for_each_entry_safe(rule, nr, &chain->rules, list) {\n\t\t\tlist_del(&rule->list);\n\t\t\tnft_use_dec(&chain->use);\n\t\t\tnf_tables_rule_release(&ctx, rule);\n\t\t}\n\t}\n\tlist_for_each_entry_safe(flowtable, nf, &table->flowtables, list) {\n\t\tlist_del(&flowtable->list);\n\t\tnft_use_dec(&table->use);\n\t\tnf_tables_flowtable_destroy(flowtable);\n\t}\n\tlist_for_each_entry_safe(set, ns, &table->sets, list) {\n\t\tlist_del(&set->list);\n\t\tnft_use_dec(&table->use);\n\t\tif (set->flags & (NFT_SET_MAP | NFT_SET_OBJECT))\n\t\t\tnft_map_deactivate(&ctx, set);\n\n\t\tnft_set_destroy(&ctx, set);\n\t}\n\tlist_for_each_entry_safe(obj, ne, &table->objects, list) {\n\t\tnft_obj_del(obj);\n\t\tnft_use_dec(&table->use);\n\t\tnft_obj_destroy(&ctx, obj);\n\t}\n\tlist_for_each_entry_safe(chain, nc, &table->chains, list) {\n\t\tctx.chain = chain;\n\t\tnft_chain_del(chain);\n\t\tnft_use_dec(&table->use);\n\t\tnf_tables_chain_destroy(&ctx);\n\t}\n\tnf_tables_table_destroy(&ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,7 +13,7 @@\n \tctx.family = table->family;\n \tctx.table = table;\n \tlist_for_each_entry(chain, &table->chains, list) {\n-\t\tif (nft_chain_is_bound(chain))\n+\t\tif (nft_chain_binding(chain))\n \t\t\tcontinue;\n \n \t\tctx.chain = chain;",
        "function_modified_lines": {
            "added": [
                "\t\tif (nft_chain_binding(chain))"
            ],
            "deleted": [
                "\t\tif (nft_chain_is_bound(chain))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nAddition and removal of rules from chain bindings within the same transaction causes leads to use-after-free.\n\nWe recommend upgrading past commit f15f29fd4779be8a418b66e9d52979bb6d6c2325.\n\n",
        "id": 4264
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static void aio_complete_rw(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct aio_kiocb *iocb = container_of(kiocb, struct aio_kiocb, rw);\n\n\tif (!list_empty_careful(&iocb->ki_list))\n\t\taio_remove_iocb(iocb);\n\n\tif (kiocb->ki_flags & IOCB_WRITE) {\n\t\tstruct inode *inode = file_inode(kiocb->ki_filp);\n\n\t\t/*\n\t\t * Tell lockdep we inherited freeze protection from submission\n\t\t * thread.\n\t\t */\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\t__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);\n\t\tfile_end_write(kiocb->ki_filp);\n\t}\n\n\tfput(kiocb->ki_filp);\n\taio_complete(iocb, res, res2);\n}",
        "code_after_change": "static void aio_complete_rw(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct aio_kiocb *iocb = container_of(kiocb, struct aio_kiocb, rw);\n\n\tif (!list_empty_careful(&iocb->ki_list))\n\t\taio_remove_iocb(iocb);\n\n\tif (kiocb->ki_flags & IOCB_WRITE) {\n\t\tstruct inode *inode = file_inode(kiocb->ki_filp);\n\n\t\t/*\n\t\t * Tell lockdep we inherited freeze protection from submission\n\t\t * thread.\n\t\t */\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\t__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);\n\t\tfile_end_write(kiocb->ki_filp);\n\t}\n\n\taio_complete(iocb, res, res2);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,5 @@\n \t\tfile_end_write(kiocb->ki_filp);\n \t}\n \n-\tfput(kiocb->ki_filp);\n \taio_complete(iocb, res, res2);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tfput(kiocb->ki_filp);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1888
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "static int vmw_cmd_dx_bind_streamoutput(struct vmw_private *dev_priv,\n\t\t\t\t\tstruct vmw_sw_context *sw_context,\n\t\t\t\t\tSVGA3dCmdHeader *header)\n{\n\tstruct vmw_ctx_validation_info *ctx_node = sw_context->dx_ctx_node;\n\tstruct vmw_resource *res;\n\tstruct {\n\t\tSVGA3dCmdHeader header;\n\t\tSVGA3dCmdDXBindStreamOutput body;\n\t} *cmd = container_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (!has_sm5_context(dev_priv))\n\t\treturn -EINVAL;\n\n\tif (!ctx_node) {\n\t\tDRM_ERROR(\"DX Context not set.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tres = vmw_dx_streamoutput_lookup(vmw_context_res_man(ctx_node->ctx),\n\t\t\t\t\t cmd->body.soid);\n\tif (IS_ERR(res)) {\n\t\tDRM_ERROR(\"Could not find streamoutput to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tvmw_dx_streamoutput_set_size(res, cmd->body.sizeInBytes);\n\n\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\tif (ret) {\n\t\tDRM_ERROR(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
        "code_after_change": "static int vmw_cmd_dx_bind_streamoutput(struct vmw_private *dev_priv,\n\t\t\t\t\tstruct vmw_sw_context *sw_context,\n\t\t\t\t\tSVGA3dCmdHeader *header)\n{\n\tstruct vmw_ctx_validation_info *ctx_node = sw_context->dx_ctx_node;\n\tstruct vmw_resource *res;\n\tstruct {\n\t\tSVGA3dCmdHeader header;\n\t\tSVGA3dCmdDXBindStreamOutput body;\n\t} *cmd = container_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (!has_sm5_context(dev_priv))\n\t\treturn -EINVAL;\n\n\tif (!ctx_node) {\n\t\tDRM_ERROR(\"DX Context not set.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tres = vmw_dx_streamoutput_lookup(vmw_context_res_man(ctx_node->ctx),\n\t\t\t\t\t cmd->body.soid);\n\tif (IS_ERR(res)) {\n\t\tDRM_ERROR(\"Could not find streamoutput to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tvmw_dx_streamoutput_set_size(res, cmd->body.sizeInBytes);\n\n\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_NONE,\n\t\t\t\t      vmw_val_add_flag_noctx);\n\tif (ret) {\n\t\tDRM_ERROR(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,8 +27,8 @@\n \n \tvmw_dx_streamoutput_set_size(res, cmd->body.sizeInBytes);\n \n-\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n-\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n+\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_NONE,\n+\t\t\t\t      vmw_val_add_flag_noctx);\n \tif (ret) {\n \t\tDRM_ERROR(\"Error creating resource validation node.\\n\");\n \t\treturn ret;",
        "function_modified_lines": {
            "added": [
                "\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_NONE,",
                "\t\t\t\t      vmw_val_add_flag_noctx);"
            ],
            "deleted": [
                "\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,",
                "\t\t\t\t\t    VMW_RES_DIRTY_NONE);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3679
    },
    {
        "cve_id": "CVE-2016-8655",
        "code_before_change": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_version = val;\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
        "code_after_change": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_version = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -71,19 +71,25 @@\n \n \t\tif (optlen != sizeof(val))\n \t\t\treturn -EINVAL;\n-\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n-\t\t\treturn -EBUSY;\n \t\tif (copy_from_user(&val, optval, sizeof(val)))\n \t\t\treturn -EFAULT;\n \t\tswitch (val) {\n \t\tcase TPACKET_V1:\n \t\tcase TPACKET_V2:\n \t\tcase TPACKET_V3:\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\treturn -EINVAL;\n+\t\t}\n+\t\tlock_sock(sk);\n+\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n+\t\t\tret = -EBUSY;\n+\t\t} else {\n \t\t\tpo->tp_version = val;\n-\t\t\treturn 0;\n-\t\tdefault:\n-\t\t\treturn -EINVAL;\n+\t\t\tret = 0;\n \t\t}\n+\t\trelease_sock(sk);\n+\t\treturn ret;\n \t}\n \tcase PACKET_RESERVE:\n \t{",
        "function_modified_lines": {
            "added": [
                "\t\t\tbreak;",
                "\t\tdefault:",
                "\t\t\treturn -EINVAL;",
                "\t\t}",
                "\t\tlock_sock(sk);",
                "\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {",
                "\t\t\tret = -EBUSY;",
                "\t\t} else {",
                "\t\t\tret = 0;",
                "\t\trelease_sock(sk);",
                "\t\treturn ret;"
            ],
            "deleted": [
                "\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)",
                "\t\t\treturn -EBUSY;",
                "\t\t\treturn 0;",
                "\t\tdefault:",
                "\t\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in net/packet/af_packet.c in the Linux kernel through 4.8.12 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging the CAP_NET_RAW capability to change a socket version, related to the packet_set_ring and packet_setsockopt functions.",
        "id": 1132
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static void io_worker_handle_work(struct io_worker *worker)\n\t__releases(wqe->lock)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wq *wq = wqe->wq;\n\n\tdo {\n\t\tstruct io_wq_work *work;\nget_next:\n\t\t/*\n\t\t * If we got some work, mark us as busy. If we didn't, but\n\t\t * the list isn't empty, it means we stalled on hashed work.\n\t\t * Mark us stalled so we don't keep looking for work when we\n\t\t * can't make progress, any work completion or insertion will\n\t\t * clear the stalled flag.\n\t\t */\n\t\twork = io_get_next_work(wqe);\n\t\tif (work)\n\t\t\t__io_worker_busy(wqe, worker, work);\n\t\telse if (!wq_list_empty(&wqe->work_list))\n\t\t\twqe->flags |= IO_WQE_FLAG_STALLED;\n\n\t\traw_spin_unlock_irq(&wqe->lock);\n\t\tif (!work)\n\t\t\tbreak;\n\t\tio_assign_current_work(worker, work);\n\n\t\t/* handle a whole dependent link */\n\t\tdo {\n\t\t\tstruct io_wq_work *next_hashed, *linked;\n\t\t\tunsigned int hash = io_get_work_hash(work);\n\n\t\t\tnext_hashed = wq_next_work(work);\n\t\t\twq->do_work(work);\n\t\t\tio_assign_current_work(worker, NULL);\n\n\t\t\tlinked = wq->free_work(work);\n\t\t\twork = next_hashed;\n\t\t\tif (!work && linked && !io_wq_is_hashed(linked)) {\n\t\t\t\twork = linked;\n\t\t\t\tlinked = NULL;\n\t\t\t}\n\t\t\tio_assign_current_work(worker, work);\n\t\t\tif (linked)\n\t\t\t\tio_wqe_enqueue(wqe, linked);\n\n\t\t\tif (hash != -1U && !next_hashed) {\n\t\t\t\traw_spin_lock_irq(&wqe->lock);\n\t\t\t\twqe->hash_map &= ~BIT_ULL(hash);\n\t\t\t\twqe->flags &= ~IO_WQE_FLAG_STALLED;\n\t\t\t\t/* skip unnecessary unlock-lock wqe->lock */\n\t\t\t\tif (!work)\n\t\t\t\t\tgoto get_next;\n\t\t\t\traw_spin_unlock_irq(&wqe->lock);\n\t\t\t}\n\t\t} while (work);\n\n\t\traw_spin_lock_irq(&wqe->lock);\n\t} while (1);\n}",
        "code_after_change": "static void io_worker_handle_work(struct io_worker *worker)\n\t__releases(wqe->lock)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wq *wq = wqe->wq;\n\n\tdo {\n\t\tstruct io_wq_work *work;\nget_next:\n\t\t/*\n\t\t * If we got some work, mark us as busy. If we didn't, but\n\t\t * the list isn't empty, it means we stalled on hashed work.\n\t\t * Mark us stalled so we don't keep looking for work when we\n\t\t * can't make progress, any work completion or insertion will\n\t\t * clear the stalled flag.\n\t\t */\n\t\twork = io_get_next_work(wqe);\n\t\tif (work)\n\t\t\t__io_worker_busy(wqe, worker, work);\n\t\telse if (!wq_list_empty(&wqe->work_list))\n\t\t\twqe->flags |= IO_WQE_FLAG_STALLED;\n\n\t\traw_spin_unlock_irq(&wqe->lock);\n\t\tif (!work)\n\t\t\tbreak;\n\t\tio_assign_current_work(worker, work);\n\n\t\t/* handle a whole dependent link */\n\t\tdo {\n\t\t\tstruct io_wq_work *next_hashed, *linked;\n\t\t\tunsigned int hash = io_get_work_hash(work);\n\n\t\t\tnext_hashed = wq_next_work(work);\n\t\t\tif (work->creds && worker->cur_creds != work->creds)\n\t\t\t\tio_wq_switch_creds(worker, work);\n\t\t\twq->do_work(work);\n\t\t\tio_assign_current_work(worker, NULL);\n\n\t\t\tlinked = wq->free_work(work);\n\t\t\twork = next_hashed;\n\t\t\tif (!work && linked && !io_wq_is_hashed(linked)) {\n\t\t\t\twork = linked;\n\t\t\t\tlinked = NULL;\n\t\t\t}\n\t\t\tio_assign_current_work(worker, work);\n\t\t\tif (linked)\n\t\t\t\tio_wqe_enqueue(wqe, linked);\n\n\t\t\tif (hash != -1U && !next_hashed) {\n\t\t\t\traw_spin_lock_irq(&wqe->lock);\n\t\t\t\twqe->hash_map &= ~BIT_ULL(hash);\n\t\t\t\twqe->flags &= ~IO_WQE_FLAG_STALLED;\n\t\t\t\t/* skip unnecessary unlock-lock wqe->lock */\n\t\t\t\tif (!work)\n\t\t\t\t\tgoto get_next;\n\t\t\t\traw_spin_unlock_irq(&wqe->lock);\n\t\t\t}\n\t\t} while (work);\n\n\t\traw_spin_lock_irq(&wqe->lock);\n\t} while (1);\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,6 +31,8 @@\n \t\t\tunsigned int hash = io_get_work_hash(work);\n \n \t\t\tnext_hashed = wq_next_work(work);\n+\t\t\tif (work->creds && worker->cur_creds != work->creds)\n+\t\t\t\tio_wq_switch_creds(worker, work);\n \t\t\twq->do_work(work);\n \t\t\tio_assign_current_work(worker, NULL);\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (work->creds && worker->cur_creds != work->creds)",
                "\t\t\t\tio_wq_switch_creds(worker, work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3280
    },
    {
        "cve_id": "CVE-2023-3610",
        "code_before_change": "static void nft_verdict_uninit(const struct nft_data *data)\n{\n\tstruct nft_chain *chain;\n\tstruct nft_rule *rule;\n\n\tswitch (data->verdict.code) {\n\tcase NFT_JUMP:\n\tcase NFT_GOTO:\n\t\tchain = data->verdict.chain;\n\t\tchain->use--;\n\n\t\tif (!nft_chain_is_bound(chain))\n\t\t\tbreak;\n\n\t\tchain->table->use--;\n\t\tlist_for_each_entry(rule, &chain->rules, list)\n\t\t\tchain->use--;\n\n\t\tnft_chain_del(chain);\n\t\tbreak;\n\t}\n}",
        "code_after_change": "static void nft_verdict_uninit(const struct nft_data *data)\n{\n\tstruct nft_chain *chain;\n\n\tswitch (data->verdict.code) {\n\tcase NFT_JUMP:\n\tcase NFT_GOTO:\n\t\tchain = data->verdict.chain;\n\t\tchain->use--;\n\t\tbreak;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,22 +1,12 @@\n static void nft_verdict_uninit(const struct nft_data *data)\n {\n \tstruct nft_chain *chain;\n-\tstruct nft_rule *rule;\n \n \tswitch (data->verdict.code) {\n \tcase NFT_JUMP:\n \tcase NFT_GOTO:\n \t\tchain = data->verdict.chain;\n \t\tchain->use--;\n-\n-\t\tif (!nft_chain_is_bound(chain))\n-\t\t\tbreak;\n-\n-\t\tchain->table->use--;\n-\t\tlist_for_each_entry(rule, &chain->rules, list)\n-\t\t\tchain->use--;\n-\n-\t\tnft_chain_del(chain);\n \t\tbreak;\n \t}\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct nft_rule *rule;",
                "",
                "\t\tif (!nft_chain_is_bound(chain))",
                "\t\t\tbreak;",
                "",
                "\t\tchain->table->use--;",
                "\t\tlist_for_each_entry(rule, &chain->rules, list)",
                "\t\t\tchain->use--;",
                "",
                "\t\tnft_chain_del(chain);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nFlaw in the error handling of bound chains causes a use-after-free in the abort path of NFT_MSG_NEWRULE. The vulnerability requires CAP_NET_ADMIN to be triggered.\n\nWe recommend upgrading past commit 4bedf9eee016286c835e3d8fa981ddece5338795.\n\n",
        "id": 4125
    },
    {
        "cve_id": "CVE-2023-1652",
        "code_before_change": "static __be32 nfsd4_ssc_setup_dul(struct nfsd_net *nn, char *ipaddr,\n\t\tstruct nfsd4_ssc_umount_item **retwork, struct vfsmount **ss_mnt)\n{\n\tstruct nfsd4_ssc_umount_item *ni = NULL;\n\tstruct nfsd4_ssc_umount_item *work = NULL;\n\tstruct nfsd4_ssc_umount_item *tmp;\n\tDEFINE_WAIT(wait);\n\n\t*ss_mnt = NULL;\n\t*retwork = NULL;\n\twork = kzalloc(sizeof(*work), GFP_KERNEL);\ntry_again:\n\tspin_lock(&nn->nfsd_ssc_lock);\n\tlist_for_each_entry_safe(ni, tmp, &nn->nfsd_ssc_mount_list, nsui_list) {\n\t\tif (strncmp(ni->nsui_ipaddr, ipaddr, sizeof(ni->nsui_ipaddr)))\n\t\t\tcontinue;\n\t\t/* found a match */\n\t\tif (ni->nsui_busy) {\n\t\t\t/*  wait - and try again */\n\t\t\tprepare_to_wait(&nn->nfsd_ssc_waitq, &wait,\n\t\t\t\tTASK_INTERRUPTIBLE);\n\t\t\tspin_unlock(&nn->nfsd_ssc_lock);\n\n\t\t\t/* allow 20secs for mount/unmount for now - revisit */\n\t\t\tif (signal_pending(current) ||\n\t\t\t\t\t(schedule_timeout(20*HZ) == 0)) {\n\t\t\t\tkfree(work);\n\t\t\t\treturn nfserr_eagain;\n\t\t\t}\n\t\t\tfinish_wait(&nn->nfsd_ssc_waitq, &wait);\n\t\t\tgoto try_again;\n\t\t}\n\t\t*ss_mnt = ni->nsui_vfsmount;\n\t\trefcount_inc(&ni->nsui_refcnt);\n\t\tspin_unlock(&nn->nfsd_ssc_lock);\n\t\tkfree(work);\n\n\t\t/* return vfsmount in ss_mnt */\n\t\treturn 0;\n\t}\n\tif (work) {\n\t\tstrscpy(work->nsui_ipaddr, ipaddr, sizeof(work->nsui_ipaddr) - 1);\n\t\trefcount_set(&work->nsui_refcnt, 2);\n\t\twork->nsui_busy = true;\n\t\tlist_add_tail(&work->nsui_list, &nn->nfsd_ssc_mount_list);\n\t\t*retwork = work;\n\t}\n\tspin_unlock(&nn->nfsd_ssc_lock);\n\treturn 0;\n}",
        "code_after_change": "static __be32 nfsd4_ssc_setup_dul(struct nfsd_net *nn, char *ipaddr,\n\t\tstruct nfsd4_ssc_umount_item **retwork, struct vfsmount **ss_mnt)\n{\n\tstruct nfsd4_ssc_umount_item *ni = NULL;\n\tstruct nfsd4_ssc_umount_item *work = NULL;\n\tstruct nfsd4_ssc_umount_item *tmp;\n\tDEFINE_WAIT(wait);\n\n\t*ss_mnt = NULL;\n\t*retwork = NULL;\n\twork = kzalloc(sizeof(*work), GFP_KERNEL);\ntry_again:\n\tspin_lock(&nn->nfsd_ssc_lock);\n\tlist_for_each_entry_safe(ni, tmp, &nn->nfsd_ssc_mount_list, nsui_list) {\n\t\tif (strncmp(ni->nsui_ipaddr, ipaddr, sizeof(ni->nsui_ipaddr)))\n\t\t\tcontinue;\n\t\t/* found a match */\n\t\tif (ni->nsui_busy) {\n\t\t\t/*  wait - and try again */\n\t\t\tprepare_to_wait(&nn->nfsd_ssc_waitq, &wait,\n\t\t\t\tTASK_INTERRUPTIBLE);\n\t\t\tspin_unlock(&nn->nfsd_ssc_lock);\n\n\t\t\t/* allow 20secs for mount/unmount for now - revisit */\n\t\t\tif (signal_pending(current) ||\n\t\t\t\t\t(schedule_timeout(20*HZ) == 0)) {\n\t\t\t\tfinish_wait(&nn->nfsd_ssc_waitq, &wait);\n\t\t\t\tkfree(work);\n\t\t\t\treturn nfserr_eagain;\n\t\t\t}\n\t\t\tfinish_wait(&nn->nfsd_ssc_waitq, &wait);\n\t\t\tgoto try_again;\n\t\t}\n\t\t*ss_mnt = ni->nsui_vfsmount;\n\t\trefcount_inc(&ni->nsui_refcnt);\n\t\tspin_unlock(&nn->nfsd_ssc_lock);\n\t\tkfree(work);\n\n\t\t/* return vfsmount in ss_mnt */\n\t\treturn 0;\n\t}\n\tif (work) {\n\t\tstrscpy(work->nsui_ipaddr, ipaddr, sizeof(work->nsui_ipaddr) - 1);\n\t\trefcount_set(&work->nsui_refcnt, 2);\n\t\twork->nsui_busy = true;\n\t\tlist_add_tail(&work->nsui_list, &nn->nfsd_ssc_mount_list);\n\t\t*retwork = work;\n\t}\n\tspin_unlock(&nn->nfsd_ssc_lock);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -24,6 +24,7 @@\n \t\t\t/* allow 20secs for mount/unmount for now - revisit */\n \t\t\tif (signal_pending(current) ||\n \t\t\t\t\t(schedule_timeout(20*HZ) == 0)) {\n+\t\t\t\tfinish_wait(&nn->nfsd_ssc_waitq, &wait);\n \t\t\t\tkfree(work);\n \t\t\t\treturn nfserr_eagain;\n \t\t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tfinish_wait(&nn->nfsd_ssc_waitq, &wait);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfsd4_ssc_setup_dul in fs/nfsd/nfs4proc.c in the NFS filesystem in the Linux Kernel. This issue could allow a local attacker to crash the system or it may lead to a kernel information leak problem.",
        "id": 3877
    },
    {
        "cve_id": "CVE-2023-3863",
        "code_before_change": "static int nfc_genl_llc_set_params(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tu8 rw = 0;\n\tu16 miux = 0;\n\tu32 idx;\n\tint rc = 0;\n\n\tif (!info->attrs[NFC_ATTR_DEVICE_INDEX] ||\n\t    (!info->attrs[NFC_ATTR_LLC_PARAM_LTO] &&\n\t     !info->attrs[NFC_ATTR_LLC_PARAM_RW] &&\n\t     !info->attrs[NFC_ATTR_LLC_PARAM_MIUX]))\n\t\treturn -EINVAL;\n\n\tif (info->attrs[NFC_ATTR_LLC_PARAM_RW]) {\n\t\trw = nla_get_u8(info->attrs[NFC_ATTR_LLC_PARAM_RW]);\n\n\t\tif (rw > LLCP_MAX_RW)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NFC_ATTR_LLC_PARAM_MIUX]) {\n\t\tmiux = nla_get_u16(info->attrs[NFC_ATTR_LLC_PARAM_MIUX]);\n\n\t\tif (miux > LLCP_MAX_MIUX)\n\t\t\treturn -EINVAL;\n\t}\n\n\tidx = nla_get_u32(info->attrs[NFC_ATTR_DEVICE_INDEX]);\n\n\tdev = nfc_get_device(idx);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tdevice_lock(&dev->dev);\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (!local) {\n\t\trc = -ENODEV;\n\t\tgoto exit;\n\t}\n\n\tif (info->attrs[NFC_ATTR_LLC_PARAM_LTO]) {\n\t\tif (dev->dep_link_up) {\n\t\t\trc = -EINPROGRESS;\n\t\t\tgoto exit;\n\t\t}\n\n\t\tlocal->lto = nla_get_u8(info->attrs[NFC_ATTR_LLC_PARAM_LTO]);\n\t}\n\n\tif (info->attrs[NFC_ATTR_LLC_PARAM_RW])\n\t\tlocal->rw = rw;\n\n\tif (info->attrs[NFC_ATTR_LLC_PARAM_MIUX])\n\t\tlocal->miux = cpu_to_be16(miux);\n\nexit:\n\tdevice_unlock(&dev->dev);\n\n\tnfc_put_device(dev);\n\n\treturn rc;\n}",
        "code_after_change": "static int nfc_genl_llc_set_params(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nfc_dev *dev;\n\tstruct nfc_llcp_local *local;\n\tu8 rw = 0;\n\tu16 miux = 0;\n\tu32 idx;\n\tint rc = 0;\n\n\tif (!info->attrs[NFC_ATTR_DEVICE_INDEX] ||\n\t    (!info->attrs[NFC_ATTR_LLC_PARAM_LTO] &&\n\t     !info->attrs[NFC_ATTR_LLC_PARAM_RW] &&\n\t     !info->attrs[NFC_ATTR_LLC_PARAM_MIUX]))\n\t\treturn -EINVAL;\n\n\tif (info->attrs[NFC_ATTR_LLC_PARAM_RW]) {\n\t\trw = nla_get_u8(info->attrs[NFC_ATTR_LLC_PARAM_RW]);\n\n\t\tif (rw > LLCP_MAX_RW)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NFC_ATTR_LLC_PARAM_MIUX]) {\n\t\tmiux = nla_get_u16(info->attrs[NFC_ATTR_LLC_PARAM_MIUX]);\n\n\t\tif (miux > LLCP_MAX_MIUX)\n\t\t\treturn -EINVAL;\n\t}\n\n\tidx = nla_get_u32(info->attrs[NFC_ATTR_DEVICE_INDEX]);\n\n\tdev = nfc_get_device(idx);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tdevice_lock(&dev->dev);\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (!local) {\n\t\trc = -ENODEV;\n\t\tgoto exit;\n\t}\n\n\tif (info->attrs[NFC_ATTR_LLC_PARAM_LTO]) {\n\t\tif (dev->dep_link_up) {\n\t\t\trc = -EINPROGRESS;\n\t\t\tgoto put_local;\n\t\t}\n\n\t\tlocal->lto = nla_get_u8(info->attrs[NFC_ATTR_LLC_PARAM_LTO]);\n\t}\n\n\tif (info->attrs[NFC_ATTR_LLC_PARAM_RW])\n\t\tlocal->rw = rw;\n\n\tif (info->attrs[NFC_ATTR_LLC_PARAM_MIUX])\n\t\tlocal->miux = cpu_to_be16(miux);\n\nput_local:\n\tnfc_llcp_local_put(local);\n\nexit:\n\tdevice_unlock(&dev->dev);\n\n\tnfc_put_device(dev);\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -44,7 +44,7 @@\n \tif (info->attrs[NFC_ATTR_LLC_PARAM_LTO]) {\n \t\tif (dev->dep_link_up) {\n \t\t\trc = -EINPROGRESS;\n-\t\t\tgoto exit;\n+\t\t\tgoto put_local;\n \t\t}\n \n \t\tlocal->lto = nla_get_u8(info->attrs[NFC_ATTR_LLC_PARAM_LTO]);\n@@ -56,6 +56,9 @@\n \tif (info->attrs[NFC_ATTR_LLC_PARAM_MIUX])\n \t\tlocal->miux = cpu_to_be16(miux);\n \n+put_local:\n+\tnfc_llcp_local_put(local);\n+\n exit:\n \tdevice_unlock(&dev->dev);\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tgoto put_local;",
                "put_local:",
                "\tnfc_llcp_local_put(local);",
                ""
            ],
            "deleted": [
                "\t\t\tgoto exit;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nfc_llcp_find_local in net/nfc/llcp_core.c in NFC in the Linux kernel. This flaw allows a local user with special privileges to impact a kernel information leak issue.",
        "id": 4156
    },
    {
        "cve_id": "CVE-2022-41850",
        "code_before_change": "int roccat_report_event(int minor, u8 const *data)\n{\n\tstruct roccat_device *device;\n\tstruct roccat_reader *reader;\n\tstruct roccat_report *report;\n\tuint8_t *new_value;\n\n\tdevice = devices[minor];\n\n\tnew_value = kmemdup(data, device->report_size, GFP_ATOMIC);\n\tif (!new_value)\n\t\treturn -ENOMEM;\n\n\treport = &device->cbuf[device->cbuf_end];\n\n\t/* passing NULL is safe */\n\tkfree(report->value);\n\n\treport->value = new_value;\n\tdevice->cbuf_end = (device->cbuf_end + 1) % ROCCAT_CBUF_SIZE;\n\n\tlist_for_each_entry(reader, &device->readers, node) {\n\t\t/*\n\t\t * As we already inserted one element, the buffer can't be\n\t\t * empty. If start and end are equal, buffer is full and we\n\t\t * increase start, so that slow reader misses one event, but\n\t\t * gets the newer ones in the right order.\n\t\t */\n\t\tif (reader->cbuf_start == device->cbuf_end)\n\t\t\treader->cbuf_start = (reader->cbuf_start + 1) % ROCCAT_CBUF_SIZE;\n\t}\n\n\twake_up_interruptible(&device->wait);\n\treturn 0;\n}",
        "code_after_change": "int roccat_report_event(int minor, u8 const *data)\n{\n\tstruct roccat_device *device;\n\tstruct roccat_reader *reader;\n\tstruct roccat_report *report;\n\tuint8_t *new_value;\n\n\tdevice = devices[minor];\n\n\tnew_value = kmemdup(data, device->report_size, GFP_ATOMIC);\n\tif (!new_value)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&device->cbuf_lock);\n\n\treport = &device->cbuf[device->cbuf_end];\n\n\t/* passing NULL is safe */\n\tkfree(report->value);\n\n\treport->value = new_value;\n\tdevice->cbuf_end = (device->cbuf_end + 1) % ROCCAT_CBUF_SIZE;\n\n\tlist_for_each_entry(reader, &device->readers, node) {\n\t\t/*\n\t\t * As we already inserted one element, the buffer can't be\n\t\t * empty. If start and end are equal, buffer is full and we\n\t\t * increase start, so that slow reader misses one event, but\n\t\t * gets the newer ones in the right order.\n\t\t */\n\t\tif (reader->cbuf_start == device->cbuf_end)\n\t\t\treader->cbuf_start = (reader->cbuf_start + 1) % ROCCAT_CBUF_SIZE;\n\t}\n\n\tmutex_unlock(&device->cbuf_lock);\n\n\twake_up_interruptible(&device->wait);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,8 @@\n \tnew_value = kmemdup(data, device->report_size, GFP_ATOMIC);\n \tif (!new_value)\n \t\treturn -ENOMEM;\n+\n+\tmutex_lock(&device->cbuf_lock);\n \n \treport = &device->cbuf[device->cbuf_end];\n \n@@ -30,6 +32,8 @@\n \t\t\treader->cbuf_start = (reader->cbuf_start + 1) % ROCCAT_CBUF_SIZE;\n \t}\n \n+\tmutex_unlock(&device->cbuf_lock);\n+\n \twake_up_interruptible(&device->wait);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tmutex_lock(&device->cbuf_lock);",
                "\tmutex_unlock(&device->cbuf_lock);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "roccat_report_event in drivers/hid/hid-roccat.c in the Linux kernel through 5.19.12 has a race condition and resultant use-after-free in certain situations where a report is received while copying a report->value is in progress.",
        "id": 3722
    },
    {
        "cve_id": "CVE-2015-8963",
        "code_before_change": "static void perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tswhash->online = true;\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}",
        "code_after_change": "static void perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,6 @@\n \tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n \n \tmutex_lock(&swhash->hlist_mutex);\n-\tswhash->online = true;\n \tif (swhash->hlist_refcount > 0) {\n \t\tstruct swevent_hlist *hlist;\n ",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tswhash->online = true;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in kernel/events/core.c in the Linux kernel before 4.4 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect handling of an swevent data structure during a CPU unplug operation.",
        "id": 872
    },
    {
        "cve_id": "CVE-2017-7374",
        "code_before_change": "static int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)\n{\n\tstruct dentry *dir;\n\tstruct fscrypt_info *ci;\n\tint dir_has_key, cached_with_key;\n\n\tif (flags & LOOKUP_RCU)\n\t\treturn -ECHILD;\n\n\tdir = dget_parent(dentry);\n\tif (!d_inode(dir)->i_sb->s_cop->is_encrypted(d_inode(dir))) {\n\t\tdput(dir);\n\t\treturn 0;\n\t}\n\n\tci = d_inode(dir)->i_crypt_info;\n\tif (ci && ci->ci_keyring_key &&\n\t    (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |\n\t\t\t\t\t  (1 << KEY_FLAG_REVOKED) |\n\t\t\t\t\t  (1 << KEY_FLAG_DEAD))))\n\t\tci = NULL;\n\n\t/* this should eventually be an flag in d_flags */\n\tspin_lock(&dentry->d_lock);\n\tcached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;\n\tspin_unlock(&dentry->d_lock);\n\tdir_has_key = (ci != NULL);\n\tdput(dir);\n\n\t/*\n\t * If the dentry was cached without the key, and it is a\n\t * negative dentry, it might be a valid name.  We can't check\n\t * if the key has since been made available due to locking\n\t * reasons, so we fail the validation so ext4_lookup() can do\n\t * this check.\n\t *\n\t * We also fail the validation if the dentry was created with\n\t * the key present, but we no longer have the key, or vice versa.\n\t */\n\tif ((!cached_with_key && d_is_negative(dentry)) ||\n\t\t\t(!cached_with_key && dir_has_key) ||\n\t\t\t(cached_with_key && !dir_has_key))\n\t\treturn 0;\n\treturn 1;\n}",
        "code_after_change": "static int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)\n{\n\tstruct dentry *dir;\n\tint dir_has_key, cached_with_key;\n\n\tif (flags & LOOKUP_RCU)\n\t\treturn -ECHILD;\n\n\tdir = dget_parent(dentry);\n\tif (!d_inode(dir)->i_sb->s_cop->is_encrypted(d_inode(dir))) {\n\t\tdput(dir);\n\t\treturn 0;\n\t}\n\n\t/* this should eventually be an flag in d_flags */\n\tspin_lock(&dentry->d_lock);\n\tcached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;\n\tspin_unlock(&dentry->d_lock);\n\tdir_has_key = (d_inode(dir)->i_crypt_info != NULL);\n\tdput(dir);\n\n\t/*\n\t * If the dentry was cached without the key, and it is a\n\t * negative dentry, it might be a valid name.  We can't check\n\t * if the key has since been made available due to locking\n\t * reasons, so we fail the validation so ext4_lookup() can do\n\t * this check.\n\t *\n\t * We also fail the validation if the dentry was created with\n\t * the key present, but we no longer have the key, or vice versa.\n\t */\n\tif ((!cached_with_key && d_is_negative(dentry)) ||\n\t\t\t(!cached_with_key && dir_has_key) ||\n\t\t\t(cached_with_key && !dir_has_key))\n\t\treturn 0;\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,6 @@\n static int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)\n {\n \tstruct dentry *dir;\n-\tstruct fscrypt_info *ci;\n \tint dir_has_key, cached_with_key;\n \n \tif (flags & LOOKUP_RCU)\n@@ -13,18 +12,11 @@\n \t\treturn 0;\n \t}\n \n-\tci = d_inode(dir)->i_crypt_info;\n-\tif (ci && ci->ci_keyring_key &&\n-\t    (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |\n-\t\t\t\t\t  (1 << KEY_FLAG_REVOKED) |\n-\t\t\t\t\t  (1 << KEY_FLAG_DEAD))))\n-\t\tci = NULL;\n-\n \t/* this should eventually be an flag in d_flags */\n \tspin_lock(&dentry->d_lock);\n \tcached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;\n \tspin_unlock(&dentry->d_lock);\n-\tdir_has_key = (ci != NULL);\n+\tdir_has_key = (d_inode(dir)->i_crypt_info != NULL);\n \tdput(dir);\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "\tdir_has_key = (d_inode(dir)->i_crypt_info != NULL);"
            ],
            "deleted": [
                "\tstruct fscrypt_info *ci;",
                "\tci = d_inode(dir)->i_crypt_info;",
                "\tif (ci && ci->ci_keyring_key &&",
                "\t    (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |",
                "\t\t\t\t\t  (1 << KEY_FLAG_REVOKED) |",
                "\t\t\t\t\t  (1 << KEY_FLAG_DEAD))))",
                "\t\tci = NULL;",
                "",
                "\tdir_has_key = (ci != NULL);"
            ]
        },
        "cwe": [
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in fs/crypto/ in the Linux kernel before 4.10.7 allows local users to cause a denial of service (NULL pointer dereference) or possibly gain privileges by revoking keyring keys being used for ext4, f2fs, or ubifs encryption, causing cryptographic transform objects to be freed prematurely.",
        "id": 1497
    },
    {
        "cve_id": "CVE-2019-8956",
        "code_before_change": "static int sctp_sendmsg(struct sock *sk, struct msghdr *msg, size_t msg_len)\n{\n\tstruct sctp_endpoint *ep = sctp_sk(sk)->ep;\n\tstruct sctp_transport *transport = NULL;\n\tstruct sctp_sndrcvinfo _sinfo, *sinfo;\n\tstruct sctp_association *asoc;\n\tstruct sctp_cmsgs cmsgs;\n\tunion sctp_addr *daddr;\n\tbool new = false;\n\t__u16 sflags;\n\tint err;\n\n\t/* Parse and get snd_info */\n\terr = sctp_sendmsg_parse(sk, &cmsgs, &_sinfo, msg, msg_len);\n\tif (err)\n\t\tgoto out;\n\n\tsinfo  = &_sinfo;\n\tsflags = sinfo->sinfo_flags;\n\n\t/* Get daddr from msg */\n\tdaddr = sctp_sendmsg_get_daddr(sk, msg, &cmsgs);\n\tif (IS_ERR(daddr)) {\n\t\terr = PTR_ERR(daddr);\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\n\t/* SCTP_SENDALL process */\n\tif ((sflags & SCTP_SENDALL) && sctp_style(sk, UDP)) {\n\t\tlist_for_each_entry(asoc, &ep->asocs, asocs) {\n\t\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\n\t\t\t\t\t\t\tmsg_len);\n\t\t\tif (err == 0)\n\t\t\t\tcontinue;\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\n\n\t\t\terr = sctp_sendmsg_to_asoc(asoc, msg, msg_len,\n\t\t\t\t\t\t   NULL, sinfo);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tiov_iter_revert(&msg->msg_iter, err);\n\t\t}\n\n\t\tgoto out_unlock;\n\t}\n\n\t/* Get and check or create asoc */\n\tif (daddr) {\n\t\tasoc = sctp_endpoint_lookup_assoc(ep, daddr, &transport);\n\t\tif (asoc) {\n\t\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\n\t\t\t\t\t\t\tmsg_len);\n\t\t\tif (err <= 0)\n\t\t\t\tgoto out_unlock;\n\t\t} else {\n\t\t\terr = sctp_sendmsg_new_asoc(sk, sflags, &cmsgs, daddr,\n\t\t\t\t\t\t    &transport);\n\t\t\tif (err)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tasoc = transport->asoc;\n\t\t\tnew = true;\n\t\t}\n\n\t\tif (!sctp_style(sk, TCP) && !(sflags & SCTP_ADDR_OVER))\n\t\t\ttransport = NULL;\n\t} else {\n\t\tasoc = sctp_id2assoc(sk, sinfo->sinfo_assoc_id);\n\t\tif (!asoc) {\n\t\t\terr = -EPIPE;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg, msg_len);\n\t\tif (err <= 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* Update snd_info with the asoc */\n\tsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\n\n\t/* Send msg to the asoc */\n\terr = sctp_sendmsg_to_asoc(asoc, msg, msg_len, transport, sinfo);\n\tif (err < 0 && err != -ESRCH && new)\n\t\tsctp_association_free(asoc);\n\nout_unlock:\n\trelease_sock(sk);\nout:\n\treturn sctp_error(sk, msg->msg_flags, err);\n}",
        "code_after_change": "static int sctp_sendmsg(struct sock *sk, struct msghdr *msg, size_t msg_len)\n{\n\tstruct sctp_endpoint *ep = sctp_sk(sk)->ep;\n\tstruct sctp_transport *transport = NULL;\n\tstruct sctp_sndrcvinfo _sinfo, *sinfo;\n\tstruct sctp_association *asoc, *tmp;\n\tstruct sctp_cmsgs cmsgs;\n\tunion sctp_addr *daddr;\n\tbool new = false;\n\t__u16 sflags;\n\tint err;\n\n\t/* Parse and get snd_info */\n\terr = sctp_sendmsg_parse(sk, &cmsgs, &_sinfo, msg, msg_len);\n\tif (err)\n\t\tgoto out;\n\n\tsinfo  = &_sinfo;\n\tsflags = sinfo->sinfo_flags;\n\n\t/* Get daddr from msg */\n\tdaddr = sctp_sendmsg_get_daddr(sk, msg, &cmsgs);\n\tif (IS_ERR(daddr)) {\n\t\terr = PTR_ERR(daddr);\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\n\t/* SCTP_SENDALL process */\n\tif ((sflags & SCTP_SENDALL) && sctp_style(sk, UDP)) {\n\t\tlist_for_each_entry_safe(asoc, tmp, &ep->asocs, asocs) {\n\t\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\n\t\t\t\t\t\t\tmsg_len);\n\t\t\tif (err == 0)\n\t\t\t\tcontinue;\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\n\n\t\t\terr = sctp_sendmsg_to_asoc(asoc, msg, msg_len,\n\t\t\t\t\t\t   NULL, sinfo);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tiov_iter_revert(&msg->msg_iter, err);\n\t\t}\n\n\t\tgoto out_unlock;\n\t}\n\n\t/* Get and check or create asoc */\n\tif (daddr) {\n\t\tasoc = sctp_endpoint_lookup_assoc(ep, daddr, &transport);\n\t\tif (asoc) {\n\t\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\n\t\t\t\t\t\t\tmsg_len);\n\t\t\tif (err <= 0)\n\t\t\t\tgoto out_unlock;\n\t\t} else {\n\t\t\terr = sctp_sendmsg_new_asoc(sk, sflags, &cmsgs, daddr,\n\t\t\t\t\t\t    &transport);\n\t\t\tif (err)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tasoc = transport->asoc;\n\t\t\tnew = true;\n\t\t}\n\n\t\tif (!sctp_style(sk, TCP) && !(sflags & SCTP_ADDR_OVER))\n\t\t\ttransport = NULL;\n\t} else {\n\t\tasoc = sctp_id2assoc(sk, sinfo->sinfo_assoc_id);\n\t\tif (!asoc) {\n\t\t\terr = -EPIPE;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg, msg_len);\n\t\tif (err <= 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* Update snd_info with the asoc */\n\tsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\n\n\t/* Send msg to the asoc */\n\terr = sctp_sendmsg_to_asoc(asoc, msg, msg_len, transport, sinfo);\n\tif (err < 0 && err != -ESRCH && new)\n\t\tsctp_association_free(asoc);\n\nout_unlock:\n\trelease_sock(sk);\nout:\n\treturn sctp_error(sk, msg->msg_flags, err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \tstruct sctp_endpoint *ep = sctp_sk(sk)->ep;\n \tstruct sctp_transport *transport = NULL;\n \tstruct sctp_sndrcvinfo _sinfo, *sinfo;\n-\tstruct sctp_association *asoc;\n+\tstruct sctp_association *asoc, *tmp;\n \tstruct sctp_cmsgs cmsgs;\n \tunion sctp_addr *daddr;\n \tbool new = false;\n@@ -29,7 +29,7 @@\n \n \t/* SCTP_SENDALL process */\n \tif ((sflags & SCTP_SENDALL) && sctp_style(sk, UDP)) {\n-\t\tlist_for_each_entry(asoc, &ep->asocs, asocs) {\n+\t\tlist_for_each_entry_safe(asoc, tmp, &ep->asocs, asocs) {\n \t\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\n \t\t\t\t\t\t\tmsg_len);\n \t\t\tif (err == 0)",
        "function_modified_lines": {
            "added": [
                "\tstruct sctp_association *asoc, *tmp;",
                "\t\tlist_for_each_entry_safe(asoc, tmp, &ep->asocs, asocs) {"
            ],
            "deleted": [
                "\tstruct sctp_association *asoc;",
                "\t\tlist_for_each_entry(asoc, &ep->asocs, asocs) {"
            ]
        },
        "cwe": [
            "CWE-787",
            "CWE-416"
        ],
        "cve_description": "In the Linux Kernel before versions 4.20.8 and 4.19.21 a use-after-free error in the \"sctp_sendmsg()\" function (net/sctp/socket.c) when handling SCTP_SENDALL flag can be exploited to corrupt memory.",
        "id": 2348
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static void __io_queue_sqe(struct io_kiocb *req)\n{\n\tstruct io_kiocb *linked_timeout = io_prep_linked_timeout(req);\n\tconst struct cred *old_creds = NULL;\n\tint ret;\n\n\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&\n\t    req->work.identity->creds != current_cred())\n\t\told_creds = override_creds(req->work.identity->creds);\n\n\tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\n\tif (old_creds)\n\t\trevert_creds(old_creds);\n\n\t/*\n\t * We async punt it if the file wasn't marked NOWAIT, or if the file\n\t * doesn't support non-blocking read/write attempts\n\t */\n\tif (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {\n\t\tif (!io_arm_poll_handler(req)) {\n\t\t\t/*\n\t\t\t * Queued up for async execution, worker will release\n\t\t\t * submit reference when the iocb is actually submitted.\n\t\t\t */\n\t\t\tio_queue_async_work(req);\n\t\t}\n\t} else if (likely(!ret)) {\n\t\t/* drop submission reference */\n\t\tif (req->flags & REQ_F_COMPLETE_INLINE) {\n\t\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\t\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\t\t\tcs->reqs[cs->nr++] = req;\n\t\t\tif (cs->nr == ARRAY_SIZE(cs->reqs))\n\t\t\t\tio_submit_flush_completions(cs, ctx);\n\t\t} else {\n\t\t\tio_put_req(req);\n\t\t}\n\t} else {\n\t\treq_set_fail_links(req);\n\t\tio_put_req(req);\n\t\tio_req_complete(req, ret);\n\t}\n\tif (linked_timeout)\n\t\tio_queue_linked_timeout(linked_timeout);\n}",
        "code_after_change": "static void __io_queue_sqe(struct io_kiocb *req)\n{\n\tstruct io_kiocb *linked_timeout = io_prep_linked_timeout(req);\n\tconst struct cred *old_creds = NULL;\n\tint ret;\n\n\tif ((req->flags & REQ_F_WORK_INITIALIZED) && req->work.creds &&\n\t    req->work.creds != current_cred())\n\t\told_creds = override_creds(req->work.creds);\n\n\tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\n\tif (old_creds)\n\t\trevert_creds(old_creds);\n\n\t/*\n\t * We async punt it if the file wasn't marked NOWAIT, or if the file\n\t * doesn't support non-blocking read/write attempts\n\t */\n\tif (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {\n\t\tif (!io_arm_poll_handler(req)) {\n\t\t\t/*\n\t\t\t * Queued up for async execution, worker will release\n\t\t\t * submit reference when the iocb is actually submitted.\n\t\t\t */\n\t\t\tio_queue_async_work(req);\n\t\t}\n\t} else if (likely(!ret)) {\n\t\t/* drop submission reference */\n\t\tif (req->flags & REQ_F_COMPLETE_INLINE) {\n\t\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\t\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\t\t\tcs->reqs[cs->nr++] = req;\n\t\t\tif (cs->nr == ARRAY_SIZE(cs->reqs))\n\t\t\t\tio_submit_flush_completions(cs, ctx);\n\t\t} else {\n\t\t\tio_put_req(req);\n\t\t}\n\t} else {\n\t\treq_set_fail_links(req);\n\t\tio_put_req(req);\n\t\tio_req_complete(req, ret);\n\t}\n\tif (linked_timeout)\n\t\tio_queue_linked_timeout(linked_timeout);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,9 +4,9 @@\n \tconst struct cred *old_creds = NULL;\n \tint ret;\n \n-\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&\n-\t    req->work.identity->creds != current_cred())\n-\t\told_creds = override_creds(req->work.identity->creds);\n+\tif ((req->flags & REQ_F_WORK_INITIALIZED) && req->work.creds &&\n+\t    req->work.creds != current_cred())\n+\t\told_creds = override_creds(req->work.creds);\n \n \tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n ",
        "function_modified_lines": {
            "added": [
                "\tif ((req->flags & REQ_F_WORK_INITIALIZED) && req->work.creds &&",
                "\t    req->work.creds != current_cred())",
                "\t\told_creds = override_creds(req->work.creds);"
            ],
            "deleted": [
                "\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&",
                "\t    req->work.identity->creds != current_cred())",
                "\t\told_creds = override_creds(req->work.identity->creds);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3289
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_buffer_object **vmw_bo_p)\n{\n\tstruct vmw_buffer_object *vmw_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tvmw_bo = vmw_user_bo_noref_lookup(sw_context->filp, handle);\n\tif (IS_ERR(vmw_bo)) {\n\t\tVMW_DEBUG_USER(\"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo, true, false);\n\tttm_bo_put(&vmw_bo->base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "code_after_change": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_buffer_object **vmw_bo_p)\n{\n\tstruct vmw_buffer_object *vmw_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo, true, false);\n\tttm_bo_put(&vmw_bo->base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,9 +9,9 @@\n \tint ret;\n \n \tvmw_validation_preload_bo(sw_context->ctx);\n-\tvmw_bo = vmw_user_bo_noref_lookup(sw_context->filp, handle);\n-\tif (IS_ERR(vmw_bo)) {\n-\t\tVMW_DEBUG_USER(\"Could not find or use MOB buffer.\\n\");\n+\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n+\tif (ret != 0) {\n+\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n \t\treturn PTR_ERR(vmw_bo);\n \t}\n \tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo, true, false);",
        "function_modified_lines": {
            "added": [
                "\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);",
                "\tif (ret != 0) {",
                "\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");"
            ],
            "deleted": [
                "\tvmw_bo = vmw_user_bo_noref_lookup(sw_context->filp, handle);",
                "\tif (IS_ERR(vmw_bo)) {",
                "\t\tVMW_DEBUG_USER(\"Could not find or use MOB buffer.\\n\");"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3689
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static vm_fault_t svm_migrate_to_ram(struct vm_fault *vmf)\n{\n\tunsigned long addr = vmf->address;\n\tstruct vm_area_struct *vma;\n\tenum svm_work_list_ops op;\n\tstruct svm_range *parent;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\tstruct mm_struct *mm;\n\tint r = 0;\n\n\tvma = vmf->vma;\n\tmm = vma->vm_mm;\n\n\tp = kfd_lookup_process_by_mm(vma->vm_mm);\n\tif (!p) {\n\t\tpr_debug(\"failed find process at fault address 0x%lx\\n\", addr);\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\tif (READ_ONCE(p->svms.faulting_task) == current) {\n\t\tpr_debug(\"skipping ram migration\\n\");\n\t\tkfd_unref_process(p);\n\t\treturn 0;\n\t}\n\taddr >>= PAGE_SHIFT;\n\tpr_debug(\"CPU page fault svms 0x%p address 0x%lx\\n\", &p->svms, addr);\n\n\tmutex_lock(&p->svms.lock);\n\n\tprange = svm_range_from_addr(&p->svms, addr, &parent);\n\tif (!prange) {\n\t\tpr_debug(\"cannot find svm range at 0x%lx\\n\", addr);\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&parent->migrate_mutex);\n\tif (prange != parent)\n\t\tmutex_lock_nested(&prange->migrate_mutex, 1);\n\n\tif (!prange->actual_loc)\n\t\tgoto out_unlock_prange;\n\n\tsvm_range_lock(parent);\n\tif (prange != parent)\n\t\tmutex_lock_nested(&prange->lock, 1);\n\tr = svm_range_split_by_granularity(p, mm, addr, parent, prange);\n\tif (prange != parent)\n\t\tmutex_unlock(&prange->lock);\n\tsvm_range_unlock(parent);\n\tif (r) {\n\t\tpr_debug(\"failed %d to split range by granularity\\n\", r);\n\t\tgoto out_unlock_prange;\n\t}\n\n\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PAGEFAULT_CPU);\n\tif (r)\n\t\tpr_debug(\"failed %d migrate 0x%p [0x%lx 0x%lx] to ram\\n\", r,\n\t\t\t prange, prange->start, prange->last);\n\n\t/* xnack on, update mapping on GPUs with ACCESS_IN_PLACE */\n\tif (p->xnack_enabled && parent == prange)\n\t\top = SVM_OP_UPDATE_RANGE_NOTIFIER_AND_MAP;\n\telse\n\t\top = SVM_OP_UPDATE_RANGE_NOTIFIER;\n\tsvm_range_add_list_work(&p->svms, parent, mm, op);\n\tschedule_deferred_list_work(&p->svms);\n\nout_unlock_prange:\n\tif (prange != parent)\n\t\tmutex_unlock(&prange->migrate_mutex);\n\tmutex_unlock(&parent->migrate_mutex);\nout:\n\tmutex_unlock(&p->svms.lock);\n\tkfd_unref_process(p);\n\n\tpr_debug(\"CPU fault svms 0x%p address 0x%lx done\\n\", &p->svms, addr);\n\n\treturn r ? VM_FAULT_SIGBUS : 0;\n}",
        "code_after_change": "static vm_fault_t svm_migrate_to_ram(struct vm_fault *vmf)\n{\n\tunsigned long addr = vmf->address;\n\tstruct vm_area_struct *vma;\n\tenum svm_work_list_ops op;\n\tstruct svm_range *parent;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\tstruct mm_struct *mm;\n\tint r = 0;\n\n\tvma = vmf->vma;\n\tmm = vma->vm_mm;\n\n\tp = kfd_lookup_process_by_mm(vma->vm_mm);\n\tif (!p) {\n\t\tpr_debug(\"failed find process at fault address 0x%lx\\n\", addr);\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\tif (READ_ONCE(p->svms.faulting_task) == current) {\n\t\tpr_debug(\"skipping ram migration\\n\");\n\t\tkfd_unref_process(p);\n\t\treturn 0;\n\t}\n\taddr >>= PAGE_SHIFT;\n\tpr_debug(\"CPU page fault svms 0x%p address 0x%lx\\n\", &p->svms, addr);\n\n\tmutex_lock(&p->svms.lock);\n\n\tprange = svm_range_from_addr(&p->svms, addr, &parent);\n\tif (!prange) {\n\t\tpr_debug(\"cannot find svm range at 0x%lx\\n\", addr);\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&parent->migrate_mutex);\n\tif (prange != parent)\n\t\tmutex_lock_nested(&prange->migrate_mutex, 1);\n\n\tif (!prange->actual_loc)\n\t\tgoto out_unlock_prange;\n\n\tsvm_range_lock(parent);\n\tif (prange != parent)\n\t\tmutex_lock_nested(&prange->lock, 1);\n\tr = svm_range_split_by_granularity(p, mm, addr, parent, prange);\n\tif (prange != parent)\n\t\tmutex_unlock(&prange->lock);\n\tsvm_range_unlock(parent);\n\tif (r) {\n\t\tpr_debug(\"failed %d to split range by granularity\\n\", r);\n\t\tgoto out_unlock_prange;\n\t}\n\n\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PAGEFAULT_CPU,\n\t\t\t\tvmf->page);\n\tif (r)\n\t\tpr_debug(\"failed %d migrate 0x%p [0x%lx 0x%lx] to ram\\n\", r,\n\t\t\t prange, prange->start, prange->last);\n\n\t/* xnack on, update mapping on GPUs with ACCESS_IN_PLACE */\n\tif (p->xnack_enabled && parent == prange)\n\t\top = SVM_OP_UPDATE_RANGE_NOTIFIER_AND_MAP;\n\telse\n\t\top = SVM_OP_UPDATE_RANGE_NOTIFIER;\n\tsvm_range_add_list_work(&p->svms, parent, mm, op);\n\tschedule_deferred_list_work(&p->svms);\n\nout_unlock_prange:\n\tif (prange != parent)\n\t\tmutex_unlock(&prange->migrate_mutex);\n\tmutex_unlock(&parent->migrate_mutex);\nout:\n\tmutex_unlock(&p->svms.lock);\n\tkfd_unref_process(p);\n\n\tpr_debug(\"CPU fault svms 0x%p address 0x%lx done\\n\", &p->svms, addr);\n\n\treturn r ? VM_FAULT_SIGBUS : 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -53,7 +53,8 @@\n \t\tgoto out_unlock_prange;\n \t}\n \n-\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PAGEFAULT_CPU);\n+\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PAGEFAULT_CPU,\n+\t\t\t\tvmf->page);\n \tif (r)\n \t\tpr_debug(\"failed %d migrate 0x%p [0x%lx 0x%lx] to ram\\n\", r,\n \t\t\t prange, prange->start, prange->last);",
        "function_modified_lines": {
            "added": [
                "\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PAGEFAULT_CPU,",
                "\t\t\t\tvmf->page);"
            ],
            "deleted": [
                "\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PAGEFAULT_CPU);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3609
    },
    {
        "cve_id": "CVE-2019-19529",
        "code_before_change": "static void mcba_usb_disconnect(struct usb_interface *intf)\n{\n\tstruct mcba_priv *priv = usb_get_intfdata(intf);\n\n\tusb_set_intfdata(intf, NULL);\n\n\tnetdev_info(priv->netdev, \"device disconnected\\n\");\n\n\tunregister_candev(priv->netdev);\n\tfree_candev(priv->netdev);\n\n\tmcba_urb_unlink(priv);\n}",
        "code_after_change": "static void mcba_usb_disconnect(struct usb_interface *intf)\n{\n\tstruct mcba_priv *priv = usb_get_intfdata(intf);\n\n\tusb_set_intfdata(intf, NULL);\n\n\tnetdev_info(priv->netdev, \"device disconnected\\n\");\n\n\tunregister_candev(priv->netdev);\n\tmcba_urb_unlink(priv);\n\tfree_candev(priv->netdev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,6 @@\n \tnetdev_info(priv->netdev, \"device disconnected\\n\");\n \n \tunregister_candev(priv->netdev);\n+\tmcba_urb_unlink(priv);\n \tfree_candev(priv->netdev);\n-\n-\tmcba_urb_unlink(priv);\n }",
        "function_modified_lines": {
            "added": [
                "\tmcba_urb_unlink(priv);"
            ],
            "deleted": [
                "",
                "\tmcba_urb_unlink(priv);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.3.11, there is a use-after-free bug that can be caused by a malicious USB device in the drivers/net/can/usb/mcba_usb.c driver, aka CID-4d6636498c41.",
        "id": 2204
    },
    {
        "cve_id": "CVE-2021-43057",
        "code_before_change": "static int smack_getprocattr(struct task_struct *p, char *name, char **value)\n{\n\tstruct smack_known *skp = smk_of_task_struct_subj(p);\n\tchar *cp;\n\tint slen;\n\n\tif (strcmp(name, \"current\") != 0)\n\t\treturn -EINVAL;\n\n\tcp = kstrdup(skp->smk_known, GFP_KERNEL);\n\tif (cp == NULL)\n\t\treturn -ENOMEM;\n\n\tslen = strlen(cp);\n\t*value = cp;\n\treturn slen;\n}",
        "code_after_change": "static int smack_getprocattr(struct task_struct *p, char *name, char **value)\n{\n\tstruct smack_known *skp = smk_of_task_struct_obj(p);\n\tchar *cp;\n\tint slen;\n\n\tif (strcmp(name, \"current\") != 0)\n\t\treturn -EINVAL;\n\n\tcp = kstrdup(skp->smk_known, GFP_KERNEL);\n\tif (cp == NULL)\n\t\treturn -ENOMEM;\n\n\tslen = strlen(cp);\n\t*value = cp;\n\treturn slen;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static int smack_getprocattr(struct task_struct *p, char *name, char **value)\n {\n-\tstruct smack_known *skp = smk_of_task_struct_subj(p);\n+\tstruct smack_known *skp = smk_of_task_struct_obj(p);\n \tchar *cp;\n \tint slen;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct smack_known *skp = smk_of_task_struct_obj(p);"
            ],
            "deleted": [
                "\tstruct smack_known *skp = smk_of_task_struct_subj(p);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.14.8. A use-after-free in selinux_ptrace_traceme (aka the SELinux handler for PTRACE_TRACEME) could be used by local attackers to cause memory corruption and escalate privileges, aka CID-a3727a8bac0a. This occurs because of an attempt to access the subjective credentials of another task.",
        "id": 3161
    },
    {
        "cve_id": "CVE-2020-15436",
        "code_before_change": "int blkdev_get(struct block_device *bdev, fmode_t mode, void *holder)\n{\n\tstruct block_device *whole = NULL;\n\tint res;\n\n\tWARN_ON_ONCE((mode & FMODE_EXCL) && !holder);\n\n\tif ((mode & FMODE_EXCL) && holder) {\n\t\twhole = bd_start_claiming(bdev, holder);\n\t\tif (IS_ERR(whole)) {\n\t\t\tbdput(bdev);\n\t\t\treturn PTR_ERR(whole);\n\t\t}\n\t}\n\n\tres = __blkdev_get(bdev, mode, 0);\n\n\tif (whole) {\n\t\tstruct gendisk *disk = whole->bd_disk;\n\n\t\t/* finish claiming */\n\t\tmutex_lock(&bdev->bd_mutex);\n\t\tif (!res)\n\t\t\tbd_finish_claiming(bdev, whole, holder);\n\t\telse\n\t\t\tbd_abort_claiming(bdev, whole, holder);\n\t\t/*\n\t\t * Block event polling for write claims if requested.  Any\n\t\t * write holder makes the write_holder state stick until\n\t\t * all are released.  This is good enough and tracking\n\t\t * individual writeable reference is too fragile given the\n\t\t * way @mode is used in blkdev_get/put().\n\t\t */\n\t\tif (!res && (mode & FMODE_WRITE) && !bdev->bd_write_holder &&\n\t\t    (disk->flags & GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE)) {\n\t\t\tbdev->bd_write_holder = true;\n\t\t\tdisk_block_events(disk);\n\t\t}\n\n\t\tmutex_unlock(&bdev->bd_mutex);\n\t\tbdput(whole);\n\t}\n\n\treturn res;\n}",
        "code_after_change": "int blkdev_get(struct block_device *bdev, fmode_t mode, void *holder)\n{\n\tstruct block_device *whole = NULL;\n\tint res;\n\n\tWARN_ON_ONCE((mode & FMODE_EXCL) && !holder);\n\n\tif ((mode & FMODE_EXCL) && holder) {\n\t\twhole = bd_start_claiming(bdev, holder);\n\t\tif (IS_ERR(whole)) {\n\t\t\tbdput(bdev);\n\t\t\treturn PTR_ERR(whole);\n\t\t}\n\t}\n\n\tres = __blkdev_get(bdev, mode, 0);\n\n\tif (whole) {\n\t\tstruct gendisk *disk = whole->bd_disk;\n\n\t\t/* finish claiming */\n\t\tmutex_lock(&bdev->bd_mutex);\n\t\tif (!res)\n\t\t\tbd_finish_claiming(bdev, whole, holder);\n\t\telse\n\t\t\tbd_abort_claiming(bdev, whole, holder);\n\t\t/*\n\t\t * Block event polling for write claims if requested.  Any\n\t\t * write holder makes the write_holder state stick until\n\t\t * all are released.  This is good enough and tracking\n\t\t * individual writeable reference is too fragile given the\n\t\t * way @mode is used in blkdev_get/put().\n\t\t */\n\t\tif (!res && (mode & FMODE_WRITE) && !bdev->bd_write_holder &&\n\t\t    (disk->flags & GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE)) {\n\t\t\tbdev->bd_write_holder = true;\n\t\t\tdisk_block_events(disk);\n\t\t}\n\n\t\tmutex_unlock(&bdev->bd_mutex);\n\t\tbdput(whole);\n\t}\n\n\tif (res)\n\t\tbdput(bdev);\n\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -41,5 +41,8 @@\n \t\tbdput(whole);\n \t}\n \n+\tif (res)\n+\t\tbdput(bdev);\n+\n \treturn res;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (res)",
                "\t\tbdput(bdev);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in fs/block_dev.c in the Linux kernel before 5.8 allows local users to gain privileges or cause a denial of service by leveraging improper access to a certain error field.",
        "id": 2544
    },
    {
        "cve_id": "CVE-2018-19824",
        "code_before_change": "static int usb_audio_probe(struct usb_interface *intf,\n\t\t\t   const struct usb_device_id *usb_id)\n{\n\tstruct usb_device *dev = interface_to_usbdev(intf);\n\tconst struct snd_usb_audio_quirk *quirk =\n\t\t(const struct snd_usb_audio_quirk *)usb_id->driver_info;\n\tstruct snd_usb_audio *chip;\n\tint i, err;\n\tstruct usb_host_interface *alts;\n\tint ifnum;\n\tu32 id;\n\n\talts = &intf->altsetting[0];\n\tifnum = get_iface_desc(alts)->bInterfaceNumber;\n\tid = USB_ID(le16_to_cpu(dev->descriptor.idVendor),\n\t\t    le16_to_cpu(dev->descriptor.idProduct));\n\tif (get_alias_id(dev, &id))\n\t\tquirk = get_alias_quirk(dev, id);\n\tif (quirk && quirk->ifnum >= 0 && ifnum != quirk->ifnum)\n\t\treturn -ENXIO;\n\n\terr = snd_usb_apply_boot_quirk(dev, intf, quirk, id);\n\tif (err < 0)\n\t\treturn err;\n\n\t/*\n\t * found a config.  now register to ALSA\n\t */\n\n\t/* check whether it's already registered */\n\tchip = NULL;\n\tmutex_lock(&register_mutex);\n\tfor (i = 0; i < SNDRV_CARDS; i++) {\n\t\tif (usb_chip[i] && usb_chip[i]->dev == dev) {\n\t\t\tif (atomic_read(&usb_chip[i]->shutdown)) {\n\t\t\t\tdev_err(&dev->dev, \"USB device is in the shutdown state, cannot create a card instance\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto __error;\n\t\t\t}\n\t\t\tchip = usb_chip[i];\n\t\t\tatomic_inc(&chip->active); /* avoid autopm */\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (! chip) {\n\t\t/* it's a fresh one.\n\t\t * now look for an empty slot and create a new card instance\n\t\t */\n\t\tfor (i = 0; i < SNDRV_CARDS; i++)\n\t\t\tif (!usb_chip[i] &&\n\t\t\t    (vid[i] == -1 || vid[i] == USB_ID_VENDOR(id)) &&\n\t\t\t    (pid[i] == -1 || pid[i] == USB_ID_PRODUCT(id))) {\n\t\t\t\tif (enable[i]) {\n\t\t\t\t\terr = snd_usb_audio_create(intf, dev, i, quirk,\n\t\t\t\t\t\t\t\t   id, &chip);\n\t\t\t\t\tif (err < 0)\n\t\t\t\t\t\tgoto __error;\n\t\t\t\t\tchip->pm_intf = intf;\n\t\t\t\t\tbreak;\n\t\t\t\t} else if (vid[i] != -1 || pid[i] != -1) {\n\t\t\t\t\tdev_info(&dev->dev,\n\t\t\t\t\t\t \"device (%04x:%04x) is disabled\\n\",\n\t\t\t\t\t\t USB_ID_VENDOR(id),\n\t\t\t\t\t\t USB_ID_PRODUCT(id));\n\t\t\t\t\terr = -ENOENT;\n\t\t\t\t\tgoto __error;\n\t\t\t\t}\n\t\t\t}\n\t\tif (!chip) {\n\t\t\tdev_err(&dev->dev, \"no available usb audio device\\n\");\n\t\t\terr = -ENODEV;\n\t\t\tgoto __error;\n\t\t}\n\t}\n\tdev_set_drvdata(&dev->dev, chip);\n\n\t/*\n\t * For devices with more than one control interface, we assume the\n\t * first contains the audio controls. We might need a more specific\n\t * check here in the future.\n\t */\n\tif (!chip->ctrl_intf)\n\t\tchip->ctrl_intf = alts;\n\n\tchip->txfr_quirk = 0;\n\terr = 1; /* continue */\n\tif (quirk && quirk->ifnum != QUIRK_NO_INTERFACE) {\n\t\t/* need some special handlings */\n\t\terr = snd_usb_create_quirk(chip, intf, &usb_audio_driver, quirk);\n\t\tif (err < 0)\n\t\t\tgoto __error;\n\t}\n\n\tif (err > 0) {\n\t\t/* create normal USB audio interfaces */\n\t\terr = snd_usb_create_streams(chip, ifnum);\n\t\tif (err < 0)\n\t\t\tgoto __error;\n\t\terr = snd_usb_create_mixer(chip, ifnum, ignore_ctl_error);\n\t\tif (err < 0)\n\t\t\tgoto __error;\n\t}\n\n\t/* we are allowed to call snd_card_register() many times */\n\terr = snd_card_register(chip->card);\n\tif (err < 0)\n\t\tgoto __error;\n\n\tusb_chip[chip->index] = chip;\n\tchip->num_interfaces++;\n\tusb_set_intfdata(intf, chip);\n\tatomic_dec(&chip->active);\n\tmutex_unlock(&register_mutex);\n\treturn 0;\n\n __error:\n\tif (chip) {\n\t\tif (!chip->num_interfaces)\n\t\t\tsnd_card_free(chip->card);\n\t\tatomic_dec(&chip->active);\n\t}\n\tmutex_unlock(&register_mutex);\n\treturn err;\n}",
        "code_after_change": "static int usb_audio_probe(struct usb_interface *intf,\n\t\t\t   const struct usb_device_id *usb_id)\n{\n\tstruct usb_device *dev = interface_to_usbdev(intf);\n\tconst struct snd_usb_audio_quirk *quirk =\n\t\t(const struct snd_usb_audio_quirk *)usb_id->driver_info;\n\tstruct snd_usb_audio *chip;\n\tint i, err;\n\tstruct usb_host_interface *alts;\n\tint ifnum;\n\tu32 id;\n\n\talts = &intf->altsetting[0];\n\tifnum = get_iface_desc(alts)->bInterfaceNumber;\n\tid = USB_ID(le16_to_cpu(dev->descriptor.idVendor),\n\t\t    le16_to_cpu(dev->descriptor.idProduct));\n\tif (get_alias_id(dev, &id))\n\t\tquirk = get_alias_quirk(dev, id);\n\tif (quirk && quirk->ifnum >= 0 && ifnum != quirk->ifnum)\n\t\treturn -ENXIO;\n\n\terr = snd_usb_apply_boot_quirk(dev, intf, quirk, id);\n\tif (err < 0)\n\t\treturn err;\n\n\t/*\n\t * found a config.  now register to ALSA\n\t */\n\n\t/* check whether it's already registered */\n\tchip = NULL;\n\tmutex_lock(&register_mutex);\n\tfor (i = 0; i < SNDRV_CARDS; i++) {\n\t\tif (usb_chip[i] && usb_chip[i]->dev == dev) {\n\t\t\tif (atomic_read(&usb_chip[i]->shutdown)) {\n\t\t\t\tdev_err(&dev->dev, \"USB device is in the shutdown state, cannot create a card instance\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto __error;\n\t\t\t}\n\t\t\tchip = usb_chip[i];\n\t\t\tatomic_inc(&chip->active); /* avoid autopm */\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (! chip) {\n\t\t/* it's a fresh one.\n\t\t * now look for an empty slot and create a new card instance\n\t\t */\n\t\tfor (i = 0; i < SNDRV_CARDS; i++)\n\t\t\tif (!usb_chip[i] &&\n\t\t\t    (vid[i] == -1 || vid[i] == USB_ID_VENDOR(id)) &&\n\t\t\t    (pid[i] == -1 || pid[i] == USB_ID_PRODUCT(id))) {\n\t\t\t\tif (enable[i]) {\n\t\t\t\t\terr = snd_usb_audio_create(intf, dev, i, quirk,\n\t\t\t\t\t\t\t\t   id, &chip);\n\t\t\t\t\tif (err < 0)\n\t\t\t\t\t\tgoto __error;\n\t\t\t\t\tchip->pm_intf = intf;\n\t\t\t\t\tbreak;\n\t\t\t\t} else if (vid[i] != -1 || pid[i] != -1) {\n\t\t\t\t\tdev_info(&dev->dev,\n\t\t\t\t\t\t \"device (%04x:%04x) is disabled\\n\",\n\t\t\t\t\t\t USB_ID_VENDOR(id),\n\t\t\t\t\t\t USB_ID_PRODUCT(id));\n\t\t\t\t\terr = -ENOENT;\n\t\t\t\t\tgoto __error;\n\t\t\t\t}\n\t\t\t}\n\t\tif (!chip) {\n\t\t\tdev_err(&dev->dev, \"no available usb audio device\\n\");\n\t\t\terr = -ENODEV;\n\t\t\tgoto __error;\n\t\t}\n\t}\n\tdev_set_drvdata(&dev->dev, chip);\n\n\t/*\n\t * For devices with more than one control interface, we assume the\n\t * first contains the audio controls. We might need a more specific\n\t * check here in the future.\n\t */\n\tif (!chip->ctrl_intf)\n\t\tchip->ctrl_intf = alts;\n\n\tchip->txfr_quirk = 0;\n\terr = 1; /* continue */\n\tif (quirk && quirk->ifnum != QUIRK_NO_INTERFACE) {\n\t\t/* need some special handlings */\n\t\terr = snd_usb_create_quirk(chip, intf, &usb_audio_driver, quirk);\n\t\tif (err < 0)\n\t\t\tgoto __error;\n\t}\n\n\tif (err > 0) {\n\t\t/* create normal USB audio interfaces */\n\t\terr = snd_usb_create_streams(chip, ifnum);\n\t\tif (err < 0)\n\t\t\tgoto __error;\n\t\terr = snd_usb_create_mixer(chip, ifnum, ignore_ctl_error);\n\t\tif (err < 0)\n\t\t\tgoto __error;\n\t}\n\n\t/* we are allowed to call snd_card_register() many times */\n\terr = snd_card_register(chip->card);\n\tif (err < 0)\n\t\tgoto __error;\n\n\tusb_chip[chip->index] = chip;\n\tchip->num_interfaces++;\n\tusb_set_intfdata(intf, chip);\n\tatomic_dec(&chip->active);\n\tmutex_unlock(&register_mutex);\n\treturn 0;\n\n __error:\n\tif (chip) {\n\t\t/* chip->active is inside the chip->card object,\n\t\t * decrement before memory is possibly returned.\n\t\t */\n\t\tatomic_dec(&chip->active);\n\t\tif (!chip->num_interfaces)\n\t\t\tsnd_card_free(chip->card);\n\t}\n\tmutex_unlock(&register_mutex);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -115,9 +115,12 @@\n \n  __error:\n \tif (chip) {\n+\t\t/* chip->active is inside the chip->card object,\n+\t\t * decrement before memory is possibly returned.\n+\t\t */\n+\t\tatomic_dec(&chip->active);\n \t\tif (!chip->num_interfaces)\n \t\t\tsnd_card_free(chip->card);\n-\t\tatomic_dec(&chip->active);\n \t}\n \tmutex_unlock(&register_mutex);\n \treturn err;",
        "function_modified_lines": {
            "added": [
                "\t\t/* chip->active is inside the chip->card object,",
                "\t\t * decrement before memory is possibly returned.",
                "\t\t */",
                "\t\tatomic_dec(&chip->active);"
            ],
            "deleted": [
                "\t\tatomic_dec(&chip->active);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel through 4.19.6, a local user could exploit a use-after-free in the ALSA driver by supplying a malicious USB Sound device (with zero interfaces) that is mishandled in usb_audio_probe in sound/usb/card.c.",
        "id": 1745
    },
    {
        "cve_id": "CVE-2022-20566",
        "code_before_change": "static struct l2cap_chan *l2cap_get_chan_by_dcid(struct l2cap_conn *conn,\n\t\t\t\t\t\t u16 cid)\n{\n\tstruct l2cap_chan *c;\n\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_dcid(conn, cid);\n\tif (c)\n\t\tl2cap_chan_lock(c);\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn c;\n}",
        "code_after_change": "static struct l2cap_chan *l2cap_get_chan_by_dcid(struct l2cap_conn *conn,\n\t\t\t\t\t\t u16 cid)\n{\n\tstruct l2cap_chan *c;\n\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_dcid(conn, cid);\n\tif (c) {\n\t\t/* Only lock if chan reference is not 0 */\n\t\tc = l2cap_chan_hold_unless_zero(c);\n\t\tif (c)\n\t\t\tl2cap_chan_lock(c);\n\t}\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn c;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,12 @@\n \n \tmutex_lock(&conn->chan_lock);\n \tc = __l2cap_get_chan_by_dcid(conn, cid);\n-\tif (c)\n-\t\tl2cap_chan_lock(c);\n+\tif (c) {\n+\t\t/* Only lock if chan reference is not 0 */\n+\t\tc = l2cap_chan_hold_unless_zero(c);\n+\t\tif (c)\n+\t\t\tl2cap_chan_lock(c);\n+\t}\n \tmutex_unlock(&conn->chan_lock);\n \n \treturn c;",
        "function_modified_lines": {
            "added": [
                "\tif (c) {",
                "\t\t/* Only lock if chan reference is not 0 */",
                "\t\tc = l2cap_chan_hold_unless_zero(c);",
                "\t\tif (c)",
                "\t\t\tl2cap_chan_lock(c);",
                "\t}"
            ],
            "deleted": [
                "\tif (c)",
                "\t\tl2cap_chan_lock(c);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In l2cap_chan_put of l2cap_core, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-165329981References: Upstream kernel",
        "id": 3386
    },
    {
        "cve_id": "CVE-2020-27786",
        "code_before_change": "static long snd_rawmidi_kernel_read1(struct snd_rawmidi_substream *substream,\n\t\t\t\t     unsigned char __user *userbuf,\n\t\t\t\t     unsigned char *kernelbuf, long count)\n{\n\tunsigned long flags;\n\tlong result = 0, count1;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\tunsigned long appl_ptr;\n\n\tspin_lock_irqsave(&runtime->lock, flags);\n\twhile (count > 0 && runtime->avail) {\n\t\tcount1 = runtime->buffer_size - runtime->appl_ptr;\n\t\tif (count1 > count)\n\t\t\tcount1 = count;\n\t\tif (count1 > (int)runtime->avail)\n\t\t\tcount1 = runtime->avail;\n\n\t\t/* update runtime->appl_ptr before unlocking for userbuf */\n\t\tappl_ptr = runtime->appl_ptr;\n\t\truntime->appl_ptr += count1;\n\t\truntime->appl_ptr %= runtime->buffer_size;\n\t\truntime->avail -= count1;\n\n\t\tif (kernelbuf)\n\t\t\tmemcpy(kernelbuf + result, runtime->buffer + appl_ptr, count1);\n\t\tif (userbuf) {\n\t\t\tspin_unlock_irqrestore(&runtime->lock, flags);\n\t\t\tif (copy_to_user(userbuf + result,\n\t\t\t\t\t runtime->buffer + appl_ptr, count1)) {\n\t\t\t\treturn result > 0 ? result : -EFAULT;\n\t\t\t}\n\t\t\tspin_lock_irqsave(&runtime->lock, flags);\n\t\t}\n\t\tresult += count1;\n\t\tcount -= count1;\n\t}\n\tspin_unlock_irqrestore(&runtime->lock, flags);\n\treturn result;\n}",
        "code_after_change": "static long snd_rawmidi_kernel_read1(struct snd_rawmidi_substream *substream,\n\t\t\t\t     unsigned char __user *userbuf,\n\t\t\t\t     unsigned char *kernelbuf, long count)\n{\n\tunsigned long flags;\n\tlong result = 0, count1;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\tunsigned long appl_ptr;\n\tint err = 0;\n\n\tspin_lock_irqsave(&runtime->lock, flags);\n\tsnd_rawmidi_buffer_ref(runtime);\n\twhile (count > 0 && runtime->avail) {\n\t\tcount1 = runtime->buffer_size - runtime->appl_ptr;\n\t\tif (count1 > count)\n\t\t\tcount1 = count;\n\t\tif (count1 > (int)runtime->avail)\n\t\t\tcount1 = runtime->avail;\n\n\t\t/* update runtime->appl_ptr before unlocking for userbuf */\n\t\tappl_ptr = runtime->appl_ptr;\n\t\truntime->appl_ptr += count1;\n\t\truntime->appl_ptr %= runtime->buffer_size;\n\t\truntime->avail -= count1;\n\n\t\tif (kernelbuf)\n\t\t\tmemcpy(kernelbuf + result, runtime->buffer + appl_ptr, count1);\n\t\tif (userbuf) {\n\t\t\tspin_unlock_irqrestore(&runtime->lock, flags);\n\t\t\tif (copy_to_user(userbuf + result,\n\t\t\t\t\t runtime->buffer + appl_ptr, count1))\n\t\t\t\terr = -EFAULT;\n\t\t\tspin_lock_irqsave(&runtime->lock, flags);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t\tresult += count1;\n\t\tcount -= count1;\n\t}\n out:\n\tsnd_rawmidi_buffer_unref(runtime);\n\tspin_unlock_irqrestore(&runtime->lock, flags);\n\treturn result > 0 ? result : err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,10 @@\n \tlong result = 0, count1;\n \tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n \tunsigned long appl_ptr;\n+\tint err = 0;\n \n \tspin_lock_irqsave(&runtime->lock, flags);\n+\tsnd_rawmidi_buffer_ref(runtime);\n \twhile (count > 0 && runtime->avail) {\n \t\tcount1 = runtime->buffer_size - runtime->appl_ptr;\n \t\tif (count1 > count)\n@@ -26,14 +28,17 @@\n \t\tif (userbuf) {\n \t\t\tspin_unlock_irqrestore(&runtime->lock, flags);\n \t\t\tif (copy_to_user(userbuf + result,\n-\t\t\t\t\t runtime->buffer + appl_ptr, count1)) {\n-\t\t\t\treturn result > 0 ? result : -EFAULT;\n-\t\t\t}\n+\t\t\t\t\t runtime->buffer + appl_ptr, count1))\n+\t\t\t\terr = -EFAULT;\n \t\t\tspin_lock_irqsave(&runtime->lock, flags);\n+\t\t\tif (err)\n+\t\t\t\tgoto out;\n \t\t}\n \t\tresult += count1;\n \t\tcount -= count1;\n \t}\n+ out:\n+\tsnd_rawmidi_buffer_unref(runtime);\n \tspin_unlock_irqrestore(&runtime->lock, flags);\n-\treturn result;\n+\treturn result > 0 ? result : err;\n }",
        "function_modified_lines": {
            "added": [
                "\tint err = 0;",
                "\tsnd_rawmidi_buffer_ref(runtime);",
                "\t\t\t\t\t runtime->buffer + appl_ptr, count1))",
                "\t\t\t\terr = -EFAULT;",
                "\t\t\tif (err)",
                "\t\t\t\tgoto out;",
                " out:",
                "\tsnd_rawmidi_buffer_unref(runtime);",
                "\treturn result > 0 ? result : err;"
            ],
            "deleted": [
                "\t\t\t\t\t runtime->buffer + appl_ptr, count1)) {",
                "\t\t\t\treturn result > 0 ? result : -EFAULT;",
                "\t\t\t}",
                "\treturn result;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel’s implementation of MIDI, where an attacker with a local account and the permissions to issue ioctl commands to midi devices could trigger a use-after-free issue. A write to this specific memory while freed and before use causes the flow of execution to change and possibly allow for memory corruption or privilege escalation. The highest threat from this vulnerability is to confidentiality, integrity, as well as system availability.",
        "id": 2633
    },
    {
        "cve_id": "CVE-2020-7053",
        "code_before_change": "int i915_gem_context_destroy_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t   struct drm_file *file)\n{\n\tstruct drm_i915_gem_context_destroy *args = data;\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\tstruct i915_gem_context *ctx;\n\tint ret;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tif (args->ctx_id == DEFAULT_CONTEXT_HANDLE)\n\t\treturn -ENOENT;\n\n\tctx = i915_gem_context_lookup(file_priv, args->ctx_id);\n\tif (!ctx)\n\t\treturn -ENOENT;\n\n\tret = mutex_lock_interruptible(&dev->struct_mutex);\n\tif (ret)\n\t\tgoto out;\n\n\tidr_remove(&file_priv->context_idr, ctx->user_handle);\n\tcontext_close(ctx);\n\n\tmutex_unlock(&dev->struct_mutex);\n\nout:\n\ti915_gem_context_put(ctx);\n\treturn 0;\n}",
        "code_after_change": "int i915_gem_context_destroy_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t   struct drm_file *file)\n{\n\tstruct drm_i915_gem_context_destroy *args = data;\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\tstruct i915_gem_context *ctx;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tif (args->ctx_id == DEFAULT_CONTEXT_HANDLE)\n\t\treturn -ENOENT;\n\n\tif (mutex_lock_interruptible(&file_priv->context_idr_lock))\n\t\treturn -EINTR;\n\n\tctx = idr_remove(&file_priv->context_idr, args->ctx_id);\n\tmutex_unlock(&file_priv->context_idr_lock);\n\tif (!ctx)\n\t\treturn -ENOENT;\n\n\tmutex_lock(&dev->struct_mutex);\n\tcontext_close(ctx);\n\tmutex_unlock(&dev->struct_mutex);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \tstruct drm_i915_gem_context_destroy *args = data;\n \tstruct drm_i915_file_private *file_priv = file->driver_priv;\n \tstruct i915_gem_context *ctx;\n-\tint ret;\n \n \tif (args->pad != 0)\n \t\treturn -EINVAL;\n@@ -12,20 +11,17 @@\n \tif (args->ctx_id == DEFAULT_CONTEXT_HANDLE)\n \t\treturn -ENOENT;\n \n-\tctx = i915_gem_context_lookup(file_priv, args->ctx_id);\n+\tif (mutex_lock_interruptible(&file_priv->context_idr_lock))\n+\t\treturn -EINTR;\n+\n+\tctx = idr_remove(&file_priv->context_idr, args->ctx_id);\n+\tmutex_unlock(&file_priv->context_idr_lock);\n \tif (!ctx)\n \t\treturn -ENOENT;\n \n-\tret = mutex_lock_interruptible(&dev->struct_mutex);\n-\tif (ret)\n-\t\tgoto out;\n-\n-\tidr_remove(&file_priv->context_idr, ctx->user_handle);\n+\tmutex_lock(&dev->struct_mutex);\n \tcontext_close(ctx);\n-\n \tmutex_unlock(&dev->struct_mutex);\n \n-out:\n-\ti915_gem_context_put(ctx);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (mutex_lock_interruptible(&file_priv->context_idr_lock))",
                "\t\treturn -EINTR;",
                "",
                "\tctx = idr_remove(&file_priv->context_idr, args->ctx_id);",
                "\tmutex_unlock(&file_priv->context_idr_lock);",
                "\tmutex_lock(&dev->struct_mutex);"
            ],
            "deleted": [
                "\tint ret;",
                "\tctx = i915_gem_context_lookup(file_priv, args->ctx_id);",
                "\tret = mutex_lock_interruptible(&dev->struct_mutex);",
                "\tif (ret)",
                "\t\tgoto out;",
                "",
                "\tidr_remove(&file_priv->context_idr, ctx->user_handle);",
                "",
                "out:",
                "\ti915_gem_context_put(ctx);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 4.14 longterm through 4.14.165 and 4.19 longterm through 4.19.96 (and 5.x before 5.2), there is a use-after-free (write) in the i915_ppgtt_close function in drivers/gpu/drm/i915/i915_gem_gtt.c, aka CID-7dc40713618c. This is related to i915_gem_context_destroy_ioctl in drivers/gpu/drm/i915/i915_gem_context.c.",
        "id": 2798
    },
    {
        "cve_id": "CVE-2023-1859",
        "code_before_change": "static void xen_9pfs_front_free(struct xen_9pfs_front_priv *priv)\n{\n\tint i, j;\n\n\twrite_lock(&xen_9pfs_lock);\n\tlist_del(&priv->list);\n\twrite_unlock(&xen_9pfs_lock);\n\n\tfor (i = 0; i < priv->num_rings; i++) {\n\t\tif (!priv->rings[i].intf)\n\t\t\tbreak;\n\t\tif (priv->rings[i].irq > 0)\n\t\t\tunbind_from_irqhandler(priv->rings[i].irq, priv->dev);\n\t\tif (priv->rings[i].data.in) {\n\t\t\tfor (j = 0;\n\t\t\t     j < (1 << priv->rings[i].intf->ring_order);\n\t\t\t     j++) {\n\t\t\t\tgrant_ref_t ref;\n\n\t\t\t\tref = priv->rings[i].intf->ref[j];\n\t\t\t\tgnttab_end_foreign_access(ref, NULL);\n\t\t\t}\n\t\t\tfree_pages_exact(priv->rings[i].data.in,\n\t\t\t\t   1UL << (priv->rings[i].intf->ring_order +\n\t\t\t\t\t   XEN_PAGE_SHIFT));\n\t\t}\n\t\tgnttab_end_foreign_access(priv->rings[i].ref, NULL);\n\t\tfree_page((unsigned long)priv->rings[i].intf);\n\t}\n\tkfree(priv->rings);\n\tkfree(priv->tag);\n\tkfree(priv);\n}",
        "code_after_change": "static void xen_9pfs_front_free(struct xen_9pfs_front_priv *priv)\n{\n\tint i, j;\n\n\twrite_lock(&xen_9pfs_lock);\n\tlist_del(&priv->list);\n\twrite_unlock(&xen_9pfs_lock);\n\n\tfor (i = 0; i < priv->num_rings; i++) {\n\t\tstruct xen_9pfs_dataring *ring = &priv->rings[i];\n\n\t\tcancel_work_sync(&ring->work);\n\n\t\tif (!priv->rings[i].intf)\n\t\t\tbreak;\n\t\tif (priv->rings[i].irq > 0)\n\t\t\tunbind_from_irqhandler(priv->rings[i].irq, priv->dev);\n\t\tif (priv->rings[i].data.in) {\n\t\t\tfor (j = 0;\n\t\t\t     j < (1 << priv->rings[i].intf->ring_order);\n\t\t\t     j++) {\n\t\t\t\tgrant_ref_t ref;\n\n\t\t\t\tref = priv->rings[i].intf->ref[j];\n\t\t\t\tgnttab_end_foreign_access(ref, NULL);\n\t\t\t}\n\t\t\tfree_pages_exact(priv->rings[i].data.in,\n\t\t\t\t   1UL << (priv->rings[i].intf->ring_order +\n\t\t\t\t\t   XEN_PAGE_SHIFT));\n\t\t}\n\t\tgnttab_end_foreign_access(priv->rings[i].ref, NULL);\n\t\tfree_page((unsigned long)priv->rings[i].intf);\n\t}\n\tkfree(priv->rings);\n\tkfree(priv->tag);\n\tkfree(priv);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,10 @@\n \twrite_unlock(&xen_9pfs_lock);\n \n \tfor (i = 0; i < priv->num_rings; i++) {\n+\t\tstruct xen_9pfs_dataring *ring = &priv->rings[i];\n+\n+\t\tcancel_work_sync(&ring->work);\n+\n \t\tif (!priv->rings[i].intf)\n \t\t\tbreak;\n \t\tif (priv->rings[i].irq > 0)",
        "function_modified_lines": {
            "added": [
                "\t\tstruct xen_9pfs_dataring *ring = &priv->rings[i];",
                "",
                "\t\tcancel_work_sync(&ring->work);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in xen_9pfs_front_removet in net/9p/trans_xen.c in Xen transport for 9pfs in the Linux Kernel. This flaw could allow a local attacker to crash the system due to a race problem, possibly leading to a kernel information leak.",
        "id": 3881
    },
    {
        "cve_id": "CVE-2021-4202",
        "code_before_change": "inline int nci_request(struct nci_dev *ndev,\n\t\t       void (*req)(struct nci_dev *ndev,\n\t\t\t\t   const void *opt),\n\t\t       const void *opt, __u32 timeout)\n{\n\tint rc;\n\n\tif (!test_bit(NCI_UP, &ndev->flags))\n\t\treturn -ENETDOWN;\n\n\t/* Serialize all requests */\n\tmutex_lock(&ndev->req_lock);\n\trc = __nci_request(ndev, req, opt, timeout);\n\tmutex_unlock(&ndev->req_lock);\n\n\treturn rc;\n}",
        "code_after_change": "inline int nci_request(struct nci_dev *ndev,\n\t\t       void (*req)(struct nci_dev *ndev,\n\t\t\t\t   const void *opt),\n\t\t       const void *opt, __u32 timeout)\n{\n\tint rc;\n\n\t/* Serialize all requests */\n\tmutex_lock(&ndev->req_lock);\n\t/* check the state after obtaing the lock against any races\n\t * from nci_close_device when the device gets removed.\n\t */\n\tif (test_bit(NCI_UP, &ndev->flags))\n\t\trc = __nci_request(ndev, req, opt, timeout);\n\telse\n\t\trc = -ENETDOWN;\n\tmutex_unlock(&ndev->req_lock);\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,12 +5,15 @@\n {\n \tint rc;\n \n-\tif (!test_bit(NCI_UP, &ndev->flags))\n-\t\treturn -ENETDOWN;\n-\n \t/* Serialize all requests */\n \tmutex_lock(&ndev->req_lock);\n-\trc = __nci_request(ndev, req, opt, timeout);\n+\t/* check the state after obtaing the lock against any races\n+\t * from nci_close_device when the device gets removed.\n+\t */\n+\tif (test_bit(NCI_UP, &ndev->flags))\n+\t\trc = __nci_request(ndev, req, opt, timeout);\n+\telse\n+\t\trc = -ENETDOWN;\n \tmutex_unlock(&ndev->req_lock);\n \n \treturn rc;",
        "function_modified_lines": {
            "added": [
                "\t/* check the state after obtaing the lock against any races",
                "\t * from nci_close_device when the device gets removed.",
                "\t */",
                "\tif (test_bit(NCI_UP, &ndev->flags))",
                "\t\trc = __nci_request(ndev, req, opt, timeout);",
                "\telse",
                "\t\trc = -ENETDOWN;"
            ],
            "deleted": [
                "\tif (!test_bit(NCI_UP, &ndev->flags))",
                "\t\treturn -ENETDOWN;",
                "",
                "\trc = __nci_request(ndev, req, opt, timeout);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in nci_request in net/nfc/nci/core.c in NFC Controller Interface (NCI) in the Linux kernel. This flaw could allow a local attacker with user privileges to cause a data race problem while the device is getting removed, leading to a privilege escalation problem.",
        "id": 3145
    },
    {
        "cve_id": "CVE-2022-1882",
        "code_before_change": "static bool post_one_notification(struct watch_queue *wqueue,\n\t\t\t\t  struct watch_notification *n)\n{\n\tvoid *p;\n\tstruct pipe_inode_info *pipe = wqueue->pipe;\n\tstruct pipe_buffer *buf;\n\tstruct page *page;\n\tunsigned int head, tail, mask, note, offset, len;\n\tbool done = false;\n\n\tif (!pipe)\n\t\treturn false;\n\n\tspin_lock_irq(&pipe->rd_wait.lock);\n\n\tif (wqueue->defunct)\n\t\tgoto out;\n\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\tif (pipe_full(head, tail, pipe->ring_size))\n\t\tgoto lost;\n\n\tnote = find_first_bit(wqueue->notes_bitmap, wqueue->nr_notes);\n\tif (note >= wqueue->nr_notes)\n\t\tgoto lost;\n\n\tpage = wqueue->notes[note / WATCH_QUEUE_NOTES_PER_PAGE];\n\toffset = note % WATCH_QUEUE_NOTES_PER_PAGE * WATCH_QUEUE_NOTE_SIZE;\n\tget_page(page);\n\tlen = n->info & WATCH_INFO_LENGTH;\n\tp = kmap_atomic(page);\n\tmemcpy(p + offset, n, len);\n\tkunmap_atomic(p);\n\n\tbuf = &pipe->bufs[head & mask];\n\tbuf->page = page;\n\tbuf->private = (unsigned long)wqueue;\n\tbuf->ops = &watch_queue_pipe_buf_ops;\n\tbuf->offset = offset;\n\tbuf->len = len;\n\tbuf->flags = PIPE_BUF_FLAG_WHOLE;\n\tsmp_store_release(&pipe->head, head + 1); /* vs pipe_read() */\n\n\tif (!test_and_clear_bit(note, wqueue->notes_bitmap)) {\n\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\tBUG();\n\t}\n\twake_up_interruptible_sync_poll_locked(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\tdone = true;\n\nout:\n\tspin_unlock_irq(&pipe->rd_wait.lock);\n\tif (done)\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\treturn done;\n\nlost:\n\tbuf = &pipe->bufs[(head - 1) & mask];\n\tbuf->flags |= PIPE_BUF_FLAG_LOSS;\n\tgoto out;\n}",
        "code_after_change": "static bool post_one_notification(struct watch_queue *wqueue,\n\t\t\t\t  struct watch_notification *n)\n{\n\tvoid *p;\n\tstruct pipe_inode_info *pipe = wqueue->pipe;\n\tstruct pipe_buffer *buf;\n\tstruct page *page;\n\tunsigned int head, tail, mask, note, offset, len;\n\tbool done = false;\n\n\tif (!pipe)\n\t\treturn false;\n\n\tspin_lock_irq(&pipe->rd_wait.lock);\n\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\tif (pipe_full(head, tail, pipe->ring_size))\n\t\tgoto lost;\n\n\tnote = find_first_bit(wqueue->notes_bitmap, wqueue->nr_notes);\n\tif (note >= wqueue->nr_notes)\n\t\tgoto lost;\n\n\tpage = wqueue->notes[note / WATCH_QUEUE_NOTES_PER_PAGE];\n\toffset = note % WATCH_QUEUE_NOTES_PER_PAGE * WATCH_QUEUE_NOTE_SIZE;\n\tget_page(page);\n\tlen = n->info & WATCH_INFO_LENGTH;\n\tp = kmap_atomic(page);\n\tmemcpy(p + offset, n, len);\n\tkunmap_atomic(p);\n\n\tbuf = &pipe->bufs[head & mask];\n\tbuf->page = page;\n\tbuf->private = (unsigned long)wqueue;\n\tbuf->ops = &watch_queue_pipe_buf_ops;\n\tbuf->offset = offset;\n\tbuf->len = len;\n\tbuf->flags = PIPE_BUF_FLAG_WHOLE;\n\tsmp_store_release(&pipe->head, head + 1); /* vs pipe_read() */\n\n\tif (!test_and_clear_bit(note, wqueue->notes_bitmap)) {\n\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\tBUG();\n\t}\n\twake_up_interruptible_sync_poll_locked(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\tdone = true;\n\nout:\n\tspin_unlock_irq(&pipe->rd_wait.lock);\n\tif (done)\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\treturn done;\n\nlost:\n\tbuf = &pipe->bufs[(head - 1) & mask];\n\tbuf->flags |= PIPE_BUF_FLAG_LOSS;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,9 +12,6 @@\n \t\treturn false;\n \n \tspin_lock_irq(&pipe->rd_wait.lock);\n-\n-\tif (wqueue->defunct)\n-\t\tgoto out;\n \n \tmask = pipe->ring_size - 1;\n \thead = pipe->head;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tif (wqueue->defunct)",
                "\t\tgoto out;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s pipes functionality in how a user performs manipulations with the pipe post_one_notification() after free_pipe_info() that is already called. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3297
    },
    {
        "cve_id": "CVE-2022-42896",
        "code_before_change": "static inline int l2cap_ecred_conn_req(struct l2cap_conn *conn,\n\t\t\t\t       struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t       u8 *data)\n{\n\tstruct l2cap_ecred_conn_req *req = (void *) data;\n\tstruct {\n\t\tstruct l2cap_ecred_conn_rsp rsp;\n\t\t__le16 dcid[L2CAP_ECRED_MAX_CID];\n\t} __packed pdu;\n\tstruct l2cap_chan *chan, *pchan;\n\tu16 mtu, mps;\n\t__le16 psm;\n\tu8 result, len = 0;\n\tint i, num_scid;\n\tbool defer = false;\n\n\tif (!enable_ecred)\n\t\treturn -EINVAL;\n\n\tif (cmd_len < sizeof(*req) || (cmd_len - sizeof(*req)) % sizeof(u16)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tcmd_len -= sizeof(*req);\n\tnum_scid = cmd_len / sizeof(u16);\n\n\tif (num_scid > ARRAY_SIZE(pdu.dcid)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tmtu  = __le16_to_cpu(req->mtu);\n\tmps  = __le16_to_cpu(req->mps);\n\n\tif (mtu < L2CAP_ECRED_MIN_MTU || mps < L2CAP_ECRED_MIN_MPS) {\n\t\tresult = L2CAP_CR_LE_UNACCEPT_PARAMS;\n\t\tgoto response;\n\t}\n\n\tpsm  = req->psm;\n\n\tBT_DBG(\"psm 0x%2.2x mtu %u mps %u\", __le16_to_cpu(psm), mtu, mps);\n\n\tmemset(&pdu, 0, sizeof(pdu));\n\n\t/* Check if we have socket listening on psm */\n\tpchan = l2cap_global_chan_by_psm(BT_LISTEN, psm, &conn->hcon->src,\n\t\t\t\t\t &conn->hcon->dst, LE_LINK);\n\tif (!pchan) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tgoto response;\n\t}\n\n\tmutex_lock(&conn->chan_lock);\n\tl2cap_chan_lock(pchan);\n\n\tif (!smp_sufficient_security(conn->hcon, pchan->sec_level,\n\t\t\t\t     SMP_ALLOW_STK)) {\n\t\tresult = L2CAP_CR_LE_AUTHENTICATION;\n\t\tgoto unlock;\n\t}\n\n\tresult = L2CAP_CR_LE_SUCCESS;\n\n\tfor (i = 0; i < num_scid; i++) {\n\t\tu16 scid = __le16_to_cpu(req->scid[i]);\n\n\t\tBT_DBG(\"scid[%d] 0x%4.4x\", i, scid);\n\n\t\tpdu.dcid[i] = 0x0000;\n\t\tlen += sizeof(*pdu.dcid);\n\n\t\t/* Check for valid dynamic CID range */\n\t\tif (scid < L2CAP_CID_DYN_START || scid > L2CAP_CID_LE_DYN_END) {\n\t\t\tresult = L2CAP_CR_LE_INVALID_SCID;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Check if we already have channel with that dcid */\n\t\tif (__l2cap_get_chan_by_dcid(conn, scid)) {\n\t\t\tresult = L2CAP_CR_LE_SCID_IN_USE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tchan = pchan->ops->new_connection(pchan);\n\t\tif (!chan) {\n\t\t\tresult = L2CAP_CR_LE_NO_MEM;\n\t\t\tcontinue;\n\t\t}\n\n\t\tbacpy(&chan->src, &conn->hcon->src);\n\t\tbacpy(&chan->dst, &conn->hcon->dst);\n\t\tchan->src_type = bdaddr_src_type(conn->hcon);\n\t\tchan->dst_type = bdaddr_dst_type(conn->hcon);\n\t\tchan->psm  = psm;\n\t\tchan->dcid = scid;\n\t\tchan->omtu = mtu;\n\t\tchan->remote_mps = mps;\n\n\t\t__l2cap_chan_add(conn, chan);\n\n\t\tl2cap_ecred_init(chan, __le16_to_cpu(req->credits));\n\n\t\t/* Init response */\n\t\tif (!pdu.rsp.credits) {\n\t\t\tpdu.rsp.mtu = cpu_to_le16(chan->imtu);\n\t\t\tpdu.rsp.mps = cpu_to_le16(chan->mps);\n\t\t\tpdu.rsp.credits = cpu_to_le16(chan->rx_credits);\n\t\t}\n\n\t\tpdu.dcid[i] = cpu_to_le16(chan->scid);\n\n\t\t__set_chan_timer(chan, chan->ops->get_sndtimeo(chan));\n\n\t\tchan->ident = cmd->ident;\n\n\t\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\t\tl2cap_state_change(chan, BT_CONNECT2);\n\t\t\tdefer = true;\n\t\t\tchan->ops->defer(chan);\n\t\t} else {\n\t\t\tl2cap_chan_ready(chan);\n\t\t}\n\t}\n\nunlock:\n\tl2cap_chan_unlock(pchan);\n\tmutex_unlock(&conn->chan_lock);\n\tl2cap_chan_put(pchan);\n\nresponse:\n\tpdu.rsp.result = cpu_to_le16(result);\n\n\tif (defer)\n\t\treturn 0;\n\n\tl2cap_send_cmd(conn, cmd->ident, L2CAP_ECRED_CONN_RSP,\n\t\t       sizeof(pdu.rsp) + len, &pdu);\n\n\treturn 0;\n}",
        "code_after_change": "static inline int l2cap_ecred_conn_req(struct l2cap_conn *conn,\n\t\t\t\t       struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t       u8 *data)\n{\n\tstruct l2cap_ecred_conn_req *req = (void *) data;\n\tstruct {\n\t\tstruct l2cap_ecred_conn_rsp rsp;\n\t\t__le16 dcid[L2CAP_ECRED_MAX_CID];\n\t} __packed pdu;\n\tstruct l2cap_chan *chan, *pchan;\n\tu16 mtu, mps;\n\t__le16 psm;\n\tu8 result, len = 0;\n\tint i, num_scid;\n\tbool defer = false;\n\n\tif (!enable_ecred)\n\t\treturn -EINVAL;\n\n\tif (cmd_len < sizeof(*req) || (cmd_len - sizeof(*req)) % sizeof(u16)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tcmd_len -= sizeof(*req);\n\tnum_scid = cmd_len / sizeof(u16);\n\n\tif (num_scid > ARRAY_SIZE(pdu.dcid)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tmtu  = __le16_to_cpu(req->mtu);\n\tmps  = __le16_to_cpu(req->mps);\n\n\tif (mtu < L2CAP_ECRED_MIN_MTU || mps < L2CAP_ECRED_MIN_MPS) {\n\t\tresult = L2CAP_CR_LE_UNACCEPT_PARAMS;\n\t\tgoto response;\n\t}\n\n\tpsm  = req->psm;\n\n\t/* BLUETOOTH CORE SPECIFICATION Version 5.3 | Vol 3, Part A\n\t * page 1059:\n\t *\n\t * Valid range: 0x0001-0x00ff\n\t *\n\t * Table 4.15: L2CAP_LE_CREDIT_BASED_CONNECTION_REQ SPSM ranges\n\t */\n\tif (!psm || __le16_to_cpu(psm) > L2CAP_PSM_LE_DYN_END) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tgoto response;\n\t}\n\n\tBT_DBG(\"psm 0x%2.2x mtu %u mps %u\", __le16_to_cpu(psm), mtu, mps);\n\n\tmemset(&pdu, 0, sizeof(pdu));\n\n\t/* Check if we have socket listening on psm */\n\tpchan = l2cap_global_chan_by_psm(BT_LISTEN, psm, &conn->hcon->src,\n\t\t\t\t\t &conn->hcon->dst, LE_LINK);\n\tif (!pchan) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tgoto response;\n\t}\n\n\tmutex_lock(&conn->chan_lock);\n\tl2cap_chan_lock(pchan);\n\n\tif (!smp_sufficient_security(conn->hcon, pchan->sec_level,\n\t\t\t\t     SMP_ALLOW_STK)) {\n\t\tresult = L2CAP_CR_LE_AUTHENTICATION;\n\t\tgoto unlock;\n\t}\n\n\tresult = L2CAP_CR_LE_SUCCESS;\n\n\tfor (i = 0; i < num_scid; i++) {\n\t\tu16 scid = __le16_to_cpu(req->scid[i]);\n\n\t\tBT_DBG(\"scid[%d] 0x%4.4x\", i, scid);\n\n\t\tpdu.dcid[i] = 0x0000;\n\t\tlen += sizeof(*pdu.dcid);\n\n\t\t/* Check for valid dynamic CID range */\n\t\tif (scid < L2CAP_CID_DYN_START || scid > L2CAP_CID_LE_DYN_END) {\n\t\t\tresult = L2CAP_CR_LE_INVALID_SCID;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Check if we already have channel with that dcid */\n\t\tif (__l2cap_get_chan_by_dcid(conn, scid)) {\n\t\t\tresult = L2CAP_CR_LE_SCID_IN_USE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tchan = pchan->ops->new_connection(pchan);\n\t\tif (!chan) {\n\t\t\tresult = L2CAP_CR_LE_NO_MEM;\n\t\t\tcontinue;\n\t\t}\n\n\t\tbacpy(&chan->src, &conn->hcon->src);\n\t\tbacpy(&chan->dst, &conn->hcon->dst);\n\t\tchan->src_type = bdaddr_src_type(conn->hcon);\n\t\tchan->dst_type = bdaddr_dst_type(conn->hcon);\n\t\tchan->psm  = psm;\n\t\tchan->dcid = scid;\n\t\tchan->omtu = mtu;\n\t\tchan->remote_mps = mps;\n\n\t\t__l2cap_chan_add(conn, chan);\n\n\t\tl2cap_ecred_init(chan, __le16_to_cpu(req->credits));\n\n\t\t/* Init response */\n\t\tif (!pdu.rsp.credits) {\n\t\t\tpdu.rsp.mtu = cpu_to_le16(chan->imtu);\n\t\t\tpdu.rsp.mps = cpu_to_le16(chan->mps);\n\t\t\tpdu.rsp.credits = cpu_to_le16(chan->rx_credits);\n\t\t}\n\n\t\tpdu.dcid[i] = cpu_to_le16(chan->scid);\n\n\t\t__set_chan_timer(chan, chan->ops->get_sndtimeo(chan));\n\n\t\tchan->ident = cmd->ident;\n\n\t\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\t\tl2cap_state_change(chan, BT_CONNECT2);\n\t\t\tdefer = true;\n\t\t\tchan->ops->defer(chan);\n\t\t} else {\n\t\t\tl2cap_chan_ready(chan);\n\t\t}\n\t}\n\nunlock:\n\tl2cap_chan_unlock(pchan);\n\tmutex_unlock(&conn->chan_lock);\n\tl2cap_chan_put(pchan);\n\nresponse:\n\tpdu.rsp.result = cpu_to_le16(result);\n\n\tif (defer)\n\t\treturn 0;\n\n\tl2cap_send_cmd(conn, cmd->ident, L2CAP_ECRED_CONN_RSP,\n\t\t       sizeof(pdu.rsp) + len, &pdu);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,6 +39,18 @@\n \t}\n \n \tpsm  = req->psm;\n+\n+\t/* BLUETOOTH CORE SPECIFICATION Version 5.3 | Vol 3, Part A\n+\t * page 1059:\n+\t *\n+\t * Valid range: 0x0001-0x00ff\n+\t *\n+\t * Table 4.15: L2CAP_LE_CREDIT_BASED_CONNECTION_REQ SPSM ranges\n+\t */\n+\tif (!psm || __le16_to_cpu(psm) > L2CAP_PSM_LE_DYN_END) {\n+\t\tresult = L2CAP_CR_LE_BAD_PSM;\n+\t\tgoto response;\n+\t}\n \n \tBT_DBG(\"psm 0x%2.2x mtu %u mps %u\", __le16_to_cpu(psm), mtu, mps);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* BLUETOOTH CORE SPECIFICATION Version 5.3 | Vol 3, Part A",
                "\t * page 1059:",
                "\t *",
                "\t * Valid range: 0x0001-0x00ff",
                "\t *",
                "\t * Table 4.15: L2CAP_LE_CREDIT_BASED_CONNECTION_REQ SPSM ranges",
                "\t */",
                "\tif (!psm || __le16_to_cpu(psm) > L2CAP_PSM_LE_DYN_END) {",
                "\t\tresult = L2CAP_CR_LE_BAD_PSM;",
                "\t\tgoto response;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities in the Linux kernel's net/bluetooth/l2cap_core.c's l2cap_connect and l2cap_le_connect_req functions which may allow code execution and leaking kernel memory (respectively) remotely via Bluetooth. A remote attacker could execute code leaking kernel memory via Bluetooth if within proximity of the victim.\n\nWe recommend upgrading past commit   https://www.google.com/url  https://github.com/torvalds/linux/commit/711f8c3fb3db61897080468586b970c87c61d9e4 https://www.google.com/url \n\n",
        "id": 3740
    },
    {
        "cve_id": "CVE-2022-1734",
        "code_before_change": "void nfcmrvl_nci_unregister_dev(struct nfcmrvl_private *priv)\n{\n\tstruct nci_dev *ndev = priv->ndev;\n\n\tif (priv->ndev->nfc_dev->fw_download_in_progress)\n\t\tnfcmrvl_fw_dnld_abort(priv);\n\n\tnfcmrvl_fw_dnld_deinit(priv);\n\n\tif (gpio_is_valid(priv->config.reset_n_io))\n\t\tgpio_free(priv->config.reset_n_io);\n\n\tnci_unregister_device(ndev);\n\tnci_free_device(ndev);\n\tkfree(priv);\n}",
        "code_after_change": "void nfcmrvl_nci_unregister_dev(struct nfcmrvl_private *priv)\n{\n\tstruct nci_dev *ndev = priv->ndev;\n\n\tnci_unregister_device(ndev);\n\tif (priv->ndev->nfc_dev->fw_download_in_progress)\n\t\tnfcmrvl_fw_dnld_abort(priv);\n\n\tnfcmrvl_fw_dnld_deinit(priv);\n\n\tif (gpio_is_valid(priv->config.reset_n_io))\n\t\tgpio_free(priv->config.reset_n_io);\n\n\tnci_free_device(ndev);\n\tkfree(priv);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n {\n \tstruct nci_dev *ndev = priv->ndev;\n \n+\tnci_unregister_device(ndev);\n \tif (priv->ndev->nfc_dev->fw_download_in_progress)\n \t\tnfcmrvl_fw_dnld_abort(priv);\n \n@@ -10,7 +11,6 @@\n \tif (gpio_is_valid(priv->config.reset_n_io))\n \t\tgpio_free(priv->config.reset_n_io);\n \n-\tnci_unregister_device(ndev);\n \tnci_free_device(ndev);\n \tkfree(priv);\n }",
        "function_modified_lines": {
            "added": [
                "\tnci_unregister_device(ndev);"
            ],
            "deleted": [
                "\tnci_unregister_device(ndev);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw in Linux Kernel found in nfcmrvl_nci_unregister_dev() in drivers/nfc/nfcmrvl/main.c can lead to use after free both read or write when non synchronized between cleanup routine and firmware download routine.",
        "id": 3277
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "static void io_apoll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tint ret;\n\n\tret = io_poll_check_events(req, locked);\n\tif (ret > 0)\n\t\treturn;\n\n\tio_poll_remove_entries(req);\n\tio_poll_req_delete(req, req->ctx);\n\n\tif (!ret)\n\t\tio_req_task_submit(req, locked);\n\telse\n\t\tio_req_complete_failed(req, ret);\n}",
        "code_after_change": "static void io_apoll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tint ret;\n\n\tret = io_poll_check_events(req, locked);\n\tif (ret > 0)\n\t\treturn;\n\n\tio_poll_remove_entries(req);\n\tio_poll_tw_hash_eject(req, locked);\n\n\tif (!ret)\n\t\tio_req_task_submit(req, locked);\n\telse\n\t\tio_req_complete_failed(req, ret);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \t\treturn;\n \n \tio_poll_remove_entries(req);\n-\tio_poll_req_delete(req, req->ctx);\n+\tio_poll_tw_hash_eject(req, locked);\n \n \tif (!ret)\n \t\tio_req_task_submit(req, locked);",
        "function_modified_lines": {
            "added": [
                "\tio_poll_tw_hash_eject(req, locked);"
            ],
            "deleted": [
                "\tio_poll_req_delete(req, req->ctx);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and 0e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4072
    },
    {
        "cve_id": "CVE-2020-27066",
        "code_before_change": "static void xfrm_policy_kill(struct xfrm_policy *policy)\n{\n\tpolicy->walk.dead = 1;\n\n\tatomic_inc(&policy->genid);\n\n\tif (del_timer(&policy->polq.hold_timer))\n\t\txfrm_pol_put(policy);\n\tskb_queue_purge(&policy->polq.hold_queue);\n\n\tif (del_timer(&policy->timer))\n\t\txfrm_pol_put(policy);\n\n\txfrm_pol_put(policy);\n}",
        "code_after_change": "static void xfrm_policy_kill(struct xfrm_policy *policy)\n{\n\twrite_lock_bh(&policy->lock);\n\tpolicy->walk.dead = 1;\n\twrite_unlock_bh(&policy->lock);\n\n\tatomic_inc(&policy->genid);\n\n\tif (del_timer(&policy->polq.hold_timer))\n\t\txfrm_pol_put(policy);\n\tskb_queue_purge(&policy->polq.hold_queue);\n\n\tif (del_timer(&policy->timer))\n\t\txfrm_pol_put(policy);\n\n\txfrm_pol_put(policy);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,8 @@\n static void xfrm_policy_kill(struct xfrm_policy *policy)\n {\n+\twrite_lock_bh(&policy->lock);\n \tpolicy->walk.dead = 1;\n+\twrite_unlock_bh(&policy->lock);\n \n \tatomic_inc(&policy->genid);\n ",
        "function_modified_lines": {
            "added": [
                "\twrite_lock_bh(&policy->lock);",
                "\twrite_unlock_bh(&policy->lock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In xfrm6_tunnel_free_spi of net/ipv6/xfrm6_tunnel.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-168043318",
        "id": 2610
    },
    {
        "cve_id": "CVE-2022-4382",
        "code_before_change": "static void\ngadgetfs_kill_sb (struct super_block *sb)\n{\n\tkill_litter_super (sb);\n\tif (the_device) {\n\t\tput_dev (the_device);\n\t\tthe_device = NULL;\n\t}\n\tkfree(CHIP);\n\tCHIP = NULL;\n}",
        "code_after_change": "static void\ngadgetfs_kill_sb (struct super_block *sb)\n{\n\tmutex_lock(&sb_mutex);\n\tkill_litter_super (sb);\n\tif (the_device) {\n\t\tput_dev (the_device);\n\t\tthe_device = NULL;\n\t}\n\tkfree(CHIP);\n\tCHIP = NULL;\n\tmutex_unlock(&sb_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,7 @@\n static void\n gadgetfs_kill_sb (struct super_block *sb)\n {\n+\tmutex_lock(&sb_mutex);\n \tkill_litter_super (sb);\n \tif (the_device) {\n \t\tput_dev (the_device);\n@@ -8,4 +9,5 @@\n \t}\n \tkfree(CHIP);\n \tCHIP = NULL;\n+\tmutex_unlock(&sb_mutex);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&sb_mutex);",
                "\tmutex_unlock(&sb_mutex);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw caused by a race among the superblock operations in the gadgetfs Linux driver was found. It could be triggered by yanking out a device that is running the gadgetfs side.",
        "id": 3748
    },
    {
        "cve_id": "CVE-2020-27067",
        "code_before_change": "static void l2tp_eth_dev_uninit(struct net_device *dev)\n{\n\tstruct l2tp_eth *priv = netdev_priv(dev);\n\tstruct l2tp_eth_net *pn = l2tp_eth_pernet(dev_net(dev));\n\n\tspin_lock(&pn->l2tp_eth_lock);\n\tlist_del_init(&priv->list);\n\tspin_unlock(&pn->l2tp_eth_lock);\n\tdev_put(dev);\n}",
        "code_after_change": "static void l2tp_eth_dev_uninit(struct net_device *dev)\n{\n\tdev_put(dev);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,4 @@\n static void l2tp_eth_dev_uninit(struct net_device *dev)\n {\n-\tstruct l2tp_eth *priv = netdev_priv(dev);\n-\tstruct l2tp_eth_net *pn = l2tp_eth_pernet(dev_net(dev));\n-\n-\tspin_lock(&pn->l2tp_eth_lock);\n-\tlist_del_init(&priv->list);\n-\tspin_unlock(&pn->l2tp_eth_lock);\n \tdev_put(dev);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tstruct l2tp_eth *priv = netdev_priv(dev);",
                "\tstruct l2tp_eth_net *pn = l2tp_eth_pernet(dev_net(dev));",
                "",
                "\tspin_lock(&pn->l2tp_eth_lock);",
                "\tlist_del_init(&priv->list);",
                "\tspin_unlock(&pn->l2tp_eth_lock);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the l2tp subsystem, there is a possible use after free due to a race condition. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-152409173",
        "id": 2611
    },
    {
        "cve_id": "CVE-2020-36313",
        "code_before_change": "static inline void kvm_memslot_delete(struct kvm_memslots *slots,\n\t\t\t\t      struct kvm_memory_slot *memslot)\n{\n\tstruct kvm_memory_slot *mslots = slots->memslots;\n\tint i;\n\n\tif (WARN_ON(slots->id_to_index[memslot->id] == -1))\n\t\treturn;\n\n\tslots->used_slots--;\n\n\tfor (i = slots->id_to_index[memslot->id]; i < slots->used_slots; i++) {\n\t\tmslots[i] = mslots[i + 1];\n\t\tslots->id_to_index[mslots[i].id] = i;\n\t}\n\tmslots[i] = *memslot;\n\tslots->id_to_index[memslot->id] = -1;\n}",
        "code_after_change": "static inline void kvm_memslot_delete(struct kvm_memslots *slots,\n\t\t\t\t      struct kvm_memory_slot *memslot)\n{\n\tstruct kvm_memory_slot *mslots = slots->memslots;\n\tint i;\n\n\tif (WARN_ON(slots->id_to_index[memslot->id] == -1))\n\t\treturn;\n\n\tslots->used_slots--;\n\n\tif (atomic_read(&slots->lru_slot) >= slots->used_slots)\n\t\tatomic_set(&slots->lru_slot, 0);\n\n\tfor (i = slots->id_to_index[memslot->id]; i < slots->used_slots; i++) {\n\t\tmslots[i] = mslots[i + 1];\n\t\tslots->id_to_index[mslots[i].id] = i;\n\t}\n\tmslots[i] = *memslot;\n\tslots->id_to_index[memslot->id] = -1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,9 @@\n \n \tslots->used_slots--;\n \n+\tif (atomic_read(&slots->lru_slot) >= slots->used_slots)\n+\t\tatomic_set(&slots->lru_slot, 0);\n+\n \tfor (i = slots->id_to_index[memslot->id]; i < slots->used_slots; i++) {\n \t\tmslots[i] = mslots[i + 1];\n \t\tslots->id_to_index[mslots[i].id] = i;",
        "function_modified_lines": {
            "added": [
                "\tif (atomic_read(&slots->lru_slot) >= slots->used_slots)",
                "\t\tatomic_set(&slots->lru_slot, 0);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.7. The KVM subsystem allows out-of-range access to memslots after a deletion, aka CID-0774a964ef56. This affects arch/s390/kvm/kvm-s390.c, include/linux/kvm_host.h, and virt/kvm/kvm_main.c.",
        "id": 2719
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "static int vmw_gb_shader_init(struct vmw_private *dev_priv,\n\t\t\t      struct vmw_resource *res,\n\t\t\t      uint32_t size,\n\t\t\t      uint64_t offset,\n\t\t\t      SVGA3dShaderType type,\n\t\t\t      uint8_t num_input_sig,\n\t\t\t      uint8_t num_output_sig,\n\t\t\t      struct vmw_bo *byte_code,\n\t\t\t      void (*res_free) (struct vmw_resource *res))\n{\n\tstruct vmw_shader *shader = vmw_res_to_shader(res);\n\tint ret;\n\n\tret = vmw_resource_init(dev_priv, res, true, res_free,\n\t\t\t\t&vmw_gb_shader_func);\n\n\tif (unlikely(ret != 0)) {\n\t\tif (res_free)\n\t\t\tres_free(res);\n\t\telse\n\t\t\tkfree(res);\n\t\treturn ret;\n\t}\n\n\tres->guest_memory_size = size;\n\tif (byte_code) {\n\t\tres->guest_memory_bo = vmw_bo_reference(byte_code);\n\t\tres->guest_memory_offset = offset;\n\t}\n\tshader->size = size;\n\tshader->type = type;\n\tshader->num_input_sig = num_input_sig;\n\tshader->num_output_sig = num_output_sig;\n\n\tres->hw_destroy = vmw_hw_shader_destroy;\n\treturn 0;\n}",
        "code_after_change": "static int vmw_gb_shader_init(struct vmw_private *dev_priv,\n\t\t\t      struct vmw_resource *res,\n\t\t\t      uint32_t size,\n\t\t\t      uint64_t offset,\n\t\t\t      SVGA3dShaderType type,\n\t\t\t      uint8_t num_input_sig,\n\t\t\t      uint8_t num_output_sig,\n\t\t\t      struct vmw_bo *byte_code,\n\t\t\t      void (*res_free) (struct vmw_resource *res))\n{\n\tstruct vmw_shader *shader = vmw_res_to_shader(res);\n\tint ret;\n\n\tret = vmw_resource_init(dev_priv, res, true, res_free,\n\t\t\t\t&vmw_gb_shader_func);\n\n\tif (unlikely(ret != 0)) {\n\t\tif (res_free)\n\t\t\tres_free(res);\n\t\telse\n\t\t\tkfree(res);\n\t\treturn ret;\n\t}\n\n\tres->guest_memory_size = size;\n\tif (byte_code) {\n\t\tres->guest_memory_bo = vmw_user_bo_ref(byte_code);\n\t\tres->guest_memory_offset = offset;\n\t}\n\tshader->size = size;\n\tshader->type = type;\n\tshader->num_input_sig = num_input_sig;\n\tshader->num_output_sig = num_output_sig;\n\n\tres->hw_destroy = vmw_hw_shader_destroy;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -24,7 +24,7 @@\n \n \tres->guest_memory_size = size;\n \tif (byte_code) {\n-\t\tres->guest_memory_bo = vmw_bo_reference(byte_code);\n+\t\tres->guest_memory_bo = vmw_user_bo_ref(byte_code);\n \t\tres->guest_memory_offset = offset;\n \t}\n \tshader->size = size;",
        "function_modified_lines": {
            "added": [
                "\t\tres->guest_memory_bo = vmw_user_bo_ref(byte_code);"
            ],
            "deleted": [
                "\t\tres->guest_memory_bo = vmw_bo_reference(byte_code);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4284
    },
    {
        "cve_id": "CVE-2022-45919",
        "code_before_change": "static int dvb_ca_en50221_io_open(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_ca_private *ca = dvbdev->priv;\n\tint err;\n\tint i;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\tif (!try_module_get(ca->pub->owner))\n\t\treturn -EIO;\n\n\terr = dvb_generic_open(inode, file);\n\tif (err < 0) {\n\t\tmodule_put(ca->pub->owner);\n\t\treturn err;\n\t}\n\n\tfor (i = 0; i < ca->slot_count; i++) {\n\t\tstruct dvb_ca_slot *sl = &ca->slot_info[i];\n\n\t\tif (sl->slot_state == DVB_CA_SLOTSTATE_RUNNING) {\n\t\t\tif (!sl->rx_buffer.data) {\n\t\t\t\t/*\n\t\t\t\t * it is safe to call this here without locks\n\t\t\t\t * because ca->open == 0. Data is not read in\n\t\t\t\t * this case\n\t\t\t\t */\n\t\t\t\tdvb_ringbuffer_flush(&sl->rx_buffer);\n\t\t\t}\n\t\t}\n\t}\n\n\tca->open = 1;\n\tdvb_ca_en50221_thread_update_delay(ca);\n\tdvb_ca_en50221_thread_wakeup(ca);\n\n\tdvb_ca_private_get(ca);\n\n\treturn 0;\n}",
        "code_after_change": "static int dvb_ca_en50221_io_open(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_ca_private *ca = dvbdev->priv;\n\tint err;\n\tint i;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\tmutex_lock(&ca->remove_mutex);\n\n\tif (ca->exit) {\n\t\tmutex_unlock(&ca->remove_mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tif (!try_module_get(ca->pub->owner)) {\n\t\tmutex_unlock(&ca->remove_mutex);\n\t\treturn -EIO;\n\t}\n\n\terr = dvb_generic_open(inode, file);\n\tif (err < 0) {\n\t\tmodule_put(ca->pub->owner);\n\t\tmutex_unlock(&ca->remove_mutex);\n\t\treturn err;\n\t}\n\n\tfor (i = 0; i < ca->slot_count; i++) {\n\t\tstruct dvb_ca_slot *sl = &ca->slot_info[i];\n\n\t\tif (sl->slot_state == DVB_CA_SLOTSTATE_RUNNING) {\n\t\t\tif (!sl->rx_buffer.data) {\n\t\t\t\t/*\n\t\t\t\t * it is safe to call this here without locks\n\t\t\t\t * because ca->open == 0. Data is not read in\n\t\t\t\t * this case\n\t\t\t\t */\n\t\t\t\tdvb_ringbuffer_flush(&sl->rx_buffer);\n\t\t\t}\n\t\t}\n\t}\n\n\tca->open = 1;\n\tdvb_ca_en50221_thread_update_delay(ca);\n\tdvb_ca_en50221_thread_wakeup(ca);\n\n\tdvb_ca_private_get(ca);\n\n\tmutex_unlock(&ca->remove_mutex);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,12 +7,22 @@\n \n \tdprintk(\"%s\\n\", __func__);\n \n-\tif (!try_module_get(ca->pub->owner))\n+\tmutex_lock(&ca->remove_mutex);\n+\n+\tif (ca->exit) {\n+\t\tmutex_unlock(&ca->remove_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tif (!try_module_get(ca->pub->owner)) {\n+\t\tmutex_unlock(&ca->remove_mutex);\n \t\treturn -EIO;\n+\t}\n \n \terr = dvb_generic_open(inode, file);\n \tif (err < 0) {\n \t\tmodule_put(ca->pub->owner);\n+\t\tmutex_unlock(&ca->remove_mutex);\n \t\treturn err;\n \t}\n \n@@ -37,5 +47,6 @@\n \n \tdvb_ca_private_get(ca);\n \n+\tmutex_unlock(&ca->remove_mutex);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_lock(&ca->remove_mutex);",
                "",
                "\tif (ca->exit) {",
                "\t\tmutex_unlock(&ca->remove_mutex);",
                "\t\treturn -ENODEV;",
                "\t}",
                "",
                "\tif (!try_module_get(ca->pub->owner)) {",
                "\t\tmutex_unlock(&ca->remove_mutex);",
                "\t}",
                "\t\tmutex_unlock(&ca->remove_mutex);",
                "\tmutex_unlock(&ca->remove_mutex);"
            ],
            "deleted": [
                "\tif (!try_module_get(ca->pub->owner))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 6.0.10. In drivers/media/dvb-core/dvb_ca_en50221.c, a use-after-free can occur is there is a disconnect after an open, because of the lack of a wait_event.",
        "id": 3758
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "struct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t}\n\treturn opt2;\n}",
        "code_after_change": "struct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t\tatomic_set(&opt2->refcnt, 1);\n\t}\n\treturn opt2;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,6 +15,7 @@\n \t\t\t*((char **)&opt2->dst1opt) += dif;\n \t\tif (opt2->srcrt)\n \t\t\t*((char **)&opt2->srcrt) += dif;\n+\t\tatomic_set(&opt2->refcnt, 1);\n \t}\n \treturn opt2;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tatomic_set(&opt2->refcnt, 1);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 995
    },
    {
        "cve_id": "CVE-2020-36557",
        "code_before_change": "static int con_install(struct tty_driver *driver, struct tty_struct *tty)\n{\n\tunsigned int currcons = tty->index;\n\tstruct vc_data *vc;\n\tint ret;\n\n\tconsole_lock();\n\tret = vc_allocate(currcons);\n\tif (ret)\n\t\tgoto unlock;\n\n\tvc = vc_cons[currcons].d;\n\n\t/* Still being freed */\n\tif (vc->port.tty) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto unlock;\n\t}\n\n\tret = tty_port_install(&vc->port, driver, tty);\n\tif (ret)\n\t\tgoto unlock;\n\n\ttty->driver_data = vc;\n\tvc->port.tty = tty;\n\n\tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n\t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;\n\t\ttty->winsize.ws_col = vc_cons[currcons].d->vc_cols;\n\t}\n\tif (vc->vc_utf)\n\t\ttty->termios.c_iflag |= IUTF8;\n\telse\n\t\ttty->termios.c_iflag &= ~IUTF8;\nunlock:\n\tconsole_unlock();\n\treturn ret;\n}",
        "code_after_change": "static int con_install(struct tty_driver *driver, struct tty_struct *tty)\n{\n\tunsigned int currcons = tty->index;\n\tstruct vc_data *vc;\n\tint ret;\n\n\tconsole_lock();\n\tret = vc_allocate(currcons);\n\tif (ret)\n\t\tgoto unlock;\n\n\tvc = vc_cons[currcons].d;\n\n\t/* Still being freed */\n\tif (vc->port.tty) {\n\t\tret = -ERESTARTSYS;\n\t\tgoto unlock;\n\t}\n\n\tret = tty_port_install(&vc->port, driver, tty);\n\tif (ret)\n\t\tgoto unlock;\n\n\ttty->driver_data = vc;\n\tvc->port.tty = tty;\n\ttty_port_get(&vc->port);\n\n\tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n\t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;\n\t\ttty->winsize.ws_col = vc_cons[currcons].d->vc_cols;\n\t}\n\tif (vc->vc_utf)\n\t\ttty->termios.c_iflag |= IUTF8;\n\telse\n\t\ttty->termios.c_iflag &= ~IUTF8;\nunlock:\n\tconsole_unlock();\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,6 +23,7 @@\n \n \ttty->driver_data = vc;\n \tvc->port.tty = tty;\n+\ttty_port_get(&vc->port);\n \n \tif (!tty->winsize.ws_row && !tty->winsize.ws_col) {\n \t\ttty->winsize.ws_row = vc_cons[currcons].d->vc_rows;",
        "function_modified_lines": {
            "added": [
                "\ttty_port_get(&vc->port);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "A race condition in the Linux kernel before 5.6.2 between the VT_DISALLOCATE ioctl and closing/opening of ttys could lead to a use-after-free.",
        "id": 2763
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static int kvmppc_svm_page_in(struct vm_area_struct *vma,\n\t\tunsigned long start,\n\t\tunsigned long end, unsigned long gpa, struct kvm *kvm,\n\t\tunsigned long page_shift,\n\t\tbool pagein)\n{\n\tunsigned long src_pfn, dst_pfn = 0;\n\tstruct migrate_vma mig;\n\tstruct page *spage;\n\tunsigned long pfn;\n\tstruct page *dpage;\n\tint ret = 0;\n\n\tmemset(&mig, 0, sizeof(mig));\n\tmig.vma = vma;\n\tmig.start = start;\n\tmig.end = end;\n\tmig.src = &src_pfn;\n\tmig.dst = &dst_pfn;\n\tmig.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\n\tret = migrate_vma_setup(&mig);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!(*mig.src & MIGRATE_PFN_MIGRATE)) {\n\t\tret = -1;\n\t\tgoto out_finalize;\n\t}\n\n\tdpage = kvmppc_uvmem_get_page(gpa, kvm);\n\tif (!dpage) {\n\t\tret = -1;\n\t\tgoto out_finalize;\n\t}\n\n\tif (pagein) {\n\t\tpfn = *mig.src >> MIGRATE_PFN_SHIFT;\n\t\tspage = migrate_pfn_to_page(*mig.src);\n\t\tif (spage) {\n\t\t\tret = uv_page_in(kvm->arch.lpid, pfn << page_shift,\n\t\t\t\t\tgpa, 0, page_shift);\n\t\t\tif (ret)\n\t\t\t\tgoto out_finalize;\n\t\t}\n\t}\n\n\t*mig.dst = migrate_pfn(page_to_pfn(dpage));\n\tmigrate_vma_pages(&mig);\nout_finalize:\n\tmigrate_vma_finalize(&mig);\n\treturn ret;\n}",
        "code_after_change": "static int kvmppc_svm_page_in(struct vm_area_struct *vma,\n\t\tunsigned long start,\n\t\tunsigned long end, unsigned long gpa, struct kvm *kvm,\n\t\tunsigned long page_shift,\n\t\tbool pagein)\n{\n\tunsigned long src_pfn, dst_pfn = 0;\n\tstruct migrate_vma mig = { 0 };\n\tstruct page *spage;\n\tunsigned long pfn;\n\tstruct page *dpage;\n\tint ret = 0;\n\n\tmemset(&mig, 0, sizeof(mig));\n\tmig.vma = vma;\n\tmig.start = start;\n\tmig.end = end;\n\tmig.src = &src_pfn;\n\tmig.dst = &dst_pfn;\n\tmig.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\n\tret = migrate_vma_setup(&mig);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!(*mig.src & MIGRATE_PFN_MIGRATE)) {\n\t\tret = -1;\n\t\tgoto out_finalize;\n\t}\n\n\tdpage = kvmppc_uvmem_get_page(gpa, kvm);\n\tif (!dpage) {\n\t\tret = -1;\n\t\tgoto out_finalize;\n\t}\n\n\tif (pagein) {\n\t\tpfn = *mig.src >> MIGRATE_PFN_SHIFT;\n\t\tspage = migrate_pfn_to_page(*mig.src);\n\t\tif (spage) {\n\t\t\tret = uv_page_in(kvm->arch.lpid, pfn << page_shift,\n\t\t\t\t\tgpa, 0, page_shift);\n\t\t\tif (ret)\n\t\t\t\tgoto out_finalize;\n\t\t}\n\t}\n\n\t*mig.dst = migrate_pfn(page_to_pfn(dpage));\n\tmigrate_vma_pages(&mig);\nout_finalize:\n\tmigrate_vma_finalize(&mig);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \t\tbool pagein)\n {\n \tunsigned long src_pfn, dst_pfn = 0;\n-\tstruct migrate_vma mig;\n+\tstruct migrate_vma mig = { 0 };\n \tstruct page *spage;\n \tunsigned long pfn;\n \tstruct page *dpage;",
        "function_modified_lines": {
            "added": [
                "\tstruct migrate_vma mig = { 0 };"
            ],
            "deleted": [
                "\tstruct migrate_vma mig;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3608
    },
    {
        "cve_id": "CVE-2019-15213",
        "code_before_change": "void dvb_usb_device_exit(struct usb_interface *intf)\n{\n\tstruct dvb_usb_device *d = usb_get_intfdata(intf);\n\tconst char *name = \"generic DVB-USB module\";\n\n\tusb_set_intfdata(intf, NULL);\n\tif (d != NULL && d->desc != NULL) {\n\t\tname = d->desc->name;\n\t\tdvb_usb_exit(d);\n\t}\n\tinfo(\"%s successfully deinitialized and disconnected.\", name);\n\n}",
        "code_after_change": "void dvb_usb_device_exit(struct usb_interface *intf)\n{\n\tstruct dvb_usb_device *d = usb_get_intfdata(intf);\n\tconst char *default_name = \"generic DVB-USB module\";\n\tchar name[40];\n\n\tusb_set_intfdata(intf, NULL);\n\tif (d != NULL && d->desc != NULL) {\n\t\tstrscpy(name, d->desc->name, sizeof(name));\n\t\tdvb_usb_exit(d);\n\t} else {\n\t\tstrscpy(name, default_name, sizeof(name));\n\t}\n\tinfo(\"%s successfully deinitialized and disconnected.\", name);\n\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,15 @@\n void dvb_usb_device_exit(struct usb_interface *intf)\n {\n \tstruct dvb_usb_device *d = usb_get_intfdata(intf);\n-\tconst char *name = \"generic DVB-USB module\";\n+\tconst char *default_name = \"generic DVB-USB module\";\n+\tchar name[40];\n \n \tusb_set_intfdata(intf, NULL);\n \tif (d != NULL && d->desc != NULL) {\n-\t\tname = d->desc->name;\n+\t\tstrscpy(name, d->desc->name, sizeof(name));\n \t\tdvb_usb_exit(d);\n+\t} else {\n+\t\tstrscpy(name, default_name, sizeof(name));\n \t}\n \tinfo(\"%s successfully deinitialized and disconnected.\", name);\n ",
        "function_modified_lines": {
            "added": [
                "\tconst char *default_name = \"generic DVB-USB module\";",
                "\tchar name[40];",
                "\t\tstrscpy(name, d->desc->name, sizeof(name));",
                "\t} else {",
                "\t\tstrscpy(name, default_name, sizeof(name));"
            ],
            "deleted": [
                "\tconst char *name = \"generic DVB-USB module\";",
                "\t\tname = d->desc->name;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.2.3. There is a use-after-free caused by a malicious USB device in the drivers/media/usb/dvb-usb/dvb-usb-init.c driver.",
        "id": 1995
    },
    {
        "cve_id": "CVE-2019-19318",
        "code_before_change": "struct inode *btrfs_new_test_inode(void)\n{\n\treturn new_inode(test_mnt->mnt_sb);\n}",
        "code_after_change": "struct inode *btrfs_new_test_inode(void)\n{\n\tstruct inode *inode;\n\n\tinode = new_inode(test_mnt->mnt_sb);\n\tif (inode)\n\t\tinode_init_owner(inode, NULL, S_IFREG);\n\n\treturn inode;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,10 @@\n struct inode *btrfs_new_test_inode(void)\n {\n-\treturn new_inode(test_mnt->mnt_sb);\n+\tstruct inode *inode;\n+\n+\tinode = new_inode(test_mnt->mnt_sb);\n+\tif (inode)\n+\t\tinode_init_owner(inode, NULL, S_IFREG);\n+\n+\treturn inode;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct inode *inode;",
                "",
                "\tinode = new_inode(test_mnt->mnt_sb);",
                "\tif (inode)",
                "\t\tinode_init_owner(inode, NULL, S_IFREG);",
                "",
                "\treturn inode;"
            ],
            "deleted": [
                "\treturn new_inode(test_mnt->mnt_sb);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.3.11, mounting a crafted btrfs image twice can cause an rwsem_down_write_slowpath use-after-free because (in rwsem_can_spin_on_owner in kernel/locking/rwsem.c) rwsem_owner_flags returns an already freed pointer,",
        "id": 2187
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "struct xt_table *xt_register_table(struct net *net,\n\t\t\t\t   const struct xt_table *input_table,\n\t\t\t\t   struct xt_table_info *bootstrap,\n\t\t\t\t   struct xt_table_info *newinfo)\n{\n\tint ret;\n\tstruct xt_table_info *private;\n\tstruct xt_table *t, *table;\n\n\t/* Don't add one object to multiple lists. */\n\ttable = kmemdup(input_table, sizeof(struct xt_table), GFP_KERNEL);\n\tif (!table) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&xt[table->af].mutex);\n\t/* Don't autoload: we'd eat our tail... */\n\tlist_for_each_entry(t, &net->xt.tables[table->af], list) {\n\t\tif (strcmp(t->name, table->name) == 0) {\n\t\t\tret = -EEXIST;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\t/* Simplifies replace_table code. */\n\ttable->private = bootstrap;\n\n\tif (!xt_replace_table(table, 0, newinfo, &ret))\n\t\tgoto unlock;\n\n\tprivate = table->private;\n\tpr_debug(\"table->private->number = %u\\n\", private->number);\n\n\t/* save number of initial entries */\n\tprivate->initial_entries = private->number;\n\n\tlist_add(&table->list, &net->xt.tables[table->af]);\n\tmutex_unlock(&xt[table->af].mutex);\n\treturn table;\n\nunlock:\n\tmutex_unlock(&xt[table->af].mutex);\n\tkfree(table);\nout:\n\treturn ERR_PTR(ret);\n}",
        "code_after_change": "struct xt_table *xt_register_table(struct net *net,\n\t\t\t\t   const struct xt_table *input_table,\n\t\t\t\t   struct xt_table_info *bootstrap,\n\t\t\t\t   struct xt_table_info *newinfo)\n{\n\tint ret;\n\tstruct xt_table_info *private;\n\tstruct xt_table *t, *table;\n\n\t/* Don't add one object to multiple lists. */\n\ttable = kmemdup(input_table, sizeof(struct xt_table), GFP_KERNEL);\n\tif (!table) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&xt[table->af].mutex);\n\t/* Don't autoload: we'd eat our tail... */\n\tlist_for_each_entry(t, &net->xt.tables[table->af], list) {\n\t\tif (strcmp(t->name, table->name) == 0) {\n\t\t\tret = -EEXIST;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\t/* Simplifies replace_table code. */\n\trcu_assign_pointer(table->private, bootstrap);\n\n\tif (!xt_replace_table(table, 0, newinfo, &ret))\n\t\tgoto unlock;\n\n\tprivate = xt_table_get_private_protected(table);\n\tpr_debug(\"table->private->number = %u\\n\", private->number);\n\n\t/* save number of initial entries */\n\tprivate->initial_entries = private->number;\n\n\tlist_add(&table->list, &net->xt.tables[table->af]);\n\tmutex_unlock(&xt[table->af].mutex);\n\treturn table;\n\nunlock:\n\tmutex_unlock(&xt[table->af].mutex);\n\tkfree(table);\nout:\n\treturn ERR_PTR(ret);\n}",
        "patch": "--- code before\n+++ code after\n@@ -24,12 +24,12 @@\n \t}\n \n \t/* Simplifies replace_table code. */\n-\ttable->private = bootstrap;\n+\trcu_assign_pointer(table->private, bootstrap);\n \n \tif (!xt_replace_table(table, 0, newinfo, &ret))\n \t\tgoto unlock;\n \n-\tprivate = table->private;\n+\tprivate = xt_table_get_private_protected(table);\n \tpr_debug(\"table->private->number = %u\\n\", private->number);\n \n \t/* save number of initial entries */",
        "function_modified_lines": {
            "added": [
                "\trcu_assign_pointer(table->private, bootstrap);",
                "\tprivate = xt_table_get_private_protected(table);"
            ],
            "deleted": [
                "\ttable->private = bootstrap;",
                "\tprivate = table->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2790
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static inline void iocb_put(struct aio_kiocb *iocb)\n{\n\tif (refcount_read(&iocb->ki_refcnt) == 0 ||\n\t    refcount_dec_and_test(&iocb->ki_refcnt)) {\n\t\tpercpu_ref_put(&iocb->ki_ctx->reqs);\n\t\tkmem_cache_free(kiocb_cachep, iocb);\n\t}\n}",
        "code_after_change": "static inline void iocb_put(struct aio_kiocb *iocb)\n{\n\tif (refcount_read(&iocb->ki_refcnt) == 0 ||\n\t    refcount_dec_and_test(&iocb->ki_refcnt)) {\n\t\tif (iocb->ki_filp)\n\t\t\tfput(iocb->ki_filp);\n\t\tpercpu_ref_put(&iocb->ki_ctx->reqs);\n\t\tkmem_cache_free(kiocb_cachep, iocb);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,8 @@\n {\n \tif (refcount_read(&iocb->ki_refcnt) == 0 ||\n \t    refcount_dec_and_test(&iocb->ki_refcnt)) {\n+\t\tif (iocb->ki_filp)\n+\t\t\tfput(iocb->ki_filp);\n \t\tpercpu_ref_put(&iocb->ki_ctx->reqs);\n \t\tkmem_cache_free(kiocb_cachep, iocb);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (iocb->ki_filp)",
                "\t\t\tfput(iocb->ki_filp);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1892
    },
    {
        "cve_id": "CVE-2022-1652",
        "code_before_change": "static void bad_flp_intr(void)\n{\n\tint err_count;\n\n\tif (probing) {\n\t\tdrive_state[current_drive].probed_format++;\n\t\tif (!next_valid_format(current_drive))\n\t\t\treturn;\n\t}\n\terr_count = ++(*errors);\n\tINFBOUND(write_errors[current_drive].badness, err_count);\n\tif (err_count > drive_params[current_drive].max_errors.abort)\n\t\tcont->done(0);\n\tif (err_count > drive_params[current_drive].max_errors.reset)\n\t\tfdc_state[current_fdc].reset = 1;\n\telse if (err_count > drive_params[current_drive].max_errors.recal)\n\t\tdrive_state[current_drive].track = NEED_2_RECAL;\n}",
        "code_after_change": "static void bad_flp_intr(void)\n{\n\tint err_count;\n\n\tif (probing) {\n\t\tdrive_state[current_drive].probed_format++;\n\t\tif (!next_valid_format(current_drive))\n\t\t\treturn;\n\t}\n\terr_count = ++floppy_errors;\n\tINFBOUND(write_errors[current_drive].badness, err_count);\n\tif (err_count > drive_params[current_drive].max_errors.abort)\n\t\tcont->done(0);\n\tif (err_count > drive_params[current_drive].max_errors.reset)\n\t\tfdc_state[current_fdc].reset = 1;\n\telse if (err_count > drive_params[current_drive].max_errors.recal)\n\t\tdrive_state[current_drive].track = NEED_2_RECAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \t\tif (!next_valid_format(current_drive))\n \t\t\treturn;\n \t}\n-\terr_count = ++(*errors);\n+\terr_count = ++floppy_errors;\n \tINFBOUND(write_errors[current_drive].badness, err_count);\n \tif (err_count > drive_params[current_drive].max_errors.abort)\n \t\tcont->done(0);",
        "function_modified_lines": {
            "added": [
                "\terr_count = ++floppy_errors;"
            ],
            "deleted": [
                "\terr_count = ++(*errors);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Linux Kernel could allow a local attacker to execute arbitrary code on the system, caused by a concurrency use-after-free flaw in the bad_flp_intr function. By executing a specially-crafted program, an attacker could exploit this vulnerability to execute arbitrary code or cause a denial of service condition on the system.",
        "id": 3266
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int\ndo_add_counters(struct net *net, sockptr_t arg, unsigned int len)\n{\n\tunsigned int i;\n\tstruct xt_counters_info tmp;\n\tstruct xt_counters *paddc;\n\tstruct xt_table *t;\n\tconst struct xt_table_info *private;\n\tint ret = 0;\n\tstruct ipt_entry *iter;\n\tunsigned int addend;\n\n\tpaddc = xt_copy_counters(arg, len, &tmp);\n\tif (IS_ERR(paddc))\n\t\treturn PTR_ERR(paddc);\n\n\tt = xt_find_table_lock(net, AF_INET, tmp.name);\n\tif (IS_ERR(t)) {\n\t\tret = PTR_ERR(t);\n\t\tgoto free;\n\t}\n\n\tlocal_bh_disable();\n\tprivate = t->private;\n\tif (private->number != tmp.num_counters) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_up_free;\n\t}\n\n\ti = 0;\n\taddend = xt_write_recseq_begin();\n\txt_entry_foreach(iter, private->entries, private->size) {\n\t\tstruct xt_counters *tmp;\n\n\t\ttmp = xt_get_this_cpu_counter(&iter->counters);\n\t\tADD_COUNTER(*tmp, paddc[i].bcnt, paddc[i].pcnt);\n\t\t++i;\n\t}\n\txt_write_recseq_end(addend);\n unlock_up_free:\n\tlocal_bh_enable();\n\txt_table_unlock(t);\n\tmodule_put(t->me);\n free:\n\tvfree(paddc);\n\n\treturn ret;\n}",
        "code_after_change": "static int\ndo_add_counters(struct net *net, sockptr_t arg, unsigned int len)\n{\n\tunsigned int i;\n\tstruct xt_counters_info tmp;\n\tstruct xt_counters *paddc;\n\tstruct xt_table *t;\n\tconst struct xt_table_info *private;\n\tint ret = 0;\n\tstruct ipt_entry *iter;\n\tunsigned int addend;\n\n\tpaddc = xt_copy_counters(arg, len, &tmp);\n\tif (IS_ERR(paddc))\n\t\treturn PTR_ERR(paddc);\n\n\tt = xt_find_table_lock(net, AF_INET, tmp.name);\n\tif (IS_ERR(t)) {\n\t\tret = PTR_ERR(t);\n\t\tgoto free;\n\t}\n\n\tlocal_bh_disable();\n\tprivate = xt_table_get_private_protected(t);\n\tif (private->number != tmp.num_counters) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_up_free;\n\t}\n\n\ti = 0;\n\taddend = xt_write_recseq_begin();\n\txt_entry_foreach(iter, private->entries, private->size) {\n\t\tstruct xt_counters *tmp;\n\n\t\ttmp = xt_get_this_cpu_counter(&iter->counters);\n\t\tADD_COUNTER(*tmp, paddc[i].bcnt, paddc[i].pcnt);\n\t\t++i;\n\t}\n\txt_write_recseq_end(addend);\n unlock_up_free:\n\tlocal_bh_enable();\n\txt_table_unlock(t);\n\tmodule_put(t->me);\n free:\n\tvfree(paddc);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,7 +21,7 @@\n \t}\n \n \tlocal_bh_disable();\n-\tprivate = t->private;\n+\tprivate = xt_table_get_private_protected(t);\n \tif (private->number != tmp.num_counters) {\n \t\tret = -EINVAL;\n \t\tgoto unlock_up_free;",
        "function_modified_lines": {
            "added": [
                "\tprivate = xt_table_get_private_protected(t);"
            ],
            "deleted": [
                "\tprivate = t->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2782
    },
    {
        "cve_id": "CVE-2022-0646",
        "code_before_change": "static void mctp_serial_close(struct tty_struct *tty)\n{\n\tstruct mctp_serial *dev = tty->disc_data;\n\tint idx = dev->idx;\n\n\tunregister_netdev(dev->netdev);\n\tcancel_work_sync(&dev->tx_work);\n\tida_free(&mctp_serial_ida, idx);\n}",
        "code_after_change": "static void mctp_serial_close(struct tty_struct *tty)\n{\n\tstruct mctp_serial *dev = tty->disc_data;\n\tint idx = dev->idx;\n\n\tunregister_netdev(dev->netdev);\n\tida_free(&mctp_serial_ida, idx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,5 @@\n \tint idx = dev->idx;\n \n \tunregister_netdev(dev->netdev);\n-\tcancel_work_sync(&dev->tx_work);\n \tida_free(&mctp_serial_ida, idx);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tcancel_work_sync(&dev->tx_work);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw use after free in the Linux kernel Management Component Transport Protocol (MCTP) subsystem was found in the way user triggers cancel_work_sync after the unregister_netdev during removing device. A local user could use this flaw to crash the system or escalate their privileges on the system. It is actual from Linux Kernel 5.17-rc1 (when mctp-serial.c introduced) till 5.17-rc5.",
        "id": 3217
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static int\nsvm_range_trigger_migration(struct mm_struct *mm, struct svm_range *prange,\n\t\t\t    bool *migrated)\n{\n\tuint32_t best_loc;\n\tint r = 0;\n\n\t*migrated = false;\n\tbest_loc = svm_range_best_prefetch_location(prange);\n\n\tif (best_loc == KFD_IOCTL_SVM_LOCATION_UNDEFINED ||\n\t    best_loc == prange->actual_loc)\n\t\treturn 0;\n\n\tif (!best_loc) {\n\t\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PREFETCH);\n\t\t*migrated = !r;\n\t\treturn r;\n\t}\n\n\tr = svm_migrate_to_vram(prange, best_loc, mm, KFD_MIGRATE_TRIGGER_PREFETCH);\n\t*migrated = !r;\n\n\treturn r;\n}",
        "code_after_change": "static int\nsvm_range_trigger_migration(struct mm_struct *mm, struct svm_range *prange,\n\t\t\t    bool *migrated)\n{\n\tuint32_t best_loc;\n\tint r = 0;\n\n\t*migrated = false;\n\tbest_loc = svm_range_best_prefetch_location(prange);\n\n\tif (best_loc == KFD_IOCTL_SVM_LOCATION_UNDEFINED ||\n\t    best_loc == prange->actual_loc)\n\t\treturn 0;\n\n\tif (!best_loc) {\n\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PREFETCH, NULL);\n\t\t*migrated = !r;\n\t\treturn r;\n\t}\n\n\tr = svm_migrate_to_vram(prange, best_loc, mm, KFD_MIGRATE_TRIGGER_PREFETCH);\n\t*migrated = !r;\n\n\treturn r;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,7 +13,8 @@\n \t\treturn 0;\n \n \tif (!best_loc) {\n-\t\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PREFETCH);\n+\t\tr = svm_migrate_vram_to_ram(prange, mm,\n+\t\t\t\t\tKFD_MIGRATE_TRIGGER_PREFETCH, NULL);\n \t\t*migrated = !r;\n \t\treturn r;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tr = svm_migrate_vram_to_ram(prange, mm,",
                "\t\t\t\t\tKFD_MIGRATE_TRIGGER_PREFETCH, NULL);"
            ],
            "deleted": [
                "\t\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PREFETCH);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3613
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static void blk_add_trace_rq_remap(void *ignore,\n\t\t\t\t   struct request_queue *q,\n\t\t\t\t   struct request *rq, dev_t dev,\n\t\t\t\t   sector_t from)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\tstruct blk_io_trace_remap r;\n\n\tif (likely(!bt))\n\t\treturn;\n\n\tr.device_from = cpu_to_be32(dev);\n\tr.device_to   = cpu_to_be32(disk_devt(rq->rq_disk));\n\tr.sector_from = cpu_to_be64(from);\n\n\t__blk_add_trace(bt, blk_rq_pos(rq), blk_rq_bytes(rq),\n\t\t\trq_data_dir(rq), 0, BLK_TA_REMAP, 0,\n\t\t\tsizeof(r), &r, blk_trace_request_get_cgid(q, rq));\n}",
        "code_after_change": "static void blk_add_trace_rq_remap(void *ignore,\n\t\t\t\t   struct request_queue *q,\n\t\t\t\t   struct request *rq, dev_t dev,\n\t\t\t\t   sector_t from)\n{\n\tstruct blk_trace *bt;\n\tstruct blk_io_trace_remap r;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tr.device_from = cpu_to_be32(dev);\n\tr.device_to   = cpu_to_be32(disk_devt(rq->rq_disk));\n\tr.sector_from = cpu_to_be64(from);\n\n\t__blk_add_trace(bt, blk_rq_pos(rq), blk_rq_bytes(rq),\n\t\t\trq_data_dir(rq), 0, BLK_TA_REMAP, 0,\n\t\t\tsizeof(r), &r, blk_trace_request_get_cgid(q, rq));\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,11 +3,15 @@\n \t\t\t\t   struct request *rq, dev_t dev,\n \t\t\t\t   sector_t from)\n {\n-\tstruct blk_trace *bt = q->blk_trace;\n+\tstruct blk_trace *bt;\n \tstruct blk_io_trace_remap r;\n \n-\tif (likely(!bt))\n+\trcu_read_lock();\n+\tbt = rcu_dereference(q->blk_trace);\n+\tif (likely(!bt)) {\n+\t\trcu_read_unlock();\n \t\treturn;\n+\t}\n \n \tr.device_from = cpu_to_be32(dev);\n \tr.device_to   = cpu_to_be32(disk_devt(rq->rq_disk));\n@@ -16,4 +20,5 @@\n \t__blk_add_trace(bt, blk_rq_pos(rq), blk_rq_bytes(rq),\n \t\t\trq_data_dir(rq), 0, BLK_TA_REMAP, 0,\n \t\t\tsizeof(r), &r, blk_trace_request_get_cgid(q, rq));\n+\trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct blk_trace *bt;",
                "\trcu_read_lock();",
                "\tbt = rcu_dereference(q->blk_trace);",
                "\tif (likely(!bt)) {",
                "\t\trcu_read_unlock();",
                "\t}",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tstruct blk_trace *bt = q->blk_trace;",
                "\tif (likely(!bt))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2237
    },
    {
        "cve_id": "CVE-2022-2318",
        "code_before_change": "static void rose_heartbeat_expiry(struct timer_list *t)\n{\n\tstruct sock *sk = from_timer(sk, t, sk_timer);\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tbh_lock_sock(sk);\n\tswitch (rose->state) {\n\tcase ROSE_STATE_0:\n\t\t/* Magic here: If we listen() and a new link dies before it\n\t\t   is accepted() it isn't 'dead' so doesn't get removed. */\n\t\tif (sock_flag(sk, SOCK_DESTROY) ||\n\t\t    (sk->sk_state == TCP_LISTEN && sock_flag(sk, SOCK_DEAD))) {\n\t\t\tbh_unlock_sock(sk);\n\t\t\trose_destroy_socket(sk);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase ROSE_STATE_3:\n\t\t/*\n\t\t * Check for the state of the receive buffer.\n\t\t */\n\t\tif (atomic_read(&sk->sk_rmem_alloc) < (sk->sk_rcvbuf / 2) &&\n\t\t    (rose->condition & ROSE_COND_OWN_RX_BUSY)) {\n\t\t\trose->condition &= ~ROSE_COND_OWN_RX_BUSY;\n\t\t\trose->condition &= ~ROSE_COND_ACK_PENDING;\n\t\t\trose->vl         = rose->vr;\n\t\t\trose_write_internal(sk, ROSE_RR);\n\t\t\trose_stop_timer(sk);\t/* HB */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\trose_start_heartbeat(sk);\n\tbh_unlock_sock(sk);\n}",
        "code_after_change": "static void rose_heartbeat_expiry(struct timer_list *t)\n{\n\tstruct sock *sk = from_timer(sk, t, sk_timer);\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tbh_lock_sock(sk);\n\tswitch (rose->state) {\n\tcase ROSE_STATE_0:\n\t\t/* Magic here: If we listen() and a new link dies before it\n\t\t   is accepted() it isn't 'dead' so doesn't get removed. */\n\t\tif (sock_flag(sk, SOCK_DESTROY) ||\n\t\t    (sk->sk_state == TCP_LISTEN && sock_flag(sk, SOCK_DEAD))) {\n\t\t\tbh_unlock_sock(sk);\n\t\t\trose_destroy_socket(sk);\n\t\t\tsock_put(sk);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase ROSE_STATE_3:\n\t\t/*\n\t\t * Check for the state of the receive buffer.\n\t\t */\n\t\tif (atomic_read(&sk->sk_rmem_alloc) < (sk->sk_rcvbuf / 2) &&\n\t\t    (rose->condition & ROSE_COND_OWN_RX_BUSY)) {\n\t\t\trose->condition &= ~ROSE_COND_OWN_RX_BUSY;\n\t\t\trose->condition &= ~ROSE_COND_ACK_PENDING;\n\t\t\trose->vl         = rose->vr;\n\t\t\trose_write_internal(sk, ROSE_RR);\n\t\t\trose_stop_timer(sk);\t/* HB */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\trose_start_heartbeat(sk);\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,7 @@\n \t\t    (sk->sk_state == TCP_LISTEN && sock_flag(sk, SOCK_DEAD))) {\n \t\t\tbh_unlock_sock(sk);\n \t\t\trose_destroy_socket(sk);\n+\t\t\tsock_put(sk);\n \t\t\treturn;\n \t\t}\n \t\tbreak;\n@@ -34,4 +35,5 @@\n \n \trose_start_heartbeat(sk);\n \tbh_unlock_sock(sk);\n+\tsock_put(sk);\n }",
        "function_modified_lines": {
            "added": [
                "\t\t\tsock_put(sk);",
                "\tsock_put(sk);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There are use-after-free vulnerabilities caused by timer handler in net/rose/rose_timer.c of linux that allow attackers to crash linux kernel without any privileges.",
        "id": 3432
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static void __io_worker_idle(struct io_wqe *wqe, struct io_worker *worker)\n\t__must_hold(wqe->lock)\n{\n\tif (!(worker->flags & IO_WORKER_F_FREE)) {\n\t\tworker->flags |= IO_WORKER_F_FREE;\n\t\thlist_nulls_add_head_rcu(&worker->nulls_node, &wqe->free_list);\n\t}\n}",
        "code_after_change": "static void __io_worker_idle(struct io_wqe *wqe, struct io_worker *worker)\n\t__must_hold(wqe->lock)\n{\n\tif (!(worker->flags & IO_WORKER_F_FREE)) {\n\t\tworker->flags |= IO_WORKER_F_FREE;\n\t\thlist_nulls_add_head_rcu(&worker->nulls_node, &wqe->free_list);\n\t}\n\tif (worker->saved_creds) {\n\t\trevert_creds(worker->saved_creds);\n\t\tworker->cur_creds = worker->saved_creds = NULL;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,4 +5,8 @@\n \t\tworker->flags |= IO_WORKER_F_FREE;\n \t\thlist_nulls_add_head_rcu(&worker->nulls_node, &wqe->free_list);\n \t}\n+\tif (worker->saved_creds) {\n+\t\trevert_creds(worker->saved_creds);\n+\t\tworker->cur_creds = worker->saved_creds = NULL;\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tif (worker->saved_creds) {",
                "\t\trevert_creds(worker->saved_creds);",
                "\t\tworker->cur_creds = worker->saved_creds = NULL;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3353
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static ssize_t aio_write(struct kiocb *req, const struct iocb *iocb,\n\t\t\t bool vectored, bool compat)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct iov_iter iter;\n\tstruct file *file;\n\tssize_t ret;\n\n\tret = aio_prep_rw(req, iocb);\n\tif (ret)\n\t\treturn ret;\n\tfile = req->ki_filp;\n\n\tret = -EBADF;\n\tif (unlikely(!(file->f_mode & FMODE_WRITE)))\n\t\tgoto out_fput;\n\tret = -EINVAL;\n\tif (unlikely(!file->f_op->write_iter))\n\t\tgoto out_fput;\n\n\tret = aio_setup_rw(WRITE, iocb, &iovec, vectored, compat, &iter);\n\tif (ret)\n\t\tgoto out_fput;\n\tret = rw_verify_area(WRITE, file, &req->ki_pos, iov_iter_count(&iter));\n\tif (!ret) {\n\t\t/*\n\t\t * Open-code file_start_write here to grab freeze protection,\n\t\t * which will be released by another thread in\n\t\t * aio_complete_rw().  Fool lockdep by telling it the lock got\n\t\t * released so that it doesn't complain about the held lock when\n\t\t * we return to userspace.\n\t\t */\n\t\tif (S_ISREG(file_inode(file)->i_mode)) {\n\t\t\t__sb_start_write(file_inode(file)->i_sb, SB_FREEZE_WRITE, true);\n\t\t\t__sb_writers_release(file_inode(file)->i_sb, SB_FREEZE_WRITE);\n\t\t}\n\t\treq->ki_flags |= IOCB_WRITE;\n\t\taio_rw_done(req, call_write_iter(file, req, &iter));\n\t}\n\tkfree(iovec);\nout_fput:\n\tif (unlikely(ret))\n\t\tfput(file);\n\treturn ret;\n}",
        "code_after_change": "static ssize_t aio_write(struct kiocb *req, const struct iocb *iocb,\n\t\t\t bool vectored, bool compat)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct iov_iter iter;\n\tstruct file *file;\n\tssize_t ret;\n\n\tret = aio_prep_rw(req, iocb);\n\tif (ret)\n\t\treturn ret;\n\tfile = req->ki_filp;\n\n\tif (unlikely(!(file->f_mode & FMODE_WRITE)))\n\t\treturn -EBADF;\n\tif (unlikely(!file->f_op->write_iter))\n\t\treturn -EINVAL;\n\n\tret = aio_setup_rw(WRITE, iocb, &iovec, vectored, compat, &iter);\n\tif (ret)\n\t\treturn ret;\n\tret = rw_verify_area(WRITE, file, &req->ki_pos, iov_iter_count(&iter));\n\tif (!ret) {\n\t\t/*\n\t\t * Open-code file_start_write here to grab freeze protection,\n\t\t * which will be released by another thread in\n\t\t * aio_complete_rw().  Fool lockdep by telling it the lock got\n\t\t * released so that it doesn't complain about the held lock when\n\t\t * we return to userspace.\n\t\t */\n\t\tif (S_ISREG(file_inode(file)->i_mode)) {\n\t\t\t__sb_start_write(file_inode(file)->i_sb, SB_FREEZE_WRITE, true);\n\t\t\t__sb_writers_release(file_inode(file)->i_sb, SB_FREEZE_WRITE);\n\t\t}\n\t\treq->ki_flags |= IOCB_WRITE;\n\t\taio_rw_done(req, call_write_iter(file, req, &iter));\n\t}\n\tkfree(iovec);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,16 +11,14 @@\n \t\treturn ret;\n \tfile = req->ki_filp;\n \n-\tret = -EBADF;\n \tif (unlikely(!(file->f_mode & FMODE_WRITE)))\n-\t\tgoto out_fput;\n-\tret = -EINVAL;\n+\t\treturn -EBADF;\n \tif (unlikely(!file->f_op->write_iter))\n-\t\tgoto out_fput;\n+\t\treturn -EINVAL;\n \n \tret = aio_setup_rw(WRITE, iocb, &iovec, vectored, compat, &iter);\n \tif (ret)\n-\t\tgoto out_fput;\n+\t\treturn ret;\n \tret = rw_verify_area(WRITE, file, &req->ki_pos, iov_iter_count(&iter));\n \tif (!ret) {\n \t\t/*\n@@ -38,8 +36,5 @@\n \t\taio_rw_done(req, call_write_iter(file, req, &iter));\n \t}\n \tkfree(iovec);\n-out_fput:\n-\tif (unlikely(ret))\n-\t\tfput(file);\n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t\treturn -EBADF;",
                "\t\treturn -EINVAL;",
                "\t\treturn ret;"
            ],
            "deleted": [
                "\tret = -EBADF;",
                "\t\tgoto out_fput;",
                "\tret = -EINVAL;",
                "\t\tgoto out_fput;",
                "\t\tgoto out_fput;",
                "out_fput:",
                "\tif (unlikely(ret))",
                "\t\tfput(file);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1889
    },
    {
        "cve_id": "CVE-2022-38457",
        "code_before_change": "struct ttm_base_object *ttm_base_object_lookup(struct ttm_object_file *tfile,\n\t\t\t\t\t       uint64_t key)\n{\n\tstruct ttm_base_object *base = NULL;\n\tstruct vmwgfx_hash_item *hash;\n\tint ret;\n\n\trcu_read_lock();\n\tret = ttm_tfile_find_ref_rcu(tfile, key, &hash);\n\n\tif (likely(ret == 0)) {\n\t\tbase = hlist_entry(hash, struct ttm_ref_object, hash)->obj;\n\t\tif (!kref_get_unless_zero(&base->refcount))\n\t\t\tbase = NULL;\n\t}\n\trcu_read_unlock();\n\n\treturn base;\n}",
        "code_after_change": "struct ttm_base_object *ttm_base_object_lookup(struct ttm_object_file *tfile,\n\t\t\t\t\t       uint64_t key)\n{\n\tstruct ttm_base_object *base = NULL;\n\tstruct vmwgfx_hash_item *hash;\n\tint ret;\n\n\tspin_lock(&tfile->lock);\n\tret = ttm_tfile_find_ref(tfile, key, &hash);\n\n\tif (likely(ret == 0)) {\n\t\tbase = hlist_entry(hash, struct ttm_ref_object, hash)->obj;\n\t\tif (!kref_get_unless_zero(&base->refcount))\n\t\t\tbase = NULL;\n\t}\n\tspin_unlock(&tfile->lock);\n\n\n\treturn base;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,15 +5,16 @@\n \tstruct vmwgfx_hash_item *hash;\n \tint ret;\n \n-\trcu_read_lock();\n-\tret = ttm_tfile_find_ref_rcu(tfile, key, &hash);\n+\tspin_lock(&tfile->lock);\n+\tret = ttm_tfile_find_ref(tfile, key, &hash);\n \n \tif (likely(ret == 0)) {\n \t\tbase = hlist_entry(hash, struct ttm_ref_object, hash)->obj;\n \t\tif (!kref_get_unless_zero(&base->refcount))\n \t\t\tbase = NULL;\n \t}\n-\trcu_read_unlock();\n+\tspin_unlock(&tfile->lock);\n+\n \n \treturn base;\n }",
        "function_modified_lines": {
            "added": [
                "\tspin_lock(&tfile->lock);",
                "\tret = ttm_tfile_find_ref(tfile, key, &hash);",
                "\tspin_unlock(&tfile->lock);",
                ""
            ],
            "deleted": [
                "\trcu_read_lock();",
                "\tret = ttm_tfile_find_ref_rcu(tfile, key, &hash);",
                "\trcu_read_unlock();"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free(UAF) vulnerability was found in function 'vmw_cmd_res_check' in drivers/gpu/vmxgfx/vmxgfx_execbuf.c in Linux kernel's vmwgfx driver with device file '/dev/dri/renderD128 (or Dxxx)'. This flaw allows a local attacker with a user account on the system to gain privilege, causing a denial of service(DoS).",
        "id": 3678
    },
    {
        "cve_id": "CVE-2019-11811",
        "code_before_change": "int ipmi_si_port_setup(struct si_sm_io *io)\n{\n\tunsigned int addr = io->addr_data;\n\tint          idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\tio->io_cleanup = port_cleanup;\n\n\t/*\n\t * Figure out the actual inb/inw/inl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = port_inb;\n\t\tio->outputb = port_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = port_inw;\n\t\tio->outputb = port_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = port_inl;\n\t\tio->outputb = port_outl;\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint I/O regions in their ACPI\n\t * tables.  This causes problems when trying to register the\n\t * entire I/O region.  Therefore we must register each I/O\n\t * port separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_region(addr + idx * io->regspacing,\n\t\t\t\t   io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\twhile (idx--)\n\t\t\t\trelease_region(addr + idx * io->regspacing,\n\t\t\t\t\t       io->regsize);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\treturn 0;\n}",
        "code_after_change": "int ipmi_si_port_setup(struct si_sm_io *io)\n{\n\tunsigned int addr = io->addr_data;\n\tint          idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\t/*\n\t * Figure out the actual inb/inw/inl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = port_inb;\n\t\tio->outputb = port_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = port_inw;\n\t\tio->outputb = port_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = port_inl;\n\t\tio->outputb = port_outl;\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint I/O regions in their ACPI\n\t * tables.  This causes problems when trying to register the\n\t * entire I/O region.  Therefore we must register each I/O\n\t * port separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_region(addr + idx * io->regspacing,\n\t\t\t\t   io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\twhile (idx--)\n\t\t\t\trelease_region(addr + idx * io->regspacing,\n\t\t\t\t\t       io->regsize);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\tio->io_cleanup = port_cleanup;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,6 @@\n \n \tif (!addr)\n \t\treturn -ENODEV;\n-\n-\tio->io_cleanup = port_cleanup;\n \n \t/*\n \t * Figure out the actual inb/inw/inl/etc routine to use based\n@@ -47,5 +45,8 @@\n \t\t\treturn -EIO;\n \t\t}\n \t}\n+\n+\tio->io_cleanup = port_cleanup;\n+\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tio->io_cleanup = port_cleanup;",
                ""
            ],
            "deleted": [
                "",
                "\tio->io_cleanup = port_cleanup;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.4. There is a use-after-free upon attempted read access to /proc/ioports after the ipmi_si module is removed, related to drivers/char/ipmi/ipmi_si_intf.c, drivers/char/ipmi/ipmi_si_mem_io.c, and drivers/char/ipmi/ipmi_si_port_io.c.",
        "id": 1934
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int do_add_counters(struct net *net, sockptr_t arg, unsigned int len)\n{\n\tunsigned int i;\n\tstruct xt_counters_info tmp;\n\tstruct xt_counters *paddc;\n\tstruct xt_table *t;\n\tconst struct xt_table_info *private;\n\tint ret = 0;\n\tstruct arpt_entry *iter;\n\tunsigned int addend;\n\n\tpaddc = xt_copy_counters(arg, len, &tmp);\n\tif (IS_ERR(paddc))\n\t\treturn PTR_ERR(paddc);\n\n\tt = xt_find_table_lock(net, NFPROTO_ARP, tmp.name);\n\tif (IS_ERR(t)) {\n\t\tret = PTR_ERR(t);\n\t\tgoto free;\n\t}\n\n\tlocal_bh_disable();\n\tprivate = t->private;\n\tif (private->number != tmp.num_counters) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_up_free;\n\t}\n\n\ti = 0;\n\n\taddend = xt_write_recseq_begin();\n\txt_entry_foreach(iter,  private->entries, private->size) {\n\t\tstruct xt_counters *tmp;\n\n\t\ttmp = xt_get_this_cpu_counter(&iter->counters);\n\t\tADD_COUNTER(*tmp, paddc[i].bcnt, paddc[i].pcnt);\n\t\t++i;\n\t}\n\txt_write_recseq_end(addend);\n unlock_up_free:\n\tlocal_bh_enable();\n\txt_table_unlock(t);\n\tmodule_put(t->me);\n free:\n\tvfree(paddc);\n\n\treturn ret;\n}",
        "code_after_change": "static int do_add_counters(struct net *net, sockptr_t arg, unsigned int len)\n{\n\tunsigned int i;\n\tstruct xt_counters_info tmp;\n\tstruct xt_counters *paddc;\n\tstruct xt_table *t;\n\tconst struct xt_table_info *private;\n\tint ret = 0;\n\tstruct arpt_entry *iter;\n\tunsigned int addend;\n\n\tpaddc = xt_copy_counters(arg, len, &tmp);\n\tif (IS_ERR(paddc))\n\t\treturn PTR_ERR(paddc);\n\n\tt = xt_find_table_lock(net, NFPROTO_ARP, tmp.name);\n\tif (IS_ERR(t)) {\n\t\tret = PTR_ERR(t);\n\t\tgoto free;\n\t}\n\n\tlocal_bh_disable();\n\tprivate = xt_table_get_private_protected(t);\n\tif (private->number != tmp.num_counters) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_up_free;\n\t}\n\n\ti = 0;\n\n\taddend = xt_write_recseq_begin();\n\txt_entry_foreach(iter,  private->entries, private->size) {\n\t\tstruct xt_counters *tmp;\n\n\t\ttmp = xt_get_this_cpu_counter(&iter->counters);\n\t\tADD_COUNTER(*tmp, paddc[i].bcnt, paddc[i].pcnt);\n\t\t++i;\n\t}\n\txt_write_recseq_end(addend);\n unlock_up_free:\n\tlocal_bh_enable();\n\txt_table_unlock(t);\n\tmodule_put(t->me);\n free:\n\tvfree(paddc);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,7 +20,7 @@\n \t}\n \n \tlocal_bh_disable();\n-\tprivate = t->private;\n+\tprivate = xt_table_get_private_protected(t);\n \tif (private->number != tmp.num_counters) {\n \t\tret = -EINVAL;\n \t\tgoto unlock_up_free;",
        "function_modified_lines": {
            "added": [
                "\tprivate = xt_table_get_private_protected(t);"
            ],
            "deleted": [
                "\tprivate = t->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2774
    },
    {
        "cve_id": "CVE-2021-37159",
        "code_before_change": "static struct hso_device *hso_create_net_device(struct usb_interface *interface,\n\t\t\t\t\t\tint port_spec)\n{\n\tint result, i;\n\tstruct net_device *net;\n\tstruct hso_net *hso_net;\n\tstruct hso_device *hso_dev;\n\n\thso_dev = hso_create_device(interface, port_spec);\n\tif (!hso_dev)\n\t\treturn NULL;\n\n\t/* allocate our network device, then we can put in our private data */\n\t/* call hso_net_init to do the basic initialization */\n\tnet = alloc_netdev(sizeof(struct hso_net), \"hso%d\", NET_NAME_UNKNOWN,\n\t\t\t   hso_net_init);\n\tif (!net) {\n\t\tdev_err(&interface->dev, \"Unable to create ethernet device\\n\");\n\t\tgoto exit;\n\t}\n\n\thso_net = netdev_priv(net);\n\n\thso_dev->port_data.dev_net = hso_net;\n\thso_net->net = net;\n\thso_net->parent = hso_dev;\n\n\thso_net->in_endp = hso_get_ep(interface, USB_ENDPOINT_XFER_BULK,\n\t\t\t\t      USB_DIR_IN);\n\tif (!hso_net->in_endp) {\n\t\tdev_err(&interface->dev, \"Can't find BULK IN endpoint\\n\");\n\t\tgoto exit;\n\t}\n\thso_net->out_endp = hso_get_ep(interface, USB_ENDPOINT_XFER_BULK,\n\t\t\t\t       USB_DIR_OUT);\n\tif (!hso_net->out_endp) {\n\t\tdev_err(&interface->dev, \"Can't find BULK OUT endpoint\\n\");\n\t\tgoto exit;\n\t}\n\tSET_NETDEV_DEV(net, &interface->dev);\n\tSET_NETDEV_DEVTYPE(net, &hso_type);\n\n\t/* start allocating */\n\tfor (i = 0; i < MUX_BULK_RX_BUF_COUNT; i++) {\n\t\thso_net->mux_bulk_rx_urb_pool[i] = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (!hso_net->mux_bulk_rx_urb_pool[i])\n\t\t\tgoto exit;\n\t\thso_net->mux_bulk_rx_buf_pool[i] = kzalloc(MUX_BULK_RX_BUF_SIZE,\n\t\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!hso_net->mux_bulk_rx_buf_pool[i])\n\t\t\tgoto exit;\n\t}\n\thso_net->mux_bulk_tx_urb = usb_alloc_urb(0, GFP_KERNEL);\n\tif (!hso_net->mux_bulk_tx_urb)\n\t\tgoto exit;\n\thso_net->mux_bulk_tx_buf = kzalloc(MUX_BULK_TX_BUF_SIZE, GFP_KERNEL);\n\tif (!hso_net->mux_bulk_tx_buf)\n\t\tgoto exit;\n\n\tadd_net_device(hso_dev);\n\n\t/* registering our net device */\n\tresult = register_netdev(net);\n\tif (result) {\n\t\tdev_err(&interface->dev, \"Failed to register device\\n\");\n\t\tgoto exit;\n\t}\n\n\thso_log_port(hso_dev);\n\n\thso_create_rfkill(hso_dev, interface);\n\n\treturn hso_dev;\nexit:\n\thso_free_net_device(hso_dev, true);\n\treturn NULL;\n}",
        "code_after_change": "static struct hso_device *hso_create_net_device(struct usb_interface *interface,\n\t\t\t\t\t\tint port_spec)\n{\n\tint result, i;\n\tstruct net_device *net;\n\tstruct hso_net *hso_net;\n\tstruct hso_device *hso_dev;\n\n\thso_dev = hso_create_device(interface, port_spec);\n\tif (!hso_dev)\n\t\treturn NULL;\n\n\t/* allocate our network device, then we can put in our private data */\n\t/* call hso_net_init to do the basic initialization */\n\tnet = alloc_netdev(sizeof(struct hso_net), \"hso%d\", NET_NAME_UNKNOWN,\n\t\t\t   hso_net_init);\n\tif (!net) {\n\t\tdev_err(&interface->dev, \"Unable to create ethernet device\\n\");\n\t\tgoto err_hso_dev;\n\t}\n\n\thso_net = netdev_priv(net);\n\n\thso_dev->port_data.dev_net = hso_net;\n\thso_net->net = net;\n\thso_net->parent = hso_dev;\n\n\thso_net->in_endp = hso_get_ep(interface, USB_ENDPOINT_XFER_BULK,\n\t\t\t\t      USB_DIR_IN);\n\tif (!hso_net->in_endp) {\n\t\tdev_err(&interface->dev, \"Can't find BULK IN endpoint\\n\");\n\t\tgoto err_net;\n\t}\n\thso_net->out_endp = hso_get_ep(interface, USB_ENDPOINT_XFER_BULK,\n\t\t\t\t       USB_DIR_OUT);\n\tif (!hso_net->out_endp) {\n\t\tdev_err(&interface->dev, \"Can't find BULK OUT endpoint\\n\");\n\t\tgoto err_net;\n\t}\n\tSET_NETDEV_DEV(net, &interface->dev);\n\tSET_NETDEV_DEVTYPE(net, &hso_type);\n\n\t/* start allocating */\n\tfor (i = 0; i < MUX_BULK_RX_BUF_COUNT; i++) {\n\t\thso_net->mux_bulk_rx_urb_pool[i] = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (!hso_net->mux_bulk_rx_urb_pool[i])\n\t\t\tgoto err_mux_bulk_rx;\n\t\thso_net->mux_bulk_rx_buf_pool[i] = kzalloc(MUX_BULK_RX_BUF_SIZE,\n\t\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!hso_net->mux_bulk_rx_buf_pool[i])\n\t\t\tgoto err_mux_bulk_rx;\n\t}\n\thso_net->mux_bulk_tx_urb = usb_alloc_urb(0, GFP_KERNEL);\n\tif (!hso_net->mux_bulk_tx_urb)\n\t\tgoto err_mux_bulk_rx;\n\thso_net->mux_bulk_tx_buf = kzalloc(MUX_BULK_TX_BUF_SIZE, GFP_KERNEL);\n\tif (!hso_net->mux_bulk_tx_buf)\n\t\tgoto err_free_tx_urb;\n\n\tadd_net_device(hso_dev);\n\n\t/* registering our net device */\n\tresult = register_netdev(net);\n\tif (result) {\n\t\tdev_err(&interface->dev, \"Failed to register device\\n\");\n\t\tgoto err_free_tx_buf;\n\t}\n\n\thso_log_port(hso_dev);\n\n\thso_create_rfkill(hso_dev, interface);\n\n\treturn hso_dev;\n\nerr_free_tx_buf:\n\tremove_net_device(hso_dev);\n\tkfree(hso_net->mux_bulk_tx_buf);\nerr_free_tx_urb:\n\tusb_free_urb(hso_net->mux_bulk_tx_urb);\nerr_mux_bulk_rx:\n\tfor (i = 0; i < MUX_BULK_RX_BUF_COUNT; i++) {\n\t\tusb_free_urb(hso_net->mux_bulk_rx_urb_pool[i]);\n\t\tkfree(hso_net->mux_bulk_rx_buf_pool[i]);\n\t}\nerr_net:\n\tfree_netdev(net);\nerr_hso_dev:\n\tkfree(hso_dev);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,7 +16,7 @@\n \t\t\t   hso_net_init);\n \tif (!net) {\n \t\tdev_err(&interface->dev, \"Unable to create ethernet device\\n\");\n-\t\tgoto exit;\n+\t\tgoto err_hso_dev;\n \t}\n \n \thso_net = netdev_priv(net);\n@@ -29,13 +29,13 @@\n \t\t\t\t      USB_DIR_IN);\n \tif (!hso_net->in_endp) {\n \t\tdev_err(&interface->dev, \"Can't find BULK IN endpoint\\n\");\n-\t\tgoto exit;\n+\t\tgoto err_net;\n \t}\n \thso_net->out_endp = hso_get_ep(interface, USB_ENDPOINT_XFER_BULK,\n \t\t\t\t       USB_DIR_OUT);\n \tif (!hso_net->out_endp) {\n \t\tdev_err(&interface->dev, \"Can't find BULK OUT endpoint\\n\");\n-\t\tgoto exit;\n+\t\tgoto err_net;\n \t}\n \tSET_NETDEV_DEV(net, &interface->dev);\n \tSET_NETDEV_DEVTYPE(net, &hso_type);\n@@ -44,18 +44,18 @@\n \tfor (i = 0; i < MUX_BULK_RX_BUF_COUNT; i++) {\n \t\thso_net->mux_bulk_rx_urb_pool[i] = usb_alloc_urb(0, GFP_KERNEL);\n \t\tif (!hso_net->mux_bulk_rx_urb_pool[i])\n-\t\t\tgoto exit;\n+\t\t\tgoto err_mux_bulk_rx;\n \t\thso_net->mux_bulk_rx_buf_pool[i] = kzalloc(MUX_BULK_RX_BUF_SIZE,\n \t\t\t\t\t\t\t   GFP_KERNEL);\n \t\tif (!hso_net->mux_bulk_rx_buf_pool[i])\n-\t\t\tgoto exit;\n+\t\t\tgoto err_mux_bulk_rx;\n \t}\n \thso_net->mux_bulk_tx_urb = usb_alloc_urb(0, GFP_KERNEL);\n \tif (!hso_net->mux_bulk_tx_urb)\n-\t\tgoto exit;\n+\t\tgoto err_mux_bulk_rx;\n \thso_net->mux_bulk_tx_buf = kzalloc(MUX_BULK_TX_BUF_SIZE, GFP_KERNEL);\n \tif (!hso_net->mux_bulk_tx_buf)\n-\t\tgoto exit;\n+\t\tgoto err_free_tx_urb;\n \n \tadd_net_device(hso_dev);\n \n@@ -63,7 +63,7 @@\n \tresult = register_netdev(net);\n \tif (result) {\n \t\tdev_err(&interface->dev, \"Failed to register device\\n\");\n-\t\tgoto exit;\n+\t\tgoto err_free_tx_buf;\n \t}\n \n \thso_log_port(hso_dev);\n@@ -71,7 +71,20 @@\n \thso_create_rfkill(hso_dev, interface);\n \n \treturn hso_dev;\n-exit:\n-\thso_free_net_device(hso_dev, true);\n+\n+err_free_tx_buf:\n+\tremove_net_device(hso_dev);\n+\tkfree(hso_net->mux_bulk_tx_buf);\n+err_free_tx_urb:\n+\tusb_free_urb(hso_net->mux_bulk_tx_urb);\n+err_mux_bulk_rx:\n+\tfor (i = 0; i < MUX_BULK_RX_BUF_COUNT; i++) {\n+\t\tusb_free_urb(hso_net->mux_bulk_rx_urb_pool[i]);\n+\t\tkfree(hso_net->mux_bulk_rx_buf_pool[i]);\n+\t}\n+err_net:\n+\tfree_netdev(net);\n+err_hso_dev:\n+\tkfree(hso_dev);\n \treturn NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tgoto err_hso_dev;",
                "\t\tgoto err_net;",
                "\t\tgoto err_net;",
                "\t\t\tgoto err_mux_bulk_rx;",
                "\t\t\tgoto err_mux_bulk_rx;",
                "\t\tgoto err_mux_bulk_rx;",
                "\t\tgoto err_free_tx_urb;",
                "\t\tgoto err_free_tx_buf;",
                "",
                "err_free_tx_buf:",
                "\tremove_net_device(hso_dev);",
                "\tkfree(hso_net->mux_bulk_tx_buf);",
                "err_free_tx_urb:",
                "\tusb_free_urb(hso_net->mux_bulk_tx_urb);",
                "err_mux_bulk_rx:",
                "\tfor (i = 0; i < MUX_BULK_RX_BUF_COUNT; i++) {",
                "\t\tusb_free_urb(hso_net->mux_bulk_rx_urb_pool[i]);",
                "\t\tkfree(hso_net->mux_bulk_rx_buf_pool[i]);",
                "\t}",
                "err_net:",
                "\tfree_netdev(net);",
                "err_hso_dev:",
                "\tkfree(hso_dev);"
            ],
            "deleted": [
                "\t\tgoto exit;",
                "\t\tgoto exit;",
                "\t\tgoto exit;",
                "\t\t\tgoto exit;",
                "\t\t\tgoto exit;",
                "\t\tgoto exit;",
                "\t\tgoto exit;",
                "\t\tgoto exit;",
                "exit:",
                "\thso_free_net_device(hso_dev, true);"
            ]
        },
        "cwe": [
            "CWE-415",
            "CWE-416"
        ],
        "cve_description": "hso_free_net_device in drivers/net/usb/hso.c in the Linux kernel through 5.13.4 calls unregister_netdev without checking for the NETREG_REGISTERED state, leading to a use-after-free and a double free.",
        "id": 3043
    },
    {
        "cve_id": "CVE-2023-0030",
        "code_before_change": "int\nnvkm_vmm_get_locked(struct nvkm_vmm *vmm, bool getref, bool mapref, bool sparse,\n\t\t    u8 shift, u8 align, u64 size, struct nvkm_vma **pvma)\n{\n\tconst struct nvkm_vmm_page *page = &vmm->func->page[NVKM_VMA_PAGE_NONE];\n\tstruct rb_node *node = NULL, *temp;\n\tstruct nvkm_vma *vma = NULL, *tmp;\n\tu64 addr, tail;\n\tint ret;\n\n\tVMM_TRACE(vmm, \"getref %d mapref %d sparse %d \"\n\t\t       \"shift: %d align: %d size: %016llx\",\n\t\t  getref, mapref, sparse, shift, align, size);\n\n\t/* Zero-sized, or lazily-allocated sparse VMAs, make no sense. */\n\tif (unlikely(!size || (!getref && !mapref && sparse))) {\n\t\tVMM_DEBUG(vmm, \"args %016llx %d %d %d\",\n\t\t\t  size, getref, mapref, sparse);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Tesla-class GPUs can only select page size per-PDE, which means\n\t * we're required to know the mapping granularity up-front to find\n\t * a suitable region of address-space.\n\t *\n\t * The same goes if we're requesting up-front allocation of PTES.\n\t */\n\tif (unlikely((getref || vmm->func->page_block) && !shift)) {\n\t\tVMM_DEBUG(vmm, \"page size required: %d %016llx\",\n\t\t\t  getref, vmm->func->page_block);\n\t\treturn -EINVAL;\n\t}\n\n\t/* If a specific page size was requested, determine its index and\n\t * make sure the requested size is a multiple of the page size.\n\t */\n\tif (shift) {\n\t\tfor (page = vmm->func->page; page->shift; page++) {\n\t\t\tif (shift == page->shift)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!page->shift || !IS_ALIGNED(size, 1ULL << page->shift)) {\n\t\t\tVMM_DEBUG(vmm, \"page %d %016llx\", shift, size);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\talign = max_t(u8, align, shift);\n\t} else {\n\t\talign = max_t(u8, align, 12);\n\t}\n\n\t/* Locate smallest block that can possibly satisfy the allocation. */\n\ttemp = vmm->free.rb_node;\n\twhile (temp) {\n\t\tstruct nvkm_vma *this = rb_entry(temp, typeof(*this), tree);\n\t\tif (this->size < size) {\n\t\t\ttemp = temp->rb_right;\n\t\t} else {\n\t\t\tnode = temp;\n\t\t\ttemp = temp->rb_left;\n\t\t}\n\t}\n\n\tif (unlikely(!node))\n\t\treturn -ENOSPC;\n\n\t/* Take into account alignment restrictions, trying larger blocks\n\t * in turn until we find a suitable free block.\n\t */\n\tdo {\n\t\tstruct nvkm_vma *this = rb_entry(node, typeof(*this), tree);\n\t\tstruct nvkm_vma *prev = node(this, prev);\n\t\tstruct nvkm_vma *next = node(this, next);\n\t\tconst int p = page - vmm->func->page;\n\n\t\taddr = this->addr;\n\t\tif (vmm->func->page_block && prev && prev->page != p)\n\t\t\taddr = ALIGN(addr, vmm->func->page_block);\n\t\taddr = ALIGN(addr, 1ULL << align);\n\n\t\ttail = this->addr + this->size;\n\t\tif (vmm->func->page_block && next && next->page != p)\n\t\t\ttail = ALIGN_DOWN(tail, vmm->func->page_block);\n\n\t\tif (addr <= tail && tail - addr >= size) {\n\t\t\trb_erase(&this->tree, &vmm->free);\n\t\t\tvma = this;\n\t\t\tbreak;\n\t\t}\n\t} while ((node = rb_next(node)));\n\n\tif (unlikely(!vma))\n\t\treturn -ENOSPC;\n\n\t/* If the VMA we found isn't already exactly the requested size,\n\t * it needs to be split, and the remaining free blocks returned.\n\t */\n\tif (addr != vma->addr) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size + vma->addr - addr))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, vma);\n\t\tvma = tmp;\n\t}\n\n\tif (size != vma->size) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size - size))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, tmp);\n\t}\n\n\t/* Pre-allocate page tables and/or setup sparse mappings. */\n\tif (sparse && getref)\n\t\tret = nvkm_vmm_ptes_sparse_get(vmm, page, vma->addr, vma->size);\n\telse if (sparse)\n\t\tret = nvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, true);\n\telse if (getref)\n\t\tret = nvkm_vmm_ptes_get(vmm, page, vma->addr, vma->size);\n\telse\n\t\tret = 0;\n\tif (ret) {\n\t\tnvkm_vmm_put_region(vmm, vma);\n\t\treturn ret;\n\t}\n\n\tvma->mapref = mapref && !getref;\n\tvma->sparse = sparse;\n\tvma->page = page - vmm->func->page;\n\tvma->refd = getref ? vma->page : NVKM_VMA_PAGE_NONE;\n\tvma->used = true;\n\tnvkm_vmm_node_insert(vmm, vma);\n\t*pvma = vma;\n\treturn 0;\n}",
        "code_after_change": "int\nnvkm_vmm_get_locked(struct nvkm_vmm *vmm, bool getref, bool mapref, bool sparse,\n\t\t    u8 shift, u8 align, u64 size, struct nvkm_vma **pvma)\n{\n\tconst struct nvkm_vmm_page *page = &vmm->func->page[NVKM_VMA_PAGE_NONE];\n\tstruct rb_node *node = NULL, *temp;\n\tstruct nvkm_vma *vma = NULL, *tmp;\n\tu64 addr, tail;\n\tint ret;\n\n\tVMM_TRACE(vmm, \"getref %d mapref %d sparse %d \"\n\t\t       \"shift: %d align: %d size: %016llx\",\n\t\t  getref, mapref, sparse, shift, align, size);\n\n\t/* Zero-sized, or lazily-allocated sparse VMAs, make no sense. */\n\tif (unlikely(!size || (!getref && !mapref && sparse))) {\n\t\tVMM_DEBUG(vmm, \"args %016llx %d %d %d\",\n\t\t\t  size, getref, mapref, sparse);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Tesla-class GPUs can only select page size per-PDE, which means\n\t * we're required to know the mapping granularity up-front to find\n\t * a suitable region of address-space.\n\t *\n\t * The same goes if we're requesting up-front allocation of PTES.\n\t */\n\tif (unlikely((getref || vmm->func->page_block) && !shift)) {\n\t\tVMM_DEBUG(vmm, \"page size required: %d %016llx\",\n\t\t\t  getref, vmm->func->page_block);\n\t\treturn -EINVAL;\n\t}\n\n\t/* If a specific page size was requested, determine its index and\n\t * make sure the requested size is a multiple of the page size.\n\t */\n\tif (shift) {\n\t\tfor (page = vmm->func->page; page->shift; page++) {\n\t\t\tif (shift == page->shift)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!page->shift || !IS_ALIGNED(size, 1ULL << page->shift)) {\n\t\t\tVMM_DEBUG(vmm, \"page %d %016llx\", shift, size);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\talign = max_t(u8, align, shift);\n\t} else {\n\t\talign = max_t(u8, align, 12);\n\t}\n\n\t/* Locate smallest block that can possibly satisfy the allocation. */\n\ttemp = vmm->free.rb_node;\n\twhile (temp) {\n\t\tstruct nvkm_vma *this = rb_entry(temp, typeof(*this), tree);\n\t\tif (this->size < size) {\n\t\t\ttemp = temp->rb_right;\n\t\t} else {\n\t\t\tnode = temp;\n\t\t\ttemp = temp->rb_left;\n\t\t}\n\t}\n\n\tif (unlikely(!node))\n\t\treturn -ENOSPC;\n\n\t/* Take into account alignment restrictions, trying larger blocks\n\t * in turn until we find a suitable free block.\n\t */\n\tdo {\n\t\tstruct nvkm_vma *this = rb_entry(node, typeof(*this), tree);\n\t\tstruct nvkm_vma *prev = node(this, prev);\n\t\tstruct nvkm_vma *next = node(this, next);\n\t\tconst int p = page - vmm->func->page;\n\n\t\taddr = this->addr;\n\t\tif (vmm->func->page_block && prev && prev->page != p)\n\t\t\taddr = ALIGN(addr, vmm->func->page_block);\n\t\taddr = ALIGN(addr, 1ULL << align);\n\n\t\ttail = this->addr + this->size;\n\t\tif (vmm->func->page_block && next && next->page != p)\n\t\t\ttail = ALIGN_DOWN(tail, vmm->func->page_block);\n\n\t\tif (addr <= tail && tail - addr >= size) {\n\t\t\tnvkm_vmm_free_remove(vmm, this);\n\t\t\tvma = this;\n\t\t\tbreak;\n\t\t}\n\t} while ((node = rb_next(node)));\n\n\tif (unlikely(!vma))\n\t\treturn -ENOSPC;\n\n\t/* If the VMA we found isn't already exactly the requested size,\n\t * it needs to be split, and the remaining free blocks returned.\n\t */\n\tif (addr != vma->addr) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size + vma->addr - addr))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, vma);\n\t\tvma = tmp;\n\t}\n\n\tif (size != vma->size) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size - size))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, tmp);\n\t}\n\n\t/* Pre-allocate page tables and/or setup sparse mappings. */\n\tif (sparse && getref)\n\t\tret = nvkm_vmm_ptes_sparse_get(vmm, page, vma->addr, vma->size);\n\telse if (sparse)\n\t\tret = nvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, true);\n\telse if (getref)\n\t\tret = nvkm_vmm_ptes_get(vmm, page, vma->addr, vma->size);\n\telse\n\t\tret = 0;\n\tif (ret) {\n\t\tnvkm_vmm_put_region(vmm, vma);\n\t\treturn ret;\n\t}\n\n\tvma->mapref = mapref && !getref;\n\tvma->sparse = sparse;\n\tvma->page = page - vmm->func->page;\n\tvma->refd = getref ? vma->page : NVKM_VMA_PAGE_NONE;\n\tvma->used = true;\n\tnvkm_vmm_node_insert(vmm, vma);\n\t*pvma = vma;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -83,7 +83,7 @@\n \t\t\ttail = ALIGN_DOWN(tail, vmm->func->page_block);\n \n \t\tif (addr <= tail && tail - addr >= size) {\n-\t\t\trb_erase(&this->tree, &vmm->free);\n+\t\t\tnvkm_vmm_free_remove(vmm, this);\n \t\t\tvma = this;\n \t\t\tbreak;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\tnvkm_vmm_free_remove(vmm, this);"
            ],
            "deleted": [
                "\t\t\trb_erase(&this->tree, &vmm->free);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s nouveau driver in how a user triggers a memory overflow that causes the nvkm_vma_tail function to fail. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3804
    },
    {
        "cve_id": "CVE-2019-15239",
        "code_before_change": "static inline void tcp_write_queue_purge(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\ttcp_chrono_stop(sk, TCP_CHRONO_BUSY);\n\twhile ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL)\n\t\tsk_wmem_free_skb(sk, skb);\n\tsk_mem_reclaim(sk);\n\ttcp_clear_all_retrans_hints(tcp_sk(sk));\n}",
        "code_after_change": "static inline void tcp_write_queue_purge(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\ttcp_chrono_stop(sk, TCP_CHRONO_BUSY);\n\twhile ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL)\n\t\tsk_wmem_free_skb(sk, skb);\n\tsk_mem_reclaim(sk);\n\ttcp_clear_all_retrans_hints(tcp_sk(sk));\n\ttcp_init_send_head(sk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,4 +7,5 @@\n \t\tsk_wmem_free_skb(sk, skb);\n \tsk_mem_reclaim(sk);\n \ttcp_clear_all_retrans_hints(tcp_sk(sk));\n+\ttcp_init_send_head(sk);\n }",
        "function_modified_lines": {
            "added": [
                "\ttcp_init_send_head(sk);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel, a certain net/ipv4/tcp_output.c change, which was properly incorporated into 4.16.12, was incorrectly backported to the earlier longterm kernels, introducing a new vulnerability that was potentially more severe than the issue that was intended to be fixed by backporting. Specifically, by adding to a write queue between disconnection and re-connection, a local attacker can trigger multiple use-after-free conditions. This can result in a kernel crash, or potentially in privilege escalation. NOTE: this affects (for example) Linux distributions that use 4.9.x longterm kernels before 4.9.190 or 4.14.x longterm kernels before 4.14.139.",
        "id": 2013
    },
    {
        "cve_id": "CVE-2022-3424",
        "code_before_change": "vm_fault_t gru_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct gru_thread_state *gts;\n\tunsigned long paddr, vaddr;\n\tunsigned long expires;\n\n\tvaddr = vmf->address;\n\tgru_dbg(grudev, \"vma %p, vaddr 0x%lx (0x%lx)\\n\",\n\t\tvma, vaddr, GSEG_BASE(vaddr));\n\tSTAT(nopfn);\n\n\t/* The following check ensures vaddr is a valid address in the VMA */\n\tgts = gru_find_thread_state(vma, TSID(vaddr, vma));\n\tif (!gts)\n\t\treturn VM_FAULT_SIGBUS;\n\nagain:\n\tmutex_lock(&gts->ts_ctxlock);\n\tpreempt_disable();\n\n\tgru_check_context_placement(gts);\n\n\tif (!gts->ts_gru) {\n\t\tSTAT(load_user_context);\n\t\tif (!gru_assign_gru_context(gts)) {\n\t\t\tpreempt_enable();\n\t\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\tschedule_timeout(GRU_ASSIGN_DELAY);  /* true hack ZZZ */\n\t\t\texpires = gts->ts_steal_jiffies + GRU_STEAL_DELAY;\n\t\t\tif (time_before(expires, jiffies))\n\t\t\t\tgru_steal_context(gts);\n\t\t\tgoto again;\n\t\t}\n\t\tgru_load_context(gts);\n\t\tpaddr = gseg_physical_address(gts->ts_gru, gts->ts_ctxnum);\n\t\tremap_pfn_range(vma, vaddr & ~(GRU_GSEG_PAGESIZE - 1),\n\t\t\t\tpaddr >> PAGE_SHIFT, GRU_GSEG_PAGESIZE,\n\t\t\t\tvma->vm_page_prot);\n\t}\n\n\tpreempt_enable();\n\tmutex_unlock(&gts->ts_ctxlock);\n\n\treturn VM_FAULT_NOPAGE;\n}",
        "code_after_change": "vm_fault_t gru_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct gru_thread_state *gts;\n\tunsigned long paddr, vaddr;\n\tunsigned long expires;\n\n\tvaddr = vmf->address;\n\tgru_dbg(grudev, \"vma %p, vaddr 0x%lx (0x%lx)\\n\",\n\t\tvma, vaddr, GSEG_BASE(vaddr));\n\tSTAT(nopfn);\n\n\t/* The following check ensures vaddr is a valid address in the VMA */\n\tgts = gru_find_thread_state(vma, TSID(vaddr, vma));\n\tif (!gts)\n\t\treturn VM_FAULT_SIGBUS;\n\nagain:\n\tmutex_lock(&gts->ts_ctxlock);\n\tpreempt_disable();\n\n\tif (gru_check_context_placement(gts)) {\n\t\tpreempt_enable();\n\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\tgru_unload_context(gts, 1);\n\t\treturn VM_FAULT_NOPAGE;\n\t}\n\n\tif (!gts->ts_gru) {\n\t\tSTAT(load_user_context);\n\t\tif (!gru_assign_gru_context(gts)) {\n\t\t\tpreempt_enable();\n\t\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\tschedule_timeout(GRU_ASSIGN_DELAY);  /* true hack ZZZ */\n\t\t\texpires = gts->ts_steal_jiffies + GRU_STEAL_DELAY;\n\t\t\tif (time_before(expires, jiffies))\n\t\t\t\tgru_steal_context(gts);\n\t\t\tgoto again;\n\t\t}\n\t\tgru_load_context(gts);\n\t\tpaddr = gseg_physical_address(gts->ts_gru, gts->ts_ctxnum);\n\t\tremap_pfn_range(vma, vaddr & ~(GRU_GSEG_PAGESIZE - 1),\n\t\t\t\tpaddr >> PAGE_SHIFT, GRU_GSEG_PAGESIZE,\n\t\t\t\tvma->vm_page_prot);\n\t}\n\n\tpreempt_enable();\n\tmutex_unlock(&gts->ts_ctxlock);\n\n\treturn VM_FAULT_NOPAGE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,12 @@\n \tmutex_lock(&gts->ts_ctxlock);\n \tpreempt_disable();\n \n-\tgru_check_context_placement(gts);\n+\tif (gru_check_context_placement(gts)) {\n+\t\tpreempt_enable();\n+\t\tmutex_unlock(&gts->ts_ctxlock);\n+\t\tgru_unload_context(gts, 1);\n+\t\treturn VM_FAULT_NOPAGE;\n+\t}\n \n \tif (!gts->ts_gru) {\n \t\tSTAT(load_user_context);",
        "function_modified_lines": {
            "added": [
                "\tif (gru_check_context_placement(gts)) {",
                "\t\tpreempt_enable();",
                "\t\tmutex_unlock(&gts->ts_ctxlock);",
                "\t\tgru_unload_context(gts, 1);",
                "\t\treturn VM_FAULT_NOPAGE;",
                "\t}"
            ],
            "deleted": [
                "\tgru_check_context_placement(gts);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s SGI GRU driver in the way the first gru_file_unlocked_ioctl function is called by the user, where a fail pass occurs in the gru_check_chiplet_assignment function. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "id": 3597
    },
    {
        "cve_id": "CVE-2017-16525",
        "code_before_change": "static int usb_console_setup(struct console *co, char *options)\n{\n\tstruct usbcons_info *info = &usbcons_info;\n\tint baud = 9600;\n\tint bits = 8;\n\tint parity = 'n';\n\tint doflow = 0;\n\tint cflag = CREAD | HUPCL | CLOCAL;\n\tchar *s;\n\tstruct usb_serial *serial;\n\tstruct usb_serial_port *port;\n\tint retval;\n\tstruct tty_struct *tty = NULL;\n\tstruct ktermios dummy;\n\n\tif (options) {\n\t\tbaud = simple_strtoul(options, NULL, 10);\n\t\ts = options;\n\t\twhile (*s >= '0' && *s <= '9')\n\t\t\ts++;\n\t\tif (*s)\n\t\t\tparity = *s++;\n\t\tif (*s)\n\t\t\tbits   = *s++ - '0';\n\t\tif (*s)\n\t\t\tdoflow = (*s++ == 'r');\n\t}\n\t\n\t/* Sane default */\n\tif (baud == 0)\n\t\tbaud = 9600;\n\n\tswitch (bits) {\n\tcase 7:\n\t\tcflag |= CS7;\n\t\tbreak;\n\tdefault:\n\tcase 8:\n\t\tcflag |= CS8;\n\t\tbreak;\n\t}\n\tswitch (parity) {\n\tcase 'o': case 'O':\n\t\tcflag |= PARODD;\n\t\tbreak;\n\tcase 'e': case 'E':\n\t\tcflag |= PARENB;\n\t\tbreak;\n\t}\n\tco->cflag = cflag;\n\n\t/*\n\t * no need to check the index here: if the index is wrong, console\n\t * code won't call us\n\t */\n\tport = usb_serial_port_get_by_minor(co->index);\n\tif (port == NULL) {\n\t\t/* no device is connected yet, sorry :( */\n\t\tpr_err(\"No USB device connected to ttyUSB%i\\n\", co->index);\n\t\treturn -ENODEV;\n\t}\n\tserial = port->serial;\n\n\tretval = usb_autopm_get_interface(serial->interface);\n\tif (retval)\n\t\tgoto error_get_interface;\n\n\ttty_port_tty_set(&port->port, NULL);\n\n\tinfo->port = port;\n\n\t++port->port.count;\n\tif (!tty_port_initialized(&port->port)) {\n\t\tif (serial->type->set_termios) {\n\t\t\t/*\n\t\t\t * allocate a fake tty so the driver can initialize\n\t\t\t * the termios structure, then later call set_termios to\n\t\t\t * configure according to command line arguments\n\t\t\t */\n\t\t\ttty = kzalloc(sizeof(*tty), GFP_KERNEL);\n\t\t\tif (!tty) {\n\t\t\t\tretval = -ENOMEM;\n\t\t\t\tgoto reset_open_count;\n\t\t\t}\n\t\t\tkref_init(&tty->kref);\n\t\t\ttty->driver = usb_serial_tty_driver;\n\t\t\ttty->index = co->index;\n\t\t\tinit_ldsem(&tty->ldisc_sem);\n\t\t\tspin_lock_init(&tty->files_lock);\n\t\t\tINIT_LIST_HEAD(&tty->tty_files);\n\t\t\tkref_get(&tty->driver->kref);\n\t\t\t__module_get(tty->driver->owner);\n\t\t\ttty->ops = &usb_console_fake_tty_ops;\n\t\t\ttty_init_termios(tty);\n\t\t\ttty_port_tty_set(&port->port, tty);\n\t\t}\n\n\t\t/* only call the device specific open if this\n\t\t * is the first time the port is opened */\n\t\tretval = serial->type->open(NULL, port);\n\t\tif (retval) {\n\t\t\tdev_err(&port->dev, \"could not open USB console port\\n\");\n\t\t\tgoto fail;\n\t\t}\n\n\t\tif (serial->type->set_termios) {\n\t\t\ttty->termios.c_cflag = cflag;\n\t\t\ttty_termios_encode_baud_rate(&tty->termios, baud, baud);\n\t\t\tmemset(&dummy, 0, sizeof(struct ktermios));\n\t\t\tserial->type->set_termios(tty, port, &dummy);\n\n\t\t\ttty_port_tty_set(&port->port, NULL);\n\t\t\ttty_kref_put(tty);\n\t\t}\n\t\ttty_port_set_initialized(&port->port, 1);\n\t}\n\t/* Now that any required fake tty operations are completed restore\n\t * the tty port count */\n\t--port->port.count;\n\t/* The console is special in terms of closing the device so\n\t * indicate this port is now acting as a system console. */\n\tport->port.console = 1;\n\n\tmutex_unlock(&serial->disc_mutex);\n\treturn retval;\n\n fail:\n\ttty_port_tty_set(&port->port, NULL);\n\ttty_kref_put(tty);\n reset_open_count:\n\tport->port.count = 0;\n\tusb_autopm_put_interface(serial->interface);\n error_get_interface:\n\tusb_serial_put(serial);\n\tmutex_unlock(&serial->disc_mutex);\n\treturn retval;\n}",
        "code_after_change": "static int usb_console_setup(struct console *co, char *options)\n{\n\tstruct usbcons_info *info = &usbcons_info;\n\tint baud = 9600;\n\tint bits = 8;\n\tint parity = 'n';\n\tint doflow = 0;\n\tint cflag = CREAD | HUPCL | CLOCAL;\n\tchar *s;\n\tstruct usb_serial *serial;\n\tstruct usb_serial_port *port;\n\tint retval;\n\tstruct tty_struct *tty = NULL;\n\tstruct ktermios dummy;\n\n\tif (options) {\n\t\tbaud = simple_strtoul(options, NULL, 10);\n\t\ts = options;\n\t\twhile (*s >= '0' && *s <= '9')\n\t\t\ts++;\n\t\tif (*s)\n\t\t\tparity = *s++;\n\t\tif (*s)\n\t\t\tbits   = *s++ - '0';\n\t\tif (*s)\n\t\t\tdoflow = (*s++ == 'r');\n\t}\n\t\n\t/* Sane default */\n\tif (baud == 0)\n\t\tbaud = 9600;\n\n\tswitch (bits) {\n\tcase 7:\n\t\tcflag |= CS7;\n\t\tbreak;\n\tdefault:\n\tcase 8:\n\t\tcflag |= CS8;\n\t\tbreak;\n\t}\n\tswitch (parity) {\n\tcase 'o': case 'O':\n\t\tcflag |= PARODD;\n\t\tbreak;\n\tcase 'e': case 'E':\n\t\tcflag |= PARENB;\n\t\tbreak;\n\t}\n\tco->cflag = cflag;\n\n\t/*\n\t * no need to check the index here: if the index is wrong, console\n\t * code won't call us\n\t */\n\tport = usb_serial_port_get_by_minor(co->index);\n\tif (port == NULL) {\n\t\t/* no device is connected yet, sorry :( */\n\t\tpr_err(\"No USB device connected to ttyUSB%i\\n\", co->index);\n\t\treturn -ENODEV;\n\t}\n\tserial = port->serial;\n\n\tretval = usb_autopm_get_interface(serial->interface);\n\tif (retval)\n\t\tgoto error_get_interface;\n\n\ttty_port_tty_set(&port->port, NULL);\n\n\tinfo->port = port;\n\n\t++port->port.count;\n\tif (!tty_port_initialized(&port->port)) {\n\t\tif (serial->type->set_termios) {\n\t\t\t/*\n\t\t\t * allocate a fake tty so the driver can initialize\n\t\t\t * the termios structure, then later call set_termios to\n\t\t\t * configure according to command line arguments\n\t\t\t */\n\t\t\ttty = kzalloc(sizeof(*tty), GFP_KERNEL);\n\t\t\tif (!tty) {\n\t\t\t\tretval = -ENOMEM;\n\t\t\t\tgoto reset_open_count;\n\t\t\t}\n\t\t\tkref_init(&tty->kref);\n\t\t\ttty->driver = usb_serial_tty_driver;\n\t\t\ttty->index = co->index;\n\t\t\tinit_ldsem(&tty->ldisc_sem);\n\t\t\tspin_lock_init(&tty->files_lock);\n\t\t\tINIT_LIST_HEAD(&tty->tty_files);\n\t\t\tkref_get(&tty->driver->kref);\n\t\t\t__module_get(tty->driver->owner);\n\t\t\ttty->ops = &usb_console_fake_tty_ops;\n\t\t\ttty_init_termios(tty);\n\t\t\ttty_port_tty_set(&port->port, tty);\n\t\t}\n\n\t\t/* only call the device specific open if this\n\t\t * is the first time the port is opened */\n\t\tretval = serial->type->open(NULL, port);\n\t\tif (retval) {\n\t\t\tdev_err(&port->dev, \"could not open USB console port\\n\");\n\t\t\tgoto fail;\n\t\t}\n\n\t\tif (serial->type->set_termios) {\n\t\t\ttty->termios.c_cflag = cflag;\n\t\t\ttty_termios_encode_baud_rate(&tty->termios, baud, baud);\n\t\t\tmemset(&dummy, 0, sizeof(struct ktermios));\n\t\t\tserial->type->set_termios(tty, port, &dummy);\n\n\t\t\ttty_port_tty_set(&port->port, NULL);\n\t\t\ttty_kref_put(tty);\n\t\t}\n\t\ttty_port_set_initialized(&port->port, 1);\n\t}\n\t/* Now that any required fake tty operations are completed restore\n\t * the tty port count */\n\t--port->port.count;\n\t/* The console is special in terms of closing the device so\n\t * indicate this port is now acting as a system console. */\n\tport->port.console = 1;\n\n\tmutex_unlock(&serial->disc_mutex);\n\treturn retval;\n\n fail:\n\ttty_port_tty_set(&port->port, NULL);\n\ttty_kref_put(tty);\n reset_open_count:\n\tport->port.count = 0;\n\tinfo->port = NULL;\n\tusb_autopm_put_interface(serial->interface);\n error_get_interface:\n\tusb_serial_put(serial);\n\tmutex_unlock(&serial->disc_mutex);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -129,6 +129,7 @@\n \ttty_kref_put(tty);\n  reset_open_count:\n \tport->port.count = 0;\n+\tinfo->port = NULL;\n \tusb_autopm_put_interface(serial->interface);\n  error_get_interface:\n \tusb_serial_put(serial);",
        "function_modified_lines": {
            "added": [
                "\tinfo->port = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The usb_serial_console_disconnect function in drivers/usb/serial/console.c in the Linux kernel before 4.13.8 allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via a crafted USB device, related to disconnection and failed setup.",
        "id": 1309
    },
    {
        "cve_id": "CVE-2022-2602",
        "code_before_change": "int __io_scm_file_account(struct io_ring_ctx *ctx, struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sk = ctx->ring_sock->sk;\n\tstruct sk_buff_head *head = &sk->sk_receive_queue;\n\tstruct scm_fp_list *fpl;\n\tstruct sk_buff *skb;\n\n\tif (likely(!io_file_need_scm(file)))\n\t\treturn 0;\n\n\t/*\n\t * See if we can merge this file into an existing skb SCM_RIGHTS\n\t * file set. If there's no room, fall back to allocating a new skb\n\t * and filling it in.\n\t */\n\tspin_lock_irq(&head->lock);\n\tskb = skb_peek(head);\n\tif (skb && UNIXCB(skb).fp->count < SCM_MAX_FD)\n\t\t__skb_unlink(skb, head);\n\telse\n\t\tskb = NULL;\n\tspin_unlock_irq(&head->lock);\n\n\tif (!skb) {\n\t\tfpl = kzalloc(sizeof(*fpl), GFP_KERNEL);\n\t\tif (!fpl)\n\t\t\treturn -ENOMEM;\n\n\t\tskb = alloc_skb(0, GFP_KERNEL);\n\t\tif (!skb) {\n\t\t\tkfree(fpl);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfpl->user = get_uid(current_user());\n\t\tfpl->max = SCM_MAX_FD;\n\t\tfpl->count = 0;\n\n\t\tUNIXCB(skb).fp = fpl;\n\t\tskb->sk = sk;\n\t\tskb->destructor = unix_destruct_scm;\n\t\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\t}\n\n\tfpl = UNIXCB(skb).fp;\n\tfpl->fp[fpl->count++] = get_file(file);\n\tunix_inflight(fpl->user, file);\n\tskb_queue_head(head, skb);\n\tfput(file);\n#endif\n\treturn 0;\n}",
        "code_after_change": "int __io_scm_file_account(struct io_ring_ctx *ctx, struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sk = ctx->ring_sock->sk;\n\tstruct sk_buff_head *head = &sk->sk_receive_queue;\n\tstruct scm_fp_list *fpl;\n\tstruct sk_buff *skb;\n\n\tif (likely(!io_file_need_scm(file)))\n\t\treturn 0;\n\n\t/*\n\t * See if we can merge this file into an existing skb SCM_RIGHTS\n\t * file set. If there's no room, fall back to allocating a new skb\n\t * and filling it in.\n\t */\n\tspin_lock_irq(&head->lock);\n\tskb = skb_peek(head);\n\tif (skb && UNIXCB(skb).fp->count < SCM_MAX_FD)\n\t\t__skb_unlink(skb, head);\n\telse\n\t\tskb = NULL;\n\tspin_unlock_irq(&head->lock);\n\n\tif (!skb) {\n\t\tfpl = kzalloc(sizeof(*fpl), GFP_KERNEL);\n\t\tif (!fpl)\n\t\t\treturn -ENOMEM;\n\n\t\tskb = alloc_skb(0, GFP_KERNEL);\n\t\tif (!skb) {\n\t\t\tkfree(fpl);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfpl->user = get_uid(current_user());\n\t\tfpl->max = SCM_MAX_FD;\n\t\tfpl->count = 0;\n\n\t\tUNIXCB(skb).fp = fpl;\n\t\tskb->sk = sk;\n\t\tskb->scm_io_uring = 1;\n\t\tskb->destructor = unix_destruct_scm;\n\t\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\t}\n\n\tfpl = UNIXCB(skb).fp;\n\tfpl->fp[fpl->count++] = get_file(file);\n\tunix_inflight(fpl->user, file);\n\tskb_queue_head(head, skb);\n\tfput(file);\n#endif\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -39,6 +39,7 @@\n \n \t\tUNIXCB(skb).fp = fpl;\n \t\tskb->sk = sk;\n+\t\tskb->scm_io_uring = 1;\n \t\tskb->destructor = unix_destruct_scm;\n \t\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tskb->scm_io_uring = 1;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "io_uring UAF, Unix SCM garbage collection",
        "id": 3483
    },
    {
        "cve_id": "CVE-2020-7053",
        "code_before_change": "int i915_gem_context_open(struct drm_i915_private *i915,\n\t\t\t  struct drm_file *file)\n{\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\tstruct i915_gem_context *ctx;\n\tint err;\n\n\tidr_init(&file_priv->context_idr);\n\n\tmutex_lock(&i915->drm.struct_mutex);\n\n\tctx = i915_gem_create_context(i915);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err;\n\t}\n\n\terr = gem_context_register(ctx, file_priv);\n\tif (err)\n\t\tgoto err_ctx;\n\n\tGEM_BUG_ON(ctx->user_handle != DEFAULT_CONTEXT_HANDLE);\n\tGEM_BUG_ON(i915_gem_context_is_kernel(ctx));\n\n\tmutex_unlock(&i915->drm.struct_mutex);\n\n\treturn 0;\n\nerr_ctx:\n\tcontext_close(ctx);\nerr:\n\tmutex_unlock(&i915->drm.struct_mutex);\n\tidr_destroy(&file_priv->context_idr);\n\treturn PTR_ERR(ctx);\n}",
        "code_after_change": "int i915_gem_context_open(struct drm_i915_private *i915,\n\t\t\t  struct drm_file *file)\n{\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\tstruct i915_gem_context *ctx;\n\tint err;\n\n\tidr_init(&file_priv->context_idr);\n\tmutex_init(&file_priv->context_idr_lock);\n\n\tmutex_lock(&i915->drm.struct_mutex);\n\tctx = i915_gem_create_context(i915);\n\tmutex_unlock(&i915->drm.struct_mutex);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err;\n\t}\n\n\terr = gem_context_register(ctx, file_priv);\n\tif (err)\n\t\tgoto err_ctx;\n\n\tGEM_BUG_ON(ctx->user_handle != DEFAULT_CONTEXT_HANDLE);\n\tGEM_BUG_ON(i915_gem_context_is_kernel(ctx));\n\n\treturn 0;\n\nerr_ctx:\n\tmutex_lock(&i915->drm.struct_mutex);\n\tcontext_close(ctx);\n\tmutex_unlock(&i915->drm.struct_mutex);\nerr:\n\tmutex_destroy(&file_priv->context_idr_lock);\n\tidr_destroy(&file_priv->context_idr);\n\treturn PTR_ERR(ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,10 +6,11 @@\n \tint err;\n \n \tidr_init(&file_priv->context_idr);\n+\tmutex_init(&file_priv->context_idr_lock);\n \n \tmutex_lock(&i915->drm.struct_mutex);\n-\n \tctx = i915_gem_create_context(i915);\n+\tmutex_unlock(&i915->drm.struct_mutex);\n \tif (IS_ERR(ctx)) {\n \t\terr = PTR_ERR(ctx);\n \t\tgoto err;\n@@ -22,14 +23,14 @@\n \tGEM_BUG_ON(ctx->user_handle != DEFAULT_CONTEXT_HANDLE);\n \tGEM_BUG_ON(i915_gem_context_is_kernel(ctx));\n \n-\tmutex_unlock(&i915->drm.struct_mutex);\n-\n \treturn 0;\n \n err_ctx:\n+\tmutex_lock(&i915->drm.struct_mutex);\n \tcontext_close(ctx);\n+\tmutex_unlock(&i915->drm.struct_mutex);\n err:\n-\tmutex_unlock(&i915->drm.struct_mutex);\n+\tmutex_destroy(&file_priv->context_idr_lock);\n \tidr_destroy(&file_priv->context_idr);\n \treturn PTR_ERR(ctx);\n }",
        "function_modified_lines": {
            "added": [
                "\tmutex_init(&file_priv->context_idr_lock);",
                "\tmutex_unlock(&i915->drm.struct_mutex);",
                "\tmutex_lock(&i915->drm.struct_mutex);",
                "\tmutex_unlock(&i915->drm.struct_mutex);",
                "\tmutex_destroy(&file_priv->context_idr_lock);"
            ],
            "deleted": [
                "",
                "\tmutex_unlock(&i915->drm.struct_mutex);",
                "",
                "\tmutex_unlock(&i915->drm.struct_mutex);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 4.14 longterm through 4.14.165 and 4.19 longterm through 4.19.96 (and 5.x before 5.2), there is a use-after-free (write) in the i915_ppgtt_close function in drivers/gpu/drm/i915/i915_gem_gtt.c, aka CID-7dc40713618c. This is related to i915_gem_context_destroy_ioctl in drivers/gpu/drm/i915/i915_gem_context.c.",
        "id": 2803
    },
    {
        "cve_id": "CVE-2021-45868",
        "code_before_change": "static int remove_tree(struct qtree_mem_dqinfo *info, struct dquot *dquot,\n\t\t       uint *blk, int depth)\n{\n\tchar *buf = kmalloc(info->dqi_usable_bs, GFP_NOFS);\n\tint ret = 0;\n\tuint newblk;\n\t__le32 *ref = (__le32 *)buf;\n\n\tif (!buf)\n\t\treturn -ENOMEM;\n\tret = read_blk(info, *blk, buf);\n\tif (ret < 0) {\n\t\tquota_error(dquot->dq_sb, \"Can't read quota data block %u\",\n\t\t\t    *blk);\n\t\tgoto out_buf;\n\t}\n\tnewblk = le32_to_cpu(ref[get_index(info, dquot->dq_id, depth)]);\n\tif (depth == info->dqi_qtree_depth - 1) {\n\t\tret = free_dqentry(info, dquot, newblk);\n\t\tnewblk = 0;\n\t} else {\n\t\tret = remove_tree(info, dquot, &newblk, depth+1);\n\t}\n\tif (ret >= 0 && !newblk) {\n\t\tint i;\n\t\tref[get_index(info, dquot->dq_id, depth)] = cpu_to_le32(0);\n\t\t/* Block got empty? */\n\t\tfor (i = 0; i < (info->dqi_usable_bs >> 2) && !ref[i]; i++)\n\t\t\t;\n\t\t/* Don't put the root block into the free block list */\n\t\tif (i == (info->dqi_usable_bs >> 2)\n\t\t    && *blk != QT_TREEOFF) {\n\t\t\tput_free_dqblk(info, buf, *blk);\n\t\t\t*blk = 0;\n\t\t} else {\n\t\t\tret = write_blk(info, *blk, buf);\n\t\t\tif (ret < 0)\n\t\t\t\tquota_error(dquot->dq_sb,\n\t\t\t\t\t    \"Can't write quota tree block %u\",\n\t\t\t\t\t    *blk);\n\t\t}\n\t}\nout_buf:\n\tkfree(buf);\n\treturn ret;\n}",
        "code_after_change": "static int remove_tree(struct qtree_mem_dqinfo *info, struct dquot *dquot,\n\t\t       uint *blk, int depth)\n{\n\tchar *buf = kmalloc(info->dqi_usable_bs, GFP_NOFS);\n\tint ret = 0;\n\tuint newblk;\n\t__le32 *ref = (__le32 *)buf;\n\n\tif (!buf)\n\t\treturn -ENOMEM;\n\tret = read_blk(info, *blk, buf);\n\tif (ret < 0) {\n\t\tquota_error(dquot->dq_sb, \"Can't read quota data block %u\",\n\t\t\t    *blk);\n\t\tgoto out_buf;\n\t}\n\tnewblk = le32_to_cpu(ref[get_index(info, dquot->dq_id, depth)]);\n\tif (newblk < QT_TREEOFF || newblk >= info->dqi_blocks) {\n\t\tquota_error(dquot->dq_sb, \"Getting block too big (%u >= %u)\",\n\t\t\t    newblk, info->dqi_blocks);\n\t\tret = -EUCLEAN;\n\t\tgoto out_buf;\n\t}\n\n\tif (depth == info->dqi_qtree_depth - 1) {\n\t\tret = free_dqentry(info, dquot, newblk);\n\t\tnewblk = 0;\n\t} else {\n\t\tret = remove_tree(info, dquot, &newblk, depth+1);\n\t}\n\tif (ret >= 0 && !newblk) {\n\t\tint i;\n\t\tref[get_index(info, dquot->dq_id, depth)] = cpu_to_le32(0);\n\t\t/* Block got empty? */\n\t\tfor (i = 0; i < (info->dqi_usable_bs >> 2) && !ref[i]; i++)\n\t\t\t;\n\t\t/* Don't put the root block into the free block list */\n\t\tif (i == (info->dqi_usable_bs >> 2)\n\t\t    && *blk != QT_TREEOFF) {\n\t\t\tput_free_dqblk(info, buf, *blk);\n\t\t\t*blk = 0;\n\t\t} else {\n\t\t\tret = write_blk(info, *blk, buf);\n\t\t\tif (ret < 0)\n\t\t\t\tquota_error(dquot->dq_sb,\n\t\t\t\t\t    \"Can't write quota tree block %u\",\n\t\t\t\t\t    *blk);\n\t\t}\n\t}\nout_buf:\n\tkfree(buf);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,6 +15,13 @@\n \t\tgoto out_buf;\n \t}\n \tnewblk = le32_to_cpu(ref[get_index(info, dquot->dq_id, depth)]);\n+\tif (newblk < QT_TREEOFF || newblk >= info->dqi_blocks) {\n+\t\tquota_error(dquot->dq_sb, \"Getting block too big (%u >= %u)\",\n+\t\t\t    newblk, info->dqi_blocks);\n+\t\tret = -EUCLEAN;\n+\t\tgoto out_buf;\n+\t}\n+\n \tif (depth == info->dqi_qtree_depth - 1) {\n \t\tret = free_dqentry(info, dquot, newblk);\n \t\tnewblk = 0;",
        "function_modified_lines": {
            "added": [
                "\tif (newblk < QT_TREEOFF || newblk >= info->dqi_blocks) {",
                "\t\tquota_error(dquot->dq_sb, \"Getting block too big (%u >= %u)\",",
                "\t\t\t    newblk, info->dqi_blocks);",
                "\t\tret = -EUCLEAN;",
                "\t\tgoto out_buf;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 5.15.3, fs/quota/quota_tree.c does not validate the block number in the quota tree (on disk). This can, for example, lead to a kernel/locking/rwsem.c use-after-free if there is a corrupted quota file.",
        "id": 3183
    },
    {
        "cve_id": "CVE-2020-27675",
        "code_before_change": "evtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))\n\t\treturn 0;\n\n\treturn info_for_irq(irq)->evtchn;\n}",
        "code_after_change": "evtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tconst struct irq_info *info = NULL;\n\n\tif (likely(irq < nr_irqs))\n\t\tinfo = info_for_irq(irq);\n\tif (!info)\n\t\treturn 0;\n\n\treturn info->evtchn;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,11 @@\n evtchn_port_t evtchn_from_irq(unsigned irq)\n {\n-\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))\n+\tconst struct irq_info *info = NULL;\n+\n+\tif (likely(irq < nr_irqs))\n+\t\tinfo = info_for_irq(irq);\n+\tif (!info)\n \t\treturn 0;\n \n-\treturn info_for_irq(irq)->evtchn;\n+\treturn info->evtchn;\n }",
        "function_modified_lines": {
            "added": [
                "\tconst struct irq_info *info = NULL;",
                "",
                "\tif (likely(irq < nr_irqs))",
                "\t\tinfo = info_for_irq(irq);",
                "\tif (!info)",
                "\treturn info->evtchn;"
            ],
            "deleted": [
                "\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))",
                "\treturn info_for_irq(irq)->evtchn;"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-476",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.9.1, as used with Xen through 4.14.x. drivers/xen/events/events_base.c allows event-channel removal during the event-handling loop (a race condition). This can cause a use-after-free or NULL pointer dereference, as demonstrated by a dom0 crash via events for an in-reconfiguration paravirtualized device, aka CID-073d0552ead5.",
        "id": 2626
    },
    {
        "cve_id": "CVE-2023-3389",
        "code_before_change": "int io_arm_poll_handler(struct io_kiocb *req, unsigned issue_flags)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct async_poll *apoll;\n\tstruct io_poll_table ipt;\n\t__poll_t mask = POLLPRI | POLLERR | EPOLLET;\n\tint ret;\n\n\tif (!def->pollin && !def->pollout)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!file_can_poll(req->file))\n\t\treturn IO_APOLL_ABORTED;\n\tif ((req->flags & (REQ_F_POLLED|REQ_F_PARTIAL_IO)) == REQ_F_POLLED)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!(req->flags & REQ_F_APOLL_MULTISHOT))\n\t\tmask |= EPOLLONESHOT;\n\n\tif (def->pollin) {\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\t/* If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN */\n\t\tif (req->flags & REQ_F_CLEAR_POLLIN)\n\t\t\tmask &= ~EPOLLIN;\n\t} else {\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t}\n\tif (def->poll_exclusive)\n\t\tmask |= EPOLLEXCLUSIVE;\n\tif (req->flags & REQ_F_POLLED) {\n\t\tapoll = req->apoll;\n\t\tkfree(apoll->double_poll);\n\t} else if (!(issue_flags & IO_URING_F_UNLOCKED) &&\n\t\t   !list_empty(&ctx->apoll_cache)) {\n\t\tapoll = list_first_entry(&ctx->apoll_cache, struct async_poll,\n\t\t\t\t\t\tpoll.wait.entry);\n\t\tlist_del_init(&apoll->poll.wait.entry);\n\t} else {\n\t\tapoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);\n\t\tif (unlikely(!apoll))\n\t\t\treturn IO_APOLL_ABORTED;\n\t}\n\tapoll->double_poll = NULL;\n\treq->apoll = apoll;\n\treq->flags |= REQ_F_POLLED;\n\tipt.pt._qproc = io_async_queue_proc;\n\n\tio_kbuf_recycle(req, issue_flags);\n\n\tret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask);\n\tif (ret || ipt.error)\n\t\treturn ret ? IO_APOLL_READY : IO_APOLL_ABORTED;\n\n\ttrace_io_uring_poll_arm(ctx, req, req->cqe.user_data, req->opcode,\n\t\t\t\tmask, apoll->poll.events);\n\treturn IO_APOLL_OK;\n}",
        "code_after_change": "int io_arm_poll_handler(struct io_kiocb *req, unsigned issue_flags)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct async_poll *apoll;\n\tstruct io_poll_table ipt;\n\t__poll_t mask = POLLPRI | POLLERR | EPOLLET;\n\tint ret;\n\n\t/*\n\t * apoll requests already grab the mutex to complete in the tw handler,\n\t * so removal from the mutex-backed hash is free, use it by default.\n\t */\n\tif (issue_flags & IO_URING_F_UNLOCKED)\n\t\treq->flags &= ~REQ_F_HASH_LOCKED;\n\telse\n\t\treq->flags |= REQ_F_HASH_LOCKED;\n\n\tif (!def->pollin && !def->pollout)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!file_can_poll(req->file))\n\t\treturn IO_APOLL_ABORTED;\n\tif ((req->flags & (REQ_F_POLLED|REQ_F_PARTIAL_IO)) == REQ_F_POLLED)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!(req->flags & REQ_F_APOLL_MULTISHOT))\n\t\tmask |= EPOLLONESHOT;\n\n\tif (def->pollin) {\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\t/* If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN */\n\t\tif (req->flags & REQ_F_CLEAR_POLLIN)\n\t\t\tmask &= ~EPOLLIN;\n\t} else {\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t}\n\tif (def->poll_exclusive)\n\t\tmask |= EPOLLEXCLUSIVE;\n\tif (req->flags & REQ_F_POLLED) {\n\t\tapoll = req->apoll;\n\t\tkfree(apoll->double_poll);\n\t} else if (!(issue_flags & IO_URING_F_UNLOCKED) &&\n\t\t   !list_empty(&ctx->apoll_cache)) {\n\t\tapoll = list_first_entry(&ctx->apoll_cache, struct async_poll,\n\t\t\t\t\t\tpoll.wait.entry);\n\t\tlist_del_init(&apoll->poll.wait.entry);\n\t} else {\n\t\tapoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);\n\t\tif (unlikely(!apoll))\n\t\t\treturn IO_APOLL_ABORTED;\n\t}\n\tapoll->double_poll = NULL;\n\treq->apoll = apoll;\n\treq->flags |= REQ_F_POLLED;\n\tipt.pt._qproc = io_async_queue_proc;\n\n\tio_kbuf_recycle(req, issue_flags);\n\n\tret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask);\n\tif (ret || ipt.error)\n\t\treturn ret ? IO_APOLL_READY : IO_APOLL_ABORTED;\n\n\ttrace_io_uring_poll_arm(ctx, req, req->cqe.user_data, req->opcode,\n\t\t\t\tmask, apoll->poll.events);\n\treturn IO_APOLL_OK;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,15 @@\n \tstruct io_poll_table ipt;\n \t__poll_t mask = POLLPRI | POLLERR | EPOLLET;\n \tint ret;\n+\n+\t/*\n+\t * apoll requests already grab the mutex to complete in the tw handler,\n+\t * so removal from the mutex-backed hash is free, use it by default.\n+\t */\n+\tif (issue_flags & IO_URING_F_UNLOCKED)\n+\t\treq->flags &= ~REQ_F_HASH_LOCKED;\n+\telse\n+\t\treq->flags |= REQ_F_HASH_LOCKED;\n \n \tif (!def->pollin && !def->pollout)\n \t\treturn IO_APOLL_ABORTED;",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * apoll requests already grab the mutex to complete in the tw handler,",
                "\t * so removal from the mutex-backed hash is free, use it by default.",
                "\t */",
                "\tif (issue_flags & IO_URING_F_UNLOCKED)",
                "\t\treq->flags &= ~REQ_F_HASH_LOCKED;",
                "\telse",
                "\t\treq->flags |= REQ_F_HASH_LOCKED;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring subsystem can be exploited to achieve local privilege escalation.\n\nRacing a io_uring cancel poll request with a linked timeout can cause a UAF in a hrtimer.\n\nWe recommend upgrading past commit ef7dfac51d8ed961b742218f526bd589f3900a59 (4716c73b188566865bdd79c3a6709696a224ac04 for 5.10 stable and 0e388fce7aec40992eadee654193cad345d62663 for 5.15 stable).\n\n",
        "id": 4070
    },
    {
        "cve_id": "CVE-2019-6974",
        "code_before_change": "static int kvm_ioctl_create_device(struct kvm *kvm,\n\t\t\t\t   struct kvm_create_device *cd)\n{\n\tstruct kvm_device_ops *ops = NULL;\n\tstruct kvm_device *dev;\n\tbool test = cd->flags & KVM_CREATE_DEVICE_TEST;\n\tint ret;\n\n\tif (cd->type >= ARRAY_SIZE(kvm_device_ops_table))\n\t\treturn -ENODEV;\n\n\tops = kvm_device_ops_table[cd->type];\n\tif (ops == NULL)\n\t\treturn -ENODEV;\n\n\tif (test)\n\t\treturn 0;\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdev->ops = ops;\n\tdev->kvm = kvm;\n\n\tmutex_lock(&kvm->lock);\n\tret = ops->create(dev, cd->type);\n\tif (ret < 0) {\n\t\tmutex_unlock(&kvm->lock);\n\t\tkfree(dev);\n\t\treturn ret;\n\t}\n\tlist_add(&dev->vm_node, &kvm->devices);\n\tmutex_unlock(&kvm->lock);\n\n\tif (ops->init)\n\t\tops->init(dev);\n\n\tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n\tif (ret < 0) {\n\t\tmutex_lock(&kvm->lock);\n\t\tlist_del(&dev->vm_node);\n\t\tmutex_unlock(&kvm->lock);\n\t\tops->destroy(dev);\n\t\treturn ret;\n\t}\n\n\tkvm_get_kvm(kvm);\n\tcd->fd = ret;\n\treturn 0;\n}",
        "code_after_change": "static int kvm_ioctl_create_device(struct kvm *kvm,\n\t\t\t\t   struct kvm_create_device *cd)\n{\n\tstruct kvm_device_ops *ops = NULL;\n\tstruct kvm_device *dev;\n\tbool test = cd->flags & KVM_CREATE_DEVICE_TEST;\n\tint ret;\n\n\tif (cd->type >= ARRAY_SIZE(kvm_device_ops_table))\n\t\treturn -ENODEV;\n\n\tops = kvm_device_ops_table[cd->type];\n\tif (ops == NULL)\n\t\treturn -ENODEV;\n\n\tif (test)\n\t\treturn 0;\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdev->ops = ops;\n\tdev->kvm = kvm;\n\n\tmutex_lock(&kvm->lock);\n\tret = ops->create(dev, cd->type);\n\tif (ret < 0) {\n\t\tmutex_unlock(&kvm->lock);\n\t\tkfree(dev);\n\t\treturn ret;\n\t}\n\tlist_add(&dev->vm_node, &kvm->devices);\n\tmutex_unlock(&kvm->lock);\n\n\tif (ops->init)\n\t\tops->init(dev);\n\n\tkvm_get_kvm(kvm);\n\tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n\tif (ret < 0) {\n\t\tkvm_put_kvm(kvm);\n\t\tmutex_lock(&kvm->lock);\n\t\tlist_del(&dev->vm_node);\n\t\tmutex_unlock(&kvm->lock);\n\t\tops->destroy(dev);\n\t\treturn ret;\n\t}\n\n\tcd->fd = ret;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -36,8 +36,10 @@\n \tif (ops->init)\n \t\tops->init(dev);\n \n+\tkvm_get_kvm(kvm);\n \tret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);\n \tif (ret < 0) {\n+\t\tkvm_put_kvm(kvm);\n \t\tmutex_lock(&kvm->lock);\n \t\tlist_del(&dev->vm_node);\n \t\tmutex_unlock(&kvm->lock);\n@@ -45,7 +47,6 @@\n \t\treturn ret;\n \t}\n \n-\tkvm_get_kvm(kvm);\n \tcd->fd = ret;\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tkvm_get_kvm(kvm);",
                "\t\tkvm_put_kvm(kvm);"
            ],
            "deleted": [
                "\tkvm_get_kvm(kvm);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel before 4.20.8, kvm_ioctl_create_device in virt/kvm/kvm_main.c mishandles reference counting because of a race condition, leading to a use-after-free.",
        "id": 2342
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}",
        "code_after_change": "static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\trcu_read_lock();\n\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t\t       np->tclass);\n\t\trcu_read_unlock();\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,9 @@\n \tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n \n \n-\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\trcu_read_lock();\n+\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n+\trcu_read_unlock();\n \n \tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \tif (IS_ERR(dst)) {\n@@ -36,7 +38,10 @@\n \t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n \t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n \t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n-\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n+\t\trcu_read_lock();\n+\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n+\t\t\t       np->tclass);\n+\t\trcu_read_unlock();\n \t\terr = net_xmit_eval(err);\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\trcu_read_lock();",
                "\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);",
                "\trcu_read_unlock();",
                "\t\trcu_read_lock();",
                "\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),",
                "\t\t\t       np->tclass);",
                "\t\trcu_read_unlock();"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);",
                "\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 991
    },
    {
        "cve_id": "CVE-2023-3610",
        "code_before_change": "static void nft_rule_expr_activate(const struct nft_ctx *ctx,\n\t\t\t\t   struct nft_rule *rule)\n{\n\tstruct nft_expr *expr;\n\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tif (expr->ops->activate)\n\t\t\texpr->ops->activate(ctx, expr);\n\n\t\texpr = nft_expr_next(expr);\n\t}\n}",
        "code_after_change": "void nft_rule_expr_activate(const struct nft_ctx *ctx, struct nft_rule *rule)\n{\n\tstruct nft_expr *expr;\n\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tif (expr->ops->activate)\n\t\t\texpr->ops->activate(ctx, expr);\n\n\t\texpr = nft_expr_next(expr);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,4 @@\n-static void nft_rule_expr_activate(const struct nft_ctx *ctx,\n-\t\t\t\t   struct nft_rule *rule)\n+void nft_rule_expr_activate(const struct nft_ctx *ctx, struct nft_rule *rule)\n {\n \tstruct nft_expr *expr;\n ",
        "function_modified_lines": {
            "added": [
                "void nft_rule_expr_activate(const struct nft_ctx *ctx, struct nft_rule *rule)"
            ],
            "deleted": [
                "static void nft_rule_expr_activate(const struct nft_ctx *ctx,",
                "\t\t\t\t   struct nft_rule *rule)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nFlaw in the error handling of bound chains causes a use-after-free in the abort path of NFT_MSG_NEWRULE. The vulnerability requires CAP_NET_ADMIN to be triggered.\n\nWe recommend upgrading past commit 4bedf9eee016286c835e3d8fa981ddece5338795.\n\n",
        "id": 4119
    },
    {
        "cve_id": "CVE-2019-2025",
        "code_before_change": "static struct binder_buffer *binder_alloc_new_buf_locked(\n\t\t\t\tstruct binder_alloc *alloc,\n\t\t\t\tsize_t data_size,\n\t\t\t\tsize_t offsets_size,\n\t\t\t\tsize_t extra_buffers_size,\n\t\t\t\tint is_async)\n{\n\tstruct rb_node *n = alloc->free_buffers.rb_node;\n\tstruct binder_buffer *buffer;\n\tsize_t buffer_size;\n\tstruct rb_node *best_fit = NULL;\n\tvoid *has_page_addr;\n\tvoid *end_page_addr;\n\tsize_t size, data_offsets_size;\n\tint ret;\n\n\tif (!binder_alloc_get_vma(alloc)) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"%d: binder_alloc_buf, no vma\\n\",\n\t\t\t\t   alloc->pid);\n\t\treturn ERR_PTR(-ESRCH);\n\t}\n\n\tdata_offsets_size = ALIGN(data_size, sizeof(void *)) +\n\t\tALIGN(offsets_size, sizeof(void *));\n\n\tif (data_offsets_size < data_size || data_offsets_size < offsets_size) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t\"%d: got transaction with invalid size %zd-%zd\\n\",\n\t\t\t\talloc->pid, data_size, offsets_size);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tsize = data_offsets_size + ALIGN(extra_buffers_size, sizeof(void *));\n\tif (size < data_offsets_size || size < extra_buffers_size) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t\"%d: got transaction with invalid extra_buffers_size %zd\\n\",\n\t\t\t\talloc->pid, extra_buffers_size);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tif (is_async &&\n\t    alloc->free_async_space < size + sizeof(struct binder_buffer)) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t     \"%d: binder_alloc_buf size %zd failed, no async space left\\n\",\n\t\t\t      alloc->pid, size);\n\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\n\t/* Pad 0-size buffers so they get assigned unique addresses */\n\tsize = max(size, sizeof(void *));\n\n\twhile (n) {\n\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\tBUG_ON(!buffer->free);\n\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\n\t\tif (size < buffer_size) {\n\t\t\tbest_fit = n;\n\t\t\tn = n->rb_left;\n\t\t} else if (size > buffer_size)\n\t\t\tn = n->rb_right;\n\t\telse {\n\t\t\tbest_fit = n;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (best_fit == NULL) {\n\t\tsize_t allocated_buffers = 0;\n\t\tsize_t largest_alloc_size = 0;\n\t\tsize_t total_alloc_size = 0;\n\t\tsize_t free_buffers = 0;\n\t\tsize_t largest_free_size = 0;\n\t\tsize_t total_free_size = 0;\n\n\t\tfor (n = rb_first(&alloc->allocated_buffers); n != NULL;\n\t\t     n = rb_next(n)) {\n\t\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\t\t\tallocated_buffers++;\n\t\t\ttotal_alloc_size += buffer_size;\n\t\t\tif (buffer_size > largest_alloc_size)\n\t\t\t\tlargest_alloc_size = buffer_size;\n\t\t}\n\t\tfor (n = rb_first(&alloc->free_buffers); n != NULL;\n\t\t     n = rb_next(n)) {\n\t\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\t\t\tfree_buffers++;\n\t\t\ttotal_free_size += buffer_size;\n\t\t\tif (buffer_size > largest_free_size)\n\t\t\t\tlargest_free_size = buffer_size;\n\t\t}\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"%d: binder_alloc_buf size %zd failed, no address space\\n\",\n\t\t\t\t   alloc->pid, size);\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"allocated: %zd (num: %zd largest: %zd), free: %zd (num: %zd largest: %zd)\\n\",\n\t\t\t\t   total_alloc_size, allocated_buffers,\n\t\t\t\t   largest_alloc_size, total_free_size,\n\t\t\t\t   free_buffers, largest_free_size);\n\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\tif (n == NULL) {\n\t\tbuffer = rb_entry(best_fit, struct binder_buffer, rb_node);\n\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\t}\n\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: binder_alloc_buf size %zd got buffer %pK size %zd\\n\",\n\t\t      alloc->pid, size, buffer, buffer_size);\n\n\thas_page_addr =\n\t\t(void *)(((uintptr_t)buffer->data + buffer_size) & PAGE_MASK);\n\tWARN_ON(n && buffer_size != size);\n\tend_page_addr =\n\t\t(void *)PAGE_ALIGN((uintptr_t)buffer->data + size);\n\tif (end_page_addr > has_page_addr)\n\t\tend_page_addr = has_page_addr;\n\tret = binder_update_page_range(alloc, 1,\n\t    (void *)PAGE_ALIGN((uintptr_t)buffer->data), end_page_addr);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tif (buffer_size != size) {\n\t\tstruct binder_buffer *new_buffer;\n\n\t\tnew_buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);\n\t\tif (!new_buffer) {\n\t\t\tpr_err(\"%s: %d failed to alloc new buffer struct\\n\",\n\t\t\t       __func__, alloc->pid);\n\t\t\tgoto err_alloc_buf_struct_failed;\n\t\t}\n\t\tnew_buffer->data = (u8 *)buffer->data + size;\n\t\tlist_add(&new_buffer->entry, &buffer->entry);\n\t\tnew_buffer->free = 1;\n\t\tbinder_insert_free_buffer(alloc, new_buffer);\n\t}\n\n\trb_erase(best_fit, &alloc->free_buffers);\n\tbuffer->free = 0;\n\tbuffer->free_in_progress = 0;\n\tbinder_insert_allocated_buffer_locked(alloc, buffer);\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: binder_alloc_buf size %zd got %pK\\n\",\n\t\t      alloc->pid, size, buffer);\n\tbuffer->data_size = data_size;\n\tbuffer->offsets_size = offsets_size;\n\tbuffer->async_transaction = is_async;\n\tbuffer->extra_buffers_size = extra_buffers_size;\n\tif (is_async) {\n\t\talloc->free_async_space -= size + sizeof(struct binder_buffer);\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC_ASYNC,\n\t\t\t     \"%d: binder_alloc_buf size %zd async free %zd\\n\",\n\t\t\t      alloc->pid, size, alloc->free_async_space);\n\t}\n\treturn buffer;\n\nerr_alloc_buf_struct_failed:\n\tbinder_update_page_range(alloc, 0,\n\t\t\t\t (void *)PAGE_ALIGN((uintptr_t)buffer->data),\n\t\t\t\t end_page_addr);\n\treturn ERR_PTR(-ENOMEM);\n}",
        "code_after_change": "static struct binder_buffer *binder_alloc_new_buf_locked(\n\t\t\t\tstruct binder_alloc *alloc,\n\t\t\t\tsize_t data_size,\n\t\t\t\tsize_t offsets_size,\n\t\t\t\tsize_t extra_buffers_size,\n\t\t\t\tint is_async)\n{\n\tstruct rb_node *n = alloc->free_buffers.rb_node;\n\tstruct binder_buffer *buffer;\n\tsize_t buffer_size;\n\tstruct rb_node *best_fit = NULL;\n\tvoid *has_page_addr;\n\tvoid *end_page_addr;\n\tsize_t size, data_offsets_size;\n\tint ret;\n\n\tif (!binder_alloc_get_vma(alloc)) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"%d: binder_alloc_buf, no vma\\n\",\n\t\t\t\t   alloc->pid);\n\t\treturn ERR_PTR(-ESRCH);\n\t}\n\n\tdata_offsets_size = ALIGN(data_size, sizeof(void *)) +\n\t\tALIGN(offsets_size, sizeof(void *));\n\n\tif (data_offsets_size < data_size || data_offsets_size < offsets_size) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t\"%d: got transaction with invalid size %zd-%zd\\n\",\n\t\t\t\talloc->pid, data_size, offsets_size);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tsize = data_offsets_size + ALIGN(extra_buffers_size, sizeof(void *));\n\tif (size < data_offsets_size || size < extra_buffers_size) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t\t\"%d: got transaction with invalid extra_buffers_size %zd\\n\",\n\t\t\t\talloc->pid, extra_buffers_size);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tif (is_async &&\n\t    alloc->free_async_space < size + sizeof(struct binder_buffer)) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t\t     \"%d: binder_alloc_buf size %zd failed, no async space left\\n\",\n\t\t\t      alloc->pid, size);\n\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\n\t/* Pad 0-size buffers so they get assigned unique addresses */\n\tsize = max(size, sizeof(void *));\n\n\twhile (n) {\n\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\tBUG_ON(!buffer->free);\n\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\n\t\tif (size < buffer_size) {\n\t\t\tbest_fit = n;\n\t\t\tn = n->rb_left;\n\t\t} else if (size > buffer_size)\n\t\t\tn = n->rb_right;\n\t\telse {\n\t\t\tbest_fit = n;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (best_fit == NULL) {\n\t\tsize_t allocated_buffers = 0;\n\t\tsize_t largest_alloc_size = 0;\n\t\tsize_t total_alloc_size = 0;\n\t\tsize_t free_buffers = 0;\n\t\tsize_t largest_free_size = 0;\n\t\tsize_t total_free_size = 0;\n\n\t\tfor (n = rb_first(&alloc->allocated_buffers); n != NULL;\n\t\t     n = rb_next(n)) {\n\t\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\t\t\tallocated_buffers++;\n\t\t\ttotal_alloc_size += buffer_size;\n\t\t\tif (buffer_size > largest_alloc_size)\n\t\t\t\tlargest_alloc_size = buffer_size;\n\t\t}\n\t\tfor (n = rb_first(&alloc->free_buffers); n != NULL;\n\t\t     n = rb_next(n)) {\n\t\t\tbuffer = rb_entry(n, struct binder_buffer, rb_node);\n\t\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\t\t\tfree_buffers++;\n\t\t\ttotal_free_size += buffer_size;\n\t\t\tif (buffer_size > largest_free_size)\n\t\t\t\tlargest_free_size = buffer_size;\n\t\t}\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"%d: binder_alloc_buf size %zd failed, no address space\\n\",\n\t\t\t\t   alloc->pid, size);\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"allocated: %zd (num: %zd largest: %zd), free: %zd (num: %zd largest: %zd)\\n\",\n\t\t\t\t   total_alloc_size, allocated_buffers,\n\t\t\t\t   largest_alloc_size, total_free_size,\n\t\t\t\t   free_buffers, largest_free_size);\n\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\tif (n == NULL) {\n\t\tbuffer = rb_entry(best_fit, struct binder_buffer, rb_node);\n\t\tbuffer_size = binder_alloc_buffer_size(alloc, buffer);\n\t}\n\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: binder_alloc_buf size %zd got buffer %pK size %zd\\n\",\n\t\t      alloc->pid, size, buffer, buffer_size);\n\n\thas_page_addr =\n\t\t(void *)(((uintptr_t)buffer->data + buffer_size) & PAGE_MASK);\n\tWARN_ON(n && buffer_size != size);\n\tend_page_addr =\n\t\t(void *)PAGE_ALIGN((uintptr_t)buffer->data + size);\n\tif (end_page_addr > has_page_addr)\n\t\tend_page_addr = has_page_addr;\n\tret = binder_update_page_range(alloc, 1,\n\t    (void *)PAGE_ALIGN((uintptr_t)buffer->data), end_page_addr);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tif (buffer_size != size) {\n\t\tstruct binder_buffer *new_buffer;\n\n\t\tnew_buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);\n\t\tif (!new_buffer) {\n\t\t\tpr_err(\"%s: %d failed to alloc new buffer struct\\n\",\n\t\t\t       __func__, alloc->pid);\n\t\t\tgoto err_alloc_buf_struct_failed;\n\t\t}\n\t\tnew_buffer->data = (u8 *)buffer->data + size;\n\t\tlist_add(&new_buffer->entry, &buffer->entry);\n\t\tnew_buffer->free = 1;\n\t\tbinder_insert_free_buffer(alloc, new_buffer);\n\t}\n\n\trb_erase(best_fit, &alloc->free_buffers);\n\tbuffer->free = 0;\n\tbuffer->allow_user_free = 0;\n\tbinder_insert_allocated_buffer_locked(alloc, buffer);\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: binder_alloc_buf size %zd got %pK\\n\",\n\t\t      alloc->pid, size, buffer);\n\tbuffer->data_size = data_size;\n\tbuffer->offsets_size = offsets_size;\n\tbuffer->async_transaction = is_async;\n\tbuffer->extra_buffers_size = extra_buffers_size;\n\tif (is_async) {\n\t\talloc->free_async_space -= size + sizeof(struct binder_buffer);\n\t\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC_ASYNC,\n\t\t\t     \"%d: binder_alloc_buf size %zd async free %zd\\n\",\n\t\t\t      alloc->pid, size, alloc->free_async_space);\n\t}\n\treturn buffer;\n\nerr_alloc_buf_struct_failed:\n\tbinder_update_page_range(alloc, 0,\n\t\t\t\t (void *)PAGE_ALIGN((uintptr_t)buffer->data),\n\t\t\t\t end_page_addr);\n\treturn ERR_PTR(-ENOMEM);\n}",
        "patch": "--- code before\n+++ code after\n@@ -137,7 +137,7 @@\n \n \trb_erase(best_fit, &alloc->free_buffers);\n \tbuffer->free = 0;\n-\tbuffer->free_in_progress = 0;\n+\tbuffer->allow_user_free = 0;\n \tbinder_insert_allocated_buffer_locked(alloc, buffer);\n \tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n \t\t     \"%d: binder_alloc_buf size %zd got %pK\\n\",",
        "function_modified_lines": {
            "added": [
                "\tbuffer->allow_user_free = 0;"
            ],
            "deleted": [
                "\tbuffer->free_in_progress = 0;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_thread_read of binder.c, there is a possible use-after-free due to improper locking. This could lead to local escalation of privilege in the kernel with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-116855682References: Upstream kernel",
        "id": 2277
    },
    {
        "cve_id": "CVE-2022-2977",
        "code_before_change": "static int tpm_add_char_device(struct tpm_chip *chip)\n{\n\tint rc;\n\n\trc = cdev_device_add(&chip->cdev, &chip->dev);\n\tif (rc) {\n\t\tdev_err(&chip->dev,\n\t\t\t\"unable to cdev_device_add() %s, major %d, minor %d, err=%d\\n\",\n\t\t\tdev_name(&chip->dev), MAJOR(chip->dev.devt),\n\t\t\tMINOR(chip->dev.devt), rc);\n\t\treturn rc;\n\t}\n\n\tif (chip->flags & TPM_CHIP_FLAG_TPM2 && !tpm_is_firmware_upgrade(chip)) {\n\t\trc = cdev_device_add(&chip->cdevs, &chip->devs);\n\t\tif (rc) {\n\t\t\tdev_err(&chip->devs,\n\t\t\t\t\"unable to cdev_device_add() %s, major %d, minor %d, err=%d\\n\",\n\t\t\t\tdev_name(&chip->devs), MAJOR(chip->devs.devt),\n\t\t\t\tMINOR(chip->devs.devt), rc);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t/* Make the chip available. */\n\tmutex_lock(&idr_lock);\n\tidr_replace(&dev_nums_idr, chip, chip->dev_num);\n\tmutex_unlock(&idr_lock);\n\n\treturn rc;\n}",
        "code_after_change": "static int tpm_add_char_device(struct tpm_chip *chip)\n{\n\tint rc;\n\n\trc = cdev_device_add(&chip->cdev, &chip->dev);\n\tif (rc) {\n\t\tdev_err(&chip->dev,\n\t\t\t\"unable to cdev_device_add() %s, major %d, minor %d, err=%d\\n\",\n\t\t\tdev_name(&chip->dev), MAJOR(chip->dev.devt),\n\t\t\tMINOR(chip->dev.devt), rc);\n\t\treturn rc;\n\t}\n\n\tif (chip->flags & TPM_CHIP_FLAG_TPM2 && !tpm_is_firmware_upgrade(chip)) {\n\t\trc = tpm_devs_add(chip);\n\t\tif (rc)\n\t\t\tgoto err_del_cdev;\n\t}\n\n\t/* Make the chip available. */\n\tmutex_lock(&idr_lock);\n\tidr_replace(&dev_nums_idr, chip, chip->dev_num);\n\tmutex_unlock(&idr_lock);\n\n\treturn 0;\n\nerr_del_cdev:\n\tcdev_device_del(&chip->cdev, &chip->dev);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,14 +12,9 @@\n \t}\n \n \tif (chip->flags & TPM_CHIP_FLAG_TPM2 && !tpm_is_firmware_upgrade(chip)) {\n-\t\trc = cdev_device_add(&chip->cdevs, &chip->devs);\n-\t\tif (rc) {\n-\t\t\tdev_err(&chip->devs,\n-\t\t\t\t\"unable to cdev_device_add() %s, major %d, minor %d, err=%d\\n\",\n-\t\t\t\tdev_name(&chip->devs), MAJOR(chip->devs.devt),\n-\t\t\t\tMINOR(chip->devs.devt), rc);\n-\t\t\treturn rc;\n-\t\t}\n+\t\trc = tpm_devs_add(chip);\n+\t\tif (rc)\n+\t\t\tgoto err_del_cdev;\n \t}\n \n \t/* Make the chip available. */\n@@ -27,5 +22,9 @@\n \tidr_replace(&dev_nums_idr, chip, chip->dev_num);\n \tmutex_unlock(&idr_lock);\n \n+\treturn 0;\n+\n+err_del_cdev:\n+\tcdev_device_del(&chip->cdev, &chip->dev);\n \treturn rc;\n }",
        "function_modified_lines": {
            "added": [
                "\t\trc = tpm_devs_add(chip);",
                "\t\tif (rc)",
                "\t\t\tgoto err_del_cdev;",
                "\treturn 0;",
                "",
                "err_del_cdev:",
                "\tcdev_device_del(&chip->cdev, &chip->dev);"
            ],
            "deleted": [
                "\t\trc = cdev_device_add(&chip->cdevs, &chip->devs);",
                "\t\tif (rc) {",
                "\t\t\tdev_err(&chip->devs,",
                "\t\t\t\t\"unable to cdev_device_add() %s, major %d, minor %d, err=%d\\n\",",
                "\t\t\t\tdev_name(&chip->devs), MAJOR(chip->devs.devt),",
                "\t\t\t\tMINOR(chip->devs.devt), rc);",
                "\t\t\treturn rc;",
                "\t\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel implementation of proxied virtualized TPM devices. On a system where virtualized TPM devices are configured (this is not the default) a local attacker can create a use-after-free and create a situation where it may be possible to escalate privileges on the system.",
        "id": 3527
    },
    {
        "cve_id": "CVE-2023-20928",
        "code_before_change": "static int binder_update_page_range(struct binder_alloc *alloc, int allocate,\n\t\t\t\t    void __user *start, void __user *end)\n{\n\tvoid __user *page_addr;\n\tunsigned long user_page_addr;\n\tstruct binder_lru_page *page;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mm_struct *mm = NULL;\n\tbool need_mm = false;\n\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: %s pages %pK-%pK\\n\", alloc->pid,\n\t\t     allocate ? \"allocate\" : \"free\", start, end);\n\n\tif (end <= start)\n\t\treturn 0;\n\n\ttrace_binder_update_page_range(alloc, allocate, start, end);\n\n\tif (allocate == 0)\n\t\tgoto free_range;\n\n\tfor (page_addr = start; page_addr < end; page_addr += PAGE_SIZE) {\n\t\tpage = &alloc->pages[(page_addr - alloc->buffer) / PAGE_SIZE];\n\t\tif (!page->page_ptr) {\n\t\t\tneed_mm = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (need_mm && mmget_not_zero(alloc->vma_vm_mm))\n\t\tmm = alloc->vma_vm_mm;\n\n\tif (mm) {\n\t\tmmap_read_lock(mm);\n\t\tvma = alloc->vma;\n\t}\n\n\tif (!vma && need_mm) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"%d: binder_alloc_buf failed to map pages in userspace, no vma\\n\",\n\t\t\t\t   alloc->pid);\n\t\tgoto err_no_vma;\n\t}\n\n\tfor (page_addr = start; page_addr < end; page_addr += PAGE_SIZE) {\n\t\tint ret;\n\t\tbool on_lru;\n\t\tsize_t index;\n\n\t\tindex = (page_addr - alloc->buffer) / PAGE_SIZE;\n\t\tpage = &alloc->pages[index];\n\n\t\tif (page->page_ptr) {\n\t\t\ttrace_binder_alloc_lru_start(alloc, index);\n\n\t\t\ton_lru = list_lru_del(&binder_alloc_lru, &page->lru);\n\t\t\tWARN_ON(!on_lru);\n\n\t\t\ttrace_binder_alloc_lru_end(alloc, index);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (WARN_ON(!vma))\n\t\t\tgoto err_page_ptr_cleared;\n\n\t\ttrace_binder_alloc_page_start(alloc, index);\n\t\tpage->page_ptr = alloc_page(GFP_KERNEL |\n\t\t\t\t\t    __GFP_HIGHMEM |\n\t\t\t\t\t    __GFP_ZERO);\n\t\tif (!page->page_ptr) {\n\t\t\tpr_err(\"%d: binder_alloc_buf failed for page at %pK\\n\",\n\t\t\t\talloc->pid, page_addr);\n\t\t\tgoto err_alloc_page_failed;\n\t\t}\n\t\tpage->alloc = alloc;\n\t\tINIT_LIST_HEAD(&page->lru);\n\n\t\tuser_page_addr = (uintptr_t)page_addr;\n\t\tret = vm_insert_page(vma, user_page_addr, page[0].page_ptr);\n\t\tif (ret) {\n\t\t\tpr_err(\"%d: binder_alloc_buf failed to map page at %lx in userspace\\n\",\n\t\t\t       alloc->pid, user_page_addr);\n\t\t\tgoto err_vm_insert_page_failed;\n\t\t}\n\n\t\tif (index + 1 > alloc->pages_high)\n\t\t\talloc->pages_high = index + 1;\n\n\t\ttrace_binder_alloc_page_end(alloc, index);\n\t}\n\tif (mm) {\n\t\tmmap_read_unlock(mm);\n\t\tmmput(mm);\n\t}\n\treturn 0;\n\nfree_range:\n\tfor (page_addr = end - PAGE_SIZE; 1; page_addr -= PAGE_SIZE) {\n\t\tbool ret;\n\t\tsize_t index;\n\n\t\tindex = (page_addr - alloc->buffer) / PAGE_SIZE;\n\t\tpage = &alloc->pages[index];\n\n\t\ttrace_binder_free_lru_start(alloc, index);\n\n\t\tret = list_lru_add(&binder_alloc_lru, &page->lru);\n\t\tWARN_ON(!ret);\n\n\t\ttrace_binder_free_lru_end(alloc, index);\n\t\tif (page_addr == start)\n\t\t\tbreak;\n\t\tcontinue;\n\nerr_vm_insert_page_failed:\n\t\t__free_page(page->page_ptr);\n\t\tpage->page_ptr = NULL;\nerr_alloc_page_failed:\nerr_page_ptr_cleared:\n\t\tif (page_addr == start)\n\t\t\tbreak;\n\t}\nerr_no_vma:\n\tif (mm) {\n\t\tmmap_read_unlock(mm);\n\t\tmmput(mm);\n\t}\n\treturn vma ? -ENOMEM : -ESRCH;\n}",
        "code_after_change": "static int binder_update_page_range(struct binder_alloc *alloc, int allocate,\n\t\t\t\t    void __user *start, void __user *end)\n{\n\tvoid __user *page_addr;\n\tunsigned long user_page_addr;\n\tstruct binder_lru_page *page;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mm_struct *mm = NULL;\n\tbool need_mm = false;\n\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: %s pages %pK-%pK\\n\", alloc->pid,\n\t\t     allocate ? \"allocate\" : \"free\", start, end);\n\n\tif (end <= start)\n\t\treturn 0;\n\n\ttrace_binder_update_page_range(alloc, allocate, start, end);\n\n\tif (allocate == 0)\n\t\tgoto free_range;\n\n\tfor (page_addr = start; page_addr < end; page_addr += PAGE_SIZE) {\n\t\tpage = &alloc->pages[(page_addr - alloc->buffer) / PAGE_SIZE];\n\t\tif (!page->page_ptr) {\n\t\t\tneed_mm = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (need_mm && mmget_not_zero(alloc->vma_vm_mm))\n\t\tmm = alloc->vma_vm_mm;\n\n\tif (mm) {\n\t\tmmap_read_lock(mm);\n\t\tvma = vma_lookup(mm, alloc->vma_addr);\n\t}\n\n\tif (!vma && need_mm) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"%d: binder_alloc_buf failed to map pages in userspace, no vma\\n\",\n\t\t\t\t   alloc->pid);\n\t\tgoto err_no_vma;\n\t}\n\n\tfor (page_addr = start; page_addr < end; page_addr += PAGE_SIZE) {\n\t\tint ret;\n\t\tbool on_lru;\n\t\tsize_t index;\n\n\t\tindex = (page_addr - alloc->buffer) / PAGE_SIZE;\n\t\tpage = &alloc->pages[index];\n\n\t\tif (page->page_ptr) {\n\t\t\ttrace_binder_alloc_lru_start(alloc, index);\n\n\t\t\ton_lru = list_lru_del(&binder_alloc_lru, &page->lru);\n\t\t\tWARN_ON(!on_lru);\n\n\t\t\ttrace_binder_alloc_lru_end(alloc, index);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (WARN_ON(!vma))\n\t\t\tgoto err_page_ptr_cleared;\n\n\t\ttrace_binder_alloc_page_start(alloc, index);\n\t\tpage->page_ptr = alloc_page(GFP_KERNEL |\n\t\t\t\t\t    __GFP_HIGHMEM |\n\t\t\t\t\t    __GFP_ZERO);\n\t\tif (!page->page_ptr) {\n\t\t\tpr_err(\"%d: binder_alloc_buf failed for page at %pK\\n\",\n\t\t\t\talloc->pid, page_addr);\n\t\t\tgoto err_alloc_page_failed;\n\t\t}\n\t\tpage->alloc = alloc;\n\t\tINIT_LIST_HEAD(&page->lru);\n\n\t\tuser_page_addr = (uintptr_t)page_addr;\n\t\tret = vm_insert_page(vma, user_page_addr, page[0].page_ptr);\n\t\tif (ret) {\n\t\t\tpr_err(\"%d: binder_alloc_buf failed to map page at %lx in userspace\\n\",\n\t\t\t       alloc->pid, user_page_addr);\n\t\t\tgoto err_vm_insert_page_failed;\n\t\t}\n\n\t\tif (index + 1 > alloc->pages_high)\n\t\t\talloc->pages_high = index + 1;\n\n\t\ttrace_binder_alloc_page_end(alloc, index);\n\t}\n\tif (mm) {\n\t\tmmap_read_unlock(mm);\n\t\tmmput(mm);\n\t}\n\treturn 0;\n\nfree_range:\n\tfor (page_addr = end - PAGE_SIZE; 1; page_addr -= PAGE_SIZE) {\n\t\tbool ret;\n\t\tsize_t index;\n\n\t\tindex = (page_addr - alloc->buffer) / PAGE_SIZE;\n\t\tpage = &alloc->pages[index];\n\n\t\ttrace_binder_free_lru_start(alloc, index);\n\n\t\tret = list_lru_add(&binder_alloc_lru, &page->lru);\n\t\tWARN_ON(!ret);\n\n\t\ttrace_binder_free_lru_end(alloc, index);\n\t\tif (page_addr == start)\n\t\t\tbreak;\n\t\tcontinue;\n\nerr_vm_insert_page_failed:\n\t\t__free_page(page->page_ptr);\n\t\tpage->page_ptr = NULL;\nerr_alloc_page_failed:\nerr_page_ptr_cleared:\n\t\tif (page_addr == start)\n\t\t\tbreak;\n\t}\nerr_no_vma:\n\tif (mm) {\n\t\tmmap_read_unlock(mm);\n\t\tmmput(mm);\n\t}\n\treturn vma ? -ENOMEM : -ESRCH;\n}",
        "patch": "--- code before\n+++ code after\n@@ -33,7 +33,7 @@\n \n \tif (mm) {\n \t\tmmap_read_lock(mm);\n-\t\tvma = alloc->vma;\n+\t\tvma = vma_lookup(mm, alloc->vma_addr);\n \t}\n \n \tif (!vma && need_mm) {",
        "function_modified_lines": {
            "added": [
                "\t\tvma = vma_lookup(mm, alloc->vma_addr);"
            ],
            "deleted": [
                "\t\tvma = alloc->vma;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "In binder_vma_close of binder.c, there is a possible use after free due to improper locking. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-254837884References: Upstream kernel",
        "id": 3909
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static void io_req_clean_work(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\treturn;\n\n\tif (req->flags & REQ_F_INFLIGHT) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\tstruct io_uring_task *tctx = req->task->io_uring;\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\t\tlist_del(&req->inflight_entry);\n\t\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\t\treq->flags &= ~REQ_F_INFLIGHT;\n\t\tif (atomic_read(&tctx->in_idle))\n\t\t\twake_up(&tctx->wait);\n\t}\n\n\treq->flags &= ~REQ_F_WORK_INITIALIZED;\n\tio_put_identity(req->task->io_uring, req);\n}",
        "code_after_change": "static void io_req_clean_work(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\treturn;\n\n\tif (req->work.creds) {\n\t\tput_cred(req->work.creds);\n\t\treq->work.creds = NULL;\n\t}\n\tif (req->flags & REQ_F_INFLIGHT) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\tstruct io_uring_task *tctx = req->task->io_uring;\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\t\tlist_del(&req->inflight_entry);\n\t\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\t\treq->flags &= ~REQ_F_INFLIGHT;\n\t\tif (atomic_read(&tctx->in_idle))\n\t\t\twake_up(&tctx->wait);\n\t}\n\n\treq->flags &= ~REQ_F_WORK_INITIALIZED;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,10 @@\n \tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n \t\treturn;\n \n+\tif (req->work.creds) {\n+\t\tput_cred(req->work.creds);\n+\t\treq->work.creds = NULL;\n+\t}\n \tif (req->flags & REQ_F_INFLIGHT) {\n \t\tstruct io_ring_ctx *ctx = req->ctx;\n \t\tstruct io_uring_task *tctx = req->task->io_uring;\n@@ -17,5 +21,4 @@\n \t}\n \n \treq->flags &= ~REQ_F_WORK_INITIALIZED;\n-\tio_put_identity(req->task->io_uring, req);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (req->work.creds) {",
                "\t\tput_cred(req->work.creds);",
                "\t\treq->work.creds = NULL;",
                "\t}"
            ],
            "deleted": [
                "\tio_put_identity(req->task->io_uring, req);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3364
    },
    {
        "cve_id": "CVE-2022-2977",
        "code_before_change": "void tpm_chip_unregister(struct tpm_chip *chip)\n{\n\ttpm_del_legacy_sysfs(chip);\n\tif (IS_ENABLED(CONFIG_HW_RANDOM_TPM) && !tpm_is_firmware_upgrade(chip))\n\t\thwrng_unregister(&chip->hwrng);\n\ttpm_bios_log_teardown(chip);\n\tif (chip->flags & TPM_CHIP_FLAG_TPM2 && !tpm_is_firmware_upgrade(chip))\n\t\tcdev_device_del(&chip->cdevs, &chip->devs);\n\ttpm_del_char_device(chip);\n}",
        "code_after_change": "void tpm_chip_unregister(struct tpm_chip *chip)\n{\n\ttpm_del_legacy_sysfs(chip);\n\tif (IS_ENABLED(CONFIG_HW_RANDOM_TPM) && !tpm_is_firmware_upgrade(chip))\n\t\thwrng_unregister(&chip->hwrng);\n\ttpm_bios_log_teardown(chip);\n\tif (chip->flags & TPM_CHIP_FLAG_TPM2 && !tpm_is_firmware_upgrade(chip))\n\t\ttpm_devs_remove(chip);\n\ttpm_del_char_device(chip);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,6 @@\n \t\thwrng_unregister(&chip->hwrng);\n \ttpm_bios_log_teardown(chip);\n \tif (chip->flags & TPM_CHIP_FLAG_TPM2 && !tpm_is_firmware_upgrade(chip))\n-\t\tcdev_device_del(&chip->cdevs, &chip->devs);\n+\t\ttpm_devs_remove(chip);\n \ttpm_del_char_device(chip);\n }",
        "function_modified_lines": {
            "added": [
                "\t\ttpm_devs_remove(chip);"
            ],
            "deleted": [
                "\t\tcdev_device_del(&chip->cdevs, &chip->devs);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux kernel implementation of proxied virtualized TPM devices. On a system where virtualized TPM devices are configured (this is not the default) a local attacker can create a use-after-free and create a situation where it may be possible to escalate privileges on the system.",
        "id": 3526
    },
    {
        "cve_id": "CVE-2023-0461",
        "code_before_change": "int inet_csk_listen_start(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err;\n\n\treqsk_queue_alloc(&icsk->icsk_accept_queue);\n\n\tsk->sk_ack_backlog = 0;\n\tinet_csk_delack_init(sk);\n\n\tif (sk->sk_txrehash == SOCK_TXREHASH_DEFAULT)\n\t\tsk->sk_txrehash = READ_ONCE(sock_net(sk)->core.sysctl_txrehash);\n\n\t/* There is race window here: we announce ourselves listening,\n\t * but this transition is still not validated by get_port().\n\t * It is OK, because this socket enters to hash table only\n\t * after validation is complete.\n\t */\n\tinet_sk_state_store(sk, TCP_LISTEN);\n\terr = sk->sk_prot->get_port(sk, inet->inet_num);\n\tif (!err) {\n\t\tinet->inet_sport = htons(inet->inet_num);\n\n\t\tsk_dst_reset(sk);\n\t\terr = sk->sk_prot->hash(sk);\n\n\t\tif (likely(!err))\n\t\t\treturn 0;\n\t}\n\n\tinet_sk_set_state(sk, TCP_CLOSE);\n\treturn err;\n}",
        "code_after_change": "int inet_csk_listen_start(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err;\n\n\terr = inet_ulp_can_listen(sk);\n\tif (unlikely(err))\n\t\treturn err;\n\n\treqsk_queue_alloc(&icsk->icsk_accept_queue);\n\n\tsk->sk_ack_backlog = 0;\n\tinet_csk_delack_init(sk);\n\n\tif (sk->sk_txrehash == SOCK_TXREHASH_DEFAULT)\n\t\tsk->sk_txrehash = READ_ONCE(sock_net(sk)->core.sysctl_txrehash);\n\n\t/* There is race window here: we announce ourselves listening,\n\t * but this transition is still not validated by get_port().\n\t * It is OK, because this socket enters to hash table only\n\t * after validation is complete.\n\t */\n\tinet_sk_state_store(sk, TCP_LISTEN);\n\terr = sk->sk_prot->get_port(sk, inet->inet_num);\n\tif (!err) {\n\t\tinet->inet_sport = htons(inet->inet_num);\n\n\t\tsk_dst_reset(sk);\n\t\terr = sk->sk_prot->hash(sk);\n\n\t\tif (likely(!err))\n\t\t\treturn 0;\n\t}\n\n\tinet_sk_set_state(sk, TCP_CLOSE);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,10 @@\n \tstruct inet_connection_sock *icsk = inet_csk(sk);\n \tstruct inet_sock *inet = inet_sk(sk);\n \tint err;\n+\n+\terr = inet_ulp_can_listen(sk);\n+\tif (unlikely(err))\n+\t\treturn err;\n \n \treqsk_queue_alloc(&icsk->icsk_accept_queue);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\terr = inet_ulp_can_listen(sk);",
                "\tif (unlikely(err))",
                "\t\treturn err;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "There is a use-after-free vulnerability in the Linux Kernel which can be exploited to achieve local privilege escalation. To reach the vulnerability kernel configuration flag CONFIG_TLS or CONFIG_XFRM_ESPINTCP has to be configured, but the operation does not require any privilege.\n\nThere is a use-after-free bug of icsk_ulp_data of a struct inet_connection_sock.\n\nWhen CONFIG_TLS is enabled, user can install a tls context (struct tls_context) on a connected tcp socket. The context is not cleared if this socket is disconnected and reused as a listener. If a new socket is created from the listener, the context is inherited and vulnerable.\n\nThe setsockopt TCP_ULP operation does not require any privilege.\n\nWe recommend upgrading past commit 2c02d41d71f90a5168391b6a5f2954112ba2307c",
        "id": 3828
    },
    {
        "cve_id": "CVE-2023-3610",
        "code_before_change": "static int nft_chain_add(struct nft_table *table, struct nft_chain *chain)\n{\n\tint err;\n\n\terr = rhltable_insert_key(&table->chains_ht, chain->name,\n\t\t\t\t  &chain->rhlhead, nft_chain_ht_params);\n\tif (err)\n\t\treturn err;\n\n\tlist_add_tail_rcu(&chain->list, &table->chains);\n\n\treturn 0;\n}",
        "code_after_change": "int nft_chain_add(struct nft_table *table, struct nft_chain *chain)\n{\n\tint err;\n\n\terr = rhltable_insert_key(&table->chains_ht, chain->name,\n\t\t\t\t  &chain->rhlhead, nft_chain_ht_params);\n\tif (err)\n\t\treturn err;\n\n\tlist_add_tail_rcu(&chain->list, &table->chains);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-static int nft_chain_add(struct nft_table *table, struct nft_chain *chain)\n+int nft_chain_add(struct nft_table *table, struct nft_chain *chain)\n {\n \tint err;\n ",
        "function_modified_lines": {
            "added": [
                "int nft_chain_add(struct nft_table *table, struct nft_chain *chain)"
            ],
            "deleted": [
                "static int nft_chain_add(struct nft_table *table, struct nft_chain *chain)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's netfilter: nf_tables component can be exploited to achieve local privilege escalation.\n\nFlaw in the error handling of bound chains causes a use-after-free in the abort path of NFT_MSG_NEWRULE. The vulnerability requires CAP_NET_ADMIN to be triggered.\n\nWe recommend upgrading past commit 4bedf9eee016286c835e3d8fa981ddece5338795.\n\n",
        "id": 4126
    },
    {
        "cve_id": "CVE-2016-4805",
        "code_before_change": "void\nppp_unregister_channel(struct ppp_channel *chan)\n{\n\tstruct channel *pch = chan->ppp;\n\tstruct ppp_net *pn;\n\n\tif (!pch)\n\t\treturn;\t\t/* should never happen */\n\n\tchan->ppp = NULL;\n\n\t/*\n\t * This ensures that we have returned from any calls into the\n\t * the channel's start_xmit or ioctl routine before we proceed.\n\t */\n\tdown_write(&pch->chan_sem);\n\tspin_lock_bh(&pch->downl);\n\tpch->chan = NULL;\n\tspin_unlock_bh(&pch->downl);\n\tup_write(&pch->chan_sem);\n\tppp_disconnect_channel(pch);\n\n\tpn = ppp_pernet(pch->chan_net);\n\tspin_lock_bh(&pn->all_channels_lock);\n\tlist_del(&pch->list);\n\tspin_unlock_bh(&pn->all_channels_lock);\n\n\tpch->file.dead = 1;\n\twake_up_interruptible(&pch->file.rwait);\n\tif (atomic_dec_and_test(&pch->file.refcnt))\n\t\tppp_destroy_channel(pch);\n}",
        "code_after_change": "void\nppp_unregister_channel(struct ppp_channel *chan)\n{\n\tstruct channel *pch = chan->ppp;\n\tstruct ppp_net *pn;\n\n\tif (!pch)\n\t\treturn;\t\t/* should never happen */\n\n\tchan->ppp = NULL;\n\n\t/*\n\t * This ensures that we have returned from any calls into the\n\t * the channel's start_xmit or ioctl routine before we proceed.\n\t */\n\tdown_write(&pch->chan_sem);\n\tspin_lock_bh(&pch->downl);\n\tpch->chan = NULL;\n\tspin_unlock_bh(&pch->downl);\n\tup_write(&pch->chan_sem);\n\tppp_disconnect_channel(pch);\n\n\tpn = ppp_pernet(pch->chan_net);\n\tspin_lock_bh(&pn->all_channels_lock);\n\tlist_del(&pch->list);\n\tspin_unlock_bh(&pn->all_channels_lock);\n\tput_net(pch->chan_net);\n\tpch->chan_net = NULL;\n\n\tpch->file.dead = 1;\n\twake_up_interruptible(&pch->file.rwait);\n\tif (atomic_dec_and_test(&pch->file.refcnt))\n\t\tppp_destroy_channel(pch);\n}",
        "patch": "--- code before\n+++ code after\n@@ -24,6 +24,8 @@\n \tspin_lock_bh(&pn->all_channels_lock);\n \tlist_del(&pch->list);\n \tspin_unlock_bh(&pn->all_channels_lock);\n+\tput_net(pch->chan_net);\n+\tpch->chan_net = NULL;\n \n \tpch->file.dead = 1;\n \twake_up_interruptible(&pch->file.rwait);",
        "function_modified_lines": {
            "added": [
                "\tput_net(pch->chan_net);",
                "\tpch->chan_net = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Use-after-free vulnerability in drivers/net/ppp/ppp_generic.c in the Linux kernel before 4.5.2 allows local users to cause a denial of service (memory corruption and system crash, or spinlock) or possibly have unspecified other impact by removing a network namespace, related to the ppp_register_net_channel and ppp_unregister_channel functions.",
        "id": 1035
    },
    {
        "cve_id": "CVE-2020-27784",
        "code_before_change": "static struct usb_function *gprinter_alloc(struct usb_function_instance *fi)\n{\n\tstruct printer_dev\t*dev;\n\tstruct f_printer_opts\t*opts;\n\n\topts = container_of(fi, struct f_printer_opts, func_inst);\n\n\tmutex_lock(&opts->lock);\n\tif (opts->minor >= minors) {\n\t\tmutex_unlock(&opts->lock);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev) {\n\t\tmutex_unlock(&opts->lock);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t++opts->refcnt;\n\tdev->minor = opts->minor;\n\tdev->pnp_string = opts->pnp_string;\n\tdev->q_len = opts->q_len;\n\tmutex_unlock(&opts->lock);\n\n\tdev->function.name = \"printer\";\n\tdev->function.bind = printer_func_bind;\n\tdev->function.setup = printer_func_setup;\n\tdev->function.unbind = printer_func_unbind;\n\tdev->function.set_alt = printer_func_set_alt;\n\tdev->function.disable = printer_func_disable;\n\tdev->function.req_match = gprinter_req_match;\n\tdev->function.free_func = gprinter_free;\n\n\tINIT_LIST_HEAD(&dev->tx_reqs);\n\tINIT_LIST_HEAD(&dev->rx_reqs);\n\tINIT_LIST_HEAD(&dev->rx_buffers);\n\tINIT_LIST_HEAD(&dev->tx_reqs_active);\n\tINIT_LIST_HEAD(&dev->rx_reqs_active);\n\n\tspin_lock_init(&dev->lock);\n\tmutex_init(&dev->lock_printer_io);\n\tinit_waitqueue_head(&dev->rx_wait);\n\tinit_waitqueue_head(&dev->tx_wait);\n\tinit_waitqueue_head(&dev->tx_flush_wait);\n\n\tdev->interface = -1;\n\tdev->printer_cdev_open = 0;\n\tdev->printer_status = PRINTER_NOT_ERROR;\n\tdev->current_rx_req = NULL;\n\tdev->current_rx_bytes = 0;\n\tdev->current_rx_buf = NULL;\n\n\treturn &dev->function;\n}",
        "code_after_change": "static struct usb_function *gprinter_alloc(struct usb_function_instance *fi)\n{\n\tstruct printer_dev\t*dev;\n\tstruct f_printer_opts\t*opts;\n\n\topts = container_of(fi, struct f_printer_opts, func_inst);\n\n\tmutex_lock(&opts->lock);\n\tif (opts->minor >= minors) {\n\t\tmutex_unlock(&opts->lock);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev) {\n\t\tmutex_unlock(&opts->lock);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tkref_init(&dev->kref);\n\t++opts->refcnt;\n\tdev->minor = opts->minor;\n\tdev->pnp_string = opts->pnp_string;\n\tdev->q_len = opts->q_len;\n\tmutex_unlock(&opts->lock);\n\n\tdev->function.name = \"printer\";\n\tdev->function.bind = printer_func_bind;\n\tdev->function.setup = printer_func_setup;\n\tdev->function.unbind = printer_func_unbind;\n\tdev->function.set_alt = printer_func_set_alt;\n\tdev->function.disable = printer_func_disable;\n\tdev->function.req_match = gprinter_req_match;\n\tdev->function.free_func = gprinter_free;\n\n\tINIT_LIST_HEAD(&dev->tx_reqs);\n\tINIT_LIST_HEAD(&dev->rx_reqs);\n\tINIT_LIST_HEAD(&dev->rx_buffers);\n\tINIT_LIST_HEAD(&dev->tx_reqs_active);\n\tINIT_LIST_HEAD(&dev->rx_reqs_active);\n\n\tspin_lock_init(&dev->lock);\n\tmutex_init(&dev->lock_printer_io);\n\tinit_waitqueue_head(&dev->rx_wait);\n\tinit_waitqueue_head(&dev->tx_wait);\n\tinit_waitqueue_head(&dev->tx_flush_wait);\n\n\tdev->interface = -1;\n\tdev->printer_cdev_open = 0;\n\tdev->printer_status = PRINTER_NOT_ERROR;\n\tdev->current_rx_req = NULL;\n\tdev->current_rx_bytes = 0;\n\tdev->current_rx_buf = NULL;\n\n\treturn &dev->function;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \t\treturn ERR_PTR(-ENOMEM);\n \t}\n \n+\tkref_init(&dev->kref);\n \t++opts->refcnt;\n \tdev->minor = opts->minor;\n \tdev->pnp_string = opts->pnp_string;",
        "function_modified_lines": {
            "added": [
                "\tkref_init(&dev->kref);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in the Linux kernel, where accessing a deallocated instance in printer_ioctl() printer_ioctl() tries to access of a printer_dev instance. However, use-after-free arises because it had been freed by gprinter_free().",
        "id": 2629
    },
    {
        "cve_id": "CVE-2019-19769",
        "code_before_change": "int locks_delete_block(struct file_lock *waiter)\n{\n\tint status = -ENOENT;\n\n\t/*\n\t * If fl_blocker is NULL, it won't be set again as this thread\n\t * \"owns\" the lock and is the only one that might try to claim\n\t * the lock.  So it is safe to test fl_blocker locklessly.\n\t * Also if fl_blocker is NULL, this waiter is not listed on\n\t * fl_blocked_requests for some lock, so no other request can\n\t * be added to the list of fl_blocked_requests for this\n\t * request.  So if fl_blocker is NULL, it is safe to\n\t * locklessly check if fl_blocked_requests is empty.  If both\n\t * of these checks succeed, there is no need to take the lock.\n\t */\n\tif (waiter->fl_blocker == NULL &&\n\t    list_empty(&waiter->fl_blocked_requests))\n\t\treturn status;\n\tspin_lock(&blocked_lock_lock);\n\tif (waiter->fl_blocker)\n\t\tstatus = 0;\n\t__locks_wake_up_blocks(waiter);\n\t__locks_delete_block(waiter);\n\tspin_unlock(&blocked_lock_lock);\n\treturn status;\n}",
        "code_after_change": "int locks_delete_block(struct file_lock *waiter)\n{\n\tint status = -ENOENT;\n\n\tspin_lock(&blocked_lock_lock);\n\tif (waiter->fl_blocker)\n\t\tstatus = 0;\n\t__locks_wake_up_blocks(waiter);\n\t__locks_delete_block(waiter);\n\tspin_unlock(&blocked_lock_lock);\n\treturn status;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,20 +2,6 @@\n {\n \tint status = -ENOENT;\n \n-\t/*\n-\t * If fl_blocker is NULL, it won't be set again as this thread\n-\t * \"owns\" the lock and is the only one that might try to claim\n-\t * the lock.  So it is safe to test fl_blocker locklessly.\n-\t * Also if fl_blocker is NULL, this waiter is not listed on\n-\t * fl_blocked_requests for some lock, so no other request can\n-\t * be added to the list of fl_blocked_requests for this\n-\t * request.  So if fl_blocker is NULL, it is safe to\n-\t * locklessly check if fl_blocked_requests is empty.  If both\n-\t * of these checks succeed, there is no need to take the lock.\n-\t */\n-\tif (waiter->fl_blocker == NULL &&\n-\t    list_empty(&waiter->fl_blocked_requests))\n-\t\treturn status;\n \tspin_lock(&blocked_lock_lock);\n \tif (waiter->fl_blocker)\n \t\tstatus = 0;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t/*",
                "\t * If fl_blocker is NULL, it won't be set again as this thread",
                "\t * \"owns\" the lock and is the only one that might try to claim",
                "\t * the lock.  So it is safe to test fl_blocker locklessly.",
                "\t * Also if fl_blocker is NULL, this waiter is not listed on",
                "\t * fl_blocked_requests for some lock, so no other request can",
                "\t * be added to the list of fl_blocked_requests for this",
                "\t * request.  So if fl_blocker is NULL, it is safe to",
                "\t * locklessly check if fl_blocked_requests is empty.  If both",
                "\t * of these checks succeed, there is no need to take the lock.",
                "\t */",
                "\tif (waiter->fl_blocker == NULL &&",
                "\t    list_empty(&waiter->fl_blocked_requests))",
                "\t\treturn status;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.3.10, there is a use-after-free (read) in the perf_trace_lock_acquire function (related to include/trace/events/lock.h).",
        "id": 2242
    },
    {
        "cve_id": "CVE-2023-4394",
        "code_before_change": "int btrfs_get_dev_args_from_path(struct btrfs_fs_info *fs_info,\n\t\t\t\t struct btrfs_dev_lookup_args *args,\n\t\t\t\t const char *path)\n{\n\tstruct btrfs_super_block *disk_super;\n\tstruct block_device *bdev;\n\tint ret;\n\n\tif (!path || !path[0])\n\t\treturn -EINVAL;\n\tif (!strcmp(path, \"missing\")) {\n\t\targs->missing = true;\n\t\treturn 0;\n\t}\n\n\targs->uuid = kzalloc(BTRFS_UUID_SIZE, GFP_KERNEL);\n\targs->fsid = kzalloc(BTRFS_FSID_SIZE, GFP_KERNEL);\n\tif (!args->uuid || !args->fsid) {\n\t\tbtrfs_put_dev_args_from_path(args);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = btrfs_get_bdev_and_sb(path, FMODE_READ, fs_info->bdev_holder, 0,\n\t\t\t\t    &bdev, &disk_super);\n\tif (ret)\n\t\treturn ret;\n\targs->devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tmemcpy(args->uuid, disk_super->dev_item.uuid, BTRFS_UUID_SIZE);\n\tif (btrfs_fs_incompat(fs_info, METADATA_UUID))\n\t\tmemcpy(args->fsid, disk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\telse\n\t\tmemcpy(args->fsid, disk_super->fsid, BTRFS_FSID_SIZE);\n\tbtrfs_release_disk_super(disk_super);\n\tblkdev_put(bdev, FMODE_READ);\n\treturn 0;\n}",
        "code_after_change": "int btrfs_get_dev_args_from_path(struct btrfs_fs_info *fs_info,\n\t\t\t\t struct btrfs_dev_lookup_args *args,\n\t\t\t\t const char *path)\n{\n\tstruct btrfs_super_block *disk_super;\n\tstruct block_device *bdev;\n\tint ret;\n\n\tif (!path || !path[0])\n\t\treturn -EINVAL;\n\tif (!strcmp(path, \"missing\")) {\n\t\targs->missing = true;\n\t\treturn 0;\n\t}\n\n\targs->uuid = kzalloc(BTRFS_UUID_SIZE, GFP_KERNEL);\n\targs->fsid = kzalloc(BTRFS_FSID_SIZE, GFP_KERNEL);\n\tif (!args->uuid || !args->fsid) {\n\t\tbtrfs_put_dev_args_from_path(args);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = btrfs_get_bdev_and_sb(path, FMODE_READ, fs_info->bdev_holder, 0,\n\t\t\t\t    &bdev, &disk_super);\n\tif (ret) {\n\t\tbtrfs_put_dev_args_from_path(args);\n\t\treturn ret;\n\t}\n\n\targs->devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tmemcpy(args->uuid, disk_super->dev_item.uuid, BTRFS_UUID_SIZE);\n\tif (btrfs_fs_incompat(fs_info, METADATA_UUID))\n\t\tmemcpy(args->fsid, disk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\telse\n\t\tmemcpy(args->fsid, disk_super->fsid, BTRFS_FSID_SIZE);\n\tbtrfs_release_disk_super(disk_super);\n\tblkdev_put(bdev, FMODE_READ);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,8 +22,11 @@\n \n \tret = btrfs_get_bdev_and_sb(path, FMODE_READ, fs_info->bdev_holder, 0,\n \t\t\t\t    &bdev, &disk_super);\n-\tif (ret)\n+\tif (ret) {\n+\t\tbtrfs_put_dev_args_from_path(args);\n \t\treturn ret;\n+\t}\n+\n \targs->devid = btrfs_stack_device_id(&disk_super->dev_item);\n \tmemcpy(args->uuid, disk_super->dev_item.uuid, BTRFS_UUID_SIZE);\n \tif (btrfs_fs_incompat(fs_info, METADATA_UUID))",
        "function_modified_lines": {
            "added": [
                "\tif (ret) {",
                "\t\tbtrfs_put_dev_args_from_path(args);",
                "\t}",
                ""
            ],
            "deleted": [
                "\tif (ret)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in btrfs_get_dev_args_from_path in fs/btrfs/volumes.c in btrfs file-system in the Linux Kernel. This flaw allows a local attacker with special privileges to cause a system crash or leak internal kernel information",
        "id": 4212
    },
    {
        "cve_id": "CVE-2022-42720",
        "code_before_change": "static inline void bss_ref_get(struct cfg80211_registered_device *rdev,\n\t\t\t       struct cfg80211_internal_bss *bss)\n{\n\tlockdep_assert_held(&rdev->bss_lock);\n\n\tbss->refcount++;\n\tif (bss->pub.hidden_beacon_bss) {\n\t\tbss = container_of(bss->pub.hidden_beacon_bss,\n\t\t\t\t   struct cfg80211_internal_bss,\n\t\t\t\t   pub);\n\t\tbss->refcount++;\n\t}\n\tif (bss->pub.transmitted_bss) {\n\t\tbss = container_of(bss->pub.transmitted_bss,\n\t\t\t\t   struct cfg80211_internal_bss,\n\t\t\t\t   pub);\n\t\tbss->refcount++;\n\t}\n}",
        "code_after_change": "static inline void bss_ref_get(struct cfg80211_registered_device *rdev,\n\t\t\t       struct cfg80211_internal_bss *bss)\n{\n\tlockdep_assert_held(&rdev->bss_lock);\n\n\tbss->refcount++;\n\n\tif (bss->pub.hidden_beacon_bss)\n\t\tbss_from_pub(bss->pub.hidden_beacon_bss)->refcount++;\n\n\tif (bss->pub.transmitted_bss)\n\t\tbss_from_pub(bss->pub.transmitted_bss)->refcount++;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,16 +4,10 @@\n \tlockdep_assert_held(&rdev->bss_lock);\n \n \tbss->refcount++;\n-\tif (bss->pub.hidden_beacon_bss) {\n-\t\tbss = container_of(bss->pub.hidden_beacon_bss,\n-\t\t\t\t   struct cfg80211_internal_bss,\n-\t\t\t\t   pub);\n-\t\tbss->refcount++;\n-\t}\n-\tif (bss->pub.transmitted_bss) {\n-\t\tbss = container_of(bss->pub.transmitted_bss,\n-\t\t\t\t   struct cfg80211_internal_bss,\n-\t\t\t\t   pub);\n-\t\tbss->refcount++;\n-\t}\n+\n+\tif (bss->pub.hidden_beacon_bss)\n+\t\tbss_from_pub(bss->pub.hidden_beacon_bss)->refcount++;\n+\n+\tif (bss->pub.transmitted_bss)\n+\t\tbss_from_pub(bss->pub.transmitted_bss)->refcount++;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (bss->pub.hidden_beacon_bss)",
                "\t\tbss_from_pub(bss->pub.hidden_beacon_bss)->refcount++;",
                "",
                "\tif (bss->pub.transmitted_bss)",
                "\t\tbss_from_pub(bss->pub.transmitted_bss)->refcount++;"
            ],
            "deleted": [
                "\tif (bss->pub.hidden_beacon_bss) {",
                "\t\tbss = container_of(bss->pub.hidden_beacon_bss,",
                "\t\t\t\t   struct cfg80211_internal_bss,",
                "\t\t\t\t   pub);",
                "\t\tbss->refcount++;",
                "\t}",
                "\tif (bss->pub.transmitted_bss) {",
                "\t\tbss = container_of(bss->pub.transmitted_bss,",
                "\t\t\t\t   struct cfg80211_internal_bss,",
                "\t\t\t\t   pub);",
                "\t\tbss->refcount++;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Various refcounting bugs in the multi-BSS handling in the mac80211 stack in the Linux kernel 5.1 through 5.19.x before 5.19.16 could be used by local attackers (able to inject WLAN frames) to trigger use-after-free conditions to potentially execute code.",
        "id": 3735
    },
    {
        "cve_id": "CVE-2023-0468",
        "code_before_change": "static int io_poll_check_events(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint v, ret;\n\n\t/* req->task == current here, checking PF_EXITING is safe */\n\tif (unlikely(req->task->flags & PF_EXITING))\n\t\treturn -ECANCELED;\n\n\tdo {\n\t\tv = atomic_read(&req->poll_refs);\n\n\t\t/* tw handler should be the owner, and so have some references */\n\t\tif (WARN_ON_ONCE(!(v & IO_POLL_REF_MASK)))\n\t\t\treturn IOU_POLL_DONE;\n\t\tif (v & IO_POLL_CANCEL_FLAG)\n\t\t\treturn -ECANCELED;\n\t\t/*\n\t\t * cqe.res contains only events of the first wake up\n\t\t * and all others are be lost. Redo vfs_poll() to get\n\t\t * up to date state.\n\t\t */\n\t\tif ((v & IO_POLL_REF_MASK) != 1)\n\t\t\treq->cqe.res = 0;\n\n\t\t/* the mask was stashed in __io_poll_execute */\n\t\tif (!req->cqe.res) {\n\t\t\tstruct poll_table_struct pt = { ._key = req->apoll_events };\n\t\t\treq->cqe.res = vfs_poll(req->file, &pt) & req->apoll_events;\n\t\t}\n\n\t\tif ((unlikely(!req->cqe.res)))\n\t\t\tcontinue;\n\t\tif (req->apoll_events & EPOLLONESHOT)\n\t\t\treturn IOU_POLL_DONE;\n\t\tif (io_is_uring_fops(req->file))\n\t\t\treturn IOU_POLL_DONE;\n\n\t\t/* multishot, just fill a CQE and proceed */\n\t\tif (!(req->flags & REQ_F_APOLL_MULTISHOT)) {\n\t\t\t__poll_t mask = mangle_poll(req->cqe.res &\n\t\t\t\t\t\t    req->apoll_events);\n\n\t\t\tif (!io_post_aux_cqe(ctx, req->cqe.user_data,\n\t\t\t\t\t     mask, IORING_CQE_F_MORE, false)) {\n\t\t\t\tio_req_set_res(req, mask, 0);\n\t\t\t\treturn IOU_POLL_REMOVE_POLL_USE_RES;\n\t\t\t}\n\t\t} else {\n\t\t\tret = io_poll_issue(req, locked);\n\t\t\tif (ret == IOU_STOP_MULTISHOT)\n\t\t\t\treturn IOU_POLL_REMOVE_POLL_USE_RES;\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\t/* force the next iteration to vfs_poll() */\n\t\treq->cqe.res = 0;\n\n\t\t/*\n\t\t * Release all references, retry if someone tried to restart\n\t\t * task_work while we were executing it.\n\t\t */\n\t} while (atomic_sub_return(v & IO_POLL_REF_MASK, &req->poll_refs));\n\n\treturn IOU_POLL_NO_ACTION;\n}",
        "code_after_change": "static int io_poll_check_events(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint v, ret;\n\n\t/* req->task == current here, checking PF_EXITING is safe */\n\tif (unlikely(req->task->flags & PF_EXITING))\n\t\treturn -ECANCELED;\n\n\tdo {\n\t\tv = atomic_read(&req->poll_refs);\n\n\t\t/* tw handler should be the owner, and so have some references */\n\t\tif (WARN_ON_ONCE(!(v & IO_POLL_REF_MASK)))\n\t\t\treturn IOU_POLL_DONE;\n\t\tif (v & IO_POLL_CANCEL_FLAG)\n\t\t\treturn -ECANCELED;\n\t\t/*\n\t\t * cqe.res contains only events of the first wake up\n\t\t * and all others are be lost. Redo vfs_poll() to get\n\t\t * up to date state.\n\t\t */\n\t\tif ((v & IO_POLL_REF_MASK) != 1)\n\t\t\treq->cqe.res = 0;\n\t\tif (v & IO_POLL_RETRY_FLAG) {\n\t\t\treq->cqe.res = 0;\n\t\t\t/*\n\t\t\t * We won't find new events that came in between\n\t\t\t * vfs_poll and the ref put unless we clear the flag\n\t\t\t * in advance.\n\t\t\t */\n\t\t\tatomic_andnot(IO_POLL_RETRY_FLAG, &req->poll_refs);\n\t\t\tv &= ~IO_POLL_RETRY_FLAG;\n\t\t}\n\n\t\t/* the mask was stashed in __io_poll_execute */\n\t\tif (!req->cqe.res) {\n\t\t\tstruct poll_table_struct pt = { ._key = req->apoll_events };\n\t\t\treq->cqe.res = vfs_poll(req->file, &pt) & req->apoll_events;\n\t\t}\n\n\t\tif ((unlikely(!req->cqe.res)))\n\t\t\tcontinue;\n\t\tif (req->apoll_events & EPOLLONESHOT)\n\t\t\treturn IOU_POLL_DONE;\n\t\tif (io_is_uring_fops(req->file))\n\t\t\treturn IOU_POLL_DONE;\n\n\t\t/* multishot, just fill a CQE and proceed */\n\t\tif (!(req->flags & REQ_F_APOLL_MULTISHOT)) {\n\t\t\t__poll_t mask = mangle_poll(req->cqe.res &\n\t\t\t\t\t\t    req->apoll_events);\n\n\t\t\tif (!io_post_aux_cqe(ctx, req->cqe.user_data,\n\t\t\t\t\t     mask, IORING_CQE_F_MORE, false)) {\n\t\t\t\tio_req_set_res(req, mask, 0);\n\t\t\t\treturn IOU_POLL_REMOVE_POLL_USE_RES;\n\t\t\t}\n\t\t} else {\n\t\t\tret = io_poll_issue(req, locked);\n\t\t\tif (ret == IOU_STOP_MULTISHOT)\n\t\t\t\treturn IOU_POLL_REMOVE_POLL_USE_RES;\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\t/* force the next iteration to vfs_poll() */\n\t\treq->cqe.res = 0;\n\n\t\t/*\n\t\t * Release all references, retry if someone tried to restart\n\t\t * task_work while we were executing it.\n\t\t */\n\t} while (atomic_sub_return(v & IO_POLL_REF_MASK, &req->poll_refs));\n\n\treturn IOU_POLL_NO_ACTION;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,6 +22,16 @@\n \t\t */\n \t\tif ((v & IO_POLL_REF_MASK) != 1)\n \t\t\treq->cqe.res = 0;\n+\t\tif (v & IO_POLL_RETRY_FLAG) {\n+\t\t\treq->cqe.res = 0;\n+\t\t\t/*\n+\t\t\t * We won't find new events that came in between\n+\t\t\t * vfs_poll and the ref put unless we clear the flag\n+\t\t\t * in advance.\n+\t\t\t */\n+\t\t\tatomic_andnot(IO_POLL_RETRY_FLAG, &req->poll_refs);\n+\t\t\tv &= ~IO_POLL_RETRY_FLAG;\n+\t\t}\n \n \t\t/* the mask was stashed in __io_poll_execute */\n \t\tif (!req->cqe.res) {",
        "function_modified_lines": {
            "added": [
                "\t\tif (v & IO_POLL_RETRY_FLAG) {",
                "\t\t\treq->cqe.res = 0;",
                "\t\t\t/*",
                "\t\t\t * We won't find new events that came in between",
                "\t\t\t * vfs_poll and the ref put unless we clear the flag",
                "\t\t\t * in advance.",
                "\t\t\t */",
                "\t\t\tatomic_andnot(IO_POLL_RETRY_FLAG, &req->poll_refs);",
                "\t\t\tv &= ~IO_POLL_RETRY_FLAG;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in io_uring/poll.c in io_poll_check_events in the io_uring subcomponent in the Linux Kernel due to a race condition of poll_refs. This flaw may cause a NULL pointer dereference.",
        "id": 3830
    },
    {
        "cve_id": "CVE-2022-1973",
        "code_before_change": "int log_replay(struct ntfs_inode *ni, bool *initialized)\n{\n\tint err;\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tstruct ntfs_log *log;\n\n\tstruct restart_info rst_info, rst_info2;\n\tu64 rec_lsn, ra_lsn, checkpt_lsn = 0, rlsn = 0;\n\tstruct ATTR_NAME_ENTRY *attr_names = NULL;\n\tstruct ATTR_NAME_ENTRY *ane;\n\tstruct RESTART_TABLE *dptbl = NULL;\n\tstruct RESTART_TABLE *trtbl = NULL;\n\tconst struct RESTART_TABLE *rt;\n\tstruct RESTART_TABLE *oatbl = NULL;\n\tstruct inode *inode;\n\tstruct OpenAttr *oa;\n\tstruct ntfs_inode *ni_oe;\n\tstruct ATTRIB *attr = NULL;\n\tu64 size, vcn, undo_next_lsn;\n\tCLST rno, lcn, lcn0, len0, clen;\n\tvoid *data;\n\tstruct NTFS_RESTART *rst = NULL;\n\tstruct lcb *lcb = NULL;\n\tstruct OPEN_ATTR_ENRTY *oe;\n\tstruct TRANSACTION_ENTRY *tr;\n\tstruct DIR_PAGE_ENTRY *dp;\n\tu32 i, bytes_per_attr_entry;\n\tu32 l_size = ni->vfs_inode.i_size;\n\tu32 orig_file_size = l_size;\n\tu32 page_size, vbo, tail, off, dlen;\n\tu32 saved_len, rec_len, transact_id;\n\tbool use_second_page;\n\tstruct RESTART_AREA *ra2, *ra = NULL;\n\tstruct CLIENT_REC *ca, *cr;\n\t__le16 client;\n\tstruct RESTART_HDR *rh;\n\tconst struct LFS_RECORD_HDR *frh;\n\tconst struct LOG_REC_HDR *lrh;\n\tbool is_mapped;\n\tbool is_ro = sb_rdonly(sbi->sb);\n\tu64 t64;\n\tu16 t16;\n\tu32 t32;\n\n\t/* Get the size of page. NOTE: To replay we can use default page. */\n#if PAGE_SIZE >= DefaultLogPageSize && PAGE_SIZE <= DefaultLogPageSize * 2\n\tpage_size = norm_file_page(PAGE_SIZE, &l_size, true);\n#else\n\tpage_size = norm_file_page(PAGE_SIZE, &l_size, false);\n#endif\n\tif (!page_size)\n\t\treturn -EINVAL;\n\n\tlog = kzalloc(sizeof(struct ntfs_log), GFP_NOFS);\n\tif (!log)\n\t\treturn -ENOMEM;\n\n\tlog->ni = ni;\n\tlog->l_size = l_size;\n\tlog->one_page_buf = kmalloc(page_size, GFP_NOFS);\n\n\tif (!log->one_page_buf) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlog->page_size = page_size;\n\tlog->page_mask = page_size - 1;\n\tlog->page_bits = blksize_bits(page_size);\n\n\t/* Look for a restart area on the disk. */\n\terr = log_read_rst(log, l_size, true, &rst_info);\n\tif (err)\n\t\tgoto out;\n\n\t/* remember 'initialized' */\n\t*initialized = rst_info.initialized;\n\n\tif (!rst_info.restart) {\n\t\tif (rst_info.initialized) {\n\t\t\t/* No restart area but the file is not initialized. */\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tlog_init_pg_hdr(log, page_size, page_size, 1, 1);\n\t\tlog_create(log, l_size, 0, get_random_int(), false, false);\n\n\t\tlog->ra = ra;\n\n\t\tra = log_create_ra(log);\n\t\tif (!ra) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tlog->ra = ra;\n\t\tlog->init_ra = true;\n\n\t\tgoto process_log;\n\t}\n\n\t/*\n\t * If the restart offset above wasn't zero then we won't\n\t * look for a second restart.\n\t */\n\tif (rst_info.vbo)\n\t\tgoto check_restart_area;\n\n\terr = log_read_rst(log, l_size, false, &rst_info2);\n\n\t/* Determine which restart area to use. */\n\tif (!rst_info2.restart || rst_info2.last_lsn <= rst_info.last_lsn)\n\t\tgoto use_first_page;\n\n\tuse_second_page = true;\n\n\tif (rst_info.chkdsk_was_run && page_size != rst_info.vbo) {\n\t\tstruct RECORD_PAGE_HDR *sp = NULL;\n\t\tbool usa_error;\n\n\t\tif (!read_log_page(log, page_size, &sp, &usa_error) &&\n\t\t    sp->rhdr.sign == NTFS_CHKD_SIGNATURE) {\n\t\t\tuse_second_page = false;\n\t\t}\n\t\tkfree(sp);\n\t}\n\n\tif (use_second_page) {\n\t\tkfree(rst_info.r_page);\n\t\tmemcpy(&rst_info, &rst_info2, sizeof(struct restart_info));\n\t\trst_info2.r_page = NULL;\n\t}\n\nuse_first_page:\n\tkfree(rst_info2.r_page);\n\ncheck_restart_area:\n\t/*\n\t * If the restart area is at offset 0, we want\n\t * to write the second restart area first.\n\t */\n\tlog->init_ra = !!rst_info.vbo;\n\n\t/* If we have a valid page then grab a pointer to the restart area. */\n\tra2 = rst_info.valid_page\n\t\t      ? Add2Ptr(rst_info.r_page,\n\t\t\t\tle16_to_cpu(rst_info.r_page->ra_off))\n\t\t      : NULL;\n\n\tif (rst_info.chkdsk_was_run ||\n\t    (ra2 && ra2->client_idx[1] == LFS_NO_CLIENT_LE)) {\n\t\tbool wrapped = false;\n\t\tbool use_multi_page = false;\n\t\tu32 open_log_count;\n\n\t\t/* Do some checks based on whether we have a valid log page. */\n\t\tif (!rst_info.valid_page) {\n\t\t\topen_log_count = get_random_int();\n\t\t\tgoto init_log_instance;\n\t\t}\n\t\topen_log_count = le32_to_cpu(ra2->open_log_count);\n\n\t\t/*\n\t\t * If the restart page size isn't changing then we want to\n\t\t * check how much work we need to do.\n\t\t */\n\t\tif (page_size != le32_to_cpu(rst_info.r_page->sys_page_size))\n\t\t\tgoto init_log_instance;\n\ninit_log_instance:\n\t\tlog_init_pg_hdr(log, page_size, page_size, 1, 1);\n\n\t\tlog_create(log, l_size, rst_info.last_lsn, open_log_count,\n\t\t\t   wrapped, use_multi_page);\n\n\t\tra = log_create_ra(log);\n\t\tif (!ra) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tlog->ra = ra;\n\n\t\t/* Put the restart areas and initialize\n\t\t * the log file as required.\n\t\t */\n\t\tgoto process_log;\n\t}\n\n\tif (!ra2) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If the log page or the system page sizes have changed, we can't\n\t * use the log file. We must use the system page size instead of the\n\t * default size if there is not a clean shutdown.\n\t */\n\tt32 = le32_to_cpu(rst_info.r_page->sys_page_size);\n\tif (page_size != t32) {\n\t\tl_size = orig_file_size;\n\t\tpage_size =\n\t\t\tnorm_file_page(t32, &l_size, t32 == DefaultLogPageSize);\n\t}\n\n\tif (page_size != t32 ||\n\t    page_size != le32_to_cpu(rst_info.r_page->page_size)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* If the file size has shrunk then we won't mount it. */\n\tif (l_size < le64_to_cpu(ra2->l_size)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tlog_init_pg_hdr(log, page_size, page_size,\n\t\t\tle16_to_cpu(rst_info.r_page->major_ver),\n\t\t\tle16_to_cpu(rst_info.r_page->minor_ver));\n\n\tlog->l_size = le64_to_cpu(ra2->l_size);\n\tlog->seq_num_bits = le32_to_cpu(ra2->seq_num_bits);\n\tlog->file_data_bits = sizeof(u64) * 8 - log->seq_num_bits;\n\tlog->seq_num_mask = (8 << log->file_data_bits) - 1;\n\tlog->last_lsn = le64_to_cpu(ra2->current_lsn);\n\tlog->seq_num = log->last_lsn >> log->file_data_bits;\n\tlog->ra_off = le16_to_cpu(rst_info.r_page->ra_off);\n\tlog->restart_size = log->sys_page_size - log->ra_off;\n\tlog->record_header_len = le16_to_cpu(ra2->rec_hdr_len);\n\tlog->ra_size = le16_to_cpu(ra2->ra_len);\n\tlog->data_off = le16_to_cpu(ra2->data_off);\n\tlog->data_size = log->page_size - log->data_off;\n\tlog->reserved = log->data_size - log->record_header_len;\n\n\tvbo = lsn_to_vbo(log, log->last_lsn);\n\n\tif (vbo < log->first_page) {\n\t\t/* This is a pseudo lsn. */\n\t\tlog->l_flags |= NTFSLOG_NO_LAST_LSN;\n\t\tlog->next_page = log->first_page;\n\t\tgoto find_oldest;\n\t}\n\n\t/* Find the end of this log record. */\n\toff = final_log_off(log, log->last_lsn,\n\t\t\t    le32_to_cpu(ra2->last_lsn_data_len));\n\n\t/* If we wrapped the file then increment the sequence number. */\n\tif (off <= vbo) {\n\t\tlog->seq_num += 1;\n\t\tlog->l_flags |= NTFSLOG_WRAPPED;\n\t}\n\n\t/* Now compute the next log page to use. */\n\tvbo &= ~log->sys_page_mask;\n\ttail = log->page_size - (off & log->page_mask) - 1;\n\n\t/*\n\t *If we can fit another log record on the page,\n\t * move back a page the log file.\n\t */\n\tif (tail >= log->record_header_len) {\n\t\tlog->l_flags |= NTFSLOG_REUSE_TAIL;\n\t\tlog->next_page = vbo;\n\t} else {\n\t\tlog->next_page = next_page_off(log, vbo);\n\t}\n\nfind_oldest:\n\t/*\n\t * Find the oldest client lsn. Use the last\n\t * flushed lsn as a starting point.\n\t */\n\tlog->oldest_lsn = log->last_lsn;\n\toldest_client_lsn(Add2Ptr(ra2, le16_to_cpu(ra2->client_off)),\n\t\t\t  ra2->client_idx[1], &log->oldest_lsn);\n\tlog->oldest_lsn_off = lsn_to_vbo(log, log->oldest_lsn);\n\n\tif (log->oldest_lsn_off < log->first_page)\n\t\tlog->l_flags |= NTFSLOG_NO_OLDEST_LSN;\n\n\tif (!(ra2->flags & RESTART_SINGLE_PAGE_IO))\n\t\tlog->l_flags |= NTFSLOG_WRAPPED | NTFSLOG_MULTIPLE_PAGE_IO;\n\n\tlog->current_openlog_count = le32_to_cpu(ra2->open_log_count);\n\tlog->total_avail_pages = log->l_size - log->first_page;\n\tlog->total_avail = log->total_avail_pages >> log->page_bits;\n\tlog->max_current_avail = log->total_avail * log->reserved;\n\tlog->total_avail = log->total_avail * log->data_size;\n\n\tlog->current_avail = current_log_avail(log);\n\n\tra = kzalloc(log->restart_size, GFP_NOFS);\n\tif (!ra) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tlog->ra = ra;\n\n\tt16 = le16_to_cpu(ra2->client_off);\n\tif (t16 == offsetof(struct RESTART_AREA, clients)) {\n\t\tmemcpy(ra, ra2, log->ra_size);\n\t} else {\n\t\tmemcpy(ra, ra2, offsetof(struct RESTART_AREA, clients));\n\t\tmemcpy(ra->clients, Add2Ptr(ra2, t16),\n\t\t       le16_to_cpu(ra2->ra_len) - t16);\n\n\t\tlog->current_openlog_count = get_random_int();\n\t\tra->open_log_count = cpu_to_le32(log->current_openlog_count);\n\t\tlog->ra_size = offsetof(struct RESTART_AREA, clients) +\n\t\t\t       sizeof(struct CLIENT_REC);\n\t\tra->client_off =\n\t\t\tcpu_to_le16(offsetof(struct RESTART_AREA, clients));\n\t\tra->ra_len = cpu_to_le16(log->ra_size);\n\t}\n\n\tle32_add_cpu(&ra->open_log_count, 1);\n\n\t/* Now we need to walk through looking for the last lsn. */\n\terr = last_log_lsn(log);\n\tif (err)\n\t\tgoto out;\n\n\tlog->current_avail = current_log_avail(log);\n\n\t/* Remember which restart area to write first. */\n\tlog->init_ra = rst_info.vbo;\n\nprocess_log:\n\t/* 1.0, 1.1, 2.0 log->major_ver/minor_ver - short values. */\n\tswitch ((log->major_ver << 16) + log->minor_ver) {\n\tcase 0x10000:\n\tcase 0x10001:\n\tcase 0x20000:\n\t\tbreak;\n\tdefault:\n\t\tntfs_warn(sbi->sb, \"\\x24LogFile version %d.%d is not supported\",\n\t\t\t  log->major_ver, log->minor_ver);\n\t\terr = -EOPNOTSUPP;\n\t\tlog->set_dirty = true;\n\t\tgoto out;\n\t}\n\n\t/* One client \"NTFS\" per logfile. */\n\tca = Add2Ptr(ra, le16_to_cpu(ra->client_off));\n\n\tfor (client = ra->client_idx[1];; client = cr->next_client) {\n\t\tif (client == LFS_NO_CLIENT_LE) {\n\t\t\t/* Insert \"NTFS\" client LogFile. */\n\t\t\tclient = ra->client_idx[0];\n\t\t\tif (client == LFS_NO_CLIENT_LE) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tt16 = le16_to_cpu(client);\n\t\t\tcr = ca + t16;\n\n\t\t\tremove_client(ca, cr, &ra->client_idx[0]);\n\n\t\t\tcr->restart_lsn = 0;\n\t\t\tcr->oldest_lsn = cpu_to_le64(log->oldest_lsn);\n\t\t\tcr->name_bytes = cpu_to_le32(8);\n\t\t\tcr->name[0] = cpu_to_le16('N');\n\t\t\tcr->name[1] = cpu_to_le16('T');\n\t\t\tcr->name[2] = cpu_to_le16('F');\n\t\t\tcr->name[3] = cpu_to_le16('S');\n\n\t\t\tadd_client(ca, t16, &ra->client_idx[1]);\n\t\t\tbreak;\n\t\t}\n\n\t\tcr = ca + le16_to_cpu(client);\n\n\t\tif (cpu_to_le32(8) == cr->name_bytes &&\n\t\t    cpu_to_le16('N') == cr->name[0] &&\n\t\t    cpu_to_le16('T') == cr->name[1] &&\n\t\t    cpu_to_le16('F') == cr->name[2] &&\n\t\t    cpu_to_le16('S') == cr->name[3])\n\t\t\tbreak;\n\t}\n\n\t/* Update the client handle with the client block information. */\n\tlog->client_id.seq_num = cr->seq_num;\n\tlog->client_id.client_idx = client;\n\n\terr = read_rst_area(log, &rst, &ra_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (!rst)\n\t\tgoto out;\n\n\tbytes_per_attr_entry = !rst->major_ver ? 0x2C : 0x28;\n\n\tcheckpt_lsn = le64_to_cpu(rst->check_point_start);\n\tif (!checkpt_lsn)\n\t\tcheckpt_lsn = ra_lsn;\n\n\t/* Allocate and Read the Transaction Table. */\n\tif (!rst->transact_table_len)\n\t\tgoto check_dirty_page_table;\n\n\tt64 = le64_to_cpu(rst->transact_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\t/* Now check that this is a valid restart table. */\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttrtbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!trtbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_dirty_page_table:\n\t/* The next record back should be the Dirty Pages Table. */\n\tif (!rst->dirty_pages_len)\n\t\tgoto check_attribute_names;\n\n\tt64 = le64_to_cpu(rst->dirty_pages_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\t/* Now check that this is a valid restart table. */\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdptbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!dptbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Convert Ra version '0' into version '1'. */\n\tif (rst->major_ver)\n\t\tgoto end_conv_1;\n\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tstruct DIR_PAGE_ENTRY_32 *dp0 = (struct DIR_PAGE_ENTRY_32 *)dp;\n\t\t// NOTE: Danger. Check for of boundary.\n\t\tmemmove(&dp->vcn, &dp0->vcn_low,\n\t\t\t2 * sizeof(u64) +\n\t\t\t\tle32_to_cpu(dp->lcns_follow) * sizeof(u64));\n\t}\n\nend_conv_1:\n\tlcb_put(lcb);\n\tlcb = NULL;\n\n\t/*\n\t * Go through the table and remove the duplicates,\n\t * remembering the oldest lsn values.\n\t */\n\tif (sbi->cluster_size <= log->page_size)\n\t\tgoto trace_dp_table;\n\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tstruct DIR_PAGE_ENTRY *next = dp;\n\n\t\twhile ((next = enum_rstbl(dptbl, next))) {\n\t\t\tif (next->target_attr == dp->target_attr &&\n\t\t\t    next->vcn == dp->vcn) {\n\t\t\t\tif (le64_to_cpu(next->oldest_lsn) <\n\t\t\t\t    le64_to_cpu(dp->oldest_lsn)) {\n\t\t\t\t\tdp->oldest_lsn = next->oldest_lsn;\n\t\t\t\t}\n\n\t\t\t\tfree_rsttbl_idx(dptbl, PtrOffset(dptbl, next));\n\t\t\t}\n\t\t}\n\t}\ntrace_dp_table:\ncheck_attribute_names:\n\t/* The next record should be the Attribute Names. */\n\tif (!rst->attr_names_len)\n\t\tgoto check_attr_table;\n\n\tt64 = le64_to_cpu(rst->attr_names_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt32 = lrh_length(lrh);\n\trec_len -= t32;\n\n\tattr_names = kmemdup(Add2Ptr(lrh, t32), rec_len, GFP_NOFS);\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_attr_table:\n\t/* The next record should be the attribute Table. */\n\tif (!rst->open_attr_len)\n\t\tgoto check_attribute_names2;\n\n\tt64 = le64_to_cpu(rst->open_attr_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toatbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!oatbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlog->open_attr_tbl = oatbl;\n\n\t/* Clear all of the Attr pointers. */\n\toe = NULL;\n\twhile ((oe = enum_rstbl(oatbl, oe))) {\n\t\tif (!rst->major_ver) {\n\t\t\tstruct OPEN_ATTR_ENRTY_32 oe0;\n\n\t\t\t/* Really 'oe' points to OPEN_ATTR_ENRTY_32. */\n\t\t\tmemcpy(&oe0, oe, SIZEOF_OPENATTRIBUTEENTRY0);\n\n\t\t\toe->bytes_per_index = oe0.bytes_per_index;\n\t\t\toe->type = oe0.type;\n\t\t\toe->is_dirty_pages = oe0.is_dirty_pages;\n\t\t\toe->name_len = 0;\n\t\t\toe->ref = oe0.ref;\n\t\t\toe->open_record_lsn = oe0.open_record_lsn;\n\t\t}\n\n\t\toe->is_attr_name = 0;\n\t\toe->ptr = NULL;\n\t}\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_attribute_names2:\n\tif (!rst->attr_names_len)\n\t\tgoto trace_attribute_table;\n\n\tane = attr_names;\n\tif (!oatbl)\n\t\tgoto trace_attribute_table;\n\twhile (ane->off) {\n\t\t/* TODO: Clear table on exit! */\n\t\toe = Add2Ptr(oatbl, le16_to_cpu(ane->off));\n\t\tt16 = le16_to_cpu(ane->name_bytes);\n\t\toe->name_len = t16 / sizeof(short);\n\t\toe->ptr = ane->name;\n\t\toe->is_attr_name = 2;\n\t\tane = Add2Ptr(ane, sizeof(struct ATTR_NAME_ENTRY) + t16);\n\t}\n\ntrace_attribute_table:\n\t/*\n\t * If the checkpt_lsn is zero, then this is a freshly\n\t * formatted disk and we have no work to do.\n\t */\n\tif (!checkpt_lsn) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\tif (!oatbl) {\n\t\toatbl = init_rsttbl(bytes_per_attr_entry, 8);\n\t\tif (!oatbl) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tlog->open_attr_tbl = oatbl;\n\n\t/* Start the analysis pass from the Checkpoint lsn. */\n\trec_lsn = checkpt_lsn;\n\n\t/* Read the first lsn. */\n\terr = read_log_rec_lcb(log, checkpt_lsn, lcb_ctx_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/* Loop to read all subsequent records to the end of the log file. */\nnext_log_record_analyze:\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (!rec_lsn)\n\t\tgoto end_log_records_enumerate;\n\n\tfrh = lcb->lrh;\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\tlrh = lcb->log_rec;\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * The first lsn after the previous lsn remembered\n\t * the checkpoint is the first candidate for the rlsn.\n\t */\n\tif (!rlsn)\n\t\trlsn = rec_lsn;\n\n\tif (LfsClientRecord != frh->record_type)\n\t\tgoto next_log_record_analyze;\n\n\t/*\n\t * Now update the Transaction Table for this transaction. If there\n\t * is no entry present or it is unallocated we allocate the entry.\n\t */\n\tif (!trtbl) {\n\t\ttrtbl = init_rsttbl(sizeof(struct TRANSACTION_ENTRY),\n\t\t\t\t    INITIAL_NUMBER_TRANSACTIONS);\n\t\tif (!trtbl) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\ttr = Add2Ptr(trtbl, transact_id);\n\n\tif (transact_id >= bytes_per_rt(trtbl) ||\n\t    tr->next != RESTART_ENTRY_ALLOCATED_LE) {\n\t\ttr = alloc_rsttbl_from_idx(&trtbl, transact_id);\n\t\tif (!tr) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\ttr->transact_state = TransactionActive;\n\t\ttr->first_lsn = cpu_to_le64(rec_lsn);\n\t}\n\n\ttr->prev_lsn = tr->undo_next_lsn = cpu_to_le64(rec_lsn);\n\n\t/*\n\t * If this is a compensation log record, then change\n\t * the undo_next_lsn to be the undo_next_lsn of this record.\n\t */\n\tif (lrh->undo_op == cpu_to_le16(CompensationLogRecord))\n\t\ttr->undo_next_lsn = frh->client_undo_next_lsn;\n\n\t/* Dispatch to handle log record depending on type. */\n\tswitch (le16_to_cpu(lrh->redo_op)) {\n\tcase InitializeFileRecordSegment:\n\tcase DeallocateFileRecordSegment:\n\tcase WriteEndOfFileRecordSegment:\n\tcase CreateAttribute:\n\tcase DeleteAttribute:\n\tcase UpdateResidentValue:\n\tcase UpdateNonresidentValue:\n\tcase UpdateMappingPairs:\n\tcase SetNewAttributeSizes:\n\tcase AddIndexEntryRoot:\n\tcase DeleteIndexEntryRoot:\n\tcase AddIndexEntryAllocation:\n\tcase DeleteIndexEntryAllocation:\n\tcase WriteEndOfIndexBuffer:\n\tcase SetIndexEntryVcnRoot:\n\tcase SetIndexEntryVcnAllocation:\n\tcase UpdateFileNameRoot:\n\tcase UpdateFileNameAllocation:\n\tcase SetBitsInNonresidentBitMap:\n\tcase ClearBitsInNonresidentBitMap:\n\tcase UpdateRecordDataRoot:\n\tcase UpdateRecordDataAllocation:\n\tcase ZeroEndOfFileRecord:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tt64 = le64_to_cpu(lrh->target_vcn);\n\t\tdp = find_dp(dptbl, t16, t64);\n\n\t\tif (dp)\n\t\t\tgoto copy_lcns;\n\n\t\t/*\n\t\t * Calculate the number of clusters per page the system\n\t\t * which wrote the checkpoint, possibly creating the table.\n\t\t */\n\t\tif (dptbl) {\n\t\t\tt32 = (le16_to_cpu(dptbl->size) -\n\t\t\t       sizeof(struct DIR_PAGE_ENTRY)) /\n\t\t\t      sizeof(u64);\n\t\t} else {\n\t\t\tt32 = log->clst_per_page;\n\t\t\tkfree(dptbl);\n\t\t\tdptbl = init_rsttbl(struct_size(dp, page_lcns, t32),\n\t\t\t\t\t    32);\n\t\t\tif (!dptbl) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tdp = alloc_rsttbl_idx(&dptbl);\n\t\tif (!dp) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tdp->target_attr = cpu_to_le32(t16);\n\t\tdp->transfer_len = cpu_to_le32(t32 << sbi->cluster_bits);\n\t\tdp->lcns_follow = cpu_to_le32(t32);\n\t\tdp->vcn = cpu_to_le64(t64 & ~((u64)t32 - 1));\n\t\tdp->oldest_lsn = cpu_to_le64(rec_lsn);\n\ncopy_lcns:\n\t\t/*\n\t\t * Copy the Lcns from the log record into the Dirty Page Entry.\n\t\t * TODO: For different page size support, must somehow make\n\t\t * whole routine a loop, case Lcns do not fit below.\n\t\t */\n\t\tt16 = le16_to_cpu(lrh->lcns_follow);\n\t\tfor (i = 0; i < t16; i++) {\n\t\t\tsize_t j = (size_t)(le64_to_cpu(lrh->target_vcn) -\n\t\t\t\t\t    le64_to_cpu(dp->vcn));\n\t\t\tdp->page_lcns[j + i] = lrh->page_lcns[i];\n\t\t}\n\n\t\tgoto next_log_record_analyze;\n\n\tcase DeleteDirtyClusters: {\n\t\tu32 range_count =\n\t\t\tle16_to_cpu(lrh->redo_len) / sizeof(struct LCN_RANGE);\n\t\tconst struct LCN_RANGE *r =\n\t\t\tAdd2Ptr(lrh, le16_to_cpu(lrh->redo_off));\n\n\t\t/* Loop through all of the Lcn ranges this log record. */\n\t\tfor (i = 0; i < range_count; i++, r++) {\n\t\t\tu64 lcn0 = le64_to_cpu(r->lcn);\n\t\t\tu64 lcn_e = lcn0 + le64_to_cpu(r->len) - 1;\n\n\t\t\tdp = NULL;\n\t\t\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\t\t\tu32 j;\n\n\t\t\t\tt32 = le32_to_cpu(dp->lcns_follow);\n\t\t\t\tfor (j = 0; j < t32; j++) {\n\t\t\t\t\tt64 = le64_to_cpu(dp->page_lcns[j]);\n\t\t\t\t\tif (t64 >= lcn0 && t64 <= lcn_e)\n\t\t\t\t\t\tdp->page_lcns[j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgoto next_log_record_analyze;\n\t\t;\n\t}\n\n\tcase OpenNonresidentAttribute:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tif (t16 >= bytes_per_rt(oatbl)) {\n\t\t\t/*\n\t\t\t * Compute how big the table needs to be.\n\t\t\t * Add 10 extra entries for some cushion.\n\t\t\t */\n\t\t\tu32 new_e = t16 / le16_to_cpu(oatbl->size);\n\n\t\t\tnew_e += 10 - le16_to_cpu(oatbl->used);\n\n\t\t\toatbl = extend_rsttbl(oatbl, new_e, ~0u);\n\t\t\tlog->open_attr_tbl = oatbl;\n\t\t\tif (!oatbl) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\t/* Point to the entry being opened. */\n\t\toe = alloc_rsttbl_from_idx(&oatbl, t16);\n\t\tlog->open_attr_tbl = oatbl;\n\t\tif (!oe) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* Initialize this entry from the log record. */\n\t\tt16 = le16_to_cpu(lrh->redo_off);\n\t\tif (!rst->major_ver) {\n\t\t\t/* Convert version '0' into version '1'. */\n\t\t\tstruct OPEN_ATTR_ENRTY_32 *oe0 = Add2Ptr(lrh, t16);\n\n\t\t\toe->bytes_per_index = oe0->bytes_per_index;\n\t\t\toe->type = oe0->type;\n\t\t\toe->is_dirty_pages = oe0->is_dirty_pages;\n\t\t\toe->name_len = 0; //oe0.name_len;\n\t\t\toe->ref = oe0->ref;\n\t\t\toe->open_record_lsn = oe0->open_record_lsn;\n\t\t} else {\n\t\t\tmemcpy(oe, Add2Ptr(lrh, t16), bytes_per_attr_entry);\n\t\t}\n\n\t\tt16 = le16_to_cpu(lrh->undo_len);\n\t\tif (t16) {\n\t\t\toe->ptr = kmalloc(t16, GFP_NOFS);\n\t\t\tif (!oe->ptr) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\toe->name_len = t16 / sizeof(short);\n\t\t\tmemcpy(oe->ptr,\n\t\t\t       Add2Ptr(lrh, le16_to_cpu(lrh->undo_off)), t16);\n\t\t\toe->is_attr_name = 1;\n\t\t} else {\n\t\t\toe->ptr = NULL;\n\t\t\toe->is_attr_name = 0;\n\t\t}\n\n\t\tgoto next_log_record_analyze;\n\n\tcase HotFix:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tt64 = le64_to_cpu(lrh->target_vcn);\n\t\tdp = find_dp(dptbl, t16, t64);\n\t\tif (dp) {\n\t\t\tsize_t j = le64_to_cpu(lrh->target_vcn) -\n\t\t\t\t   le64_to_cpu(dp->vcn);\n\t\t\tif (dp->page_lcns[j])\n\t\t\t\tdp->page_lcns[j] = lrh->page_lcns[0];\n\t\t}\n\t\tgoto next_log_record_analyze;\n\n\tcase EndTopLevelAction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->prev_lsn = cpu_to_le64(rec_lsn);\n\t\ttr->undo_next_lsn = frh->client_undo_next_lsn;\n\t\tgoto next_log_record_analyze;\n\n\tcase PrepareTransaction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->transact_state = TransactionPrepared;\n\t\tgoto next_log_record_analyze;\n\n\tcase CommitTransaction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->transact_state = TransactionCommitted;\n\t\tgoto next_log_record_analyze;\n\n\tcase ForgetTransaction:\n\t\tfree_rsttbl_idx(trtbl, transact_id);\n\t\tgoto next_log_record_analyze;\n\n\tcase Noop:\n\tcase OpenAttributeTableDump:\n\tcase AttributeNamesDump:\n\tcase DirtyPageTableDump:\n\tcase TransactionTableDump:\n\t\t/* The following cases require no action the Analysis Pass. */\n\t\tgoto next_log_record_analyze;\n\n\tdefault:\n\t\t/*\n\t\t * All codes will be explicitly handled.\n\t\t * If we see a code we do not expect, then we are trouble.\n\t\t */\n\t\tgoto next_log_record_analyze;\n\t}\n\nend_log_records_enumerate:\n\tlcb_put(lcb);\n\tlcb = NULL;\n\n\t/*\n\t * Scan the Dirty Page Table and Transaction Table for\n\t * the lowest lsn, and return it as the Redo lsn.\n\t */\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tt64 = le64_to_cpu(dp->oldest_lsn);\n\t\tif (t64 && t64 < rlsn)\n\t\t\trlsn = t64;\n\t}\n\n\ttr = NULL;\n\twhile ((tr = enum_rstbl(trtbl, tr))) {\n\t\tt64 = le64_to_cpu(tr->first_lsn);\n\t\tif (t64 && t64 < rlsn)\n\t\t\trlsn = t64;\n\t}\n\n\t/*\n\t * Only proceed if the Dirty Page Table or Transaction\n\t * table are not empty.\n\t */\n\tif ((!dptbl || !dptbl->total) && (!trtbl || !trtbl->total))\n\t\tgoto end_reply;\n\n\tsbi->flags |= NTFS_FLAGS_NEED_REPLAY;\n\tif (is_ro)\n\t\tgoto out;\n\n\t/* Reopen all of the attributes with dirty pages. */\n\toe = NULL;\nnext_open_attribute:\n\n\toe = enum_rstbl(oatbl, oe);\n\tif (!oe) {\n\t\terr = 0;\n\t\tdp = NULL;\n\t\tgoto next_dirty_page;\n\t}\n\n\toa = kzalloc(sizeof(struct OpenAttr), GFP_NOFS);\n\tif (!oa) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tinode = ntfs_iget5(sbi->sb, &oe->ref, NULL);\n\tif (IS_ERR(inode))\n\t\tgoto fake_attr;\n\n\tif (is_bad_inode(inode)) {\n\t\tiput(inode);\nfake_attr:\n\t\tif (oa->ni) {\n\t\t\tiput(&oa->ni->vfs_inode);\n\t\t\toa->ni = NULL;\n\t\t}\n\n\t\tattr = attr_create_nonres_log(sbi, oe->type, 0, oe->ptr,\n\t\t\t\t\t      oe->name_len, 0);\n\t\tif (!attr) {\n\t\t\tkfree(oa);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\toa->attr = attr;\n\t\toa->run1 = &oa->run0;\n\t\tgoto final_oe;\n\t}\n\n\tni_oe = ntfs_i(inode);\n\toa->ni = ni_oe;\n\n\tattr = ni_find_attr(ni_oe, NULL, NULL, oe->type, oe->ptr, oe->name_len,\n\t\t\t    NULL, NULL);\n\n\tif (!attr)\n\t\tgoto fake_attr;\n\n\tt32 = le32_to_cpu(attr->size);\n\toa->attr = kmemdup(attr, t32, GFP_NOFS);\n\tif (!oa->attr)\n\t\tgoto fake_attr;\n\n\tif (!S_ISDIR(inode->i_mode)) {\n\t\tif (attr->type == ATTR_DATA && !attr->name_len) {\n\t\t\toa->run1 = &ni_oe->file.run;\n\t\t\tgoto final_oe;\n\t\t}\n\t} else {\n\t\tif (attr->type == ATTR_ALLOC &&\n\t\t    attr->name_len == ARRAY_SIZE(I30_NAME) &&\n\t\t    !memcmp(attr_name(attr), I30_NAME, sizeof(I30_NAME))) {\n\t\t\toa->run1 = &ni_oe->dir.alloc_run;\n\t\t\tgoto final_oe;\n\t\t}\n\t}\n\n\tif (attr->non_res) {\n\t\tu16 roff = le16_to_cpu(attr->nres.run_off);\n\t\tCLST svcn = le64_to_cpu(attr->nres.svcn);\n\n\t\terr = run_unpack(&oa->run0, sbi, inode->i_ino, svcn,\n\t\t\t\t le64_to_cpu(attr->nres.evcn), svcn,\n\t\t\t\t Add2Ptr(attr, roff), t32 - roff);\n\t\tif (err < 0) {\n\t\t\tkfree(oa->attr);\n\t\t\toa->attr = NULL;\n\t\t\tgoto fake_attr;\n\t\t}\n\t\terr = 0;\n\t}\n\toa->run1 = &oa->run0;\n\tattr = oa->attr;\n\nfinal_oe:\n\tif (oe->is_attr_name == 1)\n\t\tkfree(oe->ptr);\n\toe->is_attr_name = 0;\n\toe->ptr = oa;\n\toe->name_len = attr->name_len;\n\n\tgoto next_open_attribute;\n\n\t/*\n\t * Now loop through the dirty page table to extract all of the Vcn/Lcn.\n\t * Mapping that we have, and insert it into the appropriate run.\n\t */\nnext_dirty_page:\n\tdp = enum_rstbl(dptbl, dp);\n\tif (!dp)\n\t\tgoto do_redo_1;\n\n\toe = Add2Ptr(oatbl, le32_to_cpu(dp->target_attr));\n\n\tif (oe->next != RESTART_ENTRY_ALLOCATED_LE)\n\t\tgoto next_dirty_page;\n\n\toa = oe->ptr;\n\tif (!oa)\n\t\tgoto next_dirty_page;\n\n\ti = -1;\nnext_dirty_page_vcn:\n\ti += 1;\n\tif (i >= le32_to_cpu(dp->lcns_follow))\n\t\tgoto next_dirty_page;\n\n\tvcn = le64_to_cpu(dp->vcn) + i;\n\tsize = (vcn + 1) << sbi->cluster_bits;\n\n\tif (!dp->page_lcns[i])\n\t\tgoto next_dirty_page_vcn;\n\n\trno = ino_get(&oe->ref);\n\tif (rno <= MFT_REC_MIRR &&\n\t    size < (MFT_REC_VOL + 1) * sbi->record_size &&\n\t    oe->type == ATTR_DATA) {\n\t\tgoto next_dirty_page_vcn;\n\t}\n\n\tlcn = le64_to_cpu(dp->page_lcns[i]);\n\n\tif ((!run_lookup_entry(oa->run1, vcn, &lcn0, &len0, NULL) ||\n\t     lcn0 != lcn) &&\n\t    !run_add_entry(oa->run1, vcn, lcn, 1, false)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tattr = oa->attr;\n\tt64 = le64_to_cpu(attr->nres.alloc_size);\n\tif (size > t64) {\n\t\tattr->nres.valid_size = attr->nres.data_size =\n\t\t\tattr->nres.alloc_size = cpu_to_le64(size);\n\t}\n\tgoto next_dirty_page_vcn;\n\ndo_redo_1:\n\t/*\n\t * Perform the Redo Pass, to restore all of the dirty pages to the same\n\t * contents that they had immediately before the crash. If the dirty\n\t * page table is empty, then we can skip the entire Redo Pass.\n\t */\n\tif (!dptbl || !dptbl->total)\n\t\tgoto do_undo_action;\n\n\trec_lsn = rlsn;\n\n\t/*\n\t * Read the record at the Redo lsn, before falling\n\t * into common code to handle each record.\n\t */\n\terr = read_log_rec_lcb(log, rlsn, lcb_ctx_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/*\n\t * Now loop to read all of our log records forwards, until\n\t * we hit the end of the file, cleaning up at the end.\n\t */\ndo_action_next:\n\tfrh = lcb->lrh;\n\n\tif (LfsClientRecord != frh->record_type)\n\t\tgoto read_next_log_do_action;\n\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\tlrh = lcb->log_rec;\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Ignore log records that do not update pages. */\n\tif (lrh->lcns_follow)\n\t\tgoto find_dirty_page;\n\n\tgoto read_next_log_do_action;\n\nfind_dirty_page:\n\tt16 = le16_to_cpu(lrh->target_attr);\n\tt64 = le64_to_cpu(lrh->target_vcn);\n\tdp = find_dp(dptbl, t16, t64);\n\n\tif (!dp)\n\t\tgoto read_next_log_do_action;\n\n\tif (rec_lsn < le64_to_cpu(dp->oldest_lsn))\n\t\tgoto read_next_log_do_action;\n\n\tt16 = le16_to_cpu(lrh->target_attr);\n\tif (t16 >= bytes_per_rt(oatbl)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toe = Add2Ptr(oatbl, t16);\n\n\tif (oe->next != RESTART_ENTRY_ALLOCATED_LE) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toa = oe->ptr;\n\n\tif (!oa) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tattr = oa->attr;\n\n\tvcn = le64_to_cpu(lrh->target_vcn);\n\n\tif (!run_lookup_entry(oa->run1, vcn, &lcn, NULL, NULL) ||\n\t    lcn == SPARSE_LCN) {\n\t\tgoto read_next_log_do_action;\n\t}\n\n\t/* Point to the Redo data and get its length. */\n\tdata = Add2Ptr(lrh, le16_to_cpu(lrh->redo_off));\n\tdlen = le16_to_cpu(lrh->redo_len);\n\n\t/* Shorten length by any Lcns which were deleted. */\n\tsaved_len = dlen;\n\n\tfor (i = le16_to_cpu(lrh->lcns_follow); i; i--) {\n\t\tsize_t j;\n\t\tu32 alen, voff;\n\n\t\tvoff = le16_to_cpu(lrh->record_off) +\n\t\t       le16_to_cpu(lrh->attr_off);\n\t\tvoff += le16_to_cpu(lrh->cluster_off) << SECTOR_SHIFT;\n\n\t\t/* If the Vcn question is allocated, we can just get out. */\n\t\tj = le64_to_cpu(lrh->target_vcn) - le64_to_cpu(dp->vcn);\n\t\tif (dp->page_lcns[j + i - 1])\n\t\t\tbreak;\n\n\t\tif (!saved_len)\n\t\t\tsaved_len = 1;\n\n\t\t/*\n\t\t * Calculate the allocated space left relative to the\n\t\t * log record Vcn, after removing this unallocated Vcn.\n\t\t */\n\t\talen = (i - 1) << sbi->cluster_bits;\n\n\t\t/*\n\t\t * If the update described this log record goes beyond\n\t\t * the allocated space, then we will have to reduce the length.\n\t\t */\n\t\tif (voff >= alen)\n\t\t\tdlen = 0;\n\t\telse if (voff + dlen > alen)\n\t\t\tdlen = alen - voff;\n\t}\n\n\t/*\n\t * If the resulting dlen from above is now zero,\n\t * we can skip this log record.\n\t */\n\tif (!dlen && saved_len)\n\t\tgoto read_next_log_do_action;\n\n\tt16 = le16_to_cpu(lrh->redo_op);\n\tif (can_skip_action(t16))\n\t\tgoto read_next_log_do_action;\n\n\t/* Apply the Redo operation a common routine. */\n\terr = do_action(log, oe, lrh, t16, data, dlen, rec_len, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\t/* Keep reading and looping back until end of file. */\nread_next_log_do_action:\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (!err && rec_lsn)\n\t\tgoto do_action_next;\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ndo_undo_action:\n\t/* Scan Transaction Table. */\n\ttr = NULL;\ntransaction_table_next:\n\ttr = enum_rstbl(trtbl, tr);\n\tif (!tr)\n\t\tgoto undo_action_done;\n\n\tif (TransactionActive != tr->transact_state || !tr->undo_next_lsn) {\n\t\tfree_rsttbl_idx(trtbl, PtrOffset(trtbl, tr));\n\t\tgoto transaction_table_next;\n\t}\n\n\tlog->transaction_id = PtrOffset(trtbl, tr);\n\tundo_next_lsn = le64_to_cpu(tr->undo_next_lsn);\n\n\t/*\n\t * We only have to do anything if the transaction has\n\t * something its undo_next_lsn field.\n\t */\n\tif (!undo_next_lsn)\n\t\tgoto commit_undo;\n\n\t/* Read the first record to be undone by this transaction. */\n\terr = read_log_rec_lcb(log, undo_next_lsn, lcb_ctx_undo_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/*\n\t * Now loop to read all of our log records forwards,\n\t * until we hit the end of the file, cleaning up at the end.\n\t */\nundo_action_next:\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (lrh->undo_op == cpu_to_le16(Noop))\n\t\tgoto read_next_log_undo_action;\n\n\toe = Add2Ptr(oatbl, le16_to_cpu(lrh->target_attr));\n\toa = oe->ptr;\n\n\tt16 = le16_to_cpu(lrh->lcns_follow);\n\tif (!t16)\n\t\tgoto add_allocated_vcns;\n\n\tis_mapped = run_lookup_entry(oa->run1, le64_to_cpu(lrh->target_vcn),\n\t\t\t\t     &lcn, &clen, NULL);\n\n\t/*\n\t * If the mapping isn't already the table or the  mapping\n\t * corresponds to a hole the mapping, we need to make sure\n\t * there is no partial page already memory.\n\t */\n\tif (is_mapped && lcn != SPARSE_LCN && clen >= t16)\n\t\tgoto add_allocated_vcns;\n\n\tvcn = le64_to_cpu(lrh->target_vcn);\n\tvcn &= ~(log->clst_per_page - 1);\n\nadd_allocated_vcns:\n\tfor (i = 0, vcn = le64_to_cpu(lrh->target_vcn),\n\t    size = (vcn + 1) << sbi->cluster_bits;\n\t     i < t16; i++, vcn += 1, size += sbi->cluster_size) {\n\t\tattr = oa->attr;\n\t\tif (!attr->non_res) {\n\t\t\tif (size > le32_to_cpu(attr->res.data_size))\n\t\t\t\tattr->res.data_size = cpu_to_le32(size);\n\t\t} else {\n\t\t\tif (size > le64_to_cpu(attr->nres.data_size))\n\t\t\t\tattr->nres.valid_size = attr->nres.data_size =\n\t\t\t\t\tattr->nres.alloc_size =\n\t\t\t\t\t\tcpu_to_le64(size);\n\t\t}\n\t}\n\n\tt16 = le16_to_cpu(lrh->undo_op);\n\tif (can_skip_action(t16))\n\t\tgoto read_next_log_undo_action;\n\n\t/* Point to the Redo data and get its length. */\n\tdata = Add2Ptr(lrh, le16_to_cpu(lrh->undo_off));\n\tdlen = le16_to_cpu(lrh->undo_len);\n\n\t/* It is time to apply the undo action. */\n\terr = do_action(log, oe, lrh, t16, data, dlen, rec_len, NULL);\n\nread_next_log_undo_action:\n\t/*\n\t * Keep reading and looping back until we have read the\n\t * last record for this transaction.\n\t */\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (rec_lsn)\n\t\tgoto undo_action_next;\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncommit_undo:\n\tfree_rsttbl_idx(trtbl, log->transaction_id);\n\n\tlog->transaction_id = 0;\n\n\tgoto transaction_table_next;\n\nundo_action_done:\n\n\tntfs_update_mftmirr(sbi, 0);\n\n\tsbi->flags &= ~NTFS_FLAGS_NEED_REPLAY;\n\nend_reply:\n\n\terr = 0;\n\tif (is_ro)\n\t\tgoto out;\n\n\trh = kzalloc(log->page_size, GFP_NOFS);\n\tif (!rh) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trh->rhdr.sign = NTFS_RSTR_SIGNATURE;\n\trh->rhdr.fix_off = cpu_to_le16(offsetof(struct RESTART_HDR, fixups));\n\tt16 = (log->page_size >> SECTOR_SHIFT) + 1;\n\trh->rhdr.fix_num = cpu_to_le16(t16);\n\trh->sys_page_size = cpu_to_le32(log->page_size);\n\trh->page_size = cpu_to_le32(log->page_size);\n\n\tt16 = ALIGN(offsetof(struct RESTART_HDR, fixups) + sizeof(short) * t16,\n\t\t    8);\n\trh->ra_off = cpu_to_le16(t16);\n\trh->minor_ver = cpu_to_le16(1); // 0x1A:\n\trh->major_ver = cpu_to_le16(1); // 0x1C:\n\n\tra2 = Add2Ptr(rh, t16);\n\tmemcpy(ra2, ra, sizeof(struct RESTART_AREA));\n\n\tra2->client_idx[0] = 0;\n\tra2->client_idx[1] = LFS_NO_CLIENT_LE;\n\tra2->flags = cpu_to_le16(2);\n\n\tle32_add_cpu(&ra2->open_log_count, 1);\n\n\tntfs_fix_pre_write(&rh->rhdr, log->page_size);\n\n\terr = ntfs_sb_write_run(sbi, &ni->file.run, 0, rh, log->page_size, 0);\n\tif (!err)\n\t\terr = ntfs_sb_write_run(sbi, &log->ni->file.run, log->page_size,\n\t\t\t\t\trh, log->page_size, 0);\n\n\tkfree(rh);\n\tif (err)\n\t\tgoto out;\n\nout:\n\tkfree(rst);\n\tif (lcb)\n\t\tlcb_put(lcb);\n\n\t/*\n\t * Scan the Open Attribute Table to close all of\n\t * the open attributes.\n\t */\n\toe = NULL;\n\twhile ((oe = enum_rstbl(oatbl, oe))) {\n\t\trno = ino_get(&oe->ref);\n\n\t\tif (oe->is_attr_name == 1) {\n\t\t\tkfree(oe->ptr);\n\t\t\toe->ptr = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (oe->is_attr_name)\n\t\t\tcontinue;\n\n\t\toa = oe->ptr;\n\t\tif (!oa)\n\t\t\tcontinue;\n\n\t\trun_close(&oa->run0);\n\t\tkfree(oa->attr);\n\t\tif (oa->ni)\n\t\t\tiput(&oa->ni->vfs_inode);\n\t\tkfree(oa);\n\t}\n\n\tkfree(trtbl);\n\tkfree(oatbl);\n\tkfree(dptbl);\n\tkfree(attr_names);\n\tkfree(rst_info.r_page);\n\n\tkfree(ra);\n\tkfree(log->one_page_buf);\n\n\tif (err)\n\t\tsbi->flags |= NTFS_FLAGS_NEED_REPLAY;\n\n\tif (err == -EROFS)\n\t\terr = 0;\n\telse if (log->set_dirty)\n\t\tntfs_set_state(sbi, NTFS_DIRTY_ERROR);\n\n\tkfree(log);\n\n\treturn err;\n}",
        "code_after_change": "int log_replay(struct ntfs_inode *ni, bool *initialized)\n{\n\tint err;\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tstruct ntfs_log *log;\n\n\tstruct restart_info rst_info, rst_info2;\n\tu64 rec_lsn, ra_lsn, checkpt_lsn = 0, rlsn = 0;\n\tstruct ATTR_NAME_ENTRY *attr_names = NULL;\n\tstruct ATTR_NAME_ENTRY *ane;\n\tstruct RESTART_TABLE *dptbl = NULL;\n\tstruct RESTART_TABLE *trtbl = NULL;\n\tconst struct RESTART_TABLE *rt;\n\tstruct RESTART_TABLE *oatbl = NULL;\n\tstruct inode *inode;\n\tstruct OpenAttr *oa;\n\tstruct ntfs_inode *ni_oe;\n\tstruct ATTRIB *attr = NULL;\n\tu64 size, vcn, undo_next_lsn;\n\tCLST rno, lcn, lcn0, len0, clen;\n\tvoid *data;\n\tstruct NTFS_RESTART *rst = NULL;\n\tstruct lcb *lcb = NULL;\n\tstruct OPEN_ATTR_ENRTY *oe;\n\tstruct TRANSACTION_ENTRY *tr;\n\tstruct DIR_PAGE_ENTRY *dp;\n\tu32 i, bytes_per_attr_entry;\n\tu32 l_size = ni->vfs_inode.i_size;\n\tu32 orig_file_size = l_size;\n\tu32 page_size, vbo, tail, off, dlen;\n\tu32 saved_len, rec_len, transact_id;\n\tbool use_second_page;\n\tstruct RESTART_AREA *ra2, *ra = NULL;\n\tstruct CLIENT_REC *ca, *cr;\n\t__le16 client;\n\tstruct RESTART_HDR *rh;\n\tconst struct LFS_RECORD_HDR *frh;\n\tconst struct LOG_REC_HDR *lrh;\n\tbool is_mapped;\n\tbool is_ro = sb_rdonly(sbi->sb);\n\tu64 t64;\n\tu16 t16;\n\tu32 t32;\n\n\t/* Get the size of page. NOTE: To replay we can use default page. */\n#if PAGE_SIZE >= DefaultLogPageSize && PAGE_SIZE <= DefaultLogPageSize * 2\n\tpage_size = norm_file_page(PAGE_SIZE, &l_size, true);\n#else\n\tpage_size = norm_file_page(PAGE_SIZE, &l_size, false);\n#endif\n\tif (!page_size)\n\t\treturn -EINVAL;\n\n\tlog = kzalloc(sizeof(struct ntfs_log), GFP_NOFS);\n\tif (!log)\n\t\treturn -ENOMEM;\n\n\tmemset(&rst_info, 0, sizeof(struct restart_info));\n\n\tlog->ni = ni;\n\tlog->l_size = l_size;\n\tlog->one_page_buf = kmalloc(page_size, GFP_NOFS);\n\tif (!log->one_page_buf) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlog->page_size = page_size;\n\tlog->page_mask = page_size - 1;\n\tlog->page_bits = blksize_bits(page_size);\n\n\t/* Look for a restart area on the disk. */\n\terr = log_read_rst(log, l_size, true, &rst_info);\n\tif (err)\n\t\tgoto out;\n\n\t/* remember 'initialized' */\n\t*initialized = rst_info.initialized;\n\n\tif (!rst_info.restart) {\n\t\tif (rst_info.initialized) {\n\t\t\t/* No restart area but the file is not initialized. */\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tlog_init_pg_hdr(log, page_size, page_size, 1, 1);\n\t\tlog_create(log, l_size, 0, get_random_int(), false, false);\n\n\t\tlog->ra = ra;\n\n\t\tra = log_create_ra(log);\n\t\tif (!ra) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tlog->ra = ra;\n\t\tlog->init_ra = true;\n\n\t\tgoto process_log;\n\t}\n\n\t/*\n\t * If the restart offset above wasn't zero then we won't\n\t * look for a second restart.\n\t */\n\tif (rst_info.vbo)\n\t\tgoto check_restart_area;\n\n\tmemset(&rst_info2, 0, sizeof(struct restart_info));\n\terr = log_read_rst(log, l_size, false, &rst_info2);\n\n\t/* Determine which restart area to use. */\n\tif (!rst_info2.restart || rst_info2.last_lsn <= rst_info.last_lsn)\n\t\tgoto use_first_page;\n\n\tuse_second_page = true;\n\n\tif (rst_info.chkdsk_was_run && page_size != rst_info.vbo) {\n\t\tstruct RECORD_PAGE_HDR *sp = NULL;\n\t\tbool usa_error;\n\n\t\tif (!read_log_page(log, page_size, &sp, &usa_error) &&\n\t\t    sp->rhdr.sign == NTFS_CHKD_SIGNATURE) {\n\t\t\tuse_second_page = false;\n\t\t}\n\t\tkfree(sp);\n\t}\n\n\tif (use_second_page) {\n\t\tkfree(rst_info.r_page);\n\t\tmemcpy(&rst_info, &rst_info2, sizeof(struct restart_info));\n\t\trst_info2.r_page = NULL;\n\t}\n\nuse_first_page:\n\tkfree(rst_info2.r_page);\n\ncheck_restart_area:\n\t/*\n\t * If the restart area is at offset 0, we want\n\t * to write the second restart area first.\n\t */\n\tlog->init_ra = !!rst_info.vbo;\n\n\t/* If we have a valid page then grab a pointer to the restart area. */\n\tra2 = rst_info.valid_page\n\t\t      ? Add2Ptr(rst_info.r_page,\n\t\t\t\tle16_to_cpu(rst_info.r_page->ra_off))\n\t\t      : NULL;\n\n\tif (rst_info.chkdsk_was_run ||\n\t    (ra2 && ra2->client_idx[1] == LFS_NO_CLIENT_LE)) {\n\t\tbool wrapped = false;\n\t\tbool use_multi_page = false;\n\t\tu32 open_log_count;\n\n\t\t/* Do some checks based on whether we have a valid log page. */\n\t\tif (!rst_info.valid_page) {\n\t\t\topen_log_count = get_random_int();\n\t\t\tgoto init_log_instance;\n\t\t}\n\t\topen_log_count = le32_to_cpu(ra2->open_log_count);\n\n\t\t/*\n\t\t * If the restart page size isn't changing then we want to\n\t\t * check how much work we need to do.\n\t\t */\n\t\tif (page_size != le32_to_cpu(rst_info.r_page->sys_page_size))\n\t\t\tgoto init_log_instance;\n\ninit_log_instance:\n\t\tlog_init_pg_hdr(log, page_size, page_size, 1, 1);\n\n\t\tlog_create(log, l_size, rst_info.last_lsn, open_log_count,\n\t\t\t   wrapped, use_multi_page);\n\n\t\tra = log_create_ra(log);\n\t\tif (!ra) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tlog->ra = ra;\n\n\t\t/* Put the restart areas and initialize\n\t\t * the log file as required.\n\t\t */\n\t\tgoto process_log;\n\t}\n\n\tif (!ra2) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If the log page or the system page sizes have changed, we can't\n\t * use the log file. We must use the system page size instead of the\n\t * default size if there is not a clean shutdown.\n\t */\n\tt32 = le32_to_cpu(rst_info.r_page->sys_page_size);\n\tif (page_size != t32) {\n\t\tl_size = orig_file_size;\n\t\tpage_size =\n\t\t\tnorm_file_page(t32, &l_size, t32 == DefaultLogPageSize);\n\t}\n\n\tif (page_size != t32 ||\n\t    page_size != le32_to_cpu(rst_info.r_page->page_size)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* If the file size has shrunk then we won't mount it. */\n\tif (l_size < le64_to_cpu(ra2->l_size)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tlog_init_pg_hdr(log, page_size, page_size,\n\t\t\tle16_to_cpu(rst_info.r_page->major_ver),\n\t\t\tle16_to_cpu(rst_info.r_page->minor_ver));\n\n\tlog->l_size = le64_to_cpu(ra2->l_size);\n\tlog->seq_num_bits = le32_to_cpu(ra2->seq_num_bits);\n\tlog->file_data_bits = sizeof(u64) * 8 - log->seq_num_bits;\n\tlog->seq_num_mask = (8 << log->file_data_bits) - 1;\n\tlog->last_lsn = le64_to_cpu(ra2->current_lsn);\n\tlog->seq_num = log->last_lsn >> log->file_data_bits;\n\tlog->ra_off = le16_to_cpu(rst_info.r_page->ra_off);\n\tlog->restart_size = log->sys_page_size - log->ra_off;\n\tlog->record_header_len = le16_to_cpu(ra2->rec_hdr_len);\n\tlog->ra_size = le16_to_cpu(ra2->ra_len);\n\tlog->data_off = le16_to_cpu(ra2->data_off);\n\tlog->data_size = log->page_size - log->data_off;\n\tlog->reserved = log->data_size - log->record_header_len;\n\n\tvbo = lsn_to_vbo(log, log->last_lsn);\n\n\tif (vbo < log->first_page) {\n\t\t/* This is a pseudo lsn. */\n\t\tlog->l_flags |= NTFSLOG_NO_LAST_LSN;\n\t\tlog->next_page = log->first_page;\n\t\tgoto find_oldest;\n\t}\n\n\t/* Find the end of this log record. */\n\toff = final_log_off(log, log->last_lsn,\n\t\t\t    le32_to_cpu(ra2->last_lsn_data_len));\n\n\t/* If we wrapped the file then increment the sequence number. */\n\tif (off <= vbo) {\n\t\tlog->seq_num += 1;\n\t\tlog->l_flags |= NTFSLOG_WRAPPED;\n\t}\n\n\t/* Now compute the next log page to use. */\n\tvbo &= ~log->sys_page_mask;\n\ttail = log->page_size - (off & log->page_mask) - 1;\n\n\t/*\n\t *If we can fit another log record on the page,\n\t * move back a page the log file.\n\t */\n\tif (tail >= log->record_header_len) {\n\t\tlog->l_flags |= NTFSLOG_REUSE_TAIL;\n\t\tlog->next_page = vbo;\n\t} else {\n\t\tlog->next_page = next_page_off(log, vbo);\n\t}\n\nfind_oldest:\n\t/*\n\t * Find the oldest client lsn. Use the last\n\t * flushed lsn as a starting point.\n\t */\n\tlog->oldest_lsn = log->last_lsn;\n\toldest_client_lsn(Add2Ptr(ra2, le16_to_cpu(ra2->client_off)),\n\t\t\t  ra2->client_idx[1], &log->oldest_lsn);\n\tlog->oldest_lsn_off = lsn_to_vbo(log, log->oldest_lsn);\n\n\tif (log->oldest_lsn_off < log->first_page)\n\t\tlog->l_flags |= NTFSLOG_NO_OLDEST_LSN;\n\n\tif (!(ra2->flags & RESTART_SINGLE_PAGE_IO))\n\t\tlog->l_flags |= NTFSLOG_WRAPPED | NTFSLOG_MULTIPLE_PAGE_IO;\n\n\tlog->current_openlog_count = le32_to_cpu(ra2->open_log_count);\n\tlog->total_avail_pages = log->l_size - log->first_page;\n\tlog->total_avail = log->total_avail_pages >> log->page_bits;\n\tlog->max_current_avail = log->total_avail * log->reserved;\n\tlog->total_avail = log->total_avail * log->data_size;\n\n\tlog->current_avail = current_log_avail(log);\n\n\tra = kzalloc(log->restart_size, GFP_NOFS);\n\tif (!ra) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tlog->ra = ra;\n\n\tt16 = le16_to_cpu(ra2->client_off);\n\tif (t16 == offsetof(struct RESTART_AREA, clients)) {\n\t\tmemcpy(ra, ra2, log->ra_size);\n\t} else {\n\t\tmemcpy(ra, ra2, offsetof(struct RESTART_AREA, clients));\n\t\tmemcpy(ra->clients, Add2Ptr(ra2, t16),\n\t\t       le16_to_cpu(ra2->ra_len) - t16);\n\n\t\tlog->current_openlog_count = get_random_int();\n\t\tra->open_log_count = cpu_to_le32(log->current_openlog_count);\n\t\tlog->ra_size = offsetof(struct RESTART_AREA, clients) +\n\t\t\t       sizeof(struct CLIENT_REC);\n\t\tra->client_off =\n\t\t\tcpu_to_le16(offsetof(struct RESTART_AREA, clients));\n\t\tra->ra_len = cpu_to_le16(log->ra_size);\n\t}\n\n\tle32_add_cpu(&ra->open_log_count, 1);\n\n\t/* Now we need to walk through looking for the last lsn. */\n\terr = last_log_lsn(log);\n\tif (err)\n\t\tgoto out;\n\n\tlog->current_avail = current_log_avail(log);\n\n\t/* Remember which restart area to write first. */\n\tlog->init_ra = rst_info.vbo;\n\nprocess_log:\n\t/* 1.0, 1.1, 2.0 log->major_ver/minor_ver - short values. */\n\tswitch ((log->major_ver << 16) + log->minor_ver) {\n\tcase 0x10000:\n\tcase 0x10001:\n\tcase 0x20000:\n\t\tbreak;\n\tdefault:\n\t\tntfs_warn(sbi->sb, \"\\x24LogFile version %d.%d is not supported\",\n\t\t\t  log->major_ver, log->minor_ver);\n\t\terr = -EOPNOTSUPP;\n\t\tlog->set_dirty = true;\n\t\tgoto out;\n\t}\n\n\t/* One client \"NTFS\" per logfile. */\n\tca = Add2Ptr(ra, le16_to_cpu(ra->client_off));\n\n\tfor (client = ra->client_idx[1];; client = cr->next_client) {\n\t\tif (client == LFS_NO_CLIENT_LE) {\n\t\t\t/* Insert \"NTFS\" client LogFile. */\n\t\t\tclient = ra->client_idx[0];\n\t\t\tif (client == LFS_NO_CLIENT_LE) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tt16 = le16_to_cpu(client);\n\t\t\tcr = ca + t16;\n\n\t\t\tremove_client(ca, cr, &ra->client_idx[0]);\n\n\t\t\tcr->restart_lsn = 0;\n\t\t\tcr->oldest_lsn = cpu_to_le64(log->oldest_lsn);\n\t\t\tcr->name_bytes = cpu_to_le32(8);\n\t\t\tcr->name[0] = cpu_to_le16('N');\n\t\t\tcr->name[1] = cpu_to_le16('T');\n\t\t\tcr->name[2] = cpu_to_le16('F');\n\t\t\tcr->name[3] = cpu_to_le16('S');\n\n\t\t\tadd_client(ca, t16, &ra->client_idx[1]);\n\t\t\tbreak;\n\t\t}\n\n\t\tcr = ca + le16_to_cpu(client);\n\n\t\tif (cpu_to_le32(8) == cr->name_bytes &&\n\t\t    cpu_to_le16('N') == cr->name[0] &&\n\t\t    cpu_to_le16('T') == cr->name[1] &&\n\t\t    cpu_to_le16('F') == cr->name[2] &&\n\t\t    cpu_to_le16('S') == cr->name[3])\n\t\t\tbreak;\n\t}\n\n\t/* Update the client handle with the client block information. */\n\tlog->client_id.seq_num = cr->seq_num;\n\tlog->client_id.client_idx = client;\n\n\terr = read_rst_area(log, &rst, &ra_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (!rst)\n\t\tgoto out;\n\n\tbytes_per_attr_entry = !rst->major_ver ? 0x2C : 0x28;\n\n\tcheckpt_lsn = le64_to_cpu(rst->check_point_start);\n\tif (!checkpt_lsn)\n\t\tcheckpt_lsn = ra_lsn;\n\n\t/* Allocate and Read the Transaction Table. */\n\tif (!rst->transact_table_len)\n\t\tgoto check_dirty_page_table;\n\n\tt64 = le64_to_cpu(rst->transact_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\t/* Now check that this is a valid restart table. */\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttrtbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!trtbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_dirty_page_table:\n\t/* The next record back should be the Dirty Pages Table. */\n\tif (!rst->dirty_pages_len)\n\t\tgoto check_attribute_names;\n\n\tt64 = le64_to_cpu(rst->dirty_pages_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\t/* Now check that this is a valid restart table. */\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdptbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!dptbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Convert Ra version '0' into version '1'. */\n\tif (rst->major_ver)\n\t\tgoto end_conv_1;\n\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tstruct DIR_PAGE_ENTRY_32 *dp0 = (struct DIR_PAGE_ENTRY_32 *)dp;\n\t\t// NOTE: Danger. Check for of boundary.\n\t\tmemmove(&dp->vcn, &dp0->vcn_low,\n\t\t\t2 * sizeof(u64) +\n\t\t\t\tle32_to_cpu(dp->lcns_follow) * sizeof(u64));\n\t}\n\nend_conv_1:\n\tlcb_put(lcb);\n\tlcb = NULL;\n\n\t/*\n\t * Go through the table and remove the duplicates,\n\t * remembering the oldest lsn values.\n\t */\n\tif (sbi->cluster_size <= log->page_size)\n\t\tgoto trace_dp_table;\n\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tstruct DIR_PAGE_ENTRY *next = dp;\n\n\t\twhile ((next = enum_rstbl(dptbl, next))) {\n\t\t\tif (next->target_attr == dp->target_attr &&\n\t\t\t    next->vcn == dp->vcn) {\n\t\t\t\tif (le64_to_cpu(next->oldest_lsn) <\n\t\t\t\t    le64_to_cpu(dp->oldest_lsn)) {\n\t\t\t\t\tdp->oldest_lsn = next->oldest_lsn;\n\t\t\t\t}\n\n\t\t\t\tfree_rsttbl_idx(dptbl, PtrOffset(dptbl, next));\n\t\t\t}\n\t\t}\n\t}\ntrace_dp_table:\ncheck_attribute_names:\n\t/* The next record should be the Attribute Names. */\n\tif (!rst->attr_names_len)\n\t\tgoto check_attr_table;\n\n\tt64 = le64_to_cpu(rst->attr_names_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt32 = lrh_length(lrh);\n\trec_len -= t32;\n\n\tattr_names = kmemdup(Add2Ptr(lrh, t32), rec_len, GFP_NOFS);\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_attr_table:\n\t/* The next record should be the attribute Table. */\n\tif (!rst->open_attr_len)\n\t\tgoto check_attribute_names2;\n\n\tt64 = le64_to_cpu(rst->open_attr_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toatbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!oatbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlog->open_attr_tbl = oatbl;\n\n\t/* Clear all of the Attr pointers. */\n\toe = NULL;\n\twhile ((oe = enum_rstbl(oatbl, oe))) {\n\t\tif (!rst->major_ver) {\n\t\t\tstruct OPEN_ATTR_ENRTY_32 oe0;\n\n\t\t\t/* Really 'oe' points to OPEN_ATTR_ENRTY_32. */\n\t\t\tmemcpy(&oe0, oe, SIZEOF_OPENATTRIBUTEENTRY0);\n\n\t\t\toe->bytes_per_index = oe0.bytes_per_index;\n\t\t\toe->type = oe0.type;\n\t\t\toe->is_dirty_pages = oe0.is_dirty_pages;\n\t\t\toe->name_len = 0;\n\t\t\toe->ref = oe0.ref;\n\t\t\toe->open_record_lsn = oe0.open_record_lsn;\n\t\t}\n\n\t\toe->is_attr_name = 0;\n\t\toe->ptr = NULL;\n\t}\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_attribute_names2:\n\tif (!rst->attr_names_len)\n\t\tgoto trace_attribute_table;\n\n\tane = attr_names;\n\tif (!oatbl)\n\t\tgoto trace_attribute_table;\n\twhile (ane->off) {\n\t\t/* TODO: Clear table on exit! */\n\t\toe = Add2Ptr(oatbl, le16_to_cpu(ane->off));\n\t\tt16 = le16_to_cpu(ane->name_bytes);\n\t\toe->name_len = t16 / sizeof(short);\n\t\toe->ptr = ane->name;\n\t\toe->is_attr_name = 2;\n\t\tane = Add2Ptr(ane, sizeof(struct ATTR_NAME_ENTRY) + t16);\n\t}\n\ntrace_attribute_table:\n\t/*\n\t * If the checkpt_lsn is zero, then this is a freshly\n\t * formatted disk and we have no work to do.\n\t */\n\tif (!checkpt_lsn) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\tif (!oatbl) {\n\t\toatbl = init_rsttbl(bytes_per_attr_entry, 8);\n\t\tif (!oatbl) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tlog->open_attr_tbl = oatbl;\n\n\t/* Start the analysis pass from the Checkpoint lsn. */\n\trec_lsn = checkpt_lsn;\n\n\t/* Read the first lsn. */\n\terr = read_log_rec_lcb(log, checkpt_lsn, lcb_ctx_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/* Loop to read all subsequent records to the end of the log file. */\nnext_log_record_analyze:\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (!rec_lsn)\n\t\tgoto end_log_records_enumerate;\n\n\tfrh = lcb->lrh;\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\tlrh = lcb->log_rec;\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * The first lsn after the previous lsn remembered\n\t * the checkpoint is the first candidate for the rlsn.\n\t */\n\tif (!rlsn)\n\t\trlsn = rec_lsn;\n\n\tif (LfsClientRecord != frh->record_type)\n\t\tgoto next_log_record_analyze;\n\n\t/*\n\t * Now update the Transaction Table for this transaction. If there\n\t * is no entry present or it is unallocated we allocate the entry.\n\t */\n\tif (!trtbl) {\n\t\ttrtbl = init_rsttbl(sizeof(struct TRANSACTION_ENTRY),\n\t\t\t\t    INITIAL_NUMBER_TRANSACTIONS);\n\t\tif (!trtbl) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\ttr = Add2Ptr(trtbl, transact_id);\n\n\tif (transact_id >= bytes_per_rt(trtbl) ||\n\t    tr->next != RESTART_ENTRY_ALLOCATED_LE) {\n\t\ttr = alloc_rsttbl_from_idx(&trtbl, transact_id);\n\t\tif (!tr) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\ttr->transact_state = TransactionActive;\n\t\ttr->first_lsn = cpu_to_le64(rec_lsn);\n\t}\n\n\ttr->prev_lsn = tr->undo_next_lsn = cpu_to_le64(rec_lsn);\n\n\t/*\n\t * If this is a compensation log record, then change\n\t * the undo_next_lsn to be the undo_next_lsn of this record.\n\t */\n\tif (lrh->undo_op == cpu_to_le16(CompensationLogRecord))\n\t\ttr->undo_next_lsn = frh->client_undo_next_lsn;\n\n\t/* Dispatch to handle log record depending on type. */\n\tswitch (le16_to_cpu(lrh->redo_op)) {\n\tcase InitializeFileRecordSegment:\n\tcase DeallocateFileRecordSegment:\n\tcase WriteEndOfFileRecordSegment:\n\tcase CreateAttribute:\n\tcase DeleteAttribute:\n\tcase UpdateResidentValue:\n\tcase UpdateNonresidentValue:\n\tcase UpdateMappingPairs:\n\tcase SetNewAttributeSizes:\n\tcase AddIndexEntryRoot:\n\tcase DeleteIndexEntryRoot:\n\tcase AddIndexEntryAllocation:\n\tcase DeleteIndexEntryAllocation:\n\tcase WriteEndOfIndexBuffer:\n\tcase SetIndexEntryVcnRoot:\n\tcase SetIndexEntryVcnAllocation:\n\tcase UpdateFileNameRoot:\n\tcase UpdateFileNameAllocation:\n\tcase SetBitsInNonresidentBitMap:\n\tcase ClearBitsInNonresidentBitMap:\n\tcase UpdateRecordDataRoot:\n\tcase UpdateRecordDataAllocation:\n\tcase ZeroEndOfFileRecord:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tt64 = le64_to_cpu(lrh->target_vcn);\n\t\tdp = find_dp(dptbl, t16, t64);\n\n\t\tif (dp)\n\t\t\tgoto copy_lcns;\n\n\t\t/*\n\t\t * Calculate the number of clusters per page the system\n\t\t * which wrote the checkpoint, possibly creating the table.\n\t\t */\n\t\tif (dptbl) {\n\t\t\tt32 = (le16_to_cpu(dptbl->size) -\n\t\t\t       sizeof(struct DIR_PAGE_ENTRY)) /\n\t\t\t      sizeof(u64);\n\t\t} else {\n\t\t\tt32 = log->clst_per_page;\n\t\t\tkfree(dptbl);\n\t\t\tdptbl = init_rsttbl(struct_size(dp, page_lcns, t32),\n\t\t\t\t\t    32);\n\t\t\tif (!dptbl) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tdp = alloc_rsttbl_idx(&dptbl);\n\t\tif (!dp) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tdp->target_attr = cpu_to_le32(t16);\n\t\tdp->transfer_len = cpu_to_le32(t32 << sbi->cluster_bits);\n\t\tdp->lcns_follow = cpu_to_le32(t32);\n\t\tdp->vcn = cpu_to_le64(t64 & ~((u64)t32 - 1));\n\t\tdp->oldest_lsn = cpu_to_le64(rec_lsn);\n\ncopy_lcns:\n\t\t/*\n\t\t * Copy the Lcns from the log record into the Dirty Page Entry.\n\t\t * TODO: For different page size support, must somehow make\n\t\t * whole routine a loop, case Lcns do not fit below.\n\t\t */\n\t\tt16 = le16_to_cpu(lrh->lcns_follow);\n\t\tfor (i = 0; i < t16; i++) {\n\t\t\tsize_t j = (size_t)(le64_to_cpu(lrh->target_vcn) -\n\t\t\t\t\t    le64_to_cpu(dp->vcn));\n\t\t\tdp->page_lcns[j + i] = lrh->page_lcns[i];\n\t\t}\n\n\t\tgoto next_log_record_analyze;\n\n\tcase DeleteDirtyClusters: {\n\t\tu32 range_count =\n\t\t\tle16_to_cpu(lrh->redo_len) / sizeof(struct LCN_RANGE);\n\t\tconst struct LCN_RANGE *r =\n\t\t\tAdd2Ptr(lrh, le16_to_cpu(lrh->redo_off));\n\n\t\t/* Loop through all of the Lcn ranges this log record. */\n\t\tfor (i = 0; i < range_count; i++, r++) {\n\t\t\tu64 lcn0 = le64_to_cpu(r->lcn);\n\t\t\tu64 lcn_e = lcn0 + le64_to_cpu(r->len) - 1;\n\n\t\t\tdp = NULL;\n\t\t\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\t\t\tu32 j;\n\n\t\t\t\tt32 = le32_to_cpu(dp->lcns_follow);\n\t\t\t\tfor (j = 0; j < t32; j++) {\n\t\t\t\t\tt64 = le64_to_cpu(dp->page_lcns[j]);\n\t\t\t\t\tif (t64 >= lcn0 && t64 <= lcn_e)\n\t\t\t\t\t\tdp->page_lcns[j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgoto next_log_record_analyze;\n\t\t;\n\t}\n\n\tcase OpenNonresidentAttribute:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tif (t16 >= bytes_per_rt(oatbl)) {\n\t\t\t/*\n\t\t\t * Compute how big the table needs to be.\n\t\t\t * Add 10 extra entries for some cushion.\n\t\t\t */\n\t\t\tu32 new_e = t16 / le16_to_cpu(oatbl->size);\n\n\t\t\tnew_e += 10 - le16_to_cpu(oatbl->used);\n\n\t\t\toatbl = extend_rsttbl(oatbl, new_e, ~0u);\n\t\t\tlog->open_attr_tbl = oatbl;\n\t\t\tif (!oatbl) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\t/* Point to the entry being opened. */\n\t\toe = alloc_rsttbl_from_idx(&oatbl, t16);\n\t\tlog->open_attr_tbl = oatbl;\n\t\tif (!oe) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* Initialize this entry from the log record. */\n\t\tt16 = le16_to_cpu(lrh->redo_off);\n\t\tif (!rst->major_ver) {\n\t\t\t/* Convert version '0' into version '1'. */\n\t\t\tstruct OPEN_ATTR_ENRTY_32 *oe0 = Add2Ptr(lrh, t16);\n\n\t\t\toe->bytes_per_index = oe0->bytes_per_index;\n\t\t\toe->type = oe0->type;\n\t\t\toe->is_dirty_pages = oe0->is_dirty_pages;\n\t\t\toe->name_len = 0; //oe0.name_len;\n\t\t\toe->ref = oe0->ref;\n\t\t\toe->open_record_lsn = oe0->open_record_lsn;\n\t\t} else {\n\t\t\tmemcpy(oe, Add2Ptr(lrh, t16), bytes_per_attr_entry);\n\t\t}\n\n\t\tt16 = le16_to_cpu(lrh->undo_len);\n\t\tif (t16) {\n\t\t\toe->ptr = kmalloc(t16, GFP_NOFS);\n\t\t\tif (!oe->ptr) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\toe->name_len = t16 / sizeof(short);\n\t\t\tmemcpy(oe->ptr,\n\t\t\t       Add2Ptr(lrh, le16_to_cpu(lrh->undo_off)), t16);\n\t\t\toe->is_attr_name = 1;\n\t\t} else {\n\t\t\toe->ptr = NULL;\n\t\t\toe->is_attr_name = 0;\n\t\t}\n\n\t\tgoto next_log_record_analyze;\n\n\tcase HotFix:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tt64 = le64_to_cpu(lrh->target_vcn);\n\t\tdp = find_dp(dptbl, t16, t64);\n\t\tif (dp) {\n\t\t\tsize_t j = le64_to_cpu(lrh->target_vcn) -\n\t\t\t\t   le64_to_cpu(dp->vcn);\n\t\t\tif (dp->page_lcns[j])\n\t\t\t\tdp->page_lcns[j] = lrh->page_lcns[0];\n\t\t}\n\t\tgoto next_log_record_analyze;\n\n\tcase EndTopLevelAction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->prev_lsn = cpu_to_le64(rec_lsn);\n\t\ttr->undo_next_lsn = frh->client_undo_next_lsn;\n\t\tgoto next_log_record_analyze;\n\n\tcase PrepareTransaction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->transact_state = TransactionPrepared;\n\t\tgoto next_log_record_analyze;\n\n\tcase CommitTransaction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->transact_state = TransactionCommitted;\n\t\tgoto next_log_record_analyze;\n\n\tcase ForgetTransaction:\n\t\tfree_rsttbl_idx(trtbl, transact_id);\n\t\tgoto next_log_record_analyze;\n\n\tcase Noop:\n\tcase OpenAttributeTableDump:\n\tcase AttributeNamesDump:\n\tcase DirtyPageTableDump:\n\tcase TransactionTableDump:\n\t\t/* The following cases require no action the Analysis Pass. */\n\t\tgoto next_log_record_analyze;\n\n\tdefault:\n\t\t/*\n\t\t * All codes will be explicitly handled.\n\t\t * If we see a code we do not expect, then we are trouble.\n\t\t */\n\t\tgoto next_log_record_analyze;\n\t}\n\nend_log_records_enumerate:\n\tlcb_put(lcb);\n\tlcb = NULL;\n\n\t/*\n\t * Scan the Dirty Page Table and Transaction Table for\n\t * the lowest lsn, and return it as the Redo lsn.\n\t */\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tt64 = le64_to_cpu(dp->oldest_lsn);\n\t\tif (t64 && t64 < rlsn)\n\t\t\trlsn = t64;\n\t}\n\n\ttr = NULL;\n\twhile ((tr = enum_rstbl(trtbl, tr))) {\n\t\tt64 = le64_to_cpu(tr->first_lsn);\n\t\tif (t64 && t64 < rlsn)\n\t\t\trlsn = t64;\n\t}\n\n\t/*\n\t * Only proceed if the Dirty Page Table or Transaction\n\t * table are not empty.\n\t */\n\tif ((!dptbl || !dptbl->total) && (!trtbl || !trtbl->total))\n\t\tgoto end_reply;\n\n\tsbi->flags |= NTFS_FLAGS_NEED_REPLAY;\n\tif (is_ro)\n\t\tgoto out;\n\n\t/* Reopen all of the attributes with dirty pages. */\n\toe = NULL;\nnext_open_attribute:\n\n\toe = enum_rstbl(oatbl, oe);\n\tif (!oe) {\n\t\terr = 0;\n\t\tdp = NULL;\n\t\tgoto next_dirty_page;\n\t}\n\n\toa = kzalloc(sizeof(struct OpenAttr), GFP_NOFS);\n\tif (!oa) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tinode = ntfs_iget5(sbi->sb, &oe->ref, NULL);\n\tif (IS_ERR(inode))\n\t\tgoto fake_attr;\n\n\tif (is_bad_inode(inode)) {\n\t\tiput(inode);\nfake_attr:\n\t\tif (oa->ni) {\n\t\t\tiput(&oa->ni->vfs_inode);\n\t\t\toa->ni = NULL;\n\t\t}\n\n\t\tattr = attr_create_nonres_log(sbi, oe->type, 0, oe->ptr,\n\t\t\t\t\t      oe->name_len, 0);\n\t\tif (!attr) {\n\t\t\tkfree(oa);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\toa->attr = attr;\n\t\toa->run1 = &oa->run0;\n\t\tgoto final_oe;\n\t}\n\n\tni_oe = ntfs_i(inode);\n\toa->ni = ni_oe;\n\n\tattr = ni_find_attr(ni_oe, NULL, NULL, oe->type, oe->ptr, oe->name_len,\n\t\t\t    NULL, NULL);\n\n\tif (!attr)\n\t\tgoto fake_attr;\n\n\tt32 = le32_to_cpu(attr->size);\n\toa->attr = kmemdup(attr, t32, GFP_NOFS);\n\tif (!oa->attr)\n\t\tgoto fake_attr;\n\n\tif (!S_ISDIR(inode->i_mode)) {\n\t\tif (attr->type == ATTR_DATA && !attr->name_len) {\n\t\t\toa->run1 = &ni_oe->file.run;\n\t\t\tgoto final_oe;\n\t\t}\n\t} else {\n\t\tif (attr->type == ATTR_ALLOC &&\n\t\t    attr->name_len == ARRAY_SIZE(I30_NAME) &&\n\t\t    !memcmp(attr_name(attr), I30_NAME, sizeof(I30_NAME))) {\n\t\t\toa->run1 = &ni_oe->dir.alloc_run;\n\t\t\tgoto final_oe;\n\t\t}\n\t}\n\n\tif (attr->non_res) {\n\t\tu16 roff = le16_to_cpu(attr->nres.run_off);\n\t\tCLST svcn = le64_to_cpu(attr->nres.svcn);\n\n\t\terr = run_unpack(&oa->run0, sbi, inode->i_ino, svcn,\n\t\t\t\t le64_to_cpu(attr->nres.evcn), svcn,\n\t\t\t\t Add2Ptr(attr, roff), t32 - roff);\n\t\tif (err < 0) {\n\t\t\tkfree(oa->attr);\n\t\t\toa->attr = NULL;\n\t\t\tgoto fake_attr;\n\t\t}\n\t\terr = 0;\n\t}\n\toa->run1 = &oa->run0;\n\tattr = oa->attr;\n\nfinal_oe:\n\tif (oe->is_attr_name == 1)\n\t\tkfree(oe->ptr);\n\toe->is_attr_name = 0;\n\toe->ptr = oa;\n\toe->name_len = attr->name_len;\n\n\tgoto next_open_attribute;\n\n\t/*\n\t * Now loop through the dirty page table to extract all of the Vcn/Lcn.\n\t * Mapping that we have, and insert it into the appropriate run.\n\t */\nnext_dirty_page:\n\tdp = enum_rstbl(dptbl, dp);\n\tif (!dp)\n\t\tgoto do_redo_1;\n\n\toe = Add2Ptr(oatbl, le32_to_cpu(dp->target_attr));\n\n\tif (oe->next != RESTART_ENTRY_ALLOCATED_LE)\n\t\tgoto next_dirty_page;\n\n\toa = oe->ptr;\n\tif (!oa)\n\t\tgoto next_dirty_page;\n\n\ti = -1;\nnext_dirty_page_vcn:\n\ti += 1;\n\tif (i >= le32_to_cpu(dp->lcns_follow))\n\t\tgoto next_dirty_page;\n\n\tvcn = le64_to_cpu(dp->vcn) + i;\n\tsize = (vcn + 1) << sbi->cluster_bits;\n\n\tif (!dp->page_lcns[i])\n\t\tgoto next_dirty_page_vcn;\n\n\trno = ino_get(&oe->ref);\n\tif (rno <= MFT_REC_MIRR &&\n\t    size < (MFT_REC_VOL + 1) * sbi->record_size &&\n\t    oe->type == ATTR_DATA) {\n\t\tgoto next_dirty_page_vcn;\n\t}\n\n\tlcn = le64_to_cpu(dp->page_lcns[i]);\n\n\tif ((!run_lookup_entry(oa->run1, vcn, &lcn0, &len0, NULL) ||\n\t     lcn0 != lcn) &&\n\t    !run_add_entry(oa->run1, vcn, lcn, 1, false)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tattr = oa->attr;\n\tt64 = le64_to_cpu(attr->nres.alloc_size);\n\tif (size > t64) {\n\t\tattr->nres.valid_size = attr->nres.data_size =\n\t\t\tattr->nres.alloc_size = cpu_to_le64(size);\n\t}\n\tgoto next_dirty_page_vcn;\n\ndo_redo_1:\n\t/*\n\t * Perform the Redo Pass, to restore all of the dirty pages to the same\n\t * contents that they had immediately before the crash. If the dirty\n\t * page table is empty, then we can skip the entire Redo Pass.\n\t */\n\tif (!dptbl || !dptbl->total)\n\t\tgoto do_undo_action;\n\n\trec_lsn = rlsn;\n\n\t/*\n\t * Read the record at the Redo lsn, before falling\n\t * into common code to handle each record.\n\t */\n\terr = read_log_rec_lcb(log, rlsn, lcb_ctx_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/*\n\t * Now loop to read all of our log records forwards, until\n\t * we hit the end of the file, cleaning up at the end.\n\t */\ndo_action_next:\n\tfrh = lcb->lrh;\n\n\tif (LfsClientRecord != frh->record_type)\n\t\tgoto read_next_log_do_action;\n\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\tlrh = lcb->log_rec;\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Ignore log records that do not update pages. */\n\tif (lrh->lcns_follow)\n\t\tgoto find_dirty_page;\n\n\tgoto read_next_log_do_action;\n\nfind_dirty_page:\n\tt16 = le16_to_cpu(lrh->target_attr);\n\tt64 = le64_to_cpu(lrh->target_vcn);\n\tdp = find_dp(dptbl, t16, t64);\n\n\tif (!dp)\n\t\tgoto read_next_log_do_action;\n\n\tif (rec_lsn < le64_to_cpu(dp->oldest_lsn))\n\t\tgoto read_next_log_do_action;\n\n\tt16 = le16_to_cpu(lrh->target_attr);\n\tif (t16 >= bytes_per_rt(oatbl)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toe = Add2Ptr(oatbl, t16);\n\n\tif (oe->next != RESTART_ENTRY_ALLOCATED_LE) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toa = oe->ptr;\n\n\tif (!oa) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tattr = oa->attr;\n\n\tvcn = le64_to_cpu(lrh->target_vcn);\n\n\tif (!run_lookup_entry(oa->run1, vcn, &lcn, NULL, NULL) ||\n\t    lcn == SPARSE_LCN) {\n\t\tgoto read_next_log_do_action;\n\t}\n\n\t/* Point to the Redo data and get its length. */\n\tdata = Add2Ptr(lrh, le16_to_cpu(lrh->redo_off));\n\tdlen = le16_to_cpu(lrh->redo_len);\n\n\t/* Shorten length by any Lcns which were deleted. */\n\tsaved_len = dlen;\n\n\tfor (i = le16_to_cpu(lrh->lcns_follow); i; i--) {\n\t\tsize_t j;\n\t\tu32 alen, voff;\n\n\t\tvoff = le16_to_cpu(lrh->record_off) +\n\t\t       le16_to_cpu(lrh->attr_off);\n\t\tvoff += le16_to_cpu(lrh->cluster_off) << SECTOR_SHIFT;\n\n\t\t/* If the Vcn question is allocated, we can just get out. */\n\t\tj = le64_to_cpu(lrh->target_vcn) - le64_to_cpu(dp->vcn);\n\t\tif (dp->page_lcns[j + i - 1])\n\t\t\tbreak;\n\n\t\tif (!saved_len)\n\t\t\tsaved_len = 1;\n\n\t\t/*\n\t\t * Calculate the allocated space left relative to the\n\t\t * log record Vcn, after removing this unallocated Vcn.\n\t\t */\n\t\talen = (i - 1) << sbi->cluster_bits;\n\n\t\t/*\n\t\t * If the update described this log record goes beyond\n\t\t * the allocated space, then we will have to reduce the length.\n\t\t */\n\t\tif (voff >= alen)\n\t\t\tdlen = 0;\n\t\telse if (voff + dlen > alen)\n\t\t\tdlen = alen - voff;\n\t}\n\n\t/*\n\t * If the resulting dlen from above is now zero,\n\t * we can skip this log record.\n\t */\n\tif (!dlen && saved_len)\n\t\tgoto read_next_log_do_action;\n\n\tt16 = le16_to_cpu(lrh->redo_op);\n\tif (can_skip_action(t16))\n\t\tgoto read_next_log_do_action;\n\n\t/* Apply the Redo operation a common routine. */\n\terr = do_action(log, oe, lrh, t16, data, dlen, rec_len, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\t/* Keep reading and looping back until end of file. */\nread_next_log_do_action:\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (!err && rec_lsn)\n\t\tgoto do_action_next;\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ndo_undo_action:\n\t/* Scan Transaction Table. */\n\ttr = NULL;\ntransaction_table_next:\n\ttr = enum_rstbl(trtbl, tr);\n\tif (!tr)\n\t\tgoto undo_action_done;\n\n\tif (TransactionActive != tr->transact_state || !tr->undo_next_lsn) {\n\t\tfree_rsttbl_idx(trtbl, PtrOffset(trtbl, tr));\n\t\tgoto transaction_table_next;\n\t}\n\n\tlog->transaction_id = PtrOffset(trtbl, tr);\n\tundo_next_lsn = le64_to_cpu(tr->undo_next_lsn);\n\n\t/*\n\t * We only have to do anything if the transaction has\n\t * something its undo_next_lsn field.\n\t */\n\tif (!undo_next_lsn)\n\t\tgoto commit_undo;\n\n\t/* Read the first record to be undone by this transaction. */\n\terr = read_log_rec_lcb(log, undo_next_lsn, lcb_ctx_undo_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/*\n\t * Now loop to read all of our log records forwards,\n\t * until we hit the end of the file, cleaning up at the end.\n\t */\nundo_action_next:\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (lrh->undo_op == cpu_to_le16(Noop))\n\t\tgoto read_next_log_undo_action;\n\n\toe = Add2Ptr(oatbl, le16_to_cpu(lrh->target_attr));\n\toa = oe->ptr;\n\n\tt16 = le16_to_cpu(lrh->lcns_follow);\n\tif (!t16)\n\t\tgoto add_allocated_vcns;\n\n\tis_mapped = run_lookup_entry(oa->run1, le64_to_cpu(lrh->target_vcn),\n\t\t\t\t     &lcn, &clen, NULL);\n\n\t/*\n\t * If the mapping isn't already the table or the  mapping\n\t * corresponds to a hole the mapping, we need to make sure\n\t * there is no partial page already memory.\n\t */\n\tif (is_mapped && lcn != SPARSE_LCN && clen >= t16)\n\t\tgoto add_allocated_vcns;\n\n\tvcn = le64_to_cpu(lrh->target_vcn);\n\tvcn &= ~(log->clst_per_page - 1);\n\nadd_allocated_vcns:\n\tfor (i = 0, vcn = le64_to_cpu(lrh->target_vcn),\n\t    size = (vcn + 1) << sbi->cluster_bits;\n\t     i < t16; i++, vcn += 1, size += sbi->cluster_size) {\n\t\tattr = oa->attr;\n\t\tif (!attr->non_res) {\n\t\t\tif (size > le32_to_cpu(attr->res.data_size))\n\t\t\t\tattr->res.data_size = cpu_to_le32(size);\n\t\t} else {\n\t\t\tif (size > le64_to_cpu(attr->nres.data_size))\n\t\t\t\tattr->nres.valid_size = attr->nres.data_size =\n\t\t\t\t\tattr->nres.alloc_size =\n\t\t\t\t\t\tcpu_to_le64(size);\n\t\t}\n\t}\n\n\tt16 = le16_to_cpu(lrh->undo_op);\n\tif (can_skip_action(t16))\n\t\tgoto read_next_log_undo_action;\n\n\t/* Point to the Redo data and get its length. */\n\tdata = Add2Ptr(lrh, le16_to_cpu(lrh->undo_off));\n\tdlen = le16_to_cpu(lrh->undo_len);\n\n\t/* It is time to apply the undo action. */\n\terr = do_action(log, oe, lrh, t16, data, dlen, rec_len, NULL);\n\nread_next_log_undo_action:\n\t/*\n\t * Keep reading and looping back until we have read the\n\t * last record for this transaction.\n\t */\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (rec_lsn)\n\t\tgoto undo_action_next;\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncommit_undo:\n\tfree_rsttbl_idx(trtbl, log->transaction_id);\n\n\tlog->transaction_id = 0;\n\n\tgoto transaction_table_next;\n\nundo_action_done:\n\n\tntfs_update_mftmirr(sbi, 0);\n\n\tsbi->flags &= ~NTFS_FLAGS_NEED_REPLAY;\n\nend_reply:\n\n\terr = 0;\n\tif (is_ro)\n\t\tgoto out;\n\n\trh = kzalloc(log->page_size, GFP_NOFS);\n\tif (!rh) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trh->rhdr.sign = NTFS_RSTR_SIGNATURE;\n\trh->rhdr.fix_off = cpu_to_le16(offsetof(struct RESTART_HDR, fixups));\n\tt16 = (log->page_size >> SECTOR_SHIFT) + 1;\n\trh->rhdr.fix_num = cpu_to_le16(t16);\n\trh->sys_page_size = cpu_to_le32(log->page_size);\n\trh->page_size = cpu_to_le32(log->page_size);\n\n\tt16 = ALIGN(offsetof(struct RESTART_HDR, fixups) + sizeof(short) * t16,\n\t\t    8);\n\trh->ra_off = cpu_to_le16(t16);\n\trh->minor_ver = cpu_to_le16(1); // 0x1A:\n\trh->major_ver = cpu_to_le16(1); // 0x1C:\n\n\tra2 = Add2Ptr(rh, t16);\n\tmemcpy(ra2, ra, sizeof(struct RESTART_AREA));\n\n\tra2->client_idx[0] = 0;\n\tra2->client_idx[1] = LFS_NO_CLIENT_LE;\n\tra2->flags = cpu_to_le16(2);\n\n\tle32_add_cpu(&ra2->open_log_count, 1);\n\n\tntfs_fix_pre_write(&rh->rhdr, log->page_size);\n\n\terr = ntfs_sb_write_run(sbi, &ni->file.run, 0, rh, log->page_size, 0);\n\tif (!err)\n\t\terr = ntfs_sb_write_run(sbi, &log->ni->file.run, log->page_size,\n\t\t\t\t\trh, log->page_size, 0);\n\n\tkfree(rh);\n\tif (err)\n\t\tgoto out;\n\nout:\n\tkfree(rst);\n\tif (lcb)\n\t\tlcb_put(lcb);\n\n\t/*\n\t * Scan the Open Attribute Table to close all of\n\t * the open attributes.\n\t */\n\toe = NULL;\n\twhile ((oe = enum_rstbl(oatbl, oe))) {\n\t\trno = ino_get(&oe->ref);\n\n\t\tif (oe->is_attr_name == 1) {\n\t\t\tkfree(oe->ptr);\n\t\t\toe->ptr = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (oe->is_attr_name)\n\t\t\tcontinue;\n\n\t\toa = oe->ptr;\n\t\tif (!oa)\n\t\t\tcontinue;\n\n\t\trun_close(&oa->run0);\n\t\tkfree(oa->attr);\n\t\tif (oa->ni)\n\t\t\tiput(&oa->ni->vfs_inode);\n\t\tkfree(oa);\n\t}\n\n\tkfree(trtbl);\n\tkfree(oatbl);\n\tkfree(dptbl);\n\tkfree(attr_names);\n\tkfree(rst_info.r_page);\n\n\tkfree(ra);\n\tkfree(log->one_page_buf);\n\n\tif (err)\n\t\tsbi->flags |= NTFS_FLAGS_NEED_REPLAY;\n\n\tif (err == -EROFS)\n\t\terr = 0;\n\telse if (log->set_dirty)\n\t\tntfs_set_state(sbi, NTFS_DIRTY_ERROR);\n\n\tkfree(log);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -55,10 +55,11 @@\n \tif (!log)\n \t\treturn -ENOMEM;\n \n+\tmemset(&rst_info, 0, sizeof(struct restart_info));\n+\n \tlog->ni = ni;\n \tlog->l_size = l_size;\n \tlog->one_page_buf = kmalloc(page_size, GFP_NOFS);\n-\n \tif (!log->one_page_buf) {\n \t\terr = -ENOMEM;\n \t\tgoto out;\n@@ -106,6 +107,7 @@\n \tif (rst_info.vbo)\n \t\tgoto check_restart_area;\n \n+\tmemset(&rst_info2, 0, sizeof(struct restart_info));\n \terr = log_read_rst(log, l_size, false, &rst_info2);\n \n \t/* Determine which restart area to use. */",
        "function_modified_lines": {
            "added": [
                "\tmemset(&rst_info, 0, sizeof(struct restart_info));",
                "",
                "\tmemset(&rst_info2, 0, sizeof(struct restart_info));"
            ],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel in log_replay in fs/ntfs3/fslog.c in the NTFS journal. This flaw allows a local attacker to crash the system and leads to a kernel information leak problem.",
        "id": 3306
    },
    {
        "cve_id": "CVE-2022-0487",
        "code_before_change": "static int moxart_remove(struct platform_device *pdev)\n{\n\tstruct mmc_host *mmc = dev_get_drvdata(&pdev->dev);\n\tstruct moxart_host *host = mmc_priv(mmc);\n\n\tdev_set_drvdata(&pdev->dev, NULL);\n\n\tif (!IS_ERR_OR_NULL(host->dma_chan_tx))\n\t\tdma_release_channel(host->dma_chan_tx);\n\tif (!IS_ERR_OR_NULL(host->dma_chan_rx))\n\t\tdma_release_channel(host->dma_chan_rx);\n\tmmc_remove_host(mmc);\n\tmmc_free_host(mmc);\n\n\twritel(0, host->base + REG_INTERRUPT_MASK);\n\twritel(0, host->base + REG_POWER_CONTROL);\n\twritel(readl(host->base + REG_CLOCK_CONTROL) | CLK_OFF,\n\t       host->base + REG_CLOCK_CONTROL);\n\n\treturn 0;\n}",
        "code_after_change": "static int moxart_remove(struct platform_device *pdev)\n{\n\tstruct mmc_host *mmc = dev_get_drvdata(&pdev->dev);\n\tstruct moxart_host *host = mmc_priv(mmc);\n\n\tdev_set_drvdata(&pdev->dev, NULL);\n\n\tif (!IS_ERR_OR_NULL(host->dma_chan_tx))\n\t\tdma_release_channel(host->dma_chan_tx);\n\tif (!IS_ERR_OR_NULL(host->dma_chan_rx))\n\t\tdma_release_channel(host->dma_chan_rx);\n\tmmc_remove_host(mmc);\n\n\twritel(0, host->base + REG_INTERRUPT_MASK);\n\twritel(0, host->base + REG_POWER_CONTROL);\n\twritel(readl(host->base + REG_CLOCK_CONTROL) | CLK_OFF,\n\t       host->base + REG_CLOCK_CONTROL);\n\tmmc_free_host(mmc);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,12 +10,12 @@\n \tif (!IS_ERR_OR_NULL(host->dma_chan_rx))\n \t\tdma_release_channel(host->dma_chan_rx);\n \tmmc_remove_host(mmc);\n-\tmmc_free_host(mmc);\n \n \twritel(0, host->base + REG_INTERRUPT_MASK);\n \twritel(0, host->base + REG_POWER_CONTROL);\n \twritel(readl(host->base + REG_CLOCK_CONTROL) | CLK_OFF,\n \t       host->base + REG_CLOCK_CONTROL);\n+\tmmc_free_host(mmc);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tmmc_free_host(mmc);"
            ],
            "deleted": [
                "\tmmc_free_host(mmc);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in rtsx_usb_ms_drv_remove in drivers/memstick/host/rtsx_usb_ms.c in memstick in the Linux kernel. In this flaw, a local attacker with a user privilege may impact system Confidentiality. This flaw affects kernel versions prior to 5.14 rc1.",
        "id": 3210
    },
    {
        "cve_id": "CVE-2017-15265",
        "code_before_change": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\tsprintf(new_port->name, \"port-%d\", num);\n\n\treturn new_port;\n}",
        "code_after_change": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\tsnd_use_lock_use(&new_port->use_lock);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\tsprintf(new_port->name, \"port-%d\", num);\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\n\treturn new_port;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,6 +26,7 @@\n \tsnd_use_lock_init(&new_port->use_lock);\n \tport_subs_info_init(&new_port->c_src);\n \tport_subs_info_init(&new_port->c_dest);\n+\tsnd_use_lock_use(&new_port->use_lock);\n \n \tnum = port >= 0 ? port : 0;\n \tmutex_lock(&client->ports_mutex);\n@@ -40,9 +41,9 @@\n \tlist_add_tail(&new_port->list, &p->list);\n \tclient->num_ports++;\n \tnew_port->addr.port = num;\t/* store the port number in the port */\n+\tsprintf(new_port->name, \"port-%d\", num);\n \twrite_unlock_irqrestore(&client->ports_lock, flags);\n \tmutex_unlock(&client->ports_mutex);\n-\tsprintf(new_port->name, \"port-%d\", num);\n \n \treturn new_port;\n }",
        "function_modified_lines": {
            "added": [
                "\tsnd_use_lock_use(&new_port->use_lock);",
                "\tsprintf(new_port->name, \"port-%d\", num);"
            ],
            "deleted": [
                "\tsprintf(new_port->name, \"port-%d\", num);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ALSA subsystem in the Linux kernel before 4.13.8 allows local users to cause a denial of service (use-after-free) or possibly have unspecified other impact via crafted /dev/snd/seq ioctl calls, related to sound/core/seq/seq_clientmgr.c and sound/core/seq/seq_ports.c.",
        "id": 1301
    },
    {
        "cve_id": "CVE-2014-4654",
        "code_before_change": "static int snd_ctl_elem_add(struct snd_ctl_file *file,\n\t\t\t    struct snd_ctl_elem_info *info, int replace)\n{\n\tstruct snd_card *card = file->card;\n\tstruct snd_kcontrol kctl, *_kctl;\n\tunsigned int access;\n\tlong private_size;\n\tstruct user_element *ue;\n\tint idx, err;\n\n\tif (!replace && card->user_ctl_count >= MAX_USER_CONTROLS)\n\t\treturn -ENOMEM;\n\tif (info->count < 1)\n\t\treturn -EINVAL;\n\taccess = info->access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE :\n\t\t(info->access & (SNDRV_CTL_ELEM_ACCESS_READWRITE|\n\t\t\t\t SNDRV_CTL_ELEM_ACCESS_INACTIVE|\n\t\t\t\t SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE));\n\tinfo->id.numid = 0;\n\tmemset(&kctl, 0, sizeof(kctl));\n\tdown_write(&card->controls_rwsem);\n\t_kctl = snd_ctl_find_id(card, &info->id);\n\terr = 0;\n\tif (_kctl) {\n\t\tif (replace)\n\t\t\terr = snd_ctl_remove(card, _kctl);\n\t\telse\n\t\t\terr = -EBUSY;\n\t} else {\n\t\tif (replace)\n\t\t\terr = -ENOENT;\n\t}\n\tup_write(&card->controls_rwsem);\n\tif (err < 0)\n\t\treturn err;\n\tmemcpy(&kctl.id, &info->id, sizeof(info->id));\n\tkctl.count = info->owner ? info->owner : 1;\n\taccess |= SNDRV_CTL_ELEM_ACCESS_USER;\n\tif (info->type == SNDRV_CTL_ELEM_TYPE_ENUMERATED)\n\t\tkctl.info = snd_ctl_elem_user_enum_info;\n\telse\n\t\tkctl.info = snd_ctl_elem_user_info;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_READ)\n\t\tkctl.get = snd_ctl_elem_user_get;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_WRITE)\n\t\tkctl.put = snd_ctl_elem_user_put;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE) {\n\t\tkctl.tlv.c = snd_ctl_elem_user_tlv;\n\t\taccess |= SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK;\n\t}\n\tswitch (info->type) {\n\tcase SNDRV_CTL_ELEM_TYPE_BOOLEAN:\n\tcase SNDRV_CTL_ELEM_TYPE_INTEGER:\n\t\tprivate_size = sizeof(long);\n\t\tif (info->count > 128)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_INTEGER64:\n\t\tprivate_size = sizeof(long long);\n\t\tif (info->count > 64)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_ENUMERATED:\n\t\tprivate_size = sizeof(unsigned int);\n\t\tif (info->count > 128 || info->value.enumerated.items == 0)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_BYTES:\n\t\tprivate_size = sizeof(unsigned char);\n\t\tif (info->count > 512)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_IEC958:\n\t\tprivate_size = sizeof(struct snd_aes_iec958);\n\t\tif (info->count != 1)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tprivate_size *= info->count;\n\tue = kzalloc(sizeof(struct user_element) + private_size, GFP_KERNEL);\n\tif (ue == NULL)\n\t\treturn -ENOMEM;\n\tue->card = card;\n\tue->info = *info;\n\tue->info.access = 0;\n\tue->elem_data = (char *)ue + sizeof(*ue);\n\tue->elem_data_size = private_size;\n\tif (ue->info.type == SNDRV_CTL_ELEM_TYPE_ENUMERATED) {\n\t\terr = snd_ctl_elem_init_enum_names(ue);\n\t\tif (err < 0) {\n\t\t\tkfree(ue);\n\t\t\treturn err;\n\t\t}\n\t}\n\tkctl.private_free = snd_ctl_elem_user_free;\n\t_kctl = snd_ctl_new(&kctl, access);\n\tif (_kctl == NULL) {\n\t\tkfree(ue->priv_data);\n\t\tkfree(ue);\n\t\treturn -ENOMEM;\n\t}\n\t_kctl->private_data = ue;\n\tfor (idx = 0; idx < _kctl->count; idx++)\n\t\t_kctl->vd[idx].owner = file;\n\terr = snd_ctl_add(card, _kctl);\n\tif (err < 0)\n\t\treturn err;\n\n\tdown_write(&card->controls_rwsem);\n\tcard->user_ctl_count++;\n\tup_write(&card->controls_rwsem);\n\n\treturn 0;\n}",
        "code_after_change": "static int snd_ctl_elem_add(struct snd_ctl_file *file,\n\t\t\t    struct snd_ctl_elem_info *info, int replace)\n{\n\tstruct snd_card *card = file->card;\n\tstruct snd_kcontrol kctl, *_kctl;\n\tunsigned int access;\n\tlong private_size;\n\tstruct user_element *ue;\n\tint idx, err;\n\n\tif (info->count < 1)\n\t\treturn -EINVAL;\n\taccess = info->access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE :\n\t\t(info->access & (SNDRV_CTL_ELEM_ACCESS_READWRITE|\n\t\t\t\t SNDRV_CTL_ELEM_ACCESS_INACTIVE|\n\t\t\t\t SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE));\n\tinfo->id.numid = 0;\n\tmemset(&kctl, 0, sizeof(kctl));\n\n\tif (replace) {\n\t\terr = snd_ctl_remove_user_ctl(file, &info->id);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (card->user_ctl_count >= MAX_USER_CONTROLS)\n\t\treturn -ENOMEM;\n\n\tmemcpy(&kctl.id, &info->id, sizeof(info->id));\n\tkctl.count = info->owner ? info->owner : 1;\n\taccess |= SNDRV_CTL_ELEM_ACCESS_USER;\n\tif (info->type == SNDRV_CTL_ELEM_TYPE_ENUMERATED)\n\t\tkctl.info = snd_ctl_elem_user_enum_info;\n\telse\n\t\tkctl.info = snd_ctl_elem_user_info;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_READ)\n\t\tkctl.get = snd_ctl_elem_user_get;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_WRITE)\n\t\tkctl.put = snd_ctl_elem_user_put;\n\tif (access & SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE) {\n\t\tkctl.tlv.c = snd_ctl_elem_user_tlv;\n\t\taccess |= SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK;\n\t}\n\tswitch (info->type) {\n\tcase SNDRV_CTL_ELEM_TYPE_BOOLEAN:\n\tcase SNDRV_CTL_ELEM_TYPE_INTEGER:\n\t\tprivate_size = sizeof(long);\n\t\tif (info->count > 128)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_INTEGER64:\n\t\tprivate_size = sizeof(long long);\n\t\tif (info->count > 64)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_ENUMERATED:\n\t\tprivate_size = sizeof(unsigned int);\n\t\tif (info->count > 128 || info->value.enumerated.items == 0)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_BYTES:\n\t\tprivate_size = sizeof(unsigned char);\n\t\tif (info->count > 512)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SNDRV_CTL_ELEM_TYPE_IEC958:\n\t\tprivate_size = sizeof(struct snd_aes_iec958);\n\t\tif (info->count != 1)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tprivate_size *= info->count;\n\tue = kzalloc(sizeof(struct user_element) + private_size, GFP_KERNEL);\n\tif (ue == NULL)\n\t\treturn -ENOMEM;\n\tue->card = card;\n\tue->info = *info;\n\tue->info.access = 0;\n\tue->elem_data = (char *)ue + sizeof(*ue);\n\tue->elem_data_size = private_size;\n\tif (ue->info.type == SNDRV_CTL_ELEM_TYPE_ENUMERATED) {\n\t\terr = snd_ctl_elem_init_enum_names(ue);\n\t\tif (err < 0) {\n\t\t\tkfree(ue);\n\t\t\treturn err;\n\t\t}\n\t}\n\tkctl.private_free = snd_ctl_elem_user_free;\n\t_kctl = snd_ctl_new(&kctl, access);\n\tif (_kctl == NULL) {\n\t\tkfree(ue->priv_data);\n\t\tkfree(ue);\n\t\treturn -ENOMEM;\n\t}\n\t_kctl->private_data = ue;\n\tfor (idx = 0; idx < _kctl->count; idx++)\n\t\t_kctl->vd[idx].owner = file;\n\terr = snd_ctl_add(card, _kctl);\n\tif (err < 0)\n\t\treturn err;\n\n\tdown_write(&card->controls_rwsem);\n\tcard->user_ctl_count++;\n\tup_write(&card->controls_rwsem);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,6 @@\n \tstruct user_element *ue;\n \tint idx, err;\n \n-\tif (!replace && card->user_ctl_count >= MAX_USER_CONTROLS)\n-\t\treturn -ENOMEM;\n \tif (info->count < 1)\n \t\treturn -EINVAL;\n \taccess = info->access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE :\n@@ -18,21 +16,16 @@\n \t\t\t\t SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE));\n \tinfo->id.numid = 0;\n \tmemset(&kctl, 0, sizeof(kctl));\n-\tdown_write(&card->controls_rwsem);\n-\t_kctl = snd_ctl_find_id(card, &info->id);\n-\terr = 0;\n-\tif (_kctl) {\n-\t\tif (replace)\n-\t\t\terr = snd_ctl_remove(card, _kctl);\n-\t\telse\n-\t\t\terr = -EBUSY;\n-\t} else {\n-\t\tif (replace)\n-\t\t\terr = -ENOENT;\n+\n+\tif (replace) {\n+\t\terr = snd_ctl_remove_user_ctl(file, &info->id);\n+\t\tif (err)\n+\t\t\treturn err;\n \t}\n-\tup_write(&card->controls_rwsem);\n-\tif (err < 0)\n-\t\treturn err;\n+\n+\tif (card->user_ctl_count >= MAX_USER_CONTROLS)\n+\t\treturn -ENOMEM;\n+\n \tmemcpy(&kctl.id, &info->id, sizeof(info->id));\n \tkctl.count = info->owner ? info->owner : 1;\n \taccess |= SNDRV_CTL_ELEM_ACCESS_USER;",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (replace) {",
                "\t\terr = snd_ctl_remove_user_ctl(file, &info->id);",
                "\t\tif (err)",
                "\t\t\treturn err;",
                "",
                "\tif (card->user_ctl_count >= MAX_USER_CONTROLS)",
                "\t\treturn -ENOMEM;",
                ""
            ],
            "deleted": [
                "\tif (!replace && card->user_ctl_count >= MAX_USER_CONTROLS)",
                "\t\treturn -ENOMEM;",
                "\tdown_write(&card->controls_rwsem);",
                "\t_kctl = snd_ctl_find_id(card, &info->id);",
                "\terr = 0;",
                "\tif (_kctl) {",
                "\t\tif (replace)",
                "\t\t\terr = snd_ctl_remove(card, _kctl);",
                "\t\telse",
                "\t\t\terr = -EBUSY;",
                "\t} else {",
                "\t\tif (replace)",
                "\t\t\terr = -ENOENT;",
                "\tup_write(&card->controls_rwsem);",
                "\tif (err < 0)",
                "\t\treturn err;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The snd_ctl_elem_add function in sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 does not check authorization for SNDRV_CTL_IOCTL_ELEM_REPLACE commands, which allows local users to remove kernel controls and cause a denial of service (use-after-free and system crash) by leveraging /dev/snd/controlCX access for an ioctl call.",
        "id": 571
    },
    {
        "cve_id": "CVE-2020-27835",
        "code_before_change": "static int pin_sdma_pages(struct user_sdma_request *req,\n\t\t\t  struct user_sdma_iovec *iovec,\n\t\t\t  struct sdma_mmu_node *node,\n\t\t\t  int npages)\n{\n\tint pinned, cleared;\n\tstruct page **pages;\n\tstruct hfi1_user_sdma_pkt_q *pq = req->pq;\n\n\tpages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\tmemcpy(pages, node->pages, node->npages * sizeof(*pages));\n\n\tnpages -= node->npages;\nretry:\n\tif (!hfi1_can_pin_pages(pq->dd, pq->mm,\n\t\t\t\tatomic_read(&pq->n_locked), npages)) {\n\t\tcleared = sdma_cache_evict(pq, npages);\n\t\tif (cleared >= npages)\n\t\t\tgoto retry;\n\t}\n\tpinned = hfi1_acquire_user_pages(pq->mm,\n\t\t\t\t\t ((unsigned long)iovec->iov.iov_base +\n\t\t\t\t\t (node->npages * PAGE_SIZE)), npages, 0,\n\t\t\t\t\t pages + node->npages);\n\tif (pinned < 0) {\n\t\tkfree(pages);\n\t\treturn pinned;\n\t}\n\tif (pinned != npages) {\n\t\tunpin_vector_pages(pq->mm, pages, node->npages, pinned);\n\t\treturn -EFAULT;\n\t}\n\tkfree(node->pages);\n\tnode->rb.len = iovec->iov.iov_len;\n\tnode->pages = pages;\n\tatomic_add(pinned, &pq->n_locked);\n\treturn pinned;\n}",
        "code_after_change": "static int pin_sdma_pages(struct user_sdma_request *req,\n\t\t\t  struct user_sdma_iovec *iovec,\n\t\t\t  struct sdma_mmu_node *node,\n\t\t\t  int npages)\n{\n\tint pinned, cleared;\n\tstruct page **pages;\n\tstruct hfi1_user_sdma_pkt_q *pq = req->pq;\n\n\tpages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\tmemcpy(pages, node->pages, node->npages * sizeof(*pages));\n\n\tnpages -= node->npages;\nretry:\n\tif (!hfi1_can_pin_pages(pq->dd, current->mm,\n\t\t\t\tatomic_read(&pq->n_locked), npages)) {\n\t\tcleared = sdma_cache_evict(pq, npages);\n\t\tif (cleared >= npages)\n\t\t\tgoto retry;\n\t}\n\tpinned = hfi1_acquire_user_pages(current->mm,\n\t\t\t\t\t ((unsigned long)iovec->iov.iov_base +\n\t\t\t\t\t (node->npages * PAGE_SIZE)), npages, 0,\n\t\t\t\t\t pages + node->npages);\n\tif (pinned < 0) {\n\t\tkfree(pages);\n\t\treturn pinned;\n\t}\n\tif (pinned != npages) {\n\t\tunpin_vector_pages(current->mm, pages, node->npages, pinned);\n\t\treturn -EFAULT;\n\t}\n\tkfree(node->pages);\n\tnode->rb.len = iovec->iov.iov_len;\n\tnode->pages = pages;\n\tatomic_add(pinned, &pq->n_locked);\n\treturn pinned;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,13 +14,13 @@\n \n \tnpages -= node->npages;\n retry:\n-\tif (!hfi1_can_pin_pages(pq->dd, pq->mm,\n+\tif (!hfi1_can_pin_pages(pq->dd, current->mm,\n \t\t\t\tatomic_read(&pq->n_locked), npages)) {\n \t\tcleared = sdma_cache_evict(pq, npages);\n \t\tif (cleared >= npages)\n \t\t\tgoto retry;\n \t}\n-\tpinned = hfi1_acquire_user_pages(pq->mm,\n+\tpinned = hfi1_acquire_user_pages(current->mm,\n \t\t\t\t\t ((unsigned long)iovec->iov.iov_base +\n \t\t\t\t\t (node->npages * PAGE_SIZE)), npages, 0,\n \t\t\t\t\t pages + node->npages);\n@@ -29,7 +29,7 @@\n \t\treturn pinned;\n \t}\n \tif (pinned != npages) {\n-\t\tunpin_vector_pages(pq->mm, pages, node->npages, pinned);\n+\t\tunpin_vector_pages(current->mm, pages, node->npages, pinned);\n \t\treturn -EFAULT;\n \t}\n \tkfree(node->pages);",
        "function_modified_lines": {
            "added": [
                "\tif (!hfi1_can_pin_pages(pq->dd, current->mm,",
                "\tpinned = hfi1_acquire_user_pages(current->mm,",
                "\t\tunpin_vector_pages(current->mm, pages, node->npages, pinned);"
            ],
            "deleted": [
                "\tif (!hfi1_can_pin_pages(pq->dd, pq->mm,",
                "\tpinned = hfi1_acquire_user_pages(pq->mm,",
                "\t\tunpin_vector_pages(pq->mm, pages, node->npages, pinned);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use after free in the Linux kernel infiniband hfi1 driver in versions prior to 5.10-rc6 was found in the way user calls Ioctl after open dev file and fork. A local user could use this flaw to crash the system.",
        "id": 2653
    },
    {
        "cve_id": "CVE-2022-42720",
        "code_before_change": "static struct cfg80211_bss *\ncfg80211_inform_single_bss_data(struct wiphy *wiphy,\n\t\t\t\tstruct cfg80211_inform_bss *data,\n\t\t\t\tenum cfg80211_bss_frame_type ftype,\n\t\t\t\tconst u8 *bssid, u64 tsf, u16 capability,\n\t\t\t\tu16 beacon_interval, const u8 *ie, size_t ielen,\n\t\t\t\tstruct cfg80211_non_tx_bss *non_tx_data,\n\t\t\t\tgfp_t gfp)\n{\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct cfg80211_bss_ies *ies;\n\tstruct ieee80211_channel *channel;\n\tstruct cfg80211_internal_bss tmp = {}, *res;\n\tint bss_type;\n\tbool signal_valid;\n\tunsigned long ts;\n\n\tif (WARN_ON(!wiphy))\n\t\treturn NULL;\n\n\tif (WARN_ON(wiphy->signal_type == CFG80211_SIGNAL_TYPE_UNSPEC &&\n\t\t    (data->signal < 0 || data->signal > 100)))\n\t\treturn NULL;\n\n\tchannel = cfg80211_get_bss_channel(wiphy, ie, ielen, data->chan,\n\t\t\t\t\t   data->scan_width, ftype);\n\tif (!channel)\n\t\treturn NULL;\n\n\tmemcpy(tmp.pub.bssid, bssid, ETH_ALEN);\n\ttmp.pub.channel = channel;\n\ttmp.pub.scan_width = data->scan_width;\n\ttmp.pub.signal = data->signal;\n\ttmp.pub.beacon_interval = beacon_interval;\n\ttmp.pub.capability = capability;\n\ttmp.ts_boottime = data->boottime_ns;\n\ttmp.parent_tsf = data->parent_tsf;\n\tether_addr_copy(tmp.parent_bssid, data->parent_bssid);\n\n\tif (non_tx_data) {\n\t\ttmp.pub.transmitted_bss = non_tx_data->tx_bss;\n\t\tts = bss_from_pub(non_tx_data->tx_bss)->ts;\n\t\ttmp.pub.bssid_index = non_tx_data->bssid_index;\n\t\ttmp.pub.max_bssid_indicator = non_tx_data->max_bssid_indicator;\n\t} else {\n\t\tts = jiffies;\n\t}\n\n\t/*\n\t * If we do not know here whether the IEs are from a Beacon or Probe\n\t * Response frame, we need to pick one of the options and only use it\n\t * with the driver that does not provide the full Beacon/Probe Response\n\t * frame. Use Beacon frame pointer to avoid indicating that this should\n\t * override the IEs pointer should we have received an earlier\n\t * indication of Probe Response data.\n\t */\n\ties = kzalloc(sizeof(*ies) + ielen, gfp);\n\tif (!ies)\n\t\treturn NULL;\n\ties->len = ielen;\n\ties->tsf = tsf;\n\ties->from_beacon = false;\n\tmemcpy(ies->data, ie, ielen);\n\n\tswitch (ftype) {\n\tcase CFG80211_BSS_FTYPE_BEACON:\n\t\ties->from_beacon = true;\n\t\tfallthrough;\n\tcase CFG80211_BSS_FTYPE_UNKNOWN:\n\t\trcu_assign_pointer(tmp.pub.beacon_ies, ies);\n\t\tbreak;\n\tcase CFG80211_BSS_FTYPE_PRESP:\n\t\trcu_assign_pointer(tmp.pub.proberesp_ies, ies);\n\t\tbreak;\n\t}\n\trcu_assign_pointer(tmp.pub.ies, ies);\n\n\tsignal_valid = data->chan == channel;\n\tres = cfg80211_bss_update(wiphy_to_rdev(wiphy), &tmp, signal_valid, ts);\n\tif (!res)\n\t\treturn NULL;\n\n\tif (channel->band == NL80211_BAND_60GHZ) {\n\t\tbss_type = res->pub.capability & WLAN_CAPABILITY_DMG_TYPE_MASK;\n\t\tif (bss_type == WLAN_CAPABILITY_DMG_TYPE_AP ||\n\t\t    bss_type == WLAN_CAPABILITY_DMG_TYPE_PBSS)\n\t\t\tregulatory_hint_found_beacon(wiphy, channel, gfp);\n\t} else {\n\t\tif (res->pub.capability & WLAN_CAPABILITY_ESS)\n\t\t\tregulatory_hint_found_beacon(wiphy, channel, gfp);\n\t}\n\n\tif (non_tx_data) {\n\t\t/* this is a nontransmitting bss, we need to add it to\n\t\t * transmitting bss' list if it is not there\n\t\t */\n\t\tspin_lock_bh(&rdev->bss_lock);\n\t\tif (cfg80211_add_nontrans_list(non_tx_data->tx_bss,\n\t\t\t\t\t       &res->pub)) {\n\t\t\tif (__cfg80211_unlink_bss(rdev, res))\n\t\t\t\trdev->bss_generation++;\n\t\t}\n\t\tspin_unlock_bh(&rdev->bss_lock);\n\t}\n\n\ttrace_cfg80211_return_bss(&res->pub);\n\t/* cfg80211_bss_update gives us a referenced result */\n\treturn &res->pub;\n}",
        "code_after_change": "static struct cfg80211_bss *\ncfg80211_inform_single_bss_data(struct wiphy *wiphy,\n\t\t\t\tstruct cfg80211_inform_bss *data,\n\t\t\t\tenum cfg80211_bss_frame_type ftype,\n\t\t\t\tconst u8 *bssid, u64 tsf, u16 capability,\n\t\t\t\tu16 beacon_interval, const u8 *ie, size_t ielen,\n\t\t\t\tstruct cfg80211_non_tx_bss *non_tx_data,\n\t\t\t\tgfp_t gfp)\n{\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct cfg80211_bss_ies *ies;\n\tstruct ieee80211_channel *channel;\n\tstruct cfg80211_internal_bss tmp = {}, *res;\n\tint bss_type;\n\tbool signal_valid;\n\tunsigned long ts;\n\n\tif (WARN_ON(!wiphy))\n\t\treturn NULL;\n\n\tif (WARN_ON(wiphy->signal_type == CFG80211_SIGNAL_TYPE_UNSPEC &&\n\t\t    (data->signal < 0 || data->signal > 100)))\n\t\treturn NULL;\n\n\tchannel = cfg80211_get_bss_channel(wiphy, ie, ielen, data->chan,\n\t\t\t\t\t   data->scan_width, ftype);\n\tif (!channel)\n\t\treturn NULL;\n\n\tmemcpy(tmp.pub.bssid, bssid, ETH_ALEN);\n\ttmp.pub.channel = channel;\n\ttmp.pub.scan_width = data->scan_width;\n\ttmp.pub.signal = data->signal;\n\ttmp.pub.beacon_interval = beacon_interval;\n\ttmp.pub.capability = capability;\n\ttmp.ts_boottime = data->boottime_ns;\n\ttmp.parent_tsf = data->parent_tsf;\n\tether_addr_copy(tmp.parent_bssid, data->parent_bssid);\n\n\tif (non_tx_data) {\n\t\ttmp.pub.transmitted_bss = non_tx_data->tx_bss;\n\t\tts = bss_from_pub(non_tx_data->tx_bss)->ts;\n\t\ttmp.pub.bssid_index = non_tx_data->bssid_index;\n\t\ttmp.pub.max_bssid_indicator = non_tx_data->max_bssid_indicator;\n\t} else {\n\t\tts = jiffies;\n\t}\n\n\t/*\n\t * If we do not know here whether the IEs are from a Beacon or Probe\n\t * Response frame, we need to pick one of the options and only use it\n\t * with the driver that does not provide the full Beacon/Probe Response\n\t * frame. Use Beacon frame pointer to avoid indicating that this should\n\t * override the IEs pointer should we have received an earlier\n\t * indication of Probe Response data.\n\t */\n\ties = kzalloc(sizeof(*ies) + ielen, gfp);\n\tif (!ies)\n\t\treturn NULL;\n\ties->len = ielen;\n\ties->tsf = tsf;\n\ties->from_beacon = false;\n\tmemcpy(ies->data, ie, ielen);\n\n\tswitch (ftype) {\n\tcase CFG80211_BSS_FTYPE_BEACON:\n\t\ties->from_beacon = true;\n\t\tfallthrough;\n\tcase CFG80211_BSS_FTYPE_UNKNOWN:\n\t\trcu_assign_pointer(tmp.pub.beacon_ies, ies);\n\t\tbreak;\n\tcase CFG80211_BSS_FTYPE_PRESP:\n\t\trcu_assign_pointer(tmp.pub.proberesp_ies, ies);\n\t\tbreak;\n\t}\n\trcu_assign_pointer(tmp.pub.ies, ies);\n\n\tsignal_valid = data->chan == channel;\n\tres = cfg80211_bss_update(wiphy_to_rdev(wiphy), &tmp, signal_valid, ts);\n\tif (!res)\n\t\treturn NULL;\n\n\tif (channel->band == NL80211_BAND_60GHZ) {\n\t\tbss_type = res->pub.capability & WLAN_CAPABILITY_DMG_TYPE_MASK;\n\t\tif (bss_type == WLAN_CAPABILITY_DMG_TYPE_AP ||\n\t\t    bss_type == WLAN_CAPABILITY_DMG_TYPE_PBSS)\n\t\t\tregulatory_hint_found_beacon(wiphy, channel, gfp);\n\t} else {\n\t\tif (res->pub.capability & WLAN_CAPABILITY_ESS)\n\t\t\tregulatory_hint_found_beacon(wiphy, channel, gfp);\n\t}\n\n\tif (non_tx_data) {\n\t\t/* this is a nontransmitting bss, we need to add it to\n\t\t * transmitting bss' list if it is not there\n\t\t */\n\t\tspin_lock_bh(&rdev->bss_lock);\n\t\tif (cfg80211_add_nontrans_list(non_tx_data->tx_bss,\n\t\t\t\t\t       &res->pub)) {\n\t\t\tif (__cfg80211_unlink_bss(rdev, res)) {\n\t\t\t\trdev->bss_generation++;\n\t\t\t\tres = NULL;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_bh(&rdev->bss_lock);\n\n\t\tif (!res)\n\t\t\treturn NULL;\n\t}\n\n\ttrace_cfg80211_return_bss(&res->pub);\n\t/* cfg80211_bss_update gives us a referenced result */\n\treturn &res->pub;\n}",
        "patch": "--- code before\n+++ code after\n@@ -97,10 +97,15 @@\n \t\tspin_lock_bh(&rdev->bss_lock);\n \t\tif (cfg80211_add_nontrans_list(non_tx_data->tx_bss,\n \t\t\t\t\t       &res->pub)) {\n-\t\t\tif (__cfg80211_unlink_bss(rdev, res))\n+\t\t\tif (__cfg80211_unlink_bss(rdev, res)) {\n \t\t\t\trdev->bss_generation++;\n+\t\t\t\tres = NULL;\n+\t\t\t}\n \t\t}\n \t\tspin_unlock_bh(&rdev->bss_lock);\n+\n+\t\tif (!res)\n+\t\t\treturn NULL;\n \t}\n \n \ttrace_cfg80211_return_bss(&res->pub);",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (__cfg80211_unlink_bss(rdev, res)) {",
                "\t\t\t\tres = NULL;",
                "\t\t\t}",
                "",
                "\t\tif (!res)",
                "\t\t\treturn NULL;"
            ],
            "deleted": [
                "\t\t\tif (__cfg80211_unlink_bss(rdev, res))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "Various refcounting bugs in the multi-BSS handling in the mac80211 stack in the Linux kernel 5.1 through 5.19.x before 5.19.16 could be used by local attackers (able to inject WLAN frames) to trigger use-after-free conditions to potentially execute code.",
        "id": 3734
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
        "code_after_change": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\tif (worker->saved_creds) {\n\t\trevert_creds(worker->saved_creds);\n\t\tworker->cur_creds = worker->saved_creds = NULL;\n\t}\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,11 @@\n \tworker->flags = 0;\n \tpreempt_enable();\n \n+\tif (worker->saved_creds) {\n+\t\trevert_creds(worker->saved_creds);\n+\t\tworker->cur_creds = worker->saved_creds = NULL;\n+\t}\n+\n \traw_spin_lock_irq(&wqe->lock);\n \thlist_nulls_del_rcu(&worker->nulls_node);\n \tlist_del_rcu(&worker->all_list);",
        "function_modified_lines": {
            "added": [
                "\tif (worker->saved_creds) {",
                "\t\trevert_creds(worker->saved_creds);",
                "\t\tworker->cur_creds = worker->saved_creds = NULL;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3352
    },
    {
        "cve_id": "CVE-2017-16939",
        "code_before_change": "static int xfrm_dump_policy_done(struct netlink_callback *cb)\n{\n\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];\n\tstruct net *net = sock_net(cb->skb->sk);\n\n\txfrm_policy_walk_done(walk, net);\n\treturn 0;\n}",
        "code_after_change": "static int xfrm_dump_policy_done(struct netlink_callback *cb)\n{\n\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;\n\tstruct net *net = sock_net(cb->skb->sk);\n\n\txfrm_policy_walk_done(walk, net);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static int xfrm_dump_policy_done(struct netlink_callback *cb)\n {\n-\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];\n+\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;\n \tstruct net *net = sock_net(cb->skb->sk);\n \n \txfrm_policy_walk_done(walk, net);",
        "function_modified_lines": {
            "added": [
                "\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;"
            ],
            "deleted": [
                "\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The XFRM dump policy implementation in net/xfrm/xfrm_user.c in the Linux kernel before 4.13.11 allows local users to gain privileges or cause a denial of service (use-after-free) via a crafted SO_RCVBUF setsockopt system call in conjunction with XFRM_MSG_GETPOLICY Netlink messages.",
        "id": 1352
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "void blk_trace_shutdown(struct request_queue *q)\n{\n\tmutex_lock(&q->blk_trace_mutex);\n\n\tif (q->blk_trace) {\n\t\t__blk_trace_startstop(q, 0);\n\t\t__blk_trace_remove(q);\n\t}\n\n\tmutex_unlock(&q->blk_trace_mutex);\n}",
        "code_after_change": "void blk_trace_shutdown(struct request_queue *q)\n{\n\tmutex_lock(&q->blk_trace_mutex);\n\tif (rcu_dereference_protected(q->blk_trace,\n\t\t\t\t      lockdep_is_held(&q->blk_trace_mutex))) {\n\t\t__blk_trace_startstop(q, 0);\n\t\t__blk_trace_remove(q);\n\t}\n\n\tmutex_unlock(&q->blk_trace_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,8 @@\n void blk_trace_shutdown(struct request_queue *q)\n {\n \tmutex_lock(&q->blk_trace_mutex);\n-\n-\tif (q->blk_trace) {\n+\tif (rcu_dereference_protected(q->blk_trace,\n+\t\t\t\t      lockdep_is_held(&q->blk_trace_mutex))) {\n \t\t__blk_trace_startstop(q, 0);\n \t\t__blk_trace_remove(q);\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif (rcu_dereference_protected(q->blk_trace,",
                "\t\t\t\t      lockdep_is_held(&q->blk_trace_mutex))) {"
            ],
            "deleted": [
                "",
                "\tif (q->blk_trace) {"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2236
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static vm_fault_t kvmppc_uvmem_migrate_to_ram(struct vm_fault *vmf)\n{\n\tstruct kvmppc_uvmem_page_pvt *pvt = vmf->page->zone_device_data;\n\n\tif (kvmppc_svm_page_out(vmf->vma, vmf->address,\n\t\t\t\tvmf->address + PAGE_SIZE, PAGE_SHIFT,\n\t\t\t\tpvt->kvm, pvt->gpa))\n\t\treturn VM_FAULT_SIGBUS;\n\telse\n\t\treturn 0;\n}",
        "code_after_change": "static vm_fault_t kvmppc_uvmem_migrate_to_ram(struct vm_fault *vmf)\n{\n\tstruct kvmppc_uvmem_page_pvt *pvt = vmf->page->zone_device_data;\n\n\tif (kvmppc_svm_page_out(vmf->vma, vmf->address,\n\t\t\t\tvmf->address + PAGE_SIZE, PAGE_SHIFT,\n\t\t\t\tpvt->kvm, pvt->gpa, vmf->page))\n\t\treturn VM_FAULT_SIGBUS;\n\telse\n\t\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \n \tif (kvmppc_svm_page_out(vmf->vma, vmf->address,\n \t\t\t\tvmf->address + PAGE_SIZE, PAGE_SHIFT,\n-\t\t\t\tpvt->kvm, pvt->gpa))\n+\t\t\t\tpvt->kvm, pvt->gpa, vmf->page))\n \t\treturn VM_FAULT_SIGBUS;\n \telse\n \t\treturn 0;",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tpvt->kvm, pvt->gpa, vmf->page))"
            ],
            "deleted": [
                "\t\t\t\tpvt->kvm, pvt->gpa))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3607
    },
    {
        "cve_id": "CVE-2022-47946",
        "code_before_change": "static int __io_sq_thread(struct io_ring_ctx *ctx, bool cap_entries)\n{\n\tunsigned int to_submit;\n\tint ret = 0;\n\n\tto_submit = io_sqring_entries(ctx);\n\t/* if we're handling multiple rings, cap submit size for fairness */\n\tif (cap_entries && to_submit > 8)\n\t\tto_submit = 8;\n\n\tif (!list_empty(&ctx->iopoll_list) || to_submit) {\n\t\tunsigned nr_events = 0;\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tif (!list_empty(&ctx->iopoll_list))\n\t\t\tio_do_iopoll(ctx, &nr_events, 0);\n\n\t\tif (to_submit && !ctx->sqo_dead &&\n\t\t    likely(!percpu_ref_is_dying(&ctx->refs)))\n\t\t\tret = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\tif (!io_sqring_full(ctx) && wq_has_sleeper(&ctx->sqo_sq_wait))\n\t\twake_up(&ctx->sqo_sq_wait);\n\n\treturn ret;\n}",
        "code_after_change": "static int __io_sq_thread(struct io_ring_ctx *ctx, bool cap_entries)\n{\n\tunsigned int to_submit;\n\tint ret = 0;\n\n\tto_submit = io_sqring_entries(ctx);\n\t/* if we're handling multiple rings, cap submit size for fairness */\n\tif (cap_entries && to_submit > 8)\n\t\tto_submit = 8;\n\n\tif (!list_empty(&ctx->iopoll_list) || to_submit) {\n\t\tunsigned nr_events = 0;\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tif (!list_empty(&ctx->iopoll_list))\n\t\t\tio_do_iopoll(ctx, &nr_events, 0);\n\n\t\tif (to_submit && likely(!percpu_ref_is_dying(&ctx->refs)))\n\t\t\tret = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\tif (!io_sqring_full(ctx) && wq_has_sleeper(&ctx->sqo_sq_wait))\n\t\twake_up(&ctx->sqo_sq_wait);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,8 +15,7 @@\n \t\tif (!list_empty(&ctx->iopoll_list))\n \t\t\tio_do_iopoll(ctx, &nr_events, 0);\n \n-\t\tif (to_submit && !ctx->sqo_dead &&\n-\t\t    likely(!percpu_ref_is_dying(&ctx->refs)))\n+\t\tif (to_submit && likely(!percpu_ref_is_dying(&ctx->refs)))\n \t\t\tret = io_submit_sqes(ctx, to_submit);\n \t\tmutex_unlock(&ctx->uring_lock);\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (to_submit && likely(!percpu_ref_is_dying(&ctx->refs)))"
            ],
            "deleted": [
                "\t\tif (to_submit && !ctx->sqo_dead &&",
                "\t\t    likely(!percpu_ref_is_dying(&ctx->refs)))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 5.10.x before 5.10.155. A use-after-free in io_sqpoll_wait_sq in fs/io_uring.c allows an attacker to crash the kernel, resulting in denial of service. finish_wait can be skipped. An attack can occur in some situations by forking a process and then quickly terminating it. NOTE: later kernel versions, such as the 5.15 longterm series, substantially changed the implementation of io_sqpoll_wait_sq.",
        "id": 3784
    },
    {
        "cve_id": "CVE-2023-5345",
        "code_before_change": "static int smb3_fs_context_parse_param(struct fs_context *fc,\n\t\t\t\t      struct fs_parameter *param)\n{\n\tstruct fs_parse_result result;\n\tstruct smb3_fs_context *ctx = smb3_fc2context(fc);\n\tint i, opt;\n\tbool is_smb3 = !strcmp(fc->fs_type->name, \"smb3\");\n\tbool skip_parsing = false;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tcifs_dbg(FYI, \"CIFS: parsing cifs mount option '%s'\\n\", param->key);\n\n\t/*\n\t * fs_parse can not handle string options with an empty value so\n\t * we will need special handling of them.\n\t */\n\tif (param->type == fs_value_is_string && param->string[0] == 0) {\n\t\tif (!strcmp(\"pass\", param->key) || !strcmp(\"password\", param->key)) {\n\t\t\tskip_parsing = true;\n\t\t\topt = Opt_pass;\n\t\t} else if (!strcmp(\"user\", param->key) || !strcmp(\"username\", param->key)) {\n\t\t\tskip_parsing = true;\n\t\t\topt = Opt_user;\n\t\t}\n\t}\n\n\tif (!skip_parsing) {\n\t\topt = fs_parse(fc, smb3_fs_parameters, param, &result);\n\t\tif (opt < 0)\n\t\t\treturn ctx->sloppy ? 1 : opt;\n\t}\n\n\tswitch (opt) {\n\tcase Opt_compress:\n\t\tctx->compression = UNKNOWN_TYPE;\n\t\tcifs_dbg(VFS,\n\t\t\t\"SMB3 compression support is experimental\\n\");\n\t\tbreak;\n\tcase Opt_nodfs:\n\t\tctx->nodfs = 1;\n\t\tbreak;\n\tcase Opt_hard:\n\t\tif (result.negated) {\n\t\t\tif (ctx->retry == 1)\n\t\t\t\tcifs_dbg(VFS, \"conflicting hard vs. soft mount options\\n\");\n\t\t\tctx->retry = 0;\n\t\t} else\n\t\t\tctx->retry = 1;\n\t\tbreak;\n\tcase Opt_soft:\n\t\tif (result.negated)\n\t\t\tctx->retry = 1;\n\t\telse {\n\t\t\tif (ctx->retry == 1)\n\t\t\t\tcifs_dbg(VFS, \"conflicting hard vs soft mount options\\n\");\n\t\t\tctx->retry = 0;\n\t\t}\n\t\tbreak;\n\tcase Opt_mapposix:\n\t\tif (result.negated)\n\t\t\tctx->remap = false;\n\t\telse {\n\t\t\tctx->remap = true;\n\t\t\tctx->sfu_remap = false; /* disable SFU mapping */\n\t\t}\n\t\tbreak;\n\tcase Opt_mapchars:\n\t\tif (result.negated)\n\t\t\tctx->sfu_remap = false;\n\t\telse {\n\t\t\tctx->sfu_remap = true;\n\t\t\tctx->remap = false; /* disable SFM (mapposix) mapping */\n\t\t}\n\t\tbreak;\n\tcase Opt_user_xattr:\n\t\tif (result.negated)\n\t\t\tctx->no_xattr = 1;\n\t\telse\n\t\t\tctx->no_xattr = 0;\n\t\tbreak;\n\tcase Opt_forceuid:\n\t\tif (result.negated)\n\t\t\tctx->override_uid = 0;\n\t\telse\n\t\t\tctx->override_uid = 1;\n\t\tbreak;\n\tcase Opt_forcegid:\n\t\tif (result.negated)\n\t\t\tctx->override_gid = 0;\n\t\telse\n\t\t\tctx->override_gid = 1;\n\t\tbreak;\n\tcase Opt_perm:\n\t\tif (result.negated)\n\t\t\tctx->noperm = 1;\n\t\telse\n\t\t\tctx->noperm = 0;\n\t\tbreak;\n\tcase Opt_dynperm:\n\t\tif (result.negated)\n\t\t\tctx->dynperm = 0;\n\t\telse\n\t\t\tctx->dynperm = 1;\n\t\tbreak;\n\tcase Opt_sfu:\n\t\tif (result.negated)\n\t\t\tctx->sfu_emul = 0;\n\t\telse\n\t\t\tctx->sfu_emul = 1;\n\t\tbreak;\n\tcase Opt_noblocksend:\n\t\tctx->noblocksnd = 1;\n\t\tbreak;\n\tcase Opt_noautotune:\n\t\tctx->noautotune = 1;\n\t\tbreak;\n\tcase Opt_nolease:\n\t\tctx->no_lease = 1;\n\t\tbreak;\n\tcase Opt_nosparse:\n\t\tctx->no_sparse = 1;\n\t\tbreak;\n\tcase Opt_nodelete:\n\t\tctx->nodelete = 1;\n\t\tbreak;\n\tcase Opt_multichannel:\n\t\tif (result.negated) {\n\t\t\tctx->multichannel = false;\n\t\t\tctx->max_channels = 1;\n\t\t} else {\n\t\t\tctx->multichannel = true;\n\t\t\t/* if number of channels not specified, default to 2 */\n\t\t\tif (ctx->max_channels < 2)\n\t\t\t\tctx->max_channels = 2;\n\t\t}\n\t\tbreak;\n\tcase Opt_uid:\n\t\tuid = make_kuid(current_user_ns(), result.uint_32);\n\t\tif (!uid_valid(uid))\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->linux_uid = uid;\n\t\tctx->uid_specified = true;\n\t\tbreak;\n\tcase Opt_cruid:\n\t\tuid = make_kuid(current_user_ns(), result.uint_32);\n\t\tif (!uid_valid(uid))\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->cred_uid = uid;\n\t\tctx->cruid_specified = true;\n\t\tbreak;\n\tcase Opt_backupuid:\n\t\tuid = make_kuid(current_user_ns(), result.uint_32);\n\t\tif (!uid_valid(uid))\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->backupuid = uid;\n\t\tctx->backupuid_specified = true;\n\t\tbreak;\n\tcase Opt_backupgid:\n\t\tgid = make_kgid(current_user_ns(), result.uint_32);\n\t\tif (!gid_valid(gid))\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->backupgid = gid;\n\t\tctx->backupgid_specified = true;\n\t\tbreak;\n\tcase Opt_gid:\n\t\tgid = make_kgid(current_user_ns(), result.uint_32);\n\t\tif (!gid_valid(gid))\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->linux_gid = gid;\n\t\tctx->gid_specified = true;\n\t\tbreak;\n\tcase Opt_port:\n\t\tctx->port = result.uint_32;\n\t\tbreak;\n\tcase Opt_file_mode:\n\t\tctx->file_mode = result.uint_32;\n\t\tbreak;\n\tcase Opt_dirmode:\n\t\tctx->dir_mode = result.uint_32;\n\t\tbreak;\n\tcase Opt_min_enc_offload:\n\t\tctx->min_offload = result.uint_32;\n\t\tbreak;\n\tcase Opt_blocksize:\n\t\t/*\n\t\t * inode blocksize realistically should never need to be\n\t\t * less than 16K or greater than 16M and default is 1MB.\n\t\t * Note that small inode block sizes (e.g. 64K) can lead\n\t\t * to very poor performance of common tools like cp and scp\n\t\t */\n\t\tif ((result.uint_32 < CIFS_MAX_MSGSIZE) ||\n\t\t   (result.uint_32 > (4 * SMB3_DEFAULT_IOSIZE))) {\n\t\t\tcifs_errorf(fc, \"%s: Invalid blocksize\\n\",\n\t\t\t\t__func__);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->bsize = result.uint_32;\n\t\tctx->got_bsize = true;\n\t\tbreak;\n\tcase Opt_rasize:\n\t\t/*\n\t\t * readahead size realistically should never need to be\n\t\t * less than 1M (CIFS_DEFAULT_IOSIZE) or greater than 32M\n\t\t * (perhaps an exception should be considered in the\n\t\t * for the case of a large number of channels\n\t\t * when multichannel is negotiated) since that would lead\n\t\t * to plenty of parallel I/O in flight to the server.\n\t\t * Note that smaller read ahead sizes would\n\t\t * hurt performance of common tools like cp and scp\n\t\t * which often trigger sequential i/o with read ahead\n\t\t */\n\t\tif ((result.uint_32 > (8 * SMB3_DEFAULT_IOSIZE)) ||\n\t\t    (result.uint_32 < CIFS_DEFAULT_IOSIZE)) {\n\t\t\tcifs_errorf(fc, \"%s: Invalid rasize %d vs. %d\\n\",\n\t\t\t\t__func__, result.uint_32, SMB3_DEFAULT_IOSIZE);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->rasize = result.uint_32;\n\t\tbreak;\n\tcase Opt_rsize:\n\t\tctx->rsize = result.uint_32;\n\t\tctx->got_rsize = true;\n\t\tbreak;\n\tcase Opt_wsize:\n\t\tctx->wsize = result.uint_32;\n\t\tctx->got_wsize = true;\n\t\tbreak;\n\tcase Opt_acregmax:\n\t\tctx->acregmax = HZ * result.uint_32;\n\t\tif (ctx->acregmax > CIFS_MAX_ACTIMEO) {\n\t\t\tcifs_errorf(fc, \"acregmax too large\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_acdirmax:\n\t\tctx->acdirmax = HZ * result.uint_32;\n\t\tif (ctx->acdirmax > CIFS_MAX_ACTIMEO) {\n\t\t\tcifs_errorf(fc, \"acdirmax too large\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_actimeo:\n\t\tif (HZ * result.uint_32 > CIFS_MAX_ACTIMEO) {\n\t\t\tcifs_errorf(fc, \"timeout too large\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tif ((ctx->acdirmax != CIFS_DEF_ACTIMEO) ||\n\t\t    (ctx->acregmax != CIFS_DEF_ACTIMEO)) {\n\t\t\tcifs_errorf(fc, \"actimeo ignored since acregmax or acdirmax specified\\n\");\n\t\t\tbreak;\n\t\t}\n\t\tctx->acdirmax = ctx->acregmax = HZ * result.uint_32;\n\t\tbreak;\n\tcase Opt_closetimeo:\n\t\tctx->closetimeo = HZ * result.uint_32;\n\t\tif (ctx->closetimeo > SMB3_MAX_DCLOSETIMEO) {\n\t\t\tcifs_errorf(fc, \"closetimeo too large\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_echo_interval:\n\t\tctx->echo_interval = result.uint_32;\n\t\tbreak;\n\tcase Opt_snapshot:\n\t\tctx->snapshot_time = result.uint_64;\n\t\tbreak;\n\tcase Opt_max_credits:\n\t\tif (result.uint_32 < 20 || result.uint_32 > 60000) {\n\t\t\tcifs_errorf(fc, \"%s: Invalid max_credits value\\n\",\n\t\t\t\t __func__);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->max_credits = result.uint_32;\n\t\tbreak;\n\tcase Opt_max_channels:\n\t\tif (result.uint_32 < 1 || result.uint_32 > CIFS_MAX_CHANNELS) {\n\t\t\tcifs_errorf(fc, \"%s: Invalid max_channels value, needs to be 1-%d\\n\",\n\t\t\t\t __func__, CIFS_MAX_CHANNELS);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->max_channels = result.uint_32;\n\t\t/* If more than one channel requested ... they want multichan */\n\t\tif (result.uint_32 > 1)\n\t\t\tctx->multichannel = true;\n\t\tbreak;\n\tcase Opt_max_cached_dirs:\n\t\tif (result.uint_32 < 1) {\n\t\t\tcifs_errorf(fc, \"%s: Invalid max_cached_dirs, needs to be 1 or more\\n\",\n\t\t\t\t    __func__);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->max_cached_dirs = result.uint_32;\n\t\tbreak;\n\tcase Opt_handletimeout:\n\t\tctx->handle_timeout = result.uint_32;\n\t\tif (ctx->handle_timeout > SMB3_MAX_HANDLE_TIMEOUT) {\n\t\t\tcifs_errorf(fc, \"Invalid handle cache timeout, longer than 16 minutes\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_source:\n\t\tkfree(ctx->UNC);\n\t\tctx->UNC = NULL;\n\t\tswitch (smb3_parse_devname(param->string, ctx)) {\n\t\tcase 0:\n\t\t\tbreak;\n\t\tcase -ENOMEM:\n\t\t\tcifs_errorf(fc, \"Unable to allocate memory for devname\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\tcase -EINVAL:\n\t\t\tcifs_errorf(fc, \"Malformed UNC in devname\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\tdefault:\n\t\t\tcifs_errorf(fc, \"Unknown error parsing devname\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->source = smb3_fs_context_fullpath(ctx, '/');\n\t\tif (IS_ERR(ctx->source)) {\n\t\t\tctx->source = NULL;\n\t\t\tcifs_errorf(fc, \"OOM when copying UNC string\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tfc->source = kstrdup(ctx->source, GFP_KERNEL);\n\t\tif (fc->source == NULL) {\n\t\t\tcifs_errorf(fc, \"OOM when copying UNC string\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_user:\n\t\tkfree(ctx->username);\n\t\tctx->username = NULL;\n\t\tif (ctx->nullauth)\n\t\t\tbreak;\n\t\tif (strlen(param->string) == 0) {\n\t\t\t/* null user, ie. anonymous authentication */\n\t\t\tctx->nullauth = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (strnlen(param->string, CIFS_MAX_USERNAME_LEN) >\n\t\t    CIFS_MAX_USERNAME_LEN) {\n\t\t\tpr_warn(\"username too long\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->username = kstrdup(param->string, GFP_KERNEL);\n\t\tif (ctx->username == NULL) {\n\t\t\tcifs_errorf(fc, \"OOM when copying username string\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_pass:\n\t\tkfree_sensitive(ctx->password);\n\t\tctx->password = NULL;\n\t\tif (strlen(param->string) == 0)\n\t\t\tbreak;\n\n\t\tctx->password = kstrdup(param->string, GFP_KERNEL);\n\t\tif (ctx->password == NULL) {\n\t\t\tcifs_errorf(fc, \"OOM when copying password string\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_ip:\n\t\tif (strlen(param->string) == 0) {\n\t\t\tctx->got_ip = false;\n\t\t\tbreak;\n\t\t}\n\t\tif (!cifs_convert_address((struct sockaddr *)&ctx->dstaddr,\n\t\t\t\t\t  param->string,\n\t\t\t\t\t  strlen(param->string))) {\n\t\t\tpr_err(\"bad ip= option (%s)\\n\", param->string);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->got_ip = true;\n\t\tbreak;\n\tcase Opt_domain:\n\t\tif (strnlen(param->string, CIFS_MAX_DOMAINNAME_LEN)\n\t\t\t\t== CIFS_MAX_DOMAINNAME_LEN) {\n\t\t\tpr_warn(\"domain name too long\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\n\t\tkfree(ctx->domainname);\n\t\tctx->domainname = kstrdup(param->string, GFP_KERNEL);\n\t\tif (ctx->domainname == NULL) {\n\t\t\tcifs_errorf(fc, \"OOM when copying domainname string\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tcifs_dbg(FYI, \"Domain name set\\n\");\n\t\tbreak;\n\tcase Opt_srcaddr:\n\t\tif (!cifs_convert_address(\n\t\t\t\t(struct sockaddr *)&ctx->srcaddr,\n\t\t\t\tparam->string, strlen(param->string))) {\n\t\t\tpr_warn(\"Could not parse srcaddr: %s\\n\",\n\t\t\t\tparam->string);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_iocharset:\n\t\tif (strnlen(param->string, 1024) >= 65) {\n\t\t\tpr_warn(\"iocharset name too long\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\n\t\tif (strncasecmp(param->string, \"default\", 7) != 0) {\n\t\t\tkfree(ctx->iocharset);\n\t\t\tctx->iocharset = kstrdup(param->string, GFP_KERNEL);\n\t\t\tif (ctx->iocharset == NULL) {\n\t\t\t\tcifs_errorf(fc, \"OOM when copying iocharset string\\n\");\n\t\t\t\tgoto cifs_parse_mount_err;\n\t\t\t}\n\t\t}\n\t\t/* if iocharset not set then load_nls_default\n\t\t * is used by caller\n\t\t */\n\t\tcifs_dbg(FYI, \"iocharset set to %s\\n\", ctx->iocharset);\n\t\tbreak;\n\tcase Opt_netbiosname:\n\t\tmemset(ctx->source_rfc1001_name, 0x20,\n\t\t\tRFC1001_NAME_LEN);\n\t\t/*\n\t\t * FIXME: are there cases in which a comma can\n\t\t * be valid in workstation netbios name (and\n\t\t * need special handling)?\n\t\t */\n\t\tfor (i = 0; i < RFC1001_NAME_LEN; i++) {\n\t\t\t/* don't ucase netbiosname for user */\n\t\t\tif (param->string[i] == 0)\n\t\t\t\tbreak;\n\t\t\tctx->source_rfc1001_name[i] = param->string[i];\n\t\t}\n\t\t/* The string has 16th byte zero still from\n\t\t * set at top of the function\n\t\t */\n\t\tif (i == RFC1001_NAME_LEN && param->string[i] != 0)\n\t\t\tpr_warn(\"netbiosname longer than 15 truncated\\n\");\n\t\tbreak;\n\tcase Opt_servern:\n\t\t/* last byte, type, is 0x20 for servr type */\n\t\tmemset(ctx->target_rfc1001_name, 0x20,\n\t\t\tRFC1001_NAME_LEN_WITH_NULL);\n\t\t/*\n\t\t * BB are there cases in which a comma can be valid in this\n\t\t * workstation netbios name (and need special handling)?\n\t\t */\n\n\t\t/* user or mount helper must uppercase the netbios name */\n\t\tfor (i = 0; i < 15; i++) {\n\t\t\tif (param->string[i] == 0)\n\t\t\t\tbreak;\n\t\t\tctx->target_rfc1001_name[i] = param->string[i];\n\t\t}\n\n\t\t/* The string has 16th byte zero still from set at top of function */\n\t\tif (i == RFC1001_NAME_LEN && param->string[i] != 0)\n\t\t\tpr_warn(\"server netbiosname longer than 15 truncated\\n\");\n\t\tbreak;\n\tcase Opt_ver:\n\t\t/* version of mount userspace tools, not dialect */\n\t\t/* If interface changes in mount.cifs bump to new ver */\n\t\tif (strncasecmp(param->string, \"1\", 1) == 0) {\n\t\t\tif (strlen(param->string) > 1) {\n\t\t\t\tpr_warn(\"Bad mount helper ver=%s. Did you want SMB1 (CIFS) dialect and mean to type vers=1.0 instead?\\n\",\n\t\t\t\t\tparam->string);\n\t\t\t\tgoto cifs_parse_mount_err;\n\t\t\t}\n\t\t\t/* This is the default */\n\t\t\tbreak;\n\t\t}\n\t\t/* For all other value, error */\n\t\tpr_warn(\"Invalid mount helper version specified\\n\");\n\t\tgoto cifs_parse_mount_err;\n\tcase Opt_vers:\n\t\t/* protocol version (dialect) */\n\t\tif (cifs_parse_smb_version(fc, param->string, ctx, is_smb3) != 0)\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->got_version = true;\n\t\tbreak;\n\tcase Opt_sec:\n\t\tif (cifs_parse_security_flavors(fc, param->string, ctx) != 0)\n\t\t\tgoto cifs_parse_mount_err;\n\t\tbreak;\n\tcase Opt_cache:\n\t\tif (cifs_parse_cache_flavor(fc, param->string, ctx) != 0)\n\t\t\tgoto cifs_parse_mount_err;\n\t\tbreak;\n\tcase Opt_witness:\n#ifndef CONFIG_CIFS_SWN_UPCALL\n\t\tcifs_errorf(fc, \"Witness support needs CONFIG_CIFS_SWN_UPCALL config option\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n#endif\n\t\tctx->witness = true;\n\t\tpr_warn_once(\"Witness protocol support is experimental\\n\");\n\t\tbreak;\n\tcase Opt_rootfs:\n#ifndef CONFIG_CIFS_ROOT\n\t\tcifs_dbg(VFS, \"rootfs support requires CONFIG_CIFS_ROOT config option\\n\");\n\t\tgoto cifs_parse_mount_err;\n#endif\n\t\tctx->rootfs = true;\n\t\tbreak;\n\tcase Opt_posixpaths:\n\t\tif (result.negated)\n\t\t\tctx->posix_paths = 0;\n\t\telse\n\t\t\tctx->posix_paths = 1;\n\t\tbreak;\n\tcase Opt_unix:\n\t\tif (result.negated) {\n\t\t\tif (ctx->linux_ext == 1)\n\t\t\t\tpr_warn_once(\"conflicting posix mount options specified\\n\");\n\t\t\tctx->linux_ext = 0;\n\t\t\tctx->no_linux_ext = 1;\n\t\t} else {\n\t\t\tif (ctx->no_linux_ext == 1)\n\t\t\t\tpr_warn_once(\"conflicting posix mount options specified\\n\");\n\t\t\tctx->linux_ext = 1;\n\t\t\tctx->no_linux_ext = 0;\n\t\t}\n\t\tbreak;\n\tcase Opt_nocase:\n\t\tctx->nocase = 1;\n\t\tbreak;\n\tcase Opt_brl:\n\t\tif (result.negated) {\n\t\t\t/*\n\t\t\t * turn off mandatory locking in mode\n\t\t\t * if remote locking is turned off since the\n\t\t\t * local vfs will do advisory\n\t\t\t */\n\t\t\tif (ctx->file_mode ==\n\t\t\t\t(S_IALLUGO & ~(S_ISUID | S_IXGRP)))\n\t\t\t\tctx->file_mode = S_IALLUGO;\n\t\t\tctx->nobrl =  1;\n\t\t} else\n\t\t\tctx->nobrl =  0;\n\t\tbreak;\n\tcase Opt_handlecache:\n\t\tif (result.negated)\n\t\t\tctx->nohandlecache = 1;\n\t\telse\n\t\t\tctx->nohandlecache = 0;\n\t\tbreak;\n\tcase Opt_forcemandatorylock:\n\t\tctx->mand_lock = 1;\n\t\tbreak;\n\tcase Opt_setuids:\n\t\tctx->setuids = result.negated;\n\t\tbreak;\n\tcase Opt_intr:\n\t\tctx->intr = !result.negated;\n\t\tbreak;\n\tcase Opt_setuidfromacl:\n\t\tctx->setuidfromacl = 1;\n\t\tbreak;\n\tcase Opt_strictsync:\n\t\tctx->nostrictsync = result.negated;\n\t\tbreak;\n\tcase Opt_serverino:\n\t\tctx->server_ino = !result.negated;\n\t\tbreak;\n\tcase Opt_rwpidforward:\n\t\tctx->rwpidforward = 1;\n\t\tbreak;\n\tcase Opt_modesid:\n\t\tctx->mode_ace = 1;\n\t\tbreak;\n\tcase Opt_cifsacl:\n\t\tctx->cifs_acl = !result.negated;\n\t\tbreak;\n\tcase Opt_acl:\n\t\tctx->no_psx_acl = result.negated;\n\t\tbreak;\n\tcase Opt_locallease:\n\t\tctx->local_lease = 1;\n\t\tbreak;\n\tcase Opt_sign:\n\t\tctx->sign = true;\n\t\tbreak;\n\tcase Opt_ignore_signature:\n\t\tctx->sign = true;\n\t\tctx->ignore_signature = true;\n\t\tbreak;\n\tcase Opt_seal:\n\t\t/* we do not do the following in secFlags because seal\n\t\t * is a per tree connection (mount) not a per socket\n\t\t * or per-smb connection option in the protocol\n\t\t * vol->secFlg |= CIFSSEC_MUST_SEAL;\n\t\t */\n\t\tctx->seal = 1;\n\t\tbreak;\n\tcase Opt_noac:\n\t\tpr_warn(\"Mount option noac not supported. Instead set /proc/fs/cifs/LookupCacheEnabled to 0\\n\");\n\t\tbreak;\n\tcase Opt_fsc:\n#ifndef CONFIG_CIFS_FSCACHE\n\t\tcifs_errorf(fc, \"FS-Cache support needs CONFIG_CIFS_FSCACHE kernel config option set\\n\");\n\t\tgoto cifs_parse_mount_err;\n#endif\n\t\tctx->fsc = true;\n\t\tbreak;\n\tcase Opt_mfsymlinks:\n\t\tctx->mfsymlinks = true;\n\t\tbreak;\n\tcase Opt_multiuser:\n\t\tctx->multiuser = true;\n\t\tbreak;\n\tcase Opt_sloppy:\n\t\tctx->sloppy = true;\n\t\tbreak;\n\tcase Opt_nosharesock:\n\t\tctx->nosharesock = true;\n\t\tbreak;\n\tcase Opt_persistent:\n\t\tif (result.negated) {\n\t\t\tctx->nopersistent = true;\n\t\t\tif (ctx->persistent) {\n\t\t\t\tcifs_errorf(fc, \"persistenthandles mount options conflict\\n\");\n\t\t\t\tgoto cifs_parse_mount_err;\n\t\t\t}\n\t\t} else {\n\t\t\tctx->persistent = true;\n\t\t\tif ((ctx->nopersistent) || (ctx->resilient)) {\n\t\t\t\tcifs_errorf(fc, \"persistenthandles mount options conflict\\n\");\n\t\t\t\tgoto cifs_parse_mount_err;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase Opt_resilient:\n\t\tif (result.negated) {\n\t\t\tctx->resilient = false; /* already the default */\n\t\t} else {\n\t\t\tctx->resilient = true;\n\t\t\tif (ctx->persistent) {\n\t\t\t\tcifs_errorf(fc, \"persistenthandles mount options conflict\\n\");\n\t\t\t\tgoto cifs_parse_mount_err;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase Opt_tcp_nodelay:\n\t\t/* tcp nodelay should not usually be needed since we CORK/UNCORK the socket */\n\t\tif (result.negated)\n\t\t\tctx->sockopt_tcp_nodelay = false;\n\t\telse\n\t\t\tctx->sockopt_tcp_nodelay = true;\n\t\tbreak;\n\tcase Opt_domainauto:\n\t\tctx->domainauto = true;\n\t\tbreak;\n\tcase Opt_rdma:\n\t\tctx->rdma = true;\n\t\tbreak;\n\t}\n\t/* case Opt_ignore: - is ignored as expected ... */\n\n\treturn 0;\n\n cifs_parse_mount_err:\n\tkfree_sensitive(ctx->password);\n\treturn -EINVAL;\n}",
        "code_after_change": "static int smb3_fs_context_parse_param(struct fs_context *fc,\n\t\t\t\t      struct fs_parameter *param)\n{\n\tstruct fs_parse_result result;\n\tstruct smb3_fs_context *ctx = smb3_fc2context(fc);\n\tint i, opt;\n\tbool is_smb3 = !strcmp(fc->fs_type->name, \"smb3\");\n\tbool skip_parsing = false;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tcifs_dbg(FYI, \"CIFS: parsing cifs mount option '%s'\\n\", param->key);\n\n\t/*\n\t * fs_parse can not handle string options with an empty value so\n\t * we will need special handling of them.\n\t */\n\tif (param->type == fs_value_is_string && param->string[0] == 0) {\n\t\tif (!strcmp(\"pass\", param->key) || !strcmp(\"password\", param->key)) {\n\t\t\tskip_parsing = true;\n\t\t\topt = Opt_pass;\n\t\t} else if (!strcmp(\"user\", param->key) || !strcmp(\"username\", param->key)) {\n\t\t\tskip_parsing = true;\n\t\t\topt = Opt_user;\n\t\t}\n\t}\n\n\tif (!skip_parsing) {\n\t\topt = fs_parse(fc, smb3_fs_parameters, param, &result);\n\t\tif (opt < 0)\n\t\t\treturn ctx->sloppy ? 1 : opt;\n\t}\n\n\tswitch (opt) {\n\tcase Opt_compress:\n\t\tctx->compression = UNKNOWN_TYPE;\n\t\tcifs_dbg(VFS,\n\t\t\t\"SMB3 compression support is experimental\\n\");\n\t\tbreak;\n\tcase Opt_nodfs:\n\t\tctx->nodfs = 1;\n\t\tbreak;\n\tcase Opt_hard:\n\t\tif (result.negated) {\n\t\t\tif (ctx->retry == 1)\n\t\t\t\tcifs_dbg(VFS, \"conflicting hard vs. soft mount options\\n\");\n\t\t\tctx->retry = 0;\n\t\t} else\n\t\t\tctx->retry = 1;\n\t\tbreak;\n\tcase Opt_soft:\n\t\tif (result.negated)\n\t\t\tctx->retry = 1;\n\t\telse {\n\t\t\tif (ctx->retry == 1)\n\t\t\t\tcifs_dbg(VFS, \"conflicting hard vs soft mount options\\n\");\n\t\t\tctx->retry = 0;\n\t\t}\n\t\tbreak;\n\tcase Opt_mapposix:\n\t\tif (result.negated)\n\t\t\tctx->remap = false;\n\t\telse {\n\t\t\tctx->remap = true;\n\t\t\tctx->sfu_remap = false; /* disable SFU mapping */\n\t\t}\n\t\tbreak;\n\tcase Opt_mapchars:\n\t\tif (result.negated)\n\t\t\tctx->sfu_remap = false;\n\t\telse {\n\t\t\tctx->sfu_remap = true;\n\t\t\tctx->remap = false; /* disable SFM (mapposix) mapping */\n\t\t}\n\t\tbreak;\n\tcase Opt_user_xattr:\n\t\tif (result.negated)\n\t\t\tctx->no_xattr = 1;\n\t\telse\n\t\t\tctx->no_xattr = 0;\n\t\tbreak;\n\tcase Opt_forceuid:\n\t\tif (result.negated)\n\t\t\tctx->override_uid = 0;\n\t\telse\n\t\t\tctx->override_uid = 1;\n\t\tbreak;\n\tcase Opt_forcegid:\n\t\tif (result.negated)\n\t\t\tctx->override_gid = 0;\n\t\telse\n\t\t\tctx->override_gid = 1;\n\t\tbreak;\n\tcase Opt_perm:\n\t\tif (result.negated)\n\t\t\tctx->noperm = 1;\n\t\telse\n\t\t\tctx->noperm = 0;\n\t\tbreak;\n\tcase Opt_dynperm:\n\t\tif (result.negated)\n\t\t\tctx->dynperm = 0;\n\t\telse\n\t\t\tctx->dynperm = 1;\n\t\tbreak;\n\tcase Opt_sfu:\n\t\tif (result.negated)\n\t\t\tctx->sfu_emul = 0;\n\t\telse\n\t\t\tctx->sfu_emul = 1;\n\t\tbreak;\n\tcase Opt_noblocksend:\n\t\tctx->noblocksnd = 1;\n\t\tbreak;\n\tcase Opt_noautotune:\n\t\tctx->noautotune = 1;\n\t\tbreak;\n\tcase Opt_nolease:\n\t\tctx->no_lease = 1;\n\t\tbreak;\n\tcase Opt_nosparse:\n\t\tctx->no_sparse = 1;\n\t\tbreak;\n\tcase Opt_nodelete:\n\t\tctx->nodelete = 1;\n\t\tbreak;\n\tcase Opt_multichannel:\n\t\tif (result.negated) {\n\t\t\tctx->multichannel = false;\n\t\t\tctx->max_channels = 1;\n\t\t} else {\n\t\t\tctx->multichannel = true;\n\t\t\t/* if number of channels not specified, default to 2 */\n\t\t\tif (ctx->max_channels < 2)\n\t\t\t\tctx->max_channels = 2;\n\t\t}\n\t\tbreak;\n\tcase Opt_uid:\n\t\tuid = make_kuid(current_user_ns(), result.uint_32);\n\t\tif (!uid_valid(uid))\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->linux_uid = uid;\n\t\tctx->uid_specified = true;\n\t\tbreak;\n\tcase Opt_cruid:\n\t\tuid = make_kuid(current_user_ns(), result.uint_32);\n\t\tif (!uid_valid(uid))\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->cred_uid = uid;\n\t\tctx->cruid_specified = true;\n\t\tbreak;\n\tcase Opt_backupuid:\n\t\tuid = make_kuid(current_user_ns(), result.uint_32);\n\t\tif (!uid_valid(uid))\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->backupuid = uid;\n\t\tctx->backupuid_specified = true;\n\t\tbreak;\n\tcase Opt_backupgid:\n\t\tgid = make_kgid(current_user_ns(), result.uint_32);\n\t\tif (!gid_valid(gid))\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->backupgid = gid;\n\t\tctx->backupgid_specified = true;\n\t\tbreak;\n\tcase Opt_gid:\n\t\tgid = make_kgid(current_user_ns(), result.uint_32);\n\t\tif (!gid_valid(gid))\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->linux_gid = gid;\n\t\tctx->gid_specified = true;\n\t\tbreak;\n\tcase Opt_port:\n\t\tctx->port = result.uint_32;\n\t\tbreak;\n\tcase Opt_file_mode:\n\t\tctx->file_mode = result.uint_32;\n\t\tbreak;\n\tcase Opt_dirmode:\n\t\tctx->dir_mode = result.uint_32;\n\t\tbreak;\n\tcase Opt_min_enc_offload:\n\t\tctx->min_offload = result.uint_32;\n\t\tbreak;\n\tcase Opt_blocksize:\n\t\t/*\n\t\t * inode blocksize realistically should never need to be\n\t\t * less than 16K or greater than 16M and default is 1MB.\n\t\t * Note that small inode block sizes (e.g. 64K) can lead\n\t\t * to very poor performance of common tools like cp and scp\n\t\t */\n\t\tif ((result.uint_32 < CIFS_MAX_MSGSIZE) ||\n\t\t   (result.uint_32 > (4 * SMB3_DEFAULT_IOSIZE))) {\n\t\t\tcifs_errorf(fc, \"%s: Invalid blocksize\\n\",\n\t\t\t\t__func__);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->bsize = result.uint_32;\n\t\tctx->got_bsize = true;\n\t\tbreak;\n\tcase Opt_rasize:\n\t\t/*\n\t\t * readahead size realistically should never need to be\n\t\t * less than 1M (CIFS_DEFAULT_IOSIZE) or greater than 32M\n\t\t * (perhaps an exception should be considered in the\n\t\t * for the case of a large number of channels\n\t\t * when multichannel is negotiated) since that would lead\n\t\t * to plenty of parallel I/O in flight to the server.\n\t\t * Note that smaller read ahead sizes would\n\t\t * hurt performance of common tools like cp and scp\n\t\t * which often trigger sequential i/o with read ahead\n\t\t */\n\t\tif ((result.uint_32 > (8 * SMB3_DEFAULT_IOSIZE)) ||\n\t\t    (result.uint_32 < CIFS_DEFAULT_IOSIZE)) {\n\t\t\tcifs_errorf(fc, \"%s: Invalid rasize %d vs. %d\\n\",\n\t\t\t\t__func__, result.uint_32, SMB3_DEFAULT_IOSIZE);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->rasize = result.uint_32;\n\t\tbreak;\n\tcase Opt_rsize:\n\t\tctx->rsize = result.uint_32;\n\t\tctx->got_rsize = true;\n\t\tbreak;\n\tcase Opt_wsize:\n\t\tctx->wsize = result.uint_32;\n\t\tctx->got_wsize = true;\n\t\tbreak;\n\tcase Opt_acregmax:\n\t\tctx->acregmax = HZ * result.uint_32;\n\t\tif (ctx->acregmax > CIFS_MAX_ACTIMEO) {\n\t\t\tcifs_errorf(fc, \"acregmax too large\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_acdirmax:\n\t\tctx->acdirmax = HZ * result.uint_32;\n\t\tif (ctx->acdirmax > CIFS_MAX_ACTIMEO) {\n\t\t\tcifs_errorf(fc, \"acdirmax too large\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_actimeo:\n\t\tif (HZ * result.uint_32 > CIFS_MAX_ACTIMEO) {\n\t\t\tcifs_errorf(fc, \"timeout too large\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tif ((ctx->acdirmax != CIFS_DEF_ACTIMEO) ||\n\t\t    (ctx->acregmax != CIFS_DEF_ACTIMEO)) {\n\t\t\tcifs_errorf(fc, \"actimeo ignored since acregmax or acdirmax specified\\n\");\n\t\t\tbreak;\n\t\t}\n\t\tctx->acdirmax = ctx->acregmax = HZ * result.uint_32;\n\t\tbreak;\n\tcase Opt_closetimeo:\n\t\tctx->closetimeo = HZ * result.uint_32;\n\t\tif (ctx->closetimeo > SMB3_MAX_DCLOSETIMEO) {\n\t\t\tcifs_errorf(fc, \"closetimeo too large\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_echo_interval:\n\t\tctx->echo_interval = result.uint_32;\n\t\tbreak;\n\tcase Opt_snapshot:\n\t\tctx->snapshot_time = result.uint_64;\n\t\tbreak;\n\tcase Opt_max_credits:\n\t\tif (result.uint_32 < 20 || result.uint_32 > 60000) {\n\t\t\tcifs_errorf(fc, \"%s: Invalid max_credits value\\n\",\n\t\t\t\t __func__);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->max_credits = result.uint_32;\n\t\tbreak;\n\tcase Opt_max_channels:\n\t\tif (result.uint_32 < 1 || result.uint_32 > CIFS_MAX_CHANNELS) {\n\t\t\tcifs_errorf(fc, \"%s: Invalid max_channels value, needs to be 1-%d\\n\",\n\t\t\t\t __func__, CIFS_MAX_CHANNELS);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->max_channels = result.uint_32;\n\t\t/* If more than one channel requested ... they want multichan */\n\t\tif (result.uint_32 > 1)\n\t\t\tctx->multichannel = true;\n\t\tbreak;\n\tcase Opt_max_cached_dirs:\n\t\tif (result.uint_32 < 1) {\n\t\t\tcifs_errorf(fc, \"%s: Invalid max_cached_dirs, needs to be 1 or more\\n\",\n\t\t\t\t    __func__);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->max_cached_dirs = result.uint_32;\n\t\tbreak;\n\tcase Opt_handletimeout:\n\t\tctx->handle_timeout = result.uint_32;\n\t\tif (ctx->handle_timeout > SMB3_MAX_HANDLE_TIMEOUT) {\n\t\t\tcifs_errorf(fc, \"Invalid handle cache timeout, longer than 16 minutes\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_source:\n\t\tkfree(ctx->UNC);\n\t\tctx->UNC = NULL;\n\t\tswitch (smb3_parse_devname(param->string, ctx)) {\n\t\tcase 0:\n\t\t\tbreak;\n\t\tcase -ENOMEM:\n\t\t\tcifs_errorf(fc, \"Unable to allocate memory for devname\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\tcase -EINVAL:\n\t\t\tcifs_errorf(fc, \"Malformed UNC in devname\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\tdefault:\n\t\t\tcifs_errorf(fc, \"Unknown error parsing devname\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->source = smb3_fs_context_fullpath(ctx, '/');\n\t\tif (IS_ERR(ctx->source)) {\n\t\t\tctx->source = NULL;\n\t\t\tcifs_errorf(fc, \"OOM when copying UNC string\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tfc->source = kstrdup(ctx->source, GFP_KERNEL);\n\t\tif (fc->source == NULL) {\n\t\t\tcifs_errorf(fc, \"OOM when copying UNC string\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_user:\n\t\tkfree(ctx->username);\n\t\tctx->username = NULL;\n\t\tif (ctx->nullauth)\n\t\t\tbreak;\n\t\tif (strlen(param->string) == 0) {\n\t\t\t/* null user, ie. anonymous authentication */\n\t\t\tctx->nullauth = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (strnlen(param->string, CIFS_MAX_USERNAME_LEN) >\n\t\t    CIFS_MAX_USERNAME_LEN) {\n\t\t\tpr_warn(\"username too long\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->username = kstrdup(param->string, GFP_KERNEL);\n\t\tif (ctx->username == NULL) {\n\t\t\tcifs_errorf(fc, \"OOM when copying username string\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_pass:\n\t\tkfree_sensitive(ctx->password);\n\t\tctx->password = NULL;\n\t\tif (strlen(param->string) == 0)\n\t\t\tbreak;\n\n\t\tctx->password = kstrdup(param->string, GFP_KERNEL);\n\t\tif (ctx->password == NULL) {\n\t\t\tcifs_errorf(fc, \"OOM when copying password string\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_ip:\n\t\tif (strlen(param->string) == 0) {\n\t\t\tctx->got_ip = false;\n\t\t\tbreak;\n\t\t}\n\t\tif (!cifs_convert_address((struct sockaddr *)&ctx->dstaddr,\n\t\t\t\t\t  param->string,\n\t\t\t\t\t  strlen(param->string))) {\n\t\t\tpr_err(\"bad ip= option (%s)\\n\", param->string);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tctx->got_ip = true;\n\t\tbreak;\n\tcase Opt_domain:\n\t\tif (strnlen(param->string, CIFS_MAX_DOMAINNAME_LEN)\n\t\t\t\t== CIFS_MAX_DOMAINNAME_LEN) {\n\t\t\tpr_warn(\"domain name too long\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\n\t\tkfree(ctx->domainname);\n\t\tctx->domainname = kstrdup(param->string, GFP_KERNEL);\n\t\tif (ctx->domainname == NULL) {\n\t\t\tcifs_errorf(fc, \"OOM when copying domainname string\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tcifs_dbg(FYI, \"Domain name set\\n\");\n\t\tbreak;\n\tcase Opt_srcaddr:\n\t\tif (!cifs_convert_address(\n\t\t\t\t(struct sockaddr *)&ctx->srcaddr,\n\t\t\t\tparam->string, strlen(param->string))) {\n\t\t\tpr_warn(\"Could not parse srcaddr: %s\\n\",\n\t\t\t\tparam->string);\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\t\tbreak;\n\tcase Opt_iocharset:\n\t\tif (strnlen(param->string, 1024) >= 65) {\n\t\t\tpr_warn(\"iocharset name too long\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n\t\t}\n\n\t\tif (strncasecmp(param->string, \"default\", 7) != 0) {\n\t\t\tkfree(ctx->iocharset);\n\t\t\tctx->iocharset = kstrdup(param->string, GFP_KERNEL);\n\t\t\tif (ctx->iocharset == NULL) {\n\t\t\t\tcifs_errorf(fc, \"OOM when copying iocharset string\\n\");\n\t\t\t\tgoto cifs_parse_mount_err;\n\t\t\t}\n\t\t}\n\t\t/* if iocharset not set then load_nls_default\n\t\t * is used by caller\n\t\t */\n\t\tcifs_dbg(FYI, \"iocharset set to %s\\n\", ctx->iocharset);\n\t\tbreak;\n\tcase Opt_netbiosname:\n\t\tmemset(ctx->source_rfc1001_name, 0x20,\n\t\t\tRFC1001_NAME_LEN);\n\t\t/*\n\t\t * FIXME: are there cases in which a comma can\n\t\t * be valid in workstation netbios name (and\n\t\t * need special handling)?\n\t\t */\n\t\tfor (i = 0; i < RFC1001_NAME_LEN; i++) {\n\t\t\t/* don't ucase netbiosname for user */\n\t\t\tif (param->string[i] == 0)\n\t\t\t\tbreak;\n\t\t\tctx->source_rfc1001_name[i] = param->string[i];\n\t\t}\n\t\t/* The string has 16th byte zero still from\n\t\t * set at top of the function\n\t\t */\n\t\tif (i == RFC1001_NAME_LEN && param->string[i] != 0)\n\t\t\tpr_warn(\"netbiosname longer than 15 truncated\\n\");\n\t\tbreak;\n\tcase Opt_servern:\n\t\t/* last byte, type, is 0x20 for servr type */\n\t\tmemset(ctx->target_rfc1001_name, 0x20,\n\t\t\tRFC1001_NAME_LEN_WITH_NULL);\n\t\t/*\n\t\t * BB are there cases in which a comma can be valid in this\n\t\t * workstation netbios name (and need special handling)?\n\t\t */\n\n\t\t/* user or mount helper must uppercase the netbios name */\n\t\tfor (i = 0; i < 15; i++) {\n\t\t\tif (param->string[i] == 0)\n\t\t\t\tbreak;\n\t\t\tctx->target_rfc1001_name[i] = param->string[i];\n\t\t}\n\n\t\t/* The string has 16th byte zero still from set at top of function */\n\t\tif (i == RFC1001_NAME_LEN && param->string[i] != 0)\n\t\t\tpr_warn(\"server netbiosname longer than 15 truncated\\n\");\n\t\tbreak;\n\tcase Opt_ver:\n\t\t/* version of mount userspace tools, not dialect */\n\t\t/* If interface changes in mount.cifs bump to new ver */\n\t\tif (strncasecmp(param->string, \"1\", 1) == 0) {\n\t\t\tif (strlen(param->string) > 1) {\n\t\t\t\tpr_warn(\"Bad mount helper ver=%s. Did you want SMB1 (CIFS) dialect and mean to type vers=1.0 instead?\\n\",\n\t\t\t\t\tparam->string);\n\t\t\t\tgoto cifs_parse_mount_err;\n\t\t\t}\n\t\t\t/* This is the default */\n\t\t\tbreak;\n\t\t}\n\t\t/* For all other value, error */\n\t\tpr_warn(\"Invalid mount helper version specified\\n\");\n\t\tgoto cifs_parse_mount_err;\n\tcase Opt_vers:\n\t\t/* protocol version (dialect) */\n\t\tif (cifs_parse_smb_version(fc, param->string, ctx, is_smb3) != 0)\n\t\t\tgoto cifs_parse_mount_err;\n\t\tctx->got_version = true;\n\t\tbreak;\n\tcase Opt_sec:\n\t\tif (cifs_parse_security_flavors(fc, param->string, ctx) != 0)\n\t\t\tgoto cifs_parse_mount_err;\n\t\tbreak;\n\tcase Opt_cache:\n\t\tif (cifs_parse_cache_flavor(fc, param->string, ctx) != 0)\n\t\t\tgoto cifs_parse_mount_err;\n\t\tbreak;\n\tcase Opt_witness:\n#ifndef CONFIG_CIFS_SWN_UPCALL\n\t\tcifs_errorf(fc, \"Witness support needs CONFIG_CIFS_SWN_UPCALL config option\\n\");\n\t\t\tgoto cifs_parse_mount_err;\n#endif\n\t\tctx->witness = true;\n\t\tpr_warn_once(\"Witness protocol support is experimental\\n\");\n\t\tbreak;\n\tcase Opt_rootfs:\n#ifndef CONFIG_CIFS_ROOT\n\t\tcifs_dbg(VFS, \"rootfs support requires CONFIG_CIFS_ROOT config option\\n\");\n\t\tgoto cifs_parse_mount_err;\n#endif\n\t\tctx->rootfs = true;\n\t\tbreak;\n\tcase Opt_posixpaths:\n\t\tif (result.negated)\n\t\t\tctx->posix_paths = 0;\n\t\telse\n\t\t\tctx->posix_paths = 1;\n\t\tbreak;\n\tcase Opt_unix:\n\t\tif (result.negated) {\n\t\t\tif (ctx->linux_ext == 1)\n\t\t\t\tpr_warn_once(\"conflicting posix mount options specified\\n\");\n\t\t\tctx->linux_ext = 0;\n\t\t\tctx->no_linux_ext = 1;\n\t\t} else {\n\t\t\tif (ctx->no_linux_ext == 1)\n\t\t\t\tpr_warn_once(\"conflicting posix mount options specified\\n\");\n\t\t\tctx->linux_ext = 1;\n\t\t\tctx->no_linux_ext = 0;\n\t\t}\n\t\tbreak;\n\tcase Opt_nocase:\n\t\tctx->nocase = 1;\n\t\tbreak;\n\tcase Opt_brl:\n\t\tif (result.negated) {\n\t\t\t/*\n\t\t\t * turn off mandatory locking in mode\n\t\t\t * if remote locking is turned off since the\n\t\t\t * local vfs will do advisory\n\t\t\t */\n\t\t\tif (ctx->file_mode ==\n\t\t\t\t(S_IALLUGO & ~(S_ISUID | S_IXGRP)))\n\t\t\t\tctx->file_mode = S_IALLUGO;\n\t\t\tctx->nobrl =  1;\n\t\t} else\n\t\t\tctx->nobrl =  0;\n\t\tbreak;\n\tcase Opt_handlecache:\n\t\tif (result.negated)\n\t\t\tctx->nohandlecache = 1;\n\t\telse\n\t\t\tctx->nohandlecache = 0;\n\t\tbreak;\n\tcase Opt_forcemandatorylock:\n\t\tctx->mand_lock = 1;\n\t\tbreak;\n\tcase Opt_setuids:\n\t\tctx->setuids = result.negated;\n\t\tbreak;\n\tcase Opt_intr:\n\t\tctx->intr = !result.negated;\n\t\tbreak;\n\tcase Opt_setuidfromacl:\n\t\tctx->setuidfromacl = 1;\n\t\tbreak;\n\tcase Opt_strictsync:\n\t\tctx->nostrictsync = result.negated;\n\t\tbreak;\n\tcase Opt_serverino:\n\t\tctx->server_ino = !result.negated;\n\t\tbreak;\n\tcase Opt_rwpidforward:\n\t\tctx->rwpidforward = 1;\n\t\tbreak;\n\tcase Opt_modesid:\n\t\tctx->mode_ace = 1;\n\t\tbreak;\n\tcase Opt_cifsacl:\n\t\tctx->cifs_acl = !result.negated;\n\t\tbreak;\n\tcase Opt_acl:\n\t\tctx->no_psx_acl = result.negated;\n\t\tbreak;\n\tcase Opt_locallease:\n\t\tctx->local_lease = 1;\n\t\tbreak;\n\tcase Opt_sign:\n\t\tctx->sign = true;\n\t\tbreak;\n\tcase Opt_ignore_signature:\n\t\tctx->sign = true;\n\t\tctx->ignore_signature = true;\n\t\tbreak;\n\tcase Opt_seal:\n\t\t/* we do not do the following in secFlags because seal\n\t\t * is a per tree connection (mount) not a per socket\n\t\t * or per-smb connection option in the protocol\n\t\t * vol->secFlg |= CIFSSEC_MUST_SEAL;\n\t\t */\n\t\tctx->seal = 1;\n\t\tbreak;\n\tcase Opt_noac:\n\t\tpr_warn(\"Mount option noac not supported. Instead set /proc/fs/cifs/LookupCacheEnabled to 0\\n\");\n\t\tbreak;\n\tcase Opt_fsc:\n#ifndef CONFIG_CIFS_FSCACHE\n\t\tcifs_errorf(fc, \"FS-Cache support needs CONFIG_CIFS_FSCACHE kernel config option set\\n\");\n\t\tgoto cifs_parse_mount_err;\n#endif\n\t\tctx->fsc = true;\n\t\tbreak;\n\tcase Opt_mfsymlinks:\n\t\tctx->mfsymlinks = true;\n\t\tbreak;\n\tcase Opt_multiuser:\n\t\tctx->multiuser = true;\n\t\tbreak;\n\tcase Opt_sloppy:\n\t\tctx->sloppy = true;\n\t\tbreak;\n\tcase Opt_nosharesock:\n\t\tctx->nosharesock = true;\n\t\tbreak;\n\tcase Opt_persistent:\n\t\tif (result.negated) {\n\t\t\tctx->nopersistent = true;\n\t\t\tif (ctx->persistent) {\n\t\t\t\tcifs_errorf(fc, \"persistenthandles mount options conflict\\n\");\n\t\t\t\tgoto cifs_parse_mount_err;\n\t\t\t}\n\t\t} else {\n\t\t\tctx->persistent = true;\n\t\t\tif ((ctx->nopersistent) || (ctx->resilient)) {\n\t\t\t\tcifs_errorf(fc, \"persistenthandles mount options conflict\\n\");\n\t\t\t\tgoto cifs_parse_mount_err;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase Opt_resilient:\n\t\tif (result.negated) {\n\t\t\tctx->resilient = false; /* already the default */\n\t\t} else {\n\t\t\tctx->resilient = true;\n\t\t\tif (ctx->persistent) {\n\t\t\t\tcifs_errorf(fc, \"persistenthandles mount options conflict\\n\");\n\t\t\t\tgoto cifs_parse_mount_err;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase Opt_tcp_nodelay:\n\t\t/* tcp nodelay should not usually be needed since we CORK/UNCORK the socket */\n\t\tif (result.negated)\n\t\t\tctx->sockopt_tcp_nodelay = false;\n\t\telse\n\t\t\tctx->sockopt_tcp_nodelay = true;\n\t\tbreak;\n\tcase Opt_domainauto:\n\t\tctx->domainauto = true;\n\t\tbreak;\n\tcase Opt_rdma:\n\t\tctx->rdma = true;\n\t\tbreak;\n\t}\n\t/* case Opt_ignore: - is ignored as expected ... */\n\n\treturn 0;\n\n cifs_parse_mount_err:\n\tkfree_sensitive(ctx->password);\n\tctx->password = NULL;\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -659,5 +659,6 @@\n \n  cifs_parse_mount_err:\n \tkfree_sensitive(ctx->password);\n+\tctx->password = NULL;\n \treturn -EINVAL;\n }",
        "function_modified_lines": {
            "added": [
                "\tctx->password = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's fs/smb/client component can be exploited to achieve local privilege escalation.\n\nIn case of an error in smb3_fs_context_parse_param, ctx->password was freed but the field was not set to NULL which could lead to double free.\n\nWe recommend upgrading past commit e6e43b8aa7cd3c3af686caf0c2e11819a886d705.\n\n",
        "id": 4266
    },
    {
        "cve_id": "CVE-2022-47946",
        "code_before_change": "SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,\n\t\tu32, min_complete, u32, flags, const void __user *, argp,\n\t\tsize_t, argsz)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tint submitted = 0;\n\tstruct fd f;\n\n\tio_run_task_work();\n\n\tif (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |\n\t\t\tIORING_ENTER_SQ_WAIT | IORING_ENTER_EXT_ARG))\n\t\treturn -EINVAL;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tret = -ENXIO;\n\tctx = f.file->private_data;\n\tif (!percpu_ref_tryget(&ctx->refs))\n\t\tgoto out_fput;\n\n\tret = -EBADFD;\n\tif (ctx->flags & IORING_SETUP_R_DISABLED)\n\t\tgoto out;\n\n\t/*\n\t * For SQ polling, the thread will do all submissions and completions.\n\t * Just return the requested submit count, and wake the thread if\n\t * we were asked to.\n\t */\n\tret = 0;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\n\t\tif (unlikely(ctx->sqo_exec)) {\n\t\t\tret = io_sq_thread_fork(ctx->sq_data, ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tctx->sqo_exec = 0;\n\t\t}\n\t\tret = -EOWNERDEAD;\n\t\tif (unlikely(ctx->sqo_dead))\n\t\t\tgoto out;\n\t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n\t\t\twake_up(&ctx->sq_data->wait);\n\t\tif (flags & IORING_ENTER_SQ_WAIT) {\n\t\t\tret = io_sqpoll_wait_sq(ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\tsubmitted = to_submit;\n\t} else if (to_submit) {\n\t\tret = io_uring_add_task_file(ctx, f.file);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tsubmitted = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tif (submitted != to_submit)\n\t\t\tgoto out;\n\t}\n\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\tconst sigset_t __user *sig;\n\t\tstruct __kernel_timespec __user *ts;\n\n\t\tret = io_get_ext_arg(flags, argp, &argsz, &ts, &sig);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\n\t\tmin_complete = min(min_complete, ctx->cq_entries);\n\n\t\t/*\n\t\t * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user\n\t\t * space applications don't need to do io completion events\n\t\t * polling again, they can rely on io_sq_thread to do polling\n\t\t * work, which can reduce cpu usage and uring_lock contention.\n\t\t */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL &&\n\t\t    !(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\tret = io_iopoll_check(ctx, min_complete);\n\t\t} else {\n\t\t\tret = io_cqring_wait(ctx, min_complete, sig, argsz, ts);\n\t\t}\n\t}\n\nout:\n\tpercpu_ref_put(&ctx->refs);\nout_fput:\n\tfdput(f);\n\treturn submitted ? submitted : ret;\n}",
        "code_after_change": "SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,\n\t\tu32, min_complete, u32, flags, const void __user *, argp,\n\t\tsize_t, argsz)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tint submitted = 0;\n\tstruct fd f;\n\n\tio_run_task_work();\n\n\tif (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |\n\t\t\tIORING_ENTER_SQ_WAIT | IORING_ENTER_EXT_ARG))\n\t\treturn -EINVAL;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tret = -ENXIO;\n\tctx = f.file->private_data;\n\tif (!percpu_ref_tryget(&ctx->refs))\n\t\tgoto out_fput;\n\n\tret = -EBADFD;\n\tif (ctx->flags & IORING_SETUP_R_DISABLED)\n\t\tgoto out;\n\n\t/*\n\t * For SQ polling, the thread will do all submissions and completions.\n\t * Just return the requested submit count, and wake the thread if\n\t * we were asked to.\n\t */\n\tret = 0;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\n\t\tif (unlikely(ctx->sqo_exec)) {\n\t\t\tret = io_sq_thread_fork(ctx->sq_data, ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tctx->sqo_exec = 0;\n\t\t}\n\t\tret = -EOWNERDEAD;\n\t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n\t\t\twake_up(&ctx->sq_data->wait);\n\t\tif (flags & IORING_ENTER_SQ_WAIT) {\n\t\t\tret = io_sqpoll_wait_sq(ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\tsubmitted = to_submit;\n\t} else if (to_submit) {\n\t\tret = io_uring_add_task_file(ctx, f.file);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tsubmitted = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tif (submitted != to_submit)\n\t\t\tgoto out;\n\t}\n\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\tconst sigset_t __user *sig;\n\t\tstruct __kernel_timespec __user *ts;\n\n\t\tret = io_get_ext_arg(flags, argp, &argsz, &ts, &sig);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\n\t\tmin_complete = min(min_complete, ctx->cq_entries);\n\n\t\t/*\n\t\t * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user\n\t\t * space applications don't need to do io completion events\n\t\t * polling again, they can rely on io_sq_thread to do polling\n\t\t * work, which can reduce cpu usage and uring_lock contention.\n\t\t */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL &&\n\t\t    !(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\tret = io_iopoll_check(ctx, min_complete);\n\t\t} else {\n\t\t\tret = io_cqring_wait(ctx, min_complete, sig, argsz, ts);\n\t\t}\n\t}\n\nout:\n\tpercpu_ref_put(&ctx->refs);\nout_fput:\n\tfdput(f);\n\treturn submitted ? submitted : ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -46,8 +46,6 @@\n \t\t\tctx->sqo_exec = 0;\n \t\t}\n \t\tret = -EOWNERDEAD;\n-\t\tif (unlikely(ctx->sqo_dead))\n-\t\t\tgoto out;\n \t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n \t\t\twake_up(&ctx->sq_data->wait);\n \t\tif (flags & IORING_ENTER_SQ_WAIT) {",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tif (unlikely(ctx->sqo_dead))",
                "\t\t\tgoto out;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel 5.10.x before 5.10.155. A use-after-free in io_sqpoll_wait_sq in fs/io_uring.c allows an attacker to crash the kernel, resulting in denial of service. finish_wait can be skipped. An attack can occur in some situations by forking a process and then quickly terminating it. NOTE: later kernel versions, such as the 5.15 longterm series, substantially changed the implementation of io_sqpoll_wait_sq.",
        "id": 3780
    },
    {
        "cve_id": "CVE-2018-20976",
        "code_before_change": "STATIC int\nxfs_fs_fill_super(\n\tstruct super_block\t*sb,\n\tvoid\t\t\t*data,\n\tint\t\t\tsilent)\n{\n\tstruct inode\t\t*root;\n\tstruct xfs_mount\t*mp = NULL;\n\tint\t\t\tflags = 0, error = -ENOMEM;\n\n\t/*\n\t * allocate mp and do all low-level struct initializations before we\n\t * attach it to the super\n\t */\n\tmp = xfs_mount_alloc(sb);\n\tif (!mp)\n\t\tgoto out;\n\tsb->s_fs_info = mp;\n\n\terror = xfs_parseargs(mp, (char *)data);\n\tif (error)\n\t\tgoto out_free_fsname;\n\n\tsb_min_blocksize(sb, BBSIZE);\n\tsb->s_xattr = xfs_xattr_handlers;\n\tsb->s_export_op = &xfs_export_operations;\n#ifdef CONFIG_XFS_QUOTA\n\tsb->s_qcop = &xfs_quotactl_operations;\n\tsb->s_quota_types = QTYPE_MASK_USR | QTYPE_MASK_GRP | QTYPE_MASK_PRJ;\n#endif\n\tsb->s_op = &xfs_super_operations;\n\n\t/*\n\t * Delay mount work if the debug hook is set. This is debug\n\t * instrumention to coordinate simulation of xfs mount failures with\n\t * VFS superblock operations\n\t */\n\tif (xfs_globals.mount_delay) {\n\t\txfs_notice(mp, \"Delaying mount for %d seconds.\",\n\t\t\txfs_globals.mount_delay);\n\t\tmsleep(xfs_globals.mount_delay * 1000);\n\t}\n\n\tif (silent)\n\t\tflags |= XFS_MFSI_QUIET;\n\n\terror = xfs_open_devices(mp);\n\tif (error)\n\t\tgoto out_free_fsname;\n\n\terror = xfs_init_mount_workqueues(mp);\n\tif (error)\n\t\tgoto out_close_devices;\n\n\terror = xfs_init_percpu_counters(mp);\n\tif (error)\n\t\tgoto out_destroy_workqueues;\n\n\t/* Allocate stats memory before we do operations that might use it */\n\tmp->m_stats.xs_stats = alloc_percpu(struct xfsstats);\n\tif (!mp->m_stats.xs_stats) {\n\t\terror = -ENOMEM;\n\t\tgoto out_destroy_counters;\n\t}\n\n\terror = xfs_readsb(mp, flags);\n\tif (error)\n\t\tgoto out_free_stats;\n\n\terror = xfs_finish_flags(mp);\n\tif (error)\n\t\tgoto out_free_sb;\n\n\terror = xfs_setup_devices(mp);\n\tif (error)\n\t\tgoto out_free_sb;\n\n\terror = xfs_filestream_mount(mp);\n\tif (error)\n\t\tgoto out_free_sb;\n\n\t/*\n\t * we must configure the block size in the superblock before we run the\n\t * full mount process as the mount process can lookup and cache inodes.\n\t */\n\tsb->s_magic = XFS_SB_MAGIC;\n\tsb->s_blocksize = mp->m_sb.sb_blocksize;\n\tsb->s_blocksize_bits = ffs(sb->s_blocksize) - 1;\n\tsb->s_maxbytes = xfs_max_file_offset(sb->s_blocksize_bits);\n\tsb->s_max_links = XFS_MAXLINK;\n\tsb->s_time_gran = 1;\n\tset_posix_acl_flag(sb);\n\n\t/* version 5 superblocks support inode version counters. */\n\tif (XFS_SB_VERSION_NUM(&mp->m_sb) == XFS_SB_VERSION_5)\n\t\tsb->s_flags |= SB_I_VERSION;\n\n\tif (mp->m_flags & XFS_MOUNT_DAX) {\n\t\txfs_warn(mp,\n\t\t\"DAX enabled. Warning: EXPERIMENTAL, use at your own risk\");\n\n\t\terror = bdev_dax_supported(sb, sb->s_blocksize);\n\t\tif (error) {\n\t\t\txfs_alert(mp,\n\t\t\t\"DAX unsupported by block device. Turning off DAX.\");\n\t\t\tmp->m_flags &= ~XFS_MOUNT_DAX;\n\t\t}\n\t\tif (xfs_sb_version_hasreflink(&mp->m_sb)) {\n\t\t\txfs_alert(mp,\n\t\t\"DAX and reflink cannot be used together!\");\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_filestream_unmount;\n\t\t}\n\t}\n\n\tif (mp->m_flags & XFS_MOUNT_DISCARD) {\n\t\tstruct request_queue *q = bdev_get_queue(sb->s_bdev);\n\n\t\tif (!blk_queue_discard(q)) {\n\t\t\txfs_warn(mp, \"mounting with \\\"discard\\\" option, but \"\n\t\t\t\t\t\"the device does not support discard\");\n\t\t\tmp->m_flags &= ~XFS_MOUNT_DISCARD;\n\t\t}\n\t}\n\n\tif (xfs_sb_version_hasreflink(&mp->m_sb) && mp->m_sb.sb_rblocks) {\n\t\txfs_alert(mp,\n\t\"reflink not compatible with realtime device!\");\n\t\terror = -EINVAL;\n\t\tgoto out_filestream_unmount;\n\t}\n\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb) && mp->m_sb.sb_rblocks) {\n\t\txfs_alert(mp,\n\t\"reverse mapping btree not compatible with realtime device!\");\n\t\terror = -EINVAL;\n\t\tgoto out_filestream_unmount;\n\t}\n\n\terror = xfs_mountfs(mp);\n\tif (error)\n\t\tgoto out_filestream_unmount;\n\n\troot = igrab(VFS_I(mp->m_rootip));\n\tif (!root) {\n\t\terror = -ENOENT;\n\t\tgoto out_unmount;\n\t}\n\tsb->s_root = d_make_root(root);\n\tif (!sb->s_root) {\n\t\terror = -ENOMEM;\n\t\tgoto out_unmount;\n\t}\n\n\treturn 0;\n\n out_filestream_unmount:\n\txfs_filestream_unmount(mp);\n out_free_sb:\n\txfs_freesb(mp);\n out_free_stats:\n\tfree_percpu(mp->m_stats.xs_stats);\n out_destroy_counters:\n\txfs_destroy_percpu_counters(mp);\n out_destroy_workqueues:\n\txfs_destroy_mount_workqueues(mp);\n out_close_devices:\n\txfs_close_devices(mp);\n out_free_fsname:\n\txfs_free_fsname(mp);\n\tkfree(mp);\n out:\n\treturn error;\n\n out_unmount:\n\txfs_filestream_unmount(mp);\n\txfs_unmountfs(mp);\n\tgoto out_free_sb;\n}",
        "code_after_change": "STATIC int\nxfs_fs_fill_super(\n\tstruct super_block\t*sb,\n\tvoid\t\t\t*data,\n\tint\t\t\tsilent)\n{\n\tstruct inode\t\t*root;\n\tstruct xfs_mount\t*mp = NULL;\n\tint\t\t\tflags = 0, error = -ENOMEM;\n\n\t/*\n\t * allocate mp and do all low-level struct initializations before we\n\t * attach it to the super\n\t */\n\tmp = xfs_mount_alloc(sb);\n\tif (!mp)\n\t\tgoto out;\n\tsb->s_fs_info = mp;\n\n\terror = xfs_parseargs(mp, (char *)data);\n\tif (error)\n\t\tgoto out_free_fsname;\n\n\tsb_min_blocksize(sb, BBSIZE);\n\tsb->s_xattr = xfs_xattr_handlers;\n\tsb->s_export_op = &xfs_export_operations;\n#ifdef CONFIG_XFS_QUOTA\n\tsb->s_qcop = &xfs_quotactl_operations;\n\tsb->s_quota_types = QTYPE_MASK_USR | QTYPE_MASK_GRP | QTYPE_MASK_PRJ;\n#endif\n\tsb->s_op = &xfs_super_operations;\n\n\t/*\n\t * Delay mount work if the debug hook is set. This is debug\n\t * instrumention to coordinate simulation of xfs mount failures with\n\t * VFS superblock operations\n\t */\n\tif (xfs_globals.mount_delay) {\n\t\txfs_notice(mp, \"Delaying mount for %d seconds.\",\n\t\t\txfs_globals.mount_delay);\n\t\tmsleep(xfs_globals.mount_delay * 1000);\n\t}\n\n\tif (silent)\n\t\tflags |= XFS_MFSI_QUIET;\n\n\terror = xfs_open_devices(mp);\n\tif (error)\n\t\tgoto out_free_fsname;\n\n\terror = xfs_init_mount_workqueues(mp);\n\tif (error)\n\t\tgoto out_close_devices;\n\n\terror = xfs_init_percpu_counters(mp);\n\tif (error)\n\t\tgoto out_destroy_workqueues;\n\n\t/* Allocate stats memory before we do operations that might use it */\n\tmp->m_stats.xs_stats = alloc_percpu(struct xfsstats);\n\tif (!mp->m_stats.xs_stats) {\n\t\terror = -ENOMEM;\n\t\tgoto out_destroy_counters;\n\t}\n\n\terror = xfs_readsb(mp, flags);\n\tif (error)\n\t\tgoto out_free_stats;\n\n\terror = xfs_finish_flags(mp);\n\tif (error)\n\t\tgoto out_free_sb;\n\n\terror = xfs_setup_devices(mp);\n\tif (error)\n\t\tgoto out_free_sb;\n\n\terror = xfs_filestream_mount(mp);\n\tif (error)\n\t\tgoto out_free_sb;\n\n\t/*\n\t * we must configure the block size in the superblock before we run the\n\t * full mount process as the mount process can lookup and cache inodes.\n\t */\n\tsb->s_magic = XFS_SB_MAGIC;\n\tsb->s_blocksize = mp->m_sb.sb_blocksize;\n\tsb->s_blocksize_bits = ffs(sb->s_blocksize) - 1;\n\tsb->s_maxbytes = xfs_max_file_offset(sb->s_blocksize_bits);\n\tsb->s_max_links = XFS_MAXLINK;\n\tsb->s_time_gran = 1;\n\tset_posix_acl_flag(sb);\n\n\t/* version 5 superblocks support inode version counters. */\n\tif (XFS_SB_VERSION_NUM(&mp->m_sb) == XFS_SB_VERSION_5)\n\t\tsb->s_flags |= SB_I_VERSION;\n\n\tif (mp->m_flags & XFS_MOUNT_DAX) {\n\t\txfs_warn(mp,\n\t\t\"DAX enabled. Warning: EXPERIMENTAL, use at your own risk\");\n\n\t\terror = bdev_dax_supported(sb, sb->s_blocksize);\n\t\tif (error) {\n\t\t\txfs_alert(mp,\n\t\t\t\"DAX unsupported by block device. Turning off DAX.\");\n\t\t\tmp->m_flags &= ~XFS_MOUNT_DAX;\n\t\t}\n\t\tif (xfs_sb_version_hasreflink(&mp->m_sb)) {\n\t\t\txfs_alert(mp,\n\t\t\"DAX and reflink cannot be used together!\");\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_filestream_unmount;\n\t\t}\n\t}\n\n\tif (mp->m_flags & XFS_MOUNT_DISCARD) {\n\t\tstruct request_queue *q = bdev_get_queue(sb->s_bdev);\n\n\t\tif (!blk_queue_discard(q)) {\n\t\t\txfs_warn(mp, \"mounting with \\\"discard\\\" option, but \"\n\t\t\t\t\t\"the device does not support discard\");\n\t\t\tmp->m_flags &= ~XFS_MOUNT_DISCARD;\n\t\t}\n\t}\n\n\tif (xfs_sb_version_hasreflink(&mp->m_sb) && mp->m_sb.sb_rblocks) {\n\t\txfs_alert(mp,\n\t\"reflink not compatible with realtime device!\");\n\t\terror = -EINVAL;\n\t\tgoto out_filestream_unmount;\n\t}\n\n\tif (xfs_sb_version_hasrmapbt(&mp->m_sb) && mp->m_sb.sb_rblocks) {\n\t\txfs_alert(mp,\n\t\"reverse mapping btree not compatible with realtime device!\");\n\t\terror = -EINVAL;\n\t\tgoto out_filestream_unmount;\n\t}\n\n\terror = xfs_mountfs(mp);\n\tif (error)\n\t\tgoto out_filestream_unmount;\n\n\troot = igrab(VFS_I(mp->m_rootip));\n\tif (!root) {\n\t\terror = -ENOENT;\n\t\tgoto out_unmount;\n\t}\n\tsb->s_root = d_make_root(root);\n\tif (!sb->s_root) {\n\t\terror = -ENOMEM;\n\t\tgoto out_unmount;\n\t}\n\n\treturn 0;\n\n out_filestream_unmount:\n\txfs_filestream_unmount(mp);\n out_free_sb:\n\txfs_freesb(mp);\n out_free_stats:\n\tfree_percpu(mp->m_stats.xs_stats);\n out_destroy_counters:\n\txfs_destroy_percpu_counters(mp);\n out_destroy_workqueues:\n\txfs_destroy_mount_workqueues(mp);\n out_close_devices:\n\txfs_close_devices(mp);\n out_free_fsname:\n\tsb->s_fs_info = NULL;\n\txfs_free_fsname(mp);\n\tkfree(mp);\n out:\n\treturn error;\n\n out_unmount:\n\txfs_filestream_unmount(mp);\n\txfs_unmountfs(mp);\n\tgoto out_free_sb;\n}",
        "patch": "--- code before\n+++ code after\n@@ -167,6 +167,7 @@\n  out_close_devices:\n \txfs_close_devices(mp);\n  out_free_fsname:\n+\tsb->s_fs_info = NULL;\n \txfs_free_fsname(mp);\n \tkfree(mp);\n  out:",
        "function_modified_lines": {
            "added": [
                "\tsb->s_fs_info = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in fs/xfs/xfs_super.c in the Linux kernel before 4.18. A use after free exists, related to xfs_fs_fill_super failure.",
        "id": 1792
    },
    {
        "cve_id": "CVE-2023-25012",
        "code_before_change": "static void bigben_worker(struct work_struct *work)\n{\n\tstruct bigben_device *bigben = container_of(work,\n\t\tstruct bigben_device, worker);\n\tstruct hid_field *report_field = bigben->report->field[0];\n\tbool do_work_led = false;\n\tbool do_work_ff = false;\n\tu8 *buf;\n\tu32 len;\n\tunsigned long flags;\n\n\tif (bigben->removed)\n\t\treturn;\n\n\tbuf = hid_alloc_report_buf(bigben->report, GFP_KERNEL);\n\tif (!buf)\n\t\treturn;\n\n\tlen = hid_report_len(bigben->report);\n\n\t/* LED work */\n\tspin_lock_irqsave(&bigben->lock, flags);\n\n\tif (bigben->work_led) {\n\t\tbigben->work_led = false;\n\t\tdo_work_led = true;\n\t\treport_field->value[0] = 0x01; /* 1 = led message */\n\t\treport_field->value[1] = 0x08; /* reserved value, always 8 */\n\t\treport_field->value[2] = bigben->led_state;\n\t\treport_field->value[3] = 0x00; /* padding */\n\t\treport_field->value[4] = 0x00; /* padding */\n\t\treport_field->value[5] = 0x00; /* padding */\n\t\treport_field->value[6] = 0x00; /* padding */\n\t\treport_field->value[7] = 0x00; /* padding */\n\t\thid_output_report(bigben->report, buf);\n\t}\n\n\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\tif (do_work_led) {\n\t\thid_hw_raw_request(bigben->hid, bigben->report->id, buf, len,\n\t\t\t\t   bigben->report->type, HID_REQ_SET_REPORT);\n\t}\n\n\t/* FF work */\n\tspin_lock_irqsave(&bigben->lock, flags);\n\n\tif (bigben->work_ff) {\n\t\tbigben->work_ff = false;\n\t\tdo_work_ff = true;\n\t\treport_field->value[0] = 0x02; /* 2 = rumble effect message */\n\t\treport_field->value[1] = 0x08; /* reserved value, always 8 */\n\t\treport_field->value[2] = bigben->right_motor_on;\n\t\treport_field->value[3] = bigben->left_motor_force;\n\t\treport_field->value[4] = 0xff; /* duration 0-254 (255 = nonstop) */\n\t\treport_field->value[5] = 0x00; /* padding */\n\t\treport_field->value[6] = 0x00; /* padding */\n\t\treport_field->value[7] = 0x00; /* padding */\n\t\thid_output_report(bigben->report, buf);\n\t}\n\n\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\tif (do_work_ff) {\n\t\thid_hw_raw_request(bigben->hid, bigben->report->id, buf, len,\n\t\t\t\t   bigben->report->type, HID_REQ_SET_REPORT);\n\t}\n\n\tkfree(buf);\n}",
        "code_after_change": "static void bigben_worker(struct work_struct *work)\n{\n\tstruct bigben_device *bigben = container_of(work,\n\t\tstruct bigben_device, worker);\n\tstruct hid_field *report_field = bigben->report->field[0];\n\tbool do_work_led = false;\n\tbool do_work_ff = false;\n\tu8 *buf;\n\tu32 len;\n\tunsigned long flags;\n\n\tbuf = hid_alloc_report_buf(bigben->report, GFP_KERNEL);\n\tif (!buf)\n\t\treturn;\n\n\tlen = hid_report_len(bigben->report);\n\n\t/* LED work */\n\tspin_lock_irqsave(&bigben->lock, flags);\n\n\tif (bigben->work_led) {\n\t\tbigben->work_led = false;\n\t\tdo_work_led = true;\n\t\treport_field->value[0] = 0x01; /* 1 = led message */\n\t\treport_field->value[1] = 0x08; /* reserved value, always 8 */\n\t\treport_field->value[2] = bigben->led_state;\n\t\treport_field->value[3] = 0x00; /* padding */\n\t\treport_field->value[4] = 0x00; /* padding */\n\t\treport_field->value[5] = 0x00; /* padding */\n\t\treport_field->value[6] = 0x00; /* padding */\n\t\treport_field->value[7] = 0x00; /* padding */\n\t\thid_output_report(bigben->report, buf);\n\t}\n\n\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\tif (do_work_led) {\n\t\thid_hw_raw_request(bigben->hid, bigben->report->id, buf, len,\n\t\t\t\t   bigben->report->type, HID_REQ_SET_REPORT);\n\t}\n\n\t/* FF work */\n\tspin_lock_irqsave(&bigben->lock, flags);\n\n\tif (bigben->work_ff) {\n\t\tbigben->work_ff = false;\n\t\tdo_work_ff = true;\n\t\treport_field->value[0] = 0x02; /* 2 = rumble effect message */\n\t\treport_field->value[1] = 0x08; /* reserved value, always 8 */\n\t\treport_field->value[2] = bigben->right_motor_on;\n\t\treport_field->value[3] = bigben->left_motor_force;\n\t\treport_field->value[4] = 0xff; /* duration 0-254 (255 = nonstop) */\n\t\treport_field->value[5] = 0x00; /* padding */\n\t\treport_field->value[6] = 0x00; /* padding */\n\t\treport_field->value[7] = 0x00; /* padding */\n\t\thid_output_report(bigben->report, buf);\n\t}\n\n\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\tif (do_work_ff) {\n\t\thid_hw_raw_request(bigben->hid, bigben->report->id, buf, len,\n\t\t\t\t   bigben->report->type, HID_REQ_SET_REPORT);\n\t}\n\n\tkfree(buf);\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,9 +8,6 @@\n \tu8 *buf;\n \tu32 len;\n \tunsigned long flags;\n-\n-\tif (bigben->removed)\n-\t\treturn;\n \n \tbuf = hid_alloc_report_buf(bigben->report, GFP_KERNEL);\n \tif (!buf)",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "",
                "\tif (bigben->removed)",
                "\t\treturn;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel through 6.1.9 has a Use-After-Free in bigben_remove in drivers/hid/hid-bigbenff.c via a crafted USB device because the LED controllers remain registered for too long.",
        "id": 3956
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n{\n\tstruct io_submit_state *state;\n\tunsigned int sqe_flags;\n\tint id, ret = 0;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags = sqe_flags = READ_ONCE(sqe->flags);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->link = NULL;\n\treq->fixed_rsrc_refs = NULL;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS)) {\n\t\treq->flags = 0;\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm_files(ctx, req)))\n\t\treturn -EFAULT;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\tstruct io_identity *iod;\n\n\t\tiod = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!iod))\n\t\t\treturn -EINVAL;\n\t\trefcount_inc(&iod->count);\n\n\t\t__io_req_init_async(req);\n\t\tget_cred(iod->creds);\n\t\treq->work.identity = iod;\n\t}\n\n\tstate = &ctx->submit_state;\n\n\t/*\n\t * Plug now if we have more than 1 IO left after this, and the target\n\t * is potentially a read/write to block based storage.\n\t */\n\tif (!state->plug_started && state->ios_left > 1 &&\n\t    io_op_defs[req->opcode].plug) {\n\t\tblk_start_plug(&state->plug);\n\t\tstate->plug_started = true;\n\t}\n\n\tif (io_op_defs[req->opcode].needs_file) {\n\t\tbool fixed = req->flags & REQ_F_FIXED_FILE;\n\n\t\treq->file = io_file_get(state, req, READ_ONCE(sqe->fd), fixed);\n\t\tif (unlikely(!req->file))\n\t\t\tret = -EBADF;\n\t}\n\n\tstate->ios_left--;\n\treturn ret;\n}",
        "code_after_change": "static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n{\n\tstruct io_submit_state *state;\n\tunsigned int sqe_flags;\n\tint id, ret = 0;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags = sqe_flags = READ_ONCE(sqe->flags);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->link = NULL;\n\treq->fixed_rsrc_refs = NULL;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS)) {\n\t\treq->flags = 0;\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm_files(ctx, req)))\n\t\treturn -EFAULT;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\t__io_req_init_async(req);\n\t\treq->work.creds = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!req->work.creds))\n\t\t\treturn -EINVAL;\n\t\tget_cred(req->work.creds);\n\t}\n\n\tstate = &ctx->submit_state;\n\n\t/*\n\t * Plug now if we have more than 1 IO left after this, and the target\n\t * is potentially a read/write to block based storage.\n\t */\n\tif (!state->plug_started && state->ios_left > 1 &&\n\t    io_op_defs[req->opcode].plug) {\n\t\tblk_start_plug(&state->plug);\n\t\tstate->plug_started = true;\n\t}\n\n\tif (io_op_defs[req->opcode].needs_file) {\n\t\tbool fixed = req->flags & REQ_F_FIXED_FILE;\n\n\t\treq->file = io_file_get(state, req, READ_ONCE(sqe->fd), fixed);\n\t\tif (unlikely(!req->file))\n\t\t\tret = -EBADF;\n\t}\n\n\tstate->ios_left--;\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -40,16 +40,11 @@\n \n \tid = READ_ONCE(sqe->personality);\n \tif (id) {\n-\t\tstruct io_identity *iod;\n-\n-\t\tiod = idr_find(&ctx->personality_idr, id);\n-\t\tif (unlikely(!iod))\n+\t\t__io_req_init_async(req);\n+\t\treq->work.creds = idr_find(&ctx->personality_idr, id);\n+\t\tif (unlikely(!req->work.creds))\n \t\t\treturn -EINVAL;\n-\t\trefcount_inc(&iod->count);\n-\n-\t\t__io_req_init_async(req);\n-\t\tget_cred(iod->creds);\n-\t\treq->work.identity = iod;\n+\t\tget_cred(req->work.creds);\n \t}\n \n \tstate = &ctx->submit_state;",
        "function_modified_lines": {
            "added": [
                "\t\t__io_req_init_async(req);",
                "\t\treq->work.creds = idr_find(&ctx->personality_idr, id);",
                "\t\tif (unlikely(!req->work.creds))",
                "\t\tget_cred(req->work.creds);"
            ],
            "deleted": [
                "\t\tstruct io_identity *iod;",
                "",
                "\t\tiod = idr_find(&ctx->personality_idr, id);",
                "\t\tif (unlikely(!iod))",
                "\t\trefcount_inc(&iod->count);",
                "",
                "\t\t__io_req_init_async(req);",
                "\t\tget_cred(iod->creds);",
                "\t\treq->work.identity = iod;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3359
    },
    {
        "cve_id": "CVE-2023-35828",
        "code_before_change": "static int renesas_usb3_remove(struct platform_device *pdev)\n{\n\tstruct renesas_usb3 *usb3 = platform_get_drvdata(pdev);\n\n\tdebugfs_remove_recursive(usb3->dentry);\n\tdevice_remove_file(&pdev->dev, &dev_attr_role);\n\n\tusb_role_switch_unregister(usb3->role_sw);\n\n\tusb_del_gadget_udc(&usb3->gadget);\n\treset_control_assert(usb3->usbp_rstc);\n\trenesas_usb3_dma_free_prd(usb3, &pdev->dev);\n\n\t__renesas_usb3_ep_free_request(usb3->ep0_req);\n\tpm_runtime_disable(&pdev->dev);\n\n\treturn 0;\n}",
        "code_after_change": "static int renesas_usb3_remove(struct platform_device *pdev)\n{\n\tstruct renesas_usb3 *usb3 = platform_get_drvdata(pdev);\n\n\tdebugfs_remove_recursive(usb3->dentry);\n\tdevice_remove_file(&pdev->dev, &dev_attr_role);\n\n\tcancel_work_sync(&usb3->role_work);\n\tusb_role_switch_unregister(usb3->role_sw);\n\n\tusb_del_gadget_udc(&usb3->gadget);\n\treset_control_assert(usb3->usbp_rstc);\n\trenesas_usb3_dma_free_prd(usb3, &pdev->dev);\n\n\t__renesas_usb3_ep_free_request(usb3->ep0_req);\n\tpm_runtime_disable(&pdev->dev);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tdebugfs_remove_recursive(usb3->dentry);\n \tdevice_remove_file(&pdev->dev, &dev_attr_role);\n \n+\tcancel_work_sync(&usb3->role_work);\n \tusb_role_switch_unregister(usb3->role_sw);\n \n \tusb_del_gadget_udc(&usb3->gadget);",
        "function_modified_lines": {
            "added": [
                "\tcancel_work_sync(&usb3->role_work);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 6.3.2. A use-after-free was found in renesas_usb3_remove in drivers/usb/gadget/udc/renesas_usb3.c.",
        "id": 4115
    },
    {
        "cve_id": "CVE-2017-16939",
        "code_before_change": "static int xfrm_dump_policy(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];\n\tstruct xfrm_dump_info info;\n\n\tBUILD_BUG_ON(sizeof(struct xfrm_policy_walk) >\n\t\t     sizeof(cb->args) - sizeof(cb->args[0]));\n\n\tinfo.in_skb = cb->skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = cb->nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = NLM_F_MULTI;\n\n\tif (!cb->args[0]) {\n\t\tcb->args[0] = 1;\n\t\txfrm_policy_walk_init(walk, XFRM_POLICY_TYPE_ANY);\n\t}\n\n\t(void) xfrm_policy_walk(net, walk, dump_one_policy, &info);\n\n\treturn skb->len;\n}",
        "code_after_change": "static int xfrm_dump_policy(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;\n\tstruct xfrm_dump_info info;\n\n\tinfo.in_skb = cb->skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = cb->nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = NLM_F_MULTI;\n\n\t(void) xfrm_policy_walk(net, walk, dump_one_policy, &info);\n\n\treturn skb->len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,22 +1,14 @@\n static int xfrm_dump_policy(struct sk_buff *skb, struct netlink_callback *cb)\n {\n \tstruct net *net = sock_net(skb->sk);\n-\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];\n+\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;\n \tstruct xfrm_dump_info info;\n-\n-\tBUILD_BUG_ON(sizeof(struct xfrm_policy_walk) >\n-\t\t     sizeof(cb->args) - sizeof(cb->args[0]));\n \n \tinfo.in_skb = cb->skb;\n \tinfo.out_skb = skb;\n \tinfo.nlmsg_seq = cb->nlh->nlmsg_seq;\n \tinfo.nlmsg_flags = NLM_F_MULTI;\n \n-\tif (!cb->args[0]) {\n-\t\tcb->args[0] = 1;\n-\t\txfrm_policy_walk_init(walk, XFRM_POLICY_TYPE_ANY);\n-\t}\n-\n \t(void) xfrm_policy_walk(net, walk, dump_one_policy, &info);\n \n \treturn skb->len;",
        "function_modified_lines": {
            "added": [
                "\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;"
            ],
            "deleted": [
                "\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];",
                "",
                "\tBUILD_BUG_ON(sizeof(struct xfrm_policy_walk) >",
                "\t\t     sizeof(cb->args) - sizeof(cb->args[0]));",
                "\tif (!cb->args[0]) {",
                "\t\tcb->args[0] = 1;",
                "\t\txfrm_policy_walk_init(walk, XFRM_POLICY_TYPE_ANY);",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The XFRM dump policy implementation in net/xfrm/xfrm_user.c in the Linux kernel before 4.13.11 allows local users to gain privileges or cause a denial of service (use-after-free) via a crafted SO_RCVBUF setsockopt system call in conjunction with XFRM_MSG_GETPOLICY Netlink messages.",
        "id": 1353
    },
    {
        "cve_id": "CVE-2016-6828",
        "code_before_change": "static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unlinked)\n{\n\tif (sk->sk_send_head == skb_unlinked)\n\t\tsk->sk_send_head = NULL;\n}",
        "code_after_change": "static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unlinked)\n{\n\tif (sk->sk_send_head == skb_unlinked)\n\t\tsk->sk_send_head = NULL;\n\tif (tcp_sk(sk)->highest_sack == skb_unlinked)\n\t\ttcp_sk(sk)->highest_sack = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,4 +2,6 @@\n {\n \tif (sk->sk_send_head == skb_unlinked)\n \t\tsk->sk_send_head = NULL;\n+\tif (tcp_sk(sk)->highest_sack == skb_unlinked)\n+\t\ttcp_sk(sk)->highest_sack = NULL;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (tcp_sk(sk)->highest_sack == skb_unlinked)",
                "\t\ttcp_sk(sk)->highest_sack = NULL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The tcp_check_send_head function in include/net/tcp.h in the Linux kernel before 4.7.5 does not properly maintain certain SACK state after a failed data copy, which allows local users to cause a denial of service (tcp_xmit_retransmit_queue use-after-free and system crash) via a crafted SACK option.",
        "id": 1091
    },
    {
        "cve_id": "CVE-2018-14625",
        "code_before_change": "static int vhost_vsock_set_cid(struct vhost_vsock *vsock, u64 guest_cid)\n{\n\tstruct vhost_vsock *other;\n\n\t/* Refuse reserved CIDs */\n\tif (guest_cid <= VMADDR_CID_HOST ||\n\t    guest_cid == U32_MAX)\n\t\treturn -EINVAL;\n\n\t/* 64-bit CIDs are not yet supported */\n\tif (guest_cid > U32_MAX)\n\t\treturn -EINVAL;\n\n\t/* Refuse if CID is already in use */\n\tspin_lock_bh(&vhost_vsock_lock);\n\tother = __vhost_vsock_get(guest_cid);\n\tif (other && other != vsock) {\n\t\tspin_unlock_bh(&vhost_vsock_lock);\n\t\treturn -EADDRINUSE;\n\t}\n\tvsock->guest_cid = guest_cid;\n\tspin_unlock_bh(&vhost_vsock_lock);\n\n\treturn 0;\n}",
        "code_after_change": "static int vhost_vsock_set_cid(struct vhost_vsock *vsock, u64 guest_cid)\n{\n\tstruct vhost_vsock *other;\n\n\t/* Refuse reserved CIDs */\n\tif (guest_cid <= VMADDR_CID_HOST ||\n\t    guest_cid == U32_MAX)\n\t\treturn -EINVAL;\n\n\t/* 64-bit CIDs are not yet supported */\n\tif (guest_cid > U32_MAX)\n\t\treturn -EINVAL;\n\n\t/* Refuse if CID is already in use */\n\tspin_lock_bh(&vhost_vsock_lock);\n\tother = vhost_vsock_get(guest_cid);\n\tif (other && other != vsock) {\n\t\tspin_unlock_bh(&vhost_vsock_lock);\n\t\treturn -EADDRINUSE;\n\t}\n\n\tif (vsock->guest_cid)\n\t\thash_del_rcu(&vsock->hash);\n\n\tvsock->guest_cid = guest_cid;\n\thash_add_rcu(vhost_vsock_hash, &vsock->hash, guest_cid);\n\tspin_unlock_bh(&vhost_vsock_lock);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,12 +13,17 @@\n \n \t/* Refuse if CID is already in use */\n \tspin_lock_bh(&vhost_vsock_lock);\n-\tother = __vhost_vsock_get(guest_cid);\n+\tother = vhost_vsock_get(guest_cid);\n \tif (other && other != vsock) {\n \t\tspin_unlock_bh(&vhost_vsock_lock);\n \t\treturn -EADDRINUSE;\n \t}\n+\n+\tif (vsock->guest_cid)\n+\t\thash_del_rcu(&vsock->hash);\n+\n \tvsock->guest_cid = guest_cid;\n+\thash_add_rcu(vhost_vsock_hash, &vsock->hash, guest_cid);\n \tspin_unlock_bh(&vhost_vsock_lock);\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tother = vhost_vsock_get(guest_cid);",
                "",
                "\tif (vsock->guest_cid)",
                "\t\thash_del_rcu(&vsock->hash);",
                "",
                "\thash_add_rcu(vhost_vsock_hash, &vsock->hash, guest_cid);"
            ],
            "deleted": [
                "\tother = __vhost_vsock_get(guest_cid);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A flaw was found in the Linux Kernel where an attacker may be able to have an uncontrolled read to kernel-memory from within a vm guest. A race condition between connect() and close() function may allow an attacker using the AF_VSOCK protocol to gather a 4 byte information leak or possibly intercept or corrupt AF_VSOCK messages destined to other clients.",
        "id": 1696
    },
    {
        "cve_id": "CVE-2023-4133",
        "code_before_change": "void cxgb4_cleanup_tc_flower(struct adapter *adap)\n{\n\tif (!adap->tc_flower_initialized)\n\t\treturn;\n\n\tif (adap->flower_stats_timer.function)\n\t\tdel_timer_sync(&adap->flower_stats_timer);\n\tcancel_work_sync(&adap->flower_stats_work);\n\trhashtable_destroy(&adap->flower_tbl);\n\tadap->tc_flower_initialized = false;\n}",
        "code_after_change": "void cxgb4_cleanup_tc_flower(struct adapter *adap)\n{\n\tif (!adap->tc_flower_initialized)\n\t\treturn;\n\n\tif (adap->flower_stats_timer.function)\n\t\ttimer_shutdown_sync(&adap->flower_stats_timer);\n\tcancel_work_sync(&adap->flower_stats_work);\n\trhashtable_destroy(&adap->flower_tbl);\n\tadap->tc_flower_initialized = false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,7 @@\n \t\treturn;\n \n \tif (adap->flower_stats_timer.function)\n-\t\tdel_timer_sync(&adap->flower_stats_timer);\n+\t\ttimer_shutdown_sync(&adap->flower_stats_timer);\n \tcancel_work_sync(&adap->flower_stats_work);\n \trhashtable_destroy(&adap->flower_tbl);\n \tadap->tc_flower_initialized = false;",
        "function_modified_lines": {
            "added": [
                "\t\ttimer_shutdown_sync(&adap->flower_stats_timer);"
            ],
            "deleted": [
                "\t\tdel_timer_sync(&adap->flower_stats_timer);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability was found in the cxgb4 driver in the Linux kernel. The bug occurs when the cxgb4 device is detaching due to a possible rearming of the flower_stats_timer from the work queue. This flaw allows a local user to crash the system, causing a denial of service condition.",
        "id": 4193
    },
    {
        "cve_id": "CVE-2022-20409",
        "code_before_change": "void __io_uring_free(struct task_struct *tsk)\n{\n\tstruct io_uring_task *tctx = tsk->io_uring;\n\n\tWARN_ON_ONCE(!xa_empty(&tctx->xa));\n\tWARN_ON_ONCE(refcount_read(&tctx->identity->count) != 1);\n\tif (tctx->identity != &tctx->__identity)\n\t\tkfree(tctx->identity);\n\tpercpu_counter_destroy(&tctx->inflight);\n\tkfree(tctx);\n\ttsk->io_uring = NULL;\n}",
        "code_after_change": "void __io_uring_free(struct task_struct *tsk)\n{\n\tstruct io_uring_task *tctx = tsk->io_uring;\n\n\tWARN_ON_ONCE(!xa_empty(&tctx->xa));\n\tpercpu_counter_destroy(&tctx->inflight);\n\tkfree(tctx);\n\ttsk->io_uring = NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,9 +3,6 @@\n \tstruct io_uring_task *tctx = tsk->io_uring;\n \n \tWARN_ON_ONCE(!xa_empty(&tctx->xa));\n-\tWARN_ON_ONCE(refcount_read(&tctx->identity->count) != 1);\n-\tif (tctx->identity != &tctx->__identity)\n-\t\tkfree(tctx->identity);\n \tpercpu_counter_destroy(&tctx->inflight);\n \tkfree(tctx);\n \ttsk->io_uring = NULL;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tWARN_ON_ONCE(refcount_read(&tctx->identity->count) != 1);",
                "\tif (tctx->identity != &tctx->__identity)",
                "\t\tkfree(tctx->identity);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In io_identity_cow of io_uring.c, there is a possible way to corrupt memory due to a use after free. This could lead to local escalation of privilege with System execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-238177383References: Upstream kernel",
        "id": 3361
    },
    {
        "cve_id": "CVE-2019-15292",
        "code_before_change": "void __exit atalk_proc_exit(void)\n{\n\tremove_proc_subtree(\"atalk\", init_net.proc_net);\n}",
        "code_after_change": "void atalk_proc_exit(void)\n{\n\tremove_proc_subtree(\"atalk\", init_net.proc_net);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-void __exit atalk_proc_exit(void)\n+void atalk_proc_exit(void)\n {\n \tremove_proc_subtree(\"atalk\", init_net.proc_net);\n }",
        "function_modified_lines": {
            "added": [
                "void atalk_proc_exit(void)"
            ],
            "deleted": [
                "void __exit atalk_proc_exit(void)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.9. There is a use-after-free in atalk_proc_exit, related to net/appletalk/atalk_proc.c, net/appletalk/ddp.c, and net/appletalk/sysctl_net_atalk.c.",
        "id": 2015
    },
    {
        "cve_id": "CVE-2018-14734",
        "code_before_change": "static struct ucma_multicast* ucma_alloc_multicast(struct ucma_context *ctx)\n{\n\tstruct ucma_multicast *mc;\n\n\tmc = kzalloc(sizeof(*mc), GFP_KERNEL);\n\tif (!mc)\n\t\treturn NULL;\n\n\tmutex_lock(&mut);\n\tmc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);\n\tmutex_unlock(&mut);\n\tif (mc->id < 0)\n\t\tgoto error;\n\n\tmc->ctx = ctx;\n\tlist_add_tail(&mc->list, &ctx->mc_list);\n\treturn mc;\n\nerror:\n\tkfree(mc);\n\treturn NULL;\n}",
        "code_after_change": "static struct ucma_multicast* ucma_alloc_multicast(struct ucma_context *ctx)\n{\n\tstruct ucma_multicast *mc;\n\n\tmc = kzalloc(sizeof(*mc), GFP_KERNEL);\n\tif (!mc)\n\t\treturn NULL;\n\n\tmutex_lock(&mut);\n\tmc->id = idr_alloc(&multicast_idr, NULL, 0, 0, GFP_KERNEL);\n\tmutex_unlock(&mut);\n\tif (mc->id < 0)\n\t\tgoto error;\n\n\tmc->ctx = ctx;\n\tlist_add_tail(&mc->list, &ctx->mc_list);\n\treturn mc;\n\nerror:\n\tkfree(mc);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \t\treturn NULL;\n \n \tmutex_lock(&mut);\n-\tmc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);\n+\tmc->id = idr_alloc(&multicast_idr, NULL, 0, 0, GFP_KERNEL);\n \tmutex_unlock(&mut);\n \tif (mc->id < 0)\n \t\tgoto error;",
        "function_modified_lines": {
            "added": [
                "\tmc->id = idr_alloc(&multicast_idr, NULL, 0, 0, GFP_KERNEL);"
            ],
            "deleted": [
                "\tmc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "drivers/infiniband/core/ucma.c in the Linux kernel through 4.17.11 allows ucma_leave_multicast to access a certain data structure after a cleanup step in ucma_process_join, which allows attackers to cause a denial of service (use-after-free).",
        "id": 1706
    },
    {
        "cve_id": "CVE-2023-1872",
        "code_before_change": "static void io_apoll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tret = io_poll_check_events(req);\n\tif (ret > 0)\n\t\treturn;\n\n\tio_poll_remove_entries(req);\n\tspin_lock(&ctx->completion_lock);\n\thash_del(&req->hash_node);\n\tspin_unlock(&ctx->completion_lock);\n\n\tif (!ret)\n\t\tio_req_task_submit(req, locked);\n\telse\n\t\tio_req_complete_failed(req, ret);\n}",
        "code_after_change": "static void io_apoll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tret = io_poll_check_events(req, *locked);\n\tif (ret > 0)\n\t\treturn;\n\n\tio_poll_remove_entries(req);\n\tspin_lock(&ctx->completion_lock);\n\thash_del(&req->hash_node);\n\tspin_unlock(&ctx->completion_lock);\n\n\tif (!ret)\n\t\tio_req_task_submit(req, locked);\n\telse\n\t\tio_req_complete_failed(req, ret);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \tstruct io_ring_ctx *ctx = req->ctx;\n \tint ret;\n \n-\tret = io_poll_check_events(req);\n+\tret = io_poll_check_events(req, *locked);\n \tif (ret > 0)\n \t\treturn;\n ",
        "function_modified_lines": {
            "added": [
                "\tret = io_poll_check_events(req, *locked);"
            ],
            "deleted": [
                "\tret = io_poll_check_events(req);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux Kernel io_uring system can be exploited to achieve local privilege escalation.\n\nThe io_file_get_fixed function lacks the presence of ctx->uring_lock which can lead to a Use-After-Free vulnerability due a race condition with fixed files getting unregistered.\n\nWe recommend upgrading past commit da24142b1ef9fd5d36b76e36bab328a5b27523e8.\n\n",
        "id": 3883
    },
    {
        "cve_id": "CVE-2019-19767",
        "code_before_change": "static int __ext4_expand_extra_isize(struct inode *inode,\n\t\t\t\t     unsigned int new_extra_isize,\n\t\t\t\t     struct ext4_iloc *iloc,\n\t\t\t\t     handle_t *handle, int *no_expand)\n{\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_xattr_ibody_header *header;\n\tint error;\n\n\traw_inode = ext4_raw_inode(iloc);\n\n\theader = IHDR(inode, raw_inode);\n\n\t/* No extended attributes present */\n\tif (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) ||\n\t    header->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC)) {\n\t\tmemset((void *)raw_inode + EXT4_GOOD_OLD_INODE_SIZE +\n\t\t       EXT4_I(inode)->i_extra_isize, 0,\n\t\t       new_extra_isize - EXT4_I(inode)->i_extra_isize);\n\t\tEXT4_I(inode)->i_extra_isize = new_extra_isize;\n\t\treturn 0;\n\t}\n\n\t/* try to expand with EAs present */\n\terror = ext4_expand_extra_isize_ea(inode, new_extra_isize,\n\t\t\t\t\t   raw_inode, handle);\n\tif (error) {\n\t\t/*\n\t\t * Inode size expansion failed; don't try again\n\t\t */\n\t\t*no_expand = 1;\n\t}\n\n\treturn error;\n}",
        "code_after_change": "static int __ext4_expand_extra_isize(struct inode *inode,\n\t\t\t\t     unsigned int new_extra_isize,\n\t\t\t\t     struct ext4_iloc *iloc,\n\t\t\t\t     handle_t *handle, int *no_expand)\n{\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_xattr_ibody_header *header;\n\tunsigned int inode_size = EXT4_INODE_SIZE(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint error;\n\n\t/* this was checked at iget time, but double check for good measure */\n\tif ((EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize > inode_size) ||\n\t    (ei->i_extra_isize & 3)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extra_isize %u (inode size %u)\",\n\t\t\t\t ei->i_extra_isize,\n\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n\t\treturn -EFSCORRUPTED;\n\t}\n\tif ((new_extra_isize < ei->i_extra_isize) ||\n\t    (new_extra_isize < 4) ||\n\t    (new_extra_isize > inode_size - EXT4_GOOD_OLD_INODE_SIZE))\n\t\treturn -EINVAL;\t/* Should never happen */\n\n\traw_inode = ext4_raw_inode(iloc);\n\n\theader = IHDR(inode, raw_inode);\n\n\t/* No extended attributes present */\n\tif (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) ||\n\t    header->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC)) {\n\t\tmemset((void *)raw_inode + EXT4_GOOD_OLD_INODE_SIZE +\n\t\t       EXT4_I(inode)->i_extra_isize, 0,\n\t\t       new_extra_isize - EXT4_I(inode)->i_extra_isize);\n\t\tEXT4_I(inode)->i_extra_isize = new_extra_isize;\n\t\treturn 0;\n\t}\n\n\t/* try to expand with EAs present */\n\terror = ext4_expand_extra_isize_ea(inode, new_extra_isize,\n\t\t\t\t\t   raw_inode, handle);\n\tif (error) {\n\t\t/*\n\t\t * Inode size expansion failed; don't try again\n\t\t */\n\t\t*no_expand = 1;\n\t}\n\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,22 @@\n {\n \tstruct ext4_inode *raw_inode;\n \tstruct ext4_xattr_ibody_header *header;\n+\tunsigned int inode_size = EXT4_INODE_SIZE(inode->i_sb);\n+\tstruct ext4_inode_info *ei = EXT4_I(inode);\n \tint error;\n+\n+\t/* this was checked at iget time, but double check for good measure */\n+\tif ((EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize > inode_size) ||\n+\t    (ei->i_extra_isize & 3)) {\n+\t\tEXT4_ERROR_INODE(inode, \"bad extra_isize %u (inode size %u)\",\n+\t\t\t\t ei->i_extra_isize,\n+\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n+\t\treturn -EFSCORRUPTED;\n+\t}\n+\tif ((new_extra_isize < ei->i_extra_isize) ||\n+\t    (new_extra_isize < 4) ||\n+\t    (new_extra_isize > inode_size - EXT4_GOOD_OLD_INODE_SIZE))\n+\t\treturn -EINVAL;\t/* Should never happen */\n \n \traw_inode = ext4_raw_inode(iloc);\n ",
        "function_modified_lines": {
            "added": [
                "\tunsigned int inode_size = EXT4_INODE_SIZE(inode->i_sb);",
                "\tstruct ext4_inode_info *ei = EXT4_I(inode);",
                "",
                "\t/* this was checked at iget time, but double check for good measure */",
                "\tif ((EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize > inode_size) ||",
                "\t    (ei->i_extra_isize & 3)) {",
                "\t\tEXT4_ERROR_INODE(inode, \"bad extra_isize %u (inode size %u)\",",
                "\t\t\t\t ei->i_extra_isize,",
                "\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));",
                "\t\treturn -EFSCORRUPTED;",
                "\t}",
                "\tif ((new_extra_isize < ei->i_extra_isize) ||",
                "\t    (new_extra_isize < 4) ||",
                "\t    (new_extra_isize > inode_size - EXT4_GOOD_OLD_INODE_SIZE))",
                "\t\treturn -EINVAL;\t/* Should never happen */"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The Linux kernel before 5.4.2 mishandles ext4_expand_extra_isize, as demonstrated by use-after-free errors in __ext4_expand_extra_isize and ext4_xattr_set_entry, related to fs/ext4/inode.c and fs/ext4/super.c, aka CID-4ea99936a163.",
        "id": 2223
    },
    {
        "cve_id": "CVE-2020-29660",
        "code_before_change": "static int tiocgsid(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\t/*\n\t * (tty == real_tty) is a cheap way of\n\t * testing if the tty is NOT a master pty.\n\t*/\n\tif (tty == real_tty && current->signal->tty != real_tty)\n\t\treturn -ENOTTY;\n\tif (!real_tty->session)\n\t\treturn -ENOTTY;\n\treturn put_user(pid_vnr(real_tty->session), p);\n}",
        "code_after_change": "static int tiocgsid(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\tunsigned long flags;\n\tpid_t sid;\n\n\t/*\n\t * (tty == real_tty) is a cheap way of\n\t * testing if the tty is NOT a master pty.\n\t*/\n\tif (tty == real_tty && current->signal->tty != real_tty)\n\t\treturn -ENOTTY;\n\n\tspin_lock_irqsave(&real_tty->ctrl_lock, flags);\n\tif (!real_tty->session)\n\t\tgoto err;\n\tsid = pid_vnr(real_tty->session);\n\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);\n\n\treturn put_user(sid, p);\n\nerr:\n\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);\n\treturn -ENOTTY;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,24 @@\n static int tiocgsid(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n {\n+\tunsigned long flags;\n+\tpid_t sid;\n+\n \t/*\n \t * (tty == real_tty) is a cheap way of\n \t * testing if the tty is NOT a master pty.\n \t*/\n \tif (tty == real_tty && current->signal->tty != real_tty)\n \t\treturn -ENOTTY;\n+\n+\tspin_lock_irqsave(&real_tty->ctrl_lock, flags);\n \tif (!real_tty->session)\n-\t\treturn -ENOTTY;\n-\treturn put_user(pid_vnr(real_tty->session), p);\n+\t\tgoto err;\n+\tsid = pid_vnr(real_tty->session);\n+\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);\n+\n+\treturn put_user(sid, p);\n+\n+err:\n+\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);\n+\treturn -ENOTTY;\n }",
        "function_modified_lines": {
            "added": [
                "\tunsigned long flags;",
                "\tpid_t sid;",
                "",
                "",
                "\tspin_lock_irqsave(&real_tty->ctrl_lock, flags);",
                "\t\tgoto err;",
                "\tsid = pid_vnr(real_tty->session);",
                "\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);",
                "",
                "\treturn put_user(sid, p);",
                "",
                "err:",
                "\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);",
                "\treturn -ENOTTY;"
            ],
            "deleted": [
                "\t\treturn -ENOTTY;",
                "\treturn put_user(pid_vnr(real_tty->session), p);"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-667"
        ],
        "cve_description": "A locking inconsistency issue was discovered in the tty subsystem of the Linux kernel through 5.9.13. drivers/tty/tty_io.c and drivers/tty/tty_jobctrl.c may allow a read-after-free attack against TIOCGSID, aka CID-c8bcd9c5be24.",
        "id": 2703
    },
    {
        "cve_id": "CVE-2023-5633",
        "code_before_change": "int vmw_user_bo_synccpu_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *file_priv)\n{\n\tstruct drm_vmw_synccpu_arg *arg =\n\t\t(struct drm_vmw_synccpu_arg *) data;\n\tstruct vmw_bo *vbo;\n\tint ret;\n\n\tif ((arg->flags & (drm_vmw_synccpu_read | drm_vmw_synccpu_write)) == 0\n\t    || (arg->flags & ~(drm_vmw_synccpu_read | drm_vmw_synccpu_write |\n\t\t\t       drm_vmw_synccpu_dontblock |\n\t\t\t       drm_vmw_synccpu_allow_cs)) != 0) {\n\t\tDRM_ERROR(\"Illegal synccpu flags.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (arg->op) {\n\tcase drm_vmw_synccpu_grab:\n\t\tret = vmw_user_bo_lookup(file_priv, arg->handle, &vbo);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\n\t\tret = vmw_user_bo_synccpu_grab(vbo, arg->flags);\n\t\tvmw_user_bo_unref(vbo);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tif (ret == -ERESTARTSYS || ret == -EBUSY)\n\t\t\t\treturn -EBUSY;\n\t\t\tDRM_ERROR(\"Failed synccpu grab on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tcase drm_vmw_synccpu_release:\n\t\tret = vmw_user_bo_synccpu_release(file_priv,\n\t\t\t\t\t\t  arg->handle,\n\t\t\t\t\t\t  arg->flags);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed synccpu release on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Invalid synccpu operation.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int vmw_user_bo_synccpu_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *file_priv)\n{\n\tstruct drm_vmw_synccpu_arg *arg =\n\t\t(struct drm_vmw_synccpu_arg *) data;\n\tstruct vmw_bo *vbo;\n\tint ret;\n\n\tif ((arg->flags & (drm_vmw_synccpu_read | drm_vmw_synccpu_write)) == 0\n\t    || (arg->flags & ~(drm_vmw_synccpu_read | drm_vmw_synccpu_write |\n\t\t\t       drm_vmw_synccpu_dontblock |\n\t\t\t       drm_vmw_synccpu_allow_cs)) != 0) {\n\t\tDRM_ERROR(\"Illegal synccpu flags.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (arg->op) {\n\tcase drm_vmw_synccpu_grab:\n\t\tret = vmw_user_bo_lookup(file_priv, arg->handle, &vbo);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\n\t\tret = vmw_user_bo_synccpu_grab(vbo, arg->flags);\n\t\tvmw_user_bo_unref(&vbo);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tif (ret == -ERESTARTSYS || ret == -EBUSY)\n\t\t\t\treturn -EBUSY;\n\t\t\tDRM_ERROR(\"Failed synccpu grab on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tcase drm_vmw_synccpu_release:\n\t\tret = vmw_user_bo_synccpu_release(file_priv,\n\t\t\t\t\t\t  arg->handle,\n\t\t\t\t\t\t  arg->flags);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed synccpu release on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Invalid synccpu operation.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,7 +21,7 @@\n \t\t\treturn ret;\n \n \t\tret = vmw_user_bo_synccpu_grab(vbo, arg->flags);\n-\t\tvmw_user_bo_unref(vbo);\n+\t\tvmw_user_bo_unref(&vbo);\n \t\tif (unlikely(ret != 0)) {\n \t\t\tif (ret == -ERESTARTSYS || ret == -EBUSY)\n \t\t\t\treturn -EBUSY;",
        "function_modified_lines": {
            "added": [
                "\t\tvmw_user_bo_unref(&vbo);"
            ],
            "deleted": [
                "\t\tvmw_user_bo_unref(vbo);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "The reference count changes made as part of the CVE-2023-33951 and CVE-2023-33952 fixes exposed a use-after-free flaw in the way memory objects were handled when they were being used to store a surface. When running inside a VMware guest with 3D acceleration enabled, a local, unprivileged user could potentially use this flaw to escalate their privileges.",
        "id": 4268
    },
    {
        "cve_id": "CVE-2021-3347",
        "code_before_change": "static int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tint ret = 0;\n\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\tret = fixup_pi_state_owner(uaddr, q, current);\n\t\treturn ret ? ret : locked;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current) {\n\t\tret = fixup_pi_state_owner(uaddr, q, NULL);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex.\n\t */\n\tif (rt_mutex_owner(&q->pi_state->pi_mutex) == current) {\n\t\tprintk(KERN_ERR \"fixup_owner: ret = %d pi-mutex: %p \"\n\t\t\t\t\"pi-state %p\\n\", ret,\n\t\t\t\tq->pi_state->pi_mutex.owner,\n\t\t\t\tq->pi_state->owner);\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "static int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\treturn fixup_pi_state_owner(uaddr, q, current);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current)\n\t\treturn fixup_pi_state_owner(uaddr, q, NULL);\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex.\n\t */\n\tif (rt_mutex_owner(&q->pi_state->pi_mutex) == current) {\n\t\tprintk(KERN_ERR \"fixup_owner: ret = %d pi-mutex: %p \"\n\t\t\t\t\"pi-state %p\\n\", ret,\n\t\t\t\tq->pi_state->pi_mutex.owner,\n\t\t\t\tq->pi_state->owner);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,5 @@\n static int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n {\n-\tint ret = 0;\n-\n \tif (locked) {\n \t\t/*\n \t\t * Got the lock. We might not be the anticipated owner if we\n@@ -12,8 +10,8 @@\n \t\t * stable state, anything else needs more attention.\n \t\t */\n \t\tif (q->pi_state->owner != current)\n-\t\t\tret = fixup_pi_state_owner(uaddr, q, current);\n-\t\treturn ret ? ret : locked;\n+\t\t\treturn fixup_pi_state_owner(uaddr, q, current);\n+\t\treturn 1;\n \t}\n \n \t/*\n@@ -24,10 +22,8 @@\n \t * Another speculative read; pi_state->owner == current is unstable\n \t * but needs our attention.\n \t */\n-\tif (q->pi_state->owner == current) {\n-\t\tret = fixup_pi_state_owner(uaddr, q, NULL);\n-\t\treturn ret;\n-\t}\n+\tif (q->pi_state->owner == current)\n+\t\treturn fixup_pi_state_owner(uaddr, q, NULL);\n \n \t/*\n \t * Paranoia check. If we did not take the lock, then we should not be\n@@ -40,5 +36,5 @@\n \t\t\t\tq->pi_state->owner);\n \t}\n \n-\treturn ret;\n+\treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t\t\treturn fixup_pi_state_owner(uaddr, q, current);",
                "\t\treturn 1;",
                "\tif (q->pi_state->owner == current)",
                "\t\treturn fixup_pi_state_owner(uaddr, q, NULL);",
                "\treturn 0;"
            ],
            "deleted": [
                "\tint ret = 0;",
                "",
                "\t\t\tret = fixup_pi_state_owner(uaddr, q, current);",
                "\t\treturn ret ? ret : locked;",
                "\tif (q->pi_state->owner == current) {",
                "\t\tret = fixup_pi_state_owner(uaddr, q, NULL);",
                "\t\treturn ret;",
                "\t}",
                "\treturn ret;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 5.10.11. PI futexes have a kernel stack use-after-free during fault handling, allowing local users to execute code in the kernel, aka CID-34b1a1ce1458.",
        "id": 2978
    },
    {
        "cve_id": "CVE-2023-1193",
        "code_before_change": "int init_smb2_rsp_hdr(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr = smb2_get_msg(work->response_buf);\n\tstruct smb2_hdr *rcv_hdr = smb2_get_msg(work->request_buf);\n\tstruct ksmbd_conn *conn = work->conn;\n\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\trsp_hdr->ProtocolId = rcv_hdr->ProtocolId;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->Command = rcv_hdr->Command;\n\n\t/*\n\t * Message is response. We don't grant oplock yet.\n\t */\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = rcv_hdr->MessageId;\n\trsp_hdr->Id.SyncId.ProcessId = rcv_hdr->Id.SyncId.ProcessId;\n\trsp_hdr->Id.SyncId.TreeId = rcv_hdr->Id.SyncId.TreeId;\n\trsp_hdr->SessionId = rcv_hdr->SessionId;\n\tmemcpy(rsp_hdr->Signature, rcv_hdr->Signature, 16);\n\n\twork->synchronous = true;\n\tif (work->async_id) {\n\t\tksmbd_release_id(&conn->async_ida, work->async_id);\n\t\twork->async_id = 0;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int init_smb2_rsp_hdr(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr = smb2_get_msg(work->response_buf);\n\tstruct smb2_hdr *rcv_hdr = smb2_get_msg(work->request_buf);\n\tstruct ksmbd_conn *conn = work->conn;\n\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\trsp_hdr->ProtocolId = rcv_hdr->ProtocolId;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->Command = rcv_hdr->Command;\n\n\t/*\n\t * Message is response. We don't grant oplock yet.\n\t */\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = rcv_hdr->MessageId;\n\trsp_hdr->Id.SyncId.ProcessId = rcv_hdr->Id.SyncId.ProcessId;\n\trsp_hdr->Id.SyncId.TreeId = rcv_hdr->Id.SyncId.TreeId;\n\trsp_hdr->SessionId = rcv_hdr->SessionId;\n\tmemcpy(rsp_hdr->Signature, rcv_hdr->Signature, 16);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,11 +22,5 @@\n \trsp_hdr->SessionId = rcv_hdr->SessionId;\n \tmemcpy(rsp_hdr->Signature, rcv_hdr->Signature, 16);\n \n-\twork->synchronous = true;\n-\tif (work->async_id) {\n-\t\tksmbd_release_id(&conn->async_ida, work->async_id);\n-\t\twork->async_id = 0;\n-\t}\n-\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\twork->synchronous = true;",
                "\tif (work->async_id) {",
                "\t\tksmbd_release_id(&conn->async_ida, work->async_id);",
                "\t\twork->async_id = 0;",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in setup_async_work in the KSMBD implementation of the in-kernel samba server and CIFS in the Linux kernel. This issue could allow an attacker to crash the system by accessing freed work.",
        "id": 3854
    },
    {
        "cve_id": "CVE-2020-36694",
        "code_before_change": "static int\ndo_add_counters(struct net *net, sockptr_t arg, unsigned int len)\n{\n\tunsigned int i;\n\tstruct xt_counters_info tmp;\n\tstruct xt_counters *paddc;\n\tstruct xt_table *t;\n\tconst struct xt_table_info *private;\n\tint ret = 0;\n\tstruct ip6t_entry *iter;\n\tunsigned int addend;\n\n\tpaddc = xt_copy_counters(arg, len, &tmp);\n\tif (IS_ERR(paddc))\n\t\treturn PTR_ERR(paddc);\n\tt = xt_find_table_lock(net, AF_INET6, tmp.name);\n\tif (IS_ERR(t)) {\n\t\tret = PTR_ERR(t);\n\t\tgoto free;\n\t}\n\n\tlocal_bh_disable();\n\tprivate = t->private;\n\tif (private->number != tmp.num_counters) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_up_free;\n\t}\n\n\ti = 0;\n\taddend = xt_write_recseq_begin();\n\txt_entry_foreach(iter, private->entries, private->size) {\n\t\tstruct xt_counters *tmp;\n\n\t\ttmp = xt_get_this_cpu_counter(&iter->counters);\n\t\tADD_COUNTER(*tmp, paddc[i].bcnt, paddc[i].pcnt);\n\t\t++i;\n\t}\n\txt_write_recseq_end(addend);\n unlock_up_free:\n\tlocal_bh_enable();\n\txt_table_unlock(t);\n\tmodule_put(t->me);\n free:\n\tvfree(paddc);\n\n\treturn ret;\n}",
        "code_after_change": "static int\ndo_add_counters(struct net *net, sockptr_t arg, unsigned int len)\n{\n\tunsigned int i;\n\tstruct xt_counters_info tmp;\n\tstruct xt_counters *paddc;\n\tstruct xt_table *t;\n\tconst struct xt_table_info *private;\n\tint ret = 0;\n\tstruct ip6t_entry *iter;\n\tunsigned int addend;\n\n\tpaddc = xt_copy_counters(arg, len, &tmp);\n\tif (IS_ERR(paddc))\n\t\treturn PTR_ERR(paddc);\n\tt = xt_find_table_lock(net, AF_INET6, tmp.name);\n\tif (IS_ERR(t)) {\n\t\tret = PTR_ERR(t);\n\t\tgoto free;\n\t}\n\n\tlocal_bh_disable();\n\tprivate = xt_table_get_private_protected(t);\n\tif (private->number != tmp.num_counters) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_up_free;\n\t}\n\n\ti = 0;\n\taddend = xt_write_recseq_begin();\n\txt_entry_foreach(iter, private->entries, private->size) {\n\t\tstruct xt_counters *tmp;\n\n\t\ttmp = xt_get_this_cpu_counter(&iter->counters);\n\t\tADD_COUNTER(*tmp, paddc[i].bcnt, paddc[i].pcnt);\n\t\t++i;\n\t}\n\txt_write_recseq_end(addend);\n unlock_up_free:\n\tlocal_bh_enable();\n\txt_table_unlock(t);\n\tmodule_put(t->me);\n free:\n\tvfree(paddc);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,7 +20,7 @@\n \t}\n \n \tlocal_bh_disable();\n-\tprivate = t->private;\n+\tprivate = xt_table_get_private_protected(t);\n \tif (private->number != tmp.num_counters) {\n \t\tret = -EINVAL;\n \t\tgoto unlock_up_free;",
        "function_modified_lines": {
            "added": [
                "\tprivate = xt_table_get_private_protected(t);"
            ],
            "deleted": [
                "\tprivate = t->private;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in netfilter in the Linux kernel before 5.10. There can be a use-after-free in the packet processing context, because the per-CPU sequence count is mishandled during concurrent iptables rules replacement. This could be exploited with the CAP_NET_ADMIN capability in an unprivileged namespace. NOTE: cc00bca was reverted in 5.12.",
        "id": 2788
    },
    {
        "cve_id": "CVE-2022-1786",
        "code_before_change": "static int io_uring_show_cred(int id, void *p, void *data)\n{\n\tstruct io_identity *iod = p;\n\tconst struct cred *cred = iod->creds;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}",
        "code_after_change": "static int io_uring_show_cred(int id, void *p, void *data)\n{\n\tconst struct cred *cred = p;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,6 @@\n static int io_uring_show_cred(int id, void *p, void *data)\n {\n-\tstruct io_identity *iod = p;\n-\tconst struct cred *cred = iod->creds;\n+\tconst struct cred *cred = p;\n \tstruct seq_file *m = data;\n \tstruct user_namespace *uns = seq_user_ns(m);\n \tstruct group_info *gi;",
        "function_modified_lines": {
            "added": [
                "\tconst struct cred *cred = p;"
            ],
            "deleted": [
                "\tstruct io_identity *iod = p;",
                "\tconst struct cred *cred = iod->creds;"
            ]
        },
        "cwe": [
            "CWE-416",
            "CWE-843"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s io_uring subsystem in the way a user sets up a ring with IORING_SETUP_IOPOLL with more than one task completing submissions on this ring. This flaw allows a local user to crash or escalate their privileges on the system.",
        "id": 3283
    },
    {
        "cve_id": "CVE-2019-11811",
        "code_before_change": "int ipmi_si_mem_setup(struct si_sm_io *io)\n{\n\tunsigned long addr = io->addr_data;\n\tint           mapsize, idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\tio->io_cleanup = mem_cleanup;\n\n\t/*\n\t * Figure out the actual readb/readw/readl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = intf_mem_inb;\n\t\tio->outputb = intf_mem_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = intf_mem_inw;\n\t\tio->outputb = intf_mem_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = intf_mem_inl;\n\t\tio->outputb = intf_mem_outl;\n\t\tbreak;\n#ifdef readq\n\tcase 8:\n\t\tio->inputb = mem_inq;\n\t\tio->outputb = mem_outq;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint memory regions in their ACPI\n\t * tables.  This causes problems when trying to request the\n\t * entire region.  Therefore we must request each register\n\t * separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_mem_region(addr + idx * io->regspacing,\n\t\t\t\t       io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\tmem_region_cleanup(io, idx);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\t/*\n\t * Calculate the total amount of memory to claim.  This is an\n\t * unusual looking calculation, but it avoids claiming any\n\t * more memory than it has to.  It will claim everything\n\t * between the first address to the end of the last full\n\t * register.\n\t */\n\tmapsize = ((io->io_size * io->regspacing)\n\t\t   - (io->regspacing - io->regsize));\n\tio->addr = ioremap(addr, mapsize);\n\tif (io->addr == NULL) {\n\t\tmem_region_cleanup(io, io->io_size);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}",
        "code_after_change": "int ipmi_si_mem_setup(struct si_sm_io *io)\n{\n\tunsigned long addr = io->addr_data;\n\tint           mapsize, idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\t/*\n\t * Figure out the actual readb/readw/readl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = intf_mem_inb;\n\t\tio->outputb = intf_mem_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = intf_mem_inw;\n\t\tio->outputb = intf_mem_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = intf_mem_inl;\n\t\tio->outputb = intf_mem_outl;\n\t\tbreak;\n#ifdef readq\n\tcase 8:\n\t\tio->inputb = mem_inq;\n\t\tio->outputb = mem_outq;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint memory regions in their ACPI\n\t * tables.  This causes problems when trying to request the\n\t * entire region.  Therefore we must request each register\n\t * separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_mem_region(addr + idx * io->regspacing,\n\t\t\t\t       io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\tmem_region_cleanup(io, idx);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\t/*\n\t * Calculate the total amount of memory to claim.  This is an\n\t * unusual looking calculation, but it avoids claiming any\n\t * more memory than it has to.  It will claim everything\n\t * between the first address to the end of the last full\n\t * register.\n\t */\n\tmapsize = ((io->io_size * io->regspacing)\n\t\t   - (io->regspacing - io->regsize));\n\tio->addr = ioremap(addr, mapsize);\n\tif (io->addr == NULL) {\n\t\tmem_region_cleanup(io, io->io_size);\n\t\treturn -EIO;\n\t}\n\n\tio->io_cleanup = mem_cleanup;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,8 +5,6 @@\n \n \tif (!addr)\n \t\treturn -ENODEV;\n-\n-\tio->io_cleanup = mem_cleanup;\n \n \t/*\n \t * Figure out the actual readb/readw/readl/etc routine to use based\n@@ -66,5 +64,8 @@\n \t\tmem_region_cleanup(io, io->io_size);\n \t\treturn -EIO;\n \t}\n+\n+\tio->io_cleanup = mem_cleanup;\n+\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\tio->io_cleanup = mem_cleanup;",
                ""
            ],
            "deleted": [
                "",
                "\tio->io_cleanup = mem_cleanup;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.0.4. There is a use-after-free upon attempted read access to /proc/ioports after the ipmi_si module is removed, related to drivers/char/ipmi/ipmi_si_intf.c, drivers/char/ipmi/ipmi_si_mem_io.c, and drivers/char/ipmi/ipmi_si_port_io.c.",
        "id": 1933
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "static int dmirror_migrate_to_device(struct dmirror *dmirror,\n\t\t\t\tstruct hmm_dmirror_cmd *cmd)\n{\n\tunsigned long start, end, addr;\n\tunsigned long size = cmd->npages << PAGE_SHIFT;\n\tstruct mm_struct *mm = dmirror->notifier.mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long src_pfns[64] = { 0 };\n\tunsigned long dst_pfns[64] = { 0 };\n\tstruct dmirror_bounce bounce;\n\tstruct migrate_vma args;\n\tunsigned long next;\n\tint ret;\n\n\tstart = cmd->addr;\n\tend = start + size;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\t/* Since the mm is for the mirrored process, get a reference first. */\n\tif (!mmget_not_zero(mm))\n\t\treturn -EINVAL;\n\n\tmmap_read_lock(mm);\n\tfor (addr = start; addr < end; addr = next) {\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (!vma || !(vma->vm_flags & VM_READ)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tnext = min(end, addr + (ARRAY_SIZE(src_pfns) << PAGE_SHIFT));\n\t\tif (next > vma->vm_end)\n\t\t\tnext = vma->vm_end;\n\n\t\targs.vma = vma;\n\t\targs.src = src_pfns;\n\t\targs.dst = dst_pfns;\n\t\targs.start = addr;\n\t\targs.end = next;\n\t\targs.pgmap_owner = dmirror->mdevice;\n\t\targs.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\t\tret = migrate_vma_setup(&args);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tpr_debug(\"Migrating from sys mem to device mem\\n\");\n\t\tdmirror_migrate_alloc_and_copy(&args, dmirror);\n\t\tmigrate_vma_pages(&args);\n\t\tdmirror_migrate_finalize_and_map(&args, dmirror);\n\t\tmigrate_vma_finalize(&args);\n\t}\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\t/*\n\t * Return the migrated data for verification.\n\t * Only for pages in device zone\n\t */\n\tret = dmirror_bounce_init(&bounce, start, size);\n\tif (ret)\n\t\treturn ret;\n\tmutex_lock(&dmirror->mutex);\n\tret = dmirror_do_read(dmirror, start, end, &bounce);\n\tmutex_unlock(&dmirror->mutex);\n\tif (ret == 0) {\n\t\tif (copy_to_user(u64_to_user_ptr(cmd->ptr), bounce.ptr,\n\t\t\t\t bounce.size))\n\t\t\tret = -EFAULT;\n\t}\n\tcmd->cpages = bounce.cpages;\n\tdmirror_bounce_fini(&bounce);\n\treturn ret;\n\nout:\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\treturn ret;\n}",
        "code_after_change": "static int dmirror_migrate_to_device(struct dmirror *dmirror,\n\t\t\t\tstruct hmm_dmirror_cmd *cmd)\n{\n\tunsigned long start, end, addr;\n\tunsigned long size = cmd->npages << PAGE_SHIFT;\n\tstruct mm_struct *mm = dmirror->notifier.mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long src_pfns[64] = { 0 };\n\tunsigned long dst_pfns[64] = { 0 };\n\tstruct dmirror_bounce bounce;\n\tstruct migrate_vma args = { 0 };\n\tunsigned long next;\n\tint ret;\n\n\tstart = cmd->addr;\n\tend = start + size;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\t/* Since the mm is for the mirrored process, get a reference first. */\n\tif (!mmget_not_zero(mm))\n\t\treturn -EINVAL;\n\n\tmmap_read_lock(mm);\n\tfor (addr = start; addr < end; addr = next) {\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (!vma || !(vma->vm_flags & VM_READ)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tnext = min(end, addr + (ARRAY_SIZE(src_pfns) << PAGE_SHIFT));\n\t\tif (next > vma->vm_end)\n\t\t\tnext = vma->vm_end;\n\n\t\targs.vma = vma;\n\t\targs.src = src_pfns;\n\t\targs.dst = dst_pfns;\n\t\targs.start = addr;\n\t\targs.end = next;\n\t\targs.pgmap_owner = dmirror->mdevice;\n\t\targs.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\t\tret = migrate_vma_setup(&args);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tpr_debug(\"Migrating from sys mem to device mem\\n\");\n\t\tdmirror_migrate_alloc_and_copy(&args, dmirror);\n\t\tmigrate_vma_pages(&args);\n\t\tdmirror_migrate_finalize_and_map(&args, dmirror);\n\t\tmigrate_vma_finalize(&args);\n\t}\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\t/*\n\t * Return the migrated data for verification.\n\t * Only for pages in device zone\n\t */\n\tret = dmirror_bounce_init(&bounce, start, size);\n\tif (ret)\n\t\treturn ret;\n\tmutex_lock(&dmirror->mutex);\n\tret = dmirror_do_read(dmirror, start, end, &bounce);\n\tmutex_unlock(&dmirror->mutex);\n\tif (ret == 0) {\n\t\tif (copy_to_user(u64_to_user_ptr(cmd->ptr), bounce.ptr,\n\t\t\t\t bounce.size))\n\t\t\tret = -EFAULT;\n\t}\n\tcmd->cpages = bounce.cpages;\n\tdmirror_bounce_fini(&bounce);\n\treturn ret;\n\nout:\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \tunsigned long src_pfns[64] = { 0 };\n \tunsigned long dst_pfns[64] = { 0 };\n \tstruct dmirror_bounce bounce;\n-\tstruct migrate_vma args;\n+\tstruct migrate_vma args = { 0 };\n \tunsigned long next;\n \tint ret;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct migrate_vma args = { 0 };"
            ],
            "deleted": [
                "\tstruct migrate_vma args;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3615
    },
    {
        "cve_id": "CVE-2023-1249",
        "code_before_change": "static bool dump_vma_snapshot(struct coredump_params *cprm)\n{\n\tstruct vm_area_struct *vma, *gate_vma;\n\tstruct mm_struct *mm = current->mm;\n\tint i;\n\n\t/*\n\t * Once the stack expansion code is fixed to not change VMA bounds\n\t * under mmap_lock in read mode, this can be changed to take the\n\t * mmap_lock in read mode.\n\t */\n\tif (mmap_write_lock_killable(mm))\n\t\treturn false;\n\n\tcprm->vma_data_size = 0;\n\tgate_vma = get_gate_vma(mm);\n\tcprm->vma_count = mm->map_count + (gate_vma ? 1 : 0);\n\n\tcprm->vma_meta = kvmalloc_array(cprm->vma_count, sizeof(*cprm->vma_meta), GFP_KERNEL);\n\tif (!cprm->vma_meta) {\n\t\tmmap_write_unlock(mm);\n\t\treturn false;\n\t}\n\n\tfor (i = 0, vma = first_vma(current, gate_vma); vma != NULL;\n\t\t\tvma = next_vma(vma, gate_vma), i++) {\n\t\tstruct core_vma_metadata *m = cprm->vma_meta + i;\n\n\t\tm->start = vma->vm_start;\n\t\tm->end = vma->vm_end;\n\t\tm->flags = vma->vm_flags;\n\t\tm->dump_size = vma_dump_size(vma, cprm->mm_flags);\n\t}\n\n\tmmap_write_unlock(mm);\n\n\tfor (i = 0; i < cprm->vma_count; i++) {\n\t\tstruct core_vma_metadata *m = cprm->vma_meta + i;\n\n\t\tif (m->dump_size == DUMP_SIZE_MAYBE_ELFHDR_PLACEHOLDER) {\n\t\t\tchar elfmag[SELFMAG];\n\n\t\t\tif (copy_from_user(elfmag, (void __user *)m->start, SELFMAG) ||\n\t\t\t\t\tmemcmp(elfmag, ELFMAG, SELFMAG) != 0) {\n\t\t\t\tm->dump_size = 0;\n\t\t\t} else {\n\t\t\t\tm->dump_size = PAGE_SIZE;\n\t\t\t}\n\t\t}\n\n\t\tcprm->vma_data_size += m->dump_size;\n\t}\n\n\treturn true;\n}",
        "code_after_change": "static bool dump_vma_snapshot(struct coredump_params *cprm)\n{\n\tstruct vm_area_struct *vma, *gate_vma;\n\tstruct mm_struct *mm = current->mm;\n\tint i;\n\n\t/*\n\t * Once the stack expansion code is fixed to not change VMA bounds\n\t * under mmap_lock in read mode, this can be changed to take the\n\t * mmap_lock in read mode.\n\t */\n\tif (mmap_write_lock_killable(mm))\n\t\treturn false;\n\n\tcprm->vma_data_size = 0;\n\tgate_vma = get_gate_vma(mm);\n\tcprm->vma_count = mm->map_count + (gate_vma ? 1 : 0);\n\n\tcprm->vma_meta = kvmalloc_array(cprm->vma_count, sizeof(*cprm->vma_meta), GFP_KERNEL);\n\tif (!cprm->vma_meta) {\n\t\tmmap_write_unlock(mm);\n\t\treturn false;\n\t}\n\n\tfor (i = 0, vma = first_vma(current, gate_vma); vma != NULL;\n\t\t\tvma = next_vma(vma, gate_vma), i++) {\n\t\tstruct core_vma_metadata *m = cprm->vma_meta + i;\n\n\t\tm->start = vma->vm_start;\n\t\tm->end = vma->vm_end;\n\t\tm->flags = vma->vm_flags;\n\t\tm->dump_size = vma_dump_size(vma, cprm->mm_flags);\n\t\tm->pgoff = vma->vm_pgoff;\n\n\t\tm->file = vma->vm_file;\n\t\tif (m->file)\n\t\t\tget_file(m->file);\n\t}\n\n\tmmap_write_unlock(mm);\n\n\tfor (i = 0; i < cprm->vma_count; i++) {\n\t\tstruct core_vma_metadata *m = cprm->vma_meta + i;\n\n\t\tif (m->dump_size == DUMP_SIZE_MAYBE_ELFHDR_PLACEHOLDER) {\n\t\t\tchar elfmag[SELFMAG];\n\n\t\t\tif (copy_from_user(elfmag, (void __user *)m->start, SELFMAG) ||\n\t\t\t\t\tmemcmp(elfmag, ELFMAG, SELFMAG) != 0) {\n\t\t\t\tm->dump_size = 0;\n\t\t\t} else {\n\t\t\t\tm->dump_size = PAGE_SIZE;\n\t\t\t}\n\t\t}\n\n\t\tcprm->vma_data_size += m->dump_size;\n\t}\n\n\treturn true;\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,6 +30,11 @@\n \t\tm->end = vma->vm_end;\n \t\tm->flags = vma->vm_flags;\n \t\tm->dump_size = vma_dump_size(vma, cprm->mm_flags);\n+\t\tm->pgoff = vma->vm_pgoff;\n+\n+\t\tm->file = vma->vm_file;\n+\t\tif (m->file)\n+\t\t\tget_file(m->file);\n \t}\n \n \tmmap_write_unlock(mm);",
        "function_modified_lines": {
            "added": [
                "\t\tm->pgoff = vma->vm_pgoff;",
                "",
                "\t\tm->file = vma->vm_file;",
                "\t\tif (m->file)",
                "\t\t\tget_file(m->file);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw was found in the Linux kernel’s core dump subsystem. This flaw allows a local user to crash the system. Only if patch 390031c94211 (\"coredump: Use the vma snapshot in fill_files_note\") not applied yet, then kernel could be affected.",
        "id": 3860
    },
    {
        "cve_id": "CVE-2019-10125",
        "code_before_change": "static ssize_t aio_poll(struct aio_kiocb *aiocb, const struct iocb *iocb)\n{\n\tstruct kioctx *ctx = aiocb->ki_ctx;\n\tstruct poll_iocb *req = &aiocb->poll;\n\tstruct aio_poll_table apt;\n\t__poll_t mask;\n\n\t/* reject any unknown events outside the normal event mask. */\n\tif ((u16)iocb->aio_buf != iocb->aio_buf)\n\t\treturn -EINVAL;\n\t/* reject fields that are not defined for poll */\n\tif (iocb->aio_offset || iocb->aio_nbytes || iocb->aio_rw_flags)\n\t\treturn -EINVAL;\n\n\tINIT_WORK(&req->work, aio_poll_complete_work);\n\treq->events = demangle_poll(iocb->aio_buf) | EPOLLERR | EPOLLHUP;\n\treq->file = fget(iocb->aio_fildes);\n\tif (unlikely(!req->file))\n\t\treturn -EBADF;\n\n\treq->head = NULL;\n\treq->woken = false;\n\treq->cancelled = false;\n\n\tapt.pt._qproc = aio_poll_queue_proc;\n\tapt.pt._key = req->events;\n\tapt.iocb = aiocb;\n\tapt.error = -EINVAL; /* same as no support for IOCB_CMD_POLL */\n\n\t/* initialized the list so that we can do list_empty checks */\n\tINIT_LIST_HEAD(&req->wait.entry);\n\tinit_waitqueue_func_entry(&req->wait, aio_poll_wake);\n\n\t/* one for removal from waitqueue, one for this function */\n\trefcount_set(&aiocb->ki_refcnt, 2);\n\n\tmask = vfs_poll(req->file, &apt.pt) & req->events;\n\tif (unlikely(!req->head)) {\n\t\t/* we did not manage to set up a waitqueue, done */\n\t\tgoto out;\n\t}\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\tspin_lock(&req->head->lock);\n\tif (req->woken) {\n\t\t/* wake_up context handles the rest */\n\t\tmask = 0;\n\t\tapt.error = 0;\n\t} else if (mask || apt.error) {\n\t\t/* if we get an error or a mask we are done */\n\t\tWARN_ON_ONCE(list_empty(&req->wait.entry));\n\t\tlist_del_init(&req->wait.entry);\n\t} else {\n\t\t/* actually waiting for an event */\n\t\tlist_add_tail(&aiocb->ki_list, &ctx->active_reqs);\n\t\taiocb->ki_cancel = aio_poll_cancel;\n\t}\n\tspin_unlock(&req->head->lock);\n\tspin_unlock_irq(&ctx->ctx_lock);\n\nout:\n\tif (unlikely(apt.error)) {\n\t\tfput(req->file);\n\t\treturn apt.error;\n\t}\n\n\tif (mask)\n\t\taio_poll_complete(aiocb, mask);\n\tiocb_put(aiocb);\n\treturn 0;\n}",
        "code_after_change": "static ssize_t aio_poll(struct aio_kiocb *aiocb, const struct iocb *iocb)\n{\n\tstruct kioctx *ctx = aiocb->ki_ctx;\n\tstruct poll_iocb *req = &aiocb->poll;\n\tstruct aio_poll_table apt;\n\t__poll_t mask;\n\n\t/* reject any unknown events outside the normal event mask. */\n\tif ((u16)iocb->aio_buf != iocb->aio_buf)\n\t\treturn -EINVAL;\n\t/* reject fields that are not defined for poll */\n\tif (iocb->aio_offset || iocb->aio_nbytes || iocb->aio_rw_flags)\n\t\treturn -EINVAL;\n\n\tINIT_WORK(&req->work, aio_poll_complete_work);\n\treq->events = demangle_poll(iocb->aio_buf) | EPOLLERR | EPOLLHUP;\n\n\treq->head = NULL;\n\treq->woken = false;\n\treq->cancelled = false;\n\n\tapt.pt._qproc = aio_poll_queue_proc;\n\tapt.pt._key = req->events;\n\tapt.iocb = aiocb;\n\tapt.error = -EINVAL; /* same as no support for IOCB_CMD_POLL */\n\n\t/* initialized the list so that we can do list_empty checks */\n\tINIT_LIST_HEAD(&req->wait.entry);\n\tinit_waitqueue_func_entry(&req->wait, aio_poll_wake);\n\n\t/* one for removal from waitqueue, one for this function */\n\trefcount_set(&aiocb->ki_refcnt, 2);\n\n\tmask = vfs_poll(req->file, &apt.pt) & req->events;\n\tif (unlikely(!req->head)) {\n\t\t/* we did not manage to set up a waitqueue, done */\n\t\tgoto out;\n\t}\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\tspin_lock(&req->head->lock);\n\tif (req->woken) {\n\t\t/* wake_up context handles the rest */\n\t\tmask = 0;\n\t\tapt.error = 0;\n\t} else if (mask || apt.error) {\n\t\t/* if we get an error or a mask we are done */\n\t\tWARN_ON_ONCE(list_empty(&req->wait.entry));\n\t\tlist_del_init(&req->wait.entry);\n\t} else {\n\t\t/* actually waiting for an event */\n\t\tlist_add_tail(&aiocb->ki_list, &ctx->active_reqs);\n\t\taiocb->ki_cancel = aio_poll_cancel;\n\t}\n\tspin_unlock(&req->head->lock);\n\tspin_unlock_irq(&ctx->ctx_lock);\n\nout:\n\tif (unlikely(apt.error))\n\t\treturn apt.error;\n\n\tif (mask)\n\t\taio_poll_complete(aiocb, mask);\n\tiocb_put(aiocb);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,9 +14,6 @@\n \n \tINIT_WORK(&req->work, aio_poll_complete_work);\n \treq->events = demangle_poll(iocb->aio_buf) | EPOLLERR | EPOLLHUP;\n-\treq->file = fget(iocb->aio_fildes);\n-\tif (unlikely(!req->file))\n-\t\treturn -EBADF;\n \n \treq->head = NULL;\n \treq->woken = false;\n@@ -59,10 +56,8 @@\n \tspin_unlock_irq(&ctx->ctx_lock);\n \n out:\n-\tif (unlikely(apt.error)) {\n-\t\tfput(req->file);\n+\tif (unlikely(apt.error))\n \t\treturn apt.error;\n-\t}\n \n \tif (mask)\n \t\taio_poll_complete(aiocb, mask);",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(apt.error))"
            ],
            "deleted": [
                "\treq->file = fget(iocb->aio_fildes);",
                "\tif (unlikely(!req->file))",
                "\t\treturn -EBADF;",
                "\tif (unlikely(apt.error)) {",
                "\t\tfput(req->file);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in aio_poll() in fs/aio.c in the Linux kernel through 5.0.4. A file may be released by aio_poll_wake() if an expected event is triggered immediately (e.g., by the close of a pair of pipes) after the return of vfs_poll(), and this will cause a use-after-free.",
        "id": 1887
    },
    {
        "cve_id": "CVE-2022-3523",
        "code_before_change": "int migrate_vma_setup(struct migrate_vma *args)\n{\n\tlong nr_pages = (args->end - args->start) >> PAGE_SHIFT;\n\n\targs->start &= PAGE_MASK;\n\targs->end &= PAGE_MASK;\n\tif (!args->vma || is_vm_hugetlb_page(args->vma) ||\n\t    (args->vma->vm_flags & VM_SPECIAL) || vma_is_dax(args->vma))\n\t\treturn -EINVAL;\n\tif (nr_pages <= 0)\n\t\treturn -EINVAL;\n\tif (args->start < args->vma->vm_start ||\n\t    args->start >= args->vma->vm_end)\n\t\treturn -EINVAL;\n\tif (args->end <= args->vma->vm_start || args->end > args->vma->vm_end)\n\t\treturn -EINVAL;\n\tif (!args->src || !args->dst)\n\t\treturn -EINVAL;\n\n\tmemset(args->src, 0, sizeof(*args->src) * nr_pages);\n\targs->cpages = 0;\n\targs->npages = 0;\n\n\tmigrate_vma_collect(args);\n\n\tif (args->cpages)\n\t\tmigrate_vma_unmap(args);\n\n\t/*\n\t * At this point pages are locked and unmapped, and thus they have\n\t * stable content and can safely be copied to destination memory that\n\t * is allocated by the drivers.\n\t */\n\treturn 0;\n\n}",
        "code_after_change": "int migrate_vma_setup(struct migrate_vma *args)\n{\n\tlong nr_pages = (args->end - args->start) >> PAGE_SHIFT;\n\n\targs->start &= PAGE_MASK;\n\targs->end &= PAGE_MASK;\n\tif (!args->vma || is_vm_hugetlb_page(args->vma) ||\n\t    (args->vma->vm_flags & VM_SPECIAL) || vma_is_dax(args->vma))\n\t\treturn -EINVAL;\n\tif (nr_pages <= 0)\n\t\treturn -EINVAL;\n\tif (args->start < args->vma->vm_start ||\n\t    args->start >= args->vma->vm_end)\n\t\treturn -EINVAL;\n\tif (args->end <= args->vma->vm_start || args->end > args->vma->vm_end)\n\t\treturn -EINVAL;\n\tif (!args->src || !args->dst)\n\t\treturn -EINVAL;\n\tif (args->fault_page && !is_device_private_page(args->fault_page))\n\t\treturn -EINVAL;\n\n\tmemset(args->src, 0, sizeof(*args->src) * nr_pages);\n\targs->cpages = 0;\n\targs->npages = 0;\n\n\tmigrate_vma_collect(args);\n\n\tif (args->cpages)\n\t\tmigrate_vma_unmap(args);\n\n\t/*\n\t * At this point pages are locked and unmapped, and thus they have\n\t * stable content and can safely be copied to destination memory that\n\t * is allocated by the drivers.\n\t */\n\treturn 0;\n\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,6 +15,8 @@\n \tif (args->end <= args->vma->vm_start || args->end > args->vma->vm_end)\n \t\treturn -EINVAL;\n \tif (!args->src || !args->dst)\n+\t\treturn -EINVAL;\n+\tif (args->fault_page && !is_device_private_page(args->fault_page))\n \t\treturn -EINVAL;\n \n \tmemset(args->src, 0, sizeof(*args->src) * nr_pages);",
        "function_modified_lines": {
            "added": [
                "\t\treturn -EINVAL;",
                "\tif (args->fault_page && !is_device_private_page(args->fault_page))"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A vulnerability was found in Linux Kernel. It has been classified as problematic. Affected is an unknown function of the file mm/memory.c of the component Driver Handler. The manipulation leads to use after free. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue. The identifier of this vulnerability is VDB-211020.",
        "id": 3621
    },
    {
        "cve_id": "CVE-2023-40283",
        "code_before_change": "static int l2cap_sock_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\tstruct l2cap_chan *chan;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\tif (!sk)\n\t\treturn 0;\n\n\tbt_sock_unlink(&l2cap_sk_list, sk);\n\n\terr = l2cap_sock_shutdown(sock, SHUT_RDWR);\n\tchan = l2cap_pi(sk)->chan;\n\n\tl2cap_chan_hold(chan);\n\tl2cap_chan_lock(chan);\n\n\tsock_orphan(sk);\n\tl2cap_sock_kill(sk);\n\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n\n\treturn err;\n}",
        "code_after_change": "static int l2cap_sock_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\tstruct l2cap_chan *chan;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\tif (!sk)\n\t\treturn 0;\n\n\tl2cap_sock_cleanup_listen(sk);\n\tbt_sock_unlink(&l2cap_sk_list, sk);\n\n\terr = l2cap_sock_shutdown(sock, SHUT_RDWR);\n\tchan = l2cap_pi(sk)->chan;\n\n\tl2cap_chan_hold(chan);\n\tl2cap_chan_lock(chan);\n\n\tsock_orphan(sk);\n\tl2cap_sock_kill(sk);\n\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,7 @@\n \tif (!sk)\n \t\treturn 0;\n \n+\tl2cap_sock_cleanup_listen(sk);\n \tbt_sock_unlink(&l2cap_sk_list, sk);\n \n \terr = l2cap_sock_shutdown(sock, SHUT_RDWR);",
        "function_modified_lines": {
            "added": [
                "\tl2cap_sock_cleanup_listen(sk);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in l2cap_sock_release in net/bluetooth/l2cap_sock.c in the Linux kernel before 6.4.10. There is a use-after-free because the children of an sk are mishandled.",
        "id": 4189
    },
    {
        "cve_id": "CVE-2023-4921",
        "code_before_change": "static void agg_dequeue(struct qfq_aggregate *agg,\n\t\t\tstruct qfq_class *cl, unsigned int len)\n{\n\tqdisc_dequeue_peeked(cl->qdisc);\n\n\tcl->deficit -= (int) len;\n\n\tif (cl->qdisc->q.qlen == 0) /* no more packets, remove from list */\n\t\tlist_del(&cl->alist);\n\telse if (cl->deficit < qdisc_pkt_len(cl->qdisc->ops->peek(cl->qdisc))) {\n\t\tcl->deficit += agg->lmax;\n\t\tlist_move_tail(&cl->alist, &agg->active);\n\t}\n}",
        "code_after_change": "static struct sk_buff *agg_dequeue(struct qfq_aggregate *agg,\n\t\t\t\t   struct qfq_class *cl, unsigned int len)\n{\n\tstruct sk_buff *skb = qdisc_dequeue_peeked(cl->qdisc);\n\n\tif (!skb)\n\t\treturn NULL;\n\n\tcl->deficit -= (int) len;\n\n\tif (cl->qdisc->q.qlen == 0) /* no more packets, remove from list */\n\t\tlist_del(&cl->alist);\n\telse if (cl->deficit < qdisc_pkt_len(cl->qdisc->ops->peek(cl->qdisc))) {\n\t\tcl->deficit += agg->lmax;\n\t\tlist_move_tail(&cl->alist, &agg->active);\n\t}\n\n\treturn skb;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,10 @@\n-static void agg_dequeue(struct qfq_aggregate *agg,\n-\t\t\tstruct qfq_class *cl, unsigned int len)\n+static struct sk_buff *agg_dequeue(struct qfq_aggregate *agg,\n+\t\t\t\t   struct qfq_class *cl, unsigned int len)\n {\n-\tqdisc_dequeue_peeked(cl->qdisc);\n+\tstruct sk_buff *skb = qdisc_dequeue_peeked(cl->qdisc);\n+\n+\tif (!skb)\n+\t\treturn NULL;\n \n \tcl->deficit -= (int) len;\n \n@@ -11,4 +14,6 @@\n \t\tcl->deficit += agg->lmax;\n \t\tlist_move_tail(&cl->alist, &agg->active);\n \t}\n+\n+\treturn skb;\n }",
        "function_modified_lines": {
            "added": [
                "static struct sk_buff *agg_dequeue(struct qfq_aggregate *agg,",
                "\t\t\t\t   struct qfq_class *cl, unsigned int len)",
                "\tstruct sk_buff *skb = qdisc_dequeue_peeked(cl->qdisc);",
                "",
                "\tif (!skb)",
                "\t\treturn NULL;",
                "",
                "\treturn skb;"
            ],
            "deleted": [
                "static void agg_dequeue(struct qfq_aggregate *agg,",
                "\t\t\tstruct qfq_class *cl, unsigned int len)",
                "\tqdisc_dequeue_peeked(cl->qdisc);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free vulnerability in the Linux kernel's net/sched: sch_qfq component can be exploited to achieve local privilege escalation.\n\nWhen the plug qdisc is used as a class of the qfq qdisc, sending network packets triggers use-after-free in qfq_dequeue() due to the incorrect .peek handler of sch_plug and lack of error checking in agg_dequeue().\n\nWe recommend upgrading past commit 8fc134fee27f2263988ae38920bc03da416b03d8.\n\n",
        "id": 4253
    },
    {
        "cve_id": "CVE-2020-0466",
        "code_before_change": "int do_epoll_ctl(int epfd, int op, int fd, struct epoll_event *epds,\n\t\t bool nonblock)\n{\n\tint error;\n\tint full_check = 0;\n\tstruct fd f, tf;\n\tstruct eventpoll *ep;\n\tstruct epitem *epi;\n\tstruct eventpoll *tep = NULL;\n\n\terror = -EBADF;\n\tf = fdget(epfd);\n\tif (!f.file)\n\t\tgoto error_return;\n\n\t/* Get the \"struct file *\" for the target file */\n\ttf = fdget(fd);\n\tif (!tf.file)\n\t\tgoto error_fput;\n\n\t/* The target file descriptor must support poll */\n\terror = -EPERM;\n\tif (!file_can_poll(tf.file))\n\t\tgoto error_tgt_fput;\n\n\t/* Check if EPOLLWAKEUP is allowed */\n\tif (ep_op_has_event(op))\n\t\tep_take_care_of_epollwakeup(epds);\n\n\t/*\n\t * We have to check that the file structure underneath the file descriptor\n\t * the user passed to us _is_ an eventpoll file. And also we do not permit\n\t * adding an epoll file descriptor inside itself.\n\t */\n\terror = -EINVAL;\n\tif (f.file == tf.file || !is_file_epoll(f.file))\n\t\tgoto error_tgt_fput;\n\n\t/*\n\t * epoll adds to the wakeup queue at EPOLL_CTL_ADD time only,\n\t * so EPOLLEXCLUSIVE is not allowed for a EPOLL_CTL_MOD operation.\n\t * Also, we do not currently supported nested exclusive wakeups.\n\t */\n\tif (ep_op_has_event(op) && (epds->events & EPOLLEXCLUSIVE)) {\n\t\tif (op == EPOLL_CTL_MOD)\n\t\t\tgoto error_tgt_fput;\n\t\tif (op == EPOLL_CTL_ADD && (is_file_epoll(tf.file) ||\n\t\t\t\t(epds->events & ~EPOLLEXCLUSIVE_OK_BITS)))\n\t\t\tgoto error_tgt_fput;\n\t}\n\n\t/*\n\t * At this point it is safe to assume that the \"private_data\" contains\n\t * our own data structure.\n\t */\n\tep = f.file->private_data;\n\n\t/*\n\t * When we insert an epoll file descriptor, inside another epoll file\n\t * descriptor, there is the change of creating closed loops, which are\n\t * better be handled here, than in more critical paths. While we are\n\t * checking for loops we also determine the list of files reachable\n\t * and hang them on the tfile_check_list, so we can check that we\n\t * haven't created too many possible wakeup paths.\n\t *\n\t * We do not need to take the global 'epumutex' on EPOLL_CTL_ADD when\n\t * the epoll file descriptor is attaching directly to a wakeup source,\n\t * unless the epoll file descriptor is nested. The purpose of taking the\n\t * 'epmutex' on add is to prevent complex toplogies such as loops and\n\t * deep wakeup paths from forming in parallel through multiple\n\t * EPOLL_CTL_ADD operations.\n\t */\n\terror = epoll_mutex_lock(&ep->mtx, 0, nonblock);\n\tif (error)\n\t\tgoto error_tgt_fput;\n\tif (op == EPOLL_CTL_ADD) {\n\t\tif (!list_empty(&f.file->f_ep_links) ||\n\t\t\t\t\t\tis_file_epoll(tf.file)) {\n\t\t\tmutex_unlock(&ep->mtx);\n\t\t\terror = epoll_mutex_lock(&epmutex, 0, nonblock);\n\t\t\tif (error)\n\t\t\t\tgoto error_tgt_fput;\n\t\t\tfull_check = 1;\n\t\t\tif (is_file_epoll(tf.file)) {\n\t\t\t\terror = -ELOOP;\n\t\t\t\tif (ep_loop_check(ep, tf.file) != 0) {\n\t\t\t\t\tclear_tfile_check_list();\n\t\t\t\t\tgoto error_tgt_fput;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tget_file(tf.file);\n\t\t\t\tlist_add(&tf.file->f_tfile_llink,\n\t\t\t\t\t\t\t&tfile_check_list);\n\t\t\t}\n\t\t\terror = epoll_mutex_lock(&ep->mtx, 0, nonblock);\n\t\t\tif (error) {\nout_del:\n\t\t\t\tlist_del(&tf.file->f_tfile_llink);\n\t\t\t\tif (!is_file_epoll(tf.file))\n\t\t\t\t\tfput(tf.file);\n\t\t\t\tgoto error_tgt_fput;\n\t\t\t}\n\t\t\tif (is_file_epoll(tf.file)) {\n\t\t\t\ttep = tf.file->private_data;\n\t\t\t\terror = epoll_mutex_lock(&tep->mtx, 1, nonblock);\n\t\t\t\tif (error) {\n\t\t\t\t\tmutex_unlock(&ep->mtx);\n\t\t\t\t\tgoto out_del;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Try to lookup the file inside our RB tree, Since we grabbed \"mtx\"\n\t * above, we can be sure to be able to use the item looked up by\n\t * ep_find() till we release the mutex.\n\t */\n\tepi = ep_find(ep, tf.file, fd);\n\n\terror = -EINVAL;\n\tswitch (op) {\n\tcase EPOLL_CTL_ADD:\n\t\tif (!epi) {\n\t\t\tepds->events |= EPOLLERR | EPOLLHUP;\n\t\t\terror = ep_insert(ep, epds, tf.file, fd, full_check);\n\t\t} else\n\t\t\terror = -EEXIST;\n\t\tif (full_check)\n\t\t\tclear_tfile_check_list();\n\t\tbreak;\n\tcase EPOLL_CTL_DEL:\n\t\tif (epi)\n\t\t\terror = ep_remove(ep, epi);\n\t\telse\n\t\t\terror = -ENOENT;\n\t\tbreak;\n\tcase EPOLL_CTL_MOD:\n\t\tif (epi) {\n\t\t\tif (!(epi->event.events & EPOLLEXCLUSIVE)) {\n\t\t\t\tepds->events |= EPOLLERR | EPOLLHUP;\n\t\t\t\terror = ep_modify(ep, epi, epds);\n\t\t\t}\n\t\t} else\n\t\t\terror = -ENOENT;\n\t\tbreak;\n\t}\n\tif (tep != NULL)\n\t\tmutex_unlock(&tep->mtx);\n\tmutex_unlock(&ep->mtx);\n\nerror_tgt_fput:\n\tif (full_check)\n\t\tmutex_unlock(&epmutex);\n\n\tfdput(tf);\nerror_fput:\n\tfdput(f);\nerror_return:\n\n\treturn error;\n}",
        "code_after_change": "int do_epoll_ctl(int epfd, int op, int fd, struct epoll_event *epds,\n\t\t bool nonblock)\n{\n\tint error;\n\tint full_check = 0;\n\tstruct fd f, tf;\n\tstruct eventpoll *ep;\n\tstruct epitem *epi;\n\tstruct eventpoll *tep = NULL;\n\n\terror = -EBADF;\n\tf = fdget(epfd);\n\tif (!f.file)\n\t\tgoto error_return;\n\n\t/* Get the \"struct file *\" for the target file */\n\ttf = fdget(fd);\n\tif (!tf.file)\n\t\tgoto error_fput;\n\n\t/* The target file descriptor must support poll */\n\terror = -EPERM;\n\tif (!file_can_poll(tf.file))\n\t\tgoto error_tgt_fput;\n\n\t/* Check if EPOLLWAKEUP is allowed */\n\tif (ep_op_has_event(op))\n\t\tep_take_care_of_epollwakeup(epds);\n\n\t/*\n\t * We have to check that the file structure underneath the file descriptor\n\t * the user passed to us _is_ an eventpoll file. And also we do not permit\n\t * adding an epoll file descriptor inside itself.\n\t */\n\terror = -EINVAL;\n\tif (f.file == tf.file || !is_file_epoll(f.file))\n\t\tgoto error_tgt_fput;\n\n\t/*\n\t * epoll adds to the wakeup queue at EPOLL_CTL_ADD time only,\n\t * so EPOLLEXCLUSIVE is not allowed for a EPOLL_CTL_MOD operation.\n\t * Also, we do not currently supported nested exclusive wakeups.\n\t */\n\tif (ep_op_has_event(op) && (epds->events & EPOLLEXCLUSIVE)) {\n\t\tif (op == EPOLL_CTL_MOD)\n\t\t\tgoto error_tgt_fput;\n\t\tif (op == EPOLL_CTL_ADD && (is_file_epoll(tf.file) ||\n\t\t\t\t(epds->events & ~EPOLLEXCLUSIVE_OK_BITS)))\n\t\t\tgoto error_tgt_fput;\n\t}\n\n\t/*\n\t * At this point it is safe to assume that the \"private_data\" contains\n\t * our own data structure.\n\t */\n\tep = f.file->private_data;\n\n\t/*\n\t * When we insert an epoll file descriptor, inside another epoll file\n\t * descriptor, there is the change of creating closed loops, which are\n\t * better be handled here, than in more critical paths. While we are\n\t * checking for loops we also determine the list of files reachable\n\t * and hang them on the tfile_check_list, so we can check that we\n\t * haven't created too many possible wakeup paths.\n\t *\n\t * We do not need to take the global 'epumutex' on EPOLL_CTL_ADD when\n\t * the epoll file descriptor is attaching directly to a wakeup source,\n\t * unless the epoll file descriptor is nested. The purpose of taking the\n\t * 'epmutex' on add is to prevent complex toplogies such as loops and\n\t * deep wakeup paths from forming in parallel through multiple\n\t * EPOLL_CTL_ADD operations.\n\t */\n\terror = epoll_mutex_lock(&ep->mtx, 0, nonblock);\n\tif (error)\n\t\tgoto error_tgt_fput;\n\tif (op == EPOLL_CTL_ADD) {\n\t\tif (!list_empty(&f.file->f_ep_links) ||\n\t\t\t\t\t\tis_file_epoll(tf.file)) {\n\t\t\tmutex_unlock(&ep->mtx);\n\t\t\terror = epoll_mutex_lock(&epmutex, 0, nonblock);\n\t\t\tif (error)\n\t\t\t\tgoto error_tgt_fput;\n\t\t\tfull_check = 1;\n\t\t\tif (is_file_epoll(tf.file)) {\n\t\t\t\terror = -ELOOP;\n\t\t\t\tif (ep_loop_check(ep, tf.file) != 0)\n\t\t\t\t\tgoto error_tgt_fput;\n\t\t\t} else {\n\t\t\t\tget_file(tf.file);\n\t\t\t\tlist_add(&tf.file->f_tfile_llink,\n\t\t\t\t\t\t\t&tfile_check_list);\n\t\t\t}\n\t\t\terror = epoll_mutex_lock(&ep->mtx, 0, nonblock);\n\t\t\tif (error)\n\t\t\t\tgoto error_tgt_fput;\n\t\t\tif (is_file_epoll(tf.file)) {\n\t\t\t\ttep = tf.file->private_data;\n\t\t\t\terror = epoll_mutex_lock(&tep->mtx, 1, nonblock);\n\t\t\t\tif (error) {\n\t\t\t\t\tmutex_unlock(&ep->mtx);\n\t\t\t\t\tgoto error_tgt_fput;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Try to lookup the file inside our RB tree, Since we grabbed \"mtx\"\n\t * above, we can be sure to be able to use the item looked up by\n\t * ep_find() till we release the mutex.\n\t */\n\tepi = ep_find(ep, tf.file, fd);\n\n\terror = -EINVAL;\n\tswitch (op) {\n\tcase EPOLL_CTL_ADD:\n\t\tif (!epi) {\n\t\t\tepds->events |= EPOLLERR | EPOLLHUP;\n\t\t\terror = ep_insert(ep, epds, tf.file, fd, full_check);\n\t\t} else\n\t\t\terror = -EEXIST;\n\t\tbreak;\n\tcase EPOLL_CTL_DEL:\n\t\tif (epi)\n\t\t\terror = ep_remove(ep, epi);\n\t\telse\n\t\t\terror = -ENOENT;\n\t\tbreak;\n\tcase EPOLL_CTL_MOD:\n\t\tif (epi) {\n\t\t\tif (!(epi->event.events & EPOLLEXCLUSIVE)) {\n\t\t\t\tepds->events |= EPOLLERR | EPOLLHUP;\n\t\t\t\terror = ep_modify(ep, epi, epds);\n\t\t\t}\n\t\t} else\n\t\t\terror = -ENOENT;\n\t\tbreak;\n\t}\n\tif (tep != NULL)\n\t\tmutex_unlock(&tep->mtx);\n\tmutex_unlock(&ep->mtx);\n\nerror_tgt_fput:\n\tif (full_check) {\n\t\tclear_tfile_check_list();\n\t\tmutex_unlock(&epmutex);\n\t}\n\n\tfdput(tf);\nerror_fput:\n\tfdput(f);\nerror_return:\n\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -83,29 +83,22 @@\n \t\t\tfull_check = 1;\n \t\t\tif (is_file_epoll(tf.file)) {\n \t\t\t\terror = -ELOOP;\n-\t\t\t\tif (ep_loop_check(ep, tf.file) != 0) {\n-\t\t\t\t\tclear_tfile_check_list();\n+\t\t\t\tif (ep_loop_check(ep, tf.file) != 0)\n \t\t\t\t\tgoto error_tgt_fput;\n-\t\t\t\t}\n \t\t\t} else {\n \t\t\t\tget_file(tf.file);\n \t\t\t\tlist_add(&tf.file->f_tfile_llink,\n \t\t\t\t\t\t\t&tfile_check_list);\n \t\t\t}\n \t\t\terror = epoll_mutex_lock(&ep->mtx, 0, nonblock);\n-\t\t\tif (error) {\n-out_del:\n-\t\t\t\tlist_del(&tf.file->f_tfile_llink);\n-\t\t\t\tif (!is_file_epoll(tf.file))\n-\t\t\t\t\tfput(tf.file);\n+\t\t\tif (error)\n \t\t\t\tgoto error_tgt_fput;\n-\t\t\t}\n \t\t\tif (is_file_epoll(tf.file)) {\n \t\t\t\ttep = tf.file->private_data;\n \t\t\t\terror = epoll_mutex_lock(&tep->mtx, 1, nonblock);\n \t\t\t\tif (error) {\n \t\t\t\t\tmutex_unlock(&ep->mtx);\n-\t\t\t\t\tgoto out_del;\n+\t\t\t\t\tgoto error_tgt_fput;\n \t\t\t\t}\n \t\t\t}\n \t\t}\n@@ -126,8 +119,6 @@\n \t\t\terror = ep_insert(ep, epds, tf.file, fd, full_check);\n \t\t} else\n \t\t\terror = -EEXIST;\n-\t\tif (full_check)\n-\t\t\tclear_tfile_check_list();\n \t\tbreak;\n \tcase EPOLL_CTL_DEL:\n \t\tif (epi)\n@@ -150,8 +141,10 @@\n \tmutex_unlock(&ep->mtx);\n \n error_tgt_fput:\n-\tif (full_check)\n+\tif (full_check) {\n+\t\tclear_tfile_check_list();\n \t\tmutex_unlock(&epmutex);\n+\t}\n \n \tfdput(tf);\n error_fput:",
        "function_modified_lines": {
            "added": [
                "\t\t\t\tif (ep_loop_check(ep, tf.file) != 0)",
                "\t\t\tif (error)",
                "\t\t\t\t\tgoto error_tgt_fput;",
                "\tif (full_check) {",
                "\t\tclear_tfile_check_list();",
                "\t}"
            ],
            "deleted": [
                "\t\t\t\tif (ep_loop_check(ep, tf.file) != 0) {",
                "\t\t\t\t\tclear_tfile_check_list();",
                "\t\t\t\t}",
                "\t\t\tif (error) {",
                "out_del:",
                "\t\t\t\tlist_del(&tf.file->f_tfile_llink);",
                "\t\t\t\tif (!is_file_epoll(tf.file))",
                "\t\t\t\t\tfput(tf.file);",
                "\t\t\t}",
                "\t\t\t\t\tgoto out_del;",
                "\t\tif (full_check)",
                "\t\t\tclear_tfile_check_list();",
                "\tif (full_check)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In do_epoll_ctl and ep_loop_check_proc of eventpoll.c, there is a possible use after free due to a logic error. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.Product: AndroidVersions: Android kernelAndroid ID: A-147802478References: Upstream kernel",
        "id": 2394
    },
    {
        "cve_id": "CVE-2014-4653",
        "code_before_change": "int snd_ctl_add(struct snd_card *card, struct snd_kcontrol *kcontrol)\n{\n\tstruct snd_ctl_elem_id id;\n\tunsigned int idx;\n\tint err = -EINVAL;\n\n\tif (! kcontrol)\n\t\treturn err;\n\tif (snd_BUG_ON(!card || !kcontrol->info))\n\t\tgoto error;\n\tid = kcontrol->id;\n\tdown_write(&card->controls_rwsem);\n\tif (snd_ctl_find_id(card, &id)) {\n\t\tup_write(&card->controls_rwsem);\n\t\tdev_err(card->dev, \"control %i:%i:%i:%s:%i is already present\\n\",\n\t\t\t\t\tid.iface,\n\t\t\t\t\tid.device,\n\t\t\t\t\tid.subdevice,\n\t\t\t\t\tid.name,\n\t\t\t\t\tid.index);\n\t\terr = -EBUSY;\n\t\tgoto error;\n\t}\n\tif (snd_ctl_find_hole(card, kcontrol->count) < 0) {\n\t\tup_write(&card->controls_rwsem);\n\t\terr = -ENOMEM;\n\t\tgoto error;\n\t}\n\tlist_add_tail(&kcontrol->list, &card->controls);\n\tcard->controls_count += kcontrol->count;\n\tkcontrol->id.numid = card->last_numid + 1;\n\tcard->last_numid += kcontrol->count;\n\tup_write(&card->controls_rwsem);\n\tfor (idx = 0; idx < kcontrol->count; idx++, id.index++, id.numid++)\n\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);\n\treturn 0;\n\n error:\n\tsnd_ctl_free_one(kcontrol);\n\treturn err;\n}",
        "code_after_change": "int snd_ctl_add(struct snd_card *card, struct snd_kcontrol *kcontrol)\n{\n\tstruct snd_ctl_elem_id id;\n\tunsigned int idx;\n\tunsigned int count;\n\tint err = -EINVAL;\n\n\tif (! kcontrol)\n\t\treturn err;\n\tif (snd_BUG_ON(!card || !kcontrol->info))\n\t\tgoto error;\n\tid = kcontrol->id;\n\tdown_write(&card->controls_rwsem);\n\tif (snd_ctl_find_id(card, &id)) {\n\t\tup_write(&card->controls_rwsem);\n\t\tdev_err(card->dev, \"control %i:%i:%i:%s:%i is already present\\n\",\n\t\t\t\t\tid.iface,\n\t\t\t\t\tid.device,\n\t\t\t\t\tid.subdevice,\n\t\t\t\t\tid.name,\n\t\t\t\t\tid.index);\n\t\terr = -EBUSY;\n\t\tgoto error;\n\t}\n\tif (snd_ctl_find_hole(card, kcontrol->count) < 0) {\n\t\tup_write(&card->controls_rwsem);\n\t\terr = -ENOMEM;\n\t\tgoto error;\n\t}\n\tlist_add_tail(&kcontrol->list, &card->controls);\n\tcard->controls_count += kcontrol->count;\n\tkcontrol->id.numid = card->last_numid + 1;\n\tcard->last_numid += kcontrol->count;\n\tcount = kcontrol->count;\n\tup_write(&card->controls_rwsem);\n\tfor (idx = 0; idx < count; idx++, id.index++, id.numid++)\n\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);\n\treturn 0;\n\n error:\n\tsnd_ctl_free_one(kcontrol);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n {\n \tstruct snd_ctl_elem_id id;\n \tunsigned int idx;\n+\tunsigned int count;\n \tint err = -EINVAL;\n \n \tif (! kcontrol)\n@@ -30,8 +31,9 @@\n \tcard->controls_count += kcontrol->count;\n \tkcontrol->id.numid = card->last_numid + 1;\n \tcard->last_numid += kcontrol->count;\n+\tcount = kcontrol->count;\n \tup_write(&card->controls_rwsem);\n-\tfor (idx = 0; idx < kcontrol->count; idx++, id.index++, id.numid++)\n+\tfor (idx = 0; idx < count; idx++, id.index++, id.numid++)\n \t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);\n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [
                "\tunsigned int count;",
                "\tcount = kcontrol->count;",
                "\tfor (idx = 0; idx < count; idx++, id.index++, id.numid++)"
            ],
            "deleted": [
                "\tfor (idx = 0; idx < kcontrol->count; idx++, id.index++, id.numid++)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 does not ensure possession of a read/write lock, which allows local users to cause a denial of service (use-after-free) and obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "id": 567
    },
    {
        "cve_id": "CVE-2019-19768",
        "code_before_change": "static void blk_add_trace_bio_remap(void *ignore,\n\t\t\t\t    struct request_queue *q, struct bio *bio,\n\t\t\t\t    dev_t dev, sector_t from)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\tstruct blk_io_trace_remap r;\n\n\tif (likely(!bt))\n\t\treturn;\n\n\tr.device_from = cpu_to_be32(dev);\n\tr.device_to   = cpu_to_be32(bio_dev(bio));\n\tr.sector_from = cpu_to_be64(from);\n\n\t__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,\n\t\t\tbio_op(bio), bio->bi_opf, BLK_TA_REMAP, bio->bi_status,\n\t\t\tsizeof(r), &r, blk_trace_bio_get_cgid(q, bio));\n}",
        "code_after_change": "static void blk_add_trace_bio_remap(void *ignore,\n\t\t\t\t    struct request_queue *q, struct bio *bio,\n\t\t\t\t    dev_t dev, sector_t from)\n{\n\tstruct blk_trace *bt;\n\tstruct blk_io_trace_remap r;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tr.device_from = cpu_to_be32(dev);\n\tr.device_to   = cpu_to_be32(bio_dev(bio));\n\tr.sector_from = cpu_to_be64(from);\n\n\t__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,\n\t\t\tbio_op(bio), bio->bi_opf, BLK_TA_REMAP, bio->bi_status,\n\t\t\tsizeof(r), &r, blk_trace_bio_get_cgid(q, bio));\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,11 +2,15 @@\n \t\t\t\t    struct request_queue *q, struct bio *bio,\n \t\t\t\t    dev_t dev, sector_t from)\n {\n-\tstruct blk_trace *bt = q->blk_trace;\n+\tstruct blk_trace *bt;\n \tstruct blk_io_trace_remap r;\n \n-\tif (likely(!bt))\n+\trcu_read_lock();\n+\tbt = rcu_dereference(q->blk_trace);\n+\tif (likely(!bt)) {\n+\t\trcu_read_unlock();\n \t\treturn;\n+\t}\n \n \tr.device_from = cpu_to_be32(dev);\n \tr.device_to   = cpu_to_be32(bio_dev(bio));\n@@ -15,4 +19,5 @@\n \t__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,\n \t\t\tbio_op(bio), bio->bi_opf, BLK_TA_REMAP, bio->bi_status,\n \t\t\tsizeof(r), &r, blk_trace_bio_get_cgid(q, bio));\n+\trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct blk_trace *bt;",
                "\trcu_read_lock();",
                "\tbt = rcu_dereference(q->blk_trace);",
                "\tif (likely(!bt)) {",
                "\t\trcu_read_unlock();",
                "\t}",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tstruct blk_trace *bt = q->blk_trace;",
                "\tif (likely(!bt))"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "In the Linux kernel 5.4.0-rc2, there is a use-after-free (read) in the __blk_add_trace function in kernel/trace/blktrace.c (which is used to fill out a blk_io_trace structure and place it in a per-cpu sub-buffer).",
        "id": 2234
    },
    {
        "cve_id": "CVE-2020-36313",
        "code_before_change": "static int kvm_s390_get_cmma(struct kvm *kvm, struct kvm_s390_cmma_log *args,\n\t\t\t     u8 *res, unsigned long bufsize)\n{\n\tunsigned long mem_end, cur_gfn, next_gfn, hva, pgstev;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tstruct kvm_memory_slot *ms;\n\n\tcur_gfn = kvm_s390_next_dirty_cmma(slots, args->start_gfn);\n\tms = gfn_to_memslot(kvm, cur_gfn);\n\targs->count = 0;\n\targs->start_gfn = cur_gfn;\n\tif (!ms)\n\t\treturn 0;\n\tnext_gfn = kvm_s390_next_dirty_cmma(slots, cur_gfn + 1);\n\tmem_end = slots->memslots[0].base_gfn + slots->memslots[0].npages;\n\n\twhile (args->count < bufsize) {\n\t\thva = gfn_to_hva(kvm, cur_gfn);\n\t\tif (kvm_is_error_hva(hva))\n\t\t\treturn 0;\n\t\t/* Decrement only if we actually flipped the bit to 0 */\n\t\tif (test_and_clear_bit(cur_gfn - ms->base_gfn, kvm_second_dirty_bitmap(ms)))\n\t\t\tatomic64_dec(&kvm->arch.cmma_dirty_pages);\n\t\tif (get_pgste(kvm->mm, hva, &pgstev) < 0)\n\t\t\tpgstev = 0;\n\t\t/* Save the value */\n\t\tres[args->count++] = (pgstev >> 24) & 0x43;\n\t\t/* If the next bit is too far away, stop. */\n\t\tif (next_gfn > cur_gfn + KVM_S390_MAX_BIT_DISTANCE)\n\t\t\treturn 0;\n\t\t/* If we reached the previous \"next\", find the next one */\n\t\tif (cur_gfn == next_gfn)\n\t\t\tnext_gfn = kvm_s390_next_dirty_cmma(slots, cur_gfn + 1);\n\t\t/* Reached the end of memory or of the buffer, stop */\n\t\tif ((next_gfn >= mem_end) ||\n\t\t    (next_gfn - args->start_gfn >= bufsize))\n\t\t\treturn 0;\n\t\tcur_gfn++;\n\t\t/* Reached the end of the current memslot, take the next one. */\n\t\tif (cur_gfn - ms->base_gfn >= ms->npages) {\n\t\t\tms = gfn_to_memslot(kvm, cur_gfn);\n\t\t\tif (!ms)\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int kvm_s390_get_cmma(struct kvm *kvm, struct kvm_s390_cmma_log *args,\n\t\t\t     u8 *res, unsigned long bufsize)\n{\n\tunsigned long mem_end, cur_gfn, next_gfn, hva, pgstev;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tstruct kvm_memory_slot *ms;\n\n\tif (unlikely(!slots->used_slots))\n\t\treturn 0;\n\n\tcur_gfn = kvm_s390_next_dirty_cmma(slots, args->start_gfn);\n\tms = gfn_to_memslot(kvm, cur_gfn);\n\targs->count = 0;\n\targs->start_gfn = cur_gfn;\n\tif (!ms)\n\t\treturn 0;\n\tnext_gfn = kvm_s390_next_dirty_cmma(slots, cur_gfn + 1);\n\tmem_end = slots->memslots[0].base_gfn + slots->memslots[0].npages;\n\n\twhile (args->count < bufsize) {\n\t\thva = gfn_to_hva(kvm, cur_gfn);\n\t\tif (kvm_is_error_hva(hva))\n\t\t\treturn 0;\n\t\t/* Decrement only if we actually flipped the bit to 0 */\n\t\tif (test_and_clear_bit(cur_gfn - ms->base_gfn, kvm_second_dirty_bitmap(ms)))\n\t\t\tatomic64_dec(&kvm->arch.cmma_dirty_pages);\n\t\tif (get_pgste(kvm->mm, hva, &pgstev) < 0)\n\t\t\tpgstev = 0;\n\t\t/* Save the value */\n\t\tres[args->count++] = (pgstev >> 24) & 0x43;\n\t\t/* If the next bit is too far away, stop. */\n\t\tif (next_gfn > cur_gfn + KVM_S390_MAX_BIT_DISTANCE)\n\t\t\treturn 0;\n\t\t/* If we reached the previous \"next\", find the next one */\n\t\tif (cur_gfn == next_gfn)\n\t\t\tnext_gfn = kvm_s390_next_dirty_cmma(slots, cur_gfn + 1);\n\t\t/* Reached the end of memory or of the buffer, stop */\n\t\tif ((next_gfn >= mem_end) ||\n\t\t    (next_gfn - args->start_gfn >= bufsize))\n\t\t\treturn 0;\n\t\tcur_gfn++;\n\t\t/* Reached the end of the current memslot, take the next one. */\n\t\tif (cur_gfn - ms->base_gfn >= ms->npages) {\n\t\t\tms = gfn_to_memslot(kvm, cur_gfn);\n\t\t\tif (!ms)\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,9 @@\n \tunsigned long mem_end, cur_gfn, next_gfn, hva, pgstev;\n \tstruct kvm_memslots *slots = kvm_memslots(kvm);\n \tstruct kvm_memory_slot *ms;\n+\n+\tif (unlikely(!slots->used_slots))\n+\t\treturn 0;\n \n \tcur_gfn = kvm_s390_next_dirty_cmma(slots, args->start_gfn);\n \tms = gfn_to_memslot(kvm, cur_gfn);",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (unlikely(!slots->used_slots))",
                "\t\treturn 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 5.7. The KVM subsystem allows out-of-range access to memslots after a deletion, aka CID-0774a964ef56. This affects arch/s390/kvm/kvm-s390.c, include/linux/kvm_host.h, and virt/kvm/kvm_main.c.",
        "id": 2717
    },
    {
        "cve_id": "CVE-2014-4653",
        "code_before_change": "int snd_ctl_replace(struct snd_card *card, struct snd_kcontrol *kcontrol,\n\t\t    bool add_on_replace)\n{\n\tstruct snd_ctl_elem_id id;\n\tunsigned int idx;\n\tstruct snd_kcontrol *old;\n\tint ret;\n\n\tif (!kcontrol)\n\t\treturn -EINVAL;\n\tif (snd_BUG_ON(!card || !kcontrol->info)) {\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tid = kcontrol->id;\n\tdown_write(&card->controls_rwsem);\n\told = snd_ctl_find_id(card, &id);\n\tif (!old) {\n\t\tif (add_on_replace)\n\t\t\tgoto add;\n\t\tup_write(&card->controls_rwsem);\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tret = snd_ctl_remove(card, old);\n\tif (ret < 0) {\n\t\tup_write(&card->controls_rwsem);\n\t\tgoto error;\n\t}\nadd:\n\tif (snd_ctl_find_hole(card, kcontrol->count) < 0) {\n\t\tup_write(&card->controls_rwsem);\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tlist_add_tail(&kcontrol->list, &card->controls);\n\tcard->controls_count += kcontrol->count;\n\tkcontrol->id.numid = card->last_numid + 1;\n\tcard->last_numid += kcontrol->count;\n\tup_write(&card->controls_rwsem);\n\tfor (idx = 0; idx < kcontrol->count; idx++, id.index++, id.numid++)\n\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);\n\treturn 0;\n\nerror:\n\tsnd_ctl_free_one(kcontrol);\n\treturn ret;\n}",
        "code_after_change": "int snd_ctl_replace(struct snd_card *card, struct snd_kcontrol *kcontrol,\n\t\t    bool add_on_replace)\n{\n\tstruct snd_ctl_elem_id id;\n\tunsigned int count;\n\tunsigned int idx;\n\tstruct snd_kcontrol *old;\n\tint ret;\n\n\tif (!kcontrol)\n\t\treturn -EINVAL;\n\tif (snd_BUG_ON(!card || !kcontrol->info)) {\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tid = kcontrol->id;\n\tdown_write(&card->controls_rwsem);\n\told = snd_ctl_find_id(card, &id);\n\tif (!old) {\n\t\tif (add_on_replace)\n\t\t\tgoto add;\n\t\tup_write(&card->controls_rwsem);\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tret = snd_ctl_remove(card, old);\n\tif (ret < 0) {\n\t\tup_write(&card->controls_rwsem);\n\t\tgoto error;\n\t}\nadd:\n\tif (snd_ctl_find_hole(card, kcontrol->count) < 0) {\n\t\tup_write(&card->controls_rwsem);\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tlist_add_tail(&kcontrol->list, &card->controls);\n\tcard->controls_count += kcontrol->count;\n\tkcontrol->id.numid = card->last_numid + 1;\n\tcard->last_numid += kcontrol->count;\n\tcount = kcontrol->count;\n\tup_write(&card->controls_rwsem);\n\tfor (idx = 0; idx < count; idx++, id.index++, id.numid++)\n\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);\n\treturn 0;\n\nerror:\n\tsnd_ctl_free_one(kcontrol);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n \t\t    bool add_on_replace)\n {\n \tstruct snd_ctl_elem_id id;\n+\tunsigned int count;\n \tunsigned int idx;\n \tstruct snd_kcontrol *old;\n \tint ret;\n@@ -37,8 +38,9 @@\n \tcard->controls_count += kcontrol->count;\n \tkcontrol->id.numid = card->last_numid + 1;\n \tcard->last_numid += kcontrol->count;\n+\tcount = kcontrol->count;\n \tup_write(&card->controls_rwsem);\n-\tfor (idx = 0; idx < kcontrol->count; idx++, id.index++, id.numid++)\n+\tfor (idx = 0; idx < count; idx++, id.index++, id.numid++)\n \t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);\n \treturn 0;\n ",
        "function_modified_lines": {
            "added": [
                "\tunsigned int count;",
                "\tcount = kcontrol->count;",
                "\tfor (idx = 0; idx < count; idx++, id.index++, id.numid++)"
            ],
            "deleted": [
                "\tfor (idx = 0; idx < kcontrol->count; idx++, id.index++, id.numid++)"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "sound/core/control.c in the ALSA control implementation in the Linux kernel before 3.15.2 does not ensure possession of a read/write lock, which allows local users to cause a denial of service (use-after-free) and obtain sensitive information from kernel memory by leveraging /dev/snd/controlCX access.",
        "id": 570
    },
    {
        "cve_id": "CVE-2014-9914",
        "code_before_change": "void ip4_datagram_release_cb(struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\tif (! __sk_dst_get(sk) || __sk_dst_check(sk, 0))\n\t\treturn;\n\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trt = ip_route_output_ports(sock_net(sk), &fl4, sk, daddr,\n\t\t\t\t   inet->inet_saddr, inet->inet_dport,\n\t\t\t\t   inet->inet_sport, sk->sk_protocol,\n\t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt))\n\t\t__sk_dst_set(sk, &rt->dst);\n\trcu_read_unlock();\n}",
        "code_after_change": "void ip4_datagram_release_cb(struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct dst_entry *dst;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\n\trcu_read_lock();\n\n\tdst = __sk_dst_get(sk);\n\tif (!dst || !dst->obsolete || dst->ops->check(dst, 0)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trt = ip_route_output_ports(sock_net(sk), &fl4, sk, daddr,\n\t\t\t\t   inet->inet_saddr, inet->inet_dport,\n\t\t\t\t   inet->inet_sport, sk->sk_protocol,\n\t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n\n\tdst = !IS_ERR(rt) ? &rt->dst : NULL;\n\tsk_dst_set(sk, dst);\n\n\trcu_read_unlock();\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,13 +3,17 @@\n \tconst struct inet_sock *inet = inet_sk(sk);\n \tconst struct ip_options_rcu *inet_opt;\n \t__be32 daddr = inet->inet_daddr;\n+\tstruct dst_entry *dst;\n \tstruct flowi4 fl4;\n \tstruct rtable *rt;\n \n-\tif (! __sk_dst_get(sk) || __sk_dst_check(sk, 0))\n+\trcu_read_lock();\n+\n+\tdst = __sk_dst_get(sk);\n+\tif (!dst || !dst->obsolete || dst->ops->check(dst, 0)) {\n+\t\trcu_read_unlock();\n \t\treturn;\n-\n-\trcu_read_lock();\n+\t}\n \tinet_opt = rcu_dereference(inet->inet_opt);\n \tif (inet_opt && inet_opt->opt.srr)\n \t\tdaddr = inet_opt->opt.faddr;\n@@ -17,7 +21,9 @@\n \t\t\t\t   inet->inet_saddr, inet->inet_dport,\n \t\t\t\t   inet->inet_sport, sk->sk_protocol,\n \t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n-\tif (!IS_ERR(rt))\n-\t\t__sk_dst_set(sk, &rt->dst);\n+\n+\tdst = !IS_ERR(rt) ? &rt->dst : NULL;\n+\tsk_dst_set(sk, dst);\n+\n \trcu_read_unlock();\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct dst_entry *dst;",
                "\trcu_read_lock();",
                "",
                "\tdst = __sk_dst_get(sk);",
                "\tif (!dst || !dst->obsolete || dst->ops->check(dst, 0)) {",
                "\t\trcu_read_unlock();",
                "\t}",
                "",
                "\tdst = !IS_ERR(rt) ? &rt->dst : NULL;",
                "\tsk_dst_set(sk, dst);",
                ""
            ],
            "deleted": [
                "\tif (! __sk_dst_get(sk) || __sk_dst_check(sk, 0))",
                "",
                "\trcu_read_lock();",
                "\tif (!IS_ERR(rt))",
                "\t\t__sk_dst_set(sk, &rt->dst);"
            ]
        },
        "cwe": [
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ip4_datagram_release_cb function in net/ipv4/datagram.c in the Linux kernel before 3.15.2 allows local users to gain privileges or cause a denial of service (use-after-free) by leveraging incorrect expectations about locking during multithreaded access to internal data structures for IPv4 UDP sockets.",
        "id": 711
    },
    {
        "cve_id": "CVE-2022-42703",
        "code_before_change": "void unlink_anon_vmas(struct vm_area_struct *vma)\n{\n\tstruct anon_vma_chain *avc, *next;\n\tstruct anon_vma *root = NULL;\n\n\t/*\n\t * Unlink each anon_vma chained to the VMA.  This list is ordered\n\t * from newest to oldest, ensuring the root anon_vma gets freed last.\n\t */\n\tlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma = avc->anon_vma;\n\n\t\troot = lock_anon_vma_root(root, anon_vma);\n\t\tanon_vma_interval_tree_remove(avc, &anon_vma->rb_root);\n\n\t\t/*\n\t\t * Leave empty anon_vmas on the list - we'll need\n\t\t * to free them outside the lock.\n\t\t */\n\t\tif (RB_EMPTY_ROOT(&anon_vma->rb_root.rb_root)) {\n\t\t\tanon_vma->parent->degree--;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_del(&avc->same_vma);\n\t\tanon_vma_chain_free(avc);\n\t}\n\tif (vma->anon_vma) {\n\t\tvma->anon_vma->degree--;\n\n\t\t/*\n\t\t * vma would still be needed after unlink, and anon_vma will be prepared\n\t\t * when handle fault.\n\t\t */\n\t\tvma->anon_vma = NULL;\n\t}\n\tunlock_anon_vma_root(root);\n\n\t/*\n\t * Iterate the list once more, it now only contains empty and unlinked\n\t * anon_vmas, destroy them. Could not do before due to __put_anon_vma()\n\t * needing to write-acquire the anon_vma->root->rwsem.\n\t */\n\tlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma = avc->anon_vma;\n\n\t\tVM_WARN_ON(anon_vma->degree);\n\t\tput_anon_vma(anon_vma);\n\n\t\tlist_del(&avc->same_vma);\n\t\tanon_vma_chain_free(avc);\n\t}\n}",
        "code_after_change": "void unlink_anon_vmas(struct vm_area_struct *vma)\n{\n\tstruct anon_vma_chain *avc, *next;\n\tstruct anon_vma *root = NULL;\n\n\t/*\n\t * Unlink each anon_vma chained to the VMA.  This list is ordered\n\t * from newest to oldest, ensuring the root anon_vma gets freed last.\n\t */\n\tlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma = avc->anon_vma;\n\n\t\troot = lock_anon_vma_root(root, anon_vma);\n\t\tanon_vma_interval_tree_remove(avc, &anon_vma->rb_root);\n\n\t\t/*\n\t\t * Leave empty anon_vmas on the list - we'll need\n\t\t * to free them outside the lock.\n\t\t */\n\t\tif (RB_EMPTY_ROOT(&anon_vma->rb_root.rb_root)) {\n\t\t\tanon_vma->parent->num_children--;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_del(&avc->same_vma);\n\t\tanon_vma_chain_free(avc);\n\t}\n\tif (vma->anon_vma) {\n\t\tvma->anon_vma->num_active_vmas--;\n\n\t\t/*\n\t\t * vma would still be needed after unlink, and anon_vma will be prepared\n\t\t * when handle fault.\n\t\t */\n\t\tvma->anon_vma = NULL;\n\t}\n\tunlock_anon_vma_root(root);\n\n\t/*\n\t * Iterate the list once more, it now only contains empty and unlinked\n\t * anon_vmas, destroy them. Could not do before due to __put_anon_vma()\n\t * needing to write-acquire the anon_vma->root->rwsem.\n\t */\n\tlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma = avc->anon_vma;\n\n\t\tVM_WARN_ON(anon_vma->num_children);\n\t\tVM_WARN_ON(anon_vma->num_active_vmas);\n\t\tput_anon_vma(anon_vma);\n\n\t\tlist_del(&avc->same_vma);\n\t\tanon_vma_chain_free(avc);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,7 @@\n \t\t * to free them outside the lock.\n \t\t */\n \t\tif (RB_EMPTY_ROOT(&anon_vma->rb_root.rb_root)) {\n-\t\t\tanon_vma->parent->degree--;\n+\t\t\tanon_vma->parent->num_children--;\n \t\t\tcontinue;\n \t\t}\n \n@@ -26,7 +26,7 @@\n \t\tanon_vma_chain_free(avc);\n \t}\n \tif (vma->anon_vma) {\n-\t\tvma->anon_vma->degree--;\n+\t\tvma->anon_vma->num_active_vmas--;\n \n \t\t/*\n \t\t * vma would still be needed after unlink, and anon_vma will be prepared\n@@ -44,7 +44,8 @@\n \tlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\n \t\tstruct anon_vma *anon_vma = avc->anon_vma;\n \n-\t\tVM_WARN_ON(anon_vma->degree);\n+\t\tVM_WARN_ON(anon_vma->num_children);\n+\t\tVM_WARN_ON(anon_vma->num_active_vmas);\n \t\tput_anon_vma(anon_vma);\n \n \t\tlist_del(&avc->same_vma);",
        "function_modified_lines": {
            "added": [
                "\t\t\tanon_vma->parent->num_children--;",
                "\t\tvma->anon_vma->num_active_vmas--;",
                "\t\tVM_WARN_ON(anon_vma->num_children);",
                "\t\tVM_WARN_ON(anon_vma->num_active_vmas);"
            ],
            "deleted": [
                "\t\t\tanon_vma->parent->degree--;",
                "\t\tvma->anon_vma->degree--;",
                "\t\tVM_WARN_ON(anon_vma->degree);"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "mm/rmap.c in the Linux kernel before 5.19.7 has a use-after-free related to leaf anon_vma double reuse.",
        "id": 3730
    },
    {
        "cve_id": "CVE-2022-2585",
        "code_before_change": "int begin_new_exec(struct linux_binprm * bprm)\n{\n\tstruct task_struct *me = current;\n\tint retval;\n\n\t/* Once we are committed compute the creds */\n\tretval = bprm_creds_from_file(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\t/*\n\t * Ensure all future errors are fatal.\n\t */\n\tbprm->point_of_no_return = true;\n\n\t/*\n\t * Make this the only thread in the thread group.\n\t */\n\tretval = de_thread(me);\n\tif (retval)\n\t\tgoto out;\n\n\t/*\n\t * Cancel any io_uring activity across execve\n\t */\n\tio_uring_task_cancel();\n\n\t/* Ensure the files table is not shared. */\n\tretval = unshare_files();\n\tif (retval)\n\t\tgoto out;\n\n\t/*\n\t * Must be called _before_ exec_mmap() as bprm->mm is\n\t * not visible until then. This also enables the update\n\t * to be lockless.\n\t */\n\tretval = set_mm_exe_file(bprm->mm, bprm->file);\n\tif (retval)\n\t\tgoto out;\n\n\t/* If the binary is not readable then enforce mm->dumpable=0 */\n\twould_dump(bprm, bprm->file);\n\tif (bprm->have_execfd)\n\t\twould_dump(bprm, bprm->executable);\n\n\t/*\n\t * Release all of the old mmap stuff\n\t */\n\tacct_arg_size(bprm, 0);\n\tretval = exec_mmap(bprm->mm);\n\tif (retval)\n\t\tgoto out;\n\n\tbprm->mm = NULL;\n\n#ifdef CONFIG_POSIX_TIMERS\n\texit_itimers(me);\n\tflush_itimer_signals();\n#endif\n\n\t/*\n\t * Make the signal table private.\n\t */\n\tretval = unshare_sighand(me);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tme->flags &= ~(PF_RANDOMIZE | PF_FORKNOEXEC |\n\t\t\t\t\tPF_NOFREEZE | PF_NO_SETAFFINITY);\n\tflush_thread();\n\tme->personality &= ~bprm->per_clear;\n\n\tclear_syscall_work_syscall_user_dispatch(me);\n\n\t/*\n\t * We have to apply CLOEXEC before we change whether the process is\n\t * dumpable (in setup_new_exec) to avoid a race with a process in userspace\n\t * trying to access the should-be-closed file descriptors of a process\n\t * undergoing exec(2).\n\t */\n\tdo_close_on_exec(me->files);\n\n\tif (bprm->secureexec) {\n\t\t/* Make sure parent cannot signal privileged process. */\n\t\tme->pdeath_signal = 0;\n\n\t\t/*\n\t\t * For secureexec, reset the stack limit to sane default to\n\t\t * avoid bad behavior from the prior rlimits. This has to\n\t\t * happen before arch_pick_mmap_layout(), which examines\n\t\t * RLIMIT_STACK, but after the point of no return to avoid\n\t\t * needing to clean up the change on failure.\n\t\t */\n\t\tif (bprm->rlim_stack.rlim_cur > _STK_LIM)\n\t\t\tbprm->rlim_stack.rlim_cur = _STK_LIM;\n\t}\n\n\tme->sas_ss_sp = me->sas_ss_size = 0;\n\n\t/*\n\t * Figure out dumpability. Note that this checking only of current\n\t * is wrong, but userspace depends on it. This should be testing\n\t * bprm->secureexec instead.\n\t */\n\tif (bprm->interp_flags & BINPRM_FLAGS_ENFORCE_NONDUMP ||\n\t    !(uid_eq(current_euid(), current_uid()) &&\n\t      gid_eq(current_egid(), current_gid())))\n\t\tset_dumpable(current->mm, suid_dumpable);\n\telse\n\t\tset_dumpable(current->mm, SUID_DUMP_USER);\n\n\tperf_event_exec();\n\t__set_task_comm(me, kbasename(bprm->filename), true);\n\n\t/* An exec changes our domain. We are no longer part of the thread\n\t   group */\n\tWRITE_ONCE(me->self_exec_id, me->self_exec_id + 1);\n\tflush_signal_handlers(me, 0);\n\n\tretval = set_cred_ucounts(bprm->cred);\n\tif (retval < 0)\n\t\tgoto out_unlock;\n\n\t/*\n\t * install the new credentials for this executable\n\t */\n\tsecurity_bprm_committing_creds(bprm);\n\n\tcommit_creds(bprm->cred);\n\tbprm->cred = NULL;\n\n\t/*\n\t * Disable monitoring for regular users\n\t * when executing setuid binaries. Must\n\t * wait until new credentials are committed\n\t * by commit_creds() above\n\t */\n\tif (get_dumpable(me->mm) != SUID_DUMP_USER)\n\t\tperf_event_exit_task(me);\n\t/*\n\t * cred_guard_mutex must be held at least to this point to prevent\n\t * ptrace_attach() from altering our determination of the task's\n\t * credentials; any time after this it may be unlocked.\n\t */\n\tsecurity_bprm_committed_creds(bprm);\n\n\t/* Pass the opened binary to the interpreter. */\n\tif (bprm->have_execfd) {\n\t\tretval = get_unused_fd_flags(0);\n\t\tif (retval < 0)\n\t\t\tgoto out_unlock;\n\t\tfd_install(retval, bprm->executable);\n\t\tbprm->executable = NULL;\n\t\tbprm->execfd = retval;\n\t}\n\treturn 0;\n\nout_unlock:\n\tup_write(&me->signal->exec_update_lock);\nout:\n\treturn retval;\n}",
        "code_after_change": "int begin_new_exec(struct linux_binprm * bprm)\n{\n\tstruct task_struct *me = current;\n\tint retval;\n\n\t/* Once we are committed compute the creds */\n\tretval = bprm_creds_from_file(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\t/*\n\t * Ensure all future errors are fatal.\n\t */\n\tbprm->point_of_no_return = true;\n\n\t/*\n\t * Make this the only thread in the thread group.\n\t */\n\tretval = de_thread(me);\n\tif (retval)\n\t\tgoto out;\n\n\t/*\n\t * Cancel any io_uring activity across execve\n\t */\n\tio_uring_task_cancel();\n\n\t/* Ensure the files table is not shared. */\n\tretval = unshare_files();\n\tif (retval)\n\t\tgoto out;\n\n\t/*\n\t * Must be called _before_ exec_mmap() as bprm->mm is\n\t * not visible until then. This also enables the update\n\t * to be lockless.\n\t */\n\tretval = set_mm_exe_file(bprm->mm, bprm->file);\n\tif (retval)\n\t\tgoto out;\n\n\t/* If the binary is not readable then enforce mm->dumpable=0 */\n\twould_dump(bprm, bprm->file);\n\tif (bprm->have_execfd)\n\t\twould_dump(bprm, bprm->executable);\n\n\t/*\n\t * Release all of the old mmap stuff\n\t */\n\tacct_arg_size(bprm, 0);\n\tretval = exec_mmap(bprm->mm);\n\tif (retval)\n\t\tgoto out;\n\n\tbprm->mm = NULL;\n\n#ifdef CONFIG_POSIX_TIMERS\n\tspin_lock_irq(&me->sighand->siglock);\n\tposix_cpu_timers_exit(me);\n\tspin_unlock_irq(&me->sighand->siglock);\n\texit_itimers(me);\n\tflush_itimer_signals();\n#endif\n\n\t/*\n\t * Make the signal table private.\n\t */\n\tretval = unshare_sighand(me);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tme->flags &= ~(PF_RANDOMIZE | PF_FORKNOEXEC |\n\t\t\t\t\tPF_NOFREEZE | PF_NO_SETAFFINITY);\n\tflush_thread();\n\tme->personality &= ~bprm->per_clear;\n\n\tclear_syscall_work_syscall_user_dispatch(me);\n\n\t/*\n\t * We have to apply CLOEXEC before we change whether the process is\n\t * dumpable (in setup_new_exec) to avoid a race with a process in userspace\n\t * trying to access the should-be-closed file descriptors of a process\n\t * undergoing exec(2).\n\t */\n\tdo_close_on_exec(me->files);\n\n\tif (bprm->secureexec) {\n\t\t/* Make sure parent cannot signal privileged process. */\n\t\tme->pdeath_signal = 0;\n\n\t\t/*\n\t\t * For secureexec, reset the stack limit to sane default to\n\t\t * avoid bad behavior from the prior rlimits. This has to\n\t\t * happen before arch_pick_mmap_layout(), which examines\n\t\t * RLIMIT_STACK, but after the point of no return to avoid\n\t\t * needing to clean up the change on failure.\n\t\t */\n\t\tif (bprm->rlim_stack.rlim_cur > _STK_LIM)\n\t\t\tbprm->rlim_stack.rlim_cur = _STK_LIM;\n\t}\n\n\tme->sas_ss_sp = me->sas_ss_size = 0;\n\n\t/*\n\t * Figure out dumpability. Note that this checking only of current\n\t * is wrong, but userspace depends on it. This should be testing\n\t * bprm->secureexec instead.\n\t */\n\tif (bprm->interp_flags & BINPRM_FLAGS_ENFORCE_NONDUMP ||\n\t    !(uid_eq(current_euid(), current_uid()) &&\n\t      gid_eq(current_egid(), current_gid())))\n\t\tset_dumpable(current->mm, suid_dumpable);\n\telse\n\t\tset_dumpable(current->mm, SUID_DUMP_USER);\n\n\tperf_event_exec();\n\t__set_task_comm(me, kbasename(bprm->filename), true);\n\n\t/* An exec changes our domain. We are no longer part of the thread\n\t   group */\n\tWRITE_ONCE(me->self_exec_id, me->self_exec_id + 1);\n\tflush_signal_handlers(me, 0);\n\n\tretval = set_cred_ucounts(bprm->cred);\n\tif (retval < 0)\n\t\tgoto out_unlock;\n\n\t/*\n\t * install the new credentials for this executable\n\t */\n\tsecurity_bprm_committing_creds(bprm);\n\n\tcommit_creds(bprm->cred);\n\tbprm->cred = NULL;\n\n\t/*\n\t * Disable monitoring for regular users\n\t * when executing setuid binaries. Must\n\t * wait until new credentials are committed\n\t * by commit_creds() above\n\t */\n\tif (get_dumpable(me->mm) != SUID_DUMP_USER)\n\t\tperf_event_exit_task(me);\n\t/*\n\t * cred_guard_mutex must be held at least to this point to prevent\n\t * ptrace_attach() from altering our determination of the task's\n\t * credentials; any time after this it may be unlocked.\n\t */\n\tsecurity_bprm_committed_creds(bprm);\n\n\t/* Pass the opened binary to the interpreter. */\n\tif (bprm->have_execfd) {\n\t\tretval = get_unused_fd_flags(0);\n\t\tif (retval < 0)\n\t\t\tgoto out_unlock;\n\t\tfd_install(retval, bprm->executable);\n\t\tbprm->executable = NULL;\n\t\tbprm->execfd = retval;\n\t}\n\treturn 0;\n\nout_unlock:\n\tup_write(&me->signal->exec_update_lock);\nout:\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -55,6 +55,9 @@\n \tbprm->mm = NULL;\n \n #ifdef CONFIG_POSIX_TIMERS\n+\tspin_lock_irq(&me->sighand->siglock);\n+\tposix_cpu_timers_exit(me);\n+\tspin_unlock_irq(&me->sighand->siglock);\n \texit_itimers(me);\n \tflush_itimer_signals();\n #endif",
        "function_modified_lines": {
            "added": [
                "\tspin_lock_irq(&me->sighand->siglock);",
                "\tposix_cpu_timers_exit(me);",
                "\tspin_unlock_irq(&me->sighand->siglock);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "It was discovered that when exec'ing from a non-leader thread, armed POSIX CPU timers would be left on a list but freed, leading to a use-after-free.",
        "id": 3477
    },
    {
        "cve_id": "CVE-2022-4382",
        "code_before_change": "static int\ngadgetfs_fill_super (struct super_block *sb, struct fs_context *fc)\n{\n\tstruct inode\t*inode;\n\tstruct dev_data\t*dev;\n\n\tif (the_device)\n\t\treturn -ESRCH;\n\n\tCHIP = usb_get_gadget_udc_name();\n\tif (!CHIP)\n\t\treturn -ENODEV;\n\n\t/* superblock */\n\tsb->s_blocksize = PAGE_SIZE;\n\tsb->s_blocksize_bits = PAGE_SHIFT;\n\tsb->s_magic = GADGETFS_MAGIC;\n\tsb->s_op = &gadget_fs_operations;\n\tsb->s_time_gran = 1;\n\n\t/* root inode */\n\tinode = gadgetfs_make_inode (sb,\n\t\t\tNULL, &simple_dir_operations,\n\t\t\tS_IFDIR | S_IRUGO | S_IXUGO);\n\tif (!inode)\n\t\tgoto Enomem;\n\tinode->i_op = &simple_dir_inode_operations;\n\tif (!(sb->s_root = d_make_root (inode)))\n\t\tgoto Enomem;\n\n\t/* the ep0 file is named after the controller we expect;\n\t * user mode code can use it for sanity checks, like we do.\n\t */\n\tdev = dev_new ();\n\tif (!dev)\n\t\tgoto Enomem;\n\n\tdev->sb = sb;\n\tdev->dentry = gadgetfs_create_file(sb, CHIP, dev, &ep0_operations);\n\tif (!dev->dentry) {\n\t\tput_dev(dev);\n\t\tgoto Enomem;\n\t}\n\n\t/* other endpoint files are available after hardware setup,\n\t * from binding to a controller.\n\t */\n\tthe_device = dev;\n\treturn 0;\n\nEnomem:\n\tkfree(CHIP);\n\tCHIP = NULL;\n\n\treturn -ENOMEM;\n}",
        "code_after_change": "static int\ngadgetfs_fill_super (struct super_block *sb, struct fs_context *fc)\n{\n\tstruct inode\t*inode;\n\tstruct dev_data\t*dev;\n\tint\t\trc;\n\n\tmutex_lock(&sb_mutex);\n\n\tif (the_device) {\n\t\trc = -ESRCH;\n\t\tgoto Done;\n\t}\n\n\tCHIP = usb_get_gadget_udc_name();\n\tif (!CHIP) {\n\t\trc = -ENODEV;\n\t\tgoto Done;\n\t}\n\n\t/* superblock */\n\tsb->s_blocksize = PAGE_SIZE;\n\tsb->s_blocksize_bits = PAGE_SHIFT;\n\tsb->s_magic = GADGETFS_MAGIC;\n\tsb->s_op = &gadget_fs_operations;\n\tsb->s_time_gran = 1;\n\n\t/* root inode */\n\tinode = gadgetfs_make_inode (sb,\n\t\t\tNULL, &simple_dir_operations,\n\t\t\tS_IFDIR | S_IRUGO | S_IXUGO);\n\tif (!inode)\n\t\tgoto Enomem;\n\tinode->i_op = &simple_dir_inode_operations;\n\tif (!(sb->s_root = d_make_root (inode)))\n\t\tgoto Enomem;\n\n\t/* the ep0 file is named after the controller we expect;\n\t * user mode code can use it for sanity checks, like we do.\n\t */\n\tdev = dev_new ();\n\tif (!dev)\n\t\tgoto Enomem;\n\n\tdev->sb = sb;\n\tdev->dentry = gadgetfs_create_file(sb, CHIP, dev, &ep0_operations);\n\tif (!dev->dentry) {\n\t\tput_dev(dev);\n\t\tgoto Enomem;\n\t}\n\n\t/* other endpoint files are available after hardware setup,\n\t * from binding to a controller.\n\t */\n\tthe_device = dev;\n\trc = 0;\n\tgoto Done;\n\n Enomem:\n\tkfree(CHIP);\n\tCHIP = NULL;\n\trc = -ENOMEM;\n\n Done:\n\tmutex_unlock(&sb_mutex);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,13 +3,20 @@\n {\n \tstruct inode\t*inode;\n \tstruct dev_data\t*dev;\n+\tint\t\trc;\n \n-\tif (the_device)\n-\t\treturn -ESRCH;\n+\tmutex_lock(&sb_mutex);\n+\n+\tif (the_device) {\n+\t\trc = -ESRCH;\n+\t\tgoto Done;\n+\t}\n \n \tCHIP = usb_get_gadget_udc_name();\n-\tif (!CHIP)\n-\t\treturn -ENODEV;\n+\tif (!CHIP) {\n+\t\trc = -ENODEV;\n+\t\tgoto Done;\n+\t}\n \n \t/* superblock */\n \tsb->s_blocksize = PAGE_SIZE;\n@@ -46,11 +53,15 @@\n \t * from binding to a controller.\n \t */\n \tthe_device = dev;\n-\treturn 0;\n+\trc = 0;\n+\tgoto Done;\n \n-Enomem:\n+ Enomem:\n \tkfree(CHIP);\n \tCHIP = NULL;\n+\trc = -ENOMEM;\n \n-\treturn -ENOMEM;\n+ Done:\n+\tmutex_unlock(&sb_mutex);\n+\treturn rc;\n }",
        "function_modified_lines": {
            "added": [
                "\tint\t\trc;",
                "\tmutex_lock(&sb_mutex);",
                "",
                "\tif (the_device) {",
                "\t\trc = -ESRCH;",
                "\t\tgoto Done;",
                "\t}",
                "\tif (!CHIP) {",
                "\t\trc = -ENODEV;",
                "\t\tgoto Done;",
                "\t}",
                "\trc = 0;",
                "\tgoto Done;",
                " Enomem:",
                "\trc = -ENOMEM;",
                " Done:",
                "\tmutex_unlock(&sb_mutex);",
                "\treturn rc;"
            ],
            "deleted": [
                "\tif (the_device)",
                "\t\treturn -ESRCH;",
                "\tif (!CHIP)",
                "\t\treturn -ENODEV;",
                "\treturn 0;",
                "Enomem:",
                "\treturn -ENOMEM;"
            ]
        },
        "cwe": [
            "CWE-416"
        ],
        "cve_description": "A use-after-free flaw caused by a race among the superblock operations in the gadgetfs Linux driver was found. It could be triggered by yanking out a device that is running the gadgetfs side.",
        "id": 3747
    }
]