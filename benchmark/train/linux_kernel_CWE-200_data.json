[
    {
        "cve_id": "CVE-2018-19854",
        "code_before_change": "static int crypto_report_comp(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_comp rcomp;\n\n\tstrlcpy(rcomp.type, \"compression\", sizeof(rcomp.type));\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,\n\t\t    sizeof(struct crypto_report_comp), &rcomp))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "code_after_change": "static int crypto_report_comp(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_comp rcomp;\n\n\tstrncpy(rcomp.type, \"compression\", sizeof(rcomp.type));\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,\n\t\t    sizeof(struct crypto_report_comp), &rcomp))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tstruct crypto_report_comp rcomp;\n \n-\tstrlcpy(rcomp.type, \"compression\", sizeof(rcomp.type));\n+\tstrncpy(rcomp.type, \"compression\", sizeof(rcomp.type));\n \tif (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,\n \t\t    sizeof(struct crypto_report_comp), &rcomp))\n \t\tgoto nla_put_failure;",
        "function_modified_lines": {
            "added": [
                "\tstrncpy(rcomp.type, \"compression\", sizeof(rcomp.type));"
            ],
            "deleted": [
                "\tstrlcpy(rcomp.type, \"compression\", sizeof(rcomp.type));"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.19.3. crypto_report_one() and related functions in crypto/crypto_user.c (the crypto user configuration API) do not fully initialize structures that are copied to userspace, potentially leaking sensitive memory to user programs. NOTE: this is a CVE-2013-2547 regression but with easier exploitability because the attacker does not need a capability (however, the system must have the CONFIG_CRYPTO_USER kconfig option).",
        "id": 1747
    },
    {
        "cve_id": "CVE-2018-19854",
        "code_before_change": "static int crypto_report_acomp(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_acomp racomp;\n\n\tstrlcpy(racomp.type, \"acomp\", sizeof(racomp.type));\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_ACOMP,\n\t\t    sizeof(struct crypto_report_acomp), &racomp))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "code_after_change": "static int crypto_report_acomp(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_acomp racomp;\n\n\tstrncpy(racomp.type, \"acomp\", sizeof(racomp.type));\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_ACOMP,\n\t\t    sizeof(struct crypto_report_acomp), &racomp))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tstruct crypto_report_acomp racomp;\n \n-\tstrlcpy(racomp.type, \"acomp\", sizeof(racomp.type));\n+\tstrncpy(racomp.type, \"acomp\", sizeof(racomp.type));\n \n \tif (nla_put(skb, CRYPTOCFGA_REPORT_ACOMP,\n \t\t    sizeof(struct crypto_report_acomp), &racomp))",
        "function_modified_lines": {
            "added": [
                "\tstrncpy(racomp.type, \"acomp\", sizeof(racomp.type));"
            ],
            "deleted": [
                "\tstrlcpy(racomp.type, \"acomp\", sizeof(racomp.type));"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.19.3. crypto_report_one() and related functions in crypto/crypto_user.c (the crypto user configuration API) do not fully initialize structures that are copied to userspace, potentially leaking sensitive memory to user programs. NOTE: this is a CVE-2013-2547 regression but with easier exploitability because the attacker does not need a capability (however, the system must have the CONFIG_CRYPTO_USER kconfig option).",
        "id": 1751
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "void __cpuinit fpu_init(void)\n{\n\tunsigned long cr0;\n\tunsigned long cr4_mask = 0;\n\n\tif (cpu_has_fxsr)\n\t\tcr4_mask |= X86_CR4_OSFXSR;\n\tif (cpu_has_xmm)\n\t\tcr4_mask |= X86_CR4_OSXMMEXCPT;\n\tif (cr4_mask)\n\t\tset_in_cr4(cr4_mask);\n\n\tcr0 = read_cr0();\n\tcr0 &= ~(X86_CR0_TS|X86_CR0_EM); /* clear TS and EM */\n\tif (!HAVE_HWFP)\n\t\tcr0 |= X86_CR0_EM;\n\twrite_cr0(cr0);\n\n\tif (!smp_processor_id())\n\t\tinit_thread_xstate();\n\n\tmxcsr_feature_mask_init();\n\t/* clean state in init */\n\tcurrent_thread_info()->status = 0;\n\tclear_used_math();\n}",
        "code_after_change": "void __cpuinit fpu_init(void)\n{\n\tunsigned long cr0;\n\tunsigned long cr4_mask = 0;\n\n\tif (cpu_has_fxsr)\n\t\tcr4_mask |= X86_CR4_OSFXSR;\n\tif (cpu_has_xmm)\n\t\tcr4_mask |= X86_CR4_OSXMMEXCPT;\n\tif (cr4_mask)\n\t\tset_in_cr4(cr4_mask);\n\n\tcr0 = read_cr0();\n\tcr0 &= ~(X86_CR0_TS|X86_CR0_EM); /* clear TS and EM */\n\tif (!HAVE_HWFP)\n\t\tcr0 |= X86_CR0_EM;\n\twrite_cr0(cr0);\n\n\tif (!smp_processor_id())\n\t\tinit_thread_xstate();\n\n\tmxcsr_feature_mask_init();\n\txsave_init();\n\teager_fpu_init();\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,7 +20,6 @@\n \t\tinit_thread_xstate();\n \n \tmxcsr_feature_mask_init();\n-\t/* clean state in init */\n-\tcurrent_thread_info()->status = 0;\n-\tclear_used_math();\n+\txsave_init();\n+\teager_fpu_init();\n }",
        "function_modified_lines": {
            "added": [
                "\txsave_init();",
                "\teager_fpu_init();"
            ],
            "deleted": [
                "\t/* clean state in init */",
                "\tcurrent_thread_info()->status = 0;",
                "\tclear_used_math();"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1809
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static void xlvbd_flush(struct blkfront_info *info)\n{\n\tblk_queue_write_cache(info->rq, info->feature_flush ? true : false,\n\t\t\t      info->feature_fua ? true : false);\n\tpr_info(\"blkfront: %s: %s %s %s %s %s\\n\",\n\t\tinfo->gd->disk_name, flush_info(info),\n\t\t\"persistent grants:\", info->feature_persistent ?\n\t\t\"enabled;\" : \"disabled;\", \"indirect descriptors:\",\n\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\");\n}",
        "code_after_change": "static void xlvbd_flush(struct blkfront_info *info)\n{\n\tblk_queue_write_cache(info->rq, info->feature_flush ? true : false,\n\t\t\t      info->feature_fua ? true : false);\n\tpr_info(\"blkfront: %s: %s %s %s %s %s %s %s\\n\",\n\t\tinfo->gd->disk_name, flush_info(info),\n\t\t\"persistent grants:\", info->feature_persistent ?\n\t\t\"enabled;\" : \"disabled;\", \"indirect descriptors:\",\n\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\",\n\t\t\"bounce buffer:\", info->bounce ? \"enabled\" : \"disabled;\");\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,9 +2,10 @@\n {\n \tblk_queue_write_cache(info->rq, info->feature_flush ? true : false,\n \t\t\t      info->feature_fua ? true : false);\n-\tpr_info(\"blkfront: %s: %s %s %s %s %s\\n\",\n+\tpr_info(\"blkfront: %s: %s %s %s %s %s %s %s\\n\",\n \t\tinfo->gd->disk_name, flush_info(info),\n \t\t\"persistent grants:\", info->feature_persistent ?\n \t\t\"enabled;\" : \"disabled;\", \"indirect descriptors:\",\n-\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\");\n+\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\",\n+\t\t\"bounce buffer:\", info->bounce ? \"enabled\" : \"disabled;\");\n }",
        "function_modified_lines": {
            "added": [
                "\tpr_info(\"blkfront: %s: %s %s %s %s %s %s %s\\n\",",
                "\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\",",
                "\t\t\"bounce buffer:\", info->bounce ? \"enabled\" : \"disabled;\");"
            ],
            "deleted": [
                "\tpr_info(\"blkfront: %s: %s %s %s %s %s\\n\",",
                "\t\tinfo->max_indirect_segments ? \"enabled;\" : \"disabled;\");"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3580
    },
    {
        "cve_id": "CVE-2015-8553",
        "code_before_change": "int xen_pcibk_enable_msix(struct xen_pcibk_device *pdev,\n\t\t\t  struct pci_dev *dev, struct xen_pci_op *op)\n{\n\tstruct xen_pcibk_dev_data *dev_data;\n\tint i, result;\n\tstruct msix_entry *entries;\n\n\tif (unlikely(verbose_request))\n\t\tprintk(KERN_DEBUG DRV_NAME \": %s: enable MSI-X\\n\",\n\t\t       pci_name(dev));\n\n\tif (op->value > SH_INFO_MAX_VEC)\n\t\treturn -EINVAL;\n\n\tif (dev->msix_enabled)\n\t\treturn -EALREADY;\n\n\tif (dev->msi_enabled)\n\t\treturn -ENXIO;\n\n\tentries = kmalloc(op->value * sizeof(*entries), GFP_KERNEL);\n\tif (entries == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < op->value; i++) {\n\t\tentries[i].entry = op->msix_entries[i].entry;\n\t\tentries[i].vector = op->msix_entries[i].vector;\n\t}\n\n\tresult = pci_enable_msix_exact(dev, entries, op->value);\n\tif (result == 0) {\n\t\tfor (i = 0; i < op->value; i++) {\n\t\t\top->msix_entries[i].entry = entries[i].entry;\n\t\t\tif (entries[i].vector) {\n\t\t\t\top->msix_entries[i].vector =\n\t\t\t\t\txen_pirq_from_irq(entries[i].vector);\n\t\t\t\tif (unlikely(verbose_request))\n\t\t\t\t\tprintk(KERN_DEBUG DRV_NAME \": %s: \" \\\n\t\t\t\t\t\t\"MSI-X[%d]: %d\\n\",\n\t\t\t\t\t\tpci_name(dev), i,\n\t\t\t\t\t\top->msix_entries[i].vector);\n\t\t\t}\n\t\t}\n\t} else\n\t\tpr_warn_ratelimited(\"%s: error enabling MSI-X for guest %u: err %d!\\n\",\n\t\t\t\t    pci_name(dev), pdev->xdev->otherend_id,\n\t\t\t\t    result);\n\tkfree(entries);\n\n\top->value = result;\n\tdev_data = pci_get_drvdata(dev);\n\tif (dev_data)\n\t\tdev_data->ack_intr = 0;\n\n\treturn result > 0 ? 0 : result;\n}",
        "code_after_change": "int xen_pcibk_enable_msix(struct xen_pcibk_device *pdev,\n\t\t\t  struct pci_dev *dev, struct xen_pci_op *op)\n{\n\tstruct xen_pcibk_dev_data *dev_data;\n\tint i, result;\n\tstruct msix_entry *entries;\n\tu16 cmd;\n\n\tif (unlikely(verbose_request))\n\t\tprintk(KERN_DEBUG DRV_NAME \": %s: enable MSI-X\\n\",\n\t\t       pci_name(dev));\n\n\tif (op->value > SH_INFO_MAX_VEC)\n\t\treturn -EINVAL;\n\n\tif (dev->msix_enabled)\n\t\treturn -EALREADY;\n\n\t/*\n\t * PCI_COMMAND_MEMORY must be enabled, otherwise we may not be able\n\t * to access the BARs where the MSI-X entries reside.\n\t */\n\tpci_read_config_word(dev, PCI_COMMAND, &cmd);\n\tif (dev->msi_enabled || !(cmd & PCI_COMMAND_MEMORY))\n\t\treturn -ENXIO;\n\n\tentries = kmalloc(op->value * sizeof(*entries), GFP_KERNEL);\n\tif (entries == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < op->value; i++) {\n\t\tentries[i].entry = op->msix_entries[i].entry;\n\t\tentries[i].vector = op->msix_entries[i].vector;\n\t}\n\n\tresult = pci_enable_msix_exact(dev, entries, op->value);\n\tif (result == 0) {\n\t\tfor (i = 0; i < op->value; i++) {\n\t\t\top->msix_entries[i].entry = entries[i].entry;\n\t\t\tif (entries[i].vector) {\n\t\t\t\top->msix_entries[i].vector =\n\t\t\t\t\txen_pirq_from_irq(entries[i].vector);\n\t\t\t\tif (unlikely(verbose_request))\n\t\t\t\t\tprintk(KERN_DEBUG DRV_NAME \": %s: \" \\\n\t\t\t\t\t\t\"MSI-X[%d]: %d\\n\",\n\t\t\t\t\t\tpci_name(dev), i,\n\t\t\t\t\t\top->msix_entries[i].vector);\n\t\t\t}\n\t\t}\n\t} else\n\t\tpr_warn_ratelimited(\"%s: error enabling MSI-X for guest %u: err %d!\\n\",\n\t\t\t\t    pci_name(dev), pdev->xdev->otherend_id,\n\t\t\t\t    result);\n\tkfree(entries);\n\n\top->value = result;\n\tdev_data = pci_get_drvdata(dev);\n\tif (dev_data)\n\t\tdev_data->ack_intr = 0;\n\n\treturn result > 0 ? 0 : result;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \tstruct xen_pcibk_dev_data *dev_data;\n \tint i, result;\n \tstruct msix_entry *entries;\n+\tu16 cmd;\n \n \tif (unlikely(verbose_request))\n \t\tprintk(KERN_DEBUG DRV_NAME \": %s: enable MSI-X\\n\",\n@@ -15,7 +16,12 @@\n \tif (dev->msix_enabled)\n \t\treturn -EALREADY;\n \n-\tif (dev->msi_enabled)\n+\t/*\n+\t * PCI_COMMAND_MEMORY must be enabled, otherwise we may not be able\n+\t * to access the BARs where the MSI-X entries reside.\n+\t */\n+\tpci_read_config_word(dev, PCI_COMMAND, &cmd);\n+\tif (dev->msi_enabled || !(cmd & PCI_COMMAND_MEMORY))\n \t\treturn -ENXIO;\n \n \tentries = kmalloc(op->value * sizeof(*entries), GFP_KERNEL);",
        "function_modified_lines": {
            "added": [
                "\tu16 cmd;",
                "\t/*",
                "\t * PCI_COMMAND_MEMORY must be enabled, otherwise we may not be able",
                "\t * to access the BARs where the MSI-X entries reside.",
                "\t */",
                "\tpci_read_config_word(dev, PCI_COMMAND, &cmd);",
                "\tif (dev->msi_enabled || !(cmd & PCI_COMMAND_MEMORY))"
            ],
            "deleted": [
                "\tif (dev->msi_enabled)"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Xen allows guest OS users to obtain sensitive information from uninitialized locations in host OS kernel memory by not enabling memory and I/O decoding control bits.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2015-0777.",
        "id": 831
    },
    {
        "cve_id": "CVE-2018-20509",
        "code_before_change": "static int binder_dec_ref(struct binder_ref *ref, int strong)\n{\n\tif (strong) {\n\t\tif (ref->strong == 0) {\n\t\t\tbinder_user_error(\"%d invalid dec strong, ref %d desc %d s %d w %d\\n\",\n\t\t\t\t\t  ref->proc->pid, ref->debug_id,\n\t\t\t\t\t  ref->desc, ref->strong, ref->weak);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tref->strong--;\n\t\tif (ref->strong == 0) {\n\t\t\tint ret;\n\n\t\t\tret = binder_dec_node(ref->node, strong, 1);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t} else {\n\t\tif (ref->weak == 0) {\n\t\t\tbinder_user_error(\"%d invalid dec weak, ref %d desc %d s %d w %d\\n\",\n\t\t\t\t\t  ref->proc->pid, ref->debug_id,\n\t\t\t\t\t  ref->desc, ref->strong, ref->weak);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tref->weak--;\n\t}\n\tif (ref->strong == 0 && ref->weak == 0)\n\t\tbinder_delete_ref(ref);\n\treturn 0;\n}",
        "code_after_change": "static bool binder_dec_ref(struct binder_ref *ref, int strong)\n{\n\tif (strong) {\n\t\tif (ref->data.strong == 0) {\n\t\t\tbinder_user_error(\"%d invalid dec strong, ref %d desc %d s %d w %d\\n\",\n\t\t\t\t\t  ref->proc->pid, ref->data.debug_id,\n\t\t\t\t\t  ref->data.desc, ref->data.strong,\n\t\t\t\t\t  ref->data.weak);\n\t\t\treturn false;\n\t\t}\n\t\tref->data.strong--;\n\t\tif (ref->data.strong == 0) {\n\t\t\tint ret;\n\n\t\t\tret = binder_dec_node(ref->node, strong, 1);\n\t\t\tif (ret)\n\t\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tif (ref->data.weak == 0) {\n\t\t\tbinder_user_error(\"%d invalid dec weak, ref %d desc %d s %d w %d\\n\",\n\t\t\t\t\t  ref->proc->pid, ref->data.debug_id,\n\t\t\t\t\t  ref->data.desc, ref->data.strong,\n\t\t\t\t\t  ref->data.weak);\n\t\t\treturn false;\n\t\t}\n\t\tref->data.weak--;\n\t}\n\tif (ref->data.strong == 0 && ref->data.weak == 0) {\n\t\tbinder_cleanup_ref(ref);\n\t\t/*\n\t\t * TODO: we could kfree(ref) here, but an upcoming\n\t\t * patch will call this with a lock held, so we\n\t\t * return an indication that the ref should be\n\t\t * freed.\n\t\t */\n\t\treturn true;\n\t}\n\treturn false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,30 +1,40 @@\n-static int binder_dec_ref(struct binder_ref *ref, int strong)\n+static bool binder_dec_ref(struct binder_ref *ref, int strong)\n {\n \tif (strong) {\n-\t\tif (ref->strong == 0) {\n+\t\tif (ref->data.strong == 0) {\n \t\t\tbinder_user_error(\"%d invalid dec strong, ref %d desc %d s %d w %d\\n\",\n-\t\t\t\t\t  ref->proc->pid, ref->debug_id,\n-\t\t\t\t\t  ref->desc, ref->strong, ref->weak);\n-\t\t\treturn -EINVAL;\n+\t\t\t\t\t  ref->proc->pid, ref->data.debug_id,\n+\t\t\t\t\t  ref->data.desc, ref->data.strong,\n+\t\t\t\t\t  ref->data.weak);\n+\t\t\treturn false;\n \t\t}\n-\t\tref->strong--;\n-\t\tif (ref->strong == 0) {\n+\t\tref->data.strong--;\n+\t\tif (ref->data.strong == 0) {\n \t\t\tint ret;\n \n \t\t\tret = binder_dec_node(ref->node, strong, 1);\n \t\t\tif (ret)\n-\t\t\t\treturn ret;\n+\t\t\t\treturn false;\n \t\t}\n \t} else {\n-\t\tif (ref->weak == 0) {\n+\t\tif (ref->data.weak == 0) {\n \t\t\tbinder_user_error(\"%d invalid dec weak, ref %d desc %d s %d w %d\\n\",\n-\t\t\t\t\t  ref->proc->pid, ref->debug_id,\n-\t\t\t\t\t  ref->desc, ref->strong, ref->weak);\n-\t\t\treturn -EINVAL;\n+\t\t\t\t\t  ref->proc->pid, ref->data.debug_id,\n+\t\t\t\t\t  ref->data.desc, ref->data.strong,\n+\t\t\t\t\t  ref->data.weak);\n+\t\t\treturn false;\n \t\t}\n-\t\tref->weak--;\n+\t\tref->data.weak--;\n \t}\n-\tif (ref->strong == 0 && ref->weak == 0)\n-\t\tbinder_delete_ref(ref);\n-\treturn 0;\n+\tif (ref->data.strong == 0 && ref->data.weak == 0) {\n+\t\tbinder_cleanup_ref(ref);\n+\t\t/*\n+\t\t * TODO: we could kfree(ref) here, but an upcoming\n+\t\t * patch will call this with a lock held, so we\n+\t\t * return an indication that the ref should be\n+\t\t * freed.\n+\t\t */\n+\t\treturn true;\n+\t}\n+\treturn false;\n }",
        "function_modified_lines": {
            "added": [
                "static bool binder_dec_ref(struct binder_ref *ref, int strong)",
                "\t\tif (ref->data.strong == 0) {",
                "\t\t\t\t\t  ref->proc->pid, ref->data.debug_id,",
                "\t\t\t\t\t  ref->data.desc, ref->data.strong,",
                "\t\t\t\t\t  ref->data.weak);",
                "\t\t\treturn false;",
                "\t\tref->data.strong--;",
                "\t\tif (ref->data.strong == 0) {",
                "\t\t\t\treturn false;",
                "\t\tif (ref->data.weak == 0) {",
                "\t\t\t\t\t  ref->proc->pid, ref->data.debug_id,",
                "\t\t\t\t\t  ref->data.desc, ref->data.strong,",
                "\t\t\t\t\t  ref->data.weak);",
                "\t\t\treturn false;",
                "\t\tref->data.weak--;",
                "\tif (ref->data.strong == 0 && ref->data.weak == 0) {",
                "\t\tbinder_cleanup_ref(ref);",
                "\t\t/*",
                "\t\t * TODO: we could kfree(ref) here, but an upcoming",
                "\t\t * patch will call this with a lock held, so we",
                "\t\t * return an indication that the ref should be",
                "\t\t * freed.",
                "\t\t */",
                "\t\treturn true;",
                "\t}",
                "\treturn false;"
            ],
            "deleted": [
                "static int binder_dec_ref(struct binder_ref *ref, int strong)",
                "\t\tif (ref->strong == 0) {",
                "\t\t\t\t\t  ref->proc->pid, ref->debug_id,",
                "\t\t\t\t\t  ref->desc, ref->strong, ref->weak);",
                "\t\t\treturn -EINVAL;",
                "\t\tref->strong--;",
                "\t\tif (ref->strong == 0) {",
                "\t\t\t\treturn ret;",
                "\t\tif (ref->weak == 0) {",
                "\t\t\t\t\t  ref->proc->pid, ref->debug_id,",
                "\t\t\t\t\t  ref->desc, ref->strong, ref->weak);",
                "\t\t\treturn -EINVAL;",
                "\t\tref->weak--;",
                "\tif (ref->strong == 0 && ref->weak == 0)",
                "\t\tbinder_delete_ref(ref);",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_ref_olocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \" ref *desc *node\" lines in a debugfs file.",
        "id": 1764
    },
    {
        "cve_id": "CVE-2013-7281",
        "code_before_change": "static int l2tp_ip_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t\t   size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}",
        "code_after_change": "static int l2tp_ip_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t\t   size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t*addr_len = sizeof(*sin);\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,9 +9,6 @@\n \n \tif (flags & MSG_OOB)\n \t\tgoto out;\n-\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(*sin);\n \n \tskb = skb_recv_datagram(sk, flags, noblock, &err);\n \tif (!skb)\n@@ -35,6 +32,7 @@\n \t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n \t\tsin->sin_port = 0;\n \t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n+\t\t*addr_len = sizeof(*sin);\n \t}\n \tif (inet->cmsg_flags)\n \t\tip_cmsg_recv(msg, skb);",
        "function_modified_lines": {
            "added": [
                "\t\t*addr_len = sizeof(*sin);"
            ],
            "deleted": [
                "",
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(*sin);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The dgram_recvmsg function in net/ieee802154/dgram.c in the Linux kernel before 3.12.4 updates a certain length value without ensuring that an associated data structure has been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 412
    },
    {
        "cve_id": "CVE-2012-6545",
        "code_before_change": "static int rfcomm_sock_getname(struct socket *sock, struct sockaddr *addr, int *len, int peer)\n{\n\tstruct sockaddr_rc *sa = (struct sockaddr_rc *) addr;\n\tstruct sock *sk = sock->sk;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\tsa->rc_family  = AF_BLUETOOTH;\n\tsa->rc_channel = rfcomm_pi(sk)->channel;\n\tif (peer)\n\t\tbacpy(&sa->rc_bdaddr, &bt_sk(sk)->dst);\n\telse\n\t\tbacpy(&sa->rc_bdaddr, &bt_sk(sk)->src);\n\n\t*len = sizeof(struct sockaddr_rc);\n\treturn 0;\n}",
        "code_after_change": "static int rfcomm_sock_getname(struct socket *sock, struct sockaddr *addr, int *len, int peer)\n{\n\tstruct sockaddr_rc *sa = (struct sockaddr_rc *) addr;\n\tstruct sock *sk = sock->sk;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\tmemset(sa, 0, sizeof(*sa));\n\tsa->rc_family  = AF_BLUETOOTH;\n\tsa->rc_channel = rfcomm_pi(sk)->channel;\n\tif (peer)\n\t\tbacpy(&sa->rc_bdaddr, &bt_sk(sk)->dst);\n\telse\n\t\tbacpy(&sa->rc_bdaddr, &bt_sk(sk)->src);\n\n\t*len = sizeof(struct sockaddr_rc);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \n \tBT_DBG(\"sock %p, sk %p\", sock, sk);\n \n+\tmemset(sa, 0, sizeof(*sa));\n \tsa->rc_family  = AF_BLUETOOTH;\n \tsa->rc_channel = rfcomm_pi(sk)->channel;\n \tif (peer)",
        "function_modified_lines": {
            "added": [
                "\tmemset(sa, 0, sizeof(*sa));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The Bluetooth RFCOMM implementation in the Linux kernel before 3.6 does not properly initialize certain structures, which allows local users to obtain sensitive information from kernel memory via a crafted application.",
        "id": 129
    },
    {
        "cve_id": "CVE-2017-7495",
        "code_before_change": "static int ext4_write_end(struct file *file,\n\t\t\t  struct address_space *mapping,\n\t\t\t  loff_t pos, unsigned len, unsigned copied,\n\t\t\t  struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tloff_t old_size = inode->i_size;\n\tint ret = 0, ret2;\n\tint i_size_changed = 0;\n\n\ttrace_ext4_write_end(inode, pos, len, copied);\n\tif (ext4_test_inode_state(inode, EXT4_STATE_ORDERED_MODE)) {\n\t\tret = ext4_jbd2_file_inode(handle, inode);\n\t\tif (ret) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\tif (ext4_has_inline_data(inode)) {\n\t\tret = ext4_write_inline_data_end(inode, pos, len,\n\t\t\t\t\t\t copied, page);\n\t\tif (ret < 0)\n\t\t\tgoto errout;\n\t\tcopied = ret;\n\t} else\n\t\tcopied = block_write_end(file, mapping, pos,\n\t\t\t\t\t len, copied, page, fsdata);\n\t/*\n\t * it's important to update i_size while still holding page lock:\n\t * page writeout could otherwise come in and zero beyond i_size.\n\t */\n\ti_size_changed = ext4_update_inode_size(inode, pos + copied);\n\tunlock_page(page);\n\tput_page(page);\n\n\tif (old_size < pos)\n\t\tpagecache_isize_extended(inode, old_size, pos);\n\t/*\n\t * Don't mark the inode dirty under page lock. First, it unnecessarily\n\t * makes the holding time of page lock longer. Second, it forces lock\n\t * ordering of page lock and transaction start for journaling\n\t * filesystems.\n\t */\n\tif (i_size_changed)\n\t\text4_mark_inode_dirty(handle, inode);\n\n\tif (pos + len > inode->i_size && ext4_can_truncate(inode))\n\t\t/* if we have allocated more blocks and copied\n\t\t * less. We will have blocks allocated outside\n\t\t * inode->i_size. So truncate them\n\t\t */\n\t\text4_orphan_add(handle, inode);\nerrout:\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\tif (pos + len > inode->i_size) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\treturn ret ? ret : copied;\n}",
        "code_after_change": "static int ext4_write_end(struct file *file,\n\t\t\t  struct address_space *mapping,\n\t\t\t  loff_t pos, unsigned len, unsigned copied,\n\t\t\t  struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tloff_t old_size = inode->i_size;\n\tint ret = 0, ret2;\n\tint i_size_changed = 0;\n\n\ttrace_ext4_write_end(inode, pos, len, copied);\n\tif (ext4_has_inline_data(inode)) {\n\t\tret = ext4_write_inline_data_end(inode, pos, len,\n\t\t\t\t\t\t copied, page);\n\t\tif (ret < 0)\n\t\t\tgoto errout;\n\t\tcopied = ret;\n\t} else\n\t\tcopied = block_write_end(file, mapping, pos,\n\t\t\t\t\t len, copied, page, fsdata);\n\t/*\n\t * it's important to update i_size while still holding page lock:\n\t * page writeout could otherwise come in and zero beyond i_size.\n\t */\n\ti_size_changed = ext4_update_inode_size(inode, pos + copied);\n\tunlock_page(page);\n\tput_page(page);\n\n\tif (old_size < pos)\n\t\tpagecache_isize_extended(inode, old_size, pos);\n\t/*\n\t * Don't mark the inode dirty under page lock. First, it unnecessarily\n\t * makes the holding time of page lock longer. Second, it forces lock\n\t * ordering of page lock and transaction start for journaling\n\t * filesystems.\n\t */\n\tif (i_size_changed)\n\t\text4_mark_inode_dirty(handle, inode);\n\n\tif (pos + len > inode->i_size && ext4_can_truncate(inode))\n\t\t/* if we have allocated more blocks and copied\n\t\t * less. We will have blocks allocated outside\n\t\t * inode->i_size. So truncate them\n\t\t */\n\t\text4_orphan_add(handle, inode);\nerrout:\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\tif (pos + len > inode->i_size) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\treturn ret ? ret : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,15 +10,6 @@\n \tint i_size_changed = 0;\n \n \ttrace_ext4_write_end(inode, pos, len, copied);\n-\tif (ext4_test_inode_state(inode, EXT4_STATE_ORDERED_MODE)) {\n-\t\tret = ext4_jbd2_file_inode(handle, inode);\n-\t\tif (ret) {\n-\t\t\tunlock_page(page);\n-\t\t\tput_page(page);\n-\t\t\tgoto errout;\n-\t\t}\n-\t}\n-\n \tif (ext4_has_inline_data(inode)) {\n \t\tret = ext4_write_inline_data_end(inode, pos, len,\n \t\t\t\t\t\t copied, page);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tif (ext4_test_inode_state(inode, EXT4_STATE_ORDERED_MODE)) {",
                "\t\tret = ext4_jbd2_file_inode(handle, inode);",
                "\t\tif (ret) {",
                "\t\t\tunlock_page(page);",
                "\t\t\tput_page(page);",
                "\t\t\tgoto errout;",
                "\t\t}",
                "\t}",
                ""
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "fs/ext4/inode.c in the Linux kernel before 4.6.2, when ext4 data=ordered mode is used, mishandles a needs-flushing-before-commit list, which allows local users to obtain sensitive information from other users' files in opportunistic circumstances by waiting for a hardware reset, creating a new file, making write system calls, and reading this file.",
        "id": 1509
    },
    {
        "cve_id": "CVE-2017-14991",
        "code_before_change": "static long\nsg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)\n{\n\tvoid __user *p = (void __user *)arg;\n\tint __user *ip = p;\n\tint result, val, read_only;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tunsigned long iflags;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t   \"sg_ioctl: cmd=0x%x\\n\", (int) cmd_in));\n\tread_only = (O_RDWR != (filp->f_flags & O_ACCMODE));\n\n\tswitch (cmd_in) {\n\tcase SG_IO:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (!scsi_block_when_processing_errors(sdp->device))\n\t\t\treturn -ENXIO;\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_IO_HDR))\n\t\t\treturn -EFAULT;\n\t\tresult = sg_new_write(sfp, filp, p, SZ_SG_IO_HDR,\n\t\t\t\t 1, read_only, 1, &srp);\n\t\tif (result < 0)\n\t\t\treturn result;\n\t\tresult = wait_event_interruptible(sfp->read_wait,\n\t\t\t(srp_done(sfp, srp) || atomic_read(&sdp->detaching)));\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\twrite_lock_irq(&sfp->rq_list_lock);\n\t\tif (srp->done) {\n\t\t\tsrp->done = 2;\n\t\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\t\tresult = sg_new_read(sfp, p, SZ_SG_IO_HDR, srp);\n\t\t\treturn (result < 0) ? result : 0;\n\t\t}\n\t\tsrp->orphan = 1;\n\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\treturn result;\t/* -ERESTARTSYS because signal hit process */\n\tcase SG_SET_TIMEOUT:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val < 0)\n\t\t\treturn -EIO;\n\t\tif (val >= mult_frac((s64)INT_MAX, USER_HZ, HZ))\n\t\t\tval = min_t(s64, mult_frac((s64)INT_MAX, USER_HZ, HZ),\n\t\t\t\t    INT_MAX);\n\t\tsfp->timeout_user = val;\n\t\tsfp->timeout = mult_frac(val, HZ, USER_HZ);\n\n\t\treturn 0;\n\tcase SG_GET_TIMEOUT:\t/* N.B. User receives timeout as return value */\n\t\t\t\t/* strange ..., for backward compatibility */\n\t\treturn sfp->timeout_user;\n\tcase SG_SET_FORCE_LOW_DMA:\n\t\t/*\n\t\t * N.B. This ioctl never worked properly, but failed to\n\t\t * return an error value. So returning '0' to keep compability\n\t\t * with legacy applications.\n\t\t */\n\t\treturn 0;\n\tcase SG_GET_LOW_DMA:\n\t\treturn put_user((int) sdp->device->host->unchecked_isa_dma, ip);\n\tcase SG_GET_SCSI_ID:\n\t\tif (!access_ok(VERIFY_WRITE, p, sizeof (sg_scsi_id_t)))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_scsi_id_t __user *sg_idp = p;\n\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t\t__put_user((int) sdp->device->host->host_no,\n\t\t\t\t   &sg_idp->host_no);\n\t\t\t__put_user((int) sdp->device->channel,\n\t\t\t\t   &sg_idp->channel);\n\t\t\t__put_user((int) sdp->device->id, &sg_idp->scsi_id);\n\t\t\t__put_user((int) sdp->device->lun, &sg_idp->lun);\n\t\t\t__put_user((int) sdp->device->type, &sg_idp->scsi_type);\n\t\t\t__put_user((short) sdp->device->host->cmd_per_lun,\n\t\t\t\t   &sg_idp->h_cmd_per_lun);\n\t\t\t__put_user((short) sdp->device->queue_depth,\n\t\t\t\t   &sg_idp->d_queue_depth);\n\t\t\t__put_user(0, &sg_idp->unused[0]);\n\t\t\t__put_user(0, &sg_idp->unused[1]);\n\t\t\treturn 0;\n\t\t}\n\tcase SG_SET_FORCE_PACK_ID:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->force_packid = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_PACK_ID:\n\t\tif (!access_ok(VERIFY_WRITE, ip, sizeof (int)))\n\t\t\treturn -EFAULT;\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned)) {\n\t\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock,\n\t\t\t\t\t\t       iflags);\n\t\t\t\t__put_user(srp->header.pack_id, ip);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t__put_user(-1, ip);\n\t\treturn 0;\n\tcase SG_GET_NUM_WAITING:\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tval = 0;\n\t\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned))\n\t\t\t\t++val;\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_SG_TABLESIZE:\n\t\treturn put_user(sdp->sg_tablesize, ip);\n\tcase SG_SET_RESERVED_SIZE:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n                if (val < 0)\n                        return -EINVAL;\n\t\tval = min_t(int, val,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\tmutex_lock(&sfp->f_mutex);\n\t\tif (val != sfp->reserve.bufflen) {\n\t\t\tif (sfp->mmap_called ||\n\t\t\t    sfp->res_in_use) {\n\t\t\t\tmutex_unlock(&sfp->f_mutex);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\n\t\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t\t\tsg_build_reserve(sfp, val);\n\t\t}\n\t\tmutex_unlock(&sfp->f_mutex);\n\t\treturn 0;\n\tcase SG_GET_RESERVED_SIZE:\n\t\tval = min_t(int, sfp->reserve.bufflen,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\treturn put_user(val, ip);\n\tcase SG_SET_COMMAND_Q:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->cmd_q = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_COMMAND_Q:\n\t\treturn put_user((int) sfp->cmd_q, ip);\n\tcase SG_SET_KEEP_ORPHAN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->keep_orphan = val;\n\t\treturn 0;\n\tcase SG_GET_KEEP_ORPHAN:\n\t\treturn put_user((int) sfp->keep_orphan, ip);\n\tcase SG_NEXT_CMD_LEN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val > SG_MAX_CDB_SIZE)\n\t\t\treturn -ENOMEM;\n\t\tsfp->next_cmd_len = (val > 0) ? val : 0;\n\t\treturn 0;\n\tcase SG_GET_VERSION_NUM:\n\t\treturn put_user(sg_version_num, ip);\n\tcase SG_GET_ACCESS_COUNT:\n\t\t/* faked - we don't have a real access count anymore */\n\t\tval = (sdp->device ? 1 : 0);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_REQUEST_TABLE:\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_REQ_INFO * SG_MAX_QUEUE))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_req_info_t *rinfo;\n\n\t\t\trinfo = kmalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!rinfo)\n\t\t\t\treturn -ENOMEM;\n\t\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\t\tsg_fill_request_table(sfp, rinfo);\n\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t\tresult = __copy_to_user(p, rinfo,\n\t\t\t\t\t\tSZ_SG_REQ_INFO * SG_MAX_QUEUE);\n\t\t\tresult = result ? -EFAULT : 0;\n\t\t\tkfree(rinfo);\n\t\t\treturn result;\n\t\t}\n\tcase SG_EMULATED_HOST:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\treturn put_user(sdp->device->host->hostt->emulated, ip);\n\tcase SCSI_IOCTL_SEND_COMMAND:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (read_only) {\n\t\t\tunsigned char opcode = WRITE_6;\n\t\t\tScsi_Ioctl_Command __user *siocp = p;\n\n\t\t\tif (copy_from_user(&opcode, siocp->data, 1))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (sg_allow_access(filp, &opcode))\n\t\t\t\treturn -EPERM;\n\t\t}\n\t\treturn sg_scsi_ioctl(sdp->device->request_queue, NULL, filp->f_mode, p);\n\tcase SG_SET_DEBUG:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsdp->sgdebug = (char) val;\n\t\treturn 0;\n\tcase BLKSECTGET:\n\t\treturn put_user(max_sectors_bytes(sdp->device->request_queue),\n\t\t\t\tip);\n\tcase BLKTRACESETUP:\n\t\treturn blk_trace_setup(sdp->device->request_queue,\n\t\t\t\t       sdp->disk->disk_name,\n\t\t\t\t       MKDEV(SCSI_GENERIC_MAJOR, sdp->index),\n\t\t\t\t       NULL, p);\n\tcase BLKTRACESTART:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 1);\n\tcase BLKTRACESTOP:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 0);\n\tcase BLKTRACETEARDOWN:\n\t\treturn blk_trace_remove(sdp->device->request_queue);\n\tcase SCSI_IOCTL_GET_IDLUN:\n\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\tcase SCSI_IOCTL_PROBE_HOST:\n\tcase SG_GET_TRANSFORM:\n\tcase SG_SCSI_RESET:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tbreak;\n\tdefault:\n\t\tif (read_only)\n\t\t\treturn -EPERM;\t/* don't know so take safe approach */\n\t\tbreak;\n\t}\n\n\tresult = scsi_ioctl_block_when_processing_errors(sdp->device,\n\t\t\tcmd_in, filp->f_flags & O_NDELAY);\n\tif (result)\n\t\treturn result;\n\treturn scsi_ioctl(sdp->device, cmd_in, p);\n}",
        "code_after_change": "static long\nsg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)\n{\n\tvoid __user *p = (void __user *)arg;\n\tint __user *ip = p;\n\tint result, val, read_only;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tunsigned long iflags;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t   \"sg_ioctl: cmd=0x%x\\n\", (int) cmd_in));\n\tread_only = (O_RDWR != (filp->f_flags & O_ACCMODE));\n\n\tswitch (cmd_in) {\n\tcase SG_IO:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (!scsi_block_when_processing_errors(sdp->device))\n\t\t\treturn -ENXIO;\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_IO_HDR))\n\t\t\treturn -EFAULT;\n\t\tresult = sg_new_write(sfp, filp, p, SZ_SG_IO_HDR,\n\t\t\t\t 1, read_only, 1, &srp);\n\t\tif (result < 0)\n\t\t\treturn result;\n\t\tresult = wait_event_interruptible(sfp->read_wait,\n\t\t\t(srp_done(sfp, srp) || atomic_read(&sdp->detaching)));\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\twrite_lock_irq(&sfp->rq_list_lock);\n\t\tif (srp->done) {\n\t\t\tsrp->done = 2;\n\t\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\t\tresult = sg_new_read(sfp, p, SZ_SG_IO_HDR, srp);\n\t\t\treturn (result < 0) ? result : 0;\n\t\t}\n\t\tsrp->orphan = 1;\n\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\treturn result;\t/* -ERESTARTSYS because signal hit process */\n\tcase SG_SET_TIMEOUT:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val < 0)\n\t\t\treturn -EIO;\n\t\tif (val >= mult_frac((s64)INT_MAX, USER_HZ, HZ))\n\t\t\tval = min_t(s64, mult_frac((s64)INT_MAX, USER_HZ, HZ),\n\t\t\t\t    INT_MAX);\n\t\tsfp->timeout_user = val;\n\t\tsfp->timeout = mult_frac(val, HZ, USER_HZ);\n\n\t\treturn 0;\n\tcase SG_GET_TIMEOUT:\t/* N.B. User receives timeout as return value */\n\t\t\t\t/* strange ..., for backward compatibility */\n\t\treturn sfp->timeout_user;\n\tcase SG_SET_FORCE_LOW_DMA:\n\t\t/*\n\t\t * N.B. This ioctl never worked properly, but failed to\n\t\t * return an error value. So returning '0' to keep compability\n\t\t * with legacy applications.\n\t\t */\n\t\treturn 0;\n\tcase SG_GET_LOW_DMA:\n\t\treturn put_user((int) sdp->device->host->unchecked_isa_dma, ip);\n\tcase SG_GET_SCSI_ID:\n\t\tif (!access_ok(VERIFY_WRITE, p, sizeof (sg_scsi_id_t)))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_scsi_id_t __user *sg_idp = p;\n\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t\t__put_user((int) sdp->device->host->host_no,\n\t\t\t\t   &sg_idp->host_no);\n\t\t\t__put_user((int) sdp->device->channel,\n\t\t\t\t   &sg_idp->channel);\n\t\t\t__put_user((int) sdp->device->id, &sg_idp->scsi_id);\n\t\t\t__put_user((int) sdp->device->lun, &sg_idp->lun);\n\t\t\t__put_user((int) sdp->device->type, &sg_idp->scsi_type);\n\t\t\t__put_user((short) sdp->device->host->cmd_per_lun,\n\t\t\t\t   &sg_idp->h_cmd_per_lun);\n\t\t\t__put_user((short) sdp->device->queue_depth,\n\t\t\t\t   &sg_idp->d_queue_depth);\n\t\t\t__put_user(0, &sg_idp->unused[0]);\n\t\t\t__put_user(0, &sg_idp->unused[1]);\n\t\t\treturn 0;\n\t\t}\n\tcase SG_SET_FORCE_PACK_ID:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->force_packid = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_PACK_ID:\n\t\tif (!access_ok(VERIFY_WRITE, ip, sizeof (int)))\n\t\t\treturn -EFAULT;\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned)) {\n\t\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock,\n\t\t\t\t\t\t       iflags);\n\t\t\t\t__put_user(srp->header.pack_id, ip);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t__put_user(-1, ip);\n\t\treturn 0;\n\tcase SG_GET_NUM_WAITING:\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tval = 0;\n\t\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned))\n\t\t\t\t++val;\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_SG_TABLESIZE:\n\t\treturn put_user(sdp->sg_tablesize, ip);\n\tcase SG_SET_RESERVED_SIZE:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n                if (val < 0)\n                        return -EINVAL;\n\t\tval = min_t(int, val,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\tmutex_lock(&sfp->f_mutex);\n\t\tif (val != sfp->reserve.bufflen) {\n\t\t\tif (sfp->mmap_called ||\n\t\t\t    sfp->res_in_use) {\n\t\t\t\tmutex_unlock(&sfp->f_mutex);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\n\t\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t\t\tsg_build_reserve(sfp, val);\n\t\t}\n\t\tmutex_unlock(&sfp->f_mutex);\n\t\treturn 0;\n\tcase SG_GET_RESERVED_SIZE:\n\t\tval = min_t(int, sfp->reserve.bufflen,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\treturn put_user(val, ip);\n\tcase SG_SET_COMMAND_Q:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->cmd_q = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_COMMAND_Q:\n\t\treturn put_user((int) sfp->cmd_q, ip);\n\tcase SG_SET_KEEP_ORPHAN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->keep_orphan = val;\n\t\treturn 0;\n\tcase SG_GET_KEEP_ORPHAN:\n\t\treturn put_user((int) sfp->keep_orphan, ip);\n\tcase SG_NEXT_CMD_LEN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val > SG_MAX_CDB_SIZE)\n\t\t\treturn -ENOMEM;\n\t\tsfp->next_cmd_len = (val > 0) ? val : 0;\n\t\treturn 0;\n\tcase SG_GET_VERSION_NUM:\n\t\treturn put_user(sg_version_num, ip);\n\tcase SG_GET_ACCESS_COUNT:\n\t\t/* faked - we don't have a real access count anymore */\n\t\tval = (sdp->device ? 1 : 0);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_REQUEST_TABLE:\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_REQ_INFO * SG_MAX_QUEUE))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_req_info_t *rinfo;\n\n\t\t\trinfo = kzalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!rinfo)\n\t\t\t\treturn -ENOMEM;\n\t\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\t\tsg_fill_request_table(sfp, rinfo);\n\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t\tresult = __copy_to_user(p, rinfo,\n\t\t\t\t\t\tSZ_SG_REQ_INFO * SG_MAX_QUEUE);\n\t\t\tresult = result ? -EFAULT : 0;\n\t\t\tkfree(rinfo);\n\t\t\treturn result;\n\t\t}\n\tcase SG_EMULATED_HOST:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\treturn put_user(sdp->device->host->hostt->emulated, ip);\n\tcase SCSI_IOCTL_SEND_COMMAND:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (read_only) {\n\t\t\tunsigned char opcode = WRITE_6;\n\t\t\tScsi_Ioctl_Command __user *siocp = p;\n\n\t\t\tif (copy_from_user(&opcode, siocp->data, 1))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (sg_allow_access(filp, &opcode))\n\t\t\t\treturn -EPERM;\n\t\t}\n\t\treturn sg_scsi_ioctl(sdp->device->request_queue, NULL, filp->f_mode, p);\n\tcase SG_SET_DEBUG:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsdp->sgdebug = (char) val;\n\t\treturn 0;\n\tcase BLKSECTGET:\n\t\treturn put_user(max_sectors_bytes(sdp->device->request_queue),\n\t\t\t\tip);\n\tcase BLKTRACESETUP:\n\t\treturn blk_trace_setup(sdp->device->request_queue,\n\t\t\t\t       sdp->disk->disk_name,\n\t\t\t\t       MKDEV(SCSI_GENERIC_MAJOR, sdp->index),\n\t\t\t\t       NULL, p);\n\tcase BLKTRACESTART:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 1);\n\tcase BLKTRACESTOP:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 0);\n\tcase BLKTRACETEARDOWN:\n\t\treturn blk_trace_remove(sdp->device->request_queue);\n\tcase SCSI_IOCTL_GET_IDLUN:\n\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\tcase SCSI_IOCTL_PROBE_HOST:\n\tcase SG_GET_TRANSFORM:\n\tcase SG_SCSI_RESET:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tbreak;\n\tdefault:\n\t\tif (read_only)\n\t\t\treturn -EPERM;\t/* don't know so take safe approach */\n\t\tbreak;\n\t}\n\n\tresult = scsi_ioctl_block_when_processing_errors(sdp->device,\n\t\t\tcmd_in, filp->f_flags & O_NDELAY);\n\tif (result)\n\t\treturn result;\n\treturn scsi_ioctl(sdp->device, cmd_in, p);\n}",
        "patch": "--- code before\n+++ code after\n@@ -183,8 +183,8 @@\n \t\telse {\n \t\t\tsg_req_info_t *rinfo;\n \n-\t\t\trinfo = kmalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,\n-\t\t\t\t\t\t\t\tGFP_KERNEL);\n+\t\t\trinfo = kzalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,\n+\t\t\t\t\tGFP_KERNEL);\n \t\t\tif (!rinfo)\n \t\t\t\treturn -ENOMEM;\n \t\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);",
        "function_modified_lines": {
            "added": [
                "\t\t\trinfo = kzalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,",
                "\t\t\t\t\tGFP_KERNEL);"
            ],
            "deleted": [
                "\t\t\trinfo = kmalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,",
                "\t\t\t\t\t\t\t\tGFP_KERNEL);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The sg_ioctl function in drivers/scsi/sg.c in the Linux kernel before 4.13.4 allows local users to obtain sensitive information from uninitialized kernel heap-memory locations via an SG_GET_REQUEST_TABLE ioctl call for /dev/sg0.",
        "id": 1287
    },
    {
        "cve_id": "CVE-2013-3231",
        "code_before_change": "static int llc_ui_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sockaddr_llc *uaddr = (struct sockaddr_llc *)msg->msg_name;\n\tconst int nonblock = flags & MSG_DONTWAIT;\n\tstruct sk_buff *skb = NULL;\n\tstruct sock *sk = sock->sk;\n\tstruct llc_sock *llc = llc_sk(sk);\n\tunsigned long cpu_flags;\n\tsize_t copied = 0;\n\tu32 peek_seq = 0;\n\tu32 *seq;\n\tunsigned long used;\n\tint target;\t/* Read at least this many bytes */\n\tlong timeo;\n\n\tlock_sock(sk);\n\tcopied = -ENOTCONN;\n\tif (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\tseq = &llc->copied_seq;\n\tif (flags & MSG_PEEK) {\n\t\tpeek_seq = llc->copied_seq;\n\t\tseq = &peek_seq;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tcopied = 0;\n\n\tdo {\n\t\tu32 offset;\n\n\t\t/*\n\t\t * We need to check signals first, to get correct SIGURG\n\t\t * handling. FIXME: Need to check this doesn't impact 1003.1g\n\t\t * and move it down to the bottom of the loop\n\t\t */\n\t\tif (signal_pending(current)) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Next get a buffer. */\n\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb) {\n\t\t\toffset = *seq;\n\t\t\tgoto found_ok_skb;\n\t\t}\n\t\t/* Well, if we have backlog, try to process it now yet. */\n\n\t\tif (copied >= target && !sk->sk_backlog.tail)\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err ||\n\t\t\t    sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    !timeo ||\n\t\t\t    (flags & MSG_PEEK))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_CLOSE) {\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * This occurs when user tries to read\n\t\t\t\t\t * from never connected socket.\n\t\t\t\t\t */\n\t\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (copied >= target) { /* Do not sleep, just process backlog. */\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t} else\n\t\t\tsk_wait_data(sk, &timeo);\n\n\t\tif ((flags & MSG_PEEK) && peek_seq != llc->copied_seq) {\n\t\t\tnet_dbg_ratelimited(\"LLC(%s:%d): Application bug, race in MSG_PEEK\\n\",\n\t\t\t\t\t    current->comm,\n\t\t\t\t\t    task_pid_nr(current));\n\t\t\tpeek_seq = llc->copied_seq;\n\t\t}\n\t\tcontinue;\n\tfound_ok_skb:\n\t\t/* Ok so how much can we use? */\n\t\tused = skb->len - offset;\n\t\tif (len < used)\n\t\t\tused = len;\n\n\t\tif (!(flags & MSG_TRUNC)) {\n\t\t\tint rc = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\t\t msg->msg_iov, used);\n\t\t\tif (rc) {\n\t\t\t\t/* Exception. Bailout! */\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t*seq += used;\n\t\tcopied += used;\n\t\tlen -= used;\n\n\t\t/* For non stream protcols we get one packet per recvmsg call */\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\tgoto copy_uaddr;\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t\t}\n\n\t\t/* Partial read */\n\t\tif (used + offset < skb->len)\n\t\t\tcontinue;\n\t} while (len > 0);\n\nout:\n\trelease_sock(sk);\n\treturn copied;\ncopy_uaddr:\n\tif (uaddr != NULL && skb != NULL) {\n\t\tmemcpy(uaddr, llc_ui_skb_cb(skb), sizeof(*uaddr));\n\t\tmsg->msg_namelen = sizeof(*uaddr);\n\t}\n\tif (llc_sk(sk)->cmsg_flags)\n\t\tllc_cmsg_rcv(msg, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t}\n\n\tgoto out;\n}",
        "code_after_change": "static int llc_ui_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sockaddr_llc *uaddr = (struct sockaddr_llc *)msg->msg_name;\n\tconst int nonblock = flags & MSG_DONTWAIT;\n\tstruct sk_buff *skb = NULL;\n\tstruct sock *sk = sock->sk;\n\tstruct llc_sock *llc = llc_sk(sk);\n\tunsigned long cpu_flags;\n\tsize_t copied = 0;\n\tu32 peek_seq = 0;\n\tu32 *seq;\n\tunsigned long used;\n\tint target;\t/* Read at least this many bytes */\n\tlong timeo;\n\n\tmsg->msg_namelen = 0;\n\n\tlock_sock(sk);\n\tcopied = -ENOTCONN;\n\tif (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\tseq = &llc->copied_seq;\n\tif (flags & MSG_PEEK) {\n\t\tpeek_seq = llc->copied_seq;\n\t\tseq = &peek_seq;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tcopied = 0;\n\n\tdo {\n\t\tu32 offset;\n\n\t\t/*\n\t\t * We need to check signals first, to get correct SIGURG\n\t\t * handling. FIXME: Need to check this doesn't impact 1003.1g\n\t\t * and move it down to the bottom of the loop\n\t\t */\n\t\tif (signal_pending(current)) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Next get a buffer. */\n\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb) {\n\t\t\toffset = *seq;\n\t\t\tgoto found_ok_skb;\n\t\t}\n\t\t/* Well, if we have backlog, try to process it now yet. */\n\n\t\tif (copied >= target && !sk->sk_backlog.tail)\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err ||\n\t\t\t    sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    !timeo ||\n\t\t\t    (flags & MSG_PEEK))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_CLOSE) {\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * This occurs when user tries to read\n\t\t\t\t\t * from never connected socket.\n\t\t\t\t\t */\n\t\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (copied >= target) { /* Do not sleep, just process backlog. */\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t} else\n\t\t\tsk_wait_data(sk, &timeo);\n\n\t\tif ((flags & MSG_PEEK) && peek_seq != llc->copied_seq) {\n\t\t\tnet_dbg_ratelimited(\"LLC(%s:%d): Application bug, race in MSG_PEEK\\n\",\n\t\t\t\t\t    current->comm,\n\t\t\t\t\t    task_pid_nr(current));\n\t\t\tpeek_seq = llc->copied_seq;\n\t\t}\n\t\tcontinue;\n\tfound_ok_skb:\n\t\t/* Ok so how much can we use? */\n\t\tused = skb->len - offset;\n\t\tif (len < used)\n\t\t\tused = len;\n\n\t\tif (!(flags & MSG_TRUNC)) {\n\t\t\tint rc = skb_copy_datagram_iovec(skb, offset,\n\t\t\t\t\t\t\t msg->msg_iov, used);\n\t\t\tif (rc) {\n\t\t\t\t/* Exception. Bailout! */\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t*seq += used;\n\t\tcopied += used;\n\t\tlen -= used;\n\n\t\t/* For non stream protcols we get one packet per recvmsg call */\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\tgoto copy_uaddr;\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t\t}\n\n\t\t/* Partial read */\n\t\tif (used + offset < skb->len)\n\t\t\tcontinue;\n\t} while (len > 0);\n\nout:\n\trelease_sock(sk);\n\treturn copied;\ncopy_uaddr:\n\tif (uaddr != NULL && skb != NULL) {\n\t\tmemcpy(uaddr, llc_ui_skb_cb(skb), sizeof(*uaddr));\n\t\tmsg->msg_namelen = sizeof(*uaddr);\n\t}\n\tif (llc_sk(sk)->cmsg_flags)\n\t\tllc_cmsg_rcv(msg, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\t\tspin_lock_irqsave(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\tsk_eat_skb(sk, skb, false);\n\t\t\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, cpu_flags);\n\t\t\t*seq = 0;\n\t}\n\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,8 @@\n \tunsigned long used;\n \tint target;\t/* Read at least this many bytes */\n \tlong timeo;\n+\n+\tmsg->msg_namelen = 0;\n \n \tlock_sock(sk);\n \tcopied = -ENOTCONN;",
        "function_modified_lines": {
            "added": [
                "",
                "\tmsg->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The llc_ui_recvmsg function in net/llc/af_llc.c in the Linux kernel before 3.9-rc7 does not initialize a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 271
    },
    {
        "cve_id": "CVE-2014-8709",
        "code_before_change": "static int ieee80211_fragment(struct ieee80211_tx_data *tx,\n\t\t\t      struct sk_buff *skb, int hdrlen,\n\t\t\t      int frag_threshold)\n{\n\tstruct ieee80211_local *local = tx->local;\n\tstruct ieee80211_tx_info *info;\n\tstruct sk_buff *tmp;\n\tint per_fragm = frag_threshold - hdrlen - FCS_LEN;\n\tint pos = hdrlen + per_fragm;\n\tint rem = skb->len - hdrlen - per_fragm;\n\n\tif (WARN_ON(rem < 0))\n\t\treturn -EINVAL;\n\n\t/* first fragment was already added to queue by caller */\n\n\twhile (rem) {\n\t\tint fraglen = per_fragm;\n\n\t\tif (fraglen > rem)\n\t\t\tfraglen = rem;\n\t\trem -= fraglen;\n\t\ttmp = dev_alloc_skb(local->tx_headroom +\n\t\t\t\t    frag_threshold +\n\t\t\t\t    tx->sdata->encrypt_headroom +\n\t\t\t\t    IEEE80211_ENCRYPT_TAILROOM);\n\t\tif (!tmp)\n\t\t\treturn -ENOMEM;\n\n\t\t__skb_queue_tail(&tx->skbs, tmp);\n\n\t\tskb_reserve(tmp,\n\t\t\t    local->tx_headroom + tx->sdata->encrypt_headroom);\n\n\t\t/* copy control information */\n\t\tmemcpy(tmp->cb, skb->cb, sizeof(tmp->cb));\n\n\t\tinfo = IEEE80211_SKB_CB(tmp);\n\t\tinfo->flags &= ~(IEEE80211_TX_CTL_CLEAR_PS_FILT |\n\t\t\t\t IEEE80211_TX_CTL_FIRST_FRAGMENT);\n\n\t\tif (rem)\n\t\t\tinfo->flags |= IEEE80211_TX_CTL_MORE_FRAMES;\n\n\t\tskb_copy_queue_mapping(tmp, skb);\n\t\ttmp->priority = skb->priority;\n\t\ttmp->dev = skb->dev;\n\n\t\t/* copy header and data */\n\t\tmemcpy(skb_put(tmp, hdrlen), skb->data, hdrlen);\n\t\tmemcpy(skb_put(tmp, fraglen), skb->data + pos, fraglen);\n\n\t\tpos += fraglen;\n\t}\n\n\t/* adjust first fragment's length */\n\tskb->len = hdrlen + per_fragm;\n\treturn 0;\n}",
        "code_after_change": "static int ieee80211_fragment(struct ieee80211_tx_data *tx,\n\t\t\t      struct sk_buff *skb, int hdrlen,\n\t\t\t      int frag_threshold)\n{\n\tstruct ieee80211_local *local = tx->local;\n\tstruct ieee80211_tx_info *info;\n\tstruct sk_buff *tmp;\n\tint per_fragm = frag_threshold - hdrlen - FCS_LEN;\n\tint pos = hdrlen + per_fragm;\n\tint rem = skb->len - hdrlen - per_fragm;\n\n\tif (WARN_ON(rem < 0))\n\t\treturn -EINVAL;\n\n\t/* first fragment was already added to queue by caller */\n\n\twhile (rem) {\n\t\tint fraglen = per_fragm;\n\n\t\tif (fraglen > rem)\n\t\t\tfraglen = rem;\n\t\trem -= fraglen;\n\t\ttmp = dev_alloc_skb(local->tx_headroom +\n\t\t\t\t    frag_threshold +\n\t\t\t\t    tx->sdata->encrypt_headroom +\n\t\t\t\t    IEEE80211_ENCRYPT_TAILROOM);\n\t\tif (!tmp)\n\t\t\treturn -ENOMEM;\n\n\t\t__skb_queue_tail(&tx->skbs, tmp);\n\n\t\tskb_reserve(tmp,\n\t\t\t    local->tx_headroom + tx->sdata->encrypt_headroom);\n\n\t\t/* copy control information */\n\t\tmemcpy(tmp->cb, skb->cb, sizeof(tmp->cb));\n\n\t\tinfo = IEEE80211_SKB_CB(tmp);\n\t\tinfo->flags &= ~(IEEE80211_TX_CTL_CLEAR_PS_FILT |\n\t\t\t\t IEEE80211_TX_CTL_FIRST_FRAGMENT);\n\n\t\tif (rem)\n\t\t\tinfo->flags |= IEEE80211_TX_CTL_MORE_FRAMES;\n\n\t\tskb_copy_queue_mapping(tmp, skb);\n\t\ttmp->priority = skb->priority;\n\t\ttmp->dev = skb->dev;\n\n\t\t/* copy header and data */\n\t\tmemcpy(skb_put(tmp, hdrlen), skb->data, hdrlen);\n\t\tmemcpy(skb_put(tmp, fraglen), skb->data + pos, fraglen);\n\n\t\tpos += fraglen;\n\t}\n\n\t/* adjust first fragment's length */\n\tskb_trim(skb, hdrlen + per_fragm);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -54,6 +54,6 @@\n \t}\n \n \t/* adjust first fragment's length */\n-\tskb->len = hdrlen + per_fragm;\n+\tskb_trim(skb, hdrlen + per_fragm);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tskb_trim(skb, hdrlen + per_fragm);"
            ],
            "deleted": [
                "\tskb->len = hdrlen + per_fragm;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The ieee80211_fragment function in net/mac80211/tx.c in the Linux kernel before 3.13.5 does not properly maintain a certain tail pointer, which allows remote attackers to obtain sensitive cleartext information by reading packets.",
        "id": 675
    },
    {
        "cve_id": "CVE-2013-3235",
        "code_before_change": "static int recv_stream(struct kiocb *iocb, struct socket *sock,\n\t\t       struct msghdr *m, size_t buf_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tipc_port *tport = tipc_sk_port(sk);\n\tstruct sk_buff *buf;\n\tstruct tipc_msg *msg;\n\tlong timeout;\n\tunsigned int sz;\n\tint sz_to_copy, target, needed;\n\tint sz_copied = 0;\n\tu32 err;\n\tint res = 0;\n\n\t/* Catch invalid receive attempts */\n\tif (unlikely(!buf_len))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (unlikely((sock->state == SS_UNCONNECTED) ||\n\t\t     (sock->state == SS_CONNECTING))) {\n\t\tres = -ENOTCONN;\n\t\tgoto exit;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, buf_len);\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\nrestart:\n\t/* Look for a message in receive queue; wait if necessary */\n\twhile (skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tif (sock->state == SS_DISCONNECTING) {\n\t\t\tres = -ENOTCONN;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (timeout <= 0L) {\n\t\t\tres = timeout ? timeout : -EWOULDBLOCK;\n\t\t\tgoto exit;\n\t\t}\n\t\trelease_sock(sk);\n\t\ttimeout = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t\t\t   tipc_rx_ready(sock),\n\t\t\t\t\t\t\t   timeout);\n\t\tlock_sock(sk);\n\t}\n\n\t/* Look at first message in receive queue */\n\tbuf = skb_peek(&sk->sk_receive_queue);\n\tmsg = buf_msg(buf);\n\tsz = msg_data_sz(msg);\n\terr = msg_errcode(msg);\n\n\t/* Discard an empty non-errored message & try again */\n\tif ((!sz) && (!err)) {\n\t\tadvance_rx_queue(sk);\n\t\tgoto restart;\n\t}\n\n\t/* Optionally capture sender's address & ancillary data of first msg */\n\tif (sz_copied == 0) {\n\t\tset_orig_addr(m, msg);\n\t\tres = anc_data_recv(m, msg, tport);\n\t\tif (res)\n\t\t\tgoto exit;\n\t}\n\n\t/* Capture message data (if valid) & compute return value (always) */\n\tif (!err) {\n\t\tu32 offset = (u32)(unsigned long)(TIPC_SKB_CB(buf)->handle);\n\n\t\tsz -= offset;\n\t\tneeded = (buf_len - sz_copied);\n\t\tsz_to_copy = (sz <= needed) ? sz : needed;\n\n\t\tres = skb_copy_datagram_iovec(buf, msg_hdr_sz(msg) + offset,\n\t\t\t\t\t      m->msg_iov, sz_to_copy);\n\t\tif (res)\n\t\t\tgoto exit;\n\n\t\tsz_copied += sz_to_copy;\n\n\t\tif (sz_to_copy < sz) {\n\t\t\tif (!(flags & MSG_PEEK))\n\t\t\t\tTIPC_SKB_CB(buf)->handle =\n\t\t\t\t(void *)(unsigned long)(offset + sz_to_copy);\n\t\t\tgoto exit;\n\t\t}\n\t} else {\n\t\tif (sz_copied != 0)\n\t\t\tgoto exit; /* can't add error msg to valid data */\n\n\t\tif ((err == TIPC_CONN_SHUTDOWN) || m->msg_control)\n\t\t\tres = 0;\n\t\telse\n\t\t\tres = -ECONNRESET;\n\t}\n\n\t/* Consume received message (optional) */\n\tif (likely(!(flags & MSG_PEEK))) {\n\t\tif (unlikely(++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))\n\t\t\ttipc_acknowledge(tport->ref, tport->conn_unacked);\n\t\tadvance_rx_queue(sk);\n\t}\n\n\t/* Loop around if more data is required */\n\tif ((sz_copied < buf_len) &&\t/* didn't get all requested data */\n\t    (!skb_queue_empty(&sk->sk_receive_queue) ||\n\t    (sz_copied < target)) &&\t/* and more is ready or required */\n\t    (!(flags & MSG_PEEK)) &&\t/* and aren't just peeking at data */\n\t    (!err))\t\t\t/* and haven't reached a FIN */\n\t\tgoto restart;\n\nexit:\n\trelease_sock(sk);\n\treturn sz_copied ? sz_copied : res;\n}",
        "code_after_change": "static int recv_stream(struct kiocb *iocb, struct socket *sock,\n\t\t       struct msghdr *m, size_t buf_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tipc_port *tport = tipc_sk_port(sk);\n\tstruct sk_buff *buf;\n\tstruct tipc_msg *msg;\n\tlong timeout;\n\tunsigned int sz;\n\tint sz_to_copy, target, needed;\n\tint sz_copied = 0;\n\tu32 err;\n\tint res = 0;\n\n\t/* Catch invalid receive attempts */\n\tif (unlikely(!buf_len))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (unlikely((sock->state == SS_UNCONNECTED) ||\n\t\t     (sock->state == SS_CONNECTING))) {\n\t\tres = -ENOTCONN;\n\t\tgoto exit;\n\t}\n\n\t/* will be updated in set_orig_addr() if needed */\n\tm->msg_namelen = 0;\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, buf_len);\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\nrestart:\n\t/* Look for a message in receive queue; wait if necessary */\n\twhile (skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tif (sock->state == SS_DISCONNECTING) {\n\t\t\tres = -ENOTCONN;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (timeout <= 0L) {\n\t\t\tres = timeout ? timeout : -EWOULDBLOCK;\n\t\t\tgoto exit;\n\t\t}\n\t\trelease_sock(sk);\n\t\ttimeout = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\t\t\t   tipc_rx_ready(sock),\n\t\t\t\t\t\t\t   timeout);\n\t\tlock_sock(sk);\n\t}\n\n\t/* Look at first message in receive queue */\n\tbuf = skb_peek(&sk->sk_receive_queue);\n\tmsg = buf_msg(buf);\n\tsz = msg_data_sz(msg);\n\terr = msg_errcode(msg);\n\n\t/* Discard an empty non-errored message & try again */\n\tif ((!sz) && (!err)) {\n\t\tadvance_rx_queue(sk);\n\t\tgoto restart;\n\t}\n\n\t/* Optionally capture sender's address & ancillary data of first msg */\n\tif (sz_copied == 0) {\n\t\tset_orig_addr(m, msg);\n\t\tres = anc_data_recv(m, msg, tport);\n\t\tif (res)\n\t\t\tgoto exit;\n\t}\n\n\t/* Capture message data (if valid) & compute return value (always) */\n\tif (!err) {\n\t\tu32 offset = (u32)(unsigned long)(TIPC_SKB_CB(buf)->handle);\n\n\t\tsz -= offset;\n\t\tneeded = (buf_len - sz_copied);\n\t\tsz_to_copy = (sz <= needed) ? sz : needed;\n\n\t\tres = skb_copy_datagram_iovec(buf, msg_hdr_sz(msg) + offset,\n\t\t\t\t\t      m->msg_iov, sz_to_copy);\n\t\tif (res)\n\t\t\tgoto exit;\n\n\t\tsz_copied += sz_to_copy;\n\n\t\tif (sz_to_copy < sz) {\n\t\t\tif (!(flags & MSG_PEEK))\n\t\t\t\tTIPC_SKB_CB(buf)->handle =\n\t\t\t\t(void *)(unsigned long)(offset + sz_to_copy);\n\t\t\tgoto exit;\n\t\t}\n\t} else {\n\t\tif (sz_copied != 0)\n\t\t\tgoto exit; /* can't add error msg to valid data */\n\n\t\tif ((err == TIPC_CONN_SHUTDOWN) || m->msg_control)\n\t\t\tres = 0;\n\t\telse\n\t\t\tres = -ECONNRESET;\n\t}\n\n\t/* Consume received message (optional) */\n\tif (likely(!(flags & MSG_PEEK))) {\n\t\tif (unlikely(++tport->conn_unacked >= TIPC_FLOW_CONTROL_WIN))\n\t\t\ttipc_acknowledge(tport->ref, tport->conn_unacked);\n\t\tadvance_rx_queue(sk);\n\t}\n\n\t/* Loop around if more data is required */\n\tif ((sz_copied < buf_len) &&\t/* didn't get all requested data */\n\t    (!skb_queue_empty(&sk->sk_receive_queue) ||\n\t    (sz_copied < target)) &&\t/* and more is ready or required */\n\t    (!(flags & MSG_PEEK)) &&\t/* and aren't just peeking at data */\n\t    (!err))\t\t\t/* and haven't reached a FIN */\n\t\tgoto restart;\n\nexit:\n\trelease_sock(sk);\n\treturn sz_copied ? sz_copied : res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,6 +23,9 @@\n \t\tres = -ENOTCONN;\n \t\tgoto exit;\n \t}\n+\n+\t/* will be updated in set_orig_addr() if needed */\n+\tm->msg_namelen = 0;\n \n \ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, buf_len);\n \ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* will be updated in set_orig_addr() if needed */",
                "\tm->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "net/tipc/socket.c in the Linux kernel before 3.9-rc7 does not initialize a certain data structure and a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 273
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static int blkfront_setup_indirect(struct blkfront_ring_info *rinfo)\n{\n\tunsigned int psegs, grants, memflags;\n\tint err, i;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\n\tmemflags = memalloc_noio_save();\n\n\tif (info->max_indirect_segments == 0) {\n\t\tif (!HAS_EXTRA_REQ)\n\t\t\tgrants = BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\t\telse {\n\t\t\t/*\n\t\t\t * When an extra req is required, the maximum\n\t\t\t * grants supported is related to the size of the\n\t\t\t * Linux block segment.\n\t\t\t */\n\t\t\tgrants = GRANTS_PER_PSEG;\n\t\t}\n\t}\n\telse\n\t\tgrants = info->max_indirect_segments;\n\tpsegs = DIV_ROUND_UP(grants, GRANTS_PER_PSEG);\n\n\terr = fill_grant_buffer(rinfo,\n\t\t\t\t(grants + INDIRECT_GREFS(grants)) * BLK_RING_SIZE(info));\n\tif (err)\n\t\tgoto out_of_memory;\n\n\tif (!info->feature_persistent && info->max_indirect_segments) {\n\t\t/*\n\t\t * We are using indirect descriptors but not persistent\n\t\t * grants, we need to allocate a set of pages that can be\n\t\t * used for mapping indirect grefs\n\t\t */\n\t\tint num = INDIRECT_GREFS(grants) * BLK_RING_SIZE(info);\n\n\t\tBUG_ON(!list_empty(&rinfo->indirect_pages));\n\t\tfor (i = 0; i < num; i++) {\n\t\t\tstruct page *indirect_page = alloc_page(GFP_KERNEL |\n\t\t\t\t\t\t\t\t__GFP_ZERO);\n\t\t\tif (!indirect_page)\n\t\t\t\tgoto out_of_memory;\n\t\t\tlist_add(&indirect_page->lru, &rinfo->indirect_pages);\n\t\t}\n\t}\n\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\trinfo->shadow[i].grants_used =\n\t\t\tkvcalloc(grants,\n\t\t\t\t sizeof(rinfo->shadow[i].grants_used[0]),\n\t\t\t\t GFP_KERNEL);\n\t\trinfo->shadow[i].sg = kvcalloc(psegs,\n\t\t\t\t\t       sizeof(rinfo->shadow[i].sg[0]),\n\t\t\t\t\t       GFP_KERNEL);\n\t\tif (info->max_indirect_segments)\n\t\t\trinfo->shadow[i].indirect_grants =\n\t\t\t\tkvcalloc(INDIRECT_GREFS(grants),\n\t\t\t\t\t sizeof(rinfo->shadow[i].indirect_grants[0]),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif ((rinfo->shadow[i].grants_used == NULL) ||\n\t\t\t(rinfo->shadow[i].sg == NULL) ||\n\t\t     (info->max_indirect_segments &&\n\t\t     (rinfo->shadow[i].indirect_grants == NULL)))\n\t\t\tgoto out_of_memory;\n\t\tsg_init_table(rinfo->shadow[i].sg, psegs);\n\t}\n\n\tmemalloc_noio_restore(memflags);\n\n\treturn 0;\n\nout_of_memory:\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\tkvfree(rinfo->shadow[i].grants_used);\n\t\trinfo->shadow[i].grants_used = NULL;\n\t\tkvfree(rinfo->shadow[i].sg);\n\t\trinfo->shadow[i].sg = NULL;\n\t\tkvfree(rinfo->shadow[i].indirect_grants);\n\t\trinfo->shadow[i].indirect_grants = NULL;\n\t}\n\tif (!list_empty(&rinfo->indirect_pages)) {\n\t\tstruct page *indirect_page, *n;\n\t\tlist_for_each_entry_safe(indirect_page, n, &rinfo->indirect_pages, lru) {\n\t\t\tlist_del(&indirect_page->lru);\n\t\t\t__free_page(indirect_page);\n\t\t}\n\t}\n\n\tmemalloc_noio_restore(memflags);\n\n\treturn -ENOMEM;\n}",
        "code_after_change": "static int blkfront_setup_indirect(struct blkfront_ring_info *rinfo)\n{\n\tunsigned int psegs, grants, memflags;\n\tint err, i;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\n\tmemflags = memalloc_noio_save();\n\n\tif (info->max_indirect_segments == 0) {\n\t\tif (!HAS_EXTRA_REQ)\n\t\t\tgrants = BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\t\telse {\n\t\t\t/*\n\t\t\t * When an extra req is required, the maximum\n\t\t\t * grants supported is related to the size of the\n\t\t\t * Linux block segment.\n\t\t\t */\n\t\t\tgrants = GRANTS_PER_PSEG;\n\t\t}\n\t}\n\telse\n\t\tgrants = info->max_indirect_segments;\n\tpsegs = DIV_ROUND_UP(grants, GRANTS_PER_PSEG);\n\n\terr = fill_grant_buffer(rinfo,\n\t\t\t\t(grants + INDIRECT_GREFS(grants)) * BLK_RING_SIZE(info));\n\tif (err)\n\t\tgoto out_of_memory;\n\n\tif (!info->bounce && info->max_indirect_segments) {\n\t\t/*\n\t\t * We are using indirect descriptors but don't have a bounce\n\t\t * buffer, we need to allocate a set of pages that can be\n\t\t * used for mapping indirect grefs\n\t\t */\n\t\tint num = INDIRECT_GREFS(grants) * BLK_RING_SIZE(info);\n\n\t\tBUG_ON(!list_empty(&rinfo->indirect_pages));\n\t\tfor (i = 0; i < num; i++) {\n\t\t\tstruct page *indirect_page = alloc_page(GFP_KERNEL |\n\t\t\t\t\t\t\t\t__GFP_ZERO);\n\t\t\tif (!indirect_page)\n\t\t\t\tgoto out_of_memory;\n\t\t\tlist_add(&indirect_page->lru, &rinfo->indirect_pages);\n\t\t}\n\t}\n\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\trinfo->shadow[i].grants_used =\n\t\t\tkvcalloc(grants,\n\t\t\t\t sizeof(rinfo->shadow[i].grants_used[0]),\n\t\t\t\t GFP_KERNEL);\n\t\trinfo->shadow[i].sg = kvcalloc(psegs,\n\t\t\t\t\t       sizeof(rinfo->shadow[i].sg[0]),\n\t\t\t\t\t       GFP_KERNEL);\n\t\tif (info->max_indirect_segments)\n\t\t\trinfo->shadow[i].indirect_grants =\n\t\t\t\tkvcalloc(INDIRECT_GREFS(grants),\n\t\t\t\t\t sizeof(rinfo->shadow[i].indirect_grants[0]),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif ((rinfo->shadow[i].grants_used == NULL) ||\n\t\t\t(rinfo->shadow[i].sg == NULL) ||\n\t\t     (info->max_indirect_segments &&\n\t\t     (rinfo->shadow[i].indirect_grants == NULL)))\n\t\t\tgoto out_of_memory;\n\t\tsg_init_table(rinfo->shadow[i].sg, psegs);\n\t}\n\n\tmemalloc_noio_restore(memflags);\n\n\treturn 0;\n\nout_of_memory:\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\tkvfree(rinfo->shadow[i].grants_used);\n\t\trinfo->shadow[i].grants_used = NULL;\n\t\tkvfree(rinfo->shadow[i].sg);\n\t\trinfo->shadow[i].sg = NULL;\n\t\tkvfree(rinfo->shadow[i].indirect_grants);\n\t\trinfo->shadow[i].indirect_grants = NULL;\n\t}\n\tif (!list_empty(&rinfo->indirect_pages)) {\n\t\tstruct page *indirect_page, *n;\n\t\tlist_for_each_entry_safe(indirect_page, n, &rinfo->indirect_pages, lru) {\n\t\t\tlist_del(&indirect_page->lru);\n\t\t\t__free_page(indirect_page);\n\t\t}\n\t}\n\n\tmemalloc_noio_restore(memflags);\n\n\treturn -ENOMEM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,10 +27,10 @@\n \tif (err)\n \t\tgoto out_of_memory;\n \n-\tif (!info->feature_persistent && info->max_indirect_segments) {\n+\tif (!info->bounce && info->max_indirect_segments) {\n \t\t/*\n-\t\t * We are using indirect descriptors but not persistent\n-\t\t * grants, we need to allocate a set of pages that can be\n+\t\t * We are using indirect descriptors but don't have a bounce\n+\t\t * buffer, we need to allocate a set of pages that can be\n \t\t * used for mapping indirect grefs\n \t\t */\n \t\tint num = INDIRECT_GREFS(grants) * BLK_RING_SIZE(info);",
        "function_modified_lines": {
            "added": [
                "\tif (!info->bounce && info->max_indirect_segments) {",
                "\t\t * We are using indirect descriptors but don't have a bounce",
                "\t\t * buffer, we need to allocate a set of pages that can be"
            ],
            "deleted": [
                "\tif (!info->feature_persistent && info->max_indirect_segments) {",
                "\t\t * We are using indirect descriptors but not persistent",
                "\t\t * grants, we need to allocate a set of pages that can be"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3582
    },
    {
        "cve_id": "CVE-2018-20509",
        "code_before_change": "static void binder_transaction_buffer_release(struct binder_proc *proc,\n\t\t\t\t\t      struct binder_buffer *buffer,\n\t\t\t\t\t      binder_size_t *failed_at)\n{\n\tbinder_size_t *offp, *off_start, *off_end;\n\tint debug_id = buffer->debug_id;\n\n\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t     \"%d buffer release %d, size %zd-%zd, failed at %p\\n\",\n\t\t     proc->pid, buffer->debug_id,\n\t\t     buffer->data_size, buffer->offsets_size, failed_at);\n\n\tif (buffer->target_node)\n\t\tbinder_dec_node(buffer->target_node, 1, 0);\n\n\toff_start = (binder_size_t *)(buffer->data +\n\t\t\t\t      ALIGN(buffer->data_size, sizeof(void *)));\n\tif (failed_at)\n\t\toff_end = failed_at;\n\telse\n\t\toff_end = (void *)off_start + buffer->offsets_size;\n\tfor (offp = off_start; offp < off_end; offp++) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size = binder_validate_object(buffer, *offp);\n\n\t\tif (object_size == 0) {\n\t\t\tpr_err(\"transaction release %d bad object at offset %lld, size %zd\\n\",\n\t\t\t       debug_id, (u64)*offp, buffer->data_size);\n\t\t\tcontinue;\n\t\t}\n\t\thdr = (struct binder_object_header *)(buffer->data + *offp);\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_node *node;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tnode = binder_get_node(proc, fp->binder);\n\t\t\tif (node == NULL) {\n\t\t\t\tpr_err(\"transaction release %d bad node %016llx\\n\",\n\t\t\t\t       debug_id, (u64)fp->binder);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        node %d u%016llx\\n\",\n\t\t\t\t     node->debug_id, (u64)node->ptr);\n\t\t\tbinder_dec_node(node, hdr->type == BINDER_TYPE_BINDER,\n\t\t\t\t\t0);\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_ref *ref;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tref = binder_get_ref(proc, fp->handle,\n\t\t\t\t\t     hdr->type == BINDER_TYPE_HANDLE);\n\t\t\tif (ref == NULL) {\n\t\t\t\tpr_err(\"transaction release %d bad handle %d\\n\",\n\t\t\t\t debug_id, fp->handle);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        ref %d desc %d (node %d)\\n\",\n\t\t\t\t     ref->debug_id, ref->desc, ref->node->debug_id);\n\t\t\tbinder_dec_ref(ref, hdr->type == BINDER_TYPE_HANDLE);\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        fd %d\\n\", fp->fd);\n\t\t\tif (failed_at)\n\t\t\t\ttask_close_fd(proc, fp->fd);\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR:\n\t\t\t/*\n\t\t\t * Nothing to do here, this will get cleaned up when the\n\t\t\t * transaction buffer gets freed\n\t\t\t */\n\t\t\tbreak;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_fd_array_object *fda;\n\t\t\tstruct binder_buffer_object *parent;\n\t\t\tuintptr_t parent_buffer;\n\t\t\tu32 *fd_array;\n\t\t\tsize_t fd_index;\n\t\t\tbinder_size_t fd_buf_size;\n\n\t\t\tfda = to_binder_fd_array_object(hdr);\n\t\t\tparent = binder_validate_ptr(buffer, fda->parent,\n\t\t\t\t\t\t     off_start,\n\t\t\t\t\t\t     offp - off_start);\n\t\t\tif (!parent) {\n\t\t\t\tpr_err(\"transaction release %d bad parent offset\",\n\t\t\t\t       debug_id);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Since the parent was already fixed up, convert it\n\t\t\t * back to kernel address space to access it\n\t\t\t */\n\t\t\tparent_buffer = parent->buffer -\n\t\t\t\tbinder_alloc_get_user_buffer_offset(\n\t\t\t\t\t\t&proc->alloc);\n\n\t\t\tfd_buf_size = sizeof(u32) * fda->num_fds;\n\t\t\tif (fda->num_fds >= SIZE_MAX / sizeof(u32)) {\n\t\t\t\tpr_err(\"transaction release %d invalid number of fds (%lld)\\n\",\n\t\t\t\t       debug_id, (u64)fda->num_fds);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (fd_buf_size > parent->length ||\n\t\t\t    fda->parent_offset > parent->length - fd_buf_size) {\n\t\t\t\t/* No space for all file descriptors here. */\n\t\t\t\tpr_err(\"transaction release %d not enough space for %lld fds in buffer\\n\",\n\t\t\t\t       debug_id, (u64)fda->num_fds);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tfd_array = (u32 *)(parent_buffer + fda->parent_offset);\n\t\t\tfor (fd_index = 0; fd_index < fda->num_fds; fd_index++)\n\t\t\t\ttask_close_fd(proc, fd_array[fd_index]);\n\t\t} break;\n\t\tdefault:\n\t\t\tpr_err(\"transaction release %d bad object type %x\\n\",\n\t\t\t\tdebug_id, hdr->type);\n\t\t\tbreak;\n\t\t}\n\t}\n}",
        "code_after_change": "static void binder_transaction_buffer_release(struct binder_proc *proc,\n\t\t\t\t\t      struct binder_buffer *buffer,\n\t\t\t\t\t      binder_size_t *failed_at)\n{\n\tbinder_size_t *offp, *off_start, *off_end;\n\tint debug_id = buffer->debug_id;\n\n\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t     \"%d buffer release %d, size %zd-%zd, failed at %p\\n\",\n\t\t     proc->pid, buffer->debug_id,\n\t\t     buffer->data_size, buffer->offsets_size, failed_at);\n\n\tif (buffer->target_node)\n\t\tbinder_dec_node(buffer->target_node, 1, 0);\n\n\toff_start = (binder_size_t *)(buffer->data +\n\t\t\t\t      ALIGN(buffer->data_size, sizeof(void *)));\n\tif (failed_at)\n\t\toff_end = failed_at;\n\telse\n\t\toff_end = (void *)off_start + buffer->offsets_size;\n\tfor (offp = off_start; offp < off_end; offp++) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size = binder_validate_object(buffer, *offp);\n\n\t\tif (object_size == 0) {\n\t\t\tpr_err(\"transaction release %d bad object at offset %lld, size %zd\\n\",\n\t\t\t       debug_id, (u64)*offp, buffer->data_size);\n\t\t\tcontinue;\n\t\t}\n\t\thdr = (struct binder_object_header *)(buffer->data + *offp);\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_node *node;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tnode = binder_get_node(proc, fp->binder);\n\t\t\tif (node == NULL) {\n\t\t\t\tpr_err(\"transaction release %d bad node %016llx\\n\",\n\t\t\t\t       debug_id, (u64)fp->binder);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        node %d u%016llx\\n\",\n\t\t\t\t     node->debug_id, (u64)node->ptr);\n\t\t\tbinder_dec_node(node, hdr->type == BINDER_TYPE_BINDER,\n\t\t\t\t\t0);\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_ref_data rdata;\n\t\t\tint ret;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_dec_ref_for_handle(proc, fp->handle,\n\t\t\t\thdr->type == BINDER_TYPE_HANDLE, &rdata);\n\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"transaction release %d bad handle %d, ret = %d\\n\",\n\t\t\t\t debug_id, fp->handle, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        ref %d desc %d\\n\",\n\t\t\t\t     rdata.debug_id, rdata.desc);\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        fd %d\\n\", fp->fd);\n\t\t\tif (failed_at)\n\t\t\t\ttask_close_fd(proc, fp->fd);\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR:\n\t\t\t/*\n\t\t\t * Nothing to do here, this will get cleaned up when the\n\t\t\t * transaction buffer gets freed\n\t\t\t */\n\t\t\tbreak;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_fd_array_object *fda;\n\t\t\tstruct binder_buffer_object *parent;\n\t\t\tuintptr_t parent_buffer;\n\t\t\tu32 *fd_array;\n\t\t\tsize_t fd_index;\n\t\t\tbinder_size_t fd_buf_size;\n\n\t\t\tfda = to_binder_fd_array_object(hdr);\n\t\t\tparent = binder_validate_ptr(buffer, fda->parent,\n\t\t\t\t\t\t     off_start,\n\t\t\t\t\t\t     offp - off_start);\n\t\t\tif (!parent) {\n\t\t\t\tpr_err(\"transaction release %d bad parent offset\",\n\t\t\t\t       debug_id);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Since the parent was already fixed up, convert it\n\t\t\t * back to kernel address space to access it\n\t\t\t */\n\t\t\tparent_buffer = parent->buffer -\n\t\t\t\tbinder_alloc_get_user_buffer_offset(\n\t\t\t\t\t\t&proc->alloc);\n\n\t\t\tfd_buf_size = sizeof(u32) * fda->num_fds;\n\t\t\tif (fda->num_fds >= SIZE_MAX / sizeof(u32)) {\n\t\t\t\tpr_err(\"transaction release %d invalid number of fds (%lld)\\n\",\n\t\t\t\t       debug_id, (u64)fda->num_fds);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (fd_buf_size > parent->length ||\n\t\t\t    fda->parent_offset > parent->length - fd_buf_size) {\n\t\t\t\t/* No space for all file descriptors here. */\n\t\t\t\tpr_err(\"transaction release %d not enough space for %lld fds in buffer\\n\",\n\t\t\t\t       debug_id, (u64)fda->num_fds);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tfd_array = (u32 *)(parent_buffer + fda->parent_offset);\n\t\t\tfor (fd_index = 0; fd_index < fda->num_fds; fd_index++)\n\t\t\t\ttask_close_fd(proc, fd_array[fd_index]);\n\t\t} break;\n\t\tdefault:\n\t\t\tpr_err(\"transaction release %d bad object type %x\\n\",\n\t\t\t\tdebug_id, hdr->type);\n\t\t\tbreak;\n\t\t}\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -51,20 +51,21 @@\n \t\tcase BINDER_TYPE_HANDLE:\n \t\tcase BINDER_TYPE_WEAK_HANDLE: {\n \t\t\tstruct flat_binder_object *fp;\n-\t\t\tstruct binder_ref *ref;\n+\t\t\tstruct binder_ref_data rdata;\n+\t\t\tint ret;\n \n \t\t\tfp = to_flat_binder_object(hdr);\n-\t\t\tref = binder_get_ref(proc, fp->handle,\n-\t\t\t\t\t     hdr->type == BINDER_TYPE_HANDLE);\n-\t\t\tif (ref == NULL) {\n-\t\t\t\tpr_err(\"transaction release %d bad handle %d\\n\",\n-\t\t\t\t debug_id, fp->handle);\n+\t\t\tret = binder_dec_ref_for_handle(proc, fp->handle,\n+\t\t\t\thdr->type == BINDER_TYPE_HANDLE, &rdata);\n+\n+\t\t\tif (ret) {\n+\t\t\t\tpr_err(\"transaction release %d bad handle %d, ret = %d\\n\",\n+\t\t\t\t debug_id, fp->handle, ret);\n \t\t\t\tbreak;\n \t\t\t}\n \t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n-\t\t\t\t     \"        ref %d desc %d (node %d)\\n\",\n-\t\t\t\t     ref->debug_id, ref->desc, ref->node->debug_id);\n-\t\t\tbinder_dec_ref(ref, hdr->type == BINDER_TYPE_HANDLE);\n+\t\t\t\t     \"        ref %d desc %d\\n\",\n+\t\t\t\t     rdata.debug_id, rdata.desc);\n \t\t} break;\n \n \t\tcase BINDER_TYPE_FD: {",
        "function_modified_lines": {
            "added": [
                "\t\t\tstruct binder_ref_data rdata;",
                "\t\t\tint ret;",
                "\t\t\tret = binder_dec_ref_for_handle(proc, fp->handle,",
                "\t\t\t\thdr->type == BINDER_TYPE_HANDLE, &rdata);",
                "",
                "\t\t\tif (ret) {",
                "\t\t\t\tpr_err(\"transaction release %d bad handle %d, ret = %d\\n\",",
                "\t\t\t\t debug_id, fp->handle, ret);",
                "\t\t\t\t     \"        ref %d desc %d\\n\",",
                "\t\t\t\t     rdata.debug_id, rdata.desc);"
            ],
            "deleted": [
                "\t\t\tstruct binder_ref *ref;",
                "\t\t\tref = binder_get_ref(proc, fp->handle,",
                "\t\t\t\t\t     hdr->type == BINDER_TYPE_HANDLE);",
                "\t\t\tif (ref == NULL) {",
                "\t\t\t\tpr_err(\"transaction release %d bad handle %d\\n\",",
                "\t\t\t\t debug_id, fp->handle);",
                "\t\t\t\t     \"        ref %d desc %d (node %d)\\n\",",
                "\t\t\t\t     ref->debug_id, ref->desc, ref->node->debug_id);",
                "\t\t\tbinder_dec_ref(ref, hdr->type == BINDER_TYPE_HANDLE);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_ref_olocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \" ref *desc *node\" lines in a debugfs file.",
        "id": 1759
    },
    {
        "cve_id": "CVE-2016-0723",
        "code_before_change": "long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct tty_struct *tty = file_tty(file);\n\tstruct tty_struct *real_tty;\n\tvoid __user *p = (void __user *)arg;\n\tint retval;\n\tstruct tty_ldisc *ld;\n\n\tif (tty_paranoia_check(tty, file_inode(file), \"tty_ioctl\"))\n\t\treturn -EINVAL;\n\n\treal_tty = tty_pair_get_tty(tty);\n\n\t/*\n\t * Factor out some common prep work\n\t */\n\tswitch (cmd) {\n\tcase TIOCSETD:\n\tcase TIOCSBRK:\n\tcase TIOCCBRK:\n\tcase TCSBRK:\n\tcase TCSBRKP:\n\t\tretval = tty_check_change(tty);\n\t\tif (retval)\n\t\t\treturn retval;\n\t\tif (cmd != TIOCCBRK) {\n\t\t\ttty_wait_until_sent(tty, 0);\n\t\t\tif (signal_pending(current))\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t *\tNow do the stuff.\n\t */\n\tswitch (cmd) {\n\tcase TIOCSTI:\n\t\treturn tiocsti(tty, p);\n\tcase TIOCGWINSZ:\n\t\treturn tiocgwinsz(real_tty, p);\n\tcase TIOCSWINSZ:\n\t\treturn tiocswinsz(real_tty, p);\n\tcase TIOCCONS:\n\t\treturn real_tty != tty ? -EINVAL : tioccons(file);\n\tcase FIONBIO:\n\t\treturn fionbio(file, p);\n\tcase TIOCEXCL:\n\t\tset_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCNXCL:\n\t\tclear_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCGEXCL:\n\t{\n\t\tint excl = test_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn put_user(excl, (int __user *)p);\n\t}\n\tcase TIOCNOTTY:\n\t\tif (current->signal->tty != tty)\n\t\t\treturn -ENOTTY;\n\t\tno_tty();\n\t\treturn 0;\n\tcase TIOCSCTTY:\n\t\treturn tiocsctty(real_tty, file, arg);\n\tcase TIOCGPGRP:\n\t\treturn tiocgpgrp(tty, real_tty, p);\n\tcase TIOCSPGRP:\n\t\treturn tiocspgrp(tty, real_tty, p);\n\tcase TIOCGSID:\n\t\treturn tiocgsid(tty, real_tty, p);\n\tcase TIOCGETD:\n\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);\n\tcase TIOCSETD:\n\t\treturn tiocsetd(tty, p);\n\tcase TIOCVHANGUP:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\ttty_vhangup(tty);\n\t\treturn 0;\n\tcase TIOCGDEV:\n\t{\n\t\tunsigned int ret = new_encode_dev(tty_devnum(real_tty));\n\t\treturn put_user(ret, (unsigned int __user *)p);\n\t}\n\t/*\n\t * Break handling\n\t */\n\tcase TIOCSBRK:\t/* Turn break on, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, -1);\n\t\treturn 0;\n\tcase TIOCCBRK:\t/* Turn break off, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, 0);\n\t\treturn 0;\n\tcase TCSBRK:   /* SVID version: non-zero arg --> no break */\n\t\t/* non-zero arg means wait for all output data\n\t\t * to be sent (performed above) but don't send break.\n\t\t * This is used by the tcdrain() termios function.\n\t\t */\n\t\tif (!arg)\n\t\t\treturn send_break(tty, 250);\n\t\treturn 0;\n\tcase TCSBRKP:\t/* support for POSIX tcsendbreak() */\n\t\treturn send_break(tty, arg ? arg*100 : 250);\n\n\tcase TIOCMGET:\n\t\treturn tty_tiocmget(tty, p);\n\tcase TIOCMSET:\n\tcase TIOCMBIC:\n\tcase TIOCMBIS:\n\t\treturn tty_tiocmset(tty, cmd, p);\n\tcase TIOCGICOUNT:\n\t\tretval = tty_tiocgicount(tty, p);\n\t\t/* For the moment allow fall through to the old method */\n        \tif (retval != -EINVAL)\n\t\t\treturn retval;\n\t\tbreak;\n\tcase TCFLSH:\n\t\tswitch (arg) {\n\t\tcase TCIFLUSH:\n\t\tcase TCIOFLUSH:\n\t\t/* flush tty buffer and allow ldisc to process ioctl */\n\t\t\ttty_buffer_flush(tty, NULL);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase TIOCSSERIAL:\n\t\ttty_warn_deprecated_flags(p);\n\t\tbreak;\n\t}\n\tif (tty->ops->ioctl) {\n\t\tretval = tty->ops->ioctl(tty, cmd, arg);\n\t\tif (retval != -ENOIOCTLCMD)\n\t\t\treturn retval;\n\t}\n\tld = tty_ldisc_ref_wait(tty);\n\tretval = -EINVAL;\n\tif (ld->ops->ioctl) {\n\t\tretval = ld->ops->ioctl(tty, file, cmd, arg);\n\t\tif (retval == -ENOIOCTLCMD)\n\t\t\tretval = -ENOTTY;\n\t}\n\ttty_ldisc_deref(ld);\n\treturn retval;\n}",
        "code_after_change": "long tty_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct tty_struct *tty = file_tty(file);\n\tstruct tty_struct *real_tty;\n\tvoid __user *p = (void __user *)arg;\n\tint retval;\n\tstruct tty_ldisc *ld;\n\n\tif (tty_paranoia_check(tty, file_inode(file), \"tty_ioctl\"))\n\t\treturn -EINVAL;\n\n\treal_tty = tty_pair_get_tty(tty);\n\n\t/*\n\t * Factor out some common prep work\n\t */\n\tswitch (cmd) {\n\tcase TIOCSETD:\n\tcase TIOCSBRK:\n\tcase TIOCCBRK:\n\tcase TCSBRK:\n\tcase TCSBRKP:\n\t\tretval = tty_check_change(tty);\n\t\tif (retval)\n\t\t\treturn retval;\n\t\tif (cmd != TIOCCBRK) {\n\t\t\ttty_wait_until_sent(tty, 0);\n\t\t\tif (signal_pending(current))\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t *\tNow do the stuff.\n\t */\n\tswitch (cmd) {\n\tcase TIOCSTI:\n\t\treturn tiocsti(tty, p);\n\tcase TIOCGWINSZ:\n\t\treturn tiocgwinsz(real_tty, p);\n\tcase TIOCSWINSZ:\n\t\treturn tiocswinsz(real_tty, p);\n\tcase TIOCCONS:\n\t\treturn real_tty != tty ? -EINVAL : tioccons(file);\n\tcase FIONBIO:\n\t\treturn fionbio(file, p);\n\tcase TIOCEXCL:\n\t\tset_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCNXCL:\n\t\tclear_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn 0;\n\tcase TIOCGEXCL:\n\t{\n\t\tint excl = test_bit(TTY_EXCLUSIVE, &tty->flags);\n\t\treturn put_user(excl, (int __user *)p);\n\t}\n\tcase TIOCNOTTY:\n\t\tif (current->signal->tty != tty)\n\t\t\treturn -ENOTTY;\n\t\tno_tty();\n\t\treturn 0;\n\tcase TIOCSCTTY:\n\t\treturn tiocsctty(real_tty, file, arg);\n\tcase TIOCGPGRP:\n\t\treturn tiocgpgrp(tty, real_tty, p);\n\tcase TIOCSPGRP:\n\t\treturn tiocspgrp(tty, real_tty, p);\n\tcase TIOCGSID:\n\t\treturn tiocgsid(tty, real_tty, p);\n\tcase TIOCGETD:\n\t\treturn tiocgetd(tty, p);\n\tcase TIOCSETD:\n\t\treturn tiocsetd(tty, p);\n\tcase TIOCVHANGUP:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\ttty_vhangup(tty);\n\t\treturn 0;\n\tcase TIOCGDEV:\n\t{\n\t\tunsigned int ret = new_encode_dev(tty_devnum(real_tty));\n\t\treturn put_user(ret, (unsigned int __user *)p);\n\t}\n\t/*\n\t * Break handling\n\t */\n\tcase TIOCSBRK:\t/* Turn break on, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, -1);\n\t\treturn 0;\n\tcase TIOCCBRK:\t/* Turn break off, unconditionally */\n\t\tif (tty->ops->break_ctl)\n\t\t\treturn tty->ops->break_ctl(tty, 0);\n\t\treturn 0;\n\tcase TCSBRK:   /* SVID version: non-zero arg --> no break */\n\t\t/* non-zero arg means wait for all output data\n\t\t * to be sent (performed above) but don't send break.\n\t\t * This is used by the tcdrain() termios function.\n\t\t */\n\t\tif (!arg)\n\t\t\treturn send_break(tty, 250);\n\t\treturn 0;\n\tcase TCSBRKP:\t/* support for POSIX tcsendbreak() */\n\t\treturn send_break(tty, arg ? arg*100 : 250);\n\n\tcase TIOCMGET:\n\t\treturn tty_tiocmget(tty, p);\n\tcase TIOCMSET:\n\tcase TIOCMBIC:\n\tcase TIOCMBIS:\n\t\treturn tty_tiocmset(tty, cmd, p);\n\tcase TIOCGICOUNT:\n\t\tretval = tty_tiocgicount(tty, p);\n\t\t/* For the moment allow fall through to the old method */\n        \tif (retval != -EINVAL)\n\t\t\treturn retval;\n\t\tbreak;\n\tcase TCFLSH:\n\t\tswitch (arg) {\n\t\tcase TCIFLUSH:\n\t\tcase TCIOFLUSH:\n\t\t/* flush tty buffer and allow ldisc to process ioctl */\n\t\t\ttty_buffer_flush(tty, NULL);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase TIOCSSERIAL:\n\t\ttty_warn_deprecated_flags(p);\n\t\tbreak;\n\t}\n\tif (tty->ops->ioctl) {\n\t\tretval = tty->ops->ioctl(tty, cmd, arg);\n\t\tif (retval != -ENOIOCTLCMD)\n\t\t\treturn retval;\n\t}\n\tld = tty_ldisc_ref_wait(tty);\n\tretval = -EINVAL;\n\tif (ld->ops->ioctl) {\n\t\tretval = ld->ops->ioctl(tty, file, cmd, arg);\n\t\tif (retval == -ENOIOCTLCMD)\n\t\t\tretval = -ENOTTY;\n\t}\n\ttty_ldisc_deref(ld);\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -70,7 +70,7 @@\n \tcase TIOCGSID:\n \t\treturn tiocgsid(tty, real_tty, p);\n \tcase TIOCGETD:\n-\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);\n+\t\treturn tiocgetd(tty, p);\n \tcase TIOCSETD:\n \t\treturn tiocsetd(tty, p);\n \tcase TIOCVHANGUP:",
        "function_modified_lines": {
            "added": [
                "\t\treturn tiocgetd(tty, p);"
            ],
            "deleted": [
                "\t\treturn put_user(tty->ldisc->ops->num, (int __user *)p);"
            ]
        },
        "cwe": [
            "CWE-200",
            "CWE-362",
            "NVD-CWE-Other"
        ],
        "cve_description": "Race condition in the tty_ioctl function in drivers/tty/tty_io.c in the Linux kernel through 4.4.1 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free and system crash) by making a TIOCGETD ioctl call during processing of a TIOCSETD ioctl call.",
        "id": 888
    },
    {
        "cve_id": "CVE-2013-7281",
        "code_before_change": "static int dgram_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\tstruct msghdr *msg, size_t len, int noblock, int flags,\n\t\tint *addr_len)\n{\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sk_buff *skb;\n\tstruct sockaddr_ieee802154 *saddr;\n\n\tsaddr = (struct sockaddr_ieee802154 *)msg->msg_name;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\t/* FIXME: skip headers if necessary ?! */\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (saddr) {\n\t\tsaddr->family = AF_IEEE802154;\n\t\tsaddr->addr = mac_cb(skb)->sa;\n\t}\n\tif (addr_len)\n\t\t*addr_len = sizeof(*saddr);\n\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err)\n\t\treturn err;\n\treturn copied;\n}",
        "code_after_change": "static int dgram_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\tstruct msghdr *msg, size_t len, int noblock, int flags,\n\t\tint *addr_len)\n{\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sk_buff *skb;\n\tstruct sockaddr_ieee802154 *saddr;\n\n\tsaddr = (struct sockaddr_ieee802154 *)msg->msg_name;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\t/* FIXME: skip headers if necessary ?! */\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (saddr) {\n\t\tsaddr->family = AF_IEEE802154;\n\t\tsaddr->addr = mac_cb(skb)->sa;\n\t\t*addr_len = sizeof(*saddr);\n\t}\n\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err)\n\t\treturn err;\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -29,9 +29,8 @@\n \tif (saddr) {\n \t\tsaddr->family = AF_IEEE802154;\n \t\tsaddr->addr = mac_cb(skb)->sa;\n+\t\t*addr_len = sizeof(*saddr);\n \t}\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(*saddr);\n \n \tif (flags & MSG_TRUNC)\n \t\tcopied = skb->len;",
        "function_modified_lines": {
            "added": [
                "\t\t*addr_len = sizeof(*saddr);"
            ],
            "deleted": [
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(*saddr);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The dgram_recvmsg function in net/ieee802154/dgram.c in the Linux kernel before 3.12.4 updates a certain length value without ensuring that an associated data structure has been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 407
    },
    {
        "cve_id": "CVE-2017-17864",
        "code_before_change": "static bool regsafe(struct bpf_reg_state *rold, struct bpf_reg_state *rcur,\n\t\t    struct idpair *idmap)\n{\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, live)) == 0)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* if we knew anything about the old value, we're not\n\t\t\t * equal, because we can't know anything about the\n\t\t\t * scalar value of the pointer in the new value.\n\t\t\t */\n\t\t\treturn rold->umin_value == 0 &&\n\t\t\t       rold->umax_value == U64_MAX &&\n\t\t\t       rold->smin_value == S64_MIN &&\n\t\t\t       rold->smax_value == S64_MAX &&\n\t\t\t       tnum_is_unknown(rold->var_off);\n\t\t}\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * We don't care about the 'id' value, because nothing\n\t\t * uses it for PTR_TO_MAP_VALUE (only for ..._OR_NULL)\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_PACKET_END:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
        "code_after_change": "static bool regsafe(struct bpf_reg_state *rold, struct bpf_reg_state *rcur,\n\t\t    struct idpair *idmap)\n{\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, live)) == 0)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * We don't care about the 'id' value, because nothing\n\t\t * uses it for PTR_TO_MAP_VALUE (only for ..._OR_NULL)\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_PACKET_END:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,15 +20,14 @@\n \t\t\treturn range_within(rold, rcur) &&\n \t\t\t       tnum_in(rold->var_off, rcur->var_off);\n \t\t} else {\n-\t\t\t/* if we knew anything about the old value, we're not\n-\t\t\t * equal, because we can't know anything about the\n-\t\t\t * scalar value of the pointer in the new value.\n+\t\t\t/* We're trying to use a pointer in place of a scalar.\n+\t\t\t * Even if the scalar was unbounded, this could lead to\n+\t\t\t * pointer leaks because scalars are allowed to leak\n+\t\t\t * while pointers are not. We could make this safe in\n+\t\t\t * special cases if root is calling us, but it's\n+\t\t\t * probably not worth the hassle.\n \t\t\t */\n-\t\t\treturn rold->umin_value == 0 &&\n-\t\t\t       rold->umax_value == U64_MAX &&\n-\t\t\t       rold->smin_value == S64_MIN &&\n-\t\t\t       rold->smax_value == S64_MAX &&\n-\t\t\t       tnum_is_unknown(rold->var_off);\n+\t\t\treturn false;\n \t\t}\n \tcase PTR_TO_MAP_VALUE:\n \t\t/* If the new min/max/var_off satisfy the old ones and",
        "function_modified_lines": {
            "added": [
                "\t\t\t/* We're trying to use a pointer in place of a scalar.",
                "\t\t\t * Even if the scalar was unbounded, this could lead to",
                "\t\t\t * pointer leaks because scalars are allowed to leak",
                "\t\t\t * while pointers are not. We could make this safe in",
                "\t\t\t * special cases if root is calling us, but it's",
                "\t\t\t * probably not worth the hassle.",
                "\t\t\treturn false;"
            ],
            "deleted": [
                "\t\t\t/* if we knew anything about the old value, we're not",
                "\t\t\t * equal, because we can't know anything about the",
                "\t\t\t * scalar value of the pointer in the new value.",
                "\t\t\treturn rold->umin_value == 0 &&",
                "\t\t\t       rold->umax_value == U64_MAX &&",
                "\t\t\t       rold->smin_value == S64_MIN &&",
                "\t\t\t       rold->smax_value == S64_MAX &&",
                "\t\t\t       tnum_is_unknown(rold->var_off);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "kernel/bpf/verifier.c in the Linux kernel through 4.14.8 mishandles states_equal comparisons between the pointer data type and the UNKNOWN_VALUE data type, which allows local users to obtain potentially sensitive address information, aka a \"pointer leak.\"",
        "id": 1385
    },
    {
        "cve_id": "CVE-2018-19854",
        "code_before_change": "static int crypto_report_one(struct crypto_alg *alg,\n\t\t\t     struct crypto_user_alg *ualg, struct sk_buff *skb)\n{\n\tstrlcpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));\n\tstrlcpy(ualg->cru_driver_name, alg->cra_driver_name,\n\t\tsizeof(ualg->cru_driver_name));\n\tstrlcpy(ualg->cru_module_name, module_name(alg->cra_module),\n\t\tsizeof(ualg->cru_module_name));\n\n\tualg->cru_type = 0;\n\tualg->cru_mask = 0;\n\tualg->cru_flags = alg->cra_flags;\n\tualg->cru_refcnt = refcount_read(&alg->cra_refcnt);\n\n\tif (nla_put_u32(skb, CRYPTOCFGA_PRIORITY_VAL, alg->cra_priority))\n\t\tgoto nla_put_failure;\n\tif (alg->cra_flags & CRYPTO_ALG_LARVAL) {\n\t\tstruct crypto_report_larval rl;\n\n\t\tstrlcpy(rl.type, \"larval\", sizeof(rl.type));\n\t\tif (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,\n\t\t\t    sizeof(struct crypto_report_larval), &rl))\n\t\t\tgoto nla_put_failure;\n\t\tgoto out;\n\t}\n\n\tif (alg->cra_type && alg->cra_type->report) {\n\t\tif (alg->cra_type->report(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tgoto out;\n\t}\n\n\tswitch (alg->cra_flags & (CRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_LARVAL)) {\n\tcase CRYPTO_ALG_TYPE_CIPHER:\n\t\tif (crypto_report_cipher(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_COMPRESS:\n\t\tif (crypto_report_comp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_ACOMPRESS:\n\t\tif (crypto_report_acomp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_AKCIPHER:\n\t\tif (crypto_report_akcipher(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_KPP:\n\t\tif (crypto_report_kpp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\t\tbreak;\n\t}\n\nout:\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "code_after_change": "static int crypto_report_one(struct crypto_alg *alg,\n\t\t\t     struct crypto_user_alg *ualg, struct sk_buff *skb)\n{\n\tstrncpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));\n\tstrncpy(ualg->cru_driver_name, alg->cra_driver_name,\n\t\tsizeof(ualg->cru_driver_name));\n\tstrncpy(ualg->cru_module_name, module_name(alg->cra_module),\n\t\tsizeof(ualg->cru_module_name));\n\n\tualg->cru_type = 0;\n\tualg->cru_mask = 0;\n\tualg->cru_flags = alg->cra_flags;\n\tualg->cru_refcnt = refcount_read(&alg->cra_refcnt);\n\n\tif (nla_put_u32(skb, CRYPTOCFGA_PRIORITY_VAL, alg->cra_priority))\n\t\tgoto nla_put_failure;\n\tif (alg->cra_flags & CRYPTO_ALG_LARVAL) {\n\t\tstruct crypto_report_larval rl;\n\n\t\tstrncpy(rl.type, \"larval\", sizeof(rl.type));\n\t\tif (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,\n\t\t\t    sizeof(struct crypto_report_larval), &rl))\n\t\t\tgoto nla_put_failure;\n\t\tgoto out;\n\t}\n\n\tif (alg->cra_type && alg->cra_type->report) {\n\t\tif (alg->cra_type->report(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tgoto out;\n\t}\n\n\tswitch (alg->cra_flags & (CRYPTO_ALG_TYPE_MASK | CRYPTO_ALG_LARVAL)) {\n\tcase CRYPTO_ALG_TYPE_CIPHER:\n\t\tif (crypto_report_cipher(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_COMPRESS:\n\t\tif (crypto_report_comp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_ACOMPRESS:\n\t\tif (crypto_report_acomp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_AKCIPHER:\n\t\tif (crypto_report_akcipher(skb, alg))\n\t\t\tgoto nla_put_failure;\n\n\t\tbreak;\n\tcase CRYPTO_ALG_TYPE_KPP:\n\t\tif (crypto_report_kpp(skb, alg))\n\t\t\tgoto nla_put_failure;\n\t\tbreak;\n\t}\n\nout:\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,10 @@\n static int crypto_report_one(struct crypto_alg *alg,\n \t\t\t     struct crypto_user_alg *ualg, struct sk_buff *skb)\n {\n-\tstrlcpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));\n-\tstrlcpy(ualg->cru_driver_name, alg->cra_driver_name,\n+\tstrncpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));\n+\tstrncpy(ualg->cru_driver_name, alg->cra_driver_name,\n \t\tsizeof(ualg->cru_driver_name));\n-\tstrlcpy(ualg->cru_module_name, module_name(alg->cra_module),\n+\tstrncpy(ualg->cru_module_name, module_name(alg->cra_module),\n \t\tsizeof(ualg->cru_module_name));\n \n \tualg->cru_type = 0;\n@@ -17,7 +17,7 @@\n \tif (alg->cra_flags & CRYPTO_ALG_LARVAL) {\n \t\tstruct crypto_report_larval rl;\n \n-\t\tstrlcpy(rl.type, \"larval\", sizeof(rl.type));\n+\t\tstrncpy(rl.type, \"larval\", sizeof(rl.type));\n \t\tif (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,\n \t\t\t    sizeof(struct crypto_report_larval), &rl))\n \t\t\tgoto nla_put_failure;",
        "function_modified_lines": {
            "added": [
                "\tstrncpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));",
                "\tstrncpy(ualg->cru_driver_name, alg->cra_driver_name,",
                "\tstrncpy(ualg->cru_module_name, module_name(alg->cra_module),",
                "\t\tstrncpy(rl.type, \"larval\", sizeof(rl.type));"
            ],
            "deleted": [
                "\tstrlcpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));",
                "\tstrlcpy(ualg->cru_driver_name, alg->cra_driver_name,",
                "\tstrlcpy(ualg->cru_module_name, module_name(alg->cra_module),",
                "\t\tstrlcpy(rl.type, \"larval\", sizeof(rl.type));"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.19.3. crypto_report_one() and related functions in crypto/crypto_user.c (the crypto user configuration API) do not fully initialize structures that are copied to userspace, potentially leaking sensitive memory to user programs. NOTE: this is a CVE-2013-2547 regression but with easier exploitability because the attacker does not need a capability (however, the system must have the CONFIG_CRYPTO_USER kconfig option).",
        "id": 1748
    },
    {
        "cve_id": "CVE-2018-19854",
        "code_before_change": "static int crypto_report_cipher(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_cipher rcipher;\n\n\tstrlcpy(rcipher.type, \"cipher\", sizeof(rcipher.type));\n\n\trcipher.blocksize = alg->cra_blocksize;\n\trcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\trcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_CIPHER,\n\t\t    sizeof(struct crypto_report_cipher), &rcipher))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "code_after_change": "static int crypto_report_cipher(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_cipher rcipher;\n\n\tstrncpy(rcipher.type, \"cipher\", sizeof(rcipher.type));\n\n\trcipher.blocksize = alg->cra_blocksize;\n\trcipher.min_keysize = alg->cra_cipher.cia_min_keysize;\n\trcipher.max_keysize = alg->cra_cipher.cia_max_keysize;\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_CIPHER,\n\t\t    sizeof(struct crypto_report_cipher), &rcipher))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tstruct crypto_report_cipher rcipher;\n \n-\tstrlcpy(rcipher.type, \"cipher\", sizeof(rcipher.type));\n+\tstrncpy(rcipher.type, \"cipher\", sizeof(rcipher.type));\n \n \trcipher.blocksize = alg->cra_blocksize;\n \trcipher.min_keysize = alg->cra_cipher.cia_min_keysize;",
        "function_modified_lines": {
            "added": [
                "\tstrncpy(rcipher.type, \"cipher\", sizeof(rcipher.type));"
            ],
            "deleted": [
                "\tstrlcpy(rcipher.type, \"cipher\", sizeof(rcipher.type));"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.19.3. crypto_report_one() and related functions in crypto/crypto_user.c (the crypto user configuration API) do not fully initialize structures that are copied to userspace, potentially leaking sensitive memory to user programs. NOTE: this is a CVE-2013-2547 regression but with easier exploitability because the attacker does not need a capability (however, the system must have the CONFIG_CRYPTO_USER kconfig option).",
        "id": 1749
    },
    {
        "cve_id": "CVE-2013-3227",
        "code_before_change": "static int caif_seqpkt_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t\tstruct msghdr *m, size_t len, int flags)\n\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint ret;\n\tint copylen;\n\n\tret = -EOPNOTSUPP;\n\tif (m->msg_flags&MSG_OOB)\n\t\tgoto read_error;\n\n\tskb = skb_recv_datagram(sk, flags, 0 , &ret);\n\tif (!skb)\n\t\tgoto read_error;\n\tcopylen = skb->len;\n\tif (len < copylen) {\n\t\tm->msg_flags |= MSG_TRUNC;\n\t\tcopylen = len;\n\t}\n\n\tret = skb_copy_datagram_iovec(skb, 0, m->msg_iov, copylen);\n\tif (ret)\n\t\tgoto out_free;\n\n\tret = (flags & MSG_TRUNC) ? skb->len : copylen;\nout_free:\n\tskb_free_datagram(sk, skb);\n\tcaif_check_flow_release(sk);\n\treturn ret;\n\nread_error:\n\treturn ret;\n}",
        "code_after_change": "static int caif_seqpkt_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t\tstruct msghdr *m, size_t len, int flags)\n\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint ret;\n\tint copylen;\n\n\tret = -EOPNOTSUPP;\n\tif (m->msg_flags&MSG_OOB)\n\t\tgoto read_error;\n\n\tm->msg_namelen = 0;\n\n\tskb = skb_recv_datagram(sk, flags, 0 , &ret);\n\tif (!skb)\n\t\tgoto read_error;\n\tcopylen = skb->len;\n\tif (len < copylen) {\n\t\tm->msg_flags |= MSG_TRUNC;\n\t\tcopylen = len;\n\t}\n\n\tret = skb_copy_datagram_iovec(skb, 0, m->msg_iov, copylen);\n\tif (ret)\n\t\tgoto out_free;\n\n\tret = (flags & MSG_TRUNC) ? skb->len : copylen;\nout_free:\n\tskb_free_datagram(sk, skb);\n\tcaif_check_flow_release(sk);\n\treturn ret;\n\nread_error:\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,8 @@\n \tret = -EOPNOTSUPP;\n \tif (m->msg_flags&MSG_OOB)\n \t\tgoto read_error;\n+\n+\tm->msg_namelen = 0;\n \n \tskb = skb_recv_datagram(sk, flags, 0 , &ret);\n \tif (!skb)",
        "function_modified_lines": {
            "added": [
                "",
                "\tm->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The caif_seqpkt_recvmsg function in net/caif/caif_socket.c in the Linux kernel before 3.9-rc7 does not initialize a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 267
    },
    {
        "cve_id": "CVE-2016-4913",
        "code_before_change": "int get_rock_ridge_filename(struct iso_directory_record *de,\n\t\t\t    char *retname, struct inode *inode)\n{\n\tstruct rock_state rs;\n\tstruct rock_ridge *rr;\n\tint sig;\n\tint retnamlen = 0;\n\tint truncate = 0;\n\tint ret = 0;\n\n\tif (!ISOFS_SB(inode->i_sb)->s_rock)\n\t\treturn 0;\n\t*retname = 0;\n\n\tinit_rock_state(&rs, inode);\n\tsetup_rock_ridge(de, inode, &rs);\nrepeat:\n\n\twhile (rs.len > 2) { /* There may be one byte for padding somewhere */\n\t\trr = (struct rock_ridge *)rs.chr;\n\t\t/*\n\t\t * Ignore rock ridge info if rr->len is out of range, but\n\t\t * don't return -EIO because that would make the file\n\t\t * invisible.\n\t\t */\n\t\tif (rr->len < 3)\n\t\t\tgoto out;\t/* Something got screwed up here */\n\t\tsig = isonum_721(rs.chr);\n\t\tif (rock_check_overflow(&rs, sig))\n\t\t\tgoto eio;\n\t\trs.chr += rr->len;\n\t\trs.len -= rr->len;\n\t\t/*\n\t\t * As above, just ignore the rock ridge info if rr->len\n\t\t * is bogus.\n\t\t */\n\t\tif (rs.len < 0)\n\t\t\tgoto out;\t/* Something got screwed up here */\n\n\t\tswitch (sig) {\n\t\tcase SIG('R', 'R'):\n\t\t\tif ((rr->u.RR.flags[0] & RR_NM) == 0)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase SIG('S', 'P'):\n\t\t\tif (check_sp(rr, inode))\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase SIG('C', 'E'):\n\t\t\trs.cont_extent = isonum_733(rr->u.CE.extent);\n\t\t\trs.cont_offset = isonum_733(rr->u.CE.offset);\n\t\t\trs.cont_size = isonum_733(rr->u.CE.size);\n\t\t\tbreak;\n\t\tcase SIG('N', 'M'):\n\t\t\tif (truncate)\n\t\t\t\tbreak;\n\t\t\tif (rr->len < 5)\n\t\t\t\tbreak;\n\t\t\t/*\n\t\t\t * If the flags are 2 or 4, this indicates '.' or '..'.\n\t\t\t * We don't want to do anything with this, because it\n\t\t\t * screws up the code that calls us.  We don't really\n\t\t\t * care anyways, since we can just use the non-RR\n\t\t\t * name.\n\t\t\t */\n\t\t\tif (rr->u.NM.flags & 6)\n\t\t\t\tbreak;\n\n\t\t\tif (rr->u.NM.flags & ~1) {\n\t\t\t\tprintk(\"Unsupported NM flag settings (%d)\\n\",\n\t\t\t\t\trr->u.NM.flags);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif ((strlen(retname) + rr->len - 5) >= 254) {\n\t\t\t\ttruncate = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tstrncat(retname, rr->u.NM.name, rr->len - 5);\n\t\t\tretnamlen += rr->len - 5;\n\t\t\tbreak;\n\t\tcase SIG('R', 'E'):\n\t\t\tkfree(rs.buffer);\n\t\t\treturn -1;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tret = rock_continue(&rs);\n\tif (ret == 0)\n\t\tgoto repeat;\n\tif (ret == 1)\n\t\treturn retnamlen; /* If 0, this file did not have a NM field */\nout:\n\tkfree(rs.buffer);\n\treturn ret;\neio:\n\tret = -EIO;\n\tgoto out;\n}",
        "code_after_change": "int get_rock_ridge_filename(struct iso_directory_record *de,\n\t\t\t    char *retname, struct inode *inode)\n{\n\tstruct rock_state rs;\n\tstruct rock_ridge *rr;\n\tint sig;\n\tint retnamlen = 0;\n\tint truncate = 0;\n\tint ret = 0;\n\tchar *p;\n\tint len;\n\n\tif (!ISOFS_SB(inode->i_sb)->s_rock)\n\t\treturn 0;\n\t*retname = 0;\n\n\tinit_rock_state(&rs, inode);\n\tsetup_rock_ridge(de, inode, &rs);\nrepeat:\n\n\twhile (rs.len > 2) { /* There may be one byte for padding somewhere */\n\t\trr = (struct rock_ridge *)rs.chr;\n\t\t/*\n\t\t * Ignore rock ridge info if rr->len is out of range, but\n\t\t * don't return -EIO because that would make the file\n\t\t * invisible.\n\t\t */\n\t\tif (rr->len < 3)\n\t\t\tgoto out;\t/* Something got screwed up here */\n\t\tsig = isonum_721(rs.chr);\n\t\tif (rock_check_overflow(&rs, sig))\n\t\t\tgoto eio;\n\t\trs.chr += rr->len;\n\t\trs.len -= rr->len;\n\t\t/*\n\t\t * As above, just ignore the rock ridge info if rr->len\n\t\t * is bogus.\n\t\t */\n\t\tif (rs.len < 0)\n\t\t\tgoto out;\t/* Something got screwed up here */\n\n\t\tswitch (sig) {\n\t\tcase SIG('R', 'R'):\n\t\t\tif ((rr->u.RR.flags[0] & RR_NM) == 0)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase SIG('S', 'P'):\n\t\t\tif (check_sp(rr, inode))\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase SIG('C', 'E'):\n\t\t\trs.cont_extent = isonum_733(rr->u.CE.extent);\n\t\t\trs.cont_offset = isonum_733(rr->u.CE.offset);\n\t\t\trs.cont_size = isonum_733(rr->u.CE.size);\n\t\t\tbreak;\n\t\tcase SIG('N', 'M'):\n\t\t\tif (truncate)\n\t\t\t\tbreak;\n\t\t\tif (rr->len < 5)\n\t\t\t\tbreak;\n\t\t\t/*\n\t\t\t * If the flags are 2 or 4, this indicates '.' or '..'.\n\t\t\t * We don't want to do anything with this, because it\n\t\t\t * screws up the code that calls us.  We don't really\n\t\t\t * care anyways, since we can just use the non-RR\n\t\t\t * name.\n\t\t\t */\n\t\t\tif (rr->u.NM.flags & 6)\n\t\t\t\tbreak;\n\n\t\t\tif (rr->u.NM.flags & ~1) {\n\t\t\t\tprintk(\"Unsupported NM flag settings (%d)\\n\",\n\t\t\t\t\trr->u.NM.flags);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tlen = rr->len - 5;\n\t\t\tif (retnamlen + len >= 254) {\n\t\t\t\ttruncate = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tp = memchr(rr->u.NM.name, '\\0', len);\n\t\t\tif (unlikely(p))\n\t\t\t\tlen = p - rr->u.NM.name;\n\t\t\tmemcpy(retname + retnamlen, rr->u.NM.name, len);\n\t\t\tretnamlen += len;\n\t\t\tretname[retnamlen] = '\\0';\n\t\t\tbreak;\n\t\tcase SIG('R', 'E'):\n\t\t\tkfree(rs.buffer);\n\t\t\treturn -1;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tret = rock_continue(&rs);\n\tif (ret == 0)\n\t\tgoto repeat;\n\tif (ret == 1)\n\t\treturn retnamlen; /* If 0, this file did not have a NM field */\nout:\n\tkfree(rs.buffer);\n\treturn ret;\neio:\n\tret = -EIO;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,8 @@\n \tint retnamlen = 0;\n \tint truncate = 0;\n \tint ret = 0;\n+\tchar *p;\n+\tint len;\n \n \tif (!ISOFS_SB(inode->i_sb)->s_rock)\n \t\treturn 0;\n@@ -71,12 +73,17 @@\n \t\t\t\t\trr->u.NM.flags);\n \t\t\t\tbreak;\n \t\t\t}\n-\t\t\tif ((strlen(retname) + rr->len - 5) >= 254) {\n+\t\t\tlen = rr->len - 5;\n+\t\t\tif (retnamlen + len >= 254) {\n \t\t\t\ttruncate = 1;\n \t\t\t\tbreak;\n \t\t\t}\n-\t\t\tstrncat(retname, rr->u.NM.name, rr->len - 5);\n-\t\t\tretnamlen += rr->len - 5;\n+\t\t\tp = memchr(rr->u.NM.name, '\\0', len);\n+\t\t\tif (unlikely(p))\n+\t\t\t\tlen = p - rr->u.NM.name;\n+\t\t\tmemcpy(retname + retnamlen, rr->u.NM.name, len);\n+\t\t\tretnamlen += len;\n+\t\t\tretname[retnamlen] = '\\0';\n \t\t\tbreak;\n \t\tcase SIG('R', 'E'):\n \t\t\tkfree(rs.buffer);",
        "function_modified_lines": {
            "added": [
                "\tchar *p;",
                "\tint len;",
                "\t\t\tlen = rr->len - 5;",
                "\t\t\tif (retnamlen + len >= 254) {",
                "\t\t\tp = memchr(rr->u.NM.name, '\\0', len);",
                "\t\t\tif (unlikely(p))",
                "\t\t\t\tlen = p - rr->u.NM.name;",
                "\t\t\tmemcpy(retname + retnamlen, rr->u.NM.name, len);",
                "\t\t\tretnamlen += len;",
                "\t\t\tretname[retnamlen] = '\\0';"
            ],
            "deleted": [
                "\t\t\tif ((strlen(retname) + rr->len - 5) >= 254) {",
                "\t\t\tstrncat(retname, rr->u.NM.name, rr->len - 5);",
                "\t\t\tretnamlen += rr->len - 5;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The get_rock_ridge_filename function in fs/isofs/rock.c in the Linux kernel before 4.5.5 mishandles NM (aka alternate name) entries containing \\0 characters, which allows local users to obtain sensitive information from kernel memory or possibly have unspecified other impact via a crafted isofs filesystem.",
        "id": 1037
    },
    {
        "cve_id": "CVE-2015-4176",
        "code_before_change": "static void umount_tree(struct mount *mnt, enum umount_tree_flags how)\n{\n\tLIST_HEAD(tmp_list);\n\tstruct mount *p;\n\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_mount_unlock(mnt);\n\n\t/* Gather the mounts to umount */\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tp->mnt.mnt_flags |= MNT_UMOUNT;\n\t\tlist_move(&p->mnt_list, &tmp_list);\n\t}\n\n\t/* Hide the mounts from mnt_mounts */\n\tlist_for_each_entry(p, &tmp_list, mnt_list) {\n\t\tlist_del_init(&p->mnt_child);\n\t}\n\n\t/* Add propogated mounts to the tmp_list */\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_umount(&tmp_list);\n\n\twhile (!list_empty(&tmp_list)) {\n\t\tbool disconnect;\n\t\tp = list_first_entry(&tmp_list, struct mount, mnt_list);\n\t\tlist_del_init(&p->mnt_expire);\n\t\tlist_del_init(&p->mnt_list);\n\t\t__touch_mnt_namespace(p->mnt_ns);\n\t\tp->mnt_ns = NULL;\n\t\tif (how & UMOUNT_SYNC)\n\t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;\n\n\t\tdisconnect = !IS_MNT_LOCKED_AND_LAZY(p);\n\n\t\tpin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt,\n\t\t\t\t disconnect ? &unmounted : NULL);\n\t\tif (mnt_has_parent(p)) {\n\t\t\tmnt_add_count(p->mnt_parent, -1);\n\t\t\tif (!disconnect) {\n\t\t\t\t/* Don't forget about p */\n\t\t\t\tlist_add_tail(&p->mnt_child, &p->mnt_parent->mnt_mounts);\n\t\t\t} else {\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\tchange_mnt_propagation(p, MS_PRIVATE);\n\t}\n}",
        "code_after_change": "static void umount_tree(struct mount *mnt, enum umount_tree_flags how)\n{\n\tLIST_HEAD(tmp_list);\n\tstruct mount *p;\n\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_mount_unlock(mnt);\n\n\t/* Gather the mounts to umount */\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tp->mnt.mnt_flags |= MNT_UMOUNT;\n\t\tlist_move(&p->mnt_list, &tmp_list);\n\t}\n\n\t/* Hide the mounts from mnt_mounts */\n\tlist_for_each_entry(p, &tmp_list, mnt_list) {\n\t\tlist_del_init(&p->mnt_child);\n\t}\n\n\t/* Add propogated mounts to the tmp_list */\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_umount(&tmp_list);\n\n\twhile (!list_empty(&tmp_list)) {\n\t\tbool disconnect;\n\t\tp = list_first_entry(&tmp_list, struct mount, mnt_list);\n\t\tlist_del_init(&p->mnt_expire);\n\t\tlist_del_init(&p->mnt_list);\n\t\t__touch_mnt_namespace(p->mnt_ns);\n\t\tp->mnt_ns = NULL;\n\t\tif (how & UMOUNT_SYNC)\n\t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;\n\n\t\tdisconnect = !(((how & UMOUNT_CONNECTED) &&\n\t\t\t\tmnt_has_parent(p) &&\n\t\t\t\t(p->mnt_parent->mnt.mnt_flags & MNT_UMOUNT)) ||\n\t\t\t       IS_MNT_LOCKED_AND_LAZY(p));\n\n\t\tpin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt,\n\t\t\t\t disconnect ? &unmounted : NULL);\n\t\tif (mnt_has_parent(p)) {\n\t\t\tmnt_add_count(p->mnt_parent, -1);\n\t\t\tif (!disconnect) {\n\t\t\t\t/* Don't forget about p */\n\t\t\t\tlist_add_tail(&p->mnt_child, &p->mnt_parent->mnt_mounts);\n\t\t\t} else {\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\tchange_mnt_propagation(p, MS_PRIVATE);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,7 +31,10 @@\n \t\tif (how & UMOUNT_SYNC)\n \t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;\n \n-\t\tdisconnect = !IS_MNT_LOCKED_AND_LAZY(p);\n+\t\tdisconnect = !(((how & UMOUNT_CONNECTED) &&\n+\t\t\t\tmnt_has_parent(p) &&\n+\t\t\t\t(p->mnt_parent->mnt.mnt_flags & MNT_UMOUNT)) ||\n+\t\t\t       IS_MNT_LOCKED_AND_LAZY(p));\n \n \t\tpin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt,\n \t\t\t\t disconnect ? &unmounted : NULL);",
        "function_modified_lines": {
            "added": [
                "\t\tdisconnect = !(((how & UMOUNT_CONNECTED) &&",
                "\t\t\t\tmnt_has_parent(p) &&",
                "\t\t\t\t(p->mnt_parent->mnt.mnt_flags & MNT_UMOUNT)) ||",
                "\t\t\t       IS_MNT_LOCKED_AND_LAZY(p));"
            ],
            "deleted": [
                "\t\tdisconnect = !IS_MNT_LOCKED_AND_LAZY(p);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "fs/namespace.c in the Linux kernel before 4.0.2 does not properly support mount connectivity, which allows local users to read arbitrary files by leveraging user-namespace root access for deletion of a file or directory.",
        "id": 766
    },
    {
        "cve_id": "CVE-2012-6536",
        "code_before_change": "static int xfrm_alloc_replay_state_esn(struct xfrm_replay_state_esn **replay_esn,\n\t\t\t\t       struct xfrm_replay_state_esn **preplay_esn,\n\t\t\t\t       struct nlattr *rta)\n{\n\tstruct xfrm_replay_state_esn *p, *pp, *up;\n\n\tif (!rta)\n\t\treturn 0;\n\n\tup = nla_data(rta);\n\n\tp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tpp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);\n\tif (!pp) {\n\t\tkfree(p);\n\t\treturn -ENOMEM;\n\t}\n\n\t*replay_esn = p;\n\t*preplay_esn = pp;\n\n\treturn 0;\n}",
        "code_after_change": "static int xfrm_alloc_replay_state_esn(struct xfrm_replay_state_esn **replay_esn,\n\t\t\t\t       struct xfrm_replay_state_esn **preplay_esn,\n\t\t\t\t       struct nlattr *rta)\n{\n\tstruct xfrm_replay_state_esn *p, *pp, *up;\n\tint klen, ulen;\n\n\tif (!rta)\n\t\treturn 0;\n\n\tup = nla_data(rta);\n\tklen = xfrm_replay_state_esn_len(up);\n\tulen = nla_len(rta) >= klen ? klen : sizeof(*up);\n\n\tp = kzalloc(klen, GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tpp = kzalloc(klen, GFP_KERNEL);\n\tif (!pp) {\n\t\tkfree(p);\n\t\treturn -ENOMEM;\n\t}\n\n\tmemcpy(p, up, ulen);\n\tmemcpy(pp, up, ulen);\n\n\t*replay_esn = p;\n\t*preplay_esn = pp;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,21 +3,27 @@\n \t\t\t\t       struct nlattr *rta)\n {\n \tstruct xfrm_replay_state_esn *p, *pp, *up;\n+\tint klen, ulen;\n \n \tif (!rta)\n \t\treturn 0;\n \n \tup = nla_data(rta);\n+\tklen = xfrm_replay_state_esn_len(up);\n+\tulen = nla_len(rta) >= klen ? klen : sizeof(*up);\n \n-\tp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);\n+\tp = kzalloc(klen, GFP_KERNEL);\n \tif (!p)\n \t\treturn -ENOMEM;\n \n-\tpp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);\n+\tpp = kzalloc(klen, GFP_KERNEL);\n \tif (!pp) {\n \t\tkfree(p);\n \t\treturn -ENOMEM;\n \t}\n+\n+\tmemcpy(p, up, ulen);\n+\tmemcpy(pp, up, ulen);\n \n \t*replay_esn = p;\n \t*preplay_esn = pp;",
        "function_modified_lines": {
            "added": [
                "\tint klen, ulen;",
                "\tklen = xfrm_replay_state_esn_len(up);",
                "\tulen = nla_len(rta) >= klen ? klen : sizeof(*up);",
                "\tp = kzalloc(klen, GFP_KERNEL);",
                "\tpp = kzalloc(klen, GFP_KERNEL);",
                "",
                "\tmemcpy(p, up, ulen);",
                "\tmemcpy(pp, up, ulen);"
            ],
            "deleted": [
                "\tp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);",
                "\tpp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "net/xfrm/xfrm_user.c in the Linux kernel before 3.6 does not verify that the actual Netlink message length is consistent with a certain header field, which allows local users to obtain sensitive information from kernel heap memory by leveraging the CAP_NET_ADMIN capability and providing a (1) new or (2) updated state.",
        "id": 120
    },
    {
        "cve_id": "CVE-2022-33741",
        "code_before_change": "static netdev_tx_t xennet_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tstruct netfront_stats *tx_stats = this_cpu_ptr(np->tx_stats);\n\tstruct xen_netif_tx_request *first_tx;\n\tunsigned int i;\n\tint notify;\n\tint slots;\n\tstruct page *page;\n\tunsigned int offset;\n\tunsigned int len;\n\tunsigned long flags;\n\tstruct netfront_queue *queue = NULL;\n\tstruct xennet_gnttab_make_txreq info = { };\n\tunsigned int num_queues = dev->real_num_tx_queues;\n\tu16 queue_index;\n\tstruct sk_buff *nskb;\n\n\t/* Drop the packet if no queues are set up */\n\tif (num_queues < 1)\n\t\tgoto drop;\n\tif (unlikely(np->broken))\n\t\tgoto drop;\n\t/* Determine which queue to transmit this SKB on */\n\tqueue_index = skb_get_queue_mapping(skb);\n\tqueue = &np->queues[queue_index];\n\n\t/* If skb->len is too big for wire format, drop skb and alert\n\t * user about misconfiguration.\n\t */\n\tif (unlikely(skb->len > XEN_NETIF_MAX_TX_SIZE)) {\n\t\tnet_alert_ratelimited(\n\t\t\t\"xennet: skb->len = %u, too big for wire format\\n\",\n\t\t\tskb->len);\n\t\tgoto drop;\n\t}\n\n\tslots = xennet_count_skb_slots(skb);\n\tif (unlikely(slots > MAX_XEN_SKB_FRAGS + 1)) {\n\t\tnet_dbg_ratelimited(\"xennet: skb rides the rocket: %d slots, %d bytes\\n\",\n\t\t\t\t    slots, skb->len);\n\t\tif (skb_linearize(skb))\n\t\t\tgoto drop;\n\t}\n\n\tpage = virt_to_page(skb->data);\n\toffset = offset_in_page(skb->data);\n\n\t/* The first req should be at least ETH_HLEN size or the packet will be\n\t * dropped by netback.\n\t */\n\tif (unlikely(PAGE_SIZE - offset < ETH_HLEN)) {\n\t\tnskb = skb_copy(skb, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\tgoto drop;\n\t\tdev_consume_skb_any(skb);\n\t\tskb = nskb;\n\t\tpage = virt_to_page(skb->data);\n\t\toffset = offset_in_page(skb->data);\n\t}\n\n\tlen = skb_headlen(skb);\n\n\tspin_lock_irqsave(&queue->tx_lock, flags);\n\n\tif (unlikely(!netif_carrier_ok(dev) ||\n\t\t     (slots > 1 && !xennet_can_sg(dev)) ||\n\t\t     netif_needs_gso(skb, netif_skb_features(skb)))) {\n\t\tspin_unlock_irqrestore(&queue->tx_lock, flags);\n\t\tgoto drop;\n\t}\n\n\t/* First request for the linear area. */\n\tinfo.queue = queue;\n\tinfo.skb = skb;\n\tinfo.page = page;\n\tfirst_tx = xennet_make_first_txreq(&info, offset, len);\n\toffset += info.tx_local.size;\n\tif (offset == PAGE_SIZE) {\n\t\tpage++;\n\t\toffset = 0;\n\t}\n\tlen -= info.tx_local.size;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t/* local packet? */\n\t\tfirst_tx->flags |= XEN_NETTXF_csum_blank |\n\t\t\t\t   XEN_NETTXF_data_validated;\n\telse if (skb->ip_summed == CHECKSUM_UNNECESSARY)\n\t\t/* remote but checksummed. */\n\t\tfirst_tx->flags |= XEN_NETTXF_data_validated;\n\n\t/* Optional extra info after the first request. */\n\tif (skb_shinfo(skb)->gso_size) {\n\t\tstruct xen_netif_extra_info *gso;\n\n\t\tgso = (struct xen_netif_extra_info *)\n\t\t\tRING_GET_REQUEST(&queue->tx, queue->tx.req_prod_pvt++);\n\n\t\tfirst_tx->flags |= XEN_NETTXF_extra_info;\n\n\t\tgso->u.gso.size = skb_shinfo(skb)->gso_size;\n\t\tgso->u.gso.type = (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6) ?\n\t\t\tXEN_NETIF_GSO_TYPE_TCPV6 :\n\t\t\tXEN_NETIF_GSO_TYPE_TCPV4;\n\t\tgso->u.gso.pad = 0;\n\t\tgso->u.gso.features = 0;\n\n\t\tgso->type = XEN_NETIF_EXTRA_TYPE_GSO;\n\t\tgso->flags = 0;\n\t}\n\n\t/* Requests for the rest of the linear area. */\n\txennet_make_txreqs(&info, page, offset, len);\n\n\t/* Requests for all the frags. */\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\txennet_make_txreqs(&info, skb_frag_page(frag),\n\t\t\t\t\tskb_frag_off(frag),\n\t\t\t\t\tskb_frag_size(frag));\n\t}\n\n\t/* First request has the packet length. */\n\tfirst_tx->size = skb->len;\n\n\t/* timestamp packet in software */\n\tskb_tx_timestamp(skb);\n\n\txennet_mark_tx_pending(queue);\n\n\tRING_PUSH_REQUESTS_AND_CHECK_NOTIFY(&queue->tx, notify);\n\tif (notify)\n\t\tnotify_remote_via_irq(queue->tx_irq);\n\n\tu64_stats_update_begin(&tx_stats->syncp);\n\ttx_stats->bytes += skb->len;\n\ttx_stats->packets++;\n\tu64_stats_update_end(&tx_stats->syncp);\n\n\t/* Note: It is not safe to access skb after xennet_tx_buf_gc()! */\n\txennet_tx_buf_gc(queue);\n\n\tif (!netfront_tx_slot_available(queue))\n\t\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, queue->id));\n\n\tspin_unlock_irqrestore(&queue->tx_lock, flags);\n\n\treturn NETDEV_TX_OK;\n\n drop:\n\tdev->stats.tx_dropped++;\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}",
        "code_after_change": "static netdev_tx_t xennet_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tstruct netfront_stats *tx_stats = this_cpu_ptr(np->tx_stats);\n\tstruct xen_netif_tx_request *first_tx;\n\tunsigned int i;\n\tint notify;\n\tint slots;\n\tstruct page *page;\n\tunsigned int offset;\n\tunsigned int len;\n\tunsigned long flags;\n\tstruct netfront_queue *queue = NULL;\n\tstruct xennet_gnttab_make_txreq info = { };\n\tunsigned int num_queues = dev->real_num_tx_queues;\n\tu16 queue_index;\n\tstruct sk_buff *nskb;\n\n\t/* Drop the packet if no queues are set up */\n\tif (num_queues < 1)\n\t\tgoto drop;\n\tif (unlikely(np->broken))\n\t\tgoto drop;\n\t/* Determine which queue to transmit this SKB on */\n\tqueue_index = skb_get_queue_mapping(skb);\n\tqueue = &np->queues[queue_index];\n\n\t/* If skb->len is too big for wire format, drop skb and alert\n\t * user about misconfiguration.\n\t */\n\tif (unlikely(skb->len > XEN_NETIF_MAX_TX_SIZE)) {\n\t\tnet_alert_ratelimited(\n\t\t\t\"xennet: skb->len = %u, too big for wire format\\n\",\n\t\t\tskb->len);\n\t\tgoto drop;\n\t}\n\n\tslots = xennet_count_skb_slots(skb);\n\tif (unlikely(slots > MAX_XEN_SKB_FRAGS + 1)) {\n\t\tnet_dbg_ratelimited(\"xennet: skb rides the rocket: %d slots, %d bytes\\n\",\n\t\t\t\t    slots, skb->len);\n\t\tif (skb_linearize(skb))\n\t\t\tgoto drop;\n\t}\n\n\tpage = virt_to_page(skb->data);\n\toffset = offset_in_page(skb->data);\n\n\t/* The first req should be at least ETH_HLEN size or the packet will be\n\t * dropped by netback.\n\t *\n\t * If the backend is not trusted bounce all data to zeroed pages to\n\t * avoid exposing contiguous data on the granted page not belonging to\n\t * the skb.\n\t */\n\tif (np->bounce || unlikely(PAGE_SIZE - offset < ETH_HLEN)) {\n\t\tnskb = bounce_skb(skb);\n\t\tif (!nskb)\n\t\t\tgoto drop;\n\t\tdev_consume_skb_any(skb);\n\t\tskb = nskb;\n\t\tpage = virt_to_page(skb->data);\n\t\toffset = offset_in_page(skb->data);\n\t}\n\n\tlen = skb_headlen(skb);\n\n\tspin_lock_irqsave(&queue->tx_lock, flags);\n\n\tif (unlikely(!netif_carrier_ok(dev) ||\n\t\t     (slots > 1 && !xennet_can_sg(dev)) ||\n\t\t     netif_needs_gso(skb, netif_skb_features(skb)))) {\n\t\tspin_unlock_irqrestore(&queue->tx_lock, flags);\n\t\tgoto drop;\n\t}\n\n\t/* First request for the linear area. */\n\tinfo.queue = queue;\n\tinfo.skb = skb;\n\tinfo.page = page;\n\tfirst_tx = xennet_make_first_txreq(&info, offset, len);\n\toffset += info.tx_local.size;\n\tif (offset == PAGE_SIZE) {\n\t\tpage++;\n\t\toffset = 0;\n\t}\n\tlen -= info.tx_local.size;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t/* local packet? */\n\t\tfirst_tx->flags |= XEN_NETTXF_csum_blank |\n\t\t\t\t   XEN_NETTXF_data_validated;\n\telse if (skb->ip_summed == CHECKSUM_UNNECESSARY)\n\t\t/* remote but checksummed. */\n\t\tfirst_tx->flags |= XEN_NETTXF_data_validated;\n\n\t/* Optional extra info after the first request. */\n\tif (skb_shinfo(skb)->gso_size) {\n\t\tstruct xen_netif_extra_info *gso;\n\n\t\tgso = (struct xen_netif_extra_info *)\n\t\t\tRING_GET_REQUEST(&queue->tx, queue->tx.req_prod_pvt++);\n\n\t\tfirst_tx->flags |= XEN_NETTXF_extra_info;\n\n\t\tgso->u.gso.size = skb_shinfo(skb)->gso_size;\n\t\tgso->u.gso.type = (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6) ?\n\t\t\tXEN_NETIF_GSO_TYPE_TCPV6 :\n\t\t\tXEN_NETIF_GSO_TYPE_TCPV4;\n\t\tgso->u.gso.pad = 0;\n\t\tgso->u.gso.features = 0;\n\n\t\tgso->type = XEN_NETIF_EXTRA_TYPE_GSO;\n\t\tgso->flags = 0;\n\t}\n\n\t/* Requests for the rest of the linear area. */\n\txennet_make_txreqs(&info, page, offset, len);\n\n\t/* Requests for all the frags. */\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\txennet_make_txreqs(&info, skb_frag_page(frag),\n\t\t\t\t\tskb_frag_off(frag),\n\t\t\t\t\tskb_frag_size(frag));\n\t}\n\n\t/* First request has the packet length. */\n\tfirst_tx->size = skb->len;\n\n\t/* timestamp packet in software */\n\tskb_tx_timestamp(skb);\n\n\txennet_mark_tx_pending(queue);\n\n\tRING_PUSH_REQUESTS_AND_CHECK_NOTIFY(&queue->tx, notify);\n\tif (notify)\n\t\tnotify_remote_via_irq(queue->tx_irq);\n\n\tu64_stats_update_begin(&tx_stats->syncp);\n\ttx_stats->bytes += skb->len;\n\ttx_stats->packets++;\n\tu64_stats_update_end(&tx_stats->syncp);\n\n\t/* Note: It is not safe to access skb after xennet_tx_buf_gc()! */\n\txennet_tx_buf_gc(queue);\n\n\tif (!netfront_tx_slot_available(queue))\n\t\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, queue->id));\n\n\tspin_unlock_irqrestore(&queue->tx_lock, flags);\n\n\treturn NETDEV_TX_OK;\n\n drop:\n\tdev->stats.tx_dropped++;\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}",
        "patch": "--- code before\n+++ code after\n@@ -48,9 +48,13 @@\n \n \t/* The first req should be at least ETH_HLEN size or the packet will be\n \t * dropped by netback.\n+\t *\n+\t * If the backend is not trusted bounce all data to zeroed pages to\n+\t * avoid exposing contiguous data on the granted page not belonging to\n+\t * the skb.\n \t */\n-\tif (unlikely(PAGE_SIZE - offset < ETH_HLEN)) {\n-\t\tnskb = skb_copy(skb, GFP_ATOMIC);\n+\tif (np->bounce || unlikely(PAGE_SIZE - offset < ETH_HLEN)) {\n+\t\tnskb = bounce_skb(skb);\n \t\tif (!nskb)\n \t\t\tgoto drop;\n \t\tdev_consume_skb_any(skb);",
        "function_modified_lines": {
            "added": [
                "\t *",
                "\t * If the backend is not trusted bounce all data to zeroed pages to",
                "\t * avoid exposing contiguous data on the granted page not belonging to",
                "\t * the skb.",
                "\tif (np->bounce || unlikely(PAGE_SIZE - offset < ETH_HLEN)) {",
                "\t\tnskb = bounce_skb(skb);"
            ],
            "deleted": [
                "\tif (unlikely(PAGE_SIZE - offset < ETH_HLEN)) {",
                "\t\tnskb = skb_copy(skb, GFP_ATOMIC);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3577
    },
    {
        "cve_id": "CVE-2017-14991",
        "code_before_change": "static void\nsg_fill_request_table(Sg_fd *sfp, sg_req_info_t *rinfo)\n{\n\tSg_request *srp;\n\tint val;\n\tunsigned int ms;\n\n\tval = 0;\n\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\tif (val > SG_MAX_QUEUE)\n\t\t\tbreak;\n\t\tmemset(&rinfo[val], 0, SZ_SG_REQ_INFO);\n\t\trinfo[val].req_state = srp->done + 1;\n\t\trinfo[val].problem =\n\t\t\tsrp->header.masked_status &\n\t\t\tsrp->header.host_status &\n\t\t\tsrp->header.driver_status;\n\t\tif (srp->done)\n\t\t\trinfo[val].duration =\n\t\t\t\tsrp->header.duration;\n\t\telse {\n\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\trinfo[val].duration =\n\t\t\t\t(ms > srp->header.duration) ?\n\t\t\t\t(ms - srp->header.duration) : 0;\n\t\t}\n\t\trinfo[val].orphan = srp->orphan;\n\t\trinfo[val].sg_io_owned = srp->sg_io_owned;\n\t\trinfo[val].pack_id = srp->header.pack_id;\n\t\trinfo[val].usr_ptr = srp->header.usr_ptr;\n\t\tval++;\n\t}\n}",
        "code_after_change": "static void\nsg_fill_request_table(Sg_fd *sfp, sg_req_info_t *rinfo)\n{\n\tSg_request *srp;\n\tint val;\n\tunsigned int ms;\n\n\tval = 0;\n\tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n\t\tif (val > SG_MAX_QUEUE)\n\t\t\tbreak;\n\t\trinfo[val].req_state = srp->done + 1;\n\t\trinfo[val].problem =\n\t\t\tsrp->header.masked_status &\n\t\t\tsrp->header.host_status &\n\t\t\tsrp->header.driver_status;\n\t\tif (srp->done)\n\t\t\trinfo[val].duration =\n\t\t\t\tsrp->header.duration;\n\t\telse {\n\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\trinfo[val].duration =\n\t\t\t\t(ms > srp->header.duration) ?\n\t\t\t\t(ms - srp->header.duration) : 0;\n\t\t}\n\t\trinfo[val].orphan = srp->orphan;\n\t\trinfo[val].sg_io_owned = srp->sg_io_owned;\n\t\trinfo[val].pack_id = srp->header.pack_id;\n\t\trinfo[val].usr_ptr = srp->header.usr_ptr;\n\t\tval++;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,6 @@\n \tlist_for_each_entry(srp, &sfp->rq_list, entry) {\n \t\tif (val > SG_MAX_QUEUE)\n \t\t\tbreak;\n-\t\tmemset(&rinfo[val], 0, SZ_SG_REQ_INFO);\n \t\trinfo[val].req_state = srp->done + 1;\n \t\trinfo[val].problem =\n \t\t\tsrp->header.masked_status &",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tmemset(&rinfo[val], 0, SZ_SG_REQ_INFO);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The sg_ioctl function in drivers/scsi/sg.c in the Linux kernel before 4.13.4 allows local users to obtain sensitive information from uninitialized kernel heap-memory locations via an SG_GET_REQUEST_TABLE ioctl call for /dev/sg0.",
        "id": 1286
    },
    {
        "cve_id": "CVE-2013-3222",
        "code_before_change": "int vcc_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\tsize_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct atm_vcc *vcc;\n\tstruct sk_buff *skb;\n\tint copied, error = -EINVAL;\n\n\tif (sock->state != SS_CONNECTED)\n\t\treturn -ENOTCONN;\n\n\t/* only handle MSG_DONTWAIT and MSG_PEEK */\n\tif (flags & ~(MSG_DONTWAIT | MSG_PEEK))\n\t\treturn -EOPNOTSUPP;\n\n\tvcc = ATM_SD(sock);\n\tif (test_bit(ATM_VF_RELEASED, &vcc->flags) ||\n\t    test_bit(ATM_VF_CLOSE, &vcc->flags) ||\n\t    !test_bit(ATM_VF_READY, &vcc->flags))\n\t\treturn 0;\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &error);\n\tif (!skb)\n\t\treturn error;\n\n\tcopied = skb->len;\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terror = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (error)\n\t\treturn error;\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tpr_debug(\"%d -= %d\\n\", atomic_read(&sk->sk_rmem_alloc),\n\t\t\t skb->truesize);\n\t\tatm_return(vcc, skb->truesize);\n\t}\n\n\tskb_free_datagram(sk, skb);\n\treturn copied;\n}",
        "code_after_change": "int vcc_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\tsize_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct atm_vcc *vcc;\n\tstruct sk_buff *skb;\n\tint copied, error = -EINVAL;\n\n\tmsg->msg_namelen = 0;\n\n\tif (sock->state != SS_CONNECTED)\n\t\treturn -ENOTCONN;\n\n\t/* only handle MSG_DONTWAIT and MSG_PEEK */\n\tif (flags & ~(MSG_DONTWAIT | MSG_PEEK))\n\t\treturn -EOPNOTSUPP;\n\n\tvcc = ATM_SD(sock);\n\tif (test_bit(ATM_VF_RELEASED, &vcc->flags) ||\n\t    test_bit(ATM_VF_CLOSE, &vcc->flags) ||\n\t    !test_bit(ATM_VF_READY, &vcc->flags))\n\t\treturn 0;\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &error);\n\tif (!skb)\n\t\treturn error;\n\n\tcopied = skb->len;\n\tif (copied > size) {\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terror = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (error)\n\t\treturn error;\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tpr_debug(\"%d -= %d\\n\", atomic_read(&sk->sk_rmem_alloc),\n\t\t\t skb->truesize);\n\t\tatm_return(vcc, skb->truesize);\n\t}\n\n\tskb_free_datagram(sk, skb);\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,8 @@\n \tstruct atm_vcc *vcc;\n \tstruct sk_buff *skb;\n \tint copied, error = -EINVAL;\n+\n+\tmsg->msg_namelen = 0;\n \n \tif (sock->state != SS_CONNECTED)\n \t\treturn -ENOTCONN;",
        "function_modified_lines": {
            "added": [
                "",
                "\tmsg->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The vcc_recvmsg function in net/atm/common.c in the Linux kernel before 3.9-rc7 does not initialize a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 263
    },
    {
        "cve_id": "CVE-2016-4486",
        "code_before_change": "static int rtnl_fill_link_ifmap(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct rtnl_link_ifmap map = {\n\t\t.mem_start   = dev->mem_start,\n\t\t.mem_end     = dev->mem_end,\n\t\t.base_addr   = dev->base_addr,\n\t\t.irq         = dev->irq,\n\t\t.dma         = dev->dma,\n\t\t.port        = dev->if_port,\n\t};\n\tif (nla_put(skb, IFLA_MAP, sizeof(map), &map))\n\t\treturn -EMSGSIZE;\n\n\treturn 0;\n}",
        "code_after_change": "static int rtnl_fill_link_ifmap(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct rtnl_link_ifmap map;\n\n\tmemset(&map, 0, sizeof(map));\n\tmap.mem_start   = dev->mem_start;\n\tmap.mem_end     = dev->mem_end;\n\tmap.base_addr   = dev->base_addr;\n\tmap.irq         = dev->irq;\n\tmap.dma         = dev->dma;\n\tmap.port        = dev->if_port;\n\n\tif (nla_put(skb, IFLA_MAP, sizeof(map), &map))\n\t\treturn -EMSGSIZE;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,13 +1,15 @@\n static int rtnl_fill_link_ifmap(struct sk_buff *skb, struct net_device *dev)\n {\n-\tstruct rtnl_link_ifmap map = {\n-\t\t.mem_start   = dev->mem_start,\n-\t\t.mem_end     = dev->mem_end,\n-\t\t.base_addr   = dev->base_addr,\n-\t\t.irq         = dev->irq,\n-\t\t.dma         = dev->dma,\n-\t\t.port        = dev->if_port,\n-\t};\n+\tstruct rtnl_link_ifmap map;\n+\n+\tmemset(&map, 0, sizeof(map));\n+\tmap.mem_start   = dev->mem_start;\n+\tmap.mem_end     = dev->mem_end;\n+\tmap.base_addr   = dev->base_addr;\n+\tmap.irq         = dev->irq;\n+\tmap.dma         = dev->dma;\n+\tmap.port        = dev->if_port;\n+\n \tif (nla_put(skb, IFLA_MAP, sizeof(map), &map))\n \t\treturn -EMSGSIZE;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct rtnl_link_ifmap map;",
                "",
                "\tmemset(&map, 0, sizeof(map));",
                "\tmap.mem_start   = dev->mem_start;",
                "\tmap.mem_end     = dev->mem_end;",
                "\tmap.base_addr   = dev->base_addr;",
                "\tmap.irq         = dev->irq;",
                "\tmap.dma         = dev->dma;",
                "\tmap.port        = dev->if_port;",
                ""
            ],
            "deleted": [
                "\tstruct rtnl_link_ifmap map = {",
                "\t\t.mem_start   = dev->mem_start,",
                "\t\t.mem_end     = dev->mem_end,",
                "\t\t.base_addr   = dev->base_addr,",
                "\t\t.irq         = dev->irq,",
                "\t\t.dma         = dev->dma,",
                "\t\t.port        = dev->if_port,",
                "\t};"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The rtnl_fill_link_ifmap function in net/core/rtnetlink.c in the Linux kernel before 4.5.5 does not initialize a certain data structure, which allows local users to obtain sensitive information from kernel stack memory by reading a Netlink message.",
        "id": 1020
    },
    {
        "cve_id": "CVE-2013-3229",
        "code_before_change": "static int iucv_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t     struct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tunsigned int copied, rlen;\n\tstruct sk_buff *skb, *rskb, *cskb;\n\tint err = 0;\n\n\tif ((sk->sk_state == IUCV_DISCONN) &&\n\t    skb_queue_empty(&iucv->backlog_skb_q) &&\n\t    skb_queue_empty(&sk->sk_receive_queue) &&\n\t    list_empty(&iucv->message_q.list))\n\t\treturn 0;\n\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\t/* receive/dequeue next skb:\n\t * the function understands MSG_PEEK and, thus, does not dequeue skb */\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb) {\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\treturn 0;\n\t\treturn err;\n\t}\n\n\trlen   = skb->len;\t\t/* real length of skb */\n\tcopied = min_t(unsigned int, rlen, len);\n\tif (!rlen)\n\t\tsk->sk_shutdown = sk->sk_shutdown | RCV_SHUTDOWN;\n\n\tcskb = skb;\n\tif (skb_copy_datagram_iovec(cskb, 0, msg->msg_iov, copied)) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn -EFAULT;\n\t}\n\n\t/* SOCK_SEQPACKET: set MSG_TRUNC if recv buf size is too small */\n\tif (sk->sk_type == SOCK_SEQPACKET) {\n\t\tif (copied < rlen)\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\t/* each iucv message contains a complete record */\n\t\tmsg->msg_flags |= MSG_EOR;\n\t}\n\n\t/* create control message to store iucv msg target class:\n\t * get the trgcls from the control buffer of the skb due to\n\t * fragmentation of original iucv message. */\n\terr = put_cmsg(msg, SOL_IUCV, SCM_IUCV_TRGCLS,\n\t\t\tCB_TRGCLS_LEN, CB_TRGCLS(skb));\n\tif (err) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn err;\n\t}\n\n\t/* Mark read part of skb as used */\n\tif (!(flags & MSG_PEEK)) {\n\n\t\t/* SOCK_STREAM: re-queue skb if it contains unreceived data */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tskb_pull(skb, copied);\n\t\t\tif (skb->len) {\n\t\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\n\t\tkfree_skb(skb);\n\t\tif (iucv->transport == AF_IUCV_TRANS_HIPER) {\n\t\t\tatomic_inc(&iucv->msg_recv);\n\t\t\tif (atomic_read(&iucv->msg_recv) > iucv->msglimit) {\n\t\t\t\tWARN_ON(1);\n\t\t\t\tiucv_sock_close(sk);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t}\n\n\t\t/* Queue backlog skbs */\n\t\tspin_lock_bh(&iucv->message_q.lock);\n\t\trskb = skb_dequeue(&iucv->backlog_skb_q);\n\t\twhile (rskb) {\n\t\t\tif (sock_queue_rcv_skb(sk, rskb)) {\n\t\t\t\tskb_queue_head(&iucv->backlog_skb_q,\n\t\t\t\t\t\trskb);\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\trskb = skb_dequeue(&iucv->backlog_skb_q);\n\t\t\t}\n\t\t}\n\t\tif (skb_queue_empty(&iucv->backlog_skb_q)) {\n\t\t\tif (!list_empty(&iucv->message_q.list))\n\t\t\t\tiucv_process_message_q(sk);\n\t\t\tif (atomic_read(&iucv->msg_recv) >=\n\t\t\t\t\t\t\tiucv->msglimit / 2) {\n\t\t\t\terr = iucv_send_ctrl(sk, AF_IUCV_FLAG_WIN);\n\t\t\t\tif (err) {\n\t\t\t\t\tsk->sk_state = IUCV_DISCONN;\n\t\t\t\t\tsk->sk_state_change(sk);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_unlock_bh(&iucv->message_q.lock);\n\t}\n\ndone:\n\t/* SOCK_SEQPACKET: return real length if MSG_TRUNC is set */\n\tif (sk->sk_type == SOCK_SEQPACKET && (flags & MSG_TRUNC))\n\t\tcopied = rlen;\n\n\treturn copied;\n}",
        "code_after_change": "static int iucv_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t     struct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tunsigned int copied, rlen;\n\tstruct sk_buff *skb, *rskb, *cskb;\n\tint err = 0;\n\n\tmsg->msg_namelen = 0;\n\n\tif ((sk->sk_state == IUCV_DISCONN) &&\n\t    skb_queue_empty(&iucv->backlog_skb_q) &&\n\t    skb_queue_empty(&sk->sk_receive_queue) &&\n\t    list_empty(&iucv->message_q.list))\n\t\treturn 0;\n\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\t/* receive/dequeue next skb:\n\t * the function understands MSG_PEEK and, thus, does not dequeue skb */\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb) {\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\treturn 0;\n\t\treturn err;\n\t}\n\n\trlen   = skb->len;\t\t/* real length of skb */\n\tcopied = min_t(unsigned int, rlen, len);\n\tif (!rlen)\n\t\tsk->sk_shutdown = sk->sk_shutdown | RCV_SHUTDOWN;\n\n\tcskb = skb;\n\tif (skb_copy_datagram_iovec(cskb, 0, msg->msg_iov, copied)) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn -EFAULT;\n\t}\n\n\t/* SOCK_SEQPACKET: set MSG_TRUNC if recv buf size is too small */\n\tif (sk->sk_type == SOCK_SEQPACKET) {\n\t\tif (copied < rlen)\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\t/* each iucv message contains a complete record */\n\t\tmsg->msg_flags |= MSG_EOR;\n\t}\n\n\t/* create control message to store iucv msg target class:\n\t * get the trgcls from the control buffer of the skb due to\n\t * fragmentation of original iucv message. */\n\terr = put_cmsg(msg, SOL_IUCV, SCM_IUCV_TRGCLS,\n\t\t\tCB_TRGCLS_LEN, CB_TRGCLS(skb));\n\tif (err) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn err;\n\t}\n\n\t/* Mark read part of skb as used */\n\tif (!(flags & MSG_PEEK)) {\n\n\t\t/* SOCK_STREAM: re-queue skb if it contains unreceived data */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tskb_pull(skb, copied);\n\t\t\tif (skb->len) {\n\t\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\n\t\tkfree_skb(skb);\n\t\tif (iucv->transport == AF_IUCV_TRANS_HIPER) {\n\t\t\tatomic_inc(&iucv->msg_recv);\n\t\t\tif (atomic_read(&iucv->msg_recv) > iucv->msglimit) {\n\t\t\t\tWARN_ON(1);\n\t\t\t\tiucv_sock_close(sk);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t}\n\n\t\t/* Queue backlog skbs */\n\t\tspin_lock_bh(&iucv->message_q.lock);\n\t\trskb = skb_dequeue(&iucv->backlog_skb_q);\n\t\twhile (rskb) {\n\t\t\tif (sock_queue_rcv_skb(sk, rskb)) {\n\t\t\t\tskb_queue_head(&iucv->backlog_skb_q,\n\t\t\t\t\t\trskb);\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\trskb = skb_dequeue(&iucv->backlog_skb_q);\n\t\t\t}\n\t\t}\n\t\tif (skb_queue_empty(&iucv->backlog_skb_q)) {\n\t\t\tif (!list_empty(&iucv->message_q.list))\n\t\t\t\tiucv_process_message_q(sk);\n\t\t\tif (atomic_read(&iucv->msg_recv) >=\n\t\t\t\t\t\t\tiucv->msglimit / 2) {\n\t\t\t\terr = iucv_send_ctrl(sk, AF_IUCV_FLAG_WIN);\n\t\t\t\tif (err) {\n\t\t\t\t\tsk->sk_state = IUCV_DISCONN;\n\t\t\t\t\tsk->sk_state_change(sk);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_unlock_bh(&iucv->message_q.lock);\n\t}\n\ndone:\n\t/* SOCK_SEQPACKET: return real length if MSG_TRUNC is set */\n\tif (sk->sk_type == SOCK_SEQPACKET && (flags & MSG_TRUNC))\n\t\tcopied = rlen;\n\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,8 @@\n \tunsigned int copied, rlen;\n \tstruct sk_buff *skb, *rskb, *cskb;\n \tint err = 0;\n+\n+\tmsg->msg_namelen = 0;\n \n \tif ((sk->sk_state == IUCV_DISCONN) &&\n \t    skb_queue_empty(&iucv->backlog_skb_q) &&",
        "function_modified_lines": {
            "added": [
                "",
                "\tmsg->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The iucv_sock_recvmsg function in net/iucv/af_iucv.c in the Linux kernel before 3.9-rc7 does not initialize a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 269
    },
    {
        "cve_id": "CVE-2016-9756",
        "code_before_change": "static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel, old_sel;\n\tstruct desc_struct old_desc, new_desc;\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\n\t/* Assignment of RIP may only fail in 64-bit mode */\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tops->get_segment(ctxt, &old_sel, &old_desc, NULL,\n\t\t\t\t VCPU_SREG_CS);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,\n\t\t\t\t       X86_TRANSFER_CALL_JMP,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\n\tif (rc != X86EMUL_CONTINUE) {\n\t\tWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);\n\t\t/* assigning eip failed; restore the old cs */\n\t\tops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);\n\t\treturn rc;\n\t}\n\treturn rc;\n}",
        "code_after_change": "static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned short sel;\n\tstruct desc_struct new_desc;\n\tu8 cpl = ctxt->ops->cpl(ctxt);\n\n\tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n\n\trc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,\n\t\t\t\t       X86_TRANSFER_CALL_JMP,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\n\t/* Error handling is not implemented. */\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,15 +1,9 @@\n static int em_jmp_far(struct x86_emulate_ctxt *ctxt)\n {\n \tint rc;\n-\tunsigned short sel, old_sel;\n-\tstruct desc_struct old_desc, new_desc;\n-\tconst struct x86_emulate_ops *ops = ctxt->ops;\n+\tunsigned short sel;\n+\tstruct desc_struct new_desc;\n \tu8 cpl = ctxt->ops->cpl(ctxt);\n-\n-\t/* Assignment of RIP may only fail in 64-bit mode */\n-\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n-\t\tops->get_segment(ctxt, &old_sel, &old_desc, NULL,\n-\t\t\t\t VCPU_SREG_CS);\n \n \tmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\n \n@@ -20,11 +14,9 @@\n \t\treturn rc;\n \n \trc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\n-\tif (rc != X86EMUL_CONTINUE) {\n-\t\tWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);\n-\t\t/* assigning eip failed; restore the old cs */\n-\t\tops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);\n-\t\treturn rc;\n-\t}\n+\t/* Error handling is not implemented. */\n+\tif (rc != X86EMUL_CONTINUE)\n+\t\treturn X86EMUL_UNHANDLEABLE;\n+\n \treturn rc;\n }",
        "function_modified_lines": {
            "added": [
                "\tunsigned short sel;",
                "\tstruct desc_struct new_desc;",
                "\t/* Error handling is not implemented. */",
                "\tif (rc != X86EMUL_CONTINUE)",
                "\t\treturn X86EMUL_UNHANDLEABLE;",
                ""
            ],
            "deleted": [
                "\tunsigned short sel, old_sel;",
                "\tstruct desc_struct old_desc, new_desc;",
                "\tconst struct x86_emulate_ops *ops = ctxt->ops;",
                "",
                "\t/* Assignment of RIP may only fail in 64-bit mode */",
                "\tif (ctxt->mode == X86EMUL_MODE_PROT64)",
                "\t\tops->get_segment(ctxt, &old_sel, &old_desc, NULL,",
                "\t\t\t\t VCPU_SREG_CS);",
                "\tif (rc != X86EMUL_CONTINUE) {",
                "\t\tWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);",
                "\t\t/* assigning eip failed; restore the old cs */",
                "\t\tops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);",
                "\t\treturn rc;",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "arch/x86/kvm/emulate.c in the Linux kernel before 4.8.12 does not properly initialize Code Segment (CS) in certain error cases, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 1165
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "static inline void drop_init_fpu(struct task_struct *tsk)\n{\n\tif (!use_xsave())\n\t\tdrop_fpu(tsk);\n\telse\n\t\txrstor_state(init_xstate_buf, -1);\n}",
        "code_after_change": "static inline void drop_init_fpu(struct task_struct *tsk)\n{\n\tif (!use_eager_fpu())\n\t\tdrop_fpu(tsk);\n\telse {\n\t\tif (use_xsave())\n\t\t\txrstor_state(init_xstate_buf, -1);\n\t\telse\n\t\t\tfxrstor_checking(&init_xstate_buf->i387);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,11 @@\n static inline void drop_init_fpu(struct task_struct *tsk)\n {\n-\tif (!use_xsave())\n+\tif (!use_eager_fpu())\n \t\tdrop_fpu(tsk);\n-\telse\n-\t\txrstor_state(init_xstate_buf, -1);\n+\telse {\n+\t\tif (use_xsave())\n+\t\t\txrstor_state(init_xstate_buf, -1);\n+\t\telse\n+\t\t\tfxrstor_checking(&init_xstate_buf->i387);\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!use_eager_fpu())",
                "\telse {",
                "\t\tif (use_xsave())",
                "\t\t\txrstor_state(init_xstate_buf, -1);",
                "\t\telse",
                "\t\t\tfxrstor_checking(&init_xstate_buf->i387);",
                "\t}"
            ],
            "deleted": [
                "\tif (!use_xsave())",
                "\telse",
                "\t\txrstor_state(init_xstate_buf, -1);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1800
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "void kernel_fpu_end(void)\n{\n\tif (use_xsave())\n\t\tmath_state_restore();\n\telse\n\t\tstts();\n\tpreempt_enable();\n}",
        "code_after_change": "void kernel_fpu_end(void)\n{\n\tif (use_eager_fpu())\n\t\tmath_state_restore();\n\telse\n\t\tstts();\n\tpreempt_enable();\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n void kernel_fpu_end(void)\n {\n-\tif (use_xsave())\n+\tif (use_eager_fpu())\n \t\tmath_state_restore();\n \telse\n \t\tstts();",
        "function_modified_lines": {
            "added": [
                "\tif (use_eager_fpu())"
            ],
            "deleted": [
                "\tif (use_xsave())"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1808
    },
    {
        "cve_id": "CVE-2018-16862",
        "code_before_change": "void truncate_inode_pages_final(struct address_space *mapping)\n{\n\tunsigned long nrexceptional;\n\tunsigned long nrpages;\n\n\t/*\n\t * Page reclaim can not participate in regular inode lifetime\n\t * management (can't call iput()) and thus can race with the\n\t * inode teardown.  Tell it when the address space is exiting,\n\t * so that it does not install eviction information after the\n\t * final truncate has begun.\n\t */\n\tmapping_set_exiting(mapping);\n\n\t/*\n\t * When reclaim installs eviction entries, it increases\n\t * nrexceptional first, then decreases nrpages.  Make sure we see\n\t * this in the right order or we might miss an entry.\n\t */\n\tnrpages = mapping->nrpages;\n\tsmp_rmb();\n\tnrexceptional = mapping->nrexceptional;\n\n\tif (nrpages || nrexceptional) {\n\t\t/*\n\t\t * As truncation uses a lockless tree lookup, cycle\n\t\t * the tree lock to make sure any ongoing tree\n\t\t * modification that does not see AS_EXITING is\n\t\t * completed before starting the final truncate.\n\t\t */\n\t\txa_lock_irq(&mapping->i_pages);\n\t\txa_unlock_irq(&mapping->i_pages);\n\n\t\ttruncate_inode_pages(mapping, 0);\n\t}\n}",
        "code_after_change": "void truncate_inode_pages_final(struct address_space *mapping)\n{\n\tunsigned long nrexceptional;\n\tunsigned long nrpages;\n\n\t/*\n\t * Page reclaim can not participate in regular inode lifetime\n\t * management (can't call iput()) and thus can race with the\n\t * inode teardown.  Tell it when the address space is exiting,\n\t * so that it does not install eviction information after the\n\t * final truncate has begun.\n\t */\n\tmapping_set_exiting(mapping);\n\n\t/*\n\t * When reclaim installs eviction entries, it increases\n\t * nrexceptional first, then decreases nrpages.  Make sure we see\n\t * this in the right order or we might miss an entry.\n\t */\n\tnrpages = mapping->nrpages;\n\tsmp_rmb();\n\tnrexceptional = mapping->nrexceptional;\n\n\tif (nrpages || nrexceptional) {\n\t\t/*\n\t\t * As truncation uses a lockless tree lookup, cycle\n\t\t * the tree lock to make sure any ongoing tree\n\t\t * modification that does not see AS_EXITING is\n\t\t * completed before starting the final truncate.\n\t\t */\n\t\txa_lock_irq(&mapping->i_pages);\n\t\txa_unlock_irq(&mapping->i_pages);\n\t}\n\n\t/*\n\t * Cleancache needs notification even if there are no pages or shadow\n\t * entries.\n\t */\n\ttruncate_inode_pages(mapping, 0);\n}",
        "patch": "--- code before\n+++ code after\n@@ -30,7 +30,11 @@\n \t\t */\n \t\txa_lock_irq(&mapping->i_pages);\n \t\txa_unlock_irq(&mapping->i_pages);\n+\t}\n \n-\t\ttruncate_inode_pages(mapping, 0);\n-\t}\n+\t/*\n+\t * Cleancache needs notification even if there are no pages or shadow\n+\t * entries.\n+\t */\n+\ttruncate_inode_pages(mapping, 0);\n }",
        "function_modified_lines": {
            "added": [
                "\t}",
                "\t/*",
                "\t * Cleancache needs notification even if there are no pages or shadow",
                "\t * entries.",
                "\t */",
                "\ttruncate_inode_pages(mapping, 0);"
            ],
            "deleted": [
                "\t\ttruncate_inode_pages(mapping, 0);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "A security flaw was found in the Linux kernel in a way that the cleancache subsystem clears an inode after the final file truncation (removal). The new file created with the same inode may contain leftover pages from cleancache and the old file data instead of the new one.",
        "id": 1715
    },
    {
        "cve_id": "CVE-2017-2584",
        "code_before_change": "static int em_fxrstor(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct fxregs_state fx_state;\n\tint rc;\n\n\trc = check_fxsr(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = segmented_read(ctxt, ctxt->memop.addr.mem, &fx_state, 512);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tif (fx_state.mxcsr >> 16)\n\t\treturn emulate_gp(ctxt, 0);\n\n\tctxt->ops->get_fpu(ctxt);\n\n\tif (ctxt->mode < X86EMUL_MODE_PROT64)\n\t\trc = fxrstor_fixup(ctxt, &fx_state);\n\n\tif (rc == X86EMUL_CONTINUE)\n\t\trc = asm_safe(\"fxrstor %[fx]\", : [fx] \"m\"(fx_state));\n\n\tctxt->ops->put_fpu(ctxt);\n\n\treturn rc;\n}",
        "code_after_change": "static int em_fxrstor(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct fxregs_state fx_state;\n\tint rc;\n\n\trc = check_fxsr(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\trc = segmented_read_std(ctxt, ctxt->memop.addr.mem, &fx_state, 512);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tif (fx_state.mxcsr >> 16)\n\t\treturn emulate_gp(ctxt, 0);\n\n\tctxt->ops->get_fpu(ctxt);\n\n\tif (ctxt->mode < X86EMUL_MODE_PROT64)\n\t\trc = fxrstor_fixup(ctxt, &fx_state);\n\n\tif (rc == X86EMUL_CONTINUE)\n\t\trc = asm_safe(\"fxrstor %[fx]\", : [fx] \"m\"(fx_state));\n\n\tctxt->ops->put_fpu(ctxt);\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \tif (rc != X86EMUL_CONTINUE)\n \t\treturn rc;\n \n-\trc = segmented_read(ctxt, ctxt->memop.addr.mem, &fx_state, 512);\n+\trc = segmented_read_std(ctxt, ctxt->memop.addr.mem, &fx_state, 512);\n \tif (rc != X86EMUL_CONTINUE)\n \t\treturn rc;\n ",
        "function_modified_lines": {
            "added": [
                "\trc = segmented_read_std(ctxt, ctxt->memop.addr.mem, &fx_state, 512);"
            ],
            "deleted": [
                "\trc = segmented_read(ctxt, ctxt->memop.addr.mem, &fx_state, 512);"
            ]
        },
        "cwe": [
            "CWE-200",
            "CWE-416"
        ],
        "cve_description": "arch/x86/kvm/emulate.c in the Linux kernel through 4.9.3 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free) via a crafted application that leverages instruction emulation for fxrstor, fxsave, sgdt, and sidt.",
        "id": 1444
    },
    {
        "cve_id": "CVE-2013-7281",
        "code_before_change": "static int raw_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = ip_recv_error(sk, msg, len);\n\t\tgoto out;\n\t}\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err)\n\t\treturn err;\n\treturn copied;\n}",
        "code_after_change": "static int raw_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = ip_recv_error(sk, msg, len);\n\t\tgoto out;\n\t}\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t*addr_len = sizeof(*sin);\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err)\n\t\treturn err;\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,9 +9,6 @@\n \n \tif (flags & MSG_OOB)\n \t\tgoto out;\n-\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(*sin);\n \n \tif (flags & MSG_ERRQUEUE) {\n \t\terr = ip_recv_error(sk, msg, len);\n@@ -40,6 +37,7 @@\n \t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n \t\tsin->sin_port = 0;\n \t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n+\t\t*addr_len = sizeof(*sin);\n \t}\n \tif (inet->cmsg_flags)\n \t\tip_cmsg_recv(msg, skb);",
        "function_modified_lines": {
            "added": [
                "\t\t*addr_len = sizeof(*sin);"
            ],
            "deleted": [
                "",
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(*sin);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The dgram_recvmsg function in net/ieee802154/dgram.c in the Linux kernel before 3.12.4 updates a certain length value without ensuring that an associated data structure has been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 408
    },
    {
        "cve_id": "CVE-2016-9756",
        "code_before_change": "static int em_ret_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned long eip, cs;\n\tu16 old_cs;\n\tint cpl = ctxt->ops->cpl(ctxt);\n\tstruct desc_struct old_desc, new_desc;\n\tconst struct x86_emulate_ops *ops = ctxt->ops;\n\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tops->get_segment(ctxt, &old_cs, &old_desc, NULL,\n\t\t\t\t VCPU_SREG_CS);\n\n\trc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = emulate_pop(ctxt, &cs, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\t/* Outer-privilege level return is not implemented */\n\tif (ctxt->mode >= X86EMUL_MODE_PROT16 && (cs & 3) > cpl)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\trc = __load_segment_descriptor(ctxt, (u16)cs, VCPU_SREG_CS, cpl,\n\t\t\t\t       X86_TRANSFER_RET,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = assign_eip_far(ctxt, eip, &new_desc);\n\tif (rc != X86EMUL_CONTINUE) {\n\t\tWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);\n\t\tops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);\n\t}\n\treturn rc;\n}",
        "code_after_change": "static int em_ret_far(struct x86_emulate_ctxt *ctxt)\n{\n\tint rc;\n\tunsigned long eip, cs;\n\tint cpl = ctxt->ops->cpl(ctxt);\n\tstruct desc_struct new_desc;\n\n\trc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = emulate_pop(ctxt, &cs, ctxt->op_bytes);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\t/* Outer-privilege level return is not implemented */\n\tif (ctxt->mode >= X86EMUL_MODE_PROT16 && (cs & 3) > cpl)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\trc = __load_segment_descriptor(ctxt, (u16)cs, VCPU_SREG_CS, cpl,\n\t\t\t\t       X86_TRANSFER_RET,\n\t\t\t\t       &new_desc);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\trc = assign_eip_far(ctxt, eip, &new_desc);\n\t/* Error handling is not implemented. */\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,14 +2,8 @@\n {\n \tint rc;\n \tunsigned long eip, cs;\n-\tu16 old_cs;\n \tint cpl = ctxt->ops->cpl(ctxt);\n-\tstruct desc_struct old_desc, new_desc;\n-\tconst struct x86_emulate_ops *ops = ctxt->ops;\n-\n-\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n-\t\tops->get_segment(ctxt, &old_cs, &old_desc, NULL,\n-\t\t\t\t VCPU_SREG_CS);\n+\tstruct desc_struct new_desc;\n \n \trc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\n \tif (rc != X86EMUL_CONTINUE)\n@@ -26,9 +20,9 @@\n \tif (rc != X86EMUL_CONTINUE)\n \t\treturn rc;\n \trc = assign_eip_far(ctxt, eip, &new_desc);\n-\tif (rc != X86EMUL_CONTINUE) {\n-\t\tWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);\n-\t\tops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);\n-\t}\n+\t/* Error handling is not implemented. */\n+\tif (rc != X86EMUL_CONTINUE)\n+\t\treturn X86EMUL_UNHANDLEABLE;\n+\n \treturn rc;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct desc_struct new_desc;",
                "\t/* Error handling is not implemented. */",
                "\tif (rc != X86EMUL_CONTINUE)",
                "\t\treturn X86EMUL_UNHANDLEABLE;",
                ""
            ],
            "deleted": [
                "\tu16 old_cs;",
                "\tstruct desc_struct old_desc, new_desc;",
                "\tconst struct x86_emulate_ops *ops = ctxt->ops;",
                "",
                "\tif (ctxt->mode == X86EMUL_MODE_PROT64)",
                "\t\tops->get_segment(ctxt, &old_cs, &old_desc, NULL,",
                "\t\t\t\t VCPU_SREG_CS);",
                "\tif (rc != X86EMUL_CONTINUE) {",
                "\t\tWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);",
                "\t\tops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "arch/x86/kvm/emulate.c in the Linux kernel before 4.8.12 does not properly initialize Code Segment (CS) in certain error cases, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 1164
    },
    {
        "cve_id": "CVE-2017-15537",
        "code_before_change": "static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)\n{\n\tint ia32_fxstate = (buf != buf_fx);\n\tstruct task_struct *tsk = current;\n\tstruct fpu *fpu = &tsk->thread.fpu;\n\tint state_size = fpu_kernel_xstate_size;\n\tu64 xfeatures = 0;\n\tint fx_only = 0;\n\n\tia32_fxstate &= (IS_ENABLED(CONFIG_X86_32) ||\n\t\t\t IS_ENABLED(CONFIG_IA32_EMULATION));\n\n\tif (!buf) {\n\t\tfpu__clear(fpu);\n\t\treturn 0;\n\t}\n\n\tif (!access_ok(VERIFY_READ, buf, size))\n\t\treturn -EACCES;\n\n\tfpu__activate_curr(fpu);\n\n\tif (!static_cpu_has(X86_FEATURE_FPU))\n\t\treturn fpregs_soft_set(current, NULL,\n\t\t\t\t       0, sizeof(struct user_i387_ia32_struct),\n\t\t\t\t       NULL, buf) != 0;\n\n\tif (use_xsave()) {\n\t\tstruct _fpx_sw_bytes fx_sw_user;\n\t\tif (unlikely(check_for_xstate(buf_fx, buf_fx, &fx_sw_user))) {\n\t\t\t/*\n\t\t\t * Couldn't find the extended state information in the\n\t\t\t * memory layout. Restore just the FP/SSE and init all\n\t\t\t * the other extended state.\n\t\t\t */\n\t\t\tstate_size = sizeof(struct fxregs_state);\n\t\t\tfx_only = 1;\n\t\t\ttrace_x86_fpu_xstate_check_failed(fpu);\n\t\t} else {\n\t\t\tstate_size = fx_sw_user.xstate_size;\n\t\t\txfeatures = fx_sw_user.xfeatures;\n\t\t}\n\t}\n\n\tif (ia32_fxstate) {\n\t\t/*\n\t\t * For 32-bit frames with fxstate, copy the user state to the\n\t\t * thread's fpu state, reconstruct fxstate from the fsave\n\t\t * header. Sanitize the copied state etc.\n\t\t */\n\t\tstruct fpu *fpu = &tsk->thread.fpu;\n\t\tstruct user_i387_ia32_struct env;\n\t\tint err = 0;\n\n\t\t/*\n\t\t * Drop the current fpu which clears fpu->fpstate_active. This ensures\n\t\t * that any context-switch during the copy of the new state,\n\t\t * avoids the intermediate state from getting restored/saved.\n\t\t * Thus avoiding the new restored state from getting corrupted.\n\t\t * We will be ready to restore/save the state only after\n\t\t * fpu->fpstate_active is again set.\n\t\t */\n\t\tfpu__drop(fpu);\n\n\t\tif (using_compacted_format())\n\t\t\terr = copy_user_to_xstate(&fpu->state.xsave, buf_fx);\n\t\telse\n\t\t\terr = __copy_from_user(&fpu->state.xsave, buf_fx, state_size);\n\n\t\tif (err || __copy_from_user(&env, buf, sizeof(env))) {\n\t\t\tfpstate_init(&fpu->state);\n\t\t\ttrace_x86_fpu_init_state(fpu);\n\t\t\terr = -1;\n\t\t} else {\n\t\t\tsanitize_restored_xstate(tsk, &env, xfeatures, fx_only);\n\t\t}\n\n\t\tfpu->fpstate_active = 1;\n\t\tpreempt_disable();\n\t\tfpu__restore(fpu);\n\t\tpreempt_enable();\n\n\t\treturn err;\n\t} else {\n\t\t/*\n\t\t * For 64-bit frames and 32-bit fsave frames, restore the user\n\t\t * state to the registers directly (with exceptions handled).\n\t\t */\n\t\tuser_fpu_begin();\n\t\tif (copy_user_to_fpregs_zeroing(buf_fx, xfeatures, fx_only)) {\n\t\t\tfpu__clear(fpu);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)\n{\n\tint ia32_fxstate = (buf != buf_fx);\n\tstruct task_struct *tsk = current;\n\tstruct fpu *fpu = &tsk->thread.fpu;\n\tint state_size = fpu_kernel_xstate_size;\n\tu64 xfeatures = 0;\n\tint fx_only = 0;\n\n\tia32_fxstate &= (IS_ENABLED(CONFIG_X86_32) ||\n\t\t\t IS_ENABLED(CONFIG_IA32_EMULATION));\n\n\tif (!buf) {\n\t\tfpu__clear(fpu);\n\t\treturn 0;\n\t}\n\n\tif (!access_ok(VERIFY_READ, buf, size))\n\t\treturn -EACCES;\n\n\tfpu__activate_curr(fpu);\n\n\tif (!static_cpu_has(X86_FEATURE_FPU))\n\t\treturn fpregs_soft_set(current, NULL,\n\t\t\t\t       0, sizeof(struct user_i387_ia32_struct),\n\t\t\t\t       NULL, buf) != 0;\n\n\tif (use_xsave()) {\n\t\tstruct _fpx_sw_bytes fx_sw_user;\n\t\tif (unlikely(check_for_xstate(buf_fx, buf_fx, &fx_sw_user))) {\n\t\t\t/*\n\t\t\t * Couldn't find the extended state information in the\n\t\t\t * memory layout. Restore just the FP/SSE and init all\n\t\t\t * the other extended state.\n\t\t\t */\n\t\t\tstate_size = sizeof(struct fxregs_state);\n\t\t\tfx_only = 1;\n\t\t\ttrace_x86_fpu_xstate_check_failed(fpu);\n\t\t} else {\n\t\t\tstate_size = fx_sw_user.xstate_size;\n\t\t\txfeatures = fx_sw_user.xfeatures;\n\t\t}\n\t}\n\n\tif (ia32_fxstate) {\n\t\t/*\n\t\t * For 32-bit frames with fxstate, copy the user state to the\n\t\t * thread's fpu state, reconstruct fxstate from the fsave\n\t\t * header. Sanitize the copied state etc.\n\t\t */\n\t\tstruct fpu *fpu = &tsk->thread.fpu;\n\t\tstruct user_i387_ia32_struct env;\n\t\tint err = 0;\n\n\t\t/*\n\t\t * Drop the current fpu which clears fpu->fpstate_active. This ensures\n\t\t * that any context-switch during the copy of the new state,\n\t\t * avoids the intermediate state from getting restored/saved.\n\t\t * Thus avoiding the new restored state from getting corrupted.\n\t\t * We will be ready to restore/save the state only after\n\t\t * fpu->fpstate_active is again set.\n\t\t */\n\t\tfpu__drop(fpu);\n\n\t\tif (using_compacted_format()) {\n\t\t\terr = copy_user_to_xstate(&fpu->state.xsave, buf_fx);\n\t\t} else {\n\t\t\terr = __copy_from_user(&fpu->state.xsave, buf_fx, state_size);\n\n\t\t\t/* xcomp_bv must be 0 when using uncompacted format */\n\t\t\tif (!err && state_size > offsetof(struct xregs_state, header) && fpu->state.xsave.header.xcomp_bv)\n\t\t\t\terr = -EINVAL;\n\t\t}\n\n\t\tif (err || __copy_from_user(&env, buf, sizeof(env))) {\n\t\t\tfpstate_init(&fpu->state);\n\t\t\ttrace_x86_fpu_init_state(fpu);\n\t\t\terr = -1;\n\t\t} else {\n\t\t\tsanitize_restored_xstate(tsk, &env, xfeatures, fx_only);\n\t\t}\n\n\t\tfpu->fpstate_active = 1;\n\t\tpreempt_disable();\n\t\tfpu__restore(fpu);\n\t\tpreempt_enable();\n\n\t\treturn err;\n\t} else {\n\t\t/*\n\t\t * For 64-bit frames and 32-bit fsave frames, restore the user\n\t\t * state to the registers directly (with exceptions handled).\n\t\t */\n\t\tuser_fpu_begin();\n\t\tif (copy_user_to_fpregs_zeroing(buf_fx, xfeatures, fx_only)) {\n\t\t\tfpu__clear(fpu);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -62,10 +62,15 @@\n \t\t */\n \t\tfpu__drop(fpu);\n \n-\t\tif (using_compacted_format())\n+\t\tif (using_compacted_format()) {\n \t\t\terr = copy_user_to_xstate(&fpu->state.xsave, buf_fx);\n-\t\telse\n+\t\t} else {\n \t\t\terr = __copy_from_user(&fpu->state.xsave, buf_fx, state_size);\n+\n+\t\t\t/* xcomp_bv must be 0 when using uncompacted format */\n+\t\t\tif (!err && state_size > offsetof(struct xregs_state, header) && fpu->state.xsave.header.xcomp_bv)\n+\t\t\t\terr = -EINVAL;\n+\t\t}\n \n \t\tif (err || __copy_from_user(&env, buf, sizeof(env))) {\n \t\t\tfpstate_init(&fpu->state);",
        "function_modified_lines": {
            "added": [
                "\t\tif (using_compacted_format()) {",
                "\t\t} else {",
                "",
                "\t\t\t/* xcomp_bv must be 0 when using uncompacted format */",
                "\t\t\tif (!err && state_size > offsetof(struct xregs_state, header) && fpu->state.xsave.header.xcomp_bv)",
                "\t\t\t\terr = -EINVAL;",
                "\t\t}"
            ],
            "deleted": [
                "\t\tif (using_compacted_format())",
                "\t\telse"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The x86/fpu (Floating Point Unit) subsystem in the Linux kernel before 4.13.5, when a processor supports the xsave feature but not the xsaves feature, does not correctly handle attempts to set reserved bits in the xstate header via the ptrace() or rt_sigreturn() system call, allowing local users to read the FPU registers of other processes on the system, related to arch/x86/kernel/fpu/regset.c and arch/x86/kernel/fpu/signal.c.",
        "id": 1307
    },
    {
        "cve_id": "CVE-2013-3226",
        "code_before_change": "static int sco_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sco_pinfo *pi = sco_pi(sk);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == BT_CONNECT2 &&\n\t    test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {\n\t\thci_conn_accept(pi->conn->hcon, 0);\n\t\tsk->sk_state = BT_CONFIG;\n\n\t\trelease_sock(sk);\n\t\treturn 0;\n\t}\n\n\trelease_sock(sk);\n\n\treturn bt_sock_recvmsg(iocb, sock, msg, len, flags);\n}",
        "code_after_change": "static int sco_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sco_pinfo *pi = sco_pi(sk);\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state == BT_CONNECT2 &&\n\t    test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {\n\t\thci_conn_accept(pi->conn->hcon, 0);\n\t\tsk->sk_state = BT_CONFIG;\n\t\tmsg->msg_namelen = 0;\n\n\t\trelease_sock(sk);\n\t\treturn 0;\n\t}\n\n\trelease_sock(sk);\n\n\treturn bt_sock_recvmsg(iocb, sock, msg, len, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,7 @@\n \t    test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {\n \t\thci_conn_accept(pi->conn->hcon, 0);\n \t\tsk->sk_state = BT_CONFIG;\n+\t\tmsg->msg_namelen = 0;\n \n \t\trelease_sock(sk);\n \t\treturn 0;",
        "function_modified_lines": {
            "added": [
                "\t\tmsg->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The sco_sock_recvmsg function in net/bluetooth/sco.c in the Linux kernel before 3.9-rc7 does not initialize a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 266
    },
    {
        "cve_id": "CVE-2012-6547",
        "code_before_change": "static long __tun_chr_ioctl(struct file *file, unsigned int cmd,\n\t\t\t    unsigned long arg, int ifreq_len)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tstruct tun_struct *tun;\n\tvoid __user* argp = (void __user*)arg;\n\tstruct sock_fprog fprog;\n\tstruct ifreq ifr;\n\tint sndbuf;\n\tint vnet_hdr_sz;\n\tint ret;\n\n\tif (cmd == TUNSETIFF || _IOC_TYPE(cmd) == 0x89)\n\t\tif (copy_from_user(&ifr, argp, ifreq_len))\n\t\t\treturn -EFAULT;\n\n\tif (cmd == TUNGETFEATURES) {\n\t\t/* Currently this just means: \"what IFF flags are valid?\".\n\t\t * This is needed because we never checked for invalid flags on\n\t\t * TUNSETIFF. */\n\t\treturn put_user(IFF_TUN | IFF_TAP | IFF_NO_PI | IFF_ONE_QUEUE |\n\t\t\t\tIFF_VNET_HDR,\n\t\t\t\t(unsigned int __user*)argp);\n\t}\n\n\trtnl_lock();\n\n\ttun = __tun_get(tfile);\n\tif (cmd == TUNSETIFF && !tun) {\n\t\tifr.ifr_name[IFNAMSIZ-1] = '\\0';\n\n\t\tret = tun_set_iff(tfile->net, file, &ifr);\n\n\t\tif (ret)\n\t\t\tgoto unlock;\n\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tgoto unlock;\n\t}\n\n\tret = -EBADFD;\n\tif (!tun)\n\t\tgoto unlock;\n\n\ttun_debug(KERN_INFO, tun, \"tun_chr_ioctl cmd %d\\n\", cmd);\n\n\tret = 0;\n\tswitch (cmd) {\n\tcase TUNGETIFF:\n\t\tret = tun_get_iff(current->nsproxy->net_ns, tun, &ifr);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETNOCSUM:\n\t\t/* Disable/Enable checksum */\n\n\t\t/* [unimplemented] */\n\t\ttun_debug(KERN_INFO, tun, \"ignored: set checksum %s\\n\",\n\t\t\t  arg ? \"disabled\" : \"enabled\");\n\t\tbreak;\n\n\tcase TUNSETPERSIST:\n\t\t/* Disable/Enable persist mode */\n\t\tif (arg)\n\t\t\ttun->flags |= TUN_PERSIST;\n\t\telse\n\t\t\ttun->flags &= ~TUN_PERSIST;\n\n\t\ttun_debug(KERN_INFO, tun, \"persist %s\\n\",\n\t\t\t  arg ? \"enabled\" : \"disabled\");\n\t\tbreak;\n\n\tcase TUNSETOWNER:\n\t\t/* Set owner of the device */\n\t\ttun->owner = (uid_t) arg;\n\n\t\ttun_debug(KERN_INFO, tun, \"owner set to %d\\n\", tun->owner);\n\t\tbreak;\n\n\tcase TUNSETGROUP:\n\t\t/* Set group of the device */\n\t\ttun->group= (gid_t) arg;\n\n\t\ttun_debug(KERN_INFO, tun, \"group set to %d\\n\", tun->group);\n\t\tbreak;\n\n\tcase TUNSETLINK:\n\t\t/* Only allow setting the type when the interface is down */\n\t\tif (tun->dev->flags & IFF_UP) {\n\t\t\ttun_debug(KERN_INFO, tun,\n\t\t\t\t  \"Linktype set failed because interface is up\\n\");\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\ttun->dev->type = (int) arg;\n\t\t\ttun_debug(KERN_INFO, tun, \"linktype set to %d\\n\",\n\t\t\t\t  tun->dev->type);\n\t\t\tret = 0;\n\t\t}\n\t\tbreak;\n\n#ifdef TUN_DEBUG\n\tcase TUNSETDEBUG:\n\t\ttun->debug = arg;\n\t\tbreak;\n#endif\n\tcase TUNSETOFFLOAD:\n\t\tret = set_offload(tun, arg);\n\t\tbreak;\n\n\tcase TUNSETTXFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != TUN_TAP_DEV)\n\t\t\tbreak;\n\t\tret = update_filter(&tun->txflt, (void __user *)arg);\n\t\tbreak;\n\n\tcase SIOCGIFHWADDR:\n\t\t/* Get hw address */\n\t\tmemcpy(ifr.ifr_hwaddr.sa_data, tun->dev->dev_addr, ETH_ALEN);\n\t\tifr.ifr_hwaddr.sa_family = tun->dev->type;\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase SIOCSIFHWADDR:\n\t\t/* Set hw address */\n\t\ttun_debug(KERN_DEBUG, tun, \"set hw address: %pM\\n\",\n\t\t\t  ifr.ifr_hwaddr.sa_data);\n\n\t\tret = dev_set_mac_address(tun->dev, &ifr.ifr_hwaddr);\n\t\tbreak;\n\n\tcase TUNGETSNDBUF:\n\t\tsndbuf = tun->socket.sk->sk_sndbuf;\n\t\tif (copy_to_user(argp, &sndbuf, sizeof(sndbuf)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETSNDBUF:\n\t\tif (copy_from_user(&sndbuf, argp, sizeof(sndbuf))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\ttun->socket.sk->sk_sndbuf = sndbuf;\n\t\tbreak;\n\n\tcase TUNGETVNETHDRSZ:\n\t\tvnet_hdr_sz = tun->vnet_hdr_sz;\n\t\tif (copy_to_user(argp, &vnet_hdr_sz, sizeof(vnet_hdr_sz)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETVNETHDRSZ:\n\t\tif (copy_from_user(&vnet_hdr_sz, argp, sizeof(vnet_hdr_sz))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (vnet_hdr_sz < (int)sizeof(struct virtio_net_hdr)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\ttun->vnet_hdr_sz = vnet_hdr_sz;\n\t\tbreak;\n\n\tcase TUNATTACHFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != TUN_TAP_DEV)\n\t\t\tbreak;\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&fprog, argp, sizeof(fprog)))\n\t\t\tbreak;\n\n\t\tret = sk_attach_filter(&fprog, tun->socket.sk);\n\t\tbreak;\n\n\tcase TUNDETACHFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != TUN_TAP_DEV)\n\t\t\tbreak;\n\t\tret = sk_detach_filter(tun->socket.sk);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\nunlock:\n\trtnl_unlock();\n\tif (tun)\n\t\ttun_put(tun);\n\treturn ret;\n}",
        "code_after_change": "static long __tun_chr_ioctl(struct file *file, unsigned int cmd,\n\t\t\t    unsigned long arg, int ifreq_len)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tstruct tun_struct *tun;\n\tvoid __user* argp = (void __user*)arg;\n\tstruct sock_fprog fprog;\n\tstruct ifreq ifr;\n\tint sndbuf;\n\tint vnet_hdr_sz;\n\tint ret;\n\n\tif (cmd == TUNSETIFF || _IOC_TYPE(cmd) == 0x89) {\n\t\tif (copy_from_user(&ifr, argp, ifreq_len))\n\t\t\treturn -EFAULT;\n\t} else\n\t\tmemset(&ifr, 0, sizeof(ifr));\n\n\tif (cmd == TUNGETFEATURES) {\n\t\t/* Currently this just means: \"what IFF flags are valid?\".\n\t\t * This is needed because we never checked for invalid flags on\n\t\t * TUNSETIFF. */\n\t\treturn put_user(IFF_TUN | IFF_TAP | IFF_NO_PI | IFF_ONE_QUEUE |\n\t\t\t\tIFF_VNET_HDR,\n\t\t\t\t(unsigned int __user*)argp);\n\t}\n\n\trtnl_lock();\n\n\ttun = __tun_get(tfile);\n\tif (cmd == TUNSETIFF && !tun) {\n\t\tifr.ifr_name[IFNAMSIZ-1] = '\\0';\n\n\t\tret = tun_set_iff(tfile->net, file, &ifr);\n\n\t\tif (ret)\n\t\t\tgoto unlock;\n\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tgoto unlock;\n\t}\n\n\tret = -EBADFD;\n\tif (!tun)\n\t\tgoto unlock;\n\n\ttun_debug(KERN_INFO, tun, \"tun_chr_ioctl cmd %d\\n\", cmd);\n\n\tret = 0;\n\tswitch (cmd) {\n\tcase TUNGETIFF:\n\t\tret = tun_get_iff(current->nsproxy->net_ns, tun, &ifr);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETNOCSUM:\n\t\t/* Disable/Enable checksum */\n\n\t\t/* [unimplemented] */\n\t\ttun_debug(KERN_INFO, tun, \"ignored: set checksum %s\\n\",\n\t\t\t  arg ? \"disabled\" : \"enabled\");\n\t\tbreak;\n\n\tcase TUNSETPERSIST:\n\t\t/* Disable/Enable persist mode */\n\t\tif (arg)\n\t\t\ttun->flags |= TUN_PERSIST;\n\t\telse\n\t\t\ttun->flags &= ~TUN_PERSIST;\n\n\t\ttun_debug(KERN_INFO, tun, \"persist %s\\n\",\n\t\t\t  arg ? \"enabled\" : \"disabled\");\n\t\tbreak;\n\n\tcase TUNSETOWNER:\n\t\t/* Set owner of the device */\n\t\ttun->owner = (uid_t) arg;\n\n\t\ttun_debug(KERN_INFO, tun, \"owner set to %d\\n\", tun->owner);\n\t\tbreak;\n\n\tcase TUNSETGROUP:\n\t\t/* Set group of the device */\n\t\ttun->group= (gid_t) arg;\n\n\t\ttun_debug(KERN_INFO, tun, \"group set to %d\\n\", tun->group);\n\t\tbreak;\n\n\tcase TUNSETLINK:\n\t\t/* Only allow setting the type when the interface is down */\n\t\tif (tun->dev->flags & IFF_UP) {\n\t\t\ttun_debug(KERN_INFO, tun,\n\t\t\t\t  \"Linktype set failed because interface is up\\n\");\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\ttun->dev->type = (int) arg;\n\t\t\ttun_debug(KERN_INFO, tun, \"linktype set to %d\\n\",\n\t\t\t\t  tun->dev->type);\n\t\t\tret = 0;\n\t\t}\n\t\tbreak;\n\n#ifdef TUN_DEBUG\n\tcase TUNSETDEBUG:\n\t\ttun->debug = arg;\n\t\tbreak;\n#endif\n\tcase TUNSETOFFLOAD:\n\t\tret = set_offload(tun, arg);\n\t\tbreak;\n\n\tcase TUNSETTXFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != TUN_TAP_DEV)\n\t\t\tbreak;\n\t\tret = update_filter(&tun->txflt, (void __user *)arg);\n\t\tbreak;\n\n\tcase SIOCGIFHWADDR:\n\t\t/* Get hw address */\n\t\tmemcpy(ifr.ifr_hwaddr.sa_data, tun->dev->dev_addr, ETH_ALEN);\n\t\tifr.ifr_hwaddr.sa_family = tun->dev->type;\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase SIOCSIFHWADDR:\n\t\t/* Set hw address */\n\t\ttun_debug(KERN_DEBUG, tun, \"set hw address: %pM\\n\",\n\t\t\t  ifr.ifr_hwaddr.sa_data);\n\n\t\tret = dev_set_mac_address(tun->dev, &ifr.ifr_hwaddr);\n\t\tbreak;\n\n\tcase TUNGETSNDBUF:\n\t\tsndbuf = tun->socket.sk->sk_sndbuf;\n\t\tif (copy_to_user(argp, &sndbuf, sizeof(sndbuf)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETSNDBUF:\n\t\tif (copy_from_user(&sndbuf, argp, sizeof(sndbuf))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\ttun->socket.sk->sk_sndbuf = sndbuf;\n\t\tbreak;\n\n\tcase TUNGETVNETHDRSZ:\n\t\tvnet_hdr_sz = tun->vnet_hdr_sz;\n\t\tif (copy_to_user(argp, &vnet_hdr_sz, sizeof(vnet_hdr_sz)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETVNETHDRSZ:\n\t\tif (copy_from_user(&vnet_hdr_sz, argp, sizeof(vnet_hdr_sz))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (vnet_hdr_sz < (int)sizeof(struct virtio_net_hdr)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\ttun->vnet_hdr_sz = vnet_hdr_sz;\n\t\tbreak;\n\n\tcase TUNATTACHFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != TUN_TAP_DEV)\n\t\t\tbreak;\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&fprog, argp, sizeof(fprog)))\n\t\t\tbreak;\n\n\t\tret = sk_attach_filter(&fprog, tun->socket.sk);\n\t\tbreak;\n\n\tcase TUNDETACHFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != TUN_TAP_DEV)\n\t\t\tbreak;\n\t\tret = sk_detach_filter(tun->socket.sk);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\nunlock:\n\trtnl_unlock();\n\tif (tun)\n\t\ttun_put(tun);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,9 +10,11 @@\n \tint vnet_hdr_sz;\n \tint ret;\n \n-\tif (cmd == TUNSETIFF || _IOC_TYPE(cmd) == 0x89)\n+\tif (cmd == TUNSETIFF || _IOC_TYPE(cmd) == 0x89) {\n \t\tif (copy_from_user(&ifr, argp, ifreq_len))\n \t\t\treturn -EFAULT;\n+\t} else\n+\t\tmemset(&ifr, 0, sizeof(ifr));\n \n \tif (cmd == TUNGETFEATURES) {\n \t\t/* Currently this just means: \"what IFF flags are valid?\".",
        "function_modified_lines": {
            "added": [
                "\tif (cmd == TUNSETIFF || _IOC_TYPE(cmd) == 0x89) {",
                "\t} else",
                "\t\tmemset(&ifr, 0, sizeof(ifr));"
            ],
            "deleted": [
                "\tif (cmd == TUNSETIFF || _IOC_TYPE(cmd) == 0x89)"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The __tun_chr_ioctl function in drivers/net/tun.c in the Linux kernel before 3.6 does not initialize a certain structure, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 131
    },
    {
        "cve_id": "CVE-2018-20449",
        "code_before_change": "static void __init\nkernel_ptr(void)\n{\n}",
        "code_after_change": "static void __init\nkernel_ptr(void)\n{\n\t/* We can't test this without access to kptr_restrict. */\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,5 @@\n static void __init\n kernel_ptr(void)\n {\n+\t/* We can't test this without access to kptr_restrict. */\n }",
        "function_modified_lines": {
            "added": [
                "\t/* We can't test this without access to kptr_restrict. */"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The hidma_chan_stats function in drivers/dma/qcom/hidma_dbg.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \"callback=\" lines in a debugfs file.",
        "id": 1757
    },
    {
        "cve_id": "CVE-2018-7755",
        "code_before_change": "static int fd_locked_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd,\n\t\t    unsigned long param)\n{\n\tint drive = (long)bdev->bd_disk->private_data;\n\tint type = ITYPE(UDRS->fd_device);\n\tint i;\n\tint ret;\n\tint size;\n\tunion inparam {\n\t\tstruct floppy_struct g;\t/* geometry */\n\t\tstruct format_descr f;\n\t\tstruct floppy_max_errors max_errors;\n\t\tstruct floppy_drive_params dp;\n\t} inparam;\t\t/* parameters coming from user space */\n\tconst void *outparam;\t/* parameters passed back to user space */\n\n\t/* convert compatibility eject ioctls into floppy eject ioctl.\n\t * We do this in order to provide a means to eject floppy disks before\n\t * installing the new fdutils package */\n\tif (cmd == CDROMEJECT ||\t/* CD-ROM eject */\n\t    cmd == 0x6470) {\t\t/* SunOS floppy eject */\n\t\tDPRINT(\"obsolete eject ioctl\\n\");\n\t\tDPRINT(\"please use floppycontrol --eject\\n\");\n\t\tcmd = FDEJECT;\n\t}\n\n\tif (!((cmd & 0xff00) == 0x0200))\n\t\treturn -EINVAL;\n\n\t/* convert the old style command into a new style command */\n\tret = normalize_ioctl(&cmd, &size);\n\tif (ret)\n\t\treturn ret;\n\n\t/* permission checks */\n\tif (((cmd & 0x40) && !(mode & (FMODE_WRITE | FMODE_WRITE_IOCTL))) ||\n\t    ((cmd & 0x80) && !capable(CAP_SYS_ADMIN)))\n\t\treturn -EPERM;\n\n\tif (WARN_ON(size < 0 || size > sizeof(inparam)))\n\t\treturn -EINVAL;\n\n\t/* copyin */\n\tmemset(&inparam, 0, sizeof(inparam));\n\tif (_IOC_DIR(cmd) & _IOC_WRITE) {\n\t\tret = fd_copyin((void __user *)param, &inparam, size);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tswitch (cmd) {\n\tcase FDEJECT:\n\t\tif (UDRS->fd_ref != 1)\n\t\t\t/* somebody else has this drive open */\n\t\t\treturn -EBUSY;\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\n\t\t/* do the actual eject. Fails on\n\t\t * non-Sparc architectures */\n\t\tret = fd_eject(UNIT(drive));\n\n\t\tset_bit(FD_DISK_CHANGED_BIT, &UDRS->flags);\n\t\tset_bit(FD_VERIFY_BIT, &UDRS->flags);\n\t\tprocess_fd_request();\n\t\treturn ret;\n\tcase FDCLRPRM:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tcurrent_type[drive] = NULL;\n\t\tfloppy_sizes[drive] = MAX_DISK_SIZE << 1;\n\t\tUDRS->keep_data = 0;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETPRM:\n\tcase FDDEFPRM:\n\t\treturn set_geometry(cmd, &inparam.g, drive, type, bdev);\n\tcase FDGETPRM:\n\t\tret = get_floppy_geometry(drive, type,\n\t\t\t\t\t  (struct floppy_struct **)&outparam);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tbreak;\n\tcase FDMSGON:\n\t\tUDP->flags |= FTD_MSG;\n\t\treturn 0;\n\tcase FDMSGOFF:\n\t\tUDP->flags &= ~FTD_MSG;\n\t\treturn 0;\n\tcase FDFMTBEG:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tret = UDRS->flags;\n\t\tprocess_fd_request();\n\t\tif (ret & FD_VERIFY)\n\t\t\treturn -ENODEV;\n\t\tif (!(ret & FD_DISK_WRITABLE))\n\t\t\treturn -EROFS;\n\t\treturn 0;\n\tcase FDFMTTRK:\n\t\tif (UDRS->fd_ref != 1)\n\t\t\treturn -EBUSY;\n\t\treturn do_format(drive, &inparam.f);\n\tcase FDFMTEND:\n\tcase FDFLUSH:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETEMSGTRESH:\n\t\tUDP->max_errors.reporting = (unsigned short)(param & 0x0f);\n\t\treturn 0;\n\tcase FDGETMAXERRS:\n\t\toutparam = &UDP->max_errors;\n\t\tbreak;\n\tcase FDSETMAXERRS:\n\t\tUDP->max_errors = inparam.max_errors;\n\t\tbreak;\n\tcase FDGETDRVTYP:\n\t\toutparam = drive_name(type, drive);\n\t\tSUPBOUND(size, strlen((const char *)outparam) + 1);\n\t\tbreak;\n\tcase FDSETDRVPRM:\n\t\t*UDP = inparam.dp;\n\t\tbreak;\n\tcase FDGETDRVPRM:\n\t\toutparam = UDP;\n\t\tbreak;\n\tcase FDPOLLDRVSTAT:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\t/* fall through */\n\tcase FDGETDRVSTAT:\n\t\toutparam = UDRS;\n\t\tbreak;\n\tcase FDRESET:\n\t\treturn user_reset_fdc(drive, (int)param, true);\n\tcase FDGETFDCSTAT:\n\t\toutparam = UFDCS;\n\t\tbreak;\n\tcase FDWERRORCLR:\n\t\tmemset(UDRWE, 0, sizeof(*UDRWE));\n\t\treturn 0;\n\tcase FDWERRORGET:\n\t\toutparam = UDRWE;\n\t\tbreak;\n\tcase FDRAWCMD:\n\t\tif (type)\n\t\t\treturn -EINVAL;\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tset_floppy(drive);\n\t\ti = raw_cmd_ioctl(cmd, (void __user *)param);\n\t\tif (i == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\treturn i;\n\tcase FDTWADDLE:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\ttwaddle();\n\t\tprocess_fd_request();\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (_IOC_DIR(cmd) & _IOC_READ)\n\t\treturn fd_copyout((void __user *)param, outparam, size);\n\n\treturn 0;\n}",
        "code_after_change": "static int fd_locked_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd,\n\t\t    unsigned long param)\n{\n\tint drive = (long)bdev->bd_disk->private_data;\n\tint type = ITYPE(UDRS->fd_device);\n\tint i;\n\tint ret;\n\tint size;\n\tunion inparam {\n\t\tstruct floppy_struct g;\t/* geometry */\n\t\tstruct format_descr f;\n\t\tstruct floppy_max_errors max_errors;\n\t\tstruct floppy_drive_params dp;\n\t} inparam;\t\t/* parameters coming from user space */\n\tconst void *outparam;\t/* parameters passed back to user space */\n\n\t/* convert compatibility eject ioctls into floppy eject ioctl.\n\t * We do this in order to provide a means to eject floppy disks before\n\t * installing the new fdutils package */\n\tif (cmd == CDROMEJECT ||\t/* CD-ROM eject */\n\t    cmd == 0x6470) {\t\t/* SunOS floppy eject */\n\t\tDPRINT(\"obsolete eject ioctl\\n\");\n\t\tDPRINT(\"please use floppycontrol --eject\\n\");\n\t\tcmd = FDEJECT;\n\t}\n\n\tif (!((cmd & 0xff00) == 0x0200))\n\t\treturn -EINVAL;\n\n\t/* convert the old style command into a new style command */\n\tret = normalize_ioctl(&cmd, &size);\n\tif (ret)\n\t\treturn ret;\n\n\t/* permission checks */\n\tif (((cmd & 0x40) && !(mode & (FMODE_WRITE | FMODE_WRITE_IOCTL))) ||\n\t    ((cmd & 0x80) && !capable(CAP_SYS_ADMIN)))\n\t\treturn -EPERM;\n\n\tif (WARN_ON(size < 0 || size > sizeof(inparam)))\n\t\treturn -EINVAL;\n\n\t/* copyin */\n\tmemset(&inparam, 0, sizeof(inparam));\n\tif (_IOC_DIR(cmd) & _IOC_WRITE) {\n\t\tret = fd_copyin((void __user *)param, &inparam, size);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tswitch (cmd) {\n\tcase FDEJECT:\n\t\tif (UDRS->fd_ref != 1)\n\t\t\t/* somebody else has this drive open */\n\t\t\treturn -EBUSY;\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\n\t\t/* do the actual eject. Fails on\n\t\t * non-Sparc architectures */\n\t\tret = fd_eject(UNIT(drive));\n\n\t\tset_bit(FD_DISK_CHANGED_BIT, &UDRS->flags);\n\t\tset_bit(FD_VERIFY_BIT, &UDRS->flags);\n\t\tprocess_fd_request();\n\t\treturn ret;\n\tcase FDCLRPRM:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tcurrent_type[drive] = NULL;\n\t\tfloppy_sizes[drive] = MAX_DISK_SIZE << 1;\n\t\tUDRS->keep_data = 0;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETPRM:\n\tcase FDDEFPRM:\n\t\treturn set_geometry(cmd, &inparam.g, drive, type, bdev);\n\tcase FDGETPRM:\n\t\tret = get_floppy_geometry(drive, type,\n\t\t\t\t\t  (struct floppy_struct **)&outparam);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tmemcpy(&inparam.g, outparam,\n\t\t\t\toffsetof(struct floppy_struct, name));\n\t\toutparam = &inparam.g;\n\t\tbreak;\n\tcase FDMSGON:\n\t\tUDP->flags |= FTD_MSG;\n\t\treturn 0;\n\tcase FDMSGOFF:\n\t\tUDP->flags &= ~FTD_MSG;\n\t\treturn 0;\n\tcase FDFMTBEG:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tret = UDRS->flags;\n\t\tprocess_fd_request();\n\t\tif (ret & FD_VERIFY)\n\t\t\treturn -ENODEV;\n\t\tif (!(ret & FD_DISK_WRITABLE))\n\t\t\treturn -EROFS;\n\t\treturn 0;\n\tcase FDFMTTRK:\n\t\tif (UDRS->fd_ref != 1)\n\t\t\treturn -EBUSY;\n\t\treturn do_format(drive, &inparam.f);\n\tcase FDFMTEND:\n\tcase FDFLUSH:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\treturn invalidate_drive(bdev);\n\tcase FDSETEMSGTRESH:\n\t\tUDP->max_errors.reporting = (unsigned short)(param & 0x0f);\n\t\treturn 0;\n\tcase FDGETMAXERRS:\n\t\toutparam = &UDP->max_errors;\n\t\tbreak;\n\tcase FDSETMAXERRS:\n\t\tUDP->max_errors = inparam.max_errors;\n\t\tbreak;\n\tcase FDGETDRVTYP:\n\t\toutparam = drive_name(type, drive);\n\t\tSUPBOUND(size, strlen((const char *)outparam) + 1);\n\t\tbreak;\n\tcase FDSETDRVPRM:\n\t\t*UDP = inparam.dp;\n\t\tbreak;\n\tcase FDGETDRVPRM:\n\t\toutparam = UDP;\n\t\tbreak;\n\tcase FDPOLLDRVSTAT:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tif (poll_drive(true, FD_RAW_NEED_DISK) == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\t/* fall through */\n\tcase FDGETDRVSTAT:\n\t\toutparam = UDRS;\n\t\tbreak;\n\tcase FDRESET:\n\t\treturn user_reset_fdc(drive, (int)param, true);\n\tcase FDGETFDCSTAT:\n\t\toutparam = UFDCS;\n\t\tbreak;\n\tcase FDWERRORCLR:\n\t\tmemset(UDRWE, 0, sizeof(*UDRWE));\n\t\treturn 0;\n\tcase FDWERRORGET:\n\t\toutparam = UDRWE;\n\t\tbreak;\n\tcase FDRAWCMD:\n\t\tif (type)\n\t\t\treturn -EINVAL;\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tset_floppy(drive);\n\t\ti = raw_cmd_ioctl(cmd, (void __user *)param);\n\t\tif (i == -EINTR)\n\t\t\treturn -EINTR;\n\t\tprocess_fd_request();\n\t\treturn i;\n\tcase FDTWADDLE:\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\ttwaddle();\n\t\tprocess_fd_request();\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (_IOC_DIR(cmd) & _IOC_READ)\n\t\treturn fd_copyout((void __user *)param, outparam, size);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -79,6 +79,9 @@\n \t\t\t\t\t  (struct floppy_struct **)&outparam);\n \t\tif (ret)\n \t\t\treturn ret;\n+\t\tmemcpy(&inparam.g, outparam,\n+\t\t\t\toffsetof(struct floppy_struct, name));\n+\t\toutparam = &inparam.g;\n \t\tbreak;\n \tcase FDMSGON:\n \t\tUDP->flags |= FTD_MSG;",
        "function_modified_lines": {
            "added": [
                "\t\tmemcpy(&inparam.g, outparam,",
                "\t\t\t\toffsetof(struct floppy_struct, name));",
                "\t\toutparam = &inparam.g;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the fd_locked_ioctl function in drivers/block/floppy.c in the Linux kernel through 4.15.7. The floppy driver will copy a kernel pointer to user memory in response to the FDGETPRM ioctl. An attacker can send the FDGETPRM ioctl and use the obtained kernel pointer to discover the location of kernel code and data and bypass kernel security protections such as KASLR.",
        "id": 1853
    },
    {
        "cve_id": "CVE-2016-0823",
        "code_before_change": "static int pagemap_open(struct inode *inode, struct file *file)\n{\n\tpr_warn_once(\"Bits 55-60 of /proc/PID/pagemap entries are about \"\n\t\t\t\"to stop being page-shift some time soon. See the \"\n\t\t\t\"linux/Documentation/vm/pagemap.txt for details.\\n\");\n\treturn 0;\n}",
        "code_after_change": "static int pagemap_open(struct inode *inode, struct file *file)\n{\n\t/* do not disclose physical addresses: attack vector */\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\tpr_warn_once(\"Bits 55-60 of /proc/PID/pagemap entries are about \"\n\t\t\t\"to stop being page-shift some time soon. See the \"\n\t\t\t\"linux/Documentation/vm/pagemap.txt for details.\\n\");\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,5 +1,8 @@\n static int pagemap_open(struct inode *inode, struct file *file)\n {\n+\t/* do not disclose physical addresses: attack vector */\n+\tif (!capable(CAP_SYS_ADMIN))\n+\t\treturn -EPERM;\n \tpr_warn_once(\"Bits 55-60 of /proc/PID/pagemap entries are about \"\n \t\t\t\"to stop being page-shift some time soon. See the \"\n \t\t\t\"linux/Documentation/vm/pagemap.txt for details.\\n\");",
        "function_modified_lines": {
            "added": [
                "\t/* do not disclose physical addresses: attack vector */",
                "\tif (!capable(CAP_SYS_ADMIN))",
                "\t\treturn -EPERM;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The pagemap_open function in fs/proc/task_mmu.c in the Linux kernel before 3.19.3, as used in Android 6.0.1 before 2016-03-01, allows local users to obtain sensitive physical-address information by reading a pagemap file, aka Android internal bug 25739721.",
        "id": 891
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static void blkif_free_ring(struct blkfront_ring_info *rinfo)\n{\n\tstruct grant *persistent_gnt, *n;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tint i, j, segs;\n\n\t/*\n\t * Remove indirect pages, this only happens when using indirect\n\t * descriptors but not persistent grants\n\t */\n\tif (!list_empty(&rinfo->indirect_pages)) {\n\t\tstruct page *indirect_page, *n;\n\n\t\tBUG_ON(info->feature_persistent);\n\t\tlist_for_each_entry_safe(indirect_page, n, &rinfo->indirect_pages, lru) {\n\t\t\tlist_del(&indirect_page->lru);\n\t\t\t__free_page(indirect_page);\n\t\t}\n\t}\n\n\t/* Remove all persistent grants. */\n\tif (!list_empty(&rinfo->grants)) {\n\t\tlist_for_each_entry_safe(persistent_gnt, n,\n\t\t\t\t\t &rinfo->grants, node) {\n\t\t\tlist_del(&persistent_gnt->node);\n\t\t\tif (persistent_gnt->gref != INVALID_GRANT_REF) {\n\t\t\t\tgnttab_end_foreign_access(persistent_gnt->gref,\n\t\t\t\t\t\t\t  NULL);\n\t\t\t\trinfo->persistent_gnts_c--;\n\t\t\t}\n\t\t\tif (info->feature_persistent)\n\t\t\t\t__free_page(persistent_gnt->page);\n\t\t\tkfree(persistent_gnt);\n\t\t}\n\t}\n\tBUG_ON(rinfo->persistent_gnts_c != 0);\n\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\t/*\n\t\t * Clear persistent grants present in requests already\n\t\t * on the shared ring\n\t\t */\n\t\tif (!rinfo->shadow[i].request)\n\t\t\tgoto free_shadow;\n\n\t\tsegs = rinfo->shadow[i].req.operation == BLKIF_OP_INDIRECT ?\n\t\t       rinfo->shadow[i].req.u.indirect.nr_segments :\n\t\t       rinfo->shadow[i].req.u.rw.nr_segments;\n\t\tfor (j = 0; j < segs; j++) {\n\t\t\tpersistent_gnt = rinfo->shadow[i].grants_used[j];\n\t\t\tgnttab_end_foreign_access(persistent_gnt->gref, NULL);\n\t\t\tif (info->feature_persistent)\n\t\t\t\t__free_page(persistent_gnt->page);\n\t\t\tkfree(persistent_gnt);\n\t\t}\n\n\t\tif (rinfo->shadow[i].req.operation != BLKIF_OP_INDIRECT)\n\t\t\t/*\n\t\t\t * If this is not an indirect operation don't try to\n\t\t\t * free indirect segments\n\t\t\t */\n\t\t\tgoto free_shadow;\n\n\t\tfor (j = 0; j < INDIRECT_GREFS(segs); j++) {\n\t\t\tpersistent_gnt = rinfo->shadow[i].indirect_grants[j];\n\t\t\tgnttab_end_foreign_access(persistent_gnt->gref, NULL);\n\t\t\t__free_page(persistent_gnt->page);\n\t\t\tkfree(persistent_gnt);\n\t\t}\n\nfree_shadow:\n\t\tkvfree(rinfo->shadow[i].grants_used);\n\t\trinfo->shadow[i].grants_used = NULL;\n\t\tkvfree(rinfo->shadow[i].indirect_grants);\n\t\trinfo->shadow[i].indirect_grants = NULL;\n\t\tkvfree(rinfo->shadow[i].sg);\n\t\trinfo->shadow[i].sg = NULL;\n\t}\n\n\t/* No more gnttab callback work. */\n\tgnttab_cancel_free_callback(&rinfo->callback);\n\n\t/* Flush gnttab callback work. Must be done with no locks held. */\n\tflush_work(&rinfo->work);\n\n\t/* Free resources associated with old device channel. */\n\txenbus_teardown_ring((void **)&rinfo->ring.sring, info->nr_ring_pages,\n\t\t\t     rinfo->ring_ref);\n\n\tif (rinfo->irq)\n\t\tunbind_from_irqhandler(rinfo->irq, rinfo);\n\trinfo->evtchn = rinfo->irq = 0;\n}",
        "code_after_change": "static void blkif_free_ring(struct blkfront_ring_info *rinfo)\n{\n\tstruct grant *persistent_gnt, *n;\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tint i, j, segs;\n\n\t/*\n\t * Remove indirect pages, this only happens when using indirect\n\t * descriptors but not persistent grants\n\t */\n\tif (!list_empty(&rinfo->indirect_pages)) {\n\t\tstruct page *indirect_page, *n;\n\n\t\tBUG_ON(info->bounce);\n\t\tlist_for_each_entry_safe(indirect_page, n, &rinfo->indirect_pages, lru) {\n\t\t\tlist_del(&indirect_page->lru);\n\t\t\t__free_page(indirect_page);\n\t\t}\n\t}\n\n\t/* Remove all persistent grants. */\n\tif (!list_empty(&rinfo->grants)) {\n\t\tlist_for_each_entry_safe(persistent_gnt, n,\n\t\t\t\t\t &rinfo->grants, node) {\n\t\t\tlist_del(&persistent_gnt->node);\n\t\t\tif (persistent_gnt->gref != INVALID_GRANT_REF) {\n\t\t\t\tgnttab_end_foreign_access(persistent_gnt->gref,\n\t\t\t\t\t\t\t  NULL);\n\t\t\t\trinfo->persistent_gnts_c--;\n\t\t\t}\n\t\t\tif (info->bounce)\n\t\t\t\t__free_page(persistent_gnt->page);\n\t\t\tkfree(persistent_gnt);\n\t\t}\n\t}\n\tBUG_ON(rinfo->persistent_gnts_c != 0);\n\n\tfor (i = 0; i < BLK_RING_SIZE(info); i++) {\n\t\t/*\n\t\t * Clear persistent grants present in requests already\n\t\t * on the shared ring\n\t\t */\n\t\tif (!rinfo->shadow[i].request)\n\t\t\tgoto free_shadow;\n\n\t\tsegs = rinfo->shadow[i].req.operation == BLKIF_OP_INDIRECT ?\n\t\t       rinfo->shadow[i].req.u.indirect.nr_segments :\n\t\t       rinfo->shadow[i].req.u.rw.nr_segments;\n\t\tfor (j = 0; j < segs; j++) {\n\t\t\tpersistent_gnt = rinfo->shadow[i].grants_used[j];\n\t\t\tgnttab_end_foreign_access(persistent_gnt->gref, NULL);\n\t\t\tif (info->bounce)\n\t\t\t\t__free_page(persistent_gnt->page);\n\t\t\tkfree(persistent_gnt);\n\t\t}\n\n\t\tif (rinfo->shadow[i].req.operation != BLKIF_OP_INDIRECT)\n\t\t\t/*\n\t\t\t * If this is not an indirect operation don't try to\n\t\t\t * free indirect segments\n\t\t\t */\n\t\t\tgoto free_shadow;\n\n\t\tfor (j = 0; j < INDIRECT_GREFS(segs); j++) {\n\t\t\tpersistent_gnt = rinfo->shadow[i].indirect_grants[j];\n\t\t\tgnttab_end_foreign_access(persistent_gnt->gref, NULL);\n\t\t\t__free_page(persistent_gnt->page);\n\t\t\tkfree(persistent_gnt);\n\t\t}\n\nfree_shadow:\n\t\tkvfree(rinfo->shadow[i].grants_used);\n\t\trinfo->shadow[i].grants_used = NULL;\n\t\tkvfree(rinfo->shadow[i].indirect_grants);\n\t\trinfo->shadow[i].indirect_grants = NULL;\n\t\tkvfree(rinfo->shadow[i].sg);\n\t\trinfo->shadow[i].sg = NULL;\n\t}\n\n\t/* No more gnttab callback work. */\n\tgnttab_cancel_free_callback(&rinfo->callback);\n\n\t/* Flush gnttab callback work. Must be done with no locks held. */\n\tflush_work(&rinfo->work);\n\n\t/* Free resources associated with old device channel. */\n\txenbus_teardown_ring((void **)&rinfo->ring.sring, info->nr_ring_pages,\n\t\t\t     rinfo->ring_ref);\n\n\tif (rinfo->irq)\n\t\tunbind_from_irqhandler(rinfo->irq, rinfo);\n\trinfo->evtchn = rinfo->irq = 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \tif (!list_empty(&rinfo->indirect_pages)) {\n \t\tstruct page *indirect_page, *n;\n \n-\t\tBUG_ON(info->feature_persistent);\n+\t\tBUG_ON(info->bounce);\n \t\tlist_for_each_entry_safe(indirect_page, n, &rinfo->indirect_pages, lru) {\n \t\t\tlist_del(&indirect_page->lru);\n \t\t\t__free_page(indirect_page);\n@@ -28,7 +28,7 @@\n \t\t\t\t\t\t\t  NULL);\n \t\t\t\trinfo->persistent_gnts_c--;\n \t\t\t}\n-\t\t\tif (info->feature_persistent)\n+\t\t\tif (info->bounce)\n \t\t\t\t__free_page(persistent_gnt->page);\n \t\t\tkfree(persistent_gnt);\n \t\t}\n@@ -49,7 +49,7 @@\n \t\tfor (j = 0; j < segs; j++) {\n \t\t\tpersistent_gnt = rinfo->shadow[i].grants_used[j];\n \t\t\tgnttab_end_foreign_access(persistent_gnt->gref, NULL);\n-\t\t\tif (info->feature_persistent)\n+\t\t\tif (info->bounce)\n \t\t\t\t__free_page(persistent_gnt->page);\n \t\t\tkfree(persistent_gnt);\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\tBUG_ON(info->bounce);",
                "\t\t\tif (info->bounce)",
                "\t\t\tif (info->bounce)"
            ],
            "deleted": [
                "\t\tBUG_ON(info->feature_persistent);",
                "\t\t\tif (info->feature_persistent)",
                "\t\t\tif (info->feature_persistent)"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3588
    },
    {
        "cve_id": "CVE-2015-8569",
        "code_before_change": "static int pptp_bind(struct socket *sock, struct sockaddr *uservaddr,\n\tint sockaddr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppox *sp = (struct sockaddr_pppox *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct pptp_opt *opt = &po->proto.pptp;\n\tint error = 0;\n\n\tlock_sock(sk);\n\n\topt->src_addr = sp->sa_addr.pptp;\n\tif (add_chan(po))\n\t\terror = -EBUSY;\n\n\trelease_sock(sk);\n\treturn error;\n}",
        "code_after_change": "static int pptp_bind(struct socket *sock, struct sockaddr *uservaddr,\n\tint sockaddr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppox *sp = (struct sockaddr_pppox *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct pptp_opt *opt = &po->proto.pptp;\n\tint error = 0;\n\n\tif (sockaddr_len < sizeof(struct sockaddr_pppox))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\topt->src_addr = sp->sa_addr.pptp;\n\tif (add_chan(po))\n\t\terror = -EBUSY;\n\n\trelease_sock(sk);\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,9 @@\n \tstruct pppox_sock *po = pppox_sk(sk);\n \tstruct pptp_opt *opt = &po->proto.pptp;\n \tint error = 0;\n+\n+\tif (sockaddr_len < sizeof(struct sockaddr_pppox))\n+\t\treturn -EINVAL;\n \n \tlock_sock(sk);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (sockaddr_len < sizeof(struct sockaddr_pppox))",
                "\t\treturn -EINVAL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The (1) pptp_bind and (2) pptp_connect functions in drivers/net/ppp/pptp.c in the Linux kernel through 4.3.3 do not verify an address length, which allows local users to obtain sensitive information from kernel memory and bypass the KASLR protection mechanism via a crafted application.",
        "id": 832
    },
    {
        "cve_id": "CVE-2019-18660",
        "code_before_change": "void setup_count_cache_flush(void)\n{\n\tbool enable = true;\n\n\tif (no_spectrev2 || cpu_mitigations_off()) {\n\t\tif (security_ftr_enabled(SEC_FTR_BCCTRL_SERIALISED) ||\n\t\t    security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED))\n\t\t\tpr_warn(\"Spectre v2 mitigations not under software control, can't disable\\n\");\n\n\t\tenable = false;\n\t}\n\n\ttoggle_count_cache_flush(enable);\n}",
        "code_after_change": "void setup_count_cache_flush(void)\n{\n\tbool enable = true;\n\n\tif (no_spectrev2 || cpu_mitigations_off()) {\n\t\tif (security_ftr_enabled(SEC_FTR_BCCTRL_SERIALISED) ||\n\t\t    security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED))\n\t\t\tpr_warn(\"Spectre v2 mitigations not fully under software control, can't disable\\n\");\n\n\t\tenable = false;\n\t}\n\n\t/*\n\t * There's no firmware feature flag/hypervisor bit to tell us we need to\n\t * flush the link stack on context switch. So we set it here if we see\n\t * either of the Spectre v2 mitigations that aim to protect userspace.\n\t */\n\tif (security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED) ||\n\t    security_ftr_enabled(SEC_FTR_FLUSH_COUNT_CACHE))\n\t\tsecurity_ftr_set(SEC_FTR_FLUSH_LINK_STACK);\n\n\ttoggle_count_cache_flush(enable);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,10 +5,19 @@\n \tif (no_spectrev2 || cpu_mitigations_off()) {\n \t\tif (security_ftr_enabled(SEC_FTR_BCCTRL_SERIALISED) ||\n \t\t    security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED))\n-\t\t\tpr_warn(\"Spectre v2 mitigations not under software control, can't disable\\n\");\n+\t\t\tpr_warn(\"Spectre v2 mitigations not fully under software control, can't disable\\n\");\n \n \t\tenable = false;\n \t}\n \n+\t/*\n+\t * There's no firmware feature flag/hypervisor bit to tell us we need to\n+\t * flush the link stack on context switch. So we set it here if we see\n+\t * either of the Spectre v2 mitigations that aim to protect userspace.\n+\t */\n+\tif (security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED) ||\n+\t    security_ftr_enabled(SEC_FTR_FLUSH_COUNT_CACHE))\n+\t\tsecurity_ftr_set(SEC_FTR_FLUSH_LINK_STACK);\n+\n \ttoggle_count_cache_flush(enable);\n }",
        "function_modified_lines": {
            "added": [
                "\t\t\tpr_warn(\"Spectre v2 mitigations not fully under software control, can't disable\\n\");",
                "\t/*",
                "\t * There's no firmware feature flag/hypervisor bit to tell us we need to",
                "\t * flush the link stack on context switch. So we set it here if we see",
                "\t * either of the Spectre v2 mitigations that aim to protect userspace.",
                "\t */",
                "\tif (security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED) ||",
                "\t    security_ftr_enabled(SEC_FTR_FLUSH_COUNT_CACHE))",
                "\t\tsecurity_ftr_set(SEC_FTR_FLUSH_LINK_STACK);",
                ""
            ],
            "deleted": [
                "\t\t\tpr_warn(\"Spectre v2 mitigations not under software control, can't disable\\n\");"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The Linux kernel before 5.4.1 on powerpc allows Information Exposure because the Spectre-RSB mitigation is not in place for all applicable CPUs, aka CID-39e72bf96f58. This is related to arch/powerpc/kernel/entry_64.S and arch/powerpc/kernel/security.c.",
        "id": 2088
    },
    {
        "cve_id": "CVE-2013-2164",
        "code_before_change": "static noinline int mmc_ioctl_cdrom_read_data(struct cdrom_device_info *cdi,\n\t\t\t\t\tvoid __user *arg,\n\t\t\t\t\tstruct packet_command *cgc,\n\t\t\t\t\tint cmd)\n{\n\tstruct request_sense sense;\n\tstruct cdrom_msf msf;\n\tint blocksize = 0, format = 0, lba;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase CDROMREADRAW:\n\t\tblocksize = CD_FRAMESIZE_RAW;\n\t\tbreak;\n\tcase CDROMREADMODE1:\n\t\tblocksize = CD_FRAMESIZE;\n\t\tformat = 2;\n\t\tbreak;\n\tcase CDROMREADMODE2:\n\t\tblocksize = CD_FRAMESIZE_RAW0;\n\t\tbreak;\n\t}\n\tIOCTL_IN(arg, struct cdrom_msf, msf);\n\tlba = msf_to_lba(msf.cdmsf_min0, msf.cdmsf_sec0, msf.cdmsf_frame0);\n\t/* FIXME: we need upper bound checking, too!! */\n\tif (lba < 0)\n\t\treturn -EINVAL;\n\n\tcgc->buffer = kmalloc(blocksize, GFP_KERNEL);\n\tif (cgc->buffer == NULL)\n\t\treturn -ENOMEM;\n\n\tmemset(&sense, 0, sizeof(sense));\n\tcgc->sense = &sense;\n\tcgc->data_direction = CGC_DATA_READ;\n\tret = cdrom_read_block(cdi, cgc, lba, 1, format, blocksize);\n\tif (ret && sense.sense_key == 0x05 &&\n\t\t   sense.asc == 0x20 &&\n\t\t   sense.ascq == 0x00) {\n\t\t/*\n\t\t * SCSI-II devices are not required to support\n\t\t * READ_CD, so let's try switching block size\n\t\t */\n\t\t/* FIXME: switch back again... */\n\t\tret = cdrom_switch_blocksize(cdi, blocksize);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tcgc->sense = NULL;\n\t\tret = cdrom_read_cd(cdi, cgc, lba, blocksize, 1);\n\t\tret |= cdrom_switch_blocksize(cdi, blocksize);\n\t}\n\tif (!ret && copy_to_user(arg, cgc->buffer, blocksize))\n\t\tret = -EFAULT;\nout:\n\tkfree(cgc->buffer);\n\treturn ret;\n}",
        "code_after_change": "static noinline int mmc_ioctl_cdrom_read_data(struct cdrom_device_info *cdi,\n\t\t\t\t\tvoid __user *arg,\n\t\t\t\t\tstruct packet_command *cgc,\n\t\t\t\t\tint cmd)\n{\n\tstruct request_sense sense;\n\tstruct cdrom_msf msf;\n\tint blocksize = 0, format = 0, lba;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase CDROMREADRAW:\n\t\tblocksize = CD_FRAMESIZE_RAW;\n\t\tbreak;\n\tcase CDROMREADMODE1:\n\t\tblocksize = CD_FRAMESIZE;\n\t\tformat = 2;\n\t\tbreak;\n\tcase CDROMREADMODE2:\n\t\tblocksize = CD_FRAMESIZE_RAW0;\n\t\tbreak;\n\t}\n\tIOCTL_IN(arg, struct cdrom_msf, msf);\n\tlba = msf_to_lba(msf.cdmsf_min0, msf.cdmsf_sec0, msf.cdmsf_frame0);\n\t/* FIXME: we need upper bound checking, too!! */\n\tif (lba < 0)\n\t\treturn -EINVAL;\n\n\tcgc->buffer = kzalloc(blocksize, GFP_KERNEL);\n\tif (cgc->buffer == NULL)\n\t\treturn -ENOMEM;\n\n\tmemset(&sense, 0, sizeof(sense));\n\tcgc->sense = &sense;\n\tcgc->data_direction = CGC_DATA_READ;\n\tret = cdrom_read_block(cdi, cgc, lba, 1, format, blocksize);\n\tif (ret && sense.sense_key == 0x05 &&\n\t\t   sense.asc == 0x20 &&\n\t\t   sense.ascq == 0x00) {\n\t\t/*\n\t\t * SCSI-II devices are not required to support\n\t\t * READ_CD, so let's try switching block size\n\t\t */\n\t\t/* FIXME: switch back again... */\n\t\tret = cdrom_switch_blocksize(cdi, blocksize);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tcgc->sense = NULL;\n\t\tret = cdrom_read_cd(cdi, cgc, lba, blocksize, 1);\n\t\tret |= cdrom_switch_blocksize(cdi, blocksize);\n\t}\n\tif (!ret && copy_to_user(arg, cgc->buffer, blocksize))\n\t\tret = -EFAULT;\nout:\n\tkfree(cgc->buffer);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,7 +26,7 @@\n \tif (lba < 0)\n \t\treturn -EINVAL;\n \n-\tcgc->buffer = kmalloc(blocksize, GFP_KERNEL);\n+\tcgc->buffer = kzalloc(blocksize, GFP_KERNEL);\n \tif (cgc->buffer == NULL)\n \t\treturn -ENOMEM;\n ",
        "function_modified_lines": {
            "added": [
                "\tcgc->buffer = kzalloc(blocksize, GFP_KERNEL);"
            ],
            "deleted": [
                "\tcgc->buffer = kmalloc(blocksize, GFP_KERNEL);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The mmc_ioctl_cdrom_read_data function in drivers/cdrom/cdrom.c in the Linux kernel through 3.10 allows local users to obtain sensitive information from kernel memory via a read operation on a malfunctioning CD-ROM drive.",
        "id": 221
    },
    {
        "cve_id": "CVE-2012-6546",
        "code_before_change": "static int pvc_getname(struct socket *sock, struct sockaddr *sockaddr,\n\t\t       int *sockaddr_len, int peer)\n{\n\tstruct sockaddr_atmpvc *addr;\n\tstruct atm_vcc *vcc = ATM_SD(sock);\n\n\tif (!vcc->dev || !test_bit(ATM_VF_ADDR, &vcc->flags))\n\t\treturn -ENOTCONN;\n\t*sockaddr_len = sizeof(struct sockaddr_atmpvc);\n\taddr = (struct sockaddr_atmpvc *)sockaddr;\n\taddr->sap_family = AF_ATMPVC;\n\taddr->sap_addr.itf = vcc->dev->number;\n\taddr->sap_addr.vpi = vcc->vpi;\n\taddr->sap_addr.vci = vcc->vci;\n\treturn 0;\n}",
        "code_after_change": "static int pvc_getname(struct socket *sock, struct sockaddr *sockaddr,\n\t\t       int *sockaddr_len, int peer)\n{\n\tstruct sockaddr_atmpvc *addr;\n\tstruct atm_vcc *vcc = ATM_SD(sock);\n\n\tif (!vcc->dev || !test_bit(ATM_VF_ADDR, &vcc->flags))\n\t\treturn -ENOTCONN;\n\t*sockaddr_len = sizeof(struct sockaddr_atmpvc);\n\taddr = (struct sockaddr_atmpvc *)sockaddr;\n\tmemset(addr, 0, sizeof(*addr));\n\taddr->sap_family = AF_ATMPVC;\n\taddr->sap_addr.itf = vcc->dev->number;\n\taddr->sap_addr.vpi = vcc->vpi;\n\taddr->sap_addr.vci = vcc->vci;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,7 @@\n \t\treturn -ENOTCONN;\n \t*sockaddr_len = sizeof(struct sockaddr_atmpvc);\n \taddr = (struct sockaddr_atmpvc *)sockaddr;\n+\tmemset(addr, 0, sizeof(*addr));\n \taddr->sap_family = AF_ATMPVC;\n \taddr->sap_addr.itf = vcc->dev->number;\n \taddr->sap_addr.vpi = vcc->vpi;",
        "function_modified_lines": {
            "added": [
                "\tmemset(addr, 0, sizeof(*addr));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The ATM implementation in the Linux kernel before 3.6 does not initialize certain structures, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 130
    },
    {
        "cve_id": "CVE-2013-7281",
        "code_before_change": "static int pn_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t\tstruct msghdr *msg, size_t len, int noblock,\n\t\t\tint flags, int *addr_len)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct sockaddr_pn sa;\n\tint rval = -EOPNOTSUPP;\n\tint copylen;\n\n\tif (flags & ~(MSG_PEEK|MSG_TRUNC|MSG_DONTWAIT|MSG_NOSIGNAL|\n\t\t\tMSG_CMSG_COMPAT))\n\t\tgoto out_nofree;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(sa);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &rval);\n\tif (skb == NULL)\n\t\tgoto out_nofree;\n\n\tpn_skb_get_src_sockaddr(skb, &sa);\n\n\tcopylen = skb->len;\n\tif (len < copylen) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopylen = len;\n\t}\n\n\trval = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copylen);\n\tif (rval) {\n\t\trval = -EFAULT;\n\t\tgoto out;\n\t}\n\n\trval = (flags & MSG_TRUNC) ? skb->len : copylen;\n\n\tif (msg->msg_name != NULL)\n\t\tmemcpy(msg->msg_name, &sa, sizeof(struct sockaddr_pn));\n\nout:\n\tskb_free_datagram(sk, skb);\n\nout_nofree:\n\treturn rval;\n}",
        "code_after_change": "static int pn_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t\tstruct msghdr *msg, size_t len, int noblock,\n\t\t\tint flags, int *addr_len)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct sockaddr_pn sa;\n\tint rval = -EOPNOTSUPP;\n\tint copylen;\n\n\tif (flags & ~(MSG_PEEK|MSG_TRUNC|MSG_DONTWAIT|MSG_NOSIGNAL|\n\t\t\tMSG_CMSG_COMPAT))\n\t\tgoto out_nofree;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &rval);\n\tif (skb == NULL)\n\t\tgoto out_nofree;\n\n\tpn_skb_get_src_sockaddr(skb, &sa);\n\n\tcopylen = skb->len;\n\tif (len < copylen) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopylen = len;\n\t}\n\n\trval = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copylen);\n\tif (rval) {\n\t\trval = -EFAULT;\n\t\tgoto out;\n\t}\n\n\trval = (flags & MSG_TRUNC) ? skb->len : copylen;\n\n\tif (msg->msg_name != NULL) {\n\t\tmemcpy(msg->msg_name, &sa, sizeof(sa));\n\t\t*addr_len = sizeof(sa);\n\t}\n\nout:\n\tskb_free_datagram(sk, skb);\n\nout_nofree:\n\treturn rval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,9 +10,6 @@\n \tif (flags & ~(MSG_PEEK|MSG_TRUNC|MSG_DONTWAIT|MSG_NOSIGNAL|\n \t\t\tMSG_CMSG_COMPAT))\n \t\tgoto out_nofree;\n-\n-\tif (addr_len)\n-\t\t*addr_len = sizeof(sa);\n \n \tskb = skb_recv_datagram(sk, flags, noblock, &rval);\n \tif (skb == NULL)\n@@ -34,8 +31,10 @@\n \n \trval = (flags & MSG_TRUNC) ? skb->len : copylen;\n \n-\tif (msg->msg_name != NULL)\n-\t\tmemcpy(msg->msg_name, &sa, sizeof(struct sockaddr_pn));\n+\tif (msg->msg_name != NULL) {\n+\t\tmemcpy(msg->msg_name, &sa, sizeof(sa));\n+\t\t*addr_len = sizeof(sa);\n+\t}\n \n out:\n \tskb_free_datagram(sk, skb);",
        "function_modified_lines": {
            "added": [
                "\tif (msg->msg_name != NULL) {",
                "\t\tmemcpy(msg->msg_name, &sa, sizeof(sa));",
                "\t\t*addr_len = sizeof(sa);",
                "\t}"
            ],
            "deleted": [
                "",
                "\tif (addr_len)",
                "\t\t*addr_len = sizeof(sa);",
                "\tif (msg->msg_name != NULL)",
                "\t\tmemcpy(msg->msg_name, &sa, sizeof(struct sockaddr_pn));"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The dgram_recvmsg function in net/ieee802154/dgram.c in the Linux kernel before 3.12.4 updates a certain length value without ensuring that an associated data structure has been initialized, which allows local users to obtain sensitive information from kernel stack memory via a (1) recvfrom, (2) recvmmsg, or (3) recvmsg system call.",
        "id": 413
    },
    {
        "cve_id": "CVE-2017-5550",
        "code_before_change": "static void pipe_advance(struct iov_iter *i, size_t size)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tstruct pipe_buffer *buf;\n\tint idx = i->idx;\n\tsize_t off = i->iov_offset, orig_sz;\n\n\tif (unlikely(i->count < size))\n\t\tsize = i->count;\n\torig_sz = size;\n\n\tif (size) {\n\t\tif (off) /* make it relative to the beginning of buffer */\n\t\t\tsize += off - pipe->bufs[idx].offset;\n\t\twhile (1) {\n\t\t\tbuf = &pipe->bufs[idx];\n\t\t\tif (size <= buf->len)\n\t\t\t\tbreak;\n\t\t\tsize -= buf->len;\n\t\t\tidx = next_idx(idx, pipe);\n\t\t}\n\t\tbuf->len = size;\n\t\ti->idx = idx;\n\t\toff = i->iov_offset = buf->offset + size;\n\t}\n\tif (off)\n\t\tidx = next_idx(idx, pipe);\n\tif (pipe->nrbufs) {\n\t\tint unused = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\t\t/* [curbuf,unused) is in use.  Free [idx,unused) */\n\t\twhile (idx != unused) {\n\t\t\tpipe_buf_release(pipe, &pipe->bufs[idx]);\n\t\t\tidx = next_idx(idx, pipe);\n\t\t\tpipe->nrbufs--;\n\t\t}\n\t}\n\ti->count -= orig_sz;\n}",
        "code_after_change": "static void pipe_advance(struct iov_iter *i, size_t size)\n{\n\tstruct pipe_inode_info *pipe = i->pipe;\n\tif (unlikely(i->count < size))\n\t\tsize = i->count;\n\tif (size) {\n\t\tstruct pipe_buffer *buf;\n\t\tsize_t off = i->iov_offset, left = size;\n\t\tint idx = i->idx;\n\t\tif (off) /* make it relative to the beginning of buffer */\n\t\t\tleft += off - pipe->bufs[idx].offset;\n\t\twhile (1) {\n\t\t\tbuf = &pipe->bufs[idx];\n\t\t\tif (left <= buf->len)\n\t\t\t\tbreak;\n\t\t\tleft -= buf->len;\n\t\t\tidx = next_idx(idx, pipe);\n\t\t}\n\t\ti->idx = idx;\n\t\ti->iov_offset = buf->offset + left;\n\t}\n\ti->count -= size;\n\t/* ... and discard everything past that point */\n\tpipe_truncate(i);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,38 +1,25 @@\n static void pipe_advance(struct iov_iter *i, size_t size)\n {\n \tstruct pipe_inode_info *pipe = i->pipe;\n-\tstruct pipe_buffer *buf;\n-\tint idx = i->idx;\n-\tsize_t off = i->iov_offset, orig_sz;\n-\n \tif (unlikely(i->count < size))\n \t\tsize = i->count;\n-\torig_sz = size;\n-\n \tif (size) {\n+\t\tstruct pipe_buffer *buf;\n+\t\tsize_t off = i->iov_offset, left = size;\n+\t\tint idx = i->idx;\n \t\tif (off) /* make it relative to the beginning of buffer */\n-\t\t\tsize += off - pipe->bufs[idx].offset;\n+\t\t\tleft += off - pipe->bufs[idx].offset;\n \t\twhile (1) {\n \t\t\tbuf = &pipe->bufs[idx];\n-\t\t\tif (size <= buf->len)\n+\t\t\tif (left <= buf->len)\n \t\t\t\tbreak;\n-\t\t\tsize -= buf->len;\n+\t\t\tleft -= buf->len;\n \t\t\tidx = next_idx(idx, pipe);\n \t\t}\n-\t\tbuf->len = size;\n \t\ti->idx = idx;\n-\t\toff = i->iov_offset = buf->offset + size;\n+\t\ti->iov_offset = buf->offset + left;\n \t}\n-\tif (off)\n-\t\tidx = next_idx(idx, pipe);\n-\tif (pipe->nrbufs) {\n-\t\tint unused = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n-\t\t/* [curbuf,unused) is in use.  Free [idx,unused) */\n-\t\twhile (idx != unused) {\n-\t\t\tpipe_buf_release(pipe, &pipe->bufs[idx]);\n-\t\t\tidx = next_idx(idx, pipe);\n-\t\t\tpipe->nrbufs--;\n-\t\t}\n-\t}\n-\ti->count -= orig_sz;\n+\ti->count -= size;\n+\t/* ... and discard everything past that point */\n+\tpipe_truncate(i);\n }",
        "function_modified_lines": {
            "added": [
                "\t\tstruct pipe_buffer *buf;",
                "\t\tsize_t off = i->iov_offset, left = size;",
                "\t\tint idx = i->idx;",
                "\t\t\tleft += off - pipe->bufs[idx].offset;",
                "\t\t\tif (left <= buf->len)",
                "\t\t\tleft -= buf->len;",
                "\t\ti->iov_offset = buf->offset + left;",
                "\ti->count -= size;",
                "\t/* ... and discard everything past that point */",
                "\tpipe_truncate(i);"
            ],
            "deleted": [
                "\tstruct pipe_buffer *buf;",
                "\tint idx = i->idx;",
                "\tsize_t off = i->iov_offset, orig_sz;",
                "",
                "\torig_sz = size;",
                "",
                "\t\t\tsize += off - pipe->bufs[idx].offset;",
                "\t\t\tif (size <= buf->len)",
                "\t\t\tsize -= buf->len;",
                "\t\tbuf->len = size;",
                "\t\toff = i->iov_offset = buf->offset + size;",
                "\tif (off)",
                "\t\tidx = next_idx(idx, pipe);",
                "\tif (pipe->nrbufs) {",
                "\t\tint unused = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);",
                "\t\t/* [curbuf,unused) is in use.  Free [idx,unused) */",
                "\t\twhile (idx != unused) {",
                "\t\t\tpipe_buf_release(pipe, &pipe->bufs[idx]);",
                "\t\t\tidx = next_idx(idx, pipe);",
                "\t\t\tpipe->nrbufs--;",
                "\t\t}",
                "\t}",
                "\ti->count -= orig_sz;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Off-by-one error in the pipe_advance function in lib/iov_iter.c in the Linux kernel before 4.9.5 allows local users to obtain sensitive information from uninitialized heap-memory locations in opportunistic circumstances by reading from a pipe after an incorrect buffer-release decision.",
        "id": 1467
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static int fill_grant_buffer(struct blkfront_ring_info *rinfo, int num)\n{\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct page *granted_page;\n\tstruct grant *gnt_list_entry, *n;\n\tint i = 0;\n\n\twhile (i < num) {\n\t\tgnt_list_entry = kzalloc(sizeof(struct grant), GFP_NOIO);\n\t\tif (!gnt_list_entry)\n\t\t\tgoto out_of_memory;\n\n\t\tif (info->feature_persistent) {\n\t\t\tgranted_page = alloc_page(GFP_NOIO | __GFP_ZERO);\n\t\t\tif (!granted_page) {\n\t\t\t\tkfree(gnt_list_entry);\n\t\t\t\tgoto out_of_memory;\n\t\t\t}\n\t\t\tgnt_list_entry->page = granted_page;\n\t\t}\n\n\t\tgnt_list_entry->gref = INVALID_GRANT_REF;\n\t\tlist_add(&gnt_list_entry->node, &rinfo->grants);\n\t\ti++;\n\t}\n\n\treturn 0;\n\nout_of_memory:\n\tlist_for_each_entry_safe(gnt_list_entry, n,\n\t                         &rinfo->grants, node) {\n\t\tlist_del(&gnt_list_entry->node);\n\t\tif (info->feature_persistent)\n\t\t\t__free_page(gnt_list_entry->page);\n\t\tkfree(gnt_list_entry);\n\t\ti--;\n\t}\n\tBUG_ON(i != 0);\n\treturn -ENOMEM;\n}",
        "code_after_change": "static int fill_grant_buffer(struct blkfront_ring_info *rinfo, int num)\n{\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct page *granted_page;\n\tstruct grant *gnt_list_entry, *n;\n\tint i = 0;\n\n\twhile (i < num) {\n\t\tgnt_list_entry = kzalloc(sizeof(struct grant), GFP_NOIO);\n\t\tif (!gnt_list_entry)\n\t\t\tgoto out_of_memory;\n\n\t\tif (info->bounce) {\n\t\t\tgranted_page = alloc_page(GFP_NOIO | __GFP_ZERO);\n\t\t\tif (!granted_page) {\n\t\t\t\tkfree(gnt_list_entry);\n\t\t\t\tgoto out_of_memory;\n\t\t\t}\n\t\t\tgnt_list_entry->page = granted_page;\n\t\t}\n\n\t\tgnt_list_entry->gref = INVALID_GRANT_REF;\n\t\tlist_add(&gnt_list_entry->node, &rinfo->grants);\n\t\ti++;\n\t}\n\n\treturn 0;\n\nout_of_memory:\n\tlist_for_each_entry_safe(gnt_list_entry, n,\n\t                         &rinfo->grants, node) {\n\t\tlist_del(&gnt_list_entry->node);\n\t\tif (info->bounce)\n\t\t\t__free_page(gnt_list_entry->page);\n\t\tkfree(gnt_list_entry);\n\t\ti--;\n\t}\n\tBUG_ON(i != 0);\n\treturn -ENOMEM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,7 @@\n \t\tif (!gnt_list_entry)\n \t\t\tgoto out_of_memory;\n \n-\t\tif (info->feature_persistent) {\n+\t\tif (info->bounce) {\n \t\t\tgranted_page = alloc_page(GFP_NOIO | __GFP_ZERO);\n \t\t\tif (!granted_page) {\n \t\t\t\tkfree(gnt_list_entry);\n@@ -30,7 +30,7 @@\n \tlist_for_each_entry_safe(gnt_list_entry, n,\n \t                         &rinfo->grants, node) {\n \t\tlist_del(&gnt_list_entry->node);\n-\t\tif (info->feature_persistent)\n+\t\tif (info->bounce)\n \t\t\t__free_page(gnt_list_entry->page);\n \t\tkfree(gnt_list_entry);\n \t\ti--;",
        "function_modified_lines": {
            "added": [
                "\t\tif (info->bounce) {",
                "\t\tif (info->bounce)"
            ],
            "deleted": [
                "\t\tif (info->feature_persistent) {",
                "\t\tif (info->feature_persistent)"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3589
    },
    {
        "cve_id": "CVE-2013-3076",
        "code_before_change": "static int hash_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\tstruct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct hash_ctx *ctx = ask->private;\n\tunsigned ds = crypto_ahash_digestsize(crypto_ahash_reqtfm(&ctx->req));\n\tint err;\n\n\tif (len > ds)\n\t\tlen = ds;\n\telse if (len < ds)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tlock_sock(sk);\n\tif (ctx->more) {\n\t\tctx->more = 0;\n\t\tahash_request_set_crypt(&ctx->req, NULL, ctx->result, 0);\n\t\terr = af_alg_wait_for_completion(crypto_ahash_final(&ctx->req),\n\t\t\t\t\t\t &ctx->completion);\n\t\tif (err)\n\t\t\tgoto unlock;\n\t}\n\n\terr = memcpy_toiovec(msg->msg_iov, ctx->result, len);\n\nunlock:\n\trelease_sock(sk);\n\n\treturn err ?: len;\n}",
        "code_after_change": "static int hash_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\tstruct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct hash_ctx *ctx = ask->private;\n\tunsigned ds = crypto_ahash_digestsize(crypto_ahash_reqtfm(&ctx->req));\n\tint err;\n\n\tif (len > ds)\n\t\tlen = ds;\n\telse if (len < ds)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tmsg->msg_namelen = 0;\n\n\tlock_sock(sk);\n\tif (ctx->more) {\n\t\tctx->more = 0;\n\t\tahash_request_set_crypt(&ctx->req, NULL, ctx->result, 0);\n\t\terr = af_alg_wait_for_completion(crypto_ahash_final(&ctx->req),\n\t\t\t\t\t\t &ctx->completion);\n\t\tif (err)\n\t\t\tgoto unlock;\n\t}\n\n\terr = memcpy_toiovec(msg->msg_iov, ctx->result, len);\n\nunlock:\n\trelease_sock(sk);\n\n\treturn err ?: len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,8 @@\n \t\tlen = ds;\n \telse if (len < ds)\n \t\tmsg->msg_flags |= MSG_TRUNC;\n+\n+\tmsg->msg_namelen = 0;\n \n \tlock_sock(sk);\n \tif (ctx->more) {",
        "function_modified_lines": {
            "added": [
                "",
                "\tmsg->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The crypto API in the Linux kernel through 3.9-rc8 does not initialize certain length variables, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call, related to the hash_recvmsg function in crypto/algif_hash.c and the skcipher_recvmsg function in crypto/algif_skcipher.c.",
        "id": 261
    },
    {
        "cve_id": "CVE-2018-12126",
        "code_before_change": "void __init nospec_auto_detect(void)\n{\n\tif (test_facility(156)) {\n\t\t/*\n\t\t * The machine supports etokens.\n\t\t * Disable expolines and disable nobp.\n\t\t */\n\t\tif (IS_ENABLED(CC_USING_EXPOLINE))\n\t\t\tnospec_disable = 1;\n\t\t__clear_facility(82, S390_lowcore.alt_stfle_fac_list);\n\t} else if (IS_ENABLED(CC_USING_EXPOLINE)) {\n\t\t/*\n\t\t * The kernel has been compiled with expolines.\n\t\t * Keep expolines enabled and disable nobp.\n\t\t */\n\t\tnospec_disable = 0;\n\t\t__clear_facility(82, S390_lowcore.alt_stfle_fac_list);\n\t}\n\t/*\n\t * If the kernel has not been compiled with expolines the\n\t * nobp setting decides what is done, this depends on the\n\t * CONFIG_KERNEL_NP option and the nobp/nospec parameters.\n\t */\n}",
        "code_after_change": "void __init nospec_auto_detect(void)\n{\n\tif (test_facility(156) || cpu_mitigations_off()) {\n\t\t/*\n\t\t * The machine supports etokens.\n\t\t * Disable expolines and disable nobp.\n\t\t */\n\t\tif (IS_ENABLED(CC_USING_EXPOLINE))\n\t\t\tnospec_disable = 1;\n\t\t__clear_facility(82, S390_lowcore.alt_stfle_fac_list);\n\t} else if (IS_ENABLED(CC_USING_EXPOLINE)) {\n\t\t/*\n\t\t * The kernel has been compiled with expolines.\n\t\t * Keep expolines enabled and disable nobp.\n\t\t */\n\t\tnospec_disable = 0;\n\t\t__clear_facility(82, S390_lowcore.alt_stfle_fac_list);\n\t}\n\t/*\n\t * If the kernel has not been compiled with expolines the\n\t * nobp setting decides what is done, this depends on the\n\t * CONFIG_KERNEL_NP option and the nobp/nospec parameters.\n\t */\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n void __init nospec_auto_detect(void)\n {\n-\tif (test_facility(156)) {\n+\tif (test_facility(156) || cpu_mitigations_off()) {\n \t\t/*\n \t\t * The machine supports etokens.\n \t\t * Disable expolines and disable nobp.",
        "function_modified_lines": {
            "added": [
                "\tif (test_facility(156) || cpu_mitigations_off()) {"
            ],
            "deleted": [
                "\tif (test_facility(156)) {"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Microarchitectural Store Buffer Data Sampling (MSBDS): Store buffers on some microprocessors utilizing speculative execution may allow an authenticated user to potentially enable information disclosure via a side channel with local access. A list of impacted products can be found here: https://www.intel.com/content/dam/www/public/us/en/documents/corporate-information/SA00233-microcode-update-guidance_05132019.pdf",
        "id": 1645
    },
    {
        "cve_id": "CVE-2018-20509",
        "code_before_change": "static int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tstruct binder_ref *ref = NULL;\n\t\t\tconst char *debug_string;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (target == 0 &&\n\t\t\t    (cmd == BC_INCREFS || cmd == BC_ACQUIRE)) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node) {\n\t\t\t\t\tref = binder_get_ref_for_node(proc,\n\t\t\t\t\t\t\tctx_mgr_node);\n\t\t\t\t\tif (ref && ref->desc != target) {\n\t\t\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc 0, got %d instead\\n\",\n\t\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t\tref->desc);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ref == NULL)\n\t\t\t\tref = binder_get_ref(proc, target,\n\t\t\t\t\t\t     cmd == BC_ACQUIRE ||\n\t\t\t\t\t\t     cmd == BC_RELEASE);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d refcount change on invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, target);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbinder_inc_ref(ref, 0, NULL);\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbinder_inc_ref(ref, 1, NULL);\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbinder_dec_ref(ref, 1);\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbinder_dec_ref(ref, 0);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string, ref->debug_id,\n\t\t\t\t     ref->desc, ref->strong, ref->weak, ref->node->debug_id);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tbinder_dec_node(node, cmd == BC_ACQUIRE_DONE, 0);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs, node->local_weak_refs);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (buffer == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!buffer->allow_user_free) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned buffer\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\n\t\t\tif (buffer->transaction) {\n\t\t\t\tbuffer->transaction->buffer = NULL;\n\t\t\t\tbuffer->transaction = NULL;\n\t\t\t}\n\t\t\tif (buffer->async_transaction && buffer->target_node) {\n\t\t\t\tBUG_ON(!buffer->target_node->has_async_transaction);\n\t\t\t\tif (list_empty(&buffer->target_node->async_todo))\n\t\t\t\t\tbuffer->target_node->has_async_transaction = 0;\n\t\t\t\telse\n\t\t\t\t\tlist_move_tail(buffer->target_node->async_todo.next, &thread->todo);\n\t\t\t}\n\t\t\ttrace_binder_transaction_buffer_release(buffer);\n\t\t\tbinder_transaction_buffer_release(proc, buffer, NULL);\n\t\t\tbinder_alloc_free_buf(&proc->alloc, buffer);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tref = binder_get_ref(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->debug_id, ref->desc,\n\t\t\t\t     ref->strong, ref->weak, ref->node->debug_id);\n\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tlist_add_tail(\n\t\t\t\t\t    &thread->return_error.work.entry,\n\t\t\t\t\t    &thread->todo);\n\t\t\t\t\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t     \"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\t     proc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\t\t\t\t\tif (thread->looper & (BINDER_LOOPER_STATE_REGISTERED | BINDER_LOOPER_STATE_ENTERED)) {\n\t\t\t\t\t\tlist_add_tail(&ref->death->work.entry, &thread->todo);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlist_add_tail(&ref->death->work.entry, &proc->todo);\n\t\t\t\t\t\twake_up_interruptible(&proc->wait);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper & (BINDER_LOOPER_STATE_REGISTERED | BINDER_LOOPER_STATE_ENTERED)) {\n\t\t\t\t\t\tlist_add_tail(&death->work.entry, &thread->todo);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlist_add_tail(&death->work.entry, &proc->todo);\n\t\t\t\t\t\twake_up_interruptible(&proc->wait);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t}\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death, entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death = container_of(w, struct binder_ref_death, work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %p\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tlist_del_init(&death->work.entry);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper & (BINDER_LOOPER_STATE_REGISTERED | BINDER_LOOPER_STATE_ENTERED)) {\n\t\t\t\t\tlist_add_tail(&death->work.entry, &thread->todo);\n\t\t\t\t} else {\n\t\t\t\t\tlist_add_tail(&death->work.entry, &proc->todo);\n\t\t\t\t\twake_up_interruptible(&proc->wait);\n\t\t\t\t}\n\t\t\t}\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tint ret;\n\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tconst char *debug_string;\n\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n\t\t\tstruct binder_ref_data rdata;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tret = -1;\n\t\t\tif (increment && !target) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node)\n\t\t\t\t\tret = binder_inc_ref_for_node(\n\t\t\t\t\t\t\tproc, ctx_mgr_node,\n\t\t\t\t\t\t\tstrong, NULL, &rdata);\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tret = binder_update_ref_for_handle(\n\t\t\t\t\t\tproc, target, increment, strong,\n\t\t\t\t\t\t&rdata);\n\t\t\tif (!ret && rdata.desc != target) {\n\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\ttarget, rdata.desc);\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, debug_string,\n\t\t\t\t\tstrong, target, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string,\n\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n\t\t\t\t     rdata.weak);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tbinder_dec_node(node, cmd == BC_ACQUIRE_DONE, 0);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs, node->local_weak_refs);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (buffer == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!buffer->allow_user_free) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned buffer\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\n\t\t\tif (buffer->transaction) {\n\t\t\t\tbuffer->transaction->buffer = NULL;\n\t\t\t\tbuffer->transaction = NULL;\n\t\t\t}\n\t\t\tif (buffer->async_transaction && buffer->target_node) {\n\t\t\t\tBUG_ON(!buffer->target_node->has_async_transaction);\n\t\t\t\tif (list_empty(&buffer->target_node->async_todo))\n\t\t\t\t\tbuffer->target_node->has_async_transaction = 0;\n\t\t\t\telse\n\t\t\t\t\tlist_move_tail(buffer->target_node->async_todo.next, &thread->todo);\n\t\t\t}\n\t\t\ttrace_binder_transaction_buffer_release(buffer);\n\t\t\tbinder_transaction_buffer_release(proc, buffer, NULL);\n\t\t\tbinder_alloc_free_buf(&proc->alloc, buffer);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tref = binder_get_ref(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->data.debug_id,\n\t\t\t\t     ref->data.desc, ref->data.strong,\n\t\t\t\t     ref->data.weak, ref->node->debug_id);\n\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tlist_add_tail(\n\t\t\t\t\t    &thread->return_error.work.entry,\n\t\t\t\t\t    &thread->todo);\n\t\t\t\t\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t     \"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\t     proc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\t\t\t\t\tif (thread->looper & (BINDER_LOOPER_STATE_REGISTERED | BINDER_LOOPER_STATE_ENTERED)) {\n\t\t\t\t\t\tlist_add_tail(&ref->death->work.entry, &thread->todo);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlist_add_tail(&ref->death->work.entry, &proc->todo);\n\t\t\t\t\t\twake_up_interruptible(&proc->wait);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper & (BINDER_LOOPER_STATE_REGISTERED | BINDER_LOOPER_STATE_ENTERED)) {\n\t\t\t\t\t\tlist_add_tail(&death->work.entry, &thread->todo);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlist_add_tail(&death->work.entry, &proc->todo);\n\t\t\t\t\t\twake_up_interruptible(&proc->wait);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t}\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death, entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death = container_of(w, struct binder_ref_death, work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %p\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tlist_del_init(&death->work.entry);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper & (BINDER_LOOPER_STATE_REGISTERED | BINDER_LOOPER_STATE_ENTERED)) {\n\t\t\t\t\tlist_add_tail(&death->work.entry, &thread->todo);\n\t\t\t\t} else {\n\t\t\t\t\tlist_add_tail(&death->work.entry, &proc->todo);\n\t\t\t\t\twake_up_interruptible(&proc->wait);\n\t\t\t\t}\n\t\t\t}\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,8 @@\n \tvoid __user *end = buffer + size;\n \n \twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n+\t\tint ret;\n+\n \t\tif (get_user(cmd, (uint32_t __user *)ptr))\n \t\t\treturn -EFAULT;\n \t\tptr += sizeof(uint32_t);\n@@ -25,62 +27,61 @@\n \t\tcase BC_RELEASE:\n \t\tcase BC_DECREFS: {\n \t\t\tuint32_t target;\n-\t\t\tstruct binder_ref *ref = NULL;\n \t\t\tconst char *debug_string;\n+\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n+\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n+\t\t\tstruct binder_ref_data rdata;\n \n \t\t\tif (get_user(target, (uint32_t __user *)ptr))\n \t\t\t\treturn -EFAULT;\n \n \t\t\tptr += sizeof(uint32_t);\n-\t\t\tif (target == 0 &&\n-\t\t\t    (cmd == BC_INCREFS || cmd == BC_ACQUIRE)) {\n+\t\t\tret = -1;\n+\t\t\tif (increment && !target) {\n \t\t\t\tstruct binder_node *ctx_mgr_node;\n-\n \t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n \t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n-\t\t\t\tif (ctx_mgr_node) {\n-\t\t\t\t\tref = binder_get_ref_for_node(proc,\n-\t\t\t\t\t\t\tctx_mgr_node);\n-\t\t\t\t\tif (ref && ref->desc != target) {\n-\t\t\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc 0, got %d instead\\n\",\n-\t\t\t\t\t\t\tproc->pid, thread->pid,\n-\t\t\t\t\t\t\tref->desc);\n-\t\t\t\t\t}\n-\t\t\t\t}\n+\t\t\t\tif (ctx_mgr_node)\n+\t\t\t\t\tret = binder_inc_ref_for_node(\n+\t\t\t\t\t\t\tproc, ctx_mgr_node,\n+\t\t\t\t\t\t\tstrong, NULL, &rdata);\n \t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n \t\t\t}\n-\t\t\tif (ref == NULL)\n-\t\t\t\tref = binder_get_ref(proc, target,\n-\t\t\t\t\t\t     cmd == BC_ACQUIRE ||\n-\t\t\t\t\t\t     cmd == BC_RELEASE);\n-\t\t\tif (ref == NULL) {\n-\t\t\t\tbinder_user_error(\"%d:%d refcount change on invalid ref %d\\n\",\n-\t\t\t\t\tproc->pid, thread->pid, target);\n-\t\t\t\tbreak;\n+\t\t\tif (ret)\n+\t\t\t\tret = binder_update_ref_for_handle(\n+\t\t\t\t\t\tproc, target, increment, strong,\n+\t\t\t\t\t\t&rdata);\n+\t\t\tif (!ret && rdata.desc != target) {\n+\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n+\t\t\t\t\tproc->pid, thread->pid,\n+\t\t\t\t\ttarget, rdata.desc);\n \t\t\t}\n \t\t\tswitch (cmd) {\n \t\t\tcase BC_INCREFS:\n \t\t\t\tdebug_string = \"IncRefs\";\n-\t\t\t\tbinder_inc_ref(ref, 0, NULL);\n \t\t\t\tbreak;\n \t\t\tcase BC_ACQUIRE:\n \t\t\t\tdebug_string = \"Acquire\";\n-\t\t\t\tbinder_inc_ref(ref, 1, NULL);\n \t\t\t\tbreak;\n \t\t\tcase BC_RELEASE:\n \t\t\t\tdebug_string = \"Release\";\n-\t\t\t\tbinder_dec_ref(ref, 1);\n \t\t\t\tbreak;\n \t\t\tcase BC_DECREFS:\n \t\t\tdefault:\n \t\t\t\tdebug_string = \"DecRefs\";\n-\t\t\t\tbinder_dec_ref(ref, 0);\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tif (ret) {\n+\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n+\t\t\t\t\tproc->pid, thread->pid, debug_string,\n+\t\t\t\t\tstrong, target, ret);\n \t\t\t\tbreak;\n \t\t\t}\n \t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n-\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d for node %d\\n\",\n-\t\t\t\t     proc->pid, thread->pid, debug_string, ref->debug_id,\n-\t\t\t\t     ref->desc, ref->strong, ref->weak, ref->node->debug_id);\n+\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n+\t\t\t\t     proc->pid, thread->pid, debug_string,\n+\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n+\t\t\t\t     rdata.weak);\n \t\t\tbreak;\n \t\t}\n \t\tcase BC_INCREFS_DONE:\n@@ -278,8 +279,9 @@\n \t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n \t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n \t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n-\t\t\t\t     (u64)cookie, ref->debug_id, ref->desc,\n-\t\t\t\t     ref->strong, ref->weak, ref->node->debug_id);\n+\t\t\t\t     (u64)cookie, ref->data.debug_id,\n+\t\t\t\t     ref->data.desc, ref->data.strong,\n+\t\t\t\t     ref->data.weak, ref->node->debug_id);\n \n \t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n \t\t\t\tif (ref->death) {",
        "function_modified_lines": {
            "added": [
                "\t\tint ret;",
                "",
                "\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;",
                "\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;",
                "\t\t\tstruct binder_ref_data rdata;",
                "\t\t\tret = -1;",
                "\t\t\tif (increment && !target) {",
                "\t\t\t\tif (ctx_mgr_node)",
                "\t\t\t\t\tret = binder_inc_ref_for_node(",
                "\t\t\t\t\t\t\tproc, ctx_mgr_node,",
                "\t\t\t\t\t\t\tstrong, NULL, &rdata);",
                "\t\t\tif (ret)",
                "\t\t\t\tret = binder_update_ref_for_handle(",
                "\t\t\t\t\t\tproc, target, increment, strong,",
                "\t\t\t\t\t\t&rdata);",
                "\t\t\tif (!ret && rdata.desc != target) {",
                "\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",",
                "\t\t\t\t\tproc->pid, thread->pid,",
                "\t\t\t\t\ttarget, rdata.desc);",
                "\t\t\t\tbreak;",
                "\t\t\t}",
                "\t\t\tif (ret) {",
                "\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",",
                "\t\t\t\t\tproc->pid, thread->pid, debug_string,",
                "\t\t\t\t\tstrong, target, ret);",
                "\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",",
                "\t\t\t\t     proc->pid, thread->pid, debug_string,",
                "\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,",
                "\t\t\t\t     rdata.weak);",
                "\t\t\t\t     (u64)cookie, ref->data.debug_id,",
                "\t\t\t\t     ref->data.desc, ref->data.strong,",
                "\t\t\t\t     ref->data.weak, ref->node->debug_id);"
            ],
            "deleted": [
                "\t\t\tstruct binder_ref *ref = NULL;",
                "\t\t\tif (target == 0 &&",
                "\t\t\t    (cmd == BC_INCREFS || cmd == BC_ACQUIRE)) {",
                "",
                "\t\t\t\tif (ctx_mgr_node) {",
                "\t\t\t\t\tref = binder_get_ref_for_node(proc,",
                "\t\t\t\t\t\t\tctx_mgr_node);",
                "\t\t\t\t\tif (ref && ref->desc != target) {",
                "\t\t\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc 0, got %d instead\\n\",",
                "\t\t\t\t\t\t\tproc->pid, thread->pid,",
                "\t\t\t\t\t\t\tref->desc);",
                "\t\t\t\t\t}",
                "\t\t\t\t}",
                "\t\t\tif (ref == NULL)",
                "\t\t\t\tref = binder_get_ref(proc, target,",
                "\t\t\t\t\t\t     cmd == BC_ACQUIRE ||",
                "\t\t\t\t\t\t     cmd == BC_RELEASE);",
                "\t\t\tif (ref == NULL) {",
                "\t\t\t\tbinder_user_error(\"%d:%d refcount change on invalid ref %d\\n\",",
                "\t\t\t\t\tproc->pid, thread->pid, target);",
                "\t\t\t\tbreak;",
                "\t\t\t\tbinder_inc_ref(ref, 0, NULL);",
                "\t\t\t\tbinder_inc_ref(ref, 1, NULL);",
                "\t\t\t\tbinder_dec_ref(ref, 1);",
                "\t\t\t\tbinder_dec_ref(ref, 0);",
                "\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d for node %d\\n\",",
                "\t\t\t\t     proc->pid, thread->pid, debug_string, ref->debug_id,",
                "\t\t\t\t     ref->desc, ref->strong, ref->weak, ref->node->debug_id);",
                "\t\t\t\t     (u64)cookie, ref->debug_id, ref->desc,",
                "\t\t\t\t     ref->strong, ref->weak, ref->node->debug_id);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_ref_olocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \" ref *desc *node\" lines in a debugfs file.",
        "id": 1767
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "static inline bool interrupted_kernel_fpu_idle(void)\n{\n\tif (use_xsave())\n\t\treturn 0;\n\n\treturn !__thread_has_fpu(current) &&\n\t\t(read_cr0() & X86_CR0_TS);\n}",
        "code_after_change": "static inline bool interrupted_kernel_fpu_idle(void)\n{\n\tif (use_eager_fpu())\n\t\treturn 0;\n\n\treturn !__thread_has_fpu(current) &&\n\t\t(read_cr0() & X86_CR0_TS);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static inline bool interrupted_kernel_fpu_idle(void)\n {\n-\tif (use_xsave())\n+\tif (use_eager_fpu())\n \t\treturn 0;\n \n \treturn !__thread_has_fpu(current) &&",
        "function_modified_lines": {
            "added": [
                "\tif (use_eager_fpu())"
            ],
            "deleted": [
                "\tif (use_xsave())"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1807
    },
    {
        "cve_id": "CVE-2015-8950",
        "code_before_change": "static void *__alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)\n{\n\tunsigned long val;\n\tvoid *ptr = NULL;\n\n\tif (!atomic_pool) {\n\t\tWARN(1, \"coherent pool not initialised!\\n\");\n\t\treturn NULL;\n\t}\n\n\tval = gen_pool_alloc(atomic_pool, size);\n\tif (val) {\n\t\tphys_addr_t phys = gen_pool_virt_to_phys(atomic_pool, val);\n\n\t\t*ret_page = phys_to_page(phys);\n\t\tptr = (void *)val;\n\t\tif (flags & __GFP_ZERO)\n\t\t\tmemset(ptr, 0, size);\n\t}\n\n\treturn ptr;\n}",
        "code_after_change": "static void *__alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)\n{\n\tunsigned long val;\n\tvoid *ptr = NULL;\n\n\tif (!atomic_pool) {\n\t\tWARN(1, \"coherent pool not initialised!\\n\");\n\t\treturn NULL;\n\t}\n\n\tval = gen_pool_alloc(atomic_pool, size);\n\tif (val) {\n\t\tphys_addr_t phys = gen_pool_virt_to_phys(atomic_pool, val);\n\n\t\t*ret_page = phys_to_page(phys);\n\t\tptr = (void *)val;\n\t\tmemset(ptr, 0, size);\n\t}\n\n\treturn ptr;\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,8 +14,7 @@\n \n \t\t*ret_page = phys_to_page(phys);\n \t\tptr = (void *)val;\n-\t\tif (flags & __GFP_ZERO)\n-\t\t\tmemset(ptr, 0, size);\n+\t\tmemset(ptr, 0, size);\n \t}\n \n \treturn ptr;",
        "function_modified_lines": {
            "added": [
                "\t\tmemset(ptr, 0, size);"
            ],
            "deleted": [
                "\t\tif (flags & __GFP_ZERO)",
                "\t\t\tmemset(ptr, 0, size);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "arch/arm64/mm/dma-mapping.c in the Linux kernel before 4.0.3, as used in the ION subsystem in Android and other products, does not initialize certain data structures, which allows local users to obtain sensitive information from kernel memory by triggering a dma_mmap call.",
        "id": 865
    },
    {
        "cve_id": "CVE-2018-20509",
        "code_before_change": "static int binder_inc_ref(struct binder_ref *ref, int strong,\n\t\t\t  struct list_head *target_list)\n{\n\tint ret;\n\n\tif (strong) {\n\t\tif (ref->strong == 0) {\n\t\t\tret = binder_inc_node(ref->node, 1, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->strong++;\n\t} else {\n\t\tif (ref->weak == 0) {\n\t\t\tret = binder_inc_node(ref->node, 0, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->weak++;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int binder_inc_ref(struct binder_ref *ref, int strong,\n\t\t\t  struct list_head *target_list)\n{\n\tint ret;\n\n\tif (strong) {\n\t\tif (ref->data.strong == 0) {\n\t\t\tret = binder_inc_node(ref->node, 1, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->data.strong++;\n\t} else {\n\t\tif (ref->data.weak == 0) {\n\t\t\tret = binder_inc_node(ref->node, 0, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->data.weak++;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,19 +4,19 @@\n \tint ret;\n \n \tif (strong) {\n-\t\tif (ref->strong == 0) {\n+\t\tif (ref->data.strong == 0) {\n \t\t\tret = binder_inc_node(ref->node, 1, 1, target_list);\n \t\t\tif (ret)\n \t\t\t\treturn ret;\n \t\t}\n-\t\tref->strong++;\n+\t\tref->data.strong++;\n \t} else {\n-\t\tif (ref->weak == 0) {\n+\t\tif (ref->data.weak == 0) {\n \t\t\tret = binder_inc_node(ref->node, 0, 1, target_list);\n \t\t\tif (ret)\n \t\t\t\treturn ret;\n \t\t}\n-\t\tref->weak++;\n+\t\tref->data.weak++;\n \t}\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\t\tif (ref->data.strong == 0) {",
                "\t\tref->data.strong++;",
                "\t\tif (ref->data.weak == 0) {",
                "\t\tref->data.weak++;"
            ],
            "deleted": [
                "\t\tif (ref->strong == 0) {",
                "\t\tref->strong++;",
                "\t\tif (ref->weak == 0) {",
                "\t\tref->weak++;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_ref_olocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \" ref *desc *node\" lines in a debugfs file.",
        "id": 1762
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "dotraplinkage void __kprobes\ndo_device_not_available(struct pt_regs *regs, long error_code)\n{\n\tBUG_ON(use_xsave());\n\n#ifdef CONFIG_MATH_EMULATION\n\tif (read_cr0() & X86_CR0_EM) {\n\t\tstruct math_emu_info info = { };\n\n\t\tconditional_sti(regs);\n\n\t\tinfo.regs = regs;\n\t\tmath_emulate(&info);\n\t\treturn;\n\t}\n#endif\n\tmath_state_restore(); /* interrupts still off */\n#ifdef CONFIG_X86_32\n\tconditional_sti(regs);\n#endif\n}",
        "code_after_change": "dotraplinkage void __kprobes\ndo_device_not_available(struct pt_regs *regs, long error_code)\n{\n\tBUG_ON(use_eager_fpu());\n\n#ifdef CONFIG_MATH_EMULATION\n\tif (read_cr0() & X86_CR0_EM) {\n\t\tstruct math_emu_info info = { };\n\n\t\tconditional_sti(regs);\n\n\t\tinfo.regs = regs;\n\t\tmath_emulate(&info);\n\t\treturn;\n\t}\n#endif\n\tmath_state_restore(); /* interrupts still off */\n#ifdef CONFIG_X86_32\n\tconditional_sti(regs);\n#endif\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n dotraplinkage void __kprobes\n do_device_not_available(struct pt_regs *regs, long error_code)\n {\n-\tBUG_ON(use_xsave());\n+\tBUG_ON(use_eager_fpu());\n \n #ifdef CONFIG_MATH_EMULATION\n \tif (read_cr0() & X86_CR0_EM) {",
        "function_modified_lines": {
            "added": [
                "\tBUG_ON(use_eager_fpu());"
            ],
            "deleted": [
                "\tBUG_ON(use_xsave());"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1813
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "static void __init xstate_enable_boot_cpu(void)\n{\n\tunsigned int eax, ebx, ecx, edx;\n\n\tif (boot_cpu_data.cpuid_level < XSTATE_CPUID) {\n\t\tWARN(1, KERN_ERR \"XSTATE_CPUID missing\\n\");\n\t\treturn;\n\t}\n\n\tcpuid_count(XSTATE_CPUID, 0, &eax, &ebx, &ecx, &edx);\n\tpcntxt_mask = eax + ((u64)edx << 32);\n\n\tif ((pcntxt_mask & XSTATE_FPSSE) != XSTATE_FPSSE) {\n\t\tpr_err(\"FP/SSE not shown under xsave features 0x%llx\\n\",\n\t\t       pcntxt_mask);\n\t\tBUG();\n\t}\n\n\t/*\n\t * Support only the state known to OS.\n\t */\n\tpcntxt_mask = pcntxt_mask & XCNTXT_MASK;\n\n\txstate_enable();\n\n\t/*\n\t * Recompute the context size for enabled features\n\t */\n\tcpuid_count(XSTATE_CPUID, 0, &eax, &ebx, &ecx, &edx);\n\txstate_size = ebx;\n\n\tupdate_regset_xstate_info(xstate_size, pcntxt_mask);\n\tprepare_fx_sw_frame();\n\n\tsetup_xstate_init();\n\n\tpr_info(\"enabled xstate_bv 0x%llx, cntxt size 0x%x\\n\",\n\t\tpcntxt_mask, xstate_size);\n\n\tcurrent->thread.fpu.state =\n\t     alloc_bootmem_align(xstate_size, __alignof__(struct xsave_struct));\n\tinit_restore_xstate();\n}",
        "code_after_change": "static void __init xstate_enable_boot_cpu(void)\n{\n\tunsigned int eax, ebx, ecx, edx;\n\n\tif (boot_cpu_data.cpuid_level < XSTATE_CPUID) {\n\t\tWARN(1, KERN_ERR \"XSTATE_CPUID missing\\n\");\n\t\treturn;\n\t}\n\n\tcpuid_count(XSTATE_CPUID, 0, &eax, &ebx, &ecx, &edx);\n\tpcntxt_mask = eax + ((u64)edx << 32);\n\n\tif ((pcntxt_mask & XSTATE_FPSSE) != XSTATE_FPSSE) {\n\t\tpr_err(\"FP/SSE not shown under xsave features 0x%llx\\n\",\n\t\t       pcntxt_mask);\n\t\tBUG();\n\t}\n\n\t/*\n\t * Support only the state known to OS.\n\t */\n\tpcntxt_mask = pcntxt_mask & XCNTXT_MASK;\n\n\txstate_enable();\n\n\t/*\n\t * Recompute the context size for enabled features\n\t */\n\tcpuid_count(XSTATE_CPUID, 0, &eax, &ebx, &ecx, &edx);\n\txstate_size = ebx;\n\n\tupdate_regset_xstate_info(xstate_size, pcntxt_mask);\n\tprepare_fx_sw_frame();\n\tsetup_init_fpu_buf();\n\n\tpr_info(\"enabled xstate_bv 0x%llx, cntxt size 0x%x\\n\",\n\t\tpcntxt_mask, xstate_size);\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,13 +31,8 @@\n \n \tupdate_regset_xstate_info(xstate_size, pcntxt_mask);\n \tprepare_fx_sw_frame();\n-\n-\tsetup_xstate_init();\n+\tsetup_init_fpu_buf();\n \n \tpr_info(\"enabled xstate_bv 0x%llx, cntxt size 0x%x\\n\",\n \t\tpcntxt_mask, xstate_size);\n-\n-\tcurrent->thread.fpu.state =\n-\t     alloc_bootmem_align(xstate_size, __alignof__(struct xsave_struct));\n-\tinit_restore_xstate();\n }",
        "function_modified_lines": {
            "added": [
                "\tsetup_init_fpu_buf();"
            ],
            "deleted": [
                "",
                "\tsetup_xstate_init();",
                "",
                "\tcurrent->thread.fpu.state =",
                "\t     alloc_bootmem_align(xstate_size, __alignof__(struct xsave_struct));",
                "\tinit_restore_xstate();"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1816
    },
    {
        "cve_id": "CVE-2014-1739",
        "code_before_change": "static long media_device_enum_entities(struct media_device *mdev,\n\t\t\t\t       struct media_entity_desc __user *uent)\n{\n\tstruct media_entity *ent;\n\tstruct media_entity_desc u_ent;\n\n\tif (copy_from_user(&u_ent.id, &uent->id, sizeof(u_ent.id)))\n\t\treturn -EFAULT;\n\n\tent = find_entity(mdev, u_ent.id);\n\n\tif (ent == NULL)\n\t\treturn -EINVAL;\n\n\tu_ent.id = ent->id;\n\tif (ent->name) {\n\t\tstrncpy(u_ent.name, ent->name, sizeof(u_ent.name));\n\t\tu_ent.name[sizeof(u_ent.name) - 1] = '\\0';\n\t} else {\n\t\tmemset(u_ent.name, 0, sizeof(u_ent.name));\n\t}\n\tu_ent.type = ent->type;\n\tu_ent.revision = ent->revision;\n\tu_ent.flags = ent->flags;\n\tu_ent.group_id = ent->group_id;\n\tu_ent.pads = ent->num_pads;\n\tu_ent.links = ent->num_links - ent->num_backlinks;\n\tmemcpy(&u_ent.raw, &ent->info, sizeof(ent->info));\n\tif (copy_to_user(uent, &u_ent, sizeof(u_ent)))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "code_after_change": "static long media_device_enum_entities(struct media_device *mdev,\n\t\t\t\t       struct media_entity_desc __user *uent)\n{\n\tstruct media_entity *ent;\n\tstruct media_entity_desc u_ent;\n\n\tmemset(&u_ent, 0, sizeof(u_ent));\n\tif (copy_from_user(&u_ent.id, &uent->id, sizeof(u_ent.id)))\n\t\treturn -EFAULT;\n\n\tent = find_entity(mdev, u_ent.id);\n\n\tif (ent == NULL)\n\t\treturn -EINVAL;\n\n\tu_ent.id = ent->id;\n\tif (ent->name) {\n\t\tstrncpy(u_ent.name, ent->name, sizeof(u_ent.name));\n\t\tu_ent.name[sizeof(u_ent.name) - 1] = '\\0';\n\t} else {\n\t\tmemset(u_ent.name, 0, sizeof(u_ent.name));\n\t}\n\tu_ent.type = ent->type;\n\tu_ent.revision = ent->revision;\n\tu_ent.flags = ent->flags;\n\tu_ent.group_id = ent->group_id;\n\tu_ent.pads = ent->num_pads;\n\tu_ent.links = ent->num_links - ent->num_backlinks;\n\tmemcpy(&u_ent.raw, &ent->info, sizeof(ent->info));\n\tif (copy_to_user(uent, &u_ent, sizeof(u_ent)))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \tstruct media_entity *ent;\n \tstruct media_entity_desc u_ent;\n \n+\tmemset(&u_ent, 0, sizeof(u_ent));\n \tif (copy_from_user(&u_ent.id, &uent->id, sizeof(u_ent.id)))\n \t\treturn -EFAULT;\n ",
        "function_modified_lines": {
            "added": [
                "\tmemset(&u_ent, 0, sizeof(u_ent));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The media_device_enum_entities function in drivers/media/media-device.c in the Linux kernel before 3.14.6 does not initialize a certain data structure, which allows local users to obtain sensitive information from kernel memory by leveraging /dev/media0 read access for a MEDIA_IOC_ENUM_ENTITIES ioctl call.",
        "id": 477
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "void fpu_finit(struct fpu *fpu)\n{\n\tif (!HAVE_HWFP) {\n\t\tfinit_soft_fpu(&fpu->state->soft);\n\t\treturn;\n\t}\n\n\tif (cpu_has_fxsr) {\n\t\tstruct i387_fxsave_struct *fx = &fpu->state->fxsave;\n\n\t\tmemset(fx, 0, xstate_size);\n\t\tfx->cwd = 0x37f;\n\t\tif (cpu_has_xmm)\n\t\t\tfx->mxcsr = MXCSR_DEFAULT;\n\t} else {\n\t\tstruct i387_fsave_struct *fp = &fpu->state->fsave;\n\t\tmemset(fp, 0, xstate_size);\n\t\tfp->cwd = 0xffff037fu;\n\t\tfp->swd = 0xffff0000u;\n\t\tfp->twd = 0xffffffffu;\n\t\tfp->fos = 0xffff0000u;\n\t}\n}",
        "code_after_change": "void fpu_finit(struct fpu *fpu)\n{\n\tif (!HAVE_HWFP) {\n\t\tfinit_soft_fpu(&fpu->state->soft);\n\t\treturn;\n\t}\n\n\tif (cpu_has_fxsr) {\n\t\tfx_finit(&fpu->state->fxsave);\n\t} else {\n\t\tstruct i387_fsave_struct *fp = &fpu->state->fsave;\n\t\tmemset(fp, 0, xstate_size);\n\t\tfp->cwd = 0xffff037fu;\n\t\tfp->swd = 0xffff0000u;\n\t\tfp->twd = 0xffffffffu;\n\t\tfp->fos = 0xffff0000u;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,12 +6,7 @@\n \t}\n \n \tif (cpu_has_fxsr) {\n-\t\tstruct i387_fxsave_struct *fx = &fpu->state->fxsave;\n-\n-\t\tmemset(fx, 0, xstate_size);\n-\t\tfx->cwd = 0x37f;\n-\t\tif (cpu_has_xmm)\n-\t\t\tfx->mxcsr = MXCSR_DEFAULT;\n+\t\tfx_finit(&fpu->state->fxsave);\n \t} else {\n \t\tstruct i387_fsave_struct *fp = &fpu->state->fsave;\n \t\tmemset(fp, 0, xstate_size);",
        "function_modified_lines": {
            "added": [
                "\t\tfx_finit(&fpu->state->fxsave);"
            ],
            "deleted": [
                "\t\tstruct i387_fxsave_struct *fx = &fpu->state->fxsave;",
                "",
                "\t\tmemset(fx, 0, xstate_size);",
                "\t\tfx->cwd = 0x37f;",
                "\t\tif (cpu_has_xmm)",
                "\t\t\tfx->mxcsr = MXCSR_DEFAULT;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1810
    },
    {
        "cve_id": "CVE-2018-20509",
        "code_before_change": "static void print_binder_proc_stats(struct seq_file *m,\n\t\t\t\t    struct binder_proc *proc)\n{\n\tstruct binder_work *w;\n\tstruct rb_node *n;\n\tint count, strong, weak;\n\n\tseq_printf(m, \"proc %d\\n\", proc->pid);\n\tseq_printf(m, \"context %s\\n\", proc->context->name);\n\tcount = 0;\n\tfor (n = rb_first(&proc->threads); n != NULL; n = rb_next(n))\n\t\tcount++;\n\tseq_printf(m, \"  threads: %d\\n\", count);\n\tseq_printf(m, \"  requested threads: %d+%d/%d\\n\"\n\t\t\t\"  ready threads %d\\n\"\n\t\t\t\"  free async space %zd\\n\", proc->requested_threads,\n\t\t\tproc->requested_threads_started, proc->max_threads,\n\t\t\tproc->ready_threads,\n\t\t\tbinder_alloc_get_free_async_space(&proc->alloc));\n\tcount = 0;\n\tfor (n = rb_first(&proc->nodes); n != NULL; n = rb_next(n))\n\t\tcount++;\n\tseq_printf(m, \"  nodes: %d\\n\", count);\n\tcount = 0;\n\tstrong = 0;\n\tweak = 0;\n\tfor (n = rb_first(&proc->refs_by_desc); n != NULL; n = rb_next(n)) {\n\t\tstruct binder_ref *ref = rb_entry(n, struct binder_ref,\n\t\t\t\t\t\t  rb_node_desc);\n\t\tcount++;\n\t\tstrong += ref->strong;\n\t\tweak += ref->weak;\n\t}\n\tseq_printf(m, \"  refs: %d s %d w %d\\n\", count, strong, weak);\n\n\tcount = binder_alloc_get_allocated_count(&proc->alloc);\n\tseq_printf(m, \"  buffers: %d\\n\", count);\n\n\tcount = 0;\n\tlist_for_each_entry(w, &proc->todo, entry) {\n\t\tswitch (w->type) {\n\t\tcase BINDER_WORK_TRANSACTION:\n\t\t\tcount++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tseq_printf(m, \"  pending transactions: %d\\n\", count);\n\n\tprint_binder_stats(m, \"  \", &proc->stats);\n}",
        "code_after_change": "static void print_binder_proc_stats(struct seq_file *m,\n\t\t\t\t    struct binder_proc *proc)\n{\n\tstruct binder_work *w;\n\tstruct rb_node *n;\n\tint count, strong, weak;\n\n\tseq_printf(m, \"proc %d\\n\", proc->pid);\n\tseq_printf(m, \"context %s\\n\", proc->context->name);\n\tcount = 0;\n\tfor (n = rb_first(&proc->threads); n != NULL; n = rb_next(n))\n\t\tcount++;\n\tseq_printf(m, \"  threads: %d\\n\", count);\n\tseq_printf(m, \"  requested threads: %d+%d/%d\\n\"\n\t\t\t\"  ready threads %d\\n\"\n\t\t\t\"  free async space %zd\\n\", proc->requested_threads,\n\t\t\tproc->requested_threads_started, proc->max_threads,\n\t\t\tproc->ready_threads,\n\t\t\tbinder_alloc_get_free_async_space(&proc->alloc));\n\tcount = 0;\n\tfor (n = rb_first(&proc->nodes); n != NULL; n = rb_next(n))\n\t\tcount++;\n\tseq_printf(m, \"  nodes: %d\\n\", count);\n\tcount = 0;\n\tstrong = 0;\n\tweak = 0;\n\tfor (n = rb_first(&proc->refs_by_desc); n != NULL; n = rb_next(n)) {\n\t\tstruct binder_ref *ref = rb_entry(n, struct binder_ref,\n\t\t\t\t\t\t  rb_node_desc);\n\t\tcount++;\n\t\tstrong += ref->data.strong;\n\t\tweak += ref->data.weak;\n\t}\n\tseq_printf(m, \"  refs: %d s %d w %d\\n\", count, strong, weak);\n\n\tcount = binder_alloc_get_allocated_count(&proc->alloc);\n\tseq_printf(m, \"  buffers: %d\\n\", count);\n\n\tcount = 0;\n\tlist_for_each_entry(w, &proc->todo, entry) {\n\t\tswitch (w->type) {\n\t\tcase BINDER_WORK_TRANSACTION:\n\t\t\tcount++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tseq_printf(m, \"  pending transactions: %d\\n\", count);\n\n\tprint_binder_stats(m, \"  \", &proc->stats);\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,8 +28,8 @@\n \t\tstruct binder_ref *ref = rb_entry(n, struct binder_ref,\n \t\t\t\t\t\t  rb_node_desc);\n \t\tcount++;\n-\t\tstrong += ref->strong;\n-\t\tweak += ref->weak;\n+\t\tstrong += ref->data.strong;\n+\t\tweak += ref->data.weak;\n \t}\n \tseq_printf(m, \"  refs: %d s %d w %d\\n\", count, strong, weak);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tstrong += ref->data.strong;",
                "\t\tweak += ref->data.weak;"
            ],
            "deleted": [
                "\t\tstrong += ref->strong;",
                "\t\tweak += ref->weak;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_ref_olocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \" ref *desc *node\" lines in a debugfs file.",
        "id": 1768
    },
    {
        "cve_id": "CVE-2018-16658",
        "code_before_change": "static int cdrom_ioctl_drive_status(struct cdrom_device_info *cdi,\n\t\tunsigned long arg)\n{\n\tcd_dbg(CD_DO_IOCTL, \"entering CDROM_DRIVE_STATUS\\n\");\n\n\tif (!(cdi->ops->capability & CDC_DRIVE_STATUS))\n\t\treturn -ENOSYS;\n\tif (!CDROM_CAN(CDC_SELECT_DISC) ||\n\t    (arg == CDSL_CURRENT || arg == CDSL_NONE))\n\t\treturn cdi->ops->drive_status(cdi, CDSL_CURRENT);\n\tif (((int)arg >= cdi->capacity))\n\t\treturn -EINVAL;\n\treturn cdrom_slot_status(cdi, arg);\n}",
        "code_after_change": "static int cdrom_ioctl_drive_status(struct cdrom_device_info *cdi,\n\t\tunsigned long arg)\n{\n\tcd_dbg(CD_DO_IOCTL, \"entering CDROM_DRIVE_STATUS\\n\");\n\n\tif (!(cdi->ops->capability & CDC_DRIVE_STATUS))\n\t\treturn -ENOSYS;\n\tif (!CDROM_CAN(CDC_SELECT_DISC) ||\n\t    (arg == CDSL_CURRENT || arg == CDSL_NONE))\n\t\treturn cdi->ops->drive_status(cdi, CDSL_CURRENT);\n\tif (arg >= cdi->capacity)\n\t\treturn -EINVAL;\n\treturn cdrom_slot_status(cdi, arg);\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \tif (!CDROM_CAN(CDC_SELECT_DISC) ||\n \t    (arg == CDSL_CURRENT || arg == CDSL_NONE))\n \t\treturn cdi->ops->drive_status(cdi, CDSL_CURRENT);\n-\tif (((int)arg >= cdi->capacity))\n+\tif (arg >= cdi->capacity)\n \t\treturn -EINVAL;\n \treturn cdrom_slot_status(cdi, arg);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (arg >= cdi->capacity)"
            ],
            "deleted": [
                "\tif (((int)arg >= cdi->capacity))"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.18.6. An information leak in cdrom_ioctl_drive_status in drivers/cdrom/cdrom.c could be used by local attackers to read kernel memory because a cast from unsigned long to int interferes with bounds checking. This is similar to CVE-2018-10940.",
        "id": 1714
    },
    {
        "cve_id": "CVE-2018-20511",
        "code_before_change": "static int ipddp_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n        struct ipddp_route __user *rt = ifr->ifr_data;\n        struct ipddp_route rcp, rcp2, *rp;\n\n        if(!capable(CAP_NET_ADMIN))\n                return -EPERM;\n\n\tif(copy_from_user(&rcp, rt, sizeof(rcp)))\n\t\treturn -EFAULT;\n\n        switch(cmd)\n        {\n\t\tcase SIOCADDIPDDPRT:\n                        return ipddp_create(&rcp);\n\n                case SIOCFINDIPDDPRT:\n\t\t\tspin_lock_bh(&ipddp_route_lock);\n\t\t\trp = __ipddp_find_route(&rcp);\n\t\t\tif (rp)\n\t\t\t\tmemcpy(&rcp2, rp, sizeof(rcp2));\n\t\t\tspin_unlock_bh(&ipddp_route_lock);\n\n\t\t\tif (rp) {\n\t\t\t\tif (copy_to_user(rt, &rcp2,\n\t\t\t\t\t\t sizeof(struct ipddp_route)))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\treturn 0;\n\t\t\t} else\n\t\t\t\treturn -ENOENT;\n\n                case SIOCDELIPDDPRT:\n                        return ipddp_delete(&rcp);\n\n                default:\n                        return -EINVAL;\n        }\n}",
        "code_after_change": "static int ipddp_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n        struct ipddp_route __user *rt = ifr->ifr_data;\n        struct ipddp_route rcp, rcp2, *rp;\n\n        if(!capable(CAP_NET_ADMIN))\n                return -EPERM;\n\n\tif(copy_from_user(&rcp, rt, sizeof(rcp)))\n\t\treturn -EFAULT;\n\n        switch(cmd)\n        {\n\t\tcase SIOCADDIPDDPRT:\n                        return ipddp_create(&rcp);\n\n                case SIOCFINDIPDDPRT:\n\t\t\tspin_lock_bh(&ipddp_route_lock);\n\t\t\trp = __ipddp_find_route(&rcp);\n\t\t\tif (rp) {\n\t\t\t\tmemset(&rcp2, 0, sizeof(rcp2));\n\t\t\t\trcp2.ip    = rp->ip;\n\t\t\t\trcp2.at    = rp->at;\n\t\t\t\trcp2.flags = rp->flags;\n\t\t\t}\n\t\t\tspin_unlock_bh(&ipddp_route_lock);\n\n\t\t\tif (rp) {\n\t\t\t\tif (copy_to_user(rt, &rcp2,\n\t\t\t\t\t\t sizeof(struct ipddp_route)))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\treturn 0;\n\t\t\t} else\n\t\t\t\treturn -ENOENT;\n\n                case SIOCDELIPDDPRT:\n                        return ipddp_delete(&rcp);\n\n                default:\n                        return -EINVAL;\n        }\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,8 +17,12 @@\n                 case SIOCFINDIPDDPRT:\n \t\t\tspin_lock_bh(&ipddp_route_lock);\n \t\t\trp = __ipddp_find_route(&rcp);\n-\t\t\tif (rp)\n-\t\t\t\tmemcpy(&rcp2, rp, sizeof(rcp2));\n+\t\t\tif (rp) {\n+\t\t\t\tmemset(&rcp2, 0, sizeof(rcp2));\n+\t\t\t\trcp2.ip    = rp->ip;\n+\t\t\t\trcp2.at    = rp->at;\n+\t\t\t\trcp2.flags = rp->flags;\n+\t\t\t}\n \t\t\tspin_unlock_bh(&ipddp_route_lock);\n \n \t\t\tif (rp) {",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (rp) {",
                "\t\t\t\tmemset(&rcp2, 0, sizeof(rcp2));",
                "\t\t\t\trcp2.ip    = rp->ip;",
                "\t\t\t\trcp2.at    = rp->at;",
                "\t\t\t\trcp2.flags = rp->flags;",
                "\t\t\t}"
            ],
            "deleted": [
                "\t\t\tif (rp)",
                "\t\t\t\tmemcpy(&rcp2, rp, sizeof(rcp2));"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.18.11. The ipddp_ioctl function in drivers/net/appletalk/ipddp.c allows local users to obtain sensitive kernel address information by leveraging CAP_NET_ADMIN to read the ipddp_route dev and next fields via an SIOCFINDIPDDPRT ioctl call.",
        "id": 1772
    },
    {
        "cve_id": "CVE-2012-6549",
        "code_before_change": "static int\nisofs_export_encode_fh(struct inode *inode,\n\t\t       __u32 *fh32,\n\t\t       int *max_len,\n\t\t       struct inode *parent)\n{\n\tstruct iso_inode_info * ei = ISOFS_I(inode);\n\tint len = *max_len;\n\tint type = 1;\n\t__u16 *fh16 = (__u16*)fh32;\n\n\t/*\n\t * WARNING: max_len is 5 for NFSv2.  Because of this\n\t * limitation, we use the lower 16 bits of fh32[1] to hold the\n\t * offset of the inode and the upper 16 bits of fh32[1] to\n\t * hold the offset of the parent.\n\t */\n\tif (parent && (len < 5)) {\n\t\t*max_len = 5;\n\t\treturn 255;\n\t} else if (len < 3) {\n\t\t*max_len = 3;\n\t\treturn 255;\n\t}\n\n\tlen = 3;\n\tfh32[0] = ei->i_iget5_block;\n \tfh16[2] = (__u16)ei->i_iget5_offset;  /* fh16 [sic] */\n\tfh32[2] = inode->i_generation;\n\tif (parent) {\n\t\tstruct iso_inode_info *eparent;\n\t\teparent = ISOFS_I(parent);\n\t\tfh32[3] = eparent->i_iget5_block;\n\t\tfh16[3] = (__u16)eparent->i_iget5_offset;  /* fh16 [sic] */\n\t\tfh32[4] = parent->i_generation;\n\t\tlen = 5;\n\t\ttype = 2;\n\t}\n\t*max_len = len;\n\treturn type;\n}",
        "code_after_change": "static int\nisofs_export_encode_fh(struct inode *inode,\n\t\t       __u32 *fh32,\n\t\t       int *max_len,\n\t\t       struct inode *parent)\n{\n\tstruct iso_inode_info * ei = ISOFS_I(inode);\n\tint len = *max_len;\n\tint type = 1;\n\t__u16 *fh16 = (__u16*)fh32;\n\n\t/*\n\t * WARNING: max_len is 5 for NFSv2.  Because of this\n\t * limitation, we use the lower 16 bits of fh32[1] to hold the\n\t * offset of the inode and the upper 16 bits of fh32[1] to\n\t * hold the offset of the parent.\n\t */\n\tif (parent && (len < 5)) {\n\t\t*max_len = 5;\n\t\treturn 255;\n\t} else if (len < 3) {\n\t\t*max_len = 3;\n\t\treturn 255;\n\t}\n\n\tlen = 3;\n\tfh32[0] = ei->i_iget5_block;\n \tfh16[2] = (__u16)ei->i_iget5_offset;  /* fh16 [sic] */\n\tfh16[3] = 0;  /* avoid leaking uninitialized data */\n\tfh32[2] = inode->i_generation;\n\tif (parent) {\n\t\tstruct iso_inode_info *eparent;\n\t\teparent = ISOFS_I(parent);\n\t\tfh32[3] = eparent->i_iget5_block;\n\t\tfh16[3] = (__u16)eparent->i_iget5_offset;  /* fh16 [sic] */\n\t\tfh32[4] = parent->i_generation;\n\t\tlen = 5;\n\t\ttype = 2;\n\t}\n\t*max_len = len;\n\treturn type;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,6 +26,7 @@\n \tlen = 3;\n \tfh32[0] = ei->i_iget5_block;\n  \tfh16[2] = (__u16)ei->i_iget5_offset;  /* fh16 [sic] */\n+\tfh16[3] = 0;  /* avoid leaking uninitialized data */\n \tfh32[2] = inode->i_generation;\n \tif (parent) {\n \t\tstruct iso_inode_info *eparent;",
        "function_modified_lines": {
            "added": [
                "\tfh16[3] = 0;  /* avoid leaking uninitialized data */"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The isofs_export_encode_fh function in fs/isofs/export.c in the Linux kernel before 3.6 does not initialize a certain structure member, which allows local users to obtain sensitive information from kernel heap memory via a crafted application.",
        "id": 133
    },
    {
        "cve_id": "CVE-2018-19854",
        "code_before_change": "static int crypto_report_kpp(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_kpp rkpp;\n\n\tstrlcpy(rkpp.type, \"kpp\", sizeof(rkpp.type));\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_KPP,\n\t\t    sizeof(struct crypto_report_kpp), &rkpp))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "code_after_change": "static int crypto_report_kpp(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_kpp rkpp;\n\n\tstrncpy(rkpp.type, \"kpp\", sizeof(rkpp.type));\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_KPP,\n\t\t    sizeof(struct crypto_report_kpp), &rkpp))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tstruct crypto_report_kpp rkpp;\n \n-\tstrlcpy(rkpp.type, \"kpp\", sizeof(rkpp.type));\n+\tstrncpy(rkpp.type, \"kpp\", sizeof(rkpp.type));\n \n \tif (nla_put(skb, CRYPTOCFGA_REPORT_KPP,\n \t\t    sizeof(struct crypto_report_kpp), &rkpp))",
        "function_modified_lines": {
            "added": [
                "\tstrncpy(rkpp.type, \"kpp\", sizeof(rkpp.type));"
            ],
            "deleted": [
                "\tstrlcpy(rkpp.type, \"kpp\", sizeof(rkpp.type));"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.19.3. crypto_report_one() and related functions in crypto/crypto_user.c (the crypto user configuration API) do not fully initialize structures that are copied to userspace, potentially leaking sensitive memory to user programs. NOTE: this is a CVE-2013-2547 regression but with easier exploitability because the attacker does not need a capability (however, the system must have the CONFIG_CRYPTO_USER kconfig option).",
        "id": 1746
    },
    {
        "cve_id": "CVE-2012-6548",
        "code_before_change": "static int udf_encode_fh(struct inode *inode, __u32 *fh, int *lenp,\n\t\t\t struct inode *parent)\n{\n\tint len = *lenp;\n\tstruct kernel_lb_addr location = UDF_I(inode)->i_location;\n\tstruct fid *fid = (struct fid *)fh;\n\tint type = FILEID_UDF_WITHOUT_PARENT;\n\n\tif (parent && (len < 5)) {\n\t\t*lenp = 5;\n\t\treturn 255;\n\t} else if (len < 3) {\n\t\t*lenp = 3;\n\t\treturn 255;\n\t}\n\n\t*lenp = 3;\n\tfid->udf.block = location.logicalBlockNum;\n\tfid->udf.partref = location.partitionReferenceNum;\n\tfid->udf.generation = inode->i_generation;\n\n\tif (parent) {\n\t\tlocation = UDF_I(parent)->i_location;\n\t\tfid->udf.parent_block = location.logicalBlockNum;\n\t\tfid->udf.parent_partref = location.partitionReferenceNum;\n\t\tfid->udf.parent_generation = inode->i_generation;\n\t\t*lenp = 5;\n\t\ttype = FILEID_UDF_WITH_PARENT;\n\t}\n\n\treturn type;\n}",
        "code_after_change": "static int udf_encode_fh(struct inode *inode, __u32 *fh, int *lenp,\n\t\t\t struct inode *parent)\n{\n\tint len = *lenp;\n\tstruct kernel_lb_addr location = UDF_I(inode)->i_location;\n\tstruct fid *fid = (struct fid *)fh;\n\tint type = FILEID_UDF_WITHOUT_PARENT;\n\n\tif (parent && (len < 5)) {\n\t\t*lenp = 5;\n\t\treturn 255;\n\t} else if (len < 3) {\n\t\t*lenp = 3;\n\t\treturn 255;\n\t}\n\n\t*lenp = 3;\n\tfid->udf.block = location.logicalBlockNum;\n\tfid->udf.partref = location.partitionReferenceNum;\n\tfid->udf.parent_partref = 0;\n\tfid->udf.generation = inode->i_generation;\n\n\tif (parent) {\n\t\tlocation = UDF_I(parent)->i_location;\n\t\tfid->udf.parent_block = location.logicalBlockNum;\n\t\tfid->udf.parent_partref = location.partitionReferenceNum;\n\t\tfid->udf.parent_generation = inode->i_generation;\n\t\t*lenp = 5;\n\t\ttype = FILEID_UDF_WITH_PARENT;\n\t}\n\n\treturn type;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,7 @@\n \t*lenp = 3;\n \tfid->udf.block = location.logicalBlockNum;\n \tfid->udf.partref = location.partitionReferenceNum;\n+\tfid->udf.parent_partref = 0;\n \tfid->udf.generation = inode->i_generation;\n \n \tif (parent) {",
        "function_modified_lines": {
            "added": [
                "\tfid->udf.parent_partref = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The udf_encode_fh function in fs/udf/namei.c in the Linux kernel before 3.6 does not initialize a certain structure member, which allows local users to obtain sensitive information from kernel heap memory via a crafted application.",
        "id": 132
    },
    {
        "cve_id": "CVE-2018-19854",
        "code_before_change": "static int crypto_report_akcipher(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_akcipher rakcipher;\n\n\tstrlcpy(rakcipher.type, \"akcipher\", sizeof(rakcipher.type));\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_AKCIPHER,\n\t\t    sizeof(struct crypto_report_akcipher), &rakcipher))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "code_after_change": "static int crypto_report_akcipher(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_akcipher rakcipher;\n\n\tstrncpy(rakcipher.type, \"akcipher\", sizeof(rakcipher.type));\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_AKCIPHER,\n\t\t    sizeof(struct crypto_report_akcipher), &rakcipher))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,7 @@\n {\n \tstruct crypto_report_akcipher rakcipher;\n \n-\tstrlcpy(rakcipher.type, \"akcipher\", sizeof(rakcipher.type));\n+\tstrncpy(rakcipher.type, \"akcipher\", sizeof(rakcipher.type));\n \n \tif (nla_put(skb, CRYPTOCFGA_REPORT_AKCIPHER,\n \t\t    sizeof(struct crypto_report_akcipher), &rakcipher))",
        "function_modified_lines": {
            "added": [
                "\tstrncpy(rakcipher.type, \"akcipher\", sizeof(rakcipher.type));"
            ],
            "deleted": [
                "\tstrlcpy(rakcipher.type, \"akcipher\", sizeof(rakcipher.type));"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the Linux kernel before 4.19.3. crypto_report_one() and related functions in crypto/crypto_user.c (the crypto user configuration API) do not fully initialize structures that are copied to userspace, potentially leaking sensitive memory to user programs. NOTE: this is a CVE-2013-2547 regression but with easier exploitability because the attacker does not need a capability (however, the system must have the CONFIG_CRYPTO_USER kconfig option).",
        "id": 1750
    },
    {
        "cve_id": "CVE-2012-6542",
        "code_before_change": "static int llc_ui_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t  int *uaddrlen, int peer)\n{\n\tstruct sockaddr_llc sllc;\n\tstruct sock *sk = sock->sk;\n\tstruct llc_sock *llc = llc_sk(sk);\n\tint rc = 0;\n\n\tmemset(&sllc, 0, sizeof(sllc));\n\tlock_sock(sk);\n\tif (sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out;\n\t*uaddrlen = sizeof(sllc);\n\tmemset(uaddr, 0, *uaddrlen);\n\tif (peer) {\n\t\trc = -ENOTCONN;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tif(llc->dev)\n\t\t\tsllc.sllc_arphrd = llc->dev->type;\n\t\tsllc.sllc_sap = llc->daddr.lsap;\n\t\tmemcpy(&sllc.sllc_mac, &llc->daddr.mac, IFHWADDRLEN);\n\t} else {\n\t\trc = -EINVAL;\n\t\tif (!llc->sap)\n\t\t\tgoto out;\n\t\tsllc.sllc_sap = llc->sap->laddr.lsap;\n\n\t\tif (llc->dev) {\n\t\t\tsllc.sllc_arphrd = llc->dev->type;\n\t\t\tmemcpy(&sllc.sllc_mac, llc->dev->dev_addr,\n\t\t\t       IFHWADDRLEN);\n\t\t}\n\t}\n\trc = 0;\n\tsllc.sllc_family = AF_LLC;\n\tmemcpy(uaddr, &sllc, sizeof(sllc));\nout:\n\trelease_sock(sk);\n\treturn rc;\n}",
        "code_after_change": "static int llc_ui_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t  int *uaddrlen, int peer)\n{\n\tstruct sockaddr_llc sllc;\n\tstruct sock *sk = sock->sk;\n\tstruct llc_sock *llc = llc_sk(sk);\n\tint rc = -EBADF;\n\n\tmemset(&sllc, 0, sizeof(sllc));\n\tlock_sock(sk);\n\tif (sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out;\n\t*uaddrlen = sizeof(sllc);\n\tif (peer) {\n\t\trc = -ENOTCONN;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tif(llc->dev)\n\t\t\tsllc.sllc_arphrd = llc->dev->type;\n\t\tsllc.sllc_sap = llc->daddr.lsap;\n\t\tmemcpy(&sllc.sllc_mac, &llc->daddr.mac, IFHWADDRLEN);\n\t} else {\n\t\trc = -EINVAL;\n\t\tif (!llc->sap)\n\t\t\tgoto out;\n\t\tsllc.sllc_sap = llc->sap->laddr.lsap;\n\n\t\tif (llc->dev) {\n\t\t\tsllc.sllc_arphrd = llc->dev->type;\n\t\t\tmemcpy(&sllc.sllc_mac, llc->dev->dev_addr,\n\t\t\t       IFHWADDRLEN);\n\t\t}\n\t}\n\trc = 0;\n\tsllc.sllc_family = AF_LLC;\n\tmemcpy(uaddr, &sllc, sizeof(sllc));\nout:\n\trelease_sock(sk);\n\treturn rc;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,14 +4,13 @@\n \tstruct sockaddr_llc sllc;\n \tstruct sock *sk = sock->sk;\n \tstruct llc_sock *llc = llc_sk(sk);\n-\tint rc = 0;\n+\tint rc = -EBADF;\n \n \tmemset(&sllc, 0, sizeof(sllc));\n \tlock_sock(sk);\n \tif (sock_flag(sk, SOCK_ZAPPED))\n \t\tgoto out;\n \t*uaddrlen = sizeof(sllc);\n-\tmemset(uaddr, 0, *uaddrlen);\n \tif (peer) {\n \t\trc = -ENOTCONN;\n \t\tif (sk->sk_state != TCP_ESTABLISHED)",
        "function_modified_lines": {
            "added": [
                "\tint rc = -EBADF;"
            ],
            "deleted": [
                "\tint rc = 0;",
                "\tmemset(uaddr, 0, *uaddrlen);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The llc_ui_getname function in net/llc/af_llc.c in the Linux kernel before 3.6 has an incorrect return value in certain circumstances, which allows local users to obtain sensitive information from kernel stack memory via a crafted application that leverages an uninitialized pointer argument.",
        "id": 126
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "static inline void __thread_fpu_begin(struct task_struct *tsk)\n{\n\tif (!use_xsave())\n\t\tclts();\n\t__thread_set_has_fpu(tsk);\n}",
        "code_after_change": "static inline void __thread_fpu_begin(struct task_struct *tsk)\n{\n\tif (!use_eager_fpu())\n\t\tclts();\n\t__thread_set_has_fpu(tsk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static inline void __thread_fpu_begin(struct task_struct *tsk)\n {\n-\tif (!use_xsave())\n+\tif (!use_eager_fpu())\n \t\tclts();\n \t__thread_set_has_fpu(tsk);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!use_eager_fpu())"
            ],
            "deleted": [
                "\tif (!use_xsave())"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1804
    },
    {
        "cve_id": "CVE-2018-20509",
        "code_before_change": "static void binder_deferred_release(struct binder_proc *proc)\n{\n\tstruct binder_context *context = proc->context;\n\tstruct rb_node *n;\n\tint threads, nodes, incoming_refs, outgoing_refs, active_transactions;\n\n\tBUG_ON(proc->files);\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_del(&proc->proc_node);\n\tmutex_unlock(&binder_procs_lock);\n\n\tmutex_lock(&context->context_mgr_node_lock);\n\tif (context->binder_context_mgr_node &&\n\t    context->binder_context_mgr_node->proc == proc) {\n\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t     \"%s: %d context_mgr_node gone\\n\",\n\t\t\t     __func__, proc->pid);\n\t\tcontext->binder_context_mgr_node = NULL;\n\t}\n\tmutex_unlock(&context->context_mgr_node_lock);\n\t/*\n\t * Make sure proc stays alive after we\n\t * remove all the threads\n\t */\n\tproc->tmp_ref++;\n\n\tproc->is_dead = true;\n\tthreads = 0;\n\tactive_transactions = 0;\n\twhile ((n = rb_first(&proc->threads))) {\n\t\tstruct binder_thread *thread;\n\n\t\tthread = rb_entry(n, struct binder_thread, rb_node);\n\t\tthreads++;\n\t\tactive_transactions += binder_thread_release(proc, thread);\n\t}\n\n\tnodes = 0;\n\tincoming_refs = 0;\n\twhile ((n = rb_first(&proc->nodes))) {\n\t\tstruct binder_node *node;\n\n\t\tnode = rb_entry(n, struct binder_node, rb_node);\n\t\tnodes++;\n\t\trb_erase(&node->rb_node, &proc->nodes);\n\t\tincoming_refs = binder_node_release(node, incoming_refs);\n\t}\n\n\toutgoing_refs = 0;\n\twhile ((n = rb_first(&proc->refs_by_desc))) {\n\t\tstruct binder_ref *ref;\n\n\t\tref = rb_entry(n, struct binder_ref, rb_node_desc);\n\t\toutgoing_refs++;\n\t\tbinder_delete_ref(ref);\n\t}\n\n\tbinder_release_work(&proc->todo);\n\tbinder_release_work(&proc->delivered_death);\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d threads %d, nodes %d (ref %d), refs %d, active transactions %d\\n\",\n\t\t     __func__, proc->pid, threads, nodes, incoming_refs,\n\t\t     outgoing_refs, active_transactions);\n\n\tbinder_proc_dec_tmpref(proc);\n}",
        "code_after_change": "static void binder_deferred_release(struct binder_proc *proc)\n{\n\tstruct binder_context *context = proc->context;\n\tstruct rb_node *n;\n\tint threads, nodes, incoming_refs, outgoing_refs, active_transactions;\n\n\tBUG_ON(proc->files);\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_del(&proc->proc_node);\n\tmutex_unlock(&binder_procs_lock);\n\n\tmutex_lock(&context->context_mgr_node_lock);\n\tif (context->binder_context_mgr_node &&\n\t    context->binder_context_mgr_node->proc == proc) {\n\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t     \"%s: %d context_mgr_node gone\\n\",\n\t\t\t     __func__, proc->pid);\n\t\tcontext->binder_context_mgr_node = NULL;\n\t}\n\tmutex_unlock(&context->context_mgr_node_lock);\n\t/*\n\t * Make sure proc stays alive after we\n\t * remove all the threads\n\t */\n\tproc->tmp_ref++;\n\n\tproc->is_dead = true;\n\tthreads = 0;\n\tactive_transactions = 0;\n\twhile ((n = rb_first(&proc->threads))) {\n\t\tstruct binder_thread *thread;\n\n\t\tthread = rb_entry(n, struct binder_thread, rb_node);\n\t\tthreads++;\n\t\tactive_transactions += binder_thread_release(proc, thread);\n\t}\n\n\tnodes = 0;\n\tincoming_refs = 0;\n\twhile ((n = rb_first(&proc->nodes))) {\n\t\tstruct binder_node *node;\n\n\t\tnode = rb_entry(n, struct binder_node, rb_node);\n\t\tnodes++;\n\t\trb_erase(&node->rb_node, &proc->nodes);\n\t\tincoming_refs = binder_node_release(node, incoming_refs);\n\t}\n\n\toutgoing_refs = 0;\n\twhile ((n = rb_first(&proc->refs_by_desc))) {\n\t\tstruct binder_ref *ref;\n\n\t\tref = rb_entry(n, struct binder_ref, rb_node_desc);\n\t\toutgoing_refs++;\n\t\tbinder_cleanup_ref(ref);\n\t\tbinder_free_ref(ref);\n\t}\n\n\tbinder_release_work(&proc->todo);\n\tbinder_release_work(&proc->delivered_death);\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d threads %d, nodes %d (ref %d), refs %d, active transactions %d\\n\",\n\t\t     __func__, proc->pid, threads, nodes, incoming_refs,\n\t\t     outgoing_refs, active_transactions);\n\n\tbinder_proc_dec_tmpref(proc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -53,7 +53,8 @@\n \n \t\tref = rb_entry(n, struct binder_ref, rb_node_desc);\n \t\toutgoing_refs++;\n-\t\tbinder_delete_ref(ref);\n+\t\tbinder_cleanup_ref(ref);\n+\t\tbinder_free_ref(ref);\n \t}\n \n \tbinder_release_work(&proc->todo);",
        "function_modified_lines": {
            "added": [
                "\t\tbinder_cleanup_ref(ref);",
                "\t\tbinder_free_ref(ref);"
            ],
            "deleted": [
                "\t\tbinder_delete_ref(ref);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_ref_olocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \" ref *desc *node\" lines in a debugfs file.",
        "id": 1760
    },
    {
        "cve_id": "CVE-2014-3917",
        "code_before_change": "static int audit_filter_inode_name(struct task_struct *tsk,\n\t\t\t\t   struct audit_names *n,\n\t\t\t\t   struct audit_context *ctx) {\n\tint word, bit;\n\tint h = audit_hash_ino((u32)n->ino);\n\tstruct list_head *list = &audit_inode_hash[h];\n\tstruct audit_entry *e;\n\tenum audit_state state;\n\n\tword = AUDIT_WORD(ctx->major);\n\tbit  = AUDIT_BIT(ctx->major);\n\n\tif (list_empty(list))\n\t\treturn 0;\n\n\tlist_for_each_entry_rcu(e, list, list) {\n\t\tif ((e->rule.mask[word] & bit) == bit &&\n\t\t    audit_filter_rules(tsk, &e->rule, ctx, n, &state, false)) {\n\t\t\tctx->current_state = state;\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int audit_filter_inode_name(struct task_struct *tsk,\n\t\t\t\t   struct audit_names *n,\n\t\t\t\t   struct audit_context *ctx) {\n\tint h = audit_hash_ino((u32)n->ino);\n\tstruct list_head *list = &audit_inode_hash[h];\n\tstruct audit_entry *e;\n\tenum audit_state state;\n\n\tif (list_empty(list))\n\t\treturn 0;\n\n\tlist_for_each_entry_rcu(e, list, list) {\n\t\tif (audit_in_mask(&e->rule, ctx->major) &&\n\t\t    audit_filter_rules(tsk, &e->rule, ctx, n, &state, false)) {\n\t\t\tctx->current_state = state;\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,20 +1,16 @@\n static int audit_filter_inode_name(struct task_struct *tsk,\n \t\t\t\t   struct audit_names *n,\n \t\t\t\t   struct audit_context *ctx) {\n-\tint word, bit;\n \tint h = audit_hash_ino((u32)n->ino);\n \tstruct list_head *list = &audit_inode_hash[h];\n \tstruct audit_entry *e;\n \tenum audit_state state;\n \n-\tword = AUDIT_WORD(ctx->major);\n-\tbit  = AUDIT_BIT(ctx->major);\n-\n \tif (list_empty(list))\n \t\treturn 0;\n \n \tlist_for_each_entry_rcu(e, list, list) {\n-\t\tif ((e->rule.mask[word] & bit) == bit &&\n+\t\tif (audit_in_mask(&e->rule, ctx->major) &&\n \t\t    audit_filter_rules(tsk, &e->rule, ctx, n, &state, false)) {\n \t\t\tctx->current_state = state;\n \t\t\treturn 1;",
        "function_modified_lines": {
            "added": [
                "\t\tif (audit_in_mask(&e->rule, ctx->major) &&"
            ],
            "deleted": [
                "\tint word, bit;",
                "\tword = AUDIT_WORD(ctx->major);",
                "\tbit  = AUDIT_BIT(ctx->major);",
                "",
                "\t\tif ((e->rule.mask[word] & bit) == bit &&"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "kernel/auditsc.c in the Linux kernel through 3.14.5, when CONFIG_AUDITSYSCALL is enabled with certain syscall rules, allows local users to obtain potentially sensitive single-bit values from kernel memory or cause a denial of service (OOPS) via a large value of a syscall number.",
        "id": 547
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "void flush_thread(void)\n{\n\tstruct task_struct *tsk = current;\n\n\tflush_ptrace_hw_breakpoint(tsk);\n\tmemset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));\n\tdrop_init_fpu(tsk);\n\t/*\n\t * Free the FPU state for non xsave platforms. They get reallocated\n\t * lazily at the first use.\n\t */\n\tif (!use_xsave())\n\t\tfree_thread_xstate(tsk);\n}",
        "code_after_change": "void flush_thread(void)\n{\n\tstruct task_struct *tsk = current;\n\n\tflush_ptrace_hw_breakpoint(tsk);\n\tmemset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));\n\tdrop_init_fpu(tsk);\n\t/*\n\t * Free the FPU state for non xsave platforms. They get reallocated\n\t * lazily at the first use.\n\t */\n\tif (!use_eager_fpu())\n\t\tfree_thread_xstate(tsk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,6 @@\n \t * Free the FPU state for non xsave platforms. They get reallocated\n \t * lazily at the first use.\n \t */\n-\tif (!use_xsave())\n+\tif (!use_eager_fpu())\n \t\tfree_thread_xstate(tsk);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!use_eager_fpu())"
            ],
            "deleted": [
                "\tif (!use_xsave())"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1812
    },
    {
        "cve_id": "CVE-2016-2117",
        "code_before_change": "static int atl2_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tstruct net_device *netdev;\n\tstruct atl2_adapter *adapter;\n\tstatic int cards_found;\n\tunsigned long mmio_start;\n\tint mmio_len;\n\tint err;\n\n\tcards_found = 0;\n\n\terr = pci_enable_device(pdev);\n\tif (err)\n\t\treturn err;\n\n\t/*\n\t * atl2 is a shared-high-32-bit device, so we're stuck with 32-bit DMA\n\t * until the kernel has the proper infrastructure to support 64-bit DMA\n\t * on these devices.\n\t */\n\tif (pci_set_dma_mask(pdev, DMA_BIT_MASK(32)) &&\n\t\tpci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32))) {\n\t\tprintk(KERN_ERR \"atl2: No usable DMA configuration, aborting\\n\");\n\t\tgoto err_dma;\n\t}\n\n\t/* Mark all PCI regions associated with PCI device\n\t * pdev as being reserved by owner atl2_driver_name */\n\terr = pci_request_regions(pdev, atl2_driver_name);\n\tif (err)\n\t\tgoto err_pci_reg;\n\n\t/* Enables bus-mastering on the device and calls\n\t * pcibios_set_master to do the needed arch specific settings */\n\tpci_set_master(pdev);\n\n\terr = -ENOMEM;\n\tnetdev = alloc_etherdev(sizeof(struct atl2_adapter));\n\tif (!netdev)\n\t\tgoto err_alloc_etherdev;\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\n\tpci_set_drvdata(pdev, netdev);\n\tadapter = netdev_priv(netdev);\n\tadapter->netdev = netdev;\n\tadapter->pdev = pdev;\n\tadapter->hw.back = adapter;\n\n\tmmio_start = pci_resource_start(pdev, 0x0);\n\tmmio_len = pci_resource_len(pdev, 0x0);\n\n\tadapter->hw.mem_rang = (u32)mmio_len;\n\tadapter->hw.hw_addr = ioremap(mmio_start, mmio_len);\n\tif (!adapter->hw.hw_addr) {\n\t\terr = -EIO;\n\t\tgoto err_ioremap;\n\t}\n\n\tatl2_setup_pcicmd(pdev);\n\n\tnetdev->netdev_ops = &atl2_netdev_ops;\n\tnetdev->ethtool_ops = &atl2_ethtool_ops;\n\tnetdev->watchdog_timeo = 5 * HZ;\n\tstrncpy(netdev->name, pci_name(pdev), sizeof(netdev->name) - 1);\n\n\tnetdev->mem_start = mmio_start;\n\tnetdev->mem_end = mmio_start + mmio_len;\n\tadapter->bd_number = cards_found;\n\tadapter->pci_using_64 = false;\n\n\t/* setup the private structure */\n\terr = atl2_sw_init(adapter);\n\tif (err)\n\t\tgoto err_sw_init;\n\n\terr = -EIO;\n\n\tnetdev->hw_features = NETIF_F_SG | NETIF_F_HW_VLAN_CTAG_RX;\n\tnetdev->features |= (NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX);\n\n\t/* Init PHY as early as possible due to power saving issue  */\n\tatl2_phy_init(&adapter->hw);\n\n\t/* reset the controller to\n\t * put the device in a known good starting state */\n\n\tif (atl2_reset_hw(&adapter->hw)) {\n\t\terr = -EIO;\n\t\tgoto err_reset;\n\t}\n\n\t/* copy the MAC address out of the EEPROM */\n\tatl2_read_mac_addr(&adapter->hw);\n\tmemcpy(netdev->dev_addr, adapter->hw.mac_addr, netdev->addr_len);\n\tif (!is_valid_ether_addr(netdev->dev_addr)) {\n\t\terr = -EIO;\n\t\tgoto err_eeprom;\n\t}\n\n\tatl2_check_options(adapter);\n\n\tsetup_timer(&adapter->watchdog_timer, atl2_watchdog,\n\t\t    (unsigned long)adapter);\n\n\tsetup_timer(&adapter->phy_config_timer, atl2_phy_config,\n\t\t    (unsigned long)adapter);\n\n\tINIT_WORK(&adapter->reset_task, atl2_reset_task);\n\tINIT_WORK(&adapter->link_chg_task, atl2_link_chg_task);\n\n\tstrcpy(netdev->name, \"eth%d\"); /* ?? */\n\terr = register_netdev(netdev);\n\tif (err)\n\t\tgoto err_register;\n\n\t/* assume we have no link for now */\n\tnetif_carrier_off(netdev);\n\tnetif_stop_queue(netdev);\n\n\tcards_found++;\n\n\treturn 0;\n\nerr_reset:\nerr_register:\nerr_sw_init:\nerr_eeprom:\n\tiounmap(adapter->hw.hw_addr);\nerr_ioremap:\n\tfree_netdev(netdev);\nerr_alloc_etherdev:\n\tpci_release_regions(pdev);\nerr_pci_reg:\nerr_dma:\n\tpci_disable_device(pdev);\n\treturn err;\n}",
        "code_after_change": "static int atl2_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tstruct net_device *netdev;\n\tstruct atl2_adapter *adapter;\n\tstatic int cards_found;\n\tunsigned long mmio_start;\n\tint mmio_len;\n\tint err;\n\n\tcards_found = 0;\n\n\terr = pci_enable_device(pdev);\n\tif (err)\n\t\treturn err;\n\n\t/*\n\t * atl2 is a shared-high-32-bit device, so we're stuck with 32-bit DMA\n\t * until the kernel has the proper infrastructure to support 64-bit DMA\n\t * on these devices.\n\t */\n\tif (pci_set_dma_mask(pdev, DMA_BIT_MASK(32)) &&\n\t\tpci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32))) {\n\t\tprintk(KERN_ERR \"atl2: No usable DMA configuration, aborting\\n\");\n\t\tgoto err_dma;\n\t}\n\n\t/* Mark all PCI regions associated with PCI device\n\t * pdev as being reserved by owner atl2_driver_name */\n\terr = pci_request_regions(pdev, atl2_driver_name);\n\tif (err)\n\t\tgoto err_pci_reg;\n\n\t/* Enables bus-mastering on the device and calls\n\t * pcibios_set_master to do the needed arch specific settings */\n\tpci_set_master(pdev);\n\n\terr = -ENOMEM;\n\tnetdev = alloc_etherdev(sizeof(struct atl2_adapter));\n\tif (!netdev)\n\t\tgoto err_alloc_etherdev;\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\n\tpci_set_drvdata(pdev, netdev);\n\tadapter = netdev_priv(netdev);\n\tadapter->netdev = netdev;\n\tadapter->pdev = pdev;\n\tadapter->hw.back = adapter;\n\n\tmmio_start = pci_resource_start(pdev, 0x0);\n\tmmio_len = pci_resource_len(pdev, 0x0);\n\n\tadapter->hw.mem_rang = (u32)mmio_len;\n\tadapter->hw.hw_addr = ioremap(mmio_start, mmio_len);\n\tif (!adapter->hw.hw_addr) {\n\t\terr = -EIO;\n\t\tgoto err_ioremap;\n\t}\n\n\tatl2_setup_pcicmd(pdev);\n\n\tnetdev->netdev_ops = &atl2_netdev_ops;\n\tnetdev->ethtool_ops = &atl2_ethtool_ops;\n\tnetdev->watchdog_timeo = 5 * HZ;\n\tstrncpy(netdev->name, pci_name(pdev), sizeof(netdev->name) - 1);\n\n\tnetdev->mem_start = mmio_start;\n\tnetdev->mem_end = mmio_start + mmio_len;\n\tadapter->bd_number = cards_found;\n\tadapter->pci_using_64 = false;\n\n\t/* setup the private structure */\n\terr = atl2_sw_init(adapter);\n\tif (err)\n\t\tgoto err_sw_init;\n\n\terr = -EIO;\n\n\tnetdev->hw_features = NETIF_F_HW_VLAN_CTAG_RX;\n\tnetdev->features |= (NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX);\n\n\t/* Init PHY as early as possible due to power saving issue  */\n\tatl2_phy_init(&adapter->hw);\n\n\t/* reset the controller to\n\t * put the device in a known good starting state */\n\n\tif (atl2_reset_hw(&adapter->hw)) {\n\t\terr = -EIO;\n\t\tgoto err_reset;\n\t}\n\n\t/* copy the MAC address out of the EEPROM */\n\tatl2_read_mac_addr(&adapter->hw);\n\tmemcpy(netdev->dev_addr, adapter->hw.mac_addr, netdev->addr_len);\n\tif (!is_valid_ether_addr(netdev->dev_addr)) {\n\t\terr = -EIO;\n\t\tgoto err_eeprom;\n\t}\n\n\tatl2_check_options(adapter);\n\n\tsetup_timer(&adapter->watchdog_timer, atl2_watchdog,\n\t\t    (unsigned long)adapter);\n\n\tsetup_timer(&adapter->phy_config_timer, atl2_phy_config,\n\t\t    (unsigned long)adapter);\n\n\tINIT_WORK(&adapter->reset_task, atl2_reset_task);\n\tINIT_WORK(&adapter->link_chg_task, atl2_link_chg_task);\n\n\tstrcpy(netdev->name, \"eth%d\"); /* ?? */\n\terr = register_netdev(netdev);\n\tif (err)\n\t\tgoto err_register;\n\n\t/* assume we have no link for now */\n\tnetif_carrier_off(netdev);\n\tnetif_stop_queue(netdev);\n\n\tcards_found++;\n\n\treturn 0;\n\nerr_reset:\nerr_register:\nerr_sw_init:\nerr_eeprom:\n\tiounmap(adapter->hw.hw_addr);\nerr_ioremap:\n\tfree_netdev(netdev);\nerr_alloc_etherdev:\n\tpci_release_regions(pdev);\nerr_pci_reg:\nerr_dma:\n\tpci_disable_device(pdev);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -76,7 +76,7 @@\n \n \terr = -EIO;\n \n-\tnetdev->hw_features = NETIF_F_SG | NETIF_F_HW_VLAN_CTAG_RX;\n+\tnetdev->hw_features = NETIF_F_HW_VLAN_CTAG_RX;\n \tnetdev->features |= (NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX);\n \n \t/* Init PHY as early as possible due to power saving issue  */",
        "function_modified_lines": {
            "added": [
                "\tnetdev->hw_features = NETIF_F_HW_VLAN_CTAG_RX;"
            ],
            "deleted": [
                "\tnetdev->hw_features = NETIF_F_SG | NETIF_F_HW_VLAN_CTAG_RX;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The atl2_probe function in drivers/net/ethernet/atheros/atlx/atl2.c in the Linux kernel through 4.5.2 incorrectly enables scatter/gather I/O, which allows remote attackers to obtain sensitive information from kernel memory by reading packet data.",
        "id": 922
    },
    {
        "cve_id": "CVE-2017-15537",
        "code_before_change": "int xstateregs_set(struct task_struct *target, const struct user_regset *regset,\n\t\t  unsigned int pos, unsigned int count,\n\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct xregs_state *xsave;\n\tint ret;\n\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE))\n\t\treturn -ENODEV;\n\n\t/*\n\t * A whole standard-format XSAVE buffer is needed:\n\t */\n\tif ((pos != 0) || (count < fpu_user_xstate_size))\n\t\treturn -EFAULT;\n\n\txsave = &fpu->state.xsave;\n\n\tfpu__activate_fpstate_write(fpu);\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVES)) {\n\t\tif (kbuf)\n\t\t\tret = copy_kernel_to_xstate(xsave, kbuf);\n\t\telse\n\t\t\tret = copy_user_to_xstate(xsave, ubuf);\n\t} else {\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);\n\t}\n\n\t/*\n\t * In case of failure, mark all states as init:\n\t */\n\tif (ret)\n\t\tfpstate_init(&fpu->state);\n\n\t/*\n\t * mxcsr reserved bits must be masked to zero for security reasons.\n\t */\n\txsave->i387.mxcsr &= mxcsr_feature_mask;\n\txsave->header.xfeatures &= xfeatures_mask;\n\t/*\n\t * These bits must be zero.\n\t */\n\tmemset(&xsave->header.reserved, 0, 48);\n\n\treturn ret;\n}",
        "code_after_change": "int xstateregs_set(struct task_struct *target, const struct user_regset *regset,\n\t\t  unsigned int pos, unsigned int count,\n\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct xregs_state *xsave;\n\tint ret;\n\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE))\n\t\treturn -ENODEV;\n\n\t/*\n\t * A whole standard-format XSAVE buffer is needed:\n\t */\n\tif ((pos != 0) || (count < fpu_user_xstate_size))\n\t\treturn -EFAULT;\n\n\txsave = &fpu->state.xsave;\n\n\tfpu__activate_fpstate_write(fpu);\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVES)) {\n\t\tif (kbuf)\n\t\t\tret = copy_kernel_to_xstate(xsave, kbuf);\n\t\telse\n\t\t\tret = copy_user_to_xstate(xsave, ubuf);\n\t} else {\n\t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);\n\n\t\t/* xcomp_bv must be 0 when using uncompacted format */\n\t\tif (!ret && xsave->header.xcomp_bv)\n\t\t\tret = -EINVAL;\n\t}\n\n\t/*\n\t * In case of failure, mark all states as init:\n\t */\n\tif (ret)\n\t\tfpstate_init(&fpu->state);\n\n\t/*\n\t * mxcsr reserved bits must be masked to zero for security reasons.\n\t */\n\txsave->i387.mxcsr &= mxcsr_feature_mask;\n\txsave->header.xfeatures &= xfeatures_mask;\n\t/*\n\t * These bits must be zero.\n\t */\n\tmemset(&xsave->header.reserved, 0, 48);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,6 +26,10 @@\n \t\t\tret = copy_user_to_xstate(xsave, ubuf);\n \t} else {\n \t\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);\n+\n+\t\t/* xcomp_bv must be 0 when using uncompacted format */\n+\t\tif (!ret && xsave->header.xcomp_bv)\n+\t\t\tret = -EINVAL;\n \t}\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "",
                "\t\t/* xcomp_bv must be 0 when using uncompacted format */",
                "\t\tif (!ret && xsave->header.xcomp_bv)",
                "\t\t\tret = -EINVAL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The x86/fpu (Floating Point Unit) subsystem in the Linux kernel before 4.13.5, when a processor supports the xsave feature but not the xsaves feature, does not correctly handle attempts to set reserved bits in the xstate header via the ptrace() or rt_sigreturn() system call, allowing local users to read the FPU registers of other processes on the system, related to arch/x86/kernel/fpu/regset.c and arch/x86/kernel/fpu/signal.c.",
        "id": 1306
    },
    {
        "cve_id": "CVE-2017-9605",
        "code_before_change": "int vmw_gb_surface_define_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *file_priv)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_user_surface *user_srf;\n\tstruct vmw_surface *srf;\n\tstruct vmw_resource *res;\n\tstruct vmw_resource *tmp;\n\tunion drm_vmw_gb_surface_create_arg *arg =\n\t    (union drm_vmw_gb_surface_create_arg *)data;\n\tstruct drm_vmw_gb_surface_create_req *req = &arg->req;\n\tstruct drm_vmw_gb_surface_create_rep *rep = &arg->rep;\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tint ret;\n\tuint32_t size;\n\tuint32_t backup_handle;\n\n\tif (req->multisample_count != 0)\n\t\treturn -EINVAL;\n\n\tif (req->mip_levels > DRM_VMW_MAX_MIP_LEVELS)\n\t\treturn -EINVAL;\n\n\tif (unlikely(vmw_user_surface_size == 0))\n\t\tvmw_user_surface_size = ttm_round_pot(sizeof(*user_srf)) +\n\t\t\t128;\n\n\tsize = vmw_user_surface_size + 128;\n\n\t/* Define a surface based on the parameters. */\n\tret = vmw_surface_gb_priv_define(dev,\n\t\t\tsize,\n\t\t\treq->svga3d_flags,\n\t\t\treq->format,\n\t\t\treq->drm_surface_flags & drm_vmw_surface_flag_scanout,\n\t\t\treq->mip_levels,\n\t\t\treq->multisample_count,\n\t\t\treq->array_size,\n\t\t\treq->base_size,\n\t\t\t&srf);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tuser_srf = container_of(srf, struct vmw_user_surface, srf);\n\tif (drm_is_primary_client(file_priv))\n\t\tuser_srf->master = drm_master_get(file_priv->master);\n\n\tret = ttm_read_lock(&dev_priv->reservation_sem, true);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tres = &user_srf->srf.res;\n\n\n\tif (req->buffer_handle != SVGA3D_INVALID_ID) {\n\t\tret = vmw_user_dmabuf_lookup(tfile, req->buffer_handle,\n\t\t\t\t\t     &res->backup,\n\t\t\t\t\t     &user_srf->backup_base);\n\t\tif (ret == 0 && res->backup->base.num_pages * PAGE_SIZE <\n\t\t    res->backup_size) {\n\t\t\tDRM_ERROR(\"Surface backup buffer is too small.\\n\");\n\t\t\tvmw_dmabuf_unreference(&res->backup);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (req->drm_surface_flags & drm_vmw_surface_flag_create_buffer)\n\t\tret = vmw_user_dmabuf_alloc(dev_priv, tfile,\n\t\t\t\t\t    res->backup_size,\n\t\t\t\t\t    req->drm_surface_flags &\n\t\t\t\t\t    drm_vmw_surface_flag_shareable,\n\t\t\t\t\t    &backup_handle,\n\t\t\t\t\t    &res->backup,\n\t\t\t\t\t    &user_srf->backup_base);\n\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\n\ttmp = vmw_resource_reference(res);\n\tret = ttm_prime_object_init(tfile, res->backup_size, &user_srf->prime,\n\t\t\t\t    req->drm_surface_flags &\n\t\t\t\t    drm_vmw_surface_flag_shareable,\n\t\t\t\t    VMW_RES_SURFACE,\n\t\t\t\t    &vmw_user_surface_base_release, NULL);\n\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&tmp);\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\n\trep->handle      = user_srf->prime.base.hash.key;\n\trep->backup_size = res->backup_size;\n\tif (res->backup) {\n\t\trep->buffer_map_handle =\n\t\t\tdrm_vma_node_offset_addr(&res->backup->base.vma_node);\n\t\trep->buffer_size = res->backup->base.num_pages * PAGE_SIZE;\n\t\trep->buffer_handle = backup_handle;\n\t} else {\n\t\trep->buffer_map_handle = 0;\n\t\trep->buffer_size = 0;\n\t\trep->buffer_handle = SVGA3D_INVALID_ID;\n\t}\n\n\tvmw_resource_unreference(&res);\n\nout_unlock:\n\tttm_read_unlock(&dev_priv->reservation_sem);\n\treturn ret;\n}",
        "code_after_change": "int vmw_gb_surface_define_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *file_priv)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_user_surface *user_srf;\n\tstruct vmw_surface *srf;\n\tstruct vmw_resource *res;\n\tstruct vmw_resource *tmp;\n\tunion drm_vmw_gb_surface_create_arg *arg =\n\t    (union drm_vmw_gb_surface_create_arg *)data;\n\tstruct drm_vmw_gb_surface_create_req *req = &arg->req;\n\tstruct drm_vmw_gb_surface_create_rep *rep = &arg->rep;\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tint ret;\n\tuint32_t size;\n\tuint32_t backup_handle = 0;\n\n\tif (req->multisample_count != 0)\n\t\treturn -EINVAL;\n\n\tif (req->mip_levels > DRM_VMW_MAX_MIP_LEVELS)\n\t\treturn -EINVAL;\n\n\tif (unlikely(vmw_user_surface_size == 0))\n\t\tvmw_user_surface_size = ttm_round_pot(sizeof(*user_srf)) +\n\t\t\t128;\n\n\tsize = vmw_user_surface_size + 128;\n\n\t/* Define a surface based on the parameters. */\n\tret = vmw_surface_gb_priv_define(dev,\n\t\t\tsize,\n\t\t\treq->svga3d_flags,\n\t\t\treq->format,\n\t\t\treq->drm_surface_flags & drm_vmw_surface_flag_scanout,\n\t\t\treq->mip_levels,\n\t\t\treq->multisample_count,\n\t\t\treq->array_size,\n\t\t\treq->base_size,\n\t\t\t&srf);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tuser_srf = container_of(srf, struct vmw_user_surface, srf);\n\tif (drm_is_primary_client(file_priv))\n\t\tuser_srf->master = drm_master_get(file_priv->master);\n\n\tret = ttm_read_lock(&dev_priv->reservation_sem, true);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tres = &user_srf->srf.res;\n\n\n\tif (req->buffer_handle != SVGA3D_INVALID_ID) {\n\t\tret = vmw_user_dmabuf_lookup(tfile, req->buffer_handle,\n\t\t\t\t\t     &res->backup,\n\t\t\t\t\t     &user_srf->backup_base);\n\t\tif (ret == 0) {\n\t\t\tif (res->backup->base.num_pages * PAGE_SIZE <\n\t\t\t    res->backup_size) {\n\t\t\t\tDRM_ERROR(\"Surface backup buffer is too small.\\n\");\n\t\t\t\tvmw_dmabuf_unreference(&res->backup);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_unlock;\n\t\t\t} else {\n\t\t\t\tbackup_handle = req->buffer_handle;\n\t\t\t}\n\t\t}\n\t} else if (req->drm_surface_flags & drm_vmw_surface_flag_create_buffer)\n\t\tret = vmw_user_dmabuf_alloc(dev_priv, tfile,\n\t\t\t\t\t    res->backup_size,\n\t\t\t\t\t    req->drm_surface_flags &\n\t\t\t\t\t    drm_vmw_surface_flag_shareable,\n\t\t\t\t\t    &backup_handle,\n\t\t\t\t\t    &res->backup,\n\t\t\t\t\t    &user_srf->backup_base);\n\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\n\ttmp = vmw_resource_reference(res);\n\tret = ttm_prime_object_init(tfile, res->backup_size, &user_srf->prime,\n\t\t\t\t    req->drm_surface_flags &\n\t\t\t\t    drm_vmw_surface_flag_shareable,\n\t\t\t\t    VMW_RES_SURFACE,\n\t\t\t\t    &vmw_user_surface_base_release, NULL);\n\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&tmp);\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\n\trep->handle      = user_srf->prime.base.hash.key;\n\trep->backup_size = res->backup_size;\n\tif (res->backup) {\n\t\trep->buffer_map_handle =\n\t\t\tdrm_vma_node_offset_addr(&res->backup->base.vma_node);\n\t\trep->buffer_size = res->backup->base.num_pages * PAGE_SIZE;\n\t\trep->buffer_handle = backup_handle;\n\t} else {\n\t\trep->buffer_map_handle = 0;\n\t\trep->buffer_size = 0;\n\t\trep->buffer_handle = SVGA3D_INVALID_ID;\n\t}\n\n\tvmw_resource_unreference(&res);\n\nout_unlock:\n\tttm_read_unlock(&dev_priv->reservation_sem);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,7 +13,7 @@\n \tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n \tint ret;\n \tuint32_t size;\n-\tuint32_t backup_handle;\n+\tuint32_t backup_handle = 0;\n \n \tif (req->multisample_count != 0)\n \t\treturn -EINVAL;\n@@ -56,12 +56,16 @@\n \t\tret = vmw_user_dmabuf_lookup(tfile, req->buffer_handle,\n \t\t\t\t\t     &res->backup,\n \t\t\t\t\t     &user_srf->backup_base);\n-\t\tif (ret == 0 && res->backup->base.num_pages * PAGE_SIZE <\n-\t\t    res->backup_size) {\n-\t\t\tDRM_ERROR(\"Surface backup buffer is too small.\\n\");\n-\t\t\tvmw_dmabuf_unreference(&res->backup);\n-\t\t\tret = -EINVAL;\n-\t\t\tgoto out_unlock;\n+\t\tif (ret == 0) {\n+\t\t\tif (res->backup->base.num_pages * PAGE_SIZE <\n+\t\t\t    res->backup_size) {\n+\t\t\t\tDRM_ERROR(\"Surface backup buffer is too small.\\n\");\n+\t\t\t\tvmw_dmabuf_unreference(&res->backup);\n+\t\t\t\tret = -EINVAL;\n+\t\t\t\tgoto out_unlock;\n+\t\t\t} else {\n+\t\t\t\tbackup_handle = req->buffer_handle;\n+\t\t\t}\n \t\t}\n \t} else if (req->drm_surface_flags & drm_vmw_surface_flag_create_buffer)\n \t\tret = vmw_user_dmabuf_alloc(dev_priv, tfile,",
        "function_modified_lines": {
            "added": [
                "\tuint32_t backup_handle = 0;",
                "\t\tif (ret == 0) {",
                "\t\t\tif (res->backup->base.num_pages * PAGE_SIZE <",
                "\t\t\t    res->backup_size) {",
                "\t\t\t\tDRM_ERROR(\"Surface backup buffer is too small.\\n\");",
                "\t\t\t\tvmw_dmabuf_unreference(&res->backup);",
                "\t\t\t\tret = -EINVAL;",
                "\t\t\t\tgoto out_unlock;",
                "\t\t\t} else {",
                "\t\t\t\tbackup_handle = req->buffer_handle;",
                "\t\t\t}"
            ],
            "deleted": [
                "\tuint32_t backup_handle;",
                "\t\tif (ret == 0 && res->backup->base.num_pages * PAGE_SIZE <",
                "\t\t    res->backup_size) {",
                "\t\t\tDRM_ERROR(\"Surface backup buffer is too small.\\n\");",
                "\t\t\tvmw_dmabuf_unreference(&res->backup);",
                "\t\t\tret = -EINVAL;",
                "\t\t\tgoto out_unlock;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The vmw_gb_surface_define_ioctl function (accessible via DRM_IOCTL_VMW_GB_SURFACE_CREATE) in drivers/gpu/drm/vmwgfx/vmwgfx_surface.c in the Linux kernel through 4.11.4 defines a backup_handle variable but does not give it an initial value. If one attempts to create a GB surface, with a previously allocated DMA buffer to be used as a backup buffer, the backup_handle variable does not get written to and is then later returned to user space, allowing local users to obtain sensitive information from uninitialized kernel memory via a crafted ioctl call.",
        "id": 1570
    },
    {
        "cve_id": "CVE-2016-5244",
        "code_before_change": "void rds_inc_info_copy(struct rds_incoming *inc,\n\t\t       struct rds_info_iterator *iter,\n\t\t       __be32 saddr, __be32 daddr, int flip)\n{\n\tstruct rds_info_message minfo;\n\n\tminfo.seq = be64_to_cpu(inc->i_hdr.h_sequence);\n\tminfo.len = be32_to_cpu(inc->i_hdr.h_len);\n\n\tif (flip) {\n\t\tminfo.laddr = daddr;\n\t\tminfo.faddr = saddr;\n\t\tminfo.lport = inc->i_hdr.h_dport;\n\t\tminfo.fport = inc->i_hdr.h_sport;\n\t} else {\n\t\tminfo.laddr = saddr;\n\t\tminfo.faddr = daddr;\n\t\tminfo.lport = inc->i_hdr.h_sport;\n\t\tminfo.fport = inc->i_hdr.h_dport;\n\t}\n\n\trds_info_copy(iter, &minfo, sizeof(minfo));\n}",
        "code_after_change": "void rds_inc_info_copy(struct rds_incoming *inc,\n\t\t       struct rds_info_iterator *iter,\n\t\t       __be32 saddr, __be32 daddr, int flip)\n{\n\tstruct rds_info_message minfo;\n\n\tminfo.seq = be64_to_cpu(inc->i_hdr.h_sequence);\n\tminfo.len = be32_to_cpu(inc->i_hdr.h_len);\n\n\tif (flip) {\n\t\tminfo.laddr = daddr;\n\t\tminfo.faddr = saddr;\n\t\tminfo.lport = inc->i_hdr.h_dport;\n\t\tminfo.fport = inc->i_hdr.h_sport;\n\t} else {\n\t\tminfo.laddr = saddr;\n\t\tminfo.faddr = daddr;\n\t\tminfo.lport = inc->i_hdr.h_sport;\n\t\tminfo.fport = inc->i_hdr.h_dport;\n\t}\n\n\tminfo.flags = 0;\n\n\trds_info_copy(iter, &minfo, sizeof(minfo));\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,5 +19,7 @@\n \t\tminfo.fport = inc->i_hdr.h_dport;\n \t}\n \n+\tminfo.flags = 0;\n+\n \trds_info_copy(iter, &minfo, sizeof(minfo));\n }",
        "function_modified_lines": {
            "added": [
                "\tminfo.flags = 0;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The rds_inc_info_copy function in net/rds/recv.c in the Linux kernel through 4.6.3 does not initialize a certain structure member, which allows remote attackers to obtain sensitive information from kernel stack memory by reading an RDS message.",
        "id": 1054
    },
    {
        "cve_id": "CVE-2014-3917",
        "code_before_change": "static enum audit_state audit_filter_syscall(struct task_struct *tsk,\n\t\t\t\t\t     struct audit_context *ctx,\n\t\t\t\t\t     struct list_head *list)\n{\n\tstruct audit_entry *e;\n\tenum audit_state state;\n\n\tif (audit_pid && tsk->tgid == audit_pid)\n\t\treturn AUDIT_DISABLED;\n\n\trcu_read_lock();\n\tif (!list_empty(list)) {\n\t\tint word = AUDIT_WORD(ctx->major);\n\t\tint bit  = AUDIT_BIT(ctx->major);\n\n\t\tlist_for_each_entry_rcu(e, list, list) {\n\t\t\tif ((e->rule.mask[word] & bit) == bit &&\n\t\t\t    audit_filter_rules(tsk, &e->rule, ctx, NULL,\n\t\t\t\t\t       &state, false)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tctx->current_state = state;\n\t\t\t\treturn state;\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn AUDIT_BUILD_CONTEXT;\n}",
        "code_after_change": "static enum audit_state audit_filter_syscall(struct task_struct *tsk,\n\t\t\t\t\t     struct audit_context *ctx,\n\t\t\t\t\t     struct list_head *list)\n{\n\tstruct audit_entry *e;\n\tenum audit_state state;\n\n\tif (audit_pid && tsk->tgid == audit_pid)\n\t\treturn AUDIT_DISABLED;\n\n\trcu_read_lock();\n\tif (!list_empty(list)) {\n\t\tlist_for_each_entry_rcu(e, list, list) {\n\t\t\tif (audit_in_mask(&e->rule, ctx->major) &&\n\t\t\t    audit_filter_rules(tsk, &e->rule, ctx, NULL,\n\t\t\t\t\t       &state, false)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tctx->current_state = state;\n\t\t\t\treturn state;\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn AUDIT_BUILD_CONTEXT;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,11 +10,8 @@\n \n \trcu_read_lock();\n \tif (!list_empty(list)) {\n-\t\tint word = AUDIT_WORD(ctx->major);\n-\t\tint bit  = AUDIT_BIT(ctx->major);\n-\n \t\tlist_for_each_entry_rcu(e, list, list) {\n-\t\t\tif ((e->rule.mask[word] & bit) == bit &&\n+\t\t\tif (audit_in_mask(&e->rule, ctx->major) &&\n \t\t\t    audit_filter_rules(tsk, &e->rule, ctx, NULL,\n \t\t\t\t\t       &state, false)) {\n \t\t\t\trcu_read_unlock();",
        "function_modified_lines": {
            "added": [
                "\t\t\tif (audit_in_mask(&e->rule, ctx->major) &&"
            ],
            "deleted": [
                "\t\tint word = AUDIT_WORD(ctx->major);",
                "\t\tint bit  = AUDIT_BIT(ctx->major);",
                "",
                "\t\t\tif ((e->rule.mask[word] & bit) == bit &&"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "kernel/auditsc.c in the Linux kernel through 3.14.5, when CONFIG_AUDITSYSCALL is enabled with certain syscall rules, allows local users to obtain potentially sensitive single-bit values from kernel memory or cause a denial of service (OOPS) via a large value of a syscall number.",
        "id": 548
    },
    {
        "cve_id": "CVE-2013-4299",
        "code_before_change": "static int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tuint32_t stride;\n\tchunk_t next_free;\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tstride = (ps->exceptions_per_area + 1);\n\tnext_free = ++ps->next_free;\n\tif (sector_div(next_free, stride) == 1)\n\t\tps->next_free++;\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}",
        "code_after_change": "static int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tps->next_free++;\n\tskip_metadata(ps);\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,8 +2,6 @@\n \t\t\t\t\tstruct dm_exception *e)\n {\n \tstruct pstore *ps = get_info(store);\n-\tuint32_t stride;\n-\tchunk_t next_free;\n \tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n \n \t/* Is there enough room ? */\n@@ -16,10 +14,8 @@\n \t * Move onto the next free pending, making sure to take\n \t * into account the location of the metadata chunks.\n \t */\n-\tstride = (ps->exceptions_per_area + 1);\n-\tnext_free = ++ps->next_free;\n-\tif (sector_div(next_free, stride) == 1)\n-\t\tps->next_free++;\n+\tps->next_free++;\n+\tskip_metadata(ps);\n \n \tatomic_inc(&ps->pending_count);\n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tps->next_free++;",
                "\tskip_metadata(ps);"
            ],
            "deleted": [
                "\tuint32_t stride;",
                "\tchunk_t next_free;",
                "\tstride = (ps->exceptions_per_area + 1);",
                "\tnext_free = ++ps->next_free;",
                "\tif (sector_div(next_free, stride) == 1)",
                "\t\tps->next_free++;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-200"
        ],
        "cve_description": "Interpretation conflict in drivers/md/dm-snap-persistent.c in the Linux kernel through 3.11.6 allows remote authenticated users to obtain sensitive information or modify data via a crafted mapping to a snapshot block device.",
        "id": 293
    },
    {
        "cve_id": "CVE-2012-4530",
        "code_before_change": "static int load_misc_binary(struct linux_binprm *bprm)\n{\n\tNode *fmt;\n\tstruct file * interp_file = NULL;\n\tchar iname[BINPRM_BUF_SIZE];\n\tconst char *iname_addr = iname;\n\tint retval;\n\tint fd_binary = -1;\n\n\tretval = -ENOEXEC;\n\tif (!enabled)\n\t\tgoto _ret;\n\n\tretval = -ENOEXEC;\n\tif (bprm->recursion_depth > BINPRM_MAX_RECURSION)\n\t\tgoto _ret;\n\n\t/* to keep locking time low, we copy the interpreter string */\n\tread_lock(&entries_lock);\n\tfmt = check_file(bprm);\n\tif (fmt)\n\t\tstrlcpy(iname, fmt->interpreter, BINPRM_BUF_SIZE);\n\tread_unlock(&entries_lock);\n\tif (!fmt)\n\t\tgoto _ret;\n\n\tif (!(fmt->flags & MISC_FMT_PRESERVE_ARGV0)) {\n\t\tretval = remove_arg_zero(bprm);\n\t\tif (retval)\n\t\t\tgoto _ret;\n\t}\n\n\tif (fmt->flags & MISC_FMT_OPEN_BINARY) {\n\n\t\t/* if the binary should be opened on behalf of the\n\t\t * interpreter than keep it open and assign descriptor\n\t\t * to it */\n \t\tfd_binary = get_unused_fd();\n \t\tif (fd_binary < 0) {\n \t\t\tretval = fd_binary;\n \t\t\tgoto _ret;\n \t\t}\n \t\tfd_install(fd_binary, bprm->file);\n\n\t\t/* if the binary is not readable than enforce mm->dumpable=0\n\t\t   regardless of the interpreter's permissions */\n\t\twould_dump(bprm, bprm->file);\n\n\t\tallow_write_access(bprm->file);\n\t\tbprm->file = NULL;\n\n\t\t/* mark the bprm that fd should be passed to interp */\n\t\tbprm->interp_flags |= BINPRM_FLAGS_EXECFD;\n\t\tbprm->interp_data = fd_binary;\n\n \t} else {\n \t\tallow_write_access(bprm->file);\n \t\tfput(bprm->file);\n \t\tbprm->file = NULL;\n \t}\n\t/* make argv[1] be the path to the binary */\n\tretval = copy_strings_kernel (1, &bprm->interp, bprm);\n\tif (retval < 0)\n\t\tgoto _error;\n\tbprm->argc++;\n\n\t/* add the interp as argv[0] */\n\tretval = copy_strings_kernel (1, &iname_addr, bprm);\n\tif (retval < 0)\n\t\tgoto _error;\n\tbprm->argc ++;\n\n\tbprm->interp = iname;\t/* for binfmt_script */\n\n\tinterp_file = open_exec (iname);\n\tretval = PTR_ERR (interp_file);\n\tif (IS_ERR (interp_file))\n\t\tgoto _error;\n\n\tbprm->file = interp_file;\n\tif (fmt->flags & MISC_FMT_CREDENTIALS) {\n\t\t/*\n\t\t * No need to call prepare_binprm(), it's already been\n\t\t * done.  bprm->buf is stale, update from interp_file.\n\t\t */\n\t\tmemset(bprm->buf, 0, BINPRM_BUF_SIZE);\n\t\tretval = kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);\n\t} else\n\t\tretval = prepare_binprm (bprm);\n\n\tif (retval < 0)\n\t\tgoto _error;\n\n\tbprm->recursion_depth++;\n\n\tretval = search_binary_handler(bprm);\n\tif (retval < 0)\n\t\tgoto _error;\n\n_ret:\n\treturn retval;\n_error:\n\tif (fd_binary > 0)\n\t\tsys_close(fd_binary);\n\tbprm->interp_flags = 0;\n\tbprm->interp_data = 0;\n\tgoto _ret;\n}",
        "code_after_change": "static int load_misc_binary(struct linux_binprm *bprm)\n{\n\tNode *fmt;\n\tstruct file * interp_file = NULL;\n\tchar iname[BINPRM_BUF_SIZE];\n\tconst char *iname_addr = iname;\n\tint retval;\n\tint fd_binary = -1;\n\n\tretval = -ENOEXEC;\n\tif (!enabled)\n\t\tgoto _ret;\n\n\t/* to keep locking time low, we copy the interpreter string */\n\tread_lock(&entries_lock);\n\tfmt = check_file(bprm);\n\tif (fmt)\n\t\tstrlcpy(iname, fmt->interpreter, BINPRM_BUF_SIZE);\n\tread_unlock(&entries_lock);\n\tif (!fmt)\n\t\tgoto _ret;\n\n\tif (!(fmt->flags & MISC_FMT_PRESERVE_ARGV0)) {\n\t\tretval = remove_arg_zero(bprm);\n\t\tif (retval)\n\t\t\tgoto _ret;\n\t}\n\n\tif (fmt->flags & MISC_FMT_OPEN_BINARY) {\n\n\t\t/* if the binary should be opened on behalf of the\n\t\t * interpreter than keep it open and assign descriptor\n\t\t * to it */\n \t\tfd_binary = get_unused_fd();\n \t\tif (fd_binary < 0) {\n \t\t\tretval = fd_binary;\n \t\t\tgoto _ret;\n \t\t}\n \t\tfd_install(fd_binary, bprm->file);\n\n\t\t/* if the binary is not readable than enforce mm->dumpable=0\n\t\t   regardless of the interpreter's permissions */\n\t\twould_dump(bprm, bprm->file);\n\n\t\tallow_write_access(bprm->file);\n\t\tbprm->file = NULL;\n\n\t\t/* mark the bprm that fd should be passed to interp */\n\t\tbprm->interp_flags |= BINPRM_FLAGS_EXECFD;\n\t\tbprm->interp_data = fd_binary;\n\n \t} else {\n \t\tallow_write_access(bprm->file);\n \t\tfput(bprm->file);\n \t\tbprm->file = NULL;\n \t}\n\t/* make argv[1] be the path to the binary */\n\tretval = copy_strings_kernel (1, &bprm->interp, bprm);\n\tif (retval < 0)\n\t\tgoto _error;\n\tbprm->argc++;\n\n\t/* add the interp as argv[0] */\n\tretval = copy_strings_kernel (1, &iname_addr, bprm);\n\tif (retval < 0)\n\t\tgoto _error;\n\tbprm->argc ++;\n\n\tbprm->interp = iname;\t/* for binfmt_script */\n\n\tinterp_file = open_exec (iname);\n\tretval = PTR_ERR (interp_file);\n\tif (IS_ERR (interp_file))\n\t\tgoto _error;\n\n\tbprm->file = interp_file;\n\tif (fmt->flags & MISC_FMT_CREDENTIALS) {\n\t\t/*\n\t\t * No need to call prepare_binprm(), it's already been\n\t\t * done.  bprm->buf is stale, update from interp_file.\n\t\t */\n\t\tmemset(bprm->buf, 0, BINPRM_BUF_SIZE);\n\t\tretval = kernel_read(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);\n\t} else\n\t\tretval = prepare_binprm (bprm);\n\n\tif (retval < 0)\n\t\tgoto _error;\n\n\tretval = search_binary_handler(bprm);\n\tif (retval < 0)\n\t\tgoto _error;\n\n_ret:\n\treturn retval;\n_error:\n\tif (fd_binary > 0)\n\t\tsys_close(fd_binary);\n\tbprm->interp_flags = 0;\n\tbprm->interp_data = 0;\n\tgoto _ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,10 +9,6 @@\n \n \tretval = -ENOEXEC;\n \tif (!enabled)\n-\t\tgoto _ret;\n-\n-\tretval = -ENOEXEC;\n-\tif (bprm->recursion_depth > BINPRM_MAX_RECURSION)\n \t\tgoto _ret;\n \n \t/* to keep locking time low, we copy the interpreter string */\n@@ -91,8 +87,6 @@\n \tif (retval < 0)\n \t\tgoto _error;\n \n-\tbprm->recursion_depth++;\n-\n \tretval = search_binary_handler(bprm);\n \tif (retval < 0)\n \t\tgoto _error;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tgoto _ret;",
                "",
                "\tretval = -ENOEXEC;",
                "\tif (bprm->recursion_depth > BINPRM_MAX_RECURSION)",
                "\tbprm->recursion_depth++;",
                ""
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The load_script function in fs/binfmt_script.c in the Linux kernel before 3.7.2 does not properly handle recursion, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 111
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "int __restore_xstate_sig(void __user *buf, void __user *buf_fx, int size)\n{\n\tint ia32_fxstate = (buf != buf_fx);\n\tstruct task_struct *tsk = current;\n\tint state_size = xstate_size;\n\tu64 xstate_bv = 0;\n\tint fx_only = 0;\n\n\tia32_fxstate &= (config_enabled(CONFIG_X86_32) ||\n\t\t\t config_enabled(CONFIG_IA32_EMULATION));\n\n\tif (!buf) {\n\t\tdrop_init_fpu(tsk);\n\t\treturn 0;\n\t}\n\n\tif (!access_ok(VERIFY_READ, buf, size))\n\t\treturn -EACCES;\n\n\tif (!used_math() && init_fpu(tsk))\n\t\treturn -1;\n\n\tif (!HAVE_HWFP) {\n\t\treturn fpregs_soft_set(current, NULL,\n\t\t\t\t       0, sizeof(struct user_i387_ia32_struct),\n\t\t\t\t       NULL, buf) != 0;\n\t}\n\n\tif (use_xsave()) {\n\t\tstruct _fpx_sw_bytes fx_sw_user;\n\t\tif (unlikely(check_for_xstate(buf_fx, buf_fx, &fx_sw_user))) {\n\t\t\t/*\n\t\t\t * Couldn't find the extended state information in the\n\t\t\t * memory layout. Restore just the FP/SSE and init all\n\t\t\t * the other extended state.\n\t\t\t */\n\t\t\tstate_size = sizeof(struct i387_fxsave_struct);\n\t\t\tfx_only = 1;\n\t\t} else {\n\t\t\tstate_size = fx_sw_user.xstate_size;\n\t\t\txstate_bv = fx_sw_user.xstate_bv;\n\t\t}\n\t}\n\n\tif (ia32_fxstate) {\n\t\t/*\n\t\t * For 32-bit frames with fxstate, copy the user state to the\n\t\t * thread's fpu state, reconstruct fxstate from the fsave\n\t\t * header. Sanitize the copied state etc.\n\t\t */\n\t\tstruct xsave_struct *xsave = &tsk->thread.fpu.state->xsave;\n\t\tstruct user_i387_ia32_struct env;\n\t\tint err = 0;\n\n\t\t/*\n\t\t * Drop the current fpu which clears used_math(). This ensures\n\t\t * that any context-switch during the copy of the new state,\n\t\t * avoids the intermediate state from getting restored/saved.\n\t\t * Thus avoiding the new restored state from getting corrupted.\n\t\t * We will be ready to restore/save the state only after\n\t\t * set_used_math() is again set.\n\t\t */\n\t\tdrop_fpu(tsk);\n\n\t\tif (__copy_from_user(xsave, buf_fx, state_size) ||\n\t\t    __copy_from_user(&env, buf, sizeof(env))) {\n\t\t\terr = -1;\n\t\t} else {\n\t\t\tsanitize_restored_xstate(tsk, &env, xstate_bv, fx_only);\n\t\t\tset_used_math();\n\t\t}\n\n\t\tif (use_xsave())\n\t\t\tmath_state_restore();\n\n\t\treturn err;\n\t} else {\n\t\t/*\n\t\t * For 64-bit frames and 32-bit fsave frames, restore the user\n\t\t * state to the registers directly (with exceptions handled).\n\t\t */\n\t\tuser_fpu_begin();\n\t\tif (restore_user_xstate(buf_fx, xstate_bv, fx_only)) {\n\t\t\tdrop_init_fpu(tsk);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int __restore_xstate_sig(void __user *buf, void __user *buf_fx, int size)\n{\n\tint ia32_fxstate = (buf != buf_fx);\n\tstruct task_struct *tsk = current;\n\tint state_size = xstate_size;\n\tu64 xstate_bv = 0;\n\tint fx_only = 0;\n\n\tia32_fxstate &= (config_enabled(CONFIG_X86_32) ||\n\t\t\t config_enabled(CONFIG_IA32_EMULATION));\n\n\tif (!buf) {\n\t\tdrop_init_fpu(tsk);\n\t\treturn 0;\n\t}\n\n\tif (!access_ok(VERIFY_READ, buf, size))\n\t\treturn -EACCES;\n\n\tif (!used_math() && init_fpu(tsk))\n\t\treturn -1;\n\n\tif (!HAVE_HWFP) {\n\t\treturn fpregs_soft_set(current, NULL,\n\t\t\t\t       0, sizeof(struct user_i387_ia32_struct),\n\t\t\t\t       NULL, buf) != 0;\n\t}\n\n\tif (use_xsave()) {\n\t\tstruct _fpx_sw_bytes fx_sw_user;\n\t\tif (unlikely(check_for_xstate(buf_fx, buf_fx, &fx_sw_user))) {\n\t\t\t/*\n\t\t\t * Couldn't find the extended state information in the\n\t\t\t * memory layout. Restore just the FP/SSE and init all\n\t\t\t * the other extended state.\n\t\t\t */\n\t\t\tstate_size = sizeof(struct i387_fxsave_struct);\n\t\t\tfx_only = 1;\n\t\t} else {\n\t\t\tstate_size = fx_sw_user.xstate_size;\n\t\t\txstate_bv = fx_sw_user.xstate_bv;\n\t\t}\n\t}\n\n\tif (ia32_fxstate) {\n\t\t/*\n\t\t * For 32-bit frames with fxstate, copy the user state to the\n\t\t * thread's fpu state, reconstruct fxstate from the fsave\n\t\t * header. Sanitize the copied state etc.\n\t\t */\n\t\tstruct xsave_struct *xsave = &tsk->thread.fpu.state->xsave;\n\t\tstruct user_i387_ia32_struct env;\n\t\tint err = 0;\n\n\t\t/*\n\t\t * Drop the current fpu which clears used_math(). This ensures\n\t\t * that any context-switch during the copy of the new state,\n\t\t * avoids the intermediate state from getting restored/saved.\n\t\t * Thus avoiding the new restored state from getting corrupted.\n\t\t * We will be ready to restore/save the state only after\n\t\t * set_used_math() is again set.\n\t\t */\n\t\tdrop_fpu(tsk);\n\n\t\tif (__copy_from_user(xsave, buf_fx, state_size) ||\n\t\t    __copy_from_user(&env, buf, sizeof(env))) {\n\t\t\terr = -1;\n\t\t} else {\n\t\t\tsanitize_restored_xstate(tsk, &env, xstate_bv, fx_only);\n\t\t\tset_used_math();\n\t\t}\n\n\t\tif (use_eager_fpu())\n\t\t\tmath_state_restore();\n\n\t\treturn err;\n\t} else {\n\t\t/*\n\t\t * For 64-bit frames and 32-bit fsave frames, restore the user\n\t\t * state to the registers directly (with exceptions handled).\n\t\t */\n\t\tuser_fpu_begin();\n\t\tif (restore_user_xstate(buf_fx, xstate_bv, fx_only)) {\n\t\t\tdrop_init_fpu(tsk);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -70,7 +70,7 @@\n \t\t\tset_used_math();\n \t\t}\n \n-\t\tif (use_xsave())\n+\t\tif (use_eager_fpu())\n \t\t\tmath_state_restore();\n \n \t\treturn err;",
        "function_modified_lines": {
            "added": [
                "\t\tif (use_eager_fpu())"
            ],
            "deleted": [
                "\t\tif (use_xsave())"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1815
    },
    {
        "cve_id": "CVE-2022-33741",
        "code_before_change": "static int xennet_connect(struct net_device *dev)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tunsigned int num_queues = 0;\n\tint err;\n\tunsigned int j = 0;\n\tstruct netfront_queue *queue = NULL;\n\n\tif (!xenbus_read_unsigned(np->xbdev->otherend, \"feature-rx-copy\", 0)) {\n\t\tdev_info(&dev->dev,\n\t\t\t \"backend does not support copying receive path\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\terr = talk_to_netback(np->xbdev, np);\n\tif (err)\n\t\treturn err;\n\tif (np->netback_has_xdp_headroom)\n\t\tpr_info(\"backend supports XDP headroom\\n\");\n\n\t/* talk_to_netback() sets the correct number of queues */\n\tnum_queues = dev->real_num_tx_queues;\n\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\terr = register_netdev(dev);\n\t\tif (err) {\n\t\t\tpr_warn(\"%s: register_netdev err=%d\\n\", __func__, err);\n\t\t\tdevice_unregister(&np->xbdev->dev);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\trtnl_lock();\n\tnetdev_update_features(dev);\n\trtnl_unlock();\n\n\t/*\n\t * All public and private state should now be sane.  Get\n\t * ready to start sending and receiving packets and give the driver\n\t * domain a kick because we've probably just requeued some\n\t * packets.\n\t */\n\tnetif_tx_lock_bh(np->netdev);\n\tnetif_device_attach(np->netdev);\n\tnetif_tx_unlock_bh(np->netdev);\n\n\tnetif_carrier_on(np->netdev);\n\tfor (j = 0; j < num_queues; ++j) {\n\t\tqueue = &np->queues[j];\n\n\t\tnotify_remote_via_irq(queue->tx_irq);\n\t\tif (queue->tx_irq != queue->rx_irq)\n\t\t\tnotify_remote_via_irq(queue->rx_irq);\n\n\t\tspin_lock_irq(&queue->tx_lock);\n\t\txennet_tx_buf_gc(queue);\n\t\tspin_unlock_irq(&queue->tx_lock);\n\n\t\tspin_lock_bh(&queue->rx_lock);\n\t\txennet_alloc_rx_buffers(queue);\n\t\tspin_unlock_bh(&queue->rx_lock);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int xennet_connect(struct net_device *dev)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tunsigned int num_queues = 0;\n\tint err;\n\tunsigned int j = 0;\n\tstruct netfront_queue *queue = NULL;\n\n\tif (!xenbus_read_unsigned(np->xbdev->otherend, \"feature-rx-copy\", 0)) {\n\t\tdev_info(&dev->dev,\n\t\t\t \"backend does not support copying receive path\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\terr = talk_to_netback(np->xbdev, np);\n\tif (err)\n\t\treturn err;\n\tif (np->netback_has_xdp_headroom)\n\t\tpr_info(\"backend supports XDP headroom\\n\");\n\tif (np->bounce)\n\t\tdev_info(&np->xbdev->dev,\n\t\t\t \"bouncing transmitted data to zeroed pages\\n\");\n\n\t/* talk_to_netback() sets the correct number of queues */\n\tnum_queues = dev->real_num_tx_queues;\n\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\terr = register_netdev(dev);\n\t\tif (err) {\n\t\t\tpr_warn(\"%s: register_netdev err=%d\\n\", __func__, err);\n\t\t\tdevice_unregister(&np->xbdev->dev);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\trtnl_lock();\n\tnetdev_update_features(dev);\n\trtnl_unlock();\n\n\t/*\n\t * All public and private state should now be sane.  Get\n\t * ready to start sending and receiving packets and give the driver\n\t * domain a kick because we've probably just requeued some\n\t * packets.\n\t */\n\tnetif_tx_lock_bh(np->netdev);\n\tnetif_device_attach(np->netdev);\n\tnetif_tx_unlock_bh(np->netdev);\n\n\tnetif_carrier_on(np->netdev);\n\tfor (j = 0; j < num_queues; ++j) {\n\t\tqueue = &np->queues[j];\n\n\t\tnotify_remote_via_irq(queue->tx_irq);\n\t\tif (queue->tx_irq != queue->rx_irq)\n\t\t\tnotify_remote_via_irq(queue->rx_irq);\n\n\t\tspin_lock_irq(&queue->tx_lock);\n\t\txennet_tx_buf_gc(queue);\n\t\tspin_unlock_irq(&queue->tx_lock);\n\n\t\tspin_lock_bh(&queue->rx_lock);\n\t\txennet_alloc_rx_buffers(queue);\n\t\tspin_unlock_bh(&queue->rx_lock);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,6 +17,9 @@\n \t\treturn err;\n \tif (np->netback_has_xdp_headroom)\n \t\tpr_info(\"backend supports XDP headroom\\n\");\n+\tif (np->bounce)\n+\t\tdev_info(&np->xbdev->dev,\n+\t\t\t \"bouncing transmitted data to zeroed pages\\n\");\n \n \t/* talk_to_netback() sets the correct number of queues */\n \tnum_queues = dev->real_num_tx_queues;",
        "function_modified_lines": {
            "added": [
                "\tif (np->bounce)",
                "\t\tdev_info(&np->xbdev->dev,",
                "\t\t\t \"bouncing transmitted data to zeroed pages\\n\");"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3579
    },
    {
        "cve_id": "CVE-2013-4299",
        "code_before_change": "static int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\treturn 0;\n}",
        "code_after_change": "static int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\tskip_metadata(ps);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,5 +21,7 @@\n \n \tps->current_area--;\n \n+\tskip_metadata(ps);\n+\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tskip_metadata(ps);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264",
            "CWE-200"
        ],
        "cve_description": "Interpretation conflict in drivers/md/dm-snap-persistent.c in the Linux kernel through 3.11.6 allows remote authenticated users to obtain sensitive information or modify data via a crafted mapping to a snapshot block device.",
        "id": 292
    },
    {
        "cve_id": "CVE-2017-7495",
        "code_before_change": "int ext4_map_blocks(handle_t *handle, struct inode *inode,\n\t\t    struct ext4_map_blocks *map, int flags)\n{\n\tstruct extent_status es;\n\tint retval;\n\tint ret = 0;\n#ifdef ES_AGGRESSIVE_TEST\n\tstruct ext4_map_blocks orig_map;\n\n\tmemcpy(&orig_map, map, sizeof(*map));\n#endif\n\n\tmap->m_flags = 0;\n\text_debug(\"ext4_map_blocks(): inode %lu, flag %d, max_blocks %u,\"\n\t\t  \"logical block %lu\\n\", inode->i_ino, flags, map->m_len,\n\t\t  (unsigned long) map->m_lblk);\n\n\t/*\n\t * ext4_map_blocks returns an int, and m_len is an unsigned int\n\t */\n\tif (unlikely(map->m_len > INT_MAX))\n\t\tmap->m_len = INT_MAX;\n\n\t/* We can handle the block number less than EXT_MAX_BLOCKS */\n\tif (unlikely(map->m_lblk >= EXT_MAX_BLOCKS))\n\t\treturn -EFSCORRUPTED;\n\n\t/* Lookup extent status tree firstly */\n\tif (ext4_es_lookup_extent(inode, map->m_lblk, &es)) {\n\t\tif (ext4_es_is_written(&es) || ext4_es_is_unwritten(&es)) {\n\t\t\tmap->m_pblk = ext4_es_pblock(&es) +\n\t\t\t\t\tmap->m_lblk - es.es_lblk;\n\t\t\tmap->m_flags |= ext4_es_is_written(&es) ?\n\t\t\t\t\tEXT4_MAP_MAPPED : EXT4_MAP_UNWRITTEN;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t} else if (ext4_es_is_delayed(&es) || ext4_es_is_hole(&es)) {\n\t\t\tmap->m_pblk = 0;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t\tretval = 0;\n\t\t} else {\n\t\t\tBUG_ON(1);\n\t\t}\n#ifdef ES_AGGRESSIVE_TEST\n\t\text4_map_blocks_es_recheck(handle, inode, map,\n\t\t\t\t\t   &orig_map, flags);\n#endif\n\t\tgoto found;\n\t}\n\n\t/*\n\t * Try to see if we can get the block without requesting a new\n\t * file system block.\n\t */\n\tdown_read(&EXT4_I(inode)->i_data_sem);\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t}\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_find_delalloc_range(inode, map->m_lblk,\n\t\t\t\t\t     map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk,\n\t\t\t\t\t    map->m_len, map->m_pblk, status);\n\t\tif (ret < 0)\n\t\t\tretval = ret;\n\t}\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\nfound:\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\n\t/* If it is only a block(s) look up */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0)\n\t\treturn retval;\n\n\t/*\n\t * Returns if the blocks have already allocated\n\t *\n\t * Note that if blocks have been preallocated\n\t * ext4_ext_get_block() returns the create = 0\n\t * with buffer head unmapped.\n\t */\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED)\n\t\t/*\n\t\t * If we need to convert extent to unwritten\n\t\t * we continue and do the actual work in\n\t\t * ext4_ext_map_blocks()\n\t\t */\n\t\tif (!(flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN))\n\t\t\treturn retval;\n\n\t/*\n\t * Here we clear m_flags because after allocating an new extent,\n\t * it will be set again.\n\t */\n\tmap->m_flags &= ~EXT4_MAP_FLAGS;\n\n\t/*\n\t * New blocks allocate and/or writing to unwritten extent\n\t * will possibly result in updating i_data, so we take\n\t * the write lock of i_data_sem, and call get_block()\n\t * with create == 1 flag.\n\t */\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\n\t/*\n\t * We need to check for EXT4 here because migrate\n\t * could have changed the inode type in between\n\t */\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags);\n\n\t\tif (retval > 0 && map->m_flags & EXT4_MAP_NEW) {\n\t\t\t/*\n\t\t\t * We allocated new blocks which will result in\n\t\t\t * i_data's format changing.  Force the migrate\n\t\t\t * to fail by clearing migrate flags\n\t\t\t */\n\t\t\text4_clear_inode_state(inode, EXT4_STATE_EXT_MIGRATE);\n\t\t}\n\n\t\t/*\n\t\t * Update reserved blocks/metadata blocks after successful\n\t\t * block allocation which had been deferred till now. We don't\n\t\t * support fallocate for non extent files. So we can update\n\t\t * reserve space here.\n\t\t */\n\t\tif ((retval > 0) &&\n\t\t\t(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE))\n\t\t\text4_da_update_reserve_space(inode, retval, 1);\n\t}\n\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\t/*\n\t\t * We have to zeroout blocks before inserting them into extent\n\t\t * status tree. Otherwise someone could look them up there and\n\t\t * use them before they are really zeroed.\n\t\t */\n\t\tif (flags & EXT4_GET_BLOCKS_ZERO &&\n\t\t    map->m_flags & EXT4_MAP_MAPPED &&\n\t\t    map->m_flags & EXT4_MAP_NEW) {\n\t\t\tret = ext4_issue_zeroout(inode, map->m_lblk,\n\t\t\t\t\t\t map->m_pblk, map->m_len);\n\t\t\tif (ret) {\n\t\t\t\tretval = ret;\n\t\t\t\tgoto out_sem;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * If the extent has been zeroed out, we don't need to update\n\t\t * extent status tree.\n\t\t */\n\t\tif ((flags & EXT4_GET_BLOCKS_PRE_IO) &&\n\t\t    ext4_es_lookup_extent(inode, map->m_lblk, &es)) {\n\t\t\tif (ext4_es_is_written(&es))\n\t\t\t\tgoto out_sem;\n\t\t}\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_find_delalloc_range(inode, map->m_lblk,\n\t\t\t\t\t     map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk, map->m_len,\n\t\t\t\t\t    map->m_pblk, status);\n\t\tif (ret < 0) {\n\t\t\tretval = ret;\n\t\t\tgoto out_sem;\n\t\t}\n\t}\n\nout_sem:\n\tup_write((&EXT4_I(inode)->i_data_sem));\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\treturn retval;\n}",
        "code_after_change": "int ext4_map_blocks(handle_t *handle, struct inode *inode,\n\t\t    struct ext4_map_blocks *map, int flags)\n{\n\tstruct extent_status es;\n\tint retval;\n\tint ret = 0;\n#ifdef ES_AGGRESSIVE_TEST\n\tstruct ext4_map_blocks orig_map;\n\n\tmemcpy(&orig_map, map, sizeof(*map));\n#endif\n\n\tmap->m_flags = 0;\n\text_debug(\"ext4_map_blocks(): inode %lu, flag %d, max_blocks %u,\"\n\t\t  \"logical block %lu\\n\", inode->i_ino, flags, map->m_len,\n\t\t  (unsigned long) map->m_lblk);\n\n\t/*\n\t * ext4_map_blocks returns an int, and m_len is an unsigned int\n\t */\n\tif (unlikely(map->m_len > INT_MAX))\n\t\tmap->m_len = INT_MAX;\n\n\t/* We can handle the block number less than EXT_MAX_BLOCKS */\n\tif (unlikely(map->m_lblk >= EXT_MAX_BLOCKS))\n\t\treturn -EFSCORRUPTED;\n\n\t/* Lookup extent status tree firstly */\n\tif (ext4_es_lookup_extent(inode, map->m_lblk, &es)) {\n\t\tif (ext4_es_is_written(&es) || ext4_es_is_unwritten(&es)) {\n\t\t\tmap->m_pblk = ext4_es_pblock(&es) +\n\t\t\t\t\tmap->m_lblk - es.es_lblk;\n\t\t\tmap->m_flags |= ext4_es_is_written(&es) ?\n\t\t\t\t\tEXT4_MAP_MAPPED : EXT4_MAP_UNWRITTEN;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t} else if (ext4_es_is_delayed(&es) || ext4_es_is_hole(&es)) {\n\t\t\tmap->m_pblk = 0;\n\t\t\tretval = es.es_len - (map->m_lblk - es.es_lblk);\n\t\t\tif (retval > map->m_len)\n\t\t\t\tretval = map->m_len;\n\t\t\tmap->m_len = retval;\n\t\t\tretval = 0;\n\t\t} else {\n\t\t\tBUG_ON(1);\n\t\t}\n#ifdef ES_AGGRESSIVE_TEST\n\t\text4_map_blocks_es_recheck(handle, inode, map,\n\t\t\t\t\t   &orig_map, flags);\n#endif\n\t\tgoto found;\n\t}\n\n\t/*\n\t * Try to see if we can get the block without requesting a new\n\t * file system block.\n\t */\n\tdown_read(&EXT4_I(inode)->i_data_sem);\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags &\n\t\t\t\t\t     EXT4_GET_BLOCKS_KEEP_SIZE);\n\t}\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_find_delalloc_range(inode, map->m_lblk,\n\t\t\t\t\t     map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk,\n\t\t\t\t\t    map->m_len, map->m_pblk, status);\n\t\tif (ret < 0)\n\t\t\tretval = ret;\n\t}\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\nfound:\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\n\t/* If it is only a block(s) look up */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0)\n\t\treturn retval;\n\n\t/*\n\t * Returns if the blocks have already allocated\n\t *\n\t * Note that if blocks have been preallocated\n\t * ext4_ext_get_block() returns the create = 0\n\t * with buffer head unmapped.\n\t */\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED)\n\t\t/*\n\t\t * If we need to convert extent to unwritten\n\t\t * we continue and do the actual work in\n\t\t * ext4_ext_map_blocks()\n\t\t */\n\t\tif (!(flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN))\n\t\t\treturn retval;\n\n\t/*\n\t * Here we clear m_flags because after allocating an new extent,\n\t * it will be set again.\n\t */\n\tmap->m_flags &= ~EXT4_MAP_FLAGS;\n\n\t/*\n\t * New blocks allocate and/or writing to unwritten extent\n\t * will possibly result in updating i_data, so we take\n\t * the write lock of i_data_sem, and call get_block()\n\t * with create == 1 flag.\n\t */\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\n\t/*\n\t * We need to check for EXT4 here because migrate\n\t * could have changed the inode type in between\n\t */\n\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tretval = ext4_ext_map_blocks(handle, inode, map, flags);\n\t} else {\n\t\tretval = ext4_ind_map_blocks(handle, inode, map, flags);\n\n\t\tif (retval > 0 && map->m_flags & EXT4_MAP_NEW) {\n\t\t\t/*\n\t\t\t * We allocated new blocks which will result in\n\t\t\t * i_data's format changing.  Force the migrate\n\t\t\t * to fail by clearing migrate flags\n\t\t\t */\n\t\t\text4_clear_inode_state(inode, EXT4_STATE_EXT_MIGRATE);\n\t\t}\n\n\t\t/*\n\t\t * Update reserved blocks/metadata blocks after successful\n\t\t * block allocation which had been deferred till now. We don't\n\t\t * support fallocate for non extent files. So we can update\n\t\t * reserve space here.\n\t\t */\n\t\tif ((retval > 0) &&\n\t\t\t(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE))\n\t\t\text4_da_update_reserve_space(inode, retval, 1);\n\t}\n\n\tif (retval > 0) {\n\t\tunsigned int status;\n\n\t\tif (unlikely(retval != map->m_len)) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"ES len assertion failed for inode \"\n\t\t\t\t     \"%lu: retval %d != map->m_len %d\",\n\t\t\t\t     inode->i_ino, retval, map->m_len);\n\t\t\tWARN_ON(1);\n\t\t}\n\n\t\t/*\n\t\t * We have to zeroout blocks before inserting them into extent\n\t\t * status tree. Otherwise someone could look them up there and\n\t\t * use them before they are really zeroed.\n\t\t */\n\t\tif (flags & EXT4_GET_BLOCKS_ZERO &&\n\t\t    map->m_flags & EXT4_MAP_MAPPED &&\n\t\t    map->m_flags & EXT4_MAP_NEW) {\n\t\t\tret = ext4_issue_zeroout(inode, map->m_lblk,\n\t\t\t\t\t\t map->m_pblk, map->m_len);\n\t\t\tif (ret) {\n\t\t\t\tretval = ret;\n\t\t\t\tgoto out_sem;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * If the extent has been zeroed out, we don't need to update\n\t\t * extent status tree.\n\t\t */\n\t\tif ((flags & EXT4_GET_BLOCKS_PRE_IO) &&\n\t\t    ext4_es_lookup_extent(inode, map->m_lblk, &es)) {\n\t\t\tif (ext4_es_is_written(&es))\n\t\t\t\tgoto out_sem;\n\t\t}\n\t\tstatus = map->m_flags & EXT4_MAP_UNWRITTEN ?\n\t\t\t\tEXTENT_STATUS_UNWRITTEN : EXTENT_STATUS_WRITTEN;\n\t\tif (!(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) &&\n\t\t    !(status & EXTENT_STATUS_WRITTEN) &&\n\t\t    ext4_find_delalloc_range(inode, map->m_lblk,\n\t\t\t\t\t     map->m_lblk + map->m_len - 1))\n\t\t\tstatus |= EXTENT_STATUS_DELAYED;\n\t\tret = ext4_es_insert_extent(inode, map->m_lblk, map->m_len,\n\t\t\t\t\t    map->m_pblk, status);\n\t\tif (ret < 0) {\n\t\t\tretval = ret;\n\t\t\tgoto out_sem;\n\t\t}\n\t}\n\nout_sem:\n\tup_write((&EXT4_I(inode)->i_data_sem));\n\tif (retval > 0 && map->m_flags & EXT4_MAP_MAPPED) {\n\t\tret = check_block_validity(inode, map);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * Inodes with freshly allocated blocks where contents will be\n\t\t * visible after transaction commit must be on transaction's\n\t\t * ordered data list.\n\t\t */\n\t\tif (map->m_flags & EXT4_MAP_NEW &&\n\t\t    !(map->m_flags & EXT4_MAP_UNWRITTEN) &&\n\t\t    !(flags & EXT4_GET_BLOCKS_ZERO) &&\n\t\t    !IS_NOQUOTA(inode) &&\n\t\t    ext4_should_order_data(inode)) {\n\t\t\tret = ext4_jbd2_file_inode(handle, inode);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -217,6 +217,21 @@\n \t\tret = check_block_validity(inode, map);\n \t\tif (ret != 0)\n \t\t\treturn ret;\n+\n+\t\t/*\n+\t\t * Inodes with freshly allocated blocks where contents will be\n+\t\t * visible after transaction commit must be on transaction's\n+\t\t * ordered data list.\n+\t\t */\n+\t\tif (map->m_flags & EXT4_MAP_NEW &&\n+\t\t    !(map->m_flags & EXT4_MAP_UNWRITTEN) &&\n+\t\t    !(flags & EXT4_GET_BLOCKS_ZERO) &&\n+\t\t    !IS_NOQUOTA(inode) &&\n+\t\t    ext4_should_order_data(inode)) {\n+\t\t\tret = ext4_jbd2_file_inode(handle, inode);\n+\t\t\tif (ret)\n+\t\t\t\treturn ret;\n+\t\t}\n \t}\n \treturn retval;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\t\t/*",
                "\t\t * Inodes with freshly allocated blocks where contents will be",
                "\t\t * visible after transaction commit must be on transaction's",
                "\t\t * ordered data list.",
                "\t\t */",
                "\t\tif (map->m_flags & EXT4_MAP_NEW &&",
                "\t\t    !(map->m_flags & EXT4_MAP_UNWRITTEN) &&",
                "\t\t    !(flags & EXT4_GET_BLOCKS_ZERO) &&",
                "\t\t    !IS_NOQUOTA(inode) &&",
                "\t\t    ext4_should_order_data(inode)) {",
                "\t\t\tret = ext4_jbd2_file_inode(handle, inode);",
                "\t\t\tif (ret)",
                "\t\t\t\treturn ret;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "fs/ext4/inode.c in the Linux kernel before 4.6.2, when ext4 data=ordered mode is used, mishandles a needs-flushing-before-commit list, which allows local users to obtain sensitive information from other users' files in opportunistic circumstances by waiting for a hardware reset, creating a new file, making write system calls, and reading this file.",
        "id": 1508
    },
    {
        "cve_id": "CVE-2018-20510",
        "code_before_change": "static void print_binder_transaction_ilocked(struct seq_file *m,\n\t\t\t\t\t     struct binder_proc *proc,\n\t\t\t\t\t     const char *prefix,\n\t\t\t\t\t     struct binder_transaction *t)\n{\n\tstruct binder_proc *to_proc;\n\tstruct binder_buffer *buffer = t->buffer;\n\n\tspin_lock(&t->lock);\n\tto_proc = t->to_proc;\n\tseq_printf(m,\n\t\t   \"%s %d: %p from %d:%d to %d:%d code %x flags %x pri %ld r%d\",\n\t\t   prefix, t->debug_id, t,\n\t\t   t->from ? t->from->proc->pid : 0,\n\t\t   t->from ? t->from->pid : 0,\n\t\t   to_proc ? to_proc->pid : 0,\n\t\t   t->to_thread ? t->to_thread->pid : 0,\n\t\t   t->code, t->flags, t->priority, t->need_reply);\n\tspin_unlock(&t->lock);\n\n\tif (proc != to_proc) {\n\t\t/*\n\t\t * Can only safely deref buffer if we are holding the\n\t\t * correct proc inner lock for this node\n\t\t */\n\t\tseq_puts(m, \"\\n\");\n\t\treturn;\n\t}\n\n\tif (buffer == NULL) {\n\t\tseq_puts(m, \" buffer free\\n\");\n\t\treturn;\n\t}\n\tif (buffer->target_node)\n\t\tseq_printf(m, \" node %d\", buffer->target_node->debug_id);\n\tseq_printf(m, \" size %zd:%zd data %p\\n\",\n\t\t   buffer->data_size, buffer->offsets_size,\n\t\t   buffer->data);\n}",
        "code_after_change": "static void print_binder_transaction_ilocked(struct seq_file *m,\n\t\t\t\t\t     struct binder_proc *proc,\n\t\t\t\t\t     const char *prefix,\n\t\t\t\t\t     struct binder_transaction *t)\n{\n\tstruct binder_proc *to_proc;\n\tstruct binder_buffer *buffer = t->buffer;\n\n\tspin_lock(&t->lock);\n\tto_proc = t->to_proc;\n\tseq_printf(m,\n\t\t   \"%s %d: %pK from %d:%d to %d:%d code %x flags %x pri %ld r%d\",\n\t\t   prefix, t->debug_id, t,\n\t\t   t->from ? t->from->proc->pid : 0,\n\t\t   t->from ? t->from->pid : 0,\n\t\t   to_proc ? to_proc->pid : 0,\n\t\t   t->to_thread ? t->to_thread->pid : 0,\n\t\t   t->code, t->flags, t->priority, t->need_reply);\n\tspin_unlock(&t->lock);\n\n\tif (proc != to_proc) {\n\t\t/*\n\t\t * Can only safely deref buffer if we are holding the\n\t\t * correct proc inner lock for this node\n\t\t */\n\t\tseq_puts(m, \"\\n\");\n\t\treturn;\n\t}\n\n\tif (buffer == NULL) {\n\t\tseq_puts(m, \" buffer free\\n\");\n\t\treturn;\n\t}\n\tif (buffer->target_node)\n\t\tseq_printf(m, \" node %d\", buffer->target_node->debug_id);\n\tseq_printf(m, \" size %zd:%zd data %pK\\n\",\n\t\t   buffer->data_size, buffer->offsets_size,\n\t\t   buffer->data);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \tspin_lock(&t->lock);\n \tto_proc = t->to_proc;\n \tseq_printf(m,\n-\t\t   \"%s %d: %p from %d:%d to %d:%d code %x flags %x pri %ld r%d\",\n+\t\t   \"%s %d: %pK from %d:%d to %d:%d code %x flags %x pri %ld r%d\",\n \t\t   prefix, t->debug_id, t,\n \t\t   t->from ? t->from->proc->pid : 0,\n \t\t   t->from ? t->from->pid : 0,\n@@ -33,7 +33,7 @@\n \t}\n \tif (buffer->target_node)\n \t\tseq_printf(m, \" node %d\", buffer->target_node->debug_id);\n-\tseq_printf(m, \" size %zd:%zd data %p\\n\",\n+\tseq_printf(m, \" size %zd:%zd data %pK\\n\",\n \t\t   buffer->data_size, buffer->offsets_size,\n \t\t   buffer->data);\n }",
        "function_modified_lines": {
            "added": [
                "\t\t   \"%s %d: %pK from %d:%d to %d:%d code %x flags %x pri %ld r%d\",",
                "\tseq_printf(m, \" size %zd:%zd data %pK\\n\","
            ],
            "deleted": [
                "\t\t   \"%s %d: %p from %d:%d to %d:%d code %x flags %x pri %ld r%d\",",
                "\tseq_printf(m, \" size %zd:%zd data %p\\n\","
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_transaction_ilocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \"*from *code *flags\" lines in a debugfs file.",
        "id": 1770
    },
    {
        "cve_id": "CVE-2012-4530",
        "code_before_change": "static int load_em86(struct linux_binprm *bprm)\n{\n\tchar *interp, *i_name, *i_arg;\n\tstruct file * file;\n\tint retval;\n\tstruct elfhdr\telf_ex;\n\n\t/* Make sure this is a Linux/Intel ELF executable... */\n\telf_ex = *((struct elfhdr *)bprm->buf);\n\n\tif (memcmp(elf_ex.e_ident, ELFMAG, SELFMAG) != 0)\n\t\treturn  -ENOEXEC;\n\n\t/* First of all, some simple consistency checks */\n\tif ((elf_ex.e_type != ET_EXEC && elf_ex.e_type != ET_DYN) ||\n\t\t(!((elf_ex.e_machine == EM_386) || (elf_ex.e_machine == EM_486))) ||\n\t\t(!bprm->file->f_op || !bprm->file->f_op->mmap)) {\n\t\t\treturn -ENOEXEC;\n\t}\n\n\tbprm->recursion_depth++; /* Well, the bang-shell is implicit... */\n\tallow_write_access(bprm->file);\n\tfput(bprm->file);\n\tbprm->file = NULL;\n\n\t/* Unlike in the script case, we don't have to do any hairy\n\t * parsing to find our interpreter... it's hardcoded!\n\t */\n\tinterp = EM86_INTERP;\n\ti_name = EM86_I_NAME;\n\ti_arg = NULL;\t\t/* We reserve the right to add an arg later */\n\n\t/*\n\t * Splice in (1) the interpreter's name for argv[0]\n\t *           (2) (optional) argument to interpreter\n\t *           (3) filename of emulated file (replace argv[0])\n\t *\n\t * This is done in reverse order, because of how the\n\t * user environment and arguments are stored.\n\t */\n\tremove_arg_zero(bprm);\n\tretval = copy_strings_kernel(1, &bprm->filename, bprm);\n\tif (retval < 0) return retval; \n\tbprm->argc++;\n\tif (i_arg) {\n\t\tretval = copy_strings_kernel(1, &i_arg, bprm);\n\t\tif (retval < 0) return retval; \n\t\tbprm->argc++;\n\t}\n\tretval = copy_strings_kernel(1, &i_name, bprm);\n\tif (retval < 0)\treturn retval;\n\tbprm->argc++;\n\n\t/*\n\t * OK, now restart the process with the interpreter's inode.\n\t * Note that we use open_exec() as the name is now in kernel\n\t * space, and we don't need to copy it.\n\t */\n\tfile = open_exec(interp);\n\tif (IS_ERR(file))\n\t\treturn PTR_ERR(file);\n\n\tbprm->file = file;\n\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\treturn retval;\n\n\treturn search_binary_handler(bprm);\n}",
        "code_after_change": "static int load_em86(struct linux_binprm *bprm)\n{\n\tchar *interp, *i_name, *i_arg;\n\tstruct file * file;\n\tint retval;\n\tstruct elfhdr\telf_ex;\n\n\t/* Make sure this is a Linux/Intel ELF executable... */\n\telf_ex = *((struct elfhdr *)bprm->buf);\n\n\tif (memcmp(elf_ex.e_ident, ELFMAG, SELFMAG) != 0)\n\t\treturn  -ENOEXEC;\n\n\t/* First of all, some simple consistency checks */\n\tif ((elf_ex.e_type != ET_EXEC && elf_ex.e_type != ET_DYN) ||\n\t\t(!((elf_ex.e_machine == EM_386) || (elf_ex.e_machine == EM_486))) ||\n\t\t(!bprm->file->f_op || !bprm->file->f_op->mmap)) {\n\t\t\treturn -ENOEXEC;\n\t}\n\n\tallow_write_access(bprm->file);\n\tfput(bprm->file);\n\tbprm->file = NULL;\n\n\t/* Unlike in the script case, we don't have to do any hairy\n\t * parsing to find our interpreter... it's hardcoded!\n\t */\n\tinterp = EM86_INTERP;\n\ti_name = EM86_I_NAME;\n\ti_arg = NULL;\t\t/* We reserve the right to add an arg later */\n\n\t/*\n\t * Splice in (1) the interpreter's name for argv[0]\n\t *           (2) (optional) argument to interpreter\n\t *           (3) filename of emulated file (replace argv[0])\n\t *\n\t * This is done in reverse order, because of how the\n\t * user environment and arguments are stored.\n\t */\n\tremove_arg_zero(bprm);\n\tretval = copy_strings_kernel(1, &bprm->filename, bprm);\n\tif (retval < 0) return retval; \n\tbprm->argc++;\n\tif (i_arg) {\n\t\tretval = copy_strings_kernel(1, &i_arg, bprm);\n\t\tif (retval < 0) return retval; \n\t\tbprm->argc++;\n\t}\n\tretval = copy_strings_kernel(1, &i_name, bprm);\n\tif (retval < 0)\treturn retval;\n\tbprm->argc++;\n\n\t/*\n\t * OK, now restart the process with the interpreter's inode.\n\t * Note that we use open_exec() as the name is now in kernel\n\t * space, and we don't need to copy it.\n\t */\n\tfile = open_exec(interp);\n\tif (IS_ERR(file))\n\t\treturn PTR_ERR(file);\n\n\tbprm->file = file;\n\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\treturn retval;\n\n\treturn search_binary_handler(bprm);\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,6 @@\n \t\t\treturn -ENOEXEC;\n \t}\n \n-\tbprm->recursion_depth++; /* Well, the bang-shell is implicit... */\n \tallow_write_access(bprm->file);\n \tfput(bprm->file);\n \tbprm->file = NULL;",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tbprm->recursion_depth++; /* Well, the bang-shell is implicit... */"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The load_script function in fs/binfmt_script.c in the Linux kernel before 3.7.2 does not properly handle recursion, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 110
    },
    {
        "cve_id": "CVE-2016-4580",
        "code_before_change": "int x25_negotiate_facilities(struct sk_buff *skb, struct sock *sk,\n\t\tstruct x25_facilities *new, struct x25_dte_facilities *dte)\n{\n\tstruct x25_sock *x25 = x25_sk(sk);\n\tstruct x25_facilities *ours = &x25->facilities;\n\tstruct x25_facilities theirs;\n\tint len;\n\n\tmemset(&theirs, 0, sizeof(theirs));\n\tmemcpy(new, ours, sizeof(*new));\n\n\tlen = x25_parse_facilities(skb, &theirs, dte, &x25->vc_facil_mask);\n\tif (len < 0)\n\t\treturn len;\n\n\t/*\n\t *\tThey want reverse charging, we won't accept it.\n\t */\n\tif ((theirs.reverse & 0x01 ) && (ours->reverse & 0x01)) {\n\t\tSOCK_DEBUG(sk, \"X.25: rejecting reverse charging request\\n\");\n\t\treturn -1;\n\t}\n\n\tnew->reverse = theirs.reverse;\n\n\tif (theirs.throughput) {\n\t\tint theirs_in =  theirs.throughput & 0x0f;\n\t\tint theirs_out = theirs.throughput & 0xf0;\n\t\tint ours_in  = ours->throughput & 0x0f;\n\t\tint ours_out = ours->throughput & 0xf0;\n\t\tif (!ours_in || theirs_in < ours_in) {\n\t\t\tSOCK_DEBUG(sk, \"X.25: inbound throughput negotiated\\n\");\n\t\t\tnew->throughput = (new->throughput & 0xf0) | theirs_in;\n\t\t}\n\t\tif (!ours_out || theirs_out < ours_out) {\n\t\t\tSOCK_DEBUG(sk,\n\t\t\t\t\"X.25: outbound throughput negotiated\\n\");\n\t\t\tnew->throughput = (new->throughput & 0x0f) | theirs_out;\n\t\t}\n\t}\n\n\tif (theirs.pacsize_in && theirs.pacsize_out) {\n\t\tif (theirs.pacsize_in < ours->pacsize_in) {\n\t\t\tSOCK_DEBUG(sk, \"X.25: packet size inwards negotiated down\\n\");\n\t\t\tnew->pacsize_in = theirs.pacsize_in;\n\t\t}\n\t\tif (theirs.pacsize_out < ours->pacsize_out) {\n\t\t\tSOCK_DEBUG(sk, \"X.25: packet size outwards negotiated down\\n\");\n\t\t\tnew->pacsize_out = theirs.pacsize_out;\n\t\t}\n\t}\n\n\tif (theirs.winsize_in && theirs.winsize_out) {\n\t\tif (theirs.winsize_in < ours->winsize_in) {\n\t\t\tSOCK_DEBUG(sk, \"X.25: window size inwards negotiated down\\n\");\n\t\t\tnew->winsize_in = theirs.winsize_in;\n\t\t}\n\t\tif (theirs.winsize_out < ours->winsize_out) {\n\t\t\tSOCK_DEBUG(sk, \"X.25: window size outwards negotiated down\\n\");\n\t\t\tnew->winsize_out = theirs.winsize_out;\n\t\t}\n\t}\n\n\treturn len;\n}",
        "code_after_change": "int x25_negotiate_facilities(struct sk_buff *skb, struct sock *sk,\n\t\tstruct x25_facilities *new, struct x25_dte_facilities *dte)\n{\n\tstruct x25_sock *x25 = x25_sk(sk);\n\tstruct x25_facilities *ours = &x25->facilities;\n\tstruct x25_facilities theirs;\n\tint len;\n\n\tmemset(&theirs, 0, sizeof(theirs));\n\tmemcpy(new, ours, sizeof(*new));\n\tmemset(dte, 0, sizeof(*dte));\n\n\tlen = x25_parse_facilities(skb, &theirs, dte, &x25->vc_facil_mask);\n\tif (len < 0)\n\t\treturn len;\n\n\t/*\n\t *\tThey want reverse charging, we won't accept it.\n\t */\n\tif ((theirs.reverse & 0x01 ) && (ours->reverse & 0x01)) {\n\t\tSOCK_DEBUG(sk, \"X.25: rejecting reverse charging request\\n\");\n\t\treturn -1;\n\t}\n\n\tnew->reverse = theirs.reverse;\n\n\tif (theirs.throughput) {\n\t\tint theirs_in =  theirs.throughput & 0x0f;\n\t\tint theirs_out = theirs.throughput & 0xf0;\n\t\tint ours_in  = ours->throughput & 0x0f;\n\t\tint ours_out = ours->throughput & 0xf0;\n\t\tif (!ours_in || theirs_in < ours_in) {\n\t\t\tSOCK_DEBUG(sk, \"X.25: inbound throughput negotiated\\n\");\n\t\t\tnew->throughput = (new->throughput & 0xf0) | theirs_in;\n\t\t}\n\t\tif (!ours_out || theirs_out < ours_out) {\n\t\t\tSOCK_DEBUG(sk,\n\t\t\t\t\"X.25: outbound throughput negotiated\\n\");\n\t\t\tnew->throughput = (new->throughput & 0x0f) | theirs_out;\n\t\t}\n\t}\n\n\tif (theirs.pacsize_in && theirs.pacsize_out) {\n\t\tif (theirs.pacsize_in < ours->pacsize_in) {\n\t\t\tSOCK_DEBUG(sk, \"X.25: packet size inwards negotiated down\\n\");\n\t\t\tnew->pacsize_in = theirs.pacsize_in;\n\t\t}\n\t\tif (theirs.pacsize_out < ours->pacsize_out) {\n\t\t\tSOCK_DEBUG(sk, \"X.25: packet size outwards negotiated down\\n\");\n\t\t\tnew->pacsize_out = theirs.pacsize_out;\n\t\t}\n\t}\n\n\tif (theirs.winsize_in && theirs.winsize_out) {\n\t\tif (theirs.winsize_in < ours->winsize_in) {\n\t\t\tSOCK_DEBUG(sk, \"X.25: window size inwards negotiated down\\n\");\n\t\t\tnew->winsize_in = theirs.winsize_in;\n\t\t}\n\t\tif (theirs.winsize_out < ours->winsize_out) {\n\t\t\tSOCK_DEBUG(sk, \"X.25: window size outwards negotiated down\\n\");\n\t\t\tnew->winsize_out = theirs.winsize_out;\n\t\t}\n\t}\n\n\treturn len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,7 @@\n \n \tmemset(&theirs, 0, sizeof(theirs));\n \tmemcpy(new, ours, sizeof(*new));\n+\tmemset(dte, 0, sizeof(*dte));\n \n \tlen = x25_parse_facilities(skb, &theirs, dte, &x25->vc_facil_mask);\n \tif (len < 0)",
        "function_modified_lines": {
            "added": [
                "\tmemset(dte, 0, sizeof(*dte));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The x25_negotiate_facilities function in net/x25/x25_facilities.c in the Linux kernel before 4.5.5 does not properly initialize a certain data structure, which allows attackers to obtain sensitive information from kernel stack memory via an X.25 Call Request.",
        "id": 1029
    },
    {
        "cve_id": "CVE-2018-18710",
        "code_before_change": "static int cdrom_ioctl_select_disc(struct cdrom_device_info *cdi,\n\t\tunsigned long arg)\n{\n\tcd_dbg(CD_DO_IOCTL, \"entering CDROM_SELECT_DISC\\n\");\n\n\tif (!CDROM_CAN(CDC_SELECT_DISC))\n\t\treturn -ENOSYS;\n\n\tif (arg != CDSL_CURRENT && arg != CDSL_NONE) {\n\t\tif ((int)arg >= cdi->capacity)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * ->select_disc is a hook to allow a driver-specific way of\n\t * seleting disc.  However, since there is no equivalent hook for\n\t * cdrom_slot_status this may not actually be useful...\n\t */\n\tif (cdi->ops->select_disc)\n\t\treturn cdi->ops->select_disc(cdi, arg);\n\n\tcd_dbg(CD_CHANGER, \"Using generic cdrom_select_disc()\\n\");\n\treturn cdrom_select_disc(cdi, arg);\n}",
        "code_after_change": "static int cdrom_ioctl_select_disc(struct cdrom_device_info *cdi,\n\t\tunsigned long arg)\n{\n\tcd_dbg(CD_DO_IOCTL, \"entering CDROM_SELECT_DISC\\n\");\n\n\tif (!CDROM_CAN(CDC_SELECT_DISC))\n\t\treturn -ENOSYS;\n\n\tif (arg != CDSL_CURRENT && arg != CDSL_NONE) {\n\t\tif (arg >= cdi->capacity)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * ->select_disc is a hook to allow a driver-specific way of\n\t * seleting disc.  However, since there is no equivalent hook for\n\t * cdrom_slot_status this may not actually be useful...\n\t */\n\tif (cdi->ops->select_disc)\n\t\treturn cdi->ops->select_disc(cdi, arg);\n\n\tcd_dbg(CD_CHANGER, \"Using generic cdrom_select_disc()\\n\");\n\treturn cdrom_select_disc(cdi, arg);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \t\treturn -ENOSYS;\n \n \tif (arg != CDSL_CURRENT && arg != CDSL_NONE) {\n-\t\tif ((int)arg >= cdi->capacity)\n+\t\tif (arg >= cdi->capacity)\n \t\t\treturn -EINVAL;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tif (arg >= cdi->capacity)"
            ],
            "deleted": [
                "\t\tif ((int)arg >= cdi->capacity)"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in the Linux kernel through 4.19. An information leak in cdrom_ioctl_select_disc in drivers/cdrom/cdrom.c could be used by local attackers to read kernel memory because a cast from unsigned long to int interferes with bounds checking. This is similar to CVE-2018-10940 and CVE-2018-16658.",
        "id": 1741
    },
    {
        "cve_id": "CVE-2012-6539",
        "code_before_change": "static int dev_ifconf(struct net *net, struct compat_ifconf __user *uifc32)\n{\n\tstruct compat_ifconf ifc32;\n\tstruct ifconf ifc;\n\tstruct ifconf __user *uifc;\n\tstruct compat_ifreq __user *ifr32;\n\tstruct ifreq __user *ifr;\n\tunsigned int i, j;\n\tint err;\n\n\tif (copy_from_user(&ifc32, uifc32, sizeof(struct compat_ifconf)))\n\t\treturn -EFAULT;\n\n\tif (ifc32.ifcbuf == 0) {\n\t\tifc32.ifc_len = 0;\n\t\tifc.ifc_len = 0;\n\t\tifc.ifc_req = NULL;\n\t\tuifc = compat_alloc_user_space(sizeof(struct ifconf));\n\t} else {\n\t\tsize_t len = ((ifc32.ifc_len / sizeof(struct compat_ifreq)) + 1) *\n\t\t\tsizeof(struct ifreq);\n\t\tuifc = compat_alloc_user_space(sizeof(struct ifconf) + len);\n\t\tifc.ifc_len = len;\n\t\tifr = ifc.ifc_req = (void __user *)(uifc + 1);\n\t\tifr32 = compat_ptr(ifc32.ifcbuf);\n\t\tfor (i = 0; i < ifc32.ifc_len; i += sizeof(struct compat_ifreq)) {\n\t\t\tif (copy_in_user(ifr, ifr32, sizeof(struct compat_ifreq)))\n\t\t\t\treturn -EFAULT;\n\t\t\tifr++;\n\t\t\tifr32++;\n\t\t}\n\t}\n\tif (copy_to_user(uifc, &ifc, sizeof(struct ifconf)))\n\t\treturn -EFAULT;\n\n\terr = dev_ioctl(net, SIOCGIFCONF, uifc);\n\tif (err)\n\t\treturn err;\n\n\tif (copy_from_user(&ifc, uifc, sizeof(struct ifconf)))\n\t\treturn -EFAULT;\n\n\tifr = ifc.ifc_req;\n\tifr32 = compat_ptr(ifc32.ifcbuf);\n\tfor (i = 0, j = 0;\n\t     i + sizeof(struct compat_ifreq) <= ifc32.ifc_len && j < ifc.ifc_len;\n\t     i += sizeof(struct compat_ifreq), j += sizeof(struct ifreq)) {\n\t\tif (copy_in_user(ifr32, ifr, sizeof(struct compat_ifreq)))\n\t\t\treturn -EFAULT;\n\t\tifr32++;\n\t\tifr++;\n\t}\n\n\tif (ifc32.ifcbuf == 0) {\n\t\t/* Translate from 64-bit structure multiple to\n\t\t * a 32-bit one.\n\t\t */\n\t\ti = ifc.ifc_len;\n\t\ti = ((i / sizeof(struct ifreq)) * sizeof(struct compat_ifreq));\n\t\tifc32.ifc_len = i;\n\t} else {\n\t\tifc32.ifc_len = i;\n\t}\n\tif (copy_to_user(uifc32, &ifc32, sizeof(struct compat_ifconf)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
        "code_after_change": "static int dev_ifconf(struct net *net, struct compat_ifconf __user *uifc32)\n{\n\tstruct compat_ifconf ifc32;\n\tstruct ifconf ifc;\n\tstruct ifconf __user *uifc;\n\tstruct compat_ifreq __user *ifr32;\n\tstruct ifreq __user *ifr;\n\tunsigned int i, j;\n\tint err;\n\n\tif (copy_from_user(&ifc32, uifc32, sizeof(struct compat_ifconf)))\n\t\treturn -EFAULT;\n\n\tmemset(&ifc, 0, sizeof(ifc));\n\tif (ifc32.ifcbuf == 0) {\n\t\tifc32.ifc_len = 0;\n\t\tifc.ifc_len = 0;\n\t\tifc.ifc_req = NULL;\n\t\tuifc = compat_alloc_user_space(sizeof(struct ifconf));\n\t} else {\n\t\tsize_t len = ((ifc32.ifc_len / sizeof(struct compat_ifreq)) + 1) *\n\t\t\tsizeof(struct ifreq);\n\t\tuifc = compat_alloc_user_space(sizeof(struct ifconf) + len);\n\t\tifc.ifc_len = len;\n\t\tifr = ifc.ifc_req = (void __user *)(uifc + 1);\n\t\tifr32 = compat_ptr(ifc32.ifcbuf);\n\t\tfor (i = 0; i < ifc32.ifc_len; i += sizeof(struct compat_ifreq)) {\n\t\t\tif (copy_in_user(ifr, ifr32, sizeof(struct compat_ifreq)))\n\t\t\t\treturn -EFAULT;\n\t\t\tifr++;\n\t\t\tifr32++;\n\t\t}\n\t}\n\tif (copy_to_user(uifc, &ifc, sizeof(struct ifconf)))\n\t\treturn -EFAULT;\n\n\terr = dev_ioctl(net, SIOCGIFCONF, uifc);\n\tif (err)\n\t\treturn err;\n\n\tif (copy_from_user(&ifc, uifc, sizeof(struct ifconf)))\n\t\treturn -EFAULT;\n\n\tifr = ifc.ifc_req;\n\tifr32 = compat_ptr(ifc32.ifcbuf);\n\tfor (i = 0, j = 0;\n\t     i + sizeof(struct compat_ifreq) <= ifc32.ifc_len && j < ifc.ifc_len;\n\t     i += sizeof(struct compat_ifreq), j += sizeof(struct ifreq)) {\n\t\tif (copy_in_user(ifr32, ifr, sizeof(struct compat_ifreq)))\n\t\t\treturn -EFAULT;\n\t\tifr32++;\n\t\tifr++;\n\t}\n\n\tif (ifc32.ifcbuf == 0) {\n\t\t/* Translate from 64-bit structure multiple to\n\t\t * a 32-bit one.\n\t\t */\n\t\ti = ifc.ifc_len;\n\t\ti = ((i / sizeof(struct ifreq)) * sizeof(struct compat_ifreq));\n\t\tifc32.ifc_len = i;\n\t} else {\n\t\tifc32.ifc_len = i;\n\t}\n\tif (copy_to_user(uifc32, &ifc32, sizeof(struct compat_ifconf)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,7 @@\n \tif (copy_from_user(&ifc32, uifc32, sizeof(struct compat_ifconf)))\n \t\treturn -EFAULT;\n \n+\tmemset(&ifc, 0, sizeof(ifc));\n \tif (ifc32.ifcbuf == 0) {\n \t\tifc32.ifc_len = 0;\n \t\tifc.ifc_len = 0;",
        "function_modified_lines": {
            "added": [
                "\tmemset(&ifc, 0, sizeof(ifc));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The dev_ifconf function in net/socket.c in the Linux kernel before 3.6 does not initialize a certain structure, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 123
    },
    {
        "cve_id": "CVE-2013-3230",
        "code_before_change": "static int l2tp_ip6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t\t    struct msghdr *msg, size_t len, int noblock,\n\t\t\t    int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *lsa = (struct sockaddr_l2tpip6 *)msg->msg_name;\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*lsa);\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (lsa) {\n\t\tlsa->l2tp_family = AF_INET6;\n\t\tlsa->l2tp_unused = 0;\n\t\tlsa->l2tp_addr = ipv6_hdr(skb)->saddr;\n\t\tlsa->l2tp_flowinfo = 0;\n\t\tlsa->l2tp_scope_id = 0;\n\t\tif (ipv6_addr_type(&lsa->l2tp_addr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tlsa->l2tp_scope_id = IP6CB(skb)->iif;\n\t}\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}",
        "code_after_change": "static int l2tp_ip6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t\t    struct msghdr *msg, size_t len, int noblock,\n\t\t\t    int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *lsa = (struct sockaddr_l2tpip6 *)msg->msg_name;\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*lsa);\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (lsa) {\n\t\tlsa->l2tp_family = AF_INET6;\n\t\tlsa->l2tp_unused = 0;\n\t\tlsa->l2tp_addr = ipv6_hdr(skb)->saddr;\n\t\tlsa->l2tp_flowinfo = 0;\n\t\tlsa->l2tp_scope_id = 0;\n\t\tlsa->l2tp_conn_id = 0;\n\t\tif (ipv6_addr_type(&lsa->l2tp_addr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tlsa->l2tp_scope_id = IP6CB(skb)->iif;\n\t}\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -40,6 +40,7 @@\n \t\tlsa->l2tp_addr = ipv6_hdr(skb)->saddr;\n \t\tlsa->l2tp_flowinfo = 0;\n \t\tlsa->l2tp_scope_id = 0;\n+\t\tlsa->l2tp_conn_id = 0;\n \t\tif (ipv6_addr_type(&lsa->l2tp_addr) & IPV6_ADDR_LINKLOCAL)\n \t\t\tlsa->l2tp_scope_id = IP6CB(skb)->iif;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\tlsa->l2tp_conn_id = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The l2tp_ip6_recvmsg function in net/l2tp/l2tp_ip6.c in the Linux kernel before 3.9-rc7 does not initialize a certain structure member, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 270
    },
    {
        "cve_id": "CVE-2017-9150",
        "code_before_change": "static int do_check(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state = &env->cur_state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs = state->regs;\n\tint insn_cnt = env->prog->len;\n\tint insn_idx, prev_insn_idx = 0;\n\tint insn_processed = 0;\n\tbool do_print_state = false;\n\n\tinit_reg_state(regs);\n\tinsn_idx = 0;\n\tenv->varlen_map_value_access = false;\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tif (insn_idx >= insn_cnt) {\n\t\t\tverbose(\"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tinsn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tinsn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (log_level) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(\"\\nfrom %d to %d: safe\\n\",\n\t\t\t\t\t\tprev_insn_idx, insn_idx);\n\t\t\t\telse\n\t\t\t\t\tverbose(\"%d: safe\\n\", insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (log_level && do_print_state) {\n\t\t\tverbose(\"\\nfrom %d to %d:\", prev_insn_idx, insn_idx);\n\t\t\tprint_verifier_state(&env->cur_state);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (log_level) {\n\t\t\tverbose(\"%d: \", insn_idx);\n\t\t\tprint_bpf_insn(insn);\n\t\t}\n\n\t\terr = ext_analyzer_insn_hook(env, insn_idx, prev_insn_idx);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(regs, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(regs, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, insn->src_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_READ,\n\t\t\t\t\t       insn->dst_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tif (BPF_SIZE(insn->code) != BPF_W &&\n\t\t\t    BPF_SIZE(insn->code) != BPF_DW) {\n\t\t\t\tinsn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tprev_src_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (src_reg_type != *prev_src_type &&\n\t\t\t\t   (src_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_src_type == PTR_TO_CTX)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(\"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_XADD) {\n\t\t\t\terr = check_xadd(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tinsn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(regs, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(regs, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       insn->src_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (dst_reg_type != *prev_dst_type &&\n\t\t\t\t   (dst_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_dst_type == PTR_TO_CTX)) {\n\t\t\t\tverbose(\"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(\"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(regs, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       -1);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->off != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(\"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\terr = check_call(env, insn->imm, insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(\"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tinsn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(\"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\t/* eBPF calling convetion is such that R0 is used\n\t\t\t\t * to return the value from eBPF program.\n\t\t\t\t * Make sure that it's readable at this time\n\t\t\t\t * of bpf_exit, which means that program wrote\n\t\t\t\t * something into it earlier\n\t\t\t\t */\n\t\t\t\terr = check_reg_arg(regs, BPF_REG_0, SRC_OP);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\t\t\t\tverbose(\"R0 leaks addr as return value\\n\");\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\nprocess_bpf_exit:\n\t\t\t\tinsn_idx = pop_stack(env, &prev_insn_idx);\n\t\t\t\tif (insn_idx < 0) {\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tinsn_idx++;\n\t\t\t} else {\n\t\t\t\tverbose(\"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\treset_reg_range_values(regs, insn->dst_reg);\n\t\t} else {\n\t\t\tverbose(\"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinsn_idx++;\n\t}\n\n\tverbose(\"processed %d insns\\n\", insn_processed);\n\treturn 0;\n}",
        "code_after_change": "static int do_check(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state = &env->cur_state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs = state->regs;\n\tint insn_cnt = env->prog->len;\n\tint insn_idx, prev_insn_idx = 0;\n\tint insn_processed = 0;\n\tbool do_print_state = false;\n\n\tinit_reg_state(regs);\n\tinsn_idx = 0;\n\tenv->varlen_map_value_access = false;\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tif (insn_idx >= insn_cnt) {\n\t\t\tverbose(\"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tinsn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tinsn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (log_level) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(\"\\nfrom %d to %d: safe\\n\",\n\t\t\t\t\t\tprev_insn_idx, insn_idx);\n\t\t\t\telse\n\t\t\t\t\tverbose(\"%d: safe\\n\", insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (log_level && do_print_state) {\n\t\t\tverbose(\"\\nfrom %d to %d:\", prev_insn_idx, insn_idx);\n\t\t\tprint_verifier_state(&env->cur_state);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (log_level) {\n\t\t\tverbose(\"%d: \", insn_idx);\n\t\t\tprint_bpf_insn(env, insn);\n\t\t}\n\n\t\terr = ext_analyzer_insn_hook(env, insn_idx, prev_insn_idx);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(regs, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(regs, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, insn->src_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_READ,\n\t\t\t\t\t       insn->dst_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tif (BPF_SIZE(insn->code) != BPF_W &&\n\t\t\t    BPF_SIZE(insn->code) != BPF_DW) {\n\t\t\t\tinsn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tprev_src_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (src_reg_type != *prev_src_type &&\n\t\t\t\t   (src_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_src_type == PTR_TO_CTX)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(\"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_XADD) {\n\t\t\t\terr = check_xadd(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tinsn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(regs, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(regs, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       insn->src_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (dst_reg_type != *prev_dst_type &&\n\t\t\t\t   (dst_reg_type == PTR_TO_CTX ||\n\t\t\t\t    *prev_dst_type == PTR_TO_CTX)) {\n\t\t\t\tverbose(\"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(\"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(regs, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, insn->dst_reg, insn->off,\n\t\t\t\t\t       BPF_SIZE(insn->code), BPF_WRITE,\n\t\t\t\t\t       -1);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->off != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(\"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\terr = check_call(env, insn->imm, insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(\"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tinsn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(\"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\t/* eBPF calling convetion is such that R0 is used\n\t\t\t\t * to return the value from eBPF program.\n\t\t\t\t * Make sure that it's readable at this time\n\t\t\t\t * of bpf_exit, which means that program wrote\n\t\t\t\t * something into it earlier\n\t\t\t\t */\n\t\t\t\terr = check_reg_arg(regs, BPF_REG_0, SRC_OP);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\t\t\t\tverbose(\"R0 leaks addr as return value\\n\");\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\nprocess_bpf_exit:\n\t\t\t\tinsn_idx = pop_stack(env, &prev_insn_idx);\n\t\t\t\tif (insn_idx < 0) {\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tinsn_idx++;\n\t\t\t} else {\n\t\t\t\tverbose(\"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\treset_reg_range_values(regs, insn->dst_reg);\n\t\t} else {\n\t\t\tverbose(\"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinsn_idx++;\n\t}\n\n\tverbose(\"processed %d insns\\n\", insn_processed);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -54,7 +54,7 @@\n \n \t\tif (log_level) {\n \t\t\tverbose(\"%d: \", insn_idx);\n-\t\t\tprint_bpf_insn(insn);\n+\t\t\tprint_bpf_insn(env, insn);\n \t\t}\n \n \t\terr = ext_analyzer_insn_hook(env, insn_idx, prev_insn_idx);",
        "function_modified_lines": {
            "added": [
                "\t\t\tprint_bpf_insn(env, insn);"
            ],
            "deleted": [
                "\t\t\tprint_bpf_insn(insn);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The do_check function in kernel/bpf/verifier.c in the Linux kernel before 4.11.1 does not make the allow_ptr_leaks value available for restricting the output of the print_bpf_insn function, which allows local users to obtain sensitive address information via crafted bpf system calls.",
        "id": 1567
    },
    {
        "cve_id": "CVE-2013-3225",
        "code_before_change": "static int rfcomm_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rfcomm_dlc *d = rfcomm_pi(sk)->dlc;\n\tint len;\n\n\tif (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {\n\t\trfcomm_dlc_accept(d);\n\t\treturn 0;\n\t}\n\n\tlen = bt_sock_stream_recvmsg(iocb, sock, msg, size, flags);\n\n\tlock_sock(sk);\n\tif (!(flags & MSG_PEEK) && len > 0)\n\t\tatomic_sub(len, &sk->sk_rmem_alloc);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= (sk->sk_rcvbuf >> 2))\n\t\trfcomm_dlc_unthrottle(rfcomm_pi(sk)->dlc);\n\trelease_sock(sk);\n\n\treturn len;\n}",
        "code_after_change": "static int rfcomm_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rfcomm_dlc *d = rfcomm_pi(sk)->dlc;\n\tint len;\n\n\tif (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {\n\t\trfcomm_dlc_accept(d);\n\t\tmsg->msg_namelen = 0;\n\t\treturn 0;\n\t}\n\n\tlen = bt_sock_stream_recvmsg(iocb, sock, msg, size, flags);\n\n\tlock_sock(sk);\n\tif (!(flags & MSG_PEEK) && len > 0)\n\t\tatomic_sub(len, &sk->sk_rmem_alloc);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= (sk->sk_rcvbuf >> 2))\n\t\trfcomm_dlc_unthrottle(rfcomm_pi(sk)->dlc);\n\trelease_sock(sk);\n\n\treturn len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \n \tif (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {\n \t\trfcomm_dlc_accept(d);\n+\t\tmsg->msg_namelen = 0;\n \t\treturn 0;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tmsg->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The rfcomm_sock_recvmsg function in net/bluetooth/rfcomm/sock.c in the Linux kernel before 3.9-rc7 does not initialize a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 265
    },
    {
        "cve_id": "CVE-2018-5750",
        "code_before_change": "static int acpi_smbus_hc_add(struct acpi_device *device)\n{\n\tint status;\n\tunsigned long long val;\n\tstruct acpi_smb_hc *hc;\n\n\tif (!device)\n\t\treturn -EINVAL;\n\n\tstatus = acpi_evaluate_integer(device->handle, \"_EC\", NULL, &val);\n\tif (ACPI_FAILURE(status)) {\n\t\tprintk(KERN_ERR PREFIX \"error obtaining _EC.\\n\");\n\t\treturn -EIO;\n\t}\n\n\tstrcpy(acpi_device_name(device), ACPI_SMB_HC_DEVICE_NAME);\n\tstrcpy(acpi_device_class(device), ACPI_SMB_HC_CLASS);\n\n\thc = kzalloc(sizeof(struct acpi_smb_hc), GFP_KERNEL);\n\tif (!hc)\n\t\treturn -ENOMEM;\n\tmutex_init(&hc->lock);\n\tinit_waitqueue_head(&hc->wait);\n\n\thc->ec = acpi_driver_data(device->parent);\n\thc->offset = (val >> 8) & 0xff;\n\thc->query_bit = val & 0xff;\n\tdevice->driver_data = hc;\n\n\tacpi_ec_add_query_handler(hc->ec, hc->query_bit, NULL, smbus_alarm, hc);\n\tprintk(KERN_INFO PREFIX \"SBS HC: EC = 0x%p, offset = 0x%0x, query_bit = 0x%0x\\n\",\n\t\thc->ec, hc->offset, hc->query_bit);\n\n\treturn 0;\n}",
        "code_after_change": "static int acpi_smbus_hc_add(struct acpi_device *device)\n{\n\tint status;\n\tunsigned long long val;\n\tstruct acpi_smb_hc *hc;\n\n\tif (!device)\n\t\treturn -EINVAL;\n\n\tstatus = acpi_evaluate_integer(device->handle, \"_EC\", NULL, &val);\n\tif (ACPI_FAILURE(status)) {\n\t\tprintk(KERN_ERR PREFIX \"error obtaining _EC.\\n\");\n\t\treturn -EIO;\n\t}\n\n\tstrcpy(acpi_device_name(device), ACPI_SMB_HC_DEVICE_NAME);\n\tstrcpy(acpi_device_class(device), ACPI_SMB_HC_CLASS);\n\n\thc = kzalloc(sizeof(struct acpi_smb_hc), GFP_KERNEL);\n\tif (!hc)\n\t\treturn -ENOMEM;\n\tmutex_init(&hc->lock);\n\tinit_waitqueue_head(&hc->wait);\n\n\thc->ec = acpi_driver_data(device->parent);\n\thc->offset = (val >> 8) & 0xff;\n\thc->query_bit = val & 0xff;\n\tdevice->driver_data = hc;\n\n\tacpi_ec_add_query_handler(hc->ec, hc->query_bit, NULL, smbus_alarm, hc);\n\tdev_info(&device->dev, \"SBS HC: offset = 0x%0x, query_bit = 0x%0x\\n\",\n\t\t hc->offset, hc->query_bit);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,8 +28,8 @@\n \tdevice->driver_data = hc;\n \n \tacpi_ec_add_query_handler(hc->ec, hc->query_bit, NULL, smbus_alarm, hc);\n-\tprintk(KERN_INFO PREFIX \"SBS HC: EC = 0x%p, offset = 0x%0x, query_bit = 0x%0x\\n\",\n-\t\thc->ec, hc->offset, hc->query_bit);\n+\tdev_info(&device->dev, \"SBS HC: offset = 0x%0x, query_bit = 0x%0x\\n\",\n+\t\t hc->offset, hc->query_bit);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tdev_info(&device->dev, \"SBS HC: offset = 0x%0x, query_bit = 0x%0x\\n\",",
                "\t\t hc->offset, hc->query_bit);"
            ],
            "deleted": [
                "\tprintk(KERN_INFO PREFIX \"SBS HC: EC = 0x%p, offset = 0x%0x, query_bit = 0x%0x\\n\",",
                "\t\thc->ec, hc->offset, hc->query_bit);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The acpi_smbus_hc_add function in drivers/acpi/sbshc.c in the Linux kernel through 4.14.15 allows local users to obtain sensitive address information by reading dmesg data from an SBS HC printk call.",
        "id": 1827
    },
    {
        "cve_id": "CVE-2017-5550",
        "code_before_change": "void iov_iter_pipe(struct iov_iter *i, int direction,\n\t\t\tstruct pipe_inode_info *pipe,\n\t\t\tsize_t count)\n{\n\tBUG_ON(direction != ITER_PIPE);\n\ti->type = direction;\n\ti->pipe = pipe;\n\ti->idx = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\ti->iov_offset = 0;\n\ti->count = count;\n}",
        "code_after_change": "void iov_iter_pipe(struct iov_iter *i, int direction,\n\t\t\tstruct pipe_inode_info *pipe,\n\t\t\tsize_t count)\n{\n\tBUG_ON(direction != ITER_PIPE);\n\tWARN_ON(pipe->nrbufs == pipe->buffers);\n\ti->type = direction;\n\ti->pipe = pipe;\n\ti->idx = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\ti->iov_offset = 0;\n\ti->count = count;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,6 +3,7 @@\n \t\t\tsize_t count)\n {\n \tBUG_ON(direction != ITER_PIPE);\n+\tWARN_ON(pipe->nrbufs == pipe->buffers);\n \ti->type = direction;\n \ti->pipe = pipe;\n \ti->idx = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);",
        "function_modified_lines": {
            "added": [
                "\tWARN_ON(pipe->nrbufs == pipe->buffers);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Off-by-one error in the pipe_advance function in lib/iov_iter.c in the Linux kernel before 4.9.5 allows local users to obtain sensitive information from uninitialized heap-memory locations in opportunistic circumstances by reading from a pipe after an incorrect buffer-release decision.",
        "id": 1468
    },
    {
        "cve_id": "CVE-2012-4530",
        "code_before_change": "static int load_script(struct linux_binprm *bprm)\n{\n\tconst char *i_arg, *i_name;\n\tchar *cp;\n\tstruct file *file;\n\tchar interp[BINPRM_BUF_SIZE];\n\tint retval;\n\n\tif ((bprm->buf[0] != '#') || (bprm->buf[1] != '!') ||\n\t    (bprm->recursion_depth > BINPRM_MAX_RECURSION))\n\t\treturn -ENOEXEC;\n\t/*\n\t * This section does the #! interpretation.\n\t * Sorta complicated, but hopefully it will work.  -TYT\n\t */\n\n\tbprm->recursion_depth++;\n\tallow_write_access(bprm->file);\n\tfput(bprm->file);\n\tbprm->file = NULL;\n\n\tbprm->buf[BINPRM_BUF_SIZE - 1] = '\\0';\n\tif ((cp = strchr(bprm->buf, '\\n')) == NULL)\n\t\tcp = bprm->buf+BINPRM_BUF_SIZE-1;\n\t*cp = '\\0';\n\twhile (cp > bprm->buf) {\n\t\tcp--;\n\t\tif ((*cp == ' ') || (*cp == '\\t'))\n\t\t\t*cp = '\\0';\n\t\telse\n\t\t\tbreak;\n\t}\n\tfor (cp = bprm->buf+2; (*cp == ' ') || (*cp == '\\t'); cp++);\n\tif (*cp == '\\0') \n\t\treturn -ENOEXEC; /* No interpreter name found */\n\ti_name = cp;\n\ti_arg = NULL;\n\tfor ( ; *cp && (*cp != ' ') && (*cp != '\\t'); cp++)\n\t\t/* nothing */ ;\n\twhile ((*cp == ' ') || (*cp == '\\t'))\n\t\t*cp++ = '\\0';\n\tif (*cp)\n\t\ti_arg = cp;\n\tstrcpy (interp, i_name);\n\t/*\n\t * OK, we've parsed out the interpreter name and\n\t * (optional) argument.\n\t * Splice in (1) the interpreter's name for argv[0]\n\t *           (2) (optional) argument to interpreter\n\t *           (3) filename of shell script (replace argv[0])\n\t *\n\t * This is done in reverse order, because of how the\n\t * user environment and arguments are stored.\n\t */\n\tretval = remove_arg_zero(bprm);\n\tif (retval)\n\t\treturn retval;\n\tretval = copy_strings_kernel(1, &bprm->interp, bprm);\n\tif (retval < 0) return retval; \n\tbprm->argc++;\n\tif (i_arg) {\n\t\tretval = copy_strings_kernel(1, &i_arg, bprm);\n\t\tif (retval < 0) return retval; \n\t\tbprm->argc++;\n\t}\n\tretval = copy_strings_kernel(1, &i_name, bprm);\n\tif (retval) return retval; \n\tbprm->argc++;\n\tbprm->interp = interp;\n\n\t/*\n\t * OK, now restart the process with the interpreter's dentry.\n\t */\n\tfile = open_exec(interp);\n\tif (IS_ERR(file))\n\t\treturn PTR_ERR(file);\n\n\tbprm->file = file;\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\treturn retval;\n\treturn search_binary_handler(bprm);\n}",
        "code_after_change": "static int load_script(struct linux_binprm *bprm)\n{\n\tconst char *i_arg, *i_name;\n\tchar *cp;\n\tstruct file *file;\n\tchar interp[BINPRM_BUF_SIZE];\n\tint retval;\n\n\tif ((bprm->buf[0] != '#') || (bprm->buf[1] != '!'))\n\t\treturn -ENOEXEC;\n\t/*\n\t * This section does the #! interpretation.\n\t * Sorta complicated, but hopefully it will work.  -TYT\n\t */\n\n\tallow_write_access(bprm->file);\n\tfput(bprm->file);\n\tbprm->file = NULL;\n\n\tbprm->buf[BINPRM_BUF_SIZE - 1] = '\\0';\n\tif ((cp = strchr(bprm->buf, '\\n')) == NULL)\n\t\tcp = bprm->buf+BINPRM_BUF_SIZE-1;\n\t*cp = '\\0';\n\twhile (cp > bprm->buf) {\n\t\tcp--;\n\t\tif ((*cp == ' ') || (*cp == '\\t'))\n\t\t\t*cp = '\\0';\n\t\telse\n\t\t\tbreak;\n\t}\n\tfor (cp = bprm->buf+2; (*cp == ' ') || (*cp == '\\t'); cp++);\n\tif (*cp == '\\0') \n\t\treturn -ENOEXEC; /* No interpreter name found */\n\ti_name = cp;\n\ti_arg = NULL;\n\tfor ( ; *cp && (*cp != ' ') && (*cp != '\\t'); cp++)\n\t\t/* nothing */ ;\n\twhile ((*cp == ' ') || (*cp == '\\t'))\n\t\t*cp++ = '\\0';\n\tif (*cp)\n\t\ti_arg = cp;\n\tstrcpy (interp, i_name);\n\t/*\n\t * OK, we've parsed out the interpreter name and\n\t * (optional) argument.\n\t * Splice in (1) the interpreter's name for argv[0]\n\t *           (2) (optional) argument to interpreter\n\t *           (3) filename of shell script (replace argv[0])\n\t *\n\t * This is done in reverse order, because of how the\n\t * user environment and arguments are stored.\n\t */\n\tretval = remove_arg_zero(bprm);\n\tif (retval)\n\t\treturn retval;\n\tretval = copy_strings_kernel(1, &bprm->interp, bprm);\n\tif (retval < 0) return retval; \n\tbprm->argc++;\n\tif (i_arg) {\n\t\tretval = copy_strings_kernel(1, &i_arg, bprm);\n\t\tif (retval < 0) return retval; \n\t\tbprm->argc++;\n\t}\n\tretval = copy_strings_kernel(1, &i_name, bprm);\n\tif (retval) return retval; \n\tbprm->argc++;\n\tbprm->interp = interp;\n\n\t/*\n\t * OK, now restart the process with the interpreter's dentry.\n\t */\n\tfile = open_exec(interp);\n\tif (IS_ERR(file))\n\t\treturn PTR_ERR(file);\n\n\tbprm->file = file;\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\treturn retval;\n\treturn search_binary_handler(bprm);\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,15 +6,13 @@\n \tchar interp[BINPRM_BUF_SIZE];\n \tint retval;\n \n-\tif ((bprm->buf[0] != '#') || (bprm->buf[1] != '!') ||\n-\t    (bprm->recursion_depth > BINPRM_MAX_RECURSION))\n+\tif ((bprm->buf[0] != '#') || (bprm->buf[1] != '!'))\n \t\treturn -ENOEXEC;\n \t/*\n \t * This section does the #! interpretation.\n \t * Sorta complicated, but hopefully it will work.  -TYT\n \t */\n \n-\tbprm->recursion_depth++;\n \tallow_write_access(bprm->file);\n \tfput(bprm->file);\n \tbprm->file = NULL;",
        "function_modified_lines": {
            "added": [
                "\tif ((bprm->buf[0] != '#') || (bprm->buf[1] != '!'))"
            ],
            "deleted": [
                "\tif ((bprm->buf[0] != '#') || (bprm->buf[1] != '!') ||",
                "\t    (bprm->recursion_depth > BINPRM_MAX_RECURSION))",
                "\tbprm->recursion_depth++;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The load_script function in fs/binfmt_script.c in the Linux kernel before 3.7.2 does not properly handle recursion, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 112
    },
    {
        "cve_id": "CVE-2012-6544",
        "code_before_change": "static int l2cap_sock_getname(struct socket *sock, struct sockaddr *addr, int *len, int peer)\n{\n\tstruct sockaddr_l2 *la = (struct sockaddr_l2 *) addr;\n\tstruct sock *sk = sock->sk;\n\tstruct l2cap_chan *chan = l2cap_pi(sk)->chan;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\taddr->sa_family = AF_BLUETOOTH;\n\t*len = sizeof(struct sockaddr_l2);\n\n\tif (peer) {\n\t\tla->l2_psm = chan->psm;\n\t\tbacpy(&la->l2_bdaddr, &bt_sk(sk)->dst);\n\t\tla->l2_cid = cpu_to_le16(chan->dcid);\n\t} else {\n\t\tla->l2_psm = chan->sport;\n\t\tbacpy(&la->l2_bdaddr, &bt_sk(sk)->src);\n\t\tla->l2_cid = cpu_to_le16(chan->scid);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int l2cap_sock_getname(struct socket *sock, struct sockaddr *addr, int *len, int peer)\n{\n\tstruct sockaddr_l2 *la = (struct sockaddr_l2 *) addr;\n\tstruct sock *sk = sock->sk;\n\tstruct l2cap_chan *chan = l2cap_pi(sk)->chan;\n\n\tBT_DBG(\"sock %p, sk %p\", sock, sk);\n\n\tmemset(la, 0, sizeof(struct sockaddr_l2));\n\taddr->sa_family = AF_BLUETOOTH;\n\t*len = sizeof(struct sockaddr_l2);\n\n\tif (peer) {\n\t\tla->l2_psm = chan->psm;\n\t\tbacpy(&la->l2_bdaddr, &bt_sk(sk)->dst);\n\t\tla->l2_cid = cpu_to_le16(chan->dcid);\n\t} else {\n\t\tla->l2_psm = chan->sport;\n\t\tbacpy(&la->l2_bdaddr, &bt_sk(sk)->src);\n\t\tla->l2_cid = cpu_to_le16(chan->scid);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,7 @@\n \n \tBT_DBG(\"sock %p, sk %p\", sock, sk);\n \n+\tmemset(la, 0, sizeof(struct sockaddr_l2));\n \taddr->sa_family = AF_BLUETOOTH;\n \t*len = sizeof(struct sockaddr_l2);\n ",
        "function_modified_lines": {
            "added": [
                "\tmemset(la, 0, sizeof(struct sockaddr_l2));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The Bluetooth protocol stack in the Linux kernel before 3.6 does not properly initialize certain structures, which allows local users to obtain sensitive information from kernel stack memory via a crafted application that targets the (1) L2CAP or (2) HCI implementation.",
        "id": 128
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "void __cpuinit xsave_init(void)\n{\n\tstatic __refdata void (*next_func)(void) = xstate_enable_boot_cpu;\n\tvoid (*this_func)(void);\n\n\tif (!cpu_has_xsave)\n\t\treturn;\n\n\tthis_func = next_func;\n\tnext_func = xstate_enable_ap;\n\tthis_func();\n}",
        "code_after_change": "void __cpuinit xsave_init(void)\n{\n\tstatic __refdata void (*next_func)(void) = xstate_enable_boot_cpu;\n\tvoid (*this_func)(void);\n\n\tif (!cpu_has_xsave)\n\t\treturn;\n\n\tthis_func = next_func;\n\tnext_func = xstate_enable;\n\tthis_func();\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,6 @@\n \t\treturn;\n \n \tthis_func = next_func;\n-\tnext_func = xstate_enable_ap;\n+\tnext_func = xstate_enable;\n \tthis_func();\n }",
        "function_modified_lines": {
            "added": [
                "\tnext_func = xstate_enable;"
            ],
            "deleted": [
                "\tnext_func = xstate_enable_ap;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1817
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static int blkif_queue_rw_req(struct request *req, struct blkfront_ring_info *rinfo)\n{\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct blkif_request *ring_req, *extra_ring_req = NULL;\n\tstruct blkif_request *final_ring_req, *final_extra_ring_req = NULL;\n\tunsigned long id, extra_id = NO_ASSOCIATED_ID;\n\tbool require_extra_req = false;\n\tint i;\n\tstruct setup_rw_req setup = {\n\t\t.grant_idx = 0,\n\t\t.segments = NULL,\n\t\t.rinfo = rinfo,\n\t\t.need_copy = rq_data_dir(req) && info->feature_persistent,\n\t};\n\n\t/*\n\t * Used to store if we are able to queue the request by just using\n\t * existing persistent grants, or if we have to get new grants,\n\t * as there are not sufficiently many free.\n\t */\n\tbool new_persistent_gnts = false;\n\tstruct scatterlist *sg;\n\tint num_sg, max_grefs, num_grant;\n\n\tmax_grefs = req->nr_phys_segments * GRANTS_PER_PSEG;\n\tif (max_grefs > BLKIF_MAX_SEGMENTS_PER_REQUEST)\n\t\t/*\n\t\t * If we are using indirect segments we need to account\n\t\t * for the indirect grefs used in the request.\n\t\t */\n\t\tmax_grefs += INDIRECT_GREFS(max_grefs);\n\n\t/* Check if we have enough persistent grants to allocate a requests */\n\tif (rinfo->persistent_gnts_c < max_grefs) {\n\t\tnew_persistent_gnts = true;\n\n\t\tif (gnttab_alloc_grant_references(\n\t\t    max_grefs - rinfo->persistent_gnts_c,\n\t\t    &setup.gref_head) < 0) {\n\t\t\tgnttab_request_free_callback(\n\t\t\t\t&rinfo->callback,\n\t\t\t\tblkif_restart_queue_callback,\n\t\t\t\trinfo,\n\t\t\t\tmax_grefs - rinfo->persistent_gnts_c);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t/* Fill out a communications ring structure. */\n\tid = blkif_ring_get_request(rinfo, req, &final_ring_req);\n\tring_req = &rinfo->shadow[id].req;\n\n\tnum_sg = blk_rq_map_sg(req->q, req, rinfo->shadow[id].sg);\n\tnum_grant = 0;\n\t/* Calculate the number of grant used */\n\tfor_each_sg(rinfo->shadow[id].sg, sg, num_sg, i)\n\t       num_grant += gnttab_count_grant(sg->offset, sg->length);\n\n\trequire_extra_req = info->max_indirect_segments == 0 &&\n\t\tnum_grant > BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\tBUG_ON(!HAS_EXTRA_REQ && require_extra_req);\n\n\trinfo->shadow[id].num_sg = num_sg;\n\tif (num_grant > BLKIF_MAX_SEGMENTS_PER_REQUEST &&\n\t    likely(!require_extra_req)) {\n\t\t/*\n\t\t * The indirect operation can only be a BLKIF_OP_READ or\n\t\t * BLKIF_OP_WRITE\n\t\t */\n\t\tBUG_ON(req_op(req) == REQ_OP_FLUSH || req->cmd_flags & REQ_FUA);\n\t\tring_req->operation = BLKIF_OP_INDIRECT;\n\t\tring_req->u.indirect.indirect_op = rq_data_dir(req) ?\n\t\t\tBLKIF_OP_WRITE : BLKIF_OP_READ;\n\t\tring_req->u.indirect.sector_number = (blkif_sector_t)blk_rq_pos(req);\n\t\tring_req->u.indirect.handle = info->handle;\n\t\tring_req->u.indirect.nr_segments = num_grant;\n\t} else {\n\t\tring_req->u.rw.sector_number = (blkif_sector_t)blk_rq_pos(req);\n\t\tring_req->u.rw.handle = info->handle;\n\t\tring_req->operation = rq_data_dir(req) ?\n\t\t\tBLKIF_OP_WRITE : BLKIF_OP_READ;\n\t\tif (req_op(req) == REQ_OP_FLUSH || req->cmd_flags & REQ_FUA) {\n\t\t\t/*\n\t\t\t * Ideally we can do an unordered flush-to-disk.\n\t\t\t * In case the backend onlysupports barriers, use that.\n\t\t\t * A barrier request a superset of FUA, so we can\n\t\t\t * implement it the same way.  (It's also a FLUSH+FUA,\n\t\t\t * since it is guaranteed ordered WRT previous writes.)\n\t\t\t */\n\t\t\tif (info->feature_flush && info->feature_fua)\n\t\t\t\tring_req->operation =\n\t\t\t\t\tBLKIF_OP_WRITE_BARRIER;\n\t\t\telse if (info->feature_flush)\n\t\t\t\tring_req->operation =\n\t\t\t\t\tBLKIF_OP_FLUSH_DISKCACHE;\n\t\t\telse\n\t\t\t\tring_req->operation = 0;\n\t\t}\n\t\tring_req->u.rw.nr_segments = num_grant;\n\t\tif (unlikely(require_extra_req)) {\n\t\t\textra_id = blkif_ring_get_request(rinfo, req,\n\t\t\t\t\t\t\t  &final_extra_ring_req);\n\t\t\textra_ring_req = &rinfo->shadow[extra_id].req;\n\n\t\t\t/*\n\t\t\t * Only the first request contains the scatter-gather\n\t\t\t * list.\n\t\t\t */\n\t\t\trinfo->shadow[extra_id].num_sg = 0;\n\n\t\t\tblkif_setup_extra_req(ring_req, extra_ring_req);\n\n\t\t\t/* Link the 2 requests together */\n\t\t\trinfo->shadow[extra_id].associated_id = id;\n\t\t\trinfo->shadow[id].associated_id = extra_id;\n\t\t}\n\t}\n\n\tsetup.ring_req = ring_req;\n\tsetup.id = id;\n\n\tsetup.require_extra_req = require_extra_req;\n\tif (unlikely(require_extra_req))\n\t\tsetup.extra_ring_req = extra_ring_req;\n\n\tfor_each_sg(rinfo->shadow[id].sg, sg, num_sg, i) {\n\t\tBUG_ON(sg->offset + sg->length > PAGE_SIZE);\n\n\t\tif (setup.need_copy) {\n\t\t\tsetup.bvec_off = sg->offset;\n\t\t\tsetup.bvec_data = kmap_atomic(sg_page(sg));\n\t\t}\n\n\t\tgnttab_foreach_grant_in_range(sg_page(sg),\n\t\t\t\t\t      sg->offset,\n\t\t\t\t\t      sg->length,\n\t\t\t\t\t      blkif_setup_rw_req_grant,\n\t\t\t\t\t      &setup);\n\n\t\tif (setup.need_copy)\n\t\t\tkunmap_atomic(setup.bvec_data);\n\t}\n\tif (setup.segments)\n\t\tkunmap_atomic(setup.segments);\n\n\t/* Copy request(s) to the ring page. */\n\t*final_ring_req = *ring_req;\n\trinfo->shadow[id].status = REQ_WAITING;\n\tif (unlikely(require_extra_req)) {\n\t\t*final_extra_ring_req = *extra_ring_req;\n\t\trinfo->shadow[extra_id].status = REQ_WAITING;\n\t}\n\n\tif (new_persistent_gnts)\n\t\tgnttab_free_grant_references(setup.gref_head);\n\n\treturn 0;\n}",
        "code_after_change": "static int blkif_queue_rw_req(struct request *req, struct blkfront_ring_info *rinfo)\n{\n\tstruct blkfront_info *info = rinfo->dev_info;\n\tstruct blkif_request *ring_req, *extra_ring_req = NULL;\n\tstruct blkif_request *final_ring_req, *final_extra_ring_req = NULL;\n\tunsigned long id, extra_id = NO_ASSOCIATED_ID;\n\tbool require_extra_req = false;\n\tint i;\n\tstruct setup_rw_req setup = {\n\t\t.grant_idx = 0,\n\t\t.segments = NULL,\n\t\t.rinfo = rinfo,\n\t\t.need_copy = rq_data_dir(req) && info->bounce,\n\t};\n\n\t/*\n\t * Used to store if we are able to queue the request by just using\n\t * existing persistent grants, or if we have to get new grants,\n\t * as there are not sufficiently many free.\n\t */\n\tbool new_persistent_gnts = false;\n\tstruct scatterlist *sg;\n\tint num_sg, max_grefs, num_grant;\n\n\tmax_grefs = req->nr_phys_segments * GRANTS_PER_PSEG;\n\tif (max_grefs > BLKIF_MAX_SEGMENTS_PER_REQUEST)\n\t\t/*\n\t\t * If we are using indirect segments we need to account\n\t\t * for the indirect grefs used in the request.\n\t\t */\n\t\tmax_grefs += INDIRECT_GREFS(max_grefs);\n\n\t/* Check if we have enough persistent grants to allocate a requests */\n\tif (rinfo->persistent_gnts_c < max_grefs) {\n\t\tnew_persistent_gnts = true;\n\n\t\tif (gnttab_alloc_grant_references(\n\t\t    max_grefs - rinfo->persistent_gnts_c,\n\t\t    &setup.gref_head) < 0) {\n\t\t\tgnttab_request_free_callback(\n\t\t\t\t&rinfo->callback,\n\t\t\t\tblkif_restart_queue_callback,\n\t\t\t\trinfo,\n\t\t\t\tmax_grefs - rinfo->persistent_gnts_c);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t/* Fill out a communications ring structure. */\n\tid = blkif_ring_get_request(rinfo, req, &final_ring_req);\n\tring_req = &rinfo->shadow[id].req;\n\n\tnum_sg = blk_rq_map_sg(req->q, req, rinfo->shadow[id].sg);\n\tnum_grant = 0;\n\t/* Calculate the number of grant used */\n\tfor_each_sg(rinfo->shadow[id].sg, sg, num_sg, i)\n\t       num_grant += gnttab_count_grant(sg->offset, sg->length);\n\n\trequire_extra_req = info->max_indirect_segments == 0 &&\n\t\tnum_grant > BLKIF_MAX_SEGMENTS_PER_REQUEST;\n\tBUG_ON(!HAS_EXTRA_REQ && require_extra_req);\n\n\trinfo->shadow[id].num_sg = num_sg;\n\tif (num_grant > BLKIF_MAX_SEGMENTS_PER_REQUEST &&\n\t    likely(!require_extra_req)) {\n\t\t/*\n\t\t * The indirect operation can only be a BLKIF_OP_READ or\n\t\t * BLKIF_OP_WRITE\n\t\t */\n\t\tBUG_ON(req_op(req) == REQ_OP_FLUSH || req->cmd_flags & REQ_FUA);\n\t\tring_req->operation = BLKIF_OP_INDIRECT;\n\t\tring_req->u.indirect.indirect_op = rq_data_dir(req) ?\n\t\t\tBLKIF_OP_WRITE : BLKIF_OP_READ;\n\t\tring_req->u.indirect.sector_number = (blkif_sector_t)blk_rq_pos(req);\n\t\tring_req->u.indirect.handle = info->handle;\n\t\tring_req->u.indirect.nr_segments = num_grant;\n\t} else {\n\t\tring_req->u.rw.sector_number = (blkif_sector_t)blk_rq_pos(req);\n\t\tring_req->u.rw.handle = info->handle;\n\t\tring_req->operation = rq_data_dir(req) ?\n\t\t\tBLKIF_OP_WRITE : BLKIF_OP_READ;\n\t\tif (req_op(req) == REQ_OP_FLUSH || req->cmd_flags & REQ_FUA) {\n\t\t\t/*\n\t\t\t * Ideally we can do an unordered flush-to-disk.\n\t\t\t * In case the backend onlysupports barriers, use that.\n\t\t\t * A barrier request a superset of FUA, so we can\n\t\t\t * implement it the same way.  (It's also a FLUSH+FUA,\n\t\t\t * since it is guaranteed ordered WRT previous writes.)\n\t\t\t */\n\t\t\tif (info->feature_flush && info->feature_fua)\n\t\t\t\tring_req->operation =\n\t\t\t\t\tBLKIF_OP_WRITE_BARRIER;\n\t\t\telse if (info->feature_flush)\n\t\t\t\tring_req->operation =\n\t\t\t\t\tBLKIF_OP_FLUSH_DISKCACHE;\n\t\t\telse\n\t\t\t\tring_req->operation = 0;\n\t\t}\n\t\tring_req->u.rw.nr_segments = num_grant;\n\t\tif (unlikely(require_extra_req)) {\n\t\t\textra_id = blkif_ring_get_request(rinfo, req,\n\t\t\t\t\t\t\t  &final_extra_ring_req);\n\t\t\textra_ring_req = &rinfo->shadow[extra_id].req;\n\n\t\t\t/*\n\t\t\t * Only the first request contains the scatter-gather\n\t\t\t * list.\n\t\t\t */\n\t\t\trinfo->shadow[extra_id].num_sg = 0;\n\n\t\t\tblkif_setup_extra_req(ring_req, extra_ring_req);\n\n\t\t\t/* Link the 2 requests together */\n\t\t\trinfo->shadow[extra_id].associated_id = id;\n\t\t\trinfo->shadow[id].associated_id = extra_id;\n\t\t}\n\t}\n\n\tsetup.ring_req = ring_req;\n\tsetup.id = id;\n\n\tsetup.require_extra_req = require_extra_req;\n\tif (unlikely(require_extra_req))\n\t\tsetup.extra_ring_req = extra_ring_req;\n\n\tfor_each_sg(rinfo->shadow[id].sg, sg, num_sg, i) {\n\t\tBUG_ON(sg->offset + sg->length > PAGE_SIZE);\n\n\t\tif (setup.need_copy) {\n\t\t\tsetup.bvec_off = sg->offset;\n\t\t\tsetup.bvec_data = kmap_atomic(sg_page(sg));\n\t\t}\n\n\t\tgnttab_foreach_grant_in_range(sg_page(sg),\n\t\t\t\t\t      sg->offset,\n\t\t\t\t\t      sg->length,\n\t\t\t\t\t      blkif_setup_rw_req_grant,\n\t\t\t\t\t      &setup);\n\n\t\tif (setup.need_copy)\n\t\t\tkunmap_atomic(setup.bvec_data);\n\t}\n\tif (setup.segments)\n\t\tkunmap_atomic(setup.segments);\n\n\t/* Copy request(s) to the ring page. */\n\t*final_ring_req = *ring_req;\n\trinfo->shadow[id].status = REQ_WAITING;\n\tif (unlikely(require_extra_req)) {\n\t\t*final_extra_ring_req = *extra_ring_req;\n\t\trinfo->shadow[extra_id].status = REQ_WAITING;\n\t}\n\n\tif (new_persistent_gnts)\n\t\tgnttab_free_grant_references(setup.gref_head);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,7 @@\n \t\t.grant_idx = 0,\n \t\t.segments = NULL,\n \t\t.rinfo = rinfo,\n-\t\t.need_copy = rq_data_dir(req) && info->feature_persistent,\n+\t\t.need_copy = rq_data_dir(req) && info->bounce,\n \t};\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "\t\t.need_copy = rq_data_dir(req) && info->bounce,"
            ],
            "deleted": [
                "\t\t.need_copy = rq_data_dir(req) && info->feature_persistent,"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3583
    },
    {
        "cve_id": "CVE-2013-3224",
        "code_before_change": "int bt_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t\tstruct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\n\tBT_DBG(\"sock %p sk %p len %zu\", sock, sk, len);\n\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb) {\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\treturn 0;\n\t\treturn err;\n\t}\n\n\tmsg->msg_namelen = 0;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\tskb_reset_transport_header(skb);\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err == 0)\n\t\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tskb_free_datagram(sk, skb);\n\n\treturn err ? : copied;\n}",
        "code_after_change": "int bt_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t\tstruct msghdr *msg, size_t len, int flags)\n{\n\tint noblock = flags & MSG_DONTWAIT;\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\n\tBT_DBG(\"sock %p sk %p len %zu\", sock, sk, len);\n\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\tmsg->msg_namelen = 0;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb) {\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\treturn 0;\n\t\treturn err;\n\t}\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\tskb_reset_transport_header(skb);\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err == 0)\n\t\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tskb_free_datagram(sk, skb);\n\n\treturn err ? : copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,14 +12,14 @@\n \tif (flags & (MSG_OOB))\n \t\treturn -EOPNOTSUPP;\n \n+\tmsg->msg_namelen = 0;\n+\n \tskb = skb_recv_datagram(sk, flags, noblock, &err);\n \tif (!skb) {\n \t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n \t\t\treturn 0;\n \t\treturn err;\n \t}\n-\n-\tmsg->msg_namelen = 0;\n \n \tcopied = skb->len;\n \tif (len < copied) {",
        "function_modified_lines": {
            "added": [
                "\tmsg->msg_namelen = 0;",
                ""
            ],
            "deleted": [
                "",
                "\tmsg->msg_namelen = 0;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The bt_sock_recvmsg function in net/bluetooth/af_bluetooth.c in the Linux kernel before 3.9-rc7 does not properly initialize a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 264
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "static inline void xstate_enable(void)\n{\n\tclts();\n\tset_in_cr4(X86_CR4_OSXSAVE);\n\txsetbv(XCR_XFEATURE_ENABLED_MASK, pcntxt_mask);\n}",
        "code_after_change": "static inline void xstate_enable(void)\n{\n\tset_in_cr4(X86_CR4_OSXSAVE);\n\txsetbv(XCR_XFEATURE_ENABLED_MASK, pcntxt_mask);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,5 @@\n static inline void xstate_enable(void)\n {\n-\tclts();\n \tset_in_cr4(X86_CR4_OSXSAVE);\n \txsetbv(XCR_XFEATURE_ENABLED_MASK, pcntxt_mask);\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tclts();"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1814
    },
    {
        "cve_id": "CVE-2018-20449",
        "code_before_change": "char *pointer(const char *fmt, char *buf, char *end, void *ptr,\n\t      struct printf_spec spec)\n{\n\tconst int default_width = 2 * sizeof(void *);\n\n\tif (!ptr && *fmt != 'K') {\n\t\t/*\n\t\t * Print (null) with the same width as a pointer so it makes\n\t\t * tabular output look nice.\n\t\t */\n\t\tif (spec.field_width == -1)\n\t\t\tspec.field_width = default_width;\n\t\treturn string(buf, end, \"(null)\", spec);\n\t}\n\n\tswitch (*fmt) {\n\tcase 'F':\n\tcase 'f':\n\t\tptr = dereference_function_descriptor(ptr);\n\t\t/* Fallthrough */\n\tcase 'S':\n\tcase 's':\n\tcase 'B':\n\t\treturn symbol_string(buf, end, ptr, spec, fmt);\n\tcase 'R':\n\tcase 'r':\n\t\treturn resource_string(buf, end, ptr, spec, fmt);\n\tcase 'h':\n\t\treturn hex_string(buf, end, ptr, spec, fmt);\n\tcase 'b':\n\t\tswitch (fmt[1]) {\n\t\tcase 'l':\n\t\t\treturn bitmap_list_string(buf, end, ptr, spec, fmt);\n\t\tdefault:\n\t\t\treturn bitmap_string(buf, end, ptr, spec, fmt);\n\t\t}\n\tcase 'M':\t\t\t/* Colon separated: 00:01:02:03:04:05 */\n\tcase 'm':\t\t\t/* Contiguous: 000102030405 */\n\t\t\t\t\t/* [mM]F (FDDI) */\n\t\t\t\t\t/* [mM]R (Reverse order; Bluetooth) */\n\t\treturn mac_address_string(buf, end, ptr, spec, fmt);\n\tcase 'I':\t\t\t/* Formatted IP supported\n\t\t\t\t\t * 4:\t1.2.3.4\n\t\t\t\t\t * 6:\t0001:0203:...:0708\n\t\t\t\t\t * 6c:\t1::708 or 1::1.2.3.4\n\t\t\t\t\t */\n\tcase 'i':\t\t\t/* Contiguous:\n\t\t\t\t\t * 4:\t001.002.003.004\n\t\t\t\t\t * 6:   000102...0f\n\t\t\t\t\t */\n\t\tswitch (fmt[1]) {\n\t\tcase '6':\n\t\t\treturn ip6_addr_string(buf, end, ptr, spec, fmt);\n\t\tcase '4':\n\t\t\treturn ip4_addr_string(buf, end, ptr, spec, fmt);\n\t\tcase 'S': {\n\t\t\tconst union {\n\t\t\t\tstruct sockaddr\t\traw;\n\t\t\t\tstruct sockaddr_in\tv4;\n\t\t\t\tstruct sockaddr_in6\tv6;\n\t\t\t} *sa = ptr;\n\n\t\t\tswitch (sa->raw.sa_family) {\n\t\t\tcase AF_INET:\n\t\t\t\treturn ip4_addr_string_sa(buf, end, &sa->v4, spec, fmt);\n\t\t\tcase AF_INET6:\n\t\t\t\treturn ip6_addr_string_sa(buf, end, &sa->v6, spec, fmt);\n\t\t\tdefault:\n\t\t\t\treturn string(buf, end, \"(invalid address)\", spec);\n\t\t\t}}\n\t\t}\n\t\tbreak;\n\tcase 'E':\n\t\treturn escaped_string(buf, end, ptr, spec, fmt);\n\tcase 'U':\n\t\treturn uuid_string(buf, end, ptr, spec, fmt);\n\tcase 'V':\n\t\t{\n\t\t\tva_list va;\n\n\t\t\tva_copy(va, *((struct va_format *)ptr)->va);\n\t\t\tbuf += vsnprintf(buf, end > buf ? end - buf : 0,\n\t\t\t\t\t ((struct va_format *)ptr)->fmt, va);\n\t\t\tva_end(va);\n\t\t\treturn buf;\n\t\t}\n\tcase 'K':\n\t\treturn restricted_pointer(buf, end, ptr, spec);\n\tcase 'N':\n\t\treturn netdev_bits(buf, end, ptr, fmt);\n\tcase 'a':\n\t\treturn address_val(buf, end, ptr, fmt);\n\tcase 'd':\n\t\treturn dentry_name(buf, end, ptr, spec, fmt);\n\tcase 'C':\n\t\treturn clock(buf, end, ptr, spec, fmt);\n\tcase 'D':\n\t\treturn dentry_name(buf, end,\n\t\t\t\t   ((const struct file *)ptr)->f_path.dentry,\n\t\t\t\t   spec, fmt);\n#ifdef CONFIG_BLOCK\n\tcase 'g':\n\t\treturn bdev_name(buf, end, ptr, spec, fmt);\n#endif\n\n\tcase 'G':\n\t\treturn flags_string(buf, end, ptr, fmt);\n\tcase 'O':\n\t\tswitch (fmt[1]) {\n\t\tcase 'F':\n\t\t\treturn device_node_string(buf, end, ptr, spec, fmt + 1);\n\t\t}\n\t}\n\tspec.flags |= SMALL;\n\tif (spec.field_width == -1) {\n\t\tspec.field_width = default_width;\n\t\tspec.flags |= ZEROPAD;\n\t}\n\tspec.base = 16;\n\n\treturn number(buf, end, (unsigned long) ptr, spec);\n}",
        "code_after_change": "char *pointer(const char *fmt, char *buf, char *end, void *ptr,\n\t      struct printf_spec spec)\n{\n\tconst int default_width = 2 * sizeof(void *);\n\n\tif (!ptr && *fmt != 'K') {\n\t\t/*\n\t\t * Print (null) with the same width as a pointer so it makes\n\t\t * tabular output look nice.\n\t\t */\n\t\tif (spec.field_width == -1)\n\t\t\tspec.field_width = default_width;\n\t\treturn string(buf, end, \"(null)\", spec);\n\t}\n\n\tswitch (*fmt) {\n\tcase 'F':\n\tcase 'f':\n\t\tptr = dereference_function_descriptor(ptr);\n\t\t/* Fallthrough */\n\tcase 'S':\n\tcase 's':\n\tcase 'B':\n\t\treturn symbol_string(buf, end, ptr, spec, fmt);\n\tcase 'R':\n\tcase 'r':\n\t\treturn resource_string(buf, end, ptr, spec, fmt);\n\tcase 'h':\n\t\treturn hex_string(buf, end, ptr, spec, fmt);\n\tcase 'b':\n\t\tswitch (fmt[1]) {\n\t\tcase 'l':\n\t\t\treturn bitmap_list_string(buf, end, ptr, spec, fmt);\n\t\tdefault:\n\t\t\treturn bitmap_string(buf, end, ptr, spec, fmt);\n\t\t}\n\tcase 'M':\t\t\t/* Colon separated: 00:01:02:03:04:05 */\n\tcase 'm':\t\t\t/* Contiguous: 000102030405 */\n\t\t\t\t\t/* [mM]F (FDDI) */\n\t\t\t\t\t/* [mM]R (Reverse order; Bluetooth) */\n\t\treturn mac_address_string(buf, end, ptr, spec, fmt);\n\tcase 'I':\t\t\t/* Formatted IP supported\n\t\t\t\t\t * 4:\t1.2.3.4\n\t\t\t\t\t * 6:\t0001:0203:...:0708\n\t\t\t\t\t * 6c:\t1::708 or 1::1.2.3.4\n\t\t\t\t\t */\n\tcase 'i':\t\t\t/* Contiguous:\n\t\t\t\t\t * 4:\t001.002.003.004\n\t\t\t\t\t * 6:   000102...0f\n\t\t\t\t\t */\n\t\tswitch (fmt[1]) {\n\t\tcase '6':\n\t\t\treturn ip6_addr_string(buf, end, ptr, spec, fmt);\n\t\tcase '4':\n\t\t\treturn ip4_addr_string(buf, end, ptr, spec, fmt);\n\t\tcase 'S': {\n\t\t\tconst union {\n\t\t\t\tstruct sockaddr\t\traw;\n\t\t\t\tstruct sockaddr_in\tv4;\n\t\t\t\tstruct sockaddr_in6\tv6;\n\t\t\t} *sa = ptr;\n\n\t\t\tswitch (sa->raw.sa_family) {\n\t\t\tcase AF_INET:\n\t\t\t\treturn ip4_addr_string_sa(buf, end, &sa->v4, spec, fmt);\n\t\t\tcase AF_INET6:\n\t\t\t\treturn ip6_addr_string_sa(buf, end, &sa->v6, spec, fmt);\n\t\t\tdefault:\n\t\t\t\treturn string(buf, end, \"(invalid address)\", spec);\n\t\t\t}}\n\t\t}\n\t\tbreak;\n\tcase 'E':\n\t\treturn escaped_string(buf, end, ptr, spec, fmt);\n\tcase 'U':\n\t\treturn uuid_string(buf, end, ptr, spec, fmt);\n\tcase 'V':\n\t\t{\n\t\t\tva_list va;\n\n\t\t\tva_copy(va, *((struct va_format *)ptr)->va);\n\t\t\tbuf += vsnprintf(buf, end > buf ? end - buf : 0,\n\t\t\t\t\t ((struct va_format *)ptr)->fmt, va);\n\t\t\tva_end(va);\n\t\t\treturn buf;\n\t\t}\n\tcase 'K':\n\t\treturn restricted_pointer(buf, end, ptr, spec);\n\tcase 'N':\n\t\treturn netdev_bits(buf, end, ptr, fmt);\n\tcase 'a':\n\t\treturn address_val(buf, end, ptr, fmt);\n\tcase 'd':\n\t\treturn dentry_name(buf, end, ptr, spec, fmt);\n\tcase 'C':\n\t\treturn clock(buf, end, ptr, spec, fmt);\n\tcase 'D':\n\t\treturn dentry_name(buf, end,\n\t\t\t\t   ((const struct file *)ptr)->f_path.dentry,\n\t\t\t\t   spec, fmt);\n#ifdef CONFIG_BLOCK\n\tcase 'g':\n\t\treturn bdev_name(buf, end, ptr, spec, fmt);\n#endif\n\n\tcase 'G':\n\t\treturn flags_string(buf, end, ptr, fmt);\n\tcase 'O':\n\t\tswitch (fmt[1]) {\n\t\tcase 'F':\n\t\t\treturn device_node_string(buf, end, ptr, spec, fmt + 1);\n\t\t}\n\t}\n\n\t/* default is to _not_ leak addresses, hash before printing */\n\treturn ptr_to_id(buf, end, ptr, spec);\n}",
        "patch": "--- code before\n+++ code after\n@@ -111,12 +111,7 @@\n \t\t\treturn device_node_string(buf, end, ptr, spec, fmt + 1);\n \t\t}\n \t}\n-\tspec.flags |= SMALL;\n-\tif (spec.field_width == -1) {\n-\t\tspec.field_width = default_width;\n-\t\tspec.flags |= ZEROPAD;\n-\t}\n-\tspec.base = 16;\n \n-\treturn number(buf, end, (unsigned long) ptr, spec);\n+\t/* default is to _not_ leak addresses, hash before printing */\n+\treturn ptr_to_id(buf, end, ptr, spec);\n }",
        "function_modified_lines": {
            "added": [
                "\t/* default is to _not_ leak addresses, hash before printing */",
                "\treturn ptr_to_id(buf, end, ptr, spec);"
            ],
            "deleted": [
                "\tspec.flags |= SMALL;",
                "\tif (spec.field_width == -1) {",
                "\t\tspec.field_width = default_width;",
                "\t\tspec.flags |= ZEROPAD;",
                "\t}",
                "\tspec.base = 16;",
                "\treturn number(buf, end, (unsigned long) ptr, spec);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The hidma_chan_stats function in drivers/dma/qcom/hidma_dbg.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \"callback=\" lines in a debugfs file.",
        "id": 1758
    },
    {
        "cve_id": "CVE-2018-15594",
        "code_before_change": "unsigned paravirt_patch_call(void *insnbuf,\n\t\t\t     const void *target, u16 tgt_clobbers,\n\t\t\t     unsigned long addr, u16 site_clobbers,\n\t\t\t     unsigned len)\n{\n\tstruct branch *b = insnbuf;\n\tunsigned long delta = (unsigned long)target - (addr+5);\n\n\tif (tgt_clobbers & ~site_clobbers)\n\t\treturn len;\t/* target would clobber too much for this site */\n\tif (len < 5)\n\t\treturn len;\t/* call too long for patch site */\n\n\tb->opcode = 0xe8; /* call */\n\tb->delta = delta;\n\tBUILD_BUG_ON(sizeof(*b) != 5);\n\n\treturn 5;\n}",
        "code_after_change": "unsigned paravirt_patch_call(void *insnbuf,\n\t\t\t     const void *target, u16 tgt_clobbers,\n\t\t\t     unsigned long addr, u16 site_clobbers,\n\t\t\t     unsigned len)\n{\n\tstruct branch *b = insnbuf;\n\tunsigned long delta = (unsigned long)target - (addr+5);\n\n\tif (len < 5) {\n#ifdef CONFIG_RETPOLINE\n\t\tWARN_ONCE(\"Failing to patch indirect CALL in %ps\\n\", (void *)addr);\n#endif\n\t\treturn len;\t/* call too long for patch site */\n\t}\n\n\tb->opcode = 0xe8; /* call */\n\tb->delta = delta;\n\tBUILD_BUG_ON(sizeof(*b) != 5);\n\n\treturn 5;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,10 +6,12 @@\n \tstruct branch *b = insnbuf;\n \tunsigned long delta = (unsigned long)target - (addr+5);\n \n-\tif (tgt_clobbers & ~site_clobbers)\n-\t\treturn len;\t/* target would clobber too much for this site */\n-\tif (len < 5)\n+\tif (len < 5) {\n+#ifdef CONFIG_RETPOLINE\n+\t\tWARN_ONCE(\"Failing to patch indirect CALL in %ps\\n\", (void *)addr);\n+#endif\n \t\treturn len;\t/* call too long for patch site */\n+\t}\n \n \tb->opcode = 0xe8; /* call */\n \tb->delta = delta;",
        "function_modified_lines": {
            "added": [
                "\tif (len < 5) {",
                "#ifdef CONFIG_RETPOLINE",
                "\t\tWARN_ONCE(\"Failing to patch indirect CALL in %ps\\n\", (void *)addr);",
                "#endif",
                "\t}"
            ],
            "deleted": [
                "\tif (tgt_clobbers & ~site_clobbers)",
                "\t\treturn len;\t/* target would clobber too much for this site */",
                "\tif (len < 5)"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "arch/x86/kernel/paravirt.c in the Linux kernel before 4.18.1 mishandles certain indirect calls, which makes it easier for attackers to conduct Spectre-v2 attacks against paravirtual guests.",
        "id": 1709
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static void blkfront_delay_work(struct work_struct *work)\n{\n\tstruct blkfront_info *info;\n\tbool need_schedule_work = false;\n\n\tmutex_lock(&blkfront_mutex);\n\n\tlist_for_each_entry(info, &info_list, info_list) {\n\t\tif (info->feature_persistent) {\n\t\t\tneed_schedule_work = true;\n\t\t\tmutex_lock(&info->mutex);\n\t\t\tpurge_persistent_grants(info);\n\t\t\tmutex_unlock(&info->mutex);\n\t\t}\n\t}\n\n\tif (need_schedule_work)\n\t\tschedule_delayed_work(&blkfront_work, HZ * 10);\n\n\tmutex_unlock(&blkfront_mutex);\n}",
        "code_after_change": "static void blkfront_delay_work(struct work_struct *work)\n{\n\tstruct blkfront_info *info;\n\tbool need_schedule_work = false;\n\n\t/*\n\t * Note that when using bounce buffers but not persistent grants\n\t * there's no need to run blkfront_delay_work because grants are\n\t * revoked in blkif_completion or else an error is reported and the\n\t * connection is closed.\n\t */\n\n\tmutex_lock(&blkfront_mutex);\n\n\tlist_for_each_entry(info, &info_list, info_list) {\n\t\tif (info->feature_persistent) {\n\t\t\tneed_schedule_work = true;\n\t\t\tmutex_lock(&info->mutex);\n\t\t\tpurge_persistent_grants(info);\n\t\t\tmutex_unlock(&info->mutex);\n\t\t}\n\t}\n\n\tif (need_schedule_work)\n\t\tschedule_delayed_work(&blkfront_work, HZ * 10);\n\n\tmutex_unlock(&blkfront_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,13 @@\n {\n \tstruct blkfront_info *info;\n \tbool need_schedule_work = false;\n+\n+\t/*\n+\t * Note that when using bounce buffers but not persistent grants\n+\t * there's no need to run blkfront_delay_work because grants are\n+\t * revoked in blkif_completion or else an error is reported and the\n+\t * connection is closed.\n+\t */\n \n \tmutex_lock(&blkfront_mutex);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * Note that when using bounce buffers but not persistent grants",
                "\t * there's no need to run blkfront_delay_work because grants are",
                "\t * revoked in blkif_completion or else an error is reported and the",
                "\t * connection is closed.",
                "\t */"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3585
    },
    {
        "cve_id": "CVE-2013-3235",
        "code_before_change": "static void set_orig_addr(struct msghdr *m, struct tipc_msg *msg)\n{\n\tstruct sockaddr_tipc *addr = (struct sockaddr_tipc *)m->msg_name;\n\n\tif (addr) {\n\t\taddr->family = AF_TIPC;\n\t\taddr->addrtype = TIPC_ADDR_ID;\n\t\taddr->addr.id.ref = msg_origport(msg);\n\t\taddr->addr.id.node = msg_orignode(msg);\n\t\taddr->addr.name.domain = 0;\t/* could leave uninitialized */\n\t\taddr->scope = 0;\t\t/* could leave uninitialized */\n\t\tm->msg_namelen = sizeof(struct sockaddr_tipc);\n\t}\n}",
        "code_after_change": "static void set_orig_addr(struct msghdr *m, struct tipc_msg *msg)\n{\n\tstruct sockaddr_tipc *addr = (struct sockaddr_tipc *)m->msg_name;\n\n\tif (addr) {\n\t\taddr->family = AF_TIPC;\n\t\taddr->addrtype = TIPC_ADDR_ID;\n\t\tmemset(&addr->addr, 0, sizeof(addr->addr));\n\t\taddr->addr.id.ref = msg_origport(msg);\n\t\taddr->addr.id.node = msg_orignode(msg);\n\t\taddr->addr.name.domain = 0;\t/* could leave uninitialized */\n\t\taddr->scope = 0;\t\t/* could leave uninitialized */\n\t\tm->msg_namelen = sizeof(struct sockaddr_tipc);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tif (addr) {\n \t\taddr->family = AF_TIPC;\n \t\taddr->addrtype = TIPC_ADDR_ID;\n+\t\tmemset(&addr->addr, 0, sizeof(addr->addr));\n \t\taddr->addr.id.ref = msg_origport(msg);\n \t\taddr->addr.id.node = msg_orignode(msg);\n \t\taddr->addr.name.domain = 0;\t/* could leave uninitialized */",
        "function_modified_lines": {
            "added": [
                "\t\tmemset(&addr->addr, 0, sizeof(addr->addr));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "net/tipc/socket.c in the Linux kernel before 3.9-rc7 does not initialize a certain data structure and a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 274
    },
    {
        "cve_id": "CVE-2018-20510",
        "code_before_change": "static int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tint ret;\n\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tconst char *debug_string;\n\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n\t\t\tstruct binder_ref_data rdata;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tret = -1;\n\t\t\tif (increment && !target) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node)\n\t\t\t\t\tret = binder_inc_ref_for_node(\n\t\t\t\t\t\t\tproc, ctx_mgr_node,\n\t\t\t\t\t\t\tstrong, NULL, &rdata);\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tret = binder_update_ref_for_handle(\n\t\t\t\t\t\tproc, target, increment, strong,\n\t\t\t\t\t\t&rdata);\n\t\t\tif (!ret && rdata.desc != target) {\n\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\ttarget, rdata.desc);\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, debug_string,\n\t\t\t\t\tstrong, target, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string,\n\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n\t\t\t\t     rdata.weak);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\t\t\tbool free_node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbinder_put_node(node);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_node_inner_lock(node);\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tfree_node = binder_dec_node_nilocked(node,\n\t\t\t\t\tcmd == BC_ACQUIRE_DONE, 0);\n\t\t\tWARN_ON(free_node);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d tr %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs,\n\t\t\t\t     node->local_weak_refs, node->tmp_refs);\n\t\t\tbinder_node_inner_unlock(node);\n\t\t\tbinder_put_node(node);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (buffer == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!buffer->allow_user_free) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned buffer\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\n\t\t\tif (buffer->transaction) {\n\t\t\t\tbuffer->transaction->buffer = NULL;\n\t\t\t\tbuffer->transaction = NULL;\n\t\t\t}\n\t\t\tif (buffer->async_transaction && buffer->target_node) {\n\t\t\t\tstruct binder_node *buf_node;\n\t\t\t\tstruct binder_work *w;\n\n\t\t\t\tbuf_node = buffer->target_node;\n\t\t\t\tbinder_node_inner_lock(buf_node);\n\t\t\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\t\t\tBUG_ON(buf_node->proc != proc);\n\t\t\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t\t\t&buf_node->async_todo);\n\t\t\t\tif (!w) {\n\t\t\t\t\tbuf_node->has_async_transaction = false;\n\t\t\t\t} else {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\tw, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t\tbinder_node_inner_unlock(buf_node);\n\t\t\t}\n\t\t\ttrace_binder_transaction_buffer_release(buffer);\n\t\t\tbinder_transaction_buffer_release(proc, buffer, NULL);\n\t\t\tbinder_alloc_free_buf(&proc->alloc, buffer);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\t/*\n\t\t\t\t * Allocate memory for death notification\n\t\t\t\t * before taking lock\n\t\t\t\t */\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tbinder_enqueue_thread_work(\n\t\t\t\t\t\tthread,\n\t\t\t\t\t\t&thread->return_error.work);\n\t\t\t\t\tbinder_debug(\n\t\t\t\t\t\tBINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t\"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->data.debug_id,\n\t\t\t\t     ref->data.desc, ref->data.strong,\n\t\t\t\t     ref->data.weak, ref->node->debug_id);\n\n\t\t\tbinder_node_lock(ref->node);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tkfree(death);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\n\t\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t&ref->death->work, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper &\n\t\t\t\t\t    (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t     BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\t\t\tthread,\n\t\t\t\t\t\t\t\t&death->work);\n\t\t\t\t\telse {\n\t\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\t\tbinder_wakeup_proc_ilocked(\n\t\t\t\t\t\t\t\tproc);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tbinder_node_unlock(ref->node);\n\t\t\tbinder_proc_unlock(proc);\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death,\n\t\t\t\t\t    entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death =\n\t\t\t\t\tcontainer_of(w,\n\t\t\t\t\t\t     struct binder_ref_death,\n\t\t\t\t\t\t     work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %p\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_dequeue_work_ilocked(&death->work);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper &\n\t\t\t\t\t(BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\tthread, &death->work);\n\t\t\t\telse {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tint ret;\n\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tconst char *debug_string;\n\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n\t\t\tstruct binder_ref_data rdata;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tret = -1;\n\t\t\tif (increment && !target) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node)\n\t\t\t\t\tret = binder_inc_ref_for_node(\n\t\t\t\t\t\t\tproc, ctx_mgr_node,\n\t\t\t\t\t\t\tstrong, NULL, &rdata);\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tret = binder_update_ref_for_handle(\n\t\t\t\t\t\tproc, target, increment, strong,\n\t\t\t\t\t\t&rdata);\n\t\t\tif (!ret && rdata.desc != target) {\n\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\ttarget, rdata.desc);\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, debug_string,\n\t\t\t\t\tstrong, target, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string,\n\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n\t\t\t\t     rdata.weak);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\t\t\tbool free_node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbinder_put_node(node);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_node_inner_lock(node);\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tfree_node = binder_dec_node_nilocked(node,\n\t\t\t\t\tcmd == BC_ACQUIRE_DONE, 0);\n\t\t\tWARN_ON(free_node);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d tr %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs,\n\t\t\t\t     node->local_weak_refs, node->tmp_refs);\n\t\t\tbinder_node_inner_unlock(node);\n\t\t\tbinder_put_node(node);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (buffer == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!buffer->allow_user_free) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned buffer\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\n\t\t\tif (buffer->transaction) {\n\t\t\t\tbuffer->transaction->buffer = NULL;\n\t\t\t\tbuffer->transaction = NULL;\n\t\t\t}\n\t\t\tif (buffer->async_transaction && buffer->target_node) {\n\t\t\t\tstruct binder_node *buf_node;\n\t\t\t\tstruct binder_work *w;\n\n\t\t\t\tbuf_node = buffer->target_node;\n\t\t\t\tbinder_node_inner_lock(buf_node);\n\t\t\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\t\t\tBUG_ON(buf_node->proc != proc);\n\t\t\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t\t\t&buf_node->async_todo);\n\t\t\t\tif (!w) {\n\t\t\t\t\tbuf_node->has_async_transaction = false;\n\t\t\t\t} else {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\tw, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t\tbinder_node_inner_unlock(buf_node);\n\t\t\t}\n\t\t\ttrace_binder_transaction_buffer_release(buffer);\n\t\t\tbinder_transaction_buffer_release(proc, buffer, NULL);\n\t\t\tbinder_alloc_free_buf(&proc->alloc, buffer);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\t/*\n\t\t\t\t * Allocate memory for death notification\n\t\t\t\t * before taking lock\n\t\t\t\t */\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tbinder_enqueue_thread_work(\n\t\t\t\t\t\tthread,\n\t\t\t\t\t\t&thread->return_error.work);\n\t\t\t\t\tbinder_debug(\n\t\t\t\t\t\tBINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t\"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->data.debug_id,\n\t\t\t\t     ref->data.desc, ref->data.strong,\n\t\t\t\t     ref->data.weak, ref->node->debug_id);\n\n\t\t\tbinder_node_lock(ref->node);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tkfree(death);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\n\t\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t&ref->death->work, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper &\n\t\t\t\t\t    (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t     BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\t\t\tthread,\n\t\t\t\t\t\t\t\t&death->work);\n\t\t\t\t\telse {\n\t\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\t\tbinder_wakeup_proc_ilocked(\n\t\t\t\t\t\t\t\tproc);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tbinder_node_unlock(ref->node);\n\t\t\tbinder_proc_unlock(proc);\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death,\n\t\t\t\t\t    entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death =\n\t\t\t\t\tcontainer_of(w,\n\t\t\t\t\t\t     struct binder_ref_death,\n\t\t\t\t\t\t     work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_dequeue_work_ilocked(&death->work);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper &\n\t\t\t\t\t(BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\tthread, &death->work);\n\t\t\t\telse {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -422,7 +422,7 @@\n \t\t\t\t}\n \t\t\t}\n \t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n-\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %p\\n\",\n+\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\",\n \t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n \t\t\t\t     death);\n \t\t\tif (death == NULL) {",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\","
            ],
            "deleted": [
                "\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %p\\n\","
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_transaction_ilocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \"*from *code *flags\" lines in a debugfs file.",
        "id": 1771
    },
    {
        "cve_id": "CVE-2017-2584",
        "code_before_change": "static int emulate_store_desc_ptr(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  void (*get)(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\t      struct desc_ptr *ptr))\n{\n\tstruct desc_ptr desc_ptr;\n\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tctxt->op_bytes = 8;\n\tget(ctxt, &desc_ptr);\n\tif (ctxt->op_bytes == 2) {\n\t\tctxt->op_bytes = 4;\n\t\tdesc_ptr.address &= 0x00ffffff;\n\t}\n\t/* Disable writeback. */\n\tctxt->dst.type = OP_NONE;\n\treturn segmented_write(ctxt, ctxt->dst.addr.mem,\n\t\t\t       &desc_ptr, 2 + ctxt->op_bytes);\n}",
        "code_after_change": "static int emulate_store_desc_ptr(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  void (*get)(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\t      struct desc_ptr *ptr))\n{\n\tstruct desc_ptr desc_ptr;\n\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tctxt->op_bytes = 8;\n\tget(ctxt, &desc_ptr);\n\tif (ctxt->op_bytes == 2) {\n\t\tctxt->op_bytes = 4;\n\t\tdesc_ptr.address &= 0x00ffffff;\n\t}\n\t/* Disable writeback. */\n\tctxt->dst.type = OP_NONE;\n\treturn segmented_write_std(ctxt, ctxt->dst.addr.mem,\n\t\t\t\t   &desc_ptr, 2 + ctxt->op_bytes);\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,6 @@\n \t}\n \t/* Disable writeback. */\n \tctxt->dst.type = OP_NONE;\n-\treturn segmented_write(ctxt, ctxt->dst.addr.mem,\n-\t\t\t       &desc_ptr, 2 + ctxt->op_bytes);\n+\treturn segmented_write_std(ctxt, ctxt->dst.addr.mem,\n+\t\t\t\t   &desc_ptr, 2 + ctxt->op_bytes);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn segmented_write_std(ctxt, ctxt->dst.addr.mem,",
                "\t\t\t\t   &desc_ptr, 2 + ctxt->op_bytes);"
            ],
            "deleted": [
                "\treturn segmented_write(ctxt, ctxt->dst.addr.mem,",
                "\t\t\t       &desc_ptr, 2 + ctxt->op_bytes);"
            ]
        },
        "cwe": [
            "CWE-200",
            "CWE-416"
        ],
        "cve_description": "arch/x86/kvm/emulate.c in the Linux kernel through 4.9.3 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free) via a crafted application that leverages instruction emulation for fxrstor, fxsave, sgdt, and sidt.",
        "id": 1445
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "static void __cpuinit mxcsr_feature_mask_init(void)\n{\n\tunsigned long mask = 0;\n\n\tclts();\n\tif (cpu_has_fxsr) {\n\t\tmemset(&fx_scratch, 0, sizeof(struct i387_fxsave_struct));\n\t\tasm volatile(\"fxsave %0\" : : \"m\" (fx_scratch));\n\t\tmask = fx_scratch.mxcsr_mask;\n\t\tif (mask == 0)\n\t\t\tmask = 0x0000ffbf;\n\t}\n\tmxcsr_feature_mask &= mask;\n\tstts();\n}",
        "code_after_change": "static void __cpuinit mxcsr_feature_mask_init(void)\n{\n\tunsigned long mask = 0;\n\n\tif (cpu_has_fxsr) {\n\t\tmemset(&fx_scratch, 0, sizeof(struct i387_fxsave_struct));\n\t\tasm volatile(\"fxsave %0\" : : \"m\" (fx_scratch));\n\t\tmask = fx_scratch.mxcsr_mask;\n\t\tif (mask == 0)\n\t\t\tmask = 0x0000ffbf;\n\t}\n\tmxcsr_feature_mask &= mask;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,7 +2,6 @@\n {\n \tunsigned long mask = 0;\n \n-\tclts();\n \tif (cpu_has_fxsr) {\n \t\tmemset(&fx_scratch, 0, sizeof(struct i387_fxsave_struct));\n \t\tasm volatile(\"fxsave %0\" : : \"m\" (fx_scratch));\n@@ -11,5 +10,4 @@\n \t\t\tmask = 0x0000ffbf;\n \t}\n \tmxcsr_feature_mask &= mask;\n-\tstts();\n }",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\tclts();",
                "\tstts();"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1806
    },
    {
        "cve_id": "CVE-2012-6538",
        "code_before_change": "static int copy_to_user_auth(struct xfrm_algo_auth *auth, struct sk_buff *skb)\n{\n\tstruct xfrm_algo *algo;\n\tstruct nlattr *nla;\n\n\tnla = nla_reserve(skb, XFRMA_ALG_AUTH,\n\t\t\t  sizeof(*algo) + (auth->alg_key_len + 7) / 8);\n\tif (!nla)\n\t\treturn -EMSGSIZE;\n\n\talgo = nla_data(nla);\n\tstrcpy(algo->alg_name, auth->alg_name);\n\tmemcpy(algo->alg_key, auth->alg_key, (auth->alg_key_len + 7) / 8);\n\talgo->alg_key_len = auth->alg_key_len;\n\n\treturn 0;\n}",
        "code_after_change": "static int copy_to_user_auth(struct xfrm_algo_auth *auth, struct sk_buff *skb)\n{\n\tstruct xfrm_algo *algo;\n\tstruct nlattr *nla;\n\n\tnla = nla_reserve(skb, XFRMA_ALG_AUTH,\n\t\t\t  sizeof(*algo) + (auth->alg_key_len + 7) / 8);\n\tif (!nla)\n\t\treturn -EMSGSIZE;\n\n\talgo = nla_data(nla);\n\tstrncpy(algo->alg_name, auth->alg_name, sizeof(algo->alg_name));\n\tmemcpy(algo->alg_key, auth->alg_key, (auth->alg_key_len + 7) / 8);\n\talgo->alg_key_len = auth->alg_key_len;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \t\treturn -EMSGSIZE;\n \n \talgo = nla_data(nla);\n-\tstrcpy(algo->alg_name, auth->alg_name);\n+\tstrncpy(algo->alg_name, auth->alg_name, sizeof(algo->alg_name));\n \tmemcpy(algo->alg_key, auth->alg_key, (auth->alg_key_len + 7) / 8);\n \talgo->alg_key_len = auth->alg_key_len;\n ",
        "function_modified_lines": {
            "added": [
                "\tstrncpy(algo->alg_name, auth->alg_name, sizeof(algo->alg_name));"
            ],
            "deleted": [
                "\tstrcpy(algo->alg_name, auth->alg_name);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The copy_to_user_auth function in net/xfrm/xfrm_user.c in the Linux kernel before 3.6 uses an incorrect C library function for copying a string, which allows local users to obtain sensitive information from kernel heap memory by leveraging the CAP_NET_ADMIN capability.",
        "id": 122
    },
    {
        "cve_id": "CVE-2017-14140",
        "code_before_change": " */\nSYSCALL_DEFINE6(move_pages, pid_t, pid, unsigned long, nr_pages,\n\t\tconst void __user * __user *, pages,\n\t\tconst int __user *, nodes,\n\t\tint __user *, status, int, flags)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct task_struct *task;\n\tstruct mm_struct *mm;\n\tint err;\n\tnodemask_t task_nodes;\n\n\t/* Check flags */\n\tif (flags & ~(MPOL_MF_MOVE|MPOL_MF_MOVE_ALL))\n\t\treturn -EINVAL;\n\n\tif ((flags & MPOL_MF_MOVE_ALL) && !capable(CAP_SYS_NICE))\n\t\treturn -EPERM;\n\n\t/* Find the mm_struct */\n\trcu_read_lock();\n\ttask = pid ? find_task_by_vpid(pid) : current;\n\tif (!task) {\n\t\trcu_read_unlock();\n\t\treturn -ESRCH;\n\t}\n\tget_task_struct(task);\n\n\t/*\n\t * Check if this process has the right to modify the specified\n\t * process. The right exists if the process has administrative\n\t * capabilities, superuser privileges or the same\n\t * userid as the target process.\n\t */\n\ttcred = __task_cred(task);\n\tif (!uid_eq(cred->euid, tcred->suid) && !uid_eq(cred->euid, tcred->uid) &&\n\t    !uid_eq(cred->uid,  tcred->suid) && !uid_eq(cred->uid,  tcred->uid) &&\n\t    !capable(CAP_SYS_NICE)) {\n\t\trcu_read_unlock();\n\t\terr = -EPERM;\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n \terr = security_task_movememory(task);\n \tif (err)\n\t\tgoto out;\n\n\ttask_nodes = cpuset_mems_allowed(task);\n\tmm = get_task_mm(task);\n\tput_task_struct(task);\n\n\tif (!mm)\n\t\treturn -EINVAL;\n\n\tif (nodes)\n\t\terr = do_pages_move(mm, task_nodes, nr_pages, pages,\n\t\t\t\t    nodes, status, flags);\n\telse\n\t\terr = do_pages_stat(mm, nr_pages, pages, status);\n\n\tmmput(mm);\n\treturn err;\n\nout:\n\tput_task_struct(task);\n\treturn err;\n}",
        "code_after_change": " */\nSYSCALL_DEFINE6(move_pages, pid_t, pid, unsigned long, nr_pages,\n\t\tconst void __user * __user *, pages,\n\t\tconst int __user *, nodes,\n\t\tint __user *, status, int, flags)\n{\n\tstruct task_struct *task;\n\tstruct mm_struct *mm;\n\tint err;\n\tnodemask_t task_nodes;\n\n\t/* Check flags */\n\tif (flags & ~(MPOL_MF_MOVE|MPOL_MF_MOVE_ALL))\n\t\treturn -EINVAL;\n\n\tif ((flags & MPOL_MF_MOVE_ALL) && !capable(CAP_SYS_NICE))\n\t\treturn -EPERM;\n\n\t/* Find the mm_struct */\n\trcu_read_lock();\n\ttask = pid ? find_task_by_vpid(pid) : current;\n\tif (!task) {\n\t\trcu_read_unlock();\n\t\treturn -ESRCH;\n\t}\n\tget_task_struct(task);\n\n\t/*\n\t * Check if this process has the right to modify the specified\n\t * process. Use the regular \"ptrace_may_access()\" checks.\n\t */\n\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {\n\t\trcu_read_unlock();\n\t\terr = -EPERM;\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n \terr = security_task_movememory(task);\n \tif (err)\n\t\tgoto out;\n\n\ttask_nodes = cpuset_mems_allowed(task);\n\tmm = get_task_mm(task);\n\tput_task_struct(task);\n\n\tif (!mm)\n\t\treturn -EINVAL;\n\n\tif (nodes)\n\t\terr = do_pages_move(mm, task_nodes, nr_pages, pages,\n\t\t\t\t    nodes, status, flags);\n\telse\n\t\terr = do_pages_stat(mm, nr_pages, pages, status);\n\n\tmmput(mm);\n\treturn err;\n\nout:\n\tput_task_struct(task);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,6 @@\n \t\tconst int __user *, nodes,\n \t\tint __user *, status, int, flags)\n {\n-\tconst struct cred *cred = current_cred(), *tcred;\n \tstruct task_struct *task;\n \tstruct mm_struct *mm;\n \tint err;\n@@ -28,14 +27,9 @@\n \n \t/*\n \t * Check if this process has the right to modify the specified\n-\t * process. The right exists if the process has administrative\n-\t * capabilities, superuser privileges or the same\n-\t * userid as the target process.\n+\t * process. Use the regular \"ptrace_may_access()\" checks.\n \t */\n-\ttcred = __task_cred(task);\n-\tif (!uid_eq(cred->euid, tcred->suid) && !uid_eq(cred->euid, tcred->uid) &&\n-\t    !uid_eq(cred->uid,  tcred->suid) && !uid_eq(cred->uid,  tcred->uid) &&\n-\t    !capable(CAP_SYS_NICE)) {\n+\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {\n \t\trcu_read_unlock();\n \t\terr = -EPERM;\n \t\tgoto out;",
        "function_modified_lines": {
            "added": [
                "\t * process. Use the regular \"ptrace_may_access()\" checks.",
                "\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {"
            ],
            "deleted": [
                "\tconst struct cred *cred = current_cred(), *tcred;",
                "\t * process. The right exists if the process has administrative",
                "\t * capabilities, superuser privileges or the same",
                "\t * userid as the target process.",
                "\ttcred = __task_cred(task);",
                "\tif (!uid_eq(cred->euid, tcred->suid) && !uid_eq(cred->euid, tcred->uid) &&",
                "\t    !uid_eq(cred->uid,  tcred->suid) && !uid_eq(cred->uid,  tcred->uid) &&",
                "\t    !capable(CAP_SYS_NICE)) {"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The move_pages system call in mm/migrate.c in the Linux kernel before 4.12.9 doesn't check the effective uid of the target process, enabling a local attacker to learn the memory layout of a setuid executable despite ASLR.",
        "id": 1282
    },
    {
        "cve_id": "CVE-2017-10911",
        "code_before_change": "static void make_response(struct xen_blkif_ring *ring, u64 id,\n\t\t\t  unsigned short op, int st)\n{\n\tstruct blkif_response  resp;\n\tunsigned long     flags;\n\tunion blkif_back_rings *blk_rings;\n\tint notify;\n\n\tresp.id        = id;\n\tresp.operation = op;\n\tresp.status    = st;\n\n\tspin_lock_irqsave(&ring->blk_ring_lock, flags);\n\tblk_rings = &ring->blk_rings;\n\t/* Place on the response ring for the relevant domain. */\n\tswitch (ring->blkif->blk_protocol) {\n\tcase BLKIF_PROTOCOL_NATIVE:\n\t\tmemcpy(RING_GET_RESPONSE(&blk_rings->native, blk_rings->native.rsp_prod_pvt),\n\t\t       &resp, sizeof(resp));\n\t\tbreak;\n\tcase BLKIF_PROTOCOL_X86_32:\n\t\tmemcpy(RING_GET_RESPONSE(&blk_rings->x86_32, blk_rings->x86_32.rsp_prod_pvt),\n\t\t       &resp, sizeof(resp));\n\t\tbreak;\n\tcase BLKIF_PROTOCOL_X86_64:\n\t\tmemcpy(RING_GET_RESPONSE(&blk_rings->x86_64, blk_rings->x86_64.rsp_prod_pvt),\n\t\t       &resp, sizeof(resp));\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\tblk_rings->common.rsp_prod_pvt++;\n\tRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&blk_rings->common, notify);\n\tspin_unlock_irqrestore(&ring->blk_ring_lock, flags);\n\tif (notify)\n\t\tnotify_remote_via_irq(ring->irq);\n}",
        "code_after_change": "static void make_response(struct xen_blkif_ring *ring, u64 id,\n\t\t\t  unsigned short op, int st)\n{\n\tstruct blkif_response *resp;\n\tunsigned long     flags;\n\tunion blkif_back_rings *blk_rings;\n\tint notify;\n\n\tspin_lock_irqsave(&ring->blk_ring_lock, flags);\n\tblk_rings = &ring->blk_rings;\n\t/* Place on the response ring for the relevant domain. */\n\tswitch (ring->blkif->blk_protocol) {\n\tcase BLKIF_PROTOCOL_NATIVE:\n\t\tresp = RING_GET_RESPONSE(&blk_rings->native,\n\t\t\t\t\t blk_rings->native.rsp_prod_pvt);\n\t\tbreak;\n\tcase BLKIF_PROTOCOL_X86_32:\n\t\tresp = RING_GET_RESPONSE(&blk_rings->x86_32,\n\t\t\t\t\t blk_rings->x86_32.rsp_prod_pvt);\n\t\tbreak;\n\tcase BLKIF_PROTOCOL_X86_64:\n\t\tresp = RING_GET_RESPONSE(&blk_rings->x86_64,\n\t\t\t\t\t blk_rings->x86_64.rsp_prod_pvt);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tresp->id        = id;\n\tresp->operation = op;\n\tresp->status    = st;\n\n\tblk_rings->common.rsp_prod_pvt++;\n\tRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&blk_rings->common, notify);\n\tspin_unlock_irqrestore(&ring->blk_ring_lock, flags);\n\tif (notify)\n\t\tnotify_remote_via_irq(ring->irq);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,34 +1,35 @@\n static void make_response(struct xen_blkif_ring *ring, u64 id,\n \t\t\t  unsigned short op, int st)\n {\n-\tstruct blkif_response  resp;\n+\tstruct blkif_response *resp;\n \tunsigned long     flags;\n \tunion blkif_back_rings *blk_rings;\n \tint notify;\n-\n-\tresp.id        = id;\n-\tresp.operation = op;\n-\tresp.status    = st;\n \n \tspin_lock_irqsave(&ring->blk_ring_lock, flags);\n \tblk_rings = &ring->blk_rings;\n \t/* Place on the response ring for the relevant domain. */\n \tswitch (ring->blkif->blk_protocol) {\n \tcase BLKIF_PROTOCOL_NATIVE:\n-\t\tmemcpy(RING_GET_RESPONSE(&blk_rings->native, blk_rings->native.rsp_prod_pvt),\n-\t\t       &resp, sizeof(resp));\n+\t\tresp = RING_GET_RESPONSE(&blk_rings->native,\n+\t\t\t\t\t blk_rings->native.rsp_prod_pvt);\n \t\tbreak;\n \tcase BLKIF_PROTOCOL_X86_32:\n-\t\tmemcpy(RING_GET_RESPONSE(&blk_rings->x86_32, blk_rings->x86_32.rsp_prod_pvt),\n-\t\t       &resp, sizeof(resp));\n+\t\tresp = RING_GET_RESPONSE(&blk_rings->x86_32,\n+\t\t\t\t\t blk_rings->x86_32.rsp_prod_pvt);\n \t\tbreak;\n \tcase BLKIF_PROTOCOL_X86_64:\n-\t\tmemcpy(RING_GET_RESPONSE(&blk_rings->x86_64, blk_rings->x86_64.rsp_prod_pvt),\n-\t\t       &resp, sizeof(resp));\n+\t\tresp = RING_GET_RESPONSE(&blk_rings->x86_64,\n+\t\t\t\t\t blk_rings->x86_64.rsp_prod_pvt);\n \t\tbreak;\n \tdefault:\n \t\tBUG();\n \t}\n+\n+\tresp->id        = id;\n+\tresp->operation = op;\n+\tresp->status    = st;\n+\n \tblk_rings->common.rsp_prod_pvt++;\n \tRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&blk_rings->common, notify);\n \tspin_unlock_irqrestore(&ring->blk_ring_lock, flags);",
        "function_modified_lines": {
            "added": [
                "\tstruct blkif_response *resp;",
                "\t\tresp = RING_GET_RESPONSE(&blk_rings->native,",
                "\t\t\t\t\t blk_rings->native.rsp_prod_pvt);",
                "\t\tresp = RING_GET_RESPONSE(&blk_rings->x86_32,",
                "\t\t\t\t\t blk_rings->x86_32.rsp_prod_pvt);",
                "\t\tresp = RING_GET_RESPONSE(&blk_rings->x86_64,",
                "\t\t\t\t\t blk_rings->x86_64.rsp_prod_pvt);",
                "",
                "\tresp->id        = id;",
                "\tresp->operation = op;",
                "\tresp->status    = st;",
                ""
            ],
            "deleted": [
                "\tstruct blkif_response  resp;",
                "",
                "\tresp.id        = id;",
                "\tresp.operation = op;",
                "\tresp.status    = st;",
                "\t\tmemcpy(RING_GET_RESPONSE(&blk_rings->native, blk_rings->native.rsp_prod_pvt),",
                "\t\t       &resp, sizeof(resp));",
                "\t\tmemcpy(RING_GET_RESPONSE(&blk_rings->x86_32, blk_rings->x86_32.rsp_prod_pvt),",
                "\t\t       &resp, sizeof(resp));",
                "\t\tmemcpy(RING_GET_RESPONSE(&blk_rings->x86_64, blk_rings->x86_64.rsp_prod_pvt),",
                "\t\t       &resp, sizeof(resp));"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The make_response function in drivers/block/xen-blkback/blkback.c in the Linux kernel before 4.11.8 allows guest OS users to obtain sensitive information from host OS (or other guest OS) kernel memory by leveraging the copying of uninitialized padding fields in Xen block-interface response structures, aka XSA-216.",
        "id": 1248
    },
    {
        "cve_id": "CVE-2012-4530",
        "code_before_change": "int search_binary_handler(struct linux_binprm *bprm)\n{\n\tunsigned int depth = bprm->recursion_depth;\n\tint try,retval;\n\tstruct linux_binfmt *fmt;\n\tpid_t old_pid, old_vpid;\n\n\tretval = security_bprm_check(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = audit_bprm(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\t/* Need to fetch pid before load_binary changes it */\n\told_pid = current->pid;\n\trcu_read_lock();\n\told_vpid = task_pid_nr_ns(current, task_active_pid_ns(current->parent));\n\trcu_read_unlock();\n\n\tretval = -ENOENT;\n\tfor (try=0; try<2; try++) {\n\t\tread_lock(&binfmt_lock);\n\t\tlist_for_each_entry(fmt, &formats, lh) {\n\t\t\tint (*fn)(struct linux_binprm *) = fmt->load_binary;\n\t\t\tif (!fn)\n\t\t\t\tcontinue;\n\t\t\tif (!try_module_get(fmt->module))\n\t\t\t\tcontinue;\n\t\t\tread_unlock(&binfmt_lock);\n\t\t\tretval = fn(bprm);\n\t\t\t/*\n\t\t\t * Restore the depth counter to its starting value\n\t\t\t * in this call, so we don't have to rely on every\n\t\t\t * load_binary function to restore it on return.\n\t\t\t */\n\t\t\tbprm->recursion_depth = depth;\n\t\t\tif (retval >= 0) {\n\t\t\t\tif (depth == 0) {\n\t\t\t\t\ttrace_sched_process_exec(current, old_pid, bprm);\n\t\t\t\t\tptrace_event(PTRACE_EVENT_EXEC, old_vpid);\n\t\t\t\t}\n\t\t\t\tput_binfmt(fmt);\n\t\t\t\tallow_write_access(bprm->file);\n\t\t\t\tif (bprm->file)\n\t\t\t\t\tfput(bprm->file);\n\t\t\t\tbprm->file = NULL;\n\t\t\t\tcurrent->did_exec = 1;\n\t\t\t\tproc_exec_connector(current);\n\t\t\t\treturn retval;\n\t\t\t}\n\t\t\tread_lock(&binfmt_lock);\n\t\t\tput_binfmt(fmt);\n\t\t\tif (retval != -ENOEXEC || bprm->mm == NULL)\n\t\t\t\tbreak;\n\t\t\tif (!bprm->file) {\n\t\t\t\tread_unlock(&binfmt_lock);\n\t\t\t\treturn retval;\n\t\t\t}\n\t\t}\n\t\tread_unlock(&binfmt_lock);\n#ifdef CONFIG_MODULES\n\t\tif (retval != -ENOEXEC || bprm->mm == NULL) {\n\t\t\tbreak;\n\t\t} else {\n#define printable(c) (((c)=='\\t') || ((c)=='\\n') || (0x20<=(c) && (c)<=0x7e))\n\t\t\tif (printable(bprm->buf[0]) &&\n\t\t\t    printable(bprm->buf[1]) &&\n\t\t\t    printable(bprm->buf[2]) &&\n\t\t\t    printable(bprm->buf[3]))\n\t\t\t\tbreak; /* -ENOEXEC */\n\t\t\tif (try)\n\t\t\t\tbreak; /* -ENOEXEC */\n\t\t\trequest_module(\"binfmt-%04x\", *(unsigned short *)(&bprm->buf[2]));\n\t\t}\n#else\n\t\tbreak;\n#endif\n\t}\n\treturn retval;\n}",
        "code_after_change": "int search_binary_handler(struct linux_binprm *bprm)\n{\n\tunsigned int depth = bprm->recursion_depth;\n\tint try,retval;\n\tstruct linux_binfmt *fmt;\n\tpid_t old_pid, old_vpid;\n\n\t/* This allows 4 levels of binfmt rewrites before failing hard. */\n\tif (depth > 5)\n\t\treturn -ELOOP;\n\n\tretval = security_bprm_check(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = audit_bprm(bprm);\n\tif (retval)\n\t\treturn retval;\n\n\t/* Need to fetch pid before load_binary changes it */\n\told_pid = current->pid;\n\trcu_read_lock();\n\told_vpid = task_pid_nr_ns(current, task_active_pid_ns(current->parent));\n\trcu_read_unlock();\n\n\tretval = -ENOENT;\n\tfor (try=0; try<2; try++) {\n\t\tread_lock(&binfmt_lock);\n\t\tlist_for_each_entry(fmt, &formats, lh) {\n\t\t\tint (*fn)(struct linux_binprm *) = fmt->load_binary;\n\t\t\tif (!fn)\n\t\t\t\tcontinue;\n\t\t\tif (!try_module_get(fmt->module))\n\t\t\t\tcontinue;\n\t\t\tread_unlock(&binfmt_lock);\n\t\t\tbprm->recursion_depth = depth + 1;\n\t\t\tretval = fn(bprm);\n\t\t\tbprm->recursion_depth = depth;\n\t\t\tif (retval >= 0) {\n\t\t\t\tif (depth == 0) {\n\t\t\t\t\ttrace_sched_process_exec(current, old_pid, bprm);\n\t\t\t\t\tptrace_event(PTRACE_EVENT_EXEC, old_vpid);\n\t\t\t\t}\n\t\t\t\tput_binfmt(fmt);\n\t\t\t\tallow_write_access(bprm->file);\n\t\t\t\tif (bprm->file)\n\t\t\t\t\tfput(bprm->file);\n\t\t\t\tbprm->file = NULL;\n\t\t\t\tcurrent->did_exec = 1;\n\t\t\t\tproc_exec_connector(current);\n\t\t\t\treturn retval;\n\t\t\t}\n\t\t\tread_lock(&binfmt_lock);\n\t\t\tput_binfmt(fmt);\n\t\t\tif (retval != -ENOEXEC || bprm->mm == NULL)\n\t\t\t\tbreak;\n\t\t\tif (!bprm->file) {\n\t\t\t\tread_unlock(&binfmt_lock);\n\t\t\t\treturn retval;\n\t\t\t}\n\t\t}\n\t\tread_unlock(&binfmt_lock);\n#ifdef CONFIG_MODULES\n\t\tif (retval != -ENOEXEC || bprm->mm == NULL) {\n\t\t\tbreak;\n\t\t} else {\n#define printable(c) (((c)=='\\t') || ((c)=='\\n') || (0x20<=(c) && (c)<=0x7e))\n\t\t\tif (printable(bprm->buf[0]) &&\n\t\t\t    printable(bprm->buf[1]) &&\n\t\t\t    printable(bprm->buf[2]) &&\n\t\t\t    printable(bprm->buf[3]))\n\t\t\t\tbreak; /* -ENOEXEC */\n\t\t\tif (try)\n\t\t\t\tbreak; /* -ENOEXEC */\n\t\t\trequest_module(\"binfmt-%04x\", *(unsigned short *)(&bprm->buf[2]));\n\t\t}\n#else\n\t\tbreak;\n#endif\n\t}\n\treturn retval;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,10 @@\n \tint try,retval;\n \tstruct linux_binfmt *fmt;\n \tpid_t old_pid, old_vpid;\n+\n+\t/* This allows 4 levels of binfmt rewrites before failing hard. */\n+\tif (depth > 5)\n+\t\treturn -ELOOP;\n \n \tretval = security_bprm_check(bprm);\n \tif (retval)\n@@ -29,12 +33,8 @@\n \t\t\tif (!try_module_get(fmt->module))\n \t\t\t\tcontinue;\n \t\t\tread_unlock(&binfmt_lock);\n+\t\t\tbprm->recursion_depth = depth + 1;\n \t\t\tretval = fn(bprm);\n-\t\t\t/*\n-\t\t\t * Restore the depth counter to its starting value\n-\t\t\t * in this call, so we don't have to rely on every\n-\t\t\t * load_binary function to restore it on return.\n-\t\t\t */\n \t\t\tbprm->recursion_depth = depth;\n \t\t\tif (retval >= 0) {\n \t\t\t\tif (depth == 0) {",
        "function_modified_lines": {
            "added": [
                "",
                "\t/* This allows 4 levels of binfmt rewrites before failing hard. */",
                "\tif (depth > 5)",
                "\t\treturn -ELOOP;",
                "\t\t\tbprm->recursion_depth = depth + 1;"
            ],
            "deleted": [
                "\t\t\t/*",
                "\t\t\t * Restore the depth counter to its starting value",
                "\t\t\t * in this call, so we don't have to rely on every",
                "\t\t\t * load_binary function to restore it on return.",
                "\t\t\t */"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The load_script function in fs/binfmt_script.c in the Linux kernel before 3.7.2 does not properly handle recursion, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 113
    },
    {
        "cve_id": "CVE-2014-9903",
        "code_before_change": "static int sched_read_attr(struct sched_attr __user *uattr,\n\t\t\t   struct sched_attr *attr,\n\t\t\t   unsigned int usize)\n{\n\tint ret;\n\n\tif (!access_ok(VERIFY_WRITE, uattr, usize))\n\t\treturn -EFAULT;\n\n\t/*\n\t * If we're handed a smaller struct than we know of,\n\t * ensure all the unknown bits are 0 - i.e. old\n\t * user-space does not get uncomplete information.\n\t */\n\tif (usize < sizeof(*attr)) {\n\t\tunsigned char *addr;\n\t\tunsigned char *end;\n\n\t\taddr = (void *)attr + usize;\n\t\tend  = (void *)attr + sizeof(*attr);\n\n\t\tfor (; addr < end; addr++) {\n\t\t\tif (*addr)\n\t\t\t\tgoto err_size;\n\t\t}\n\n\t\tattr->size = usize;\n\t}\n\n\tret = copy_to_user(uattr, attr, usize);\n\tif (ret)\n\t\treturn -EFAULT;\n\nout:\n\treturn ret;\n\nerr_size:\n\tret = -E2BIG;\n\tgoto out;\n}",
        "code_after_change": "static int sched_read_attr(struct sched_attr __user *uattr,\n\t\t\t   struct sched_attr *attr,\n\t\t\t   unsigned int usize)\n{\n\tint ret;\n\n\tif (!access_ok(VERIFY_WRITE, uattr, usize))\n\t\treturn -EFAULT;\n\n\t/*\n\t * If we're handed a smaller struct than we know of,\n\t * ensure all the unknown bits are 0 - i.e. old\n\t * user-space does not get uncomplete information.\n\t */\n\tif (usize < sizeof(*attr)) {\n\t\tunsigned char *addr;\n\t\tunsigned char *end;\n\n\t\taddr = (void *)attr + usize;\n\t\tend  = (void *)attr + sizeof(*attr);\n\n\t\tfor (; addr < end; addr++) {\n\t\t\tif (*addr)\n\t\t\t\tgoto err_size;\n\t\t}\n\n\t\tattr->size = usize;\n\t}\n\n\tret = copy_to_user(uattr, attr, attr->size);\n\tif (ret)\n\t\treturn -EFAULT;\n\nout:\n\treturn ret;\n\nerr_size:\n\tret = -E2BIG;\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,7 +27,7 @@\n \t\tattr->size = usize;\n \t}\n \n-\tret = copy_to_user(uattr, attr, usize);\n+\tret = copy_to_user(uattr, attr, attr->size);\n \tif (ret)\n \t\treturn -EFAULT;\n ",
        "function_modified_lines": {
            "added": [
                "\tret = copy_to_user(uattr, attr, attr->size);"
            ],
            "deleted": [
                "\tret = copy_to_user(uattr, attr, usize);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The sched_read_attr function in kernel/sched/core.c in the Linux kernel 3.14-rc before 3.14-rc4 uses an incorrect size, which allows local users to obtain sensitive information from kernel stack memory via a crafted sched_getattr system call.",
        "id": 709
    },
    {
        "cve_id": "CVE-2018-6412",
        "code_before_change": "int sbusfb_ioctl_helper(unsigned long cmd, unsigned long arg,\n\t\t\tstruct fb_info *info,\n\t\t\tint type, int fb_depth, unsigned long fb_size)\n{\n\tswitch(cmd) {\n\tcase FBIOGTYPE: {\n\t\tstruct fbtype __user *f = (struct fbtype __user *) arg;\n\n\t\tif (put_user(type, &f->fb_type) ||\n\t\t    __put_user(info->var.yres, &f->fb_height) ||\n\t\t    __put_user(info->var.xres, &f->fb_width) ||\n\t\t    __put_user(fb_depth, &f->fb_depth) ||\n\t\t    __put_user(0, &f->fb_cmsize) ||\n\t\t    __put_user(fb_size, &f->fb_cmsize))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase FBIOPUTCMAP_SPARC: {\n\t\tstruct fbcmap __user *c = (struct fbcmap __user *) arg;\n\t\tstruct fb_cmap cmap;\n\t\tu16 red, green, blue;\n\t\tu8 red8, green8, blue8;\n\t\tunsigned char __user *ured;\n\t\tunsigned char __user *ugreen;\n\t\tunsigned char __user *ublue;\n\t\tint index, count, i;\n\n\t\tif (get_user(index, &c->index) ||\n\t\t    __get_user(count, &c->count) ||\n\t\t    __get_user(ured, &c->red) ||\n\t\t    __get_user(ugreen, &c->green) ||\n\t\t    __get_user(ublue, &c->blue))\n\t\t\treturn -EFAULT;\n\n\t\tcmap.len = 1;\n\t\tcmap.red = &red;\n\t\tcmap.green = &green;\n\t\tcmap.blue = &blue;\n\t\tcmap.transp = NULL;\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tint err;\n\n\t\t\tif (get_user(red8, &ured[i]) ||\n\t\t\t    get_user(green8, &ugreen[i]) ||\n\t\t\t    get_user(blue8, &ublue[i]))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tred = red8 << 8;\n\t\t\tgreen = green8 << 8;\n\t\t\tblue = blue8 << 8;\n\n\t\t\tcmap.start = index + i;\n\t\t\terr = fb_set_cmap(&cmap, info);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\treturn 0;\n\t}\n\tcase FBIOGETCMAP_SPARC: {\n\t\tstruct fbcmap __user *c = (struct fbcmap __user *) arg;\n\t\tunsigned char __user *ured;\n\t\tunsigned char __user *ugreen;\n\t\tunsigned char __user *ublue;\n\t\tstruct fb_cmap *cmap = &info->cmap;\n\t\tint index, count, i;\n\t\tu8 red, green, blue;\n\n\t\tif (get_user(index, &c->index) ||\n\t\t    __get_user(count, &c->count) ||\n\t\t    __get_user(ured, &c->red) ||\n\t\t    __get_user(ugreen, &c->green) ||\n\t\t    __get_user(ublue, &c->blue))\n\t\t\treturn -EFAULT;\n\n\t\tif (index + count > cmap->len)\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tred = cmap->red[index + i] >> 8;\n\t\t\tgreen = cmap->green[index + i] >> 8;\n\t\t\tblue = cmap->blue[index + i] >> 8;\n\t\t\tif (put_user(red, &ured[i]) ||\n\t\t\t    put_user(green, &ugreen[i]) ||\n\t\t\t    put_user(blue, &ublue[i]))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
        "code_after_change": "int sbusfb_ioctl_helper(unsigned long cmd, unsigned long arg,\n\t\t\tstruct fb_info *info,\n\t\t\tint type, int fb_depth, unsigned long fb_size)\n{\n\tswitch(cmd) {\n\tcase FBIOGTYPE: {\n\t\tstruct fbtype __user *f = (struct fbtype __user *) arg;\n\n\t\tif (put_user(type, &f->fb_type) ||\n\t\t    __put_user(info->var.yres, &f->fb_height) ||\n\t\t    __put_user(info->var.xres, &f->fb_width) ||\n\t\t    __put_user(fb_depth, &f->fb_depth) ||\n\t\t    __put_user(0, &f->fb_cmsize) ||\n\t\t    __put_user(fb_size, &f->fb_cmsize))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase FBIOPUTCMAP_SPARC: {\n\t\tstruct fbcmap __user *c = (struct fbcmap __user *) arg;\n\t\tstruct fb_cmap cmap;\n\t\tu16 red, green, blue;\n\t\tu8 red8, green8, blue8;\n\t\tunsigned char __user *ured;\n\t\tunsigned char __user *ugreen;\n\t\tunsigned char __user *ublue;\n\t\tunsigned int index, count, i;\n\n\t\tif (get_user(index, &c->index) ||\n\t\t    __get_user(count, &c->count) ||\n\t\t    __get_user(ured, &c->red) ||\n\t\t    __get_user(ugreen, &c->green) ||\n\t\t    __get_user(ublue, &c->blue))\n\t\t\treturn -EFAULT;\n\n\t\tcmap.len = 1;\n\t\tcmap.red = &red;\n\t\tcmap.green = &green;\n\t\tcmap.blue = &blue;\n\t\tcmap.transp = NULL;\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tint err;\n\n\t\t\tif (get_user(red8, &ured[i]) ||\n\t\t\t    get_user(green8, &ugreen[i]) ||\n\t\t\t    get_user(blue8, &ublue[i]))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tred = red8 << 8;\n\t\t\tgreen = green8 << 8;\n\t\t\tblue = blue8 << 8;\n\n\t\t\tcmap.start = index + i;\n\t\t\terr = fb_set_cmap(&cmap, info);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\treturn 0;\n\t}\n\tcase FBIOGETCMAP_SPARC: {\n\t\tstruct fbcmap __user *c = (struct fbcmap __user *) arg;\n\t\tunsigned char __user *ured;\n\t\tunsigned char __user *ugreen;\n\t\tunsigned char __user *ublue;\n\t\tstruct fb_cmap *cmap = &info->cmap;\n\t\tunsigned int index, count, i;\n\t\tu8 red, green, blue;\n\n\t\tif (get_user(index, &c->index) ||\n\t\t    __get_user(count, &c->count) ||\n\t\t    __get_user(ured, &c->red) ||\n\t\t    __get_user(ugreen, &c->green) ||\n\t\t    __get_user(ublue, &c->blue))\n\t\t\treturn -EFAULT;\n\n\t\tif (index + count > cmap->len)\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tred = cmap->red[index + i] >> 8;\n\t\t\tgreen = cmap->green[index + i] >> 8;\n\t\t\tblue = cmap->blue[index + i] >> 8;\n\t\t\tif (put_user(red, &ured[i]) ||\n\t\t\t    put_user(green, &ugreen[i]) ||\n\t\t\t    put_user(blue, &ublue[i]))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -23,7 +23,7 @@\n \t\tunsigned char __user *ured;\n \t\tunsigned char __user *ugreen;\n \t\tunsigned char __user *ublue;\n-\t\tint index, count, i;\n+\t\tunsigned int index, count, i;\n \n \t\tif (get_user(index, &c->index) ||\n \t\t    __get_user(count, &c->count) ||\n@@ -62,7 +62,7 @@\n \t\tunsigned char __user *ugreen;\n \t\tunsigned char __user *ublue;\n \t\tstruct fb_cmap *cmap = &info->cmap;\n-\t\tint index, count, i;\n+\t\tunsigned int index, count, i;\n \t\tu8 red, green, blue;\n \n \t\tif (get_user(index, &c->index) ||",
        "function_modified_lines": {
            "added": [
                "\t\tunsigned int index, count, i;",
                "\t\tunsigned int index, count, i;"
            ],
            "deleted": [
                "\t\tint index, count, i;",
                "\t\tint index, count, i;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "In the function sbusfb_ioctl_helper() in drivers/video/fbdev/sbuslib.c in the Linux kernel through 4.15, an integer signedness error allows arbitrary information leakage for the FBIOPUTCMAP_SPARC and FBIOGETCMAP_SPARC commands.",
        "id": 1841
    },
    {
        "cve_id": "CVE-2014-4027",
        "code_before_change": "static int rd_build_device_space(struct rd_dev *rd_dev)\n{\n\tu32 i = 0, j, page_offset = 0, sg_per_table, sg_tables, total_sg_needed;\n\tu32 max_sg_per_table = (RD_MAX_ALLOCATION_SIZE /\n\t\t\t\tsizeof(struct scatterlist));\n\tstruct rd_dev_sg_table *sg_table;\n\tstruct page *pg;\n\tstruct scatterlist *sg;\n\n\tif (rd_dev->rd_page_count <= 0) {\n\t\tpr_err(\"Illegal page count: %u for Ramdisk device\\n\",\n\t\t\trd_dev->rd_page_count);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Don't need backing pages for NULLIO */\n\tif (rd_dev->rd_flags & RDF_NULLIO)\n\t\treturn 0;\n\n\ttotal_sg_needed = rd_dev->rd_page_count;\n\n\tsg_tables = (total_sg_needed / max_sg_per_table) + 1;\n\n\tsg_table = kzalloc(sg_tables * sizeof(struct rd_dev_sg_table), GFP_KERNEL);\n\tif (!sg_table) {\n\t\tpr_err(\"Unable to allocate memory for Ramdisk\"\n\t\t\t\" scatterlist tables\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\trd_dev->sg_table_array = sg_table;\n\trd_dev->sg_table_count = sg_tables;\n\n\twhile (total_sg_needed) {\n\t\tsg_per_table = (total_sg_needed > max_sg_per_table) ?\n\t\t\tmax_sg_per_table : total_sg_needed;\n\n\t\tsg = kzalloc(sg_per_table * sizeof(struct scatterlist),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!sg) {\n\t\t\tpr_err(\"Unable to allocate scatterlist array\"\n\t\t\t\t\" for struct rd_dev\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tsg_init_table(sg, sg_per_table);\n\n\t\tsg_table[i].sg_table = sg;\n\t\tsg_table[i].rd_sg_count = sg_per_table;\n\t\tsg_table[i].page_start_offset = page_offset;\n\t\tsg_table[i++].page_end_offset = (page_offset + sg_per_table)\n\t\t\t\t\t\t- 1;\n\n\t\tfor (j = 0; j < sg_per_table; j++) {\n\t\t\tpg = alloc_pages(GFP_KERNEL, 0);\n\t\t\tif (!pg) {\n\t\t\t\tpr_err(\"Unable to allocate scatterlist\"\n\t\t\t\t\t\" pages for struct rd_dev_sg_table\\n\");\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tsg_assign_page(&sg[j], pg);\n\t\t\tsg[j].length = PAGE_SIZE;\n\t\t}\n\n\t\tpage_offset += sg_per_table;\n\t\ttotal_sg_needed -= sg_per_table;\n\t}\n\n\tpr_debug(\"CORE_RD[%u] - Built Ramdisk Device ID: %u space of\"\n\t\t\" %u pages in %u tables\\n\", rd_dev->rd_host->rd_host_id,\n\t\trd_dev->rd_dev_id, rd_dev->rd_page_count,\n\t\trd_dev->sg_table_count);\n\n\treturn 0;\n}",
        "code_after_change": "static int rd_build_device_space(struct rd_dev *rd_dev)\n{\n\tstruct rd_dev_sg_table *sg_table;\n\tu32 sg_tables, total_sg_needed;\n\tu32 max_sg_per_table = (RD_MAX_ALLOCATION_SIZE /\n\t\t\t\tsizeof(struct scatterlist));\n\tint rc;\n\n\tif (rd_dev->rd_page_count <= 0) {\n\t\tpr_err(\"Illegal page count: %u for Ramdisk device\\n\",\n\t\t       rd_dev->rd_page_count);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Don't need backing pages for NULLIO */\n\tif (rd_dev->rd_flags & RDF_NULLIO)\n\t\treturn 0;\n\n\ttotal_sg_needed = rd_dev->rd_page_count;\n\n\tsg_tables = (total_sg_needed / max_sg_per_table) + 1;\n\n\tsg_table = kzalloc(sg_tables * sizeof(struct rd_dev_sg_table), GFP_KERNEL);\n\tif (!sg_table) {\n\t\tpr_err(\"Unable to allocate memory for Ramdisk\"\n\t\t       \" scatterlist tables\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\trd_dev->sg_table_array = sg_table;\n\trd_dev->sg_table_count = sg_tables;\n\n\trc = rd_allocate_sgl_table(rd_dev, sg_table, total_sg_needed, 0x00);\n\tif (rc)\n\t\treturn rc;\n\n\tpr_debug(\"CORE_RD[%u] - Built Ramdisk Device ID: %u space of\"\n\t\t \" %u pages in %u tables\\n\", rd_dev->rd_host->rd_host_id,\n\t\t rd_dev->rd_dev_id, rd_dev->rd_page_count,\n\t\t rd_dev->sg_table_count);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,15 +1,14 @@\n static int rd_build_device_space(struct rd_dev *rd_dev)\n {\n-\tu32 i = 0, j, page_offset = 0, sg_per_table, sg_tables, total_sg_needed;\n+\tstruct rd_dev_sg_table *sg_table;\n+\tu32 sg_tables, total_sg_needed;\n \tu32 max_sg_per_table = (RD_MAX_ALLOCATION_SIZE /\n \t\t\t\tsizeof(struct scatterlist));\n-\tstruct rd_dev_sg_table *sg_table;\n-\tstruct page *pg;\n-\tstruct scatterlist *sg;\n+\tint rc;\n \n \tif (rd_dev->rd_page_count <= 0) {\n \t\tpr_err(\"Illegal page count: %u for Ramdisk device\\n\",\n-\t\t\trd_dev->rd_page_count);\n+\t\t       rd_dev->rd_page_count);\n \t\treturn -EINVAL;\n \t}\n \n@@ -24,52 +23,21 @@\n \tsg_table = kzalloc(sg_tables * sizeof(struct rd_dev_sg_table), GFP_KERNEL);\n \tif (!sg_table) {\n \t\tpr_err(\"Unable to allocate memory for Ramdisk\"\n-\t\t\t\" scatterlist tables\\n\");\n+\t\t       \" scatterlist tables\\n\");\n \t\treturn -ENOMEM;\n \t}\n \n \trd_dev->sg_table_array = sg_table;\n \trd_dev->sg_table_count = sg_tables;\n \n-\twhile (total_sg_needed) {\n-\t\tsg_per_table = (total_sg_needed > max_sg_per_table) ?\n-\t\t\tmax_sg_per_table : total_sg_needed;\n-\n-\t\tsg = kzalloc(sg_per_table * sizeof(struct scatterlist),\n-\t\t\t\tGFP_KERNEL);\n-\t\tif (!sg) {\n-\t\t\tpr_err(\"Unable to allocate scatterlist array\"\n-\t\t\t\t\" for struct rd_dev\\n\");\n-\t\t\treturn -ENOMEM;\n-\t\t}\n-\n-\t\tsg_init_table(sg, sg_per_table);\n-\n-\t\tsg_table[i].sg_table = sg;\n-\t\tsg_table[i].rd_sg_count = sg_per_table;\n-\t\tsg_table[i].page_start_offset = page_offset;\n-\t\tsg_table[i++].page_end_offset = (page_offset + sg_per_table)\n-\t\t\t\t\t\t- 1;\n-\n-\t\tfor (j = 0; j < sg_per_table; j++) {\n-\t\t\tpg = alloc_pages(GFP_KERNEL, 0);\n-\t\t\tif (!pg) {\n-\t\t\t\tpr_err(\"Unable to allocate scatterlist\"\n-\t\t\t\t\t\" pages for struct rd_dev_sg_table\\n\");\n-\t\t\t\treturn -ENOMEM;\n-\t\t\t}\n-\t\t\tsg_assign_page(&sg[j], pg);\n-\t\t\tsg[j].length = PAGE_SIZE;\n-\t\t}\n-\n-\t\tpage_offset += sg_per_table;\n-\t\ttotal_sg_needed -= sg_per_table;\n-\t}\n+\trc = rd_allocate_sgl_table(rd_dev, sg_table, total_sg_needed, 0x00);\n+\tif (rc)\n+\t\treturn rc;\n \n \tpr_debug(\"CORE_RD[%u] - Built Ramdisk Device ID: %u space of\"\n-\t\t\" %u pages in %u tables\\n\", rd_dev->rd_host->rd_host_id,\n-\t\trd_dev->rd_dev_id, rd_dev->rd_page_count,\n-\t\trd_dev->sg_table_count);\n+\t\t \" %u pages in %u tables\\n\", rd_dev->rd_host->rd_host_id,\n+\t\t rd_dev->rd_dev_id, rd_dev->rd_page_count,\n+\t\t rd_dev->sg_table_count);\n \n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct rd_dev_sg_table *sg_table;",
                "\tu32 sg_tables, total_sg_needed;",
                "\tint rc;",
                "\t\t       rd_dev->rd_page_count);",
                "\t\t       \" scatterlist tables\\n\");",
                "\trc = rd_allocate_sgl_table(rd_dev, sg_table, total_sg_needed, 0x00);",
                "\tif (rc)",
                "\t\treturn rc;",
                "\t\t \" %u pages in %u tables\\n\", rd_dev->rd_host->rd_host_id,",
                "\t\t rd_dev->rd_dev_id, rd_dev->rd_page_count,",
                "\t\t rd_dev->sg_table_count);"
            ],
            "deleted": [
                "\tu32 i = 0, j, page_offset = 0, sg_per_table, sg_tables, total_sg_needed;",
                "\tstruct rd_dev_sg_table *sg_table;",
                "\tstruct page *pg;",
                "\tstruct scatterlist *sg;",
                "\t\t\trd_dev->rd_page_count);",
                "\t\t\t\" scatterlist tables\\n\");",
                "\twhile (total_sg_needed) {",
                "\t\tsg_per_table = (total_sg_needed > max_sg_per_table) ?",
                "\t\t\tmax_sg_per_table : total_sg_needed;",
                "",
                "\t\tsg = kzalloc(sg_per_table * sizeof(struct scatterlist),",
                "\t\t\t\tGFP_KERNEL);",
                "\t\tif (!sg) {",
                "\t\t\tpr_err(\"Unable to allocate scatterlist array\"",
                "\t\t\t\t\" for struct rd_dev\\n\");",
                "\t\t\treturn -ENOMEM;",
                "\t\t}",
                "",
                "\t\tsg_init_table(sg, sg_per_table);",
                "",
                "\t\tsg_table[i].sg_table = sg;",
                "\t\tsg_table[i].rd_sg_count = sg_per_table;",
                "\t\tsg_table[i].page_start_offset = page_offset;",
                "\t\tsg_table[i++].page_end_offset = (page_offset + sg_per_table)",
                "\t\t\t\t\t\t- 1;",
                "",
                "\t\tfor (j = 0; j < sg_per_table; j++) {",
                "\t\t\tpg = alloc_pages(GFP_KERNEL, 0);",
                "\t\t\tif (!pg) {",
                "\t\t\t\tpr_err(\"Unable to allocate scatterlist\"",
                "\t\t\t\t\t\" pages for struct rd_dev_sg_table\\n\");",
                "\t\t\t\treturn -ENOMEM;",
                "\t\t\t}",
                "\t\t\tsg_assign_page(&sg[j], pg);",
                "\t\t\tsg[j].length = PAGE_SIZE;",
                "\t\t}",
                "",
                "\t\tpage_offset += sg_per_table;",
                "\t\ttotal_sg_needed -= sg_per_table;",
                "\t}",
                "\t\t\" %u pages in %u tables\\n\", rd_dev->rd_host->rd_host_id,",
                "\t\trd_dev->rd_dev_id, rd_dev->rd_page_count,",
                "\t\trd_dev->sg_table_count);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The rd_build_device_space function in drivers/target/target_core_rd.c in the Linux kernel before 3.14 does not properly initialize a certain data structure, which allows local users to obtain sensitive information from ramdisk_mcp memory by leveraging access to a SCSI initiator.",
        "id": 557
    },
    {
        "cve_id": "CVE-2019-18660",
        "code_before_change": "ssize_t cpu_show_spectre_v2(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct seq_buf s;\n\tbool bcs, ccd;\n\n\tseq_buf_init(&s, buf, PAGE_SIZE - 1);\n\n\tbcs = security_ftr_enabled(SEC_FTR_BCCTRL_SERIALISED);\n\tccd = security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED);\n\n\tif (bcs || ccd) {\n\t\tseq_buf_printf(&s, \"Mitigation: \");\n\n\t\tif (bcs)\n\t\t\tseq_buf_printf(&s, \"Indirect branch serialisation (kernel only)\");\n\n\t\tif (bcs && ccd)\n\t\t\tseq_buf_printf(&s, \", \");\n\n\t\tif (ccd)\n\t\t\tseq_buf_printf(&s, \"Indirect branch cache disabled\");\n\t} else if (count_cache_flush_type != COUNT_CACHE_FLUSH_NONE) {\n\t\tseq_buf_printf(&s, \"Mitigation: Software count cache flush\");\n\n\t\tif (count_cache_flush_type == COUNT_CACHE_FLUSH_HW)\n\t\t\tseq_buf_printf(&s, \" (hardware accelerated)\");\n\t} else if (btb_flush_enabled) {\n\t\tseq_buf_printf(&s, \"Mitigation: Branch predictor state flush\");\n\t} else {\n\t\tseq_buf_printf(&s, \"Vulnerable\");\n\t}\n\n\tseq_buf_printf(&s, \"\\n\");\n\n\treturn s.len;\n}",
        "code_after_change": "ssize_t cpu_show_spectre_v2(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct seq_buf s;\n\tbool bcs, ccd;\n\n\tseq_buf_init(&s, buf, PAGE_SIZE - 1);\n\n\tbcs = security_ftr_enabled(SEC_FTR_BCCTRL_SERIALISED);\n\tccd = security_ftr_enabled(SEC_FTR_COUNT_CACHE_DISABLED);\n\n\tif (bcs || ccd) {\n\t\tseq_buf_printf(&s, \"Mitigation: \");\n\n\t\tif (bcs)\n\t\t\tseq_buf_printf(&s, \"Indirect branch serialisation (kernel only)\");\n\n\t\tif (bcs && ccd)\n\t\t\tseq_buf_printf(&s, \", \");\n\n\t\tif (ccd)\n\t\t\tseq_buf_printf(&s, \"Indirect branch cache disabled\");\n\n\t\tif (link_stack_flush_enabled)\n\t\t\tseq_buf_printf(&s, \", Software link stack flush\");\n\n\t} else if (count_cache_flush_type != COUNT_CACHE_FLUSH_NONE) {\n\t\tseq_buf_printf(&s, \"Mitigation: Software count cache flush\");\n\n\t\tif (count_cache_flush_type == COUNT_CACHE_FLUSH_HW)\n\t\t\tseq_buf_printf(&s, \" (hardware accelerated)\");\n\n\t\tif (link_stack_flush_enabled)\n\t\t\tseq_buf_printf(&s, \", Software link stack flush\");\n\n\t} else if (btb_flush_enabled) {\n\t\tseq_buf_printf(&s, \"Mitigation: Branch predictor state flush\");\n\t} else {\n\t\tseq_buf_printf(&s, \"Vulnerable\");\n\t}\n\n\tseq_buf_printf(&s, \"\\n\");\n\n\treturn s.len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,11 +19,19 @@\n \n \t\tif (ccd)\n \t\t\tseq_buf_printf(&s, \"Indirect branch cache disabled\");\n+\n+\t\tif (link_stack_flush_enabled)\n+\t\t\tseq_buf_printf(&s, \", Software link stack flush\");\n+\n \t} else if (count_cache_flush_type != COUNT_CACHE_FLUSH_NONE) {\n \t\tseq_buf_printf(&s, \"Mitigation: Software count cache flush\");\n \n \t\tif (count_cache_flush_type == COUNT_CACHE_FLUSH_HW)\n \t\t\tseq_buf_printf(&s, \" (hardware accelerated)\");\n+\n+\t\tif (link_stack_flush_enabled)\n+\t\t\tseq_buf_printf(&s, \", Software link stack flush\");\n+\n \t} else if (btb_flush_enabled) {\n \t\tseq_buf_printf(&s, \"Mitigation: Branch predictor state flush\");\n \t} else {",
        "function_modified_lines": {
            "added": [
                "",
                "\t\tif (link_stack_flush_enabled)",
                "\t\t\tseq_buf_printf(&s, \", Software link stack flush\");",
                "",
                "",
                "\t\tif (link_stack_flush_enabled)",
                "\t\t\tseq_buf_printf(&s, \", Software link stack flush\");",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The Linux kernel before 5.4.1 on powerpc allows Information Exposure because the Spectre-RSB mitigation is not in place for all applicable CPUs, aka CID-39e72bf96f58. This is related to arch/powerpc/kernel/entry_64.S and arch/powerpc/kernel/security.c.",
        "id": 2086
    },
    {
        "cve_id": "CVE-2012-6536",
        "code_before_change": "static inline int verify_replay(struct xfrm_usersa_info *p,\n\t\t\t\tstruct nlattr **attrs)\n{\n\tstruct nlattr *rt = attrs[XFRMA_REPLAY_ESN_VAL];\n\n\tif ((p->flags & XFRM_STATE_ESN) && !rt)\n\t\treturn -EINVAL;\n\n\tif (!rt)\n\t\treturn 0;\n\n\tif (p->id.proto != IPPROTO_ESP)\n\t\treturn -EINVAL;\n\n\tif (p->replay_window != 0)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}",
        "code_after_change": "static inline int verify_replay(struct xfrm_usersa_info *p,\n\t\t\t\tstruct nlattr **attrs)\n{\n\tstruct nlattr *rt = attrs[XFRMA_REPLAY_ESN_VAL];\n\tstruct xfrm_replay_state_esn *rs;\n\n\tif (p->flags & XFRM_STATE_ESN) {\n\t\tif (!rt)\n\t\t\treturn -EINVAL;\n\n\t\trs = nla_data(rt);\n\n\t\tif (rs->bmp_len > XFRMA_REPLAY_ESN_MAX / sizeof(rs->bmp[0]) / 8)\n\t\t\treturn -EINVAL;\n\n\t\tif (nla_len(rt) < xfrm_replay_state_esn_len(rs) &&\n\t\t    nla_len(rt) != sizeof(*rs))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!rt)\n\t\treturn 0;\n\n\tif (p->id.proto != IPPROTO_ESP)\n\t\treturn -EINVAL;\n\n\tif (p->replay_window != 0)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,9 +2,21 @@\n \t\t\t\tstruct nlattr **attrs)\n {\n \tstruct nlattr *rt = attrs[XFRMA_REPLAY_ESN_VAL];\n+\tstruct xfrm_replay_state_esn *rs;\n \n-\tif ((p->flags & XFRM_STATE_ESN) && !rt)\n-\t\treturn -EINVAL;\n+\tif (p->flags & XFRM_STATE_ESN) {\n+\t\tif (!rt)\n+\t\t\treturn -EINVAL;\n+\n+\t\trs = nla_data(rt);\n+\n+\t\tif (rs->bmp_len > XFRMA_REPLAY_ESN_MAX / sizeof(rs->bmp[0]) / 8)\n+\t\t\treturn -EINVAL;\n+\n+\t\tif (nla_len(rt) < xfrm_replay_state_esn_len(rs) &&\n+\t\t    nla_len(rt) != sizeof(*rs))\n+\t\t\treturn -EINVAL;\n+\t}\n \n \tif (!rt)\n \t\treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tstruct xfrm_replay_state_esn *rs;",
                "\tif (p->flags & XFRM_STATE_ESN) {",
                "\t\tif (!rt)",
                "\t\t\treturn -EINVAL;",
                "",
                "\t\trs = nla_data(rt);",
                "",
                "\t\tif (rs->bmp_len > XFRMA_REPLAY_ESN_MAX / sizeof(rs->bmp[0]) / 8)",
                "\t\t\treturn -EINVAL;",
                "",
                "\t\tif (nla_len(rt) < xfrm_replay_state_esn_len(rs) &&",
                "\t\t    nla_len(rt) != sizeof(*rs))",
                "\t\t\treturn -EINVAL;",
                "\t}"
            ],
            "deleted": [
                "\tif ((p->flags & XFRM_STATE_ESN) && !rt)",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "net/xfrm/xfrm_user.c in the Linux kernel before 3.6 does not verify that the actual Netlink message length is consistent with a certain header field, which allows local users to obtain sensitive information from kernel heap memory by leveraging the CAP_NET_ADMIN capability and providing a (1) new or (2) updated state.",
        "id": 119
    },
    {
        "cve_id": "CVE-2013-0160",
        "code_before_change": "static ssize_t tty_read(struct file *file, char __user *buf, size_t count,\n\t\t\tloff_t *ppos)\n{\n\tint i;\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct tty_struct *tty = file_tty(file);\n\tstruct tty_ldisc *ld;\n\n\tif (tty_paranoia_check(tty, inode, \"tty_read\"))\n\t\treturn -EIO;\n\tif (!tty || (test_bit(TTY_IO_ERROR, &tty->flags)))\n\t\treturn -EIO;\n\n\t/* We want to wait for the line discipline to sort out in this\n\t   situation */\n\tld = tty_ldisc_ref_wait(tty);\n\tif (ld->ops->read)\n\t\ti = (ld->ops->read)(tty, file, buf, count);\n\telse\n\t\ti = -EIO;\n\ttty_ldisc_deref(ld);\n\tif (i > 0)\n\t\tinode->i_atime = current_fs_time(inode->i_sb);\n\treturn i;\n}",
        "code_after_change": "static ssize_t tty_read(struct file *file, char __user *buf, size_t count,\n\t\t\tloff_t *ppos)\n{\n\tint i;\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct tty_struct *tty = file_tty(file);\n\tstruct tty_ldisc *ld;\n\n\tif (tty_paranoia_check(tty, inode, \"tty_read\"))\n\t\treturn -EIO;\n\tif (!tty || (test_bit(TTY_IO_ERROR, &tty->flags)))\n\t\treturn -EIO;\n\n\t/* We want to wait for the line discipline to sort out in this\n\t   situation */\n\tld = tty_ldisc_ref_wait(tty);\n\tif (ld->ops->read)\n\t\ti = (ld->ops->read)(tty, file, buf, count);\n\telse\n\t\ti = -EIO;\n\ttty_ldisc_deref(ld);\n\n\treturn i;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,6 @@\n \telse\n \t\ti = -EIO;\n \ttty_ldisc_deref(ld);\n-\tif (i > 0)\n-\t\tinode->i_atime = current_fs_time(inode->i_sb);\n+\n \treturn i;\n }",
        "function_modified_lines": {
            "added": [
                ""
            ],
            "deleted": [
                "\tif (i > 0)",
                "\t\tinode->i_atime = current_fs_time(inode->i_sb);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The Linux kernel through 3.7.9 allows local users to obtain sensitive information about keystroke timing by using the inotify API on the /dev/ptmx device.",
        "id": 145
    },
    {
        "cve_id": "CVE-2015-8575",
        "code_before_change": "static int sco_sock_bind(struct socket *sock, struct sockaddr *addr,\n\t\t\t int addr_len)\n{\n\tstruct sockaddr_sco *sa = (struct sockaddr_sco *) addr;\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\n\tBT_DBG(\"sk %p %pMR\", sk, &sa->sco_bdaddr);\n\n\tif (!addr || addr->sa_family != AF_BLUETOOTH)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != BT_OPEN) {\n\t\terr = -EBADFD;\n\t\tgoto done;\n\t}\n\n\tif (sk->sk_type != SOCK_SEQPACKET) {\n\t\terr = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tbacpy(&sco_pi(sk)->src, &sa->sco_bdaddr);\n\n\tsk->sk_state = BT_BOUND;\n\ndone:\n\trelease_sock(sk);\n\treturn err;\n}",
        "code_after_change": "static int sco_sock_bind(struct socket *sock, struct sockaddr *addr,\n\t\t\t int addr_len)\n{\n\tstruct sockaddr_sco *sa = (struct sockaddr_sco *) addr;\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\n\tBT_DBG(\"sk %p %pMR\", sk, &sa->sco_bdaddr);\n\n\tif (!addr || addr->sa_family != AF_BLUETOOTH)\n\t\treturn -EINVAL;\n\n\tif (addr_len < sizeof(struct sockaddr_sco))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != BT_OPEN) {\n\t\terr = -EBADFD;\n\t\tgoto done;\n\t}\n\n\tif (sk->sk_type != SOCK_SEQPACKET) {\n\t\terr = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tbacpy(&sco_pi(sk)->src, &sa->sco_bdaddr);\n\n\tsk->sk_state = BT_BOUND;\n\ndone:\n\trelease_sock(sk);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,9 @@\n \tBT_DBG(\"sk %p %pMR\", sk, &sa->sco_bdaddr);\n \n \tif (!addr || addr->sa_family != AF_BLUETOOTH)\n+\t\treturn -EINVAL;\n+\n+\tif (addr_len < sizeof(struct sockaddr_sco))\n \t\treturn -EINVAL;\n \n \tlock_sock(sk);",
        "function_modified_lines": {
            "added": [
                "\t\treturn -EINVAL;",
                "",
                "\tif (addr_len < sizeof(struct sockaddr_sco))"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The sco_sock_bind function in net/bluetooth/sco.c in the Linux kernel before 4.3.4 does not verify an address length, which allows local users to obtain sensitive information from kernel memory and bypass the KASLR protection mechanism via a crafted application.",
        "id": 834
    },
    {
        "cve_id": "CVE-2013-3076",
        "code_before_change": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tunsigned long iovlen;\n\tstruct iovec *iov;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\tfor (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;\n\t     iovlen--, iov++) {\n\t\tunsigned long seglen = iov->iov_len;\n\t\tchar __user *from = iov->iov_base;\n\n\t\twhile (seglen) {\n\t\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t\t       struct skcipher_sg_list, list);\n\t\t\tsg = sgl->sg;\n\n\t\t\twhile (!sg->length)\n\t\t\t\tsg++;\n\n\t\t\tused = ctx->used;\n\t\t\tif (!used) {\n\t\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto unlock;\n\t\t\t}\n\n\t\t\tused = min_t(unsigned long, used, seglen);\n\n\t\t\tused = af_alg_make_sg(&ctx->rsgl, from, used, 1);\n\t\t\terr = used;\n\t\t\tif (err < 0)\n\t\t\t\tgoto unlock;\n\n\t\t\tif (ctx->more || used < ctx->used)\n\t\t\t\tused -= used % bs;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (!used)\n\t\t\t\tgoto free;\n\n\t\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t\t     ctx->iv);\n\n\t\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\n\t\t\tcopied += used;\n\t\t\tfrom += used;\n\t\t\tseglen -= used;\n\t\t\tskcipher_pull_sgl(sk, used);\n\t\t}\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
        "code_after_change": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tunsigned long iovlen;\n\tstruct iovec *iov;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\tmsg->msg_namelen = 0;\n\tfor (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;\n\t     iovlen--, iov++) {\n\t\tunsigned long seglen = iov->iov_len;\n\t\tchar __user *from = iov->iov_base;\n\n\t\twhile (seglen) {\n\t\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t\t       struct skcipher_sg_list, list);\n\t\t\tsg = sgl->sg;\n\n\t\t\twhile (!sg->length)\n\t\t\t\tsg++;\n\n\t\t\tused = ctx->used;\n\t\t\tif (!used) {\n\t\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto unlock;\n\t\t\t}\n\n\t\t\tused = min_t(unsigned long, used, seglen);\n\n\t\t\tused = af_alg_make_sg(&ctx->rsgl, from, used, 1);\n\t\t\terr = used;\n\t\t\tif (err < 0)\n\t\t\t\tgoto unlock;\n\n\t\t\tif (ctx->more || used < ctx->used)\n\t\t\t\tused -= used % bs;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (!used)\n\t\t\t\tgoto free;\n\n\t\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t\t     ctx->iv);\n\n\t\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\n\t\t\tcopied += used;\n\t\t\tfrom += used;\n\t\t\tseglen -= used;\n\t\t\tskcipher_pull_sgl(sk, used);\n\t\t}\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,6 +15,7 @@\n \tlong copied = 0;\n \n \tlock_sock(sk);\n+\tmsg->msg_namelen = 0;\n \tfor (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;\n \t     iovlen--, iov++) {\n \t\tunsigned long seglen = iov->iov_len;",
        "function_modified_lines": {
            "added": [
                "\tmsg->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The crypto API in the Linux kernel through 3.9-rc8 does not initialize certain length variables, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call, related to the hash_recvmsg function in crypto/algif_hash.c and the skcipher_recvmsg function in crypto/algif_skcipher.c.",
        "id": 262
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static void blkfront_gather_backend_features(struct blkfront_info *info)\n{\n\tunsigned int indirect_segments;\n\n\tinfo->feature_flush = 0;\n\tinfo->feature_fua = 0;\n\n\t/*\n\t * If there's no \"feature-barrier\" defined, then it means\n\t * we're dealing with a very old backend which writes\n\t * synchronously; nothing to do.\n\t *\n\t * If there are barriers, then we use flush.\n\t */\n\tif (xenbus_read_unsigned(info->xbdev->otherend, \"feature-barrier\", 0)) {\n\t\tinfo->feature_flush = 1;\n\t\tinfo->feature_fua = 1;\n\t}\n\n\t/*\n\t * And if there is \"feature-flush-cache\" use that above\n\t * barriers.\n\t */\n\tif (xenbus_read_unsigned(info->xbdev->otherend, \"feature-flush-cache\",\n\t\t\t\t 0)) {\n\t\tinfo->feature_flush = 1;\n\t\tinfo->feature_fua = 0;\n\t}\n\n\tif (xenbus_read_unsigned(info->xbdev->otherend, \"feature-discard\", 0))\n\t\tblkfront_setup_discard(info);\n\n\tif (info->feature_persistent)\n\t\tinfo->feature_persistent =\n\t\t\t!!xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t       \"feature-persistent\", 0);\n\n\tindirect_segments = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\"feature-max-indirect-segments\", 0);\n\tif (indirect_segments > xen_blkif_max_segments)\n\t\tindirect_segments = xen_blkif_max_segments;\n\tif (indirect_segments <= BLKIF_MAX_SEGMENTS_PER_REQUEST)\n\t\tindirect_segments = 0;\n\tinfo->max_indirect_segments = indirect_segments;\n\n\tif (info->feature_persistent) {\n\t\tmutex_lock(&blkfront_mutex);\n\t\tschedule_delayed_work(&blkfront_work, HZ * 10);\n\t\tmutex_unlock(&blkfront_mutex);\n\t}\n}",
        "code_after_change": "static void blkfront_gather_backend_features(struct blkfront_info *info)\n{\n\tunsigned int indirect_segments;\n\n\tinfo->feature_flush = 0;\n\tinfo->feature_fua = 0;\n\n\t/*\n\t * If there's no \"feature-barrier\" defined, then it means\n\t * we're dealing with a very old backend which writes\n\t * synchronously; nothing to do.\n\t *\n\t * If there are barriers, then we use flush.\n\t */\n\tif (xenbus_read_unsigned(info->xbdev->otherend, \"feature-barrier\", 0)) {\n\t\tinfo->feature_flush = 1;\n\t\tinfo->feature_fua = 1;\n\t}\n\n\t/*\n\t * And if there is \"feature-flush-cache\" use that above\n\t * barriers.\n\t */\n\tif (xenbus_read_unsigned(info->xbdev->otherend, \"feature-flush-cache\",\n\t\t\t\t 0)) {\n\t\tinfo->feature_flush = 1;\n\t\tinfo->feature_fua = 0;\n\t}\n\n\tif (xenbus_read_unsigned(info->xbdev->otherend, \"feature-discard\", 0))\n\t\tblkfront_setup_discard(info);\n\n\tif (info->feature_persistent)\n\t\tinfo->feature_persistent =\n\t\t\t!!xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t       \"feature-persistent\", 0);\n\tif (info->feature_persistent)\n\t\tinfo->bounce = true;\n\n\tindirect_segments = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\"feature-max-indirect-segments\", 0);\n\tif (indirect_segments > xen_blkif_max_segments)\n\t\tindirect_segments = xen_blkif_max_segments;\n\tif (indirect_segments <= BLKIF_MAX_SEGMENTS_PER_REQUEST)\n\t\tindirect_segments = 0;\n\tinfo->max_indirect_segments = indirect_segments;\n\n\tif (info->feature_persistent) {\n\t\tmutex_lock(&blkfront_mutex);\n\t\tschedule_delayed_work(&blkfront_work, HZ * 10);\n\t\tmutex_unlock(&blkfront_mutex);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -34,6 +34,8 @@\n \t\tinfo->feature_persistent =\n \t\t\t!!xenbus_read_unsigned(info->xbdev->otherend,\n \t\t\t\t\t       \"feature-persistent\", 0);\n+\tif (info->feature_persistent)\n+\t\tinfo->bounce = true;\n \n \tindirect_segments = xenbus_read_unsigned(info->xbdev->otherend,\n \t\t\t\t\t\"feature-max-indirect-segments\", 0);",
        "function_modified_lines": {
            "added": [
                "\tif (info->feature_persistent)",
                "\t\tinfo->bounce = true;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3584
    },
    {
        "cve_id": "CVE-2017-18549",
        "code_before_change": "static int aac_get_hba_info(struct aac_dev *dev, void __user *arg)\n{\n\tstruct aac_hba_info hbainfo;\n\n\thbainfo.adapter_number\t\t= (u8) dev->id;\n\thbainfo.system_io_bus_number\t= dev->pdev->bus->number;\n\thbainfo.device_number\t\t= (dev->pdev->devfn >> 3);\n\thbainfo.function_number\t\t= (dev->pdev->devfn & 0x0007);\n\n\thbainfo.vendor_id\t\t= dev->pdev->vendor;\n\thbainfo.device_id\t\t= dev->pdev->device;\n\thbainfo.sub_vendor_id\t\t= dev->pdev->subsystem_vendor;\n\thbainfo.sub_system_id\t\t= dev->pdev->subsystem_device;\n\n\tif (copy_to_user(arg, &hbainfo, sizeof(struct aac_hba_info))) {\n\t\tdprintk((KERN_DEBUG \"aacraid: Could not copy hba info\\n\"));\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int aac_get_hba_info(struct aac_dev *dev, void __user *arg)\n{\n\tstruct aac_hba_info hbainfo;\n\n\tmemset(&hbainfo, 0, sizeof(hbainfo));\n\thbainfo.adapter_number\t\t= (u8) dev->id;\n\thbainfo.system_io_bus_number\t= dev->pdev->bus->number;\n\thbainfo.device_number\t\t= (dev->pdev->devfn >> 3);\n\thbainfo.function_number\t\t= (dev->pdev->devfn & 0x0007);\n\n\thbainfo.vendor_id\t\t= dev->pdev->vendor;\n\thbainfo.device_id\t\t= dev->pdev->device;\n\thbainfo.sub_vendor_id\t\t= dev->pdev->subsystem_vendor;\n\thbainfo.sub_system_id\t\t= dev->pdev->subsystem_device;\n\n\tif (copy_to_user(arg, &hbainfo, sizeof(struct aac_hba_info))) {\n\t\tdprintk((KERN_DEBUG \"aacraid: Could not copy hba info\\n\"));\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,7 @@\n {\n \tstruct aac_hba_info hbainfo;\n \n+\tmemset(&hbainfo, 0, sizeof(hbainfo));\n \thbainfo.adapter_number\t\t= (u8) dev->id;\n \thbainfo.system_io_bus_number\t= dev->pdev->bus->number;\n \thbainfo.device_number\t\t= (dev->pdev->devfn >> 3);",
        "function_modified_lines": {
            "added": [
                "\tmemset(&hbainfo, 0, sizeof(hbainfo));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An issue was discovered in drivers/scsi/aacraid/commctrl.c in the Linux kernel before 4.13. There is potential exposure of kernel stack memory because aac_send_raw_srb does not initialize the reply structure.",
        "id": 1438
    },
    {
        "cve_id": "CVE-2016-4482",
        "code_before_change": "static int proc_connectinfo(struct usb_dev_state *ps, void __user *arg)\n{\n\tstruct usbdevfs_connectinfo ci = {\n\t\t.devnum = ps->dev->devnum,\n\t\t.slow = ps->dev->speed == USB_SPEED_LOW\n\t};\n\n\tif (copy_to_user(arg, &ci, sizeof(ci)))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "code_after_change": "static int proc_connectinfo(struct usb_dev_state *ps, void __user *arg)\n{\n\tstruct usbdevfs_connectinfo ci;\n\n\tmemset(&ci, 0, sizeof(ci));\n\tci.devnum = ps->dev->devnum;\n\tci.slow = ps->dev->speed == USB_SPEED_LOW;\n\n\tif (copy_to_user(arg, &ci, sizeof(ci)))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,9 +1,10 @@\n static int proc_connectinfo(struct usb_dev_state *ps, void __user *arg)\n {\n-\tstruct usbdevfs_connectinfo ci = {\n-\t\t.devnum = ps->dev->devnum,\n-\t\t.slow = ps->dev->speed == USB_SPEED_LOW\n-\t};\n+\tstruct usbdevfs_connectinfo ci;\n+\n+\tmemset(&ci, 0, sizeof(ci));\n+\tci.devnum = ps->dev->devnum;\n+\tci.slow = ps->dev->speed == USB_SPEED_LOW;\n \n \tif (copy_to_user(arg, &ci, sizeof(ci)))\n \t\treturn -EFAULT;",
        "function_modified_lines": {
            "added": [
                "\tstruct usbdevfs_connectinfo ci;",
                "",
                "\tmemset(&ci, 0, sizeof(ci));",
                "\tci.devnum = ps->dev->devnum;",
                "\tci.slow = ps->dev->speed == USB_SPEED_LOW;"
            ],
            "deleted": [
                "\tstruct usbdevfs_connectinfo ci = {",
                "\t\t.devnum = ps->dev->devnum,",
                "\t\t.slow = ps->dev->speed == USB_SPEED_LOW",
                "\t};"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The proc_connectinfo function in drivers/usb/core/devio.c in the Linux kernel through 4.6 does not initialize a certain data structure, which allows local users to obtain sensitive information from kernel stack memory via a crafted USBDEVFS_CONNECTINFO ioctl call.",
        "id": 1018
    },
    {
        "cve_id": "CVE-2018-20509",
        "code_before_change": "static int binder_translate_handle(struct flat_binder_object *fp,\n\t\t\t\t   struct binder_transaction *t,\n\t\t\t\t   struct binder_thread *thread)\n{\n\tstruct binder_ref *ref;\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\n\tref = binder_get_ref(proc, fp->handle,\n\t\t\t     fp->hdr.type == BINDER_TYPE_HANDLE);\n\tif (!ref) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid handle, %d\\n\",\n\t\t\t\t  proc->pid, thread->pid, fp->handle);\n\t\treturn -EINVAL;\n\t}\n\tif (security_binder_transfer_binder(proc->tsk, target_proc->tsk))\n\t\treturn -EPERM;\n\n\tif (ref->node->proc == target_proc) {\n\t\tif (fp->hdr.type == BINDER_TYPE_HANDLE)\n\t\t\tfp->hdr.type = BINDER_TYPE_BINDER;\n\t\telse\n\t\t\tfp->hdr.type = BINDER_TYPE_WEAK_BINDER;\n\t\tfp->binder = ref->node->ptr;\n\t\tfp->cookie = ref->node->cookie;\n\t\tbinder_inc_node(ref->node, fp->hdr.type == BINDER_TYPE_BINDER,\n\t\t\t\t0, NULL);\n\t\ttrace_binder_transaction_ref_to_node(t, ref);\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"        ref %d desc %d -> node %d u%016llx\\n\",\n\t\t\t     ref->debug_id, ref->desc, ref->node->debug_id,\n\t\t\t     (u64)ref->node->ptr);\n\t} else {\n\t\tstruct binder_ref *new_ref;\n\n\t\tnew_ref = binder_get_ref_for_node(target_proc, ref->node);\n\t\tif (!new_ref)\n\t\t\treturn -ENOMEM;\n\n\t\tfp->binder = 0;\n\t\tfp->handle = new_ref->desc;\n\t\tfp->cookie = 0;\n\t\tbinder_inc_ref(new_ref, fp->hdr.type == BINDER_TYPE_HANDLE,\n\t\t\t       NULL);\n\t\ttrace_binder_transaction_ref_to_ref(t, ref, new_ref);\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"        ref %d desc %d -> ref %d desc %d (node %d)\\n\",\n\t\t\t     ref->debug_id, ref->desc, new_ref->debug_id,\n\t\t\t     new_ref->desc, ref->node->debug_id);\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int binder_translate_handle(struct flat_binder_object *fp,\n\t\t\t\t   struct binder_transaction *t,\n\t\t\t\t   struct binder_thread *thread)\n{\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\tstruct binder_node *node;\n\tstruct binder_ref_data src_rdata;\n\n\tnode = binder_get_node_from_ref(proc, fp->handle,\n\t\t\tfp->hdr.type == BINDER_TYPE_HANDLE, &src_rdata);\n\tif (!node) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid handle, %d\\n\",\n\t\t\t\t  proc->pid, thread->pid, fp->handle);\n\t\treturn -EINVAL;\n\t}\n\tif (security_binder_transfer_binder(proc->tsk, target_proc->tsk))\n\t\treturn -EPERM;\n\n\tif (node->proc == target_proc) {\n\t\tif (fp->hdr.type == BINDER_TYPE_HANDLE)\n\t\t\tfp->hdr.type = BINDER_TYPE_BINDER;\n\t\telse\n\t\t\tfp->hdr.type = BINDER_TYPE_WEAK_BINDER;\n\t\tfp->binder = node->ptr;\n\t\tfp->cookie = node->cookie;\n\t\tbinder_inc_node(node,\n\t\t\t\tfp->hdr.type == BINDER_TYPE_BINDER,\n\t\t\t\t0, NULL);\n\t\ttrace_binder_transaction_ref_to_node(t, node, &src_rdata);\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"        ref %d desc %d -> node %d u%016llx\\n\",\n\t\t\t     src_rdata.debug_id, src_rdata.desc, node->debug_id,\n\t\t\t     (u64)node->ptr);\n\t} else {\n\t\tint ret;\n\t\tstruct binder_ref_data dest_rdata;\n\n\t\tret = binder_inc_ref_for_node(target_proc, node,\n\t\t\t\tfp->hdr.type == BINDER_TYPE_HANDLE,\n\t\t\t\tNULL, &dest_rdata);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tfp->binder = 0;\n\t\tfp->handle = dest_rdata.desc;\n\t\tfp->cookie = 0;\n\t\ttrace_binder_transaction_ref_to_ref(t, node, &src_rdata,\n\t\t\t\t\t\t    &dest_rdata);\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"        ref %d desc %d -> ref %d desc %d (node %d)\\n\",\n\t\t\t     src_rdata.debug_id, src_rdata.desc,\n\t\t\t     dest_rdata.debug_id, dest_rdata.desc,\n\t\t\t     node->debug_id);\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,13 +2,14 @@\n \t\t\t\t   struct binder_transaction *t,\n \t\t\t\t   struct binder_thread *thread)\n {\n-\tstruct binder_ref *ref;\n \tstruct binder_proc *proc = thread->proc;\n \tstruct binder_proc *target_proc = t->to_proc;\n+\tstruct binder_node *node;\n+\tstruct binder_ref_data src_rdata;\n \n-\tref = binder_get_ref(proc, fp->handle,\n-\t\t\t     fp->hdr.type == BINDER_TYPE_HANDLE);\n-\tif (!ref) {\n+\tnode = binder_get_node_from_ref(proc, fp->handle,\n+\t\t\tfp->hdr.type == BINDER_TYPE_HANDLE, &src_rdata);\n+\tif (!node) {\n \t\tbinder_user_error(\"%d:%d got transaction with invalid handle, %d\\n\",\n \t\t\t\t  proc->pid, thread->pid, fp->handle);\n \t\treturn -EINVAL;\n@@ -16,37 +17,41 @@\n \tif (security_binder_transfer_binder(proc->tsk, target_proc->tsk))\n \t\treturn -EPERM;\n \n-\tif (ref->node->proc == target_proc) {\n+\tif (node->proc == target_proc) {\n \t\tif (fp->hdr.type == BINDER_TYPE_HANDLE)\n \t\t\tfp->hdr.type = BINDER_TYPE_BINDER;\n \t\telse\n \t\t\tfp->hdr.type = BINDER_TYPE_WEAK_BINDER;\n-\t\tfp->binder = ref->node->ptr;\n-\t\tfp->cookie = ref->node->cookie;\n-\t\tbinder_inc_node(ref->node, fp->hdr.type == BINDER_TYPE_BINDER,\n+\t\tfp->binder = node->ptr;\n+\t\tfp->cookie = node->cookie;\n+\t\tbinder_inc_node(node,\n+\t\t\t\tfp->hdr.type == BINDER_TYPE_BINDER,\n \t\t\t\t0, NULL);\n-\t\ttrace_binder_transaction_ref_to_node(t, ref);\n+\t\ttrace_binder_transaction_ref_to_node(t, node, &src_rdata);\n \t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n \t\t\t     \"        ref %d desc %d -> node %d u%016llx\\n\",\n-\t\t\t     ref->debug_id, ref->desc, ref->node->debug_id,\n-\t\t\t     (u64)ref->node->ptr);\n+\t\t\t     src_rdata.debug_id, src_rdata.desc, node->debug_id,\n+\t\t\t     (u64)node->ptr);\n \t} else {\n-\t\tstruct binder_ref *new_ref;\n+\t\tint ret;\n+\t\tstruct binder_ref_data dest_rdata;\n \n-\t\tnew_ref = binder_get_ref_for_node(target_proc, ref->node);\n-\t\tif (!new_ref)\n-\t\t\treturn -ENOMEM;\n+\t\tret = binder_inc_ref_for_node(target_proc, node,\n+\t\t\t\tfp->hdr.type == BINDER_TYPE_HANDLE,\n+\t\t\t\tNULL, &dest_rdata);\n+\t\tif (ret)\n+\t\t\treturn ret;\n \n \t\tfp->binder = 0;\n-\t\tfp->handle = new_ref->desc;\n+\t\tfp->handle = dest_rdata.desc;\n \t\tfp->cookie = 0;\n-\t\tbinder_inc_ref(new_ref, fp->hdr.type == BINDER_TYPE_HANDLE,\n-\t\t\t       NULL);\n-\t\ttrace_binder_transaction_ref_to_ref(t, ref, new_ref);\n+\t\ttrace_binder_transaction_ref_to_ref(t, node, &src_rdata,\n+\t\t\t\t\t\t    &dest_rdata);\n \t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n \t\t\t     \"        ref %d desc %d -> ref %d desc %d (node %d)\\n\",\n-\t\t\t     ref->debug_id, ref->desc, new_ref->debug_id,\n-\t\t\t     new_ref->desc, ref->node->debug_id);\n+\t\t\t     src_rdata.debug_id, src_rdata.desc,\n+\t\t\t     dest_rdata.debug_id, dest_rdata.desc,\n+\t\t\t     node->debug_id);\n \t}\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct binder_node *node;",
                "\tstruct binder_ref_data src_rdata;",
                "\tnode = binder_get_node_from_ref(proc, fp->handle,",
                "\t\t\tfp->hdr.type == BINDER_TYPE_HANDLE, &src_rdata);",
                "\tif (!node) {",
                "\tif (node->proc == target_proc) {",
                "\t\tfp->binder = node->ptr;",
                "\t\tfp->cookie = node->cookie;",
                "\t\tbinder_inc_node(node,",
                "\t\t\t\tfp->hdr.type == BINDER_TYPE_BINDER,",
                "\t\ttrace_binder_transaction_ref_to_node(t, node, &src_rdata);",
                "\t\t\t     src_rdata.debug_id, src_rdata.desc, node->debug_id,",
                "\t\t\t     (u64)node->ptr);",
                "\t\tint ret;",
                "\t\tstruct binder_ref_data dest_rdata;",
                "\t\tret = binder_inc_ref_for_node(target_proc, node,",
                "\t\t\t\tfp->hdr.type == BINDER_TYPE_HANDLE,",
                "\t\t\t\tNULL, &dest_rdata);",
                "\t\tif (ret)",
                "\t\t\treturn ret;",
                "\t\tfp->handle = dest_rdata.desc;",
                "\t\ttrace_binder_transaction_ref_to_ref(t, node, &src_rdata,",
                "\t\t\t\t\t\t    &dest_rdata);",
                "\t\t\t     src_rdata.debug_id, src_rdata.desc,",
                "\t\t\t     dest_rdata.debug_id, dest_rdata.desc,",
                "\t\t\t     node->debug_id);"
            ],
            "deleted": [
                "\tstruct binder_ref *ref;",
                "\tref = binder_get_ref(proc, fp->handle,",
                "\t\t\t     fp->hdr.type == BINDER_TYPE_HANDLE);",
                "\tif (!ref) {",
                "\tif (ref->node->proc == target_proc) {",
                "\t\tfp->binder = ref->node->ptr;",
                "\t\tfp->cookie = ref->node->cookie;",
                "\t\tbinder_inc_node(ref->node, fp->hdr.type == BINDER_TYPE_BINDER,",
                "\t\ttrace_binder_transaction_ref_to_node(t, ref);",
                "\t\t\t     ref->debug_id, ref->desc, ref->node->debug_id,",
                "\t\t\t     (u64)ref->node->ptr);",
                "\t\tstruct binder_ref *new_ref;",
                "\t\tnew_ref = binder_get_ref_for_node(target_proc, ref->node);",
                "\t\tif (!new_ref)",
                "\t\t\treturn -ENOMEM;",
                "\t\tfp->handle = new_ref->desc;",
                "\t\tbinder_inc_ref(new_ref, fp->hdr.type == BINDER_TYPE_HANDLE,",
                "\t\t\t       NULL);",
                "\t\ttrace_binder_transaction_ref_to_ref(t, ref, new_ref);",
                "\t\t\t     ref->debug_id, ref->desc, new_ref->debug_id,",
                "\t\t\t     new_ref->desc, ref->node->debug_id);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_ref_olocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \" ref *desc *node\" lines in a debugfs file.",
        "id": 1766
    },
    {
        "cve_id": "CVE-2015-7884",
        "code_before_change": "static int vivid_fb_ioctl(struct fb_info *info, unsigned cmd, unsigned long arg)\n{\n\tstruct vivid_dev *dev = (struct vivid_dev *)info->par;\n\n\tswitch (cmd) {\n\tcase FBIOGET_VBLANK: {\n\t\tstruct fb_vblank vblank;\n\n\t\tvblank.flags = FB_VBLANK_HAVE_COUNT | FB_VBLANK_HAVE_VCOUNT |\n\t\t\tFB_VBLANK_HAVE_VSYNC;\n\t\tvblank.count = 0;\n\t\tvblank.vcount = 0;\n\t\tvblank.hcount = 0;\n\t\tif (copy_to_user((void __user *)arg, &vblank, sizeof(vblank)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\n\tdefault:\n\t\tdprintk(dev, 1, \"Unknown ioctl %08x\\n\", cmd);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int vivid_fb_ioctl(struct fb_info *info, unsigned cmd, unsigned long arg)\n{\n\tstruct vivid_dev *dev = (struct vivid_dev *)info->par;\n\n\tswitch (cmd) {\n\tcase FBIOGET_VBLANK: {\n\t\tstruct fb_vblank vblank;\n\n\t\tmemset(&vblank, 0, sizeof(vblank));\n\t\tvblank.flags = FB_VBLANK_HAVE_COUNT | FB_VBLANK_HAVE_VCOUNT |\n\t\t\tFB_VBLANK_HAVE_VSYNC;\n\t\tvblank.count = 0;\n\t\tvblank.vcount = 0;\n\t\tvblank.hcount = 0;\n\t\tif (copy_to_user((void __user *)arg, &vblank, sizeof(vblank)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\n\tdefault:\n\t\tdprintk(dev, 1, \"Unknown ioctl %08x\\n\", cmd);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,7 @@\n \tcase FBIOGET_VBLANK: {\n \t\tstruct fb_vblank vblank;\n \n+\t\tmemset(&vblank, 0, sizeof(vblank));\n \t\tvblank.flags = FB_VBLANK_HAVE_COUNT | FB_VBLANK_HAVE_VCOUNT |\n \t\t\tFB_VBLANK_HAVE_VSYNC;\n \t\tvblank.count = 0;",
        "function_modified_lines": {
            "added": [
                "\t\tmemset(&vblank, 0, sizeof(vblank));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The vivid_fb_ioctl function in drivers/media/platform/vivid/vivid-osd.c in the Linux kernel through 4.3.3 does not initialize a certain structure member, which allows local users to obtain sensitive information from kernel memory via a crafted application.",
        "id": 794
    },
    {
        "cve_id": "CVE-2017-17449",
        "code_before_change": "static int __netlink_deliver_tap_skb(struct sk_buff *skb,\n\t\t\t\t     struct net_device *dev)\n{\n\tstruct sk_buff *nskb;\n\tstruct sock *sk = skb->sk;\n\tint ret = -ENOMEM;\n\n\tdev_hold(dev);\n\n\tif (is_vmalloc_addr(skb->head))\n\t\tnskb = netlink_to_full_skb(skb, GFP_ATOMIC);\n\telse\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\tif (nskb) {\n\t\tnskb->dev = dev;\n\t\tnskb->protocol = htons((u16) sk->sk_protocol);\n\t\tnskb->pkt_type = netlink_is_kernel(sk) ?\n\t\t\t\t PACKET_KERNEL : PACKET_USER;\n\t\tskb_reset_network_header(nskb);\n\t\tret = dev_queue_xmit(nskb);\n\t\tif (unlikely(ret > 0))\n\t\t\tret = net_xmit_errno(ret);\n\t}\n\n\tdev_put(dev);\n\treturn ret;\n}",
        "code_after_change": "static int __netlink_deliver_tap_skb(struct sk_buff *skb,\n\t\t\t\t     struct net_device *dev)\n{\n\tstruct sk_buff *nskb;\n\tstruct sock *sk = skb->sk;\n\tint ret = -ENOMEM;\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\treturn 0;\n\n\tdev_hold(dev);\n\n\tif (is_vmalloc_addr(skb->head))\n\t\tnskb = netlink_to_full_skb(skb, GFP_ATOMIC);\n\telse\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\tif (nskb) {\n\t\tnskb->dev = dev;\n\t\tnskb->protocol = htons((u16) sk->sk_protocol);\n\t\tnskb->pkt_type = netlink_is_kernel(sk) ?\n\t\t\t\t PACKET_KERNEL : PACKET_USER;\n\t\tskb_reset_network_header(nskb);\n\t\tret = dev_queue_xmit(nskb);\n\t\tif (unlikely(ret > 0))\n\t\t\tret = net_xmit_errno(ret);\n\t}\n\n\tdev_put(dev);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,9 @@\n \tstruct sk_buff *nskb;\n \tstruct sock *sk = skb->sk;\n \tint ret = -ENOMEM;\n+\n+\tif (!net_eq(dev_net(dev), sock_net(sk)))\n+\t\treturn 0;\n \n \tdev_hold(dev);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (!net_eq(dev_net(dev), sock_net(sk)))",
                "\t\treturn 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The __netlink_deliver_tap_skb function in net/netlink/af_netlink.c in the Linux kernel through 4.14.4, when CONFIG_NLMON is enabled, does not restrict observations of Netlink messages to a single net namespace, which allows local users to obtain sensitive information by leveraging the CAP_NET_ADMIN capability to sniff an nlmon interface for all Netlink activity on the system.",
        "id": 1362
    },
    {
        "cve_id": "CVE-2018-20509",
        "code_before_change": "static int binder_translate_binder(struct flat_binder_object *fp,\n\t\t\t\t   struct binder_transaction *t,\n\t\t\t\t   struct binder_thread *thread)\n{\n\tstruct binder_node *node;\n\tstruct binder_ref *ref;\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\n\tnode = binder_get_node(proc, fp->binder);\n\tif (!node) {\n\t\tnode = binder_new_node(proc, fp->binder, fp->cookie);\n\t\tif (!node)\n\t\t\treturn -ENOMEM;\n\n\t\tnode->min_priority = fp->flags & FLAT_BINDER_FLAG_PRIORITY_MASK;\n\t\tnode->accept_fds = !!(fp->flags & FLAT_BINDER_FLAG_ACCEPTS_FDS);\n\t}\n\tif (fp->cookie != node->cookie) {\n\t\tbinder_user_error(\"%d:%d sending u%016llx node %d, cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fp->binder,\n\t\t\t\t  node->debug_id, (u64)fp->cookie,\n\t\t\t\t  (u64)node->cookie);\n\t\treturn -EINVAL;\n\t}\n\tif (security_binder_transfer_binder(proc->tsk, target_proc->tsk))\n\t\treturn -EPERM;\n\n\tref = binder_get_ref_for_node(target_proc, node);\n\tif (!ref)\n\t\treturn -ENOMEM;\n\n\tif (fp->hdr.type == BINDER_TYPE_BINDER)\n\t\tfp->hdr.type = BINDER_TYPE_HANDLE;\n\telse\n\t\tfp->hdr.type = BINDER_TYPE_WEAK_HANDLE;\n\tfp->binder = 0;\n\tfp->handle = ref->desc;\n\tfp->cookie = 0;\n\tbinder_inc_ref(ref, fp->hdr.type == BINDER_TYPE_HANDLE, &thread->todo);\n\n\ttrace_binder_transaction_node_to_ref(t, node, ref);\n\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t     \"        node %d u%016llx -> ref %d desc %d\\n\",\n\t\t     node->debug_id, (u64)node->ptr,\n\t\t     ref->debug_id, ref->desc);\n\n\treturn 0;\n}",
        "code_after_change": "static int binder_translate_binder(struct flat_binder_object *fp,\n\t\t\t\t   struct binder_transaction *t,\n\t\t\t\t   struct binder_thread *thread)\n{\n\tstruct binder_node *node;\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\tstruct binder_ref_data rdata;\n\tint ret;\n\n\tnode = binder_get_node(proc, fp->binder);\n\tif (!node) {\n\t\tnode = binder_new_node(proc, fp->binder, fp->cookie);\n\t\tif (!node)\n\t\t\treturn -ENOMEM;\n\n\t\tnode->min_priority = fp->flags & FLAT_BINDER_FLAG_PRIORITY_MASK;\n\t\tnode->accept_fds = !!(fp->flags & FLAT_BINDER_FLAG_ACCEPTS_FDS);\n\t}\n\tif (fp->cookie != node->cookie) {\n\t\tbinder_user_error(\"%d:%d sending u%016llx node %d, cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fp->binder,\n\t\t\t\t  node->debug_id, (u64)fp->cookie,\n\t\t\t\t  (u64)node->cookie);\n\t\treturn -EINVAL;\n\t}\n\tif (security_binder_transfer_binder(proc->tsk, target_proc->tsk))\n\t\treturn -EPERM;\n\n\tret = binder_inc_ref_for_node(target_proc, node,\n\t\t\tfp->hdr.type == BINDER_TYPE_BINDER,\n\t\t\t&thread->todo, &rdata);\n\tif (ret)\n\t\treturn ret;\n\n\tif (fp->hdr.type == BINDER_TYPE_BINDER)\n\t\tfp->hdr.type = BINDER_TYPE_HANDLE;\n\telse\n\t\tfp->hdr.type = BINDER_TYPE_WEAK_HANDLE;\n\tfp->binder = 0;\n\tfp->handle = rdata.desc;\n\tfp->cookie = 0;\n\n\ttrace_binder_transaction_node_to_ref(t, node, &rdata);\n\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t     \"        node %d u%016llx -> ref %d desc %d\\n\",\n\t\t     node->debug_id, (u64)node->ptr,\n\t\t     rdata.debug_id, rdata.desc);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,9 +3,10 @@\n \t\t\t\t   struct binder_thread *thread)\n {\n \tstruct binder_node *node;\n-\tstruct binder_ref *ref;\n \tstruct binder_proc *proc = thread->proc;\n \tstruct binder_proc *target_proc = t->to_proc;\n+\tstruct binder_ref_data rdata;\n+\tint ret;\n \n \tnode = binder_get_node(proc, fp->binder);\n \tif (!node) {\n@@ -26,24 +27,24 @@\n \tif (security_binder_transfer_binder(proc->tsk, target_proc->tsk))\n \t\treturn -EPERM;\n \n-\tref = binder_get_ref_for_node(target_proc, node);\n-\tif (!ref)\n-\t\treturn -ENOMEM;\n+\tret = binder_inc_ref_for_node(target_proc, node,\n+\t\t\tfp->hdr.type == BINDER_TYPE_BINDER,\n+\t\t\t&thread->todo, &rdata);\n+\tif (ret)\n+\t\treturn ret;\n \n \tif (fp->hdr.type == BINDER_TYPE_BINDER)\n \t\tfp->hdr.type = BINDER_TYPE_HANDLE;\n \telse\n \t\tfp->hdr.type = BINDER_TYPE_WEAK_HANDLE;\n \tfp->binder = 0;\n-\tfp->handle = ref->desc;\n+\tfp->handle = rdata.desc;\n \tfp->cookie = 0;\n-\tbinder_inc_ref(ref, fp->hdr.type == BINDER_TYPE_HANDLE, &thread->todo);\n \n-\ttrace_binder_transaction_node_to_ref(t, node, ref);\n+\ttrace_binder_transaction_node_to_ref(t, node, &rdata);\n \tbinder_debug(BINDER_DEBUG_TRANSACTION,\n \t\t     \"        node %d u%016llx -> ref %d desc %d\\n\",\n \t\t     node->debug_id, (u64)node->ptr,\n-\t\t     ref->debug_id, ref->desc);\n-\n+\t\t     rdata.debug_id, rdata.desc);\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct binder_ref_data rdata;",
                "\tint ret;",
                "\tret = binder_inc_ref_for_node(target_proc, node,",
                "\t\t\tfp->hdr.type == BINDER_TYPE_BINDER,",
                "\t\t\t&thread->todo, &rdata);",
                "\tif (ret)",
                "\t\treturn ret;",
                "\tfp->handle = rdata.desc;",
                "\ttrace_binder_transaction_node_to_ref(t, node, &rdata);",
                "\t\t     rdata.debug_id, rdata.desc);"
            ],
            "deleted": [
                "\tstruct binder_ref *ref;",
                "\tref = binder_get_ref_for_node(target_proc, node);",
                "\tif (!ref)",
                "\t\treturn -ENOMEM;",
                "\tfp->handle = ref->desc;",
                "\tbinder_inc_ref(ref, fp->hdr.type == BINDER_TYPE_HANDLE, &thread->todo);",
                "\ttrace_binder_transaction_node_to_ref(t, node, ref);",
                "\t\t     ref->debug_id, ref->desc);",
                ""
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The print_binder_ref_olocked function in drivers/android/binder.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \" ref *desc *node\" lines in a debugfs file.",
        "id": 1765
    },
    {
        "cve_id": "CVE-2020-2732",
        "code_before_change": "static int vmx_check_intercept(struct kvm_vcpu *vcpu,\n\t\t\t       struct x86_instruction_info *info,\n\t\t\t       enum x86_intercept_stage stage)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct x86_emulate_ctxt *ctxt = &vcpu->arch.emulate_ctxt;\n\n\t/*\n\t * RDPID causes #UD if disabled through secondary execution controls.\n\t * Because it is marked as EmulateOnUD, we need to intercept it here.\n\t */\n\tif (info->intercept == x86_intercept_rdtscp &&\n\t    !nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDTSCP)) {\n\t\tctxt->exception.vector = UD_VECTOR;\n\t\tctxt->exception.error_code_valid = false;\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\t/* TODO: check more intercepts... */\n\treturn X86EMUL_CONTINUE;\n}",
        "code_after_change": "static int vmx_check_intercept(struct kvm_vcpu *vcpu,\n\t\t\t       struct x86_instruction_info *info,\n\t\t\t       enum x86_intercept_stage stage)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct x86_emulate_ctxt *ctxt = &vcpu->arch.emulate_ctxt;\n\n\t/*\n\t * RDPID causes #UD if disabled through secondary execution controls.\n\t * Because it is marked as EmulateOnUD, we need to intercept it here.\n\t */\n\tif (info->intercept == x86_intercept_rdtscp &&\n\t    !nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDTSCP)) {\n\t\tctxt->exception.vector = UD_VECTOR;\n\t\tctxt->exception.error_code_valid = false;\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\t/* TODO: check more intercepts... */\n\treturn X86EMUL_UNHANDLEABLE;\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,5 +17,5 @@\n \t}\n \n \t/* TODO: check more intercepts... */\n-\treturn X86EMUL_CONTINUE;\n+\treturn X86EMUL_UNHANDLEABLE;\n }",
        "function_modified_lines": {
            "added": [
                "\treturn X86EMUL_UNHANDLEABLE;"
            ],
            "deleted": [
                "\treturn X86EMUL_CONTINUE;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "A flaw was discovered in the way that the KVM hypervisor handled instruction emulation for an L2 guest when nested virtualisation is enabled. Under some circumstances, an L2 guest may trick the L0 guest into accessing sensitive L1 resources that should be inaccessible to the L2 guest.",
        "id": 2618
    },
    {
        "cve_id": "CVE-2017-16911",
        "code_before_change": "static void port_show_vhci(char **out, int hub, int port, struct vhci_device *vdev)\n{\n\tif (hub == HUB_SPEED_HIGH)\n\t\t*out += sprintf(*out, \"hs  %04u %03u \",\n\t\t\t\t      port, vdev->ud.status);\n\telse /* hub == HUB_SPEED_SUPER */\n\t\t*out += sprintf(*out, \"ss  %04u %03u \",\n\t\t\t\t      port, vdev->ud.status);\n\n\tif (vdev->ud.status == VDEV_ST_USED) {\n\t\t*out += sprintf(*out, \"%03u %08x \",\n\t\t\t\t      vdev->speed, vdev->devid);\n\t\t*out += sprintf(*out, \"%16p %s\",\n\t\t\t\t      vdev->ud.tcp_socket,\n\t\t\t\t      dev_name(&vdev->udev->dev));\n\n\t} else {\n\t\t*out += sprintf(*out, \"000 00000000 \");\n\t\t*out += sprintf(*out, \"0000000000000000 0-0\");\n\t}\n\n\t*out += sprintf(*out, \"\\n\");\n}",
        "code_after_change": "static void port_show_vhci(char **out, int hub, int port, struct vhci_device *vdev)\n{\n\tif (hub == HUB_SPEED_HIGH)\n\t\t*out += sprintf(*out, \"hs  %04u %03u \",\n\t\t\t\t      port, vdev->ud.status);\n\telse /* hub == HUB_SPEED_SUPER */\n\t\t*out += sprintf(*out, \"ss  %04u %03u \",\n\t\t\t\t      port, vdev->ud.status);\n\n\tif (vdev->ud.status == VDEV_ST_USED) {\n\t\t*out += sprintf(*out, \"%03u %08x \",\n\t\t\t\t      vdev->speed, vdev->devid);\n\t\t*out += sprintf(*out, \"%u %s\",\n\t\t\t\t      vdev->ud.sockfd,\n\t\t\t\t      dev_name(&vdev->udev->dev));\n\n\t} else {\n\t\t*out += sprintf(*out, \"000 00000000 \");\n\t\t*out += sprintf(*out, \"0000000000000000 0-0\");\n\t}\n\n\t*out += sprintf(*out, \"\\n\");\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,8 +10,8 @@\n \tif (vdev->ud.status == VDEV_ST_USED) {\n \t\t*out += sprintf(*out, \"%03u %08x \",\n \t\t\t\t      vdev->speed, vdev->devid);\n-\t\t*out += sprintf(*out, \"%16p %s\",\n-\t\t\t\t      vdev->ud.tcp_socket,\n+\t\t*out += sprintf(*out, \"%u %s\",\n+\t\t\t\t      vdev->ud.sockfd,\n \t\t\t\t      dev_name(&vdev->udev->dev));\n \n \t} else {",
        "function_modified_lines": {
            "added": [
                "\t\t*out += sprintf(*out, \"%u %s\",",
                "\t\t\t\t      vdev->ud.sockfd,"
            ],
            "deleted": [
                "\t\t*out += sprintf(*out, \"%16p %s\",",
                "\t\t\t\t      vdev->ud.tcp_socket,"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The vhci_hcd driver in the Linux Kernel before version 4.14.8 and 4.4.114 allows allows local attackers to disclose kernel memory addresses. Successful exploitation requires that a USB device is attached over IP.",
        "id": 1346
    },
    {
        "cve_id": "CVE-2017-2584",
        "code_before_change": "static int em_fxsave(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct fxregs_state fx_state;\n\tsize_t size;\n\tint rc;\n\n\trc = check_fxsr(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tctxt->ops->get_fpu(ctxt);\n\n\trc = asm_safe(\"fxsave %[fx]\", , [fx] \"+m\"(fx_state));\n\n\tctxt->ops->put_fpu(ctxt);\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tif (ctxt->ops->get_cr(ctxt, 4) & X86_CR4_OSFXSR)\n\t\tsize = offsetof(struct fxregs_state, xmm_space[8 * 16/4]);\n\telse\n\t\tsize = offsetof(struct fxregs_state, xmm_space[0]);\n\n\treturn segmented_write(ctxt, ctxt->memop.addr.mem, &fx_state, size);\n}",
        "code_after_change": "static int em_fxsave(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct fxregs_state fx_state;\n\tsize_t size;\n\tint rc;\n\n\trc = check_fxsr(ctxt);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tctxt->ops->get_fpu(ctxt);\n\n\trc = asm_safe(\"fxsave %[fx]\", , [fx] \"+m\"(fx_state));\n\n\tctxt->ops->put_fpu(ctxt);\n\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tif (ctxt->ops->get_cr(ctxt, 4) & X86_CR4_OSFXSR)\n\t\tsize = offsetof(struct fxregs_state, xmm_space[8 * 16/4]);\n\telse\n\t\tsize = offsetof(struct fxregs_state, xmm_space[0]);\n\n\treturn segmented_write_std(ctxt, ctxt->memop.addr.mem, &fx_state, size);\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,5 +22,5 @@\n \telse\n \t\tsize = offsetof(struct fxregs_state, xmm_space[0]);\n \n-\treturn segmented_write(ctxt, ctxt->memop.addr.mem, &fx_state, size);\n+\treturn segmented_write_std(ctxt, ctxt->memop.addr.mem, &fx_state, size);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn segmented_write_std(ctxt, ctxt->memop.addr.mem, &fx_state, size);"
            ],
            "deleted": [
                "\treturn segmented_write(ctxt, ctxt->memop.addr.mem, &fx_state, size);"
            ]
        },
        "cwe": [
            "CWE-200",
            "CWE-416"
        ],
        "cve_description": "arch/x86/kvm/emulate.c in the Linux kernel through 4.9.3 allows local users to obtain sensitive information from kernel memory or cause a denial of service (use-after-free) via a crafted application that leverages instruction emulation for fxrstor, fxsave, sgdt, and sidt.",
        "id": 1446
    },
    {
        "cve_id": "CVE-2014-2038",
        "code_before_change": "static int nfs_can_extend_write(struct file *file, struct page *page, struct inode *inode)\n{\n\tif (file->f_flags & O_DSYNC)\n\t\treturn 0;\n\tif (NFS_PROTO(inode)->have_delegation(inode, FMODE_WRITE))\n\t\treturn 1;\n\tif (nfs_write_pageuptodate(page, inode) && (inode->i_flock == NULL ||\n\t\t\t(inode->i_flock->fl_start == 0 &&\n\t\t\tinode->i_flock->fl_end == OFFSET_MAX &&\n\t\t\tinode->i_flock->fl_type != F_RDLCK)))\n\t\treturn 1;\n\treturn 0;\n}",
        "code_after_change": "static int nfs_can_extend_write(struct file *file, struct page *page, struct inode *inode)\n{\n\tif (file->f_flags & O_DSYNC)\n\t\treturn 0;\n\tif (!nfs_write_pageuptodate(page, inode))\n\t\treturn 0;\n\tif (NFS_PROTO(inode)->have_delegation(inode, FMODE_WRITE))\n\t\treturn 1;\n\tif (inode->i_flock == NULL || (inode->i_flock->fl_start == 0 &&\n\t\t\tinode->i_flock->fl_end == OFFSET_MAX &&\n\t\t\tinode->i_flock->fl_type != F_RDLCK))\n\t\treturn 1;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,12 +2,13 @@\n {\n \tif (file->f_flags & O_DSYNC)\n \t\treturn 0;\n+\tif (!nfs_write_pageuptodate(page, inode))\n+\t\treturn 0;\n \tif (NFS_PROTO(inode)->have_delegation(inode, FMODE_WRITE))\n \t\treturn 1;\n-\tif (nfs_write_pageuptodate(page, inode) && (inode->i_flock == NULL ||\n-\t\t\t(inode->i_flock->fl_start == 0 &&\n+\tif (inode->i_flock == NULL || (inode->i_flock->fl_start == 0 &&\n \t\t\tinode->i_flock->fl_end == OFFSET_MAX &&\n-\t\t\tinode->i_flock->fl_type != F_RDLCK)))\n+\t\t\tinode->i_flock->fl_type != F_RDLCK))\n \t\treturn 1;\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!nfs_write_pageuptodate(page, inode))",
                "\t\treturn 0;",
                "\tif (inode->i_flock == NULL || (inode->i_flock->fl_start == 0 &&",
                "\t\t\tinode->i_flock->fl_type != F_RDLCK))"
            ],
            "deleted": [
                "\tif (nfs_write_pageuptodate(page, inode) && (inode->i_flock == NULL ||",
                "\t\t\t(inode->i_flock->fl_start == 0 &&",
                "\t\t\tinode->i_flock->fl_type != F_RDLCK)))"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The nfs_can_extend_write function in fs/nfs/write.c in the Linux kernel before 3.13.3 relies on a write delegation to extend a write operation without a certain up-to-date verification, which allows local users to obtain sensitive information from kernel memory in opportunistic circumstances by writing to a file in an NFS filesystem and then reading the same file.",
        "id": 479
    },
    {
        "cve_id": "CVE-2015-8964",
        "code_before_change": "static void tty_set_termios_ldisc(struct tty_struct *tty, int num)\n{\n\tdown_write(&tty->termios_rwsem);\n\ttty->termios.c_line = num;\n\tup_write(&tty->termios_rwsem);\n}",
        "code_after_change": "static void tty_set_termios_ldisc(struct tty_struct *tty, int num)\n{\n\tdown_write(&tty->termios_rwsem);\n\ttty->termios.c_line = num;\n\tup_write(&tty->termios_rwsem);\n\n\ttty->disc_data = NULL;\n\ttty->receive_room = 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,4 +3,7 @@\n \tdown_write(&tty->termios_rwsem);\n \ttty->termios.c_line = num;\n \tup_write(&tty->termios_rwsem);\n+\n+\ttty->disc_data = NULL;\n+\ttty->receive_room = 0;\n }",
        "function_modified_lines": {
            "added": [
                "",
                "\ttty->disc_data = NULL;",
                "\ttty->receive_room = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The tty_set_termios_ldisc function in drivers/tty/tty_ldisc.c in the Linux kernel before 4.5 allows local users to obtain sensitive information from kernel memory by reading a tty data structure.",
        "id": 874
    },
    {
        "cve_id": "CVE-2013-3228",
        "code_before_change": "static int irda_recvmsg_dgram(struct kiocb *iocb, struct socket *sock,\n\t\t\t      struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct irda_sock *self = irda_sk(sk);\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\n\tIRDA_DEBUG(4, \"%s()\\n\", __func__);\n\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\tflags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tskb_reset_transport_header(skb);\n\tcopied = skb->len;\n\n\tif (copied > size) {\n\t\tIRDA_DEBUG(2, \"%s(), Received truncated frame (%zd < %zd)!\\n\",\n\t\t\t   __func__, copied, size);\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\tskb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tskb_free_datagram(sk, skb);\n\n\t/*\n\t *  Check if we have previously stopped IrTTP and we know\n\t *  have more free space in our rx_queue. If so tell IrTTP\n\t *  to start delivering frames again before our rx_queue gets\n\t *  empty\n\t */\n\tif (self->rx_flow == FLOW_STOP) {\n\t\tif ((atomic_read(&sk->sk_rmem_alloc) << 2) <= sk->sk_rcvbuf) {\n\t\t\tIRDA_DEBUG(2, \"%s(), Starting IrTTP\\n\", __func__);\n\t\t\tself->rx_flow = FLOW_START;\n\t\t\tirttp_flow_request(self->tsap, FLOW_START);\n\t\t}\n\t}\n\n\treturn copied;\n}",
        "code_after_change": "static int irda_recvmsg_dgram(struct kiocb *iocb, struct socket *sock,\n\t\t\t      struct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct irda_sock *self = irda_sk(sk);\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\n\tIRDA_DEBUG(4, \"%s()\\n\", __func__);\n\n\tmsg->msg_namelen = 0;\n\n\tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n\t\t\t\tflags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tskb_reset_transport_header(skb);\n\tcopied = skb->len;\n\n\tif (copied > size) {\n\t\tIRDA_DEBUG(2, \"%s(), Received truncated frame (%zd < %zd)!\\n\",\n\t\t\t   __func__, copied, size);\n\t\tcopied = size;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\tskb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\n\tskb_free_datagram(sk, skb);\n\n\t/*\n\t *  Check if we have previously stopped IrTTP and we know\n\t *  have more free space in our rx_queue. If so tell IrTTP\n\t *  to start delivering frames again before our rx_queue gets\n\t *  empty\n\t */\n\tif (self->rx_flow == FLOW_STOP) {\n\t\tif ((atomic_read(&sk->sk_rmem_alloc) << 2) <= sk->sk_rcvbuf) {\n\t\t\tIRDA_DEBUG(2, \"%s(), Starting IrTTP\\n\", __func__);\n\t\t\tself->rx_flow = FLOW_START;\n\t\t\tirttp_flow_request(self->tsap, FLOW_START);\n\t\t}\n\t}\n\n\treturn copied;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,8 @@\n \tint err;\n \n \tIRDA_DEBUG(4, \"%s()\\n\", __func__);\n+\n+\tmsg->msg_namelen = 0;\n \n \tskb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,\n \t\t\t\tflags & MSG_DONTWAIT, &err);",
        "function_modified_lines": {
            "added": [
                "",
                "\tmsg->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The irda_recvmsg_dgram function in net/irda/af_irda.c in the Linux kernel before 3.9-rc7 does not initialize a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 268
    },
    {
        "cve_id": "CVE-2013-4516",
        "code_before_change": "static int mp_get_count(struct sb_uart_state *state, struct serial_icounter_struct *icnt)\n{\n\tstruct serial_icounter_struct icount;\n\tstruct sb_uart_icount cnow;\n\tstruct sb_uart_port *port = state->port;\n\n\tspin_lock_irq(&port->lock);\n\tmemcpy(&cnow, &port->icount, sizeof(struct sb_uart_icount));\n\tspin_unlock_irq(&port->lock);\n\n\ticount.cts         = cnow.cts;\n\ticount.dsr         = cnow.dsr;\n\ticount.rng         = cnow.rng;\n\ticount.dcd         = cnow.dcd;\n\ticount.rx          = cnow.rx;\n\ticount.tx          = cnow.tx;\n\ticount.frame       = cnow.frame;\n\ticount.overrun     = cnow.overrun;\n\ticount.parity      = cnow.parity;\n\ticount.brk         = cnow.brk;\n\ticount.buf_overrun = cnow.buf_overrun;\n\n\treturn copy_to_user(icnt, &icount, sizeof(icount)) ? -EFAULT : 0;\n}",
        "code_after_change": "static int mp_get_count(struct sb_uart_state *state, struct serial_icounter_struct *icnt)\n{\n\tstruct serial_icounter_struct icount = {};\n\tstruct sb_uart_icount cnow;\n\tstruct sb_uart_port *port = state->port;\n\n\tspin_lock_irq(&port->lock);\n\tmemcpy(&cnow, &port->icount, sizeof(struct sb_uart_icount));\n\tspin_unlock_irq(&port->lock);\n\n\ticount.cts         = cnow.cts;\n\ticount.dsr         = cnow.dsr;\n\ticount.rng         = cnow.rng;\n\ticount.dcd         = cnow.dcd;\n\ticount.rx          = cnow.rx;\n\ticount.tx          = cnow.tx;\n\ticount.frame       = cnow.frame;\n\ticount.overrun     = cnow.overrun;\n\ticount.parity      = cnow.parity;\n\ticount.brk         = cnow.brk;\n\ticount.buf_overrun = cnow.buf_overrun;\n\n\treturn copy_to_user(icnt, &icount, sizeof(icount)) ? -EFAULT : 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static int mp_get_count(struct sb_uart_state *state, struct serial_icounter_struct *icnt)\n {\n-\tstruct serial_icounter_struct icount;\n+\tstruct serial_icounter_struct icount = {};\n \tstruct sb_uart_icount cnow;\n \tstruct sb_uart_port *port = state->port;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct serial_icounter_struct icount = {};"
            ],
            "deleted": [
                "\tstruct serial_icounter_struct icount;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The mp_get_count function in drivers/staging/sb105x/sb_pci_mp.c in the Linux kernel before 3.12 does not initialize a certain data structure, which allows local users to obtain sensitive information from kernel stack memory via a TIOCGICOUNT ioctl call.",
        "id": 327
    },
    {
        "cve_id": "CVE-2017-14156",
        "code_before_change": "static int atyfb_ioctl(struct fb_info *info, u_int cmd, u_long arg)\n{\n\tstruct atyfb_par *par = (struct atyfb_par *) info->par;\n#ifdef __sparc__\n\tstruct fbtype fbtyp;\n#endif\n\n\tswitch (cmd) {\n#ifdef __sparc__\n\tcase FBIOGTYPE:\n\t\tfbtyp.fb_type = FBTYPE_PCI_GENERIC;\n\t\tfbtyp.fb_width = par->crtc.vxres;\n\t\tfbtyp.fb_height = par->crtc.vyres;\n\t\tfbtyp.fb_depth = info->var.bits_per_pixel;\n\t\tfbtyp.fb_cmsize = info->cmap.len;\n\t\tfbtyp.fb_size = info->fix.smem_len;\n\t\tif (copy_to_user((struct fbtype __user *) arg, &fbtyp,\n\t\t\t\t sizeof(fbtyp)))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n#endif /* __sparc__ */\n\n\tcase FBIO_WAITFORVSYNC:\n\t\t{\n\t\t\tu32 crtc;\n\n\t\t\tif (get_user(crtc, (__u32 __user *) arg))\n\t\t\t\treturn -EFAULT;\n\n\t\t\treturn aty_waitforvblank(par, crtc);\n\t\t}\n\n#if defined(DEBUG) && defined(CONFIG_FB_ATY_CT)\n\tcase ATYIO_CLKR:\n\t\tif (M64_HAS(INTEGRATED)) {\n\t\t\tstruct atyclk clk;\n\t\t\tunion aty_pll *pll = &par->pll;\n\t\t\tu32 dsp_config = pll->ct.dsp_config;\n\t\t\tu32 dsp_on_off = pll->ct.dsp_on_off;\n\t\t\tclk.ref_clk_per = par->ref_clk_per;\n\t\t\tclk.pll_ref_div = pll->ct.pll_ref_div;\n\t\t\tclk.mclk_fb_div = pll->ct.mclk_fb_div;\n\t\t\tclk.mclk_post_div = pll->ct.mclk_post_div_real;\n\t\t\tclk.mclk_fb_mult = pll->ct.mclk_fb_mult;\n\t\t\tclk.xclk_post_div = pll->ct.xclk_post_div_real;\n\t\t\tclk.vclk_fb_div = pll->ct.vclk_fb_div;\n\t\t\tclk.vclk_post_div = pll->ct.vclk_post_div_real;\n\t\t\tclk.dsp_xclks_per_row = dsp_config & 0x3fff;\n\t\t\tclk.dsp_loop_latency = (dsp_config >> 16) & 0xf;\n\t\t\tclk.dsp_precision = (dsp_config >> 20) & 7;\n\t\t\tclk.dsp_off = dsp_on_off & 0x7ff;\n\t\t\tclk.dsp_on = (dsp_on_off >> 16) & 0x7ff;\n\t\t\tif (copy_to_user((struct atyclk __user *) arg, &clk,\n\t\t\t\t\t sizeof(clk)))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase ATYIO_CLKW:\n\t\tif (M64_HAS(INTEGRATED)) {\n\t\t\tstruct atyclk clk;\n\t\t\tunion aty_pll *pll = &par->pll;\n\t\t\tif (copy_from_user(&clk, (struct atyclk __user *) arg,\n\t\t\t\t\t   sizeof(clk)))\n\t\t\t\treturn -EFAULT;\n\t\t\tpar->ref_clk_per = clk.ref_clk_per;\n\t\t\tpll->ct.pll_ref_div = clk.pll_ref_div;\n\t\t\tpll->ct.mclk_fb_div = clk.mclk_fb_div;\n\t\t\tpll->ct.mclk_post_div_real = clk.mclk_post_div;\n\t\t\tpll->ct.mclk_fb_mult = clk.mclk_fb_mult;\n\t\t\tpll->ct.xclk_post_div_real = clk.xclk_post_div;\n\t\t\tpll->ct.vclk_fb_div = clk.vclk_fb_div;\n\t\t\tpll->ct.vclk_post_div_real = clk.vclk_post_div;\n\t\t\tpll->ct.dsp_config = (clk.dsp_xclks_per_row & 0x3fff) |\n\t\t\t\t((clk.dsp_loop_latency & 0xf) << 16) |\n\t\t\t\t((clk.dsp_precision & 7) << 20);\n\t\t\tpll->ct.dsp_on_off = (clk.dsp_off & 0x7ff) |\n\t\t\t\t((clk.dsp_on & 0x7ff) << 16);\n\t\t\t/*aty_calc_pll_ct(info, &pll->ct);*/\n\t\t\taty_set_pll_ct(info, pll);\n\t\t} else\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase ATYIO_FEATR:\n\t\tif (get_user(par->features, (u32 __user *) arg))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\tcase ATYIO_FEATW:\n\t\tif (put_user(par->features, (u32 __user *) arg))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n#endif /* DEBUG && CONFIG_FB_ATY_CT */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int atyfb_ioctl(struct fb_info *info, u_int cmd, u_long arg)\n{\n\tstruct atyfb_par *par = (struct atyfb_par *) info->par;\n#ifdef __sparc__\n\tstruct fbtype fbtyp;\n#endif\n\n\tswitch (cmd) {\n#ifdef __sparc__\n\tcase FBIOGTYPE:\n\t\tfbtyp.fb_type = FBTYPE_PCI_GENERIC;\n\t\tfbtyp.fb_width = par->crtc.vxres;\n\t\tfbtyp.fb_height = par->crtc.vyres;\n\t\tfbtyp.fb_depth = info->var.bits_per_pixel;\n\t\tfbtyp.fb_cmsize = info->cmap.len;\n\t\tfbtyp.fb_size = info->fix.smem_len;\n\t\tif (copy_to_user((struct fbtype __user *) arg, &fbtyp,\n\t\t\t\t sizeof(fbtyp)))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n#endif /* __sparc__ */\n\n\tcase FBIO_WAITFORVSYNC:\n\t\t{\n\t\t\tu32 crtc;\n\n\t\t\tif (get_user(crtc, (__u32 __user *) arg))\n\t\t\t\treturn -EFAULT;\n\n\t\t\treturn aty_waitforvblank(par, crtc);\n\t\t}\n\n#if defined(DEBUG) && defined(CONFIG_FB_ATY_CT)\n\tcase ATYIO_CLKR:\n\t\tif (M64_HAS(INTEGRATED)) {\n\t\t\tstruct atyclk clk = { 0 };\n\t\t\tunion aty_pll *pll = &par->pll;\n\t\t\tu32 dsp_config = pll->ct.dsp_config;\n\t\t\tu32 dsp_on_off = pll->ct.dsp_on_off;\n\t\t\tclk.ref_clk_per = par->ref_clk_per;\n\t\t\tclk.pll_ref_div = pll->ct.pll_ref_div;\n\t\t\tclk.mclk_fb_div = pll->ct.mclk_fb_div;\n\t\t\tclk.mclk_post_div = pll->ct.mclk_post_div_real;\n\t\t\tclk.mclk_fb_mult = pll->ct.mclk_fb_mult;\n\t\t\tclk.xclk_post_div = pll->ct.xclk_post_div_real;\n\t\t\tclk.vclk_fb_div = pll->ct.vclk_fb_div;\n\t\t\tclk.vclk_post_div = pll->ct.vclk_post_div_real;\n\t\t\tclk.dsp_xclks_per_row = dsp_config & 0x3fff;\n\t\t\tclk.dsp_loop_latency = (dsp_config >> 16) & 0xf;\n\t\t\tclk.dsp_precision = (dsp_config >> 20) & 7;\n\t\t\tclk.dsp_off = dsp_on_off & 0x7ff;\n\t\t\tclk.dsp_on = (dsp_on_off >> 16) & 0x7ff;\n\t\t\tif (copy_to_user((struct atyclk __user *) arg, &clk,\n\t\t\t\t\t sizeof(clk)))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase ATYIO_CLKW:\n\t\tif (M64_HAS(INTEGRATED)) {\n\t\t\tstruct atyclk clk;\n\t\t\tunion aty_pll *pll = &par->pll;\n\t\t\tif (copy_from_user(&clk, (struct atyclk __user *) arg,\n\t\t\t\t\t   sizeof(clk)))\n\t\t\t\treturn -EFAULT;\n\t\t\tpar->ref_clk_per = clk.ref_clk_per;\n\t\t\tpll->ct.pll_ref_div = clk.pll_ref_div;\n\t\t\tpll->ct.mclk_fb_div = clk.mclk_fb_div;\n\t\t\tpll->ct.mclk_post_div_real = clk.mclk_post_div;\n\t\t\tpll->ct.mclk_fb_mult = clk.mclk_fb_mult;\n\t\t\tpll->ct.xclk_post_div_real = clk.xclk_post_div;\n\t\t\tpll->ct.vclk_fb_div = clk.vclk_fb_div;\n\t\t\tpll->ct.vclk_post_div_real = clk.vclk_post_div;\n\t\t\tpll->ct.dsp_config = (clk.dsp_xclks_per_row & 0x3fff) |\n\t\t\t\t((clk.dsp_loop_latency & 0xf) << 16) |\n\t\t\t\t((clk.dsp_precision & 7) << 20);\n\t\t\tpll->ct.dsp_on_off = (clk.dsp_off & 0x7ff) |\n\t\t\t\t((clk.dsp_on & 0x7ff) << 16);\n\t\t\t/*aty_calc_pll_ct(info, &pll->ct);*/\n\t\t\taty_set_pll_ct(info, pll);\n\t\t} else\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase ATYIO_FEATR:\n\t\tif (get_user(par->features, (u32 __user *) arg))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\tcase ATYIO_FEATW:\n\t\tif (put_user(par->features, (u32 __user *) arg))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n#endif /* DEBUG && CONFIG_FB_ATY_CT */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -33,7 +33,7 @@\n #if defined(DEBUG) && defined(CONFIG_FB_ATY_CT)\n \tcase ATYIO_CLKR:\n \t\tif (M64_HAS(INTEGRATED)) {\n-\t\t\tstruct atyclk clk;\n+\t\t\tstruct atyclk clk = { 0 };\n \t\t\tunion aty_pll *pll = &par->pll;\n \t\t\tu32 dsp_config = pll->ct.dsp_config;\n \t\t\tu32 dsp_on_off = pll->ct.dsp_on_off;",
        "function_modified_lines": {
            "added": [
                "\t\t\tstruct atyclk clk = { 0 };"
            ],
            "deleted": [
                "\t\t\tstruct atyclk clk;"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The atyfb_ioctl function in drivers/video/fbdev/aty/atyfb_base.c in the Linux kernel through 4.12.10 does not initialize a certain data structure, which allows local users to obtain sensitive information from kernel stack memory by reading locations associated with padding bytes.",
        "id": 1283
    },
    {
        "cve_id": "CVE-2015-8374",
        "code_before_change": "int btrfs_truncate_inode_items(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root,\n\t\t\t       struct inode *inode,\n\t\t\t       u64 new_size, u32 min_type)\n{\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tu64 extent_start = 0;\n\tu64 extent_num_bytes = 0;\n\tu64 extent_offset = 0;\n\tu64 item_end = 0;\n\tu64 last_size = new_size;\n\tu32 found_type = (u8)-1;\n\tint found_extent;\n\tint del_item;\n\tint pending_del_nr = 0;\n\tint pending_del_slot = 0;\n\tint extent_type = -1;\n\tint ret;\n\tint err = 0;\n\tu64 ino = btrfs_ino(inode);\n\tu64 bytes_deleted = 0;\n\tbool be_nice = 0;\n\tbool should_throttle = 0;\n\tbool should_end = 0;\n\n\tBUG_ON(new_size > 0 && min_type != BTRFS_EXTENT_DATA_KEY);\n\n\t/*\n\t * for non-free space inodes and ref cows, we want to back off from\n\t * time to time\n\t */\n\tif (!btrfs_is_free_space_inode(inode) &&\n\t    test_bit(BTRFS_ROOT_REF_COWS, &root->state))\n\t\tbe_nice = 1;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tpath->reada = -1;\n\n\t/*\n\t * We want to drop from the next block forward in case this new size is\n\t * not block aligned since we will be keeping the last block of the\n\t * extent just the way it is.\n\t */\n\tif (test_bit(BTRFS_ROOT_REF_COWS, &root->state) ||\n\t    root == root->fs_info->tree_root)\n\t\tbtrfs_drop_extent_cache(inode, ALIGN(new_size,\n\t\t\t\t\troot->sectorsize), (u64)-1, 0);\n\n\t/*\n\t * This function is also used to drop the items in the log tree before\n\t * we relog the inode, so if root != BTRFS_I(inode)->root, it means\n\t * it is used to drop the loged items. So we shouldn't kill the delayed\n\t * items.\n\t */\n\tif (min_type == 0 && root == BTRFS_I(inode)->root)\n\t\tbtrfs_kill_delayed_inode_items(inode);\n\n\tkey.objectid = ino;\n\tkey.offset = (u64)-1;\n\tkey.type = (u8)-1;\n\nsearch_again:\n\t/*\n\t * with a 16K leaf size and 128MB extents, you can actually queue\n\t * up a huge file in a single leaf.  Most of the time that\n\t * bytes_deleted is > 0, it will be huge by the time we get here\n\t */\n\tif (be_nice && bytes_deleted > 32 * 1024 * 1024) {\n\t\tif (btrfs_should_end_transaction(trans, root)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\n\tpath->leave_spinning = 1;\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\n\tif (ret > 0) {\n\t\t/* there are no items in the tree for us to truncate, we're\n\t\t * done\n\t\t */\n\t\tif (path->slots[0] == 0)\n\t\t\tgoto out;\n\t\tpath->slots[0]--;\n\t}\n\n\twhile (1) {\n\t\tfi = NULL;\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\tfound_type = found_key.type;\n\n\t\tif (found_key.objectid != ino)\n\t\t\tbreak;\n\n\t\tif (found_type < min_type)\n\t\t\tbreak;\n\n\t\titem_end = found_key.offset;\n\t\tif (found_type == BTRFS_EXTENT_DATA_KEY) {\n\t\t\tfi = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t    struct btrfs_file_extent_item);\n\t\t\textent_type = btrfs_file_extent_type(leaf, fi);\n\t\t\tif (extent_type != BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\titem_end +=\n\t\t\t\t    btrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\titem_end += btrfs_file_extent_inline_len(leaf,\n\t\t\t\t\t\t\t path->slots[0], fi);\n\t\t\t}\n\t\t\titem_end--;\n\t\t}\n\t\tif (found_type > min_type) {\n\t\t\tdel_item = 1;\n\t\t} else {\n\t\t\tif (item_end < new_size)\n\t\t\t\tbreak;\n\t\t\tif (found_key.offset >= new_size)\n\t\t\t\tdel_item = 1;\n\t\t\telse\n\t\t\t\tdel_item = 0;\n\t\t}\n\t\tfound_extent = 0;\n\t\t/* FIXME, shrink the extent if the ref count is only 1 */\n\t\tif (found_type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto delete;\n\n\t\tif (del_item)\n\t\t\tlast_size = found_key.offset;\n\t\telse\n\t\t\tlast_size = new_size;\n\n\t\tif (extent_type != BTRFS_FILE_EXTENT_INLINE) {\n\t\t\tu64 num_dec;\n\t\t\textent_start = btrfs_file_extent_disk_bytenr(leaf, fi);\n\t\t\tif (!del_item) {\n\t\t\t\tu64 orig_num_bytes =\n\t\t\t\t\tbtrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t\textent_num_bytes = ALIGN(new_size -\n\t\t\t\t\t\tfound_key.offset,\n\t\t\t\t\t\troot->sectorsize);\n\t\t\t\tbtrfs_set_file_extent_num_bytes(leaf, fi,\n\t\t\t\t\t\t\t extent_num_bytes);\n\t\t\t\tnum_dec = (orig_num_bytes -\n\t\t\t\t\t   extent_num_bytes);\n\t\t\t\tif (test_bit(BTRFS_ROOT_REF_COWS,\n\t\t\t\t\t     &root->state) &&\n\t\t\t\t    extent_start != 0)\n\t\t\t\t\tinode_sub_bytes(inode, num_dec);\n\t\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t\t} else {\n\t\t\t\textent_num_bytes =\n\t\t\t\t\tbtrfs_file_extent_disk_num_bytes(leaf,\n\t\t\t\t\t\t\t\t\t fi);\n\t\t\t\textent_offset = found_key.offset -\n\t\t\t\t\tbtrfs_file_extent_offset(leaf, fi);\n\n\t\t\t\t/* FIXME blocksize != 4096 */\n\t\t\t\tnum_dec = btrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t\tif (extent_start != 0) {\n\t\t\t\t\tfound_extent = 1;\n\t\t\t\t\tif (test_bit(BTRFS_ROOT_REF_COWS,\n\t\t\t\t\t\t     &root->state))\n\t\t\t\t\t\tinode_sub_bytes(inode, num_dec);\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t/*\n\t\t\t * we can't truncate inline items that have had\n\t\t\t * special encodings\n\t\t\t */\n\t\t\tif (!del_item &&\n\t\t\t    btrfs_file_extent_compression(leaf, fi) == 0 &&\n\t\t\t    btrfs_file_extent_encryption(leaf, fi) == 0 &&\n\t\t\t    btrfs_file_extent_other_encoding(leaf, fi) == 0) {\n\t\t\t\tu32 size = new_size - found_key.offset;\n\n\t\t\t\tif (test_bit(BTRFS_ROOT_REF_COWS, &root->state))\n\t\t\t\t\tinode_sub_bytes(inode, item_end + 1 -\n\t\t\t\t\t\t\tnew_size);\n\n\t\t\t\t/*\n\t\t\t\t * update the ram bytes to properly reflect\n\t\t\t\t * the new size of our item\n\t\t\t\t */\n\t\t\t\tbtrfs_set_file_extent_ram_bytes(leaf, fi, size);\n\t\t\t\tsize =\n\t\t\t\t    btrfs_file_extent_calc_inline_size(size);\n\t\t\t\tbtrfs_truncate_item(root, path, size, 1);\n\t\t\t} else if (test_bit(BTRFS_ROOT_REF_COWS,\n\t\t\t\t\t    &root->state)) {\n\t\t\t\tinode_sub_bytes(inode, item_end + 1 -\n\t\t\t\t\t\tfound_key.offset);\n\t\t\t}\n\t\t}\ndelete:\n\t\tif (del_item) {\n\t\t\tif (!pending_del_nr) {\n\t\t\t\t/* no pending yet, add ourselves */\n\t\t\t\tpending_del_slot = path->slots[0];\n\t\t\t\tpending_del_nr = 1;\n\t\t\t} else if (pending_del_nr &&\n\t\t\t\t   path->slots[0] + 1 == pending_del_slot) {\n\t\t\t\t/* hop on the pending chunk */\n\t\t\t\tpending_del_nr++;\n\t\t\t\tpending_del_slot = path->slots[0];\n\t\t\t} else {\n\t\t\t\tBUG();\n\t\t\t}\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tshould_throttle = 0;\n\n\t\tif (found_extent &&\n\t\t    (test_bit(BTRFS_ROOT_REF_COWS, &root->state) ||\n\t\t     root == root->fs_info->tree_root)) {\n\t\t\tbtrfs_set_path_blocking(path);\n\t\t\tbytes_deleted += extent_num_bytes;\n\t\t\tret = btrfs_free_extent(trans, root, extent_start,\n\t\t\t\t\t\textent_num_bytes, 0,\n\t\t\t\t\t\tbtrfs_header_owner(leaf),\n\t\t\t\t\t\tino, extent_offset, 0);\n\t\t\tBUG_ON(ret);\n\t\t\tif (btrfs_should_throttle_delayed_refs(trans, root))\n\t\t\t\tbtrfs_async_run_delayed_refs(root,\n\t\t\t\t\ttrans->delayed_ref_updates * 2, 0);\n\t\t\tif (be_nice) {\n\t\t\t\tif (truncate_space_check(trans, root,\n\t\t\t\t\t\t\t extent_num_bytes)) {\n\t\t\t\t\tshould_end = 1;\n\t\t\t\t}\n\t\t\t\tif (btrfs_should_throttle_delayed_refs(trans,\n\t\t\t\t\t\t\t\t       root)) {\n\t\t\t\t\tshould_throttle = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (found_type == BTRFS_INODE_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tif (path->slots[0] == 0 ||\n\t\t    path->slots[0] != pending_del_slot ||\n\t\t    should_throttle || should_end) {\n\t\t\tif (pending_del_nr) {\n\t\t\t\tret = btrfs_del_items(trans, root, path,\n\t\t\t\t\t\tpending_del_slot,\n\t\t\t\t\t\tpending_del_nr);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\troot, ret);\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tpending_del_nr = 0;\n\t\t\t}\n\t\t\tbtrfs_release_path(path);\n\t\t\tif (should_throttle) {\n\t\t\t\tunsigned long updates = trans->delayed_ref_updates;\n\t\t\t\tif (updates) {\n\t\t\t\t\ttrans->delayed_ref_updates = 0;\n\t\t\t\t\tret = btrfs_run_delayed_refs(trans, root, updates * 2);\n\t\t\t\t\tif (ret && !err)\n\t\t\t\t\t\terr = ret;\n\t\t\t\t}\n\t\t\t}\n\t\t\t/*\n\t\t\t * if we failed to refill our space rsv, bail out\n\t\t\t * and let the transaction restart\n\t\t\t */\n\t\t\tif (should_end) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tgoto search_again;\n\t\t} else {\n\t\t\tpath->slots[0]--;\n\t\t}\n\t}\nout:\n\tif (pending_del_nr) {\n\t\tret = btrfs_del_items(trans, root, path, pending_del_slot,\n\t\t\t\t      pending_del_nr);\n\t\tif (ret)\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t}\nerror:\n\tif (root->root_key.objectid != BTRFS_TREE_LOG_OBJECTID)\n\t\tbtrfs_ordered_update_i_size(inode, last_size, NULL);\n\n\tbtrfs_free_path(path);\n\n\tif (be_nice && bytes_deleted > 32 * 1024 * 1024) {\n\t\tunsigned long updates = trans->delayed_ref_updates;\n\t\tif (updates) {\n\t\t\ttrans->delayed_ref_updates = 0;\n\t\t\tret = btrfs_run_delayed_refs(trans, root, updates * 2);\n\t\t\tif (ret && !err)\n\t\t\t\terr = ret;\n\t\t}\n\t}\n\treturn err;\n}",
        "code_after_change": "int btrfs_truncate_inode_items(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_root *root,\n\t\t\t       struct inode *inode,\n\t\t\t       u64 new_size, u32 min_type)\n{\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tu64 extent_start = 0;\n\tu64 extent_num_bytes = 0;\n\tu64 extent_offset = 0;\n\tu64 item_end = 0;\n\tu64 last_size = new_size;\n\tu32 found_type = (u8)-1;\n\tint found_extent;\n\tint del_item;\n\tint pending_del_nr = 0;\n\tint pending_del_slot = 0;\n\tint extent_type = -1;\n\tint ret;\n\tint err = 0;\n\tu64 ino = btrfs_ino(inode);\n\tu64 bytes_deleted = 0;\n\tbool be_nice = 0;\n\tbool should_throttle = 0;\n\tbool should_end = 0;\n\n\tBUG_ON(new_size > 0 && min_type != BTRFS_EXTENT_DATA_KEY);\n\n\t/*\n\t * for non-free space inodes and ref cows, we want to back off from\n\t * time to time\n\t */\n\tif (!btrfs_is_free_space_inode(inode) &&\n\t    test_bit(BTRFS_ROOT_REF_COWS, &root->state))\n\t\tbe_nice = 1;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tpath->reada = -1;\n\n\t/*\n\t * We want to drop from the next block forward in case this new size is\n\t * not block aligned since we will be keeping the last block of the\n\t * extent just the way it is.\n\t */\n\tif (test_bit(BTRFS_ROOT_REF_COWS, &root->state) ||\n\t    root == root->fs_info->tree_root)\n\t\tbtrfs_drop_extent_cache(inode, ALIGN(new_size,\n\t\t\t\t\troot->sectorsize), (u64)-1, 0);\n\n\t/*\n\t * This function is also used to drop the items in the log tree before\n\t * we relog the inode, so if root != BTRFS_I(inode)->root, it means\n\t * it is used to drop the loged items. So we shouldn't kill the delayed\n\t * items.\n\t */\n\tif (min_type == 0 && root == BTRFS_I(inode)->root)\n\t\tbtrfs_kill_delayed_inode_items(inode);\n\n\tkey.objectid = ino;\n\tkey.offset = (u64)-1;\n\tkey.type = (u8)-1;\n\nsearch_again:\n\t/*\n\t * with a 16K leaf size and 128MB extents, you can actually queue\n\t * up a huge file in a single leaf.  Most of the time that\n\t * bytes_deleted is > 0, it will be huge by the time we get here\n\t */\n\tif (be_nice && bytes_deleted > 32 * 1024 * 1024) {\n\t\tif (btrfs_should_end_transaction(trans, root)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\n\tpath->leave_spinning = 1;\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\n\tif (ret > 0) {\n\t\t/* there are no items in the tree for us to truncate, we're\n\t\t * done\n\t\t */\n\t\tif (path->slots[0] == 0)\n\t\t\tgoto out;\n\t\tpath->slots[0]--;\n\t}\n\n\twhile (1) {\n\t\tfi = NULL;\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\tfound_type = found_key.type;\n\n\t\tif (found_key.objectid != ino)\n\t\t\tbreak;\n\n\t\tif (found_type < min_type)\n\t\t\tbreak;\n\n\t\titem_end = found_key.offset;\n\t\tif (found_type == BTRFS_EXTENT_DATA_KEY) {\n\t\t\tfi = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t    struct btrfs_file_extent_item);\n\t\t\textent_type = btrfs_file_extent_type(leaf, fi);\n\t\t\tif (extent_type != BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\titem_end +=\n\t\t\t\t    btrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t\titem_end += btrfs_file_extent_inline_len(leaf,\n\t\t\t\t\t\t\t path->slots[0], fi);\n\t\t\t}\n\t\t\titem_end--;\n\t\t}\n\t\tif (found_type > min_type) {\n\t\t\tdel_item = 1;\n\t\t} else {\n\t\t\tif (item_end < new_size)\n\t\t\t\tbreak;\n\t\t\tif (found_key.offset >= new_size)\n\t\t\t\tdel_item = 1;\n\t\t\telse\n\t\t\t\tdel_item = 0;\n\t\t}\n\t\tfound_extent = 0;\n\t\t/* FIXME, shrink the extent if the ref count is only 1 */\n\t\tif (found_type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto delete;\n\n\t\tif (del_item)\n\t\t\tlast_size = found_key.offset;\n\t\telse\n\t\t\tlast_size = new_size;\n\n\t\tif (extent_type != BTRFS_FILE_EXTENT_INLINE) {\n\t\t\tu64 num_dec;\n\t\t\textent_start = btrfs_file_extent_disk_bytenr(leaf, fi);\n\t\t\tif (!del_item) {\n\t\t\t\tu64 orig_num_bytes =\n\t\t\t\t\tbtrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t\textent_num_bytes = ALIGN(new_size -\n\t\t\t\t\t\tfound_key.offset,\n\t\t\t\t\t\troot->sectorsize);\n\t\t\t\tbtrfs_set_file_extent_num_bytes(leaf, fi,\n\t\t\t\t\t\t\t extent_num_bytes);\n\t\t\t\tnum_dec = (orig_num_bytes -\n\t\t\t\t\t   extent_num_bytes);\n\t\t\t\tif (test_bit(BTRFS_ROOT_REF_COWS,\n\t\t\t\t\t     &root->state) &&\n\t\t\t\t    extent_start != 0)\n\t\t\t\t\tinode_sub_bytes(inode, num_dec);\n\t\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t\t} else {\n\t\t\t\textent_num_bytes =\n\t\t\t\t\tbtrfs_file_extent_disk_num_bytes(leaf,\n\t\t\t\t\t\t\t\t\t fi);\n\t\t\t\textent_offset = found_key.offset -\n\t\t\t\t\tbtrfs_file_extent_offset(leaf, fi);\n\n\t\t\t\t/* FIXME blocksize != 4096 */\n\t\t\t\tnum_dec = btrfs_file_extent_num_bytes(leaf, fi);\n\t\t\t\tif (extent_start != 0) {\n\t\t\t\t\tfound_extent = 1;\n\t\t\t\t\tif (test_bit(BTRFS_ROOT_REF_COWS,\n\t\t\t\t\t\t     &root->state))\n\t\t\t\t\t\tinode_sub_bytes(inode, num_dec);\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\t/*\n\t\t\t * we can't truncate inline items that have had\n\t\t\t * special encodings\n\t\t\t */\n\t\t\tif (!del_item &&\n\t\t\t    btrfs_file_extent_encryption(leaf, fi) == 0 &&\n\t\t\t    btrfs_file_extent_other_encoding(leaf, fi) == 0) {\n\n\t\t\t\t/*\n\t\t\t\t * Need to release path in order to truncate a\n\t\t\t\t * compressed extent. So delete any accumulated\n\t\t\t\t * extent items so far.\n\t\t\t\t */\n\t\t\t\tif (btrfs_file_extent_compression(leaf, fi) !=\n\t\t\t\t    BTRFS_COMPRESS_NONE && pending_del_nr) {\n\t\t\t\t\terr = btrfs_del_items(trans, root, path,\n\t\t\t\t\t\t\t      pending_del_slot,\n\t\t\t\t\t\t\t      pending_del_nr);\n\t\t\t\t\tif (err) {\n\t\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\t\troot,\n\t\t\t\t\t\t\t\t\terr);\n\t\t\t\t\t\tgoto error;\n\t\t\t\t\t}\n\t\t\t\t\tpending_del_nr = 0;\n\t\t\t\t}\n\n\t\t\t\terr = truncate_inline_extent(inode, path,\n\t\t\t\t\t\t\t     &found_key,\n\t\t\t\t\t\t\t     item_end,\n\t\t\t\t\t\t\t     new_size);\n\t\t\t\tif (err) {\n\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\troot, err);\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t} else if (test_bit(BTRFS_ROOT_REF_COWS,\n\t\t\t\t\t    &root->state)) {\n\t\t\t\tinode_sub_bytes(inode, item_end + 1 - new_size);\n\t\t\t}\n\t\t}\ndelete:\n\t\tif (del_item) {\n\t\t\tif (!pending_del_nr) {\n\t\t\t\t/* no pending yet, add ourselves */\n\t\t\t\tpending_del_slot = path->slots[0];\n\t\t\t\tpending_del_nr = 1;\n\t\t\t} else if (pending_del_nr &&\n\t\t\t\t   path->slots[0] + 1 == pending_del_slot) {\n\t\t\t\t/* hop on the pending chunk */\n\t\t\t\tpending_del_nr++;\n\t\t\t\tpending_del_slot = path->slots[0];\n\t\t\t} else {\n\t\t\t\tBUG();\n\t\t\t}\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tshould_throttle = 0;\n\n\t\tif (found_extent &&\n\t\t    (test_bit(BTRFS_ROOT_REF_COWS, &root->state) ||\n\t\t     root == root->fs_info->tree_root)) {\n\t\t\tbtrfs_set_path_blocking(path);\n\t\t\tbytes_deleted += extent_num_bytes;\n\t\t\tret = btrfs_free_extent(trans, root, extent_start,\n\t\t\t\t\t\textent_num_bytes, 0,\n\t\t\t\t\t\tbtrfs_header_owner(leaf),\n\t\t\t\t\t\tino, extent_offset, 0);\n\t\t\tBUG_ON(ret);\n\t\t\tif (btrfs_should_throttle_delayed_refs(trans, root))\n\t\t\t\tbtrfs_async_run_delayed_refs(root,\n\t\t\t\t\ttrans->delayed_ref_updates * 2, 0);\n\t\t\tif (be_nice) {\n\t\t\t\tif (truncate_space_check(trans, root,\n\t\t\t\t\t\t\t extent_num_bytes)) {\n\t\t\t\t\tshould_end = 1;\n\t\t\t\t}\n\t\t\t\tif (btrfs_should_throttle_delayed_refs(trans,\n\t\t\t\t\t\t\t\t       root)) {\n\t\t\t\t\tshould_throttle = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (found_type == BTRFS_INODE_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tif (path->slots[0] == 0 ||\n\t\t    path->slots[0] != pending_del_slot ||\n\t\t    should_throttle || should_end) {\n\t\t\tif (pending_del_nr) {\n\t\t\t\tret = btrfs_del_items(trans, root, path,\n\t\t\t\t\t\tpending_del_slot,\n\t\t\t\t\t\tpending_del_nr);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_abort_transaction(trans,\n\t\t\t\t\t\t\t\troot, ret);\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tpending_del_nr = 0;\n\t\t\t}\n\t\t\tbtrfs_release_path(path);\n\t\t\tif (should_throttle) {\n\t\t\t\tunsigned long updates = trans->delayed_ref_updates;\n\t\t\t\tif (updates) {\n\t\t\t\t\ttrans->delayed_ref_updates = 0;\n\t\t\t\t\tret = btrfs_run_delayed_refs(trans, root, updates * 2);\n\t\t\t\t\tif (ret && !err)\n\t\t\t\t\t\terr = ret;\n\t\t\t\t}\n\t\t\t}\n\t\t\t/*\n\t\t\t * if we failed to refill our space rsv, bail out\n\t\t\t * and let the transaction restart\n\t\t\t */\n\t\t\tif (should_end) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tgoto search_again;\n\t\t} else {\n\t\t\tpath->slots[0]--;\n\t\t}\n\t}\nout:\n\tif (pending_del_nr) {\n\t\tret = btrfs_del_items(trans, root, path, pending_del_slot,\n\t\t\t\t      pending_del_nr);\n\t\tif (ret)\n\t\t\tbtrfs_abort_transaction(trans, root, ret);\n\t}\nerror:\n\tif (root->root_key.objectid != BTRFS_TREE_LOG_OBJECTID)\n\t\tbtrfs_ordered_update_i_size(inode, last_size, NULL);\n\n\tbtrfs_free_path(path);\n\n\tif (be_nice && bytes_deleted > 32 * 1024 * 1024) {\n\t\tunsigned long updates = trans->delayed_ref_updates;\n\t\tif (updates) {\n\t\t\ttrans->delayed_ref_updates = 0;\n\t\t\tret = btrfs_run_delayed_refs(trans, root, updates * 2);\n\t\t\tif (ret && !err)\n\t\t\t\terr = ret;\n\t\t}\n\t}\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -181,27 +181,40 @@\n \t\t\t * special encodings\n \t\t\t */\n \t\t\tif (!del_item &&\n-\t\t\t    btrfs_file_extent_compression(leaf, fi) == 0 &&\n \t\t\t    btrfs_file_extent_encryption(leaf, fi) == 0 &&\n \t\t\t    btrfs_file_extent_other_encoding(leaf, fi) == 0) {\n-\t\t\t\tu32 size = new_size - found_key.offset;\n-\n-\t\t\t\tif (test_bit(BTRFS_ROOT_REF_COWS, &root->state))\n-\t\t\t\t\tinode_sub_bytes(inode, item_end + 1 -\n-\t\t\t\t\t\t\tnew_size);\n \n \t\t\t\t/*\n-\t\t\t\t * update the ram bytes to properly reflect\n-\t\t\t\t * the new size of our item\n+\t\t\t\t * Need to release path in order to truncate a\n+\t\t\t\t * compressed extent. So delete any accumulated\n+\t\t\t\t * extent items so far.\n \t\t\t\t */\n-\t\t\t\tbtrfs_set_file_extent_ram_bytes(leaf, fi, size);\n-\t\t\t\tsize =\n-\t\t\t\t    btrfs_file_extent_calc_inline_size(size);\n-\t\t\t\tbtrfs_truncate_item(root, path, size, 1);\n+\t\t\t\tif (btrfs_file_extent_compression(leaf, fi) !=\n+\t\t\t\t    BTRFS_COMPRESS_NONE && pending_del_nr) {\n+\t\t\t\t\terr = btrfs_del_items(trans, root, path,\n+\t\t\t\t\t\t\t      pending_del_slot,\n+\t\t\t\t\t\t\t      pending_del_nr);\n+\t\t\t\t\tif (err) {\n+\t\t\t\t\t\tbtrfs_abort_transaction(trans,\n+\t\t\t\t\t\t\t\t\troot,\n+\t\t\t\t\t\t\t\t\terr);\n+\t\t\t\t\t\tgoto error;\n+\t\t\t\t\t}\n+\t\t\t\t\tpending_del_nr = 0;\n+\t\t\t\t}\n+\n+\t\t\t\terr = truncate_inline_extent(inode, path,\n+\t\t\t\t\t\t\t     &found_key,\n+\t\t\t\t\t\t\t     item_end,\n+\t\t\t\t\t\t\t     new_size);\n+\t\t\t\tif (err) {\n+\t\t\t\t\tbtrfs_abort_transaction(trans,\n+\t\t\t\t\t\t\t\troot, err);\n+\t\t\t\t\tgoto error;\n+\t\t\t\t}\n \t\t\t} else if (test_bit(BTRFS_ROOT_REF_COWS,\n \t\t\t\t\t    &root->state)) {\n-\t\t\t\tinode_sub_bytes(inode, item_end + 1 -\n-\t\t\t\t\t\tfound_key.offset);\n+\t\t\t\tinode_sub_bytes(inode, item_end + 1 - new_size);\n \t\t\t}\n \t\t}\n delete:",
        "function_modified_lines": {
            "added": [
                "\t\t\t\t * Need to release path in order to truncate a",
                "\t\t\t\t * compressed extent. So delete any accumulated",
                "\t\t\t\t * extent items so far.",
                "\t\t\t\tif (btrfs_file_extent_compression(leaf, fi) !=",
                "\t\t\t\t    BTRFS_COMPRESS_NONE && pending_del_nr) {",
                "\t\t\t\t\terr = btrfs_del_items(trans, root, path,",
                "\t\t\t\t\t\t\t      pending_del_slot,",
                "\t\t\t\t\t\t\t      pending_del_nr);",
                "\t\t\t\t\tif (err) {",
                "\t\t\t\t\t\tbtrfs_abort_transaction(trans,",
                "\t\t\t\t\t\t\t\t\troot,",
                "\t\t\t\t\t\t\t\t\terr);",
                "\t\t\t\t\t\tgoto error;",
                "\t\t\t\t\t}",
                "\t\t\t\t\tpending_del_nr = 0;",
                "\t\t\t\t}",
                "",
                "\t\t\t\terr = truncate_inline_extent(inode, path,",
                "\t\t\t\t\t\t\t     &found_key,",
                "\t\t\t\t\t\t\t     item_end,",
                "\t\t\t\t\t\t\t     new_size);",
                "\t\t\t\tif (err) {",
                "\t\t\t\t\tbtrfs_abort_transaction(trans,",
                "\t\t\t\t\t\t\t\troot, err);",
                "\t\t\t\t\tgoto error;",
                "\t\t\t\t}",
                "\t\t\t\tinode_sub_bytes(inode, item_end + 1 - new_size);"
            ],
            "deleted": [
                "\t\t\t    btrfs_file_extent_compression(leaf, fi) == 0 &&",
                "\t\t\t\tu32 size = new_size - found_key.offset;",
                "",
                "\t\t\t\tif (test_bit(BTRFS_ROOT_REF_COWS, &root->state))",
                "\t\t\t\t\tinode_sub_bytes(inode, item_end + 1 -",
                "\t\t\t\t\t\t\tnew_size);",
                "\t\t\t\t * update the ram bytes to properly reflect",
                "\t\t\t\t * the new size of our item",
                "\t\t\t\tbtrfs_set_file_extent_ram_bytes(leaf, fi, size);",
                "\t\t\t\tsize =",
                "\t\t\t\t    btrfs_file_extent_calc_inline_size(size);",
                "\t\t\t\tbtrfs_truncate_item(root, path, size, 1);",
                "\t\t\t\tinode_sub_bytes(inode, item_end + 1 -",
                "\t\t\t\t\t\tfound_key.offset);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "fs/btrfs/inode.c in the Linux kernel before 4.3.3 mishandles compressed inline extents, which allows local users to obtain sensitive pre-truncation information from a file via a clone action.",
        "id": 821
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "static inline void __thread_fpu_end(struct task_struct *tsk)\n{\n\t__thread_clear_has_fpu(tsk);\n\tif (!use_xsave())\n\t\tstts();\n}",
        "code_after_change": "static inline void __thread_fpu_end(struct task_struct *tsk)\n{\n\t__thread_clear_has_fpu(tsk);\n\tif (!use_eager_fpu())\n\t\tstts();\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,6 @@\n static inline void __thread_fpu_end(struct task_struct *tsk)\n {\n \t__thread_clear_has_fpu(tsk);\n-\tif (!use_xsave())\n+\tif (!use_eager_fpu())\n \t\tstts();\n }",
        "function_modified_lines": {
            "added": [
                "\tif (!use_eager_fpu())"
            ],
            "deleted": [
                "\tif (!use_xsave())"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1799
    },
    {
        "cve_id": "CVE-2017-16911",
        "code_before_change": "static ssize_t store_attach(struct device *dev, struct device_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\tstruct socket *socket;\n\tint sockfd = 0;\n\t__u32 port = 0, pdev_nr = 0, rhport = 0, devid = 0, speed = 0;\n\tstruct usb_hcd *hcd;\n\tstruct vhci_hcd *vhci_hcd;\n\tstruct vhci_device *vdev;\n\tstruct vhci *vhci;\n\tint err;\n\tunsigned long flags;\n\n\t/*\n\t * @rhport: port number of vhci_hcd\n\t * @sockfd: socket descriptor of an established TCP connection\n\t * @devid: unique device identifier in a remote host\n\t * @speed: usb device speed in a remote host\n\t */\n\tif (sscanf(buf, \"%u %u %u %u\", &port, &sockfd, &devid, &speed) != 4)\n\t\treturn -EINVAL;\n\tpdev_nr = port_to_pdev_nr(port);\n\trhport = port_to_rhport(port);\n\n\tusbip_dbg_vhci_sysfs(\"port(%u) pdev(%d) rhport(%u)\\n\",\n\t\t\t     port, pdev_nr, rhport);\n\tusbip_dbg_vhci_sysfs(\"sockfd(%u) devid(%u) speed(%u)\\n\",\n\t\t\t     sockfd, devid, speed);\n\n\t/* check received parameters */\n\tif (!valid_args(pdev_nr, rhport, speed))\n\t\treturn -EINVAL;\n\n\thcd = platform_get_drvdata(vhcis[pdev_nr].pdev);\n\tif (hcd == NULL) {\n\t\tdev_err(dev, \"port %d is not ready\\n\", port);\n\t\treturn -EAGAIN;\n\t}\n\n\tvhci_hcd = hcd_to_vhci_hcd(hcd);\n\tvhci = vhci_hcd->vhci;\n\n\tif (speed == USB_SPEED_SUPER)\n\t\tvdev = &vhci->vhci_hcd_ss->vdev[rhport];\n\telse\n\t\tvdev = &vhci->vhci_hcd_hs->vdev[rhport];\n\n\t/* Extract socket from fd. */\n\tsocket = sockfd_lookup(sockfd, &err);\n\tif (!socket)\n\t\treturn -EINVAL;\n\n\t/* now need lock until setting vdev status as used */\n\n\t/* begin a lock */\n\tspin_lock_irqsave(&vhci->lock, flags);\n\tspin_lock(&vdev->ud.lock);\n\n\tif (vdev->ud.status != VDEV_ST_NULL) {\n\t\t/* end of the lock */\n\t\tspin_unlock(&vdev->ud.lock);\n\t\tspin_unlock_irqrestore(&vhci->lock, flags);\n\n\t\tsockfd_put(socket);\n\n\t\tdev_err(dev, \"port %d already used\\n\", rhport);\n\t\t/*\n\t\t * Will be retried from userspace\n\t\t * if there's another free port.\n\t\t */\n\t\treturn -EBUSY;\n\t}\n\n\tdev_info(dev, \"pdev(%u) rhport(%u) sockfd(%d)\\n\",\n\t\t pdev_nr, rhport, sockfd);\n\tdev_info(dev, \"devid(%u) speed(%u) speed_str(%s)\\n\",\n\t\t devid, speed, usb_speed_string(speed));\n\n\tvdev->devid         = devid;\n\tvdev->speed         = speed;\n\tvdev->ud.tcp_socket = socket;\n\tvdev->ud.status     = VDEV_ST_NOTASSIGNED;\n\n\tspin_unlock(&vdev->ud.lock);\n\tspin_unlock_irqrestore(&vhci->lock, flags);\n\t/* end the lock */\n\n\tvdev->ud.tcp_rx = kthread_get_run(vhci_rx_loop, &vdev->ud, \"vhci_rx\");\n\tvdev->ud.tcp_tx = kthread_get_run(vhci_tx_loop, &vdev->ud, \"vhci_tx\");\n\n\trh_port_connect(vdev, speed);\n\n\treturn count;\n}",
        "code_after_change": "static ssize_t store_attach(struct device *dev, struct device_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\tstruct socket *socket;\n\tint sockfd = 0;\n\t__u32 port = 0, pdev_nr = 0, rhport = 0, devid = 0, speed = 0;\n\tstruct usb_hcd *hcd;\n\tstruct vhci_hcd *vhci_hcd;\n\tstruct vhci_device *vdev;\n\tstruct vhci *vhci;\n\tint err;\n\tunsigned long flags;\n\n\t/*\n\t * @rhport: port number of vhci_hcd\n\t * @sockfd: socket descriptor of an established TCP connection\n\t * @devid: unique device identifier in a remote host\n\t * @speed: usb device speed in a remote host\n\t */\n\tif (sscanf(buf, \"%u %u %u %u\", &port, &sockfd, &devid, &speed) != 4)\n\t\treturn -EINVAL;\n\tpdev_nr = port_to_pdev_nr(port);\n\trhport = port_to_rhport(port);\n\n\tusbip_dbg_vhci_sysfs(\"port(%u) pdev(%d) rhport(%u)\\n\",\n\t\t\t     port, pdev_nr, rhport);\n\tusbip_dbg_vhci_sysfs(\"sockfd(%u) devid(%u) speed(%u)\\n\",\n\t\t\t     sockfd, devid, speed);\n\n\t/* check received parameters */\n\tif (!valid_args(pdev_nr, rhport, speed))\n\t\treturn -EINVAL;\n\n\thcd = platform_get_drvdata(vhcis[pdev_nr].pdev);\n\tif (hcd == NULL) {\n\t\tdev_err(dev, \"port %d is not ready\\n\", port);\n\t\treturn -EAGAIN;\n\t}\n\n\tvhci_hcd = hcd_to_vhci_hcd(hcd);\n\tvhci = vhci_hcd->vhci;\n\n\tif (speed == USB_SPEED_SUPER)\n\t\tvdev = &vhci->vhci_hcd_ss->vdev[rhport];\n\telse\n\t\tvdev = &vhci->vhci_hcd_hs->vdev[rhport];\n\n\t/* Extract socket from fd. */\n\tsocket = sockfd_lookup(sockfd, &err);\n\tif (!socket)\n\t\treturn -EINVAL;\n\n\t/* now need lock until setting vdev status as used */\n\n\t/* begin a lock */\n\tspin_lock_irqsave(&vhci->lock, flags);\n\tspin_lock(&vdev->ud.lock);\n\n\tif (vdev->ud.status != VDEV_ST_NULL) {\n\t\t/* end of the lock */\n\t\tspin_unlock(&vdev->ud.lock);\n\t\tspin_unlock_irqrestore(&vhci->lock, flags);\n\n\t\tsockfd_put(socket);\n\n\t\tdev_err(dev, \"port %d already used\\n\", rhport);\n\t\t/*\n\t\t * Will be retried from userspace\n\t\t * if there's another free port.\n\t\t */\n\t\treturn -EBUSY;\n\t}\n\n\tdev_info(dev, \"pdev(%u) rhport(%u) sockfd(%d)\\n\",\n\t\t pdev_nr, rhport, sockfd);\n\tdev_info(dev, \"devid(%u) speed(%u) speed_str(%s)\\n\",\n\t\t devid, speed, usb_speed_string(speed));\n\n\tvdev->devid         = devid;\n\tvdev->speed         = speed;\n\tvdev->ud.sockfd     = sockfd;\n\tvdev->ud.tcp_socket = socket;\n\tvdev->ud.status     = VDEV_ST_NOTASSIGNED;\n\n\tspin_unlock(&vdev->ud.lock);\n\tspin_unlock_irqrestore(&vhci->lock, flags);\n\t/* end the lock */\n\n\tvdev->ud.tcp_rx = kthread_get_run(vhci_rx_loop, &vdev->ud, \"vhci_rx\");\n\tvdev->ud.tcp_tx = kthread_get_run(vhci_tx_loop, &vdev->ud, \"vhci_tx\");\n\n\trh_port_connect(vdev, speed);\n\n\treturn count;\n}",
        "patch": "--- code before\n+++ code after\n@@ -78,6 +78,7 @@\n \n \tvdev->devid         = devid;\n \tvdev->speed         = speed;\n+\tvdev->ud.sockfd     = sockfd;\n \tvdev->ud.tcp_socket = socket;\n \tvdev->ud.status     = VDEV_ST_NOTASSIGNED;\n ",
        "function_modified_lines": {
            "added": [
                "\tvdev->ud.sockfd     = sockfd;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The vhci_hcd driver in the Linux Kernel before version 4.14.8 and 4.4.114 allows allows local attackers to disclose kernel memory addresses. Successful exploitation requires that a USB device is attached over IP.",
        "id": 1347
    },
    {
        "cve_id": "CVE-2015-8950",
        "code_before_change": "static void *__dma_alloc_coherent(struct device *dev, size_t size,\n\t\t\t\t  dma_addr_t *dma_handle, gfp_t flags,\n\t\t\t\t  struct dma_attrs *attrs)\n{\n\tif (dev == NULL) {\n\t\tWARN_ONCE(1, \"Use an actual device structure for DMA allocation\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (IS_ENABLED(CONFIG_ZONE_DMA) &&\n\t    dev->coherent_dma_mask <= DMA_BIT_MASK(32))\n\t\tflags |= GFP_DMA;\n\tif (IS_ENABLED(CONFIG_DMA_CMA) && (flags & __GFP_WAIT)) {\n\t\tstruct page *page;\n\t\tvoid *addr;\n\n\t\tsize = PAGE_ALIGN(size);\n\t\tpage = dma_alloc_from_contiguous(dev, size >> PAGE_SHIFT,\n\t\t\t\t\t\t\tget_order(size));\n\t\tif (!page)\n\t\t\treturn NULL;\n\n\t\t*dma_handle = phys_to_dma(dev, page_to_phys(page));\n\t\taddr = page_address(page);\n\t\tif (flags & __GFP_ZERO)\n\t\t\tmemset(addr, 0, size);\n\t\treturn addr;\n\t} else {\n\t\treturn swiotlb_alloc_coherent(dev, size, dma_handle, flags);\n\t}\n}",
        "code_after_change": "static void *__dma_alloc_coherent(struct device *dev, size_t size,\n\t\t\t\t  dma_addr_t *dma_handle, gfp_t flags,\n\t\t\t\t  struct dma_attrs *attrs)\n{\n\tif (dev == NULL) {\n\t\tWARN_ONCE(1, \"Use an actual device structure for DMA allocation\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (IS_ENABLED(CONFIG_ZONE_DMA) &&\n\t    dev->coherent_dma_mask <= DMA_BIT_MASK(32))\n\t\tflags |= GFP_DMA;\n\tif (IS_ENABLED(CONFIG_DMA_CMA) && (flags & __GFP_WAIT)) {\n\t\tstruct page *page;\n\t\tvoid *addr;\n\n\t\tsize = PAGE_ALIGN(size);\n\t\tpage = dma_alloc_from_contiguous(dev, size >> PAGE_SHIFT,\n\t\t\t\t\t\t\tget_order(size));\n\t\tif (!page)\n\t\t\treturn NULL;\n\n\t\t*dma_handle = phys_to_dma(dev, page_to_phys(page));\n\t\taddr = page_address(page);\n\t\tmemset(addr, 0, size);\n\t\treturn addr;\n\t} else {\n\t\treturn swiotlb_alloc_coherent(dev, size, dma_handle, flags);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,8 +22,7 @@\n \n \t\t*dma_handle = phys_to_dma(dev, page_to_phys(page));\n \t\taddr = page_address(page);\n-\t\tif (flags & __GFP_ZERO)\n-\t\t\tmemset(addr, 0, size);\n+\t\tmemset(addr, 0, size);\n \t\treturn addr;\n \t} else {\n \t\treturn swiotlb_alloc_coherent(dev, size, dma_handle, flags);",
        "function_modified_lines": {
            "added": [
                "\t\tmemset(addr, 0, size);"
            ],
            "deleted": [
                "\t\tif (flags & __GFP_ZERO)",
                "\t\t\tmemset(addr, 0, size);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "arch/arm64/mm/dma-mapping.c in the Linux kernel before 4.0.3, as used in the ION subsystem in Android and other products, does not initialize certain data structures, which allows local users to obtain sensitive information from kernel memory by triggering a dma_mmap call.",
        "id": 864
    },
    {
        "cve_id": "CVE-2016-7917",
        "code_before_change": "static void nfnetlink_rcv_batch(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\t\tu_int16_t subsys_id)\n{\n\tstruct sk_buff *oskb = skb;\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct nfnetlink_subsystem *ss;\n\tconst struct nfnl_callback *nc;\n\tstatic LIST_HEAD(err_list);\n\tu32 status;\n\tint err;\n\n\tif (subsys_id >= NFNL_SUBSYS_COUNT)\n\t\treturn netlink_ack(skb, nlh, -EINVAL);\nreplay:\n\tstatus = 0;\n\n\tskb = netlink_skb_clone(oskb, GFP_KERNEL);\n\tif (!skb)\n\t\treturn netlink_ack(oskb, nlh, -ENOMEM);\n\n\tnfnl_lock(subsys_id);\n\tss = nfnl_dereference_protected(subsys_id);\n\tif (!ss) {\n#ifdef CONFIG_MODULES\n\t\tnfnl_unlock(subsys_id);\n\t\trequest_module(\"nfnetlink-subsys-%d\", subsys_id);\n\t\tnfnl_lock(subsys_id);\n\t\tss = nfnl_dereference_protected(subsys_id);\n\t\tif (!ss)\n#endif\n\t\t{\n\t\t\tnfnl_unlock(subsys_id);\n\t\t\tnetlink_ack(oskb, nlh, -EOPNOTSUPP);\n\t\t\treturn kfree_skb(skb);\n\t\t}\n\t}\n\n\tif (!ss->commit || !ss->abort) {\n\t\tnfnl_unlock(subsys_id);\n\t\tnetlink_ack(oskb, nlh, -EOPNOTSUPP);\n\t\treturn kfree_skb(skb);\n\t}\n\n\twhile (skb->len >= nlmsg_total_size(0)) {\n\t\tint msglen, type;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\terr = 0;\n\n\t\tif (nlmsg_len(nlh) < sizeof(struct nfgenmsg) ||\n\t\t    skb->len < nlh->nlmsg_len) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto ack;\n\t\t}\n\n\t\t/* Only requests are handled by the kernel */\n\t\tif (!(nlh->nlmsg_flags & NLM_F_REQUEST)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto ack;\n\t\t}\n\n\t\ttype = nlh->nlmsg_type;\n\t\tif (type == NFNL_MSG_BATCH_BEGIN) {\n\t\t\t/* Malformed: Batch begin twice */\n\t\t\tnfnl_err_reset(&err_list);\n\t\t\tstatus |= NFNL_BATCH_FAILURE;\n\t\t\tgoto done;\n\t\t} else if (type == NFNL_MSG_BATCH_END) {\n\t\t\tstatus |= NFNL_BATCH_DONE;\n\t\t\tgoto done;\n\t\t} else if (type < NLMSG_MIN_TYPE) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto ack;\n\t\t}\n\n\t\t/* We only accept a batch with messages for the same\n\t\t * subsystem.\n\t\t */\n\t\tif (NFNL_SUBSYS_ID(type) != subsys_id) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto ack;\n\t\t}\n\n\t\tnc = nfnetlink_find_client(type, ss);\n\t\tif (!nc) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto ack;\n\t\t}\n\n\t\t{\n\t\t\tint min_len = nlmsg_total_size(sizeof(struct nfgenmsg));\n\t\t\tu_int8_t cb_id = NFNL_MSG_TYPE(nlh->nlmsg_type);\n\t\t\tstruct nlattr *cda[ss->cb[cb_id].attr_count + 1];\n\t\t\tstruct nlattr *attr = (void *)nlh + min_len;\n\t\t\tint attrlen = nlh->nlmsg_len - min_len;\n\n\t\t\terr = nla_parse(cda, ss->cb[cb_id].attr_count,\n\t\t\t\t\tattr, attrlen, ss->cb[cb_id].policy);\n\t\t\tif (err < 0)\n\t\t\t\tgoto ack;\n\n\t\t\tif (nc->call_batch) {\n\t\t\t\terr = nc->call_batch(net, net->nfnl, skb, nlh,\n\t\t\t\t\t\t     (const struct nlattr **)cda);\n\t\t\t}\n\n\t\t\t/* The lock was released to autoload some module, we\n\t\t\t * have to abort and start from scratch using the\n\t\t\t * original skb.\n\t\t\t */\n\t\t\tif (err == -EAGAIN) {\n\t\t\t\tstatus |= NFNL_BATCH_REPLAY;\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t}\nack:\n\t\tif (nlh->nlmsg_flags & NLM_F_ACK || err) {\n\t\t\t/* Errors are delivered once the full batch has been\n\t\t\t * processed, this avoids that the same error is\n\t\t\t * reported several times when replaying the batch.\n\t\t\t */\n\t\t\tif (nfnl_err_add(&err_list, nlh, err) < 0) {\n\t\t\t\t/* We failed to enqueue an error, reset the\n\t\t\t\t * list of errors and send OOM to userspace\n\t\t\t\t * pointing to the batch header.\n\t\t\t\t */\n\t\t\t\tnfnl_err_reset(&err_list);\n\t\t\t\tnetlink_ack(oskb, nlmsg_hdr(oskb), -ENOMEM);\n\t\t\t\tstatus |= NFNL_BATCH_FAILURE;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t\t/* We don't stop processing the batch on errors, thus,\n\t\t\t * userspace gets all the errors that the batch\n\t\t\t * triggers.\n\t\t\t */\n\t\t\tif (err)\n\t\t\t\tstatus |= NFNL_BATCH_FAILURE;\n\t\t}\nnext:\n\t\tmsglen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (msglen > skb->len)\n\t\t\tmsglen = skb->len;\n\t\tskb_pull(skb, msglen);\n\t}\ndone:\n\tif (status & NFNL_BATCH_REPLAY) {\n\t\tss->abort(net, oskb);\n\t\tnfnl_err_reset(&err_list);\n\t\tnfnl_unlock(subsys_id);\n\t\tkfree_skb(skb);\n\t\tgoto replay;\n\t} else if (status == NFNL_BATCH_DONE) {\n\t\tss->commit(net, oskb);\n\t} else {\n\t\tss->abort(net, oskb);\n\t}\n\n\tnfnl_err_deliver(&err_list, oskb);\n\tnfnl_unlock(subsys_id);\n\tkfree_skb(skb);\n}",
        "code_after_change": "static void nfnetlink_rcv_batch(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\t\tu_int16_t subsys_id)\n{\n\tstruct sk_buff *oskb = skb;\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct nfnetlink_subsystem *ss;\n\tconst struct nfnl_callback *nc;\n\tstatic LIST_HEAD(err_list);\n\tu32 status;\n\tint err;\n\n\tif (subsys_id >= NFNL_SUBSYS_COUNT)\n\t\treturn netlink_ack(skb, nlh, -EINVAL);\nreplay:\n\tstatus = 0;\n\n\tskb = netlink_skb_clone(oskb, GFP_KERNEL);\n\tif (!skb)\n\t\treturn netlink_ack(oskb, nlh, -ENOMEM);\n\n\tnfnl_lock(subsys_id);\n\tss = nfnl_dereference_protected(subsys_id);\n\tif (!ss) {\n#ifdef CONFIG_MODULES\n\t\tnfnl_unlock(subsys_id);\n\t\trequest_module(\"nfnetlink-subsys-%d\", subsys_id);\n\t\tnfnl_lock(subsys_id);\n\t\tss = nfnl_dereference_protected(subsys_id);\n\t\tif (!ss)\n#endif\n\t\t{\n\t\t\tnfnl_unlock(subsys_id);\n\t\t\tnetlink_ack(oskb, nlh, -EOPNOTSUPP);\n\t\t\treturn kfree_skb(skb);\n\t\t}\n\t}\n\n\tif (!ss->commit || !ss->abort) {\n\t\tnfnl_unlock(subsys_id);\n\t\tnetlink_ack(oskb, nlh, -EOPNOTSUPP);\n\t\treturn kfree_skb(skb);\n\t}\n\n\twhile (skb->len >= nlmsg_total_size(0)) {\n\t\tint msglen, type;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\terr = 0;\n\n\t\tif (nlh->nlmsg_len < NLMSG_HDRLEN ||\n\t\t    skb->len < nlh->nlmsg_len ||\n\t\t    nlmsg_len(nlh) < sizeof(struct nfgenmsg)) {\n\t\t\tnfnl_err_reset(&err_list);\n\t\t\tstatus |= NFNL_BATCH_FAILURE;\n\t\t\tgoto done;\n\t\t}\n\n\t\t/* Only requests are handled by the kernel */\n\t\tif (!(nlh->nlmsg_flags & NLM_F_REQUEST)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto ack;\n\t\t}\n\n\t\ttype = nlh->nlmsg_type;\n\t\tif (type == NFNL_MSG_BATCH_BEGIN) {\n\t\t\t/* Malformed: Batch begin twice */\n\t\t\tnfnl_err_reset(&err_list);\n\t\t\tstatus |= NFNL_BATCH_FAILURE;\n\t\t\tgoto done;\n\t\t} else if (type == NFNL_MSG_BATCH_END) {\n\t\t\tstatus |= NFNL_BATCH_DONE;\n\t\t\tgoto done;\n\t\t} else if (type < NLMSG_MIN_TYPE) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto ack;\n\t\t}\n\n\t\t/* We only accept a batch with messages for the same\n\t\t * subsystem.\n\t\t */\n\t\tif (NFNL_SUBSYS_ID(type) != subsys_id) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto ack;\n\t\t}\n\n\t\tnc = nfnetlink_find_client(type, ss);\n\t\tif (!nc) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto ack;\n\t\t}\n\n\t\t{\n\t\t\tint min_len = nlmsg_total_size(sizeof(struct nfgenmsg));\n\t\t\tu_int8_t cb_id = NFNL_MSG_TYPE(nlh->nlmsg_type);\n\t\t\tstruct nlattr *cda[ss->cb[cb_id].attr_count + 1];\n\t\t\tstruct nlattr *attr = (void *)nlh + min_len;\n\t\t\tint attrlen = nlh->nlmsg_len - min_len;\n\n\t\t\terr = nla_parse(cda, ss->cb[cb_id].attr_count,\n\t\t\t\t\tattr, attrlen, ss->cb[cb_id].policy);\n\t\t\tif (err < 0)\n\t\t\t\tgoto ack;\n\n\t\t\tif (nc->call_batch) {\n\t\t\t\terr = nc->call_batch(net, net->nfnl, skb, nlh,\n\t\t\t\t\t\t     (const struct nlattr **)cda);\n\t\t\t}\n\n\t\t\t/* The lock was released to autoload some module, we\n\t\t\t * have to abort and start from scratch using the\n\t\t\t * original skb.\n\t\t\t */\n\t\t\tif (err == -EAGAIN) {\n\t\t\t\tstatus |= NFNL_BATCH_REPLAY;\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t}\nack:\n\t\tif (nlh->nlmsg_flags & NLM_F_ACK || err) {\n\t\t\t/* Errors are delivered once the full batch has been\n\t\t\t * processed, this avoids that the same error is\n\t\t\t * reported several times when replaying the batch.\n\t\t\t */\n\t\t\tif (nfnl_err_add(&err_list, nlh, err) < 0) {\n\t\t\t\t/* We failed to enqueue an error, reset the\n\t\t\t\t * list of errors and send OOM to userspace\n\t\t\t\t * pointing to the batch header.\n\t\t\t\t */\n\t\t\t\tnfnl_err_reset(&err_list);\n\t\t\t\tnetlink_ack(oskb, nlmsg_hdr(oskb), -ENOMEM);\n\t\t\t\tstatus |= NFNL_BATCH_FAILURE;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t\t/* We don't stop processing the batch on errors, thus,\n\t\t\t * userspace gets all the errors that the batch\n\t\t\t * triggers.\n\t\t\t */\n\t\t\tif (err)\n\t\t\t\tstatus |= NFNL_BATCH_FAILURE;\n\t\t}\nnext:\n\t\tmsglen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (msglen > skb->len)\n\t\t\tmsglen = skb->len;\n\t\tskb_pull(skb, msglen);\n\t}\ndone:\n\tif (status & NFNL_BATCH_REPLAY) {\n\t\tss->abort(net, oskb);\n\t\tnfnl_err_reset(&err_list);\n\t\tnfnl_unlock(subsys_id);\n\t\tkfree_skb(skb);\n\t\tgoto replay;\n\t} else if (status == NFNL_BATCH_DONE) {\n\t\tss->commit(net, oskb);\n\t} else {\n\t\tss->abort(net, oskb);\n\t}\n\n\tnfnl_err_deliver(&err_list, oskb);\n\tnfnl_unlock(subsys_id);\n\tkfree_skb(skb);\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,10 +47,12 @@\n \t\tnlh = nlmsg_hdr(skb);\n \t\terr = 0;\n \n-\t\tif (nlmsg_len(nlh) < sizeof(struct nfgenmsg) ||\n-\t\t    skb->len < nlh->nlmsg_len) {\n-\t\t\terr = -EINVAL;\n-\t\t\tgoto ack;\n+\t\tif (nlh->nlmsg_len < NLMSG_HDRLEN ||\n+\t\t    skb->len < nlh->nlmsg_len ||\n+\t\t    nlmsg_len(nlh) < sizeof(struct nfgenmsg)) {\n+\t\t\tnfnl_err_reset(&err_list);\n+\t\t\tstatus |= NFNL_BATCH_FAILURE;\n+\t\t\tgoto done;\n \t\t}\n \n \t\t/* Only requests are handled by the kernel */",
        "function_modified_lines": {
            "added": [
                "\t\tif (nlh->nlmsg_len < NLMSG_HDRLEN ||",
                "\t\t    skb->len < nlh->nlmsg_len ||",
                "\t\t    nlmsg_len(nlh) < sizeof(struct nfgenmsg)) {",
                "\t\t\tnfnl_err_reset(&err_list);",
                "\t\t\tstatus |= NFNL_BATCH_FAILURE;",
                "\t\t\tgoto done;"
            ],
            "deleted": [
                "\t\tif (nlmsg_len(nlh) < sizeof(struct nfgenmsg) ||",
                "\t\t    skb->len < nlh->nlmsg_len) {",
                "\t\t\terr = -EINVAL;",
                "\t\t\tgoto ack;"
            ]
        },
        "cwe": [
            "CWE-200",
            "CWE-125"
        ],
        "cve_description": "The nfnetlink_rcv_batch function in net/netfilter/nfnetlink.c in the Linux kernel before 4.5 does not check whether a batch message's length field is large enough, which allows local users to obtain sensitive information from kernel memory or cause a denial of service (infinite loop or out-of-bounds read) by leveraging the CAP_NET_ADMIN capability.",
        "id": 1115
    },
    {
        "cve_id": "CVE-2016-4485",
        "code_before_change": "static void llc_cmsg_rcv(struct msghdr *msg, struct sk_buff *skb)\n{\n\tstruct llc_sock *llc = llc_sk(skb->sk);\n\n\tif (llc->cmsg_flags & LLC_CMSG_PKTINFO) {\n\t\tstruct llc_pktinfo info;\n\n\t\tinfo.lpi_ifindex = llc_sk(skb->sk)->dev->ifindex;\n\t\tllc_pdu_decode_dsap(skb, &info.lpi_sap);\n\t\tllc_pdu_decode_da(skb, info.lpi_mac);\n\t\tput_cmsg(msg, SOL_LLC, LLC_OPT_PKTINFO, sizeof(info), &info);\n\t}\n}",
        "code_after_change": "static void llc_cmsg_rcv(struct msghdr *msg, struct sk_buff *skb)\n{\n\tstruct llc_sock *llc = llc_sk(skb->sk);\n\n\tif (llc->cmsg_flags & LLC_CMSG_PKTINFO) {\n\t\tstruct llc_pktinfo info;\n\n\t\tmemset(&info, 0, sizeof(info));\n\t\tinfo.lpi_ifindex = llc_sk(skb->sk)->dev->ifindex;\n\t\tllc_pdu_decode_dsap(skb, &info.lpi_sap);\n\t\tllc_pdu_decode_da(skb, info.lpi_mac);\n\t\tput_cmsg(msg, SOL_LLC, LLC_OPT_PKTINFO, sizeof(info), &info);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,6 +5,7 @@\n \tif (llc->cmsg_flags & LLC_CMSG_PKTINFO) {\n \t\tstruct llc_pktinfo info;\n \n+\t\tmemset(&info, 0, sizeof(info));\n \t\tinfo.lpi_ifindex = llc_sk(skb->sk)->dev->ifindex;\n \t\tllc_pdu_decode_dsap(skb, &info.lpi_sap);\n \t\tllc_pdu_decode_da(skb, info.lpi_mac);",
        "function_modified_lines": {
            "added": [
                "\t\tmemset(&info, 0, sizeof(info));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The llc_cmsg_rcv function in net/llc/af_llc.c in the Linux kernel before 4.5.5 does not initialize a certain data structure, which allows attackers to obtain sensitive information from kernel stack memory by reading a message.",
        "id": 1019
    },
    {
        "cve_id": "CVE-2018-20449",
        "code_before_change": "static void __init\nplain(void)\n{\n\ttest(PTR1_ZEROES PTR1_STR \" \" PTR2_STR, \"%p %p\", PTR1, PTR2);\n\t/*\n\t * The field width is overloaded for some %p extensions to\n\t * pass another piece of information. For plain pointers, the\n\t * behaviour is slightly odd: One cannot pass either the 0\n\t * flag nor a precision to %p without gcc complaining, and if\n\t * one explicitly gives a field width, the number is no longer\n\t * zero-padded.\n\t */\n\ttest(\"|\" PTR1_STR PTR1_SPACES \"  |  \" PTR1_SPACES PTR1_STR \"|\",\n\t     \"|%-*p|%*p|\", PTR_WIDTH+2, PTR1, PTR_WIDTH+2, PTR1);\n\ttest(\"|\" PTR2_STR \"  |  \" PTR2_STR \"|\",\n\t     \"|%-*p|%*p|\", PTR_WIDTH+2, PTR2, PTR_WIDTH+2, PTR2);\n\n\t/*\n\t * Unrecognized %p extensions are treated as plain %p, but the\n\t * alphanumeric suffix is ignored (that is, does not occur in\n\t * the output.)\n\t */\n\ttest(\"|\"PTR1_ZEROES PTR1_STR\"|\", \"|%p0y|\", PTR1);\n\ttest(\"|\"PTR2_STR\"|\", \"|%p0y|\", PTR2);\n}",
        "code_after_change": "static void __init\nplain(void)\n{\n\tint err;\n\n\terr = plain_hash();\n\tif (err) {\n\t\tpr_warn(\"plain 'p' does not appear to be hashed\\n\");\n\t\tfailed_tests++;\n\t\treturn;\n\t}\n\n\terr = plain_format();\n\tif (err) {\n\t\tpr_warn(\"hashing plain 'p' has unexpected format\\n\");\n\t\tfailed_tests++;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,25 +1,18 @@\n static void __init\n plain(void)\n {\n-\ttest(PTR1_ZEROES PTR1_STR \" \" PTR2_STR, \"%p %p\", PTR1, PTR2);\n-\t/*\n-\t * The field width is overloaded for some %p extensions to\n-\t * pass another piece of information. For plain pointers, the\n-\t * behaviour is slightly odd: One cannot pass either the 0\n-\t * flag nor a precision to %p without gcc complaining, and if\n-\t * one explicitly gives a field width, the number is no longer\n-\t * zero-padded.\n-\t */\n-\ttest(\"|\" PTR1_STR PTR1_SPACES \"  |  \" PTR1_SPACES PTR1_STR \"|\",\n-\t     \"|%-*p|%*p|\", PTR_WIDTH+2, PTR1, PTR_WIDTH+2, PTR1);\n-\ttest(\"|\" PTR2_STR \"  |  \" PTR2_STR \"|\",\n-\t     \"|%-*p|%*p|\", PTR_WIDTH+2, PTR2, PTR_WIDTH+2, PTR2);\n+\tint err;\n \n-\t/*\n-\t * Unrecognized %p extensions are treated as plain %p, but the\n-\t * alphanumeric suffix is ignored (that is, does not occur in\n-\t * the output.)\n-\t */\n-\ttest(\"|\"PTR1_ZEROES PTR1_STR\"|\", \"|%p0y|\", PTR1);\n-\ttest(\"|\"PTR2_STR\"|\", \"|%p0y|\", PTR2);\n+\terr = plain_hash();\n+\tif (err) {\n+\t\tpr_warn(\"plain 'p' does not appear to be hashed\\n\");\n+\t\tfailed_tests++;\n+\t\treturn;\n+\t}\n+\n+\terr = plain_format();\n+\tif (err) {\n+\t\tpr_warn(\"hashing plain 'p' has unexpected format\\n\");\n+\t\tfailed_tests++;\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\tint err;",
                "\terr = plain_hash();",
                "\tif (err) {",
                "\t\tpr_warn(\"plain 'p' does not appear to be hashed\\n\");",
                "\t\tfailed_tests++;",
                "\t\treturn;",
                "\t}",
                "",
                "\terr = plain_format();",
                "\tif (err) {",
                "\t\tpr_warn(\"hashing plain 'p' has unexpected format\\n\");",
                "\t\tfailed_tests++;",
                "\t}"
            ],
            "deleted": [
                "\ttest(PTR1_ZEROES PTR1_STR \" \" PTR2_STR, \"%p %p\", PTR1, PTR2);",
                "\t/*",
                "\t * The field width is overloaded for some %p extensions to",
                "\t * pass another piece of information. For plain pointers, the",
                "\t * behaviour is slightly odd: One cannot pass either the 0",
                "\t * flag nor a precision to %p without gcc complaining, and if",
                "\t * one explicitly gives a field width, the number is no longer",
                "\t * zero-padded.",
                "\t */",
                "\ttest(\"|\" PTR1_STR PTR1_SPACES \"  |  \" PTR1_SPACES PTR1_STR \"|\",",
                "\t     \"|%-*p|%*p|\", PTR_WIDTH+2, PTR1, PTR_WIDTH+2, PTR1);",
                "\ttest(\"|\" PTR2_STR \"  |  \" PTR2_STR \"|\",",
                "\t     \"|%-*p|%*p|\", PTR_WIDTH+2, PTR2, PTR_WIDTH+2, PTR2);",
                "\t/*",
                "\t * Unrecognized %p extensions are treated as plain %p, but the",
                "\t * alphanumeric suffix is ignored (that is, does not occur in",
                "\t * the output.)",
                "\t */",
                "\ttest(\"|\"PTR1_ZEROES PTR1_STR\"|\", \"|%p0y|\", PTR1);",
                "\ttest(\"|\"PTR2_STR\"|\", \"|%p0y|\", PTR2);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The hidma_chan_stats function in drivers/dma/qcom/hidma_dbg.c in the Linux kernel 4.14.90 allows local users to obtain sensitive address information by reading \"callback=\" lines in a debugfs file.",
        "id": 1756
    },
    {
        "cve_id": "CVE-2014-4027",
        "code_before_change": "static void rd_release_device_space(struct rd_dev *rd_dev)\n{\n\tu32 i, j, page_count = 0, sg_per_table;\n\tstruct rd_dev_sg_table *sg_table;\n\tstruct page *pg;\n\tstruct scatterlist *sg;\n\n\tif (!rd_dev->sg_table_array || !rd_dev->sg_table_count)\n\t\treturn;\n\n\tsg_table = rd_dev->sg_table_array;\n\n\tfor (i = 0; i < rd_dev->sg_table_count; i++) {\n\t\tsg = sg_table[i].sg_table;\n\t\tsg_per_table = sg_table[i].rd_sg_count;\n\n\t\tfor (j = 0; j < sg_per_table; j++) {\n\t\t\tpg = sg_page(&sg[j]);\n\t\t\tif (pg) {\n\t\t\t\t__free_page(pg);\n\t\t\t\tpage_count++;\n\t\t\t}\n\t\t}\n\n\t\tkfree(sg);\n\t}\n\n\tpr_debug(\"CORE_RD[%u] - Released device space for Ramdisk\"\n\t\t\" Device ID: %u, pages %u in %u tables total bytes %lu\\n\",\n\t\trd_dev->rd_host->rd_host_id, rd_dev->rd_dev_id, page_count,\n\t\trd_dev->sg_table_count, (unsigned long)page_count * PAGE_SIZE);\n\n\tkfree(sg_table);\n\trd_dev->sg_table_array = NULL;\n\trd_dev->sg_table_count = 0;\n}",
        "code_after_change": "static void rd_release_device_space(struct rd_dev *rd_dev)\n{\n\tu32 page_count;\n\n\tif (!rd_dev->sg_table_array || !rd_dev->sg_table_count)\n\t\treturn;\n\n\tpage_count = rd_release_sgl_table(rd_dev, rd_dev->sg_table_array,\n\t\t\t\t\t  rd_dev->sg_table_count);\n\n\tpr_debug(\"CORE_RD[%u] - Released device space for Ramdisk\"\n\t\t\" Device ID: %u, pages %u in %u tables total bytes %lu\\n\",\n\t\trd_dev->rd_host->rd_host_id, rd_dev->rd_dev_id, page_count,\n\t\trd_dev->sg_table_count, (unsigned long)page_count * PAGE_SIZE);\n\n\trd_dev->sg_table_array = NULL;\n\trd_dev->sg_table_count = 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,36 +1,18 @@\n static void rd_release_device_space(struct rd_dev *rd_dev)\n {\n-\tu32 i, j, page_count = 0, sg_per_table;\n-\tstruct rd_dev_sg_table *sg_table;\n-\tstruct page *pg;\n-\tstruct scatterlist *sg;\n+\tu32 page_count;\n \n \tif (!rd_dev->sg_table_array || !rd_dev->sg_table_count)\n \t\treturn;\n \n-\tsg_table = rd_dev->sg_table_array;\n-\n-\tfor (i = 0; i < rd_dev->sg_table_count; i++) {\n-\t\tsg = sg_table[i].sg_table;\n-\t\tsg_per_table = sg_table[i].rd_sg_count;\n-\n-\t\tfor (j = 0; j < sg_per_table; j++) {\n-\t\t\tpg = sg_page(&sg[j]);\n-\t\t\tif (pg) {\n-\t\t\t\t__free_page(pg);\n-\t\t\t\tpage_count++;\n-\t\t\t}\n-\t\t}\n-\n-\t\tkfree(sg);\n-\t}\n+\tpage_count = rd_release_sgl_table(rd_dev, rd_dev->sg_table_array,\n+\t\t\t\t\t  rd_dev->sg_table_count);\n \n \tpr_debug(\"CORE_RD[%u] - Released device space for Ramdisk\"\n \t\t\" Device ID: %u, pages %u in %u tables total bytes %lu\\n\",\n \t\trd_dev->rd_host->rd_host_id, rd_dev->rd_dev_id, page_count,\n \t\trd_dev->sg_table_count, (unsigned long)page_count * PAGE_SIZE);\n \n-\tkfree(sg_table);\n \trd_dev->sg_table_array = NULL;\n \trd_dev->sg_table_count = 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tu32 page_count;",
                "\tpage_count = rd_release_sgl_table(rd_dev, rd_dev->sg_table_array,",
                "\t\t\t\t\t  rd_dev->sg_table_count);"
            ],
            "deleted": [
                "\tu32 i, j, page_count = 0, sg_per_table;",
                "\tstruct rd_dev_sg_table *sg_table;",
                "\tstruct page *pg;",
                "\tstruct scatterlist *sg;",
                "\tsg_table = rd_dev->sg_table_array;",
                "",
                "\tfor (i = 0; i < rd_dev->sg_table_count; i++) {",
                "\t\tsg = sg_table[i].sg_table;",
                "\t\tsg_per_table = sg_table[i].rd_sg_count;",
                "",
                "\t\tfor (j = 0; j < sg_per_table; j++) {",
                "\t\t\tpg = sg_page(&sg[j]);",
                "\t\t\tif (pg) {",
                "\t\t\t\t__free_page(pg);",
                "\t\t\t\tpage_count++;",
                "\t\t\t}",
                "\t\t}",
                "",
                "\t\tkfree(sg);",
                "\t}",
                "\tkfree(sg_table);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The rd_build_device_space function in drivers/target/target_core_rd.c in the Linux kernel before 3.14 does not properly initialize a certain data structure, which allows local users to obtain sensitive information from ramdisk_mcp memory by leveraging access to a SCSI initiator.",
        "id": 556
    },
    {
        "cve_id": "CVE-2014-9895",
        "code_before_change": "static long __media_device_enum_links(struct media_device *mdev,\n\t\t\t\t      struct media_links_enum *links)\n{\n\tstruct media_entity *entity;\n\n\tentity = find_entity(mdev, links->entity);\n\tif (entity == NULL)\n\t\treturn -EINVAL;\n\n\tif (links->pads) {\n\t\tunsigned int p;\n\n\t\tfor (p = 0; p < entity->num_pads; p++) {\n\t\t\tstruct media_pad_desc pad;\n\t\t\tmedia_device_kpad_to_upad(&entity->pads[p], &pad);\n\t\t\tif (copy_to_user(&links->pads[p], &pad, sizeof(pad)))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (links->links) {\n\t\tstruct media_link_desc __user *ulink;\n\t\tunsigned int l;\n\n\t\tfor (l = 0, ulink = links->links; l < entity->num_links; l++) {\n\t\t\tstruct media_link_desc link;\n\n\t\t\t/* Ignore backlinks. */\n\t\t\tif (entity->links[l].source->entity != entity)\n\t\t\t\tcontinue;\n\n\t\t\tmedia_device_kpad_to_upad(entity->links[l].source,\n\t\t\t\t\t\t  &link.source);\n\t\t\tmedia_device_kpad_to_upad(entity->links[l].sink,\n\t\t\t\t\t\t  &link.sink);\n\t\t\tlink.flags = entity->links[l].flags;\n\t\t\tif (copy_to_user(ulink, &link, sizeof(*ulink)))\n\t\t\t\treturn -EFAULT;\n\t\t\tulink++;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static long __media_device_enum_links(struct media_device *mdev,\n\t\t\t\t      struct media_links_enum *links)\n{\n\tstruct media_entity *entity;\n\n\tentity = find_entity(mdev, links->entity);\n\tif (entity == NULL)\n\t\treturn -EINVAL;\n\n\tif (links->pads) {\n\t\tunsigned int p;\n\n\t\tfor (p = 0; p < entity->num_pads; p++) {\n\t\t\tstruct media_pad_desc pad;\n\n\t\t\tmemset(&pad, 0, sizeof(pad));\n\t\t\tmedia_device_kpad_to_upad(&entity->pads[p], &pad);\n\t\t\tif (copy_to_user(&links->pads[p], &pad, sizeof(pad)))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (links->links) {\n\t\tstruct media_link_desc __user *ulink;\n\t\tunsigned int l;\n\n\t\tfor (l = 0, ulink = links->links; l < entity->num_links; l++) {\n\t\t\tstruct media_link_desc link;\n\n\t\t\t/* Ignore backlinks. */\n\t\t\tif (entity->links[l].source->entity != entity)\n\t\t\t\tcontinue;\n\n\t\t\tmemset(&link, 0, sizeof(link));\n\t\t\tmedia_device_kpad_to_upad(entity->links[l].source,\n\t\t\t\t\t\t  &link.source);\n\t\t\tmedia_device_kpad_to_upad(entity->links[l].sink,\n\t\t\t\t\t\t  &link.sink);\n\t\t\tlink.flags = entity->links[l].flags;\n\t\t\tif (copy_to_user(ulink, &link, sizeof(*ulink)))\n\t\t\t\treturn -EFAULT;\n\t\t\tulink++;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,8 @@\n \n \t\tfor (p = 0; p < entity->num_pads; p++) {\n \t\t\tstruct media_pad_desc pad;\n+\n+\t\t\tmemset(&pad, 0, sizeof(pad));\n \t\t\tmedia_device_kpad_to_upad(&entity->pads[p], &pad);\n \t\t\tif (copy_to_user(&links->pads[p], &pad, sizeof(pad)))\n \t\t\t\treturn -EFAULT;\n@@ -29,6 +31,7 @@\n \t\t\tif (entity->links[l].source->entity != entity)\n \t\t\t\tcontinue;\n \n+\t\t\tmemset(&link, 0, sizeof(link));\n \t\t\tmedia_device_kpad_to_upad(entity->links[l].source,\n \t\t\t\t\t\t  &link.source);\n \t\t\tmedia_device_kpad_to_upad(entity->links[l].sink,",
        "function_modified_lines": {
            "added": [
                "",
                "\t\t\tmemset(&pad, 0, sizeof(pad));",
                "\t\t\tmemset(&link, 0, sizeof(link));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "drivers/media/media-device.c in the Linux kernel before 3.11, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not properly initialize certain data structures, which allows local users to obtain sensitive information via a crafted application, aka Android internal bug 28750150 and Qualcomm internal bug CR570757, a different vulnerability than CVE-2014-1739.",
        "id": 708
    },
    {
        "cve_id": "CVE-2013-3237",
        "code_before_change": "static int\nvsock_stream_recvmsg(struct kiocb *kiocb,\n\t\t     struct socket *sock,\n\t\t     struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tint err;\n\tsize_t target;\n\tssize_t copied;\n\tlong timeout;\n\tstruct vsock_transport_recv_notify_data recv_data;\n\n\tDEFINE_WAIT(wait);\n\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\terr = 0;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != SS_CONNECTED) {\n\t\t/* Recvmsg is supposed to return 0 if a peer performs an\n\t\t * orderly shutdown. Differentiate between that case and when a\n\t\t * peer has not connected or a local shutdown occured with the\n\t\t * SOCK_DONE flag.\n\t\t */\n\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\terr = 0;\n\t\telse\n\t\t\terr = -ENOTCONN;\n\n\t\tgoto out;\n\t}\n\n\tif (flags & MSG_OOB) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\t/* We don't check peer_shutdown flag here since peer may actually shut\n\t * down, but there can be data in the queue that a local socket can\n\t * receive.\n\t */\n\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* It is valid on Linux to pass in a zero-length receive buffer.  This\n\t * is not an error.  We may as well bail out now.\n\t */\n\tif (!len) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* We must not copy less than target bytes into the user's buffer\n\t * before returning successfully, so we wait for the consume queue to\n\t * have that much data to consume before dequeueing.  Note that this\n\t * makes it impossible to handle cases where target is greater than the\n\t * queue size.\n\t */\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tif (target >= transport->stream_rcvhiwat(vsk)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\tcopied = 0;\n\n\terr = transport->notify_recv_init(vsk, target, &recv_data);\n\tif (err < 0)\n\t\tgoto out;\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\twhile (1) {\n\t\ts64 ready = vsock_stream_has_data(vsk);\n\n\t\tif (ready < 0) {\n\t\t\t/* Invalid queue pair content. XXX This should be\n\t\t\t * changed to a connection reset in a later change.\n\t\t\t */\n\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_wait;\n\t\t} else if (ready > 0) {\n\t\t\tssize_t read;\n\n\t\t\terr = transport->notify_recv_pre_dequeue(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\n\t\t\tread = transport->stream_dequeue(\n\t\t\t\t\tvsk, msg->msg_iov,\n\t\t\t\t\tlen - copied, flags);\n\t\t\tif (read < 0) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcopied += read;\n\n\t\t\terr = transport->notify_recv_post_dequeue(\n\t\t\t\t\tvsk, target, read,\n\t\t\t\t\t!(flags & MSG_PEEK), &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_wait;\n\n\t\t\tif (read >= target || flags & MSG_PEEK)\n\t\t\t\tbreak;\n\n\t\t\ttarget -= read;\n\t\t} else {\n\t\t\tif (sk->sk_err != 0 || (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t    || (vsk->peer_shutdown & SEND_SHUTDOWN)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* Don't wait for non-blocking sockets. */\n\t\t\tif (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = transport->notify_recv_pre_block(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\n\t\t\trelease_sock(sk);\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tlock_sock(sk);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeout);\n\t\t\t\tbreak;\n\t\t\t} else if (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tprepare_to_wait(sk_sleep(sk), &wait,\n\t\t\t\t\tTASK_INTERRUPTIBLE);\n\t\t}\n\t}\n\n\tif (sk->sk_err)\n\t\terr = -sk->sk_err;\n\telse if (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\terr = 0;\n\n\tif (copied > 0) {\n\t\t/* We only do these additional bookkeeping/notification steps\n\t\t * if we actually copied something out of the queue pair\n\t\t * instead of just peeking ahead.\n\t\t */\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\t/* If the other side has shutdown for sending and there\n\t\t\t * is nothing more to read, then modify the socket\n\t\t\t * state.\n\t\t\t */\n\t\t\tif (vsk->peer_shutdown & SEND_SHUTDOWN) {\n\t\t\t\tif (vsock_stream_has_data(vsk) <= 0) {\n\t\t\t\t\tsk->sk_state = SS_UNCONNECTED;\n\t\t\t\t\tsock_set_flag(sk, SOCK_DONE);\n\t\t\t\t\tsk->sk_state_change(sk);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr = copied;\n\t}\n\nout_wait:\n\tfinish_wait(sk_sleep(sk), &wait);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "code_after_change": "static int\nvsock_stream_recvmsg(struct kiocb *kiocb,\n\t\t     struct socket *sock,\n\t\t     struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tint err;\n\tsize_t target;\n\tssize_t copied;\n\tlong timeout;\n\tstruct vsock_transport_recv_notify_data recv_data;\n\n\tDEFINE_WAIT(wait);\n\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\terr = 0;\n\n\tmsg->msg_namelen = 0;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_state != SS_CONNECTED) {\n\t\t/* Recvmsg is supposed to return 0 if a peer performs an\n\t\t * orderly shutdown. Differentiate between that case and when a\n\t\t * peer has not connected or a local shutdown occured with the\n\t\t * SOCK_DONE flag.\n\t\t */\n\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\terr = 0;\n\t\telse\n\t\t\terr = -ENOTCONN;\n\n\t\tgoto out;\n\t}\n\n\tif (flags & MSG_OOB) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\t/* We don't check peer_shutdown flag here since peer may actually shut\n\t * down, but there can be data in the queue that a local socket can\n\t * receive.\n\t */\n\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* It is valid on Linux to pass in a zero-length receive buffer.  This\n\t * is not an error.  We may as well bail out now.\n\t */\n\tif (!len) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* We must not copy less than target bytes into the user's buffer\n\t * before returning successfully, so we wait for the consume queue to\n\t * have that much data to consume before dequeueing.  Note that this\n\t * makes it impossible to handle cases where target is greater than the\n\t * queue size.\n\t */\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tif (target >= transport->stream_rcvhiwat(vsk)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\tcopied = 0;\n\n\terr = transport->notify_recv_init(vsk, target, &recv_data);\n\tif (err < 0)\n\t\tgoto out;\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\twhile (1) {\n\t\ts64 ready = vsock_stream_has_data(vsk);\n\n\t\tif (ready < 0) {\n\t\t\t/* Invalid queue pair content. XXX This should be\n\t\t\t * changed to a connection reset in a later change.\n\t\t\t */\n\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_wait;\n\t\t} else if (ready > 0) {\n\t\t\tssize_t read;\n\n\t\t\terr = transport->notify_recv_pre_dequeue(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\n\t\t\tread = transport->stream_dequeue(\n\t\t\t\t\tvsk, msg->msg_iov,\n\t\t\t\t\tlen - copied, flags);\n\t\t\tif (read < 0) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcopied += read;\n\n\t\t\terr = transport->notify_recv_post_dequeue(\n\t\t\t\t\tvsk, target, read,\n\t\t\t\t\t!(flags & MSG_PEEK), &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_wait;\n\n\t\t\tif (read >= target || flags & MSG_PEEK)\n\t\t\t\tbreak;\n\n\t\t\ttarget -= read;\n\t\t} else {\n\t\t\tif (sk->sk_err != 0 || (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t    || (vsk->peer_shutdown & SEND_SHUTDOWN)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* Don't wait for non-blocking sockets. */\n\t\t\tif (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = transport->notify_recv_pre_block(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\n\t\t\trelease_sock(sk);\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tlock_sock(sk);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeout);\n\t\t\t\tbreak;\n\t\t\t} else if (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tprepare_to_wait(sk_sleep(sk), &wait,\n\t\t\t\t\tTASK_INTERRUPTIBLE);\n\t\t}\n\t}\n\n\tif (sk->sk_err)\n\t\terr = -sk->sk_err;\n\telse if (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\terr = 0;\n\n\tif (copied > 0) {\n\t\t/* We only do these additional bookkeeping/notification steps\n\t\t * if we actually copied something out of the queue pair\n\t\t * instead of just peeking ahead.\n\t\t */\n\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\t/* If the other side has shutdown for sending and there\n\t\t\t * is nothing more to read, then modify the socket\n\t\t\t * state.\n\t\t\t */\n\t\t\tif (vsk->peer_shutdown & SEND_SHUTDOWN) {\n\t\t\t\tif (vsock_stream_has_data(vsk) <= 0) {\n\t\t\t\t\tsk->sk_state = SS_UNCONNECTED;\n\t\t\t\t\tsock_set_flag(sk, SOCK_DONE);\n\t\t\t\t\tsk->sk_state_change(sk);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr = copied;\n\t}\n\nout_wait:\n\tfinish_wait(sk_sleep(sk), &wait);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,8 @@\n \tsk = sock->sk;\n \tvsk = vsock_sk(sk);\n \terr = 0;\n+\n+\tmsg->msg_namelen = 0;\n \n \tlock_sock(sk);\n ",
        "function_modified_lines": {
            "added": [
                "",
                "\tmsg->msg_namelen = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The vsock_stream_sendmsg function in net/vmw_vsock/af_vsock.c in the Linux kernel before 3.9-rc7 does not initialize a certain length variable, which allows local users to obtain sensitive information from kernel stack memory via a crafted recvmsg or recvfrom system call.",
        "id": 276
    },
    {
        "cve_id": "CVE-2016-4569",
        "code_before_change": "static int snd_timer_user_params(struct file *file,\n\t\t\t\t struct snd_timer_params __user *_params)\n{\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_params params;\n\tstruct snd_timer *t;\n\tstruct snd_timer_read *tr;\n\tstruct snd_timer_tread *ttr;\n\tint err;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\tt = tu->timeri->timer;\n\tif (!t)\n\t\treturn -EBADFD;\n\tif (copy_from_user(&params, _params, sizeof(params)))\n\t\treturn -EFAULT;\n\tif (!(t->hw.flags & SNDRV_TIMER_HW_SLAVE) && params.ticks < 1) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tif (params.queue_size > 0 &&\n\t    (params.queue_size < 32 || params.queue_size > 1024)) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tif (params.filter & ~((1<<SNDRV_TIMER_EVENT_RESOLUTION)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_TICK)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_START)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_STOP)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_CONTINUE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_PAUSE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_SUSPEND)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_RESUME)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSTART)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSTOP)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MCONTINUE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MPAUSE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSUSPEND)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MRESUME))) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tsnd_timer_stop(tu->timeri);\n\tspin_lock_irq(&t->lock);\n\ttu->timeri->flags &= ~(SNDRV_TIMER_IFLG_AUTO|\n\t\t\t       SNDRV_TIMER_IFLG_EXCLUSIVE|\n\t\t\t       SNDRV_TIMER_IFLG_EARLY_EVENT);\n\tif (params.flags & SNDRV_TIMER_PSFLG_AUTO)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_AUTO;\n\tif (params.flags & SNDRV_TIMER_PSFLG_EXCLUSIVE)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_EXCLUSIVE;\n\tif (params.flags & SNDRV_TIMER_PSFLG_EARLY_EVENT)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_EARLY_EVENT;\n\tspin_unlock_irq(&t->lock);\n\tif (params.queue_size > 0 &&\n\t    (unsigned int)tu->queue_size != params.queue_size) {\n\t\tif (tu->tread) {\n\t\t\tttr = kmalloc(params.queue_size * sizeof(*ttr),\n\t\t\t\t      GFP_KERNEL);\n\t\t\tif (ttr) {\n\t\t\t\tkfree(tu->tqueue);\n\t\t\t\ttu->queue_size = params.queue_size;\n\t\t\t\ttu->tqueue = ttr;\n\t\t\t}\n\t\t} else {\n\t\t\ttr = kmalloc(params.queue_size * sizeof(*tr),\n\t\t\t\t     GFP_KERNEL);\n\t\t\tif (tr) {\n\t\t\t\tkfree(tu->queue);\n\t\t\t\ttu->queue_size = params.queue_size;\n\t\t\t\ttu->queue = tr;\n\t\t\t}\n\t\t}\n\t}\n\ttu->qhead = tu->qtail = tu->qused = 0;\n\tif (tu->timeri->flags & SNDRV_TIMER_IFLG_EARLY_EVENT) {\n\t\tif (tu->tread) {\n\t\t\tstruct snd_timer_tread tread;\n\t\t\ttread.event = SNDRV_TIMER_EVENT_EARLY;\n\t\t\ttread.tstamp.tv_sec = 0;\n\t\t\ttread.tstamp.tv_nsec = 0;\n\t\t\ttread.val = 0;\n\t\t\tsnd_timer_user_append_to_tqueue(tu, &tread);\n\t\t} else {\n\t\t\tstruct snd_timer_read *r = &tu->queue[0];\n\t\t\tr->resolution = 0;\n\t\t\tr->ticks = 0;\n\t\t\ttu->qused++;\n\t\t\ttu->qtail++;\n\t\t}\n\t}\n\ttu->filter = params.filter;\n\ttu->ticks = params.ticks;\n\terr = 0;\n _end:\n\tif (copy_to_user(_params, &params, sizeof(params)))\n\t\treturn -EFAULT;\n\treturn err;\n}",
        "code_after_change": "static int snd_timer_user_params(struct file *file,\n\t\t\t\t struct snd_timer_params __user *_params)\n{\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_params params;\n\tstruct snd_timer *t;\n\tstruct snd_timer_read *tr;\n\tstruct snd_timer_tread *ttr;\n\tint err;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\tt = tu->timeri->timer;\n\tif (!t)\n\t\treturn -EBADFD;\n\tif (copy_from_user(&params, _params, sizeof(params)))\n\t\treturn -EFAULT;\n\tif (!(t->hw.flags & SNDRV_TIMER_HW_SLAVE) && params.ticks < 1) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tif (params.queue_size > 0 &&\n\t    (params.queue_size < 32 || params.queue_size > 1024)) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tif (params.filter & ~((1<<SNDRV_TIMER_EVENT_RESOLUTION)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_TICK)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_START)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_STOP)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_CONTINUE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_PAUSE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_SUSPEND)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_RESUME)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSTART)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSTOP)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MCONTINUE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MPAUSE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSUSPEND)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MRESUME))) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tsnd_timer_stop(tu->timeri);\n\tspin_lock_irq(&t->lock);\n\ttu->timeri->flags &= ~(SNDRV_TIMER_IFLG_AUTO|\n\t\t\t       SNDRV_TIMER_IFLG_EXCLUSIVE|\n\t\t\t       SNDRV_TIMER_IFLG_EARLY_EVENT);\n\tif (params.flags & SNDRV_TIMER_PSFLG_AUTO)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_AUTO;\n\tif (params.flags & SNDRV_TIMER_PSFLG_EXCLUSIVE)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_EXCLUSIVE;\n\tif (params.flags & SNDRV_TIMER_PSFLG_EARLY_EVENT)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_EARLY_EVENT;\n\tspin_unlock_irq(&t->lock);\n\tif (params.queue_size > 0 &&\n\t    (unsigned int)tu->queue_size != params.queue_size) {\n\t\tif (tu->tread) {\n\t\t\tttr = kmalloc(params.queue_size * sizeof(*ttr),\n\t\t\t\t      GFP_KERNEL);\n\t\t\tif (ttr) {\n\t\t\t\tkfree(tu->tqueue);\n\t\t\t\ttu->queue_size = params.queue_size;\n\t\t\t\ttu->tqueue = ttr;\n\t\t\t}\n\t\t} else {\n\t\t\ttr = kmalloc(params.queue_size * sizeof(*tr),\n\t\t\t\t     GFP_KERNEL);\n\t\t\tif (tr) {\n\t\t\t\tkfree(tu->queue);\n\t\t\t\ttu->queue_size = params.queue_size;\n\t\t\t\ttu->queue = tr;\n\t\t\t}\n\t\t}\n\t}\n\ttu->qhead = tu->qtail = tu->qused = 0;\n\tif (tu->timeri->flags & SNDRV_TIMER_IFLG_EARLY_EVENT) {\n\t\tif (tu->tread) {\n\t\t\tstruct snd_timer_tread tread;\n\t\t\tmemset(&tread, 0, sizeof(tread));\n\t\t\ttread.event = SNDRV_TIMER_EVENT_EARLY;\n\t\t\ttread.tstamp.tv_sec = 0;\n\t\t\ttread.tstamp.tv_nsec = 0;\n\t\t\ttread.val = 0;\n\t\t\tsnd_timer_user_append_to_tqueue(tu, &tread);\n\t\t} else {\n\t\t\tstruct snd_timer_read *r = &tu->queue[0];\n\t\t\tr->resolution = 0;\n\t\t\tr->ticks = 0;\n\t\t\ttu->qused++;\n\t\t\ttu->qtail++;\n\t\t}\n\t}\n\ttu->filter = params.filter;\n\ttu->ticks = params.ticks;\n\terr = 0;\n _end:\n\tif (copy_to_user(_params, &params, sizeof(params)))\n\t\treturn -EFAULT;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -78,6 +78,7 @@\n \tif (tu->timeri->flags & SNDRV_TIMER_IFLG_EARLY_EVENT) {\n \t\tif (tu->tread) {\n \t\t\tstruct snd_timer_tread tread;\n+\t\t\tmemset(&tread, 0, sizeof(tread));\n \t\t\ttread.event = SNDRV_TIMER_EVENT_EARLY;\n \t\t\ttread.tstamp.tv_sec = 0;\n \t\t\ttread.tstamp.tv_nsec = 0;",
        "function_modified_lines": {
            "added": [
                "\t\t\tmemset(&tread, 0, sizeof(tread));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The snd_timer_user_params function in sound/core/timer.c in the Linux kernel through 4.6 does not initialize a certain data structure, which allows local users to obtain sensitive information from kernel stack memory via crafted use of the ALSA timer interface.",
        "id": 1027
    },
    {
        "cve_id": "CVE-2012-6537",
        "code_before_change": "static int copy_to_user_tmpl(struct xfrm_policy *xp, struct sk_buff *skb)\n{\n\tstruct xfrm_user_tmpl vec[XFRM_MAX_DEPTH];\n\tint i;\n\n\tif (xp->xfrm_nr == 0)\n\t\treturn 0;\n\n\tfor (i = 0; i < xp->xfrm_nr; i++) {\n\t\tstruct xfrm_user_tmpl *up = &vec[i];\n\t\tstruct xfrm_tmpl *kp = &xp->xfrm_vec[i];\n\n\t\tmemcpy(&up->id, &kp->id, sizeof(up->id));\n\t\tup->family = kp->encap_family;\n\t\tmemcpy(&up->saddr, &kp->saddr, sizeof(up->saddr));\n\t\tup->reqid = kp->reqid;\n\t\tup->mode = kp->mode;\n\t\tup->share = kp->share;\n\t\tup->optional = kp->optional;\n\t\tup->aalgos = kp->aalgos;\n\t\tup->ealgos = kp->ealgos;\n\t\tup->calgos = kp->calgos;\n\t}\n\n\treturn nla_put(skb, XFRMA_TMPL,\n\t\t       sizeof(struct xfrm_user_tmpl) * xp->xfrm_nr, vec);\n}",
        "code_after_change": "static int copy_to_user_tmpl(struct xfrm_policy *xp, struct sk_buff *skb)\n{\n\tstruct xfrm_user_tmpl vec[XFRM_MAX_DEPTH];\n\tint i;\n\n\tif (xp->xfrm_nr == 0)\n\t\treturn 0;\n\n\tfor (i = 0; i < xp->xfrm_nr; i++) {\n\t\tstruct xfrm_user_tmpl *up = &vec[i];\n\t\tstruct xfrm_tmpl *kp = &xp->xfrm_vec[i];\n\n\t\tmemset(up, 0, sizeof(*up));\n\t\tmemcpy(&up->id, &kp->id, sizeof(up->id));\n\t\tup->family = kp->encap_family;\n\t\tmemcpy(&up->saddr, &kp->saddr, sizeof(up->saddr));\n\t\tup->reqid = kp->reqid;\n\t\tup->mode = kp->mode;\n\t\tup->share = kp->share;\n\t\tup->optional = kp->optional;\n\t\tup->aalgos = kp->aalgos;\n\t\tup->ealgos = kp->ealgos;\n\t\tup->calgos = kp->calgos;\n\t}\n\n\treturn nla_put(skb, XFRMA_TMPL,\n\t\t       sizeof(struct xfrm_user_tmpl) * xp->xfrm_nr, vec);\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,7 @@\n \t\tstruct xfrm_user_tmpl *up = &vec[i];\n \t\tstruct xfrm_tmpl *kp = &xp->xfrm_vec[i];\n \n+\t\tmemset(up, 0, sizeof(*up));\n \t\tmemcpy(&up->id, &kp->id, sizeof(up->id));\n \t\tup->family = kp->encap_family;\n \t\tmemcpy(&up->saddr, &kp->saddr, sizeof(up->saddr));",
        "function_modified_lines": {
            "added": [
                "\t\tmemset(up, 0, sizeof(*up));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "net/xfrm/xfrm_user.c in the Linux kernel before 3.6 does not initialize certain structures, which allows local users to obtain sensitive information from kernel memory by leveraging the CAP_NET_ADMIN capability.",
        "id": 121
    },
    {
        "cve_id": "CVE-2017-0627",
        "code_before_change": "int uvc_ctrl_add_mapping(struct uvc_video_chain *chain,\n\tconst struct uvc_control_mapping *mapping)\n{\n\tstruct uvc_device *dev = chain->dev;\n\tstruct uvc_control_mapping *map;\n\tstruct uvc_entity *entity;\n\tstruct uvc_control *ctrl;\n\tint found = 0;\n\tint ret;\n\n\tif (mapping->id & ~V4L2_CTRL_ID_MASK) {\n\t\tuvc_trace(UVC_TRACE_CONTROL, \"Can't add mapping '%s', control \"\n\t\t\t\"id 0x%08x is invalid.\\n\", mapping->name,\n\t\t\tmapping->id);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Search for the matching (GUID/CS) control on the current chain */\n\tlist_for_each_entry(entity, &chain->entities, chain) {\n\t\tunsigned int i;\n\n\t\tif (UVC_ENTITY_TYPE(entity) != UVC_VC_EXTENSION_UNIT ||\n\t\t    !uvc_entity_match_guid(entity, mapping->entity))\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < entity->ncontrols; ++i) {\n\t\t\tctrl = &entity->controls[i];\n\t\t\tif (ctrl->index == mapping->selector - 1) {\n\t\t\t\tfound = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (found)\n\t\t\tbreak;\n\t}\n\tif (!found)\n\t\treturn -ENOENT;\n\n\tif (mutex_lock_interruptible(&chain->ctrl_mutex))\n\t\treturn -ERESTARTSYS;\n\n\t/* Perform delayed initialization of XU controls */\n\tret = uvc_ctrl_init_xu_ctrl(dev, ctrl);\n\tif (ret < 0) {\n\t\tret = -ENOENT;\n\t\tgoto done;\n\t}\n\n\tlist_for_each_entry(map, &ctrl->info.mappings, list) {\n\t\tif (mapping->id == map->id) {\n\t\t\tuvc_trace(UVC_TRACE_CONTROL, \"Can't add mapping '%s', \"\n\t\t\t\t\"control id 0x%08x already exists.\\n\",\n\t\t\t\tmapping->name, mapping->id);\n\t\t\tret = -EEXIST;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\t/* Prevent excess memory consumption */\n\tif (atomic_inc_return(&dev->nmappings) > UVC_MAX_CONTROL_MAPPINGS) {\n\t\tatomic_dec(&dev->nmappings);\n\t\tuvc_trace(UVC_TRACE_CONTROL, \"Can't add mapping '%s', maximum \"\n\t\t\t\"mappings count (%u) exceeded.\\n\", mapping->name,\n\t\t\tUVC_MAX_CONTROL_MAPPINGS);\n\t\tret = -ENOMEM;\n\t\tgoto done;\n\t}\n\n\tret = __uvc_ctrl_add_mapping(dev, ctrl, mapping);\n\tif (ret < 0)\n\t\tatomic_dec(&dev->nmappings);\n\ndone:\n\tmutex_unlock(&chain->ctrl_mutex);\n\treturn ret;\n}",
        "code_after_change": "int uvc_ctrl_add_mapping(struct uvc_video_chain *chain,\n\tconst struct uvc_control_mapping *mapping)\n{\n\tstruct uvc_device *dev = chain->dev;\n\tstruct uvc_control_mapping *map;\n\tstruct uvc_entity *entity;\n\tstruct uvc_control *ctrl;\n\tint found = 0;\n\tint ret;\n\n\tif (mapping->id & ~V4L2_CTRL_ID_MASK) {\n\t\tuvc_trace(UVC_TRACE_CONTROL, \"Can't add mapping '%s', control \"\n\t\t\t\"id 0x%08x is invalid.\\n\", mapping->name,\n\t\t\tmapping->id);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Search for the matching (GUID/CS) control on the current chain */\n\tlist_for_each_entry(entity, &chain->entities, chain) {\n\t\tunsigned int i;\n\n\t\tif (UVC_ENTITY_TYPE(entity) != UVC_VC_EXTENSION_UNIT ||\n\t\t    !uvc_entity_match_guid(entity, mapping->entity))\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < entity->ncontrols; ++i) {\n\t\t\tctrl = &entity->controls[i];\n\t\t\tif (ctrl->index == mapping->selector - 1) {\n\t\t\t\tfound = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (found)\n\t\t\tbreak;\n\t}\n\tif (!found)\n\t\treturn -ENOENT;\n\n\tif (mutex_lock_interruptible(&chain->ctrl_mutex))\n\t\treturn -ERESTARTSYS;\n\n\t/* Perform delayed initialization of XU controls */\n\tret = uvc_ctrl_init_xu_ctrl(dev, ctrl);\n\tif (ret < 0) {\n\t\tret = -ENOENT;\n\t\tgoto done;\n\t}\n\n\t/* Validate the user-provided bit-size and offset */\n\tif (mapping->size > 32 ||\n\t    mapping->offset + mapping->size > ctrl->info.size * 8) {\n\t\tret = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tlist_for_each_entry(map, &ctrl->info.mappings, list) {\n\t\tif (mapping->id == map->id) {\n\t\t\tuvc_trace(UVC_TRACE_CONTROL, \"Can't add mapping '%s', \"\n\t\t\t\t\"control id 0x%08x already exists.\\n\",\n\t\t\t\tmapping->name, mapping->id);\n\t\t\tret = -EEXIST;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\t/* Prevent excess memory consumption */\n\tif (atomic_inc_return(&dev->nmappings) > UVC_MAX_CONTROL_MAPPINGS) {\n\t\tatomic_dec(&dev->nmappings);\n\t\tuvc_trace(UVC_TRACE_CONTROL, \"Can't add mapping '%s', maximum \"\n\t\t\t\"mappings count (%u) exceeded.\\n\", mapping->name,\n\t\t\tUVC_MAX_CONTROL_MAPPINGS);\n\t\tret = -ENOMEM;\n\t\tgoto done;\n\t}\n\n\tret = __uvc_ctrl_add_mapping(dev, ctrl, mapping);\n\tif (ret < 0)\n\t\tatomic_dec(&dev->nmappings);\n\ndone:\n\tmutex_unlock(&chain->ctrl_mutex);\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -47,6 +47,13 @@\n \t\tgoto done;\n \t}\n \n+\t/* Validate the user-provided bit-size and offset */\n+\tif (mapping->size > 32 ||\n+\t    mapping->offset + mapping->size > ctrl->info.size * 8) {\n+\t\tret = -EINVAL;\n+\t\tgoto done;\n+\t}\n+\n \tlist_for_each_entry(map, &ctrl->info.mappings, list) {\n \t\tif (mapping->id == map->id) {\n \t\t\tuvc_trace(UVC_TRACE_CONTROL, \"Can't add mapping '%s', \"",
        "function_modified_lines": {
            "added": [
                "\t/* Validate the user-provided bit-size and offset */",
                "\tif (mapping->size > 32 ||",
                "\t    mapping->offset + mapping->size > ctrl->info.size * 8) {",
                "\t\tret = -EINVAL;",
                "\t\tgoto done;",
                "\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "An information disclosure vulnerability in the kernel UVC driver could enable a local malicious application to access data outside of its permission levels. This issue is rated as Moderate because it first requires compromising a privileged process. Product: Android. Versions: Kernel-3.10, Kernel-3.18. Android ID: A-33300353.",
        "id": 1172
    },
    {
        "cve_id": "CVE-2012-6543",
        "code_before_change": "static int l2tp_ip6_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t    int *uaddr_len, int peer)\n{\n\tstruct sockaddr_l2tpip6 *lsa = (struct sockaddr_l2tpip6 *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct l2tp_ip6_sock *lsk = l2tp_ip6_sk(sk);\n\n\tlsa->l2tp_family = AF_INET6;\n\tlsa->l2tp_flowinfo = 0;\n\tlsa->l2tp_scope_id = 0;\n\tif (peer) {\n\t\tif (!lsk->peer_conn_id)\n\t\t\treturn -ENOTCONN;\n\t\tlsa->l2tp_conn_id = lsk->peer_conn_id;\n\t\tlsa->l2tp_addr = np->daddr;\n\t\tif (np->sndflow)\n\t\t\tlsa->l2tp_flowinfo = np->flow_label;\n\t} else {\n\t\tif (ipv6_addr_any(&np->rcv_saddr))\n\t\t\tlsa->l2tp_addr = np->saddr;\n\t\telse\n\t\t\tlsa->l2tp_addr = np->rcv_saddr;\n\n\t\tlsa->l2tp_conn_id = lsk->conn_id;\n\t}\n\tif (ipv6_addr_type(&lsa->l2tp_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tlsa->l2tp_scope_id = sk->sk_bound_dev_if;\n\t*uaddr_len = sizeof(*lsa);\n\treturn 0;\n}",
        "code_after_change": "static int l2tp_ip6_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t    int *uaddr_len, int peer)\n{\n\tstruct sockaddr_l2tpip6 *lsa = (struct sockaddr_l2tpip6 *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct l2tp_ip6_sock *lsk = l2tp_ip6_sk(sk);\n\n\tlsa->l2tp_family = AF_INET6;\n\tlsa->l2tp_flowinfo = 0;\n\tlsa->l2tp_scope_id = 0;\n\tlsa->l2tp_unused = 0;\n\tif (peer) {\n\t\tif (!lsk->peer_conn_id)\n\t\t\treturn -ENOTCONN;\n\t\tlsa->l2tp_conn_id = lsk->peer_conn_id;\n\t\tlsa->l2tp_addr = np->daddr;\n\t\tif (np->sndflow)\n\t\t\tlsa->l2tp_flowinfo = np->flow_label;\n\t} else {\n\t\tif (ipv6_addr_any(&np->rcv_saddr))\n\t\t\tlsa->l2tp_addr = np->saddr;\n\t\telse\n\t\t\tlsa->l2tp_addr = np->rcv_saddr;\n\n\t\tlsa->l2tp_conn_id = lsk->conn_id;\n\t}\n\tif (ipv6_addr_type(&lsa->l2tp_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tlsa->l2tp_scope_id = sk->sk_bound_dev_if;\n\t*uaddr_len = sizeof(*lsa);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,7 @@\n \tlsa->l2tp_family = AF_INET6;\n \tlsa->l2tp_flowinfo = 0;\n \tlsa->l2tp_scope_id = 0;\n+\tlsa->l2tp_unused = 0;\n \tif (peer) {\n \t\tif (!lsk->peer_conn_id)\n \t\t\treturn -ENOTCONN;",
        "function_modified_lines": {
            "added": [
                "\tlsa->l2tp_unused = 0;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The l2tp_ip6_getname function in net/l2tp/l2tp_ip6.c in the Linux kernel before 3.6 does not initialize a certain structure member, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 127
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "void kernel_fpu_begin(void)\n{\n\tstruct task_struct *me = current;\n\n\tWARN_ON_ONCE(!irq_fpu_usable());\n\tpreempt_disable();\n\tif (__thread_has_fpu(me)) {\n\t\t__save_init_fpu(me);\n\t\t__thread_clear_has_fpu(me);\n\t\t/* We do 'stts()' in kernel_fpu_end() */\n\t} else if (!use_xsave()) {\n\t\tthis_cpu_write(fpu_owner_task, NULL);\n\t\tclts();\n\t}\n}",
        "code_after_change": "void kernel_fpu_begin(void)\n{\n\tstruct task_struct *me = current;\n\n\tWARN_ON_ONCE(!irq_fpu_usable());\n\tpreempt_disable();\n\tif (__thread_has_fpu(me)) {\n\t\t__save_init_fpu(me);\n\t\t__thread_clear_has_fpu(me);\n\t\t/* We do 'stts()' in kernel_fpu_end() */\n\t} else if (!use_eager_fpu()) {\n\t\tthis_cpu_write(fpu_owner_task, NULL);\n\t\tclts();\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \t\t__save_init_fpu(me);\n \t\t__thread_clear_has_fpu(me);\n \t\t/* We do 'stts()' in kernel_fpu_end() */\n-\t} else if (!use_xsave()) {\n+\t} else if (!use_eager_fpu()) {\n \t\tthis_cpu_write(fpu_owner_task, NULL);\n \t\tclts();\n \t}",
        "function_modified_lines": {
            "added": [
                "\t} else if (!use_eager_fpu()) {"
            ],
            "deleted": [
                "\t} else if (!use_xsave()) {"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1811
    },
    {
        "cve_id": "CVE-2012-6541",
        "code_before_change": "static int ccid3_hc_tx_getsockopt(struct sock *sk, const int optname, int len,\n\t\t\t\t  u32 __user *optval, int __user *optlen)\n{\n\tconst struct ccid3_hc_tx_sock *hc = ccid3_hc_tx_sk(sk);\n\tstruct tfrc_tx_info tfrc;\n\tconst void *val;\n\n\tswitch (optname) {\n\tcase DCCP_SOCKOPT_CCID_TX_INFO:\n\t\tif (len < sizeof(tfrc))\n\t\t\treturn -EINVAL;\n\t\ttfrc.tfrctx_x\t   = hc->tx_x;\n\t\ttfrc.tfrctx_x_recv = hc->tx_x_recv;\n\t\ttfrc.tfrctx_x_calc = hc->tx_x_calc;\n\t\ttfrc.tfrctx_rtt\t   = hc->tx_rtt;\n\t\ttfrc.tfrctx_p\t   = hc->tx_p;\n\t\ttfrc.tfrctx_rto\t   = hc->tx_t_rto;\n\t\ttfrc.tfrctx_ipi\t   = hc->tx_t_ipi;\n\t\tlen = sizeof(tfrc);\n\t\tval = &tfrc;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen) || copy_to_user(optval, val, len))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
        "code_after_change": "static int ccid3_hc_tx_getsockopt(struct sock *sk, const int optname, int len,\n\t\t\t\t  u32 __user *optval, int __user *optlen)\n{\n\tconst struct ccid3_hc_tx_sock *hc = ccid3_hc_tx_sk(sk);\n\tstruct tfrc_tx_info tfrc;\n\tconst void *val;\n\n\tswitch (optname) {\n\tcase DCCP_SOCKOPT_CCID_TX_INFO:\n\t\tif (len < sizeof(tfrc))\n\t\t\treturn -EINVAL;\n\t\tmemset(&tfrc, 0, sizeof(tfrc));\n\t\ttfrc.tfrctx_x\t   = hc->tx_x;\n\t\ttfrc.tfrctx_x_recv = hc->tx_x_recv;\n\t\ttfrc.tfrctx_x_calc = hc->tx_x_calc;\n\t\ttfrc.tfrctx_rtt\t   = hc->tx_rtt;\n\t\ttfrc.tfrctx_p\t   = hc->tx_p;\n\t\ttfrc.tfrctx_rto\t   = hc->tx_t_rto;\n\t\ttfrc.tfrctx_ipi\t   = hc->tx_t_ipi;\n\t\tlen = sizeof(tfrc);\n\t\tval = &tfrc;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen) || copy_to_user(optval, val, len))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,7 @@\n \tcase DCCP_SOCKOPT_CCID_TX_INFO:\n \t\tif (len < sizeof(tfrc))\n \t\t\treturn -EINVAL;\n+\t\tmemset(&tfrc, 0, sizeof(tfrc));\n \t\ttfrc.tfrctx_x\t   = hc->tx_x;\n \t\ttfrc.tfrctx_x_recv = hc->tx_x_recv;\n \t\ttfrc.tfrctx_x_calc = hc->tx_x_calc;",
        "function_modified_lines": {
            "added": [
                "\t\tmemset(&tfrc, 0, sizeof(tfrc));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The ccid3_hc_tx_getsockopt function in net/dccp/ccids/ccid3.c in the Linux kernel before 3.6 does not initialize a certain structure, which allows local users to obtain sensitive information from kernel stack memory via a crafted application.",
        "id": 125
    },
    {
        "cve_id": "CVE-2015-7885",
        "code_before_change": "long dgnc_mgmt_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tunsigned long flags;\n\tvoid __user *uarg = (void __user *)arg;\n\n\tswitch (cmd) {\n\tcase DIGI_GETDD:\n\t{\n\t\t/*\n\t\t * This returns the total number of boards\n\t\t * in the system, as well as driver version\n\t\t * and has space for a reserved entry\n\t\t */\n\t\tstruct digi_dinfo ddi;\n\n\t\tspin_lock_irqsave(&dgnc_global_lock, flags);\n\n\t\tddi.dinfo_nboards = dgnc_NumBoards;\n\t\tsprintf(ddi.dinfo_version, \"%s\", DG_PART);\n\n\t\tspin_unlock_irqrestore(&dgnc_global_lock, flags);\n\n\t\tif (copy_to_user(uarg, &ddi, sizeof(ddi)))\n\t\t\treturn -EFAULT;\n\n\t\tbreak;\n\t}\n\n\tcase DIGI_GETBD:\n\t{\n\t\tint brd;\n\n\t\tstruct digi_info di;\n\n\t\tif (copy_from_user(&brd, uarg, sizeof(int)))\n\t\t\treturn -EFAULT;\n\n\t\tif (brd < 0 || brd >= dgnc_NumBoards)\n\t\t\treturn -ENODEV;\n\n\t\tmemset(&di, 0, sizeof(di));\n\n\t\tdi.info_bdnum = brd;\n\n\t\tspin_lock_irqsave(&dgnc_Board[brd]->bd_lock, flags);\n\n\t\tdi.info_bdtype = dgnc_Board[brd]->dpatype;\n\t\tdi.info_bdstate = dgnc_Board[brd]->dpastatus;\n\t\tdi.info_ioport = 0;\n\t\tdi.info_physaddr = (ulong)dgnc_Board[brd]->membase;\n\t\tdi.info_physsize = (ulong)dgnc_Board[brd]->membase\n\t\t\t- dgnc_Board[brd]->membase_end;\n\t\tif (dgnc_Board[brd]->state != BOARD_FAILED)\n\t\t\tdi.info_nports = dgnc_Board[brd]->nasync;\n\t\telse\n\t\t\tdi.info_nports = 0;\n\n\t\tspin_unlock_irqrestore(&dgnc_Board[brd]->bd_lock, flags);\n\n\t\tif (copy_to_user(uarg, &di, sizeof(di)))\n\t\t\treturn -EFAULT;\n\n\t\tbreak;\n\t}\n\n\tcase DIGI_GET_NI_INFO:\n\t{\n\t\tstruct channel_t *ch;\n\t\tstruct ni_info ni;\n\t\tunsigned char mstat = 0;\n\t\tuint board = 0;\n\t\tuint channel = 0;\n\n\t\tif (copy_from_user(&ni, uarg, sizeof(ni)))\n\t\t\treturn -EFAULT;\n\n\t\tboard = ni.board;\n\t\tchannel = ni.channel;\n\n\t\t/* Verify boundaries on board */\n\t\tif (board >= dgnc_NumBoards)\n\t\t\treturn -ENODEV;\n\n\t\t/* Verify boundaries on channel */\n\t\tif (channel >= dgnc_Board[board]->nasync)\n\t\t\treturn -ENODEV;\n\n\t\tch = dgnc_Board[board]->channels[channel];\n\n\t\tif (!ch || ch->magic != DGNC_CHANNEL_MAGIC)\n\t\t\treturn -ENODEV;\n\n\t\tmemset(&ni, 0, sizeof(ni));\n\t\tni.board = board;\n\t\tni.channel = channel;\n\n\t\tspin_lock_irqsave(&ch->ch_lock, flags);\n\n\t\tmstat = (ch->ch_mostat | ch->ch_mistat);\n\n\t\tif (mstat & UART_MCR_DTR) {\n\t\t\tni.mstat |= TIOCM_DTR;\n\t\t\tni.dtr = TIOCM_DTR;\n\t\t}\n\t\tif (mstat & UART_MCR_RTS) {\n\t\t\tni.mstat |= TIOCM_RTS;\n\t\t\tni.rts = TIOCM_RTS;\n\t\t}\n\t\tif (mstat & UART_MSR_CTS) {\n\t\t\tni.mstat |= TIOCM_CTS;\n\t\t\tni.cts = TIOCM_CTS;\n\t\t}\n\t\tif (mstat & UART_MSR_RI) {\n\t\t\tni.mstat |= TIOCM_RI;\n\t\t\tni.ri = TIOCM_RI;\n\t\t}\n\t\tif (mstat & UART_MSR_DCD) {\n\t\t\tni.mstat |= TIOCM_CD;\n\t\t\tni.dcd = TIOCM_CD;\n\t\t}\n\t\tif (mstat & UART_MSR_DSR)\n\t\t\tni.mstat |= TIOCM_DSR;\n\n\t\tni.iflag = ch->ch_c_iflag;\n\t\tni.oflag = ch->ch_c_oflag;\n\t\tni.cflag = ch->ch_c_cflag;\n\t\tni.lflag = ch->ch_c_lflag;\n\n\t\tif (ch->ch_digi.digi_flags & CTSPACE ||\n\t\t    ch->ch_c_cflag & CRTSCTS)\n\t\t\tni.hflow = 1;\n\t\telse\n\t\t\tni.hflow = 0;\n\n\t\tif ((ch->ch_flags & CH_STOPI) ||\n\t\t    (ch->ch_flags & CH_FORCED_STOPI))\n\t\t\tni.recv_stopped = 1;\n\t\telse\n\t\t\tni.recv_stopped = 0;\n\n\t\tif ((ch->ch_flags & CH_STOP) || (ch->ch_flags & CH_FORCED_STOP))\n\t\t\tni.xmit_stopped = 1;\n\t\telse\n\t\t\tni.xmit_stopped = 0;\n\n\t\tni.curtx = ch->ch_txcount;\n\t\tni.currx = ch->ch_rxcount;\n\n\t\tni.baud = ch->ch_old_baud;\n\n\t\tspin_unlock_irqrestore(&ch->ch_lock, flags);\n\n\t\tif (copy_to_user(uarg, &ni, sizeof(ni)))\n\t\t\treturn -EFAULT;\n\n\t\tbreak;\n\t}\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "long dgnc_mgmt_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tunsigned long flags;\n\tvoid __user *uarg = (void __user *)arg;\n\n\tswitch (cmd) {\n\tcase DIGI_GETDD:\n\t{\n\t\t/*\n\t\t * This returns the total number of boards\n\t\t * in the system, as well as driver version\n\t\t * and has space for a reserved entry\n\t\t */\n\t\tstruct digi_dinfo ddi;\n\n\t\tspin_lock_irqsave(&dgnc_global_lock, flags);\n\n\t\tmemset(&ddi, 0, sizeof(ddi));\n\t\tddi.dinfo_nboards = dgnc_NumBoards;\n\t\tsprintf(ddi.dinfo_version, \"%s\", DG_PART);\n\n\t\tspin_unlock_irqrestore(&dgnc_global_lock, flags);\n\n\t\tif (copy_to_user(uarg, &ddi, sizeof(ddi)))\n\t\t\treturn -EFAULT;\n\n\t\tbreak;\n\t}\n\n\tcase DIGI_GETBD:\n\t{\n\t\tint brd;\n\n\t\tstruct digi_info di;\n\n\t\tif (copy_from_user(&brd, uarg, sizeof(int)))\n\t\t\treturn -EFAULT;\n\n\t\tif (brd < 0 || brd >= dgnc_NumBoards)\n\t\t\treturn -ENODEV;\n\n\t\tmemset(&di, 0, sizeof(di));\n\n\t\tdi.info_bdnum = brd;\n\n\t\tspin_lock_irqsave(&dgnc_Board[brd]->bd_lock, flags);\n\n\t\tdi.info_bdtype = dgnc_Board[brd]->dpatype;\n\t\tdi.info_bdstate = dgnc_Board[brd]->dpastatus;\n\t\tdi.info_ioport = 0;\n\t\tdi.info_physaddr = (ulong)dgnc_Board[brd]->membase;\n\t\tdi.info_physsize = (ulong)dgnc_Board[brd]->membase\n\t\t\t- dgnc_Board[brd]->membase_end;\n\t\tif (dgnc_Board[brd]->state != BOARD_FAILED)\n\t\t\tdi.info_nports = dgnc_Board[brd]->nasync;\n\t\telse\n\t\t\tdi.info_nports = 0;\n\n\t\tspin_unlock_irqrestore(&dgnc_Board[brd]->bd_lock, flags);\n\n\t\tif (copy_to_user(uarg, &di, sizeof(di)))\n\t\t\treturn -EFAULT;\n\n\t\tbreak;\n\t}\n\n\tcase DIGI_GET_NI_INFO:\n\t{\n\t\tstruct channel_t *ch;\n\t\tstruct ni_info ni;\n\t\tunsigned char mstat = 0;\n\t\tuint board = 0;\n\t\tuint channel = 0;\n\n\t\tif (copy_from_user(&ni, uarg, sizeof(ni)))\n\t\t\treturn -EFAULT;\n\n\t\tboard = ni.board;\n\t\tchannel = ni.channel;\n\n\t\t/* Verify boundaries on board */\n\t\tif (board >= dgnc_NumBoards)\n\t\t\treturn -ENODEV;\n\n\t\t/* Verify boundaries on channel */\n\t\tif (channel >= dgnc_Board[board]->nasync)\n\t\t\treturn -ENODEV;\n\n\t\tch = dgnc_Board[board]->channels[channel];\n\n\t\tif (!ch || ch->magic != DGNC_CHANNEL_MAGIC)\n\t\t\treturn -ENODEV;\n\n\t\tmemset(&ni, 0, sizeof(ni));\n\t\tni.board = board;\n\t\tni.channel = channel;\n\n\t\tspin_lock_irqsave(&ch->ch_lock, flags);\n\n\t\tmstat = (ch->ch_mostat | ch->ch_mistat);\n\n\t\tif (mstat & UART_MCR_DTR) {\n\t\t\tni.mstat |= TIOCM_DTR;\n\t\t\tni.dtr = TIOCM_DTR;\n\t\t}\n\t\tif (mstat & UART_MCR_RTS) {\n\t\t\tni.mstat |= TIOCM_RTS;\n\t\t\tni.rts = TIOCM_RTS;\n\t\t}\n\t\tif (mstat & UART_MSR_CTS) {\n\t\t\tni.mstat |= TIOCM_CTS;\n\t\t\tni.cts = TIOCM_CTS;\n\t\t}\n\t\tif (mstat & UART_MSR_RI) {\n\t\t\tni.mstat |= TIOCM_RI;\n\t\t\tni.ri = TIOCM_RI;\n\t\t}\n\t\tif (mstat & UART_MSR_DCD) {\n\t\t\tni.mstat |= TIOCM_CD;\n\t\t\tni.dcd = TIOCM_CD;\n\t\t}\n\t\tif (mstat & UART_MSR_DSR)\n\t\t\tni.mstat |= TIOCM_DSR;\n\n\t\tni.iflag = ch->ch_c_iflag;\n\t\tni.oflag = ch->ch_c_oflag;\n\t\tni.cflag = ch->ch_c_cflag;\n\t\tni.lflag = ch->ch_c_lflag;\n\n\t\tif (ch->ch_digi.digi_flags & CTSPACE ||\n\t\t    ch->ch_c_cflag & CRTSCTS)\n\t\t\tni.hflow = 1;\n\t\telse\n\t\t\tni.hflow = 0;\n\n\t\tif ((ch->ch_flags & CH_STOPI) ||\n\t\t    (ch->ch_flags & CH_FORCED_STOPI))\n\t\t\tni.recv_stopped = 1;\n\t\telse\n\t\t\tni.recv_stopped = 0;\n\n\t\tif ((ch->ch_flags & CH_STOP) || (ch->ch_flags & CH_FORCED_STOP))\n\t\t\tni.xmit_stopped = 1;\n\t\telse\n\t\t\tni.xmit_stopped = 0;\n\n\t\tni.curtx = ch->ch_txcount;\n\t\tni.currx = ch->ch_rxcount;\n\n\t\tni.baud = ch->ch_old_baud;\n\n\t\tspin_unlock_irqrestore(&ch->ch_lock, flags);\n\n\t\tif (copy_to_user(uarg, &ni, sizeof(ni)))\n\t\t\treturn -EFAULT;\n\n\t\tbreak;\n\t}\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -15,6 +15,7 @@\n \n \t\tspin_lock_irqsave(&dgnc_global_lock, flags);\n \n+\t\tmemset(&ddi, 0, sizeof(ddi));\n \t\tddi.dinfo_nboards = dgnc_NumBoards;\n \t\tsprintf(ddi.dinfo_version, \"%s\", DG_PART);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tmemset(&ddi, 0, sizeof(ddi));"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "The dgnc_mgmt_ioctl function in drivers/staging/dgnc/dgnc_mgmt.c in the Linux kernel through 4.3.3 does not initialize a certain structure member, which allows local users to obtain sensitive information from kernel memory via a crafted application.",
        "id": 795
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "static inline void fpu_copy(struct task_struct *dst, struct task_struct *src)\n{\n\tif (use_xsave()) {\n\t\tstruct xsave_struct *xsave = &dst->thread.fpu.state->xsave;\n\n\t\tmemset(&xsave->xsave_hdr, 0, sizeof(struct xsave_hdr_struct));\n\t\txsave_state(xsave, -1);\n\t} else {\n\t\tstruct fpu *dfpu = &dst->thread.fpu;\n\t\tstruct fpu *sfpu = &src->thread.fpu;\n\n\t\tunlazy_fpu(src);\n\t\tmemcpy(dfpu->state, sfpu->state, xstate_size);\n\t}\n}",
        "code_after_change": "static inline void fpu_copy(struct task_struct *dst, struct task_struct *src)\n{\n\tif (use_eager_fpu()) {\n\t\tmemset(&dst->thread.fpu.state->xsave, 0, xstate_size);\n\t\t__save_fpu(dst);\n\t} else {\n\t\tstruct fpu *dfpu = &dst->thread.fpu;\n\t\tstruct fpu *sfpu = &src->thread.fpu;\n\n\t\tunlazy_fpu(src);\n\t\tmemcpy(dfpu->state, sfpu->state, xstate_size);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,8 @@\n static inline void fpu_copy(struct task_struct *dst, struct task_struct *src)\n {\n-\tif (use_xsave()) {\n-\t\tstruct xsave_struct *xsave = &dst->thread.fpu.state->xsave;\n-\n-\t\tmemset(&xsave->xsave_hdr, 0, sizeof(struct xsave_hdr_struct));\n-\t\txsave_state(xsave, -1);\n+\tif (use_eager_fpu()) {\n+\t\tmemset(&dst->thread.fpu.state->xsave, 0, xstate_size);\n+\t\t__save_fpu(dst);\n \t} else {\n \t\tstruct fpu *dfpu = &dst->thread.fpu;\n \t\tstruct fpu *sfpu = &src->thread.fpu;",
        "function_modified_lines": {
            "added": [
                "\tif (use_eager_fpu()) {",
                "\t\tmemset(&dst->thread.fpu.state->xsave, 0, xstate_size);",
                "\t\t__save_fpu(dst);"
            ],
            "deleted": [
                "\tif (use_xsave()) {",
                "\t\tstruct xsave_struct *xsave = &dst->thread.fpu.state->xsave;",
                "",
                "\t\tmemset(&xsave->xsave_hdr, 0, sizeof(struct xsave_hdr_struct));",
                "\t\txsave_state(xsave, -1);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1801
    },
    {
        "cve_id": "CVE-2022-33742",
        "code_before_change": "static struct grant *get_grant(grant_ref_t *gref_head,\n\t\t\t       unsigned long gfn,\n\t\t\t       struct blkfront_ring_info *rinfo)\n{\n\tstruct grant *gnt_list_entry = get_free_grant(rinfo);\n\tstruct blkfront_info *info = rinfo->dev_info;\n\n\tif (gnt_list_entry->gref != INVALID_GRANT_REF)\n\t\treturn gnt_list_entry;\n\n\t/* Assign a gref to this page */\n\tgnt_list_entry->gref = gnttab_claim_grant_reference(gref_head);\n\tBUG_ON(gnt_list_entry->gref == -ENOSPC);\n\tif (info->feature_persistent)\n\t\tgrant_foreign_access(gnt_list_entry, info);\n\telse {\n\t\t/* Grant access to the GFN passed by the caller */\n\t\tgnttab_grant_foreign_access_ref(gnt_list_entry->gref,\n\t\t\t\t\t\tinfo->xbdev->otherend_id,\n\t\t\t\t\t\tgfn, 0);\n\t}\n\n\treturn gnt_list_entry;\n}",
        "code_after_change": "static struct grant *get_grant(grant_ref_t *gref_head,\n\t\t\t       unsigned long gfn,\n\t\t\t       struct blkfront_ring_info *rinfo)\n{\n\tstruct grant *gnt_list_entry = get_free_grant(rinfo);\n\tstruct blkfront_info *info = rinfo->dev_info;\n\n\tif (gnt_list_entry->gref != INVALID_GRANT_REF)\n\t\treturn gnt_list_entry;\n\n\t/* Assign a gref to this page */\n\tgnt_list_entry->gref = gnttab_claim_grant_reference(gref_head);\n\tBUG_ON(gnt_list_entry->gref == -ENOSPC);\n\tif (info->bounce)\n\t\tgrant_foreign_access(gnt_list_entry, info);\n\telse {\n\t\t/* Grant access to the GFN passed by the caller */\n\t\tgnttab_grant_foreign_access_ref(gnt_list_entry->gref,\n\t\t\t\t\t\tinfo->xbdev->otherend_id,\n\t\t\t\t\t\tgfn, 0);\n\t}\n\n\treturn gnt_list_entry;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \t/* Assign a gref to this page */\n \tgnt_list_entry->gref = gnttab_claim_grant_reference(gref_head);\n \tBUG_ON(gnt_list_entry->gref == -ENOSPC);\n-\tif (info->feature_persistent)\n+\tif (info->bounce)\n \t\tgrant_foreign_access(gnt_list_entry, info);\n \telse {\n \t\t/* Grant access to the GFN passed by the caller */",
        "function_modified_lines": {
            "added": [
                "\tif (info->bounce)"
            ],
            "deleted": [
                "\tif (info->feature_persistent)"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "id": 3581
    },
    {
        "cve_id": "CVE-2018-3665",
        "code_before_change": "static inline void save_init_fpu(struct task_struct *tsk)\n{\n\tWARN_ON_ONCE(!__thread_has_fpu(tsk));\n\n\tif (use_xsave()) {\n\t\txsave_state(&tsk->thread.fpu.state->xsave, -1);\n\t\treturn;\n\t}\n\n\tpreempt_disable();\n\t__save_init_fpu(tsk);\n\t__thread_fpu_end(tsk);\n\tpreempt_enable();\n}",
        "code_after_change": "static inline void save_init_fpu(struct task_struct *tsk)\n{\n\tWARN_ON_ONCE(!__thread_has_fpu(tsk));\n\n\tif (use_eager_fpu()) {\n\t\t__save_fpu(tsk);\n\t\treturn;\n\t}\n\n\tpreempt_disable();\n\t__save_init_fpu(tsk);\n\t__thread_fpu_end(tsk);\n\tpreempt_enable();\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,8 +2,8 @@\n {\n \tWARN_ON_ONCE(!__thread_has_fpu(tsk));\n \n-\tif (use_xsave()) {\n-\t\txsave_state(&tsk->thread.fpu.state->xsave, -1);\n+\tif (use_eager_fpu()) {\n+\t\t__save_fpu(tsk);\n \t\treturn;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\tif (use_eager_fpu()) {",
                "\t\t__save_fpu(tsk);"
            ],
            "deleted": [
                "\tif (use_xsave()) {",
                "\t\txsave_state(&tsk->thread.fpu.state->xsave, -1);"
            ]
        },
        "cwe": [
            "CWE-200"
        ],
        "cve_description": "System software utilizing Lazy FP state restore technique on systems using Intel Core-based microprocessors may potentially allow a local process to infer data from another process through a speculative execution side channel.",
        "id": 1802
    }
]